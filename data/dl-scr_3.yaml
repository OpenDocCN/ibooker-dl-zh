- en: Chapter 3\. Deep Learning from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。从头开始的深度学习
- en: 'You may not realize it, but you now have all the mathematical and conceptual
    foundations to answer the key questions about deep learning models that I posed
    at the beginning of the book: you understand *how* neural networks work—the computations
    involved with the matrix multiplications, the loss, and the partial derivatives
    with respect to that loss—as well as *why* those computations work (namely, the
    chain rule from calculus). We achieved this understanding by building neural networks
    from first principles, representing them as a series of “building blocks” where
    each building block was a single mathematical function. In this chapter, you’ll
    learn to represent these building blocks themselves as abstract Python classes
    and then use these classes to build deep learning models; by the end of this chapter,
    you will indeed have done “deep learning from scratch”!'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能没有意识到，但现在您已经具备回答本书开头提出的关于深度学习模型的关键问题的所有数学和概念基础：您了解*神经网络*是如何工作的——涉及矩阵乘法、损失和相对于该损失的偏导数的计算，以及这些计算为什么有效（即微积分中的链式法则）。通过从第一原理构建神经网络，将它们表示为一系列“构建块”，我们实现了这种理解。在本章中，您将学习将这些构建块本身表示为抽象的Python类，然后使用这些类构建深度学习模型；到本章结束时，您确实将完成“从头开始的深度学习”！
- en: 'We’ll also map the descriptions of neural networks in terms of these building
    blocks to more conventional descriptions of deep learning models that you may
    have heard before. For example, by the end of this chapter, you’ll know what it
    means for a deep learning model to have “multiple hidden layers.” This is really
    the essence of understanding a concept: being able to translate between high-level
    descriptions and low-level details of what is actually going on. Let’s begin building
    toward this translation. So far, we’ve described models just in terms of the operations
    that happen at a low level. In the first part of this chapter, we’ll map this
    description of models to common higher-level concepts such as “layers” that will
    ultimately allow us to more easily describe more complex models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将描述神经网络的描述与您之前可能听过的深度学习模型的更传统描述相匹配。例如，在本章结束时，您将了解深度学习模型具有“多个隐藏层”是什么意思。这实际上是理解一个概念的本质：能够在高级描述和实际发生的低级细节之间进行翻译。让我们开始朝着这个翻译的方向构建。到目前为止，我们只是根据低级别发生的操作来描述模型。在本章的第一部分中，我们将将模型的这种描述映射到常见的更高级概念，例如“层”，最终使我们能够更轻松地描述更复杂的模型。
- en: 'Deep Learning Definition: A First Pass'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习定义：第一次尝试
- en: 'What *is* a “deep learning” model? In the previous chapter, we defined a model
    as a mathematical function represented by a computational graph. The purpose of
    such a model was to try to map inputs, each drawn from some dataset with common
    characteristics (such as separate inputs representing different features of houses)
    to outputs drawn from a related distribution (such as the prices of those houses).
    We found that if we defined the model as a function that included *parameters*
    as inputs to some of its operations, we could “fit” it to optimally describe the
    data using the following procedure:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习”模型是什么？在前一章中，我们将模型定义为由计算图表示的数学函数。这样的模型的目的是尝试将输入映射到输出，每个输入都来自具有共同特征的数据集（例如，表示房屋不同特征的单独输入），输出来自相关分布（例如，这些房屋的价格）。我们发现，如果我们将模型定义为一个包括*参数*作为某些操作的输入的函数，我们可以通过以下过程“拟合”它以最佳地描述数据：
- en: Repeatedly feed observations through the model, keeping track of the quantities
    computed along the way during this “forward pass.”
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反复通过模型传递观察数据，跟踪在这个“前向传递”过程中计算的量。
- en: Calculate a *loss* representing how far off our model’s predictions were from
    the desired outputs or *target*.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算代表我们模型预测与期望输出或目标之间差距有多大的*损失*。
- en: Using the quantities computed on the forward pass and the chain rule math worked
    out in [Chapter 1](ch01.html#foundations), compute how much each of the input
    *parameters* ultimately affects this loss.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在“前向传递”中计算的量和在[第1章](ch01.html#foundations)中推导出的链式法则，计算每个输入*参数*最终对这个损失产生了多大影响。
- en: Update the values of the parameters so that the loss will hopefully be reduced
    when the next set of observations is passed through the model.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新参数的值，以便在下一组观察通过模型时，损失有望减少。
- en: We started out with a model containing just a series of linear operations transforming
    our features into the target (which turned out to be equivalent to a traditional
    linear regression model). This had the expected limitation that, even when fit
    “optimally,” the model could nevertheless represent only linear relationships
    between our features and our target.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初使用的模型只包含一系列线性操作，将我们的特征转换为目标（结果等同于传统的线性回归模型）。这带来了一个预期的限制，即即使在“最佳拟合”时，模型仍然只能表示特征和目标之间的线性关系。
- en: We then defined a function structure that applied these linear operations first,
    then a *non*linear operation (the `sigmoid` function), and then a final set of
    linear operations. We showed that with this modification, our model *could* learn
    something closer to the true, nonlinear relationship between input and output,
    while having the additional benefit that it could learn relationships between
    *combinations* of our input features and the target.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了一个函数结构，首先应用这些线性操作，然后是一个*非*线性操作（`sigmoid`函数），最后是一组线性操作。我们展示了通过这种修改，我们的模型*可以*学习更接近输入和输出之间真实的非线性关系，同时还具有额外的好处，即它可以学习输入特征和目标之间的*组合*关系。
- en: 'What is the connection between models like these and deep learning models?
    We’ll start with a somewhat clumsy attempt at a definition: deep learning models
    are represented by series of operations that have *at least two, nonconsecutive*
    nonlinear functions involved.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型与深度学习模型之间的联系是什么？我们将从一个有些笨拙的定义开始：深度学习模型由一系列操作表示，其中至少涉及两个非连续的非线性函数。
- en: I’ll show where this definition comes from shortly, but first note that since
    deep learning models are just a series of operations, the process of training
    them is in fact *identical* to the process we’ve been using for the simpler models
    we’ve already seen. After all, what allows this training process to work is the
    differentiability of the model with respect to its inputs; and as mentioned in
    [Chapter 1](ch01.html#foundations), the composition of differentiable functions
    is differentiable, so as long as the individual operations making up the function
    are differentiable, the whole function will be differentiable, and we’ll be able
    to train it using the same four-step training procedure just described.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我将很快展示这个定义的来源，但首先要注意的是，由于深度学习模型只是一系列操作，训练它们的过程实际上与我们已经看到的简单模型的训练过程是*相同*的。毕竟，使得这个训练过程能够工作的是模型相对于其输入的可微性；正如在[第1章](ch01.html#foundations)中提到的，可微函数的组合是可微的，因此只要组成函数的各个操作是可微的，整个函数就是可微的，我们就能够使用刚刚描述的相同的四步训练过程来训练它。
- en: However, so far our approach to actually training these models has been to compute
    these derivatives by manually coding the forward and backward passes and then
    multiplying the appropriate quantities together to get the derivatives. For the
    simple neural network model in [Chapter 2](ch02.html#fundamentals), this required
    17 steps. Because we’re describing the model at such a low level, it isn’t immediately
    clear how we could add more complexity to this model (or what exactly what that
    would mean) or even make a simple change such as swapping out a different nonlinear
    function for the sigmoid function. To transition to being able to build arbitrarily
    “deep” and otherwise “complex” deep learning models, we’ll have to think about
    where in these 17 steps we can create reusable components, at a higher level than
    individual operations, that we can swap in and out to build different models.
    To guide us in the right direction as far as which abstractions to create, we’ll
    try to map the operations we’ve been using to traditional descriptions of neural
    networks as being made up of “layers,” “neurons,” and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到目前为止，我们实际上训练这些模型的方法是通过手动编写前向和后向传递来计算这些导数，然后将适当的量相乘以获得导数。对于[第2章](ch02.html#fundamentals)中的简单神经网络模型，这需要17个步骤。由于我们在如此低的层次上描述模型，不清楚如何向这个模型添加更多复杂性（或者这到底意味着什么），甚至进行简单的更改，比如将另一个非线性函数替换为sigmoid函数。为了能够构建任意“深度”和其他“复杂”的深度学习模型，我们必须考虑在这17个步骤中哪里可以创建可重用的组件，比单个操作的层次更高，可以替换并构建不同的模型。为了指导我们创建哪些抽象，我们将尝试将我们一直在使用的操作映射到传统的神经网络描述，即由“层”、“神经元”等组成。
- en: As our first step, we’ll have to create an abstraction to represent the individual
    operations we’ve been working with so far, instead of continuing to code the same
    matrix multiplication and bias addition over and over again.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将不再重复编写相同的矩阵乘法和偏置添加，而是创建一个抽象来表示我们目前所使用的各个操作。
- en: 'The Building Blocks of Neural Networks: Operations'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的构建模块：操作
- en: 'The `Operation` class will represent one of the constituent functions in our
    neural networks. We know that at a high level, based on the way we’ve used such
    functions in our models, it should have `forward` and `backward` methods, each
    of which receives an `ndarray` as an input and outputs an `ndarray`. Some operations,
    such as matrix multiplication, seem to have *another* special kind of input, also
    an `ndarray`: the parameters. In our `Operation` class—or perhaps in another class
    that inherits from it—we should allow for `params` as another instance variable.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “Operation”类将代表我们神经网络中的一个组成函数。根据我们在模型中使用这些函数的方式，从高层次来看，它应该有“forward”和“backward”方法，每个方法接收一个“ndarray”作为输入并输出一个“ndarray”。一些操作，比如矩阵乘法，似乎有*另一种*特殊类型的输入，也是一个“ndarray”：参数。在我们的“Operation”类中——或者可能是在另一个继承自它的类中——我们应该允许“params”作为另一个实例变量。
- en: 'Another insight is that there seem to be two types of `Operation`s: some, such
    as the matrix multiplication, return an `ndarray` as output that is a different
    shape than the `ndarray` they received as input; by contrast, some `Operation`s,
    such as the `sigmoid` function, simply apply some function to each element of
    the input `ndarray`. What, then, is the “general rule” about the shapes of the
    `ndarray`s that get passed between our operations? Let’s consider the `ndarray`s
    passed through our `Operation`s: each `Operation` will send outputs forward on
    the forward pass and will receive an “output gradient” on the backward pass, which
    will represent the partial derivative of the loss with respect to every element
    of the `Operation`’s output (computed by the other `Operation`s that make up the
    network). Also on the backward pass, each `Operation` will send an “input gradient”
    backward, representing the partial derivative of the loss with respect to each
    element of the input.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个观点是，似乎有两种类型的“Operation”：一些，比如矩阵乘法，返回一个与其输入不同形状的“ndarray”作为输出；相比之下，一些“Operation”，比如“sigmoid”函数，只是将某个函数应用于输入“ndarray”的每个元素。那么，关于在我们的操作之间传递的“ndarray”的形状，有什么“一般规则”呢？让我们考虑通过我们的“Operation”传递的“ndarray”：每个“Operation”将在前向传递中向前发送输出，并在后向传递中接收一个“输出梯度”，这将代表“Operation”输出的每个元素相对于损失的偏导数（由组成网络的其他“Operation”计算）。此外，在后向传递中，每个“Operation”将向后发送一个“输入梯度”，表示相对于输入的每个元素的损失的偏导数。
- en: 'These facts place a few important restrictions on the workings of our `Operation`s
    that will help us ensure we’re computing the gradients correctly:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事实对我们的`Operation`的工作方式施加了一些重要的限制，这将帮助我们确保我们正确计算梯度：
- en: The shape of the *output gradient* `ndarray` must match the shape of the *output*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出梯度* `ndarray`的形状必须与*输出*的形状匹配。'
- en: The shape of the *input gradient* that the `Operation` sends backward during
    the backward pass must match the shape of the `Operation`’s *input*.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Operation`在反向传递期间发送的*输入梯度*的形状必须与`Operation`的*输入*的形状匹配。'
- en: This will all be clearer once you see it in a diagram; let’s look at that next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在图表中看到这一切，一切都会更清晰；让我们接着看。
- en: Diagram
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图表
- en: This is all summarized in [Figure 3-1](#fig_03-01), for an operation `O` that
    is receiving inputs from an operation `N` and passing outputs on to another operation
    `P`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都总结在[图3-1](#fig_03-01)中，对于一个操作`O`，它从操作`N`接收输入，然后将输出传递给另一个操作`P`。
- en: '![Neural net diagram](assets/dlfs_0301.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0301.png)'
- en: Figure 3-1\. An Operation, with input and output
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 一个带有输入和输出的Operation
- en: '[Figure 3-2](#fig_03-02) covers the case of an `Operation` with parameters.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-2](#fig_03-02)涵盖了带有参数的`Operation`的情况。'
- en: '![Neural net diagram](assets/dlfs_0302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0302.png)'
- en: Figure 3-2\. A ParamOperation, with input and output and parameters
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 一个带有输入、输出和参数的ParamOperation
- en: Code
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'With all this, we can write the fundamental building block for our neural network,
    an `Operation`, as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这一切，我们可以将我们的神经网络的基本构建模块，即`Operation`，写成：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For any individual `Operation` that we define, we’ll have to implement the `_output`
    and `_input_grad` functions, so named because of the quantities they compute.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们定义的任何单个`Operation`，我们将不得不实现`_output`和`_input_grad`函数，这两个函数的名称是因为它们计算的数量。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'We’re defining base classes like this primarily for pedagogical reasons: it
    is important to have the mental model that *all* `Operation`s you’ll encounter
    throughout deep learning fit this blueprint of sending inputs forward and gradients
    backward, with the shapes of what they receive on the forward pass matching the
    shapes of what they send backward on the backward pass, and vice versa.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要为了教学目的而定义这样的基类：重要的是要有这样的心智模型，即*所有*您在深度学习中遇到的`Operation`都符合这种向前发送输入和向后发送梯度的蓝图，前向传递时接收的形状与反向传递时发送的形状匹配，反之亦然。
- en: 'We’ll define the specific `Operation`s we’ve used thus far—matrix multiplication
    and so on—later in this chapter. First we’ll define another class that inherits
    from `Operation` that we’ll use specifically for `Operation`s that involve parameters:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面定义迄今为止使用的特定`Operation`，如矩阵乘法等。首先，我们将定义另一个从`Operation`继承的类，专门用于涉及参数的`Operation`：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to the base `Operation`, an individual `ParamOperation` would have to
    define the `_param_grad` function in addition to the `_output` and `_input_grad`
    functions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与基本`Operation`类似，一个单独的`ParamOperation`还必须定义`_param_grad`函数，除了`_output`和`_input_grad`函数。
- en: 'We have now formalized the neural network building blocks we’ve been using
    in our models so far. We could skip ahead and define neural networks directly
    in terms of these `Operation`s, but there is an intermediate class we’ve been
    dancing around for a chapter and a half that we’ll define first: the `Layer`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经正式化了迄今为止我们在模型中使用的神经网络构建模块。我们可以直接根据这些`Operation`定义神经网络，但是有一个我们已经绕了一个半章的中间类，我们将首先定义它：`Layer`。
- en: 'The Building Blocks of Neural Networks: Layers'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的构建模块：层
- en: 'In terms of `Operation`s, layers are a series of linear operations followed
    by a nonlinear operation. For example, our neural network from the last chapter
    could be said to have had five total operations: two linear operations—a weight
    multiplication and the addition of a bias term—followed the `sigmoid` function
    and then two more linear operations. In this case, we would say that the first
    three operations, up to and including the nonlinear one, would constitute the
    first layer, and the last two operations would constitute the second layer. In
    addition, we say that the input itself represents a special kind of layer called
    the *input* layer (in terms of numbering the layers, this layer doesn’t count,
    so that we can think of it as the “zeroth” layer). The last layer, similarly,
    is called the *output* layer. The middle layer—the “first one,” according to our
    numbering—also has an important name: it is called a *hidden* layer, since it
    is the only layer whose values we don’t typically see explicitly during the course
    of training.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就`Operation`而言，层是一系列线性操作后跟一个非线性操作。例如，我们上一章的神经网络可以说有五个总操作：两个线性操作——权重乘法和添加偏置项——跟随`sigmoid`函数，然后又两个线性操作。在这种情况下，我们会说前三个操作，包括非线性操作在内，构成第一层，而最后两个操作构成第二层。此外，我们说输入本身代表一种特殊类型的层，称为*输入*层（在编号层次上，这一层不计算，因此我们可以将其视为“零”层）。同样地，最后一层称为*输出*层。中间层——根据我们的编号，“第一个”——也有一个重要的名称：它被称为*隐藏*层，因为它是唯一一个在训练过程中我们通常不明确看到其值的层。
- en: The output layer is an important exception to this definition of layers, in
    that it does not *have* to have a nonlinear operation applied to it; this is simply
    because we often want the values that come out of this layer to have values between
    negative infinity and infinity (or at least between 0 and infinity), whereas nonlinear
    functions typically “squash down” their input to some subset of that range relevant
    to the particular problem we’re trying to solve (for example, the `sigmoid` function
    squashes down its input to between 0 and 1).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层是对层的这一定义的一个重要例外，因为它不一定*必须*对其应用非线性操作；这仅仅是因为我们通常希望这一层的输出值在负无穷到正无穷之间（或至少在0到正无穷之间），而非线性函数通常会将其输入“压缩”到与我们尝试解决的特定问题相关的该范围的某个子集（例如，`sigmoid`函数将其输入压缩到0到1之间）。
- en: Diagrams
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图表
- en: To make the connection explicit, [Figure 3-3](#fig_03-03) shows the diagram
    of the neural network from the prior chapter with the individual operations grouped
    into layers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确连接，[图3-3](#fig_03-03)显示了前一章中的神经网络的图表，其中将单独的操作分组到层中。
- en: '![Neural net diagram](assets/dlfs_0303.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图表](assets/dlfs_0303.png)'
- en: Figure 3-3\. The neural network from the prior chapter with the operations grouped
    into layers
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3。前一章中的神经网络，操作分组成层
- en: You can see that the input represents an “input” layer, the next three operations
    (ending with the `sigmoid` function) represent the next layer, and the last two
    operations represent the last layer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到输入表示“输入”层，接下来的三个操作（以“sigmoid”函数结束）表示下一层，最后两个操作表示最后一层。
- en: 'This is, of course, rather cumbersome. And that’s the point: representing neural
    networks as a series of individual operations, while showing clearly how neural
    networks work and how to train them, is too “low level” for anything more complicated
    than a two-layer neural network. That’s why the more common way to represent neural
    networks is in terms of layers, as shown in [Figure 3-4](#fig_03-04).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这相当繁琐。这就是问题所在：将神经网络表示为一系列单独的操作，同时清楚地显示神经网络的工作原理以及如何训练它们，对于比两层神经网络更复杂的任何东西来说都太“低级”。这就是为什么更常见的表示神经网络的方式是以层为单位，如[图3-4](#fig_03-04)所示。
- en: '![Neural net diagram](assets/dlfs_0304.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图表](assets/dlfs_0304.png)'
- en: Figure 3-4\. The neural network from the prior chapter in terms of layers
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。以层为单位的前一章中的神经网络
- en: Connection to the brain
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与大脑的连接
- en: 'Finally, let’s make one last connection between what we’ve seen so far and
    a notion you’ve likely heard before: each layer can be said to have a certain
    number of *neurons* equal to *the dimensionality of the vector that represents
    each observation in the layer’s output*. The neural network from the prior example
    can thus be thought of as having 13 neurons in the input layer, then 13 neurons
    (again) in the hidden layer, and one neuron in the output layer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将我们迄今所见的内容与您可能之前听过的概念之间建立最后一个连接：每个层可以说具有等于*表示该层输出中每个观察的向量的维度*的*神经元*数量。因此，前一个示例中的神经网络可以被认为在输入层有13个神经元，然后在隐藏层中有13个神经元（再次），在输出层中有一个神经元。
- en: 'Neurons in the brain have the property that they can receive inputs from many
    other neurons and will “fire” and send a signal forward only if the signals they
    receive cumulatively reach a certain “activation energy.” Neurons in the context
    of neural networks have a loosely analogous property: they do indeed send signals
    forward based on their inputs, but the inputs are transformed into outputs simply
    via a nonlinear function. Thus, this nonlinear function is called the *activation
    function*, and the values that come out of it are called the *activations* for
    that layer.^([1](ch03.html#idm45732624417528))'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑中的神经元具有这样的特性，它们可以从许多其他神经元接收输入，只有当它们累积接收到的信号达到一定的“激活能量”时，它们才会“发射”并向前发送信号。神经网络的神经元具有类似的属性：它们确实根据其输入向前发送信号，但是输入仅通过非线性函数转换为输出。因此，这个非线性函数被称为*激活函数*，从中出来的值被称为该层的*激活*。^([1](ch03.html#idm45732624417528))
- en: 'Now that we’ve defined layers, we can state the more conventional definition
    of deep learning: *deep learning models are neural networks with more than one
    hidden layer.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了层，我们可以陈述更传统的深度学习定义：*深度学习模型是具有多个隐藏层的神经网络。*
- en: We can see that this is equivalent to the earlier definition that was purely
    in terms of `Operation`s, since a layer is just a series of `Operation`s with
    a nonlinear operation at the end.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这等同于早期纯粹基于“操作”定义的定义，因为层只是一系列具有非线性操作的“操作”，最后是一个非线性操作。
- en: Now that we’ve defined a base class for our `Operation`s, let’s show how it
    can serve as the fundamental building block of the models we saw in the prior
    chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的“操作”定义了一个基类，让我们展示它如何可以作为我们在前一章中看到的模型的基本构建模块。
- en: Building Blocks on Building Blocks
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模块上的构建模块
- en: 'What specific `Operation`s do we need to implement for the models in the prior
    chapter to work? Based on our experience of implementing that neural network step
    by step, we know there are three kinds:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为前一章中的模型实现哪些特定的“操作”？根据我们逐步实现神经网络的经验，我们知道有三种：
- en: The matrix multiplication of the input with the matrix of parameters
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入与参数矩阵的矩阵乘法
- en: The addition of a bias term
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加偏置项
- en: The `sigmoid` activation function
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “sigmoid”激活函数
- en: 'Let’s start with the `WeightMultiply` `Operation`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从“WeightMultiply”“操作”开始：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here we simply code up the matrix multiplication on the forward pass, as well
    as the rules for “sending gradients backward” to both the inputs and the parameters
    on the backward pass (using the rules for doing so that we reasoned through at
    the end of [Chapter 1](ch01.html#foundations)). As you’ll see shortly, we can
    now use this as a *building block* that we can simply plug into our `Layer`s.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递中，我们简单地编码矩阵乘法，以及在反向传递中“向输入和参数发送梯度”的规则（使用我们在[第1章](ch01.html#foundations)末尾推理出的规则）。很快您将看到，我们现在可以将其用作我们可以简单插入到我们的“层”中的*构建模块*。
- en: 'Next up is the addition operation, which we’ll call `BiasAdd`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是加法操作，我们将其称为“BiasAdd”：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let’s do `sigmoid`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们做“sigmoid”：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This simply implements the math described in the previous chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是实现了前一章描述的数学。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For both `sigmoid` and the `ParamOperation`, the step during the backward pass
    where we compute:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“sigmoid”和“ParamOperation”，在反向传播期间计算的步骤是：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'is the step where we are applying the chain rule, and the corresponding rule
    for `WeightMultiply`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 是我们应用链规则的步骤，以及“WeightMultiply”的相应规则：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: is, as I argued in [Chapter 1](ch01.html#foundations), the analogue of the chain
    rule when the function in question is a matrix multiplication.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[第1章](ch01.html#foundations)中所说的，当涉及的函数是矩阵乘法时，这相当于链规则的类比。
- en: Now that we’ve defined these `Operation`s precisely, we can use *them* as building
    blocks to define a `Layer`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准确定义了这些`Operation`，我们可以将它们用作定义`Layer`的构建块。
- en: The Layer Blueprint
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层蓝图
- en: 'Because of the way we’ve written the `Operation`s, writing the `Layer` class
    is easy:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们编写了`Operation`的方式，编写`Layer`类很容易：
- en: 'The `forward` and `backward` methods simply involve sending the input successively
    forward through a series of `Operation`s—exactly as we’ve been doing in the diagrams
    all along! This is the most important fact about the working of `Layer`s; the
    rest of the code is a wrapper around this and mostly involves bookkeeping:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward`和`backward`方法只涉及将输入依次通过一系列`Operation`向前传递 - 就像我们一直在图表中所做的那样！这是关于`Layer`工作最重要的事实；代码的其余部分是围绕这一点的包装，并且主要涉及簿记：'
- en: Defining the correct series of `Operation`s in the `_setup_layer` function and
    initializing and storing the parameters in these `Operation`s (which will also
    take place in the `_setup_layer` function)
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`_setup_layer`函数中定义正确的`Operation`系列，并在这些`Operation`中初始化和存储参数（这也将在`_setup_layer`函数中进行）
- en: Storing the correct values in `self.input_` and `self.output` on the `forward`
    method
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`forward`方法中存储正确的值在`self.input_`和`self.output`中
- en: Performing the correct assertion checking in the `backward` method
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`backward`方法中执行正确的断言检查
- en: Finally, the `_params` and `_param_grads` functions simply extract the parameters
    and their gradients (with respect to the loss) from the `ParamOperation`s within
    the layer.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`_params`和`_param_grads`函数只是从层内的`ParamOperation`中提取参数及其梯度（相对于损失）。
- en: 'Here’s what all that looks like:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些看起来是这样的：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Just as we moved from an abstract definition of an `Operation` to the implementation
    of specific `Operation`s needed for the neural network from [Chapter 2](ch02.html#fundamentals),
    let’s now implement the `Layer` from that network as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们从抽象定义的`Operation`转向实现神经网络所需的特定`Operation`一样，让我们现在也实现该网络中的`Layer`。
- en: The Dense Layer
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密集层
- en: We called the `Operation`s we’ve been dealing with `WeightMultiply`, `BiasAdd`,
    and so on. What should we call the layer we’ve been using so far? A `LinearNonLinear`
    layer?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称我们一直在处理的`Operation`为`WeightMultiply`，`BiasAdd`等等。到目前为止我们一直在使用的层应该叫什么？`LinearNonLinear`层？
- en: 'A defining characteristic of this layer is that *each output neuron is a function
    of all of the input neurons*. That is what the matrix multiplication is really
    doing: if the matrix is <math><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    rows by <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    columns, the multiplication itself is computing <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    new features, each of which is a weighted linear combination of *all* of the <math><msub><mi>n</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math> input features.^([2](ch03.html#idm45732623512888))
    Thus these layers are often called *fully connected* layers; recently, in the
    popular `Keras` library, they are also often called `Dense` layers, a more concise
    term that gets across the same idea.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层的一个定义特征是*每个输出神经元都是所有输入神经元的函数*。这就是矩阵乘法真正做的事情：如果矩阵是<math><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>行乘以<math><msub><mi>n</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>列，那么乘法本身计算的是<math><msub><mi>n</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>个新特征，每个特征都是*所有*<math><msub><mi>n</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>个输入特征的加权线性组合。^([2](ch03.html#idm45732623512888))
    因此，这些层通常被称为*全连接*层；最近，在流行的`Keras`库中，它们也经常被称为`Dense`层，这是一个更简洁的术语，传达了相同的概念。
- en: Now that we know what to call it and why, let’s define the `Dense` layer in
    terms of the operations we’ve already defined—as you’ll see, because of how we
    defined our `Layer` base class, all we need to do is to put the `Operation`s defined
    in the previous section in as a list in the `_setup_layer` function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道该如何称呼它以及为什么，让我们根据我们已经定义的操作来定义`Dense`层 - 正如您将看到的，由于我们如何定义了我们的`Layer`基类，我们所需要做的就是在`_setup_layer`函数中将前一节中定义的`Operation`作为列表放入其中。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we’ll make the default activation a `Linear` activation, which really
    means we apply no activation, and simply apply the identity function to the output
    of the layer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将默认激活函数设置为`Linear`激活函数，这实际上意味着我们不应用激活函数，只是将恒等函数应用于层的输出。
- en: What building blocks should we now add on top of `Operation` and `Layer`? To
    train our model, we know we’ll need a `NeuralNetwork` class to wrap around `Layer`s,
    just as `Layer`s wrapped around `Operation`s. It isn’t obvious what other classes
    will be needed, so we’ll just dive in and build `NeuralNetwork` and figure out
    the other classes we’ll need as we go.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Operation`和`Layer`之上，我们现在应该添加哪些构建块？为了训练我们的模型，我们知道我们将需要一个`NeuralNetwork`类来包装`Layer`，就像`Layer`包装`Operation`一样。不明显需要哪些其他类，所以我们将直接着手构建`NeuralNetwork`，并在进行过程中找出我们需要的其他类。
- en: The NeuralNetwork Class, and Maybe Others
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络类，也许还有其他类
- en: 'What should our `NeuralNetwork` class be able to do? At a high level, it should
    be able to *learn from data*: more precisely, it should be able to take in batches
    of data representing “observations” (`X`) and “correct answers” (`y`) and learn
    the relationship between `X` and `y`, which means learning a function that can
    transform `X` into predictions `p` that are very close to `y`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`NeuralNetwork`类应该能做什么？在高层次上，它应该能够*从数据中学习*：更准确地说，它应该能够接收代表“观察”（`X`）和“正确答案”（`y`）的数据批次，并学习`X`和`y`之间的关系，这意味着学习一个能够将`X`转换为非常接近`y`的预测`p`的函数。
- en: 'How exactly will this learning take place, given the `Layer` and `Operation`
    classes just defined? Recalling how the model from the last chapter worked, we’ll
    implement the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于刚刚定义的`Layer`和`Operation`类，这种学习将如何进行？回顾上一章的模型是如何工作的，我们将实现以下内容：
- en: The neural network should take `X` and pass it successively forward through
    each `Layer` (which is really a convenient wrapper around feeding it through many
    `Operation`s), at which point the result will represent the `prediction`.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络应该接受`X`并将其逐步通过每个`Layer`（实际上是一个方便的包装器，用于通过许多`Operation`进行馈送），此时结果将代表`prediction`。
- en: Next, `prediction` should be compared with the value `y` to calculate the loss
    and generate the “loss gradient,” which is the partial derivative of the loss
    with respect to each element in the last layer in the network (namely, the one
    that generated the `prediction`).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，应该将`prediction`与值`y`进行比较，计算损失并生成“损失梯度”，这是与网络中最后一个层（即生成`prediction`的层）中的每个元素相关的损失的偏导数。
- en: Finally, we’ll send this loss gradient successively backward through each layer,
    along the way computing the “parameter gradients”—the partial derivative of the
    loss with respect to each of the parameters—and storing them in the corresponding
    `Operation`s.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将通过每个层将这个损失梯度逐步向后发送，同时计算“参数梯度”——损失对每个参数的偏导数，并将它们存储在相应的`Operation`中。
- en: Diagram
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图
- en: '[Figure 3-5](#backpropagation_now_in_terms) captures this description of a
    neural network in terms of `Layer`s.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-5](#backpropagation_now_in_terms)以`Layer`的术语捕捉了神经网络的描述。'
- en: '![Neural net diagram](assets/dlfs_0305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0305.png)'
- en: Figure 3-5\. Backpropagation, now in terms of Layers instead of Operations
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5。反向传播，现在以Layer而不是Operation的术语
- en: Code
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'How should we implement this? First, we’ll want our neural network to ultimately
    deal with `Layer`s the same way our `Layer`s dealt with `Operation`s. For example,
    we want the `forward` method to receive `X` as input and simply do something like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何实现这一点？首先，我们希望我们的神经网络最终处理`Layer`的方式与我们的`Layer`处理`Operation`的方式相同。例如，我们希望`forward`方法接收`X`作为输入，然后简单地执行类似以下的操作：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, we’ll want our `backward` method to take in an argument—let’s initially
    call it `grad`—and do something like:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们希望我们的`backward`方法接收一个参数——我们最初称之为`grad`——然后执行类似以下的操作：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Where will `grad` come from? It has to come from the *loss*, a special function
    that takes in the `prediction` along with `y` and:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`grad`将从哪里来？它必须来自*损失*，一个特殊的函数，它接收`prediction`以及`y`，然后：'
- en: Computes a single number representing the “penalty” for the network making that
    `prediction`.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算代表网络进行该`prediction`的“惩罚”的单个数字。
- en: Sends backward a gradient for every element of the `prediction` with respect
    to the loss. This gradient is what the last `Layer` in the network will receive
    as the input to its `backward` function.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对每个`prediction`中的元素，发送一个梯度与损失相关的反向梯度。这个梯度是网络中最后一个`Layer`将作为其`backward`函数输入接收的内容。
- en: In the example from the prior chapter, the loss function was the squared difference
    between the `prediction` and the target, and the gradient of the `prediction`
    with respect to the loss was computed accordingly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章的示例中，损失函数是`prediction`和目标之间的平方差，相应地计算了`prediction`相对于损失的梯度。
- en: How should we implement this? It seems like this concept is important enough
    to deserve its own class. Furthermore, this class can be implemented similarly
    to the `Layer` class, except the `forward` method will produce an actual number
    (a `float`) as the loss, instead of an `ndarray` to be sent forward to the next
    `Layer`. Let’s formalize this.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何实现这一点？这个概念似乎很重要，值得拥有自己的类。此外，这个类可以类似于`Layer`类实现，只是`forward`方法将产生一个实际数字（一个`float`）作为损失，而不是一个`ndarray`被发送到下一个`Layer`。让我们正式化这一点。
- en: Loss Class
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失类
- en: 'The `Loss` base class will be similar to `Layer`—the `forward` and `backward`
    methods will check that the shapes of the appropriate `ndarray`s are identical
    and define two methods, `_output` and `_input_grad`, that any subclass of `Loss`
    will have to define:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`Loss`基类将类似于`Layer`——`forward`和`backward`方法将检查适当的`ndarray`的形状是否相同，并定义两个方法，`_output`和`_input_grad`，任何`Loss`子类都必须定义：'
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As in the `Operation` class, we check that the gradient that the loss sends
    backward is the same shape as the `prediction` received as input from the last
    layer of the network:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Operation`类一样，我们检查损失向后发送的梯度与从网络的最后一层接收的`prediction`的形状是否相同：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we simply code the forward and backward rules of the mean squared error
    loss formula.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简单地编写均方误差损失公式的前向和反向规则。
- en: This is the last key building block we need to build deep learning from scratch.
    Let’s review how these pieces fit together and then proceed with building a model!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要从头开始构建深度学习的最后一个关键构建块。让我们回顾一下这些部分如何组合在一起，然后继续构建模型！
- en: Deep Learning from Scratch
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的深度学习
- en: 'We ultimately want to build a `NeuralNetwork` class, using [Figure 3-5](#backpropagation_now_in_terms)
    as a guide, that we can use to define and train deep learning models. Before we
    dive in and start coding, let’s describe precisely what such a class would be
    and how it would interact with the `Operation`, `Layer`, and `Loss` classes we
    just defined:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终希望构建一个`NeuralNetwork`类，使用[图3-5](#backpropagation_now_in_terms)作为指南，我们可以用来定义和训练深度学习模型。在我们深入编码之前，让我们准确描述一下这样一个类会是什么样的，以及它将如何与我们刚刚定义的`Operation`、`Layer`和`Loss`类进行交互：
- en: A `NeuralNetwork` will have a list of `Layer`s as an attribute. The `Layer`s
    would be as defined previously, with `forward` and `backward` methods. These methods
    take in `ndarray` objects and return `ndarray` objects.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NeuralNetwork`将具有`Layer`列表作为属性。`Layer`将如先前定义的那样，具有`forward`和`backward`方法。这些方法接受`ndarray`对象并返回`ndarray`对象。'
- en: Each `Layer` will have a list of `Operation`s saved in the `operations` attribute
    of the layer during the `_setup_layer` function.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个`Layer`在`_setup_layer`函数期间的`operations`属性中保存了一个`Operation`列表。
- en: These `Operation`s, just like the `Layer` itself, have `forward` and `backward`
    methods that take in `ndarray` objects as arguments and return `ndarray` objects
    as outputs.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些`Operation`，就像`Layer`本身一样，有`forward`和`backward`方法，接受`ndarray`对象作为参数并返回`ndarray`对象作为输出。
- en: In each operation, the shape of the `output_grad` received in the `backward`
    method must be the same as the shape of the `output` attribute of the `Layer`.
    The same is true for the shapes of the `input_grad` passed backward during the
    `backward` method and the `input_` attribute.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个操作中，`backward`方法中接收的`output_grad`的形状必须与`Layer`的`output`属性的形状相同。在`backward`方法期间向后传递的`input_grad`的形状和`input_`属性的形状也是如此。
- en: Some operations have parameters (stored in the `param` attribute); these operations
    inherit from the `ParamOperation` class. The same constraints on input and output
    shapes apply to `Layer`s and their `forward` and `backward` methods as well—they
    take in `ndarray` objects and output `ndarray` objects, and the shapes of the
    `input` and `output` attributes and their corresponding gradients must match.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些操作具有参数（存储在`param`属性中）；这些操作继承自`ParamOperation`类。`Layer`及其`forward`和`backward`方法的输入和输出形状的约束也适用于它们——它们接收`ndarray`对象并输出`ndarray`对象，`input`和`output`属性及其相应梯度的形状必须匹配。
- en: A `NeuralNetwork` will also have a `Loss`. This class will take the output of
    the last operation from the `NeuralNetwork` and the target, check that their shapes
    are the same, and calculate both a loss value (a number) and an `ndarray` `loss_grad`
    that will be fed into the output layer, starting backpropagation.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NeuralNetwork`还将有一个`Loss`。这个类将获取`NeuralNetwork`最后一个操作的输出和目标，检查它们的形状是否相同，并计算损失值（一个数字）和一个`ndarray`
    `loss_grad`，该`loss_grad`将被馈送到输出层，开始反向传播。'
- en: Implementing Batch Training
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现批量训练
- en: 'We’ve covered several times the high-level steps for training a model one batch
    at a time. They are important and worth repeating:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次介绍了逐批次训练模型的高级步骤。这些步骤很重要，值得重复：
- en: Feed input through the model function (the “forward pass”) to get a prediction.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型函数（“前向传递”）将输入馈送。
- en: Calculate the number representing the loss.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算代表损失的数字。
- en: Calculate the gradient of the loss with respect to the parameters, using the
    chain rule and the quantities computed during the forward pass.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用链式法则和在前向传递期间计算的量，计算损失相对于参数的梯度。
- en: Update the parameters using these gradients.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些梯度更新参数。
- en: We would then feed a new batch of data through and repeat these steps.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将通过一批新数据并重复这些步骤。
- en: 'Translating these steps into the `NeuralNetwork` framework just described is
    straightforward:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤转换为刚刚描述的`NeuralNetwork`框架是直接的：
- en: Receive `X` and `y` as inputs, both `ndarray`s.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收`X`和`y`作为输入，都是`ndarray`。
- en: Feed `X` successively forward through each `Layer`.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐个将`X`通过每个`Layer`向前传递。
- en: Use the `Loss` to produce loss value and the loss gradient to be sent backward.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Loss`生成损失值和损失梯度以进行反向传播。
- en: Use the loss gradient as input to the `backward` method for the network, which
    will calculate the `param_grads` for each layer in the network.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用损失梯度作为网络`backward`方法的输入，该方法将计算网络中每一层的`param_grads`。
- en: Call the `update_params` function on each layer, which will use the overall
    learning rate for the `NeuralNetwork` as well as the newly calculated `param_grads`.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每一层上调用`update_params`函数，该函数将使用`NeuralNetwork`的整体学习率以及新计算的`param_grads`。
- en: We finally have our full definition of a neural network that can accommodate
    batch training. Now let’s code it up.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终有了一个完整的神经网络定义，可以进行批量训练。现在让我们编写代码。
- en: 'NeuralNetwork: Code'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络：代码
- en: 'Coding all of this up is pretty straightforward:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 编写所有这些代码非常简单：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this `NeuralNetwork` class, we can implement the models from the prior
    chapter in a more modular, flexible way and define other models to represent complex
    nonlinear relationships between input and output. For example, here’s how to easily
    instantiate the two models we covered in the last chapter—the linear regression
    and the neural network:^([3](ch03.html#idm45732622822120))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个`NeuralNetwork`类，我们可以以更模块化、灵活的方式实现上一章中的模型，并定义其他模型来表示输入和输出之间的复杂非线性关系。例如，这里是如何轻松实例化我们在上一章中介绍的两个模型——线性回归和神经网络：^([3](ch03.html#idm45732622822120))
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’re basically done; now we just feed data repeatedly through the network in
    order for it to learn. To make this process cleaner and easier to extend to the
    more complicated deep learning scenarios we’ll see in the following chapter, however,
    it will help us to define another class that carries out the training, as well
    as an additional class that carries out the “learning,” or the actual updating
    of the `NeuralNetwork` parameters given the gradients computed on the backward
    pass. Let’s quickly define these two classes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们已经完成了；现在我们只需反复将数据通过网络以便学习。然而，为了使这个过程更清晰、更容易扩展到下一章中将看到的更复杂的深度学习场景，定义另一个类来执行训练以及另一个类来执行“学习”，即根据反向传播计算的梯度实际更新`NeuralNetwork`参数。让我们快速定义这两个类。
- en: Trainer and Optimizer
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练器和优化器
- en: 'First, let’s note the similarities between these classes and the code we used
    to train the network in [Chapter 2](ch02.html#fundamentals). There, we used the
    following code to implement the four steps described earlier for training the
    model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们注意这些类与我们在[第2章](ch02.html#fundamentals)中用于训练网络的代码之间的相似之处。在那里，我们使用以下代码来实现用于训练模型的前述四个步骤：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code was within a `for` loop that repeatedly fed data through the function
    defining and updated our network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码位于一个`for`循环中，该循环反复将数据通过定义和更新我们的网络的函数。
- en: 'With the classes we have now, we’ll ultimately do this inside a `fit` function
    within the `Trainer` class that will mostly be a wrapper around the `train` function
    used in the prior chapter. (The full code for it is in this chapter’s [Jupyter
    Notebook](https://oreil.ly/2MV0aZI) on the book’s GitHub page.) The main difference
    is that inside this new function, the first two lines from the preceding code
    block will be replaced with this line:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们现在的类，我们最终将在`Trainer`类中的`fit`函数内部执行这些操作，该函数将主要是对前一章中使用的`train`函数的包装。 （完整的代码在本章的[Jupyter
    Notebook](https://oreil.ly/2MV0aZI)中的书的GitHub页面上。）主要区别是，在这个新函数内部，前面代码块中的前两行将被替换为这一行：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Updating the parameters, which happens in the following two lines, will take
    place in a separate `Optimizer` class. And finally, the `for` loop that previously
    wrapped around all of this will take place in the `Trainer` class that wraps around
    the `NeuralNetwork` and the `Optimizer`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更新参数将在以下两行中进行，这将在一个单独的`Optimizer`类中进行。最后，之前包围所有这些内容的`for`循环将在包围`NeuralNetwork`和`Optimizer`的`Trainer`类中进行。
- en: Next, let’s discuss why we need an `Optimizer` class and what it should look
    like.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论为什么需要一个`Optimizer`类以及它应该是什么样子。
- en: Optimizer
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: In the model we described in the last chapter, each `Layer` contains a simple
    rule for updating the weights based on the parameters and their gradients. As
    we’ll touch on in the next chapter, there are many other update rules we can use,
    such as ones involving the *history* of gradient updates rather than just the
    gradient updates from the specific batch that was fed in at that iteration. Creating
    a separate `Optimizer` class will give us the flexibility to swap in one update
    rule for another, something that we’ll explore in more detail in the next chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章描述的模型中，每个`Layer`包含一个简单的规则，根据参数和它们的梯度来更新权重。正如我们将在下一章中提到的，我们可以使用许多其他更新规则，例如涉及梯度更新*历史*而不仅仅是在该迭代中传入的特定批次的梯度更新。创建一个单独的`Optimizer`类将使我们能够灵活地将一个更新规则替换为另一个，这是我们将在下一章中更详细地探讨的内容。
- en: Description and code
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述和代码
- en: 'The base `Optimizer` class will take in a `NeuralNetwork` and, every time the
    `step` function is called, will update the parameters of the network based on
    their current values, their gradients, and any other information stored in the
    `Optimizer`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的`Optimizer`类将接受一个`NeuralNetwork`，每次调用`step`函数时，将根据它们当前的值、梯度和`Optimizer`中存储的任何其他信息来更新网络的参数：
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And here’s how this looks with the straightforward update rule we’ve seen so
    far, known as *stochastic gradient descent*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们迄今为止看到的简单更新规则的实际情况，即*随机梯度下降*：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that while our `NeuralNetwork` class does not have an `_update_params`
    method, we do rely on the `params()` and `param_grads()` methods to extract the
    correct `ndarray`s for optimization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然我们的`NeuralNetwork`类没有`_update_params`方法，但我们依赖于`params()`和`param_grads()`方法来提取正确的`ndarray`以进行优化。
- en: That’s the basic `Optimizer` class; let’s cover the `Trainer` class next.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是基本的`Optimizer`类；接下来让我们来看一下`Trainer`类。
- en: Trainer
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练器
- en: 'In addition to training the model as described previously, the `Trainer` class
    also links together the `NeuralNetwork` with the `Optimizer`, ensuring the latter
    trains the former properly. You may have noticed in the previous section that
    we didn’t pass in a `NeuralNetwork` when initializing our `Optimizer`; instead,
    we’ll assign the `NeuralNetwork` to be an attribute of the `Optimizer` when we
    initialize the `Trainer` class shortly, with this line:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了按照前面描述的方式训练模型外，`Trainer`类还将`NeuralNetwork`与`Optimizer`连接在一起，确保后者正确训练前者。您可能已经注意到，在前一节中，我们在初始化`Optimizer`时没有传入`NeuralNetwork`；相反，我们将在不久后初始化`Trainer`类时将`NeuralNetwork`分配为`Optimizer`的属性，使用以下代码行：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the following subsection, I show a simplified but working version of the
    `Trainer` class that for now contains just the `fit` method. This method trains
    our model for a number of *epochs* and prints out the loss value after each set
    number of epochs. In each epoch, we:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我展示了一个简化但有效的`Trainer`类的工作版本，目前只包含`fit`方法。该方法为我们的模型训练了一定数量的*epochs*，并在每个一定数量的epochs后打印出损失值。在每个epoch中，我们：
- en: Shuffle the data at the beginning of the epoch
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在epoch开始时对数据进行洗牌
- en: Feed the data through the network in batches, updating the parameters after
    each batch has been fed through
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过网络以批次方式传递数据，每传递完一个批次后更新参数
- en: The epoch ends when we have fed the entire training set through the `Trainer`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过`Trainer`将整个训练集传递完时，该epoch结束。
- en: Trainer code
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练器代码
- en: 'In the following is the code for a simple version of the `Trainer` class, we
    hide two self-explanatory helper methods used during the `fit` function: `generate_batches`,
    which generates batches of data from `X_train` and `y_train` for training, and
    `permute_data`, which shuffles `X_train` and `y_train` at the beginning of each
    epoch. We also include a `restart` argument in the `train` function: if `True`
    (default), it will reinitialize the model’s parameters to random values upon calling
    the `train` function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单版本的`Trainer`类的代码，我们隐藏了在`fit`函数中使用的两个不言自明的辅助方法：`generate_batches`，它从`X_train`和`y_train`生成用于训练的数据批次，以及`permute_data`，它在每个epoch开始时对`X_train`和`y_train`进行洗牌。在`train`函数中还包括一个`restart`参数：如果为`True`（默认值），则在调用`train`函数时会重新初始化模型的参数为随机值：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the full version of this function in the book’s [GitHub repository](https://oreil.ly/2MV0aZI),
    we also implement *early stopping*, which does the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的[GitHub存储库](https://oreil.ly/2MV0aZI)中的这个函数的完整版本中，我们还实现了*提前停止*，它执行以下操作：
- en: It saves the loss value every `eval_every` epochs.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它每`eval_every`个epoch保存一次损失值。
- en: It checks whether the validation loss is lower than the last time it was calculated.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它检查验证损失是否低于上次计算时的值。
- en: If the validation loss is *not* lower, it uses the model from `eval_every` epochs
    ago.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果验证损失*不*更低，则使用`eval_every`个epoch之前的模型。
- en: Finally, we have everything we need to train these models!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经准备好训练这些模型了！
- en: Putting Everything Together
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Here is the full code to train our network using all the `Trainer` and `Optimizer`
    classes and the two models defined before—`linear_regression` and `neural_network`.
    We’ll set the learning rate to `0.01` and the maximum number of epochs to `50`
    and evaluate our models every `10` epochs:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用所有“Trainer”和“Optimizer”类以及之前定义的两个模型——“linear_regression”和“neural_network”来训练我们的网络的完整代码。我们将学习率设置为“0.01”，最大迭代次数设置为“50”，并且每“10”次迭代评估我们的模型：
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using the same model-scoring functions from [Chapter 2](ch02.html#fundamentals),
    and wrapping them inside an `eval_regression_model` function, gives us these results:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自[第2章](ch02.html#fundamentals)的相同模型评分函数，并将它们包装在一个“eval_regression_model”函数中，我们得到以下结果：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: These are similar to the results of the linear regression we ran in the last
    chapter, confirming that our framework is working.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与我们在上一章中运行的线性回归的结果类似，证实了我们的框架正在工作。
- en: 'Running the same code with the `neural_network` model with a hidden layer with
    13 neurons, we get the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用具有13个神经元的隐藏层的“neural_network”模型运行相同的代码，我们得到以下结果：
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again, these results are similar to what we saw in the prior chapter, and they’re
    significantly better than our straightforward linear regression.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这些结果与我们在上一章中看到的结果类似，它们比我们直接的线性回归要好得多。
- en: Our First Deep Learning Model (from Scratch)
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的第一个深度学习模型（从头开始）
- en: 'Now that all of that setup is out of the way, defining our first deep learning
    model is trivial:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 既然所有的设置都已经完成，定义我们的第一个深度学习模型就变得微不足道了：
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We won’t even try to be clever with this (yet). We’ll just add a hidden layer
    with the same dimensionality as the first layer, so that our network now has two
    hidden layers, each with 13 neurons.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至不会试图在这方面变得聪明（尚未）。我们只会添加一个与第一层具有相同维度的隐藏层，这样我们的网络现在有两个隐藏层，每个隐藏层有13个神经元。
- en: 'Training this using the same learning rate and evaluation schedule as the prior
    models yields the following result:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前模型相同的学习率和评估计划进行训练会产生以下结果：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We finally worked up to doing deep learning from scratch—and indeed, on this
    real-world problem, without the use of any tricks (just a bit of learning rate
    tuning), our deep learning model does perform slightly better than a neural network
    with just one hidden layer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终从头开始进行了深度学习，事实上，在这个真实世界的问题上，没有使用任何技巧（只是稍微调整学习率），我们的深度学习模型的表现略好于只有一个隐藏层的神经网络。
- en: More importantly, we did so by building a framework that is easily extensible.
    We could easily implement other kinds of `Operation`s, wrap them in new `Layer`s,
    and drop them right in, assuming that they have defined `_output` and `_input_grad`
    methods and that the dimensions of their inputs, outputs, and parameters match
    those of their respective gradients. Similarly, we could easily drop different
    activation functions into our existing layers and see if it decreases our error
    metrics; I encourage you to clone the book’s [GitHub repo](https://oreil.ly/deep-learning-github)
    and try this!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，我们通过构建一个易于扩展的框架来实现这一点。我们可以很容易地实现其他类型的“Operation”，将它们包装在新的“Layer”中，并将它们直接放入其中，假设它们已经定义了“_output”和“_input_grad”方法，并且它们的输入、输出和参数的维度与它们各自的梯度相匹配。同样地，我们可以很容易地将不同的激活函数放入我们现有的层中，看看是否会降低我们的错误指标；我鼓励你克隆本书的[GitHub存储库](https://oreil.ly/deep-learning-github)并尝试一下！
- en: Conclusion and Next Steps
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论和下一步
- en: In the next chapter, I’ll cover several tricks that will be essential to getting
    our models to train properly once we get to more challenging problems than this
    simple one^([4](ch03.html#idm45732621371848))—in particular, defining other `Loss`es
    and `Optimizer`s. I’ll also cover additional tricks for tuning our learning rates
    and modifying them throughout training, and I’ll show how to incorporate this
    into the `Optimizer` and `Trainer` classes. Finally, we’ll see Dropout, a new
    kind of `Operation` that has proven essential for increasing the training stability
    of deep learning models. Onward!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我将介绍几种技巧，这些技巧对于让我们的模型在面对比这个简单问题更具挑战性的问题时能够正确训练是至关重要的^([4](ch03.html#idm45732621371848))——特别是定义其他“Loss”和“Optimizer”。我还将介绍调整学习率和在整个训练过程中修改学习率的其他技巧，并展示如何将这些技巧融入“Optimizer”和“Trainer”类中。最后，我们将看到Dropout，这是一种新型的“Operation”，已被证明对增加深度学习模型的训练稳定性至关重要。继续前进！
- en: ^([1](ch03.html#idm45732624417528-marker)) Among all activation functions, the
    `sigmoid` function, which maps inputs to between 0 and 1, most closely mimics
    the actual activation of neurons in the brain, but in general activation functions
    can be any monotonic, nonlinear function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45732624417528-marker)) 在所有激活函数中，“sigmoid”函数最接近大脑中神经元的实际激活，它将输入映射到0到1之间，但一般来说，激活函数可以是任何单调的非线性函数。
- en: '^([2](ch03.html#idm45732623512888-marker)) As we’ll see in [Chapter 5](ch05.html#convolution),
    this is not true of all layers: in *convolutional* layers, for example, each output
    feature is a combination of *only a small subset* of the input features.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#idm45732623512888-marker)) 正如我们将在[第5章](ch05.html#convolution)中看到的，这并不适用于所有层：例如，在*卷积*层中，每个输出特征是输入特征的*一个小子集*的组合。
- en: ^([3](ch03.html#idm45732622822120-marker)) The learning rate of 0.01 isn’t special;
    we simply found it to be optimal in the course of experimenting while writing
    the prior chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#idm45732622822120-marker)) 学习率0.01并不特殊；我们只是在写前一章时在实验过程中发现它是最佳的。
- en: ^([4](ch03.html#idm45732621371848-marker)) Even on this simple problem, changing
    the hyperparameters slightly can cause the deep learning model to fail to beat
    the two-layer neural network. Clone the [GitHub repo](https://oreil.ly/deep-learning-github)
    and try it yourself!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#idm45732621371848-marker)) 即使在这个简单的问题上，稍微改变超参数可能会导致深度学习模型无法击败两层神经网络。克隆[GitHub存储库](https://oreil.ly/deep-learning-github)并尝试一下吧！
