- en: Chapter 10\. Using APIs in Data Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。在数据管道中使用API
- en: In their simplest form, pipelines may extract only data from one source such
    as a REST API and load to a destination such as a SQL table in a data warehouse.
    In practice, however, pipelines typically consist of multiple steps ... before
    delivering data to its final destination.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在最简单的情况下，管道可能只从单个源（如REST API）提取数据，并将其加载到目的地（如数据仓库中的SQL表）。然而，在实践中，管道通常由多个步骤组成...在将数据交付到最终目的地之前。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: James Densmore *Data Pipelines Pocket Reference* (O’Reilly, 2021)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: James Densmore *数据管道口袋参考* (O’Reilly, 2021)
- en: In [Chapter 9](ch09.html#chapter_9), you used a Jupyter Notebook to query APIs
    and create data analytics. Querying directly in a notebook is useful for exploratory
    data analysis, but it requires you to keep querying the API over and over again.
    When data teams create analytics products for production, they implement scheduled
    processes to keep an up-to-date copy of source data in the format they need. These
    structured processes are called *data pipelines* because source data flows into
    the pipeline and is prepared and stored to create data products. Other common
    terms for these processes are *Extract, Transform, Load (ETL)* or *Extract, Load,
    Transform (ELT)*, depending on the technical details of how they are implemented.
    *Data engineer* is the specialized role that focuses on the development and operation
    of data pipelines, but in many organizations, data scientists, data analysts,
    and infrastructure engineers also perform this work.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#chapter_9)中，您使用Jupyter Notebook查询API并创建数据分析。在笔记本中直接查询对于探索性数据分析很有用，但它要求您反复查询API。当数据团队为生产创建分析产品时，他们实施计划流程以保持源数据的最新副本，并按所需格式准备和存储。这些结构化流程被称为*数据管道*，因为源数据流入管道并准备和存储以创建数据产品。这些流程的其他常见术语是*提取、转换、加载(ETL)*或*提取、加载、转换(ELT)*，具体取决于它们实施的详细技术。*数据工程师*是专注于数据管道开发和运营的专门角色，但在许多组织中，数据科学家、数据分析师和基础设施工程师也执行这项工作。
- en: In this chapter, you will create a data pipeline to read SportsWorldCentral
    fantasy football player data using Apache Airflow, a popular open source tool
    for managing data pipelines using Python.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使用Apache Airflow创建一个数据管道来读取SportsWorldCentral梦幻足球球员数据，Apache Airflow是一个流行的开源工具，用于使用Python管理数据管道。
- en: Types of Data Sources for Data Pipelines
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道的数据源类型
- en: 'The potential data sources for data pipelines are almost endless. Here are
    a few examples:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的潜在数据源几乎是无限的。以下是一些示例：
- en: APIs
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: API
- en: REST APIs are the focus of this book, and they are an important data source
    for data pipelines. They are better suited for incremental updates than full loads,
    because sending the full contents of a data source may require many network calls.
    Other API styles such as GraphQL and SOAP are also common.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: REST API是本书的重点，它们是数据管道的重要数据源。与全量加载相比，它们更适合增量更新，因为发送数据源的全部内容可能需要许多网络调用。其他API样式，如GraphQL和SOAP，也很常见。
- en: Bulk files
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 批量文件
- en: Large datasets are often shared in some type of bulk file that can be downloaded
    and processed. This is an efficient way to process a very large data source. The
    file format of these may vary, but CSV and Parquet are popular formats for data
    science applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型数据集通常以某种批量文件的形式共享，可以下载和处理。这是一种处理非常大的数据源的高效方式。这些文件的格式可能不同，但CSV和Parquet是数据科学应用中流行的格式。
- en: Streaming data and message queues
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据和信息队列
- en: For near-real-time updates of data, streaming sources such as Apache Kafka or
    AWS Kinesis provide continuous feeds of updates.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于近实时数据更新，Apache Kafka或AWS Kinesis等流式源提供连续的更新流。
- en: Message queues
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列
- en: Message queue software such as RabbitMQ or AWS SQS provides asynchronous messaging,
    which allows transactions to be published in a holding location and picked up
    later by a subscriber.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列软件，如RabbitMQ或AWS SQS，提供异步消息传递，允许事务在暂存位置发布，并由订阅者稍后拾取。
- en: Direct database connections
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 直接数据库连接
- en: A connection to the source database allows a consumer to get data in its original
    format. These are more common for sharing data inside organizations than to outside
    consumers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与源数据库的连接允许消费者以原始格式获取数据。这些在组织内部共享数据比对外部消费者更常见。
- en: You will be creating a pipeline that uses REST APIs and bulk files in this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在本章中创建一个使用REST API和批量文件的管道。
- en: Planning Your Data Pipeline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划您的数据管道
- en: Your goal is to read SportsWorldCentral data and store it in a local database
    that you can keep up to date. This allows you to create analytics products such
    as reports and dashboards. For this scenario, you’ll assume that the API does
    not allow full downloads of the data, so you will need to use a bulk file for
    the initial load.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是读取 SportsWorldCentral 数据并将其存储在你可以保持更新的本地数据库中。这允许你创建分析产品，如报告和仪表板。对于这个场景，你将假设
    API 不允许完整下载数据，因此你需要使用批量文件进行初始加载。
- en: After that initial load, you want to get a daily update of any new records or
    records that have been updated. These changed records are commonly referred to
    as *delta* or *deltas*, using the mathematical term for “change.” By processing
    only the changed records, the update process will run more quickly and use fewer
    resources (and spend less money).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始加载之后，你希望获取任何新记录或已更新记录的每日更新。这些更改记录通常被称为 *delta* 或 *deltas*，使用数学术语“变化”来表示。通过仅处理更改记录，更新过程将运行得更快，并使用更少的资源（并节省更多资金）。
- en: '[Figure 10-1](#date_pipeline_plan_ch10) displays the data pipeline you are
    planning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#date_pipeline_plan_ch10) 显示了你计划中的数据管道。'
- en: '![Plan for your data pipeline](assets/haad_1001.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![你的数据管道计划](assets/haad_1001.png)'
- en: Figure 10-1\. Plan for your data pipeline
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 你的数据管道计划
- en: 'The pipeline includes two sources: bulk data files and an API. The rounded
    boxes represent two ETL processes and they both will update the analytics database,
    a local database that is used to create analytics products like dashboards and
    reports.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 管道包括两个来源：批量数据文件和 API。圆形框代表两个 ETL 流程，它们都将更新分析数据库，这是一个用于创建分析产品（如仪表板和报告）的本地数据库。
- en: Orchestrating the Data Pipeline with Apache Airflow
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Airflow 编排数据管道
- en: 'Airflow is best thought of as a spider in a web: it sits in the middle of your
    data processes and coordinates work happening across the different (distributed)
    systems.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Airflow 最好被看作是一个网络中的蜘蛛：它位于你的数据处理过程中间，协调跨不同（分布式）系统的工作。
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Julian de Ruiter and Bas Harenslak, *Data Pipelines with Apache Airflow* (Manning,
    2021)
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Julian de Ruiter 和 Bas Harenslak 著，*Apache Airflow 数据管道*（Manning，2021）
- en: Running multiple data processing work streams in production gets complicated
    quickly. Scheduling, error handling, and restarting failed processes require significant
    planning and design. These tasks are called *orchestration*, and this is what
    Apache Airflow is used for. As the number of data pipelines grows, you will benefit
    from using orchestration software instead of coding all of these tasks yourself.
    Airflow is a full-featured open source engine that uses Python for its configuration,
    and it handles many of the recurring tasks involved in data pipelines.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中运行多个数据处理工作流会迅速变得复杂。调度、错误处理和重启失败进程需要大量的规划和设计。这些任务被称为 *编排*，这正是 Apache Airflow
    的用途。随着数据管道数量的增加，使用编排软件而不是自己编写所有这些任务将为你带来好处。Airflow 是一个功能齐全的开源引擎，它使用 Python 进行配置，并处理数据管道中涉及到的许多重复性任务。
- en: Airflow has some specialized terminology that is not used in other data science
    programming. Astronomer’s [Airflow glossary](https://oreil.ly/IjTM4) is a complete
    source for these, but I will share some of the most important ones with you.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 有一些专门的术语，这些术语在其它数据科学编程中并不常用。天文学家的 [Airflow 术语表](https://oreil.ly/IjTM4)
    是这些术语的完整来源，但我会与你分享其中一些最重要的术语。
- en: Airflow uses terminology from mathematical graph theory. In graph theory, a
    *node* is a process and an *edge* is a flow between nodes. Using this terminology,
    a *directed acyclic graph* (DAG) is a top-level process that contains steps proceeding
    in one direction without any loops or recursive logic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 使用来自数学图论的专业术语。在图论中，一个 *节点* 是一个进程，一个 *边* 是节点之间的流动。使用这个术语，一个 *有向无环图* (DAG)
    是一个包含单向流程且没有循环或递归逻辑的最高级进程。
- en: '[Figure 10-2](#directed_acyclic_graph_ch10) shows how nodes and edges relate
    to each other in a DAG.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](#directed_acyclic_graph_ch10) 显示了 DAG 中节点和边之间的关系。'
- en: '![Directed acyclic graph](assets/haad_1002.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![有向无环图](assets/haad_1002.png)'
- en: Figure 10-2\. Directed acyclic graph
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 有向无环图
- en: You will create one Python file for each DAG. Each of the steps in a DAG is
    called a *task*, the basic unit of execution in Airflow. Each task will be displayed
    as a single box on the graph diagram of a DAG.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你将为每个 DAG 创建一个 Python 文件。DAG 中的每个步骤都称为 *任务*，这是 Airflow 中执行的基本单元。每个任务将在 DAG 的图形图中显示为一个单独的框。
- en: An *operator* is a predefined template for a task. In this chapter, you will
    use an `Http​Op⁠erator` to call your API and a `PythonOperator` to update your
    analytics database. Airflow has built-in operators to interact with databases,
    S3 buckets, and several other functions. Dozens more are available from the community
    and are listed in the [Airflow Operators and Hooks Reference](https://oreil.ly/8k6mr).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*operator* 是任务的一个预定义模板。在本章中，你将使用 `HttpOperator` 来调用你的 API 和 `PythonOperator`
    来更新你的分析数据库。Airflow 内置了与数据库、S3 存储桶和几个其他功能交互的运算符。社区中还有更多，列在 [Airflow 运算符和钩子参考](https://oreil.ly/8k6mr)
    中。'
- en: The last thing you will learn to use is an *XCom*, which stands for *cross-communications*.
    XComs are used to pass information and data between tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你将要学习的最后一件事情是使用 *XCom*，它代表 *跨通信*。XCom 用于在任务之间传递信息和数据。
- en: Installing Apache Airflow in GitHub Codespaces
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GitHub Codespaces 中安装 Apache Airflow
- en: '[Figure 10-3](#ch10_architecture_airflow) shows the high-level architecture
    of the project you will create in this chapter.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](#ch10_architecture_airflow) 展示了本章将创建的项目的高级架构。'
- en: '![Architecture of Airflow project](assets/haad_1003.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Airflow 项目的架构](assets/haad_1003.png)'
- en: Figure 10-3\. Architecture of the Airflow project
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. Airflow 项目的架构
- en: You will be working with the Part II GitHub Codespace that you created in [“Getting
    Started with Your GitHub Codespace”](ch08.html#ch08_getting_started). If you haven’t
    created your Part II Codespace yet, you can complete that section now.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用在 [“开始使用您的 GitHub Codespace”](ch08.html#ch08_getting_started) 中创建的 Part
    II GitHub Codespace。如果你还没有创建你的 Part II Codespace，你现在可以完成该部分。
- en: Before launching the Codespace, change the machine type to a four-core machine
    by clicking the ellipsis next to the Codespace and then clicking “Change machine
    type.” This is necessary because Airflow runs multiple services at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动 Codespace 之前，通过点击 Codespace 旁边的省略号并点击“更改机器类型”将机器类型更改为四核机器。这是必要的，因为 Airflow
    同时运行多个服务。
- en: You will be installing Airflow in the Codespace and performing that basic configuration
    that allows you to create the data pipeline from the diagram. (This will be a
    non-production setup for demonstration purposes. Before using Airflow in production,
    additional setup would be required.)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在 Codespace 中安装 Airflow 并执行基本配置，这将允许你从图中创建数据管道。（这将是一个用于演示的非生产环境设置。在生产环境中使用
    Airflow 之前，需要额外的设置。）
- en: Airflow can be installed using Docker or `pip`. You will be using the Docker
    version. You will follow the instructions from [“Running Airflow in Docker”](https://oreil.ly/ORZKy),
    with a few customizations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 可以使用 Docker 或 `pip` 安装。你将使用 Docker 版本。你将遵循 [“在 Docker 中运行 Airflow”](https://oreil.ly/ORZKy)
    中的说明，并进行一些自定义。
- en: 'To begin, create an *airflow* directory in the *chapter10* folder of your Codespace
    and change to that directory:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在 Codespace 的 *chapter10* 文件夹中创建一个 *airflow* 目录，并切换到该目录：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, use the `curl` command to retrieve a copy of the *docker-compose.yaml*
    file that is used to run the Docker version of Airflow. Get this from the [official
    Airflow website](https://airflow.apache.org), and specify the version. This chapter
    demonstrates with version 2.9.3, but you can follow the latest stable version
    listed in the [Airflow documentation](https://oreil.ly/QTlk_):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 `curl` 命令检索用于运行 Airflow Docker 版本的 *docker-compose.yaml* 文件。从 [官方 Airflow
    网站](https://airflow.apache.org) 获取此文件，并指定版本。本章使用的是版本 2.9.3，但你也可以遵循 [Airflow 文档](https://oreil.ly/QTlk_)
    中列出的最新稳定版本：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The file *docker-compose.yaml* contains instructions for the images to download
    from [Docker Hub](https://oreil.ly/q7y53) along with environment options for configuring
    the software in your environment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 *docker-compose.yaml* 包含从 [Docker Hub](https://oreil.ly/q7y53) 下载镜像的说明，以及配置软件的环境选项。
- en: 'Open *docker_compose.yaml* and take a look at the `volumes:` section:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 *docker_compose.yaml* 并查看 `volumes:` 部分：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This section creates Docker *volumes*, which are virtual drives available inside
    the Docker containers that are mapped to files in your Codespace storage. They
    are relative to the Airflow project directory, which will be *airflow* in your
    Codespace. For example, *airflow/dags* in your Codespace will be referenced as
    */opt/airflow/dags* to the Airflow application running in Docker. (This will be
    important when you create connections later in this chapter.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节创建 Docker *卷*，这些卷是 Docker 容器内部可用的虚拟驱动器，它们映射到你的 Codespace 存储中的文件。它们相对于 Airflow
    项目目录，在你的 Codespace 中将是 *airflow*。例如，你的 Codespace 中的 *airflow/dags* 将被引用为 */opt/airflow/dags*，指向在
    Docker 中运行的 Airflow 应用程序。（这在你在本章后面创建连接时将非常重要。）
- en: 'Create the directories that are mapped to those volumes and then configure
    an environment variable for the Airflow user ID:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 创建映射到这些卷的目录，然后为 Airflow 用户 ID 配置一个环境变量：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create *docker-compose.override.yaml*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 *docker-compose.override.yaml*：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You will use this file to override some of the standard configuration settings
    from the *docker-compose.yaml* file you downloaded. Using an override file allows
    you to keep the *docker-compose.yaml* file exactly like you downloaded it and
    put all of your customizations together, which makes troubleshooting easier. It
    also allows you to update *docker-compose.yaml* with a new version when Airflow
    is upgraded. Update *docker-compose.override.yaml* with the following contents:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用此文件来覆盖从下载的 *docker-compose.yaml* 文件中的一些标准配置设置。使用覆盖文件允许你保持下载的 *docker-compose.yaml*
    文件与原始文件完全一致，并将所有自定义设置集中在一起，这使得故障排除更容易。它还允许你在 Airflow 升级时使用新版本更新 *docker-compose.yaml*。使用以下内容更新
    *docker-compose.override.yaml*：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO1-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO1-1)'
- en: This setting will hide the built-in Airflow examples so that they are not distracting
    in this chapter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置将隐藏内置的 Airflow 示例，以便在本章中不会分散注意力。
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO1-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO1-2)'
- en: This setting will allow you to use the Airflow web interface in Codespaces.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置将允许你在 Codespaces 中使用 Airflow Web 界面。
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO1-3)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO1-3)'
- en: This setting tells Airflow to look for changes to your code more frequently
    while you are developing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置告诉 Airflow 在你开发时更频繁地查找代码更改。
- en: 'Now you are ready to initialize the Docker environment using *docker-compose.yaml*
    and *docker-compose.override.yaml* with the `docker compose up airflow-init` command.
    This command will download the Airflow software and provision user IDs and other
    configuration details. Execute the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用 `docker-compose.yaml` 和 `docker-compose.override.yaml` 文件以及 `docker
    compose up airflow-init` 命令初始化 Docker 环境。此命令将下载 Airflow 软件，并配置用户 ID 和其他配置细节。执行以下命令：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command will run for several minutes, with many commands executed. If the
    output ends with “exited with code 0” it was successful. Your environment has
    been initialized, and you don’t need to execute this command again.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将运行几分钟，执行许多命令。如果输出以“exited with code 0”结束，则表示成功。你的环境已初始化，你不需要再次执行此命令。
- en: 'You are ready to run Airflow. To launch the Airflow web interface, execute
    the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以运行 Airflow。要启动 Airflow Web 界面，执行以下命令：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Although you will see a pop-up window to launch the web UI, I have found that
    sometimes the web UI takes a few minutes to prepare, so don’t click OK. Instead,
    wait a couple of minutes and then select the Ports tab in your Codespace. You
    will see the forwarded address of the web interface. Click the globe icon to open
    the UI in the browser, as shown in [Figure 10-4](#open_in_browser_ch10).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你会看到一个弹出窗口来启动 Web UI，但我发现有时 Web UI 需要几分钟的准备时间，所以不要点击“确定”。相反，等待几分钟，然后选择你的 Codespace
    中的“端口”选项卡。你将看到 Web 界面的转发地址。点击地球图标在浏览器中打开 UI，如图 [图 10-4](#open_in_browser_ch10)
    所示。
- en: '![Open Airflow web interface](assets/haad_1004.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![打开 Airflow Web 界面](assets/haad_1004.png)'
- en: Figure 10-4\. Open Airflow web interface
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 打开 Airflow Web 界面
- en: You will see the login page. Enter a username of **`airflow`** and password
    of **`airflow`** and click “Sign in.” (These starter credentials are used for
    demonstration only.) You will see the web interface of the Airflow application
    running in your Codespace, as shown in [Figure 10-5](#airflow_home_page_ch10).
    When you begin, there are no DAGs listed. You will learn more about the capabilities
    of Airflow as you create DAGs to complete your data pipeline requirements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到登录页面。输入用户名**`airflow`**和密码**`airflow`**，然后点击“登录”。（这些初始凭据仅用于演示。）您将看到在您的 Codespace
    中运行的 Airflow 应用程序的 Web 界面，如图[图 10-5](#airflow_home_page_ch10)所示。当您开始时，列表中没有 DAG。您将在创建
    DAG 以完成您的数据管道需求时了解更多关于 Airflow 的功能。
- en: '![Airflow home page](assets/haad_1005.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![Airflow 主页](assets/haad_1005.png)'
- en: Figure 10-5\. Airflow home page
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. Airflow 主页
- en: Creating Your Local Analytics Database
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您的本地分析数据库
- en: 'Your data pipeline will be used to insert and update player records into a
    local database. This is a common data science pattern: updating a database from
    source data and then creating models, metrics, and reports from the database.
    Change the directory to *dags* and create a database and the `player` table as
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据管道将用于将球员记录插入和更新到本地数据库中。这是一个常见的数据科学模式：从源数据更新数据库，然后从数据库创建模型、指标和报告。将目录更改为*dags*并创建数据库和`player`表，如下所示：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Launching Your API in Codespaces
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Codespaces 中启动您的 API
- en: Your Airflow pipeline needs a running copy of the SportsWorldCentral API to
    gather updates. Follow the directions in [“Running the SportsWorldCentral (SWC)
    API Locally”](ch08.html#sportsworldcentral) to get your API running in a separate
    terminal window of Codespaces, and copy the base URL from the browser bar. You
    will configure Airflow to reference the base URL from that API in the next section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 Airflow 管道需要一个 SportsWorldCentral API 的运行副本来收集更新。按照[“在本地运行 SportsWorldCentral
    (SWC) API”](ch08.html#sportsworldcentral)中的说明，在 Codespaces 的另一个终端窗口中运行您的 API，并从浏览器栏中复制基本
    URL。您将在下一节中将配置 Airflow 以引用该 API 的基本 URL。
- en: Configuring Airflow Connections
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Airflow 连接
- en: Airflow *connections* allow you to store information about data sources and
    targets in the server instead of in your code. This is useful for maintaining
    separate Airflow environments for development, testing, and production. You will
    create connections for your API and your analytics database.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow *连接*允许您在服务器中而不是在代码中存储有关数据源和目标的信息。这对于维护用于开发、测试和生产的不同 Airflow 环境非常有用。您将为您的
    API 和分析数据库创建连接。
- en: 'In the Airflow UI, select Admin > Connections. Click the plus sign to add a
    new connection record. Now you will use the *volume* mappings that you viewed
    earlier in the *docker-compose.yaml* file. Use the following values:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Airflow UI 中，选择 Admin > Connections。点击加号以添加新的连接记录。现在您将使用之前在*docker-compose.yaml*文件中查看的*volume*映射。使用以下值：
- en: '*Connection ID*: **`analytics_database`**'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接 ID*：**`analytics_database`**'
- en: '*Connection Type*: Sqlite'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接类型*：Sqlite'
- en: '*Description*: **`Database to store local analytics data.`**'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*描述*：**`用于存储本地分析数据的数据库。`**'
- en: '*Schema*: **`/opt/airflow/dags/analytics_database.db`**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式*：**`/opt/airflow/dags/analytics_database.db`**'
- en: Leave the rest of the values empty, and click Save.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其余值留空，然后点击保存。
- en: 'Next, add the connection for the API connection:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，添加 API 连接的连接：
- en: '*Connection ID*: **`sportsworldcentral_url`**'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接 ID*：**`sportsworldcentral_url`**'
- en: '*Connection Type*: HTTP'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接类型*：HTTP'
- en: '*Description*: **`URL for calling the SportsWorldCentral API.`**'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*描述*：**`调用 SportsWorldCentral API 的 URL。`**'
- en: '*Host*: Enter the base URL of the API running in Codespaces.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主机*：输入运行在 Codespaces 中的 API 的基本 URL。'
- en: Leave the rest of the values empty, and click Save. You should see two connections
    listed, as shown in [Figure 10-6](#configured_connections_ch10).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其余值留空，然后点击保存。您应该看到列出两个连接，如图[图 10-6](#configured_connections_ch10)所示。
- en: '![Configured Airflow connections](assets/haad_1006.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![配置的 Airflow 连接](assets/haad_1006.png)'
- en: Figure 10-6\. Configured Airflow connections
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 配置的 Airflow 连接
- en: Creating Your First DAG
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您的第一个 DAG
- en: '[Figure 10-7](#date_pipeline_airflow_ch10) displays an implementation of your
    pipeline with Airflow, using two DAGs. The *bulk_player_file_load.py* DAG would
    perform an initial load of the analytics database from a bulk file, which was
    provided in [Part I](part01.html#part_1) of this book. That file is available
    in the *chapter10/complete* folder of your repository, but this chapter does not
    walk through it due to space constraints.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10-7](#date_pipeline_airflow_ch10)显示了使用Airflow实现您的管道的示例，使用两个DAG。*bulk_player_file_load.py*
    DAG将执行从提供的批量文件到分析数据库的初始加载，该文件在本书的[第一部分](part01.html#part_1)中提供。该文件位于您的存储库的*chapter10/complete*文件夹中，但由于篇幅限制，本章没有介绍它。'
- en: '![Airflow components of your data pipeline](assets/haad_1007.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Airflow components of your data pipeline](assets/haad_1007.png)'
- en: Figure 10-7\. Airflow components of your data pipeline
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. 您的数据管道的Airflow组件
- en: 'Create the DAG that uses API data, *recurring_player_api_insert_update_dag.py*.
    This DAG performs incremental updates of your database, using the SportsWorldCentral
    API. Change the directory to *dags* and create the *recurring_player_api_insert_update_dag.py*
    file:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建使用API数据的DAG，*recurring_player_api_insert_update_dag.py*。此DAG使用SportsWorldCentral
    API执行数据库的增量更新。将目录更改为*dags*并创建*recurring_player_api_insert_update_dag.py*文件：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add the following contents to the *recurring_player_api_insert_update_dag.py*
    file:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到*recurring_player_api_insert_update_dag.py*文件中：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO2-1)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_using_apis_in_data_pipelines_CO2-1)'
- en: This import allows you to define the DAG using a `@dag` decorator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此导入允许您使用`@dag`装饰器定义DAG。
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO2-2)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_using_apis_in_data_pipelines_CO2-2)'
- en: These two imports allow you to use predefined operators in your tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个导入允许您在任务中使用预定义的操作符。
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO2-3)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_using_apis_in_data_pipelines_CO2-3)'
- en: This is an import of a separate Python file with a function that is shared between
    two DAGs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个导入包含在两个DAG之间共享的函数的单独Python文件。
- en: '[![4](assets/4.png)](#co_using_apis_in_data_pipelines_CO2-4)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_using_apis_in_data_pipelines_CO2-4)'
- en: This is the code to verify the response of `api_health_check_task` defined below.
    This is the first task, and it allows the DAG to verify the status of the API
    before proceeding with other tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是验证以下定义的`api_health_check_task`响应的代码。这是第一个任务，它允许DAG在执行其他任务之前验证API的状态。
- en: '[![5](assets/5.png)](#co_using_apis_in_data_pipelines_CO2-5)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_using_apis_in_data_pipelines_CO2-5)'
- en: This defines a function that will be called by a task.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了一个将被任务调用的函数。
- en: '[![6](assets/6.png)](#co_using_apis_in_data_pipelines_CO2-6)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)(#co_using_apis_in_data_pipelines_CO2-6)'
- en: This line of code uses XCom to retrieve data from the second task.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码使用XCom从第二个任务检索数据。
- en: '[![7](assets/7.png)](#co_using_apis_in_data_pipelines_CO2-7)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![7](assets/7.png)(#co_using_apis_in_data_pipelines_CO2-7)'
- en: Here it passes the data from XCom to the shared `upsert_player_data` function,
    which is defined in a separate file.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里它将数据从XCom传递到共享的`upsert_player_data`函数，该函数定义在单独的文件中。
- en: '[![8](assets/8.png)](#co_using_apis_in_data_pipelines_CO2-8)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![8](assets/8.png)(#co_using_apis_in_data_pipelines_CO2-8)'
- en: This is the main DAG definition. It uses the `@dag` decorator to define the
    Python function as a DAG. The tasks are defined within this method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是主DAG定义。它使用`@dag`装饰器将Python函数定义为DAG。任务定义在此方法内。
- en: '[![9](assets/9.png)](#co_using_apis_in_data_pipelines_CO2-9)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![9](assets/9.png)(#co_using_apis_in_data_pipelines_CO2-9)'
- en: The first task uses an `HttpOperator` template to call the API’s health check
    endpoint. It adds a `response_check` method to check the API’s status before continuing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务使用`HttpOperator`模板调用API的健康检查端点。它在继续之前添加了一个`response_check`方法来检查API的状态。
- en: '[![10](assets/10.png)](#co_using_apis_in_data_pipelines_CO2-10)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![10](assets/10.png)(#co_using_apis_in_data_pipelines_CO2-10)'
- en: The minimum last changed date is hardcoded in this example. In production, an
    [Airflow template variable](https://oreil.ly/pFHaG) could be used to get the last
    day’s updates.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，最小最后更改日期是硬编码的。在生产中，可以使用[Airflow模板变量](https://oreil.ly/pFHaG)来获取最后一天的更新。
- en: '[![11](assets/11.png)](#co_using_apis_in_data_pipelines_CO2-11)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![11](assets/11.png)(#co_using_apis_in_data_pipelines_CO2-11)'
- en: The second task uses an `HttpOperator` to call the API’s player endpoint with
    a query parameter to restrict the records that are returned.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个任务使用`HttpOperator`调用API的玩家端点，并使用查询参数来限制返回的记录。
- en: '[![12](assets/12.png)](#co_using_apis_in_data_pipelines_CO2-12)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![12](assets/12.png)(#co_using_apis_in_data_pipelines_CO2-12)'
- en: The third task is a `PythonOperator` that calls the `insert_update_player_data`
    function.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个任务是一个 `PythonOperator`，它调用 `insert_update_player_data` 函数。
- en: '[![13](assets/13.png)](#co_using_apis_in_data_pipelines_CO2-13)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![13](assets/13.png)(#co_using_apis_in_data_pipelines_CO2-13)'
- en: This statement sets the dependency of the tasks using bitshift operators.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句使用位移运算符设置任务的依赖关系。
- en: '[![14](assets/14.png)](#co_using_apis_in_data_pipelines_CO2-14)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![14](assets/14.png)(#co_using_apis_in_data_pipelines_CO2-14)'
- en: The final statement is required to instantiate the DAG that is defined by the
    `@dag` decorator.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条语句是必需的，用于实例化由 `@dag` 装饰器定义的 DAG。
- en: 'Take a moment to compare this code to [Figure 10-7](#date_pipeline_airflow_ch10).
    The key parts of the DAG file are toward the bottom of this file: the `@dag` decorator
    defines the main DAG wrapper. Inside the DAG are three tasks: two that use `HttpOperator`s
    to connect to the API and one that uses a `PythonOperator` to connect to the SQLite
    database.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 抽空比较一下这段代码与 [图 10-7](#date_pipeline_airflow_ch10)。DAG 文件的关键部分位于文件底部：`@dag` 装饰器定义了主要的
    DAG 包装器。在 DAG 内部有三个任务：两个使用 `HttpOperator` 连接到 API，一个使用 `PythonOperator` 连接到 SQLite
    数据库。
- en: The statement `api_health_check_task >> api_player_query_task >> player_sqlite_upsert_task`
    sets the dependency between the tasks using a right-shift operator, `>>`. These
    tasks have a very simple sequential dependency, but Airflow is capable of implementing
    very intricate dependencies between tasks. For more information about this capability,
    read Astronomer’s [“Manage task and task group dependencies in Airflow”](https://oreil.ly/PTa4M).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 语句 `api_health_check_task >> api_player_query_task >> player_sqlite_upsert_task`
    使用右移运算符 `>>` 设置任务之间的依赖关系。这些任务具有非常简单的顺序依赖关系，但 Airflow 能够实现任务之间非常复杂的依赖关系。有关此功能的更多信息，请参阅
    Astronomer 的 [“在 Airflow 中管理任务和任务组依赖”](https://oreil.ly/PTa4M)。
- en: Coding a Shared Function
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写共享函数
- en: 'Although the sources of the two DAGs are different, they both perform an *upsert*
    on the analytics database, which means that if a source record already exists
    in the database, the code updates it, otherwise it inserts a new record. Because
    this task is shared between the two DAGs, you will create a separate Python file
    with a shared function. Create the *shared_functions.py* file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两个 DAG 的来源不同，但它们都在分析数据库上执行 *upsert* 操作，这意味着如果源记录已存在于数据库中，则代码会更新它，否则会插入一条新记录。因为此任务在两个
    DAG 之间是共享的，所以你需要创建一个包含共享函数的单独 Python 文件。创建 *shared_functions.py* 文件：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Add the following contents to the *shared_functions.py* file:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到 *shared_functions.py* 文件中：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO3-1)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_using_apis_in_data_pipelines_CO3-1)'
- en: These two import statements are placed inside the Python method. This is because
    Airflow frequently parses DAG code and will reload imported libraries that are
    at the top of the Python file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个导入语句被放置在 Python 方法内部。这是因为 Airflow 经常解析 DAG 代码，并将 Python 文件顶部的导入库重新加载。
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO3-2)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_using_apis_in_data_pipelines_CO3-2)'
- en: This statement uses an Airflow hook to retrieve the connection that you defined
    in the Airflow user interface.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句使用 Airflow 钩子检索你在 Airflow 用户界面中定义的连接。
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO3-3)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_using_apis_in_data_pipelines_CO3-3)'
- en: This uses a database cursor to execute SQL queries on your analytics database.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用数据库游标在你的分析数据库上执行 SQL 查询。
- en: '[![4](assets/4.png)](#co_using_apis_in_data_pipelines_CO3-4)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_using_apis_in_data_pipelines_CO3-4)'
- en: This statement uses the database cursor to execute a parameterized SQL query.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句使用数据库游标来执行参数化 SQL 查询。
- en: '[![5](assets/5.png)](#co_using_apis_in_data_pipelines_CO3-5)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_using_apis_in_data_pipelines_CO3-5)'
- en: This SQL statement provides the upsert capability, which updates a record if
    it already exists or inserts it if not.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 SQL 语句提供了 upsert 功能，如果记录已存在则更新它，如果不存在则插入它。
- en: This function receives the data from the API as a parameter and then loads data
    into the SQLite database using the Airflow connection that you defined in the
    user interface. This is a parameterized SQL query, in which the input data is
    referenced with `VALUES (?, ?, ?, ?, ?, ?)`. This is an important measure to protect
    against SQL injection attacks, which could occur if a malicious actor inserted
    code into the source data’s fields, where your process was expecting data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接收 API 的数据作为参数，然后使用你在用户界面中定义的 Airflow 连接将数据加载到 SQLite 数据库中。这是一个参数化 SQL 查询，其中输入数据通过
    `VALUES (?, ?, ?, ?, ?, ?)` 引用。这是一项重要的措施，可以防止 SQL 注入攻击，如果恶意行为者将代码插入到源数据字段中，而你的进程期望数据，则可能会发生此类攻击。
- en: Running Your DAG
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行你的 DAG
- en: Before you run the DAG, check that your API is up and running. Navigate back
    to the Airflow UI and you will see your DAG listed, as shown in [Figure 10-8](#DAG_listing_main_page_ch10).
    The user interface has too many features to cover in this chapter, but you can
    read about the user interface at [“UI / Screenshots”](https://oreil.ly/DfOSC).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 DAG 之前，请确保你的 API 正在运行。导航回 Airflow UI，你将看到你的 DAG 列出，如图 [图 10-8](#DAG_listing_main_page_ch10)
    所示。用户界面功能众多，本章无法全部涵盖，但你可以在 [“UI / Screenshots”](https://oreil.ly/DfOSC) 中了解用户界面。
- en: '![DAG listed on Airflow home page](assets/haad_1008.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![DAG 列在 Airflow 主页上](assets/haad_1008.png)'
- en: Figure 10-8\. DAG listed on the Airflow home page
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. DAG 列在 Airflow 主页上
- en: Click `recurring_player_api_insert_update_dag`, and then Graph. You will see
    the sequence of Airflow tasks using the `task_id` names that you assigned in your
    code, as shown in [Figure 10-9](#graph_view_ch10).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 `recurring_player_api_insert_update_dag`，然后点击图形。你将看到使用你在代码中指定的 `task_id`
    名称的 Airflow 任务序列，如图 [图 10-9](#graph_view_ch10) 所示。
- en: '![Graph view of first DAG](assets/haad_1009.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![第一个 DAG 的图形视图](assets/haad_1009.png)'
- en: Figure 10-9\. Graph view of the first DAG
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 第一个 DAG 的图形视图
- en: Click the Trigger DAG button, which has a triangle icon to your DAG. If everything
    is configured correctly with your code and connections, each of the tasks in your
    DAG should complete with a green box in a minute or so. Click the first box, labeled
    `check_api_health_check_endpoint`. Your view should look similar to [Figure 10-10](#successful_dag_run_ch10).
    If you encounter an error, click the task that has the error, and click Logs to
    diagnose the issue.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 点击触发 DAG 按钮，该按钮有一个指向你的 DAG 的三角形图标。如果代码和连接配置正确，你的 DAG 中的每个任务应在约一分钟内完成，绿色框表示成功。点击第一个框，标记为
    `check_api_health_check_endpoint`。你的视图应类似于 [图 10-10](#successful_dag_run_ch10)。如果你遇到错误，点击有错误的任务，然后点击日志来诊断问题。
- en: '![Successful DAG run](assets/haad_1010.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![成功的 DAG 运行](assets/haad_1010.png)'
- en: Figure 10-10\. Successful DAG run
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 成功的 DAG 运行
- en: 'To confirm that your analytics database was successfully upserted, go back
    to the terminal and open the database with SQLite. Query the Player table, to
    confirm that 1,018 player records are present in the table. These are the records
    retrieved from your API:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认你的分析数据库已成功更新，请回到终端并使用 SQLite 打开数据库。查询 Player 表，以确认表中存在 1,018 条玩家记录。这些是从你的
    API 中检索到的记录：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Congratulations! You created a data pipeline that updates your database with
    records from an API!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你创建了一个使用 API 记录更新数据库的数据管道！
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to create a data pipeline calling APIs to maintain
    current data for analytics products. You installed and configured Apache Airflow,
    and you created a DAG with multiple tasks to update your database from an API.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何创建一个数据管道，通过调用 API 来维护分析产品的当前数据。你安装并配置了 Apache Airflow，并创建了一个包含多个任务的
    DAG，用于从 API 更新你的数据库。
- en: In [Chapter 11](ch11.html#chapter_11), you will create a Streamlit data app
    using data from an API.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 11 章](ch11.html#chapter_11) 中，你将使用 API 的数据创建一个 Streamlit 数据应用。
