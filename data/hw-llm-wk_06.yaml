- en: 7 Misconceptions, limits, and eminent abilities of LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的7个误解、限制和显著能力
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How LLMs and humans differ in learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs和人类在学习上的差异
- en: Making LLMs better at latency and scale-sensitive applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使LLMs在延迟和规模敏感的应用中表现更好
- en: Producing intermediate outputs for better final results
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成中间输出以获得更好的最终结果
- en: How computational complexity limits what an LLM can do
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算复杂性如何限制LLMs能做什么
- en: Thanks to ChatGPT, the world has become more broadly aware of LLMs and their
    capabilities. Despite this awareness, many misconceptions and misunderstandings
    about LLMs still exist. Many people believe that LLMs are continually learning
    and self-improving, are more intelligent than people, and will soon be able to
    solve every problem on earth. While these statements are hyperbolic, some earnestly
    fear that LLMs will seriously disrupt the world.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了ChatGPT，世界对LLMs及其能力有了更广泛的了解。尽管如此，关于LLMs的许多误解和误解仍然存在。许多人认为LLMs持续学习和自我改进，比人类更聪明，并且很快就能解决地球上的一切问题。虽然这些说法是夸张的，但有些人真诚地担心LLMs将严重扰乱世界。
- en: We are not here to say there are no legitimate concerns about LLMs, and we will
    discuss these in more depth in the book’s last two chapters. Still, many thoughts
    and worries about LLMs that you may encounter are blown out of proportion compared
    to how LLMs and technology broadly evolve.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是在这里说关于LLMs没有合法的担忧，我们将在本书的最后两章中更深入地讨论这些问题。然而，你可能会遇到的许多关于LLMs的想法和担忧，与LLMs和技术的广泛发展相比，都是被夸大了的。
- en: This chapter will discuss a few critical aspects of how LLMs work and how these
    aspects relate to these misconceptions. Ultimately, these operational aspects
    of LLMs affect how you may want to use or avoid an LLM in practice.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论LLMs工作的一些关键方面以及这些方面如何与这些误解相关。最终，LLMs的这些操作方面会影响你在实践中如何使用或避免使用LLMs。
- en: First, we will discuss the differences between how humans and LLMs learn. Humans
    are fast learners, but LLMs are static by default. Although LLMs can be incredibly
    effective at processing data, people are better equipped to be maximally productive
    when learning new things.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论人类和LLMs学习方式的差异。人类是快速学习者，但LLMs默认是静态的。尽管LLMs在处理数据方面可能非常有效，但人们在学习新事物时更有可能达到最大生产力。
- en: Next, we will tackle why the word *thinking* is misleading when considering
    how an LLM works. We will highlight that it is better to think of an LLM’s operation
    as *computing* because LLMs have no distinction between formulating and emitting
    output. In contrast, people often “think before they speak.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨在考虑LLMs的工作方式时，“思考”一词是如何具有误导性的。我们将强调，将LLMs的操作视为“计算”更为合适，因为LLMs在制定和输出输出之间没有区别。相比之下，人们通常“三思而后行”。
- en: Finally, we will discuss the scope of what LLMs can compute and how computer
    science concepts help us understand some of the intrinsic limitations behind an
    LLM’s current and future capabilities. These three topics are interrelated, so
    you will see how they connect as we discuss each in more detail.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论LLMs可以计算的范围以及计算机科学概念如何帮助我们理解LLMs当前和未来能力背后的某些内在限制。这三个主题是相互关联的，所以你将看到在我们详细讨论每个主题时它们是如何相互联系的。
- en: 7.1 Human rate of learning vs. LLMs
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 人类学习速度与大型语言模型（LLMs）的比较
- en: While we have discussed it implicitly, it is helpful to be explicit about how
    an LLM’s training differs from a person’s learning. The fluid and often lucid
    text produced by generative AI and the analogies we use to relate the capabilities
    of LLMs to human capabilities may make it seem as if there were some relationship
    between the two. Many people online are touting the idea that such a connection
    between what an LLM can do and what a human can do is real. In reality, the two
    are very different and have important considerations for when, how, and why you
    might prefer a person over an AI and how humans and AI can work together.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然我们隐晦地讨论了它，但明确地说明LLMs的训练与人类的学习不同是有帮助的。由生成式AI产生的流畅且通常清晰的文本以及我们将LLMs的能力与人类能力联系起来的类比，可能会让人感觉两者之间似乎存在某种联系。许多在线人士都在宣扬这种联系——LLMs能做什么和人类能做什么之间的联系是真实的。实际上，两者非常不同，并且对于何时、如何以及为什么你可能更愿意选择人类而不是AI，以及人类和AI如何合作，都有重要的考虑。 '
- en: 'From the material we have covered so far, we know that LLMs learn by predicting
    the next word using hundreds of millions of documents as examples. In chapter
    [4](../Text/chapter-4.html), we presented the algorithmic process of “learning”
    in LLMs: the gradient descent algorithm, which alters the parameters of an LLM’s
    neural network by attempting to predict the next token in a sample input. Then,
    in chapter [5](../Text/chapter-5.html), we showed how fine-tuning algorithms,
    like RLHF, alter the parameters of the LLM again. These two components of learning
    in an LLM have minimal resemblance to human learning and impose some crucial limitations
    on what we can expect the LLM to do. One of the most critical aspects is the rate
    and efficacy of this learning approach as it relates to the volume of data provided
    to the training process.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们迄今为止所涵盖的内容来看，我们知道LLM通过使用数亿份文档作为示例来预测下一个单词来学习。在第[4](../Text/chapter-4.html)章中，我们介绍了LLM中“学习”的算法过程：梯度下降算法，通过尝试预测样本输入中的下一个标记来改变LLM神经网络的参数。然后，在第[5](../Text/chapter-5.html)章中，我们展示了微调算法，如RLHF，如何再次改变LLM的参数。LLM学习中的这两个组成部分与人类学习有最小的相似之处，并对我们期望LLM能做什么施加了一些关键的限制。其中最关键的一个方面是这种学习方法的速度和效率，它与训练过程中提供的数据量相关。
- en: To explore this further, consider how an LLM learns relative to how people learn.
    Have you ever met anyone who never spoke to anyone else, never had a parent talk
    to them, and yet somehow understood language? Likely not. Indeed, conversation
    is a key part of linguistic acquisition [1]. At least initially, you acquire knowledge
    and language from interaction and communication with others and the environment.
    Consequentially, you can learn effectively with much less information than an
    LLM has in the data that it trains on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨这个问题，考虑一下LLM的学习方式与人类学习方式的差异。你是否遇到过从未与他人交谈过、从未有父母与他们交谈，但 somehow 理解了语言的人？很可能没有。确实，对话是语言习得的关键部分
    [1]。至少最初，你通过与他人和环境的互动和交流来获取知识和语言。因此，你可以用比LLM在训练数据中拥有的信息少得多的信息来有效地学习。
- en: In the best-case scenarios of childhood language acquisition, studies have observed
    that children are exposed to around 15,000 total spoken words a month [2]. If
    we were to be generous and round this figure up to 20,000 words and consider this
    over 100 years, a person would encounter as many as 24 million spoken words throughout
    their entire life. This is clearly a vast overestimate. Couple this with the fact
    that most people can speak their native language fluently, with an implicit understanding
    of vocabulary and linguistic structure, by at least age 18\. Now compare this
    with LLMs. GPT-3, for example, was trained on hundreds of billions of words. Based
    on word counts alone, this is a very inefficient way to learn language!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在儿童语言习得的最佳情况下，研究表明，儿童每月接触到大约15,000个总词汇量 [2]。如果我们慷慨地将这个数字四舍五入到20,000个单词，并且考虑这个数字超过100年，一个人在其一生中会遇到多达2400万个口语词汇。这显然是一个巨大的高估。再考虑到大多数人至少在18岁之前就能流利地使用母语，并且对词汇和语言结构有隐含的理解，现在将这一点与LLM（大型语言模型）进行比较。例如，GPT-3是在数百亿个单词上训练的。仅从单词数量来看，这是一种非常低效的语言学习方法！
- en: Language acquisition also helps us recognize the stark differences in how words
    are acquired. Babies and toddlers start with simple words, such as *mama* and
    *dada*, and eventually learn basic concepts like colors, *no*, *food*, etc. More
    complex words are added over time, building on the prior words. Yet an LLM begins
    with seeing all words simultaneously based on their frequency of use. Indeed,
    it is accurate to imagine an LLM tokenizing this very book as part of its first
    “learning,” acquiring knowledge of all of its eventual vocabulary simultaneously
    instead of starting with simple concepts and building knowledge on top of those
    foundations. While this process contributes to the rate at which an LLM learns,
    it may detract from the LLM’s capabilities of drawing high-level relationships
    between concepts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 语言习得还帮助我们认识到词汇习得方式之间的显著差异。婴儿和幼儿从简单的词汇开始，例如*mama*和*dada*，并最终学习基本概念，如颜色、*no*、*food*等。随着时间的推移，会逐渐增加更复杂的词汇，建立在先前词汇的基础上。然而，LLM（大型语言模型）是从根据使用频率同时看到所有词汇开始的。确实，可以想象LLM将这本书作为其首次“学习”的一部分进行分词，同时获得其最终词汇的全部知识，而不是从简单概念开始，并在这些基础上构建知识。虽然这个过程有助于提高LLM的学习速度，但它可能会降低LLM在概念之间建立高级关系的能力。
- en: 'An LLM’s key advantage over humans is the *scale* at which it operates and
    its ability to perform multiple tasks simultaneously. This advantage is a common
    theme throughout machine learning and deep learning. You cannot easily hire an
    army of people to comb through books, expense reports, internal documents, or
    whatever medium of information to perform knowledge work like writing a review,
    finding potential fraud, or answering an arcane policy question. However, you
    can quickly get an army of computers to attempt to automate these tasks. While
    an individual LLM can analyze multiple parts of a sentence simultaneously, you
    can employ multiple computers running the same LLM to work in parallel. Training
    the LLM presents a similar opportunity: LLMs are trained on more words than you
    will ever read or hear in your lifetime, and you can train a large LLM by renting
    or buying thousands of computers to do the work concurrently.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类相比，LLM的关键优势在于其操作的**规模**和同时执行多项任务的能力。这一优势是机器学习和深度学习中的一个常见主题。你不可能轻易雇佣一支军队来翻阅书籍、费用报告、内部文件或任何信息媒介来执行像撰写评论、寻找潜在的欺诈或回答复杂政策问题这样的知识工作。然而，你可以快速组建一支计算机军队来尝试自动化这些任务。虽然单个LLM可以同时分析句子的多个部分，但你可以通过运行相同LLM的多个计算机并行工作。训练LLM也提供了类似的机会：LLMs是在你一生中可能阅读或听到的单词数量之上训练的，你可以通过租赁或购买数千台计算机来同时进行大量工作以训练一个大型LLM。
- en: Considering these facts in conjunction with the material we’ve covered in previous
    chapters, we can list several high-level pros and cons of using LLMs for tasks
    compared to humans. A summary of these factors is shown in figure [7.1](#fig__llm_pro_con),
    which describes how the advantages and disadvantages of LLMs will lead to natural
    benefits and drawbacks of their use and, thus, provide insights about where LLMs
    should and should not be used.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 结合我们在前几章中讨论的内容，我们可以列出使用LLMs进行任务与人类相比的几个高级优缺点。这些因素的总览如图[7.1](#fig__llm_pro_con)所示，它描述了LLMs的优势和劣势将如何导致其使用的自然益处和弊端，从而提供关于LLMs应该和不应该在何处使用的见解。
- en: '![figure](../Images/CH07_F01_Boozallen.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Boozallen.png)'
- en: Figure 7.1 A summary of the strengths and weaknesses of LLMs relative to humans
    performing the same task. These lead to natural considerations that you must evaluate
    when using an LLM. From these, we can draw broad recommendations for successful
    LLM use.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 相对于人类执行相同任务，LLMs（大型语言模型）的优缺点总结。这些自然地引发了你使用LLMs时必须评估的考虑因素。从这些因素中，我们可以得出关于成功使用LLMs的广泛建议。
- en: 'Some of the benefits of LLMs are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一些好处如下：
- en: Well-trained LLMs have a broad collection of background information, so they
    perform well on many tasks that are not that different from what has been seen
    before, and little work is needed to make the model effective. While this is not
    necessarily correct or detailed information, the breadth of the topic areas that
    an LLM can receive and generate reasonable responses about is far beyond the areas
    that most individual people can cover.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过良好训练的LLMs拥有广泛的背景信息，因此在许多与之前所见不太不同的任务上表现良好，而且几乎不需要做额外的工作来使模型有效。虽然这并不一定是正确或详细的信息，但LLM可以接收和就合理回应的领域范围远远超过大多数个人可以覆盖的范围。
- en: For many tasks, there is no need to get a precisely correct response. Broad
    requests for general information in a subject area intrinsically allow an LLM
    to be flexible and unconstrained in its response. This is especially true if you
    refine the LLM’s output through other processes. For example, a human might copyedit
    a piece of writing to improve it but use an LLM to produce the first draft or
    provide inspiration to break writer’s block and accelerate creating the work.
    Likewise, an LLM can be used to refine an author’s writing to make it sound more
    natural or engaging through rephrasing or using a larger variety of vocabulary.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于许多任务来说，没有必要得到一个精确无误的回应。在某一学科领域内对一般信息的广泛请求本质上允许LLM在回应时保持灵活性和不受限制。这一点在通过其他过程精炼LLM的输出时尤其正确。例如，一个人可能会复制编辑一篇写作来改进它，但使用LLM来产生初稿或提供灵感以打破写作障碍并加速创作工作。同样，LLM也可以用来精炼作者的写作，通过改写或使用更丰富的词汇量使其听起来更自然或更有吸引力。
- en: LLMs can be trained quickly in comparison to people. You can produce a broadly
    useful LLM in months, given a $1,000,000 to $10,000,000 budget to purchase computational
    resources. Humans take many years to become useful. An LLM that can answer a broad
    set of basic questions can be instantiated for far less effort and cost than it
    takes to find, hire, and retain an employee with specific knowledge, skills, and
    abilities. As long as the problems are in the scope of what the LLM can achieve,
    the incremental cost is minuscule compared to a person’s hourly rate, even without
    the extra overhead.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与人类相比，LLMs 的训练速度要快得多。只要投入 $1,000,000 到 $10,000,000 的预算来购买计算资源，你就可以在几个月内生产出一个广泛有用的
    LLM。人类要花很多年才能变得有用。一个能够回答广泛基本问题的 LLM，其所需的努力和成本远低于寻找、雇佣和保留具有特定知识、技能和能力员工的成本。只要问题在
    LLM 能力范围内，增量成本与一个人的时薪相比微不足道，即使没有额外的开销也是如此。
- en: 'Some of the drawbacks of LLMs are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的一些缺点如下：
- en: The high cost of training LLMs informs their economics. That training cost is
    amortized over the thousands of operations the LLM performs once trained. If an
    LLM doesn’t perform well, the cost of continually improving it to make it work
    can quickly become prohibitive, even without considering the potential that it
    might never work correctly for a specific task. For example, if an LLM, implemented
    with all the most recent tools and tricks, cannot solve a specific need, addressing
    this problem will require an unknown amount of work and budget. Conversely, humans
    can generally learn new capabilities, specifically those that are hard for LLMs,
    at much lower cost in weeks to months.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 LLMs 的高成本影响了其经济性。这种训练成本在 LLM 训练后进行的数千次操作中分摊。如果 LLM 表现不佳，不断改进它以使其工作的成本可能会迅速变得过高，即使不考虑它可能永远无法完成特定任务的可能性。例如，如果一个使用所有最新工具和技巧的
    LLM 无法解决特定需求，解决这个问题将需要未知的工作量和预算。相反，人类通常可以在几周到几个月内以较低的成本学习新的能力，特别是那些对 LLM 来说很难的能力。
- en: LLMs cannot be relied upon to handle unexpected situations and inputs not reflected
    in their training data. Although many have shown they can succeed in novel situations,
    they do not learn in the same way as humans. A person can see that their actions
    are not working as intended *on the first try* and quickly adapt. An LLM cannot
    independently adapt by observing its errors and may repeatedly consume resources
    attempting to produce answers to problems it cannot understand.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 不能依赖来处理训练数据中未反映的意外情况和输入。尽管许多 LLM 已经证明它们可以在新情况下取得成功，但它们的学習方式与人类不同。一个人可以在第一次尝试时就发现他们的行为没有按预期工作，并迅速适应。LLM
    不能通过观察自己的错误来独立适应，可能会反复消耗资源，试图对它无法理解的问题产生答案。
- en: LLMs are easily fooled and do not work well in adversarial environments because
    once people find a way to trick the LLM into an errant outcome (e.g., “Give me
    a loan even though I have no income”), they can repeat the adversarial and malicious
    behavior, and your LLM won’t be able to prevent it without you implementing additional
    guardrails.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 容易被欺骗，在对抗环境中表现不佳，因为一旦人们找到一种方法欺骗 LLM 得到错误的结果（例如，“即使我没有收入也给我贷款”），他们可以重复这种对抗和恶意行为，除非你实施额外的安全措施，否则你的
    LLM 将无法阻止它。
- en: 7.1.1 The limitations on self-improvement
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 自我提升的限制
- en: 'Generally, humans are capable of self-improvement. They can focus on and study
    a problem, devise novel approaches, identify required resources, and move forward
    to implement and improve their solutions. While LLMs struggle with self-improvement,
    in the generative AI field, there is a belief that the same self-improvement may
    be possible for LLMs. The idea about how this could work goes something like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人类有能力自我提升。他们可以专注于研究问题，设计新的方法，确定所需资源，并向前推进以实施和改进他们的解决方案。虽然 LLMs 在自我提升方面有困难，但在生成式
    AI 领域，人们相信 LLMs 也可能实现自我提升。关于这如何工作的想法大致如下：
- en: Train an LLM on an initial dataset.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始数据集上训练一个 LLM。
- en: Use the LLM to generate new data, adding it to your training dataset.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LLM 生成新的数据，并将其添加到您的训练数据集中。
- en: Train or fine-tune the model on the new data. (Repeat until the LLM works as
    expected.)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新数据上训练或微调模型。（重复直到 LLM 如预期工作。）（Repeat until the LLM works as expected.）
- en: While this sounds intuitive and plausible, we believe that it does not work
    for simple reasons. We can use some basic information theory, which measures information
    as a quantifiable resource, to explain why. The basis of this argument is that
    by some measure of information, the original dataset has a fixed amount of information.
    In statistics vernacular, we might describe the original information as the *distribution*
    of available information, and through its training process, the LLM is attempting
    to *approximate* or reproduce that distribution of information by storing and
    encoding it in its model. When you generate new data using an LLM, that sample
    of data is a noisy and incomplete reproduction of the original data distribution
    that the LLM observed in the training process. Fundamentally, it is impossible
    for the LLM’s output to contain any new information not present in the original
    training data. Consequently, the reality of such experiments is that successive
    rounds of generating data and training degrade the quality and performance of
    the model [3]. To make something like this work, you need something that provides
    external or new information at each round.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这听起来直观且合理，但我们认为这并不适用于简单的原因。我们可以使用一些基本的信息理论，将信息视为可量化的资源，来解释原因。这个论点的依据是，通过某些信息度量，原始数据集具有固定量的信息。在统计学术语中，我们可能会将原始信息描述为可用信息的*分布*，并且通过其训练过程，大型语言模型（LLM）试图通过存储和编码到其模型中来*近似*或重现这种信息分布。当你使用LLM生成新数据时，这些数据样本是LLM在训练过程中观察到的原始数据分布的嘈杂和不完整的复制。从根本上讲，LLM的输出不可能包含原始训练数据中不存在的新信息。因此，这类实验的现实情况是，连续生成数据和训练会降低模型的质量和性能[3]。要让类似的事情起作用，你需要在每个回合提供外部或新信息。
- en: These concepts also relate to some people’s fear of AI improving itself until
    it becomes so intelligent that we have no hope of understanding or controlling
    it. Some arguments are that the LLM can use other tools, somehow acquiring outside
    information or more training data, to improve itself. Ultimately, this requires
    a belief that while there are limitations as to how far you can improve most technologies,
    LLMs will be immune to these limits, such as the law of diminishing returns. Figure
    [7.2](#fig__self-training) describes the inherent limits to LLM self-improvement.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念也与一些人害怕AI自我改进，直到它变得如此智能，以至于我们无法理解或控制它有关。一些论点是LLM可以使用其他工具，以某种方式获取外部信息或更多训练数据来改进自己。最终，这需要相信虽然大多数技术的改进都有局限性，但LLM将不受这些限制的影响，例如递减回报定律。图[7.2](#fig__self-training)描述了LLM自我改进的固有限制。
- en: '![figure](../Images/CH07_F02_Boozallen.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F02_Boozallen.png)'
- en: Figure 7.2 Concerns that LLMs will self-improve require the belief that LLMs
    won’t follow the normal sigmoid or S-curve of diminishing returns that describes
    the development of almost all other technologies. For infinite self-improvement
    to happen, we must believe that constraints such as power, data, or computational
    capacity are always solvable and that somehow, humans would not otherwise solve
    them for areas outside of LLMs. Constraints such as these are why we can describe
    most tech-nology development using S-curves, where progress slows as more constraints
    take effect. In other words, we’ll eventually reach a state where we can’t just
    build a bigger computer.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 LLM将自我改进的担忧需要相信LLM不会遵循描述几乎所有其他技术发展的正常sigmoid或S型递减回报曲线。为了无限自我改进的发生，我们必须相信诸如电力、数据或计算能力之类的限制总是可以解决的，并且人类会在LLM之外的其他领域解决这些问题。这类限制是为什么我们可以用S型曲线来描述大多数技术发展，随着更多限制的生效，进步会放缓。换句话说，我们最终会达到一个状态，我们不能再仅仅通过建造更大的计算机来解决问题。
- en: A great example of limitations on technical improvement is Moore’s law, which
    roughly states that the number of transistors on a chip would double every 18
    to 24 months. Moore’s law has mostly accurately predicted the growth of transistors
    on a chip, but there are signs of the S-curve of diminishing returns in transistors.
    The rate of the number of transistors on a chip doubling is decreasing. More importantly,
    the total system performance has already entered this S-curve. The number of transistors
    correlates with total compute performance but does not directly indicate compute
    performance. Looking at the whole picture in figure [7.3](#fig__moores_law), you
    will see that other constraints prevent boundless improvements across the entire
    system. Moore’s law aside, the practical cost of high-performance GPUs and the
    infrastructure that hosts them is another barrier to boundless improvement.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 技术改进限制的一个很好的例子是摩尔定律，它大致表明，芯片上的晶体管数量每18到24个月就会翻倍。摩尔定律在很大程度上准确地预测了芯片上晶体管数量的增长，但晶体管数量减少的S曲线迹象已经出现。芯片上晶体管数量翻倍的速度正在下降。更重要的是，整个系统的性能已经进入了这个S曲线。晶体管数量与总计算性能相关，但并不直接指示计算性能。从图[7.3](#fig__moores_law)的整体情况来看，你会看到其他限制因素阻止了整个系统的无限改进。抛开摩尔定律不谈，高性能GPU及其基础设施的实际成本也是无限改进的另一个障碍。
- en: LLMs are not humans—do not judge them by human standards!
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM不是人类——不要用人类的标准来评判它们！
- en: Many catchy headlines have proclaimed LLM performance on the MCAT exam for medical
    school, the bar exam for lawyers to practice law, and IQ tests to measure their
    intelligence. While these are always interesting and full of caveats such as “How
    many examples of the same kinds of questions are in the LLM’s training data?”
    these are not good ways to extrapolate about LLMs and their abilities relative
    to humans. Indeed, pinning down an exact definition of *intelligence* is complex
    and one of the reasons why multiple types of IQ tests exist [4]. Ultimately, these
    tests have been helpful in predicting people’s outcomes in various tasks. Still,
    these tests are not designed to evaluate AI algorithms, and we have no reason
    to believe they do so accurately or reasonably! The problem is correlation, notcausation.
    IQ tests all *correlate* with desirable outcomes, but they do not measure an underlying
    property that controls or causes outcomes in the same way, for example, that a
    blood sugar test does. In a blood sugar test, if your blood sugar is too low or
    too high, we know what will happen because it measures an importantunderlying
    property that *causes* the outcome of some process that we understand quite well.
    IQ tests are useful, but their usefulness comes from years of iteration and improvement.
    We now better understand which answers on these tests correlate to people’s performance,
    but they don’t measure the underlying causes of this performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多引人注目的标题宣称了大型语言模型（LLM）在医学院入学考试、律师执业资格考试以及智商测试中的表现，这些测试旨在衡量他们的智力。虽然这些内容总是很有趣，但它们充满了诸如“LLM的训练数据中有多少个相同类型的问题的例子？”之类的警告，但这些并不是关于LLM及其相对于人类能力的推断的好方法。事实上，精确地定义*智力*是复杂的，这也是为什么存在多种类型的智商测试的原因之一[4]。最终，这些测试在预测人们在各种任务中的结果方面是有帮助的。然而，这些测试并不是为了评估AI算法，我们没有理由相信它们能够准确或合理地做到这一点！问题是相关性，而不是因果关系。智商测试都与期望的结果相关，但它们并不测量控制或以相同方式导致结果的潜在属性，例如血糖测试那样。在血糖测试中，如果你的血糖过高或过低，我们知道会发生什么，因为它是测量一个重要的潜在属性，这个属性*导致*了我们非常了解的一些过程的后果。智商测试是有用的，但它们的用处来自于多年的迭代和改进。我们现在更好地理解了这些测试上的哪些答案与人们的表现相关，但它们并不测量这种表现的潜在原因。
- en: '![figure](../Images/CH07_F03_Boozallen.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F03_Boozallen.png)'
- en: Figure 7.3 Moores’s law is a common example of boundless growth, but it is misleading.
    Transistors keep doubling, but frequency, power, single-threaded performance,
    and total computing do not. So the total system performance has not continued
    to double approximately every two years. Other similar factors will constrain
    LLM performance and affect capability over time. Used under CC4.0 license from
    [https://github.com/karlrupp/microprocessor-trend-data](https://github.com/karlrupp/microprocessor-trend-data).
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3摩尔定律是无限增长的一个常见例子，但它具有误导性。晶体管数量持续翻倍，但频率、功率、单线程性能和总计算并没有。因此，整个系统的性能并没有继续大约每两年翻倍。其他类似因素将限制LLM的性能，并随着时间的推移影响其能力。本图使用CC4.0许可，来自[https://github.com/karlrupp/microprocessor-trend-data](https://github.com/karlrupp/microprocessor-trend-data)。
- en: There are many examples of outside information being used to improve generative
    AI. Some algorithms created for robotic hands use external information from a
    physics simulator. Apple uses 3D modeling software to generate data that improves
    iris recognition on their phones [5]. In the examples in chapter [6](../Text/chapter-6.html),
    you saw a potential path for improving an LLM using a compiler for code or the
    Lean language to verify mathematics. These examples demonstrate fully automatable
    processes that generate new information that can lead to self-improvement.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多使用外部信息来改进生成式AI的例子。一些为机器人手设计的算法使用来自物理模拟器的外部信息。苹果公司使用3D建模软件生成数据，以改善其手机上的虹膜识别[5]。在[6](../Text/chapter-6.html)章的例子中，你看到了使用代码编译器或Lean语言来验证数学的潜在途径，以改善LLM。这些例子展示了完全可自动化的过程，可以生成新的信息，从而实现自我改进。
- en: Yet, there has never been an example of boundless self-improvement; the gains
    observed from using these external tools eventually reach a plateau and ultimately
    rely on humans to develop the side information by writing better physics simulators
    for the robots, better compilers for code, and better domain-knowledge systems
    like Lean. Improving these tools compounds a major expense of training LLMs, thus
    imposing a second economic limitation on the self-improvement of LLMs beyond what
    is practical.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从未有过无限自我改进的例子；使用这些外部工具观察到的收益最终会达到一个平台期，最终依赖于人类通过为机器人编写更好的物理模拟器、更好的代码编译器和更好的领域知识系统（如Lean）来开发辅助信息。改善这些工具增加了训练LLM的主要成本，从而在实用范围内对LLM的自我改进施加了第二个经济限制。
- en: 7.1.2 Few-shot learning
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 少样本学习
- en: Few-shot learning is also called *in-context learning*. This technique involves
    providing examples of the type of output you want an LLM to produce as a part
    of the prompt you send it. Say you want an LLM to respond to a help-desk question
    with accurate information. You may give the LLM a prompt with a user’s question
    to the help desk, followed by an example of the appropriate kind of response.
    If you give only one example, it’s called *one-shot learning*. Providing two examples
    instead of a single example is known as *two-shot learning*, and so on, hence
    describing this approach as few-shot because the precise number of examples is
    generally not as important as the fact that only a few examples are provided.
    This method of incorporating examples in a prompt is a specific kind of prompt
    engineering, as demonstrated in figure [7.4](#fig__few-shot-learning).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习也称为*上下文学习*。这种技术涉及在发送给LLM的提示中提供你希望LLM产生的输出类型的示例。比如说，你希望LLM能够用准确的信息回应帮助台的问题。你可以给LLM一个用户向帮助台提出的问题的提示，然后提供一个适当的响应示例。如果你只提供一个示例，那么它被称为*单样本学习*。用两个示例而不是一个示例来代替称为*双样本学习*，依此类推，因此将这种方法描述为少样本，因为提供的示例的确切数量通常并不像只提供少量示例这一事实那么重要。在图[7.4](#fig__few-shot-learning)中展示的将示例纳入提示的方法是一种特定的提示工程。
- en: '![figure](../Images/CH07_F04_Boozallen.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F04_Boozallen.png)'
- en: Figure 7.4 Prompts with examples of how you want the LLM to produce output are
    called few-shot prompts because the LLM has not seen any examples of this specific
    behavior in its training data. In your prompt, you can include examples of input
    and output similar to RLHF/supervised fine-tuning (SFT). This prompting style
    encourages the model to produce the desired output by providing examples of what
    the desired output should look like. Because LLMs train on such a large amount
    of unlabeled data, k-shot examples are an effective way to get better results
    with minimal effort.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 以你希望LLM产生输出为例的提示称为少样本提示，因为LLM在其训练数据中尚未看到任何此类特定行为的示例。在你的提示中，你可以包括类似于RLHF/监督微调（SFT）的输入和输出示例。这种提示风格通过提供所需输出的示例来鼓励模型产生所需的输出。由于LLM在如此大量的未标记数据上训练，k样本示例是获得更好结果的最小努力的有效方式。
- en: Including examples in your prompts is useful for improving an LLM’s performance
    at new tasks. You don’t need to use RLHF or SFT to alter the model, and it works
    better than zero-shot prompting, where we ask the LLM to do the task without examples.
    But is it efficient learning?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的提示中包含示例对于提高LLM在新任务上的表现很有用。你不需要使用RLHF或SFT来改变模型，而且它比零样本提示更有效，在零样本提示中，我们要求LLM在没有示例的情况下完成任务。但这是否是有效的学习？
- en: Few-shot prompting is not training because we are not altering the model in
    any way, as we would in the training or fine-tuning process. The “state” or weights
    of the LLM remain the same. However accurately the LLM performs the task on Monday,
    it will be exactly as accurate on Tuesday and Wednesday, no matter how many thousands
    or millions of few-shot prompts it deals with. There is no improvement to the
    model’s abilities unless you manually do something to include better examples
    in the prompt, provide more examples, or otherwise intervene somehow. In this
    sense, no true learning is happening, and nothing about the model changes. We
    just get improved output from the model by changing our prompt.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示不是训练，因为我们没有以训练或微调过程中那样以任何方式改变模型。LLM的“状态”或权重保持不变。无论LLM在周一的任务表现多么准确，它在周二和周三的表现也将完全一样，无论它处理了多少个千或百万个少样本提示。除非你手动做一些事情，比如在提示中包含更好的示例，提供更多示例，或者以其他方式干预，否则模型的能力不会有所提高。从这个意义上说，并没有真正的学习发生，模型也没有任何变化。我们只是通过改变提示来获得模型改进的输出。
- en: Yet, in an abstract sense, the LLM is learning because the prompt changes the
    model’s behavior by providing additional context to describe the problem. The
    behavior exhibited via prompting correlates with behavior achieved through fine-tuning
    on similar examples [6]. What that means, in short, is that few-shot learning
    does not fundamentally reflect anything different from what gradient descent can
    already do.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从抽象意义上讲，LLM（大型语言模型）在学习，因为提示通过提供额外的上下文来描述问题，从而改变了模型的行为。通过提示所表现出的行为与在类似示例上通过微调所实现的行为相关联[6]。简而言之，这意味着少样本学习并没有从根本上反映与梯度下降可以做到的不同之处。
- en: Note If you do not have a lot of data, few-shot prompting is probably the most
    effective way for you as a practitioner or user to get an LLM to work well on
    your data. Because we can think of this prompting as inefficient gradient descent
    or fine-tuning, you should expect diminishing returns as you add examples in a
    few-shot style. For example, if you include many examples of how you’d like an
    LLM to respond in your prompt and still do not get the needed performance, you
    should look at SFT, RLHF, and the other fine-tuning approaches we discussed in
    chapter [5](../Text/chapter-5.html).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你没有很多数据，作为从业者或用户，少样本提示可能是你让LLM在你数据上工作得好的最有效方式。因为我们可以把这种提示看作是低效的梯度下降或微调，所以当你以少样本风格添加示例时，你应该预期到收益递减。例如，如果你在提示中包含了大量你希望LLM如何响应的示例，但仍然没有得到所需的表现，你应该考虑SFT（强化学习与人类反馈）、RLHF（强化学习与人类反馈）以及我们在第[5](../Text/chapter-5.html)章中讨论的其他微调方法。
- en: '7.2 Efficiency of work: A 10-watt human brain vs. a 2000-watt computer'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 工作效率：10瓦特的人脑与2000瓦特的计算机
- en: The human brain takes the equivalent of 10 watts to maintain consciousness,
    allowing you to read this book. A high-end workstation with a GPU for AI/ML work
    could easily use 2,000 watts. A high-end server for running the larger LLMs available
    today gets into the 10,000 to 15,000 watt range. Off the bat, it would seem like
    using an LLM could thus be 1,500![equation image](../Images/eq-chapter-7-51-1.png)
    more power inefficient than having a human do some task. We should be very proud
    of this aspect of our evolutionary success and efficiency, but it is also only
    one aspect of what we might mean by efficiency. We show that many different kinds
    of efficiency might benefit a person versus machines in figure [7.5](#fig__efficency-of-work).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑维持意识所需的能量相当于10瓦特，这使得你能够阅读这本书。一个配备GPU用于AI/ML工作的高端工作站可能轻易消耗2000瓦特。而运行今天可用的更大LLM的高端服务器，其功耗可达到10000至15000瓦特。乍一看，使用LLM似乎比让人类完成任务要低效1500倍![方程式图片](../Images/eq-chapter-7-51-1.png)。我们应该为此方面我们进化成功和效率感到非常自豪，但这只是我们可能所说的效率的一个方面。我们在图[7.5](#fig__efficency-of-work)中展示了许多不同类型的效率可能对人类和机器都有益。
- en: '![figure](../Images/CH07_F05_Boozallen.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F05_Boozallen.png)'
- en: Figure 7.5 The expensive hardware that makes LLMs work leads to several trade-offs.
    For example, the startup cost of using LLMs is often high, and they do not adapt
    independently. This lack of independent adaptation leads to many natural weaknesses
    where a human would outperform an LLM. Some weak-nesses, such as the fact that
    a model doesn’t change without training, can be considered strengths. You don’t
    get repeatable processes that are easy to scale if each new LLM running behaves
    differently and unpredictably.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 使LLM工作的昂贵硬件导致了许多权衡。例如，使用LLM的启动成本通常很高，而且它们不能独立适应。这种缺乏独立适应导致了许多自然弱点，在这些弱点中，人类的表现会优于LLM。一些弱点，如模型未经训练不会改变的事实，可以被视为优点。如果每个新运行的LLM行为不同且不可预测，您将无法获得可重复的过程，这些过程易于扩展。
- en: 7.2.1 Power
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 电力
- en: Power is one of the driving factors in determining the financial cost of creating
    and running an LLM, but the true need is not yet entirely clear. Yes, many providers
    will quote you a price for running an LLM, but we do not know the true costs each
    provider incurs or the margins each provider has established. For example, an
    LLM provider may be running a negative margin or loss-leader strategy, and the
    long-term cost of using an LLM could be higher than it appears based on today’s
    prices. We do know that LLMs generate significant demand for power, to such an
    extent that big tech companies are developing plans to build nuclear power plants
    to support the power needed by future data centers to run all the models they
    anticipate [7]. Based on this, it seems we can expect that new LLMs will be bigger
    and more power-hungry, yet their value will offset the cost of building dedicated
    power plants for their datacenters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 电力是决定创建和运行LLM财务成本的一个驱动因素，但真正的需求尚不明确。是的，许多提供商会为您提供运行LLM的价格，但我们不知道每个提供商实际承担的成本或每个提供商设定的利润率。例如，LLM提供商可能正在运行负利润率或损失领袖策略，而使用LLM的长期成本可能比基于今天的价格看起来要高。我们知道LLM对电力产生了巨大的需求，以至于大型科技公司正在制定计划，建设核电站以支持未来数据中心运行所有预期模型所需的电力[7]。基于这一点，我们可以预期，新的LLM将更大、更耗电，但它们的价值将抵消为数据中心建设专用电厂的成本。
- en: Based on this factor, one needs to be careful when a successful LLM solution
    creates more demand; you may run into power capacity problems when satisfying
    that demand. You also may need to be careful about the elasticity of power costs.
    Not only could LLM providers change cost structures, but if you host an LLM yourself,
    power price fluctuations of ![equation image](../Images/eq-chapter-7-55-1.png)
    do happen in the United States [8]. This may not be a problem if your intended
    customer base is only 20,000 users, but if you plan on building something that
    will serve millions of users or more, the cost of power could be a major operational
    and environmental hazard.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此因素，当成功的LLM解决方案产生更多需求时，需要格外小心；在满足这一需求时，可能会遇到电力容量问题。同时，还需要注意电力成本的弹性。不仅LLM提供商可能会改变成本结构，而且如果您自己托管LLM，美国![方程式图片](../Images/eq-chapter-7-55-1.png)的电力价格波动确实存在[8]。如果您的目标用户群只有20,000人，这可能不是问题，但如果您计划构建能够服务数百万甚至更多用户的产品，电力成本可能成为重大的运营和环境风险。
- en: 7.2.2 Latency, scalability, and availability
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 延迟、可扩展性和可用性
- en: Latency is the time it takes from querying an LLM to getting some output, scalability
    describes how quickly one can go from one to a thousand LLMs running, and availability
    describes the ability to have an LLM operational 24/7\. These are all major advantages
    of LLM—and more broadly, computers in general—over people. LLMs and AI/ML can
    react to more situations faster, at any time, than humans. This reaction speed
    can be both good and bad. When you have a system that requires supervision and
    review of outputs, you do not get the full availability benefit of an LLM without
    developing a staffing plan to match.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是从查询LLM到获得某些输出的时间，可扩展性描述了从单个LLM到一千个LLM运行的快速转换能力，可用性描述了LLM 24/7运行的能力。这些都是LLM——更广泛地说，是计算机——相对于人类的重大优势。LLM和AI/ML可以比人类更快地应对更多情况，在任何时候都能做出反应。这种反应速度既有好的一面，也有不好的一面。当您有一个需要监督和审查输出的系统时，如果没有制定与人员配备计划相匹配的方案，您将无法获得LLM的全部可用性优势。
- en: 7.2.3 Refinement
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 精炼
- en: As we discussed in section [7.1.1](#sec__self_improvement), LLMs cannot easily
    self-improve. However, people can and do improve, and it is a common goal to improve
    the efficiency of a process over time. You will need to keep people in the loop
    to engineer better prompts and create better training regimes to improve efficiency
    with LLMs; without them, LLM performance will not improve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[7.1.1](#sec__self_improvement)节中讨论的，LLM难以自我改进。然而，人们可以并且确实在改进，随着时间的推移提高过程效率是一个共同的目标。您需要让人们在循环中，以设计更好的提示和创建更好的训练方案来提高LLM的效率；没有他们，LLM的性能将不会提高。
- en: Improving LLM efficiency does not just involve upgrading to newer LLMs or fine-tuning
    existing models but also includes building the infrastructure and recording inputs,
    outputs, and performance metrics to study what is working and what is not. You
    can use frameworks like DSPy that we discussed in section [5.5.2](../Text/chapter-5.html#sec__llm_programming)
    to capture these items and to identify and handle the cases that do not work or
    start failing over time as world circumstances change. For example, you might
    develop an initial LLM that is working well. But those damn kids keep adding new
    emojis to the iDroids and appleBots [9]. Without additional training, your LLM
    will not understand these new emojis, but your customers will inevitably start
    using them, so the system will start performing poorly. You’ll never figure this
    out if you don’t record the input and output of the LLM in logs or solicit feedback
    from your users who can provide information about areas where the LLM is failing
    or succeeding. Capturing this information is essential for improving and refining
    the process, which LLMs cannot do without human intervention.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提高大型语言模型（LLM）的效率并不仅仅涉及升级到更新的LLM或微调现有模型，还包括构建基础设施和记录输入、输出以及性能指标，以研究哪些方法有效，哪些方法无效。您可以使用我们在第[5.5.2](../Text/chapter-5.html#sec__llm_programming)节中讨论的DSPy等框架来捕捉这些项目，并识别和处理随着世界环境变化而无法正常工作或开始失败的案例。例如，您可能会开发出一个表现良好的初始LLM。但那些该死的孩子们不断向iDroids和appleBots[9]添加新的表情符号。如果没有额外的训练，您的LLM将无法理解这些新的表情符号，但您的客户不可避免地会开始使用它们，因此系统将开始表现不佳。如果您不记录LLM的输入和输出日志，或者不征求能够提供LLM失败或成功领域信息的用户反馈，您将永远无法发现这一点。捕捉这些信息对于改进和精炼过程至关重要，而LLM没有人类干预是无法做到这一点的。
- en: Note The emoji problem is a great example of why eliminating coding and using
    only LLMs will probably never happen. The emojis will be new tokens that LLM will
    have never seen in training data, so it intrinsically will not be able to handle
    them. How would we handle this in practice? Our first attempt would be to write
    code that detects emojis and replaces them with a description of the emoji’s appearance,
    intent, and connotations. It might not work in every case, but that’s why you
    test and validate.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：表情符号问题是一个很好的例子，说明了为什么消除编码并仅使用LLM可能永远不会发生。表情符号将是LLM在训练数据中从未见过的新的标记，因此它本质上将无法处理它们。我们如何在实践中处理这个问题呢？我们的第一次尝试将是编写代码来检测表情符号，并用表情符号的外观、意图和内涵的描述来替换它们。这可能在每种情况下都不起作用，这就是为什么你需要测试和验证。
- en: In the ML field, considerable attention is given to the concept of data drift,
    where data in the real world constantly evolves beyond what is captured in a model’s
    training data. When dealing with natural language, emojis are just one concrete
    example of how real-world data will change over time as language use evolves.
    The emoji example can be extended to include the problems created by new terminology
    or new ways of using existing words in a language. By looking at the existing
    work in the field, we can identify additional techniques for measuring and mitigating
    data drift for LLMs, such as collecting additional training data and fine-tuning
    models or altering prompts to include supplementary definitions for previously
    unseen terminology.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，人们给予了数据漂移概念相当多的关注，即现实世界中的数据不断演变，超出了模型训练数据所捕获的内容。在处理自然语言时，表情符号只是现实世界数据随着时间的推移而变化的一个具体例子，因为语言的使用方式在演变。表情符号的例子可以扩展到包括由新术语或语言中现有单词的新用法引起的问题。通过查看该领域的现有工作，我们可以确定用于测量和减轻LLM数据漂移的额外技术，例如收集额外的训练数据、微调模型或修改提示以包括对先前未见术语的补充定义。
- en: 7.3 Language models are not models of the world
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 语言模型不是世界模型
- en: You can frequently elicit accurate information about the world from an LLM.
    As a result, it’s easy to assume that a language model knows things about the
    world. Indeed, as a reader of this book, you can reason about the world and what
    will happen without taking any particular action. Now, we are not discussing anything
    so sophisticated as predicting the stock market, but even simple actions and thoughts.
    For example, what would happen if you told someone their sweater was ugly?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从一个语言模型（LLM）中频繁地获取关于世界的准确信息。因此，很容易假设一个语言模型知道关于世界的一些事情。确实，作为这本书的读者，你可以在不采取任何特定行动的情况下对世界和将要发生的事情进行推理。现在，我们不是在讨论像预测股市这样复杂的事情，即使是简单的行动和思考。例如，如果你告诉某人他们的毛衣很丑会发生什么？
- en: You do not need to interact with the environment or find an ugly sweater to
    answer this question. You do not need to speak or interact with anyone or anything
    to answer this question. You can imagine the “world” of sweaters and the feelings
    someone else may have and infer the results. If I told you someone was wearing
    the sweater at a Christmas party (an ugly sweater contest, perhaps?), you could
    update your mental model of the world and infer outcomes without having lived
    them. An LLM cannot think before it speaks. Generating text is the closest an
    LLM gets to “thinking” (using the word loosely in this context). You can see a
    simple example of this in figure [7.6](#fig__uglySweater), where an LLM’s overly
    verbose reasoning ultimately leads it to reach a nice comment. Reasoning, whether
    done implicitly or explicitly by us humans, is distinct from us speaking about
    the thing we are reasoning about. For an LLM, there is no separation of processes;
    producing more output is required to “think more” about the answer. Therefore,
    LLMs are not capable of thought independent from generating output.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要与环境互动或找到一件丑陋的毛衣来回答这个问题。你不需要说话或与任何人或任何事物互动来回答这个问题。你可以想象毛衣的“世界”以及其他人可能有的感受，并推断出结果。如果我告诉你有人在圣诞派对上（可能是一个丑陋毛衣大赛）穿着这件毛衣，你就可以更新你对世界的心理模型，并推断出结果，即使你没有亲身经历过。LLM在说话之前不能思考。生成文本是LLM最接近“思考”的时候（在这个上下文中我们松散地使用这个词）。你可以在图[7.6](#fig__uglySweater)中看到一个简单的例子，其中LLM过于冗长的推理最终导致它给出一个很好的评论。推理，无论是我们人类隐式或显式地进行的，与我们谈论我们推理的事物是不同的。对于LLM来说，没有过程之间的分离；要“思考”更多关于答案，就需要产生更多的输出。因此，LLM不能在没有生成输出的情况下进行思考。
- en: Warning We loosely use the word “think” in the context of an LLM. To be pedantic,
    we mean that the calculations an LLM does to answer a question are not dynamic.
    Outputting 10 tokens takes the same amount of work regardless of the content of
    those tokens. Answering a complex problem that requires humans to think more will
    probably require an LLM to perform more computation, but that usually means the
    LLM must also produce longer output, even if the answer shouldn’t be any longer.
    Whenever anyone uses the term *thinking* in conjunction with an LLM, it is better
    to replace *thinking* with *calculating*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：在LLM的上下文中，我们松散地使用“思考”这个词。为了严谨，我们的意思是LLM为了回答问题所做的计算不是动态的。输出10个标记所需的工作量与这些标记的内容无关。回答一个需要人类更多思考的复杂问题可能需要LLM进行更多的计算，但这通常意味着LLM也必须产生更长的输出，即使答案本身不应该更长。每当有人将“思考”一词与LLM结合使用时，最好将“思考”替换为“计算”。
- en: '![figure](../Images/CH07_F06_Boozallen.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F06_Boozallen.png)'
- en: Figure 7.6 The context and reason why someone is wearing or doing something
    unusual may be in the realm of something that an LLM properly recognizes and for
    which it produces an appropriate response. However, it might not be possible for
    an LLM to reach that appropriate response without producing some intermediate
    text. For a math problem, this intermediate text could be useful, but the intermediate
    text may not always be appropriate or desirable for a user to see.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 某人穿着或做某件不寻常的事情的背景和原因可能属于LLM能够正确识别并对此产生适当反应的领域。然而，LLM可能无法不产生一些中间文本就达到那个适当的反应。对于一个数学问题，这个中间文本可能是有用的，但中间文本可能并不总是适合或希望用户看到。
- en: This example demonstrates that an LLM cannot plan without generating text about
    the planning process. If the LLM is not producing text, it is as if it does not
    exist. There are methods for constructing prompts that will encourage LLMs to
    break down their outputs to simulate planning. This is often called *chain-of-thought*
    (CoT) prompting, where you include in the prompt a statement like “Let’s think
    step by step.” This step-by-step instruction often improves the model’s ability
    to perform tasks [10], but it is unclear why this improves performance. Once again,
    the ambiguity of what it means to “think” can cause unreasonable expectations
    of what LLMs can and cannot do.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，LLM 在没有生成关于规划过程的文本的情况下无法进行规划。如果 LLM 没有生成文本，它就好像不存在一样。有一些方法可以构建提示，以鼓励
    LLM 将其输出分解以模拟规划。这通常被称为 *思维链*（CoT）提示，其中在提示中包含类似“让我们逐步思考”的陈述。这种逐步指令通常可以提高模型执行任务的能力
    [10]，但为什么这能提高性能尚不清楚。再次强调，对“思考”含义的不确定性可能导致对 LLM 能做什么和不能做什么的不合理期望。
- en: 'Even with CoT, LLMs will still make many mistakes, such as missing steps, missing
    calculations, and performing logically invalid reasoning [11]. Other factors may
    contribute to the performance gains observed when an LLM produces output broken
    into a series of steps. Consider:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有认知理论（CoT），LLM 仍然会犯许多错误，例如遗漏步骤、遗漏计算和进行逻辑上无效的推理 [11]。其他因素可能也会导致 LLM 将输出分解成一系列步骤时观察到的性能提升。考虑以下情况：
- en: Back in chapter 3, we learned about transformers and the attention mechanism
    used in their implementations. We learned that the longer the input received and
    outputs produced by an LLM, the more calculations the transformer does. So does
    thinking step by step work better just because the LLM, via the transformer, gets
    to do more *computation*? If the LLM had a world model, it could do this computation
    about the output without generating the output.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们学习了关于变压器及其实现中使用的注意力机制。我们了解到，LLM 收到的输入和产生的输出越长，变压器进行的计算就越多。那么，逐步思考之所以更有效，仅仅是因为
    LLM 通过变压器能够进行更多的 *计算* 吗？如果 LLM 有一个世界模型，它可以在不生成输出的情况下对输出进行这些计算。
- en: LLMs reflect the nature of their training data. There may be content in that
    training data correlated with “think step by step” and other pedagogical materials
    with more verbose and usually correct content. Ultimately, we may manually align
    the LLM’s fuzzy recall with more relevant training documents rather than get the
    LLMs to perform a fundamentally different function.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 反映了其训练数据的特点。训练数据中可能包含与“逐步思考”和其他更冗长且通常正确的教学材料相关的内容。最终，我们可能需要手动将 LLM 的模糊回忆与更相关的训练文档对齐，而不是让
    LLM 执行根本不同的功能。
- en: Warning The precise definition of a “world model” is not yet well agreed upon
    and can have different connotations for different people. When discussing world
    models, it is a good idea to discuss the definition first so that folks are on
    the same page. A lot of LLM discourse talks past each other, something we will
    discuss further in the last two chapters of this book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：对“世界模型”的精确定义尚未达成共识，不同的人可能会有不同的理解。在讨论世界模型时，首先讨论定义是一个好主意，这样大家才能有共同的理解。许多关于大型语言模型（LLM）的讨论都是各自为政，这一点我们将在本书的最后两章中进一步讨论。
- en: These problems are challenging and involve open-ended research questions. Our
    stance is that the dramatic failures of LLMs highlight that these are more likely
    explanations than something deeper. Importantly, some niche research focuses on
    imbuing machine learning methods with world models. A technical but fairly accessible
    2018 example of this from David Ha and Jürgen Schmidhuber is available online
    ([https://worldmodels.github.io/](https://worldmodels.github.io/)) and shows massive
    performance improvements compared with existing methods back then. Others are
    working on making world models for LLMs and using LLMs as world models [12]. Current
    methods do not have the same high degree of flexibility as humans; these examples
    are more limited in scope and work for one general class of problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题具有挑战性，涉及开放性研究问题。我们的观点是，LLM 的戏剧性失败表明这些解释比更深层次的原因更有可能。重要的是，一些专门的研究关注于将世界模型注入机器学习方法。一个技术但相对容易理解的
    2018 年例子来自 David Ha 和 Jürgen Schmidhuber，可在网上找到（[https://worldmodels.github.io/](https://worldmodels.github.io/)），与当时的方法相比，它展示了巨大的性能提升。其他人正在努力为
    LLM 创建世界模型，并使用 LLM 作为世界模型 [12]。当前的方法没有人类那样高的灵活性；这些例子在范围上更为有限，适用于一类通用问题。
- en: '7.4 Computational limits: Hard problems are still hard'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 计算限制：难题仍然很难
- en: Some people are worried about “runaway” AI, where an AI algorithm becomes so
    advanced and capable that it can solve problems we never could and that such an
    AI would not have objectives that align with human welfare. If such an AI existed,
    it could improve itself in ways we couldn’t improve ourselves, resulting in an
    even more powerful AI. Many folks have allowed this thought to run rampant, imagining
    that an LLM will become almost godlike in capability and ability to outreason
    humans. There is an ethics question here that we will discuss more in the last
    chapter of the book. For now, there is a simple technical reason why we are not
    so concerned about this idea, and it also helps us understand the realistic limitations
    of LLMs. Essentially, there are many ways to measure what we can call computational
    complexity or algorithmic complexity. By comparing the complexity of LLMs with
    other well-studied algorithms, we can be more specific about what LLMs can and
    cannot achieve. We will also discuss how approximate solutions to problems using
    LLMs can, where appropriate, avoid some of the complexity of precise solutions
    to the same problems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人担心“失控”的人工智能，即一个AI算法变得如此先进和强大，以至于它可以解决我们从未能解决的问题，并且这样的AI可能不会有与人类福祉相一致的目标。如果这样的AI存在，它可以通过我们无法改进自己的方式来改进自己，从而产生更加强大的人工智能。许多人让这种想法泛滥，想象一个大型语言模型（LLM）将在能力和推理能力上几乎达到神一般，并超越人类。这里有一个伦理问题，我们将在本书的最后一章进行更多讨论。目前，有一个简单的技术原因让我们对这种想法不太担心，这也帮助我们理解了LLM的现实局限性。本质上，有许多方法可以衡量我们所说的计算复杂度或算法复杂度。通过比较LLM与其他研究得很好的算法的复杂度，我们可以更具体地了解LLM能够和不能做到什么。我们还将讨论使用LLM对问题进行近似求解时，在适当的情况下，如何避免对相同问题的精确解的一些复杂性。
- en: In computer science, we spend a lot of time learning about algorithmic complexity.
    For most students or practitioners, this means understanding how a change in the
    amount of input data changes how long it will take a process to produce results.
    One of the more ideal cases, which rarely happens in reality, is that if you double
    the inputs, the process will take twice as long. In other words, a process that
    could take 2 days for ![equation image](../Images/eq-chapter-7-76-1.png) items
    (in the case of an LLM, an item might be a token) takes 4 days for ![equation
    image](../Images/eq-chapter-7-76-2.png). When discussing complexity in computer
    science, we often use mathematical notation, known as Big-O notation, to communicate
    different levels of complexity. When a process’s computation time grows at the
    same rate as the size of its input, it is called linear complexity and is denoted
    in Big-O notation as ![equation image](../Images/eq-chapter-7-76-3.png)). If you
    draw a graph with data size on the x-axis and computation time on the y-axis,
    you would get a line because both data and computation time grow at the same rate.
    Other common real-world complexities include log-linear (![equation image](../Images/eq-chapter-7-76-4.png)),
    where ![equation image](../Images/eq-chapter-7-76-5.png) might be closer to 4.4
    days; quadratic (![equation image](../Images/eq-chapter-7-76-6.png), where ![equation
    image](../Images/eq-chapter-7-76-7.png) might be closer to 8 days; and exponential
    (![equation image](../Images/eq-chapter-7-76-8.png)), where computation time grows
    so quickly as the size of the input increases that there is a good chance the
    world will no longer exist before your algorithm finishes. In each of these cases,
    the graph of input size versus computation time becomes steeper as systems get
    more complex. In other words, for more complex algorithms, the processing time
    will grow faster as the amount of data processed increases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，我们花费大量时间学习算法的复杂性。对于大多数学生或从业者来说，这意味着理解输入数据量的变化如何影响一个过程产生结果所需的时间。在现实中很少发生的一个更理想的案例是，如果你将输入加倍，过程将需要两倍的时间。换句话说，对于一个处理![equation
    image](../Images/eq-chapter-7-76-1.png)个项的过程（在LLM的情况下，一个项可能是一个标记），需要4天时间处理![equation
    image](../Images/eq-chapter-7-76-2.png)。在讨论计算机科学中的复杂性时，我们经常使用一种称为大O记法的数学符号来传达不同复杂性的级别。当一个过程的计算时间与其输入大小以相同的速率增长时，它被称为线性复杂度，在大O记法中表示为![equation
    image](../Images/eq-chapter-7-76-3.png))。如果你在x轴上绘制数据大小，在y轴上绘制计算时间，你会得到一条直线，因为数据和计算时间以相同的速率增长。其他常见的现实世界复杂性包括对数线性(![equation
    image](../Images/eq-chapter-7-76-4.png))，其中![equation image](../Images/eq-chapter-7-76-5.png)可能接近4.4天；二次(![equation
    image](../Images/eq-chapter-7-76-6.png))，其中![equation image](../Images/eq-chapter-7-76-7.png)可能接近8天；以及指数(![equation
    image](../Images/eq-chapter-7-76-8.png))，随着输入大小的增加，计算时间增长得如此之快，以至于有很好的机会在算法完成之前世界将不再存在。在这些情况下，输入大小与计算时间的图表随着系统的复杂度增加而变得更加陡峭。换句话说，对于更复杂的算法，随着处理的数据量增加，处理时间将增长得更快。
- en: We’ve taken this short trip into computer science to help you understand the
    computational complexity of running an LLM. For an input of ![equation image](../Images/eq-chapter-7-77-1.png)
    items, the LLM has a computational complexity of ![equation image](../Images/eq-chapter-7-77-2.png)
    or quadratic complexity. If we can prove that an algorithm/task takes more than
    ![equation image](../Images/eq-chapter-7-77-3.png) work, then we have essentially
    proven that an LLM cannot efficiently solve the problem because an LLM’s core
    algorithms aren’t able to execute algorithms with that level of complexity, precisely.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这次短暂的计算机科学之旅是为了帮助你理解运行LLM的计算复杂性。对于一个包含![equation image](../Images/eq-chapter-7-77-1.png)个项的输入，LLM的计算复杂度为![equation
    image](../Images/eq-chapter-7-77-2.png)或二次复杂度。如果我们能证明一个算法/任务需要超过![equation image](../Images/eq-chapter-7-77-3.png)的工作量，那么我们实际上已经证明了LLM无法有效地解决这个问题，因为LLM的核心算法无法执行这种复杂度的算法，精确地说。
- en: 'Warning This isn’t a graduate class on formal methods or algorithms; we are
    providing a quick overview of the study of algorithmic complexity. The goal is
    to give you, the reader, a technical intuition for the problem, but we haven’t
    fully armed you with all the knowledge needed to discuss this subject in detail.
    To learn more about algorithms and complexity, see Aditya Y. Bhargava’s book *Grokking
    Algorithms: An Illustrated Guide for Programmers and Other Curious People* [13].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：这不是一个关于形式方法或算法的硕士课程；我们正在提供一个关于算法复杂性的快速概述。目标是给你，读者，一个关于这个问题的技术直觉，但我们还没有完全装备你所有讨论这个主题所需的知识。要了解更多关于算法和复杂度的信息，请参阅Aditya
    Y. Bhargava的书籍《Grokking Algorithms：程序员和其他好奇者的图解指南》[13]。
- en: If it was possible to get an LLM to solve a problem that required, say, cubic
    complexity of ![equation image](../Images/eq-chapter-7-79-1.png), but the LLM
    itself had a faster (smaller) complexity of ![equation image](../Images/eq-chapter-7-79-2.png),
    then we would have a logical contradiction. In other words, an LLM can’t solve
    a complex problem faster than the complexity analysis states. Many real-world
    tasks and algorithms have worse than ![equation image](../Images/eq-chapter-7-79-3.png)
    complexities. We describe a few examples in table [7.1](#tab__algorithm_complexity),
    and you’ll notice that the handful we’ve listed relate to logistics or resource
    allocation. For example, delivering packages and rescheduling flights are problems
    that have majorly painful algorithmic complexities.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能让一个LLM解决一个需要，比如说，![方程式图片](../Images/eq-chapter-7-79-1.png)的三次复杂度的问题，但LLM本身的复杂度（更小）为![方程式图片](../Images/eq-chapter-7-79-2.png)，那么我们就会有一个逻辑矛盾。换句话说，LLM不能比复杂度分析所表明的更快地解决复杂问题。许多现实世界的任务和算法的复杂度比![方程式图片](../Images/eq-chapter-7-79-3.png)更差。我们在表[7.1](#tab__algorithm_complexity)中描述了一些例子，你会发现我们所列出的少数与物流或资源分配有关。例如，递送包裹和重新安排航班是具有主要痛苦算法复杂度的问题。
- en: Table 7.1 Some examples of important algorithms with different time complexities
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 一些具有不同时间复杂度的重要算法示例
- en: '![figure](../Images/table_7_1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/table_7_1.png)'
- en: A second important and related reason we care about algorithms is the *complexity
    class* of an algorithm. A complexity class defines the scope of possible algorithms
    that an algorithm can solve. The most famous complexity classes are ![equation
    image](../Images/eq-chapter-7-81-1.png) (for polynomial) and ![equation image](../Images/eq-chapter-7-81-2.png),
    which are problems that take at least ![equation image](../Images/eq-chapter-7-81-3.png)
    time to finish. These very broad classes contain basically all the problems you
    might ever care about.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注算法的第二个重要且相关的原因是算法的**复杂度类**。复杂度类定义了一个算法可以解决的算法的可能范围。最著名的复杂度类是![方程式图片](../Images/eq-chapter-7-81-1.png)（对于多项式）和![方程式图片](../Images/eq-chapter-7-81-2.png)，这些是需要至少![方程式图片](../Images/eq-chapter-7-81-3.png)时间才能完成的问题。这些非常广泛的类别基本上包含了你可能关心的所有问题。
- en: Note Many people think that ![equation image](../Images/eq-chapter-7-82-1.png)
    stands for *not-polynomial*, but this is false! It actually means *nondeterministic
    polynomial*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：许多人认为![方程式图片](../Images/eq-chapter-7-82-1.png)代表“非多项式”，但这是不正确的！它实际上意味着“非确定性多项式”。
- en: What is interesting and informative is that William Merrill and Ashish Sabharwal
    [14] proved that an LLM’s ability to solve problems correlates to the number of
    tokens it generates in intermediate steps. For an LLM, generating a response falls
    into a complexity class called ![equation image](../Images/eq-chapter-7-83-1.png)
    (we know, computer scientists are the worst at naming things). This complexity
    class is very restrictive, meaning an LLM can barely solve anything. As the intermediate
    steps ![equation image](../Images/eq-chapter-7-83-2.png) become longer, you eventually
    reach the complexity class of ![equation image](../Images/eq-chapter-7-83-3.png).
    This means an LLM can never solve real-world problems that are NP or harder! We
    tie this all together in figure [7.7](#fig__computationalComplexity), which shows
    how these layers of complexity classes relate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣且信息量丰富的是，William Merrill和Ashish Sabharwal [14]证明了LLM解决问题的能力与它在中间步骤中生成的标记数量相关。对于LLM来说，生成响应属于一个称为![方程式图片](../Images/eq-chapter-7-83-1.png)的复杂度类（我们知道，计算机科学家在命名事物方面是最差的）。这个复杂度类非常严格，意味着LLM几乎解决不了任何问题。随着中间步骤![方程式图片](../Images/eq-chapter-7-83-2.png)变长，你最终会达到![方程式图片](../Images/eq-chapter-7-83-3.png)的复杂度类。这意味着LLM永远不能解决NP或更难的现实世界问题！我们在图[7.7](#fig__computationalComplexity)中将这一切联系起来，该图显示了这些复杂度类层之间的关系。
- en: '![figure](../Images/CH07_F07_Boozallen.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F07_Boozallen.png)'
- en: Figure 7.7 A Venn diagram of computational complexities (assuming ![equation
    image](../Images/eq-chapter-7-84-1.png), a minor point for the nerds) relate to
    each other. The top arrows give examples of the kind of problem that a new complexity
    class lets you solve. The bottom arrows show where LLMs land in terms of their
    complexity.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 计算复杂性（假设![方程图](../Images/eq-chapter-7-84-1.png)，对专家来说是一个小问题）之间的关系。顶部的箭头给出了新复杂性类允许你解决的问题的例子。底部的箭头显示了LLMs在其复杂性方面的位置。
- en: This finding is even more damaging because complexity classes describe the kinds
    of problems you can solve, not how efficiently you can solve them. For example,
    an LLM must generate on the order of ![equation image](../Images/eq-chapter-7-85-1.png)
    tokens to solve an algorithm that involves ![equation image](../Images/eq-chapter-7-85-2.png)
    complexity. Yet, an LLM also needs ![equation image](../Images/eq-chapter-7-85-3.png)
    time to process ![equation image](../Images/eq-chapter-7-85-4.png) tokens, so
    you end up with ![equation image](../Images/eq-chapter-7-85-5.png) computational
    effort, a massive blow-up in complexity. Also, this complexity estimation does
    not account for LLM training data and the time required to develop prompts to
    get the LLM to perform the algorithm successfully without errors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个发现甚至更有害，因为复杂性类描述了你能够解决的问题的类型，而不是你解决它们的效率。例如，LLM必须生成大约![方程图](../Images/eq-chapter-7-85-1.png)个标记来解决涉及![方程图](../Images/eq-chapter-7-85-2.png)复杂性的算法。然而，LLM还需要![方程图](../Images/eq-chapter-7-85-3.png)时间来处理![方程图](../Images/eq-chapter-7-85-4.png)个标记，因此你最终得到![方程图](../Images/eq-chapter-7-85-5.png)的计算工作量，这是一个复杂性的巨大爆炸。此外，这个复杂性估计没有考虑到LLM的训练数据和开发提示以使LLM成功执行算法而不出错所需的时间。
- en: 7.4.1 Using fuzzy algorithms for fuzzy problems
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 使用模糊算法解决模糊问题
- en: This discussion about algorithms and complexity may sound very damning for LLMs.
    In truth, it is only damning if you want to apply LLMs to problems that require
    correct outputs. If even the smallest error is unacceptable in your system, you
    should not use machine learning, let alone an LLM.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关于算法和复杂性的这次讨论可能听起来对LLMs非常不利。实际上，只有当你想将LLMs应用于需要正确输出的问题时，它才是有害的。如果你的系统中即使是最小的错误也是不可接受的，那么你不应该使用机器学习，更不用说LLM了。
- en: Like machine learning at large, LLMs work best for fuzzy problems, where what
    makes something correct or incorrect is hard to describe. In fuzzy problems, it
    is often the case that it is OK if errors exist; other processes can remediate
    those errors, or the cost of errors is potentially small enough to ignore. That’s
    why text and natural language are a good fit for LLMs. The answers to problems
    like “What did Suzy mean in that email?” or “Did John mean to imply that in his
    text?” are intrinsically fuzzy. Human language is fraught with imprecision, clarification,
    and repetition that align well with the difficulty of getting LLMs to solve problems
    that require consistent and precise answers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像机器学习一样，LLMs在模糊问题上表现最好，因为难以描述什么是对的或错的。在模糊问题中，通常情况下，如果存在错误是可以接受的；其他过程可以纠正这些错误，或者错误的成本可能足够小，可以忽略。这就是为什么文本和自然语言非常适合LLMs。像“苏西在电子邮件中是什么意思？”或“约翰在文本中是否有意暗示？”这样的问题答案本质上是模糊的。人类语言充满了不精确、澄清和重复，这与LLMs解决需要一致和精确答案的问题的难度相吻合。
- en: 7.4.2 When close enough is good enough for hard problems
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 当对于难题来说足够接近就是好的时候
- en: To argue against ourselves for a moment, we should also point out that humans
    cannot solve NP-hard problems when we use *solve* to mean “arrive at the optimal
    solution for which no better solution exists.” We use approximations to solve
    complex problems because we know they are too hard to solve perfectly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反驳我们自己一会儿，我们也应该指出，当我们用“solve”来表示“找到没有更好解决方案的最优解”时，人类无法解决NP-hard问题。我们使用近似值来解决复杂问题，因为我们知道它们太难完美解决了。
- en: For example, in table 7.1 and figure 7.7, we mentioned the traveling salesman
    problem, a famous and important problem for delivery route planning. The mail
    courier wants to deliver everyone’s mail in the minimum amount of time and distance
    traveled without repeating any routes. Computationally, finding the best route
    is NP-hard, so you can only apply it to a few hundred or maybe a thousand delivery
    destinations. However, there are much faster quadratic algorithms that approximate
    the problem, and we can prove they give us a path that is no worse than ![equation
    image](../Images/eq-chapter-7-91-1.png) the travel distance of the minimum distance
    route. So in the real world, we use these and other techniques to get “close enough
    is good enough” solutions. So too can LLMs potentially get “close enough is good
    enough” solutions, but they are still constrained by the fact that they are inefficient
    for exact problems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在第7.1表和第7.7图中，我们提到了旅行商问题，这是配送路线规划中的一个著名且重要的问题。邮递员希望在尽可能短的时间和距离内，不重复任何路线，将所有人的邮件全部投递。从计算的角度来看，找到最佳路线是NP难题，因此只能应用于几百个或可能是一千个配送目的地。然而，存在许多更快的二次算法可以近似这个问题，并且我们可以证明它们给出的路径不会比![equation
    image](../Images/eq-chapter-7-91-1.png)最小距离路线的旅行距离更差。因此，在现实世界中，我们使用这些和其他技术来获得“足够接近即可”的解决方案。同样，LLM可能也能获得“足够接近即可”的解决方案，但它们仍然受限于它们在精确问题上的低效性。
- en: Without an understanding of an LLM’s training data, we have difficulty estimating
    how well it might solve a difficult problem through approximation. Consider that
    the game of chess is technically harder than NP-hard. GPT-3.5 can play a decent
    game of chess that can defeat a real human [15], although not at the “dominating
    all humans” level that dedicated chess programs can achieve. Does this show that
    LLMs are good at approximately solving very hard problems?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有理解LLM的训练数据，我们很难估计它通过近似可能解决一个困难问题的效果如何。考虑一下，棋局在技术上比NP难题更难。GPT-3.5可以玩一局相当不错的棋局，可以击败一个真实的人类[15]，尽管它达不到专业棋程序所能达到的“支配所有人类”的水平。这表明LLM擅长近似解决非常困难的问题吗？
- en: Probably not. First, ChatGPT’s chess game dramatically improved after adding
    chess as an evaluation metric ([https://github.com/openai/evals/pull/45](https://github.com/openai/evals/pull/45)).
    It’s not unreasonable to suspect that the makers of ChatGPT performed fine-tuning
    that incorporated chess as an explicit goal. Second, the internet is full of games
    of chess for people to study and explore ([https://old.chesstempo.com/game-database.html](https://old.chesstempo.com/game-database.html)),
    so ChatGPT has likely been trained on full games of chess captured in its training
    data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不是。首先，ChatGPT的棋局在添加棋局作为评估指标后显著改进了([https://github.com/openai/evals/pull/45](https://github.com/openai/evals/pull/45))。怀疑ChatGPT的制作者进行了包含将棋作为明确目标的微调是合情合理的。其次，互联网上充满了供人们学习和探索的棋局([https://old.chesstempo.com/game-database.html](https://old.chesstempo.com/game-database.html))，因此ChatGPT很可能在其训练数据中训练了捕获的完整棋局。
- en: 'Still, it is interesting that ChatGPT can use what is in its training data
    to play a reasonable game of chess, matching what it has seen before to slightly
    different situations in the future. When considering where an LLM-based solution
    will work best, we recommend this mental framework: apply LLMs to repetitive,
    mildly varying problems to maximize their utility. Applications such as text summarization,
    language translation, writing first drafts of documents, and checking existing
    writing all fit into this category.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然很有趣的是，ChatGPT可以利用其训练数据中的内容来玩一局合理的棋局，将之前看到的情况与未来略微不同的情境相匹配。在考虑基于LLM的解决方案在哪里最能发挥作用时，我们推荐以下思维框架：将LLM应用于重复性、轻微变化的难题，以最大化其效用。例如，文本摘要、语言翻译、撰写文档初稿和检查现有写作都属于这一类别。
- en: Similar lessons come from other areas of deep learning, where it is easier to
    reason about what is happening inside a model than for LLMs. For example, playing
    the game of Go has been one of the longest-standing challenges in AI research
    for decades. AI has only recently been able to beat champion-level players in
    the game. Like LLMs, Go-playing AIs train by observing many example games. Yet,
    if you built a Go-playing bot that performed unusual and/or nonsensical moves,
    it would defeat the “superhuman” AI but lose to human amateurs [16]. This example
    also highlights the risk of using LLMs in adversarial environments, where humans
    are far better at dealing with significant novelty in a situation than current
    AI/LLMs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的其他领域也提供了类似的教训，在这些领域中，比LLM更容易推理出模型内部发生的事情。例如，玩围棋几十年来一直是人工智能研究中的一个长期挑战。人工智能最近才能够在游戏中击败冠军级选手。像LLM一样，玩围棋的人工智能通过观察许多示例游戏进行训练。然而，如果你构建了一个执行不寻常和/或无意义的移动的围棋机器人，它可能会击败“超人”人工智能，但会输给人类业余爱好者[16]。这个例子也突出了在对抗性环境中使用LLM的风险，在这种环境中，人类在处理情况中的重大新颖性方面远比当前的AI/LLM更胜一筹。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The biggest advantage LLMs have over humans is the scale they achieve. LLMs
    can run at low cost, 24/7, and be resized to meet demand with far less effort
    than training up or reducing a human workforce.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM相较于人类最大的优势是它们实现的规模。LLM可以以低成本运行，24/7不间断，并且可以轻松调整规模以满足需求，而无需像培训或减少人力那样付出大量努力。
- en: Humans are better at handling highly novel situations, which is important if
    the people interacting with the LLM might be adversaries (e.g., trying to commit
    fraud).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类在处理高度新颖的情况方面更胜一筹，这在人们与LLM互动可能是对手（例如，试图进行欺诈）的情况下非常重要。
- en: We know LLMs work well for problems similar to what they have seen before in
    their training data, making them useful for repetitive work.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道LLM在处理与它们在训练数据中之前看到的问题类似的问题时表现良好，这使得它们对重复性工作很有用。
- en: Prompt engineering is likely the most effective starting point to “teach” LLMs
    something new unless you can dedicate large amounts of effort and money to data
    collection and fine-tuning.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非你能投入大量努力和资金进行数据收集和微调，否则提示工程可能是“教授”LLM新知识最有效的起点。
- en: LLMs cannot self-improve and are inefficient at solving algorithmic problems
    requiring a specific correct answer. They work best on “fuzzy” problems where
    there is some range of satisfying outputs and some amount of error is acceptable.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM无法自我改进，在需要特定正确答案的算法问题解决上效率低下。它们在存在一定范围内令人满意的输出和一定程度的错误可接受的情况下工作得最好。
