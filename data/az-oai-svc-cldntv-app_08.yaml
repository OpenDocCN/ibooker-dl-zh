- en: Chapter 7\. Exploring the Big Picture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章\. 探索大局
- en: This chapter includes the last pieces of knowledge for your generative AI learning
    with Azure OpenAI and other Microsoft technologies. It includes some future visions,
    interviews with experts, and success stories. Remember, generative AI (and artificial
    intelligence in general) is a highly evolving domain, so use this book as your
    entry to a whole universe of knowledge and learning assets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含了使用 Azure OpenAI 和其他 Microsoft 技术进行生成式 AI 学习的最后一些知识。它包括一些未来愿景、专家访谈和成功案例。记住，生成式
    AI（以及人工智能总体而言）是一个高度演进的领域，所以请将这本书视为您进入一个知识和学习资源的宇宙的入口。
- en: Let’s start this last chapter by discussing what’s next, from an Azure OpenAI
    perspective. For an avid learner and AI adopter like you, what are the other areas
    you should explore?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论 Azure OpenAI 视角中的“接下来是什么”开始这一最后一章。对于一个像您这样热衷于学习和采用 AI 的人来说，您还应该探索哪些其他领域？
- en: What’s Next? The Evolution Toward Microsoft Copilot
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？向 Microsoft Copilot 的演变
- en: Azure OpenAI Service is part of a wider ecosystem. All architectures, APIs,
    and integrations with other generative AI building blocks contribute to the notion
    of AI copilots, which we mentioned in the first chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务是更广泛生态系统的一部分。所有架构、API 以及与其他生成式 AI 构建块的集成都为 AI Copilot 的概念做出了贡献，我们在第一章中提到了这个概念。
- en: AI copilots are technology-enabled assistants, companions that help human agents
    become better, more efficient, workers. The principle behind them is to provide
    an interface (written or spoken) that helps people perform complex tasks, such
    as finding specific information or adding information to a third-party system
    (e.g., a CRM, a support ticketing system).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: AI Copilot 是技术驱动的助手，是帮助人类代理人变得更好、更高效的伴侣。其背后的原则是提供一个界面（书面或口头），帮助人们执行复杂任务，例如查找特定信息或将信息添加到第三方系统（例如
    CRM、支持票务系统）。
- en: As you can see in [Figure 7-1](#fig_1_the_end_to_end_copilot_architecture),
    the end-to-end Microsoft vision for AI copilots includes models from Azure OpenAI,
    but also their connection with other systems such as Microsoft 365, which already
    includes [its own copilot](https://oreil.ly/Jwuff). This can be expanded with
    additional capabilities by leveraging the data from the [Microsoft Graph API](https://oreil.ly/3qy8E),
    the development interface to access data from the 365 suite (including calendar
    and emails from Outlook, and meeting recordings and transcript from Teams, to
    name a few), and the [Microsoft Dataverse](https://oreil.ly/onOnS), previously
    known as Common Data Model, a data store for the Power Platform and [Dynamics
    365](https://oreil.ly/omP3a) ecosystems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图 7-1](#fig_1_the_end_to_end_copilot_architecture)中可以看到的，端到端 Microsoft
    对 AI Copilot 的愿景包括 Azure OpenAI 的模型，但也包括与其他系统（如 Microsoft 365，其中已经包括[它自己的 Copilot](https://oreil.ly/Jwuff)）的连接。这可以通过利用[Microsoft
    Graph API](https://oreil.ly/3qy8E)（365 套件的数据访问开发接口，包括 Outlook 的日历和电子邮件、Teams 的会议记录和转录，等等）和[Microsoft
    Dataverse](https://oreil.ly/onOnS)（之前称为 Common Data Model，是 Power Platform 和 [Dynamics
    365](https://oreil.ly/omP3a) 生态系统中的数据存储）的数据来扩展额外的功能。
- en: '![](assets/aoas_0701.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_0701.png)'
- en: Figure 7-1\. The end-to-end copilot architecture
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 端到端 Copilot 架构
- en: The combination of all these building blocks enables new development patterns,
    augmenting generative AI models with other data sources and systems, and the notion
    of a copilot will certainly evolve over the next few years. You will see this
    kind of end-to-end architecture becoming an industry standard, so I recommend
    you understand how all these pieces connect and enable new productivity and generative
    AI scenarios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些构建块的组合使得新的开发模式成为可能，通过将其他数据源和系统与生成式 AI 模型相结合，Copilot 的概念无疑将在接下来的几年中演变。您将看到这种端到端架构成为行业标准，所以我建议您了解所有这些部分是如何连接并启用新的生产力和生成式
    AI 场景的。
- en: That said, it would take two or three more books to explore all these pieces,
    but to leverage some official Microsoft resources, take a look at the [Learning
    Pathways site](https://oreil.ly/kljOi) from the Microsoft UK team, and check the
    [AI Learning Companion path](https://oreil.ly/SntTJ) as it contains a huge variety
    of videos, articles, and training programs. You can also explore an [illustrative
    example](https://oreil.ly/1_p3Q) of the Microsoft Copilot technology stack, which
    includes Azure OpenAI and other Microsoft services—very useful if you have a technical
    background.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，要全面探索这些内容可能需要两三本书，但为了利用一些官方的微软资源，请查看微软英国团队提供的[学习路径网站](https://oreil.ly/kljOi)，并检查[人工智能学习伴侣路径](https://oreil.ly/SntTJ)，因为它包含大量视频、文章和培训项目。您还可以探索微软Copilot技术栈的[示例](https://oreil.ly/1_p3Q)，其中包括Azure
    OpenAI和其他微软服务——如果您有技术背景，这将非常有用。
- en: 'Let’s now go to what I consider the hidden gem of this book: valuable and exclusive
    insights from interviews with some of the biggest experts out there, which will
    complement the content of this book with diverse perspectives on related topics
    such as design, data quality, the future of AI, etc.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看我认为这本书的隐藏宝藏：从一些最大专家那里获得的宝贵且独家见解，这些见解将从设计、数据质量、人工智能的未来等相关的多个角度补充本书的内容。
- en: Expert Insights for the Generative AI Era
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能时代的专家见解
- en: It is pretty rare, and an extraordinary privilege, to get access to some of
    the most relevant experts on generative AI, people who have a hand in shaping
    generative AI and how organizations are adopting Azure OpenAI and other related
    building blocks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 能够接触到一些最相关的生成式人工智能专家，这些人参与了生成式人工智能的塑造以及组织如何采用Azure OpenAI和其他相关构建块，这是非常罕见的，也是一项非凡的特权。
- en: 'This section includes a series of interviews with:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括一系列访谈：
- en: '[David Carmona](https://oreil.ly/FOSlv)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[大卫·卡莫纳](https://oreil.ly/FOSlv)'
- en: Vice president and CTO for Strategic Incubations at Microsoft and author of
    the [*The AI Organization* (O’Reilly)](https://oreil.ly/JvO8O). This interview
    includes a discussion on AI adoption and advanced use cases, as well as his vision
    of the future of generative AI. We’ll gain top insights from a visionary leader.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 微软战略孵化副总裁兼首席技术官，著有[*《人工智能组织》（O’Reilly）*](https://oreil.ly/JvO8O)。这次访谈包括对人工智能采用和高级用例的讨论，以及他对生成式人工智能未来的展望。我们将从一位有远见的领导者那里获得顶级见解。
- en: '[Brendan Burns](https://oreil.ly/e_vU5)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[布伦丹·伯恩斯](https://oreil.ly/e_vU5)'
- en: 'Corporate vice president at Microsoft, an authentic legend of the cloud native
    ecosystem thanks to his role as Kubernetes cofounder and the author of multiple
    O’Reilly books such as [*Kubernetes: Up and Running*](https://oreil.ly/c86hW),
    [*Kubernetes Best Practices*](https://oreil.ly/DIsrj), [*Managing Kubernetes*](https://oreil.ly/cJDzq),
    and [*Designing Distributed Systems*](https://oreil.ly/5GcFO). This conversation
    discusses the convergence between the generative AI era cloud native architectures.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '微软公司副总裁，云原生生态系统的真实传奇人物，这得益于他在Kubernetes共同创始人以及多本O''Reilly书籍（如[*Kubernetes:
    Up and Running*](https://oreil.ly/c86hW)、[*Kubernetes Best Practices*](https://oreil.ly/DIsrj)、[*Managing
    Kubernetes*](https://oreil.ly/cJDzq)、[*Designing Distributed Systems*](https://oreil.ly/5GcFO)）的作者身份。这次对话讨论了生成式人工智能时代云原生架构的融合。'
- en: '[John Maeda](https://oreil.ly/4jxQ0)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[约翰·梅达](https://oreil.ly/4jxQ0)'
- en: Vice president of Engineering and head of Computational Design/AI Platform at
    Microsoft, and the main sponsor of the Semantic Kernel project. This is an amazing
    exploration of the role of design for AI solutions and the importance of LLM orchestration
    technologies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 微软工程副总裁兼计算设计/人工智能平台负责人，以及语义内核项目的主要赞助人。这是对人工智能解决方案设计作用和LLM编排技术重要性的一个令人惊叹的探索。
- en: '[Sarah Bird](https://oreil.ly/N_NK9)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[莎拉·伯德](https://oreil.ly/N_NK9)'
- en: Microsoft’s chief product officer of Responsible AI. This interview includes
    a conversation with the leader in charge of RAI developments for the Azure AI
    platform, including Azure OpenAI. Sarah gives us a different perspective on such
    an important topic.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 微软负责任人工智能首席产品官。这次访谈包括与负责Azure AI平台（包括Azure OpenAI）RAI发展的领导者的对话，莎拉为我们提供了关于这个重要话题的不同视角。
- en: '[Tim Ward](https://oreil.ly/cersn)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[蒂姆·沃德](https://oreil.ly/cersn)'
- en: CEO at CluedIn, is a great source for data management topics. This discussion
    dives into data quality as an enabler for generative AI developments, but also
    looks at how AI is changing the way companies perform master data management (MDM)
    and quality control.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CluedIn的CEO，是数据管理主题的绝佳来源。这次讨论深入探讨了数据质量作为生成式AI发展的推动力，同时也探讨了AI如何改变公司执行主数据管理（MDM）和质量控制的方式。
- en: '[Seth Juarez](https://oreil.ly/oIrgI)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[Seth Juarez](https://oreil.ly/oIrgI)'
- en: Principal program manager for the AI Platform at Microsoft. Seth has been one
    of the more visible faces of the Azure OpenAI era, thanks to his role as host
    of the [AI Show](https://oreil.ly/h-Fvw). Seth is one of the most well-known professionals
    for Azure OpenAI and Azure AI Studio topics, and a great storyteller who makes
    complex topics look a bit simpler.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 微软AI平台的首席项目经理。塞思是Azure OpenAI时代的标志性人物之一，这得益于他作为[AI Show](https://oreil.ly/h-Fvw)节目主持人的角色。塞思是Azure
    OpenAI和Azure AI Studio主题中最知名的专业人士之一，他是一位出色的讲故事者，能够使复杂的话题看起来简单一些。
- en: '[Saurabh Tiwary](https://oreil.ly/qUK9P)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[Saurabh Tiwary](https://oreil.ly/qUK9P)'
- en: Corporate VP for Microsoft Copilot & Turing. This interview includes a great
    exchange about the vision for Microsoft Copilot as the end-to-end architecture
    that leverages Azure OpenAI Service.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Copilot & Turing的副总裁。这次访谈包括了对微软Copilot愿景的深入交流，它是一个利用Azure OpenAI服务的端到端架构。
- en: Let’s dig into these interviews!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入这些访谈！
- en: 'David Carmona: AI Adoption and the Future of Generative AI'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大卫·卡罗纳：AI的采用和生成式AI的未来
- en: '**A.G.**: So, I know your background, I know about your career at Microsoft,
    but who is David and what’s your role at the Microsoft organization?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：所以，我知道你的背景，也知道你在微软的职业生涯，但谁是大卫，你在微软组织中的角色是什么？'
- en: '![](assets/aoas_07in01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_07in01.png)'
- en: '**D.C.:** Well, thank you for inviting me. It’s a big pleasure. I think we
    share the pain of writing a book, so I am always in awe when somebody takes that
    big adventure. It’s amazing that you did it, so congratulations for that. I think
    when I look at my role in Microsoft at the end of the day, it’s all about creating
    new incubation businesses. I’ve been in Microsoft for almost 23 years now, and
    it’s always been very focused on that function. I’m originally from Spain, I was
    working for Microsoft in Western Europe, then I moved 15 years ago to Corp in
    Seattle. I was part of the cloud incubation, which was amazing to be part of that.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**：感谢您的邀请，这是一件非常愉快的事情。我认为我们都有写书的痛苦经历，所以当有人勇敢地踏上这段冒险之旅时，我总是感到敬畏。你做到了这一点，所以恭喜你。我认为，当我最终审视我在微软的角色时，它全部关于创建新的孵化业务。我在微软已经工作了近23年，一直专注于这个职能。我最初来自西班牙，曾在西欧的微软工作，15年前搬到了西雅图的总部。我曾参与云孵化项目，那是一段非常令人兴奋的经历。'
- en: Then after that, when the cloud became mainstream, and I didn’t have to prove
    all the time the importance of the cloud, then I was offered to lead AI incubation.
    It was just when cloud was becoming mainstream at that time, which was eight,
    nine years ago as it was something that started in Microsoft Research. I was working
    with Microsoft Research at that time, and it was all about creating a new business
    category for Microsoft. Just when that started to be mainstream too, I recently—two
    years ago, so just when I didn’t have to prove again in every conversation the
    importance of AI—I moved to the next businesses. I’m working now on areas like
    the future of AI, which are the new frontiers of AI that we will see in the future.
    For example, applying AI to science, which is an amazing use case scenario. Then
    other areas like quantum computing, which I also have the pleasure to incubate,
    and some others like space, communications, future of communication, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，随着云服务变得主流，我不再需要不断证明云的重要性，于是我被邀请领导AI孵化项目。那时，云服务刚刚开始变得主流，大约是八九年以前，它是从微软研究院开始的。当时我正在与微软研究院合作，我们的目标是创建一个新的业务类别。就在那开始变得主流的时候，大约两年前，也就是在我不再需要在每次对话中证明AI的重要性之后，我转到了下一个业务。我现在正在研究像AI的未来这样的领域，这些是我们在未来将会看到的AI的新前沿。例如，将AI应用于科学，这是一个令人惊叹的应用场景。然后还有其他领域，如量子计算，我也很荣幸地参与了孵化，还有一些其他领域，如太空、通信、通信的未来等等。
- en: '**A.G.**: You’re a good person to talk to about the vision of generative AI,
    artificial intelligence in general, and how we will be using them in the next
    few years. What’s the potential and the cool things that you’re seeing now? What’s
    your vision for this generative AI era?'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你是谈论生成式AI、一般人工智能以及我们将在未来几年如何使用它们的理想人选。你现在看到了哪些潜在的和有趣的东西？你对这个生成式AI时代的展望是什么？'
- en: '**D.C.**: For me, the big difference is the scenarios that you can address
    with this new AI that you couldn’t address before, of course. The whole concept
    of reasoning on top of language, or any other modality, and not only data, that
    is something super powerful and we can speak about a lot. But for me, the big
    transformation, what is really the revolution of this new generation of AI is
    that it is broader, it is more generalized. In the past, to create an AI model,
    you required a specific dataset and a specific model. I still remember those early
    times of AI when we were creating these milestones for AI in Microsoft Research
    of image classification of human parity, speech recognition, etc. All of those
    require a very specific team, super specialized on that, with a very specific
    dataset and model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 对于我来说，最大的不同之处在于你可以用这个新的AI解决之前无法解决的问题，当然。在语言或其他任何模态之上进行推理的概念，而不仅仅是数据，这是非常强大的，我们可以有很多话要说。但对我来说，真正的变革，这一代新AI的革命性之处在于它更广泛，更通用。在过去，要创建一个AI模型，你需要一个特定的数据集和一个特定的模型。我仍然记得那些早期AI的时光，当时我们在微软研究院为AI创造了一些里程碑，比如图像分类、语音识别等。所有这些都需要一个非常专业的团队，对那个领域有非常具体的了解，拥有非常具体的数据集和模型。'
- en: The big change of that, the big impact of that is the possibility. Now that
    model has become something that is not only for data scientists to create, but
    for even end users to customize and use in their daily lives and jobs, which is
    the concept of Copilot. The rest is history after that. But for me, that is the
    core difference of this new AI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个大变化，这个大影响在于可能性。现在这个模型已经变成不仅仅是数据科学家可以创建的，甚至最终用户也可以根据他们的日常生活和工作进行定制和使用，这就是Copilot的概念。在那之后，其余的都是历史。但对我来说，这就是新AI的核心区别。
- en: '**A.G.**: Exactly. Because we have been using AI. We had AI in different products,
    but most people weren’t aware that they were using AI or being subject to AI.
    Now it’s natural, and that’s the concept of democratization, access to technology,
    because we use language, which is the purest way to communicate. I think it’s
    very exciting.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 正是。因为我们一直在使用AI。我们在不同的产品中都有AI，但大多数人并没有意识到他们正在使用AI或受到AI的影响。现在这是自然而然的，这就是民主化的概念，技术的普及，因为我们使用语言，这是最纯粹的方式去沟通。我认为这非常令人兴奋。'
- en: '**D.C.**: This is just the beginning. As you know, I’m super excited about
    what is going to come next. Not only on the technology itself, of course, the
    technology will evolve, but also I think as we understand the technology better,
    and we start using it in more use cases, we’re going to see scenarios that we
    can not even think about today. The one that I mentioned at the beginning that
    I work on a lot lately is, of course, the applicability to scientific discovery,
    which is going to open areas that are just amazing, that we cannot even imagine
    yet.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 这只是开始。正如你所知，我对接下来会发生什么感到非常兴奋。当然，不仅仅是技术本身会发展，我认为随着我们对技术的理解更加深入，我们开始将其应用于更多用例，我们将看到我们今天甚至无法想象的场景。我最初提到的一个我最近一直在研究的应用领域是，当然，科学发现的适用性，这将开辟一些我们甚至无法想象的令人惊叹的领域。'
- en: '**A.G.:** Yes. Bringing that scalability to situations where maybe in the traditional
    world we couldn’t take care of that. There’s this public case with the SERMAS,
    the health department of Madrid in Spain, with Julian Isla that you probably know.
    They’re trying to leverage generative AI to spot the good information, to retrieve
    the good information for rare diseases. Usually, you have a business case behind
    everything, and with traditional AI, you will say, OK, there’s not enough of a
    target public because it’s a rare disease. With this, you can actually bring that
    to all the doctors in the world, and they can spot the situation in a faster and
    more efficient way. That’s a perfect example of the kind of things that even if
    they don’t sound super advanced because it’s just retrieving information, I personally
    love it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。将这种可扩展性带到那些在传统世界中我们可能无法照顾到的情况中。有一个公共案例，是西班牙马德里SERMAS，即卫生部门，与Julian
    Isla合作。他们试图利用生成式AI来发现好的信息，检索罕见疾病的好信息。通常，每件事背后都有一个商业案例，而使用传统AI时，你会说，好吧，目标公众不够多，因为这是一种罕见疾病。而通过这种方式，你实际上可以将它带给世界各地的医生，他们可以更快、更有效地发现情况。这是那种即使它们听起来并不特别先进，因为这只是检索信息，但我个人非常喜欢的一个完美例子。'
- en: '**D.C.**: Yeah. I’m in love with that use case. I’m also part of the nonprofit
    [Foundation 29](https://oreil.ly/DCC5D) that Julian Isla leads, with Carlos Mascias
    and others. For me, it is a great example, as you said, because it’s focused,
    as you know, on diagnosing rare diseases. The problem that we have with a profession
    like doctors is that it relies a lot on the experience of the doctor. That works
    perfectly fine when you are diagnosing a common disease. But the exposure to rare
    diseases in primary care doctors is very low. It’s very difficult for them to
    diagnose those diseases. Consider that the average time of diagnosis of a rare
    disease is seven years. Those are seven years that you are not applying the right
    treatment to that disease. With things like this where a model can help because
    it’s always helping the doctor, so guiding the doctor, giving them clues on where
    the disease could come from, that is an amazing tool that I think is a great example
    of this new paradigm of humans and machines working together.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 是的。我对那个用例很感兴趣。我也是Julian Isla领导的非营利组织[Foundation 29](https://oreil.ly/DCC5D)的一部分，还有Carlos
    Mascias和其他人。对我来说，这是一个很好的例子，正如你所说，因为它专注于诊断罕见疾病。我们面临的一个问题是，像医生这样的职业很大程度上依赖于医生的经验。在诊断常见疾病时，这工作得非常好。但是，初级保健医生接触罕见疾病的机会非常少。他们很难诊断这些疾病。考虑一下，罕见疾病的平均诊断时间为七年。这七年里，你都没有对这种疾病应用正确的治疗方法。对于像这样的模型可以帮助的地方，因为它总是在帮助医生，指导医生，给他们提供疾病可能来源的线索，这是一个了不起的工具，我认为它是人类和机器共同工作这一新范例的绝佳例子。'
- en: '**A.G.**: Exactly, and improving the status quo. Something that is impossible
    to deny is that there’s something that we can improve with technology. You have
    mentioned the models, you have mentioned the platform. This book is about Azure
    OpenAI, but how do you see Azure OpenAI as a piece of technology or as a platform,
    as a technology enabler for this era? How do you see this going? It’s about the
    model, it’s about the platform, the different layers, the thing around it. I have
    my opinion, but I want to hear yours.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 完全正确，并且改善了现状。一个无法否认的事实是，我们可以通过技术来改进某些事情。你提到了模型，你提到了平台。这本书是关于Azure
    OpenAI的，但你怎么看待Azure OpenAI作为一项技术或作为一个平台，作为这个时代的技术推动者？你怎么看待它的未来发展？这关乎模型，关乎平台，不同层次，以及与之相关的事物。我有我的看法，但我想听听你的。'
- en: '**D.C.**: I think it’s a deeper conversation, I could say. If you look at this
    only at one particular layer of the ecosystem, you are probably missing a lot
    of things. For me, AI is more than a technology, it’s a new paradigm, it’s a new
    economy actually. You look at the impact that AI is going to have even for core
    GDP growth and it’s huge. We are addressing it with just a layer of the stack,
    it’s not enough. You need to take a look at the entire ecosystem. In that entire
    ecosystem, you have many players. Of course, you have at the bottom of it, you
    have even the chips, even the pure hardware that you need to consider for this.
    On top of that, you have the big data centers that you need to address and to
    target one of these applications. Then on top of that, you have the foundational
    models, which are super visible, of course, they are a critical part of it. But
    on top of that, you also need the tooling, the platform to really get the most
    of these models. It’s very easy and you know this more than anybody, it’s very
    easy to start a proof of concept on a service and model. Very easy to start doing
    prompts and start getting more of that model. But to create a real use case, to
    create a full scenario, you need way more than that. You need to start talking
    about grounding, safety, things like services integration, such as plug-ins and
    many other areas that are satellite to the model but are equally important. Then
    on top of that, you still have more layers. You have responsibility, which is
    critical. You have the applications, you have the distribution.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**：我认为这是一个更深层次的对话，我可以这么说。如果你只从生态系统的某一特定层来看待这个问题，你可能会错过很多东西。对我来说，AI不仅仅是一种技术，它是一个新的范式，实际上是一个新的经济。你看，AI对核心GDP增长的影响是巨大的。我们只是用技术栈的一层来应对这个问题，这还不够。你需要审视整个生态系统。在整个生态系统中，有众多参与者。当然，在最底层，你有芯片，甚至有你需要考虑的纯硬件。然后，在最上面，你需要解决和针对这些应用的大数据中心。再往上，你有基础模型，它们当然非常明显，它们是其中的关键部分。但除此之外，你还需要工具和平台，才能真正充分利用这些模型。这很容易，你知道这一点比任何人都清楚，很容易在服务和模型上启动一个概念验证，很容易开始进行提示并获取更多模型。但要创建一个真正的用例，要创建一个完整的场景，你需要的东西远不止这些。你需要开始谈论基础，安全性，以及像服务集成、插件等许多其他领域，这些虽然与模型相关，但同样重要。然后，在这之上，你还有更多的层次。你有责任，这是至关重要的。你有应用，你有分发。'
- en: In the case of Azure OpenAI, I think the key thing, of course it’s a critical
    piece of that full stack. But in our case, the principle that we have from the
    very beginning since we started with AI, is that we believe in a speed of innovation
    that goes side by side with the platform. Even internally in Microsoft, the way
    that we look at innovation is providing that innovation as a platform to the rest
    of the company. Then at the same time, we bring that platform to Azure so our
    customers can use it. So Azure OpenAI is a perfect example of that because what
    we did was create the concept of model as a service, making it super easy to be
    accessible from customers, and making it like a first-class citizen of Azure.
    You access it just like any other service, which is again, bringing that to the
    broader platform for developers to create new applications with it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure OpenAI的情况下，我认为关键之处，当然它是整个技术栈中不可或缺的一部分。但在我们的情况下，从我们开始使用AI的那一刻起，我们就坚持的一个原则是，我们相信创新的速度应该与平台并驾齐驱。即使在微软内部，我们看待创新的方式也是将这种创新作为平台提供给公司的其他部分。同时，我们将这个平台带到Azure，让我们的客户能够使用它。因此，Azure
    OpenAI是这一点的完美例证，因为我们所做的是创造了“模型即服务”的概念，使得它对客户来说变得极其容易访问，并且让它成为Azure的第一公民。你可以像访问任何其他服务一样访问它，这再次将这一点带到了更广泛的平台，让开发者能够用它来创建新的应用程序。
- en: '**A.G.**: Yes. With all these layers that you have mentioned from the platform,
    that’s why I was asking the question, because usually the discussion is around
    the model. We are creating a bigger model and this is, before it was like more
    parameters, now it’s the one that is better on the benchmark. But I think the
    models are becoming a commodity, very expensive and difficult to create. But now
    the value, the real value is in the combination of these models with the rest
    of the platform. That’s what I have liked the most from the evolution of Azure
    OpenAI and Azure AI Studio in general during this 2023-2024 period.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。你提到的所有这些平台层，这就是我提问的原因，因为通常讨论都集中在模型上。我们正在创建一个更大的模型，在此之前，它可能只是参数更多，但现在它在基准测试上表现更好。但我认为模型正在成为一种商品，非常昂贵且难以创建。但现在，真正的价值在于将这些模型与平台的其他部分相结合。这就是我最喜欢在2023-2024年期间Azure
    OpenAI和Azure AI Studio整体演变中的部分。'
- en: '**D.C.**: Yeah. Completely agree.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 是的。完全同意。'
- en: '**A.G.:** You have a general view of everything that’s happening at Microsoft,
    like internally, externally, like platforms, models, cool projects, Microsoft
    research, papers, the new things that are coming. What’s the part that gets you
    most excited? Is this about the large language models, the small language models—do
    you have any preference?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你对微软正在发生的一切都有整体的了解，无论是内部还是外部，比如平台、模型、酷炫的项目、微软研究、论文，以及即将到来的一些新事物。哪一部分最让你兴奋？这是关于大型语言模型，还是小型语言模型——你有什么偏好吗？'
- en: '**D.C.:** From the research probably because, of course, in that full stack
    there are things happening on every layer. I’m excited with each of them because
    they are super important. In my heart, I’m a software developer. I’m especially
    passionate about anything that has to do with the platform because it’s what really
    enables developers to create important cool stuff on top of it. I’m a super fan
    of, for example, Azure AI Studio and all the tooling that is there. Anything that
    has to do with the orchestration of the entire lifecycle of models, super big
    fan. In my initial role, I was very focused on how we can transform the development
    process with the cloud. The concept of DevOps, continuous integration, continuous
    deployment, and so on. We released what was at that time called [Visual Studio
    Online (VSO)](https://oreil.ly/IJeOu), now it’s Azure DevOps. I think that we
    always forget that part of the stack and it is hugely important. There’s no way
    that you can be successful in an enterprise adopting AI if you just take a very
    specific tooling and model approach, so you need to look at the entire lifecycle,
    and orchestrate that lifecycle. That, I’m super passionate about.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 可能是因为研究，当然，在全栈中，每一层都有事情发生。我对每一层都感到兴奋，因为它们都非常重要。在我心中，我是一个软件开发者。我对与平台相关的一切特别热情，因为它是真正使开发者能够在其上创建重要酷炫事物的基础。我是Azure
    AI Studio和那里所有工具的超级粉丝。任何与模型整个生命周期编排相关的事物，我都超级喜欢。在我最初的职位上，我非常专注于如何利用云来改变开发过程。DevOps的概念，持续集成，持续部署等等。我们发布了当时被称为[Visual
    Studio Online (VSO)](https://oreil.ly/IJeOu)，现在它是Azure DevOps。我认为我们总是忘记这部分栈，它非常重要。如果你只是采用非常具体的工具和模型方法来采用AI，那么你无法在企业中取得成功，所以你需要查看整个生命周期，并编排这个生命周期。这一点，我超级热情。'
- en: But now, if you ask me about the wow stuff, the things that are coming from
    research that gets me excited, I have to say that, probably because of my current
    job, I’m a huge fan of all the work coming on applying AI to science. There are
    things in there that are just mind-blowing, that we’re just scratching the surface
    of right now. One example that we recently announced was the application of some
    of these models to an actual scientific discovery. In this case, it was a battery
    discovery, the rest of the material to create a battery. It was discovered fully
    with these tools. They are the three things that these models can do, but in a
    sense, at the core of it the concept is as simple as saying, hey, just like AI
    can reason on top of text, just like AI can reason on top of images, video, and
    so on, AI can also reason on top of graphs. A very important graph that is all
    around us is molecules. They are just graphs of atoms. The possibility of AI to
    reason on top of those structures, on top of molecules, is just amazing. We see
    the same concept that we see with generative AI apply for images. Think of DALL·E
    when you write a prompt and the model will deliver an image like an output. We’re
    starting to see that and Microsoft Research has already delivered some of those
    externally on models that can do that with molecules. Think of explaining the
    model, what are the properties that you are looking for in a particular model,
    and the model creating a lot of variance and a lot of possibilities for that molecule.
    That is mind-blowing, think of the possibilities of that.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，如果你问我关于令人惊叹的东西，那些让我兴奋的研究成果，我必须说我，可能是因为我的当前工作，我对将AI应用于科学的所有工作都非常着迷。其中有些东西令人震惊，我们现在只是触及了表面。我们最近宣布的一个例子是将这些模型应用于实际的科学发现。在这种情况下，这是一项电池发现，其余的材料来制造电池。它是完全使用这些工具发现的。它们是这些模型能做的三件事，但在某种程度上，其核心概念就像说，嘿，就像AI可以在文本之上进行推理，就像AI可以在图像、视频等之上进行推理一样，AI也可以在图之上进行推理。一个非常重要的图就在我们周围，那就是分子。它们只是原子的图。AI在那些结构、分子之上进行推理的可能性是惊人的。我们看到与生成AI相同的理念也适用于图像。想想当你写一个提示时，DALL·E会提供一个图像作为输出。我们开始看到这一点，微软研究院已经在一些模型上实现了这一点，这些模型可以用分子来做。想想解释模型，你在特定的模型中寻找哪些特性，模型为该分子创造了很多变体和可能性。这太令人震惊了，想想这个的可能性。
- en: But then, that is the generation part. Then the second part is the simulation.
    With AI, we can simulate the properties and the interactions of these molecules,
    which are, imagine the equivalent of that could be to go to a “wet lab” and do
    that in person. Now, if you are able to do it with AI, accelerating thousands
    of times, the time that was needed on traditional compute, what that allows you
    is to just expand your search space. Now, you can screen millions of molecules
    to find those properties. Then the last one is also helping us to synthesize those
    molecules, giving us the best and more efficient ways of synthesizing those molecules.
    The implications of that in any science—so from materials to health, to sustainability,
    to climate change, to many other domains—is just amazing. When you combine it
    with the concept of reasoning on top of knowledge, now you have on one side AI
    models that can simulate nature. On the other side, you have the concept of a
    copilot for the scientists, where the scientists can use it to reason with all
    the past scientific knowledge and all the current knowledge of that domain. It’s
    just mind-blowing the possibilities of it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但那部分是关于生成的。接下来的一部分是模拟。有了AI，我们可以模拟这些分子的特性和相互作用，想象一下，这相当于亲自去“湿实验室”进行操作。现在，如果你能通过AI做到这一点，速度提高数千倍，那么在传统计算上所需的时间就缩短了，这让你能够仅仅扩展你的搜索空间。现在，你可以筛选数百万种分子来找到那些特性。最后一点是帮助我们合成这些分子，给我们提供合成这些分子的最佳和更有效的方法。这在任何科学领域——从材料到健康，到可持续性，到气候变化，到许多其他领域——的影响都是惊人的。当你结合知识推理的概念时，现在你有一边是能够模拟自然的AI模型。另一方面，你有科学家的“副驾驶”概念，科学家可以利用它来与所有过去的科学知识和该领域的当前知识进行推理。这真是太令人震惊了。
- en: '**A.G.**: It’s impressive, the impact at all levels, even the academic level.
    People that are learning can retrieve all the information, and they can accelerate
    their learning, and they can contribute more and more to the research. You see,
    that’s why I invited you, because you have the vision of these kinds of things.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这在所有层面上都给人留下了深刻的印象，甚至在学术层面上。正在学习的人可以检索所有信息，他们可以加速他们的学习，他们可以越来越多地贡献于研究。你看，这就是为什么我邀请你的原因，因为你对这些事物有远见。'
- en: '**D.C.:** It’s funny because I think what we always talk about, the work that
    AI can do on behalf of humans, but with this concept of AI reasoning on top of
    the collective knowledge of the scientific community, what it can do is actually
    bring in that community even closer, because right now there’s a big barrier for
    scientists to reason on top of the knowledge that was created by other scientists,
    because there’s so much. It’s almost impossible for a scientist to be on top of
    all the collective knowledge of the community. Now, with these tools, it will
    make it easier for scientists to build on top of the discoveries and the progress
    of others, which is amazing.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.**: 这很有趣，因为我们总是谈论人工智能能够代表人类完成的工作，但有了在科学界集体知识之上的AI推理这一概念，它实际上能够将这个社区拉得更近，因为现在科学家在基于其他科学家创造的知识进行推理时面临很大的障碍，因为知识太多了。几乎不可能让一个科学家掌握社区的集体知识。现在，有了这些工具，它将使科学家更容易建立在他人发现和进步的基础上，这真是太神奇了。'
- en: '**A.G.**: To finish the discussion, I’ll come back to your book, *The AI Organization*.
    Students are wondering OK, is this relevant? Descriptive AI, like traditional
    AI, do I need to learn this when I’m talking about generative AI? Now we have
    so many new experts talking about the topic. I said, yes, of course, the kind
    of consideration, the technical consideration, but also organizational considerations
    of adoption and the barriers and the tricks and the things that you need to do
    and the data component, the data strategy of the companies, this is very important.
    And I feel like your book includes a lot of good examples. I remember the one
    with Telefonica and Chema, Alonso, that I like especially because it’s very illustrative
    and very creative. But what would be, if you had to sell the value of that book
    for the generative AI adopters at the company level, what would be the value of
    it?'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 为了完成这次讨论，我会回到你的书，*《人工智能组织》*。学生们在思考，好吧，这和我们有关系吗？描述性人工智能，就像传统人工智能一样，当我谈论生成式人工智能时，我需要学习这些吗？现在有那么多新的专家在谈论这个话题。我说，当然，这种考虑，技术考虑，还有组织上的考虑，包括采用障碍、技巧以及你需要做的事情，还有公司的数据组件，数据战略，这些都非常重要。我感觉你的书包含了很多很好的例子。我记得Telefonica和Chema,
    Alonso的那个例子，我特别喜欢，因为它非常具有说明性和创造性。但如果要向公司层面的生成式人工智能采用者推销这本书的价值，它的价值会是什么？'
- en: '**D.C.:** Yeah, I mean, the book was written just thinking of the learnings
    that I was seeing with big companies embracing AI, right? So, I’ve seen many of
    those in the early days, right? So, early days in AI are like eight years ago.
    So not a long time ago. And it’s funny because usually the blockers that I was
    seeing for adopting AI at scale had nothing to do with technology. So that made
    me wonder, hey, there’s a lot of books talking about the technology, but there’s
    a gap in there on telling the broader story that leaders in organizations of any
    level should know to be successful with AI. So that was the approach to the book.
    I identify four big areas that you need to address to be successful in adopting
    AI at scale. So again, not proof of concept, not specific use cases, but really
    transforming your company with AI, right? And becoming that concept of AI organization.
    And it’s, yeah, technology is one of them. So, of course, it’s there and I talk
    a lot about technology, but I talk about the strategy. You need to have a full
    comprehensive strategy that is inclusive of the short term, but also the long
    term and connecting between those two, right? So I share the learnings in Microsoft
    as well. How we approach that, we call it the horizon framework and how we actually
    make sure that we balance those investments across the horizon. We have a connected
    strategy that is investing in the short term because it has value for the short
    term, but also in the long term and how to connect both, right? So that is good.
    I also talk about the importance of having an approach that is from the technology
    to the business, but then also from the business to the technology, right? That
    is critical. I see, and I was seeing at that time, a lot of conversations that
    started on the possibilities of the technologies, but not what the business needed,
    right? So you need a framework. And I also share the framework that we use in
    Microsoft to have a conversation that is business centric. And then connecting
    that to the technology to focus on identifying the use cases and the long-term
    bets in my company. So that’s the strategy.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**D.C.:** 是的，我的意思是，这本书是想着那些我看到的，大型公司拥抱人工智能时的学习经验而写的，对吧？所以，我在早期就看到了很多这样的例子，对吧？所以，人工智能的早期阶段就像是在八年前。所以并不是很久以前。而且很有趣，因为我看到阻碍大规模采用人工智能的因素与技术无关。所以这让我想，嘿，有很多书在谈论技术，但在这其中有一个缺口，就是没有讲述一个更广泛的故事，这是任何级别的组织领导者为了在人工智能方面取得成功而应该知道的。所以这就是这本书的写作方法。我确定了四个你需要解决的大领域，才能在规模化的采用人工智能方面取得成功。所以，再次强调，不是概念验证，也不是具体用例，而是真正用人工智能来转型你的公司，对吧？并成为那个人工智能组织的概念。是的，技术是其中之一。所以，当然，技术是存在的，我谈了很多关于技术的事情，但我还谈到了策略。你需要有一个全面综合的策略，它包括短期，也包括长期，并且在这两者之间建立联系，对吧？所以我分享了在微软的所学。我们如何处理这个问题，我们称之为“地平线框架”，以及我们如何确保我们在地平线上平衡这些投资。我们有一个连接性的策略，它投资于短期，因为这对短期有价值，但也投资于长期，以及如何将两者联系起来，对吧？所以这是好的。我还谈到了有一个从技术到业务，然后从业务到技术的方法的必要性，对吧？这是至关重要的。我看到，当时我看到很多对话都是从技术的可能性开始的，但没有考虑到业务需求，对吧？所以你需要一个框架。我还分享了我们在微软使用的框架，以进行以业务为中心的对话，并将其与技术联系起来，以专注于识别我公司的用例和长期赌注。所以这就是策略。'
- en: The second one is culture because that’s another critical one. This AI transformation
    is not something that happens in a laboratory. It’s not that you can create a
    center of excellence of AI and consider it like a black box and forget about this
    problem. This is something that will impact, as a leader, you need to know that
    this is something that will impact the entire organization. So every employee
    has to be part of it. And that’s something that requires specific action and need.
    And I also share ways of doing that, some learnings from Microsoft. We have a
    lot of learnings on that one. And you realize how important it is if you compare
    failures with successes, you see that culture is usually a huge part of that.
    When you have the organization not fully bought in, where you have things that
    are isolated, that are not connected, it’s very difficult to have an impact in
    the business by doing that. And then the last one is responsibility. So that is
    a critical one, as you know. And it’s something that we tend to think that it’s
    just creating principles for AI. It’s far more than that, right? You need to turn
    those principles into reality. And now even more as at that time, there was no
    regulation, but now with regulation coming, it’s not going to be like a good addition.
    It’s going to be absolutely critical that every company does that. And it’s not
    something that you can think of at the end of the process, it’s something that
    you have to consider in every step of your development, from the ideation to the
    development to the deploying and monitoring.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是文化，因为这也是一个关键因素。这种AI转型不是在实验室里发生的。不是你可以创建一个AI卓越中心，把它当作一个黑盒子，然后忘记这个问题。这是会影响整个组织的事情。作为领导者，你需要知道这一点。所以每个员工都必须参与其中。这是一项需要具体行动和需求的事情。我也分享了如何做到这一点，一些来自微软的经验。我们在那方面有很多经验。你意识到如果将失败与成功进行比较，你会看到文化通常是其中的一个巨大部分。当组织没有完全投入，有些事情是孤立的，没有连接起来时，通过这种方式很难在业务上产生影响。最后一个是责任。所以这也是一个关键因素，正如你所知。这是我们倾向于认为只是为AI制定原则。这远远不止这些，对吧？你需要将这些原则转化为现实。现在，即使在那时没有法规，但现在随着法规的出现，它不仅仅是一个好的补充。它将变得绝对关键，每个公司都必须这样做。这不是你可以在过程结束时考虑的事情，这是你必须在开发的每一步中考虑的事情，从构思到开发，再到部署和监控。
- en: '**A.G.**: I totally agree. And look, these four pillars are exactly the same
    today. We have the same kind of cases, people get excited about technology, then
    forget the overall strategy of the company, creating a case that has nothing to
    do with the strategy of the company, or the return investment, or the potential
    value for the company. The culture, all the education parts, now it’s becoming
    more obvious that people need to learn about genetic AI, and we see that trend
    beside the technology teams. And the responsible AI part, which is, I call it
    accountability. It’s not AI at this point, because it’s accountable. It’s trustworthy,
    it’s responsible, it’s ethical, everything you want, but there’s a regulation.
    So now we want to be compliant with regulations. So it’s still the same. And that’s
    why I think that it’s still a very good classic for any generative AI adopter
    out there.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：我完全同意。看，这四个支柱今天仍然是一样的。我们面临的情况相同，人们对于技术的兴奋，然后忘记公司的整体战略，创造出与公司战略、回报投资或公司潜在价值无关的案例。文化，所有的教育部分，现在越来越明显，人们需要了解基因AI，我们在技术团队旁边看到了这个趋势。还有负责AI的部分，我称之为问责制。现在这不再是AI，因为它是有责任的。它是值得信赖的，是负责任的，是道德的，是一切你想要的，但是有法规。所以现在我们希望遵守法规。所以它还是一样的。这就是为什么我认为它对于任何生成式AI的采用者来说仍然是一本非常好的经典。'
- en: 'Brendan Burns: The Role of Cloud Native for Generative AI Developments'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 布兰登·伯恩斯：原生云在生成式AI发展中的作用
- en: '**A.G.**: I’m very happy to have you here. I know that a lot of people know
    you, but what’s your current role and journey at Microsoft?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：我很高兴你能来。我知道很多人都知道你，但你在微软的当前角色和经历是怎样的？'
- en: '![](assets/aoas_07in02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_07in02.png)'
- en: '**B.B.**: Sure. I’m currently the corporate vice president for cloud native
    open source and the Azure Management platform. So that’s a focus on, I guess the
    best summary is maybe all things DevOps and modern application development on
    Azure, with a special focus on containers and Linux.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**：当然。我目前是云原生开源和Azure管理平台的副总裁。所以重点是，我想最好的总结可能是所有关于Azure上的DevOps和现代应用开发的事情，特别关注容器和Linux。'
- en: '**A.G.**: So everything related to cloud native and the Microsoft ecosystem,
    you’re there.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 所以与云原生和微软生态系统相关的一切，你都在那里。'
- en: '**B.B.**: Yep. As well as the Azure Resource Manager, which is sort of the
    API gateway with policy, and all sort of infrastructure as code tooling.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的。以及 Azure 资源管理器，它是一种带有策略的 API 网关，以及所有基础设施即代码工具。'
- en: '**A.G.**: Very important for the kind of architecture we discuss in the book.
    And even if it’s an obvious question, what’s your experience in cloud native and
    Kubernetes?'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 对于我们在书中讨论的那种架构来说，这非常重要。即使这是一个明显的问题，你在云原生和 Kubernetes 方面的经验是什么？'
- en: '**B.B.**: Sure, yeah. So I mean, obviously I started the Kubernetes project.
    It’s closing in on a decade actually, which is kind of, I guess, why there’s some
    gray hair, but, you know, I was responsible for the early days of that project,
    shaping, helping shape the community. And then I came to Azure and focused on
    really figuring out how Azure can be the best place to run open source and cloud
    native workloads. And as part of that also, I think helping a lot of enterprises,
    traditional Microsoft customers with their transition to cloud native applications.
    I think there’s a sense that cloud native is like a new startup thing, but actually
    the truth is that I think most of the cloud native applications that are being
    built today are being built by large companies that need this kind of development
    agility and reliability for their applications.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 当然，是的。我的意思是，显然我是开始 Kubernetes 项目的。实际上，这已经接近十年了，这也是为什么我可能会有一些白发的原因，但你知道，我负责了那个项目的早期阶段，塑造并帮助塑造了社区。然后我来到了
    Azure，专注于真正弄清楚 Azure 如何成为运行开源和云原生工作负载的最佳场所。作为那部分工作的一部分，我也认为帮助了许多企业，传统的微软客户，他们在向云原生应用程序的过渡中。我认为云原生就像是一个新的初创企业事物，但实际上，我认为今天正在构建的大多数云原生应用程序都是由需要这种开发敏捷性和可靠性的大型公司构建的。'
- en: '**A.G.**: Yeah, which is connected to the current era of generative AI here
    at Microsoft. What’s your personal point of view on this new wave?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，这与微软当前的时代生成式 AI 相连接。你对这场新潮流的个人观点是什么？'
- en: '**B.B.**: Well, I’m super excited about it. I think everybody’s excited about
    it. I’m really excited about it at a personal level, because it’s actually helped
    me. I think using things like GitHub Copilot actually really does speed up, especially
    when I’m learning something new. I was just learning Rust over maybe six months
    ago. And I found that when you’re in a new language, it just made a huge difference
    in the speed with which I could pick up the idioms. And you think, because sometimes,
    when you’re learning a new language, you’re programming it like the other language.
    So you end up writing Python, like you used to write Java, for example. I think
    having access to those idiomatic patterns helps you become fluent in the language
    a lot faster. Plus I found Rust also is a little bit, the error messages are not
    as good as they could be, I think. And so again, having that ability to be like,
    please fix this error message for me, right? It would just give me the code snippet
    that I needed. And that was pretty useful as well. So I think that’s cool.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 我对此非常兴奋。我认为每个人都对此感到兴奋。我在个人层面上也非常兴奋，因为这实际上帮助了我。我认为使用 GitHub Copilot
    这样的工具实际上确实可以加快速度，尤其是在我学习新事物的时候。我大概在六个月前开始学习 Rust。我发现当你处于一种新的语言中时，它对提高你掌握惯用法的速度产生了巨大的影响。你可能会想，因为有时候，当你学习一种新语言时，你会像编程其他语言一样编程。所以你最终会像以前写
    Java 一样写 Python，例如。我认为能够访问那些惯用模式可以帮助你更快地掌握语言。此外，我发现 Rust 也有一点，错误信息并不像它们本可以那样好，我认为。所以再次，拥有那种请为我修复这个错误信息的能力，对吧？它会给我我需要的代码片段。这也很有用。所以我认为这很酷。'
- en: I think it’s really exciting also how we can help our customers have similar
    reductions in complexity, whether it’s for programming languages, or their infrastructure,
    or you know, any number of things. Becoming cloud native is a good example, actually,
    infrastructure as code (IaC) can be hard for people and enabling people to transition
    from, you know, ClickOps in the portal to infrastructure as code easily is really
    great. And things like mechanical export of an infrastructure as code template
    hasn’t always been that great. And I think generative AI gives us the opportunity
    to go in a different direction and get better, more fluid templates than we do
    if we just, you know, sort of write code that tries to do it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们也非常兴奋地看到我们如何帮助我们的客户在编程语言、基础设施或任何其他众多事物上实现类似的复杂性降低。成为云原生实际上是一个很好的例子，基础设施即代码
    (IaC) 对人们来说可能很困难，而使人们能够轻松地从门户中的 ClickOps 过渡到基础设施即代码是非常棒的。而且，像基础设施即代码模板的机械导出这类事情并不总是那么好。我认为生成式
    AI 给我们带来了机会，让我们能够走向不同的方向，得到比我们仅仅编写试图完成这项工作的代码更好的、更流畅的模板。
- en: '**A.G.**: Yeah, and I think it’s exciting because it goes both ways, no? We
    can leverage generative AI for all cloud native purposes. And we can leverage
    the good practices of cloud native to implement generative AI on existing and
    new applications.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，我认为这很令人兴奋，因为它双向都适用，不是吗？我们可以利用生成式 AI 来实现所有云原生目的。我们还可以利用云原生的良好实践来实现现有和新应用上的生成式
    AI。'
- en: '**B.B.**: Absolutely. Yeah, I mean, it’s interesting to think, right? I mean,
    it sounds sort of grandiose to claim that generative AI wouldn’t exist without
    Kubernetes. But I think actually, it’s kind of true, right? In the sense of not
    like Kubernetes is special, but in the sense of it enabled a lot of people who
    wanted to build large-scale systems to kind of forget about machine management.
    The first step to doing AI inferences is no longer figuring out how to get a bunch
    of machines to work together. Containers and Kubernetes took care of that for
    you. And so you can just say, OK, I’ve got this fleet of machines with GPUs and
    everything else. How do I get my application out there to do the training? And
    I think that that’s the history of computer science in general, building higher
    levels of abstraction to enable the next platform to build on top.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 绝对如此。我的意思是，想想看，这听起来可能有点夸张，声称没有 Kubernetes 就不会有生成式 AI。但我认为实际上这是真的，对吧？不是像
    Kubernetes 特别，而是它使许多想要构建大规模系统的人能够忘记机器管理。进行 AI 推理的第一步不再是弄清楚如何让一堆机器协同工作。容器和 Kubernetes
    为你处理了这一点。因此，你可以说，好吧，我有一支配备了 GPU 和其他一切设备的机器队。我如何将我的应用程序部署出去进行训练？我认为这是计算机科学的一般历史，构建更高层次的抽象，以使下一个平台能够在此基础上构建。'
- en: '**A.G.**: And that’s one of the cases I love the most. If you check the success
    stories at *kubernetes.io* or [CNCF](https://oreil.ly/epXxD), they talk about
    [OpenAI with Cloud Native](https://oreil.ly/b5EfH) and how that was enabling all
    the kinds of things that we have seen, like wider scale, a lot of people can connect
    to ChatGPT. Of course, there is the AI infrastructure from Microsoft behind that.
    But that’s a new enabler for all these areas of applications and the AI compilers.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这是我最喜欢的情况之一。如果你查看 *kubernetes.io* 上的成功故事或 [CNCF](https://oreil.ly/epXxD)，他们会谈到
    [OpenAI with Cloud Native](https://oreil.ly/b5EfH) 以及它是如何使我们看到的所有各种事物成为可能的，比如更广泛的规模，很多人可以连接到
    ChatGPT。当然，背后有微软的 AI 基础设施。但这是所有这些应用领域和 AI 编译器的新推动力。'
- en: '**B.B.**: Yeah. And a reduction in complexity again, too. So not only can generative
    AI reduce complexity, but having that orchestrator reduces the complexity for
    the AI engineers who just don’t need to worry about that problem. And then when
    you get it from Azure, you don’t even have to worry about running it. It just
    takes care of it for you.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的。再次减少复杂性。所以，生成式 AI 不仅能够减少复杂性，而且拥有那个协调器还可以减少 AI 工程师们的复杂性，他们不必担心那个问题。而且当你从
    Azure 获取它时，你甚至不必担心运行它。它只是为你处理一切。'
- en: '**A.G.**: Saving a lot of people like me.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 为像我这样的人节省了很多。'
- en: '**B.B.**: I think it’s also always the goal, right? It’s much easier to consume
    the idea than it is to implement the idea. You can say, OK I know how to use a
    sorting algorithm. You can probably write a sorting algorithm, but it’s going
    to take you a lot more time to write it than it is to use it. It empowers a lot
    of people, which is great.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 我认为这也是一个目标，对吧？消费一个想法比实现一个想法要容易得多。你可以这么说，好吧，我知道如何使用排序算法。你可能能写出一个排序算法，但写出来要比使用它花更多的时间。它赋予了许多人力量，这是很棒的。'
- en: '**A.G.**: Yeah. And accelerates the implementation, something that would take
    so long before now is becoming like a, I wouldn’t say commodity, but certainly
    like something easier to implement.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。这加速了实施，以前需要花费很长时间的事情现在变得更容易实现，我不会说是商品，但确实更容易实现了。'
- en: '**B.B.**: Yeah. And I think you’ll see, I think you see that creative explosion
    then afterwards, where a lot of people who maybe wouldn’t have the patience or
    the skillset to implement generative AI, but they can have really creative ideas
    about how to use it. And so when you make that capability available, you’re going
    to just generate a ton of creativity about how to use it.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的。我认为你会在之后看到那种创造性的爆炸，很多可能没有耐心或技能集来实现生成式AI的人，但他们可以有很多关于如何使用它的创意想法。所以当你使这种能力可用时，你会产生大量的关于如何使用它的创意。'
- en: '**A.G.**: Certainly. From the cloud native perspective at Microsoft, how do
    you see all this explosion of the [technology stack for Copilot](https://oreil.ly/jGrqE),
    Semantic Kernel, all the different cool pieces that we mention in this book? What’s
    your personal opinion on that?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 当然。从微软的云原生视角来看，您如何看待所有这些[为Copilot的技术堆栈](https://oreil.ly/jGrqE)、语义内核，以及我们在本书中提到的所有不同的酷炫组件？您对这一点有什么个人看法？'
- en: '**B.B.**: Well, I mean, you still have to run your application somewhere, right?
    You know, you don’t get to, generative AI doesn’t enable you to not have a webpage
    there or not having a Restful API somewhere. And so not only do tools like Azure
    Machine Learning build on top of Azure Kubernetes Service (AKS), but we’re actually
    also seeing a lot of people building, you know, the frontend application or the
    APIs that they need to have, even plug-ins for OpenAI on top of AKS. It’s still
    a really great place to host code and integrate there. And of course, we have
    GPU support in AKS, so there are people who are doing their own inference or building
    their own models. And some of our largest clusters actually are built to do that
    kind of AI for a variety of different groups. And again, I think it’s about simplicity,
    right? Because if you want to focus on AI, you don’t want to focus on what it
    takes to run a 5,000-node Kubernetes cluster. It’s not the easiest thing to do.
    And if you can just click a bunch of buttons or do an infrastructure as code template
    and have 5,000 GPU nodes, that’s pretty good. And then know that my team is on
    call for those. Saves you a lot of time.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 嗯，我的意思是，你仍然需要在某处运行你的应用程序，对吧？你知道，生成式AI并不能让你不需要网页或者不需要某个地方的Restful
    API。因此，不仅像Azure Machine Learning这样的工具建立在Azure Kubernetes Service (AKS)之上，而且我们实际上还看到很多人在AKS上构建他们需要的客户端应用程序或API，甚至还有在AKS上为OpenAI开发的插件。它仍然是一个非常好的托管代码和集成的场所。当然，我们在AKS中提供了GPU支持，所以有些人正在自己进行推理或构建自己的模型。实际上，我们最大的几个集群都是为了为各种不同的群体提供这种类型的AI而构建的。再次强调，我认为这关乎简单性，对吧？因为如果你想要专注于AI，你就不想专注于运行一个5,000节点Kubernetes集群所需的一切。那不是一件容易的事情。如果你只需点击几个按钮或执行一个基础设施即代码模板，就能拥有5,000个GPU节点，那就相当不错了。然后知道我的团队会随时待命处理这些事情。这能为你节省很多时间。'
- en: '**A.G.**: Yes. And that’s what we’re seeing with Azure AI Studio now, and with
    all these applications and the ability to deploy any kind of model, because the
    book is about Azure OpenAI and a proprietary model. How do you see the role of
    open source as an enabler for generative AI?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。这正是我们现在在Azure AI Studio中看到的情况，以及所有这些应用程序和部署任何类型模型的能力，因为本书是关于Azure
    OpenAI和专有模型的。您如何看待开源在生成式AI中的推动作用？'
- en: '**B.B.**: Yeah. I think, over time, I suspect that there’s going to be more
    and more models that happen, that people tune, that people build for different
    situations. I mean, you’re already seeing that kind of model sharing and model
    retraining happening. I think that’s really great. I think you look at something,
    you know, Semantic Kernel is out in open source sharing our best practices. LangChain
    is out there in open source. I think it’s all rooted in open source. I think also
    one of the things that is going to happen over time, I think will be higher level
    frameworks, too. I think people are still trying to figure out exactly what it
    takes to build a complete copilot. I think there’s a lot of what I would call
    sort of vertical copilots, you know, copilots that are good at one thing. But
    I think there’s value in sort of saying, well, actually, there are some really
    broad areas out there and you may actually want something that knows how to choose
    between copilots. I think of it sort of like search, maybe, right, when you do
    a web search, maps, video? Like there’s a variety of different kinds of content
    you could be searching for. And I think the same thing is going to be true with
    Copilot. There’s going to be multiple levels in terms of choosing what you want
    to generate. I mean, in some sense, it’s like your friends, I guess. Like you
    go to one friend for tech advice, you go to one friend for sports or whatever.
    And you’re going to find the same thing.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的。我认为随着时间的推移，我怀疑会有越来越多的模型出现，人们会为不同的情况调整、构建。我的意思是，你已经在看到这种模型共享和模型重新训练发生了。我认为这真的很棒。我认为你看一些东西，你知道，Semantic
    Kernel正在开源分享我们的最佳实践。LangChain也在开源中。我认为这一切都根植于开源。我认为随着时间的推移，还将发生一些事情，那就是更高层次的框架。我认为人们仍在试图弄清楚构建一个完整的副驾驶需要什么。我认为有很多我称之为垂直副驾驶的东西，你知道，擅长某一件事的副驾驶。但我认为，实际上，有一些非常广泛的领域，你可能实际上需要一种知道如何选择副驾驶的东西。我想象它有点像搜索，对吧，当你进行网络搜索时，地图、视频？你可以搜索各种不同类型的内容。我认为Copilot也会是这样。在选择你想要生成的内容方面，将会有多个层次。我的意思是，在某种程度上，它就像你的朋友一样。比如，你去找一个朋友寻求技术建议，你去找一个朋友寻求体育或其他什么。你将会找到同样的事情。'
- en: '**A.G.**: On one side, there are all the building blocks that are being created
    and the orchestrators that you have mentioned, different approaches like retrieval
    (RAG), like the kind of knowledge bases that we can use, that can be databases,
    etc. And I feel like that’s evolving a lot, of course.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 在一方面，有所有正在创建的构建块以及你提到的协调者，不同的方法，比如检索（RAG），比如我们可以使用的知识库，可以是数据库等。我感觉这正在快速发展，当然。'
- en: '**B.B.**: For sure. And I think there’s a little bit of one of the questions
    I’ve had that, you know, I don’t necessarily have the right answer for, which
    is, when do you do retraining versus when do you do retrieval-augmented generation?
    Because they kind of both do the same thing at some level, you can influence what
    your results are either by retraining or retraining on your corpus or by doing
    retrieval-augmented generation. I think questions like that people are going to
    need to struggle with for a while.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 当然。我认为我有一个问题，你可能不知道，我并不一定有正确的答案，那就是，何时进行重新训练，何时进行检索增强生成？因为它们在某种程度上都做同样的事情，你可以通过重新训练或在你自己的语料库上重新训练，或者通过进行检索增强生成来影响你的结果。我认为像这样的问题，人们可能需要一段时间去努力解决。'
- en: '**A.G.**: Yes. There’s no single answer for that. In one of the chapters from
    this book, I mention (very carefully as it is such a new topic) that we need to
    try and test depending on the dataset, depending on the kind of retraining, the
    kind of fine-tuning you want to have or the general behavior of the LLM compared
    to the kind of tasks that you’re assigning.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。这个问题没有单一的答案。在这本书的其中一个章节中，我提到（非常谨慎地，因为这个话题非常新）我们需要根据数据集，根据你想要的重新训练类型，细调类型，或者与分配的任务相比的LLM的一般行为来尝试和测试。'
- en: '**B.B.**: Or even the app. You’re probably not going to be able to retrain
    it for every single customer. You may have to do it because you’re like, well,
    I have such a diverse set of users. I want to provide personalized content for
    each user, but I can’t retrain every user, so I’m going to use retrieval-augmented
    generation. But on the other hand, you can be like, I am my company and it’s worth
    it to retrain because I know my company and I’m only going to have results for
    my company. I think it’s interesting stuff.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 或者甚至是一个应用。你可能无法为每个客户重新训练它。你可能必须这样做，因为你可能觉得，嗯，我有一群多样化的用户。我想为每个用户提供个性化的内容，但我不能为每个用户重新训练，所以我将使用检索增强生成。但另一方面，你可以这样想，我是我的公司，重新训练是值得的，因为我了解我的公司，我只会有我公司的结果。我认为这是很有趣的事情。'
- en: '**A.G.:** Right, and maybe it’s a combination with segmentation or a recommender
    system, something that will pre-filter the kind of users you have in front of
    you. And then based on the ability that user may have to access the information,
    the knowledge base based on the active directory or whatever, you can customize
    that answer then.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 对，也许它是与分段或推荐系统的结合，某种预先过滤你面前用户类型的东西。然后根据用户可能访问信息的能力，基于活动目录或 whatever
    的知识库，你可以定制那个答案。'
- en: '**B.B.:** I mean, role-based access control (RBAC) is a fascinating part. We
    have this challenge even in the Azure Resource Graph, which is used for at-scale
    querying. It’s an index of all your Azure resources. Applying access control to
    that is a very interesting problem. Because obviously you can’t build an index
    for each user, right? There’s one index of all the resources. And so then you
    have to basically be like, OK, I did the query. I found some data. Now, which
    of the data that I got back did this user actually have access to or put it into
    the query itself and actually say, as I do my search query, only show me things
    that also this person has access to. And yeah, obviously, it’s important to get
    right.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 我的意思是，基于角色的访问控制（RBAC）是一个很有趣的部分。即使在用于大规模查询的Azure资源图中，我们也面临着这样的挑战。它是所有Azure资源的索引。将访问控制应用于它是一个非常有趣的问题。因为显然你不能为每个用户构建一个索引，对吧？只有一个所有资源的索引。然后你就必须基本上这样想，好吧，我已经做了查询。我找到了一些数据。现在，我得到的数据中哪些是这位用户实际上可以访问的，或者把它放入查询本身，并在我做搜索查询时，只显示这位用户也有权限访问的东西。显然，正确地做到这一点非常重要。'
- en: '**A.G.:** Totally. With all this complexity, what would be your recommendation
    in terms of upskilling, for people to follow in this area, like any kind of thing
    that will help learners and readers to keep track of everything that’s going on?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 完全同意。面对所有这些复杂性，你对于提升技能有什么建议，对于想要在这个领域跟进的人来说，比如任何有助于学习者读者跟上所有发生的事情的东西？'
- en: '**B.B.**: The two things I would say is, I would definitely recommend playing
    around with it. I think the Bing chat is a great way to get in and give it a try,
    because it’s really important, I think, to get a sense for what it’s good at and
    what it’s not good at. Because I think when you see or read the articles or you
    hear, or even when you see examples, they’ve been kind of cherry-picked. They’re
    never going to show you bad examples. And I think it’s really valuable to get
    in there and realize that it’s not perfect. Even beyond the hallucinations, which
    I think people are getting a handle on how to deal with, I think with some questions
    it just isn’t very good. And I think that experiential is the way to go. Give
    yourself a task, try and figure out what the system is good at.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 我要说的两件事是，我肯定会推荐尝试一下。我认为必应聊天是一个很好的入门方式，因为它真的很重要，我认为，要了解它的优点和不足。因为我认为当你看到或阅读文章，或者听到，甚至当你看到例子时，它们已经被挑选出来了。它们永远不会展示给你不好的例子。我认为真正有价值的是进去并意识到它并不完美。即使超出了幻觉，我认为人们已经掌握了如何处理它，我认为对于一些问题，它就不是很擅长。我认为经验是走向成功的途径。给自己一个任务，尝试找出系统擅长什么。'
- en: In particular, I would say I think it’s really good at summarization in general.
    I found that it’s quite good at taking information and distilling it down. It
    can be good at things like error messages for compilers. It can be really bad
    also sometimes. I think you have to get your own sense of what you think it is
    good at and what you think it’s bad at. Because it will give you a sense of which
    ideas you could use it for. Because you may think, I could use generative AI to
    do this and you’re like, well, in practice, that’s not going to work out very
    well. So that’s part one, and then I think part two is that I’m a big believer
    in getting your hands dirty with a toy project that’s meaningful to you. I do
    a lot of hacking with random stuff in my house to turn on lights or whatever.
    Don’t just go through the toy examples because you don’t have a personal connection.
    And I think that personal connection helps you build. Of course, you can’t build
    the whole app at the start. You need something small and constrained to make sure
    you continue to make progress. I think that’s usually the way I go when I’m learning
    new tech. I really want to get a sense of how it works and how I put a whole piece
    of it together. That skeleton. And then, you know, then you can move on to saying,
    OK, I now have the knowledge to go build the real app that I was thinking about
    building.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我会说它在一般意义上的总结方面做得非常好。我发现它在提取信息并提炼它方面相当出色。它可以擅长像编译器的错误消息这样的东西。有时它也可能非常糟糕。我认为你必须自己判断它擅长什么，不擅长什么。因为这会给你一个感觉，你可以用它来实现哪些想法。因为你可能会想，我可以用生成式
    AI 来做这件事，但实际上，这可能不会很顺利。所以这是第一部分，然后我认为第二部分是我非常相信通过一个对你有意义的玩具项目来“动手实践”。我在家里用一些随机的东西做很多黑客攻击，比如打开灯之类的。不要只是通过玩具示例，因为你没有个人联系。我认为这种个人联系有助于你构建。当然，你不可能一开始就构建整个应用程序。你需要一个小而受限制的东西来确保你继续取得进步。我认为我学习新技术时通常就是这样做的。我真的想了解它是如何工作的，以及我是如何把它拼凑在一起的。那个框架。然后，你知道的，然后你就可以继续说，好吧，我现在有了构建我真正想要构建的应用程序的知识。
- en: '**A.G.**: I think those two points are very precise on what acquiring the experience
    is like. Of course with Azure OpenAI, but also the different technologies out
    there or the flavors of Azure OpenAI on different products, and what the limitations
    and the advantages are. Because there are very good advantages, but also limitations.
    For example, I was checking something related to finding information related to
    a specific person. Maybe that’s not always the best use case scenario because
    it’s linguistics, you have Adrián González from Microsoft and another Adrián González,
    the baseball player. So, yeah, I totally agree on that.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：我认为这两个点非常精确地描述了获得经验的过程。当然，这是在 Azure OpenAI 的背景下，但也是关于那里不同的技术或 Azure
    OpenAI 在不同产品上的不同版本，以及它们的局限性和优势。因为确实有很多优势，但也有局限性。例如，我正在检查与查找与特定人物相关的信息相关的内容。也许这并不总是最佳用例场景，因为它是语言学，你有来自微软的
    Adrián González 和另一位棒球运动员 Adrián González。所以，是的，我完全同意这一点。'
- en: And remind me, you have several O’Reilly books as well, right? You’re in the
    club of authors with several books. Can you tell us a bit about them and what
    they are about?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请提醒我，你也有几本 O’Reilly 的书，对吧？你是拥有多本书籍的作者俱乐部成员。你能告诉我们一些关于它们的内容吗？
- en: '**B.B.**: Well, I’ve written a couple of different books on Kubernetes. [*Kubernetes:
    Up and Running*](https://oreil.ly/NZcsJ), which I wrote with Kelsey Hightower
    and Joe Beda. And then more recently, [the third edition](https://oreil.ly/IvoU6)
    was written with Lachlan Evanson, who’s another person at Microsoft. And then
    actually right now I’m working on the [second edition](https://oreil.ly/7HkaR)
    of [*Designing Distributed Systems*](https://oreil.ly/vbUwd). And it’s actually
    going to go into a little bit, probably not in as much depth as your book, but
    go into a little bit of how to build AI systems in the context of distributed
    systems.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**：嗯，我写过几本关于 Kubernetes 的不同书籍。[《Kubernetes: Up and Running》](https://oreil.ly/NZcsJ)，这是我与
    Kelsey Hightower 和 Joe Beda 合著的。然后最近，[第三版](https://oreil.ly/IvoU6)是与微软的另一位人士 Lachlan
    Evanson 合著的。现在实际上我正在编写[*Designing Distributed Systems*](https://oreil.ly/vbUwd)的第二版。实际上，它将涉及到一些内容，可能不会像你的书那样深入，但会涉及到在分布式系统背景下构建
    AI 系统的一些内容。'
- en: And then actually the most exciting chapter that I’m adding there for the second
    edition is what I’m going to call the greatest hits chapter, which is all of the
    problems that people had, which discusses all of the mistakes that people make
    that keep coming up over and over again. Because we go to live sites and you sit
    through outages and postmortems and all this kind of stuff. And after you do it
    for a few years you see that there are patterns that repeat. And I’ve been taking
    some notes and I’ve written down a bunch of the ones that repeat over and over
    again. For example, one of the ones that comes up a lot is our monitoring didn’t
    think that the absence of errors should be an error. If there were a lot of errors
    you’d notice. But if it goes absolutely quiet and there’s nothing, it could mean
    you’re totally OK, but it could also mean that you’re not processing anything.
    And several times we’ve seen systems where they have a monitoring gap, where for
    some reason they stopped processing anything. With this idea that no news was
    treated as good news, they didn’t alert anybody until a customer was like, hey,
    wait a minute, where are my deliveries? You could be monitoring delivery lanes,
    anything like that in terms of an online retailer, right? An online retailer could
    monitor how long it takes a package to get from point A to point B from my delivery
    center to the customer. And they could alert if that goes over 12 hours or whatever.
    But if you stop delivering all packages, that alert does not fire. Because there’s
    no deliveries, it didn’t take any time. It’s subtleties like that and it doesn’t
    occur to you in the first place because you’re so used to the steady state.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，实际上我为第二版添加的最激动人心的章节，我将称之为“经典之作”章节，其中包含了人们遇到的所有问题，讨论了人们反复犯的错误。因为我们去现场，你会在故障和事后分析中度过时间，以及所有这类事情。在你做了几年之后，你会发现有一些模式是重复出现的。我已经做了一些笔记，并记下了那些反复出现的许多问题。例如，其中一个经常出现的问题是我们的监控没有意识到错误的缺失应该被视为一个错误。如果有大量的错误，你会注意到。但是，如果它完全安静下来，什么都没有，这可能意味着你完全正常，但也可能意味着你没有处理任何东西。我们曾多次看到系统存在监控缺口，由于某种原因，它们停止了处理任何东西。这种“没有消息就是好消息”的想法，他们直到客户说，嘿，等一下，我的货物在哪里？你可以在在线零售商的交付通道上进行监控，任何类似的事情。在线零售商可以监控包裹从我的配送中心到客户点A到点B所需的时间。如果超过12小时或
    whatever，他们可以发出警报。但是，如果你停止交付所有包裹，这个警报就不会触发。因为没有交付，所以没有花费时间。这样的细微差别，你一开始可能不会想到，因为你已经习惯了稳定状态。
- en: '**A.G.**: I think that applies to what we’ll see in future editions of this
    book. Like the kind of learnings from the industry, the kind of things we don’t
    know because we don’t know it yet. It will be based on experience. We are just
    starting this wave for generative AI, but certainly the same case here.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我认为这适用于我们将在本书的未来版本中看到的内容。就像从行业中获得的那些学习经验，以及我们不知道的事情，因为我们还没有意识到。它将基于经验。我们刚刚开始这股生成式AI的浪潮，但在这里情况肯定是一样的。'
- en: '**B.B.**: Yeah. Oh, yeah. I imagine it’s going to change rapidly, actually,
    as more and more people get in there. The first couple of years when people got
    in, the same thing happened with cloud native open source, right? Even with UI
    frameworks. I think mostly people use React now, but like there was a solid two
    or three years where I felt like people were changing every three months. It seemed
    like every time I talked to somebody, they’d switch their JavaScript framework.
    I’m sure the same thing will happen with AI, right? Because I think it takes a
    little bit for people to figure out what abstractions actually work. What are
    the abstractions that make sense? What are the common problems that we can turn
    into libraries? I think there’s a lot of free-form prompt engineering happening
    right now. I think there’s going to be a lot more science that comes into that.
    And I don’t know if science is the right word, but a lot more like rigor that
    comes into that kind of stuff over time as people figure out kind of what works
    and what doesn’t work. The templates, operations, the best practices, the countermeasures.
    I think at some point you’ll probably just be able to hit a checkbox and get a
    bunch of the fixes and all that kind of stuff, hallucination prevention and stuff
    like that.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 嗯。哦，是的。我想它实际上会迅速变化，因为越来越多的人参与其中。在最初的几年里，当人们开始参与时，云原生开源社区也发生了同样的事情，对吧？甚至包括UI框架。我认为现在大多数人使用React，但就像有一段时间，我觉得人们每三个月就会更换一次JavaScript框架。每次我与人交谈时，他们似乎都会切换他们的JavaScript框架。我确信AI也会发生同样的事情，对吧？因为我认为人们需要一点时间去弄清楚哪些抽象实际上有效。哪些抽象是有意义的？哪些是我们可以将其转化为库的常见问题？我认为现在正在进行大量的自由形式提示工程。我认为将来会有更多的科学进入这个领域。我不知道“科学”这个词是否恰当，但随着人们弄清楚什么有效什么无效，会有更多类似严谨的东西进入这类事物。模板、操作、最佳实践、对策。我认为在某个时候，你可能只需勾选一个复选框，就能获得一大堆修复和类似的东西，幻觉预防等。'
- en: '**A.G.**: Hey, just a last question, because you mentioned the postmortem,
    but there’s something we mentioned in the book, which is the premortem. Did you
    use the notion of a premortem to see what could go wrong?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 嘿，就问最后一个问题，因为你提到了事后分析，但我们书中还提到了一个概念，即“事前分析”。你使用“事前分析”的概念来预见可能出错的事情了吗？'
- en: '**B.B.:** Yeah, it’s sort of like what we call red teaming as well, where you’re
    trying to break that, you know, you’re purposely trying to break stuff. And yeah,
    I think that’s really important. I think you check both bad stuff, obviously,
    like there’s been stuff in the press and otherwise about, you know, ways you can
    trick these models, but also honestly, just to see if it does a good job. I think
    it’s more prosaic. You know, nobody writes a headline about something that they’re
    like, this query was not answered very well. But obviously, if you’re building
    a product, that’s really important to understand: does it actually work? And I
    think actually measuring, that’s the other thing I think is going to be really
    interesting. And a lot of growth is happening. It’s like measuring the quality
    of a model. I don’t think we’ve done a ton of rigorous scientific measurement.
    I mean, there are scoreboards and things like that to measure against benchmarks
    but it’s not clear that’s 100 percent connected to the reality of a user experience
    once you build it into a product. And I think just as we’ve done a lot of work,
    you know, in the Azure portal and things like that on figuring out, where we are
    confusing the user? You know, where do we have a UI that’s not great? I think
    we’re going to do the same thing with these chat systems, right, where it’s probably
    going to be, how many times did people click on the prompts we suggested or how
    many times did they hit the clear button or, you know, there’s a lot of ways you
    can figure out, are we giving them the answers that they want?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的，这有点像我们所说的“红队”行动，你试图打破它，你知道，你是有意去破坏东西的。是的，我认为这非常重要。我认为你检查了坏的东西，显然，媒体上和其它地方都有关于如何欺骗这些模型的方法，但老实说，只是为了看看它是否做得好。我认为这更实际。你知道，没有人会写一个标题说，这个查询回答得不好。但显然，如果你在构建一个产品，了解它是否真的工作是非常重要的。我认为实际上测量，这也是我认为将会非常有趣的事情。现在有很多增长。这就像测量模型的质量。我认为我们并没有做很多严格的科学测量。我的意思是，有一些排行榜和类似的东西可以用来与基准进行比较，但一旦你把它集成到产品中，这并不一定与用户体验的现实情况有100%的联系。我认为我们将在Azure门户和类似的东西上做很多工作，以找出我们哪里让用户感到困惑？你知道，我们的UI哪里不好？我认为我们也会用这些聊天系统做同样的事情，对吧，可能就是人们点击我们建议的提示的次数有多少，或者他们按清除按钮的次数有多少，你知道，有很多方法可以找出我们是否给了他们想要的答案？'
- en: '**A.G.**: Totally, because right now with the benchmarks and the evals type
    of projects on LangChain and Azure AI Studio, we are focusing on the core model
    parts. But then you are mentioning all the quantitative and qualitative measures
    that we usually do in product analytics, for example. That’s something I mention
    in the book. You’ll see it in some of the chapters because obviously that part
    will evolve a lot, but getting the sense of the metrics, how good or bad you are
    doing from a user perspective, is crucial. I think that would also be very useful
    for companies.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 完全同意，因为目前我们在LangChain和Azure AI Studio上的基准和评估类型的项目中，我们专注于核心模型部分。但你提到的所有定量和定性指标，这是我们通常在产品分析中做的。我在书中也提到了这一点。你会在一些章节中看到它，因为显然这部分会有很多变化，但了解指标，从用户的角度来看，你做得好还是不好，这是至关重要的。我认为这对公司也会非常有用。'
- en: '**B.B.**: For sure. Yeah, absolutely. I think that it’s in its infancy right
    now. It’s going to be really interesting to see how we figure that out. So I also
    am pretty excited. Microsoft is also really big into accessibility and computing
    for all. And I think that it’s also going to be a game changer in terms of usability,
    because we see people with challenges and we do a lot of work in our UX for accessibility,
    but I think a chat or a voice-based UX that is actually empowered by generative
    language could be significantly better than what we provide with a mouse-based
    or sound-based UI.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 当然。是的，绝对是这样。我认为它目前还处于初级阶段。看到我们如何解决这个问题将会非常有趣。所以我也非常兴奋。微软也非常重视可访问性和面向所有人的计算。我认为这也会在可用性方面带来变革，因为我们看到人们面临挑战，我们在我们的用户体验可访问性方面做了很多工作，但我认为由生成式语言支持的聊天或基于语音的用户界面可能会比我们通过鼠标或声音界面提供的要好得多。'
- en: '**A.G.**: I’m loving this case with the Portuguese government [creating an
    avatar](https://oreil.ly/jMgb6) for people who cannot write and then you have
    another for people who can write but cannot talk. I think that’s where we are
    leading to. We’re saying that this generative AI is equivalent to what the visual
    interface was to command lines. And I believe that’s kind of true.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我非常喜欢这个案例，葡萄牙政府为无法书写的人创建了一个[头像](https://oreil.ly/jMgb6)，然后还有一个为可以书写但无法说话的人。我认为这就是我们将要走向的方向。我们说这种生成式人工智能相当于视觉界面与命令行之间的关系。我相信这是真的。'
- en: '**B.B.**: Yeah, I think it’s going to be really exciting to see how it transforms
    things. And it’s fun to be a part of it. I guess that’s why we’re always here.
    It’s fun to be involved in the transformation as well.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**B.B.**: 是的，我认为看到它如何改变事物将会非常令人兴奋。成为其中一员也很有趣。我想这就是我们一直在这里的原因。参与这种转变也很有趣。'
- en: 'John Maeda: About AI Design and Orchestration'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 约翰·梅达：关于人工智能设计与编排
- en: '**A.G.**: I know you very well just because I’m some sort of groupie of what
    you are doing with your learning resources, but let’s learn a little bit about
    who John is and what your role at Microsoft is as well as your previous background.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我非常了解你，只是因为我对你使用学习资源的方式有一种狂热。但让我们稍微了解一下约翰是谁，以及你在微软的角色以及你的先前背景。'
- en: '![](assets/aoas_07in03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_07in03.png)'
- en: '**J.M.**: I’m lucky to get to work in the middle of the AI Superstorm and there
    is a project called Semantic Kernel that I am helping to advance. It’s a way to
    enable more enterprises to take advantage of this new kind of AI. Before that,
    I was in the physical security industry. I was a chief technology officer of a
    mid-cap security company called Everbridge. We took care of the world, countries,
    cities, and corporations. Before that I worked in places like venture capitals.
    I was at MIT for a while and did research and I also worked in a late-stage startup
    to really understand where the world is heading.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 我很幸运能够参与到人工智能超级风暴的中心，有一个名为语义内核的项目，我在帮助推进它。这是一种让更多企业能够利用这种新型人工智能的方法。在那之前，我在物理安全行业工作。我是Everbridge这家中型安全公司的首席技术官。我们负责照顾世界、国家、城市和公司。在那之前，我在风险投资公司工作过。我在麻省理工学院待了一段时间，进行过研究，还在一个后期初创公司工作，以真正了解世界的发展方向。'
- en: '**A.G.**: Amazing. Such an interesting background. One of the things I really
    like about you is that you are converging the design world and the AI world, which
    is very intuitive for some people. Like, of course, if we’re interacting with
    artificial intelligence, we want to have an interface and a design with a human-centric
    process. But what’s your opinion on the importance of this kind of design process
    and design thinking for AI applications, including generative AI?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 太棒了。背景如此有趣。我真的很喜欢你的地方之一是你将设计界和人工智能界结合起来，这对一些人来说是非常直观的。比如，当然，如果我们与人工智能互动，我们希望有一个以人为中心的过程的界面和设计。但你对这种设计过程和设计思维在人工智能应用中的重要性，包括生成式人工智能，有什么看法？'
- en: '**J.M.**: Yeah, well, I give an annual report at South by Southwest about the
    intersection of design, technology, and business. This year was called [Design
    Against AI](https://oreil.ly/pOZmj), which has two meanings. One meaning is design
    protest against AI, and the other is design competes against AI. So one is more
    of a kind of like, you know, give up. Stop it. The other is to, say, maybe I’ll
    take it on. I think creative people should be competing with AI instead. Trying
    to see how to advance their craft. Many people say it’s about collaborating with
    AI instead of just competing. That said, I think that this kind of AI is not about
    the pictures or the text. It really is about the tools, functions, actions. That’s
    why in Semantic Kernel we say plug-ins, planners, personas. I’ve heard nowadays
    people say large action model instead of just large language model where large
    action model assumes you’re using functions, plug-ins, function calling. I think
    that verb aspect of AI is going to be the thing that unlocks much more value than
    we could ever imagine.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 是的，嗯，我在西南偏南大会上做了一份关于设计、技术和商业交叉点的年度报告。今年被称为[设计对抗AI](https://oreil.ly/pOZmj)，有两个含义。一个含义是设计抗议AI，另一个含义是设计与AI竞争。所以一个更像是，你知道的，放弃。停止。另一个是说，也许我会接受挑战。我认为创意人士应该与AI竞争，试图提升他们的技艺。许多人说这是关于与AI合作而不是竞争。尽管如此，我认为这种AI不仅仅是关于图片或文本。它真正关于的是工具、函数、动作。这就是为什么在语义内核中我们说插件、规划器、角色。我听说现在人们说大型动作模型而不是仅仅说大型语言模型，因为大型动作模型假设你正在使用函数、插件、函数调用。我认为AI的这种动词方面将会解锁比我们想象的更多的价值。'
- en: '**A.G.**: Yeah, because it’s the interaction with tools. And in general, people
    are worried about AI replacing some basic functions of society. But some people
    are skipping the part where generative AI can be the interface to interact with
    very complex functions like designing 3D or analyzing SQL databases. So that draws
    the sign as an interface. Models and tools like Semantic Kernel.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，因为这是与工具的交互。一般来说，人们担心AI会取代社会的一些基本功能。但有些人跳过了生成AI可以作为与非常复杂功能（如设计3D或分析SQL数据库）交互的界面的部分。所以它作为一个界面。像语义内核这样的模型和工具。'
- en: '**J.M.**: Well, I think plug-ins are so powerful. Whether you call them functions
    or tools or whatever you want to call them, when you integrate them with a large
    language model, of course, you get the kind of planning capability. And that’s
    what we saw with Semantic Kernel. When you use GPT-4, you give it plug-ins that
    can plan. And once it can plan, it’s basically writing code that you could never
    have written. It writes code on the fly, basically. From a design perspective,
    a lot of time has been spent making the perfect user experience. That’s something
    very hard to do. We’ll build a journey to take you step by step through it. In
    reality, though, with function calling, you don’t need a journey. You just say,
    I want to do this, and it’s done. You didn’t have to have a user interface. That’s
    why you hear people calling it sort of like a zero UI era, where you don’t have
    a journey, you teleport to the goal.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 嗯，我认为插件非常强大。无论你称它们为函数、工具还是你想叫的任何名字，当你将它们与大型语言模型集成时，当然，你会获得某种规划能力。这正是我们在语义内核中看到的情况。当你使用GPT-4时，你给它提供可以规划的插件。一旦它能够规划，它基本上就是在编写你永远无法编写的代码。它即兴编写代码。从设计角度来看，我们投入了大量时间来打造完美的用户体验。这是一件非常困难的事情。我们将为您构建一个旅程，逐步引导您了解整个过程。然而，在现实中，通过函数调用，你不需要这样的旅程。你只需说，我想做这个，它就完成了。你不需要用户界面。这就是为什么人们称其为某种零界面时代，在那里你没有旅程，你直接传送到目标。'
- en: '**A.G.**: I love that notion because from my perspective with the book, I think
    that function calling and the planning part was the most difficult part to explain,
    to be honest.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我喜欢这个概念，因为从我的角度来看，我认为函数调用和规划部分是书中最难解释的部分，坦白说。'
- en: '**J.M.**: It is. It’s so hard. It’s hard because if you’re a developer right
    now, you’re just too busy shipping regular code. You’re tired at the end of the
    week. You know, it’s a weekend, you want to take a break, and like, what, this
    new thing, what, embeddings? What, you’ve got to do language model understanding,
    testing, what, these are all new tools. You know, Python may not be your thing
    you do every day too. It’s like, oh, I don’t want to, I played with Python, whatever.
    And so that’s why we’re trying to make it easier for enterprise devs who live
    in .NET or Java or boring languages. So I tell people, Semantic Kernel is for
    boring AI people.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 是的。这太难了。难就难在如果你现在是一名开发者，你只是太忙于发布常规代码。你到周末的时候已经筋疲力尽。你知道，周末你想休息一下，然后，这个新东西，什么，嵌入？什么，你必须做语言模型的理解和测试，什么，这些都是新工具。你知道，Python可能不是你每天都会做的事情。就像，哦，我不想，我玩过Python，随便吧。这就是为什么我们试图让那些生活在.NET或Java或无聊语言中的企业开发者更容易上手。所以我告诉人们，语义内核是为无聊的AI人而设计的。'
- en: '**A.G.**: Boring AI people. That’s such good marketing.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 无聊的AI人。这真是一个好的营销点子。'
- en: '**J.M.:** Well, it’s because enterprise likes “boring.” I mean, we also have
    a Python branch, but I find that the Python stuff is so advanced that actually
    integrating into an enterprise, it’s not so easy because it’s a different developer.
    An app dev is more about shipping “real code.” So we need an easier way to do
    that. That’s why Semantic Kernel exists.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 嗯，这是因为企业喜欢“无聊”。我的意思是，我们也有一个Python分支，但我发现Python的东西太先进了，实际上集成到企业中并不容易，因为它是不同的开发者。应用开发者更关注的是发布“真正的代码”。所以我们需要一个更简单的方式来做到这一点。这就是为什么存在语义内核的原因。'
- en: '**A.G.**: That’s very smart positioning. And how can you define Semantic Kernel?
    You have explained the plug-ins, the personas, but if you think of Semantic Kernel
    as a thing, today and in the future, if we’re able to get a sneak peek here, what’s
    your vision? How is it helping companies?'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这是一个非常聪明的定位。那么你怎么定义语义内核？你已经解释了插件和角色，但如果把语义内核看作一个东西，今天和未来，如果我们能在这里提前一瞥，你的愿景是什么？它是如何帮助公司的？'
- en: '**J.M.**: Well, you know, I think the biggest way it helps companies is it
    helps you be boring because the latest thing is the latest thing, but the problem
    with the latest thing is it just got new today. And so you’re just so distracted.
    Like, what do I do? Oh my gosh, it changes every day. So Semantic Kernel is good
    insurance for building on a middleware layer. When the lower parts change, it’s
    easy to adapt it at the middleware level. So it’s like insurance for the high
    speed of AI change. And it’s grounded in the plug-in because the plug-ins are
    where function calling becomes valuable. We have so many ways to do plug-ins with
    native code or native plus semantic code, you know, pick your own language. And
    the planners are designed to not just leverage the plug-ins to call them automatically,
    but to generate a script basically that you can read yourself, not a Python program,
    but a handlebars-formatted plan. We found many enterprises are happy that AI generated
    the plan and they want to freeze the plan because they know it works. They don’t
    need it to invent something new. So frozen plans. And we’re all talking about
    agents now. So it also incorporates agents.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 嗯，你知道，我认为它对公司最大的帮助是它帮助你变得无聊，因为最新的事物就是最新的事物，但最新事物的问题是它今天才刚刚出现。所以你很容易分心。比如，我该怎么办？哦，我的天，它每天都在变化。所以语义内核是构建在中间件层上的好保险。当底层发生变化时，在中间件级别上适应它很容易。所以它就像是AI变化高速的保险。它基于插件，因为插件是功能调用的价值所在。我们有多种方式来做插件，无论是原生代码还是原生加语义代码，你知道，选择你自己的语言。规划者被设计成不仅利用插件自动调用它们，而且生成一个你可以自己阅读的脚本，不是Python程序，而是一个handlebars格式的计划。我们发现许多企业很高兴AI生成了计划，并且他们想冻结计划，因为他们知道它有效。他们不需要发明新的东西。所以有冻结计划。现在我们都在谈论代理。所以它也包含了代理。'
- en: '**A.G.**: Agents, we’re talking about the difference with someone I cannot
    disclose right now, the difference between agents and copilots.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我们在谈论的是与我现在不能透露的人的区别，即代理和副驾驶之间的区别。'
- en: '**J.M.**: I don’t know if I could get into that conversation. It’s very meta,
    I believe.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 我不知道我能否进入那个对话。我认为这非常具有元属性。'
- en: '**A.G.**: It’s very meta. I think it’s a matter of the audience. The people
    talking about agents are probably a more developer-oriented audience.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这非常具有元属性。我认为这是一个关于受众的问题。谈论代理的人可能是一个更面向开发者的受众。'
- en: '**J.M.**: Yes. Good point. Well, if you remember the shift to object oriented
    programming, I remember that being a radical idea. It’s like, how do you do that?
    I’m so used to programming in this linear, compartmentalized way. Object? What’s
    an object? The number one thing you learn in object oriented programming is don’t
    make everything into an object. I think agent oriented programming also, sometimes
    agents are useful. Sometimes they’re not. It’s just a new pattern, I believe.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 是的。很好的观点。嗯，如果你记得面向对象编程的转型，我记得那是一个激进的观念。就像是，你怎么做到这一点？我习惯了以这种线性、分区的编程方式。对象？什么是对象？面向对象编程的第一件事就是不要把所有东西都变成对象。我认为面向代理编程也是如此，有时代理是有用的，有时则不然。这仅仅是一个新的模式，我相信。'
- en: '**A.G.**: Yes, totally. I like that example because I was born during the object
    oriented era. And I could see how the previous era was like, this doesn’t make
    sense. You’re doing this in a linear way when you need to create relations between
    objects.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，完全同意。我喜欢这个例子，因为我出生在面向对象的时代。我可以看到前一个时代是怎样的，这没有意义。当你需要创建对象之间的关系时，你以线性方式做这件事。'
- en: '**J.M.**: And you remember, you suddenly make everything into an object and
    then you can’t understand it anymore. You create some kind of compromise. I think
    agents are a new way of improving the output of models through iteration, through
    feedback loops. It’s a more clever way to do prompting. It’s more compartmentalized.
    But sometimes if you need a linear workflow, that might be what your application
    requires. In that case, you don’t need agents. In that case.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 你还记得，你突然把所有东西都变成对象，然后你就无法理解它们了。你创造了一些妥协。我认为代理是一种通过迭代、通过反馈循环来提高模型输出的新方法。这是一种更聪明的提示方法。它更加分区化。但有时如果你需要一个线性工作流程，那可能就是你的应用程序所需。在这种情况下，你不需要代理。在这种情况下。'
- en: '**A.G.**: Interesting. And from an Azure OpenAI perspective and any kind of
    generative AI in Azure, how do you see that connection with Semantic Kernel? And
    how do you see in general the role of orchestration? Like in Copilot, we’re talking
    about Prometheus and other orchestration engines. How do you make sense of this?
    There are so many things in this area.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 很有趣。从Azure OpenAI的角度来看，以及Azure中的任何生成式AI，你怎么看待它与语义内核的联系？你一般如何看待编排的角色？比如在Copilot中，我们谈论的是Prometheus和其他编排引擎。你怎么理解这一点？这个领域有太多东西了。'
- en: '**J.M.**: Well, you know, there’s people who want to call the model directly,
    call the APIs. I’m sure you’ve seen things like [Ollama](https://oreil.ly/Eyl-u)
    or [LM Studio](https://oreil.ly/qQBmG), they’re all adapting to the OpenAI API
    specification. I kind of feel like OpenAI has become a kind of interfacing body.
    And because Azure OpenAI is super tight, a fast follow, I think anyone in that
    ecosystem gets to take advantage of that. And then you may want to orchestrate
    directly to talk to the API, or you want to talk in a layer. And a layer is like
    clothing. There’s all kinds of brands of clothing, basically. And the particular
    brand of clothing that is Symantec Kernel is plug-ins first. And then the plug-ins
    are the foundation. And the neat thing is planners are also plug-ins, and also
    our agents, personas, are plug-ins too. We say we’re plug-ins all the way down.
    So we’re very boring.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 嗯，你知道，有些人想直接调用模型，调用API。我确信你见过像[Ollama](https://oreil.ly/Eyl-u)或[LM
    Studio](https://oreil.ly/qQBmG)这样的东西，它们都在适应OpenAI API规范。我有点感觉OpenAI已经成为了一种接口机构。由于Azure
    OpenAI非常紧密，快速跟进，我认为生态系统中的任何人都可以利用这一点。然后你可能想直接编排以与API通信，或者你想要在某一层进行通信。一层就像衣服。基本上有各种各样的品牌。特别品牌是Symantec
    Kernel，以插件为首要。然后插件是基础。而且很酷的是，规划者也是插件，我们的代理、角色也是插件。我们说我们从头到尾都是插件。所以我们非常无聊。'
- en: '**A.G.**: This is like a multilayer architecture in which they’re communicating,
    and then you have different options to communicate with this service and another.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这是一个多层架构，其中它们在通信，然后你有不同的选项来与这个服务和其他服务通信。'
- en: '**J.M.**: Everything is just code. We’re not trying to make a magical spell
    that you’re not programming anymore. You’re still programming. And everything
    is a computational unit. It’s a plug-in. And you can make plans that are also
    plug-ins, or you can make agents that are plug-ins. And it’s just like connecting
    those dots.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 所有的东西都是代码。我们并不是试图创造一个魔法咒语，让你不再编程。你仍然在编程。而且所有东西都是一个计算单元。它是一个插件。你可以制定也是插件的计划，或者你可以创建也是插件的代理。这就像连接那些点一样。'
- en: '**A.G.:** Amazing. Let me ask this question. I’ve got several people asking
    the same question. And you can tell me if this is not a good question, but what’s
    the difference, convergence, compatibility between Semantic Kernel and LangChain?'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.:** 太棒了。让我问一下这个问题。有好几个人问我同样的问题。你可以告诉我这不是一个好问题，但语义内核和LangChain之间的区别、收敛性和兼容性是什么？'
- en: '**J.M.**: Very common question. Yeah. LangChain and Semantic Kernel are both
    open source projects. Open source projects support each other. All I have is good
    things to say about LangChain and also [Harrison](https://oreil.ly/ttbiM), that
    community. I also love [LlamaIndex](https://oreil.ly/QZtYV), which I think of
    as like a sister or cousin project, which I adore. And the difference really is
    in the fact that LangChain is running the fastest with the latest and greatest
    AI ideas. Semantic Kernel, that’s not the role. The role of Semantic Kernel is
    to enable enterprises to leverage this large language model or large action model
    revolution. And they’re gonna tend to move slower and need more sense of safety
    and security. So Semantic Kernel is architected with very few package dependencies,
    if at all. It’s designed to be loved by the CISO (Chief Information Security Officer).
    And it’s designed to be loved by procurement because it’s free, but it’s also
    part of the Microsoft world.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 这是一个非常常见的问题。是的。LangChain和Semantic Kernel都是开源项目。开源项目相互支持。我对LangChain和[Harrison](https://oreil.ly/ttbiM)社区都有很多好评。我也非常喜欢[LlamaIndex](https://oreil.ly/QZtYV)，我认为它就像是一个姐妹或堂兄弟项目，我非常喜欢。真正的区别在于LangChain正在运行最新的AI想法，而Semantic
    Kernel并不扮演这个角色。Semantic Kernel的作用是使企业能够利用大型语言模型或大型动作模型的革命。他们可能会走得慢一些，需要更多的安全感和保障。因此，Semantic
    Kernel的设计非常少有包依赖，如果有的话。它被设计成CISO（首席信息安全官）会喜欢的。它也被设计成采购部门会喜欢的，因为它是免费的，但也是微软世界的一部分。'
- en: '**A.G.:** Yeah. Which makes total sense. I think both of them are necessary.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.:** 是的。这完全合理。我认为两者都是必要的。'
- en: '**J.M.:** Yeah, yeah, yeah. I mean, like I say that if you want to drive a
    Tesla Model S Plaid, then LangChain’s fun. If you wanna drive a Toyota Camry XLE
    Hybrid, then you’ve got Semantic Kernel. And the neat thing is with a Python branch,
    everything is becoming 1.0\. .NET became 1.0 first. We’re aligning Python and
    Java releases. If you’re a Python team, usually a data science–oriented team,
    and you use Semantic Kernel, all of your YAML files and everything easily shift
    to the App Dev. So that’s the advantage.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.:** 是的，是的，是的。我的意思是，就像我说的，如果你想驾驶特斯拉Model S Plaid，那么LangChain很有趣。如果你想驾驶丰田Camry
    XLE混合动力车，那么你就有了Semantic Kernel。而且，有趣的是，随着Python分支的出现，一切都在成为1.0。.NET首先成为1.0。我们正在对齐Python和Java的发布。如果你是一个Python团队，通常是一个以数据科学为导向的团队，并且使用Semantic
    Kernel，所有你的YAML文件和一切都可以轻松转移到App Dev。所以这是优势。'
- en: '**A.G.:** And what’s the role of orchestration? I know that it’s a bit of a
    stretch, but what’s the role of orchestration to bring the proper information
    and the proper format and good timing in a timely manner for compliance? I’m located
    in Madrid, in Spain, Europe, AI Act, similar things in Canada, in the future in
    the United States. I feel like there’s a lot of potential for that layer in the
    middle to distribute information that is required at log level.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.:** 那协调的作用是什么？我知道这有点牵强，但协调在及时提供适当的信息、适当格式和良好时机以符合法规方面有什么作用？我位于西班牙的马德里，在欧洲，AI法案，加拿大也有类似的东西，未来在美国。我觉得中间层在分配日志级别所需信息方面有很大的潜力。'
- en: '**J.M.**: Well, I mean, that’s a nice thing about Semantic Kernel having been
    built 1.0 first with .NET C#, because it has all the logging everywhere. It’s
    got all the Azure kind of security, safeness built into how it’s architected.
    You often hear people who love how Semantic Kernel has been architected, because
    it’s been Microsoft architected. If you haven’t seen, or if any of your readers
    or viewers haven’t seen how we wrap plug-ins, you’re gonna be pleasantly surprised
    because it’s so little code to enable function calling of complex plug-ins. And
    you’re thinking, wait, this is all the code I need? And you’re like, yep, we can
    get going. People love that. And that was architected by [Stephen Toub](https://oreil.ly/i7kqx),
    one of the .NET architect legends. I remember when he said it’s gotta be this
    way. And we’re like, OK. And he said wow, that’s really good. It’s really good.
    But anyone who sees it, they’re kind of, where’s the code? It’s all there because
    it’s using the abstractions available already of an enterprise-class language.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 嗯，我认为 Semantic Kernel 首先以 .NET C# 构建 1.0 版本是个好事，因为它在所有地方都有日志记录。它将
    Azure 类型的安全性和安全性构建到其架构中。你经常听到人们喜欢 Semantic Kernel 的架构，因为它是由微软设计的。如果你还没有看到，或者如果你的读者或观众还没有看到我们如何封装插件，你会感到非常惊喜，因为启用复杂插件的功能调用只需要很少的代码。你可能会想，等等，这就是我需要的所有代码吗？然后你会说，是的，我们可以开始了。人们都喜欢这样。这是由
    [Stephen Toub](https://oreil.ly/i7kqx)，一位 .NET 架构传奇人物设计的。我记得他曾经说过，它必须是这样。我们就说，好吧。然后他说，哇，这真的很棒。真的很棒。但任何看到它的人都会觉得，代码在哪里？因为它是使用企业级语言已经可用的抽象来实现的，所以所有的代码都在那里。'
- en: '**A.G.:** Learning is the goal of this interview and I know that you are a
    humble person because you are not talking that much about your activities and
    your background, but you’re creating learning resources, which I personally love.
    That’s why I’m writing this book. That’s why we’re creating all this stuff. And
    I have two examples, LinkedIn Learning and DeepLearning.AI. What are they about,
    for people to continue learning?'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.:** 学习是这个采访的目标，我知道你是一个谦逊的人，因为你没有太多谈论你的活动和背景，但你正在创建学习资源，这我个人非常喜欢。这就是我写这本书的原因。这就是我们创建所有这些内容的原因。我有两个例子，LinkedIn
    Learning 和 DeepLearning.AI。它们是关于什么的，让人们继续学习？'
- en: '**J.M.:** Oh, thanks. Let’s see, I have a [LinkedIn Learning course](https://oreil.ly/TXE5e).
    Now I have [multiple](https://oreil.ly/G6grC), including one for [AI engineering
    for leadership](https://oreil.ly/z-nXp). Because AI engineering is about leading
    change. And most developers love to be introverts, but they sometimes become managers
    and they have to lead people. And this AI stuff is kind of scary for people. It’s
    also very technical to understand. I have a whole new course themed on the kitchen.
    Also, Microsoft Dev Channel has a new show we’ve made, called [Mr. Maeda’s Cozy
    AI Kitchen](https://oreil.ly/N9oCg). Yes, we cook AI every two weeks in my kitchen.
    And we have guests and they try the AI.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.:** 哦，谢谢。让我看看，我有一个 [LinkedIn Learning 课程](https://oreil.ly/TXE5e)。现在我有很多，包括一个关于
    [AI 领导工程](https://oreil.ly/z-nXp) 的课程。因为 AI 工程关乎引领变革。大多数开发者喜欢内向，但他们有时会变成管理者，不得不领导他人。对人们来说，这些
    AI 东西有点可怕。理解起来也非常技术性。我还有一个全新的以厨房为主题的课程。此外，微软开发者频道有一个我们制作的新节目，叫做 [梅达先生的温馨 AI 厨房](https://oreil.ly/N9oCg)。是的，每两周在我的厨房里我们都会烹饪
    AI。我们还有嘉宾，他们尝试使用 AI。'
- en: 'And the [DeepLearning.AI course](https://oreil.ly/nxusL) was an opportunity
    to talk with [Andrew Ng](https://oreil.ly/CDLm-), who I think is one of the great
    minds of our time. And he gave this talk at the Wall Street Journal CIO Summit,
    where someone asked him if this is going to change how people’s jobs are, and
    all the fear around it. And he said the best thing I’ve ever heard anyone say:
    you should think of AI as automating tasks, not jobs. Any given job has many tasks.
    And if there are a lot of tasks that you don’t like to do as a human, that aren’t
    of high value as a human, then automating them with AI makes a lot of sense and
    can improve your job. Whether it’s a gnarly testing function you’re writing where
    you’re thinking, “Oh, that’s going to be such a pain with all the cases,” and
    boom! It’s there. Or something like a shell script that is always different in
    every language with a little subtlety. You just say I need a shell script. Just
    a half an hour ago, I did that. I need a shell script. And then find out, oh,
    that was easy. And you debug it yourself too. So, it’s taking tasks that I don’t
    like to do and having it be done for me.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 而[DeepLearning.AI课程](https://oreil.ly/nxusL)是一个与[安德鲁·吴](https://oreil.ly/CDLm-)交谈的机会，我认为他是我们这个时代的伟大思想家之一。他在《华尔街日报》CIO峰会上发表了演讲，有人问他这会不会改变人们的就业方式，以及围绕这一点的所有恐惧。他说的是我听过的最好的一句话：你应该把AI看作是自动化任务，而不是工作。任何一项工作都有许多任务。如果你有很多作为人类不喜欢做、对人类来说价值不高的任务，那么用AI自动化它们就很有意义，并且可以提高你的工作效率。无论是你正在编写的复杂的测试功能，你想着，“哦，所有这些情况都会很痛苦，”然后砰！它就出现了。或者像shell脚本那样，在每种语言中都有细微的差别。你只需要说，我需要一个shell脚本。就在半小时前，我就做了这件事。我需要一个shell脚本。然后发现，哦，这很简单。你自己也调试了它。所以，它是在为我完成我不喜欢做的任务。
- en: '**A.G.:** I can really see that. Like, right after this discussion, I’m creating
    the transcription and I’m creating the action points and the summary of the most
    important information. No one likes to do that and that’s maybe 10% of my job
    because we are having so many meetings. I just want to go back to your design
    background. Looking at this adoption pattern where companies and people are using
    generative AI, LLMs, and they are learning how to evaluate them, how to use them,
    how to orchestrate, from a design point of view, do you see anything happening
    in the near future that will be radically different from a design UX, UI point
    of view?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我真的能理解这一点。就像，就在这次讨论之后，我正在创建转录、创建行动要点和总结最重要的信息。没有人喜欢做这件事，这可能占我工作的10%，因为我们开了很多会议。我只是想回到你的设计背景。看着这种采用模式，公司和个人正在使用生成式AI、LLM，他们正在学习如何评估它们、如何使用它们、如何从设计角度进行编排，从UX、UI设计的角度来看，你认为在不久的将来会发生什么将彻底不同的事情吗？'
- en: '**J.M.**: Yeah. Well, I definitely think that this zero UI revolution is happening
    where you don’t need a lot of user interface, user experience, psychology when
    the machine can discover your intent and execute the task. There’s this thing
    called [Jobs to Be Done by Clayton Christensen](https://oreil.ly/Aq12m). It’s
    almost as if we create user experiences to get a job done, but if the machine
    knows what job you want to get done and you tell it what to do and it does it,
    did you really need any experience in the first place?'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 是的。嗯，我确实认为这场零界面革命正在发生，当你不需要很多用户界面、用户体验、心理学知识时，因为机器可以发现你的意图并执行任务。有件事叫做[克雷顿·克里斯滕森的“待完成工作”](https://oreil.ly/Aq12m)。几乎就像我们创建用户体验是为了完成一项工作，但如果机器知道你想要完成的工作，你告诉它该做什么，然后它就去做，你真的需要任何经验吗？'
- en: '**A.G.**: That’s amazing, that is funny. Just today I was getting that question
    from a student about how to define the jobs to be done for artificial intelligence.
    I was like, I don’t know, I don’t know.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 太神奇了，这很有趣。就在今天，我有一个学生问我如何定义人工智能的“待完成工作”。我想，我不知道，我不知道。'
- en: '**J.M.**: Yeah, because with tool calling and function calling, you give it,
    like in Semantic Kernel, just last week I had this weird moment where I gave it
    five plug-ins I wrote, and then I didn’t have to construct the logic of how to
    make them all work. It was actually too hard for me to write the logic, and the
    planner just constructed the flow the way that I couldn’t write.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 是的，因为有了工具调用和函数调用，你给它，就像在Semantic Kernel中，上周我给了它我写的五个插件，然后我就不必构建它们如何协同工作的逻辑了。实际上，对我来说写逻辑太难了，规划者就按照我无法编写的方式构建了流程。'
- en: '**A.G.**: Wow, that’s incredible. And just to finish, since you mentioned the
    kitchen, if you had to choose, did you have one recipe that you say, this is something
    that someone needs to learn for the next stage of adoption?'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 太棒了。最后，既然你提到了厨房，如果你必须选择，有没有一个食谱，你说这是一个人在下一个采用阶段需要学习的东西？'
- en: '**J.M.**: Oh, good question. Yeah, I tell everyone that in the kitchen that
    you have to realize there’s two kinds of AI models. One AI model does completion,
    the other does similarity. And this is called the embeddings model. This is called
    the completion or chat completion model. It’s a combination of these two together
    that are making this revolution amazing. If you only have one, it’s no good. If
    you have chat completion or completion, it’s going to be ungrounded. It’ll say
    things that make no sense. If you have similarity models, which are basically
    search, you can find something, but you can’t synthesize. The two together make
    an incredible pair. It’s like one is butter and one is flour. Like together you
    can make great cookies. And this is the core recipe for everything with large
    language model AIs. You can create function calling models, you can create sophisticated
    chat, you can create supply chain automation, all from these two models. But one
    model alone is not good enough, you need the two together.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**J.M.**: 哦，好问题。是的，我告诉每个人在厨房里你必须意识到有两种AI模型。一种AI模型负责完成，另一种负责相似度。这被称为嵌入模型。这被称为完成或聊天完成模型。这两个模型的结合使得这次革命变得惊人。如果你只有一个，那就没什么用了。如果你有聊天完成或完成模型，它将变得没有根据。它会说出一些没有意义的话。如果你有相似度模型，基本上是搜索，你可以找到一些东西，但你不能综合。这两个模型结合起来是一对不可思议的搭档。就像一个是黄油，一个是面粉。就像在一起你可以做出美味的饼干。这是大型语言模型AI的一切核心配方。你可以创建功能调用模型，你可以创建复杂的聊天，你可以创建供应链自动化，所有这些都可以从这两个模型中实现。但一个模型单独是不够的，你需要两个一起。'
- en: '**A.G.**: You’re right. And I think this in the human comparison would be something
    like IQ and EQ together. Like the ability to remember information, traditional
    intelligence, but that ability to explain in a proper way that is adapting to
    the audience. Yes, I love it.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你说得对。我认为在人类比较中，这就像IQ和EQ的结合。就像记住信息的能力，传统智力，但那种以适应听众的方式正确解释的能力。是的，我喜欢它。'
- en: 'Sarah Bird: Responsible AI for LLMs and Generative AI'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Sarah Bird: 负责任的AI和生成式AI'
- en: '**A.G.**: Do you want to start by explaining your role in the organization
    and what you are doing at Microsoft?'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你想先解释一下你在组织中的角色以及你在微软的工作内容吗？'
- en: '![](assets/aoas_07in04.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_07in04.png)'
- en: '**S.B.**: Yeah. I’m Microsoft’s chief product officer of responsible AI. What
    that means is my team is responsible for figuring out how we take a new AI technology
    and ensure that it’s developed responsibly. In the case of a lot of the AI we
    build at Microsoft, we’re figuring that out ourselves. If we’re partnering with
    other organizations such as OpenAI, then we work with them to ensure that the
    right things are happening as they develop the AI. But then it’s not just about
    the model, it’s really about how we ship a complete application safely. We take
    that new AI technology and look at what the entire approach we need to follow
    is to use this technology effectively.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的。我是微软负责任AI的首席产品官。这意味着我的团队负责找出我们如何将新的AI技术应用于实践，并确保其负责任地开发。在微软构建的许多AI中，我们自己正在解决这个问题。如果我们与其他组织如OpenAI合作，那么我们与他们合作，确保他们在开发AI时发生正确的事情。但那不仅仅是关于模型，真正重要的是我们如何安全地发布一个完整的应用程序。我们采用这项新的AI技术，并审视我们需要遵循的整个方法，以有效地使用这项技术。'
- en: For example, for GPT-4, an exciting new piece of technology, the first place
    we shipped this was in Microsoft Copilot, originally called Bing Chat. Our team
    went in and basically led the responsible AI (RAI) development of that. We developed
    new mitigations, we developed new testing tools, we developed new techniques for
    red teaming. All that we learned, we built into the Azure AI platform and that
    enables it to power all of the AI at Microsoft as well as enable our customers
    who are building their own AI applications to use the same best practices. That’s
    the mission of the team, figuring out how we really put AI into practice, and
    then ensuring that we’re using those best practices across Microsoft and empowering
    others to do that as well.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于GPT-4这个令人兴奋的新技术，我们第一次发布这个技术是在微软Copilot中，最初被称为Bing Chat。我们的团队进去并基本上领导了负责任AI（RAI）的开发。我们开发了新的缓解措施，开发了新的测试工具，开发了新的红队技术。我们学到的一切，我们都构建到了Azure
    AI平台上，这使得它能够为微软的所有AI提供动力，同时也使我们的客户在构建自己的AI应用时能够使用相同的最佳实践。这就是团队的任务，找出我们如何真正将AI付诸实践，并确保我们在微软内部使用这些最佳实践，并赋予他人这样做的能力。
- en: '**A.G.**: That’s a lovely mission. And it’s not a new one. There’s a journey
    at Microsoft with responsible AI even before the GPT models.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这是一个很棒的任务。而且它不是一个新的任务。在微软，负责任AI的旅程甚至早于GPT模型。'
- en: '**S.B.**: Yeah, it’s something that we’ve actually been doing a long time.
    I was fortunate to be part of founding the first research group in responsible
    AI in Microsoft, and this is the [FATE group](https://oreil.ly/hmo69), back in
    2015\. This is something we’ve been doing for almost 10 years. But we’ve come
    a long way during that time. It went from just some ideas in research to, the
    next thing that we founded was the Office of Responsible AI, which was really
    starting to set what is the policy or the standard we want to follow. But even
    creating a policy without much experience in implementation is really hard. A
    lot of the journey since then has been figuring out how we really do this, and
    iterating between policy, engineering, and research to really mature our practices,
    tools, and technology. But even with generative AI, for a lot of people, the first
    moment they were aware of it was ChatGPT. But actually, well before ChatGPT came
    out, Microsoft shipped GitHub Copilot, which was really the first generative AI
    application that we produced at scale. A lot of the things that we used in Bing
    Chat and other applications were actually originally developed for GitHub Copilot,
    because that was the first real-time generative AI application. [Azure AI Content
    Safety](https://oreil.ly/uB6d-), the safety system that we use in our gen AI applications
    today, was actually first developed for GitHub Copilot.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，这是我们实际上已经做了很长时间的事情。我很幸运能成为微软第一个负责任AI研究团队的创始人之一，那就是[FATE团队](https://oreil.ly/hmo69)，那是2015年。这是我们几乎已经做了10年的事情。但在这段时间里，我们已经走了很长的路。它从研究中的几个想法发展到，我们接下来成立的是负责任AI办公室，这实际上开始设定我们想要遵循的政策或标准。但即使在没有太多实施经验的情况下制定政策也是非常困难的。从那时起，我们的大部分旅程都是
    figuring out how we really do this，在政策、工程和研究之间迭代，以真正成熟我们的实践、工具和技术。但即使有了生成式AI，对于很多人来说，他们第一次意识到它的那一刻是ChatGPT。但实际上，在ChatGPT推出之前，微软就已经发布了GitHub
    Copilot，这是我们大规模生产的第一个生成式AI应用。我们在Bing Chat和其他应用中使用的大多数东西实际上最初是为GitHub Copilot开发的，因为那是第一个真正的实时生成式AI应用。[Azure
    AI内容安全](https://oreil.ly/uB6d-)，我们今天在gen AI应用中使用的安全系统，最初也是为GitHub Copilot开发的。'
- en: '**A.G.**: It’s funny because a lot of people forget, including ourselves, when
    we are talking about different Copilots, that GitHub Copilot is actually the patient
    zero, the first one and the original one.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这很有趣，因为很多人，包括我们自己，在谈论不同的Copilot时，往往会忘记，GitHub Copilot实际上是零号病人，第一个，也是最初的那个。'
- en: '**S.B.**: It was eye-opening for us working on it because the GPT technology
    was exciting, but it felt like it could still be, it was still a toy. Then when
    the GitHub team really showed the early prototypes of GitHub Copilot, we were
    like, wow, this is real, this is really exciting. But at that time, we weren’t
    sure, is it just this one application? How narrow is the technology? How many
    more GitHub Copilots will there be? Then when the next wave of it came out, going
    from GPT-3 to GPT-4, GPT-4 was when we were like…oh, this is not narrow anymore.
    There will be many more Copilots that are possible. That jump in the technology,
    I think, really unlocked many more applications, but GitHub Copilot really showed
    the way first.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 对于我们这些参与其中的人来说，这是一个令人耳目一新的经历，因为GPT技术令人兴奋，但它仍然感觉像是一个玩具。然后当GitHub团队真正展示了GitHub
    Copilot的早期原型时，我们想，哇，这是真的，这真的很令人兴奋。但当时我们还不确定，这只是这一个应用吗？这项技术的应用范围有多窄？还会有多少个GitHub
    Copilot呢？然后当下一波技术推出，从GPT-3到GPT-4时，当我们看到GPT-4时，我们想……哦，这不再是狭窄的领域了。会有更多可能的Copilot。这次技术的飞跃，我认为，真正解锁了更多应用，但GitHub
    Copilot是第一个展示这条道路的。'
- en: '**A.G.**: Yes, and I think from an RAI perspective, the idea or the adoption
    pattern of having a regular completion, something that is a singular interaction
    with the machine, and then moving to something that is chat-related, with memory,
    with all the benefits and all the considerations. I think that that’s probably
    the evolution of that learning, the engineering and policy duet that you’re talking
    about.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，我认为从RAI（人工智能伦理）的角度来看，有一个定期的完成想法，这是一种与机器的单次交互，然后转向与聊天相关的、具有记忆功能的东西，以及所有的好处和考虑。我认为这可能就是你所谈论的学习、工程和政策二重奏的演变。'
- en: '**S.B.**: Yes, certainly. There are nuances in the GitHub Copilot application.
    I actually really loved the design of it because it is a paradigm people are already
    familiar with, with the autosuggest. We’re already comfortable with the idea of…hey,
    the suggestion might not be perfect, but if I like it, I can keep it and I can
    still go and edit it. We all know that it makes us go faster in natural language.
    But then knowing that actually was going to be effective for code, that wasn’t
    obvious. But we did have to look at both with that natural language risk, hateful
    content, violent content, things like that, and also code risks, like the ability
    to produce security vulnerabilities or known weaknesses in the code. We had to
    address both of those dimensions. Because the application is only useful if it
    goes faster than people actually can type, there were really extreme latency requirements.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 当然，GitHub Copilot应用中确实有一些细微差别。我实际上非常喜欢它的设计，因为它是一个人们已经熟悉的范例，有自动建议功能。我们已经习惯了这样的想法……嘿，建议可能并不完美，但如果我喜欢，我可以保留它，我仍然可以去编辑它。我们都知道这使我们用自然语言更快。但后来知道这实际上对代码也有效，这一点并不明显。但我们确实不得不考虑这两个方面，即自然语言的风险，如仇恨内容、暴力内容等，以及代码风险，如产生安全漏洞或代码中的已知弱点。我们必须解决这两个维度。因为只有当应用比人们实际打字速度快时，它才有用，所以我们确实有非常极端的延迟要求。'
- en: Now, the transition to Bing and Copilot since then in the chat applications,
    as you said, adds this multiturn dimension. Now, if you’re trying to look at an
    interaction and say “Hey, did the AI system do the right thing?” you actually
    have to score a multiturn natural language conversation, and that’s much more
    challenging. There’s a much bigger diversity of topics and types of interaction
    that the system is going to look at. We started with a strong foundation with
    GitHub Copilot, but certainly with the power of GPT-4 and the power of the search
    engine, and the breadth of things that we wanted to cover there, we really had
    to look much broader. So that’s when you start having conversations about things
    like hallucination, because accuracy really matters, or missing disinformation
    because the search engine is so connected with information integrity. And so the
    aperture really broadened with that application.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从那时起，在聊天应用中过渡到Bing和Copilot，正如你所说，增加了多轮对话的维度。现在，如果你试图观察一个交互并说“嘿，AI系统做得对吗？”你实际上必须对多轮自然语言对话进行评分，这要困难得多。系统将要考虑的话题和交互类型更加多样化。我们从GitHub
    Copilot的坚实基础开始，但当然，有了GPT-4的力量、搜索引擎的力量以及我们想要涵盖的广泛内容，我们真的必须考虑得更广泛。所以，这就是你开始讨论诸如幻觉等问题的时候，因为准确性真的很重要，或者因为搜索引擎与信息完整性紧密相连，可能会遗漏错误信息。因此，随着该应用的发展，视野确实变得更加宽广。
- en: '**A.G.**: It must be very interesting, that moment that we realized we actually
    need new metrics, because you have mentioned the performance, and we had the ROC
    curve, the F1 and F2 scores for classification topics and stuff. And then we arrived
    there and we said, OK, we have a new kind of application that is based on something
    called generative AI. We need to test the performance of this. We have the metrics
    from traditional linguistics like BLUE and ROUGE. How was that? At the AI level,
    like what do we do now?'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 那一刻我们必须觉得非常有趣，因为我们意识到我们实际上需要新的度量标准，因为你提到了性能，我们有了ROC曲线、F1和F2分数用于分类主题等。然后我们到达那里，我们说，好吧，我们有一种新的应用，它基于一种叫做生成式AI的东西。我们需要测试这个应用的性能。我们有来自传统语言学的度量标准，比如BLUE和ROUGE。那怎么样？在AI层面，我们现在该怎么做？'
- en: '**S.B.**: You know, the thing is we always knew we needed metrics to actually
    address these risks, right? It’s very hard to understand if mitigation is effective
    or if a risk is present without actual metrics. And one of the big challenges
    in RAI for a long time is that these metrics were really difficult to get. For
    example, let’s go back to saying, “How do I rate a multiturn conversation?” So
    if you’re looking at doing that for “hate” (as an AI content safety metric), our
    guidelines internally are more than 20 pages long to kind of score that conversation,
    and they’re built for expert linguists. And so that meant that we can measure
    for response by risk, but only very infrequently as sort of the outer loop. OK,
    an application is basically ready to ship. We can run one set of tests that are
    very manual, and have the human reviewer score them. And if the results look good,
    great, we can ship it. But with that, you’re not able to really innovate in the
    inner loop and really try different things, and find which one works the best.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 你知道，问题是我们一直都知道我们需要度量标准来真正解决这些风险，对吧？如果没有实际的度量标准，很难理解缓解措施是否有效或是否存在风险。在RAI（负责任的人工智能）领域的一个大挑战是，这些度量标准一直很难获得。例如，让我们回到“如何评估多轮对话？”这个问题。如果你是看“仇恨”（作为一个AI内容安全度量标准），我们内部的指南有超过20页长，用来评估这个对话，它们是为专家语言学家设计的。这意味着我们可以按风险来衡量响应，但只能非常偶尔地作为外循环。好吧，一个应用基本上准备发货了。我们可以运行一组非常手动化的测试，由人工评审员评分。如果结果看起来不错，那就太好了，我们可以发货。但这样，你实际上无法在内循环中真正创新，真正尝试不同的事情，并找出哪一种效果最好。'
- en: Actually, one of the most memorable things for me about developing Bing Chat
    quite early, as we were using GPT-4, was realizing that it actually had the potential
    to help us automate these metrics. We were actually able to use GPT-4, with a
    lot of prompt engineering, and get it to score similar to the level of those expert
    humans. And so that meant we went from…hey, we’re gonna be able to check this
    very rarely, maybe once a month, maybe at the very end, to every single night
    when we make a change to the system, we can run the safety test overnight, look
    at the scores, and iterate. And so that unlocked just a whole new wave of responsible
    AI innovation. The technology is obviously a significant breakthrough for AI,
    but it’s also a significant breakthrough for responsible AI and safety and security
    because it’s this amazing new technology that just understands language and context
    so much more. We’ve really put that to use in our own development of AI.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，关于早期开发Bing Chat，对我来说最难忘的事情之一，当我们使用GPT-4时，是意识到它实际上有潜力帮助我们自动化这些度量标准。我们实际上能够使用GPT-4，通过大量的提示工程，让它评分达到那些专家人类水平。这意味着我们从……嘿，我们只能偶尔检查一次，可能一个月一次，或者在最末尾，到每次我们修改系统时，我们都可以在夜间运行安全测试，查看分数，并迭代。这样，就解锁了整个新的负责任AI创新浪潮。这项技术显然是AI的一个重大突破，但它也是负责任AI和安全的一个重大突破，因为它是一种理解语言和上下文如此之多的惊人新技术。我们真的在我们的AI开发中充分利用了这一点。
- en: '**A.G.**: And you have mentioned the key words like safety, security, even
    compliance, regulations, and responsible AI. Everything is converging at this
    point. Everything is going towards something that in the beginning was the ethical
    way to do things, like the willingness to do something that is good, towards something
    that is responsible, that is accountable. And I think that that’s a wonderful
    thing from a technology perspective, that organic evolution.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你提到了关键词，比如安全、安全、甚至合规、法规、以及负责任的AI。所有这些都在这个点上汇聚。所有东西都在朝着最初是道德行事方式的方向发展，比如愿意做有益的事情，朝着负责任、可问责的方向发展。我认为从技术角度来看，这是一个美妙的事情，一种有机的进化。'
- en: '**S.B.**: Yeah, I think with generative AI, one of the things that’s been exciting,
    but also challenging, frankly, is that with a lot of the responsible AI work we
    did before, just the AI developer could manage it. And everyone kind of got the
    benefit, but they didn’t need to really understand the details as long as you
    work with a great AI provider like Microsoft. You were set. With generative AI,
    we really end up needing both for safety and security to use a defense in depth
    approach where the model developer needs to do things, the safety system developer
    needs to do things, the application developer needs to look at the meta-prompt
    and the grounding information, the final application developer needs to look at
    how the human interacts. What does that UX look like?'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，我认为在生成式AI方面，令人兴奋但坦白说也具有挑战性的一点是，在我们之前进行的许多负责任AI工作中，只有AI开发者能够管理这些工作。每个人都能从中受益，但只要与像微软这样的优秀AI提供商合作，他们就不需要真正了解细节。但在生成式AI方面，我们实际上需要采取深度防御的方法来确保安全和安全，这意味着模型开发者需要做一些事情，安全系统开发者需要做一些事情，应用程序开发者需要查看元提示和基础信息，最终的应用程序开发者需要查看人类如何与之互动。这种用户体验是什么样的？'
- en: There’s so much more that needs to be done to use this technology effectively.
    And it’s not surprising, it’s a much more general-purpose, more powerful technology.
    This went from something that was really just housed in a small number of responsible
    AI experts to something that now every organization, every security professional,
    every AI developer needs to think about. It’s been really fun to see the interesting
    growth and support for the work, but also the explosion of demand means there’s
    so much more that we have to do. And that’s really exciting, but also can be challenging.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地使用这项技术，还有很多工作要做。这并不令人惊讶，它是一种用途更广泛、功能更强大的技术。这项技术已经从仅由少数负责任AI专家掌握，发展到如今每个组织、每个安全专业人士、每个AI开发者都需要考虑的问题。看到这项工作的有趣增长和支持，真是令人兴奋，但需求的激增也意味着我们还有更多的工作要做。这确实令人兴奋，但也具有挑战性。
- en: '**A.G.**: It is very exciting. And I think it’s very aligned with the kind
    of artifacts and material that the [Responsible AI Initiative at Microsoft](https://oreil.ly/tCL6L)
    is putting out there, available for organizations at both the technical and organizational
    level. I’m thinking about the [impact assessment](https://oreil.ly/bJAeg), the
    [HAX toolkit](https://oreil.ly/AtDRJ) for the interfaces. What’s your favorite?
    If you had to choose different pieces of material that are useful for organizations
    in terms of responsible AI, what would be your selection?'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这非常令人兴奋。我认为它与微软[负责任AI倡议](https://oreil.ly/tCL6L)所推出的各种工具和材料非常契合，这些工具和材料既适用于技术层面也适用于组织层面。我在想[影响评估](https://oreil.ly/bJAeg)和[HAX工具包](https://oreil.ly/AtDRJ)对于界面的应用。你最喜欢哪一个？如果你必须从对组织在负责任AI方面有用的材料中选择不同的部分，你会选择哪些？'
- en: '**S.B.**: Oh, it’s so hard. I love all the responsible AI things. But I think
    what you’ve called out is really important because it is a mix of practices, policies,
    and technologies. And you really have to look at the whole spectrum and customers
    and organizations are asking us for that. So, for example, one of the things I
    really like is that we’ve put out our [Responsible AI Standard](https://oreil.ly/j5tBY),
    which is really the guide for how we do this overall out there. So organizations
    can look at it. They can adopt something similar if that works for them. We also
    put it out there so we can get feedback. People can tell us what they think we’re
    missing, what they’re finding works, what they’re finding doesn’t work. And so
    that’s really kind of where everything starts with us. But then if you want to
    go and put that into practice, you need to first start with a process like an
    impact assessment where you’re really mapping the risk. You then need to be able
    to measure risk effectively. And so actually we just released [new safety evaluations
    for generative AI](https://oreil.ly/GcCSo), which are the tests that we run ourselves
    to really measure these risks. And that’s actually the breakthrough I was telling
    you about earlier.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**：哦，这太难了。我喜欢所有负责任的AI事物。但我认为你指出的这一点非常重要，因为它涉及实践、政策和技术的混合。你真的需要审视整个范围，客户和组织都在向我们要求这一点。例如，我非常喜欢我们发布的[负责任AI标准](https://oreil.ly/j5tBY)，这实际上是我们如何整体上做这件事的指南。组织可以查看它。如果那对他们有效，他们可以采用类似的东西。我们还发布它，以便我们能够获得反馈。人们可以告诉我们他们认为我们遗漏了什么，他们发现什么有效，什么无效。所以，实际上，一切都是从我们这里开始的。但如果你想要将其付诸实践，你首先需要从像影响评估这样的流程开始，你实际上是在映射风险。然后你需要能够有效地衡量风险。因此，我们实际上刚刚发布了[新的生成式AI安全评估](https://oreil.ly/GcCSo)，这是我们运行的自测，以真正衡量这些风险。这实际上就是我之前告诉你的突破。'
- en: And then you also need to be able to mitigate the risk. Azure AI Content Safety
    is a great way we mitigate the risk. That’s the safety system layer. The HAX toolkit
    really helps with the application, the UX layer. We’ve also put out prompt engineering
    guides and meta-prompt templates to help with the prompt layer. You really have
    to look holistically across all of these to adopt that. Another one that we get
    asked about a lot from customers is how to red team, how to do that kind of final
    expert validation. We put out [red teaming guidelines](https://oreil.ly/oDBO_),
    but we know red teams are limited resources. So we’ve just released [PyRIT](https://oreil.ly/azSbW),
    which is a tool that helps accelerate the productivity of red teamers by helping
    them get more ideas for the next thing to try, basically using AI to assist them
    the same way we’re using AI to assist many other roles now with what we’ve developed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你还需要能够减轻风险。Azure AI 内容安全是我们减轻风险的一种很好的方式。这是安全系统层。HAX 工具包在应用、用户体验层方面提供了很大帮助。我们还发布了提示工程指南和元提示模板，以帮助处理提示层。你真的需要全面地审视所有这些才能采用它们。客户经常向我们询问的另一件事是如何进行红队攻击，如何进行那种最终专家验证。我们发布了[红队指南](https://oreil.ly/oDBO_)，但我们知道红队资源有限。因此，我们刚刚发布了[PyRIT](https://oreil.ly/azSbW)，这是一个帮助红队人员通过帮助他们获得更多尝试新事物想法来提高生产力的工具，基本上是使用人工智能以与我们现在使用人工智能协助许多其他角色相同的方式协助他们。
- en: We’re finding that people really need all of these pieces. A lot of the work
    we’re doing is trying to make sure that they understand that complete spectrum
    of practices and policies and tools that they’re going to need to achieve this.
    And we want to make it easy for everybody to just pick these up and go running
    with them, but also customize them as they need. We know different domains are
    different, organizations are different, and so we don’t want it to be just the
    Microsoft way. We just want to make it really easy for people to start with the
    responsible AI state of the art and then adapt it to them.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现人们真的需要所有这些部分。我们正在做的大量工作都是为了确保他们理解他们将要需要的完整实践、政策和工具范围。我们希望让每个人都能轻松地拿起这些工具并开始使用，同时根据需要对其进行定制。我们知道不同领域不同，组织不同，所以我们不希望它只是微软的方式。我们只想让人们能够轻松地从负责任的AI最前沿开始，然后根据他们的需要进行调整。
- en: '**A.G.:** Yeah, and this is useful. In my case, I’m using it with partners,
    with integrators, with consulting firms, with clients also who are asking for
    inspiration or some good practices on how they can approach responsible AI. Traditionally,
    it was about defining the AI principles, like we want to be accountable, transparent,
    etc. But now we’re going farther on how to approach this at an organizational
    and technical level.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，这很有用。在我的情况下，我正在与合作伙伴、集成商、咨询公司以及寻求灵感或一些良好实践以了解如何接近负责任的人工智能的客户一起使用它。传统上，这关乎定义人工智能原则，比如我们希望负责、透明等。但现在我们正在进一步探讨如何在组织和技术层面上接近这个问题。'
- en: '**S.B.**: Yeah, and I think we have people ask both, how do I do a practice
    like red teaming or evaluation that we mentioned? Or they ask, how do I address
    a particular potential risk, such as hallucination or prompt injection attacks?
    We see people looking for guidance in both of those dimensions. And the answer
    for something like hallucination is…here are the steps: here is where you identify
    that risk, here’s how you measure that risk, how you red team it, here’s layers
    of mitigation for that. They’re actually a horizontal and vertical pattern, but
    we hear people asking for guidance in both of those ways.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，我认为人们既询问如何进行我们提到的像红队或评估这样的实践，也询问如何应对特定的潜在风险，比如幻觉或提示注入攻击。我们看到人们在两个维度上都在寻找指导。对于像幻觉这样的问题的答案是……这里有步骤：这里是你识别该风险的地方，这里是你如何衡量该风险，如何进行红队测试，这里是对该风险的缓解层。它们实际上是横向和纵向的模式，但我们听到人们以这两种方式寻求指导。'
- en: '**A.G.**: Yes, indeed. And I think you have mentioned the experiences with
    GitHub, or Microsoft Bing/Copilot, that I think have been extremely illustrative
    for everyone trying to create their own copilot or whatever platform, even for
    competitors. I remember people saying…“Hey, Jordi Ribas (CVP Microsoft) and the
    team are releasing learnings every week, and this is so useful now for everyone.”
    So that’s very exciting, that movement from model to platform, and all the learnings
    that go with it.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，确实如此。我认为你已经提到了与GitHub或微软必应/Copilot的经验，我认为这对所有试图创建自己的Copilot或任何平台，甚至竞争对手的人来说都极具说明性。我记得人们说……“嘿，Jordi
    Ribas（微软CVP）和他的团队每周都在发布经验教训，这对现在所有人来说都非常有用。”所以这非常令人兴奋，从模型到平台的转变，以及与之相关的所有经验教训。'
- en: '**S.B.**: Yeah, and we’re still learning every day now, as technology becomes
    more accessible to more people, all of the exciting new use cases that people
    can think of. But I think those early days were very special. The rate of learning
    was just insanely high. And we had the first ones where we had brought experts
    from around the company to work on this. Quite a few folks from Microsoft Research
    volunteered to work on that full time. We had these great minds all working together,
    iterating every day. And I think for a lot of people, that experience also kind
    of changed their work after that in their research directions, because they went
    in and really saw what the real challenges we have right now are, but also the
    real amazing potential of the technology. That hands-on experience where you were
    learning so much and we were all learning together, I think was really shaped
    in the way we’ve done AI at Microsoft and many different people’s outlook on that.
    So that certainly I think was a really special innovative time for us.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，现在我们每天都在学习，随着技术对更多人变得可访问，人们可以想到的所有令人兴奋的新用例。但我认为那些早期是非常特别的。学习的速度非常高。我们第一次将来自公司各地的专家召集起来共同工作。微软研究部门的很多人自愿全职参与其中。我们这些伟大的思想家一起工作，每天迭代。我认为对于很多人来说，那次经历也改变了他们在研究方向上的工作，因为他们真正看到了我们现在面临的真正挑战，以及技术的真正惊人潜力。那种亲身体验，你在其中学到了很多，我们都在一起学习，我认为这对我们在微软做AI的方式以及许多人对它的看法产生了真正的影响。所以这确实是我们一个非常特别的创新时期。'
- en: '**A.G.**: That must be amazing. I can imagine those days and those discussions,
    the daily work. It was very exciting also from the consumer point of view, just
    to see the news and all the new functionalities, not only the models, but everything
    that goes with that. What’s your vision for the next…it’s too difficult not to
    say two or three years, but just for the next year, your vision of how this will
    evolve, the kind of things that we may see, the kind of challenges that we may
    have, what do you think will happen?'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这一定很神奇。我可以想象那些日子和那些讨论，日常的工作。从消费者的角度来看，看到新闻和所有新的功能也很兴奋，不仅限于模型，还有与之相关的所有东西。你对未来的展望是什么……不说两三年太难了，但就下一年而言，你对它如何发展的看法，我们可能会看到什么，我们可能会面临的挑战，你认为会发生什么？'
- en: '**S.B.**: Yeah, I think there’s a couple of patterns that we’re seeing. Certainly
    one of them is multimodal, right? A lot of the applications are still kind of
    mostly text, but there’s so much more potential when you can understand together
    different modalities, images, audio, video, etc. We’re starting to see very exciting
    examples of that technology. I think over the next year, a lot more multimodal
    will come in, and that certainly brings new types of risks from a responsible
    AI point of view. I think there’s a lot of excitement about the next wave of technology
    and AI agents, having the technology that can perform more actions. That of course
    greatly increases the space of things you need to think about in terms of responsible
    AI, but also the level of quality and inaccuracy that you need, because if you’re
    taking an action, a mistake can have a much bigger impact.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，我认为我们正在看到一些模式。当然，其中之一是多模态，对吧？许多应用仍然主要是文本，但当你能够理解不同的模态，如图像、音频、视频等时，潜力就大得多。我们开始看到这个技术的非常令人兴奋的例子。我认为在未来一年里，会有更多的多模态应用出现，这当然会从负责任的AI角度来看带来新的风险。我认为人们对下一波技术和AI代理非常兴奋，拥有能够执行更多操作的技术。这当然大大增加了在负责任的AI方面需要考虑的事物范围，同时也提高了所需的质量和准确性，因为如果你采取行动，错误可能会产生更大的影响。'
- en: Those are kind of two big ones that are on my mind, but maybe in the kind of
    bigger picture sense, Kevin Scott (Microsoft CTO) says regularly that right now
    this technology is on an exponential curve, but we only get to see the next points
    on the curve every year or two, when the next wave of technology comes out. I
    think a lot of us are asking ourselves, “Is the next one really gonna be exponentially
    better than GPT-4?” And if it is, what does that really mean? Like our minds have
    a hard time thinking in exponentials, we really kind of project out linearly.
    There’s also this chance that we’re gonna be just seeing another extreme breakthrough,
    sometime soon. And so, I think, one of the exciting open questions is just how
    much better will the next wave of technology be?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我心中想的两个大问题，但在更宏观的角度上，凯文·斯科特（微软首席技术官）经常说，目前这项技术正处于指数级增长，但我们只能每年或每两年看到曲线上的下一个点，当下一波技术出现时。我认为我们很多人都在问自己，“下一个版本真的会比GPT-4指数级地更好吗？”如果是的话，那究竟意味着什么？我们的思维在指数上很难思考，我们实际上是在线性地预测。还有可能我们很快就会看到另一个极端的突破。因此，我认为，一个令人兴奋的开放问题是下一波技术将会变得多好？
- en: '**A.G.**: Yeah, I think it’s exponential, the kind of performance that we’ll
    see, and the kind of considerations that you’re mentioning. Like I said, there
    are multiple dimensions that we need to consider on that journey of new applications
    being created. A good example is what we saw with [OpenAI releasing Sora](https://oreil.ly/ppSjf)
    and just putting it out there, and showing the benefits but also sharing it with
    different parts of the community to analyze the potential considerations, because
    we can do a lot of things with this technology. But it’s exciting.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，我认为它是指数级的，我们将会看到的性能，以及你提到的那些考虑因素。就像我说的，在创建新应用的旅程中，我们需要考虑多个维度。一个很好的例子是我们看到[OpenAI发布Sora](https://oreil.ly/ppSjf)并将其公之于众，展示了其好处，同时也与社区的不同部分分享，以分析潜在的考虑因素，因为我们可以用这项技术做很多事情。但这很令人兴奋。'
- en: '**S.B.**: Yeah, I think it’s as a technologist, it’s your hope, but certainly
    not expectation, that you’re gonna be around when the technology goes through
    a critical transformation, really crosses the threshold from being something that’s
    an exciting research idea to something that is really ready for practice. And
    so, I remind my team every day, we need to be enjoying every moment of this, because
    obviously I think the impact of the technology will only grow and that will be
    really exciting as well, but nothing is quite like the beginning in terms of the
    change that that brings, and the rate of learning and everything. And so, we’re
    just enjoying the ride, but also very, very aware that we’re in a position of
    leadership where we need to steer the direction of the future of this technology,
    and we need to help the world be able to use it in effective ways, but also ensure
    that it is not used in ways that I think society really doesn’t want. And so,
    I think we’re also very aware of the weight of the responsibility of being here
    in this position at the beginning and really making sure that we make decisions
    we think are going to be right for the future.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.B.**: 是的，我认为作为一个技术专家，这是你的希望，但当然不是期望，当技术经历一个关键的转型，真正跨越从令人兴奋的研究想法到真正准备好实践的门槛时，你会在场。因此，我每天都会提醒我的团队，我们需要享受这个过程中的每一刻，因为显然我认为这项技术的影响只会增长，这也会非常令人兴奋，但没有什么能比这个开始带来的变化更令人激动了，以及学习和一切的速度。因此，我们只是在享受这个过程，但同时也非常清楚我们处于一个领导地位，我们需要引导这项技术的未来方向，我们需要帮助世界能够以有效的方式使用它，同时确保它不会被用于我认为社会不希望的方式。因此，我认为我们也非常清楚在这个位置上所承担的责任的重量，确保我们做出我们认为对未来是正确的决定。'
- en: 'Tim Ward: The Impact of Data Quality on LLM Implementations'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Tim Ward: 数据质量对LLM实施的影响'
- en: '**A.G.**: Of course, you’re the CEO of CluedIn, but what’s your role in the
    company? What’s CluedIn doing in terms of data management, data quality, and so
    on?'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 当然，你是CluedIn的CEO，但在公司中你的角色是什么？CluedIn在数据管理、数据质量等方面做了些什么？'
- en: '![](assets/aoas_07in05.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_07in05.png)'
- en: '**T.W.**: Yeah, sure. I’m actually joining you from a hotel room in Seattle.
    I am literally about 200 meters away from the Microsoft headquarters in Redmond,
    so I’ve been working with that group all week. This has a little bit to do with
    my role. I run the CluedIn team, I’m the CEO of CluedIn, but I come from a very
    product-driven software engineering background. I’ve been architecting products
    and building enterprise-grade products for some time. What we do at CluedIn is
    bring some pretty critical and necessary elements to Microsoft customers, and
    that is in the form of data quality and master data management (MDM).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 是的，当然。我实际上是从西雅图的一家酒店房间加入你们的。我实际上就在雷德蒙德微软总部大约200米远的地方，所以我整个星期都在和那个团队一起工作。这和我所扮演的角色有点关系。我负责CluedIn团队，我是CluedIn的CEO，但我来自一个非常以产品驱动的软件开发背景。我已经在架构产品和构建企业级产品一段时间了。我们在CluedIn所做的是为微软客户提供一些非常关键和必要的元素，那就是数据质量和主数据管理（MDM）。'
- en: 'Data quality is probably one of those aspects we’re all aware of, we know we
    need to fix. MDM is somewhat one of those mysterious topics that I think people
    might even say is synonymous with data quality. What’s MDM versus data quality?
    At CluedIn, we really see there’s quite a lot of similarities between what data
    quality fixes and MDM. What we’ve really done is find those different elements
    or categories, and here is the kicker: CluedIn is a tool that’s really targeted
    at nontechnical users. That’s because we think that data quality doesn’t seem
    like one of those stubborn things that we’re always tripping over, and we know
    we need to do it. What we believe and what we’ve seen with our customers is we
    were never able to bring the business in and make them responsible for this. Often,
    the tooling was a little bit too complex, and so what I’m happy to say is that
    we bring those capabilities to anyone that’s in that Microsoft ecosystem. They’ve
    got Fabric, they’ve got maybe Purview, they’ve got Azure Data Factory. But at
    some point, they need to go, how do I bring the business in and have them play
    a role in this supply chain of data as well.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量可能是我们所有人都意识到的方面之一，我们知道我们需要修复它。MDM是那些可能被说成与数据质量同义的神秘话题之一。MDM与数据质量有什么区别？在CluedIn，我们真正看到数据质量修复和MDM之间有很多相似之处。我们真正做的是找到那些不同的元素或类别，这里是关键：CluedIn是一个真正针对非技术用户的工具。这是因为我们认为数据质量似乎不是那种我们总是跌倒的顽固事物，我们知道我们需要做这件事。我们相信的，以及我们从客户那里看到的，是我们从未能够将业务带入并让他们对此负责。通常，工具过于复杂，所以我很高兴地说，我们将这些能力带给任何在Microsoft生态系统中的人。他们有Fabric，可能有Purview，有Azure
    Data Factory。但到了某个时候，他们需要考虑如何将业务带入并让他们在这个数据供应链中发挥作用。
- en: '**A.G.**: Well, that’s amazing because I’m pretty sure you heard with this
    initial generative AI wave like…oh, we don’t need data anymore, so we don’t need
    to care about the quality. Then what happened, people were saying they would like
    to customize development and use their own data, but wait, we haven’t taken care
    of the data quality for a while, so what to do?'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 哇，这太棒了，因为我非常确信你听到了这个初始生成式AI浪潮，就像……哦，我们不再需要数据了，所以我们不需要关心质量。然后发生了什么，人们说他们想要定制开发并使用自己的数据，但是等等，我们已经有一段时间没有处理数据质量了，那怎么办？'
- en: '**T.W.**: Exactly. Now, there’s somewhat a race condition here in that to yield
    value and insights with data, and specifically AI, we probably need AI to help
    us with the data quality piece. There’s this quite self-fulfilling recursive nature
    to the yin and the yang between solving data quality and actually yielding value
    AI. Spot on with, I think, your analogy there.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 正确。现在，这里似乎存在某种竞争条件，为了从数据中获取价值和洞察，特别是AI，我们可能需要AI帮助我们解决数据质量问题。解决数据质量问题和真正产生AI价值之间存在着一种相当自我实现的阴阳循环性质。我认为你的类比非常准确。'
- en: '**A.G.**: Yes. Before we go into the details of data quality, because I think
    it deserves some discussion here, but how was 2023 for CluedIn from a generative
    AI perspective? I know that you have been working on a lot of things, including
    your own product. How did you experience this?'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的。在我们深入探讨数据质量之前，因为我认为这里值得讨论，但从生成式AI的角度来看，2023年CluedIn的情况如何？我知道你一直在做很多事情，包括你自己的产品。你是如何体验这一点的？'
- en: '**T.W.**: Many facets. Number one thing, being a company just “slightly” smaller
    than Microsoft, just slightly, that I guess we adopted AI ourselves, just internally
    as a business very early on, and it started with GitHub Copilot. It then progressed.
    Fortunately, due to our great relationship and partnership with Microsoft, we
    were given early access to Azure OpenAI in a private preview. That was something
    we instantly realized, wow, that’s how we’re going to build this in our own products.
    This also gave us a bit of time to learn about the guardrails that were necessary.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 许多方面。首先，我们是一家比微软“稍微”小一点的公司，只是稍微小一点，我想我们很早就开始自己采用AI了，只是作为一家企业内部，而且是从GitHub
    Copilot开始的。然后它逐渐发展。幸运的是，由于我们与微软之间伟大的关系和合作伙伴关系，我们被赋予了早期访问Azure OpenAI的私人预览版。我们立刻意识到，哇，这就是我们将在自己的产品中构建的方式。这也给了我们一些时间来了解必要的护栏。'
- en: You and I both know, Adrián, GenAI forms some pretty spectacular demonstrations,
    but being in the data management space and data governance and data quality, often
    the discussion goes to how do I make sure that our generative AI initiatives are
    actually going to survive the tumultuous nature of the enterprise? Is it secure?
    Is it governed? Do I have an audit trail of what happened? Is someone responsible
    for the data that’s being used? What about all the data sovereignty questions
    about where is data? Pretty early, we were having those discussions internally,
    but also with our early customer adopters that were saying, as soon as you guys
    start to implement AI in your platform, please let me know because there just
    seems like such an opportunity to apply AI to the actual data management practices
    itself, not just use it as an end consumption piece of software.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道，Adrián，生成式AI形成了一些相当惊人的演示，但在数据管理空间、数据治理和数据质量方面，讨论往往转向如何确保我们的生成式AI倡议能够真正地经受住企业动荡的本质？它是安全的吗？它是受管理的吗？我有没有发生事件的审计跟踪？谁对使用的数据负责？关于数据主权的问题，数据在哪里？我们很早就开始内部讨论这些问题，也与我们的早期客户采用者讨论，他们表示，一旦你们开始在平台上实施AI，请让我知道，因为似乎有如此多的机会将AI应用于实际的数据管理实践，而不仅仅是将其用作软件的最终消费部分。
- en: '**A.G.**: Which makes total sense from a Copilot point of view of interacting,
    of adding something to the user interface, we are saying, MDM or data quality
    in general, they are traditionally some sort of technical task, but we want to
    bring this to a business because they know their data, they know the information,
    so we can put this layer to infuse generative AI, and that’s what you guys did,
    and did it very early.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 从Copilot的角度来看，从交互、向用户界面添加内容的角度来看，我们说，MDM或数据质量在传统上是一种技术任务，但我们希望将其带入业务，因为他们了解他们的数据，他们了解信息，因此我们可以添加这一层来注入生成式AI，这正是你们所做，而且做得非常早。'
- en: '**T.W.**: Yeah. I would argue that’s been the biggest gap that we haven’t been
    able to bring the business into, because we often give them this software and
    we say, hey I bought this great MDM platform for you, just put all your data quality
    rules in there. Then someone comes in and says, OK, it says put a regular expression.
    Sorry, what’s a regular expression? I’ve been a software engineer for 19 years,
    and I still don’t know how to build regular expressions, but we’re asking people
    to somehow do this and that’s how they’ll play a role. And I think that’s why
    often these initiatives get thrown back to IT naturally because this seems like
    it’s for them. Then IT says, “No, I’ve got my tools, I’ve got Fabric, and I’ve
    got Azure Data Factory, that allows me to play my role, but it asks me to be very
    technical.” You could argue, Adrián, haven’t we been trying to bring the business
    in for 30 years? What has changed? Well, apart from the fact technology has just
    changed in general, getting access to different software is easier and easier
    all the time, and the cloud of course brought part of that.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 是的，我认为这是我们一直未能将业务带入的最大差距，因为我们经常给他们提供这种软件，然后说，嘿，我为你买了一个很棒的MDM平台，你只需把所有数据质量规则放进去。然后有人进来问，好吧，它说要放一个正则表达式。抱歉，什么是正则表达式？我已经做了19年的软件工程师，我仍然不知道如何构建正则表达式，但我们却要求人们做这样或那样的事情，这就是他们的角色。我认为这就是为什么这些倡议往往自然地被推回给IT，因为这看起来像是为他们准备的。然后IT部门会说，“不，我们有自己的工具，我们有Fabric，我们有Azure
    Data Factory，这让我能够扮演我的角色，但它要求我非常技术化。”你可以争论，Adrián，我们不是已经尝试将业务带入30年了么？有什么变化？好吧，除了技术总体上发生了变化，获取不同软件的途径越来越容易，当然，云也带来了其中的一部分。'
- en: The other piece is we’ve been handed in this nice little wrapped up bow an easy
    way to interact with LLMs; that is that chasm, it’s that bridge between, you can
    tell me what you intend and I’ll translate it underneath into what the underlying
    system needs. Because the thing is, for detecting patterns in data, especially
    in a deterministic way, regular expressions are somewhat just the way we do that.
    You need some underlying function to be able to do that, especially in a cost-efficient
    economic way. We can’t at this point, which is one of the things I’m looking forward
    to, we can’t just throw a large language model at every problem. Actually, we
    shouldn’t. I personally wouldn’t sleep as well if I realized my whole supply chain
    was just running off a model that sometimes gets things right and sometimes gets
    things wrong. It’s that bridge, how do I use the general knowledge to bridge that
    technical thing that these tools will still ask you to do, but now it’s not so
    much in this very like, oh, I need to be technical to do it.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们得到了一个简单易行的方式来与大型语言模型（LLMs）互动；那就是那个鸿沟，它是连接我们的桥梁，你可以告诉我你的意图，我会将其翻译成底层系统所需的内容。因为，对于在数据中检测模式，尤其是在确定性方式下，正则表达式只是我们这样做的一种方式。你需要一些底层函数来实现这一点，尤其是在成本效益和经济的方式下。目前我们做不到这一点，这也是我期待的事情之一，我们不能把大型语言模型应用到每一个问题上。实际上，我们也不应该这样做。如果我发现我的整个供应链只是运行在一个有时对有时错的模型上，我恐怕会睡不好觉。这就是那个桥梁，我如何使用一般知识来弥合这些工具仍然要求你做的技术问题，但现在这并不是那么需要技术。
- en: '**A.G.**: These are the cases where when we’re interacting with the tools,
    or we are processing information, or we are trying to fulfill some JSON file by
    using generative AI, the engine itself becomes more deterministic. It’s like we
    are not giving that much creativity because we are trying to find that connection
    with the system. In your case, you have software, you have a backend layer with
    all the data, you are connected to the data. I think that that’s the perfect example
    of evolution on the interfaces. When Bill Gates said this is like the evolution
    from the command line to Windows, and then from Windows to this kind of generative
    AI interface. How do you see the relationship? I know that this is not an answer
    for now, it could be an answer for later, and for later, and for later on the
    roadmap of the different products. But how do you see the current relationship
    between a company like CluedIn, or even the MDM and data quality solution, and
    the Azure OpenAI engine?'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：这些是我们与工具互动、处理信息或尝试通过使用生成式AI来满足某些JSON文件的情况，引擎本身变得更加确定。就像我们并没有给予太多创造力，因为我们试图与系统建立联系。在你的情况下，你有软件，你有包含所有数据的后端层，你与数据相连。我认为这是界面进化的一个完美例子。当比尔·盖茨说这是从命令行到Windows，再到这种生成式AI界面的进化时，你怎么看待这种关系？我知道这现在不是一个答案，它可能是未来的答案，在产品路线图上的未来答案。但是，你怎么看待像CluedIn这样的公司，甚至MDM和数据质量解决方案，与Azure
    OpenAI引擎之间的当前关系？'
- en: '**T.W.**: So I think the relationship is somewhat symbiotic, and a good example
    is the plug-in architecture for Azure OpenAI. The fact that you can plug in something
    like Uber, KAYAK, or TripAdvisor, and the LLM knows. I know when you want general
    chat and general knowledge, but then I also can do the smart thing of saying,
    actually, when do you just want to talk to KAYAK or TripAdvisor, and book a trip?
    A very similar thing happens on the data management side, via CluedIn’s copilot
    that we have in our platforms, very similar to what you have in Microsoft 365
    or Power BI, or up-and-coming Fabric. If you had a big dataset with a million
    records, right now, without actually training your own model on that data, there’s
    really no easy way via context windows to just, not in an economically viable
    way anyway, to say what’s the value in column 4 in row 464,000? But the symbiosis
    is, how do I translate that language into an underlying language that can then
    do that query in a very efficient way? That could be translating it locally into
    SQL. In our case, it’s translating it locally into something like an elastic search
    query that says, I’ll build the query, so the LLM is not actually looking at a
    million records, it’s transposing into the local environment.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**：所以我认为这种关系是某种共生关系，一个很好的例子是Azure OpenAI的插件架构。你可以插入像Uber、KAYAK或TripAdvisor这样的东西，LLM知道。我知道当你想要一般聊天和一般知识时，但然后我也可以做智能的事情，比如说，实际上，你什么时候只想和KAYAK或TripAdvisor聊天并预订旅行？在数据管理方面，通过我们平台上的CluedIn副驾驶，发生着非常类似的事情，就像你在Microsoft
    365或Power BI或新兴的Fabric中看到的那样。如果你有一个包含一百万条记录的大数据集，现在，实际上没有在你自己的数据上训练模型的情况下，通过上下文窗口真的没有简单的方法，至少在经济上不可行，来说明第4列第464,000行的价值是什么？但共生关系在于，我如何将这种语言翻译成底层语言，然后以非常高效的方式执行查询？这可能是将其本地翻译成SQL。在我们的情况下，它将翻译成类似弹性搜索查询的东西，即我会构建查询，这样LLM实际上不会查看一百万条记录，而是将其转换到本地环境中。'
- en: Listen, I think at some point you will have these unlimited token sizes where
    you can either just say, I want the whole one million rows in the context, or
    potentially it’s going to be…load that data into a model, and your copilot is
    running off your custom model. And Azure AI Studio is a great tool that makes
    it so easy already to build your own copilot of Llama and Mistral and things like
    this, and also throw your own data from quite heterogeneous file types as well.
    Everything from PDF to images, to CSV, to Excel, to text, to video, and even C#
    and SQL files, it can swallow that stuff up. At some point, you are going to get
    to a point where you might not even need to do that local translation in all situations.
    You could literally talk to your entire data estate with a native feel in chat.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 听着，我认为在某个时刻，你将拥有这些无限的令牌大小，你可以要么只说，我想在上下文中获取全部一百万行，或者可能它将会是……将数据加载到模型中，你的副驾驶正在运行你的自定义模型。Azure
    AI Studio 是一个伟大的工具，它使得构建自己的副驾驶（如Llama和Mistral等）变得非常容易，同时也可以处理来自各种异构文件类型的数据。从PDF到图片，再到CSV、Excel、文本、视频，甚至C#和SQL文件，它都能处理这些内容。在某个时刻，你可能甚至不需要在所有情况下都进行本地翻译。你几乎可以用聊天的方式与整个数据资产进行原生交流。
- en: '**A.G.**: Yes, there is another discussion with Dr. John Maeda, he was mentioning
    the notion of plug-ins, everything is interacting, and we are even building the
    code based on needs. It’s like function calling, but imagine automatic function
    calling, on which the model can realize, I need to check on my storage or Cosmos
    DB, I need to check on whatever source of information I have. Even more, if I
    was imagining (and I know this interview is about asking you questions), but just
    imagining the future, and this is not even related to roadmaps or whatever, but
    imagine in the future you have your data state, then you are handling general
    data governance with a solution like Purview, then you are going to the details
    of data quality and MDM to prepare all the data, and then there’s a smooth two-clicks
    way to push to a data store.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：是的，还有与约翰·梅达博士的另一个讨论，他提到了插件的概念，一切都在交互，我们甚至根据需求构建代码。这就像函数调用，但想象一下自动函数调用，模型可以实现，我需要检查我的存储或Cosmos
    DB，我需要检查我所拥有的任何信息源。更进一步，如果我正在想象（我知道这次采访是关于提问的），但只是想象一下未来，这甚至与路线图无关，但想象一下在未来，你拥有你的数据状态，然后你使用像Purview这样的解决方案来处理一般数据治理，然后你进入数据质量和MDM的细节来准备所有数据，然后有一个平滑的两击推送方式到数据存储。'
- en: '**T.W.**: I then have to comment on this, Adrián, because very early on when
    OpenAI came out, almost within a matter of days, this concept of LangChain came
    out as well, in that I want to chain multiple things together, and of course this
    was one thing we said is we have to have this included, because what we want via
    the plug-in architecture is to say, go get me all of our employee files that we
    have, bring them in, map them into the same concept, and trade the semantics of
    column names for me. If you’ve got F name, first name, first, of course it easily
    swallows that up, but in many cases you might bring on an SAP system, and it’s
    column names, not that obvious, they are German acronyms in a lot of cases. And
    for it to be able to chew that up and say, “I know what you mean,” but then to
    chain things, and then after that, check every column and apply appropriate data
    quality checks, and that’s the one thing I love, the fact that you can just be
    very dynamic. That you’re not being prescriptive and saying, “No please enforce
    this standard of phone numbers,” that will work, but also the fact you can be
    very dynamic and fluid in the way you interact.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 我必须对此发表评论，阿德里安，因为在OpenAI出现之初，几乎就在几天之内，LangChain这个概念也随之出现，我想将多个事物串联起来，当然这也是我们说必须包含在内的原因，因为通过插件架构，我们想要的是获取我们所有的员工文件，将它们导入，将它们映射到相同的概念，并为我转换列名语义。如果你有F名，即名字，那么它很容易就吸收了，但在许多情况下，你可能会引入一个SAP系统，它的列名并不明显，在许多情况下是德语缩写。而且为了让它能够理解并说“我知道你的意思”，然后将其串联起来，之后检查每一列并应用适当的数据质量检查，这正是我最喜欢的一点，那就是你可以非常灵活。你不是在规定性地要求，比如说，“请强制执行电话号码的标准”，这会有效，但你也可以非常灵活和流畅地互动。'
- en: I am in the data government space, but to be honest, I don’t know ISO codes
    off the top of my head, I don’t know how fun that person would be at a party if
    they actually did know that, and you’re wanting the large language model—the truth
    is it knows that stuff, it knows the ISO codes, it knows what they do and that
    chaining of things, I think this is what takes the use of GenAI from something
    that saves you 5, 10 seconds, to something that genuinely saves hours of research
    or trial and error. And that’s where I think the key thing is in bringing it into
    products, we have this kind of standard or set of ethics we have on the use of
    AI in our product, and we take a couple of these from inspiration from Microsoft
    as well, one of them being your data is your data, we’re never going to use cross-customer
    data to train this general model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我在数据治理领域，但说实话，我并不清楚ISO代码，我不知道如果那个人真的知道这些，在聚会上会有多有趣，而你希望大型语言模型——实际上它知道这些，它知道ISO代码，它知道它们的作用以及这些事物的串联，我认为这正是将GenAI的使用从节省你5到10秒的事情转变为真正节省数小时研究和试错的时间的关键。我认为关键在于将其引入产品中，我们对我们产品中AI的使用有一套标准或伦理规范，我们也从微软那里汲取了一些灵感，其中之一就是“你的数据是你的数据”，我们永远不会使用跨客户数据来训练这个通用模型。
- en: But one of the ones we’ve added ourselves is, no AI for AI’s sake, and what
    that means is, if we build something on our platform, and actually you could probably
    do the same thing in the old way, probably faster or relatively the same, why
    bother using AI? You know a good example would be, it’s technically impressive
    in a chat to say “Find me all the employees that are over 64,” but actually by
    the time you’ve just used our rule builder, you’ve probably taken the same amount
    of time to do it by hand than using AI, and at that point it’s like, what’s the
    value there? And I would argue, it’s not so much. There are cases where it is
    smart, for example if I said, “Go get me all of our customers in the Nordic region,”
    and I don’t have to say, “where the country is Denmark or Iceland” or this or
    this. Now that’s great you’ve saved 15 seconds, but really the things we should
    be focusing on is, how did I save you complexity? How did I increase simplicity?
    And what were the things that saved me one or two hours, three days, that’s what
    we’re really trying to focus on here at CluedIn.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们自己添加的一个案例是，不要为了AI而使用AI，这意味着，如果我们在我们平台上构建了一些东西，实际上你也许可以用旧的方式做同样的事情，可能更快或相对相同，为什么要使用AI呢？你知道一个很好的例子是在聊天中说出“找到所有超过64岁的员工”，但实际上当你使用我们的规则构建器时，你可能用与使用AI相同的时间手动完成它，那么在那个点上，它的价值在哪里？我会说，价值并不大。有些情况下它是聪明的，例如，如果我这么说，“为我找到所有北欧地区的客户”，我不用说我需要说“国家是丹麦还是冰岛”或这样那样。现在你节省了15秒，但真正应该关注的是，我如何节省了你的复杂性？我如何增加了简单性？以及哪些事情为我节省了一两个小时，三天，这正是我们在CluedIn这里真正试图关注的。
- en: '**A.G.**: Yes, I love that vision of how the end-to-end architecture chains
    different functions that handle the data and AI activities of any company. But
    I am seeing cases in which people are using generative AI to re-create chatbots
    all the time, for example, because I want it to be deterministic, I want to use
    it like a knowledge base, and then I have 10 questions, 10 answers, and I say
    that’s not necessarily something we maybe want to do with generative AI. Do you
    have interesting stories or insights on how data quality is already impacting,
    either in a negative or positive way, generative AI implementations with Azure
    OpenAI, or any other technology? Like clients saying that because we have been
    working on data quality, and we have this data state properly done, we have seen
    the difference.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，我喜欢这种端到端架构将处理任何公司数据和处理AI活动的不同功能链起来的愿景。但我看到一些案例，人们一直在使用生成式AI来不断重新创建聊天机器人，例如，因为我希望它是确定性的，我希望像使用知识库一样使用它，然后我有10个问题，10个答案，我说这并不一定是我们可能想要用生成式AI做的事情。你有没有关于数据质量如何已经影响（无论是负面还是正面）Azure
    OpenAI的生成式AI实现或任何其他技术的有趣故事或见解？比如客户说，因为我们一直在做数据质量工作，我们已经正确地完成了这种数据状态，我们看到了差异。'
- en: '**T.W.**: One of the things that’s so interesting about the form factor of
    chatting to your data is that it surfaces bad data quality quicker than probably
    any other form factor like search, or anything like this. It becomes abundantly
    clear, and I think also it’s because people have high expectations of LLMs, so
    even when it does something slightly silly, in my head I go like, “I appreciate
    this so much, this is so amazing,” like with my children, I will forgive my ChatGPT
    more often than I don’t, and I think one of the cases is when you start throwing
    your own data into a LLM, what happens is the chat interface starts to surface
    your data quality issues really clearly.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 与你的数据聊天的一个非常有趣的形式因素是，它比其他任何形式因素，如搜索或类似的东西，更快地揭示数据质量问题。它变得非常明显，我认为这也是因为人们对LLM有很高的期望，所以即使它做了点愚蠢的事情，在我的脑海中，我会想，“我非常感激这一点，这太神奇了，”就像对我的孩子一样，我会比不原谅ChatGPT更经常地原谅它，我认为其中一个案例是当你开始将你的数据投入LLM时，会发生的事情是聊天界面开始清楚地揭示你的数据质量问题。'
- en: A good example would be a case where, pulling in HR data on employees as part
    of a HR onboarding process, to make new employees feel a little bit like they
    don’t have to go and find out, “Who do I talk to about this and that?” Now of
    course they have a HR system where it’s tagged with information like, this person
    is a software engineer, and they’ve got these responsibilities, but also with
    the large amount of employees, it was improbable that that would be the best way
    to do it, so the form factor of being able to use your own natural language was
    great. What happened is when you typed in something like, “Can you give me the
    contact details for the person that knows the most about Azure OpenAI?” or something
    like this, it would come back very confidently and say, “Not a problem, I’ve got
    this and this and this phone number.” And you know, there’s a couple of challenges
    with that, number one is, you’re more confused now, the second thing is, without
    proper attribution, you’re not 100% aware if the AI is making this up.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是，在HR入职流程中，将员工的人力资源数据纳入其中，让新员工感觉他们不必去寻找，“我该找谁谈论这件事？”当然，他们有一个带有类似信息的HR系统，比如，这个人是一名软件工程师，他们有这些职责，但随着员工数量的增加，这种方式可能不是最好的，因此能够使用自己的自然语言的形式非常好。发生的情况是，当你输入类似“你能给我知道Azure
    OpenAI最多的人的联系详情吗？”这样的信息时，它会非常有信心地回复，“没问题，我这里有这个和这个电话号码。”当然，这里有几个挑战，首先是，你现在更困惑了，第二点是，如果没有适当的归属，你就不完全清楚AI是否在编造东西。
- en: One of the great elements about Azure AI Studio, and if you’re using that particular
    place to host your copilots and anything you’ve done with your custom models,
    you get attribution at a file input level, for free. The challenge is that the
    data lineage doesn’t start there, it started a long time ago in a different place,
    but you get it at one of the places where it landed, so you could put it into
    your AI model, but actually the lineage of what happened to that file, where was
    the source system, what happened along the way, who changed what, why did they
    change this…this is some of the lineage that things like Microsoft Purview brings
    in at an asset level, and CluedIn is bringing us at a record level. Purview can
    say, these four assets on employee data were fed into your model, great. Then
    CluedIn says, see that Martin there, and that Martin there, there’s no way for
    me to stitch this data together, but I actually put those together into the same
    record, and so once you’re using the Copilot on this cleaner data, the answers
    are much more precise, and much more trustworthy because you can see…here is the
    phone number for Martin, oh, and that’s where I got it from, that’s what made
    me decide on it, so it becomes so abundantly clear, when you start to use the
    copilot, you’re like technology is great and super interesting, did it just make
    this up? And you’ll know from these AI models, they’re not self-aware. They cannot
    let you know if they made something up. Isn’t that a weird difference between
    the large language models and us? Like I am self-aware if I’ve made something
    up, but the LLM is not.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AI Studio 的一个重要元素是，如果你在那个特定的地方托管你的共飞行员（copilots）以及你用自定义模型所做的任何事情，你可以在文件输入级别免费获得归属。挑战在于数据溯源并不始于那里，它始于很久以前的一个不同地方，但你可以在数据最终到达的地方获得它，因此你可以将其放入你的AI模型中，但实际上，该文件的溯源，包括源系统是什么，过程中发生了什么，谁改变了什么，为什么他们要改变……这些都是像Microsoft
    Purview在资产级别引入的一些溯源，CluedIn在记录级别为我们带来的。Purview可以说，这些四个与员工数据相关的资产被输入到了你的模型中，很好。然后CluedIn说，看那里的Martin，还有那里的Martin，我无法将这些数据拼接在一起，但我实际上已经将它们放到了同一个记录中，因此一旦你在这个更干净的数据上使用共飞行员，答案就会更加精确，也更加可靠，因为你可以看到……这是Martin的电话号码，哦，还有我是从哪里得到它的，这就是我决定使用它的原因，所以当你开始使用共飞行员时，你会觉得技术很棒，超级有趣，它只是凭空想出来的吗？而且你会从这些AI模型中知道，它们没有自我意识。它们不能让你知道它们是否编造了东西。这不是大型语言模型和我们之间的一个奇怪差异吗？比如，如果是我编造了东西，我会意识到，但LLM（大型语言模型）不会。
- en: '**A.G.**: And it depends. I really like that, because imagine you are a data
    scientist, and you are performing an exploratory data analysis. If you don’t have
    the context on the business, you’re not able to understand if something that you
    see on the data, or even your own analysis, will be actually true, and I was thinking
    about that notion of EDA or exploratory data analysis, like the next barrier,
    because you have probably seen it in your projects. The best EDAs are those that
    include people who are from a mathematical and technical background, but also
    those that are from the business side, and usually they (business folks) don’t
    perform EDAs because they don’t have the technical means to explore the data,
    and to ask questions to the data.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这取决于。我真的很喜欢这个，因为想象一下，你是一名数据科学家，你正在进行探索性数据分析。如果你没有业务背景，你就无法理解你看到的数据或甚至你自己的分析是否真正正确，我正在思考这个EDA（探索性数据分析）或探索性数据分析的概念，就像下一个障碍，因为你可能已经在你的项目中看到了它。最好的EDA是那些包括来自数学和技术背景的人，但也包括来自业务方面的人，通常他们（业务人士）不进行EDA，因为他们没有技术手段来探索数据，向数据提问。'
- en: But this notion that you’re bringing, exploring the data and understanding that
    something is wrong, leads to the discussion where people are talking about “model
    hallucination.” I don’t like that expression, hallucination, because it’s not
    like a human, but they were talking about the model, and I feel like this chaining
    of capabilities shows that it’s not only about the model, it’s also about the
    data. Because you can have the best model, GPT-5 or whatever, and even if it’s
    very precise, we combine it with our information, which is feasible for a lot
    of scenarios, and we need to take care of that data so the LLM retrieves the good
    information. What’s your vision for these topics on generative AI in regards to
    CluedIn for this and the years to come? How do you see this evolution of a platform
    at a functional level?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 但你提出的这个概念，探索数据并理解某处出了问题，导致人们讨论“模型幻觉”。我不喜欢这个表达，幻觉，因为它不像人类，但他们是在谈论模型，我觉得这种能力的连锁展示表明，这不仅仅关于模型，也关于数据。因为你可以拥有最好的模型，GPT-5或任何其他，即使它非常精确，我们结合我们的信息，这在很多场景下是可行的，我们需要注意这些数据，以便LLM检索到好的信息。你对这些关于生成式AI的主题在CluedIn及其未来几年的愿景是什么？你在功能层面上如何看待这个平台的演变？
- en: '**T.W.**: You know, the part where I realized that LLMs were something super
    powerful was the first time it clicked that I can translate my input to a targeted
    output, where I can say, here’s what I want, and can you actually return your
    answer, like this JSON structure? And it was at that moment that I went back to
    our team, and I said, all right, let’s look through all of the functionality that
    includes, and I want us to go through and really understand, could I use AI in
    every different part of the platform? And you could look at some of the things,
    like that when people make a change we cause an audit trail, and you can think,
    no, that’s like a log, why would AI have anything to do with that? And you realize,
    well, some of these records over time, they change a lot, and instead of having
    to go through a huge changelog, can I summarize the history of the change? And
    you could literally put this in all different places and have a net positive.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**T.W.**: 你知道，我意识到LLMs（大型语言模型）非常强大的部分，是在我第一次想到可以将我的输入翻译成目标输出的时候，也就是说，我可以这样说，这是我想要的，你能实际返回这样的答案吗，就像这个JSON结构？就在那一刻，我回到我们团队，我说，好吧，让我们看看所有包含的功能，我希望我们能够真正理解，我能否在平台的每个不同部分使用AI？你可以看看一些事情，比如当人们做出改变时，我们会留下审计记录，你可以想，不，那就像一个日志，AI怎么会与此有关呢？你意识到，嗯，这些记录随着时间的推移会发生变化很多，而不是不得不通过一个巨大的变更日志，我能总结一下变更的历史吗？你几乎可以把这个应用到所有不同的地方，并且带来净正面效果。'
- en: For me, the vision is, where are the biggest wins? Where are the wins that are
    2 hours instead of 20 seconds? We need to focus on the things that actually are
    time-saving, and hit some type of business metric, make more money, lower operational
    costs, lower risk, complexity, etc. so we can just get things done without stressing
    all the time. So, I would argue to some degree, this space of AI is moving so
    fast, that for ChatGPT or even the DaVinci model, we haven’t milked all the value
    out of that. There’s just so much you could start to do, and, of course, the beautiful
    part is we get to wake up every day, and think…that same prompt, it’s just more
    reliable with an answer now, and I didn’t really have to do anything besides change
    to a model that also supports that functionality, like function calls, or completions,
    or something like that. For me, the vision of CluedIn’s use of GenAI is very much
    self-aware that you need AI to solve the data quality problem in a more complete
    way. You will still use traditional techniques that are kind of deterministic,
    I can sleep at night because it’ll do the same thing, every time, you still need
    those.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我来说，愿景是最大的胜利在哪里？那些需要2小时而不是20秒就能完成的胜利在哪里？我们需要关注那些真正能节省时间的事情，并达到某种业务指标，比如增加收入、降低运营成本、降低风险、复杂性等，这样我们就可以在不总是感到压力的情况下完成任务。所以，我认为在某种程度上，这个AI领域发展得太快，以至于对于ChatGPT甚至DaVinci模型，我们还没有完全挖掘出它们的价值。你可以开始做的事情太多了，而且，当然，美妙的部分是我们可以每天醒来，思考……同样的提示，现在有了答案，而且我实际上除了改变到一个也支持该功能（如函数调用、完成或类似的东西）的模型之外，真的不需要做任何事情。对我来说，CluedIn使用GenAI的愿景非常清楚，你需要AI以更完整的方式解决数据质量问题。你仍然会使用那种有点确定性的传统技术，我可以在晚上安心睡觉，因为每次它都会做同样的事情，但你仍然需要那些。
- en: And then, I think, what our job here at CluedIn is, I need to bring that to
    the data management process, so we can finally bring the business in, and make
    them responsible for data quality, because up to this point, I think it’s one
    of those elephants in the room we never talk about. Like why is data quality never
    cracked? Why is it never cracked? That’s just hard, and yes, it is, but actually,
    the thing is, we’ve never brought the business in and said, “Right, see this list
    of clients, that data comes from across 15 different places,” and so I love the
    fact that, even in their own 15 systems, it’s perfect.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我认为，我们在这里CluedIn的工作是，我需要将其带入数据管理流程，这样我们最终可以引入业务，让他们对数据质量负责，因为到目前为止，我认为这是房间里的大象，我们从未谈论过。比如为什么数据质量从未被解决？为什么从未被解决？这太难了，是的，它确实很难，但实际上，问题是，我们从未让业务介入并说，“好吧，看看这个客户列表，这些数据来自15个不同的地方，”所以我喜欢这样一个事实，即使在他们的15个系统中，它也是完美的。
- en: The problem, when we start to bring data together, is that there are things,
    despite all the governance that we could set up in the world, that take their
    own path, and someone needs to be responsible for that. And IT will still play
    a role in that, definitely in things like, “I can get the data to you reliably,
    I can get it with rollback, we can process it again really fast, we can scale,”
    but at the end of the day, someone from the business comes in and says, “That’s
    wrong and I know it because I work with this data every day.” And being an engineer
    myself, I know how to work with data, but I don’t know how to do those things,
    I have no idea how to just look at a record and go check if it meets all the right
    patterns. But I didn’t know that those were actually the same company, and there’s
    some real impact that happens if you don’t crack that, it could be as simple as
    sending the invoice to the wrong email address, and then you wonder why aren’t
    they paying us, and then you realize that’s not the team that pays it. And you
    go, “Wow, I’m now 30 days overdue,” and realistically, I need to send them another
    invoice and wait another 30 days,and cause maybe a cash flow issue. And you realize
    that’s where it really hits the bottom line, and you need to be exposed to that
    reality to then appreciate the effort of cracking that data quality. AI just in
    a way exacerbates the need for it, because as soon as you use it, it will stand
    out really obviously because of the form factor.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始汇集数据时的问题是，尽管我们可以在世界上设置所有的治理，但有些事情会走自己的路，有人需要对此负责。IT在这个问题中仍然会发挥作用，肯定是在像“我可以可靠地获取数据，我可以进行回滚，我们可以非常快地再次处理它，我们可以进行扩展”这样的方面，但最终，业务部门的人会进来并说，“这是错误的，我知道，因为我每天都在处理这些数据。”作为一个工程师，我知道如何处理数据，但我不懂如何做那些事情，我不知道如何仅仅查看一条记录并检查它是否符合所有正确的模式。但我不了解那些实际上是同一家公司，如果你不解决这个问题，可能会很简单，比如把发票发送到错误的电子邮件地址，然后你不知道为什么他们不付我们钱，然后你意识到那不是付款的团队。然后你去，“哇，我现在已经逾期30天了，”现实情况下，我需要给他们发送另一张发票并等待另一个30天，可能造成现金流问题。你意识到这真的触及了底线，你需要了解这个现实，然后才能欣赏解决数据质量问题所付出的努力。AI只是以一种方式加剧了这种需求，因为一旦你使用它，它就会因为形式因素而非常明显地突出。
- en: '**A.G.**: I love this part of the discussion. I was thinking about the roles
    and responsibilities of data governance in the company, even if you take a framework
    like DAMA or whatever, with the different roles, this is a new archetype, this
    is like the Ultron (Marvel reference) of the DAMA roles, like you have an Ultron
    (well, maybe not an Ultron, let’s get a Jarvis), a Jarvis working for you, to
    do the things that, let’s be honest, humans are not doing, because it’s a manual
    process, and we don’t have time to do that every time, every day, in a proper
    way.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我喜欢这个讨论的部分。我在想公司在数据治理中的角色和责任，即使你采用DAMA或任何框架，不同的角色，这是一个新的原型，这就像是DAMA角色的奥丁（漫威参考），就像你有一个奥丁（嗯，也许不是奥丁，让我们来一个贾维斯），一个为你工作的贾维斯，去做那些，让我们说实话，人类不做的事情，因为这些是手动过程，我们没有时间每次、每天以正确的方式去做。'
- en: 'Seth Juarez: From Generative AI Models to a Full LLM Platform'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seth Juarez：从生成式AI模型到完整的LLM平台
- en: '**A.G.**: So, what’s your role at Microsoft? What are you doing in the organization?'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 那么，你在微软的角色是什么？你在组织中做什么？'
- en: '![](assets/aoas_07in06.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_07in06.png)'
- en: '**S.J.**: So, I work in the Azure AI platform as a program manager. My job
    is to work in incubations and narrative. Those are the two mandates that I have.
    Incubations, meaning we build stuff. For example, if we want to explain how something
    works, we will build prototypes, etc. Wherever those prototypes may differ from
    what our product is doing, we feed back into our product prioritization and what
    it is that we build, just to make sure that the narratives work out, which leads
    us to the second part. We also do technical narrative, which is to explain what
    is going on and how these things work. Again, anytime the ideal story doesn’t
    match with product truth, we feed back into our product group, and sometimes they
    even put us in charge of certain features. We’re like every person, the primary
    goal being incubations, building samples and stuff that help people understand
    how to do this, and then the technical narrative that follows from that.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 因此，我在Azure AI平台工作，担任项目经理。我的工作是参与孵化和技术叙事。这是我拥有的两个任务。孵化意味着我们构建东西。例如，如果我们想解释某物是如何工作的，我们会构建原型等。无论这些原型与我们的产品有何不同，我们都会将其反馈到我们的产品优先级和构建内容中，以确保叙事能够顺利进行，这引出了第二部分。我们还进行技术叙事，即解释正在发生的事情以及这些事物是如何工作的。同样，任何理想的故事与产品真相不符时，我们都会将其反馈到我们的产品团队，有时他们甚至会让我们负责某些功能。我们就像其他人一样，主要目标是孵化，构建帮助人们理解如何做到这一点的样本和东西，然后是随之而来的技术叙事。'
- en: '**A.G.**: You are like Marvel’s The Watcher, seeing everything that’s happening
    at the product level, accelerators, repositories, prototypes, and new functionalities.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 你就像漫威的《守望者》，看到产品层面发生的所有事情，加速器、存储库、原型和新功能。'
- en: '**S.J.**: That’s right. But it’s not just me. Obviously, there’s a couple of
    us that work on this. For example, you may know [Cassie](https://oreil.ly/33PBM).
    She’s appeared on the [AI Show](https://oreil.ly/h-Fvw) as well, but her and I
    work together on this stuff.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 正是如此。但不仅仅是我在做。显然，我们有一群人在做这个。例如，你可能认识[Cassie](https://oreil.ly/33PBM)。她也出现在了[AI
    Show](https://oreil.ly/h-Fvw)上，但她和我一起做这个。'
- en: '**A.G.**: Wonderful. Thank you for the introduction. You have seen the whole
    evolution of Azure OpenAI Service, and the convergence with the rest of Azure
    AI Studio. How do you see something that started as a model, and now is becoming
    a platform, and a very good one with a lot of functionality?'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 太棒了。感谢你的介绍。你已经看到了Azure OpenAI服务的整个演变过程，以及与Azure AI Studio其他部分的融合。你是如何看待一个最初只是模型，现在却成为一个功能丰富的平台的发展？'
- en: '**S.J.**: That’s a great question. It turns out that there’s a lot of AI models
    and the Azure AI platform has been in the business of surfacing models, enabling
    you to customize these models and then enabling you to create your own models
    for a number of years, maybe half a decade already. The idea that a new model
    comes in, the reason why it was awesome for us is because we already had the infrastructure
    in place to make these models shine. While the new GPT series of models, and LLMs
    and AI models in general, may seem like a new thing when it comes to infrastructure
    and how these things are actually run, it’s not new to us. We were able to ramp
    up quickly and deliver these models to folks at scale, which is really nice, obviously
    in partnership with OpenAI.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 这是一个很好的问题。实际上，有很多AI模型，Azure AI平台已经从事模型曝光的业务，让您能够定制这些模型，并且已经允许您创建自己的模型多年了，可能已经有半个世纪了。一个新模型的出现，它对我们来说之所以很棒，是因为我们已经有基础设施来让这些模型发光。虽然新的GPT系列模型、LLMs以及AI模型在基础设施和这些事物实际运行方面可能看起来像是新事物，但这对我们来说并不新鲜。我们能够快速扩展并大规模地向人们提供这些模型，这真是太好了，显然是在与OpenAI的合作下。'
- en: '**A.G.**: That was one of the things I was surprised about during these recent
    months, to see this model as a service. Basically, being able to consume the APIs
    so quickly and so easily, that was almost magic for any developer out there.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 在最近几个月里，看到这个模型作为一个服务出现，这让我感到惊讶。基本上，能够如此快速和容易地消费API，对于任何开发者来说都像是魔法。'
- en: '**S.J.**: Yeah, and the cool thing is that the reason why it seemed magical
    is because we’ve been doing this, like I said, for a number of years with our
    Cognitive Services or AI services. We’ve been delivering, for example, text-to-speech,
    speech-to-text, translation, etc. as APIs. And those, if you think about them,
    are basically models as a service as well. They just happen to be more at the
    application layer, but it’s actually the same thing. These models as a service
    are similar. The weights happen to be different, and the model structure is different,
    but the things that are needed to run them are actually quite similar.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J**：是的，而且有趣的是，它之所以看起来神奇，是因为我们像我说的一样，已经用我们的认知服务或AI服务做了这件事好几年了。我们一直在提供，例如，文本到语音、语音到文本、翻译等作为API。如果你想想，这些基本上也是模型作为服务。它们只是碰巧更多地处于应用层，但实际上是同一件事。这些模型作为服务是相似的。权重可能不同，模型结构也不同，但运行它们所需的东西实际上非常相似。'
- en: '**A.G.**: Yes, I like this convergence with the AI Studio and the model catalog.
    Just to see all the models available out there, and not only Azure OpenAI. How
    do you see this platform evolving, in which we have different models, a full catalog,
    so easy to deploy, with the APIs out there, we just consume them, we have Llama,
    Mistral, and then we have this ecosystem of prompt flow that I know is another
    part of the platform, all the evaluation, etc. What’s going on? How do you see
    it?'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G**：是的，我喜欢与AI工作室和模型目录的这种融合。只是看看所有可用的模型，而不仅仅是Azure OpenAI。您如何看待这个平台的发展，其中我们有不同的模型，一个完整的目录，部署起来非常容易，有API可供使用，我们只需消费它们，我们有Llama、Mistral，然后我们还有这个提示流生态系统，我知道这是平台的一部分，所有的评估等等。发生了什么？您如何看待它？'
- en: '**S.J.**: Yeah, that’s a wonderful question. It turns out that we want to commoditize
    the ability for folks to find, consume, and refine models. That’s basically what
    we want to do. So generative AI happens to be one of the most exciting in my opinion.
    For example, if you have an idea and you want to add AI to it, the basic thing
    is go to the model catalog, see if you can find something, look at some of our
    services, and see if you can incorporate it. My particular opinion on this, and
    this is something that I’m coming to realize, is that much like in the early 2000-2005s,
    if you didn’t have a phone app, an iPhone app, people were like…are you a technical
    company?'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J**：是的，这是一个非常好的问题。实际上，我们希望使人们找到、消费和改进模型的能力商品化。这正是我们想要做的。所以，在我看来，生成式AI是最令人兴奋的之一。例如，如果你有一个想法，你想要添加AI，基本步骤是去模型目录看看是否能找到一些东西，看看我们的服务，看看是否能将其整合。我特别对这个的看法，这也是我逐渐意识到的事情，就像在2000-2005年早期，如果你没有手机应用，一个iPhone应用，人们会问……你是一个技术公司吗？'
- en: My sense is that people will think the same thing when it comes to AI experiences
    inside of your applications, where people will think if you do not include these
    things that normalize human interaction with computers, they’re going to feel
    like your app might be fundamentally broken. We’re coming to a place where we
    will no longer have to adapt to computers, but computers will adapt to us using
    AI, making experiences more natural. So my sense is that we need to start thinking
    about how we can add those niceties and soften the edges around our software,
    and how can AI help with those experiences? My sense is that we’re going to start
    to see these things included wholesale into almost everything we do, to the point
    where someone’s going to come to you in 2025 and be like, wow, your app doesn’t
    have AI? Maybe you should add it, because people will feel like it may be fundamentally
    broken.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我的看法是，当涉及到您应用程序内的AI体验时，人们可能会有相同的想法，他们会认为如果您不包含这些使人与计算机互动标准化的东西，他们可能会觉得您的应用程序可能从根本上是有缺陷的。我们正走向一个不再需要适应计算机，而是计算机通过AI来适应我们的地方，使体验更加自然。因此，我的看法是我们需要开始思考如何添加这些细微之处并软化我们软件的边缘，以及AI如何帮助这些体验？我的看法是我们将开始看到这些事物被大量地纳入我们做的几乎所有事情中，到2025年，有人可能会来找你，说：“哇，你的应用程序没有AI？也许你应该添加它，因为人们可能会觉得它可能从根本上是有缺陷的。”
- en: '**A.G.**: I love it. This is very related to the discussion we were having
    with Dr. John Maeda on Semantic Kernel, and the influence of design for artificial
    intelligence, and then this reflection about how AI just brings a new kind of
    interface, which is way beyond just the visual interface that we know today. And
    I know that this is just a personal opinion, something that you imagine, but what’s
    your vision of what’s going to happen in the next one or two years?'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我非常喜欢这个。这与我们与约翰·梅达博士讨论语义内核以及人工智能设计的影响非常相关，以及关于人工智能仅仅带来一种新类型的界面的反思，这种界面远远超出了我们今天所知道的视觉界面。我知道这仅仅是一个个人观点，一些你想象的东西，但你对未来一两年将要发生的事情有什么看法？'
- en: '**S.J.**: Two things, and they’re going to seem diametrically opposed, but
    they work together. The first is the expansion and proliferation of the use of
    these models inside of software. You’re going to see these models being used to
    do all sorts of things, to make experiences more delightful, to make experiences
    more user and customer focused. You’re going to see a big expansion of the use
    of these models, and we’re already seeing that today. I mean, it’s only been like
    a year and a couple of months since ChatGPT came out, and it’s actually now everyone’s
    expecting it to be part of the experience. So that’s getting bigger in terms of
    the volume of people using these things.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 两件事情，它们看起来似乎是截然相反的，但它们是相互配合的。第一点是这些模型在软件中的使用范围扩大和激增。你将看到这些模型被用于做各种事情，使体验更加愉悦，使体验更加以用户和客户为中心。你将看到这些模型的使用范围大幅扩大，而且我们今天已经看到了这一点。我的意思是，自从ChatGPT发布以来，还不到一年半的时间，现在每个人都期待它成为体验的一部分。所以，在用户使用量方面，它正在变得越来越大。'
- en: But there are also things that are going to get smaller. There’s going to be
    more specialized GenAI models that are going to become smaller and that are going
    to be used in a more targeted way to do specific things. Like the GenAI models
    we have now are quite general and big. However, you can make small language models
    (SLM), much smaller but more targeted. The smaller the model, the more targeted
    the task you need to make it do. You’re going to see the proliferation of small
    language models included perhaps even on devices within the next two to three
    years. You’re going to have those experiences move into a native local experience
    as well as a larger cloud experience together, and together those models are going
    to work to really get to laser focus into what the customer experience is for
    each task that it’s solving, as well as a general way to actually solve other
    language problems. This is for language models, for example. Two diametrically
    opposed things, things are going to get bigger in terms of volume of people using,
    and things are going to get smaller in terms of the models that people use, and
    those two things are going to be used in combination, I think quite effectively.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 但是也有一些事情会变得更小。会有更多专门化的通用人工智能模型变得较小，并且将以更针对性的方式用于完成特定的事情。就像我们现在拥有的通用人工智能模型相当通用且庞大。然而，你可以制作小型语言模型（SLM），它们更小但更具有针对性。模型越小，你需要让它完成的任务就越具有针对性。你将看到小型语言模型的激增，可能在接下来的两三年内，这些模型甚至会被集成到设备中。你将看到这些体验从本地原生体验转移到更大的云体验，并且这些模型将共同努力，真正聚焦于解决每个任务时的客户体验，以及以更一般的方式解决其他语言问题。这是针对语言模型的。两件截然相反的事情，一方面是使用这些模型的人数在增加，另一方面是人们使用的模型在缩小，我认为这两者将非常有效地结合使用。
- en: '**A.G.**: I believe so. For the second point that you mentioned, this kind
    of multilayer or multimodal architecture on which we could dispatch our first
    model, depending on the topic that it identifies. We can even fine-tune that first
    model and then use GPT-4 for one purpose and another model for another purpose.
    How do you see it?'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我相信是这样。对于你提到的第二点，这种多层或多模态架构，我们可以根据它识别的主题来部署我们的第一个模型。我们甚至可以微调第一个模型，然后使用GPT-4来完成一个目的，另一个模型来完成另一个目的。你怎么看待这个问题？'
- en: '**S.J.**: I think that’s a great way of putting it. We’re going to get into
    this multimodal, which means multiple models, as well as multimodal, which means
    multiple modalities like maybe speech, vision, and text, for example.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 我认为这是一种很好的表达方式。我们将进入多模态，这意味着多个模型，以及多模态，这意味着多种模态，比如可能是语音、视觉和文本，例如。'
- en: '**A.G.**: What about combining models of different providers?'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 那么结合不同提供商的模型呢？'
- en: '**S.J.**: Yeah, I think that’s great. In Azure, on Azure AI Studio, we really
    don’t care what models you bring, and we are trying to forge partnerships with
    multiple folks. We’ve [announced a partnership with Meta last year](https://oreil.ly/ehdf3),
    and some of the Llama 2 models are already in there. We recently [announced another
    partnership with Mistral](https://oreil.ly/SU0PT), and we have Mistral Large directly
    in our model catalog, and some of these models you can even fine-tune.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 是的，我认为这很好。在Azure上，在Azure AI Studio中，我们并不关心你带来什么模型，我们正在尝试与多个人建立合作关系。我们去年[宣布与Meta建立合作关系](https://oreil.ly/ehdf3)，其中一些Llama
    2模型已经包含在内。我们最近[宣布与Mistral建立另一项合作关系](https://oreil.ly/SU0PT)，Mistral Large模型直接在我们的模型目录中，而且其中一些模型你甚至可以进行微调。'
- en: But the reality is that the Azure AI platform is built on top of something we
    call Azure Machine Learning Studio, a general-purpose machine learning, MLOps
    platform for you to build any model that you like. In theory, you could start
    from something like PyTorch or TensorFlow, build your own model or do any code,
    and you could train those models directly in Azure Machine Learning, and surface
    them in AI Studio. The reality is, we really don’t care what AI models you bring,
    whether they’re stuff that we’ve forged through partnerships or things that you
    literally make yourselves, all of those things should and will be available to
    you in your applications.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 但现实是，Azure AI平台是建立在Azure Machine Learning Studio之上的，这是一个通用的机器学习、MLOps平台，你可以用它来构建你喜欢的任何模型。从理论上讲，你可以从PyTorch或TensorFlow开始，构建自己的模型或进行任何编码，你可以在Azure
    Machine Learning中直接训练这些模型，并在AI Studio中展示它们。但现实是，我们真的不关心你带来什么AI模型，无论是我们通过合作建立的，还是你亲自制作的，所有这些都应该并且将在你的应用程序中对你可用。
- en: '**A.G.**: Even that curation of models from Hugging Face, we have basically
    a variety of good models out there.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 即使是从Hugging Face精选的模型，我们也有各种各样的好模型。'
- en: '**S.J.**: Yeah, Hugging Face is an amazing partner. They’ve done a really good
    job of surfacing tons of functionalities, and we hope that with those functionalities,
    you will be able to deploy and use them reliably in your applications.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 是的，Hugging Face是一个了不起的合作伙伴。他们做了很多工作，展示了大量的功能，我们希望这些功能能够帮助你可靠地在你的应用程序中部署和使用它们。'
- en: '**A.G.**: Yes, and you mentioned machine learning operations (MLOps), now LLM
    operations, and we know that this is a new area. We’ve been discussing responsible
    AI. But if we go to the core aspect of measuring the performance of the models,
    how do you see this? Because this is evolving and getting more and more complex
    but also more intuitive. Because one year ago, it was not easy to know how to
    measure the performance of an LLM. I think that is getting clear now. Where are
    we going with that part?'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，你提到了机器学习操作（MLOps），现在LLM操作，我们知道这是一个新领域。我们一直在讨论负责任的AI。但如果我们深入到衡量模型性能的核心方面，你是如何看待这个问题的？因为这是在演变，变得越来越复杂，但也越来越直观。因为一年前，知道如何衡量LLM的性能并不容易。我认为现在这一点已经变得清晰了。我们在这一部分将走向何方？'
- en: '**S.J.**: Great question. We’ll start first with DevOps. DevOps is not an old
    concept, but it’s one that’s pretty well known. DevOps is a union of people, processes,
    and products to enable the continuous delivery of value. MLOps is the same thing.
    The union of people, processes, and products enable a continuous delivery of value,
    but with machine learning, LLMOps is the same thing but with LLMs. The idea of
    unifying this three-legged stool, people, process, and products, is super important.
    Because the thing about DevOps, LLMOps, and MLOps is that a product is not going
    to solve your process problem, and it’s not going to solve a people problem. If
    people don’t buy into the process, then no tool is going to satisfy that need.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**: 很好的问题。我们首先从DevOps开始。DevOps不是一个旧概念，但它是相当知名的。DevOps是人员、流程和产品的结合，以实现价值的持续交付。MLOps也是如此。人员、流程和产品的结合实现了价值的持续交付，但与机器学习结合时，LLMOps与LLMs相同。统一这个三脚架——人员、流程和产品——的想法非常重要。因为关于DevOps、LLMOps和MLOps，产品不能解决你的流程问题，也不能解决人员问题。如果人们不接受这个流程，那么没有任何工具能够满足这种需求。'
- en: I think that’s the first thing to bring out is that there is no magic bullet
    or elixir or product that’s going to solve a process thing, and the fact that
    people buy into the process. That’s the first thing. The second thing is, once
    you do want to buy into a process, people want to buy into a process, the question
    is what do we do in LLMOps to make this process reliable and useful, when it comes
    to how we evaluate the prompts that we do, and how to evaluate or how to make
    sure that the thing is working in production. In Azure AI Studio, we have a number
    of ways to do that.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为首先要指出的是，没有一劳永逸的解决方案或灵丹妙药或产品能够解决流程问题，以及人们相信流程的事实。这是第一点。第二点是，一旦你想要相信一个流程，人们想要相信一个流程，问题是我们如何在LLMOps中使这个流程可靠且有用，当我们评估我们做的提示时，以及如何评估或确保在生产中事物能够正常工作。在Azure
    AI Studio中，我们有多种方法来实现这一点。
- en: 'There’s two kinds of evaluations that I think exist (but obviously, this is
    an evolving space, and so we’re learning a ton here too). But the two kinds are
    unsupervised checks and then supervised checks. Supervised checks are probably
    the easiest. Imagine you have a call to an LLM and you want to make sure that
    it gets a certain answer out that’s similar to what you have. Basically, you would
    need to have a dataset of inputs and the expected outputs. To me, that’s a supervised
    test because you have the answer that you want. There’s a number of metrics that
    you can do. Here’s a simple one: for example, you can take the answer that’s ground
    truth, project that into, for example, an Ada embedding or any other embedding,
    and take the answer that the LLM gives you, project that into an LLM embedding
    and then measure the angle. Maybe there’s a particular tolerance that you have
    that gives a semantic closeness or meaning. That’s one way of checking.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为存在两种评估方式（但显然，这是一个不断发展的领域，所以我们在这里也在学习很多）。这两种方式是无监督检查和监督检查。监督检查可能是最简单的。想象一下，你有一个对LLM的调用，并且你想要确保它给出一个与你的预期相似的答案。基本上，你需要有一个输入和预期输出的数据集。对我来说，这是一个监督测试，因为你已经有了你想要的答案。你可以使用多种指标。这里有一个简单的例子：例如，你可以将真实答案投影到，比如说，一个Ada嵌入或其他任何嵌入中，然后将LLM给出的答案投影到一个LLM嵌入中，然后测量角度。可能有一个特定的容忍度，它给出了语义接近度或意义。这是一种检查方式。
- en: You can also use what we like to call GPT star metrics where you have the ground
    truth, and once you get the ground truth, you give that to a GPT star metric like
    similarity, and you ask the LLM as a language problem, how similar are these things
    on a scale of one to five? You give it some few-shot learning. That’s an example
    of two supervised learning methodologies, one that’s more empirical and one that’s
    more stochastic because it’s using the actual LLM to do it. The other one is also
    stochastic because it’s using embeddings, but you have a way of just projecting
    these things and then measuring an actual metric.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用我们喜欢称之为GPT star指标的方法，其中你有真实答案，一旦你得到真实答案，你就将其提供给一个GPT star指标，比如相似度，然后你作为一个语言问题询问LLM，这些事物在1到5的尺度上有多相似？你给它一些少样本学习。这是两种监督学习方法的例子，一种更实证，一种更随机，因为它使用了实际的LLM来完成。另一种也是随机的，因为它使用了嵌入，但你有一种方法可以直接投影这些事物并测量实际指标。
- en: 'Then there’s these other ones that I like to call unsupervised that look at
    the structure of the actual thing that’s going into the prompt and the context,
    and with the answer it measures what the structural integrity of the entire thing
    is. Let me give you one example: groundedness, which is a measure of how grounded
    the answer is in the context that was given to the LLM. For example, and this
    is the canonical example we use, we have a “Contoso” outdoor store and you ask
    a question about tents. Notice you have the question and then whatever the answer
    is, but the internal structure of the prompt flow or LangChain or whatever you’re
    using fetches some information from a data source, which we know to be true. That
    to me is the context, and that generally gets embedded directly into the prompt
    in something we call retrieval-augmented generation (RAG). You get the question,
    you fetch some data, you put it into the prompt. With this particular call to
    the LLM, you have a question, you have the answer, and then you have the context
    that was fetched. With those three things, we’re going to measure something called
    groundedness, which is how grounded the answer is that we fetched in the context,
    the answer that it generated from the context that we fetched. This is a normal
    interaction. You might ask me a question and I’ll start answering with facts and
    good faith, and you might say, “No, that’s not what I meant” and that’s totally
    cool. We want the LLM to be grounded in that way.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后还有一些我喜欢称之为无监督的，它们关注的是实际输入到提示和上下文中的结构，以及答案，它衡量的是整个结构的完整性。让我给你举一个例子：扎根度，这是衡量答案在提供给
    LLM 的上下文中的扎根程度的指标。例如，这是我们常用的一个典型例子，我们有一个“Contoso”户外商店，你问了一个关于帐篷的问题。注意，你有一个问题，然后是答案，但提示的内部结构，无论是
    LangChain 还是你在使用的其他工具，都会从数据源中获取一些信息，我们知道这是真实的。对我来说，这就是上下文，它通常直接嵌入到我们称之为检索增强生成（RAG）的提示中。你得到问题，你获取一些数据，你将其放入提示中。在这个特定的
    LLM 调用中，你有一个问题，你有一个答案，然后你有获取的上下文。有了这三样东西，我们将要衡量一个叫做扎根度的东西，即我们获取的答案在上下文中的扎根程度，以及它从我们获取的上下文中生成的答案。这是一个正常的交互。你可能会问我一个问题，我会开始用事实和诚意来回答，然后你可能会说，“不，这不是我想要的”那是完全正常的。我们希望
    LLM 以这种方式扎根。
- en: There’s a measurement on a scale of one to five of how grounded the answer is
    in the context we fetched and the question that was given, which is super nice.
    It turns out that this measurement is another GPT star metric where we give the
    question/context/answer, and then we say, “On a scale of one to five, how grounded
    is the answer in the context?” and then we do some few-shot learning in the actual
    prompt. This is something that you can also control. For example, you can change
    the entire groundedness prompt to directly match your few-shot learning priorities.
    Let’s just say you’re an outdoor company, you put those things in there and it’s
    able to do that. Those are the two kinds of evaluations that I see coming out.
    A supervised one where you have ground truth and you measure ground truth against
    the answers, and then unsupervised evaluations where you’re measuring the internal
    consistency of the truth versus the answers that you provide.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个衡量答案在获取的上下文和给出的问题中的扎根程度的量表，从一到五，这非常棒。结果证明，这个衡量标准是另一个 GPT 星级指标，我们给出问题/上下文/答案，然后我们说，“在一到五的量表上，答案在上下文中的扎根程度如何？”然后我们在实际的提示中进行一些少样本学习。这也是你可以控制的事情。例如，你可以改变整个扎根度提示，直接匹配你的少样本学习优先级。假设你是一家户外公司，你把这些东西放进去，它就能做到这一点。这些都是我看到即将出现的两种评估类型。一种是监督评估，你有一个真实情况，你将真实情况与答案进行衡量，然后是无监督评估，你是在衡量真实与提供的答案之间的内部一致性。
- en: There are other ones like fluency, coherence, relevance, and those are all GPT
    star unsupervised metrics, and you can even invent your own. There’s some clever
    ones that folks have invented like the apology metric, which is how many times
    it apologizes, and do we want to minimize that. But you’ve entered basically a
    world where you can evaluate these things in a way that is tailored to your business
    needs, your voice, and maybe even your ground truth, if that makes sense.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些，比如流畅性、连贯性、相关性，这些都是 GPT 星级无监督指标，你甚至可以发明自己的。有些人发明了一些很巧妙的指标，比如道歉指标，它衡量的是道歉了多少次，我们是否希望最小化这个数值。但你现在基本上进入了一个可以按你的业务需求、你的声音，甚至可能是你的真实情况来评估这些事物的世界，如果这样理解的话。
- en: '**A.G.**: That’s why the platform evolves so much along with the evaluation
    flows that we have on the platform, and what we can do with these metrics. By
    the way, you have explained this in a better way than the book. This is why I
    wanted to interview you, because I know that you are so good at explaining these
    terms! This is amazing.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：这就是为什么平台随着我们在平台上拥有的评估流程以及我们可以使用这些指标的能力而不断演变。顺便说一句，你比书中解释得更好。这就是为什么我想采访你，因为我知道你非常擅长解释这些术语！这太棒了。'
- en: '**S.J.**: Yeah. It’s like I said, I remember talking to some nontechnical folks,
    business folks, and they were concerned about using these LLMs because they’re
    like, well, how do we ensure that they’re doing the right thing? I showed them
    and they’re like, oh, so I can use English or the language of my choice, because
    they are trained in multiple languages, to actually evaluate these models. And
    I said, absolutely, and that’s not all. For those unsupervised tests, notice that
    you do not need ground truth. As you deploy in Azure AI Studio, you have something
    called a model data collector, which enables you to capture the input/context/answer
    and store it in your storage (we don’t see any of this stuff, we take this very
    seriously). Then even in production, you’re able to create jobs that look at this
    data and measure those same metrics, and even alerts you when those things go
    out of whack.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**：是的。就像我说的，我记得和一些非技术人士、商业人士交谈过，他们担心使用这些LLM，因为他们说，嗯，我们如何确保它们做的是正确的事情？我向他们展示了，他们就像，哦，所以我可以用英语或我选择的任何语言，因为它们在多种语言中受过训练，来评估这些模型。我说，绝对可以，而且不仅如此。对于这些无监督测试，请注意，你不需要真实标签。当你部署在Azure
    AI Studio时，你有一个叫做模型数据收集器的东西，它使你能够捕获输入/上下文/答案并将其存储在你的存储中（我们看不到任何这些内容，我们对此非常重视）。然后即使在生产中，你也能够创建查看这些数据并测量相同指标的工作，甚至在这些事情出现问题时提醒你。'
- en: Now we’re getting into the actual LLMOps, which enables the continuous delivery
    of value. That’s the thing. If the value goes down, you need to be alerted and
    to fix it, and then have that go back into the process. With these unsupervised
    evaluations and metrics, you’re able to actually run them on a subset of your
    production data if you so choose, which makes this even better for folks who are
    concerned about these things staying on track in inference time or production.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经进入了实际的LLMOps阶段，它使得价值的持续交付成为可能。这就是关键所在。如果价值下降，你需要得到警报并修复它，然后让它回到流程中。有了这些无监督评估和指标，如果你选择的话，你实际上可以在你的生产数据的一个子集上运行它们，这对于那些担心这些事情在推理时间或生产中保持一致的人来说，这甚至更好。
- en: '**A.G.**: That’s amazing. I just want to see the roadmap of what’s coming next
    in this aspect, because there is so much discussion on LLMOps, but it’s such an
    initial discussion for obvious reasons. This is a pretty new area. If you had
    to recommend one resource besides the AI Show, and the documentation, and all
    the official resources from Microsoft, do you have something in mind that you
    would recommend to learners and people who are listening to the discussion, that
    you consider good for their upskilling journey?'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：这太令人惊叹了。我只想看看这个方面接下来要发生什么的路线图，因为关于LLMOps的讨论有很多，但显然这是一个初步的讨论。这是一个相当新的领域。如果你必须推荐除了《AI
    Show》、文档以及来自微软的所有官方资源之外的资源，你有没有什么推荐给学习者以及正在听讨论的人，你认为这对他们的技能提升之旅是有益的？'
- en: '**S.J.**: Yes. I will say this, and this is going to be counterintuitive again
    because I like being counterintuitive. You are your own ultimate resource. My
    sense is that there is no amount of reading, or looking, or thinking about this
    stuff that really beats getting in there and trying something. That’s what I would
    suggest you do. Try something, make a prompt, have the answer come out and see
    what happens. In my mental model, maybe this will help: you should not think of
    these “LLM things” as repositories of knowledge. They’re not databases that have
    information. They are basically language calculators or language synthesizers.
    Think of your prompt as the language arithmetic that you’re putting into the LLM
    and think of the response as the answer.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.J.**：是的。我要说的是，这又将是一个反直觉的观点，因为我喜欢反直觉。你是你自己的终极资源。我的感觉是，没有多少阅读、观察或思考这些内容能真正胜过亲自尝试。这就是我建议你做的。尝试一下，做一个提示，看看答案出来会发生什么。在我的思维模式中，也许这会有所帮助：你不应该把这些“LLM事物”看作是知识的存储库。它们不是包含信息的数据库。它们基本上是语言计算器或语言合成器。把你的提示看作是放入LLM的语言算术，把响应看作是答案。'
- en: For example, this paragraph plus this paragraph minus this paragraph, what does
    that look like? I use LLMs like this all the time, even this morning. I wrote
    down a jumble of thoughts that I wanted to make so that it was smoothed out and
    GPT-4 was super nice and said, yeah, you should write it like this. It wasn’t
    perfect, but it enabled me to start with garbage, get something more refined,
    and now I could become an editor. Editing is much easier than creating. Think
    of LLMs as language calculators and start using them to solve tasks. Once you
    do that as your own resource, you’re going to get super far. These things are
    not hard to get started with, you just see the endpoint and you need to just put
    in a prompt and have something come out.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个段落加上这个段落减去这个段落，看起来会是什么样子？我经常使用LLMs，甚至今天早上。我写下了一堆我想表达的想法，以便使其变得平滑，GPT-4非常友好地告诉我，是的，你应该这样写。它并不完美，但它使我能够从垃圾开始，得到一些更精致的东西，现在我可以成为一个编辑。编辑比创作容易得多。将LLMs视为语言计算器，并开始使用它们来解决任务。一旦你将其作为自己的资源，你将走得很远。这些事情并不难开始，你只需要看到终点，然后输入一个提示，就会有所输出。
- en: '**A.G.**: Yeah, there are plenty of examples of notebooks where people can
    play with the API and test and see how it reacts. This notion of a calculator,
    I love it. It’s aligned with the notion of a copilot for a person that will be
    interacting with the system by using the linguistic ability of the models. It’s
    amazing.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，有很多笔记本的例子，人们可以在其中玩转API并进行测试，看看它的反应。这个计算器的概念，我非常喜欢。它与通过使用模型的言语能力与系统交互的人的Copilot概念相一致。这太神奇了。'
- en: 'Saurabh Tiwary: The New Microsoft Copilot Era'
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Saurabh Tiwary: 新的微软Copilot时代'
- en: '**A.G.**: For those who don’t know, could you please explain who you are and
    what your role at Microsoft is with which unit?'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 对于那些不了解情况的人来说，你能解释一下你是谁，你在微软的哪个部门担任什么角色吗？'
- en: '![](assets/aoas_07in07.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_07in07.png)'
- en: '**S.T.**: I lead a team called Turing, and the team has been training LLMs
    and applying them. It’s an applied team, so it uses these models in a variety
    of products from the Edge browser to Bing question answering, or if you are getting
    emails in Outlook, you will see those text predictions as you type, you will see
    the sentences complete, so a lot of features like that. More recently, my team
    has been driving the Copilot experience across many of the Copilots that Microsoft
    has announced. Most of the heavily used ones like the Windows Copilot, Edge Copilot,
    Bing, in a similar way, even on the enterprise side. The backend of all of those
    Copilots is the same with an extensibility model, and so my team is building that.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 我领导一个名为Turing的团队，这个团队一直在训练LLMs并将其应用于实践。这是一个应用型团队，因此它将这些模型应用于从Edge浏览器到Bing问答的各种产品中，或者如果你在Outlook中收发邮件，你会在输入时看到那些文本预测，你会看到句子被完成，所以有很多这样的功能。最近，我的团队一直在推动微软宣布的许多Copilot体验。其中大部分被广泛使用的，如Windows
    Copilot、Edge Copilot、Bing，以类似的方式，甚至在企业端也是如此。所有这些Copilot的后端都是相同的，具有可扩展性模型，因此我的团队正在构建这个模型。'
- en: '**A.G.**: It’s amazing. Legend has it that you created the Turing team, along
    with that first set of GPUs with which you were preparing the first models. That
    was some time ago.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 太神奇了。传说中你创建了Turing团队，以及那套第一组GPU，你用它们准备第一批模型。那是在很久以前。'
- en: '**S.T.**: Yeah, it was quite some time back, like maybe eight or nine years
    back, at least on the product side. Obviously, Microsoft Research has been pushing
    the state of the art for a very long period of time. But on the product side,
    I kind of bought the first GPUs, then built the first clusters, and ran the software
    layer on top of it, so that you can do some large-scale (which is not large-scale
    by any standards now), but some large-scale training on that small cluster, then
    evolve that to a bunch of GPU running in Azure, and now there we are, where even
    the inference GPUs are massive.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的，那是在很久以前，比如大约八到九年前，至少在产品方面。显然，微软研究部门一直在推动技术前沿很长时间。但在产品方面，我买了第一块GPU，然后构建了第一个集群，并在其上运行软件层，这样你就可以进行一些大规模的训练（现在按任何标准都不是大规模），在那个小型集群上进行一些大规模的训练，然后将其发展到Azure上运行的多个GPU，现在我们就在这里，甚至推理GPU也变得巨大。'
- en: '**A.G.**: You mentioned Microsoft Bing Chat, and Microsoft Copilot now. How
    was that journey? Because I think that after GitHub Copilot, this has been the
    best exponent of experimentation and learnings; it has even been shared by Jordi
    Ribas and the team on the blog, and it was amazing just to see what you were doing.
    How was that?'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：你提到了微软Bing Chat和微软Copilot。这个旅程是如何的？因为我认为在GitHub Copilot之后，这已经成为实验和学习最好的例证；它甚至被Jordi
    Ribas和他的团队在博客上分享，看到你所做的一切真是太令人惊叹了。那这个过程是如何的？'
- en: '**S.T.:** This is a phenomenal experience. Obviously, the team has been pushing
    very hard in terms of adding features, improving the experiences, etc. Let me
    maybe share a little bit of the backstory behind this. As I was mentioning, we
    were training our own LLMs in the past, and we had this conviction that conversational
    AI will be the next step in the journey. Even before ChatGPT or GPT-4 came out,
    we had our own conversational experience like a chat experience, which we were
    running in a stealth mode in India and MSIB countries (Malaysia, Singapore, Indonesia,
    and Philippines). We were working on it for a few years, I would say a couple
    of years before ChatGPT came out. We were iterating safety mechanisms, like not
    touching controversial topics, how do you address jailbreaks, etc. A lot of these
    things we were already experimenting with at a much smaller scale, and the surface
    area and the interaction mechanism was also a little bit different.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**：这是一个神奇的经历。显然，团队在添加功能、改进体验等方面付出了极大的努力。让我分享一下背后的故事。正如我提到的，我们过去一直在训练自己的LLM，并且我们有这样的信念，即对话式AI将是旅程的下一步。甚至在ChatGPT或GPT-4出现之前，我们就已经有了自己的对话式体验，就像聊天体验一样，我们在印度和MSIB国家（马来西亚、新加坡、印度尼西亚和菲律宾）以隐秘模式运行。我们为此工作了几年，可以说是在ChatGPT出现前两年。我们一直在迭代安全机制，比如不触及有争议的话题，如何处理越狱等问题。我们已经在更小的规模上对这些事情进行了大量实验，表面面积和交互机制也有所不同。'
- en: But, even within that experiment, we were finding that there was a lot of user
    engagement, in the sense that I remember one of the longer conversations was running
    for 13 or 14 hours. The user was talking to the bot for 13 to 14 hours straight
    with, I don’t know, maybe 1,800 messages back and forth going between the user
    and the bot, etc. That actually, that initial experiment gave us some baseline
    footing, so that when we got access to GPT-4, we had our paths somewhat mapped
    out. Hence, within a fairly short period of time, I think we got access to GPT-4
    somewhere around August or September of 2022, and then we released it within four
    or five months on February 7, 2023\. We released what was called the Bing Chat
    experience, which is right now the Microsoft Copilot experience.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，即使在那个实验中，我们也发现用户参与度很高，也就是说，我记得有一场持续时间较长的对话进行了13或14个小时。用户连续与机器人交谈了13到14个小时，我不知道，可能来回有1800条消息。实际上，那个初步实验为我们提供了一些基准，所以当我们获得GPT-4的访问权限时，我们的路径已经有所规划。因此，在相对较短的时间内，我想我们大约在2022年8月或9月获得了GPT-4的访问权限，然后在2023年2月7日前后四到五个月内发布了它。我们发布了所谓的Bing
    Chat体验，现在就是微软Copilot体验。
- en: It has been a fantastic journey, lots of late nights. Actually, the team worked
    through Thanksgiving and holidays and stuff like that, but it was a very exciting
    journey and seeing it populate across all the different surfaces that Microsoft
    has, whether it be Word, PowerPoint, M365, Edge, Bing as well as across our app
    family, the mobile app family, it has been just amazing. The partnerships have
    been great. The company has been working like one to propagate this belief or
    mission about Copilot across all these surfaces. If you take a step back, being
    able to do something like this is just phenomenal.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一次精彩的旅程，有很多个深夜。实际上，团队在感恩节和假日等时候也在工作，但这是一次非常激动人心的旅程。看到它在微软的所有不同表面上普及，无论是Word、PowerPoint、M365、Edge、Bing，还是我们的应用家族、移动应用家族，都令人惊叹。合作伙伴关系非常出色。公司像一个人一样努力传播关于Copilot的这种信念或使命，覆盖了所有这些表面。如果你退后一步，能够做这样的事情真是太神奇了。
- en: '**A.G.**: It’s true. From the field, I see it like this belief that there is
    a notion of Copilot that is everywhere, aligned with all the products. Even this
    end-to-end architecture of what a Copilot is, that is certainly new, but it’s
    something exciting. I feel like one of the most incredible things was to see the
    day-to-day, night-to-night, week-to-week progress of Bing Chat, Microsoft Copilot,
    all the learnings that you were sharing with the industry, even with the competitors
    on the blog, I was like, “This is gold!” All the improvements, I could really
    feel the improvements on the product. That pace of innovation is something that
    I think is difficult to replicate.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 这是真的。从现场来看，我觉得这是一种信念，即存在一种无处不在的Copilot概念，与所有产品相一致。即使是Copilot的端到端架构，这确实是一项新事物，但它非常令人兴奋。我觉得最令人难以置信的事情是看到Bing
    Chat、Microsoft Copilot日复一日、夜复一夜、周复一周的进步，你所分享给行业的所有学习成果，甚至在与博客上的竞争对手交流时，我都觉得这是“金子”！所有的改进，我都能真正感受到产品上的改进。这种创新的速度是我认为难以复制的。'
- en: '**S.T.**:I will say kudos to the team members who have been working super hard,
    and across Microsoft, working towards this common goal, and providing a delightful
    as well as a useful experience. It’s not just conversations. At the end of the
    day, we have our mission statement of making every person and organization in
    the world more productive. Along that particular goal, we don’t just want just
    a chit-chat experience. We also want people to accomplish things, do things. Within
    that mission, how do we connect all of the Copilots, add features like these plug-ins
    and GPTs that we have added? Actually, some of the things which might be coming
    up soon are even going towards task completion, etc. We are trying to evolve the
    product in a very significant way.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 我要称赞那些一直非常努力工作的团队成员，他们在微软内部共同努力，朝着这个共同目标前进，提供既愉快又实用的体验。这不仅仅是对话。最终，我们有我们的使命宣言，即让世界上每一个人和组织都更加高效。沿着这个特定的目标，我们不仅想要闲聊的体验。我们还希望人们能够完成事情，做事情。在这个使命中，我们如何连接所有的Copilot，添加我们添加的这些插件和GPT等特性？实际上，一些即将出现的事情甚至涉及到任务完成等。我们正在以非常显著的方式发展产品。'
- en: '**A.G.**: Yes, and the notion of the end-to-end Copilot. People ask, what’s
    Copilot exactly? Then there’s this and the other Copilot, all the Copilots on
    the products. But this notion of end-to-end Copilot that goes way beyond the model…it’s
    not just the model, it is the orchestration, it is the combination with Copilot
    for 365, etc. All this architecture, how do you design something that is so massive
    and so interesting from a combination of capabilities point of view?'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，以及端到端Copilot的概念。人们问，Copilot究竟是什么？然后有这个Copilot、那个Copilot，产品上的所有Copilot。但这个端到端Copilot的概念远远超出了模型……它不仅仅是模型，它是编排，是与Copilot
    for 365等的结合。所有这个架构，你是如何从能力组合的角度设计出如此庞大且有趣的东西的？'
- en: '**S.T.**: Yeah. The design principle of Copilot is that it is in the true sense
    a Copilot, as there is in aircraft. That if you are interacting with any piece
    of software from Microsoft, whether it be Teams or Outlook or wherever you are,
    the Copilot will be there next to you to help you. Obviously, it won’t be a static
    experience depending upon which interface you are in, so if you are at the operating
    system level, at Windows level, you may want to do system-level commands, like
    for example, turn on focus mode or change my Bluetooth settings or open an app.
    If you are in Outlook, you may want to summarize your email while having general-purpose
    conversations as well.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的。Copilot的设计原则是，它真正意义上是一个Copilot，就像飞机上的一样。如果你在与微软的任何软件进行交互，无论是Teams、Outlook还是其他任何地方，Copilot都会在你身边帮助你。显然，这不会是一个静态的体验，取决于你所在的界面，所以如果你处于操作系统级别，在Windows级别，你可能想执行系统级命令，比如打开专注模式或更改我的蓝牙设置或打开一个应用。如果你在Outlook中，你可能想在进行通用对话的同时总结你的电子邮件。'
- en: The way we have architected it is so that irrespective of whatever surface you
    are using the Copilot in, you get a slightly different experience, which is conditioned
    on what surface area you are in. That’s where we are pushing this default idea
    of a Copilot, that it will be there for you to help you do your work or do your
    task or whatever you’re planning to do, with a much better experience than if
    you were to do it alone.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的方式是，无论你在哪里使用Copilot，你都会得到一个略有不同的体验，这取决于你所处的表面区域。这就是我们推动这个默认的Copilot理念的地方，它将为你提供帮助，让你能够以比独自完成更好的体验来完成你的工作或任务或你计划要做的事情。
- en: '**A.G.**: Yes, and one of the original tasks, I mean, the main one is search.
    How did you and the team reinvent this notion of internet search by combining
    LLMs with traditional search? How was that, getting to that idea? I feel like
    it’s mind-blowing.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，其中一个原始任务，我的意思是，主要任务是搜索。你们是如何通过结合LLMs和传统搜索来重新发明互联网搜索这一概念的？你们是如何得到这个想法的？我觉得这太令人震惊了。'
- en: '**S.T.**: Yes, I have been working in search for quite some time. One of the
    things that we were thinking was that, if you look at us as a user, either it’s
    me or you or anyone else. Why do we want to search? Search is actually a simplification
    of what the technology can offer to us and users or us as humanity have gotten
    used to using it in a particular way, which is that, for example, you will see
    in search, if you look into search logs, you will see a lot of queries which are
    very like two words, three words each. You will see something like “best elementary
    school,” for example. Now, it’s very abstract. The reason why people search, the
    uber problem that the person may want to solve with a search like “best elementary
    school’” is something like…I have a kid who has been going to this prep school
    and is looking for something that is nearby and good quality, or maybe the user
    is planning to buy a house, and they’re wondering whether they should buy the
    house in this particular neighborhood or somewhere else and so on. There might
    be a lot of deeper intent, which the user does not express in search engines,
    because they have learned from past experience.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的，我在搜索领域工作已经有一段时间了。我们一直在思考的一件事是，如果你把我们当作用户，无论是我自己、你，还是其他任何人。我们为什么想要搜索？搜索实际上是对技术能为我们提供的东西的一种简化，而我们作为用户或人类已经习惯了以某种特定方式使用它，比如，在搜索中，如果你查看搜索日志，你会看到很多非常像两三个词的查询。例如，你会看到“最好的小学”这样的内容。现在，这非常抽象。人们搜索的原因，即使用“最好的小学”这样的搜索想要解决的问题可能是这样的……我有一个孩子一直在上这所预备学校，正在寻找附近且质量好的地方，或者用户可能在计划买房，他们在想是否应该在这个特定的社区买房，等等。可能会有很多更深层次的意图，但用户并没有在搜索引擎中表达出来，因为他们已经从过去的经验中学到了这一点。'
- en: People who have been in this business for a long time may remember [Ask Jeeves](https://oreil.ly/yuhNk),
    which existed around 20 to 25 years back. With Ask Jeeves, you were told to just
    express yourself in long sentences and it will find the information. At that time
    the technology was not great, and most of the time when you ask questions like,
    “I’m trying to buy a house, can you share what will be the good elementary school
    in this particular area?” and so on, it wouldn’t find the information, which is
    why search engines trained humans to type in very specific keywords. Even when
    you type those keywords, if you look into a user session or how we engage, what
    happens is we try to click on a search result, we read some content, then we click
    on something else, then we modify the queries, and for this example, we may find
    that there are public school and private school options. We might decide we should
    look into those depending upon whether they’re affordable or not, and then you
    iterate and so on. Given that the large language models have become a lot more
    powerful, we can try to compress this complex effort, which as humans we have
    broken down into these very specific sets. Let’s issue this first query, look
    at results, then modify it, then ask the other thing, and then the other thing.
    At the end of the day, you may open a map and then say, where is that region and
    where is the house, and what’s the distance, and so on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域工作很长时间的人可能还记得[Ask Jeeves](https://oreil.ly/yuhNk)，它大约在20到25年前存在。使用Ask Jeeves，你会被告知只需用长句表达自己，它就会找到信息。当时的技术并不出色，而且大多数时候当你提出像“我正在尝试买房，你能告诉我这个特定地区有哪些好的小学吗？”等问题时，它找不到信息，这就是为什么搜索引擎训练人类输入非常具体的关键词。即使你输入了这些关键词，如果你查看用户会话或我们如何互动，发生的事情是我们尝试点击搜索结果，阅读一些内容，然后点击其他东西，然后修改查询，在这个例子中，我们可能会发现存在公立学校和私立学校的选择。我们可能会决定我们应该根据它们是否负担得起来考虑这些选择，然后你迭代等等。鉴于大型语言模型变得非常强大，我们可以尝试压缩这个复杂的工作，这是我们人类将这些工作分解成非常具体的集合。让我们发出第一个查询，查看结果，然后修改它，然后询问其他事情，然后是其他事情。最终，你可能会打开地图，然后说，那个地区在哪里，房子在哪里，距离有多远，等等。
- en: Instead of that, with LLMs and with this Copilot experience, you can type what
    you really want. You can say, “I’m new to this area, I’m looking into buying a
    house, I have two young kids, where should I find good schools in this region?”
    etc. You can express all of that. Then the model is actually, as a Copilot, breaking
    that, because they know they have access to the search engine, the model itself.
    It will now break your complex scenario into smaller subcomponents, issue search
    queries, look at results, follow up again, etc., and then provide that comprehensive
    view. Instead of you doing all of these tasks, the model is helping you do that.
    That was our initial thing, because we have seen our users struggle in a session,
    trying lots of different things, iterating, making changes, etc. Anyways, that’s
    the story behind this improved search experience with the Copilot.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与其那样，使用大型语言模型（LLMs）和这个Copilot体验，你可以输入你真正想要的内容。你可以这样说，“我对这个领域不太熟悉，我正在考虑买房，我有两个孩子，我应该在这个地区哪里找好的学校？”等等。你可以表达所有这些内容。然后，作为Copilot，模型实际上会将其分解，因为它们知道它们可以访问搜索引擎，即模型本身。现在，它会将你的复杂场景分解成更小的子组件，发出搜索查询，查看结果，再次跟进，等等，然后提供全面的视图。不是你完成所有这些任务，而是模型在帮助你完成这些任务。这就是我们的初衷，因为我们看到我们的用户在会话中挣扎，尝试很多不同的事情，迭代，做出改变，等等。无论如何，这就是Copilot改进搜索体验背后的故事。
- en: '**A.G.**: I guess you had started with the models, and then there was the orchestration
    part, now we talk about LlamaIndex, LangChain, Semantic Kernel, but [Prometheus](https://oreil.ly/Vkgtt)
    was there. That concept of orchestrating knowledge and combining pieces, skills,
    etc.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：我想你们最初是从模型开始的，然后是编排部分，现在我们谈论LlamaIndex、LangChain、Semantic Kernel，但[Prometheus](https://oreil.ly/Vkgtt)已经存在了。这种编排知识和结合片段、技能等的概念。'
- en: '**S.T.**: Yes, one of the other things for people who have been playing, or
    trying to build bots using LLMs, with one of these options like LangChain and
    stuff like that, one of the challenges that initially doesn’t show up, but it
    happens when you start doing more complex things, is that when you have a database,
    or an interface through which you can pull in information, it’s very easy to write
    a prompt, and you can add things over there. In Microsoft’s case, there are many,
    many different complex things which the system can access. For example, even for
    Bing, there is a search index which is there, and that is one interface. But we
    also have our Ads engine, we have introduced image creation as a capability, we
    have GPT4-V for image understanding. There are many, we have actually introduced
    this notion of plug-ins if you want real-time flight information from KAYAK, being
    able to access that.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的，对于那些已经玩过或者尝试使用LLM（大型语言模型）来构建机器人的人来说，使用像LangChain这样的选项，最初可能不会显现出挑战，但当你开始做更复杂的事情时，挑战就会出现。当你有一个数据库或者一个可以拉取信息的接口时，编写提示词非常容易，你可以在那里添加内容。在微软的情况下，系统可以访问许多不同的复杂事物。例如，即使是Bing，也有一个搜索索引，这是一个接口。但我们还有我们的广告引擎，我们引入了图像创建作为一项功能，我们有GPT4-V用于图像理解。有很多，我们实际上引入了插件的概念，如果你想要从KAYAK获取实时航班信息，能够访问到这些信息。'
- en: Once you start adding many of these pieces, what happens is if you start using
    the raw prompt-based method, the prompt will become just ginormous, and even the
    model quality will suffer because it now has to follow instructions, a very long
    instruction set, and just like humans where if you give too many instructions,
    you will be confused at the end of the page about what was written at the top.
    You see similar behaviors over here as well, that the model quality may not follow
    everything to the right intent of what is written. Hence, we had to build a more
    sophisticated orchestration engine which can do state-based prompting, it can
    do dynamic prompting so that the prompt which is sent to the model is a lot smaller.
    There’s a lot of sophistication which has gone inside it so that we can provide
    a really compelling experience for our users.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始添加许多这些组件，如果你开始使用基于原始提示的方法，提示词会变得非常大，甚至模型质量也会受到影响，因为现在它必须遵循指令，一个非常长的指令集，就像人类一样，如果你给出太多的指令，你会在页面底部对上面写了什么感到困惑。你在这里也会看到类似的行为，即模型质量可能不会完全遵循所写内容的正确意图。因此，我们必须构建一个更复杂的编排引擎，它可以进行基于状态的提示，可以进行动态提示，这样发送给模型的提示词就会小得多。它内部有很多复杂性，这样我们才能为用户提供真正令人信服的体验。
- en: '**A.G.**: Totally. There’s the model, the orchestration, and the third leg
    of this chair, I think, is the user interface. How did you experiment with that
    experience, how to adapt, because there is a learning process for users from traditional
    keyword search to something where we will be interacting and then understanding
    the results? How was that, all that experimentation?'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 完全同意。这里有模型、编排，这个椅子的第三条腿，我认为，是用户界面。你是如何实验这种体验的，如何适应，因为用户从传统的关键词搜索到我们将要互动并理解结果的过程有一个学习过程？这个过程是如何的，所有的实验都进行了哪些？'
- en: '**S.T.**: Yeah, right now, there are some standard interfaces which a lot of
    companies who are entering this area have settled on. But since we were almost
    first to market, we had to design and iterate on this a lot. From day zero when
    we released the product to where it is now, it has gone through a ton of iterative
    improvements. One of the first things that we did was we introduced the conversational
    interface on top of our search engine. So the question was, well, most of the
    users don’t even know this product exists. So how would they engage? Because maybe
    you and I are more connected to technology, and maybe we have read the latest
    news, but how does a common user engage with this kind of technology? How do they
    even know this is there? One of the very subtle things that we did was on Bing.com,
    you would have icons to go to Copilot, like you can click and go on. But if you
    do a mouse scroll, you can switch seamlessly between conversations and search
    results. So that provides a very natural way in which I type this query and most
    of the time, the user will be typing the query for the search results. They get
    the search results, but let me just do a mouse scroll, just a couple of clicks.
    You land into the conversational interface and you can have that conversation.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的，目前，许多进入这个领域的公司都选择了一些标准接口。但因为我们几乎是第一个进入市场的，所以我们不得不设计和迭代这个接口很多次。从产品发布的第一天到如今，它已经经历了很多次迭代改进。我们最早做的事情之一就是在我们的搜索引擎上引入了对话界面。所以问题是，大多数用户甚至不知道这个产品的存在。他们该如何参与进来呢？因为我们可能你和我对技术更熟悉，也许我们阅读了最新的新闻，但普通用户如何与这种技术互动？他们甚至怎么知道它的存在？我们做的非常微妙的一件事是在Bing.com上，你可以点击图标进入Copilot，就像你可以点击并进入。但如果你滚动鼠标，你可以无缝地在对话和搜索结果之间切换。这样提供了一种非常自然的方式，我输入这个查询，大多数时候，用户也会输入查询以获取搜索结果。他们得到了搜索结果，但让我只是滚动鼠标，点击几下。你就可以进入对话界面，并进行对话。'
- en: There were other very subtle things that we did, for example, for question answering
    or weather, sports, etc. We had these follow-up queries, which we show on the
    Bing search page itself. When you click on that, you land into the chat interface.
    This is how we were trying to educate, instead of having a tutorial or a notification
    bar that now you can do this, we were having these subtle ways in which, now if
    you click on this, you see you go into a conversation interface, the Copilot will
    respond to your follow-on question and then the user knows that, OK, there is
    something else, something smarter, richer, which is behind the scenes, and then
    they can engage further with that.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还做了其他一些非常微妙的事情，例如，对于问答或天气、体育等。我们在Bing搜索页面上展示了这些后续查询。当你点击它时，你会进入聊天界面。这就是我们试图教育用户的方式，而不是通过教程或通知栏告诉他们现在可以这样做，我们通过这些微妙的方式，现在如果你点击这个，你会看到你进入了一个对话界面，Copilot会回答你的后续问题，然后用户就会知道，好吧，背后还有其他一些更智能、更丰富的东西，然后他们可以进一步参与其中。
- en: '**A.G.**: Yeah, totally product-led. So people will just get into a product
    and they will learn in an organic way how to handle this new functionality, very
    smart. So, because this book is about Azure OpenAI, then this transition, this
    evolution towards the Microsoft Copilot notion, this new era of AI that we are
    living today…what’s your vision of what’s next and the evolution in the industry
    or even the upcoming research topics that you think may be interesting to take
    a look at?'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，完全是以产品为导向。所以人们会直接进入产品，并以一种有机的方式学习如何处理这种新的功能，非常聪明。因此，因为这本书是关于Azure
    OpenAI的，所以这种过渡，这种向Microsoft Copilot概念、我们今天所生活的这个AI新时代的演变……你对未来有什么愿景？在行业或即将到来的研究主题中，你认为哪些可能是有趣的，值得一看的？'
- en: '**S.T.**: Yeah, I mean, that’s a very important and challenging question. A
    lot of people have argued that the last year has been transformational and a lot
    of stuff has been happening. But given the trend lines that we are seeing, I think,
    and it is hard to believe, but I think the rate of change will actually accelerate
    and not slow down, that’s our belief. I know we have released Copilot across many
    surfaces, but the way we see the Copilot today, I believe it is going to evolve
    from these conversational text-based interfaces to a much richer experience very,
    very quickly. Yes, typing is something and there are certain places, so for example,
    messaging apps, etc., have educated us that you can type and you can get back
    and forth conversations, etc. We are following that mode. But models are becoming
    more powerful, think about true multimodality, just like we have as humans.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 是的，我的意思是，这是一个非常重要且具有挑战性的问题。很多人认为去年是变革性的，发生了许多事情。但鉴于我们看到的趋势线，我认为，虽然很难相信，但我认为变化的速率实际上会加速而不是减缓，这是我们相信的。我知道我们已经将Copilot发布到许多表面，但就我们今天看到的Copilot而言，我相信它将从这些基于对话的文本界面迅速演变成为一个更加丰富的体验。是的，打字是一种方式，有些地方，例如，消息应用等，已经教育我们，你可以打字，你可以进行来回的对话，等等。我们正在遵循这种模式。但模型变得更加强大，想想真正的多模态，就像我们人类一样。'
- en: For example, when I’m talking to you, I’m seeing this Microsoft Teams window,
    I’m seeing your face, I also see your name written next to it. As a human, I’m
    not going into a text mode or an image mode like, I’m just looking at the face
    and now I’m going to read the text, etc. Everything is very seamless for me when
    I’m looking at your screen. In a similar way, even for the Copilot, we will start
    seeing things where the engagement will be very natural. People could speak and
    the model could produce an image. They could actually put in an image with some
    text and it may respond back, etc. Very similar, it’s not going through pipelines
    of like, this model has got called and then text to speech, and so on. So that
    is one axis which will be fairly powerful that the models may start operating
    at the pixel level. So instead of a text model or image model, it just looks at
    pixels and some of those pixels end up being text and some of them end up being
    images and that’s OK.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我与你交谈时，我可以看到这个微软团队窗口，我看到你的脸，我还能看到你名字旁边写的名字。作为一个人类，我不会进入文本模式或图像模式，我只是看着你的脸，现在我准备阅读文本，等等。当我看着你的屏幕时，一切对我来说都非常流畅。以类似的方式，即使是对于Copilot，我们也将开始看到那些互动将非常自然的地方。人们可以说话，模型可以生成图像。他们实际上可以插入带有一些文本的图像，它可能会做出回应，等等。非常相似，它不是通过像这样的管道，这个模型被调用，然后是文本到语音，等等。所以这是一个相当强大的轴，模型可能会开始以像素级别进行操作。所以，不是文本模型或图像模型，它只是查看像素，其中一些像素最终变成文本，而另一些像素最终变成图像，这是可以的。
- en: Another thing that I feel is where the world is heading is towards this behavior
    of agents. Meaning, right now, I was giving the search example earlier that you
    had a very complex task, and as a user we were interacting with the search engine
    by breaking down that problem into very small pieces, issuing those queries, looking
    at results, and we were the orchestrator in our mind, doing this complex thing
    even though we never explicitly said that we are the orchestrator. I think the
    same is true even with our Copilot today, they are limited to a certain extent
    because they are a lot about information gathering. We are trying to do some of
    the task completions with the KAYAK reference that I was adding.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事，我觉得世界正朝着代理的行为发展。这意味着，现在，我之前给出了搜索的例子，你有一个非常复杂的任务，作为用户，我们通过与搜索引擎交互，将问题分解成非常小的部分，发出这些查询，查看结果，我们在心中是协调者，即使我们从未明确说过我们是协调者。我认为我们的Copilot今天也是这样，它们在某种程度上受到限制，因为它们很多是关于信息收集的。我们正在尝试使用我添加的KAYAK参考来完成一些任务。
- en: Ultimately, what you want to do is, one of the popular examples in our Copilot
    is, “Can you plan an itinerary for me?” If I’m going on vacation, do whatever,
    pick your favorite place. London, for example. It will search, pick up various
    places, etc. It can create an itinerary, and it can also recommend hotels and
    so on. But ultimately, what you want to do is, you want to just plan the vacation.
    Because right now, for example, my wife hands that task to me, and it’s super
    painful. I need to go read all the reviews, make sure that the hotels are good,
    nearby to the places we want to go, etc. I have to do a lot of these things, then
    go to the website, search for the hotel, do the booking, and so on.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你想要做的事情是，在我们Copilot的流行例子中之一是，“你能为我规划一个行程吗？”如果我正在度假，随便什么，选你最喜欢的地方。比如伦敦。它会搜索，挑选各种地方，等等。它可以创建行程，也可以推荐酒店等等。但最终，你想要做的是，你只想规划这次度假。因为现在，比如，我的妻子把这个任务交给我，这非常痛苦。我需要去读所有的评论，确保酒店是好的，离我们想去的地方近，等等。我必须做很多这些事情，然后去网站，搜索酒店，进行预订，等等。
- en: I would say it is not a simple process. It takes quite a bit of cognitive load
    as well as time and energy. But if you think about it, a lot of these capabilities
    are there on the web. They are designed for humans because we can open a browser,
    we can click on results, we can book stuff, and so on. I think where these models
    are heading is towards having this agentic behavior or an agent-like behavior
    where you just tell them, obviously with some user interface, etc. Obviously,
    you can’t run it in a fully autonomous way. “I want to go to London for X number
    of days. Can you plan that trip?” But plan here means really planning that trip
    and not just a text interface. Where it goes and it starts doing booking, looking
    for flights, and all of that. Checking weather, that’s another thing, whether
    the weather is good or not. If it is not, if it is going to rain on a particular
    day, maybe you want to plan some different activities on those days, etc. Being
    able to do that where the Copilot on users’ behalf can start doing those types
    of things like open web pages, clicking on things, and these are all arbitrary
    interfaces. I feel like that’s where the world is heading towards.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以说这并不是一个简单的过程。它需要相当多的认知负荷，以及时间和精力。但如果你想想，网络上有许多这些能力。它们是为人类设计的，因为我们可以打开浏览器，我们可以点击结果，我们可以预订东西，等等。我认为这些模型的发展方向是拥有这种代理行为或类似代理的行为，你只需告诉它们，显然还需要一些用户界面等等。显然，你不能完全自主地运行它。“我想去伦敦X天。你能帮我规划这次旅行吗？”但这里的计划意味着真正地规划这次旅行，而不仅仅是文本界面。它会开始进行预订，寻找航班，等等。检查天气，这也是另一件事，看天气是否好。如果不好，如果某天会下雨，你可能想在那些天规划一些不同的活动等等。能够代表用户去做这些事情，比如Copilot可以开始打开网页，点击东西，这些都是任意的界面。我感觉这就是世界正在走向的方向。
- en: '**A.G.**: Yes, I think so. It’s not even orchestration. It’s a multilayer architecture
    in which you are going and performing all these actions, and instead of being
    the agent ourselves and booking all the vacations, the hotel, and the trip, etc.,
    we have a system that is relying on the existing interfaces that we have today,
    the frontends and the backends.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**：是的，我认为是这样。这甚至不是编排。这是一个多层架构，你正在执行所有这些动作，而不是我们自己作为代理去预订所有的假期、酒店和旅行等等，我们有一个系统，它依赖于我们今天已有的现有接口，即前端和后端。'
- en: '**S.T.**: Think of it as your own personal assistant. If you had a personal
    assistant who really knew you well, obviously, it’s just like if you were to hire
    a new personal assistant for yourself. Initially, it will be like, OK, just do
    simple things, etc. But at some point when the trust grows, and you know that
    person can do these things and they understand my interests and boundaries and
    so on, they can start acting on your behalf much, much more. I think that’s where
    I feel like these models will go. On day zero, obviously, they won’t do anything
    because people will be freaked out like, why did you make this particular choice
    or that particular choice? But as things evolve, I think we are going toward that
    future.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**：把它想象成你自己的私人助理。如果你有一个非常了解你的私人助理，显然，这就好像你为自己雇佣了一个新的私人助理。最初，可能就是做一些简单的事情等等。但到了某个时候，当信任建立起来，你知道那个人可以做到这些事情，并且他们理解我的兴趣和界限等等，他们就可以开始代表你做很多事情。我认为这就是我觉得这些模型会走向的方向。在零天，显然，它们什么都不会做，因为人们会感到恐慌，为什么你做出了这个特定的选择或那个特定的选择？但随着事情的发展，我认为我们正走向那个未来。'
- en: '**A.G.**: I think so, it’s very promising what we’ll see in the next one, two,
    three years. That’s why I don’t ask for your vision for the next five years because
    I know that it’s impossible to do so. Look, this is super interesting, wonderful
    insights. I’m just so happy that you are sharing all this information here. Did
    you have a last recommendation for our readers to continue exploring?'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 我认为如此，我们将在未来一两年内看到的事情非常有希望。这就是为什么我不要求你为未来五年提供愿景，因为我知道这是不可能的。看，这非常有趣，见解非常精彩。我非常高兴你能在这里分享所有这些信息。你对我们读者有什么最后的建议，以继续探索？'
- en: '**S.T.**: I think this field is moving so fast that, personally for me because
    I have been in this area for the last 9 to 10 years, I think a lot of the social
    sites generally are useful, Twitter, if you follow the right set of people and
    so on. Right now, actually, things have evolved a lot. There are newsletters that
    are there, which actually provide the latest, and are actually using LLMs to simplify
    and summarize a lot of the conversation so that you can have a synthesized set
    of information and obviously you can drill down a little bit more if something
    is of interest. I think those are probably the more interesting routes.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 我认为这个领域发展得如此之快，对我个人来说，因为我已经在这个领域工作了9到10年，我认为很多社交网站通常是有用的，比如Twitter，如果你关注正确的一群人等等。现在，实际上，事情已经发生了很大的变化。有一些新闻简报，它们实际上提供了最新的信息，并且实际上使用LLMs来简化并总结很多对话，这样你就可以获得一个综合的信息集，显然，如果你对某件事感兴趣，你可以进一步深入了解。我认为这些可能是更有趣的途径。'
- en: I can give one other example. Generally, conferences were used for knowledge
    dissemination. Right now, if you look into the deep learning space, conferences
    are mostly for networking. The papers were already put on [arXiv](https://oreil.ly/O9tW3)
    probably three months to six months back. If they were useful, they would have
    already been discussed till death by the time the conference happens. Even the
    research community has shaped itself. Then I will say the same is true for us
    as well. I would say just keep being on the cutting edge and the latest edge.
    I think things are evolving very quickly in this world.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以再举一个例子。通常，会议用于知识传播。现在，如果你看看深度学习领域，会议主要是为了建立联系。论文可能已经在上次会议前三个月到六个月就放在了[arXiv](https://oreil.ly/O9tW3)上。如果它们有用，那么在会议发生时，它们可能已经被讨论到无休无止。甚至研究社区本身也形成了。那么，我想对我们来说也是如此。我会说，只需保持处于最前沿和最新的前沿。我认为这个世界上的事情发展非常快。
- en: '**A.G.**: So quick. Even this Azure OpenAI book has evolved a lot, I don’t
    know how many iterations every week, just to keep updating and adding all the
    information.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 所以很快。即使是这本Azure OpenAI的书也发展了很多，我不知道每周有多少次迭代，只是为了不断更新和添加所有信息。'
- en: '**S.T.**: Maybe even after publishing, you may have to iterate on, I don’t
    know, like a book pointed to an appendix or something.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**S.T.**: 也许在出版后，你可能还需要迭代，比如指向附录或什么的书。'
- en: '**A.G.**: Yeah, appendix, second, third edition…almost every month.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**A.G.**: 是的，附录，第二版，第三版……几乎每个月都有。'
- en: Conclusion
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'The experts interviewed in this chapter offer exclusive insights related to
    the current state of generative AI and LLMs (e.g., why data quality matters for
    RAG patterns, new LLMOps trends for model performance tracking, advanced use cases,
    the underlying cloud native infrastructure) but also insights into its future.
    There are two recurrent topics here: multimodality for models to analyze different
    kinds of information, and the evolution of AI agents, as a sort of combined automation
    of steps leading to complete complex tasks—all of this without forgetting the
    responsible and safe implementation of generative AI systems.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本章采访的专家们提供了与生成式AI和LLMs（例如，为什么数据质量对RAG模式很重要，新的LLMOps趋势用于模型性能跟踪，高级用例，以及底层云原生基础设施）相关的独家见解，同时也提供了对其未来的见解。这里有两个反复出现的话题：多模态模型用于分析不同类型的信息，以及AI代理的演变，作为一种将步骤自动化以完成复杂任务的组合——所有这一切都不要忘记生成式AI系统的负责任和安全实施。
- en: Overall, this last chapter was my way to bring together all the topics from
    Chapters [1](ch01.html#introduction_to_generative_ai_and_azure_openai_ser) to
    [6](ch06.html#elaborating_generative_ai_business_cases), and to discuss them in
    a highly applied manner with an amazing set of professionals. From technology
    building blocks and architectures to organizational considerations. From Azure
    OpenAI Service to the rest of the related “pieces” that enable your cloud native
    generative AI developments.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这一章是我将第[1](ch01.html#introduction_to_generative_ai_and_azure_openai_ser)章到第[6](ch06.html#elaborating_generative_ai_business_cases)章的所有主题汇集在一起的方式，并且与一组令人惊叹的专业人士以高度应用的方式进行了讨论。从技术构建块和架构到组织考虑因素。从Azure
    OpenAI服务到其他能够支持您云原生生成式AI开发的“相关部分”。
- en: And here we are, at the end of this book’s journey. More than 200 pages full
    of explanations, examples, and technical topics related to generative AI and Azure
    OpenAI. But this is, of course, just the beginning. Companies are adopting Azure
    OpenAI, and the Microsoft product teams are working to continue evolving the platform,
    by adding not only new AI models, but also product features related to the operationalization
    of enterprise-level deployments. This is an amazing race, and the generative era
    has just started. This book is a small contribution for AI adopters around the
    world to get the most from Azure OpenAI Service. Let’s keep innovating.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来到了这本书的终点。超过200页，充满了与生成式AI和Azure OpenAI相关的解释、示例和技术主题。但当然，这只是一个开始。公司正在采用Azure
    OpenAI，微软的产品团队正在努力继续演进该平台，不仅添加新的AI模型，还添加与企业级部署的运营化相关的产品功能。这是一场惊人的竞赛，生成式时代才刚刚开始。这本书是对全球AI采用者的小小贡献，以便他们从Azure
    OpenAI服务中获得最大收益。让我们继续创新。
