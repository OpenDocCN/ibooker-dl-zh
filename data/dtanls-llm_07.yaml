- en: 6 Analyzing images and videos
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 分析图像和视频
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Analyzing images
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析图像
- en: Comparing images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较图像
- en: Analyzing videos
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析视频
- en: In the previous chapters, we have seen how to analyze text and structured data.
    Does that cover everything? Not even close! By far, the largest portion of data
    out there comes in the form of images and videos. For instance, videos alone account
    for an impressive two-thirds of the total data volume exchanged over the internet!
    In this chapter, we will see how language models can also help us extract useful
    insights from such data types.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何分析文本和结构化数据。这涵盖了所有内容吗？远远没有！到目前为止，最大的数据部分是以图像和视频的形式存在的。例如，仅视频就占互联网交换的总数据量的令人印象深刻的三分之二！在本章中，我们将看到语言模型如何帮助我们从这些数据类型中提取有用的见解。
- en: The following sections introduce a couple of small projects that process images
    and video data. GPT-4o is a natively multimodal model; we can use it for all these
    tasks. First, we will see how to use GPT-4o to answer free-form questions (in
    natural language) about images. Second, we will use GPT-4o to build an automated
    picture-tagging application, automatically tagging our holiday pictures with the
    people who appear in them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节介绍了一些处理图像和视频数据的小型项目。GPT-4o 是一个本地的多模态模型；我们可以用它来完成所有这些任务。首先，我们将看到如何使用 GPT-4o
    来回答关于图像的自由形式问题（用自然语言）。其次，我们将使用 GPT-4o 来构建一个自动图片标记应用程序，自动为我们的假日照片标记出现的人物。
- en: Finally, we will use GPT-4o to automatically generate titles for video files.
    The goal of these mini-projects is to illustrate features for visual data processing
    offered by the latest generation of large language models. After working through
    those projects, you should be able to build your own applications for image and
    video data processing in various scenarios.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用 GPT-4o 来自动为视频文件生成标题。这些小型项目的目标是展示最新一代大型语言模型提供的视觉数据处理功能。完成这些项目后，你应该能够构建自己的图像和视频数据处理应用程序，用于各种场景。
- en: 6.1 Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 设置
- en: 'You will need to install one more Python package to run the example code. Specifically,
    you need OpenCV, a library for image processing. In the terminal, run the following
    command to install OpenCV:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装一个额外的 Python 包来运行示例代码。具体来说，你需要 OpenCV，这是一个用于图像处理的库。在终端中，运行以下命令来安装 OpenCV：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use this library to, for example, read images from disk and split videos
    into frames.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个库，例如，从磁盘读取图像并将视频分割成帧。
- en: 'Next, you need to install one more library, enabling you to send requests directly
    to OpenAI’s web services (which you will use to send pictures stored locally to
    OpenAI):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要安装一个额外的库，这将使你能够直接向 OpenAI 的网络服务发送请求（你将使用它来发送存储在本地的图片到 OpenAI）：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Well done! If you didn’t encounter any error messages running these commands,
    your system is now configured for image and video data analysis using GPT-4o.
    Let’s start with our first project in the next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！如果你在运行这些命令时没有遇到任何错误信息，那么你的系统现在已经配置好了，可以使用 GPT-4o 进行图像和视频数据分析。让我们从下一节开始我们的第一个项目。
- en: 6.2 Answering questions about images
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 回答关于图像的问题
- en: Neural networks for detecting objects (such as cars) in images have been around
    for many years. So what’s the big deal about processing images with GPT-4o?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中检测物体（如汽车）的神经网络已经存在很多年了。那么使用 GPT-4o 处理图像有什么大不了的？
- en: The primary limitation of classical models for image processing is that they
    need to be trained for specific analysis tasks. For example, let’s say you have
    a neural network that is really good at detecting pictures of cats. You can use
    that to filter out cat pictures from your personal collection. However, maybe
    you’re not into cats in general but are specifically interested in golden Persian
    cats. Unless your model is trained to detect that specific type of cat, you’re
    out of luck and need to label enough example pictures yourself. Doing that is
    tedious, and you may end up not using the model and instead going through the
    pictures by hand. The big deal about image processing with GPT-4o (and similar
    models) is that it solves a wide range of tasks with images based on just a description
    of the task (in natural language).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 经典图像处理模型的局限性在于它们需要针对特定的分析任务进行训练。例如，假设你有一个神经网络，它非常擅长检测猫的图片。你可以用它来过滤掉你个人收藏中的猫的图片。然而，也许你对猫整体不感兴趣，但特别感兴趣的是金色波斯猫。除非你的模型被训练来检测这种特定的猫，否则你将无计可施，需要自己标记足够的示例图片。这样做很繁琐，你可能会最终不使用该模型，而是手动查看图片。GPT-4o（以及类似模型）进行图像处理的一大亮点是，它仅基于任务的描述（自然语言）就能解决一系列基于图像的任务。
- en: We will use that to build a generic question-answering system for images. As
    a user, you will formulate arbitrary questions in natural language and point to
    a picture, and the system will generate a text answer. For example, asking the
    system to detect “golden Persian cats” in a picture should work out of the box
    without needing task-specific training data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用它来构建一个通用的图像问答系统。作为用户，你可以用自然语言提出任意问题，并指向一张图片，系统将生成文本答案。例如，要求系统检测图片中的“金色波斯猫”应该能够直接工作，而无需特定任务的训练数据。
- en: 6.2.1 Specifying multimodal input
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 指定多模态输入
- en: 'In this section, we will create a system that takes two inputs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个系统，该系统接受两个输入：
- en: A URL leading to an image on the web
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向网络中图像的URL
- en: A natural language question about the image
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于图像的自然语言问题
- en: 'The output is an answer to the question (as text). Internally, the system uses
    GPT-4o to process the question on the input image. It generates multimodal prompts,
    combining multiple types of data (here, text and images). Figure [6.1](#fig__vqaprompt)
    shows an example prompt: it contains one image (of an apple) and a question about
    the image (whether the image shows a banana). The correct answer is “No” in this
    case.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是对问题的答案（以文本形式）。内部，系统使用GPT-4o处理输入图像上的问题。它生成多模态提示，结合多种类型的数据（在这里，文本和图像）。图[6.1](#fig__vqaprompt)展示了示例提示：它包含一张图片（苹果的图片）和关于这张图片的问题（图片是否显示香蕉）。在这种情况下，正确的答案是“不”。
- en: '![figure](../Images/CH06_F01_Trummer.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F01_Trummer.png)'
- en: Figure 6.1 Multimodal prompt containing an image and text. The prompt instructs
    the language model to decide whether the picture shows a banana. In this case,
    the expected output is “No” (otherwise “Yes”).
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 包含图像和文本的多模态提示。提示指示语言模型判断图片是否显示香蕉。在这种情况下，预期的输出是“不”（否则“是”）。
- en: How can we create such prompts for GPT-4o? We can reuse the chat completions
    endpoint for that. As a reminder, this endpoint takes as input a list of prior
    messages exchanged between the user and (potentially) the system. For our visual
    question-answering system, we only need a single message (that originates from
    the user).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何为GPT-4o创建这样的提示？我们可以重用聊天完成端点。提醒一下，这个端点接受用户和（可能）系统之间交换的先前消息列表作为输入。对于我们的视觉问答系统，我们只需要一条消息（这条消息来自用户）。
- en: 'Unlike the prior code, messages can now contain multimodal content. In this
    specific case, this content consists of one text snippet (the question asked by
    the user) and one image (specified as a URL for the moment). This is the message
    we will use in the following code (`question` is a variable containing the question
    text, and `image_url` is the URL to the image):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的代码不同，现在消息可以包含多模态内容。在这个特定的情况下，这种内容由一个文本片段（用户提出的问题）和一个图像（目前指定为URL）组成。这是我们将在以下代码中使用的消息（`question`是一个包含问题文本的变量，而`image_url`是图像的URL）：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Question text'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 问题文本'
- en: '#2 Image URL'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 图像URL'
- en: First, note the `role` attribute identifying the message as generated by the
    user. Second, the message content is specified as a list of Python dictionaries.
    Each of those dictionaries describes one element of the message. As we are now
    considering multimodal data—that is, images and text—we need to clarify the type
    (or *modality*) of each input element. This is accomplished by setting the `type`
    attribute to either `text` or `image_url`. The actual content is specified using
    either the `text` attribute (**1**) or (in the case of an image) the `image_url`
    attribute (**2**). GPT-4o is flexible enough to understand that the question refers
    to the image and to process both appropriately.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意`role`属性标识消息是由用户生成的。其次，消息内容被指定为一个Python字典列表。每个字典描述消息的一个元素。由于我们现在考虑的是多模态数据——即图像和文本——我们需要明确每个输入元素的类型（或*模态*）。这是通过将`type`属性设置为`text`或`image_url`来实现的。实际内容使用`text`属性（**1**）或（在图像的情况下）`image_url`属性（**2**）来指定。GPT-4o足够灵活，能够理解问题指的是图像，并适当地处理它们。
- en: 'Tip Whereas the input contains a single picture, the content of a message may
    contain multiple elements of the same type: for example, multiple images. We will
    exploit that capability for the project in the next section.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：虽然输入只包含一张图片，但消息的内容可能包含多个相同类型的元素：例如，多张图片。我们将在下一节的项目中利用这一功能。
- en: 6.2.2 Code discussion
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 代码讨论
- en: The following listing shows the complete code for our visual question-answering
    system. Taking the image URL and a question as input (**3**), the actual magic
    (i.e., visual question-answering) happens in the `analyze_image` function (**1**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了我们视觉问答系统的完整代码。以图像URL和问题作为输入（**3**），实际的魔法（即视觉问答）发生在`analyze_image`函数（**1**）中。
- en: Listing 6.1 Answering questions about images via language models
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1 通过语言模型回答图像问题
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Answers question about an image'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 回答关于图像的问题'
- en: '#2 Multimodal content'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 多模态内容'
- en: '#3 Input parameters'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输入参数'
- en: As you see, all it takes is a few lines of Python code to answer questions about
    images! The function `analyze_image` (**1**) contains but a single call to GPT-4o,
    using the message described in the previous subsection (**2**). The fact that
    we now provide multimodal input does not change the format of the answer. Again,
    we get an object containing a message generated by the language model. Although
    the input may now be multimodal, the output is text. As we instructed the language
    model to generate an answer to the input question (**3**) (i.e., exactly what
    the user is looking for), the output is directly printed out for the user.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，只需几行Python代码就能回答关于图像的问题！函数`analyze_image`（**1**）仅包含对GPT-4o的一次调用，使用的是前一小节中描述的消息（**2**）。我们现在提供多模态输入并不会改变答案的格式。再次强调，我们得到的是一个包含由语言模型生成的消息的对象。尽管输入现在可能是多模态的，但输出仍然是文本。正如我们指示语言模型生成对输入问题的答案（**3**）（即用户所寻找的确切答案），输出会直接打印给用户。
- en: 6.2.3 Trying it out
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 尝试运行
- en: Time to test our visual question-answering system! You’re back at Banana (a
    producer of various consumer electronics, including laptops and smartphones, introduced
    in chapter 2) and looking for a new company logo. You want to base your logo on
    a picture of a banana. Searching the web, you find large repositories with images
    of fruit. But which of them are bananas? Instead of going through images by hand,
    you would much rather delegate that task to a language model. Luckily, you can
    directly use the code from the previous section by specifying the URL of each
    fruit picture, together with the question “Is this a banana (“Yes”,“No”)?” This
    means you’re using the visual question-answering system essentially as a classification
    method (which is only one of many possible use cases). You can then write a simple
    script, iterating over all relevant URLs and retaining the ones where the answer
    is “Yes.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候测试我们的视觉问答系统了！你回到了香蕉公司（一家生产各种消费电子产品，包括笔记本电脑和智能手机，在第2章中介绍）并寻找一个新的公司标志。你希望以香蕉的图片为基础设计你的标志。在网上搜索后，你找到了大量水果图片的仓库。但哪一个是香蕉？你更愿意手动逐个查看图片，而不是将这项任务委托给语言模型。幸运的是，你可以直接使用前一小节中的代码，通过指定每张水果图片的URL，以及问题“这是香蕉吗？（“是”，“否”）”。这意味着你实际上是将视觉问答系统作为一个分类方法（这仅仅是许多可能用例之一）来使用。然后你可以编写一个简单的脚本，遍历所有相关的URL，并保留那些回答为“是”的URL。
- en: 'On the book’s companion website, you will find the code from listing [6.1](#code__visualqa)
    as well as pictures of fruit (look for the links labeled Fruit 1 to Fruit 5).
    Download the code, change to the containing repository in the terminal, and run
    the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的配套网站上，你可以找到列表[6.1](#code__visualqa)中的代码以及水果图片（寻找标记为Fruit 1到Fruit 5的链接）。下载代码，在终端中切换到包含的存储库，并运行以下代码：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this command, replace `[URL]` with the URL of the image (you can obtain a
    suitable URL by, for example, copying the Fruit 1 link on the book’s website).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，将 `[URL]` 替换为图片的URL（例如，你可以通过复制书网站上“水果1”的链接来获取合适的URL）。
- en: Object classification is relatively easy, particularly for objects as common
    as bananas. So you should see accurate results for most examples. Try a few different
    fruits and possibly other images of your choice. The range of questions you can
    ask is virtually unlimited (putting aside the rather generous input length limit
    of 128,000 tokens, about 300 pages of text).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 物体分类相对容易，尤其是像香蕉这样常见的物体。因此，你应该看到大多数示例都能得到准确的结果。尝试几种不同的水果，以及可能的其他你选择的图片。你可以提出的问题范围几乎是无限的（不考虑相对宽裕的输入长度限制，即128,000个标记，大约300页的文本）。
- en: 'A word of caution may be in order when it comes to processing costs. Processing
    images via GPT-4o can be expensive! The precise cost depends on the image size
    and the degree of detail used for image processing. You can control the degree
    of precision using the `detail` parameter. For instance, choose a low degree of
    precision using the following specification of image URLs (in the model input):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理成本方面，可能需要提醒一下。通过GPT-4o处理图像可能会很昂贵！确切的成本取决于图像大小和图像处理的详细程度。你可以使用`detail`参数来控制精确度。例如，使用以下图像URL的指定（在模型输入中）选择低精度：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Set the `detail` attribute to `low` to pay a cost equivalent to 85 tokens per
    image (i.e., the cost equivalent of processing a text with 85 tokens using GPT-4o).
    If you set the degree of detail to `high` (the default), the cost consists of
    a fixed amount of 85 tokens and a variable amount that depends on the image size.
    To calculate the variable cost component, we first scale the image to a size (in
    pixels) of 2,048 × 2,048 (while maintaining the aspect ratio). This scaling step
    only applies to pictures with a size beyond 2,048 × 2,048 pixels. The second scaling
    step is performed in any case. It scales the shorter side of the image to a size
    of 768 pixels. Now consider the minimum number of 512 × 512 pixel squares needed
    to cover the image after the second scaling step. The variable cost component
    is proportional to the number of squares multiplied by a factor of 170 tokens
    (the cost per square, set by OpenAI).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将`detail`属性设置为`low`，以支付相当于每张图像85个标记的成本（即使用GPT-4o处理包含85个标记的文本的成本）。如果你将详细程度设置为`high`（默认值），成本包括一个固定数量的85个标记和一个可变数量，这取决于图像大小。为了计算可变成本部分，我们首先将图像缩放到2,048
    × 2,048像素的大小（同时保持宽高比）。这一缩放步骤仅适用于超过2,048 × 2,048像素的图片。第二个缩放步骤无论如何都会执行。它将图像的较短边缩放到768像素的大小。现在考虑在第二次缩放步骤之后覆盖图像所需的512
    × 512像素方块的最低数量。可变成本部分与方块的数量的乘积成正比，乘以170个标记（由OpenAI设定的每平方的成本）。
- en: 'For instance, let’s say we want to process an image of size 1,024 × 1,024 pixels
    with high precision. In that case, we can skip the first scaling step, as the
    image still fits within a 2,048 × 2,048 pixel square. The second scaling step,
    however, is performed in any case. It scales the image to a size of 768 × 768
    pixels. To cover a square with a length of 768 pixels on both sides, we require
    four squares of size 512 × 512 pixels. That means that to process our image, we
    pay the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要以高精度处理一个1,024 × 1,024像素大小的图像。在这种情况下，我们可以跳过第一个缩放步骤，因为图像仍然适合在一个2,048
    × 2,048像素的方块内。然而，第二个缩放步骤无论如何都会执行。它将图像缩放到768 × 768像素的大小。为了覆盖两边长度为768像素的方块，我们需要四个512
    × 512像素的方块。这意味着为了处理我们的图像，我们需要支付以下费用：
- en: 85 tokens (the fixed cost component)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 85个标记（固定成本部分）
- en: 4 × 170 tokens = 680 tokens (the variable cost component)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 × 170个标记 = 680个标记（可变成本部分）
- en: In total, we therefore pay 85 + 4 × 170 = 765 tokens. Given current prices,
    this corresponds to $0.003825 (i.e., less than one cent). Although that may seem
    acceptable, always keep costs in mind when processing large repositories of images
    via language models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们总共支付85 + 4 × 170 = 765个标记。根据当前价格，这相当于0.003825美元（即不到一美分）。尽管这可能看起来是可以接受的，但在通过语言模型处理大量图像库时，始终要考虑成本。
- en: 'Tip To find the price for processing images with a specific resolution, you
    can also use the OpenAI price calculator: [https://openai.com/pricing](https://openai.com/pricing).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：要查找处理具有特定分辨率的图像的价格，您还可以使用OpenAI价格计算器：[https://openai.com/pricing](https://openai.com/pricing)。
- en: 6.3 Tagging people in images
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 在图像中标记人
- en: 'Imagine the following situation: you just came back from a (well-deserved)
    holiday with friends, and, of course, you have taken a large number of pictures.
    You want to send your friends the pictures in which they appear. But how to do
    that efficiently? You could go through the pictures by hand and tag each friend
    individually. But having just come back from vacation, your email inbox is overflowing,
    and you don’t have time to go through holiday pictures. What can you do?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 想象以下情况：你刚刚和朋友（应得的）度假回来，当然，你拍了很多照片。你想要发送出现你朋友的照片。但如何高效地做到这一点？你可以手动浏览照片并为每个朋友单独标记。但是，刚刚度假回来，你的邮箱已经爆满，你没有时间浏览假期照片。你能做什么？
- en: 6.3.1 Overview
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 概述
- en: 'In this section, we will create a small application that automatically tags
    people in images. Users provide three inputs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个小应用程序来自动在图像中标记人。用户提供三个输入：
- en: The path of a directory containing the pictures to tag
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要标记的图片的目录路径
- en: The path of a directory containing pictures of the people to look for
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要寻找的人的图片的目录路径
- en: The path to an output directory into which tagged pictures are written
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记的图片写入的输出目录的路径
- en: To keep things simple, we will use filenames to represent tags. We assume that
    pictures showing people to look for are named after the person they show. For
    example, let’s say we have images named Joe.png and Jane.png in our directory
    containing the people to look for. Given a picture to tag, we will simply change
    the filename by prefixing it with the names of people that appear in it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将使用文件名来表示标签。我们假设显示要寻找的人的图片以显示的人的名字命名。例如，假设我们在包含要寻找的人的目录中有名为Joe.png和Jane.png的图片。给定一个要标记的图片，我们将简单地通过在其名称前加上图片中出现的名字来更改文件名。
- en: For instance, assume we have an image called beach.png in which both Joe and
    Jane appear. Then, in the output directory, we will create two files called Joebeach.png
    and Janebeach.png, showing that both appear in the beach picture. If we want to
    send out all pictures showing the same person, such as Joe, we can search for
    all files whose name satisfies the regular expression `Joe*.png` (with `*` representing
    arbitrary strings).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个名为beach.png的图片，其中Joe和Jane都出现了。那么，在输出目录中，我们将创建两个名为Joebeach.png和Janebeach.png的文件，显示他们都在海滩图片中。如果我们想发送显示同一人的所有图片，例如Joe，我们可以搜索所有名称满足正则表达式`Joe*.png`（其中`*`代表任意字符串）的文件。
- en: Internally, as a first step, we need to load pictures representing people to
    look for, as well as the pictures to tag. We will consider each pair of a person
    to look for and a picture to tag. For example, if we are looking for five people
    and have 10 pictures to tag, that makes 50 pairs to consider. For each of these
    pairs, we use GPT-4o to decide whether the corresponding person appears in the
    picture to tag.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，作为第一步，我们需要加载代表要寻找的人的图片以及要标记的图片。我们将考虑每一对人要寻找的人和要标记的图片。例如，如果我们正在寻找五个人，并且有10张图片要标记，那么我们需要考虑50对人。对于这些对中的每一个，我们使用GPT-4o来判断对应的人是否出现在要标记的图片中。
- en: To make that happen, we will need multimodal prompts containing text and two
    pictures. The first picture shows the person to look for, and the second picture
    shows the picture to tag. Via text, we can instruct the language model to compare
    the pictures to decide whether the same person appears. Whenever we find a match—that
    is, a combination of a person and a picture in which that person appears—we will
    copy the corresponding picture to the output folder, prefixing its name with the
    name of the person.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，我们需要包含文本和两张图片的多模态提示。第一张图片显示要寻找的人，第二张图片显示要标记的图片。通过文本，我们可以指示语言模型比较图片以判断是否出现同一个人。每当我们发现匹配项——即出现该人的一个人和图片的组合——我们就会将相应的图片复制到输出文件夹，并在其名称前加上该人的名字。
- en: Figure [6.2](#fig__tagprompt) shows an example prompt. On the left, we have
    a picture of Jane, one of the people we are looking for. On the right side, we
    have a picture to tag. The text instructions ask the language model to compare
    the two pictures, producing the answer “Yes” if they show the same person (and
    “No” otherwise). In this case, the pictures do not show the same person, and the
    correct answer should be “No.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6.2](#fig__tagprompt) 展示了一个示例提示。在左侧，我们有一张 Jane 的照片，她是我们要找的人之一。在右侧，我们有一张需要标记的照片。文本指令要求语言模型比较这两张图片，如果它们显示的是同一个人（否则为“否”），则产生答案“是”。在这种情况下，图片显示的不是同一个人，正确的答案应该是“否”。
- en: '![figure](../Images/CH06_F02_Trummer.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F02_Trummer.png)'
- en: 'Figure 6.2 Multimodal prompt containing two images and text: the prompt instructs
    the language model to check whether the two pictures show the same person (expected
    answer: “Yes”) or not (expected answer: “No”).'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2 多模态提示包含两张图像和文本：提示要求语言模型检查这两张图片是否显示的是同一个人（预期答案：“是”）或不是（预期答案：“否”）。
- en: 6.3.2 Encoding locally stored images
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 本地存储图像编码
- en: In the previous section, we used GPT-4o to analyze images on the web. Now we
    are talking about our private holiday pictures. We may not want to make all of
    them publicly accessible on the web. So how can we share them with GPT-4o alone?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用 GPT-4o 分析了网上的图片。现在我们正在谈论我们私人的假日照片。我们可能不希望将它们全部公开在网络上。那么我们如何仅与 GPT-4o
    分享它们呢？
- en: We may have to convert images into a format suitable for GPT-4o. GPT-4o supports
    a wide range of image formats, including PNG, JPEG, WEBP, and GIF. For any format,
    the image file size is currently limited to 20 MB. To upload pictures of the supported
    types to GPT-4o, we first need to encode them using a base64 encoding.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能需要将图像转换为适合 GPT-4o 的格式。GPT-4o 支持广泛的图像格式，包括 PNG、JPEG、WEBP 和 GIF。对于任何格式，图像文件大小目前限制为
    20 MB。要将支持的类型的图片上传到 GPT-4o，我们首先需要使用 base64 编码对它们进行编码。
- en: What is base64 encoding?
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: base64 编码是什么？
- en: The base64 encoding is a way to encode binary data as a printable string. As
    the name base64 suggests, the alphabet we use for the string is based on 64 characters.
    This means we can represent each character using six bits (because six bits allow
    representing 2⁶ = 64 possible characters). As computers store data at the granularity
    of bytes (i.e., 8 bits), it is convenient to encode groups of three bytes (i.e.,
    24 bits) together. Using base64 encoding, three bytes can be used to represent
    four characters (as 24/6 = 4).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: base64 编码是将二进制数据编码为可打印字符串的一种方法。正如其名称 base64 所暗示的，我们用于字符串的字母表基于 64 个字符。这意味着我们可以使用六个比特（因为六个比特可以表示
    2⁶ = 64 个可能的字符）来表示每个字符。由于计算机以字节（即 8 比特）的粒度存储数据，因此将三个字节的组（即 24 比特）一起编码是很方便的。使用
    base64 编码，三个字节可以用来表示四个字符（因为 24/6 = 4）。
- en: 'In Python, we can use the `base64` library to encode binary data in the base64
    format. The following code opens an image file stored at `image_path` and encodes
    it using base64 format:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以使用 `base64` 库以 base64 格式编码二进制数据。以下代码打开存储在 `image_path` 的图像文件，并使用
    base64 格式对其进行编码：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have transformed the binary image data into a string in base64 format. Before
    sending such images to GPT-4o, we still need to make one final transformation:
    we must represent the string using the UTF-8 encoding.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将二进制图像数据转换成了 base64 格式的字符串。在将此类图像发送到 GPT-4o 之前，我们仍然需要进行最后一次转换：我们必须使用 UTF-8
    编码来表示该字符串。
- en: What is UTF-8 encoding?
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: UTF-8 编码是什么？
- en: 'UTF-8 is a way to represent string data. It is extremely popular and used by
    about 98% of sites on the web. UTF-8 can represent over a million characters,
    covering a variety of languages. We can represent those characters using a fixed
    number of bytes: four bytes to represent each character. However, this is inefficient
    because it does not exploit the fact that certain characters are much more common
    than others. If we encode common characters with fewer bytes while reserving many-byte
    representations for the less common ones, we can represent the same text with
    fewer bytes. This is what UTF-8 does, and because different characters may need
    a different number of bytes for representation, it is also called a *variable-length
    standard*. At the same time, UTF-8 is designed to be backward-compatible with
    the older ASCII standard, using the same encoding as ASCII for the first 128 characters.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: UTF-8 是一种表示字符串数据的方式。它非常流行，大约有 98% 的网站使用 UTF-8。UTF-8 可以表示超过一百万个字符，覆盖各种语言。我们可以使用固定数量的字节来表示这些字符：每个字符四个字节。然而，这并不高效，因为它没有利用到某些字符比其他字符更常见的这一事实。如果我们用更少的字节来编码常见的字符，而将多字节表示留给不常见的字符，我们可以用更少的字节来表示相同的文本。这正是
    UTF-8 所做的，而且由于不同的字符可能需要不同数量的字节来表示，它也被称为 *可变长度标准*。同时，UTF-8 被设计成与较旧的 ASCII 标准向后兼容，对于前
    128 个字符使用与 ASCII 相同的编码。
- en: 'To transform our base64 string encoding of the image into UTF-8, we can use
    Python’s `decode` function. Assuming that the image is still encoded in the `encoded`
    string variable, we can do so using the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的图像 base64 编码转换为 UTF-8，我们可以使用 Python 的 `decode` 函数。假设图像仍然编码在 `encoded` 字符串变量中，我们可以使用以下代码进行转换：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting image, encoded as UTF-8 text string, is suitable as input for
    GPT-4o. Next, we will see how we can upload images in this format to the OpenAI
    platform. After uploading them, we can include references to those pictures in
    our prompts. Images are generally specified as components of the prompt:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像，编码为 UTF-8 文本字符串，适合作为 GPT-4o 的输入。接下来，我们将看到如何以这种格式将图像上传到 OpenAI 平台。上传后，我们可以在提示中包含对这些图片的引用。图像通常指定为提示的组成部分：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, `image_url` represents the URL that leads to the image to analyze. Previously,
    we used publicly accessible URLs for that. Now we are analyzing private images
    that we will send to OpenAI to be used only to process specific requests. Assuming
    that `image` still represents the image encoded as a string, we can set the image
    URL as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`image_url` 代表通向要分析的图像的 URL。之前，我们使用的是公开可访问的 URL。现在我们正在分析私有图像，这些图像将发送到 OpenAI，仅用于处理特定请求。假设
    `image` 仍然代表作为字符串编码的图像，我们可以按以下方式设置图像 URL：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code assumes that the image is of type PNG (if not, replace the string
    `png` with the appropriate format identifier such as `jpeg`). The URL combines
    metadata about the image (such as the image type and encoding) with a string suffix
    representing the picture itself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码假设图像为 PNG 类型（如果不是，请将字符串 `png` 替换为适当的格式标识符，例如 `jpeg`）。URL 结合了关于图像的元数据（例如图像类型和编码）以及表示图片本身的字符串后缀。
- en: 6.3.3 Sending locally stored images to OpenAI
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 将本地存储的图像发送到 OpenAI
- en: We will use this project as an opportunity to demonstrate an alternative way
    to interact with GPT models. Doing so will give us insights into how OpenAI’s
    Python library works internally. So far, we have been using Python wrappers that
    send requests to OpenAI’s platform in the background. To send our local images
    to GPT-4o, we will create those requests ourselves.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用这个项目作为展示与 GPT 模型交互的替代方式的机会。这样做将使我们深入了解 OpenAI 的 Python 库内部工作原理。到目前为止，我们一直在使用
    Python 包装器在后台向 OpenAI 的平台发送请求。为了将我们的本地图像发送到 GPT-4o，我们将自己创建这些请求。
- en: We use Python’s `requests` library to create HTTP requests, sending our prompts
    (with text and images) to GPT-4o and collecting the answer. More precisely, we
    will be sending HTTP Post requests. This is the type of request accepted by the
    OpenAI platform. Such requests can be sent via the `requests.post` method.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Python 的 `requests` 库来创建 HTTP 请求，将我们的提示（包括文本和图像）发送到 GPT-4o 并收集答案。更确切地说，我们将发送
    HTTP POST 请求。这是 OpenAI 平台接受的请求类型。此类请求可以通过 `requests.post` 方法发送。
- en: 'Our requests will contain all relevant information needed by GPT-4o to solve
    the task we are interested in (in this case, verifying whether two images show
    the same person). First, we need to include headers in the request. We will use
    the following headers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求将包含 GPT-4o 解决我们感兴趣的任务（在这种情况下，验证两张图片是否显示同一人）所需的所有相关信息。首先，我们需要在请求中包含头信息。我们将使用以下头信息：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You see that we’re specifying headers as a Python dictionary. For our use case,
    we only need to store two properties: the type of our payload (we plan to send
    JSON content) and our access credentials (the three dots represent our OpenAI
    access key).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们正在指定一个 Python 字典作为头信息。对于我们的用例，我们只需要存储两个属性：我们有效负载的类型（我们计划发送 JSON 内容）和我们的访问凭证（三个点代表我们的
    OpenAI 访问密钥）。
- en: 'Next, we need to specify the payload—that is, the content that we primarily
    want to send via the request:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要指定有效负载——即我们主要通过请求发送的内容：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Model specification'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 模型指定'
- en: '#2 First message'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第一条消息'
- en: '#3 Output length'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出长度'
- en: 'You may notice that the payload contains exactly the fields we would typically
    specify in our invocations to the `completions.create` method. That is not a coincidence,
    as the latter method creates requests with a similar payload internally. First,
    the payload specifies the model (**1**): `gpt-4o` (to be able to process multimodal
    input prompts). We specify a list of messages with a single entry (**2**). This
    message is marked as originating from the user (`role:user`), and its content,
    abbreviated by three dots, will contain text instructions and images. Finally,
    we limit the answer length to a single token (`max_tokens:1`) (**3**). That makes
    sense because we are searching for a binary result: either the same person appears
    in multiple input images (expected answer: “Yes”) or not (expected answer: “No”).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到有效负载中恰好包含我们在调用 `completions.create` 方法时通常会指定的字段。这不是巧合，因为后者方法在内部创建具有类似有效负载的请求。首先，有效负载指定了模型（**1**）：`gpt-4o`（以便能够处理多模态输入提示）。我们指定了一个包含单个条目的消息列表（**2**）。这条消息被标记为来自用户（`role:user`），其内容用三个点缩写，将包含文本指令和图片。最后，我们将答案长度限制为单个标记（`max_tokens:1`）（**3**）。这很有意义，因为我们正在寻找二元结果：要么多个输入图像中出现了同一人（预期答案：“是”），要么没有（预期答案：“否”）。
- en: 'Having generated headers and a payload, we can invoke GPT-4o using the following
    code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成了头信息和有效负载之后，我们可以使用以下代码调用 GPT-4o：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As the first parameter, the invocation of `requests.post` specifies the URL
    to send the request to. In this case, `https://api.openai.com/v1/chat/completions`
    indicates that we want to perform a task of type `Completion`, using one of OpenAI’s
    chat models (which applies to GPT-4o). We use the headers and payload created
    previously.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个参数，`requests.post` 的调用指定了发送请求的 URL。在这种情况下，`https://api.openai.com/v1/chat/completions`
    表示我们想要执行类型为 `Completion` 的任务，使用 OpenAI 的一种聊天模型（这适用于 GPT-4o）。我们使用之前创建的头信息和有效负载。
- en: 'The response contains the GPT-4o result object. We can access the answer (indicating
    whether two images show the same person) via the following code snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 响应中包含 GPT-4o 结果对象。我们可以通过以下代码片段访问答案（指示两张图片是否显示同一人）：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 6.3.4 The end-to-end implementation
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 端到端实现
- en: 'We are now ready to discuss the end-to-end implementation! Listing [6.2](#code__taggingpictures)
    contains code for tagging people in pictures. Have a look at the main function
    (**4**) first. As discussed previously, users specify three directories as command-line
    parameters (**5**): a directory containing pictures to tag, a directory containing
    people to use for tagging, and an output directory.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好讨论端到端实现！列表 [6.2](#code__taggingpictures) 包含了在图片中标记人物的代码。首先查看主函数（**4**）。如前所述，用户指定三个目录作为命令行参数（**5**）：一个包含要标记的图片的目录，一个包含用于标记的人物目录，以及一个输出目录。
- en: As a first step, we load all images to tag as well as all images of the people
    to search for. We use the `load_images` function (**1**) for that. This function
    retrieves a list of all files in the input directory and then considers those
    ending with the suffix .png (i.e., we consider all PNG images). As discussed previously,
    we need to encode images as strings (via base64 encoding) that are ultimately
    represented via the UTF-8 encoding. The result of `load_images` is a Python dictionary
    mapping filenames to the associated, encoded images. This dictionary is returned
    as the result of the function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们加载所有要标记的图像以及所有要搜索的人物图像。我们使用`load_images`函数（**1**）来做这件事。这个函数检索输入目录中的所有文件列表，然后考虑那些以`.png`后缀结尾的文件（即，我们考虑所有PNG图像）。如前所述，我们需要将图像编码为字符串（通过base64编码），最终通过UTF-8编码表示。`load_images`的结果是一个Python字典，将文件名映射到相关的编码图像。这个字典作为函数的结果返回。
- en: Listing 6.2 Tagging people in pictures stored locally
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2 在本地存储的图片中标记人物
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Loads images from disk'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从磁盘加载图像'
- en: '#2 Creates a multimodal prompt'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建多模态提示'
- en: '#3 Generates an answer for the prompt'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为提示生成答案'
- en: '#4 Tags images with people'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用人物标记图像'
- en: '#5 Command-line parameters'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 命令行参数'
- en: '#6 Over people'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 关于人'
- en: '#7 Over images'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 关于图像'
- en: '#8 Copies image in case of a match'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 在匹配的情况下复制图像'
- en: After applying the `load_images` function to each of the two input directories,
    we end up with two Python dictionaries. One maps filenames of images showing people
    (which, by convention, are the names of those people) to the corresponding encoded
    image. The other maps the filenames of images to be tagged to the encoded images.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在将`load_images`函数应用于两个输入目录中的每个目录之后，我们最终得到两个Python字典。一个将显示人物的图像的文件名（按照惯例，这些名称是那些人的名字）映射到相应的编码图像。另一个将待标记的图像的文件名映射到编码图像。
- en: 'Our goal is to match each picture to be tagged to all people that appear in
    it. As we use prompts comparing only two pictures at once, we need to look at
    each combination of a person and of an image to tag. That is why we use a double-nested
    `for` loop: one iterates over people (**6**) and the other over images to tag
    (**7**).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将每个要标记的图片与其中出现的所有人物匹配。由于我们一次只比较两张图片的提示，我们需要查看每个人物和要标记的图像的组合。这就是为什么我们使用双重嵌套的`for`循环：一个遍历人物（**6**），另一个遍历要标记的图像（**7**）。
- en: For each combination of an image to tag and a person, we create a multimodal
    prompt using `create_prompt`. This function (**2**) assembles both encoded pictures,
    together with text instructions, into a prompt. The text instructions (“Do the
    images show the same person (“Yes”/“No”)?”) define the task as well as the expected
    output format (“Yes” or “No”). Each prompt is sent to GPT-4o via `call_llm`. As
    discussed previously, this function (**3**) uses the requests API to send locally
    stored images, together with text instructions, to GPT-4o. If GPT-4o answers “Yes,”
    the currently considered person appears in the currently considered image to tag.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个要标记的图像和人物的组合，我们使用`create_prompt`创建一个多模态提示。这个函数（**2**）将编码后的图片和文本指令组合成一个提示。文本指令（“这些图像显示的是同一个人吗？（是/否）？”）定义了任务以及预期的输出格式（“是”或“否”）。每个提示都通过`call_llm`发送给GPT-4o。如前所述，这个函数（**3**）使用请求API将本地存储的图像和文本指令发送给GPT-4o。如果GPT-4o回答“是”，则当前考虑的人物出现在当前考虑的标记图像中。
- en: If the person appears in the image (**8**), we tag the image as follows. We
    use the name of the person (the name of the associated picture file without the
    .png suffix) and prepend it to the name of the file to tag. Next, we copy the
    file to tag to the output directory using the new filename (which indicates the
    tagging result).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人物出现在图像中（**8**），我们将按照以下方式标记图像。我们使用人物的名字（关联图片文件的名字，不带`.png`后缀）并将其添加到要标记的文件名之前。接下来，我们使用新的文件名（表示标记结果）将待标记的文件复制到输出目录。
- en: 6.3.5 Trying it out
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 尝试一下
- en: 'Let’s try it! If you have real vacation pictures to tag, you can use them.
    Otherwise, you will find suitable test data on the book’s companion website. Look
    for the Tagging link to access a zipped file; download this file and unzip its
    contents. After decompression, you should see three subdirectories in the resulting
    folder:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试！如果你有要标记的真实假期照片，你可以使用它们。否则，你可以在本书的配套网站上找到合适的测试数据。查找标记链接以访问一个压缩文件；下载此文件并解压缩其内容。解压缩后，你应该在结果文件夹中看到三个子目录：
- en: '*people*—A folder containing pictures of people (in this case, actors from
    the *Avengers* series). Filenames contain the names of the corresponding actors.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*people*—一个包含人物图片的文件夹（在这种情况下，是《复仇者联盟》系列中的演员）。文件名包含相应演员的名称。'
- en: '*pics*—Another set of pictures (in this case, more pictures of the same actors
    as in the people folder) to tag with the names of actors.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pics*—另一组图片（在这种情况下，与 people 文件夹中相同的演员的更多图片）要标记上演员的名称。'
- en: '*processed*—An empty folder that can be used as the output directory.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*processed*—一个空文件夹，可以用作输出目录。'
- en: 'We’ll assume that the decompressed folder is stored under the path /tagging
    (e.g., the path /tagging/people then leads to the subfolder with pictures of people
    to search for). Execute the code by running the following command from the terminal:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设解压缩的文件夹存储在路径 /tagging 下（例如，路径 /tagging/people 将指向包含要搜索的人的图片的子文件夹）。通过在终端中运行以下命令来执行代码：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip If you are invoking the code on a Windows platform, you will have to adapt
    these paths. In particular, you will have to replace / with \.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你在 Windows 平台上调用代码，你必须适应这些路径。特别是，你必须用 \ 替换 /。
- en: During processing, the implementation prints updates on whether specific people
    appear in specific pictures. The sample data contains two people to look for and
    four images to tag. This means processing should not take more than a few minutes
    (typically less than two).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理过程中，实现会打印出有关特定人物是否出现在特定图片中的更新。样本数据包含两个要查找的人物和四个要标记的图片。这意味着处理不应超过几分钟（通常是少于两分钟）。
- en: After processing finishes, look in the output folder. You should see pictures
    to tag, prefixed with the names of people appearing in them. Not bad for a few
    lines of Python code!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完成后，查看输出文件夹。你应该会看到要标记的图片，图片前缀是其中出现的人物名称。对于几行Python代码来说，这已经很不错了！
- en: 6.4 Generating titles for videos
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 为视频生成标题
- en: Besides many pictures (which we can now automatically tag, thanks to the code
    outlined in the previous section!), you also took quite a few videos on vacation.
    Automatically assigned filenames are not very informative. Which video is the
    one showing you swimming in the ocean? It would be great to assign meaningful
    captions to those videos and help you find the ones you’re looking for faster.
    But who has time to manually label videos? Again, we can use language models to
    do that task automatically.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了许多图片（我们现在可以自动标记，多亏了上一节中概述的代码！）之外，你在度假期间还拍摄了许多视频。自动分配的文件名并不很有信息量。哪个视频是你在大海里游泳的那个？如果能给这些视频分配有意义的标题，帮助你更快地找到你想要的视频那就太好了。但谁有那么多时间去手动标记视频呢？再次，我们可以使用语言模型来自动完成这项任务。
- en: 6.4.1 Overview
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 概述
- en: We will develop a system that automatically assigns suitable titles to videos.
    This system uses GPT-4o in the background. To assign titles to videos, we will
    submit multimodal prompts containing video frames (i.e., images) together with
    text instructing the language model to come up with a title. Figure [6.3](#fig__videoprompt)
    shows an example prompt.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个系统，该系统可以自动为视频分配合适的标题。该系统在后台使用 GPT-4o。为了给视频分配标题，我们将提交包含视频帧（即图像）和指示语言模型生成标题的文本的多模态提示。图
    [6.3](#fig__videoprompt) 展示了一个示例提示。
- en: '![figure](../Images/CH06_F03_Trummer.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Trummer.png)'
- en: 'Figure 6.3 Multimodal prompt for video processing: based on a selection of
    video frames, the language model is instructed to generate a suitable title.'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.3 视频处理的多模态提示：基于视频帧的选择，指示语言模型生成一个合适的标题。
- en: It consists of multiple video frames (we only see the first and the last frame
    in figure [6.3](#fig__videoprompt); the three dots represent the ones in between)
    and the text instructions “Generate a concise title for the video.” Note that
    we have to pay for each video frame we’re submitting to GPT-4o. That means video
    data processing via GPT-4o quickly becomes expensive!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它由多个视频帧组成（我们在图 [6.3](#fig__videoprompt) 中只看到第一帧和最后一帧；三个点代表中间的帧）和文本指令“为视频生成一个简洁的标题。”请注意，我们必须为提交给
    GPT-4o 的每个视频帧付费。这意味着通过 GPT-4o 进行视频数据处理很快就会变得昂贵！
- en: As an answer, GPT-4o should send back a reasonable title. In the example shown
    in figure [6.3](#fig__videoprompt), this can be a reference to cars and, potentially,
    even a reference to the location (shown as white text in the frames).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回答，GPT-4o 应该发送回一个合理的标题。在图 [6.3](#fig__videoprompt) 中显示的示例中，这可能是对汽车的引用，甚至可能是对位置的引用（在帧中显示为白色文本）。
- en: 6.4.2 Encoding video frames
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 视频帧编码
- en: First we need to discuss video formats. In the last section, we saw how to encode
    images stored locally. Now we will expand that to videos. Ultimately, our goal
    is to extract a sequence of frames. However, videos are typically not stored as
    a sequence of frames but using more efficient encodings. For us, that means we
    first have to extract images from a video.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要讨论视频格式。在上一个章节中，我们看到了如何编码存储在本地的图像。现在我们将这个概念扩展到视频。最终，我们的目标是提取一系列帧。然而，视频通常不是以帧序列的形式存储，而是使用更高效的编码。对我们来说，这意味着我们首先必须从视频中提取图像。
- en: We will use the OpenCV library for that. OpenCV is the Open Source Computer
    Vision Library. It provides various functionalities for computer vision as well
    as for image and video processing in general. Of course, we will use GPT-4o to
    do the computer vision part. Nevertheless, OpenCV will be useful for extracting
    frames from videos. If you haven’t done so already, now would be a good time to
    set up OpenCV by following the instructions in section [6.1](#sec__videosetup).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用OpenCV库来完成这个任务。OpenCV是开源计算机视觉库。它提供了计算机视觉以及图像和视频处理的各种功能。当然，我们将使用GPT-4o来完成计算机视觉部分。尽管如此，OpenCV对于从视频中提取帧来说仍然很有用。如果您还没有这样做，现在是一个好时机，按照第[6.1](#sec__videosetup)节中的说明设置OpenCV。
- en: Let’s assume that the installation has worked and you can access OpenCV from
    Python. The corresponding Python library is called `cv2` (a name you will often
    see as a prefix in the following code snippets).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设安装已经成功，并且您可以从Python访问OpenCV。相应的Python库被称作`cv2`（您将在接下来的代码片段中经常看到这个名字作为前缀）。
- en: 'To work with a video stored locally, we first need to open the corresponding
    file. Run this code to open a video stored under the path `video_path`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理存储在本地的视频，我们首先需要打开相应的文件。运行以下代码以打开存储在`video_path`路径下的视频：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using the variable `video`, we can now read the video’s content via the `read`
    method:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变量`video`，我们现在可以通过`read`方法读取视频的内容：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result consists of tuples with two components: a `success` flag and a video
    frame. The `success` flag indicates whether we were able to read another frame.
    That’s no longer the case once we reach the end of the video. In that case, we
    do not obtain a valid frame, and the `success` flag is set to `False`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 结果由包含两个组件的元组组成：一个`success`标志和一个视频帧。`success`标志指示我们是否能够读取另一个帧。一旦我们到达视频的末尾，这种情况就不再成立。在这种情况下，我们不会获得有效的帧，`success`标志被设置为`False`。
- en: 'Let’s assume that we are able to read another frame. In that case, we will
    turn the frame into an image we can send to GPT-4o. OpenCV has us covered and
    provides the corresponding functionality:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们能够读取另一个帧。在这种情况下，我们将帧转换成可以发送给GPT-4o的图像。OpenCV为我们提供了相应的功能：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `imencode` function turns a video frame into an image of the corresponding
    type. Here, we transform the frame into a JPEG picture. From the resulting tuple,
    the second component (`buffer`) is interesting for our purposes. It contains a
    binary representation of the corresponding picture.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`imencode`函数将视频帧转换成相应类型的图像。在这里，我们将帧转换成JPEG图片。从生成的元组中，第二个组件（`buffer`）对我们来说很有趣。它包含对应图片的二进制表示。'
- en: 'That’s a situation we know from the previous section: we have a binary representation
    of an image and want to turn it into a suitable format for GPT-4o. Again, we first
    encode the image as a string via base64 encoding and then represent that string
    via UTF-8:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前章节中提到的情况：我们有一个图像的二进制表示，并希望将其转换成适合GPT-4o的格式。同样，我们首先通过base64编码将图像编码为字符串，然后通过UTF-8表示该字符串：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The resulting `frame` is encoded properly to be included as part of a GPT-4o
    prompt. Once you’re done processing the video, close the corresponding video capture
    object using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`frame`被正确编码，可以作为GPT-4o提示的一部分。一旦您处理完视频，请使用以下代码关闭相应的视频捕获对象：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we will put everything together to generate video titles for arbitrary
    videos.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把所有这些整合起来，为任意视频生成标题。
- en: 6.4.3 The end-to-end implementation
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 端到端实现
- en: Listing [6.3](#code__videocaption) generates titles for videos stored locally.
    The only input parameter is the path to the video. Given that, the implementation
    extracts some of the video frames (**4**) and then generates a prompt instructing
    GPT-4o to generate a video title based on a sample of frames. After sending this
    prompt to the language model, the answer contains a proposed video title.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[6.3](#code__videocaption)为存储在本地的视频生成标题。唯一的输入参数是视频的路径。鉴于这一点，实现提取了一些视频帧（**4**），然后生成一个提示，指示GPT-4o根据帧样本生成视频标题。将此提示发送给语言模型后，答案包含一个建议的视频标题。
- en: Listing 6.3 Generating a video title via language models
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3 通过语言模型生成视频标题
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Extracts video frames'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提取视频帧'
- en: '#2 Creates multimodal prompt'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建多模态提示'
- en: '#3 Queries the language model'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 查询语言模型'
- en: '#4 Titles videos'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 标题视频'
- en: The code extracts video frames using `extract_frames` (**1**). As discussed
    previously, this function uses the OpenCV library to open the video for frame
    extraction and proceeds to read each frame consecutively. We will only use up
    to 10 frames to generate a video title. That’s why extraction ends after at most
    10 frames (or fewer if the video is very short). Each extracted frame is encoded
    according to GPT-4o’s requirements (i.e., JPEG images encoded as strings). The
    result of the function is a list of encoded frames.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用 `extract_frames`（**1**）提取视频帧。如前所述，此函数使用OpenCV库打开视频以提取帧，然后连续读取每个帧。我们将只使用最多10帧来生成视频标题。这就是为什么提取在最多10帧（或更少，如果视频非常短）后结束。每个提取的帧都根据GPT-4o的要求进行编码（即，将JPEG图像编码为字符串）。函数的结果是编码帧的列表。
- en: During prompt generation (**2**), we combine relevant text instructions (“Generate
    a concise title for the video.”) with the first 10 frames from the video. To send
    those images, along with instructions, to GPT-4o, we use a Python wrapper again
    (**3**). Alternatively, we can create requests ourselves (as in the previous project).
    The response of the language model should contain a suitable title for our video.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示生成（**2**）过程中，我们将相关的文本指令（“为视频生成一个简洁的标题。”）与视频的前10帧结合起来。为了将那些图像以及指令发送到GPT-4o，我们再次使用Python包装器（**3**）。或者，我们可以自己创建请求（如前一个项目）。语言模型的响应应包含适合我们视频的适当标题。
- en: Of course, we are only sending the first few frames of the video. If the video
    content changes drastically after those few frames, the title may not be optimal.
    The reason we only send 10 frames is computation fees. Keep in mind that you’re
    paying for each picture submitted in the prompt! Sending all frames of larger
    videos is typically prohibitively expensive. That’s why we content ourselves with
    sending only a small subset of video frames.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们只发送视频的前几帧。如果视频内容在这几帧之后发生剧烈变化，标题可能不是最优的。我们只发送10帧的原因是计算费用。请记住，你为提示中提交的每一张图片都要付费！发送较大视频的所有帧通常费用高昂。这就是为什么我们只满足于发送视频帧的小子集。
- en: 6.4.4 Trying it out
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 尝试一下
- en: Let’s try our video title generator! On the book’s companion website, this chapter’s
    section includes a Cars link that will guide you to a short video from a traffic
    camera showing traffic on a busy road. Download the video to your local machine.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试我们的视频标题生成器！在书的配套网站上，这一章节的部分包含一个“汽车”链接，它会引导你到一个交通摄像头的短视频，展示繁忙道路上的交通情况。将视频下载到你的本地机器上。
- en: Open the terminal, and change to the directory containing the code for this
    chapter. We’ll assume that the video was downloaded into the same directory (if
    not, replace the name of the video, cars.mp4, with the full path leading to it).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端，切换到包含本章代码的目录。我们将假设视频被下载到同一个目录中（如果不是，请将视频名称cars.mp4替换为指向它的完整路径）。
- en: 'Run the following command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After a few seconds of computation time, you should see a proposal for a video
    title: for example, “Traffic Conditions on I-5 at SR 516 and 188th Street” (the
    precise title may vary across different runs due to randomization).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几秒钟的计算时间，你应该会看到一个视频标题的提案：例如，“I-5号公路在SR 516和188街的交通状况”（精确标题可能因随机化而有所不同）。
- en: 'Note that the title integrates information—the name of the location—that is
    only available in the form of text in the video. Using GPT-4o to extract text
    from images may be useful in various scenarios: for example, to extract data from
    forms.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，标题整合了仅以文本形式存在于视频中的信息——地点的名称。使用GPT-4o从图像中提取文本可能在各种场景中很有用：例如，从表格中提取数据。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GPT-4o processes images as well as text.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o可以处理图像和文本。
- en: Prompts can integrate text snippets and images.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示可以集成文本片段和图像。
- en: GPT-4o supports multiple image formats.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o 支持多种图像格式。
- en: Images can be specified via a public image URL.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过公共图像 URL 指定图像。
- en: Locally stored images can be uploaded to OpenAI.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地存储的图像可以上传到 OpenAI。
- en: GPT-4o processes images in string encoding.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o 以字符串编码处理图像。
- en: Processing images is costly compared to processing text. The cost of image processing
    may depend on the image size. Processing images with a low degree of detail reduces
    costs.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与处理文本相比，处理图像的成本较高。图像处理的成本可能取决于图像大小。以低分辨率处理图像可以降低成本。
- en: The `base64` library can encode images as strings.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base64` 库可以将图像编码为字符串。'
- en: Decompose videos into their frames to send them to GPT-4o.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将视频分解成帧以发送给 GPT-4o。
- en: The OpenCV library can be used to extract frames from videos.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 OpenCV 库从视频中提取帧。
