- en: 6 Fine-tuning for classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 分类微调
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing different LLM fine-tuning approaches
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍不同的LLM微调方法
- en: Preparing a dataset for text classification
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于文本分类的数据集
- en: Modifying a pretrained LLM for fine-tuning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改预训练的LLM进行微调
- en: Fine-tuning an LLM to identify spam messages
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调一个LLM以识别垃圾邮件
- en: Evaluating the accuracy of a fine-tuned LLM classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估微调LLM分类器的准确性
- en: Using a fine-tuned LLM to classify new data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调的LLM对新的数据进行分类
- en: 'So far, we have coded the LLM architecture, pretrained it, and learned how
    to import pretrained weights from an external source, such as OpenAI, into our
    model. Now we will reap the fruits of our labor by fine-tuning the LLM on a specific
    target task, such as classifying text. The concrete example we examine is classifying
    text messages as “spam” or “not spam.” Figure 6.1 highlights the two main ways
    of fine-tuning an LLM: fine-tuning for classification (step 8) and fine-tuning
    to follow instructions (step 9).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经编写了LLM架构，对其进行了预训练，并学习了如何将来自外部来源（如OpenAI）的预训练权重导入我们的模型。现在，我们将通过在特定目标任务（如文本分类）上微调LLM来收获我们的劳动成果。我们考察的具体例子是将短信分类为“垃圾邮件”或“非垃圾邮件”。图6.1突出了微调LLM的两种主要方式：用于分类的微调（步骤8）和用于遵循指令的微调（步骤9）。
- en: '![figure](../Images/6-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-1.png)'
- en: 'Figure 6.1 The three main stages of coding an LLM. This chapter focus on stage
    3 (step 8): fine-tuning a pretrained LLM as a classifier.'
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 编写LLM的三个主要阶段。本章重点介绍第3阶段（步骤8）：将预训练的LLM微调为分类器。
- en: 6.1 Different categories of fine-tuning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 微调的不同类别
- en: The most common ways to fine-tune language models are *instruction fine-tuning*
    and *classification fine-tuning*. Instruction fine-tuning involves training a
    language model on a set of tasks using specific instructions to improve its ability
    to understand and execute tasks described in natural language prompts, as illustrated
    in figure 6.2\.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微调语言模型最常见的方法是*指令微调*和*分类微调*。指令微调涉及使用特定指令在一系列任务上训练语言模型，以提高其理解和执行自然语言提示中描述的任务的能力，如图6.2所示。
- en: '![figure](../Images/6-2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-2.png)'
- en: Figure 6.2 Two different instruction fine-tuning scenarios. At the top, the
    model is tasked with determining whether a given text is spam. At the bottom,
    the model is given an instruction on how to translate an English sentence into
    German.
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2 两种不同的指令微调场景。在顶部，模型被要求判断给定的文本是否为垃圾邮件。在底部，模型被给出如何将英语句子翻译成德语的指令。
- en: 'In classification fine-tuning, a concept you might already be acquainted with
    if you have a background in machine learning, the model is trained to recognize
    a specific set of class labels, such as “spam” and “not spam.” Examples of classification
    tasks extend beyond LLMs and email filtering: they include identifying different
    species of plants from images; categorizing news articles into topics like sports,
    politics, and technology; and distinguishing between benign and malignant tumors
    in medical imaging.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类微调中，如果你有机器学习背景，你可能已经熟悉了这个概念：模型被训练来识别一组特定的类别标签，例如“垃圾邮件”和“非垃圾邮件”。分类任务的例子不仅限于LLM和电子邮件过滤，还包括从图像中识别不同的植物种类；将新闻文章分类到体育、政治和技术等主题；以及在医学成像中区分良性肿瘤和恶性肿瘤。
- en: The key point is that a classification fine-tuned model is restricted to predicting
    classes it has encountered during its training. For instance, it can determine
    whether something is “spam” or “not spam,” as illustrated in figure 6.3, but it
    can’t say anything else about the input text.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点是，分类微调模型仅限于在训练期间遇到的类别进行预测。例如，它可以确定某物是否是“垃圾邮件”或“非垃圾邮件”，如图6.3所示，但它对输入文本的其他内容无话可说。
- en: '![figure](../Images/6-3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-3.png)'
- en: Figure 6.3 A text classification scenario using an LLM. A model fine-tuned for
    spam classification does not require further instruction alongside the input.
    In contrast to an instruction fine-tuned model, it can only respond with “spam”
    or “not spam.”
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 使用LLM进行文本分类的场景。针对垃圾邮件分类进行微调的模型不需要在输入旁边提供进一步指令。与指令微调模型相比，它只能响应“垃圾邮件”或“非垃圾邮件”。
- en: In contrast to the classification fine-tuned model depicted in figure 6.3, an
    instruction fine-tuned model typically can undertake a broader range of tasks.
    We can view a classification fine-tuned model as highly specialized, and generally,
    it is easier to develop a specialized model than a generalist model that works
    well across various tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与图6.3中展示的分类微调模型相比，指令微调模型通常可以承担更广泛的任务范围。我们可以将分类微调模型视为高度专业化的，通常来说，开发一个适用于各种任务的通用模型比开发一个适用于特定类别的模型要容易。
- en: Choosing the right approach
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择正确的方法
- en: Instruction fine-tuning improves a model’s ability to understand and generate
    responses based on specific user instructions. Instruction fine-tuning is best
    suited for models that need to handle a variety of tasks based on complex user
    instructions, improving flexibility and interaction quality. Classification fine-tuning
    is ideal for projects requiring precise categorization of data into predefined
    classes, such as sentiment analysis or spam detection.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调提高了模型根据特定用户指令理解和生成响应的能力。指令微调最适合需要根据复杂用户指令处理各种任务的模型，提高灵活性和交互质量。分类微调适用于需要将数据精确分类到预定义类别的项目，例如情感分析或垃圾邮件检测。
- en: While instruction fine-tuning is more versatile, it demands larger datasets
    and greater computational resources to develop models proficient in various tasks.
    In contrast, classification fine-tuning requires less data and compute power,
    but its use is confined to the specific classes on which the model has been trained.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然指令微调更加灵活，但它需要更大的数据集和更多的计算资源来开发擅长各种任务的模型。相比之下，分类微调需要较少的数据和计算能力，但其应用范围仅限于模型已训练的特定类别。
- en: 6.2 Preparing the dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 准备数据集
- en: We will modify and classification fine-tune the GPT model we previously implemented
    and pretrained. We begin by downloading and preparing the dataset, as highlighted
    in figure 6.4\. To provide an intuitive and useful example of classification fine-tuning,
    we will work with a text message dataset that consists of spam and non-spam messages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改并微调之前实现和预训练的GPT模型。我们首先下载和准备数据集，如图6.4所示。为了提供一个直观且有用的分类微调示例，我们将使用一个包含垃圾邮件和非垃圾邮件的短信数据集。
- en: '![figure](../Images/6-4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-4.png)'
- en: Figure 6.4 The three-stage process for classification fine-tuning an LLM. Stage
    1 involves dataset preparation. Stage 2 focuses on model setup. Stage 3 covers
    fine-tuning and evaluating the model.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 对分类微调LLM的三个阶段过程。第一阶段涉及数据集准备。第二阶段侧重于模型设置。第三阶段涵盖微调和评估模型。
- en: Note  Text messages typically sent via phone, not email. However, the same steps
    also apply to email classification, and interested readers can find links to email
    spam classification datasets in appendix B.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通常通过手机发送的短信，而不是电子邮件。然而，相同的步骤也适用于电子邮件分类，感兴趣的读者可以在附录B中找到电子邮件垃圾邮件分类数据集的链接。
- en: The first step is to download the dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是下载数据集。
- en: Listing 6.1 Downloading and unzipping the dataset
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1 下载和解压数据集
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Downloads the file'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 下载文件'
- en: '#2 Unzips the file'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 解压文件'
- en: '#3 Adds a .tsv file extension'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 添加.ts文件扩展名'
- en: 'After executing the preceding code, the dataset is saved as a tab-separated
    text file, `SMSSpamCollection.tsv`, in the `sms_spam_collection` folder. We can
    load it into a pandas `DataFrame` as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，数据集被保存为制表符分隔的文本文件，`SMSSpamCollection.tsv`，在`sms_spam_collection`文件夹中。我们可以按照以下方式将其加载到pandas
    `DataFrame`中：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Renders the data frame in a Jupyter notebook. Alternatively, use print(df).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在Jupyter笔记本中渲染数据框。或者使用print(df)。'
- en: Figure 6.5 shows the resulting data frame of the spam dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5显示了垃圾邮件数据集的结果数据框。
- en: '![figure](../Images/6-5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-5.png)'
- en: Figure 6.5 Preview of the `SMSSpamCollection` dataset in a pandas `DataFrame`,
    showing class labels (“ham” or “spam”) and corresponding text messages. The dataset
    consists of 5,572 rows (text messages and labels).
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 在pandas `DataFrame`中预览`SMSSpamCollection`数据集，显示类别标签（“ham”或“spam”）和相应的文本消息。该数据集包含5,572行（文本消息和标签）。
- en: 'Let’s examine the class label distribution:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查类别标签分布：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Executing the previous code, we find that the data contains “ham” (i.e., not
    spam) far more frequently than “spam”:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码，我们发现数据中“ham”（即非垃圾邮件）的出现频率远高于“spam”：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For simplicity, and because we prefer a small dataset (which will facilitate
    faster fine-tuning of the LLM), we choose to undersample the dataset to include
    747 instances from each class.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，并且因为我们更喜欢小数据集（这将有助于更快地微调LLM），我们选择对数据集进行下采样，包括每个类别747个实例。
- en: Note  There are several other methods to handle class imbalances, but these
    are beyond the scope of this book. Readers interested in exploring methods for
    dealing with imbalanced data can find additional information in appendix B.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：处理类别不平衡的方法还有几种，但这些超出了本书的范围。对探索处理不平衡数据方法感兴趣的读者可以在附录B中找到更多信息。
- en: We can use the code in the following listing to undersample and create a balanced
    dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下列表中的代码进行下采样并创建一个平衡数据集。
- en: Listing 6.2 Creating a balanced dataset
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2 创建平衡数据集
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Counts the instances of “spam”'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算“spam”实例的数量'
- en: '#2 Randomly samples “ham” instances to match the number of “spam” instances'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 随机采样“ham”实例以匹配“spam”实例的数量'
- en: '#3 Combines ham subset with “spam”'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将“ham”子集与“spam”合并'
- en: 'After executing the previous code to balance the dataset, we can see that we
    now have equal amounts of spam and non-spam messages:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 执行之前的代码以平衡数据集后，我们可以看到现在我们有相等数量的垃圾邮件和非垃圾邮件消息：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we convert the “string” class labels `"ham"` and `"spam"` into integer
    class labels 0 and 1, respectively:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将“string”类标签`"ham"`和`"spam"`分别转换为整数类标签0和1：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This process is similar to converting text into token IDs. However, instead
    of using the GPT vocabulary, which consists of more than 50,000 words, we are
    dealing with just two token IDs: 0 and 1.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程类似于将文本转换为标记ID。然而，我们处理的是仅包含两个标记ID：0和1，而不是使用包含超过50,000个单词的GPT词汇表。
- en: 'Next, we create a `random_split` function to split the dataset into three parts:
    70% for training, 10% for validation, and 20% for testing. (These ratios are common
    in machine learning to train, adjust, and evaluate models.)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`random_split`函数，将数据集分为三部分：70%用于训练，10%用于验证，20%用于测试。（这些比例在机器学习中很常见，用于训练、调整和评估模型。）
- en: Listing 6.3 Splitting the dataset
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3 分割数据集
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Shuffles the entire DataFrame'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 打乱整个DataFrame'
- en: '#2 Calculates split indices'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算分割索引'
- en: '#3 Splits the DataFrame'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 分割DataFrame'
- en: '#4 Test size is implied to be 0.2 as the remainder.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 测试大小隐含为0.2，作为剩余部分。'
- en: 'Let’s save the dataset as CSV (comma-separated value) files so we can reuse
    it later:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据集保存为CSV（逗号分隔值）文件，这样我们以后可以重用它：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Thus far, we have downloaded the dataset, balanced it, and split it into training
    and evaluation subsets. Now we will set up the PyTorch data loaders that will
    be used to train the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经下载了数据集，平衡了它，并将其分为训练和评估子集。现在我们将设置用于训练模型的PyTorch数据加载器。
- en: 6.3 Creating data loaders
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 创建数据加载器
- en: 'We will develop PyTorch data loaders conceptually similar to those we implemented
    while working with text data. Previously, we utilized a sliding window technique
    to generate uniformly sized text chunks, which we then grouped into batches for
    more efficient model training. Each chunk functioned as an individual training
    instance. However, we are now working with a spam dataset that contains text messages
    of varying lengths. To batch these messages as we did with the text chunks, we
    have two primary options:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发与我们在处理文本数据时实现的PyTorch数据加载器概念上相似的数据加载器。以前，我们利用滑动窗口技术生成均匀大小的文本块，然后将它们分组到批次中，以更有效地进行模型训练。每个块作为一个单独的训练实例。然而，我们现在正在处理包含不同长度文本消息的垃圾邮件数据集。为了像处理文本块那样批量处理这些消息，我们有两种主要选项：
- en: Truncate all messages to the length of the shortest message in the dataset or
    batch.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有消息截断到数据集或批次中最短消息的长度。
- en: Pad all messages to the length of the longest message in the dataset or batch.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有消息填充到数据集或批次中最长消息的长度。
- en: The first option is computationally cheaper, but it may result in significant
    information loss if shorter messages are much smaller than the average or longest
    messages, potentially reducing model performance. So, we opt for the second option,
    which preserves the entire content of all messages.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法在计算上更便宜，但如果较短的消息比平均或最长的消息小得多，可能会造成显著的信息损失，从而降低模型性能。因此，我们选择第二种选项，它保留了所有消息的全部内容。
- en: To implement batching, where all messages are padded to the length of the longest
    message in the dataset, we add padding tokens to all shorter messages. For this
    purpose, we use `"<|endoftext|>"` as a padding token.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现批处理，将所有消息填充到数据集中最长消息的长度，我们向所有较短的消息添加填充标记。为此，我们使用 `"<|endoftext|>"` 作为填充标记。
- en: 'However, instead of appending the string `"<|endoftext|>"` to each of the text
    messages directly, we can add the token ID corresponding to `"<|endoftext|>"`
    to the encoded text messages, as illustrated in figure 6.6\. `50256` is the token
    ID of the padding token `"<|endoftext|>"`. We can double-check whether the token
    ID is correct by encoding the `"<|endoftext|>"` using the *GPT-2 tokenizer* from
    the `tiktoken` package that we used previously:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不必直接将字符串 `"<|endoftext|>"` 添加到每个文本消息中，我们可以将对应于 `"<|endoftext|>"` 的标记 ID
    添加到编码后的文本消息中，如图 6.6 所示。`50256` 是填充标记 `"<|endoftext|>"` 的标记 ID。我们可以通过使用之前使用的 `tiktoken`
    包中的 *GPT-2 分词器* 对 `"<|endoftext|>"` 进行编码来双重检查标记 ID 是否正确：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![figure](../Images/6-6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-6.png)'
- en: Figure 6.6 The input text preparation process. First, each input text message
    is converted into a sequence of token IDs. Then, to ensure uniform sequence lengths,
    shorter sequences are padded with a padding token (in this case, token ID 50256)
    to match the length of the longest sequence.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.6 输入文本准备过程。首先，每个输入文本消息被转换成一个标记 ID 序列。然后，为了确保序列长度一致，较短的序列使用填充标记（在这种情况下，标记
    ID 50256）填充到最长序列的长度。
- en: Indeed, executing the preceding code returns `[50256]`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，执行前面的代码返回 `[50256]`。
- en: 'We first need to implement a PyTorch `Dataset`, which specifies how the data
    is loaded and processed before we can instantiate the data loaders. For this purpose,
    we define the `SpamDataset` class, which implements the concepts in figure 6.6\.
    This `SpamDataset` class handles several key tasks: it encodes the text messages
    into token sequences, identifies the longest sequence in the training dataset,
    and ensures that all other sequences are padded with a *padding token* to match
    the length of the longest sequence.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要实现一个 PyTorch `Dataset`，它指定了在实例化数据加载器之前数据的加载和处理方式。为此，我们定义了 `SpamDataset`
    类，该类实现了图 6.6 中的概念。这个 `SpamDataset` 类处理几个关键任务：它将文本消息编码成标记序列，识别训练数据集中的最长序列，并确保所有其他序列都使用
    *填充标记* 填充到最长序列的长度。
- en: Listing 6.4 Setting up a Pytorch `Dataset` class
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4 设置 Pytorch `Dataset` 类
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Pretokenizes texts'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 预分词文本'
- en: '#2 Truncates sequences if they are longer than max_length'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果序列长度超过 max_length 则截断序列'
- en: '#3 Pads sequences to the longest sequence'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将序列填充到最长序列'
- en: 'The `SpamDataset` class loads data from the CSV files we created earlier, tokenizes
    the text using the GPT-2 tokenizer from `tiktoken`, and allows us to *pad* or
    *truncate* the sequences to a uniform length determined by either the longest
    sequence or a predefined maximum length. This ensures each input tensor is of
    the same size, which is necessary to create the batches in the training data loader
    we implement next:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`SpamDataset` 类从我们之前创建的 CSV 文件中加载数据，使用 `tiktoken` 的 GPT-2 分词器对文本进行分词，并允许我们将序列
    *填充* 或 *截断* 到由最长序列或预定义的最大长度决定的统一长度。这确保了每个输入张量的大小相同，这对于创建我们接下来实现的训练数据加载器中的批次是必要的：'
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The longest sequence length is stored in the dataset’s `max_length` attribute.
    If you are curious to see the number of tokens in the longest sequence, you can
    use the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最长序列长度存储在数据集的 `max_length` 属性中。如果你想知道最长序列中标记的数量，可以使用以下代码：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code outputs `120`, showing that the longest sequence contains no more than
    120 tokens, a common length for text messages. The model can handle sequences
    of up to 1,024 tokens, given its context length limit. If your dataset includes
    longer texts, you can pass `max_length=1024` when creating the training dataset
    in the preceding code to ensure that the data does not exceed the model’s supported
    input (context) length.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出 `120`，表明最长序列中不超过 120 个标记，这是文本消息的常见长度。考虑到其上下文长度限制，模型可以处理最多 1,024 个标记的序列。如果你的数据集中包含更长的文本，你可以在创建训练数据集时传递
    `max_length=1024`，以确保数据不超过模型支持的输入（上下文）长度。
- en: 'Next, we pad the validation and test sets to match the length of the longest
    training sequence. Importantly, any validation and test set samples exceeding
    the length of the longest training example are truncated using `encoded_text[:self.max_length]`
    in the `SpamDataset` code we defined earlier. This truncation is optional; you
    can set `max_length=None` for both validation and test sets, provided there are
    no sequences exceeding 1,024 tokens in these sets:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将验证集和测试集填充到最长训练序列的长度。重要的是，任何超过最长训练示例长度的验证集和测试集样本都会使用我们之前定义的`SpamDataset`代码中的`encoded_text[:self.max_length]`进行截断。这种截断是可选的；如果验证集和测试集中没有超过1,024个标记的序列，则可以将`max_length=None`设置为两者：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Exercise 6.1 Increasing the context length
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习6.1 增加上下文长度
- en: Pad the inputs to the maximum number of tokens the model supports and observe
    how it affects the predictive performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入填充到模型支持的标记数最大值，并观察它如何影响预测性能。
- en: Using the datasets as inputs, we can instantiate the data loaders similarly
    to when we were working with text data. However, in this case, the targets represent
    class labels rather than the next tokens in the text. For instance, if we choose
    a batch size of 8, each batch will consist of eight training examples of length
    120 and the corresponding class label of each example, as illustrated in figure
    6.7.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据集作为输入，我们可以像处理文本数据时一样实例化数据加载器。然而，在这种情况下，目标表示类别标签，而不是文本中的下一个标记。例如，如果我们选择批大小为8，每个批次将包含八个长度为120的训练示例以及每个示例对应的类别标签，如图6.7所示。
- en: '![figure](../Images/6-7.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-7.png)'
- en: Figure 6.7 A single training batch consisting of eight text messages represented
    as token IDs. Each text message consists of 120 token IDs. A class label array
    stores the eight class labels corresponding to the text messages, which can be
    either `0` (“not spam”) or `1` (“spam”).
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 显示了一个包含八个文本消息的训练批次，这些文本消息以标记ID表示。每个文本消息由120个标记ID组成。一个类别标签数组存储了与文本消息对应的八个类别标签，这些标签可以是`0`（“非垃圾邮件”）或`1`（“垃圾邮件”）。
- en: The code in the following listing creates the training, validation, and test
    set data loaders that load the text messages and labels in batches of size 8.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码创建训练、验证和测试集数据加载器，这些加载器以8个批次的容量加载文本消息和标签。
- en: Listing 6.5 Creating PyTorch data loaders
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5 创建PyTorch数据加载器
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 This setting ensures compatibility with most computers.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 此设置确保与大多数计算机的兼容性。'
- en: 'To ensure that the data loaders are working and are, indeed, returning batches
    of the expected size, we iterate over the training loader and then print the tensor
    dimensions of the last batch:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据加载器正在正常工作，并且确实返回了预期大小的批次，我们遍历训练加载器，然后打印最后一个批次的张量维度：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The output is
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, the input batches consist of eight training examples with 120
    tokens each, as expected. The label tensor stores the class labels corresponding
    to the eight training examples.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，输入批次由八个训练示例组成，每个示例有120个标记，正如预期的那样。标签张量存储了与八个训练示例对应的类别标签。
- en: 'Lastly, to get an idea of the dataset size, let’s print the total number of
    batches in each dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了了解数据集的大小，让我们打印每个数据集中的总批次数：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The number of batches in each dataset are
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集中的批次数是
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we’ve prepared the data, we need to prepare the model for fine-tuning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，我们需要为微调准备模型。
- en: 6.4 Initializing a model with pretrained weights
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 使用预训练权重初始化模型
- en: We must prepare the model for classification fine-tuning to identify spam messages.
    We start by initializing our pretrained model, as highlighted in figure 6.8.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为分类微调准备模型以识别垃圾邮件。我们首先初始化我们的预训练模型，如图6.8所示。
- en: '![figure](../Images/6-8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-8.png)'
- en: Figure 6.8 The three-stage process for classification fine-tuning the LLM. Having
    completed stage 1, preparing the dataset, we now must initialize the LLM, which
    we will then fine-tune to classify spam messages.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8 对LLM进行分类微调的三阶段过程。在完成阶段1，准备数据集后，我们现在必须初始化LLM，然后我们将对其进行微调以分类垃圾邮件。
- en: 'To begin the model preparation process, we employ the same configurations we
    used to pretrain unlabeled data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始模型准备过程，我们使用与预训练未标记数据相同的配置：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Vocabulary size'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 词汇量大小'
- en: '#2 Context length'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 上下文长度'
- en: '#3 Dropout rate'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Dropout率'
- en: '#4 Query-key-value bias'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 查询-键-值偏差'
- en: Next, we import the `download_and_load_gpt2` function from the `gpt_download.py`
    file and reuse the `GPTModel` class and `load_weights_into_gpt` function from
    pretraining (see chapter 5) to load the downloaded weights into the GPT model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从`gpt_download.py`文件中导入`download_and_load_gpt2`函数，并重用预训练中的`GPTModel`类和`load_weights_into_gpt`函数（参见第5章）将下载的权重加载到GPT模型中。
- en: Listing 6.6 Loading a pretrained GPT model
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6 加载预训练的GPT模型
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After loading the model weights into the `GPTModel`, we reuse the text generation
    utility function from chapters 4 and 5 to ensure that the model generates coherent
    text:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型权重加载到`GPTModel`之后，我们重用第4章和第5章中的文本生成实用函数，以确保模型生成连贯的文本：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following output shows the model generates coherent text, which is indicates
    that the model weights have been loaded correctly:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示模型生成了连贯的文本，这表明模型权重已正确加载：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before we start fine-tuning the model as a spam classifier, let’s see whether
    the model already classifies spam messages by prompting it with instructions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始将模型作为垃圾邮件分类器进行微调之前，让我们看看模型是否已经通过提示指令来对垃圾邮件消息进行分类：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The model output is
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出是
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Based on the output, it’s apparent that the model is struggling to follow instructions.
    This result is expected, as it has only undergone pretraining and lacks instruction
    fine-tuning. So, let’s prepare the model for classification fine-tuning.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，很明显模型在遵循指令方面有困难。这个结果是预期的，因为它只经过了预训练，缺乏指令微调。因此，让我们为分类微调准备模型。
- en: 6.5 Adding a classification head
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 添加分类头
- en: 'We must modify the pretrained LLM to prepare it for classification fine-tuning.
    To do so, we replace the original output layer, which maps the hidden representation
    to a vocabulary of 50,257, with a smaller output layer that maps to two classes:
    `0` (“not spam”) and `1` (“spam”), as shown in figure 6.9\. We use the same model
    as before, except we replace the output layer.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须修改预训练的LLM，以便为分类微调做准备。为此，我们用一个小型的输出层替换了原始的输出层，该输出层将隐藏表示映射到50,257个词汇，新的输出层映射到两个类别：`0`（“非垃圾邮件”）和`1`（“垃圾邮件”），如图6.9所示。我们使用与之前相同的模型，只是替换了输出层。
- en: '![figure](../Images/6-9.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-9.png)'
- en: Figure 6.9 Adapting a GPT model for spam classification by altering its architecture.
    Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary
    of 50,257 tokens. To detect spam, we replace this layer with a new output layer
    that maps the same 768 hidden units to just two classes, representing “spam” and
    “not spam.”
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 通过改变其架构来适应GPT模型进行垃圾邮件分类。最初，模型的线性输出层将768个隐藏单元映射到50,257个标记的词汇表。为了检测垃圾邮件，我们用一个新的输出层替换了这个层，该层将相同的768个隐藏单元映射到仅两个类别，代表“垃圾邮件”和“非垃圾邮件”。
- en: Output layer nodes
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输出层节点
- en: We could technically use a single output node since we are dealing with a binary
    classification task. However, it would require modifying the loss function, as
    I discuss in “Losses Learned—Optimizing Negative Log-Likelihood and Cross-Entropy
    in PyTorch” ([https://mng.bz/NRZ2](https://mng.bz/NRZ2)). Therefore, we choose
    a more general approach, where the number of output nodes matches the number of
    classes. For example, for a three-class problem, such as classifying news articles
    as “Technology,” “Sports,” or “Politics,” we would use three output nodes, and
    so forth.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们可以使用单个输出节点，因为我们处理的是一个二元分类任务。然而，这将需要修改损失函数，正如我在“损失函数学习——在PyTorch中优化负对数似然和交叉熵”中讨论的那样。因此，我们选择了一个更通用的方法，其中输出节点的数量与类别的数量相匹配。例如，对于将新闻文章分类为“技术”、“体育”或“政治”的三个类别问题，我们将使用三个输出节点，依此类推。
- en: 'Before we attempt the modification shown in figure 6.9, let’s print the model
    architecture via `print(model`):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试图6.9中显示的修改之前，让我们通过`print(model)`打印出模型架构：
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This output neatly lays out the architecture we laid out in chapter 4\. As previously
    discussed, the `GPTModel` consists of embedding layers followed by 12 identical
    *transformer blocks* (only the last block is shown for brevity), followed by a
    final `LayerNorm` and the output layer, `out_head`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出清晰地展示了我们在第4章中提出的架构。正如之前讨论的那样，`GPTModel`由嵌入层组成，随后是12个相同的*transformer blocks*（为了简洁起见，只显示了最后一个块），然后是一个最终的`LayerNorm`和输出层`out_head`。
- en: Next, we replace the `out_head` with a new output layer (see figure 6.9) that
    we will fine-tune.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`out_head`替换为一个新的输出层（见图6.9），我们将对其进行微调。
- en: Fine-tuning selected layers vs. all layers
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调选定层与所有层
- en: Since we start with a pretrained model, it’s not necessary to fine-tune all
    model layers. In neural network-based language models, the lower layers generally
    capture basic language structures and semantics applicable across a wide range
    of tasks and datasets. So, fine-tuning only the last layers (i.e., layers near
    the output), which are more specific to nuanced linguistic patterns and task-specific
    features, is often sufficient to adapt the model to new tasks. A nice side effect
    is that it is computationally more efficient to fine-tune only a small number
    of layers. Interested readers can find more information, including experiments,
    on which layers to fine-tune in appendix B.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从一个预训练模型开始，没有必要微调所有模型层。在基于神经网络的自然语言模型中，底层通常捕捉适用于广泛任务和数据集的基本语言结构和语义。因此，仅微调最后几层（即靠近输出的层），这些层更具体于细微的语言模式和特定任务的特性，通常足以使模型适应新任务。一个很好的副作用是，仅微调少量层在计算上更有效率。感兴趣的读者可以在附录B中找到更多关于应该微调哪些层的信息，包括实验。
- en: 'To get the model ready for classification fine-tuning, we first *freeze* the
    model, meaning that we make all layers nontrainable:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让模型准备好进行分类微调，我们首先将模型**冻结**，这意味着我们使所有层不可训练：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Then, we replace the output layer (`model.out_head`), which originally maps
    the layer inputs to 50,257 dimensions, the size of the vocabulary (see figure
    6.9).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们替换输出层（`model.out_head`），它最初将层输入映射到50,257维，即词汇表的大小（见图6.9）。
- en: Listing 6.7 Adding a classification layer
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7 添加分类层
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: To keep the code more general, we use `BASE_CONFIG["emb_dim"]`, which is equal
    to 768 in the `"gpt2-small` `(124M)"` model. Thus, we can also use the same code
    to work with the larger GPT-2 model variants.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码更通用，我们使用`BASE_CONFIG["emb_dim"]`，在`"gpt2-small` `(124M)"`模型中等于768。因此，我们也可以使用相同的代码来处理更大的GPT-2模型变体。
- en: This new `model.out_head` output layer has its `requires_grad` attribute set
    to `True` by default, which means that it’s the only layer in the model that will
    be updated during training. Technically, training the output layer we just added
    is sufficient. However, as I found in experiments, fine-tuning additional layers
    can noticeably improve the predictive performance of the model. (For more details,
    refer to appendix B.) We also configure the last transformer block and the final
    `LayerNorm` module, which connects this block to the output layer, to be trainable,
    as depicted in figure 6.10.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的`model.out_head`输出层默认将其`requires_grad`属性设置为`True`，这意味着它是模型中唯一将在训练期间更新的层。技术上，仅训练我们刚刚添加的输出层就足够了。然而，正如我在实验中发现的那样，微调额外的层可以显著提高模型的预测性能。（更多细节请参阅附录B。）我们还配置了最后一个转换器块和最终的`LayerNorm`模块，该模块将此块连接到输出层，如图6.10所示。
- en: '![figure](../Images/6-10.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-10.png)'
- en: Figure 6.10 The GPT model includes 12 repeated transformer blocks. Alongside
    the output layer, we set the final `LayerNorm` and the last transformer block
    as trainable. The remaining 11 transformer blocks and the embedding layers are
    kept nontrainable.
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10 GPT模型包含12个重复的转换器块。与输出层一起，我们将最终的`LayerNorm`和最后一个转换器块设置为可训练。其余的11个转换器块和嵌入层保持不可训练。
- en: 'To make the final `LayerNorm` and last transformer block trainable, we set
    their respective `requires_grad` to `True`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使最终的`LayerNorm`和最后一个转换器块可训练，我们将它们的`requires_grad`分别设置为`True`：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Exercise 6.2 Fine-tuning the whole model
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习6.2 微调整个模型
- en: Instead of fine-tuning just the final transformer block, fine-tune the entire
    model and assess the effect on predictive performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是仅微调最后的转换器块，微调整个模型并评估其对预测性能的影响。
- en: 'Even though we added a new output layer and marked certain layers as trainable
    or nontrainable, we can still use this model similarly to how we have previously.
    For instance, we can feed it an example text identical to our previously used
    example text:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们添加了一个新的输出层并将某些层标记为可训练或不可训练，我们仍然可以像以前一样使用这个模型。例如，我们可以给它一个与之前使用的示例文本相同的示例文本：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 shape: (batch_size, num_tokens)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 形状：`(batch_size, num_tokens)`'
- en: 'The print output shows that the preceding code encodes the inputs into a tensor
    consisting of four input tokens:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出显示，前面的代码将输入编码成一个包含四个输入标记的张量：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we can pass the encoded token IDs to the model as usual:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像往常一样将编码后的标记ID传递给模型：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output tensor looks like the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出张量看起来如下：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A similar input would have previously produced an output tensor of `[1,` `4,`
    `50257]`, where `50257` represents the vocabulary size. The number of output rows
    corresponds to the number of input tokens (in this case, four). However, each
    output’s embedding dimension (the number of columns) is now 2 instead of 50,257
    since we replaced the output layer of the model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的输入之前会产生一个 `[1,` `4,` `50257]` 的输出张量，其中 `50257` 代表词汇量。输出行的数量对应于输入标记的数量（在这种情况下，四个）。然而，每个输出的嵌入维度（列数）现在为2，而不是50,257，因为我们替换了模型的输出层。
- en: Remember that we are interested in fine-tuning this model to return a class
    label indicating whether a model input is “spam” or “not spam.” We don’t need
    to fine-tune all four output rows; instead, we can focus on a single output token.
    In particular, we will focus on the last row corresponding to the last output
    token, as shown in figure 6.11.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们感兴趣的是微调这个模型以返回一个类别标签，指示模型输入是“垃圾邮件”还是“非垃圾邮件”。我们不需要微调所有四行输出；相反，我们可以专注于单个输出标记。特别是，我们将专注于最后一行，对应于最后一个输出标记，如图6.11所示。
- en: '![figure](../Images/6-11.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-11.png)'
- en: Figure 6.11 The GPT model with a four-token example input and output. The output
    tensor consists of two columns due to the modified output layer. We are only interested
    in the last row corresponding to the last token when fine-tuning the model for
    spam classification.
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11 GPT模型，具有四个标记示例输入和输出。由于修改后的输出层，输出张量由两列组成。在微调模型进行垃圾邮件分类时，我们只对最后一个行对应最后一个标记感兴趣。
- en: 'To extract the last output token from the output tensor, we use the following
    code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要从输出张量中提取最后一个输出标记，我们使用以下代码：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This prints
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We still need to convert the values into a class-label prediction. But first,
    let’s understand why we are particularly interested in the last output token only.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要将值转换为类别标签预测。但首先，让我们了解为什么我们特别关注最后一个输出标记。
- en: We have already explored the attention mechanism, which establishes a relationship
    between each input token and every other input token, and the concept of a *causal
    attention mask*, commonly used in GPT-like models (see chapter 3). This mask restricts
    a token’s focus to its current position and the those before it, ensuring that
    each token can only be influenced by itself and the preceding tokens, as illustrated
    in figure 6.12.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了注意力机制，它建立了每个输入标记与每个其他输入标记之间的关系，以及*因果注意力掩码*的概念，这在GPT-like模型中常用（见第3章）。这个掩码限制了标记的关注点为其当前位置及其之前的那些位置，确保每个标记只能受到自身和前面标记的影响，如图6.12所示。
- en: '![figure](../Images/6-12.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-12.png)'
- en: Figure 6.12 The causal attention mechanism, where the attention scores between
    input tokens are displayed in a matrix format. The empty cells indicate masked
    positions due to the causal attention mask, preventing tokens from attending to
    future tokens. The values in the cells represent attention scores; the last token,
    `time`, is the only one that computes attention scores for all preceding tokens.
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12 因果注意力机制，其中输入标记之间的注意力得分以矩阵格式显示。空单元格表示由于因果注意力掩码而屏蔽的位置，防止标记关注未来的标记。单元格中的值代表注意力得分；最后一个标记“time”是唯一一个为所有先前标记计算注意力得分的标记。
- en: Given the causal attention mask setup in figure 6.12, the last token in a sequence
    accumulates the most information since it is the only token with access to data
    from all the previous tokens. Therefore, in our spam classification task, we focus
    on this last token during the fine-tuning process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.12中设置的因果注意力掩码下，序列中的最后一个标记积累了最多的信息，因为它是有权访问所有先前标记数据的唯一标记。因此，在我们的垃圾邮件分类任务中，我们在微调过程中关注这个最后一个标记。
- en: We are now ready to transform the last token into class label predictions and
    calculate the model’s initial prediction accuracy. Subsequently, we will fine-tune
    the model for the spam classification task.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好将最后一个标记转换为类别标签预测，并计算模型的初始预测准确率。随后，我们将微调模型以进行垃圾邮件分类任务。
- en: Exercise 6.3 Fine-tuning the first vs. last token
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习6.3 调整第一个与最后一个标记
- en: Try fine-tuning the first output token. Notice the changes in predictive performance
    compared to fine-tuning the last output token.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试微调第一个输出标记。注意与微调最后一个输出标记相比，预测性能的变化。
- en: 6.6 Calculating the classification loss and accuracy
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 计算分类损失和准确率
- en: 'Only one small task remains before we fine-tune the model: we must implement
    the model evaluation functions used during fine-tuning, as illustrated in figure
    6.13.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们微调模型之前，只剩下一个小的任务：我们必须实现微调期间使用的模型评估函数，如图 6.13 所示。
- en: '![figure](../Images/6-13.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-13.png)'
- en: 'Figure 6.13 The three-stage process for classification fine-tuning the LLM.
    We''ve completed the first six steps. We are now ready to undertake the last step
    of stage 2: implementing the functions to evaluate the model’s performance to
    classify spam messages before, during, and after the fine-tuning.'
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.13 对 LLM 进行分类微调的三个阶段过程。我们已经完成了前六个步骤。我们现在准备进行第二阶段最后一步：实现评估模型在微调前后以及分类垃圾邮件性能的函数。
- en: Before implementing the evaluation utilities, let’s briefly discuss how we convert
    the model outputs into class label predictions. We previously computed the token
    ID of the next token generated by the LLM by converting the 50,257 outputs into
    probabilities via the `softmax` function and then returning the position of the
    highest probability via the `argmax` function. We take the same approach here
    to calculate whether the model outputs a “spam” or “not spam” prediction for a
    given input, as shown in figure 6.14\. The only difference is that we work with
    2-dimensional instead of 50,257-dimensional outputs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现评估工具之前，让我们简要讨论一下我们如何将模型输出转换为类别标签预测。我们之前通过将 50,257 个输出转换为通过 `softmax` 函数的概率，然后通过
    `argmax` 函数返回最高概率的位置来计算 LLM 生成的下一个标记的标记 ID。我们在这里采取相同的方法来计算模型对于给定输入输出“垃圾邮件”或“非垃圾邮件”预测，如图
    6.14 所示。唯一的区别是我们处理的是二维输出而不是 50,257 维输出。
- en: '![figure](../Images/6-14.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6-14.png)'
- en: Figure 6.14 The model outputs corresponding to the last token are converted
    into probability scores for each input text. The class labels are obtained by
    looking up the index position of the highest probability score. The model predicts
    the spam labels incorrectly because it has not yet been trained.
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.14 对应于最后一个标记的模型输出被转换为每个输入文本的概率分数。通过查找最高概率分数的索引位置来获得类别标签。由于模型尚未经过训练，模型错误地预测了垃圾邮件标签。
- en: 'Let’s consider the last token output using a concrete example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来考虑最后一个标记的输出：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The values of the tensor corresponding to the last token are
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于最后一个标记的张量值是
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can obtain the class label:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获取类别标签：
- en: '[PRE37]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In this case, the code returns `1`, meaning the model predicts that the input
    text is “spam.” Using the `softmax` function here is optional because the largest
    outputs directly correspond to the highest probability scores. Hence, we can simplify
    the code without using softmax:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，代码返回 `1`，意味着模型预测输入文本是“垃圾邮件。”在这里使用 `softmax` 函数是可选的，因为最大的输出直接对应于最高的概率分数。因此，我们可以简化代码而不使用
    softmax：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This concept can be used to compute the classification accuracy, which measures
    the percentage of correct predictions across a dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以用来计算分类准确率，它衡量的是数据集中正确预测的百分比。
- en: To determine the classification accuracy, we apply the `argmax`-based prediction
    code to all examples in the dataset and calculate the proportion of correct predictions
    by defining a `calc_accuracy_loader` function.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定分类准确率，我们将基于 `argmax` 的预测代码应用于数据集中的所有示例，并通过定义一个 `calc_accuracy_loader` 函数来计算正确预测的比例。
- en: Listing 6.8 Calculating the classification accuracy
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.8 计算分类准确率
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#1 Logits of last output token'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 最后输出标记的对数'
- en: 'Let’s use the function to determine the classification accuracies across various
    datasets estimated from 10 batches for efficiency:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用该函数来确定跨各种数据集的分类准确率，这些数据集是从 10 批次中估计出来的，以提高效率：
- en: '[PRE40]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Via the `device` setting, the model automatically runs on a GPU if a GPU with
    Nvidia CUDA support is available and otherwise runs on a CPU. The output is
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `device` 设置，如果可用带有 Nvidia CUDA 支持的 GPU，则模型自动在 GPU 上运行，否则在 CPU 上运行。输出是
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As we can see, the prediction accuracies are near a random prediction, which
    would be 50% in this case. To improve the prediction accuracies, we need to fine-tune
    the model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，预测准确率接近随机预测，在这种情况下为 50%。为了提高预测准确率，我们需要微调模型。
- en: 'However, before we begin fine-tuning the model, we must define the loss function
    we will optimize during training. Our objective is to maximize the spam classification
    accuracy of the model, which means that the preceding code should output the correct
    class labels: `0` for non-spam and `1` for spam.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始微调模型之前，我们必须定义我们将在训练中优化的损失函数。我们的目标是最大化模型的垃圾邮件分类准确性，这意味着前面的代码应该输出正确的类别标签：`0`
    表示非垃圾邮件，`1` 表示垃圾邮件。
- en: 'Because classification accuracy is not a differentiable function, we use cross-entropy
    loss as a proxy to maximize accuracy. Accordingly, the `calc_loss_batch` function
    remains the same, with one adjustment: we focus on optimizing only the last token,
    `model(input_batch)[:,` `-1,` `:]`, rather than all tokens, `model(input_batch)`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类准确性不是一个可微分的函数，我们使用交叉熵损失作为最大化准确性的代理。因此，`calc_loss_batch` 函数保持不变，但有一个调整：我们只优化最后一个标记，`model(input_batch)[:,
    -1, :]`，而不是所有标记，`model(input_batch)`：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '#1 Logits of last output token'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 最后输出标记的 logits'
- en: We use the `calc_loss_batch` function to compute the loss for a single batch
    obtained from the previously defined data loaders. To calculate the loss for all
    batches in a data loader, we define the `calc_loss_loader` function as before.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `calc_loss_batch` 函数计算从先前定义的数据加载器中获得的单个批次的损失。为了计算数据加载器中所有批次的损失，我们像以前一样定义
    `calc_loss_loader` 函数。
- en: Listing 6.9 Calculating the classification loss
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.9 计算分类损失
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#1 Ensures number of batches doesn’t exceed batches in data loader'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 确保批次数不超过数据加载器中的批次数'
- en: 'Similar to calculating the training accuracy, we now compute the initial loss
    for each data set:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算训练准确性类似，我们现在计算每个数据集的初始损失：
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '#1 Disables gradient tracking for efficiency because we are not training yet'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为了效率，禁用梯度跟踪，因为我们还没有开始训练'
- en: The initial loss values are
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 初始损失值是
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Next, we will implement a training function to fine-tune the model, which means
    adjusting the model to minimize the training set loss. Minimizing the training
    set loss will help increase the classification accuracy, which is our overall
    goal.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个训练函数来微调模型，这意味着调整模型以最小化训练集损失。最小化训练集损失将有助于提高分类准确性，这是我们整体目标。
- en: 6.7 Fine-tuning the model on supervised data
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 在监督数据上微调模型
- en: We must define and use the training function to fine-tune the pretrained LLM
    and improve its spam classification accuracy. The training loop, illustrated in
    figure 6.15, is the same overall training loop we used for pretraining; the only
    difference is that we calculate the classification accuracy instead of generating
    a sample text to evaluate the model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须定义并使用训练函数来微调预训练的 LLM 并提高其垃圾邮件分类的准确性。如图 6.15 所示的训练循环，与我们用于预训练的整体训练循环相同；唯一的区别是我们计算分类准确性而不是生成样本文本来评估模型。
- en: '![figure](../Images/6-15.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-15.png)'
- en: Figure 6.15 A typical training loop for training deep neural networks in PyTorch
    consists of several steps, iterating over the batches in the training set for
    several epochs. In each loop, we calculate the loss for each training set batch
    to determine loss gradients, which we use to update the model weights to minimize
    the training set loss.
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.15 PyTorch 中训练深度神经网络的典型训练循环包括几个步骤，迭代训练集中的批次几个时期。在每个循环中，我们计算每个训练集批次的损失以确定损失梯度，我们使用这些梯度来更新模型权重以最小化训练集损失。
- en: The training function implementing the concepts shown in figure 6.15 also closely
    mirrors the `train_model_simple` function used for pretraining the model. The
    only two distinctions are that we now track the number of training examples seen
    (`examples_seen`) instead of the number of tokens, and we calculate the accuracy
    after each epoch instead of printing a sample text.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 实现图 6.15 中所示概念的训练函数也与用于预训练模型的 `train_model_simple` 函数非常相似。唯一的两个区别是，我们现在跟踪看到的训练示例数量（`examples_seen`）而不是标记数量，并在每个时期后计算准确性而不是打印样本文本。
- en: Listing 6.10 Fine-tuning the model to classify spam
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10 微调模型以分类垃圾邮件
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '#1 Initialize lists to track losses and examples seen'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化跟踪损失和已看到示例的列表'
- en: '#2 Main training loop'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 主要训练循环'
- en: '#3 Sets model to training mode'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将模型设置为训练模式'
- en: '#4 Resets loss gradients from the previous batch iteration'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 重置上一批迭代中的损失梯度'
- en: '#5 Calculates loss gradients'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 计算损失梯度'
- en: '#6 Updates model weights using loss gradients'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 使用损失梯度更新模型权重'
- en: '#7 New: tracks examples instead of tokens'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 新增：跟踪示例而不是标记'
- en: '#8 Optional evaluation step'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 可选评估步骤'
- en: '#9 Calculates accuracy after each epoch'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 计算每个轮数后的准确率'
- en: 'The `evaluate_model` function is identical to the one we used for pretraining:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`函数与我们用于预训练的函数相同：'
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we initialize the optimizer, set the number of training epochs, and initiate
    the training using the `train_classifier_simple` function. The training takes
    about 6 minutes on an M3 MacBook Air laptop computer and less than half a minute
    on a V100 or A100 GPU:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化优化器，设置训练轮数，并使用`train_classifier_simple`函数开始训练。在M3 MacBook Air笔记本电脑上训练大约需要6分钟，在V100或A100
    GPU上不到半分钟：
- en: '[PRE48]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output we see during the training is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中我们看到的输出如下：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We then use Matplotlib to plot the loss function for the training and validation
    set.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用Matplotlib绘制训练和验证集的损失函数。
- en: Listing 6.11 Plotting the classification loss
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11 绘制分类损失
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '#1 Plots training and validation loss against epochs'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将训练和验证损失与轮数对比绘图'
- en: '#2 Creates a second x-axis for examples seen'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为示例创建第二个x轴'
- en: '#3 Invisible plot for aligning ticks'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 隐藏的绘图用于对齐刻度'
- en: '#4 Adjusts layout to make room'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 调整布局以留出空间'
- en: Figure 6.16 plots the resulting loss curves.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16绘制了结果损失曲线。
- en: '![figure](../Images/6-16.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-16.png)'
- en: Figure 6.16 The model’s training and validation loss over the five training
    epochs. Both the training loss, represented by the solid line, and the validation
    loss, represented by the dashed line, sharply decline in the first epoch and gradually
    stabilize toward the fifth epoch. This pattern indicates good learning progress
    and suggests that the model learned from the training data while generalizing
    well to the unseen validation data.
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.16展示了模型在五个训练轮数中的训练和验证损失。训练损失（用实线表示）和验证损失（用虚线表示）在第一个轮数急剧下降，并逐渐稳定在第五个轮数。这种模式表明良好的学习进度，并表明模型从训练数据中学习，同时很好地泛化到未见过的验证数据。
- en: As we can see based on the sharp downward slope in figure 6.16, the model is
    learning well from the training data, and there is little to no indication of
    overfitting; that is, there is no noticeable gap between the training and validation
    set losses.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如基于图6.16中急剧下降的斜率所示，模型正在很好地从训练数据中学习，几乎没有过拟合的迹象；也就是说，训练集和验证集损失之间没有明显的差距。
- en: Choosing the number of epochs
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择训练轮数
- en: Earlier, when we initiated the training, we set the number of epochs to five.
    The number of epochs depends on the dataset and the task’s difficulty, and there
    is no universal solution or recommendation, although an epoch number of five is
    usually a good starting point. If the model overfits after the first few epochs
    as a loss plot (see figure 6.16), you may need to reduce the number of epochs.
    Conversely, if the trendline suggests that the validation loss could improve with
    further training, you should increase the number of epochs. In this concrete case,
    five epochs is a reasonable number as there are no signs of early overfitting,
    and the validation loss is close to 0.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，当我们开始训练时，我们将训练轮数设置为五次。训练轮数取决于数据集和任务的难度，没有通用的解决方案或建议，尽管通常五个轮数是一个良好的起点。如果在最初的几个轮数后模型出现过拟合（如损失图所示，见图6.16），你可能需要减少轮数。相反，如果趋势线表明验证损失可以通过进一步训练而改善，你应该增加轮数。在这个具体案例中，五个轮数是一个合理的数字，因为没有出现早期过拟合的迹象，验证损失接近0。
- en: 'Using the same `plot_values` function, let’s now plot the classification accuracies:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的`plot_values`函数，现在让我们绘制分类准确率：
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Figure 6.17 graphs the resulting accuracy. The model achieves a relatively high
    training and validation accuracy after epochs 4 and 5. Importantly, we previously
    set `eval_iter=5` when using the `train_classifier_simple` function, which means
    our estimations of training and validation performance are based on only five
    batches for efficiency during training.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17显示了结果准确率。模型在第四和第五个轮数后实现了相对较高的训练和验证准确率。重要的是，我们之前在使用`train_classifier_simple`函数时设置了`eval_iter=5`，这意味着我们的训练和验证性能估计仅基于五个批次以提高训练效率。
- en: '![figure](../Images/6-17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-17.png)'
- en: Figure 6.17 Both the training accuracy (solid line) and the validation accuracy
    (dashed line) increase substantially in the early epochs and then plateau, achieving
    almost perfect accuracy scores of 1.0\. The close proximity of the two lines throughout
    the epochs suggests that the model does not overfit the training data very much.
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.17 训练准确率（实线）和验证准确率（虚线）在早期epoch中大幅增加，然后达到平台期，几乎完美的准确率得分为1.0。两条线在整个epoch中的接近表明模型并没有过度拟合训练数据。
- en: 'Now we must calculate the performance metrics for the training, validation,
    and test sets across the entire dataset by running the following code, this time
    without defining the `eval_iter` value:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须通过运行以下代码，在整个数据集上计算训练、验证和测试集的性能指标，这次不定义`eval_iter`值：
- en: '[PRE52]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The resulting accuracy values are
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 结果准确率值是
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The training and test set performances are almost identical. The slight discrepancy
    between the training and test set accuracies suggests minimal overfitting of the
    training data. Typically, the validation set accuracy is somewhat higher than
    the test set accuracy because the model development often involves tuning hyperparameters
    to perform well on the validation set, which might not generalize as effectively
    to the test set. This situation is common, but the gap could potentially be minimized
    by adjusting the model’s settings, such as increasing the dropout rate (`drop_rate`)
    or the `weight_ decay` parameter in the optimizer configuration.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集的性能几乎相同。训练集和测试集准确率之间的微小差异表明训练数据过拟合最小。通常，验证集准确率略高于测试集准确率，因为模型开发通常涉及调整超参数以在验证集上表现良好，这可能不会像在测试集上那样有效地推广。这种情况很常见，但通过调整模型的设置，如增加dropout率（`drop_rate`）或优化器配置中的`weight_decay`参数，可以最大限度地减少差距。
- en: 6.8 Using the LLM as a spam classifier
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 使用LLM作为垃圾邮件分类器
- en: Having fine-tuned and evaluated the model, we are now ready to classify spam
    messages (see figure 6.18). Let’s use our fine-tuned GPT-based spam classification
    model. The following `classify_review` function follows data preprocessing steps
    similar to those we used in the `SpamDataset` implemented earlier. Then, after
    processing text into token IDs, the function uses the model to predict an integer
    class label, similar to what we implemented in section 6.6, and then returns the
    corresponding class name.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调和评估了模型之后，我们现在准备好对垃圾邮件进行分类（见图6.18）。让我们使用我们的基于GPT的微调垃圾邮件分类模型。下面的`classify_review`函数遵循与我们在之前实现的`SpamDataset`中使用的类似的数据预处理步骤。然后，在将文本处理成标记ID之后，该函数使用模型预测一个整数类标签，类似于我们在第6.6节中实现的方法，然后返回相应的类名。
- en: '![figure](../Images/6-18.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/6-18.png)'
- en: Figure 6.18 The three-stage process for classification fine-tuning our LLM.
    Step 10 is the final step of stage 3—using the fine-tuned model to classify new
    spam messages.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.18 对LLM进行分类微调的三阶段过程。步骤10是第三阶段的最后一步——使用微调后的模型对新的垃圾邮件消息进行分类。
- en: Listing 6.12 Using the model to classify new texts
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.12 使用模型对新的文本进行分类
- en: '[PRE54]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '#1 Prepares inputs to the model'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 准备模型输入'
- en: '#2 Truncates sequences if they are too long'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果序列太长则截断'
- en: '#3 Pads sequences to the longest sequence'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将序列填充到最长序列'
- en: '#4 Adds batch dimension'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 添加批量维度'
- en: '#5 Models inference without gradient tracking'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 模型推理不跟踪梯度'
- en: '#6 Logits of the last output token'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 最后输出标记的logits'
- en: '#7 Returns the classified result'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 返回分类结果'
- en: 'Let’s try this `classify_review` function on an example text:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个示例文本上尝试这个`classify_review`函数：
- en: '[PRE55]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The resulting model correctly predicts `"spam"`. Let’s try another example:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型正确预测了“垃圾邮件”。让我们再试一个例子：
- en: '[PRE56]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The model again makes a correct prediction and returns a “not spam” label.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 模型再次做出正确预测并返回“非垃圾邮件”标签。
- en: 'Finally, let’s save the model in case we want to reuse the model later without
    having to train it again. We can use the `torch.save` method:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们保存模型，以防我们以后想重用模型而无需再次训练。我们可以使用`torch.save`方法：
- en: '[PRE57]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Once saved, the model can be loaded:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 保存后，可以加载模型：
- en: '[PRE58]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There are different strategies for fine-tuning LLMs, including classification
    fine-tuning and instruction fine-tuning.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于微调LLM，有不同策略，包括分类微调和指令微调。
- en: Classification fine-tuning involves replacing the output layer of an LLM via
    a small classification layer.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类微调涉及通过一个小型分类层替换LLM的输出层。
- en: In the case of classifying text messages as “spam” or “not spam,” the new classification
    layer consists of only two output nodes. Previously, we used the number of output
    nodes equal to the number of unique tokens in the vocabulary (i.e., 50,256).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将文本消息分类为“垃圾邮件”或“非垃圾邮件”的情况下，新的分类层仅包含两个输出节点。之前，我们使用输出节点的数量等于词汇表中的唯一标记数量（即，50,256）。
- en: Instead of predicting the next token in the text as in pretraining, classification
    fine-tuning trains the model to output a correct class label—for example, “spam”
    or “not spam.”
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与预训练时预测文本中的下一个标记不同，分类微调训练模型输出正确的类别标签——例如，“垃圾邮件”或“非垃圾邮件”。
- en: The model input for fine-tuning is text converted into token IDs, similar to
    pretraining.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型的输入是转换为标记ID的文本，类似于预训练。
- en: Before fine-tuning an LLM, we load the pretrained model as a base model.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微调大型语言模型（LLM）之前，我们加载预训练模型作为基础模型。
- en: Evaluating a classification model involves calculating the classification accuracy
    (the fraction or percentage of correct predictions).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型涉及计算分类准确率（正确预测的分数或百分比）。
- en: Fine-tuning a classification model uses the same cross entropy loss function
    as when pretraining the LLM.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调分类模型使用与预训练LLM时相同的交叉熵损失函数。
