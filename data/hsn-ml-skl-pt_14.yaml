- en: Chapter 12\. Deep Computer Vision Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章\. 使用卷积神经网络的深度计算机视觉
- en: 'Although IBM’s Deep Blue supercomputer beat the chess world champion Garry
    Kasparov back in 1996, it wasn’t until fairly recently that computers were able
    to reliably perform seemingly trivial tasks such as detecting a puppy in a picture
    or recognizing spoken words. Why are these tasks so effortless to us humans? The
    answer lies in the fact that perception largely takes place outside the realm
    of our consciousness, within specialized visual, auditory, and other sensory modules
    in our brains. By the time sensory information reaches our consciousness, it is
    already adorned with high-level features; for example, when you look at a picture
    of a cute puppy, you cannot choose *not* to see the puppy, *not* to notice its
    cuteness. Nor can you explain *how* you recognize a cute puppy; it’s just obvious
    to you. Thus, we cannot trust our subjective experience: perception is not trivial
    at all, and to understand it we must look at how our sensory modules work.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管IBM的Deep Blue超级计算机在1996年击败了国际象棋世界冠军加里·卡斯帕罗夫，但直到最近，计算机才能可靠地执行看似微不足道的任务，比如在图片中检测小狗或识别语音。为什么这些任务对我们人类来说如此容易？答案在于，感知在很大程度上发生在我们的意识之外，在我们大脑中专门的视觉、听觉和其他感官模块内。当感官信息到达我们的意识时，它已经带有高级特征；例如，当你看一张可爱小狗的图片时，你无法选择*不*看到小狗，*不*注意到它的可爱。你也不能解释*如何*你识别出可爱的小狗；这对你来说只是显而易见的。因此，我们不能相信我们的主观经验：感知绝非微不足道，要理解它，我们必须看看我们的感官模块是如何工作的。
- en: '*Convolutional neural networks* (CNNs) emerged from the study of the brain’s
    visual cortex, and they have been used in computer image recognition since the
    1980s. Over the last 15 years, thanks to the increase in computational power,
    the amount of available training data, and the tricks presented in [Chapter 11](ch11.html#deep_chapter)
    for training deep nets, CNNs have managed to achieve superhuman performance on
    some complex visual tasks. They power image search services, self-driving cars,
    automatic video classification systems, and more. Moreover, CNNs are not restricted
    to visual perception: they are also successful at many other tasks, such as voice
    recognition and natural language processing. However, we will focus on visual
    applications for now.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNN）起源于对大脑视觉皮层的研究，自20世纪80年代以来，它们已被用于计算机图像识别。在过去15年里，得益于计算能力的提升、可用训练数据的增加，以及[第11章](ch11.html#deep_chapter)中提出的用于训练深度网络的技巧，CNN在处理一些复杂的视觉任务上已经达到了超越人类的表现。它们为图像搜索服务、自动驾驶汽车、自动视频分类系统等提供了动力。此外，CNN不仅限于视觉感知：它们在许多其他任务中也取得了成功，例如语音识别和自然语言处理。然而，我们现在将专注于视觉应用。'
- en: In this chapter we will explore where CNNs came from, what their building blocks
    look like, and how to implement them using PyTorch. Then we will discuss some
    of the best CNN architectures, as well as other visual tasks, including object
    detection (classifying multiple objects in an image and placing bounding boxes
    around them) and semantic segmentation (classifying each pixel according to the
    class of the object it belongs to).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨卷积神经网络（CNN）的起源，它们的基本构建块是什么样的，以及如何使用PyTorch来实现它们。然后我们将讨论一些最佳的CNN架构，以及其他视觉任务，包括目标检测（在图像中分类多个对象并在它们周围放置边界框）和语义分割（根据对象所属的类别对每个像素进行分类）。
- en: The Architecture of the Visual Cortex
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉皮层的架构
- en: David H. Hubel and Torsten Wiesel performed a series of experiments on cats
    in [1958](https://homl.info/71)⁠^([1](ch12.html#id2734)) and [1959](https://homl.info/72)⁠^([2](ch12.html#id2735))
    (and a [few years later on monkeys](https://homl.info/73)⁠^([3](ch12.html#id2736))),
    giving crucial insights into the structure of the visual cortex (the authors received
    the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular,
    they showed that many neurons in the visual cortex have a small *local receptive
    field*, meaning they react only to visual stimuli located in a limited region
    of the visual field (see [Figure 12-1](#cat_visual_cortex_diagram), in which the
    local receptive fields of five neurons are represented by dashed circles). The
    receptive fields of different neurons may overlap, and together they tile the
    whole visual field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·H·休伯尔（David H. Hubel）和托斯顿·威塞尔（Torsten Wiesel）在[1958年](https://homl.info/71)⁠^([1](ch12.html#id2734))和[1959年](https://homl.info/72)⁠^([2](ch12.html#id2735))（以及几年后对猴子进行的实验](https://homl.info/73)⁠^([3](ch12.html#id2736)))进行了一系列对猫的实验，为视觉皮层的结构（1981年，作者因他们的工作获得了诺贝尔生理学或医学奖）提供了关键见解。特别是，他们展示了视觉皮层中的许多神经元具有小的*局部感受野*，这意味着它们只对视觉场中有限区域的视觉刺激做出反应（参见[图12-1](#cat_visual_cortex_diagram)，其中五个神经元的局部感受野由虚线圆表示）。不同神经元的感受野可能重叠，并且共同覆盖整个视觉场。
- en: '![Diagram illustrating how biological neurons in the visual cortex respond
    to specific patterns within small receptive fields and integrate this information
    to recognize complex shapes like a house.](assets/hmls_1201.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![说明视觉皮层中的生物神经元如何对小型感受野内的特定模式做出反应并整合这些信息以识别复杂形状（如房屋）的图](assets/hmls_1201.png)'
- en: Figure 12-1\. Biological neurons in the visual cortex respond to specific patterns
    in small regions of the visual field called receptive fields; as the visual signal
    makes its way through consecutive brain modules, neurons respond to more complex
    patterns in larger receptive fields
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. 视觉皮层中的生物神经元对视觉场中小区域内的特定模式做出反应；随着视觉信号通过连续的大脑模块，神经元对更大感受野中的更复杂模式做出反应
- en: Moreover, the authors showed that some neurons react only to images of horizontal
    lines, while others react only to lines with different orientations (two neurons
    may have the same receptive field but react to different line orientations). They
    also noticed that some neurons have larger receptive fields, and they react to
    more complex patterns that are combinations of the lower-level patterns. These
    observations led to the idea that the higher-level neurons are based on the outputs
    of neighboring lower-level neurons (in [Figure 12-1](#cat_visual_cortex_diagram),
    notice that each neuron is connected only to nearby neurons from the previous
    layer). This powerful architecture is able to detect all sorts of complex patterns
    in any area of the visual field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者还表明，一些神经元只对水平线的图像做出反应，而另一些神经元只对具有不同方向的线条做出反应（两个神经元可能具有相同的感觉野，但对不同的线条方向做出反应）。他们还注意到，一些神经元具有更大的感受野，并对更复杂的模式做出反应，这些模式是较低级别模式的组合。这些观察导致了一个想法，即高级神经元基于相邻的低级神经元的输出（在[图12-1](#cat_visual_cortex_diagram)中，注意每个神经元只与前一层的附近神经元相连）。这种强大的架构能够检测视觉场任何区域的复杂模式。
- en: 'These studies of the visual cortex inspired the [neocognitron](https://homl.info/74),⁠^([4](ch12.html#id2738))
    introduced in 1980, which gradually evolved into what we now call convolutional
    neural networks. An important milestone was a [1998 paper](https://homl.info/75)⁠^([5](ch12.html#id2739))
    by Yann LeCun et al. that introduced the famous *LeNet-5* architecture, which
    became widely used by banks to recognize handwritten digits on checks. This architecture
    has some building blocks that you already know, such as fully connected layers
    and sigmoid activation functions, but it also introduces two new building blocks:
    *convolutional layers* and *pooling layers*. Let’s look at them now.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对视觉皮层的研究启发了1980年引入的[新认知机](https://homl.info/74)⁠^([4](ch12.html#id2738))，它逐渐演变成了我们现在所说的卷积神经网络。一个重要的里程碑是Yann
    LeCun等人于[1998年发表的一篇论文](https://homl.info/75)⁠^([5](ch12.html#id2739))，介绍了著名的*LeNet-5*架构，该架构被银行广泛用于识别支票上的手写数字。这个架构有一些你已经知道的构建块，例如全连接层和sigmoid激活函数，但它还引入了两个新的构建块：*卷积层*和*池化层*。现在让我们来看看它们。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not simply use a deep neural network with fully connected layers for image
    recognition tasks? Unfortunately, although this works fine for small images (e.g.,
    Fashion MNIST), it breaks down for larger images because of the huge number of
    parameters it requires. For example, a 100 × 100–pixel image has 10,000 pixels,
    and if the first layer has just 1,000 neurons (which already severely restricts
    the amount of information transmitted to the next layer), this means a total of
    10 million connections. And that’s just the first layer. CNNs solve this problem
    using partially connected layers and weight sharing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不简单地使用具有全连接层的深度神经网络进行图像识别任务呢？遗憾的是，尽管这对于小图像（例如，Fashion MNIST）效果很好，但由于它需要大量的参数，因此对于大图像来说就会失效。例如，一个100
    × 100像素的图像有10,000个像素，如果第一层只有1,000个神经元（这已经严重限制了传递给下一层的信息量），这意味着总共需要1,000万个连接。而这只是第一层。CNN通过使用部分连接层和权重共享来解决这一问题。
- en: Convolutional Layers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'The most important building block of a CNN is the *convolutional layer*:⁠^([6](ch12.html#id2744))
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (like they were in the layers discussed in previous chapters),
    but only to pixels in their receptive fields (see [Figure 12-2](#cnn_layers_diagram)).
    In turn, each neuron in the second convolutional layer is connected only to neurons
    located within a small rectangle in the first layer. This architecture allows
    the network to concentrate on small low-level features in the first hidden layer,
    then assemble them into larger higher-level features in the next hidden layer,
    and so on. This hierarchical structure is well-suited to deal with composite objects,
    which are common in real-world images: this is one of the reasons why CNNs work
    so well for image recognition.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN最重要的构建块是**卷积层**：⁠^([6](ch12.html#id2744)) 第一卷积层中的神经元并不连接到输入图像中的每一个像素（就像在前面章节中讨论的层那样），而是只连接到它们感受野中的像素（见[图12-2](#cnn_layers_diagram))。反过来，第二卷积层中的每个神经元只连接到第一层中位于一个小矩形内的神经元。这种架构使得网络能够专注于第一隐藏层中的小低级特征，然后将它们组装成下一隐藏层中的更大高级特征，依此类推。这种层次结构非常适合处理现实世界图像中常见的复合对象：这也是CNN在图像识别中表现如此出色的原因之一。
- en: '![Diagram illustrating the structure of CNN layers, highlighting the local
    receptive fields connecting the first convolutional layer to the second through
    the input.](assets/hmls_1202.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图示CNN层的结构，突出显示通过输入将第一卷积层与第二卷积层连接起来的局部感受野。](assets/hmls_1202.png)'
- en: Figure 12-2\. CNN layers with rectangular local receptive fields
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. 具有矩形局部感受野的CNN层
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All the multilayer neural networks we’ve looked at so far had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the neural network. In a CNN each layer is represented in 2D, which makes
    it easier to match neurons with their corresponding inputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所查看的所有多层神经网络都是由一长串神经元组成的层，我们必须在将它们输入神经网络之前将输入图像展平为1D。在CNN中，每一层都以2D形式表示，这使得匹配神经元与其相应的输入变得更容易。
- en: A neuron located in row *i*, column *j* of a given layer is connected to the
    outputs of the neurons in the previous layer located in rows *i* to *i* + *f*[*h*]
    – 1, columns *j* to *j* + *f*[*w*] – 1, where *f*[*h*] and *f*[*w*] are the height
    and width of the receptive field (see [Figure 12-3](#slide_and_padding_diagram)).
    In order for a layer to have the same height and width as the previous layer,
    it is common to add zeros around the inputs, as shown in the diagram. This is
    called *zero* *padding*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 位于给定层第*i*行、第*j*列的神经元连接到前一层中位于*i*到*i* + *f*[*h*] – 1行、*j*到*j* + *f*[*w*] – 1列的神经元的输出，其中*f*[*h*]和*f*[*w*]是感受野的高度和宽度（见[图12-3](#slide_and_padding_diagram))。为了使层具有与前一层相同的高度和宽度，通常会在输入周围添加零，如图所示。这被称为**零填充**。
- en: It is also possible to connect a large input layer to a much smaller layer by
    spacing out the receptive fields, as shown in [Figure 12-4](#stride_diagram).
    This dramatically reduces the model’s computational complexity. The horizontal
    or vertical step size from one receptive field to the next is called the *stride*.
    In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 ×
    4 layer, using 3 × 3 receptive fields and a stride of 2\. In this example the
    stride is the same in both directions, which is generally the case (although there
    are exceptions). A neuron located in row *i*, column *j* in the upper layer is
    connected to the outputs of the neurons in the previous layer located in rows
    *i* × *s*[*h*] to *i* × *s*[*h*] + *f*[*h*] – 1, columns *j* × *s*[*w*] to *j*
    × *s*[*w*] + *f*[*w*] – 1, where *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过间隔感受野，也可以将一个大型输入层连接到一个远小的层，如图[图12-4](#stride_diagram)所示。这大大降低了模型的计算复杂度。从一个感受野到下一个感受野的水平或垂直步长称为**步长**。在图中，一个5
    × 7输入层（包括零填充）通过3 × 3感受野和2的步长连接到一个3 × 4层。在这个例子中，两个方向的步长是相同的，这通常是情况（尽管也有例外）。上层中位于第*i*行、第*j*列的神经元连接到前一层中位于*i*
    × *s*[*h*]到*i* × *s*[*h*] + *f*[*h*] – 1行、*j* × *s*[*w*]到*j* × *s*[*w*] + *f*[*w*]
    – 1列的神经元的输出，其中*s*[*h*]和*s*[*w*]是垂直和水平步长。
- en: '![Diagram illustrating the connection between a 5 × 7 input layer and a 3 ×
    4 layer using 3 × 3 receptive fields with a stride of 2, including zero padding.](assets/hmls_1203.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图解5 × 7输入层与3 × 4层之间的连接，使用3 × 3感受野和2的步长，包括零填充](assets/hmls_1203.png)'
- en: Figure 12-3\. Connections between layers and zero padding
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3。层之间的连接和零填充
- en: '![Diagram illustrating the concept of reducing dimensionality with a stride
    of 2 on a grid, showing overlapping operations with different colored boxes and
    lines.](assets/hmls_1204.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图解在网格上以2为步长降低维度的概念，显示不同颜色框和线条的重叠操作](assets/hmls_1204.png)'
- en: Figure 12-4\. Reducing dimensionality using a stride of 2
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4。使用2的步长降低维度
- en: Filters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器
- en: A neuron’s weights can be represented as a small image the size of the receptive
    field. For example, [Figure 12-5](#filters_diagram) shows two possible sets of
    weights, called *filters* (or *convolution kernels*, or just *kernels*). The first
    one is represented as a black square with a vertical white line in the middle
    (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of
    1s); neurons using these weights will ignore everything in their receptive field
    except for the central vertical line (since all inputs will be multiplied by 0,
    except for the ones in the central vertical line). The second filter is a black
    square with a horizontal white line in the middle. Neurons using these weights
    will ignore everything in their receptive field except for the central horizontal
    line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的权重可以表示为与感受野大小相同的小图像。例如，[图12-5](#filters_diagram)显示了两组可能的权重，称为**滤波器**（或**卷积核**，或简称**核**）。第一个是一个中间有垂直白色线的黑色正方形（它是一个7
    × 7的全0矩阵，除了中间列，该列全为1）；使用这些权重的神经元将忽略它们感受野中的所有内容，除了中心垂直线（因为所有输入都将乘以0，除了中心垂直线上的输入）。第二个滤波器是一个中间有水平白色线的黑色正方形。使用这些权重的神经元将忽略它们感受野中的所有内容，除了中心水平线。
- en: '![Diagram showing the input image processed by vertical and horizontal filters
    to produce two feature maps, each highlighting different line orientations.](assets/hmls_1205.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图解通过垂直和水平滤波器处理输入图像以产生两个特征图，每个特征图突出显示不同的线方向](assets/hmls_1205.png)'
- en: Figure 12-5\. Applying two different filters to get two feature maps
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5。应用两个不同的滤波器以获得两个特征图
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In deep learning, we often build a single model that takes the raw inputs and
    produces the final outputs. This is called *end-to-end learning*. In contrast,
    classical vision systems would usually divide the system into a sequence of specialized
    modules.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们通常构建一个单一模型，该模型接受原始输入并产生最终输出。这被称为**端到端学习**。相比之下，经典视觉系统通常会将系统划分为一系列专门的模块。
- en: 'Now if all neurons in a layer use the same vertical line filter (and the same
    bias term), and you feed the network the input image shown in [Figure 12-5](#filters_diagram)
    (the bottom image), the layer will output the top-left image. Notice that the
    vertical white lines get enhanced while the rest gets blurred. Similarly, the
    upper-right image is what you get if all neurons use the same horizontal line
    filter; notice that the horizontal white lines get enhanced while the rest is
    blurred out. Thus, a layer full of neurons using the same filter outputs a *feature
    map*, which highlights the areas in an image that activate the filter the most.
    But don’t worry, you won’t have to define the filters manually: instead, during
    training the convolutional layer will automatically learn the most useful filters
    for its task, and the layers above will learn to combine them into more complex
    patterns.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一层中的所有神经元都使用相同的垂直线滤波器（以及相同的偏置项），并且你将网络输入[图12-5](#filters_diagram)（底部图像）所示的输入图像，该层将输出顶部左边的图像。注意，垂直的白色线条被增强，而其余部分则被模糊。同样，如果所有神经元都使用相同的水平线滤波器，你将得到右上方的图像；注意，水平白色线条被增强，而其余部分则被模糊掉。因此，使用相同滤波器的神经元层输出一个*特征图*，该图突出了图像中激活滤波器最强烈的区域。但不用担心，你不需要手动定义滤波器：相反，在训练过程中，卷积层将自动学习其任务中最有用的滤波器，而上面的层将学会将它们组合成更复杂的模式。
- en: Stacking Multiple Feature Maps
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个特征图的堆叠
- en: Up to now, for simplicity, I have represented each convolutional layer as a
    2D layer, but in reality a convolutional layer has multiple filters (you decide
    how many) and it outputs one feature map per filter, so the output is more accurately
    represented in 3D (see [Figure 12-6](#cnn_layers_volume_diagram)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，为了简单起见，我将每个卷积层表示为一个二维层，但现实中卷积层有多个滤波器（你决定有多少个）并且每个滤波器输出一个特征图，因此输出更准确地用三维表示（参见[图12-6](#cnn_layers_volume_diagram)）。
- en: '![Diagram illustrating two convolutional layers with multiple filters processing
    a color image with three RGB channels, producing one feature map per filter.](assets/hmls_1206.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![说明具有多个滤波器的两个卷积层处理具有三个RGB通道的彩色图像的示意图，每个滤波器生成一个特征图。](assets/hmls_1206.png)'
- en: Figure 12-6\. Two convolutional layers with multiple filters each (kernels),
    processing a color image with three color channels; each convolutional layer outputs
    one feature map per filter
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6\. 每个卷积层有多个滤波器（核）处理具有三个颜色通道的彩色图像；每个卷积层为每个滤波器输出一个特征图
- en: There is one neuron per pixel in each feature map, and all neurons within a
    given feature map share the same parameters (i.e., the same kernel and bias term).
    Neurons in different feature maps use different parameters. A neuron’s receptive
    field is the same as described earlier, but it extends across all the feature
    maps of the previous layer. In short, a convolutional layer simultaneously applies
    multiple trainable filters to its inputs, making it capable of detecting multiple
    features anywhere in its inputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征图中的每个像素对应一个神经元，并且给定特征图内的所有神经元共享相同的参数（即相同的核和偏置项）。不同特征图中的神经元使用不同的参数。神经元的感受野与之前描述的相同，但它扩展到前一层的所有特征图中。简而言之，卷积层同时对其输入应用多个可训练的滤波器，使其能够在输入的任何位置检测到多个特征。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fact that all neurons in a feature map share the same parameters dramatically
    reduces the number of parameters in the model. Once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location. In contrast,
    once a fully connected neural network has learned to recognize a pattern in one
    location, it can only recognize it in that particular location.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征图中的所有神经元共享相同的参数，这大大减少了模型中的参数数量。一旦CNN学会在某个位置识别一个模式，它就可以在任何其他位置识别该模式。相比之下，一旦全连接神经网络学会在某个位置识别一个模式，它只能在那个特定位置识别该模式。
- en: 'Input images are also composed of multiple sublayers: one per *color channel*.
    As mentioned in [Chapter 8](ch08.html#unsupervised_learning_chapter), there are
    typically three: red, green, and blue (RGB). Grayscale images have just one channel,
    but some images may have many more—for example, satellite images that capture
    extra light frequencies (such as infrared).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像也由多个子层组成：每个*颜色通道*一个。如[第8章](ch08.html#unsupervised_learning_chapter)中所述，通常有三个：红色、绿色和蓝色（RGB）。灰度图像只有一个通道，但有些图像可能有更多——例如，捕捉额外光频的卫星图像（如红外线）。
- en: Specifically, a neuron located in row *i*, column *j* of the feature map *k*
    in a given convolutional layer *l* is connected to the outputs of the neurons
    in the previous layer *l* – 1, located in rows *i* × *s*[*h*] to *i* × *s*[*h*]
    + *f*[*h*] – 1 and columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, across
    all feature maps (in layer *l* – *1*). Note that, within a layer, all neurons
    located in the same row *i* and column *j* but in different feature maps are connected
    to the outputs of the exact same neurons in the previous layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，给定卷积层 *l* 中特征图 *k* 中行 *i*、列 *j* 的神经元连接到前一层 *l* – 1 中行 *i* × *s*[*h*] 到
    *i* × *s*[*h*] + *f*[*h*] – 1 和列 *j* × *s*[*w*] 到 *j* × *s*[*w*] + *f*[*w*] –
    1 的神经元的输出，跨越所有特征图（在层 *l* – *1*）。注意，在一个层中，位于同一行 *i* 和列 *j* 但在不同特征图中的所有神经元都连接到前一层中完全相同的神经元的输出。
- en: '[Equation 12-1](#convolutional_layer_equation) summarizes the preceding explanations
    in one big mathematical equation: it shows how to compute the output of a given
    neuron in a convolutional layer. It is a bit ugly due to all the different indices,
    but all it does is calculate the weighted sum of all the inputs, plus the bias
    term.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 12-1](#convolutional_layer_equation) 总结了前面的解释，用一个大的数学方程表示：它展示了如何计算卷积层中给定神经元的输出。由于所有不同的索引，它看起来有点丑，但它所做的只是计算所有输入的加权和，加上偏置项。'
- en: Equation 12-1\. Computing the output of a neuron in a convolutional layer
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 12-1\. 计算卷积层中神经元的输出
- en: <mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mi>b</mi>
    <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>h</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi>
    <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <msub><mi>x</mi>
    <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo
    lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup> <mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi> <mo>'</mo>
    <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub> <mo>+</mo>
    <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi> <mo>'</mo>
    <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub> <mo>+</mo>
    <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mi>b</mi>
    <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>h</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi>
    <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <msub><mi>x</mi>
    <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo
    lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup> <mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi> <mo>'</mo>
    <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub> <mo>+</mo>
    <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi> <mo>'</mo>
    <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub> <mo>+</mo>
    <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow>
- en: 'In this equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*z*[*i*,] [*j*,] [*k*] is the output of the neuron located in row *i*, column
    *j* in feature map *k* of the convolutional layer (layer *l*).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z*[*i*,] [*j*,] [*k*] 是位于特征图 *k* 中行 *i*、列 *j* 的神经元的输出，该神经元位于卷积层（层 *l*）中。'
- en: As explained earlier, *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides, *f*[*h*] and *f*[*w*] are the height and width of the receptive field,
    and *f*[*n*′] is the number of feature maps in the previous layer (layer *l* –
    1).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，*s*[*h*] 和 *s*[*w*] 是垂直和水平步长，*f*[*h*] 和 *f*[*w*] 是感受野的高度和宽度，*f*[*n*′] 是前一层（层
    *l* – 1）中的特征图数量。
- en: '*x*[*i*′,] [*j*′,] [*k*′] is the output of the neuron located in layer *l*
    – 1, row *i*′, column *j*′, feature map *k*′ (or channel *k*′ if the previous
    layer is the input layer).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*′,] [*j*′,] [*k*′] 是位于层 *l* – 1、行 *i*′、列 *j*′、特征图 *k*′（如果前一层是输入层，则为通道
    *k*′）的神经元的输出。'
- en: '*b*[*k*] is the bias term for feature map *k* (in layer *l*). You can think
    of it as a knob that tweaks the overall brightness of the feature map *k*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*[*k*] 是特征图 *k*（在层 *l* 中）的偏置项。你可以将其视为一个旋钮，用于调整特征图 *k* 的整体亮度。'
- en: '*w*[*u*,] [*v*,] [*k*′,] [*k*] is the connection weight between any neuron
    in feature map *k* of the layer *l* and its input located at row *u*, column *v*
    (relative to the neuron’s receptive field), and feature map *k*′.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*u*,] [*v*,] [*k*′,] [*k*] 是层 *l* 中特征图 *k* 的任意神经元与其位于行 *u*、列 *v*（相对于神经元的感受野）及其输入之间的连接权重，以及特征图
    *k*′。'
- en: Let’s see how to create and use a convolutional layer using PyTorch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 PyTorch 创建和使用卷积层。
- en: Implementing Convolutional Layers with PyTorch
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现卷积层
- en: 'First, let’s load a couple of sample images using Scikit-Learn’s `load_sample_images()`
    function. The first image represents the tower of buddhist incense in China, while
    the second one represents a beautiful *Dahlia pinnata* flower. These images are
    represented as a Python list of NumPy unsigned byte arrays, so let’s stack these
    images into a single NumPy array, then convert it to a 32-bit float tensor, and
    rescale the pixel values from 0–255 to 0–1:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 Scikit-Learn 的 `load_sample_images()` 函数加载一些样本图像。第一幅图像代表中国佛教香塔，而第二幅图像则代表一朵美丽的
    *Dahlia pinnata* 花朵。这些图像以 Python 列表的形式表示 NumPy 无符号字节数组，因此让我们将这些图像堆叠成一个单一的 NumPy
    数组，然后将其转换为 32 位浮点张量，并将像素值从 0–255 缩放到 0–1：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at this tensor’s shape:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个张量的形状：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2][PRE3]``py[PRE4]`py` Let’s also use TorchVision’s `CenterCrop` class
    to center-crop the images:    [PRE5]py` `>>>` `cropped_images` `=` `T``.``CenterCrop``((``70``,`
    `120``))(``sample_images_permuted``)` [PRE6]py [PRE7]``py[PRE8]`` [PRE9]`` Now
    let’s create a 2D convolutional layer and feed it these cropped images to see
    what comes out. For this, PyTorch provides the `nn.Conv2d` layer. Under the hood,
    this layer relies on the `torch.nn.((("torch", "F.conv2d()")))functional.conv2d()`
    function. Let’s create a convolutional layer with 32 filters, each of size 7 ×
    7 (using `kernel_size=7`, which is equivalent to using `kernel_size=(7 , 7)`),
    and apply this layer to our small batch of two images:    [PRE10]    ###### Note    When
    we talk about a 2D convolutional layer, “2D” refers to the number of *spatial*
    dimensions (height and width), but as you can see, the layer takes 4D inputs:
    as we saw, the two additional dimensions are the batch size (first dimension)
    and the channels (second dimension).    Now let’s look at the output’s shape:    [PRE11]   [PRE12]`
    The output shape is similar to the input shape, with two main differences. First,
    there are 32 channels instead of 3\. This is because we set `out_channels=32`,
    so we get 32 output feature maps: instead of the intensity of red, green, and
    blue at each location, we now have the intensity of each feature at each location.
    Second, the height and width have both shrunk by 6 pixels. This is due to the
    fact that the `nn.Conv2d` layer does not use any zero-padding by default, which
    means that we lose a few pixels on the sides of the output feature maps, depending
    on the size of the filters. In this case, since the kernel size is 7, we lose
    6 pixels horizontally and 6 pixels vertically (i.e., 3 pixels on each side).    ######
    Warning    By default, the `padding` hyperparameter is set to 0, which means that
    padding is turned off. Oddly, this is also called *valid padding* since every
    neuron’s receptive field lies strictly within *valid* positions inside the input
    (it does not go out of bounds). You can actually set `padding="valid"`, which
    is equivalent to `padding=0`. It’s not a PyTorch naming quirk: everyone uses this
    confusing nomenclature.    If instead we set `padding="same"`, then the inputs
    are padded with enough zeros on all sides to ensure that the output feature maps
    end up with the *same* size as the inputs (hence the name of this option):    [PRE13]``
    `...` [PRE14] `>>>` `fmaps``.``shape` `` `torch.Size([2, 32, 70, 120])` `` [PRE15]`
    [PRE16]   [PRE17] [PRE18]`py [PRE19]py`` [PRE20]py[PRE21][PRE22][PRE23][PRE24]py[PRE25]py
    [PRE26] [PRE27][PRE28]``py[PRE29][PRE30] >>> class_names = weights.meta["categories"]
    `>>>` `[``class_names``[``class_id``]` `for` `class_id` `in` `y_pred``]` `` `[''palace'',
    ''daisy'']` `` [PRE31][PRE32][PRE33] >>> y_top3_logits, y_top3_class_ids = y_logits.topk(k=3,
    dim=1) `>>>` `[[``class_names``[``class_id``]` `for` `class_id` `in` `top3``]`
    `for` `top3` `in` `y_top3_class_ids``]` `` `[[''palace'', ''monastery'', ''lakeside''],
    [''daisy'', ''pot'', ''ant'']]` `` [PRE34]`` [PRE35] >>> y_top3_logits.softmax(dim=1)
    `tensor([[0.8618, 0.1185, 0.0197],`  `[0.8106, 0.0964, 0.0930]], device=''cuda:0'')`
    [PRE36]` [PRE37][PRE38][PRE39][PRE40][PRE41]  [PRE42] [PRE43]`` # Pretrained Models
    for Transfer Learning    If you want to build an image classifier but you do not
    have enough data to train it from scratch, then it is often a good idea to reuse
    the lower layers of a pretrained model, as we discussed in [Chapter 11](ch11.html#deep_chapter).
    In this section we will reuse the ConvNeXt model we loaded earlier—which was pretrained
    on ImageNet—and after replacing its classification head, we will fine-tune it
    on the [*102 Category Flower Dataset*](https://homl.info/flowers102)⁠^([29](ch12.html#id2920))
    (Flowers102 for short). This dataset only contains 10 images per class, and there
    are 102 classes in total (as the name indicates), so if you try to train a model
    from scratch, you will really struggle to get high accuracy. However, it’s quite
    easy to get over 90% accuracy using a good pretrained model. Let’s see how. First,
    let’s download the dataset using Torchvision:    [PRE44]    This code uses `partial()`
    to avoid repeating the same arguments three times. We also set `transform=weights.transforms()`
    to preprocess the images immediately when they are loaded. The Flowers102 dataset
    comes with three predefined splits, for training, validation, and testing. The
    first two have 10 images per class, but surprisingly the test set has many more
    (it has a variable number of images per class, between 20 and 238). In a real
    project, you would normally use most of your data for training rather than for
    testing, but this dataset was designed for computer vision research, and the authors
    purposely restricted the training set and the validation set.    We then create
    the data loaders, as usual:    [PRE45]    Many TorchVision datasets conveniently
    contain the class names in the `classes` attribute, but sadly not this dataset.^([30](ch12.html#id2922))
    If you prefer to see lovely names like “tiger lily”, “monkshood”, or “snapdragon”
    rather than boring class IDs, then you need to manually define the list of class
    names:    [PRE46]    Now let’s adapt our pretrained ConvNeXt-base model to this
    dataset. Since it was pretrained on ImageNet, which has 1,000 classes, the model’s
    head (i.e., its upper layers) was designed to output 1,000 logits. But we only
    have 102 classes, so we must chop the model’s head off and replace it with a smaller
    one. But how can we find it? Well let’s use the model’s `named_children()` method
    to find the name of its submodules:    [PRE47]   [PRE48] >>> model.classifier
    `Sequential(`  `(0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)`  `(1):
    Flatten(start_dim=1, end_dim=-1)`  `(2): Linear(in_features=1024, out_features=1000,
    bias=True)` `)` [PRE49]`As you can see, it’s an `nn.Sequential` module composed
    of a layer normalization layer, an `nn.Flatten` layer, and an `nn.Linear` layer
    with 1,024 inputs and 1,000 outputs. This `nn.Linear` layer is the output layer,
    and it’s the one we need to replace. We must only change the number of outputs:    [PRE50]    As
    explained in [Chapter 11](ch11.html#deep_chapter), it’s usually a good idea to
    freeze the weights of the pretrained layers, at least at the beginning of training.
    We can do this by freezing every single parameter in the model, and then unfreezing
    only the parameters of the head:    [PRE51]    Next, you can train this model
    for a few epochs, and you will already reach about 90% accuracy just by training
    the new head, without even fine-tuning the pretrained layers. After that, you
    can unfreeze the whole model, lower the learning rate—typically by a factor of
    10—and continue training the model. Give this a try, and see what accuracy you
    can reach!    To reach an even higher accuracy, it’s usually a good idea to perform
    some data augmentation on the training images. For this, you can try randomly
    flipping the training images horizontally, randomly rotating them by a small angle,
    randomly resizing and cropping them, and randomly tweaking their colors. This
    must all be done before running the ImageNet normalization step, which you can
    implement using a `Normalize` transform:    [PRE52]    ###### Tip    TorchVision
    comes with an `AutoAugment` transform which applies multiple augmentation operations
    optimized for ImageNet. It generalizes well to many other image datasets, and
    it also offers predefined settings for two other datasets: CIFAR10 and the street
    view house numbers (SVHN) dataset.    Here are some more ideas to continue to
    improve your model’s accuracy:    *   Try other pretrained models.           *   Extend
    the training set: find more flower images and label them.           *   Create
    an ensemble of models, and combine their predictions.           *   Analyze failure
    cases, and see whether they share specific characteristics, such as similar texture
    or color. You can then try to tweak image preprocessing to address these issues.           *   Use
    a learning schedule such as performance scheduling.           *   Unfreeze the
    layers gradually, starting from the top. Alternatively, you can use *differential
    learning rates*: apply a smaller learning rate to lower layers, and a larger learning
    rate to upper layers. You can do this by using parameter groups (see [Chapter 11](ch11.html#deep_chapter)).           *   Explore
    different optimizers and fine-tune their hyperparameters.           *   Try different
    regularization techniques.              ###### Tip    It’s worth spending time
    looking for models that were pretrained on similar images. For example, if you’re
    dealing with satellite images, aerial images, or even raster data such as digital
    elevation models (DEM), then models pretrained on ImageNet won’t help much. Instead,
    check out Microsoft’s *TorchGeo* library, which is similar to TorchVision but
    for geospatial data. For medical images, check out Project MONAI. For agricultural
    images, check out AgML. And so on.    With that, you can start training amazing
    image classifiers on your own images and classes! But there’s more to computer
    vision than just classification. For example, what if you also want to know *where*
    the flower is in a picture? Let’s look at this now.[PRE53]``  [PRE54]` [PRE55]
    # Classification and Localization    Localizing an object in a picture can be
    expressed as a regression task, as discussed in [Chapter 9](ch09.html#ann_chapter):
    to predict a bounding box around the object, a common approach is to predict the
    location of the bounding box’s center, as well as its width and height (alternatively,
    you could predict the horizontal and vertical coordinates of the object’s upper-left
    and lower-right corners). This means we have four numbers to predict. It does
    not require much change to the ConvNeXt model; we just need to add a second dense
    output layer with four units (e.g., on top of the global average pooling layer).
    Here’s a `FlowerLocator` model that adds a localization head to a given base model,
    such as our ConvNeXt model:    [PRE56]py    This locator model has two heads:
    the first outputs class logits, while the second outputs the bounding box. The
    localization head has the same number of inputs as the `nn.Linear` layer of the
    classification head, but it outputs just four numbers. The `forward()` method
    takes a batch of preprocessed images as input and outputs both the predicted class
    logits (102 per image) and the predicted bounding boxes (1 per image). After training
    this model, you can use it as follows:    [PRE57]py    But how can we train this
    model? Well, we saw how to train a model with two or more outputs in [Chapter 10](ch10.html#pytorch_chapter),
    and this one is no different: in this case, we can use the `nn.CrossEntropyLoss`
    for the classification head, and the `nn.MSELoss` for the localization head. The
    final loss can just be a weighted sum of the two. Voilà, that’s all there is to
    it.    Hey, not so fast! We have a problem: the Flowers102 dataset does not include
    any bounding boxes, so we need to add them ourselves. This is often one of the
    hardest and most costly parts of a machine learning project: labeling and annotating
    the data. It’s a good idea to spend time looking for the right tools. To annotate
    images with bounding boxes, you may want to use an open source labeling tool like
    Label Studio, OpenLabeler, ImgLab, Labelme, VoTT, or VGG Image Annotator, or perhaps
    a commercial tool like LabelBox, Supervisely, Roboflow, or RectLabel. Many of
    these are now AI assisted, greatly speeding up the annotation task. You may also
    want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you
    have a very large number of images to annotate. However, it is quite a lot of
    work to set up a crowdsourcing platform, prepare the form to be sent to the workers,
    supervise them, and ensure that the quality of the bounding boxes they produce
    is good, so make sure it is worth the effort. If there are just a few hundred
    or a even a couple of thousand images to label, and you don’t plan to do this
    frequently, it may be preferable to do it yourself: with the right tools, it will
    only take a few days, and you’ll also gain a better understanding of your dataset
    and task.    You can then create a custom dataset (see [Chapter 10](ch10.html#pytorch_chapter))
    where each entry contains an image, a label, and a bounding box. TorchVision conveniently
    includes a `BoundingBoxes` class that represents a list of bounding boxes. For
    example, the following code creates a bounding box for the largest flower in the
    first image of the Flowers102 training set (for now we only consider one bounding
    box per image, but we’ll discuss multiple bounding boxes per image later in this
    chapter):    [PRE58]py    ###### Tip    To visualize bounding boxes, use the `torchvi⁠sion.utils.draw_​bounding_boxes()`
    function. You will first need to convert the bounding boxes to the XYXY format
    using `torchvi⁠sion.​ops.box_convert()`.    The `BoundingBoxes` class is a subclass
    of `TVTensor`, which is a subclass of `torch.​Ten⁠sor`, so you can treat bounding
    boxes exactly like regular tensors, with extra features. Most importantly, you
    can transform bounding boxes using TorchVision’s transforms API v2\. For example,
    let’s use the transform we defined earlier to preprocess this bounding box:    [PRE59]py   [PRE60]`
    # Object Detection    The task of classifying and localizing multiple objects
    in an image is called *object detection*. Until a few years ago, a common approach
    was to take a CNN that was trained to classify and locate a single object roughly
    centered in the image, then slide this CNN across the image and make predictions
    at each step. The CNN was generally trained to predict not only class probabilities
    and a bounding box, but also an *objectness score*: this is the estimated probability
    that the image does indeed contain an object centered near the middle. This is
    a binary classification output; it can be produced by a dense output layer with
    a single unit, using the sigmoid activation function and trained using the binary
    cross-entropy loss.    ###### Note    Instead of an objectness score, a “no-object”
    class was sometimes added, but in general this did not work as well. The questions
    “Is an object present?” and “What type of object is it?” are best answered separately.    This
    sliding-CNN approach is illustrated in [Figure 12-25](#sliding_cnn_diagram). In
    this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the thick
    black rectangle—sliding across all 3 × 3 regions and making predictions at each
    step.  ![Diagram illustrating a sliding CNN approach on a grid over an image of
    pink roses, with colored rectangles indicating regions where predictions are made.](assets/hmls_1225.png)  ######
    Figure 12-25\. Detecting multiple objects by sliding a CNN across the image    In
    this figure, the CNN has already made predictions for three of these 3 × 3 regions:    *   When
    looking at the top-left 3 × 3 region (centered on the red-shaded grid cell located
    in the second row and second column), it detected the leftmost rose. Notice that
    the predicted bounding box exceeds the boundary of this 3 × 3 region. That’s absolutely
    fine: even though the CNN could not see the bottom part of the rose, it was able
    to make a reasonable guess as to where it might be. It also predicted class probabilities,
    giving a high probability to the “rose” class. Lastly, it predicted a fairly high
    objectness score, since the center of the bounding box lies within the central
    grid cell (in this figure, the objectness score is represented by the thickness
    of the bounding box).           *   When looking at the next 3 × 3 region, one
    grid cell to the right (centered on the shaded blue square), it did not detect
    any flower centered in that region, so it predicted a very low objectness score;
    therefore, the predicted bounding box and class probabilities can safely be ignored.
    You can see that the predicted bounding box was no good anyway.           *   Finally,
    when looking at the next 3 × 3 region, again one grid cell to the right (centered
    on the shaded green cell), it detected the rose at the top, although not perfectly.
    This rose is not well centered within this region, so the predicted objectness
    score was not very high.              You can imagine how sliding the CNN across
    the whole image would give you a total of 15 predicted bounding boxes, organized
    in a 3 × 5 grid, with each bounding box accompanied by its estimated class probabilities
    and objectness score. Since objects can have varying sizes, you may then want
    to slide the CNN again across 2 × 2 and 4 × 4 regions as well, to capture smaller
    and larger objects.    This technique is fairly straightforward, but as you can
    see it will often detect the same object multiple times, at slightly different
    positions. Some post-processing is needed to get rid of all the unnecessary bounding
    boxes. A common approach for this is called *non-max suppression* (NMS). Here’s
    how it works:    1.  First, get rid of all the bounding boxes for which the objectness
    score is below some threshold; since the CNN believes there’s no object at that
    location, the bounding box is useless.           2.  Find the remaining bounding
    box with the highest objectness score, and get rid of all the other remaining
    bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%).
    For example, in [Figure 12-25](#sliding_cnn_diagram), the bounding box with the
    max objectness score is the thick bounding box over the leftmost rose. The other
    bounding box that touches this same rose overlaps a lot with the max bounding
    box, so we will get rid of it (although in this example it would already have
    been removed in the previous step).           3.  Repeat step 2 until there are
    no more bounding boxes to get rid of.              This simple approach to object
    detection works pretty well, but it requires running the CNN many times (15 times
    in this example), so it is quite slow. Fortunately, there is a much faster way
    to slide a CNN across an image: using a *fully convolutional network* (FCN).    ##
    Fully Convolutional Networks    The idea of FCNs was first introduced in a [2015
    paper](https://homl.info/fcn)⁠^([33](ch12.html#id2962)) by Jonathan Long et al.,
    for semantic segmentation (the task of classifying every pixel in an image according
    to the class of the object it belongs to). The authors pointed out that you could
    replace the dense layers at the top of a CNN with convolutional layers. To understand
    this, let’s look at an example: suppose a dense layer with 200 neurons sits on
    top of a convolutional layer that outputs 100 feature maps, each of size 7 × 7
    (this is the feature map size, not the kernel size). Each neuron will compute
    a weighted sum of all 100 × 7 × 7 activations from the convolutional layer (plus
    a bias term). Now let’s see what happens if we replace the dense layer with a
    convolutional layer using 200 filters, each of size 7 × 7, and with `"valid"`
    padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel
    is exactly the size of the input feature maps and we are using `"valid"` padding).
    In other words, it will output 200 numbers, just like the dense layer did; and
    if you look closely at the computations performed by a convolutional layer, you
    will notice that these numbers will be precisely the same as those the dense layer
    produced. The only difference is that the dense layer’s output was a tensor of
    shape [*batch size*, 200], while the convolutional layer will output a tensor
    of shape [*batch size*, 200, 1, 1].    ###### Tip    To convert a dense layer
    to a convolutional layer, the number of filters in the convolutional layer must
    be equal to the number of units in the dense layer, the filter size must be equal
    to the size of the input feature maps, and you must use `"valid"` padding. The
    stride may be set to 1 or more, as we will see shortly.    Why is this important?
    Well, while a dense layer expects a specific input size (since it has one weight
    per input feature), a convolutional layer will happily process images of any size⁠^([34](ch12.html#id2967))
    (however, it does expect its inputs to have a specific number of channels, since
    each kernel contains a different set of weights for each input channel). Since
    an FCN contains only convolutional layers (and pooling layers, which have the
    same property), it can be trained and executed on images of any size!    For example,
    suppose we’d already trained a CNN for flower classification and localization,
    with an extra head for objectness. It was trained on 224 × 224 images, and it
    outputs 107 values per image:    *   The classification head outputs 102 class
    logits (one per class), trained using the `nn.CrossEntropyLoss`.           *   The
    objectness head outputs a single objectness logit, trained using the `nn.BCELoss`.           *   The
    localization head outputs four numbers describing the bounding box, trained using
    the CIoU loss.              We can now convert the CNN’s dense layers (`nn.Linear`)
    to convolutional layers (`nn.Conv2d`). In fact, we don’t even need to retrain
    the model; we can just copy the weights from the dense layers to the convolutional
    layers! Alternatively, we could have converted the CNN into an FCN before training.    Now
    suppose the last convolutional layer before the output layer (also called the
    bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224
    image (see the left side of [Figure 12-26](#fcn_diagram)). For example, this would
    be the case if the network contains 5 layers with stride 2 and `"same"` padding,
    so the spatial dimensions get divided by 2⁵ = 32 overall. If we feed the FCN a
    448 × 448 image (see the righthand side of [Figure 12-26](#fcn_diagram)), the
    bottleneck layer will now output 14 × 14 feature maps. Since the dense output
    layer was replaced by a convolutional layer using 107 filters of size 7 × 7, with
    `"valid"` padding and stride 1, the output will be composed of 107 feature maps,
    each of size 8 × 8 (since 14 – 7 + 1 = 8).    In other words, the FCN will process
    the whole image only once, and it will output an 8 × 8 grid where each cell contains
    the predictions for one region of the image: 107 numbers representing 102 class
    probabilities, 1 objectness score, and 4 bounding box coordinates. It’s exactly
    like taking the original CNN and sliding it across the image using 8 steps per
    row and 8 steps per column. To visualize this, imagine chopping the original image
    into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will
    be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions. However,
    the FCN approach is *much* more efficient, since the network only looks at the
    image once. In fact, *You Only Look Once* (YOLO) is the name of a very popular
    object detection architecture, which we’ll look at next.  ![A diagram illustrating
    a fully convolutional network processing a small and a large image, showing the
    progression from CNN layers to feature maps and convolution outputs.](assets/hmls_1226.png)  ######
    Figure 12-26\. The same fully convolutional network processing a small image (left)
    and a large one (right)    ## You Only Look Once    YOLO is a fast and accurate
    object detection architecture proposed by Joseph Redmon et al. in a [2015 paper](https://homl.info/yolo).⁠^([35](ch12.html#id2972))
    It is so fast that it can run in real time on a video, as seen in Redmon’s [demo](https://homl.info/yolodemo2).
    YOLO’s architecture is quite similar to the one we just discussed, but with a
    few important differences:    *   For each grid cell, YOLO only considers objects
    whose bounding box center lies within that cell. The bounding box coordinates
    are relative to that cell, where (0, 0) means the top-left corner of the cell
    and (1, 1) means the bottom right. However, the bounding box’s height and width
    may extend well beyond the cell.              *   It outputs two bounding boxes
    for each grid cell (instead of just one), which allows the model to handle cases
    where two objects are so close to each other that their bounding box centers lie
    within the same cell. Each bounding box also comes with its own objectness score.           *   YOLO
    also outputs a class probability distribution for each grid cell, predicting 20
    class probabilities per grid cell since YOLO was trained on the PASCAL VOC dataset,
    which contains 20 classes. This produces a coarse *class probability map*. Note
    that the model predicts one class probability distribution per grid cell, not
    per bounding box. However, it’s possible to estimate class probabilities for each
    bounding box during post-processing by measuring how well each bounding box matches
    each class in the class probability map. For example, imagine a picture of a person
    standing in front of a car. There will be two bounding boxes: one large horizontal
    one for the car, and a smaller vertical one for the person. These bounding boxes
    may have their centers within the same grid cell. So how can we tell which class
    should be assigned to each bounding box? Well, the class probability map will
    contain a large region where the “car” class is dominant, and inside it there
    will be a smaller region where the “person” class is dominant. Hopefully, the
    car’s bounding box will roughly match the “car” region, while the person’s bounding
    box will roughly match the “person” region: this will allow the correct class
    to be assigned to each bounding box.              YOLO was originally developed
    using Darknet, an open source deep learning framework initially developed in C
    by Joseph Redmon, but it was soon ported to PyTorch and other libraries. It has
    been continuously improved over the years, initially by Joseph Redmon et al. (YOLOv2,
    YOLOv3, and YOLO9000), then by various other teams since 2020\. Each version brought
    some impressive improvements in speed and accuracy, using a variety of techniques;
    for example, YOLOv3 boosted accuracy in part thanks to *anchor priors*, exploiting
    the fact that some bounding box shapes are more likely than others, depending
    on the class (e.g., people tend to have vertical bounding boxes, while cars usually
    don’t). They also increased the number of bounding boxes per grid cell, they trained
    on different datasets with many more classes (up to 9,000 classes organized in
    a hierarchy in the case of YOLO9000), they added skip connections to recover some
    of the spatial resolution that is lost in the CNN (we will discuss this shortly
    when we look at semantic segmentation), and much more. There are many variants
    of these models too, such as scaled down “tiny” YOLOs, optimized to be trained
    on less powerful machines and which can run extremely fast (at over 1,000 frames
    per second!), but with a slightly lower *mean average precision* (mAP).    TorchVision
    does not include any YOLO model, but you can use the Ultralytics library, which
    provides a simple API to download and use various pretrained YOLO models, based
    on PyTorch. These models were pretrained on the COCO dataset which contains over
    330,000 images, including 200,000 images annotated for object detection with 80
    different classes (person, car, truck, bicycle, ball, etc.). The Ultralytics library
    is not installed on Colab by default, so we must run `%pip install ultralytics`.
    Then we can download a YOLO model and use it. For example, here is how to use
    this library to download the YOLOv9 model (medium variant) and detect objects
    in a batch of images (the model accepts PIL images, NumPy arrays, and even URLs):    [PRE61]    The
    output is a list of `Results` objects which offers a handy `summary()` method.
    For example, here is how we can see the first detected object in the first image:    [PRE62]   ``######
    Tip    The Ultralytics library also provides a simple API to train a YOLO model
    on other common object detection datasets, or on your own dataset. See [*https://docs.ultralytics.com/modes/train*](https://docs.ultralytics.com/modes/train)
    for more details.    Several other pretrained object detection models are available
    via TorchVision. You can use them just like the pretrained classification models
    (e.g., ConvNeXt), except that each image prediction is a represented as a dictionary
    containing two entries: `"labels"` (i.e., class IDs) and `"boxes"`. The available
    models are listed here (see the [models page](https://pytorch.org/vision/main/models)
    for the full list of variants available):    [Faster R-CNN](https://homl.info/fasterrcnn)⁠^([36](ch12.html#id2982))      This
    model has two stages: the image first goes through a CNN, then the output is passed
    to a *region proposal network* (RPN) that proposes bounding boxes that are most
    likely to contain an object; a classifier is then run for each bounding box, based
    on the cropped output of the CNN.      [SSD](https://homl.info/ssd)⁠^([37](ch12.html#id2987))      SSD
    is a single-stage detector (“look once”) similar to YOLO.      [SSDlite](https://homl.info/ssdlite)⁠^([38](ch12.html#id2989))      A
    lightweight version of SSD, well suited for mobile devices.      [RetinaNet](https://homl.info/retinanet)⁠^([39](ch12.html#id2991))      A
    single-stage detector which introduced a variant of the cross-entropy loss called
    the *focal loss* (see `torchvision.ops.sigmoid_focal_loss()`). This loss gives
    more weight to difficult samples and thereby improves performance on small objects
    and less frequent classes.      [FCOS](https://homl.info/fcos)⁠^([40](ch12.html#id2996))      A
    single-stage fully convolutional net which directly predicts bounding boxes without
    relying on anchor boxes.      So far, we’ve only considered detecting objects
    in single images. But what about videos? Objects must not only be detected in
    each frame, they must also be tracked over time. Let’s take a quick look at object
    tracking now.``  [PRE63] my_video = "https://homl.info/cars.mp4" results = model.track(source=my_video,
    stream=True, save=True) for frame_results in results:     summary = frame_results.summary()  #
    similar summary as earlier + track id     track_ids = [obj["track_id"] for obj
    in summary]     print("Track ids:", track_ids) [PRE64]` [PRE65][PRE66][PRE67][PRE68][PRE69]
    [PRE70]`py` [PRE71] [PRE72][PRE73]'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2][PRE3]``py[PRE4]`py` 让我们也使用TorchVision的`CenterCrop`类来对图像进行中心裁剪：[PRE5]py`
    `>>>` `cropped_images` `=` `T``.``CenterCrop``((``70``,` `120``))(``sample_images_permuted``)`
    [PRE6]py [PRE7]``py[PRE8]`` [PRE9]`` 现在让我们创建一个二维卷积层，并将这些裁剪后的图像输入其中，看看会得到什么结果。为此，PyTorch提供了`nn.Conv2d`层。在底层，这个层依赖于`torch.nn.((("torch",
    "F.conv2d()")))functional.conv2d()`函数。让我们创建一个具有32个滤波器、每个滤波器大小为7 × 7（使用`kernel_size=7`，相当于使用`kernel_size=(7
    , 7)`）的卷积层，并将其应用于我们的小批量两张图像：[PRE10]    ###### 注意    当我们谈论二维卷积层时，“2D”指的是空间维度（高度和宽度），但正如你所见，这个层接受4D输入：正如我们所见，另外两个维度是批量大小（第一个维度）和通道（第二个维度）。    现在让我们看看输出的形状：[PRE11]   [PRE12]`
    输出形状与输入形状相似，但有两大区别。首先，有32个通道而不是3个。这是因为我们设置了`out_channels=32`，所以我们得到了32个输出特征图：而不是每个位置的红色、绿色和蓝色的强度，我们现在在每个位置都有每个特征的强度。其次，高度和宽度都缩小了6个像素。这是因为`nn.Conv2d`层默认不使用任何零填充，这意味着我们在输出特征图的边缘会丢失一些像素，具体取决于滤波器的大小。在这种情况下，由于滤波器大小为7，我们在水平方向上丢失了6个像素，在垂直方向上也丢失了6个像素（即每边3个像素）。    ######
    警告    默认情况下，`padding`超参数设置为0，这意味着填充被关闭。奇怪的是，这也被称为*有效填充*，因为每个神经元的感受野严格位于输入内的*有效*位置（它不会越界）。实际上，你可以设置`padding="valid"`，这相当于`padding=0`。这不是PyTorch的命名怪癖：每个人都使用这种令人困惑的命名法。    如果我们改为设置`padding="same"`，那么输入将在所有边上填充足够的零，以确保输出特征图最终具有与输入相同的*大小*（因此得名此选项）：[PRE13]``
    `...` [PRE14] `>>>` `fmaps``.``shape` `` `torch.Size([2, 32, 70, 120])` `` [PRE15]`
    [PRE16]   [PRE17] [PRE18]`py [PRE19]py`` [PRE20]py[PRE21][PRE22][PRE23][PRE24]py[PRE25]py
    [PRE26] [PRE27][PRE28]``py[PRE29][PRE30] >>> class_names = weights.meta["categories"]
    `>>>` `[``class_names``[``class_id``]` `for` `class_id` `in` `y_pred``]` `` `[''palace'',
    ''daisy'']` `` [PRE31][PRE32][PRE33] >>> y_top3_logits, y_top3_class_ids = y_logits.topk(k=3,
    dim=1) `>>>` `[[``class_names``[``class_id``]` `for` `class_id` `in` `top3``]`
    `for` `top3` `in` `y_top3_class_ids``]` `` `[[''palace'', ''monastery'', ''lakeside''],
    [''daisy'', ''pot'', ''ant'']]` `` [PRE34]`` [PRE35] >>> y_top3_logits.softmax(dim=1)
    `tensor([[0.8618, 0.1185, 0.0197],`  `[0.8106, 0.0964, 0.0930]], device=''cuda:0'')`
    [PRE36]` [PRE37][PRE38][PRE39][PRE40][PRE41]  [PRE42] [PRE43]`` # 预训练模型用于迁移学习    如果你想要构建一个图像分类器，但你没有足够的数据从头开始训练它，那么重用预训练模型的底层通常是一个好主意，正如我们在[第11章](ch11.html#deep_chapter)中讨论的那样。在本节中，我们将重用我们之前加载的ConvNeXt模型——它'
