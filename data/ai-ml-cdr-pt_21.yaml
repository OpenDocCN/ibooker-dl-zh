- en: Chapter 20\. Tuning Generative Image Models with LoRA and Diffusers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 20 章\. 使用 LoRA 和 Diffusers 调优生成图像模型
- en: In [Chapter 19](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373),
    you explored the idea of diffusers and how models trained with diffusion techniques
    can generate images based on prompts. Like text-based models (as we explored in
    [Chapter 16](ch16.html#ch16_using_llms_with_custom_data_1748550037719939)), text-to-image
    models can be fine-tuned for specific tasks. The architecture of diffusion models
    and how to fine-tune them is enough for a full book in its own right, so in this
    chapter, you’ll just explore these concepts at a high level. There are several
    techniques for doing this, including *DreamBooth, textual inversion,* and the
    more recent *low-ranking adaptation* (LoRA), which you’ll go through step-by step
    in this chapter. This last technique allows you to customize models for a specific
    subject or style with very little data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 19 章](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373)中，你探讨了
    diffusers 的概念以及使用扩散技术训练的模型如何根据提示生成图像。就像我们在[第 16 章](ch16.html#ch16_using_llms_with_custom_data_1748550037719939)中探讨的基于文本的模型一样，文本到图像模型也可以针对特定任务进行微调。扩散模型的架构以及如何微调它们足以写成一整本书，所以在本章中，你将只对这些概念进行高层次探讨。为此，有几种技术，包括
    *DreamBooth, 文本反转* 和较新的 *低秩适应* (LoRA)，你将在本章中逐步了解这些技术。最后一种技术允许你使用非常少的数据来定制特定主题或风格的模型。
- en: As with transformers, the diffusers Hugging Face library is designed to make
    using diffusers, as well as fine-tuning them, as easy as possible. To that end,
    it includes pre-built scripts that you can use.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与 transformers 一样，Hugging Face 的 diffusers 库旨在尽可能简化 diffusers 的使用以及微调。为此，它包括你可以使用的预构建脚本。
- en: We’ll go through a full sample of creating a dataset of a fictitious digital
    influencer called Misato, using LoRA and diffusers to fine-tune a text-to-image
    model called Stable Diffusion 2 for her. Then, we’ll perform text-to-image inference
    to demonstrate how to create new images of Misato (see [Figure 20-1](#ch20_figure_1_1748550104889464)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过创建一个虚构的数字影响者 Misato 的数据集的完整示例来开始，使用 LoRA 和 diffusers 微调用于她的文本到图像模型 Stable
    Diffusion 2。然后，我们将执行文本到图像推理以展示如何创建 Misato 的新图像（见[图 20-1](#ch20_figure_1_1748550104889464)）。
- en: '![](assets/aiml_2001.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2001.png)'
- en: Figure 20-1\. LoRA-tuned Stable Diffusion 2 images
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-1\. LoRA 调优的 Stable Diffusion 2 图像
- en: Training a LoRA with Diffusers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Diffusers 训练 LoRA
- en: To train a LoRA with diffusers, you’ll need to perform the following steps.
    First, you’ll need to get the source code for diffusers so you can have access
    to its premade training scripts. Then, you’ll get or create a dataset that you
    can use to fine-tune Stable Diffusion. After that, you’ll run the training scripts
    to get a fine-tune for the model, publish the fine-tune to Hugging Face, and run
    inference against the base model with the LoRA layers applied. Once you’re done,
    you should be able to create images like those shown in [Figure 20-1](#ch20_figure_1_1748550104889464).
    Let’s walk through each of these steps.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Diffusers 训练 LoRA，你需要执行以下步骤。首先，你需要获取 diffusers 的源代码，以便你可以访问其预制的训练脚本。然后，你需要获取或创建一个数据集，你可以用它来微调
    Stable Diffusion。之后，你将运行训练脚本以获取模型的微调，将微调发布到 Hugging Face，并使用 LoRA 层对基础模型进行推理。一旦完成，你应该能够创建像[图
    20-1](#ch20_figure_1_1748550104889464)中展示的图像。让我们逐一了解这些步骤。
- en: Getting Diffusers
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Diffusers
- en: To get started with LoRA, I have found the best thing to do is to first clone
    the source code for diffusers to get the training scripts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 LoRA，我发现最好的做法是首先克隆 diffusers 的源代码以获取训练脚本。
- en: 'You can do this quite simply by git-cloning it, changing into the directory,
    and running `pip install` at the current location:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 git 克隆它，切换到目录，并在当前位置运行 `pip install` 来非常简单地完成这项操作：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you’re using Colab or another hosted notebook, you’ll use syntax like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Colab 或其他托管笔记本，你将使用如下语法：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give you a local version of diffusers that you can use. The text-to-image
    LoRA fine-tuning scripts are in the */diffusers/examples/text_to_image* directory,
    and you’ll need to install their dependencies like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供一个本地版本的 diffusers，你可以使用。文本到图像的 LoRA 微调脚本位于 */diffusers/examples/text_to_image*
    目录中，你需要像这样安装它们的依赖项：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These dependencies include the specific versions of tools like accelerate, transformers,
    and torchvision. It’s good to git-clone from source so that you get the latest
    versions of the *requirements.txt* to make your life easier!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖关系包括加速、transformers和torchvision等工具的特定版本。从源代码git-clone是一个好主意，这样你可以获得最新的*requirements.txt*版本，使你的生活更加轻松！
- en: 'Finally, you’ll also need the xformers library, which is designed to make transformers
    more efficient and thus speed up the process for you. You can get it like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还需要xformers库，它旨在使transformers更高效，从而加快你的处理过程。你可以这样获取它：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, you have a diffusers environment that you can use for fine-tuning. In the
    next step, you’ll get the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有一个可以用于微调的diffusers环境。在下一步中，你将获取数据。
- en: Getting Data for Fine-Tuning a LoRA
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取用于微调LoRA的数据
- en: The two main ways in which you’ll fine-tune a LoRA are for *style* and for *subject.*
    In the former case, you can get a number of images of the specific style that
    you want and train the model so that it will output in that style. I would urge
    caution when doing this because many artists earn their livelihood from their
    style of creation, and you should respect that. Similarly, you should consider
    the impact of training models based on commercial styles. Unfortunately, many
    of the tutorials I have seen online ignore this, and such practices bring down
    the overall impact of AI and drive the narrative of generative AI away from being
    *creative* and toward *stealing IP*. So, please be careful with that.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你将微调LoRA的两种主要方式是用于*风格*和用于*主题*。在前者的情况下，你可以获取你想要的特定风格的许多图像，并训练模型以便它将以该风格输出。我强烈建议你在做这件事时要小心，因为许多艺术家通过他们的创作风格谋生，你应该尊重这一点。同样，你也应该考虑基于商业风格训练模型的影响。不幸的是，我看到的许多在线教程都忽略了这一点，这种做法降低了AI的整体影响，并将生成AI的叙事从*创造性*推向了*窃取知识产权*。所以，请小心行事。
- en: Similarly, when it comes to the subject, I see many tutorials that use examples
    of doing a Google Image search for a celebrity so you can create a LoRA of them.
    Again, I would urge you *not* to do this. Please only create a LoRA for someone
    whose likeness you have permission to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当涉及到主题时，我看到许多教程使用谷歌图片搜索名人示例来创建他们的LoRA。再次，我强烈建议你*不要*这样做。请只为那些你拥有使用其肖像权的人创建LoRA。
- en: So that you can have something you *can* use, I created a dataset for a digital
    influencer. I call her Misato, after my favorite character in a popular anime.
    All of the images were rendered by me using the popular Daz 3D rendering software.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你有东西可以使用，我创建了一个针对数字影响者的数据集。我称她为Misato，这个名字来源于我喜欢的流行动漫中的角色。所有图像都是我使用流行的Daz
    3D渲染软件渲染的。
- en: You can find this dataset on the [Hugging Facewebsite](https://oreil.ly/Y1qeY).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Hugging Face网站](https://oreil.ly/Y1qeY)上找到这个数据集。
- en: 'If you want to create a dataset like this, I would recommend that you use images
    of the same figure from multiple angles that also focus on specific segments.
    For example, you can use these:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要创建这样的数据集，我建议你使用从多个角度拍摄的同一个人像，并专注于特定部分。例如，你可以使用以下这些：
- en: 3–4 portrait headshots (passport-style photos)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张肖像照（护照风格照片）
- en: 3–4 three-quarters headshots from each side
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每侧3-4张四分之三头部照
- en: 3–4 profile pictures, showing the side of the face
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张侧面照，显示脸部侧面
- en: 3–4 full-length body shots
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张全身照
- en: For each of these images, you also need a prompt that describes the image. You’ll
    use this in training to give context to the image and how it should be represented.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些图像中的每一个，你还需要一个描述图像的提示。你将在训练中使用这个提示来为图像提供上下文以及它应该如何被表示。
- en: So, for example, consider [Figure 20-2](#ch20_figure_2_1748550104889522), which
    is a portrait shot that I generated for Misato.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑[图20-2](#ch20_figure_2_1748550104889522)，这是我为Misato生成的一张肖像照。
- en: '![](assets/aiml_2002.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2002.png)'
- en: Figure 20-2\. Portrait shot of Misato from the dataset
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-2\. 数据集中的Misato肖像照
- en: 'This image is paired with the following prompt: “Photo of (lora-misato-token),
    high-quality portrait, clear facial features, neutral expression, front view,
    natural lighting.”'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片与以下提示配对：“(lora-misato-token)的高质量肖像，清晰的面部特征，中性表情，正面视角，自然光照。”
- en: Note the use of (lora-misato-token), where we indicate the subject of the image.
    Later, when we create prompts to generate new images, we can use the same token—for
    example, “(lora-misato-token) in food ad, billboard sign, 90s, anime, Japanese
    pop, Japanese words, front view, plain background.” This prompt will give us what
    you can see in [Figure 20-3](#ch20_figure_3_1748550104889557). We have an entirely
    new composition, with Misato as the model in a fast-food campaign!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 (lora-misato-token) 的使用，我们在这里指明了图像的主题。稍后，当我们创建生成新图像的提示时，我们可以使用相同的令牌——例如，“(lora-misato-token)
    in food ad, billboard sign, 90s, anime, Japanese pop, Japanese words, front view,
    plain background。” 这个提示将给我们展示 [图 20-3](#ch20_figure_3_1748550104889557) 中的内容。我们有一个全新的组合，Misato
    作为快餐广告中的模特！
- en: Once you have a set of images, you’ll need to create a *metadata.jsonl* file
    that contains the images associated with their prompts in a standard format that
    you can use when fine-tuning. It’s JSON with a link to the filename and the prompt
    for that image. The one for Misato is on the [Hugging Face website](https://oreil.ly/MfmGh).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一组图像，你需要创建一个 *metadata.jsonl* 文件，该文件以标准格式包含与图像相关的提示，这样你就可以在微调时使用。这是一个包含文件名链接和图像提示的
    JSON 文件。Misato 的文件位于 [Hugging Face 网站](https://oreil.ly/MfmGh)。
- en: '![](assets/aiml_2003.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2003.png)'
- en: Figure 20-3\. Inference from a LoRA token
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-3\. 从 LoRA 令牌进行推理
- en: 'A snippet of the *metadata.jsonl* file is here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*metadata.jsonl* 文件的一个片段如下：'
- en: '[PRE4]` `{` `"file_name"``:` `"rightprofile-neutral.png"``,`                 `"prompt"``:`
    `"photo of (lora-misato-token),` [PRE5] [PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4]` `{` `"file_name"``:` `"rightprofile-neutral.png"``,`                 `"prompt"``:`
    `"photo of (lora-misato-token),` [PRE5] [PRE6]'
- en: '[PRE7]  [PRE8][PRE9]``py[PRE10]`` ## Fine-Tuning a Model with Diffusers    As
    mentioned earlier, when you clone the diffusers repo, you get access to a number
    of example pre-written scripts that give you a head start in various tasks. One
    of these is training text-to-image LoRAs. But before running the script, it’s
    a good idea to use `accelerate`, which abstracts underlying accelerator hardware,
    including distribution across multiple chips. With `accelerate`, you can define
    a configuration. Find the details on the [Hugging Face website](https://oreil.ly/TnaII).    For
    the purposes of simplicity, when you’re using Colab, here’s how you can set up
    a basic `accelerate` profile:    [PRE11]    Then, once you have that, you can
    use `accelerate launch` to run the training script. Here’s an example:    [PRE12]    Note
    that running this is very computationally intensive. With the preceding set of
    hyperparameters (I’ll explain each one in a moment), using an A100 in Google Colab
    took me about 2 hours (or 17 compute units) to train. Compute units cost money
    (at the time of publication, about 10 cents each), so be sure to understand how
    this all works and that it does cost money!    The script takes the following
    hyperparameters:    Pretrained_model_name_or_path      This can be a local folder
    (for example, */content/model/*) or the location on *huggingface.co*—so for example,
    [*http://huggingface.co/stabilityai/stable-diffusion-2*](http://huggingface.co/stabilityai/stable-diffusion-2)
    is the location of the model called Stable Diffusion 2\. You can also specify
    this without the *huggingface.co* part of the URL.      Dataset-name      Similarly,
    this can be a local directory containing the dataset or the address of it on *huggingface.co*.
    As you can see, I’m using the Misato dataset here.      Caption_column      This
    is the column in the *jsonl* file that contains the caption for the images. You
    can specify the caption here.      Resolution      This is the resolution that
    we’ll train the images for. In this case, it’s 512 × 512.      Random_Flip      This
    is image augmentation (as in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)).
    As the Misato dataset already has multiple angles covered, this probably isn’t
    needed.      Train_batch_size      This is the number of images per batch. It’s
    good to start with 1 and then tweak it as you see fit. When I was using the A100
    GPU in Colab, I noticed that training was only using about 7 GB of the 40 GB,
    so this could be safely turned up to speed up training.      Num_training_epochs      This
    is how many epochs to train for.      Checkpointing_steps      This is how often
    you should save a checkpoint.      Learning_rate      This is the LR hyperparameter.      LR_scheduler      If
    you want to use an adjustable learning rate, you can specify the scheduler here.
    The nice thing with an adjustable LR is that the best LR later in the training
    cycle isn’t always the same as the best one from earlier in the cycle, so you
    can adjust it on the fly.      LR_Warmup_steps      This is the number of steps
    you’ll take to set the initial LR.      Seed      This is a random seed.      Output_dir      This
    is where you save the checkpoints as training happens.      Then, when training,
    you’ll see a status that looks something like this:    [PRE13]    Once the model
    is trained, in its directory folder, you’ll see a structure like the one depicted
    in [Figure 20-5](#ch20_figure_5_1748550104889622).  ![](assets/aiml_2005.png)  ######
    Figure 20-5\. A trained directory    The original `model.safetensors` model is
    highlighted, and you can see that it is 3.47 GB in size! The fine-tuned LoRA,
    on the other hand, is much smaller at just 3.4 MB.    You can use this in the
    next step, where you upload the model to the Hugging Face repository to make it
    very easy for inference to use it.    ## Publishing Your Model    The fine-tuned
    directory that you’ve saved while training contains a lot more information than
    you need, including clones of the base model. As a result, if you try to publish
    and upload the model, you’ll end up taking a lot longer because you’ll have to
    upload lots of unneeded gigabytes!    Therefore, you should edit your directory
    structure to remove the *model.safetensors* files from the checkpoint directories
    and keep the rest.    Then, when you’re signed into Hugging Face, you can visit
    [*huggingface.co/new*](http://huggingface.co/new) to see the “Create New Model
    Repository” page (see [Figure 20-6](#ch20_figure_6_1748550104889653)).  ![](assets/aiml_2006.png)  ######
    Figure 20-6\. Creating a new repository    Follow the steps, and be sure to select
    a license. Then, when you’re done, you can upload the files via the web interface
    in the next step. When you’re done with that, you should see something like the
    screen depicted in [Figure 20-7](#ch20_figure_7_1748550104889686), where I named
    the model “finetuned-misato-sd2,” given that the data was “misato” and the model
    I tuned was Stable Diffusion 2.    You can see this for yourself on the [Hugging
    Face website](https://oreil.ly/zmlal).  ![](assets/aiml_2007.png)  ###### Figure
    20-7\. The fine-tuned Misato LoRA for Stable Diffusion 2    Now that the dataset
    and the model are both published on Hugging Face, using diffusers to do an inference
    with it is super simple. We’ll see that in the next step.    ## Generating an
    Image with the Custom LoRA    To create an image using the custom LoRA, we’ll
    go through a process that’s similar to the one in [Chapter 19](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373).
    You’ll use diffusers to create a pipeline, but you’ll also add a scheduler. In
    stable diffusion, the role of the scheduler determines how the image evolves from
    random noise to the final image. Not all schedulers work with LoRA, and you’ll
    have to ensure that the scheduler you use works with the base model you’re working
    with.    There are lots of schedulers you can use, and you can find them on the
    [Hugging Face website](https://oreil.ly/SUlZl).    In this case, you can experiment
    with using the `EulerAncestralDiscreteScheduler`:    [PRE14]    Then, specify
    our `model_id` and pick the appropriate version of the scheduler for it:    [PRE15]    Once
    you’ve done that, you can create the pipeline from the `StableDiffusionPipeline`
    class and load it to the accelerator device:    [PRE16]    The next step is to
    assign the new LoRA weights, which are the retrained layers that determine the
    new behavior of the model:    [PRE17]    Stable diffusion supports both a prompt
    *and* a negative prompt, where the first prompt defines what you want in the image
    and the second prompt defines what you *do not* want. Here’s an example:    [PRE18][PRE19][PRE20][PRE21]``       `fingers``:``1.4``),`
    `"` [PRE22]       `amputation``"` `` `)` `` [PRE23]` [PRE24][PRE25][PRE26][PRE27]   [PRE28]
    [PRE29]`py [PRE30]py[PRE31][PRE32] `` `# Summary    In this chapter, you had a
    walk-through of how to fine tune a text-to-image model like stable diffusion by
    using LoRA and the diffusers library. This technique allows you to customize models
    for a specific subject or style with a small custom file. In this case, you saw
    how to tune Stable Diffusion 2 for a synthetic character. In this chapter, you
    also went through all the steps—from cloning diffusers to creating a training
    environment for them that included a fully custom dataset. You learned how to
    use the training scripts to create a new LoRA based on the synthetic character
    and how to publish that to Hugging Face. Finally, you saw how to apply the LoRA
    to the model at inference time to create novel images using the LoRA for the Misato
    character!` `` ```'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]  [PRE8][PRE9]``py[PRE10]`` ## 使用扩散器微调模型    如前所述，当你克隆 diffusers 仓库时，你将获得访问一些示例预写脚本的权限，这些脚本可以帮助你在各种任务中取得先机。其中之一是训练文本到图像
    LoRAs。但在运行脚本之前，使用 `accelerate` 是一个好主意，它抽象化了底层加速硬件，包括跨多个芯片的分布。使用 `accelerate`，你可以定义一个配置。有关详细信息，请参阅
    [Hugging Face 网站](https://oreil.ly/TnaII)。    为了简化，当你使用 Colab 时，以下是设置基本 `accelerate`
    配置的方法：    [PRE11]    然后，一旦你有了这个配置，你就可以使用 `accelerate launch` 来运行训练脚本。以下是一个示例：    [PRE12]    注意，运行这个脚本非常计算密集。使用前面的一组超参数（我将在下面解释每个参数），在
    Google Colab 中使用 A100 GPU 进行训练大约需要 2 小时（或 17 个计算单元）。计算单元是付费的（在发布时，每个大约 10 美分），所以请确保你了解这一切，并且确实需要付费！    脚本采用以下超参数：    Pretrained_model_name_or_path      这可以是本地文件夹（例如，*/content/model/*）或
    *huggingface.co* 上的位置——例如，[*http://huggingface.co/stabilityai/stable-diffusion-2*](http://huggingface.co/stabilityai/stable-diffusion-2)
    是名为 Stable Diffusion 2 的模型的存储位置。你也可以不包含 *huggingface.co* 部分的 URL 来指定它。      Dataset-name      同样，这可以是包含数据集的本地目录或
    *huggingface.co* 上的地址。正如你所看到的，我在这里使用的是 Misato 数据集。      Caption_column      这是
    *jsonl* 文件中包含图像标题的列。你可以在这里指定标题。      Resolution      这是我们将为图像训练的分辨率。在这种情况下，它是
    512 × 512。      Random_Flip      这是图像增强（如 [第 3 章](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)
    中所述）。由于 Misato 数据集已经涵盖了多个角度，这可能不是必需的。      Train_batch_size      这是每个批次的图像数量。最好从
    1 开始，然后根据需要调整。当我使用 Colab 中的 A100 GPU 时，我注意到训练只使用了大约 7 GB 的 40 GB，所以这可以安全地提高以加快训练速度。      Num_training_epochs      这是训练的轮数。      Checkpointing_steps      这是保存检查点的频率。      Learning_rate      这是学习率超参数。      LR_scheduler      如果你想要使用可调整的学习率，你可以在这里指定调度器。可调整
    LR 的好处是，训练周期后期最好的 LR 不一定与周期早期最好的 LR 相同，因此你可以动态调整它。      LR_Warmup_steps      这是设置初始
    LR 的步数。      Seed      这是一个随机种子。      Output_dir      这是训练过程中保存检查点的位置。      然后，在训练过程中，你会看到类似以下的状态：    [PRE13]    一旦模型训练完成，在其目录文件夹中，你会看到类似
    [图 20-5](#ch20_figure_5_1748550104889622) 中所示的结构。  ![](assets/aiml_2005.png)  ######
    图 20-5\. 训练后的目录    原始的 `model.safetensors` 模型被突出显示，你可以看到它的大小为 3.47 GB！另一方面，微调后的
    LoRA 只需 3.4 MB。    你可以在下一步中使用它，将模型上传到 Hugging Face 仓库，以便推理时使用它变得非常简单。    ## 发布你的模型    你在训练过程中保存的微调目录包含比你所需多得多的信息，包括基础模型的副本。因此，如果你尝试发布和上传模型，你将花费更多的时间，因为你必须上传大量的不必要的大容量数据！    因此，你应该编辑你的目录结构，从检查点目录中删除
    *model.safetensors* 文件，并保留其余部分。    然后，当你登录到 Hugging Face 时，你可以访问 [*huggingface.co/new*](http://huggingface.co/new)
    来查看“创建新模型仓库”页面（见 [图 20-6](#ch20_figure_6_1748550104889653)）。  ![](assets/aiml_2006.png)  ######
    图 20-6\. 创建新仓库    按照步骤操作，并确保选择一个许可证。然后，当你完成时，你可以在下一步通过网页界面上传文件。完成此操作后，你应该会看到类似
    [图 20-7](#ch20_figure_7_1748550104889686) 中所示的屏幕，其中我命名了模型为“finetuned-misato-sd2”，因为数据是“misato”，而我调整的模型是
    Stable Diffusion 2。    你可以在 [Hugging Face 网站](https://oreil.ly/zmlal) 上亲自查看。  ![](assets/aiml_2007.png)  ######
    图 20-7\. 微调后的 Misato LoRA for Stable Diffusion 2    现在，数据集和模型都已发布在 Hugging Face
    上，使用 diffusers 进行推理变得超级简单。我们将在下一步中看到这一点。    ## 使用自定义 LoRA 生成图像    要使用自定义 LoRA
    创建图像，我们将通过一个类似于 [第 19 章](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373)
    中的过程。你将使用 diffusers 创建一个管道，但也会添加一个调度器。在稳定扩散中，调度器的角色决定了图像如何从随机噪声演变到最终图像。并非所有调度器都与
    LoRA 兼容，你必须确保你使用的调度器与你的基础模型兼容。    你可以使用很多调度器，你可以在 [Hugging Face 网站](https://oreil.ly/SUlZl)
    上找到它们。    在这个例子中，你可以尝试使用 `EulerAncestralDiscreteScheduler`：    [PRE14]    然后，指定我们的
    `model_id` 并选择适合它的调度器版本：    [PRE15]    一旦完成这些，你可以从 `StableDiffusionPipeline` 类创建管道并将其加载到加速器设备上：    [PRE16]    下一步是分配新的
    LoRA 权重，这些权重是重新训练的层，它们决定了模型的新行为：    [PRE17]    稳定扩散支持提示 *和* 负提示，其中第一个提示定义了图像中你想要的内容，第二个提示定义了你
    *不* 想要的内容。以下是一个示例：    [PRE18][PRE19][PRE20][PRE21]``       `fingers``:``1.4``),`
    `"` [PRE22]       `amputation``"` `` `)` `` [PRE23]` [PRE24][PRE25][PRE26][PRE27]   [PRE28]
    [PRE29]`py [PRE30]py[PRE31][PRE32] `` `# 摘要    在本章中，你了解了如何通过使用 LoRA 和 diffusers
    库来微调像稳定扩散这样的文本到图像模型。这项技术允许你使用一个小型自定义文件来定制特定主题或风格的模型。在这种情况下，你看到了如何调整 Stable Diffusion
    2 以适应合成角色。在本章中，你还经历了从克隆 diffusers 到为它们创建训练环境的所有步骤，该环境包括一个完全自定义的数据集。你学习了如何使用训练脚本创建基于合成角色的新
    LoRA，以及如何将其发布到 Hugging Face。最后，你看到了如何在推理时将 LoRA 应用到模型上，以使用 Misato 角色的 LoRA 创建新颖的图像！`
    `` ``'
