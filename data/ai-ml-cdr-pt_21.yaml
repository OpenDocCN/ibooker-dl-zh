- en: Chapter 20\. Tuning Generative Image Models with LoRA and Diffusers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 20 章\. 使用 LoRA 和 Diffusers 调整生成图像模型
- en: In [Chapter 19](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373),
    you explored the idea of diffusers and how models trained with diffusion techniques
    can generate images based on prompts. Like text-based models (as we explored in
    [Chapter 16](ch16.html#ch16_using_llms_with_custom_data_1748550037719939)), text-to-image
    models can be fine-tuned for specific tasks. The architecture of diffusion models
    and how to fine-tune them is enough for a full book in its own right, so in this
    chapter, you’ll just explore these concepts at a high level. There are several
    techniques for doing this, including *DreamBooth, textual inversion,* and the
    more recent *low-ranking adaptation* (LoRA), which you’ll go through step-by step
    in this chapter. This last technique allows you to customize models for a specific
    subject or style with very little data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 19 章](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373)中，你探讨了
    diffusers 的概念以及使用扩散技术训练的模型如何根据提示生成图像。就像我们在[第 16 章](ch16.html#ch16_using_llms_with_custom_data_1748550037719939)中探讨的基于文本的模型一样，文本到图像模型也可以针对特定任务进行微调。扩散模型的结构以及如何微调它们本身就可以写成一整本书，所以在本章中，你将只对这些概念进行高层次探讨。为此，有几种技术，包括
    *DreamBooth、文本反转* 以及更近期的 *低秩适应* (LoRA)，你将在本章中逐步了解。最后一种技术允许你使用非常少的数据来定制特定主题或风格的模型。
- en: As with transformers, the diffusers Hugging Face library is designed to make
    using diffusers, as well as fine-tuning them, as easy as possible. To that end,
    it includes pre-built scripts that you can use.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与变压器类似，Hugging Face 的 diffusers 库旨在尽可能简化使用 diffusers 以及微调它们的过程。为此，它包括预构建的脚本，您可以直接使用。
- en: We’ll go through a full sample of creating a dataset of a fictitious digital
    influencer called Misato, using LoRA and diffusers to fine-tune a text-to-image
    model called Stable Diffusion 2 for her. Then, we’ll perform text-to-image inference
    to demonstrate how to create new images of Misato (see [Figure 20-1](#ch20_figure_1_1748550104889464)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 LoRA 和 diffusers 创建一个虚构数字影响者 Misato 的数据集的完整示例，并对其中的文本到图像模型 Stable Diffusion
    2 进行微调。然后，我们将执行文本到图像推理以展示如何创建 Misato 的新图像（见[图 20-1](#ch20_figure_1_1748550104889464)）。
- en: '![](assets/aiml_2001.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_2001.png)'
- en: Figure 20-1\. LoRA-tuned Stable Diffusion 2 images
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-1\. LoRA 调整后的 Stable Diffusion 2 图像
- en: Training a LoRA with Diffusers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Diffusers 训练 LoRA
- en: To train a LoRA with diffusers, you’ll need to perform the following steps.
    First, you’ll need to get the source code for diffusers so you can have access
    to its premade training scripts. Then, you’ll get or create a dataset that you
    can use to fine-tune Stable Diffusion. After that, you’ll run the training scripts
    to get a fine-tune for the model, publish the fine-tune to Hugging Face, and run
    inference against the base model with the LoRA layers applied. Once you’re done,
    you should be able to create images like those shown in [Figure 20-1](#ch20_figure_1_1748550104889464).
    Let’s walk through each of these steps.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 diffusers 训练 LoRA，你需要执行以下步骤。首先，你需要获取 diffusers 的源代码，以便访问其预制的训练脚本。然后，你需要获取或创建一个数据集，你可以用它来微调
    Stable Diffusion。之后，你将运行训练脚本以获取模型的微调版本，将微调版本发布到 Hugging Face，并使用应用了 LoRA 层的基础模型进行推理。一旦完成，你应该能够创建像[图
    20-1](#ch20_figure_1_1748550104889464)中显示的图像。让我们逐一了解这些步骤。
- en: Getting Diffusers
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Diffusers
- en: To get started with LoRA, I have found the best thing to do is to first clone
    the source code for diffusers to get the training scripts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 LoRA，我发现最好的做法是首先克隆 diffusers 的源代码，以获取训练脚本。
- en: 'You can do this quite simply by git-cloning it, changing into the directory,
    and running `pip install` at the current location:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 git 克隆它，切换到目录，并在当前位置运行 `pip install` 来非常简单地完成这项操作：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you’re using Colab or another hosted notebook, you’ll use syntax like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Colab 或其他托管笔记本，你将使用如下语法：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give you a local version of diffusers that you can use. The text-to-image
    LoRA fine-tuning scripts are in the */diffusers/examples/text_to_image* directory,
    and you’ll need to install their dependencies like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供一个本地版本的 diffusers，你可以使用。文本到图像的 LoRA 微调脚本位于 */diffusers/examples/text_to_image*
    目录中，你需要安装它们的依赖项，如下所示：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These dependencies include the specific versions of tools like accelerate, transformers,
    and torchvision. It’s good to git-clone from source so that you get the latest
    versions of the *requirements.txt* to make your life easier!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖关系包括加速、transformers和torchvision等工具的特定版本。从源代码git-clone是一个好主意，这样你可以获得最新的*requirements.txt*版本，使你的生活更加轻松！
- en: 'Finally, you’ll also need the xformers library, which is designed to make transformers
    more efficient and thus speed up the process for you. You can get it like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还需要xformers库，它旨在使transformers更高效，从而加快你的过程。你可以这样获取它：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, you have a diffusers environment that you can use for fine-tuning. In the
    next step, you’ll get the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有一个可以用于微调的diffusers环境。在下一步中，你将获取数据。
- en: Getting Data for Fine-Tuning a LoRA
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为微调LoRA获取数据
- en: The two main ways in which you’ll fine-tune a LoRA are for *style* and for *subject.*
    In the former case, you can get a number of images of the specific style that
    you want and train the model so that it will output in that style. I would urge
    caution when doing this because many artists earn their livelihood from their
    style of creation, and you should respect that. Similarly, you should consider
    the impact of training models based on commercial styles. Unfortunately, many
    of the tutorials I have seen online ignore this, and such practices bring down
    the overall impact of AI and drive the narrative of generative AI away from being
    *creative* and toward *stealing IP*. So, please be careful with that.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你将主要在两种情况下微调LoRA：*风格*和*主题*。在前一种情况下，你可以获取你想要的特定风格的图片，并训练模型使其能够以该风格输出。我强烈建议你在做这件事时要小心，因为许多艺术家靠他们的创作风格谋生，你应该尊重这一点。同样，你应该考虑基于商业风格训练模型的影响。不幸的是，我看到的许多在线教程都忽略了这一点，这种做法降低了AI的整体影响，并将生成AI的叙事从*创造性*推向了*窃取知识产权*。所以，请小心行事。
- en: Similarly, when it comes to the subject, I see many tutorials that use examples
    of doing a Google Image search for a celebrity so you can create a LoRA of them.
    Again, I would urge you *not* to do this. Please only create a LoRA for someone
    whose likeness you have permission to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当涉及到主题时，我看到很多教程使用名人谷歌图片搜索的例子来创建LoRA。再次，我强烈建议你*不要*这样做。请只为那些你拥有使用其肖像权的个人创建LoRA。
- en: So that you can have something you *can* use, I created a dataset for a digital
    influencer. I call her Misato, after my favorite character in a popular anime.
    All of the images were rendered by me using the popular Daz 3D rendering software.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你有东西可以使用，我创建了一个针对数字影响者的数据集。我称她为Misato，这个名字来源于我喜欢的流行动漫中的角色。所有图片都是我使用流行的Daz
    3D渲染软件渲染的。
- en: You can find this dataset on the [Hugging Facewebsite](https://oreil.ly/Y1qeY).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Hugging Face网站](https://oreil.ly/Y1qeY)上找到这个数据集。
- en: 'If you want to create a dataset like this, I would recommend that you use images
    of the same figure from multiple angles that also focus on specific segments.
    For example, you can use these:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建这样一个数据集，我建议你使用从多个角度拍摄的同一个人像图片，同时也要关注特定的部分。例如，你可以使用以下这些：
- en: 3–4 portrait headshots (passport-style photos)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张肖像头像（护照风格照片）
- en: 3–4 three-quarters headshots from each side
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一边的3-4张四分之三头像
- en: 3–4 profile pictures, showing the side of the face
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张侧面照，展示脸部侧面
- en: 3–4 full-length body shots
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-4张全身照
- en: For each of these images, you also need a prompt that describes the image. You’ll
    use this in training to give context to the image and how it should be represented.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些图片中的每一张，你还需要一个描述图片的提示词。你将在训练中使用这个提示词为图片提供上下文，并说明它应该如何被表示。
- en: So, for example, consider [Figure 20-2](#ch20_figure_2_1748550104889522), which
    is a portrait shot that I generated for Misato.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑[图20-2](#ch20_figure_2_1748550104889522)，这是我为Misato生成的肖像照。
- en: '![](assets/aiml_2002.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2002.png)'
- en: Figure 20-2\. Portrait shot of Misato from the dataset
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-2. 数据集中的Misato肖像照
- en: 'This image is paired with the following prompt: “Photo of (lora-misato-token),
    high-quality portrait, clear facial features, neutral expression, front view,
    natural lighting.”'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片配以下提示词：“(lora-misato-token)的高质量肖像，清晰的五官特征，中性表情，正面视角，自然光照。”
- en: Note the use of (lora-misato-token), where we indicate the subject of the image.
    Later, when we create prompts to generate new images, we can use the same token—for
    example, “(lora-misato-token) in food ad, billboard sign, 90s, anime, Japanese
    pop, Japanese words, front view, plain background.” This prompt will give us what
    you can see in [Figure 20-3](#ch20_figure_3_1748550104889557). We have an entirely
    new composition, with Misato as the model in a fast-food campaign!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意(lora-misato-token)的使用，其中我们指出了图像的主题。稍后，当我们创建生成新图像的提示时，我们可以使用相同的标记——例如，“(lora-misato-token)在食品广告，广告牌标志，90年代，动漫，日本流行，日本单词，正面视角，纯背景。”这个提示将给我们[图20-3](#ch20_figure_3_1748550104889557)中可以看到的内容。我们有一个全新的组合，Misato作为快餐广告中的模特！
- en: Once you have a set of images, you’ll need to create a *metadata.jsonl* file
    that contains the images associated with their prompts in a standard format that
    you can use when fine-tuning. It’s JSON with a link to the filename and the prompt
    for that image. The one for Misato is on the [Hugging Face website](https://oreil.ly/MfmGh).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一组图像，你需要创建一个*metadata.jsonl*文件，该文件包含与图像相关的提示，并采用一个标准格式，这样你就可以在微调时使用。这是一个包含文件名链接和图像提示的JSON文件。Misato的文件在[Hugging
    Face网站上](https://oreil.ly/MfmGh)。
- en: '![](assets/aiml_2003.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_2003.png)'
- en: Figure 20-3\. Inference from a LoRA token
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-3. 从LoRA标记进行推理
- en: 'A snippet of the *metadata.jsonl* file is here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*metadata.jsonl*文件的一个片段如下：'
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s pretty much all you need. For training with diffusers, I’ve found it
    much easier if you publish your dataset on Hugging Face. To do this, when logged
    in, visit the [Hugging Face website](https://oreil.ly/Ez3Gp). There, you’ll be
    able to specify the name of the new dataset and whether or not it’s public. Once
    you’ve done this, you’ll be able to upload the files through the web interface
    (see [Figure 20-4](#ch20_figure_4_1748550104889591)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是你需要的一切。对于使用diffusers进行训练，我发现如果你在Hugging Face上发布你的数据集会容易得多。为此，当你登录后，访问[Hugging
    Face网站](https://oreil.ly/Ez3Gp)。在那里，你可以指定新数据集的名称以及它是否公开。完成这些后，你将能够通过网页界面上传文件（见[图20-4](#ch20_figure_4_1748550104889591)）。
- en: Once you’ve done this, your dataset will be available at [*https://huggingface.co/datasets/*](https://huggingface.co/datasets/)*<yourname>/<datasetname>*.
    So, for example, my username (see [Figure 20-4](#ch20_figure_4_1748550104889591))
    is “lmoroney,” and the dataset name is “misato,” so you can see this dataset at
    [*https://huggingface.co/datasets/lmoroney/misato*](https://huggingface.co/datasets/lmoroney/misato).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你的数据集将在[*https://huggingface.co/datasets/*](https://huggingface.co/datasets/)*<你的用户名>/<数据集名称>*处可用。例如，我的用户名（见[图20-4](#ch20_figure_4_1748550104889591)）是“lmoroney”，数据集名称是“misato”，因此你可以在这个[*https://huggingface.co/datasets/lmoroney/misato*](https://huggingface.co/datasets/lmoroney/misato)看到这个数据集。
- en: '![](assets/aiml_2004.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_2004.png)'
- en: Figure 20-4\. Creating a new dataset on Hugging Face
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-4. 在Hugging Face上创建新的数据集
- en: Fine-Tuning a Model with Diffusers
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Diffusers微调模型
- en: As mentioned earlier, when you clone the diffusers repo, you get access to a
    number of example pre-written scripts that give you a head start in various tasks.
    One of these is training text-to-image LoRAs. But before running the script, it’s
    a good idea to use `accelerate`, which abstracts underlying accelerator hardware,
    including distribution across multiple chips. With `accelerate`, you can define
    a configuration. Find the details on the [Hugging Face website](https://oreil.ly/TnaII).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当你克隆diffusers仓库时，你可以访问一些示例预写的脚本，这些脚本可以帮助你在各种任务上取得先机。其中之一是训练文本到图像LoRAs。但在运行脚本之前，使用`accelerate`是一个好主意，它抽象了底层加速硬件，包括跨多个芯片的分布。使用`accelerate`，你可以定义一个配置。有关详细信息，请参阅[Hugging
    Face网站](https://oreil.ly/TnaII)。
- en: 'For the purposes of simplicity, when you’re using Colab, here’s how you can
    set up a basic `accelerate` profile:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，当你使用Colab时，以下是设置基本`accelerate`配置的方法：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, once you have that, you can use `accelerate launch` to run the training
    script. Here’s an example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦你有了这些，你可以使用`accelerate launch`来运行训练脚本。以下是一个示例：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that running this is very computationally intensive. With the preceding
    set of hyperparameters (I’ll explain each one in a moment), using an A100 in Google
    Colab took me about 2 hours (or 17 compute units) to train. Compute units cost
    money (at the time of publication, about 10 cents each), so be sure to understand
    how this all works and that it does cost money!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，运行这个过程非常计算密集。使用前面的一组超参数（我稍后会解释每一个），在Google Colab中使用A100大约花费了我2小时（或17个计算单元）来训练。计算单元是需要付费的（在出版时，每个大约10美分），所以请确保你理解这一切是如何工作的，并且确实需要付费！
- en: 'The script takes the following hyperparameters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本使用以下超参数：
- en: Pretrained_model_name_or_path
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Pretrained_model_name_or_path
- en: This can be a local folder (for example, */content/model/*) or the location
    on *huggingface.co*—so for example, [*http://huggingface.co/stabilityai/stable-diffusion-2*](http://huggingface.co/stabilityai/stable-diffusion-2)
    is the location of the model called Stable Diffusion 2\. You can also specify
    this without the *huggingface.co* part of the URL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是本地文件夹（例如，*/content/model/*）或 *huggingface.co* 上的位置——例如，[*http://huggingface.co/stabilityai/stable-diffusion-2*](http://huggingface.co/stabilityai/stable-diffusion-2)
    是名为 Stable Diffusion 2 的模型的存储位置。您也可以不包含 *huggingface.co* 部分的 URL 来指定此位置。
- en: Dataset-name
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset-name
- en: Similarly, this can be a local directory containing the dataset or the address
    of it on *huggingface.co*. As you can see, I’m using the Misato dataset here.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，这可以是包含数据集的本地目录或 *huggingface.co* 上的地址。如您所见，我在这里使用 Misato 数据集。
- en: Caption_column
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Caption_column
- en: This is the column in the *jsonl* file that contains the caption for the images.
    You can specify the caption here.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 *jsonl* 文件中包含图像标题的列。您可以在此处指定标题。
- en: Resolution
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Resolution
- en: This is the resolution that we’ll train the images for. In this case, it’s 512
    × 512.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将为图像训练的分辨率。在这种情况下，它是 512 × 512。
- en: Random_Flip
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Random_Flip
- en: This is image augmentation (as in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)).
    As the Misato dataset already has multiple angles covered, this probably isn’t
    needed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像增强（如 [第 3 章](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)
    中所述）。由于 Misato 数据集已经涵盖了多个角度，这可能不是必需的。
- en: Train_batch_size
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Train_batch_size
- en: This is the number of images per batch. It’s good to start with 1 and then tweak
    it as you see fit. When I was using the A100 GPU in Colab, I noticed that training
    was only using about 7 GB of the 40 GB, so this could be safely turned up to speed
    up training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个批次中图像的数量。最好从 1 开始，然后根据您的需要进行调整。当我使用 Colab 中的 A100 GPU 时，我注意到训练只使用了大约 7 GB
    的 40 GB，因此可以安全地将其调高以加快训练速度。
- en: Num_training_epochs
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Num_training_epochs
- en: This is how many epochs to train for.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练的轮数。
- en: Checkpointing_steps
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Checkpointing_steps
- en: This is how often you should save a checkpoint.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您应该保存检查点的频率。
- en: Learning_rate
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Learning_rate
- en: This is the LR hyperparameter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 LR 超参数。
- en: LR_scheduler
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LR_scheduler
- en: If you want to use an adjustable learning rate, you can specify the scheduler
    here. The nice thing with an adjustable LR is that the best LR later in the training
    cycle isn’t always the same as the best one from earlier in the cycle, so you
    can adjust it on the fly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用可调整的学习率，您可以在此处指定调度器。可调整 LR 的好处是，训练周期后期最佳 LR 不一定与周期早期最佳 LR 相同，因此您可以在运行时进行调整。
- en: LR_Warmup_steps
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LR_Warmup_steps
- en: This is the number of steps you’ll take to set the initial LR.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您将用于设置初始 LR 的步骤数。
- en: Seed
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Seed
- en: This is a random seed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个随机种子。
- en: Output_dir
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Output_dir
- en: This is where you save the checkpoints as training happens.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在训练过程中保存检查点的位置。
- en: 'Then, when training, you’ll see a status that looks something like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练时，您将看到类似以下的状态：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once the model is trained, in its directory folder, you’ll see a structure like
    the one depicted in [Figure 20-5](#ch20_figure_5_1748550104889622).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，在其目录文件夹中，您将看到类似于 [图 20-5](#ch20_figure_5_1748550104889622) 中描述的结构。
- en: '![](assets/aiml_2005.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2005.png)'
- en: Figure 20-5\. A trained directory
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-5\. 训练后的目录
- en: The original `model.safetensors` model is highlighted, and you can see that
    it is 3.47 GB in size! The fine-tuned LoRA, on the other hand, is much smaller
    at just 3.4 MB.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 `model.safetensors` 模型被突出显示，您可以看到它的大小为 3.47 GB！另一方面，微调后的 LoRA 仅 3.4 MB。
- en: You can use this in the next step, where you upload the model to the Hugging
    Face repository to make it very easy for inference to use it.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在下一步中使用此参数，将模型上传到 Hugging Face 仓库，以便推理时使用更加方便。
- en: Publishing Your Model
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布您的模型
- en: The fine-tuned directory that you’ve saved while training contains a lot more
    information than you need, including clones of the base model. As a result, if
    you try to publish and upload the model, you’ll end up taking a lot longer because
    you’ll have to upload lots of unneeded gigabytes!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您在训练过程中保存的微调目录包含比您所需多得多的信息，包括基础模型的副本。因此，如果您尝试发布和上传模型，您将花费更多时间，因为您将不得不上传大量不必要的吉字节！
- en: Therefore, you should edit your directory structure to remove the *model.safetensors*
    files from the checkpoint directories and keep the rest.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你应该编辑你的目录结构，从检查点目录中删除 *model.safetensors* 文件，并保留其余部分。
- en: Then, when you’re signed into Hugging Face, you can visit [*huggingface.co/new*](http://huggingface.co/new)
    to see the “Create New Model Repository” page (see [Figure 20-6](#ch20_figure_6_1748550104889653)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当你登录到 Hugging Face 后，你可以访问[*huggingface.co/new*](http://huggingface.co/new)来查看“创建新模型仓库”页面（见[图
    20-6](#ch20_figure_6_1748550104889653)）。
- en: '![](assets/aiml_2006.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_2006.png](assets/aiml_2006.png)'
- en: Figure 20-6\. Creating a new repository
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-6\. 创建新仓库
- en: Follow the steps, and be sure to select a license. Then, when you’re done, you
    can upload the files via the web interface in the next step. When you’re done
    with that, you should see something like the screen depicted in [Figure 20-7](#ch20_figure_7_1748550104889686),
    where I named the model “finetuned-misato-sd2,” given that the data was “misato”
    and the model I tuned was Stable Diffusion 2.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 按照步骤操作，并确保选择一个许可证。完成之后，你可以在下一步通过网页界面上传文件。完成这一步后，你应该会看到类似[图 20-7](#ch20_figure_7_1748550104889686)所示的屏幕，其中我给模型命名为“finetuned-misato-sd2”，因为数据是“misato”，而我调整的模型是稳定扩散
    2。
- en: You can see this for yourself on the [Hugging Face website](https://oreil.ly/zmlal).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Hugging Face 网站上](https://oreil.ly/zmlal)亲自查看。
- en: '![](assets/aiml_2007.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_2007.png](assets/aiml_2007.png)'
- en: Figure 20-7\. The fine-tuned Misato LoRA for Stable Diffusion 2
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-7\. 为稳定扩散 2 定制的 Misato LoRA
- en: Now that the dataset and the model are both published on Hugging Face, using
    diffusers to do an inference with it is super simple. We’ll see that in the next
    step.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据集和模型都已发布在 Hugging Face 上，使用 diffusers 进行推理非常简单。我们将在下一步中看到这一点。
- en: Generating an Image with the Custom LoRA
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自定义 LoRA 生成图像
- en: To create an image using the custom LoRA, we’ll go through a process that’s
    similar to the one in [Chapter 19](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373).
    You’ll use diffusers to create a pipeline, but you’ll also add a scheduler. In
    stable diffusion, the role of the scheduler determines how the image evolves from
    random noise to the final image. Not all schedulers work with LoRA, and you’ll
    have to ensure that the scheduler you use works with the base model you’re working
    with.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用自定义 LoRA 创建图像，我们将通过一个类似于[第 19 章](ch19.html#ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373)中的过程。你将使用
    diffusers 创建管道，但还会添加一个调度器。在稳定扩散中，调度器的角色决定了图像如何从随机噪声演变到最终图像。并非所有调度器都与 LoRA 兼容，你必须确保你使用的调度器与你的基础模型兼容。
- en: There are lots of schedulers you can use, and you can find them on the [Hugging
    Face website](https://oreil.ly/SUlZl).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多调度器可供使用，你可以在[Hugging Face 网站上](https://oreil.ly/SUlZl)找到它们。
- en: 'In this case, you can experiment with using the `EulerAncestralDiscreteScheduler`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以尝试使用`EulerAncestralDiscreteScheduler`：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, specify our `model_id` and pick the appropriate version of the scheduler
    for it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，指定我们的`model_id`并选择适合它的调度器版本：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once you’ve done that, you can create the pipeline from the `StableDiffusionPipeline`
    class and load it to the accelerator device:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你可以从`StableDiffusionPipeline`类创建管道并将其加载到加速设备：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to assign the new LoRA weights, which are the retrained layers
    that determine the new behavior of the model:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是分配新的 LoRA 权重，这些是重新训练的层，决定了模型的新行为：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Stable diffusion supports both a prompt *and* a negative prompt, where the
    first prompt defines what you want in the image and the second prompt defines
    what you *do not* want. Here’s an example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散支持**提示**和**负面提示**，其中第一个提示定义了图像中你想要的内容，第二个提示定义了你**不想要**的内容。以下是一个示例：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The negative prompt is very useful in helping you avoid some of the issues with
    AI-generated visuals, such as deformed hands and faces.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 负面提示非常有用，可以帮助你避免一些与 AI 生成的视觉问题，例如变形的手和脸。
- en: 'Next up is to define the hyperparameters, such as the number of inference steps,
    the size of the image, and the seed. There’s also a parameter called *guidance
    scale*, which controls how imaginative your model is. A guidance scale value of
    less than 5 gives the model more creative freedom, but the model may not follow
    your prompt closely. A guidance scale value that’s higher than 7 will make the
    model adhere more strongly to your prompt, but it can also lead to strange artifacts.
    The guidance scale value in the middle—6—is a nice balance between freedom and
    adherence. There’s no hard and fast rule, so feel free to experiment:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是定义超参数，例如推理步骤的数量、图像的大小和种子。还有一个叫做*指导尺度*的参数，它控制着你的模型有多具想象力。指导尺度值小于5会给模型更多的创造性自由，但模型可能不会严格遵循你的提示。指导尺度值高于7会使模型更紧密地遵循你的提示，但也可能导致奇怪的伪影。中间的指导尺度值6在自由和遵循之间提供了一个很好的平衡。没有固定的规则，所以请随意实验：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, you just generate the image as usual:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你只需像往常一样生成图像：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As an experiment, you can try using a different scheduler with the same hyperparameters
    to yield similar results (see [Figure 20-8](#ch20_figure_8_1748550104889721)):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实验，你可以尝试使用具有相同超参数的不同调度器来产生类似的结果（见[图20-8](#ch20_figure_8_1748550104889721)）：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](assets/aiml_2008.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2008.png)'
- en: Figure 20-8\. The same prompt and hyperparameters with different schedulers
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-8\. 使用不同调度器的相同提示和超参数
- en: Note that the text in the image is entirely made up, but given that the prompt
    is about advertisements, the tone is similar. In the picture on the left, the
    characters represent “loneliness” and “no,” while in the image on the right, they
    suggest “husband split?”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图片中的文字完全是虚构的，但鉴于提示是关于广告的，语气相似。在左边的图片中，人物代表“孤独”和“不”，而在右边的图片中，它们暗示着“丈夫分居？”
- en: What’s most interesting is the consistency in the character! For example, consider
    [Figure 20-9](#ch20_figure_9_1748550104889755), in which Misato was painted in
    the styles of Monet and Picasso. We can see that the features learned by LoRA
    were consistent enough to (mostly) survive the restyling process.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的是角色的连贯性！例如，考虑[图20-9](#ch20_figure_9_1748550104889755)，其中Misato被画成了莫奈和毕加索的风格。我们可以看到，LoRA学习到的特征足够一致，足以（大部分）在重新设计过程中幸存下来。
- en: '![](assets/aiml_2009.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_2009.png)'
- en: Figure 20-9\. Character consistency across styles
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-9\. 不同风格下的角色一致性
- en: This example used Stable Diffusion 2, which is an older model but one that’s
    easy to tune with LoRA. As you use more advanced models and tune them, you can
    get much better results, but the time and costs of tuning will be much higher.
    I’d recommend starting with a simpler model like this one and working on your
    craft. From there, you can build up to the more advanced models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用了Stable Diffusion 2，这是一个较老的模型，但使用LoRA很容易调整。随着你使用更先进的模型并对其进行调整，你可以得到更好的结果，但调整的时间和成本会更高。我建议从这样一个简单的模型开始，并专注于你的技艺。从那里，你可以逐步过渡到更先进的模型。
- en: Additionally, Misato’s synthetic nature has triggered different features in
    the LoRA retraining, leading to the new images that have been created from her
    having a low-res, highly synthetic look. While the images have been close to photoreal
    to the human eye, they clearly haven’t been to the model, which learned a LoRA
    that was very CGI in nature and lower resolution than the ones in the training
    set!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Misato的合成特性在LoRA重新训练中触发了不同的特征，导致从她低分辨率、高度合成的外观中创建出了新的图像。虽然这些图像在人类眼中几乎接近照片真实，但它们显然没有达到模型的要求，该模型学习到的LoRA本质上是CGI风格，并且分辨率低于训练集中的图像！
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you had a walk-through of how to fine tune a text-to-image
    model like stable diffusion by using LoRA and the diffusers library. This technique
    allows you to customize models for a specific subject or style with a small custom
    file. In this case, you saw how to tune Stable Diffusion 2 for a synthetic character.
    In this chapter, you also went through all the steps—from cloning diffusers to
    creating a training environment for them that included a fully custom dataset.
    You learned how to use the training scripts to create a new LoRA based on the
    synthetic character and how to publish that to Hugging Face. Finally, you saw
    how to apply the LoRA to the model at inference time to create novel images using
    the LoRA for the Misato character!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了如何通过使用LoRA和diffusers库来微调像stable diffusion这样的文本到图像模型。这项技术允许您通过一个小型自定义文件来定制特定主题或风格的模型。在这种情况下，您看到了如何调整Stable
    Diffusion 2以适应合成角色。在本章中，您还经历了所有步骤——从克隆diffusers到为它们创建一个包含完整自定义数据集的训练环境。您学习了如何使用训练脚本创建基于合成角色的新LoRA，以及如何将其发布到Hugging
    Face。最后，您看到了如何在推理时将LoRA应用于模型，以使用Misato角色的LoRA创建新颖的图像！
