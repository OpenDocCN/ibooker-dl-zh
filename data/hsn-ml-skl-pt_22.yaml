- en: Appendix A. Autodiff
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A. 自动微分
- en: This appendix explains how PyTorch’s automatic differentiation (autodiff) feature
    works, and how it compares to other solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录解释了 PyTorch 的自动微分（autodiff）功能是如何工作的，以及它与其他解决方案的比较。
- en: Suppose you define a function *f*(*x*, *y*) = *x*²*y* + *y* + 2, and you need
    its partial derivatives ∂*f*/∂*x* and ∂*f*/∂*y*, typically to perform gradient
    descent (or some other optimization algorithm). Your main options are manual differentiation,
    finite difference approximation, forward-mode autodiff, and reverse-mode autodiff.
    PyTorch implements reverse-mode autodiff, but to fully understand it, it’s useful
    to look at the other options first. So let’s go through each of them, starting
    with manual differentiation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你定义了一个函数 *f*(*x*, *y*) = *x*²*y* + *y* + 2，你需要它的偏导数 ∂*f*/∂*x* 和 ∂*f*/∂*y*，通常用于执行梯度下降（或某些其他优化算法）。你的主要选项是手动微分、有限差分近似、前向自动微分和反向自动微分。PyTorch
    实现了反向自动微分，但要完全理解它，先查看其他选项是有用的。所以，让我们逐一介绍它们，从手动微分开始。
- en: Manual Differentiation
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动微分
- en: 'The first approach to compute derivatives is to pick up a pencil and a piece
    of paper and use your calculus knowledge to derive the appropriate equation. For
    the function *f*(*x*, *y*) just defined, it is not too hard; you just need to
    use five rules:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算导数的第一个方法就是拿起一支铅笔和一张纸，运用你的微积分知识来推导出适当的方程。对于刚刚定义的函数 *f*(*x*, *y*)，这并不太难；你只需要使用五个规则：
- en: The derivative of a constant is 0.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常数的导数是 0。
- en: The derivative of *λx* is *λ* (where *λ* is a constant).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λx* 的导数是 *λ*（其中 *λ* 是一个常数）。'
- en: The derivative of *x*^λ is *λx*^(*λ*) ^– ¹, so the derivative of *x*² is 2*x*.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*^λ 的导数是 *λx*^(*λ*) ^– ¹，所以 *x*² 的导数是 2*x*。'
- en: The derivative of a sum of functions is the sum of these functions’ derivatives.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数和的导数是这些函数导数的和。
- en: The derivative of *λ* times a function is *λ* times its derivative.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λ* 乘以一个函数的导数是 *λ* 乘以其导数。'
- en: From these rules, you can derive [Equation A-1](#partial_derivatives_equations).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些规则中，你可以推导出 [方程 A-1](#partial_derivatives_equations)。
- en: Equation A-1\. Partial derivatives of *f*(*x*, *y*)
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 A-1\. *f*(*x*, *y*) 的偏导数
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <mi>y</mi> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mn>0</mn> <mo>+</mo> <mn>0</mn> <mo>=</mo> <mn>2</mn> <mi>x</mi> <mi>y</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn> <mo>+</mo>
    <mn>0</mn> <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn></mrow></mtd></mtr></mtable>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <mi>y</mi> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mn>0</mn> <mo>+</mo> <mn>0</mn> <mo>=</mo> <mn>2</mn> <mi>x</mi> <mi>y</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn> <mo>+</mo>
    <mn>0</mn> <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn></mrow></mtd></mtr></mtable>
- en: This approach can become very tedious for more complex functions, and you run
    the risk of making mistakes. Fortunately, there are other options. Let’s look
    at finite difference approximation now.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的函数，这种方法可能会变得非常繁琐，并且你可能会犯错误。幸运的是，还有其他选择。现在让我们看看有限差分近似。
- en: Finite Difference Approximation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有限差分近似
- en: Recall that the derivative *h*′(*x*[0]) of a function *h*(*x*) at a point *x*[0]
    is the slope of the function at that point. More precisely, the derivative is
    defined as the limit of the slope of a straight line going through this point
    *x*[0] and another point *x* on the function, as *x* gets infinitely close to
    *x*[0] (see [Equation A-2](#derivative_definition)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，函数 *h*(*x*) 在点 *x*[0] 处的导数 *h*′(*x*[0]) 是该点处的函数斜率。更精确地说，导数定义为通过该点 *x*[0]
    和函数上的另一点 *x* 的直线的斜率的极限，当 *x* 无限接近 *x*[0] 时（见 [方程 A-2](#derivative_definition)）。
- en: Equation A-2\. Definition of the derivative of a function *h*(*x*) at point
    *x*[0]
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 A-2\. 函数 *h*(*x*) 在点 *x*[0] 处的导数定义
- en: <mrow><msup><mi>h</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></mrow> <mrow><mo>=</mo> <munder><mo movablelimits="true" form="prefix">lim</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>→</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mstyle></munder> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mfrac></mstyle></mrow> <mrow><mo>=</mo> <munder><mo
    movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>ε</mi><mo>→</mo><mn>0</mn></mrow></mstyle></munder>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mi>ε</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mi>ε</mi></mfrac></mstyle></mrow>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msup><mi>h</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></mrow> <mrow><mo>=</mo> <munder><mo movablelimits="true" form="prefix">lim</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>→</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mstyle></munder> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mfrac></mstyle></mrow> <mrow><mo>=</mo> <munder><mo
    movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>ε</mi><mo>→</mo><mn>0</mn></mrow></mstyle></munder>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mi>ε</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mi>ε</mi></mfrac></mstyle></mrow>
- en: 'So if we wanted to calculate the partial derivative of *f*(*x*, *y*) with regard
    to *x* at *x* = 3 and *y* = 4, we could compute *f*(3 + *ε*, 4) – *f*(3, 4) and
    divide the result by *ε*, using a very small value for *ε*. This type of numerical
    approximation of the derivative is called a *finite difference approximation*,
    and this specific equation is called *Newton’s difference quotient*. That’s exactly
    what the following code does:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想计算函数 *f*(*x*, *y*) 关于 *x* 在 *x* = 3 和 *y* = 4 处的偏导数，我们可以计算 *f*(3 + *ε*,
    4) – *f*(3, 4) 并将结果除以 *ε*，使用一个非常小的 *ε* 值。这种导数的数值近似称为有限差分近似，这个特定的方程称为牛顿差商。这正是以下代码所做的事情：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unfortunately, the result is imprecise (and it gets worse for more complicated
    functions). The correct results are respectively 24 and 10, but instead we get:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，结果是近似的（对于更复杂的函数，结果会更差）。正确的结果分别是24和10，但相反，我们得到：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that to compute both partial derivatives, we have to call `f()` at least
    three times (we called it four times in the preceding code, but it could be optimized).
    If there were 1,000 parameters, we would need to call `f()` at least 1,001 times.
    When you are dealing with large neural networks, this makes finite difference
    approximation way too inefficient.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了计算两个偏导数，我们至少需要调用 `f()` 三次（在前面的代码中我们调用了四次，但可以进行优化）。如果有1,000个参数，我们至少需要调用
    `f()` 1,001次。当您处理大型神经网络时，这使得有限差分近似变得非常低效。
- en: However, this method is so simple to implement that it is a great tool to check
    that the other methods are implemented correctly. For example, if it disagrees
    with your manually derived function, then your function probably contains a mistake.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法实现起来非常简单，因此它是一个检查其他方法是否正确实现的绝佳工具。例如，如果它与您手动推导出的函数不一致，那么您的函数可能包含错误。
- en: 'So far, we have considered two ways to compute gradients: using manual differentiation
    and using finite difference approximation. Unfortunately, both are fatally flawed
    for training a large-scale neural network. So let’s turn to autodiff, starting
    with forward mode.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了两种计算梯度的方法：使用手动微分和使用有限差分近似。不幸的是，这两种方法对于训练大规模神经网络都是致命的缺陷。因此，让我们转向自动微分，从前向模式开始。
- en: Forward-Mode Autodiff
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向自动微分
- en: '[Figure A-1](#symbolic_differentiation_diagram) shows how forward-mode autodiff
    works on an even simpler function, *g*(*x*, *y*) = 5 + *xy*. The graph for that
    function is represented on the left. After forward-mode autodiff, we get the graph
    on the right, which represents the partial derivative ∂*g*/∂*x* = 0 + (0 × *x*
    + *y* × 1) = *y* (we could similarly obtain the partial derivative with regard
    to *y*).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图A-1](#symbolic_differentiation_diagram) 展示了前向自动微分在更简单的函数 *g*(*x*, *y*) =
    5 + *xy* 上的工作原理。该函数的图像在左侧表示。经过前向自动微分后，我们得到右侧的图像，它表示偏导数 ∂*g*/∂*x* = 0 + (0 × *x*
    + *y* × 1) = *y*（我们可以类似地获得关于 *y* 的偏导数）。'
- en: The algorithm will go through the computation graph from the inputs to the outputs
    (hence the name “forward mode”). It starts by getting the partial derivatives
    of the leaf nodes. The constant node (5) returns the constant 0, since the derivative
    of a constant is always 0\. The variable *x* returns the constant 1 since ∂*x*/∂*x*
    = 1, and the variable *y* returns the constant 0 since ∂*y*/∂*x* = 0 (if we were
    looking for the partial derivative with regard to *y*, it would be the reverse).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 算法将从输入遍历计算图到输出（因此得名“前向模式”）。它首先获取叶节点的偏导数。常数节点（5）返回常数 0，因为常数的导数始终为 0。变量 *x* 返回常数
    1，因为 ∂*x*/∂*x* = 1，而变量 *y* 返回常数 0，因为 ∂*y*/∂*x* = 0（如果我们寻找关于 *y* 的偏导数，结果将是相反的）。
- en: Now we have all we need to move up the graph to the multiplication node in function
    *g*. Calculus tells us that the derivative of the product of two functions *u*
    and *v* is ∂(*u* × *v*)/∂*x* = ∂*v*/∂*x* × *u* + *v* × ∂*u*/∂*x*. We can therefore
    construct a large part of the graph on the right, representing 0 × *x* + *y* ×
    1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了向上移动到函数 *g* 中的乘法节点所需的所有信息。微积分告诉我们，两个函数 *u* 和 *v* 的乘积的导数是 ∂(*u* × *v*)/∂*x*
    = ∂*v*/∂*x* × *u* + *v* × ∂*u*/∂*x*。因此，我们可以构建右边的图形的大部分，代表 0 × *x* + *y* × 1。
- en: 'Finally, we can go up to the addition node in function *g*. As mentioned, the
    derivative of a sum of functions is the sum of these functions’ derivatives, so
    we just need to create an addition node and connect it to the parts of the graph
    we have already computed. We get the correct partial derivative: ∂*g*/∂*x* = 0
    + (0 × *x* + *y* × 1).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以移动到函数 *g* 中的加法节点。如前所述，函数和的导数是这些函数导数的和，所以我们只需要创建一个加法节点并将其连接到我们已计算的部分图。我们得到正确的偏导数：∂*g*/∂*x*
    = 0 + (0 × *x* + *y* × 1)。
- en: '![Forward-mode autodiff diagram illustrating the derivative calculation of
    the function g(x, y) = 5 + xy, highlighting the step-by-step computation and simplification
    to ∂g/∂x = y.](assets/hmls_aa01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![前向模式自动微分图，展示了函数 g(x, y) = 5 + xy 的导数计算过程，突出了逐步计算和简化至 ∂g/∂x = y 的步骤。](assets/hmls_aa01.png)'
- en: Figure A-1\. Forward-mode autodiff
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-1\. 前向模式自动微分
- en: 'However, this equation can be simplified (a lot). By applying a few pruning
    steps to the computation graph to get rid of all the unnecessary operations, we
    get a much smaller graph with just one node: ∂*g*/∂*x* = *y*. In this case simplification
    is fairly easy, but for a more complex function, forward-mode autodiff can produce
    a huge graph that may be tough to simplify and lead to suboptimal performance.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个方程可以被大大简化。通过在计算图上应用一些修剪步骤来去除所有不必要的操作，我们得到一个只有一个节点的更小的图：∂*g*/∂*x* = *y*。在这种情况下简化相对容易，但对于更复杂的函数，前向模式自动微分可能会产生一个巨大的图，这可能很难简化并导致性能不佳。
- en: Note that we started with a computation graph, and forward-mode autodiff produced
    another computation graph. This is called *symbolic differentiation*, and it has
    two nice features. First, once the computation graph of the derivative has been
    produced, we can use it as many times as we want to compute the derivatives of
    the given function for any value of *x* and *y*. Second, we can run forward-mode
    autodiff again on the resulting graph to get second-order derivatives if we ever
    need to (i.e., derivatives of derivatives). We could even compute third-order
    derivatives, and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们从一个计算图开始，前向模式自动微分产生了另一个计算图。这被称为 *符号微分*，它有两个很好的特性。首先，一旦导数的计算图被生成，我们可以多次使用它来计算给定函数关于
    *x* 和 *y* 任意值的导数。其次，如果我们需要的话，我们可以再次在前向模式自动微分的结果图上运行，以获取二阶导数（即导数的导数）。我们甚至可以计算三阶导数，以此类推。
- en: But it is also possible to run forward-mode autodiff without constructing a
    graph (i.e., numerically, not symbolically) just by computing intermediate results
    on the fly. One way to do this is to use *dual numbers*, which are weird but fascinating
    numbers of the form *a* + *bε*, where *a* and *b* are real numbers, and *ε* is
    an infinitesimal number such that *ε*² = 0 (but *ε* ≠ 0). You can think of the
    dual number 42 + 24*ε* as something akin to 42.0000⋯000024 with an infinite number
    of 0s (but of course this is simplified just to give you some idea of what dual
    numbers are). A dual number is represented in memory as a pair of floats. For
    example, 42 + 24*ε* is represented by the pair (42.0, 24.0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但也可以在不构建图（即数值上，而不是符号上）的情况下运行前向自动微分，只需在飞行中计算中间结果即可。这样做的一种方法就是使用*双数*，这是一种奇特但迷人的形式为
    *a* + *bε* 的数字，其中 *a* 和 *b* 是实数，而 *ε* 是一个无穷小数，使得 *ε*² = 0（但 *ε* ≠ 0）。你可以把双数 42
    + 24*ε* 看作类似于 42.0000⋯000024 的东西，有无限多个 0（但当然这只是为了给你一些关于双数的概念）。双数在内存中用一对浮点数表示。例如，42
    + 24*ε* 由对 (42.0, 24.0) 表示。
- en: Dual numbers can be added, multiplied, and so on, as shown in [Equation A-3](#dual_numbers_operations).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 双数可以进行加法、乘法等操作，如[方程 A-3](#dual_numbers_operations)所示。
- en: Equation A-3\. A few operations with dual numbers
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 A-3\. 双数的一些操作
- en: <mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>λ</mi> <mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mi>λ</mi> <mi>a</mi>
    <mo>+</mo> <mi>λ</mi> <mi>b</mi> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>c</mi>
    <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>a</mi> <mo>+</mo>
    <mi>c</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo>
    <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo>
    <mi>c</mi> <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi>
    <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi>
    <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>b</mi>
    <mi>d</mi> <mo>)</mo></mrow> <msup><mi>ε</mi> <mn>2</mn></msup> <mo>=</mo> <mi>a</mi>
    <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi>
    <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi></mrow></mtd></mtr></mtable>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>λ</mi> <mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mi>λ</mi> <mi>a</mi>
    <mo>+</mo> <mi>λ</mi> <mi>b</mi> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>c</mi>
    <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>a</mi> <mo>+</mo>
    <mi>c</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo>
    <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo>
    <mi>c</mi> <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi>
    <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi>
    <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>b</mi>
    <mi>d</mi> <mo>)</mo></mrow> <msup><mi>ε</mi> <mn>2</mn></msup> <mo>=</mo> <mi>a</mi>
    <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi>
    <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi></mrow></mtd></mtr></mtable>
- en: Most importantly, it can be shown that *h*(*a* + *bε*) = *h*(*a*) + *b* × *h*′(*a*)*ε*,
    so computing *h*(*a* + *ε*) gives you both *h*(*a*) and the derivative *h*′(*a*)
    in just one shot. [Figure A-2](#autodiff_forward_diagram) shows that the partial
    derivative of *f*(*x*, *y*) with regard to *x* at *x* = 3 and *y* = 4 (which I
    will write as ∂*f*/∂*x* (3, 4)) can be computed using dual numbers. All we need
    to do is compute *f*(3 + *ε*, 4); this will output a dual number whose first component
    is equal to *f*(3, 4) and whose second component is equal to ∂*f*/∂*x* (3, 4).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，可以证明 *h*(*a* + *bε*) = *h*(*a*) + *b* × *h*′(*a*)*ε*，因此计算 *h*(*a* + *ε*)
    一次就可以得到 *h*(*a*) 和导数 *h*′(*a*)。[图 A-2](#autodiff_forward_diagram) 显示，在 *x* = 3
    和 *y* = 4 处对函数 *f*(*x*, *y*) 关于 *x* 的偏导数（我将写作 ∂*f*/∂*x* (3, 4)）可以使用双数来计算。我们只需要计算
    *f*(3 + *ε*, 4)；这将输出一个双数，其第一个分量等于 *f*(3, 4)，其第二个分量等于 ∂*f*/∂*x* (3, 4)。
- en: '![Diagram illustrating forward-mode autodiff using dual numbers to compute
    the partial derivative of a function \( f(x, y) = x^2y + y + 2 \) with respect
    to \( x \) at \( x = 3 \) and \( y = 4 \), resulting in the derivative value of
    24.](assets/hmls_aa02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图示使用双数进行前向自动微分，计算函数 \( f(x, y) = x^2y + y + 2 \) 在 \( x = 3 \) 和 \( y = 4
    \) 处关于 \( x \) 的偏导数，结果为导数值 24。](assets/hmls_aa02.png)'
- en: Figure A-2\. Forward-mode autodiff using dual numbers
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-2\. 使用双数进行前向自动微分
- en: To compute ∂*f*/∂*y* (3, 4) we would have to go through the graph again, but
    this time with *x* = 3 and *y* = 4 + *ε*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 ∂*f*/∂*y* (3, 4)，我们不得不再次遍历图，但这次是带有 *x* = 3 和 *y* = 4 + *ε*。
- en: 'So, forward-mode autodiff is much more accurate than finite difference approximation,
    but it suffers from the same major flaw, at least when there are many inputs and
    few outputs (as is the case when dealing with neural networks): if there were
    1,000 parameters, it would require 1,000 passes through the graph to compute all
    the partial derivatives. This is where reverse-mode autodiff shines: it can compute
    all of them in just two passes through the graph. Let’s see how.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正向模式自动微分比有限差分近似更准确，但它也存在着相同的重大缺陷，至少当有多个输入和少量输出时（例如处理神经网络时）：如果有 1,000 个参数，就需要遍历图
    1,000 次来计算所有的偏导数。这就是反向模式自动微分发光的地方：它只需遍历图两次就能计算所有这些。让我们看看它是如何做到的。
- en: Reverse-Mode Autodiff
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向模式自动微分
- en: 'Reverse-mode autodiff is the solution implemented by PyTorch. It first goes
    through the graph in the forward direction (i.e., from the inputs to the output)
    to compute the value of each node. Then it does a second pass, this time in the
    reverse direction (i.e., from the output to the inputs) to compute all the partial
    derivatives. The name “reverse mode” comes from this second pass through the graph,
    where gradients flow in the reverse direction. [Figure A-3](#autodiff_reverse_diagram)
    represents the second pass. During the first pass, all the node values were computed,
    starting from *x* = 3 and *y* = 4\. You can see those values at the bottom right
    of each node (e.g., *x* × *x* = 9). The nodes are labeled *n*[1] to *n*[7] for
    clarity. The output node is *n*[7]: *f*(3, 4) = *n*[7] = 42.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式自动微分是 PyTorch 实现的解决方案。它首先沿着图的正向（即从输入到输出）遍历图来计算每个节点的值。然后它进行第二次遍历，这次是反向方向（即从输出到输入）来计算所有的偏导数。名称“反向模式”来自于对图的第二次遍历，其中梯度以相反的方向流动。[图
    A-3](#autodiff_reverse_diagram) 表示第二次遍历。在第一次遍历期间，所有节点值都已计算，从 *x* = 3 和 *y* = 4
    开始。您可以在每个节点的右下角看到这些值（例如，*x* × *x* = 9）。节点用 *n*[1] 到 *n*[7] 标记以供清晰识别。输出节点是 *n*[7]：*f*(3,
    4) = *n*[7] = 42。
- en: '![Diagram illustrating reverse-mode autodiff showing the computation of partial
    derivatives in the reverse pass from output node \( n_7 \) with labeled values
    and derivatives for each node.](assets/hmls_aa03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![说明反向模式自动微分的图，展示了从标记值和每个节点的导数的输出节点 \( n_7 \) 开始的反向遍历中的偏导数计算。](assets/hmls_aa03.png)'
- en: Figure A-3\. Reverse-mode autodiff
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-3\. 反向模式自动微分
- en: The idea is to gradually go down the graph, computing the partial derivative
    of *f*(*x*, *y*) with regard to each consecutive node, until we reach the variable
    nodes. For this, reverse-mode autodiff relies heavily on the *chain rule*, shown
    in [Equation A-4](#chain_rule).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是逐步向下遍历图，计算 *f*(*x*, *y*) 对每个连续节点的偏导数，直到我们达到变量节点。为此，反向模式自动微分严重依赖于链式法则，如 [方程
    A-4](#chain_rule) 所示。
- en: Equation A-4\. Chain rule
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 A-4\. 链式法则
- en: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow></mfrac></mstyle> <mo>×</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mrow>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow></mfrac></mstyle> <mo>×</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mrow>
- en: Since *n*[7] is the output node, *f* = *n*[7] so ∂*f* / ∂*n*[7] = 1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *n*[7] 是输出节点，*f* = *n*[7] 因此 ∂*f* / ∂*n*[7] = 1。
- en: 'Let’s continue down the graph to *n*[5]: how much does *f* vary when *n*[5]
    varies? The answer is ∂*f* / ∂*n*[5] = ∂*f* / ∂*n*[7] × ∂*n*[7] / ∂*n*[5]. We
    already know that ∂*f* / ∂*n*[7] = 1, so all we need is ∂*n*[7] / ∂*n*[5]. Since
    *n*[7] simply performs the sum *n*[5] + *n*[6], we find that ∂*n*[7] / ∂*n*[5]
    = 1, so ∂*f* / ∂*n*[5] = 1 × 1 = 1.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续沿着图向下到 *n*[5]：当 *n*[5] 变化时，*f* 会如何变化？答案是 ∂*f* / ∂*n*[5] = ∂*f* / ∂*n*[7]
    × ∂*n*[7] / ∂*n*[5]。我们已经知道 ∂*f* / ∂*n*[7] = 1，所以我们只需要 ∂*n*[7] / ∂*n*[5]。由于 *n*[7]
    简单地执行了求和 *n*[5] + *n*[6]，我们发现 ∂*n*[7] / ∂*n*[5] = 1，因此 ∂*f* / ∂*n*[5] = 1 × 1
    = 1。
- en: 'Now we can proceed to node *n*[4]: how much does *f* vary when *n*[4] varies?
    The answer is ∂*f* / ∂*n*[4] = ∂*f* / ∂*n*[5] × ∂*n*[5] / ∂*n*[4]. Since *n*[5]
    = *n*[4] × *n*[2], we find that ∂*n*[5] / ∂*n*[4] = *n*[2], so ∂*f* / ∂*n*[4]
    = 1 × *n*[2] = 4.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续到节点 *n*[4]：当 *n*[4] 变化时，*f* 会如何变化？答案是 ∂*f* / ∂*n*[4] = ∂*f* / ∂*n*[5]
    × ∂*n*[5] / ∂*n*[4]。由于 *n*[5] = *n*[4] × *n*[2]，我们发现 ∂*n*[5] / ∂*n*[4] = *n*[2]，所以
    ∂*f* / ∂*n*[4] = 1 × *n*[2] = 4。
- en: The process continues until we reach the bottom of the graph. At that point
    we will have calculated all the partial derivatives of *f*(*x*, *y*) at the point
    *x* = 3 and *y* = 4\. In this example, we find ∂*f* / ∂*x* = 24 and ∂*f* / ∂*y*
    = 10\. Sounds about right!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会一直持续到我们到达图的底部。在那个点上，我们将计算出在点 *x* = 3 和 *y* = 4 处的 *f*(*x*, *y*) 的所有偏导数。在这个例子中，我们找到
    ∂*f* / ∂*x* = 24 和 ∂*f* / ∂*y* = 10。听起来很合理！
- en: Reverse-mode autodiff is a very powerful and accurate technique, especially
    when there are many inputs and few outputs, since it requires only one forward
    pass plus one reverse pass per output to compute all the partial derivatives for
    all outputs with regard to all the inputs. When training neural networks, we generally
    want to minimize the loss, so there is a single output (the loss), and hence only
    two passes through the graph are needed to compute the gradients.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式自动微分是一种非常强大且精确的技术，尤其是在输入很多而输出很少的情况下，因为它只需要对每个输出进行一次前向传播和一次反向传播，就可以计算所有输出相对于所有输入的所有偏导数。在训练神经网络时，我们通常希望最小化损失，因此只有一个输出（损失），因此只需要通过图两次来计算梯度。
- en: PyTorch builds a new graph on the fly during each forward pass. Whenever you
    run an operation on a tensor with `requires_grad=True`, PyTorch computes the resulting
    tensor and sets its `grad_fn` attribute to an operation-specific object that allows
    PyTorch to propagate the gradients backwards through this operation. Since the
    graph is built on the fly, your code can be highly dynamic, containing loops and
    conditionals, and everything will still work fine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在每次前向传播过程中动态构建一个新的图。无论何时你在具有`requires_grad=True`的张量上运行操作，PyTorch都会计算结果张量并将其`grad_fn`属性设置为特定于操作的物体，这允许PyTorch通过此操作反向传播梯度。由于图是动态构建的，你的代码可以非常动态，包含循环和条件语句，而且一切都会正常工作。
- en: Reverse-mode autodiff can also handle functions that are not entirely differentiable,
    as long as you ask it to compute the partial derivatives at points that *are*
    differentiable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式自动微分也可以处理不完全可微分的函数，只要你要求它在可微分的点上计算偏导数。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Creating a tiny autodiff framework is a great exercise to truly master autodiff.
    Try creating one from scratch for a small set of operations. If you get stuck,
    check out this project’s `extra_autodiff.ipynb` notebook, which you can run on
    Colab at [*https://homl.info/colab-p*](https://homl.info/colab-p). You can also
    watch Andrej Karpathy’s excellent YouTube video where he builds the [micrograd
    library from scratch](https://homl.info/micrograd).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个微小的自动微分框架是一个很好的练习，可以真正掌握自动微分。尝试为小批操作从头开始创建一个。如果你卡住了，可以查看这个项目的`extra_autodiff.ipynb`笔记本，你可以在Colab上运行[*https://homl.info/colab-p*](https://homl.info/colab-p)。你还可以观看Andrej
    Karpathy的出色YouTube视频，他在视频中从头开始构建了[micrograd库](https://homl.info/micrograd)。
