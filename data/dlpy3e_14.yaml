- en: Text classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter14_text-classification](https://deeplearningwithpython.io/chapters/chapter14_text-classification)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter14_text-classification](https://deeplearningwithpython.io/chapters/chapter14_text-classification)
- en: This chapter will lay the foundation for working with text input that we will
    build on in the next two chapters of this book. By the end of this chapter, you
    will be able to build a simple text classifier in a number of different ways.
    This will set the stage for building more complicated models, like the Transformer,
    in the next chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为处理文本输入奠定基础，我们将在本书的下一章中继续构建。到本章结束时，你将能够以多种不同的方式构建一个简单的文本分类器。这将为本章构建更复杂的模型，如下一章中的Transformer，做好准备。
- en: A brief history of natural language processing
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理简史
- en: 'In computer science, we refer to human languages, like English or Mandarin,
    as “natural” languages to distinguish them from languages that were designed for
    machines, like LISP, Assembly, and XML. Every machine language was designed: its
    starting point was an engineer writing down a set of formal rules to describe
    what statements you can make and what they mean. The rules came first, and people
    only started using the language once the rule set was complete. With human language,
    it’s the reverse: usage comes first, and rules arise later. Natural language was
    shaped by an evolutionary process, much like biological organisms — that’s what
    makes it “natural.” Its “rules,” like the grammar of English, were formalized
    after the fact and are often ignored or broken by its users. As a result, while
    machine-readable language is highly structured and rigorous, natural language
    is messy — ambiguous, chaotic, sprawling, and constantly in flux.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，我们将人类语言，如英语或普通话，称为“自然”语言，以区别于为机器设计的语言，如LISP、汇编和XML。每种机器语言都是被设计的：它的起点是一位工程师写下一系列形式规则来描述你可以做出哪些陈述以及它们的含义。规则先出现，人们只有在规则集完成后才开始使用这种语言。对于人类语言来说，情况正好相反：使用先于规则的出现。自然语言是通过一个进化过程形成的，就像生物有机体一样——这就是它被称为“自然”的原因。它的“规则”，比如英语的语法，是在事后形式化的，并且经常被使用者忽略或违反。因此，尽管机器可读语言高度结构化和严格，自然语言却是混乱的——模糊的、混乱的、蔓延的，并且始终处于变化之中。
- en: Computer scientists have long fixated on the potential of systems that can ingest
    or produce natural language. Language, particularly written text, underpins most
    of our communications and cultural production. Centuries of human knowledge are
    stored via text; the internet is mostly text, and even our thoughts are based
    on language! The practice of using computers to interpret and manipulate language
    is called natural language processing, or NLP for short. It was first proposed
    as a field of study immediately following World War II, where some thought we
    could view understanding language as a form of “code cracking,” where natural
    language is the “code” used to transmit information.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家长期以来一直专注于能够摄入或产生自然语言的系统的潜力。语言，尤其是书面文本，支撑着我们的大多数沟通和文化生产。几个世纪的人类知识都是通过文本存储的；互联网主要是文本，甚至我们的思想也是基于语言的！使用计算机来解释和操作语言的做法被称为自然语言处理，简称NLP。它是在第二次世界大战后立即作为一个研究领域提出的，当时有人认为我们可以将理解语言视为一种“密码破解”，其中自然语言是传输信息的“密码”。
- en: In the early days of the field, many people naively thought that you could write
    down the “rule set of English,” much like one can write down the rule set of LISP.
    In the early 1950s, researchers at IBM and Georgetown demonstrated a system that
    could translate Russian into English. The system used a grammar with six hardcoded
    rules and a lookup table with a couple of hundred elements (words and suffixes)
    to translate 60 handpicked Russian sentences accurately. The goal was to drum
    up excitement and funding for machine translation, and in that sense, it was a
    huge success. Despite the limited nature of the demo, the authors claimed that
    within five years, translation would be a solved problem. Funding poured in for
    the better part of a decade. However, generalizing such a system proved to be
    maddeningly difficult. Words change their meaning dramatically depending on context.
    Any grammar rules needed countless exceptions. Developing a program that could
    shine on a few handpicked examples was simple enough, but building a robust system
    that could compete with human translators was another matter. An influential US
    report a decade later picked apart the lack of progress, and funding dried up.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在该领域的早期，许多人天真地认为可以写下“英语的规则集”，就像可以写下LISP的规则集一样。在20世纪50年代初，IBM和乔治敦的研究人员展示了一个可以将俄语翻译成英语的系统。该系统使用了一个包含六个硬编码规则的语法和一个包含几百个元素（单词和后缀）的查找表，以准确翻译60个精心挑选的俄语句子。目标是激起对机器翻译的热情和资金支持，从这个意义上说，这是一个巨大的成功。尽管演示的性质有限，但作者声称，在五年内，翻译问题将得到解决。在接下来的十年中，资金大量涌入。然而，将这样的系统推广开来证明是极其困难的。单词的意义会根据上下文发生巨大变化。任何语法规则都需要无数例外。开发一个能够在几个精心挑选的例子上表现出色的程序是足够的简单，但建立一个能够与人类翻译者竞争的健壮系统则是另一回事。十年后，一份有影响力的美国报告剖析了缺乏进展的原因，资金也随之枯竭。
- en: Despite these setbacks and repeated swings from excitement to disillusionment,
    handcrafted rules held out as the dominant approach well into the 1990s. The problems
    were obvious, but there was simply no viable alternative to writing down symbolic
    rules to describe grammar. However, as faster computers and greater quantities
    of data became available in the late 1980s, research began to head in a new direction.
    When you find yourself building systems that are big piles of ad hoc rules, as
    a clever engineer, you’re likely to start asking, “Could I use a corpus of data
    to automate the process of finding these rules? Could I search for the rules within
    some rule space, instead of having to come up with them myself?” And just like
    that, you’ve graduated to doing machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些挫折和从兴奋到幻灭的反复波动，手工编写的规则在20世纪90年代中期仍然作为主导方法存在。问题很明显，但简单地写下描述语法的符号规则似乎没有可行的替代方案。然而，随着20世纪80年代末更快计算机和更大数据量的可用性，研究开始走向新的方向。当你发现自己正在构建大量临时规则的系统时，作为一个聪明的工程师，你可能会开始问自己，“我能用数据语料库来自动化寻找这些规则的过程吗？我能否在某个规则空间内搜索规则，而不是自己想出来？”就这样，你进入了机器学习的领域。
- en: In the late 1980s, we started seeing machine learning approaches to natural
    language processing. The earliest ones were based on decision trees — the intent
    was literally to automate the development of the kind of if/then/else rules of
    hardcoded language systems. Then, statistical approaches started gaining speed,
    starting with logistic regression. Over time, learned parametric models took over,
    and linguistics came to be seen by some as a hindrance when baked directly into
    a model. Frederick Jelinek, an early speech recognition researcher, joked in the
    1990s, “Every time I fire a linguist, the performance of the speech recognizer
    goes up.”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪80年代末，我们开始看到自然语言处理中机器学习方法的兴起。最早的这些方法基于决策树——其意图实际上是要自动化开发类似于硬编码语言系统的if/then/else规则。随后，统计方法开始加速发展，从逻辑回归开始。随着时间的推移，学习到的参数模型逐渐取代了主导地位，并且有些人认为，当直接嵌入到模型中时，语言学成了一种阻碍。早期语音识别研究者弗雷德里克·杰利内克在20世纪90年代开玩笑说：“每次我解雇一个语言学家，语音识别器的性能就会提高。”
- en: 'Much as computer vision is pattern recognition applied to pixels, the modern
    field of NLP is all about pattern recognition applied to words in text. There’s
    no shortage of practical applications:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像计算机视觉是将模式识别应用于像素一样，现代自然语言处理领域完全是关于将模式识别应用于文本中的单词。实际应用并不缺乏：
- en: Given the text of an email, what is the probability that it is spam? (*text
    classification*)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一封电子邮件的文本，它被判定为垃圾邮件的概率是多少？（*文本分类*）
- en: Given an English sentence, what is the most likely Russian translation? (*translation*)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个英文句子，最可能的俄语翻译是什么？(*翻译*)
- en: Given an incomplete sentence, what word will likely come next? (*language modeling*)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个不完整的句子，下一个可能出现的单词是什么？(*语言建模*)
- en: The text-processing models you will train in this book won’t possess a human-like
    understanding of language; rather, they simply look for statistical regularities
    in their input data, which turns out to be sufficient to perform well on a wide
    array of real-world tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中你将要训练的文本处理模型不会拥有类似人类对语言的理解；相反，它们只是在输入数据中寻找统计规律，而事实证明这足以在广泛的现实世界任务中表现良好。
- en: In the last decade, NLP researchers and practitioners have discovered just how
    shockingly effective it can be to learn the answer to narrow statistical questions
    about text. In the 2010s, researchers began applying LSTM models to text, dramatically
    increasing the number of parameters in NLP models and the compute resources required
    to train them. The results were encouraging — LSTMs could generalize to unseen
    examples with far greater accuracy than previous approaches, but they eventually
    hit limits. LSTMs struggled to track dependencies in long chains of text with
    many sentences and paragraphs, and compared to computer vision models, they were
    slow and unwieldy to train.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，NLP研究人员和实践者发现了学习关于文本的狭窄统计问题的答案可以有多么惊人地有效。在2010年代，研究人员开始将LSTM模型应用于文本，大大增加了NLP模型中的参数数量以及训练它们所需的计算资源。结果是令人鼓舞的——LSTMs能够以比以前的方法更高的准确性泛化到未见过的例子，但它们最终遇到了限制。LSTMs在处理包含许多句子和段落的文本中的长链依赖关系时遇到了困难，与计算机视觉模型相比，它们的训练既慢又难以控制。
- en: Toward the end of the 2010s, researchers at Google discovered a new architecture
    called the Transformer that solved many scalability issues plaguing LSTMs. As
    long as you increased the size of a model and its training data together, Transformers
    appeared to perform more and more accurately. Better yet, the computations needed
    for training a Transformer could be effectively parallelized, even for long sequences.
    If you doubled the number of machines doing training, you could roughly halve
    the time you need to wait for a result.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到2010年代末，谷歌的研究人员发现了一种名为Transformer的新架构，该架构解决了许多困扰LSTMs的可扩展性问题。只要同时增加模型的大小和训练数据，Transformer似乎就能越来越准确地执行。更好的是，训练Transformer所需的计算可以有效地并行化，即使是对于长序列也是如此。如果你将进行训练的机器数量加倍，你大约可以将等待结果的时间减半。
- en: The discovery of the Transformer architecture, along with ever-faster GPUs and
    CPUs, has led to a dramatic explosion of investment and interest in NLP models
    over the past few years. Chat systems like ChatGPT have captivated public attention
    with their ability to produce fluent and natural text on seemingly arbitrary topics
    and questions. The raw text used to train these models is a significant portion
    of all written language available on the internet, and the compute to train individual
    models can cost tens of millions of dollars. Some hype is worth cutting down to
    size — these are pattern recognition machines. Despite our persistent human tendency
    to find intelligence in “things that talk,” these models copy and synthesize training
    data in a way that is wholly distinct (and much less efficient!) than human intelligence.
    However, it is also fair to say that the emergence of complex behaviors from incredibly
    simple “guess the missing word” training setups has been one of the most shocking
    empirical results in the last decade of machine learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的发现，以及GPU和CPU的持续加速，在过去几年中导致了NLP模型投资和兴趣的显著增长。ChatGPT等聊天系统凭借其能够在看似任意的话题和问题上产生流畅自然的文本的能力，吸引了公众的注意。用于训练这些模型的原始文本是互联网上所有可用书面语言的一个很大部分，而训练单个模型所需的计算成本可能高达数百万美元。一些炒作是值得削减的——这些是模式识别机器。尽管我们持续的人类倾向是在“会说话的事物”中寻找智能，但这些模型以与人类智能截然不同（而且效率低得多）的方式复制和综合训练数据。然而，公平地说，从极其简单的“猜测缺失单词”训练设置中产生复杂行为，是过去十年机器学习中最令人震惊的实证结果之一。
- en: In the following three chapters, we will look at a range of techniques for machine
    learning with text data. We will skip discussion of the hardcoded linguistic features
    that prevailed until the 1990s, but we will look at everything from running logistic
    regressions for classifying text to training LSTMs for machine translation. We
    will closely examine the Transformer model and discuss what makes it so scalable
    and effective at generalizing in the text domain. Let’s dig in.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个章节中，我们将探讨一系列用于文本数据的机器学习技术。我们将跳过关于直到20世纪90年代盛行的硬编码语言特征的讨论，但我们将从运行逻辑回归以对文本进行分类到训练LSTM进行机器翻译的一切进行探讨。我们将仔细检查Transformer模型，并讨论它在文本领域进行泛化时为什么如此可扩展和有效。让我们深入探讨。
- en: Preparing text data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备文本数据
- en: 'Let’s consider an English sentence:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个英文句子：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There is an obvious blocker before we can start applying any of the deep learning
    techniques of previous chapters — our input is not numeric! Before beginning any
    modeling, we need to translate the written word into tensors of numbers. Unlike
    images, which have a relatively natural numeric representation, you could build
    a numeric representation of text in several ways.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以开始应用前几章中提到的任何深度学习技术之前，有一个明显的障碍——我们的输入不是数字！在开始任何建模之前，我们需要将书面文字转换成数字张量。与具有相对自然数字表示的图像不同，你可以以几种方式构建文本的数字表示。
- en: A simple approach would be to borrow from standard text file formats for text
    and use something like an ASCII encoding. We could chop the input into a sequence
    of characters and assign each a unique index. Another intuitive approach would
    be building a representation based on words, first breaking sentences apart on
    all spaces and punctuation and then mapping each word to a unique numeric representation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是借鉴标准的文本文件格式，并使用类似ASCII编码的东西。我们可以将输入切割成字符序列，并为每个字符分配一个唯一的索引。另一种直观的方法是构建基于单词的表示，首先在所有空格和标点符号处将句子分开，然后将每个单词映射到一个唯一的数字表示。
- en: These are both good approaches to try, and in general, all text preprocessing
    will include a *splitting* step, where text is split into small individual units,
    called *tokens*. A powerful tool for splitting text is regular expressions, which
    can flexibly match patterns of characters in text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都是值得尝试的，通常，所有文本预处理都会包括一个*分割*步骤，即将文本分割成小的单个单元，称为*标记*。分割文本的一个强大工具是正则表达式，它可以灵活地匹配文本中字符的模式。
- en: 'Let’s look at how to use a regular expression to split a string into a sequence
    of characters. The most basic regex we can apply is `"."`, which matches any character
    in the input text:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用正则表达式将字符串分割成字符序列。我们可以应用的最基本的正则表达式是`"."`，它可以匹配输入文本中的任何字符：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can apply the function to our example input string:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将该函数应用于我们的示例输入字符串：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Regex can easily be applied to split our text into words instead. The `"[\w]+"`
    regular expression will grab consecutive non-whitespace characters, and the `"[.,!?;]"`
    can match the punctuation marks between the brackets. We can combine the two to
    achieve a regular expression that splits each word and punctuation mark into a
    token:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式可以很容易地应用于将我们的文本分割成单词。正则表达式`"[\w]+"`将抓取连续的非空白字符，而`"[.,!?;]"`可以匹配括号中的标点符号。我们可以将这两个结合起来，得到一个正则表达式，将每个单词和标点符号分割成标记：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here’s what it does to a test sentence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了它对一个测试句子的作用：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Splitting takes us from a single string to a token sequence, but we still need
    to transform our string tokens into numeric inputs. By far the most common approach
    is to map each token to a unique integer index, often called *indexing* our input.
    This is a flexible and reversible representation of our tokenized input that can
    work with a wide range of modeling approaches. Later on, we can decide how to
    map from token indices into a latent space ingested by the model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 分割将我们从一个单独的字符串转换成标记序列，但我们仍然需要将我们的字符串标记转换成数字输入。迄今为止最常见的方法是将每个标记映射到一个唯一的整数索引，通常称为*索引*我们的输入。这是一种灵活且可逆的表示，可以与广泛的建模方法一起工作。稍后，我们可以决定如何将标记索引映射到模型摄入的潜在空间。
- en: For character tokens, we could use ASCII lookups to index each token — for example,
    `ord('A') → 65` and `ord('z') → 122`. However, this can scale poorly when you
    start to consider other languages — there are over a million characters in the
    Unicode specification! A more robust technique is to build a mapping from specific
    tokens in our training data to indices that occur in the data we care about, which
    in NLP is called a *vocabulary*. This has the nice property of working for word-level
    tokens as easily as for character-level tokens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字符标记，我们可以使用ASCII查找来索引每个标记——例如，`ord('A') → 65` 和 `ord('z') → 122`。然而，当你开始考虑其他语言时，这可能会扩展得不好——Unicode规范中超过一百万个字符！一种更稳健的技术是从我们的训练数据中的特定标记构建到我们关心的数据的索引的映射，这在NLP中称为*词汇表*。它有一个很好的特性，即它对单词级标记和字符级标记都同样有效。
- en: 'Let’s take a look at how we might use a vocabulary to transform text. We will
    build a simple Python dictionary that maps tokens to indices, split our input
    into tokens, and finally index our tokens:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何可能使用一个词汇表来转换文本。我们将构建一个简单的Python字典，将标记映射到索引，将输入拆分为标记，并最终索引我们的标记：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This outputs the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下内容：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We introduce a special token called `"[UNK]"` to our vocabulary, which represents
    a token that is unknown to the vocabulary. This way, we can index all input we
    come across, even if some terms only occur in our test data. In the previous example
    `"lazy"` maps to the `"[UNK]"` index 0, as it was not included in our vocabulary.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向我们的词汇表中引入一个特殊标记`"[UNK]"`，它代表一个对词汇表来说是未知的标记。这样，我们可以索引我们遇到的所有输入，即使某些术语只出现在我们的测试数据中。在先前的例子中，“lazy”映射到`"[UNK]"`索引0，因为它不包括在我们的词汇表中。
- en: With these simple text transformations, we are well on our way to building a
    text preprocessing pipeline. However, there is one more common type of text manipulation
    we should consider — standardization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些简单的文本转换，我们正在稳步构建一个文本预处理管道。然而，我们还需要考虑一种常见的文本操作类型——标准化。
- en: 'Consider these two sentences:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个句子：
- en: “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日落时分。我凝视着墨西哥的天空。大自然多么壮丽??”
- en: “Sunset came; I stared at the México sky. Isn’t nature splendid?”
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Sunset came; I stared at the México sky. Isn’t nature splendid?”
- en: They are very similar — in fact, they are almost identical. Yet, if you were
    to convert them to indices as previously described, you would end up with very
    different representations because “i” and “I” are two distinct characters, “Mexico”
    and “México” are two distinct words, “isnt” isn’t “isn’t,” and so on. A machine
    learning model doesn’t know a priori that “i” and “I” are the same letter, that
    “é” is an “e” with an accent, or that “staring” and “stared” are two forms of
    the same verb. *Standardizing* text is a basic form of feature engineering that
    aims to erase encoding differences that you don’t want your model to have to deal
    with. It’s not exclusive to machine learning, either — you’d have to do the same
    thing if you were building a search engine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它们非常相似——事实上，它们几乎是相同的。然而，如果你将它们转换为之前描述的索引，你会得到非常不同的表示，因为“i”和“I”是两个不同的字符，“Mexico”和“México”是两个不同的单词，“isnt”不是“isn’t”，等等。将文本标准化是一种基本的特征工程形式，旨在消除你不想让模型处理的编码差异。这也不局限于机器学习——如果你在构建搜索引擎，你也必须做同样的事情。
- en: One simple and widespread standardization scheme is to convert to lowercase
    and remove punctuation characters. Our two sentences would become
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单且广泛使用的标准化方案是将所有内容转换为小写并删除标点符号。我们的两个句子将变成
- en: “sunset came i was staring at the mexico sky isnt nature splendid”
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日落时分，我凝视着墨西哥的天空，大自然多么壮丽”
- en: “sunset came i stared at the méxico sky isnt nature splendid”
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日落时分，我凝视着墨西哥的天空，大自然多么壮丽”
- en: Much closer already. We could get even closer if we removed accent marks on
    all characters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 已经非常接近了。如果我们从所有字符中移除重音符号，我们可以更接近。
- en: There’s a lot you can do with standardization, and it used to be one of the
    most critical areas to improve model performance. For many decades in NLP, it
    was common practice to use regular expressions to attempt to map words to a common
    root (e.g. “tired” → “tire” and “trophies” → “trophy”), called *stemming* or *lemmatization*.
    But as models have grown more expressive, this type of standardization tends to
    do more harm than good. The tense and plurality of a word are necessary signals
    to its meaning. For the larger models used today, most standardization is as light
    as possible — for example, converting all inputs to a standard character encoding
    before further processing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化有很多用途，它曾经是提高模型性能最关键的领域之一。在几十年的自然语言处理中，使用正则表达式尝试将单词映射到共同的词根（例如，“tired”→“tire”和“trophies”→“trophy”），称为*词干提取*或*词形还原*，是一种常见的做法。但随着模型的表达能力增强，这种标准化往往弊大于利。单词的时态和复数是传达其意义的重要信号。对于今天使用的较大模型，大多数标准化尽可能轻量级——例如，在进一步处理之前将所有输入转换为标准字符编码。
- en: 'With standardization, we have now seen three distinct stages for preprocessing
    text (figure 14.1):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化，我们现在已经看到了文本预处理的三个不同阶段（图14.1）：
- en: '*Standardization* — Where we normalize input with basic text-to-text transformations'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*标准化* — 我们通过基本的文本到文本转换来规范化输入'
- en: '*Splitting* — Where we split our text into sequences of *tokens*'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分割* — 我们将文本分割成标记序列'
- en: '*Indexing* — Where we map our tokens to indices using a *vocabulary*'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*索引化* — 我们使用*词汇表*将我们的标记映射到索引'
- en: '![](../Images/9f4ea8a1c24081171e0e3edc998bcbdc.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f4ea8a1c24081171e0e3edc998bcbdc.png)'
- en: '[Figure 14.1](#figure-14-1): The text preprocessing pipeline'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.1](#figure-14-1)：文本预处理流程'
- en: People often refer to the entire process as *tokenization*, and to an object
    that maps text to sequence of token indices as a *tokenizer*. Let’s try building
    a few.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常将整个过程称为*分词*，将映射文本到标记索引序列的对象称为*分词器*。让我们尝试构建几个。
- en: Character and word tokenization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符和词分词
- en: To start, let’s build a character-level tokenizer that maps each character in
    an input string to an integer. To keep things simple, we will use only one standardization
    step — we lowercase all input.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建一个字符级分词器，该分词器将输入字符串中的每个字符映射到一个整数。为了简化问题，我们将只使用一个标准化步骤——我们将所有输入转换为小写。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 14.1](#listing-14-1): A basic character-level tokenizer'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.1](#listing-14-1)：一个基本的字符级分词器'
- en: Pretty simple. Before using this, we also need to build a function that computes
    a vocabulary of tokens based on some input text. Rather than simply mapping all
    characters to a unique index, let’s give ourselves the ability to limit our vocabulary
    size to only the most common tokens in our input data. When we get into the modeling
    side of things, limiting the vocabulary size will be an important way to limit
    the number of parameters in a model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单。在使用这个之前，我们还需要构建一个函数，该函数根据一些输入文本计算标记的词汇表。而不是简单地将所有字符映射到唯一的索引，让我们给自己一个能力，将我们的词汇表大小限制在我们输入数据中最常见的标记。当我们进入建模方面的事情时，限制词汇表大小将是一个限制模型中参数数量的重要方法。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 14.2](#listing-14-2): Computing a character-level vocabulary'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.2](#listing-14-2)：计算字符级词汇表'
- en: We can now do the same for a word-level tokenizer. We can use the same code
    as our character-level tokenizer with a different splitting step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以为词级分词器做同样的事情。我们可以使用与我们的字符级分词器相同的代码，但使用不同的分割步骤。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 14.3](#listing-14-3): A basic word-level tokenizer'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.3](#listing-14-3)：一个基本的词级分词器'
- en: We can also substitute this new split rule into our vocabulary function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将这个新的分割规则替换到我们的词汇函数中。
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 14.4](#listing-14-4): Computing a word-level vocabulary'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.4](#listing-14-4)：计算词级词汇表'
- en: 'Let’s try out our tokenizers on some real-world input — the full text of *Moby
    Dick* by Herman Melville. We will first build a vocabulary for both tokenizers
    and then use it to tokenize some text:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一些真实世界的输入上尝试我们的分词器——赫尔曼·梅尔维尔的全文本《白鲸》。我们将首先为这两个分词器构建一个词汇表，然后使用它来分词一些文本：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s inspect what our character-level tokenizer has computed:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的字符级分词器计算出的结果：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now what about the word-level tokenizer?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于词级分词器呢？
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can print out the same data for our word-level tokenizer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以为我们的词级分词器打印出相同的数据：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can already see some of the strengths and weaknesses of both tokenization
    techniques. A character-level tokenizer needs only 64 vocabulary terms to cover
    the entire book but will encode each input as a very long sequence. A word-level
    tokenizer quickly fills a 2,000-term vocabulary (you would need a dictionary with
    17,000 terms to index every word in the book!), but the outputs of the word-level
    tokenizer are much shorter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到这两种标记化技术的优势和劣势。字符级标记化器只需要64个词汇项就可以覆盖整本书，但会将每个输入编码为非常长的序列。词级标记化器很快就会填满一个2,000个术语的词汇表（你需要一个包含17,000个术语的字典来索引书中的每个单词！），但词级标记化器的输出要短得多。
- en: As machine learning practitioners have scaled models up with more and more data
    and parameters, the downsides of both word and character tokenization have become
    apparent. The “compression” offered by word-level tokenization turns out to be
    very important — it allows feeding longer sequences into a model. However, if
    you attempt to build a word-level vocabulary for a large dataset (today, you might
    see a dataset with trillions of words), you would have an unworkably large vocabulary
    with hundreds of millions of terms. If you aggressively restrict your word-level
    vocabulary size, you will encode a lot of text to the `"[UNK]"` token, throwing
    out valuable information.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习从业者使用越来越多的数据和参数扩展模型，词和字符标记化的缺点变得明显。词级标记化提供的“压缩”实际上非常重要——它允许将更长的序列输入到模型中。然而，如果你尝试为大型数据集（今天，你可能看到包含万亿个单词的数据集）构建词级词汇表，你将得到一个无法工作的巨大词汇表，包含数亿个术语。如果你激进地限制你的词级词汇表大小，你将把大量文本编码为`"[UNK]"`标记，从而丢弃有价值的信息。
- en: These issues have led to the rise in popularity of a third type of tokenization,
    called *subword tokenization*, which attempts to bridge the gap between word-
    and character-level approaches.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题导致了第三种类型的标记化方法流行起来，称为*子词标记化*，它试图弥合词和字符级方法之间的差距。
- en: Subword tokenization
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子词标记化
- en: Subword tokenization aims to combine the best of both character- and word-level
    encoding techniques. We want the `WordTokenizer`’s ability to produce concise
    output and the `CharTokenizer`’s ability to encode a wide range of inputs with
    a small vocabulary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 子词标记化旨在结合字符级和词级编码技术的优点。我们希望`WordTokenizer`能够产生简洁的输出，同时希望`CharTokenizer`能够用一个小型的词汇表编码广泛的输入。
- en: We can think of the search for the ideal tokenizer as the hunt for an ideal
    compression of the input data. Reducing token length compresses the overall length
    of our examples. A small vocabulary reduces the number of bytes we would need
    to represent each token. If we achieve both, we will be able to feed short, information-rich
    sequences to our deep learning model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将寻找理想标记化器的搜索视为寻找理想输入数据压缩的狩猎。减少标记长度压缩了我们的示例的整体长度。一个小型的词汇表减少了表示每个标记所需的字节数。如果我们两者都能实现，我们就能向我们的深度学习模型提供短而信息丰富的序列。
- en: This analogy between compression and tokenization was not always obvious, but
    it turns out to be powerful. One of the most practically effective tricks found
    in the last decade of NLP research was repurposing a 1990s algorithm for lossless
    compression called *byte-pair encoding*^([[1]](#footnote-1)) for tokenization.
    It is used by ChatGPT and many other models to this day. In this section, we will
    build a tokenizer that uses the byte-pair encoding algorithm.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩和标记化之间的这种类比并不总是显而易见，但它证明是非常有力的。在过去十年自然语言处理研究中最实用的技巧之一是将用于无损压缩的1990年代算法*字节对编码*^([[1]](#footnote-1))重新用于标记化。它至今仍被ChatGPT和其他许多模型使用。在本节中，我们将构建一个使用字节对编码算法的标记化器。
- en: 'The idea with byte-pair encoding is to start with a basic vocabulary of characters
    and progressively “merge” common pairings into longer and longer sequences of
    characters. Let’s say we start with the following input text:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 字节对编码的思路是从一个基本的字符词汇表开始，逐步“合并”常见的配对到越来越长的字符序列中。假设我们从一个以下输入文本开始：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Like the `WordTokenizer`, we will start by computing word counts for all the
    words in the text. As we create our dictionary of word counts, we will split all
    our text into characters and join the characters with a space. This will make
    it easier to consider pairs of characters in our next step.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与`WordTokenizer`类似，我们首先会计算文本中所有单词的词频。在我们创建词频字典的同时，我们会将所有文本拆分为字符，并用空格连接字符。这将使我们在下一步考虑字符对变得更容易。
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 14.5](#listing-14-5): Initializing state for the byte-pair encoding
    algorithm'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.5](#listing-14-5)：初始化字节对编码算法的状态'
- en: 'Let’s try this out on our data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的数据上试一试：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To apply byte-pair encoding to our split word counts, we will find two characters
    and merge them into a new symbol. We consider all pairs of characters in all words
    and only merge the most common one we find. In the previous example, the most
    common character pair is `("o", "w")`, in both the word `"brown"` (which occurs
    three times in our data) and `"slow"` (which occurs once). We combine this pair
    into a new symbol `"ow"` and merge all occurrences of `"o w"`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要将字节对编码应用于我们的单词分割计数，我们将找到两个字符并将它们合并成一个新的符号。我们考虑所有单词中的所有字符对，并且只合并我们找到的最常见的字符对。在先前的例子中，最常见的字符对是
    `("o", "w")`，在单词 `"brown"`（在我们的数据中出现了三次）和 `"slow"`（出现了一次）中。我们将这个字符对组合成一个新的符号 `"ow"`，并将所有
    `"o w"` 的出现合并。
- en: Then we continue, counting pairs and merging pairs, except now `"ow"` will be
    a single unit that could merge with, say, `"l"` to form `"low"`. By progressively
    merging the most frequent symbol pair, we build up a vocabulary of larger and
    larger subwords.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续，计数字符对并合并字符对，但现在 `"ow"` 将是一个单独的单元，它可以与，比如说，`"l"` 合并形成 `"low"`。通过逐步合并最频繁的符号对，我们构建起一个更大和更大的子词词汇表。
- en: Let’s try this out on our toy dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的玩具数据集上试一试：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Listing 14.6](#listing-14-6): Running a few steps of byte-pair merging'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.6](#listing-14-6)：运行字节对合并的几个步骤'
- en: 'Here’s what we get:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的结果：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see how common words are merged entirely, whereas less common words are
    only partially merged.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到常见单词被完全合并，而较少见的单词则只部分合并。
- en: We can now extend this to a full function for computing a byte-pair encoding
    vocabulary. We start our vocabulary with all characters found in the input text,
    and we will progressively add merged symbols (larger and larger subwords) to our
    vocabulary until it reaches our desired length. We also keep a separate dictionary
    of our merge rules, including a rank order in which we applied them. Next, we
    will see how to use these merge rules to tokenize new input text.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将此扩展为计算字节对编码词汇表的完整函数。我们以输入文本中找到的所有字符开始我们的词汇表，并将逐步添加合并符号（更大和更大的子词）到我们的词汇表中，直到它达到我们期望的长度。我们还将保留一个包含我们应用顺序的合并规则单独字典。接下来，我们将看到如何使用这些合并规则来分词新的输入文本。
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 14.7](#listing-14-7): Computing a byte-pair encoding vocabulary'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.7](#listing-14-7)：计算字节对编码词汇表'
- en: Let’s build a `SubWordTokenizer` that applies our merge rules to tokenize new
    input text. The `standardize()` and `index()` steps can stay the same as the `WordTokenizer`,
    with all changes coming in the `split()` method.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个 `SubWordTokenizer`，它将我们的合并规则应用于对新输入文本进行分词。`standardize()` 和 `index()`
    步骤可以保持与 `WordTokenizer` 相同，所有更改都发生在 `split()` 方法中。
- en: In our splitting step, we first split all input into words, then split all words
    into characters, and finally apply our learned merge rules to the split characters.
    What is left are subwords — tokens that may be entire words, partial words, or
    simple characters, depending on the input word’s frequency in our training data.
    These subwords are tokens in our output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分割步骤中，我们首先将所有输入分割成单词，然后将所有单词分割成字符，最后将我们学到的合并规则应用于分割后的字符。剩下的就是子词——根据输入单词在我们训练数据中的频率，这些子词可能是完整的单词、部分单词或简单的字符。这些子词是我们输出中的标记。
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 14.8](#listing-14-8): A byte-pair encoding tokenizer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.8](#listing-14-8)：字节对编码分词器'
- en: 'Let’s try out our tokenizer on the full text of *Moby Dick*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 *Moby Dick* 的全文上试一试我们的分词器：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can take a look at our vocabulary and try a test sentence on our tokenizer,
    as we did with `WordTokenizer` and `CharTokenizer`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看我们的词汇表，并在我们的分词器上尝试一个测试句子，就像我们在 `WordTokenizer` 和 `CharTokenizer` 上做的那样：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `SubWordTokenizer` has a slightly longer length for our test sentence than
    the `WordTokenizer` (16 versus 13 tokens), but unlike the `WordTokenizer`, it
    can tokenize every word in *Moby Dick* without using the `"[UNK]"` token. The
    vocabulary contains every character in our source text, so the worst-case performance
    will be tokenizing a word into individual characters. We have achieved a short
    *average* token length while handling rare words with a small vocabulary. This
    is the advantage of subword tokenizers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`SubWordTokenizer` 对于我们的测试句子比 `WordTokenizer` 的长度略长（16个标记与13个标记），但与 `WordTokenizer`
    不同，它可以在不使用 `"[UNK]"` 标记的情况下对 *Moby Dick* 中的每个单词进行分词。词汇表包含我们源文本中的每个字符，所以最坏的情况是将单词分词成单个字符。我们在处理罕见单词的同时，实现了较短的
    *平均* 标记长度。这是子词分词器的优势。'
- en: You might notice that running this code is noticeably slower than the word and
    character tokenizers; it takes about a minute on our reference hardware. Learning
    merge rules is much more complex than simply counting the words in an input dataset.
    While this is a downside to subword tokenization, it is rarely an important concern
    in practice. You only need to learn a vocabulary once per model, and the cost
    of learning a subword vocabulary is generally negligible compared to model training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到运行这段代码比单词和字符分词器慢得多；在我们的参考硬件上大约需要一分钟。学习合并规则比简单地统计输入数据集中的单词要复杂得多。虽然这是子词分词的一个缺点，但在实践中很少是一个重要的问题。你只需要为每个模型学习一次词汇表，与模型训练相比，学习子词词汇表的成本通常是可以忽略不计的。
- en: We have now seen three separate approaches for tokenizing input. Now that we
    can translate from text to numeric input, we can move on to training a model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了三种不同的输入分词方法。现在我们能够将文本转换为数值输入，我们可以继续进行模型训练。
- en: One final note on tokenization — while it is quite important to understand how
    tokenizers work, it is rarely the case that you will need to build one yourself.
    Keras comes with utilities for tokenizing text input, as do most deep learning
    frameworks. For the rest of the chapter, we will make use of the built-in functionality
    in Keras for tokenization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分词的最后一点说明——虽然理解分词器的工作原理非常重要，但很少需要你自己去构建一个分词器。Keras自带了用于分词文本输入的实用工具，大多数深度学习框架也是如此。在接下来的章节中，我们将利用Keras内置的分词功能。
- en: Sets vs. sequences
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合与序列
- en: 'How a machine learning model should represent individual tokens is a relatively
    uncontroversial question: they’re categorical features (values from a predefined
    set), and we know how to handle those. They should be encoded as dimensions in
    a feature space or as category vectors (token vectors in this case). A much more
    problematic question, however, is how to encode the ordering of tokens in text.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型应该如何表示单个标记是一个相对无争议的问题：它们是分类特征（来自预定义集合的值），我们知道如何处理这些特征。它们应该被编码为特征空间中的维度或作为类别向量（在这种情况下是标记向量）。然而，一个更成问题的问题是，如何编码文本中标记的顺序。
- en: 'The problem of order in natural language is an interesting one: unlike the
    steps of a timeseries, words in a sentence don’t have a natural, canonical order.
    Different languages order similar words in very different ways. For instance,
    the sentence structure of English is quite different from that of Japanese. Even
    within a given language, you can typically say the same thing in different ways
    by reshuffling the words a bit. If you were to fully randomize the words in a
    short sentence, you could still sometimes figure out what it was saying — though,
    in many cases, significant ambiguity would arise. Order is clearly important,
    but its relationship to meaning isn’t straightforward.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言中的顺序问题很有趣：与时间序列的步骤不同，句子中的单词没有自然、规范化的顺序。不同的语言以非常不同的方式排列相似的单词。例如，英语的句子结构与日语的句子结构大不相同。即使在给定的语言中，你通常也可以通过稍微重新排列单词来用不同的方式表达相同的事情。如果你要完全随机化一个短句中的单词，有时你仍然可以弄清楚它在说什么——尽管在许多情况下，会出现显著的歧义。顺序显然很重要，但它的意义关系并不直接。
- en: 'How to represent word order is the pivotal question from which different kinds
    of NLP architectures spring. The simplest thing you could do is discard order
    and treat text as an unordered set of words — this gives you bag-of-words models.
    You could also decide that words should be processed strictly in the order in
    which they appear, one at a time, like steps in a timeseries — you could then
    use the recurrent models from the previous chapter. Finally, a hybrid approach
    is also possible: the Transformer architecture is technically order-agnostic,
    yet it injects word-position information into the representations it processes,
    which enables it to simultaneously look at different parts of a sentence (unlike
    RNNs) while still being order-aware. Because they take into account word order,
    both RNNs and Transformers are called *sequence models*.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如何表示词序是不同类型的NLP架构产生的关键问题。你可以做的最简单的事情就是忽略顺序，将文本视为一个无序的单词集合——这给你的是词袋模型。你也可以决定单词应该严格按照它们出现的顺序进行处理，一次一个，就像时间序列中的步骤一样——然后你可以使用上一章中的循环模型。最后，还可以采用混合方法：Transformer架构在技术上是无序的，但它将单词位置信息注入到它处理的表示中，这使得它能够同时查看句子的不同部分（与RNN不同），同时仍然保持对顺序的感知。由于它们考虑了词序，RNN和Transformer都被称为*序列模型*。
- en: Historically, most early applications of machine learning to NLP just involved
    bag-of-words models that discarded sequence data. Interest in sequence models
    only started rising in 2015, with the rebirth of RNNs. Today, both approaches
    remain relevant. Let’s see how they work and when to use which.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，大多数早期将机器学习应用于NLP的应用仅涉及丢弃序列数据的词袋模型。对序列模型感兴趣的情况直到2015年才开始上升，随着RNN的复兴。今天，这两种方法仍然相关。让我们看看它们是如何工作的，以及在何时使用哪种方法。
- en: 'We will demonstrate each approach on a well-known text classification benchmark:
    the IMDb movie review sentiment-classification dataset. In chapters 4 and 5, you
    worked with a prevectorized version of the IMDb dataset; now let’s process the
    raw IMDb text data, just like you would do when approaching a new text-classification
    problem in the real world.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个著名的文本分类基准数据集上展示每种方法：IMDb电影评论情感分类数据集。在第4章和第5章中，你使用的是IMDb数据集的预向量化版本；现在让我们处理原始的IMDb文本数据，就像你在现实生活中处理一个新的文本分类问题一样。
- en: Loading the IMDb classification dataset
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载IMDb分类数据集
- en: To begin, let’s download and extract our dataset.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们下载并提取我们的数据集。
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 14.9](#listing-14-9): Downloading the IMDb movie review dataset'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.9](#listing-14-9)：下载IMDb电影评论数据集'
- en: 'Let’s list out our directory structure:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出我们的目录结构：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see both a train and test set with positive and negative examples. Movie
    reviews with a low user rating on the IMDb site were sorted into the `neg/` directory
    and those with a high rating into the `pos/` directory. We can also see an `unsup/`
    directory, which is short for unsupervised. These are reviews deliberately left
    unlabeled by the dataset creator; they could be negative or positive reviews.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到包含正例和负例的训练集和测试集。在IMDb网站上用户评分低的影评被归类到`neg/`目录中，而评分高的影评被归类到`pos/`目录中。我们还可以看到一个`unsup/`目录，这是无监督的缩写。这些是数据集创建者故意留下未标记的评论；它们可能是负评或正评。
- en: Let’s look at the content of a few of these text files. Whether you’re working
    with text or image data, remember to inspect what your data looks like before
    you dive into modeling. It will ground your intuition about what your model is
    actually doing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一些这些文本文件的内容。无论你是在处理文本数据还是图像数据，记得在深入建模之前检查你的数据看起来是什么样子。这将帮助你理解你的模型实际上在做什么。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 14.10](#listing-14-10): Previewing a single IMDb review'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.10](#listing-14-10)：预览单个IMDb评论'
- en: Before we begin tokenizing our input text, we will make a copy of our training
    data with a few important modifications. We can ignore the unsupervised reviews
    for now and create a separate validation set to monitor our accuracy while training.
    We do this by splitting 20% of the training text files into a new directory.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始标记输入文本之前，我们将对训练数据进行一些重要修改的副本。现在我们可以忽略无监督评论，并创建一个单独的验证集来监控训练过程中的准确率。我们通过将20%的训练文本文件分割到一个新目录中来实现这一点。
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 14.11](#listing-14-11): Splitting validation from the IMDb dataset'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.11](#listing-14-11)：从IMDb数据集中分割验证集'
- en: We are now ready to load the data. Remember how, in chapter 8, we used the `image_dataset_from_directory`
    utility to create a `Dataset` of images and their labels for a directory structure?
    You can do the exact same thing for text files using the `text_dataset_from_directory`
    utility. Let’s create three `Dataset` objects for training, validation, and testing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好加载数据了。记得在第8章中，我们是如何使用`image_dataset_from_directory`实用工具来创建一个包含图像及其标签的`Dataset`对象，用于目录结构？你可以使用`text_dataset_from_directory`实用工具做完全相同的事情来处理文本文件。让我们创建三个`Dataset`对象，分别用于训练、验证和测试。
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 14.12](#listing-14-12): Loading the IMDb dataset for use with Keras'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.12](#listing-14-12)：使用Keras加载IMDb数据集'
- en: Originally we had 25,000 training and testing examples each, and after our validation
    split, we have 20,000 reviews to train on and 5,000 for validation. Let’s try
    learning something from this data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最初我们各有25,000个训练和测试示例，经过验证分割后，我们有20,000条评论用于训练，5,000条用于验证。让我们尝试从这些数据中学习一些东西。
- en: Set models
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置模型
- en: The simplest approach we can take regarding the ordering of tokens in text is
    to discard it. We still tokenize our input reviews normally as a sequence of token
    IDs, but immediately after tokenization, we convert the entire training example
    to a set — a simple unordered “bag” of tokens that are either present or absent
    in a movie review.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于文本中标记顺序的最简单方法就是忽略它。我们仍然将输入评论正常标记为一系列标记ID，但在标记化之后，立即将整个训练示例转换为集合——一个简单的无序“包”，其中包含或不存在于影评中的标记。
- en: The idea here is to use these sets to build a very simple model that assigns
    a weight to every individual word in a review. The presence of the word `"terrible"`
    would probably (though not always) indicate a bad review, and `"riveting"` might
    indicate a good review. We can build a small model that can learn these weights
    — called a bag-of-words model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是使用这些集合构建一个非常简单的模型，为每个评论中的每个单词分配一个权重。单词“terrible”的存在可能（尽管不总是）表示一个差评，而“riveting”可能表示一个好评。我们可以构建一个小型模型来学习这些权重——称为词袋模型。
- en: 'For example, let’s say you had a simple input sentence and vocabulary:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个简单的输入句子和词汇表：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We would tokenize this tiny review as
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个微小的回顾文本标记化如下
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Discarding order, we can turn this into a set of token IDs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略顺序，我们可以将其转换为一系列标记ID：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we could use a multi-hot encoding to transform the set to a fixed-sized
    vector with the same length as a vocabulary:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用多热编码将集合转换为一个与词汇表长度相同的固定大小的向量：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The 0 in the fifth position here means the word `"laugh"` is absent in our review,
    and the 1 in the sixth position means `"cry"` is present. This simple encoding
    of our input review can be used directly to train a model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里第五位上的0表示“laugh”这个词在我们的回顾中不存在，而第六位上的1表示“cry”这个词存在。这种简单的编码可以直接用于训练模型。
- en: Training a bag-of-words model
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练词袋模型
- en: To do this text processing in code, it would be easy enough to extend our `WordTokenizer`
    from earlier in the chapter. An even easier solution is to use the `TextVectorization`
    layer built into Keras. The `TextVectorization` handles word and character tokenization
    and comes with several additional features, including multi-hot encoding of the
    layer output.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要在代码中执行此文本处理，可以很容易地扩展本章早些时候的`WordTokenizer`。一个更简单的解决方案是使用Keras内置的`TextVectorization`层。`TextVectorization`处理单词和字符标记化，并附带一些附加功能，包括层输出的多热编码。
- en: The `TextVectorization` layer, like many preprocessing layers in Keras, has
    an `adapt()` method to learn a layer state from input data. In the case of `TextVectorization`,
    `adapt()` will learn a vocabulary for a dataset on the fly by iterating over an
    input dataset. Let’s use it to tokenize and encode our input data. We will build
    a vocabulary of 20,000 words, a good starting place for text classification problems.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextVectorization`层，像Keras中的许多预处理层一样，有一个`adapt()`方法，可以从输入数据中学习层状态。在`TextVectorization`的情况下，`adapt()`将通过遍历输入数据集动态地学习数据集的词汇表。让我们使用它来标记化和编码我们的输入数据。我们将构建一个包含20,000个单词的词汇表，这对于文本分类问题是一个很好的起点。'
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Listing 14.13](#listing-14-13): Applying a bag-of-words encoding to the IMDb
    reviews'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.13](#listing-14-13)：将词袋编码应用于IMDb评论'
- en: 'Let’s look a single batch of our preprocessed input data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们预处理后的单个输入数据批次：
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can see that after preprocessing, each sample in our batch is converted
    into a vector of 20,000 numbers, each tracking the presence or absence of a vocabulary
    term.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在预处理之后，我们批次中的每个样本都被转换成了一个包含20,000个数字的向量，每个数字跟踪词汇表中每个术语的存在或不存在。
- en: Next, we can train a very simple linear model. We will save our model-building
    code as a function so we can use it again later.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以训练一个非常简单的线性模型。我们将把我们的模型构建代码保存为一个函数，以便以后再次使用。
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[Listing 14.14](#listing-14-14): Building a bag-of-words regression model'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.14](#listing-14-14)：构建词袋回归模型'
- en: 'Let’s take a look at our model’s summary:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们模型的摘要：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This model is dead simple. We have only 20,001 parameters, one for each word
    in our vocabulary and one for a bias term. Let’s train it. We’ll add on the `EarlyStopping`
    callback first covered in chapter 7, which will automatically stop when training
    when the validation loss stops improving and restore weights from the best epoch.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型非常简单。我们只有20,001个参数，一个对应于词汇表中的每个词，一个对应于偏置项。让我们来训练它。我们将添加在第七章中首次介绍的`EarlyStopping`回调，当验证损失停止改进时，它将自动停止训练，并从最佳时期恢复权重。
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[Listing 14.15](#listing-14-15): Training the bag-of-words regression model'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.15](#listing-14-15)：训练词袋回归模型'
- en: 'Our model trains in much less than a minute, which is unsurprising given its
    size. The tokenization and encoding of our input is actually quite a bit more
    expensive than updating our model parameters. Let’s plot the model accuracy (figure
    14.2):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型训练时间不到一分钟，考虑到其规模，这并不令人惊讶。实际上，我们的输入的标记化和编码比更新模型参数要昂贵得多。让我们绘制模型准确率（图14.2）：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](../Images/7fb94f024744bea3143d52da5189a5e6.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7fb94f024744bea3143d52da5189a5e6.png)'
- en: '[Figure 14.2](#figure-14-2): Training and validation metrics for our bag of
    words model'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14.2](#figure-14-2)：我们的词袋模型的训练和验证指标'
- en: We can see that validation performance levels off rather than significantly
    declining; our model is so simple it cannot really overfit. Let’s try evaluating
    it on our test set.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，验证性能趋于平稳而不是显著下降；我们的模型如此简单，实际上无法过度拟合。让我们尝试在测试集上评估它。
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[Listing 14.16](#listing-14-16): Evaluating the bag-of-words regression model'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.16](#listing-14-16)：评估词袋回归模型'
- en: We can correctly predict the sentiment of a review 88% of the time with a training
    job light enough that it could run efficiently on a single CPU.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练作业足够轻的情况下正确预测 88% 的评论情感，这样它就可以在单个 CPU 上高效运行。
- en: It is worth noting our choice of word tokenization in this example. The reason
    to avoid character-level tokenization here is pretty obvious — a “bag” of all
    characters in a movie review will tell you very little about its content. Subword
    tokenization with a large enough vocabulary would be a good choice, but there
    is little need for it here. Since the model we are training is so small, it’s
    convenient to use a vocabulary that is quick to train and have our weights correspond
    to actual English words.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们在这个例子中选择的单词分词方式。避免字符级分词的原因很明显——一个包含电影评论中所有字符的“袋”将告诉你很少关于其内容的信息。使用足够大的词汇量的子词分词将是一个不错的选择，但在这里几乎没有必要。由于我们正在训练的模型非常小，使用一个快速训练且权重对应实际英语单词的词汇表是很方便的。
- en: Training a bigram model
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练二元组模型
- en: 'Of course, we can intuitively guess that discarding all word order is very
    reductive because even atomic concepts can be expressed via multiple words: the
    term “United States” conveys a concept that is quite distinct from the meaning
    of the words “states” and “united” taken separately. A movie that is “not bad”
    and a movie that is “bad” should probably get different sentiment scores.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以直观地猜测，丢弃所有单词顺序是非常简化的，因为即使是原子概念也可以通过多个单词来表达：术语“美国”传达的概念与单独的“州”和“联合”这两个词的意义截然不同。一部“不错”的电影和一部“糟糕”的电影可能应该得到不同的情感分数。
- en: Therefore, it is usually a good idea to inject some knowledge of local word
    ordering into a model, even for these simple set-based models we are currently
    building. One easy way to do that is to consider *bigrams* — a term for two tokens
    that appear consecutively in the input text. Given our example “this movie made
    me cry,” `{"this", "movie", "made", "me", "cry"}` is the set of all word *unigrams*
    in the input, and `{"this movie", "movie made", "made me", "me cry"}` is the set
    of all bigrams. The bag-of-words model we just trained could equivalently be called
    a unigram model, and the term *n-gram* refers to an ordered sequence of *n* tokens
    for any *n*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常在模型中注入一些关于局部单词顺序的知识是一个好主意，即使对于我们目前正在构建的这些简单的集合模型也是如此。一个简单的方法是考虑*二元组*——一个术语，指的是在输入文本中连续出现的两个标记。在我们的例子“这部电影让我哭了”中，`{"this",
    "movie", "made", "me", "cry"}`是输入中所有单词*单语素*的集合，而`{"this movie", "movie made",
    "made me", "me cry"}`是所有二元组的集合。我们刚刚训练的词袋模型可以等价地称为单语素模型，而*n-gram*术语指的是任何*n*的有序标记序列。
- en: 'To add bigrams to our model, we want to consider the frequency of all bigrams
    while building our vocabulary. We could do this in two ways: by creating a vocabulary
    of only bigrams or by allowing both bigrams and unigrams to compete for space
    in the same vocabulary. For the latter case, the term `"United States"` will be
    included in our vocabulary before `"ventriloquism"` if it occurs more frequently
    in the input text.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要将二元组添加到我们的模型中，我们希望在构建词汇表时考虑所有二元组的频率。我们可以通过两种方式做到这一点：创建仅包含二元组的词汇表，或者允许二元组和单语素在同一个词汇表中竞争空间。在后一种情况下，如果“美国”在输入文本中出现的频率高于“ventriloquism”，则“美国”将包含在我们的词汇表中。
- en: Again, we could build this by extending our `WordTokenizer` from earlier in
    the chapter, but there is no need. `TextVectorization` provides this out of the
    box. We will train a slightly larger vocabulary to account for the presence of
    bigrams, `adapt()` a new vocabulary, and multi-hot encode output vectors including
    bigrams.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过扩展本章早些时候的`WordTokenizer`来构建这个模型，但这是不必要的。`TextVectorization`提供了这个功能。我们将训练一个稍微大一点的词汇表来考虑二元组的存在，`adapt()`一个新的词汇表，并对包括二元组的多热编码输出向量。
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Listing 14.17](#listing-14-17): Applying a bigram encoding to the IMDb reviews'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.17](#listing-14-17)：将二元编码应用于 IMDb 评论'
- en: 'Let’s examine a batch of our preprocessed input again:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查我们预处理的输入批次：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If we look at a small subsection of our vocabulary, we can see both unigram
    and bigram terms:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看词汇表的一个小部分，我们可以看到单语和二元术语：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: With our new encoding for our input data, we can train a linear model unaltered
    from before.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们为输入数据的新编码，我们可以训练一个与之前相同的线性模型。
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[Listing 14.18](#listing-14-18): Training the bigram regression model'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.18](#listing-14-18)：训练二元回归模型'
- en: This model will be slightly larger than our bag-of words models (30,001 parameters
    instead of 20,001 parameters), but it still trains in about the same amount of
    time. How did it do?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将比我们的词袋模型稍大一些（30,001 个参数而不是 20,001 个参数），但它仍然需要大约相同的时间来训练。它做得怎么样？
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[Listing 14.19](#listing-14-19): Evaluating the bigram regression model'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.19](#listing-14-19)：评估二元回归模型'
- en: We’re now getting 90% test accuracy, a noticeable improvement!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在获得了 90% 的测试准确率，这是一个明显的提升！
- en: We could improve this number even further by considering trigrams (triplets
    of words), although beyond trigrams, the problem quickly becomes intractable.
    The space of possible 4-grams of words in the English language is immense, and
    the problem grows exponentially as sequences get longer and longer. You would
    need an immense vocabulary to provide decent coverage of 4-grams, and your model
    would lose its ability to generalize, simply memorizing entire snippets of sentences
    with weights attached. To robustly consider longer-ordered text sequences, we
    will need more advanced modeling techniques.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑三元组（单词的三重组合），我们可以进一步提高这个数字，尽管超过三元组，问题会迅速变得难以解决。英语语言中可能的 4-gram 单词空间是巨大的，随着序列变得越来越长，问题呈指数增长。你需要一个庞大的词汇量来提供对
    4-gram 的良好覆盖，而你的模型将失去其泛化能力，仅仅通过附加权重来记忆整个句子的片段。为了稳健地考虑更长的有序文本序列，我们需要更高级的建模技术。
- en: Sequence models
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列模型
- en: Our last two models indicated that sequence information is important. We improved
    a basic linear model by adding features with some info on local word order.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后两个模型表明，序列信息很重要。我们通过添加一些关于局部单词顺序的信息来改进了一个基本的线性模型。
- en: However, this was done by manually engineering input features, and we can see
    how the approach will only scale up to a local ordering of just a few words. As
    is often the case in deep learning, rather than attempting to build these features
    ourselves, we should expose the model to the raw word sequence and let it directly
    learn positional dependencies between tokens.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是通过手动设计输入特征来完成的，我们可以看到这种方法只能扩展到只有几个单词的局部顺序。在深度学习中，通常不是试图自己构建这些特征，而应该让模型直接接触到原始单词序列，并让它直接学习标记之间的位置依赖关系。
- en: Models that ingest a complete token sequence are called, simply enough, *sequence
    models*. We have a few choices for architecture here. We could build an RNN model
    as we just did for timeseries modeling. We could build a 1D ConvNet, similar to
    our image processing models, but convolving filters over a single sequence dimension.
    And as we will dig into in the next chapter, we can build a Transformer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 消费完整标记序列的模型简单地被称为 *序列模型*。在这里我们有几种架构选择。我们可以构建一个 RNN 模型，就像我们刚才为时间序列建模所做的那样。我们可以构建一个
    1D ConvNet，类似于我们的图像处理模型，但只是在单个序列维度上卷积滤波器。而且正如我们将在下一章中深入探讨的，我们可以构建一个 Transformer。
- en: Before taking on any of these approaches, we must preprocess our inputs into
    ordered sequences. We want an integer sequence of token IDs, as we saw in the
    tokenization portion of this chapter, but with one additional wrinkle to handle.
    When we run computations on a batch of inputs, we want all inputs to be rectangular
    so all calculations can be effectively parallelized across the batch on a GPU.
    However, tokenized inputs will almost always have varying lengths. IMDb movie
    reviews range from just a few sentences to multiple paragraphs, with varying word
    counts.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在采取任何这些方法之前，我们必须将我们的输入预处理为有序序列。我们希望有一个整数序列的标记 ID，就像我们在本章的标记化部分所看到的那样，但有一个额外的复杂性要处理。当我们对一个输入批次运行计算时，我们希望所有输入都是矩形的，这样所有计算都可以在
    GPU 上有效地并行化。然而，标记化输入几乎总是具有不同的长度。IMDb 电影评论从只有几句话到多个段落不等，单词数量也各不相同。
- en: To accommodate this fact, we can truncate our input sequences or “pad” them
    with another special token `"[PAD]"`, similar to the `"[UNK]"` token we used earlier.
    For example, given two input sentences and a desired length of eight
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这一事实，我们可以截断我们的输入序列或“填充”它们，使用另一个特殊标记 `"[PAD]"`，类似于我们之前使用的 `"[UNK]"` 标记。例如，给定两个输入句子和一个期望的长度为八
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'we would tokenize to the integer IDs for the following tokens:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对以下标记进行标记化到整数 ID：
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This will allow our batch computation to proceed much faster, although we will
    need to be careful with our padding tokens to ensure they do not affect the quality
    of our model predictions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们的批次计算速度大大加快，尽管我们需要小心处理填充标记，以确保它们不会影响我们模型预测的质量。
- en: To keep a manageable input size, we can truncate our IMDb reviews after the
    first 600 words. This is a reasonable choice, since the average review length
    is 233 words, and only 5% of reviews are longer than 600 words. Once again, we
    can use the `TextVecotorization` layer, which has an option for padding or truncating
    inputs and includes a `"[PAD]"` at index zero of the learned vocabulary.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持可管理的输入大小，我们可以在第一个 600 个单词之后截断我们的 IMDb 评论。这是一个合理的选择，因为平均评论长度为 233 个单词，只有
    5% 的评论长度超过 600 个单词。再次，我们可以使用 `TextVecotorization` 层，该层具有填充或截断输入的选项，并在学习词汇表的索引
    0 处包含一个 `"[PAD]"`。
- en: '[PRE47]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[Listing 14.20](#listing-14-20): Padding IMDb reviews to a fixed sequence length'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.20](#listing-14-20)：将 IMDb 评论填充到固定序列长度'
- en: 'Let’s take a look at a single input batch:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看单个输入批次：
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Each batch has the shape `(batch_size, sequence_length)` after preprocessing,
    and almost all training samples have a number of 0s for padding at the end.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次在预处理后具有形状 `(batch_size, sequence_length)`，并且几乎所有训练样本在末尾都有一定数量的 0 用于填充。
- en: Training a recurrent model
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练循环模型
- en: Let’s try training an LSTM. As we saw in the previous chapter, LSTMs can work
    efficiently with sequence data. Before we can apply it, we still need to map our
    token ID *integers* into floating-point data ingestible by a `Dense` layer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练一个 LSTM。正如我们在上一章中看到的，LSTM 可以有效地处理序列数据。在我们能够应用它之前，我们仍然需要将我们的标记 ID *整数*
    映射到 `Dense` 层可以接受的浮点数据。
- en: The most straightforward approach is to *one-hot* our input IDs, similar to
    the multi-hot encoding we did for an entire sequence. Each token will become a
    long vector with all 0s and a single 1 at the index of the token in our vocabulary.
    Let’s build a layer to one-hot encode our input sequence.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法是将我们的输入 ID *one-hot* 编码，类似于我们对整个序列所做的多热编码。每个标记将变成一个长向量，其中所有元素都是 0，只有一个
    1 在词汇表中的标记索引处。让我们构建一个层来 one-hot 编码我们的输入序列。
- en: '[PRE49]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[Listing 14.21](#listing-14-21): Building a one-hot encoding layer with Keras
    ops'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.21](#listing-14-21)：使用 Keras 操作构建 one-hot 编码层'
- en: 'Let’s try this layer out on a single input batch:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在单个输入批次上尝试这个层：
- en: '[PRE50]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can build this layer directly into a model and use a bidirectional LSTM to
    allow information to propagate both forward and backward along the token sequence.
    Later, when we look at generation, we will see the need for unidirectional sequence
    models (where a token state only depends on the token state before it). For classification
    tasks, a bidirectional LSTM is a good fit.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将这个层构建到模型中，并使用双向 LSTM 来允许信息在标记序列中向前和向后传播。稍后，当我们查看生成时，我们将看到需要单向序列模型（其中标记状态只依赖于它之前的标记状态）的需求。对于分类任务，双向
    LSTM 是一个很好的选择。
- en: Let’s build our model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的模型。
- en: '[PRE51]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[Listing 14.22](#listing-14-22): Building an LSTM sequence model'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.22](#listing-14-22)：构建 LSTM 序列模型'
- en: 'We can take a look at our model summary to get a sense of our parameter count:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看我们的模型摘要，以了解我们的参数数量：
- en: '[PRE52]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This is quite the step up in size from the unigram and bigram models. At about
    15 million parameters, this is one of the larger models we have trained in the
    book so far, with only a single LSTM layer. Let’s trying training the model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这在规模上比单语模型和二元模型有了相当大的提升。大约有 1500 万个参数，这是我们迄今为止在书中训练的较大模型之一，只有一个 LSTM 层。让我们尝试训练这个模型。
- en: '[PRE53]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[Listing 14.23](#listing-14-23): Training the LSTM sequence model'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.23](#listing-14-23)：训练 LSTM 序列模型'
- en: How does it perform?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 它的表现如何？
- en: '[PRE54]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[Listing 14.24](#listing-14-24): Evaluating the LSTM sequence model'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.24](#listing-14-24)：评估 LSTM 序列模型'
- en: 'This model works, but it trains very slowly, especially compared to the lightweight
    model of the previous section. That’s because our inputs are quite large: each
    input sample is encoded as a matrix of size `(600, 30000)` (600 words per sample,
    30,000 possible words). That is 18,000,000 floating-point numbers for a single
    movie review! Our bidirectional LSTM has a lot of work to do. In addition to being
    slow, the model only gets to 84% test accuracy — it doesn’t perform nearly as
    well as our very fast set-based models.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是可行的，但它训练得非常慢，尤其是与上一节中的轻量级模型相比。这是因为我们的输入相当大：每个输入样本被编码为一个大小为`(600, 30000)`的矩阵（每个样本600个单词，30000个可能的单词）。这是一篇电影评论的1800万个浮点数！我们的双向LSTM有很多工作要做。除了速度慢之外，该模型在测试集上的准确率只有84%——它远不如我们非常快速的基于集合的模型。
- en: Clearly, using one-hot encoding to turn words into vectors, which was the simplest
    thing we could do, wasn’t a great idea. There’s a better way — word embeddings.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，使用独热编码将单词转换为向量，这是我们能够做的最简单的事情，并不是一个好主意。有更好的方法——词嵌入。
- en: Understanding word embeddings
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: 'When you encode something via one-hot encoding, you’re making a feature engineering
    decision. You’re injecting into your model a fundamental assumption about the
    structure of your feature space. That assumption is that the different tokens
    you’re encoding are all independent from each other: indeed, one-hot vectors are
    all orthogonal to one another. In the case of words, that assumption is clearly
    wrong. Words form a structured space: they share information with each other.
    The words “movie” and “film” are interchangeable in most sentences, so the vector
    that represents “movie” should not be orthogonal to the vector that represents
    “film” — they should be the same vector, or close enough.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当你通过独热编码对某个事物进行编码时，你正在做出一个特征工程决策。你正在向你的模型注入一个关于你的特征空间结构的根本性假设。这个假设是，你正在编码的不同标记之间都是相互独立的：确实，独热向量都是相互正交的。在单词的情况下，这个假设显然是错误的。单词构成一个结构化的空间：它们相互之间共享信息。在大多数句子中，“电影”和“电影”是可以互换的，所以代表“电影”的向量不应该与代表“电影”的向量正交——它们应该是同一个向量，或者足够接近。
- en: To get more abstract, the geometric relationship between two-word vectors should
    reflect the semantic relationship between these words. For instance, in a reasonable
    word vector space, you would expect synonyms to be embedded into similar word
    vectors, and in general, you would expect the geometric distance (such as the
    cosine distance or L2 distance) between any two-word vectors to relate to the
    “semantic distance” between the associated words. Words that mean different things
    should lie far away from each other, whereas related words should be closer.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要更抽象一点，两个词向量之间的几何关系应该反映这些词之间的语义关系。例如，在一个合理的词向量空间中，你预计同义词将被嵌入到相似的词向量中，通常，你预计任何两个词向量之间的几何距离（如余弦距离或L2距离）将与相关单词之间的“语义距离”相关。意义不同的单词应该彼此远离，而相关的单词应该更接近。
- en: 'Word embeddings are vector representations of words that achieve precisely
    this: they map human language into a structured geometric space.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是单词的向量表示，它们精确地实现了这一点：它们将人类语言映射到结构化的几何空间中。
- en: Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly
    made of zeros), and very high-dimensional (the same dimensionality as the number
    of words in the vocabulary), word embeddings are low-dimensional floating-point
    vectors (that is, dense vectors, as opposed to sparse vectors); see figure 14.3.
    It’s common to see word embeddings that are 256-dimensional, 512-dimensional,
    or 1,024-dimensional when dealing with very large vocabularies. On the other hand,
    one-hot encoding words generally leads to vectors that are 30,000-dimensional
    in the case of our current vocabulary. So word embeddings pack more information
    into far fewer dimensions.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过独热编码获得的向量是二进制、稀疏（主要由零组成）且非常高维（与词汇表中的单词数量相同维度的维度）不同，词嵌入是低维浮点向量（即密集向量，与稀疏向量相对）；参见图14.3。在处理非常大的词汇表时，常见的词嵌入维度是256维、512维或1024维。另一方面，在我们的当前词汇表中，独热编码单词通常会导致30,000维的向量。因此，词嵌入将更多信息压缩到更少的维度中。
- en: '![](../Images/52f950874788bcc19ef66ae130652080.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/52f950874788bcc19ef66ae130652080.png)'
- en: '[Figure 14.3](#figure-14-3): Word representations obtained from one-hot encoding
    or hashing are sparse, high-dimensional, and hardcoded. Word embeddings are dense,
    relatively low-dimensional, and learned from data.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.3](#figure-14-3)：从one-hot编码或哈希得到的词表示是稀疏的、高维的且是硬编码的。词嵌入是密集的、相对低维的，并且是从数据中学习的。'
- en: 'Besides being dense representations, word embeddings are also structured representations,
    and their structure is learned from data. Similar words get embedded in close
    locations, and further, specific directions in the embedding space are meaningful.
    To make this clearer, let’s look at a concrete example. In figure 14.4, four words
    are embedded on a 2D plane: cat, dog, wolf, and tiger. With the vector representations
    we chose here, some semantic relationships between these words can be encoded
    as geometric transformations. For instance, the same vector allows us to go from
    cat to tiger and from dog to wolf: this vector could be interpreted as the “from
    pet to wild animal” vector. Similarly, another vector lets us go from dog to cat
    and from wolf to tiger, which could be interpreted as a “from canine to feline”
    vector.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是密集表示之外，词嵌入也是结构化表示，它们的结构是从数据中学习的。相似词语被嵌入在接近的位置，并且更进一步，嵌入空间中的特定方向是有意义的。为了使这一点更清晰，让我们看一个具体的例子。在图14.4中，四个词语被嵌入在一个2D平面上：猫、狗、狼和老虎。使用我们这里选择的向量表示，这些词语之间的一些语义关系可以编码为几何变换。例如，相同的向量使我们能够从猫到老虎，从狗到狼：这个向量可以解释为“从宠物到野生动物”的向量。同样，另一个向量使我们能够从狗到猫，从狼到老虎，这可以解释为“从犬科到猫科”的向量。
- en: '![](../Images/06fa638a0bceb2fc667ebbdc4a467a38.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fa638a0bceb2fc667ebbdc4a467a38.png)'
- en: '[Figure 14.4](#figure-14-4): A toy example of a word-embedding space'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.4](#figure-14-4)：词嵌入空间的玩具示例'
- en: In real-world word-embedding spaces, typical examples of meaningful geometric
    transformations are “gender” vectors and “plural” vectors. For instance, by adding
    a “female” vector to the vector “king,” we obtain the vector “queen.” By adding
    a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature
    thousands of such interpretable and potentially useful vectors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的词嵌入空间中，典型的有意义的几何变换示例是“性别”向量和“复数”向量。例如，通过向“国王”向量添加一个“女性”向量，我们得到“王后”向量。通过添加一个“复数”向量，我们得到“国王们”。词嵌入空间通常具有数千个这样的可解释且可能有用的向量。
- en: Let’s look at how to use such an embedding space in practice.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实践中使用这样的嵌入空间。
- en: Using a word embedding
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用词嵌入
- en: 'Is there an ideal word-embedding space that perfectly maps human language and
    can be used for any NPL task? Possibly, but we have yet to compute anything of
    the sort. Also, there is no single human language we could attempt to map — there
    are many different languages, and they aren’t isomorphic to one another because
    a language is the reflection of a specific culture and a particular context. More
    pragmatically, what makes a good word-embedding space depends heavily on your
    task: the perfect word-embedding space for an English-language movie review sentiment-analysis
    model may look different from the ideal embedding space for an English-language
    legal document classification model because the importance of certain semantic
    relationships varies from task to task.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 是否存在一个理想的词嵌入空间，可以完美映射人类语言，并且可以用于任何自然语言处理任务？可能存在，但我们还没有计算出这样的东西。此外，没有单一的人类语言我们可以尝试去映射——存在许多不同的语言，它们之间不是同构的，因为一种语言是特定文化和特定背景的反映。更实际地说，一个好的词嵌入空间取决于你的任务：对于英语电影评论情感分析模型来说，完美的词嵌入空间可能与英语法律文档分类模型的理想嵌入空间不同，因为某些语义关系的重要性因任务而异。
- en: It’s thus reasonable to learn a new embedding space with every new task. Fortunately,
    backpropagation makes this easy, and Keras makes it even easier. It’s about learning
    the weights of the Keras `Embedding` layer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每次新任务中学习一个新的嵌入空间是合理的。幸运的是，反向传播使得这一点变得容易，而Keras则使得这一点更加容易。这涉及到学习Keras `Embedding`层的权重。
- en: The `Embedding` layer is best understood as a dictionary that maps integer indices
    (which stand for specific words) to dense vectors. It takes integers as input,
    looks them up in an internal dictionary, and returns the associated vectors. It’s
    effectively a dictionary lookup (see figure 14.5).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding`层最好理解为将整数索引（代表特定单词）映射到密集向量的字典。它接受整数作为输入，在内部字典中查找它们，并返回相关的向量。它实际上是一个字典查找（见图14.5）。'
- en: '![](../Images/d678d0a68641b601e36329f4dbb61538.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d678d0a68641b601e36329f4dbb61538.png)'
- en: '[Figure 14.5](#figure-14-5): An `Embedding` layer acts as a dictionary mapping
    ints to floating point vectors.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14.5](#figure-14-5)：`Embedding` 层充当一个将整数映射到浮点向量的字典。'
- en: The `Embedding` layer takes as input a rank-2 tensor with shape `(batch_size,
    sequence_length)`, where each entry is a sequence of integers. The layer returns
    a floating-point tensor of shape `(batch_size, sequence_length, embedding_size)`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding` 层接受一个形状为 `(batch_size, sequence_length)` 的二维张量作为输入，其中每个条目是一个整数的序列。该层返回一个形状为
    `(batch_size, sequence_length, embedding_size)` 的浮点张量。'
- en: When you instantiate an `Embedding` layer, its weights (its internal dictionary
    of token vectors) are initially random, just as with any other layer. During training,
    these word vectors are gradually adjusted via backpropagation, structuring the
    space into something the downstream model can exploit. Once fully trained, the
    embedding space will show a lot of structure — a kind of structure specialized
    for the specific problem for which you’re training your model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当你实例化一个 `Embedding` 层时，其权重（其内部标记向量的字典）最初是随机的，就像任何其他层一样。在训练过程中，这些词向量通过反向传播逐渐调整，将空间结构化，以便下游模型可以利用。一旦完全训练，嵌入空间将显示出很多结构——一种专门针对你训练模型的具体问题的结构。
- en: Let’s build a model with an `Embedding` layer and benchmark it on our task.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个带有 `Embedding` 层的模型，并在我们的任务上对其进行基准测试。
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[Listing 14.25](#listing-14-25): Building an LSTM sequence model with an `Embedding`
    layer'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.25](#listing-14-25)：使用 `Embedding` 层构建 LSTM 序列模型'
- en: The first two arguments to the `Embedding` layer are fairly straightforward.
    `input_dim` sets the total range of possible values for the integer inputs to
    the layer — that is, how many possible keys are there in our dictionary lookup.
    `output_dim` sets the dimensionality of the output vector we look up — that is,
    the dimensionality of our structured vector space for words.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding` 层的前两个参数相当直接。`input_dim` 设置层中整数输入的可能值的总范围——即我们的字典查找中有多少可能的键。`output_dim`
    设置我们查找的输出向量的维度——即我们单词的结构化向量空间的维度。'
- en: The third argument, `mask_zero=True`, is a little more subtle. This argument
    tells Keras which inputs in our sequence are `"[PAD]"` tokens, so we can *mask*
    these entries later in the model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个参数 `mask_zero=True` 稍微有点微妙。此参数告诉 Keras 我们序列中的哪些输入是 `"[PAD]"` 标记，这样我们就可以在模型中稍后对这些条目进行掩码。
- en: 'Remember that when preprocessing our sequence input, we might add a lot of
    padding tokens to our original input so that a token sequence might look like:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在预处理我们的序列输入时，我们可能会向原始输入添加很多填充标记，以便一个标记序列可能看起来像：
- en: '[PRE56]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: All of those padding tokens will be first embedded and then fed into the `LSTM`
    layer. This means the last representation we receive from the `LSTM` cell might
    contain the results of processing the `"[PAD]"` token representation over and
    over recurrently. We are not very interested in the learned `LSTM` representation
    for the last `"[PAD]"` token in the previous sequence. Instead, we are interested
    in the representation of `"awful"`, the last non-padding token. Or, put equivalently,
    we want to mask all of the `"[PAD]"` tokens so that they do not affect our final
    output prediction.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些填充标记都将首先嵌入，然后输入到 `LSTM` 层。这意味着我们从 `LSTM` 单元接收的最后表示可能包含反复处理 `"[PAD]"` 标记表示的结果。我们对上一个序列中最后一个
    `"[PAD]"` 标记的 `LSTM` 学习到的表示并不感兴趣。相反，我们感兴趣的是 `"awful"` 的表示，即最后一个非填充标记。或者，等价地，我们想要掩码所有的
    `"[PAD]"` 标记，这样它们就不会影响我们的最终输出预测。
- en: '`mask_zero=True` is simply a shorthand to easily do such masking in Keras with
    the `Embedding` layer. Keras will mark all elements in our sequence that initially
    contained a zero value, where zero is assumed to be the token ID for the `"[PAD]"`
    token. This mask will be used internally by the `LSTM` layer. Instead of outputting
    the last learned representation for the whole sequence, it will output the last
    non-masked representation.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`mask_zero=True` 只是一个简写，用于在 Keras 中通过 `Embedding` 层轻松地进行此类掩码。Keras 将标记我们序列中最初包含零值的所有元素，其中零被认为是
    `"[PAD]"` 标记的标记 ID。这个掩码将由 `LSTM` 层内部使用。它不会输出整个序列的最后学习到的表示，而是输出最后非掩码的表示。'
- en: This form of masking is *implicit* and easy to use, but you can always be explicit
    about which items in a sequence you would like to mask if the need arises. The
    `LSTM` layer takes an optional `mask` call argument, for explicit or custom masking.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这种掩码形式是隐式的且易于使用，但如果需要，你总是可以明确指出序列中你想掩码哪些项目。`LSTM` 层接受一个可选的 `mask` 调用参数，用于显式或自定义掩码。
- en: 'Before we train this new model, let’s take a look at the model summary:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练这个新模型之前，让我们先看看模型摘要：
- en: '[PRE57]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We have reduced the number of parameters for our one-hot-encoded LSTM model
    from 15 million to 2 million. Let’s train and evaluate the model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的one-hot编码的LSTM模型参数数量从1500万减少到200万。让我们训练和评估这个模型。
- en: '[PRE58]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[Listing 14.26](#listing-14-26): Training and evaluating the LSTM with an `Embedding`
    layer'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.26](#listing-14-26)：使用`Embedding`层训练和评估LSTM'
- en: With the embedding, we have reduced both our training time and model size by
    an order of magnitude. A learned embedding is clearly far more efficient than
    one-hot encoding our input.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 通过嵌入，我们减少了我们的训练时间和模型大小一个数量级。学习到的嵌入显然比输入的一热编码更有效率。
- en: However, the LSTM’s overall performance did not change. Accuracy was stubbornly
    around 84%, still a far cry from the bag-of-words and bigram models. Does this
    mean that a “structured embedding space” for input tokens is not that practically
    useful? Or is it not useful for text classification tasks?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LSTM的整体性能并没有改变。准确率固执地徘徊在84%左右，仍然与词袋模型和二元模型相去甚远。这意味着“结构化嵌入空间”对于输入标记来说在实践上并不那么有用吗？或者它对于文本分类任务来说没有用？
- en: Quite the contrary, a well-trained token embedding space can dramatically improve
    the practical performance ceiling of a model like this. The issue in this particular
    case is with our training setup. We lack enough data in our 20,000 review examples
    to effectively train a good word embedding. By the end of our 10 training epochs,
    our train set accuracy has cracked 99%. Our model has begun to overfit and memorize
    our input, and it turns out it is doing so well before we have learned an optimal
    set of word embeddings for the task at hand.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 恰恰相反，一个训练良好的标记嵌入空间可以显著提高此类模型的实际性能上限。在这个特定案例中，问题在于我们的训练设置。在我们的20,000条评论示例中，我们缺乏足够的数据来有效地训练一个好的词嵌入。到我们10个训练周期结束时，我们的训练集准确率已经突破了99%。我们的模型已经开始过拟合并记住我们的输入，结果发现它在我们还没有学习到针对当前任务的优化词嵌入集之前就已经表现得很好。
- en: For cases like this, we can turn to *pretraining*. Rather than training our
    word embedding jointly with the classification task, we can train it separately,
    on more data, without the need for positive and negative review labels. Let’s
    take a look.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这类情况，我们可以转向*预训练*。与其将我们的词嵌入与分类任务联合训练，我们可以在更多数据上单独训练它，无需正负样本标签。让我们看看。
- en: Pretraining a word embedding
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练词嵌入
- en: The last decade of rapid advancement in NLP has coincided with the rise of *pretraining*
    as the dominant approach for text modeling problems. Once we move past simple
    set-based regression models to sequence models with millions or even billions
    of parameters, text models become incredibly data-hungry. We are usually limited
    by our ability to find labeled examples for a particular problem in the text domain.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年NLP的快速发展中，*预训练*作为文本建模问题的主要方法也随之兴起。一旦我们超越了简单的基于集合的回归模型，转向具有数百万甚至数十亿参数的序列模型，文本模型对数据的需求变得极其巨大。我们通常受限于在文本领域中找到特定问题的标记示例的能力。
- en: The idea is to devise an unsupervised task to train model parameters that do
    not need labeled data. Pretraining data can be text in a similar domain to our
    final task, or even just arbitrary text in the languages we are interested in
    working with. Pretraining allows us to learn general patterns in language, effectively
    priming our model before we specialize it to the final task we are interested
    in.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是设计一个无监督任务来训练模型参数，这些参数不需要标记数据。预训练数据可以是与我们最终任务类似领域的文本，或者甚至是我们在感兴趣工作的语言中的任意文本。预训练使我们能够学习语言中的通用模式，有效地在我们将模型专门化到感兴趣的最终任务之前对其进行初始化。
- en: Word embeddings were one of the first big successes with text pretraining, and
    we will show how to pretrain a word embedding in this section. Remember the `unsup/`
    directory we ignored in our IMDb dataset preparation? It contains another 25,000
    reviews — the same size as our training data. We will combine all our training
    data together and show how to pretrain the parameters of an `Embedding` layer
    with an unsupervised task.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是文本预训练的第一个重大成功之一，我们将在本节中展示如何预训练词嵌入。还记得我们在IMDb数据集准备中忽略的`unsup/`目录吗？它包含另外25,000条评论——与我们的训练数据大小相同。我们将结合所有我们的训练数据，并展示如何通过无监督任务预训练`Embedding`层的参数。
- en: One of the most straightforward setups for training a word embedding is called
    the Continuous Bag of Words (CBOW) model^([[2]](#footnote-2)). The idea is to
    slide a window over all the text in a dataset, where we continuously attempt to
    guess a missing word based on the words that appear to its direct right and left
    (figure 14.6). For example, if our “bag” of surrounding words contained the words
    “sail,” “wave,” and “mast,” we might guess that the middle word is “boat” or “ocean.”
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 训练词嵌入的最直接设置之一被称为连续词袋（CBOW）模型^([[2]](#footnote-2))。其思路是在数据集的所有文本上滑动一个窗口，我们持续尝试根据出现在其直接左右两侧的词来猜测一个缺失的词（图14.6）。例如，如果我们的“词袋”周围包含“sail”、“wave”和“mast”这些词，我们可能会猜测中间的词是“boat”或“ocean”。
- en: '![](../Images/9a0544ddd85c2ddb1332fbe1d1c5febb.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a0544ddd85c2ddb1332fbe1d1c5febb.png)'
- en: '[Figure 14.6](#figure-14-6): The Continuous Bag of Words predicts a word based
    on its surrounding context with a shallow neural network.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.6](#figure-14-6)：连续词袋模型使用浅层神经网络根据其周围上下文预测一个词。'
- en: In our particular IMDb classification problem, we are interested in “priming”
    the word embedding of the LSTM model we just trained. We can reuse the `TextVectorization`
    vocabulary we computed earlier. All we are trying to do here is to learn a good
    64-dimensional vector for each word in this vocabulary.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特定的IMDb分类问题中，我们感兴趣的是“初始化”我们刚刚训练的LSTM模型的词嵌入。我们可以重用之前计算出的`TextVectorization`词汇表。我们在这里试图做的只是为这个词汇表中的每个词学习一个良好的64维向量。
- en: We can create a new `TextVectorization` layer with the same vocabulary that
    does not truncate or pad input. We will preprocess the output tokens of this layer
    by sliding a context window across our text.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个新的`TextVectorization`层，它具有相同的词汇表，但不截断或填充输入。我们将通过在文本上滑动上下文窗口来预处理这个层的输出标记。
- en: '[PRE59]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[Listing 14.27](#listing-14-27): Removing padding from our `TextVectorization`
    preprocessing layer'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表14.27](#listing-14-27)：从我们的`TextVectorization`预处理层中移除填充'
- en: To preprocess our data, we will slide a window across our training data, creating
    “bags” of nine consecutive tokens. Then, we use the middle word as our label and
    the remaining eight words as an unordered context to predict our label.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预处理我们的数据，我们将在训练数据上滑动一个窗口，创建包含九个连续标记的“词袋”。然后，我们使用中间词作为我们的标签，并将剩余的八个词作为无序上下文来预测我们的标签。
- en: To do this, we will again use `tf.data` to preprocess our inputs, although this
    choice does not limit the backend we use for actual model training.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们再次使用`tf.data`来预处理我们的输入，尽管这个选择并不限制我们实际模型训练时使用的后端。
- en: '[PRE60]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[Listing 14.28](#listing-14-28): Preprocessing our IMDb data for pretraining
    a CBOW model'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表14.28](#listing-14-28)：预处理我们的IMDb数据以预训练CBOW模型'
- en: After preprocessing, we can see that we have eight integer token IDs as context
    paired with a single token ID label.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理之后，我们可以看到我们有八个整数标记ID作为上下文与一个单独的标记ID标签配对。
- en: The model we train with this data is exceedingly simple. We will use an `Embedding`
    layer to embed all context tokens and a `GlobalAveragePooling1D` to compute the
    average embedding of our “bag” of context tokens. Then, we use that average embedding
    to predict the value of our middle label token.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些数据训练的模型极其简单。我们将使用一个`Embedding`层来嵌入所有上下文标记，并使用`GlobalAveragePooling1D`来计算上下文“词袋”的平均嵌入。然后，我们使用这个平均嵌入来预测中间标签标记的值。
- en: That’s it! By repeatedly refining our embedding space so that we are good at
    predicting a word based on nearby word embeddings, we learn a rich embedding of
    tokens used in movie reviews.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！通过反复细化我们的嵌入空间，以便我们擅长根据附近的词嵌入来预测一个词，我们学会了电影评论中使用的标记的丰富嵌入。
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[Listing 14.29](#listing-14-29): Building a CBOW model'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表14.29](#listing-14-29)：构建CBOW模型'
- en: '[PRE62]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Because our model is so simple, we can use a large batch size to speed up training
    without worrying about memory constraints.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型非常简单，我们可以使用较大的批处理大小来加快训练速度，而不用担心内存限制。
- en: We will also call `cache()` on this batched dataset so that we store the entire
    preprocessed dataset in memory rather than recomputing it each epoch. This is
    because for this very simple model, we are bottlenecked on preprocessing rather
    than training. That is, it is slower to tokenize our text and compute sliding
    windows on the CPU than to update our model parameters on the GPU.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将对这个批处理数据集调用`cache()`，以便我们将整个预处理的整个数据集存储在内存中，而不是在每个epoch重新计算它。这是因为对于这个非常简单的模型，我们的瓶颈在于预处理而不是训练。也就是说，在CPU上对文本进行标记和计算滑动窗口比在GPU上更新我们的模型参数要慢。
- en: In such cases, saving your preprocessed outputs in memory or on disk is usually
    a good idea. You will notice how our later epochs are more than three times faster
    than the first. This is thanks to the cache of preprocessed training data.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，将您的预处理输出保存在内存中或磁盘上通常是一个好主意。您会注意到我们的后续epoch比第一次快三倍以上。这要归功于预处理的训练数据缓存。
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[Listing 14.30](#listing-14-30): Training the CBOW model'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.30](#listing-14-30)：训练CBOW模型'
- en: At the end of training, we are able to guess the middle word around 12% of the
    time based solely on the neighboring eight words. This may not sound like a great
    result, but given that we have 30,000 words to guess from each time, this is actually
    a reasonable accuracy score.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结束时，我们能够大约12%的时间仅根据邻近的八个单词来猜测中间的单词。这可能听起来不是一个很好的结果，但考虑到每次我们有30,000个单词可供猜测，这实际上是一个合理的准确率。
- en: Let’s use this word embedding to improve the performance of our LSTM model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个词嵌入来提高我们LSTM模型的表现。
- en: Using the pretrained embedding for classification
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的嵌入进行分类
- en: Now that we have trained a new word embedding, applying it to our LSTM model
    is simple. First, we create the model precisely as we did before.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个新的词嵌入，将其应用于我们的LSTM模型很简单。首先，我们创建的模型与之前完全相同。
- en: '[PRE64]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[Listing 14.31](#listing-14-31): Building another LSTM sequence model with
    an `Embedding` layer'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.31](#listing-14-31)：使用`Embedding`层构建另一个LSTM序列模型'
- en: Then, we apply our embedding weights from the CBOW embedding layer to the LSTM
    embedding layer. This effectively acts as a new and better initializer for the
    roughly 2 million embedding parameters in the LSTM model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将CBOW嵌入层的嵌入权重应用到LSTM嵌入层。这实际上为LSTM模型中大约200万个嵌入参数提供了一个新的、更好的初始化器。
- en: '[PRE65]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[Listing 14.32](#listing-14-32): Reusing the CBOW embedding to prime the LSTM
    model'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.32](#listing-14-32)：重用CBOW嵌入以初始化LSTM模型'
- en: With that, we can compile and train our LSTM model as normal.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以像平常一样编译和训练我们的LSTM模型。
- en: '[PRE66]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[Listing 14.33](#listing-14-33): Training the LSTM model with a pretrained
    embedding.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.33](#listing-14-33)：使用预训练嵌入训练LSTM模型。'
- en: Let’s evaluate our LSTM model.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估我们的LSTM模型。
- en: '[PRE67]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[Listing 14.34](#listing-14-34): Evaluating the LSTM model with a pretrained
    embedding'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.34](#listing-14-34)：使用预训练嵌入评估LSTM模型'
- en: With the pretrained embedding weights, we have boosted our LSTM performance
    to roughly the same as our set-based models. We do slightly better than the unigram
    model and slightly worse than the bigram model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的嵌入权重，我们将LSTM的性能提升到了与我们的集合模型大致相同。我们略好于单语模型，略差于双语模型。
- en: This may seem like a bit of a letdown after all the work we put in. Is learning
    on the entire sequence, with order information, just a bad idea? The problem is
    that we are *still data-constrained* for our final LSTM model. The model is expressive
    and powerful enough that with enough movie reviews, we would easily outperform
    set-based approaches, but we need a lot more training on *ordered data* before
    our model’s performance ceiling is reached.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在投入了所有这些工作之后，这可能会让人有些失望。在整个序列上，带有顺序信息进行学习，难道真是一个糟糕的想法吗？问题是，我们的最终LSTM模型仍然受到数据限制。模型的表达能力和强大程度足以在拥有足够的电影评论的情况下，我们很容易超越基于集合的方法，但我们需要在有序数据上进行更多的训练，才能达到我们模型性能的上限。
- en: This is an easily solvable problem with enough compute resources. In the next
    chapter, we will cover the transformer model. The model is slightly better at
    learning dependencies across longer token sequences, but most critically, these
    models are often trained on large amounts of English text, including all word
    order information. This allows the model to learn, roughly speaking, a statistical
    form of the grammatical patterns that govern language. These types of statistical
    patterns around word order are precisely why our current LSTM model is too data-constrained
    to learn effectively.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在足够的计算资源下可以轻松解决的问题。在下一章中，我们将介绍transformer模型。该模型在跨较长的标记序列学习依赖关系方面略胜一筹，但最重要的是，这些模型通常在大量的英文文本上训练，包括所有单词顺序信息。这允许模型学习，粗略地说，一种统计形式的语法模式，该模式控制着语言。这些关于单词顺序的统计模式正是我们当前的LSTM模型过于数据受限而无法有效学习的原因。
- en: However, as we move on to large, more advanced models that will push the limits
    of text-classification performance, it is worth pointing out that simple set-based
    regression approaches like our bigram model give you a lot of “bang for your buck.”
    Set-based models are lightning-fast and can contain just a few thousand parameters,
    a far cry from the billion-parameter large language models that dominate the news
    today.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们转向更大、更先进的模型，这些模型将推动文本分类性能的极限时，值得注意的是，像我们的二元模型这样的简单基于集合的回归方法可以给你带来很多“物有所值”。基于集合的模型速度极快，并且可以只包含几千个参数，这与今天新闻中占据主导地位的数十亿参数的大型语言模型相去甚远。
- en: If you are working in an environment where compute is limited and you can sacrifice
    some accuracy, set-based models can often be the most cost-effective approach.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个计算资源有限的环境中工作，并且可以牺牲一些准确性，那么基于集合的模型通常是最具成本效益的方法。
- en: Summary
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: All text modeling problems involve a preprocessing step where text is broken
    up and transformed to integer data, called *tokenization*.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有文本建模问题都涉及一个预处理步骤，其中文本被拆分并转换为整数数据，称为*标记化*。
- en: 'Tokenization can be divided into three steps: *standardization*, *splitting*,
    and *indexing*. Standardization normalizes text, splitting breaks text up into
    tokens, and indexing assigns each token a unique integer ID.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化可以分为三个步骤：*标准化*、*分割*和*索引*。标准化使文本标准化，分割将文本拆分成标记，索引为每个标记分配一个唯一的整数ID。
- en: 'There are three main types of tokenization: *character*, *word*, and *subword*
    tokenization. With an expressive-enough model and sufficient training data, the
    *subword* tokenization is usually the most effective.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化主要有三种类型：*字符*、*单词*和*子词*标记化。在有足够表达力和训练数据的情况下，*子词*标记化通常是效果最好的。
- en: 'NLP models differ primarily in handling the order of input tokens:'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP模型在处理输入标记的顺序上存在主要差异：
- en: '*Set models* discard most order information and learn simple and fast models
    based solely on the presence or absence of tokens in the input. *Bigram* or *trigram*
    models consider the presence or absence of two or three consecutive tokens. Set
    models are incredibly fast to train and deploy.'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集合模型*丢弃了大部分顺序信息，并仅基于输入中标记的存在或不存在来学习简单且快速的模型。*二元*或*三元*模型考虑两个或三个连续标记的存在或不存在。集合模型训练和部署速度极快。'
- en: '*Sequence models* attempt to learn with the ordered sequence of tokens in the
    input data. Sequence models need large amounts of data to learn effectively.'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列模型*试图通过输入数据中标记的有序序列来学习。序列模型需要大量的数据才能有效地学习。'
- en: An *embedding* is an efficient way to transform token IDs into a learned latent
    space. Embeddings can be trained normally with gradient descent.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入*是将标记ID转换为学习到的潜在空间的一种有效方式。嵌入可以通过梯度下降正常训练。'
- en: '*Pretraining* is critical for sequence models as a way to get around the data-hungry
    nature of these models. During *pretraining*, an unsupervised task allows models
    to learn from large amounts of unlabeled text data. The learned parameters can
    then be transferred to a downstream task.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预训练*对于序列模型至关重要，因为它可以克服这些模型对数据的贪婪需求。在*预训练*期间，一个无监督任务允许模型从大量未标记的文本数据中学习。然后可以将学习到的参数转移到下游任务。'
- en: Footnotes
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Phillip Gage, “A New Algorithm for Data Compression,” *The C Users Journal Archive*
    (1994), [https://dl.acm.org/doi/10.5555/177910.177914](https://dl.acm.org/doi/10.5555/177910.177914).
    [[↩]](#footnote-link-1)
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Phillip Gage，“一种新的数据压缩算法”，*C用户杂志档案*（1994年），[https://dl.acm.org/doi/10.5555/177910.177914](https://dl.acm.org/doi/10.5555/177910.177914)。[[↩]](#footnote-link-1)
- en: Mikolov et al., “Efficient Estimation of Word Representations in Vector Space,”
    International Conference on Learning Representations (2013), [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781).
    [[↩]](#footnote-link-2)
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mikolov等人，“在向量空间中高效估计词表示”，国际学习表示会议（2013年），[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)。[[↩]](#footnote-link-2)
