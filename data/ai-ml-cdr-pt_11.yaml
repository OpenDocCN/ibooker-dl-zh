- en: Chapter 10\. Creating ML Models to Predict Sequences
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 创建用于预测序列的ML模型
- en: '[Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    introduced sequence data and the attributes of a time series, including seasonality,
    trend, autocorrelation, and noise. You created a synthetic series to use for predictions,
    and you explored how to do basic statistical forecasting.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)介绍了序列数据和时间序列的属性，包括季节性、趋势、自相关性和噪声。你创建了一个合成序列用于预测，并探讨了如何进行基本的统计预测。'
- en: Over the next couple of chapters, you’ll learn how to use ML for forecasting.
    But before you start creating models, you need to understand how to structure
    the time series data for training predictive models by creating what we’ll call
    a *windowed dataset.*
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，你将学习如何使用ML进行预测。但在你开始创建模型之前，你需要了解如何通过创建我们称之为*窗口化数据集*的方式来结构化时间序列数据以进行训练预测模型。
- en: To understand why you need to do this, consider the time series you created
    in [Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578).
    You can see a plot of it in [Figure 10-1](#ch10_figure_1_1748549713788310).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么你需要这样做，考虑你在[第9章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)中创建的时间序列。你可以在[图10-1](#ch10_figure_1_1748549713788310)中看到它的一个图表。
- en: If at any point, you want to predict a value at time *t*, you’ll want to predict
    it as a function of the values preceding time *t*. For example, say you want to
    predict the value of the time series at time step 1,200 as a function of the 30
    values preceding it. In this case, the values from time steps 1,170 to 1,199 would
    determine the value at time step 1,200 (see [Figure 10-2](#ch10_figure_2_1748549713788360)).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在任何时候，你想预测时间`t`的值，你将想将其作为时间`t`之前值的函数进行预测。例如，假设你想将时间序列在时间步1,200的值作为其之前30个值的函数进行预测。在这种情况下，时间步1,170到1,199的值将决定时间步1,200的值（参见[图10-2](#ch10_figure_2_1748549713788360)）。
- en: '![](assets/aiml_1001.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1001.png)'
- en: Figure 10-1\. Synthetic time series
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 合成时间序列
- en: '![](assets/aiml_1002.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1002.png)'
- en: Figure 10-2\. Previous values impacting prediction
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 前期值对预测的影响
- en: 'Now, this begins to look familiar: you can consider the values from 1,170 to
    1,199 to be your *features* and the value at 1,200 to be your *label*. If you
    can get your dataset into a condition where you have a certain number of values
    as features and the following value as the label, and if you do this for every
    known value in the dataset, then you’ll end up with a pretty decent set of features
    and labels that you can use to train a model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这开始看起来熟悉了：你可以将1,170到1,199的值视为你的*特征*，将1,200的值视为你的*标签*。如果你能将你的数据集整理成具有一定数量的特征值和随后的值作为标签的状态，并且如果你对数据集中的每个已知值都这样做，那么你最终将得到一组相当不错的特征和标签，你可以使用这些特征和标签来训练模型。
- en: Before doing this for the time series dataset from [Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578),
    let’s create a very simple dataset that has all the same attributes but a much
    smaller amount of data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在对[第9章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)中的时间序列数据集进行此操作之前，让我们创建一个非常简单的数据集，它具有所有相同的属性，但数据量要小得多。
- en: Creating a Windowed Dataset
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建窗口化数据集
- en: 'PyTorch has a lot of APIs that are useful for manipulating data. For example,
    you can use `torch.arange(10)` to create a basic dataset containing the numbers
    0–9, thus emulating a time series. You can then turn that dataset into the beginnings
    of a windowed dataset. Here’s the code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch有很多用于操作数据的API。例如，你可以使用`torch.arange(10)`创建一个包含数字0到9的基本数据集，从而模拟时间序列。然后，你可以将此数据集转换为窗口化数据集的开始。以下是代码：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, it creates the dataset by using a range, which simply makes the dataset
    contain the values 0 to *n* – 1, where *n* is, in this case, 10.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它通过使用范围创建数据集，这仅仅使数据集包含从0到*n* – 1的值，其中*n*在这种情况下是10。
- en: 'Next, calling `create_sliding_windows` and passing a parameter of `5` specifies
    that the network should split the dataset into windows of five items. Specifying
    `shift=1` causes each window to then be shifted one spot from the previous one:
    the first window will contain the five items beginning at 0, the next window will
    contain the five items beginning at 1, etc.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`create_sliding_windows`并传递参数`5`表示网络应将数据集分割成包含五个项目的窗口。指定`shift=1`会导致每个窗口相对于前一个窗口移动一个位置：第一个窗口将包含从0开始的五个项目，下一个窗口将包含从1开始的五个项目，依此类推。
- en: 'Running this code will give you the following result:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将给出以下结果：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Earlier, you saw that we want to make training data out of this, where there
    are *n* values defining a feature and there is a subsequent value giving a label.
    You can do this with some simple Python list slicing that splits each window into
    two things: everything before the last value and the last value only.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你看到我们想要将此数据转换为训练数据，其中 *n* 个值定义了一个特征，并且有一个随后的值给出标签。你可以通过一些简单的 Python 列表切片来完成此操作，将每个窗口拆分为两部分：最后一个值之前的一切和仅最后一个值。
- en: This uses the `unfold` technique on tensor data, which creates a sliding window
    over your data to turn it into sets like those outlined previously.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这在张量数据上使用 `unfold` 技术，它通过在数据上创建滑动窗口来将其转换为之前概述的集合。
- en: 'It also takes three parameters:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它还接受三个参数：
- en: Dimension (0 in this case)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 维度（在本例中为 0）
- en: This is the dimension along which to unfold.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是展开的维度。
- en: window_size
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: window_size
- en: This is the size of each sliding window.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个滑动窗口的大小。
- en: shift
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: shift
- en: This is the stride/step size between windows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是窗口之间的步长/步长大小。
- en: 'This process gives us an `*x*` and a `*y*` dataset, as shown here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程为我们提供了一个 `*x*` 和一个 `*y*` 数据集，如下所示：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results are now in line with what you’d expect. The first four values in
    the window can be thought of as the features, with the subsequent value being
    the label:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果现在符合你的预期。窗口中的前四个值可以被认为是特征，随后的值是标签：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, with the PyTorch TensorDataset type, you can turn this into a dataset and
    do things like shuffling and batching natively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 PyTorch TensorDataset 类型，你可以将其转换为数据集，并原生地执行诸如洗牌和分批等操作。
- en: Note that when shuffling data, it’s good practice to ensure that the validation
    and test datasets are separated first. In time series data, shuffling before the
    split can cause information from the training set to bleed into the test set,
    which would compromise the evaluation process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在洗牌数据时，确保首先将验证集和测试集分开是一个好习惯。在时间序列数据中，在拆分之前洗牌可能会导致训练集的信息泄露到测试集中，这会损害评估过程。
- en: 'Here, it’s been shuffled and batched with a batch size of 2:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，它已经被洗牌并分批，批大小为 2：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The results show that the first batch has two sets of `*x*` (starting at 5
    and 0, respectively) with their labels, the second batch has two sets of `*x*`
    (starting at 1 and 3, respectively) with their labels, and so on:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，第一个批次有两个 `*x*` 集合（分别从 5 和 0 开始）及其标签，第二个批次有两个 `*x*` 集合（分别从 1 和 3 开始）及其标签，依此类推：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With this technique, you can now turn any time series dataset into a set of
    training data for a neural network. In the next section, you’ll explore how to
    take the synthetic data from [Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    and create a training set from it. From there, you’ll move on to creating a simple
    DNN that is trained on this data and can be used to predict future values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，你现在可以将任何时间序列数据集转换为神经网络训练数据集。在下一节中，你将探索如何从 [第 9 章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    中的合成数据创建训练集。从那里，你将继续创建一个简单的 DNN，该 DNN 在此数据上训练，并可用于预测未来值。
- en: Creating a Windowed Version of the Time Series Dataset
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建时间序列数据集的窗口版本
- en: 'As a recap, here’s the code we used in the previous chapter to create a synthetic
    time series dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回顾，以下是我们在上一章中用于创建合成时间序列数据集的代码：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will create a time series that looks like the one in [Figure 10-3](#ch10_figure_3_1748549713788388).
    If you want to change it, feel free to tweak the values of the various constants.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个看起来像 [图 10-3](#ch10_figure_3_1748549713788388) 中的时间序列。如果你想改变它，请随意调整各种常数的值。
- en: '![](assets/aiml_1003.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1003.png)'
- en: Figure 10-3\. Plotting the time series with trend, seasonality, and noise
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 绘制具有趋势、季节性和噪声的时间序列
- en: 'Once you have the series, you can turn it into a windowed dataset with code
    similar to that in the previous section:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了序列，你可以使用与上一节中类似的代码将其转换为窗口数据集：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see the output here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到输出：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To train a model with this data, you’ll split the series into training and validation
    datasets. In this case, we’ll train on 1,000 records by splitting the list and
    turning it into `train_dataset` and `val_dataset` subsets. This uses the `Subset`
    class from `torch.utils.data`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些数据训练模型，你需要将序列拆分为训练集和验证集。在这种情况下，我们将通过拆分列表并将其转换为 `train_dataset` 和 `val_dataset`
    子集来在 1,000 条记录上进行训练。这使用了来自 `torch.utils.data` 的 `Subset` 类。
- en: 'You can then load these with a `DataLoader`, as we saw in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用`DataLoader`来加载这些数据，就像我们在[第3章](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)中看到的那样：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The important thing to remember now is that your data is a dataset, so you can
    easily use it in model training without further coding.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要记住的重要事情是，你的数据是一个数据集，因此你可以轻松地在模型训练中使用它，而无需进一步编码。
- en: 'If you want to inspect what the data looks like, you can do so with code like
    this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想检查数据的样子，可以使用如下代码：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, the `batch_size` is set to `1`, just to make the results more readable.
    You’ll therefore end up with output like this, in which a single set of data is
    in the batch:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`batch_size`设置为`1`，只是为了使结果更易读。因此，你最终会得到这样的输出，其中单个数据集在批次中：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first batch of numbers are the features. We’ve set the window size to 30,
    so it’s a 1 × 30 tensor. The second number is the label (28.710 in this case),
    which the model will try to fit the features to. You’ll see how that works in
    the next section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前几个数字是特征。我们将窗口大小设置为30，因此它是一个1 × 30的张量。第二个数字是标签（在这个例子中是28.710），模型将尝试将其拟合到特征上。你将在下一节中看到它是如何工作的。
- en: Creating and Training a DNN to Fit the Sequence Data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和训练一个深度神经网络（DNN）以适应序列数据
- en: 'Now that you have the data, creating a neural network model becomes very straightforward.
    Let’s first explore a simple DNN. Here’s the model definition:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了数据，创建一个神经网络模型变得非常简单。让我们首先探索一个简单的DNN。以下是模型定义：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It’s a super-simple model with three linear layers, the first of which accepts
    the input shape of `window_size` and then a hidden layer of 10 neurons, before
    an output layer that will contain the predicted value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个超级简单的模型，包含三个线性层，第一个层接受`window_size`的输入形状，然后是一个包含10个神经元的隐藏层，最后是一个包含预测值的输出层。
- en: 'The model is initialized with a loss function and optimizer, as before:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型初始化时使用损失函数和优化器，就像之前一样：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, the loss function is specified as `MSELoss`, which stands for
    “mean squared error” and is commonly used in regression problems (which is what
    this ultimately boils down to). For the optimizer, `Adam` is a good fit. I won’t
    go into detail on these types of functions in this book, but any good resource
    on ML will teach you about them—Andrew Ng’s seminal “[Deep Learning Specialization](https://oreil.ly/A8QzN)”
    on Coursera is a great place to start.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，损失函数指定为`MSELoss`，代表“均方误差”，在回归问题中常用（这正是最终归结为的问题）。对于优化器，`Adam`是一个很好的选择。在这本书中，我不会详细介绍这些类型的函数，但任何好的机器学习资源都会教你关于它们的知识——Coursera上的Andrew
    Ng的标志性“[深度学习专项课程](https://oreil.ly/A8QzN)”是一个很好的起点。
- en: 'Training is then pretty standard. It’s composed of loading the batches from
    the training loader and performing a forward pass with them, followed by a backward
    pass with optimization:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程相当标准。它由从训练加载器中加载数据批次，并使用它们执行前向传递，然后执行带有优化的反向传递组成：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Given that we also have a validation dataset, we can also perform a validation
    pass for each epoch:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还有一个验证数据集，我们还可以在每个epoch执行验证传递：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you train, you’ll see the loss function report a number that will start high
    but decline steadily. [Figure 10-4](#ch10_figure_4_1748549713788413) shows the
    loss over 100 epochs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，你会看到损失函数报告的数字将从高开始但会稳步下降。[图10-4](#ch10_figure_4_1748549713788413)显示了100个epoch的损失。
- en: '![](assets/aiml_1004.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1004.png)'
- en: Figure 10-4\. The DNN predictor of model loss over time for the time series
    data
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4. 模型损失随时间对时间序列数据的预测
- en: Evaluating the Results of the DNN
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估DNN的结果
- en: Once you have a trained DNN, you can start predicting with it. But remember,
    you have a windowed dataset, so the prediction for a given point is based on the
    values of a certain number of time steps before it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了训练好的DNN，你就可以开始使用它进行预测。但请记住，你有一个窗口数据集，因此给定点的预测是基于它之前的一定数量的时间步长的值。
- en: Also, given that the dataset is batched, we can easily use the loaders to access
    batches and explore what it looks like to predict on them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于数据集是分批的，我们可以轻松使用加载器来访问批次并探索预测它们的样子。
- en: 'Here’s the code. We iterate through each batch in the loader and get the features
    and targets, and then we can get the predictions for the batch by sending the
    batch to the model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码。我们遍历加载器中的每个批次，获取特征和目标，然后我们可以通过将批次发送到模型来获取批次的预测：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The predictions are then converted from PyTorch tensors into NumPy arrays using
    `.numpy()`, and then the batches are turned into a single list with the `extend()`
    call.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将预测值从PyTorch张量转换为NumPy数组使用`.numpy()`，然后使用`extend()`调用将批次转换为单个列表。
- en: You might have also noticed the `.cpu()` in this code. PyTorch allows you to
    designate *where* your code runs, and if you have a GPU or other accelerator available,
    you can push the intense calculations of ML to it. You can also use a CPU to do
    other things like processing and preprocessing data to save accelerator time.
    This code allows you to explicitly express that.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到了代码中的`.cpu()`。PyTorch允许你指定代码运行的*位置*，如果你有一个GPU或其他加速器可用，你可以将ML的密集计算推送到它。你也可以使用CPU来做其他事情，比如处理和预处理数据以节省加速器时间。这段代码允许你明确地表达这一点。
- en: 'So, you can then compare your predictions with the actual values quite easily.
    Here’s the code you use to plot the predicted values against the actual ones using
    matplotlib:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以很容易地比较你的预测值与实际值。这是你用来使用matplotlib绘制预测值与实际值对比的代码：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see the results of this in [Figure 10-5](#ch10_figure_5_1748549713788437).
    The line for the predicted values (in red) closely matches the overall pattern
    of the original data, but it’s less noisy, with a lot less variance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图10-5](#ch10_figure_5_1748549713788437)中看到这个结果。预测值（红色线条）与原始数据的整体模式非常接近，但噪声更少，方差更小。
- en: '![](assets/aiml_1005.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1005.png)'
- en: Figure 10-5\. Predicted versus actual values in the validation set
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 验证集中的预测值与实际值
- en: From a quick visual inspection, you can see that the prediction isn’t bad because
    it’s generally following the curve of the original data. When there are rapid
    changes in the data, the prediction takes a little time to catch up, but on the
    whole, it isn’t bad.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从快速视觉检查中，你可以看到预测并不差，因为它通常遵循原始数据的曲线。当数据有快速变化时，预测需要一点时间来赶上，但总体来说，并不差。
- en: 'However, it’s hard to be precise when eyeballing the curve. It’s best to have
    a good metric, and in [Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    you learned about one—the MAE. Now that you have the valid data and the results,
    you can measure the MAE with this code by using `torch.mean` and `torch.abs`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当用肉眼观察曲线时很难做到精确。最好有一个好的指标，在[第9章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)中你学到了一个——MAE。现在你有了有效数据和结果，你可以使用`torch.mean`和`torch.abs`这个代码来测量MAE：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Randomness has been introduced into the data, so your results may vary, but
    when I tried it, I got a value of 4.57 as the MAE.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性已被引入数据中，因此你的结果可能会有所不同，但当我尝试时，我得到了4.57作为MAE的值。
- en: You could also argue that at this point, the process of getting the predictions
    as accurate has become the process of minimizing that MAE. There are some techniques
    that you can use to do this, including the obvious changing of the window size.
    I’ll leave you to experiment with that, but in the next section, you’ll do some
    basic hyperparameter tuning on the optimizer to improve how your neural network
    learns, and see what impact that will have on the MAE.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以争论说，到这一点，获取准确预测的过程已经变成了最小化MAE的过程。你可以使用一些技术来完成这项工作，包括显然的改变窗口大小。我将让你去实验这个，但在下一节中，你将对优化器进行一些基本的超参数调整，以改善你的神经网络学习方式，并看看这将对MAE产生什么影响。
- en: Tuning the Learning Rate
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整学习率
- en: 'In the previous example, you might recall that you compiled the model with
    an optimizer that looked like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，你可能记得你使用了一个看起来像这样的优化器编译了模型：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In that case, you didn’t specify an LR, so the model used the default LR of
    1 × 10^–³. But that seemed to be a really arbitrary number. What if you changed
    it, and how should you go about changing it? It would take a lot of experimentation
    to find the best rate.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在那种情况下，你没有指定学习率（LR），所以模型使用了默认的1 × 10^–³ LR。但这似乎是一个相当随机的数字。如果你改变了它，你应该如何去做？找到最佳速率需要大量的实验。
- en: One way to experiment with this is by using a `torch.optim.lr_scheduler,` which
    can change the LR on the fly, epoch by epoch, as the model trains.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实验这个的方法是使用`torch.optim.lr_scheduler`，它可以在模型训练的每个epoch动态地改变LR。
- en: 'A good practice is to start with a higher LR and gradually reduce it as the
    network learns. Here’s an example:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的做法是开始时使用较高的LR，随着网络的学习逐渐降低它。以下是一个例子：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this case, you’re going to start the LR at 1e – 2, which is really high.
    However, you can set a scheduler to multiply the LR rate by a “gamma” amount every
    *n* epochs. So, for example, the following code will change it every 30 epochs.
    We’ll start at 0.01, and then, after the thirtieth epoch, it will multiply the
    LR by .1 to get 0.001\. After 60, it will be 0.0001, etc.:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你将学习率（LR）初始化为1e – 2，这实际上是非常高的。然而，你可以设置一个调度器，每经过*n*个epoch，将LR率乘以一个“gamma”值。例如，以下代码将在每30个epoch时改变学习率。我们将从0.01开始，然后在第三个epoch之后，将LR乘以0.1得到0.001。在60个epoch之后，它将是0.0001，等等：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When we run the scheduler like this, the performance improves a little—giving
    MAE of 4.36\.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们以这种方式运行调度器时，性能略有提高——给出MAE为4.36。
- en: You can continue to explore like this by tweaking the LR and also the size of
    the window—30 days of data to predict 1 day may not be enough, so you might want
    to try a window of 40 days. Also, try training for more epochs. With a bit of
    experimentation, you could get an MAE of close to 4, which isn’t bad.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调整LR和窗口大小继续探索，例如，用30天的数据预测1天可能不够，你可能想尝试一个40天的窗口。此外，尝试进行更多的epoch训练。通过一些实验，你可能会得到接近4的MAE，这并不差。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you took the statistical analysis of the time series from [Chapter 9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    and applied ML to try to do a better job of prediction. ML really is all about
    pattern matching, and, as expected, you were able to quickly create a deep neural
    network to spot the patterns with low error before exploring some hyperparameter
    tuning to improve the accuracy further.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你从[第9章](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)中学习了时间序列的统计分析，并应用机器学习（ML）来尝试提高预测的准确性。机器学习实际上就是关于模式匹配的，正如预期的那样，你能够迅速创建一个深度神经网络来识别低误差的模式，在探索一些超参数调整以进一步提高准确率之前。
- en: In [Chapter 11](ch11.html#ch11_using_convolutional_and_recurrent_methods_for_sequ_1748549734762226),
    you’ll go beyond a simple DNN and examine the implications of using an RNN to
    predict sequential values.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#ch11_using_convolutional_and_recurrent_methods_for_sequ_1748549734762226)中，你将超越简单的深度神经网络（DNN），并探讨使用循环神经网络（RNN）预测序列值的影响。
