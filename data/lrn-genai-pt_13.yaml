- en: 11 Building a generative pretrained Transformer from scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始构建生成式预训练变换器
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building a generative pretrained Transformer from scratch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建生成式预训练变换器
- en: Causal self-attention
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果自注意力
- en: Extracting and loading weights from a pretrained model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从预训练模型中提取和加载权重
- en: Generating coherent text with GPT-2, the predecessor of ChatGPT and GPT-4
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-2生成连贯的文本，ChatGPT和GPT-4的前辈
- en: Generative Pretrained Transformer 2 (GPT-2) is an advanced large language model
    (LLM) developed by OpenAI and announced in February 2019\. It represents a significant
    milestone in the field of natural language processing (NLP) and has paved the
    way for the development of even more sophisticated models, including its successors,
    ChatGPT and GPT-4\.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式预训练变换器2（GPT-2）是由OpenAI开发的高级大型语言模型（LLM），于2019年2月宣布发布。它在自然语言处理（NLP）领域取得了重大里程碑，并为开发更复杂的模型铺平了道路，包括其继任者ChatGPT和GPT-4。
- en: GPT-2, an improvement over its predecessor, GPT-1, was designed to generate
    coherent and contextually relevant text based on a given prompt, demonstrating
    a remarkable ability to mimic human-like text generation across various styles
    and topics. Upon its announcement, OpenAI initially decided not to release to
    the public the most powerful version of GPT-2 (also the one you’ll build from
    scratch in this chapter, with 1.5 billion parameters). The main concern was potential
    misuse, such as generating misleading news articles, impersonating individuals
    online, or automating the production of abusive or fake content. This decision
    sparked a significant debate within the AI and tech communities about the ethics
    of AI development and the balance between innovation and safety.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是其前辈GPT-1的改进，旨在根据给定的提示生成连贯且上下文相关的文本，展示了在多种风格和主题上模仿人类文本生成的非凡能力。在其宣布时，OpenAI最初决定不向公众发布GPT-2最强大的版本（也是本章中你将从头开始构建的，拥有150亿参数）。主要担忧是潜在的误用，例如生成误导性新闻文章、在线冒充个人或自动化生产侮辱性或虚假内容。这一决定在AI和科技社区中引发了关于AI开发伦理和创新与安全之间平衡的激烈辩论。
- en: OpenAI later adopted a staggered release strategy, gradually making smaller
    versions of the model available while monitoring the effect and exploring safe
    deployment strategies. Eventually, in November 2019, OpenAI released the full
    model, along with several datasets and a tool to detect model-generated text,
    contributing to discussions on responsible AI usage. Because of this release,
    you’ll learn to extract the pretrained weights from GPT-2 and load them to the
    GPT-2 model that you create.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI后来采用了分阶段发布策略，逐步使模型的小版本可用，同时监控效果并探索安全部署策略。最终，在2019年11月，OpenAI发布了完整模型，以及几个数据集和一个检测模型生成文本的工具，为负责任的AI使用讨论做出了贡献。正因为这次发布，你将学习如何从GPT-2中提取预训练权重并将它们加载到你创建的GPT-2模型中。
- en: GPT-2 is based on the Transformer architecture that we discussed in chapters
    9 and 10\. However, unlike the English-to-French translator you created before,
    GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the
    model. When translating an English phrase into French, the encoder captures the
    meaning of the English phrase and passes it to the decoder to generate the translation.
    However, in text generation tasks, the model does not need an encoder to understand
    a different language. Instead, it generates text based on the previous tokens
    in the sentence, using only a decoder-only architecture. Like other Transformer
    models, GPT-2 uses self-attention mechanisms to process input data in parallel,
    significantly improving the efficiency and effectiveness of training LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2基于我们在第9章和第10章讨论的变换器架构。然而，与之前创建的英法翻译器不同，GPT-2是一个仅解码器的变换器，这意味着模型中没有编码器堆栈。在将英语短语翻译成法语时，编码器捕捉英语短语的含义并将其传递给解码器以生成翻译。然而，在文本生成任务中，模型不需要编码器来理解不同的语言。相反，它仅使用解码器架构根据句子中的前一个标记生成文本。与其他变换器模型一样，GPT-2使用自注意力机制并行处理输入数据，显著提高了训练LLM的效率和效果。
- en: GPT-2 is pretrained on a large corpus of text data, essentially predicting the
    next word in a sentence given the words that precede it. This training enables
    the model to learn a wide range of language patterns, grammar, and knowledge.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2在大量文本数据语料库上进行了预训练，本质上是在给定句子中前一个词的情况下预测句子中的下一个词。这种训练使模型能够学习广泛的语言模式、语法和知识。
- en: In this chapter, you’ll learn to build GPT-2XL, the largest version of GPT-2,
    from scratch. After that, you’ll learn how to extract the pretrained weights from
    Hugging Face (an AI community that hosts and collaborates on machine learning
    models, datasets, and applications) and load them to your own GPT-2 model. You’ll
    use your GPT-2 to generate text by feeding a prompt to the model. GPT-2 calculates
    the probabilities of possible next tokens and samples from these probabilities.
    It can produce coherent and contextually relevant paragraphs of text based on
    the input prompt it receives. Additionally, as you did in chapter 8, you can control
    the creativeness of the generated text by using `temperature` and `top-K` sampling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将从零开始学习构建 GPT-2XL，这是 GPT-2 的最大版本。之后，你将学习如何从 Hugging Face（一个托管和协作机器学习模型、数据集和应用的
    AI 社区）中提取预训练的权重并将它们加载到自己的 GPT-2 模型中。你将通过向模型提供提示来使用你的 GPT-2 生成文本。GPT-2 计算可能下一个标记的概率并从中采样。它可以根据接收到的输入提示生成连贯且与上下文相关的段落文本。此外，正如你在第
    8 章中所做的那样，你可以通过使用 `temperature` 和 `top-K` 采样来控制生成文本的创造性。
- en: While GPT-2 marks a notable advance in NLP, it’s essential to moderate your
    expectations and recognize its inherent limitations. It’s crucial not to compare
    GPT-2 with ChatGPT or GPT-4 directly, as GPT-2XL has only 1.5 billion parameters
    compared to ChatGPT’s 175 billion and GPT-4’s estimated 1.76 trillion parameters.
    One of the main limitations of GPT-2 is its lack of genuine comprehension of the
    content it generates. The model predicts the next word in a sequence based on
    the probability distribution of words in its training data, which can produce
    syntactically correct and seemingly logical text. However, the model lacks a true
    understanding of the meaning behind the words, leading to potential inaccuracies,
    nonsensical statements, or superficial content.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GPT-2 在自然语言处理领域取得了显著的进步，但调整你的期望并认识到其固有的局限性是至关重要的。直接将 GPT-2 与 ChatGPT 或 GPT-4
    进行比较是不恰当的，因为 GPT-2XL 只有 15 亿个参数，而 ChatGPT 有 1750 亿个参数，GPT-4 的估计参数量为 1.76 万亿。GPT-2
    的主要局限性之一是它对其生成的内容的真正理解不足。该模型根据其训练数据中单词的概率分布预测序列中的下一个单词，这可以生成语法正确且看似合逻辑的文本。然而，该模型缺乏对词语背后含义的真正理解，可能导致潜在的不准确、无意义的陈述或肤浅的内容。
- en: Another key factor is GPT-2’s limited contextual awareness. While it can maintain
    coherence over short spans of text, it struggles with longer passages, potentially
    resulting in a loss of coherence, contradictions, or irrelevant content. We should
    be cautious not to overestimate the model’s ability to generate long-form content
    that requires sustained attention to context and detail. Therefore, while GPT-2
    represents a significant step forward in NLP, it’s important to approach its generated
    text with a healthy dose of skepticism and set realistic expectations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键因素是 GPT-2 的有限上下文意识。虽然它可以在短文本跨度内保持连贯性，但在较长的段落中会遇到困难，可能导致连贯性丧失、矛盾或不相关的内容。我们应谨慎不要高估模型生成需要持续关注上下文和细节的长篇内容的能力。因此，虽然
    GPT-2 在自然语言处理领域迈出了重要的一步，但以健康程度的怀疑态度对待其生成的文本并设定现实期望是非常重要的。
- en: 11.1 GPT-2 architecture and causal self-attention
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 GPT-2 架构和因果自注意力
- en: GPT-2 operates as a solely decoder-based Transformer (it generates text based
    on previous tokens in the sentence without the need for an encoder to understand
    a different language), mirroring the decoder component of the English-to-French
    translator discussed in chapters 9 and 10\. Unlike its bilingual counterpart,
    GPT-2 lacks an encoder and thus does not incorporate encoder-derived inputs in
    its output generation process. The model relies entirely on preceding tokens within
    the sequence to produce its output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 作为仅基于解码器的 Transformer（它根据句子中的先前标记生成文本，无需编码器理解不同语言），与第 9 章和第 10 章中讨论的英法翻译器的解码器组件相呼应。与它的双语版本不同，GPT-2
    缺少编码器，因此在输出生成过程中不包含编码器派生的输入。该模型完全依赖于序列中的先前标记来生成其输出。
- en: In this section, we’ll discuss the architecture of GPT-2\. We will also dive
    into the causal self-attention mechanism, which is the core of the GPT-2 model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 GPT-2 的架构。我们还将深入了解因果自注意力机制，这是 GPT-2 模型的核心。
- en: 11.1.1 The architecture of GPT-2
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 GPT-2 的架构
- en: 'GPT-2 comes in four different sizes: small (S), medium (M), large (L), and
    extra-large (XL), each varying in capability. Our primary focus will be on the
    most powerful version, GPT-2XL. The smallest GPT-2 model has around 124 million
    parameters, while the extra-large version has about 1.5 billion parameters. It
    is the most powerful among the GPT-2 models, with the highest number of parameters.
    GPT-2XL can understand complex contexts, generating coherent and nuanced text.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2有四种不同的尺寸：小（S）、中（M）、大（L）和超大（XL），每种尺寸的能力各不相同。我们的主要关注点将是功能最强大的版本，即GPT-2XL。最小的GPT-2模型大约有124百万个参数，而超大版本则有大约15亿个参数。它是GPT-2模型中最强大的，拥有最多的参数。GPT-2XL能够理解复杂语境，生成连贯且细腻的文本。
- en: GPT-2 consists of many identical decoder blocks. The extra-large version has
    48 decoder blocks, while the other three versions have 12, 24, and 36 decoder
    blocks, respectively. Each of these decoder blocks comprises two distinct sublayers.
    The first sublayer is a causal self-attention layer, which I’ll explain in detail
    soon. The second sublayer is a basic, position-wise, fully connected feed-forward
    network, as we have seen in the encoder and decoder blocks in the English-to-French
    translator. Each sublayer incorporates layer normalization and a residual connection
    to stabilize the training process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2由许多相同的解码器块组成。超大版本有48个解码器块，而其他三个版本分别有12、24和36个解码器块。每个解码器块由两个不同的子层组成。第一个子层是一个因果自注意力层，我将在不久的将来详细解释。第二个子层是一个基本的、位置相关的、全连接的前馈网络，正如我们在英语到法语翻译器中的编码器和解码器块中所看到的。每个子层都包含层归一化和残差连接，以稳定训练过程。
- en: Figure 11.1 is a diagram of the architecture of GPT-2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1是GPT-2架构的示意图。
- en: '![](../../OEBPS/Images/CH11_F01_Liu.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F01_Liu.png)'
- en: Figure 11.1 The architecture of the GPT-2 model. GPT-2 is a decoder-only Transformer,
    consisting of N identical decoder layers. Each decoder block contains two sublayers.
    The first sublayer is a causal self-attention layer. The second is a feed-forward
    network. Each sublayer uses layer normalization and a residual connection. The
    input is first passed through word embedding and positional encoding, and the
    sum is then passed through the decoder. The output from the decoder goes through
    layer normalization and a linear layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 GPT-2模型的架构。GPT-2是一个仅包含解码器的Transformer，由N个相同的解码器层组成。每个解码器块包含两个子层。第一个子层是一个因果自注意力层。第二个是一个前馈网络。每个子层都使用层归一化和残差连接。输入首先通过词嵌入和位置编码，然后将总和传递给解码器。解码器的输出经过层归一化和线性层。
- en: GPT-2 first passes indexes for a sequence of tokens through word embedding and
    positional encoding to obtain input embedding (I’ll explain soon how this process
    works). The input embedding is passed through N decoder blocks sequentially. After
    that, the output is passed through layer normalization and a linear layer. The
    number of outputs in GPT-2 is the number of unique tokens in the vocabulary (50,257
    tokens for all GPT-2 versions). The model is designed to predict the next token
    based on all previous tokens in the sequence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2首先将一系列标记的索引通过词嵌入和位置编码传递，以获得输入嵌入（我将在不久的将来解释这一过程）。输入嵌入依次通过N个解码器块。之后，输出通过层归一化和线性层。GPT-2的输出数量是词汇表中的唯一标记数量（所有GPT-2版本共有50,257个标记）。该模型旨在根据序列中的所有前一个标记预测下一个标记。
- en: To train GPT-2, OpenAI used a dataset called WebText, which was collected automatically
    from the internet. The dataset contained a wide variety of text, including websites
    like Reddit links that were highly upvoted, aiming to cover a broad spectrum of
    human languages and topics. This dataset is estimated to contain about 40GB of
    text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GPT-2，OpenAI使用了一个名为WebText的数据集，该数据集是从互联网上自动收集的。该数据集包含各种文本，包括Reddit链接等高度点赞的网站，旨在涵盖广泛的人类语言和主题。估计该数据集包含大约40GB的文本。
- en: The training data was broken into sequences of a fixed length (1,024 tokens
    for all GPT-2 versions) and used as inputs. The sequences were shifted to the
    right by one token and used as outputs to the model during training. Since the
    model uses causal self-attention, in which future tokens in a sequence are masked
    (i.e., hidden) during the training process, this is effectively training the model
    to predict the next token based on all previous tokens in the sequence.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据被分解成固定长度的序列（所有 GPT-2 版本的长度为 1,024 个标记）并用作输入。这些序列向右移动一个标记，并在训练过程中用作模型的输出。由于模型使用因果自注意力，其中序列中的未来标记在训练过程中被屏蔽（即隐藏），这实际上是在训练模型根据序列中所有之前的标记来预测下一个标记。
- en: 11.1.2 Word embedding and positional encoding in GPT-2
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 GPT-2 中的词嵌入和位置编码
- en: GPT-2 uses a subword tokenization method called the Byte Pair Encoder (BPE)
    to break text into individual tokens (whole words or punctuation marks in most
    cases but syllables for uncommon words). These tokens are then mapped into an
    index between 0 and 50,256 since the vocabulary size is 50,257\. GPT-2 transforms
    text in the training data into vector representations that capture its meaning
    through word embedding, similar to what you’ve done in the previous two chapters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 使用一种称为字节对编码器（Byte Pair Encoder，BPE）的子词分词方法将文本分解成单个标记（在大多数情况下是整个单词或标点符号，但对于不常见的单词则是音节）。这些标记随后被映射到
    0 到 50,256 之间的索引，因为词汇量大小为 50,257。GPT-2 将训练数据中的文本转换为通过词嵌入捕获其意义的向量表示，这与你在前两章中所做的方式类似。
- en: To give you a concrete example, the phrase “this is a prompt” is first converted
    into four tokens through BPE tokenization, `['this', ' is', ' a', ' prompt']`.
    Each token is then represented by a one-hot variable of size 50,257\. The GPT-2
    model passes them through a word embedding layer to compress them into condensed
    vectors with floating point values of a much smaller size, such as a length of
    1,600 in GPT-2XL (the lengths are 768, 1,024, and 1,280, for the other three versions
    of GPT-2, respectively). With word embedding, the phrase “this is a prompt” is
    represented by a matrix with size 4 × 1,600 instead of the original 4 × 50,257.
    Word embedding significantly reduces the number of the model’s parameters and
    makes training more efficient. The left side of figure 11.2 depicts how word embedding
    works.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个具体的例子，短语“this is a prompt”首先通过 BPE 分词转换为四个标记，`['this', ' is', ' a', '
    prompt']`。然后每个标记由一个大小为 50,257 的独热变量表示。GPT-2 模型将它们通过词嵌入层压缩成具有更小浮点值大小的压缩向量，例如 GPT-2XL
    中的长度为 1,600（其他三个版本的 GPT-2 的长度分别为 768、1,024 和 1,280）。通过词嵌入，短语“this is a prompt”被表示为一个
    4 × 1,600 大小的矩阵，而不是原始的 4 × 50,257。词嵌入显著减少了模型参数的数量，并使训练更加高效。图 11.2 的左侧展示了词嵌入的工作原理。
- en: '![](../../OEBPS/Images/CH11_F02_Liu.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F02_Liu.png)'
- en: Figure 11.2 GPT-2 first represents each token in a sequence with a 50,276-value
    one-hot vector. The token representation of the sequence goes through a word embedding
    layer to compress it into an embedding with a dimension of 1,600\. GPT-2 also
    represents each position in a sequence with a 1,024-value one-hot vector. The
    positional representation of the sequence goes through a positional encoding layer
    to compress it into an embedding also with a dimension of 1,600\. The word embedding
    and positional encoding are added together to form the input embedding.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 GPT-2 首先将序列中的每个标记表示为一个 50,276 位的独热向量。序列的标记表示通过词嵌入层压缩，形成一个维度为 1,600 的嵌入。GPT-2
    还使用一个 1,024 位的独热向量来表示序列中的每个位置。序列的位置表示通过位置编码层压缩，形成一个同样维度为 1,600 的嵌入。词嵌入和位置编码被相加以形成输入嵌入。
- en: GPT-2, like other Transformers, processes input data in parallel, and this inherently
    doesn’t allow it to recognize the sequence order of the input. To address this,
    we need to add positional encodings to the input embeddings. GPT-2 adopts a unique
    approach to positional encoding, diverging from the methodology outlined in the
    seminal 2017 “Attention Is All You Need” paper. Instead, GPT-2’s technique for
    positional encoding parallels that of word embeddings. Given the model’s capacity
    to handle up to 1,024 tokens in an input sequence, each position within the sequence
    is initially denoted by a one-hot vector of the same size. For instance, in the
    sequence “this is a prompt,” the first token is represented by a one-hot vector
    where all elements are zero except for the first, which is set to one. The second
    token follows suit, represented by a vector where all but the second element are
    zero. Consequently, the positional representation for the phrase “this is a prompt”
    manifests as a 4 × 1,024 matrix, as illustrated in the upper right section of
    figure 11.2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2，与其他Transformer类似，并行处理输入数据，这本质上导致它无法识别输入数据的序列顺序。为了解决这个问题，我们需要向输入嵌入中添加位置编码。GPT-2采用了一种独特的方法来进行位置编码，与2017年发表的具有里程碑意义的论文“Attention
    Is All You Need”中概述的方法不同。相反，GPT-2的位置编码技术与词嵌入相似。鉴于该模型能够处理输入序列中的最多1,024个标记，序列中的每个位置最初由一个同样大小的one-hot向量表示。例如，在序列“this
    is a prompt”中，第一个标记由一个one-hot向量表示，其中所有元素都是零，除了第一个，它被设置为1。第二个标记遵循同样的模式，由一个向量表示，其中除了第二个元素之外的所有元素都是零。因此，“this
    is a prompt”这个短语的序列表示表现为一个4 × 1,024的矩阵，如图11.2右上角所示。
- en: To generate positional encoding, the sequence’s positional representation undergoes
    processing through a linear neural network, which is dimensioned at 1,024 × 1,600.
    The weights within this network are randomly initialized and subsequently refined
    through the training process. As a result, the positional encoding for each token
    in the sequence is a 1,600-value vector, matching the dimension of the word embedding
    vector. A sequence’s input embedding is the sum of its word embedding and positional
    encoding, as depicted at the bottom of figure 11.2\. In the context of the phrase
    “this is a prompt,” both the word embedding and positional encoding are structured
    as 4 × 1,600 matrices. Therefore, the input embedding for “this is a prompt,”
    which is the sum of these two matrices, maintains a dimensionality of 4 × 1,600.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成位置编码，序列的位置表示通过一个维度为1,024 × 1,600的线性神经网络进行处理。该网络中的权重在初始化时是随机的，并在训练过程中进行优化。因此，序列中每个标记的位置编码是一个1,600维的向量，与词嵌入向量的维度相匹配。一个序列的输入嵌入是其词嵌入和位置编码的总和，如图11.2底部所示。在短语“this
    is a prompt”的上下文中，词嵌入和位置编码都结构化为4 × 1,600的矩阵。因此，“this is a prompt”的输入嵌入，即这两个矩阵的总和，保持了4
    × 1,600的维度。
- en: 11.1.3 Causal self-attention in GPT-2
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 GPT-2中的因果自注意力
- en: Causal self-attention is a crucial mechanism within the GPT-2 model (and broadly
    in the GPT series of models), enabling the model to generate text by conditioning
    on the sequence of previously generated tokens. It’s similar to the masked self-attention
    in the first sublayer of each decoder layer in the English-to-French translator
    we discussed in chapters 9 and 10, though the implementation differs slightly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力是GPT-2模型（以及在GPT系列模型中）中的一个关键机制，它使模型能够通过条件化先前生成的标记序列来生成文本。这与我们在第9章和第10章讨论的英语到法语翻译器中每个解码器层第一子层的掩码自注意力相似，尽管实现上略有不同。
- en: Note The concept of “causal” in this context refers to the model’s ability to
    ensure that predictions for a given token can only be influenced by the tokens
    that precede it in the sequence, respecting the causal (time-forward) direction
    of text generation. This is essential for generating coherent and contextually
    relevant text outputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在此上下文中，“因果”这一概念指的是模型确保对给定标记的预测只能受到序列中先于它的标记的影响，尊重文本生成的因果（时间向前）方向。这对于生成连贯且上下文相关的文本输出至关重要。
- en: Self-attention is a mechanism that allows each token in the input sequence to
    attend to all other tokens in the same sequence. In the context of Transformer
    models like GPT-2, self-attention enables the model to weigh the importance of
    other tokens when processing a specific token, thereby capturing the context and
    relationships between words in a sentence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是一种机制，允许输入序列中的每个标记关注同一序列中的所有其他标记。在GPT-2等Transformer模型的情况下，自注意力使模型能够在处理特定标记时权衡其他标记的重要性，从而捕捉句子中单词的上下文和关系。
- en: To ensure causality, GPT-2’s self-attention mechanism is modified so that any
    given token can only attend to itself and the tokens that have come before it
    in the sequence. This is achieved by masking future tokens (i.e., tokens that
    come after the current token in the sequence) in the attention calculation, ensuring
    that the model cannot “see” or be influenced by future tokens when predicting
    the next token in a sequence. For example, in the phrase “this is a prompt,” the
    mask hides the last three words in the first time step when the model uses the
    word “this” to predict the word “is.” To implement this, positions corresponding
    to future tokens are set to minus infinity when we compute the attention scores.
    After softmax activation, future tokens are allocated zero weights, effectively
    removing them from the attention calculation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保因果性，GPT-2的自注意力机制被修改，使得任何给定的标记只能关注它自己和序列中之前出现的标记。这是通过在注意力计算中屏蔽未来标记（即在序列中当前标记之后出现的标记）来实现的，确保模型在预测序列中的下一个标记时不能“看到”或受到未来标记的影响。例如，在短语“this
    is a prompt”中，当模型使用单词“this”来预测单词“is”时，掩码隐藏了第一次时间步中的最后三个单词。为了实现这一点，我们在计算注意力分数时将对应未来标记的位置设置为负无穷大。在softmax激活后，未来标记被分配零权重，从而有效地从注意力计算中移除。
- en: Let’s use a concrete example to illustrate exactly how causal self-attention
    works in code. The input embedding for the phrase “this is a prompt” is a 4 ×
    1,600 matrix after word embedding and positional encoding. We then pass this input
    embedding through N decoder layers in GPT-2\. In each decoder layer, it first
    goes through the causal self-attention sublayer as follows. The input embedding
    is passed through three neural networks to create query Q, key K, and value V,
    as shown in the following listing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个具体的例子来说明因果自注意力在代码中是如何工作的。短语“this is a prompt”的输入嵌入在词嵌入和位置编码之后是一个4 × 1,600的矩阵。然后我们通过GPT-2的N个解码器层传递这个输入嵌入。在每个解码器层中，它首先通过以下因果自注意力子层。输入嵌入通过三个神经网络传递以创建查询Q、键K和值V，如下所示。
- en: Listing 11.1 Creating `query`, `key`, and `value` vectors
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1 创建`query`、`key`和`value`向量
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Creates three neural networks
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建三个神经网络
- en: ② Creates an input embedding x
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建输入嵌入x
- en: ③ Passes the input embedding the three neural networks to create Q, K, and V
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入嵌入传递给三个神经网络以创建Q、K和V
- en: ④ Prints out the sizes of Q, K, and V
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印出Q、K和V的大小
- en: 'We first create a matrix with size 4 × 1,600, the same size as the input embedding
    for “this is a prompt”. We then pass the input embedding through three neural
    networks, each with a size of 1,600 × 1,600, to obtain query Q, key K, and value
    V. If you run the preceding code block, you’ll see the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个大小为4 × 1,600的矩阵，与“this is a prompt”的输入嵌入大小相同。然后我们通过三个大小为1,600 × 1,600的神经网络传递输入嵌入，以获得查询Q、键K和值V。如果您运行前面的代码块，您将看到以下输出：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The shapes of Q, K, and V are all 4 × 1,600\. Next, instead of using one head,
    we split them into 25 parallel heads. Each head pays attention to different parts
    or aspects of the input, enabling the model to capture a broader range of information
    and form a more detailed and contextual understanding of the input data. As a
    result, we have 25 sets of Q, K, and V:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Q、K和V的形状都是4 × 1,600。接下来，我们不是使用一个头，而是将它们分成25个并行头。每个头关注输入的不同部分或方面，使模型能够捕捉更广泛的信息，并对输入数据形成更详细和上下文化的理解。因此，我们有了25组Q、K和V：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Splits Q, K, and V into 25 heads
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将Q、K和V分为25个头
- en: ② Prints out the size of the multihead Q, K, and V
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ② 打印出多头Q、K和V的大小
- en: 'If you run the preceding code block, you’ll see the following output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行前面的代码块，您将看到以下输出：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The shapes of Q, K, and V are now 25 × 4 × 64: this means we have 25 heads;
    each head has a set of query, key, and value, all having a size of 4 × 64.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Q、K和V的形状是25 × 4 × 64：这意味着我们有25个头；每个头有一组查询、键和值，大小都是4 × 64。
- en: 'Next, we calculate the scaled attention scores in each head:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个头中的缩放注意力分数：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The scaled attention scores are the dot product of Q and K in each head, scaled
    by the square root of the dimension of K, which is 1,600/25 = 64. The scaled attention
    scores form a 4 × 4 matrix in each head, and we print out those in the first head:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后的注意力分数是每个头部中 Q 和 K 的点积，并按 K 的维度的平方根进行缩放，即 1,600/25 = 64。缩放后的注意力分数在每个头部形成一个
    4 × 4 矩阵，我们在第一个头部打印出这些值：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The scaled attention scores in the first head are also shown in the bottom left
    table in figure 11.3.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个头部的缩放注意力分数也显示在图 11.3 底部左边的表格中。
- en: Exercise 11.1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.1
- en: The tensor `scaled_att` contains the scaled attention scores in the 25 heads.
    We have printed out those in the first head previously. How do you print out the
    scaled attention scores in the second head?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 张量 `scaled_att` 包含 25 个头部中的缩放注意力分数。我们之前已经打印出了第一个头部的这些值。你是如何打印出第二个头部的缩放注意力分数的？
- en: 'Next, we apply a mask to the scaled attention scores to hide future tokens
    in the sequence:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对缩放后的注意力分数应用一个掩码，以隐藏序列中的未来标记：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Creates a mask
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个掩码
- en: ② Applies the mask on the scaled attention scores by changing the values to
    –∞ for future tokens
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过将未来标记的值更改为 –∞ 来对缩放后的注意力分数应用掩码
- en: '![](../../OEBPS/Images/CH11_F03_Liu.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH11_F03_Liu.png)'
- en: Figure 11.3 How to calculate masked attention weights in causal self-attention.
    A mask is applied to the scaled attention scores so that values corresponding
    to future tokens (those above the main diagonal in the matrix) become –∞. We then
    apply the softmax function on the masked scaled attention scores and obtain the
    masked attention weights. The masking ensures that predictions for a given token
    can only be influenced by the tokens that precede it in the sequence, not by future
    tokens. This is essential for generating coherent and contextually relevant text
    outputs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 如何在因果自注意力中计算掩码注意力权重。掩码应用于缩放后的注意力分数，使得对应未来标记的值（矩阵中主对角线以上的值）变为 –∞。然后我们对掩码后的缩放注意力分数应用
    softmax 函数，从而获得掩码注意力权重。掩码确保给定标记的预测只能受到序列中先于它的标记的影响，而不是未来标记的影响。这对于生成连贯且上下文相关的文本输出至关重要。
- en: 'If you run the preceding code, you’ll see the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码，你会看到以下输出：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The mask is a 4 × 4 matrix as shown at the top of figure 11.3\. The lower half
    of the mask (values below the main diagonal) are 1s while the upper half of the
    mask (values above the main diagonal) are 0s. When this mask is applied to the
    scaled attention scores, the values in the upper half of the matrix become –∞
    (the middle bottom of figure 11.3). This way, when we apply the softmax function
    on the scaled attention scores, the upper half of the attention weights matrix
    is filled with 0s (bottom right of figure 11.3):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该掩码是一个 4 × 4 矩阵，如图 11.3 顶部所示。掩码的下半部分（主对角线以下的值）为 1，而掩码的上半部分（主对角线以上的值）为 0。当这个掩码应用于缩放后的注意力分数时，矩阵上半部分的值变为
    –∞（图 11.3 中间底部）。这样，当我们对缩放后的注意力分数应用 softmax 函数时，注意力权重矩阵的上半部分被填充为 0（图 11.3 右下角）：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We print out the attention weights in the first head with the following values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下值打印出第一个头部的注意力权重：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first row means in the first time step, the token “this” attends only to
    itself and not to any future tokens. Similarly, if you look at the second row,
    the tokens “this is” attend to each other but not to future tokens “a prompt”.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行表示在第一个时间步，标记“this”只关注自身，而不关注任何未来的标记。同样，如果你看第二行，标记“this is”相互关注，但不关注未来的标记“a
    prompt”。
- en: Note The weights in this numerical example are not trained, so don’t take these
    values in attention weights literally. We use them as an example to illustrate
    how causal self-attention works.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这个数值示例中，权重未经过训练，所以不要将这些值直接理解为注意力权重。我们使用它们作为示例来说明因果自注意力是如何工作的。
- en: Exercise 11.2
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.2
- en: We have printed out the attention weights in the first head. How do you print
    out the attention weights in the last (i.e., the 25^(th)) head?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经打印出了第一个头部的注意力权重。你是如何打印出最后一个（即第 25 个）头部的注意力权重的？
- en: 'Finally, we calculate the attention vector in each head as the dot product
    of attention weights and the value vector. The attention vectors in the 25 heads
    are then joined together as one single attention vector:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算每个头部的注意力向量，它是注意力权重和值向量的点积。然后，将 25 个头部的注意力向量合并为一个单一的注意力向量：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output is
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The final output after causal self-attention is a 4 × 1,600 matrix, the same
    size as the input to the causal self-attention sublayer. The decoder layers are
    designed in such a way that the input and output have the same dimensions, and
    this allows us to stack many decoder layers together to increase the representation
    capacity of the model and to enable hierarchical feature extraction during training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力机制后的最终输出是一个4×1,600的矩阵，与因果自注意力子层的输入大小相同。解码器层被设计成输入和输出具有相同的维度，这使得我们可以堆叠许多解码器层来增加模型的表示能力，并在训练期间实现层次特征提取。
- en: 11.2 Building GPT-2XL from scratch
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 从头构建GPT-2XL
- en: Now that you understand the architecture of GPT-2 and how its core ingredient,
    causal self-attention, functions, let’s create the largest version of GPT-2 from
    scratch.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了GPT-2的架构以及其核心成分因果自注意力机制的工作原理，让我们从头开始创建GPT-2的最大版本。
- en: In this section, you’ll first learn to use the subword tokenization method in
    GPT-2, the byte pair encoder (BPE) tokenizer, to break text into individual tokens.
    You’ll also learn the GELU activation function used in the feed-forward network
    in GPT-2\. After that, you’ll code in the causal self-attention mechanism and
    combine it with a feed-forward network to form a decoder block. Finally, you’ll
    stack 48 decoder blocks to create the GPT-2XL model. The code in this chapter
    is adapted from the excellent GitHub repository by Andrej Kaparthy ([https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)).
    I encourage you to read through the repository if you want to dive deeper into
    how GPT-2 works.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将首先学习使用GPT-2中的子词分词方法，即字节对编码器（BPE）分词器，将文本分解成单个标记。你还将学习GPT-2中前馈网络使用的GELU激活函数。之后，你将编写因果自注意力机制，并将其与前馈网络结合形成一个解码器块。最后，你将堆叠48个解码器块来创建GPT-2XL模型。本章的代码改编自Andrej
    Kaparthy的优秀GitHub仓库（[https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)）。如果你想要深入了解GPT-2的工作原理，我鼓励你阅读该仓库。
- en: 11.2.1 BPE tokenization
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 BPE分词
- en: GPT-2 uses a subword tokenization method called byte pair encoder (BPE), which
    is a data compression technique that has been adapted for use in tokenizing text
    in NLP tasks. It’s particularly well-known for its application in training LLMs,
    such as the GPT series and BERT (Bidirectional Encoder Representations from Transformers).
    The primary goal of BPE is to encode a piece of text into a sequence of tokens
    in a way that balances the vocabulary size and the length of the tokenized text.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2使用一种称为字节对编码器（BPE）的子词分词方法，这是一种数据压缩技术，已被改编用于NLP任务中的文本分词。它因在训练LLM（如GPT系列和BERT）中的应用而特别知名。BPE的主要目标是以一种平衡词汇量和分词文本长度的方法将一段文本编码成一系列标记。
- en: BPE operates by iteratively merging the most frequent pair of consecutive characters
    in a dataset into a single new token, subject to certain conditions. This process
    is repeated until a desired vocabulary size is reached or no more merges are beneficial.
    BPE allows for an efficient representation of text, balancing between character-level
    and word-level tokenization. It helps to reduce the vocabulary size without significantly
    increasing the sequence length, which is crucial for the performance of NLP models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: BPE通过迭代地将数据集中最频繁出现的连续字符对合并成一个新的标记来工作，前提是满足某些条件。这个过程会重复进行，直到达到所需的词汇量或没有更多的合并是有益的。BPE允许对文本进行高效表示，在字符级和词级分词之间取得平衡。它有助于在不显著增加序列长度的同时减少词汇量，这对于NLP模型的性能至关重要。
- en: We discussed the pros and cons of the three types of tokenization methods (character-level,
    word-level, and subword tokenizations) in chapter 8\. Further, you implemented
    a word-level tokenizer from scratch in chapter 8 (and will do so again in chapter
    12). Therefore, in this chapter, we’ll borrow the tokenization method from OpenAI
    directly. The detailed workings of BPE are beyond the scope of this book. All
    you need to know is that it first converts text into subword tokens and then the
    corresponding indexes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第8章讨论了三种类型分词方法的优缺点（字符级、词级和子词分词）。此外，你还在第8章从头实现了词级分词器（并在第12章再次这样做）。因此，在本章中，我们将直接借用OpenAI的分词方法。BPE的详细工作原理超出了本书的范围。你需要知道的是，它首先将文本转换为子词标记，然后是相应的索引。
- en: Download the file `bpe.py` from Andrej Karpathy’s GitHub repository, [https://mng.bz/861B](https://mng.bz/861B),
    and place the file in the folder /utils/ on your computer. We’ll use the file
    as a local module in this chapter. As Andrej Karpathy explained in his GitHub
    repository, the module is based on OpenAI’s implementation at [https://mng.bz/EOlj](https://mng.bz/EOlj)
    but was mildly modified to make it easier to understand.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从安德烈·卡帕西（Andrej Karpathy）的GitHub仓库下载文件`bpe.py`，[https://mng.bz/861B](https://mng.bz/861B)，并将其放置在您的计算机上的/utils/文件夹中。在本章中，我们将使用该文件作为本地模块。正如安德烈·卡帕西在他的GitHub仓库中解释的那样，该模块基于OpenAI的实现[https://mng.bz/EOlj](https://mng.bz/EOlj)，但进行了轻微修改，使其更容易理解。
- en: 'To see how the module `bpe.py` converts text into tokens and then indexes,
    let’s try an example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解模块`bpe.py`如何将文本转换为标记然后转换为索引，让我们尝试一个示例：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① The text for an example sentence
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 示例句子的文本
- en: ② Instantiates the get_encoder() class from the bpe.py module
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从bpe.py模块实例化get_encoder()类
- en: ③ Tokenizes the example text and print out the tokens
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 分词示例文本并打印出标记
- en: The output is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The BPE tokenizer splits the example text “This is the original text.” into
    six tokens as shown in the preceding output. Note that the BPE tokenizer doesn’t
    convert uppercase letters to lowercase ones. This leads to more meaningful tokenization
    but also a much larger number of unique tokens. In fact, all versions of GPT-2
    models have a vocabulary size of 50,276, several times larger than the vocabulary
    size in the previous chapters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: BPE分词器将示例文本“这是原始文本。”分割成六个标记，如前述输出所示。请注意，BPE分词器不会将大写字母转换为小写字母。这导致更具有意义的分词，但也导致独特的标记数量大大增加。实际上，所有版本的GPT-2模型词汇量大小为50,276，比前几章的词汇量大几倍。
- en: 'We can also use the module `bpe.py` to map tokens to indexes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`bpe.py`模块将标记映射到索引：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output is
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding list contains the six indexes corresponding to the six tokens
    in the example text “This is the original text.”
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表包含对应于示例文本“这是原始文本。”中的六个标记的六个索引。
- en: 'We can also restore the text based on the indexes:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以根据索引恢复文本：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Instantiates the BPETokenizer() class from the bpe.py module
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从bpe.py模块实例化BPETokenizer()类
- en: ② Uses the tokenizer to restore text based on indexes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用分词器根据索引恢复文本
- en: The output from the preceding code block is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码块输出的结果为
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, the BPE tokenizer has restored the example text to its original
    form.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，BPE分词器已将示例文本恢复到其原始形式。
- en: Exercise 11.3
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 练习11.3
- en: Use the BPE tokenizer to split the phrase “this is a prompt” into tokens. After
    that, map the tokens to indexes. Finally, restore the phrase based on the indexes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BPE分词器将短语“this is a prompt”分割成标记。之后，将标记映射到索引。最后，根据索引恢复短语。
- en: 11.2.2 The Gaussian error linear unit activation function
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 高斯误差线性单元激活函数
- en: The Gaussian error linear unit (GELU) activation function is used in the feed-forward
    sublayers of each decoder block in GPT-2\. GELU provides a blend of linear and
    nonlinear activation properties that have been found to enhance model performance
    in deep learning tasks, particularly NLP.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯误差线性单元（GELU）激活函数用于GPT-2中每个解码器块的馈送前子层。GELU提供了一种线性和非线性激活特性的混合，这在深度学习任务中已被发现可以增强模型性能，尤其是在NLP领域。
- en: 'GELU offers a nonlinear, smooth curve that allows for more nuanced adjustments
    during training compared to other functions like the rectified linear unit (ReLU).
    This smoothness helps in optimizing the neural network more effectively, as it
    provides a more continuous gradient for backpropagation. To compare GELU with
    ReLU, our go-to activation function, let’s first define a GELU() class:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GELU提供了一个非线性、平滑的曲线，与像ReLU这样的其他函数相比，在训练期间允许进行更细微的调整。这种平滑性有助于更有效地优化神经网络，因为它为反向传播提供了更连续的梯度。为了比较GELU与我们的首选激活函数ReLU，我们首先定义一个GELU()类：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The ReLU function is not differentiable everywhere since it has a kink in it.
    The GELU activation function, in contrast, is differentiable everywhere and provides
    a better learning process. Next we draw a picture of the GELU activation function
    and compare it to ReLU.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数在它有尖角的地方不可微分。相比之下，GELU激活函数在所有地方都是可微分的，并提供了一个更好的学习过程。接下来，我们绘制GELU激活函数的图像，并与ReLU进行比较。
- en: 'Listing 11.2 Comparing two activation functions: GELU and ReLU'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.2 比较两个激活函数：GELU和ReLU
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Defines a function to represent ReLU
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个表示ReLU的函数
- en: ② Plots the ReLU activation function in solid lines
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ② 用实线绘制ReLU激活函数
- en: ③ Plots the GELU activation function in dashed lines
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 用虚线绘制GELU激活函数
- en: If you run the preceding code block, you’ll see a graph as shown in figure 11.4.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码块，你会看到一个如图11.4所示的图形。
- en: '![](../../OEBPS/Images/CH11_F04_Liu.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH11_F04_Liu.png)'
- en: Figure 11.4 Comparing the GELU activation function with ReLU. The solid line
    is the ReLU activation function, while the dashed line is the GELU activation
    function. ReLU is not differentiable everywhere since there is a kink in it. GELU,
    in contrast, is differentiable everywhere. This smoothness in GELU helps to optimize
    the neural network more effectively, as it provides a more continuous gradient
    for backpropagation during the training process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 比较GELU激活函数与ReLU。实线是ReLU激活函数，而虚线是GELU激活函数。ReLU在某个地方有拐角，因此不是处处可导。相比之下，GELU在所有地方都是可导的。GELU的这种平滑性有助于更有效地优化神经网络，因为它在训练过程中为反向传播提供了更连续的梯度。
- en: Furthermore, the formulation of GELU allows it to model input data distributions
    more effectively. It combines the properties of linear and Gaussian distribution
    modeling, which can be particularly beneficial for the complex, varied data encountered
    in NLP tasks. This capability helps in capturing subtle patterns in language data,
    improving the model’s understanding and generation of text.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GELU公式的制定使其能够更有效地模拟输入数据分布。它结合了线性和高斯分布建模的特性，这对于在NLP任务中遇到的复杂、多变的数据特别有益。这种能力有助于捕捉语言数据中的微妙模式，提高模型对文本的理解和生成。
- en: 11.2.3 Causal self-attention
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 因果自注意力
- en: As we explained earlier, causal self-attention is the core element in GPT-2
    models. Next, we’ll implement this mechanism from scratch in PyTorch.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所解释的，因果自注意力是GPT-2模型的核心元素。接下来，我们将从头开始在PyTorch中实现这一机制。
- en: We first specify the hyperparameters in the GPT-2XL model that we’ll build in
    this chapter. To that end, we define a `Config()` class with the values shown
    in the following listing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先指定本章将要构建的GPT-2XL模型中的超参数。为此，我们定义了一个`Config()`类，其值如下所示。
- en: Listing 11.3 Specifying hyperparameters in GPT-2XL
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3 在GPT-2XL中指定超参数
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Defines a Config() class
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个Config()类
- en: ② Places model hyperparameters as attributes in the class
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将模型超参数作为类的属性放置
- en: ③ Instantiates the Config() class
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化Config()类
- en: 'We define a `Config()` class and create several attributes in it to be used
    as the hyperparameters in the GPT-2XL model. The `n_layer` attribute means the
    GPT-2XL model we construct will have 48 decoder layers (we use the terms “decoder
    block” and “decoder layer” interchangeably). The `n_head` attribute means we’ll
    split Q, K, and V into 25 parallel heads when calculating causal self-attention.
    The `n_embd` attribute means the embedding dimension is 1,600: each token will
    be represented by a 1,600-value vector. The `vocab_size` attribute means there
    are 50,257 unique tokens in the vocabulary. The `block_size` attribute means the
    input sequence to the GPT-2XL model contains at most 1,024 tokens. The dropout
    rates are all set to 0.1\.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`Config()`类，并在其中创建了一些属性，用作GPT-2XL模型中的超参数。`n_layer`属性表示我们构建的GPT-2XL模型将包含48个解码器层（我们使用“解码器块”和“解码器层”这两个术语可以互换）。`n_head`属性表示在计算因果自注意力时，我们将Q、K和V分割成25个并行头。`n_embd`属性表示嵌入维度是1,600：每个标记将由一个1,600值的向量表示。`vocab_size`属性表示词汇表中有50,257个独特的标记。`block_size`属性表示输入到GPT-2XL模型中的序列最多包含1,024个标记。dropout率都设置为0.1。
- en: In the last section, I explained in detail how causal self-attention works.
    Next, we define a `CausalSelfAttention()` class to implement it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我详细解释了因果自注意力是如何工作的。接下来，我们将定义一个`CausalSelfAttention()`类来实现它。
- en: Listing 11.4 Implementing causal self-attention
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.4 实现因果自注意力
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Creates a mask and registers it as a buffer since it doesn’t need to be updated
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个掩码并将其注册为缓冲区，因为它不需要更新
- en: ② Passes input embedding through three neural networks to obtain Q, K, and V
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将输入嵌入通过三个神经网络传递以获得Q、K和V
- en: ③ Splits Q, K, and V into multiple heads
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将Q、K和V分割成多个头
- en: ④ Calculates masked attention weights in each head
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算每个头的掩码注意力权重
- en: ⑤ Concatenates attention vectors in all heads into one single attention vector
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将所有头的注意力向量连接成一个单一的注意力向量
- en: 'In PyTorch, `register_buffer` is a method used to register a tensor as a buffer.
    Variables in a buffer are not considered learnable parameters of the model; hence
    they are not updated during backpropagation. In the preceding code block, we have
    created a mask and registered it as a buffer. This has implications for how we
    extract and load model weights later: we’ll omit the masks when retrieving weights
    from GPT-2XL.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '在PyTorch中，`register_buffer`是一种将张量注册为缓冲区的方法。缓冲区中的变量不被视为模型的可学习参数；因此，它们在反向传播期间不会被更新。在前面的代码块中，我们创建了一个掩码并将其注册为缓冲区。这会影响我们稍后提取和加载模型权重的方式：在从GPT-2XL检索权重时，我们将省略掩码。 '
- en: As we explained in the first section, the input embedding is passed through
    three neural networks to obtain query Q, key K, and value V. We then split them
    into 25 heads and calculate masked self-attention in each head. After that, we
    join the 25 attention vectors back into one single attention vector, which is
    the output of the previous `CausalSelfAttention()` class.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一节中解释的，输入嵌入通过三个神经网络来获取查询Q、键K和值V。然后我们将它们分成25个头，并在每个头中计算掩码自注意力。之后，我们将25个注意力向量重新组合成一个单一的注意力向量，这是前一个`CausalSelfAttention()`类的输出。
- en: 11.2.4 Constructing the GPT-2XL model
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.4 构建GPT-2XL模型
- en: Next, we add a feed-forward network to the causal self-attention sublayer to
    form a decoder block, as follows.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在因果自注意力子层中添加一个前馈网络，以形成一个解码器块，如下所示。
- en: Listing 11.5 Constructing a decoder block
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.5 构建解码器块
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Initiates the Block() class
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化Block()类
- en: ② The first sublayer in the block is the causal self-attention sublayer, with
    layer normalization and residual connection.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ② 块中的第一个子层是因果自注意力子层，包含层归一化和残差连接。
- en: ③ The second sublayer in the block is a feed-forward network, with GELU activation,
    layer normalization, and residual connection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 块中的第二个子层是一个前馈网络，包含GELU激活、层归一化和残差连接。
- en: Every decoder block is composed of two sublayers. The first sublayer is the
    causal self-attention mechanism, with the integration of layer normalization and
    residual connection. The second sublayer within the decoder block is the feed-forward
    network, which incorporates the GELU activation function, alongside layer normalization
    and residual connection.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解码器块由两个子层组成。第一个子层是因果自注意力机制，包含层归一化和残差连接。解码器块内的第二个子层是前馈网络，它结合了GELU激活函数，以及层归一化和残差连接。
- en: We stack 48 decoder layers to form the main body of the GPT-2XL model, as shown
    in the following listing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们堆叠48个解码器层来形成GPT-2XL模型的主体，如下所示。
- en: Listing 11.6 Building the GPT-2XL model
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.6 构建GPT-2XL模型
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Calculates input embedding as the sum of word embedding and positional encoding
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ① 计算输入嵌入为词嵌入和位置编码之和
- en: ② Passes the input embedding through 48 decoder blocks
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将输入嵌入通过48个解码器块
- en: ③ Applies layer normalization one more time
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 再次应用层归一化
- en: ④ Attaches a linear head to the output so the number of outputs equals the number
    of unique tokens
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将线性头附加到输出上，使得输出的数量等于唯一标记的数量
- en: We construct the model in the `GPT2XL()` class as we explained in the first
    section of this chapter. The input to the model consists of sequences of indexes
    corresponding to tokens in the vocabulary. We first pass the input through word
    embedding and positional encoding; we then add the two to form the input embedding.
    The input embedding goes through 48 decoder blocks. After that, we apply layer
    normalization to the output and then attach a linear head to it so that the number
    of outputs is 50,257, the size of the vocabulary. The outputs are the logits corresponding
    to the 50,257 tokens in the vocabulary. Later, we’ll apply the softmax activation
    on the logits to obtain the probability distribution over the unique tokens in
    the vocabulary when generating text.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的第一节中解释了如何在`GPT2XL()`类中构建模型。模型的输入由对应于词汇表中标记的索引序列组成。我们首先将输入通过词嵌入和位置编码；然后我们将这两个嵌入相加形成输入嵌入。输入嵌入经过48个解码器块。之后，我们对输出应用层归一化，然后附加一个线性头，使得输出的数量为50,257，即词汇表的大小。输出是词汇表中50,257个标记的对数几率。稍后，我们将对对数几率应用softmax激活函数，以获得生成文本时词汇表中唯一标记的概率分布。
- en: NOTE Since the model size is too large, we didn’t move the model to a GPU. This
    leads to a lower speed in text generation later in the chapter. However, if you
    have access to a CUDA-enabled GPU with large memory (say, above 32GB), you can
    move the model to a GPU for faster text generation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于模型太大，我们没有将其移动到 GPU 上。这导致本章后面文本生成的速度较低。然而，如果你有访问带有大内存（例如，超过 32GB）的 CUDA
    启用 GPU 的权限，你可以将模型移动到 GPU 上以实现更快的文本生成。
- en: 'Next, we’ll create the GPT-2XL model by instantiating the `GPT2XL()` class
    we defined earlier:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过实例化我们之前定义的 `GPT2XL()` 类来创建 GPT-2XL 模型：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We also count the number of parameters in the main body of the model. The output
    is
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计算模型主体中的参数数量。输出结果是
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding output shows that GPT-2XL has more than 1.5 billion parameters.
    Note that the number doesn’t include the parameters in the linear head at the
    end of the model. Depending on what the downstream task is, we can attach different
    heads to the model. Since our focus is on text generation, we have attached a
    linear head to ensure the number of outputs is equal to the number of unique tokens
    in the vocabulary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示 GPT-2XL 有超过 15 亿个参数。请注意，这个数字不包括模型末尾线性头部的参数。根据下游任务的不同，我们可以将不同的头部附加到模型上。由于我们的重点是文本生成，我们附加了一个线性头部以确保输出的数量等于词汇表中的唯一标记数量。
- en: NOTE In LLMs like GPT-2, ChatGPT, or BERT, an output head refers to the final
    layer of the model that is responsible for producing the actual output based on
    the processed input. This output can vary depending on the downstream task the
    model is performing. In text generation, the output head is often a linear layer
    that transforms the final hidden states into logits for each token in the vocabulary.
    These logits are then passed through a softmax function to generate a probability
    distribution over the vocabulary, which is used to predict the next token in a
    sequence. For classification tasks, the output head typically consists of a linear
    layer followed by a softmax function. The linear layer transforms the final hidden
    states of the model into logits for each class, and the softmax function converts
    these logits into probabilities for each class. The specific architecture of the
    output head can vary depending on the model and the task, but its primary function
    is to map the processed input to the desired output format (e.g., class probabilities,
    token probabilities, etc.).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在 GPT-2、ChatGPT 或 BERT 等大型语言模型中，输出头部指的是模型中负责根据处理后的输入产生实际输出的最后一层。这个输出会根据模型执行的任务而变化。在文本生成中，输出头部通常是一个线性层，它将最终的隐藏状态转换为词汇表中每个标记的
    logits。这些 logits 然后通过 softmax 函数生成词汇表上的概率分布，用于预测序列中的下一个标记。对于分类任务，输出头部通常由一个线性层和一个
    softmax 函数组成。线性层将模型的最终隐藏状态转换为每个类别的 logits，softmax 函数将这些 logits 转换为每个类别的概率。输出头部的具体架构可能因模型和任务而异，但其主要功能是将处理后的输入映射到所需的输出格式（例如，类别概率、标记概率等）。
- en: 'Finally, you can print out the GPT-2XL model structure:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以打印出 GPT-2XL 模型的结构：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output is
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It shows the detailed blocks and layers in the GPT-2XL model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示了 GPT-2XL 模型中的详细块和层。
- en: And just like that, you have created the GPT-2XL model from scratch!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你从头开始创建了 GPT-2XL 模型！
- en: 11.3 Loading up pretrained weights and generating text
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 加载预训练权重并生成文本
- en: Even though you have just created the GPT-2XL model, it is not trained. Therefore,
    you cannot use it to generate any meaningful text.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你刚刚创建了 GPT-2XL 模型，但它尚未经过训练。因此，你不能用它生成任何有意义的文本。
- en: Given the sheer number of the model’s parameters, it’s impossible to train the
    model without supercomputing facilities, let alone the amount of data needed to
    train the model. Luckily, the pretrained weights of GPT-2 models, including the
    largest one, GPT-2XL, were released by OpenAI to the public on November 5, 2019
    (see the statement on the OpenAI website, [https://openai.com/research/gpt-2-1-5b-release](https://openai.com/research/gpt-2-1-5b-release),
    as well as a report by an American technology news website, The Verge, [https://mng.bz/NBm7](https://mng.bz/NBm7)).
    We, therefore, will load up the pretrained weights to generate text in this section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型参数数量庞大，没有超级计算设施就无法训练模型，更不用说训练模型所需的数据量了。幸运的是，包括最大的 GPT-2 模型 GPT-2XL 在内的预训练权重于
    2019 年 11 月 5 日由 OpenAI 向公众发布（参见 OpenAI 网站上的声明，[https://openai.com/research/gpt-2-1-5b-release](https://openai.com/research/gpt-2-1-5b-release)，以及一家美国科技新闻网站
    The Verge 的报道，[https://mng.bz/NBm7](https://mng.bz/NBm7)）。因此，我们将加载预训练权重以在本节中生成文本。
- en: 11.3.1 Loading up pretrained parameters in GPT-2XL
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 加载 GPT-2XL 的预训练参数
- en: We’ll use the `transformers` library developed by the Hugging Face team to extract
    pretrained weights in GPT-2XL.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Hugging Face 团队开发的 `transformers` 库来提取 GPT-2XL 中的预训练权重。
- en: 'First, run the following line of code in a new cell in this Jupyter Notebook
    to install the `transformers` library on your computer:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在 Jupyter Notebook 的新单元中运行以下代码行以在您的计算机上安装 `transformers` 库：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we import the GPT2 model from the `transformers` library and extract
    the pretrained weights in GPT-2XL:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 `transformers` 库中导入 GPT2 模型并提取 GPT-2XL 中的预训练权重：
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ① Loads the pretrained GPT-2XL model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载预训练的 GPT-2XL 模型
- en: ② Extracts model weights
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取模型权重
- en: ③ Prints out the model structure of the original OpenAI GTP-2XL model
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印出原始 OpenAI GTP-2XL 模型的模型结构
- en: The output from the preceding code block is
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码块输出的结果是
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① OpenAI used a Conv1d layer instead of a linear layer as we did
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ① OpenAI 使用了 Conv1d 层而不是我们使用的线性层
- en: If you compare this model structure with the one from the previous section,
    you’ll notice that they are the same except that the linear layers are replaced
    with Conv1d layers. As we explained in chapters 9 and 10, in feed-forward networks,
    we treat values in an input as independent elements rather than a sequence. Therefore,
    we often call it a 1D convolutional network. OpenAI checkpoints use a Conv1d module
    in places of the model where we use a linear layer. As a result, we need to transpose
    certain weight matrices when we extract model weights from Hugging Face and place
    them in our model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个模型结构与上一节中的模型结构进行比较，你会注意到它们是相同的，只是线性层被 Conv1d 层所取代。正如我们在第 9 章和第 10 章中解释的，在前馈网络中，我们将输入中的值视为独立的元素，而不是一个序列。因此，我们通常称它为一维卷积网络。OpenAI
    检查点在模型中使用线性层的地方使用了 Conv1d 模块。因此，当我们从 Hugging Face 提取模型权重并将其放置在我们的模型中时，我们需要转置某些权重矩阵。
- en: 'To understand how this works, let’s look at the weights in the first layer
    of the feed-forward network in the first decoder block of the OpenAI GPT-2XL model.
    We can print out its shape as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解它是如何工作的，让我们看看 OpenAI GPT-2XL 模型第一个解码块中前馈网络第一层的权重。我们可以按以下方式打印出其形状：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output is
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The weight matrix in the Conv1d layer is a tensor with size (1,600, 6,400).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Conv1d 层中的权重矩阵是一个大小为 (1,600, 6,400) 的张量。
- en: Now, if we look at the same weight matrix in the model we just constructed,
    its shape is
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们看看我们刚刚构建的模型中相同的权重矩阵，其形状是
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output this time is
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这次的输出结果是
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The weight matrix in the linear layer in our model is a tensor with size (6,400,
    1,600), which is a transposed matrix of the weight matrix in OpenAI GPT-2XL. Therefore,
    we need to transpose the weight matrix in all Conv1d layers in the OpenAI GPT-2XL
    model before we place them in our model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中线性层的权重矩阵是一个大小为 (6,400, 1,600) 的张量，它是 OpenAI GPT-2XL 权重矩阵的转置矩阵。因此，在我们将权重矩阵放置在我们的模型之前，我们需要将
    OpenAI GPT-2XL 模型中所有 Conv1d 层的权重矩阵进行转置。
- en: 'Next, we name the parameters in the original OpenAI GPT-2XL model as `keys`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将原始 OpenAI GPT-2XL 模型中的参数命名为 `keys`：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that we have excluded parameters ending with `attn.masked_bias` in the
    preceding line of code. OpenAI GPT-2 uses them to implement future token masking.
    Since we have created our own masking in the `CausalSelfAttention()` class and
    registered it as a buffer in PyTorch, we don’t need to load parameters ending
    with `attn.masked_bias` from OpenAI.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在上一行代码中排除了以`attn.masked_bias`结尾的参数。OpenAI GPT-2使用它们来实现未来标记的掩码。由于我们在`CausalSelfAttention()`类中创建了我们的掩码并将其注册为PyTorch中的缓冲区，因此我们不需要从OpenAI加载以`attn.masked_bias`结尾的参数。
- en: 'We name the parameters in the GPT-2XL model we created from scratch as `sd`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头创建的GPT-2XL模型中的参数命名为`sd`：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we’ll extract the pretrained weights in OpenAI GPT-2XL and place them
    in our own model:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从OpenAI GPT-2XL中提取预训练权重并将它们放置到我们自己的模型中：
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ① Finds out layers that OpenAI uses a Conv1d module instead of a linear module
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ① 发现OpenAI使用Conv1d模块而不是线性模块的层
- en: ② For those layers, we transpose the weight matrix before placing weights in
    our model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ② 对于这些层，我们在将权重放置到我们的模型中之前，转置权重矩阵。
- en: ③ Otherwise, simply copies the weights from OpenAI and places them in our model
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 否则，简单地从OpenAI复制权重并将它们放置到我们的模型中
- en: We extract the OpenAI pretrained weights from Hugging Face and place them in
    our own model. In the process, we make sure that we transpose the weight matrix
    whenever OpenAI checkpoints use a Conv1d module instead of a plain linear module.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Hugging Face提取OpenAI的预训练权重并将它们放置到我们自己的模型中。在这个过程中，我们确保每当OpenAI检查点使用Conv1d模块而不是普通线性模块时，我们都会转置权重矩阵。
- en: Now our model is equipped with pre-trained weights from OpenAI. We can use the
    model to generate coherent text.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经配备了来自OpenAI的预训练权重。我们可以使用该模型生成连贯的文本。
- en: 11.3.2 Defining a generate() function to produce text
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 定义一个generate()函数以生成文本
- en: Armed with pretrained weights from the OpenAI GPT-2XL model, we’ll use the GPT2
    model we created from scratch to generate text.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 借助来自OpenAI GPT-2XL模型的预训练权重，我们将使用我们从头创建的GPT2模型来生成文本。
- en: When generating text, we’ll feed a sequence of indexes that correspond to tokens
    in a prompt to the model. The model predicts the index corresponding to the next
    token and attaches the prediction to the end of the sequence to form a new sequence.
    It then uses the new sequence to make predictions again. It keeps doing this until
    the model has generated a fixed number of new tokens or the conversation is over
    (signified by the special token `<|endoftext|>`).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本时，我们将一个与提示中的标记对应的索引序列输入到模型中。模型预测下一个标记对应的索引，并将预测附加到序列的末尾以形成新的序列。然后它使用新的序列再次进行预测。它一直这样做，直到模型生成固定数量的新标记或对话结束（由特殊标记`<|endoftext|>`表示）。
- en: The special token <|endoftext|> in GPTs
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: GPT中的特殊标记<|endoftext|>
- en: GPT models undergo training using text from a diverse range of sources. A unique
    token, `<|endoftext|>`, is employed during this phase to delineate text from different
    origins. In the text generation phase, it’s crucial to halt the conversation upon
    encountering this special token. Failing to do so may trigger the initiation of
    an unrelated new topic, resulting in subsequent generated text that bears no relevance
    to the ongoing discussion.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型使用来自各种来源的文本进行训练。在这个阶段，使用一个独特的标记`<|endoftext|>`来区分不同来源的文本。在文本生成阶段，在遇到这个特殊标记时停止对话至关重要。如果不这样做，可能会触发无关新话题的启动，导致随后生成的文本与当前讨论无关。
- en: To that end, we define a `sample()` function to add a certain number of new
    indexes to the current sequence. It takes a sequence of indexes as input to feed
    to the GPT-2XL model. It predicts one index at a time and adds the new index to
    the end of the running sequence. It stops until the specified number of time steps,
    `max_new_tokens`, is reached or when the predicted next token is `<|endoftext|>`,
    which signals the end of the conversation. If we don’t stop, the model will randomly
    start an unrelated topic. The `sample()` function is defined as shown in the following
    listing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们定义了一个`sample()`函数，向当前序列中添加一定数量的新索引。它接受一个索引序列作为输入，以供GPT-2XL模型使用。它一次预测一个索引，并将新索引添加到运行序列的末尾。它停止直到达到指定的步数`max_new_tokens`或预测的下一个标记是`<|endoftext|>`，这表示对话结束。如果我们不停下来，模型可能会随机开始一个无关的话题。`sample()`函数的定义如下所示。
- en: Listing 11.7 Iteratively predicting the next index
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.7 逐个预测下一个索引
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Generates a fixed number of new indexes
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成固定数量的新索引
- en: ② Predicts the next index using GPT-2XL
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用GPT-2XL预测下一个索引
- en: ③ If using top-K sampling, sets the logits below the top K choices to –∞
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果使用top-K采样，将低于top K选择的logits设置为-∞
- en: ④ Stops predicting if the next token is <|endoftext|>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如果下一个标记是<|endoftext|>，则停止预测
- en: ⑤ Attaches the new prediction to the end of the sequence
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将新的预测附加到序列的末尾
- en: The `sample()` function uses GPT-2XL to add new indexes to a running sequence.
    It incorporates two arguments, `temperature` and `top_k`, to modulate the generated
    output’s novelty, operating in the same manner as described in chapter 8\. The
    function returns a new sequence of indexes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数使用GPT-2XL向运行序列中添加新的索引。它包含两个参数，`temperature`和`top_k`，以调节生成输出的新颖性，其工作方式与第8章中描述的相同。该函数返回一个新的索引序列。'
- en: Next, we define a `generate()` function to generate text based on a prompt.
    It first converts the text in the prompt to a sequence of indexes. It then feeds
    the sequence to the `sample()` function we just defined to generate a new sequence
    of indexes. Finally, the function `generate()` converts the new sequence of indexes
    back to text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`generate()`函数，根据提示（prompt）生成文本。它首先将提示中的文本转换为一系列索引。然后，它将这个序列输入到我们刚刚定义的`sample()`函数中，以生成一个新的索引序列。最后，`generate()`函数将新的索引序列转换回文本。
- en: Listing 11.8 A function to generate text with GPT-2XL
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.8：使用GPT-2XL生成文本的函数
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① If the prompt is empty, uses <|endoftext|> as the prompt
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如果提示为空，则使用<|endoftext|>作为提示
- en: ② Converts prompt into a sequence of indexes
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将提示转换为一系列索引
- en: ③ Uses the sample() function to generate new indexes
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用sample()函数生成新的索引
- en: ④ Converts the new sequence of indexes back to text
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将新的索引序列转换回文本
- en: 'The `generate()` function bears resemblance to the version we introduced in
    chapter 8 but with a notable distinction: it employs GPT-2XL for prediction purposes,
    moving away from the LSTM model previously utilized. The function accepts a prompt
    as its initial input, transforming this prompt into a series of indexes that are
    then fed into the model to forecast the subsequent index. Upon producing a predetermined
    number of new indexes, the function reverts the entire index sequence back into
    textual form.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`函数与我们在第8章中介绍的那个版本相似，但有一个显著的区别：它使用GPT-2XL进行预测，远离之前使用的LSTM模型。该函数接受一个提示作为其初始输入，将这个提示转换成一系列索引，然后输入到模型中预测后续的索引。在生成预定数量的新索引后，该函数将整个索引序列转换回文本形式。'
- en: 11.3.3 Text generation with GPT-2XL
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 使用GPT-2XL进行文本生成
- en: Now that we have defined the `generate()` function, we can use it to generate
    text.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`generate()`函数，我们可以使用它来生成文本。
- en: 'In particular, the `generate()` function allows for unconditional text generation,
    which means the prompt is empty. The model will generate text randomly. This can
    be beneficial in creative writing: the generated text can be used as inspiration
    or a starting point for one’s own creative work. Let’s try that:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是使用`generate()`函数可以进行无条件文本生成，这意味着提示（prompt）为空。模型将随机生成文本。这在创意写作中可能很有用：生成的文本可以用作灵感或个人创意作品的起点。让我们试试看：
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The output is
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE41]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the preceding output is coherent and grammatically correct but
    may not be factually accurate. I did a quick Google search, and the text doesn’t
    seem to be copied from any online source.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，前面的输出在逻辑上是一致的，语法正确，但可能不是事实准确的。我快速进行了谷歌搜索，文本似乎并未从任何在线来源复制。
- en: Exercise 11.4
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 练习11.4
- en: Generate text unconditionally by setting the prompt as an empty string, temperature
    to 0.9, maximum number of new tokens to 100, and `top_k` to 40\. Set the random
    seed number to 42 in PyTorch. See what the output is.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将提示设置为空字符串，温度设置为0.9，最大新标记数量设置为100，`top_k`设置为40，并在PyTorch中将随机种子数设置为42，无条件生成文本。看看输出结果是什么。
- en: 'To evaluate whether GPT-2XL can produce coherent text based on preceding tokens,
    we will use the prompt “I went to the kitchen and” and generate 10 additional
    tokens after the prompt. We will repeat this process five times to determine if
    the generated text aligns with typical kitchen activities:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估GPT-2XL能否根据前面的标记生成连贯的文本，我们将使用提示“I went to the kitchen and”并在提示后生成10个额外的标记。我们将重复这个过程五次，以确定生成的文本是否与典型的厨房活动相符：
- en: '[PRE42]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The output is
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: These results indicate that the generated text includes activities such as conversing
    with someone, noticing something, and taking beverages, all of which are typical
    kitchen activities. This demonstrates that GPT-2XL can generate text relevant
    to the given context.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，生成的文本包括与某人交谈、注意到某事以及饮用饮料等活动，这些都是典型的厨房活动。这证明了GPT-2XL可以生成与给定上下文相关的文本。
- en: 'Next, we use “Lexington is the second largest city in the state of Kentucky”
    as the prompt and ask the `generate()` function to add up to 100 new tokens:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用“Lexington是肯塔基州第二大城市”作为提示，并要求`generate()`函数添加多达100个新标记：
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output is
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Again, this text is coherent. Even though the generated content may not be
    factually accurate. The GPT-2XL model is, fundamentally, trained to predict the
    next token based on preceding tokens in the sentence. The preceding output shows
    that the model has achieved that goal: the generated text is grammatically correct
    and seemingly logical. It shows the ability to remember the text in the early
    parts of the sequence and generate subsequent words that are relevant to the context.
    For example, while the first sentence discusses the city of Lexington, about 90
    tokens later, the model mentions the music acts from the Lexington area.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这段文本是连贯的。尽管生成的文本可能不是事实准确的。GPT-2XL模型在本质上是被训练来根据句子中的前一个标记预测下一个标记的。前一个输出显示，该模型已经达到了这个目标：生成的文本在语法上是正确的，看起来似乎是逻辑的。它显示了在序列的早期部分记住文本并生成与上下文相关的后续单词的能力。例如，当第一句话讨论列克星敦市时，大约90个标记后，模型提到了列克星敦地区的音乐表演。
- en: Additionally, as noted in the introduction, GPT-2 has its limitations. It should
    not be held to the same standard as ChatGPT or GPT-4, given that its size is less
    than 1% of ChatGPT and less than 0.1% of GPT-4\. GPT-3 has 175 billion parameters
    and produces more coherent text than GPT-2, but the pretrained weights are not
    released to the public.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如引言中提到的，GPT-2有其局限性。鉴于其大小小于ChatGPT的1%和GPT-4的0.1%，它不应被要求与ChatGPT或GPT-4保持相同的标准。GPT-3有1750亿参数，生成的文本比GPT-2更连贯，但预训练的权重并未向公众发布。
- en: 'Next, we’ll explore how `temperature` and `top-K` sampling affect the generated
    text from GPT-2XL. We’ll set the `temperature` to 0.9 and `top_k` to 50 and keep
    other arguments the same. Let’s see what the generated text looks like:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨`temperature`和`top-K`采样如何影响GPT-2XL生成的文本。我们将`temperature`设置为0.9，`top_k`设置为50，并保持其他参数不变。让我们看看生成的文本是什么样的：
- en: '[PRE46]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The output is
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE47]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The generated text seems more coherent than before. However, the content is
    not factually accurate. It made up many facts about the city of Lexington, Kentucky,
    such as “The population of Lexington was 1,731,947 in the 2011 Census.”
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本看起来比以前更连贯。然而，内容在事实上并不准确。它编造了许多关于肯塔基州列克星敦市的事实，例如“2011年人口普查中，列克星敦的人口为1,731,947。”
- en: Exercise 11.5
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 练习11.5
- en: Generate text by setting the `temperature` to 1.2 and `top_k` to None and using
    “Lexington is the second largest city in the state of Kentucky” as the starting
    prompt. Set the random seed number to 42 in PyTorch and the maximum number of
    new tokens to 100.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`temperature`设置为1.2和`top_k`设置为None，并使用“Lexington是肯塔基州第二大城市”作为起始提示来生成文本。在PyTorch中将随机种子数设置为42，并将最大新标记数设置为100。
- en: In this chapter, you have learned how to build GPT-2, the predecessor of ChatGPT
    and GPT-4, from scratch. After that, you extracted the pretrained weights from
    the GPT-2XL model released by OpenAI and loaded them into your model. You witnessed
    the coherent text generated by the model.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你从头开始学习了如何构建GPT-2，它是ChatGPT和GPT-4的前身。之后，你从OpenAI发布的GPT-2XL模型中提取了预训练的权重，并将它们加载到你的模型中。你见证了模型生成的连贯文本。
- en: Due to the large size of the GPT-2XL model (1.5 billion parameters), it’s impossible
    to train the model without supercomputing facilities. In the next chapter, you’ll
    create a smaller version of a GPT model, with a similar structure as GPT-2 but
    only about 5.12 million parameters. You’ll train the model with the text from
    Ernest Hemingway’s novels. The trained model will generate coherent text with
    a style matching that of Hemingway!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT-2XL模型（15亿参数）的体积庞大，没有超级计算设施就无法训练该模型。在下一章中，你将创建一个与GPT-2结构相似但只有约512万个参数的小型GPT模型。你将使用欧内斯特·海明威的小说文本来训练模型。训练后的模型将生成与海明威风格相匹配的连贯文本！
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GPT-2 is an advanced LLM developed by OpenAI and announced in February 2019\.
    It represents a significant milestone in the field of NLP and has paved the way
    for the development of even more sophisticated models, including its successors,
    ChatGPT and GPT-4.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 是 OpenAI 开发的高级 LLM，于 2019 年 2 月宣布。它在自然语言处理领域取得了重大突破，并为开发更复杂的模型铺平了道路，包括其继任者
    ChatGPT 和 GPT-4。
- en: GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the
    model. Like other Transformer models, GPT-2 uses self-attention mechanisms to
    process input data in parallel, significantly improving the efficiency and effectiveness
    of training LLMs.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 是一个仅包含解码器的 Transformer 模型，这意味着模型中没有编码器堆栈。与其他 Transformer 模型一样，GPT-2 使用自注意力机制并行处理输入数据，显著提高了训练大型语言模型（LLMs）的效率和效果。
- en: GPT-2 adopts a different approach to positional encoding than the one used in
    the seminal 2017 paper “Attention Is All You Need.” Instead, GPT-2’s technique
    for positional encoding parallels that of word embeddings.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 在位置编码方面采用了与 2017 年开创性论文“Attention Is All You Need”中使用的不同方法。相反，GPT-2 的位置编码技术与词嵌入技术相平行。
- en: The GELU activation function is used in the feed-forward sublayers of GPT-2\.
    GELU provides a blend of linear and nonlinear activation properties that have
    been found to enhance model performance in deep learning tasks, particularly in
    NLPs and in training LLMs.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 的前馈子层中使用了 GELU 激活函数。GELU 提供了线性和非线性激活特性的混合，这些特性被发现可以增强深度学习任务中的模型性能，尤其是在自然语言处理（NLPs）和训练
    LLMs 方面。
- en: We can build a GPT-2 model from scratch and load up the pretrained weights released
    by OpenAI. The GPT-2 model you created can generate coherent text just as the
    original OpenAI GPT-2 model does.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从头开始构建一个 GPT-2 模型，并加载 OpenAI 发布的预训练权重。你创建的 GPT-2 模型可以生成与原始 OpenAI GPT-2
    模型一样连贯的文本。
