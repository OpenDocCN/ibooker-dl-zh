- en: Chapter 5\. Text Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬5ç« ã€‚æ–‡æœ¬ç”Ÿæˆ
- en: One of the most uncanny features of transformer-based language models is their
    ability to generate text that is almost indistinguishable from text written by
    humans. A famous example is OpenAIâ€™s GPT-2, which when given the prompt:^([1](ch05.xhtml#idm46238719467344))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹æœ€ä»¤äººä¸å®‰çš„ç‰¹ç‚¹ä¹‹ä¸€æ˜¯å®ƒä»¬ç”Ÿæˆçš„æ–‡æœ¬å‡ ä¹æ— æ³•ä¸äººç±»å†™çš„æ–‡æœ¬åŒºåˆ†å¼€ã€‚ä¸€ä¸ªè‘—åçš„ä¾‹å­æ˜¯OpenAIçš„GPT-2ï¼Œå½“ç»™å‡ºä»¥ä¸‹æç¤ºæ—¶ï¼š^([1](ch05.xhtml#idm46238719467344))
- en: In a shocking finding, scientist discovered a herd of unicorns living in a remote,
    previously unexplored valley, in the Andes Mountains. Even more surprising to
    the researchers was the fact that the unicorns spoke perfect English.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªä»¤äººéœ‡æƒŠçš„å‘ç°ä¸­ï¼Œç§‘å­¦å®¶ä»¬å‘ç°äº†ä¸€ç¾¤ç”Ÿæ´»åœ¨å®‰ç¬¬æ–¯å±±è„‰åè¿œã€ä»¥å‰æœªè¢«æ¢ç´¢è¿‡çš„å±±è°·ä¸­çš„ç‹¬è§’å…½ã€‚æ›´ä»¤ç ”ç©¶äººå‘˜æ„Ÿåˆ°æƒŠè®¶çš„æ˜¯ï¼Œè¿™äº›ç‹¬è§’å…½ç«Ÿç„¶è®²ç€ä¸€å£æµåˆ©çš„è‹±è¯­ã€‚
- en: 'was able to generate a compelling news article about talking unicorns:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿç”Ÿæˆä¸€ç¯‡å…³äºè¯´è¯çš„ç‹¬è§’å…½çš„å¼•äººå…¥èƒœçš„æ–°é—»æ–‡ç« ï¼š
- en: The scientist named the population, after their distinctive horn, Ovidâ€™s Unicorn.
    These four-horned, silver-white unicorns were previously unknown to science. Now,
    after almost two centuries, the mystery of what sparked this odd phenomenon is
    finally solved. Dr. Jorge PÃ©rez, an evolutionary biologist from the University
    of La Paz, and several companions, were exploring the Andes Mountains when they
    found a small valley, with no other animals or humans. PÃ©rez noticed that the
    valley had what appeared to be a natural fountain, surrounded by two peaks of
    rock and silver snow. PÃ©rez and the others then ventured further into the valley.
    â€œBy the time we reached the top of one peak, the water looked blue, with some
    crystals on top,â€ said PÃ©rez. PÃ©rez and his friends were astonished to see the
    unicorn herd. These creatures could be seen from the air without having to move
    too much to see themâ€”they were so close they could touch their horns. While examining
    these bizarre creatures the scientists discovered that the creatures also spoke
    some fairly regular English â€¦
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç§‘å­¦å®¶ä»¬æ ¹æ®å®ƒä»¬ç‹¬ç‰¹çš„è§’ï¼Œå°†è¿™ä¸€ç§ç¾¤å‘½åä¸ºå¥¥ç»´å¾·ç‹¬è§’å…½ã€‚è¿™ç§å››è§’ã€é“¶ç™½è‰²çš„ç‹¬è§’å…½ä»¥å‰å¯¹ç§‘å­¦ç•Œæ¥è¯´æ˜¯æœªçŸ¥çš„ã€‚ç°åœ¨ï¼Œç»è¿‡å°†è¿‘ä¸¤ä¸ªä¸–çºªçš„æ—¶é—´ï¼Œè¿™ä¸€å¥‡æ€ªç°è±¡çš„èµ·å› ç»ˆäºå¾—åˆ°äº†è§£å†³ã€‚æ‹‰å·´æ–¯å¤§å­¦çš„è¿›åŒ–ç”Ÿç‰©å­¦å®¶è±ªå°”èµ«Â·ä½©é›·æ–¯åšå£«å’Œå‡ ä¸ªåŒä¼´åœ¨æ¢ç´¢å®‰ç¬¬æ–¯å±±è„‰æ—¶å‘ç°äº†ä¸€ä¸ªæ²¡æœ‰å…¶ä»–åŠ¨ç‰©æˆ–äººç±»çš„å°å±±è°·ã€‚ä½©é›·æ–¯æ³¨æ„åˆ°å±±è°·ä¸­æœ‰ä¸€ä¸ªçœ‹èµ·æ¥åƒæ˜¯å¤©ç„¶å–·æ³‰çš„åœ°æ–¹ï¼Œå‘¨å›´æœ‰ä¸¤åº§å²©çŸ³å’Œé“¶è‰²çš„é›ªå³°ã€‚ä½©é›·æ–¯å’Œå…¶ä»–äººéšåè¿›ä¸€æ­¥æ¢ç´¢äº†å±±è°·ã€‚â€œå½“æˆ‘ä»¬åˆ°è¾¾ä¸€ä¸ªå±±å³°çš„é¡¶éƒ¨æ—¶ï¼Œæ°´çœ‹èµ·æ¥æ˜¯è“è‰²çš„ï¼Œä¸Šé¢æœ‰ä¸€äº›æ°´æ™¶ï¼Œâ€ä½©é›·æ–¯è¯´ã€‚ä½©é›·æ–¯å’Œä»–çš„æœ‹å‹ä»¬æƒŠè®¶åœ°çœ‹åˆ°äº†ç‹¬è§’å…½ç¾¤ã€‚è¿™äº›ç”Ÿç‰©å¯ä»¥ä»ç©ºä¸­çœ‹åˆ°ï¼Œè€Œä¸éœ€è¦ç§»åŠ¨å¤ªå¤šå°±èƒ½çœ‹åˆ°å®ƒä»¬â€”â€”å®ƒä»¬ç¦»å¾—å¾ˆè¿‘ï¼Œå¯ä»¥è§¦æ‘¸åˆ°å®ƒä»¬çš„è§’ã€‚åœ¨æ£€æŸ¥è¿™äº›å¥‡æ€ªçš„ç”Ÿç‰©æ—¶ï¼Œç§‘å­¦å®¶ä»¬å‘ç°è¿™äº›ç”Ÿç‰©è¿˜èƒ½è¯´ä¸€äº›ç›¸å½“è§„èŒƒçš„è‹±è¯­â€¦â€¦
- en: What makes this example so remarkable is that it was generated without any explicit
    supervision! By simply learning to predict the next word in the text of millions
    of web pages, GPT-2 and its more powerful descendants like GPT-3 are able to acquire
    a broad set of skills and pattern recognition abilities that can be activated
    with different kinds of input prompts. [FigureÂ 5-1](#lm-meta-learning) shows how
    language models are sometimes exposed during pretraining to sequences of tasks
    where they need to predict the following tokens based on the context alone, like
    addition, unscrambling words, and translation. This allows them to transfer this
    knowledge effectively during fine-tuning or (if the model is large enough) at
    inference time. These tasks are not chosen ahead of time, but occur naturally
    in the huge corpora used to train billion-parameter language models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­ä¹‹æ‰€ä»¥å¦‚æ­¤å¼•äººæ³¨ç›®ï¼Œæ˜¯å› ä¸ºå®ƒæ˜¯åœ¨æ²¡æœ‰ä»»ä½•æ˜ç¡®ç›‘ç£çš„æƒ…å†µä¸‹ç”Ÿæˆçš„ï¼é€šè¿‡ç®€å•åœ°å­¦ä¹ é¢„æµ‹æ•°ç™¾ä¸‡ç½‘é¡µæ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ŒGPT-2åŠå…¶æ›´å¼ºå¤§çš„åä»£å¦‚GPT-3èƒ½å¤Ÿè·å¾—å¹¿æ³›çš„æŠ€èƒ½å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ä¸åŒç±»å‹çš„è¾“å…¥æç¤ºæ¥æ¿€æ´»ã€‚[å›¾5-1](#lm-meta-learning)æ˜¾ç¤ºäº†è¯­è¨€æ¨¡å‹æœ‰æ—¶åœ¨é¢„è®­ç»ƒæœŸé—´ä¼šæ¥è§¦åˆ°éœ€è¦ä»…åŸºäºä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°çš„ä»»åŠ¡åºåˆ—ï¼Œæ¯”å¦‚åŠ æ³•ã€å•è¯é‡ç»„å’Œç¿»è¯‘ã€‚è¿™ä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨å¾®è°ƒæœŸé—´æˆ–ï¼ˆå¦‚æœæ¨¡å‹è¶³å¤Ÿå¤§ï¼‰åœ¨æ¨æ–­æœŸé—´æœ‰æ•ˆåœ°è½¬ç§»è¿™äº›çŸ¥è¯†ã€‚è¿™äº›ä»»åŠ¡å¹¶ä¸æ˜¯æå‰é€‰æ‹©çš„ï¼Œè€Œæ˜¯åœ¨ç”¨äºè®­ç»ƒç™¾äº¿å‚æ•°è¯­è¨€æ¨¡å‹çš„åºå¤§è¯­æ–™åº“ä¸­è‡ªç„¶å‘ç”Ÿçš„ã€‚
- en: '![LM Meta Learning](Images/nlpt_0501.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![LM Meta Learning](Images/nlpt_0501.png)'
- en: Figure 5-1\. During pretraining, language models are exposed to sequences of
    tasks that can be adapted during inference (courtesy of Tom B. Brown)
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-1ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œè¯­è¨€æ¨¡å‹ä¼šæ¥è§¦åˆ°å¯ä»¥åœ¨æ¨æ–­æœŸé—´è¿›è¡Œè°ƒæ•´çš„ä»»åŠ¡åºåˆ—ï¼ˆç”±æ±¤å§†Â·å¸ƒæœ—æä¾›ï¼‰
- en: The ability of transformers to generate realistic text has led to a diverse
    range of applications, like [InferKit](https://oreil.ly/I4adh), [Write With Transformer](https://oreil.ly/ipkap),
    [AI Dungeon](https://oreil.ly/8ubC1), and conversational agents like [Googleâ€™s
    Meena](https://oreil.ly/gMegC) that can even tell corny jokes, as shown in [FigureÂ 5-2](#meena)!^([2](ch05.xhtml#idm46238719449616))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å˜å‹å™¨ç”Ÿæˆé€¼çœŸæ–‡æœ¬çš„èƒ½åŠ›å·²ç»å¯¼è‡´äº†å„ç§å„æ ·çš„åº”ç”¨ï¼Œæ¯”å¦‚[InferKit](https://oreil.ly/I4adh)ã€[Write With Transformer](https://oreil.ly/ipkap)ã€[AI
    Dungeon](https://oreil.ly/8ubC1)ï¼Œä»¥åŠåƒ[Googleçš„Meena](https://oreil.ly/gMegC)è¿™æ ·çš„å¯¹è¯ä»£ç†ï¼Œç”šè‡³å¯ä»¥è®²ä¸€äº›é™ˆè…çš„ç¬‘è¯ï¼Œå°±åƒ[å›¾5-2](#meena)ä¸­æ‰€ç¤ºçš„é‚£æ ·ï¼^([2](ch05.xhtml#idm46238719449616))
- en: '![Meena](Images/nlpt_0502.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Meena](Images/nlpt_0502.png)'
- en: Figure 5-2\. Meena on the left telling a corny joke to a human on the right
    (courtesy of Daniel Adiwardana and Thang Luong)
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-2ã€‚å·¦è¾¹æ˜¯ç±³å¨œï¼Œå³è¾¹æ˜¯ä¸€ä¸ªäººï¼Œç±³å¨œæ­£åœ¨è®²ä¸€ä¸ªé™ˆè…çš„ç¬‘è¯ï¼ˆç”±ä¸¹å°¼å°”Â·é˜¿è¿ªç“¦è¾¾çº³å’ŒThang Luongæä¾›ï¼‰
- en: In this chapter weâ€™ll use GPT-2 to illustrate how text generation works for
    language models and explore how different decoding strategies impact the generated
    texts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPT-2æ¥è¯´æ˜è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŸç†ï¼Œå¹¶æ¢è®¨ä¸åŒçš„è§£ç ç­–ç•¥å¦‚ä½•å½±å“ç”Ÿæˆçš„æ–‡æœ¬ã€‚
- en: The Challenge with Generating Coherent Text
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè¿è´¯æ–‡æœ¬çš„æŒ‘æˆ˜
- en: 'So far in this book, we have focused on tackling NLP tasks via a combination
    of pretraining and supervised fine-tuning. As weâ€™ve seen, for task-specific heads
    like sequence or token classification, generating predictions is fairly straightforward;
    the model produces some logits and we either take the maximum value to get the
    predicted class, or apply a softmax function to obtain the predicted probabilities
    per class. By contrast, converting the modelâ€™s probabilistic output to text requires
    a *decoding method*, which introduces a few challenges that are unique to text
    generation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä¸“æ³¨äºé€šè¿‡é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒçš„ç»„åˆæ¥è§£å†³NLPä»»åŠ¡ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå¯¹äºè¯¸å¦‚åºåˆ—æˆ–æ ‡è®°åˆ†ç±»çš„ä»»åŠ¡ç‰¹å®šå¤´éƒ¨ï¼Œç”Ÿæˆé¢„æµ‹æ˜¯ç›¸å½“ç®€å•çš„ï¼›æ¨¡å‹äº§ç”Ÿä¸€äº›logitsï¼Œæˆ‘ä»¬è¦ä¹ˆå–æœ€å¤§å€¼å¾—åˆ°é¢„æµ‹ç±»ï¼Œè¦ä¹ˆåº”ç”¨softmaxå‡½æ•°ä»¥è·å¾—æ¯ä¸ªç±»çš„é¢„æµ‹æ¦‚ç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°†æ¨¡å‹çš„æ¦‚ç‡è¾“å‡ºè½¬æ¢ä¸ºæ–‡æœ¬éœ€è¦*è§£ç æ–¹æ³•*ï¼Œè¿™å¼•å…¥äº†ä¸€äº›å¯¹æ–‡æœ¬ç”Ÿæˆç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼š
- en: The decoding is done *iteratively* and thus involves significantly more compute
    than simply passing inputs once through the forward pass of a model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£ç æ˜¯*è¿­ä»£*è¿›è¡Œçš„ï¼Œå› æ­¤æ¶‰åŠçš„è®¡ç®—é‡æ¯”ç®€å•åœ°é€šè¿‡æ¨¡å‹çš„å‰å‘ä¼ é€’ä¸€æ¬¡ä¼ é€’è¾“å…¥è¦å¤šå¾—å¤šã€‚
- en: The *quality* and *diversity* of the generated text depend on the choice of
    decoding method and associated hyperparameters.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„æ–‡æœ¬çš„*è´¨é‡*å’Œ*å¤šæ ·æ€§*å–å†³äºè§£ç æ–¹æ³•å’Œç›¸å…³è¶…å‚æ•°çš„é€‰æ‹©ã€‚
- en: To understand how this decoding process works, letâ€™s start by examining how
    GPT-2 is pretrained and subsequently applied to generate text.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™ä¸ªè§£ç è¿‡ç¨‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬ä»æ£€æŸ¥GPT-2æ˜¯å¦‚ä½•é¢„è®­ç»ƒå’Œéšååº”ç”¨äºç”Ÿæˆæ–‡æœ¬å¼€å§‹ã€‚
- en: 'Like other *autoregressive* or *causal language models*, GPT-2 is pretrained
    to estimate the probability <math alttext="upper P left-parenthesis bold y vertical-bar
    bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></math> of a sequence of tokens <math alttext="bold y equals
    y 1 comma y 2 comma ellipsis y Subscript t Baseline"><mrow><mi>ğ²</mi> <mo>=</mo>
    <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub></mrow></math> occurring
    in the text, given some initial prompt or context sequence <math alttext="bold
    x equals x 1 comma x 2 comma ellipsis x Subscript k Baseline"><mrow><mi>ğ±</mi>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>x</mi> <mi>k</mi></msub></mrow></math> . Since
    it is impractical to acquire enough training data to estimate <math alttext="upper
    P left-parenthesis bold y vertical-bar bold x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math> directly,
    it is common to use the chain rule of probability to factorize it as a product
    of *conditional* probabilities:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–*è‡ªå›å½’*æˆ–*å› æœè¯­è¨€æ¨¡å‹*ä¸€æ ·ï¼ŒGPT-2è¢«é¢„è®­ç»ƒæ¥ä¼°è®¡åœ¨ç»™å®šä¸€äº›åˆå§‹æç¤ºæˆ–ä¸Šä¸‹æ–‡åºåˆ—<math alttext="bold x equals
    x 1 comma x 2 comma ellipsis x Subscript k Baseline"><mrow><mi>ğ±</mi> <mo>=</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡æ–‡æœ¬ä¸­å‡ºç°çš„ä¸€ç³»åˆ—æ ‡è®°<math
    alttext="bold y equals y 1 comma y 2 comma ellipsis y Subscript t Baseline"><mrow><mi>ğ²</mi>
    <mo>=</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub></mrow></math>çš„æ¦‚ç‡<mrow><mi>P</mi>
    <mo>(</mo> <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow>ã€‚ç”±äºç›´æ¥è·å–è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®æ¥ä¼°è®¡<math
    alttext="upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math>æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå› æ­¤é€šå¸¸ä½¿ç”¨æ¦‚ç‡çš„é“¾å¼æ³•åˆ™å°†å…¶åˆ†è§£ä¸º*æ¡ä»¶*æ¦‚ç‡çš„ä¹˜ç§¯ï¼š
- en: <math alttext="upper P left-parenthesis y 1 comma ellipsis comma y Subscript
    t Baseline vertical-bar bold x right-parenthesis equals product Underscript t
    equals 1 Overscript upper N Endscripts upper P left-parenthesis y Subscript t
    Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>âˆ</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi>
    <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y 1 comma ellipsis comma y Subscript
    t Baseline vertical-bar bold x right-parenthesis equals product Underscript t
    equals 1 Overscript upper N Endscripts upper P left-parenthesis y Subscript t
    Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>âˆ</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi>
    <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="y Subscript t"><msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub></math>
    is a shorthand notation for the sequence <math alttext="y 1 comma ellipsis comma
    y Subscript t minus 1 Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    . It is from these conditional probabilities that we pick up the intuition that
    autoregressive language modeling amounts to predicting each word given the preceding
    words in a sentence; this is exactly what the probability on the righthand side
    of the preceding equation describes. Notice that this pretraining objective is
    quite different from BERTâ€™s, which utilizes both *past* and *future* contexts
    to predict a *masked* token.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="y Subscript t"><msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub></math>æ˜¯åºåˆ—<math
    alttext="y 1 comma ellipsis comma y Subscript t minus 1 Baseline"><mrow><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>çš„ç®€å†™ç¬¦å·ã€‚æˆ‘ä»¬ä»è¿™äº›æ¡ä»¶æ¦‚ç‡ä¸­å¾—åˆ°çš„ç›´è§‰æ˜¯ï¼Œè‡ªå›å½’è¯­è¨€å»ºæ¨¡ç­‰äºåœ¨ç»™å®šå¥å­ä¸­çš„å‰é¢å•è¯çš„æƒ…å†µä¸‹é¢„æµ‹æ¯ä¸ªå•è¯ï¼›è¿™æ­£æ˜¯å‰é¢æ–¹ç¨‹å³è¾¹çš„æ¦‚ç‡æè¿°çš„å†…å®¹ã€‚è¯·æ³¨æ„ï¼Œè¿™ç§é¢„è®­ç»ƒç›®æ ‡ä¸BERTçš„ç›®æ ‡éå¸¸ä¸åŒï¼ŒBERTåˆ©ç”¨*è¿‡å»*å’Œ*æœªæ¥*ä¸Šä¸‹æ–‡æ¥é¢„æµ‹*æ©ç *æ ‡è®°ã€‚
- en: By now you may have guessed how we can adapt this next token prediction task
    to generate text sequences of arbitrary length. As shown in [FigureÂ 5-3](#text-generation),
    we start with a prompt like â€œTransformers are theâ€ and use the model to predict
    the next token. Once we have determined the next token, we append it to the prompt
    and then use the new input sequence to generate another token. We do this until
    we have reached a special end-of-sequence token or a predefined maximum length.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å¯èƒ½å·²ç»çŒœåˆ°äº†æˆ‘ä»¬å¦‚ä½•å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡è°ƒæ•´ä¸ºç”Ÿæˆä»»æ„é•¿åº¦çš„æ–‡æœ¬åºåˆ—ã€‚å¦‚[å›¾5-3](#text-generation)æ‰€ç¤ºï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªæç¤ºå¼€å§‹ï¼Œæ¯”å¦‚â€œå˜å‹å™¨æ˜¯â€ï¼Œç„¶åä½¿ç”¨æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚ä¸€æ—¦ç¡®å®šäº†ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œæˆ‘ä»¬å°†å…¶é™„åŠ åˆ°æç¤ºä¸Šï¼Œç„¶åä½¿ç”¨æ–°çš„è¾“å…¥åºåˆ—ç”Ÿæˆå¦ä¸€ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬ä¸€ç›´è¿™æ ·åšï¼Œç›´åˆ°è¾¾åˆ°ç‰¹æ®Šçš„åºåˆ—ç»“æŸæ ‡è®°æˆ–é¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦ã€‚
- en: '![Text generation](Images/nlpt_0503.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![æ–‡æœ¬ç”Ÿæˆ](Images/nlpt_0503.png)'
- en: Figure 5-3\. Generating text from an input sequence by adding a new word to
    the input at each step
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-3ã€‚é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤å‘è¾“å…¥æ·»åŠ ä¸€ä¸ªæ–°å•è¯æ¥ä»è¾“å…¥åºåˆ—ç”Ÿæˆæ–‡æœ¬
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: Since the output sequence is *conditioned* on the choice of input prompt, this
    type of text generation is often called *conditional text generation*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¾“å‡ºåºåˆ—*å–å†³äº*è¾“å…¥æç¤ºçš„é€‰æ‹©ï¼Œè¿™ç§ç±»å‹çš„æ–‡æœ¬ç”Ÿæˆé€šå¸¸è¢«ç§°ä¸º*æ¡ä»¶æ–‡æœ¬ç”Ÿæˆ*ã€‚
- en: 'At the heart of this process lies a decoding method that determines which token
    is selected at each timestep. Since the language model head produces a logit <math
    alttext="z Subscript t comma i"><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    per token in the vocabulary at each step, we can get the probability distribution
    over the next possible token <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    by taking the softmax:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè§£ç æ–¹æ³•ï¼Œå®ƒç¡®å®šåœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤é€‰æ‹©å“ªä¸ªæ ‡è®°ã€‚ç”±äºè¯­è¨€æ¨¡å‹å¤´åœ¨æ¯ä¸ªæ­¥éª¤çš„è¯æ±‡è¡¨ä¸­ä¸ºæ¯ä¸ªæ ‡è®°ç”Ÿæˆä¸€ä¸ªlogit <math alttext="z
    Subscript t comma i"><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub></math>ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨softmaxå¾—åˆ°ä¸‹ä¸€ä¸ªå¯èƒ½æ ‡è®°<math
    alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>çš„æ¦‚ç‡åˆ†å¸ƒï¼š
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
- en: 'The goal of most decoding methods is to search for the most likely overall
    sequence by picking a <math alttext="ModifyingAbove bold y With caret"><mover
    accent="true"><mi>ğ²</mi> <mo>^</mo></mover></math> such that:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°è§£ç æ–¹æ³•çš„ç›®æ ‡æ˜¯é€šè¿‡é€‰æ‹©<math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>ğ²</mi>
    <mo>^</mo></mover></math>æ¥æœç´¢æœ€å¯èƒ½çš„æ•´ä½“åºåˆ—ï¼Œä½¿å¾—ï¼š
- en: <math alttext="ModifyingAbove bold y With caret equals a r g m a x Underscript
    bold y Endscripts upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>ğ²</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>ğ²</mi></munder> <mi>P</mi> <mrow><mo>(</mo>
    <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove bold y With caret equals a r g m a x Underscript
    bold y Endscripts upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>ğ²</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>ğ²</mi></munder> <mi>P</mi> <mrow><mo>(</mo>
    <mi>ğ²</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: Finding <math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>ğ²</mi>
    <mo>^</mo></mover></math> directly would involve evaluating every possible sequence
    with the language model. Since there does not exist an algorithm that can do this
    in a reasonable amount of time, we rely on approximations instead. In this chapter
    weâ€™ll explore a few of these approximations and gradually build up toward smarter
    and more complex algorithms that can be used to generate high-quality texts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥æ‰¾åˆ°<math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>ğ²</mi>
    <mo>^</mo></mover></math>å°†æ¶‰åŠè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ¯ä¸ªå¯èƒ½åºåˆ—ã€‚ç”±äºä¸å­˜åœ¨å¯ä»¥åœ¨åˆç†æ—¶é—´å†…æ‰§è¡Œæ­¤æ“ä½œçš„ç®—æ³•ï¼Œå› æ­¤æˆ‘ä»¬ä¾èµ–äºè¿‘ä¼¼ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å…¶ä¸­ä¸€äº›è¿‘ä¼¼ï¼Œå¹¶é€æ¸æ„å»ºæ›´èªæ˜å’Œæ›´å¤æ‚çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•å¯ä»¥ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ã€‚
- en: Greedy Search Decoding
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´ªå©ªæœç´¢è§£ç 
- en: 'The simplest decoding method to get discrete tokens from a modelâ€™s continuous
    output is to greedily select the token with the highest probability at each timestep:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹çš„è¿ç»­è¾“å‡ºä¸­è·å¾—ç¦»æ•£æ ‡è®°çš„æœ€ç®€å•è§£ç æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤è´ªå©ªåœ°é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ï¼š
- en: <math alttext="ModifyingAbove y With caret Subscript t Baseline equals a r g
    m a x Underscript y Subscript t Baseline Endscripts upper P left-parenthesis y
    Subscript t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>t</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <msub><mi>y</mi>
    <mi>t</mi></msub></munder> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove y With caret Subscript t Baseline equals a r g
    m a x Underscript y Subscript t Baseline Endscripts upper P left-parenthesis y
    Subscript t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>t</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <msub><mi>y</mi>
    <mi>t</mi></msub></munder> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: To see how greedy search works, letâ€™s start by loading the 1.5-billion-parameter
    version of GPT-2 with a language modeling head:^([3](ch05.xhtml#idm46238719316128))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çœ‹çœ‹è´ªå©ªæœç´¢æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬ä»åŠ è½½å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ GPT-2 çš„ 15äº¿å‚æ•°ç‰ˆæœ¬å¼€å§‹ï¼š^([3](ch05.xhtml#idm46238719316128))
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now letâ€™s generate some text! Although ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers provides a `generate()` function for autoregressive models like GPT-2,
    weâ€™ll implement this decoding method ourselves to see what goes on under the hood.
    To warm up, weâ€™ll take the same iterative approach shown in [FigureÂ 5-3](#text-generation):
    weâ€™ll use â€œTransformers are theâ€ as the input prompt and run the decoding for
    eight timesteps. At each timestep, we pick out the modelâ€™s logits for the last
    token in the prompt and wrap them with a softmax to get a probability distribution.
    We then pick the next token with the highest probability, add it to the input
    sequence, and run the process again. The following code does the job, and also
    stores the five most probable tokens at each timestep so we can visualize the
    alternatives:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›æ–‡æœ¬ï¼è™½ç„¶ ![nlpt_pin01](Images/nlpt_pin01.png) Transformers ä¸º GPT-2 è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹æä¾›äº†
    `generate()` å‡½æ•°ï¼Œä½†æˆ‘ä»¬å°†è‡ªå·±å®ç°è¿™ç§è§£ç æ–¹æ³•ï¼Œä»¥äº†è§£åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆã€‚ä¸ºäº†çƒ­èº«ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨[å›¾5-3](#text-generation)ä¸­æ˜¾ç¤ºçš„ç›¸åŒçš„è¿­ä»£æ–¹æ³•ï¼šæˆ‘ä»¬å°†ä½¿ç”¨â€œTransformers
    are theâ€ä½œä¸ºè¾“å…¥æç¤ºï¼Œå¹¶è¿è¡Œå…«ä¸ªæ—¶é—´æ­¥çš„è§£ç ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬æŒ‘é€‰å‡ºæ¨¡å‹å¯¹æç¤ºä¸­æœ€åä¸€ä¸ªæ ‡è®°çš„logitsï¼Œå¹¶ç”¨softmaxåŒ…è£…å®ƒä»¬ä»¥è·å¾—æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶åæˆ‘ä»¬é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå°†å…¶æ·»åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œç„¶åå†æ¬¡è¿è¡Œè¯¥è¿‡ç¨‹ã€‚ä»¥ä¸‹ä»£ç å®Œæˆäº†è¿™é¡¹å·¥ä½œï¼Œå¹¶ä¸”è¿˜å­˜å‚¨äº†æ¯ä¸ªæ—¶é—´æ­¥çš„äº”ä¸ªæœ€æœ‰å¯èƒ½çš„æ ‡è®°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ›¿ä»£æ–¹æ¡ˆï¼š
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | Input | Choice 1 | Choice 2 | Choice 3 | Choice 4 | Choice 5 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | è¾“å…¥ | é€‰æ‹©1 | é€‰æ‹©2 | é€‰æ‹©3 | é€‰æ‹©4 | é€‰æ‹©5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | Transformers are the | most (8.53%) | only (4.96%) | best (4.65%) | Transformers
    (4.37%) | ultimate (2.16%) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 0 | Transformers are the | most (8.53%) | only (4.96%) | best (4.65%) | Transformers
    (4.37%) | ultimate (2.16%) |'
- en: '| 1 | Transformers are the most | popular (16.78%) | powerful (5.37%) | common
    (4.96%) | famous (3.72%) | successful (3.20%) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Transformers are the most | popular (16.78%) | powerful (5.37%) | common
    (4.96%) | famous (3.72%) | successful (3.20%) |'
- en: '| 2 | Transformers are the most popular | toy (10.63%) | toys (7.23%) | Transformers
    (6.60%) | of (5.46%) | and (3.76%) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Transformers are the most popular | toy (10.63%) | toys (7.23%) | Transformers
    (6.60%) | of (5.46%) | and (3.76%) |'
- en: '| 3 | Transformers are the most popular toy | line (34.38%) | in (18.20%) |
    of (11.71%) | brand (6.10%) | line (2.69%) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Transformers are the most popular toy | line (34.38%) | in (18.20%) |
    of (11.71%) | brand (6.10%) | line (2.69%) |'
- en: '| 4 | Transformers are the most popular toy line | in (46.28%) | of (15.09%)
    | , (4.94%) | on (4.40%) | ever (2.72%) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Transformers are the most popular toy line | in (46.28%) | of (15.09%)
    | , (4.94%) | on (4.40%) | ever (2.72%) |'
- en: '| 5 | Transformers are the most popular toy line in | the (65.99%) | history
    (12.42%) | America (6.91%) | Japan (2.44%) | North (1.40%) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Transformers are the most popular toy line in | the (65.99%) | history
    (12.42%) | America (6.91%) | Japan (2.44%) | North (1.40%) |'
- en: '| 6 | Transformers are the most popular toy line in the | world (69.26%) |
    United (4.55%) | history (4.29%) | US (4.23%) | U (2.30%) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Transformers are the most popular toy line in the | world (69.26%) |
    United (4.55%) | history (4.29%) | US (4.23%) | U (2.30%) |'
- en: '| 7 | Transformers are the most popular toy line in the world | , (39.73%)
    | . (30.64%) | and (9.87%) | with (2.32%) | today (1.74%) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Transformers are the most popular toy line in the world | , (39.73%)
    | . (30.64%) | and (9.87%) | with (2.32%) | today (1.74%) |'
- en: With this simple method we were able to generate the sentence â€œTransformers
    are the most popular toy line in the worldâ€. Interestingly, this indicates that
    GPT-2 has internalized some knowledge about the Transformers media franchise,
    which was created by two toy companies (Hasbro and Takara Tomy). We can also see
    the other possible continuations at each step, which shows the iterative nature
    of text generation. Unlike other tasks such as sequence classification where a
    single forward pass suffices to generate the predictions, with text generation
    we need to decode the output tokens one at a time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§ç®€å•çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆå¥å­â€œTransformers are the most popular toy line in the worldâ€ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™è¡¨æ˜
    GPT-2 å·²ç»å†…åŒ–äº†ä¸€äº›å…³äºå˜å½¢é‡‘åˆšåª’ä½“ç‰¹è®¸ç»è¥çš„çŸ¥è¯†ï¼Œè¿™æ˜¯ç”±ä¸¤å®¶ç©å…·å…¬å¸ï¼ˆå­©ä¹‹å®å’ŒTakara Tomyï¼‰åˆ›é€ çš„ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°æ¯ä¸€æ­¥çš„å…¶ä»–å¯èƒ½çš„å»¶ç»­ï¼Œè¿™æ˜¾ç¤ºäº†æ–‡æœ¬ç”Ÿæˆçš„è¿­ä»£æ€§è´¨ã€‚ä¸å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚åºåˆ—åˆ†ç±»ï¼‰ä¸åŒï¼Œåœ¨é‚£äº›ä»»åŠ¡ä¸­ï¼Œå•æ¬¡å‰å‘ä¼ é€’å°±è¶³ä»¥ç”Ÿæˆé¢„æµ‹ï¼Œè€Œåœ¨æ–‡æœ¬ç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸ªè§£ç è¾“å‡ºæ ‡è®°ã€‚
- en: 'Implementing greedy search wasnâ€™t too hard, but weâ€™ll want to use the built-in
    `generate()` function from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers to
    explore more sophisticated decoding methods. To reproduce our simple example,
    letâ€™s make sure sampling is switched off (itâ€™s off by default, unless the specific
    configuration of the model you are loading the checkpoint from states otherwise)
    and specify the `max_new_tokens` for the number of newly generated tokens:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°è´ªå©ªæœç´¢å¹¶ä¸å¤ªéš¾ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨æ¥è‡ª ![nlpt_pin01](Images/nlpt_pin01.png) Transformers çš„å†…ç½® `generate()`
    å‡½æ•°æ¥æ¢ç´¢æ›´å¤æ‚çš„è§£ç æ–¹æ³•ã€‚ä¸ºäº†é‡ç°æˆ‘ä»¬çš„ç®€å•ç¤ºä¾‹ï¼Œè®©æˆ‘ä»¬ç¡®ä¿æŠ½æ ·è¢«å…³é—­ï¼ˆé»˜è®¤æƒ…å†µä¸‹å…³é—­ï¼Œé™¤éæ‚¨ä»åŠ è½½æ£€æŸ¥ç‚¹çš„ç‰¹å®šæ¨¡å‹é…ç½®ä¸­å¦æœ‰è¯´æ˜ï¼‰ï¼Œå¹¶æŒ‡å®š `max_new_tokens`
    ä¸ºæ–°ç”Ÿæˆæ ‡è®°çš„æ•°é‡ï¼š
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now letâ€™s try something a bit more interesting: can we reproduce the unicorn
    story from OpenAI? As we did previously, weâ€™ll encode the prompt with the tokenizer,
    and weâ€™ll specify a larger value for `max_length` to generate a longer sequence
    of text:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°è¯•ä¸€äº›æ›´æœ‰è¶£çš„ä¸œè¥¿ï¼šæˆ‘ä»¬èƒ½å¦é‡ç° OpenAI çš„ç‹¬è§’å…½æ•…äº‹ï¼Ÿä¸ä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åˆ†è¯å™¨å¯¹æç¤ºè¿›è¡Œç¼–ç ï¼Œå¹¶ä¸º `max_length` æŒ‡å®šä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œä»¥ç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬åºåˆ—ã€‚
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Well, the first few sentences are quite different from the OpenAI example and
    amusingly involve different universities being credited with the discovery! We
    can also see one of the main drawbacks with greedy search decoding: it tends to
    produce repetitive output sequences, which is certainly undesirable in a news
    article. This is a common problem with greedy search algorithms, which can fail
    to give you the optimal solution; in the context of decoding, they can miss word
    sequences whose overall probability is higher just because high-probability words
    happen to be preceded by low-probability ones.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œå‰å‡ å¥ä¸OpenAIçš„ç¤ºä¾‹æœ‰å¾ˆå¤§ä¸åŒï¼Œæœ‰è¶£çš„æ˜¯æ¶‰åŠåˆ°ä¸åŒçš„å¤§å­¦è¢«è®¤ä¸ºæ˜¯å‘ç°è€…ï¼æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°è´ªå©ªæœç´¢è§£ç çš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹ï¼šå®ƒå€¾å‘äºäº§ç”Ÿé‡å¤çš„è¾“å‡ºåºåˆ—ï¼Œåœ¨æ–°é—»æ–‡ç« ä¸­æ˜¾ç„¶æ˜¯ä¸å¯å–çš„ã€‚è¿™æ˜¯è´ªå©ªæœç´¢ç®—æ³•çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œå®ƒå¯èƒ½æ— æ³•ç»™å‡ºæœ€ä½³è§£å†³æ–¹æ¡ˆï¼›åœ¨è§£ç çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒå¯èƒ½ä¼šé”™è¿‡æ•´ä½“æ¦‚ç‡æ›´é«˜çš„è¯åºåˆ—ï¼Œåªæ˜¯å› ä¸ºé«˜æ¦‚ç‡çš„è¯æ°å¥½æ˜¯ç”±ä½æ¦‚ç‡çš„è¯å‰å¯¼çš„ã€‚
- en: Fortunately, we can do betterâ€”letâ€™s examine a popular method known as *beam
    search decoding*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾—æ›´å¥½â€”â€”è®©æˆ‘ä»¬æ¥çœ‹ä¸€ç§è¢«ç§°ä¸º*æŸæœç´¢è§£ç *çš„æµè¡Œæ–¹æ³•ã€‚
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: Although greedy search decoding is rarely used for text generation tasks that
    require diversity, it can be useful for producing short sequences like arithmetic
    where a deterministic and factually correct output is preferred.^([4](ch05.xhtml#idm46238718765760))
    For these tasks, you can condition GPT-2 by providing a few line-separated examples
    in the format `"5 + 8 => 13 \n 7 + 2 => 9 \n 1 + 0 =>"` as the input prompt.`  `#
    Beam Search Decoding
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è´ªå©ªæœç´¢è§£ç åœ¨éœ€è¦å¤šæ ·æ€§çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å¾ˆå°‘ä½¿ç”¨ï¼Œä½†å®ƒå¯¹äºç”Ÿæˆåƒç®—æœ¯è¿™æ ·éœ€è¦ç¡®å®šæ€§å’Œäº‹å®æ­£ç¡®çš„è¾“å‡ºçš„çŸ­åºåˆ—æ˜¯æœ‰ç”¨çš„ã€‚^([4](ch05.xhtml#idm46238718765760))
    å¯¹äºè¿™äº›ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥é€šè¿‡æä¾›æ ¼å¼ä¸º`"5 + 8 => 13 \n 7 + 2 => 9 \n 1 + 0 =>"`çš„å‡ ä¸ªä»¥æ¢è¡Œç¬¦åˆ†éš”çš„ç¤ºä¾‹æ¥å¯¹GPT-2è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚`  `#æŸæœç´¢è§£ç 
- en: Instead of decoding the token with the highest probability at each step, beam
    search keeps track of the top-*b* most probable next tokens, where *b* is referred
    to as the number of *beams* or *partial hypotheses*. The next set of beams are
    chosen by considering all possible next-token extensions of the existing set and
    selecting the *b* most likely extensions. The process is repeated until we reach
    the maximum length or an EOS token, and the most likely sequence is selected by
    ranking the *b* beams according to their log probabilities. An example of beam
    search is shown in [FigureÂ 5-4](#beam-search).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æŸæœç´¢ä¸æ˜¯åœ¨æ¯ä¸€æ­¥è§£ç æ—¶é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ï¼Œè€Œæ˜¯è·Ÿè¸ªå‰*b*ä¸ªæœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå…¶ä¸­*b*è¢«ç§°ä¸º*æŸ*æˆ–*éƒ¨åˆ†å‡è®¾*çš„æ•°é‡ã€‚ä¸‹ä¸€ç»„æŸæ˜¯é€šè¿‡è€ƒè™‘ç°æœ‰é›†åˆçš„æ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ‰©å±•ï¼Œå¹¶é€‰æ‹©*b*ä¸ªæœ€æœ‰å¯èƒ½çš„æ‰©å±•æ¥é€‰æ‹©çš„ã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–EOSæ ‡è®°ï¼Œå¹¶ä¸”é€šè¿‡æ ¹æ®å®ƒä»¬çš„å¯¹æ•°æ¦‚ç‡å¯¹*b*æŸè¿›è¡Œæ’åæ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„åºåˆ—ã€‚æŸæœç´¢çš„ä¸€ä¸ªç¤ºä¾‹æ˜¾ç¤ºåœ¨[å›¾5-4](#beam-search)ä¸­ã€‚
- en: '![Beam search](Images/nlpt_0504.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![æŸæœç´¢](Images/nlpt_0504.png)'
- en: Figure 5-4\. Beam search with two beams
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-4ã€‚å…·æœ‰ä¸¤ä¸ªæŸçš„æŸæœç´¢
- en: 'Why do we score the sequences using log probabilities instead of the probabilities
    themselves? That calculating the overall probability of a sequence <math alttext="upper
    P left-parenthesis y 1 comma y 2 comma ellipsis comma y Subscript t Baseline vertical-bar
    bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math>
    involves calculating a *product* of conditional probabilities <math alttext="upper
    P left-parenthesis y Subscript t Baseline vertical-bar y Subscript t Baseline
    comma bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></math> is one reason. Since each conditional probability
    is typically a small number in the range [ <math alttext="0 comma 1"><mrow><mn>0</mn>
    <mo>,</mo> <mn>1</mn></mrow></math> ], taking their product can lead to an overall
    probability that can easily underflow. This means that the computer can no longer
    precisely represent the result of the calculation. For example, suppose we have
    a sequence of <math alttext="t equals 1024"><mrow><mi>t</mi> <mo>=</mo> <mn>1024</mn></mrow></math>
    tokens and generously assume that the probability for each token is 0.5\. The
    overall probability for this sequence is an extremely small number:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ä½¿ç”¨å¯¹æ•°æ¦‚ç‡è€Œä¸æ˜¯æ¦‚ç‡æœ¬èº«æ¥å¯¹åºåˆ—è¿›è¡Œè¯„åˆ†ï¼Ÿè®¡ç®—åºåˆ—çš„æ€»ä½“æ¦‚ç‡<math alttext="upper P left-parenthesis
    y 1 comma y 2 comma ellipsis comma y Subscript t Baseline vertical-bar bold x
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math>æ¶‰åŠè®¡ç®—æ¡ä»¶æ¦‚ç‡<math
    alttext="upper P left-parenthesis y Subscript t Baseline vertical-bar y Subscript
    t Baseline comma bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi>
    <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math>çš„*ä¹˜ç§¯*æ˜¯ä¸€ä¸ªåŸå› ã€‚ç”±äºæ¯ä¸ªæ¡ä»¶æ¦‚ç‡é€šå¸¸æ˜¯åœ¨[ <math alttext="0
    comma 1"><mrow><mn>0</mn> <mo>,</mo> <mn>1</mn></mrow></math> ]èŒƒå›´å†…çš„ä¸€ä¸ªå°æ•°ï¼Œå–å®ƒä»¬çš„ä¹˜ç§¯å¯èƒ½å¯¼è‡´æ€»ä½“æ¦‚ç‡å¾ˆå®¹æ˜“ä¸‹æº¢ã€‚è¿™æ„å‘³ç€è®¡ç®—æœºä¸èƒ½å‡†ç¡®åœ°è¡¨ç¤ºè®¡ç®—ç»“æœã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«<math
    alttext="t equals 1024"><mrow><mi>t</mi> <mo>=</mo> <mn>1024</mn></mrow></math>ä¸ªæ ‡è®°çš„åºåˆ—ï¼Œå¹¶æ…·æ…¨åœ°å‡è®¾æ¯ä¸ªæ ‡è®°çš„æ¦‚ç‡ä¸º0.5ã€‚è¿™ä¸ªåºåˆ—çš„æ€»ä½“æ¦‚ç‡æ˜¯ä¸€ä¸ªæå°çš„æ•°ï¼š
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'which leads to numerical instability as we run into underflow. We can avoid
    this by calculating a related term, the log probability. If we apply the logarithm
    to the joint and conditional probabilities, then with the help of the product
    rule for logarithms we get:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´äº†æ•°å€¼ä¸ç¨³å®šï¼Œå› ä¸ºæˆ‘ä»¬é‡åˆ°äº†ä¸‹æº¢ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—ä¸€ä¸ªç›¸å…³çš„æœ¯è¯­ï¼Œå³å¯¹æ•°æ¦‚ç‡æ¥é¿å…è¿™ç§æƒ…å†µã€‚å¦‚æœæˆ‘ä»¬å¯¹è”åˆå’Œæ¡ä»¶æ¦‚ç‡åº”ç”¨å¯¹æ•°ï¼Œç„¶åå€ŸåŠ©å¯¹æ•°çš„ä¹˜æ³•è§„åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: <math alttext="log upper P left-parenthesis y 1 comma ellipsis y Subscript t
    Baseline vertical-bar bold x right-parenthesis equals sigma-summation Underscript
    t equals 1 Overscript upper N Endscripts log upper P left-parenthesis y Subscript
    t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>âˆ‘</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log upper P left-parenthesis y 1 comma ellipsis y Subscript t
    Baseline vertical-bar bold x right-parenthesis equals sigma-summation Underscript
    t equals 1 Overscript upper N Endscripts log upper P left-parenthesis y Subscript
    t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>âˆ‘</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: 'In other words, the product of probabilities we saw earlier becomes a sum of
    log probabilities, which is much less likely to run into numerical instabilities.
    For example, calculating the log probability of the same example as before gives:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ¦‚ç‡ä¹˜ç§¯å˜æˆäº†å¯¹æ•°æ¦‚ç‡çš„æ€»å’Œï¼Œè¿™æ ·å°±ä¸å¤ªå¯èƒ½é‡åˆ°æ•°å€¼ä¸ç¨³å®šæ€§çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œè®¡ç®—ä¸ä¹‹å‰ç›¸åŒç¤ºä¾‹çš„å¯¹æ•°æ¦‚ç‡ä¸ºï¼š
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is a number we can easily deal with, and this approach still works for
    much smaller numbers. Since we only want to compare relative probabilities, we
    can do this directly with log probabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥è½»æ¾å¤„ç†çš„æ•°å­—ï¼Œè¿™ç§æ–¹æ³•å¯¹äºæ›´å°çš„æ•°å­—ä»ç„¶æœ‰æ•ˆã€‚å› ä¸ºæˆ‘ä»¬åªæƒ³æ¯”è¾ƒç›¸å¯¹æ¦‚ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨å¯¹æ•°æ¦‚ç‡è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'Letâ€™s calculate and compare the log probabilities of the texts generated by
    greedy and beam search to see if beam search can improve the overall probability.
    Since ![nlpt_pin01](Images/nlpt_pin01.png) Transformers models return the unnormalized
    logits for the next token given the input tokens, we first need to normalize the
    logits to create a probability distribution over the whole vocabulary for each
    token in the sequence. We then need to select only the token probabilities that
    were present in the sequence. The following function implements these steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¡ç®—å¹¶æ¯”è¾ƒè´ªå©ªæœç´¢å’ŒæŸæœç´¢ç”Ÿæˆçš„æ–‡æœ¬çš„å¯¹æ•°æ¦‚ç‡ï¼Œçœ‹çœ‹æŸæœç´¢æ˜¯å¦å¯ä»¥æé«˜æ•´ä½“æ¦‚ç‡ã€‚ç”±äº ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers æ¨¡å‹è¿”å›ç»™å®šè¾“å…¥æ ‡è®°çš„ä¸‹ä¸€ä¸ªæ ‡è®°çš„éå½’ä¸€åŒ–logitsï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¯¹logitsè¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥åˆ›å»ºæ•´ä¸ªè¯æ±‡è¡¨ä¸Šæ¯ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶åæˆ‘ä»¬éœ€è¦é€‰æ‹©ä»…å‡ºç°åœ¨åºåˆ—ä¸­çš„æ ‡è®°æ¦‚ç‡ã€‚ä»¥ä¸‹å‡½æ•°å®ç°äº†è¿™äº›æ­¥éª¤ï¼š
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the log probability for a single token, so to get the total log
    probability of a sequence we just need to sum the log probabilities for each token:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™æˆ‘ä»¬äº†å•ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ï¼Œæ‰€ä»¥è¦å¾—åˆ°åºåˆ—çš„æ€»å¯¹æ•°æ¦‚ç‡ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹æ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡æ±‚å’Œï¼š
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we ignore the log probabilities of the input sequence because they
    are not generated by the model. We can also see that it is important to align
    the logits and the labels; since the model predicts the next token, we do not
    get a logit for the first label, and we donâ€™t need the last logit because we donâ€™t
    have a ground truth token for it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¿½ç•¥è¾“å…¥åºåˆ—çš„å¯¹æ•°æ¦‚ç‡ï¼Œå› ä¸ºå®ƒä»¬ä¸æ˜¯æ¨¡å‹ç”Ÿæˆçš„ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°å¯¹é½logitså’Œæ ‡ç­¾çš„é‡è¦æ€§ï¼›å› ä¸ºæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œæˆ‘ä»¬ä¸ä¼šå¾—åˆ°ç¬¬ä¸€ä¸ªæ ‡ç­¾çš„logitï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸éœ€è¦æœ€åä¸€ä¸ªlogitï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®ƒçš„çœŸå®æ ‡è®°ã€‚
- en: 'Letâ€™s use these functions to first calculate the sequence log probability of
    the greedy decoder on the OpenAI prompt:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨è¿™äº›å‡½æ•°æ¥è®¡ç®—OpenAIæç¤ºä¸­è´ªå©ªè§£ç å™¨çš„åºåˆ—å¯¹æ•°æ¦‚ç‡ï¼š
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now letâ€™s compare this to a sequence that is generated with beam search. To
    activate beam search with the `generate()` function we just need to specify the
    number of beams with the `num_beams` parameter. The more beams we choose, the
    better the result potentially gets; however, the generation process becomes much
    slower since we generate parallel sequences for each beam:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°†å…¶ä¸ä½¿ç”¨æŸæœç´¢ç”Ÿæˆçš„åºåˆ—è¿›è¡Œæ¯”è¾ƒã€‚è¦ä½¿ç”¨`generate()`å‡½æ•°æ¿€æ´»æŸæœç´¢ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨`num_beams`å‚æ•°æŒ‡å®šæŸçš„æ•°é‡ã€‚æˆ‘ä»¬é€‰æ‹©çš„æŸæ•°è¶Šå¤šï¼Œç»“æœå¯èƒ½å°±è¶Šå¥½ï¼›ç„¶è€Œï¼Œç”Ÿæˆè¿‡ç¨‹å˜å¾—æ›´æ…¢ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºæ¯ä¸ªæŸç”Ÿæˆå¹¶è¡Œåºåˆ—ï¼š
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can see that we get a better log probability (higher is better) with beam
    search than we did with simple greedy decoding. However, we can see that beam
    search also suffers from repetitive text. One way to address this is to impose
    an *n*-gram penalty with the `no_repeat_ngram_size` parameter that tracks which
    *n*-grams have been seen and sets the next token probability to zero if it would
    produce a previously seen *n*-gram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨æŸæœç´¢å¾—åˆ°çš„å¯¹æ•°æ¦‚ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰æ¯”ç®€å•çš„è´ªå©ªè§£ç è¦å¥½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°æŸæœç´¢ä¹Ÿå­˜åœ¨é‡å¤çš„æ–‡æœ¬é—®é¢˜ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`no_repeat_ngram_size`å‚æ•°æ¥æ–½åŠ *n*-gramæƒ©ç½šï¼Œè¯¥å‚æ•°è·Ÿè¸ªå·²ç»çœ‹åˆ°çš„*n*-gramï¼Œå¹¶ä¸”å¦‚æœç”Ÿæˆçš„ä¸‹ä¸€ä¸ªæ ‡è®°ä¼šäº§ç”Ÿå…ˆå‰çœ‹åˆ°çš„*n*-gramï¼Œåˆ™å°†å…¶æ¦‚ç‡è®¾ç½®ä¸ºé›¶ï¼š
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This isnâ€™t too bad! Weâ€™ve managed to stop the repetitions, and we can see that
    despite producing a lower score, the text remains coherent. Beam search with *n*-gram
    penalty is a good way to find a trade-off between focusing on high-probability
    tokens (with beam search) while reducing repetitions (with *n*-gram penalty),
    and itâ€™s commonly used in applications such as summarization or machine translation
    where factual correctness is important. When factual correctness is less important
    than the diversity of generated output, for instance in open-domain chitchat or
    story generation, another alternative to reduce repetitions while improving diversity
    is to use sampling. Letâ€™s round out our exploration of text generation by examining
    a few of the most common sampling methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸å¤ªç³Ÿç³•ï¼æˆ‘ä»¬æˆåŠŸåœ°åœæ­¢äº†é‡å¤ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡äº§ç”Ÿäº†è¾ƒä½çš„åˆ†æ•°ï¼Œæ–‡æœ¬ä»ç„¶è¿è´¯ã€‚å¸¦æœ‰*n*-gramæƒ©ç½šçš„æ³¢æŸæœç´¢æ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å…³æ³¨é«˜æ¦‚ç‡æ ‡è®°ï¼ˆä½¿ç”¨æ³¢æŸæœç´¢ï¼‰çš„åŒæ—¶å‡å°‘é‡å¤ï¼ˆä½¿ç”¨*n*-gramæƒ©ç½šï¼‰ï¼Œåœ¨æ‘˜è¦æˆ–æœºå™¨ç¿»è¯‘ç­‰éœ€è¦äº‹å®æ­£ç¡®æ€§çš„åº”ç”¨ä¸­é€šå¸¸ä½¿ç”¨ã€‚å½“äº‹å®æ­£ç¡®æ€§ä¸å¦‚ç”Ÿæˆè¾“å‡ºçš„å¤šæ ·æ€§é‡è¦æ—¶ï¼Œä¾‹å¦‚åœ¨å¼€æ”¾åŸŸé—²èŠæˆ–æ•…äº‹ç”Ÿæˆä¸­ï¼Œå¦ä¸€ç§å‡å°‘é‡å¤å¹¶æé«˜å¤šæ ·æ€§çš„æ›¿ä»£æ–¹æ³•æ˜¯ä½¿ç”¨æŠ½æ ·ã€‚è®©æˆ‘ä»¬é€šè¿‡æ£€æŸ¥ä¸€äº›æœ€å¸¸è§çš„æŠ½æ ·æ–¹æ³•æ¥å®Œæˆæˆ‘ä»¬å¯¹æ–‡æœ¬ç”Ÿæˆçš„æ¢ç´¢ã€‚
- en: Sampling Methods
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ½æ ·æ–¹æ³•
- en: 'The simplest sampling method is to randomly sample from the probability distribution
    of the modelâ€™s outputs over the full vocabulary at each timestep:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç®€å•çš„æŠ½æ ·æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä»æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­éšæœºæŠ½æ ·æ•´ä¸ªè¯æ±‡è¡¨ï¼š
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis equals StartFraction exp left-parenthesis
    z Subscript t comma i Baseline right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp
    left-parenthesis z Subscript t comma j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis equals StartFraction exp left-parenthesis
    z Subscript t comma i Baseline right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp
    left-parenthesis z Subscript t comma j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'where <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> denotes the cardinality of the vocabulary.
    We can easily control the diversity of the output by adding a temperature parameter
    *T* that rescales the logits before taking the softmax:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math>è¡¨ç¤ºè¯æ±‡è¡¨çš„åŸºæ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ ä¸€ä¸ªæ¸©åº¦å‚æ•°*T*æ¥æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œè¯¥å‚æ•°åœ¨è¿›è¡Œsoftmaxä¹‹å‰é‡æ–°è°ƒæ•´logitsï¼š
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals StartFraction exp left-parenthesis z Subscript t comma i Baseline slash
    upper T right-parenthesis Over sigma-summation Underscript j equals 1 Overscript
    StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp left-parenthesis z
    Subscript t comma j Baseline slash upper T right-parenthesis EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals StartFraction exp left-parenthesis z Subscript t comma i Baseline slash
    upper T right-parenthesis Over sigma-summation Underscript j equals 1 Overscript
    StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp left-parenthesis z
    Subscript t comma j Baseline slash upper T right-parenthesis EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: By tuning *T* we can control the shape of the probability distribution.^([5](ch05.xhtml#idm46238718164208))
    When <math alttext="upper T much-less-than 1"><mrow><mi>T</mi> <mo>â‰ª</mo> <mn>1</mn></mrow></math>
    , the distribution becomes peaked around the origin and the rare tokens are suppressed.
    On the other hand, when <math alttext="upper T much-greater-than 1"><mrow><mi>T</mi>
    <mo>â‰«</mo> <mn>1</mn></mrow></math> , the distribution flattens out and each token
    becomes equally likely. The effect of temperature on token probabilities is shown
    in [FigureÂ 5-5](#temperature).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒæ•´*T*ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„å½¢çŠ¶ã€‚^([5](ch05.xhtml#idm46238718164208)) å½“<math alttext="upper
    T much-less-than 1"><mrow><mi>T</mi> <mo>â‰ª</mo> <mn>1</mn></mrow></math>æ—¶ï¼Œåˆ†å¸ƒåœ¨åŸç‚¹é™„è¿‘å‘ˆå³°å€¼ï¼Œç½•è§çš„æ ‡è®°è¢«æŠ‘åˆ¶ã€‚å¦ä¸€æ–¹é¢ï¼Œå½“<math
    alttext="upper T much-greater-than 1"><mrow><mi>T</mi> <mo>â‰«</mo> <mn>1</mn></mrow></math>æ—¶ï¼Œåˆ†å¸ƒå˜å¾—å¹³å¦ï¼Œæ¯ä¸ªæ ‡è®°å˜å¾—åŒæ ·å¯èƒ½ã€‚æ¸©åº¦å¯¹æ ‡è®°æ¦‚ç‡çš„å½±å“åœ¨[å›¾5-5](#temperature)ä¸­æ˜¾ç¤ºã€‚
- en: '![Token probabilities as a function of temperature](Images/nlpt_0505.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![æ ‡è®°æ¦‚ç‡ä½œä¸ºæ¸©åº¦çš„å‡½æ•°](Images/nlpt_0505.png)'
- en: Figure 5-5\. Distribution of randomly generated token probabilities for three
    selected temperatures
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-5. ä¸‰ä¸ªé€‰å®šæ¸©åº¦ä¸‹éšæœºç”Ÿæˆçš„æ ‡è®°æ¦‚ç‡åˆ†å¸ƒ
- en: 'To see how we can use temperature to influence the generated text, letâ€™s sample
    with <math alttext="upper T equals 2"><mrow><mi>T</mi> <mo>=</mo> <mn>2</mn></mrow></math>
    by setting the `temperature` parameter in the `generate()` function (weâ€™ll explain
    the meaning of the `top_k` parameter in the next section):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æ¸©åº¦æ¥å½±å“ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè®©æˆ‘ä»¬é€šè¿‡åœ¨`generate()`å‡½æ•°ä¸­è®¾ç½®`temperature`å‚æ•°æ¥ä½¿ç”¨<math alttext="upper
    T equals 2"><mrow><mi>T</mi> <mo>=</mo> <mn>2</mn></mrow></math>è¿›è¡ŒæŠ½æ ·ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è§£é‡Š`top_k`å‚æ•°çš„å«ä¹‰ï¼‰ï¼š
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can clearly see that a high temperature has produced mostly gibberish; by
    accentuating the rare tokens, weâ€™ve caused the model to create strange grammar
    and quite a few made-up words! Letâ€™s see what happens if we cool down the temperature:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œé«˜æ¸©å¤§å¤šäº§ç”Ÿäº†èƒ¡è¨€ä¹±è¯­ï¼›é€šè¿‡çªå‡ºç½•è§çš„æ ‡è®°ï¼Œæˆ‘ä»¬å¯¼è‡´æ¨¡å‹åˆ›å»ºäº†å¥‡æ€ªçš„è¯­æ³•å’Œç›¸å½“å¤šçš„è™šæ„è¯ï¼è®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬é™ä½æ¸©åº¦ä¼šå‘ç”Ÿä»€ä¹ˆï¼š
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is significantly more coherent, and even includes a quote from yet another
    university being credited with the discovery! The main lesson we can draw from
    temperature is that it allows us to control the quality of the samples, but thereâ€™s
    always a trade-off between coherence (low temperature) and diversity (high temperature)
    that one has to tune to the use case at hand.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç„¶æ›´åŠ è¿è´¯ï¼Œç”šè‡³åŒ…æ‹¬äº†å¦ä¸€æ‰€å¤§å­¦è¢«è®¤ä¸ºæ˜¯å‘ç°çš„å¼•ç”¨ï¼æˆ‘ä»¬å¯ä»¥ä»æ¸©åº¦ä¸­å¾—å‡ºçš„ä¸»è¦æ•™è®­æ˜¯ï¼Œå®ƒå…è®¸æˆ‘ä»¬æ§åˆ¶æ ·æœ¬çš„è´¨é‡ï¼Œä½†åœ¨è¿è´¯æ€§ï¼ˆä½æ¸©ï¼‰å’Œå¤šæ ·æ€§ï¼ˆé«˜æ¸©ï¼‰ä¹‹é—´æ€»æ˜¯å­˜åœ¨ä¸€ä¸ªæƒè¡¡ï¼Œéœ€è¦æ ¹æ®æ‰‹å¤´çš„ä½¿ç”¨æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚
- en: 'Another way to adjust the trade-off between coherence and diversity is to truncate
    the distribution of the vocabulary. This allows us to adjust the diversity freely
    with the temperature, but in a more limited range that excludes words that would
    be too strange in the context (i.e., low-probability words). There are two main
    ways to do this: top-*k* and nucleus (or top-*p*) sampling. Letâ€™s take a look.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´è¿è´¯æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡çš„å¦ä¸€ç§æ–¹æ³•æ˜¯æˆªæ–­è¯æ±‡çš„åˆ†å¸ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ¸©åº¦è‡ªç”±è°ƒæ•´å¤šæ ·æ€§ï¼Œä½†åœ¨ä¸€ä¸ªæ›´æœ‰é™çš„èŒƒå›´å†…ï¼Œæ’é™¤äº†åœ¨ä¸Šä¸‹æ–‡ä¸­å¤ªå¥‡æ€ªçš„å•è¯ï¼ˆå³ä½æ¦‚ç‡å•è¯ï¼‰ã€‚æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼štop-*k*å’Œæ ¸ï¼ˆæˆ–top-*p*ï¼‰æŠ½æ ·ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: Top-k and Nucleus Sampling
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-kå’Œæ ¸æŠ½æ ·
- en: Top-*k* and nucleus (top-*p*) sampling are two popular alternatives or extensions
    to using temperature. In both cases, the basic idea is to restrict the number
    of possible tokens we can sample from at each timestep. To see how this works,
    letâ€™s first visualize the cumulative probability distribution of the modelâ€™s outputs
    at <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    as seen in [FigureÂ 5-6](#distribution).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Top-*k*å’Œæ ¸ï¼ˆtop-*p*ï¼‰æŠ½æ ·æ˜¯ä½¿ç”¨æ¸©åº¦çš„ä¸¤ç§æµè¡Œçš„æ›¿ä»£æ–¹æ³•æˆ–æ‰©å±•ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼ŒåŸºæœ¬æ€æƒ³æ˜¯é™åˆ¶æˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ä»ä¸­æŠ½æ ·çš„å¯èƒ½æ ‡è®°çš„æ•°é‡ã€‚ä¸ºäº†äº†è§£è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬é¦–å…ˆå¯è§†åŒ–æ¨¡å‹åœ¨<math
    alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>æ—¶çš„ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒï¼Œå¦‚[å›¾5-6](#distribution)ä¸­æ‰€ç¤ºã€‚
- en: Letâ€™s tease apart these plots, since they contain a lot of information. In the
    upper plot we can see a histogram of the token probabilities. It has a peak around
    <math alttext="10 Superscript negative 8"><msup><mn>10</mn> <mrow><mo>-</mo><mn>8</mn></mrow></msup></math>
    and a second, smaller peak around <math alttext="10 Superscript negative 4"><msup><mn>10</mn>
    <mrow><mo>-</mo><mn>4</mn></mrow></msup></math> , followed by a sharp drop with
    just a handful of tokens occurring with probability between <math alttext="10
    Superscript negative 2"><msup><mn>10</mn> <mrow><mo>-</mo><mn>2</mn></mrow></msup></math>
    and <math alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Looking at this diagram, we can see that the probability of picking the token
    with the highest probability (the isolated bar at <math alttext="10 Superscript
    negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ) is 1 in 10.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ†è§£è¿™äº›å›¾è¡¨ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«äº†å¤§é‡ä¿¡æ¯ã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ‡è®°æ¦‚ç‡çš„ç›´æ–¹å›¾ã€‚å®ƒåœ¨<math alttext="10 Superscript negative
    8"><msup><mn>10</mn> <mrow><mo>-</mo><mn>8</mn></mrow></msup></math>å·¦å³æœ‰ä¸€ä¸ªå³°å€¼ï¼Œç„¶ååœ¨<math
    alttext="10 Superscript negative 4"><msup><mn>10</mn> <mrow><mo>-</mo><mn>4</mn></mrow></msup></math>å·¦å³æœ‰ç¬¬äºŒä¸ªè¾ƒå°çš„å³°å€¼ï¼Œæ¥ç€æ€¥å‰§ä¸‹é™ï¼Œåªæœ‰å°‘æ•°æ ‡è®°çš„æ¦‚ç‡åœ¨<math
    alttext="10 Superscript negative 2"><msup><mn>10</mn> <mrow><mo>-</mo><mn>2</mn></mrow></msup></math>å’Œ<math
    alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>ä¹‹é—´ã€‚ä»è¿™ä¸ªå›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ï¼ˆåœ¨<math
    alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>å¤„çš„å­¤ç«‹æ¡ï¼‰çš„æ¦‚ç‡æ˜¯10ä¸­çš„1ã€‚
- en: '![Probability distribution of next token prediction.](Images/nlpt_0506.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ](Images/nlpt_0506.png)'
- en: Figure 5-6\. Probability distribution of next token prediction (upper) and cumulative
    distribution of descending token probabilities (lower)
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-6. ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä¸Šï¼‰å’Œä¸‹é™æ ‡è®°æ¦‚ç‡çš„ç´¯ç§¯åˆ†å¸ƒï¼ˆä¸‹ï¼‰
- en: In the lower plot, weâ€™ve ordered the tokens by descending probability and calculated
    the cumulative sum of the first 10,000 tokens (in total, there are 50,257 tokens
    in GPT-2â€™s vocabulary). The curved line represents the probability of picking
    any of the preceding tokens. For example, there is roughly a 96% chance of picking
    any of the 1,000 tokens with the highest probability. We see that the probability
    rises quickly above 90% but saturates to close to 100% only after several thousand
    tokens. The plot shows that there is a 1 in 100 chance of not picking any of the
    tokens that are not even in the top 2,000.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬æŒ‰æ¦‚ç‡é™åºæ’åˆ—äº†æ ‡è®°ï¼Œå¹¶è®¡ç®—äº†å‰10,000ä¸ªæ ‡è®°çš„ç´¯ç§¯å’Œï¼ˆGPT-2çš„è¯æ±‡è¡¨ä¸­æ€»å…±æœ‰50,257ä¸ªæ ‡è®°ï¼‰ã€‚æ›²çº¿ä»£è¡¨é€‰æ‹©ä»»ä½•å‰é¢æ ‡è®°çš„æ¦‚ç‡ã€‚ä¾‹å¦‚ï¼Œé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„1,000ä¸ªæ ‡è®°çš„æ¦‚ç‡å¤§çº¦ä¸º96%ã€‚æˆ‘ä»¬çœ‹åˆ°æ¦‚ç‡åœ¨90%ä»¥ä¸Šè¿…é€Ÿä¸Šå‡ï¼Œä½†åªæœ‰åœ¨å‡ åƒä¸ªæ ‡è®°åæ‰æ¥è¿‘100%ã€‚å›¾è¡¨æ˜¾ç¤ºï¼Œæœ‰100åˆ†ä¹‹ä¸€çš„æœºä¼šä¸é€‰æ‹©ä»»ä½•ä¸åœ¨å‰2,000ä½çš„æ ‡è®°ã€‚
- en: Although these numbers might appear small at first sight, they become important
    because we sample once per token when generating text. So even if there is only
    a 1 in 100 or 1,000 chance, if we sample hundreds of times there is a significant
    chance of picking an unlikely token at some pointâ€”and picking such tokens when
    sampling can badly influence the quality of the generated text. For this reason,
    we generally want to avoid these very unlikely tokens. This is where top-*k* and
    top-*p* sampling come into play.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›æ•°å­—ä¹ä¸€çœ‹å¯èƒ½å¾ˆå°ï¼Œä½†å®ƒä»¬å˜å¾—é‡è¦ï¼Œå› ä¸ºåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬æ¯ä¸ªæ ‡è®°åªé‡‡æ ·ä¸€æ¬¡ã€‚å› æ­¤ï¼Œå³ä½¿åªæœ‰100åˆ†ä¹‹ä¸€æˆ–1000åˆ†ä¹‹ä¸€çš„å‡ ç‡ï¼Œå¦‚æœæˆ‘ä»¬é‡‡æ ·æ•°ç™¾æ¬¡ï¼Œå°±æœ‰å¾ˆå¤§çš„æœºä¼šåœ¨æŸä¸ªæ—¶å€™é€‰æ‹©åˆ°ä¸€ä¸ªä¸å¤ªå¯èƒ½çš„æ ‡è®°ï¼Œå¹¶ä¸”åœ¨é‡‡æ ·æ—¶é€‰æ‹©è¿™äº›æ ‡è®°å¯èƒ½ä¼šä¸¥é‡å½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›é¿å…è¿™äº›éå¸¸ä¸å¤ªå¯èƒ½çš„æ ‡è®°ã€‚è¿™å°±æ˜¯top-*k*å’Œtop-*p*é‡‡æ ·å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: 'The idea behind top-*k* sampling is to avoid the low-probability choices by
    only sampling from the *k* tokens with the highest probability. This puts a fixed
    cut on the long tail of the distribution and ensures that we only sample from
    likely choices. Going back to [FigureÂ 5-6](#distribution), top-*k* sampling is
    equivalent to defining a vertical line and sampling from the tokens on the left.
    Again, the `generate()` function provides an easy method to achieve this with
    the `top_k` argument:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: top-*k*é‡‡æ ·çš„ç†å¿µæ˜¯é€šè¿‡ä»…ä»å…·æœ‰æœ€é«˜æ¦‚ç‡çš„*k*ä¸ªæ ‡è®°ä¸­è¿›è¡Œé‡‡æ ·æ¥é¿å…ä½æ¦‚ç‡çš„é€‰æ‹©ã€‚è¿™å¯¹åˆ†å¸ƒçš„é•¿å°¾éƒ¨åˆ†è¿›è¡Œäº†å›ºå®šçš„æˆªæ–­ï¼Œå¹¶ç¡®ä¿æˆ‘ä»¬åªä»å¯èƒ½çš„é€‰æ‹©ä¸­è¿›è¡Œé‡‡æ ·ã€‚å›åˆ°[å›¾5-6](#distribution)ï¼Œtop-*k*é‡‡æ ·ç›¸å½“äºå®šä¹‰ä¸€æ¡å‚ç›´çº¿ï¼Œå¹¶ä»å·¦ä¾§çš„æ ‡è®°ä¸­è¿›è¡Œé‡‡æ ·ã€‚åŒæ ·ï¼Œ`generate()`å‡½æ•°æä¾›äº†ä¸€ä¸ªä½¿ç”¨`top_k`å‚æ•°è½»æ¾å®ç°è¿™ä¸€ç‚¹çš„æ–¹æ³•ï¼š
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is arguably the most human-looking text weâ€™ve generated so far. But how
    do we choose *k*? The value of *k* is chosen manually and is the same for each
    choice in the sequence, independent of the actual output distribution. We can
    find a good value for *k* by looking at some text quality metrics, which we will
    explore in the next chapterâ€”but that fixed cutoff might not be very satisfactory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ˜¯æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ç”Ÿæˆçš„æœ€æ¥è¿‘äººç±»çš„æ–‡æœ¬ã€‚ä½†æ˜¯æˆ‘ä»¬å¦‚ä½•é€‰æ‹©*k*ï¼Ÿ*k*çš„å€¼æ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ï¼Œå¹¶ä¸”å¯¹äºåºåˆ—ä¸­çš„æ¯ä¸ªé€‰æ‹©éƒ½æ˜¯ç›¸åŒçš„ï¼Œç‹¬ç«‹äºå®é™…çš„è¾“å‡ºåˆ†å¸ƒã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ä¸€äº›æ–‡æœ¬è´¨é‡æŒ‡æ ‡æ¥æ‰¾åˆ°*k*çš„åˆé€‚å€¼ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­æ¢è®¨è¿™äº›æŒ‡æ ‡ï¼Œä½†æ˜¯è¿™ç§å›ºå®šçš„æˆªæ–­å¯èƒ½å¹¶ä¸ä»¤äººæ»¡æ„ã€‚
- en: 'An alternative is to use a *dynamic* cutoff. With nucleus or top-*p* sampling,
    instead of choosing a fixed cutoff value, we set a condition of when to cut off.
    This condition is when a certain probability mass in the selection is reached.
    Letâ€™s say we set that value to 95%. We then order all tokens in descending order
    by probability and add one token after another from the top of the list until
    the sum of the probabilities of the selected tokens is 95%. Returning to [FigureÂ 5-6](#distribution),
    the value for *p* defines a horizontal line on the cumulative sum of probabilities
    plot, and we sample only from tokens below the line. Depending on the output distribution,
    this could be just one (very likely) token or a hundred (more equally likely)
    tokens. At this point, you are probably not surprised that the `generate()` function
    also provides an argument to activate top-*p* sampling. Letâ€™s try it out:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§é€‰æ‹©æ˜¯ä½¿ç”¨*åŠ¨æ€*æˆªæ–­ã€‚ä½¿ç”¨æ ¸æˆ–top-*p*é‡‡æ ·æ—¶ï¼Œæˆ‘ä»¬ä¸æ˜¯é€‰æ‹©å›ºå®šçš„æˆªæ–­å€¼ï¼Œè€Œæ˜¯è®¾ç½®ä¸€ä¸ªæˆªæ–­æ¡ä»¶ã€‚è¿™ä¸ªæ¡ä»¶æ˜¯åœ¨é€‰æ‹©ä¸­è¾¾åˆ°ä¸€å®šçš„æ¦‚ç‡è´¨é‡æ—¶æˆªæ–­ã€‚å‡è®¾æˆ‘ä»¬å°†è¯¥å€¼è®¾ç½®ä¸º95%ã€‚ç„¶åï¼Œæˆ‘ä»¬æŒ‰æ¦‚ç‡é™åºæ’åˆ—æ‰€æœ‰æ ‡è®°ï¼Œå¹¶ä»åˆ—è¡¨é¡¶éƒ¨é€ä¸ªæ·»åŠ æ ‡è®°ï¼Œç›´åˆ°æ‰€é€‰æ ‡è®°çš„æ¦‚ç‡æ€»å’Œè¾¾åˆ°95%ã€‚å›åˆ°[å›¾5-6](#distribution)ï¼Œ*p*çš„å€¼å®šä¹‰äº†ç´¯ç§¯æ¦‚ç‡å›¾ä¸Šçš„æ°´å¹³çº¿ï¼Œå¹¶ä¸”æˆ‘ä»¬åªä»çº¿ä¸‹çš„æ ‡è®°ä¸­è¿›è¡Œé‡‡æ ·ã€‚æ ¹æ®è¾“å‡ºåˆ†å¸ƒï¼Œè¿™å¯èƒ½åªæ˜¯ä¸€ä¸ªï¼ˆéå¸¸å¯èƒ½çš„ï¼‰æ ‡è®°ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€ç™¾ä¸ªï¼ˆåŒç­‰å¯èƒ½çš„ï¼‰æ ‡è®°ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæ‚¨å¯èƒ½ä¸ä¼šæ„Ÿåˆ°æƒŠè®¶ï¼Œ`generate()`å‡½æ•°è¿˜æä¾›äº†ä¸€ä¸ªå‚æ•°æ¥æ¿€æ´»top-*p*é‡‡æ ·ã€‚è®©æˆ‘ä»¬è¯•ä¸€è¯•ï¼š
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Top-*p* sampling has also produced a coherent story, and this time with a new
    twist about migrations from Australia to South America. You can even combine the
    two sampling approaches to get the best of both worlds. Setting `top_k=50` and
    `top_p=0.9` corresponds to the rule of choosing tokens with a probability mass
    of 90%, from a pool of at most 50 tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Top-*p*é‡‡æ ·ä¹Ÿäº§ç”Ÿäº†ä¸€ä¸ªè¿è´¯çš„æ•…äº‹ï¼Œè¿™æ¬¡æ˜¯å…³äºä»æ¾³å¤§åˆ©äºšè¿ç§»åˆ°å—ç¾æ´²çš„æ–°æƒ…èŠ‚ã€‚æ‚¨ç”šè‡³å¯ä»¥ç»“åˆè¿™ä¸¤ç§é‡‡æ ·æ–¹æ³•ï¼Œä»¥è·å¾—ä¸¤å…¨å…¶ç¾ã€‚è®¾ç½®`top_k=50`å’Œ`top_p=0.9`ç›¸å½“äºé€‰æ‹©å…·æœ‰90%æ¦‚ç‡è´¨é‡çš„æ ‡è®°ï¼Œæœ€å¤šä»50ä¸ªæ ‡è®°ä¸­è¿›è¡Œé€‰æ‹©çš„è§„åˆ™ã€‚
- en: Note
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: We can also apply beam search when we use sampling. Instead of selecting the
    next batch of candidate tokens greedily, we can sample them and build up the beams
    in the same way.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä½¿ç”¨é‡‡æ ·æ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åº”ç”¨æŸæœç´¢ã€‚æˆ‘ä»¬å¯ä»¥å¯¹ä¸‹ä¸€æ‰¹å€™é€‰æ ‡è®°è¿›è¡Œé‡‡æ ·å¹¶ä»¥ç›¸åŒçš„æ–¹å¼æ„å»ºæŸï¼Œè€Œä¸æ˜¯è´ªå©ªåœ°é€‰æ‹©å®ƒä»¬ã€‚
- en: Which Decoding Method Is Best?
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å“ªç§è§£ç æ–¹æ³•æœ€å¥½ï¼Ÿ
- en: Unfortunately, there is no universally â€œbestâ€ decoding method. Which approach
    is best will depend on the nature of the task you are generating text for. If
    you want your model to perform a precise task like arithmetic or providing an
    answer to a specific question, then you should lower the temperature or use deterministic
    methods like greedy search in combination with beam search to guarantee getting
    the most likely answer. If you want the model to generate longer texts and even
    be a bit creative, then you should switch to sampling methods and increase the
    temperature or use a mix of top-*k* and nucleus sampling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œæ²¡æœ‰æ™®éâ€œæœ€ä½³â€çš„è§£ç æ–¹æ³•ã€‚å“ªç§æ–¹æ³•æœ€å¥½å°†å–å†³äºä½ ä¸ºç”Ÿæˆæ–‡æœ¬çš„ä»»åŠ¡çš„æ€§è´¨ã€‚å¦‚æœä½ å¸Œæœ›ä½ çš„æ¨¡å‹æ‰§è¡Œç²¾ç¡®çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ç®—æœ¯æˆ–æä¾›å¯¹ç‰¹å®šé—®é¢˜çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆä½ åº”è¯¥é™ä½æ¸©åº¦æˆ–ä½¿ç”¨è´ªå©ªæœç´¢ä¸æ³¢æŸæœç´¢çš„ç¡®å®šæ€§æ–¹æ³•æ¥ä¿è¯å¾—åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆã€‚å¦‚æœä½ å¸Œæœ›æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬ï¼Œç”šè‡³æœ‰ç‚¹åˆ›é€ æ€§ï¼Œé‚£ä¹ˆä½ åº”è¯¥åˆ‡æ¢åˆ°é‡‡æ ·æ–¹æ³•ï¼Œå¢åŠ æ¸©åº¦æˆ–ä½¿ç”¨top-*k*å’Œæ ¸é‡‡æ ·çš„æ··åˆã€‚
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this chapter we looked at text generation, which is a very different task
    from the NLU tasks we encountered previously. Generating text requires at least
    one forward pass per generated token, and even more if we use beam search. This
    makes text generation computationally demanding, and one needs the right infrastructure
    to run a text generation model at scale. In addition, a good decoding strategy
    that transforms the modelâ€™s output probabilities into discrete tokens can improve
    the text quality. Finding the best decoding strategy requires some experimentation
    and a subjective evaluation of the generated texts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ–‡æœ¬ç”Ÿæˆï¼Œè¿™ä¸æˆ‘ä»¬ä¹‹å‰é‡åˆ°çš„NLUä»»åŠ¡éå¸¸ä¸åŒã€‚ç”Ÿæˆæ–‡æœ¬è‡³å°‘éœ€è¦æ¯ä¸ªç”Ÿæˆçš„æ ‡è®°è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œå¦‚æœä½¿ç”¨æ³¢æŸæœç´¢ï¼Œç”šè‡³éœ€è¦æ›´å¤šã€‚è¿™ä½¿å¾—æ–‡æœ¬ç”Ÿæˆåœ¨è®¡ç®—ä¸Šè¦æ±‚å¾ˆé«˜ï¼Œéœ€è¦é€‚å½“çš„åŸºç¡€è®¾æ–½æ¥è§„æ¨¡åŒ–è¿è¡Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªè‰¯å¥½çš„è§£ç ç­–ç•¥ï¼Œå°†æ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡è½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ï¼Œå¯ä»¥æé«˜æ–‡æœ¬è´¨é‡ã€‚æ‰¾åˆ°æœ€ä½³çš„è§£ç ç­–ç•¥éœ€è¦ä¸€äº›å®éªŒå’Œå¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œä¸»è§‚è¯„ä»·ã€‚
- en: In practice, however, we donâ€™t want to make these decisions based on gut feeling
    alone! Like with other NLP tasks, we should choose a model performance metric
    that reflects the problem we want to solve. Unsurprisingly, there are a wide range
    of choices, and we will encounter the most common ones in the next chapter, where
    we have a look at how to train and evaluate a model for text summarization. Or,
    if you canâ€™t wait to learn how to train a GPT-type model from scratch, you can
    skip right to [ChapterÂ 10](ch10.xhtml#chapter_fromscratch), where we collect a
    large dataset of code and then train an autoregressive language model on it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä»…å‡­ç›´è§‰åšå‡ºè¿™äº›å†³å®šï¼ä¸å…¶ä»–NLPä»»åŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©ä¸€ä¸ªåæ˜ æˆ‘ä»¬æƒ³è¦è§£å†³çš„é—®é¢˜çš„æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ã€‚æ¯«ä¸å¥‡æ€ªï¼Œæœ‰å¾ˆå¤šé€‰æ‹©ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­é‡åˆ°æœ€å¸¸è§çš„é€‰æ‹©ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•è®­ç»ƒå’Œè¯„ä¼°æ–‡æœ¬æ‘˜è¦æ¨¡å‹ã€‚æˆ–è€…ï¼Œå¦‚æœä½ è¿«ä¸åŠå¾…åœ°æƒ³å­¦ä¹ å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒGPTç±»å‹çš„æ¨¡å‹ï¼Œä½ å¯ä»¥ç›´æ¥è·³åˆ°[ç¬¬10ç« ](ch10.xhtml#chapter_fromscratch)ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬æ”¶é›†äº†å¤§é‡çš„ä»£ç æ•°æ®é›†ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚
- en: ^([1](ch05.xhtml#idm46238719467344-marker)) This example comes from OpenAIâ€™s
    [blog post on GPT-2](https://openai.com/blog/better-language-models).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm46238719467344-marker)) è¿™ä¸ªä¾‹å­æ¥è‡ªOpenAIçš„[GPT-2åšå®¢æ–‡ç« ](https://openai.com/blog/better-language-models)ã€‚
- en: ^([2](ch05.xhtml#idm46238719449616-marker)) However, as [Delip Rao points out](https://oreil.ly/mOM3V),
    whether Meena *intends* to tell corny jokes is a subtle question.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm46238719449616-marker)) ç„¶è€Œï¼Œæ­£å¦‚[Delip RaoæŒ‡å‡ºçš„](https://oreil.ly/mOM3V)ï¼ŒMeenaæ˜¯å¦*æ‰“ç®—*è®²å†·ç¬‘è¯æ˜¯ä¸€ä¸ªå¾®å¦™çš„é—®é¢˜ã€‚
- en: ^([3](ch05.xhtml#idm46238719316128-marker)) If you run out of memory on your
    machine, you can load a smaller GPT-2 version by replacing `model_name = "gpt-xl"`
    with `model_name = "gpt"`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm46238719316128-marker)) å¦‚æœä½ çš„æœºå™¨å†…å­˜ä¸è¶³ï¼Œä½ å¯ä»¥é€šè¿‡å°†`model_name = "gpt-xl"`æ›¿æ¢ä¸º`model_name
    = "gpt"`æ¥åŠ è½½ä¸€ä¸ªè¾ƒå°çš„GPT-2ç‰ˆæœ¬ã€‚
- en: '^([4](ch05.xhtml#idm46238718765760-marker)) N.S. Keskar et al., [â€œCTRL: A Conditional
    Transformer Language Model for Controllable Generationâ€](https://arxiv.org/abs/1909.05858),
    (2019).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch05.xhtml#idm46238718765760-marker)) N.S. Keskarç­‰äººï¼Œ[â€œCTRL: A Conditional
    Transformer Language Model for Controllable Generationâ€](https://arxiv.org/abs/1909.05858)ï¼Œï¼ˆ2019ï¼‰ã€‚'
- en: ^([5](ch05.xhtml#idm46238718164208-marker)) If you know some physics, you may
    recognize a striking resemblance to the [Boltzmann distribution](https://oreil.ly/ZsMmx).`
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm46238718164208-marker)) å¦‚æœä½ æ‡‚ä¸€äº›ç‰©ç†å­¦ï¼Œä½ å¯èƒ½ä¼šå‘ç°ä¸[ç»å°”å…¹æ›¼åˆ†å¸ƒ](https://oreil.ly/ZsMmx)æœ‰æƒŠäººçš„ç›¸ä¼¼ä¹‹å¤„ã€‚
