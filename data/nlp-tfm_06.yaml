- en: Chapter 5\. Text Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。文本生成
- en: One of the most uncanny features of transformer-based language models is their
    ability to generate text that is almost indistinguishable from text written by
    humans. A famous example is OpenAI’s GPT-2, which when given the prompt:^([1](ch05.xhtml#idm46238719467344))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器的语言模型最令人不安的特点之一是它们生成的文本几乎无法与人类写的文本区分开。一个著名的例子是OpenAI的GPT-2，当给出以下提示时：^([1](ch05.xhtml#idm46238719467344))
- en: In a shocking finding, scientist discovered a herd of unicorns living in a remote,
    previously unexplored valley, in the Andes Mountains. Even more surprising to
    the researchers was the fact that the unicorns spoke perfect English.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在一个令人震惊的发现中，科学家们发现了一群生活在安第斯山脉偏远、以前未被探索过的山谷中的独角兽。更令研究人员感到惊讶的是，这些独角兽竟然讲着一口流利的英语。
- en: 'was able to generate a compelling news article about talking unicorns:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 能够生成一篇关于说话的独角兽的引人入胜的新闻文章：
- en: The scientist named the population, after their distinctive horn, Ovid’s Unicorn.
    These four-horned, silver-white unicorns were previously unknown to science. Now,
    after almost two centuries, the mystery of what sparked this odd phenomenon is
    finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University
    of La Paz, and several companions, were exploring the Andes Mountains when they
    found a small valley, with no other animals or humans. Pérez noticed that the
    valley had what appeared to be a natural fountain, surrounded by two peaks of
    rock and silver snow. Pérez and the others then ventured further into the valley.
    “By the time we reached the top of one peak, the water looked blue, with some
    crystals on top,” said Pérez. Pérez and his friends were astonished to see the
    unicorn herd. These creatures could be seen from the air without having to move
    too much to see them—they were so close they could touch their horns. While examining
    these bizarre creatures the scientists discovered that the creatures also spoke
    some fairly regular English …
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 科学家们根据它们独特的角，将这一种群命名为奥维德独角兽。这种四角、银白色的独角兽以前对科学界来说是未知的。现在，经过将近两个世纪的时间，这一奇怪现象的起因终于得到了解决。拉巴斯大学的进化生物学家豪尔赫·佩雷斯博士和几个同伴在探索安第斯山脉时发现了一个没有其他动物或人类的小山谷。佩雷斯注意到山谷中有一个看起来像是天然喷泉的地方，周围有两座岩石和银色的雪峰。佩雷斯和其他人随后进一步探索了山谷。“当我们到达一个山峰的顶部时，水看起来是蓝色的，上面有一些水晶，”佩雷斯说。佩雷斯和他的朋友们惊讶地看到了独角兽群。这些生物可以从空中看到，而不需要移动太多就能看到它们——它们离得很近，可以触摸到它们的角。在检查这些奇怪的生物时，科学家们发现这些生物还能说一些相当规范的英语……
- en: What makes this example so remarkable is that it was generated without any explicit
    supervision! By simply learning to predict the next word in the text of millions
    of web pages, GPT-2 and its more powerful descendants like GPT-3 are able to acquire
    a broad set of skills and pattern recognition abilities that can be activated
    with different kinds of input prompts. [Figure 5-1](#lm-meta-learning) shows how
    language models are sometimes exposed during pretraining to sequences of tasks
    where they need to predict the following tokens based on the context alone, like
    addition, unscrambling words, and translation. This allows them to transfer this
    knowledge effectively during fine-tuning or (if the model is large enough) at
    inference time. These tasks are not chosen ahead of time, but occur naturally
    in the huge corpora used to train billion-parameter language models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子之所以如此引人注目，是因为它是在没有任何明确监督的情况下生成的！通过简单地学习预测数百万网页文本中的下一个单词，GPT-2及其更强大的后代如GPT-3能够获得广泛的技能和模式识别能力，可以通过不同类型的输入提示来激活。[图5-1](#lm-meta-learning)显示了语言模型有时在预训练期间会接触到需要仅基于上下文预测下一个标记的任务序列，比如加法、单词重组和翻译。这使它们能够在微调期间或（如果模型足够大）在推断期间有效地转移这些知识。这些任务并不是提前选择的，而是在用于训练百亿参数语言模型的庞大语料库中自然发生的。
- en: '![LM Meta Learning](Images/nlpt_0501.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![LM Meta Learning](Images/nlpt_0501.png)'
- en: Figure 5-1\. During pretraining, language models are exposed to sequences of
    tasks that can be adapted during inference (courtesy of Tom B. Brown)
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1。在预训练期间，语言模型会接触到可以在推断期间进行调整的任务序列（由汤姆·布朗提供）
- en: The ability of transformers to generate realistic text has led to a diverse
    range of applications, like [InferKit](https://oreil.ly/I4adh), [Write With Transformer](https://oreil.ly/ipkap),
    [AI Dungeon](https://oreil.ly/8ubC1), and conversational agents like [Google’s
    Meena](https://oreil.ly/gMegC) that can even tell corny jokes, as shown in [Figure 5-2](#meena)!^([2](ch05.xhtml#idm46238719449616))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器生成逼真文本的能力已经导致了各种各样的应用，比如[InferKit](https://oreil.ly/I4adh)、[Write With Transformer](https://oreil.ly/ipkap)、[AI
    Dungeon](https://oreil.ly/8ubC1)，以及像[Google的Meena](https://oreil.ly/gMegC)这样的对话代理，甚至可以讲一些陈腐的笑话，就像[图5-2](#meena)中所示的那样！^([2](ch05.xhtml#idm46238719449616))
- en: '![Meena](Images/nlpt_0502.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Meena](Images/nlpt_0502.png)'
- en: Figure 5-2\. Meena on the left telling a corny joke to a human on the right
    (courtesy of Daniel Adiwardana and Thang Luong)
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2。左边是米娜，右边是一个人，米娜正在讲一个陈腐的笑话（由丹尼尔·阿迪瓦达纳和Thang Luong提供）
- en: In this chapter we’ll use GPT-2 to illustrate how text generation works for
    language models and explore how different decoding strategies impact the generated
    texts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用GPT-2来说明语言模型的文本生成工作原理，并探讨不同的解码策略如何影响生成的文本。
- en: The Challenge with Generating Coherent Text
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成连贯文本的挑战
- en: 'So far in this book, we have focused on tackling NLP tasks via a combination
    of pretraining and supervised fine-tuning. As we’ve seen, for task-specific heads
    like sequence or token classification, generating predictions is fairly straightforward;
    the model produces some logits and we either take the maximum value to get the
    predicted class, or apply a softmax function to obtain the predicted probabilities
    per class. By contrast, converting the model’s probabilistic output to text requires
    a *decoding method*, which introduces a few challenges that are unique to text
    generation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经专注于通过预训练和监督微调的组合来解决NLP任务。正如我们所看到的，对于诸如序列或标记分类的任务特定头部，生成预测是相当简单的；模型产生一些logits，我们要么取最大值得到预测类，要么应用softmax函数以获得每个类的预测概率。相比之下，将模型的概率输出转换为文本需要*解码方法*，这引入了一些对文本生成独特的挑战：
- en: The decoding is done *iteratively* and thus involves significantly more compute
    than simply passing inputs once through the forward pass of a model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码是*迭代*进行的，因此涉及的计算量比简单地通过模型的前向传递一次传递输入要多得多。
- en: The *quality* and *diversity* of the generated text depend on the choice of
    decoding method and associated hyperparameters.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的文本的*质量*和*多样性*取决于解码方法和相关超参数的选择。
- en: To understand how this decoding process works, let’s start by examining how
    GPT-2 is pretrained and subsequently applied to generate text.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个解码过程是如何工作的，让我们从检查GPT-2是如何预训练和随后应用于生成文本开始。
- en: 'Like other *autoregressive* or *causal language models*, GPT-2 is pretrained
    to estimate the probability <math alttext="upper P left-parenthesis bold y vertical-bar
    bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></math> of a sequence of tokens <math alttext="bold y equals
    y 1 comma y 2 comma ellipsis y Subscript t Baseline"><mrow><mi>𝐲</mi> <mo>=</mo>
    <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub></mrow></math> occurring
    in the text, given some initial prompt or context sequence <math alttext="bold
    x equals x 1 comma x 2 comma ellipsis x Subscript k Baseline"><mrow><mi>𝐱</mi>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>x</mi> <mi>k</mi></msub></mrow></math> . Since
    it is impractical to acquire enough training data to estimate <math alttext="upper
    P left-parenthesis bold y vertical-bar bold x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math> directly,
    it is common to use the chain rule of probability to factorize it as a product
    of *conditional* probabilities:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他*自回归*或*因果语言模型*一样，GPT-2被预训练来估计在给定一些初始提示或上下文序列<math alttext="bold x equals
    x 1 comma x 2 comma ellipsis x Subscript k Baseline"><mrow><mi>𝐱</mi> <mo>=</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>的情况下，估计文本中出现的一系列标记<math
    alttext="bold y equals y 1 comma y 2 comma ellipsis y Subscript t Baseline"><mrow><mi>𝐲</mi>
    <mo>=</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub></mrow></math>的概率<mrow><mi>P</mi>
    <mo>(</mo> <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow>。由于直接获取足够的训练数据来估计<math
    alttext="upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math>是不切实际的，因此通常使用概率的链式法则将其分解为*条件*概率的乘积：
- en: <math alttext="upper P left-parenthesis y 1 comma ellipsis comma y Subscript
    t Baseline vertical-bar bold x right-parenthesis equals product Underscript t
    equals 1 Overscript upper N Endscripts upper P left-parenthesis y Subscript t
    Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>∏</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi>
    <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y 1 comma ellipsis comma y Subscript
    t Baseline vertical-bar bold x right-parenthesis equals product Underscript t
    equals 1 Overscript upper N Endscripts upper P left-parenthesis y Subscript t
    Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>∏</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi>
    <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="y Subscript t"><msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub></math>
    is a shorthand notation for the sequence <math alttext="y 1 comma ellipsis comma
    y Subscript t minus 1 Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    . It is from these conditional probabilities that we pick up the intuition that
    autoregressive language modeling amounts to predicting each word given the preceding
    words in a sentence; this is exactly what the probability on the righthand side
    of the preceding equation describes. Notice that this pretraining objective is
    quite different from BERT’s, which utilizes both *past* and *future* contexts
    to predict a *masked* token.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="y Subscript t"><msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub></math>是序列<math
    alttext="y 1 comma ellipsis comma y Subscript t minus 1 Baseline"><mrow><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>的简写符号。我们从这些条件概率中得到的直觉是，自回归语言建模等于在给定句子中的前面单词的情况下预测每个单词；这正是前面方程右边的概率描述的内容。请注意，这种预训练目标与BERT的目标非常不同，BERT利用*过去*和*未来*上下文来预测*掩码*标记。
- en: By now you may have guessed how we can adapt this next token prediction task
    to generate text sequences of arbitrary length. As shown in [Figure 5-3](#text-generation),
    we start with a prompt like “Transformers are the” and use the model to predict
    the next token. Once we have determined the next token, we append it to the prompt
    and then use the new input sequence to generate another token. We do this until
    we have reached a special end-of-sequence token or a predefined maximum length.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经猜到了我们如何将下一个标记预测任务调整为生成任意长度的文本序列。如[图5-3](#text-generation)所示，我们从一个提示开始，比如“变压器是”，然后使用模型预测下一个标记。一旦确定了下一个标记，我们将其附加到提示上，然后使用新的输入序列生成另一个标记。我们一直这样做，直到达到特殊的序列结束标记或预定义的最大长度。
- en: '![Text generation](Images/nlpt_0503.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![文本生成](Images/nlpt_0503.png)'
- en: Figure 5-3\. Generating text from an input sequence by adding a new word to
    the input at each step
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3。通过在每个步骤向输入添加一个新单词来从输入序列生成文本
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since the output sequence is *conditioned* on the choice of input prompt, this
    type of text generation is often called *conditional text generation*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出序列*取决于*输入提示的选择，这种类型的文本生成通常被称为*条件文本生成*。
- en: 'At the heart of this process lies a decoding method that determines which token
    is selected at each timestep. Since the language model head produces a logit <math
    alttext="z Subscript t comma i"><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    per token in the vocabulary at each step, we can get the probability distribution
    over the next possible token <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    by taking the softmax:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的核心是一个解码方法，它确定在每个时间步骤选择哪个标记。由于语言模型头在每个步骤的词汇表中为每个标记生成一个logit <math alttext="z
    Subscript t comma i"><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub></math>，我们可以通过使用softmax得到下一个可能标记<math
    alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>的概率分布：
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
- en: 'The goal of most decoding methods is to search for the most likely overall
    sequence by picking a <math alttext="ModifyingAbove bold y With caret"><mover
    accent="true"><mi>𝐲</mi> <mo>^</mo></mover></math> such that:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数解码方法的目标是通过选择<math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>𝐲</mi>
    <mo>^</mo></mover></math>来搜索最可能的整体序列，使得：
- en: <math alttext="ModifyingAbove bold y With caret equals a r g m a x Underscript
    bold y Endscripts upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>𝐲</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>𝐲</mi></munder> <mi>P</mi> <mrow><mo>(</mo>
    <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove bold y With caret equals a r g m a x Underscript
    bold y Endscripts upper P left-parenthesis bold y vertical-bar bold x right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>𝐲</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>𝐲</mi></munder> <mi>P</mi> <mrow><mo>(</mo>
    <mi>𝐲</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: Finding <math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>𝐲</mi>
    <mo>^</mo></mover></math> directly would involve evaluating every possible sequence
    with the language model. Since there does not exist an algorithm that can do this
    in a reasonable amount of time, we rely on approximations instead. In this chapter
    we’ll explore a few of these approximations and gradually build up toward smarter
    and more complex algorithms that can be used to generate high-quality texts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直接找到<math alttext="ModifyingAbove bold y With caret"><mover accent="true"><mi>𝐲</mi>
    <mo>^</mo></mover></math>将涉及评估语言模型的每个可能序列。由于不存在可以在合理时间内执行此操作的算法，因此我们依赖于近似。在本章中，我们将探讨其中一些近似，并逐渐构建更聪明和更复杂的算法，这些算法可以用于生成高质量的文本。
- en: Greedy Search Decoding
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贪婪搜索解码
- en: 'The simplest decoding method to get discrete tokens from a model’s continuous
    output is to greedily select the token with the highest probability at each timestep:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的连续输出中获得离散标记的最简单解码方法是在每个时间步骤贪婪地选择具有最高概率的标记：
- en: <math alttext="ModifyingAbove y With caret Subscript t Baseline equals a r g
    m a x Underscript y Subscript t Baseline Endscripts upper P left-parenthesis y
    Subscript t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>t</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <msub><mi>y</mi>
    <mi>t</mi></msub></munder> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove y With caret Subscript t Baseline equals a r g
    m a x Underscript y Subscript t Baseline Endscripts upper P left-parenthesis y
    Subscript t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>t</mi></msub> <mo>=</mo> <munder><mo form="prefix">argmax</mo> <msub><mi>y</mi>
    <mi>t</mi></msub></munder> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: To see how greedy search works, let’s start by loading the 1.5-billion-parameter
    version of GPT-2 with a language modeling head:^([3](ch05.xhtml#idm46238719316128))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看贪婪搜索是如何工作的，让我们从加载带有语言建模头的 GPT-2 的 15亿参数版本开始：^([3](ch05.xhtml#idm46238719316128))
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let’s generate some text! Although ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers provides a `generate()` function for autoregressive models like GPT-2,
    we’ll implement this decoding method ourselves to see what goes on under the hood.
    To warm up, we’ll take the same iterative approach shown in [Figure 5-3](#text-generation):
    we’ll use “Transformers are the” as the input prompt and run the decoding for
    eight timesteps. At each timestep, we pick out the model’s logits for the last
    token in the prompt and wrap them with a softmax to get a probability distribution.
    We then pick the next token with the highest probability, add it to the input
    sequence, and run the process again. The following code does the job, and also
    stores the five most probable tokens at each timestep so we can visualize the
    alternatives:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成一些文本！虽然 ![nlpt_pin01](Images/nlpt_pin01.png) Transformers 为 GPT-2 这样的自回归模型提供了
    `generate()` 函数，但我们将自己实现这种解码方法，以了解底层发生了什么。为了热身，我们将采用[图5-3](#text-generation)中显示的相同的迭代方法：我们将使用“Transformers
    are the”作为输入提示，并运行八个时间步的解码。在每个时间步，我们挑选出模型对提示中最后一个标记的logits，并用softmax包装它们以获得概率分布。然后我们选择具有最高概率的下一个标记，将其添加到输入序列中，然后再次运行该过程。以下代码完成了这项工作，并且还存储了每个时间步的五个最有可能的标记，以便我们可以可视化替代方案：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | Input | Choice 1 | Choice 2 | Choice 3 | Choice 4 | Choice 5 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 选择1 | 选择2 | 选择3 | 选择4 | 选择5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | Transformers are the | most (8.53%) | only (4.96%) | best (4.65%) | Transformers
    (4.37%) | ultimate (2.16%) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 0 | Transformers are the | most (8.53%) | only (4.96%) | best (4.65%) | Transformers
    (4.37%) | ultimate (2.16%) |'
- en: '| 1 | Transformers are the most | popular (16.78%) | powerful (5.37%) | common
    (4.96%) | famous (3.72%) | successful (3.20%) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Transformers are the most | popular (16.78%) | powerful (5.37%) | common
    (4.96%) | famous (3.72%) | successful (3.20%) |'
- en: '| 2 | Transformers are the most popular | toy (10.63%) | toys (7.23%) | Transformers
    (6.60%) | of (5.46%) | and (3.76%) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Transformers are the most popular | toy (10.63%) | toys (7.23%) | Transformers
    (6.60%) | of (5.46%) | and (3.76%) |'
- en: '| 3 | Transformers are the most popular toy | line (34.38%) | in (18.20%) |
    of (11.71%) | brand (6.10%) | line (2.69%) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Transformers are the most popular toy | line (34.38%) | in (18.20%) |
    of (11.71%) | brand (6.10%) | line (2.69%) |'
- en: '| 4 | Transformers are the most popular toy line | in (46.28%) | of (15.09%)
    | , (4.94%) | on (4.40%) | ever (2.72%) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Transformers are the most popular toy line | in (46.28%) | of (15.09%)
    | , (4.94%) | on (4.40%) | ever (2.72%) |'
- en: '| 5 | Transformers are the most popular toy line in | the (65.99%) | history
    (12.42%) | America (6.91%) | Japan (2.44%) | North (1.40%) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Transformers are the most popular toy line in | the (65.99%) | history
    (12.42%) | America (6.91%) | Japan (2.44%) | North (1.40%) |'
- en: '| 6 | Transformers are the most popular toy line in the | world (69.26%) |
    United (4.55%) | history (4.29%) | US (4.23%) | U (2.30%) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Transformers are the most popular toy line in the | world (69.26%) |
    United (4.55%) | history (4.29%) | US (4.23%) | U (2.30%) |'
- en: '| 7 | Transformers are the most popular toy line in the world | , (39.73%)
    | . (30.64%) | and (9.87%) | with (2.32%) | today (1.74%) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Transformers are the most popular toy line in the world | , (39.73%)
    | . (30.64%) | and (9.87%) | with (2.32%) | today (1.74%) |'
- en: With this simple method we were able to generate the sentence “Transformers
    are the most popular toy line in the world”. Interestingly, this indicates that
    GPT-2 has internalized some knowledge about the Transformers media franchise,
    which was created by two toy companies (Hasbro and Takara Tomy). We can also see
    the other possible continuations at each step, which shows the iterative nature
    of text generation. Unlike other tasks such as sequence classification where a
    single forward pass suffices to generate the predictions, with text generation
    we need to decode the output tokens one at a time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种简单的方法，我们能够生成句子“Transformers are the most popular toy line in the world”。有趣的是，这表明
    GPT-2 已经内化了一些关于变形金刚媒体特许经营的知识，这是由两家玩具公司（孩之宝和Takara Tomy）创造的。我们还可以看到每一步的其他可能的延续，这显示了文本生成的迭代性质。与其他任务（如序列分类）不同，在那些任务中，单次前向传递就足以生成预测，而在文本生成中，我们需要逐个解码输出标记。
- en: 'Implementing greedy search wasn’t too hard, but we’ll want to use the built-in
    `generate()` function from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers to
    explore more sophisticated decoding methods. To reproduce our simple example,
    let’s make sure sampling is switched off (it’s off by default, unless the specific
    configuration of the model you are loading the checkpoint from states otherwise)
    and specify the `max_new_tokens` for the number of newly generated tokens:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实现贪婪搜索并不太难，但我们希望使用来自 ![nlpt_pin01](Images/nlpt_pin01.png) Transformers 的内置 `generate()`
    函数来探索更复杂的解码方法。为了重现我们的简单示例，让我们确保抽样被关闭（默认情况下关闭，除非您从加载检查点的特定模型配置中另有说明），并指定 `max_new_tokens`
    为新生成标记的数量：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s try something a bit more interesting: can we reproduce the unicorn
    story from OpenAI? As we did previously, we’ll encode the prompt with the tokenizer,
    and we’ll specify a larger value for `max_length` to generate a longer sequence
    of text:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一些更有趣的东西：我们能否重现 OpenAI 的独角兽故事？与之前一样，我们将使用分词器对提示进行编码，并为 `max_length` 指定一个较大的值，以生成更长的文本序列。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Well, the first few sentences are quite different from the OpenAI example and
    amusingly involve different universities being credited with the discovery! We
    can also see one of the main drawbacks with greedy search decoding: it tends to
    produce repetitive output sequences, which is certainly undesirable in a news
    article. This is a common problem with greedy search algorithms, which can fail
    to give you the optimal solution; in the context of decoding, they can miss word
    sequences whose overall probability is higher just because high-probability words
    happen to be preceded by low-probability ones.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，前几句与OpenAI的示例有很大不同，有趣的是涉及到不同的大学被认为是发现者！我们还可以看到贪婪搜索解码的一个主要缺点：它倾向于产生重复的输出序列，在新闻文章中显然是不可取的。这是贪婪搜索算法的一个常见问题，它可能无法给出最佳解决方案；在解码的上下文中，它可能会错过整体概率更高的词序列，只是因为高概率的词恰好是由低概率的词前导的。
- en: Fortunately, we can do better—let’s examine a popular method known as *beam
    search decoding*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以做得更好——让我们来看一种被称为*束搜索解码*的流行方法。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although greedy search decoding is rarely used for text generation tasks that
    require diversity, it can be useful for producing short sequences like arithmetic
    where a deterministic and factually correct output is preferred.^([4](ch05.xhtml#idm46238718765760))
    For these tasks, you can condition GPT-2 by providing a few line-separated examples
    in the format `"5 + 8 => 13 \n 7 + 2 => 9 \n 1 + 0 =>"` as the input prompt.`  `#
    Beam Search Decoding
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管贪婪搜索解码在需要多样性的文本生成任务中很少使用，但它对于生成像算术这样需要确定性和事实正确的输出的短序列是有用的。^([4](ch05.xhtml#idm46238718765760))
    对于这些任务，您可以通过提供格式为`"5 + 8 => 13 \n 7 + 2 => 9 \n 1 + 0 =>"`的几个以换行符分隔的示例来对GPT-2进行条件设置。`  `#束搜索解码
- en: Instead of decoding the token with the highest probability at each step, beam
    search keeps track of the top-*b* most probable next tokens, where *b* is referred
    to as the number of *beams* or *partial hypotheses*. The next set of beams are
    chosen by considering all possible next-token extensions of the existing set and
    selecting the *b* most likely extensions. The process is repeated until we reach
    the maximum length or an EOS token, and the most likely sequence is selected by
    ranking the *b* beams according to their log probabilities. An example of beam
    search is shown in [Figure 5-4](#beam-search).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索不是在每一步解码时选择具有最高概率的标记，而是跟踪前*b*个最有可能的下一个标记，其中*b*被称为*束*或*部分假设*的数量。下一组束是通过考虑现有集合的所有可能的下一个标记扩展，并选择*b*个最有可能的扩展来选择的。这个过程重复进行，直到达到最大长度或EOS标记，并且通过根据它们的对数概率对*b*束进行排名来选择最有可能的序列。束搜索的一个示例显示在[图5-4](#beam-search)中。
- en: '![Beam search](Images/nlpt_0504.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![束搜索](Images/nlpt_0504.png)'
- en: Figure 5-4\. Beam search with two beams
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。具有两个束的束搜索
- en: 'Why do we score the sequences using log probabilities instead of the probabilities
    themselves? That calculating the overall probability of a sequence <math alttext="upper
    P left-parenthesis y 1 comma y 2 comma ellipsis comma y Subscript t Baseline vertical-bar
    bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math>
    involves calculating a *product* of conditional probabilities <math alttext="upper
    P left-parenthesis y Subscript t Baseline vertical-bar y Subscript t Baseline
    comma bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></math> is one reason. Since each conditional probability
    is typically a small number in the range [ <math alttext="0 comma 1"><mrow><mn>0</mn>
    <mo>,</mo> <mn>1</mn></mrow></math> ], taking their product can lead to an overall
    probability that can easily underflow. This means that the computer can no longer
    precisely represent the result of the calculation. For example, suppose we have
    a sequence of <math alttext="t equals 1024"><mrow><mi>t</mi> <mo>=</mo> <mn>1024</mn></mrow></math>
    tokens and generously assume that the probability for each token is 0.5\. The
    overall probability for this sequence is an extremely small number:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要使用对数概率而不是概率本身来对序列进行评分？计算序列的总体概率<math alttext="upper P left-parenthesis
    y 1 comma y 2 comma ellipsis comma y Subscript t Baseline vertical-bar bold x
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math>涉及计算条件概率<math
    alttext="upper P left-parenthesis y Subscript t Baseline vertical-bar y Subscript
    t Baseline comma bold x right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>y</mi>
    <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math>的*乘积*是一个原因。由于每个条件概率通常是在[ <math alttext="0
    comma 1"><mrow><mn>0</mn> <mo>,</mo> <mn>1</mn></mrow></math> ]范围内的一个小数，取它们的乘积可能导致总体概率很容易下溢。这意味着计算机不能准确地表示计算结果。例如，假设我们有一个包含<math
    alttext="t equals 1024"><mrow><mi>t</mi> <mo>=</mo> <mn>1024</mn></mrow></math>个标记的序列，并慷慨地假设每个标记的概率为0.5。这个序列的总体概率是一个极小的数：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'which leads to numerical instability as we run into underflow. We can avoid
    this by calculating a related term, the log probability. If we apply the logarithm
    to the joint and conditional probabilities, then with the help of the product
    rule for logarithms we get:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了数值不稳定，因为我们遇到了下溢。我们可以通过计算一个相关的术语，即对数概率来避免这种情况。如果我们对联合和条件概率应用对数，然后借助对数的乘法规则，我们得到：
- en: <math alttext="log upper P left-parenthesis y 1 comma ellipsis y Subscript t
    Baseline vertical-bar bold x right-parenthesis equals sigma-summation Underscript
    t equals 1 Overscript upper N Endscripts log upper P left-parenthesis y Subscript
    t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log upper P left-parenthesis y 1 comma ellipsis y Subscript t
    Baseline vertical-bar bold x right-parenthesis equals sigma-summation Underscript
    t equals 1 Overscript upper N Endscripts log upper P left-parenthesis y Subscript
    t Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mo form="prefix">log</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: 'In other words, the product of probabilities we saw earlier becomes a sum of
    log probabilities, which is much less likely to run into numerical instabilities.
    For example, calculating the log probability of the same example as before gives:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们之前看到的概率乘积变成了对数概率的总和，这样就不太可能遇到数值不稳定性的问题。例如，计算与之前相同示例的对数概率为：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is a number we can easily deal with, and this approach still works for
    much smaller numbers. Since we only want to compare relative probabilities, we
    can do this directly with log probabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个我们可以轻松处理的数字，这种方法对于更小的数字仍然有效。因为我们只想比较相对概率，所以我们可以直接使用对数概率进行比较。
- en: 'Let’s calculate and compare the log probabilities of the texts generated by
    greedy and beam search to see if beam search can improve the overall probability.
    Since ![nlpt_pin01](Images/nlpt_pin01.png) Transformers models return the unnormalized
    logits for the next token given the input tokens, we first need to normalize the
    logits to create a probability distribution over the whole vocabulary for each
    token in the sequence. We then need to select only the token probabilities that
    were present in the sequence. The following function implements these steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算并比较贪婪搜索和束搜索生成的文本的对数概率，看看束搜索是否可以提高整体概率。由于 ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers 模型返回给定输入标记的下一个标记的非归一化logits，我们首先需要对logits进行归一化，以创建整个词汇表上每个标记的概率分布。然后我们需要选择仅出现在序列中的标记概率。以下函数实现了这些步骤：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the log probability for a single token, so to get the total log
    probability of a sequence we just need to sum the log probabilities for each token:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们了单个标记的对数概率，所以要得到序列的总对数概率，我们只需要对每个标记的对数概率求和：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we ignore the log probabilities of the input sequence because they
    are not generated by the model. We can also see that it is important to align
    the logits and the labels; since the model predicts the next token, we do not
    get a logit for the first label, and we don’t need the last logit because we don’t
    have a ground truth token for it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们忽略输入序列的对数概率，因为它们不是模型生成的。我们还可以看到对齐logits和标签的重要性；因为模型预测下一个标记，我们不会得到第一个标签的logit，并且我们不需要最后一个logit，因为我们没有它的真实标记。
- en: 'Let’s use these functions to first calculate the sequence log probability of
    the greedy decoder on the OpenAI prompt:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用这些函数来计算OpenAI提示中贪婪解码器的序列对数概率：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let’s compare this to a sequence that is generated with beam search. To
    activate beam search with the `generate()` function we just need to specify the
    number of beams with the `num_beams` parameter. The more beams we choose, the
    better the result potentially gets; however, the generation process becomes much
    slower since we generate parallel sequences for each beam:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其与使用束搜索生成的序列进行比较。要使用`generate()`函数激活束搜索，我们只需要使用`num_beams`参数指定束的数量。我们选择的束数越多，结果可能就越好；然而，生成过程变得更慢，因为我们为每个束生成并行序列：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can see that we get a better log probability (higher is better) with beam
    search than we did with simple greedy decoding. However, we can see that beam
    search also suffers from repetitive text. One way to address this is to impose
    an *n*-gram penalty with the `no_repeat_ngram_size` parameter that tracks which
    *n*-grams have been seen and sets the next token probability to zero if it would
    produce a previously seen *n*-gram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用束搜索得到的对数概率（越高越好）比简单的贪婪解码要好。然而，我们也可以看到束搜索也存在重复的文本问题。解决这个问题的一种方法是使用`no_repeat_ngram_size`参数来施加*n*-gram惩罚，该参数跟踪已经看到的*n*-gram，并且如果生成的下一个标记会产生先前看到的*n*-gram，则将其概率设置为零：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This isn’t too bad! We’ve managed to stop the repetitions, and we can see that
    despite producing a lower score, the text remains coherent. Beam search with *n*-gram
    penalty is a good way to find a trade-off between focusing on high-probability
    tokens (with beam search) while reducing repetitions (with *n*-gram penalty),
    and it’s commonly used in applications such as summarization or machine translation
    where factual correctness is important. When factual correctness is less important
    than the diversity of generated output, for instance in open-domain chitchat or
    story generation, another alternative to reduce repetitions while improving diversity
    is to use sampling. Let’s round out our exploration of text generation by examining
    a few of the most common sampling methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太糟糕！我们成功地停止了重复，并且可以看到，尽管产生了较低的分数，文本仍然连贯。带有*n*-gram惩罚的波束搜索是一种很好的方法，可以在关注高概率标记（使用波束搜索）的同时减少重复（使用*n*-gram惩罚），在摘要或机器翻译等需要事实正确性的应用中通常使用。当事实正确性不如生成输出的多样性重要时，例如在开放域闲聊或故事生成中，另一种减少重复并提高多样性的替代方法是使用抽样。让我们通过检查一些最常见的抽样方法来完成我们对文本生成的探索。
- en: Sampling Methods
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽样方法
- en: 'The simplest sampling method is to randomly sample from the probability distribution
    of the model’s outputs over the full vocabulary at each timestep:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的抽样方法是在每个时间步从模型输出的概率分布中随机抽样整个词汇表：
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis equals StartFraction exp left-parenthesis
    z Subscript t comma i Baseline right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp
    left-parenthesis z Subscript t comma j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals normal s normal o normal f normal t normal m normal a normal x left-parenthesis
    z Subscript t comma i Baseline right-parenthesis equals StartFraction exp left-parenthesis
    z Subscript t comma i Baseline right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp
    left-parenthesis z Subscript t comma j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'where <math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math> denotes the cardinality of the vocabulary.
    We can easily control the diversity of the output by adding a temperature parameter
    *T* that rescales the logits before taking the softmax:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="StartAbsoluteValue upper V EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>V</mi> <mo>|</mo></mrow></math>表示词汇表的基数。我们可以通过添加一个温度参数*T*来控制输出的多样性，该参数在进行softmax之前重新调整logits：
- en: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals StartFraction exp left-parenthesis z Subscript t comma i Baseline slash
    upper T right-parenthesis Over sigma-summation Underscript j equals 1 Overscript
    StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp left-parenthesis z
    Subscript t comma j Baseline slash upper T right-parenthesis EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis y Subscript t Baseline equals w Subscript
    i Baseline vertical-bar y Subscript t Baseline comma bold x right-parenthesis
    equals StartFraction exp left-parenthesis z Subscript t comma i Baseline slash
    upper T right-parenthesis Over sigma-summation Underscript j equals 1 Overscript
    StartAbsoluteValue upper V EndAbsoluteValue Endscripts exp left-parenthesis z
    Subscript t comma j Baseline slash upper T right-parenthesis EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mrow><mo><</mo><mi>t</mi></mrow></msub>
    <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi> <mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: By tuning *T* we can control the shape of the probability distribution.^([5](ch05.xhtml#idm46238718164208))
    When <math alttext="upper T much-less-than 1"><mrow><mi>T</mi> <mo>≪</mo> <mn>1</mn></mrow></math>
    , the distribution becomes peaked around the origin and the rare tokens are suppressed.
    On the other hand, when <math alttext="upper T much-greater-than 1"><mrow><mi>T</mi>
    <mo>≫</mo> <mn>1</mn></mrow></math> , the distribution flattens out and each token
    becomes equally likely. The effect of temperature on token probabilities is shown
    in [Figure 5-5](#temperature).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整*T*，我们可以控制概率分布的形状。^([5](ch05.xhtml#idm46238718164208)) 当<math alttext="upper
    T much-less-than 1"><mrow><mi>T</mi> <mo>≪</mo> <mn>1</mn></mrow></math>时，分布在原点附近呈峰值，罕见的标记被抑制。另一方面，当<math
    alttext="upper T much-greater-than 1"><mrow><mi>T</mi> <mo>≫</mo> <mn>1</mn></mrow></math>时，分布变得平坦，每个标记变得同样可能。温度对标记概率的影响在[图5-5](#temperature)中显示。
- en: '![Token probabilities as a function of temperature](Images/nlpt_0505.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![标记概率作为温度的函数](Images/nlpt_0505.png)'
- en: Figure 5-5\. Distribution of randomly generated token probabilities for three
    selected temperatures
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5. 三个选定温度下随机生成的标记概率分布
- en: 'To see how we can use temperature to influence the generated text, let’s sample
    with <math alttext="upper T equals 2"><mrow><mi>T</mi> <mo>=</mo> <mn>2</mn></mrow></math>
    by setting the `temperature` parameter in the `generate()` function (we’ll explain
    the meaning of the `top_k` parameter in the next section):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看我们如何使用温度来影响生成的文本，让我们通过在`generate()`函数中设置`temperature`参数来使用<math alttext="upper
    T equals 2"><mrow><mi>T</mi> <mo>=</mo> <mn>2</mn></mrow></math>进行抽样（我们将在下一节解释`top_k`参数的含义）：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can clearly see that a high temperature has produced mostly gibberish; by
    accentuating the rare tokens, we’ve caused the model to create strange grammar
    and quite a few made-up words! Let’s see what happens if we cool down the temperature:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，高温大多产生了胡言乱语；通过突出罕见的标记，我们导致模型创建了奇怪的语法和相当多的虚构词！让我们看看如果我们降低温度会发生什么：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is significantly more coherent, and even includes a quote from yet another
    university being credited with the discovery! The main lesson we can draw from
    temperature is that it allows us to control the quality of the samples, but there’s
    always a trade-off between coherence (low temperature) and diversity (high temperature)
    that one has to tune to the use case at hand.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然更加连贯，甚至包括了另一所大学被认为是发现的引用！我们可以从温度中得出的主要教训是，它允许我们控制样本的质量，但在连贯性（低温）和多样性（高温）之间总是存在一个权衡，需要根据手头的使用情况进行调整。
- en: 'Another way to adjust the trade-off between coherence and diversity is to truncate
    the distribution of the vocabulary. This allows us to adjust the diversity freely
    with the temperature, but in a more limited range that excludes words that would
    be too strange in the context (i.e., low-probability words). There are two main
    ways to do this: top-*k* and nucleus (or top-*p*) sampling. Let’s take a look.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 调整连贯性和多样性之间的权衡的另一种方法是截断词汇的分布。这使我们能够通过温度自由调整多样性，但在一个更有限的范围内，排除了在上下文中太奇怪的单词（即低概率单词）。有两种主要方法可以做到这一点：top-*k*和核（或top-*p*）抽样。让我们来看看。
- en: Top-k and Nucleus Sampling
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-k和核抽样
- en: Top-*k* and nucleus (top-*p*) sampling are two popular alternatives or extensions
    to using temperature. In both cases, the basic idea is to restrict the number
    of possible tokens we can sample from at each timestep. To see how this works,
    let’s first visualize the cumulative probability distribution of the model’s outputs
    at <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    as seen in [Figure 5-6](#distribution).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Top-*k*和核（top-*p*）抽样是使用温度的两种流行的替代方法或扩展。在这两种情况下，基本思想是限制我们可以在每个时间步骤从中抽样的可能标记的数量。为了了解这是如何工作的，让我们首先可视化模型在<math
    alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>时的累积概率分布，如[图5-6](#distribution)中所示。
- en: Let’s tease apart these plots, since they contain a lot of information. In the
    upper plot we can see a histogram of the token probabilities. It has a peak around
    <math alttext="10 Superscript negative 8"><msup><mn>10</mn> <mrow><mo>-</mo><mn>8</mn></mrow></msup></math>
    and a second, smaller peak around <math alttext="10 Superscript negative 4"><msup><mn>10</mn>
    <mrow><mo>-</mo><mn>4</mn></mrow></msup></math> , followed by a sharp drop with
    just a handful of tokens occurring with probability between <math alttext="10
    Superscript negative 2"><msup><mn>10</mn> <mrow><mo>-</mo><mn>2</mn></mrow></msup></math>
    and <math alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Looking at this diagram, we can see that the probability of picking the token
    with the highest probability (the isolated bar at <math alttext="10 Superscript
    negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ) is 1 in 10.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这些图表，因为它们包含了大量信息。在上图中，我们可以看到标记概率的直方图。它在<math alttext="10 Superscript negative
    8"><msup><mn>10</mn> <mrow><mo>-</mo><mn>8</mn></mrow></msup></math>左右有一个峰值，然后在<math
    alttext="10 Superscript negative 4"><msup><mn>10</mn> <mrow><mo>-</mo><mn>4</mn></mrow></msup></math>左右有第二个较小的峰值，接着急剧下降，只有少数标记的概率在<math
    alttext="10 Superscript negative 2"><msup><mn>10</mn> <mrow><mo>-</mo><mn>2</mn></mrow></msup></math>和<math
    alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>之间。从这个图表中，我们可以看到选择具有最高概率的标记（在<math
    alttext="10 Superscript negative 1"><msup><mn>10</mn> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>处的孤立条）的概率是10中的1。
- en: '![Probability distribution of next token prediction.](Images/nlpt_0506.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![下一个标记预测的概率分布](Images/nlpt_0506.png)'
- en: Figure 5-6\. Probability distribution of next token prediction (upper) and cumulative
    distribution of descending token probabilities (lower)
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6. 下一个标记预测的概率分布（上）和下降标记概率的累积分布（下）
- en: In the lower plot, we’ve ordered the tokens by descending probability and calculated
    the cumulative sum of the first 10,000 tokens (in total, there are 50,257 tokens
    in GPT-2’s vocabulary). The curved line represents the probability of picking
    any of the preceding tokens. For example, there is roughly a 96% chance of picking
    any of the 1,000 tokens with the highest probability. We see that the probability
    rises quickly above 90% but saturates to close to 100% only after several thousand
    tokens. The plot shows that there is a 1 in 100 chance of not picking any of the
    tokens that are not even in the top 2,000.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们按概率降序排列了标记，并计算了前10,000个标记的累积和（GPT-2的词汇表中总共有50,257个标记）。曲线代表选择任何前面标记的概率。例如，选择具有最高概率的1,000个标记的概率大约为96%。我们看到概率在90%以上迅速上升，但只有在几千个标记后才接近100%。图表显示，有100分之一的机会不选择任何不在前2,000位的标记。
- en: Although these numbers might appear small at first sight, they become important
    because we sample once per token when generating text. So even if there is only
    a 1 in 100 or 1,000 chance, if we sample hundreds of times there is a significant
    chance of picking an unlikely token at some point—and picking such tokens when
    sampling can badly influence the quality of the generated text. For this reason,
    we generally want to avoid these very unlikely tokens. This is where top-*k* and
    top-*p* sampling come into play.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些数字乍一看可能很小，但它们变得重要，因为在生成文本时，我们每个标记只采样一次。因此，即使只有100分之一或1000分之一的几率，如果我们采样数百次，就有很大的机会在某个时候选择到一个不太可能的标记，并且在采样时选择这些标记可能会严重影响生成文本的质量。因此，我们通常希望避免这些非常不太可能的标记。这就是top-*k*和top-*p*采样发挥作用的地方。
- en: 'The idea behind top-*k* sampling is to avoid the low-probability choices by
    only sampling from the *k* tokens with the highest probability. This puts a fixed
    cut on the long tail of the distribution and ensures that we only sample from
    likely choices. Going back to [Figure 5-6](#distribution), top-*k* sampling is
    equivalent to defining a vertical line and sampling from the tokens on the left.
    Again, the `generate()` function provides an easy method to achieve this with
    the `top_k` argument:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: top-*k*采样的理念是通过仅从具有最高概率的*k*个标记中进行采样来避免低概率的选择。这对分布的长尾部分进行了固定的截断，并确保我们只从可能的选择中进行采样。回到[图5-6](#distribution)，top-*k*采样相当于定义一条垂直线，并从左侧的标记中进行采样。同样，`generate()`函数提供了一个使用`top_k`参数轻松实现这一点的方法：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is arguably the most human-looking text we’ve generated so far. But how
    do we choose *k*? The value of *k* is chosen manually and is the same for each
    choice in the sequence, independent of the actual output distribution. We can
    find a good value for *k* by looking at some text quality metrics, which we will
    explore in the next chapter—but that fixed cutoff might not be very satisfactory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是我们迄今为止生成的最接近人类的文本。但是我们如何选择*k*？*k*的值是手动选择的，并且对于序列中的每个选择都是相同的，独立于实际的输出分布。我们可以通过查看一些文本质量指标来找到*k*的合适值，我们将在下一章中探讨这些指标，但是这种固定的截断可能并不令人满意。
- en: 'An alternative is to use a *dynamic* cutoff. With nucleus or top-*p* sampling,
    instead of choosing a fixed cutoff value, we set a condition of when to cut off.
    This condition is when a certain probability mass in the selection is reached.
    Let’s say we set that value to 95%. We then order all tokens in descending order
    by probability and add one token after another from the top of the list until
    the sum of the probabilities of the selected tokens is 95%. Returning to [Figure 5-6](#distribution),
    the value for *p* defines a horizontal line on the cumulative sum of probabilities
    plot, and we sample only from tokens below the line. Depending on the output distribution,
    this could be just one (very likely) token or a hundred (more equally likely)
    tokens. At this point, you are probably not surprised that the `generate()` function
    also provides an argument to activate top-*p* sampling. Let’s try it out:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用*动态*截断。使用核或top-*p*采样时，我们不是选择固定的截断值，而是设置一个截断条件。这个条件是在选择中达到一定的概率质量时截断。假设我们将该值设置为95%。然后，我们按概率降序排列所有标记，并从列表顶部逐个添加标记，直到所选标记的概率总和达到95%。回到[图5-6](#distribution)，*p*的值定义了累积概率图上的水平线，并且我们只从线下的标记中进行采样。根据输出分布，这可能只是一个（非常可能的）标记，也可能是一百个（同等可能的）标记。在这一点上，您可能不会感到惊讶，`generate()`函数还提供了一个参数来激活top-*p*采样。让我们试一试：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Top-*p* sampling has also produced a coherent story, and this time with a new
    twist about migrations from Australia to South America. You can even combine the
    two sampling approaches to get the best of both worlds. Setting `top_k=50` and
    `top_p=0.9` corresponds to the rule of choosing tokens with a probability mass
    of 90%, from a pool of at most 50 tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Top-*p*采样也产生了一个连贯的故事，这次是关于从澳大利亚迁移到南美洲的新情节。您甚至可以结合这两种采样方法，以获得两全其美。设置`top_k=50`和`top_p=0.9`相当于选择具有90%概率质量的标记，最多从50个标记中进行选择的规则。
- en: Note
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can also apply beam search when we use sampling. Instead of selecting the
    next batch of candidate tokens greedily, we can sample them and build up the beams
    in the same way.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用采样时，我们还可以应用束搜索。我们可以对下一批候选标记进行采样并以相同的方式构建束，而不是贪婪地选择它们。
- en: Which Decoding Method Is Best?
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪种解码方法最好？
- en: Unfortunately, there is no universally “best” decoding method. Which approach
    is best will depend on the nature of the task you are generating text for. If
    you want your model to perform a precise task like arithmetic or providing an
    answer to a specific question, then you should lower the temperature or use deterministic
    methods like greedy search in combination with beam search to guarantee getting
    the most likely answer. If you want the model to generate longer texts and even
    be a bit creative, then you should switch to sampling methods and increase the
    temperature or use a mix of top-*k* and nucleus sampling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有普遍“最佳”的解码方法。哪种方法最好将取决于你为生成文本的任务的性质。如果你希望你的模型执行精确的任务，比如算术或提供对特定问题的答案，那么你应该降低温度或使用贪婪搜索与波束搜索的确定性方法来保证得到最可能的答案。如果你希望模型生成更长的文本，甚至有点创造性，那么你应该切换到采样方法，增加温度或使用top-*k*和核采样的混合。
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter we looked at text generation, which is a very different task
    from the NLU tasks we encountered previously. Generating text requires at least
    one forward pass per generated token, and even more if we use beam search. This
    makes text generation computationally demanding, and one needs the right infrastructure
    to run a text generation model at scale. In addition, a good decoding strategy
    that transforms the model’s output probabilities into discrete tokens can improve
    the text quality. Finding the best decoding strategy requires some experimentation
    and a subjective evaluation of the generated texts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了文本生成，这与我们之前遇到的NLU任务非常不同。生成文本至少需要每个生成的标记进行一次前向传递，如果使用波束搜索，甚至需要更多。这使得文本生成在计算上要求很高，需要适当的基础设施来规模化运行文本生成模型。此外，一个良好的解码策略，将模型的输出概率转换为离散标记，可以提高文本质量。找到最佳的解码策略需要一些实验和对生成的文本进行主观评价。
- en: In practice, however, we don’t want to make these decisions based on gut feeling
    alone! Like with other NLP tasks, we should choose a model performance metric
    that reflects the problem we want to solve. Unsurprisingly, there are a wide range
    of choices, and we will encounter the most common ones in the next chapter, where
    we have a look at how to train and evaluate a model for text summarization. Or,
    if you can’t wait to learn how to train a GPT-type model from scratch, you can
    skip right to [Chapter 10](ch10.xhtml#chapter_fromscratch), where we collect a
    large dataset of code and then train an autoregressive language model on it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，我们不希望仅凭直觉做出这些决定！与其他NLP任务一样，我们应该选择一个反映我们想要解决的问题的模型性能指标。毫不奇怪，有很多选择，我们将在下一章中遇到最常见的选择，在那里我们将看看如何训练和评估文本摘要模型。或者，如果你迫不及待地想学习如何从头开始训练GPT类型的模型，你可以直接跳到[第10章](ch10.xhtml#chapter_fromscratch)，在那里我们收集了大量的代码数据集，然后对其进行自回归语言模型的训练。
- en: ^([1](ch05.xhtml#idm46238719467344-marker)) This example comes from OpenAI’s
    [blog post on GPT-2](https://openai.com/blog/better-language-models).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm46238719467344-marker)) 这个例子来自OpenAI的[GPT-2博客文章](https://openai.com/blog/better-language-models)。
- en: ^([2](ch05.xhtml#idm46238719449616-marker)) However, as [Delip Rao points out](https://oreil.ly/mOM3V),
    whether Meena *intends* to tell corny jokes is a subtle question.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm46238719449616-marker)) 然而，正如[Delip Rao指出的](https://oreil.ly/mOM3V)，Meena是否*打算*讲冷笑话是一个微妙的问题。
- en: ^([3](ch05.xhtml#idm46238719316128-marker)) If you run out of memory on your
    machine, you can load a smaller GPT-2 version by replacing `model_name = "gpt-xl"`
    with `model_name = "gpt"`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm46238719316128-marker)) 如果你的机器内存不足，你可以通过将`model_name = "gpt-xl"`替换为`model_name
    = "gpt"`来加载一个较小的GPT-2版本。
- en: '^([4](ch05.xhtml#idm46238718765760-marker)) N.S. Keskar et al., [“CTRL: A Conditional
    Transformer Language Model for Controllable Generation”](https://arxiv.org/abs/1909.05858),
    (2019).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch05.xhtml#idm46238718765760-marker)) N.S. Keskar等人，[“CTRL: A Conditional
    Transformer Language Model for Controllable Generation”](https://arxiv.org/abs/1909.05858)，（2019）。'
- en: ^([5](ch05.xhtml#idm46238718164208-marker)) If you know some physics, you may
    recognize a striking resemblance to the [Boltzmann distribution](https://oreil.ly/ZsMmx).`
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm46238718164208-marker)) 如果你懂一些物理学，你可能会发现与[玻尔兹曼分布](https://oreil.ly/ZsMmx)有惊人的相似之处。
