- en: Appendix A. Introduction to PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. PyTorch简介
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An overview of the PyTorch deep learning library
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch深度学习库概述
- en: Setting up an environment and workspace for deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为深度学习设置环境和工作空间
- en: Tensors as a fundamental data structure for deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量作为深度学习的基本数据结构
- en: The mechanics of training deep neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度神经网络的机制
- en: Training models on GPUs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上训练模型
- en: This chapter is designed to equip you with the necessary skills and knowledge
    to put deep learning into practice and implement large language models (LLMs)
    from scratch.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为您提供将深度学习付诸实践和从零开始实现大型语言模型（LLMs）所需的技能和知识。
- en: We will introduce PyTorch, a popular Python-based deep learning library, which
    will be our primary tool for the remainder of this book. This chapter will also
    guide you through setting up a deep learning workspace armed with PyTorch and
    GPU support.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍PyTorch，这是一款流行的基于Python的深度学习库，它将是本书余下部分的主要工具。本章还将指导您设置一个配备PyTorch和GPU支持的深度学习工作空间。
- en: Then, you'll learn about the essential concept of tensors and their usage in
    PyTorch. We will also delve into PyTorch's automatic differentiation engine, a
    feature that enables us to conveniently and efficiently use backpropagation, which
    is a crucial aspect of neural network training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将学习张量的基本概念及其在PyTorch中的使用。我们还将*深入探讨*PyTorch的自动微分引擎，这一特性使我们能够方便高效地使用反向传播，这是神经网络训练的关键方面。
- en: Note that this chapter is meant as a primer for those who are new to deep learning
    in PyTorch. While this chapter explains PyTorch from the ground up, it's not meant
    to be an exhaustive coverage of the PyTorch library. Instead, this chapter focuses
    on the PyTorch fundamentals that we will use to implement LLMs throughout this
    book. If you are already familiar with deep learning, you may skip this appendix
    and directly move on to chapter 2, working with text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章是为那些对PyTorch深度学习不熟悉的读者准备的入门指南。虽然本章从基础讲解了PyTorch，但并不打算全面覆盖PyTorch库。相反，本章关注我们将在本书中用于实现LLMs的PyTorch基础知识。如果您已经熟悉深度学习，可以跳过此附录，直接进入第二章，处理文本数据。
- en: A.1 What is PyTorch
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 什么是PyTorch
- en: '*PyTorch* ([https://pytorch.org/](pytorch.org.html)) is an open-source Python-based
    deep learning library. According to *Papers With Code* ([https://paperswithcode.com/trends](paperswithcode.com.html)),
    a platform that tracks and analyzes research papers, PyTorch has been the most
    widely used deep learning library for research since 2019 by a wide margin. And
    according to the *Kaggle Data Science and Machine Learning Survey 2022* ([https://www.kaggle.com/c/kaggle-survey-2022](c.html)),
    the number of respondents using PyTorch is approximately 40% and constantly grows
    every year.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*PyTorch* ([https://pytorch.org/](pytorch.org.html))是一个开源的基于Python的深度学习库。根据*Papers
    With Code* ([https://paperswithcode.com/trends](paperswithcode.com.html))，一个跟踪和分析研究论文的平台，自2019年以来，PyTorch已经成为研究中使用最广泛的深度学习库。而根据*2022年Kaggle数据科学与机器学习调查*
    ([https://www.kaggle.com/c/kaggle-survey-2022](c.html))，使用PyTorch的受访者人数约为40%，并且每年不断增长。'
- en: One of the reasons why PyTorch is so popular is its user-friendly interface
    and efficiency. However, despite its accessibility, it doesn't compromise on flexibility,
    providing advanced users the ability to tweak lower-level aspects of their models
    for customization and optimization. In short, for many practitioners and researchers,
    PyTorch offers just the right balance between usability and features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch如此受欢迎的原因之一是其用户友好的界面和高效性。然而，尽管其易用性不妥协灵活性，仍为高级用户提供了调整模型底层细节以进行定制和优化的能力。简而言之，对于许多从业者和研究人员而言，PyTorch在可用性和功能之间提供了恰到好处的平衡。
- en: In the following subsections, we will define the main features PyTorch has to
    offer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们将定义PyTorch所提供的主要功能。
- en: A.1.1 The three core components of PyTorch
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.1 PyTorch的三个核心组成部分
- en: PyTorch is a relatively comprehensive library, and one way to approach it is
    to focus on its three broad components, which are summarized in figure A.1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个相对全面的库，一种方法是关注其三个广泛的组成部分，这些组成部分在图A.1中进行了总结。
- en: Figure A.1 PyTorch's three main components include a tensor library as a fundamental
    building block for computing, automatic differentiation for model optimization,
    and deep learning utility functions, making it easier to implement and train deep
    neural network models.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.1 PyTorch 的三个主要组件包括张量库，作为计算的基本构建块，自动微分用于模型优化，以及深度学习工具函数，使得实现和训练深度神经网络模型更加简便。
- en: '![](images/A__image001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image001.png)'
- en: Firstly, PyTorch is a *tensor library* that extends the concept of array-oriented
    programming library NumPy with the additional feature of accelerated computation
    on GPUs, thus providing a seamless switch between CPUs and GPUs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，PyTorch 是一个*张量库*，扩展了数组导向编程库 NumPy 的概念，并增加了在 GPU 上加速计算的功能，从而实现 CPU 和 GPU 之间的无缝切换。
- en: Secondly, PyTorch is an *automatic differentiation engine*, also known as autograd,
    which enables the automatic computation of gradients for tensor operations, simplifying
    backpropagation and model optimization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，PyTorch 是一个*自动微分引擎*，也称为 autograd，能够自动计算张量操作的梯度，从而简化反向传播和模型优化。
- en: Finally, PyTorch is a *deep learning library*, meaning that it offers modular,
    flexible, and efficient building blocks (including pre-trained models, loss functions,
    and optimizers) for designing and training a wide range of deep learning models,
    catering to both researchers and developers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PyTorch 是一个*深度学习库*，意味着它提供了模块化、灵活和高效的构建块（包括预训练模型、损失函数和优化器），用于设计和训练各种深度学习模型，满足研究人员和开发者的需求。
- en: After defining the term deep learning and installing PyTorch in the two following
    subsections, the remainder of this chapter will go over these three core components
    of PyTorch in more detail, along with hands-on code examples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义深度学习这个术语并在接下来的两个小节中安装 PyTorch 后，本章的其余部分将更详细地探讨 PyTorch 的这三个核心组件，并提供实践代码示例。
- en: A.1.2 Defining deep learning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.2 定义深度学习
- en: LLMs are often referred to as *AI* models in the news. However, as illustrated
    in the first section of chapter 1 (*1.1 What is an LLM?*) LLMs are also a type
    of deep neural network, and PyTorch is a deep learning library. Sounds confusing?
    Let's take a brief moment and summarize the relationship between these terms before
    we proceed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 通常在新闻中被称为*AI*模型。然而，如第一章（*1.1 什么是 LLM?*）的第一部分所示，LLMs 也是一种深度神经网络，而 PyTorch
    是一个深度学习库。这听起来让人困惑吗？在继续之前，我们花点时间总结一下这些术语之间的关系。
- en: AI is fundamentally about creating computer systems capable of performing tasks
    that usually require human intelligence. These tasks include understanding natural
    language, recognizing patterns, and making decisions. (Despite significant progress,
    AI is still far from achieving this level of general intelligence.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AI 的本质是创建能够执行通常需要人类智慧的任务的计算机系统。这些任务包括理解自然语言、识别模式和做出决策。（尽管取得了显著进展，AI 仍然离实现这种普遍智能的水平很远。）
- en: '*Machine learnin*g represents a subfield of AI (as illustrated in figure A.2)
    that focuses on developing and improving learning algorithms. The key idea behind
    machine learning is to enable computers to learn from data and make predictions
    or decisions without being explicitly programmed to perform the task. This involves
    developing algorithms that can identify patterns and learn from historical data
    and improve their performance over time with more data and feedback.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习*代表了 AI 的一个子领域（如图 A.2 所示），专注于开发和改进学习算法。机器学习的关键思想是使计算机能够从数据中学习，并在没有明确编程的情况下做出预测或决策。这涉及开发能够识别模式并从历史数据中学习的算法，随着数据和反馈的增加，改善其性能。'
- en: Figure A.2 Deep learning is a subcategory of machine learning that is focused
    on the implementation of deep neural networks. In turn, machine learning is a
    subcategory of AI that is concerned with algorithms that learn from data. AI is
    the broader concept of machines being able to perform tasks that typically require
    human intelligence.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.2 深度学习是机器学习的一个子类别，专注于深度神经网络的实现。反过来，机器学习是 AI 的一个子类别，关注从数据中学习的算法。AI 是机器能够执行通常需要人类智慧的任务的更广泛概念。
- en: '![](images/A__image003.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image003.png)'
- en: Machine learning has been integral in the evolution of AI, powering many of
    the advancements we see today, including LLMs. Machine learning is also behind
    technologies like recommendation systems used by online retailers and streaming
    services, email spam filtering, voice recognition in virtual assistants, and even
    self-driving cars. The introduction and advancement of machine learning have significantly
    enhanced AI's capabilities, enabling it to move beyond strict rule-based systems
    and adapt to new inputs or changing environments.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在人工智能的发展中起到了不可或缺的作用，为我们今天看到的许多进步提供了动力，包括 LLMs。机器学习还推动了在线零售商和流媒体服务所使用的推荐系统、电子邮件垃圾邮件过滤、虚拟助手中的语音识别，甚至自动驾驶汽车等技术的发展。机器学习的引入和进步显著增强了人工智能的能力，使其能够超越严格的基于规则的系统，并适应新的输入或变化的环境。
- en: '*Deep learning* is a subcategory of machine learning that focuses on the training
    and application of deep neural networks. These deep neural networks were originally
    inspired by how the human brain works, particularly the interconnection between
    many neurons. The "deep" in deep learning refers to the multiple hidden layers
    of artificial neurons or nodes that allow them to model complex, nonlinear relationships
    in the data.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是机器学习的一个子类别，专注于深度神经网络的训练和应用。这些深度神经网络最初受到人脑工作方式的启发，特别是许多神经元之间的相互连接。深度学习中的“深度”是指人工神经元或节点的多个隐藏层，使它们能够建模数据中的复杂非线性关系。'
- en: Unlike traditional machine learning techniques that excel at simple pattern
    recognition, deep learning is particularly good at handling unstructured data
    like images, audio, or text, so deep learning is particularly well suited for
    LLMs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统机器学习技术擅长简单模式识别不同，深度学习在处理非结构化数据（如图像、音频或文本）方面特别优秀，因此深度学习特别适合 LLMs。
- en: The typical predictive modeling workflow (also referred to as *supervised learning*)
    in machine learning and deep learning is summarized in figure A.3.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习中的典型预测建模工作流程（也称为*监督学习*）在图 A.3 中进行了总结。
- en: Figure A.3 The supervised learning workflow for predictive modeling consists
    of a training stage where a model is trained on labeled examples in a training
    dataset. The trained model can then be used to predict the labels of new observations.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.3 监督学习的预测建模工作流程包括一个训练阶段，在该阶段模型在标记示例的训练数据集上进行训练。然后，可以使用训练好的模型预测新观测的标签。
- en: '![](images/A__image005.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image005.png)'
- en: Using a learning algorithm, a model is trained on a training dataset consisting
    of examples and corresponding labels. In the case of an email spam classifier,
    for example, the training dataset consists of emails and their *spam* and *not-spam*
    labels that a human identified. Then, the trained model can be used on new observations
    (new emails) to predict their unknown label (*spam* or *not spam*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习算法，模型在由示例和相应标签组成的训练数据集上进行训练。例如，在电子邮件垃圾邮件分类器的案例中，训练数据集包含了电子邮件及其被人工识别的*垃圾邮件*和*非垃圾邮件*标签。然后，训练好的模型可以用于新的观测（新电子邮件），以预测其未知标签（*垃圾邮件*或*非垃圾邮件*）。
- en: Of course, we also want to add a model evaluation between the training and inference
    stages to ensure that the model satisfies our performance criteria before using
    it in a real-world application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还希望在训练和推理阶段之间添加模型评估，以确保模型在用于实际应用之前满足我们的性能标准。
- en: Note that the workflow for training and using LLMs, as we will see later in
    this book, is similar to the workflow depicted in figure A.3 if we train them
    to classify texts. And if we are interested in training LLMs for generating texts,
    which is the main focus of this book, figure A.3 still applies. In this case,
    the labels during pretraining can be derived from the text itself (the next-word
    prediction task introduced in chapter 1). And the LLM will generate entirely new
    text (instead of predicting labels) given an input prompt during inference.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练和使用大型语言模型（LLMs）的工作流程，如我们在本书后面将看到的，类似于图 A.3 中所示的工作流程，尤其是当我们训练它们以分类文本时。如果我们感兴趣的是训练
    LLMs 以生成文本，这是本书的主要重点，图 A.3 仍然适用。在这种情况下，预训练期间的标签可以从文本本身派生（第 1 章介绍的下一个词预测任务）。在推理时，LLM
    将根据输入提示生成全新的文本（而不是预测标签）。
- en: A.1.3 Installing PyTorch
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.3 安装 PyTorch
- en: PyTorch can be installed just like any other Python library or package. However,
    since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes,
    the installation may require additional explanation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 可以像其他任何 Python 库或包一样进行安装。然而，由于 PyTorch 是一个全面的库，包含 CPU 和 GPU 兼容的代码，安装可能需要额外的说明。
- en: Python version
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Python 版本
- en: Many scientific computing libraries do not immediately support the newest version
    of Python. Therefore, when installing PyTorch, it's advisable to use a version
    of Python that is one or two releases older. For instance, if the latest version
    of Python is 3.13, using Python 3.10 or 3.11 is recommended.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多科学计算库并不立即支持最新版本的 Python。因此，在安装 PyTorch 时，建议使用版本比最新版本低一到两个发布的 Python。例如，如果
    Python 的最新版本是 3.13，建议使用 Python 3.10 或 3.11。
- en: 'For instance, there are two versions of PyTorch: a leaner version that only
    supports CPU computing and a version that supports both CPU and GPU computing.
    If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally
    an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version.
    Regardless, the default command for installing PyTorch is as follows in a code
    terminal:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，PyTorch 有两个版本：一个仅支持 CPU 计算的精简版本和一个支持 CPU 和 GPU 计算的版本。如果您的机器有一个可以用于深度学习的 CUDA
    兼容 GPU（理想情况下是 NVIDIA T4、RTX 2080 Ti 或更新型号），我建议安装 GPU 版本。无论如何，安装 PyTorch 的默认命令如下，在代码终端中执行：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Suppose your computer supports a CUDA-compatible GPU. In that case, this will
    automatically install the PyTorch version that supports GPU acceleration via CUDA,
    given that the Python environment you're working on has the necessary dependencies
    (like pip) installed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的计算机支持 CUDA 兼容的 GPU。在这种情况下，如果您所使用的 Python 环境安装了必要的依赖项（如 pip），将自动安装支持 CUDA
    加速的 PyTorch 版本。
- en: AMD GPUs for deep learning
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于深度学习的 AMD GPU
- en: As of this writing, PyTorch has also added experimental support for AMD GPUs
    via ROCm. Please see [https://pytorch.org](.html) for additional instructions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，PyTorch 还通过 ROCm 增加了对 AMD GPU 的实验性支持。请参见 [https://pytorch.org](https://pytorch.org)
    以获取额外的说明。
- en: However, to explicitly install the CUDA-compatible version of PyTorch, it's
    often better to specify the CUDA you want PyTorch to be compatible with. PyTorch's
    official website ([https://pytorch.org](.html)) provides commands to install PyTorch
    with CUDA support for different operating systems as shown in figure A.4.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了明确安装兼容 CUDA 的 PyTorch 版本，通常最好指定希望 PyTorch 兼容的 CUDA 版本。PyTorch 的官方网站 ([https://pytorch.org](https://pytorch.org))
    提供了针对不同操作系统安装带 CUDA 支持的 PyTorch 的命令，如图 A.4 所示。
- en: Figure A.4 Access the PyTorch installation recommendation on [https://pytorch.org](.html)
    to customize and select the installation command for your system.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.4 访问 PyTorch 安装推荐页面 [https://pytorch.org](https://pytorch.org)，以自定义并选择适合您系统的安装命令。
- en: '![](images/A__image007.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image007.png)'
- en: (Note that the command shown in figure A.4 will also install the `torchvision`
    and `torchaudio` libraries, which are optional for this book.)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意，图 A.4 中显示的命令还将安装 `torchvision` 和 `torchaudio` 库，这些库对本书是可选的。）
- en: 'As of this writing, this book is based on PyTorch 2.0.1, so it''s recommended
    to use the following installation command to install the exact version to guarantee
    compatibility with this book:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，本书基于 PyTorch 2.0.1，因此建议使用以下安装命令来安装确切版本，以确保与本书的兼容性：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, as mentioned earlier, given your operating system, the installation
    command might slightly differ from the one shown above. Thus, I recommend visiting
    the [https://pytorch.org](pytorch.org.html) website and using the installation
    menu (see figure A4) to select the installation command for your operating system
    and replace `torch` with `torch==2.0.1` in this command.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如前面提到的，考虑到您的操作系统，安装命令可能与上述显示的略有不同。因此，我建议访问 [https://pytorch.org](https://pytorch.org)
    网站，并使用安装菜单（见图 A4）选择适合您操作系统的安装命令，并在该命令中将 `torch` 替换为 `torch==2.0.1`。
- en: 'To check the version of PyTorch, you can execute the following code in PyTorch:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 PyTorch 的版本，您可以在 PyTorch 中执行以下代码：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This prints:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyTorch and Torch
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PyTorch 和 Torch
- en: Note that the Python library is named "torch" primarily because it's a continuation
    of the Torch library but adapted for Python (hence, "PyTorch"). The name "torch"
    acknowledges the library's roots in Torch, a scientific computing framework with
    wide support for machine learning algorithms, which was initially created using
    the Lua programming language.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Python 库名为“torch”，主要是因为它是 Torch 库的延续，但适用于 Python（因此称为“PyTorch”）。名称“torch”承认该库在
    Torch 中的根基，Torch 是一个广泛支持机器学习算法的科学计算框架，最初使用 Lua 编程语言创建。
- en: If you are looking for additional recommendations and instructions for setting
    up your Python environment or installing the other libraries used later in this
    book, I recommend visiting the supplementary GitHub repository of this book at
    [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找有关设置 Python 环境或安装本书后面使用的其他库的额外建议和说明，我建议访问本书的补充 GitHub 存储库 [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)。
- en: 'After installing PyTorch, you can check whether your installation recognizes
    your built-in NVIDIA GPU by running the following code in Python:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 PyTorch 后，你可以通过在 Python 中运行以下代码来检查你的安装是否识别内置的 NVIDIA GPU：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This returns:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If the command returns True, you are all set. If the command returns False,
    your computer may not have a compatible GPU, or PyTorch does not recognize it.
    While GPUs are not required for the initial chapters in this book, which are focused
    on implementing LLMs for educational purposes, they can significantly speed up
    deep learning-related computations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令返回 True，说明你一切准备就绪。如果命令返回 False，你的计算机可能没有兼容的 GPU，或者 PyTorch 无法识别它。虽然本书前面的章节集中于实现
    LLMs 以供教育目的，并不要求使用 GPU，但它们可以显著加速与深度学习相关的计算。
- en: If you don't have access to a GPU, there are several cloud computing providers
    where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like
    environment is Google Colab ([https://colab.research.google.com](colab.research.google.com.html)),
    which provides time-limited access to GPUs as of this writing. Using the "Runtime"
    menu, it is possible to select a GPU, as shown in the screenshot in figure A.5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有 GPU，可以使用几个云计算提供商，在每小时收费的基础上运行 GPU 计算。一个流行的类似 Jupyter Notebook 的环境是 Google
    Colab（[https://colab.research.google.com](https://colab.research.google.com)），它在撰写本文时提供时间限制的
    GPU 访问。使用“运行时”菜单，可以选择 GPU，如图 A.5 所示的屏幕截图。
- en: Figure A.5 Select a GPU device for Google Colab under the *Runtime/Change runtime
    type* menu.
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.5 在 *运行时/更改运行时类型* 菜单中为 Google Colab 选择 GPU 设备。
- en: '![](images/A__image009.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image009.png)'
- en: PyTorch on Apple Silicon
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 Apple Silicon 上的 PyTorch
- en: 'If you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or
    newer models), you have the option to leverage its capabilities to accelerate
    PyTorch code execution. To use your Apple Silicon chip for PyTorch, you first
    need to install PyTorch as you normally would. Then, to check if your Mac supports
    PyTorch acceleration with its Apple Silicon chip, you can run a simple code snippet
    in Python:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一台配备 Apple Silicon 芯片（如 M1、M2、M3 或更新型号）的 Apple Mac，你可以利用其功能加速 PyTorch 代码执行。要在
    PyTorch 中使用你的 Apple Silicon 芯片，你需要像平常一样安装 PyTorch。然后，为了检查你的 Mac 是否支持利用 Apple Silicon
    芯片加速 PyTorch，你可以在 Python 中运行一个简单的代码片段：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If it returns `True`, it means that your Mac has an Apple Silicon chip that
    can be used to accelerate PyTorch code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果返回 `True`，这意味着你的 Mac 具有可以用于加速 PyTorch 代码的 Apple Silicon 芯片。
- en: Exercise A.1
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 A.1
- en: Install and set up PyTorch on your computer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的计算机上安装和设置 PyTorch。
- en: Exercise A.2
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 A.2
- en: Run the supplementary Chapter 2 code at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html)
    that checks whether your environment is set up correctly..
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
    运行补充的第 2 章代码，检查你的环境是否设置正确。
- en: A.2 Understanding tensors
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2 理解张量
- en: Tensors represent a mathematical concept that generalizes vectors and matrices
    to potentially higher dimensions. In other words, tensors are mathematical objects
    that can be characterized by their order (or rank), which provides the number
    of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a
    vector is a tensor of rank 1, and a matrix is a tensor of rank 2, as illustrated
    in figure A.6
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 张量代表一个数学概念，它将向量和矩阵推广到更高的维度。换句话说，张量是可以通过其阶（或秩）来表征的数学对象，这提供了维度的数量。例如，标量（仅为一个数字）是秩为
    0 的张量，向量是秩为 1 的张量，矩阵是秩为 2 的张量，如图 A.6 所示。
- en: Figure A.6 An illustration of tensors with different ranks. Here 0D corresponds
    to rank 0, 1D to rank 1, and 2D to rank 2\. Note that a 3D vector, which consists
    of 3 elements, is still a rank 1 tensor.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.6展示了不同秩的张量的示例。这里0D对应秩0，1D对应秩1，2D对应秩2。请注意，由3个元素组成的3D向量仍然是秩1的张量。
- en: '![](images/A__image011.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image011.png)'
- en: From a computational perspective, tensors serve as data containers. For instance,
    they hold multi-dimensional data, where each dimension represents a different
    feature. Tensor libraries, such as PyTorch, can create, manipulate, and compute
    with these multi-dimensional arrays efficiently. In this context, a tensor library
    functions as an array library.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算的角度来看，张量作为数据容器。例如，它们存储多维数据，每个维度代表不同的特征。张量库，如PyTorch，可以高效地创建、操作和计算这些多维数组。在这个背景下，张量库的功能类似于数组库。
- en: PyTorch tensors are similar to NumPy arrays but have several additional features
    important for deep learning. For example, PyTorch adds an automatic differentiation
    engine, simplifying *computing gradients*, as discussed later in section 2.4\.
    PyTorch tensors also support GPU computations to speed up deep neural network
    training, which we will discuss later in section 2.8.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量类似于NumPy数组，但具有对深度学习重要的多个附加功能。例如，PyTorch添加了自动微分引擎，简化了*计算梯度*的过程，如第2.4节后面讨论的那样。PyTorch张量还支持GPU计算，以加速深度神经网络的训练，我们将在第2.8节中讨论。
- en: PyTorch's has a NumPy-like API
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PyTorch具有类似NumPy的API
- en: 'As you will see in the upcoming sections, PyTorch adopts most of the NumPy
    array API and syntax for its tensor operations. If you are new to NumPy, you can
    get a brief overview of the most relevant concepts via my article Scientific Computing
    in Python: Introduction to NumPy and Matplotlib at [https://sebastianraschka.com/blog/2020/numpy-intro.html](2020.html).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在接下来的章节中看到的，PyTorch在张量操作中采用了大多数NumPy数组API和语法。如果你对NumPy不太熟悉，可以通过我的文章《Python中的科学计算：NumPy和Matplotlib入门》获得对最相关概念的简要概述，链接为[https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html)。
- en: The following subsections will look at the basic operations of the PyTorch tensor
    library, showing how to create simple tensors and going over some of the essential
    operations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将探讨PyTorch张量库的基本操作，展示如何创建简单张量并介绍一些基本操作。
- en: A.2.1 Scalars, vectors, matrices, and tensors
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.1 标量、向量、矩阵和张量
- en: As mentioned earlier, PyTorch tensors are data containers for array-like structures.
    A scalar is a 0-dimensional tensor (for instance, just a number), a vector is
    a 1-dimensional tensor, and a matrix is a 2-dimensional tensor. There is no specific
    term for higher-dimensional tensors, so we typically refer to a 3-dimensional
    tensor as just a 3D tensor, and so forth.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PyTorch张量是数组结构的数据容器。标量是0维张量（例如，仅一个数字），向量是1维张量，矩阵是2维张量。对于更高维的张量，没有特定的术语，因此我们通常将3维张量称为3D张量，以此类推。
- en: 'We can create objects of PyTorch''s `Tensor` class using the `torch.tensor`
    function as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torch.tensor`函数创建PyTorch的`Tensor`类对象，如下所示：
- en: Listing A.1 Creating PyTorch tensors
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.1 创建PyTorch张量
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A.2.2 Tensor data types
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.2 张量数据类型
- en: 'In the previous section, we created tensors from Python integers. In this case,
    PyTorch adopts the default 64-bit integer data type from Python. We can access
    the data type of a tensor via the `.dtype` attribute of a tensor:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们从Python整数创建了张量。在这种情况下，PyTorch采用Python的默认64位整数数据类型。我们可以通过张量的`.dtype`属性访问张量的数据类型：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This prints:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we create tensors from Python floats, PyTorch creates tensors with a 32-bit
    precision by default, as we can see below:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从Python浮点数创建张量，PyTorch默认会创建32位精度的张量，正如下文所示：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This choice is primarily due to the balance between precision and computational
    efficiency. A 32-bit floating point number offers sufficient precision for most
    deep learning tasks, while consuming less memory and computational resources than
    a 64-bit floating point number. Moreover, GPU architectures are optimized for
    32-bit computations, and using this data type can significantly speed up model
    training and inference.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一选择主要是由于精度和计算效率之间的平衡。32位浮点数对大多数深度学习任务提供了足够的精度，同时占用的内存和计算资源少于64位浮点数。此外，GPU架构已针对32位计算进行优化，使用这种数据类型可以显著加快模型训练和推理的速度。
- en: 'Moreover, it is possible to readily change the precision using a tensor''s
    `.to` method. The following code demonstrates this by changing a 64-bit integer
    tensor into a 32-bit float tensor:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用张量的`.to`方法轻松更改精度。以下代码演示了如何将64位整数张量更改为32位浮点张量：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This returns:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For more information about different tensor data types available in PyTorch,
    I recommend checking the official documentation at [https://pytorch.org/docs/stable/tensors.html](stable.html).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PyTorch中不同张量数据类型的更多信息，我建议查看官方文档 [https://pytorch.org/docs/stable/tensors.html](stable.html)。
- en: A.2.3 Common PyTorch tensor operations
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.3 常见的PyTorch张量操作
- en: Comprehensive coverage of all the different PyTorch tensor operations and commands
    is outside the scope of this book. However, we will briefly describe relevant
    operations as we introduce them throughout the book.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有不同PyTorch张量操作和命令的全面覆盖超出了本书的范围。然而，我们将在书中介绍时简要描述相关操作。
- en: Before we move on to the next section covering the concept behind computation
    graphs, below is a list of the most essential PyTorch tensor operations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一个涵盖计算图背后概念的部分之前，下面是最基本的PyTorch张量操作列表。
- en: We already introduced the `torch.tensor()` function to create new tensors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了`torch.tensor()`函数来创建新的张量。
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This prints:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In addition, the `.shape` attribute allows us to access the shape of a tensor:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`.shape`属性允许我们访问张量的形状：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see above, `.shape` returns `[2, 3]`, which means that the tensor
    has 2 rows and 3 columns. To reshape the tensor into a 3 by 2 tensor, we can use
    the `.reshape` method:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，`.shape`返回`[2, 3]`，这意味着张量有2行3列。要将张量重塑为3乘2的张量，我们可以使用`.reshape`方法：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This prints:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'However, note that the more common command for reshaping tensors in PyTorch
    is `.view()`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，请注意，PyTorch中更常见的张量重塑命令是`.view()`：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Similar to `.reshape` and `.view`, there are several cases where PyTorch offers
    multiple syntax options for executing the same computation. This is because PyTorch
    initially followed the original Lua Torch syntax convention but then also added
    syntax to make it more similar to NumPy upon popular request.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`.reshape`和`.view`，PyTorch在执行相同计算时提供了多种语法选项。这是因为PyTorch最初遵循原始Lua Torch语法约定，但在广泛请求下也增加了与NumPy更相似的语法。
- en: 'Next, we can use `.T` to transpose a tensor, which means flipping it across
    its diagonal. Note that this is similar from reshaping a tensor as you can see
    based on the result below:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`.T`来转置一个张量，这意味着在其对角线上翻转它。请注意，这与重塑张量类似，正如您在下面的结果中看到的：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, the common way to multiply two matrices in PyTorch is the `.matmul`
    method:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在PyTorch中乘以两个矩阵的常见方式是`.matmul`方法：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'However, we can also adopt the `@` operator, which accomplishes the same thing
    more compactly:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们也可以采用`@`运算符，它以更紧凑的方式实现相同的功能：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This prints:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As mentioned earlier, we will introduce additional operations throughout this
    book when needed. For readers who''d like to browse through all the different
    tensor operations available in PyTorch (hint: we won''t need most of these), I
    recommend checking out the official documentation at [https://pytorch.org/docs/stable/tensors.html](stable.html).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将在本书中根据需要介绍额外的操作。对于希望浏览PyTorch中所有不同张量操作的读者（提示：我们不需要其中的大部分），我推荐查看官方文档
    [https://pytorch.org/docs/stable/tensors.html](stable.html)。
- en: A.3 Seeing models as computation graphs
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3 将模型视为计算图
- en: In the previous section, we covered one of the major three components of PyTorch,
    namely, its tensor library. Next in line is PyTorch's automatic differentiation
    engine, also known as autograd. PyTorch's autograd system provides functions to
    compute gradients in dynamic computational graphs automatically. But before we
    dive deeper into computing gradients in the next section, let's define the concept
    of a computational graph.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了PyTorch的三个主要组成部分之一，即其张量库。接下来是PyTorch的自动微分引擎，也称为autograd。PyTorch的autograd系统提供了在动态计算图中自动计算梯度的功能。但是在我们深入探讨下一节的梯度计算之前，让我们定义一下计算图的概念。
- en: A computational graph (or computation graph in short) is a directed graph that
    allows us to express and visualize mathematical expressions. In the context of
    deep learning, a computation graph lays out the sequence of calculations needed
    to compute the output of a neural network -- we will need this later to compute
    the required gradients for backpropagation, which is the main training algorithm
    for neural networks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图（或简称计算图）是一个有向图，允许我们表达和可视化数学表达式。在深度学习的背景下，计算图展示了计算神经网络输出所需的计算顺序——我们稍后需要这个图来计算反向传播所需的梯度，这是神经网络的主要训练算法。
- en: 'Let''s look at a concrete example to illustrate the concept of a computation
    graph. The following code implements the forward pass (prediction step) of a simple
    logistic regression classifier, which can be seen as a single-layer neural network,
    returning a score between 0 and 1 that is compared to the true class label (0
    or 1) when computing the loss:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子来阐明计算图的概念。以下代码实现了一个简单逻辑回归分类器的前向传播（预测步骤），可以视为一个单层神经网络，返回一个在0和1之间的分数，该分数在计算损失时与真实类别标签（0或1）进行比较：
- en: Listing A.2 A logistic regression forward pass
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单A.2 逻辑回归前向传播
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If not all components in the code above make sense to you, don't worry. The
    point of this example is not to implement a logistic regression classifier but
    rather to illustrate how we can think of a sequence of computations as a computation
    graph, as shown in figure A.7.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述代码中的所有组件对你来说不太清楚，不用担心。这个例子的重点不是实现逻辑回归分类器，而是阐明我们如何将一系列计算视为计算图，如图A.7所示。
- en: Figure A.7 A logistic regression forward pass as a computation graph. The input
    feature *x*[1] is multiplied by a model weight *w*[1] and passed through an activation
    function *σ* after adding the bias. The loss is computed by comparing the model
    output *a* with a given label *y*.
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.7 逻辑回归前向传播作为计算图。输入特征*x*[1]与模型权重*w*[1]相乘，并在加上偏置后通过激活函数*σ*。通过将模型输出*a*与给定标签*y*进行比较来计算损失。
- en: '![](images/A__image013.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image013.png)'
- en: In fact, PyTorch builds such a computation graph in the background, and we can
    use this to calculate gradients of a loss function with respect to the model parameters
    (here w1 and b) to train the model, which is the topic of the upcoming sections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，PyTorch在后台构建这样的计算图，我们可以利用它来计算损失函数相对于模型参数（这里是w1和b）的梯度，以训练模型，这是接下来部分的主题。
- en: A.4 Automatic differentiation made easy
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.4 自动微分简易入门
- en: In the previous section, we introduced the concept of computation graphs. If
    we carry out computations in PyTorch, it will build such a graph internally by
    default if one of its terminal nodes has the `requires_grad` attribute set to
    `True`. This is useful if we want to compute gradients. Gradients are required
    when training neural networks via the popular backpropagation algorithm, which
    can be thought of as an implementation of the *chain rule* from calculus for neural
    networks, which is illustrated in figure A.8.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了计算图的概念。如果我们在PyTorch中进行计算，如果其一个终端节点的`requires_grad`属性被设置为`True`，它将默认在内部构建这样的图。这在我们想计算梯度时非常有用。训练神经网络时需要梯度，通过流行的反向传播算法实现，可以视为神经网络中的微积分*链式法则*的实现，图A.8对此进行了说明。
- en: Figure A.8 The most common way of computing the loss gradients in a computation
    graph involves applying the chain rule from right to left, which is also called
    reverse-model automatic differentiation or backpropagation. It means we start
    from the output layer (or the loss itself) and work backward through the network
    to the input layer. This is done to compute the gradient of the loss with respect
    to each parameter (weights and biases) in the network, which informs how we update
    these parameters during training.
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.8 在计算图中计算损失梯度的最常见方法是从右到左应用链式法则，这也称为反向模型自动微分或反向传播。这意味着我们从输出层（或损失本身）开始，向后通过网络到达输入层。这样做是为了计算损失相对于网络中每个参数（权重和偏置）的梯度，从而告知我们在训练过程中如何更新这些参数。
- en: '![](images/A__image015.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image015.png)'
- en: Partial derivatives and gradients
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏导数和梯度
- en: Figure A.8 shows partial derivatives, which measure the rate at which a function
    changes with respect to one of its variables. A gradient is a vector containing
    all of the partial derivatives of a multivariate function, a function with more
    than one variable as input.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.8 显示了偏导数，它们测量函数相对于其变量之一的变化率。梯度是一个向量，包含了多元函数的所有偏导数，即输入变量不止一个的函数。
- en: If you are not familiar or don't remember the partial derivatives, gradients,
    or the chain rule from calculus, don't worry. On a high level, all you need to
    know for this book is that the chain rule is a way to compute gradients of a loss
    function with respect to the model's parameters in a computation graph. This provides
    the information needed to update each parameter in a way that minimizes the loss
    function, which serves as a proxy for measuring the model's performance, using
    a method such as gradient descent. We will revisit the computational implementation
    of this training loop in PyTorch in section 2.7, *A typical training loop*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对偏导数、梯度或微积分的链式法则不熟悉或记不清，不用担心。总体来说，本书中你需要知道的是，链式法则是一种计算损失函数相对于模型参数在计算图中梯度的方法。这提供了更新每个参数所需的信息，以最小化损失函数，损失函数用来作为衡量模型性能的代理，使用梯度下降等方法。我们将在第
    2.7 节 *典型的训练循环* 中重新审视 PyTorch 中这一训练循环的计算实现。
- en: 'Now, how is this all related to the second component of the PyTorch library
    we mentioned earlier, the automatic differentiation (autograd) engine? By tracking
    every operation performed on tensors, PyTorch''s autograd engine constructs a
    computational graph in the background. Then, calling the `grad` function, we can
    compute the gradient of the loss with respect to model parameter `w1` as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这与我们之前提到的 PyTorch 库的第二个组件，即自动微分（autograd）引擎有何关系呢？通过跟踪对张量执行的每个操作，PyTorch 的
    autograd 引擎在后台构建了一个计算图。然后，通过调用 `grad` 函数，我们可以计算损失相对于模型参数 `w1` 的梯度，具体如下：
- en: Listing A.3 Computing gradients via autograd
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.3 通过 autograd 计算梯度
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s show the resulting values of the loss with respect to the model''s parameters:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示损失相对于模型参数的结果值：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The prints:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Above, we have been using the grad function "manually," which can be useful
    for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch
    provides even more high-level tools to automate this process. For instance, we
    can call `.backward` on the loss, and PyTorch will compute the gradients of all
    the leaf nodes in the graph, which will be stored via the tensors'' `.grad` attributes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以上，我们一直在“手动”使用 grad 函数，这对实验、调试和概念演示非常有用。但在实践中，PyTorch 提供了更高级的工具来自动化这一过程。例如，我们可以在损失上调用
    `.backward`，PyTorch 将计算图中所有叶子节点的梯度，这些梯度将通过张量的 `.grad` 属性存储：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The outputs are:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If this section is packed with a lot of information and you may be overwhelmed
    by the calculus concepts, don't worry. While this calculus jargon was a means
    to explain PyTorch's autograd component, all you need to take away from this section
    is that PyTorch takes care of the calculus for us via the `.backward` method --
    we won't need to compute any derivatives or gradients by hand in this book.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一部分的信息量很大，而你对微积分的概念感到困惑，不用担心。虽然这些微积分术语是用来解释 PyTorch 的 autograd 组件，但你只需记住这一部分的关键是
    PyTorch 通过 `.backward` 方法为我们处理微积分——在本书中我们不需要手动计算任何导数或梯度。
- en: A.5 Implementing multilayer neural networks
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.5 实现多层神经网络
- en: In the previous sections, we covered PyTorch's tensor and autograd components.
    This section focuses on PyTorch as a library for implementing deep neural networks.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的部分中，我们讨论了 PyTorch 的张量和 autograd 组件。本节重点介绍 PyTorch 作为实现深度神经网络的库。
- en: To provide a concrete example, we focus on a multilayer perceptron, which is
    a fully connected neural network, as illustrated in figure A.9.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个具体的例子，我们关注于多层感知器，这是一种全连接神经网络，如图 A.9 所示。
- en: Figure A.9 An illustration of a multilayer perceptron with 2 hidden layers.
    Each node represents a unit in the respective layer. Each layer has only a very
    small number of nodes for illustration purposes.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.9 展示了一个具有 2 个隐藏层的多层感知器的示意图。每个节点代表相应层中的一个单元。为了说明，每层只有非常少量的节点。
- en: '![](images/A__image017.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image017.png)'
- en: When implementing a neural network in PyTorch, we typically subclass the `torch.nn.Module`
    class to define our own custom network architecture. This `Module` base class
    provides a lot of functionality, making it easier to build and train models. For
    instance, it allows us to encapsulate layers and operations and keep track of
    the model's parameters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现神经网络时，我们通常会子类化`torch.nn.Module`类以定义我们自己的自定义网络架构。这个`Module`基类提供了许多功能，使构建和训练模型变得更容易。例如，它允许我们封装层和操作，并跟踪模型的参数。
- en: Within this subclass, we define the network layers in the `__init__` constructor
    and specify how they interact in the forward method. The forward method describes
    how the input data passes through the network and comes together as a computation
    graph.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子类中，我们在`__init__`构造函数中定义网络层，并指定它们在前向方法中的交互。前向方法描述了输入数据如何通过网络，并作为计算图结合在一起。
- en: In contrast, the backward method, which we typically do not need to implement
    ourselves, is used during training to compute gradients of the loss function with
    respect to the model parameters, as we will see in section 2.7, *A typical training
    loop*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，反向方法通常不需要我们自己实现，在训练期间用于计算损失函数相对于模型参数的梯度，正如我们将在2.7节中看到的，*典型的训练循环*。
- en: 'The following code implements a classic multilayer perceptron with two hidden
    layers to illustrate a typical usage of the `Module` class:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了一个经典的具有两个隐藏层的多层感知机，以说明`Module`类的典型用法：
- en: Listing A.4 A multilayer perceptron with two hidden layers
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.4 一个具有两个隐藏层的多层感知机
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can then instantiate a new neural network object as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以如下实例化一个新的神经网络对象：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'But before using this new `model` object, it is often useful to call `print`
    on the model to see a summary of its structure:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 但在使用这个新的`model`对象之前，通常调用`print`以查看模型结构的摘要是很有用的：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This prints:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that we used the `Sequential` class when we implemented the `NeuralNetwork`
    class. Using Sequential is not required, but it can make our life easier if we
    have a series of layers that we want to execute in a specific order, as is the
    case here. This way, after instantiating `self.layers = Sequential(...)` in the
    `__init__` constructor, we just have to call the `self.layers` instead of calling
    each layer individually in the `NeuralNetwork`'s `forward` method.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在实现`NeuralNetwork`类时，我们使用了`Sequential`类。使用Sequential不是必须的，但如果我们有一系列希望按特定顺序执行的层，这会让我们的生活更轻松，就像这里的情况一样。这样，在`__init__`构造函数中实例化`self.layers
    = Sequential(...)`后，我们只需调用`self.layers`，而不是在`NeuralNetwork`的`forward`方法中逐个调用每层。
- en: 'Next, let''s check the total number of trainable parameters of this model:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查该模型的可训练参数总数：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This prints:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that each parameter for which `requires_grad=True` counts as a trainable
    parameter and will be updated during training (more on that later in section 2.7,
    *A typical training loop*).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`requires_grad=True`的每个参数都被视为一个可训练参数，并将在训练期间更新（稍后在2.7节中会详细介绍，*典型的训练循环*）。
- en: In the case of our neural network model with the two hidden layers above, these
    trainable parameters are contained in the `torch.nn.Linear` layers. A *linear*
    layer multiplies the inputs with a weight matrix and adds a bias vector. This
    is sometimes also referred to as a *feedforward* or *fully connected* layer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面的两个隐藏层的神经网络模型中，这些可训练参数包含在`torch.nn.Linear`层中。一个*线性*层将输入与权重矩阵相乘，并添加一个偏置向量。这有时也被称为*前馈*层或*全连接*层。
- en: 'Based on the `print(model)` call we executed above, we can see that the first
    `Linear` layer is at index position 0 in the layers attribute. We can access the
    corresponding weight parameter matrix as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们上面执行的`print(model)`调用，我们可以看到第一个`Linear`层位于层属性的索引位置0。我们可以如下访问相应的权重参数矩阵：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This prints:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Since this is a large matrix that is not shown in its entirety, let''s use
    the `.shape` attribute to show its dimensions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个未完全显示的大矩阵，让我们使用`.shape`属性来显示它的维度：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The result is:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: (Similarly, you could access the bias vector via `model.layers[0].bias`.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: （同样，你可以通过`model.layers[0].bias`访问偏置向量。）
- en: The weight matrix above is a 30x50 matrix, and we can see that the `requires_grad`
    is set to `True`, which means its entries are trainable -- this is the default
    setting for weights and biases in `torch.nn.Linear`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的权重矩阵是一个30x50的矩阵，我们可以看到`requires_grad`被设置为`True`，这意味着其条目是可训练的——这是`torch.nn.Linear`中权重和偏置的默认设置。
- en: Note that if you execute the code above on your computer, the numbers in the
    weight matrix will likely differ from those shown above. This is because the model
    weights are initialized with small random numbers, which are different each time
    we instantiate the network. In deep learning, initializing model weights with
    small random numbers is desired to break symmetry during training -- otherwise,
    the nodes would be just performing the same operations and updates during backpropagation,
    which would not allow the network to learn complex mappings from inputs to outputs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您在计算机上执行上面的代码，权重矩阵中的数字可能与上述数字不同。这是因为模型权重是用小随机数初始化的，每次实例化网络时都不同。在深度学习中，用小随机数初始化模型权重是为了在训练期间打破对称性——否则，节点将在反向传播期间执行相同的操作和更新，这将不允许网络学习输入到输出的复杂映射。
- en: 'However, while we want to keep using small random numbers as initial values
    for our layer weights, we can make the random number initialization reproducible
    by seeding PyTorch''s random number generator via `manual_seed`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管我们希望继续使用小随机数作为层权重的初始值，但我们可以通过使用`manual_seed`为PyTorch的随机数生成器设置种子来使随机数初始化可重现：
- en: '[PRE44]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result is:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, after we spent some time inspecting the `NeuraNetwork` instance, let''s
    briefly see how it''s used via the forward pass:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在花了一些时间检查`NeuraNetwork`实例后，让我们简要看看它是如何通过前向传播使用的：
- en: '[PRE46]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The result is:tensor([[-0.1262, 0.1080, -0.1792]], `grad_fn=<AddmmBackward0>)`
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：tensor([[-0.1262, 0.1080, -0.1792]], `grad_fn=<AddmmBackward0>`)
- en: In the code above, we generated a single random training example `X` as a toy
    input (note that our network expects 50-dimensional feature vectors) and fed it
    to the model, returning three scores. When we call `model(x)`, it will automatically
    execute the forward pass of the model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们生成了一个单一的随机训练示例`X`作为玩具输入（注意我们的网络期望50维特征向量），并将其输入到模型中，返回三个分数。当我们调用`model(x)`时，它会自动执行模型的前向传播。
- en: The forward pass refers to calculating output tensors from input tensors. This
    involves passing the input data through all the neural network layers, starting
    from the input layer, through hidden layers, and finally to the output layer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是指从输入张量计算输出张量。这涉及将输入数据通过所有神经网络层，开始于输入层，通过隐藏层，最终到达输出层。
- en: These three numbers returned above correspond to a score assigned to each of
    the three output nodes. Notice that the output tensor also includes a `grad_fn`
    value.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上面返回的这三个数字对应于分配给三个输出节点的分数。注意，输出张量也包含一个`grad_fn`值。
- en: Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute
    a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>`
    means that the tensor we are inspecting was created via a matrix multiplication
    and addition operation. PyTorch will use this information when it computes gradients
    during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>`
    specifies the operation that was performed. In this case, it is an `Addmm` operation.
    `Addmm` stands for matrix multiplication (`mm`) followed by an addition (`Add`).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`grad_fn=<AddmmBackward0>`表示在计算图中计算变量时最后使用的函数。特别地，`grad_fn=<AddmmBackward0>`意味着我们正在检查的张量是通过矩阵乘法和加法操作创建的。PyTorch将在反向传播期间计算梯度时使用此信息。`grad_fn=<AddmmBackward0>`中的`<AddmmBackward0>`部分指定了执行的操作。在这种情况下，它是一个`Addmm`操作。`Addmm`代表矩阵乘法（`mm`）后接加法（`Add`）。
- en: If we just want to use a network without training or backpropagation, for example,
    if we use it for prediction after training, constructing this computational graph
    for backpropagation can be wasteful as it performs unnecessary computations and
    consumes additional memory. So, when we use a model for inference (for instance,
    making predictions) rather than training, it is a best practice to use the `torch.no_grad()`
    context manager, as shown below. This tells PyTorch that it doesn't need to keep
    track of the gradients, which can result in significant savings in memory and
    computation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只是想使用一个网络而不进行训练或反向传播，例如，如果我们在训练后用于预测，那么为反向传播构建这个计算图可能是浪费，因为它会执行不必要的计算并消耗额外的内存。因此，当我们使用模型进行推理（例如，进行预测）而不是训练时，使用`torch.no_grad()`上下文管理器是一种最佳实践，如下所示。这告诉PyTorch它不需要跟踪梯度，这可以显著节省内存和计算。
- en: '[PRE47]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The result is:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE48]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In PyTorch, it''s common practice to code models such that they return the
    outputs of the last layer (logits) without passing them to a nonlinear activation
    function. That''s because PyTorch''s commonly used loss functions combine the
    softmax (or sigmoid for binary classification) operation with the negative log-likelihood
    loss in a single class. The reason for this is numerical efficiency and stability.
    So, if we want to compute class-membership probabilities for our predictions,
    we have to call the `softmax` function explicitly:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，常见的做法是编写模型，使其返回最后一层的输出（logits），而不将其传递给非线性激活函数。这是因为 PyTorch 常用的损失函数将
    softmax（对于二分类为 sigmoid）操作与负对数似然损失结合在一个类中。这样做的原因是为了数值效率和稳定性。因此，如果我们想为预测计算类别成员概率，就必须显式调用
    `softmax` 函数：
- en: '[PRE49]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This prints:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE50]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The values can now be interpreted as class-membership probabilities that sum
    up to 1\. The values are roughly equal for this random input, which is expected
    for a randomly initialized model without training.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值现在可以解释为类别成员概率，总和为 1。这些值对于这个随机输入大致相等，这是对于一个没有经过训练的随机初始化模型的预期结果。
- en: In the following two sections, we will learn how to set up an efficient data
    loader and train the model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将学习如何设置高效的数据加载器并训练模型。
- en: A.6 Setting up efficient data loaders
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.6 设置高效的数据加载器
- en: In the previous section, we defined a custom neural network model. Before we
    can train this model, we have to briefly talk about creating efficient data loaders
    in PyTorch, which we will iterate over when training the model. The overall idea
    behind data loading in PyTorch is illustrated in figure A.10.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们定义了一个自定义神经网络模型。在训练该模型之前，我们需要简要讨论在 PyTorch 中创建高效数据加载器的问题，我们将在训练模型时迭代这些加载器。数据加载的整体思路在图
    A.10 中进行了说明。
- en: Figure A.10 PyTorch implements a Dataset and a DataLoader class. The Dataset
    class is used to instantiate objects that define how each data record is loaded.
    The DataLoader handles how the data is shuffled and assembled into batches.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.10 PyTorch 实现了 Dataset 和 DataLoader 类。Dataset 类用于实例化定义每个数据记录如何加载的对象。DataLoader
    处理数据的洗牌和组装成批次的方式。
- en: '![](images/A__image019.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image019.png)'
- en: Following the illustration in figure A.10, in this section, we will implement
    a custom Dataset class that we will use to create a training and a test dataset
    that we'll then use to create the data loaders.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 A.10 中的示例，在本节中，我们将实现一个自定义 Dataset 类，用于创建一个训练数据集和一个测试数据集，然后我们将用它们来创建数据加载器。
- en: 'Let''s start by creating a simple toy dataset of five training examples with
    two features each. Accompanying the training examples, we also create a tensor
    containing the corresponding class labels: three examples below to class 0, and
    two examples belong to class 1\. In addition, we also make a test set consisting
    of two entries. The code to create this dataset is shown below.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先创建一个简单的玩具数据集，包含五个训练示例，每个示例有两个特征。与训练示例相伴，我们还创建一个张量，包含相应的类标签：三个示例属于类 0，两个示例属于类
    1。此外，我们还创建了一个包含两个条目的测试集。创建该数据集的代码如下所示。
- en: Listing A.5 Creating a small toy dataset
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.5 创建一个小型玩具数据集
- en: '[PRE51]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Class label numbering
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类标签编号
- en: PyTorch requires that class labels start with label 0, and the largest class
    label value should not exceed the number of output nodes minus 1 (since Python
    index counting starts at 0\. So, if we have class labels 0, 1, 2, 3, and 4, the
    neural network output layer should consist of 5 nodes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 要求类标签从标签 0 开始，最大类标签值不得超过输出节点数量减 1（因为 Python 的索引从 0 开始）。因此，如果我们有类标签 0、1、2、3
    和 4，神经网络输出层应由 5 个节点组成。
- en: Next, we create a custom dataset class, `ToyDataset`, by subclassing from PyTorch's
    `Dataset` parent class, as shown below.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过从 PyTorch 的 `Dataset` 父类继承来创建一个自定义数据集类 `ToyDataset`，如下所示。
- en: Listing A.6 Defining a custom Dataset class
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.6 定义自定义 Dataset 类
- en: '[PRE52]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This custom `ToyDataset` class's purpose is to use it to instantiate a PyTorch
    `DataLoader`. But before we get to this step, let's briefly go over the general
    structure of the `ToyDataset` code.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自定义的 `ToyDataset` 类的目的是将其用于实例化 PyTorch 的 `DataLoader`。但在我们进入这一步之前，让我们简要回顾一下
    `ToyDataset` 代码的一般结构。
- en: In PyTorch, the three main components of a custom Dataset class are the `__init__`
    constructor, the `__getitem__` method, and the `__len__` method, as shown in code
    listing A.6 above.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，自定义 Dataset 类的三个主要组成部分是 `__init__` 构造函数、`__getitem__` 方法和 `__len__`
    方法，如上面的代码列表 A.6 所示。
- en: In the `__init__` method, we set up attributes that we can access later in the
    `__getitem__` and `__len__` methods. This could be file paths, file objects, database
    connectors, and so on. Since we created a tensor dataset that sits in memory,
    we are simply assigning `X` and `y` to these attributes, which are placeholders
    for our tensor objects.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在`__init__`方法中，我们设置了可以在`__getitem__`和`__len__`方法中访问的属性。这些可以是文件路径、文件对象、数据库连接等。由于我们创建了一个驻留在内存中的张量数据集，我们只是将`X`和`y`分配给这些属性，它们是我们张量对象的占位符。
- en: In the `__getitem__` method, we define instructions for returning exactly one
    item from the dataset via an `index`. This means the features and the class label
    corresponding to a single training example or test instance. (The data loader
    will provide this `index`, which we will cover shortly.)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在`__getitem__`方法中，我们定义了通过`index`从数据集中返回确切一项的指令。这意味着特征和与单个训练示例或测试实例对应的类别标签。（数据加载器将提供这个`index`，我们稍后会详细介绍。）
- en: 'Finally, the `__len__` method constrains instructions for retrieving the length
    of the dataset. Here, we use the `.shape` attribute of a tensor to return the
    number of rows in the feature array. In the case of the training dataset, we have
    five rows, which we can double-check as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`__len__`方法限制了检索数据集长度的指令。在这里，我们使用张量的`.shape`属性返回特征数组中的行数。在训练数据集的情况下，我们有五行，可以通过以下方式进行双重检查：
- en: '[PRE53]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The result is:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE54]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now that we defined a PyTorch Dataset class we can use for our toy dataset,
    we can use PyTorch''s `DataLoader` class to sample from it, as shown in the code
    listing below:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们定义了一个可以用于玩具数据集的PyTorch Dataset类，我们可以使用PyTorch的`DataLoader`类从中进行采样，如下面的代码清单所示：
- en: Listing A.7 Instantiating data loaders
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单A.7 实例化数据加载器
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: After instantiating the training data loader, we can iterate over it as shown
    below. (The iteration over the `test_loader` works similarly but is omitted for
    brevity.)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化训练数据加载器后，我们可以如下面所示进行迭代。（对`test_loader`的迭代类似，但出于简洁性省略了。）
- en: '[PRE56]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The result is:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE57]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As we can see based on the output above, the `train_loader` iterates over the
    training dataset visiting each training example exactly once. This is known as
    a training epoch. Since we seeded the random number generator using `torch.manual_seed(123)`
    above, you should get the exact same shuffling order of training examples as shown
    above. However if you iterate over the dataset a second time, you will see that
    the shuffling order will change. This is desired to prevent deep neural networks
    getting caught in repetitive update cycles during training.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面的输出，我们可以看到，`train_loader`遍历训练数据集，每个训练示例恰好访问一次。这被称为训练周期。由于我们在上面使用`torch.manual_seed(123)`设置了随机数生成器的种子，你应该得到与上面相同的训练示例洗牌顺序。然而，如果你第二次遍历数据集，你会看到洗牌顺序会改变。这是为了防止深度神经网络在训练期间陷入重复的更新循环。
- en: 'Note that we specified a batch size of 2 above, but the 3rd batch only contains
    a single example. That''s because we have five training examples, which is not
    evenly divisible by 2\. In practice, having a substantially smaller batch as the
    last batch in a training epoch can disturb the convergence during training. To
    prevent this, it''s recommended to set `drop_last=True`, which will drop the last
    batch in each epoch, as shown below:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在上面指定了批量大小为2，但第3个批次仅包含一个示例。这是因为我们有五个训练示例，无法被2整除。在实践中，训练周期的最后一个批次要小得多，可能会影响训练过程中的收敛。为此，建议设置`drop_last=True`，这将丢弃每个周期中的最后一个批次，如下所示：
- en: Listing A.8 A training loader that drops the last batch
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单A.8 一个丢弃最后批次的训练加载器
- en: '[PRE58]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, iterating over the training loader, we can see that the last batch is
    omitted:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，遍历训练加载器，我们可以看到最后一个批次被省略：
- en: '[PRE59]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The result is:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE60]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Lastly, let's discuss the setting `num_workers=0` in the `DataLoader`. This
    parameter in PyTorch's `DataLoader` function is crucial for parallelizing data
    loading and preprocessing. When `num_workers` is set to 0, the data loading will
    be done in the main process and not in separate worker processes. This might seem
    unproblematic, but it can lead to significant slowdowns during model training
    when we train larger networks on a GPU. This is because instead of focusing solely
    on the processing of the deep learning model, the CPU must also take time to load
    and preprocess the data. As a result, the GPU can sit idle while waiting for the
    CPU to finish these tasks. In contrast, when `num_workers` is set to a number
    greater than zero, multiple worker processes are launched to load data in parallel,
    freeing the main process to focus on training your model and better utilizing
    your system's resources, which is illustrated in figure A.11
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论在`DataLoader`中设置`num_workers=0`。在PyTorch的`DataLoader`函数中，这个参数对于并行化数据加载和预处理至关重要。当`num_workers`设置为0时，数据加载将在主进程中进行，而不是在单独的工作进程中进行。这看似没有问题，但在我们对GPU训练较大的网络时，它可能导致显著的减速。这是因为CPU不仅要专注于深度学习模型的处理，还要花时间加载和预处理数据。因此，GPU可能在等待CPU完成这些任务时处于闲置状态。相比之下，当`num_workers`设置为大于零的数字时，会启动多个工作进程以并行加载数据，从而使主进程能够专注于训练模型，更好地利用系统资源，如图A.11所示。
- en: Figure A.11 Loading data without multiple workers (setting `num_workers=0`)
    will create a data loading bottleneck where the model sits idle until the next
    batch is loaded as illustrated in the left subpanel. If multiple workers are enabled,
    the data loader can already queue up the next batch in the background as shown
    in the right subpanel.
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.11 在没有多个工作进程的情况下加载数据（设置`num_workers=0`）将会造成数据加载瓶颈，使得模型在下一批次加载之前处于闲置状态，如左侧子面板所示。如果启用了多个工作进程，数据加载器可以在后台提前排队下一批数据，如右侧子面板所示。
- en: '![](images/A__image021.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image021.png)'
- en: However, if we are working with very small datasets, setting `num_workers` to
    1 or larger may not be necessary since the total training time takes only fractions
    of a second anyway. On the contrary, if you are working with tiny datasets or
    interactive environments such as Jupyter notebooks, increasing `num_workers` may
    not provide any noticeable speedup. They might, in fact, lead to some issues.
    One potential issue is the overhead of spinning up multiple worker processes,
    which could take longer than the actual data loading when your dataset is small.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们处理的是非常小的数据集，将`num_workers`设置为1或更大可能不是必要的，因为总训练时间只需几秒钟的碎片时间。相反，如果你在处理微小数据集或交互环境（如Jupyter
    notebooks），增加`num_workers`可能不会带来明显的加速。实际上，它们可能会导致一些问题。一个潜在的问题是启动多个工作进程的开销，这可能会比实际数据加载所需的时间更长，当数据集较小时。
- en: Furthermore, for Jupyter notebooks, setting `num_workers` to greater than 0
    can sometimes lead to issues related to the sharing of resources between different
    processes, resulting in errors or notebook crashes. Therefore, it's essential
    to understand the trade-off and make a calculated decision on setting the `num_workers`
    parameter. When used correctly, it can be a beneficial tool but should be adapted
    to your specific dataset size and computational environment for optimal results.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于Jupyter notebooks，将`num_workers`设置为大于0有时可能会导致不同进程之间资源共享相关的问题，从而导致错误或笔记本崩溃。因此，理解权衡是至关重要的，并对设置`num_workers`参数做出计算过的决定。正确使用时，它可以是一个有益的工具，但应根据特定的数据集大小和计算环境进行调整，以获得最佳效果。
- en: In my experience, setting `num_workers=4` usually leads to optimal performance
    on many real-world datasets, but optimal settings depend on your hardware and
    the code used for loading a training example defined in the `Dataset` class.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，将`num_workers=4`通常在许多实际数据集上会导致最佳性能，但最佳设置依赖于你的硬件以及在`Dataset`类中定义的加载训练示例的代码。
- en: A.7 A typical training loop
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.7 一个典型的训练循环
- en: 'So far, we''ve discussed all the requirements for training neural networks:
    PyTorch''s tensor library, autograd, the `Module` API, and efficient data loaders.
    Let''s now combine all these things and train a neural network on the toy dataset
    from the previous section. The training code is shown in code listing A.9 below.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了训练神经网络的所有要求：PyTorch的张量库、自动求导、`Module` API和高效的数据加载器。现在，让我们将这些结合起来，在上一节中的玩具数据集上训练一个神经网络。训练代码如下所示于代码列表A.9。
- en: Listing A.9 Neural network training in PyTorch
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.9 在PyTorch中训练神经网络
- en: '[PRE61]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Running the code in listing A.9 above yields the following outputs:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上面列表A.9中的代码会产生以下输出：
- en: '[PRE62]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As we can see, the loss reaches zero after 3 epochs, a sign that the model converged
    on the training set. However, before we evaluate the model's predictions, let's
    go over some of the details of the preceding code listing.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，损失在3个周期后达到了零，这表明模型在训练集上收敛。然而，在评估模型的预测之前，让我们回顾一下前面代码清单的一些细节。
- en: First, note that we initialized a model with two inputs and two outputs. That's
    because the toy dataset from the previous section has two input features and two
    class labels to predict. We used a stochastic gradient descent (`SGD`) optimizer
    with a learning rate (`lr`) of 0.5\. The learning rate is a hyperparameter, meaning
    it's a tunable setting that we have to experiment with based on observing the
    loss. Ideally, we want to choose a learning rate such that the loss converges
    after a certain number of epochs -- the number of epochs is another hyperparameter
    to choose.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意我们初始化了一个具有两个输入和两个输出的模型。这是因为前一节的玩具数据集有两个输入特征和两个类标签要预测。我们使用了一个随机梯度下降（`SGD`）优化器，学习率（`lr`）为0.5。学习率是一个超参数，意味着这是一个可调的设置，我们必须根据观察损失来进行实验。理想情况下，我们希望选择一个学习率，使得损失在一定数量的周期后收敛——周期数是另一个需要选择的超参数。
- en: Exercise A.3
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 A.3
- en: How many parameters does the neural network introduced at the beginning of this
    section have?
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本节开头介绍的神经网络有多少个参数？
- en: In practice, we often use a third dataset, a so-called validation dataset, to
    find the optimal hyperparameter settings. A validation dataset is similar to a
    test set. However, while we only want to use a test set precisely once to avoid
    biasing the evaluation, we usually use the validation set multiple times to tweak
    the model settings.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常会使用第三个数据集，即所谓的验证数据集，以找到最佳的超参数设置。验证数据集类似于测试集。然而，由于我们只希望使用测试集一次以避免偏倚评估，我们通常会多次使用验证集来调整模型设置。
- en: We also introduced new settings called `model.train()` and `model.eval()`. As
    these names imply, these settings are used to put the model into a training and
    an evaluation mode. This is necessary for components that behave differently during
    training and inference, such as *dropout* or *batch normalization* layers. Since
    we don't have dropout or other components in our `NeuralNetwork` class that are
    affected by these settings, using `model.train()` and `model.eval()` is redundant
    in our code above. However, it's best practice to include them anyway to avoid
    unexpected behaviors when we change the model architecture or reuse the code to
    train a different model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还引入了新的设置`model.train()`和`model.eval()`。正如这些名称所暗示的，这些设置用于将模型置于训练模式和评估模式。这对在训练和推断期间表现不同的组件是必要的，例如*dropout*或*batch
    normalization*层。由于在我们的`NeuralNetwork`类中没有受到这些设置影响的dropout或其他组件，因此在上面的代码中使用`model.train()`和`model.eval()`是多余的。然而，出于最佳实践，仍然最好包含它们，以避免在更改模型架构或重用代码以训练不同模型时出现意外行为。
- en: As discussed earlier, we pass the logits directly into the `cross_entropy` loss
    function, which will apply the softmax function internally for efficiency and
    numerical stability reasons. Then, calling `loss.backward()` will calculate the
    gradients in the computation graph that PyTorch constructed in the background.
    The `optimizer.step()` method will use the gradients to update the model parameters
    to minimize the loss. In the case of the SGD optimizer, this means multiplying
    the gradients with the learning rate and adding the scaled negative gradient to
    the parameters.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所讨论的，我们将logits直接传入`cross_entropy`损失函数，该函数将在内部应用softmax函数，以提高效率和数值稳定性。然后，调用`loss.backward()`将计算PyTorch在后台构建的计算图中的梯度。`optimizer.step()`方法将使用梯度更新模型参数，以最小化损失。在SGD优化器的情况下，这意味着将梯度与学习率相乘，并将缩放的负梯度添加到参数中。
- en: Preventing undesired gradient accumulation
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 防止不必要的梯度累积
- en: It is important to include an `optimizer.zero_grad()` call in each update round
    to reset the gradients to zero. Otherwise, the gradients will accumulate, which
    may be undesired.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次更新轮次中，调用`optimizer.zero_grad()`以重置梯度为零是很重要的。否则，梯度会累积，这可能不是我们想要的。
- en: 'After we trained the model, we can use it to make predictions, as shown below:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完模型后，我们可以用它进行预测，如下所示：
- en: '[PRE63]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The results are as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To obtain the class membership probabilities, we can then use PyTorch''s softmax
    function, as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得类别成员概率，我们可以使用PyTorch的softmax函数，如下所示：
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This outputs:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出：
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Let's consider the first row in the code output above. Here, the first value
    (column) means that the training example has a 99.91% probability of belonging
    to class 0 and a 0.09% probability of belonging to class 1\. (The `set_printoptions`
    call is used here to make the outputs more legible.)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑上面代码输出的第一行。这里，第一个值（列）意味着训练示例属于类别0的概率为99.91%，属于类别1的概率为0.09%。（`set_printoptions`调用用于使输出更易读。）
- en: 'We can convert these values into class labels predictions using PyTorch''s
    argmax function, which returns the index position of the highest value in each
    row if we set `dim=1` (setting `dim=0` would return the highest value in each
    column, instead):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch的argmax函数将这些值转换为类别标签预测，如果我们设置`dim=1`，它将返回每行中最高值的索引位置（设置`dim=0`将返回每列中的最高值）：
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This prints:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出：
- en: '[PRE68]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Note that it is unnecessary to compute softmax probabilities to obtain the
    class labels. We could also apply the `argmax` function to the logits (outputs)
    directly:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，计算类标签时不需要计算softmax概率。我们也可以直接对logits（输出）应用`argmax`函数：
- en: '[PRE69]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE70]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Above, we computed the predicted labels for the training dataset. Since the
    training dataset is relatively small, we could compare it to the true training
    labels by eye and see that the model is 100% correct. We can double-check this
    using the == comparison operator:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们计算了训练数据集的预测标签。由于训练数据集相对较小，我们可以通过目测将其与真实训练标签进行比较，看到模型是100%正确的。我们可以使用==比较运算符进行二次检查：
- en: '[PRE71]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The results are:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE72]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Using `torch.sum`, we can count the number of correct prediction as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torch.sum`，我们可以计算正确预测的数量，如下所示：
- en: '[PRE73]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output is:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE74]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Since the dataset consists of 5 training examples, we have 5 out of 5 predictions
    that are correct, which equals 5/5 × 100% = 100% prediction accuracy.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集由5个训练示例组成，我们有5个正确的预测，即5/5 × 100% = 100%预测准确率。
- en: However, to generalize the computation of the prediction accuracy, let's implement
    a `compute_accuracy` function as shown in the following code listing.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使预测准确率的计算更具通用性，让我们实现一个`compute_accuracy`函数，如以下代码列表所示。
- en: Listing A.10 A function to compute the prediction accuracy
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.10 一个计算预测准确率的函数
- en: '[PRE75]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note that the following code listing iterates over a data loader to compute
    the number and fraction of the correct predictions. This is because when we work
    with large datasets, we typically can only call the model on a small part of the
    dataset due to memory limitations. The `compute_accuracy` function above is a
    general method that scales to datasets of arbitrary size since, in each iteration,
    the dataset chunk that the model receives is the same size as the batch size seen
    during training.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以下代码列表遍历数据加载器以计算正确预测的数量和比例。这是因为当我们处理大型数据集时，通常只能对数据集的一小部分进行模型调用，因为内存限制。上面的`compute_accuracy`函数是一种通用方法，可以扩展到任意大小的数据集，因为在每次迭代中，模型接收到的数据集块大小与训练期间看到的批次大小相同。
- en: Notice that the internals of the `compute_accuracy` function are similar to
    what we used before when we converted the logits to the class labels.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`compute_accuracy`函数的内部逻辑与我们之前将logits转换为类别标签时使用的相似。
- en: 'We can then apply the function to the training as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将该函数应用于训练集，如下所示：
- en: '[PRE76]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The results is:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE77]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Similarly, we can apply the function to the test set as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以将该函数应用于测试集，如下所示：
- en: '[PRE78]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This prints:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出：
- en: '[PRE79]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In this section, we learned how we can train a neural network using PyTorch.
    Next, let's see how we can save and restore models after training.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用PyTorch训练神经网络。接下来，让我们看看如何在训练后保存和恢复模型。
- en: A.8 Saving and loading models
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.8 保存和加载模型
- en: In the previous section, we successfully trained a model. Let's now see how
    we can save a trained model to reuse it later.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们成功训练了一个模型。现在让我们看看如何保存一个训练好的模型以便后续使用。
- en: 'Here''s the recommended way how we can save and load models in PyTorch:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们在PyTorch中保存和加载模型的推荐方法：
- en: '[PRE80]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: The model's state_dict is a Python dictionary object that maps each layer in
    the model to its trainable parameters (weights and biases). Note that `"model.pth"`
    is an arbitrary filename for the model file saved to disk. We can give it any
    name and file ending we like; however, `.pth` and `.pt` are the most common conventions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的state_dict是一个Python字典对象，它将模型中的每一层映射到其可训练参数（权重和偏差）。请注意，`"model.pth"`是保存到磁盘的模型文件的任意文件名。我们可以给它任何我们喜欢的名称和文件后缀；然而，`.pth`和`.pt`是最常见的约定。
- en: 'Once we saved the model, we can restore it from disk as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们保存了模型，就可以按如下方式从磁盘恢复它：
- en: '[PRE81]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The `torch.load("model.pth")` function reads the file `"model.pth"` and reconstructs
    the Python dictionary object containing the model's parameters while `model.load_state_dict()`
    applies these parameters to the model, effectively restoring its learned state
    from when we saved it.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.load("model.pth")` 函数读取文件 `"model.pth"` 并重建包含模型参数的Python字典对象，而 `model.load_state_dict()`
    将这些参数应用于模型，有效地恢复其学习状态。'
- en: Note that the line `model = NeuralNetwork(2, 2)` above is not strictly necessary
    if you execute this code in the same session where you saved a model. However,
    I included it here to illustrate that we need an instance of the model in memory
    to apply the saved parameters. Here, the `NeuralNetwork(2, 2)` architecture needs
    to match the original saved model exactly.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面的行 `model = NeuralNetwork(2, 2)` 如果你在同一会话中执行此代码并且已保存了模型，则并不是严格必要的。然而，我在这里包含它是为了说明我们需要在内存中有模型的实例，以便应用保存的参数。在这里，`NeuralNetwork(2,
    2)` 架构需要与原始保存模型完全匹配。
- en: Now, we are well equipped to use PyTorch to implement large language models
    in the upcoming chapters. However, before we jump to the next chapter, the last
    section will show you how to train PyTorch models faster using one or more GPUs
    (if available).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好在接下来的章节中使用PyTorch实现大型语言模型。然而，在我们跳到下一章之前，最后一节将向你展示如何使用一个或多个GPU（如果可用）更快地训练PyTorch模型。
- en: A.9 Optimizing training performance with GPUs
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.9 使用GPU优化训练性能
- en: In this last section of this chapter, we will see how we can utilize GPUs, which
    will accelerate deep neural network training compared to regular CPUs. First,
    we will introduce the main concepts behind GPU computing in PyTorch. Then, we
    will train a model on a single GPU. Finally, we'll then look at distributed training
    using multiple GPUs.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将看到如何利用GPU，这将加速深度神经网络训练，相比于常规CPU。首先，我们将介绍PyTorch中GPU计算的主要概念。然后，我们将在单个GPU上训练一个模型。最后，我们将看看如何使用多个GPU进行分布式训练。
- en: A.9.1 PyTorch computations on GPU devices
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.1 PyTorch在GPU设备上的计算
- en: As you will see, modifying the training loop from section 2.7 to optionally
    run on a GPU is relatively simple and only requires changing three lines of code.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，将第2.7节中的训练循环修改为可选择在GPU上运行相对简单，仅需更改三行代码。
- en: Before we make the modifications, it's crucial to understand the main concept
    behind GPU computations within PyTorch. First, we need to introduce the notion
    of devices. In PyTorch, a device is where computations occur, and data resides.
    The CPU and the GPU are examples of devices. A PyTorch tensor resides in a device,
    and its operations are executed on the same device.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行修改之前，理解PyTorch中GPU计算的主要概念至关重要。首先，我们需要引入设备的概念。在PyTorch中，设备是计算发生的地方，数据也驻留在此。CPU和GPU就是设备的例子。PyTorch张量驻留在一个设备上，其操作在同一设备上执行。
- en: 'Let''s see how this works in action. Assuming that you installed a GPU-compatible
    version of PyTorch as explained in section 2.1.3, Installing PyTorch, we can double-check
    that our runtime indeed supports GPU computing via the following code:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实际中是如何工作的。假设你已安装了与GPU兼容的PyTorch版本，如第2.1.3节“安装PyTorch”中所述，我们可以通过以下代码双重检查我们的运行时确实支持GPU计算：
- en: '[PRE82]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The result is:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE83]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, suppose we have two tensors that we can add as follows -- this computation
    will be carried out on the CPU by default:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有两个张量，我们可以如下进行加法运算——这个计算默认将在CPU上进行：
- en: '[PRE84]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This outputs:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE85]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We can now use the `.to()` method[[1]](#_ftn1) to transfer these tensors onto
    a GPU and perform the addition there:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `.to()` 方法[[1]](#_ftn1) 将这些张量传输到GPU上并在那里进行加法运算：
- en: '[PRE86]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE87]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Notice that the resulting tensor now includes the device information, `device='cuda:0'`,
    which means that the tensors reside on the first GPU. If your machine hosts multiple
    GPUs, you have the option to specify which GPU you'd like to transfer the tensors
    to. You can do this by indicating the device ID in the transfer command. For instance,
    you can use `.to("cuda:0")`, `.to("cuda:1")`, and so on.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成的张量现在包含设备信息 `device='cuda:0'`，这意味着张量驻留在第一个GPU上。如果你的机器上有多个GPU，你可以选择指定要将张量传输到哪个GPU。你可以通过在传输命令中指明设备ID来做到这一点。例如，你可以使用
    `.to("cuda:0")`、`.to("cuda:1")`，等等。
- en: 'However, it is important to note that all tensors must be on the same device.
    Otherwise, the computation will fail, as shown below, where one tensor resides
    on the CPU and the other on the GPU:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，需要注意的是，所有张量必须位于同一设备上。否则，计算将失败，如下所示，其中一个张量位于CPU上，另一个位于GPU上：
- en: '[PRE88]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'This results in the following:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下结果：
- en: '[PRE89]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: In this section, we learned that GPU computations on PyTorch are relatively
    straightforward. All we have to do is transfer the tensors onto the same GPU device,
    and PyTorch will handle the rest. Equipped with this information, we can now train
    the neural network from the previous section on a GPU.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们了解到在PyTorch上进行GPU计算相对简单。我们所需做的就是将张量转移到同一个GPU设备上，PyTorch会处理其余的部分。掌握了这些信息后，我们现在可以在GPU上训练前一节的神经网络。
- en: A.9.2 Single-GPU training
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.2 单GPU训练
- en: Now that we are familiar with transferring tensors to the GPU, we can modify
    the training loop from *section 2.7, A typical training loop*, to run on a GPU.
    This requires only changing three lines of code, as shown in code listing A.11
    below.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经熟悉了将张量转移到GPU，我们可以修改来自*第2.7节，典型训练循环*的训练循环以在GPU上运行。这只需要更改三行代码，如下面的代码列表A.11所示。
- en: Listing A.11 A training loop on a GPU
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.11 GPU上的训练循环
- en: '[PRE90]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Running the above code will output the following, similar to the results obtained
    on the CPU previously in section 2.7:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码将输出以下内容，类似于之前在第2.7节中获得的CPU结果：
- en: '[PRE91]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We can also use `.to("cuda")` instead of `device = torch.device("cuda")`. As
    we saw in section 2.9.1, transferring a tensor to "cuda" instead of `torch.device("cuda")`
    works as well and is shorter. We can also modify the statement to the following,
    which will make the same code executable on a CPU if a GPU is not available, which
    is usually considered best practice when sharing PyTorch code:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`.to("cuda")`替代`device = torch.device("cuda")`。正如我们在第2.9.1节中看到的，将张量转移到“cuda”而不是`torch.device("cuda")`同样有效且更简洁。我们还可以将语句修改为以下内容，这将使相同的代码在没有GPU的情况下也能在CPU上执行，这通常被认为是共享PyTorch代码时的最佳实践：
- en: '[PRE92]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: In the case of the modified training loop above, we probably won't see a speed-up
    because of the memory transfer cost from CPU to GPU. However, we can expect a
    significant speed-up when training deep neural networks, especially large language
    models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述修改后的训练循环中，我们可能不会看到加速，因为从CPU到GPU的内存传输成本。但是，在训练深度神经网络，特别是大型语言模型时，我们可以期待显著的加速。
- en: 'As we saw in this section, training a model on a single GPU in PyTorch is relatively
    easy. Next, let''s introduce another concept: training models on multiple GPUs.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这一节中看到的，在PyTorch上在单个GPU上训练模型相对简单。接下来，让我们介绍另一个概念：在多个GPU上训练模型。
- en: PyTorch on macOS
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: macOS上的PyTorch
- en: On an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models)
    instead of a computer with an Nvidia GPU, you can change
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在带有Apple Silicon芯片（如M1、M2、M3或更新型号）的Apple Mac上，而不是带有Nvidia GPU的计算机上，你可以更改
- en: '[PRE93]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: to
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '[PRE94]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: to take advantage of this chip.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个芯片。
- en: Exercise A.4
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习A.4
- en: 'Compare the runtime of matrix multiplication on a CPU to a GPU. At what matrix
    size do you begin to see the matrix multiplication on the GPU being faster than
    on the CPU? Hint: I recommend using the `%timeit` command in Jupyter to compare
    the runtime. For example, given matrices `a` and `b`, run the command `%timeit
    a @ b` in a new notebook cell.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 比较CPU和GPU上的矩阵乘法运行时间。在哪个矩阵大小开始出现GPU上的矩阵乘法比CPU快的现象？提示：我建议在Jupyter中使用`%timeit`命令来比较运行时间。例如，给定矩阵`a`和`b`，在新的笔记本单元中运行命令`%timeit
    a @ b`。
- en: A.9.3 Training with multiple GPUs
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9.3 使用多个GPU进行训练
- en: In this section, we will briefly go over the concept of distributed training.
    Distributed training is the concept of dividing the model training across multiple
    GPUs and machines.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将简要概述分布式训练的概念。分布式训练是将模型训练分配到多个GPU和机器上的概念。
- en: Why do we need this? Even when it is possible to train a model on a single GPU
    or machine, the process could be exceedingly time-consuming. The training time
    can be significantly reduced by distributing the training process across multiple
    machines, each with potentially multiple GPUs. This is particularly crucial in
    the experimental stages of model development, where numerous training iterations
    might be necessary to finetune the model parameters and architecture.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么需要这样做？即使可以在单个GPU或机器上训练模型，过程也可能非常耗时。通过将训练过程分散到多个机器上，每台机器可能有多个GPU，可以显著减少训练时间。这在模型开发的实验阶段尤为重要，因为可能需要进行大量的训练迭代来微调模型参数和架构。
- en: Multi-GPU computing is optional
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多 GPU 计算是可选的。
- en: For this book, it is not required to have access to or use multiple-GPU. This
    section is included for those who are interested in how multi-GPU computing works
    in PyTorch.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，并不要求访问或使用多个 GPU。本节内容是为那些对 PyTorch 中多 GPU 计算如何运作感兴趣的读者而准备的。
- en: 'In this section, we will look at the most basic case of distributed training:
    PyTorch''s `DistributedDataParallel` (DDP) strategy. DDP enables parallelism by
    splitting the input data across the available devices and processing these data
    subsets simultaneously.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看分布式训练的最基本案例：PyTorch 的 `DistributedDataParallel`（DDP）策略。DDP 通过将输入数据分配到可用设备上，并同时处理这些数据子集来实现并行化。
- en: How does this work? PyTorch launches a separate process on each GPU, and each
    process receives and keeps a copy of the model -- these copies will be synchronized
    during training. To illustrate this, suppose we have two GPUs that we want to
    use to train a neural network, as shown in figure A.12.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何运作？PyTorch 在每个 GPU 上启动一个独立的进程，每个进程接收并保留模型的副本——这些副本将在训练过程中进行同步。为了说明这一点，假设我们有两个
    GPU 想要用来训练一个神经网络，如图 A.12 所示。
- en: Figure A.12 The model and data transfer in DDP involves two key steps. First,
    we create a copy of the model on each of the GPUs. Then we divide the input data
    into unique minibatches that we pass on to each model copy.
  id: totrans-379
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.12 DDP 中模型和数据的传输涉及两个关键步骤。首先，我们在每个 GPU 上创建模型的副本。然后我们将输入数据划分为独特的小批量，并将其传递给每个模型副本。
- en: '![](images/A__image023.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image023.png)'
- en: Each of the two GPUs will receive a copy of the model. Then, in every training
    iteration, each model will receive a minibatch (or just batch) from the data loader.
    We can use a `DistributedSampler` to ensure that each GPU will receive a different,
    non-overlapping batch when using DDP.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 GPU 中的每一个都会接收模型的副本。然后，在每次训练迭代中，每个模型将从数据加载器接收一个小批量（或只是批量）。我们可以使用 `DistributedSampler`
    来确保每个 GPU 在使用 DDP 时接收不同的、无重叠的批次。
- en: Since each model copy will see a different sample of the training data, the
    model copies will return different logits as outputs and compute different gradients
    during the backward pass. These gradients are then averaged and synchronized during
    training to update the models. This way, we ensure that the models don't diverge,
    as illustrated in figure A.13.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型副本将看到不同的训练数据样本，模型副本在后向传播时将返回不同的 logits 作为输出，并计算出不同的梯度。这些梯度在训练期间进行平均和同步，以更新模型。通过这种方式，我们确保模型不会偏离，如图
    A.13 所示。
- en: Figure A.13 The forward and backward pass in DDP are executed independently
    on each GPU with its corresponding data subset. Once the forward and backward
    passes are completed, gradients from each model replica (on each GPU) are synchronized
    across all GPUs. This ensures that every model replica has the same updated weights.
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.13 DDP 中的前向和后向传播在每个 GPU 上独立执行，使用各自的数据子集。一旦前向和后向传播完成，来自每个模型副本（在每个 GPU 上）的梯度将在所有
    GPU 之间同步。这确保了每个模型副本具有相同的更新权重。
- en: '![](images/A__image025.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](images/A__image025.png)'
- en: The benefit of using DDP is the enhanced speed it offers for processing the
    dataset compared to a single GPU. Barring a minor communication overhead between
    devices that comes with DDP use, it can theoretically process a training epoch
    in half the time with two GPUs compared to just one. The time efficiency scales
    up with the number of GPUs, allowing us to process an epoch eight times faster
    if we have eight GPUs, and so on.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DDP 的好处是，相较于单个 GPU，它在处理数据集时提供了更快的速度。除了使用 DDP 时设备之间的小通信开销外，它在理论上可以在两个 GPU
    上将一个训练周期的处理时间缩短到仅一个 GPU 的一半。随着 GPU 数量的增加，时间效率会进一步提升，如果我们有八个 GPU，则可以将一个周期的处理速度提高八倍，依此类推。
- en: Multi-GPU computing in interactive environments
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交互环境中的多 GPU 计算。
- en: DDP does not function properly within interactive Python environments like Jupyter
    notebooks, which don't handle multiprocessing in the same way a standalone Python
    script does. Therefore, the following code should be executed as a script, not
    within a notebook interface like Jupyter. This is because DDP needs to spawn multiple
    processes, and each process should have its own Python interpreter instance.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: DDP 在交互式 Python 环境（如 Jupyter notebooks）中无法正常工作，这些环境处理多进程的方式与独立的 Python 脚本不同。因此，以下代码应作为脚本执行，而不是在
    Jupyter 等笔记本接口中执行。这是因为 DDP 需要生成多个进程，每个进程应有自己独立的 Python 解释器实例。
- en: Let's now see how this works in practice. For brevity, we will only focus on
    the core parts of the previous code that need to be adjusted for DDP training.
    However, for readers who want to run the code on their own multi-GPU machine or
    a cloud instance of their choice, it is recommended to use the standalone script
    provided in this book's GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这在实践中的运作情况。为了简洁起见，我们将重点关注之前代码中需要为DDP训练进行调整的核心部分。然而，对于希望在自己多GPU机器或选择的云实例上运行代码的读者，建议使用本书GitHub库中提供的独立脚本，[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)。
- en: First, we will import a few additional submodules, classes, and functions for
    distributed training PyTorch as shown in code listing A.13 below.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入一些额外的子模块、类和函数，以支持PyTorch的分布式训练，如列表A.13中的代码所示。
- en: Listing A.12 PyTorch utilities for distributed training
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.12 PyTorch的分布式训练工具
- en: '[PRE95]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Before we dive deeper into the changes to make the training compatible with
    DDP, let's briefly go over the rationale and usage for these newly imported utilities
    that we need alongside the `DistributedDataParallel` class.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解使训练兼容DDP的更改之前，让我们简要回顾一下这些新导入的工具的基本原理和用法，这些工具与`DistributedDataParallel`类一起使用。
- en: PyTorch's `multiprocessing` submodule contains functions such as `multiprocessing.spawn`,
    which we will use to spawn multiple processes and apply a function to multiple
    inputs in parallel. We will use it to spawn one training process per GPU.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`multiprocessing`子模块包含诸如`multiprocessing.spawn`等函数，我们将使用它来生成多个进程，并将一个函数应用于多个输入进行并行处理。我们将用它为每个GPU生成一个训练进程。
- en: If we spawn multiple processes for training, we will need a way to divide the
    dataset among these different processes. For this, we will use the `DistributedSampler`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为训练生成多个进程，我们需要一种方法来将数据集划分到这些不同的进程中。为此，我们将使用`DistributedSampler`。
- en: The `init_process_group` and `destroy_process_group` are used to initialize
    and quit the distributed training mods. The `init_process_group` function should
    be called at the beginning of the training script to initialize a process group
    for each process in the distributed setup, and `destroy_process_group` should
    be called at the end of the training script to destroy a given process group and
    release its resources.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_process_group`和`destroy_process_group`用于初始化和退出分布式训练模式。`init_process_group`函数应在训练脚本开始时调用，以初始化分布式设置中的每个进程的进程组，`destroy_process_group`应在训练脚本结束时调用，以销毁特定的进程组并释放其资源。'
- en: The following code in listing A.13 below illustrates how these new components
    are used to implement DDP training for the `NeuralNetwork` model we implemented
    earlier.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在列表A.13中说明了如何使用这些新组件来实现我们之前实现的`NeuralNetwork`模型的DDP训练。
- en: Listing A.13 Model training with DistributedDataParallel strategy
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.13 使用DistributedDataParallel策略进行模型训练
- en: '[PRE96]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Before we run the code from listing A.13, here is a summary of how it works,
    in addition to the annotations above. We have a `__name__ == "__main__"` clause
    at the bottom containing code that is executed when we run the code as a Python
    script instead of importing it as a module. This code first prints the number
    of available GPUs using `torch.cuda.device_count()`, sets a random seed for reproducibility
    and then spawns new processes using PyTorch's `multiprocesses.spawn` function.
    Here, the `spawn` function launches one process per GPU setting `nproces=world_size`,
    where the world size is the number of available GPUs. This `spawn` function launches
    the code in the `main` function we define in the same script with some additional
    arguments provided via `args`. Note that the `main` function has a `rank` argument
    that we don't include in the `mp.spawn()` call. That's because the `rank`, which
    refers to the process ID we use as the GPU ID, is already passed automatically.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行列表A.13中的代码之前，这里总结一下它的工作原理，除了上述注释外。我们在底部有一个`__name__ == "__main__"`子句，其中包含在将代码作为Python脚本运行而不是作为模块导入时执行的代码。该代码首先使用`torch.cuda.device_count()`打印可用的GPU数量，设置随机种子以确保可重复性，然后使用PyTorch的`multiprocesses.spawn`函数生成新进程。这里，`spawn`函数为每个GPU启动一个进程，设置`nproces=world_size`，其中world
    size是可用GPU的数量。此`spawn`函数启动我们在同一脚本中定义的`main`函数，并通过`args`提供一些附加参数。请注意，`main`函数有一个`rank`参数，我们不在`mp.spawn()`调用中包含它。这是因为`rank`（指我们用作GPU
    ID的进程ID）已经被自动传递。
- en: The `main` function sets up the distributed environment via `ddp_setup` -- another
    function we defined, loads the training and test sets, sets up the model, and
    carries out the training. Compared to the single-GPU training in section 2.12,
    we now transfer the model and data to the target device via .`to(rank)`, which
    we use to refer to the GPU device ID. Also, we wrap the model via `DDP`, which
    enables the synchronization of the gradients between the different GPUs during
    training. After the training finishes and we evaluate the models, we use `destroy_process_group()`
    to cleanly exit the distributed training and free up the allocated resources.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数通过`ddp_setup`设置分布式环境——这是我们定义的另一个函数，加载训练和测试集，设置模型并进行训练。与第2.12节中的单GPU训练相比，我们现在通过`.to(rank)`将模型和数据转移到目标设备，这里使用的是GPU设备ID。此外，我们通过`DDP`包装模型，使得不同GPU之间在训练过程中能够同步梯度。在训练结束后，我们评估模型，使用`destroy_process_group()`干净地退出分布式训练并释放分配的资源。'
- en: Earlier, we mentioned that each GPU will receive a different subsample of the
    training data. To ensure this, we set `sampler=DistributedSampler(train_ds)` in
    the training loader.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到过，每个GPU将接收不同的训练数据子样本。为确保这一点，我们在训练加载器中设置`sampler=DistributedSampler(train_ds)`。
- en: The last function to discuss is `ddp_setup`. It sets the main node's address
    and port to allow for communication between the different processes, initializes
    the process group with the NCCL backend (designed for GPU-to-GPU communication),
    and sets the `rank` (process identifier) and world size (total number of processes).
    Finally, it specifies the GPU device corresponding to the current model training
    process rank.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要讨论的函数是`ddp_setup`。它设置主节点的地址和端口，以便允许不同进程之间的通信，使用NCCL后端初始化进程组（为GPU之间的通信设计），并设置`rank`（进程标识符）和世界大小（进程总数）。最后，它指定与当前模型训练进程rank相对应的GPU设备。
- en: Selecting available GPUs on a multi-GPU machine
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在多GPU机器上选择可用的GPU
- en: 'If you wish to restrict the number of GPUs used for training on a multi-GPU
    machine, the simplest way is to use the `CUDA_VISIBLE_DEVICES` environment variable.
    To illustrate this, suppose your machine has multiple GPUs, and you only want
    to use one GPU, for example, the GPU with index 0\. Instead of `python some_script.py`,
    you can run the code from the terminal as follows:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望限制在多GPU机器上用于训练的GPU数量，最简单的方法是使用`CUDA_VISIBLE_DEVICES`环境变量。为了说明这一点，假设你的机器有多个GPU，而你只想使用一个GPU，例如索引为0的GPU。你可以从终端以如下方式运行代码，而不是`python
    some_script.py`：
- en: '[PRE97]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Or, if your machine has four GPUs and you only want to use the first and third
    GPU, you can use
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你的机器有四个GPU，而你只想使用第一个和第三个GPU，你可以使用
- en: '[PRE98]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Setting `CUDA_VISIBLE_DEVICES` in this way is a simple and effective way to
    manage GPU allocation without modifying your PyTorch scripts.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式设置`CUDA_VISIBLE_DEVICES`是一种简单有效的方法，可以管理GPU分配，而无需修改你的PyTorch脚本。
- en: 'Let''s now run this code and see how it works in practice by launching the
    code as a script from the terminal:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行这个代码，看看它在实践中是如何工作的，通过从终端以脚本方式启动代码：
- en: '[PRE99]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Note that it should work on both single- and multi-GPU machines. If we run
    this code on a single GPU, we should see the following output:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它应该在单GPU和多GPU机器上都能工作。如果我们在单个GPU上运行此代码，我们应该看到以下输出：
- en: '[PRE100]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: The code output looks similar to the one in section 2.9.2, which is a good sanity
    check.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出看起来类似于第2.9.2节中的输出，这是一个良好的合理性检查。
- en: 'Now, if we run the same command and code on a machine with two GPUs, we should
    see the following:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在一台有两个GPU的机器上运行相同的命令和代码，我们应该看到以下内容：
- en: '[PRE101]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: As expected, we can see that some batches are processed on the first GPU (`GPU0`)
    and others on the second (`GPU1`). However, we see duplicated output lines when
    printing the training and test accuracies. This is because each process (in other
    words, each GPU) prints the test accuracy independently. Since DDP replicates
    the model onto each GPU and each process runs independently, if you have a print
    statement inside your testing loop, each process will execute it, leading to repeated
    output lines.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，我们可以看到一些批次在第一个GPU（`GPU0`）上处理，而其他批次在第二个GPU（`GPU1`）上处理。然而，当打印训练和测试准确度时，我们看到重复的输出行。这是因为每个进程（换句话说，每个GPU）独立打印测试准确度。由于DDP将模型复制到每个GPU上，并且每个进程独立运行，如果在测试循环中有打印语句，每个进程都会执行它，导致重复输出行。
- en: If this bothers you, you can fix this using the rank of each process to control
    your print statements.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让你感到困扰，你可以使用每个进程的rank来控制打印语句。
- en: 'if rank == 0: # only print in the first process'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 'if rank == 0: # 仅在第一个进程中打印'
- en: 'print("Test accuracy: ", accuracy)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: print("测试准确率：", accuracy)
- en: This is, in a nutshell, how distributed training via DDP works. If you are interested
    in additional details, I recommend checking the official API documentation at
    [https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html](generated.html).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是通过 DDP 进行分布式训练的基本原理。如果你对更多细节感兴趣，建议查看官方 API 文档：[https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html](generated.html)
- en: Alternative PyTorch APIs for multi-GPU training
  id: totrans-421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于多 GPU 训练的替代 PyTorch API
- en: 'If you prefer more straightforward ways to use multiple GPUs in PyTorch, you
    can also consider add-on APIs like the open-source Fabric library, which I''ve
    written about in Accelerating PyTorch Model Training: Using Mixed-Precision and
    Fully Sharded Data Parallelism [https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training](p.html).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢在 PyTorch 中以更直接的方式使用多个 GPU，也可以考虑像开源 Fabric 库这样的附加 API，我在《加速 PyTorch 模型训练：使用混合精度和完全分片数据并行性》中写过这方面的内容。[https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training](p.html)
- en: A.10 Summary
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.10 小结
- en: 'PyTorch is an open-source library that consists of three core components: a
    tensor library, automatic differentiation functions, and deep learning utilities.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 是一个开源库，由三个核心组件组成：张量库、自动微分功能和深度学习工具。
- en: PyTorch's tensor library is similar to array libraries like NumPy
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 的张量库类似于像 NumPy 这样的数组库。
- en: In the context of PyTorch, tensors are array-like data structures to represent
    scalars, vectors, matrices, and higher-dimensional arrays.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 的上下文中，张量是表示标量、向量、矩阵和更高维数组的类似数组的数据结构。
- en: PyTorch tensors can be executed on the CPU, but one major advantage of PyTorch's
    tensor format is its GPU support to accelerate computations.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量可以在 CPU 上执行，但 PyTorch 张量格式的一个主要优势是其对 GPU 的支持，从而加速计算。
- en: The automatic differentiation (autograd) capabilities in PyTorch allow us to
    conveniently train neural networks using backpropagation without manually deriving
    gradients.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的自动微分（autograd）能力使我们能够方便地使用反向传播训练神经网络，而无需手动推导梯度。
- en: The deep learning utilities in PyTorch provide building blocks for creating
    custom deep neural networks.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的深度学习工具提供了创建自定义深度神经网络的构建模块。
- en: PyTorch includes `Dataset` and `DataLoader` classes to set up efficient data
    loading pipelines.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 包括 `Dataset` 和 `DataLoader` 类，以设置高效的数据加载管道。
- en: It's easiest to train models on a CPU or single GPU.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CPU 或单个 GPU 上训练模型是最简单的。
- en: Using `DistributedDataParallel` is the simplest way in PyTorch to accelerate
    the training if multiple GPUs are available.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中使用 `DistributedDataParallel` 是加速训练的最简单方法，前提是有多个 GPU 可用。
- en: A.11 Further reading
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.11 进一步阅读
- en: 'While this chapter should be sufficient to get you up to speed, in addition,
    if you are looking for more comprehensive introductions to deep learning, I recommend
    the following books:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章应该足以让你跟上进度，但如果你在寻找更全面的深度学习介绍，我推荐以下书籍：
- en: '*Machine Learning with PyTorch and Scikit-Learn* (2022) by Sebastian Raschka,
    Hayden Liu, and Vahid Mirjalili. ISBN 978-1801819312'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 PyTorch 和 Scikit-Learn 的机器学习*（2022），作者：Sebastian Raschka、Hayden Liu 和 Vahid
    Mirjalili。ISBN 978-1801819312'
- en: '*Deep Learning with PyTorch* (2021) by Eli Stevens, Luca Antiga, and Thomas
    Viehmann. ISBN 978-1617295263'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 PyTorch 深度学习*（2021），作者：Eli Stevens、Luca Antiga 和 Thomas Viehmann。ISBN 978-1617295263'
- en: 'For a more thorough introduction to the concepts of tensors, readers can find
    a 15 min video tutorial that I recorded:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对于想更深入了解张量概念的读者，可以找到我录制的 15 分钟视频教程：
- en: 'Lecture 4.1: Tensors in Deep Learning, [https://www.youtube.com/watch?v=JXfDlgrfOBY](www.youtube.com.html)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讲座 4.1：深度学习中的张量，[https://www.youtube.com/watch?v=JXfDlgrfOBY](www.youtube.com.html)
- en: 'If you want to learn more about model evaluation in machine learning, I recommend
    my article:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于机器学习中模型评估的内容，我推荐我的文章：
- en: '*Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning*
    (2018) by Sebastian Raschka, [https://arxiv.org/abs/1811.12808](abs.html)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习中的模型评估、模型选择和算法选择*（2018），作者：Sebastian Raschka，[https://arxiv.org/abs/1811.12808](abs.html)'
- en: 'For readers who are interested in a refresher or gentle introduction to calculus,
    I''ve written a chapter on calculus that is freely available on my website:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对微积分感兴趣的读者，我在自己的网站上写了一章微积分，供免费阅读：
- en: '*Introduction to Calculus* by Sebastian Raschka, [https://sebastianraschka.com/pdf/supplementary/calculus.pdf](supplementary.html)'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微积分导论*，作者：Sebastian Raschka，[https://sebastianraschka.com/pdf/supplementary/calculus.pdf](supplementary.html)'
- en: 'Why does PyTorch not call `optimizer.zero_grad()` automatically for us in the
    background? In some instances, it may be desirable to accumulate the gradients,
    and PyTorch will leave this as an option for us. If you want to learn more about
    gradient accumulation, please see the following article:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 PyTorch 不会在后台自动为我们调用 `optimizer.zero_grad()`？在某些情况下，可能希望累积梯度，PyTorch 将此保留为我们的选择。如果您想了解更多关于梯度累积的内容，请参见以下文章：
- en: '*Finetuning Large Language Models On A Single GPU Using Gradient Accumulation*
    by Sebastian Raschka, [https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html](2023.html)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在单个 GPU 上使用梯度累积微调大型语言模型* 由 Sebastian Raschka 撰写，[https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html](2023.html)'
- en: 'This chapter covered DDP, which is a popular approach for training deep learning
    models across multiple GPUs. For more advanced use cases where a single model
    doesn''t fit onto the GPU, you may also consider PyTorch''s *Fully Sharded Data
    Parallel* (FSDP) method, which performs distributed data parallelism and distributes
    large layers across different GPUs. For more information, see this overview with
    further links to the API documentation:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 DDP，这是一种在多个 GPU 上训练深度学习模型的流行方法。对于单个模型无法适配到 GPU 的更高级用例，您还可以考虑 PyTorch 的
    *完全分片数据并行*（FSDP）方法，该方法执行分布式数据并行，并在不同 GPU 之间分配大层。有关更多信息，请查看这个概述，其中有 API 文档的进一步链接：
- en: Introducing PyTorch Fully Sharded Data Parallel (FSDP) API, [https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/](introducing-pytorch-fully-sharded-data-parallel-api.html)
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 PyTorch 完全分片数据并行（FSDP）API，[https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/](introducing-pytorch-fully-sharded-data-parallel-api.html)
- en: A.12 Exercise answers
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.12 练习答案
- en: 'Exercise A.3:'
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 A.3：
- en: 'The network has 2 inputs and 2 outputs. In addition, there are 2 hidden layers
    with 30 and 20 nodes, respectively. Programmatically, we can calculate the number
    of parameters as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 网络有 2 个输入和 2 个输出。此外，还有 2 个隐藏层，分别有 30 和 20 个节点。从程序上讲，我们可以如下计算参数数量：
- en: '[PRE102]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This returns:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回：
- en: '[PRE103]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We can also calculate this manually as follows:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以手动计算如下：
- en: 'first hidden layer: 2 inputs times 30 hidden units plus 30 bias units.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个隐藏层：2 个输入乘以 30 个隐藏单元，加上 30 个偏置单元。
- en: 'second hidden layer: 30 incoming units times 20 nodes plus 20 bias units.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个隐藏层：30 个输入单元乘以 20 个节点，加上 20 个偏置单元。
- en: 'output layer: 20 incoming nodes times 2 output nodes plus 2 bias units.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：20 个输入节点乘以 2 个输出节点，加上 2 个偏置单元。
- en: Then, adding all the parameters in each layer results in 2×30+30 + 30×20+20
    + 20×2+2 = 752.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将每一层中的所有参数相加，结果为 2×30+30 + 30×20+20 + 20×2+2 = 752。
- en: 'Exercise A.4:'
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 A.4：
- en: 'The exact runtime results will be specific to the hardware used for this experiment.
    In my experiments, I observed significant speed-ups even for small matrix multiplications
    as the following one when using a Google Colab instance connected to a V100 GPU:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的运行时结果将特定于用于此实验的硬件。在我的实验中，我观察到即使对于小矩阵乘法，使用连接到 V100 GPU 的 Google Colab 实例时，速度也显著提升：
- en: '[PRE104]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'On the CPU this resulted in:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，结果为：
- en: '[PRE105]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'When executed on a GPU:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上执行时：
- en: '[PRE106]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The result was:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE107]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: In this case, on a V100, the computation was approximately four times faster.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，在 V100 上，计算速度大约快了四倍。
- en: '[[1]](#_ftnref1) This is the same .to() method we previously used to change
    a tensor''s datatype in section 2.2.2, Tensor data types.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_ftnref1) 这与我们之前在 2.2.2 节“张量数据类型”中用于更改张量数据类型的.to() 方法是相同的。'
