- en: '3 Generative adversarial networks: Shape and number generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 生成对抗网络：形状和数量生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building generator and discriminator networks in generative adversarial networks
    from scratch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始构建生成对抗网络中的生成网络和判别网络
- en: Using GANs to generate data points to form shapes (e.g., exponential growth
    curve)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GAN生成数据点以形成形状（例如，指数增长曲线）
- en: Generating integer sequences that are all multiples of 5
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成所有都是5的倍数的整数序列
- en: Training, saving, loading, and using GANs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、保存、加载和使用GANs
- en: Evaluating GAN performance and determining training stop points
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估GAN性能和确定训练停止点
- en: Close to half of the generative models in this book belong to a category called
    generative adversarial networks (GANs). The method was first proposed by Ian Goodfellow
    and his coauthors in 2014.^([1](#footnote-001)) GANs, celebrated for their ease
    of implementation and versatility, empower individuals with even rudimentary knowledge
    of deep learning to construct their models from the ground up. The word “adversarial”
    in GAN refers to the fact that the two neural networks compete against each other
    in a zero-sum game framework. The generative network tries to create data instances
    indistinguishable from real samples. In contrast, the discriminative network tries
    to identify the generated samples from real ones. These versatile models can generate
    various content formats, from geometric shapes and sequences of numbers to high-resolution
    color images and even realistic-sounding musical compositions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书近一半的生成模型属于一种称为生成对抗网络（GANs）的类别。这种方法最初由Ian Goodfellow及其合著者在2014年提出。[1](#footnote-001)
    GANs因其易于实现和多功能性而备受赞誉，使那些连深度学习基础知识都相当有限的人也能从头开始构建自己的模型。GAN中的“对抗”一词指的是两个神经网络在零和博弈框架中相互竞争的事实。生成网络试图创建与真实样本不可区分的数据实例。相比之下，判别网络试图从真实样本中识别出生成的样本。这些多才多艺的模型可以生成各种内容格式，从几何形状和数字序列到高分辨率彩色图像，甚至逼真的音乐作品。
- en: 'In this chapter, we’ll briefly review the theory behind GANs. Then, I’ll show
    you how to implement that knowledge in PyTorch. You’ll learn to build your first
    GAN from scratch so that all the details are demystified. To make the example
    relatable, imagine you put $1 in a savings account that pays 8% a year. You want
    to find out the balance in your account based on the number of years you have
    invested. The true relation is an exponential growth curve. You’ll learn to use
    GANs to generate data samples—pairs of values (x, y) that form such an exponential
    growth curve, with a mathematical relation y = 1.08^x. Armed with this skill,
    you’ll be able to generate data to mimic any shape: sine, cosine, quadratic, and
    so on.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要回顾GANs背后的理论。然后，我将向您展示如何将这一知识应用于PyTorch。您将学习从头开始构建您的第一个GAN，以便所有细节都变得不再神秘。为了使示例更具相关性，想象您将1美元存入年利率为8%的储蓄账户。您想根据您投资了多少年找出账户中的余额。真实的关系是一个指数增长曲线。您将学习如何使用GAN生成数据样本——形成这种指数增长曲线的值对（x,
    y），其数学关系为y = 1.08^x。掌握这项技能后，您将能够生成模拟任何形状的数据：正弦、余弦、二次等。
- en: In the second project in this chapter, you’ll learn how to use GANs to generate
    a sequence of numbers that are all multiples of 5\. But you can change the pattern
    to multiples of 2, 3, 7, or other patterns. Along the way, you’ll learn how to
    create a generator network and a discriminator network from scratch. You’ll learn
    how to train, save, and use GANs. Further, you’ll also learn to assess the performance
    of GANs either by visualizing samples generated by the generator network or by
    measuring the divergence between the generated sample distribution and the real
    data distribution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二个项目中，您将学习如何使用GAN生成一系列都是5的倍数的数字。但您可以将模式更改为2、3、7或其他模式。在这个过程中，您将学习如何从头开始创建生成网络和判别网络。您将学习如何训练、保存和使用GAN。此外，您还将学习如何通过可视化生成网络生成的样本或通过测量生成样本分布与真实数据分布之间的差异来评估GAN的性能。
- en: 'Imagine that you need data to train a machine learning (ML) model to predict
    the relation between pairs of values (x, y). However, the training dataset is
    costly and time-consuming for human beings to prepare by hand. GANs can be well-suited
    to generate data in such cases: while the generated values of x and y generally
    conform to a mathematical relation, there is also noise in the generated data.
    The noise can be useful for preventing overfitting when the generated data is
    used to train the ML model.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你需要数据来训练一个机器学习（ML）模型以预测一对值（x，y）之间的关系。然而，准备训练数据集既昂贵又耗时。在这种情况下，生成对抗网络（GANs）非常适合生成数据：虽然
    x 和 y 的生成值通常符合数学关系，但生成的数据中也有噪声。当使用生成的数据来训练 ML 模型时，噪声可以有助于防止过拟合。
- en: The primary goal of this chapter is not necessarily to generate novel content
    with the most practical use. Instead, my objective is to teach you how to train
    and use GANs to create various formats of content from scratch. Along the way,
    you will gain a solid understanding of the inner workings of GANs. This foundation
    will allow us to concentrate on other, more advanced, aspects of GANs in later
    chapters when generating other content such as high-resolution images or realistic-sounding
    music (e.g., convolutional neural networks or how to represent a piece of music
    as a multidimensional object).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标不一定是为了生成具有最实用用途的新颖内容。相反，我的目标是教你如何从头开始训练和使用 GANs 创建各种格式的内 容。在这个过程中，你将获得对
    GANs 内部运作的坚实基础。这个基础将使我们能够在后续章节中集中精力在其他更高级的 GANs 方面，例如生成高分辨率图像或听起来逼真的音乐（例如，卷积神经网络或如何将音乐表示为多维对象）。
- en: 3.1 Steps involved in training GANs
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 训练 GANs 所涉及的步骤
- en: In chapter 1, you gained a high-level overview of the theories behind GANs.
    In this section, I’ll provide a summary of the steps involved in training GANs
    in general and in creating data points to form an exponential growth curve in
    particular.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，你获得了 GANs 背后理论的概览。在本节中，我将总结训练 GANs 的步骤，特别是创建数据点以形成指数增长曲线的步骤。
- en: 'Let’s return to our previous example: you plan to invest in a savings account
    that pays 8% annual interest. You put $1 in the account today and want to know
    how much money you’ll have in the account in the future.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到先前的例子：你计划投资一个年利率为 8% 的储蓄账户。你今天将 $1 存入账户，并想知道未来账户中会有多少钱。
- en: The amount in your account in the future, y, depends on how long you invest
    in the savings account. Let’s denote the number of years you invest by x, which
    can be a number, say, between 0 and 50\. For example, if you invest for 1 year,
    the balance is $1.08; if you invest for 2 years, the balance is 1.08² = $1.17.
    To generalize, the relationship between x and y is y = 1.08^x. The function depicts
    an exponential growth curve. Note here that x can be a whole number such as 1
    or 2, as well as a decimal number such as 1.14 or 2.35 and the formula still works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 未来你账户中的金额，y，取决于你投资储蓄账户的时间长短。让我们用 x 表示你投资年数，它可以是介于 0 到 50 之间的数字。例如，如果你投资 1 年，余额是
    $1.08；如果你投资 2 年，余额是 1.08² = $1.17。为了概括，x 和 y 之间的关系是 y = 1.08^x。这个函数描绘了一个指数增长曲线。请注意，x
    可以是整数，如 1 或 2，也可以是小数，如 1.14 或 2.35，公式仍然适用。
- en: Training GANs to generate data points that conform to a specific mathematical
    relation, like the preceding example, is a multistep process. In your case, you
    want to generate data points (x, y) such that y = 1.08^x. Figure 3.1 provides
    a diagram of the architecture of GANs and the steps involved in generating an
    exponential growth curve. When you generate other content such as a sequence of
    integers, images, or music, you follow similar steps, as you’ll see in the second
    project in this chapter, as well as in other GAN models later in this book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 GANs 生成符合特定数学关系的数据点，如前面的例子，是一个多步骤的过程。在你的情况下，你想要生成数据点（x，y），使得 y = 1.08^x。图
    3.1 提供了 GANs 架构和生成指数增长曲线的步骤图。当你生成其他内容，如整数序列、图像或音乐时，你将遵循类似的步骤，正如你将在本章的第二项目中看到的那样，以及在本书中稍后其他
    GAN 模型中。
- en: '![](../../OEBPS/Images/CH03_F01_Liu.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F01_Liu.png)'
- en: Figure 3.1 The steps involved in training GANs to generate an exponential growth
    curve and the dual-network architecture in GANs. The generator obtains a random
    noise vector Z from the latent space (top left) to create a fake sample and presents
    it to the discriminator (middle). The discriminator classifies a sample as real
    (from the training set) or fake (created by the generator). The predictions are
    compared to the ground truth and both the discriminator and the generator learn
    from the predictions. After many iterations of training, the generator learns
    to create shapes that are indistinguishable from real samples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 训练GAN生成指数增长曲线的步骤以及GAN中的双网络架构。生成器从潜在空间（左上角）获取一个随机噪声向量Z，以创建一个假样本并将其展示给判别器（中间）。判别器将样本分类为真实（来自训练集）或假（由生成器创建）。预测结果与真实值进行比较，判别器和生成器都会从预测结果中学习。经过多次训练迭代后，生成器学会创建与真实样本难以区分的形状。
- en: 'Before we start, we need to obtain a training dataset to train GANs. In our
    running example, we’ll generate a dataset of (x, y) pairs using the mathematical
    relation y = 1.08^x. We use the savings account example so that the numbers are
    relatable. The techniques you learn in this chapter can be applied to other shapes:
    sine, cosine, U-shape, and so on. You can choose a range of x values (say, 0 to
    50) and calculate the corresponding y values. Since we usually train models in
    batches of data in deep learning, the number of observations in your training
    dataset is usually set to a multiple of the batch size. A real sample is located
    at the top of figure 3.1, which has an exponential growth curve shape.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要获得一个训练数据集来训练GAN。在我们的示例中，我们将使用数学关系y = 1.08^x生成一个(x, y)对的集合。我们使用储蓄账户的例子，以便数字更容易理解。本章中你学到的技术可以应用于其他形状：正弦、余弦、U形等。你可以选择一个x值的范围（比如，0到50）并计算相应的y值。由于我们通常在深度学习中以数据批次的形式训练模型，因此训练数据集中的观测数通常设置为批次大小的倍数。一个真实样本位于图3.1的顶部，具有指数增长曲线的形状。
- en: 'Once you have the training set ready, you need to create two networks in GANs:
    a generator and a discriminator. The generator, located at the bottom left of
    figure 3.1, takes a random noise vector Z as the input and generates data points
    (step 1 of our training loop). The random noise vector Z used by the generator
    is obtained from the latent space, which represents the range of possible outputs
    the GAN can produce and is central to the GAN’s ability to generate diverse data
    samples. In chapter 5, we’ll explore the latent space to select the attributes
    of the content created by the generator. The discriminator, located at the center
    of figure 3.1, evaluates whether a given data point (x, y) is real (from the training
    dataset) or fake (created by the generator); this is step 2 of our training loop.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备好了训练集，你需要在GAN中创建两个网络：一个生成器和判别器。生成器位于图3.1的左下角，它以一个随机噪声向量Z作为输入并生成数据点（我们训练循环的第1步）。生成器使用的随机噪声向量Z来自潜在空间，这代表了GAN可以产生的可能输出范围，并且对于GAN生成多样化的数据样本的能力至关重要。在第5章中，我们将探索潜在空间以选择生成器创建的内容的属性。判别器位于图3.1的中心，它评估给定的数据点(x,
    y)是真实（来自训练数据集）还是假（由生成器创建）；这是我们训练循环的第2步。
- en: The meaning of the latent space
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间的意义
- en: The latent space in a GAN is a conceptual space where each point can be transformed
    into a realistic data instance by the generator. This space represents the range
    of possible outputs the GAN can produce and is central to the GAN’s ability to
    generate varied and complex data. The latent space acquires its significance exclusively
    when it is employed in conjunction with the generative model. Within this context,
    one can interpolate between points in the latent space to affect the attributes
    of output, which we’ll discuss in chapter 5.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GAN中的潜在空间是一个概念空间，其中每个点都可以通过生成器转换成一个真实的数据实例。这个空间代表了GAN可以产生的可能输出范围，并且对于GAN生成多样化和复杂数据的能力至关重要。潜在空间只有在与生成模型结合使用时才具有其重要性。在这个背景下，可以在潜在空间中的点之间进行插值，以影响输出的属性，我们将在第5章中讨论这一点。
- en: To know how to adjust model parameters, we must choose the right loss functions.
    We need to define the loss functions for both the generator and discriminator.
    The loss function encourages the generator to generate data points that resemble
    data points from the training dataset, making the discriminator classify them
    as real. The loss function encourages the discriminator to correctly classify
    real and generated data points.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要知道如何调整模型参数，我们必须选择正确的损失函数。我们需要为生成器和判别器定义损失函数。损失函数鼓励生成器生成类似于训练数据集数据点的数据点，使判别器将它们分类为真实。损失函数鼓励判别器正确分类真实和生成数据点。
- en: In each iteration of the training loop, we alternate between training the discriminator
    and the generator. During each training iteration, we sample a batch of real (x,
    y) data points from the training dataset and a batch of fake data points generated
    by the generator. When training the discriminator, we compare the predictions
    by the discriminative model, which is a probability that the sample is from the
    training set, with the ground truth, which is 1 if the sample is real and 0 if
    the sample is fake (shown at the right of figure 3.1); this constitutes half of
    step 3 in the training loop. We adjust the weights in the discriminator network
    slightly so that in the next iteration, the predicted probability moves closer
    to the ground truth (half of step 4 in our training loop).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环的每一次迭代中，我们交替训练判别器和生成器。在每次训练迭代中，我们从训练数据集中采样一批真实的数据点（x, y）和一批由生成器生成的虚假数据点。当训练判别器时，我们比较判别模型（它是一个样本来自训练集的概率）的预测与真实值，真实值为1（如果样本是真实的）和0（如果样本是虚假的）（如图3.1右侧所示）；这构成了训练循环中的第3步的一半。我们稍微调整判别器网络中的权重，以便在下一个迭代中，预测概率更接近真实值（这是我们的训练循环中第4步的一半）。
- en: When training the generator, we feed fake samples to the discriminative model
    and obtain a probability that the sample is real (the other half of step 3). We
    then adjust the weights in the generator network slightly so that in the next
    iteration, the predicted probability moves closer to 1 (since the generator wants
    to create samples to fool the discriminator into thinking they are real); this
    constitutes the other half of step 4\. We repeat this process for many iterations,
    making the generator network create more realistic data points.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练生成器时，我们将虚假样本输入到判别模型中，并获得样本是真实的概率（第3步的另一半）。然后我们稍微调整生成器网络中的权重，以便在下一个迭代中，预测概率更接近1（因为生成器想要创建样本来欺骗判别器，使其认为它们是真实的）；这构成了第4步的另一半。我们重复这个过程多次迭代，使生成器网络创建更真实的数据点。
- en: A natural question is when to stop training the GANs. For that, you evaluate
    the GAN’s performance by generating a set of synthetic data points and comparing
    them to the real data points from the training dataset. In most cases, we use
    visualization techniques to assess how well the generated data conforms to the
    desired relation. However, in our running example, since we know the distribution
    of the training data, we can calculate the mean squared error (MSE) between the
    generated data and the true data distribution. We stop training GANs when the
    generated samples stop improving their qualities after a fixed number of rounds
    of training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是何时停止训练GANs。为此，你通过生成一组合成数据点并将其与训练数据集中的真实数据点进行比较来评估GAN的性能。在大多数情况下，我们使用可视化技术来评估生成的数据与所需关系的一致性。然而，在我们的运行示例中，由于我们知道训练数据的分布，我们可以计算生成数据与真实数据分布之间的均方误差（MSE）。当在固定数量的训练轮次后，生成的样本停止提高其质量时，我们停止训练GANs。
- en: At this point, the model is considered trained. We then discard the discriminator
    and keep the generator. To create an exponential growth curve, we feed a random
    noise vector Z to the trained generator and obtain pairs of (x, y) to form the
    desired shape.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，模型被认为是训练好的。然后我们丢弃判别器，保留生成器。为了创建指数增长曲线，我们将随机噪声向量Z输入到训练好的生成器中，并获取(x, y)对以形成所需的形状。
- en: 3.2 Preparing training data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 准备训练数据
- en: In this section, you’ll create the training dataset so that you can use it to
    train the GAN model later in this chapter. Specifically, you’ll create pairs of
    data points (x, y) that conform to the exponential growth shape. You’ll place
    them in batches so that they are ready to be fed to deep neural networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将创建训练数据集，以便你可以使用它来训练本章后面的GAN模型。具体来说，你将创建符合指数增长形状的数据点对(x, y)。你将将它们放入批次中，以便它们可以准备好输入到深度神经网络中。
- en: 'NOTE The code for this chapter, as well as other chapters in this book, is
    available at the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章以及本书其他章节的代码可在本书的 GitHub 仓库中找到：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: 3.2.1 A training dataset that forms an exponential growth curve
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 形成指数增长曲线的训练数据集
- en: We’ll create a dataset that contains many observations of data pairs, (x, y),
    where x is uniformly distributed in the interval [0, 50] and y is related to x
    based on the formula y = 1.08^x, as shown in the following listing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含许多数据对观察结果的数据集，即 (x, y)，其中 x 在区间 [0, 50] 内均匀分布，y 根据公式 y = 1.08^x 与 x
    相关，如下所示。
- en: Listing 3.1 Creating training data to form an exponential growth shape
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 创建训练数据以形成指数增长形状
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Fixes the random state so results are reproducible
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ① 固定随机状态以确保结果可重现
- en: ② Creates a tensor with 2,048 rows and 2 columns
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有 2,048 行和 2 列的张量
- en: ③ Generates values of x between 0 and 50
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 生成 0 到 50 之间的 x 的值
- en: ④ Generates values of y based on the relation y = 1.08^x
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 根据关系 y = 1.08^x 生成 y 的值
- en: First, we create 2,048 values of x between 0 and 50 using the `torch.rand()`
    method. We use the `manual_seed()` method in PyTorch to fix the random state so
    that all results are reproducible. We first create a PyTorch tensor, `train_data`,
    with 2,048 rows and 2 columns. The values of x are placed in the first column
    in the tensor `train_data`. The `rand()` method in PyTorch generates random values
    between 0.0 and 1.0\. By multiplying the value by 50, the resulting values of
    x are between 0.0 and 50.0\. We then fill the second column of `train_data` with
    values of y = 1.08^x.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `torch.rand()` 方法创建 0 到 50 之间 2,048 个 x 的值。我们使用 PyTorch 中的 `manual_seed()`
    方法固定随机状态，以确保所有结果都是可重现的。我们首先创建一个具有 2,048 行和 2 列的 PyTorch 张量 `train_data`。x 的值放置在张量
    `train_data` 的第一列中。PyTorch 中的 `rand()` 方法生成介于 0.0 和 1.0 之间的随机值。通过将值乘以 50，得到的 x
    的值介于 0.0 和 50.0 之间。然后，我们将 y = 1.08^x 的值填充到 `train_data` 的第二列中。
- en: Exercise 3.1
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.1
- en: 'Modify listing 3.1 so that the relation between x and y is y = sin(x) by using
    the `torch.sin()` function. Set the value of x between –5 and 5 by using this
    line of code: `train_data[:,0]=10*(torch.rand(observations)-0.5)`.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表 3.1 修改为使用 `torch.sin()` 函数，使 x 和 y 之间的关系为 y = sin(x)。通过以下代码行设置 x 的值为 -5
    到 5：`train_data[:,0]=10*(torch.rand(observations)-0.5)`。
- en: We plot the relation between x and y by using the Matplotlib library.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Matplotlib 库绘制 x 和 y 之间的关系。
- en: Listing 3.2 Visualizing the relation between x and y
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 可视化 x 和 y 之间的关系
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Plots the relation between x and y
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ① 绘制 x 和 y 之间的关系
- en: ② Labels y-axis
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ② 标记 y 轴
- en: ③ Creates a title for the plot
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为图表创建标题
- en: You will see an exponential growth curve shape after running listing 3.2, which
    is similar to the top graph in figure 3.1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表 3.2 后，您将看到一个指数增长曲线形状，类似于图 3.1 中的顶部图表。
- en: Exercise 3.2
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.2
- en: Modify listing 3.2 to plot the relation between x and y = sin(x) based on your
    changes in exercise 3.1\. Make sure you change the y-axis label and the title
    in the plot to reflect the changes you made.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据练习 3.1 中的更改，修改列表 3.2 以根据 x 和 y = sin(x) 的关系绘制图表。确保您更改图表中的 y 轴标签和标题，以反映您所做的更改。
- en: 3.2.2 Preparing the training dataset
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 准备训练数据集
- en: 'We’ll place the data samples you just created into batches so that we can feed
    them to the discriminator network. We use the `DataLoader()` class in PyTorch
    to wrap an iterable around the training dataset so that we can easily access the
    samples during training, like so:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把您刚刚创建的数据样本放入批次中，以便我们可以将它们输入到判别网络中。我们使用 PyTorch 中的 `DataLoader()` 类将可迭代对象包装在训练数据集周围，这样我们就可以在训练过程中轻松访问样本，如下所示：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Make sure you select the total number of observations and the batch size so
    that all batches have the same number of samples in them. We chose 2,048 observations
    with a batch size of 128\. As a result, we have 2,048/128 = 16 batches. The `shuffle=True`
    argument in `DataLoader()` shuffles the observations randomly before dividing
    them into batches.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您选择观察总数和批量大小，以便所有批次中的样本数量相同。我们选择了 2,048 个观察结果和 128 的批量大小。因此，我们有 2,048/128
    = 16 个批次。`DataLoader()` 中的 `shuffle=True` 参数在将观察结果分成批次之前随机打乱观察结果。
- en: NOTE Shuffling makes sure that the data samples are evenly distributed and observations
    within a batch are not correlated, which, in turn, stabilizes training. In this
    specific example, shuffling ensures that values of x fall randomly between 0 and
    50, instead of clustering in a certain range, say, between 0 and 5.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：打乱数据确保数据样本均匀分布，并且批次内的观察结果不相关，这反过来又稳定了训练。在这个特定示例中，打乱确保x的值在0到50之间随机分布，而不是聚集在某个范围内，比如0到5之间。
- en: 'You can access a batch of data by using the `next()` and `iter()` methods,
    like so:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`next()`和`iter()`方法来访问一批数据，如下所示：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will see 128 pairs of numbers (x, y), where the value of x falls randomly
    between 0 and 50\. Further, the values of x and y in each pair conform to the
    relation y = 1.08^x.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到128对数字（x, y），其中x的值在0到50之间随机分布。此外，每对中的x和y的值符合关系 y = 1.08^x。
- en: 3.3 Creating GANs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 创建 GANs
- en: Now that the training dataset is ready, we’ll create a discriminator network
    and a generator network. The discriminator network is a binary classifier, which
    is very similar to the binary classifier for clothing items we have created and
    trained in chapter 2\. Here, the discriminator’s job is to classify the samples
    into either real or fake. The generator network, on the other hand, tries to create
    data points (x, y) that are indistinguishable from those in the training set so
    that the discriminator will classify them as real.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练数据集已经准备好了，我们将创建一个判别网络和一个生成网络。判别网络是一个二元分类器，这与我们在第2章中创建和训练的服装物品二元分类器非常相似。在这里，判别网络的职责是将样本分类为真实或伪造。另一方面，生成网络试图创建与训练集中那些不可区分的数据点（x,
    y），以便判别器将它们分类为真实。
- en: 3.3.1 The discriminator network
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 判别网络
- en: We use PyTorch to create a discriminator neural network. We’ll use fully connected
    (dense) layers with `ReLU` activations. We’ll also use dropout layers to prevent
    overfitting. We create a sequential deep neural network in PyTorch to represent
    the discriminator, as shown in the following listing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch创建判别神经网络。我们将使用带有`ReLU`激活的全连接（密集）层。我们还将使用dropout层来防止过拟合。我们在PyTorch中创建一个顺序深度神经网络来表示判别器，如下所示。
- en: Listing 3.3 Creating a discriminator network
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3 创建判别网络
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Automatically checks if CUDA-enabled GPU is available.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ① 自动检查CUDA启用GPU是否可用。
- en: ② The number of input features in the first layer is 2, matching the number
    of elements in each data instance, which has two values, x and y .
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ② 第一层的输入特征数是2，与每个数据实例中的元素数量相匹配，每个数据实例有两个值，x和y。
- en: ③ The dropout layer prevents overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ③ dropout 层可以防止过拟合。
- en: ④ The number of output features in the last layer is 1 so that we can squeeze
    it into a value between 0 and 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 最后层的输出特征数是1，这样我们就可以将其压缩到0到1之间的值。
- en: 'Make sure that in the first layer, the input shape is 2 because, in our sample,
    each data instance has two values in it: x and y . The number of inputs in the
    first layer should always match with the size of the input data. Also, make sure
    that the number of output features is 1 in the last layer: the output of the discriminator
    network is a single value. We use the sigmoid activation function to squeeze the
    output to the range [0, 1] so that it can be interpreted as the probability, p,
    that the sample is real. With the complementary probability, 1 – p, the sample
    is fake. This is very similar to what we have done in chapter 2 when a binary
    classifier attempts to identify a piece of clothing item as either an ankle boot
    or a t-shirt.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在第一层中，输入形状是2，因为在我们这个示例中，每个数据实例中包含两个值：x和y。第一层的输入数量应始终与输入数据的大小相匹配。同时，确保最后一层的输出特征数是1：判别网络的输出是一个单一值。我们使用sigmoid激活函数将输出压缩到[0,
    1]的范围内，以便它可以被解释为样本是真实的概率，p。使用互补概率1 – p，样本是伪造的。这与我们在第2章中做的非常相似，当时二元分类器试图识别一件服装物品是长靴还是T恤。
- en: The hidden layers have 256, 128, and 64 neurons in them, respectively. There
    is nothing magical about these numbers, and you can easily change them and have
    similar results as long as they are in a reasonable range. If the number of neurons
    in hidden layers is too large, it may lead to overfitting of the model; if the
    number is too small, it may lead to underfitting. The number of neurons can be
    optimized separately using a validation set through hyperparameter tuning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层分别包含256、128和64个神经元。这些数字并没有什么神奇之处，只要在合理的范围内，你可以轻松地更改它们，并得到相似的结果。如果隐藏层中的神经元数量过多，可能会导致模型过拟合；如果数量过少，可能会导致模型欠拟合。可以通过使用验证集和超参数调整来单独优化神经元数量。
- en: Dropout layers randomly deactivate (or “drop out”) a certain percentage of neurons
    in the layer to which they are applied. This means that these neurons do not participate
    in forward or backward passes during training. Overfitting occurs when a model
    learns not only the underlying patterns in the training data but also the noise
    and random fluctuations, leading to poor performance on unseen data. Dropout layers
    are an effective way to prevent overfitting.^([2](#footnote-000))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout层随机地使应用到的层中的某些神经元失效（或“丢弃”）。这意味着这些神经元在训练过程中不参与正向或反向传播。当模型不仅学习到训练数据中的潜在模式，还学习到噪声和随机波动时，就会发生过拟合，导致在未见过的数据上表现不佳。Dropout层是防止过拟合的有效方法.^([2](#footnote-000))
- en: 3.3.2 The generator network
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 生成器网络
- en: The generator’s job is to create a pair of numbers (x, y) so that it can pass
    the screening of the discriminator. That is, the generator is trying to create
    a pair of numbers to maximize the probability that the discriminator thinks that
    the numbers are from the training dataset (i.e., they conform to the relation
    y = 1.08^x). We create the neural network in the following listing to represent
    the generator.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的任务是创建一对数字（x，y），以便它可以通过判别器的筛选。也就是说，生成器试图创建一对数字，以最大化判别器认为这些数字来自训练数据集（即，它们符合关系y
    = 1.08^x）的概率。我们以下列表中的神经网络来表示生成器。
- en: Listing 3.4 Creating a generator network
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 创建生成器网络
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① The number of input features in the first layer is 2, the same as the dimension
    of the random noise vector from the latent space.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ① 第一层的输入特征数量为2，与来自潜在空间的随机噪声向量的维度相同。
- en: ② The number of output features in the last layer is 2, the same as the dimension
    of the data sample, which contains two values (x, y).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ② 最后层的输出特征数量为2，与数据样本的维度相同，包含两个值（x，y）。
- en: We feed a random noise vector from a 2D latent space, (z[1], z[2]), to the generator.
    The generator then generates a pair of values (x, y), based on the input from
    the latent space. Here we use a 2D latent space, but changing the dimension to
    other numbers such as 5 or 10 wouldn’t affect our results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将来自二维潜在空间（z[1]，z[2]）的随机噪声向量输入到生成器中。然后，生成器根据潜在空间的输入生成一对值（x，y）。在这里，我们使用二维潜在空间，但将维度更改为其他数字，如5或10，不会影响我们的结果。
- en: 3.3.3 Loss functions, optimizers, and early stopping
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 损失函数、优化器和早停
- en: 'Since the discriminator network is essentially performing a binary classification
    task (identifying a data sample as real or fake), we use binary cross-entropy
    loss, the preferred loss function in binary classifications, for the discriminator
    network. The discriminator is trying to maximize the accuracy of the binary classification:
    identify a real sample as real and a fake sample as fake. The weights in the discriminator
    network are updated based on the gradient of the loss function with respect to
    the weights.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于判别器网络本质上执行的是二元分类任务（识别数据样本为真实或假），因此我们为判别器网络使用二元交叉熵损失函数，这是二元分类中首选的损失函数。判别器试图最大化二元分类的准确性：将真实样本识别为真实，将假样本识别为假。判别器网络中的权重根据损失函数相对于权重的梯度进行更新。
- en: 'The generator is trying to minimize the probability that the fake sample is
    being identified as fake. Therefore, we’ll also use binary cross-entropy loss
    for the generator network: the generator updates its network weights so that the
    generated samples will be classified as real by the discriminator in a binary
    classification problem.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器试图最小化假样本被识别为假样本的概率。因此，我们也将使用二元交叉熵损失函数来训练生成器网络：生成器更新其网络权重，使得生成的样本在二元分类问题中被判别器分类为真实样本。
- en: 'As we have done in chapter 2, we use the Adam optimizer as the gradient descent
    algorithm. We set the learning rate to 0.0005\. Let’s code those steps in by using
    PyTorch:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第2章中所做的那样，我们使用 Adam 优化器作为梯度下降算法。我们将学习率设置为 0.0005。让我们通过使用 PyTorch 来实现这些步骤：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One question remains before we get to the actual training: How many epochs
    should we train the GANs? How do we know the model is well trained so that the
    generator is ready to create samples that can mimic the exponential growth curve
    shape? If you recall, in chapter 2, we split the training set further into a train
    set and a validation set. We then used the loss in the validation set to determine
    whether the parameters had converged so that we could stop training. However,
    GANs are trained using a different approach compared to traditional supervised
    learning models (such as the classification models you have seen in chapter 2).
    Since the quality of the generated samples improves throughout training, the discriminator’s
    task becomes more and more difficult (in a way, the discriminator in GANs is making
    predictions on a moving target). The loss from the discriminator network is not
    a good indicator of the quality of the model.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行实际训练之前，还有一个问题：我们应该训练多少个epoch？我们如何知道模型已经很好地训练，以至于生成器可以创建可以模仿指数增长曲线形状的样本？如果你还记得，在第2章中，我们将训练集进一步分为训练集和验证集。然后我们使用验证集中的损失来确定参数是否收敛，以便我们可以停止训练。然而，GANs
    的训练方法与传统监督学习模型（如你在第2章中看到的分类模型）不同。由于生成样本的质量在整个训练过程中都在提高，判别器的任务变得越来越困难（在某种程度上，GANs
    中的判别器是在对一个移动目标进行预测）。判别器网络的损失并不是模型质量的良好指标。
- en: 'One common method to measure the performance of GANs is through visual inspection.
    Humans can assess the quality and realism of generated data instances by simply
    looking at them. This is a qualitative approach but can be very informative. But
    in our simple case, since we know the exact distribution of the training dataset,
    we’ll look at the MSE of the generated samples relative to samples in the training
    set and use it as a measure of the performance of the generator. Let’s code that
    in:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测量 GANs 性能的一种常见方法是通过视觉检查。人类可以通过简单地查看它们来评估生成数据实例的质量和真实性。这是一种定性方法，但可以提供非常有价值的信息。但在我们的简单案例中，因为我们知道训练数据集的确切分布，我们将查看生成样本相对于训练集中样本的均方误差（MSE），并将其用作生成器性能的衡量标准。让我们用代码来实现这一点：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Uses MSE as the criterion to measure performance
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 MSE 作为衡量性能的标准
- en: ② Finds out the true distribution
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ② 找出真实分布
- en: ③ Compares the generated distribution with the true distribution and calculates
    MSE
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将生成的分布与真实分布进行比较，并计算 MSE
- en: We’ll stop training the model if the performance of the generator doesn’t improve
    in, say, 1,000 epochs. Therefore, we define an early stopping class, as we did
    in chapter 2, to decide when to stop training the model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成器的性能在1,000个epoch内没有提高，我们将停止训练模型。因此，我们定义了一个早期停止类，就像我们在第2章中所做的那样，以决定何时停止训练模型。
- en: Listing 3.5 An early stopping class to decide when to stop training
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 一个早期停止类，用于决定何时停止训练
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Sets the default value of patience to 1000
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将耐心（patience）的默认值设置为 1000
- en: ② Defines the stop() method
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义 stop() 方法
- en: ③ If a new minimum difference between the generated distribution and true distribution
    is reached, updates the value of min_gdif.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 当达到生成分布和真实分布之间新的最小差异时，更新 min_gdif 的值。
- en: ④ Stops training if the model stops improving for 1,000 epochs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如果模型在1,000个epoch内停止改进，则停止训练
- en: With that, we have all the components we need to train our GANs, which we’ll
    do in the next section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '这样，我们就拥有了训练我们的 GANs 所需的所有组件，我们将在下一节中进行训练。 '
- en: 3.4 Training and using GANs for shape generation
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 使用 GANs 进行形状生成训练和利用
- en: Now that we have the training data and two networks, we’ll train the model.
    After that, we’ll discard the discriminator and use the generator to generate
    data points to form an exponential growth curve shape.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练数据和两个网络，我们将训练模型。之后，我们将丢弃判别器，并使用生成器生成数据点以形成指数增长曲线形状。
- en: 3.4.1 The training of GANs
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 GANs 的训练
- en: We first create labels for real samples and fake samples, respectively. Specifically,
    we’ll label all real samples as 1s and all fake samples as 0s. During the training
    process, the discriminator compares its own predictions with the labels to receive
    feedback so that it can adjust model parameters to make better predictions in
    the next iteration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为真实样本和假样本分别创建标签。具体来说，我们将所有真实样本标记为 1，所有假样本标记为 0。在训练过程中，判别器将其自己的预测与标签进行比较，以获得反馈，以便它可以调整模型参数，在下一个迭代中做出更好的预测。
- en: 'Here, we define two tensors, `real_labels` and `fake_labels`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两个张量，`real_labels` 和 `fake_labels`：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The tensor `real_labels` is 2D with a shape of `(batch_size, 1)`—that is, 128
    rows and 1 column. We use 128 rows because we’ll feed a batch of 128 real samples
    to the discriminator network to obtain 128 predictions. Similarly, the tensor
    `fake_labels` is 2D with a shape of `(batch_size, 1)`. We’ll feed a batch of 128
    fake samples to the discriminator network to obtain 128 predictions and compare
    them with the ground truth: 128 labels of 0s. We move the two tensors to the GPU
    for fast training if your computer has a CUDA-enabled GPU.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 张量 `real_labels` 是一个形状为 `(batch_size, 1)` 的二维张量——即 128 行和 1 列。我们使用 128 行，因为我们将会向判别网络提供
    128 个真实样本以获得 128 个预测。同样，张量 `fake_labels` 也是一个形状为 `(batch_size, 1)` 的二维张量。我们将向判别网络提供
    128 个假样本以获得 128 个预测，并将它们与真实标签：128 个 0 进行比较。如果你的计算机具有启用 CUDA 的 GPU，我们将这两个张量移动到
    GPU 上以实现快速训练。
- en: To train the GANs, we define a few functions so that the training loop looks
    organized. The first function, `train_D_on_real()`, trains the discriminator network
    with a batch of real samples.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练 GANs，我们定义了一些函数，以便训练循环看起来更有组织。第一个函数 `train_D_on_real()` 使用一批真实样本来训练判别网络。
- en: Listing 3.6 Defining a `train_D_on_real()` function
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 定义 `train_D_on_real()` 函数
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Makes predictions on real samples
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对真实样本进行预测
- en: ② Calculates loss
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算损失
- en: ③ Backpropagation (i.e., updates model weights in the discriminator network
    so predictions are more accurate in the next iteration)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 反向传播（即更新判别网络中的模型权重，以便在下一个迭代中做出更准确的预测）
- en: The function `train_D_on_real()` first moves the real samples to GPU if the
    computer has a CUDA-enabled GPU. The discriminator network, D, makes predictions
    on the batch of samples. The model then compares the discriminator’s predictions,
    `out_D`, with the ground truth, `real_labels`, and calculates the loss of the
    predictions accordingly. The `backward()` method calculates the gradients of the
    loss function with respect to model parameters. The `step()` method adjusts the
    model parameters (that is, backpropagation). The `zero_grad()` method means that
    we explicitly set the gradients to 0 before backpropagation. Otherwise, the accumulated
    gradients instead of the incremental gradients are used on every `backward()`
    call.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `train_D_on_real()` 首先将真实样本移动到 GPU 上，如果计算机具有启用 CUDA 的 GPU。判别网络 D 对样本批次进行预测。然后模型将判别器的预测
    `out_D` 与真实标签 `real_labels` 进行比较，并相应地计算预测的损失。`backward()` 方法计算损失函数相对于模型参数的梯度。`step()`
    方法调整模型参数（即反向传播）。`zero_grad()` 方法表示我们在反向传播之前显式地将梯度设置为 0。否则，在每次 `backward()` 调用中都会使用累积的梯度而不是增量梯度。
- en: TIP We call the `zero_grad()` method before updating model weights when training
    each batch of data. We explicitly set the gradients to 0 before backpropagation
    to use incremental gradients instead of the accumulated gradients on every `backward()`
    call.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 在训练每个数据批次时更新模型权重之前，我们调用 `zero_grad()` 方法。我们在反向传播之前显式地将梯度设置为 0，以使用增量梯度而不是在每次
    `backward()` 调用中累积的梯度。
- en: The second function, `train_D_on_fake()`, trains the discriminator network with
    a batch of fake samples.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数 `train_D_on_fake()` 使用一批假样本来训练判别网络。
- en: Listing 3.7 Defining the `train_D_on_fake()` function
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 定义 `train_D_on_fake()` 函数
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Generates a batch of fake samples
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成一批假样本
- en: ② Makes predictions on the fake samples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ② 对假样本进行预测
- en: ③ Calculates loss
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算损失
- en: ④ Backpropagation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 反向传播
- en: The function `train_D_on_fake()` first feeds a batch of random noise vectors
    from the latent space to the generator to obtain a batch of fake samples. The
    function then presents the fake samples to the discriminator to obtain predictions.
    The function compares the discriminator’s predictions, `out_D`, with the ground
    truth, `fake_labels`, and calculates the loss of the predictions accordingly.
    Finally, it adjusts the model parameters based on the gradients of the loss function
    with respect to model weights.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`train_D_on_fake()`首先将来自潜在空间的一批随机噪声向量输入到生成器中，以获得一批假样本。然后，该函数将假样本展示给判别器以获取预测结果。该函数将判别器的预测结果`out_D`与真实标签`fake_labels`进行比较，并相应地计算预测的损失。最后，它根据损失函数相对于模型权重的梯度调整模型参数。
- en: Note We use the terms *weights* and *parameters* interchangeably. Strictly speaking,
    model parameters also include bias terms, but we use the term *model weights*
    loosely to include model biases. Similarly, we use the terms *adjusting weights*,
    *adjusting parameters*, and *backpropagation* interchangeably.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们互换使用术语*权重*和*参数*。严格来说，模型参数还包括偏置项，但我们使用术语*模型权重*来松散地包括模型偏置。同样，我们互换使用术语*调整权重*、*调整参数*和*反向传播*。
- en: The third function, `train_G()`, trains the generator network with a batch of
    fake samples.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个函数`train_G()`使用一批假样本训练生成器网络。
- en: Listing 3.8 Defining the `train_G()` function
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 定义`train_G()`函数
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Creates a batch of fake samples
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一批假样本
- en: ② Presents the fake samples to the discriminator to obtain predictions
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ② 向判别器展示假样本以获取预测结果
- en: ③ Calculates the loss based on whether G has succeeded
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 根据G是否成功计算损失
- en: ④ Backpropagation (i.e., updates weights in the generator network so the generated
    samples are more realistic in the next iteration)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 反向传播（即更新生成器网络中的权重，以便在下一迭代中生成的样本更加逼真）
- en: To train the generator, we first feed a batch of random noise vectors from the
    latent space to the generator to obtain a batch of fake samples. We then present
    the fake samples to the discriminator network to obtain a batch of predictions.
    We compare the discriminator’s predictions with `real_labels`, a tensor of 1s,
    and calculate the loss. It’s important that we use a tensor of 1s, not a tensor
    of 0s, as the labels, because the objective of the generator is to fool the discriminator
    into thinking that fake samples are real. Finally, we adjust the model parameters
    based on the gradients of the loss function with respect to model weights so that
    in the next iteration, the generator can create more realistic samples.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练生成器，我们首先将来自潜在空间的一批随机噪声向量输入到生成器中，以获得一批假样本。然后，我们将这些假样本展示给判别器网络以获取一批预测结果。我们比较判别器的预测结果与`real_labels`（一个全为1的张量）进行比较，并计算损失。使用全为1的张量而不是全为0的张量作为标签非常重要，因为生成器的目标是欺骗判别器，使其认为假样本是真实的。最后，我们根据损失函数相对于模型权重的梯度调整模型参数，以便在下一迭代中，生成器可以创建更加逼真的样本。
- en: Note We use the tensor `real_labels` (a tensor of 1s) instead of `fake_labels`
    (a tensor of 0s) when calculating loss and assessing the generator network because
    the generator wants the discriminator to predict fake samples as real.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在计算损失和评估生成器网络时，我们使用`real_labels`（一个全为1的张量）而不是`fake_labels`（一个全为0的张量），因为生成器希望判别器将假样本预测为真实样本。
- en: Finally, we define a function, `test_epoch()`, which prints out the losses for
    the discriminator and the generator periodically. Further, it plots the data points
    generated by the generator and compares them to those in the training set. The
    function `test_epoch()` is shown in the following listing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了一个函数`test_epoch()`，该函数定期打印出判别器和生成器的损失。此外，它绘制了生成器生成的数据点，并将它们与训练集中的数据点进行比较。`test_epoch()`函数的代码如下所示。
- en: Listing 3.9 Defining the `test_epoch()` function
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.9 定义`test_epoch()`函数
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Creates a folder to hold files
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个文件夹来存放文件
- en: ② Periodically prints out losses
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定期打印出损失
- en: ③ Plots the generated points as asterisks (*)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将生成的点以星号（*）的形式绘制出来
- en: ④ Plots training data as dots (.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将训练数据以点（.）的形式绘制出来
- en: After every 25 epochs, the function prints out the average losses for the generator
    and the discriminator in the epoch. Further, it plots a batch of fake data points
    generated by the generator (in asterisks) and compares them to the data points
    in the training set (in dots). The plot is saved as an image in your local folder
    /files/.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每过 25 个周期，函数会在该周期打印出生成器和判别器的平均损失。此外，它还会绘制生成器生成的一批假数据点（用星号表示）并与训练集中的数据点（用点表示）进行比较。该图被保存为图像保存在你的本地文件夹
    /files/ 中。
- en: Now we are ready to train the model. We iterate through all batches in the training
    dataset. For each batch of data, we first train the discriminator using the real
    samples. After that, the generator creates a batch of fake samples, and we use
    them to train the discriminator again. Finally, we let the generator create a
    batch of fake samples again, but this time, we use them to train the generator
    instead. We train the model until the early stopping condition is satisfied, as
    shown in the following listing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练模型了。我们遍历训练数据集中的所有批次。对于每一批数据，我们首先使用真实样本来训练判别器。之后，生成器创建一批假样本，我们再次使用这些样本来训练判别器。最后，我们让生成器再次创建一批假样本，但这次，我们使用它们来训练生成器。我们训练模型直到满足早期停止条件，如下所示。
- en: Listing 3.10 Training GANs to generate an exponential growth curve
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.10 训练生成指数增长曲线的 GAN
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Starts training loops
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ① 开始训练循环
- en: ② Iterates through all batches in the training dataset
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历训练数据集中的所有批次
- en: ③ Shows generated samples periodically
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定期展示生成的样本
- en: ④ Determines if training should stop
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 确定是否停止训练
- en: 'The training takes a few minutes if you are using GPU training. Otherwise,
    it may take 20 to 30 minutes, depending on the hardware configuration on your
    computer. Alternatively, you can download the trained model from the book’s GitHub
    repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 GPU 训练，训练可能需要几分钟。否则，可能需要 20 到 30 分钟，具体取决于你电脑的硬件配置。或者，你可以从本书的 GitHub 仓库下载训练好的模型：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: 'After 25 epochs of training, the generated data are scattered around the point
    (0,0) and don’t form any meaningful shape (an epoch is when all training data
    is used for training once). After 200 epochs of training, the data points start
    to form an exponential growth curve shape, even though many points are far away
    from the dotted curve, which is formed by points from the training set. After
    1,025 epochs, the generated points fit closely with the exponential growth curve.
    Figure 3.2 provides subplots of the output from six different epochs. Our GANs
    work really well: the generator is able to generate data points to form the desired
    shape.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 25 个训练周期后，生成的数据点围绕点 (0,0) 散布，并没有形成任何有意义的形状（一个周期是指所有训练数据被用于训练一次）。经过 200 个训练周期后，数据点开始形成指数增长曲线的形状，尽管许多点远离由训练集点形成的虚线曲线。经过
    1,025 个训练周期后，生成的点与指数增长曲线非常吻合。图 3.2 提供了六个不同周期的输出子图。我们的 GAN 工作得非常好：生成器能够生成形成所需形状的数据点。
- en: '![](../../OEBPS/Images/CH03_F02_Liu.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F02_Liu.png)'
- en: Figure 3.2 Subplots of the comparison of the generated shape (asterisks in the
    graph) with the true exponential growth curve shape (dots in the graph) at different
    stages of the training process. At epoch 25, the generated samples don’t form
    any meaningful shape. At epoch 200, the samples start to look like an exponential
    growth curve shape. At epoch 1025, the generated samples align closely with the
    exponential growth curve.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 训练过程中不同阶段的生成形状（图中星号所示）与真实指数增长曲线形状（图中点所示）的比较子图。在第 25 个周期，生成的样本没有形成任何有意义的形状。在第
    200 个周期，样本开始看起来像指数增长曲线的形状。在第 1,025 个周期，生成的样本与指数增长曲线非常吻合。
- en: 3.4.2 Saving and using the trained generator
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 保存和使用训练好的生成器
- en: 'Now that the GANs are trained, we’ll discard the discriminator network, as
    we always do in GANs, and save the trained generator network in the local folder,
    as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 既然 GAN 已经训练完成，我们将丢弃判别器网络，就像在 GAN 中我们通常所做的那样，并将训练好的生成器网络保存在本地文件夹中，如下所示：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `torch.jit.script()` method scripts a function or a `nn.Module` class as
    TorchScript code using the TorchScript compiler. We use the method to script our
    trained generator network and save it as a file, `exponential.pt`, on your computer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.jit.script()` 方法使用 TorchScript 编译器将函数或 `nn.Module` 类脚本化为 TorchScript
    代码。我们使用此方法将训练好的生成器网络脚本化，并将其保存为文件 `exponential.pt` 在你的电脑上。'
- en: 'To use the generator, we don’t even need to define the model. We simply load
    up the saved file and use it to generate data points as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用生成器，我们甚至不需要定义模型。我们只需加载保存的文件，并使用它来生成数据点，如下所示：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The trained generator is now loaded to your device, which is either `CPU` or
    `CUDA` depending on if you have a CUDA-enabled GPU on your computer. The `map_location=device`
    argument in `torch.jit.load()` specifies where to load the generator. We can now
    use the trained generator to generate a batch of data points:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的生成器现在已加载到你的设备上，这取决于你是否在电脑上安装了CUDA支持的GPU，它可能是`CPU`或`CUDA`。`torch.jit.load()`中的`map_location=device`参数指定了加载生成器的位置。现在我们可以使用训练好的生成器生成一批数据点：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we first obtain a batch of random noise vectors from the latent space.
    We then feed them to the generator to produce the fake data. We can plot the generated
    data:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先从潜在空间中获取一批随机噪声向量。然后，我们将它们输入到生成器中以生成假数据。我们可以绘制生成的数据：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Plots the generated data samples as asterisks
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ① 以星号形式绘制生成的数据样本
- en: ② Plots the training data as dots
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ② 以点形式绘制训练数据
- en: 'You should see a plot similar to the last subplot in figure 3.2: the generated
    data samples closely resemble an exponential growth curve.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个类似于图3.2中最后一个子图的图表：生成的数据样本非常接近指数增长曲线。
- en: Congratulations! You have created and trained your very first GANs. Armed with
    this skill, you can easily change the code so that the generated data matches
    other shapes such as sine, cosine, U-shape, and so on.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经创建并训练了你的第一个GAN。掌握了这项技能，你可以轻松地修改代码，使生成的数据匹配其他形状，如正弦、余弦、U形等。
- en: Exercise 3.3
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3.3
- en: Modify the programs in the first project so that the generator generates data
    samples to form a sine shape between x = –5 and x = 5. When you plot the data
    samples, set the value of y between –1.2 and 1.2.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 修改第一个项目中的程序，使生成器生成在x = –5和x = 5之间形成正弦形状的数据样本。当你绘制数据样本时，将y的值设置为–1.2到1.2之间。
- en: 3.5 Generating numbers with patterns
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 使用模式生成数字
- en: In this second project, you’ll build and train GANs to generate a sequence of
    10 integers between 0 and 99, all of them multiples of 5\. The main steps involved
    are similar to those to generate an exponential growth curve, with the exception
    that the training set is not data points with two values (x, y). Instead, the
    training dataset is a sequence of integers that are all multiples of 5 between
    0 and 99.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第二个项目中，你将构建和训练GANs以生成0到99之间所有都是5的倍数的10个整数的序列。涉及的主要步骤与生成指数增长曲线的步骤类似，只是训练集不是具有两个值（x,
    y）的数据点。相反，训练数据集是0到99之间所有都是5的倍数的整数序列。
- en: 'In this section, you’ll first learn to convert the training data into a format
    that neural networks understand: one-hot variables. Further, you’ll convert one-hot
    variables back to an integer between 0 and 99, so it’s easy for human beings to
    understand. Hence you are essentially translating data between human-readable
    and model-ready formats. After that, you’ll create a discriminator and a generator
    and train the GANs. You’ll also use early stopping to determine when the training
    is finished. You then discard the discriminator and use the trained generator
    to create a sequence of integers with the pattern you want.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将首先学习将训练数据转换为神经网络可以理解的格式：独热变量。进一步，你将把独热变量转换回0到99之间的整数，这样人类就可以更容易理解。因此，你实际上是在将数据在人类可读格式和模型准备格式之间进行转换。之后，你将创建一个判别器和生成器，并训练GANs。你还将使用早期停止来确定何时结束训练。然后，你将丢弃判别器，并使用训练好的生成器创建一个具有你想要的模式的整数序列。
- en: 3.5.1 What are one-hot variables?
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 什么是独热变量？
- en: One-hot encoding is a technique used in ML and data preprocessing to represent
    categorical data as binary vectors. Categorical data consists of categories or
    labels, such as colors, types of animals, or cities, which are not inherently
    numeric. ML algorithms typically work with numerical data, so converting categorical
    data into a numerical format is necessary.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是机器学习和数据预处理中用于将分类数据表示为二进制向量的技术。分类数据由类别或标签组成，如颜色、动物类型或城市，它们本身不是数值。机器学习算法通常处理数值数据，因此将分类数据转换为数值格式是必要的。
- en: Imagine you are working with a categorical feature—for example, the color of
    a house that can take values “red,” “green,” and “blue.” With one-hot encoding,
    each category is represented as a binary vector. You’ll create three binary columns,
    one for each category. The color “red” is one-hot encoded as [1, 0, 0], “green”
    as [0, 1, 0], and “blue” as [0, 0, 1]. Doing so preserves the categorical information
    without introducing any ordinal relationship between the categories. Each category
    is treated as independent.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在处理一个分类特征——例如，房屋的颜色，它可以取“红色”、“绿色”和“蓝色”等值。使用one-hot编码，每个类别都表示为一个二进制向量。你将创建三个二进制列，每个类别一个。颜色“红色”被one-hot编码为[1,
    0, 0]，“绿色”为[0, 1, 0]，而“蓝色”为[0, 0, 1]。这样做可以保留分类信息，而不在类别之间引入任何顺序关系。每个类别都被视为独立的。
- en: 'Here we define a `onehot_encoder()` function to convert an integer to a one-hot
    variable:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个`onehot_encoder()`函数，用于将整数转换为one-hot变量：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The function takes two arguments: the first argument, `position`, is the index
    at which the value is turned on as 1, and the second argument, `depth`, is the
    length of the one-hot variable. For example, if we print out the value of `onehot_encoder(1,5)`,
    it will look like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受两个参数：第一个参数`position`是值被打开为1的索引，第二个参数`depth`是one-hot变量的长度。例如，如果我们打印出`onehot_encoder(1,5)`的值，它将看起来像这样：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The result is
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The result shows a five-value tensor with the second place (the index value
    of which is 1) turned on as 1 and the rest turned off as 0s.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示一个五值张量，第二个位置（其索引值为1）被打开为1，其余位置被关闭为0。
- en: 'Now that you understand how one-hot encoding works, you can convert any integer
    between 0 and 99 to a one-hot variable:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了one-hot编码的工作原理，你可以将0到99之间的任何整数转换为one-hot变量：
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s use the function to convert the number 75 to a 100-value tensor:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用该函数将数字75转换为100值张量：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE24]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The result is a 100-value tensor with the 76^(th) place (the index value of
    which is 75) turned on as 1 and all other positions turned off as 0s.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个100值张量，第76位（其索引值为75）被打开为1，其余位置被关闭为0。
- en: To function `int_to_onehot()` converts an integer into a one-hot variable. In
    a way, the function is translating human-readable language into model-ready language.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`int_to_onehot()`将整数转换为one-hot变量。从某种意义上说，该函数是将人类可读语言转换为模型准备语言。
- en: 'Next, we want to translate model-ready language back to human-readable language.
    Suppose we have a one-hot variable: How can we convert it into an integer that
    humans understand? The following function `onehot_to_int()` accomplishes that
    goal:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望将模型准备语言翻译回人类可读语言。假设我们有一个one-hot变量：我们如何将其转换为人类可理解的整数？以下函数`onehot_to_int()`实现了这一目标：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The function `onehot_to_int()` takes the argument `onehot` and converts it into
    an integer based on which position has the highest value.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`onehot_to_int()`接受`onehot`参数，并根据哪个位置具有最高值将其转换为整数。
- en: 'Let’s test the function to see what happens if we use the tensor `onehot75`
    we just created as the input:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下函数，看看如果我们使用刚刚创建的`onehot75`张量作为输入会发生什么：
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output is
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The result shows that the function converts the one-hot variable to an integer
    75, which is the right answer. So we know that the functions are defined properly.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，该函数将one-hot变量转换为整数75，这是正确答案。因此，我们知道这些函数已正确定义。
- en: Next, we’ll build and train GANs to generate multiples of 5.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建和训练GAN以生成5的倍数。
- en: 3.5.2 GANs to generate numbers with patterns
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用GAN生成具有模式的数字
- en: Our goal is to build and train a model so that the generator can generate a
    sequence of 10 integers, all multiples of 5\. We first prepare the training data
    and then convert them to model-ready numbers in batches. Finally, we use the trained
    generator to generate the patterns we want.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建和训练一个模型，以便生成器可以生成一个由10个整数组成的序列，它们都是5的倍数。我们首先准备训练数据，然后将它们分批转换为模型准备好的数字。最后，我们使用训练好的生成器生成我们想要的模式。
- en: For simplicity, we’ll generate a sequence of 10 integers between 0 and 99\.
    We’ll then convert the sequence into 10 model-ready numbers.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将生成0到99之间的10个整数序列。然后，我们将该序列转换为10个模型准备好的数字。
- en: 'The following function generates a sequence of 10 integers, all multiples of
    5:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数生成一个由10个整数组成的序列，它们都是5的倍数：
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We first use the `randint()` method in PyTorch to generate 10 numbers between
    0 and 19\. We then multiply them by 5 and convert them to PyTorch tensors. This
    creates 10 integers that are all multiples of 5.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用PyTorch中的`randint()`方法生成0到19之间的10个数字。然后我们将它们乘以5并转换为PyTorch张量。这创建了10个都是5的倍数的整数。
- en: 'Let’s try to generate a sequence of training data:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试生成一个训练数据序列：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The values in the preceding output are all multiples of 5.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出中的所有值都是5的倍数。
- en: 'Next, we convert each number to a one-hot variable so that we can feed them
    to the neural network later:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将每个数字转换为独热变量，以便我们可以在以后将其输入到神经网络中：
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ① Creates a sequence of 10 numbers, all multiples of 5
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个由10个数字组成的序列，这些数字都是5的倍数
- en: ② Converts each integer to a 100-value one-hot variable
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将每个整数转换为100值的独热变量
- en: The preceding function `gen_batch()` creates a batch of data so that we can
    feed them to the neural network for training purposes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数`gen_batch()`创建了一组数据，以便我们可以将其输入到神经网络进行训练。
- en: 'We also define a function `data_to_num()` to convert one-hot variables to a
    sequence of integers so that humans can understand the output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数`data_to_num()`，用于将独热变量转换为一系列整数，以便人类可以理解输出：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Converts vectors to integers based on the largest values in a 100-value vector
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ① 根据100值向量中的最大值将向量转换为整数
- en: ② Applies the function on an example
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在一个示例上应用该函数
- en: 'The `dim=-1` argument in the `torch.argmax()` function means we are trying
    to find the position (i.e., index) of the largest value in the last dimension:
    that is, among the 100-value one-hot vector, which position has the highest value.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.argmax()`函数中的`dim=-1`参数意味着我们试图找到最后一个维度（即索引）中最大值的位（即位置）：即在100值的独热向量中，哪个位置具有最高值。'
- en: 'Next, we’ll create two neural networks: one for the discriminator D and one
    for the generator G. We’ll build GANs to generate the desired pattern of numbers.
    Similar to what we did earlier in this chapter, we create a discriminator network,
    which is a binary classifier that distinguishes fake samples from real samples.
    We also create a generator network to generate a sequence of 10 numbers. Here
    is the discriminator neural network:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建两个神经网络：一个用于判别器D，另一个用于生成器G。我们将构建GAN来生成所需的数字模式。类似于本章前面所做的那样，我们创建了一个判别器网络，它是一个二元分类器，用于区分假样本和真实样本。我们还创建了一个生成器网络来生成一系列10个数字。以下是判别器神经网络：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Since we’ll convert integers into 100-value one-hot variables, we use 100 as
    the input size in the first `Linear` layer in the model. The last `Linear` layer
    has just one output feature in it, and we use the sigmoid activation function
    to squeeze the output to the range [0, 1] so it can be interpreted as the probability,
    p, that the sample is real. With the complementary probability 1 – p, the sample
    is fake.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将整数转换为100值的独热变量，我们在模型的第一层`Linear`中使用100作为输入大小。最后一层`Linear`只有一个输出特征，我们使用sigmoid激活函数将输出挤压到[0,
    1]的范围内，以便它可以被解释为样本是真实的概率，p。使用互补概率1 – p，样本是假的。
- en: The generator’s job is to create a sequence of numbers so that they can pass
    as real in front of the discriminator D. That is, G is trying to create a sequence
    of numbers to maximize the probability that D thinks that the numbers are from
    the training dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的任务是创建一个数字序列，以便它们可以通过判别器D。也就是说，G试图创建一个数字序列，以最大化D认为这些数字来自训练数据集的概率。
- en: 'We create the following neural network to represent the generator G:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了以下神经网络来表示生成器G：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We’ll feed random noise vectors from a 100-dimensional latent space to the generator.
    The generator then creates a tensor of 100 values based on the input. Note here
    that we use the `ReLU` activation function at the last layer so that the output
    is nonnegative. Since we are trying to generate 100 values of 0 or 1, nonnegative
    values are appropriate here.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向生成器输入来自100维潜在空间的随机噪声向量。生成器随后根据输入创建一个包含100个值的张量。注意，在这里我们使用`ReLU`激活函数在最后一层，以确保输出为非负值。由于我们试图生成100个0或1的值，非负值在这里是合适的。
- en: 'As in the first project, we use the Adam optimizer for both the discriminator
    and the generator, with a learning rate of 0.0005:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如同第一个项目一样，我们使用Adam优化器对判别器和生成器进行优化，学习率为0.0005：
- en: '[PRE35]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we have the training data and two networks, we’ll train the model.
    After that, we’ll discard the discriminator and use the generator to generate
    a sequence of 10 integers.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练数据和两个网络，我们将训练模型。之后，我们将丢弃判别器并使用生成器生成一个10个整数的序列。
- en: 3.5.3 Training the GANs to generate numbers with patterns
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 训练GAN生成具有模式的数字
- en: The training process for this project is very similar to that in our first project
    in which you generated an exponential growth shape.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的训练过程与我们第一个项目中生成指数增长形状的项目非常相似。
- en: 'We define a function `train_D_G()`, which is a combination of the three functions
    `train_D_on_real()`, `train_D_on_fake()`, and `train_G()` that we have defined
    for the first project. The function `train_D_G()` is in the Jupyter Notebook for
    this chapter in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).
    Take a look at the function `train_D_G()` so you can see what minor changes we
    have made compared to the three functions we defined for the first project.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个函数`train_D_G()`，它是我们为第一个项目定义的三个函数`train_D_on_real()`、`train_D_on_fake()`和`train_G()`的组合。函数`train_D_G()`在本书GitHub仓库的该章节Jupyter
    Notebook中：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。查看函数`train_D_G()`，你可以看到我们与第一个项目定义的三个函数相比做了哪些细微的修改。
- en: We use the same early stopping class that we defined for the first project so
    we know when to stop training. However, we have modified the `patience` argument
    to 800 when we instantiate the class, as shown in the following listing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与第一个项目定义相同的早期停止类，这样我们就可以知道何时停止训练。然而，当我们实例化类时，我们将`patience`参数修改为800，如以下列表所示。
- en: Listing 3.11 Training GANs to generate multiples of 5
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.11 训练GAN生成5的倍数
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Creates an instance of the early stopping class
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建早期停止类的实例
- en: ② Defines a distance() function to calculate the loss in the generated numbers
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义一个distance()函数来计算生成数字中的损失
- en: ③ Trains the GANs for one epoch
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 训练GAN进行一个epoch的训练
- en: ④ Prints out the generated sequence of integers after every 50 epochs
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 每过50个epoch打印出生成的整数序列
- en: 'We have also defined a `distance()` function to measure the difference between
    the training set and the generated data samples: it calculates the MSE of the
    remainder of each generated number when divided by 5\. The measure is 0 when all
    generated numbers are multiples of 5\.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个`distance()`函数来衡量训练集和生成数据样本之间的差异：它计算每个生成的数字除以5的余数的均方误差。当所有生成的数字都是5的倍数时，该度量值为0。
- en: 'If you run the preceding code cell, you’ll see the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码单元，你会看到以下输出：
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In each iteration, we generate a batch of 10 numbers. We first train the discriminator
    D using real samples. After that, the generator creates a batch of fake samples,
    and we use them to train the discriminator D again. Finally, we let the generator
    create a batch of fake samples again, but we use them to train the generator G
    instead. We stop training if the generator network stops improving after 800 epochs
    since the last time the minimum loss was achieved. After every 50 epochs, we print
    out the sequence of 10 numbers created by the generator so you can tell if they
    are indeed all multiples of 5.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们生成一批10个数字。我们首先使用真实样本训练判别器D。然后，生成器创建一批假样本，我们再次使用它们来训练判别器D。最后，我们让生成器再次创建一批假样本，但这次我们使用它们来训练生成器G。如果生成器网络在达到最小损失后的800个epoch内停止改进，我们将停止训练。每过50个epoch，我们打印出由生成器创建的10个数字序列，这样你就可以判断它们是否确实是5的倍数。
- en: 'The output during the training process is as shown previously. In the first
    few hundred epochs, the generator still produces numbers that are not multiples
    of 5\. But after 900 epochs, all the numbers generated are multiples of 5\. The
    training process takes just a minute or so with GPU training. It takes less than
    10 minutes if you use CPU training. Alternatively, you can download the trained
    model from the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的输出如图所示。在前几百个epoch中，生成器仍然产生不是5的倍数的数字。但在900个epoch之后，所有生成的数字都是5的倍数。使用GPU训练时，训练过程只需一分钟左右。如果你使用CPU训练，则不到10分钟。或者，你可以从书籍的GitHub仓库下载训练好的模型：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: 3.5.4 Saving and using the trained model
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 保存和使用训练好的模型
- en: 'We’ll discard the discriminator and save the trained generator in the local
    folder:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将丢弃判别器并将训练好的生成器保存在本地文件夹中：
- en: '[PRE38]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We have now saved the generator to the local folder. To use the generator,
    we simply load up the model and use it to generate a sequence of integers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将生成器保存到了本地文件夹。要使用生成器，我们只需加载模型并使用它来生成一系列整数：
- en: '[PRE39]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① Loads the saved generator
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载保存的生成器
- en: ② Obtains random noise vectors
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取随机噪声向量
- en: ③ Feeds the random noise vectors to the trained model to generate a sequence
    of integers
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将随机噪声向量输入到训练好的模型中以生成一系列整数
- en: 'The output is as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The generated numbers are all multiples of 5.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数字都是 5 的倍数。
- en: You can easily change the code to generate other patterns, such as odd numbers,
    even numbers, multiples of 3, and so on.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地更改代码以生成其他模式，例如奇数、偶数、3 的倍数等等。
- en: Exercise 3.4
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.4
- en: Modify the programs in the second project so that the generator generates a
    sequence of ten integers that are all multiples of 3.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 修改第二个项目的程序，使生成器生成一个由所有都是 3 的倍数的十个整数组成的序列。
- en: Now that you know how GANs work, you’ll be able to extend the idea behind GANs
    to other formats in later chapters, including high-resolution images and realistic-sounding
    music.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 GANs 的工作原理，你将在后面的章节中能够将 GANs 的理念扩展到其他格式，包括高分辨率图像和听起来逼真的音乐。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'GANs consist of two networks: a discriminator to distinguish fake samples from
    real samples and a generator to create samples that are indistinguishable from
    those in the training set.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 由两个网络组成：一个用于区分伪造样本和真实样本的判别器，以及一个用于创建与训练集中样本不可区分的样本的生成器。
- en: The steps involved in GANs are preparing training data, creating a discriminator
    and a generator, training the model and deciding when to stop training, and finally,
    discarding the discriminator and using the trained generator to create new samples.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 的步骤包括准备训练数据、创建判别器和生成器、训练模型并决定何时停止训练，最后，丢弃判别器并使用训练好的生成器来创建新的样本。
- en: The content generated by GANs depends on the training data. When the training
    dataset contains data pairs (x, y) that form an exponential growth curve, the
    generated samples are also data pairs that mimic such a shape. When the training
    dataset has sequences of numbers that are all multiples of 5, the generated samples
    are also sequences of numbers, with multiples of 5 in them.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 生成的内容取决于训练数据。当训练数据集包含形成指数增长曲线的数据对 (x, y) 时，生成的样本也是模仿这种形状的数据对。当训练数据集包含所有都是
    5 的倍数的数字序列时，生成的样本也是包含 5 的倍数的数字序列。
- en: GANs are versatile and capable of generating many different formats of content.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 是多才多艺的，能够生成许多不同格式的内 容。
- en: '* * *'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-001-backlink))  Goodfellow et al, 2014, “Generative Adversarial
    Nets.” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-001-backlink))  Goodfellow 等人，2014年，“Generative Adversarial
    Nets.” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
- en: '^([2](#footnote-000-backlink))  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov, 2014, “Dropout: A Simple Way to Prevent
    Neural Networks from Overfitting.” *Journal of Machine Learning Research* 15 (56):
    1929−1958.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](#footnote-000-backlink))  尼提什·斯里瓦斯塔瓦（Nitish Srivastava）、杰弗里·辛顿（Geoffrey
    Hinton）、亚历克斯·克里泽夫斯基（Alex Krizhevsky）、伊利亚·苏茨克维（Ilya Sutskever）和鲁斯兰·萨拉胡丁诺夫（Ruslan
    Salakhutdinov），2014年，“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”
    *《机器学习研究杂志》* 15 (56): 1929−1958.'
