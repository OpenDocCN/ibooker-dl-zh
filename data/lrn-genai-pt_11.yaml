- en: 9 A line-by-line implementation of attention and Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 注意力和Transformer的逐行实现
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: The architecture and functionalities of encoders and decoders in Transformers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer中编码器和解码器的架构和功能
- en: How the attention mechanism uses query, key, and value to assign weights to
    elements in a sequence
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制如何使用查询、键和值来为序列中的元素分配权重
- en: Different types of Transformers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的Transformer
- en: Building a Transformer from scratch for language translation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建用于语言翻译的Transformer
- en: Transformers are advanced deep learning models that excel in handling sequence-to-sequence
    prediction challenges, outperforming older models like recurrent neural networks
    (RNNs) and convolutional neural networks (CNNs). Their strength lies in effectively
    understanding the relationships between elements in input and output sequences
    over long distances, such as two words far apart in the text. Unlike RNNs, Transformers
    are capable of parallel training, significantly cutting down training times and
    enabling the handling of vast datasets. This transformative architecture has been
    pivotal in the development of large language models (LLMs) like ChatGPT, BERT,
    and T5, marking a significant milestone in AI progress.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是先进的深度学习模型，在处理序列到序列预测挑战方面表现出色，优于旧模型如循环神经网络（RNNs）和卷积神经网络（CNNs）。它们的优势在于有效地理解输入和输出序列中元素之间的关系，尤其是在文本中相隔较远的两个单词。与RNNs不同，Transformers能够进行并行训练，显著缩短训练时间，并能够处理大量数据集。这种变革性的架构在大型语言模型（LLMs）如ChatGPT、BERT和T5的开发中发挥了关键作用，标志着人工智能进步的重要里程碑。
- en: Prior to the introduction of Transformers in the groundbreaking 2017 paper “Attention
    Is All You Need” by a group of Google researchers,^([1](#footnote-001)) natural
    language processing (NLP) and similar tasks primarily relied on RNNs, including
    long short-term memory (LSTM) models. RNNs, however, process information sequentially,
    limiting their speed due to the inability to train in parallel and struggling
    with maintaining information about earlier parts of a sequence, thus failing to
    capture long-term dependencies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年一篇开创性的论文“Attention Is All You Need”中，由一组谷歌研究人员提出之前（^([1](#footnote-001)))，自然语言处理（NLP）和类似任务主要依赖于循环神经网络（RNNs），包括长短期记忆（LSTM）模型。然而，RNNs按顺序处理信息，由于无法并行训练以及难以保持序列早期部分的信息，因此限制了它们的速度，并且无法捕捉长期依赖关系。
- en: The revolutionary aspect of the Transformer architecture is its attention mechanism.
    This mechanism assesses the relationship between words in a sequence by assigning
    weights, determining the degree of relatedness in meaning among words based on
    the training data. This enables models like ChatGPT to comprehend relationships
    between words, thus understanding human language more effectively. The nonsequential
    processing of inputs allows for parallel training, reducing training time and
    facilitating the use of large datasets, thereby powering the rise of knowledgeable
    LLMs and the current surge in AI advancements.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的革命性方面是其注意力机制。该机制通过分配权重来评估序列中单词之间的关系，根据训练数据确定单词之间在意义上的相关程度。这使得模型如ChatGPT能够理解单词之间的关系，从而更有效地理解人类语言。输入的非顺序处理允许并行训练，减少训练时间，并促进大量数据集的使用，从而推动了知识型LLMs的兴起和当前人工智能进步的激增。
- en: In this chapter, we will implement, line by line, the creation of a Transformer
    from the ground up, based on the paper “Attention Is All You Need.” The Transformer,
    once trained, can handle translations between any two languages (such as German
    to English or English to Chinese). In the next chapter, we’ll focus on training
    the Transformer developed here to perform English to French translations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐行实现从零开始创建Transformer的过程，基于论文“Attention Is All You Need.”。一旦训练完成，Transformer可以处理任何两种语言之间的翻译（例如德语到英语或英语到中文）。在下一章中，我们将专注于训练本章开发的Transformer以执行英语到法语翻译。
- en: To build the Transformer from scratch, we’ll explore the inner workings of the
    self-attention mechanism, including the roles of query, key, and value vectors,
    and the computation of scaled dot product attention (SDPA). We’ll construct an
    encoder layer by integrating layer normalization and residual connection into
    a multihead attention layer and combining it with a feed-forward layer. We’ll
    then stack six of these encoder layers to form the encoder. Similarly, we’ll develop
    a decoder in the Transformer that is capable of generating translation one token
    at a time, based on previous tokens in the translation and the encoder’s output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要从头开始构建Transformer，我们将探索自注意力机制的内部工作原理，包括查询、键和值向量的作用，以及缩放点积注意力（SDPA）的计算。我们将通过将层归一化和残差连接集成到多头注意力层中，并将其与前馈层结合来构建编码器层。然后，我们将堆叠六个这样的编码器层来形成编码器。同样，我们将在Transformer中开发一个解码器，它能够一次生成一个翻译标记，基于之前的翻译标记和编码器的输出。
- en: This groundwork will equip you to train the Transformer for translations between
    any two languages. In the next chapter, you’ll learn to train the Transformer
    using a dataset containing more than 47,000 English-to-French translations. You’ll
    witness the trained model translating common English phrases to French with an
    accuracy comparable to using Google Translate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基础将使你能够训练Transformer进行任何两种语言之间的翻译。在下一章中，你将学习如何使用包含超过47,000个英法翻译数据集来训练Transformer。你将见证训练好的模型将常见的英语短语翻译成法语，其准确度与使用谷歌翻译相当。
- en: 9.1 Introduction to attention and Transformer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 注意力和Transformer简介
- en: To grasp the concept of Transformers in machine learning, it’s essential to
    first understand the attention mechanism. This mechanism allows Transformers to
    recognize long-range dependencies between sequence elements, a feature that sets
    them apart from earlier sequence prediction models like RNNs. With this mechanism,
    Transformers can simultaneously focus on every element in a sequence, comprehending
    the context of each word.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要掌握机器学习中Transformer的概念，首先理解注意力机制是至关重要的。这种机制使Transformer能够识别序列元素之间的长距离依赖关系，这是它们与早期的序列预测模型（如RNNs）的区别之一。有了这个机制，Transformer可以同时关注序列中的每个元素，理解每个单词的上下文。
- en: Consider the word “bank” to illustrate how the attention mechanism interprets
    words based on context. In the sentence “I went fishing by the river yesterday,
    remaining near the bank the whole afternoon,” the word ”bank“ is linked to “fishing”
    because it refers to the area beside a river. Here, a Transformer understands
    “bank” as part of the river’s terrain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以“bank”这个词为例，说明注意力机制如何根据上下文来解释单词。在句子“我昨天在河边钓鱼，整个下午都待在河岸附近”中，“bank”与“fishing”相关联，因为它指的是河流旁边的区域。在这里，Transformer将“bank”理解为河流地形的一部分。
- en: By contrast, in “Kate went to the bank after work yesterday and deposited a
    check there,” “bank” is connected to “check,” leading the Transformer to identify
    “bank” as a financial institution. This example showcases how Transformers discern
    word meanings based on their surrounding context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在“Kate昨天下班后去了银行，并在那里存了一张支票”这句话中，“bank”与“check”相关联，导致Transformer将“bank”识别为金融机构。这个例子展示了Transformer如何根据其周围的上下文来辨别单词的含义。
- en: In this section, you’ll dive deeper into the attention mechanism, exploring
    how it works. This process is crucial for determining the importance, or weights,
    of various words within a sentence. After that, we’ll examine the structure of
    different Transformer models, including one that can translate between any two
    languages.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将更深入地了解注意力机制，探索它是如何工作的。这个过程对于确定句子中各个单词的重要性或权重至关重要。之后，我们将检查不同Transformer模型的结构，包括一种能够翻译任何两种语言之间的模型。
- en: 9.1.1 The attention mechanism
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 注意力机制
- en: The attention mechanism is a method used to determine the interconnections between
    elements in a sequence. It calculates scores to indicate how one element relates
    to others in the sequence, with higher scores denoting a stronger relationship.
    In NLP, this mechanism is instrumental in linking words within a sentence meaningfully.
    This chapter will guide you through implementing the attention mechanism for language
    translation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是一种用于确定序列中元素之间相互连接的方法。它计算分数来指示一个元素与序列中其他元素的关系，分数越高表示关系越强。在NLP中，这种机制对于在句子中有意义地连接单词至关重要。本章将指导你实现用于语言翻译的注意力机制。
- en: We’ll construct a Transformer composed of an encoder and a decoder for that
    purpose. We’ll then train the Transformer to translate English to French in the
    next chapter. The encoder transforms an English sentence, such as “How are you?”,
    into vector representations that capture its meaning. The decoder then uses these
    vector representations to generate the French translation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们将构建一个由编码器和解码器组成的Transformer。然后，在下一章中，我们将训练这个Transformer将英语翻译成法语。编码器将一个英语句子，例如“你好吗？”，转换成捕捉其意义的向量表示。然后解码器使用这些向量表示来生成法语翻译。
- en: To transform the phrase “How are you?” into vector representations, the model
    first breaks it down into tokens `[how, are, you, ?]`, a process similar to what
    you have done in chapter 8\. These tokens are each represented by a 256-dimensional
    vector known as word embeddings, which capture the meaning of each token. The
    encoder also employs positional encoding, a method to determine the positions
    of tokens in the sequence. This positional encoding is added to the word embeddings
    to create input embeddings, which are then used to calculate self-attention. The
    input embedding for “How are you?” forms a tensor with dimensions (4, 256), where
    4 represents the number of tokens and 256 is the dimensionality of each embedding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将短语“你好吗？”转换成向量表示，模型首先将其分解成标记`[how, are, you, ?]`，这个过程与你在第8章中做过的类似。这些标记每个都由一个256维的向量表示，称为词嵌入，它捕捉每个标记的意义。编码器还采用了位置编码，这是一种确定标记在序列中位置的方法。这种位置编码被添加到词嵌入中，以创建输入嵌入，然后用于计算自注意力。短语“你好吗？”的输入嵌入形成一个维度为(4,
    256)的张量，其中4代表标记的数量，256是每个嵌入的维度性。
- en: While there are different ways to calculate attention, we’ll use the most common
    method, SDPA. This mechanism is also called self-attention because the algorithm
    calculates how a word attends to all words in the sequence, including the word
    itself. Figure 9.1 provides a diagram of how to calculate SDPA.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算注意力的方法有很多种，但我们将使用最常见的方法，即SDPA。这种机制也被称为自注意力，因为算法计算一个词如何关注序列中的所有词，包括它自己。图9.1展示了如何计算SDPA的示意图。
- en: '![](../../OEBPS/Images/CH09_F01_Liu.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F01_Liu.png)'
- en: Figure 9.1 A diagram of the self-attention mechanism. To calculate attention,
    the input embedding X is first passed through three neural layers with weights,
    W^Q, W^K, and W^V, respectively. The outputs are query Q, key K, and value V.
    The scaled attention score is the product of Q and K divided by the square root
    of the dimension of K, d[k]. We apply the softmax function on the scaled attention
    score to obtain the attention weight. The attention is the product of the attention
    weight and value V.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 自注意力机制的示意图。为了计算注意力，首先将输入嵌入X通过三个带有权重W^Q、W^K和W^V的神经网络层。输出是查询Q、键K和值V。缩放后的注意力分数是Q和K的乘积除以K的维度的平方根，d[k]。我们对缩放后的注意力分数应用softmax函数以获得注意力权重。注意力是注意力权重和值V的乘积。
- en: The utilization of query, key, and value in calculating attention is inspired
    by retrieval systems. Consider visiting a public library to find a book. If you
    search for “machine learning in finance” in the library’s search engine, this
    phrase becomes your query. The book titles and descriptions in the library serve
    as the keys. Based on the similarity between your query and these keys, the library’s
    retrieval system suggests a list of books (values). Books containing “machine
    learning,” “finance,” or both in their titles or descriptions are likely to rank
    higher. In contrast, books unrelated to these terms will have a lower matching
    score and thus are less likely to be recommended.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算注意力时使用查询、键和值的方法受到了检索系统的启发。考虑访问一个公共图书馆来寻找一本书。如果你在图书馆的搜索引擎中搜索“金融中的机器学习”，这个短语就变成了你的查询。图书馆中的书籍标题和描述作为键。根据你的查询与这些键之间的相似性，图书馆的检索系统会建议一系列书籍（值）。标题或描述中包含“机器学习”、“金融”或两者之一的书籍可能会排名更高。相比之下，与这些术语无关的书籍将具有较低的匹配分数，因此不太可能被推荐。
- en: To calculate SDPA, the input embedding X is processed through three distinct
    neural network layers. The corresponding weights for these layers are W^Q, W^K,
    and W^V; each has a dimension of 256 × 256. These weights are learned from data
    during the training phase. Thus, we can calculate query Q, key K, and value V
    as Q = X * W^Q, K = X * Q^K, and V = X * W^V. The dimensions of Q, K, and V match
    those of the input embedding X, which are 4 × 256.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算SDPA，输入嵌入X通过三个不同的神经网络层进行处理。这些层的相应权重是W^Q、W^K和W^V；每个的维度为256 × 256。这些权重在训练阶段从数据中学习。因此，我们可以计算查询Q、键K和值V，Q
    = X * W^Q，K = X * Q^K，V = X * W^V。Q、K和V的维度与输入嵌入X的维度相匹配，即4 × 256。
- en: 'Similar to the retrieval system example we mentioned earlier, in the attention
    mechanism, we assess the similarities between the query and key vectors using
    the SDPA approach. SDPA involves calculating the dot product of the query (Q)
    and key (K) vectors. A high dot product indicates a strong similarity between
    the two vectors and vice versa. For instance, in the sentence “How are you?”,
    the scaled attention score is computed as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前提到的检索系统示例类似，在注意力机制中，我们使用SDPA方法来评估查询向量和键向量之间的相似性。SDPA包括计算查询（Q）和键（K）向量的点积。高点积表示两个向量之间有很强的相似性，反之亦然。例如，在句子“你怎么样？”中，缩放后的注意力分数计算如下：
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH09_F01_Liu_EQ01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F01_Liu_EQ01.png)'
- en: '| (9.1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (9.1) |'
- en: where d[k] represents the dimension of the key vector K, which in our case is
    256\. We scale the dot product of Q and K by the square root of d[k] to stabilize
    training. This scaling is done to prevent the dot product from growing too large
    in magnitude. The dot product between the query and key vectors can become very
    large when the dimension of these vectors (i.e., the depth of the embedding) is
    high. This is because each element of the query vector is multiplied by each element
    of the key vector, and these products are then summed up.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中d[k]表示键向量K的维度，在我们的情况下是256。我们将Q和K的点积乘以d[k]的平方根以稳定训练。这种缩放是为了防止点积的幅度过大。当这些向量的维度（即嵌入的深度）很高时，查询和键向量之间的点积可以变得非常大。这是因为查询向量的每个元素都与键向量的每个元素相乘，然后这些乘积相加。
- en: The next step is to apply the softmax function to these attention scores, converting
    them into attention weights. This ensures that the total attention a word gives
    to all words in the sentence sums to 100%.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将softmax函数应用于这些注意力分数，将它们转换为注意力权重。这确保了单词对句子中所有单词的总注意力加起来为100%。
- en: '![](../../OEBPS/Images/CH09_F02_Liu.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02_Liu.png)'
- en: Figure 9.2 Steps to calculate attention weights. The input embedding is passed
    through two neural networks to obtain query Q and key K. The scaled attention
    scores are calculated as the dot product of Q and K divided by the square root
    of the dimension of K. Finally, we apply the softmax function on the scaled attention
    scores to obtain attention weights, which demonstrate how each element is related
    to all other elements in the sequence.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2计算注意力权重的步骤。输入嵌入通过两个神经网络传递以获得查询Q和键K。缩放后的注意力分数是Q和K的点积除以K的维度平方根。最后，我们对缩放后的注意力分数应用softmax函数以获得注意力权重，这些权重展示了序列中每个元素与其他所有元素的关系。
- en: Figure 9.2 shows how this is done. For the sentence “How are you?”, the attention
    weights form a 4 × 4 matrix, which shows how each token in `["How", "are," "you,"
    "?"]` is related to all other tokens (including itself). The numbers in figure
    9.2 are made-up numbers to illustrate the point. For example, the first row in
    the attention weights shows that the token `"How"` gives 10% of its attention
    to itself and 40%, 40%, and 10% to the other three tokens, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2展示了这一过程。对于句子“你怎么样？”，注意力权重形成一个4 × 4矩阵，显示了`["How", "are," "you," "?"]`中的每个标记如何与其他所有标记（包括自身）相关。图9.2中的数字是为了说明这一点而编造的。例如，注意力权重的第一行显示标记`"How"`将其10%的注意力分配给自己，将40%、40%和10%分别分配给其他三个标记。
- en: 'The final attention is then calculated as the dot product of these attention
    weights and the value vector V (also illustrated in figure 9.3):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的注意力是这些注意力权重与值向量V（如图9.3所示）的点积：
- en: '|'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH09_F02_Liu_EQ02.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02_Liu_EQ02.png)'
- en: '| (9.2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| (9.2) |'
- en: '![](../../OEBPS/Images/CH09_F03_Liu.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F03_Liu.png)'
- en: Figure 9.3 Use attention weights and the value vector to calculate the attention
    vector. The input embedding is passed through a neural network to obtain value
    V. The final attention is the dot product of the attention weights that we calculated
    earlier and the value vector V.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 使用注意力权重和价值向量计算注意力向量。输入嵌入通过神经网络传递以获得价值V。最终的注意力是我们之前计算的注意力权重与价值向量V的点积。
- en: This output also maintains a dimension of 4 × 256, consistent with our input
    dimensions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出也保持了一个4 × 256的维度，与我们的输入维度一致。
- en: To summarize, the process begins with the input embedding X of the sentence
    “How are you?”, which has a dimension of 4 × 256. This embedding captures the
    meanings of the four individual tokens but lacks contextualized understanding.
    The attention mechanism ends with the output `attention(Q,K,V)`, which maintains
    the same dimension of 4 × 256. This output can be viewed as a contextually enriched
    combination of the original four tokens. The weighting of the original tokens
    varies based on the contextual relevance of each token, granting more significance
    to words that are more important within the sentence’s context. Through this procedure,
    the attention mechanism transforms vectors representing isolated tokens into vectors
    imbued with contextualized meanings, thereby extracting a richer, more nuanced
    understanding from the sentence.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这个过程从句子“你好吗？”的输入嵌入X开始，其维度为4 × 256。这个嵌入捕捉了四个单独标记的含义，但缺乏上下文理解。注意力机制以输出`attention(Q,K,V)`结束，其维度保持为4
    × 256。这个输出可以看作是原始四个标记的上下文丰富组合。原始标记的权重根据每个标记的上下文相关性而变化，赋予句子上下文中更重要的单词更多的意义。通过这一过程，注意力机制将代表孤立标记的向量转化为充满上下文意义的向量，从而从句子中提取更丰富、更细腻的理解。
- en: Further, instead of using one set of query, key, and value vectors, Transformer
    models use a concept called multihead attention. For example, the 256-dimensional
    query, key, and value vectors can be split into say, 8, heads, and each head has
    a set of query, key, and value vectors with dimensions of 32 (because 256/8 =
    32). Each head pays attention to different parts or aspects of the input, enabling
    the model to capture a broader range of information and form a more detailed and
    contextual understanding of the input data. Multihead attention is especially
    useful when a word has multiple meanings in a sentence, such as in a pun. Let’s
    continue the “bank” example we mentioned earlier. Consider the pun joke, “Why
    is the river so rich? Because it has two banks.” In the project of translating
    English to French in the next chapter, you’ll implement first-hand splitting Q,
    K, and V into multiple heads to calculate attention in each head before concatenating
    them back into one single attention vector.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Transformer模型不是使用一套查询、键和值向量，而是使用一个称为多头注意力的概念。例如，256维度的查询、键和值向量可以被分成，比如说，8个头，每个头有一组查询、键和值向量，其维度为32（因为256/8
    = 32）。每个头关注输入的不同部分或方面，使模型能够捕捉更广泛的信息，并形成对输入数据的更详细和上下文化的理解。多头注意力在句子中一个单词有多个意义时特别有用，比如在双关语中。让我们继续我们之前提到的“银行”例子。考虑这个双关语笑话，“为什么河流这么富饶？因为它有两个河岸。”在下一章将英语翻译成法语的项目中，你将亲自动手将Q、K和V分割成多个头，在每个头中计算注意力，然后再将它们连接成一个单一的注意力向量。
- en: 9.1.2 The Transformer architecture
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 Transformer架构
- en: The concept of the attention mechanism was introduced by Bahdanau, Cho, and
    Bengio in 2014.^([2](#footnote-000)) It became widely used after the groundbreaking
    paper “Attention Is All You Need,” which focused on creating a model for machine
    language translation. The architecture of this model, known as the Transformer,
    is depicted in figure 9.4\. It features an encoder-decoder structure that relies
    heavily on the attention mechanism. In this chapter, you’ll build this model from
    scratch, coding it line by line, intending to train it for translation between
    any two languages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的这一概念是由巴哈纳乌、乔和本吉奥在2014年提出的.^([2](#footnote-000)) 在开创性的论文“注意力即一切”发表后，它得到了广泛的应用，该论文专注于为机器语言翻译创建一个模型。这个模型的结构，被称为Transformer，在图9.4中展示。它具有一个编码器-解码器结构，该结构高度依赖于注意力机制。在本章中，你将从头开始构建这个模型，逐行编码，目的是训练它进行任何两种语言之间的翻译。
- en: '![](../../OEBPS/Images/CH09_F04_Liu.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F04_Liu.png)'
- en: Figure 9.4 The Transformer architecture. The encoder in the Transformer (left
    side of the diagram), which consists of N identical encoder layers, learns the
    meaning of the input sequence and converts it into vectors that represent its
    meaning. It then passes these vectors to the decoder (right side of the diagram),
    which consists of N identical decoder layers. The decoder constructs the output
    (e.g., the French translation of an English phrase) by predicting one token at
    a time, based on previous tokens in the sequence and vector representations from
    the encoder. The generator on the top right is the head attached to the output
    from the decoder so that the output is the probability distribution over all tokens
    in the target language (e.g., the French vocabulary).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 Transformer架构。Transformer中的编码器（图左侧），由N个相同的编码器层组成，学习输入序列的意义并将其转换为表示其意义的向量。然后，它将这些向量传递给解码器（图右侧），解码器由N个相同的解码器层组成。解码器通过预测序列中的每个标记，并根据序列中的先前标记和解码器从编码器获得的向量表示来构建输出（例如，英语短语的法语翻译）。右上角的生成器是连接到解码器输出的头部，以便输出是目标语言中所有标记的概率分布（例如，法语词汇）。
- en: Let’s use English-to-French translation as our example. The Transformer’s encoder
    transforms an English sentence like “I don’t speak French” into vector representations
    that store its meaning. The Transformer’s decoder then processes them to produce
    the French translation “Je ne parle pas français.” The encoder’s role is to capture
    the essence of the original English sentence. For instance, if the encoder is
    effective, it should translate both “I don’t speak French” and “I do not speak
    French” into similar vector representations. Consequently, the decoder will interpret
    these vectors and generate similar translations. Interestingly, when using ChatGPT,
    these two English phrases indeed result in the same French translation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以英语到法语的翻译为例。Transformer的编码器将英语句子“我不说法语”转换成存储其意义的向量表示。然后，Transformer的解码器处理这些表示以生成法语翻译“Je
    ne parle pas français”。编码器的角色是捕捉原始英语句子的本质。例如，如果编码器有效，它应该将“我不说法语”和“我并不说法语”翻译成相似的向量表示。因此，解码器将解释这些向量并生成相似的翻译。有趣的是，当使用ChatGPT时，这两个英语短语确实产生了相同的法语翻译。
- en: 'The encoder in the Transformer approaches the task by first tokenizing both
    the English and French sentences. This is similar to the process described in
    chapter 8 but with a key difference: it employs subword tokenization. Subword
    tokenization is a technique used in NLP to break words into smaller components,
    or subwords, allowing for more efficient and nuanced processing. For example,
    as you’ll see in the next chapter, the English phrase “I do not speak French”
    is divided into six tokens: (`i, do, not, speak, fr, ench`). Similarly, its French
    counterpart “Je ne parle pas français” is tokenized into six parts: (`je, ne,
    parle, pas, franc, ais`). This method of tokenization enhances the Transformer’s
    ability to handle language variations and complexities.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中的编码器通过首先对英语和法语句子进行标记化来处理任务。这与第8章中描述的过程类似，但有一个关键的区别：它采用子词标记化。子词标记化是NLP中用于将单词分解成更小的组成部分或子词的技术，这使得处理更加高效和细致。例如，正如你将在下一章中看到的，英语短语“我不说法语”被分为六个标记：（`i,
    do, not, speak, fr, ench`）。同样，其法语对应短语“Je ne parle pas français”被标记化为六个部分：（`je,
    ne, parle, pas, franc, ais`）。这种标记化方法增强了Transformer处理语言变化和复杂性的能力。
- en: Deep learning models, including Transformers, can’t directly process text, so
    tokens are indexed using integers before being fed to the model. These tokens
    are typically first represented using one-hot encoding, as we discussed in chapter
    8\. We then pass them through a word embedding layer to compress them into vectors
    with continuous values of a much smaller size, such as a length of 256\. Thus,
    after applying word embedding, the sentence “I do not speak French” is represented
    by a 6 × 256 matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型，包括Transformer，不能直接处理文本，因此在输入模型之前，需要对标记进行索引。这些标记通常首先使用我们第8章讨论的一热编码表示。然后，我们将它们通过词嵌入层传递，以将它们压缩成具有连续值的更小向量，例如长度为256的向量。因此，在应用词嵌入后，“我不说法语”这个句子被表示为一个6×256的矩阵。
- en: Transformers process input data such as sentences in parallel, unlike RNNs,
    which handle data sequentially. This parallelism enhances their efficiency but
    doesn’t inherently allow them to recognize the sequence order of the input. To
    address this, Transformers add positional encodings to the input embeddings. These
    positional encodings are unique vectors assigned to each position in the input
    sequence and align in dimension with the input embeddings. The vector values are
    determined by a specific positional function, particularly involving sine and
    cosine functions of varying frequencies, defined as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与处理数据序列的RNN不同，Transformers并行处理输入数据，如句子。这种并行性提高了它们的效率，但并不固有地允许它们识别输入序列的顺序。为了解决这个问题，Transformers将位置编码添加到输入嵌入中。这些位置编码是分配给输入序列中每个位置的独特向量，并且与输入嵌入在维度上对齐。向量值由一个特定的位置函数确定，特别是涉及不同频率的正弦和余弦函数，定义为
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH09_F04_Liu_EQ03.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F04_Liu_EQ03.png)'
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/CH09_F04_Liu_EQ04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F04_Liu_EQ04.png)'
- en: '| (9.3) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| (9.3) |'
- en: In these equations, vectors are calculated using the sine function for even
    indexes and the cosine function for odd indexes. The two parameters *pos* and
    *i* represent the position of a token within the sequence and the index within
    the vector, respectively. As an illustration, consider the positional encoding
    for the phrase “I do not speak French.” This is depicted as a 6 × 256 matrix,
    the same size as the word embedding for the sentence. Here, *pos* ranges from
    0 to 5, and the indexes 2i and 2i + 1 collectively span 256 distinct values (from
    0 to 255). A beneficial aspect of this positional encoding approach is that all
    values are constrained within the range of –1 to 1.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程中，向量使用正弦函数计算偶数索引，使用余弦函数计算奇数索引。参数*pos*和*i*分别代表序列中标记的位置和向量中的索引。作为一个例子，考虑短语“我不会说法语”的位置编码。这被表示为一个6
    × 256的矩阵，与句子的词嵌入大小相同。在这里，*pos*的范围从0到5，索引2i和2i + 1共同涵盖了256个不同的值（从0到255）。这种位置编码方法的一个有益方面是，所有值都被限制在-1到1的范围内。
- en: It’s important to note that each token position is uniquely identified by a
    256-dimensional vector, and these vector values remain constant throughout training.
    Before being input to the attention layers, these positional encodings are added
    to the word embeddings of the sequence. In the example of the sentence “I do not
    speak French,” the encoder generates both word embedding and positional encoding,
    each having dimensions of 6 × 256, before combining them into a single 6 × 256-dimensional
    representation. Subsequently, the encoder applies the attention mechanism to refine
    this embedding into more sophisticated vector representations that capture the
    overall meaning of the phrase, before passing them to the decoder.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，每个标记位置都由一个256维向量唯一标识，并且这些向量值在整个训练过程中保持不变。在输入到注意力层之前，这些位置编码被添加到序列的词嵌入中。以句子“我不会说法语”为例，编码器在将它们组合成一个单一的6
    × 256维表示之前，生成了词嵌入和位置编码，每个都有6 × 256的维度。随后，编码器应用注意力机制来细化这个嵌入，形成更复杂的向量表示，以捕捉短语的整体意义，然后再将它们传递给解码器。
- en: The Transformer’s encoder, as depicted in figure 9.5, is made up of six identical
    layers (N = 6). Each of these layers comprises two distinct sublayers. The first
    sublayer is a multihead self-attention layer, similar to what was discussed earlier.
    The second sublayer is a basic, position-wise, fully connected feed-forward network.
    This network treats each position in the sequence independently rather than as
    sequential elements. In the model’s architecture, each sublayer incorporates layer
    normalization and a residual connection. Layer normalization normalizes observations
    to have zero mean and unit standard deviation. Such normalization helps stabilize
    the training process. After the normalization layer, we perform the residual connection.
    This means the input to each sublayer is added to its output, enhancing the flow
    of information through the network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.5所示，Transformer的编码器由六个相同的层（N = 6）组成。这些层中的每一层都包含两个不同的子层。第一个子层是一个多头自注意力层，类似于之前讨论过的。第二个子层是一个基本的、位置性的、全连接的前馈网络。这个网络独立地处理序列中的每个位置，而不是将其作为序列元素。在模型的架构中，每个子层都包含层归一化和残差连接。层归一化将观察值规范化为零均值和单位标准差。这种归一化有助于稳定训练过程。在归一化层之后，我们执行残差连接。这意味着每个子层的输入被添加到其输出中，增强了网络中信息流的流动。
- en: '![](../../OEBPS/Images/CH09_F05_Liu.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 Transformer中编码器的结构](../../OEBPS/Images/CH09_F05_Liu.png)'
- en: Figure 9.5 The structure of the encoder in the Transformer. The encoder consists
    of N = 6 identical encoder layers. Each encoder layer contains two sublayers.
    The first sublayer is a multihead self-attention layer and the second is a feed-forward
    network. Each sublayer uses layer normalization and residual connection.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 Transformer中编码器的结构。编码器由N = 6个相同的编码器层组成。每个编码器层包含两个子层。第一个子层是一个多头自注意力层，第二个子层是一个前馈网络。每个子层都使用层归一化和残差连接。
- en: 'The decoder of the Transformer model, as seen in figure 9.6, is comprised of
    six identical decoder layers (N = 6). Each of these decoder layers features three
    sublayers: a multihead self-attention sublayer, a sublayer that performs multihead
    cross attention between the output from the first sublayer and the encoder’s output,
    and a feed-forward sublayer. Note that the input to each sublayer is the output
    from the previous sublayer. Further, the second sublayer in the decoder layer
    also takes the output from the encoder as input. This design is crucial for integrating
    information from the encoder: this is how the decode generates translations based
    on the output from the encoder.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.6所示，Transformer模型的解码器由六个相同的解码器层（N = 6）组成。这些解码器层中的每一个都包含三个子层：一个多头自注意力子层，一个在第一个子层输出和编码器输出之间执行多头交叉注意力的子层，以及一个前馈子层。请注意，每个子层的输入是前一个子层的输出。此外，解码器层中的第二个子层还接受编码器的输出作为输入。这种设计对于整合编码器中的信息至关重要：这就是解码器如何根据编码器的输出生成翻译。
- en: '![](../../OEBPS/Images/CH09_F06_Liu.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 Transformer中编码器的结构](../../OEBPS/Images/CH09_F06_Liu.png)'
- en: Figure 9.6 The structure of the decoder in the Transformer. The decoder consists
    of N = 6 identical decoder layers. Each decoder layer contains three sublayers.
    The first sublayer is a masked multihead self-attention layer. The second is a
    multihead cross-attention layer to calculate the cross attention between the output
    from the first sublayer and the output from the encoder. The third sublayer is
    a feed-forward network. Each sublayer uses layer normalization and residual connection.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 Transformer中解码器的结构。解码器由N = 6个相同的解码器层组成。每个解码器层包含三个子层。第一个子层是一个带掩码的多头自注意力层。第二个子层是一个多头交叉注意力层，用于计算第一个子层输出和编码器输出之间的交叉注意力。第三个子层是一个前馈网络。每个子层都使用层归一化和残差连接。
- en: A key aspect of the decoder’s self-attention sublayer is the masking mechanism.
    This mask prevents the model from accessing future positions in the sequence,
    ensuring that predictions for a particular position can only depend on previously
    known elements. This sequential dependency is vital for tasks like language translation
    or text generation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器自注意力子层的一个关键方面是掩码机制。这个掩码防止模型访问序列中的未来位置，确保特定位置的预测只能依赖于之前已知的元素。这种序列依赖性对于语言翻译或文本生成等任务至关重要。
- en: The decoding process begins with the decoder receiving an input phrase in French.
    The decoder transforms the French tokens into word embeddings and positional encodings
    before combining them into a single embedding. This step ensures that the model
    not only understands the semantic content of the phrase but also maintains the
    sequential context, which is crucial for accurate translation or generation tasks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 解码过程从解码器接收一个法语输入短语开始。解码器将法语标记转换为词嵌入和位置编码，然后将它们组合成一个单一的嵌入。这一步骤确保模型不仅理解短语的语义内容，而且保持序列上下文，这对于准确的翻译或生成任务至关重要。
- en: The decoder operates in an autoregressive manner, generating the output sequence
    one token at a time. At the first time step, it starts with the `"BOS"` token,
    which indicates the beginning of a sentence. Using this start token as its initial
    input, the decoder examines vector representations of the English phrase “I do
    not speak French” and attempts to predict the first token following `"BOS"`. Suppose
    the decoder’s first prediction is `"Je"`. In the next time step, it then uses
    the sequence `"BOS Je"` as its new input to predict the following token. This
    process continues iteratively, with the decoder adding each newly predicted token
    to its input sequence for the subsequent prediction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器以自回归的方式运行，一次生成一个标记的输出序列。在第一次时间步，它从 `"BOS"` 标记开始，这表示句子的开始。使用这个起始标记作为其初始输入，解码器检查英语短语
    “I do not speak French” 的向量表示，并尝试预测 `"BOS"` 后的第一个标记。假设解码器的第一个预测是 `"Je"`。在下一个时间步，它然后使用序列
    `"BOS Je"` 作为其新的输入来预测下一个标记。这个过程以迭代方式继续，解码器将每个新预测的标记添加到其输入序列中，以便进行后续预测。
- en: The translation process is designed to conclude when the decoder predicts the
    `"EOS"` token, signifying the end of the sentence. When preparing for the training
    data, we add EOS to the end of each phrase, so the model has learned that it means
    the end of a sentence. Upon reaching this token, the decoder recognizes the completion
    of the translation task and ceases its operation. This autoregressive approach
    ensures that each step in the decoding process is informed by all previously predicted
    tokens, allowing for coherent and contextually appropriate translations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译过程被设计为在解码器预测 `"EOS"` 标记时结束，这标志着句子的结束。在准备训练数据时，我们在每个短语的末尾添加 EOS，这样模型就学会了这意味着句子的结束。当达到这个标记时，解码器识别到翻译任务的完成并停止其操作。这种自回归方法确保解码过程中的每一步都由之前预测的所有标记提供信息，从而实现连贯和上下文适当的翻译。
- en: 9.1.3 Different types of Transformers
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 不同类型的 Transformer
- en: 'There are three types of Transformers: encoder-only Transformers, decoder-only
    Transformers, and encoder-decoder Transformers. We are using an encoder-decoder
    Transformer in this chapter and the next, but you’ll get a chance to explore firsthand
    decoder-only Transformers later in the book.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种类型的 Transformer：仅编码器 Transformer、仅解码器 Transformer 和编码器-解码器 Transformer。我们在这章和下一章中使用的是编码器-解码器
    Transformer，但你将在本书的后面有机会亲自探索仅解码器 Transformer。
- en: An encoder-only Transformer consists of N identical encoder layers as shown
    on the left side of figure 9.4 and is capable of converting a sequence into abstract
    continuous vector representations. For example, BERT is an encoder-only Transformer
    that contains 12 encoder layers. An encoder-only Transformer can be used for text
    classification, for example. If two sentences have similar vector representations,
    we can classify the two sentences into one category. On the other hand, if two
    sequences have very different vector representations, we can put them in different
    categories.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅包含编码器的 Transformer 由图 9.4 左侧所示的 N 个相同的编码器层组成，并且能够将序列转换为抽象的连续向量表示。例如，BERT 就是一个仅包含
    12 个编码器层的编码器-only Transformer。仅包含编码器的 Transformer 可以用于文本分类，例如。如果两个句子具有相似的向量表示，我们可以将这两个句子分类到同一类别。另一方面，如果两个序列具有非常不同的向量表示，我们可以将它们放入不同的类别中。
- en: A decoder-only Transformer also consists of N identical layers, and each layer
    is a decoder layer as shown on the right side of figure 9.4\. For example, ChatGPT
    is a decoder-only Transformer that contains many decoder layers. The decoder-only
    Transformer can generate text based on a prompt, for example. It extracts the
    semantic meaning of the words in the prompt and predicts the most likely next
    token. It then adds the token to the end of the prompt and repeats the process
    until the text reaches a certain length.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器 Transformer 也由 N 个相同的层组成，每个层都是图 9.4 右侧所示的解码器层。例如，ChatGPT 是一个包含许多解码器层的仅解码器
    Transformer。仅解码器 Transformer 可以根据提示生成文本，例如。它提取提示中单词的语义含义并预测最可能的下一个标记。然后它将标记添加到提示的末尾并重复此过程，直到文本达到一定长度。
- en: The machine language translation Transformer we discussed earlier is an example
    of an encoder-decoder Transformer. They are needed for handling complicated tasks,
    such as text-to-image generation or speech recognition. Encoder-decoder Transformers
    combine the strengths of both encoders and decoders. Encoders are efficient in
    processing and understanding input data, while decoders excel in generating output.
    This combination allows the model to effectively understand complex inputs (like
    text or speech) and generate intricate outputs (like images or transcribed text).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的机器翻译 Transformer 是编码器-解码器 Transformer 的一个例子。它们对于处理复杂任务（如文本到图像生成或语音识别）是必需的。编码器-解码器
    Transformer 结合了编码器和解码器的优点。编码器在处理和理解输入数据方面效率高，而解码器在生成输出方面表现卓越。这种组合使模型能够有效地理解复杂的输入（如文本或语音）并生成复杂的输出（如图像或转录文本）。
- en: 9.2 Building an encoder
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 构建编码器
- en: We’ll develop and train an encoder-decoder Transformer designed for machine
    language translation. The coding in this project is adapted from the work of Chris
    Cui in translating Chinese to English ([https://mng.bz/9o1o](https://mng.bz/9o1o))
    and Alexander Rush’s German-to-English translation project ([https://mng.bz/j0mp](https://mng.bz/j0mp)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发并训练一个针对机器语言翻译设计的编码器-解码器 Transformer。本项目中的编码来自 Chris Cui 的将中文翻译成英文的工作([https://mng.bz/9o1o](https://mng.bz/9o1o))和
    Alexander Rush 的德语到英语翻译项目([https://mng.bz/j0mp](https://mng.bz/j0mp))。
- en: This section discusses how to construct an encoder in the Transformer. Specifically,
    we’ll dive into the process of building various sublayers within each encoder
    layer and implementing the multihead self-attention mechanism.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何在 Transformer 中构建编码器。具体来说，我们将深入探讨构建每个编码器层内各种子层的过程以及实现多头自注意力机制。
- en: 9.2.1 The attention mechanism
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 注意力机制
- en: While there are different attention mechanisms, we’ll use the SDPA because it’s
    widely used and effective. The SDPA attention mechanism uses query, key, and value
    to calculate the relationships among elements in a sequence. It assigns scores
    to show how an element is related to all elements in a sequence (including the
    element itself).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在不同的注意力机制，但我们将使用 SDPA，因为它被广泛使用且有效。SDPA 注意力机制使用查询、键和值来计算序列中元素之间的关系。它分配分数以显示一个元素与序列中所有元素（包括该元素本身）的关系。
- en: Instead of using one set of query, key, and value vectors, the Transformer model
    uses a concept called multihead attention. Our 256-dimensional query, key, and
    value vectors are split into 8 heads, and each head has a set of query, key, and
    value vectors with dimensions of 32 (because 256/8 = 32). Each head pays attention
    to different parts or aspects of the input, enabling the model to capture a broader
    range of information and form a more detailed and contextual understanding of
    the input data. For example, multihead attention allows the model to capture the
    multiple meanings of the word “bank” in the pun joke, “Why is the river so rich?
    Because it has two banks.”
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用一组查询、键和值向量不同，Transformer 模型使用了一个称为多头注意力的概念。我们的 256 维查询、键和值向量被分成 8 个头，每个头都有一组查询、键和值向量，维度为
    32（因为 256/8 = 32）。每个头关注输入的不同部分或方面，使模型能够捕捉更广泛的信息，并对输入数据形成更详细和上下文化的理解。例如，多头注意力允许模型捕捉双关语笑话中“bank”一词的多种含义，“为什么这条河这么富饶？因为它有两个河岸。”
- en: To implement this, we define an `attention()` function in the local module ch09util.
    Download the file ch09util.py from the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and store it in the /utils/ directory on your computer. The attention() function
    is defined as shown in the following listing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们在本地模块 ch09util 中定义了一个 `attention()` 函数。从本书的 GitHub 仓库([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))下载文件
    ch09util.py 并将其存储在您计算机的 /utils/ 目录中。`attention()` 函数的定义如下所示。
- en: Listing 9.1 Calculating attention based on query, key, and value
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 基于查询、键和值计算注意力
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Scaled attention score is the dot product of query and key, scaled by the
    square root of d[k].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ① 缩放后的注意力分数是查询和键的点积，乘以 d[k] 的平方根。
- en: ② If there is a mask, hides future elements in the sequence
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果有掩码，则隐藏序列中的未来元素
- en: ③ Calculates attention weights
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算注意力权重
- en: ④ Returns both attention and attention weights
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 返回注意力和注意力权重
- en: The `attention()` function takes query, key, and value as inputs and calculates
    attention and attention weights as we discussed earlier in this chapter. The scaled
    attention score is the dot product of query and key, scaled by the square root
    of the dimension of the key, *d[k]*. We apply the softmax function on the scaled
    attention score to obtain attention weights. Finally, attention is calculated
    as the dot product of attention weights and value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`attention()`函数接受查询、键和值作为输入，并计算注意力和注意力权重，正如我们在本章前面讨论的那样。缩放后的注意力分数是查询和键的点积，乘以键的维度的平方根*d[k]*。我们应用softmax函数到缩放后的注意力分数以获得注意力权重。最后，注意力是注意力权重和值的点积。'
- en: Let’s use our running example to show how multihead attention works (see figure
    9.7). The embedding for “How are you?” is a tensor with a size of (1, 6, 256),
    as we explained in the last section (after we add positional encoding to word
    embedding). Note that 1 means there is one sentence in the batch, and there are
    six tokens in the sentence instead of four because we add BOS and EOS to the beginning
    and the end of the sequence. This embedding is passed through three linear layers
    to obtain query Q, key K, and value V, each of the same size (1, 6, 256). These
    are divided into eight heads, resulting in eight distinct sets of Q, K, and V,
    now sized (1, 6, 256/8 = 32) each. The attention function, as defined earlier,
    is applied to each of these sets, yielding eight attention outputs, each also
    sized (1, 6, 32). We then concatenate the eight attention outputs into one single
    attention, and the result is a tensor with a size of (1, 6, 32 × 8 = 256). Finally,
    this combined attention passes through another linear layer sized 256 × 256, leading
    to the output from the `MultiHeadAttention()` class. This output maintains the
    original input’s dimensions, which are (1, 6, 256).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的运行示例来展示多头注意力的工作原理（见图9.7）。"你好吗？"的嵌入是一个大小为(1, 6, 256)的张量，正如我们在上一节中解释的（在我们将位置编码添加到词嵌入之后）。请注意，1表示批处理中有一个句子，句子中有六个标记而不是四个，因为我们向序列的开始和结束添加了BOS和EOS。这个嵌入通过三个线性层传递，以获得查询Q、键K和值V，每个的大小相同（1,
    6, 256）。这些被分成八个头，现在每个头的大小为(1, 6, 256/8 = 32)。根据前面定义的注意力函数，将这些集合中的每个都应用注意力函数，产生八个注意力输出，每个的大小也是(1,
    6, 32)。然后，我们将八个注意力输出连接成一个单一的注意力，结果是大小为(1, 6, 32 × 8 = 256)的张量。最后，这个组合注意力通过另一个大小为256
    × 256的线性层，导致`MultiHeadAttention()`类的输出。这个输出保持了原始输入的维度，即(1, 6, 256)。
- en: '![](../../OEBPS/Images/CH09_F07_Liu.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F07_Liu.png)'
- en: Figure 9.7 An example of multihead attention. This diagram uses the calculation
    of the multihead self attention for the phrase “How are you?” as an example. We
    first pass the embedding through three neural networks to obtain query Q, key
    K, and value V, each with a size of (1, 6, 256). We split them into eight heads,
    each with a set of Q, k, and V, with a size of (1, 6, 32). We calculate the attention
    in each head. The attention vectors from the eight heads are then joined back
    into one single attention vector, with a size of (1, 6, 256).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 多头注意力的一个示例。此图以短语“你好吗？”的多头自注意力计算为例。我们首先将嵌入通过三个神经网络传递，以获得查询Q、键K和值V，每个的大小为(1,
    6, 256)。我们将它们分成八个头，每个头都有一组Q、k和V，大小为(1, 6, 32)。我们在每个头中计算注意力。然后，将八个头的注意力向量合并回一个单一的注意力向量，大小为(1,
    6, 256)。
- en: This is implemented in the following code listing in the local module.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下本地模块的代码列表中实现。
- en: Listing 9.2 Calculating multihead attention
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 计算多头注意力
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Passes input through three linear layers to obtain Q, K, V, and splits them
    into multiheads
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将输入通过三个线性层传递以获得Q、K、V，并将它们分成多头
- en: ② Calculates attention and attention weights for each head
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算每个头的注意力和注意力权重
- en: ③ Concatenates attention vectors from multiheads into one single attention vector
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将多头注意力向量连接成一个单一的注意力向量
- en: ④ Passes the output through a linear layer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将输出通过一个线性层
- en: 'Each encoder layer and decoder layer also contain a feed-forward sublayer,
    which is a two-layer fully connected neural network, with the purpose of enhancing
    the model’s ability to capture and learn intricate features in the training dataset.
    Further, the neural network processes each embedding independently. It doesn’t
    treat the sequence of embeddings as a single vector. Therefore, we often call
    it a position-wide feed-forward network (or a 1D convolutional network). For that
    purpose, we define a `PositionwiseFeedForward()` class in the local module as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器层和解码器层也包含一个前馈子层，这是一个两层全连接神经网络，其目的是增强模型在训练数据集中捕获和学习的复杂特征的能力。此外，神经网络独立处理每个嵌入。它不将嵌入序列视为单个向量。因此，我们通常称其为位置感知的前馈网络（或一维卷积网络）。为此，我们在本地模块中定义了一个`PositionwiseFeedForward()`类，如下所示：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `PositionwiseFeedForward()` class is defined with two key parameters: `d_ff`,
    the dimensionality of the feed-forward layer, and `d_model`, representing the
    model’s dimension size. Typically, `d_ff` is chosen to be four times the size
    of `d_model`. In our example, `d_model` is 256, and we therefore set `d_ff` to
    256 * 4 = 1024. This practice of enlarging the hidden layer in comparison to the
    model size is a standard approach in Transformer architectures. It enhances the
    network’s ability to capture and learn intricate features in the training dataset.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`PositionwiseFeedForward()`类使用两个关键参数定义：`d_ff`，表示前馈层的维度，和`d_model`，表示模型的维度大小。通常，`d_ff`被选择为`d_model`的四倍。在我们的例子中，`d_model`是256，因此我们将`d_ff`设置为256
    * 4 = 1024。与模型大小相比扩大隐藏层的方法是Transformer架构中的标准做法。这增强了网络在训练数据集中捕获和学习的复杂特征的能力。'
- en: 9.2.2 Creating an encoder
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 创建编码器
- en: To create an encoder layer, we first define the following `EncoderLayer()` class
    and `SublayerConnection()` class.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个编码器层，我们首先定义以下`EncoderLayer()`类和`SublayerConnection()`类。
- en: Listing 9.3 A class to define an encoder layer
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 定义编码器层的类
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① The first sublayer in each encoder layer is a multihead self-attention network.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ① 每个编码器层中的第一个子层是一个多头自注意力网络。
- en: ② The second sublayer in each encoder layer is a feed-forward network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ② 每个编码器层中的第二个子层是一个前馈网络。
- en: ③ Each sublayer goes through residual connection and layer normalization.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 每个子层都经过残差连接和层归一化。
- en: 'Each encoder layer is composed of two distinct sublayers: one is a multihead
    self-attention layer, as outlined in the `MultiHeadAttention()` class, and the
    other is a straightforward, position-wise, fully connected feed-forward network,
    as specified in the `PositionwiseFeedForward()` class. Additionally, both of these
    sublayers incorporate layer normalization and residual connections. As explained
    in chapter 6, a residual connection involves passing the input through a sequence
    of transformations (either the attention or the feed-forward layer in this context)
    and then adding the input back to these transformations’ output. The method of
    residual connection is employed to combat the problem of vanishing gradients,
    which is a common challenge in very deep networks. Another benefit of residual
    connections in Transformers is to provide a passage to pass the positional encodings
    (which are calculated only before the first layer) to subsequent layers.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器层由两个不同的子层组成：一个是`MultiHeadAttention()`类中概述的多头自注意力层，另一个是`PositionwiseFeedForward()`类中指定的简单、位置感知的全连接前馈网络。此外，这两个子层都包含层归一化和残差连接。正如第6章所述，残差连接涉及通过一系列变换（在此上下文中是注意力或前馈层）传递输入，然后将输入添加到这些变换的输出中。残差连接的方法被用来对抗梯度消失问题，这是非常深的网络中常见的挑战。在Transformers中，残差连接的另一个好处是提供一个通道，将位置编码（仅在第一层之前计算）传递到后续层。
- en: 'Layer normalization is somewhat similar to the batch normalization we implemented
    in chapter 4\. It standardizes the observations in a layer to have a zero mean
    and a unit standard deviation. To achieve this within the local module, we define
    the `LayerNorm()` class, which executes layer normalization as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化与我们在第4章中实现的批归一化有些类似。它将层中的观察值标准化为零均值和单位标准差。为了在本地模块中实现这一点，我们定义了`LayerNorm()`类，该类执行层归一化如下：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `mean` and `std` values in the preceding `LayerNorm()` class are the mean
    and standard deviation of the inputs in each layer. The `a_2` and `b_2` layers
    in the `LayerNorm()` class expand `x_zscore` back to the shape of the input `x`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的`LayerNorm()`类中的`mean`和`std`值是每个层中输入的均值和标准差。`LayerNorm()`类中的`a_2`和`b_2`层将`x_zscore`扩展回输入`x`的形状。
- en: 'We can now create an encoder by stacking six encoder layers together. For that
    purpose, we define the `Encoder()` class in the local module:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过堆叠六个编码器层来创建一个编码器。为此，我们在本地模块中定义了`Encoder()`类：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, the `Encoder()` class is defined with two arguments: `layer`, which is
    an encoder layer as defined in the `EncoderLayer()` class in listing 9.3, and
    `N`, the number of encoder layers in the encoder. The `Encoder()` class takes
    input `x` (for example, a batch of English phrases) and the mask (to mask out
    sequence padding, as I’ll explain in chapter 10) to generate output (vector representations
    that capture the meanings of the English phrases).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Encoder()`类使用两个参数定义：`layer`，它是一个编码器层，如列表9.3中定义的`EncoderLayer()`类中的编码器层，以及`N`，编码器中编码器层的数量。`Encoder()`类接受输入`x`（例如，一批英语短语）和掩码（以屏蔽序列填充，如我在第10章中解释的）以生成输出（捕获英语短语意义的向量表示）。
- en: With that, you have created an encoder. Next, you’ll learn how to create a decoder.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你就创建了一个编码器。接下来，你将学习如何创建一个解码器。
- en: 9.3 Building an encoder-decoder Transformer
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 构建编码器-解码器Transformer
- en: Now that you understand how to create an encoder in the Transformer, let’s move
    on to the decoder. You’ll first learn how to create a decoder layer in this section.
    You’ll then stack N = 6 identical decoder layers to form a decoder.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何在Transformer中创建编码器，让我们继续到解码器。在本节中，你将首先学习如何创建解码器层。然后，你将堆叠N = 6个相同的解码器层来形成一个解码器。
- en: 'We then create an encoder-decoder transformer with five components: `encoder`,
    `decoder`, `src_embed, tgt_embed`, and `generator`, which I’ll explain in this
    section.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个包含五个组件的编码器-解码器Transformer：`encoder`、`decoder`、`src_embed`、`tgt_embed`和`generator`，我将在本节中解释这些。
- en: 9.3.1 Creating a decoder layer
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 创建解码器层
- en: 'Each decoder layer consists of three sublayers: (1) a multihead self-attention
    layer, (2) the cross attention between the output from the first sublayer and
    the encoder’s output, and (3) a feed-forward network. Each of these three sublayers
    incorporates a layer normalization and the residual connection, similar to what
    we have done in encoder layers. Furthermore, the decoder stack’s multihead self-attention
    sublayer is masked to prevent positions from attending to subsequent positions.
    The mask forces the model to use previous elements in a sequence to predict later
    elements. I’ll explain how masked multihead self-attention works in a moment.
    To implement this, we define the `DecoderLayer()` class in the local module.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解码器层由三个子层组成：（1）一个多头自注意力层，（2）第一个子层的输出与编码器输出的交叉注意力，以及（3）一个前馈网络。这三个子层都包含一个层归一化和残差连接，类似于我们在编码器层中做的。此外，解码器堆叠的多头自注意力子层被掩码，以防止位置关注后续位置。掩码迫使模型使用序列中的先前元素来预测后续元素。我将在稍后解释掩码多头自注意力是如何工作的。为了实现这一点，我们在本地模块中定义了`DecoderLayer()`类。
- en: Listing 9.4 Creating a decoder layer
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 创建解码器层
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① The first sublayer is a masked multihead self-attention layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ① 第一个子层是一个掩码多头自注意力层。
- en: ② The second sublayer is a cross-attention layer between the target language
    and the source language.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ② 第二个子层是在目标语言和源语言之间的交叉注意力层。
- en: ③ The third sublayer is a feed-forward network.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 第三个子层是一个前馈网络。
- en: 'To illustrate the operation of a decoder layer, let’s consider our ongoing
    example. The decoder takes in tokens `[''BOS'', ''comment'', ''et'', ''es-vous'',
    ''?'']`, along with the output from the encoder (referred to as `memory` in the
    preceding code block), to predict the sequence `[''comment'', ''et'', ''es-vous'',
    ''?'', ''EOS'']`. The embedding of `[''BOS'', ''comment'', ''et'', ''es-vous'',
    ''?'']` is a tensor of size (1, 5, 256): 1 is the number of sequences in the batch,
    5 is the number of tokens in the sequence, and 256 means each token is represented
    by a 256-value vector. We pass this embedding through the first sublayer, a masked
    multihead self-attention layer. This process is similar to the multihead self-attention
    calculation you saw earlier in the encoder layer. However, the process utilizes
    a mask, designated as `tgt_mask` in the preceding code block, which is a 5 × 5
    tensor with the following values in the ongoing example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明解码器层的操作，让我们考虑我们正在进行的例子。解码器接收标记`['BOS', 'comment', 'et', 'es-vous', '?']`以及编码器的输出（在前面代码块中称为`memory`），来预测序列`['comment',
    'et', 'es-vous', '?', 'EOS']`。`['BOS', 'comment', 'et', 'es-vous', '?']`的嵌入是一个大小为（1，5，256）的张量：1是批次中的序列数量，5是序列中的标记数量，256表示每个标记由一个256值的向量表示。我们把这个嵌入通过第一个子层，一个带掩码的多头自注意力层。这个过程与您在编码器层中看到的早期多头自注意力计算类似。然而，这个过程利用了一个掩码，在前面代码块中指定为`tgt_mask`，它是一个5
    × 5的张量，在当前例子中的值如下：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you may have noticed, the lower half of the mask (values below the main diagonal
    in the tensor) is turned on as `True`, and the upper half of the mask (values
    above the main diagonal) is turned off as `False`. When this mask is applied to
    the attention scores, it results in the first token attending only to itself during
    the first time step. In the second time step, attention scores are calculated
    exclusively between the first two tokens. As the process continues, for example,
    in the third time step, the decoder uses tokens `['BOS', 'comment', 'et']` to
    predict the token `'es-vous'`, and the attention scores are computed only among
    these three tokens, effectively hiding the future tokens `['es-vous', '?']`
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，掩码的下半部分（张量中低于主对角线的值）被设置为`True`，而上半部分（主对角线以上的值）被设置为`False`。当这个掩码应用于注意力分数时，它会导致在第一次时间步中第一个标记只关注自身。在第二次时间步中，注意力分数仅计算在第一个和第二个标记之间。随着过程的继续，例如，在第三次时间步中，解码器使用标记`['BOS',
    'comment', 'et']`来预测标记`'es-vous'`，并且注意力分数仅在这三个标记之间计算，有效地隐藏了未来的标记`['es-vous', '?']`
- en: Following this process, the output generated from the first sublayer, which
    is a tensor of size (1, 5, 256), matches the input’s size. This output, which
    we can refer to as x, is then fed into the second sublayer. Here, cross attention
    is computed between x and the output of the encoder stack, termed `memory`. You
    may remember that `memory` has a dimension of (1, 6, 256) since the English phrase
    “How are you?” is converted to six tokens `['BOS', 'how', 'are', 'you', '?', 'EOS']`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个流程，从第一个子层生成的输出，它是一个大小为（1，5，256）的张量，与输入的大小相匹配。这个输出，我们可以称之为x，然后被输入到第二个子层。在这里，x和编码器堆栈的输出（在前面代码块中称为`memory`）之间计算交叉注意力。您可能还记得，`memory`的维度是（1，6，256），因为英语短语“你好吗？”被转换成六个标记`['BOS',
    'how', 'are', 'you', '?', 'EOS']`。
- en: 'Figure 9.8 shows how cross-attention weights are calculated. To calculate the
    cross attention between x and `memory`, we first pass x through a neural network
    to obtain query, which has a dimension of (1, 5, 256). We then pass `memory` through
    two neural networks to obtain key and value, each having a dimension of (1, 6,
    256). The scaled attention score is calculated using the formula as specified
    in equation 9.1\. This scaled attention score has a dimension of (1, 5, 6): the
    query Q has a dimension of (1, 5, 256) and the transposed key K has a dimension
    of (1, 256, 6). Therefore, the scaled attention score, which is the dot product
    of the two, scaled by √*d[k]*, has a size of (1, 5, 6). After applying the softmax
    function to the scaled attention score, we obtain attention weights, which is
    a 5 × 6 matrix. This matrix tells us how the five tokens in the French input `[''BOS'',
    ''comment'', ''et'', ''es-vous'', ''?'']` attend to the six tokens in the English
    phrase `[''BOS'', ''how'', ''are'', ''you'', ''?'', ''EOS'']`. This is how the
    decoder captures the meaning of the English phrase when translating.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 展示了如何计算交叉注意力权重。为了计算 x 和 `memory` 之间的交叉注意力，我们首先将 x 通过一个神经网络来获得查询，其维度为 (1,
    5, 256)。然后我们将 `memory` 通过两个神经网络来获得键和值，每个的维度为 (1, 6, 256)。使用方程 9.1 中指定的公式计算缩放后的注意力分数。这个缩放后的注意力分数的维度为
    (1, 5, 6)：查询 Q 的维度为 (1, 5, 256)，转置后的键 K 的维度为 (1, 256, 6)。因此，缩放后的注意力分数，即这两个向量的点积，并乘以
    √*d[k]*，其大小为 (1, 5, 6)。在将缩放后的注意力分数应用 softmax 函数后，我们获得注意力权重，这是一个 5 × 6 的矩阵。这个矩阵告诉我们，在法语输入
    `['BOS', 'comment', 'et', 'es-vous', '?']` 中的五个标记如何关注英语短语 `['BOS', 'how', 'are',
    'you', '?', 'EOS']` 中的六个标记。这就是解码器在翻译时如何捕捉英语短语的含义。
- en: '![](../../OEBPS/Images/CH09_F08_Liu.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8](../../OEBPS/Images/CH09_F08_Liu.png)'
- en: Figure 9.8 An example of how cross-attention weights are calculated between
    the input to the decoder and the output from the encoder. The input to the decoder
    is passed through a neural network to obtain query Q. The output from the encoder
    is passed through a different neural network to obtain key K. The scaled cross-attention
    scores are calculated as the dot product of Q and K divided by the square root
    of the dimension of K. Finally, we apply the softmax function on the scaled cross-attention
    scores to obtain the cross-attention weights, which demonstrate how each element
    in Q is related to all elements in K.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 展示了解码器输入和编码器输出之间如何计算交叉注意力权重的一个例子。解码器的输入通过一个神经网络来获得查询 Q。编码器的输出通过一个不同的神经网络来获得键
    K。缩放后的交叉注意力分数是通过 Q 和 K 的点积除以 K 的维度平方根来计算的。最后，我们对缩放后的交叉注意力分数应用 softmax 函数，以获得交叉注意力权重，这显示了
    Q 中的每个元素与 K 中的所有元素之间的关系。
- en: The final cross attention in the second sublayer is then calculated as the dot
    product of attention weights and the value vector V. The attention weights have
    a dimension of (1, 5, 6) and the value vector has a dimension of (1, 6, 256),
    so the final cross attention, which is the dot product of the two, has a size
    of (1, 5, 256). Therefore, the input and output of the second sublayer have the
    same dimension of (1, 5, 256). After processing through this second sublayer,
    the output is then directed through the third sublayer, which is a feed-forward
    network.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个子层中的最终交叉注意力是通过注意力权重和值向量 V 的点积来计算的。注意力权重的维度为 (1, 5, 6)，值向量的维度为 (1, 6, 256)，因此最终的交叉注意力，即这两个向量的点积，其大小为
    (1, 5, 256)。因此，第二个子层的输入和输出具有相同的维度 (1, 5, 256)。经过这个第二个子层的处理之后，输出随后通过第三个子层，这是一个前馈网络。
- en: 9.3.2 Creating an encoder-decoder Transformer
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 创建编码器-解码器 Transformer
- en: The decoder consists of N = 6 identical decoder layers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由 N = 6 个相同的解码器层组成。
- en: 'The `Decoder()` class is defined in the local module as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`Decoder()` 类在本地模块中的定义如下：'
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To create an encoder-decoder transformer, we first define a `Transformer()`
    class in the local module. Open the file ch09util.py, and you’ll see the definition
    of the class as shown in the following listing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个编码器-解码器 Transformer，我们首先在本地模块中定义一个 `Transformer()` 类。打开文件 ch09util.py，你会看到类的定义如下所示。
- en: Listing 9.5 A class to represent an encoder-decoder Transformer
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 表示编码器-解码器 Transformer 的类
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Defines the encoder in the Transformer
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在 Transformer 中定义编码器
- en: ② Defines the decoder in the Transformer
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在 Transformer 中定义解码器
- en: ③ Source language is encoded into abstract vector representations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 源语言被编码成抽象向量表示。
- en: ④ The decoder uses these vector representations to generate translation in the
    target language.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 解码器使用这些向量表示来生成目标语言的翻译。
- en: 'The `Transformer()` class is constructed with five key components: `encoder`,
    `decoder`, `src_embed`, `tgt_embed`, and `generator`. The encoder and decoder
    are represented by the `Encoder()` and `Decoder()` classes defined previously.
    In the next chapter, you’ll learn to generate the source language embedding: we’ll
    process numerical representations of English phrases using word embedding and
    positional encoding, combining the results to form the `src_embed` component.
    Similarly, for the target language, we process numerical representations of French
    phrases in the same manner, using the combined output as the `tgt_embed` component.
    The generator produces predicted probabilities for each index that corresponds
    to the tokens in the target language. We’ll define a `Generator()` class in the
    next section for this purpose.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer()`类由五个关键组件构成：`encoder`、`decoder`、`src_embed`、`tgt_embed`和`generator`。编码器和解码器由之前定义的`Encoder()`和`Decoder()`类表示。在下一章中，你将学习生成源语言嵌入：我们将使用词嵌入和位置编码处理英语短语的数值表示，将结果组合形成`src_embed`组件。同样，对于目标语言，我们以相同的方式处理法语短语的数值表示，将组合输出作为`tgt_embed`组件。生成器为与目标语言中的标记相对应的每个索引生成预测概率。我们将在下一节定义一个`Generator()`类来完成此目的。'
- en: 9.4 Putting all the pieces together
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 将所有部件组合在一起
- en: In this section, we’ll put all the pieces together to create a model that can
    translate between any two languages.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把所有部件组合起来，创建一个可以翻译任何两种语言的模型。
- en: 9.4.1 Defining a generator
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 定义生成器
- en: First, we define a `Generator()` class in the local module to generate the probability
    distribution of the next token (see figure 9.9). The idea is to attach a head
    to the decoder for downstream tasks. In our example in the next chapter, the downstream
    task is to predict the next token in the French translation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在本地模块中定义一个`Generator()`类来生成下一个标记的概率分布（见图9.9）。想法是为解码器附加一个头部以用于下游任务。在我们下一章的例子中，下游任务是预测法语翻译中的下一个标记。
- en: '![](../../OEBPS/Images/CH09_F09_Liu.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F09_Liu.png)'
- en: Figure 9.9 The structure of the generator in the Transformer. The generator
    converts the output from the decoder stack to a probability distribution over
    the target language’s vocabulary, so that the Transformer can use the distribution
    to predict the next token in the French translation of an English phrase. The
    generator contains a linear layer so that the number of outputs is the same as
    the number of tokens in the French vocabulary. The generator also applies a softmax
    activation to the output so that the output is a probability distribution.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 Transformer中生成器的结构。生成器将解码器堆栈的输出转换为目标语言词汇表上的概率分布，以便Transformer可以使用该分布来预测英语短语的法语翻译中的下一个标记。生成器包含一个线性层，因此输出数量与法语词汇表中的标记数量相同。生成器还对输出应用softmax激活，以便输出是一个概率分布。
- en: 'The class is defined as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 该类定义如下：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `Generator()` class produces predicted probabilities for each index that
    corresponds to the tokens in the target language. This enables the model to sequentially
    predict tokens in an autoregressive manner, utilizing previously generated tokens
    and the encoder’s output.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Generator()`类为与目标语言中的标记相对应的每个索引生成预测概率。这使得模型能够以自回归的方式顺序预测标记，利用先前生成的标记和解码器的输出。'
- en: 9.4.2 Creating a model to translate between two languages
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 创建用于两种语言之间翻译的模型
- en: Now we are ready to create a Transformer model to translate between any two
    languages (e.g., English to French or Chinese to English). The `create_model()`
    function defined in the local module accomplishes that.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好创建一个Transformer模型，用于翻译任何两种语言（例如，英语到法语或中文到英语）。在本地模块中定义的`create_model()`函数实现了这一点。
- en: Listing 9.6 Creating a Transformer to translate between two languages
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 创建用于两种语言之间翻译的Transformer
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Creates an encoder by instantiating the Encoder() class
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ① 通过实例化Encoder()类创建编码器
- en: ② Creates a decoder by instantiating the Decoder() class
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过实例化Decoder()类创建解码器
- en: ③ Creates src_embed by passing source language through word embedding and positional
    encoding
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过将源语言通过词嵌入和位置编码传递来创建src_embed
- en: ④ Creates tgt_embed by passing target language through word embedding and positional
    encoding
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 通过将目标语言通过词嵌入和位置编码传递来创建tgt_embed
- en: ⑤ Creates a generator by instantiating the Generator() class
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过实例化Generator()类创建生成器
- en: 'The primary element of the `create_model()` function is the `Transformer()`
    class, which was previously defined. Recall that the `Transformer()` class is
    built with five essential elements: `encoder`, `decoder`, `src_embed`, `tgt_embed`,
    and `generator`. Within the create_model() function, we sequentially construct
    these five components, using the recently defined `Encoder()`, `Decoder()`, and
    `Generator()` classes. In the next chapter, we’ll discuss in detail how to generate
    the source and target language embeddings, `src_embed` and `tgt_embed`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_model()`函数的主要元素是之前定义的`Transformer()`类。回想一下，`Transformer()`类由五个基本元素组成：`encoder`、`decoder`、`src_embed`、`tgt_embed`和`generator`。在`create_model()`函数中，我们依次构建这五个组件，使用最近定义的`Encoder()`、`Decoder()`和`Generator()`类。在下一章中，我们将详细讨论如何生成源语言和目标语言嵌入`src_embed`和`tgt_embed`。'
- en: In the next chapter, you’ll apply the Transformer you created here to English-to-French
    translation. You’ll train the model using more than 47,000 pairs of English-to-French
    translations. You’ll then use the trained model to translate common English phrases
    into French.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将应用在这里创建的Transformer进行英语到法语翻译。你将使用超过47,000对英语到法语的翻译对来训练模型。然后，你将使用训练好的模型将常见的英语短语翻译成法语。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Transformers are advanced deep-learning models that excel in handling sequence-to-sequence
    prediction challenges. Their strength lies in effectively understanding the relationships
    between elements in input and output sequences over long distances.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer是先进的深度学习模型，擅长处理序列到序列预测挑战。它们的优势在于能够有效地理解输入和输出序列中元素之间的长距离关系。
- en: The revolutionary aspect of the Transformer architecture is its attention mechanism.
    This mechanism assesses the relationship between words in a sequence by assigning
    weights, determining how closely words are related based on the training data.
    This enables Transformer models like ChatGPT to comprehend relationships between
    words, thus understanding human language more effectively.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer架构的革命性方面是其注意力机制。该机制通过分配权重来评估序列中单词之间的关系，根据训练数据确定单词之间关系的紧密程度。这使得像ChatGPT这样的Transformer模型能够理解单词之间的关系，从而更有效地理解人类语言。
- en: 'To calculate SDPA, the input embedding X is processed through three distinct
    neural network layers, query (Q), key (K), and value (V). The corresponding weights
    for these layers are W^Q, W^K, and W^V. We can calculate Q, K, and V as Q = X
    * W^Q, K = X * Q^K, and V = X * W^V. SDPA is calculated as follows:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要计算SDPA，输入嵌入X通过三个不同的神经网络层进行处理，查询（Q）、键（K）和值（V）。这些层的相应权重分别是W^Q、W^K和W^V。我们可以计算Q、K和V如下：Q
    = X * W^Q，K = X * Q^K，V = X * W^V。SDPA的计算如下：
- en: '![](../../OEBPS/Images/CH09_F09_Liu_EQ05.png)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F09_Liu_EQ05.png)'
- en: where d[k] represents the dimension of the key vector K. The softmax function
    is applied to the attention scores, converting them into attention weights. This
    ensures that the total attention a word gives to all words in the sentence sums
    to 100%. The final attention is the dot product of these attention weights and
    the value vector V.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其中d[k]表示键向量K的维度。softmax函数应用于注意力分数，将它们转换为注意力权重。这确保了单词对句子中所有单词的总注意力加起来为100%。最终的注意力是这些注意力权重与值向量V的点积。
- en: Instead of using one set of query, key, and value vectors, Transformer models
    use a concept called multihead attention. The query, key, and value vectors are
    split into multiple heads. Each head pays attention to different parts or aspects
    of the input, enabling the model to capture a broader range of information and
    form a more detailed and contextual understanding of the input data. Multihead
    attention is especially useful when a word has multiple meanings in a sentence.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与使用一组查询、键和值向量不同，Transformer模型使用一个称为多头注意力的概念。查询、键和值向量被分成多个头。每个头关注输入的不同部分或方面，使模型能够捕捉更广泛的信息，并对输入数据形成更详细和情境化的理解。多头注意力在句子中一个单词有多个意义时特别有用。
- en: '* * *'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-001-backlink))  Vaswani et al., 2017, “Attention Is All You
    Need.” [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-001-backlink)) Vaswani等人，2017年，“Attention Is All You Need.”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)。
- en: ^([2](#footnote-000-backlink))  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
    Bengio, 2014, “Neural Machine Translation by Jointly Learning to Align and Translate.”
    h[ttps://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](#footnote-000-backlink))  德米特里·巴汉诺夫（Dzmitry Bahdanau）、金亨勋（Kyunghyun Cho）和约舒亚·本吉奥（Yoshua
    Bengio），2014年，“通过联合学习对齐和翻译进行神经机器翻译。” h[ttps://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
