- en: 1 What is deep learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: High-level definitions of fundamental concepts
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本概念的高级定义
- en: Timeline of the development of machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习发展的时间线
- en: Key factors behind deep learning’s rising popularity and future potential
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习日益普及和未来潜力背后的关键因素
- en: 'In the past few years, artificial intelligence (AI) has been a subject of intense
    media hype. Machine learning, deep learning, and AI come up in countless articles,
    often outside of technology-minded publications. We’re promised a future of intelligent
    chatbots, self-driving cars, and virtual assistants—a future sometimes painted
    in a grim light and other times as utopian, where human jobs will be scarce and
    most economic activity will be handled by robots or AI agents. For a future or
    current practitioner of machine learning, it’s important to be able to recognize
    the signal amid the noise, so that you can tell world-changing developments from
    overhyped press releases. Our future is at stake, and it’s a future in which you
    have an active role to play: after reading this book, you’ll be one of those who
    develop those AI systems. So let’s tackle these questions: What has deep learning
    achieved so far? How significant is it? Where are we headed next? Should you believe
    the hype?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，人工智能（AI）一直是媒体炒作的对象。机器学习、深度学习和人工智能在无数文章中出现，通常是在技术类出版物之外。我们被承诺一个智能聊天机器人、自动驾驶汽车和虚拟助手的未来——有时被描绘成一个阴暗的未来，有时被描绘成乌托邦，人类的工作将变得稀缺，大部分经济活动将由机器人或人工智能代理处理。对于一个未来或现在从事机器学习的从业者来说，能够辨别出噪音中的信号是很重要的，这样你就可以从被炒作的新闻稿中找出改变世界的发展。我们的未来岌岌可危，这是一个你有积极参与的未来：阅读完本书后，你将成为那些开发这些人工智能系统的人之一。所以让我们来解决这些问题：深度学习到目前为止取得了什么成就？它有多重要？我们接下来将走向何方？你应该相信这种炒作吗？
- en: This chapter provides essential context around artificial intelligence, machine
    learning, and deep learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了围绕人工智能、机器学习和深度学习的基本背景。
- en: 1.1 Artificial intelligence, machine learning, and deep learning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 人工智能、机器学习和深度学习
- en: First, we need to define clearly what we’re talking about when we mention AI.
    What are artificial intelligence, machine learning, and deep learning (see figure
    1.1)? How do they relate to each other?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当我们提到人工智能时，我们需要清楚地定义我们所讨论的内容。人工智能、机器学习和深度学习是什么（见图1.1）？它们之间的关系是怎样的？
- en: '![](../Images/01-01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-01.png)'
- en: Figure 1.1 Artificial intelligence, machine learning, and deep learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 人工智能、机器学习和深度学习
- en: 1.1.1 Artificial intelligence
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 人工智能
- en: Artificial intelligence was born in the 1950s, when a handful of pioneers from
    the nascent field of computer science started asking whether computers could be
    made to “think”—a question whose ramifications we’re still exploring today.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能诞生于20世纪50年代，当时一小群计算机科学领域的先驱开始思考计算机是否能够“思考”——这个问题的影响至今仍在探索中。
- en: 'While many of the underlying ideas had been brewing in the years and even decades
    prior, “artificial intelligence” finally crystallized as a field of research in
    1956, when John McCarthy, then a young Assistant Professor of Mathematics at Dartmouth
    College, organized a summer workshop under the following proposal:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多潜在的想法在之前的几年甚至几十年中一直在酝酿，“人工智能”最终在1956年作为一个研究领域得以凝结，当时约翰·麦卡锡（John McCarthy）在达特茅斯学院（Dartmouth
    College）担任年轻的数学助理教授，组织了一个夏季研讨会，提出了以下建议：
- en: '*The study is to proceed on the basis of the conjecture that every aspect of
    learning or any other feature of intelligence can in principle be so precisely
    described that a machine can be made to simulate it. An attempt will be made to
    find how to make machines use language, form abstractions and concepts, solve
    kinds of problems now reserved for humans, and improve themselves. We think that
    a significant advance can be made in one or more of these problems if a carefully
    selected group of scientists work on it together for a summer*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项研究的基础是一个假设，即学习的每个方面或智能的任何其他特征原则上都可以被描述得如此精确，以至于可以制造一台机器来模拟它。我们将尝试找出如何使机器使用语言，形成抽象和概念，解决目前仅保留给人类的问题，并改进自己。我们认为，如果一组精心挑选的科学家们在一起为此工作一个夏天，就可以在这些问题中的一个或多个问题上取得重大进展*。'
- en: At the end of the summer, the workshop concluded without having fully solved
    the riddle it set out to investigate. Nevertheless, it was attended by many people
    who would move on to become pioneers in the field, and it set in motion an intellectual
    revolution that is still ongoing to this day.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 夏天结束时，研讨会没有完全解决它旨在调查的谜团。然而，许多参与者后来成为该领域的先驱，并引发了一场至今仍在进行的知识革命。
- en: Concisely, AI can be described as *the effort to automate intellectual tasks
    normally performed by humans*. As such, AI is a general field that encompasses
    machine learning and deep learning, but that also includes many more approaches
    that may not involve any learning. Consider that until the 1980s, most AI textbooks
    didn’t mention “learning” at all! Early chess programs, for instance, only involved
    hardcoded rules crafted by programmers, and didn’t qualify as machine learning.
    In fact, for a fairly long time, most experts believed that human-level artificial
    intelligence could be achieved by having programmers handcraft a sufficiently
    large set of explicit rules for manipulating knowledge stored in explicit databases.
    This approach is known as *symbolic AI*. It was the dominant paradigm in AI from
    the 1950s to the late 1980s, and it reached its peak popularity during the *expert
    systems* boom of the 1980s.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，人工智能可以被描述为*自动执行人类通常执行的智力任务的努力*。因此，人工智能是一个涵盖机器学习和深度学习的广泛领域，但也包括许多不涉及任何学习的方法。考虑到直到1980年代，大多数人工智能教科书根本没有提到“学习”！例如，早期的下棋程序只涉及由程序员精心制作的硬编码规则，并不符合机器学习的条件。事实上，相当长一段时间，大多数专家认为，通过让程序员手工制作足够大量的明确规则来操作存储在明确数据库中的知识，就可以实现人类水平的人工智能。这种方法被称���*符号人工智能*。它是从1950年代到1980年代末的人工智能中的主导范式，并在1980年代的*专家系统*繁荣期达到了其最高流行度。
- en: 'Although symbolic AI proved suitable to solve well-defined, logical problems,
    such as playing chess, it turned out to be intractable to figure out explicit
    rules for solving more complex, fuzzy problems, such as image classification,
    speech recognition, or natural language translation. A new approach arose to take
    symbolic AI’s place: *machine learning*.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管符号人工智能适用于解决定义明确的逻辑问题，例如下棋，但发现解决更复杂、模糊问题的明确规则是困难的，例如图像分类、语音识别或自然语言翻译。出现了一种新的方法来取代符号人工智能：*机器学习*。
- en: 1.1.2 Machine learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 机器学习
- en: 'In Victorian England, Lady Ada Lovelace was a friend and collaborator of Charles
    Babbage, the inventor of the *Analytical Engine*: the first-known general-purpose
    mechanical computer. Although visionary and far ahead of its time, the Analytical
    Engine wasn’t meant as a general-purpose computer when it was designed in the
    1830s and 1840s, because the concept of general-purpose computation was yet to
    be invented. It was merely meant as a way to use mechanical operations to automate
    certain computations from the field of mathematical analysis—hence the name Analytical
    Engine. As such, it was the intellectual descendant of earlier attempts at encoding
    mathematical operations in gear form, such as the Pascaline, or Leibniz’s step
    reckoner, a refined version of the Pascaline. Designed by Blaise Pascal in 1642
    (at age 19!), the Pascaline was the world’s first mechanical calculator—it could
    add, subtract, multiply, or even divide digits.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在维多利亚时代的英格兰，艾达·洛夫莱斯夫人是查尔斯·巴贝奇的朋友和合作者，他是第一台已知的通用机械计算机——分析引擎的发明者。尽管分析引擎具有远见卓识，超前于其时代，但在1830年代和1840年代设计时，并不是作为通用计算机，因为通用计算的概念尚未被发明。它只是作为一种使用机械操作来自动执行数学分析领域中某些计算的方式——因此得名为分析引擎。因此，它是早期尝试将数学运算编码为齿轮形式的智力后代，例如帕斯卡计算器或莱布尼茨的步进计算器，后者是帕斯卡计算器的改进版本。由布莱斯·帕斯卡于1642年（19岁时！）设计，帕斯卡计算器是世界上第一台机械计算器——它可以加法、减法、乘法，甚至除法。
- en: In 1843, Ada Lovelace remarked on the invention of the Analytical Engine,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1843年，艾达·洛夫莱斯评论了分析引擎的发明，
- en: '*The Analytical Engine has no pretensions whatever to originate anything. It
    can do whatever we know how to order it to perform. . . . Its province is to assist
    us in making available what we’re already acquainted with*.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*分析引擎根本没有创造任何东西的意图。它只能执行我们知道如何命令它执行的任务……它的职责是帮助我们利用我们已经熟悉的东西*。'
- en: Even with 178 years of historical perspective, Lady Lovelace’s observation remains
    arresting. Could a general-purpose computer “originate” anything, or would it
    always be bound to dully execute processes we humans fully understand? Could it
    ever be capable of any original thought? Could it learn from experience? Could
    it show creativity?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有着178年的历史视角，洛夫莱斯夫人的观察仍然令人震撼。通用计算机是否能“创造”任何东西，或者它是否总是被束缚在我们人类完全理解的过程中？它是否能够产生任何原创思想？它是否能够从经验中学习？它是否能展现创造力？
- en: Her remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objection”
    in his landmark 1950 paper “Computing Machinery and Intelligence,”[¹](../Text/01.htm#pgfId-1012032)
    which introduced the *Turing test* as well as key concepts that would come to
    shape AI.[²](../Text/01.htm#pgfId-1012070) Turing was of the opinion—highly provocative
    at the time—that computers could in principle be made to emulate all aspects of
    human intelligence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 她的言论后来被人工智能先驱艾伦·图灵在他1950年的里程碑论文“计算机与智能”中引用为“洛夫莱斯夫人的反对意见”，该论文引入了图灵测试以及后来塑造人工智能的关键概念。图灵当时认为——这在当时是极具挑衅性的——计算机原则上可以模拟人类智能的所有方面。
- en: 'The usual way to make a computer do useful work is to have a human programmer
    write down *rules*—a computer program—to be followed to turn input data into appropriate
    answers, just like Lady Lovelace writing down step-by-step instructions for the
    Analytical Engine to perform. Machine learning turns this around: the machine
    looks at the input data and the corresponding answers, and figures out what the
    rules should be (see figure 1.2). A machine learning system is *trained* rather
    than explicitly programmed. It’s presented with many examples relevant to a task,
    and it finds statistical structure in these examples that eventually allows the
    system to come up with rules for automating the task. For instance, if you wished
    to automate the task of tagging your vacation pictures, you could present a machine
    learning system with many examples of pictures already tagged by humans, and the
    system would learn statistical rules for associating specific pictures to specific
    tags.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让计算机执行有用工作的通常方法是让人类程序员编写*规则*——一个计算机程序——以将输入数据转换为适当的答案，就像洛夫莱斯夫人为分析引擎编写逐步指令一样。机器学习将这个过程颠倒过来：机器查看输入数据和相应的答案，并找出规则应该是什么（见图1.2）。机器学习系统是*训练*而不是明���编程的。它被呈现许多与任务相关的示例，并在这些示例中找到统计结构，最终使系统能够提出自动化任务的规则。例如，如果您希望自动化标记您的度假照片的任务，您可以向机器学习系统提供许多已由人类标记的图片示例，系统将学习将特定图片与特定标签相关联的统计规则。
- en: '![](../Images/01-02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-02.png)'
- en: 'Figure 1.2 Machine learning: a new programming paradigm'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 机器学习：一种新的编程范式
- en: Although machine learning only started to flourish in the 1990s, it has quickly
    become the most popular and most successful subfield of AI, a trend driven by
    the availability of faster hardware and larger datasets. Machine learning is related
    to mathematical statistics, but it differs from statistics in several important
    ways, in the same sense that medicine is related to chemistry but cannot be reduced
    to chemistry, as medicine deals with its own distinct systems with their own distinct
    properties. Unlike statistics, machine learning tends to deal with large, complex
    datasets (such as a dataset of millions of images, each consisting of tens of
    thousands of pixels) for which classical statistical analysis such as Bayesian
    analysis would be impractical. As a result, machine learning, and especially deep
    learning, exhibits comparatively little mathematical theory—maybe too little—and
    is fundamentally an engineering discipline. Unlike theoretical physics or mathematics,
    machine learning is a very hands-on field driven by empirical findings and deeply
    reliant on advances in software and hardware.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习在1990年代才开始蓬勃发展，但它迅速成为人工智能中最受欢迎和最成功的子领域，这一趋势受到更快硬件和更大数据集的推动。机器学习与数理统计有关，但在几个重要方面与统计学不同，就像医学与化学有关但不能简化为化学一样，因为医学处理具有独特属性的独特系统。与统计学不同，机器学习往往处理大型、复杂的数据集（例如包含数百万图像的数据集，每个图像由数万像素组成），传统的统计分析如贝叶斯分析在这种情况下将不切实际。因此，机器学习，尤其是深度学习，展示了相对较少的数学理论——也许太少了——并且基本上是一门工程学科。与理论物理或数学不同，机器学习是一个非常实践的领域，受到经验发现的驱动，并且深度依赖于软件和硬件的进步。
- en: 1.1.3 Learning rules and representations from data
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 从数据中学习规则和表示
- en: 'To define *deep learning* and understand the difference between deep learning
    and other machine learning approaches, first we need some idea of what machine
    learning algorithms do. We just stated that machine learning discovers rules for
    executing a data processing task, given examples of what’s expected. So, to do
    machine learning, we need three things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义*深度学习*并了解深度学习与其他机器学习方法的区别，首先我们需要对机器学习算法的工作原理有一些了解。我们刚刚说过，机器学习发现执行数据处理任务的规则，给定预期的示例。因此，要进行机器学习，我们需要三样东西：
- en: '*Input data points*—For instance, if the task is speech recognition, these
    data points could be sound files of people speaking. If the task is image tagging,
    they could be pictures.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入数据点*——例如，如果任务是语音识别，这些数据点可以是人们说话的声音文件。如果任务是图像标记，它们可以是图片。'
- en: '*Examples of the expected output*—In a speech-recognition task, these could
    be human-generated transcripts of sound files. In an image task, expected outputs
    could be tags such as “dog,” “cat,” and so on.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预期输出的示例*——在语音识别任务中，这些可以是人类生成的声音文件转录。在图像任务中，预期输出可以是“狗”、“猫”等标签。'
- en: '*A way to measure whether the algorithm is doing a good job*—This is necessary
    in order to determine the distance between the algorithm’s current output and
    its expected output. The measurement is used as a feedback signal to adjust the
    way the algorithm works. This adjustment step is what we call *learning*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*衡量算法表现的方法*——这是为了确定算法当前输出与预期输出之间的距离。该测量用作反馈信号，以调整算法的工作方式。这个调整步骤就是我们所说的*学习*。'
- en: 'A machine learning model transforms its input data into meaningful outputs,
    a process that is “learned” from exposure to known examples of inputs and outputs.
    Therefore, the central problem in machine learning and deep learning is to *meaningfully
    transform data*: in other words, to learn useful *representations* of the input
    data at hand—representations that get us closer to the expected output.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型将其输入数据转换为有意义的输出，这个过程是从已知输入和输出示例中“学习”的。因此，机器学习和深度学习的核心问题是*有意义地转换数据*：换句话说，学习输入数据的有用*表示*——这些表示使我们更接近预期的输出。
- en: 'Before we go any further: what’s a representation? At its core, it’s a different
    way to look at data—to represent or encode data. For instance, a color image can
    be encoded in the RGB format (red-green-blue) or in the HSV format (hue-saturation-value):
    these are two different representations of the same data. Some tasks that may
    be difficult with one representation can become easy with another. For example,
    the task “select all red pixels in the image” is simpler in the RGB format, whereas
    “make the image less saturated” is simpler in the HSV format. Machine learning
    models are all about finding appropriate representations for their input data—transformations
    of the data that make it more amenable to the task at hand.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前：什么是表示？在其核心，它是查看数据的不同方式——表示或编码数据。例如，彩色图像可以用RGB格式（红-绿-蓝）或HSV格式（色调-饱和度-值）编码：这是相同数据的两种不同表示。一些在一种表示中可能困难的任务，在另一种表示中可能变得简单。例如，“选择图像中的所有红色像素”任务在RGB格式中更简单，而“使图像饱和度降低”在HSV格式中更简单。机器学习模型的全部内容都是找到适合其输入数据的适当表示——使数据更适合手头任务的转换。
- en: Let’s make this concrete. Consider an *x*-axis, a *y*-axis, and some points
    represented by their coordinates in the (*x*, *y*) system, as shown in figure
    1.3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们具体化一下。考虑一个*x*轴，一个*y*轴，以及一些通过它们在(*x*, *y*)系统中的坐标表示的点，如图1.3所示。
- en: '![](../Images/01-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-03.png)'
- en: Figure 1.3 Some sample data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 一些示例数据
- en: As you can see, we have a few white points and a few black points. Let’s say
    we want to develop an algorithm that can take the coordinates (*x*, *y*) of a
    point and output whether that point is likely to be black or to be white. In this
    case,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有一些白点和一些黑点。假设我们想开发一个算法，可以接受一个点的坐标(*x*, *y*)并输出该点可能是黑色还是白色。在这种情况下，
- en: The inputs are the coordinates of our points.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是我们点的坐标。
- en: The expected outputs are the colors of our points.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期的输出是我们点的颜色。
- en: A way to measure whether our algorithm is doing a good job could be, for instance,
    the percentage of points that are being correctly classified.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量我们的算法是否做得好的一种方法可能是，例如，被正确分类的点的百分比。
- en: What we need here is a new representation of our data that cleanly separates
    the white points from the black points. One transformation we could use, among
    many other possibilities, would be a coordinate change, illustrated in figure
    1.4.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是我们数据的一个新表示，清晰地将白点与黑点分开。我们可以使用的一种转换，除了许多其他可能性之外，是一个坐标变换，如图1.4所示。
- en: '![](../Images/01-04.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-04.png)'
- en: Figure 1.4 Coordinate change
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 坐标变换
- en: 'In this new coordinate system, the coordinates of our points can be said to
    be a new representation of our data. And it’s a good one! With this representation,
    the black/white classification problem can be expressed as a simple rule: “Black
    points are such that *x* > 0,” or “White points are such that *x* < 0.” This new
    representation, combined with this simple rule, neatly solves the classification
    problem.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的坐标系中，我们点的坐标可以说是我们数据的新表示。而且这是一个好的表示！通过这个表示，黑/白分类问题可以表达为一个简单的规则：“黑点是那些*x*>0的点”，或者“白点是那些*x*<0的点”。这个新表示，结合这个简单规则，巧妙地解决了分类问题。
- en: 'In this case we defined the coordinate change by hand: we used our human intelligence
    to come up with our own appropriate representation of the data. This is fine for
    such an extremely simple problem, but could you do the same if the task were to
    classify images of handwritten digits? Could you write down explicit, computer-executable
    image transformations that would illuminate the difference between a 6 and an
    8, between a 1 and a 7, across all kinds of different handwriting?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们手动定义了坐标变换：我们利用我们的人类智慧提出了我们自己的数据适当表示。对于这样一个极其简单的问题来说这是可以的，但是如果任务是分类手写数字的图像，你能做到同样吗？你能写出明确的、可由计算机执行的图像转换，以阐明6和8之间的差异，1和7之间的差异，以及各种不同手写之间的差异吗？
- en: This is possible to an extent. Rules based on representations of digits such
    as “number of closed loops” or vertical and horizontal pixel histograms can do
    a decent job of telling apart handwritten digits. But finding such useful representations
    by hand is hard work, and, as you can imagine, the resulting rule-based system
    is brittle—a nightmare to maintain. Every time you come across a new example of
    handwriting that breaks your carefully thought-out rules, you will have to add
    new data transformations and new rules, while taking into account their interaction
    with every previous rule.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一定程度上是可能的。基于数字表示的规则，比如“闭环数量”或者垂直和水平像素直方图，可以很好地区分手写数字。但是手动找到这样有用的表示是一项艰苦的工作，而且，正如你可以想象的，由此产生的基于规则的系统是脆弱的——难以维护的噩梦。每当你遇到一个打破你精心考虑的规则的新手写示例时，你将不得不添加新的数据转换和新的规则，同时考虑它们与每个先前规则的互动。
- en: You’re probably thinking, if this process is so painful, could we automate it?
    What if we tried systematically searching for different sets of automatically
    generated representations of the data and rules based on them, identifying good
    ones by using as feedback the percentage of digits being correctly classified
    in some development dataset? We would then be doing machine learning. *Learning*,
    in the context of machine learning, describes an automatic search process for
    data transformations that produce useful representations of some data, guided
    by some feedback signal—representations that are amenable to simpler rules solving
    the task at hand.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想，如果这个过程如此痛苦，我们能自动化吗？如果我们尝试系统地搜索不同集合的自动生成的数据表示和基于它们的规则，通过使用一些开发数据集中被正确分类的数字的百分比作为反馈来识别好的表示，我们将会进行机器学习。*学习*，在机器学习的背景下，描述了一种自动搜索数据转换的过程，产生一些有用的数据表示，通过一些反馈信号引导——这些表示适合于解决手头任务的简单规则。
- en: These transformations can be coordinate changes (like in our 2D coordinates
    classification example), or taking a histogram of pixels and counting loops (like
    in our digits classification example), but they could also be linear projections,
    translations, nonlinear operations (such as “select all points such that *x* >
    0”), and so on. Machine learning algorithms aren’t usually creative in finding
    these transformations; they’re merely searching through a predefined set of operations,
    called a *hypothesis space*. For instance, the space of all possible coordinate
    changes would be our hypothesis space in the 2D coordinates classification example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换可以是坐标变换（就像我们的2D坐标分类示例中），或者是取像素直方图并计算循环次数（就像我们的数字分类示例中），但它们也可以是线性投影、平移、非线性操作（比如“选择所有*
    x *> 0的点”）等。机器学习算法通常不会在发现这些转换时具有创造性；它们只是在预定义的一组操作中搜索，称为*假设空间*。例如，在2D坐标分类示例中，所有可能的坐标变换空间将是我们的假设空间。
- en: 'So that’s what machine learning is, concisely: searching for useful representations
    and rules over some input data, within a predefined space of possibilities, using
    guidance from a feedback signal. This simple idea allows for solving a remarkably
    broad range of intellectual tasks, from speech recognition to autonomous driving.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，简洁地说，机器学习就是在预定义的可能性空间内，通过反馈信号的指导，搜索一些输入数据的有用表示和规则。这个简单的想法允许解决一系列广泛��智力任务，从语音识别到自动驾驶。
- en: Now that you understand what we mean by *learning*, let’s take a look at what
    makes *deep learning* special.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了我们所说的*学习*是什么意思，让我们看看*深度学习*有什么特别之处。
- en: 1.1.4 The “deep” in “deep learning”
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.4 “深度学习”中的“深度”
- en: 'Deep learning is a specific subfield of machine learning: a new take on learning
    representations from data that puts an emphasis on learning successive layers
    of increasingly meaningful representations. The “deep” in “deep learning” isn’t
    a reference to any kind of deeper understanding achieved by the approach; rather,
    it stands for this idea of successive layers of representations. How many layers
    contribute to a model of the data is called the *depth* of the model. Other appropriate
    names for the field could have been *layered representations learning* or *hierarchical
    representations learning*. Modern deep learning often involves tens or even hundreds
    of successive layers of representations, and they’re all learned automatically
    from exposure to training data. Meanwhile, other approaches to machine learning
    tend to focus on learning only one or two layers of representations of the data
    (say, taking a pixel histogram and then applying a classification rule); hence,
    they’re sometimes called *shallow learning*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个特定子领域：一种从数据中学习表示的新方法，强调学习逐渐具有意义的表示的连续层。 “深度学习”中的“深度”并不是指这种方法所达到的更深层次的理解；相反，它代表了这种连续表示层的概念。对数据模型有多少层贡献被称为模型的*深度*。该领域的其他适当名称可能是*分层表示学习*或*层次表示学习*。现代深度学习通常涉及数十甚至数百个连续的表示层，它们都是通过暴露于训练数据中自动学习的。与此同时，其他机器学习方法往往专注于学习数据的一两层表示（比如，获取像素直方图然后应用分类规则）；因此，它们有时被称为*浅层学习*。
- en: In deep learning, these layered representations are learned via models called
    *neural networks*, structured in literal layers stacked on top of each other.
    The term “neural network” refers to neurobiology, but although some of the central
    concepts in deep learning were developed in part by drawing inspiration from our
    understanding of the brain (in particular, the visual cortex), deep learning models
    are not models of the brain. There’s no evidence that the brain implements anything
    like the learning mechanisms used in modern deep learning models. You may come
    across pop-science articles proclaiming that deep learning works like the brain
    or was modeled after the brain, but that isn’t the case. It would be confusing
    and counterproductive for newcomers to the field to think of deep learning as
    being in any way related to neurobiology; you don’t need that shroud of “just
    like our minds” mystique and mystery, and you may as well forget anything you
    may have read about hypothetical links between deep learning and biology. For
    our purposes, deep learning is a mathematical framework for learning representations
    from data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，这些分层表示是通过称为*神经网络*的模型学习的，这些模型以字面意义上相互堆叠的层结构。术语“神经网络”指的是神经生物学，但尽管深度学习中的一些核心概念部分是通过从我们对大脑的理解中汲取灵感而发展的（特别是视觉皮层），深度学习模型并不是大脑的模型。没有证据表明大脑实现了任何类似于现代深度学习模型中使用的学习机制。你可能会看到一些流行科学文章宣称深度学习如何像大脑工作或是模仿大脑，但事实并非如此。对于新手来说，将深度学习与神经生物学有任何关联是令人困惑和适得其反的；你不需要那种“就像我们的思维一样”的神秘感和神秘性，你可能会忘记任何你可能读到的关于深度学习和生物学之间假设联系的内容。对于我们的目的，深度学习是从数据中学习表示的数学框架。
- en: What do the representations learned by a deep learning algorithm look like?
    Let’s examine how a network several layers deep (see figure 1.5) transforms an
    image of a digit in order to recognize what digit it is.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法学习的表示是什么样子的？让我们看看一个几层深的网络（见图1.5）如何转换图像以识别出是哪个数字。
- en: '![](../Images/01-05.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-05.png)'
- en: Figure 1.5 A deep neural network for digit classification
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 数字分类的深度神经网络
- en: As you can see in figure 1.6, the network transforms the digit image into representations
    that are increasingly different from the original image and increasingly informative
    about the final result. You can think of a deep network as a multistage *information-distillation*
    process, where information goes through successive filters and comes out increasingly
    *purified* (that is, useful with regard to some task).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图1.6中看到的，网络将数字图像转换为与原始图像越来越不同且越来越有关于最终结果的信息的表示。你可以将深度网络看作是一个多阶段的*信息提炼*过程，其中信息通过连续的滤波器并最终*纯化*（即，对某个任务有用）出来。
- en: '![](../Images/01-06.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-06.png)'
- en: Figure 1.6 Data representations learned by a digit-classification model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 由数字分类模型学习到的数据表示
- en: 'So that’s what deep learning is, technically: a multistage way to learn data
    representations. It’s a simple idea—but, as it turns out, very simple mechanisms,
    sufficiently scaled, can end up looking like magic.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从技术上讲，这就是深度学习：学习数据表示的多阶段方式。这是一个简单的想法—但事实证明，非常简单的机制，足够扩展，最终看起来像魔术。
- en: 1.1.5 Understanding how deep learning works, in three figures
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.5 通过三个图形理解深度学习的工作原理
- en: At this point, you know that machine learning is about mapping inputs (such
    as images) to targets (such as the label “cat”), which is done by observing many
    examples of input and targets. You also know that deep neural networks do this
    input-to-target mapping via a deep sequence of simple data transformations (layers)
    and that these data transformations are learned by exposure to examples. Now let’s
    look at how this learning happens, concretely.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你知道机器学习是关于将输入（如图像）映射到目标（如标签“猫”），这是通过观察许多输入和目标示例来完成的。你还知道深度神经网络通过一系列简单的数据转换（层）的深度序列来实现这种输入到目标的映射，并且这些数据转换是通过示例学习的。现在让我们具体看看这个学习是如何进行的。
- en: 'The specification of what a layer does to its input data is stored in the layer’s
    *weights*, which in essence are a bunch of numbers. In technical terms, we’d say
    that the transformation implemented by a layer is *parameterized* by its weights
    (see figure 1.7). (Weights are also sometimes called the *parameters* of a layer.)
    In this context, *learning* means finding a set of values for the weights of all
    layers in a network, such that the network will correctly map example inputs to
    their associated targets. But here’s the thing: a deep neural network can contain
    tens of millions of parameters. Finding the correct values for all of them may
    seem like a daunting task, especially given that modifying the value of one parameter
    will affect the behavior of all the others!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 层对输入数据执行的操作规范存储在层的*权重*中，本质上是一堆数字。在技术术语中，我们会说层实现的转换是由其权重*参数化*的（见图1.7）。（权重有时也称为层的*参数*。）在这个背景下，*学习*意味着找到网络中所有层的权重的一组值，使网络能够正确地将示例输入映射到它们关联的目标。但问题在于：一个深度神经网络可能包含数千万个参数。找到所有这些参数的正确值可能看起来像一项艰巨的任务，特别是考虑到修改一个参数的值将影响所有其他参数的行为！
- en: '![](../Images/01-07.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-07.png)'
- en: Figure 1.7 A neural network is parameterized by its weights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 一个神经网络由其权重参数化。
- en: To control something, first you need to be able to observe it. To control the
    output of a neural network, you need to be able to measure how far this output
    is from what you expected. This is the job of the *loss function* of the network,
    also sometimes called the *objective function* or *cost function*. The loss function
    takes the predictions of the network and the true target (what you wanted the
    network to output) and computes a distance score, capturing how well the network
    has done on this specific example (see figure 1.8).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制某物，首先你需要能够观察它。要控制神经网络的输出，你需要能够衡量这个输出与你期望的结果有多大差距。这就是网络的*损失函数*的工作，有时也称为*目标函数*或*成本函数*。损失函数接受网络的预测和真实目标（你希望网络输出的内容）并计算一个距离分数，捕捉网络在这个特定示例上表现如何（见图1.8）。
- en: '![](../Images/01-08.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-08.png)'
- en: Figure 1.8 A loss function measures the quality of the network’s output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 损失函数衡量网络输出的质量。
- en: 'The fundamental trick in deep learning is to use this score as a feedback signal
    to adjust the value of the weights a little, in a direction that will lower the
    loss score for the current example (see figure 1.9). This adjustment is the job
    of the *optimizer*, which implements what’s called the *Backpropagation* algorithm:
    the central algorithm in deep learning. The next chapter explains in more detail
    how backpropagation works.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的基本技巧是使用这个分数作为反馈信号，稍微调整权重的值，以降低当前示例的损失分数（见图1.9）。这种调整是*优化器*的工作，它实现了所谓的*反向传播*算法：深度学习中的核心算法。下一章将更详细地解释反向传播的工作原理。
- en: '![](../Images/01-09.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-09.png)'
- en: Figure 1.9 The loss score is used as a feedback signal to adjust the weights.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 损失分数被用作反馈信号来调整权重。
- en: 'Initially, the weights of the network are assigned random values, so the network
    merely implements a series of random transformations. Naturally, its output is
    far from what it should ideally be, and the loss score is accordingly very high.
    But with every example the network processes, the weights are adjusted a little
    in the correct direction, and the loss score decreases. This is the *training
    loop*, which, repeated a sufficient number of times (typically tens of iterations
    over thousands of examples), yields weight values that minimize the loss function.
    A network with a minimal loss is one for which the outputs are as close as they
    can be to the targets: a trained network. Once again, it’s a simple mechanism
    that, once scaled, ends up looking like magic.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，网络的权重被分配随机值，因此网络仅实现一系列随机转换。自然地，它的输出与理想情况相去甚远，损失分数相应地非常高。但随着网络处理每个示例，权重会稍微朝正确方向调整，损失分数会减少。这就是*训练循环*，重复足够多次（通常是数千个示例的数十次迭代），得到最小化损失函数的权重值。具有最小损失的网络是输出尽可能接近目标的网络：一个经过训练的网络。再一次，这是一个简单的机制，一旦扩展，最终看起来像魔术。
- en: 1.1.6 What deep learning has achieved so far
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.6 到目前为止深度学习取得了什么成就
- en: Although deep learning is a fairly old subfield of machine learning, it only
    rose to prominence in the early 2010s. In the few years since, it has achieved
    nothing short of a revolution in the field, producing remarkable results on perceptual
    tasks and even natural language processing tasks—problems involving skills that
    seem natural and intuitive to humans but have long been elusive for machines.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习是机器学习的一个相当古老的子领域，但直到2010年代初才开始崭露头角。在此后的几年里，它在领域中取得了一场革命性的成就，对感知任务甚至自然语言处理任务产生了显著的结果——这些问题涉及到对人类似乎自然和直观的技能，但长期以来一直是机器难以掌握的。
- en: 'In particular, deep learning has enabled the following breakthroughs, all in
    historically difficult areas of machine learning:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，深度学习已经在历史上困难的机器学习领域实现了以下突破：
- en: Near-human-level image classification
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的图像分类
- en: Near-human-level speech transcription
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的语音转录
- en: Near-human-level handwriting transcription
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的手写转录
- en: Dramatically improved machine translation
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大幅改进的机器翻译
- en: Dramatically improved text-to-speech conversion
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大幅改进的文本转语音转换
- en: Digital assistants such as Google Assistant and Amazon Alexa
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如Google Assistant和Amazon Alexa等数字助手
- en: Near-human-level autonomous driving
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的自动驾驶
- en: Improved ad targeting, as used by Google, Baidu, or Bing
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的广告定位，如Google、百度或必应所使用的
- en: Improved search results on the web
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络搜索结果的改进
- en: Ability to answer natural language questions
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够回答自然语言问题
- en: Superhuman Go playing
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超人类水平的围棋对弈
- en: We’re still exploring the full extent of what deep learning can do. We’ve started
    applying it with great success to a wide variety of problems that were thought
    to be impossible to solve just a few years ago—automatically transcribing the
    tens of thousands of ancient manuscripts held in the Vatican’s Apostolic Archive,
    detecting and classifying plant diseases in fields using a simple smartphone,
    assisting oncologists or radiologists with interpreting medical imaging data,
    predicting natural disasters such as floods, hurricanes, or even earthquakes,
    and so on. With every milestone, we’re getting closer to an age where deep learning
    assists us in every activity and every field of human endeavor—science, medicine,
    manufacturing, energy, transportation, software development, agriculture, and
    even artistic creation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍在探索深度学习的全部潜力。我们已经开始将其成功应用于一系列几年前被认为无法解决的问题——自动转录梵蒂冈宗座档案馆中���存的数以万计的古代手稿，使用简单的智能手机检测和分类田间植物疾病，协助肿瘤学家或放射科医生解释医学成像数据，预测洪水、飓风甚至地震等自然灾害等。随着每一个里程碑的实现，我们越来越接近一个时代，在这个时代，深度学习将在人类的每一个活动和领域中协助我们——科学、医学、制造业、能源、交通、软件开发、农业，甚至艺术创作。
- en: 1.1.7 Don’t believe the short-term hype
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.7 不要相信短期炒作
- en: Although deep learning has led to remarkable achievements in recent years, expectations
    for what the field will be able to achieve in the next decade tend to run much
    higher than what will likely be possible. Although some world-changing applications
    like autonomous cars are already within reach, many more are likely to remain
    elusive for a long time, such as believable dialogue systems, human-level machine
    translation across arbitrary languages, and human-level natural language understanding.
    In particular, talk of human-level general intelligence shouldn’t be taken too
    seriously. The risk with high expectations for the short term is that, as technology
    fails to deliver, research investment will dry up, slowing progress for a long
    time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在近年取得了显著的成就，对于该领域在未来十年能够实现的期望往往比实际可能实现的要高得多。尽管一些像自动驾驶汽车这样改变世界的应用已经近在眼前，但许多其他应用可能在很长一段时间内仍然难以实现，比如可信的对话系统、跨任意语言的人类级机器翻译以及人类级自然语言理解。特别是，对于短期内实现人类级通用智能的讨论不应该太过认真。对于短期内高期望的风险在于，随着技术无法交付，研究投资将枯竭，长期减缓进展。
- en: 'This has happened before. Twice in the past, AI went through a cycle of intense
    optimism followed by disappointment and skepticism, with a dearth of funding as
    a result. It started with symbolic AI in the 1960s. In those early days, projections
    about AI were flying high. One of the best-known pioneers and proponents of the
    symbolic AI approach was Marvin Minsky, who claimed in 1967, “Within a generation
    . . . the problem of creating ‘artificial intelligence’ will substantially be
    solved.” Three years later, in 1970, he made a more precisely quantified prediction:
    “In from three to eight years we will have a machine with the general intelligence
    of an average human being.” In 2021 such an achievement still appears to be far
    in the future—so far that we have no way to predict how long it will take—but
    in the 1960s and early 1970s, several experts believed it to be right around the
    corner (as do many people today). A few years later, as these high expectations
    failed to materialize, researchers and government funds turned away from the field,
    marking the start of the first *AI winter* (a reference to a nuclear winter, because
    this was shortly after the height of the Cold War).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况已经发生过。在过去的两次中，人工智能经历了一轮强烈的乐观主义，随后是失望和怀疑，导致资金短缺。它始于20世纪60年代的符号人工智能。在那些早期，关于人工智能的预测飞得很高。符号人工智能方法中最著名的先驱和支持者之一是马文·明斯基，他在1967年声称：“在一代人内……创造‘人工智能’的问题将得到实质性解决。”三年后，即1970年，他做出了更为精确的预测：“在三到八年内，我们将拥有一台具有普通人类智能的机器。”到2021年，这样的成就似乎仍然遥不可及——远远超出我们无法预测需要多长时间才能实现的范围——但在20世纪60年代和70年代初，一些专家相信这一成就就在眼前（就像今天的许多人一样）。几年后，随着这些高期望未能实现，研究人员和政府资金转向其他领域，标志着第一次*人工智能寒冬*的开始（这是对核冬天的一个参考，因为这是在冷战的高峰之后不久）。
- en: It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, *expert
    systems*, started gathering steam among large companies. A few initial success
    stories triggered a wave of investment, with corporations around the world starting
    their own in-house AI departments to develop expert systems. Around 1985, companies
    were spending over $1 billion each year on the technology; but by the early 1990s,
    these systems had proven expensive to maintain, difficult to scale, and limited
    in scope, and interest died down. Thus began the second AI winter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会是最后一个。在20世纪80年代，一种新的符号人工智能*专家系统*开始在大公司中蓬勃发展。一些最初的成功案例引发了一波投资热潮，全球各地的公司开始建立自己的内部人工智能部门来开发专家系统。到1985年左右，公司每年在这项技术上的支出超过10亿美元；但到了90年代初，这些系统已经被证明难以维护、难以扩展和范围有限，兴趣逐渐消退。于是第二次人工智能寒冬开始了。
- en: We may be currently witnessing the third cycle of AI hype and disappointment,
    and we’re still in the phase of intense optimism. It’s best to moderate our expectations
    for the short term and make sure people less familiar with the technical side
    of the field have a clear idea of what deep learning can and can’t deliver.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能目前正在见证人工智能炒作和失望的第三个周期，我们仍处于强烈乐观的阶段。最好是对短期期望保持适度，确保对该领域技术方面不太熟悉的人清楚了解深度学习能够做什么和不能做什么。
- en: 1.1.8 The promise of AI
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.8 人工智能的承诺
- en: Although we may have unrealistic short-term expectations for AI, the long-term
    picture is looking bright. We’re only getting started in applying deep learning
    to many important problems for which it could prove transformative, from medical
    diagnoses to digital assistants. AI research has been moving forward amazingly
    quickly in the past ten years, in large part due to a level of funding never before
    seen in the short history of AI, but so far relatively little of this progress
    has made its way into the products and processes that form our world. Most of
    the research findings of deep learning aren’t yet applied, or at least are not
    applied to the full range of problems they could solve across all industries.
    Your doctor doesn’t yet use AI, and neither does your accountant. You probably
    don’t use AI technologies very often in your day-to-day life. Of course, you can
    ask your smartphone simple questions and get reasonable answers, you can get fairly
    useful product recommendations on Amazon.com, and you can search for “birthday”
    on Google Photos and instantly find those pictures of your daughter’s birthday
    party from last month. That’s a far cry from where such technologies used to stand.
    But such tools are still only accessories to our daily lives. AI has yet to transition
    to being central to the way we work, think, and live.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能对人工智能有着不切实际的短期期望，但长期前景看起来是光明的。我们才刚刚开始将深度学习应用于许多重要问题，这些问题可能会发生转变，从医学诊断到数字助手。过去十年来，人工智能研究取得了惊人的快速进展，这在很大程度上是由于人工智能短暂历史中前所未有的资金水平，但迄今为止，这些进展中相对较少的部分已经应用到构成我们世界的产品和流程中。深度学习的大部分研究成果尚未得到应用，或者至少没有应用到它们可以解决的所有行业的所有问题范围内。你的医生还没有使用人工智能，你的会计师也没有。在日常生活中，你可能并不经常使用人工智能技术。当然，你可以向智能手机提出简单问题并得到合理的答案，你可以在Amazon.com上获得相当有用的产品推荐，你可以在Google照片中搜索“生日”并立即找到上个月女儿生日派对的照片。这与这些技术过去所处的位置相去甚远。但这些工具仍然只是我们日常生活的附件。人工智能尚未过渡到成为我们工作、思考和生活方式的核心。
- en: 'Right now, it may seem hard to believe that AI could have a large impact on
    our world, because it isn’t yet widely deployed—much as, back in 1995, it would
    have been difficult to believe in the future impact of the internet. Back then,
    most people didn’t see how the internet was relevant to them and how it was going
    to change their lives. The same is true for deep learning and AI today. But make
    no mistake: AI is coming. In a not-so-distant future, AI will be your assistant,
    even your friend; it will answer your questions, help educate your kids, and watch
    over your health. It will deliver your groceries to your door and drive you from
    point A to point B. It will be your interface to an increasingly complex and information-intensive
    world. And, even more important, AI will help humanity as a whole move forward,
    by assisting human scientists in new breakthrough discoveries across all scientific
    fields, from genomics to mathematics.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，也许很难相信人工智能会对我们的世界��生巨大影响，因为它尚未被广泛应用——就像在1995年，很难相信互联网未来的影响一样。那时，大多数人并没有看到互联网对他们的相关性以及它将如何改变他们的生活。对于今天的深度学习和人工智能也是如此。但不要误解：人工智能即将到来。在不久的将来，人工智能将成为你的助手，甚至是你的朋友；它将回答你的问题，帮助教育你的孩子，并关注你的健康。它将把你的杂货送到家门口，并把你从A点开车到B点。它将成为你与日益复杂和信息密集的世界接触的接口。更重要的是，人工智能将帮助整个人类向前迈进，通过协助人类科学家在所有科学领域中进行新的突破性发现，从基因组学到数学。
- en: On the way, we may face a few setbacks and maybe even a new AI winter—in much
    the same way the internet industry was overhyped in 1998–99 and suffered from
    a crash that dried up investment throughout the early 2000s. But we’ll get there
    eventually. AI will end up being applied to nearly every process that makes up
    our society and our daily lives, much like the internet is today.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们可能会遇到一些挫折，甚至可能会迎来新的人工智能寒冬——就像互联网行业在1998-99年被过度炒作并遭受了一场导致在2000年代初期投资枯竭的崩溃一样。但我们最终会到达那里。人工智能最终将应用于构成我们社会和日常生活的几乎每一个过程，就像互联网今天一样。
- en: Don’t believe the short-term hype, but do believe in the long-term vision. It
    may take a while for AI to be deployed to its true potential—a potential the full
    extent of which no one has yet dared to dream—but AI is coming, and it will transform
    our world in a fantastic way.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不要相信短期炒作，但要相信长期愿景。人工智能可能需要一段时间才能发挥其真正潜力——一个全面程度还没有人敢于梦想的潜力——但人工智能即将到来，它将以一种奇妙的方式改变我们的世界。
- en: '1.2 Before deep learning: A brief history of machine learning'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 在深度学习之前：机器学习的简史
- en: Deep learning has reached a level of public attention and industry investment
    never before seen in the history of AI, but it isn’t the first successful form
    of machine learning. It’s safe to say that most of the machine learning algorithms
    used in the industry today aren’t deep learning algorithms. Deep learning isn’t
    always the right tool for the job—sometimes there isn’t enough data for deep learning
    to be applicable, and sometimes the problem is better solved by a different algorithm.
    If deep learning is your first contact with machine learning, you may find yourself
    in a situation where all you have is the deep learning hammer, and every machine
    learning problem starts to look like a nail. The only way not to fall into this
    trap is to be familiar with other approaches and practice them when appropriate.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经达到了公众关注和行业投资的水平，这在人工智能历史上从未见过，但它并不是机器学习的第一个成功形式。可以肯定地说，今天工业中使用的大多数机器学习算法并不是深度学习算法。深度学习并不总是解决问题的正确工具——有时候数据不足以应用深度学习，有时候问题更适合用不同的算法解决。如果深度学习是你与机器学习的第一次接触，你可能会发现自己处于这样一种情况：你手头只有深度学习的锤子，而每个机器学习问题看起来都像一个钉子。避免陷入这种陷阱的唯一方法是熟悉其他方法，并在适当时练习它们。
- en: A detailed discussion of classical machine learning approaches is outside of
    the scope of this book, but I’ll briefly go over them and describe the historical
    context in which they were developed. This will allow us to place deep learning
    in the broader context of machine learning and better understand where deep learning
    comes from and why it matters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对经典机器学习方法的详细讨论超出了本书的范围，但我将简要介绍它们，并描述它们开发的历史背景。这将使我们能够将深度学习置于机器学习更广泛的背景中，并更好地理解深度学习的来源和重要性。
- en: 1.2.1 Probabilistic modeling
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 概率建模
- en: '*Probabilistic modeling* is the application of the principles of statistics
    to data analysis. It is one of the earliest forms of machine learning, and it’s
    still widely used to this day. One of the best-known algorithms in this category
    is the Naive Bayes algorithm.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率建模*是将统计原理应用于数据分析的过程。它是机器学习的最早形式之一，至今仍然广泛使用。在这一类别中最著名的算法之一是朴素贝叶斯算法。'
- en: Naive Bayes is a type of machine learning classifier based on applying Bayes’
    theorem while assuming that the features in the input data are all independent
    (a strong, or “naive” assumption, which is where the name comes from). This form
    of data analysis predates computers and was applied by hand decades before its
    first computer implementation (most likely dating back to the 1950s). Bayes’ theorem
    and the foundations of statistics date back to the eighteenth century, and these
    are all you need to start using Naive Bayes classifiers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种基于应用贝叶斯定理的机器学习分类器，假设输入���据中的特征都是独立的（这是一个强大或“朴素”的假设，这也是名称的由来）。这种形式的数据分析早在计算机出现之前就存在，并且在首次计算机实现之前就通过手工应用（很可能可以追溯到20世纪50年代）了。贝叶斯定理和统计学的基础可以让您开始使用朴素贝叶斯分类器。
- en: A closely related model is *logistic regression* (logreg for short), which is
    sometimes considered to be the “Hello World” of modern machine learning. Don’t
    be misled by its name—logreg is a classification algorithm rather than a regression
    algorithm. Much like Naive Bayes, logreg predates computing by a long time, yet
    it’s still useful to this day, thanks to its simple and versatile nature. It’s
    often the first thing a data scientist will try on a dataset to get a feel for
    the classification task at hand.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个密切相关的模型是*逻辑回归*（简称logreg），有时被认为是现代机器学习的“Hello World”。不要被它的名字误导——logreg是一个分类算法而不是回归算法。与朴素贝叶斯类似，logreg在计算机出现很久之前就存在了，但由于其简单且多功能的特性，至今仍然很有用。它通常是数据科学家在数据集上尝试的第一件事，以了解手头的分类任务。
- en: 1.2.2 Early neural networks
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 早期神经网络
- en: Early iterations of neural networks have been completely supplanted by the modern
    variants covered in these pages, but it’s helpful to be aware of how deep learning
    originated. Although the core ideas of neural networks were investigated in toy
    forms as early as the 1950s, the approach took decades to get started. For a long
    time, the missing piece was an efficient way to train large neural networks. This
    changed in the mid-1980s, when multiple people independently rediscovered the
    Backpropagation algorithm—a way to train chains of parametric operations using
    gradient-descent optimization (we’ll precisely define these concepts later in
    the book)—and started applying it to neural networks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的神经网络已经被这些页面中涵盖的现代变体完全取代，但了解深度学习的起源仍然很有帮助。尽管神经网络的核心思想早在20世纪50年代就以玩具形式进行了研究，但这种方法花了几十年才开始。很长一段时间，缺失的部分是训练大型神经网络的有效方法。这种情况在20世纪80年代中期发生了变化，当时多人独立重新发现了反向传播算法——一种使用梯度下降优化训练参数化操作链的方法（我们将在本书后面精确定义这些概念）——并开始将其应用于神经网络。
- en: The first successful practical application of neural nets came in 1989 from
    Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural
    networks and backpropagation, and applied them to the problem of classifying handwritten
    digits. The resulting network, dubbed *LeNet*, was used by the United States Postal
    Service in the 1990s to automate the reading of ZIP codes on mail envelopes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 1989年，贝尔实验室首次成功应用神经网络的实际应用来自**杨立昆**，他将卷积神经网络和反向传播的早期思想结合起来，并将它们应用于手写数字分类问题。由此产生的网络被称为*LeNet*，在上世纪90年代被美国邮政服务用于自动读取信封上的邮政编码。
- en: 1.2.3 Kernel methods
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 核方法
- en: 'As neural networks started to gain some respect among researchers in the 1990s,
    thanks to this first success, a new approach to machine learning rose to fame
    and quickly sent neural nets back to oblivion: kernel methods. *Kernel methods*
    are a group of classification algorithms, the best known of which is the *Support
    Vector Machine* (SVM). The modern formulation of an SVM was developed by Vladimir
    Vapnik and Corinna Cortes in the early 1990s at Bell Labs and published in 1995,[³](../Text/01.htm#pgfId-1013448)
    although an older linear formulation was published by Vapnik and Alexey Chervonenkis
    as early as 1963.[⁴](../Text/01.htm#pgfId-1013470)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络在1990年代开始在研究人员中获得一些尊重时，由于这一初步成功，一种新的机器学习方法崭露头角，并迅速将神经网络送回到遗忘之中：核方法。*核方法*是一组分类算法，其中最著名的是*支持向量机*（SVM）。SVM的现代形式是由弗拉基米尔·瓦普尼克和Corinna
    Cortes在贝尔实验室于1990年代初开发，并于1995年发表的，尽管早在1963年，瓦普尼克和Alexey Chervonenkis就已经发表了一个较早的线性形式。
- en: 'SVM is a classification algorithm that works by finding “decision boundaries”
    separating two classes (see figure 1.10). SVMs proceed to find these boundaries
    in two steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一种分类算法，通过找到分隔两类的“决策边界”来工作（见图1.10）。SVM通过两个步骤来找到这些边界：
- en: The data is mapped to a new high-dimensional representation where the decision
    boundary can be expressed as a hyperplane (if the data was two-dimensional, as
    in figure 1.10, a hyperplane would be a straight line).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被映射到一个新的高维表示，其中决策边界可以表示为一个超平面（如果数据是二维的，如图1.10，超平面将是一条直线）。
- en: A good decision boundary (a separation hyperplane) is computed by trying to
    maximize the distance between the hyperplane and the closest data points from
    each class, a step called *maximizing the margin*. This allows the boundary to
    generalize well to new samples outside of the training dataset.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过尝试最大化超平面与每个类别最近数据点之间的距离来计算一个良好的决策边界（一个分离超平面），这一步骤称为*最大化间隔*。这使得边界能够很好地泛化到训练数据集之外的新样本。
- en: '![](../Images/01-10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-10.png)'
- en: Figure 1.10 A decision boundary
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 决策边界
- en: 'The technique of mapping data to a high-dimensional representation where a
    classification problem becomes simpler may look good on paper, but in practice
    it’s often computationally intractable. That’s where the *kernel trick* comes
    in (the key idea that kernel methods are named after). Here’s the gist of it:
    to find good decision hyperplanes in the new representation space, you don’t have
    to explicitly compute the coordinates of your points in the new space; you just
    need to compute the distance between pairs of points in that space, which can
    be done efficiently using a kernel function. A *kernel function* is a computationally
    tractable operation that maps any two points in your initial space to the distance
    between these points in your target representation space, completely bypassing
    the explicit computation of the new representation. Kernel functions are typically
    crafted by hand rather than learned from data—in the case of an SVM, only the
    separation hyperplane is learned.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据映射到高维表示的技术，使得分类问题变得更简单，看起来在理论上很不错，但在实践中通常是计算上难以处理的。这就是*核技巧*的用武之地（核方法以此命名的关键思想）。这是其要点：为了在新表示空间中找到良好的决策超平面，你不必显式计算点在新空间中的坐标；你只需要计算该空间中点对之间的距离，这可以通过核函数有效地完成。*核函数*是一种计算上易处理的操作，将初始空间中的任意两点映射到目标表示空间中这些点之间的距离，完全绕过了新表示的显式计算。核函数通常是手工制作而非从数据中学习的——在SVM的情况下，只有分离超平面是被学习的。
- en: At the time they were developed, SVMs exhibited state-of-the-art performance
    on simple classification problems and were one of the few machine learning methods
    backed by extensive theory and amenable to serious mathematical analysis, making
    them well understood and easily interpretable. Because of these useful properties,
    SVMs became extremely popular in the field for a long time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在它们被开发的时候，SVM在简单分类问题上表现出色，并且是少数几种机器学习方法之一，具有广泛的理论支持，并且易于进行严格的数学分析，使得它们被充分理解和容易解释。由于这些有用的特性，SVM在该领域长期以来非常受欢迎。
- en: But SVMs proved hard to scale to large datasets and didn’t provide good results
    for perceptual problems such as image classification. Because an SVM is a shallow
    method, applying an SVM to perceptual problems requires first extracting useful
    representations manually (a step called *feature engineering*), which is difficult
    and brittle. For instance, if you want to use an SVM to classify handwritten digits,
    you can’t start from the raw pixels; you should first find by hand useful representations
    that make the problem more tractable, like the pixel histograms I mentioned earlier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但SVM很难扩展到大型数据集，并且对于感知问题（如图像分类）没有提供良好的结果。因为SVM是一种浅层方法，将SVM应用于感知问题需要首先手动提取有用的表示（称为*特征工程*），这是困难且脆弱的���例如，如果你想使用SVM来分类手写数字，你不能从原始像素开始；你应该首先手动找到使问题更易处理的有用表示，就像我之前提到的像素直方图一样。
- en: 1.2.4 Decision trees, random forests, and gradient boosting machines
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 决策树、随机森林和梯度提升机
- en: '*Decision trees* are flowchart-like structures that let you classify input
    data points or predict output values given inputs (see figure 1.11). They’re easy
    to visualize and interpret. Decision trees learned from data began to receive
    significant research interest in the 2000s, and by 2010 they were often preferred
    to kernel methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是类似流程图的结构，让你对输入数据点进行分类或根据输入预测输出值（见图1.11）。它们易于可视化和解释。从数据中学习的决策树在2000年代开始受到重要的研究兴趣，到2010年，它们通常被优先于核方法。'
- en: '![](../Images/01-11.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-11.png)'
- en: 'Figure 1.11 A decision tree: the parameters that are learned are the questions
    about the data. A question could be, for instance, “Is coefficient 2 in the data
    greater than 3.5?”'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 决策树：学习的参数是关于数据的问题。一个问题可能是，“数据中的系数2是否大于3.5？”
- en: In particular, the *Random Forest* algorithm introduced a robust, practical
    take on decision-tree learning that involves building a large number of specialized
    decision trees and then ensembling their outputs. Random forests are applicable
    to a wide range of problems—you could say that they’re almost always the second-best
    algorithm for any shallow machine learning task. When the popular machine learning
    competition website Kaggle ([http://kaggle.com](http://kaggle.com)) got started
    in 2010, random forests quickly became a favorite on the platform—until 2014,
    when *gradient boosting machines* took over. A gradient boosting machine, much
    like a random forest, is a machine learning technique based on ensembling weak
    prediction models, generally decision trees. It uses *gradient boosting*, a way
    to improve any machine learning model by iteratively training new models that
    specialize in addressing the weak points of the previous models. Applied to decision
    trees, the use of the gradient boosting technique results in models that strictly
    outperform random forests most of the time, while having similar properties. It
    may be one of the best, if not *the* best, algorithm for dealing with nonperceptual
    data today. Alongside deep learning, it’s one of the most commonly used techniques
    in Kaggle competitions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是*随机森林*算法引入了一种稳健、实用的决策树学习方法，涉及构建大量专门的决策树，然后将它们的输出组合在一起。 随机森林适用于各种问题——你可以说它们几乎总是任何浅层机器学习任务的第二好算法。
    当流行的机器学习竞赛网站Kaggle ([http://kaggle.com](http://kaggle.com))在2010年开始时，随机森林迅速成为该平台上的热门选择，直到2014年，*梯度提升机*取代了它。
    梯度提升机与随机森林类似，是一种基于集成弱预测模型的机器学习技术，通常是决策树。 它使用*梯度提升*，一种通过迭代训练新模型来改进任何机器学习模型的方法，这些新模型专门解决前一模型的弱点。
    应用于决策树时，使用梯度提升技术会导致大多数情况下严格优于随机森林的模型，同时具有类似的性质。 它可能是处理非感知数据的最佳算法之一，如果不是*最佳*的话。
    与深度学习并列，它是Kaggle竞赛中最常用的技术之一。
- en: 1.2.5 Back to neural networks
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.5 回到神经网络
- en: 'Around 2010, although neural networks were almost completely shunned by the
    scientific community at large, a number of people still working on neural networks
    started to make important breakthroughs: the groups of Geoffrey Hinton at the
    University of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun
    at New York University, and IDSIA in Switzerland.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2010年，尽管神经网络几乎被科学界完全忽视，但仍有一些人在神经网络上取得重要突破：多伦多大学的Geoffrey Hinton小组，蒙特利尔大学的Yoshua
    Bengio小组，纽约大学的Yann LeCun小组，以及瑞士的IDSIA。
- en: In 2011, Dan Ciresan from IDSIA began to win academic image-classification competitions
    with GPU-trained deep neural networks—the first practical success of modern deep
    learning. But the watershed moment came in 2012, with the entry of Hinton’s group
    in the yearly large-scale image-classification challenge ImageNet (ImageNet Large
    Scale Visual Recognition Challenge, or ILSVRC for short). The ImageNet challenge
    was notoriously difficult at the time, consisting of classifying high-resolution
    color images into 1,000 different categories after training on 1.4 million images.
    In 2011, the top-five accuracy of the winning model, based on classical approaches
    to computer vision, was only 74.3%.[⁵](../Text/01.htm#pgfId-1013784) Then, in
    2012, a team led by Alex Krizhevsky and advised by Geoffrey Hinton was able to
    achieve a top-five accuracy of 83.6%—a significant breakthrough. The competition
    has been dominated by deep convolutional neural networks every year since. By
    2015, the winner reached an accuracy of 96.4%, and the classification task on
    ImageNet was considered to be a completely solved problem.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年，来自IDSIA的Dan Ciresan开始使用GPU训练的深度神经网络赢得学术图像分类竞赛——这是现代深度学习的第一个实际成功案例。 但转折点发生在2012年，当Hinton的小组参加了每年一次的大规模图像分类挑战ImageNet（ImageNet大规模视觉识别挑战，简称ILSVRC）。
    当时，ImageNet挑战非常困难，包括在训练了140万张图像后，将高分辨率彩色图像分类为1,000个不同类别。 2011年，基于传统计算机视觉方法的获胜模型的前五准确率仅为74.3%。[⁵](../Text/01.htm#pgfId-1013784)
    然后，在2012年，由Alex Krizhevsky领导并由Geoffrey Hinton指导的团队取得了83.6%的前五准确率——这是一个重大突破。 从那时起，每年的比赛都被深度卷积神经网络所主导。
    到2015年，获胜者的准确率达到96.4%，而ImageNet上的分类任务被认为是一个完全解决的问题。
- en: Since 2012, deep convolutional neural networks (*convnets*) have become the
    go-to algorithm for all computer vision tasks; more generally, they work on all
    perceptual tasks. At any major computer vision conference after 2015, it was nearly
    impossible to find presentations that didn’t involve convnets in some form. At
    the same time, deep learning has also found applications in many other types of
    problems, such as natural language processing. It has completely replaced SVMs
    and decision trees in a wide range of applications. For instance, for several
    years, the European Organization for Nuclear Research, CERN, used decision tree–based
    methods for analyzing particle data from the ATLAS detector at the Large Hadron
    Collider (LHC), but CERN eventually switched to Keras-based deep neural networks
    due to their higher performance and ease of training on large datasets.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年以来，深度卷积神经网络（*convnets*）已成为所有计算机视觉任务的首选算法；更一般地，它们适用于所有感知任务。 在2015年之后的任何一次重要计算机视觉会议上，几乎不可能找到不涉及convnets的演示。
    与此同时，深度学习还在许多其他类型的问题中找到了应用，如自然语言处理。 它已经完全取代了SVM和决策���在许多应用中的使用。 例如，多年来，欧洲核子研究组织CERN一直使用基于决策树的方法来分析大型强子对撞机（LHC）上ATLAS探测器的粒子数据，但最终CERN转而使用基于Keras的深度神经网络，因为它们在大型数据集上具有更高的性能和训练的便利性。
- en: 1.2.6 What makes deep learning different
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.6 深度学习的不同之处
- en: 'The primary reason deep learning took off so quickly is that it offered better
    performance for many problems. But that’s not the only reason. Deep learning also
    makes problem-solving much easier, because it completely automates what used to
    be the most crucial step in a machine learning workflow: feature engineering.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习之所以迅速崛起的主要原因是它在许多问题上提供了更好的性能。但这并不是唯一的原因。深度学习还使问题解决变得更加容易，因为它完全自动化了曾经是机器学习工作流程中最关键的步骤：特征工程。
- en: 'Previous machine learning techniques—shallow learning—only involved transforming
    the input data into one or two successive representation spaces, usually via simple
    transformations such as high-dimensional non-linear projections (SVMs) or decision
    trees. But the refined representations required by complex problems generally
    can’t be attained by such techniques. As such, humans had to go to great lengths
    to make the initial input data more amenable to processing by these methods: they
    had to manually engineer good layers of representations for their data. This is
    called *feature engineering*. Deep learning, on the other hand, completely automates
    this step: with deep learning, you learn all features in one pass rather than
    having to engineer them yourself. This has greatly simplified machine learning
    workflows, often replacing sophisticated multistage pipelines with a single, simple,
    end-to-end deep learning model.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的机器学习技术——浅层学习——只涉及将输入数据转换为一两个连续的表示空间，通常通过简单的转换，如高维非线性投影（SVM）或决策树。但复杂问题所需的精细表示通常无法通过这种技术实现。因此，人们不得不费尽心思地使初始输入数据更易于通过这些方法处理：他们必须手动为数据工程好表示层。这就是所谓的*特征工程*。另一方面，深度学习完全自动化了这一步骤：通过深度学习，你可以一次性学习所有特征，而不必自己进行工程设计。这大大简化了机器学习工作流程，通常用一个简单的端到端深度学习模型取代了复杂的多阶段流水线。
- en: 'You may ask, if the crux of the issue is to have multiple successive layers
    of representations, could shallow methods be applied repeatedly to emulate the
    effects of deep learning? In practice, successive applications of shallow-learning
    methods produce fast-diminishing returns, because the optimal first representation
    layer in a three-layer model isn’t the optimal first layer in a one-layer or two-layer
    model. What is transformative about deep learning is that it allows a model to
    learn all layers of representation *jointly*, at the same time, rather than in
    succession (*greedily*, as it’s called). With joint feature learning, whenever
    the model adjusts one of its internal features, all other features that depend
    on it automatically adapt to the change, without requiring human intervention.
    Everything is supervised by a single feedback signal: every change in the model
    serves the end goal. This is much more powerful than greedily stacking shallow
    models, because it allows for complex, abstract representations to be learned
    by breaking them down into long series of intermediate spaces (layers); each space
    is only a simple transformation away from the previous one.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，如果问题的关键在于具有多个连续的表示层，那么浅层方法是否可以重复应用以模拟深度学习的效果？实际上，连续应用浅层学习方法会产生快速递减的回报，因为在三层模型中的最佳第一表示层并不是一层或两层模型中的最佳第一层。深度学习的革命性之处在于它允许模型同时学习所有表示层，而不是按顺序（贪婪地）学习。通过联合特征学习，每当模型调整其内部特征时，所有依赖于它的其他特征都会自动适应变化，而无需人为干预。一切都由单一的反馈信号监督：模型中的每一次变化都服务于最终目标。这比贪婪地堆叠浅层模型更加强大，因为它允许通过将复杂的抽象表示分解为一系列中间空间（层）来学习它们；每个空间与前一个空间之间只有一个简单的转换。
- en: 'These are the two essential characteristics of how deep learning learns from
    data: the *incremental, layer-by-layer way in which increasingly complex representations
    are developed*, and the fact that *these intermediate incremental representations
    are learned jointly*, each layer being updated to follow both the representational
    needs of the layer above and the needs of the layer below. Together, these two
    properties have made deep learning vastly more successful than previous approaches
    to machine learning.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习从数据中学习的两个基本特征：*逐渐增加、逐层发展越来越复杂的表示方式*，以及*这些中间逐步增加的表示是联合学习的*，每一层都被更新以同时遵循上面一层的表示需求和下面一层的需求。这两个特性共同使深度学习比以前的机器学习方法更加成功。
- en: 1.2.7 The modern machine learning landscape
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.7 现代机器学习格局
- en: A great way to get a sense of the current landscape of machine learning algorithms
    and tools is to look at machine learning competitions on Kaggle. Due to its highly
    competitive environment (some contests have thousands of entrants and million-dollar
    prizes) and to the wide variety of machine learning problems covered, Kaggle offers
    a realistic way to assess what works and what doesn’t. So what kind of algorithm
    is reliably winning competitions? What tools do top entrants use?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 了解当前机器学习算法和工具的现状的一个好方法是查看Kaggle上的机器学习竞赛。由于其高度竞争的环境（一些比赛有数千名参与者和百万美元的奖金）以及涵盖的各种机器学习问题，Kaggle提供了一个评估什么有效、什么无效的现实方法。那么哪种算法可靠地赢得比赛？顶尖参与者使用什么工具？
- en: In early 2019, Kaggle ran a survey asking teams that ended in the top five of
    any competition since 2017 which primary software tool they had used in the competition
    (see figure 1.12). It turns out that top teams tend to use either deep learning
    methods (most often via the Keras library) or gradient boosted trees (most often
    via the LightGBM or XGBoost libraries).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年初，Kaggle进行了一项调查，询问自2017年以来在任何比赛中获得前五名的团队使用的主要软件工具是什么（见图1.12）。结果表明，顶尖团队倾向于使用深度学习方法（通常通过Keras库）或梯度提升树（通常通过LightGBM或XGBoost库）。
- en: '![](../Images/01-12.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-12.png)'
- en: Figure 1.12 Machine learning tools used by top teams on Kaggle
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 Kaggle 顶尖团队使用的机器学习工具
- en: It’s not just competition champions, either. Kaggle also runs a yearly survey
    among machine learning and data science professionals worldwide. With tens of
    thousands of respondents, this survey is one of our most reliable sources about
    the state of the industry. Figure 1.13 shows the percentage of usage of different
    machine learning software frameworks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是竞赛冠军。Kaggle 还每年对全球机器学习和数据科学专业人士进行调查。有数万名受访者参与，这项调查是我们关于行业状况最可靠的信息来源之一。图
    1.13 显示了不同机器学习软件框架的使用百分比。
- en: '![](../Images/01-13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-13.png)'
- en: 'Figure 1.13 Tool usage across the machine learning and data science industry
    (Source: [www.kaggle.com/kaggle-survey-2020](https://www.kaggle.com/kaggle-survey-2020))'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 工具在机器学习和数据科学行业中的使用情况（来源：[www.kaggle.com/kaggle-survey-2020](https://www.kaggle.com/kaggle-survey-2020)）
- en: 'From 2016 to 2020, the entire machine learning and data science industry has
    been dominated by these two approaches: deep learning and gradient boosted trees.
    Specifically, gradient boosted trees is used for problems where structured data
    is available, whereas deep learning is used for perceptual problems such as image
    classification.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2016 年到 2020 年，整个机器学习和数据科学行业都被这两种方法主导：深度学习和梯度提升树。具体来说，梯度提升树用于有结构化数据的问题，而深度学习用于感知问题，如图像分类。
- en: 'Users of gradient boosted trees tend to use Scikit-learn, XGBoost, or LightGBM.
    Meanwhile, most practitioners of deep learning use Keras, often in combination
    with its parent framework TensorFlow. The common point of these tools is they’re
    all Python libraries: Python is by far the most widely used language for machine
    learning and data science.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度提升树的用户倾向于使用 Scikit-learn、XGBoost 或 LightGBM。与此同时，大多数深度学习从业者使用 Keras，通常与其母框架
    TensorFlow 结合使用。这些工具的共同点是它们都是 Python 库：Python 是迄今为止机器学习和数据科学中最广泛使用的语言。
- en: 'These are the two techniques you should be the most familiar with in order
    to be successful in applied machine learning today: gradient boosted trees, for
    shallow-learning problems; and deep learning, for perceptual problems. In technical
    terms, this means you’ll need to be familiar with Scikit-learn, XGBoost, and Keras—the
    three libraries that currently dominate Kaggle competitions. With this book in
    hand, you’re already one big step closer.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您今天在应用机器学习中应该最熟悉的两种技术：梯度提升树，用于浅层学习问题；深度学习，用于感知问题。在技术上，这意味着您需要熟悉 Scikit-learn、XGBoost
    和 Keras 这三个目前主导 Kaggle 竞赛的库。有了这本书，您已经更接近成功了一大步。
- en: 1.3 Why deep learning? Why now?
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 为什么深度学习？为什么现在？
- en: The two key ideas of deep learning for computer vision—convolutional neural
    networks and backpropagation—were already well understood by 1990\. The Long Short-Term
    Memory (LSTM) algorithm, which is fundamental to deep learning for timeseries,
    was developed in 1997 and has barely changed since. So why did deep learning only
    take off after 2012? What changed in these two decades?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉中的两个关键思想——卷积神经网络和反向传播——在 1990 年就已经被充分理解。长短期记忆（LSTM）算法，对于时间序列的深度学习至关重要，于
    1997 年开发，至今几乎没有改变。那么为什么深度学习直到 2012 年后才起飞？这两个十年发生了什么变化？
- en: 'In general, three technical forces are driving advances in machine learning:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，有三个技术力量推动了机器学习的进步：
- en: Hardware
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件
- en: Datasets and benchmarks
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和基准测试
- en: Algorithmic advances
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法进步
- en: Because the field is guided by experimental findings rather than by theory,
    algorithmic advances only become possible when appropriate data and hardware are
    available to try new ideas (or to scale up old ideas, as is often the case). Machine
    learning isn’t mathematics or physics, where major advances can be done with a
    pen and a piece of paper. It’s an engineering science.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因为该领域是由实验结果而不是理论指导的，只有在适当的数据和硬件可用于尝试新想法（或扩展旧想法，通常情况下）时，算法进步才成为可能。机器学习不是数学或物理学，那里可以用一支笔和一张纸做出重大进步。这是一门工程科学。
- en: 'The real bottlenecks throughout the 1990s and 2000s were data and hardware.
    But here’s what happened during that time: the internet took off and high-performance
    graphics chips were developed for the needs of the gaming market.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 20 世纪 90 年代和 2000 年代的真正瓶颈是数据和硬件。但在那段时间发生了什么呢：互联网蓬勃发展，高性能图形芯片为游戏市场的需求而开发。
- en: 1.3.1 Hardware
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 硬件
- en: Between 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately
    5,000\. As a result, nowadays it’s possible to run small deep learning models
    on your laptop, whereas this would have been intractable 25 years ago.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从 1990 年到 2010 年，现成的 CPU 速度提高了约 5,000 倍。因此，现在可以在笔记本电脑上运行小型深度学习模型，而在 25 年前这是不可行的。
- en: But typical deep learning models used in computer vision or speech recognition
    require orders of magnitude more computational power than your laptop can deliver.
    Throughout the 2000s, companies like NVIDIA and AMD invested billions of dollars
    in developing fast, massively parallel chips (graphical processing units, or GPUs)
    to power the graphics of increasingly photorealistic video games—cheap, single-purpose
    supercomputers designed to render complex 3D scenes on your screen in real time.
    This investment came to benefit the scientific community when, in 2007, NVIDIA
    launched CUDA ([https://developer.nvidia.com/about-cuda](https://developer.nvidia.com/about-cuda)),
    a programming interface for its line of GPUs. A small number of GPUs started replacing
    massive clusters of CPUs in various highly parallelizable applications, beginning
    with physics modeling. Deep neural networks, consisting mostly of many small matrix
    multiplications, are also highly parallelizable, and around 2011 some researchers
    began to write CUDA implementations of neural nets—Dan Ciresan[⁶](../Text/01.htm#pgfId-1014349)
    and Alex Krizhevsky[⁷](../Text/01.htm#pgfId-1014371) were among the first.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在计算机视觉或语音识别中使用的典型深度学习模型需要比您的笔记本电脑提供的计算能力高出几个数量级。在2000年代，像NVIDIA和AMD这样的公司投资了数十亿美元开发快速、大规模并行芯片（图形处理单元，或GPU），以提供越来越逼真的视频游戏图形
    - 廉价、单一用途的超级计算机，旨在实时在屏幕上渲染复杂的3D场景。当NVIDIA于2007年推出CUDA（[https://developer.nvidia.com/about-cuda](https://developer.nvidia.com/about-cuda)）时，这项投资开始造福科学界，CUDA是其GPU系列的编程接口。少量GPU开始取代各种高度可并行化的应用程序中的大型CPU集群，从物理建模开始。由许多小矩阵乘法组成的深度神经网络也是高度可并行化的，大约在2011年左右，一些研究人员开始编写神经网络的CUDA实现
    - 丹·西雷桑[⁶](../Text/01.htm#pgfId-1014349)和亚历克斯·克里兹赫夫斯基[⁷](../Text/01.htm#pgfId-1014371)是最早的几位。
- en: What happened is that the gaming market subsidized supercomputing for the next
    generation of artificial intelligence applications. Sometimes, big things begin
    as games. Today, the NVIDIA Titan RTX, a GPU that cost $2,500 at the end of 2019,
    can deliver a peak of 16 teraFLOPS in single precision (16 trillion `float32`
    operations per second). That’s about 500 times more computing power than the world’s
    fastest supercomputer from 1990, the Intel Touchstone Delta. On a Titan RTX, it
    takes only a few hours to train an ImageNet model of the sort that would have
    won the ILSVRC competition around 2012 or 2013\. Meanwhile, large companies train
    deep learning models on clusters of hundreds of GPUs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的事情是游戏市场为下一代人工智能应用程序提供了超级计算的资助。有时，大事情从游戏开始。如今，NVIDIA Titan RTX，一款在2019年底售价为2500美元的GPU，在单精度（每秒16万亿`float32`操作）方面达到峰值16
    teraFLOPS。这大约是1990年世界上最快超级计算机Intel Touchstone Delta的500倍计算能力。在Titan RTX上，只需要几个小时就可以训练出类似于2012或2013年将赢得ILSVRC比赛的ImageNet模型。与此同时，大公司在数百个GPU的集群上训练深度学习模型。
- en: 'What’s more, the deep learning industry has been moving beyond GPUs and is
    investing in increasingly specialized, efficient chips for deep learning. In 2016,
    at its annual I/O convention, Google revealed its Tensor Processing Unit (TPU)
    project: a new chip design developed from the ground up to run deep neural networks
    significantly faster and far more energy efficient than top-of-the-line GPUs.
    Today, in 2020, the third iteration of the TPU card represents 420 teraFLOPS of
    computing power. That’s 10,000 times more than the Intel Touchstone Delta from
    1990.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习行业已经超越了GPU，正在投资于��来越专门化、高效的深度学习芯片。2016年，在其年度I/O大会上，Google公布了其张量处理单元（TPU）项目：一种全新的芯片设计，旨在比顶级GPU运行深度神经网络更快且更节能。如今，在2020年，TPU卡的第三代代表着420
    teraFLOPS的计算能力。这比1990年的Intel Touchstone Delta高出10,000倍。
- en: These TPU cards are designed to be assembled into large-scale configurations,
    called “pods.” One pod (1024 TPU cards) peaks at 100 petaFLOPS. For scale, that’s
    about 10% of the peak computing power of the current largest supercomputer, the
    IBM Summit at Oak Ridge National Lab, which consists of 27,000 NVIDIA GPUs and
    peaks at around 1.1 exaFLOPS.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些TPU卡设计为组装成大规模配置，称为“pods”。一个pod（1024个TPU卡）峰值为100 petaFLOPS。就规模而言，这大约是当前最大超级计算机IBM
    Summit在奥克岭国家实验室的峰值计算能力的10%，该计算机由27,000个NVIDIA GPU组成，峰值约为1.1 exaFLOPS。
- en: 1.3.2 Data
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 数据
- en: 'AI is sometimes heralded as the new industrial revolution. If deep learning
    is the steam engine of this revolution, then data is its coal: the raw material
    that powers our intelligent machines, without which nothing would be possible.
    When it comes to data, in addition to the exponential progress in storage hardware
    over the past 20 years (following Moore’s law), the game changer has been the
    rise of the internet, making it feasible to collect and distribute very large
    datasets for machine learning. Today, large companies work with image datasets,
    video datasets, and natural language datasets that couldn’t have been collected
    without the internet. User-generated image tags on Flickr, for instance, have
    been a treasure trove of data for computer vision. So are YouTube videos. And
    Wikipedia is a key dataset for natural language processing.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能有时被誉为新的工业革命。如果深度学习是这场革命的蒸汽机，那么数据就是其煤炭：为我们的智能机器提供动力的原材料，没有它什么也不可能。在数据方面，除了过去20年存储硬件的指数级进步（遵循摩尔定律）之外，互联网的兴起也是一个改变游戏规则的因素，使得收集和分发非常大的机器学习数据集成为可能。如今，大公司使用图像数据集、视频数据集和自然语言数据集，这些数据集如果没有互联网是无法收集的。例如，Flickr上用户生成的图像标签一直是计算机视觉的数据宝库。YouTube视频也是如此。维基百科是自然语言处理的关键数据集。
- en: If there’s one dataset that has been a catalyst for the rise of deep learning,
    it’s the ImageNet dataset, consisting of 1.4 million images that have been hand
    annotated with 1,000 image categories (one category per image). But what makes
    ImageNet special isn’t just its large size, but also the yearly competition associated
    with it.[⁸](../Text/01.htm#pgfId-1014455)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个数据集促进了深度学习的崛起，那就是ImageNet数据集，包含了140万张手动注释的图像，涵盖了1000个图像类别（每个图像一个类别）。但是ImageNet之所以特别，不仅仅是因为其规模庞大，还因为与之相关的年度竞赛。
- en: As Kaggle has been demonstrating since 2010, public competitions are an excellent
    way to motivate researchers and engineers to push the envelope. Having common
    benchmarks that researchers compete to beat has greatly helped the rise of deep
    learning, by highlighting its success against classical machine learning approaches.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Kaggle自2010年以来一直在展示的那样，公开竞赛是激励研究人员和工程师突破界限的绝佳方式。研究人员竞争击败的共同基准已经极大地帮助了深度学习的崛起，突显了其成功与传统机器学习方法的对比。
- en: 1.3.3 Algorithms
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 算法
- en: In addition to hardware and data, until the late 2000s, we were missing a reliable
    way to train very deep neural networks. As a result, neural networks were still
    fairly shallow, using only one or two layers of representations; thus, they weren’t
    able to shine against more-refined shallow methods such as SVMs and random forests.
    The key issue was that of *gradient propagation* through deep stacks of layers.
    The feedback signal used to train neural networks would fade away as the number
    of layers increased.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件和数据外，直到2000年代末，我们一直缺乏一种可靠的方法来训练非常深的神经网络。因此，神经网络仍然相对较浅，只使用一两层表示；因此，它们无法与更精细的浅层方法（如SVM和随机森林）相媲美。关键问题在于*梯度传播*穿过深层堆栈的问题。用于训练神经网络的反馈信号会随着层数的增加而逐渐消失。
- en: 'This changed around 2009–2010 with the advent of several simple but important
    algorithmic improvements that allowed for better gradient propagation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这在2009-2010年左右发生了变化，凭借几项简单但重要的算法改进，使得更好的梯度传播成为可能：
- en: Better *activation functions* for neural layers
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的神经层*激活函数*
- en: Better *weight-initialization schemes*, starting with layer-wise pretraining,
    which was then quickly abandoned
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的*权重初始化方案*，从逐层预训练开始，然后很快被放弃
- en: Better *optimization schemes*, such as RMSProp and Adam
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的*优化方案*，如RMSProp和Adam
- en: Only when these improvements began to allow for training models with 10 or more
    layers did deep learning start to shine.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当这些改进开始允许训练具有10层或更多层的模型时，深度学习才开始发光。
- en: Finally, in 2014, 2015, and 2016, even more advanced ways to improve gradient
    propagation were discovered, such as batch normalization, residual connections,
    and depthwise separable convolutions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在2014年、2015年和2016年，发现了更先进的改进梯度传播的方法，如批量归一化、残差连接和深度可分离卷积。
- en: Today, we can train models that are arbitrarily deep from scratch. This has
    unlocked the use of extremely large models, which hold considerable representational
    power—that is to say, which encode very rich hypothesis spaces. This extreme scalability
    is one of the defining characteristics of modern deep learning. Large-scale model
    architectures, which feature tens of layers and tens of millions of parameters,
    have brought about critical advances both in computer vision (for instance, architectures
    such as ResNet, Inception, or Xception) and natural language processing (for instance,
    large Transformer-based architectures such as BERT, GPT-3, or XLNet).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们可以从头开始训练任意深度的模型。这解锁了使用极其庞大模型的可能性，这些模型具有相当大的表征能力，即编码非常丰富的假设空间。这种极端的可扩展性是现代深度学习的定义特征之一。大规模模型架构，具有数十层和数千万参数，已经在计算机视觉（例如ResNet、Inception或Xception等架构）和自然语言处理（例如大型基于Transformer的架构，如BERT、GPT-3或XLNet）方面带来了关键进展。
- en: 1.3.4 A new wave of investment
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 新一轮投资
- en: As deep learning became the new state of the art for computer vision in 2012–2013,
    and eventually for all perceptual tasks, industry leaders took note. What followed
    was a gradual wave of industry investment far beyond anything previously seen
    in the history of AI (see figure 1.14).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习在2012-2013年成为计算机视觉的新技术标准，最终成为所有感知任务的新标准，行业领袖开始关注。随之而来的是一波逐渐增长的行业投资，远远超出了人工智能历史上此前所见的任何规模（见图1.14）。
- en: '![](../Images/01-14.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-14.png)'
- en: 'Figure 1.14 OECD estimate of total investments in AI startups (Source: [http://mng.bz/zGN6](http://mng.bz/zGN6))'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 OECD估计的人工智能初创公司的总投资（来源：[http://mng.bz/zGN6](http://mng.bz/zGN6)）
- en: In 2011, right before deep learning took the spotlight, the total venture capital
    investment in AI worldwide was less than a billion dollars, which went almost
    entirely to practical applications of shallow machine learning approaches. In
    2015, it had risen to over $5 billion, and in 2017, to a staggering $16 billion.
    Hundreds of startups launched in these few years, trying to capitalize on the
    deep learning hype. Meanwhile, large tech companies such as Google, Amazon, and
    Microsoft have invested in internal research departments in amounts that would
    most likely dwarf the flow of venture-capital money.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习引起关注之前的2011年，全球人工智能风险投资总额不到10亿美元，几乎全部投向了浅层机器学习方法的实际应用。到了2015年，这一数字已经上升到50多亿美元，2017年更是激增至160亿美元。在这几年间，数百家初创公司涌现，试图利用深度学习的热潮。与��同时，谷歌、亚马逊和微软等大型科技公司在内部研究部门的投资金额很可能超过了风险投资资金的流入量。
- en: Machine learning—in particular, deep learning—has become central to the product
    strategy of these tech giants. In late 2015, Google CEO Sundar Pichai stated,
    “Machine learning is a core, transformative way by which we’re rethinking how
    we’re doing everything. We’re thoughtfully applying it across all our products,
    be it search, ads, YouTube, or Play. And we’re in early days, but you’ll see us—in
    a systematic way—apply machine learning in all these areas.” [⁹](../Text/01.htm#pgfId-1014652)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习——特别是深度学习——已经成为这些科技巨头的产品战略的核心。2015年底，谷歌CEO桑达尔·皮查伊表示：“机器学习是我们重新思考我们如何做一切的核心、变革性方式。我们正在全面应用它，无论是搜索、广告、YouTube还是Play。我们还处于早期阶段，但你会看到我们以系统化的方式在所有这些领域应用机器学习。”
- en: As a result of this wave of investment, the number of people working on deep
    learning went from a few hundred to tens of thousands in less than 10 years, and
    research progress has reached a frenetic pace.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这波投资浪潮，从事深度学习工作的人数在不到10年的时间里从几百人增加到数万人，研究进展达到了疯狂的速度。
- en: 1.3.5 The democratization of deep learning
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 深度学习的民主化
- en: One of the key factors driving this inflow of new faces in deep learning has
    been the democratization of the toolsets used in the field. In the early days,
    doing deep learning required significant C++ and CUDA expertise, which few people
    possessed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 推动深度学习中新面孔涌入的一个关键因素是该领域使用的工具集的民主化。在早期，进行深度学习需要大量的C++和CUDA专业知识，而这种专业知识很少有人掌握。
- en: Nowadays, basic Python scripting skills suffice to do advanced deep learning
    research. This has been driven most notably by the development of the now-defunct
    Theano library, and then the TensorFlow library—two symbolic tensor-manipulation
    frameworks for Python that support autodifferentiation, greatly simplifying the
    implementation of new models—and by the rise of user-friendly libraries such as
    Keras, which makes deep learning as easy as manipulating LEGO bricks. After its
    release in early 2015, Keras quickly became the go-to deep learning solution for
    large numbers of new startups, graduate students, and researchers pivoting into
    the field.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，基本的Python脚本技能就足以进行高级深度学习研究。这主要得益于现已废弃的Theano库的发展，以及TensorFlow库——这两个用于Python的符号张量操作框架支持自动微分，极大地简化了新模型的实现——以及像Keras这样的用户友好库的崛起，使得深度学习就像操纵乐高积木一样简单。在2015年初发布后，Keras迅速成为大量新创企业、研究生和转入该领域的研究人员的首选深度学习解决方案。
- en: 1.3.6 Will it last?
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.6 它会持续吗？
- en: Is there anything special about deep neural networks that makes them the “right”
    approach for companies to be investing in and for researchers to flock to? Or
    is deep learning just a fad that may not last? Will we still be using deep neural
    networks in 20 years?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络有什么特别之处，使得它们成为公司投资和研究人员涌入的“正确”选择？或者深度学习只是一个可能不会持续的时尚？20年后我们还会使用深度神经网络吗？
- en: 'Deep learning has several properties that justify its status as an AI revolution,
    and it’s here to stay. We may not be using neural networks two decades from now,
    but whatever we use will directly inherit from modern deep learning and its core
    concepts. These important properties can be broadly sorted into three categories:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习具有几个属性，这些属性证明了它作为人工智能革命的地位，并且它将会持续存在。也许20年后我们不会再使用神经网络，但无论我们使用什么，都将直接继承现代深度学习及其核心概念。这些重要属性可以广泛分为三类：
- en: '*Simplicity*—Deep learning removes the need for feature engineering, replacing
    complex, brittle, engineering-heavy pipelines with simple, end-to-end trainable
    models that are typically built using only five or six different tensor operations.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单性*——深度学习消除了特征工程的需求，用简单的端到端可训练模型取代了复杂、脆弱、工程密集型的流水线，通常仅使用五到六种不同的张量操作构建。'
- en: '*Scalability*—Deep learning is highly amenable to parallelization on GPUs or
    TPUs, so it can take full advantage of Moore’s law. In addition, deep learning
    models are trained by iterating over small batches of data, allowing them to be
    trained on datasets of arbitrary size. (The only bottleneck is the amount of parallel
    computational power available, which, thanks to Moore’s law, is a fast-moving
    barrier.)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*——深度学习非常适合在GPU或TPU上并行化，因此可以充分利用摩尔定律。此外，深度学习模型通过迭代小批量数据进行训练，使其能够在任意大小的数据集上进行训练。（唯一的瓶颈是可用的并行计算能力量，由于摩尔定律的存在，这是一个快速移动的障碍。）'
- en: '*Versatility and reusability*—Unlike many prior machine learning approaches,
    deep learning models can be trained on additional data without restarting from
    scratch, making them viable for continuous online learning—an important property
    for very large production models. Furthermore, trained deep learning models are
    repurposable and thus reusable: for instance, it’s possible to take a deep learning
    model trained for image classification and drop it into a video-processing pipeline.
    This allows us to reinvest previous work into increasingly complex and powerful
    models. This also makes deep learning applicable to fairly small datasets.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多功能性和可重用性*——与许多先前的机器学习方法不同，深度学习模型可以在不从头开始重新启动的情况下训练额外的数据，使其适用于连续在线学习——这对于非常大的生产模型是一个重要的特性。此外，经过训练的深度学习模型是可重用的：例如，可以将经过图像分类训练的深度学习模型放入视频处理流水线中。这使我们能够将以前的工作重新投资到越来越复杂和强大的模型中。这也使得深度学习适用于相当小的数据集。'
- en: 'Deep learning has only been in the spotlight for a few years, and we may not
    yet have established the full scope of what it can do. With every passing year,
    we learn about new use cases and engineering improvements that lift previous limitations.
    Following a scientific revolution, progress generally follows a sigmoid curve:
    it starts with a period of fast progress, which gradually stabilizes as researchers
    hit hard limitations, and then further improvements become incremental.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习仅仅在聚光灯下曝光了几年，我们可能还没有确定其能够做到的全部范围。随着每一年的过去，我们了解到新的用例和工程改进，这些改进消除了以前的限制。在科学革命之后，进展通常遵循S形曲线：它从快速进展的阶段开始，逐渐稳定下来，研究人员遇到严重限制，然后进一步的改进变得渐进式。
- en: When I was writing the first edition of this book, in 2016, I predicted that
    deep learning was still in the first half of that sigmoid, with much more transformative
    progress to come in the following few years. This has proven true in practice,
    as 2017 and 2018 have seen the rise of Transformer-based deep learning models
    for natural language processing, which have been a revolution in the field, while
    deep learning also kept delivering steady progress in computer vision and speech
    recognition. Today, in 2021, deep learning seems to have entered the second half
    of that sigmoid. We should still expect significant progress in the years to come,
    but we’re probably out of the initial phase of explosive progress.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当我写第一版这本书时，也就是2016年，我预测深度学习仍处于S形曲线的上半部，接下来几年将会有更多变革性的进展。实践证明这一点是正确的，因为2017年和2018年见证了基于Transformer的深度学习模型在自然语言处理领域的崛起，这在该领域引起了一场革命，同时深度学习在计算机视觉和语音识别领域也持续稳步取得进展。如今，2021年，深度学习似乎已经进入S形曲线的下半部。我们仍然应该期待未来几年的重大进展，但我们可能已经走出了爆炸性进展的初始阶段。
- en: Today, I’m extremely excited about the deployment of deep learning technology
    to every problem it can solve—the list is endless. Deep learning is still a revolution
    in the making, and it will take many years to realize its full potential.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我对深度学习技术应用于解决各种问题感到非常兴奋——问题的范围是无限的。深度学习仍然是一场正在进行中的革命，要实现其全部潜力还需要很多年。
- en: '* * *'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[¹](#Id-1012032) A.M. Turing, “Computing Machinery and Intelligence,” *Mind*
    59, no. 236 (1950): 433–460.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[¹](#Id-1012032) 艾伦·图灵，“计算机器械与智能”，*心灵* 59，第236期（1950年）：433–460。'
- en: '[²](#Id-1012070) Although the Turing test has sometimes been interpreted as
    a literal test—a goal the field of AI should set out to reach—Turing merely meant
    it as a conceptual device in a philosophical discussion about the nature of cognition.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[²](#Id-1012070) 尽管图灵测试有时被解释为一种字面测试——人工智能领域应该设定的目标——但图灵只是将其作为一个概念设备，用于关于认知本质的哲学讨论。'
- en: '[³](#Id-1013448) Vladimir Vapnik and Corinna Cortes, “Support-Vector Networks,”
    *Machine Learning* 20, no. 3 (1995): 273–297.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[³](#Id-1013448) 弗拉基米尔·瓦普尼克和科琳娜·科尔特斯，“支持向量网络”，*机器学习* 20，第3期（1995年）：273–297。'
- en: '[⁴](#Id-1013470) Vladimir Vapnik and Alexey Chervonenkis, “A Note on One Class
    of Perceptrons,” *Automation and Remote Control* 25 (1964).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁴](#Id-1013470) 弗拉基米尔·瓦普尼克和亚历克谢·切尔沃年基斯，“关于一类感知机的注记”，*自动化与遥感控制* 25（1964年）。'
- en: '[⁵](#Id-1013784) “Top-five accuracy” measures how often the model selects the
    correct answer as part of its top five guesses (out of 1,000 possible answers,
    in the case of ImageNet).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁵](#Id-1013784) “前五准确率”衡量模型在其前五个猜测中多少次选择了正确答案（在ImageNet的情况下，共有1,000个可能的答案）。'
- en: '[⁶](#Id-1014349) See “Flexible, High Performance Convolutional Neural Networks
    for Image Classification,” *Proceedings of the 22nd International Joint Conference
    on Artificial Intelligence* (2011), [www.ijcai.org/Proceedings/11/Papers/210.pdf](https://www.ijcai.org/Proceedings/11/Papers/210.pdf).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁶](#Id-1014349) 参见“用于图像分类的灵活、高性能卷积神经网络”，*第22届国际人工智能联合会议论文集*（2011），[www.ijcai.org/Proceedings/11/Papers/210.pdf](https://www.ijcai.org/Proceedings/11/Papers/210.pdf)。'
- en: '[⁷](#Id-1014371) See “ImageNet Classification with Deep Convolutional Neural
    Networks,” *Advances in Neural Information Processing Systems* 25 (2012), [http://mng.bz/2286](https://mng.bz/2286).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁷](#Id-1014371) 参见“使用深度卷积神经网络进行ImageNet分类”，*神经信息处理系统进展* 25（2012），[http://mng.bz/2286](https://mng.bz/2286)。'
- en: '[⁸](#Id-1014455) The ImageNet Large Scale Visual Recognition Challenge (ILSVRC),
    [www.image-net.org/challenges/LSVRC](https://www.image-net.org/challenges/LSVRC).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁸](#Id-1014455) ImageNet大规模视觉识别挑战（ILSVRC），[www.image-net.org/challenges/LSVRC](https://www.image-net.org/challenges/LSVRC)。'
- en: '[⁹](#Id-1014652) Sundar Pichai, Alphabet earnings call, Oct. 22, 2015.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[⁹](#Id-1014652) 桑达尔·皮查伊，Alphabet财报电话会议，2015年10月22日。'
