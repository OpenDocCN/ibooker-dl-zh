- en: Chapter 13\. Processing Sequences Using RNNs and CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章\. 使用RNNs和CNNs处理序列
- en: Predicting the future is something you do all the time, whether you are finishing
    a friend’s sentence or anticipating the smell of coffee at breakfast. In this
    chapter we will discuss recurrent neural networks (RNNs)—a class of nets that
    can predict the future (well, up to a point). RNNs can analyze time series data,
    such as the number of daily active users on your website, the hourly temperature
    in your city, your home’s daily power consumption, the trajectories of nearby
    cars, and more. Once an RNN learns past patterns in the data, it is able to use
    its knowledge to forecast the future, assuming, of course, that past patterns
    still hold in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来是你在做所有事情时都会做的事情，无论是完成朋友的句子还是预测早餐时咖啡的气味。在本章中，我们将讨论循环神经网络（RNNs）——一类可以预测未来的网络（好吧，至少在一定范围内）。RNNs可以分析时间序列数据，如你网站上每日活跃用户数量、你城市的小时温度、你家的每日电力消耗、附近汽车的轨迹等等。一旦RNN学会了数据中的过去模式，它就能利用其知识来预测未来，当然，前提是过去的模式在未来仍然成立。
- en: More generally, RNNs can work on sequences of arbitrary lengths, rather than
    on fixed-sized inputs. For example, they can take sentences, documents, or audio
    samples as input, which makes them extremely useful for natural language processing
    applications such as automatic translation or speech-to-text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，RNNs可以处理任意长度的序列，而不是固定大小的输入。例如，它们可以接受句子、文档或音频样本作为输入，这使得它们在自然语言处理应用中极为有用，如自动翻译或语音转文本。
- en: 'In this chapter, we will first go through the fundamental concepts underlying
    RNNs and how to train them using backpropagation through time. Then, we will use
    them to forecast a time series. Along the way, we will look at the popular autoregressive
    moving average (ARMA) family of models, often used to forecast time series, and
    use them as baselines to compare with our RNNs. After that, we’ll explore the
    two main difficulties that RNNs face:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍RNNs的基本概念以及如何使用时间反向传播来训练它们。然后，我们将使用它们来预测时间序列。在这个过程中，我们将探讨常用于预测时间序列的流行自回归移动平均（ARMA）模型族，并将它们作为基准与我们的RNNs进行比较。之后，我们将探讨RNNs面临的两个主要困难：
- en: Unstable gradients (discussed in [Chapter 11](ch11.html#deep_chapter)), which
    can be alleviated using various techniques, including *recurrent dropout* and
    *recurrent layer normalization*.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不稳定的梯度（在第11章[深度学习](ch11.html#deep_chapter)中讨论），可以使用各种技术来缓解，包括**循环dropout**和**循环层归一化**。
- en: A (very) limited short-term memory, which can be extended using long short-term
    memory (LSTM) and gated recurrent unit (GRU) cells.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个（非常）有限的短期记忆，这可以通过使用长短期记忆（LSTM）和门控循环单元（GRU）细胞来扩展。
- en: 'RNNs are not the only types of neural networks capable of handling sequential
    data. For small sequences, a regular dense network can do the trick, and for very
    long sequences, such as audio samples or text, convolutional neural networks can
    actually work quite well too. We will discuss both of these possibilities, and
    we will finish this chapter by implementing a WaveNet—a CNN architecture capable
    of handling sequences of tens of thousands of time steps. But we can do even better!
    In [Chapter 14](ch14.html#nlp_chapter), we will apply RNNs to natural language
    processing (NLP), and we will see how to boost them using attention mechanisms.
    Attention is at the core of transformers, which we will discover in [Chapter 15](ch15.html#transformer_chapter):
    they are now the state of the art for sequence processing, NLP, and even computer
    vision. But before we get there, let’s start with the simplest RNNs!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs并不是唯一能够处理序列数据的神经网络类型。对于短序列，一个常规的密集网络就可以解决问题，而对于非常长的序列，如音频样本或文本，卷积神经网络实际上也可以工作得相当好。我们将讨论这两种可能性，并在本章结束时实现一个WaveNet——一个能够处理成千上万个时间步序列的CNN架构。但我们可以做得更好！在[第14章](ch14.html#nlp_chapter)中，我们将应用RNNs到自然语言处理（NLP）中，并了解如何使用注意力机制来提升它们。注意力是transformers的核心，我们将在[第15章](ch15.html#transformer_chapter)中揭示：它们现在是序列处理、NLP甚至计算机视觉的尖端技术。但在我们到达那里之前，让我们从最简单的RNNs开始！
- en: Recurrent Neurons and Layers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经元和层
- en: Up to now we have focused on feedforward neural networks, where the activations
    flow only in one direction, from the input layer to the output layer. A recurrent
    neural network looks very much like a feedforward neural network, except it also
    has connections pointing backward.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注前馈神经网络，其中激活只在一个方向上流动，从输入层到输出层。循环神经网络看起来非常像前馈神经网络，除了它还有指向后方的连接。
- en: Let’s look at the simplest possible RNN, composed of one neuron receiving inputs,
    producing an output, and sending that output back to itself (see [Figure 13-1](#simple_rnn_diagram),
    left). At each *time step* *t* (also called a *frame*), this *recurrent neuron*
    receives the inputs **x**[(*t*)] as well as its own output from the previous time
    step, *ŷ*[(*t*–1)]. Since there is no previous output at the first time step,
    it is generally set to zero. We can represent this tiny network against the time
    axis (see [Figure 13-1](#simple_rnn_diagram), right). This is called *unrolling
    the network through time* (it’s the same recurrent neuron represented once per
    time step).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们看看最简单的 RNN，它由一个接收输入、产生输出并将该输出发送回自身的神经元组成（见图 13-1，左侧）。在每个 *时间步* *t*（也称为 *帧*），这个
    *循环神经元* 接收输入 **x**[(*t*)] 以及前一个时间步的输出 *ŷ*[(*t*–1)]。由于第一个时间步没有前一个输出，通常将其设置为零。我们可以将这个微小的网络与时间轴相对比（见图
    13-1，右侧）。这被称为 *随时间展开网络*（它是在每个时间步表示一次的相同循环神经元）。 '
- en: '![Diagram of a recurrent neuron loop (left) and its unrolled representation
    over time (right), illustrating input and output flow across time steps.](assets/hmls_1301.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经元循环（左侧）及其随时间展开的表示（右侧），展示了跨越时间步的输入和输出流。](assets/hmls_1301.png)'
- en: Figure 13-1\. A recurrent neuron (left) unrolled through time (right)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 左侧为循环神经元，右侧为随时间展开的表示
- en: You can easily create a layer of recurrent neurons. At each time step *t*, every
    neuron receives both the input vector **x**[(*t*)] and the output vector from
    the previous time step **ŷ**[(*t*–1)], as shown in [Figure 13-2](#rnn_layer_diagram).
    Note that both the inputs and outputs are now vectors (when there was just a single
    neuron, the output was a scalar).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松创建一个循环神经元层。在每个时间步 *t*，每个神经元都会接收到输入向量 **x**[(*t*)] 和前一个时间步的输出向量 **ŷ**[(*t*–1)]，如图[图
    13-2](#rnn_layer_diagram)所示。注意，现在输入和输出都是向量（当只有一个神经元时，输出是一个标量）。
- en: '![Diagram illustrating a layer of recurrent neurons on the left and their unrolled
    representation through time on the right, showing input and output vectors at
    each time step.](assets/hmls_1302.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示左侧展示了一层循环神经元及其在右侧随时间展开的表示，显示了每个时间步的输入和输出向量。](assets/hmls_1302.png)'
- en: Figure 13-2\. A layer of recurrent neurons (left) unrolled through time (right)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 一层循环神经元随时间展开的表示（左侧）
- en: 'Each recurrent neuron has two sets of weights: one for the inputs **x**[(*t*)]
    and the other for the outputs of the previous time step, **ŷ**[(*t*–1)]. Let’s
    call these weight vectors **w**[*x*] and **w**[*ŷ*]. If we consider the whole
    recurrent layer instead of just one recurrent neuron, we can place all the weight
    vectors in two weight matrices: **W**[*x*] and **W**[*ŷ*].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每个循环神经元有两套权重：一套用于输入 **x**[(*t*)]，另一套用于前一个时间步的输出 **ŷ**[(*t*–1)]。让我们称这些权重向量为 **w**[*x*]
    和 **w**[*ŷ*]。如果我们考虑整个循环层而不是单个循环神经元，我们可以将所有权重向量放入两个权重矩阵中：**W**[*x*] 和 **W**[*ŷ*]。
- en: The output vector of the whole recurrent layer can then be computed pretty much
    as you might expect, as shown in [Equation 13-1](#rnn_output_equation), where
    **b** is the bias vector and *ϕ*(·) is the activation function (e.g., ReLU⁠^([1](ch13.html#id3062))).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 整个循环层的输出向量可以像你预期的那样计算，如[方程 13-1](#rnn_output_equation)所示，其中**b**是偏置向量，*ϕ*(·)是激活函数（例如，ReLU⁠^([1](ch13.html#id3062))）。
- en: Equation 13-1\. Output of a recurrent layer for a single instance
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 13-1\. 单个实例的循环层输出
- en: $dollar-sign ModifyingAbove bold y With caret Subscript left-parenthesis t right-parenthesis
    Baseline equals phi left-parenthesis bold upper W Subscript x Baseline Superscript
    upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline
    plus bold upper W Subscript ModifyingAbove y With caret Baseline Superscript upper
    T Baseline ModifyingAbove bold y With caret Subscript left-parenthesis t minus
    1 right-parenthesis Baseline plus bold b right-parenthesis dollar-sign$
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: $dollar-sign 修改上述粗体 y 带上标 Subscript 左括号 t 右括号 基线等于 phi 左括号 粗体上标 W Subscript
    x 基线 上标 T 基线 粗体 x Subscript 左括号 t 右括号 基线加上 粗体上标 W Subscript 修改上述 y 带上标 基线 上标 T
    基线 修改上述粗体 y 带上标 左括号 t 减 1 右括号 基线加上 粗体 b 右括号 dollar-sign$
- en: Just as with feedforward neural networks, we can compute a recurrent layer’s
    output in one shot for an entire mini-batch by placing all the inputs at time
    step *t* into an input matrix **X**[(*t*)] (see [Equation 13-2](#rnn_output_vectorized_equation)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前馈神经网络一样，我们可以通过将所有时间步 *t* 的输入放入输入矩阵 **X**[(*t*)] 中，一次性计算整个迷你批次的循环层的输出（参见 [方程
    13-2](#rnn_output_vectorized_equation)）。
- en: Equation 13-2\. Outputs of a layer of recurrent neurons for all instances in
    a mini-batch
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 13-2\. 迷你批次中所有实例的循环神经元层的输出
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ϕ</mi> <mn>(</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi>
    <mn>(</mn> <mn>[</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mn>]</mn> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn> <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mo>[</mo>
    <mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ϕ</mi> <mn>(</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi>
    <mn>(</mn> <mn>[</mn> <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mn>]</mn> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mn>)</mn> <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mo>[</mo>
    <mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: 'In this equation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**Ŷ**[(*t*)] is an *m* × *n*[neurons] matrix containing the layer’s outputs
    at time step *t* for each instance in the mini-batch (*m* is the number of instances
    in the mini-batch, and *n*[neurons] is the number of neurons).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ŷ**[(*t*)] 是一个 *m* × *n*[神经元] 矩阵，包含在时间步 *t* 时每个实例的层的输出（*m* 是迷你批次的实例数量，*n*[神经元]
    是神经元的数量）。'
- en: '**X**[(*t*)] is an *m* × *n*[inputs] matrix containing the inputs for all instances
    (*n*[inputs] is the number of input features).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**[(*t*)] 是一个 *m* × *n*[输入] 矩阵，包含所有实例的输入（*n*[输入] 是输入特征的数量）。'
- en: '**W**[*x*] is an *n*[inputs] × *n*[neurons] matrix containing the connection
    weights for the inputs of the current time step.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*x*] 是一个 *n*[输入] × *n*[神经元] 的矩阵，包含当前时间步输入的连接权重。'
- en: '**W**[*ŷ*] is an *n*[neurons] × *n*[neurons] matrix containing the connection
    weights for the outputs of the previous time step.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*ŷ*] 是一个 *n*[神经元] × *n*[神经元] 的矩阵，包含前一时间步输出的连接权重。'
- en: '**b** is a vector of size *n*[neurons] containing each neuron’s bias term.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b** 是一个大小为 *n*[神经元] 的向量，包含每个神经元的偏置项。'
- en: The weight matrices **W**[*x*] and **W**[*ŷ*] are often concatenated vertically
    into a single weight matrix **W** of shape (*n*[inputs] + *n*[neurons]) × *n*[neurons]
    (see the second line of [Equation 13-2](#rnn_output_vectorized_equation)).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵 **W**[*x*] 和 **W**[*ŷ*] 通常垂直拼接成一个形状为 (*n*[输入] + *n*[神经元]) × *n*[神经元] 的单个权重矩阵
    **W**（参见 [方程 13-2](#rnn_output_vectorized_equation) 的第二行）。
- en: The notation [**X**[(*t*)] **Ŷ**[(*t*–1)]] represents the horizontal concatenation
    of the matrices **X**[(*t*)] and **Ŷ**[(*t*–1)].
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号 [**X**[(*t*)] **Ŷ**[(*t*–1)]] 表示矩阵 **X**[(*t*)] 和 **Ŷ**[(*t*–1)] 的水平拼接。
- en: Notice that **Ŷ**[(*t*)] is a function of **X**[(*t*)] and **Ŷ**[(*t*–1)], which
    is a function of **X**[(*t*–1)] and **Ŷ**[(*t*–2)], which is a function of **X**[(*t*–2)]
    and **Ŷ**[(*t*–3)], and so on. This makes **Ŷ**[(*t*)] a function of all the inputs
    since time *t* = 0 (that is, **X**[(0)], **X**[(1)], …​, **X**[(*t*)]). At the
    first time step, *t* = 0, there are no previous outputs, so they are typically
    assumed to be all zeros.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 **Ŷ**[(*t*)] 是 **X**[(*t*)] 和 **Ŷ**[(*t*–1)] 的函数，而 **Ŷ**[(*t*–1)] 是 **X**[(*t*–1)]
    和 **Ŷ**[(*t*–2)] 的函数，**Ŷ**[(*t*–2)] 是 **X**[(*t*–2)] 和 **Ŷ**[(*t*–3)] 的函数，以此类推。这使得
    **Ŷ**[(*t*)] 成为从时间 *t* = 0（即 **X**[(0)], **X**[(1)], …​, **X**[(*t*)]）以来的所有输入的函数。在第一个时间步，*t*
    = 0，没有先前的输出，因此它们通常被假定为都是零。
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The idea of introducing backward connections (i.e., loops) in artificial neural
    networks dates back to the very origins of ANNs, but the first modern RNN architecture
    was [proposed by Michael I. Jordan in 1986](https://homl.info/jordanrnn).⁠^([2](ch13.html#id3066))
    At each time step, his RNN would look at the inputs for that time step, plus its
    own outputs from the previous time step. This is called *output feedback*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络中引入反向连接（即循环）的想法可以追溯到 ANNs 的起源，但第一个现代 RNN 架构是由迈克尔·I·乔丹在 1986 年提出的（[参见](https://homl.info/jordanrnn)）。⁠^([2](ch13.html#id3066))
    在每个时间步，他的 RNN 会查看该时间步的输入，以及它前一个时间步的输出。这被称为 *输出反馈*。
- en: Memory Cells
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆细胞
- en: Since the output of a recurrent neuron at time step *t* is a function of all
    the inputs from previous time steps, you could say it has a form of *memory*.
    A part of a neural network that preserves some state across time steps is called
    a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of
    recurrent neurons, is a very basic cell, capable of learning only short patterns
    (typically about 10 steps long, but this varies depending on the task). Later
    in this chapter, we will look at some more complex and powerful types of cells
    capable of learning longer patterns (roughly 10 times longer, but again, this
    depends on the task).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于循环神经元在时间步 *t* 的输出是所有先前时间步输入的函数，可以说它具有一种形式的 *记忆*。在时间步之间保持某些状态的神经网络的一部分称为 *记忆细胞*（或简单地称为
    *细胞*）。单个循环神经元或循环神经元层是一个非常基本的细胞，只能学习短模式（通常长度约为 10 步，但这个数字根据任务而变化）。在本章的后面部分，我们将探讨一些更复杂且功能强大的细胞类型，它们能够学习更长的模式（大约长
    10 倍，但同样，这取决于任务）。
- en: 'A cell’s state at time step *t*, denoted **h**[(*t*)] (the “h” stands for “hidden”),
    is a function of some inputs at that time step and its state at the previous time
    step: **h**[(*t*)] = *f*(**x**[(*t*)], **h**[(*t*–1)]). Its output at time step
    *t*, denoted **ŷ**[(*t*)], is also a function of the previous state and the current
    inputs, and typically it’s just a function of the current state. In the case of
    the basic cells we have discussed so far, the output is just equal to the state,
    but in more complex cells this is not always the case, as shown in [Figure 13-3](#hidden_state_diagram).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 *t* 的细胞状态，表示为 **h**[(*t*)]（“h”代表“隐藏”），是该时间步的一些输入及其前一个时间步的状态的函数：**h**[(*t*)]
    = *f*(**x**[(*t*)], **h**[(*t*–1)]）。它在时间步 *t* 的输出，表示为 **ŷ**[(*t*)]，也是前一个状态和当前输入的函数，通常它只是当前状态的函数。在我们迄今为止讨论的基本细胞中，输出等于状态，但在更复杂的细胞中，情况并不总是如此，如图
    [图 13-3](#hidden_state_diagram) 所示。
- en: '![Diagram illustrating how a cell''s hidden state and output evolve over time,
    with feedback loops showing the interaction between inputs, hidden states, and
    outputs at different time steps.](assets/hmls_1303.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![说明细胞隐藏状态和输出随时间演化的图，其中反馈回路显示了不同时间步输入、隐藏状态和输出之间的交互。](assets/hmls_1303.png)'
- en: Figure 13-3\. A cell’s hidden state and its output may be different
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-3\. 细胞的隐藏状态及其输出可能不同
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first modern RNN that fed back the hidden state rather than the outputs
    was [proposed by Jeffrey Elman in 1990](https://homl.info/elmanrnn).⁠^([3](ch13.html#id3069))
    This is called *state feedback*, and it’s the most common approach today.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个现代循环神经网络（RNN）是杰弗里·埃尔曼在 1990 年提出的，它将隐藏状态而不是输出反馈回去（[参见](https://homl.info/elmanrnn)）。⁠^([3](ch13.html#id3069))
    这被称为 *状态反馈*，并且是今天最常见的方法。
- en: Input and Output Sequences
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入和输出序列
- en: 'An RNN can simultaneously take a sequence of inputs and produce a sequence
    of outputs (see the top-left network in [Figure 13-4](#seq_to_seq_diagram)). This
    type of *sequence-to-sequence network* is useful to forecast time series, such
    as your home’s daily power consumption: you feed it the data over the last *N*
    days, and you train it to output the power consumption shifted by one day into
    the future (i.e., from *N* – 1 days ago to tomorrow).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RNN可以同时接受一个输入序列并产生一个输出序列（参见[图13-4](#seq_to_seq_diagram)左上角的网络）。这种类型的*序列到序列网络*对于预测时间序列很有用，例如你家的每日电力消耗：你给它过去*N*天的数据，并训练它输出未来一天内的电力消耗（即从*N*
    - 1天到明天）。
- en: Alternatively, you could feed the network a sequence of inputs and ignore all
    outputs except for the last one (see the top-right network in [Figure 13-4](#seq_to_seq_diagram)).
    This is a *sequence-to-vector network*. For example, you could feed the network
    a sequence of words corresponding to a movie review, and the network would output
    a sentiment score (e.g., from 0 [hate] to 1 [love]).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以向网络输入一个输入序列，并忽略除了最后一个输出之外的所有输出（参见[图13-4](#seq_to_seq_diagram)右上角的网络）。这是一个*序列到向量网络*。例如，你可以向网络输入一个与电影评论对应的单词序列，网络将输出一个情感评分（例如，从0[恨]到1[爱]）。
- en: Conversely, you could feed the network the same input vector over and over again
    at each time step and let it output a sequence (see the bottom-left network of
    [Figure 13-4](#seq_to_seq_diagram)). This is a *vector-to-sequence network*. For
    example, the input could be an image (or the output of a CNN), and the output
    could be a caption for that image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以在每个时间步重复输入相同的输入向量，并让网络输出一个序列（参见[图13-4](#seq_to_seq_diagram)左下角的网络）。这是一个*向量到序列网络*。例如，输入可以是图像（或CNN的输出），输出可以是该图像的标题。
- en: '![Diagram illustrating different neural network architectures: sequence-to-sequence,
    sequence-to-vector, vector-to-sequence, and encoder-decoder models.](assets/hmls_1304.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![说明不同神经网络架构的图：序列到序列、序列到向量、向量到序列和编码器-解码器模型。](assets/hmls_1304.png)'
- en: Figure 13-4\. Sequence-to-sequence (top left), sequence-to-vector (top right),
    vector-to-sequence (bottom left), and encoder-decoder (bottom right) networks
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4\. 序列到序列（左上角）、序列到向量（右上角）、向量到序列（左下角）和编码器-解码器（右下角）网络
- en: 'Lastly, you could have a sequence-to-vector network, called an *encoder*, followed
    by a vector-to-sequence network, called a *decoder* (see the bottom-right network
    of [Figure 13-4](#seq_to_seq_diagram)). For example, this could be used for translating
    a sentence from one language to another. You would feed the network a sentence
    in one language, the encoder would convert this sentence into a single vector
    representation, and then the decoder would decode this vector into a sentence
    in another language. This two-step model, called an [*encoder-decoder*](https://homl.info/seq2seq),⁠^([4](ch13.html#id3076))
    works much better than trying to translate on the fly with a single sequence-to-sequence
    RNN (like the one represented at the top left): the last words of a sentence can
    affect the first words of the translation, so you need to wait until you have
    seen the whole sentence before translating it. We will go through the implementation
    of an encoder-decoder in [Chapter 14](ch14.html#nlp_chapter) (as you will see,
    it is a bit more complex than what [Figure 13-4](#seq_to_seq_diagram) suggests).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以有一个序列到向量网络，称为*编码器*，后面跟着一个向量到序列网络，称为*解码器*（参见[图13-4](#seq_to_seq_diagram)右下角的网络）。例如，这可以用于将一种语言的句子翻译成另一种语言。你将网络输入一种语言的句子，编码器会将这个句子转换成一个单一的向量表示，然后解码器会将这个向量解码成另一种语言的句子。这个被称为[*编码器-解码器*](https://homl.info/seq2seq)的两步模型⁠^([4](ch13.html#id3076))比尝试使用单个序列到序列RNN（如左上角表示的那个）即时翻译要好得多：句子的最后几个词可能会影响翻译的第一个词，因此你需要等到看到整个句子后再进行翻译。我们将在[第14章](ch14.html#nlp_chapter)中介绍编码器-解码器的实现（正如你将看到的，它比[图13-4](#seq_to_seq_diagram)所暗示的要复杂一些）。
- en: This versatility sounds promising, but how do you train a recurrent neural network?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多功能性听起来很有希望，但你是如何训练一个循环神经网络的呢？
- en: Training RNNs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络训练
- en: To train an RNN, the trick is to unroll it through time (like we just did) and
    then use regular backpropagation (see [Figure 13-5](#bptt_diagram)). This strategy
    is called *backpropagation through time* (BPTT).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个RNN，技巧是将其在时间上展开（就像我们刚才做的那样），然后使用常规的反向传播（参见[图13-5](#bptt_diagram)）。这种策略被称为*时间反向传播*（BPTT）。
- en: Just like in regular backpropagation, there is a first forward pass through
    the unrolled network (represented by the dashed arrows). Then the output sequence
    is evaluated using a loss function ℒ(**Y**[(0)], **Y**[(1)], …​, **Y**[(*T*)];
    **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)]) (where **Y**[(*i*)] is the *i*^(th)
    target, **Ŷ**[(*i*)] is the *i*^(th) prediction, and *T* is the max time step).
    Note that this loss function may ignore some outputs. For example, in a sequence-to-vector
    RNN, all outputs are ignored except for the very last one. In [Figure 13-5](#bptt_diagram),
    the loss function is computed based on the last three outputs only. The gradients
    of that loss function are then propagated backward through the unrolled network
    (represented by the solid arrows). In this example, since the outputs **Ŷ**[(0)]
    and **Ŷ**[(1)] are not used to compute the loss, the gradients do not flow backward
    through them; they only flow through **Ŷ**[(2)], **Ŷ**[(3)], and **Ŷ**[(4)]. Moreover,
    since the same parameters **W** and **b** are used at each time step, their gradients
    will be tweaked multiple times during backprop. Once the backward phase is complete
    and all the gradients have been computed, BPTT can perform a gradient descent
    step to update the parameters (this is no different from regular backprop).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在常规反向传播中一样，有一个通过展开网络（由虚线箭头表示）的第一个正向传递。然后使用损失函数 ℒ(**Y**[(0)], **Y**[(1)], …​,
    **Y**[(*T*)]; **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)])（其中 **Y**[(*i*)] 是第 *i*
    个目标，**Ŷ**[(*i*)] 是第 *i* 个预测，*T* 是最大时间步）评估输出序列。请注意，这个损失函数可能会忽略一些输出。例如，在序列到向量的 RNN
    中，除了最后一个输出之外，所有输出都被忽略。在 [图 13-5](#bptt_diagram) 中，损失函数仅基于最后三个输出进行计算。然后，该损失函数的梯度通过展开网络（由实线箭头表示）向后传播。在这个例子中，由于输出
    **Ŷ**[(0)] 和 **Ŷ**[(1)] 没有用于计算损失，因此梯度不会通过它们向后传播；它们只通过 **Ŷ**[(2)], **Ŷ**[(3)],
    和 **Ŷ**[(4)]。此外，由于在每个时间步都使用相同的参数 **W** 和 **b**，它们的梯度将在反向传播期间多次调整。一旦反向阶段完成并且所有梯度都已计算，BPTT
    可以执行梯度下降步骤来更新参数（这与常规反向传播没有不同）。
- en: '![Diagram illustrating backpropagation through time in an unrolled RNN, showing
    forward and backward passes with gradients flowing only through certain outputs
    for loss computation.](assets/hmls_1305.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![说明在展开的 RNN 中进行时间反向传播的图表，显示带有梯度仅通过某些输出进行损失计算的向前和向后传递。](assets/hmls_1305.png)'
- en: Figure 13-5\. Backpropagation through time
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-5\. 时间反向传播
- en: Fortunately, PyTorch takes care of all of this complexity for you, as you will
    see. But before we get there, let’s load a time series and start analyzing it
    using classical tools to better understand what we’re dealing with, and to get
    some baseline metrics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PyTorch 会为你处理所有这些复杂性，正如你将看到的。但在我们到达那里之前，让我们加载一个时间序列，并使用经典工具开始分析它，以便更好地了解我们正在处理的内容，并获得一些基线指标。
- en: Forecasting a Time Series
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测
- en: All right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s
    Transit Authority. Your first task is to build a model capable of forecasting
    the number of passengers that will ride on bus and rail the next day. You have
    access to daily ridership data since 2001\. Let’s walk through how you would handle
    this. We’ll start by loading and cleaning up the data:^([5](ch13.html#id3084))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！让我们假设你刚刚被芝加哥交通管理局雇佣为数据科学家。你的第一个任务是构建一个能够预测第二天乘坐公交车和铁路的乘客数量的模型。你从 2001 年起就有每日客流量数据。让我们看看你会如何处理这个问题。我们将从加载数据并对其进行清理开始：^([5](ch13.html#id3084))
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load the CSV file, set short column names, sort the rows by date, remove
    the redundant `total` column, and drop duplicate rows. Now let’s check what the
    first few rows look like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载 CSV 文件，设置简短的列名，按日期排序行，删除冗余的 `total` 列，并删除重复的行。现在让我们看看前几行看起来像什么：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2][PRE3] On January 1st, 2001, 297,192 people boarded a bus in Chicago,
    and 126,455 boarded a train. The `day_type` column contains `W` for **W**eekdays,
    `A` for S**a**turdays, and `U` for S**u**ndays or holidays.    Now let’s plot
    the bus and rail ridership figures over a few months in 2019, to see what it looks
    like (see [Figure 13-6](#daily_ridership_plot)):    [PRE4]  ![Line graph showing
    daily bus and rail ridership in Chicago from March to May 2019, illustrating regular
    peaks and troughs over time.](assets/hmls_1306.png)  ###### Figure 13-6\. Daily
    ridership in Chicago    Note that Pandas includes both the start and end month
    in the range, so this plots the data from the 1st of March all the way up to the
    31st of May. This is a *time series*: data with values at different time steps,
    usually at regular intervals. More specifically, since there are multiple values
    per time step, this is called a *multivariate time series*. If we only looked
    at the `bus` column, it would be a *univariate time series*, with a single value
    per time step. Predicting future values (i.e., forecasting) is the most typical
    task when dealing with time series, and this is what we will focus on in this
    chapter. Other tasks include imputation (filling in missing past values), classification,
    anomaly detection, and more.    Looking at [Figure 13-6](#daily_ridership_plot),
    we can see that a similar pattern is clearly repeated every week. This is called
    a weekly *seasonality*. In fact, it’s so strong in this case that forecasting
    tomorrow’s ridership by just copying the values from a week earlier will yield
    reasonably good results. This is called *naive forecasting*: simply copying a
    past value to make our forecast. Naive forecasting is often a great baseline,
    and it can even be tricky to beat in some cases.    ###### Note    In general,
    naive forecasting means copying the latest known value (e.g., forecasting that
    tomorrow will be the same as today). However, in our case, copying the value from
    the previous week works better, due to the strong weekly seasonality.    To visualize
    these naive forecasts, let’s overlay the two time series (for bus and rail) as
    well as the same time series lagged by one week (i.e., shifted toward the right)
    using dotted lines. We’ll also plot the difference between the two (i.e., the
    value at time *t* minus the value at time *t* – 7); this is called *differencing*
    (see [Figure 13-7](#differencing_plot)):    [PRE5]  ![Two-panel plot showing the
    original and 7-day lagged time series for bus and rail (top), and the differences
    between them (bottom), demonstrating autocorrelation patterns.](assets/hmls_1307.png)  ######
    Figure 13-7\. Time series overlaid with 7-day lagged time series (top), and difference
    between *t* and *t* – 7 (bottom)    Not too bad! Notice how closely the lagged
    time series track the actual time series. When a time series is correlated with
    a lagged version of itself, we say that the time series is *autocorrelated*. As
    you can see, most of the differences are fairly small, except at the end of May.
    Maybe there was a holiday at that time? Let’s check the `day_type` column:    [PRE6]   [PRE7][PRE8]``py[PRE9]`py`
    [PRE10] [PRE11]`` [PRE12]` It looks like our `TimeSeriesDataset` class works fine!
    Now we can create a `DataLoader` for this tiny dataset, shuffling the windows
    and grouping them into batches of two:    [PRE13][PRE14]`` `>>>` `for` `X``,`
    `y` `in` `my_loader``:` [PRE15] `...` `` `X: tensor([[[0], [1], [2]], [[2], [3],
    [4]]])  y: tensor([[3], [5]])` `X: tensor([[[1], [2], [3]]])  y: tensor([[4]])`
    `` [PRE16]` [PRE17][PRE18]   [PRE19] [PRE20]`py [PRE21]py`` [PRE22]py` [PRE23]py
    [PRE24]py[PRE25][PRE26][PRE27][PRE28]py[PRE29]py`  [PRE30]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2][PRE3] 2001年1月1日，在芝加哥，297,192人乘坐公交车，126,455人乘坐火车。`day_type`列包含`W`代表**工作日**，`A`代表**星期六**，`U`代表**星期日或节假日**。现在让我们绘制2019年几个月的公交车和铁路客流量图，看看它是什么样子（见[图13-6](#daily_ridership_plot)）：    [PRE4]  ![Line
    graph showing daily bus and rail ridership in Chicago from March to May 2019,
    illustrating regular peaks and troughs over time.](assets/hmls_1306.png)  ######
    图13-6\. 芝加哥每日客流量    注意到Pandas包括范围中的起始和结束月份，因此此图显示了从3月1日到5月31日的数据。这是一个**时间序列**：在不同时间步长上有值的序列，通常间隔均匀。更具体地说，由于每个时间步长有多个值，这被称为**多元时间序列**。如果我们只看`bus`列，它将是一个**一元时间序列**，每个时间步长只有一个值。预测未来值（即，预测）是处理时间序列时最典型的任务，这也是本章我们将关注的重点。其他任务包括插补（填补过去缺失的值）、分类、异常检测等。    观察到[图13-6](#daily_ridership_plot)，我们可以看到每个星期都有明显的相似模式重复出现。这被称为**周季节性**。实际上，在这个案例中，它非常强烈，仅通过复制一周前的值来预测明天的客流量将产生合理的结果。这被称为**简单预测**：简单地复制过去的一个值来做出预测。简单预测通常是一个很好的基线，在某些情况下甚至可能很难超越。    ######
    注意    通常，简单预测意味着复制最新的已知值（例如，预测明天将与今天相同）。然而，在我们的案例中，由于强烈的周季节性，复制上周的值效果更好。    为了可视化这些简单预测，让我们使用虚线叠加两个时间序列（公交车和铁路）以及相同时间序列滞后一周（即，向右移动）的时间序列。我们还将绘制两者之间的差异（即，时间*t*的值减去时间*t*–7的值）；这被称为**差分**（见[图13-7](#differencing_plot)）：    [PRE5]  ![Two-panel
    plot showing the original and 7-day lagged time series for bus and rail (top),
    and the differences between them (bottom), demonstrating autocorrelation patterns.](assets/hmls_1307.png)  ######
    图13-7\. 原始和7天滞后时间序列叠加（顶部），以及*t*和*t*–7之间的差异（底部）    还不错！注意滞后时间序列如何紧密跟踪实际时间序列。当一个时间序列与其滞后版本相关时，我们说该时间序列是**自相关的**。正如你所见，大多数差异都相当小，除了五月底。也许那时有假期？让我们检查`day_type`列：    [PRE6]   [PRE7][PRE8]``py[PRE9]`py`
    [PRE10] [PRE11]`` [PRE12]` 看起来我们的`TimeSeriesDataset`类工作得很好！现在我们可以为这个小型数据集创建一个`DataLoader`，对窗口进行洗牌并将它们分成两个批次的组：    [PRE13][PRE14]``
    `>>>` `for` `X``,` `y` `in` `my_loader``:` [PRE15] `...` `` `X: tensor([[[0],
    [1], [2]], [[2], [3], [4]]])  y: tensor([[3], [5]])` `X: tensor([[[1], [2], [3]]])  y:
    tensor([[4]])` `` [PRE16]` [PRE17][PRE18]   [PRE19] [PRE20]`py [PRE21]py`` [PRE22]py`
    [PRE23]py [PRE24]py[PRE25][PRE26][PRE27][PRE28]py[PRE29]py`  [PRE30]'
