- en: Timeseries forecasting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter13_timeseries-forecasting](https://deeplearningwithpython.io/chapters/chapter13_timeseries-forecasting)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter13_timeseries-forecasting](https://deeplearningwithpython.io/chapters/chapter13_timeseries-forecasting)
- en: 'This chapter tackles timeseries, where temporal order is everything. We’ll
    focus on the most common and valuable timeseries task: forecasting. Using the
    recent past to predict the near future is a powerful capability, whether you’re
    trying to anticipate energy demand, manage inventory, or simply forecast the weather.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨时间序列，其中时间顺序至关重要。我们将重点关注最常见且最有价值的时间序列任务：预测。使用最近的过去来预测近未来的能力非常强大，无论您是试图预测能源需求、管理库存还是简单地预测天气。
- en: Different kinds of timeseries tasks
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同类型的时间序列任务
- en: A *timeseries* can be any data obtained via measurements at regular intervals,
    like the daily price of a stock, the hourly electricity consumption of a city,
    or the weekly sales of a store. Timeseries are everywhere, whether we’re looking
    at natural phenomena (like seismic activity, the evolution of fish populations
    in a river, or the weather at a location) or human activity patterns (like visitors
    to a website, a country’s GDP, or credit card transactions). Unlike the types
    of data you’ve encountered so far, working with timeseries involves understanding
    the *dynamics* of a system — its periodic cycles, how it trends over time, its
    regular regime, and its sudden spikes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *时间序列* 可以是任何通过定期测量获得的数据，例如股票的每日价格、城市的每小时电力消耗或商店的每周销售额。时间序列无处不在，无论是观察自然现象（如地震活动、河流中鱼类种群的变化或某地的天气）还是人类活动模式（如网站访问者、一个国家的GDP或信用卡交易）。与您迄今为止遇到的数据类型不同，处理时间序列需要理解系统的
    *动态* —— 它的周期性循环、随时间的变化趋势、常规状态和突然的峰值。
- en: 'By far, the most common timeseries-related task is *forecasting*: predicting
    what happens next in the series. Forecast electricity consumption a few hours
    in advance so you can anticipate demand, forecast revenue a few months in advance
    so you can plan your budget, forecast the weather a few days in advance so you
    can plan your schedule. Forecasting is what this chapter focuses on. But there’s
    actually a wide range of other things you can do with timeseries, such as'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，最常见的时间序列相关任务是 *预测*：预测序列中接下来会发生什么。提前几小时预测电力消耗，以便您可以预测需求；提前几个月预测收入，以便您可以规划预算；提前几天预测天气，以便您可以规划日程。预测是本章的重点。但实际上，您可以用时间序列做很多事情，例如
- en: '*Anomaly detection* — Detect anything unusual happening within a continuous
    data stream. Unusual activity on your corporate network? Might be an attacker.
    Unusual readings on a manufacturing line? Time for a human to go take a look.
    Anomaly detection is typically done via unsupervised learning, because you often
    don’t know what kind of anomaly you’re looking for, and thus you can’t train on
    specific anomaly examples.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测* —— 在连续数据流中检测任何异常事件。公司网络上的异常活动？可能是攻击者。生产线上的异常读数？是时候让人类去看看了。异常检测通常通过无监督学习来完成，因为您通常不知道您在寻找哪种异常，因此您无法在特定的异常示例上进行训练。'
- en: '*Classification* — Assign one or more categorical labels to a timeseries. For
    instance, given the timeseries of activity of a visitor on a website, classify
    whether the visitor is a bot or a human.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类* —— 将一个或多个分类标签分配给时间序列。例如，给定网站访问者活动的时间序列，判断该访问者是机器人还是人类。'
- en: '*Event detection* — Identify the occurrence of a specific, expected event within
    a continuous data stream. A particularly useful application is “hotword detection”,
    where a model monitors an audio stream and detects utterances like “OK, Google”
    or “Hey, Alexa.”'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事件检测* —— 在连续数据流中识别特定、预期的事件发生。特别有用的应用是“热词检测”，其中模型监控音频流并检测“OK, Google”或“Hey,
    Alexa”等语音。'
- en: In this chapter, you’ll learn about recurrent neural networks (RNNs) and how
    to apply them to timeseries forecasting.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解循环神经网络（RNNs）及其在时间序列预测中的应用。
- en: A temperature forecasting example
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个温度预测示例
- en: 'Throughout this chapter, all of our code examples will target a single problem:
    predicting the temperature 24 hours in the future, given a timeseries of hourly
    measurements of quantities such as atmospheric pressure and humidity, recorded
    over the recent past by a set of sensors on the roof of a building. As you will
    see, it’s a fairly challenging problem!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们所有的代码示例都将针对一个问题：根据最近过去由一组安装在建筑物屋顶的传感器记录的诸如大气压力和湿度等量的每小时测量值的时间序列，预测未来24小时的温度。正如你将看到的，这是一个相当具有挑战性的问题！
- en: 'We’ll use this temperature forecasting task to highlight what makes timeseries
    data fundamentally different from the kinds of datasets you’ve encountered so
    far, to show that densely connected networks and convolutional networks aren’t
    well equipped to deal with it, and to demonstrate a new kind of machine learning
    technique that really shines on this type of problem: recurrent neural networks
    (RNNs).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个温度预测任务来突出时间序列数据与迄今为止你遇到的数据集的基本不同之处，以表明密集连接的网络和卷积网络并不适合处理它，并展示一种新的机器学习技术，这种技术在处理这类问题上表现得非常出色：循环神经网络（RNNs）。
- en: We’ll work with a weather timeseries dataset recorded at the weather station
    at the Max Planck Institute for Biogeochemistry in Jena, Germany.^([[1]](#footnote-1))
    In this dataset, 14 different quantities (such as temperature, atmospheric pressure,
    humidity, wind direction, and so on) were recorded every 10 minutes, over several
    years. The original data goes back to 2003, but the subset of the data we’ll download
    is limited to 2009–2016.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用德国耶拿马克斯·普朗克生物地球化学研究所气象站记录的气象时间序列数据集进行工作.^([[1]](#footnote-1)) 在这个数据集中，记录了14个不同的量（如温度、大气压力、湿度、风向等），每隔10分钟记录一次，持续了数年。原始数据可以追溯到2003年，但我们将要下载的数据子集限制在2009-2016年。
- en: 'Let’s start by downloading and uncompressing the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载和解压缩数据：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s look at the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数据。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 13.1](#listing-13-1): Inspecting the data of the Jena weather dataset'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.1](#listing-13-1)：检查耶拿气象数据集的数据'
- en: 'This outputs a count of 420,551 lines of data (each line is a timestep: a record
    of a date and 14 weather-related values), as well as the following header:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出了420,551行数据的计数（每行是一个时间步长：一个日期和14个与天气相关的值的记录），以及以下标题：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, convert all 420,551 lines of data into NumPy arrays: one array for the
    temperature (in degrees Celsius), and another one for the rest of the data — the
    features we will use to predict future temperatures. Note that we discard the
    “Date Time” column.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将所有420,551行数据转换为NumPy数组：一个数组用于温度（以摄氏度为单位），另一个数组用于其余的数据——我们将使用这些特征来预测未来的温度。请注意，我们丢弃了“日期时间”列。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 13.2](#listing-13-2): Parsing the data'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.2](#listing-13-2)：解析数据'
- en: Figure 13.1 shows the plot of temperature (in degrees Celsius) over time. On
    this plot, you can clearly see the yearly periodicity of temperature — the data
    spans eight years.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1显示了温度（以摄氏度为单位）随时间变化的图表。在这张图上，你可以清楚地看到温度的年度周期性——数据跨度为八年。
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 13.3](#listing-13-3): Plotting the temperature timeseries'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.3](#listing-13-3)：绘制温度时间序列图'
- en: '![](../Images/149d51397c1deac9439c06ada3e22192.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/149d51397c1deac9439c06ada3e22192.png)'
- en: '[Figure 13.1](#figure-13-1): Temperature over the full temporal range of the
    dataset (ºC)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.1](#figure-13-1)：数据集整个时间范围内的温度（ºC）'
- en: Figure 13.2 shows a more narrow plot of the first 10 days of temperature data.
    Because the data is recorded every 10 minutes, you get 24 × 6 = 144 data points
    per day.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2显示了温度数据的前10天的更窄的图表。因为数据每10分钟记录一次，所以每天有24 × 6 = 144个数据点。
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 13.4](#listing-13-4): Plotting the first 10 days of the temperature
    timeseries'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.4](#listing-13-4)：绘制温度时间序列的前10天'
- en: '![](../Images/255b274a022adb5680908111339b3299.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/255b274a022adb5680908111339b3299.png)'
- en: '[Figure 13.2](#figure-13-2): Temperature over the first 10 days of the dataset
    (ºC)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.2](#figure-13-2)：数据集前10天的温度（ºC）'
- en: On this plot, you can see daily periodicity, especially evident for the last
    four days. Also note that this 10-day period must be coming from a fairly cold
    winter month.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图上，你可以看到日周期性，特别是在最后四天尤为明显。同时请注意，这个10天的周期必须来自一个相当寒冷的冬季月份。
- en: With our dataset, if you were trying to predict average temperature for the
    next month given a few months of past data, the problem would be easy, due to
    the reliable year-scale periodicity of the data. But looking at the data over
    a scale of days, the temperature looks a lot more chaotic. Is this timeseries
    predictable at a daily scale? Let’s find out.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，如果您试图根据几个月的过去数据预测下一个月的平均温度，这个问题将很容易，因为数据的可靠年周期性。但是，从日的时间尺度来看，温度看起来要混乱得多。这个时间序列在日尺度上可预测吗？让我们来看看。
- en: In all our experiments, we’ll use the first 50% of the data for training, the
    following 25% for validation, and the last 25% for testing. When working with
    timeseries data, it’s important to use validation and test data that is more recent
    than the training data because you’re trying to predict the future given the past,
    not the reverse, and your validation/test splits should reflect this temporal
    ordering. Some problems happen to be considerably simpler if you reverse the time
    axis!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所有的实验中，我们将使用数据的前50%进行训练，接下来的25%进行验证，最后的25%进行测试。当处理时间序列数据时，使用比训练数据更近期的验证和测试数据非常重要，因为您试图根据过去预测未来，而不是相反，并且您的验证/测试拆分应该反映这种时间顺序。如果反转时间轴，某些问题可能会变得相当简单！
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 13.5](#listing-13-5): Computing the number of samples for each data
    split'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.5](#listing-13-5)：计算每个数据拆分的样本数量'
- en: Preparing the data
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'The exact formulation of the problem will be as follows: given data covering
    the previous five days and sampled once per hour, can we predict the temperature
    in 24 hours?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的确切表述如下：给定覆盖前五天且每小时采样一次的数据，我们能否预测24小时后的温度？
- en: 'First, let’s preprocess the data to a format a neural network can ingest. This
    is easy: the data is already numerical, so you don’t need to do any vectorization.
    But each timeseries in the data is on a different scale (for example, atmospheric
    pressure, measured in mbar, is around 1,000, while H2OC, measured in millimoles
    per mole, is around 3). We’ll normalize each timeseries independently so that
    they all take small values on a similar scale. We’re going to use the first 210,225
    timesteps as training data, so we’ll compute the mean and standard deviation only
    on this fraction of the data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们预处理数据，使其成为神经网络可以摄入的格式。这很简单：数据已经是数值的，因此您不需要进行任何向量化。但是，数据中的每个时间序列都在不同的尺度上（例如，大气压力，以毫巴为单位，约为1,000，而H2OC，以每摩尔毫摩尔为单位，约为3）。我们将独立归一化每个时间序列，使它们都在相似的尺度上取小值。我们将使用前210,225个时间步作为训练数据，因此我们只计算这个数据分量的均值和标准差。
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 13.6](#listing-13-6): Normalizing the data'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.6](#listing-13-6)：数据归一化'
- en: Next, let’s create a `Dataset` object that yields batches of data from the past
    five days along with a target temperature 24 hours in the future. Because the
    samples in the dataset are highly redundant (sample `N` and sample `N + 1` will
    have most of their timesteps in common), it would be wasteful to explicitly allocate
    memory for every sample. Instead, we’ll generate the samples on the fly while
    only keeping in memory the original `raw_data` and `temperature` arrays, and nothing
    more.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个 `Dataset` 对象，它提供过去五天的数据批次以及未来24小时的温度目标。由于数据集中的样本高度冗余（样本 `N` 和样本
    `N + 1` 将有大部分时间步长是相同的），明确为每个样本分配内存将是浪费的。相反，我们将动态生成样本，同时只在内存中保留原始 `raw_data` 和
    `temperature` 数组，不再需要其他任何东西。
- en: We could easily write a Python generator to do this, but there’s a built-in
    dataset utility in Keras that does just that (`timeseries_dataset_from_array()`),
    so we can save ourselves some work by using it. You can generally use it for any
    kind of timeseries forecasting task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地编写一个Python生成器来完成这项工作，但Keras中有一个内置的数据集实用工具可以做到这一点（`timeseries_dataset_from_array()`），因此我们可以通过使用它来节省一些工作。您通常可以使用它来完成任何类型的时序预测任务。
- en: 'We’ll use `timeseries_dataset_from_array` to instantiate three datasets: one
    for training, one for validation, and one for testing.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `timeseries_dataset_from_array` 来实例化三个数据集：一个用于训练，一个用于验证，一个用于测试。
- en: 'We’ll use the following parameter values:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下参数值：
- en: '`sampling_rate = 6` — Observations will be sampled at one data point per hour:
    we will only keep one data point out of six.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate = 6` — 观测将以每小时一个数据点的频率进行采样：我们将只保留六个数据点中的一个。'
- en: '`sequence_length = 120` — Observations will go back five days (120 hours).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length = 120` — 观测将回溯五天（120小时）。'
- en: '`delay = sampling_rate * (sequence_length + 24 - 1)` — The target for a sequence
    will be the temperature 24 hours after the end of the sequence.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delay = sampling_rate * (sequence_length + 24 - 1)` — 序列的目标将是序列结束后的24小时温度。'
- en: '`start_index = 0` and `end_index = num_train_samples` — For the training dataset,
    to only use the first 50% of the data.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_index = 0` 和 `end_index = num_train_samples` — 对于训练数据集，仅使用前50%的数据。'
- en: '`start_index = num_train_samples` and `end_index = num_train_samples + num_val_samples`
    — For the validation dataset, to only use the next 25% of the data.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_index = num_train_samples` 和 `end_index = num_train_samples + num_val_samples`
    — 对于验证数据集，仅使用接下来的25%的数据。'
- en: '`start_index = num_train_samples + num_val_samples` — For the test dataset,
    to use the remaining samples.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_index = num_train_samples + num_val_samples` — 对于测试数据集，使用剩余的样本。'
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 13.7](#listing-13-7): Instantiating datasets for training, validation,
    and testing'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.7](#listing-13-7)：实例化用于训练、验证和测试的数据集'
- en: Each dataset yields a tuple `(samples, targets)`, where `samples` is a batch
    of 256 samples, each containing 120 consecutive hours of input data, and `targets`
    is the corresponding array of 256 target temperatures. Note that the samples are
    randomly shuffled, so two consecutive sequences in a batch (like `samples[0]`
    and `samples[1]`) aren’t necessarily temporally close.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集都产生一个元组 `(samples, targets)`，其中 `samples` 是包含256个样本的批次，每个样本包含120个连续小时的输入数据，而
    `targets` 是相应的256个目标温度数组。请注意，样本是随机打乱的，因此批次中的连续序列（如 `samples[0]` 和 `samples[1]`）不一定在时间上接近。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 13.8](#listing-13-8): Inspecting the dataset'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.8](#listing-13-8)：检查数据集'
- en: A commonsense, non-machine-learning baseline
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常识性、非机器学习基线
- en: Before you start using black box, deep learning models to solve the temperature
    prediction problem, let’s try a simple, commonsense approach. It will serve as
    a sanity check, and it will establish a baseline that you’ll have to beat to demonstrate
    the usefulness of more advanced, machine learning models. Such commonsense baselines
    can be useful when you’re approaching a new problem for which there is no known
    solution (yet). A classic example is that of unbalanced classification tasks,
    where some classes are much more common than others. If your dataset contains
    90% instances of class A and 10% instances of class B, then a commonsense approach
    to the classification task is to always predict “A” when presented with a new
    sample. Such a classifier is 90% accurate overall, and any learning-based approach
    should therefore beat this 90% score to demonstrate usefulness. Sometimes, such
    elementary baselines can prove surprisingly hard to beat.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始使用黑盒深度学习模型来解决温度预测问题之前，让我们尝试一种简单、常识性的方法。这将作为合理性检查，并建立一个基线，您必须超越这个基线才能证明更高级、基于机器学习模型的实用性。这种常识性基线在您面对一个尚未找到已知解决方案的新问题时可能很有用。一个经典的例子是不平衡分类任务，其中某些类别比其他类别更常见。如果您的数据集中包含90%的类别A实例和10%的类别B实例，那么对分类任务的常识性方法是，在呈现新样本时始终预测“A”。这样的分类器总体准确率为90%，因此任何基于学习的算法都应该超越这个90%的分数以证明其有用性。有时，这样的基本基线可能难以超越。
- en: 'In this case, the temperature timeseries can safely be assumed to be continuous
    (the temperatures tomorrow are likely to be close to the temperatures today) as
    well as periodic with a daily period. Thus, a commonsense approach is to always
    predict that the temperature 24 hours from now will be equal to the temperature
    right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric,
    defined as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，可以安全地假设温度时间序列是连续的（明天的温度很可能接近今天的温度），并且具有每日周期性。因此，一种常识性的方法是始终预测24小时后的温度将与现在的温度相等。让我们使用以下定义的均方误差（MAE）指标来评估这种方法：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here’s the evaluation loop.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是评估循环。
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 13.9](#listing-13-9): Computing the commonsense baseline MAE'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.9](#listing-13-9)：计算常识性基线MAE'
- en: This commonsense baseline achieves a validation MAE of 2.44 degrees Celsius,
    and a test MAE of 2.62 degrees Celsius. So if you always assume that the temperature
    24 hours in the future will be the same as it is now, you will be off by two and
    a half degrees on average. It’s not too bad, but you probably won’t launch a weather
    forecasting service based on this heuristic. Now, the game is to use your knowledge
    of deep learning to do better.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个常识性基线实现了2.44摄氏度的验证MAE和2.62摄氏度的测试MAE。所以如果你总是假设24小时后的温度将与现在相同，你平均会差两度半。这并不太糟糕，但你可能不会基于这个启发式方法推出天气预报服务。现在，游戏规则是利用你对深度学习的知识做得更好。
- en: Let’s try a basic machine learning model
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们尝试一个基本的机器学习模型
- en: In the same way that it’s useful to establish a commonsense baseline before
    trying machine learning approaches, it’s useful to try simple, cheap, machine
    learning models (such as small, densely connected networks) before looking into
    complicated and computationally expensive models such as RNNs. This is the best
    way to make sure any further complexity you throw at the problem is legitimate
    and delivers real benefits.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在尝试机器学习方法之前建立常识性基线一样有用，在考虑复杂且计算成本高的模型（如RNNs）之前，尝试简单的、便宜的机器学习模型（如小型、密集连接的网络）也是有用的。这是确保你向问题添加的任何进一步复杂性都是合法的并且能带来真正好处的方法。
- en: Listing 13.10 shows a fully connected model that starts by flattening the data
    and then runs it through two `Dense` layers. Note the lack of activation function
    on the last `Dense` layer, which is typical for a regression problem. We use mean
    squared error (MSE) as the loss, rather than MAE, because unlike MAE, it’s smooth
    around zero, a useful property for gradient descent. We will monitor MAE by adding
    it as a metric in `compile()`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.10显示了从数据开始扁平化，然后通过两个`Dense`层运行的完全连接模型。注意最后一个`Dense`层上缺少激活函数，这在回归问题中是典型的。我们使用均方误差（MSE）作为损失，而不是MAE，因为与MAE不同，它在零点周围是平滑的，这对于梯度下降是一个有用的属性。我们将通过在`compile()`中添加它作为度量来监控MAE。
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 13.10](#listing-13-10): Training and evaluating a densely connected
    model'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.10](#listing-13-10)：训练和评估密集连接模型'
- en: Let’s display the loss curves for validation and training (see figure 13.3).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示验证和训练的损失曲线（见图13.3）。
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 13.11](#listing-13-11): Plotting results'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.11](#listing-13-11)：绘制结果'
- en: '![](../Images/75470ada569d71086842110f2269e6be.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75470ada569d71086842110f2269e6be.png)'
- en: '[Figure 13.3](#figure-13-3): Training and validation MAE on the Jena temperature-forecasting
    task with a simple, densely connected network'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.3](#figure-13-3)：使用简单、密集连接网络在Jena温度预测任务上的训练和验证MAE'
- en: 'Some of the validation losses are close to the no-learning baseline, but not
    reliably. This goes to show the merit of having this baseline in the first place:
    it turns out to be not easy to outperform. Your common sense contains a lot of
    valuable information to which a machine learning model doesn’t have access.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些验证损失接近无学习基线，但并不可靠。这证明了最初建立这个基线的价值：结果证明很难超越。你的常识中包含了许多机器学习模型无法访问的有价值信息。
- en: You may wonder, if a simple, well-performing model exists to go from the data
    to the targets (the commonsense baseline), why doesn’t the model you’re training
    find it and improve on it? Well, the space of models in which you’re searching
    for a solution — that is, your hypothesis space — is the space of all possible
    two-layer networks with the configuration you defined. The commonsense heuristic
    is just one model among millions that can be represented in this space. It’s like
    looking for a needle in a haystack. Just because a good solution technically exists
    in your hypothesis space doesn’t mean you’ll be able to find it via gradient descent.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果存在一个简单且表现良好的模型可以从数据到目标（常识性基线）进行转换，为什么你正在训练的模型找不到并改进它？好吧，你正在寻找解决方案的模型空间——即你的假设空间——是你定义的配置下所有可能的二层网络的空间。常识性启发式方法只是这个空间中可以表示的数百万个模型之一。这就像在干草堆里找针一样。仅仅因为你的假设空间中技术上存在一个好的解决方案，并不意味着你能够通过梯度下降找到它。
- en: 'That’s a pretty significant limitation of machine learning in general: unless
    the learning algorithm is hardcoded to look for a specific kind of simple model,
    it can sometimes fail to find a simple solution to a simple problem. That’s why
    using good feature engineering and relevant architecture priors is essential:
    you need to be precisely telling your model what it should be looking for.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对机器学习的一般限制相当大的一个：除非学习算法被硬编码为寻找特定类型的简单模型，否则它有时可能无法找到简单问题的简单解决方案。这就是为什么使用良好的特征工程和相关的架构先验是至关重要的：你需要精确地告诉你的模型它应该寻找什么。
- en: Let’s try a 1D convolutional model
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们尝试一个 1D 卷积模型
- en: 'Speaking of using the right architecture priors: since our input sequences
    feature daily cycles, perhaps a convolutional model could work? A temporal ConvNet
    could reuse the same representations across different days, much like a spatial
    ConvNet can reuse the same representations across different locations in an image.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 说到使用正确的架构先验：由于我们的输入序列具有日周期，也许一个卷积模型可以工作？时间卷积网络可以在不同天之间重用相同的表示，就像空间卷积网络可以在图像的不同位置重用相同的表示一样。
- en: 'You already know about the `Conv2D` and `SeparableConv2D` layers, which see
    their inputs through small windows that swipe across 2D grids. There are also
    1D and even 3D versions of these layers: `Conv1D`, `SeparableConv1D`, and `Conv3D`.^([[2]](#footnote-2))
    The `Conv1D` layer relies on 1D windows that slide across input sequences, and
    the `Conv3D` layer relies on cubic windows that slide across input volumes.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解 `Conv2D` 和 `SeparableConv2D` 层，这些层通过小窗口在 2D 网格上滑动来观察其输入。这些层也有 1D 和甚至 3D
    的版本：`Conv1D`、`SeparableConv1D` 和 `Conv3D`。^([[2]](#footnote-2)) `Conv1D` 层依赖于在输入序列上滑动的
    1D 窗口，而 `Conv3D` 层依赖于在输入体积上滑动的立方窗口。
- en: You can thus build 1D ConvNets, strictly analogous to 2D ConvNets. They’re a
    great fit for any sequence data that follows the translation invariance assumption
    (meaning that if you slide a window over the sequence, the content of the window
    should follow the same properties independently of the location of the window).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，你可以构建 1D ConvNets，严格类似于 2D ConvNets。它们非常适合遵循平移不变性假设的任何序列数据（这意味着如果你在序列上滑动一个窗口，窗口的内容应该独立于窗口的位置遵循相同的属性）。 '
- en: 'Let’s try one on our temperature forecasting problem. We’ll pick an initial
    window length of 24, so that we look at 24 hours of data at a time (one cycle).
    As we downsample the sequences (via `MaxPooling1D` layers), we’ll reduce the window
    size accordingly (figure 13.4):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在温度预测问题上尝试一下。我们将选择一个初始窗口长度为 24，这样我们一次查看 24 小时的数据（一个周期）。当我们通过 `MaxPooling1D`
    层下采样序列时，我们将相应地减小窗口大小（图 13.4）：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/9c13e3309cef8c5ac98ecbb49d2581a7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c13e3309cef8c5ac98ecbb49d2581a7.png)'
- en: '[Figure 13.4](#figure-13-4): Training and validation MAE on the Jena temperature
    forecasting task with a 1D ConvNet'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.4](#figure-13-4)：使用 1D ConvNet 在 Jena 温度预测任务上的训练和验证 MAE'
- en: 'As it turns out, this model performs even worse than the densely connected
    one, only achieving a validation MAE of about 2.9 degrees, far from the commonsense
    baseline. What went wrong here? Two things:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个模型的表现甚至比密集连接的模型还要差，只实现了大约 2.9 度的验证 MAE，远低于常识基线。这里出了什么问题？两件事：
- en: First, weather data doesn’t quite respect the translation invariance assumption.
    While the data does feature daily cycles, data from a morning follows different
    properties than data from an evening or from the middle of the night. Weather
    data is only translation-invariant for a very specific timescale.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，天气数据并不完全遵守平移不变性假设。虽然数据确实具有日周期，但早晨的数据与傍晚或半夜的数据具有不同的属性。天气数据只在非常特定的时间尺度上具有平移不变性。
- en: Second, order in our data matters — a lot. The recent past is far more informative
    for predicting the next day’s temperature than data from five days ago. A 1D ConvNet
    is not able to make use of this fact. In particular, our max pooling and global
    average pooling layers are largely destroying order information.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们数据中的顺序很重要——非常重要。最近的数据对于预测第二天温度的信息量远大于五天前的数据。1D ConvNet 无法利用这一事实。特别是，我们的最大池化和全局平均池化层在很大程度上破坏了顺序信息。
- en: Recurrent neural networks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'Neither the fully connected approach nor the convolutional approach did well,
    but that doesn’t mean machine learning isn’t applicable to this problem. The densely
    connected approach first flattened the timeseries, which removed the notion of
    time from the input data. The convolutional approach treated every segment of
    the data in the same way, even applying pooling, which destroyed order information.
    Let’s instead look at the data as what it is: a sequence, where causality and
    order matter.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是全连接方法还是卷积方法都没有取得好成绩，但这并不意味着机器学习不适用于这个问题。全连接方法首先将时间序列展平，这从输入数据中移除了时间的概念。卷积方法以相同的方式处理数据的每个部分，甚至应用池化，这破坏了顺序信息。让我们相反地看待数据：它是一个序列，其中因果关系和顺序很重要。
- en: 'There’s a family of neural network architectures that were designed specifically
    for this use case: recurrent neural networks. Among them, the Long Short-Term
    Memory (LSTM) layer in particular has long been very popular. We’ll see in a minute
    how these models work — but let’s start by giving the LSTM layer a try.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种神经网络架构族是专门为这种用例设计的：循环神经网络。其中，特别是长短期记忆（LSTM）层长期以来一直非常受欢迎。我们将在下一分钟看到这些模型是如何工作的——但让我们先尝试一下
    LSTM 层。
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 13.12](#listing-13-12): A simple LSTM-based model'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.12](#listing-13-12)：一个简单的基于 LSTM 的模型'
- en: Figure 13.5 shows the results. Much better! We achieve a validation MAE as low
    as 2.39 degrees and a test MAE of 2.55 degrees. The LSTM-based model can finally
    beat the commonsense baseline (albeit just by a bit, for now), demonstrating the
    value of machine learning on this task.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 展示了结果。好多了！我们实现了验证 MAE 低至 2.39 度，测试 MAE 为 2.55 度。基于 LSTM 的模型终于打败了常识基线（尽管目前只是略微领先），展示了机器学习在此任务中的价值。
- en: '![](../Images/31db8edd4eaf08143fe010bd06572a76.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31db8edd4eaf08143fe010bd06572a76.png)'
- en: '[Figure 13.5](#figure-13-5): Training and validation MAE on the Jena temperature
    forecasting task with an LSTM-based model. (Note that we omit epoch 1 on this
    graph because the high training MAE (7.75) at epoch 1 would distort the scale.)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.5](#figure-13-5)：使用基于 LSTM 的模型在耶拿温度预测任务上的训练和验证 MAE。（注意，我们在此图中省略了第 1 个
    epoch，因为第 1 个 epoch 的高训练 MAE（7.75）会扭曲比例。）'
- en: But why did the LSTM model perform markedly better than the densely connected
    one or the ConvNet? And how can we further refine the model? To answer this, let’s
    take a closer look at recurrent neural networks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么 LSTM 模型的表现明显优于密集连接模型或卷积神经网络？我们如何进一步优化模型？为了回答这个问题，让我们更深入地研究循环神经网络。
- en: Understanding recurrent neural networks
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解循环神经网络
- en: 'A major characteristic of all neural networks you’ve seen so far, such as densely
    connected networks and ConvNets, is that they have no memory. Each input shown
    to them is processed independently, with no state kept between inputs. With such
    networks, to process a sequence or a temporal series of data points, you have
    to show the entire sequence to the network at once: turn it into a single data
    point. For instance, this is what we did in the densely connected network example:
    we flattened our five days of data into a single large vector and processed it
    in one go. Such networks are called *feedforward networks*.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你迄今为止看到的所有神经网络的主要特征，如密集连接网络和卷积神经网络，是它们没有记忆。它们展示给每个输入的处理都是独立的，输入之间没有保持状态。对于这样的网络，要处理一个序列或时间序列数据点，你必须一次性将整个序列展示给网络：将其转换成一个单一的数据点。例如，这就是我们在密集连接网络示例中所做的：我们将五天的数据展平成一个大的向量，并一次性处理它。这样的网络被称为
    *前馈网络*。
- en: In contrast, as you’re reading the present sentence, you’re processing it word
    by word — or rather, eye saccade by eye saccade — while keeping memories of what
    came before; this gives you a fluid representation of the meaning conveyed by
    this sentence. Biological intelligence processes information incrementally while
    maintaining an internal model of what it’s processing, built from past information
    and constantly updated as new information comes in.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当你阅读本句时，你正在逐字逐句地处理它——或者说，通过眼球运动逐个处理——同时保持对之前内容的记忆；这为你提供了对句子所传达意义的流畅表征。生物智能在处理信息时是逐步进行的，同时保持一个内部模型，该模型由过去的信息构建而成，并随着新信息的到来而不断更新。
- en: 'A *recurrent neural network* (RNN) adopts the same principle, albeit in an
    extremely simplified version: it processes sequences by iterating through the
    sequence elements and maintaining a *state* containing information relative to
    what it has seen so far. In effect, an RNN is a type of neural network that has
    an internal *loop* (see figure 13.6).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *循环神经网络* (RNN) 采用相同的原理，尽管是一个极其简化的版本：它通过迭代序列元素并维护一个包含有关其迄今为止所看到的信息的 *状态* 来处理序列。实际上，RNN是一种具有内部
    *循环* 的神经网络（见图13.6）。
- en: '![](../Images/f1d1ae7492ad9b66f897b9d67dfd509c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/f1d1ae7492ad9b66f897b9d67dfd509c.png)'
- en: '[Figure 13.6](#figure-13-6): A recurrent network: a network with a loop'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.6](#figure-13-6)：循环网络：具有循环的网络'
- en: 'The state of the RNN is reset between processing two different, independent
    sequences (such as two samples in a batch), so you still consider one sequence
    to be a single data point: a single input to the network. What changes is that
    this data point is no longer processed in a single step; rather, the network internally
    loops over sequence elements.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理两个不同、独立的序列（例如批处理中的两个样本）之间，RNN的状态被重置，因此你仍然将一个序列视为一个单独的数据点：网络的单一输入。变化的是，这个数据点不再在单个步骤中处理；相反，网络内部遍历序列元素。
- en: To make these notions of *loop* and *state* clear, let’s implement the forward
    pass of a toy RNN. This RNN takes as input a sequence of vectors, which we’ll
    encode as a rank-2 tensor of size `(timesteps, input_features)`. It loops over
    timesteps, and at each timestep, it considers its current state at `t` and the
    input at `t` (of shape `(input_features,)`), and combines them to obtain the output
    at `t`. We’ll then set the state for the next step to be this previous output.
    For the first timestep, the previous output isn’t defined; hence, there is no
    current state. So we’ll initialize the state as an all-zero vector called the
    *initial* state of the network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些关于 *循环* 和 *状态* 的概念清晰，让我们实现一个玩具RNN的前向传递。这个RNN将一个向量序列作为输入，我们将它编码为一个大小为 `(timesteps,
    input_features)` 的二维张量。它遍历时间步长，并在每个时间步长，它考虑其在 `t` 时的当前状态和 `t` 时的输入（形状为 `(input_features,)`），并将它们结合起来以获得
    `t` 时的输出。然后我们将下一个步骤的状态设置为这个先前的输出。对于第一个时间步长，先前的输出没有定义；因此，没有当前状态。所以我们将状态初始化为名为网络的
    *初始状态* 的全零向量。
- en: In pseudocode, this is the RNN.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪代码中，这是RNN。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 13.13](#listing-13-13): Pseudocode RNN'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.13](#listing-13-13)：伪代码RNN'
- en: 'You can even flesh out the function `f`: the transformation of the input and
    state into an output will be parameterized by two matrices, `W` and `U`, and a
    bias vector. It’s similar to the transformation operated by a densely connected
    layer in a feedforward network.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以细化函数 `f`：输入和状态到输出的转换将由两个矩阵 `W` 和 `U` 以及一个偏置向量来参数化。这类似于前馈网络中密集连接层所进行的转换。
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 13.14](#listing-13-14): More detailed pseudocode for the RNN'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.14](#listing-13-14)：RNN的更详细伪代码'
- en: To make these notions absolutely unambiguous, let’s write a naive NumPy implementation
    of the forward pass of the simple RNN.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些概念绝对明确，让我们编写一个简单的NumPy实现，用于简单RNN的前向传递。
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Listing 13.15](#listing-13-15): NumPy implementation of a simple RNN'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.15](#listing-13-15)：简单RNN的NumPy实现'
- en: 'Easy enough: in summary, an RNN is a `for` loop that reuses quantities computed
    during the previous iteration of the loop, nothing more. Of course, there are
    many different RNNs fitting this definition that you could build — this example
    is one of the simplest RNN formulations. RNNs are characterized by their step
    function, such as the following function in this case (see figure 13.7):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 足够简单：总的来说，RNN是一个 `for` 循环，它重复使用循环前一次迭代中计算出的量，仅此而已。当然，有许多不同的RNN符合这个定义，你可以构建——这个例子是其中最简单的RNN公式之一。RNN以其步函数为特征，例如本例中的以下函数（见图13.7）：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/92f69e6ec62d80d9da42bbc0accab9b9.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/92f69e6ec62d80d9da42bbc0accab9b9.png)'
- en: '[Figure 13.7](#figure-13-7): A simple RNN, unrolled over time'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.7](#figure-13-7)：随时间展开的简单RNN'
- en: A recurrent layer in Keras
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras中的循环层
- en: The process you just naively implemented in NumPy corresponds to an actual Keras
    layer — the `SimpleRNN` layer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚才在NumPy中天真地实现的流程对应于实际的Keras层——`SimpleRNN` 层。
- en: 'There is one minor difference: `SimpleRNN` processes batches of sequences,
    like all other Keras layers, not a single sequence as in the NumPy example. This
    means it takes inputs of shape `(batch_size, timesteps, input_features)` rather
    than `(timesteps, input_features)`. When specifying the `shape` argument of your
    initial `Input()`, note that you can set the `timesteps` entry to `None`, which
    enables your network to process sequences of arbitrary length.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个细微的差别：`SimpleRNN` 处理序列批次，就像所有其他 Keras 层一样，而不是像 NumPy 示例中的单个序列。这意味着它接受形状为
    `(batch_size, timesteps, input_features)` 的输入，而不是 `(timesteps, input_features)`。当指定你的初始
    `Input()` 的 `shape` 参数时，请注意你可以将 `timesteps` 项设置为 `None`，这使你的网络能够处理任意长度的序列。
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 13.16](#listing-13-16): An RNN layer that can process sequences of
    any length'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.16](#listing-13-16)：一个可以处理任意长度序列的 RNN 层'
- en: This is especially useful if your model is meant to process sequences of variable
    length. However, if all of your sequences have the same length, I recommend specifying
    a complete input shape, since it enables `model.summary()` to display output length
    information, which is always nice, and it can unlock some performance optimizations
    (see the note “On RNN runtime performance” later in the chapter).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型旨在处理可变长度的序列，这尤其有用。然而，如果你的所有序列长度都相同，我建议指定完整的输入形状，因为它使 `model.summary()`
    能够显示输出长度信息，这总是很棒，并且它可以解锁一些性能优化（请参阅本章后面的“关于 RNN 运行时性能”的注释）。
- en: 'All recurrent layers in Keras (`SimpleRNN`, `LSTM`, and `GRU`) can be run in
    two different modes: they can return either full sequences of successive outputs
    for each timestep (a rank-3 tensor of shape `(batch_size, timesteps, output_features)`)
    or only the last output for each input sequence (a rank-2 tensor of shape `(batch_size,
    output_features)`). These two modes are controlled by the `return_sequences` constructor
    argument. Let’s look at an example that uses `SimpleRNN` and returns only the
    output at the last timestep.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的所有循环层（`SimpleRNN`、`LSTM` 和 `GRU`）都可以以两种不同的模式运行：它们可以返回每个时间步的连续输出序列（形状为
    `(batch_size, timesteps, output_features)` 的秩 3 张量）或每个输入序列的最后一个输出（形状为 `(batch_size,
    output_features)` 的秩 2 张量）。这两个模式由 `return_sequences` 构造函数参数控制。让我们看看使用 `SimpleRNN`
    并仅返回最后一个时间步输出的示例。
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 13.17](#listing-13-17): An RNN layer that returns only its last output
    step'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.17](#listing-13-17)：一个仅返回其最后一个输出步骤的 RNN 层'
- en: The following example returns the full output sequence.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例返回完整的输出序列。
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Listing 13.18](#listing-13-18): An RNN layer that returns its full output
    sequence'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.18](#listing-13-18)：一个返回其完整输出序列的 RNN 层'
- en: It’s sometimes useful to stack several recurrent layers one after the other
    to increase the representational power of a network. In such a setup, you have
    to get all of the intermediate layers to return the full sequence of outputs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将几个循环层一个接一个地堆叠以增加网络的表示能力是有用的。在这种设置中，你必须确保所有中间层都返回完整的输出序列。
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 13.19](#listing-13-19): Stacking RNN layers'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.19](#listing-13-19)：堆叠 RNN 层'
- en: 'Now, in practice, you’ll rarely work with the `SimpleRNN` layer. It’s generally
    too simplistic to be of real use. In particular, `SimpleRNN` has a major issue:
    although it should theoretically be able to retain at time `t` information about
    inputs seen many timesteps before, in practice, such long-term dependencies prove
    impossible to learn. This is due to the *vanishing gradients problem*, an effect
    that is similar to what is observed with non-recurrent networks (feedforward networks)
    that are many layers deep: as you keep adding layers to a network, the network
    eventually becomes untrainable. The theoretical reasons for this effect were studied
    by Hochreiter, Schmidhuber, and Bengio in the early 1990s.^([[3]](#footnote-3))'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在实践中，你很少会使用 `SimpleRNN` 层。它通常过于简单，没有实际用途。特别是，`SimpleRNN` 有一个主要问题：尽管从理论上讲，它应该能够在时间
    `t` 保留关于许多时间步之前看到的输入的信息，但在实践中，这样的长期依赖关系证明是无法学习的。这是由于 *梯度消失问题*，这是一种类似于观察到的非循环网络（前馈网络）深层的效应：当你继续向网络添加层时，网络最终变得无法训练。这种效应的理论原因在
    20 世纪 90 年代初期由 Hochreiter、Schmidhuber 和 Bengio 研究了.^([[3]](#footnote-3))
- en: 'Thankfully, `SimpleRNN` isn’t the only recurrent layer available in Keras.
    There are two others: `LSTM` and `GRU`, which were designed to address these issues.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`SimpleRNN` 并不是 Keras 中唯一的循环层。还有两个其他选项：`LSTM` 和 `GRU`，它们被设计来解决这个问题。
- en: Let’s consider the `LSTM` layer. The underlying Long Short-Term Memory (LSTM)
    algorithm was developed by Hochreiter and Schmidhuber in 1997;^([[4]](#footnote-4))
    it was the culmination of their research on the vanishing gradients problem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 `LSTM` 层。在 1997 年，Hochreiter 和 Schmidhuber 开发了底层的长短期记忆（LSTM）算法；^([[4]](#footnote-4))
    这是他们关于梯度消失问题的研究的高潮。
- en: 'This layer is a variant of the `SimpleRNN` layer you already know about; it
    adds a way to carry information across many timesteps. Imagine a conveyor belt
    running parallel to the sequence you’re processing. Information from the sequence
    can jump onto the conveyor belt at any point, be transported to a later timestep,
    and jump off, intact, when you need it. This is essentially what LSTM does: it
    saves information for later, thus preventing older signals from gradually vanishing
    during processing. This should remind you of *residual connections*, which you
    learned about in chapter 9: it’s pretty much the same idea.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层是你已经了解的 `SimpleRNN` 层的一个变体；它增加了一种在许多时间步长之间传递信息的方式。想象一下一个与你要处理的序列平行运行的传送带。序列中的信息可以在任何一点跳上传送带，被运送到一个更晚的时间步长，并在你需要时完整地跳下来。这正是
    LSTM 所做的：它为以后保存信息，从而防止在处理过程中较老的信号逐渐消失。这应该让你想起在第 9 章中学到的 *残差连接*，基本上是同一个想法。
- en: To understand this process in detail, let’s start from the `SimpleRNN` cell
    (see figure 13.8). Because you’ll have a lot of weight matrices, index the `W`
    and `U` matrices in the cell with the letter `o` (`Wo` and `Uo`) for *output*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解这一过程，让我们从 `SimpleRNN` 单元开始（见图 13.8）。因为你会有很多权重矩阵，所以用字母 `o`（`Wo` 和 `Uo`）对单元中的
    `W` 和 `U` 矩阵进行索引，代表 *output*。
- en: '![](../Images/92f69e6ec62d80d9da42bbc0accab9b9.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92f69e6ec62d80d9da42bbc0accab9b9.png)'
- en: '[Figure 13.8](#figure-13-8): The starting point of an `LSTM` layer: a `SimpleRNN`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.8](#figure-13-8)：`LSTM` 层的起点：一个 `SimpleRNN`'
- en: 'Let’s add to this picture an additional data flow that carries information
    across timesteps. Call its values at different timesteps `Ct`, where *C* stands
    for *carry*. This information will have the following effect on the cell: it will
    be combined with the input connection and the recurrent connection (via a dense
    transformation: a dot product with a weight matrix followed by a bias add and
    the application of an activation function), and it will affect the state being
    sent to the next timestep (via an activation function and a multiplication operation).
    Conceptually, the carry dataflow is a way to modulate the next output and the
    next state (see figure 13.9). Simple so far.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向这幅图添加一个额外的数据流，它携带信息跨越时间步长。在不同的时间步长中，称其值为 `Ct`，其中 *C* 代表 *carry*。这一信息将对细胞产生以下影响：它将与输入连接和循环连接（通过一个密集变换：与权重矩阵的点积，然后加上偏差并应用激活函数）相结合，并且它将影响发送到下一个时间步长的状态（通过一个激活函数和一个乘法操作）。从概念上讲，携带数据流是一种调节下一个输出和下一个状态的方式（见图
    13.9）。到目前为止，很简单。
- en: '![](../Images/3cadb26dfc414ebb62acb849eb21e2d2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cadb26dfc414ebb62acb849eb21e2d2.png)'
- en: '[Figure 13.9](#figure-13-9): Going from a SimpleRNN to an LSTM: adding a carry
    track'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.9](#figure-13-9)：从 SimpleRNN 到 LSTM：添加一个携带轨迹'
- en: 'Now the subtlety: the way the next value of the carry dataflow is computed.
    It involves three distinct transformations. All three have the form of a `SimpleRNN`
    cell:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的微妙之处：计算携带数据流下一个值的方式。它涉及三个不同的变换。所有三个都具有 `SimpleRNN` 单元的形态：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: But all three transformations have their own weight matrices, which you’ll index
    with the letters `i`, `f`, and `k`. Here’s what you have so far (it may seem a
    bit arbitrary, but bear with me).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 但所有三个变换都有自己的权重矩阵，你将用字母 `i`、`f` 和 `k` 来索引。到目前为止，你有的如下（可能看起来有点随意，但请耐心等待）。
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 13.20](#listing-13-20): Pseudocode details of the LSTM architecture
    (1/2)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.20](#listing-13-20)：LSTM 架构的伪代码细节（1/2）'
- en: You obtain the new carry state (the next `c_t`) by combining `i_t`, `f_t`, and
    `k_t`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过结合 `i_t`、`f_t` 和 `k_t` 来获得新的携带状态（下一个 `c_t`）。
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 13.21](#listing-13-21): Pseudocode details of the LSTM architecture
    (2/2)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.21](#listing-13-21)：LSTM 架构的伪代码细节（2/2）'
- en: Add this as shown in figure 13.10\. And that’s it. Not so complicated — merely
    a tad complex.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 13.10 所示添加。就是这样。并不复杂——只是稍微有点复杂。
- en: '![](../Images/fdb06283cb3f12f34e07d4153b093988.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdb06283cb3f12f34e07d4153b093988.png)'
- en: '[Figure 13.10](#figure-13-10): Anatomy of an `LSTM`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.10](#figure-13-10)：`LSTM` 的解剖结构'
- en: If you want to get philosophical, you can interpret what each of these operations
    is meant to do. For instance, you can say that multiplying `c_t` and `f_t` is
    a way to deliberately forget irrelevant information in the carry dataflow. Meanwhile,
    `i_t` and `k_t` provide information about the present, updating the carry track
    with new information. But at the end of the day, these interpretations don’t mean
    much because what these operations *actually* do is determined by the contents
    of the weights parameterizing them, and the weights are learned in an end-to-end
    fashion, starting over with each training round, making it impossible to credit
    this or that operation with a specific purpose. The specification of an RNN cell
    (as just described) determines your hypothesis space — the space in which you’ll
    search for a good model configuration during training — but it doesn’t determine
    what the cell does; that is up to the cell weights. The same cell with different
    weights can be doing very different things. So the combination of operations making
    up an RNN cell is better interpreted as a set of *constraints* on your search,
    not as a *design* in an engineering sense.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从哲学的角度来解释，你可以解释这些操作的目的。例如，你可以认为乘以 `c_t` 和 `f_t` 是在携带数据流中故意忘记无关信息的一种方式。同时，`i_t`
    和 `k_t` 提供了关于当前的信息，用新信息更新携带轨迹。但最终，这些解释并没有太多意义，因为这些操作*实际上*做什么是由参数化它们的权重内容决定的，而权重是以端到端的方式学习的，每次训练循环都会重新开始，这使得无法将特定目的归因于这个或那个操作。正如刚才所描述的，RNN
    单元的指定确定了你的假设空间——你在训练过程中将搜索良好模型配置的空间——但它并不确定单元做什么；这取决于单元权重。具有不同权重的相同单元可以执行非常不同的操作。因此，构成
    RNN 单元的操作组合最好解释为对搜索的*约束*，而不是从工程角度的*设计*。
- en: 'Arguably, the choice of such constraints — the question of how to implement
    RNN cells — is better left to optimization algorithms (like genetic algorithms
    or reinforcement learning processes) than to human engineers. In the future, that’s
    how we’ll build our models. In summary, you don’t need to understand anything
    about the specific architecture of an LSTM cell; as a human, it shouldn’t be your
    job to understand it. Just keep in mind what the LSTM cell is meant to do: allow
    past information to be reinjected at a later time, thus fighting the vanishing
    gradients problem.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 争议性地，这种约束的选择——如何实现 RNN 单元的疑问——最好留给优化算法（如遗传算法或强化学习过程）而不是人类工程师。在未来，我们将以此方式构建我们的模型。总之，你不需要了解
    LSTM 单元的具体架构；作为人类，这不应该成为你的工作。只需记住 LSTM 单元的目的：允许过去的信息在稍后时间重新注入，从而对抗梯度消失问题。
- en: Getting the most out of recurrent neural networks
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 充分利用循环神经网络
- en: By this point, you’ve learned
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了
- en: What RNNs are and how they work
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 是什么以及它们是如何工作的
- en: What an LSTM is and why it works better on long sequences than a naive RNN
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 是什么以及为什么它在长序列上比简单的 RNN 工作得更好
- en: How to use Keras RNN layers to process sequence data
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Keras RNN 层处理序列数据
- en: Next, we’ll review a number of more advanced features of RNNs, which can help
    you get the most out of your deep learning sequence models. By the end of the
    section, you’ll know most of what there is to know about using recurrent networks
    with Keras.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾 RNN 的许多更高级功能，这些功能可以帮助你充分利用你的深度学习序列模型。到本节结束时，你将了解关于使用 Keras 中的循环网络的大部分知识。
- en: 'We’ll cover the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下内容：
- en: '*Recurrent dropout*  — This is a variant of dropout, used to fight overfitting
    in recurrent layers.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*循环dropout*  — 这是 dropout 的一种变体，用于对抗循环层中的过拟合。'
- en: '*Stacking recurrent layers*  — This increases the representational power of
    the model (at the cost of higher computational loads).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*堆叠循环层*  — 这增加了模型的表示能力（但以更高的计算负载为代价）。'
- en: '*Bidirectional recurrent layers* — These present the same information to a
    recurrent network in different ways, increasing accuracy and mitigating forgetting
    issues.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*双向循环层* — 这些层以不同的方式向循环网络呈现相同的信息，从而提高准确率并减轻遗忘问题。'
- en: We’ll use these techniques to refine our temperature forecasting RNN.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些技术来完善我们的温度预测 RNN。
- en: Using recurrent dropout to fight overfitting
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用循环dropout来对抗过拟合
- en: 'Let’s go back to the LSTM-based model we used earlier in the chapter — our
    first model able to beat the commonsense baseline. If you look at the training
    and validation curves, it’s evident that the model is quickly overfitting, despite
    only having very few units: the training and validation losses start to diverge
    considerably after a few epochs. You’re already familiar with a classic technique
    for fighting this phenomenon: dropout, which randomly zeros out input units of
    a layer to break happenstance correlations in the training data that the layer
    is exposed to. But how to correctly apply dropout in recurrent networks isn’t
    a trivial question.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章早期使用的基于LSTM的模型——我们第一个能够击败常识基线的模型。如果你查看训练和验证曲线，很明显，尽管模型只有非常少的单元，但它很快就开始过拟合了：训练和验证损失在几个epoch之后开始显著发散。你已经熟悉了对抗这种现象的经典技术：dropout，它随机将层的输入单元置零，以打破层所暴露的训练数据中的偶然相关性。但是，如何在循环网络中正确应用dropout并不是一个简单的问题。
- en: 'It has long been known that applying dropout before a recurrent layer hinders
    learning rather than helping with regularization. In 2015, Yarin Gal, as part
    of his PhD thesis on Bayesian deep learning,^([[5]](#footnote-5)) determined the
    proper way to use dropout with a recurrent network: the same dropout mask (the
    same pattern of dropped units) should be applied at every timestep, instead of
    a dropout mask that varies randomly from timestep to timestep. What’s more, to
    regularize the representations formed by the recurrent gates of layers such as
    `GRU` and `LSTM`, a temporally constant dropout mask should be applied to the
    inner recurrent activations of the layer (a recurrent dropout mask). Using the
    same dropout mask at every timestep allows the network to properly propagate its
    learning error through time; a temporally random dropout mask would disrupt this
    error signal and be harmful to the learning process.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 很早就已经知道，在循环层之前应用dropout会阻碍学习而不是帮助正则化。2015年，Yarin Gal在他的关于贝叶斯深度学习的博士论文中，^([[5]](#footnote-5))确定了在循环网络中使用dropout的正确方法：应该在每个timestep应用相同的dropout掩码（相同的丢弃单元模式），而不是在每个timestep随机变化的dropout掩码。更重要的是，为了正则化由`GRU`和`LSTM`等层的循环门形成的表示，应该对层的内部循环激活应用时间常数dropout掩码（循环dropout掩码）。在每个timestep使用相同的dropout掩码允许网络正确地通过时间传播其学习误差；时间随机的dropout掩码会破坏这个误差信号，对学习过程有害。
- en: 'Yarin Gal did his research using Keras and helped build this mechanism directly
    into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related
    arguments: `dropout`, a float specifying the dropout rate for input units of the
    layer, and `recurrent_dropout`, specifying the dropout rate of the recurrent units.
    Let’s add recurrent dropout to the `LSTM` layer of our first LSTM example and
    see how doing so affects overfitting.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Yarin Gal使用Keras进行了他的研究，并帮助将这种机制直接集成到Keras循环层中。Keras中的每个循环层都有两个与dropout相关的参数：`dropout`，一个浮点数，指定层输入单元的dropout率，以及`recurrent_dropout`，指定循环单元的dropout率。让我们将循环dropout添加到我们第一个LSTM示例的`LSTM`层中，看看这样做会如何影响过拟合。
- en: Thanks to dropout, we won’t need to rely as much on network size for regularization,
    so we’ll use an `LSTM` layer with twice as many units, which should hopefully
    be more expressive (without dropout, this network would have started overfitting
    right away — try it). Because networks being regularized with dropout always take
    much longer to fully converge, we’ll train the model for five times as many epochs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了dropout，我们不需要那么依赖网络大小来进行正则化，所以我们将使用具有两倍单元数量的`LSTM`层，这应该能够更充分地表达（如果没有dropout，这个网络会立即开始过拟合——试试看）。因为使用dropout进行正则化的网络总是需要更长的时间才能完全收敛，我们将模型训练的epoch数增加到五倍。
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 13.22](#listing-13-22): Training and evaluating a dropout-regularized
    LSTM'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表13.22](#listing-13-22)：使用dropout正则化的LSTM进行训练和评估'
- en: Figure 13.11 shows the results. Success! We’re no longer overfitting during
    the first 20 epochs. We achieve a validation MAE as low as 2.27 degrees (7% improvement
    over the no-learning baseline) and a test MAE of 2.45 degrees (6.5% improvement
    over the baseline). Not too bad.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11显示了结果。成功了！在前20个epoch中，我们不再过拟合。我们实现了2.27度的验证MAE（相对于无学习基线提高了7%）和2.45度的测试MAE（相对于基线提高了6.5%）。还不错。
- en: '![](../Images/d35aa0612c18200d677a9d3915a59ed2.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d35aa0612c18200d677a9d3915a59ed2.png)'
- en: '[Figure 13.11](#figure-13-11): Training and validation loss on the Jena temperature
    forecasting task with a dropout-regularized LSTM'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.11](#figure-13-11)：使用dropout正则化的LSTM在耶拿温度预测任务上的训练和验证损失'
- en: Stacking recurrent layers
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠循环层
- en: 'Because you’re no longer overfitting, but seem to have hit a performance bottleneck,
    you should consider increasing the capacity and expressive power of the network.
    Recall the description of the universal machine learning workflow: it’s generally
    a good idea to increase the capacity of your model until overfitting becomes the
    primary obstacle (assuming you’re already taking basic steps to mitigate overfitting,
    such as using dropout). As long as you aren’t overfitting too badly, you’re likely
    under capacity.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不再过度拟合，但似乎已经达到了性能瓶颈，你应该考虑增加网络的容量和表达能力。回想一下通用机器学习工作流程的描述：通常，直到过度拟合成为主要障碍（假设你已经采取了基本步骤来减轻过度拟合，例如使用
    dropout），增加你模型的容量是一个好主意。只要你不严重过度拟合，你很可能容量不足。
- en: 'Increasing network capacity is typically done by increasing the number of units
    in the layers or adding more layers. Recurrent layer stacking is a classic way
    to build more powerful recurrent networks: for instance, not too long ago the
    Google Translate algorithm was powered by a stack of seven large `LSTM` layers
    — that’s huge.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 增加网络容量通常是通过增加层中的单元数量或添加更多层来实现的。循环层堆叠是构建更强大循环网络的经典方法：例如，不久前，谷歌翻译算法是由七个大型 `LSTM`
    层堆叠而成的——这非常庞大。
- en: To stack recurrent layers on top of each other in Keras, all intermediate layers
    should return their full sequence of outputs (a rank-3 tensor) rather than their
    output at the last timestep. As you’ve already learned, this is done by specifying
    `return_sequences=True`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中堆叠循环层时，所有中间层应返回它们的完整输出序列（一个秩为 3 的张量），而不是最后一个时间步的输出。正如你已经学到的，这是通过指定
    `return_sequences=True` 来实现的。
- en: In the following example, we’ll try a stack of two dropout-regularized recurrent
    layers. For a change, we’ll use `GRU` layers instead of `LSTM`. A Gated Recurrent
    Unit (GRU) is very similar to an LSTM — you can think of it as a slightly simpler,
    streamlined version of the LSTM architecture. It was introduced in 2014 by Cho
    et al. just when recurrent networks were starting to gain interest anew in the
    then-tiny research community.^([[6]](#footnote-6))
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将尝试堆叠两个带有 dropout 正则化的循环层。为了改变一下，我们将使用 `GRU` 层而不是 `LSTM`。门控循环单元（GRU）与
    LSTM 非常相似——你可以将其视为 LSTM 架构的一个略微简单、精简的版本。它由 Cho 等人在 2014 年引入，当时循环网络正开始重新引起当时微小研究社区的注意.^([[6]](#footnote-6))
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 13.23](#listing-13-23): Training and evaluating a dropout-regularized,
    stacked GRU model'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.23](#listing-13-23)：训练和评估一个带有 dropout 正则化的堆叠 GRU 模型'
- en: Figure 13.12 shows the results. We achieve a test MAE of 2.39 degrees (an 8.8%
    improvement over the baseline). You can see that the added layer does improve
    the results a bit, though not dramatically. You may be seeing diminishing returns
    from increasing network capacity at this point.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 展示了结果。我们实现了 2.39 度的测试 MAE（相对于基线提高了 8.8%）。你可以看到添加的层确实略微提高了结果，但并不显著。你可能会看到增加网络容量所带来的回报正在减少。
- en: '![](../Images/d1d38df47225efcb2cbc27963d2bf531.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d38df47225efcb2cbc27963d2bf531.png)'
- en: '[Figure 13.12](#figure-13-12): Training and validation loss on the Jena temperature
    forecasting task with a stacked GRU network'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.12](#figure-13-12)：使用堆叠 GRU 网络在耶拿温度预测任务上的训练和验证损失'
- en: Using bidirectional RNNs
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用双向 RNN
- en: The last technique introduced in this section is called *bidirectional RNNs*.
    A bidirectional RNN is a common RNN variant that can offer greater performance
    than a regular RNN on certain tasks. It’s frequently used in natural language
    processing — you could call it the Swiss Army knife of deep learning for natural
    language processing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本节最后介绍的技术称为 *双向 RNN*。双向 RNN 是一种常见的 RNN 变体，在某些任务上可以提供比常规 RNN 更好的性能。它经常用于自然语言处理——你可以称其为自然语言处理的深度学习瑞士军刀。
- en: 'RNNs are notably order dependent: they process the timesteps of their input
    sequences in order, and shuffling or reversing the timesteps can completely change
    the representations the RNN extracts from the sequence. This is precisely the
    reason they perform well on problems where order is meaningful, such as the temperature
    forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs:
    it consists of using two regular RNNs, such as the `GRU` and `LSTM` layers you’re
    already familiar with, each of which processes the input sequence in one direction
    (chronologically and antichronologically), and then merging their representations.
    By processing a sequence both ways, a bidirectional RNN can catch patterns that
    may be overlooked by a unidirectional RNN.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs（循环神经网络）特别依赖于顺序：它们按顺序处理输入序列的时间步长，并且打乱或反转时间步长可以完全改变RNN从序列中提取的表示。这正是它们在顺序有意义的任务上表现良好的原因，例如温度预测问题。双向RNN利用了RNN的顺序敏感性：它由两个常规RNN组成，例如你已熟悉的`GRU`和`LSTM`层，每个层都按一个方向（按时间顺序和逆时间顺序）处理输入序列，然后合并它们的表示。通过两种方式处理序列，双向RNN可以捕捉到单方向RNN可能忽略的图案。
- en: Remarkably, the fact that the RNN layers in this section have processed sequences
    in chronological order (older timesteps first) may have been an arbitrary decision.
    At least, it’s a decision we made no attempt to question so far. Could the RNNs
    have performed well enough if they processed input sequences in antichronological
    order, for instance, newer timesteps first? Let’s try this in practice and see
    what happens. All you need to do is write a variant of the data generator where
    the input sequences are reverted along the time dimension (replace the last line
    with `yield samples[:, ::-1, :], targets`).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本节中RNN层按时间顺序处理序列（较早的时间步长首先）可能是一个任意决定。至少，这是我们迄今为止没有尝试质疑的决定。如果RNN按逆时间顺序处理输入序列，例如，先处理较新的时间步长，它们是否仍然能足够好地表现？让我们在实践中尝试一下，看看会发生什么。你所需要做的就是编写一个数据生成器的变体，其中输入序列沿时间维度被反转（将最后一行替换为`yield
    samples[:, ::-1, :], targets`）。
- en: 'When training the same LSTM-based model that you used in the first experiment
    in this section, you would find that such a reversed-order LSTM strongly underperforms
    even the commonsense baseline. This indicates that, in this case, chronological
    processing is important to the success of the approach. This makes perfect sense:
    the underlying `LSTM` layer will typically be better at remembering the recent
    past than the distant past, and, naturally, the more recent weather data points
    are more predictive than older data points for the problem (that’s what makes
    the commonsense baseline fairly strong). Thus the chronological version of the
    layer is bound to outperform the reversed-order version.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练本节第一实验中使用的相同基于LSTM的模型时，你会发现这种逆序LSTM的表现甚至比常识基线还要差。这表明，在这种情况下，按时间顺序处理对于方法的成功至关重要。这完全说得通：底层的`LSTM`层通常在记住近期过去方面比记住遥远过去要好，而且，自然地，对于这个问题（这就是常识基线相当强大）来说，较近的天气数据点比较旧的数据点更有预测性。因此，按时间顺序的层版本必然会优于逆序版本。
- en: 'However, this isn’t true for many other problems, including natural language:
    intuitively, the importance of a word in understanding a sentence isn’t strongly
    dependent on its position in the sentence. On text data, reversed-order processing
    works just as well as chronological processing — you can read text backwards just
    fine (try it!). Although word order does matter in understanding language, *which
    order* you use isn’t crucial.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种情况并不适用于许多其他问题，包括自然语言：直观上，一个词在理解一个句子中的重要性并不强烈依赖于它在句子中的位置。在文本数据上，逆序处理与按时间顺序处理一样有效——你可以很好地倒着阅读文本（试试看！）尽管词序在理解语言中确实很重要，但*使用哪种顺序*并不关键。
- en: 'Importantly, an RNN trained on reversed sequences will learn different representations
    than one trained on the original sequences, much as you would have different mental
    models if time flowed backward in the real world — if you lived a life where you
    died on your first day and were born on your last day. In machine learning, representations
    that are *different* yet *useful* are always worth exploiting, and the more they
    differ, the better: they offer a new angle from which to look at your data, capturing
    aspects of the data that were missed by other approaches, and thus they can help
    boost performance on a task. This is the intuition behind *ensembling*, a concept
    we’ll explore in chapter 18.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，在反转序列上训练的RNN将学习与在原始序列上训练的不同表示，就像如果你在真实世界中时间倒流，你会拥有不同的心理模型——如果你在第一天死去，在最后一天出生。在机器学习中，*不同*但*有用*的表示总是值得利用的，而且它们越不同，越好：它们提供了从新的角度看待你的数据，捕捉到其他方法遗漏的数据方面，从而有助于提高任务的性能。这是*集成*背后的直觉，我们将在第18章中探讨这个概念。
- en: A bidirectional RNN exploits this idea to improve on the performance of chronological-order
    RNNs. It looks at its input sequence both ways (see figure 13.13), obtaining potentially
    richer representations and capturing patterns that may have been missed by the
    chronological-order version alone.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 双向RNN利用这个想法来提高按时间顺序的RNN的性能。它以两种方式查看其输入序列（见图13.13），获得可能更丰富的表示并捕捉到仅由时间顺序版本单独遗漏的模式。
- en: '![](../Images/2ab217681ffa806192f265087b47a896.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2ab217681ffa806192f265087b47a896.png)'
- en: '[Figure 13.13](#figure-13-13): How a bidirectional RNN layer works'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.13](#figure-13-13)：双向RNN层的工作原理'
- en: To instantiate a bidirectional RNN in Keras, you use the `Bidirectional` layer,
    which takes as its first argument a recurrent layer instance. `Bidirectional`
    creates a second, separate instance of this recurrent layer and uses one instance
    for processing the input sequences in chronological order and the other instance
    for processing the input sequences in reversed order. You can try it on our temperature
    forecasting task.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实例化一个双向RNN，你使用`Bidirectional`层，它将一个循环层实例作为其第一个参数。`Bidirectional`创建这个循环层的第二个、独立的实例，并使用一个实例按时间顺序处理输入序列，另一个实例按相反顺序处理输入序列。你可以在我们的温度预测任务上尝试它。
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Listing 13.24](#listing-13-24): Training and evaluating a bidirectional LSTM'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13.24](#listing-13-24)：训练和评估双向LSTM'
- en: 'You’ll find that it doesn’t perform as well as the plain `LSTM` layer. It’s
    easy to understand why: all the predictive capacity must come from the chronological
    half of the network because the antichronological half is known to be severely
    underperforming on this task (again, because the recent past matters much more
    than the distant past in this case). At the same time, the presence of the antichronological
    half doubles the network’s capacity and causes it to start overfitting much earlier.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现它的表现不如普通的`LSTM`层。这很容易理解：所有的预测能力都必须来自网络的时间顺序部分，因为已知反时间顺序部分在这个任务上表现严重不足（再次强调，在这个案例中，近期过去比遥远过去更重要）。同时，反时间顺序部分的存在使网络的能力加倍，并导致它过早地开始过拟合。
- en: However, bidirectional RNNs are a great fit for text data — or any other kind
    of data where order matters, yet where *which* order you use doesn’t matter. In
    fact, for a while in 2016, bidirectional LSTMs were considered the state of the
    art on many natural language processing tasks (before the rise of the Transformer
    architecture, which you will learn about in chapter 15).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，双向RNN非常适合文本数据——或者任何其他顺序很重要，但使用哪种顺序无关紧要的数据。事实上，在2016年一段时间内，双向LSTM被认为在许多自然语言处理任务中是最佳实践（在Transformer架构兴起之前，你将在第15章中了解到这一点）。
- en: Going even further
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更进一步
- en: 'There are many other things you could try to improve performance on the temperature
    forecasting problem:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试许多其他方法来提高温度预测问题的性能：
- en: Adjust the number of units in each recurrent layer in the stacked setup, as
    well as the amount of dropout. The current choices are largely arbitrary and thus
    probably suboptimal.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整堆叠设置中每个循环层的单元数量以及dropout的数量。当前的选择很大程度上是任意的，因此可能不是最优的。
- en: Adjust the learning rate used by the `Adam` optimizer or try a different optimizer.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整`Adam`优化器使用的学习率或尝试不同的优化器。
- en: Try using a stack of `Dense` layers as the regressor on top of the recurrent
    layer, instead of a single `Dense` layer.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`Dense`层的堆叠作为循环层上的回归器，而不是单个`Dense`层。
- en: 'Improve the input to the model: try using longer or shorter sequences or a
    different sampling rate, or start doing feature engineering.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善模型的输入：尝试使用更长或更短的序列，或不同的采样率，或者开始进行特征工程。
- en: As always, deep learning is more an art than a science. We can provide guidelines
    that suggest what is likely to work or not work on a given problem, but, ultimately,
    every dataset is unique; you’ll have to evaluate different strategies empirically.
    There is currently no theory that will tell you in advance precisely what you
    should do to optimally solve a problem. You must iterate.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，深度学习更是一门艺术而不是科学。我们可以提供一些指南，这些指南可能会告诉你某个特定问题可能或可能不会有效，但最终，每个数据集都是独特的；你必须通过经验评估不同的策略。目前还没有一种理论可以提前精确地告诉你应该做什么来最优地解决问题。你必须迭代。
- en: 'In our experience, improving on the no-learning baseline by about 10% is likely
    the best you can do with this dataset. This isn’t so great, but these results
    make sense: while near-future weather is highly predictable if you have access
    to data from a wide grid of different locations, it’s not very predictable if
    you only have measurements from a single location. The evolution of the weather
    where you are depends on current weather patterns in surrounding locations.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，通过大约10%提高无学习基准可能是你用这个数据集能做的最好了。这并不那么出色，但这些结果是有道理的：如果你能获取来自不同位置广泛网格的数据，那么近未来的天气是高度可预测的，但如果只有来自单个位置的数据，那么它就不是很可预测了。你所在地区的天气演变取决于周围地区的当前天气模式。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: As you first learned in chapter 6, when approaching a new problem, it’s good
    to first establish commonsense baselines for your metric of choice. If you don’t
    have a baseline to beat, you can’t tell whether you’re making real progress.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你在第6章首次学习的那样，在处理一个新问题时，首先为你的选择指标建立常识性基准是好的。如果你没有基准可以超越，你就无法判断你是否真的取得了进步。
- en: Try simple models before expensive ones, to justify the additional expense.
    Sometimes a simple model will turn out to be your best option.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试昂贵的模型之前，先尝试简单的模型，以证明额外支出的合理性。有时一个简单的模型最终会变成你最佳的选择。
- en: When you have data where ordering matters — in particular, for timeseries data
    — *recurrent networks* are a great fit and easily outperform models that first
    flatten the temporal data. The two essential RNN layers available in Keras are
    the `LSTM` layer and the `GRU` layer.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你拥有顺序重要数据时——特别是对于时间序列数据——*循环网络*是一个很好的选择，并且可以轻易超越首先将时间数据展平的模型。Keras中可用的两个基本RNN层是`LSTM`层和`GRU`层。
- en: To use dropout with recurrent networks, you should use a time-constant dropout
    mask and recurrent dropout mask. These are built into Keras recurrent layers,
    so all you have to do is use the `recurrent_dropout` arguments of recurrent layers.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在循环网络中使用dropout，你应该使用时间恒定的dropout掩码和循环dropout掩码。这些已经内置在Keras循环层中，所以你只需要使用循环层的`recurrent_dropout`参数。
- en: Stacked RNNs provide more representational power than a single RNN layer. They’re
    also much more expensive and thus not always worth it. Although they offer clear
    gains on complex problems (such as machine translation), they may not always be
    relevant to smaller, simpler problems.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠的RNN比单个RNN层具有更强的表示能力。但它们也昂贵得多，因此并不总是值得。尽管它们在复杂问题（如机器翻译）上提供了明显的收益，但它们可能并不总是适用于较小、较简单的问题。
- en: Footnotes
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Adam Erickson and Olaf Kolle, [https://www.bgc-jena.mpg.de/wetter](https://www.bgc-jena.mpg.de/wetter).
    [[↩]](#footnote-link-1)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adam Erickson 和 Olaf Kolle，[https://www.bgc-jena.mpg.de/wetter](https://www.bgc-jena.mpg.de/wetter)。[[↩]](#footnote-link-1)
- en: There isn’t a `SeparableConv3D` layer, not for any theoretical reason, but simply
    because we haven’t implemented it. [[↩]](#footnote-link-2)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有实现`SeparableConv3D`层，不是出于任何理论原因，而是因为我们还没有实现它。[[↩]](#footnote-link-2)
- en: See, for example, Yoshua Bengio, Patrice Simard, and Paolo Frasconi, “Learning
    Long-Term Dependencies with Gradient Descent Is Difficult,” *IEEE Transactions
    on Neural Networks* 5, no. 2 (1994). [[↩]](#footnote-link-3)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，参见Yoshua Bengio、Patrice Simard 和 Paolo Frasconi，“使用梯度下降学习长期依赖性是困难的”，*IEEE神经网络杂志*
    5，第2期（1994年）。[[↩]](#footnote-link-3)
- en: Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” *Neural Computation*
    9, no. 8 (1997). [[↩]](#footnote-link-4)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sepp Hochreiter 和 Jürgen Schmidhuber，“长短期记忆”，*神经计算* 9，第8期（1997年）。[[↩]](#footnote-link-4)
- en: See Yarin Gal, “Uncertainty in Deep Learning (PhD Thesis),” October 13, 2016,
    [https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html).
    [[↩]](#footnote-link-5)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参见 Yarin Gal 的论文，“深度学习中的不确定性（博士论文）”，2016年10月13日，[https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html).
    [[↩]](#footnote-link-5)
- en: 'See Cho et al., “On the Properties of Neural Machine Translation: Encoder-Decoder
    Approaches,” 2014, [https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259).
    [[↩]](#footnote-link-6)'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参见 Cho 等人，“关于神经机器翻译的性质：编码器-解码器方法”，2014年，[https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259).
    [[↩]](#footnote-link-6)
