- en: 12 Causal decisions and reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 因果决策与强化学习
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using causal models to automate decisions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因果模型来自动化决策
- en: Setting up causal bandit algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置因果伯努利算法
- en: How to incorporate causality into reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将因果关系融入强化学习
- en: 'When we apply methods from statistics and machine learning, it is typically
    in service of making a decision or automating decision-making. Algorithms for
    automated decision-making, such as *bandit* and *reinforcement learning* (RL)
    algorithms, involve agents that *learn* how to make good decisions. In both cases,
    decision-making is fundamentally a causal problem: a decision to take some course
    of action leads to consequences, and the objective is to choose the action that
    leads to consequences favorable to the decision-maker. That motivates a causal
    framing.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用来自统计学和机器学习的方法时，通常是为了做出决策或自动化决策。自动化决策算法，如 *伯努利* 和 *强化学习* (RL) 算法，涉及学习如何做出良好决策的代理。在这两种情况下，决策本质上是一个因果问题：采取某些行动的决策会导致后果，目标是选择对决策者有利的后果。这促使我们采用因果框架。
- en: Often, the path from action to consequences has a degree of randomness. For
    example, your choice of how to play a hand of poker may be optimal, but you still
    might lose due to chance. That motivates a probabilistic modeling approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从行动到后果的路径具有一定的随机性。例如，你选择如何玩一副扑克牌可能是最优的，但你仍然可能因为运气而输掉。这促使我们采用概率建模方法。
- en: The causal probabilistic modeling approach we’ve used so far in this book is
    a stone that hits both these birds. This chapter will provide a *causality-first*
    introduction to basic ideas in statistical decision theory, sequential decision-making,
    bandits, and RL. By “causality-first,” I mean I’ll use the foundation we’ve built
    in previous chapters to introduce these ideas in a causal light. I’ll also present
    the ideas in a way that is compatible with our probabilistic ML framing. Even
    if you are already familiar with these decision-making and RL concepts, I encourage
    you to read on and see them again through a causal lens. Once we do that, we’ll
    see cases where the causal approach to RL gets a better result than the noncausal
    approach.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中迄今为止使用的因果概率建模方法是一块同时击中这两只鸟的石头。本章将提供一个 *因果优先* 的介绍，介绍统计决策理论、顺序决策、伯努利和RL的基本思想。这里的“因果优先”意味着我将使用我们在前几章中建立的基础，以因果的角度介绍这些思想。我还会以与我们的概率机器学习框架兼容的方式呈现这些思想。即使你已经熟悉这些决策和RL概念，我也鼓励你继续阅读，并通过因果的视角再次审视它们。一旦我们这样做，我们就会看到因果方法在RL中比非因果方法得到更好的结果的情况。
- en: 12.1 A causal primer on decision theory
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 决策理论的因果入门
- en: Decision theory is concerned with the reasoning underlying an agent’s choice
    of some course of action. An “agent” here is an entity that chooses an action.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策理论关注的是代理选择某些行动路线背后的推理。这里的“代理”是一个选择行动的实体。
- en: For example, suppose you were deciding whether to invest in a company by purchasing
    equity or purchasing debt (i.e., loaning money to the company and receiving interest
    payments). We’ll call this variable *X*. Whether the company is successful (*Y*)
    depends on the type of investment it receives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在决定是否通过购买股权或购买债务（即向公司贷款并收取利息）来投资一家公司。我们将这个变量称为 *X*。公司是否成功 (*Y*) 取决于它所获得的投资类型。
- en: '![figure](../Images/CH12_F01_Ness.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F01_Ness.png)'
- en: Figure 12.1 A simple causal DAG where action *X* causes some outcome *Y*. Decision
    theory is a causal problem because if deciding on an action didn’t have causal
    consequences, what would be the point of making decisions?
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 一个简单的因果DAG，其中行动 *X* 导致某些结果 *Y*。决策理论是一个因果问题，因为如果决定采取的行动没有因果后果，那么做出决策的意义何在？
- en: Since *X* causally drives *Y*, we can immediately introduce a causal DAG, as
    in figure 12.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *X* 因果地驱动 *Y*，我们可以立即引入一个因果DAG，如图12.1所示。
- en: We’ll use this example to illustrate basic concepts in decision theory from
    a causal point of view.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个例子来从因果的角度说明决策理论的基本概念。
- en: 12.1.1 Utility, reward, loss, and cost
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 效用、奖励、损失和成本
- en: 'The agent generally chooses actions that will cause them to gain some utility
    (or minimize some loss). In decision modeling, you can define a utility function
    (aka a reward function) that quantifies the desirability of various outcomes of
    a decision. Suppose you invest at $1,000:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通常选择那些能让他们获得一些效用（或最小化一些损失）的行动。在决策建模中，你可以定义一个效用函数（也称为奖励函数），它量化了决策各种结果的吸引力。假设你投资了$1,000：
- en: If the company becomes successful, you get $100,000\. Your utility is 100,000
    – 1,000 = $99,000\.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果公司变得成功，你将得到$100,000。你的效用是 100,000 – 1,000 = $99,000。
- en: If the company fails, you get $0 and lose your investment. Your utility is –1,000\.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果公司失败，你将得到$0并损失你的投资。你的效用是 –1,000。
- en: We can add this utility as a node on the graph, as in figure 12.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图上添加这个效用节点，如图12.2所示。
- en: '![figure](../Images/CH12_F02_Ness.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F02_Ness.png)'
- en: Figure 12.2 A utility node can represent utility/reward, loss/cost.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2 效用节点可以代表效用/奖励，损失/成本。
- en: Note that utility is a deterministic function of *Y* in this model, which we’ll
    denote *U*(*Y*).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个模型中，效用是 *Y* 的确定性函数，我们将用 *U*(*Y*) 表示。
- en: '![figure](../Images/ness-ch12-eqs-0x.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-0x.png)'
- en: Instead of a utility/reward function, we could define a loss function (aka,
    a cost function), which is simply –1 times the utility/reward function. For example,
    in the second scenario, where you purchase stock and the company fails, your utility
    is –$1,000 and your loss is $1,000.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是效用/奖励函数，我们可以定义一个损失函数（也称为成本函数），它只是效用/奖励函数的-1倍。例如，在第二个情景中，你购买股票而公司失败的情况下，你的效用是
    –$1,000，你的损失是$1,000。
- en: While the agent’s goal is to decide on a course of action that will maximize
    utility, doing so is challenging because there is typically some uncertainty in
    whether an action will lead to the desired result. In our example, it may seem
    obvious to invest in equity because equity will lead to business success, and
    business success will definitely lead to more utility. But there is some uncertainty
    in whether an equity investment will lead to business success. In other words
    we don’t assume *P*(*Y*=success|*X*=equity) = 1\. Both success and failure have
    nonzero probability in *P*(*Y*|*X*=equity).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然代理的目标是决定一条将最大化效用的行动路线，但这样做是有挑战性的，因为通常在某个行动是否会导致期望的结果方面存在一些不确定性。在我们的例子中，投资股票可能看起来很明显，因为股票将导致商业成功，而商业成功无疑将导致更多的效用。但是，股票投资是否会导致商业成功存在一些不确定性。换句话说，我们不假设
    *P*(*Y*=success|*X*=equity) = 1。在 *P*(*Y*|*X*=equity) 中，成功和失败都有非零的概率。
- en: 12.1.2 Uncertainty comes from other causes
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 不确定性来自其他原因
- en: In causal terms, given action *X*, there is still some uncertainty in the outcome
    *Y* because there are other causal factors driving that outcome. For example,
    suppose the success of the business depends on economic conditions, as in figure
    12.3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果术语中，给定行动 *X*，结果 *Y* 仍然存在一些不确定性，因为还有其他因果因素在推动这个结果。例如，假设商业的成功取决于经济条件，如图12.3所示。
- en: '![figure](../Images/CH12_F03_Ness.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F03_Ness.png)'
- en: Figure 12.3 We typically have uncertainty in our decision-making. From a causal
    perspective, uncertainty is because of other causal factors out of our control
    that affect variables downstream of our actions.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3 我们在决策中通常存在不确定性。从因果的角度来看，不确定性是由于我们无法控制的其他因果因素影响我们行动下游的变量。
- en: Alternatively, those other causal factors could affect utility directly. For
    example, rather than the two discrete scenarios of profit or loss I outlined for
    our business investment, the amount of utility (or loss) could depend on how well
    or how poorly the economy fares, as in figure 12.4\. We can leverage statistical
    and probability modeling to address this uncertainty.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，那些其他因果因素可能直接影响效用。例如，而不是我为我们商业投资概述的两个离散情景——盈利或亏损，效用（或损失）可能取决于经济表现的好坏，如图12.4所示。我们可以利用统计和概率建模来应对这种不确定性。
- en: '![figure](../Images/CH12_F04_Ness.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F04_Ness.png)'
- en: Figure 12.4 Causal factors outside of our control can impact utility (or loss)
    directly.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4中，我们无法控制的因素可能直接影响效用（或损失）。
- en: Suppose you are thinking about whether to invest in this business. You want
    your decision to be data-driven, so you research what other investors in this
    market have done before. You consider the causal DAG in figure 12.5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在考虑是否要投资这个业务。你希望你的决策是基于数据的，因此你研究了这个市场中其他投资者之前都做了什么。你考虑了图12.5中的因果DAG。
- en: '![figure](../Images/CH12_F05_Ness.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F05_Ness.png)'
- en: Figure 12.5 In this DAG, economic conditions drive how investors choose to invest.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5 在这个DAG中，经济条件驱动投资者选择如何投资。
- en: Based on your research, you conclude that past investors’ equity vs. debt choice
    also depends on the economic conditions. *P*(*X*|*C*) represents an action distribution—the
    distribution of actions that the population of investors you are studying take.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的研究，你得出结论，过去投资者的股票与债务选择也取决于经济条件。*P*(*X*|*C*)代表一个行动分布——你正在研究的投资者群体采取的行动分布。
- en: However, the goal of your analysis centers on yourself, not other investors.
    You want to answer questions like “what if *I* bought equity?” That question puts
    us in causal territory. We are not reasoning about observational investment trends;
    we are reasoning about conditional hypotheticals. That is an indicator that we
    need to introduce intervention-based reasoning and counterfactual notation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你分析的目标是围绕你自己，而不是其他投资者。你想要回答像“如果我买股票会怎样？”这样的问题。这个问题让我们进入了因果领域。我们不是在推理观察到的投资趋势；我们是在推理条件假设。这是一个我们需要引入基于干预的推理和反事实记号的指标。
- en: 12.2 Causal decision theory
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 因果决策理论
- en: In this section, we’ll highlight decision-making as a causal query and examine
    what that means for modeling decision-making.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将强调决策作为因果查询，并探讨这对建模决策意味着什么。
- en: 12.2.1 Decisions as a level 2 query
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 决策作为第二层查询
- en: A major source of confusion for causal decision modeling is the difference between
    actions and interventions. In many decision contexts, especially in RL, the action
    is a thing that the agent *does* that changes their environment. Yet, the action
    is also a variable *driven by* the environment. We see this when we look at the
    investment example, shown again in figure 12.6.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因果决策建模的混淆来源之一是行动和干预之间的区别。在许多决策情境中，特别是在强化学习（RL）中，行动是代理人*做*的事情，它改变了他们的环境。然而，行动也是由环境*驱动*的变量。当我们查看投资例子时，我们会看到这一点，如图12.6所示。
- en: '![figure](../Images/CH12_F06_Ness.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F06_Ness.png)'
- en: Figure 12.6 In this version of the investment DAG, the choice of action is caused
    by external factors.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.6 在这个投资DAG版本中，行动的选择是由外部因素引起的。
- en: The action of selecting equity or debt is a variable causally driven by the
    economy. What does that mean? Is an action a variable with causes, or is it an
    intervention?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择股票或债券的行为是由经济因素因果驱动的变量。这意味着什么？这个行为是一个有原因的变量，还是一个干预措施？
- en: The answer is *both*, depending on context. When it is which depends on the
    question we are asking and where that question sits in the causal hierarchy (discussed
    in chapter 10). When we are talking about what actions usually happen, such as
    when we are observing the actions of other agents (or even when reflecting on
    our own past actions) and what results those actions led to, we are reflecting
    on trends in population, and we are on level 1 of the causal hierarchy. In the
    case of our investment example, we’re reasoning about *P*(*C*, *X*, *Y*, *U*).
    But if we’re asking questions like “what would happen if I made an equity investment?”
    then we’re asking a level 2 question, and we need the proposed action as an intervention.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是**两者都是**，这取决于上下文。当我们谈论通常会发生什么行为，例如当我们观察其他代理人的行为（甚至当我们反思我们自己的过去行为）以及这些行为导致了什么结果时，我们正在反思人口趋势，我们处于因果层次的第一层。在我们的投资例子中，我们正在推理*P*(*C*,
    *X*, *Y*, *U*)。但如果我们问的问题是“如果我进行股票投资会发生什么？”那么我们就是在问一个第二层的问题，我们需要将提议的行为作为干预措施。
- en: Next, we’ll characterize common decision rules using our causal notation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们的因果符号来描述常见的决策规则。
- en: 12.2.2 Causal characterization of decision rules and policies
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 决策规则和策略的因果特征
- en: A decision rule is a rule for choosing an action based on the utility distribution
    *P*(*U*(*Y*[*X*][=][*x*])). The agent chooses an optimal action according to a
    decision rule. For example, a common decision rule is choosing the action that
    minimizes loss or cost or maximizes utility or reward.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 决策规则是基于效用分布*P*(*U*(*Y*[*X*][=][*x*]))选择行动的规则。代理人根据决策规则选择最优行动。例如，一个常见的决策规则是选择最小化损失或成本或最大化效用或奖励的行动。
- en: In automated decision-making, the decision rule is often called a “policy.”
    In public health settings, decision rules are sometimes called “treatment regimes.”
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动决策中，决策规则通常被称为“策略”。在公共卫生环境中，决策规则有时被称为“治疗方案”。
- en: Maximizing expected utility
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大化预期效用
- en: The most intuitive and commonly seen decision rule is to choose the action that
    maximizes expected utility. First, we can look at the expectation of the utility
    distribution. Since utility is a deterministic function of *Y*[*X*][=][*x*], this
    is just the expectation of *U*(*Y*[*X*][=][*x*]) over the intervention distribution
    of *Y*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观且最常见的决策规则是选择最大化预期效用的行动。首先，我们可以查看效用分布的期望值。由于效用是 *Y*[*X*][=][*x*] 的确定性函数，这仅仅是
    *U*(*Y*[*X*][=][*x*]) 在 *Y* 的干预分布上的期望。
- en: '![figure](../Images/ness-ch12-eqs-1x.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-1x.png)'
- en: 'We then choose the action (value of *x*) that maximizes expected utility:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后选择最大化预期效用的行动（*x* 的值）：
- en: '![figure](../Images/ness-ch12-eqs-2x.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-2x.png)'
- en: In our investment example, this means choosing the investment approach that
    is expected to make you the most money.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的投资示例中，这意味着选择预期能让你赚最多钱的投资方法。
- en: Minimax decision rules
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最小-最大决策规则
- en: To understand the minimax decision rule, recall that the terms “utility” and
    “loss” are two sides of the same coin; utility == negative loss. Let *L*(*y*)
    = –*U*(*y*). Then a minimax decision rule is
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解最小-最大决策规则，请记住，“效用”和“损失”是同一枚硬币的两面；效用 == 负损失。设 *L*(*y*) = –*U*(*y*)。那么最小-最大决策规则是
- en: '![figure](../Images/ness-ch12-eqs-3x.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-3x.png)'
- en: In plain English, this means “choose the action that minimizes the maximum amount
    of possible loss.” In our investment example, this means choosing the investment
    approach that will minimize the amount of money you’d lose in the worst case scenario.
    There are many variants of minimax rules, but they have the same flavor—minimizing
    loss or maximizing utility during bad times.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说，这意味着“选择一个行动，以最小化可能的最大损失。”在我们的投资示例中，这意味着选择一个投资方法，以最小化在最坏情况下的损失。存在许多最小-最大规则变体，但它们有相同的味道——在困难时期最小化损失或最大化效用。
- en: Softmax rules
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Softmax 规则
- en: A softmax decision rule randomly selects an action with a probability proportional
    to the resulting utility.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 决策规则随机选择一个行动，其概率与结果的效用成比例。
- en: Let’s define *C*(*x*) as the probability of choosing the action *x*. Then *C*(*x*)
    is defined as a probability value proportional to
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义 *C*(*x*) 为选择行动 *x* 的概率。那么 *C*(*x*) 被定义为与以下成比例的概率值
- en: '![figure](../Images/ness-ch12-eqs-4x.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-4x.png)'
- en: The noise parameter *α* modulates between the two extremes. When *α*=0, we have
    a uniform distribution on all the choices. As *α* gets larger, we approach maximizing
    expected utility.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声参数 *α* 在两种极端之间调节。当 *α*=0 时，我们在所有选择上都有均匀分布。随着 *α* 的增大，我们接近最大化预期效用。
- en: Sometimes our goal is to model the decision-making of other agents, such as
    in inverse RL. The softmax decision rule is useful when agents don’t always make
    the utility-optimizing choice. The softmax decision rule provides a simple, analytically
    tractable, and empirically validated model of suboptimal choice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们的目标是模拟其他智能体的决策，例如在逆强化学习（inverse RL）中。当智能体不总是做出效用优化的选择时，softmax 决策规则很有用。softmax
    决策规则提供了一个简单、可分析的、经验验证的次优选择模型。
- en: Another reason we might want to use the softmax rule is when there is a trade-off
    between *exploring* and *exploiting,* such as with bandit problems. Suppose the
    agent is uncertain about the shape of the distibution *P*(*Y*[*X*][=][*x*]). The
    optimal action according to an incorrect model of *P*(*Y*[*X*][=][*x*]) might
    be different from the optimal choice according to the correct model of *P*(*Y*[*X*][=][*x*]).
    The softmax decision rule allows us to choose various actions, get some data on
    the results, and use that data to update our model of *P*(*Y*[*X*][=][*x*]). When
    this is done in sequence, it’s often called *Thompson sampling*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可能想要使用 softmax 规则的原因是在 *探索* 和 *利用* 之间存在权衡，例如在老虎机问题中。假设智能体对分布 *P*(*Y*[*X*][=][*x*])
    的形状不确定。根据 *P*(*Y*[*X*][=][*x*]) 的错误模型所采取的最优行动可能与根据正确模型 *P*(*Y*[*X*][=][*x*]) 所做的最优选择不同。softmax
    决策规则允许我们选择各种行动，获取一些关于结果的数据，并使用这些数据来更新我们对 *P*(*Y*[*X*][=][*x*]) 的模型。当这些操作按顺序进行时，通常被称为
    *汤普森抽样*。
- en: In our investment analogy, suppose we were to invest in several businesses.
    Perhaps, according to our current model, equity investment maximizes expected
    utility, but we’re not fully confident in our current model, so we opt to select
    debt investment even though the current model says its less optimal. The goal
    is to add diversity to our dataset, so that we can learn a better model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的投资类比中，假设我们要投资几个企业。也许，根据我们的当前模型，股权投资最大化预期效用，但我们对我们当前模型并不完全自信，所以我们选择选择债务投资，尽管当前模型表示它不太优。目标是增加我们的数据集的多样性，这样我们就可以学习更好的模型。
- en: Other types of decision rules
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他类型的决策规则
- en: There are other types of decision rules, and they can become complicated, especially
    when they involve statistical estimation. For example, using *p*-values in statistical
    hypothesis testing involves a nuanced utility function that balances the chances
    of a false positive (incorrectly choosing the alternative hypothesis) and a false
    negative (incorrectly choosing the null hypothesis).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他类型的决策规则，并且它们可能会变得复杂，尤其是在涉及统计估计时。例如，在统计假设检验中使用*p*值涉及一个微妙的效用函数，它平衡了假阳性（错误地选择备择假设）和假阴性（错误地选择零假设）的机会。
- en: Fortunately, when we work with probabilistic causal models, the math tends to
    be easier, and we get a nice guarantee called *admissibility*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，当我们与概率因果模型一起工作时，数学通常更容易，我们得到一个称为*可接受性*的保证。
- en: 12.2.3 Causal probabilistic decision-modeling and admissibility
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 因果概率决策建模和可接受性
- en: In this section, I’ll provide a short justification for choosing a causal probabilistic
    modeling approach to decision-making. When you implement an automated decision-making
    algorithm in a production setting, you might have to explain why your implementation
    is better than another. In that setting, it is useful if you know if your algorithm
    is *admissible*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将简要说明选择因果概率建模方法进行决策的理由。当你在一个生产环境中实施自动化决策算法时，你可能需要解释为什么你的实现比另一个更好。在这种情况下，如果你知道你的算法是*可接受的*，那将是有用的。
- en: A decision rule is *admissible* if there are no other rules that dominate it.
    A decision rule dominates another rule if the performance of the former is sometimes
    better, and never worse, than that of the other rule with respect to the utility
    function. For example, the softmax decision rule is dominated by maximizing expected
    utility (assuming you know the true shape of *P*(*Y**[X]*[=]*[x]*)) because sometimes
    it will select suboptimal actions, and it is thus inadmissible. Determining *admissibility*
    is a key task in decision theory.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他规则支配它，则决策规则是*可接受的*。一个决策规则支配另一个规则，如果前者的性能在某些情况下比后者好，并且永远不会比后者差，相对于效用函数而言。例如，softmax决策规则被最大化预期效用所支配（假设你知道*P*(*Y**[X]*[=]*[x]*)的真实形状），因为它有时会选择次优行动，因此是不可接受的。确定*可接受性*是决策理论中的一个关键任务。
- en: The challenge for us occurs when we use data and statistics to deal with unknowns,
    such as parameters or latent variables. If we want to use data to estimate a parameter
    or work with latent variables, there are usually a variety of statistical approaches
    to choose from. If our decision-making algorithm depends on a statistical procedure,
    the choice of procedure can influence which action is considered optimal. How
    do we know if our statistical decision-making procedure is admissible?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，挑战出现在我们使用数据和统计来处理未知情况时，例如参数或潜在变量。如果我们想使用数据来估计一个参数或处理潜在变量，通常有多种统计方法可供选择。如果我们的决策算法依赖于一个统计过程，过程的选择可能会影响被认为最优的行动。我们如何知道我们的统计决策过程是可接受的？
- en: Probabilistic modeling libraries like Pyro leverage Bayesian inference to estimate
    parameters or impute latent variables. Bayesian decision theory tells us that
    *Bayes rules*, (not to be confused with Bayes’s rule) decision rules that optimize
    posterior expected utility, have an admissibility guarantee under mild regularity
    conditions. This means that if we use Bayesian inference in Pyro or similar libraries
    to calculate and optimize posterior expected loss, we have an admissibility guarantee
    (if those mild conditions hold, and they usually do). That means you needn’t worry
    that someone else’s decision-making model (that makes the same modeling assumptions,
    has the same utility function, and uses the same data) will beat yours.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 概率建模库如 Pyro 利用贝叶斯推理来估计参数或推断潜在变量。贝叶斯决策理论告诉我们，*贝叶斯规则*（不要与贝叶斯定理混淆），即优化后验期望效用的决策规则，在温和的正规性条件下具有可接受性保证。这意味着如果我们使用
    Pyro 或类似库中的贝叶斯推理来计算和优化后验期望损失，我们就有可接受性保证（如果这些温和的条件成立，通常都是成立的）。这意味着你不必担心别人的决策模型（做出相同的建模假设，具有相同的效用函数，并使用相同的数据）会打败你。
- en: 12.2.4 The deceptive alignment of argmax values of causal and non-causal expectations
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 因果和非因果期望的 argmax 值的欺骗性对齐
- en: Most conventional approaches to decision-making, including in RL, focus on maximizing
    *E*(*U*(*Y*)|*X*=*x*) rather than *E*(*U*(*Y*[*X*][=][*x*])). Let’s implement
    the model in figure 12.6 with pgmpy and compare the two approaches.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数传统的决策方法，包括在强化学习（RL）中，都侧重于最大化 *E*(*U*(*Y*)|*X*=*x*) 而不是 *E*(*U*(*Y*[*X*][=][*x*])).
    让我们使用 pgmpy 在图 12.6 中实现模型，并比较两种方法。
- en: First, we’ll build the DAG in the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将构建模型中的 DAG。
- en: Setting up your environment
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: This code was written with pgmpy version 0.1.24\. See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to the notebook that runs this code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是用 pgmpy 版本 0.1.24 编写的。有关运行此代码的笔记本链接，请参阅章节注释 [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。
- en: Listing 12.1 DAG for investment decision model
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.1 投资决策模型的 DAG
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Set up the DAG'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置 DAG'
- en: 'Next we’ll build the causal Markov kernels for *Economy* (*C*), *Debt vs. Equity*
    (*X*), and *Business Success* (*Y*). The causal Markov kernel for *Economy* (*C*)
    will take two values: “bear” for bad economic conditions and “bull” for good.
    The causal Markov kernel for *Debt vs. Equity* (*X*) will depend on *C*, reflecting
    the fact that investors tend to prefer equity in a bull economy and debt in a
    bear economy. *Success* (*Y*) depends on the economy and the choice of debt or
    equity investment.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为 *Economy* (*C*)、*Debt vs. Equity* (*X*) 和 *Business Success* (*Y*)
    构建因果马尔可夫核。*Economy* (*C*) 的因果马尔可夫核将取两个值：“熊”代表不良经济状况，“牛”代表良好。*Debt vs. Equity*
    (*X*) 的因果马尔可夫核将取决于 *C*，反映投资者在牛市倾向于偏好股权，在熊市倾向于偏好债务的事实。*Success* (*Y*) 取决于经济状况和债务或股权投资的选择。
- en: Listing 12.2 Create causal Markov kernels for *C*, *X*, and *Y*
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.2 创建 *C*、*X* 和 *Y* 的因果马尔可夫核
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Set up causal Markov kernel for C (economy). It takes two values: “bull”
    and “bear”.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为 C（经济）设置因果马尔可夫核。它有两个值：“牛”代表良好经济状况，“熊”代表不良。'
- en: '#2 Set up causal Markov kernel for action X, either making a debt investment
    or equity investment depending on the economy.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为行动 X 设置因果马尔可夫核，根据经济状况，要么是债务投资，要么是股权投资。'
- en: '#3 Set up causal Markov kernel for business outcome Y, either success or failure,
    depending on the type of investment provided (X) and the economy (C).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为业务结果 Y 设置因果马尔可夫核，成功或失败取决于提供的投资类型（X）和经济状况（C）。'
- en: Finally, we’ll add the *Utility* node (*U*). We use probabilities of 1 and 0
    to represent a deterministic function of *Y*. We end by adding all the kernels
    to the model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加 *Utility* 节点 (*U*)。我们使用概率 1 和 0 来表示 *Y* 的确定性函数。我们通过添加所有核到模型中结束。
- en: Listing 12.3 Implement the utility node and initialize the model
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.3 实现效用节点并初始化模型
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Set up the utility node.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置效用节点。'
- en: '#2 Set up the utility node.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 设置效用节点。'
- en: 'This code prints out the following conditional probability tables for our causal
    Markov kernels. This one is for the *Utility* variable:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码打印出以下因果马尔可夫核的条件概率表。这是针对 *Utility* 变量的：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This reflects the investor trends of favoring equity investments in a bull market
    and debt investments in a bear market.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这反映了投资者在牛市倾向于偏好股权投资，在熊市倾向于偏好债务投资的趋势。
- en: 'The following probability table is for the *Business Success* variable *Y*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的概率表是针对 *Business Success* 变量 *Y* 的：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This reflects debt being a less preferred source of financing in a bear market
    when interest rate payments are higher, and equity being preferred in a bull market
    because equity is cheaper.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这反映了在熊市中，由于利率支付较高，债务融资不如债券融资受欢迎，而在牛市中，由于债券更便宜，债券融资更受欢迎。
- en: 'Finally, the *Utility* node is a simple deterministic function that maps *Y*
    to utility values:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*效用*节点是一个简单的确定性函数，将 *Y* 映射到效用值：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Next, we’ll calculate* *E*(*U*(*Y*[*X*][=][*x*])) and *E*(*U*(*Y*)|*X*=*x*).
    Before proceeding, download and load a helper function that implements an ideal
    intervention. To allay any security concerns of directly executing downloaded
    code, the code prints the downloaded script and prompts you to confirm before
    executing the script.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我们将计算* *E*(*U*(*Y*[*X*][=][*x*])) 和 *E*(*U*(*Y*)|*X*=*x*). 在继续之前，下载并加载一个实现理想干预的辅助函数。为了缓解直接执行下载代码可能带来的安全担忧，代码会打印下载的脚本并提示您在执行脚本之前确认。*'
- en: Listing 12.4 Download helper function for implementing an ideal intervention
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.4 下载实现理想干预的辅助函数
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Load an implementation of an ideal intervention.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载理想干预的实现。'
- en: '#2 To allay security concerns, you can inspect the downloaded script and confirm
    it before running.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为了缓解安全担忧，您可以检查下载的脚本并在运行之前确认。'
- en: By now, in this book, you should not be surprised that *E*(*U*(*Y*[*X*][=][*x*]))
    is different from *E*(*U*(*Y*)|*X*=*x*). Let’s look at these values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，在这本书中，您不应该对 *E*(*U*(*Y*[*X*][=][*x*])) 与 *E*(*U*(*Y*)|*X*=*x*) 不同感到惊讶。让我们看看这些值。
- en: Listing 12.5 Calculate *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y**[X]*[=]*[x]*))
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.5 计算 *E*(*U*(*Y*)|*X*=*x*) 和 *E*(*U*(*Y**[X]*[=]*[x]*))
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 A helper function for calculating the expected utility'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于计算预期效用的辅助函数'
- en: '#2 Set X by intervention to debt and equity and calculate the expectation of
    U under each intervention.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过干预将 X 设置为债务和债券，并计算每个干预下的 U 的期望值。'
- en: '#3 Condition on X = debt and X = equity, and calculate the expectation of U.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在 X = debt 和 X = equity 的条件下，计算 U 的期望值。'
- en: 'This gives us the following conditional expected utilities (I’ve marked the
    highest with *):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下条件预期效用（我已经用 * 标记了最高的）：
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
- en: '*E*(*U*(*Y*)|*X*=equity) = 37000'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=equity) = 37000'
- en: 'It also gives us the following interventional expected utilities:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 它还给我们以下干预预期效用：
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000 *'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=debt])) = 39000 *'
- en: '*E*(*U*(*Y*[*X*][=equity])) = 34000'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=equity])) = 34000'
- en: So *E*(*U*(*Y*)|*X*=debt) is different from *E*(*U*(*Y*[*X*][=debt])), and *E*(*U*(*Y*)|*X*=equity)
    is different from *E*(*U*(*Y*[*X*][=][equity])). However, our goal is to optimize
    expected utility, and in this case, debt maximizes both *E*(*U*(*Y*)|*X*=x) and
    *E*(*U*(*Y*[*X*][=][*x*])).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*E*(*U*(*Y*)|*X*=debt) 与 *E*(*U*(*Y*[*X*][=debt])) 不同，*E*(*U*(*Y*)|*X*=equity)
    与 *E*(*U*(*Y*[*X*][=][equity])) 不同。然而，我们的目标是优化预期效用，在这种情况下，债务最大化了 *E*(*U*(*Y*)|*X*=x)
    和 *E*(*U*(*Y*[*X*][=][*x*]))。
- en: '![figure](../Images/ness-ch12-eqs-5x.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-5x.png)'
- en: If “debt” maximizes both queries, what is the point of causal decision theory?
    What does it matter if *E*(*U*(*Y*)|*X*=*x*) and *E*(*U*(*Y*[*X*][=][*x*])) are
    different if the optimal action for both is the same?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“债务”最大化了两个查询，因果决策理论有什么用？如果最优行动对于两者都是相同的，*E*(*U*(*Y*)|*X*=*x*) 和 *E*(*U*(*Y*[*X*][=][*x*]))
    是否不同又有什么关系？
- en: In decision problems, it is quite common that a causal formulation of the problem
    provides the same answer as more traditional noncausal formulations. This is especially
    true in higher dimensional problems common in RL. You might observe this and wonder
    why the causal formulation is needed at all.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策问题中，通常情况下，问题的因果表述与更传统的非因果表述提供相同的答案。这在 RL 中常见的更高维问题中尤其如此。您可能会观察到这一点，并想知道为什么因果表述是必需的。
- en: To answer, watch what happens when we make a slight change to the parameters
    of *Y* in the model. Specifically, we’ll change the parameter for *P*(*Y*=success|*X*=equity,
    *C*=bull) from .4 to .6\. First, we’ll rebuild the model with the parameter change.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，观察当我们对模型中 *Y* 的参数进行轻微修改时会发生什么。具体来说，我们将把 *P*(*Y*=success|*X*=equity,
    *C*=bull) 的参数从 .4 改为 .6。首先，我们将使用参数更改重建模型。
- en: Listing 12.6 Change a parameter in the causal Markov kernel for *Y*
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6 修改 *Y* 的因果马尔可夫核中的参数
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Initialize a new model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化一个新的模型。'
- en: '#2 Create a new conditional probability distribution for Y.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为 Y 创建一个新的条件概率分布。'
- en: '#3 Change the parameter P(Y=success|X=equity, C=bull) = 0.4 (the last parameter
    in the first list) to 0.6.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将第一个列表中的最后一个参数 P(Y=success|X=equity, C=bull) = 0.4 改为 0.6。'
- en: '#4 Add the causal Markov kernels to the model.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将因果马尔可夫核添加到模型中。'
- en: Next, we rerun inference.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重新运行推理。
- en: Listing 12.7 Compare outcomes with changed parameters
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.7 比较改变参数后的结果
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Set X by intervention to debt and equity, and calculate the expectation
    of U under each intervention.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 通过干预将X设置为debt和equity，并计算每种干预下的U的期望。'
- en: '#2 Condition on X = debt and X = equity, and calculate the expectation of U.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在X = debt和X = equity的条件下进行条件化，并计算U的期望。'
- en: 'This gives us the following conditional expectations (* indicates the optimal
    choice):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下条件期望（*表示最优选择）：
- en: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=debt) = 57000 *'
- en: '*E*(*U*(*Y*)|*X*=equity) = 53000'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*)|*X*=equity) = 53000'
- en: 'It also gives us the following interventional expectations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它还给我们以下干预期望：
- en: '*E*(*U*(*Y*[*X*][=debt])) = 39000'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=debt])) = 39000'
- en: '*E*(*U*(*Y*[*X*][=equity])) = 44000 *'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*X*][=equity])) = 44000 *'
- en: With that slight change in a single parameter, “debt” is still the optimal value
    of *x* in *E*(*U*(*Y*)|*X*=*x*), but now “equity” is the optimal value of *x*
    in *E*(*U*(*Y*[*X*][=][*x*])). This is a case where the causal answer and the
    answer from conditioning on evidence are different. Since we are trying to answer
    a level 2 query, the causal approach is the right approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个参数的微小变化下，“债务”仍然是 *E*(*U*(*Y*)|*X*=*x*) 中 *x* 的最优值，但现在“股权”是 *E*(*U*(*Y*[*X*][=][*x*]))
    中 *x* 的最优值。这是一个因果答案与基于证据的条件答案不同的情况。由于我们正在尝试回答一个第二级查询，因果方法是正确的方法。
- en: This means that while simply optimizing a conditional expectation often gets
    you the right answer, you are vulnerable to getting the wrong answer in certain
    circumstances. Compare this to our discussion of semi-supervised learning in chapter
    4—often the unlabeled data can help with learning, but, in specific circumstances,
    the unlabeled data adds no value. Causal analysis helped us characterize those
    circumstances in precise terms. Similarly, in this case, there are specific scenarios
    where the causal formulation of the problem will lead to a different and more
    correct result relative to the traditional noncausal formulation. Even the most
    popular decision-optimization algorithms, including the deep learning-based approaches
    used in deep RL, can improve performance by leveraging the causal structure of
    a decision problem.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然简单地优化条件期望通常可以得到正确答案，但在某些情况下，你可能会得到错误答案。这与我们在第4章中关于半监督学习的讨论类似——通常未标记数据可以帮助学习，但在特定情况下，未标记数据不会增加任何价值。因果分析帮助我们以精确的术语描述了这些情况。同样，在这种情况下，有特定的场景，因果问题的因果表述相对于传统的非因果表述将导致不同的、更正确的结果。即使是包括在深度强化学习中使用的基于深度学习的最流行的决策优化算法，也可以通过利用决策问题的因果结构来提高性能。
- en: Next, we’ll see another example with Newcomb’s paradox.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到另一个Newcomb悖论的例子。
- en: 12.2.5 Newcomb’s paradox
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.5 Newcomb悖论
- en: A famous thought experiment called Newcomb’s paradox contrasts the causal approach
    to decision theory, maximizing utility under intervention, with the conventional
    approach of maximizing utility conditional on some action. We’ll look at an AI-inspired
    version of this thought experiment in this section, and the next section will
    show how to approach it with a formal causal model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的思想实验，称为Newcomb悖论，对比了决策理论中的因果方法，即在干预下最大化效用，与传统的在某种行动条件下最大化效用的方法。在本节中，我们将探讨这个思想实验的AI版本，下一节将展示如何使用形式化的因果模型来处理它。
- en: There are two boxes designated A and B as shown in figure 12.7\. Box A always
    contains $1,000\. Box B contains either $1,000,000 or $0\. The decision-making
    agent must choose between taking only box B or *both* boxes. The agent does not
    know what is in box B until they decide. Given this information, it is obvious
    the agent should take both boxes—choosing both yields either $1,000 or $1,001,000,
    while choosing only B yields either $0 or $1,000,000.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如图12.7所示，有两个标记为A和B的盒子。盒子A总是包含$1,000。盒子B包含$1,000,000或$0。决策代理必须选择只拿盒子B或*两个*盒子。代理在做出决定之前不知道盒子B里有什么。根据这个信息，很明显代理应该拿两个盒子——选择两个盒子可以得到$1,000或$1,001,000，而只选择B可以得到$0或$1,000,000。
- en: '![figure](../Images/CH12_F07_Ness.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F07_Ness.png)'
- en: Figure 12.7 An illustration of the boxes in Newcomb's paradox
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.7 Newcomb悖论中盒子的示意图
- en: Now, suppose there is an AI that can predict with high accuracy what choice
    the agent intends to make. If the AI predicts that the agent intends to take both
    boxes, it will put no money in box B. If the AI is correct and the agent takes
    both boxes, the agent only gets $1,000\. However, if the AI predicts that the
    agent intends to take only box B, it will put $1,000,000 in box B. If the AI predicts
    correctly, the agent gets the $1,000,000 in box B but not the $1,000 in box A.
    The agent does not know for sure what the AI predicted or what box B contains
    until they make their choice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设有一个 AI 可以高精度地预测代理打算做出的选择。如果 AI 预测代理打算选择两个盒子，它将不会在盒子 B 中放钱。如果 AI 预测正确，代理选择两个盒子，代理只能得到
    $1,000。然而，如果 AI 预测代理打算只选择盒子 B，它将在盒子 B 中放入 $1,000,000。如果 AI 预测正确，代理将在盒子 B 中得到 $1,000,000，但不会在盒子
    A 中得到 $1,000。代理在做出选择之前不知道 AI 的预测或盒子 B 中有什么。
- en: The traditional paradox arises as follows. A causality-minded agent reasons
    that the actions of the AI are out of their control. They only focus on what they
    can control—the causal consequences of their choice. They can’t *cause* the content
    of box B, so they pick both boxes on the off-chance box B has the million, just
    as one would if the AI didn’t exist. But if the agent knows how the AI works,
    doesn’t it make more sense to choose only box B and get the million with certainty?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 传统悖论的产生如下。一个因果论者认为，AI 的行为超出了他们的控制。他们只关注他们能控制的部分——他们选择因果后果。他们不能 *造成* 盒子 B 的内容，所以他们选择两个盒子，以备万一盒子
    B 有钱，就像 AI 不存在时一样。但如果代理知道 AI 的工作原理，选择只选择盒子 B 并确定性地获得一百万不是更有意义吗？
- en: Let’s dig in further by enumerating the possible outcomes and their probabilities.
    Let’s assume the AI’s predictions are 95% accurate. If the agent chooses both
    boxes, there is a 95% chance the AI will have guessed the agent’s choice and put
    no money in B, in which case the agent only gets the $1,000\. There is a 5% chance
    the algorithm will guess wrong, in which case it puts 1,000,000 in box B, and
    the agent wins $1,001,000\. If the agent chooses only box B, there is a 95% chance
    the AI will have predicted the choice and placed $1,000,000 in box B, giving the
    agent $1,000,000 in winnings. There is a 5% chance it will not, and the agent
    will take home nothing. We see these outcomes in table 12.1\. The expected utility
    calculations are shown in table 12.2.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过列举可能的结果及其概率来进一步深入探讨。假设 AI 的预测准确率为 95%。如果代理选择两个盒子，那么 AI 有 95% 的可能性猜对了代理的选择，将没有钱放入
    B，在这种情况下，代理只能得到 $1,000。有 5% 的可能性算法会猜错，在这种情况下，它会将 1,000,000 放入盒子 B，代理赢得 $1,001,000。如果代理只选择盒子
    B，那么 AI 有 95% 的可能性预测了选择，并将 1,000,000 放入盒子 B，给代理 $1,000,000 的奖金。有 5% 的可能性它不会这样做，代理将一无所获。我们在表
    12.1 中看到这些结果。预期效用计算显示在表 12.2 中。
- en: Table 12.1 Newcomb’s problem outcomes and their probabilities
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 12.1 新科姆问题的结果及其概率
- en: '| Strategy | AI action | Winnings | Probability |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | AI 行动 | 奖金 | 概率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Choose both  | Put $0 in box B  | $1,000  | .95  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 选择两者 | 将 $0 放入盒子 B | $1,000 | .95 |'
- en: '| Choose both  | Put $1,000,000 in box B  | $1,001,000  | .05  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 选择两者 | 将 $1,000,000 放入盒子 B | $1,001,000 | .05 |'
- en: '| Choose only box B  | Put $1,000,000 in box B  | $1,000,000  | .95  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 只选择盒子 B | 将 $1,000,000 放入盒子 B | $1,000,000 | .95 |'
- en: '| Choose only box B  | Put $0 in box B  | $0  | .05  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 只选择盒子 B | 将 $0 放入盒子 B | $0 | .05 |'
- en: Table 12.2 Expected utility of each choice in Newcomb’s problem
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 12.2 新科姆问题中每个选择的预期效用
- en: '| Strategy ( *x*) | *E*( *U*&#124; *X*= *x*) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 策略 ( *x*) | *E*( *U*&#124; *X*= *x*) |'
- en: '| --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Choose both  | 1,000 × .95 + 1,001,000 × .05 = $51,000  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 选择两者 | 1,000 × .95 + 1,001,000 × .05 = $51,000 |'
- en: '| Choose only box B  | 1,000,000 × .05 + 0 × .05 = $950,000  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 只选择盒子 B | 1,000,000 × .05 + 0 × .05 = $950,000 |'
- en: The conventional approach suggests choosing box only box B.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法建议只选择盒子 B。
- en: When the paradox was created, taking a causal approach to the problem meant
    only attending to the causal consequences of one’s actions. Remember that the
    AI makes the prediction *before* the agent acts. Since effects cannot precede
    causes in time, the AI’s behavior is not a consequence of the agent’s actions,
    so the agent with the causal view ignores the AI and goes with the original strategy
    of choosing both boxes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当悖论被创造出来时，对问题采取因果方法意味着只关注自己行为的因果后果。记住，AI 在代理行动之前做出预测。由于效果不能在时间上先于原因，AI 的行为不是代理行为的后果，因此具有因果观点的代理忽略
    AI，并坚持选择两个盒子的原始策略。
- en: It would seem that the agent with the causal view is making an error in failing
    to account for the actions of the AI. But we can resolve this error by having
    the agent use a formal causal model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，具有因果观点的代理在未能考虑人工智能的行动时犯了错误。但我们可以通过让代理使用正式的因果模型来解决这个问题。
- en: 12.2.6 Newcomb’s paradox with a causal model
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.6 带有因果模型的 Newcomb 悖论
- en: In the traditional formulation of Newcomb’s paradox, the assumption is that
    the agent using causal decision theory only attends to the consequences of their
    actions—they are reasoning on the causal DAG in figure 12.8\. But the true data
    generating process (DGP) is better captured by figure 12.9.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Newcomb 悖论的传统表述中，假设是使用因果决策理论的代理只关注他们行动的后果——他们在图 12.8 中的因果有向图中进行推理。但真实的数据生成过程（DGP）被图
    12.9 更好地捕捉。
- en: '![figure](../Images/CH12_F08_Ness.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F08_Ness.png)'
- en: Figure 12.8 Newcomb’s paradox assumes a version of causal decision theory where
    a naive agent uses this incorrect causal DAG.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.8 Newcomb 悖论假设了一种因果决策理论版本，其中天真代理使用这个错误的因果有向图。
- en: '![figure](../Images/CH12_F09_Ness.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F09_Ness.png)'
- en: Figure 12.9 A better causal DAG representing the framing of Newcomb’s paradox
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.9 代表 Newcomb 悖论框架的更好的因果有向图
- en: The choice of the agent can’t *cause* the AI’s prediction, because the prediction
    happens first. Thus, we assume the AI agent is inferring the agent’s *intent*,
    and thus the intent of the agent is the cause of the AI’s prediction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的选择不能 *cause* 人工智能的预测，因为预测先发生。因此，我们假设人工智能代理正在推断代理的 *intent*，因此代理的意图是人工智能预测的原因。
- en: The causal decision-making agent would prefer the graph in figure 12.9 because
    it is a better representation of the DGP. The clever agent wouldn’t focus on maximizing
    *E*(*U*[*choice*][=][*x*]). The clever agent is aware of its own intention, and
    knowing that this intention is a cause of the content of box B, it focuses on
    optimizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*), where *i* is their original
    intention of which box to pick.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因果决策代理会更喜欢图 12.9 中的图，因为它更好地代表了 DGP。聪明的代理不会专注于最大化 *E*(*U*[*choice*][=][*x*])。聪明的代理意识到自己的意图，并知道这种意图是盒子
    B 内容的原因，它专注于优化 *E*(*U*[*choice*][=][*x*]|*intent*=*i*)，其中 *i* 是他们最初选择哪个盒子的原始意图。
- en: '![figure](../Images/ness-ch12-eqs-6x.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-6x.png)'
- en: We’ll assume the agent’s initial intention is an impulse it cannot control.
    But while they can’t control their initial intent, they can do some introspection
    and become aware of this intent. Further, we’ll assume that upon doing so, they
    have the ability to change their choice to something different from what it initially
    intended, after the AI has made their prediction and set the contents of box B.
    Let’s model this system in pgmpy and evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设代理的初始意图是一种它无法控制的冲动。但尽管他们无法控制他们的初始意图，他们可以进行一些内省并意识到这种意图。进一步，我们将假设，在这样做之后，他们有能力在人工智能做出预测并设置盒子
    B 的内容之后，将他们的选择改变为与最初意图不同的东西。让我们在 pgmpy 中模拟这个系统并评估最大化 *E*(*U*[*choice*][=][*x*]|*intent*=*i*)。
- en: First, let’s build the DAG.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建 DAG。
- en: Listing 12.8 Create the DAG
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.8 创建 DAG
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we’ll create causal Markov kernels for intent and choice.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为意图和选择创建因果马尔可夫内核。
- en: Listing 12.9 Create causal Markov kernels for intent and choice
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.9 创建意图和选择的因果马尔可夫内核
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 We assume a 50-50 chance the agent will prefer both boxes vs. box B.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们假设代理将更喜欢两个盒子而不是盒子 B 的概率为 50-50。'
- en: '#2 We assume the agent’s choice is deterministically driven by their intent.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们假设代理的选择是由他们的意图决定性地驱动的。'
- en: Similarly, we’ll create the causal Markov kernels for the AI’s decision and
    the content of box B.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将为人工智能的决策和盒子 B 的内容创建因果马尔可夫内核。
- en: Listing 12.10 Create causal Markov kernels for AI prediction and box B content
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.10 创建人工智能预测和盒子 B 内容的因果马尔可夫内核
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 The AI’s prediction is 95% accurate.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 人工智能的预测准确率为 95%。'
- en: '#2 Box B contents are set deterministically by the AI’s prediction.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 盒子 B 的内容是由人工智能的预测决定性地设置的。'
- en: Finally, we’ll create a causal Markov kernel for utility and add all the kernels
    to the model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将为效用创建一个因果马尔可夫内核，并将所有内核添加到模型中。
- en: Listing 12.11 Create utility kernel and build the model
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.11 创建效用内核并构建模型
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Set up the utility node.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置效用节点。'
- en: '#2 Build the model.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 构建模型。'
- en: Now we’ll evaluate maximizing *E*(*U*[*choice*][=][*x*]|*intent*=*i*).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将评估最大化 *E*(*U*[*choice*][=][*x*]|*intent*=*i*)。
- en: Listing 12.12 Infer optimal choice using intervention and conditioning on intent
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.12 使用干预和基于意图的条件推断最佳选择
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Infer E(U(Y [choice=both]|intent=both)).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 推断E(U(Y [选择=两个]|意图=两个))。'
- en: '#2 Infer E(U(Y [choice=box B]|intent=both)).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 推断E(U(Y [选择=盒子B]|意图=两个))。'
- en: '#3 Infer E(U(Y [choice=both]|intent=B)).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 推断E(U(Y [选择=两个]|意图=B))。'
- en: '#4 Infer E(U(Y [choice=box B]|intent=B)).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 推断E(U(Y [选择=盒子B]|意图=B))。'
- en: 'This code produces the following results (* indicates the optimal choice for
    a given intent):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码产生了以下结果（*表示给定意图的最优选择）：
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=both)) = 51000 *'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*选择*][=两个]|*意图*=两个)) = 51000 *'
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=both)) = 50000'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*选择*][=盒子B]|*意图*=两个)) = 50000'
- en: '*E*(*U*(*Y*[*choice*][=both]|*intent*=B)) = 951000 *'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*选择*][=两个]|*意图*=B)) = 951000 *'
- en: '*E*(*U*(*Y*[*choice*][=box B]|*intent*=B)) = 950000'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*[*选择*][=盒子B]|*意图*=B)) = 950000'
- en: When the agent’s initial intention is to select both, the best choice is to
    select both. When the agent intends to choose only box B, the best choice is to
    ignore those intentions and choose both. Either way, the agent should choose both.
    Note that when the agent initially intends to choose only box B, switching to
    both boxes gives them an expected utility of $951,000 which is greater than the
    optimal choice utility of $950,000 in the noncausal approach.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理的初始意图是选择两个时，最佳选择是选择两个。当代理意图只选择盒子B时，最佳选择是忽略那些意图并选择两个。无论如何，代理都应该选择两个。请注意，当代理最初意图只选择盒子B时，切换到两个盒子给了他们$951,000的预期效用，这比非因果方法中最佳选择效用$950,000要高。
- en: The agent, unfortunately, cannot control their initial intent; if they could,
    they would deliberately ‘intend’ to pick box B and then switch at the last minute
    to choosing both boxes after the AI placed the million in box B. However, they
    can engage in a form of introspection, factoring their initial intent into their
    decision and, in so doing, accounting for the AI’s behavior rather than ignoring
    it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，代理无法控制他们的初始意图；如果他们能，他们会故意‘意图’选择盒子B，然后在最后时刻切换到选择两个盒子，在AI将一百万放入盒子B之后。然而，他们可以进行一种内省，将他们的初始意图纳入他们的决策中，并在此过程中，考虑到AI的行为而不是忽略它。
- en: 12.2.7 Introspection in causal decision theory
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.7 因果决策理论中的内省
- en: Newcomb’s problem illustrates a key capability of causal decision theory—the
    ability for us to include introspection as part of the DGP.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 新问题说明了因果决策理论的关键能力——我们能够将内省包括在DGP中。
- en: '![figure](../Images/CH12_F10_Ness.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F10_Ness.png)'
- en: Figure 12.10 Often our actions are simply reactions to our environment, rather
    than the result of deliberate decision-making.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.10我们的行为通常是环境反应，而不是故意决策的结果。
- en: To illustrate, consider that often our actions are simply *reactions* to our
    environment, as in figure 12.10.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，考虑我们的行为通常是环境反应，就像图12.10所示。
- en: For example, you might have purchased a chocolate bar *because* you were hungry
    and it was positioned to tempt you as you waited in the checkout aisle of the
    grocery store. Rather than go through some deliberative decision-making process,
    you had a simple, perhaps even unconscious, *reaction* to your craving and an
    easy way to satisfy it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会因为饥饿而购买一盒巧克力，因为它在你等待在杂货店的结账通道时被放置在诱惑你的位置。你不会经历一些深思熟虑的决策过程，而是对你的渴望有一个简单、甚至可能是无意识的反应，并有一种简单的方法来满足它。
- en: However, humans are capable of introspection—observing and thinking about their
    internal states. A human might consider their normal reactive behavior as part
    of the DGP. This introspection is illustrated in figure 12.11.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人类能够进行内省——观察和思考他们的内部状态。一个人可能会将他们的正常反应行为视为DGP的一部分。这种内省在图12.11中得到了说明。
- en: '![figure](../Images/CH12_F11_Ness.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F11_Ness.png)'
- en: Figure 12.11 Humans and some other agents can think about a DGP that includes
    them as a component of that process.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.11人类和一些其他代理可以思考一个包括他们作为该过程组成部分的DGP。
- en: Through this introspection, the agent can perform level 2 hierarchical reasoning
    about what would happen if they did not react as usual but acted deliberately
    (e.g., sticking to their diet and not buying the chocolate bar), as in figure
    12.12.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种内省，代理可以对以下情况进行2级分层推理：如果他们不按常规反应，而是故意行动（例如，坚持饮食并不要买巧克力棒），如图12.12所示。
- en: '![figure](../Images/CH12_F12_Ness.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F12_Ness.png)'
- en: Figure 12.12 The agent reasons about a DGP that includes them as a component.
    They then use that reasoning in asking level 2 “what would happen if...” questions
    about that process.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.12代理对包括他们作为组成部分的DGP进行推理。然后他们使用这种推理来询问关于该过程的2级“如果...会发生什么”问题。
- en: In many cases, the agent may not know the full state of their environment. However,
    if the agent can disentangle their urge to react a certain way from their action,
    they can use that “urge” as evidence in deliberative decision-making, as in figure
    12.13.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，代理可能不知道他们环境的完整状态。然而，如果代理能够将他们想要以某种方式反应的冲动与他们的行为分开，他们可以使用这种“冲动”作为深思熟虑的决策的证据，如图12.13所示。
- en: '![figure](../Images/CH12_F13_Ness.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F13_Ness.png)'
- en: Figure 12.13 The agent may not know the states of other variables in the environment,
    but through introspection, they may have an intuition about those variables. That
    intuition can be used as evidence in conditional causal inferences.
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.13 代理可能不知道环境中其他变量的状态，但通过内省，他们可能对这些变量有一种直觉。这种直觉可以用作条件因果推断的证据。
- en: We saw this pattern in the Newcomb example; the agent does not know what the
    AI has predicted, but, through introspection, they can use their initial intention
    to choose both boxes as *evidence* of what the AI has chosen.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Newcomb例子中看到了这种模式；代理不知道AI的预测是什么，但通过内省，他们可以使用他们的初始意图选择两个盒子作为AI选择的*证据*。
- en: Was there ever a time where you noticed you had started to make clumsy errors
    in your work and used that as evidence that you were fatigued, even though you
    didn’t feel so, and you thought, “what if I take a break?” Have you had a gut
    feeling that something was off, despite not knowing what, and based on this feeling
    started to make different decisions? Causal modeling, particularly with causal
    generative models, make it easy to write algorithms that capture this type of
    self-introspection in decision-making.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到你在工作中开始犯一些笨拙的错误，并以此作为你疲劳的证据，尽管你并不觉得如此，你心想，“如果我休息一下会怎样？” 你有没有一种直觉，觉得某件事不对劲，尽管你不知道是什么，基于这种感觉开始做出不同的决策？因果建模，尤其是因果生成模型，使得编写能够捕捉这种自我内省的决策算法变得容易。
- en: Next, we’ll look at causal modeling of sequential decision-making.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨顺序决策的因果建模。
- en: 12.3 Causal DAGs and sequential decisions
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 因果图和顺序决策
- en: Sequential decision processes are processes of back-to-back decision-making.
    These processes can involve sequential decisions made by humans or by algorithms
    and engineered agents.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序决策过程是连续决策的过程。这些过程可能涉及人类或算法和工程代理做出的顺序决策。
- en: When I model decision processes in sequence, I use a subscript to indicate a
    discrete step in the series, such as *Y*[1], *Y*[2], *Y*[3]. When I want to indicate
    an intervention subscript, I’ll place it to the right of the time-step subscript,
    as in *Y*[1,][*X*][=][*x*], *Y*[2,][*X*][=][*x*], *Y*[3,][*X*][=][*x*].
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我按顺序建模决策过程时，我使用下标来表示序列中的离散步骤，例如 *Y*[1]，*Y*[2]，*Y*[3]。当我想要表示干预下标时，我将它放在时间步下标的右侧，例如
    *Y*[1,][*X*][=][*x*]，*Y*[2,][*X*][=][*x*]，*Y*[3,][*X*][=][*x*]。
- en: In this section, I’ll show causal DAGs for several canonical sequential decision-making
    processes, but you should view these as templates, not as fixed structures. You
    can add or remove edges in whatever way you deem appropriate for a given problem.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示几个典型顺序决策过程的因果图，但你应该将这些视为模板，而不是固定结构。你可以根据给定问题适当添加或删除边。
- en: Let’s look at the simplest case, bandit feedback.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最简单的情况，即投币机反馈。
- en: 12.3.1 Bandit feedback
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 投币机反馈
- en: '*Bandit feedback* refers to cases where, at each step in the sequence, there
    is an act *X* that leads to an outcome *Y*, with some utility *U*(*Y*). A bandit
    sequence has two key features. The first is that, at every step, there is instant
    feedback after an act occurs. The second is independent trials, meaning that the
    variables at the *t*^(th) timestep are independent of variables at other timesteps.
    The term “bandit” comes from an analogy to “one-armed bandits,” which is a slang
    term for casino slot machines that traditionally have an arm that the player pulls
    to initiate gameplay. Slot machine gameplay provides bandit feedback—you deposit
    a token, pull the arm, and instantly find out if you win or lose. That outcome
    is independent of previous plays.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*投币机反馈*指的是在序列中的每个步骤，都有一个导致结果 *Y* 的行为 *X*，并伴随一些效用 *U*(*Y*)。投币机序列有两个关键特征。第一个特征是，在每个步骤，行为发生后立即有反馈。第二个特征是独立试验，意味着在
    *t*^(th) 时间步的变量与其他时间步的变量是独立的。术语“投币机”来自对“单臂老虎机”的类比，这是一个用于赌场的老虎机的俚语，传统上有一个玩家拉动以启动游戏的手臂。老虎机游戏提供投币机反馈——你投入代币，拉动手臂，并立即知道你是否赢了或输了。这个结果与之前的游戏无关。'
- en: We can capture bandit feedback with the causal DAG in figure 12.14.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用图12.14中的因果DAG捕捉多臂老虎机反馈。
- en: '![figure](../Images/CH12_F14_Ness.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F14_Ness.png)'
- en: Figure 12.14 A causal DAG illustrating simple bandit feedback
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.14 一个因果DAG，说明了简单的多臂老虎机反馈
- en: The causal DAG in figure 12.14 captures instant feedback with a utility node
    at each timestep, and with a lack of edges, reflecting an independence of variables
    across timesteps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14中的因果DAG通过每个时间步的效用节点捕捉即时反馈，并且由于缺乏边，反映了变量在时间步之间的独立性。
- en: 12.3.2 Contextual bandit feedback
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 上下文多臂老虎机反馈
- en: In contextual bandit feedback, one or more variables are common causes for both
    the act and the outcome. In figure 12.15, the context variable *C* is common to
    each {*X*, *Y*} tuple in the sequence. In this case, the context variable *C*
    could represent the profile of a particular individual, and the act variable *X*
    is that user’s behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文多臂老虎机反馈中，一个或多个变量是动作和结果的共同原因。在图12.15中，上下文变量 *C* 是序列中每个 {*X*, *Y*} 元组的共同因素。在这种情况下，上下文变量
    *C* 可以代表某个特定个体的档案，而动作变量 *X* 是该用户的行为。
- en: '![figure](../Images/CH12_F15_Ness.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F15_Ness.png)'
- en: Figure 12.15 A causal DAG illustrating contextual bandit feedback
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.15 一个因果DAG，说明了上下文多臂老虎机反馈
- en: Alternatively, the context variable could change at each step, as in figure
    12.16.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，上下文变量可以在每一步改变，如图12.16所示。
- en: '![figure](../Images/CH12_F16_Ness.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F16_Ness.png)'
- en: Figure 12.16 A causal DAG illustrating contextual bandit feedback where the
    context changes at each timestep
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.16 一个因果DAG，说明了上下文多臂老虎机反馈，其中上下文在每个时间步改变
- en: We can vary this template in different ways. For example, we could have the
    actions drive the context variables in the next timestep, as in figure 12.17\.
    The choice depends on your specific problem.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以不同的方式改变这个模板。例如，我们可以在下一个时间步中让动作驱动上下文变量，如图12.17所示。选择取决于你的具体问题。
- en: '![figure](../Images/CH12_F17_Ness.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F17_Ness.png)'
- en: Figure 12.17 A causal DAG where the action at one timestep influences the context
    at the next timestep
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.17 一个因果DAG，其中一个时间步的动作影响下一个时间步的上下文
- en: 12.3.3 Delayed feedback
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 延迟反馈
- en: In a delayed-feedback setting, the outcome variable and corresponding utility
    are no longer instant feedback. Instead, they come at the end of a sequence. Let’s
    consider an example where a context variable drives the acts. The acts affect
    the next instance of the context variable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在延迟反馈设置中，结果变量和相应的效用不再是即时反馈。相反，它们在序列的末尾出现。让我们考虑一个上下文变量驱动动作的例子。动作影响上下文变量的下一个实例。
- en: '![figure](../Images/CH12_F18_Ness.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F18_Ness.png)'
- en: Figure 12.18 Example of a causal DAG for sequential decision making with delayed
    feedback
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.18 带有延迟反馈的顺序决策因果DAG示例
- en: Again, figure 12.18 shows an example of this approach based on the previous
    model. Here the act at time *k* influences the context variable (*C*) at time
    *k* + 1, which in turn affects the act at time *k* + 1.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，图12.18展示了基于先前模型此方法的一个示例。在这里，时间 *k* 的动作影响时间 *k* + 1 的上下文变量 (*C*)，这反过来又影响时间
    *k* + 1 的动作。
- en: Consider a case of chronic pain. Here the context variable represents whether
    a subject is experiencing pain (*C*). The presence of pain drives the act of taking
    a painkiller (*X*). Taking the painkiller (or not) affects whether there is pain
    in the next step. Figure 12.19 illustrates this DAG.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个慢性疼痛的案例。在这里，上下文变量代表受试者是否正在经历疼痛 (*C*)。疼痛的存在驱动了服用止痛药的动作 (*X*)。服用止痛药（或不服用）影响下一个步骤中是否有疼痛。图12.19说明了这个DAG。
- en: '![figure](../Images/CH12_F19_Ness.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F19_Ness.png)'
- en: Figure 12.19 A causal DAG representing the treatment of chronic pain
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.19 表示慢性疼痛治疗的因果DAG
- en: '*Y* here is the ultimate health outcome of the subject, and it is driven both
    by the overall amount of pain over time, and the amount of drugs the subject took
    (because perhaps overuse of painkillers has a detrimental health effect).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* 这里是受试者的最终健康结果，它既由随时间推移的总疼痛量驱动，也由受试者所服用的药物量驱动（因为可能过度使用止痛药会有害健康）。'
- en: 12.3.4 Causal queries on a sequential model
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.4 对顺序模型进行因果查询
- en: 'We may want to calculate some causal query for our sequential decision problem.
    For example, given the DAG in figure 12.19, we might want to calculate the causal
    effect of *X*[0] on *U*(*Y*):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要计算我们的顺序决策问题的一些因果查询。例如，给定图12.19中的DAG，我们可能想要计算 *X*[0] 对 *U*(*Y*) 的因果效应：
- en: '![figure](../Images/ness-ch12-eqs-7x.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch12-eqs-7x.png)'
- en: 'Or perhaps we might be interested in the causal effect of the full sequence
    of acts on *U*(*Y*):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可能对一系列行为对 *U*(*Y*) 的因果效应感兴趣：
- en: '![figure](../Images/ness-ch12-eqs-8x.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-8x.png)'
- en: Either way, now that we have framed the sequential problem as a causal model,
    we are in familiar territory; we can simply use the causal inference tools we’ve
    learned in previous chapters to answer causal queries with this model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，现在我们已经将顺序问题作为因果模型来构建，我们进入了熟悉的领域；我们可以简单地使用我们在前几章中学到的因果推断工具来使用这个模型回答因果查询。
- en: As usual, we must be attentive to the possibility of latent causes that can
    confound our causal inference. In the case of causal effects, our concern is latent
    common confounding causes between acts (*X*) and outcomes (*Y*), or alternatively
    between acts (*X*) and utilities (*U*). Figure 12.20 is the same as figure 12.15,
    except it introduces a latent *Z* confounder.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们必须注意潜在原因的可能性，这些原因可能会混淆我们的因果推断。在因果效应的情况下，我们的关注点是行为 (*X*) 和结果 (*Y*) 之间，或者行为
    (*X*) 和效用 (*U*) 之间的潜在共同混淆原因。图12.20与图12.15相同，除了它引入了一个潜在的 *Z* 混淆因子。
- en: '![figure](../Images/CH12_F20_Ness.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F20_Ness.png)'
- en: Figure 12.20 Contextual bandit with a latent confounder
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.20 具有潜在混淆因子的上下文Bandit
- en: Similarly, we could have a unique confounder at every timestep, as in figure
    12.21.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在每个时间步长有一个独特的混淆因子，如图12.21所示。
- en: '![figure](../Images/CH12_F21_Ness.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F21_Ness.png)'
- en: Figure 12.21 Bandit feedback with a different context and latent confounders
    at each timestep
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.21 每个时间步长都有不同上下文和潜在混淆因子的Bandit反馈
- en: Similarly, figure 12.22 shows a second version of the chronic pain graph where
    the confounders affect each other and the context variables. This confounder could
    be some external factor in the subject’s environment that triggers the pain and
    affects well-being.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，图12.22显示了慢性疼痛图的第二个版本，其中混淆因子相互影响，并且影响上下文变量。这个混淆因子可能是影响疼痛和福祉的受试者环境中的某些外部因素。
- en: These confounders become an issue when we want to infer the causal effect of
    a sequence of actions on *U*(*Y*).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要推断一系列行为对 *U*(*Y*) 的因果效应时，这些混淆因子成为一个问题。
- en: '![figure](../Images/CH12_F22_Ness.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F22_Ness.png)'
- en: Figure 12.22 A version of the chronic pain DAG where the confounders affect
    each other and the context variables
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.22 一个慢性疼痛DAG的版本，其中混淆因子相互影响，并且影响上下文变量
- en: Next, we’ll look at how we can view policies for automatic decision making in
    sequential decision-making processes as stochastic interventions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何将顺序决策过程中的自动决策策略视为随机干预。
- en: 12.4 Policies as stochastic interventions
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 政策作为随机干预
- en: 'In automated sequential decision-making, the term “policy” is preferred to
    “decision rule.” I’ll introduce a special notation for a policy: *π*(.). It will
    be a function that takes in observed outcomes of other variables and returns an
    action.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动顺序决策中，术语“策略”比“决策规则”更受欢迎。我将介绍一个用于策略的特殊符号：*π*(.). 它将是一个函数，它接受其他变量的观察结果并返回一个动作。
- en: To consider how a policy affects the model, we’ll contrast the DAG before and
    after a policy is implemented. Figure 12.23 illustrates a simple example with
    a context variable *C* and a latent variable *Z*. The policy uses context *C*
    to select a value of *X*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑策略如何影响模型，我们将比较实施策略前后的DAG。图12.23展示了带有上下文变量 *C* 和潜在变量 *Z* 的简单示例。策略使用上下文 *C*
    来选择 *X* 的值。
- en: '![figure](../Images/CH12_F23_Ness.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F23_Ness.png)'
- en: Figure 12.23 The dashed lines show edges modulated by the policy. The policy
    breaks the influence of the confounder *Z* like an ideal intervention, but dependence
    on *C* remains through the policy.
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.23 虚线表示由策略调节的边。策略像理想的干预一样打破了混淆因子 *Z* 的影响，但通过策略对 *C* 的依赖仍然存在。
- en: The policy is a type of stochastic intervention; it selects a intervention value
    for *X* from some process that depends on *C*. Like an ideal intervention, it
    changes the graph. The left of figure 12.23 shows the DAG prior to deployment
    of the policy. On the right is the DAG after the policy is deployed. I add a special
    policy node to the graph to illustrate how the policy modulates the graph. The
    dashed edges highlight edges modulated by the policy. Just like an ideal intervention,
    the policy-generated intervention removes *X*’s original incoming edges *C*→*X*
    and *Z*→*X*. However, because the policy depends on *C*, the dashed edges illustrate
    the new flow of influence from *C* to *X*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是一种随机干预；它从依赖于C的某个过程中为X选择一个干预值。像理想的干预一样，它改变了图。图12.23的左侧显示了策略部署前的DAG。右侧是策略部署后的DAG。我在图中添加了一个特殊的策略节点来展示策略如何调节图。虚线边强调了由策略调节的边。就像理想的干预一样，策略生成的干预移除了X原始的输入边C→X和Z→X。然而，因为策略依赖于C，虚线边说明了从C到X的新影响流。
- en: Suppose we are interested in what value *Y* would have for a policy-selected
    action *X*=Π. In counterfactual notation, we write
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣的是策略选择的动作X=Π对于值Y会有什么影响。在反事实记法中，我们写成
- en: '![figure](../Images/ness-ch12-eqs-9x.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch12-eqs-9x.png)'
- en: 'In sequence settings, the policy applies a stochastic intervention at multiple
    steps in the sequence. From a possible worlds perspective, each intervention induces
    a new hypothetical world. This can stretch the counterfactual notation a bit,
    so going forward, I’ll simplify the counterfactual notation to look like this:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列设置中，策略在序列的多个步骤中应用随机干预。从可能世界视角来看，每个干预都会诱导一个新的假设世界。这可能会使反事实记法变得有些复杂，所以从现在开始，我将简化反事实记法，使其看起来像这样：
- en: '![figure](../Images/ness-ch12-eqs-10x.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch12-eqs-10x.png)'
- en: This means *Y*[3] (*Y* at timestep 3) is under influence of the policy’s outcomes
    at times 0, 1, and 2.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着Y[3]（第3个时间步的Y）受到策略在时间0、1和2的效用结果的影响。
- en: 12.4.1 Examples in sequential decision-making
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 序列决策中的例子
- en: In the case of bandit feedback, the actions are produced by a *bandit algorithm*,
    which is a type of policy that incorporates the entire history of actions and
    utility outcomes in deciding the optimal current action. Though actions and outcomes
    in the bandit feedback process are independent at each time step, the policy introduces
    dependence on past actions and outcomes, as shown in figure 12.24\.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在奖励机反馈的情况下，动作由一个奖励机算法产生，这是一种将整个动作和效用结果的历史纳入决定当前最优动作的策略。尽管在每一个时间步中，奖励机反馈过程中的动作和结果都是独立的，但策略引入了对过去动作和结果的依赖，如图12.24所示。
- en: '![figure](../Images/CH12_F24_Ness.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F24_Ness.png)'
- en: Figure 12.24 Bandit feedback where a bandit policy algorithm selects the next
    action based on past actions and reward outcomes
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.24 奖励机反馈，其中奖励机策略算法根据过去的行为和奖励结果选择下一个动作
- en: Recall our previous example of an agent taking pain medication in response to
    the onset of pain. Figure 12.25 shows how a policy would take in the history of
    degree of pain and how much medication was provided.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们之前关于一个代理对疼痛发作做出药物反应的例子。图12.25显示了策略如何接受疼痛程度的历史以及提供的药物剂量。
- en: '![figure](../Images/CH12_F25_Ness.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH12_F25_Ness.png)'
- en: Figure 12.25 In the pain example, the policy considers the history of recorded
    levels of pain and corresponding dosages of medication.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.25 在疼痛示例中，策略考虑了记录的疼痛程度和相应的药物剂量历史。
- en: The policy is like a doctor making the rounds on a hospital floor. They come
    to a patient’s bed, and the patient reports some level of pain. The doctor looks
    at that patient’s history of pain reports and the subsequent dosages of medication
    and uses that information to decide what dosage to provide this time. The doctor’s
    utility function is in terms of pain, risk of overdose, and risk of addiction.
    They need to consider historic data, not just the current level of pain, to optimize
    this utility function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 策略就像在医院病房里巡视的医生。他们来到病人的床边，病人报告了某种程度的疼痛。医生查看该病人的疼痛报告历史和随后的药物剂量，并使用这些信息来决定这次提供多少剂量。医生的效用函数是以疼痛、超量风险和成瘾风险为标准的。他们需要考虑历史数据，而不仅仅是当前的疼痛水平，以优化这个效用函数。
- en: 12.4.2 How policies can introduce confounding
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 策略如何引入混杂因素
- en: As stochastic interventions, policies introduce interventions conditional on
    other nodes in the graph. Because of this, there is a possibility that the policy
    will introduce new backdoor paths that can confound causal inferences. For example,
    consider again the DAG in figure 12.26.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 作为随机干预，策略在图中的其他节点上引入条件干预。正因为如此，策略可能会引入新的后门路径，这可能会混淆因果推断。例如，再次考虑图12.26中的DAG。
- en: '![figure](../Images/CH12_F26_Ness.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F26_Ness.png)'
- en: Figure 12.26 The policy eliminates the backdoor path through *Z* but not the
    backdoor path through *C*.
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.26 政策消除了通过*Z*的后门路径，但没有消除通过*C*的后门路径。
- en: The policy breaks the backdoor path from *X* to *Y* through *Z*, but there is
    still a path from *X* to *Y* through *C*. Thus, typical causal queries involving
    *X* and *Y* would have to condition on or adjust for *C*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 政策打破了从*X*到*Y*通过*Z*的后门路径，但仍然存在从*X*到*Y*通过*C*的路径。因此，涉及*X*和*Y*的典型因果查询将不得不对*C*进行条件化或调整。
- en: In the next section, we’ll characterize causal RL in causal terms.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将用因果关系来描述因果强化学习。
- en: 12.5 Causal reinforcement learning
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 因果强化学习
- en: Reinforcement learning (RL) is a branch of machine learning that generally involves
    an agent learning policies that maximize cumulative reward (utility). The agent
    learns from the consequences of its actions, rather than from being explicitly
    taught, and adjusts its behavior based on the rewards or losses (reinforcements)
    it receives. Many sequential decision-making problems can be cast as RL problems.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个分支，通常涉及代理学习最大化累积奖励（效用）的策略。代理从其行动的后果中学习，而不是从被明确教导中学习，并根据它收到的奖励或损失（强化）调整其行为。许多序列决策问题都可以表示为RL问题。
- en: 12.5.1 Connecting causality and Markov decision processes
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.1 因果关系与马尔可夫决策过程的连接
- en: RL typically casts a decision process as a Markov decision process (MDP). A
    canonical toy example of an MDP is a grid world, illustrated in figure 12.27\.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: RL通常将决策过程表示为马尔可夫决策过程（MDP）。一个典型的MDP玩具示例是网格世界，如图12.27所示。
- en: Figure 12.27 presents a 3 × 4 grid world. An agent can act within this grid
    world with a fixed set of actions, moving up, down, left, and right. The agent
    wants to execute a set of actions that deliver it to the upper-right corner {0,
    3}, where it gains a reward of 100\. The agent wants to avoid the middle-right
    square {1, 3}, where it has a reward of –100 (a *loss* of 100). Position {1, 1}
    contains an obstacle the agent cannot traverse.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.27展示了一个3 × 4的网格世界。代理可以在这个网格世界中执行一组固定的动作，向上、向下、向左和向右移动。代理想要执行一组动作，将其带到右上角{0,
    3}，在那里它获得100的奖励。代理想要避开中间右边的方块{1, 3}，在那里它有-100的奖励（损失100）。位置{1, 1}包含代理无法穿越的障碍物。
- en: '![figure](../Images/CH12_F27_Ness.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F27_Ness.png)'
- en: Figure 12.27 A simple grid world
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.27 简单的网格世界
- en: We can think of it as a game. When the game starts, the agent “spawns” randomly
    in one of the squares, except for {0, 3}, {1, 3}, and {1, 1}. When the agent moves
    into a goal square, the game ends. To win, the agent must navigate around the
    obstacle in {1, 1}, avoid {1, 3}, and reach {0, 3}.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其视为一个游戏。当游戏开始时，代理“随机”出现在一个方块中，除了{0, 3}、{1, 3}和{1, 1}。当代理移动到目标方块时，游戏结束。为了获胜，代理必须绕过{1,
    1}中的障碍物，避开{1, 3}，并到达{0, 3}。
- en: A Markov decision process models this and much more complicated “worlds” (aka
    domains, problems, etc.) with abstractions for states, actions, transition functions,
    and rewards.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程使用状态、动作、转移函数和奖励的抽象来模拟这个和更多复杂的“世界”（即领域、问题等）。
- en: States
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 状态
- en: States are a set that represents the current situation or context that the agent
    is in, within its environment. In the grid-world example, a state represents the
    agent being at a specific cell. In this grid, there are 12 different states (the
    cell at {1, 1} is an unreachable state). We assume the agent has some way of knowing
    which state they are in.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 状态集表示代理在其环境中当前所处的情况或上下文。在网格世界示例中，一个状态表示代理位于特定的单元格。在这个网格中，有12种不同的状态（{1, 1}单元格是不可达状态）。我们假设代理有某种方式知道他们处于哪个状态。
- en: We’ll denote state as a variable *S*. In a grid world, *S* is a discrete variable,
    but in other problems, *S* could be continuous.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用变量*S*表示状态。在网格世界中，*S*是一个离散变量，但在其他问题中，*S*可能是连续的。
- en: Actions
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作
- en: Actions are the things the agent can do, and they lead to a change of state.
    Some actions might not be available when in a particular state. For example, in
    the grid world, the borders of the grid are constraints on the movements of the
    agent. If the agent is in the bottom-left square {2, 0}, and they try to move
    left or down, they will stay in place. Similarly, the cell at {1, 1} is an obstacle
    the agent must navigate around. We denote actions with the variable *A*, which
    has four possible outcomes {up, down, right, left}.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 动作是智能体可以执行的事情，并且会导致状态的变化。某些动作在特定状态下可能不可用。例如，在网格世界中，网格的边缘是智能体运动的约束。如果智能体处于左下角方格
    {2, 0}，并且尝试向左或向下移动，它们将保持在原地。同样，{1, 1} 中的单元格是智能体必须绕过的障碍。我们用变量 *A* 表示动作，它有四种可能的结果
    {上，下，右，左}。
- en: Transition function
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转移函数
- en: The transition function is a probability distribution function. It tells us
    the probability of moving to a specific next state, given the current state and
    the action taken.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 转移函数是一个概率分布函数。它告诉我们，在给定当前状态和采取的动作的情况下，移动到特定下一个状态的概率。
- en: 'If states are discrete, the transition function looks like this:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态是离散的，转移函数看起来像这样：
- en: '![figure](../Images/ness-ch12-eqs-11x.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-11x.png)'
- en: Here, *S**[t]*=*s* means the agent is currently in state *s*. *A**[t]*=*a* means
    the agent performs action *a*. *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    is the probability that the agent transitions to a new state s' given it is in
    state s and performs action *a*. When the action leads to a new state with complete
    certainty, this probability distribution function becomes degenerate (all probability
    is concentrated on one value).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*S**[t]*=*s* 表示智能体当前处于状态 *s*。*A**[t]*=*a* 表示智能体执行动作 *a*。*P*(*S**[t]**[+1]*=*s'|S**[t]*=*s,
    A**[t]*=*a*) 是智能体在处于状态 s 并执行动作 *a* 的情况下过渡到新状态 s' 的概率。当动作以完全确定的方式导致新状态时，这个概率分布函数变得退化（所有概率都集中在单个值上）。
- en: Rewards
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励
- en: The term “reward” is preferred to “utility” in RL. In the context of MDPs, the
    reward function will always take a state *s* as an argument. We will write it
    as *U*(*s*).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，“奖励”一词比“效用”更受欢迎。在MDP的上下文中，奖励函数将始终以状态 *s* 作为参数。我们将它写成 *U*(*s*)。
- en: In the grid-world example, *U*({0, 3}) = 100, *U*({1, 3)) = –100\. The reward
    of all other states is 0\. Note that sometimes in the MDP/RL literature, *U*()
    is a function of state and an action, as in *U*(*s*, *a*). We don’t lose anything
    by just having actions be a function of state because you can always fold actions
    into the definition of a state.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格世界示例中，*U*({0, 3}) = 100，*U*({1, 3}) = –100。所有其他状态的奖励为0。请注意，有时在MDP/RL文献中，*U*()是状态和动作的函数，如*U*(*s*,
    *a*)。仅仅让动作成为状态的函数并不会失去任何东西，因为你可以总是将动作折叠到状态的定义中。
- en: 12.5.2 The MDP as a causal DAG
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.2 MDP作为因果DAG
- en: Figure 12.28 shows the MDP as a causal DAG.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.28展示了MDP作为一个因果DAG。
- en: '![figure](../Images/CH12_F28_Ness.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F28_Ness.png)'
- en: Figure 12.28 The Markov decision process represented as a DAG
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.28 以DAG表示的马尔可夫决策过程
- en: As a causal DAG, the MDP looks like the other sequential decision processes
    we’ve outlined, except that we limit ourselves to states, actions, and rewards.
    In figure 12.28, the process continues until we reach a terminal state (*S*[*k*]),
    such as getting to the terminal cells in the grid-world example.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 作为因果DAG，MDP看起来像我们概述的其他顺序决策过程，只不过我们限制了自己只关注状态、动作和奖励。在图12.28中，过程一直持续到我们达到终端状态
    (*S*[*k*])，例如在网格世界示例中到达终端单元格。
- en: The causal Markov property and the MDP
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果马尔可夫性质和MDP
- en: 'The “Markov” in “Markov decision process” comes from the fact that the current
    state is independent of the full history of states given the last state. Contrast
    this with the causal Markov property of causal DAGs: a node in the DAG is independent
    of indirect “ancestor” causes given its direct causal parents. We can see that
    when we view the MDP as a causal DAG, this Markovian assumption is equivalent
    to the causal Markov property. That means we can use our d-separation-based causal
    reasoning, including the do-calculus, in the MDP graphical setting.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: “马尔可夫决策过程”中的“马尔可夫”一词来源于当前状态在给定最后一个状态的情况下与状态的完整历史无关的事实。这与因果DAG的因果马尔可夫性质形成对比：在DAG中，一个节点在其直接因果父节点给定的情况下，与间接“祖先”原因无关。我们可以看到，当我们把MDP看作因果DAG时，这个马尔可夫假设等价于因果马尔可夫性质。这意味着我们可以在MDP图形设置中使用基于d-separation的因果推理，包括do-calculus。
- en: The transition function and the causal Markov kernel
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转移函数和因果马尔可夫核
- en: Note that based on this DAG, the parents of a state *S*[(]*[t]*[+1)] are the
    previous state *S**[t]* and the action *A**[t]* taken when in that previous state.
    Therefore, the causal Markov kernel is *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*),
    i.e., the transition function. Thus, the transition function is the causal Markov
    kernel for a given state.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，基于这个DAG，状态 *S*[(]*[t]*[+1)] 的父节点是先前的状态 *S**[t]* 和在先前状态下采取的动作 *A**[t]*。因此，因果马尔可夫核是
    *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)，即转移函数。因此，转移函数是给定状态的因果马尔可夫核。
- en: 12.5.3 Partially observable MDPs
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.3 部分可观察MDP
- en: An extension of MDPs is *partially observed MDPs* (POMDPs). In a POMDP, the
    agent doesn’t know with certainty what state they are in, and they must make inferences
    about that state given incomplete evidence from their environment. This applies
    to many practical problems where the agent cannot observe the full state of the
    environment.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的扩展是 *部分可观察MDP*（POMDP）。在POMDP中，代理不能确定他们处于什么状态，他们必须根据从环境中的不完整证据对该状态进行推断。这适用于许多实际问题，其中代理无法观察到环境的完整状态。
- en: A POMDP can entail different causal structures depending on our assumptions
    about the causal relationships between the unobserved and observed states. For
    example, suppose a latent state *S* is a cause of the observed state *X*. The
    observed state *X* now drives the act *A* instead of *S*. Figure 12.29 illustrates
    this formulation of a POMDP as a causal DAG.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 一个POMDP可以根据我们对未观察状态和观察状态之间因果关系的假设具有不同的因果结构。例如，假设一个潜在状态 *S* 是观察状态 *X* 的原因。现在观察状态
    *X* 驱动行为 *A* 而不是 *S*。图12.29展示了将POMDP作为因果有向无环图（DAG）的这种表述。
- en: '![figure](../Images/CH12_F29_Ness.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F29_Ness.png)'
- en: Figure 12.29 A POMDP where a latent state *S* causes an observed state *X*.
    *X* drives the actions *A*.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.29展示了潜在状态 *S* 导致观察状态 *X* 的POMDP。*X* 驱动动作 *A*。
- en: In contrast, figure 12.30 illustrates an example where the latent state is a
    latent common cause (denoted Z) of the observed state (mediated through the agent’s
    action) and the utility (note a slight change of notation from *U*(*S*[*i*]) to
    *U*[*i*]). Here, unobserved factors influence both the agent’s behavior and the
    resulting utility of that behavior.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，图12.30展示了一个例子，其中潜在状态是观察状态（通过代理的动作中介）和效用（注意从 *U*(*S*[*i*]) 到 *U*[*i*] 的符号略有变化）的潜在共同原因（表示为Z）。在这里，未观察因素影响代理的行为以及该行为的效用。
- en: Again, the basic MDP and POMDP DAGs should be seen as templates for starting
    our analysis. Once we understand what causal queries we are interested in answering,
    we can explicitly represent various components of observed and unobserved states
    as specific nodes in the graph, and then use identification and adjustment techniques
    to answer our causal queries.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，基本的MDP和POMDP DAG应被视为我们分析的模板。一旦我们理解了我们感兴趣回答的因果查询，我们就可以将观察状态和未观察状态的各个组成部分明确地表示为图中的特定节点，然后使用识别和调整技术来回答我们的因果查询。
- en: '![figure](../Images/CH12_F30_Ness.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F30_Ness.png)'
- en: Figure 12.30 A POMPD formulation where the unobserved states are latent common
    causes that could act as confounders in causal inferences
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.30展示了未观察状态是潜在共同原因的POMDP表述，这些原因可能作为因果推断中的混杂因素。
- en: 12.5.4 Policy in an MDP
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.4 MDP中的策略
- en: As before, policies in an MDP act as stochastic interventions. Figure 12.31
    illustrates a policy that selects an optimal action based on the current state
    in a way that disrupts any influence on the action from a confounder.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，MDP中的策略作为随机干预。图12.31展示了在当前状态下选择最优动作的策略，这种方式会破坏任何来自混杂因素的影响。
- en: '![figure](../Images/CH12_F31_Ness.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F31_Ness.png)'
- en: Figure 12.31 Modification of an MDP DAG by a policy
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.31展示了MDP DAG的修改
- en: Figure 12.31 is simple in that it only selects an action based on the current
    state. The challenge is in the implementation, because in most RL settings, states
    can be high-dimensional objects.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.31在简单性上，它仅基于当前状态选择一个动作。挑战在于实现，因为在大多数RL设置中，状态可以是高维对象。
- en: 12.5.5 Causal Bellman equation
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.5 因果Bellman方程
- en: 'RL is about searching for the optimal policy, which is characterized with the
    Bellman equation, often written as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是关于寻找最优策略，该策略由Bellman方程表征，通常如下所示：
- en: '![figure](../Images/ness-ch12-eqs-12x.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-12x.png)'
- en: In plain words, we’re looking for a policy Π^* maximizes the cumulative reward
    over time. Here *γ* is a discount rate, a value between 0 and 1, that makes sure
    the agent values rewards in the near future more than rewards in the far future.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说，我们正在寻找一个策略Π^*，它最大化了随时间累积的奖励。在这里，*γ*是一个折现率，其值在0和1之间，确保智能体更重视近期的奖励而不是远期的奖励。
- en: 'Since we’re reasoning about what would happen if we deployed the policy, the
    causal formulation would be as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在推理如果我们部署了策略会发生什么，因此因果公式如下：
- en: '![figure](../Images/ness-ch12-eqs-13x.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-13x.png)'
- en: Note that we could do the same causal rewrite for other variants of the Bellman
    equation, such as the Q-function used in Q-learning.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以对贝尔曼方程的其他变体执行相同的因果重写，例如Q学习中使用的Q函数。
- en: The difference between the noncausal and causal formulations of the Bellman
    equation is the same as the difference between optimizing *E*(*U*(*Y*)|*X*=*x*)
    and *E*(*U*(*Y*[*X*][=][*x*])) in section 12.2.4\. The process of solving the
    causally naive version of the Bellman equation may introduce biases from latent
    confounders or from conditioning on colliders and mediators. Our causally attuned
    approach can help avoid these biases. In many cases, the solution of the naive
    approach will coincide with the causal approach because those biases might not
    affect the ranking of the top policy relative to others. However, as in the *E*(*U*(*Y*)|*X*=*x*)
    versus *E*(*U*(*Y*[*X*][=][*x*])) example, there will be cases where the solutions
    to the noncausal and causal formulations differ, and your RL problem might be
    one of those cases.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程的非因果和因果公式的区别与第12.2.4节中优化*E*(*U*(*Y*)|*X*=*x*)和*E*(*U*(*Y*[*X*][=][*x*]))的区别相同。解决贝尔曼方程因果天真版本的进程可能会引入来自潜在混杂因素或来自对碰撞者和中介条件的条件的偏差。我们调整因果的方法可以帮助避免这些偏差。在许多情况下，天真方法的解决方案将与因果方法一致，因为那些偏差可能不会影响相对于其他策略的顶级策略的排名。然而，正如*E*(*U*(*Y*)|*X*=*x*)与*E*(*U*(*Y*[*X*][=][*x*]))的例子一样，将会有一些情况下非因果和因果公式的解决方案不同，你的强化学习问题可能是那些情况之一。
- en: 12.6 Counterfactual reasoning for decision-theory
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 决策理论中的反事实推理
- en: So far, we’ve discussed the problem of choosing optimal actions with respect
    to a utility function as a level 2 query on the causal hierarchy. Is there a use
    for level 3 counterfactual reasoning in decision theory? In this section, we’ll
    briefly review some applications for level 3 reasoning.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在因果层次结构中作为第二级查询的，关于根据效用函数选择最优动作的问题。在决策理论中，第三级反事实推理有什么用？在本节中，我们将简要回顾一些第三级推理的应用。
- en: 12.6.1 Counterfactual policy evaluation
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.1 反事实策略评估
- en: Counterfactual policy evaluation involves taking logged data from a policy in
    production and asking, “given we used this policy and got this cumulative reward,
    how much cumulative reward would we have gotten had we used a different policy?”
    See the chapter notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references to techniques such as *counterfactually guided policy search* and
    *counterfactual risk minimization*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实策略评估涉及从生产中的策略中提取日志数据，并问自己，“既然我们使用了这个策略并获得了这个累积奖励，如果我们使用了不同的策略，我们会获得多少累积奖励？”参见章节注释[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)，了解诸如*反事实引导策略搜索*和*反事实风险最小化*等技术。
- en: 12.6.2 Counterfactual regret minimization
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.2 反事实遗憾最小化
- en: In chapters 8 and 9, I introduced *regret* as a counterfactual concept. We can
    further clarify the idea now that we have introduced the language of decision-making;
    regret is the difference between the utility/reward that was realized given a
    specific action or set of actions, and the utility/reward that would have been
    realized had another action or set of actions been taken.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章和第9章中，我介绍了*遗憾*作为一个反事实概念。现在我们已经引入了决策语言，我们可以进一步阐明这个想法；遗憾是在给定特定动作或一组动作时实现的效用/奖励与如果采取了另一个动作或一组动作本应实现的效用/奖励之间的差异。
- en: '*Counterfactual regret minimization* is an approach to optimizing policies
    that seeks to minimize regret. To illustrate, suppose we have a policy variable
    *Π*, which can return one of several available policies. The policies take in
    the context and return an action. The action leads to some reward *U*.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '*反事实遗憾最小化*是一种优化策略的方法，旨在最小化遗憾。为了说明，假设我们有一个策略变量*Π*，它可以返回几个可用的策略之一。这些策略接受上下文并返回一个动作。这个动作导致一些奖励*U*。'
- en: Suppose, for a single instance in our logged data, the policy was *Π*=*π* and
    the context was *C*=*c*. We get a certain action *A*=*π*(*c*) and reward *U*=*u*.
    For some policy *π**'*, regret is the answer to the counterfactual question, “How
    much more reward would we have gotten if the policy had been *π*=*π**'*?” In terms
    of expectation,
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，在我们的日志数据中，对于单个实例，策略是 *Π*=*π*，上下文是 *C*=*c*。我们得到一个特定的动作 *A*=*π*(*c*) 和奖励 *U*=*u*。对于某个策略
    *π**'*)，遗憾是对反事实问题的回答，“如果策略是 *π*=*π**'*，我们会得到多少更多的奖励？”从期望的角度来看，
- en: '![figure](../Images/ness-ch12-eqs-14x.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-14x.png)'
- en: Again, this is regret for a single instance in logged data where the context
    was *C*=*c* and the utility was *u*. There are many variations, but the general
    idea is to find the policy that would have minimized cumulative regret over all
    the cases of *C*=*c* in the logged data, with the goal of favoring that policy
    in cases of *C*=*c* in the future.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是在日志数据中单个实例的遗憾，其中上下文是 *C*=*c*，效用是 *u*。有许多变化，但总体思想是找到能够最小化所有 *C*=*c* 情况的累积遗憾的策略，目标是优先考虑在未来的
    *C*=*c* 情况下该策略。
- en: 12.6.3 Making level 3 assumptions in decision problems
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.3 在决策问题中做出第3级假设
- en: The question, of course, is how to make the level 3 assumptions that enable
    counterfactual inferences. One approach would be to specify an SCM and use the
    general algorithm for counterfactual reasoning (discussed in chapter 9). For example,
    in RL, the transition function *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*)
    captures the rules of state changes in the environment. As I mentioned, *P*(*S**[t]**[+]**[1]**|S**[t]*=*s,
    A**[t]*=*a*) is the causal Markov kernel for a given state *S**[t]**[+1]*. We
    could specify an SCM with an assignment function that entails that causal Markov
    kernel, and write that assignment function as
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，问题是如何做出第3级假设，以实现反事实推理。一种方法可能是指定一个SCM并使用反事实推理的通用算法（在第9章中讨论）。例如，在强化学习（RL）中，转移函数
    *P*(*S**[t]**[+1]*=*s'|S**[t]*=*s, A**[t]*=*a*) 捕获环境状态变化的规则。正如我提到的，*P*(*S**[t]**[+]**[1]**|S**[t]*=*s,
    A**[t]*=*a*) 是给定状态 *S**[t]**[+1]* 的因果马尔可夫核。我们可以指定一个具有赋值函数的SCM，该赋值函数包含因果马尔可夫核，并将该赋值函数写成
- en: '![figure](../Images/ness-ch12-eqs-15x.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch12-eqs-15x.png)'
- en: Here, *n*[*s*]*[']* is the value of an exogenous variable for *S**[t]*.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n*[*s*]*[']* 是外生变量 *S**[t]* 的值。
- en: The challenge is specifying assignment functions that encode the correct counterfactual
    distributions. This is easier in domains where we know more about the underlying
    causal mechanisms. A key example is in rule-based games; game rules can provide
    the level 3 constraints that enable simulation of counterfactuals. Recall how,
    in chapter 9, the simple rules of the Monte Hall problem enabled us to simulate
    counterfactual outcomes for stay versus switch strategies. Or consider multiplayer
    games like poker, where in a round of play each player is dealt a hand of cards
    and can take certain actions (check, bet, call, raise, or fold) that lead to outcomes
    (win, lose, tie) based on simple rules, which in turn determine the amount of
    chips won or lost in that round. A player’s counterfactual regret is the difference
    between the chips they netted and the most they could have netted had they decided
    on different actions. This is done while accounting for the information available
    at the time of the decision, not using hindsight about the opponents’ cards.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于指定编码正确反事实分布的赋值函数。在我们对底层因果机制了解更多的领域里，这更容易做到。一个关键例子是在基于规则的博弈中；游戏规则可以提供第3级约束，从而实现反事实的模拟。回想一下，在第9章中，蒙特霍尔问题的简单规则使我们能够模拟保持和切换策略的反事实结果。或者考虑像扑克这样的多人游戏，在每一轮游戏中，每位玩家都会被发一手牌，并可以采取某些行动（检查、下注、跟注、加注或弃牌），这些行动基于简单的规则，进而决定该轮赢得或输掉筹码的数量。玩家的反事实遗憾是他们实际获得的筹码与如果他们决定采取不同的行动可能获得的最多筹码之间的差异。这是在考虑决策时可用信息的同时完成的，而不是使用关于对手牌的后见之明。
- en: Counterfactual regret minimization algorithms in this domain attempt to find
    game playing policies that minimize counterfactual regret across multiple players.
    The concrete rules of the game enable simulation of counterfactual game trajectories.
    The challenge lies in searching for optimal policies within a space of possible
    counterfactual trajectories that is quite large because of multiple player interactions
    over several rounds of play. See the chapter notes on counterfactual regret minimization
    in multiagent games at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的反事实后悔最小化算法试图找到游戏策略，以最小化多个玩家之间的反事实后悔。游戏的规则具体，使得可以模拟反事实的游戏轨迹。挑战在于在由于多轮游戏中的多个玩家交互而相当大的可能反事实轨迹空间中寻找最优策略。有关多智能体游戏中反事实后悔最小化的章节注释，请参阅[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Decision-making is naturally a causal problem because decisions cause consequences,
    and our goal is to make the decision that leads to favorable consequences.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策天然是一个因果问题，因为决策导致后果，我们的目标是做出导致有利后果的决策。
- en: Choosing the optimal decision is a level 2 query as we are asking “what would
    happen if I made this decision?”
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最优决策是一个二级查询，因为我们是在问“如果我做出这个决策会发生什么？”
- en: '*E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are different quantities.
    Usually, people want to know the value of *X* that optimizes *E*(*U*(*Y*[*X*][=][*x*])),
    but optimizing *E*(*U*(*Y*|*X*=*x*)) will often yield the same answer without
    the bother of specifying a causal model.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*(*U*(*Y*|*X*=*x*)) 和 *E*(*U*(*Y*[*X*][=][*x*])) 是不同的量。通常，人们想知道优化 *E*(*U*(*Y*[*X*][=][*x*]))
    的 *X* 的值，但优化 *E*(*U*(*Y*|*X*=*x*)) 通常会在不指定因果模型的情况下得到相同的答案。'
- en: This is especially true in reinforcement learning (RL), where the analogs to
    *E*(*U*(*Y*|*X*=*x*)) and *E*(*U*(*Y*[*X*][=][*x*])) are, respectively, the conventional
    and causal formulations of the Bellman equation. Confounder, mediator, and collider
    biases may be present in conventional approaches to solving the Bellman equation.
    But those bias often don’t influence the ranking of the top policy relative to
    other policies.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这在强化学习（RL）中尤其如此，其中 *E*(*U*(*Y*|*X*=*x*)) 和 *E*(*U*(*Y*[*X*][=][*x*])) 分别是Bellman方程的传统和因果形式。在解决Bellman方程的传统方法中可能存在混杂因子、中介因子和碰撞因子偏差。但那些偏差通常不会影响相对于其他策略的顶级策略的排名。
- en: Nonetheless, sometimes the value of *X* that optimizes *E*(*U*(*Y*|*X*=*x*))
    is different from that which optimizes *E*(*U*(*Y*[*X*][=][*x*])). Similarly,
    addressing causal nuances when solving the Bellman equation may result in a different
    policy than ignoring them. If your decision problem falls into this category,
    causal approaches are the better choice.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管如此，有时优化 *E*(*U*(*Y*|*X*=*x*)) 的 *X* 的值与优化 *E*(*U*(*Y*[*X*][=][*x*])) 的值不同。同样，在解决Bellman方程时处理因果细微差别可能会导致与忽略它们时不同的策略。如果你的决策问题属于此类，因果方法是一个更好的选择。
- en: Newcomb’s paradox is a thought experiment meant to contrast causal and noncausal
    approaches to decision theory. The “paradox” is less mysterious once we use a
    formal causal model.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新comb悖论是一个旨在对比决策理论中因果和非因果方法的思维实验。“悖论”一旦我们使用正式的因果模型，就会显得不那么神秘。
- en: Causal decision theory, combined with probabilistic modeling tools like Pyro
    and pgmpy, is well suited to modeling introspection, where an agent reflects on
    their internal state (feelings, intuition, urges, intent) and uses that information
    to predict the “what-if” outcomes of their decisions.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合概率建模工具如Pyro和pgmpy的因果决策理论非常适合于建模内省，其中智能体反思其内部状态（感觉、直觉、冲动、意图），并使用这些信息来预测决策的“如果”结果。
- en: When we represent a sequential decision process with a causal DAG, we can employ
    all the tools of graphical causal inference in that decision problem.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们用一个因果有向图（DAG）来表示一个顺序决策过程时，我们可以在这个决策问题中运用所有图形因果推理的工具。
- en: Policies operate like stochastic interventions. They change the graph but still
    have dependence on observed nodes in the past, and that dependence can introduce
    backdoor confounding.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略像随机干预一样运作。它们改变图，但仍然依赖于过去观察到的节点，这种依赖可能会引入后门混杂。
- en: In causal RL, we can represent MDPs and POMDPs as causal DAGs and, again, make
    use of graphical causal inference theory.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在因果RL中，我们可以将MDPs和POMDPs表示为因果DAG，并再次利用图形因果推理理论。
- en: We can use template DAGs to represent sequential decision processes, but you
    should tailor these templates for your problem.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用模板有向无环图（DAG）来表示顺序决策过程，但你应该根据你的问题对这些模板进行定制。
- en: Common use cases for counterfactual reasoning in decision theory are counterfactual
    policy evaluation and counterfactual regret minimization.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策理论中关于反事实推理的常见用例包括反事实政策评估和反事实后悔最小化。
- en: If you have access to the rules underlying state transitions in your MDP, such
    as in physical systems or games, you could build an SCM that is counter- factually
    faithful to those rules, and use it to handle counterfactual use cases in decision-making.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你能够访问你的马尔可夫决策过程（MDP）中状态转换的底层规则，例如在物理系统或游戏中，你可以构建一个与这些规则反事实忠实对应的SCM（结构化因果模型），并利用它来处理决策中的反事实用例。
