- en: Chapter 7\. Energy-Based Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬7ç« ã€‚åŸºäºèƒ½é‡çš„æ¨¡å‹
- en: Energy-based models are a broad class of generative model that borrow a key
    idea from modeling physical systemsâ€”namely, that the probability of an event can
    be expressed using a Boltzmann distribution, a specific function that normalizes
    a real-valued energy function between 0 and 1\. This distribution was originally
    formulated in 1868 by Ludwig Boltzmann, who used it to describe gases in thermal
    equilibrium.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹æ˜¯ä¸€ç±»å¹¿æ³›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå€Ÿé‰´äº†å»ºæ¨¡ç‰©ç†ç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®æ€æƒ³â€”â€”å³ï¼Œäº‹ä»¶çš„æ¦‚ç‡å¯ä»¥ç”¨ç»å°”å…¹æ›¼åˆ†å¸ƒæ¥è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šå‡½æ•°ï¼Œå°†å®å€¼èƒ½é‡å‡½æ•°å½’ä¸€åŒ–åœ¨0åˆ°1ä¹‹é—´ã€‚è¿™ä¸ªåˆ†å¸ƒæœ€åˆæ˜¯ç”±è·¯å¾·ç»´å¸ŒÂ·ç»å°”å…¹æ›¼åœ¨1868å¹´æå‡ºçš„ï¼Œä»–ç”¨å®ƒæ¥æè¿°å¤„äºçƒ­å¹³è¡¡çŠ¶æ€çš„æ°”ä½“ã€‚
- en: In this chapter, we will see how we can use this idea to train a generative
    model that can be used to produce images of handwritten digits. We will explore
    several new concepts, including contrastive divergence for training the EBM and
    Langevin dynamics for sampling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæƒ³æ³•æ¥è®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ‰‹å†™æ•°å­—çš„å›¾åƒã€‚æˆ‘ä»¬å°†æ¢ç´¢å‡ ä¸ªæ–°æ¦‚å¿µï¼ŒåŒ…æ‹¬ç”¨äºè®­ç»ƒEBMçš„å¯¹æ¯”æ•£åº¦å’Œç”¨äºé‡‡æ ·çš„æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ã€‚
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: We will begin with a short story to illustrate the key concepts behind energy-based
    models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹å¼€å§‹ï¼Œä»¥è¯´æ˜åŸºäºèƒ½é‡çš„æ¨¡å‹èƒŒåçš„å…³é”®æ¦‚å¿µã€‚
- en: The story of Diane Mixx and the Long-au-Vin running club captures the key ideas
    behind energy-based modeling. Letâ€™s now explore the theory in more detail, before
    we implement a practical example using Keras.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Diane Mixxå’ŒLong-au-Vinè·‘æ­¥ä¿±ä¹éƒ¨çš„æ•…äº‹æ•æ‰äº†åŸºäºèƒ½é‡å»ºæ¨¡çš„å…³é”®æ€æƒ³ã€‚ç°åœ¨è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¢è®¨ç†è®ºï¼Œç„¶åæˆ‘ä»¬å°†ä½¿ç”¨Keraså®ç°ä¸€ä¸ªå®é™…ç¤ºä¾‹ã€‚
- en: Energy-Based Models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹
- en: Energy-based models attempt to model the true data-generating distribution using
    a *Boltzmann distribution* ([Equation 7-1](#boltzmann_equation)) where <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> is know as the *energy function* (or *score*) of an observation
    <math alttext="x"><mi>x</mi></math> .
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹è¯•å›¾ä½¿ç”¨*ç»å°”å…¹æ›¼åˆ†å¸ƒ*ï¼ˆ[æ–¹ç¨‹å¼7-1](#boltzmann_equation)ï¼‰æ¥å»ºæ¨¡çœŸå®çš„æ•°æ®ç”Ÿæˆåˆ†å¸ƒï¼Œå…¶ä¸­ <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> è¢«ç§°ä¸ºè§‚å¯Ÿç»“æœ <math alttext="x"><mi>x</mi></math> çš„*èƒ½é‡å‡½æ•°*ï¼ˆæˆ–*åˆ†æ•°*ï¼‰ã€‚
- en: Equation 7-1\. Boltzmann distribution
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼7-1ã€‚ç»å°”å…¹æ›¼åˆ†å¸ƒ
- en: <math alttext="p left-parenthesis bold x right-parenthesis equals StartFraction
    e Superscript minus upper E left-parenthesis bold x right-parenthesis Baseline
    Over integral Underscript ModifyingAbove bold x With bold caret element-of bold
    upper X Endscripts e Superscript minus upper E left-parenthesis ModifyingAbove
    bold x With bold caret right-parenthesis Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mi>ğ±</mi><mo>)</mo></mrow></msup> <mrow><msub><mo>âˆ«</mo>
    <mrow><mover accent="true"><mi>ğ±</mi> <mo>^</mo></mover><mo>âˆˆ</mo><mi>ğ—</mi></mrow></msub>
    <msup><mi>e</mi> <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mover accent="true"><mi>ğ±</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p left-parenthesis bold x right-parenthesis equals StartFraction
    e Superscript minus upper E left-parenthesis bold x right-parenthesis Baseline
    Over integral Underscript ModifyingAbove bold x With bold caret element-of bold
    upper X Endscripts e Superscript minus upper E left-parenthesis ModifyingAbove
    bold x With bold caret right-parenthesis Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mi>ğ±</mi><mo>)</mo></mrow></msup> <mrow><msub><mo>âˆ«</mo>
    <mrow><mover accent="true"><mi>ğ±</mi> <mo>^</mo></mover><mo>âˆˆ</mo><mi>ğ—</mi></mrow></msub>
    <msup><mi>e</mi> <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mover accent="true"><mi>ğ±</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: In practice, this amounts to training a neural network <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> to output low scores for likely observations (so <math
    alttext="p bold x"><mrow><mi>p</mi> <mi>ğ±</mi></mrow></math> is close to 1) and
    high scores for unlikely observations (so <math alttext="p bold x"><mrow><mi>p</mi>
    <mi>ğ±</mi></mrow></math> is close to 0).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œè¿™æ„å‘³ç€è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>ï¼Œå¯¹å¯èƒ½çš„è§‚å¯Ÿè¾“å‡ºä½åˆ†æ•°ï¼ˆä½¿å¾— <math alttext="p
    bold x"><mrow><mi>p</mi> <mi>ğ±</mi></mrow></math> æ¥è¿‘äº1ï¼‰ï¼Œå¯¹ä¸å¤ªå¯èƒ½çš„è§‚å¯Ÿè¾“å‡ºé«˜åˆ†æ•°ï¼ˆä½¿å¾— <math
    alttext="p bold x"><mrow><mi>p</mi> <mi>ğ±</mi></mrow></math> æ¥è¿‘äº0ï¼‰ã€‚
- en: There are two challenges with modeling the data in this way. Firstly, it is
    not clear how we should use our model for sampling new observationsâ€”we can use
    it to generate a score given an observation, but how do we generate an observation
    that has a low score (i.e., a plausible observation)?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨è¿™ç§æ–¹å¼å»ºæ¨¡æ•°æ®å­˜åœ¨ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸æ¸…æ¥šå¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹æ¥é‡‡æ ·æ–°çš„è§‚å¯Ÿç»“æœâ€”â€”æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥ç”Ÿæˆç»™å®šè§‚å¯Ÿç»“æœçš„åˆ†æ•°ï¼Œä½†å¦‚ä½•ç”Ÿæˆä¸€ä¸ªå¾—åˆ†ä½çš„è§‚å¯Ÿç»“æœï¼ˆå³ï¼Œä¸€ä¸ªå¯ä¿¡çš„è§‚å¯Ÿç»“æœï¼‰ï¼Ÿ
- en: Secondly, the normalizing denominator of [Equation 7-1](#boltzmann_equation)
    contains an integral that is intractable for all but the simplest of problems.
    If we cannot calculate this integral, then we cannot use maximum likelihood estimation
    to train the model, as this requires that <math alttext="p bold x"><mrow><mi>p</mi>
    <mi>ğ±</mi></mrow></math> is a valid probability distribution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œ[æ–¹ç¨‹å¼7-1](#boltzmann_equation)çš„æ ‡å‡†åŒ–åˆ†æ¯åŒ…å«ä¸€ä¸ªå¯¹äºé™¤äº†æœ€ç®€å•çš„é—®é¢˜å¤–éƒ½éš¾ä»¥è®¡ç®—çš„ç§¯åˆ†ã€‚å¦‚æœæˆ‘ä»¬æ— æ³•è®¡ç®—è¿™ä¸ªç§¯åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ— æ³•ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºè¿™è¦æ±‚
    <math alttext="p bold x"><mrow><mi>p</mi> <mi>ğ±</mi></mrow></math> æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: The key idea behind an energy-based model is that we can use approximation techniques
    to ensure we never need to calculate the intractable denominator. This is in contrast
    to, say, a normalizing flow, where we go to great lengths to ensure that the transformations
    that we apply to our standard Gaussian distribution do not change the fact that
    the output is still a valid probability distribution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹çš„å…³é”®æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿‘ä¼¼æŠ€æœ¯æ¥ç¡®ä¿æˆ‘ä»¬æ°¸è¿œä¸éœ€è¦è®¡ç®—éš¾ä»¥è®¡ç®—çš„åˆ†æ¯ã€‚è¿™ä¸æ ‡å‡†åŒ–æµå½¢å½¢æˆå¯¹æ¯”ï¼Œæ ‡å‡†åŒ–æµå½¢éœ€è¦æˆ‘ä»¬ä»˜å‡ºå¾ˆå¤§çš„åŠªåŠ›ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯¹æ ‡å‡†é«˜æ–¯åˆ†å¸ƒåº”ç”¨çš„å˜æ¢ä¸ä¼šæ”¹å˜è¾“å‡ºä»ç„¶æ˜¯æœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒçš„äº‹å®ã€‚
- en: We sidestep the tricky intractable denominator problem by using a technique
    called contrastive divergence (for training) and a technique called Langevin dynamics
    (for sampling), following the ideas from Du and Mordatchâ€™s 2019 paper â€œImplicit
    Generation and Modeling with Energy-Based Models.â€^([1](ch07.xhtml#idm45387012482384))
    We shall explore these techniques in detail while building our own EBM later in
    the chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸€ç§ç§°ä¸ºå¯¹æ¯”æ•£åº¦ï¼ˆç”¨äºè®­ç»ƒï¼‰å’Œä¸€ç§ç§°ä¸ºæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼ˆç”¨äºé‡‡æ ·ï¼‰çš„æŠ€æœ¯æ¥é¿å¼€æ£˜æ‰‹çš„éš¾ä»¥è®¡ç®—çš„åˆ†æ¯é—®é¢˜ï¼Œè¿™äº›æŠ€æœ¯éµå¾ªäº†æœå’Œè«è¾¾å¥‡åœ¨2019å¹´çš„è®ºæ–‡â€œåŸºäºèƒ½é‡çš„æ¨¡å‹çš„éšå¼ç”Ÿæˆå’Œå»ºæ¨¡â€çš„æ€æƒ³ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢è¯¦ç»†æ¢è®¨è¿™äº›æŠ€æœ¯ï¼ŒåŒæ—¶æ„å»ºæˆ‘ä»¬è‡ªå·±çš„EBMã€‚
- en: First, letâ€™s get set up with a dataset and design a simple neural network that
    will represent our real-valued energy function <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    .
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªæ•°æ®é›†å¹¶è®¾è®¡ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œæ¥è¡¨ç¤ºæˆ‘ä»¬çš„å®å€¼èƒ½é‡å‡½æ•° <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>ã€‚
- en: Running the Code for This Example
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ç¤ºä¾‹çš„ä»£ç 
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/07_ebm/01_ebm/ebm.ipynb*
    in the book repository.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç¤ºä¾‹çš„ä»£ç å¯ä»¥åœ¨ä¹¦ç±å­˜å‚¨åº“ä¸­çš„*notebooks/07_ebm/01_ebm/ebm.ipynb*ä¸­æ‰¾åˆ°ã€‚
- en: The code is adapted from the excellent [tutorial on deep energy-based generative
    models](https://oreil.ly/kyO9B) by Phillip Lippe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç æ˜¯æ ¹æ®Philip Lippeçš„ä¼˜ç§€æ•™ç¨‹â€œæ·±åº¦åŸºäºèƒ½é‡çš„ç”Ÿæˆæ¨¡å‹â€è¿›è¡Œè°ƒæ•´çš„ã€‚
- en: The MNIST Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNISTæ•°æ®é›†
- en: Weâ€™ll be using the standard [MNIST dataset](https://oreil.ly/mSvhc), consisting
    of grayscale images of handwritten digits. Some example images from the dataset
    are shown in [FigureÂ 7-2](#mnist_examples).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ ‡å‡†çš„[MNISTæ•°æ®é›†](https://oreil.ly/mSvhc)ï¼Œå…¶ä¸­åŒ…å«æ‰‹å†™æ•°å­—çš„ç°åº¦å›¾åƒã€‚æ•°æ®é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹å›¾åƒæ˜¾ç¤ºåœ¨[å›¾7-2](#mnist_examples)ä¸­ã€‚
- en: '![](Images/gdl2_0702.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0702.png)'
- en: Figure 7-2\. Examples of images from the MNIST dataset
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾7-2ã€‚MNISTæ•°æ®é›†ä¸­çš„å›¾åƒç¤ºä¾‹
- en: The dataset comes prepackaged with TensorFlow, so it can be downloaded as shown
    in [ExampleÂ 7-1](#mnist-ex).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å·²ç»é¢„å…ˆæ‰“åŒ…åˆ°TensorFlowä¸­ï¼Œå› æ­¤å¯ä»¥æŒ‰ç…§[ç¤ºä¾‹7-1](#mnist-ex)ä¸­æ‰€ç¤ºä¸‹è½½ã€‚
- en: Example 7-1\. Loading the MNIST dataset
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-1ã€‚åŠ è½½MNISTæ•°æ®é›†
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, weâ€™ll scale the pixel values to the range [-1, 1] and add some padding
    to make the images 32 Ã— 32 pixels in size. We also convert it to a TensorFlow
    Dataset, as shown in [ExampleÂ 7-2](#mnist-preprocessing-ex).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å°†åƒç´ å€¼ç¼©æ”¾åˆ°èŒƒå›´[-1, 1]ï¼Œå¹¶æ·»åŠ ä¸€äº›å¡«å……ä½¿å›¾åƒå¤§å°ä¸º32Ã—32åƒç´ ã€‚æˆ‘ä»¬è¿˜å°†å…¶è½¬æ¢ä¸ºTensorFlowæ•°æ®é›†ï¼Œå¦‚[ç¤ºä¾‹7-2](#mnist-preprocessing-ex)ä¸­æ‰€ç¤ºã€‚
- en: Example 7-2\. Preprocessing the MNIST dataset
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-2ã€‚é¢„å¤„ç†MNISTæ•°æ®é›†
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our dataset, we can build the neural network that will represent
    our energy function <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> .
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä»£è¡¨æˆ‘ä»¬èƒ½é‡å‡½æ•°<math alttext="ä¸Šæ ‡Eå·¦æ‹¬å·xå³æ‹¬å·"><mrow><mi>E</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math>çš„ç¥ç»ç½‘ç»œã€‚
- en: The Energy Function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒ½é‡å‡½æ•°
- en: The energy function <math alttext="upper E Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> is a neural network with parameters
    <math alttext="theta"><mi>Î¸</mi></math> that can transform an input image <math
    alttext="x"><mi>x</mi></math> into a scalar value. Throughout this network, we
    make use of an activation function called *swish*, as described in the following
    sidebar.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½é‡å‡½æ•°<math alttext="ä¸Šæ ‡Eä¸‹æ ‡Î¸å·¦æ‹¬å·xå³æ‹¬å·"><mrow><msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>æ˜¯ä¸€ä¸ªå…·æœ‰å‚æ•°<math alttext="Î¸"><mi>Î¸</mi></math>çš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥å°†è¾“å…¥å›¾åƒ<math
    alttext="x"><mi>x</mi></math>è½¬æ¢ä¸ºæ ‡é‡å€¼ã€‚åœ¨æ•´ä¸ªç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç§°ä¸º*swish*çš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚ä¸‹é¢çš„ä¾§è¾¹æ æ‰€è¿°ã€‚
- en: The network is a set of stacked `Conv2D` layers that gradually reduce the size
    of the image while increasing the number of channels. The final layer is a single
    fully connected unit with linear activation, so the network can output values
    in the range ( <math alttext="negative normal infinity"><mrow><mo>-</mo> <mi>âˆ</mi></mrow></math>
    , <math alttext="normal infinity"><mi>âˆ</mi></math> ). The code to build it is
    given in [ExampleÂ 7-3](#ebm-model).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ˜¯ä¸€ç»„å †å çš„`Conv2D`å±‚ï¼Œé€æ¸å‡å°å›¾åƒçš„å°ºå¯¸åŒæ—¶å¢åŠ é€šé“æ•°ã€‚æœ€åä¸€å±‚æ˜¯ä¸€ä¸ªå…·æœ‰çº¿æ€§æ¿€æ´»çš„å•ä¸ªå®Œå…¨è¿æ¥å•å…ƒï¼Œå› æ­¤ç½‘ç»œå¯ä»¥è¾“å‡ºèŒƒå›´å†…çš„å€¼ï¼ˆ<math
    alttext="è´Ÿæ— ç©·"><mrow><mo>-</mo> <mi>âˆ</mi></mrow></math>ï¼Œ<math alttext="æ­£æ— ç©·"><mi>âˆ</mi></math>ï¼‰ã€‚æ„å»ºå®ƒçš„ä»£ç åœ¨[ç¤ºä¾‹7-3](#ebm-model)ä¸­ç»™å‡ºã€‚
- en: Example 7-3\. Building the energy function <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    neural network
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-3ã€‚æ„å»ºèƒ½é‡å‡½æ•°<math alttext="ä¸Šæ ‡Eå·¦æ‹¬å·xå³æ‹¬å·"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>ç¥ç»ç½‘ç»œ
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO1-1)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO1-1)'
- en: The energy function is a set of stacked `Conv2D` layers, with swish activation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½é‡å‡½æ•°æ˜¯ä¸€ç»„å †å çš„`Conv2D`å±‚ï¼Œå¸¦æœ‰swishæ¿€æ´»ã€‚
- en: '[![2](Images/2.png)](#co_energy_based_models_CO1-2)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO1-2)'
- en: The final layer is a single fully connected unit, with a linear activation function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€å±‚æ˜¯ä¸€ä¸ªå•ä¸ªå®Œå…¨è¿æ¥å•å…ƒï¼Œå…·æœ‰çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '[![3](Images/3.png)](#co_energy_based_models_CO1-3)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO1-3)'
- en: A Keras `Model` that converts the input image into a scalar energy value.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºæ ‡é‡èƒ½é‡å€¼çš„Keras `Model`ã€‚
- en: Sampling Using Langevin Dynamics
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LangevinåŠ¨åŠ›å­¦è¿›è¡Œé‡‡æ ·
- en: The energy function only outputs a score for a given inputâ€”how can we use this
    function to generate new samples that have a low energy score?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½é‡å‡½æ•°åªä¸ºç»™å®šè¾“å…¥è¾“å‡ºä¸€ä¸ªåˆ†æ•°â€”â€”æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨è¿™ä¸ªå‡½æ•°ç”Ÿæˆèƒ½é‡åˆ†æ•°ä½çš„æ–°æ ·æœ¬ï¼Ÿ
- en: We will use a technique called *Langevin dynamics*, which makes use of the fact
    that we can compute the gradient of the energy function with respect to its input.
    If we start from a random point in the sample space and take small steps in the
    opposite direction of the calculated gradient, we will gradually reduce the energy
    function. If our neural network is trained correctly, then the random noise should
    transform into an image that resembles an observation from the training set before
    our eyes!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç§°ä¸º*LangevinåŠ¨åŠ›å­¦*çš„æŠ€æœ¯ï¼Œåˆ©ç”¨äº†æˆ‘ä»¬å¯ä»¥è®¡ç®—èƒ½é‡å‡½æ•°ç›¸å¯¹äºå…¶è¾“å…¥çš„æ¢¯åº¦çš„äº‹å®ã€‚å¦‚æœæˆ‘ä»¬ä»æ ·æœ¬ç©ºé—´ä¸­çš„ä¸€ä¸ªéšæœºç‚¹å¼€å§‹ï¼Œå¹¶æœç€è®¡ç®—å‡ºçš„æ¢¯åº¦çš„ç›¸åæ–¹å‘è¿ˆå‡ºå°æ­¥ï¼Œæˆ‘ä»¬å°†é€æ¸å‡å°èƒ½é‡å‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œè®­ç»ƒæ­£ç¡®ï¼Œé‚£ä¹ˆéšæœºå™ªå£°åº”è¯¥åœ¨æˆ‘ä»¬çœ¼å‰è½¬å˜æˆç±»ä¼¼äºè®­ç»ƒé›†ä¸­çš„è§‚å¯Ÿç»“æœçš„å›¾åƒï¼
- en: Stochastic Gradient Langevin Dynamics
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éšæœºæ¢¯åº¦LangevinåŠ¨åŠ›å­¦
- en: Importantly, we must also add a small amount of random noise to the input as
    we travel across the sample space; otherwise, there is a risk of falling into
    local minima. The technique is therefore known as stochastic gradient Langevin
    dynamics.^([3](ch07.xhtml#idm45387012049200))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯ï¼Œå½“æˆ‘ä»¬ç©¿è¶Šæ ·æœ¬ç©ºé—´æ—¶ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»å‘è¾“å…¥æ·»åŠ å°‘é‡éšæœºå™ªå£°ï¼›å¦åˆ™ï¼Œæœ‰å¯èƒ½é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚å› æ­¤ï¼Œè¯¥æŠ€æœ¯è¢«ç§°ä¸ºéšæœºæ¢¯åº¦LangevinåŠ¨åŠ›å­¦ã€‚^([3](ch07.xhtml#idm45387012049200))
- en: We can visualize this gradient descent as shown in [FigureÂ 7-4](#langevin_diagram),
    for a two-dimensional space with the energy function value on the third dimension.
    The path is a noisy descent downhill, following the negative gradient of the energy
    function <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> with respect to the input <math
    alttext="x"><mi>x</mi></math> . In the MNIST image dataset, we have 1,024 pixels
    so are navigating a 1,024-dimensional space, but the same principles apply!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ¢¯åº¦ä¸‹é™å¯è§†åŒ–ä¸º [å›¾7-4](#langevin_diagram) ä¸­æ‰€ç¤ºï¼Œå¯¹äºä¸€ä¸ªå…·æœ‰èƒ½é‡å‡½æ•°å€¼çš„ä¸‰ç»´ç©ºé—´ã€‚è·¯å¾„æ˜¯ä¸€ä¸ªå˜ˆæ‚çš„ä¸‹é™ï¼Œæ²¿ç€èƒ½é‡å‡½æ•°
    <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> çš„è´Ÿæ¢¯åº¦ç›¸å¯¹äºè¾“å…¥ <math alttext="x"><mi>x</mi></math>
    ä¸‹é™ã€‚åœ¨MNISTå›¾åƒæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æœ‰1,024ä¸ªåƒç´ ï¼Œå› æ­¤åœ¨ä¸€ä¸ª1,024ç»´ç©ºé—´ä¸­å¯¼èˆªï¼Œä½†æ˜¯ç›¸åŒçš„åŸåˆ™é€‚ç”¨ï¼
- en: '![https://www.math3d.org/C1sQPhizZ](Images/gdl2_0704.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![https://www.math3d.org/C1sQPhizZ](Images/gdl2_0704.png)'
- en: Figure 7-4\. Gradient descent using Langevin dynamics
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾7-4\. ä½¿ç”¨æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦çš„æ¢¯åº¦ä¸‹é™
- en: It is worth noting the difference between this kind of gradient descent and
    the kind of gradient descent we normally use to train a neural network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ¢¯åº¦ä¸‹é™ä¸æˆ‘ä»¬é€šå¸¸ç”¨æ¥è®­ç»ƒç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¸‹é™ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: When training a neural network, we calculate the gradient of the *loss function*
    with respect to the *parameters* of the network (i.e., the weights) using backpropagation.
    Then we update the parameters a small amount in the direction of the negative
    gradient, so that over many iterations, we gradually minimize the loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—*æŸå¤±å‡½æ•°*ç›¸å¯¹äºç½‘ç»œçš„*å‚æ•°*ï¼ˆå³æƒé‡ï¼‰çš„æ¢¯åº¦ã€‚ç„¶åæˆ‘ä»¬å°†å‚æ•°åœ¨è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šå¾®è°ƒï¼Œè¿™æ ·ç»è¿‡å¤šæ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬é€æ¸æœ€å°åŒ–æŸå¤±ã€‚
- en: With Langevin dynamics, we keep the neural network weights *fixed* and calculate
    the gradient of the *output* with respect to the *input*. Then we update the input
    a small amount in the direction of the negative gradient, so that over many iterations,
    we gradually minimize the output (the energy score).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼Œæˆ‘ä»¬ä¿æŒç¥ç»ç½‘ç»œæƒé‡*å›ºå®š*ï¼Œè®¡ç®—*è¾“å‡º*ç›¸å¯¹äº*è¾“å…¥*çš„æ¢¯åº¦ã€‚ç„¶åæˆ‘ä»¬å°†è¾“å…¥åœ¨è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šå¾®è°ƒï¼Œè¿™æ ·ç»è¿‡å¤šæ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬é€æ¸æœ€å°åŒ–è¾“å‡ºï¼ˆèƒ½é‡åˆ†æ•°ï¼‰ã€‚
- en: Both processes utilize the same idea (gradient descent), but are applied to
    different functions and with respect to different entities.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªè¿‡ç¨‹éƒ½åˆ©ç”¨äº†ç›¸åŒçš„æ€æƒ³ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ï¼Œä½†æ˜¯åº”ç”¨äºä¸åŒçš„å‡½æ•°ï¼Œå¹¶ä¸”é’ˆå¯¹ä¸åŒçš„å®ä½“ã€‚
- en: 'Formally, Langevin dynamics can be described by the following equation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å½¢å¼ä¸Šï¼Œæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦å¯ä»¥ç”¨ä»¥ä¸‹æ–¹ç¨‹æè¿°ï¼š
- en: <math alttext="x Superscript k Baseline equals x Superscript k minus 1 Baseline
    minus eta normal nabla Subscript x Baseline upper E Subscript theta Baseline left-parenthesis
    x Superscript k minus 1 Baseline right-parenthesis plus omega" display="block"><mrow><msup><mi>x</mi>
    <mi>k</mi></msup> <mo>=</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>-</mo> <mi>Î·</mi> <msub><mi>âˆ‡</mi> <mi>x</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mi>Ï‰</mi></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x Superscript k Baseline equals x Superscript k minus 1 Baseline
    minus eta normal nabla Subscript x Baseline upper E Subscript theta Baseline left-parenthesis
    x Superscript k minus 1 Baseline right-parenthesis plus omega" display="block"><mrow><msup><mi>x</mi>
    <mi>k</mi></msup> <mo>=</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>-</mo> <mi>Î·</mi> <msub><mi>âˆ‡</mi> <mi>x</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mi>Ï‰</mi></mrow></math>
- en: where <math alttext="omega tilde script upper N left-parenthesis 0 comma sigma
    right-parenthesis"><mrow><mi>Ï‰</mi> <mo>âˆ¼</mo> <mi>ğ’©</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mi>Ïƒ</mi> <mo>)</mo></mrow></math> and <math alttext="x Superscript
    0 Baseline tilde script upper U"><mrow><msup><mi>x</mi> <mn>0</mn></msup> <mo>âˆ¼</mo>
    <mi>ğ’°</mi></mrow></math> (â€“1,1). <math alttext="eta"><mi>Î·</mi></math> is the
    step size hyperparameter that must be tunedâ€”too large and the steps jump over
    minima, too small and the algorithm will be too slow to converge.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <math alttext="omega tilde script upper N left-parenthesis 0 comma sigma
    right-parenthesis"><mrow><mi>Ï‰</mi> <mo>âˆ¼</mo> <mi>ğ’©</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mi>Ïƒ</mi> <mo>)</mo></mrow></math> å’Œ <math alttext="x Superscript
    0 Baseline tilde script upper U"><mrow><msup><mi>x</mi> <mn>0</mn></msup> <mo>âˆ¼</mo>
    <mi>ğ’°</mi></mrow></math>ï¼ˆ-1,1ï¼‰ã€‚<math alttext="eta"><mi>Î·</mi></math> æ˜¯å¿…é¡»è°ƒæ•´çš„æ­¥é•¿è¶…å‚æ•°â€”â€”å¤ªå¤§ä¼šå¯¼è‡´æ­¥éª¤è·³è¿‡æœ€å°å€¼ï¼Œå¤ªå°åˆ™ç®—æ³•æ”¶æ•›é€Ÿåº¦å¤ªæ…¢ã€‚
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: <math alttext="x Superscript 0 Baseline tilde script upper U"><mrow><msup><mi>x</mi>
    <mn>0</mn></msup> <mo>âˆ¼</mo> <mi>ğ’°</mi></mrow></math> (â€“1,1) is the uniform distribution
    on the range [â€“1, 1].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x Superscript 0 Baseline tilde script upper U"><mrow><msup><mi>x</mi>
    <mn>0</mn></msup> <mo>âˆ¼</mo> <mi>ğ’°</mi></mrow></math>ï¼ˆ-1,1ï¼‰æ˜¯èŒƒå›´[-1,1]ä¸Šçš„å‡åŒ€åˆ†å¸ƒã€‚
- en: We can code up our Langevin sampling function as illustrated in [ExampleÂ 7-4](#langevin-sampler).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç¼–å†™æˆ‘ä»¬çš„æœ—ä¹‹ä¸‡é‡‡æ ·å‡½æ•°ï¼Œå¦‚ [ç¤ºä¾‹7-4](#langevin-sampler) æ‰€ç¤ºã€‚
- en: Example 7-4\. The Langevin sampling function
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-4\. æœ—ä¹‹ä¸‡é‡‡æ ·å‡½æ•°
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO2-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO2-1)'
- en: Loop over given number of steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯æ‰§è¡Œç»™å®šæ•°é‡çš„æ­¥éª¤ã€‚
- en: '[![2](Images/2.png)](#co_energy_based_models_CO2-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO2-2)'
- en: Add a small amount of noise to the image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å‘å›¾åƒä¸­æ·»åŠ å°‘é‡å™ªéŸ³ã€‚
- en: '[![3](Images/3.png)](#co_energy_based_models_CO2-3)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO2-3)'
- en: Pass the image through the model to obtain the energy score.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¨¡å‹ä¼ é€’å›¾åƒä»¥è·å¾—èƒ½é‡åˆ†æ•°ã€‚
- en: '[![4](Images/4.png)](#co_energy_based_models_CO2-4)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO2-4)'
- en: Calculate the gradient of the output with respect to the input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¾“å‡ºç›¸å¯¹äºè¾“å…¥çš„æ¢¯åº¦ã€‚
- en: '[![5](Images/5.png)](#co_energy_based_models_CO2-5)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO2-5)'
- en: Add a small amount of the gradient to the input image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å‘è¾“å…¥å›¾åƒä¸­æ·»åŠ å°‘é‡æ¢¯åº¦ã€‚
- en: Training with Contrastive Divergence
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¯¹æ¯”æ•£åº¦è¿›è¡Œè®­ç»ƒ
- en: Now that we know how to sample a novel low-energy point from the sample space,
    letâ€™s turn our attention to training the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“å¦‚ä½•ä»æ ·æœ¬ç©ºé—´ä¸­é‡‡æ ·ä¸€ä¸ªæ–°çš„ä½èƒ½é‡ç‚¹ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘è®­ç»ƒæ¨¡å‹ã€‚
- en: We cannot apply maximum likelihood estimation, because the energy function does
    not output a probability; it outputs a score that does not integrate to 1 across
    the sample space. Instead, we will apply a technique first proposed in 2002 by
    Geoffrey Hinton, called *contrastive divergence*, for training unnormalized scoring
    models.^([4](ch07.xhtml#idm45387011649104))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•åº”ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå› ä¸ºèƒ½é‡å‡½æ•°ä¸è¾“å‡ºæ¦‚ç‡ï¼›å®ƒè¾“å‡ºçš„æ˜¯ä¸€ä¸ªåœ¨æ ·æœ¬ç©ºé—´ä¸­ä¸ç§¯åˆ†ä¸º1çš„åˆ†æ•°ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†åº”ç”¨ Geoffrey Hinton åœ¨2002å¹´é¦–æ¬¡æå‡ºçš„ä¸€ç§æŠ€æœ¯ï¼Œç§°ä¸º*å¯¹æ¯”æ•£åº¦*ï¼Œç”¨äºè®­ç»ƒéå½’ä¸€åŒ–è¯„åˆ†æ¨¡å‹ã€‚^([4](ch07.xhtml#idm45387011649104))
- en: 'The value that we want to minimize (as always) is the negative log-likelihood
    of the data:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–çš„å€¼ï¼ˆä¸€å¦‚æ—¢å¾€ï¼‰æ˜¯æ•°æ®çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š
- en: <math alttext="script upper L equals minus double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis bold x right-parenthesis right-bracket" display="block"><mrow><mi>â„’</mi>
    <mo>=</mo> <mo>-</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo> <mi>data</mi></mrow></msub>
    <mfenced separators="" open="[" close="]"><mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L equals minus double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis bold x right-parenthesis right-bracket" display="block"><mrow><mi>â„’</mi>
    <mo>=</mo> <mo>-</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo> <mi>data</mi></mrow></msub>
    <mfenced separators="" open="[" close="]"><mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mfenced></mrow></math>
- en: When <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
    has the form of a Boltzmann distribution, with energy function <math alttext="upper
    E Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>E</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
    , it can be shown that the gradient of this value can be written as follows (Oliver
    Woodfordâ€™s â€œNotes on Contrastive Divergenceâ€ for the full derivation):^([5](ch07.xhtml#idm45387011627440))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
    å…·æœ‰ç»å°”å…¹æ›¼åˆ†å¸ƒçš„å½¢å¼ï¼Œèƒ½é‡å‡½æ•°ä¸º <math alttext="upper E Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> ï¼Œå¯ä»¥è¯æ˜è¯¥å€¼çš„æ¢¯åº¦å¯ä»¥å†™æˆä»¥ä¸‹å½¢å¼ï¼ˆOliver Woodfordçš„â€œå¯¹æ¯”æ•£åº¦ç¬”è®°â€è¿›è¡Œå®Œæ•´æ¨å¯¼ï¼‰ï¼š^([5](ch07.xhtml#idm45387011627440))
- en: <math alttext="StartLayout 1st Row 1st Column normal nabla Subscript theta Baseline
    script upper L 2nd Column equals 3rd Column double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket normal nabla Subscript
    theta Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket minus double-struck upper E Subscript x tilde normal m normal o
    normal d normal e normal l Baseline left-bracket normal nabla Subscript theta
    Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>âˆ‡</mi> <mi>Î¸</mi></msub> <mi>â„’</mi></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo>
    <mi>data</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>âˆ‡</mi>
    <mi>Î¸</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></mfenced> <mo>-</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo>
    <mi>model</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>âˆ‡</mi>
    <mi>Î¸</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column normal nabla Subscript theta Baseline
    script upper L 2nd Column equals 3rd Column double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket normal nabla Subscript
    theta Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket minus double-struck upper E Subscript x tilde normal m normal o
    normal d normal e normal l Baseline left-bracket normal nabla Subscript theta
    Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>âˆ‡</mi> <mi>Î¸</mi></msub> <mi>â„’</mi></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo>
    <mi>data</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>âˆ‡</mi>
    <mi>Î¸</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></mfenced> <mo>-</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo>
    <mi>model</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>âˆ‡</mi>
    <mi>Î¸</mi></msub> <msub><mi>E</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr></mtable></math>
- en: This intuitively makes a lot of senseâ€”we want to train the model to output large
    negative energy scores for real observations and large positive energy scores
    for generated fake observations so that the contrast between these two extremes
    is as large as possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ç›´è§‰ä¸Šæ˜¯æœ‰å¾ˆå¤šæ„ä¹‰çš„-æˆ‘ä»¬å¸Œæœ›è®­ç»ƒæ¨¡å‹è¾“å‡ºçœŸå®è§‚å¯Ÿçš„å¤§è´Ÿèƒ½é‡åˆ†æ•°ï¼Œå¹¶ä¸ºç”Ÿæˆçš„å‡è§‚å¯Ÿè¾“å‡ºå¤§æ­£èƒ½é‡åˆ†æ•°ï¼Œä»¥ä¾¿è¿™ä¸¤ä¸ªæç«¯ä¹‹é—´çš„å¯¹æ¯”å°½å¯èƒ½å¤§ã€‚
- en: In other words, we can calculate the difference between the energy scores of
    real and fake samples and use this as our loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—çœŸå®å’Œå‡æ ·æœ¬çš„èƒ½é‡åˆ†æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶ç”¨ä½œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ã€‚
- en: To calculate the energy scores of fake samples, we would need to be able to
    sample exactly from the distribution <math alttext="p Subscript theta Baseline
    left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> , which isnâ€™t possible
    due to the intractable denominator. Instead, we can use our Langevin sampling
    procedure to generate a set of observations with low energy scores. The process
    would need to run for infinitely many steps to produce a perfect sample (which
    is obviously impractical), so instead we run for some small number of steps, on
    the assumption that this is good enough to produce a meaningful loss function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¡ç®—å‡æ ·æœ¬çš„èƒ½é‡åˆ†æ•°ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿä»åˆ†å¸ƒ <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> ä¸­ç²¾ç¡®æŠ½æ ·ï¼Œä½†ç”±äºä¸å¯è§£çš„åˆ†æ¯ï¼Œè¿™æ˜¯ä¸å¯èƒ½çš„ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Langeviné‡‡æ ·è¿‡ç¨‹ç”Ÿæˆä¸€ç»„èƒ½é‡åˆ†æ•°è¾ƒä½çš„è§‚å¯Ÿã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦è¿è¡Œæ— é™å¤šæ­¥æ‰èƒ½äº§ç”Ÿå®Œç¾æ ·æœ¬ï¼ˆæ˜¾ç„¶æ˜¯ä¸åˆ‡å®é™…çš„ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬è¿è¡Œä¸€äº›å°æ­¥æ•°ï¼Œå‡è®¾è¿™è¶³ä»¥äº§ç”Ÿæœ‰æ„ä¹‰çš„æŸå¤±å‡½æ•°ã€‚
- en: We also maintain a buffer of samples from previous iterations, so that we can
    use this as the starting point for the next batch, rather than pure random noise.
    The code to produce the sampling buffer is shown in [ExampleÂ 7-5](#sampling_buffer).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ç»´æŠ¤ä¸€ä¸ªæ¥è‡ªå…ˆå‰è¿­ä»£çš„æ ·æœ¬ç¼“å†²åŒºï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨ä½œä¸‹ä¸€æ‰¹çš„èµ·ç‚¹ï¼Œè€Œä¸æ˜¯çº¯éšæœºå™ªå£°ã€‚ç”Ÿæˆé‡‡æ ·ç¼“å†²åŒºçš„ä»£ç å¦‚[ç¤ºä¾‹7-5](#sampling_buffer)æ‰€ç¤ºã€‚
- en: Example 7-5\. The `Buffer`
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-5ã€‚`ç¼“å†²åŒº`
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO3-1)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO3-1)'
- en: The sampling buffer is initialized with a batch of random noise.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡æ ·ç¼“å†²åŒºç”¨ä¸€æ‰¹éšæœºå™ªå£°åˆå§‹åŒ–ã€‚
- en: '[![2](Images/2.png)](#co_energy_based_models_CO3-2)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO3-2)'
- en: On average, 5% of observations are generated from scratch (i.e., random noise)
    each time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³å‡è€Œè¨€ï¼Œæ¯æ¬¡æœ‰5%çš„è§‚å¯Ÿæ˜¯ä»å¤´å¼€å§‹ç”Ÿæˆçš„ï¼ˆå³ï¼Œéšæœºå™ªå£°ï¼‰ã€‚
- en: '[![3](Images/3.png)](#co_energy_based_models_CO3-3)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO3-3)'
- en: The rest are pulled at random from the existing buffer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä½™çš„éšæœºä»ç°æœ‰ç¼“å†²åŒºä¸­æå–ã€‚
- en: '[![4](Images/4.png)](#co_energy_based_models_CO3-4)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO3-4)'
- en: The observations are concatenated and run through the Langevin sampler.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è§‚å¯Ÿè¢«è¿æ¥å¹¶é€šè¿‡Langeviné‡‡æ ·å™¨è¿è¡Œã€‚
- en: '[![5](Images/5.png)](#co_energy_based_models_CO3-5)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO3-5)'
- en: The resulting sample is added to the buffer, which is trimmed to a max length
    of 8,192 observations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„æ ·æœ¬è¢«æ·»åŠ åˆ°ç¼“å†²åŒºä¸­ï¼Œç¼“å†²åŒºè¢«ä¿®å‰ªä¸ºæœ€å¤š8,192ä¸ªè§‚å¯Ÿã€‚
- en: '[FigureÂ 7-5](#contrastive_divergence) shows one training step of contrastive
    divergence. The scores of real observations are pushed down by the algorithm and
    the scores of fake observations are pulled up, without caring about normalizing
    these scores after each step.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾7-5](#contrastive_divergence)æ˜¾ç¤ºäº†å¯¹æ¯”æ•£åº¦çš„ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ã€‚çœŸå®è§‚å¯Ÿçš„åˆ†æ•°è¢«ç®—æ³•æ¨ä½ï¼Œè€Œå‡è§‚å¯Ÿçš„åˆ†æ•°è¢«æ‹‰é«˜ï¼Œæ¯ä¸€æ­¥ä¹‹åéƒ½ä¸è€ƒè™‘å¯¹è¿™äº›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '![](Images/gdl2_0705.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0705.png)'
- en: Figure 7-5\. One step of contrastive divergence
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾7-5ã€‚å¯¹æ¯”æ•£åº¦çš„ä¸€æ­¥
- en: We can code up the training step of the contrastive divergence algorithm within
    a custom Keras model as shown in [ExampleÂ 7-6](#contrastive_divergence_python).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å¯ä»¥ç¼–å†™å¯¹æ¯”æ•£åº¦ç®—æ³•çš„è®­ç»ƒæ­¥éª¤ï¼Œå¦‚[ç¤ºä¾‹7-6](#contrastive_divergence_python)æ‰€ç¤ºï¼Œåœ¨è‡ªå®šä¹‰Kerasæ¨¡å‹ä¸­ã€‚ '
- en: Example 7-6\. EBM trained using contrastive divergence
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹7-6ã€‚ä½¿ç”¨å¯¹æ¯”æ•£åº¦è®­ç»ƒçš„EBM
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO4-1)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO4-1)'
- en: A small amount of random noise is added to the real images, to avoid the model
    overfitting to the training set.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºçœŸå®å›¾åƒæ·»åŠ å°‘é‡éšæœºå™ªå£°ï¼Œä»¥é¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†ã€‚
- en: '[![2](Images/2.png)](#co_energy_based_models_CO4-2)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO4-2)'
- en: A set of fake images are sampled from the buffer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»„å‡å›¾åƒä»ç¼“å†²åŒºä¸­æŠ½æ ·ã€‚
- en: '[![3](Images/3.png)](#co_energy_based_models_CO4-3)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO4-3)'
- en: The real and fake images are run through the model to produce real and fake
    scores.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸå®å’Œå‡å›¾åƒé€šè¿‡æ¨¡å‹è¿è¡Œä»¥äº§ç”ŸçœŸå®å’Œå‡åˆ†æ•°ã€‚
- en: '[![4](Images/4.png)](#co_energy_based_models_CO4-4)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO4-4)'
- en: The contrastive divergence loss is simply the difference between the scores
    of real and fake observations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯”æ•£åº¦æŸå¤±ç®€å•åœ°æ˜¯çœŸå®å’Œå‡è§‚å¯Ÿçš„åˆ†æ•°ä¹‹é—´çš„å·®å¼‚ã€‚
- en: '[![5](Images/5.png)](#co_energy_based_models_CO4-5)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO4-5)'
- en: A regularization loss is added to avoid the scores becoming too large.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æ­£åˆ™åŒ–æŸå¤±ä»¥é¿å…åˆ†æ•°å˜å¾—è¿‡å¤§ã€‚
- en: '[![6](Images/6.png)](#co_energy_based_models_CO4-6)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_energy_based_models_CO4-6)'
- en: Gradients of the loss function with respect to the weights of the network are
    calculated for backpropagation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åå‘ä¼ æ’­è®¡ç®—ç½‘ç»œæƒé‡ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ã€‚
- en: '[![7](Images/7.png)](#co_energy_based_models_CO4-7)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_energy_based_models_CO4-7)'
- en: The `test_step` is used during validation and calculates the contrastive divergence
    between the scores of a set of random noise and data from the training set. It
    can be used as a measure for how well the model is training (see the following
    section).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_step` ç”¨äºåœ¨éªŒè¯è¿‡ç¨‹ä¸­è®¡ç®—ä¸€ç»„éšæœºå™ªå£°å’Œè®­ç»ƒé›†ä¸­çš„æ•°æ®ä¹‹é—´çš„å¯¹æ¯”æ•£åº¦ã€‚å®ƒå¯ä»¥ä½œä¸ºè¡¡é‡æ¨¡å‹è®­ç»ƒæ•ˆæœçš„æŒ‡æ ‡ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚'
- en: Analysis of the Energy-Based Model
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒ½é‡åŸºæ¨¡å‹çš„åˆ†æ
- en: The loss curves and supporting metrics from the training process are shown in
    [FigureÂ 7-6](#ebm_loss_curves).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ›²çº¿å’Œæ”¯æŒæŒ‡æ ‡æ˜¾ç¤ºåœ¨ [Figure 7-6](#ebm_loss_curves) ä¸­ã€‚
- en: '![](Images/gdl2_0706.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0706.png)'
- en: Figure 7-6\. Loss curves and metrics for the training process of the EBM
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 7-6\. EBM è®­ç»ƒè¿‡ç¨‹çš„æŸå¤±æ›²çº¿å’ŒæŒ‡æ ‡
- en: Firstly, notice that the loss calculated during the training step is approximately
    constant and small across epochs. While the model is constantly improving, so
    is the quality of generated images in the buffer that it is required to compare
    against real images from the training set, so we shouldnâ€™t expect the training
    loss to fall significantly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ³¨æ„åˆ°åœ¨è®­ç»ƒæ­¥éª¤ä¸­è®¡ç®—çš„æŸå¤±åœ¨å„ä¸ªå‘¨æœŸä¸­å¤§è‡´ä¿æŒä¸å˜ä¸”è¾ƒå°ã€‚è™½ç„¶æ¨¡å‹ä¸æ–­æ”¹è¿›ï¼Œä½†ä¸è®­ç»ƒé›†ä¸­çš„çœŸå®å›¾åƒè¿›è¡Œæ¯”è¾ƒçš„ç¼“å†²åŒºä¸­ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¹Ÿåœ¨æé«˜ï¼Œå› æ­¤æˆ‘ä»¬ä¸åº”è¯¥æœŸæœ›è®­ç»ƒæŸå¤±æ˜¾è‘—ä¸‹é™ã€‚
- en: Therefore, to judge model performance, we also set up a validation process that
    doesnâ€™t sample from the buffer, but instead scores a sample of random noise and
    compares this against the scores of examples from the training set. If the model
    is improving, we should see that the contrastive divergence falls over the epochs
    (i.e., it is getting better at distinguishing random noise from real images),
    as can be seen in [FigureÂ 7-6](#ebm_loss_curves).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ºäº†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªéªŒè¯è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹ä¸ä»ç¼“å†²åŒºä¸­é‡‡æ ·ï¼Œè€Œæ˜¯å¯¹ä¸€ç»„éšæœºå™ªå£°è¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†å…¶ä¸è®­ç»ƒé›†ä¸­çš„ç¤ºä¾‹è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœæ¨¡å‹æ­£åœ¨æ”¹è¿›ï¼Œæˆ‘ä»¬åº”è¯¥çœ‹åˆ°å¯¹æ¯”æ•£åº¦éšç€å‘¨æœŸçš„å¢åŠ è€Œä¸‹é™ï¼ˆå³ï¼Œå®ƒåœ¨åŒºåˆ†éšæœºå™ªå£°å’ŒçœŸå®å›¾åƒæ–¹é¢å˜å¾—æ›´å¥½ï¼‰ï¼Œå¦‚
    [Figure 7-6](#ebm_loss_curves) ä¸­æ‰€ç¤ºã€‚
- en: Generating new samples from the EBM is simply a case of running the Langevin
    sampler for a large number of steps, from a standing start (random noise), as
    shown in [ExampleÂ 7-7](#generating_ebm). The observation is forced *downhill*,
    following the gradients of the scoring function with respect to the input, so
    that out of the noise, a plausible observation appears.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä» EBM ç”Ÿæˆæ–°æ ·æœ¬åªéœ€è¿è¡Œ Langevin é‡‡æ ·å™¨è¿›è¡Œå¤§é‡æ­¥éª¤ï¼Œä»ä¸€ä¸ªé™æ­¢çŠ¶æ€ï¼ˆéšæœºå™ªå£°ï¼‰å¼€å§‹ï¼Œå¦‚ [Example 7-7](#generating_ebm)
    ä¸­æ‰€ç¤ºã€‚è§‚æµ‹è¢«è¿« *ä¸‹å¡*ï¼Œæ²¿ç€ç›¸å¯¹äºè¾“å…¥çš„è¯„åˆ†å‡½æ•°çš„æ¢¯åº¦ï¼Œä»¥ä¾¿åœ¨å™ªå£°ä¸­å‡ºç°ä¸€ä¸ªåˆç†çš„è§‚æµ‹ã€‚
- en: Example 7-7\. Generating new observations using the EBM
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 7-7\. ä½¿ç”¨ EBM ç”Ÿæˆæ–°è§‚æµ‹
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Some examples of observations produced by the sampler after 50 epochs of training
    are shown in [FigureÂ 7-7](#ebm_examples).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»è¿‡ 50 ä¸ªå‘¨æœŸçš„è®­ç»ƒåï¼Œé‡‡æ ·å™¨ç”Ÿæˆçš„ä¸€äº›è§‚æµ‹ç¤ºä¾‹æ˜¾ç¤ºåœ¨ [Figure 7-7](#ebm_examples) ä¸­ã€‚
- en: '![](Images/gdl2_0707.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0707.png)'
- en: Figure 7-7\. Examples produced by the Langevin sampler using the EBM model to
    direct the gradient descent
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 7-7\. ä½¿ç”¨ EBM æ¨¡å‹çš„ Langevin é‡‡æ ·å™¨ç”Ÿæˆçš„ç¤ºä¾‹ä»¥æŒ‡å¯¼æ¢¯åº¦ä¸‹é™
- en: We can even show a replay of how a single observation is generated by taking
    snapshots of the current observations during the Langevin sampling processâ€”this
    is shown in [FigureÂ 7-8](#langevin_examples).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”šè‡³å¯ä»¥å±•ç¤ºå•ä¸ªè§‚æµ‹æ˜¯å¦‚ä½•é€šè¿‡åœ¨ Langevin é‡‡æ ·è¿‡ç¨‹ä¸­æ‹æ‘„å½“å‰è§‚æµ‹çš„å¿«ç…§ç”Ÿæˆçš„â€”â€”è¿™åœ¨ [Figure 7-8](#langevin_examples)
    ä¸­å±•ç¤ºã€‚
- en: '![](Images/gdl2_0708.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0708.png)'
- en: Figure 7-8\. Snapshots of an observation at different steps of the Langevin
    sampling process
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 7-8\. Langevin é‡‡æ ·è¿‡ç¨‹ä¸­ä¸åŒæ­¥éª¤çš„è§‚æµ‹å¿«ç…§
- en: Other Energy-Based Models
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–èƒ½é‡åŸºæ¨¡å‹
- en: In the previous example we made use of a deep EBM trained using contrastive
    divergence with a Langevin dynamics sampler. However, early EBM models did not
    make use of Langevin sampling, but instead relied on other techniques and architectures.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä½¿ç”¨å¯¹æ¯”æ•£åº¦å’Œ Langevin åŠ¨åŠ›å­¦é‡‡æ ·å™¨è®­ç»ƒçš„æ·±åº¦ EBMã€‚ç„¶è€Œï¼Œæ—©æœŸçš„ EBM æ¨¡å‹å¹¶æ²¡æœ‰ä½¿ç”¨ Langevin é‡‡æ ·ï¼Œè€Œæ˜¯ä¾èµ–äºå…¶ä»–æŠ€æœ¯å’Œæ¶æ„ã€‚
- en: 'One of the earliest examples of an EBM was the *Boltzmann machine*.^([6](ch07.xhtml#idm45387010130976))
    This is a fully connected, undirected neural network, where binary units are either
    *visible* (*v*) or *hidden* (*h*). The energy of a given configuration of the
    network is defined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æ—©çš„èƒ½é‡åŸºæ¨¡å‹ä¹‹ä¸€æ˜¯ *Boltzmann æœº*ã€‚^([6](ch07.xhtml#idm45387010130976)) è¿™æ˜¯ä¸€ä¸ªå…¨è¿æ¥çš„æ— å‘ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­äºŒè¿›åˆ¶å•å…ƒè¦ä¹ˆæ˜¯
    *å¯è§*ï¼ˆ*v*ï¼‰ï¼Œè¦ä¹ˆæ˜¯ *éšè—*ï¼ˆ*h*ï¼‰ã€‚ç½‘ç»œçš„ç»™å®šé…ç½®çš„èƒ½é‡å®šä¹‰å¦‚ä¸‹ï¼š
- en: <math alttext="upper E Subscript theta Baseline left-parenthesis v comma h right-parenthesis
    equals minus one-half left-parenthesis v Superscript upper T Baseline upper L
    v plus h Superscript upper T Baseline upper J h plus v Superscript upper T Baseline
    upper W h right-parenthesis" display="block"><mrow><msub><mi>E</mi> <mi>Î¸</mi></msub>
    <mrow><mo>(</mo> <mi>v</mi> <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mfenced separators="" open="("
    close=")"><msup><mi>v</mi> <mi>T</mi></msup> <mi>L</mi> <mi>v</mi> <mo>+</mo>
    <msup><mi>h</mi> <mi>T</mi></msup> <mi>J</mi> <mi>h</mi> <mo>+</mo> <msup><mi>v</mi>
    <mi>T</mi></msup> <mi>W</mi> <mi>h</mi></mfenced></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript theta Baseline left-parenthesis v comma h right-parenthesis
    equals minus one-half left-parenthesis v Superscript upper T Baseline upper L
    v plus h Superscript upper T Baseline upper J h plus v Superscript upper T Baseline
    upper W h right-parenthesis" display="block"><mrow><msub><mi>E</mi> <mi>Î¸</mi></msub>
    <mrow><mo>(</mo> <mi>v</mi> <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mfenced separators="" open="("
    close=")"><msup><mi>v</mi> <mi>T</mi></msup> <mi>L</mi> <mi>v</mi> <mo>+</mo>
    <msup><mi>h</mi> <mi>T</mi></msup> <mi>J</mi> <mi>h</mi> <mo>+</mo> <msup><mi>v</mi>
    <mi>T</mi></msup> <mi>W</mi> <mi>h</mi></mfenced></mrow></math>
- en: where <math alttext="upper W comma upper L comma upper J"><mrow><mi>W</mi> <mo>,</mo>
    <mi>L</mi> <mo>,</mo> <mi>J</mi></mrow></math> are the weights matrices that are
    learned by the model. Training is achieved by contrastive divergence, but using
    Gibbs sampling to alternate between the visible and hidden layers until an equilibrium
    is found. In practice this is very slow and not scalable to large numbers of hidden
    units.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <math alttext="upper W comma upper L comma upper J"><mrow><mi>W</mi> <mo>,</mo>
    <mi>L</mi> <mo>,</mo> <mi>J</mi></mrow></math> æ˜¯æ¨¡å‹å­¦ä¹ çš„æƒé‡çŸ©é˜µã€‚è®­ç»ƒé€šè¿‡å¯¹æ¯”æ•£åº¦å®ç°ï¼Œä½†ä½¿ç”¨ Gibbs
    é‡‡æ ·åœ¨å¯è§å±‚å’Œéšè—å±‚ä¹‹é—´äº¤æ›¿ï¼Œç›´åˆ°æ‰¾åˆ°å¹³è¡¡ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯éå¸¸ç¼“æ…¢çš„ï¼Œä¸é€‚ç”¨äºå¤§é‡éšè—å•å…ƒã€‚
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: See Jessica Stringhamâ€™s blog post [â€œGibbs Sampling in Pythonâ€](https://oreil.ly/tXmOq)
    for an excellent simple example of Gibbs sampling.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ Jessica Stringham çš„åšå®¢æ–‡ç«  [â€œPython ä¸­çš„ Gibbs é‡‡æ ·â€](https://oreil.ly/tXmOq) ä»¥è·å–
    Gibbs é‡‡æ ·çš„ä¼˜ç§€ç®€å•ç¤ºä¾‹ã€‚
- en: An extension to this model, the *restricted Boltzmann machine* (RBM), removes
    the connections between units of the same type, therefore creating a two-layer
    bipartite graph. This allows RBMs to be stacked into *deep belief networks* to
    model more complex distributions. However, modeling high-dimensional data with
    RBMs remains impractical, due to the fact that Gibbs sampling with long mixing
    times is still required.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹çš„æ‰©å±•ï¼Œ*å—é™ç»å°”å…¹æ›¼æœº*ï¼ˆRBMï¼‰ï¼Œç§»é™¤äº†ç›¸åŒç±»å‹å•å…ƒä¹‹é—´çš„è¿æ¥ï¼Œå› æ­¤åˆ›å»ºäº†ä¸€ä¸ªä¸¤å±‚çš„äºŒéƒ¨å›¾ã€‚è¿™ä½¿å¾—RBMå¯ä»¥å †å æˆ*æ·±åº¦ä¿¡å¿µç½‘ç»œ*ï¼Œä»¥å»ºæ¨¡æ›´å¤æ‚çš„åˆ†å¸ƒã€‚ç„¶è€Œï¼Œä½¿ç”¨RBMå¯¹é«˜ç»´æ•°æ®è¿›è¡Œå»ºæ¨¡ä»ç„¶æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå› ä¸ºä»ç„¶éœ€è¦é•¿æ··åˆæ—¶é—´çš„å‰å¸ƒæ–¯é‡‡æ ·ã€‚
- en: It was only in the late 2000s that EBMs were shown to have potential for modeling
    more high-dimensional datasets and a framework for building deep EBMs was established.^([7](ch07.xhtml#idm45387010525504))
    Langevin dynamics became the preferred sampling method for EBMs, which later evolved
    into a training technique known as *score matching*. This further developed into
    a model class known as *Denoising Diffusion Probabilistic Models*, which power
    state-of-the-art generative models such as DALL.E 2 and ImageGen. We will explore
    diffusion models in more detail in [ChapterÂ 8](ch08.xhtml#chapter_diffusion).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°2000å¹´ä»£æœ«ï¼ŒEBMæ‰è¢«è¯æ˜å…·æœ‰å¯¹æ›´é«˜ç»´æ•°æ®é›†è¿›è¡Œå»ºæ¨¡çš„æ½œåŠ›ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªæ„å»ºæ·±åº¦EBMçš„æ¡†æ¶ã€‚^([7](ch07.xhtml#idm45387010525504))
    LangevinåŠ¨åŠ›å­¦æˆä¸ºEBMçš„é¦–é€‰é‡‡æ ·æ–¹æ³•ï¼Œåæ¥æ¼”å˜æˆä¸€ç§ç§°ä¸º*å¾—åˆ†åŒ¹é…*çš„è®­ç»ƒæŠ€æœ¯ã€‚è¿™è¿›ä¸€æ­¥å‘å±•æˆä¸€ç§ç§°ä¸º*å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹*çš„æ¨¡å‹ç±»ï¼Œä¸ºDALL.E
    2å’ŒImageGenç­‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æä¾›åŠ¨åŠ›ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬8ç« ](ch08.xhtml#chapter_diffusion)ä¸­æ›´è¯¦ç»†åœ°æ¢è®¨æ‰©æ•£æ¨¡å‹ã€‚
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Energy-based models are a class of generative model that make use of an energy
    scoring functionâ€”a neural network that is trained to output low scores for real
    observations and high scores for generated observations. Calculating the probability
    distribution given by this score function would require normalizing by an intractable
    denominator. EBMs avoid this problem by utilizing two tricks: contrastive divergence
    for training the network and Langevin dynamics for sampling new observations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨èƒ½é‡è¯„åˆ†å‡½æ•°â€”â€”ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œç”¨äºä¸ºçœŸå®è§‚å¯Ÿè¾“å‡ºä½åˆ†æ•°ï¼Œä¸ºç”Ÿæˆè§‚å¯Ÿè¾“å‡ºé«˜åˆ†æ•°ã€‚è®¡ç®—ç”±è¯¥å¾—åˆ†å‡½æ•°ç»™å‡ºçš„æ¦‚ç‡åˆ†å¸ƒéœ€è¦é€šè¿‡ä¸€ä¸ªéš¾ä»¥å¤„ç†çš„åˆ†æ¯è¿›è¡Œå½’ä¸€åŒ–ã€‚EBMé€šè¿‡åˆ©ç”¨ä¸¤ä¸ªæŠ€å·§æ¥é¿å…è¿™ä¸ªé—®é¢˜ï¼šå¯¹æ¯”æ•£åº¦ç”¨äºè®­ç»ƒç½‘ç»œï¼ŒLangevinåŠ¨åŠ›å­¦ç”¨äºé‡‡æ ·æ–°è§‚å¯Ÿã€‚
- en: The energy function is trained by minimizing the difference between the generated
    sample scores and the scores of the training data, a technique known as contrastive
    divergence. This can be shown to be equivalent to minimizing the negative log-likelihood,
    as required by maximum likelihood estimation, but does not require us to calculate
    the intractable normalizing denominator. In practice, we approximate the sampling
    process for the fake samples to ensure the algorithm remains efficient.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½é‡å‡½æ•°é€šè¿‡æœ€å°åŒ–ç”Ÿæˆæ ·æœ¬å¾—åˆ†ä¸è®­ç»ƒæ•°æ®å¾—åˆ†ä¹‹é—´çš„å·®å¼‚æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§æŠ€æœ¯ç§°ä¸ºå¯¹æ¯”æ•£åº¦ã€‚å¯ä»¥è¯æ˜è¿™ç­‰ä»·äºæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œè¿™æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‰€è¦æ±‚çš„ï¼Œä½†ä¸éœ€è¦æˆ‘ä»¬è®¡ç®—éš¾ä»¥å¤„ç†çš„å½’ä¸€åŒ–åˆ†æ¯ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬è¿‘ä¼¼ä¸ºå‡æ ·æœ¬çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿ç®—æ³•ä¿æŒé«˜æ•ˆã€‚
- en: Sampling of deep EBMs is achieved through Langevin dynamics, a technique that
    uses the gradient of the score with respect to the input image to gradually transform
    random noise into a plausible observation by updating the input in small steps,
    following the gradient downhill. This improves upon earlier methods such as Gibbs
    sampling, which is utilized by restricted Boltzmann machines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦EBMçš„é‡‡æ ·æ˜¯é€šè¿‡LangevinåŠ¨åŠ›å­¦å®ç°çš„ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¾—åˆ†ç›¸å¯¹äºè¾“å…¥å›¾åƒçš„æ¢¯åº¦é€æ¸å°†éšæœºå™ªå£°è½¬åŒ–ä¸ºåˆç†è§‚å¯Ÿçš„æŠ€æœ¯ï¼Œé€šè¿‡æ›´æ–°è¾“å…¥è¿›è¡Œå°æ­¥éª¤ï¼Œæ²¿ç€æ¢¯åº¦ä¸‹é™ã€‚è¿™æ”¹è¿›äº†æ—©æœŸçš„æ–¹æ³•ï¼Œå¦‚å—é™ç»å°”å…¹æ›¼æœºä½¿ç”¨çš„å‰å¸ƒæ–¯é‡‡æ ·ã€‚
- en: ^([1](ch07.xhtml#idm45387012482384-marker)) Yilun Du and Igor Mordatch, â€œImplicit
    Generation and Modeling with Energy-Based Models,â€ March 20, 2019, [*https://arxiv.org/abs/1903.08689*](https://arxiv.org/abs/1903.08689).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm45387012482384-marker)) æœä¸€ä¼¦å’Œä¼Šæˆˆå°”Â·è«è¾¾å¥‡ï¼Œâ€œåŸºäºèƒ½é‡çš„æ¨¡å‹çš„éšå¼ç”Ÿæˆå’Œå»ºæ¨¡â€ï¼Œ2019å¹´3æœˆ20æ—¥ï¼Œ[*https://arxiv.org/abs/1903.08689*](https://arxiv.org/abs/1903.08689)ã€‚
- en: ^([2](ch07.xhtml#idm45387012241152-marker)) Prajit Ramachandran et al., â€œSearching
    for Activation Functions,â€ October 16, 2017, [*https://arxiv.org/abs/1710.05941v2*](https://arxiv.org/abs/1710.05941v2).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.xhtml#idm45387012241152-marker)) Prajit Ramachandranç­‰äººï¼Œâ€œæœç´¢æ¿€æ´»å‡½æ•°â€ï¼Œ2017å¹´10æœˆ16æ—¥ï¼Œ[*https://arxiv.org/abs/1710.05941v2*](https://arxiv.org/abs/1710.05941v2)ã€‚
- en: ^([3](ch07.xhtml#idm45387012049200-marker)) Max Welling and Yee Whye Teh, â€œBayesian
    Learning via Stochastic Gradient Langevin Dynamics,â€ 2011, *[*https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf*](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.xhtml#idm45387012049200-marker)) Max Wellingå’ŒYee Whye Tehï¼Œâ€œé€šè¿‡éšæœºæ¢¯åº¦LangevinåŠ¨åŠ›å­¦è¿›è¡Œè´å¶æ–¯å­¦ä¹ â€ï¼Œ2011å¹´ï¼Œ[*https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf*](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)
- en: ^([4](ch07.xhtml#idm45387011649104-marker)) Geoffrey E. Hinton, â€œTraining Products
    of Experts by Minimizing Contrastive Divergence,â€ 2002, *[*https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf*](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.xhtml#idm45387011649104-marker)) Geoffrey E. Hintonï¼Œâ€œé€šè¿‡æœ€å°åŒ–å¯¹æ¯”æ•£åº¦è®­ç»ƒä¸“å®¶ä¹˜ç§¯â€ï¼Œ2002å¹´ï¼Œ[*https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf*](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)ã€‚
- en: ^([5](ch07.xhtml#idm45387011627440-marker)) Oliver Woodford, â€œNotes on Contrastive
    Divergence,â€ 2006, [*https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf*](https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.xhtml#idm45387011627440-marker)) Oliver Woodfordï¼Œâ€œå¯¹æ¯”æ•£åº¦ç¬”è®°â€ï¼Œ2006å¹´ï¼Œ[*https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf*](https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)ã€‚
- en: ^([6](ch07.xhtml#idm45387010130976-marker)) David H. Ackley et al., â€œA Learning
    Algorithm for Boltzmann Machines,â€ 1985, *Cognitive Science* 9(1), 147-165.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.xhtml#idm45387010130976-marker)) David H. Ackleyç­‰äººï¼Œâ€œç»å°”å…¹æ›¼æœºçš„å­¦ä¹ ç®—æ³•â€ï¼Œ1985å¹´ï¼Œ*è®¤çŸ¥ç§‘å­¦*
    9(1), 147-165ã€‚
- en: ^([7](ch07.xhtml#idm45387010525504-marker)) Yann Lecun et al., â€œA Tutorial on
    Energy-Based Learning,â€ 2006, *[*https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning*](https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning)*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.xhtml#idm45387010525504-marker)) Yann Lecunç­‰äººï¼Œâ€œåŸºäºèƒ½é‡çš„å­¦ä¹ æ•™ç¨‹â€ï¼Œ2006å¹´ï¼Œ*[*https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning*](https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning)*.
