- en: Chapter 4\. Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。生成对抗网络
- en: In 2014, Ian Goodfellow et al. presented a paper entitled “Generative Adversarial
    Nets”^([1](ch04.xhtml#idm45387021611344)) at the Neural Information Processing
    Systems conference (NeurIPS) in Montreal. The introduction of generative adversarial
    networks (or GANs, as they are more commonly known) is now regarded as a key turning
    point in the history of generative modeling, as the core ideas presented in this
    paper have spawned some of the most successful and impressive generative models
    ever created.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Ian Goodfellow等人在蒙特利尔的神经信息处理系统会议（NeurIPS）上发表了一篇名为“生成对抗网络”的论文^([1](ch04.xhtml#idm45387021611344))。生成对抗网络（或者更常见的称为GAN）的引入现在被认为是生成建模历史上的一个关键转折点，因为这篇论文中提出的核心思想衍生出了一些最成功和令人印象深刻的生成模型。
- en: This chapter will first lay out the theoretical underpinning of GANs, then we
    will see how to build our own GAN using Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先阐述GAN的理论基础，然后我们将看到如何使用Keras构建我们自己的GAN。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Let’s start with a short story to illustrate some of the fundamental concepts
    used in the GAN training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简短的故事开始，以阐明GAN训练过程中使用的一些基本概念。
- en: The story of Brickki bricks and the forgers describes the training process of
    a generative adversarial network.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Brickki砖块和伪造者的故事描述了生成对抗网络的训练过程。
- en: A GAN is a battle between two adversaries, the *generator* and the *discriminator*.
    The generator tries to convert random noise into observations that look as if
    they have been sampled from the original dataset, and the discriminator tries
    to predict whether an observation comes from the original dataset or is one of
    the generator’s forgeries. Examples of the inputs and outputs to the two networks
    are shown in [Figure 4-2](#gan_diagram_simple).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是生成器和鉴别器之间的一场战斗。生成器试图将随机噪声转换为看起来像是从原始数据集中抽样的观察结果，而鉴别器试图预测一个观察结果是来自原始数据集还是生成器的伪造品之一。两个网络的输入和输出示例显示在[图4-2](#gan_diagram_simple)中。
- en: '![](Images/gdl2_0402.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0402.png)'
- en: Figure 4-2\. Inputs and outputs of the two networks in a GAN
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. GAN中两个网络的输入和输出
- en: At the start of the process, the generator outputs noisy images and the discriminator
    predicts randomly. The key to GANs lies in how we alternate the training of the
    two networks, so that as the generator becomes more adept at fooling the discriminator,
    the discriminator must adapt in order to maintain its ability to correctly identify
    which observations are fake. This drives the generator to find new ways to fool
    the discriminator, and so the cycle continues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程开始时，生成器输出嘈杂的图像，鉴别器随机预测。GAN的关键在于我们如何交替训练这两个网络，使得随着生成器变得更擅长欺骗鉴别器，鉴别器必须适应以保持其正确识别哪些观察结果是伪造的能力。这驱使生成器找到欺骗鉴别器的新方法，循环继续。
- en: Deep Convolutional GAN (DCGAN)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络（DCGAN）
- en: To see this in action, let’s start building our first GAN in Keras, to generate
    pictures of bricks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这个过程，让我们开始在Keras中构建我们的第一个GAN，以生成砖块的图片。
- en: We will be closely following one of the first major papers on GANs, “Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.”^([2](ch04.xhtml#idm45387021585984))
    In this 2015 paper, the authors show how to build a deep convolutional GAN to
    generate realistic images from a variety of datasets. They also introduce several
    changes that significantly improve the quality of the generated images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将密切关注GAN的第一篇重要论文之一，“使用深度卷积生成对抗网络进行无监督表示学习”^([2](ch04.xhtml#idm45387021585984))。在这篇2015年的论文中，作者展示了如何构建一个深度卷积GAN来从各种数据集中生成逼真的图像。他们还引入了一些显著改进生成图像质量的变化。
- en: Running the Code for This Example
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/01_dcgan/dcgan.ipynb*
    in the book repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的代码可以在位于书籍存储库中的Jupyter笔记本中找到，路径为*notebooks/04_gan/01_dcgan/dcgan.ipynb*。
- en: The Bricks Dataset
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 砖块数据集
- en: First, you’ll need to download the training data. We’ll be using the [Images
    of LEGO Bricks dataset](https://oreil.ly/3vp9f) that is available through Kaggle.
    This is a computer-rendered collection of 40,000 photographic images of 50 different
    toy bricks, taken from multiple angles. Some example images of Brickki products
    are shown in [Figure 4-3](Images/#gan_bricks_images).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要下载训练数据。我们将使用通过Kaggle提供的[乐高砖块图像数据集](https://oreil.ly/3vp9f)。这是一个包含40,000张来自多个角度拍摄的50种不同玩具砖块的照片的计算机渲染集合。一些Brickki产品的示例图像显示在[图4-3](Images/#gan_bricks_images)中。
- en: '![](Images/gdl2_0403.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0403.png)'
- en: Figure 4-3\. Examples of images from the Bricks dataset
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 砖块数据集中的图像示例
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [Example 4-1](#downloading-lego-dataset).
    This will save the images and accompanying metadata locally to the */data* folder.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在书籍存储库中运行Kaggle数据集下载脚本来下载数据集，如[示例4-1](#downloading-lego-dataset)所示。这将把图像和相关元数据保存到*/data*文件夹中。
- en: Example 4-1\. Downloading the Bricks dataset
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-1\. 下载砖块数据集
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`We use the Keras function `image_dataset_from_directory` to create a TensorFlow
    Dataset pointed at the directory where the images are stored, as shown in [Example 4-2](#preprocessing-lego-data).
    This allows us to read batches of images into memory only when required (e.g.,
    during training), so that we can work with large datasets and not worry about
    having to fit the entire dataset into memory. It also resizes the images to 64
    × 64, interpolating between pixel values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Keras函数`image_dataset_from_directory`创建一个指向存储图像的目录的TensorFlow数据集，如[示例4-2](#preprocessing-lego-data)所示。这使我们能够在需要时（例如在训练期间）将图像批量读入内存，以便我们可以处理大型数据集而不必担心必须将整个数据集装入内存。它还将图像调整为64×64大小，插值像素值之间的差值。
- en: Example 4-2\. Creating a TensorFlow Dataset from image files in a directory
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-2\. 从目录中的图像文件创建TensorFlow数据集
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The original data is scaled in the range [0, 255] to denote the pixel intensity.
    When training GANs we rescale the data to the range [–1, 1] so that we can use
    the tanh activation function on the final layer of the generator, which tends
    to provide stronger gradients than the sigmoid function ([Example 4-3](#preprocessing-lego-data_2)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据在范围[0, 255]内缩放以表示像素强度。在训练GAN时，我们将数据重新缩放到范围[-1, 1]，以便我们可以在生成器的最后一层使用tanh激活函数，该函数提供比sigmoid函数更强的梯度（[示例4-3](#preprocessing-lego-data_2)）。
- en: Example 4-3\. Preprocessing the Bricks dataset
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-3。预处理砖块数据集
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s now take a look at how we build the discriminator.`  `## The Discriminator
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何构建鉴别器。## 鉴别器
- en: 'The goal of the discriminator is to predict if an image is real or fake. This
    is a supervised image classification problem, so we can use a similar architecture
    to those we worked with in [Chapter 2](ch02.xhtml#chapter_deep_learning): stacked
    convolutional layers, with a single output node.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器的目标是预测图像是真实的还是伪造的。这是一个监督图像分类问题，因此我们可以使用与我们在[第2章](ch02.xhtml#chapter_deep_learning)中使用的类似架构：堆叠的卷积层，带有单个输出节点。
- en: The full architecture of the discriminator we will be building is shown in [Table 4-1](#gan_bricks_discriminator).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建的鉴别器的完整架构显示在[表4-1](#gan_bricks_discriminator)中。
- en: Table 4-1\. Model summary of the discriminator
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1。鉴别器的模型摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, 64, 64, 1) | 0 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| InputLayer | (None, 64, 64, 1) | 0 |'
- en: '| Conv2D | (None, 32, 32, 64) | 1,024 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 32, 32, 64) | 1,024 |'
- en: '| LeakyReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 32, 32, 64) | 0 |'
- en: '| Dropout | (None, 32, 32, 64) | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | (None, 32, 32, 64) | 0 |'
- en: '| Conv2D | (None, 16, 16, 128) | 131,072 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 16, 16, 128) | 131,072 |'
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
- en: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
- en: '| Dropout | (None, 16, 16, 128) | 0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | (None, 16, 16, 128) | 0 |'
- en: '| Conv2D | (None, 8, 8, 256) | 524,288 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 8, 8, 256) | 524,288 |'
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
- en: '| LeakyReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 8, 8, 256) | 0 |'
- en: '| Dropout | (None, 8, 8, 256) | 0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | (None, 8, 8, 256) | 0 |'
- en: '| Conv2D | (None, 4, 4, 512) | 2,097,152 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 4, 4, 512) | 2,097,152 |'
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
- en: '| LeakyReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 4, 4, 512) | 0 |'
- en: '| Dropout | (None, 4, 4, 512) | 0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | (None, 4, 4, 512) | 0 |'
- en: '| Conv2D | (None, 1, 1, 1) | 8,192 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 1, 1, 1) | 8,192 |'
- en: '| Flatten | (None, 1) | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Flatten | (None, 1) | 0 |'
- en: '| Total params | 2,765,312 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 2,765,312 |'
- en: '| Trainable params | 2,763,520 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 2,763,520 |'
- en: '| Non-trainable params | 1,792 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 1,792 |'
- en: The Keras code to build the discriminator is provided in [Example 4-4](#the-discriminator-ex).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提供构建鉴别器的Keras代码在[示例4-4](#the-discriminator-ex)中。
- en: Example 4-4\. The discriminator
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-4。鉴别器
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO1-1)'
- en: Define the `Input` layer of the discriminator (the image).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 定义鉴别器的`Input`层（图像）。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO1-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO1-2)'
- en: Stack `Conv2D` layers on top of each other, with `BatchNormalization`, `LeakyReLU`
    activation, and `Dropout` layers sandwiched in between.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将`Conv2D`层堆叠在一起，中间夹有`BatchNormalization`、`LeakyReLU`激活和`Dropout`层。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO1-3)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO1-3)'
- en: Flatten the last convolutional layer—by this point, the shape of the tensor
    is 1 × 1 × 1, so there is no need for a final `Dense` layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将最后一个卷积层展平-到这一点，张量的形状为1×1×1，因此不需要最终的`Dense`层。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO1-4)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO1-4)'
- en: The Keras model that defines the discriminator—a model that takes an input image
    and outputs a single number between 0 and 1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 定义鉴别器的Keras模型-一个接受输入图像并输出介于0和1之间的单个数字的模型。
- en: Notice how we use a stride of 2 in some of the `Conv2D` layers to reduce the
    spatial shape of the tensor as it passes through the network (64 in the original
    image, then 32, 16, 8, 4, and finally 1), while increasing the number of channels
    (1 in the grayscale input image, then 64, 128, 256, and finally 512), before collapsing
    to a single prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在一些`Conv2D`层中使用步幅为2，以减少通过网络时张量的空间形状（原始图像中为64，然后32、16、8、4，最后为1），同时增加通道数（灰度输入图像中为1，然后64、128、256，最后为512），最终折叠为单个预测。
- en: We use a sigmoid activation on the final `Conv2D` layer to output a number between
    0 and 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最后一个`Conv2D`层上使用sigmoid激活函数，输出一个介于0和1之间的数字。
- en: The Generator
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器
- en: Now let’s build the generator. The input to the generator will be a vector drawn
    from a multivariate standard normal distribution. The output is an image of the
    same size as an image in the original training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建生成器。生成器的输入将是从多元标准正态分布中抽取的向量。输出是与原始训练数据中的图像大小相同的图像。
- en: 'This description may remind you of the decoder in a variational autoencoder.
    In fact, the generator of a GAN fulfills exactly the same purpose as the decoder
    of a VAE: converting a vector in the latent space to an image. The concept of
    mapping from a latent space back to the original domain is very common in generative
    modeling, as it gives us the ability to manipulate vectors in the latent space
    to change high-level features of images in the original domain.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个描述可能让你想起变分自动编码器中的解码器。事实上，GAN的生成器与VAE的解码器完全履行相同的目的：将潜在空间中的向量转换为图像。在生成建模中，从潜在空间映射回原始域的概念非常常见，因为它使我们能够操纵潜在空间中的向量以改变原始域中图像的高级特征。
- en: The architecture of the generator we will be building is shown in [Table 4-2](#gan_bricks_generator).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建的生成器的架构显示在[表4-2](#gan_bricks_generator)中。
- en: Table 4-2\. Model summary of the generator
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2。生成器的模型摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, 100) | 0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: InputLayer（无，100）0
- en: '| Reshape | (None, 1, 1, 100) | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: Reshape（无，1，1，100）0
- en: '| Conv2DTranspose | (None, 4, 4, 512) | 819,200 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，4，4，512）819,200
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，4，4，512）2,048
- en: '| ReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，4，4，512）0
- en: '| Conv2DTranspose | (None, 8, 8, 256) | 2,097,152 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，8，8，256）2,097,152
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，8，8，256）1,024
- en: '| ReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，8，8，256）0
- en: '| Conv2DTranspose | (None, 16, 16, 128) | 524,288 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，16，16，128）524,288
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，16，16，128）512
- en: '| ReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，16，16，128）0
- en: '| Conv2DTranspose | (None, 32, 32, 64) | 131,072 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，32，32，64）131,072
- en: '| BatchNormalization | (None, 32, 32, 64) | 256 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，32，32，64）256
- en: '| ReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，32，32，64）0
- en: '| Conv2DTranspose | (None, 64, 64, 1) | 1,024 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，64，64，1）1,024
- en: '| Total params | 3,576,576 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: 总参数3,576,576
- en: '| Trainable params | 3,574,656 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: 可训练参数3,574,656
- en: '| Non-trainable params | 1,920 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: 不可训练参数1,920
- en: The code for building the generator is given in [Example 4-5](#the-generator-ex).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生成器的代码在[示例4-5](#the-generator-ex)中给出。
- en: Example 4-5\. The generator
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-5。生成器
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
- en: Define the `Input` layer of the generator—a vector of length 100.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器的“Input”层-长度为100的向量。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
- en: We use a `Reshape` layer to give a 1 × 1 × 100 tensor, so that we can start
    applying convolutional transpose operations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个“Reshape”层来给出一个1×1×100的张量，这样我们就可以开始应用卷积转置操作。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
- en: We pass this through four `Conv2DTranspose` layers, with `BatchNormalization`
    and `LeakyReLU` layers sandwiched in between.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过四个“Conv2DTranspose”层传递这些数据，其中夹在中间的是“BatchNormalization”和“LeakyReLU”层。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
- en: The final `Conv2DTranspose` layer uses a tanh activation function to transform
    the output to the range [–1, 1], to match the original image domain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的“Conv2DTranspose”层使用tanh激活函数将输出转换为范围[-1,1]，以匹配原始图像域。
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
- en: The Keras model that defines the generator—a model that accepts a vector of
    length 100 and outputs a tensor of shape `[64, 64, 1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器的Keras模型-接受长度为100的向量并输出形状为`[64，64，1]`的张量。
- en: Notice how we use a stride of 2 in some of the `Conv2DTranspose` layers to increase
    the spatial shape of the tensor as it passes through the network (1 in the original
    vector, then 4, 8, 16, 32, and finally 64), while decreasing the number of channels
    (512 then 256, 128, 64, and finally 1 to match the grayscale output).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在一些“Conv2DTranspose”层中使用步幅为2，以增加通过网络传递时张量的空间形状（原始向量中为1，然后为4，8，16，32，最终为64），同时减少通道数（512，然后为256，128，64，最终为1以匹配灰度输出）。
- en: Training the DCGAN
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练DCGAN
- en: As we have seen, the architectures of the generator and discriminator in a DCGAN
    are very simple and not so different from the VAE models that we looked at in
    [Chapter 3](ch03.xhtml#chapter_vae). The key to understanding GANs lies in understanding
    the training process for the generator and discriminator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在DCGAN中生成器和鉴别器的架构非常简单，并且与我们在第3章中看到的VAE模型并没有太大不同。理解GAN的关键在于理解生成器和鉴别器的训练过程。
- en: We can train the discriminator by creating a training set where some of the
    images are *real* observations from the training set and some are *fake* outputs
    from the generator. We then treat this as a supervised learning problem, where
    the labels are 1 for the real images and 0 for the fake images, with binary cross-entropy
    as the loss function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个训练集来训练鉴别器，其中一些图像是来自训练集的*真实*观察结果，一些是来自生成器的*假*输出。然后我们将其视为一个监督学习问题，其中真实图像的标签为1，假图像的标签为0，损失函数为二元交叉熵。
- en: How should we train the generator? We need to find a way of scoring each generated
    image so that it can optimize toward high-scoring images. Luckily, we have a discriminator
    that does exactly that! We can generate a batch of images and pass these through
    the discriminator to get a score for each image. The loss function for the generator
    is then simply the binary cross-entropy between these probabilities and a vector
    of ones, because we want to train the generator to produce images that the discriminator
    thinks are real.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何训练生成器？我们需要找到一种评分每个生成的图像的方法，以便它可以优化到高分图像。幸运的是，我们有一个鉴别器正是这样做的！我们可以生成一批图像并将其通过鉴别器以获得每个图像的分数。然后生成器的损失函数就是这些概率与一个全为1的向量之间的二元交叉熵，因为我们希望训练生成器生成鉴别器认为是真实的图像。
- en: Crucially, we must alternate the training of these two networks, making sure
    that we only update the weights of one network at a time. For example, during
    the generator training process, only the generator’s weights are updated. If we
    allowed the discriminator’s weights to change as well, the discriminator would
    just adjust so that it is more likely to predict the generated images to be real,
    which is not the desired outcome. We want generated images to be predicted close
    to 1 (real) because the generator is strong, not because the discriminator is
    weak.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，我们必须交替训练这两个网络，确保我们一次只更新一个网络的权重。例如，在生成器训练过程中，只有生成器的权重会被更新。如果我们允许鉴别器的权重也发生变化，那么鉴别器将只是调整自己，以便更有可能预测生成的图像是真实的，这不是期望的结果。我们希望生成的图像被预测接近1（真实），因为生成器强大，而不是因为鉴别器弱。
- en: A diagram of the training process for the discriminator and generator is shown
    in [Figure 4-5](#gan_bricks_training).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器和生成器的训练过程的图示如[图4-5](#gan_bricks_training)所示。
- en: '![](Images/gdl2_0405.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0405.png)'
- en: Figure 4-5\. Training the DCGAN—gray boxes indicate that the weights are frozen
    during training
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。训练DCGAN-灰色框表示在训练过程中权重被冻结
- en: Keras provides us with the ability to create a custom `train_step` function
    to implement this logic. [Example 4-7](#building-the-gan-ex) shows the full `DCGAN`
    model class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了创建自定义`train_step`函数来实现这一逻辑的能力。[示例4-7](#building-the-gan-ex)展示了完整的`DCGAN`模型类。
- en: Example 4-7\. Compiling the DCGAN
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-7。编译DCGAN
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO3-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO3-1)'
- en: The loss function for the generator and discriminator is `BinaryCrossentropy`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和鉴别器的损失函数是`BinaryCrossentropy`。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO3-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO3-2)'
- en: To train the network, first sample a batch of vectors from a multivariate standard
    normal distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，首先从多元标准正态分布中抽取一批向量。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO3-3)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO3-3)'
- en: Next, pass these through the generator to produce a batch of generated images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过生成器生成一批生成的图像。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO3-4)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO3-4)'
- en: Now ask the discriminator to predict the realness of the batch of real images…​
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让鉴别器预测一批真实图像的真实性...​
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO3-5)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO3-5)'
- en: …​and the batch of generated images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '...​和一批生成的图像。'
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO3-6)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO3-6)'
- en: The discriminator loss is the average binary cross-entropy across both the real
    images (with label 1) and the fake images (with label 0).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器损失是真实图像（标签为1）和假图像（标签为0）之间的平均二元交叉熵。
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO3-7)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO3-7)'
- en: The generator loss is the binary cross-entropy between the discriminator predictions
    for the generated images and a label of 1.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失是鉴别器对生成图像的预测与标签1之间的二元交叉熵。
- en: '[![8](Images/8.png)](#co_generative_adversarial_networks_CO3-8)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_generative_adversarial_networks_CO3-8)'
- en: Update the weights of the discriminator and generator separately.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分别更新鉴别器和生成器的权重。
- en: The discriminator and generator are constantly fighting for dominance, which
    can make the DCGAN training process unstable. Ideally, the training process will
    find an equilibrium that allows the generator to learn meaningful information
    from the discriminator and the quality of the images will start to improve. After
    enough epochs, the discriminator tends to end up dominating, as shown in [Figure 4-6](#gan_bricks_loss_accuracy),
    but this may not be a problem as the generator may have already learned to produce
    sufficiently high-quality images by this point.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器和生成器不断争夺主导地位，这可能使DCGAN训练过程不稳定。理想情况下，训练过程将找到一个平衡点，使生成器能够从鉴别器那里学习有意义的信息，图像的质量将开始提高。经过足够的epochs，鉴别器往往最终占据主导地位，如[图4-6](#gan_bricks_loss_accuracy)所示，但这可能不是问题，因为生成器可能已经学会在这一点上生成足够高质量的图像。
- en: '![](Images/gdl2_0406.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0406.png)'
- en: Figure 4-6\. Loss and accuracy of the discriminator and generator during training
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。训练过程中鉴别器和生成器的损失和准确率
- en: Adding Noise to the Labels
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向标签添加噪声
- en: A useful trick when training GANs is to add a small amount of random noise to
    the training labels. This helps to improve the stability of the training process
    and sharpen the generated images. This *label smoothing* acts as way to tame the
    discriminator, so that it is presented with a more challenging task and doesn’t
    overpower the generator.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练GAN时的一个有用技巧是向训练标签添加少量随机噪声。这有助于改善训练过程的稳定性并增强生成的图像。这种*标签平滑*作为一种驯服鉴别器的方式，使其面临更具挑战性的任务，不会压倒生成器。
- en: Analysis of the DCGAN
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DCGAN的分析
- en: By observing images produced by the generator at specific epochs during training
    ([Figure 4-7](#gan_bricks_by_epoch)), it is clear that the generator is becoming
    increasingly adept at producing images that could have been drawn from the training
    set.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察训练过程中特定时期生成器生成的图像（[图4-7](#gan_bricks_by_epoch)），可以清楚地看到生成器越来越擅长生成可能来自训练集的图像。
- en: '![](Images/gdl2_0407.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0407.png)'
- en: Figure 4-7\. Output from the generator at specific epochs during training
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。训练过程中特定时期生成器的输出
- en: It is somewhat miraculous that a neural network is able to convert random noise
    into something meaningful. It is worth remembering that we haven’t provided the
    model with any additional features beyond the raw pixels, so it has to work out
    high-level concepts such as how to draw shadows, cuboids, and circles entirely
    by itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 神奇的是神经网络能够将随机噪声转换为有意义的东西。值得记住的是，我们除了原始像素之外没有提供模型任何额外的特征，因此它必须自行解决如何绘制阴影、立方体和圆等高级概念。
- en: 'Another requirement of a successful generative model is that it doesn’t only
    reproduce images from the training set. To test this, we can find the image from
    the training set that is closest to a particular generated example. A good measure
    for distance is the *L1 distance*, defined as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 成功生成模型的另一个要求是它不仅仅是复制训练集中的图像。为了测试这一点，我们可以找到训练集中与特定生成示例最接近的图像。一个好的距离度量是*L1距离*，定义为：
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 4-8](#gan_bricks_closest) shows the closest observations in the training
    set for a selection of generated images. We can see that while there is some degree
    of similarity between the generated images and the training set, they are not
    identical. This shows that the generator has understood these high-level features
    and can generate examples that are distinct from those it has already seen.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-8](#gan_bricks_closest)显示了一些生成图像在训练集中最接近的观察结果。我们可以看到，虽然生成图像与训练集之间存在一定程度的相似性，但它们并不完全相同。这表明生成器已经理解了这些高级特征，并且能够生成与已经看到的图像不同的示例。'
- en: '![](Images/gdl2_0408.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0408.png)'
- en: Figure 4-8\. Closest matches of generated images from the training set
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. 从训练集中生成图像的最接近匹配
- en: 'GAN Training: Tips and Tricks'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN训练：技巧和窍门
- en: While GANs are a major breakthrough for generative modeling, they are also notoriously
    difficult to train. We will explore some of the most common problems and challenges
    encountered when training GANs in this section, alongside potential solutions.
    In the next section, we will look at some more fundamental adjustments to the
    GAN framework that we can make to remedy many of these problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GAN是生成建模的重大突破，但训练起来也非常困难。在本节中，我们将探讨训练GAN时遇到的一些最常见问题和挑战，以及潜在的解决方案。在下一节中，我们将看一些更基本的调整GAN框架的方法，以解决许多这些问题。
- en: Discriminator overpowers the generator
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鉴别器压倒生成器
- en: If the discriminator becomes too strong, the signal from the loss function becomes
    too weak to drive any meaningful improvements in the generator. In the worst-case
    scenario, the discriminator perfectly learns to separate real images from fake
    images and the gradients vanish completely, leading to no training whatsoever,
    as can be seen in [Figure 4-9](#gan_discriminator_dominant_ex).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果鉴别器变得过于强大，损失函数的信号变得太弱，无法驱动生成器中的任何有意义的改进。在最坏的情况下，鉴别器完全学会区分真实图像和假图像，梯度完全消失，导致没有任何训练，如[图4-9](#gan_discriminator_dominant_ex)所示。
- en: '![](Images/gdl2_0409.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0409.png)'
- en: Figure 4-9\. Example output when the discriminator overpowers the generator
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. 当鉴别器压倒生成器时的示例输出
- en: 'If you find your discriminator loss function collapsing, you need to find ways
    to weaken the discriminator. Try the following suggestions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发现鉴别器损失函数坍缩，需要找到削弱鉴别器的方法。尝试以下建议：
- en: Increase the `rate` parameter of the `Dropout` layers in the discriminator to
    dampen the amount of information that flows through the network.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加鉴别器中`Dropout`层的`rate`参数，以减少通过网络的信息量。
- en: Reduce the learning rate of the discriminator.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低鉴别器的学习率。
- en: Reduce the number of convolutional filters in the discriminator.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少鉴别器中的卷积滤波器数量。
- en: Add noise to the labels when training the discriminator.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练鉴别器时向标签添加噪音。
- en: Flip the labels of some images at random when training the discriminator.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练鉴别器时，随机翻转一些图像的标签。
- en: Generator overpowers the discriminator
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器压倒鉴别器
- en: If the discriminator is not powerful enough, the generator will find ways to
    easily trick the discriminator with a small sample of nearly identical images.
    This is known as *mode collapse*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果鉴别器不够强大，生成器将找到一种方法轻松欺骗鉴别器，只需少量几乎相同的图像样本。这被称为*模式坍塌*。
- en: For example, suppose we were to train the generator over several batches without
    updating the discriminator in between. The generator would be inclined to find
    a single observation (also known as a *mode*) that always fools the discriminator
    and would start to map every point in the latent input space to this image. Moreover,
    the gradients of the loss function would collapse to near 0, so it wouldn’t be
    able to recover from this state.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在不更新鉴别器的情况下训练生成器多个批次。生成器会倾向于找到一个始终欺骗鉴别器的单个观察（也称为*模式*），并开始将潜在输入空间中的每个点映射到这个图像。此外，损失函数的梯度会坍缩到接近0，因此无法从这种状态中恢复。
- en: Even if we then tried to retrain the discriminator to stop it being fooled by
    this one point, the generator would simply find another mode that fools the discriminator,
    since it has already become numb to its input and therefore has no incentive to
    diversify its output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们尝试重新训练鉴别器以阻止它被这一点欺骗，生成器也会简单地找到另一个欺骗鉴别器的模式，因为它已经对其输入麻木，因此没有多样化其输出的动机。
- en: The effect of mode collapse can be seen in [Figure 4-10](#gan_mode_collapse).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 模式坍塌的效果可以在[图4-10](#gan_mode_collapse)中看到。
- en: '![](Images/gdl2_0410.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0410.png)'
- en: Figure 4-10\. Example of mode collapse when the generator overpowers the discriminator
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10\. 当生成器压倒鉴别器时模式坍塌的示例
- en: If you find that your generator is suffering from mode collapse, you can try
    strengthening the discriminator using the opposite suggestions to those listed
    in the previous section. Also, you can try reducing the learning rate of both
    networks and increasing the batch size.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发现生成器遭受模式坍塌，可以尝试使用与前一节中列出的相反建议来加强鉴别器。此外，可以尝试降低两个网络的学习率并增加批量大小。
- en: Uninformative loss
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无信息损失
- en: Since the deep learning model is compiled to minimize the loss function, it
    would be natural to think that the smaller the loss function of the generator,
    the better the quality of the images produced. However, since the generator is
    only graded against the current discriminator and the discriminator is constantly
    improving, we cannot compare the loss function evaluated at different points in
    the training process. Indeed, in [Figure 4-6](#gan_bricks_loss_accuracy), the
    loss function of the generator actually increases over time, even though the quality
    of the images is clearly improving. This lack of correlation between the generator
    loss and image quality sometimes makes GAN training difficult to monitor.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习模型被编译为最小化损失函数，自然会认为生成器的损失函数越小，生成的图像质量就越好。然而，由于生成器只针对当前鉴别器进行评分，而鉴别器不断改进，我们无法比较在训练过程中不同点评估的损失函数。实际上，在[图4-6](#gan_bricks_loss_accuracy)中，生成器的损失函数随时间增加，尽管图像质量明显提高。生成器损失与图像质量之间的缺乏相关性有时使得GAN训练难以监控。
- en: Hyperparameters
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: As we have seen, even with simple GANs, there are a large number of hyperparameters
    to tune. As well as the overall architecture of both the discriminator and the
    generator, there are the parameters that govern batch normalization, dropout,
    learning rate, activation layers, convolutional filters, kernel size, striding,
    batch size, and latent space size to consider. GANs are highly sensitive to very
    slight changes in all of these parameters, and finding a set of parameters that
    works is often a case of educated trial and error, rather than following an established
    set of guidelines.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，即使是简单的GAN，也有大量的超参数需要调整。除了鉴别器和生成器的整体架构外，还有控制批量归一化、丢弃、学习率、激活层、卷积滤波器、内核大小、步幅、批量大小和潜在空间大小的参数需要考虑。GAN对所有这些参数的微小变化非常敏感，找到一组有效的参数通常是经过有教养的试错过程，而不是遵循一套已建立的指导方针。
- en: This is why it is important to understand the inner workings of the GAN and
    know how to interpret the loss function—so that you can identify sensible adjustments
    to the hyperparameters that might improve the stability of the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么重要理解GAN的内部工作原理并知道如何解释损失函数——这样你就可以识别出可能改善模型稳定性的超参数的合理调整。
- en: Tackling GAN challenges
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决GAN的挑战
- en: In recent years, several key advancements have drastically improved the overall
    stability of GAN models and diminished the likelihood of some of the problems
    listed earlier, such as mode collapse.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，一些关键进展大大提高了GAN模型的整体稳定性，并减少了一些早期列出的问题的可能性，比如模式崩溃。
- en: In the remainder of this chapter we shall examine the Wasserstein GAN with Gradient
    Penalty (WGAN-GP), which makes several key adjustments to the GAN framework we
    have explored thus far to improve the stability and quality of the image generation
    process.`  `# Wasserstein GAN with Gradient Penalty (WGAN-GP)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将研究带有梯度惩罚的Wasserstein GAN（WGAN-GP），该模型对我们迄今为止探索的GAN框架进行了几个关键调整，以改善图像生成过程的稳定性和质量。`
    `#带有梯度惩罚的Wasserstein GAN（WGAN-GP）
- en: In this section we will build a WGAN-GP to generate faces from the CelebA dataset
    that we utilized in [Chapter 3](ch03.xhtml#chapter_vae).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个WGAN-GP来从我们在[第3章](ch03.xhtml#chapter_vae)中使用的CelebA数据集中生成人脸。
- en: Running the Code for This Example
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*
    in the book repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码可以在书库中的*notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*中找到。
- en: The code has been adapted from the excellent [WGAN-GP tutorial](https://oreil.ly/dHYbC)
    created by Aakash Kumar Nain, available on the Keras website.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是从由Aakash Kumar Nain创建的优秀的[WGAN-GP教程](https://oreil.ly/dHYbC)中改编而来，该教程可在Keras网站上找到。
- en: 'The Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al.,^([4](ch04.xhtml#idm45387021016704))
    was one of the first big steps toward stabilizing GAN training. With a few changes,
    the authors were able to show how to train GANs that have the following two properties
    (quoted from the paper):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein GAN（WGAN）是由Arjovsky等人在2017年的一篇论文中引入的，是稳定GAN训练的第一步。通过一些改变，作者们能够展示如何训练具有以下两个特性的GAN（引用自论文）：
- en: A meaningful loss metric that correlates with the generator’s convergence and
    sample quality
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与生成器的收敛和样本质量相关的有意义的损失度量
- en: Improved stability of the optimization process
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化过程的稳定性提高
- en: Specifically, the paper introduces the *Wasserstein loss function* for both
    the discriminator and the generator. Using this loss function instead of binary
    cross-entropy results in a more stable convergence of the GAN.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，该论文为鉴别器和生成器引入了*Wasserstein损失函数*。使用这个损失函数而不是二元交叉熵会导致GAN更稳定地收敛。
- en: In this section we’ll define the Wasserstein loss function and then see what
    other changes we need to make to the model architecture and training process to
    incorporate our new loss function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义Wasserstein损失函数，然后看看我们需要对模型架构和训练过程做哪些其他更改以整合我们的新损失函数。
- en: You can find the full model class in the Jupyter notebook located at *chapter05/wgan-gp/faces/train.ipynb*
    in the book repository.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在书库中的*chapter05/wgan-gp/faces/train.ipynb*中找到完整的模型类。
- en: Wasserstein Loss
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wasserstein损失
- en: Let’s first remind ourselves of the definition of binary cross-entropy loss—the
    function that we are currently using to train the discriminator and generator
    of the GAN ([Equation 4-1](#binary-cross-entropy-loss)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下二元交叉熵损失的定义——我们目前用来训练GAN的函数（[方程4-1](#binary-cross-entropy-loss)）。
- en: Equation 4-1\. Binary cross-entropy loss
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-1. 二元交叉熵损失
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: To train the GAN discriminator <math alttext="upper D"><mi>D</mi></math> , we
    calculate the loss when comparing predictions for real images <math alttext="p
    Subscript i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i Baseline equals 0"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mn>0</mn></mrow></math> . Therefore, for the GAN discriminator, minimizing the
    loss function can be written as shown in [Equation 4-2](#GAN-discriminator-loss-minimization).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GAN鉴别器D，我们计算了对比真实图像x_i的预测p_i与响应y_i=1以及对比生成图像G(z_i)的预测p_i与响应y_i=0的损失。因此，对于GAN鉴别器，最小化损失函数可以写成[方程4-2](#GAN-discriminator-loss-minimization)所示。
- en: Equation 4-2\. GAN discriminator loss minimization
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-2. GAN鉴别器损失最小化
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
- en: To train the GAN generator <math alttext="upper G"><mi>G</mi></math> , we calculate
    the loss when comparing predictions for generated images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response
    <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the GAN generator, minimizing
    the loss function can be written as shown in [Equation 4-3](#GAN-generator-loss-minimization).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GAN生成器G，我们计算了对比生成图像G(z_i)的预测p_i与响应y_i=1的损失。因此，对于GAN生成器，最小化损失函数可以写成[方程4-3](#GAN-generator-loss-minimization)所示。
- en: Equation 4-3\. GAN generator loss minimization
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-3. GAN生成器损失最小化
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: Now let’s compare this to the Wasserstein loss function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其与Wasserstein损失函数进行比较。
- en: First, the Wasserstein loss requires that we use <math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> =
    –1 as labels, rather than 1 and 0\. We also remove the sigmoid activation from
    the final layer of the discriminator, so that predictions <math alttext="p Subscript
    i"><msub><mi>p</mi> <mi>i</mi></msub></math> are no longer constrained to fall
    in the range [0, 1] but instead can now be any number in the range ( <math alttext="negative
    normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math> , <math alttext="normal
    infinity"><mi>∞</mi></math> ). For this reason, the discriminator in a WGAN is
    usually referred to as a *critic* that outputs a *score* rather than a probability.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Wasserstein损失要求我们使用y_i=1和y_i=-1作为标签，而不是1和0。我们还从鉴别器的最后一层中移除了sigmoid激活，使得预测p_i不再受限于[0,
    1]范围，而是可以是任何范围内的任意数字（负无穷，正无穷）。因此，在WGAN中，鉴别器通常被称为*评论家*，输出*分数*而不是概率。
- en: 'The Wasserstein loss function is defined as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein损失函数定义如下：
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'To train the WGAN critic <math alttext="upper D"><mi>D</mi></math> , we calculate
    the loss when comparing predictions for real images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> = –1\. Therefore, for the
    WGAN critic, minimizing the loss function can be written as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练WGAN评论家<math alttext="upper D"><mi>D</mi></math>，我们计算真实图像的预测与响应之间的损失<math
    alttext="p Subscript i Baseline equals upper D left-parenthesis x Subscript i
    Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>与响应<math
    alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</msub>
    <mo>=</mo> <mn>1</mn></mrow></math>，以及生成图像的预测与响应之间的损失<math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>与响应<math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</msub></math> = -1。因此，对于WGAN评论家，最小化损失函数可以写成如下形式：
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN critic tries to maximize the difference between its
    predictions for real images and generated images.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，WGAN评论家试图最大化其对真实图像和生成图像的预测之间的差异。
- en: 'To train the WGAN generator, we calculate the loss when comparing predictions
    for generated images <math alttext="p Subscript i Baseline equals upper D left-parenthesis
    upper G left-parenthesis z Subscript i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    to the response <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the WGAN
    generator, minimizing the loss function can be written as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练WGAN生成器，我们计算生成图像的预测与响应之间的损失<math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>与响应<math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>。因此，对于WGAN生成器，最小化损失函数可以写成如下形式：
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN generator tries to produce images that are scored as
    highly as possible by the critic (i.e., the critic is fooled into thinking they
    are real).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，WGAN生成器试图生成评论家尽可能高分的图像（即，评论家被欺骗以为它们是真实的）。
- en: The Lipschitz Constraint
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利普希茨约束
- en: It may surprise you that we are now allowing the critic to output any number
    in the range ( <math alttext="negative normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math>
    , <math alttext="normal infinity"><mi>∞</mi></math> ), rather than applying a
    sigmoid function to restrict the output to the usual [0, 1] range. The Wasserstein
    loss can therefore be very large, which is unsettling—usually, large numbers in
    neural networks are to be avoided!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 也许让你惊讶的是，我们现在允许评论家输出范围内的任何数字（<math alttext="negative normal infinity"><mrow><mo>-</mo>
    <mi>∞</mi></mrow></math>，<math alttext="normal infinity"><mi>∞</mi></math>），而不是应用Sigmoid函数将输出限制在通常的[0,1]范围内。因此，Wasserstein损失可能非常大，这令人不安——通常情况下，神经网络中的大数值应该避免！
- en: In fact, the authors of the WGAN paper show that for the Wasserstein loss function
    to work, we also need to place an additional constraint on the critic. Specifically,
    it is required that the critic is a *1-Lipschitz continuous function*. Let’s pick
    this apart to understand what it means in more detail.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，WGAN论文的作者表明，为了使Wasserstein损失函数起作用，我们还需要对评论家施加额外的约束。具体来说，评论家必须是*1-Lipschitz连续函数*。让我们详细解释一下这意味着什么。
- en: 'The critic is a function <math alttext="upper D"><mi>D</mi></math> that converts
    an image into a prediction. We say that this function is 1-Lipschitz if it satisfies
    the following inequality for any two input images, <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> and <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    :'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家是一个将图像转换为预测的函数<math alttext="upper D"><mi>D</mi></math>。如果对于任意两个输入图像<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>和<math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math>，该函数满足以下不等式，则我们称该函数为1-Lipschitz：
- en: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>≤</mo> <mn>1</mn></mrow></math>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>≤</mo> <mn>1</mn></mrow></math>
- en: Here, <math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math> is the average pixelwise absolute difference
    between two images and <math alttext="StartAbsoluteValue upper D left-parenthesis
    x 1 right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo>
    <mi>D</mi></mrow> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math> is the absolute difference between the critic
    predictions. Essentially, we require a limit on the rate at which the predictions
    of the critic can change between two images (i.e., the absolute value of the gradient
    must be at most 1 everywhere). We can see this applied to a Lipschitz continuous
    1D function in [Figure 4-11](#lipschitz)—at no point does the line enter the cone,
    wherever you place the cone on the line. In other words, there is a limit on the
    rate at which the line can rise or fall at any point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math>是两个图像之间的平均像素绝对差异，<math alttext="StartAbsoluteValue
    upper D left-parenthesis x 1 right-parenthesis minus upper D left-parenthesis
    x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo> <mi>D</mi></mrow>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math>是评论家预测之间的绝对差异。基本上，我们要求评论家的预测在两个图像之间变化的速率有限（即，梯度的绝对值必须在任何地方最多为1）。我们可以看到这应用于利普希茨连续的一维函数中，如[图4-11](#lipschitz)所示——在任何位置放置锥体时，线都不会进入锥体。换句话说，线在任何点上升或下降的速率都有限制。
- en: '![](Images/gdl2_0411.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0411.png)'
- en: 'Figure 4-11\. A Lipschitz continuous function (source: [Wikipedia](https://oreil.ly/Ki7ds))'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。利普希茨连续函数（来源：[维基百科](https://oreil.ly/Ki7ds)）
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For those who want to delve deeper into the mathematical rationale behind why
    the Wasserstein loss only works when this constraint is enforced, Jonathan Hui
    offers [an excellent explanation](https://oreil.ly/devy5).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想深入了解为什么只有在强制执行这个约束时，Wasserstein损失才有效的数学原理的人，Jonathan Hui提供了[一个优秀的解释](https://oreil.ly/devy5)。
- en: Enforcing the Lipschitz Constraint
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制利普希茨约束
- en: In the original WGAN paper, the authors show how it is possible to enforce the
    Lipschitz constraint by clipping the weights of the critic to lie within a small
    range, [–0.01, 0.01], after each training batch.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的WGAN论文中，作者展示了通过在每个训练批次后将评论家的权重剪辑到一个小范围[-0.01, 0.01]内来强制执行利普希茨约束的可能性。
- en: One of the criticisms of this approach is that the capacity of the critic to
    learn is greatly diminished, since we are clipping its weights. In fact, even
    in the original WGAN paper the authors write, “Weight clipping is a clearly terrible
    way to enforce a Lipschitz constraint.” A strong critic is pivotal to the success
    of a WGAN, since without accurate gradients, the generator cannot learn how to
    adapt its weights to produce better samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的批评之一是，评论家学习的能力大大降低，因为我们正在剪辑它的权重。事实上，即使在原始的WGAN论文中，作者们也写道，“权重剪辑显然是一种强制利普希茨约束的可怕方式。”强大的评论家对于WGAN的成功至关重要，因为没有准确的梯度，生成器无法学习如何调整其权重以生成更好的样本。
- en: Therefore, other researchers have looked for alternative ways to enforce the
    Lipschitz constraint and improve the capacity of the WGAN to learn complex features.
    One such method is the Wasserstein GAN with Gradient Penalty.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，其他研究人员寻找了其他方法来强制执行利普希茨约束，并提高WGAN学习复杂特征的能力。其中一种方法是带有梯度惩罚的Wasserstein GAN。
- en: In the paper introducing this variant,^([5](ch04.xhtml#idm45387019298976)) the
    authors show how the Lipschitz constraint can be enforced directly by including
    a *gradient penalty* term in the loss function for the critic that penalizes the
    model if the gradient norm deviates from 1\. This results in a far more stable
    training process.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入这种变体的论文中，作者展示了如何通过在评论家的损失函数中直接包含*梯度惩罚*项来强制执行利普希茨约束，如果梯度范数偏离1，模型将受到惩罚。这导致了一个更加稳定的训练过程。
- en: In the next section, we’ll see how to build this extra term into the loss function
    for our critic.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何将这个额外项构建到我们评论家的损失函数中。
- en: The Gradient Penalty Loss
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度惩罚损失
- en: '[Figure 4-12](#wgangp_critic) is a diagram of the training process for the
    critic of a WGAN-GP. If we compare this to the original discriminator training
    process from [Figure 4-5](#gan_bricks_training), we can see that the key addition
    is the gradient penalty loss included as part of the overall loss function, alongside
    the Wasserstein loss from the real and fake images.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-12](#wgangp_critic)是WGAN-GP评论家的训练过程的图表。如果我们将其与[图4-5](#gan_bricks_training)中原始鉴别器训练过程进行比较，我们可以看到的关键添加是梯度惩罚损失作为整体损失函数的一部分，与真实和虚假图像的Wasserstein损失一起。'
- en: '![](Images/gdl2_0412.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0412.png)'
- en: Figure 4-12\. The WGAN-GP critic training process
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。WGAN-GP评论家训练过程
- en: The gradient penalty loss measures the squared difference between the norm of
    the gradient of the predictions with respect to the input images and 1\. The model
    will naturally be inclined to find weights that ensure the gradient penalty term
    is minimized, thereby encouraging the model to conform to the Lipschitz constraint.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度惩罚损失衡量了预测梯度的范数与输入图像之间的平方差异和1之间的差异。模型自然倾向于找到确保梯度惩罚项最小化的权重，从而鼓励模型符合利普希茨约束。
- en: It is intractable to calculate this gradient everywhere during the training
    process, so instead the WGAN-GP evaluates the gradient at only a handful of points.
    To ensure a balanced mix, we use a set of interpolated images that lie at randomly
    chosen points along lines connecting the batch of real images to the batch of
    fake images pairwise, as shown in [Figure 4-13](#wgangp_randomweighting).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中无法计算每个地方的梯度，因此WGAN-GP只在少数几个点处评估梯度。为了确保平衡混合，我们使用一组插值图像，这些图像位于连接真实图像批次和虚假图像批次的线上随机选择的点上，如[图4-13](#wgangp_randomweighting)所示。
- en: '![](Images/gdl2_0413.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0413.png)'
- en: Figure 4-13\. Interpolating between images
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13。图像之间的插值
- en: In [Example 4-8](#gradient-penalty-loss-ex), we show how the gradient penalty
    is calculated in code.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例4-8](#gradient-penalty-loss-ex)中，我们展示了如何在代码中计算梯度惩罚。
- en: Example 4-8\. The gradient penalty loss function
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-8。梯度惩罚损失函数
- en: '[PRE7]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO4-1)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO4-1)'
- en: Each image in the batch gets a random number, between 0 and 1, stored as the
    vector `alpha`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 批次中的每个图像都会得到一个介于0和1之间的随机数，存储为向量`alpha`。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO4-2)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO4-2)'
- en: A set of interpolated images is calculated.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一组插值图像。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO4-3)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO4-3)'
- en: The critic is asked to score each of these interpolated images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 批评家被要求对这些插值图像进行评分。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO4-4)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO4-4)'
- en: The gradient of the predictions is calculated with respect to the input images.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入图像计算预测的梯度。
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO4-5)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO4-5)'
- en: The L2 norm of this vector is calculated.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这个向量的L2范数。
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO4-6)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO4-6)'
- en: The function returns the average squared distance between the L2 norm and 1.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回L2范数与1之间的平均平方距离。
- en: Training the WGAN-GP
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练WGAN-GP
- en: A key benefit of using the Wasserstein loss function is that we no longer need
    to worry about balancing the training of the critic and the generator—in fact,
    when using the Wasserstein loss, the critic must be trained to convergence before
    updating the generator, to ensure that the gradients for the generator update
    are accurate. This is in contrast to a standard GAN, where it is important not
    to let the discriminator get too strong.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Wasserstein损失函数的一个关键优势是我们不再需要担心平衡批评家和生成器的训练——事实上，当使用Wasserstein损失时，必须在更新生成器之前将批评家训练到收敛，以确保生成器更新的梯度准确。这与标准GAN相反，标准GAN中重要的是不要让鉴别器变得太强。
- en: Therefore, with Wasserstein GANs, we can simply train the critic several times
    between generator updates, to ensure it is close to convergence. A typical ratio
    used is three to five critic updates per generator update.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用Wasserstein GAN，我们可以简单地在生成器更新之间多次训练批评家，以确保它接近收敛。通常使用的比例是每次生成器更新三到五次批评家更新。
- en: We have now introduced both of the key concepts behind the WGAN-GP—the Wasserstein
    loss and the gradient penalty term that is included in the critic loss function.
    The training step of the WGAN model that incorporates all of these ideas is shown
    in [Example 4-9](#training-wgan-ex).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍了WGAN-GP背后的两个关键概念——Wasserstein损失和包含在批评家损失函数中的梯度惩罚项。包含所有这些想法的WGAN模型的训练步骤显示在[示例4-9](#training-wgan-ex)中。
- en: Example 4-9\. Training the WGAN-GP
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-9。训练WGAN-GP
- en: '[PRE8]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO5-1)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO5-1)'
- en: Perform three critic updates.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 执行三次批评家更新。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO5-2)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO5-2)'
- en: Calculate the Wasserstein loss for the critic—the difference between the average
    prediction for the fake images and the real images.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为批评家计算Wasserstein损失——虚假图像和真实图像的平均预测之间的差异。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO5-3)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO5-3)'
- en: Calculate the gradient penalty term (see [Example 4-8](#gradient-penalty-loss-ex)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度惩罚项（参见[示例4-8](#gradient-penalty-loss-ex)）。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO5-4)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO5-4)'
- en: The critic loss function is a weighted sum of the Wasserstein loss and the gradient
    penalty.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 批评家损失函数是Wasserstein损失和梯度惩罚的加权和。
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO5-5)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO5-5)'
- en: Update the weights of the critic.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 更新批评家的权重。
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO5-6)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO5-6)'
- en: Calculate the Wasserstein loss for the generator.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为生成器计算Wasserstein损失。
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO5-7)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO5-7)'
- en: Update the weights of the generator.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 更新生成器的权重。
- en: Batch Normalization in a WGAN-GP
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WGAN-GP中的批归一化
- en: One last consideration we should note before training a WGAN-GP is that batch
    normalization shouldn’t be used in the critic. This is because batch normalization
    creates correlation between images in the same batch, which makes the gradient
    penalty loss less effective. Experiments have shown that WGAN-GPs can still produce
    excellent results even without batch normalization in the critic.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练WGAN-GP之前我们应该注意的最后一个考虑是批归一化不应该在批评家中使用。这是因为批归一化会在同一批次中的图像之间创建相关性，使得梯度惩罚损失效果不佳。实验证明，即使在批评家中没有批归一化，WGAN-GP仍然可以产生出色的结果。
- en: 'We have now covered all of the key differences between a standard GAN and a
    WGAN-GP. To recap:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了标准GAN和WGAN-GP之间的所有关键区别。回顾一下：
- en: A WGAN-GP uses the Wasserstein loss.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WGAN-GP使用Wasserstein损失。
- en: The WGAN-GP is trained using labels of 1 for real and –1 for fake.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WGAN-GP使用标签1表示真实和-1表示虚假进行训练。
- en: There is no sigmoid activation in the final layer of the critic.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在批评家的最后一层没有Sigmoid激活。
- en: Include a gradient penalty term in the loss function for the critic.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评论者的损失函数中包含一个梯度惩罚项。
- en: Train the critic multiple times for each update of the generator.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成器进行多次更新之前多次训练评论者。
- en: There are no batch normalization layers in the critic.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论中没有批量归一化层。
- en: Analysis of the WGAN-GP
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WGAN-GP的分析
- en: Let’s take a look at some example outputs from the generator, after 25 epochs
    of training ([Figure 4-14](#wgangp_examples)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下生成器在训练25个时期后的一些示例输出（[图4-14](#wgangp_examples)）。
- en: '![](Images/gdl2_0414.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0414.png)'
- en: Figure 4-14\. WGAN-GP face examples
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14\. WGAN-GP面部示例
- en: The model has learned the significant high-level attributes of a face, and there
    is no sign of mode collapse.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已经学会了面部的重要高级属性，没有出现模式坍塌的迹象。
- en: We can also see how the loss functions of the model evolve over time ([Figure 4-15](#wgangp_losses))—the
    loss functions of both the critic and generator are highly stable and convergent.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到模型的损失函数随时间的演变（[图4-15](#wgangp_losses)）—评论者和生成器的损失函数都非常稳定和收敛。
- en: If we compare the WGAN-GP output to the VAE output from the previous chapter,
    we can see that the GAN images are generally sharper—especially the definition
    between the hair and the background. This is true in general; VAEs tend to produce
    softer images that blur color boundaries, whereas GANs are known to produce sharper,
    more well-defined images.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将WGAN-GP的输出与上一章的VAE输出进行比较，我们可以看到GAN图像通常更清晰—特别是头发和背景之间的定义。这在一般情况下是正确的；VAE倾向于产生模糊颜色边界的柔和图像，而众所周知，GAN倾向于产生更清晰、更明确定义的图像。
- en: '![](Images/gdl2_0415.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0415.png)'
- en: 'Figure 4-15\. WGAN-GP loss curves: the critic loss (`epoch_c_loss`) is broken
    down into the Wasserstein loss (`epoch_c_wass`) and the gradient penalty loss
    (`epoch_c_gp`)'
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. WGAN-GP损失曲线：评论者损失（`epoch_c_loss`）分解为Wasserstein损失（`epoch_c_wass`）和梯度惩罚损失（`epoch_c_gp`）
- en: It is also true that GANs are generally more difficult to train than VAEs and
    take longer to reach a satisfactory quality. However, many state-of-the-art generative
    models today are GAN-based, as the rewards for training large-scale GANs on GPUs
    over a longer period of time are significant.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，GAN通常比VAE更难训练，并需要更长的时间达到令人满意的质量。然而，今天许多最先进的生成模型都是基于GAN的，因为在GPU上训练大规模GAN并花费更长时间的回报是显著的。
- en: Conditional GAN (CGAN)
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件GAN（CGAN）
- en: So far in this chapter, we have built GANs that are able to generate realistic
    images from a given training set. However, we haven’t been able to control the
    type of image we would like to generate—for example, a male or female face, or
    a large or small brick. We can sample a random point from the latent space, but
    we do not have the ability to easily understand what kind of image will be produced
    given the choice of latent variable.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经构建了能够从给定的训练集生成逼真图像的GAN。然而，我们无法控制我们想要生成的图像类型—例如，男性或女性的面孔，或大砖或小砖。我们可以从潜在空间中随机采样一个点，但我们无法轻松地了解在选择潜在变量的情况下将产生什么样的图像。
- en: In the final part of this chapter we shall turn our attention to building a
    GAN where we are able to control the output—a so called *conditional GAN*. This
    idea, first introduced in “Conditional Generative Adversarial Nets” by Mirza and
    Osindero in 2014,^([6](ch04.xhtml#idm45387018921312)) is a relatively simple extension
    to the GAN architecture.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将把注意力转向构建一个能够控制输出的GAN—所谓的*条件GAN*。这个想法最早是在2014年由Mirza和Osindero在“条件生成对抗网络”中首次提出的，是对GAN架构的一个相对简单的扩展。
- en: Running the Code for This Example
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/03_cgan/cgan.ipynb*
    in the book repository.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的*notebooks/04_gan/03_cgan/cgan.ipynb*的Jupyter笔记本中找到。
- en: The code has been adapted from the excellent [CGAN tutorial](https://oreil.ly/Ey11I)
    created by Sayak Paul, available on the Keras website.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经从由Sayak Paul创建的优秀[CGAN教程](https://oreil.ly/Ey11I)中调整，该教程可在Keras网站上找到。
- en: CGAN Architecture
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CGAN架构
- en: In this example, we will condition our CGAN on the *blond hair* attribute of
    the faces dataset. That is, we will be able to explicitly specify whether we want
    to generate an image with blond hair or not. This label is provided as part of
    the CelebA dataset.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在面部数据集的*金发*属性上对我们的CGAN进行条件化。也就是说，我们可以明确指定我们是否想要生成一张有金发的图像。这个标签作为CelebA数据集的一部分提供。
- en: The high-level CGAN architecture is shown in [Figure 4-16](#cgan_architecture).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 高级CGAN架构如[图4-16](#cgan_architecture)所示。
- en: '![](Images/gdl2_0416.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0416.png)'
- en: Figure 4-16\. Inputs and outputs of the generator and critic in a CGAN
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16\. CGAN中生成器和评论者的输入和输出
- en: The key difference between a standard GAN and a CGAN is that in a CGAN we pass
    in extra information to the generator and critic relating to the label. In the
    generator, this is simply appended to the latent space sample as a one-hot encoded
    vector. In the critic, we add the label information as extra channels to the RGB
    image. We do this by repeating the one-hot encoded vector to fill the same shape
    as the input images.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 标准GAN和CGAN之间的关键区别在于，在CGAN中，我们向生成器和评论者传递与标签相关的额外信息。在生成器中，这只是作为一个独热编码向潜在空间样本附加。在评论者中，我们将标签信息作为额外通道添加到RGB图像中。我们通过重复独热编码向量来填充与输入图像相同形状的方式来实现这一点。
- en: CGANs work because the critic now has access to extra information regarding
    the content of the image, so the generator must ensure that its output agrees
    with the provided label, in order to keep fooling the critic. If the generator
    produced perfect images that disagreed with the image label the critic would be
    able to tell that they were fake simply because the images and labels did not
    match.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN之所以有效是因为评论者现在可以访问有关图像内容的额外信息，因此生成器必须确保其输出与提供的标签一致，以继续愚弄评论者。如果生成器生成了与图像标签不符的完美图像，评论者将能够简单地判断它们是假的，因为图像和标签不匹配。
- en: Tip
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In our example, our one-hot encoded label will have length 2, because there
    are two classes (Blonde and Not Blond). However, you can have as many labels as
    you like—for example, you could train a CGAN on the Fashion-MNIST dataset to output
    one of the 10 different fashion items, by incorporating a one-hot encoded label
    vector of length 10 into the input of the generator and 10 additional one-hot
    encoded label channels into the input of the critic.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们的独热编码标签长度为2，因为有两个类别（金发和非金发）。但是，您可以有任意数量的标签——例如，您可以在Fashion-MNIST数据集上训练一个CGAN，以输出10种不同的时尚物品之一，通过将长度为10的独热编码标签向量合并到生成器的输入中，并将10个额外的独热编码标签通道合并到评论家的输入中。
- en: The only change we need to make to the architecture is to concatenate the label
    information to the existing inputs of the generator and the critic, as shown in
    [Example 4-10](#cgan_input_shapes).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对架构进行的唯一更改是将标签信息连接到生成器和评论家的现有输入中，如[示例4-10](#cgan_input_shapes)所示。
- en: Example 4-10\. Input layers in the CGAN
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-10。CGAN中的输入层
- en: '[PRE9]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO6-1)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO6-1)'
- en: The image channels and label channels are passed in separately to the critic
    and concatenated.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通道和标签通道分别传递给评论家并连接。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO6-2)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO6-2)'
- en: The latent vector and the label classes are passed in separately to the generator
    and concatenated before being reshaped.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在向量和标签类别分别传递给生成器，并在重塑之前连接。
- en: Training the CGAN
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练CGAN
- en: We must also make some changes to the `train_step` of the CGAN to match the
    new input formats of the generator and critic, as shown in [Example 4-11](#cgan_train_step).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对CGAN的`train_step`进行一些更改，以匹配生成器和评论家的新输入格式，如[示例4-11](#cgan_train_step)所示。
- en: Example 4-11\. The `train_step` of the CGAN
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-11。CGAN的`train_step`
- en: '[PRE10]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO7-1)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO7-1)'
- en: The images and labels are unpacked from the input data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和标签从输入数据中解压缩。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO7-2)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO7-2)'
- en: The one-hot encoded vectors are expanded to one-hot encoded images that have
    the same spatial size as the input images (64 × 64).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码向量被扩展为具有与输入图像相同空间大小（64×64）的独热编码图像。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO7-3)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO7-3)'
- en: The generator is now fed with a list of two inputs—the random latent vectors
    and the one-hot encoded label vectors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，生成器被提供了两个输入的列表——随机潜在向量和独热编码标签向量。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO7-4)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO7-4)'
- en: The critic is now fed with a list of two inputs—the fake/real images and the
    one-hot encoded label channels.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家现在被提供了两个输入的列表——假/真实图像和独热编码标签通道。
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO7-5)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO7-5)'
- en: The gradient penalty function also requires the one-hot encoded label channels
    to be passed through as it uses the critic.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度惩罚函数还需要将独热编码标签通道传递给评论家，因为它使用评论家。
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO7-6)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO7-6)'
- en: The changes made to the critic training step also apply to the generator training
    step.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对评论家培训步骤所做的更改也适用于生成器训练步骤。
- en: Analysis of the CGAN
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CGAN的分析
- en: We can control the CGAN output by passing a particular one-hot encoded label
    into the input of the generator. For example, to generate a face with nonblond
    hair, we pass in the vector `[1, 0]`. To generate a face with blond hair, we pass
    in the vector `[0, 1]`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将特定的独热编码标签传递到生成器的输入来控制CGAN的输出。例如，要生成一个头发不是金色的脸，我们传入向量`[1, 0]`。要生成一个金发的脸，我们传入向量`[0,
    1]`。
- en: The output from the CGAN can be seen in [Figure 4-17](#cgan_output). Here, we
    keep the random latent vectors the same across the examples and change only the
    conditional label vector. It is clear that the CGAN has learned to use the label
    vector to control only the hair color attribute of the images. It is impressive
    that the rest of the image barely changes—this is proof that GANs are able to
    organize points in the latent space in such a way that individual features can
    be decoupled from each other.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[图4-17](#cgan_output)中看到CGAN的输出。在这里，我们保持示例中的随机潜在向量相同，只改变条件标签向量。很明显，CGAN已经学会使用标签向量来控制图像的头发颜色属性。令人印象深刻的是，图像的其余部分几乎没有改变——这证明了GAN能够以这样一种方式组织潜在空间中的点，以便将各个特征解耦。
- en: '![](Images/gdl2_0417.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0417.png)'
- en: Figure 4-17\. Output from the CGAN when the *Blond* and *Not Blond* vectors
    are appended to the latent sample
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-17。当*Blond*和*Not Blond*向量附加到潜在样本时，CGAN的输出
- en: Tip
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If labels are available for your dataset, it is generally a good idea to include
    them as input to your GAN even if you do not necessarily need to condition the
    generated output on the label, as they tend to improve the quality of images generated.
    You can think of the labels as just a highly informative extension to the pixel
    input.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集中有标签，通常最好将它们包含在GAN的输入中，即使您不一定需要将生成的输出条件化为标签，因为它们往往会提高生成的图像质量。您可以将标签视为像素输入的高度信息性扩展。
- en: Summary
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we explored three different generative adversarial network
    (GAN) models: the deep convolutional GAN (DCGAN), the more sophisticated Wasserstein
    GAN with Gradient Penalty (WGAN-GP), and the conditional GAN (CGAN).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了三种不同的生成对抗网络（GAN）模型：深度卷积GAN（DCGAN）、更复杂的带有梯度惩罚的Wasserstein GAN（WGAN-GP）和条件GAN（CGAN）。
- en: All GANs are characterized by a generator versus discriminator (or critic) architecture,
    with the discriminator trying to “spot the difference” between real and fake images
    and the generator aiming to fool the discriminator. By balancing how these two
    adversaries are trained, the GAN generator can gradually learn how to produce
    similar observations to those in the training set.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 所有GAN都以生成器与鉴别器（或评论家）架构为特征，鉴别器试图“发现”真假图像之间的差异，生成器旨在欺骗鉴别器。通过平衡这两个对手的训练方式，GAN生成器可以逐渐学习如何产生与训练集中的观察结果相似的图像。
- en: We first saw how to train a DCGAN to generate images of toy bricks. It was able
    to learn how to realistically represent 3D objects as images, including accurate
    representations of shadow, shape, and texture. We also explored the different
    ways in which GAN training can fail, including mode collapse and vanishing gradients.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先看到如何训练DCGAN生成玩具积木的图像。它能够学习如何以图像形式真实地表示3D物体，包括阴影、形状和纹理的准确表示。我们还探讨了GAN训练可能失败的不同方式，包括模式坍塌和梯度消失。
- en: We then explored how the Wasserstein loss function remedied many of these problems
    and made GAN training more predictable and reliable. The WGAN-GP places the 1-Lipschitz
    requirement at the heart of the training process by including a term in the loss
    function to pull the gradient norm toward 1.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了Wasserstein损失函数如何纠正了许多问题，并使GAN训练更加可预测和可靠。WGAN-GP通过在损失函数中包含一个术语来将1-Lipschitz要求置于训练过程的核心，以将梯度范数拉向1。
- en: We applied the WGAN-GP to the problem of face generation and saw how by simply
    choosing points from a standard normal distribution, we can generate new faces.
    This sampling process is very similar to a VAE, though the faces produced by a
    GAN are quite different—often sharper, with greater distinction between different
    parts of the image.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将WGAN-GP应用于人脸生成问题，并看到通过简单地从标准正态分布中选择点，我们可以生成新的人脸。这个采样过程与VAE非常相似，尽管GAN生成的人脸通常更加清晰，图像的不同部分之间的区别更大。
- en: Finally, we built a CGAN that allowed us to control the type of image that is
    generated. This works by passing in the label as input to the critic and generator,
    thereby giving the network the additional information it needs in order to condition
    the generated output on a given label.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个CGAN，使我们能够控制生成的图像类型。这通过将标签作为输入传递给评论家和生成器来实现，从而为网络提供了所需的额外信息，以便根据给定的标签对生成的输出进行条件化。
- en: Overall, we have seen how the GAN framework is extremely flexible and able to
    be adapted to many interesting problem domains. In particular, GANs have driven
    significant progress in the field of image generation with many interesting extensions
    to the underlying framework, as we shall see in [Chapter 10](ch10.xhtml#chapter_image_generation).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们已经看到GAN框架非常灵活，能够适应许多有趣的问题领域。特别是，GAN已经在图像生成领域取得了显著进展，有许多有趣的扩展到基础框架中，我们将在[第10章](ch10.xhtml#chapter_image_generation)中看到。
- en: In the next chapter, we will explore a different family of generative model
    that is ideal for modeling sequential data—autoregressive models.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一种适合建模序列数据的不同生成模型家族——自回归模型。
- en: ^([1](ch04.xhtml#idm45387021611344-marker)) Ian J. Goodfellow et al., “Generative
    Adversarial Nets,” June 10, 2014, [*https://arxiv.org/abs/1406.2661*](https://arxiv.org/abs/1406.2661)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45387021611344-marker)) Ian J. Goodfellow等人，“生成对抗网络”，2014年6月10日，[*https://arxiv.org/abs/1406.2661*](https://arxiv.org/abs/1406.2661)
- en: ^([2](ch04.xhtml#idm45387021585984-marker)) Alec Radford et al., “Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks,”
    January 7, 2016, [*https://arxiv.org/abs/1511.06434*](https://arxiv.org/abs/1511.06434).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm45387021585984-marker)) Alec Radford等人，“使用深度卷积生成对抗网络进行无监督表示学习”，2016年1月7日，[*https://arxiv.org/abs/1511.06434*](https://arxiv.org/abs/1511.06434)。
- en: ^([3](ch04.xhtml#idm45387021101472-marker)) Augustus Odena et al., “Deconvolution
    and Checkerboard Artifacts,” October 17, 2016, [*https://distill.pub/2016/deconv-checkerboard*](https://distill.pub/2016/deconv-checkerboard).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#idm45387021101472-marker)) Augustus Odena等人，“反卷积和棋盘伪影”，2016年10月17日，[*https://distill.pub/2016/deconv-checkerboard*](https://distill.pub/2016/deconv-checkerboard)。
- en: ^([4](ch04.xhtml#idm45387021016704-marker)) Martin Arjovsky et al., “Wasserstein
    GAN,” January 26, 2017, [*https://arxiv.org/abs/1701.07875*](https://arxiv.org/abs/1701.07875).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#idm45387021016704-marker)) Martin Arjovsky等人，“Wasserstein GAN”，2017年1月26日，[*https://arxiv.org/abs/1701.07875*](https://arxiv.org/abs/1701.07875)。
- en: ^([5](ch04.xhtml#idm45387019298976-marker)) Ishaan Gulrajani et al., “Improved
    Training of Wasserstein GANs,” March 31, 2017, [*https://arxiv.org/abs/1704.00028*](https://arxiv.org/abs/1704.00028).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#idm45387019298976-marker)) Ishaan Gulrajani等人，“改进的Wasserstein
    GANs训练”，2017年3月31日，[*https://arxiv.org/abs/1704.00028*](https://arxiv.org/abs/1704.00028)。
- en: ^([6](ch04.xhtml#idm45387018921312-marker)) Mehdi Mirza and Simon Osindero,
    “Conditional Generative Adversarial Nets,” November 6, 2014, [*https://arxiv.org/abs/1411.1784*](https://arxiv.org/abs/1411.1784).`
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#idm45387018921312-marker)) Mehdi Mirza和Simon Osindero，“条件生成对抗网络”，2014年11月6日，[*https://arxiv.org/abs/1411.1784*](https://arxiv.org/abs/1411.1784)。
