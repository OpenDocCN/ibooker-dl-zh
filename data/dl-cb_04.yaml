- en: Chapter 4\. Building a Recommender System Based on Outgoing Wikipedia Links
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。基于维基百科外部链接构建推荐系统
- en: Recommender systems are traditionally trained on previously collected ratings
    from users. We want to predict ratings from users, so starting with historical
    ratings feels like a natural fit. However, this requires us to have a substantial
    set of ratings before we can get going and it doesn’t allow us to do a good job
    on new items for which we don’t have ratings yet. Moreover, we deliberately ignore
    the metainformation that we have on items.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统传统上是根据用户先前收集的评分进行训练的。我们希望预测用户的评分，因此从历史评分开始似乎是一个自然的选择。然而，这要求我们在开始之前有一个大量的评分集，并且不允许我们对尚未评分的新项目做出良好的工作。此外，我们故意忽略了我们对项目的元信息。
- en: In this chapter you’ll explore how to build a simple movie recommender system
    based solely on outgoing Wikipedia links. You’ll start by extracting a training
    set from Wikipedia and then train embeddings based on these links. You’ll then
    implement a simple support vector machine classifier to give recommendations.
    Finally, you’ll explore how you can use your newly trained embeddings to predict
    review scores for the movies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将探索如何仅基于维基百科的外部链接构建一个简单的电影推荐系统。您将首先从维基百科中提取一个训练集，然后基于这些链接训练嵌入。然后，您将实现一个简单的支持向量机分类器来提供建议。最后，您将探索如何使用新训练的嵌入来预测电影的评分。
- en: 'The code in this chapter can be found in these notebooks:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码可以在这些笔记本中找到：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4.1 Collecting the Data
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.1 收集数据
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to obtain a dataset for training for a specific domain, like movies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望获得一个特定领域的训练数据集，比如电影。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Parse a Wikipedia dump and extract only the pages that are movies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 解析维基百科转储文件并仅提取电影页面。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The code in this recipe shows how to fetch and extract training data from Wikipedia,
    which is a very useful skill. However, downloading and processing a full dump
    takes a rather long time. The *data* directory of the notebook folder contains
    the top 10,000 movies pre-extracted that we’ll use in the rest of the chapter,
    so you don’t need to run the steps in this recipe.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方中的代码展示了如何从维基百科获取和提取训练数据，这是一个非常有用的技能。然而，下载和处理完整的转储文件需要相当长的时间。笔记本文件夹的*data*目录包含了预先提取的前10000部电影，我们将在本章的其余部分中使用，因此您不需要运行本配方中的步骤。
- en: 'Let’s start by downloading a recent dump from Wikipedia. You can easily do
    this using your favorite browser, and if you don’t need the very latest version,
    you should probably pick a nearby mirror. But you can also do it programmatically.
    Here’s how to get the latest dump pages:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从维基百科下载最新的转储文件开始。您可以使用您喜欢的浏览器轻松完成这一操作，如果您不需要最新版本，您可能应该选择附近的镜像。但您也可以通过编程方式完成。以下是获取最新转储页面的方法：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll now go through the dumps and find the newest one that has actually finished
    processing:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将浏览转储文件，并找到最新的已完成处理的文件：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note the sleep to stay under the rate limiting of Wikipedia. Now let’s fetch
    the dump:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意睡眠以保持在维基百科的速率限制之下。现在让我们获取转储文件：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dump we retrieved is a bz2-compressed XML file. We’ll use `sax` to parse
    the Wikipedia XML. We’re interested in the `<title>` and the `<page>` tags so
    our `Content​Handler` looks like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索到的转储文件是一个bz2压缩的XML文件。我们将使用`sax`来解析维基百科的XML。我们对`<title>`和`<page>`标签感兴趣，因此我们的`Content​Handler`看起来像这样：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For each `<page>` tag this collects the contents of the title and of the text
    into the `self._values` dictionary and calls `process_article` with the collected
    values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`<page>`标签，这将收集标题和文本内容到`self._values`字典中，并使用收集到的值调用`process_article`。
- en: 'Although Wikipedia started out as a hyperlinked text-based encyclopedia, over
    the years it has developed into a more structured data dump. One way this is done
    is by having pages link back to so-called *category pages*. These links function
    as tags. The page for the film *One Flew Over the Cuckoo’s Nest* links to the
    category page “1975 films,” so we know it is a movie from 1975\. Unfortunately,
    there is no such thing as a category page for just movies. Fortunately, there
    is a better way: Wikipedia templates.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管维基百科最初是一个超链接文本型百科全书，但多年来它已经发展成一个更结构化的数据转储。其中一种方法是让页面链接回所谓的*分类页面*。这些链接起到标签的作用。电影《飞越疯人院》的页面链接到“1975年电影”分类页面，因此我们知道这是一部1975年的电影。不幸的是，并没有仅仅针对电影的分类页面。幸运的是，有一个更好的方法：维基百科模板。
- en: Templates started out as a way to make sure that pages that contain similar
    information have that information rendered in the same way. The “infobox” template
    is very useful for data processing. Not only does it contain a list of key/value
    pairs applicable to the subject of the page, but it also has a type. One of the
    types is “film,” which makes the task of extracting all movies a lot easier.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模板最初是一种确保包含相似信息的页面以相同方式呈现该信息的方法。“信息框”模板对数据处理非常有用。它不仅包含适用于页面主题的键/值对列表，还有一个类型。其中之一是“电影”，这使得提取所有电影的任务变得更加容易。
- en: 'For each movie we want to extract the name, the outgoing links and, just because
    we can, the properties stored in the infobox. The aptly named `mwparserfromhell`
    does a decent job of parsing Wikipedia:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每部电影，我们想要提取名称、外部链接以及，仅仅是因为我们可以，存储在信息框中的属性。名为`mwparserfromhell`的工具在解析维基百科时表现得相当不错：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now feed the bzipped dump into the parser:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将bzipped转储文件输入解析器：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, let’s save the results so next time we need the data, we don’t have
    to process for hours:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们保存结果，这样下次我们需要数据时就不必再处理几个小时：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Wikipedia is not only a great resource to answer questions about almost any
    area of human knowledge; it also is the starting point for many deep learning
    experiments. Knowing how to parse the dumps and extract the relevant bits is a
    skill useful for many projects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科不仅是回答几乎任何人类知识领域问题的重要资源；它也是许多深度学习实验的起点。了解如何解析转储文件并提取相关部分是许多项目中有用的技能。
- en: 'At 13 GB the dumps are sizeable downloads. Parsing the Wikipedia markup language
    comes with its own challenges: the language has grown organically over the years
    and doesn’t seem to have a strong underlying design. But with today’s fast connections
    and some great open source libraries to help with the parsing, it has all become
    quite doable.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 13 GB的数据转储是相当大的下载。解析维基百科标记语言带来了自己的挑战：这种语言多年来有机地发展，似乎没有强大的基础设计。但是随着今天快速的连接和一些出色的开源库来帮助解析，这一切都变得相当可行。
- en: In some situations the Wikipedia API might be more appropriate. This REST interface
    to Wikipedia allows you to search and query in a number of powerful ways and only
    fetch the articles that you need. Getting all the movies that way would take a
    long time given the rate limiting, but for smaller domains it is an option.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，维基百科API可能更合适。这个对维基百科的REST接口允许您以多种强大的方式搜索和查询，并且只获取您需要的文章。考虑到速率限制，以这种方式获取所有电影将需要很长时间，但对于较小的领域来说，这是一个选择。
- en: If you end up parsing Wikipedia for many projects, it might be worth it to first
    import the dump into a database like Postgres so you can query the dataset directly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您最终要为许多项目解析维基百科，那么首先将转储导入到像Postgres这样的数据库中可能是值得的，这样您就可以直接查询数据集。
- en: 4.2 Training Movie Embeddings
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.2 训练电影嵌入
- en: Problem
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you use link data between entities to produce suggestions like “If you
    liked this, you might also be interested in that”?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用实体之间的链接数据生成建议，比如“如果您喜欢这个，您可能也对那个感兴趣”？
- en: Solution
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train embeddings using some metainformation as connectors. This recipe builds
    on the previous one by using the movies and links extracted there. To make the
    dataset a bit smaller and less noisy, we’ll work with only the top 10,000 movies
    determined by popularity on Wikipedia.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些元信息作为连接器来训练嵌入。这个示例建立在之前的示例之上，使用了那里提取的电影和链接。为了使数据集变得更小且更少噪音，我们将仅使用维基百科上受欢迎程度确定的前10,000部电影。
- en: We’ll treat the outgoing links as the connectors. The intuition here is that
    movies that link to the same page are similar. They might have the same director
    or be of the same genre. As the model trains, it learns not only which movies
    are similar, but also which links are similar. This way it can generalize and
    discover that a link to the year 1978 has a similar meaning as a link to 1979,
    which in turn helps with movie similarity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将外链视为连接器。这里的直觉是链接到同一页面的电影是相似的。它们可能有相同的导演或属于相同的类型。随着模型的训练，它不仅学习哪些电影相似，还学习哪些链接相似。这样它可以泛化并发现指向1978年的链接与指向1979年的链接具有相似的含义，从而有助于电影的相似性。
- en: 'We’ll start by counting the outgoing links as a quick way to see whether what
    we have is reasonable:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从计算外链开始，这是一个快速查看我们是否合理的方法：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our model’s task is to determine whether a certain link can be found on the
    Wikipedia page of a movie, so we need to feed it labeled examples of matches and
    nonmatches. We’ll keep only links that occur at least three times and build a
    list of all valid (link, movie) pairs, which we’ll store for quick lookups later.
    We keep the same handy as a set for quick lookups later:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的任务是确定某个链接是否可以在电影的维基百科页面上找到，因此我们需要提供标记的匹配和不匹配示例。我们只保留至少出现三次的链接，并构建所有有效的（链接，电影）对的列表，我们将其存储以供以后快速查找。我们将保留相同的便于以后快速查找：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are now ready to introduce our model. Schematically, we take both the `link_id`
    and the `movie_id` as a number and feed those into their respective embedding
    layers. The embedding layer will allocate a vector of `embedding_size` for each
    possible input. We then set the dot product of these two vectors to be the output
    of our model. The model will learn weights such that this dot product will be
    close to the label. These weights will then project movies and links into a space
    such that movies that are similar end up in a similar location:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备介绍我们的模型。从图表上看，我们将`link_id`和`movie_id`作为数字输入到它们各自的嵌入层中。嵌入层将为每个可能的输入分配一个大小为`embedding_size`的向量。然后我们将这两个向量的点积设置为我们模型的输出。模型将学习权重，使得这个点积接近标签。然后这些权重将把电影和链接投影到一个空间中，使得相似的电影最终位于相似的位置：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’ll feed the model using a generator. The generator yields batches of data
    made up of positive and negative examples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用生成器来喂养模型。生成器产生由正样本和负样本组成的数据批次。
- en: 'We sample the positive samples from the pairs array and then fill it up with
    negative examples. The negative examples are randomly picked and we make sure
    they are not in the `pairs_set`. We then return the data in a format that our
    network expects, an input/output tuple:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从对数组中的正样本进行采样，然后用负样本填充它。负样本是随机选择的，并确保它们不在`pairs_set`中。然后我们以我们的网络期望的格式返回数据，即输入/输出元组：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Time to train the model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的时间：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training times will depend on your hardware, but if you start with the 10,000
    movie dataset they should be fairly short, even on a laptop without GPU acceleration.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间将取决于您的硬件，但如果您从10,000部电影数据集开始，即使在没有GPU加速的笔记本电脑上，训练时间也应该相当短。
- en: 'We can now extract the movie embeddings from our model by accessing the weights
    of the `movie_embedding` layer. We normalize them so we can use the dot product
    as an approximation of the cosine similarity:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过访问`movie_embedding`层的权重从我们的模型中提取电影嵌入。我们对它们进行归一化，以便我们可以使用点积作为余弦相似度的近似：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let’s see if the embeddings make some sense:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看嵌入是否有些意义：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Discussion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Embeddings are a useful technique, and not just for words. In this recipe we’ve
    trained a simple network and produced embeddings for movies with reasonable results.
    This technique can be applied any time we have a way to connect items. In this
    case we used the outgoing Wikipedia links, but we could also use incoming links
    or the words that appear on the page.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种有用的技术，不仅适用于单词。在这个示例中，我们训练了一个简单的网络，并为电影生成了嵌入，取得了合理的结果。这种技术可以应用于任何我们有办法连接项目的时间。在这种情况下，我们使用了维基百科的外链，但我们也可以使用内链或页面上出现的单词。
- en: The model we trained here is extremely simple. All we do is ask it to come up
    with an embedding space such that the combination of the vector for the movie
    and the vector for the link can be used to predict whether or not they will co-occur.
    This forces the network to project movies into a space such that similar movies
    end up in a similar location. We can use this space to find similar movies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里训练的模型非常简单。我们只需要让它提供一个嵌入空间，使得电影的向量和链接的向量的组合可以用来预测它们是否会共同出现。这迫使网络将电影投影到一个空间中，使得相似的电影最终位于相似的位置。我们可以使用这个空间来找到相似的电影。
- en: In the Word2vec model we use the context of a word to predict the word. In the
    example of this recipe we don’t use the context of the link. For outgoing links
    it doesn’t seem like a particularly useful signal, but if we were using incoming
    links, it might have made sense. Pages linking to movies do this in a certain
    order, and we could use the context of the links to improve our embedding.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在Word2vec模型中，我们使用一个词的上下文来预测这个词。在这个示例中，我们不使用链接的上下文。对于外部链接来说，这似乎不是一个特别有用的信号，但如果我们使用的是内部链接，这可能是有意义的。链接到电影的页面以一定的顺序进行链接，我们可以利用链接的上下文来改进我们的嵌入。
- en: Alternatively, we could use the actual Word2vec code and run it over any of
    the pages that link to movies, but keep the links to movies as special tokens.
    This would then create a mixed movie and word embedding space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用实际的Word2vec代码，并在链接到电影的任何页面上运行它，但保留电影链接作为特殊标记。这样就会创建一个混合的电影和单词嵌入空间。
- en: 4.3 Building a Movie Recommender
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.3 建立电影推荐系统
- en: Problem
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you build a recommender system based on embeddings?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如何基于嵌入构建推荐系统？
- en: Solution
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a support vector machine to separate the positively ranked items from the
    negatively ranked items.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用支持向量机将排名靠前的项目与排名靠后的项目分开。
- en: 'The previous recipe let us cluster movies and make suggestions like “If you
    liked *Rogue One*, you should also check out *Interstellar*.” In a typical recommender
    system we want to show suggestions based on a series of movies that the user has
    rated. As we did in [Chapter 3](ch03.html#word_embeddings), we can use an SVM
    to do just this. Let’s take the best and worst movies according to *Rolling Stone*
    from 2015 and pretend they are user ratings:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法让我们对电影进行聚类，并提出建议，比如“如果你喜欢《侠盗一号》，你也应该看看《星际穿越》。”在典型的推荐系统中，我们希望根据用户评分的一系列电影来显示建议。就像我们在[第3章](ch03.html#word_embeddings)中所做的那样，我们可以使用SVM来做到这一点。让我们按照*滚石*杂志2015年评选的最佳和最差电影，并假装它们是用户评分：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Constructing and training a simple SVM classifier based on this is easy:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此构建和训练一个简单的SVM分类器很容易：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can now run the new classifier over all the movies in our dataset and print
    the best five and the worst five:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行新的分类器，打印出数据集中所有电影中最好的五部和最差的五部：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Discussion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As we saw in the previous chapter, we can use support vector machines to efficiently
    construct a classifier that distinguishes between two classes. In this case, we
    have it distinguish between good movies and bad movies based on the embeddings
    that we have previously learned.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，我们可以使用支持向量机高效地构建一个区分两个类别的分类器。在这种情况下，我们让它根据我们先前学习到的嵌入来区分好电影和坏电影。
- en: Since an SVM finds one or more hyperplanes that separate the “good” examples
    from the “bad” examples, we can use this as the personalization function—the movies
    that are the furthest from the separating hyperplane and on the right side are
    the movies that should be liked best.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于支持向量机找到一个或多个超平面来将“好”的示例与“坏”的示例分开，我们可以将其用作个性化功能——距离分隔超平面最远且在右侧的电影应该是最受喜爱的电影。
- en: 4.4 Predicting Simple Movie Properties
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.4 预测简单的电影属性
- en: Problem
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to predict simple movie properties, like Rotten Tomatoes ratings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要预测简单的电影属性，比如烂番茄评分。
- en: Solution
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a linear regression model on the learned vectors of the embedding model
    to predict movie properties.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习到的嵌入模型的向量进行线性回归模型，以预测电影属性。
- en: 'Let’s try this for Rotten Tomatoes ratings. Luckily they are already present
    in our data in `movie[-2]` as a string of the form `*N*%`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下这个方法来处理烂番茄评分。幸运的是，它们已经以`movie[-2]`的形式作为字符串存在于我们的数据中，形式为`*N*%`：
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This should get us data for about half our movies. Let’s train on the first
    80%:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该为我们大约一半的电影提供数据。让我们在前80%的数据上进行训练：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let’s see how we’re doing on the last 20%:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们在最后20%的进展如何：
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'That looks really impressive! But while it is a testament to how effective
    linear regression can be, there is an issue with our data that makes predicting
    the Rotten Tomatoes score easier: we’ve been training on the top 10,000 movies,
    and while popular movies aren’t always better, on average they do get better ratings.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来真的很令人印象深刻！但虽然这证明了线性回归的有效性，但我们的数据存在一个问题，使得预测烂番茄评分变得更容易：我们一直在训练前10000部电影，而热门电影并不总是更好，但平均来说它们得到更高的评分。
- en: 'We can get an idea of how well we’re doing by comparing our predictions with
    just always predicting the average score:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将我们的预测与始终预测平均分数进行比较，我们可以大致了解我们的表现如何：
- en: '[PRE25]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our model does perform quite a bit better, but the underlying data makes it
    easy to produce a reasonable result.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型确实表现得更好一些，但基础数据使得产生一个合理的结果变得容易。
- en: Discussion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Complex problems often need complex solutions, and deep learning can definitely
    give us those. However, starting with the simplest thing that could possibly work
    is often a good approach. It gets us started quickly and gives us an idea of whether
    we’re looking in the right direction: if the simple model doesn’t produce any
    useful results at all it’s not that likely that a complex model will help, whereas
    if the simple model does work there’s a good chance that a more complex model
    can help us achieve better results.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的问题通常需要复杂的解决方案，深度学习肯定可以给我们这些。然而，从可能起作用的最简单的事物开始通常是一个不错的方法。这让我们能够快速开始，并让我们知道我们是否朝着正确的方向努力：如果简单模型根本不产生任何有用的结果，那么复杂模型帮助的可能性不大，而如果简单模型有效，更复杂的模型有很大机会帮助我们取得更好的结果。
- en: Linear regression models are as simple as they come. The model tries to find
    a set of factors such that the linear combination of these factors and our vectors
    approach the target value as closely as possible. One nice aspect of these models
    compared to most machine learning models is that we can actually see what the
    contribution of each of the factors is.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型就是最简单的模型之一。该模型试图找到一组因素，使这些因素与我们的向量的线性组合尽可能接近目标值。与大多数机器学习模型相比，这些模型的一个好处是我们实际上可以看到每个因素的贡献是什么。
