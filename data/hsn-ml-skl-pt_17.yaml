- en: Chapter 15\. Transformers for Natural Language Processing and Chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章\. 用于自然语言处理和聊天机器人的Transformer
- en: 'In a landmark 2017 paper titled [“Attention Is All You Need”](https://homl.info/transformer),⁠^([1](ch15.html#id3414))
    a team of Google researchers proposed a novel neural net architecture named the
    *Transformer*, which significantly improved the state of the art in neural machine
    translation (NMT). In short, the Transformer architecture is simply an encoder-decoder
    model, very much like the one we built in [Chapter 14](ch14.html#nlp_chapter)
    for English-to-Spanish translation, and it can be used in exactly the same way
    (see [Figure 15-1](#transformer_encoder_decoder_diagram)):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年一篇具有里程碑意义的论文《“Attention Is All You Need”》（[“注意力即是所需”](https://homl.info/transformer)，⁠^([1](ch15.html#id3414)))中，一组谷歌研究人员提出了一种名为*Transformer*的新型神经网络架构，该架构显著提高了神经机器翻译（NMT）的现有水平。简而言之，Transformer架构只是一个编码器-解码器模型，非常类似于我们在第14章（[第14章](ch14.html#nlp_chapter)）中为英语到西班牙语翻译所构建的模型，并且可以以完全相同的方式使用（参见[图15-1](#transformer_encoder_decoder_diagram)）：
- en: The source text goes in the encoder, which outputs contextualized embeddings
    (one per token).
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源文本进入编码器，编码器输出上下文化的嵌入（每个标记一个）。
- en: The encoder’s output is then fed to the decoder, along with the translated text
    so far (starting with a start-of-sequence token).
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器的输出随后被送入解码器，同时包括到目前为止的翻译文本（从序列开始标记开始）。
- en: The decoder predicts the next token for each input token.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器为每个输入标记预测下一个标记。
- en: The last token output by the decoder is appended to the translation.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器输出的最后一个标记被添加到翻译中。
- en: Steps 2 to 4 are repeated again and again to produce the full translation, one
    extra token at a time, until an end-of-sequence token is generated. During training,
    we already have the full translation—it’s the target—so it is fed to the decoder
    in step 2 (starting with a start-of-sequence token), and steps 4 and 5 are not
    needed.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2到4会反复进行，以逐步生成完整的翻译，每次增加一个额外的标记，直到生成一个序列结束标记。在训练过程中，我们已经有完整的翻译——它是目标——因此，在步骤2（从序列开始标记开始）中将其送入解码器，步骤4和5就不需要了。
- en: '![Diagram illustrating the Transformer model''s process for translating English
    to Spanish, showing how the encoder generates contextual embeddings and the decoder
    predicts the next token in the translated sequence.](assets/hmls_1501.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图15-1. 展示Transformer模型将英语翻译成西班牙语的过程的示意图，显示了编码器如何生成上下文嵌入和解码器如何预测翻译序列中的下一个标记。](assets/hmls_1501.png)'
- en: Figure 15-1\. Using the Transformer model for English-to-Spanish translation
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-1\. 使用Transformer模型进行英语到西班牙语的翻译
- en: So what’s new? Well, inside the black box, there are some important differences
    with our previous encoder-decoder. Crucially, the Transformer architecture does
    not contain any recurrent or convolutional layers, just regular dense layers combined
    with a new kind of attention mechanism called *multi-head attention* (MHA), plus
    a few bells and whistles.⁠^([2](ch15.html#id3417)) Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it scales surprisingly well. Moreover, thanks to multi-head attention,
    the model can capture long-range patterns much better than RNNs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有什么新东西呢？嗯，在黑盒内部，与我们的先前编码器-解码器有一些重要的不同之处。关键的是，Transformer架构不包含任何循环或卷积层，只有常规的密集层，结合了一种新的注意力机制，称为*多头注意力*（MHA），还有一些额外的功能。⁠^([2](ch15.html#id3417))
    由于模型不是循环的，它不像RNN那样容易受到梯度消失或爆炸问题的影响，因此它可以以更少的步骤进行训练，更容易在多个GPU上并行化，并且扩展性出奇地好。此外，多亏了多头注意力，模型能够比RNN更好地捕捉长距离模式。
- en: The Transformer architecture also turned out to be extremely versatile. It was
    initially designed for NMT, but researchers quickly tweaked the architecture for
    many other language tasks. The year 2018 was even called the “ImageNet moment
    for NLP”. In June 2018, OpenAI released the first GPT model, based solely on the
    Transformer’s decoder module. It was pretrained on a large corpus of text, its
    ability to generate text was unprecedented, it could auto-complete sentences,
    invent stories, and even answer some questions. GPT could also be fine-tuned to
    perform a wide range of language tasks. Just a few months later, Google released
    the BERT model, based solely on the Transformer’s encoder module. It was excellent
    at a variety of *natural language understanding* (NLU) tasks, such as text classification,
    text embedding, multiple choice question answering, or finding the answer to a
    question within some text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构也证明了自己极其灵活。它最初是为 NMT 设计的，但研究人员很快对架构进行了调整，以适应许多其他语言任务。2018 年甚至被称为“NLP
    的 ImageNet 时刻”。2018 年 6 月，OpenAI 发布了第一个基于 Transformer 解码模块的 GPT 模型。它在大量文本语料库上进行了预训练，其生成文本的能力是前所未有的，它可以自动完成句子，创作故事，甚至回答一些问题。GPT
    还可以微调以执行各种语言任务。仅仅几个月后，谷歌发布了基于 Transformer 编码模块的 BERT 模型。它在各种自然语言理解（NLU）任务上表现出色，例如文本分类、文本嵌入、多项选择题回答，或在某些文本中找到问题的答案。
- en: Surprisingly, transformers also turned out to be great at computer vision, audio
    processing (e.g., speech-to-text), robotics (using inputs from sensors and sending
    the outputs to actuators), and more. For example, if you split an image into little
    chunks and feed them to a transformer (instead of token embeddings), you get a
    *vision transformer* (ViT). In fact, some transformers can even handle multiple
    *modalities* at once (e.g., text + image); these are called *multimodal models*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，Transformer 在计算机视觉、音频处理（例如，语音转文本）、机器人技术（使用传感器输入并将输出发送到执行器）等领域也表现出色。例如，如果你将图像分割成小块并喂给一个
    Transformer（而不是标记嵌入），你得到的是一个 *视觉 Transformer*（ViT）。实际上，一些 Transformer 甚至可以同时处理多种
    *模态*（例如，文本 + 图像）；这些被称为 *多模态模型*。
- en: 'This outstanding combination of performance, flexibility, and scalability encouraged
    Google, OpenAI, Facebook (Meta), Microsoft, Anthropic, and many other organizations
    to train larger and larger transformer models. The original Transformer model
    had about 65 million parameters—which was considered quite large at the time—but
    new transformers grew at a mind-boggling rate, reaching 1.6 trillion parameters
    by January 2021—that’s 1.6 million million parameters! Training such a gigantic
    transformer model from scratch is sadly restricted to organizations with deep
    pockets, as it requires a large and costly infrastructure for several months:
    training typically costs tens of millions of dollars, and even up to hundreds
    of millions according to some estimates (the exact figures are generally not public).
    [Figure 15-2](#transformer_growth_diagram) shows some of the most influential
    transformers released between June 2018 and April 2025.⁠^([3](ch15.html#id3423))
    Note that the vertical axis is in *billions* of parameters, and it uses a logarithmic
    scale.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种出色的性能、灵活性和可扩展性的组合促使谷歌、OpenAI、Facebook（Meta）、微软、Anthropic 以及许多其他组织训练更大规模的 Transformer
    模型。原始 Transformer 模型大约有 6500 万个参数——在当时被认为相当大——但新的 Transformer 以令人难以置信的速度增长，到 2021
    年 1 月达到了 1600 万亿个参数——即 1.6 亿亿个参数！从头开始训练如此庞大的 Transformer 模型遗憾的是仅限于财力雄厚的组织，因为它需要几个月时间的大型且昂贵的基础设施：训练通常需要数千万美元，据一些估计甚至高达数亿美元（确切数字通常不公开）。[图
    15-2](#transformer_growth_diagram) 展示了 2018 年 6 月至 2025 年 4 月间发布的一些最具影响力的 Transformer。请注意，垂直轴是以
    *十亿* 参数为单位的，并且使用了对数刻度。
- en: '![Diagram showing the exponential growth of transformer models from 2018 to
    2025, highlighting influential releases by organizations like OpenAI, Google,
    and Facebook, with parameter counts rising into trillions.](assets/hmls_1502.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![展示 2018 年至 2025 年 Transformer 模型指数增长的图表，突出 OpenAI、谷歌和 Facebook 等组织的影响力发布，参数数量上升至万亿级别。](assets/hmls_1502.png)'
- en: Figure 15-2\. Some of the most influential transformers released since 2018;
    see it larger [online](https://homl.info/fig15-2)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-2\. 自 2018 年以来发布的一些最具影响力的 Transformer；[在线查看更大版本](https://homl.info/fig15-2)
- en: 'Then, in November 2022, OpenAI released ChatGPT, an amazing *conversational
    AI*—or *chatbot*—that took the world by storm: it reached one million users in
    just five days, and over one hundred million monthly active users after just two
    months! Under the hood, it used GPT-3.5-turbo, a variant of GPT-3.5 which was
    fine-tuned to be conversational, helpful, and safe. Others soon followed: Perplexity
    AI, Google’s Gemini (initially called Bard), Anthropic’s Claude, Mistral AI, DeepSeek,
    and more.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2022年11月，OpenAI发布了ChatGPT，一个惊人的*对话式AI*——或*聊天机器人*——它一夜之间震惊了世界：它仅用五天时间就达到了一百万用户，两个月后月活跃用户超过一亿！在底层，它使用了GPT-3.5-turbo，这是GPT-3.5的一个变种，经过微调以实现对话性、有帮助性和安全性。其他人很快也效仿：Perplexity
    AI、Google的Gemini（最初称为Bard）、Anthropic的Claude、Mistral AI、DeepSeek以及更多。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before ChatGPT was released, Google had actually developed a powerful chatbot
    named LaMDA, but it wasn’t made public, likely for fear of reputational and legal
    risks, as the model was not deemed safe enough. This allowed OpenAI to become
    the first company to train a reasonably safe and helpful model and to package
    it as a useful chatbot product.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT发布之前，谷歌实际上已经开发了一个名为LaMDA的强大聊天机器人，但它并没有公开，可能是因为担心声誉和法律风险，因为这个模型被认为还不够安全。这允许OpenAI成为第一家训练一个相对安全和有帮助的模型并将其作为有用的聊天机器人产品的公司。
- en: 'So how can you use these models and chatbots? Well, many of them are proprietary
    (e.g., OpenAI’s GPT-3.5, GPT-4 and GPT-5 models, Anthropic’s Claude models, and
    Google’s Gemini models), and they can only be used via a web UI, an app, or an
    API: you must create an account, choose an offer (or use the free tier), and for
    the API you must get an access token and use it to query the API programmatically.
    However, many other models are *open weights*, meaning they can be downloaded
    for free (e.g., using the Hugging Face Hub): some of these have licensing restrictions
    (e.g., Meta’s Llama models are only free for noncommercial use), while others
    are truly open source (e.g., DeepSeek’s R1 or Mistral AI’s Mistral-7B). Some even
    include the training code and data (e.g., the OLMo models by Ai2).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何使用这些模型和聊天机器人呢？嗯，其中许多是专有的（例如，OpenAI的GPT-3.5、GPT-4和GPT-5模型，Anthropic的Claude模型，以及Google的Gemini模型），并且只能通过Web
    UI、应用程序或API使用：你必须创建一个账户，选择一个套餐（或使用免费层），对于API，你必须获取访问令牌并使用它来程序化查询API。然而，许多其他模型是*开放权重*，这意味着它们可以免费下载（例如，使用Hugging
    Face Hub）：其中一些有许可限制（例如，Meta的Llama模型仅限非商业用途免费），而其他则是真正的开源（例如，DeepSeek的R1或Mistral
    AI的Mistral-7B）。一些甚至包括训练代码和数据（例如，Ai2的OLMo模型）。
- en: 'So what are we waiting for? Let’s join the transformer revolution! Here’s the
    plan:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在等什么呢？让我们加入transformer革命！以下是计划：
- en: We will start by opening up the original Transformer architecture and inspecting
    its components to fully understand how everything works.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将首先打开原始Transformer架构，检查其组件，以全面了解一切是如何工作的。
- en: Then we will build and train a transformer from scratch for English-to-Spanish
    translation.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将从头开始构建和训练一个用于英语到西班牙语翻译的transformer。
- en: After that, we will look into encoder-only models like BERT, learn how they
    are pretrained, and see how to use them for tasks like text classification, semantic
    search, and text clustering, with or without fine-tuning.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们将研究仅编码器模型如BERT，了解它们是如何预训练的，并看看如何使用它们进行文本分类、语义搜索和文本聚类等任务，这些任务可以带或不带微调。
- en: Next, we will look into decoder-only models like GPT, and see how they are pretrained.
    These models are capable of generating text, which is great if you want to write
    a poem, but it can also be used to tackle many other tasks.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将研究仅解码器模型如GPT，看看它们是如何预训练的。这些模型能够生成文本，如果你想要写一首诗，这很好，但它也可以用于解决许多其他任务。
- en: 'Then we will use a decoder-only model to build our own chatbot! This involves
    a few steps: first, you must download a pretrained model (or train your own if
    you have the time and money), then you must fine-tune it to make it more conversational,
    helpful, and safe (or you can download an already fine-tuned model, or even use
    a conversational model via an API), and lastly you must deploy the model to a
    chatbot system that offers a user interface, stores conversations, and can also
    give the model access to tools, such as searching the web or using a calculator.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将使用仅解码器模型来构建我们自己的聊天机器人！这涉及几个步骤：首先，你必须下载一个预训练模型（或者如果你有时间和金钱，可以自己训练），然后你必须微调它，使其更具对话性、有帮助性和安全性（或者你可以下载已经微调好的模型，甚至可以通过API使用对话模型），最后你必须将模型部署到一个提供用户界面、存储对话并可以给模型提供访问工具（如搜索网络或使用计算器）的聊天机器人系统中。
- en: Lastly, we will take a quick look at encoder-decoder models, such as T5 and
    BART, which are great for tasks such as translation and summarization.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将快速浏览编码器-解码器模型，如T5和BART，这些模型非常适合翻译和摘要等任务。
- en: In [Chapter 16](ch16.html#vit_chapter), we will look at vision transformers
    and multimodal transformers. [Chapter 17](ch17.html#speedup_chapter) and “State-Space
    Models (SSMs)” (both available at [*https://homl.info*](https://homl.info)) also
    discuss some advanced techniques to allow Transformers to scale and process longer
    input sequences.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第16章](ch16.html#vit_chapter)中，我们将探讨视觉Transformer和多模态Transformer。[第17章](ch17.html#speedup_chapter)和“状态空间模型（SSMs）”（均可在[*https://homl.info*](https://homl.info)找到）也讨论了一些高级技术，以允许Transformer进行扩展并处理更长的输入序列。
- en: 'Let’s start by dissecting the Transformer architecture: take out your scalpel!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从剖析Transformer架构开始：拿出你的解剖刀！
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力即是所有你需要的东西：原始的Transformer架构
- en: The original 2017 Transformer architecture is represented in [Figure 15-3](#transformer_diagram).
    The left part of the figure represents the encoder, the right part represents
    the decoder.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的2017年Transformer架构在[图15-3](#transformer_diagram)中表示。图的左侧表示编码器，右侧表示解码器。
- en: 'As we saw earlier, the encoder’s role is to gradually *transform* the inputs
    (e.g., sequences of English tokens) until each token’s representation perfectly
    captures the meaning of that token in the context of the sentence: the encoder’s
    output is a sequence of contextualized token embeddings. Apart from the embedding
    layer, every layer in the encoder takes as input a tensor of shape [*batch size*,
    *max English sequence length in the batch*, *embedding size*] and returns a tensor
    of the exact same shape. This means that token representations get gradually transformed,
    hence the name of the architecture. For example, if you feed the sentence “I like
    soccer” to the encoder, then the token “like” will start off with a rather vague
    representation, since “like” could mean different things in different contexts
    (e.g., “I’m like a cat” versus “I like my cat”). But after going through the encoder,
    the token’s representation should capture the correct meaning of “like” in the
    given sentence (in this case, to be fond of), as well as any other information
    that may be required for translation (e.g., it’s a verb).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，编码器的角色是逐步*转换*输入（例如，英语标记的序列）直到每个标记的表示完美捕捉该标记在句子中的含义：编码器的输出是一系列上下文化的标记嵌入。除了嵌入层之外，编码器中的每一层都接受形状为[*批大小*，*批中最大英语序列长度*，*嵌入大小*]的张量作为输入，并返回形状完全相同的张量。这意味着标记表示会逐步转换，因此得名该架构。例如，如果你将句子“我喜欢足球”输入到编码器中，那么标记“喜欢”一开始将有一个相当模糊的表示，因为“喜欢”在不同的上下文中可能有不同的含义（例如，“我喜欢猫”与“我喜欢我的猫”）。但是经过编码器处理后，标记的表示应该捕捉到给定句子中“喜欢”的正确含义（在这种情况下，表示喜爱），以及可能需要翻译的任何其他信息（例如，它是一个动词）。
- en: 'The decoder’s role is to take the encoder’s outputs, along with the translated
    sentence so far, and predict the next token in the translation. For this, the
    decoder layers gradually transform each input token’s representation into a representation
    that can be used to predict the following token. For example, suppose the sentence
    to translate is “I like soccer” and we’ve already called the decoder four times,
    producing one new token each time: first “me”, then “me gusta”, then “me gusta
    el”, and finally “me gusta el fútbol”. Since this translation does not end with
    an EoS token `"</s>"`, we must call the decoder once again. The decoder’s input
    sequence is now "`<s>` me gusta el fútbol”. As the representation of each token
    goes through the decoder, it gets transformed: the representation of `"<s>"` becomes
    a rich enough representation to predict “me” (for simplicity, I’ll say this more
    concisely as: `"<s>"` becomes “me”), “me” becomes “gusta”, “gusta” becomes “el”,
    “el” becomes “fútbol”, and if everything goes well, “fútbol” becomes the EoS token
    `"</s>"`. Apart from the embedding layer and the output `Linear` layer, every
    layer in the decoder takes as input a tensor of shape [*batch size*, *max Spanish
    sequence length in the batch*, *embedding size*] and returns a tensor of the exact
    same shape.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的角色是接收编码器的输出以及迄今为止翻译的句子，并预测翻译中的下一个标记。为此，解码器层逐渐将每个输入标记的表示转换为可以用来预测下一个标记的表示。例如，假设要翻译的句子是“我喜欢足球”并且我们已经调用了解码器四次，每次产生一个新标记：首先“我”，然后“我喜欢”，然后“我喜欢足球的”，最后“我喜欢足球”。由于这个翻译不以EoS标记`"</s>"`结束，我们必须再次调用解码器。解码器的输入序列现在是"`<s>`我喜欢足球”。随着每个标记的表示通过解码器，它会被转换：`"<s>"`的表示变成了足够丰富的表示，可以预测“我”（为了简单起见，我会更简洁地说：`"<s>"`变成了“我”），“我”变成了“喜欢”，“喜欢”变成了“的”，“的”变成了“足球”，如果一切顺利，“足球”将变成EoS标记`"</s>"`。除了嵌入层和输出`Linear`层之外，解码器中的每一层都接受形状为[*batch
    size*, *batch中的最大西班牙语序列长度*, *嵌入大小*]的张量作为输入，并返回形状完全相同的张量。
- en: '![Diagram of the original 2017 transformer architecture, showing the encoder
    and decoder layers with multi-head attention, feed forward, and positional encoding
    processes, illustrating how inputs are transformed into outputs.](assets/hmls_1503.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![原始2017年transformer架构的示意图，展示了具有多头注意力、前馈和位置编码过程的编码器和解码器层，说明了输入是如何转换为输出的。](assets/hmls_1503.png)'
- en: Figure 15-3\. The original 2017 transformer architecture⁠^([4](ch15.html#id3427))
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3. 原始2017年transformer架构⁠^([4](ch15.html#id3427))
- en: 'After going through the decoder, each token representation goes through a final
    `Linear` layer which will hopefully output a high logit for the correct token
    and a low logit for all other tokens in the vocabulary. The decoder’s output shape
    is [*batch size*, *max Spanish sequence length in the batch*, *vocabulary size*].
    The final predicted sentence should be “me gusta el fútbol `</s>`“. Note that
    the figure shows a softmax layer at the top, but in PyTorch we usually don’t explicitly
    add it: instead, we let the model output logits, and we train the model using
    `nn.CrossEntropyLoss`, which computes the cross-entropy loss based on logits instead
    of estimated probabilities (as we saw in previous chapters). If you ever need
    estimated probabilities, you can always convert the logits to estimated probabilities
    using the `F.softmax()` function.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器处理后，每个标记的表示会通过一个最终的`Linear`层，希望输出正确的标记的高logit值，以及词汇表中所有其他标记的低logit值。解码器的输出形状是[*batch
    size*, *batch中的最大西班牙语序列长度*, *词汇表大小*]。最终的预测句子应该是“我喜欢足球 `</s>`”。注意，图中显示了顶部的softmax层，但在PyTorch中我们通常不会明确添加它：相反，我们让模型输出logit，并使用`nn.CrossEntropyLoss`训练模型，该损失函数基于logit而不是估计的概率（如我们之前章节中看到的）。如果您需要估计的概率，您始终可以使用`F.softmax()`函数将logit转换为估计的概率。
- en: 'Now let’s zoom in a bit further into [Figure 15-3](#transformer_diagram):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进一步聚焦于[图15-3](#transformer_diagram)：
- en: First, notice that both the encoder and the decoder contain blocks that are
    stacked *N* times. In the paper, *N* = 6\. Note that the final outputs of the
    whole encoder stack are fed to each of the decoder’s *N* blocks.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，请注意，编码器和解码器都包含堆叠了*N*次的块。在论文中，*N* = 6。请注意，整个编码器堆栈的最终输出被馈送到解码器的每个*N*个块中。
- en: 'As you can see, you are already familiar with most components: there are two
    embedding layers; several skip connections, each of them followed by a layer normalization
    module; several feedforward modules composed of two dense layers each (the first
    one using the ReLU activation function, the second with no activation function);
    and finally, the output layer is a linear layer. Notice that all layers treat
    each token independently from all the others. But how can we translate a sentence
    by looking at the tokens completely separately? Well, we can’t, so that’s where
    the new components come in:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如您所见，您已经熟悉了大多数组件：有两个嵌入层；几个跳过连接，每个连接后面都跟着一个层归一化模块；几个由两个密集层组成的前馈模块（第一个使用ReLU激活函数，第二个没有激活函数）；最后，输出层是一个线性层。请注意，所有层都独立于其他所有层处理每个标记。但我们是怎样通过完全分开地看待标记来翻译一个句子的呢？嗯，我们做不到，这就是新组件发挥作用的地方：
- en: The encoder’s *multi-head attention* layer updates each token representation
    by attending to (i.e., paying attention to) every token in the same sentence,
    including itself. This is called *self-attention*. That’s where the vague representation
    of the word “like” becomes a richer and more accurate representation, capturing
    its precise meaning within the given sentence (e.g., the layer notices the subject
    “I” so it infers that “like” must be a verb). We will discuss exactly how this
    works shortly.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的*多头注意力*层通过关注（即，关注）同一句子中的每个标记（包括自身）来更新每个标记表示。这被称为*自*注意力。这就是单词“like”的模糊表示变成一个更丰富、更准确的表示，捕捉它在给定句子中的精确含义（例如，层注意到主语“I”，因此推断“like”必须是一个动词）。我们将在稍后详细讨论这是如何工作的。
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a token, it doesn’t attend to tokens located after it: it’s
    a causal layer. For example, when it processes the token “gusta”, it only attends
    to the tokens `"<s>"`, “me”, and “gusta”, and it ignores the tokens “el” and “fútbol”
    (or else the model could cheat during training).'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的*掩码多头注意力*层做的是同样的事情，但在处理一个标记时，它不会关注位于其后的标记：它是一个因果层。例如，当它处理标记“gusta”时，它只关注标记`"<s>"`、“me”和“gusta”，而忽略标记“el”和“fútbol”（否则模型在训练期间可能会作弊）。
- en: The decoder’s upper multi-head attention layer is where the decoder pays attention
    to the contextualized token representations output by the encoder stack. This
    is called *cross*-attention, as opposed to *self*-attention. For example, the
    decoder will probably pay close attention to the word “soccer” when it processes
    the word “el” and outputs a representation of the word “fútbol”.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的上层多头注意力层是解码器关注编码器堆栈输出的上下文化标记表示的地方。这被称为*交叉*注意力，与*自*注意力相对。例如，当解码器处理单词“el”并输出单词“fútbol”的表示时，它可能会非常关注单词“soccer”。
- en: 'The *positional encodings* are dense vectors that represent the position of
    each token in the sentence. The *n*^(th) positional encoding is added to the token
    embedding of the *n*^(th) token in each sentence. This is needed because all layers
    in the Transformer architecture are position-agnostic, meaning they treat all
    positions equally (unlike recurrent or convolutional layers): when they process
    a token, they have no idea where that token is located in the sentence or relative
    to other words. But the order of words matters, so we must somehow give positional
    information to the Transformer. Adding positional encodings to the token representations
    is a good way to achieve this.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置编码*是密集向量，表示句子中每个标记的位置。第 *n* 个位置编码被添加到每个句子中第 *n* 个标记的标记嵌入中。这是必需的，因为Transformer架构中的所有层都是位置无关的，这意味着它们对所有位置同等对待（与循环或卷积层不同）：当它们处理一个标记时，它们不知道该标记在句子中的位置或相对于其他词的位置。但是单词的顺序很重要，因此我们必须以某种方式向Transformer提供位置信息。将位置编码添加到标记表示中是实现这一目标的好方法。'
- en: Note
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first two arrows going into each multi-head attention layer in [Figure 15-3](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries.⁠^([5](ch15.html#id3439))
    In the self-attention layers, all three are equal to the token representations
    output by the previous layer, while in the cross-attention layers (i.e., the decoder’s
    upper attention layers), the keys and values are equal to the encoder’s final
    token representations, and the queries are equal to the token representations
    output by the previous decoder layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15-3](#transformer_diagram)中，每个多头注意力层进入的前两个箭头代表键和值，第三个箭头代表查询。⁠^([5](ch15.html#id3439))
    在自注意力层中，这三个都等于前一层输出的标记表示，而在交叉注意力层（即解码器的上层注意力层）中，键和值等于编码器的最终标记表示，而查询等于前一个解码器层输出的标记表示。
- en: Now let’s go through the novel components of the Transformer architecture in
    more detail, starting with the positional encodings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更详细地探讨Transformer架构的独创组件，从位置编码开始。
- en: Positional Encodings
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'A positional encoding is a dense vector that encodes the position of a token
    within a sentence: the *i*^(th) positional encoding is added to the token embedding
    of the *i*^(th) token in each sentence. A simple way to implement this is to use
    an `Embedding` layer: just add embedding #0 to the representation of token #0,
    add embedding #1 to the representation of token #1, and so on. Alternatively,
    you can use an `nn.Parameter` to store the embedding matrix (initialized using
    small random weights), then add its first *L* rows to the inputs (where *L* is
    the max input sequence length): the result is the same, but it’s much faster.
    You can also add a bit of dropout to reduce the risk of overfitting. Here’s an
    implementation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是一个密集向量，它编码了标记在句子中的位置：第*i*个位置编码被添加到每个句子中第*i*个标记的嵌入表示。实现这一点的简单方法是用一个`Embedding`层：只需将嵌入编号#0添加到标记编号#0的表示中，将嵌入编号#1添加到标记编号#1的表示中，依此类推。或者，你可以使用一个`nn.Parameter`来存储嵌入矩阵（使用小的随机权重初始化），然后将它的前*L*行添加到输入中（其中*L*是最大输入序列长度）：结果是相同的，但速度要快得多。你还可以添加一些dropout来降低过拟合的风险。以下是一个实现示例：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The inputs have shape [*batch size*, *sequence length*, *embedding size*],
    but we are adding positional encodings of shape [*sequence length*, *embedding
    size*]. This works thanks to the broadcasting rules: the *i*^(th) positional embedding
    is added to the *i*^(th) token’s representation of each sentence in the batch.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的形状为[*批大小*，*序列长度*，*嵌入大小*]，但我们添加了形状为[*序列长度*，*嵌入大小*]的位置编码。这得益于广播规则：第*i*个位置编码被添加到每个句子中第*i*个标记的表示。
- en: The authors of the Transformer paper also proposed using fixed positional encodings
    rather than trainable ones. Their approach used a pretty smart scheme based on
    the sine and cosine functions, but it’s not much used anymore, as it doesn’t really
    perform any better than trainable positional embeddings (except perhaps on small
    transformers, if you’re lucky). Please see this chapter’s notebook for more details.
    Moreover, newer approaches such as *relative position bias* (RPB), *rotary positional
    encoding* (RoPE), and *attention with linear bias* (ALiBi) generally perform better.
    To learn more about all of these alternative approaches to positional encoding,
    see “Relative Positional Encoding”.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer论文的作者还提出了使用固定位置编码而不是可训练位置编码。他们的方法使用了一种相当聪明的基于正弦和余弦函数的方案，但现在已经很少使用了，因为它实际上并没有比可训练位置嵌入表现得更好（除非你很幸运，在小型Transformer上）。有关更多详细信息，请参阅本章的笔记本。此外，像**相对位置偏置**（RPB）、**旋转位置编码**（RoPE）和**带线性偏置的注意力**（ALiBi）等更新的方法通常表现更好。要了解更多关于所有这些位置编码的替代方法，请参阅“相对位置编码”。
- en: 'Now let’s look deeper into the heart of the Transformer model: the multi-head
    attention layer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更深入地看看Transformer模型的核心：多头注意力层。
- en: Multi-Head Attention
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: The multi-head attention (MHA) layer is based on *scaled dot-product attention*,
    a variant of dot-product attention (introduced in [Chapter 14](ch14.html#nlp_chapter))
    that scales down the similarity scores by a constant factor. See [Equation 15-1](#scaled_dot_product_attention)
    for its vectorized equation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力（MHA）层基于**缩放点积注意力**，这是点积注意力（在第14章中介绍）的一种变体，通过一个常数因子缩放相似度得分。参见[方程15-1](#scaled_dot_product_attention)以了解其向量方程。
- en: Equation 15-1\. Scaled dot-product attention
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-1\. 缩放点积注意力
- en: <mo>Attention</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%"
    rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi
    mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi
    mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <mo>注意力</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%"
    rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi
    mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi
    mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi>
- en: 'In this equation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中：
- en: '**Q** is a matrix representing a *query* (e.g., an English or Spanish sequence,
    depending on the attention layer). Its shape is [*L*[q], *d*[q]], where *L*[q]
    is the length of the query and *d*[q] is the query’s dimensionality (i.e., the
    number of dimensions in the token representations).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q** 是一个表示 *查询* 的矩阵（例如，一个英语或西班牙语序列，具体取决于注意力层）。其形状为 [*L*[q], *d*[q]]，其中 *L*[q]
    是查询的长度，*d*[q] 是查询的维度性（即标记表示中的维度数）。'
- en: '**K** is a matrix representing a key. Its shape is [*L*[k], *d*[k]], where
    *L*[k] is the length of the key and *d*[k] is the key’s dimensionality. Note that
    *d*[k] must equal *d*[q].'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K** 是一个表示键的矩阵。其形状为 [*L*[k], *d*[k]]，其中 *L*[k] 是键的长度，*d*[k] 是键的维度性。请注意，*d*[k]
    必须等于 *d*[q]。'
- en: '**V** is a matrix representing a value. Its shape is [*L*[v], *d*[v]], where
    *L*[v] is the length of the value and *d*[v] is the value’s dimensionality. Note
    that *L*[v] must equal *L*[k].'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V** 是一个表示值的矩阵。其形状为 [*L*[v], *d*[v]]，其中 *L*[v] 是值的长度，*d*[v] 是值的维度性。请注意，*L*[v]
    必须等于 *L*[k]。'
- en: 'The shape of **Q** **K**^⊺ is [*L*[q], *L*[k]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long: this is the critical *quadratic context window*
    problem (we will discuss various ways to alleviate this issue in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter). The softmax function is applied to each row:
    the output has the same shape as the input, but now each row sums up to 1\. The
    final output has a shape of [*L*[q], *d*[v]]. There is one row per query token,
    and each row represents the query result: a weighted sum of the value tokens,
    favoring value tokens whose corresponding key tokens are most aligned with the
    given query token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q** **K**^⊺ 的形状为 [*L*[q], *L*[k]]：它包含每个查询/键对的相似度分数。为了防止这个矩阵变得非常大，输入序列不能太长：这是关键的
    *二次上下文窗口* 问题（我们将在第 [16](ch16.html#vit_chapter) 章和第 [17](ch17.html#speedup_chapter)
    章中讨论缓解这一问题的各种方法）。softmax 函数应用于每一行：输出具有与输入相同的形状，但现在每一行的总和为 1。最终的输出形状为 [*L*[q],
    *d*[v]]。每一行对应一个查询标记，每一行代表查询结果：值标记的加权和，优先考虑与给定查询标记最对齐的键标记的值标记。'
- en: The scaling factor 1 / <msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt>
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients. This factor was empirically shown to speed up and
    stabilize training.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放因子 1 / <msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt> 将相似度分数缩小，以避免饱和
    softmax 函数，这会导致梯度非常小。这个因子在经验上被证明可以加速并稳定训练。
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax
    (in practice, we can add `–torch.inf`). The resulting weights will be equal to
    zero. This is useful to mask padding tokens, as well as future tokens in the masked
    multi-head attention layer.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在计算 softmax 之前，通过向相应的相似度分数添加一个非常大的负值，可以屏蔽一些键/值对。在实践中，我们可以添加 `–torch.inf`）。结果权重将等于零。这有助于屏蔽填充标记，以及屏蔽多头注意力层中的未来标记。 '
- en: PyTorch comes with the `F.scaled_dot_product_attention()` function. Its inputs
    are just like **Q**, **K**, and **V**, but these inputs can have extra dimensions
    at the start, such as the batch size and the number of heads (when used for multi-head
    attention). The equation is applied simultaneously across all of these extra dimensions.
    In other words, the function computes the results simultaneously across all sentences
    in the batch and across all attention heads, making it very efficient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 包含 `F.scaled_dot_product_attention()` 函数。其输入与 **Q**、**K** 和 **V** 相同，但这些输入可以在开头有额外的维度，例如批处理大小和头数（当用于多头注意力时）。该方程会在所有这些额外维度上同时应用。换句话说，该函数会在批处理中的所有句子和所有注意力头之间同时计算结果，这使得它非常高效。
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 15-4](#multihead_attention_diagram).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好查看多头注意力层。其架构在 [图 15-4](#multihead_attention_diagram) 中显示。
- en: As you can see, it is just a bunch of scaled dot-product attention layers, called
    *attention heads*, each preceded by a linear transformation of the values, keys,
    and queries (across all tokens). The outputs of all the attention heads are simply
    concatenated, and they go through a final linear transformation (again, across
    all tokens).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它只是一系列缩放点积注意力层，称为 *注意力头*，每个注意力头之前都有一个值、键和查询的线性变换（跨所有标记）。所有注意力头的输出只是简单地连接起来，然后通过最终的线性变换（再次，跨所有标记）。
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was hopefully
    smart enough to encode its meaning, the fact that it’s a verb, and many other
    features that are useful for its translation, such as the fact that it is in the
    present tense. The token representation also includes the position, thanks to
    the positional encodings. In short, the token representation encodes many different
    characteristics of the token. If we just used a single scaled dot-product attention
    layer, we would only be able to query all of these characteristics in one shot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么？这种架构背后的直觉是什么？好吧，再次考虑句子“我喜欢足球”中的单词“喜欢”。编码器可能足够聪明，能够编码其含义，即它是一个动词，以及许多对翻译有用的其他特征，例如它是一般现在时。标记表示还包括位置，多亏了位置编码。简而言之，标记表示编码了标记的许多不同特征。如果我们只使用单个缩放点积注意力层，我们只能一次性查询所有这些特征。
- en: '![Diagram illustrating the architecture of a multi-head attention layer, highlighting
    the flow from linear transformations of values, keys, and queries through split
    layers, scaled dot-product attention, concatenation, and final linear transformation.](assets/hmls_1504.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![展示多头注意力层架构的图表，突出显示值、键和查询的线性变换、缩放点积注意力、连接和最终线性变换的流程](assets/hmls_1504.png)'
- en: Figure 15-4\. Multi-head attention layer architecture⁠^([6](ch15.html#id3452))
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-4\. 多头注意力层架构⁠^([6](ch15.html#id3452))
- en: Warning
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The Transformer architecture is extremely flexible, so the model has plenty
    of freedom during training to choose its own knowledge representation and strategies.
    As a result, it ends up being somewhat of a black box: understanding how transformers
    truly “think” is an area of active research, called *model interpretability*.
    For example, check out this [fascinating post by Anthropic](https://homl.info/tracing-thoughts).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构极其灵活，因此在训练过程中模型有充分的自由度来选择自己的知识表示和策略。结果，它最终变成了一种黑盒：理解变压器真正“思考”的方式是一个活跃的研究领域，被称为
    *模型可解释性*。例如，可以查看这个 [Anthropic 的迷人帖子](https://homl.info/tracing-thoughts)。
- en: 'This is why the MHA layer splits the values, keys, and queries across multiple
    heads: this way, each head can focus on specific characteristics of the token.
    The first linear layer lets the model choose which characteristics each head should
    focus on. For example, the linear layer may ensure that the first head gets a
    projection of the “like” token’s representation into a subspace where all that
    remains is the information that this token is a verb in the present tense. Another
    head may focus on the word’s meaning, and so on. Then the scaled dot-product attention
    layers implement the actual lookup phase, and finally the results are all concatenated
    and run through a final linear layer that lets the model reorganize the representation
    as it pleases.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么 MHA 层将值、键和查询分割到多个头中：这样，每个头都可以专注于特定标记的特性。第一个线性层让模型选择每个头应该关注哪些特性。例如，线性层可以确保第一个头将“like”标记的表示投影到只包含该标记是现在时态动词的信息的子空间中。另一个头可能专注于单词的意义，等等。然后缩放点积注意力层实现实际的查找阶段，最后将所有结果连接起来，并通过一个最终的线性层运行，允许模型按其喜好重新组织表示。
- en: 'To really understand the Transformer architecture, the key is to understand
    multi-head attention, and for this, it helps to look at a basic implementation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正理解 Transformer 架构，关键在于理解多头注意力机制，为此，查看一个基本实现会有所帮助：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s go through this code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: The constructor stores the number of heads `self.h` and computes the number
    of dimensions per head `self.d`, then it creates the necessary modules. Note that
    the embedding size must be divisible by the number of heads.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数存储头的数量 `self.h` 并计算每个头的维度数 `self.d`，然后创建必要的模块。注意，嵌入大小必须能被头的数量整除。
- en: 'The `split_heads()` method is used in the `forward()` method. It splits its
    input `X` along its last dimension (one split per head), converting it from a
    3D tensor of shape [*B*, *L*, *h* × *d*] to a 4D tensor of shape [*B*, *L*, *h*,
    *d*], where *B* is the batch size, *L* is the max length of the input sequences
    (specifically *L*[k] for the key and value, or *L*[q] for the query), *h* is the
    number of heads, and *d* is the number of dimensions per head (i.e., *h* × *d*
    = embedding size). The dimensions 1 and 2 are then swapped to get a tensor of
    shape [*B*, *h*, *L*, *d*]: since the matrix multiplication operator `@` only
    works on the last two dimensions, it won’t touch the first two dimensions *B*
    and *h*, so we will be able to use this operator to compute the scores across
    all instances in the batch and across all attention heads, all in one shot (`q
    @ k.transpose(2, 3)`). The same will be true when computing all the attention
    outputs (`weights @ v`).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_heads()` 方法在 `forward()` 方法中使用。它将其输入 `X` 沿其最后一个维度（每个头一个分割）进行分割，将其从形状为
    [*B*, *L*, *h* × *d*] 的 3D 张量转换为形状为 [*B*, *L*, *h*, *d*] 的 4D 张量，其中 *B* 是批大小，*L*
    是输入序列的最大长度（对于键和值是 *L*[k]，对于查询是 *L*[q]），*h* 是头的数量，*d* 是每个头的维度数（即 *h* × *d* = 嵌入大小）。然后交换维度
    1 和 2 以获得形状为 [*B*, *h*, *L*, *d*] 的张量：由于矩阵乘法运算符 `@` 只在最后两个维度上工作，它不会触及前两个维度 *B*
    和 *h*，因此我们可以使用此运算符一次性计算批中所有实例和所有注意力头之间的分数（`q @ k.transpose(2, 3)`）。在计算所有注意力输出时（`weights
    @ v`）也将如此。'
- en: 'The `forward()` method starts by applying a linear transformation to the query,
    key, and value, and passes the result through the `split_heads()` method. The
    next three lines compute [Equation 15-1](#scaled_dot_product_attention), plus
    a bit of dropout on the weights. Next we swap back dimensions 1 and 2 to ensure
    that the dimensions *h* and *d* are next to each other again, then we reshape
    the tensor back to 3D: this will concatenate the outputs of all heads. We can
    then apply the output linear transformation and return the result, along with
    the weights (in case we need them later).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法首先对查询、键和值应用线性变换，并通过 `split_heads()` 方法传递结果。接下来的三行计算 [方程 15-1](#scaled_dot_product_attention)，并在权重上添加一些
    dropout。然后我们交换维度 1 和 2，以确保维度 *h* 和 *d* 再次相邻，然后将张量重塑回 3D：这将连接所有头的输出。然后我们可以应用输出线性变换并返回结果，以及权重（以防以后需要它们）。'
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Don’t worry if it takes some time to fully grasp this, it’s not easy. Of course,
    you can drive a car without fully understanding how the engine works, but some
    of the transformer improvements described in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter) will only make sense if you understand MHA.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要一些时间才能完全理解，请不要担心，这并不容易。当然，你可以在不完全理解引擎工作原理的情况下开车，但第 [16](ch16.html#vit_chapter)
    章和 [17](ch17.html#speedup_chapter) 中描述的一些 Transformer 改进只有在理解 MHA 的情况下才有意义。
- en: 'But wait! We’re missing one important detail: masking. Indeed, as we discussed
    earlier, the decoder’s masked self-attention layers must only consider previous
    tokens when trying to predict what the next token is (or else it would be cheating).
    Moreover, if the key contains padding tokens, we want to ignore them as well.
    So let’s update the `forward()` method to support two additional arguments:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！我们遗漏了一个重要的细节：掩码。确实，正如我们之前讨论的，解码器的掩码自注意力层在尝试预测下一个标记时必须只考虑之前的标记（否则就是作弊）。此外，如果键包含填充标记，我们还想忽略它们。因此，让我们更新
    `forward()` 方法以支持两个额外的参数：
- en: '`attn_mask`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`attn_mask`'
- en: A boolean mask of shape [*L*[q], *L*[k]] that we will use to control which key
    tokens each query token should ignore (`True` to ignore, `False` to attend)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为 [*L*[q], *L*[k]] 的布尔掩码，我们将用它来控制每个查询标记应该忽略哪些键标记（`True` 表示忽略，`False` 表示关注）
- en: '`key_padding_mask`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`key_padding_mask`'
- en: A boolean mask of shape [*B*, *L*[k]] to locate the padding tokens in each key
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为 [*B*, *L*[k]] 的布尔掩码，用于定位每个键中的填充标记
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code replaces the scores we want to ignore with negative infinity, so
    the corresponding weights will be zero after the softmax operation (if we tried
    to zero out these weights directly, the remaining weights would not add up to
    1). Note that the masks are broadcast automatically: `attn_mask` is broadcast
    across the whole batch and all attention heads, and `key_padding_mask` is broadcast
    across all heads and all query tokens.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将我们想要忽略的分数替换为负无穷大，因此在 softmax 操作后相应的权重将为零（如果我们直接尝试将这些权重置零，剩余的权重将不会加起来等于
    1）。请注意，掩码会自动广播：`attn_mask` 在整个批次和所有注意力头之间广播，而 `key_padding_mask` 在所有头和所有查询标记之间广播。
- en: 'PyTorch has a very similar `nn.MultiheadAttention` module, which is much more
    optimized (e.g., it can often fuse the three input projections into one). It has
    the same arguments, which behave in exactly the same way. It also has a few more.
    Here are the most important:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 有一个非常相似的 `nn.MultiheadAttention` 模块，它进行了更多优化（例如，它通常可以将三个输入投影融合成一个）。它具有相同的参数，并且行为完全相同。它还有一些额外的参数。以下是最重要的：
- en: The constructor has a `batch_first` argument which defaults to `False`, so the
    module expects the batch dimension to come after the sequence length dimension.
    You must set `batch_first=True` if you prefer the batch dimension to come first,
    like in our custom implementation.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数有一个 `batch_first` 参数，默认为 `False`，因此模块期望批次维度在序列长度维度之后。如果你更喜欢批次维度在前面，就像在我们的自定义实现中一样，你必须设置
    `batch_first=True`。
- en: The `forward()` method has a `need_weights` argument that defaults to `True`.
    If you don’t need to use the weights returned by this module, you should set this
    argument to `False`, as it sometimes allows for some optimizations. When `need_weights`
    is set to `False`, the method returns `None` instead of the weights.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法有一个 `need_weights` 参数，默认为 `True`。如果你不需要使用此模块返回的权重，你应该将此参数设置为
    `False`，因为这有时允许进行一些优化。当 `need_weights` 设置为 `False` 时，该方法返回 `None` 而不是权重。'
- en: 'The `forward()` method also has an `is_causal` argument: if (and only if) the
    `attn_mask` is set and is a *causal mask*, then you can set `is_causal=True` to
    allow for some performance optimizations. A causal mask allows each query token
    to attend to all previous tokens (including itself), but doesn’t allow it to attend
    to tokens located after it. In other words, a causal mask contains `True` above
    the main diagonal, and `False` everywhere else. This is the mask needed for the
    masked self-attention layers.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法还有一个 `is_causal` 参数：如果（并且仅当）`attn_mask` 被设置并且是一个 *causal mask*，那么你可以设置
    `is_causal=True` 以允许一些性能优化。一个因果掩码允许每个查询标记关注所有之前的标记（包括它自己），但不会允许它关注位于其后的标记。换句话说，因果掩码在主对角线上方包含
    `True`，在其他所有位置包含 `False`。这是掩码自注意力层所需的掩码。'
- en: Now that we have the main ingredient, we’re ready to implement the rest of the
    Transformer model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了主要成分，我们就可以实现 Transformer 模型的其余部分了。
- en: Building the Rest of the Transformer
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Transformer 的其余部分
- en: 'The rest of the Transformer architecture is much more straightforward. Let’s
    start with the encoder block. The following implementation closely matches the
    encoder block represented on the left side of [Figure 15-3](#transformer_diagram),
    except it sprinkles a bit of dropout after the self-attention layer and after
    both dense layers in the feedforward module:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的其余部分要简单得多。让我们从编码器块开始。以下实现与[图15-3](#transformer_diagram)左侧所表示的编码器块非常相似，只是在自注意力层和前馈模块中的两个密集层之后都添加了一些dropout：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the feedforward block is composed of a first `Linear` layer that
    expands the dimensionality to 2048 (by default), followed by a nonlinearity (ReLU
    in this case), then a second `Linear` layer that projects the data back down to
    the original embedding size (also called the *model dimension*, `d_model`). This
    *reverse bottleneck* increases the expressive power of the nonlinearity, allowing
    the model to learn much richer combinations of features. This idea was explored
    further in the later MobileNetv2 paper, whose authors coined the term *inverted
    residual network*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到前馈块由一个首先将维度扩展到2048（默认值）的`Linear`层组成，然后是一个非线性（在这种情况下是ReLU），然后是一个将数据投影回原始嵌入大小（也称为*模型维度*，`d_model`）的第二个`Linear`层。这个*反向瓶颈*增加了非线性的表达能力，允许模型学习更丰富的特征组合。这个想法在后来的MobileNetv2论文中得到了进一步探索，该论文的作者提出了*逆残差网络*这个术语。
- en: In the encoder, the `src_mask` argument is generally not used, since the encoder
    allows each token to attend to all tokens, even ones located after it. However,
    the user is expected to set the `key_padding_mask` appropriately.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中，`src_mask`参数通常不使用，因为编码器允许每个标记关注所有标记，即使是在它之后的位置。然而，用户预计会适当地设置`key_padding_mask`。
- en: 'Now here’s an implementation of the decoder block. It closely matches the decoder
    block represented on the righthand side of [Figure 15-3](#transformer_diagram),
    with some additional dropout:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一个解码器块的实现。它与[图15-3](#transformer_diagram)右侧所表示的解码器块非常相似，只是增加了一些dropout：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `memory` argument corresponds to the output of the encoder. For full flexibility,
    we let the user pass the appropriate masks to the `forward()` method. In general,
    you will need to set the padding masks appropriately (both for the memory and
    target), and set the `tgt_mask` to a causal mask (we will see how shortly).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory`参数对应于编码器的输出。为了实现完全的灵活性，我们允许用户将适当的掩码传递给`forward()`方法。通常，您需要适当地设置填充掩码（对于记忆和目标），并将`tgt_mask`设置为因果掩码（我们很快就会看到）。'
- en: 'PyTorch actually provides `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`
    out of the box, with the same arguments, plus a few more: most importantly `batch_first`,
    which you must set to `True` if the batch dimension is first, plus one `*_is_causal`
    argument for each attention mask, and an `activation` argument that defaults to
    “relu”. Many state-of-the-art transformers use a more advanced activation such
    as GELU (introduced in [Chapter 11](ch11.html#deep_chapter)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch实际上提供了`nn.TransformerEncoderLayer`和`nn.TransformerDecoderLayer`，直接使用相同的参数，还有一些额外的参数：最重要的是`batch_first`，如果您将批处理维度设置为第一个，则必须将其设置为`True`，以及每个注意力掩码的一个`*_is_causal`参数，还有一个默认为“relu”的`activation`参数。许多最先进的transformer使用更先进的激活，如GELU（在第11章中介绍）。
- en: 'PyTorch also provides three more transformer modules (writing a custom module
    for each of these is left as an exercise for the reader—see the notebook for a
    solution):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了三个额外的transformer模块（为这些中的每一个编写自定义模块留给读者作为练习——参见笔记本以获取解决方案）：
- en: '`nn.TransformerEncoder`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.TransformerEncoder`'
- en: Simply chains the desired number of encoder layers. Its constructor takes an
    encoder layer plus the desired number of layers `num_layers`, and it clones the
    given encoder layer `num_layers` times. The constructor also takes an optional
    normalization layer, which (if provided) is applied to the final output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地链接所需数量的编码器层。其构造函数接受一个编码器层和所需的层数`num_layers`，并将给定的编码器层克隆`num_layers`次。构造函数还接受一个可选的归一化层，如果提供，则应用于最终输出。
- en: '`nn.TransformerDecoder`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.TransformerDecoder`'
- en: Same, except it chains decoder layers instead of encoder layers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相同，只是它链接解码器层而不是编码器层。
- en: '`nn.Transformer`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Transformer`'
- en: Creates an encoder and a decoder (both with layer norm), and chains them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个编码器和解码器（都具有层归一化），并将它们链接起来。
- en: Congratulations! You now know how to build a full Transformer model from scratch.
    You only need to add a final `Linear` layer and use the `nn.CrossEntropyLoss`
    to get the full architecture shown in [Figure 15-3](#transformer_diagram) (as
    we saw in earlier chapters, the softmax layer is implicitly included in the loss).
    Now let’s see how to use a Transformer model to translate English to Spanish.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在知道如何从头开始构建一个完整的Transformer模型。你只需要添加一个最终的`Linear`层，并使用`nn.CrossEntropyLoss`来获得[图15-3](#transformer_diagram)中显示的完整架构（正如我们在前面的章节中看到的，softmax层隐式包含在损失中）。现在让我们看看如何使用Transformer模型将英语翻译成西班牙语。
- en: Building an English-to-Spanish Transformer
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建英语到西班牙语的Transformer
- en: 'It’s time to build our NMT Transformer model. For this, we’ll use our `Positional​Embedding`
    module and PyTorch’s `nn.Transformer` (our custom `Transformer` module works fine,
    but it’s slower):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候构建我们的NMT Transformer模型了。为此，我们将使用我们的`PositionalEmbedding`模块和PyTorch的`nn.Transformer`（我们的自定义`Transformer`模块运行良好，但速度较慢）：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s go through this code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这段代码：
- en: 'The constructor is straightforward: we just create the necessary modules.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数很简单：我们只需创建必要的模块。
- en: The `forward()` method takes an `NmtPair` as input (this class was defined in
    [Chapter 14](ch14.html#nlp_chapter)). The method starts by embedding the input
    tokens for both the source and target inputs, and it adds the positional encodings
    to both.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法接受一个`NmtPair`作为输入（这个类在第14章中定义）。该方法首先对源和目标输入的输入标记进行嵌入，并给它们添加位置编码。'
- en: Then the code uses the *not* operator (`~`) to invert both the source and target
    masks because they contain `False` for each padding token, but `nn.Multihead​Attention`
    expects `True` for tokens that it should ignore.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后代码使用`not`运算符（`~`）反转源和目标掩码，因为它们对于每个填充标记都包含`False`，但`nn.MultiheadAttention`期望对于它应该忽略的标记使用`True`。
- en: 'Next, we create a square matrix of shape [*L*[q], *L*[q]], full of `True`,
    and we get all elements above the main diagonal using the `torch.triu()` function,
    with the rest defaulting to `False`. This results in a causal mask that we can
    use as the `tgt_mask` for the transformer: it will use this mask for the masked
    self-attention layer. Alternatively, you could call `nn.Transformer.gen⁠erate_​square_subsequent_mask()`
    to create the causal mask: just pass it the sequence length (`pair.tgt_token_ids.size(1)`)
    and set `dtype=torch.bool`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个形状为[*L*[q]，*L*[q]]的正方形矩阵，填充`True`，然后使用`torch.triu()`函数获取主对角线以上的所有元素，其余默认为`False`。这会产生一个因果掩码，我们可以将其用作transformer的`tgt_mask`：它将使用此掩码进行掩码自注意力层。或者，你也可以调用`nn.Transformer.generate_square_subsequent_mask()`来创建因果掩码：只需传递序列长度（`pair.tgt_token_ids.size(1)`）并设置`dtype=torch.bool`。
- en: We then call the transformer, passing it the source and target embeddings, as
    well as all the appropriate masks.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们调用transformer，传递给它源和目标嵌入以及所有适当的掩码。
- en: Lastly, we pass the result through the output `Linear` layer, and we permute
    the last two dimensions because `nn.CrossEntropyLoss` expects the class dimension
    to be dimension 1.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将结果通过输出`Linear`层传递，并交换最后两个维度，因为`nn.CrossEntropyLoss`期望类别维度是维度1。
- en: 'We can now create an instance of this model and train it exactly like our RNN
    encoder-decoder in [Chapter 14](ch14.html#nlp_chapter). To speed up training and
    reduce overfitting, you can shrink the transformer quite a bit—use 4 heads instead
    of 8, just 2 layers in both the encoder and the decoder, and use an embedding
    size of 128:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建这个模型的实例并像我们在[第14章](ch14.html#nlp_chapter)中的RNN编码器-解码器一样训练它。为了加快训练速度并减少过拟合，你可以将transformer缩小很多——使用4个头而不是8个，编码器和解码器都只有2层，并且使用128的嵌入大小：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s see how well this model performs:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的表现如何：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Great, even this tiny transformer trained for 20 epochs works rather well, so
    imagine a much bigger one trained on a much larger dataset, and you can start
    to see how ChatGPT and its friends can be so impressive.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，即使这个经过20个epoch训练的微型transformer也运行得相当好，所以想象一下在一个更大的数据集上训练的更大型的transformer，你就可以开始看到ChatGPT及其朋友们为何如此令人印象深刻。
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Before we move on to other models, it’s important to clean up the GPU RAM, or
    else it will quickly become saturated. For this, delete all variables that are
    no longer needed—especially models, optimizers, tensors, and datasets—using the
    `del` keyword, then call the `gc.collect()` function to run Python’s garbage collector.
    When using a CUDA or AMD device, you must also call `torch.cuda.empty_cache()`.
    On Colab, you can view the available GPU RAM by selecting Runtime → “View resources”
    from the menu.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续探讨其他模型之前，清理GPU RAM是很重要的，否则它很快就会变得饱和。为此，使用`del`关键字删除所有不再需要的变量——特别是模型、优化器、张量和数据集——然后调用`gc.collect()`函数来运行Python的垃圾回收器。当使用CUDA或AMD设备时，还必须调用`torch.cuda.empty_cache()`。在Colab上，你可以通过从菜单中选择“运行”→“查看资源”来查看可用的GPU
    RAM。
- en: Now that you have a good understanding of the original Transformer architecture,
    let’s look at encoder-only transformers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经很好地理解了原始Transformer架构，让我们看看仅编码器Transformer。
- en: Encoder-Only Transformers for Natural Language Understanding
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于自然语言理解的仅编码器Transformer
- en: 'When Google released the [BERT model in 2018](https://homl.info/bert),⁠^([7](ch15.html#id3475))
    it proved that an encoder-only transformer can tackle a wide variety of natural
    language tasks: sentence classification, token classification, multiple choice
    question answering, and more! BERT also confirmed the effectiveness of self-supervised
    pretraining on a large corpus for transfer learning: BERT can indeed achieve excellent
    performance on many tasks, just by fine-tuning on a fairly small dataset for each
    task. Let’s start by looking at BERT’s architecture, then we’ll look at how it
    was pretrained, and how you can fine-tune it for your own tasks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当谷歌在2018年发布了[BERT模型](https://homl.info/bert)时，⁠^([7](ch15.html#id3475))，它证明了仅编码器的Transformer可以处理各种自然语言任务：句子分类、标记分类、多项选择题回答等等！BERT还证实了在大语料库上进行自监督预训练对迁移学习是有效的：BERT确实可以在许多任务上实现优异的性能，只需在每个任务上对相当小的数据集进行微调即可。让我们首先看看BERT的架构，然后我们将看看它是如何进行预训练的，以及如何为你自己的任务进行微调。
- en: Warning
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Encoder-only models are generally not used for text generation tasks, such as
    autocompletion, translation, summarization, or chatbots, because they’re much
    slower at this task than decoders. Decoders are faster because they are causal,
    so a good implementation can cache and reuse its previous state when predicting
    a new token. Conversely, encoders use nonmasked multi-head attention layers only,
    so they are naturally bidirectional; hence the B in BERT (Bidirectional Encoder
    Representations from Transformers). Whenever a new token is added, everything
    needs to be recomputed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仅编码器模型通常不用于文本生成任务，如自动补全、翻译、摘要或聊天机器人，因为它们在这个任务上比解码器慢得多。解码器更快，因为它们是因果的，所以一个好的实现可以在预测新标记时缓存和重用其先前状态。相反，编码器仅使用非掩码的多头注意力层，因此它们自然是双向的；这就是BERT中的B（双向Transformer编码器表示）。每当添加新标记时，都需要重新计算。
- en: BERT’s Architecture
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT的架构
- en: 'BERT’s architecture is almost identical to the original Transformer’s encoder,
    with just three differences:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的架构几乎与原始Transformer的编码器完全相同，只有三个区别：
- en: It’s much bigger. BERT-base has 12 encoder blocks, 12 attention heads, and 768-dimensional
    embeddings, and BERT-large has 24 blocks, 16 heads, and 1,024 dimensions (while
    the original Transformer has 6 blocks, 8 heads, and 512 dimensions). It also uses
    trainable positional embeddings and supports input sentences up to 512 tokens.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它要大得多。BERT-base有12个编码器块、12个注意力头和768维的嵌入，而BERT-large有24个块、16个头和1,024维（而原始Transformer有6个块、8个头和512维）。它还使用可训练的位置嵌入，并支持输入长度最多为512个标记的句子。
- en: It applies layer-norm just *before* each sublayer (attention or feedforward)
    rather than *after* each skip connection. This is called *pre-LN*, as opposed
    to *post-LN*, and it ensures that the inputs of each sublayer are normalized,
    which stabilizes training and reduces sensitivity to weight initialization. PyTorch’s
    transformer modules default to post-LN, but they have a `norm_first` argument
    which you can set to `True` if you prefer pre-LN (however, some optimizations
    may not be implemented for pre-LN).
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在每个子层（注意力或前馈）之前而不是之后应用层归一化。这被称为*预-LN*，与*后-LN*相对，这确保了每个子层的输入都被归一化，从而稳定了训练并减少了对于权重初始化的敏感性。PyTorch的transformer模块默认使用后-LN，但它们有一个`norm_first`参数，你可以将其设置为`True`如果你更喜欢预-LN（然而，对于预-LN的一些优化可能没有实现）。
- en: 'It lets you split the input sentence into two *segments* if needed. This is
    useful for tasks that require a pair of input sentences, such as natural language
    inference (i.e., does sentence A entail sentence B?) or multiple choice question
    answering (i.e., given question A, how good is answer B?). To pass two sentences
    to BERT, you must first append a *separation token* [SEP] to each one, then concatenate
    them. Furthermore, a trainable *segment embedding* is added to each token’s representation:
    segment embedding #0 is added to all tokens within segment #0, and segment embedding
    #1 is added to all tokens within segment #1\. In theory, we could have more segments,
    but BERT was only pretrained on inputs composed of one or two segments. Note that
    the positional encodings are also added to each token’s representation, as usual
    (i.e., relative to the full input sequence, not relative to the individual segments).'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果需要，它允许将输入句子分成两个 *段*。这对于需要一对输入句子的任务很有用，例如自然语言推理（即句子 A 是否蕴含句子 B？）或多项选择题回答（即给定问题
    A，答案 B 有多好？）。要将两个句子传递给 BERT，你必须首先在每个句子后附加一个 *分隔标记* [SEP]，然后将它们连接起来。此外，还添加了一个可训练的
    *段嵌入* 到每个标记的表示中：段嵌入 #0 被添加到段 #0 内的所有标记中，段嵌入 #1 被添加到段 #1 内的所有标记中。理论上，我们可以有更多的段，但
    BERT 只在由一个或两个段组成的输入上进行预训练。注意，位置编码也像往常一样添加到每个标记的表示中（即相对于完整输入序列，而不是相对于单个段）。'
- en: That’s all! Now let’s look at how BERT was pretrained.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 那就结束了！现在让我们看看 BERT 是如何进行预训练的。
- en: BERT Pretraining
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 预训练
- en: 'The authors proposed two self-supervised pretraining tasks:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了两个自监督预训练任务：
- en: Masked language model (MLM)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言模型 (MLM)
- en: 'Each token in a sentence has a 15% probability of being replaced with a mask
    token, and the model is trained to predict what the original tokens were. This
    is often called a *cloze task* (i.e., fill in the blanks). For example, if the
    original sentence is “She had fun at the birthday party”, then the model may be
    given the sentence “She [MASK] fun at the [MASK] party” and it must predict the
    original sentence: the loss is only computed on the mask token outputs.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的每个标记有 15% 的概率被替换为掩码标记，模型被训练来预测原始标记是什么。这通常被称为 *完形填空任务*（即填空）。例如，如果原始句子是“她在生日派对上玩得很开心”，那么模型可能会得到句子“她在生日派对上玩得很开心”，然后它必须预测原始句子：损失只计算在掩码标记输出上。
- en: 'To be more precise, some of the masked tokens are not truly masked: 10% are
    instead replaced by random tokens, and 10% are just left alone, neither masked
    nor randomized. Why is that? Well, the random tokens force the model to perform
    well even when mask tokens are absent: this is important since most downstream
    tasks don’t use any mask tokens. As for the untouched tokens, they make the prediction
    trivial, which encourages the model to pay attention to the input token located
    at the position of the token being predicted. Without them, the model would soon
    learn to ignore this token and rely solely on the other tokens.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 更精确地说，一些掩码标记并不是真正被掩码的：10% 被替换为随机标记，10% 则保持不变，既没有被掩码也没有被随机化。为什么是这样呢？好吧，随机标记迫使模型在没有掩码标记的情况下也能表现良好：这对于大多数下游任务来说很重要，因为大多数下游任务都不使用任何掩码标记。至于未更改的标记，它们使得预测变得简单，这鼓励模型关注预测标记位置处的输入标记。如果没有它们，模型很快就会学会忽略这个标记，并仅依赖于其他标记。
- en: Next sentence prediction (NSP)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个句子预测 (NSP)
- en: The model is trained to predict whether two sentences are consecutive or not.
    For example, it should predict that “The dog sleeps” and “It snores loudly” are
    consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are
    not consecutive.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型被训练来预测两个句子是否连续。例如，它应该预测“狗在睡觉”和“它大声打鼾”是连续的句子，而“狗在睡觉”和“地球围绕太阳转”则不是连续的。
- en: 'This is a binary classification task, which the authors chose to implement
    by introducing a new *class token* [CLS]: this token is inserted at the beginning
    of the input sequence (position #0, segment #0), and during training the encoder’s
    output, this token is passed through a binary classification head (i.e., a linear
    layer with a single unit, followed by the sigmoid function, and trained using
    `nn.BCELoss`, or just a linear layer with a single unit trained using `nn.BCEWith​Lo⁠gitsLoss`).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '这是一个二元分类任务，作者选择通过引入一个新的 *类别标记* [CLS] 来实现：这个标记被插入到输入序列的开头（位置 #0，段 #0），在训练过程中，编码器的输出通过一个二元分类头（即一个单单元的线性层，后跟
    sigmoid 函数，并使用 `nn.BCELoss` 进行训练，或者只是一个使用 `nn.BCEWithLogitsLoss` 训练的单单元线性层）。'
- en: BERT was pretrained on both MLM and NSP simultaneously (see [Figure 15-5](#bert_pretraining_diagram)
    and the left side of [Figure 15-6](#bert_diagram)), using a large corpus of text—specifically
    the English Wikipedia and BooksCorpus. The goal of NSP was to make the class token’s
    contextualized embedding a good representation of the whole input sequence. At
    first, it seemed that it indeed produced good sentence embeddings, but it was
    later shown that simply pooling all the contextualized embeddings (e.g., by computing
    their mean) yielded better results. In fact, researchers showed that NSP did not
    help much overall, so it was dropped in most later architectures.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 同时在 MLM 和 NSP 上进行了预训练（参见 [图 15-5](#bert_pretraining_diagram) 和 [图 15-6](#bert_diagram)
    的左侧），使用大量的文本语料库——具体来说是英文维基百科和 BooksCorpus。NSP 的目标是使类标记的上下文嵌入成为整个输入序列的良好表示。起初，它似乎确实产生了好的句子嵌入，但后来发现简单地池化所有上下文嵌入（例如，通过计算它们的平均值）会产生更好的结果。事实上，研究人员表明，NSP
    在整体上帮助不大，因此它在大多数后来的架构中被删除。
- en: In [Chapter 14](ch14.html#nlp_chapter), we saw how to use the Transformers library
    to download a pretrained BERT model and its tokenizer. But you may want to train
    a BERT model from scratch, for example, if you’re dealing with a domain-specific
    corpus of text. For this, one option is to build BERT yourself using the `nn.TransformerEncoder`
    module (e.g., based on an `nn.TransformerEncoderLayer` with `norm_first=True`
    to respect BERT’s architecture), then preprocess your dataset according to the
    MLM algorithm, and train your model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 14 章](ch14.html#nlp_chapter) 中，我们看到了如何使用 Transformers 库下载预训练的 BERT 模型和它的分词器。但你可能想从头开始训练一个
    BERT 模型，例如，如果你处理的是一个特定领域的文本语料库。为此，一个选择是使用 `nn.TransformerEncoder` 模块自己构建 BERT（例如，基于具有
    `norm_first=True` 的 `nn.TransformerEncoderLayer` 以尊重 BERT 的架构），然后根据 MLM 算法预处理你的数据集，并训练你的模型。
- en: '![Diagram illustrating BERT pretraining showing input and target sequences
    for MLM and NSP tasks, highlighting masked, random, and unchanged tokens.](assets/hmls_1505.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![说明 BERT 预训练的图，展示了 MLM 和 NSP 任务的输入和目标序列，突出显示掩码、随机和未更改的标记。](assets/hmls_1505.png)'
- en: Figure 15-5\. Input and target during BERT pretraining, using MLM and NSP
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-5\. BERT 预训练期间的输入和目标，使用 MLM 和 NSP
- en: 'However, there’s an easier way, using the Transformers library. Let’s start
    by creating a tokenizer and a randomly initialized BERT model. For simplicity,
    we use a pretrained tokenizer, but of course you can train one from scratch instead,
    if you prefer. Make sure to tweak the `BertConfig` depending on your training
    budget, and the size and complexity of your dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个更简单的方法，使用 Transformers 库。让我们先创建一个分词器和一个随机初始化的 BERT 模型。为了简单起见，我们使用一个预训练的分词器，但当然，如果你愿意，也可以从头开始训练一个。确保根据你的训练预算、数据集的大小和复杂性调整
    `BertConfig`：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, let’s download the WikiText dataset (in real life, you would use your
    own dataset instead), and tokenize it:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们下载 WikiText 数据集（在现实生活中，你会使用自己的数据集），并对它进行分词：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is where MLM comes in. We create a data collator, whose role is to bundle
    samples into batches, and we set its `mlm` argument to `True` to activate MLM,
    and also set `mlm_probability=0.15`: each token has a 15% probability of being
    masked (or possibly randomized or left alone, as we just discussed). We also pass
    the tokenizer to the collator: it will not be used to tokenize the text—we’ve
    already done that—but it lets the data collator know the masking and padding token
    IDs, as well as the vocabulary size (which is needed to sample random token IDs).
    With that, we just need to specify the `TrainingArguments`, pass everything to
    the `Trainer`, and call its `train()` method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 MLM 发挥作用的地方。我们创建了一个数据合并器，其作用是将样本捆绑成批次，并将它的 `mlm` 参数设置为 `True` 以激活 MLM，同时将
    `mlm_probability` 设置为 `0.15`：每个标记有 15% 的概率被掩码（或可能随机化或保持不变，正如我们刚才讨论的）。我们还把分词器传递给合并器：它将不会被用来分词文本——我们已经做了这件事——但它让数据合并器知道掩码和填充标记的
    ID，以及词汇表的大小（这是为了采样随机标记 ID 所必需的）。有了这些，我们只需要指定 `TrainingArguments`，将所有内容传递给 `Trainer`，并调用它的
    `train()` 方法：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once your model is pretrained, you can try it out using the pipelines API:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型完成了预训练，你可以尝试使用 pipelines API 来测试它：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: What? Rome is not the capital of a comma! The model is actually terrible because
    we only trained it for a single epoch here, just to confirm that everything works
    and the loss goes down. To get better results, we would need to train it for a
    *very* long time. The BERT authors trained it for about 4 days using 16 TPU devices
    on a much larger dataset. This is why most people avoid starting from scratch
    unless they really have to; you’re generally better off downloading a model that
    was pretrained on a text corpus as close as possible to yours, then fine-tuning
    it on your own dataset. This can be done using MLM, like we just did, but starting
    from a pretrained model instead. Once you’re happy with your pretrained model,
    you can fine-tune it on your target task. Let’s see how.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 什么？逗号不是罗马的首都！实际上，这个模型真的很糟糕，因为我们在这里只训练了一个epoch，只是为了确认一切正常并且损失下降。为了获得更好的结果，我们需要训练很长时间。BERT的作者使用16个TPU设备在一个更大的数据集上训练了大约4天。这就是为什么大多数人除非真的需要，否则会避免从头开始；通常，下载一个在尽可能接近你的文本语料库上预训练的模型，然后在你的数据集上进行微调会更好。这可以通过MLM完成，就像我们刚才做的那样，但从一个预训练模型开始。一旦你对预训练模型满意，你就可以在目标任务上对其进行微调。让我们看看如何操作。
- en: BERT Fine-Tuning
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT微调
- en: BERT can be fine-tuned for many different tasks, changing very little for each
    task (see the righthand side of [Figure 15-6](#bert_diagram)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: BERT可以针对许多不同的任务进行微调，对每个任务的变化非常小（参见[图15-6](#bert_diagram)的右侧）。
- en: '![Diagram illustrating the BERT model''s pre-training on the left and fine-tuning
    on the right, highlighting different tasks achieved by modifying the classification
    head.](assets/hmls_1506.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图15-6. 展示BERT模型在左侧进行预训练和在右侧进行微调的过程，突出通过修改分类头实现的不同任务](assets/hmls_1506.png)'
- en: Figure 15-6\. BERT pre-training (left) and fine-tuning process (right)⁠^([8](ch15.html#id3498))
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-6. BERT预训练（左侧）和微调过程（右侧）⁠^([8](ch15.html#id3498))
- en: 'For sentence classification tasks such as sentiment analysis, all output tokens
    are ignored except for the first one, which corresponds to the class token, and
    a new classification head replaces the NSP binary classification head (see the
    lefthand side of [Figure 15-7](#bert_fine_tuning_diagram)). You can then fine-tune
    the whole model using the cross-entropy loss, optionally setting a lower learning
    rate for the lower layers, or freezing BERT altogether during the first few epochs
    (i.e., training only the new classification head). Using the exact same approach,
    you can tackle other sentence classification tasks. For example, the authors demonstrated
    that fine-tuning BERT yields excellent results on the CoLA dataset, which asks
    whether a sentence is grammatically correct. Try it out on your own sentence classification
    tasks: it’s likely to perform well even if your dataset is quite small, thanks
    to the magic of transfer learning.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子分类任务，如情感分析，除了对应于类别标记的第一个输出标记外，所有输出标记都被忽略，并用新的分类头替换NSP二进制分类头（参见[图15-7](#bert_fine_tuning_diagram)的左侧）。然后，你可以使用交叉熵损失对整个模型进行微调，可选地为底层设置更低的学习率，或者在最初的几个epoch中冻结BERT（即只训练新的分类头）。使用完全相同的方法，你可以处理其他句子分类任务。例如，作者展示了在CoLA数据集上微调BERT可以获得非常好的结果，该数据集要求判断句子是否语法正确。在你的句子分类任务上试一试：即使你的数据集相当小，由于迁移学习的魔力，它也很可能表现良好。
- en: Tip
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The BERT authors found that adding the MLM loss to the fine-tuning loss (scaled
    by a hyperparameter) helps stabilize training and reduces overfitting.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者发现，将MLM损失（通过超参数缩放）添加到微调损失中有助于稳定训练并减少过拟合。
- en: For token classification, the classification head is applied to every token
    (see the righthand side of [Figure 15-7](#bert_fine_tuning_diagram)). For example,
    BERT can be fine-tuned for *named entity recognition* (NER), where the model tags
    the parts of the text that correspond to names, dates, places, organizations,
    or other *entities*. This is often used in legal, financial, or medical applications.
    The same approach can be used for other token classification tasks, such as tagging
    grammatical errors; analyzing sentiment at the token level; locating subjects,
    nouns, and verbs (this is *part-of-speech tagging*); or locating questions, statements,
    and greetings (this is *dialogue act tagging*); and more.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标记分类，分类头应用于每个标记（参见[图15-7](#bert_fine_tuning_diagram)的右侧）。例如，BERT可以被微调用于*命名实体识别*（NER），其中模型标记文本中对应于名称、日期、地点、组织或其他*实体*的部分。这通常用于法律、金融或医疗应用。同样的方法也可以用于其他标记分类任务，例如标记语法错误；在标记级别分析情感；定位主语、名词和动词（这是*词性标注*）；或定位问题、陈述和问候语（这是*对话行为标注*）；等等。
- en: '![Diagram showing BERT fine-tuning: left side illustrates sentence classification
    for sentiment analysis, right side shows token classification for named entity
    recognition (NER).](assets/hmls_1507.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![BERT微调示意图：左侧展示用于情感分析的句子分类，右侧展示用于命名实体识别（NER）的标记分类](assets/hmls_1507.png)'
- en: Figure 15-7\. Fine-tuning BERT for sentence classification such as sentiment
    analysis (left) or for token classification such as NER (right)
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-7\. 微调BERT进行句子分类，如情感分析（左侧）或标记分类，如NER（右侧）
- en: BERT can also be used to classify pairs of sentences. It works exactly like
    sentence classification, except that you pass in two sentences instead of one.
    For example, this can be used for *natural language inference* (NLI) where the
    model must determine whether sentence A entails sentence B, or contradicts it,
    or neither (e.g., the *multi-genre NLI* dataset, or MNLI). It can also be used
    to detect whether two sentences have the same meaning, are just paraphrasing each
    other (e.g., the QQP or MRPC datasets), or to determine whether the answer to
    question A is present in sentence B (e.g., QNLI dataset).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: BERT还可以用于对句子对进行分类。它的工作方式与句子分类完全相同，只是你传递两个句子而不是一个。例如，这可以用于*自然语言推理*（NLI），其中模型必须确定句子A是否蕴涵句子B，或与之矛盾，或两者都不是（例如，*多体裁NLI*数据集，或MNLI）。它还可以用于检测两个句子是否有相同的意思，是否只是相互释义（例如，QQP或MRPC数据集），或者确定问题A的答案是否存在于句子B中（例如，QNLI数据集）。
- en: 'For *multiple choice question answering* (MCQA), BERT is called once for each
    possible answer, placing the question in segment #0 and the possible answer in
    segment #1\. For each answer, the class token output is passed through a linear
    layer with a single unit, producing a score. Once we have all the answer scores,
    we can convert them to probabilities using a softmax layer (see [Figure 15-8](#multiple_choice_diagram)),
    and we can use the cross-entropy loss for fine-tuning (or better, drop the softmax
    layer and use the `nn.CrossEntropyLoss` directly on the answer scores).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*多项选择题回答*（MCQA），BERT对每个可能的答案调用一次，将问题放入段#0，将可能的答案放入段#1。对于每个答案，类标记输出通过一个单单元的线性层传递，产生一个分数。一旦我们有了所有答案的分数，我们可以使用softmax层将它们转换为概率（参见[图15-8](#multiple_choice_diagram)），并且我们可以使用交叉熵损失进行微调（或者更好，省略softmax层，直接在答案分数上使用`nn.CrossEntropyLoss`）。
- en: '![Diagram illustrating the use of an encoder-only model, like BERT, to answer
    multiple-choice questions by generating scores for each option and applying a
    softmax layer to convert them to probabilities.](assets/hmls_1508.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![说明仅编码器模型（如BERT）如何通过为每个选项生成分数并应用softmax层将它们转换为概率来回答多项选择题的示意图](assets/hmls_1508.png)'
- en: Figure 15-8\. Using an encoder-only model to answer multiple-choice questions
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-8\. 使用仅编码器模型回答多项选择题
- en: 'BERT is also great for *extractive question answering*: you ask it a question
    (in segment #0) about some text called the *context* (in segment #1), and BERT
    must locate the answer within the context. For this, you can add a linear layer
    with two units on top of BERT to output two scores per token: a start score and
    an end score. During fine-tuning, you can treat them as logits for two separate
    binary classification tasks: the first determines whether a token is the first
    token in the answer, and the second determines whether it is the last. Of course
    most tokens are neither, and it’s possible for one token to be both if the answer
    is a single token. At inference time, we select the pair of indices *i* and *j*
    that maximizes the sum of the start score of token *i* and the end score of token
    *j*, subject to *i* ≤ *j* and *j* – *i* + 1 ≤ maximum acceptable answer length.
    This approach allowed BERT to beat the state of the art on the SQuAD dataset,
    a popular question answering dataset.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在*抽取式问答*方面也非常出色：你向它提出一个问题（在段#0），关于一些称为*上下文*（在段#1）的文本，BERT必须在上下文中定位答案。为此，你可以在BERT之上添加一个带有两个单位的线性层，为每个标记输出两个分数：一个起始分数和一个结束分数。在微调期间，你可以将它们视为两个独立的二元分类任务的logits：第一个确定一个标记是否是答案的第一个标记，第二个确定它是否是最后一个。当然，大多数标记都不是，如果答案是单个标记，一个标记可能同时是两者。在推理时间，我们选择一对索引*i*和*j*，使得标记*i*的起始分数与标记*j*的结束分数之和最大化，同时满足*i*
    ≤ *j*和*j* – *i* + 1 ≤ 最大可接受答案长度。这种方法使得BERT在SQuAD数据集上击败了当时最先进的技术，SQuAD是一个流行的问答数据集。
- en: Tip
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The Transformers library provides convenient classes and checkpoints for each
    of these use cases, such as `BertForSequenceClassification` or `BertForQuestionAnswering`
    (see [Chapter 14](ch14.html#nlp_chapter)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库为这些用例中的每一个都提供了方便的类和检查点，例如`BertForSequenceClassification`或`BertForQuestionAnswering`（见[第14章](ch14.html#nlp_chapter)）。
- en: 'The BERT authors also showed that BERT could be fine-tuned to measure *semantic
    textual similarity* (STS); for example, on the *STS benchmark* dataset (STS-B),
    you feed the model two sentences and it outputs a score that indicates how semantically
    similar the sentences are. That said, if you want to find the most similar pair
    of sentences in a dataset containing *N* sentences, you will need to run BERT
    on *O*(*N*²) pairs: this could take hours if the dataset is large. Instead, it’s
    preferable to use a model such as [Sentence-BERT (SBERT)](https://homl.info/sbert)⁠^([9](ch15.html#id3515))
    which is a variant of BERT that was fine-tuned to produce good sentence embeddings.
    Start by running each sentence through SBERT to get its sentence embedding, then
    measure the similarity between each pair of sentence embeddings using a similarity
    measure such as the *cosine similarity* (e.g., using PyTorch’s `F.cosine_similarity()`
    function). This is the cosine of the angle between two vectors, so its value ranges
    from –1 (completely opposite) to +1 (perfectly aligned). Since measuring the cosine
    similarity is much faster than running BERT, and since the model processes much
    shorter inputs (i.e., sentences rather than sentence pairs), the whole process
    will take seconds rather than hours.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者还展示了BERT可以被微调来测量*语义文本相似度*（STS）；例如，在*STS基准*数据集（STS-B）中，你向模型输入两个句子，它输出一个分数，表示这两个句子在语义上的相似程度。但话虽如此，如果你想在包含*N*个句子的数据集中找到最相似的句子对，你需要对*N*²对运行BERT：如果数据集很大，这可能需要数小时。相反，最好使用像[Sentence-BERT
    (SBERT)](https://homl.info/sbert)⁠^([9](ch15.html#id3515))这样的模型，它是一种BERT的变体，经过微调以产生良好的句子嵌入。首先，通过SBERT运行每个句子以获取其句子嵌入，然后使用诸如*余弦相似度*（例如，使用PyTorch的`F.cosine_similarity()`函数）之类的相似度度量来测量每个句子嵌入对之间的相似度。这是两个向量之间角度的余弦值，因此其值范围从-1（完全相反）到+1（完全对齐）。由于测量余弦相似度比运行BERT快得多，并且模型处理的是更短的输入（即句子而不是句子对），整个过程将只需几秒钟而不是几小时。
- en: 'Sentence embedding can also be extremely useful in many other applications:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 句子嵌入在许多其他应用中也非常有用：
- en: Text clustering, to organize and better understand your data
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类，以组织和更好地理解你的数据
- en: You can process a large number of documents through SBERT to obtain their sentence
    embeddings, then apply a clustering algorithm such as *k*-means or HDBSCAN (see
    [Chapter 8](ch08.html#unsupervised_learning_chapter)) on the embeddings to group
    your documents based on semantic similarity. It often helps to reduce dimensionality
    before running the clustering algorithm, for example, using PCA or UMAP (see [Chapter 7](ch07.html#dimensionality_chapter)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过SBERT处理大量文档以获得它们的句子嵌入，然后对嵌入应用聚类算法，如*k*-means或HDBSCAN（见[第8章](ch08.html#unsupervised_learning_chapter)），根据语义相似度对文档进行分组。在运行聚类算法之前，通常有助于降低维度，例如，使用PCA或UMAP（见[第7章](ch07.html#dimensionality_chapter)）。
- en: Semantic search
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索
- en: The goal is to let the user find documents based on the query’s meaning rather
    than just keyword matching. First, encode your documents (or chunks of documents)
    using SBERT and store the sentence embeddings. When a user submits a search query,
    encode it using SBERT and find the documents whose embeddings are most similar
    to the query’s embedding, for example, based on cosine similarity.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让用户根据查询的意义而不是仅仅基于关键词匹配来找到文档。首先，使用SBERT对文档（或文档块）进行编码并存储句子嵌入。当用户提交搜索查询时，使用SBERT对其进行编码，并找到与查询嵌入最相似的文档，例如，基于余弦相似度。
- en: Reranking search results
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序搜索结果
- en: If you have an existing search system that you don’t want to replace, you can
    often improve it significantly by reranking the search results based on semantic
    similarity with the query.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个现有的搜索系统，你不想替换它，你通常可以通过根据查询的语义相似度重新排序搜索结果来显著改进它。
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Vector databases, such as Pinecone, Weaviate, ChromaDB, Qdrant, or Milvus, are
    designed for storing and searching for documents based on their embeddings. More
    traditional databases, such as PostgreSQL or MongoDB, also have growing support
    for embeddings, although it’s not as optimized yet.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库，如Pinecone、Weaviate、ChromaDB、Qdrant或Milvus，旨在根据其嵌入存储和搜索文档。更传统的数据库，如PostgreSQL或MongoDB，也对嵌入提供了越来越多的支持，尽管目前还没有得到优化。
- en: 'Over the years, many variants of SBERT have been released. One of the easiest
    ways to download and use them is via the [Sentence Transformers library](https://sbert.net)
    created by UKPLab and maintained by Hugging Face (it’s preinstalled on Colab).
    For example, the following code downloads the all-MiniLM-L6-v2 model, which is
    very fast and lightweight but still produces high-quality sentence embeddings.
    The code uses it to encode three sentences, then it measures the similarity between
    each pair of sentences:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多SBERT的变体已经发布。其中一种下载和使用它们的最简单方法是通过UKPLab创建并由Hugging Face维护的[Sentence Transformers库](https://sbert.net)。例如，以下代码下载了all-MiniLM-L6-v2模型，这个模型非常快且轻量，但仍然能产生高质量的句子嵌入。代码使用它来编码三个句子，然后测量每对句子之间的相似度：
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s look at the similarity matrix:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看相似度矩阵：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We see that there are 1s in the main diagonal, confirming that each sentence
    is perfectly similar to itself, and we also see that “She’s shopping” is more
    similar to “She bought some shoes” (the cosine similarity is 0.6328) than to “She’s
    working” (0.5841).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到主对角线上的1，这证实了每个句子与其自身完美相似，我们还看到“她正在购物”与“她买了一些鞋子”的相似度更高（余弦相似度为0.6328），而不是与“她正在工作”的相似度（0.5841）。
- en: Now that we’ve examined BERT in detail, let’s look at some of its offspring.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细研究了BERT，让我们看看它的后代。
- en: Other Encoder-Only Models
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他仅编码器模型
- en: Following Google’s footsteps, many organizations released their own encoder-only
    models. Let’s look at the most popular ones and discuss their main innovations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随Google的脚步，许多组织发布了他们自己的仅编码器模型。让我们看看最受欢迎的几个，并讨论它们的主要创新。
- en: RoBERTa by Facebook AI, July 2019 (125M to 355M parameters)
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Facebook AI 的 RoBERTa，2019年7月（参数量从125M增加到355M）
- en: This model is similar to BERT but its performance is better across the board
    in large part because it was pretrained for longer and on a larger dataset. MLM
    was used for pretraining, but NSP was dropped. Importantly, the authors used *dynamic
    masking*, meaning that the tokens to mask were masked on the fly *during* training
    rather than just once before training (as BERT did), so the same piece of text
    is masked differently across different epochs. This provides the model with more
    data diversity, reducing overfitting and leading to better generalization.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与BERT相似，但它在各方面的性能都更好，这在很大程度上是因为它在更大的数据集上进行了更长时间的预训练。预训练使用了MLM，但放弃了NSP。重要的是，作者使用了*动态掩码*，这意味着在训练过程中动态地掩码要掩码的标记，而不是像BERT那样在训练之前只掩码一次，因此同一篇文本在不同的epoch中会有不同的掩码方式。这为模型提供了更多数据多样性，减少了过拟合，并导致了更好的泛化。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When we fine-tuned BERT earlier in this chapter, we actually used dynamic masking
    and we dropped NSP, so we were following RoBERTa’s pretraining approach.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本章早期微调BERT时，我们实际上使用了动态掩码并放弃了NSP，因此我们遵循了RoBERTa的预训练方法。
- en: DistilBERT by Hugging Face, October 2019 (66M)
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face的DistilBERT，2019年10月（66M）
- en: 'This model is a scaled-down version of BERT: it’s 40% smaller and 60% faster,
    yet it manages to reach about 97% of BERT’s performance on most tasks, making
    it a great choice for low-resource environments (e.g., mobile devices), for low-latency
    applications, or for quick fine-tuning.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是BERT的缩小版：它比BERT小40%，速度快60%，但仍然在大多数任务上达到了BERT大约97%的性能，使其成为低资源环境（例如，移动设备）、低延迟应用或快速微调的理想选择。
- en: 'As its name suggests, DistilBERT was trained using a technique called *model
    distillation*, first introduced by Geoffrey Hinton et al. [in 2015](https://homl.info/distillation).⁠^([10](ch15.html#id3531))
    The idea of distillation is to train a small *student model* (e.g., DistilBERT)
    using the estimated probabilities from a larger *teacher model* (e.g., BERT) as
    the targets (see [Figure 15-9](#distilbert_diagram)). These are *soft targets*
    rather than the usual one-hot vectors: it makes training much faster and more
    data efficient, as it allows the student to directly aim for the correct distribution,
    rather than having to learn it over many samples, bouncing between one extreme
    and the other and slowly settling somewhere in between. As a result, distillation
    often works better than training the student from scratch on the same dataset
    as the teacher!'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，DistilBERT是使用称为*模型蒸馏*的技术进行训练的，该技术最早由Geoffrey Hinton等人于2015年[提出](https://homl.info/distillation)。⁠^([10](ch15.html#id3531))
    蒸馏的想法是使用来自更大的*教师模型*（例如BERT）的估计概率作为目标来训练一个小的*学生模型*（例如DistilBERT）（参见[图15-9](#distilbert_diagram)）。这些是*软目标*而不是通常的one-hot向量：这使得训练更快、更高效，因为它允许学生直接针对正确的分布，而不是需要在许多样本中学习它，在两个极端之间弹跳，并逐渐在某处稳定下来。因此，蒸馏通常比在教师相同的数据集上从头开始训练学生效果更好！
- en: '![Diagram illustrating DistilBERT pretraining using distillation losses from
    a teacher model (BERT) and a masked language modeling (MLM) loss.](assets/hmls_1509.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图示DistilBERT使用来自教师模型（BERT）的蒸馏损失和掩码语言建模（MLM）损失进行预训练。](assets/hmls_1509.png)'
- en: Figure 15-9\. DistilBERT pretraining using a weighted sum of two distillation
    losses and the MLM loss
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-9. 使用两个蒸馏损失和MLM损失的加权总和进行DistilBERT预训练
- en: Note that the estimated probabilities for both the teacher and the student are
    smoothed a bit—during training only—by dividing the final logits by a temperature
    greater than 1 (typically 2). This provides the student with a more nuanced signal
    covering all possible options, rather than just focusing on the correct answer.
    Hinton dubbed this *dark knowledge*. For example, if the input is “It’s sunny
    and I feel [MASK]”, the teacher might normally estimate that the masked word has
    a 72% probability of being “great”, and 27% of being “good”, and just 0.5% of
    being “bad”. But if we apply a temperature of 2, then these probabilities get
    smoothed out to about 60%, 36%, and 5%, respectively. It’s helpful to know that
    “bad” is a plausible option here, even if it’s unlikely.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，教师和学生两者的估计概率在训练过程中都会稍微平滑一些——通过将最终logits除以大于1的温度（通常是2）。这为学生提供了一个更细致的信号，涵盖了所有可能的选择，而不仅仅是关注正确答案。Hinton将其称为“暗知识”。例如，如果输入是“天气晴朗，我感觉很”，教师可能会通常估计被掩码的词有72%的概率是“好”，27%的概率是“很好”，只有0.5%的概率是“坏”。但如果我们应用2的温度，那么这些概率会平滑到大约60%，36%和5%。了解“坏”在这里是一个可能的选项是有帮助的，即使它不太可能。
- en: 'DistilBERT’s training loss also had two more components: the standard MLM loss,
    as well as a *cosine embedding loss* which minimizes the cosine similarity between
    the student’s and teacher’s final hidden states (i.e., the output embeddings just
    before the classification head). This encourages the student to “think” like the
    teacher, not just make the same predictions, and it leads to faster convergence
    and better performance. Later models, such as TinyBERT, pushed this idea further
    by aligning other internal states, such as the attention weights. DistilBERT’s
    final loss is a weighted sum of the three losses (the authors used weights α=5,
    β=2, γ=1).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 的训练损失还包括两个额外组成部分：标准的 MLM 损失，以及一个 *余弦嵌入损失*，该损失最小化学生和教师最终隐藏状态（即分类头之前的输出嵌入）之间的余弦相似度。这鼓励学生“思考”像教师一样，而不仅仅是做出相同的预测，这有助于更快地收敛和更好的性能。后续模型，如
    TinyBERT，通过对齐其他内部状态（如注意力权重）进一步推进了这个想法。DistilBERT 的最终损失是三个损失（作者使用了权重 α=5，β=2，γ=1）的加权总和。
- en: ALBERT by Google Research, December 2019 (12M–235M)
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ALBERT 由谷歌研究团队发布于 2019 年 12 月（12M–235M）
- en: All encoder layers in this model share the same weights, making it much smaller
    than BERT, but not faster. This makes it great for use cases where memory size
    is limited. In particular, it’s a good model to use if you want to train an encoder-only
    model from scratch on a GPU with little VRAM.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型中所有编码层共享相同的权重，这使得它比 BERT 小得多，但速度并不更快。这使得它在内存大小有限的使用场景中非常适用。特别是，如果你想在具有少量
    VRAM 的 GPU 上从头开始训练一个仅编码器模型，这是一个很好的模型。
- en: 'ALBERT also introduced *factorized embeddings* to reduce the size of the embedding
    layer: in BERT-large, the vocabulary size is about 30,000, and the embedding size
    is 1,024, which means that the embedding matrix has over 30 million parameters!
    ALBERT replaces this huge matrix with the product of two much smaller matrices
    (see [Figure 15-10](#matrix_factorization_diagram)). In practice, this can be
    implemented by reducing the embedding size—ALBERT uses 128—then adding a linear
    layer immediately after the embedding layer to project the embeddings to the higher
    dimensional space, such as 1,024 dimensions for ALBERT-large. The embedding layer
    ends up with roughly 3.8M parameters (~30,000 × 128), and the linear layer has
    about 0.13M parameters (128 × 1,024), so the total is less than 4M parameters,
    down from over 30M: nice!'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT 还引入了 *分解嵌入* 来减少嵌入层的大小：在 BERT-large 中，词汇量约为 30,000，嵌入大小为 1,024，这意味着嵌入矩阵有超过
    3000 万个参数！ALBERT 将这个巨大的矩阵替换为两个更小矩阵的乘积（参见[图 15-10](#matrix_factorization_diagram)）。在实践中，这可以通过减少嵌入大小来实现——ALBERT
    使用 128，然后在嵌入层之后立即添加一个线性层，将嵌入投影到更高维的空间，例如 ALBERT-large 的 1,024 维。嵌入层最终有大约 380 万个参数（~30,000
    × 128），线性层大约有 130 万个参数（128 × 1,024），因此总参数量不到 400 万，从超过 3000 万减少：太棒了！
- en: '![Diagram illustrating the reduction of a large embedding matrix into smaller
    embeddings combined with a linear layer to achieve dimensional projection.](assets/hmls_1510.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![展示将大型嵌入矩阵减少为较小的嵌入，并结合线性层以实现维度投影的示意图](assets/hmls_1510.png)'
- en: Figure 15-10\. An excessively large embedding matrix can be replaced with the
    product of two smaller matrices. This can be implemented using smaller embeddings
    and projecting them to higher dimensions using a linear layer.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-10\. 可以用两个较小矩阵的乘积来替换一个过大的嵌入矩阵。这可以通过使用较小的嵌入并使用线性层将它们投影到更高维度来实现。
- en: 'ALBERT also replaced NSP with *sentence order prediction* (SOP): given two
    consecutive sentences, the goal is to predict which one comes first. This is a
    much harder task than NSP, and it led to significantly better sentence embeddings.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT 还用 *句子顺序预测*（SOP）替换了 NSP：给定两个连续的句子，目标是预测哪一个先出现。这是一个比 NSP 更困难的任务，并且导致了显著更好的句子嵌入。
- en: ELECTRA by Google Research, March 2020 (14M–335M)
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELECTRA 由谷歌研究团队发布于 2020 年 3 月（14M–335M）
- en: 'This model introduced a new pretraining technique called *replaced token detection*
    (RTD): they trained two models jointly—a small generator model and a larger discriminator
    model, both encoder only. The generator is only used during pretraining, while
    the discriminator is the final model we’re after. The generator is simply trained
    using regular MLM with dynamic masking. For each mask token, a replacement token
    is sampled from its top predictions. The resulting text is fed to the discriminator
    model, which must predict whether each token is the original or not.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型引入了一种新的预训练技术，称为*替换标记检测*（RTD）：他们联合训练了两个模型——一个小型生成模型和一个较大的判别模型，两者都是编码器。生成器仅在预训练期间使用，而判别器是我们最终追求的最终模型。生成器只是使用常规的MLM和动态掩码进行训练。对于每个掩码标记，从其最高预测中采样一个替换标记。生成的文本被输入到判别模型中，该模型必须预测每个标记是否是原始的。
- en: For example (see [Figure 15-11](#rtd_diagram)), if the original text is “She
    likes him” and it is masked as “She [MASK] him”, the generator’s top predictions
    might include “likes”, “loves”, “hears”, “pushes”, and one of these is chosen
    randomly, say “pushes”, so the sentence becomes “She pushes him”. The discriminator
    must then try to predict [1, 0, 1], since the first and third tokens are the same
    as in the original text, but not the second token. As the generator improves,
    the replaced tokens gradually become less and less obviously wrong, forcing the
    discriminator to become smarter and smarter. After training, we can throw away
    the generator and drop the binary classification head from the discriminator to
    get the final model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如（见图15-11），如果原始文本是“她喜欢他”，并且被掩码为“她  他”，生成器的最高预测可能包括“喜欢”、“爱”、“听见”、“推”，其中一个被随机选择，比如“推”，所以句子变成了“她推他”。然后判别器必须尝试预测[1,
    0, 1]，因为第一个和第三个标记与原始文本相同，但第二个标记不同。随着生成器的改进，替换的标记逐渐变得不那么明显地错误，迫使判别器变得越来越聪明。训练完成后，我们可以丢弃生成器，并从判别器中移除二进制分类头，以获得最终模型。
- en: This technique is more sample-efficient than MLM since the discriminator learns
    from more tokens per example, thus it converges faster, generally achieving the
    same performance as larger BERT models. That said, the benefits are not always
    worth the additional complexity.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与MLM相比，这种技术更具有样本效率，因为判别器从每个示例中学习更多标记，因此它收敛得更快，通常可以达到与更大的BERT模型相同的性能。尽管如此，这些好处并不总是值得额外的复杂性。
- en: '![Diagram illustrating the replaced token detection (RTD) process, showing
    how the generator suggests possible replacements for a masked word, and the discriminator
    evaluates the sentence for accuracy.](assets/hmls_1511.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![展示替换标记检测（RTD）过程的图解，显示生成器如何为掩码词提出可能的替换，以及判别器如何评估句子的准确性。](assets/hmls_1511.png)'
- en: Figure 15-11\. Replaced token detection (RTD)
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-11. 替换标记检测（RTD）
- en: DeBERTa by Microsoft, January 2021 (139M–1.5B)
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微软的DeBERTa，2021年1月（139M–1.5B）
- en: 'DeBERTa is a fairly large model that beat the state of the art on many NLU
    tasks. It removes the usual positional embedding layer, and instead uses *relative
    positional embeddings* when computing the attention scores inside every multi-head
    attention layer: when deciding how much the *i*^(th) query token should attend
    to the *j*^(th) key token, the model has access to a learned embedding for the
    relative position *i* – *j*. DeBERTa wasn’t the first model to do this, as we
    will see later in this chapter, but it introduced a variant of this technique—named
    *disentangled attention*—which gives the model more flexibility in how it can
    combine semantic and positional information.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa是一个相当大的模型，在许多NLU任务上击败了当时最先进的模型。它去除了通常的位置嵌入层，并在计算每个多头注意力层内的注意力分数时，使用*相对位置嵌入*：当决定第*i*个查询标记应该关注第*j*个键标记多少时，模型可以访问一个用于相对位置*i*
    – *j*的学习嵌入。DeBERTa并不是第一个这样做模型的，正如我们将在本章后面看到的那样，但它引入了这种技术的变体——称为*解耦注意力*——这为模型在如何结合语义和位置信息方面提供了更多的灵活性。
- en: DeBERTaV3, released in July 2021, combined the ideas from DeBERTa with ELECTRA-style
    RTD, and it reached even higher performance. It remains a popular model for NLU
    tasks to this day. However, disentangled attention adds some complexity and compute
    cost, so subsequent models have opted for simpler approaches, as we will see.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年7月发布的DeBERTaV3结合了DeBERTa和ELECTRA风格的RTD的想法，并达到了更高的性能。它至今仍然是NLU任务中流行的模型。然而，解耦注意力增加了一些复杂性和计算成本，因此后续模型选择了更简单的方法，正如我们将看到的。
- en: More encoder-only models on Hugging Face Hub
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face Hub上的更多仅编码器模型
- en: 'If you explore the encoder-only models on the Hugging Face Hub, you will find
    many variants of the standard models we discussed so far:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你探索Hugging Face Hub上的编码器模型，你会找到我们之前讨论的标准模型的许多变体：
- en: With various sizes (e.g., BERT-base versus BERT-large)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有各种大小（例如，BERT-base与BERT-large）
- en: Pretrained on a non-English language (e.g., CamemBERT for French) or even on
    multiple languages (e.g., IndicBERT for 12 major Indian languages)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非英语语言（例如，用于法语的CamemBERT）或甚至多种语言（例如，用于12种主要印度语言的IndicBERT）上预训练
- en: Pretrained on cased or uncased text
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大小写文本（例如，cased或uncased文本）上预训练
- en: Tweaked for specific tasks (e.g., BERT for question answering)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为特定任务进行调整（例如，BERT用于问答）
- en: 'You will also find many domain-specific models, such as:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会找到许多特定领域的模型，例如：
- en: ClinicalBERT for clinical applications
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ClinicalBERT用于临床应用
- en: SciBERT for scientific applications
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciBERT用于科学应用
- en: PubMedBERT for biomedicine
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PubMedBERT用于生物医学
- en: FinBERT for finance
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FinBERT用于金融
- en: GraphCodeBERT for coding applications
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphCodeBERT用于编码应用
- en: Twitter-RoBERTa-base for social media applications
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter-RoBERTa-base用于社交媒体应用
- en: PatentBERT for patent applications
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PatentBERT用于专利申请
- en: LexLM for legal applications
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LexLM用于法律应用
- en: 'Most of these are simply fine-tuned versions of standard encoder-only models
    such as BERT or RoBERTa, but some were pretrained entirely from scratch. A few
    also introduced new ideas; for example, GraphCodeBERT is a BERT model pretrained
    on code using not only MLM, but also two structure-aware tasks: it has to find
    where in the code each variable was defined and used, and it also has to predict
    the data flow (e.g., in `z = x + y`, variable `z` comes from variables `x` and
    `y`).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些模型仅仅是标准编码器模型（如BERT或RoBERTa）的微调版本，但也有一些是从头开始预训练的。其中一些还引入了新想法；例如，GraphCodeBERT是一个在代码上预训练的BERT模型，它不仅使用MLM，还使用了两个结构感知任务：它必须找到代码中每个变量定义和使用的地方，还必须预测数据流（例如，在`z
    = x + y`中，变量`z`来自变量`x`和`y`）。
- en: 'The Hugging Face Hub also contains many compressed variants of standard models.
    They are small and usually fast, and were trained using distillation, weight-sharing,
    and/or other techniques such as quantization (see [Appendix B](app02.html#precision_appendix)).
    Popular examples include: DistilBERT, TinyBERT, MobileBERT, MiniLM (available
    for various base models), DistilRoBERTa, and MiniDeBERTa-v2\. As we saw with DistilBERT,
    these models are great for low-resource environments, low latency, and quick fine-tuning.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub还包含许多标准模型的压缩变体。它们体积小，通常速度快，并使用蒸馏、权重共享和/或量化等技术进行训练（见[附录B](app02.html#precision_appendix)）。流行的例子包括：DistilBERT、TinyBERT、MobileBERT、MiniLM（适用于各种基础模型）、DistilRoBERTa和MiniDeBERTa-v2。正如我们通过DistilBERT所看到的，这些模型非常适合资源有限的环境、低延迟和快速微调。
- en: 'Speaking of quick fine-tuning, you will also find many *adapter models* on
    the Hugging Face Hub. An adapter model is based on a frozen standard model such
    as BERT, plus some small trainable components called *adapters*: when you fine-tune
    the adapter model, the base model doesn’t change, only the adapters. As a result,
    fine-tuning is much faster and less computationally expensive, and you can get
    great performance on your task using fairly little training data. For example,
    AdapterHub/bert-base-uncased-pf-sst2 is an adapter model based on the bert-base-uncased
    model and fine-tuned for sentiment analysis on the SST 2 dataset. [Chapter 17](ch17.html#speedup_chapter)
    shows how to build and fine-tune your own adapter models.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到快速微调，你也会在Hugging Face Hub上找到许多*适配器模型*。适配器模型基于冻结的标准模型，如BERT，加上一些称为*适配器*的小型可训练组件：当你微调适配器模型时，基础模型不会改变，只有适配器。因此，微调变得更快，计算成本更低，并且你可以使用相当少的训练数据在你的任务上获得很好的性能。例如，AdapterHub/bert-base-uncased-pf-sst2是基于bert-base-uncased模型，并在SST
    2数据集上进行情感分析的适配器模型。[第17章](ch17.html#speedup_chapter)展示了如何构建和微调你自己的适配器模型。
- en: OK, time to step back. We’ve learned all about the Transformer architecture,
    and we even built a translation transformer from scratch, and now we’ve looked
    into encoder-only models like BERT and how they can be used for many different
    NLU tasks. Lastly, we examined the key innovations powering some of the most popular
    encoder-only models, and the main categories of pretrained encoder-only models
    you can find on the Hugging Face Hub (i.e., standard, multilingual, task-specific,
    domain-specific, compressed, and adapter models—these categories are not exclusive).
    It’s now time to look at decoder-only models such as GPT.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在是时候退一步了。我们已经了解了 Transformer 架构，甚至从头开始构建了一个翻译 Transformer，现在我们已经研究了仅编码器模型如
    BERT 以及它们如何用于许多不同的 NLU 任务。最后，我们考察了推动一些最受欢迎的仅编码器模型的关键创新，以及你可以在 Hugging Face Hub
    上找到的预训练仅编码器模型的主要类别（即标准、多语言、特定任务、特定领域、压缩和适配器模型——这些类别不是互斥的）。现在是时候看看仅解码器模型，如 GPT
    了。
- en: Tip
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Over the last few years, large organizations have shifted their focus toward
    decoders, but encoder-only models are still alive and kicking. Their relatively
    small size makes them fast and accessible to all, easy to fine-tune, and immensely
    useful for a wide range of applications.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，大型组织将他们的重点转向了解码器，但仅编码器模型仍然充满活力。它们相对较小的尺寸使它们运行速度快，易于访问，易于微调，并且对于广泛的应用非常有用。
- en: Decoder-Only Transformers
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅解码器 Transformer
- en: 'While Google was working on the first encoder-only model (i.e., BERT), Alec
    Radford and other OpenAI researchers were taking a different route: they built
    the first decoder-only model, named GPT.⁠^([11](ch15.html#id3567)) This model
    paved the way for today’s most impressive models, including most of the ones used
    in famous chatbots like ChatGPT or Claude.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Google 正在研究第一个仅编码器模型（即 BERT）时，Alec Radford 和其他 OpenAI 研究人员正在走不同的路线：他们构建了第一个仅解码器模型，名为
    GPT。⁠^([11](ch15.html#id3567)) 这个模型为今天最令人印象深刻的模型铺平了道路，包括大多数用于著名聊天机器人如 ChatGPT
    或 Claude 的模型。
- en: 'The GPT model (now known as GPT-1) was released in June 2018\. GPT stands for
    *Generative Pre-Training*: it was pretrained on a dataset of about 7,000 books
    and learned to predict the next token, so it can be used to generate text one
    token at a time, just like the original Transformer’s decoder. For example, if
    you feed it “Happy birthday”, it will predict “birthday to”, so you can append
    “to” to the input and repeat the process (see [Figure 15-12](#gpt_generation_diagram)).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型（现称为 GPT-1）于 2018 年 6 月发布。GPT 代表**生成预训练**：它在约 7,000 本书的数据集上进行了预训练，学会了预测下一个标记，因此可以逐个生成文本，就像原始
    Transformer 的解码器一样。例如，如果你给它输入“Happy birthday”，它将预测“birthday to”，因此你可以将“to”添加到输入中并重复此过程（见[图
    15-12](#gpt_generation_diagram)）。
- en: '![Diagram illustrating how a decoder-only model, like GPT, generates text one
    token at a time by predicting the next word in the sequence, such as continuing
    "Happy birthday" with "to you!".](assets/hmls_1512.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图示仅解码器模型（如 GPT）如何通过预测序列中的下一个单词（例如，继续“Happy birthday”为“to you!”）逐个生成文本。](assets/hmls_1512.png)'
- en: Figure 15-12\. Generating text one token at a time using a decoder-only model
    like GPT
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-12\. 使用仅解码器模型如 GPT 逐个生成文本
- en: Decoder-only models are great at *text generation* tasks, such as auto-completion,
    code generation, question answering (including free text answers), math and logical
    reasoning (to some extent), and chatbots. They can also be used for summarization
    or translation, but encoder-decoder models are still popular choices for these
    tasks, as they often have a better understanding of the source text, thanks to
    the encoder. Decoder-only models can also perform text classification quite well,
    but encoder-only models shine in this area, as they are faster and often provide
    a similar performance with a smaller model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器模型在**文本生成**任务上表现优秀，例如自动完成、代码生成、问答（包括自由文本答案）、数学和逻辑推理（在一定程度上）以及聊天机器人。它们也可以用于摘要或翻译，但编码器-解码器模型仍然是这些任务的热门选择，因为编码器通常对源文本有更好的理解。仅解码器模型在文本分类方面也能表现得相当好，但仅编码器模型在这一领域更加出色，因为它们运行速度快，并且通常在更小的模型上提供相似的性能。
- en: Warning
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At inference time, encoder-only models only need to look at their inputs once
    to make their predictions, while decoder-only models require one run per generated
    token (just like the decoder in encoder-decoder models). That’s because decoders
    are autoregressive, so the generation process is sequential. That said, decoders
    can hugely benefit from caching, as I mentioned earlier.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时间，仅编码器模型只需要查看一次输入来做出预测，而仅解码器模型需要为每个生成的标记运行一次（就像编码器-解码器模型中的解码器一样）。这是因为解码器是自回归的，所以生成过程是顺序的。尽管如此，正如我之前提到的，解码器可以从缓存中受益巨大。
- en: In this section, we will look at the architecture of GPT-1 and its successor
    GPT-2, and we will see how decoder-only models like these can be used for various
    tasks. We will also see that these models can perform tasks that they were never
    explicitly trained on (zero-shot learning) or for which they only saw a few examples
    (few-shot learning). Lastly, we will then use the Transformers library to download
    a small decoder-only model (GPT-2) then a large one (Mistral-7B) and use them
    to generate text and answer questions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨GPT-1及其继任者GPT-2的架构，并了解如何使用这些仅解码器模型来完成各种任务。我们还将看到，这些模型可以执行它们从未明确训练过的任务（零样本学习）或只看到少量示例的任务（少量样本学习）。最后，我们将使用Transformers库下载一个小型仅解码器模型（GPT-2）然后是一个大型模型（Mistral-7B），并使用它们来生成文本和回答问题。
- en: GPT-1 Architecture and Generative Pretraining
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-1 架构和生成预训练
- en: During pretraining, GPT-1 was fed batches of 64 sequences randomly sampled from
    the book corpus, and it was trained to predict the next token for every single
    input token. Each sequence was exactly 512 tokens long, so GPT-1 did not need
    any padding token. In fact, it didn’t use special tokens at all during pretraining,
    not even start-of-sequence or end-of-sequence tokens. Compared to BERT, it’s a
    much simpler pretraining process. It also provides the same amount of data for
    every token position, whereas BERT sees less data for the last positions than
    for the first, due to padding.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，GPT-1被喂入从书籍语料库中随机抽取的64个序列批次，并被训练来预测每个输入标记的下一个标记。每个序列恰好有512个标记长，因此GPT-1不需要任何填充标记。实际上，在整个预训练过程中，它根本不使用任何特殊标记，甚至没有起始序列或结束序列标记。与BERT相比，这是一个预训练过程要简单得多。它还为每个标记位置提供相同数量的数据，而BERT由于填充，对于最后的位置看到的数据比第一个位置少。
- en: 'GPT-1’s architecture has two important differences compared to the original
    Transformer’s decoder:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始Transformer的解码器相比，GPT-1的架构有两个重要差异：
- en: 'There’s no cross-attention block since there’s no encoder output to attend
    to: each decoder block only contains a masked multi-head attention layer and a
    two-layer feedforward network (each with its own skip connection and layer norm).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有编码器输出需要关注，所以没有交叉注意力块：每个解码器块只包含一个掩码多头注意力层和两层前馈网络（每个都有自己的跳过连接和层归一化）。
- en: 'It’s much bigger: it has 12 decoder layers instead of 6, the embedding size
    is 768 instead of 512, and it has 12 attention heads instead of 8\. That’s a total
    of 117 million parameters.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它要大得多：它有12个解码器层而不是6个，嵌入大小是768而不是512，并且它有12个注意力头而不是8个。总共有1170万个参数。
- en: Warning
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Counterintuitively, you cannot use PyTorch’s `nn.TransformerDecoder` module
    to build a decoder-only model. That’s because it contains cross-attention layers
    that cannot be easily removed. Instead, you can use the `nn.TransformerEncoder`
    module, and always call it with a causal mask.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 令人意外的是，你不能使用PyTorch的`nn.TransformerDecoder`模块来构建仅解码器模型。这是因为它包含无法轻易移除的交叉注意力层。相反，你可以使用`nn.TransformerEncoder`模块，并且始终使用因果掩码调用它。
- en: Out-of-the-box, GPT-1 was very impressive at text generation. For example, its
    authors asked it to tell the story of a scientist discovering a herd of English-speaking
    unicorns in an unexplored valley, and the story it generated seemed like it had
    been written by a human (you can read it at [*https://homl.info/unicorns*](https://homl.info/unicorns)).
    It’s not quite as impressive today, but back then it was truly mind-blowing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1在文本生成方面一开始就给人留下了深刻的印象。例如，它的作者要求它讲述一个科学家在一个未开发的山谷中发现一群说英语的独角兽的故事，它生成的故事看起来像是人类写的（你可以在[*https://homl.info/unicorns*](https://homl.info/unicorns)上阅读）。今天它并不那么令人印象深刻，但当时它确实令人震惊。
- en: 'The authors also fine-tuned GPT-1 on various tasks, including textual entailment,
    semantic similarity, reading comprehension, or common sense reasoning, and it
    beat the state of the art on many of them, confirming the power of pretraining
    for NLP. For each task, the authors only made minor changes to the architecture:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还在各种任务上对GPT-1进行了微调，包括文本蕴含、语义相似度、阅读理解或常识推理，并在其中许多任务上击败了当时的最佳水平，证实了预训练在NLP中的力量。对于每个任务，作者只对架构进行了微小的修改：
- en: For text classification tasks, a classification head is added on top of the
    last token’s output embedding. See the righthand side of [Figure 15-13](#gpt_training_diagram).
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文本分类任务，在最后一个标记的输出嵌入之上添加了一个分类头。参见[图15-13](#gpt_training_diagram)的右侧。
- en: For entailment and other classification tasks requiring two input sentences,
    the model is fed both sentences separated by a delimiter token (just a regular
    $ sign), and again a classification head is added on top of the last token’s output
    embedding.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于需要两个输入句子的蕴含和其他分类任务，模型被输入两个通过分隔符标记（只是一个普通的 $ 符号）分隔的句子，然后在最后一个标记的输出嵌入之上添加了一个分类头。
- en: 'For semantic similarity, since the order of the two sentences shouldn’t matter,
    the model gets called twice: once with sentence 1 $ sentence 2, and once with
    sentence 2 $ sentence 1\. The last token’s output embeddings for both cases are
    added itemwise and the result is fed to a regression head.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语义相似度，由于两个句子的顺序不应该影响，模型被调用两次：一次是句子1 $ 句子2，另一次是句子2 $ 句子1。两种情况下的最后一个标记的输出嵌入被逐项相加，然后将结果输入到回归头。
- en: 'For multiple choice question answering, the approach is very similar to BERT’s:
    the model is called once per possible answer, with both the context (including
    the question) and the possible answer as input, separated by a $ sign, then the
    last token’s output embedding is passed through a linear layer to get a score.
    All the answer scores are then passed through a softmax layer.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多选题问答，方法与BERT的非常相似：对于每个可能的答案，模型被调用一次，输入包括上下文（包括问题）和可能的答案，通过 $ 符号分隔，然后最后一个标记的输出嵌入通过线性层得到分数。然后所有答案分数通过softmax层。
- en: In all cases they added a start-of-sequence token and an end-of-sequence token.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有情况下，他们都添加了一个序列开始标记和一个序列结束标记。
- en: '![Diagram illustrating GPT-1 pretraining with next token prediction using softmax
    and fine-tuning for classification with a sigmoid function.](assets/hmls_1513.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![说明使用 softmax 和 sigmoid 函数进行分类预训练的 GPT-1 的图](assets/hmls_1513.png)'
- en: Figure 15-13\. Pretraining GPT-1 using next token prediction (NTP, left) and
    fine-tuning it for classification (right)
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-13. 使用下一个标记预测（NTP，左侧）预训练 GPT-1 并对其进行分类微调（右侧）
- en: GPT-2 and Zero-Shot Learning
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2和无监督学习
- en: Just a few months later, in February 2019, Alec Radford, Jeffrey Wu, and other
    OpenAI researchers published the GPT-2 paper,⁠^([12](ch15.html#id3586)) which
    proposed a very similar architecture to GPT-1,⁠^([13](ch15.html#id3587)) but larger
    still. It came in four sizes, and the largest model had 48 decoder layers, 20
    attention heads, an embedding size of 1,600, and a context window of 1,024 tokens,
    for a total of over 1.5 billion parameters!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 只过了几个月，在2019年2月，Alec Radford、Jeffrey Wu和其他OpenAI研究人员发表了GPT-2论文，⁠^([12](ch15.html#id3586))，它提出了与GPT-1非常相似的架构，⁠^([13](ch15.html#id3587))，但规模更大。它有四种大小，最大的模型有48个解码器层，20个注意力头，嵌入大小为1,600，上下文窗口为1,024个标记，总参数超过15亿个！
- en: For such a large model, the authors needed a gigantic dataset, so they initially
    tried using Common Crawl which contains over two billion web pages. However, many
    of these pages are just gibberish (e.g., long tables of data). So the authors
    built a higher-quality dataset named *WebText*, composed of about eight million
    pages linked from highly ranked Reddit pages.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个大型模型，作者需要一个巨大的数据集，所以他们最初尝试使用包含超过20亿个网页的Common Crawl。然而，其中许多页面只是胡言乱语（例如，长数据表）。因此，作者构建了一个名为
    *WebText* 的高质量数据集，它由大约800万个来自排名靠前的Reddit页面的链接组成。
- en: 'Most importantly, GPT-2 performed incredibly well on many tasks without any
    fine-tuning: this is called *zero-shot learning* (ZSL). For example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，GPT-2在许多任务上表现极其出色，而无需任何微调：这被称为*无监督学习*（ZSL）。例如：
- en: For question answering, you can simply append “A:” to the question (e.g., “What
    is the capital of New-Zealand? A:”) then feed this prompt to GPT-2\. It will complete
    it with the answer (e.g., “Wellington”).
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于问答任务，你只需在问题后简单添加“A:”（例如，“新西兰的首都是什么？A:”），然后将这个提示输入到GPT-2中。它将用答案（例如，“惠灵顿”）来完成它。
- en: For summarization, you can append “TL;DR:” to the document you want to summarize,
    and GPT-2 will often produce a decent summary.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于摘要，你可以在你想要总结的文档中附加“TL;DR:”，GPT-2通常会生成一个不错的摘要。
- en: 'For translation, you can create a prompt containing a few examples to guide
    the model, such as “Bonjour papa = Hello dad” and “Le chien dort = The dog is
    sleeping”, then append the text you want to translate, for example “Elle aime
    le chocolat =”, and GPT-2 will hopefully complete the prompt with the correct
    English translation: “She loves chocolate”.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于翻译，你可以创建一个包含几个示例的提示来引导模型，例如“Bonjour papa = Hello dad”和“Le chien dort = The
    dog is sleeping”，然后附加你想要翻译的文本，例如“Elle aime le chocolat =”，然后GPT-2有望用正确的英文翻译来补充提示：“She
    loves chocolate”。
- en: 'Importantly, the authors showed that ZSL performance seemed to increase regularly
    with the model size: doubling the model size offered a roughly constant improvement
    (that’s a log-linear relationship). Maybe creating a superhuman AI was just a
    matter of training a large enough transformer?'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，作者们表明，ZSL的性能似乎随着模型大小的增加而规律性地提高：模型大小加倍提供了大致恒定的改进（这是一个对数线性关系）。也许创建一个超人类AI只是训练足够大的transformer的事情？
- en: Note
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: GPT-2’s performance was so impressive that OpenAI initially chose not to release
    the largest model. Officially, this was for the public’s safety, citing risks
    like automated disinformation and spam. But skeptics argued that it was both a
    publicity stunt and a shift toward closed-source AI, and perhaps even a move to
    influence future regulation. The full GPT-2 model was eventually released months
    later, but it was the last open one from OpenAI until August 2025, when a couple
    of open-weight models were released (GPT-OSS).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的表现如此令人印象深刻，以至于OpenAI最初选择不发布最大的模型。官方说法是为了公众的安全，引用了自动化虚假信息和垃圾邮件等风险。但怀疑者认为，这既是一场宣传噱头，也是向封闭源AI转变，甚至可能是影响未来监管的举措。几个月后，完整的GPT-2模型最终发布，但这是OpenAI直到2025年8月发布的最后一个公开模型，当时发布了几款开放权重模型（GPT-OSS）。
- en: GPT-3, In-Context Learning, One-Shot Learning, and Few-Shot Learning
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3，上下文学习，单样本学习，和少样本学习
- en: Following their bigger-is-better philosophy, OpenAI created [GPT-3](https://homl.info/gpt3)
    in 2020.⁠^([14](ch15.html#id3595)) It had roughly 40 billion parameters, and was
    trained on a monstrously large dataset of about 570 gigabytes (including WebCrawl
    this time).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 依据“越大越好”的哲学，OpenAI于2020年创建了[GPT-3](https://homl.info/gpt3)。⁠^([14](ch15.html#id3595))
    它大约有400亿个参数，并在大约570GB的巨大数据集上进行了训练（这次包括WebCrawl）。
- en: 'This model indeed was far better across the board than GPT-2\. In particular,
    it was much better at zero-shot tasks. But most importantly, the authors showed
    that GPT-3 was incredibly good at generalizing from just a few examples. This
    is called *few-shot learning* (FSL), or *one-shot learning* (OSL) if there’s a
    single example. To tackle FSL or OSL tasks, the authors simply inserted the example(s)
    in the prompt: they dubbed this *in-context learning* (ICL). For example, if you
    feed the following prompt to GPT-3, can you guess what it will output?'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在各个方面都比GPT-2好得多。特别是，它在零样本任务上表现得更好。但最重要的是，作者们表明，GPT-3在仅从几个示例中进行泛化方面非常出色。这被称为*少样本学习*（FSL），如果只有一个示例，则称为*单样本学习*（OSL）。为了处理FSL或OSL任务，作者们简单地在提示中插入示例：他们将这种方法称为*上下文学习*（ICL）。例如，如果你将以下提示提供给GPT-3，你能猜到它会输出什么吗？
- en: '[PRE14]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That’s right, it will output the missing word, “bat”. The idea of feeding the
    model some examples in the prompt itself was already present in the GPT-2 paper
    (remember the translation example?), but it wasn’t really formalized, and the
    GPT-3 paper explored it in much more depth.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，它将输出缺失的单词，“bat”。在提示中向模型提供一些示例的想法，在GPT-2的论文中就已经存在（还记得翻译示例吗？），但并没有真正规范化，GPT-3的论文则对其进行了更深入的探讨。
- en: Note
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In-context learning is an increasingly popular approach to one-shot learning
    and few-shot learning, but there are many others. ICL is new, but OSL and FSL
    are old (like ZSL).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文中学习是越来越受欢迎的一种单样本学习和少样本学习的方法，但还有许多其他方法。ICL是新的，但OSL和FSL是旧的（就像ZSL）。
- en: Let’s download GPT-2 and generate some text with it (we will play with GPT-3
    via the API later in this chapter).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载GPT-2并使用它生成一些文本（我们将在本章后面通过API玩转GPT-3）。
- en: Using GPT-2 to Generate Text
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPT-2生成文本
- en: 'As you might expect, we can use the Transformers library to download GPT-2\.
    By default, we get the small version (124M parameters):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，我们可以使用Transformers库来下载GPT-2。默认情况下，我们得到的是小型版本（124M参数）：
- en: '[PRE15]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s go through this code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: After the imports, we load GPT-2’s pretrained tokenizer and the model itself.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在导入之后，我们加载GPT-2的预训练分词器和模型本身。
- en: To load the model, we use `AutoModelForCausalLM.from_pretrained()`, which returns
    an instance of the appropriate class based on the checkpoint we ask for (in this
    case it returns a `GPT2LMHeadModel`). Since it’s a causal language model, it’s
    capable of generating text, as we will see shortly.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要加载模型，我们使用`AutoModelForCausalLM.from_pretrained()`，它根据我们请求的检查点返回适当的类实例（在这种情况下，它返回`GPT2LMHeadModel`）。由于它是一个因果语言模型，因此能够生成文本，正如我们很快就会看到的。
- en: The `device_map="auto"` option tells the function to automatically place the
    model on the best available device, typically the GPU. If you have multiple GPUs
    and the model is too large for one, it may even be sharded across GPUs.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map="auto"`选项告诉函数自动将模型放置在最佳可用设备上，通常是GPU。如果您有多个GPU且模型太大无法在一个上运行，它甚至可能跨多个GPU进行分片。'
- en: The `dtype="auto"` option asks the function to choose the most appropriate data
    type for the model weights, based on what’s available in the model checkpoint
    and your hardware. Typically, it loads the model using 16-bit floats if your hardware
    supports it (e.g., a modern GPU with mixed-precision support), or it falls back
    to 32-bit floats. Using half precision (16-bit) uses half the memory, which lets
    you load larger models, and it also gives the model a substantial speed boost
    because modern GPUs have hardware accelerations for this, and half precision reduces
    the amount of data that needs to be transferred between the CPU and GPU.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype="auto"`选项要求函数根据模型检查点和您的硬件中可用的内容选择最适合模型权重的数据类型。通常情况下，如果您的硬件支持（例如，具有混合精度支持的现代GPU），它会使用16位浮点数加载模型，否则会回退到32位浮点数。使用半精度（16位）可以节省一半的内存，这使得您可以加载更大的模型，同时它还因为现代GPU对此有硬件加速，半精度还能减少需要在CPU和GPU之间传输的数据量。'
- en: 'Now let’s write a little wrapper function around the model’s `generate()` method
    to make it very easy to generate text:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们围绕模型的`generate()`方法编写一个小包装函数，使其非常容易生成文本：
- en: '[PRE16]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our `generate()` function tokenizes the given prompt, transfers the resulting
    token IDs to the GPU, calls the given model’s `generate()` method to extend the
    prompt, adding up to 50 new tokens (by default) or less if it runs into an end-of-sequence
    token, and lastly it decodes the resulting token IDs to return a nice string containing
    the extended text. Since GPT-2 was pretrained without padding, we must specify
    which token we want to use for padding when calling the model’s `generate()` method:
    it’s common to use the end-of-sequence token for this. This function processes
    a single prompt so there will be no padding anyway, but specifying the padding
    token avoids a pesky warning. Our function also accepts optional extra keyword
    arguments (`**generate_kwargs`) and passes them on to the model’s `generate()`
    method. This will come handy very soon.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`generate()`函数将给定的提示进行分词，将生成的标记ID传输到GPU，调用给定模型的`generate()`方法来扩展提示，最多添加50个新标记（默认值）或更少，如果遇到序列结束标记，最后将生成的标记ID解码成包含扩展文本的字符串。由于GPT-2是在没有填充的情况下预训练的，我们必须在调用模型的`generate()`方法时指定我们想要用于填充的标记：通常使用序列结束标记。这个函数处理单个提示，所以无论如何都不会进行填充，但指定填充标记可以避免一个讨厌的警告。我们的函数还接受可选的额外关键字参数（`**generate_kwargs`），并将它们传递给模型的`generate()`方法。这很快就会派上用场。
- en: Note
  id: totrans-294
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Decoder-only models often pad on the left side, for more efficient generation,
    since new tokens are added on the right.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器模型通常在左侧填充，以提高生成效率，因为新标记是在右侧添加的。
- en: 'Now let’s try generating some text about a talking unicorn:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试生成一些关于会说话的独角兽的文本：
- en: '[PRE17]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Hmm, it starts out pretty well, but then it just repeats itself—what’s happening?
    Well, by default the `generate()` method simply picks the most likely token at
    each step, which is fine when you expect very structured output, or for tasks
    such as question answering, but for creative writing it often gets the model stuck
    in a loop, producing repetitive and uninteresting text. To fix this, we can set
    `do_sample=True` to make the `generate()` method randomly sample each token based
    on the model’s estimated probabilities for the possible tokens, like we did with
    our Shakespeare model in [Chapter 14](ch14.html#nlp_chapter). Let’s see if this
    works:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 哼，它一开始相当不错，但后来就只是重复自己——发生了什么？嗯，默认情况下，`generate()`方法只是在每个步骤中选择最可能的标记，当你期望非常结构化的输出，或者对于问答等任务时，这是可以的，但对于创意写作，它往往会使模型陷入循环，产生重复且无趣的文本。为了解决这个问题，我们可以设置`do_sample=True`来使`generate()`方法根据模型对可能标记的估计概率随机采样每个标记，就像我们在第14章中的Shakespeare模型所做的那样。让我们看看这行不行：
- en: '[PRE18]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Well, that’s certainly less repetitive! To get better results, you can play
    with the `generate()` method’s many arguments, such as:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这确实不那么重复了！为了获得更好的结果，你可以玩转`generate()`方法的许多参数，例如：
- en: '`temperature`'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`temperature`'
- en: Defaults to 1; decrease for more predictable outputs, or increase for more diverse
    outputs (as we saw in [Chapter 14](ch14.html#nlp_chapter))
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 默认为1；减少以获得更可预测的输出，或增加以获得更多样化的输出（如我们在第14章中看到的）
- en: '`top_k`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`top_k`'
- en: Only sample from the top *k* most probable tokens
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 只从最可能的top *k*个标记中进行采样
- en: '`top_p`'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`top_p`'
- en: Restrict sampling to the smallest set of most probable tokens whose total probability
    is a least `top_p`
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 限制采样到最可能的一组标记的最小集合，其总概率至少为`top_p`
- en: '`num_beams`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_beams`'
- en: The beam width for beam search (introduced in [Chapter 14](ch14.html#nlp_chapter));
    defaults to 1 (i.e., no beam search)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 光束搜索的波束宽度（在第14章中介绍）；默认为1（即没有光束搜索）
- en: Top-*p* sampling (a.k.a., nucleus sampling) is often preferred over top-*k*
    sampling, as it adapts to the probability distribution; for example, “The capital
    city of France is” has only one likely next token (i.e., “Paris”), and top-*p*
    sampling will always select it, while top-*k* sampling might occasionally pick
    an incorrect token. Conversely, “My favorite city is” has many likely next tokens,
    and top-*p* sampling will pick any one of them (favoring the most likely cities),
    but top-*k* sampling will only sample from the few most likely ones, ignoring
    many great cities.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Top-*p*采样（也称为核采样）通常比top-*k*采样更受欢迎，因为它适应概率分布；例如，“法国的首都是”只有一个可能的下一个标记（即“巴黎”），top-*p*采样将始终选择它，而top-*k*采样可能会偶尔选择一个错误的标记。相反，“我最喜欢的城市是”有许多可能的下一个标记，top-*p*采样将从中选择任何一个（优先考虑最可能的城市），而top-*k*采样将只从少数最可能的标记中进行采样，忽略许多伟大的城市。
- en: 'So let’s see if top-*p* sampling helps:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看top-*p*采样是否有帮助：
- en: '[PRE19]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s much better! Now let’s see how to use GPT-2 for question answering.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！现在让我们看看如何使用GPT-2进行问答。
- en: Using GPT-2 for Question Answering
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPT-2进行问答
- en: 'Let’s write a little function that takes a country name and asks GPT-2 to return
    its capital city:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写一个简单的函数，它接受一个国家名称并让GPT-2返回其首都城市：
- en: '[PRE20]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The function starts by creating a prompt from a *prompt template*: it replaces
    the `{country}` placeholder with the given country name. Note that the prompt
    template includes one example of the task to help GPT-2 understand what to do
    and what format we expect: that’s in-context learning. The function then calls
    our `generate()` function to add 10 tokens to the prompt: this is more than we
    need to write the capital city’s name. Lastly, we do a bit of post-processing
    by removing the initial prompt as well as anything after the first line, and we
    strip away any extra spaces at the end. Let’s try it out!'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先从一个*提示模板*创建提示：它将给定的国家名称替换为`{country}`占位符。请注意，提示模板包括一个任务的示例，以帮助GPT-2理解要做什么以及我们期望的格式：这就是情境学习。然后，函数调用我们的`generate()`函数向提示添加10个标记：这比我们写首都名称所需的要多。最后，我们进行一些后处理，移除初始提示以及第一行之后的所有内容，并删除末尾的任何额外空格。让我们试试看！
- en: '[PRE21]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It works beautifully! Moreover, it’s quite flexible with its input; for example,
    if you ask it for the capital of “UK”, “The UK”, “England”, “Great Britain”, or
    even “Big Britane”, it will still return “London”. That said, it’s far from perfect:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 它工作得非常漂亮！此外，它对输入非常灵活；例如，如果你要求它提供“UK”的首都，无论是“UK”、“The UK”、“England”、“Great Britain”还是甚至“Big
    Britane”，它仍然会返回“London”。话虽如此，它远非完美：
- en: It makes many common mistakes (e.g., for Canada, it answers Toronto instead
    of Ottawa). Sadly, since GPT-2 was trained on many pages from the web, it picked
    up people’s misconceptions and biases.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它犯了许多常见的错误（例如，对于加拿大，它回答多伦多而不是渥太华）。遗憾的是，由于GPT-2是在许多网页页面上训练的，它吸收了人们的误解和偏见。
- en: When it’s not sure, it just repeats the country’s name, roughly 30% of the time.
    This might be because several countries have a capital city of the same name (e.g.,
    Djibouti, Luxembourg, Singapore) or close (e.g., Guatemala City, Kuwait City).
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当不确定时，它只是重复国家的名字，大约30%的时间。这可能是由于几个国家的首都名称相同（例如，吉布提、卢森堡、新加坡）或者非常接近（例如，危地马拉城、科威特城）。
- en: When the input is not a country, the model often answers “Paris”, since that’s
    the only example it had in its prompt.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入不是国家时，模型通常会回答“巴黎”，因为这是它在提示中唯一的例子。
- en: One way to fix these issues is to simply use a much bigger and smarter model.
    For example, try using “gpt2-xl” (1.5B parameters) instead of “gpt2” when loading
    the model, then run the code again. It still won’t be perfect, but you should
    notice a clear improvement. So let’s see if an much larger model can do even better!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法之一是简单地使用一个更大、更智能的模型。例如，在加载模型时尝试使用“gpt2-xl”（1.5B参数）而不是“gpt2”，然后再次运行代码。它仍然不会完美，但你应该会注意到明显的改进。那么，让我们看看一个更大的模型是否可以做得更好！
- en: 'Downloading and Running an Even Larger Model: Mistral-7B'
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和运行更大的模型：Mistral-7B
- en: Mistral-7B is a decoder-only model released by a French startup named Mistral
    AI in May 2024\. As its name suggests, it has seven billion parameters, and it
    implements several advanced Transformer techniques, such as grouped-query attention
    and sliding-window attention (see [Chapter 17](ch17.html#speedup_chapter)), which
    increase its speed and performance.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral-7B是Mistral AI这家法国初创公司在2024年5月发布的一个仅解码器模型。正如其名称所示，它有70亿参数，并实现了几个高级Transformer技术，如分组查询注意力和滑动窗口注意力（见[第17章](ch17.html#speedup_chapter)），这提高了其速度和性能。
- en: 'The good news is that it’s released under the permissive Apache 2.0 license,
    and it’s not too big to run on Colab GPUs. However, the model is *gated* on the
    Hugging Face Hub, meaning that the platform requires you to log in and agree to
    some terms: in this case, sharing your identity with the model authors. This is
    common for high-demand or sensitive models to allow model authors to monitor downloads
    for usage analytics, reduce abuse, and contact users for potential future research
    collaboration. Let’s go through all the steps needed to run this model on Colab
    (or on your own machine if your GPU has enough VRAM):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，它是在宽松的Apache 2.0许可证下发布的，并且大小适中，可以在Colab GPU上运行。然而，该模型在Hugging Face Hub上受到限制，这意味着平台要求您登录并同意一些条款：在这种情况下，与模型作者分享您的身份。这对于需求量大或敏感的模型来说是常见的，以便模型作者可以监控下载以进行使用分析，减少滥用，并联系用户进行潜在的未来研究合作。让我们通过所有必要的步骤在Colab（或如果您自己的GPU有足够的VRAM，在自己的机器上）运行此模型：
- en: Go to [*https://huggingface.co*](https://huggingface.co) and log in if you already
    have an account. If not, click on Sign Up and follow the instructions.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前往[*https://huggingface.co*](https://huggingface.co)并登录，如果您已有账户。如果没有，点击“注册”并按照指示操作。
- en: Once you have logged in to your account, go to [*https://huggingface.co/mistralai/Mistral-7B-v0.3*](https://huggingface.co/mistralai/Mistral-7B-v0.3)
    (or use the Hub’s search feature to find this page). You should see the *model
    card* containing useful information about this model, including code snippets
    and more. For this particular model, you should also see the message asking you
    to agree to share your contact information (see [Figure 15-14](#agreement_screenshot)).
    If you agree, click “Agree and access repository”. Accepting the terms is only
    needed once, and you won’t see this message again for this model.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦您登录到您的账户，请访问[*https://huggingface.co/mistralai/Mistral-7B-v0.3*](https://huggingface.co/mistralai/Mistral-7B-v0.3)（或使用Hub的搜索功能找到此页面）。您应该会看到一个包含有关此模型的有用信息的*模型卡片*，包括代码片段等。对于这个特定的模型，您还应该看到一个要求您同意分享联系信息的消息（见[图15-14](#agreement_screenshot)）。如果您同意，请点击“同意并访问存储库”。接受条款只需要一次，并且您将不会再次看到此消息为此模型。
- en: '![Screenshot of the Hugging Face model page for Mistral-7B-v0.3 showing a notification
    requiring users to agree to share contact information to access the model.](assets/hmls_1514.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![Mistral-7B-v0.3模型页面的截图，显示一个要求用户同意分享联系信息以访问模型的提示](assets/hmls_1514.png)'
- en: Figure 15-14\. The Mistral-7B-v0.3 model on the Hugging Face Hub requires agreeing
    to share your identity with the model authors
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-14\. Hugging Face Hub 上的 Mistral-7B-v0.3 模型要求您同意与模型作者共享您的身份
- en: 'Next, you need an access token, which we will use to log in to the Hub from
    our code:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要一个访问令牌，我们将使用它从我们的代码中登录到 Hub：
- en: In the top righthand corner of the website, click on your profile icon, then
    select Access Tokens from the drop-down menu (or go to [*https://huggingface.co/settings/tokens*](https://huggingface.co/settings/tokens)).
    The website may ask you to confirm your identity at this point.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网站的右上角，点击您的个人资料图标，然后从下拉菜单中选择访问令牌（或转到[*https://huggingface.co/settings/tokens*](https://huggingface.co/settings/tokens))。此时网站可能会要求您确认您的身份。
- en: Enter a name for your token, for example, `hf-read-mistral`.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的令牌输入一个名称，例如，`hf-read-mistral`。
- en: 'You must now select the “Token type”: it can be Fine-grained, Read, or Write.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您现在必须选择“令牌类型”：它可以是细粒度、读取或写入。
- en: In production, it’s important to use an access token with very limited authorizations
    in case the token gets compromised. You would select the Fine-grained option (see
    [Figure 15-15](#access_token_screenshot)), then scroll down to the “Repositories
    permissions” section, search for mistralai/Mistral-7B-v0.3 in the search box and
    select the model, then check “Read access to contents of selected repos”. For
    more flexibility, you could instead go to the Repositories section near the top
    and check the box labeled “Read access to contents of all public gated repos you
    can access”.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中，如果令牌被泄露，使用具有非常有限授权的访问令牌非常重要。您会选择“细粒度”选项（见[图 15-15](#access_token_screenshot))，然后滚动到“存储库权限”部分，在搜索框中搜索
    mistralai/Mistral-7B-v0.3，选择模型，然后勾选“读取所选存储库内容”的复选框。为了获得更多灵活性，您也可以转到页面顶部的“存储库”部分，勾选“读取您可访问的所有公共门控存储库内容”的复选框。
- en: During development, using excessively restrictive access tokens can often slow
    you down, so you may prefer to select the Read token type, which gives full read
    access to your account, or even the Write token type, which gives full read/write
    access.
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发过程中，使用过于限制性的访问令牌通常会减慢您的速度，因此您可能更喜欢选择读取令牌类型，它为您提供了对账户的完全读取访问权限，或者甚至选择写入令牌类型，它提供了完全的读取/写入访问权限。
- en: Click the Create Token button, and copy the access token. Save it carefully,
    as it will never be shown again.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“创建令牌”按钮，并复制访问令牌。请务必妥善保存，因为它将不会再显示。
- en: Warning
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Keep your access tokens safe (e.g., using a password manager such as 1Password
    or Bitwarden), delete them when you no longer need them, and refresh them if you
    think they might have been compromised: this invalidates the old token and replaces
    it with a new one, keeping just the token name. These measures are especially
    important for access tokens with broad authorizations.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您的访问令牌安全（例如，使用 1Password 或 Bitwarden 等密码管理器），当您不再需要它们时删除它们，并在您认为它们可能已被泄露时刷新它们：这将使旧令牌失效，并用新令牌替换它，只保留令牌名称。这些措施对于具有广泛授权的访问令牌尤为重要。
- en: '![Interface showing the steps to create a Hugging Face access token with options
    for setting fine-grained permissions for repositories.](assets/hmls_1515.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![显示创建 Hugging Face 访问令牌的步骤，并设置存储库细粒度权限的选项的界面](assets/hmls_1515.png)'
- en: Figure 15-15\. Creating a Hugging Face access token
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-15\. 创建 Hugging Face 访问令牌
- en: 'OK, let’s go back to Colab now. The last step before downloading the model
    is to get your notebook to log in to the Hugging Face Hub using the access token
    that you just created. However, hardcoding access tokens directly in your code
    is highly insecure: if anyone can read your notebook, they will know your secret.
    Luckily, Colab has a convenient feature to save your secrets safely and make them
    available to any notebooks you like without any hardcoding:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们回到 Colab。在下载模型之前的最后一步是让您的笔记本使用您刚刚创建的访问令牌登录到 Hugging Face Hub。然而，直接在代码中硬编码访问令牌是非常不安全的：如果任何人都能阅读您的笔记本，他们就会知道您的秘密。幸运的是，Colab
    有一个方便的功能可以安全地保存您的秘密，并使它们对您喜欢的任何笔记本都可用，而无需任何硬编码：
- en: Click on the key icon located in the vertical bar on the lefthand side of Colab’s
    interface (see [Figure 15-16](#colab_secrets_screenshot)).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击 Colab 界面左侧垂直栏中的密钥图标（见[图 15-16](#colab_secrets_screenshot))。
- en: Click “Add new secret”, then enter your secret’s name (e.g., `token-hf-read-mistral`)
    and the secret value (i.e., your access token). The secret will be stored safely
    on Google’s servers.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“添加新秘密”，然后输入您的秘密名称（例如，`token-hf-read-mistral`)和秘密值（即您的访问令牌）。秘密将安全地存储在 Google
    的服务器上。
- en: Click the button located in the “Notebook access” column of your secret to give
    the current notebook access to your secret. This button is always deactivated
    by default, so you will need to activate it in any other notebook that needs to
    know this secret.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“笔记本访问”列中的按钮，为当前笔记本提供秘密访问权限。默认情况下，此按钮始终处于禁用状态，因此你需要在任何需要知道这个秘密的其他笔记本中激活它。
- en: Warning
  id: totrans-345
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you run a Colab notebook written by someone else, then make sure you trust
    the author or verify the code before activating notebook access for any of your
    secrets.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行的是别人编写的 Colab 笔记本，那么在激活笔记本访问任何你的秘密之前，请确保你信任作者或验证代码。
- en: '![Screenshot of a Colab interface showing a secrets manager, which includes
    options for configuring private environment variables, with fields for adding
    secret keys and managing their access.](assets/hmls_1516.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![Colab 界面截图，显示一个秘密管理器，其中包括配置私有环境变量的选项，以及添加秘密密钥和管理其访问的字段。](assets/hmls_1516.png)'
- en: Figure 15-16\. Storing the access token using Colab’s secrets manager
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-16\. 使用 Colab 的秘密管理器存储访问令牌
- en: 'Now you can run the following code to retrieve the secret access token:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行以下代码来检索秘密访问令牌：
- en: '[PRE22]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Great! You now have your access token ready, so let’s use it to log in to the
    Hugging Face Hub:⁠^([15](ch15.html#id3613))
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！你现在有了你的访问令牌，所以让我们用它来登录到 Hugging Face Hub：⁠^([15](ch15.html#id3613))
- en: '[PRE23]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, you can load Mistral-7B, exactly like you loaded GPT-2:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以像加载 GPT-2 一样加载 Mistral-7B：
- en: '[PRE24]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now you can play around with this model, make it write stories about talking
    unicorns, or use it to answer all sorts of questions. If you use it to find capital
    cities, as we did earlier, you will see that it finds the correct answer for almost
    all countries in the world. Moreover, the very few mistakes it makes are actually
    quite reasonable.⁠^([16](ch15.html#id3618))
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以与这个模型玩玩，让它写关于会说话的独角兽的故事，或者用它来回答各种问题。如果你用它来查找首都，就像我们之前做的那样，你会发现它几乎为世界上几乎所有国家找到了正确的答案。此外，它犯的少数错误实际上相当合理。⁠^([16](ch15.html#id3618))
- en: But what if we want to chat with this model?
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想与这个模型聊天呢？
- en: Turning a Large Language Model into a Chatbot
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将大型语言模型转变为聊天机器人
- en: 'To build a chatbot, you need more than a base model. For example, let’s try
    asking Mistral-7B for something:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个聊天机器人，你需要的不只是一个基础模型。例如，让我们尝试向 Mistral-7B 提出一些问题：
- en: '[PRE25]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'That’s not helpful at all; the model doesn’t answer the question, it just completes
    it! How can we get this model to be more conversational? Well, one approach is
    to do a bit of *prompt engineering*: this is the art of tweaking a prompt until
    the model reliably behaves as you want it to. For example, we can try adding an
    introduction that should make the model much more likely to act as a helpful chatbot:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全不起作用；模型没有回答问题，只是完成了它！我们如何让这个模型变得更加会话式？嗯，一种方法是对提示进行一些*提示工程*：这是调整提示直到模型可靠地按照你的期望行为的艺术。例如，我们可以尝试添加一个介绍，这样模型更有可能表现得像一个有帮助的聊天机器人：
- en: '[PRE26]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To build the full prompt, we just concatenate this introduction and the prompt,
    adding “Me:” and “Bob:” to clearly indicate who is talking. These are called *role
    tags*:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建完整的提示，我们只需将这个介绍和提示连接起来，并添加“我:”和“鲍勃:”以清楚地表明谁在说话。这些被称为*角色标签*：
- en: '[PRE27]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now let’s see how the model completes this new prompt:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看模型如何完成这个新的提示：
- en: '[PRE28]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we’re getting somewhere! Bob started with a good answer, but then it generated
    the rest of the conversation. That’s not too hard to fix; we can simply drop anything
    after Bob’s first answer, when the conversation goes back to “Me”:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在取得进展！鲍勃开始给出了一个很好的答案，但随后它生成了剩余的对话。这并不难修复；我们只需简单地删除鲍勃第一次回答之后的所有内容，当对话回到“我”时：
- en: '[PRE29]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There we go, good answer! Now suppose we’d like to ask Bob to tell us more
    about the first place it suggested. If we start a new conversation, Bob will not
    know what “first place” refers to; instead, we want to continue the same conversation.
    To do this, we can take the current context (i.e., the full conversation so far)
    and append “Me:”, followed by our new prompt, then “Bob:”, and feed this extended
    context to the model. It should generate Bob’s response for this second prompt.
    We can then repeat this process for any subsequent question. Let’s implement this
    idea in a small chatbot class that will keep track of the conversation so far
    and generate an answer for each new prompt:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这是一个很好的回答！现在假设我们想请Bob告诉我们更多关于它建议的第一个地方的信息。如果我们开始一个新的对话，Bob将不知道“第一个地方”指的是什么；相反，我们希望继续同一个对话。为了做到这一点，我们可以取当前上下文（即到目前为止的完整对话），然后加上“我：”，接着是我们的新提示，然后是“Bob：”，并将这个扩展的上下文输入到模型中。它应该为这个第二个提示生成Bob的回答。然后我们可以重复这个过程来回答任何后续的问题。让我们在一个小的聊天机器人类中实现这个想法，这个类将跟踪到目前为止的对话并针对每个新的提示生成答案：
- en: '[PRE30]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Each instance of this class holds a full conversation in its `context` attribute
    (starting with “Bob is an amazing chatbot […​]”). Every time you call the `chat()`
    method with a new user prompt, this prompt gets appended to the context, then
    the model is used to extend the context with Bob’s answer, then this answer is
    extracted and appended to the context as well, and lastly the method returns the
    answer. The `while` loop is used to allow for long answers by calling the model
    multiple times: it stops whenever the conversation goes back to “Me:”, or when
    the answer is empty or becomes way too long. OK, time to chat with Bob:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的每个实例都持有完整的对话在其`context`属性中（从“Bob是一个令人惊叹的聊天机器人 […​]”开始）。每次你用新的用户提示调用`chat()`方法时，这个提示会被添加到上下文中，然后使用模型扩展上下文以Bob的回答，然后这个回答被提取并添加到上下文中，最后这个方法返回答案。`while`循环用于通过多次调用模型来允许长回答：它会在对话回到“我：”时停止，或者当答案为空或变得非常长时停止。好吧，现在是时候和Bob聊天了：
- en: '[PRE31]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Cool, we’ve built a working chatbot, based on Mistral-7B, in about 20 lines
    of code! Try chatting with Bob for a few minutes; it’s quite fun. However, after
    a while, you may notice some issues:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们用大约20行代码构建了一个基于Mistral-7B的工作聊天机器人！试着和Bob聊几分钟，很有趣。然而，过了一会儿，你可能会发现一些问题：
- en: 'Bob can fall into loops. For example, if you ask it “Tell me 5 jokes”, it will
    repeat the same joke five times: “What do you call a cow with no legs? Ground
    beef”.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bob可能会陷入循环。例如，如果你问他“告诉我5个笑话”，它会重复同一个笑话五次：“没有腿的牛叫什么？碎牛肉”。
- en: 'Its answers are not always very helpful, and its tone is not very conversational.
    For example, if you ask it “How can I make cookies?”, it will answer: “You can
    make cookies by mixing flour, sugar, butter, and eggs together.” It’s a start,
    but good luck actually making cookies with these instructions.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的回答并不总是非常有帮助，它的语气也不是很对话。例如，如果你问它“我怎样才能做饼干？”，它会回答：“你可以通过混合面粉、糖、黄油和鸡蛋来做饼干。”这是一个开始，但用这些说明实际上做饼干可能很困难。
- en: 'Bob can also be a bad boy: if you ask it how to prepare a bank robbery, it
    will happily answer that you should wear a mask and carry a gun.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bob也可能是个坏孩子：如果你问他如何准备银行抢劫，它会高兴地回答你应该戴上面具并携带枪支。
- en: We can improve Bob with some more prompt engineering (e.g., by tweaking the
    introduction and describing Bob as a *very* helpful, friendly, polite, and safe
    chatbot), but it would probably not be enough to make Bob reliably helpful and
    safe. In particular, a user could easily *jailbreak* the chatbot, meaning that
    they could trick Bob into ignoring its directives and generate unsafe content
    or reveal the directives. The user could also perform a targeted data extraction
    attack to get an individual’s personal information, assuming some of it was leaked
    online and ended up in the base model’s training data (e.g., address, email, or
    credit card info). Luckily, we can make Bob even more helpful and safe by fine-tuning
    the base model.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一些更多的提示工程来改进Bob（例如，通过调整介绍并描述Bob为一个**非常**有帮助、友好、礼貌和安全的聊天机器人），但这可能不足以让Bob可靠地有帮助和安全。特别是，用户可以很容易地“越狱”聊天机器人，这意味着他们可以诱骗Bob忽略其指令并生成不安全的内容或泄露指令。用户还可以执行有针对性的数据提取攻击以获取个人的个人信息，假设其中一些信息已经泄露在线并最终出现在基础模型的训练数据中（例如，地址、电子邮件或信用卡信息）。幸运的是，我们可以通过微调基础模型来使Bob更加有帮助和安全。
- en: Fine-Tuning a Model for Chatting and Following Instructions Using SFT and RLHF
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SFT和RLHF进行聊天和遵循指令的模型微调
- en: '[Figure 15-17](#chatbot_diagram) summarizes the steps required to build a full
    chatbot system. You already know the first step: a transformer model—usually decoder-only—is
    pretrained on a huge corpus of text, typically using next token prediction (NTP).
    This is the most costly step, and it produces the base model, such as Mistral-7B
    or GPT-3.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-17](#chatbot_diagram) 总结了构建完整聊天机器人系统所需的步骤。您已经知道第一步：一个通常只有解码器的transformer模型在大量的文本语料库上预训练，通常使用下一个标记预测（NTP）。这是最昂贵的步骤，它产生了基础模型，例如Mistral-7B或GPT-3。'
- en: This base model can then be fine-tuned for many applications. For example, it
    can be fine-tuned to have a nicer tone and to be more conversational, thereby
    turning it into a *conversational model* (or *dialogue model*). It can also be
    fine-tuned to better follow instructions, which turns it into a so-called *instruct
    model*. A *chatbot model* is usually fine-tuned for both. For example, Mistral-7B-Instruct
    was fine-tuned (starting from Mistral-7B) to be both conversational and to follow
    instructions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基础模型可以用于许多应用进行微调。例如，它可以微调以拥有更友好的语气和更会交谈，从而变成一个*对话模型*（或*对话模型*）。它也可以微调以更好地遵循指令，从而变成所谓的*指令模型*。聊天机器人模型通常同时进行这两种微调。例如，Mistral-7B-Instruct是从Mistral-7B开始微调的，使其既能进行对话又能遵循指令。
- en: Note
  id: totrans-380
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'A note on terminology: a *base model* is a model that was only pretrained (e.g.,
    using NTP), but not fine-tuned yet. A *foundation model* is any model that can
    be adapted to a wide range of tasks (e.g., via prompting or fine-tuning). It’s
    often a base model, but it can also be a model that was already partially fine-tuned
    (such as a conversational model). However, these terms are often used interchangeably.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 关于术语的说明：*基础模型*是指仅经过预训练（例如，使用NTP）但尚未微调的模型。*基础模型*是指可以适应广泛任务的任何模型（例如，通过提示或微调）。它通常是基础模型，但它也可以是已经部分微调的模型（例如，对话模型）。然而，这些术语通常可以互换使用。
- en: '![Diagram illustrating the process of building a chatbot, detailing stages
    from pretraining an untrained transformer, to fine-tuning a base model, and finally
    deploying the chatbot model with additional modules.](assets/hmls_1517.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![展示构建聊天机器人过程的图解，详细说明了从预训练未训练的transformer到微调基础模型，最后部署带有附加模块的聊天机器人模型的各个阶段。](assets/hmls_1517.png)'
- en: 'Figure 15-17\. How to build a chatbot: pretraining, two-step fine-tuning, and
    deployment'
  id: totrans-383
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-17\. 如何构建聊天机器人：预训练、两步微调以及部署
- en: 'To fine-tune a model for a chatbot, the fine-tuning process is typically performed
    in two steps:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 要为聊天机器人微调模型，微调过程通常分为两个步骤：
- en: '*Supervised Fine-Tuning* (SFT): the model is fine-tuned on a curated dataset
    which typically contains conversations, question/answer pairs, code generation
    examples, math problems with solutions, role-playing (e.g., “You are a gourmet
    chef. How do I make perfect risotto”?), safety-aligned responses (e.g., “How do
    I rob a bank”? → “Sorry, that’s illegal”.), and more. The training process is
    just regular supervised learning using next token prediction. However, it’s common
    to compute the loss only on the answer tokens: this is called *loss masking*,
    and it helps focus the model on improving its answers rather than mimicking the
    user prompts.'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监督式微调*（SFT）：模型在经过精心挑选的数据集上进行微调，该数据集通常包含对话、问答对、代码生成示例、带解答的数学问题、角色扮演（例如，“你是一位美食大厨。我如何做出完美的烩饭”？），以及与安全相关的响应（例如，“我如何抢劫银行”？→“抱歉，这是非法的。”），等等。训练过程是使用下一个标记预测的常规监督学习。然而，通常只计算答案标记的损失：这被称为*损失掩码*，它有助于模型专注于改进其答案而不是模仿用户提示。'
- en: 'Fine-tuning with human feedback: in this step, human evaluators rank the model’s
    responses, then the model is fine-tuned to output higher-ranking responses. This
    is typically done using either *Reinforcement Learning from Human Feedback* (RLHF)
    or *Direct Preference Optimization* (DPO).'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有人工反馈的微调：在这个步骤中，人工评估者对模型的响应进行排名，然后模型被微调以输出更高排名的响应。这通常使用*来自人类反馈的强化学习*（RLHF）或*直接偏好优化*（DPO）来完成。
- en: This two-step approach was first introduced by OpenAI in January 2022 when [InstructGPT](https://homl.info/instructgpt)
    was released (via an API), a model based on GPT-3 and fine-tuned using SFT + RLHF.
    SFT is just straightforward supervised fine-tuning, and RLHF had been introduced
    several years earlier, in a [2017 paper](https://homl.info/rlhf)⁠^([24](ch15.html#id3655))
    by a group of OpenAI and DeepMind researchers, but the combination worked great.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这种两步方法最早由 OpenAI 在 2022 年 1 月推出 [InstructGPT](https://homl.info/instructgpt)
    时引入（通过 API），这是一个基于 GPT-3 并使用 SFT + RLHF 微调的模型。SFT 只是简单的监督微调，RLHF 几年前就已经被介绍，在 2017
    年一篇 [论文](https://homl.info/rlhf)⁠^([24](ch15.html#id3655)) 中，由一群 OpenAI 和 DeepMind
    研究人员提出，但两者的结合效果极佳。
- en: 'RLHF is based on a reinforcement learning (RL) technique named *proximal policy
    optimization* (PPO, not to be confused with DPO), which we will discuss in [Chapter 19](ch19.html#rl_chapter).
    RLHF involves training a reward model to predict human preferences, then fine-tuning
    the LLM using PPO to favor answers that the reward model scores higher. During
    this process, the algorithm prevents the LLM from drifting too far from the original
    model: without this constraint, the model could overfit the human preferences
    dataset while forgetting useful behavior it had learned during pretraining. This
    is called *reward hacking*.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 基于一种名为 **近端策略优化**（PPO，不要与 DPO 混淆）的强化学习（RL）技术，我们将在 [第 19 章](ch19.html#rl_chapter)
    中讨论。RLHF 包括训练一个奖励模型来预测人类偏好，然后使用 PPO 微调 LLM 以偏好奖励模型评分更高的答案。在这个过程中，算法防止 LLM 过度偏离原始模型：如果没有这个约束，模型可能会过度拟合人类偏好数据集，同时忘记它在预训练期间学习到的有用行为。这被称为
    **奖励黑客**。
- en: RLHF works rather well, and it’s still widely used today, but like many RL techniques,
    training can be unstable and tricky to get right. Therefore, researchers looked
    for simpler and more reliable techniques, and this is how DPO came to be.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 工作得相当好，至今仍被广泛使用，但像许多 RL 技术一样，训练可能不稳定，且难以正确进行。因此，研究人员寻找更简单、更可靠的技巧，这就是 DPO
    产生的原因。
- en: Direct Preference Optimization (DPO)
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接偏好优化（DPO）
- en: '[DPO](https://homl.info/dpo) was proposed in May 2023 by a team of Stanford
    University researchers.⁠^([25](ch15.html#id3661)) It often works just as well
    as RLHF or better, and it’s simpler, more stable, and more data efficient, so
    it is quickly gaining popularity.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPO](https://homl.info/dpo) 由斯坦福大学的研究团队于 2023 年 5 月提出。⁠^([25](ch15.html#id3661))
    它通常与 RLHF 一样有效，甚至更好，而且更简单、更稳定、更数据高效，因此它迅速获得了人气。'
- en: 'Just like RLHF, DPO works with a dataset of human preferences. Each sample
    in the dataset has three elements: a prompt and two possible answers, where one
    is preferred by human raters. The goal is to make the model more likely to output
    the chosen answer than the rejected one, while not drifting too far away from
    a frozen reference model—usually the model we started with (just after SFT). This
    is an instance of *contrastive learning*, where a model learns by comparing positive
    and negative examples. To do this, the researchers showed that we can just minimize
    the loss defined in [Equation 15-2](#dpo_loss_equation). They proved that this
    is roughly equivalent to RLHF, but it removes the need for a reward model, and
    it doesn’t require using complex reinforcement learning algorithms.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 RLHF 一样，DPO 与人类偏好数据集一起工作。数据集中的每个样本有三个元素：一个提示和两个可能的答案，其中一个是人类评分者偏好的。目标是使模型更有可能输出所选答案而不是被拒绝的答案，同时不偏离冻结的参考模型太远——通常是我们在
    SFT 后开始的模型。这是一个 **对比学习** 的例子，其中模型通过比较正例和负例来学习。为了做到这一点，研究人员表明，我们只需最小化 [方程式 15-2](#dpo_loss_equation)
    中定义的损失。他们证明这大致等同于 RLHF，但它消除了需要奖励模型的需求，并且不需要使用复杂的强化学习算法。
- en: Equation 15-2\. Direct preference optimization (DPO) loss
  id: totrans-393
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 15-2\. 直接偏好优化（DPO）损失
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>J</mi> <mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mo>-</mo> <mo form="prefix">log</mo> <mi>σ</mi> <mfenced separators="" open="["
    close="]"><mi>β</mi> <mfenced separators="" open="(" close=")"><mi>δ</mi> <mrow><mo>(</mo>
    <msub><mi>𝐲</mi> <mtext>c</mtext></msub> <mo>)</mo></mrow> <mo>-</mo> <mi>δ</mi>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">y</mi> <mtext>r</mtext></msub> <mo>)</mo></mrow></mfenced></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mtext>with</mtext> <mi>δ</mi> <mo>(</mo>
    <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mo form="prefix">log</mo> <msub><mi>p</mi> <mi mathvariant="bold">θ</mi></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">y</mi> <mo>∣</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mtext>ref</mtext></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">y</mi> <mo>∣</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable>
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>J</mi> <mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mo>-</mo> <mo form="prefix">log</mo> <mi>σ</mi> <mfenced separators="" open="["
    close="]"><mi>β</mi> <mfenced separators="" open="(" close=")"><mi>δ</mi> <mrow><mo>(</mo>
    <msub><mi>𝐲</mi> <mtext>c</mtext></msub> <mo>)</mo></mrow> <mo>-</mo> <mi>δ</mi>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">y</mi> <mtext>r</mtext></msub> <mo>)</mo></mrow></mfenced></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mtext>with</mtext> <mi>δ</mi> <mo>(</mo>
    <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mo form="prefix">log</mo> <msub><mi>p</mi> <mi mathvariant="bold">θ</mi></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">y</mi> <mo>∣</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mtext>ref</mtext></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">y</mi> <mo>∣</mo> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable>
- en: 'In this equation:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*J*(**θ**) is the DPO loss for an instance (**x**, **y**[c], **y**[r]), given
    the current model parameters **θ** and a frozen reference model.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*J*(**θ**) 是给定当前模型参数 **θ** 和一个冻结的参考模型的实例（**x**，**y**[c]，**y**[r]）的 DPO 损失。'
- en: '**x** is the prompt, **y**[c] is the chosen answer, and **y**[r] is the rejected
    answer.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x** 是提示，**y**[c] 是选择的答案，**y**[r] 是拒绝的答案。'
- en: '*σ*(·) is the usual sigmoid function: $sigma left-parenthesis x right-parenthesis
    equals StartFraction 1 Over 1 plus exp left-parenthesis negative x right-parenthesis
    EndFraction$ .'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*(·) 是通常的 sigmoid 函数：$sigma left-parenthesis x right-parenthesis equals StartFraction
    1 Over 1 plus exp left-parenthesis negative x right-parenthesis EndFraction$ .'
- en: log *p*[**θ**](**y** | **x**) is our model’s estimated log probability for answer
    **y** (either **y**[c] or **y**[r]), given the prompt **x**.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: log *p*[**θ**](**y** | **x**) 是我们模型对答案 **y**（无论是 **y**[c] 还是 **y**[r]）在提示 **x**
    下的估计对数概率。
- en: log *p*[ref](**y** | **x**) is the reference model’s estimated log probability
    for answer **y** given **x**.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: log *p*[ref](**y** | **x**) 是参考模型在 **x** 下对答案 **y** 的估计对数概率。
- en: '*β* is a temperature-like hyperparameter that controls how steep the sigmoid
    function is, which impacts how much the loss will focus on sticking to the reference
    model (high *β*), versus following human preferences (low *β*). It’s typically
    between 0.1 and 0.5.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β* 是一个类似于温度的超参数，它控制 sigmoid 函数的陡峭程度，这会影响损失将多少关注于坚持参考模型（高 *β*），与遵循人类偏好（低 *β*）。它通常在
    0.1 和 0.5 之间。'
- en: Tip
  id: totrans-402
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When computing log(*σ*(·)) it’s best to use the `F.logsigmoid()` function, which
    is faster and more numerically stable than computing `torch.log(torch.sigmoid(·))`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算 log(*σ*(·)) 时，最好使用 `F.logsigmoid()` 函数，它比计算 `torch.log(torch.sigmoid(·))`
    更快且更数值稳定。
- en: 'To compute log *p*(**y** | **x**), where *p* is either *p*[**θ**] or *p*[ref],
    and **y** is either **y**[c] or **y**[r], we start by concatenating **x** and
    **y**, then we tokenize the result and run it through the model to get the output
    logits. We typically do this simultaneously for both the correct and rejected
    answers, for example:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 log *p*(**y** | **x**)，其中 *p* 是 *p*[**θ**] 或 *p*[ref]，而 **y** 是 **y**[c]
    或 **y**[r]，我们首先将 **x** 和 **y** 连接起来，然后对结果进行分词，并通过模型运行以获取输出 logits。我们通常同时为正确答案和拒绝答案执行此操作，例如：
- en: '[PRE32]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next we can call the `F.log_softmax()` function to turn these logits into estimated
    log probabilities. Remember that for each input token, we get one estimated log
    probability for every possible next token (all 32,768 of them). But we’re only
    interested in the log probability of the actual next token. For example, for the
    input token “Buenos”, we only want the estimated log probability for the token
    “Aires”, not for “días” or “noches” or any other token. We can use the `torch.gather()`
    function to extract only the log probability of the next token (given its token
    ID) for each input token except the last one (since it doesn’t have a next token):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以调用 `F.log_softmax()` 函数将这些 logits 转换为估计的对数概率。记住，对于每个输入标记，我们都会得到每个可能的下一个标记（共
    32,768 个）的估计对数概率。但我们只对实际下一个标记的对数概率感兴趣。例如，对于输入标记“Buenos”，我们只想得到标记“Aires”的估计对数概率，而不是“días”、“noches”或其他任何标记的。我们可以使用
    `torch.gather()` 函数提取每个输入标记（除了最后一个，因为它没有下一个标记）的下一个标记（给定其标记 ID）的对数概率：
- en: '[PRE33]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `torch.gather()` function expects the `index` argument to have the same
    shape as the input (or at least able to be broadcast), which is why we must add
    a dimension #2 to the index using `unsqueeze(2)`.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.gather()` 函数期望 `index` 参数与输入具有相同的形状（或至少能够广播），这就是为什么我们必须使用 `unsqueeze(2)`
    向索引添加一个维度 #2。'
- en: 'There’s actually a little shortcut that some people prefer—if we pass the logits
    to the `F.cross_entropy()` function, and specify the next token IDs as the targets,
    then we get the desired log probabilities directly, in one step instead of two:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有一个小捷径，有些人更喜欢使用——如果我们把 logits 传递给 `F.cross_entropy()` 函数，并指定下一个标记 ID 作为目标，那么我们就可以直接一步得到所需的对数概率，而不是两步：
- en: '[PRE34]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that we must set `reduction="none"` to prevent the function from computing
    the mean of all the log probabilities (as it does by default). We must also flip
    the result’s sign, since `F.cross_entropy()` returns the *negative* log likelihood.
    Lastly, we must swap the last two dimensions of the input tensor, since `F.cross_entropy()`
    expects the class dimension to be dimension 1.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们必须将 `reduction="none"` 设置为防止函数计算所有对数概率的平均值（因为它默认这样做）。我们还必须翻转结果的正负号，因为 `F.cross_entropy()`
    返回的是 *负* 对数似然。最后，我们必须交换输入张量的最后两个维度，因为 `F.cross_entropy()` 期望类别维度是维度 1。
- en: 'Now let’s inspect each token’s estimated probability by computing the exponential
    of the log probabilities:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过计算对数概率的指数来检查每个标记的估计概率：
- en: '[PRE35]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The first estimated probability is for the token “The” (3.27%), then “capital”
    (0.02%), and so on. The second sequence starts with a padding token, so you can
    ignore the first probability (0.14%). The estimated probabilities are the same
    in both sequences for the prompt tokens,⁠^([26](ch15.html#id3667)) but they differ
    for the answer tokens: 11.38% for “Buenos”, versus 0.00% for “Madrid”. The model
    seems to know a bit of geography! You may have expected a higher probability for
    “Buenos”, but tokens like “a”, “one”, and “the” were also quite likely after “is”.
    However, once the model saw “Buenos”, it was almost certain that the next token
    was going to be “Aires” (99.61%), and of course it was correct.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个估计概率是针对标记“The”（3.27%），然后是“capital”（0.02%），依此类推。第二个序列以填充标记开始，所以你可以忽略第一个概率（0.14%）。对于提示标记，两个序列中的估计概率是相同的，⁠^([26](ch15.html#id3667))
    但对于答案标记则不同：对于“Buenos”是11.38%，而“Madrid”是0.00%。模型似乎对地理知识有点了解！你可能预期“Buenos”的概率会更高，但“a”、“one”和“the”这样的标记在“is”之后也很可能。然而，一旦模型看到“Buenos”，它几乎可以肯定下一个标记将是“Aires”（99.61%），当然它也是正确的。
- en: 'Now if we add up the log probabilities of all answer tokens (e.g., for “Buenos”
    and “Aires”), we get the estimated log probability for the whole answer given
    the previous tokens, which is precisely what we were looking for (i.e., log *p*(**y**
    | **x**)). In this example, it corresponds to an estimated probability of 11.38%:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们把所有答案标记的对数概率加起来（例如，“Buenos”和“Aires”），我们就得到了给定前缀标记的整个答案的估计对数概率，这正是我们想要的（即
    log *p*(**y** | **x**))。在这个例子中，它对应于一个估计概率为 11.38%：
- en: '[PRE36]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'However, having to find the exact location of the answer is cumbersome, especially
    when dealing with padded batches. Luckily, we can actually compute the DPO loss
    using the log probability of the full input **xy** (including both the prompt
    **x** and the answer **y**), rather than the log probability of the answer **y**
    given the prompt **x**. In other words, we can replace every log *p*(**y** | **x**)
    with log *p*(**xy**) in [Equation 15-2](#dpo_loss_equation) (for both *p*[**θ**]
    and *p*[ref], and for both **y**[c] and **y**[r]). This is because log *p*(**xy**)
    = log *p*(**x**) + log *p*(**y** | **x**), and the extra *p*(**x**) for the chosen
    answer cancels out exactly with the extra *p*(**x**) for the rejected answer.
    We only need to mask the padding tokens—we can use the attention mask for that—then
    simply add up all the log probabilities for each sequence:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须找到答案的确切位置是繁琐的，尤其是在处理填充批次时。幸运的是，我们实际上可以使用完整输入 **xy**（包括提示 **x** 和答案 **y**）的对数概率来计算
    DPO 损失，而不是给定提示 **x** 的答案 **y** 的对数概率。换句话说，我们可以在 [方程式 15-2](#dpo_loss_equation)
    中将每个 log *p*(**y** | **x**) 替换为 log *p*(**xy**)（对于 *p*[**θ**] 和 *p*[ref]，以及对于
    **y**[c] 和 **y**[r]）。这是因为 log *p*(**xy**) = log *p*(**x**) + log *p*(**y** | **x**)，并且所选答案的额外
    *p*(**x**) 与被拒绝答案的额外 *p*(**x**) 完全抵消。我们只需要屏蔽填充标记——我们可以使用注意力掩码来做到这一点——然后简单地累加每个序列的所有对数概率：
- en: '[PRE37]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The first sequence, which contains the prompt and the chosen answer, has a
    higher log probability than the second sequence, which contains the prompt and
    the rejected answer, just as we expect. Now if you write a little `sum_of_log_probas()`
    function that wraps everything we just did to compute the sum of log probabilities
    for every sequence in a batch, then you’re ready to write a function that computes
    the DPO loss:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 包含提示和所选答案的第一个序列的对数概率比包含提示和被拒绝答案的第二个序列的对数概率要高，正如我们所期望的那样。现在，如果您编写一个小的 `sum_of_log_probas()`
    函数，将我们刚才所做的所有事情包装起来以计算批次中每个序列的对数概率之和，那么您就可以编写一个计算 DPO 损失的函数了：
- en: '[PRE38]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can then use this loss to fine-tune your model with human preferences (don’t
    forget to put your model in training mode, and the reference model in eval mode).
    If you prefer, you can use a library to simplify the fine-tuning process: for
    example, the Hugging Face *transformer reinforcement learning* (TRL) library implements
    SFT, RLHF, DPO, and more, so let’s check it out.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用这个损失来使用人类偏好微调您的模型（别忘了将您的模型置于训练模式，并将参考模型置于评估模式）。如果您愿意，可以使用库来简化微调过程：例如，Hugging
    Face 的 *transformer reinforcement learning* (TRL) 库实现了 SFT、RLHF、DPO 等功能，所以让我们来看看。
- en: Fine-Tuning a Model Using the TRL Library
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TRL 库微调模型
- en: 'Let’s use the TRL library to fine-tune a base model using SFT then DPO. For
    SFT, we need a conversational dataset. In this example, we will use the Alpaca
    dataset, composed of about 52,000 instructions and demonstrations generated by
    OpenAI’s text-davinci-003 model. Let’s load the dataset and look at an example:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 TRL 库通过 SFT 然后DPO 来微调一个基础模型。对于 SFT，我们需要一个对话数据集。在这个例子中，我们将使用 Alpaca 数据集，该数据集由
    OpenAI 的 text-davinci-003 模型生成的约 52,000 条指令和演示组成。让我们加载数据集并查看一个示例：
- en: '[PRE39]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see, the goal of this dataset is to train the model to follow a single
    instruction and generate a coherent and helpful response. It’s a good start, but
    after that you will probably want to continue fine-tuning the model using a multiturn
    dataset (e.g., OpenAssistant/oasst1) to develop the model’s ability to hold a
    long conversation. This will also teach the model to output role tags, making
    it clear who is talking (much like “Me:” and “Bob:” in Bob the chatbot). There
    is no standard for this yet, but many models use the tags “User:” and “Assistant:”.
    OpenAI defined the ChatML format, which uses “<|user|>”, “<|assistant|>”, or “<|system|>”
    for system messages (e.g., for text similar to our Bob introduction). Each section
    ends with “<|end|>”. Lastly, Anthropic uses “Human:” and “Assistant:”.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个数据集的目标是训练模型遵循单个指令并生成一个连贯且有帮助的响应。这是一个好的开始，但之后您可能希望继续使用多轮数据集（例如，OpenAssistant/oasst1）来提高模型进行长时间对话的能力。这将教会模型输出角色标签，使说话者身份明确（就像在
    Bob 聊天机器人中的“我:”和“Bob:”一样）。目前还没有标准，但许多模型使用“用户:”和“助手:”标签。OpenAI 定义了 ChatML 格式，该格式使用“<|user|>”、“<|assistant|>”或“<|system|>”来表示系统消息（例如，类似于我们
    Bob 介绍中的文本）。每个部分都以“<|end|>”结束。最后，Anthropic 使用“人类:”和“助手:”。
- en: 'Let’s preprocess the dataset to use Anthropic-style role tags. Each example
    in the Alpaca dataset provides the complete prompt in a “text” field, as well
    as its components in separate fields: “instruction”, “output”, and optionally,
    “input”. The “text” field will be used for training, so let’s use the individual
    components to compose a new “text” field and replace the existing one:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先预处理数据集以使用Anthropic风格的标签。Alpaca数据集中的每个示例都在“text”字段中提供了完整的提示，以及其单独的字段：“instruction”（指令）、“output”（输出）和可选的“input”（输入）。“text”字段将用于训练，所以让我们使用这些单独的组件来组成一个新的“text”字段，并替换现有的一个：
- en: '[PRE40]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now our previous example looks like this:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们之前的例子看起来是这样的：
- en: '[PRE41]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The training set is ready, so we can run SFT. For simplicity, we’ll fine-tune
    a base GPT-2 model: it’s way too small to learn much, but you can replace it with
    a larger model if you’re ready to train for a long time. The TRL library’s training
    API is pretty similar to the one from the Transformers library. The code is self-explanatory:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集已经准备好了，所以我们可以运行SFT。为了简单起见，我们将微调一个基础GPT-2模型：它太小了，学不到很多东西，但如果你准备长时间训练，你可以用更大的模型替换它。TRL库的训练API与Transformers库中的非常相似。代码是自解释的：
- en: '[PRE42]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now on to the DPO phase. We’ll need a human-preference dataset. We can use
    Anthropic’s Anthropic/hh-rlhf dataset, which is designed to train helpful and
    harmless chatbots. Let’s load it and look at an example:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入DPO阶段。我们需要一个人类偏好数据集。我们可以使用Anthropic的Anthropic/hh-rlhf数据集，该数据集旨在训练有益且无害的聊天机器人。让我们加载它并查看一个示例：
- en: '[PRE43]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In this dataset, the prompt is already included (prepended) in both the chosen
    answer and the rejected answer. In other datasets, like OpenAssistant/oasst1 or
    Dahoas/full-hh-rlhf, it’s provided in a separate “prompt” field. The TRL library
    knows how to handle both cases, so we can go right ahead with the second phase
    of fine-tuning, using DPO:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，提示信息已经包含（前置）在所选答案和被拒绝的答案中。在其他数据集中，如OpenAssistant/oasst1或Dahoas/full-hh-rlhf，它在一个单独的“提示”字段中提供。TRL库知道如何处理这两种情况，因此我们可以直接进入微调的第二阶段，使用DPO：
- en: '[PRE44]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s take a second to appreciate the fact that you now know how to build a
    large transformer from scratch, pretrain it using NTP (if you have enough time
    and money), then fine-tune it using SFT and DPO to turn it into a chatbot model.
    Bravo!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来欣赏一下这个事实：你现在已经知道如何从头开始构建一个大型变压器，如果时间足够且资金充足，可以使用NTP进行预训练，然后使用SFT和DPO进行微调，将其转变为聊天机器人模型。太棒了！
- en: 'Alternatively, you can simply download a chatbot model directly, already pretrained
    and fine-tuned. For example, you can download the Mistral-7B-Instruct-v0.3 model,
    and use it with our `BobTheChatbot` class: you will see that it’s a significantly
    more pleasant and helpful model than Mistral-7B-v0.3\. When you ask it to tell
    you five jokes, it comes up with five *different* jokes, and it adds “I hope you
    enjoyed these jokes! If you have any other requests, feel free to ask”. Its cookie
    recipe is clear and detailed. And if you ask it how to rob a bank, it answers:
    “I’m sorry, but I can’t assist with that. It’s illegal and unethical to provide
    advice on criminal activities”.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以直接下载一个已经预训练和微调的聊天机器人模型。例如，你可以下载Mistral-7B-Instruct-v0.3模型，并使用我们的`BobTheChatbot`类：你会发现它比Mistral-7B-v0.3模型要愉快得多，也更有帮助。当你要求它讲五个笑话时，它会讲五个*不同*的笑话，并加上“我希望你喜欢这些笑话！如果你有其他要求，请随时告诉我”。它的饼干配方清晰且详细。如果你问它如何抢劫银行，它会回答：“很抱歉，但我不能协助你。提供关于犯罪活动的建议是不合法和不道德的”。
- en: Warning
  id: totrans-438
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Mistral-7B-Instruct-v0.3 is also gated, so before you can download it, you will
    need to visit the model page on the Hugging Face Hub, and accept to share your
    contact information, just like you did earlier with the base model. Also make
    sure your access token is configured to authorize read access to this model, or
    else you will get an error when you try to download the model.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral-7B-Instruct-v0.3也是受保护的，所以在你能够下载它之前，你需要访问Hugging Face Hub上的模型页面，并接受分享你的联系信息，就像你之前与基础模型所做的那样。同时确保你的访问令牌已配置为授权读取此模型的访问，否则当你尝试下载模型时，你会得到一个错误。
- en: Now that we have a good chat model, how can we get people to use it?
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个好的聊天模型，我们如何让人们使用它呢？
- en: From a Chatbot Model to a Full Chatbot System
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从聊天机器人模型到完整的聊天机器人系统
- en: 'The last step in building a chatbot is deploying the model inside a complete
    chatbot system (see [Figure 15-17](#chatbot_diagram)). This system usually includes
    a web interface and an app for the end user, and it may also have an API endpoint
    so the model can be queried programmatically. Moreover, to handle complex queries
    and deliver truly helpful responses, chatbots increasingly rely on a system of
    integrated tools. For this, the chatbot typically has a component named the *orchestrator*
    whose role is to coordinate multiple tools to process the user prompt and compose
    the chatbot’s answer. Here are some of the most important tools:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 构建聊天机器人的最后一步是在完整的聊天机器人系统中部署模型（见[图15-17](#chatbot_diagram)）。该系统通常包括面向最终用户的网页界面和应用程序，并且可能还有一个API端点，以便可以通过编程查询模型。此外，为了处理复杂查询并提供真正有用的响应，聊天机器人越来越多地依赖于集成工具系统。为此，聊天机器人通常有一个名为*协调器*的组件，其作用是协调多个工具以处理用户提示并编写聊天机器人的答案。以下是一些最重要的工具：
- en: Calculator
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 计算器
- en: 'If the user asks “What’s 525.6 * 315 / 3942?”, the orchestrator may detect
    the presence of a math expression. Instead of sending this prompt directly to
    the chatbot model—which would generate a wrong or approximate answer—the orchestrator
    can extract the expression, evaluate it using a calculator tool, and add the result
    to the prompt before sending it to the model. The augmented prompt might look
    like this: “User: What’s 525.6 * 315 / 3942?\nSystem: Calculator result = 42.\nAssistant:”.
    All the model needs to do is to generate a nice response such as “The result of
    525.6 * 315 / 3942 is 42”. No math needed.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户询问“525.6 * 315 / 3942 是多少？”，协调器可能会检测到数学表达式的存在。而不是直接将此提示发送到聊天机器人模型——这会产生错误或近似答案——协调器可以提取表达式，使用计算器工具进行评估，并在将其发送到模型之前将结果添加到提示中。增强后的提示可能看起来像这样：“用户：525.6
    * 315 / 3942 是多少？\n系统：计算器结果 = 42。\n助手：”。模型需要做的只是生成一个不错的响应，例如“525.6 * 315 / 3942
    的结果是 42”。不需要数学知识。
- en: 'Alternatively, the chatbot model itself can be fine-tuned to invoke tools,
    such as a calculator. This is called *tool augmentation*, or *function calling*.
    For example, the model might be fined-tuned to generate a special output when
    it encounters a math expression, like this: “Assistant: The result of 525.6 *
    315 / 3942 is [calculator_tool] 525.6 * 315 / 3942 [/calculator_tool]”. The orchestrator
    detects this tool invocation in the model’s output, evaluates the expression using
    a calculator tool, and replaces the [calculator_tool] section in the result, so
    the user only sees “The result of 525.6 * 315 / 3942 is 42”. Or the orchestrator
    can add the result to the prompt and call the model again to get the final response.
    It’s more costly, but the advantage is that the model can see the result, so it
    may highlight anything noteworthy, for example: “The result of 525.6 * 315 / 3942
    is 42\. It’s interesting that the result is an integer”.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以将聊天机器人模型本身微调以调用工具，例如计算器。这被称为*工具增强*或*功能调用*。例如，模型可能被微调，以便在遇到数学表达式时生成特殊输出，如下所示：“助手：525.6
    * 315 / 3942 的结果是 [calculator_tool] 525.6 * 315 / 3942 [/calculator_tool]”。协调器检测到模型输出中的工具调用，使用计算器工具评估表达式，并替换结果中的
    [calculator_tool] 部分，因此用户只看到“525.6 * 315 / 3942 的结果是 42”。或者，协调器可以将结果添加到提示中，并再次调用模型以获取最终响应。这更耗费成本，但优点是模型可以看到结果，因此它可以突出任何值得注意的内容，例如：“525.6
    * 315 / 3942 的结果是 42。有趣的是，结果是整数”。
- en: Web search
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 网络搜索
- en: If the user asks about a URL, the orchestrator can fetch the corresponding web
    page and inject its text into the prompt. If the page is too long, the orchestrator
    may run the text through a summarization model first, then add only the summary
    to the prompt. Or it may chop the text into chunks (e.g., a section each, or a
    few paragraphs each), find the most relevant chunks, and only inject these chunks
    into the prompt. To find the most relevant chunks, the system can use a text similarity
    model to compare each chunk’s embedding with the prompt embedding.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户询问关于URL的问题，协调器可以获取相应的网页并将其文本注入到提示中。如果页面太长，协调器可能会先通过摘要模型运行文本，然后只添加摘要到提示中。或者，它可以将文本分割成块（例如，每个部分，或每几个段落），找到最相关的块，然后只将这些块注入到提示中。为了找到最相关的块，系统可以使用文本相似度模型比较每个块的嵌入与提示嵌入。
- en: 'Just like with the calculator tool, the chatbot model itself can be fine-tuned
    to ask the orchestrator to run a web search. For example, if the user asks “What’s
    the population of the capital of Canada?”, the model may first output “[search_tool]
    What is the population of Ottawa? [/search_tool]”. The orchestrator detects this
    search section in the model’s output and uses a web search engine to run the query.
    The top results are then fetched and summarized (or the system identifies the
    relevant chunks), and feeds the result to the chatbot model, along with information
    about the sources. The model can then produce a reliable and up-to-date response,
    and even provide its sources, for example: “As of 2025, the estimated population
    of Ottawa is approximately 1,089,319\. Source: [*https://worldpopulationreview.com*](https://worldpopulationreview.com)“.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 就像计算器工具一样，聊天机器人模型本身可以被微调以要求指挥者运行网络搜索。例如，如果用户问“加拿大的首都是多少人口？”，模型可能会首先输出“[search_tool]渥太华的人口是多少？[/search_tool]”。指挥者检测到模型输出中的这个搜索部分，并使用网络搜索引擎运行查询。然后检索并总结（或系统识别相关片段），并将结果连同来源信息一起反馈给聊天机器人模型。然后，模型可以生成可靠且最新的响应，甚至提供其来源，例如：“截至2025年，渥太华的估计人口约为1,089,319。来源：[*https://worldpopulationreview.com*](https://worldpopulationreview.com)“。
- en: Retrieval Augmented Generation (RAG)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: The web search idea can be generalized to all sorts of data sources, including
    private and structured sources of data, like a company’s SQL database, or PDF
    documents, knowledge bases, and so on. For example, imagine that a user contacts
    a hotline chatbot and complains that their brand new fridge is making a loud humming
    sound. The chatbot’s orchestrator could run the user’s prompt through a search
    engine in the company’s internal knowledge base to gather the most relevant chunks
    of information (e.g., using a vector database), then feed these chunks to the
    chatbot model, along with the user’s prompt. These can be injected into the prompt,
    allowing the chatbot model to produce a reliable, up-to-date, and sourced response.
    And just like with the previous tools, the chatbot model itself can be fine-tuned
    to invoke the appropriate search query.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 网络搜索的想法可以推广到各种数据源，包括私有和结构化数据源，如公司的SQL数据库或PDF文档、知识库等。例如，想象一个用户联系热线聊天机器人并抱怨他们的新冰箱发出很大的嗡嗡声。聊天机器人的指挥者可以通过公司的内部知识库中的搜索引擎运行用户的提示，收集最相关的信息片段（例如，使用向量数据库），然后将这些片段连同用户的提示一起反馈给聊天机器人模型。这些可以注入到提示中，使聊天机器人模型能够生成可靠、最新且带有来源的响应。并且，就像之前的工具一样，聊天机器人模型本身可以被微调以调用适当的搜索查询。
- en: Tip
  id: totrans-451
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: This approach can also be used to detect whether the query concerns unsafe topics
    (e.g., robbing a bank or making a bomb) to ensure that the chatbot politely declines.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也可以用来检测查询是否涉及不安全的话题（例如，抢劫银行或制造炸弹），以确保聊天机器人礼貌地拒绝。
- en: Memory (a.k.a., persistent context)
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆（即持久上下文）
- en: This tool stores user-specific facts and preferences across conversations. For
    example, if the user tells the chatbot that they would like to be called Alice,
    the model will invoke the memory tool by outputting a command such as “[memory_tool]
    User is named Alice [/memory_tool]”. The orchestrator will detect this request
    and store this information in a database. Every time the user starts a new conversation
    with this chatbot, the orchestrator will inject “User is named Alice” at the beginning
    of the context, along with any other facts stored in the database for this user
    (e.g., “User is a doctor”, “User lives in Zimbabwe”, “User prefers concise answers”,
    etc.). Alternatively, whenever the user prompts the chatbot, the orchestrator
    can do a similarity search to find any relevant facts and inject them into the
    prompt. This allows the memory to grow without crowding the context window.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 此工具存储用户在对话中的特定事实和偏好。例如，如果用户告诉聊天机器人他们希望被称为Alice，模型将通过输出如“[memory_tool] 用户名为Alice
    [/memory_tool]”之类的命令来调用记忆工具。指挥者将检测此请求并将此信息存储在数据库中。每次用户与该聊天机器人开始新的对话时，指挥者都会在上下文开始时注入“用户名为Alice”，以及存储在数据库中关于此用户的任何其他事实（例如，“用户是医生”，“用户住在津巴布韦”，“用户喜欢简洁的回答”等）。或者，每当用户提示聊天机器人时，指挥者可以进行相似度搜索以找到任何相关的信息并将它们注入到提示中。这允许记忆增长而不使上下文窗口拥挤。
- en: Agentic behavior
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 代理行为
- en: 'The chatbot model may be fine-tuned to be more autonomous and execute a multistep
    task with planning and tools. This turns it into an *agentic model*, or simply
    an *agent*. For example, if the user asks the chatbot to perform a *deep search*
    on a given topic, the model may start by asking the user for a few clarifications,
    then it will plan the main steps of its task and go ahead and execute each step;
    for example, invoking a few web searches (with the help of the orchestrator) or
    tools, analyzing the results, planning more steps, running more tools, and repeating
    the process until it has gathered all the information it needs to write a nice
    document about the topic. Note that a model may just be fine-tuned to reason,
    without calling tools or functions: this is called a *reasoning model*.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人模型可以被微调以更加自主，并能够通过规划和工具执行多步骤任务。这使得它成为一个**代理模型**，或者简单地称为**代理**。例如，如果用户要求聊天机器人对一个特定主题进行**深度搜索**，模型可能会先向用户询问一些澄清，然后它会规划任务的主要步骤并继续执行每个步骤；例如，调用一些网络搜索（借助协调器）或工具，分析结果，规划更多步骤，运行更多工具，并重复这个过程，直到它收集到它需要编写关于该主题的漂亮文档所需的所有信息。请注意，一个模型可能只是被微调进行推理，而不调用工具或函数：这被称为**推理模型**。
- en: Other tools
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 其他工具
- en: 'Just about any tool you can think of can be added to a chatbot system. Here
    are just a few examples:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何你可以想到的工具都可以添加到聊天机器人系统中。这里只是几个例子：
- en: A unit or currency conversion tool.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单位或货币转换工具。
- en: A weather tool.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个天气预报工具。
- en: A tool to upload a document.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个上传文档的工具。
- en: 'A code interpreter: for data analysis, plotting, or running simulations.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代码解释器：用于数据分析、绘图或运行模拟。
- en: An integration with an external system, for example, Wolfram Alpha for symbolic
    math, plots, and scientific knowledge.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与外部系统的集成，例如，Wolfram Alpha用于符号数学、绘图和科学知识。
- en: A nuclear missile tool…​or not! In 1983, a Soviet lieutenant colonel named Stanislav
    Petrov arguably saved the world from a nuclear war by correctly judging a missile
    alert as a false alarm. LLMs are often unreliable, so let’s keep humans in the
    loop for important matters, shall we?
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个核导弹工具……或者不是！1983年，一名苏联上校斯坦尼斯拉夫·彼得罗夫通过正确判断导弹警报为误报，可能拯救了世界免于核战争。LLM通常不可靠，所以对于重要事项，我们还是让人类参与其中，好吗？
- en: Model Context Protocol
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型上下文协议
- en: If you’re interested in tool-augmented chatbots and agents, you should definitely
    check out the [*Model Context Protocol* (MCP)](https://homl.info/mcp), an open
    standard proposed by Anthropic that specifies how your AI system can communicate
    with *MCP servers* to get access to all sorts of tools and resources, such as
    the ones listed in [Anthropic’s MCP server repository](https://github.com/modelcontextprotocol/servers).
    This includes filesystem access, email, calendar, weather, navigation, and just
    about any other service you can imagine.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对手动增强聊天机器人和代理感兴趣，你绝对应该查看[*模型上下文协议*（MCP）](https://homl.info/mcp），这是Anthropic提出的一个开放标准，它规定了你的AI系统如何与*MCP服务器*通信以获取访问各种工具和资源，例如在[Anthropic的MCP服务器存储库](https://github.com/modelcontextprotocol/servers)中列出的那些。这包括文件系统访问、电子邮件、日历、天气、导航以及你几乎可以想象到的任何其他服务。
- en: 'MCP does not specify anything about the LLM itself: it’s your LLM orchestrator’s
    responsibility to interact with the LLM and detect when it wants to access a given
    tool or resource. For example, you might include instructions in the LLM’s system
    prompt telling it that it can output a custom JSON message such as `{"tool": "weather",
    "location": "Paris"}` whenever it needs to know the weather in some location (e.g.
    Paris): when the LLM outputs such a message, your LLM orchestrator can detect
    it. That’s when MCP comes in: your orchestrator sends an MCP request (i.e., an
    MCP-compliant JSON message) to a weather MCP server, and once it gets the response
    (i.e., another MCP-compliant JSON message), it can feed the response to the LLM,
    which can use it to compose a good answer for the user (e.g., “It will be sunny
    today in Paris, with a high of 23°C”.).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 'MCP没有指定关于LLM本身的内容：与LLM交互并检测它何时想要访问某个工具或资源的责任在于你的LLM协调器。例如，你可以在LLM的系统提示中包含指令，告诉它每当它需要知道某个地点的天气时（例如巴黎），它可以输出一个自定义的JSON消息，例如`{"tool":
    "weather", "location": "Paris"}`：当LLM输出这样的消息时，你的LLM协调器可以检测到它。这时MCP就派上用场了：你的协调器向天气MCP服务器发送一个MCP请求（即一个符合MCP的JSON消息），一旦它收到响应（即另一个符合MCP的JSON消息），它可以将响应反馈给LLM，LLM可以使用它来为用户编写一个良好的答案（例如，“今天巴黎将晴朗，最高气温23°C”。）。'
- en: Tip
  id: totrans-468
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The LLM can be instructed to output MCP requests directly rather than custom
    JSON messages. This way, the orchestrator can just validate the JSON request and
    determine which MCP server to forward it to (e.g., the weather server).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指导LLM直接输出MCP请求，而不是自定义JSON消息。这样，编排器只需验证JSON请求并确定将其转发到哪个MCP服务器（例如，天气服务器）。
- en: 'But why use MCP rather than a more common protocol such as REST or gRPC? Aren’t
    we’re just querying an API? Well, it’s more than that:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么使用MCP而不是更常见的协议，如REST或gRPC？我们不是只是在查询API吗？好吧，这不仅仅是那样：
- en: Firstly, the connections between the LLM orchestrator and the MCP servers are
    long-lived, allowing fast, stateful, and bidirectional communication. In the MCP
    architecture, the client-side components that manage these connections are called
    *MCP clients*. The software that hosts them—typically your LLM orchestrator—is
    referred to as the *MCP host*.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，LLM编排器与MCP服务器之间的连接是长连接，允许快速、有状态和双向通信。在MCP架构中，管理这些连接的客户端组件被称为*MCP客户端*。托管这些组件的软件——通常是您的LLM编排器——被称为*MCP主机*。
- en: 'Secondly, MCP includes an AI-friendly *discovery mechanism* which lets the
    MCP client ask the MCP server for a rich, textual description of what the service
    does, and how exactly to use it, including the list of available functions and
    their parameters. In other words, it’s a self-documented API for AIs. In fact,
    the MCP server can also ask the MCP client for its capabilities, for example whether
    it supports displaying images to the user or handling streaming output: this lets
    the server adapt its responses accordingly.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，MCP包含一个对AI友好的*发现机制*，允许MCP客户端向MCP服务器请求关于服务做什么以及如何确切使用它的丰富文本描述，包括可用函数及其参数的列表。换句话说，它是一个为AI提供的自文档API。事实上，MCP服务器还可以向MCP客户端请求其功能，例如是否支持向用户显示图像或处理流式输出：这使服务器能够相应地调整其响应。
- en: 'The real power of MCP comes when you tell the LLM which services are available
    and instruct it on how to access the discovery mechanism: your LLM can then figure
    out on its own what each available service does, and how to use it. Connecting
    your LLM to a new MCP server then becomes little more than adding the server to
    the orchestrator’s configuration and telling the LLM about it.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: MCP的真正威力在于您告诉LLM哪些服务可用，并指导它如何访问发现机制：然后您的LLM可以自行了解每个可用服务的作用以及如何使用它。将您的LLM连接到新的MCP服务器几乎只需将其添加到编排器的配置中，并告诉LLM关于它。
- en: That said, building a chatbot from scratch can be complex, and fortunately many
    libraries and tools are available to simplify the process. Let’s look at some
    of the most popular ones.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，从头开始构建聊天机器人可能很复杂，幸运的是，有许多库和工具可以简化这个过程。让我们看看其中一些最受欢迎的。
- en: Libraries and Tools
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库和工具
- en: 'Various open source Python libraries are available to implement your own chatbot
    system, including:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于实现您自己的聊天机器人系统的各种开源Python库包括：
- en: '[LangChain](https://www.langchain.com)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangChain](https://www.langchain.com)'
- en: A library designed to help you build applications powered by LLMs, by chaining
    together components such as prompt templates, models, memory, and other tools.
    It simplifies the orchestration of complex workflows.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 一个旨在帮助您构建由LLM驱动的应用程序的库，通过连接提示模板、模型、内存和其他工具等组件。它简化了复杂工作流的编排。
- en: '[LangGraph](https://www.langchain.com/langgraph)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangGraph](https://www.langchain.com/langgraph)'
- en: This built on LangChain and is more specifically designed to build long-running
    stateful agentic workflows.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在LangChain的基础上构建的，并且更具体地设计用于构建长期运行的有状态代理工作流。
- en: '[Smolagents](https://homl.info/smolagents)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[Smolagents](https://homl.info/smolagents)'
- en: This is a Hugging Face library designed to build agentic systems. It is a standalone
    successor to the Transformers Agents library.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Hugging Face库，旨在构建代理系统。它是Transformers Agents库的独立继任者。
- en: '[Haystack](https://haystack.deepset.ai)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '[Haystack](https://haystack.deepset.ai)'
- en: Haystack lets you build systems that can understand complex questions, retrieve
    relevant information, and provide accurate answers, typically using RAG.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack让您构建能够理解复杂问题、检索相关信息并提供准确答案的系统，通常使用RAG。
- en: '[LlamaIndex](https://www.llamaindex.ai)'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[LlamaIndex](https://www.llamaindex.ai)'
- en: LlamaIndex lets you ingest, index, and query your data (e.g., PDFs, databases,
    APIs).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: LlamaIndex让您能够摄取、索引和查询您的数据（例如，PDF、数据库、API）。
- en: 'There are also several popular open source user interfaces to chat with LLMs
    locally:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些流行的开源用户界面，可以在本地与LLM进行聊天：
- en: '[LM Studio](https://lmstudio.ai)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '[LM Studio](https://lmstudio.ai)'
- en: This is a nice GUI app which lets you easily download and chat with various
    models. It supports chat history, prompt formatting, and a few other features.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的GUI应用程序，让你可以轻松下载并与其他模型聊天。它支持聊天历史记录、提示格式化以及一些其他功能。
- en: '[Ollama](https://ollama.com)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.com)'
- en: This is a simple command-line tool that lets you download various LLMs and chat
    with them locally, right in your terminal (e.g., `ollama run mistral:7b`). Ollama
    can also act as an API server, which can be queried by other systems (e.g., LangChain).
    The `ollama` Python library lets you query this API easily. Ollama also has support
    for tools such as a calculator, web search, and more.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的命令行工具，允许你下载各种LLMs并在本地与他们聊天，直接在你的终端中（例如，`ollama run mistral:7b`）。Ollama还可以作为API服务器，可以被其他系统查询（例如LangChain）。`ollama`
    Python库让你可以轻松查询这个API。Ollama还支持计算器、网络搜索等工具。
- en: '[text-generation-webui](https://homl.info/tgw) (TGWUI)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[text-generation-webui](https://homl.info/tgw) (TGWUI)'
- en: This is a web interface for chatting with local LLMs. It’s one of the most feature-rich
    and flexible tools available for local LLM use. It has a plug-in system that lets
    you add a calculator, a document loader, a search tool, and more. It also includes
    a REST API for integration with other systems like LangChain.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于与本地LLMs聊天的网页界面。它是本地LLM使用中最功能丰富和灵活的工具之一。它有一个插件系统，允许你添加计算器、文档加载器、搜索工具等。它还包括一个REST
    API，用于与其他系统如LangChain集成。
- en: Under the hood, these tools require a backend library to actually run the LLMs.
    LM Studio and Ollama are based on a highly optimized C++ library named [llama.cpp](https://github.com/ggml-org/llama.cpp),
    while TGWUI supports multiple backends, including llama.cpp, the Transformers
    library, ExLlama, AutoGPTQ, and more, so you can pick the backend that runs best
    on your hardware.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这些工具需要后端库来实际运行LLMs。LM Studio和Ollama基于一个高度优化的C++库，名为[llama.cpp](https://github.com/ggml-org/llama.cpp)，而TGWUI支持多个后端，包括llama.cpp、Transformers库、ExLlama、AutoGPTQ等，因此你可以选择在硬件上运行最好的后端。
- en: With that, you should have everything you need. For example, you could use LangChain
    to orchestrate a workflow that uses Ollama to run a local LLM, and Haystack to
    retrieve relevant information from a vector database. Before we close this chapter,
    let’s take a brief look at some of the most influential encoder-decoder transformers.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你应该拥有所需的一切。例如，你可以使用LangChain来编排一个工作流程，使用Ollama运行本地LLM，并使用Haystack从向量数据库中检索相关信息。在我们结束本章之前，让我们简要地看看一些最有影响力的编码器-解码器Transformer。
- en: Encoder-Decoder Models
  id: totrans-496
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器模型
- en: In this chapter, other than the original Transformer architecture, we have focused
    solely on encoder-only and decoder-only models. This might have given you the
    impression that encoder-decoder models are over, but for some problems, they are
    still very relevant, especially for tasks like translation or summarization. Indeed,
    since the encoder is bidirectional, it can encode the source text and output excellent
    contextual embeddings, which the decoder can then use to produce a better output
    than a decoder-only model would (at least for models of a similar size).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，除了原始的Transformer架构外，我们主要关注了仅编码器模型和仅解码器模型。这可能会让你觉得编码器-解码器模型已经过时了，但对于某些问题，它们仍然非常相关，尤其是在翻译或摘要等任务中。确实，由于编码器是双向的，它可以编码源文本并输出优秀的上下文嵌入，解码器可以使用这些嵌入产生比仅解码器模型更好的输出（至少对于类似大小的模型）。
- en: 'The [T5 model](https://homl.info/t5)⁠^([27](ch15.html#id3708)) released by
    Google in 2019 is a particularly influential encoder-decoder model: it was the
    first to frame all NLP tasks as text to text. For example, to translate “I like
    soccer” to Spanish, you can just call the model with the input sentence “translate
    English to Spanish: I like soccer”, and it outputs “me gusta el fútbol”. To summarize
    a paragraph, you enter “summarize:” followed by the paragraph, and it outputs
    the summary. For classification, you only need to change the prefix to “classify:”,
    and the model outputs the class name as text. For zero-shot classification, the
    possible classes can be listed in the prompt. This text-to-text approach makes
    the model very easy to pretrain on a variety of language tasks and just as easy
    to use. T5 was pretrained using a *masked span corruption* objective, similar
    to MLM, but masking one or more contiguous sections.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '2019 年谷歌发布的 [T5 模型](https://homl.info/t5)⁠^([27](ch15.html#id3708)) 是一个特别有影响力的编码器-解码器模型：它首次将所有
    NLP 任务框架化为文本到文本。例如，要将“我喜欢足球”翻译成西班牙语，你只需调用模型并输入句子“translate English to Spanish:
    I like soccer”，它就会输出“me gusta el fútbol”。要总结一段文字，你只需输入“summarize:”后跟段落，它就会输出摘要。对于分类，你只需将前缀更改为“classify:”，模型就会以文本形式输出类别名称。对于零样本分类，可以在提示中列出可能的类别。这种文本到文本的方法使得模型在多种语言任务上的预训练非常容易，同样使用起来也非常方便。T5
    使用 *掩码跨度损坏* 目标进行预训练，类似于 MLM，但掩码一个或多个连续的段落。'
- en: 'Google also released several variants of T5:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌还发布了 T5 的几个变体：
- en: mT5 (2020)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: mT5 (2020)
- en: This is a multilingual T5 supporting over 100 languages. It’s great for translation
    and cross-lingual tasks (e.g., asking a question in English about a Spanish text).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个支持超过 100 种语言的跨语言 T5。它非常适合翻译和跨语言任务（例如，用英语询问关于西班牙语文本的问题）。
- en: ByT5 (2021)
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ByT5 (2021)
- en: This is a byte-level variant of T5 that removes the need for tokenization entirely
    (not even BPE). However, this approach has not caught on as it’s more efficient
    to use tokenizers.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个字节级别的 T5 变体，完全消除了对分词的需求（甚至不是 BPE）。然而，这种方法并没有流行起来，因为使用分词器更有效率。
- en: FLAN-T5 (2022)
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5 (2022)
- en: This is an instruction-tuned T5, with excellent ZSL and FSL capability.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个指令调整的 T5，具有出色的零样本学习（ZSL）和Few-shot学习（FSL）能力。
- en: UL2 (2022)
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: UL2 (2022)
- en: This is pretrained using several objectives, including masked span denoising
    like T5, but also standard next token prediction, and masked token prediction.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用多个目标进行预训练，包括类似于 T5 的掩码跨度去噪，但也包括标准的下一个标记预测和掩码标记预测。
- en: FLAN-UL2 (2023)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-UL2 (2023)
- en: This improved on UL2 using instruction tuning.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过指令调整改进了 UL2。
- en: 'Meta also released some encoder-decoder models, starting with BART in 2020\.
    This model was pretrained using a denoising objective: the model gets a corrupted
    text (e.g., masked, modified, deleted, or inserted tokens, shuffled sentences,
    etc.) and it must clean it up. It’s particularly effective for text generation
    and summarization. There’s also a multilingual variant named mBART.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 也发布了一些编码器-解码器模型，从 2020 年的 BART 开始。该模型使用去噪目标进行预训练：模型获得一个损坏的文本（例如，掩码、修改、删除或插入标记、打乱句子等），并必须对其进行清理。这对于文本生成和摘要特别有效。还有一个名为
    mBART 的多语言变体。
- en: Last but not least, the encoder-decoder architecture is quite common for vision
    models, typically when there are multiple outputs such as in object detection
    and image segmentation. They’re also common for multimodal models. This leads
    us to the next chapter, where we will discuss vision transformers and multimodal
    transformers. It’s time for transformers to open their eyes!
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，编码器-解码器架构在视觉模型中相当常见，尤其是在存在多个输出时，例如在目标检测和图像分割中。它们在多模态模型中也同样常见。这引出了下一章，我们将讨论视觉转换器和多模态转换器。是时候让转换器睁开眼睛了！
- en: Exercises
  id: totrans-512
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is the most important layer in the Transformer architecture? What is its
    purpose?
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在转换器架构中最重要的层是什么？它的作用是什么？
- en: Why does the Transformer architecture need positional encodings?
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么转换器架构需要位置编码？
- en: What tasks are encoder-only models best at? How about decoder-only models? And
    encoder-decoder models?
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器仅模型最适合哪些任务？解码器仅模型呢？以及编码器-解码器模型？
- en: What is the most important technique used to pretrain BERT?
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练 BERT 最重要的技术是什么？
- en: Can you name four BERT variants and explain their main benefits?
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出四种 BERT 变体并解释它们的主要优点吗？
- en: What is the main task used to pretrain GPT and its successors?
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要用于预训练 GPT 及其后续模型的任务是什么？
- en: The `generate()` method has many arguments, including `do_sample`, `top_k`,
    `top_p`, `temperature`, and `num_beams`. What do these five arguments do?
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`generate()`方法有许多参数，包括`do_sample`、`top_k`、`top_p`、`temperature`和`num_beams`。这五个参数分别做什么？'
- en: What is prompt engineering? Can you describe five prompt engineering techniques?
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是提示工程？你能描述五种提示工程技术吗？
- en: What are the main steps to build a chatbot, starting from a pretrained decoder-only
    model?
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预训练的仅解码器模型开始构建聊天机器人的主要步骤是什么？
- en: How can a chatbot use tools like a calculator or web search?
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天机器人如何使用计算器或网络搜索等工具？
- en: What is MCP used for?
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MCP用于什么？
- en: Fine-tune BERT for sentiment analysis on the IMDb dataset.
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IMDb数据集上微调BERT进行情感分析。
- en: Fine-tune GPT-2 on the Shakespeare dataset (from [Chapter 14](ch14.html#nlp_chapter)),
    then generate Shakespeare-like text.
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Shakespeare数据集（来自[第14章](ch14.html#nlp_chapter)）上微调GPT-2，然后生成类似莎士比亚的文本。
- en: Download the [Wikipedia Movie Plots dataset](https://homl.info/movieplots),
    and use SBERT to embed every movie description. Then write a function that takes
    a search query, embeds it, finds the most similar embeddings, and lists the corresponding
    movies.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载[维基百科电影情节数据集](https://homl.info/movieplots)，并使用SBERT将每个电影描述嵌入。然后编写一个函数，该函数接受一个搜索查询，将其嵌入，找到最相似的嵌入，并列出相应的电影。
- en: Use an instruction-tuned model such as Qwen-7B-Instruct to build a little chatbot
    which acts like a movie expert. Then try adding some RAG functionality, for example
    by automatically injecting the most relevant movie plot into the prompt (see the
    previous exercise).
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Qwen-7B-Instruct这样的指令调整模型构建一个像电影专家一样的聊天机器人。然后尝试添加一些RAG功能，例如通过自动将最相关的电影情节注入提示中（参见前面的练习）。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，见[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch15.html#id3414-marker)) Ashish Vaswani et al., “Attention Is All You
    Need”, *Proceedings of the 31st International Conference on Neural Information
    Processing Systems* (2017): 6000–6010.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch15.html#id3414-marker)) Ashish Vaswani等人，“Attention Is All You Need”，*第31届国际神经网络信息处理系统会议论文集*（2017年）：6000–6010。
- en: ^([2](ch15.html#id3417-marker)) When applying a linear layer to a sequence,
    all tokens are treated independently, using the same parameters. This is equivalent
    to using a `Conv1d` layer with `kernel_size=1`. This is why you will sometimes
    see Transformer diagrams showing convolutional layers instead of linear layers.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch15.html#id3417-marker)) 在将线性层应用于序列时，所有标记都被独立处理，使用相同的参数。这相当于使用`Conv1d`层，其中`kernel_size=1`。这就是为什么有时你会看到Transformer图中显示的是卷积层而不是线性层。
- en: ^([3](ch15.html#id3423-marker)) The number of parameters is not public for some
    models (e.g., Gemini models), so I used some rough estimates. Also, many models
    have smaller variants, not shown here. Lastly, several other organizations released
    influential models, such as the Allen Institute for AI (Ai2), Amazon, Baidu, Beijing
    Academy of AI, Cohere, Huawei, LAION, LMSYS, Nvidia, Stanford University, Talent
    International Institute (TII), Tsinghua University, xAI, Zhipu AI, and others.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch15.html#id3423-marker)) 一些模型（例如Gemini模型）的参数数量不是公开的，所以我使用了一些粗略的估计。此外，许多模型都有更小的变体，这里没有展示。最后，其他一些组织发布了有影响力的模型，例如艾伦人工智能研究所（Ai2）、亚马逊、百度、北京人工智能研究院、Cohere、华为、LAION、LMSYS、Nvidia、斯坦福大学、国际人才研究院（TII）、清华大学、xAI、智谱AI等。
- en: ^([4](ch15.html#id3427-marker)) This is adapted from Figure 1 from “Attention
    Is All You Need”, with the kind permission of the authors.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch15.html#id3427-marker)) 这是从“Attention Is All You Need”中的图1改编的，得到了作者的许可。
- en: ^([5](ch15.html#id3439-marker)) Queries, keys, and values were introduced in
    [Chapter 14](ch14.html#nlp_chapter) when we discussed dot-product attention.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch15.html#id3439-marker)) 查询、键和值是在[第14章](ch14.html#nlp_chapter)中介绍的，当时我们讨论了点积注意力。
- en: ^([6](ch15.html#id3452-marker)) This is adapted from the righthand part of Figure
    2 from “Attention Is All You Need”, with the kind authorization of the authors.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch15.html#id3452-marker)) 这是从“Attention Is All You Need”中的图2的右侧改编的，得到了作者的授权。
- en: '^([7](ch15.html#id3475-marker)) Jacob Devlin et al., “BERT: Pre-Training of
    Deep Bidirectional Transformers for Language Understanding”, *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies* 1 (2019).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch15.html#id3475-marker)) Jacob Devlin等人，“BERT：用于语言理解的深度双向Transformer预训练”，*北美计算语言学协会第2018年会议论文集：人机语言技术*第1卷（2019年）。
- en: ^([8](ch15.html#id3498-marker)) This is adapted from Figure 1 from the BERT
    paper, with the kind authorization of the authors.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch15.html#id3498-marker)) 这是从BERT论文中的图1改编的，得到了作者的友好授权。
- en: '^([9](ch15.html#id3515-marker)) Nils Reimers, Iryna Gurevych, “Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks”, arXiv preprint arXiv:1908.10084
    (2019).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch15.html#id3515-marker)) 尼尔斯·雷梅尔斯，伊里娜·古列维奇，“Sentence-BERT：使用Siamese BERT-Networks的句子嵌入”，arXiv预印本
    arXiv:1908.10084 (2019)。
- en: ^([10](ch15.html#id3531-marker)) Geoffrey Hinton et al., “Distilling the Knowledge
    in a Neural Network”, arXiv preprint arXiv:1503.02531 (2015).
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch15.html#id3531-marker)) 几何·辛顿等人，“从神经网络中蒸馏知识”，arXiv预印本 arXiv:1503.02531
    (2015)。
- en: ^([11](ch15.html#id3567-marker)) Alec Radford et al., [“Improving Language Understanding
    by Generative Pre-Training”](https://homl.info/gpt) (2018).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch15.html#id3567-marker)) 阿莱克·拉德福德等人，“通过生成预训练提高语言理解”，[“Improving Language
    Understanding by Generative Pre-Training”](https://homl.info/gpt) (2018)。
- en: ^([12](ch15.html#id3586-marker)) Alec Radford et al., [“Language Models Are
    Unsupervised Multitask Learners”](https://homl.info/gpt2) (2019).
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch15.html#id3586-marker)) 阿莱克·拉德福德等人，“语言模型是无监督的多任务学习者”，[“Language Models
    Are Unsupervised Multitask Learners”](https://homl.info/gpt2) (2019)。
- en: ^([13](ch15.html#id3587-marker)) There were a few minor tweaks, such as using
    pre-LN rather than post-LN, downscaling the weights depending on the number of
    residual layers, and tweaks to the BPE tokenizer. Please see the paper for more
    details.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch15.html#id3587-marker)) 进行了一些小的调整，例如使用预LN而不是后LN，根据残差层数量调整权重，以及调整BPE分词器。更多细节请参阅论文。
- en: ^([14](ch15.html#id3595-marker)) Tom B. Brown et al., “Language Models are Few-Shot
    Learners”, arXiv preprint arXiv:2005.14165 (2020).
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch15.html#id3595-marker)) 托马斯·B·布朗等人，“语言模型是少样本学习者”，arXiv预印本 arXiv:2005.14165
    (2020)。
- en: ^([15](ch15.html#id3613-marker)) If you are not running the notebook on Colab,
    you can save the access token in a file and load its content in your code to avoid
    hardcoding it. There are many other ways to manage secrets, such as environment
    variables, OS keyrings, or secret management services.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch15.html#id3613-marker)) 如果你没有在Colab上运行笔记本，你可以将访问令牌保存到文件中，并在你的代码中加载其内容，以避免硬编码。还有许多其他管理机密的方法，例如环境变量、OS密钥圈或密钥管理服务。
- en: ^([16](ch15.html#id3618-marker)) For the Vatican, it answers Rome, which contains
    Vatican City. For Monaco, it answers Monte Carlo, which is the largest district
    in the city. For Burundi, it answers Bujumbura, which was the capital city until
    2019\. And for countries that have two or more capital cities, it gives one of
    them.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch15.html#id3618-marker)) 对于梵蒂冈，它回答罗马，罗马包含梵蒂冈城。对于摩纳哥，它回答蒙特卡洛，这是城市中最大的区。对于布隆迪，它回答布琼布拉，直到2019年一直是首都。对于有两个或更多首都的国家，它给出其中一个。
- en: ^([17](ch15.html#id3623-marker)) Brian Lester et al., “The Power of Scale for
    Parameter-Efficient Prompt Tuning”, arXiv preprint arXiv:2104.08691 (2021).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch15.html#id3623-marker)) 布莱恩·莱斯特等人，“规模的力量对参数高效提示调整的影响”，arXiv预印本 arXiv:2104.08691
    (2021)。
- en: ^([18](ch15.html#id3627-marker)) Jason Wei et al., “Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models”, arXiv preprint arxiv 2201.11903 (2022).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch15.html#id3627-marker)) 杰森·魏等人，“思维链提示激发大型语言模型中的推理”，arXiv预印本 arxiv 2201.11903
    (2022)。
- en: ^([19](ch15.html#id3629-marker)) Xuezhi Wang et al., “Self-Consistency Improves
    Chain of Thought Reasoning in Language Models”, arXiv preprint arXiv:2203.11171
    (2022).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch15.html#id3629-marker)) 王雪志等人，“自我一致性提高语言模型中的思维链推理”，arXiv预印本 arXiv:2203.11171
    (2022)。
- en: '^([20](ch15.html#id3631-marker)) Shunyu Yao et al., “Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models”, arXiv preprint arXiv:2305.10601 (2023).'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch15.html#id3631-marker)) 邵宇尧等人，“思维树：大型语言模型中的深思熟虑问题解决”，arXiv预印本 arXiv:2305.10601
    (2023)。
- en: ^([21](ch15.html#id3634-marker)) Yilun Du et al., “Improving Factuality and
    Reasoning in Language Models through Multiagent Debate”, arXiv preprint arXiv:2305.14325
    (2023).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch15.html#id3634-marker)) 杜一伦等人，“通过多智能体辩论提高语言模型的事实性和推理能力”，arXiv预印本 arXiv:2305.14325
    (2023)。
- en: '^([22](ch15.html#id3635-marker)) Aman Madaan et al., “Self-Refine: Iterative
    Refinement with Self-Feedback”, arXiv preprint arXiv:2303.17651 (2023).'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch15.html#id3635-marker)) 阿曼·马达安等人，“自我精炼：带有自我反馈的迭代精炼”，arXiv预印本 arXiv:2303.17651
    (2023)。
- en: ^([23](ch15.html#id3639-marker)) Patrick Lewis et al., “Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”, arXiv preprint arXiv:2005.11401
    (2020).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch15.html#id3639-marker)) 帕特里克·刘易斯等人，“用于知识密集型NLP任务的检索增强生成”，arXiv预印本 arXiv:2005.11401
    (2020)。
- en: ^([24](ch15.html#id3655-marker)) Paul Christiano et al., “Deep reinforcement
    learning from human preferences”, arXiv preprint arXiv:1706.03741 (2017).
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch15.html#id3655-marker)) Paul Christiano等人，“从人类偏好中进行深度强化学习”，arXiv预印本
    arXiv:1706.03741 (2017)。
- en: '^([25](ch15.html#id3661-marker)) Rafael Rafailov et al., “Direct Preference
    Optimization: Your Language Model is Secretly a Reward Model”, arXiv preprint
    arXiv:2305.18290 (2023).'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch15.html#id3661-marker)) Rafael Rafailov等人，“直接偏好优化：你的语言模型实际上是一个奖励模型”，arXiv预印本
    arXiv:2305.18290 (2023)。
- en: ^([26](ch15.html#id3667-marker)) There’s a slight difference for the tokens
    “Argentina” and “is”, which I assume is due to the accumulation of floating-point
    errors in this large model.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch15.html#id3667-marker)) 对于“阿根廷”和“是”这两个标记，存在细微的差异，我认为这可能是由于在这个大型模型中浮点误差的累积造成的。
- en: ^([27](ch15.html#id3708-marker)) Colin Raffel et al., “Exploring the Limits
    of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv preprint
    arXiv:1910.10683 (2019).
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch15.html#id3708-marker)) Colin Raffel等人，“使用统一的文本到文本转换器探索迁移学习的极限”，arXiv预印本
    arXiv:1910.10683 (2019)。
