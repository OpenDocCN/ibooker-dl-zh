- en: Chapter 7\. Recurrent Neural Networks for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章\. 自然语言处理中的循环神经网络
- en: 'In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you saw how to tokenize and sequence text, turning sentences into tensors of numbers
    that could then be fed into a neural network. You then extended that in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    by looking at embeddings, which constitute a way to have words with similar meanings
    cluster together to enable the calculation of sentiment. This worked really well,
    as you saw by building a sarcasm classifier. But there’s a limitation to that:
    namely, sentences aren’t just collections of words—and often, the *order* in which
    the words appear will dictate their overall meaning. Also, adjectives can add
    to or change the meaning of the nouns they appear beside. For example, the word
    *blue* might be meaningless from a sentiment perspective, as might *sky*, but
    when you put them together to get *blue sky*, it indicates a clear sentiment that’s
    usually positive. Finally, some nouns may qualify others, such as in *rain cloud*,
    *writing desk*, and *coffee mug*.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中，你看到了如何标记和序列化文本，将句子转换为可以被神经网络输入的数字张量。然后你在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中扩展了这一点，通过查看嵌入，这些嵌入构成了一种让具有相似意义的单词聚集在一起的方法，以便计算情感。正如你所看到的，通过构建讽刺分类器，这确实很有效。但是，这有一个限制：即句子不仅仅是单词的集合——而且，单词出现的*顺序*往往决定了它们的整体意义。此外，形容词可以增加或改变它们旁边名词的意义。例如，单词*蓝色*从情感角度来看可能没有意义，*天空*也是如此，但是当你把它们放在一起得到*蓝天*时，它表明了一种通常积极的明确情感。最后，一些名词可能对其他名词有资格限制，例如在*雨云*、*写字台*和*咖啡杯*中。
- en: 'To take sequences like this into account, you need to take an additional approach:
    you need to factor *recurrence* into the model architecture. In this chapter,
    you’ll look at different ways of doing this. We’ll explore how sequence information
    can be learned and how you can use this information to create a type of model
    that is better able to understand text: the *recurrent neural network* (RNN).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑这样的序列，你需要采取一个额外的方法：你需要在模型架构中考虑*递归*。在本章中，你将了解不同的实现方式。我们将探讨如何学习序列信息，以及你如何利用这些信息来创建一种能够更好地理解文本的模型：*循环神经网络*（RNN）。
- en: The Basis of Recurrence
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归的基础
- en: To understand how recurrence might work, let’s first consider the limitations
    of the models used thus far in the book. Ultimately, creating a model looks a
    little bit like [Figure 7-1](#ch07_figure_1_1748549654877957). You provide data
    and labels and define a model architecture, and the model learns the rules that
    fit the data to the labels. Those rules then become available to you as an application
    programming interface (API) that will give you back predicted labels for future
    data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解递归可能的工作方式，让我们首先考虑本书迄今为止使用的模型的局限性。最终，创建一个模型看起来有点像[图7-1](#ch07_figure_1_1748549654877957)。你提供数据和标签，并定义模型架构，然后模型学习适合数据的规则，这些规则随后作为应用程序编程接口（API）提供给你，为你提供对未来数据的预测标签。
- en: '![](assets/aiml_0701.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0701.png)'
- en: Figure 7-1\. High-level view of model creation
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 模型创建的高级视图
- en: But, as you can see, the data is lumped in wholesale. There’s no granularity
    involved and no effort to understand the sequence in which that data occurs. This
    means the words *blue* and *sky* have no different meaning in sentences such as,
    “Today I am blue, because the sky is gray,” and “Today I am happy, and there’s
    a beautiful blue sky.” To us, the difference in the use of these words is obvious,
    but to a model, with the architecture shown here, there really is no difference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如你所见，数据是整体处理的。没有涉及粒度，也没有努力理解数据发生的顺序。这意味着单词*蓝色*和*天空*在句子“今天我很沮丧，因为天空是灰的”和“今天我很高兴，有一片美丽的蓝天”中没有任何不同的意义。对我们来说，这些词的使用差异是明显的，但对具有这里所示架构的模型来说，实际上并没有差异。
- en: So, how do we fix this? Let’s first explore the nature of recurrence, and from
    there, you’ll be able to see how a basic RNN can work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何解决这个问题呢？让我们首先探索递归的本质，然后你将能够看到基本的RNN是如何工作的。
- en: Consider the famous Fibonacci sequence of numbers. In case you aren’t familiar
    with it, I’ve put some of them into [Figure 7-2](#ch07_figure_2_1748549654878009).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑著名的斐波那契数列。如果你不熟悉它，我已经在[图7-2](#ch07_figure_2_1748549654878009)中列出了一些。
- en: '![](assets/aiml_0702.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0702.png)'
- en: Figure 7-2\. The first few numbers in the Fibonacci sequence
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 斐波那契序列的前几个数字
- en: The idea behind this sequence is that every number is the sum of the two numbers
    preceding it. So if we start with 1 and 2, the next number is 1 + 2, which is
    3\. The one after that is 2 + 3, which is 5, and then there’s 3 + 5, which is
    8, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列背后的想法是每个数字都是它前面两个数字的和。所以如果我们从1和2开始，下一个数字是1 + 2，即3。接下来的数字是2 + 3，即5，然后是3 +
    5，即8，以此类推。
- en: We can place this in a computational graph to get [Figure 7-3](#ch07_figure_3_1748549654878036).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个放在计算图中，得到[图7-3](#ch07_figure_3_1748549654878036)。
- en: '![](assets/aiml_0703.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0703.png](assets/aiml_0703.png)'
- en: Figure 7-3\. A computational graph representation of the Fibonacci sequence
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 斐波那契序列的计算图表示
- en: Here, you can see that we feed 1 and 2 into the function and get 3 as the output.
    We then carry the second parameter (2) over to the next step and feed it into
    the function along with the output from the previous step (3). The output of this
    is 5, and it gets fed into the function with the second parameter from the previous
    step (3) to produce an output of 8\. This process continues indefinitely, with
    every operation depending on those before it. The 1 at the top left sort of “survives”
    through the process—it’s an element of the 3 that gets fed into the second operation,
    it’s an element of the 5 that gets fed into the third operation, and so on. Thus,
    some of the essence of the 1 is preserved throughout the sequence, though its
    impact on the overall value is diminished.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到我们将1和2输入到函数中，得到输出3。然后我们将第二个参数（2）带到下一步，并连同前一步的输出（3）一起输入到函数中。这个输出的结果是5，然后它与前一步的第二个参数（3）一起输入到函数中，产生输出8。这个过程无限期地继续下去，每个操作都依赖于之前的操作。左上角的1在过程中“存活”下来——它是被输入到第二个操作中的3的一个元素，它是被输入到第三个操作中的5的一个元素，以此类推。因此，1的一些本质在整个序列中得到了保留，尽管它对整体值的影响减弱了。
- en: This is analogous to how a recurrent neuron is architected. You can see the
    typical representation of a recurrent neuron in [Figure 7-4](#ch07_figure_4_1748549654878060).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这与循环神经元的架构类似。您可以在[图7-4](#ch07_figure_4_1748549654878060)中看到循环神经元的典型表示。
- en: '![](assets/aiml_0704.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0704.png](assets/aiml_0704.png)'
- en: Figure 7-4\. A recurrent neuron
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 循环神经元
- en: A value *x* is fed into the function *F* at a time step, so it’s typically labeled
    *x*[*t*]. This produces an output *y* at that time step, which is typically labeled
    *y*[*t*]. It also produces a value that is fed forward to the next step, which
    is indicated by the arrow from *F* to itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值*x*在时间步长被输入到函数*F*中，所以它通常被标记为*x*[*t*]。这在该时间步长产生一个输出*y*，通常被标记为*y*[*t*]。它还产生一个传递到下一步的值，这由从*F*到自身的箭头表示。
- en: This is made a little clearer if you look at how recurrent neurons work beside
    one another across time steps, which you can see in [Figure 7-5](#ch07_figure_5_1748549654878082).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看循环神经元如何在时间步长中相互工作，这会使得这个过程更加清晰，您可以在[图7-5](#ch07_figure_5_1748549654878082)中看到这一点。
- en: Here, *x*[0] is operated on to get *y*[0] and a value that’s passed forward.
    The next step gets that value and *x*[1] and produces *y*[1] and a value that’s
    passed forward. The next one gets that value and *x*[2] and produces *y*[2] and
    a passed-forward value, and so on down the line. This is similar to what we saw
    with the Fibonacci sequence, and I always find it to be a handy mnemonic when
    trying to remember how an RNN works.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*[0]被操作以得到*y*[0]和一个传递的值。下一步得到这个值和*x*[1]，并产生*y*[1]和一个传递的值。接下来的一步得到这个值和*x*[2]，并产生*y*[2]和一个传递的值，依此类推。这与我们看到的斐波那契序列类似，而且我在尝试记住RNN的工作方式时，总是发现这是一个有用的记忆法。
- en: '![](assets/aiml_0705.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0705.png](assets/aiml_0705.png)'
- en: Figure 7-5\. Recurrent neurons in time steps
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. 时间步长中的循环神经元
- en: Extending Recurrence for Language
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展语言中的递归
- en: 'In the previous section, you saw how an RNN operating over several time steps
    can help maintain context across a sequence. Indeed, we’ll use RNNs for sequence
    modeling later in this book—but there’s a nuance when it comes to language that
    you can miss when using a simple RNN like those shown in [Figure 7-4](#ch07_figure_4_1748549654878060)
    and [Figure 7-5](#ch07_figure_5_1748549654878082). As in the Fibonacci sequence
    example mentioned earlier, the amount of context that’s carried over will diminish
    over time. The effect of the output of the neuron at step 1 is huge at step 2,
    smaller at step 3, smaller still at step 4, and so on. So, if we have a sentence
    like “Today has a beautiful blue <something>,” the word *blue* will have a strong
    impact on what the next word could be: we can guess that it’s likely to be *sky*.
    But what about context that comes from earlier in a sentence? For example, consider
    the sentence “I lived in Ireland, so in high school, I had to learn how to speak
    and write <something>.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了一个RNN如何在多个时间步长上操作以帮助在序列中维持上下文。确实，我们将在本书的后面部分使用RNN进行序列建模——但在处理语言时，使用像[图7-4](#ch07_figure_4_1748549654878060)和[图7-5](#ch07_figure_5_1748549654878082)中展示的简单RNN时，可能会错过一些细微之处。就像前面提到的斐波那契数列示例一样，携带的上下文量会随着时间的推移而减少。第1步神经元输出的影响在第2步时很大，在第3步时较小，在第4步时更小，依此类推。因此，如果我们有一个句子像“今天有一个美丽的蓝色<某物>”，那么单词*蓝色*将对下一个单词可能是什么有强烈的影响：我们可以猜测它很可能是*天空*。但是，句子中较早的部分的上下文怎么办？例如，考虑句子“我在爱尔兰生活过，所以在高中时，我不得不学习如何说和写<某物>。”
- en: That <something> is *Gaelic*, but the word that really gives us that context
    is *Ireland*, which is much earlier in the sentence. Thus, for us to be able to
    recognize what <something> should be, we need a way to preserve context across
    a longer distance. The short-term memory of an RNN needs to get longer, and in
    recognition of this, an enhancement to the architecture called *long short-term
    memory* (LSTM) was invented.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那个<某物>是*盖尔语*，但真正给我们这个上下文的单词是*爱尔兰*，它在句子中要早得多。因此，为了能够识别<某物>应该是什么，我们需要一种方法来在更长的距离上保持上下文。RNN的短期记忆需要更长，为此，人们发明了一种称为*长短期记忆*（LSTM）的架构增强。
- en: While I won’t go into detail on the underlying architecture of how LSTMs work,
    the high-level diagram shown in [Figure 7-6](#ch07_figure_6_1748549654878103)
    gets the main point across. To learn more about the internal operations of LSTM,
    check out Christopher Olah’s excellent [blog post on the subject](https://oreil.ly/6KcFA).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不会深入探讨LSTM工作原理的底层架构，但[图7-6](#ch07_figure_6_1748549654878103)中展示的高级图解已经清楚地说明了主要观点。要了解更多关于LSTM内部操作的信息，请查看Christopher
    Olah关于此主题的出色[博客文章](https://oreil.ly/6KcFA)。
- en: The LSTM architecture enhances the basic RNN by adding a “cell state” that enables
    context to be maintained not just from step to step but across the entire sequence
    of steps. Remembering that these are neurons that learn in the way neurons do,
    you can see that this enhancement ensures that the context that is important will
    be learned over time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM架构通过添加一个“细胞状态”来增强基本的RNN，这使得上下文不仅可以从一步到下一步维持，还可以在整个步骤序列中维持。记住这些是像神经元一样学习的神经元，你可以看到这种增强确保了随着时间的推移，重要的上下文将被学习。
- en: '![](assets/aiml_0706.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0706.png)'
- en: Figure 7-6\. High-level view of LSTM architecture
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6. LSTM架构的高级视图
- en: An important part of an LSTM is that it can be *bidirectional*—the time steps
    can be iterated both forward and backward so that context can be learned in both
    directions. Often, context for a word can come *after* it in the sentence and
    not just before.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的一个重要部分是它可以*双向*——时间步长可以向前和向后迭代，以便可以从两个方向学习上下文。通常，一个单词的上下文可以来自句子中的*之后*，而不仅仅是之前。
- en: See [Figure 7-7](#ch07_figure_7_1748549654878124) for a high-level view of this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[图7-7](#ch07_figure_7_1748549654878124)以了解其高级视图。
- en: '![](assets/aiml_0707.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0707.png)'
- en: Figure 7-7\. High-level view of LSTM bidirectional architecture
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7. LSTM双向架构的高级视图
- en: This is how evaluation in the direction from 0 to `number_of_steps` is done,
    and it’s also how evaluation from `number_of_steps` to 0 is done. At each step,
    the *y* result is an aggregation of the “forward” pass and the “backward” pass.
    You can see this in [Figure 7-8](#ch07_figure_8_1748549654878143).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是从0到`number_of_steps`方向的评估方式，也是从`number_of_steps`到0的评估方式。在每一步，*y*结果是对“正向”传递和“反向”传递的聚合。您可以在[图7-8](#ch07_figure_8_1748549654878143)中看到这一点。
- en: '![](assets/aiml_0708.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0708.png)'
- en: Figure 7-8\. Bidirectional LSTM
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8. 双向LSTM
- en: It’s easy to confuse the bidirectional nature of the LSTM with the terms *forward*
    and *backward* when it comes to the training of the network, but they’re very
    different. When I refer to the forward and backward pass, I’m referring to the
    setting of the parameters of the neurons and their updating from the learning
    process, respectively. Don’t confuse this with the values that the LSTM is paying
    attention to as being the next or previous tokens in the sequence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到网络的训练时，很容易将 LSTM 的双向性质与 *forward* 和 *backward* 这些术语混淆，但它们是非常不同的。当我提到正向和反向传递时，我指的是设置神经元参数以及它们从学习过程中进行更新的过程。不要将
    LSTM 关注的值（作为序列中的下一个或前一个标记）与此混淆。
- en: 'Also, consider each neuron at each time step to be F0, F1, F2, etc. The direction
    of the time step is shown, so the calculation at F1 in the forward direction is
    F1(->), and in the reverse direction, it’s (<-)F1\. The values of these are aggregated
    to give the *y* value for that time step. Additionally, the cell state is bidirectional,
    and this can be really useful for managing context in sentences. Again, considering
    the sentence “I lived in Ireland, so in high school, I had to learn how to speak
    and write <something>,” you can see how the <something> was qualified to be *Gaelic*
    by the context word *Ireland*. But what if it were the other way around: “I lived
    in <this country>, so in high school, I had to learn how to speak and write Gaelic”?
    You can see that by going *backward* through the sentence, we can learn about
    what <this country> should be. Thus, using bidirectional LSTMs can be very powerful
    for understanding sentiment in text. (And as you’ll see in [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453),
    they’re really powerful for generating text, too!)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将每个时间步的每个神经元视为 F0, F1, F2 等。时间步的方向已显示，因此正向计算 F1 的值为 F1(->)，反向计算为 (<-)F1\.
    这些值的总和给出了该时间步的 *y* 值。此外，细胞状态是双向的，这在管理句子中的上下文中非常有用。再次考虑句子“我在爱尔兰生活过，所以在高中时，我不得不学习如何说和写
    <something>”，你可以看到 <something> 是如何通过上下文词 *Ireland* 被认定为 *Gaelic* 的。但如果情况相反：“我在
    <this country> 生活过，所以在高中时，我不得不学习如何说和写 Gaelic”？你可以看到通过句子中的 *backward* 追溯，我们可以了解
    <this country> 应该是什么。因此，使用双向 LSTM 对于理解文本中的情感非常强大。（而且正如你将在第 8 章 [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453)
    中看到的那样，它们在生成文本方面也非常强大！）
- en: Of course, there’s a lot going on with LSTMs, in particular bidirectional ones,
    so expect training to be slow. Here’s where it’s worth investing in a GPU or at
    the very least using a hosted one in Google Colab if you can.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，LSTM（尤其是双向 LSTM）有很多内容，因此预期训练会较慢。这就是为什么值得投资 GPU，或者至少在 Google Colab 中使用托管 GPU
    的原因。
- en: Creating a Text Classifier with RNNs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNNs 创建文本分类器
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    you experimented with creating a classifier for the Sarcasm dataset by using embeddings.
    In that case, you turn words into vectors before aggregating them and then feeding
    them into dense layers for classification. But when you’re using an RNN layer
    such as an LSTM, you don’t do the aggregation, and you can feed the output of
    the embedding layer directly into the recurrent layer. When it comes to the dimensionality
    of the recurrent layer, a rule of thumb you’ll often see is that it’s the same
    size as the embedding dimension. This isn’t necessary, but it can be a good starting
    point. Also note that while in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    I mentioned that the embedding dimension is often the fourth root of the size
    of the vocabulary, when using RNNs, you’ll often see that that rule may be ignored
    because it would make the size of the recurrent layer too small.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 6 章 [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    中，你通过使用嵌入来创建 Sarcasm 数据集的分类器进行了实验。在那个例子中，你将单词转换为向量，然后聚合它们，并将它们输入到密集层进行分类。但是，当你使用
    RNN 层（如 LSTM）时，你不需要进行聚合，可以直接将嵌入层的输出输入到循环层。当涉及到循环层的维度时，你经常会看到的一个经验法则是它的大小与嵌入维度相同。这不是必需的，但它可以是一个好的起点。此外，请注意，虽然我在第
    6 章 [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    中提到嵌入维度通常是词汇表大小的四次方根，但在使用 RNN 时，你经常会看到这个规则可能被忽略，因为这会使循环层的大小变得太小。
- en: For this example, I have used the number of neurons in the hidden layer as a
    starting point, and you can experiment from there.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我将隐藏层中神经元的数量作为一个起点，你可以从这里进行实验。
- en: 'So, for example, you could update the simple model architecture for the sarcasm
    classifier you developed in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    to the following to use a bidirectional LSTM:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以将您在 [第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    中开发的讽刺分类器的简单模型架构更新为以下内容，以使用双向 LSTM：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can then set the loss function and classifier to this. (Note that the LR
    is 0.001, or 1e–3.):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将损失函数和分类器设置为这个。注意，学习率 LR 是 0.001，或 1e–3：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you print out the model architecture summary, you’ll see something like
    the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当您打印出模型架构摘要时，您会看到如下内容：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the vocab size is 2,000 and the embedding dimension is 7\. This gives
    14,000 parameters in the embedding layer, and the bidirectional layer will have
    48 neurons (24 out, 24 back) with a sequence length of 85 characters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，词汇量大小为 2,000，嵌入维度为 7。这给嵌入层带来了 14,000 个参数，双向层将有 48 个神经元（24 个输出，24 个回传）和 85
    个字符的序列长度
- en: '[Figure 7-9](#ch07_figure_9_1748549654878164) shows the results of training
    with this over three hundred epochs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-9](#ch07_figure_9_1748549654878164) 展示了在超过三个时期使用此方法训练的结果。'
- en: This gives us a network with only 21,537 parameters. As you can see, the accuracy
    of the network on training data rapidly climbs toward 85%, but the validation
    data plateaus at around 75%. This is similar to the figures we got earlier, but
    inspecting the loss chart in [Figure 7-10](#ch07_figure_10_1748549654878185) shows
    that while the loss for the test set diverged after 15 epochs, the validation
    loss turned to increase, indicating we have overfitting.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个只有 21,537 个参数的网络。如您所见，该网络在训练数据上的准确率迅速攀升至 85%，但验证数据在约 75% 处停滞不前。这与我们之前得到的结果相似，但检查
    [图 7-10](#ch07_figure_10_1748549654878185) 中的损失图表显示，尽管测试集在 15 个时期后损失发散，但验证损失开始增加，表明我们出现了过拟合。
- en: '![](assets/aiml_0709.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0709.png)'
- en: Figure 7-9\. Accuracy for LSTM over 30 epochs
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9\. 在 30 个时期内 LSTM 的准确率
- en: '![](assets/aiml_0710.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0710.png)'
- en: Figure 7-10\. Loss with LSTM over 30 epochs
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. 在 30 个时期内使用 LSTM 的损失
- en: However, this was just using a single LSTM layer with a hidden layer of 24 neurons.
    In the next section, you’ll see how to use stacked LSTMs and explore the impact
    on the accuracy of classifying this dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是一个使用具有 24 个神经元的隐藏层的单个 LSTM 层。在下一节中，您将看到如何使用堆叠的 LSTM 并探讨其对分类此数据集准确率的影响。
- en: Stacking LSTMs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠 LSTM
- en: In the previous section, you saw how to use an LSTM layer after the embedding
    layer to help classify the contents of the sarcasm dataset. But LSTMs can be stacked
    on top of one another, and this approach is used in many state-of-the-art NLP
    models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了如何使用 LSTM 层在嵌入层之后来帮助分类讽刺数据集的内容。但是，LSTM 可以堆叠在一起，这种方法在许多最先进的 NLP 模型中都有应用。
- en: 'Stacking LSTMs with PyTorch is pretty straightforward. You add them as extra
    layers just like you would with any other layer, but you will need to be careful
    in specifying the dimensions. So, for example, if the first LSTM has *x* number
    of hidden layers, then the next LSTM will have *x* number of inputs. If the LST
    is bidirectional, then the next will need to double the size. Here’s an example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 堆叠 LSTM 非常简单。您只需将其作为额外的层添加，就像添加任何其他层一样，但您需要小心指定维度。例如，如果第一个 LSTM 有
    *x* 个隐藏层，那么下一个 LSTM 将有 *x* 个输入。如果 LSTM 是双向的，那么下一个需要加倍大小。以下是一个示例：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the `input_size` for the first layer is the embedding dimension because
    it’s preceded by the embedding layer. The second LSTM then has its input size
    as (`hidden_dim * 2`) because the output from the first LSTM is that size, given
    that it’s bidirectional.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一层的 `input_size` 是嵌入维度，因为它位于嵌入层之前。第二个 LSTM 的输入大小为 (`hidden_dim * 2`)，因为第一个
    LSTM 的输出大小就是该尺寸，考虑到它是双向的。
- en: 'The model architecture will look like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构将看起来像这样：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Adding the extra layer will give us roughly 14,000 extra parameters that need
    to be learned, which is an increase of about 75%. So, it might slow the network
    down, but the cost is relatively low if there’s a reasonable benefit.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 添加额外的层将给我们带来大约 14,000 个额外的参数需要学习，这增加了大约 75%。因此，它可能会减慢网络的运行速度，但如果收益合理，成本相对较低。
- en: After training for three hundred epochs, the result looks like [Figure 7-11](#ch07_figure_11_1748549654878205).
    While the accuracy on the validation set is flat, examining the loss (shown in
    [Figure 7-12](#ch07_figure_12_1748549654878225)) tells a different story. As you
    can see in [Figure 7-12](#ch07_figure_12_1748549654878225), while the accuracy
    for both training and validation looked good, the validation loss quickly took
    off upward, which is a clear sign of overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了三百个周期后，结果看起来像 [图 7-11](#ch07_figure_11_1748549654878205)。虽然验证集上的准确率保持平稳，但检查损失（如图
    [7-12](#ch07_figure_12_1748549654878225) 所示）却讲述了一个不同的故事。如图 [7-12](#ch07_figure_12_1748549654878225)
    所示，虽然训练和验证的准确率看起来不错，但验证损失迅速上升，这是一个明显的过拟合信号。
- en: '![](assets/aiml_0711.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0711.png)'
- en: Figure 7-11\. Accuracy for stacked LSTM architecture
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. 堆叠 LSTM 架构的准确率
- en: This overfitting (which is indicated by the training accuracy climbing toward
    100% as the loss falls smoothly while the validation accuracy is relatively steady
    and the loss increases drastically) is a result of the model getting overspecialized
    for the training set. As with the examples in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    this shows that it’s easy to be lulled into a false sense of security if you just
    look at the accuracy metrics without examining the loss.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过拟合（表现为训练准确率随着损失平滑下降而接近 100%，而验证准确率相对稳定且损失急剧增加）是模型对训练集过度专业化的结果。正如 [第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    中的例子所示，这表明如果你只看准确率指标而不检查损失，很容易陷入虚假的安全感。
- en: '![](assets/aiml_0712.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0712.png)'
- en: Figure 7-12\. Loss for stacked LSTM architecture
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12\. 堆叠 LSTM 架构的损失
- en: Optimizing stacked LSTMs
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化堆叠 LSTM
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    you saw that a very effective method of reducing overfitting was to reduce the
    LR. It’s worth exploring here whether that will have a positive effect on an RNN,
    too.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    中，你看到了降低过拟合的一个非常有效的方法是降低学习率。在这里探索它是否对 RNN 也有积极的影响是值得的。
- en: 'For example, the following code reduces the LR by 50%, from 0.0001 to 0.00005:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码将学习率 (LR) 降低 50%，从 0.0001 降低到 0.00005：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure 7-13](#ch07_figure_13_1748549654878247) demonstrates the impact of
    this on training. As you can see, there’s a small difference in the validation
    accuracy, indicating that we’re overfitting a bit less.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-13](#ch07_figure_13_1748549654878247) 展示了这对其训练的影响。如图所示，验证准确率有细微差异，表明我们过拟合的程度有所减少。'
- en: '![](assets/aiml_0713.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0713.png)'
- en: Figure 7-13\. Impact of reduced LR on accuracy with stacked LSTMs
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. 降低学习率对堆叠 LSTM 准确率的影响
- en: While an initial look at [Figure 7-14](#ch07_figure_14_1748549654878268) similarly
    suggests a decent impact on loss due to the reduced LR, with the curve not moving
    up so sharply, it’s worth looking a little closer. We see that the loss on the
    training set is actually a little higher (~0.35 versus ~0.27) than the previous
    example, while the loss on the validation set is lower (~0.5 versus 0.6).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 [图 7-14](#ch07_figure_14_1748549654878268) 的初步观察同样表明降低学习率对损失有良好的影响，曲线没有如此急剧上升，但值得更仔细地观察。我们看到训练集上的损失实际上略高于前一个例子（约
    0.35 与约 0.27），而验证集上的损失较低（约 0.5 与 0.6）。
- en: Adjusting the LR hyperparameter certainly seems worth investigation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 调整学习率超参数显然值得研究。
- en: Indeed, further experimentation with the LR showed a marked improvement in getting
    training and validation curves to converge, indicating that while the network
    was less accurate after training, we could tell that it was generalizing better.
    Figures [7-15](#ch07_figure_15_1748549654878288) and [7-16](#ch07_figure_16_1748549654878310)
    show the impact of using a lower LR (.0003 rather than .0005).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，进一步实验表明，降低学习率 (LR) 可以显著提高训练和验证曲线的收敛性，这表明虽然网络在训练后准确性有所下降，但我们能看出它泛化得更好。图 [7-15](#ch07_figure_15_1748549654878288)
    和 [7-16](#ch07_figure_16_1748549654878310) 展示了使用较低学习率 (.0003 而不是 .0005) 的影响。
- en: '![](assets/aiml_0714.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0714.png)'
- en: Figure 7-14\. Impact of reduced LR on loss with stacked LSTMs
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. 降低学习率对堆叠 LSTM 损失的影响
- en: '![](assets/aiml_0715.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0715.png)'
- en: Figure 7-15\. Accuracy with further-reduced LR with stacked LSTM
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-15\. 堆叠 LSTM 的进一步降低学习率后的准确率
- en: '![](assets/aiml_0716.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0716.png)'
- en: Figure 7-16\. Loss with further-reduced LR and stacked LSTM
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-16\. 进一步降低学习率 (LR) 和堆叠 LSTM 的损失
- en: Indeed, reducing the LR even further, to .00001, gave potentially even better
    results, as shown in Figures [7-17](#ch07_figure_17_1748549654878331) and [7-18](#ch07_figure_18_1748549654878358).
    As with the previous diagrams, while the overall accuracy isn’t as good and the
    loss is higher, that’s an indication that we’re getting closer to a “realistic”
    result for this network architecture and not being led into having a false sense
    of security by overfitting on the training data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，将LR进一步降低到0.00001，可能给出了更好的结果，如图[7-17](#ch07_figure_17_1748549654878331)和[7-18](#ch07_figure_18_1748549654878358)所示。与之前的图表一样，虽然整体准确率不是很好，损失也更高，但这表明我们正在接近这个网络架构的“真实”结果，而不是被训练数据上的过拟合所误导，从而产生虚假的安全感。
- en: In addition to changing the LR parameter, you should also consider using dropout
    in the LSTM layers. It works exactly the same as for dense layers, as discussed
    in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    where random neurons are dropped to prevent a proximity bias from impacting the
    learning. That being said, you should be careful about setting it *too* low, because
    when you start tweaking with different architectures, you might freeze the ability
    of the network to learn.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变LR参数外，你还应该考虑在LSTM层中使用dropout。它的工作方式与密集层完全相同，如第3章[第3章：超越基础，检测图像中的特征](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)中所述，其中随机神经元被丢弃以防止邻近偏差影响学习。话虽如此，你应小心不要将其设置得太低，因为当你开始调整不同的架构时，你可能会冻结网络的学习能力。
- en: '![](assets/aiml_0717.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '**![](assets/aiml_0717.png)'
- en: Figure 7-17\. Accuracy with lower LR
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17\. 较低LR的准确率
- en: '**![](assets/aiml_0718.png)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](assets/aiml_0718.png)'
- en: Figure 7-18\. Loss with lower LR**  **### Using dropout
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-18\. 较低LR的损失
- en: In addition to changing the LR parameter, you should also consider using dropout
    in the LSTM layers. It works exactly the same as for dense layers, as discussed
    in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    where random neurons are dropped to prevent a proximity bias from impacting the
    learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变LR参数外，你还应该考虑在LSTM层中使用dropout。它的工作方式与密集层完全相同，如第3章[第3章：超越基础，检测图像中的特征](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912)中所述，其中随机神经元被丢弃以防止邻近偏差影响学习。
- en: 'You can implement dropout by using `nn.Dropout`. Here’s an example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`nn.Dropout`来实现dropout。以下是一个示例：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, in your forward pass, you can apply the dropouts at the appropriate levels,
    like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在你的前向传递中，你可以应用适当的dropout，如下所示：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When I ran this with the lowest LR I had tested prior to dropout, the network
    didn’t learn. So, I moved the LR back up to 0.0003 and ran for 300 epochs using
    this dropout (note that the dropout rate is 0.2, so about 20% of neurons are dropped
    at random). The accuracy results can be seen in [Figure 7-19](#ch07_figure_19_1748549654878378).
    The curves for training and validation are still close to each other, but they’re
    hitting greater than 75% accuracy, whereas without dropout, it was hard to get
    above 70%.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我使用在dropout之前测试过的最低LR运行时，网络没有学习。因此，我将LR调整回0.0003，并使用此dropout运行了300个epoch（注意，dropout率是0.2，因此大约有20%的神经元被随机丢弃）。准确率结果可以在[图7-19](#ch07_figure_19_1748549654878378)中看到。训练和验证曲线仍然很接近，但它们达到了大于75%的准确率，而没有dropout时，很难超过70%。
- en: '![](assets/aiml_0719.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '**![](assets/aiml_0719.png)'
- en: Figure 7-19\. Accuracy of stacked LSTMs using dropout
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-19\. 使用dropout的堆叠LSTMs的准确率
- en: As you can see, using dropout can have a positive impact on the accuracy of
    the network, which is good! There’s always a worry that losing neurons will make
    your model perform worse, but as we can see here, that’s not the case. But do
    be careful when using dropout because it can lead to underfitting or overfitting
    if not used appropriately.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用dropout可以对网络的准确率产生积极影响，这是好事！人们总是担心丢失神经元会使你的模型表现更差，但正如我们在这里所看到的，情况并非如此。但使用dropout时一定要小心，因为它如果不适当使用可能会导致欠拟合或过拟合。
- en: There’s also a positive impact on loss, as you can see in [Figure 7-20](#ch07_figure_20_1748549654878397).
    While the curves are clearly diverging, they are closer than they were previously,
    and the validation set is flattening out at a loss of about 0.45, which also demonstrates
    an improvement! As this example shows, dropout is another handy technique that
    you can use to improve the performance of LSTM-based RNNs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7-20](#ch07_figure_20_1748549654878397)所示，这也有助于降低损失。虽然曲线明显发散，但它们比之前更接近，验证集在约0.45的损失处趋于平坦，这也证明了改进！正如这个例子所示，dropout是另一种可以用来提高基于LSTM的RNN性能的实用技术。
- en: 'It’s worth exploring these techniques for avoiding overfitting in your data,
    and it’s also worth exploring the techniques for preprocessing your data that
    we covered in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    But there’s one thing that we haven’t yet tried: a form of transfer learning in
    which you can use pre-learned embeddings for words instead of trying to learn
    your own. We’ll explore that next.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值得探索这些避免数据过拟合的技术，也值得探索我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中提到的数据预处理技术。但还有一件事我们还没有尝试：一种迁移学习的形式，其中你可以使用预训练的词嵌入而不是尝试学习自己的。我们将在下一节中探讨这一点。
- en: '![](assets/aiml_0720.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0720.png)'
- en: Figure 7-20\. Loss curves for dropout-enabled LSTMs**  **# Using Pretrained
    Embeddings with RNNs
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-20.启用dropout的LSTMs的损失曲线**  **# 使用预训练嵌入的RNN
- en: In all the previous examples, you gathered the full set of words to be used
    in the training set and then trained embeddings with them. You initially aggregated
    them before feeding them into a dense network, and in this chapter, you explored
    how to improve the results using an RNN. While doing this, you were restricted
    to the words in your dataset and how their embeddings could be learned by using
    the labels from that dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的例子中，你收集了用于训练集的完整单词集，并使用它们训练嵌入。你最初将它们聚合起来，然后输入到密集网络中，在本章中，你探讨了如何使用RNN来改进结果。在这个过程中，你受到数据集中单词的限制以及如何使用该数据集的标签来学习它们的嵌入。
- en: Now, think back to [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246),
    where we discussed transfer learning. What if instead of learning the embeddings
    for yourself, you could use pre-learned embeddings, where researchers have already
    done the hard work of turning words into vectors and those vectors are proven?
    One example of this, as we saw in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    is the [GloVe (Global Vectors for Word Representation) model](https://oreil.ly/4ENdQ)
    developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)，我们讨论了迁移学习。如果你不是自己学习嵌入，而是可以使用预训练的嵌入，那么会怎样？研究人员已经完成了将单词转换为向量的艰苦工作，并且这些向量已经被证明是有效的。正如我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中看到的，一个例子是斯坦福大学的Jeffrey
    Pennington、Richard Socher和Christopher Manning开发的[GloVe (Global Vectors for Word
    Representation)模型](https://oreil.ly/4ENdQ)。
- en: 'In this case, the researchers have shared their pretrained word vectors for
    a variety of datasets:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，研究人员已经分享了他们在各种数据集上的预训练词向量：
- en: A 6-billion-token, 400,000-word vocabulary set in 50, 100, 200, and 300 dimensions
    with words taken from Wikipedia and Gigaword
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含60亿个标记、40万个单词的词汇表，在50、100、200和300个维度上，单词来自维基百科和Gigaword。
- en: A 42-billion-token, 1.9-million-word vocabulary in 300 dimensions from a common
    crawl
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自公共爬取数据的420亿个标记、1900万个单词的词汇表，在300个维度上。
- en: An 840-billion-token, 2.2-million-word vocabulary in 300 dimensions from a common
    crawl
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自公共爬取数据的840亿个标记、2200万个单词的词汇表，在300个维度上。
- en: A 27-billion-token, 1.2-million-word vocabulary in 25, 50, 100, and 200 dimensions
    from a Twitter crawl of 2 billion tweets
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自2亿条推文的Twitter爬取数据中，一个包含27亿个标记、1200万个单词的词汇表，在25、50、100和200个维度上。
- en: 'Given that the vectors are already pretrained, it’s simple for you to reuse
    them in your PyTorch code, instead of learning them from scratch. First, you’ll
    have to download the GloVe data. I’ve opted to use the 6-billion-word version,
    in 50 dimensions, using this code to download and unzip it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些向量已经预训练，你可以在PyTorch代码中简单地重用它们，而不是从头开始学习。首先，你将不得不下载GloVe数据。我选择使用50维的60亿单词版本，使用以下代码下载并解压它：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each entry in the file is a word, followed by the dimensional coefficients
    that were learned for it. The easiest way to use this is to create a dictionary
    where the key is the word and the values are the embeddings. You can set up this
    dictionary like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的每个条目都是一个单词，后面跟着为其学习到的维度系数。使用这种方法最简单的方式是创建一个字典，其中键是单词，值是嵌入。你可以这样设置这个字典：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point, you’ll be able to look up the set of coefficients for any word
    simply by using it as the key. So, for example, to see the embeddings for the
    word *frog*, you could use this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你将能够通过使用它作为键来查找任何单词的系数集。例如，要查看单词*frog*的嵌入，你可以使用这个：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With these pretrained embeddings in hand, you can now load them into the embeddings
    layer in your neural architecture and use them as pretrained embeddings instead
    of learning them from scratch. See the following model architecture definition.
    If the `pretrained_embeddings` value is not null, then the weights for the embedding
    layer will be loaded from that. If `freeze_embeddings` is `True`, then they’ll
    be frozen; otherwise, they’ll be used as the starting point for learning (i.e.,
    you’ll fine-tune the embeddings based on your corpus):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些预训练嵌入后，你现在可以将它们加载到你的神经网络架构中的嵌入层，并使用它们作为预训练嵌入而不是从头开始学习。请参阅以下模型架构定义。如果`pretrained_embeddings`的值不为空，则嵌入层的权重将从该值加载。如果`freeze_embeddings`为`True`，则它们将被冻结；否则，它们将用作学习的起点（即，你将根据你的语料库微调嵌入）：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This model shows a total of 406.817 parameters of which only 6,817 are trainable,
    so training will be fast!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型显示了总共406.817个参数，其中只有6,817个是可训练的，所以训练将会很快！
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can now train as before, and you can see how this architecture, with the
    pretrained embeddings and stacked LSTMs, reduces overfitting really nicely! [Figure 7-21](#ch07_figure_21_1748549654878419)
    shows the Training versus Validation accuracy on the sarcasm dataset using LSTMs
    and pretrained GloVe embeddings, while [Figure 7-22](#ch07_figure_22_1748549654878440)
    shows the loss on training versus validation, where the closeness of the curves
    demonstrates that we’re not overfitting.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以像以前一样进行训练，并可以看到这个架构，即使用预训练嵌入和堆叠的LSTMs，如何真正地减少过拟合！[图7-21](#ch07_figure_21_1748549654878419)显示了使用LSTMs和预训练GloVe嵌入在讽刺语料库上的训练与验证准确率，而[图7-22](#ch07_figure_22_1748549654878440)显示了训练与验证的损失，曲线的接近程度表明我们没有过拟合。
- en: '![](assets/aiml_0721.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0721.png)'
- en: Figure 7-21\. Training versus validation accuracy on the sarcasm dataset with
    LSTMs and GloVe
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-21。使用LSTMs和GloVe在讽刺语料库上的训练与验证准确率
- en: '![](assets/aiml_0722.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0722.png)'
- en: Figure 7-22\. Training and validation loss on the sarcasm dataset with LSTMs
    and GloVe
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-22。使用LSTMs和GloVe在讽刺语料库上的训练与验证损失
- en: 'For further analysis, you’ll want to consider your vocab size. One of the optimizations
    you did in the previous chapter to avoid overfitting was intended to prevent the
    embeddings becoming overburdened with learning low-frequency words: you avoided
    overfitting by using a smaller vocabulary of frequently used words. In this case,
    as the word embeddings have already been learned for you with GloVe, you could
    expand the vocabulary—but by how much?'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析，你需要考虑你的词汇量大小。在前一章中，你为了避免过拟合所做的优化之一是为了防止嵌入层被低频词的学习所负担：你通过使用常用词的较小词汇量来避免过拟合。在这种情况下，由于GloVe已经为你学习了单词嵌入，你可以扩展词汇量——但扩展多少呢？
- en: The first thing to explore is how many of the words in your corpus are actually
    in the GloVe set. It has 1.2 million words, but there’s no guarantee it has *all*
    of your words.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要探索的是你的语料库中有多少单词实际上在GloVe集中。它有120万个单词，但无法保证它有*所有*你的单词。
- en: 'When building the `word_index`, you can call `build_vocab_glove` with a *really*
    large number and it will ignore any words over the total amount. So, for example,
    say you call this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建`word_index`时，你可以使用一个非常大的数字调用`build_vocab_glove`，并且它将忽略超过总量的任何单词。例如，假设你调用这个：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the sarcasm dataset, you’ll get a vocab_size of 22,457 returned. If you
    like, you can then explore the GloVe embeddings to see just how many of these
    words are present in GloVE. Start by creating a dictionary for the embeddings
    and reading the GloVE file into it:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用讽刺语料库，你将获得一个词汇量大小为22,457的结果。如果你愿意，可以探索GloVe嵌入以查看其中有多少单词存在于GloVE中。首先，为嵌入创建一个字典并将GloVE文件读取到其中：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, you can compare this with your `word_index` that you created from the
    entire corpus with the preceding line:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将这个与你从整个语料库中创建的 `word_index` 进行比较：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the case of sarcasm, 21,291 of the words were found in GloVE, which is the
    vast majority, so the principles you used in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    to choose how many you should train on (i.e., picking those with sufficient frequency
    to have a signal) will still apply!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在讽刺的情况下，有 21,291 个单词被发现在 GloVE 中，这几乎是全部，所以你在[第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中使用的原则来选择你应该训练多少（即，选择那些频率足够高以产生信号的单词）仍然适用！
- en: 'Using this method, I chose to use a vocabulary size of 8,000 (instead of the
    2,000 that was previously used to avoid overfitting) to get the results you saw
    just now. I then tested it with headlines from *The Onion*, the source of the
    sarcastic headlines in the sarcasm dataset, against other sentences, as shown
    here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我选择使用 8,000 个词汇量（而不是之前为了避免过拟合而使用的 2,000 个），以获得你刚才看到的成果。然后我用 *The Onion*
    的标题（讽刺数据集中讽刺标题的来源）和其他句子进行了测试，如下所示：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The results for these headlines are as follows—remember that values close to
    50% (0.5) are considered neutral, those close to 0 are considered nonsarcastic,
    and those close to 1 are considered sarcastic:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标题的结果如下——记住，接近 50%（0.5）的值被认为是中性的，接近 0 的被认为是非讽刺的，而接近 1 的被认为是讽刺的：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first and fourth sentences, which are taken from *The Onion*, showed 93%+
    likelihood of sarcasm. The statement about the weather was strongly nonsarcastic
    (16%), and the sentence about going to high school in Ireland was deemed to be
    potentially sarcastic but not with high confidence (69%).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前四句，其中一句来自 *The Onion*，显示有 93% 以上的可能性是讽刺。关于天气的陈述被强烈地认为是非讽刺的（16%），而关于在爱尔兰上高中的句子被认为可能是讽刺的，但信心不高（69%）。
- en: Summary
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced you to recurrent neural networks, which use sequence-oriented
    logic in their design and can help you understand the sentiment in sentences based
    not only on the words they contain but also on the order in which they appear.
    You saw how a basic RNN works, as well as how an LSTM can build on this to enable
    context to be preserved over the long term. These models are the precursors to
    the popular and famous “transformers” models used to underpin generative AI.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向你介绍了循环神经网络，它们在设计上使用面向序列的逻辑，可以帮助你根据句子中包含的单词以及它们的顺序来理解句子的情感。你看到了一个基本的 RNN 是如何工作的，以及
    LSTM 如何在此基础上构建以使长期保持上下文。这些模型是流行且著名的“transformers”模型的先驱，这些模型被用来支撑生成式 AI。
- en: You also used LSTMs to improve the sentiment analysis model you’ve been working
    on, and you then looked into overfitting issues with RNNs and techniques to improve
    them, including by using transfer learning from pretrained embeddings.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你还使用了 LSTM 来改进你一直在工作的情感分析模型，然后你研究了 RNN 的过拟合问题以及改进它们的技术，包括使用预训练嵌入的迁移学习。
- en: In [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453), you’ll
    use what you’ve learned so far to explore how to predict words, and from there,
    you’ll be able to create a model that creates text and writes poetry for you!**
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 8 章](ch08.html#ch08_using_ml_to_create_text_1748549671852453)中，你将使用你迄今为止所学的内容来探索如何预测单词，从那里，你将能够创建一个为你创建文本和写诗的模型！**
