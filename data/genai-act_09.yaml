- en: 8 Chatting with your data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 与您的数据聊天
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How bringing your data benefits enterprises
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的数据引入企业的好处
- en: Installing and using a vector database and vector index
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用向量数据库和向量索引
- en: Planning and retrieving your proprietary data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划和检索您的专有数据
- en: Using a vector database to conduct searches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量数据库进行搜索
- en: How to implement an end-to-end chat powered by RAG using a vector database and
    an LLM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用向量数据库和LLM实现端到端聊天
- en: The benefits of bringing your data and RAG jointly
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的数据和RAG联合起来的好处
- en: How RAG benefits AI safety for enterprises
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG如何为企业的人工智能安全性带来益处
- en: Utilizing large language models (LLMs) for a chat-with-data implementation is
    a promising strategy uniquely suitable for enterprises seeking to harness the
    power of generative artificial intelligence (AI) for their specific business requirements.
    By synergizing the LLM capabilities with enterprise-specific data sources and
    tools, businesses can forge intelligent and context-aware chatbots that deliver
    invaluable insights and recommendations to their clientele and stakeholders.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大型语言模型（LLMs）进行数据交互实现是一种有希望的策略，特别适合那些寻求利用生成人工智能（AI）力量满足其特定商业需求的企业。通过将LLM的能力与企业特定的数据源和工具相结合，企业可以打造智能且具有情境意识的聊天机器人，为他们的客户和利益相关者提供宝贵的见解和建议。
- en: At a high level, there are two ways to chat with your data using an LLM—one
    is by employing a retrieval engine as implemented using the retrieval-augmented
    generation (RAG) pattern, and another is to custom-train the LLM on your data.
    The latter is more involved and complex and not available to most users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，使用LLM与数据聊天有两种方式——一种是通过采用检索增强生成（RAG）模式实现的检索引擎，另一种是为您的数据定制训练LLM。后者更为复杂且大多数用户无法使用。
- en: This chapter builds on the RAG pattern from the last chapter used to enhance
    LLMs with your data, especially when enterprises want to implement it at the scale
    for production workloads. When enterprises integrate their data using a RAG pattern
    with LLMs, they unlock many advantages, enhancing the functionality and applicability
    of these AI systems in their unique business contexts. The chapter outlines how
    these are different and, in many cases, better than larger context windows. Let’s
    start by identifying the advantages enterprises can get when wanting to bring
    in their data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章基于上一章中使用的RAG模式，用于增强LLMs与您的数据，特别是当企业希望在生产工作负载的规模上实施时。当企业使用RAG模式与LLMs整合其数据时，他们可以解锁许多优势，增强这些AI系统在其独特商业环境中的功能性和适用性。本章概述了这些差异，以及在许多情况下，它们比更大的上下文窗口更好。让我们首先确定企业在希望引入其数据时可以获得的优势。
- en: 8.1 Advantages to enterprises using their data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 企业使用其数据的优势
- en: In the dynamic realm of business technology, integrating LLMs into enterprise
    data systems heralds a transformative era of interactive and intuitive processes.
    As we explored earlier, these cutting-edge AI-driven tools are reshaping how businesses
    engage with their data, thus opening up unprecedented avenues of efficiency and
    accessibility.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业技术动态领域，将LLMs集成到企业数据系统中预示着一个互动和直观过程的变革时代。正如我们之前所探讨的，这些尖端的人工智能驱动工具正在重塑企业与其数据互动的方式，从而开辟了前所未有的效率和可访问性途径。
- en: LLMs have achieved impressive results in various natural language processing
    (NLP) tasks, such as answering questions, summarization, translation, and dialogue.
    However, LLMs have limitations and challenges, such as data quality, ethical problems,
    and scalability. Therefore, many enterprises are interested in implementing a
    chat with their data implementation using LLMs, which offer several advantages
    for their business goals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了令人印象深刻的成果，例如问答、摘要、翻译和对话。然而，LLMs存在局限性和挑战，如数据质量、伦理问题和可扩展性。因此，许多企业对使用LLMs实现与数据交互的聊天功能感兴趣，这对他们的商业目标具有多项优势。
- en: One of the main advantages of using LLMs for this purpose is that they can provide
    intelligent and context-aware chatbots that can handle customer queries and concerns
    with human-like proficiency. LLMs can understand the meaning and intent of the
    user’s input, generate relevant and coherent responses, and even take action by
    invoking APIs as needed. This improves customer satisfaction and frees human agents
    to focus on more complex tasks. Another advantage of using LLMs for chat with
    data implementation is that they can be customized with enterprise-specific data,
    which leads to more accurate and relevant AI-generated insights and recommendations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM为此目的的主要优势之一是它们可以提供智能和上下文感知的聊天机器人，能够以类似人类的熟练程度处理客户查询和关注。LLM可以理解用户的输入的意义和意图，生成相关且连贯的响应，并在需要时通过调用API采取行动。这提高了客户满意度，并使人工代理能够专注于更复杂的工作。使用LLM进行数据聊天实现的另一个优势是它们可以用企业特定的数据进行定制，这导致更准确和相关的AI生成见解和建议。
- en: Finally, using LLMs for chat with data implementation can enable more efficient
    and effective data analysis. LLMs can generate natural language summaries or explanations
    of the data analysis results, which can help users understand the key findings
    and implications. In addition, LLMs can generate interactive charts or graphs
    highlighting the patterns or trends in the data. These features can enhance the
    user experience and facilitate data-driven decision-making across the organization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用LLM进行数据聊天实现可以使得数据分析更加高效和有效。LLM可以生成数据分析结果的自然语言摘要或解释，这有助于用户理解关键发现和影响。此外，LLM可以生成交互式图表或图形，突出数据中的模式或趋势。这些功能可以增强用户体验，并促进组织内部的数据驱动决策。
- en: 8.1.1 What about large context windows?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 关于大上下文窗口怎么办？
- en: 'The most recent models from OpenAI—for example, the GPT-4 Turbo with a 128K
    context window and Google’s newest Gemini Pro 1.5 with 1.5 million token content
    windows—have generated much enthusiasm and interest. However, a bigger context
    window alone is not enough. Training an LLM on your data has the following benefits
    over just using an LLM with a larger context window:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI最新的模型——例如，具有128K上下文窗口的GPT-4 Turbo和谷歌最新的Gemini Pro 1.5，具有150万个标记的内容窗口——已经引起了极大的热情和兴趣。然而，仅仅更大的上下文窗口是不够的。在您的数据上训练LLM，与仅使用具有更大上下文窗口的LLM相比，有以下好处：
- en: '*More accurate and informative answers*—When chatting with your data, the LLM
    can access much more information than it would with a larger context window alone.
    This allows the LLM to provide more accurate and informative answers to your questions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更准确和有信息量的答案*——当与你的数据聊天时，LLM可以访问比仅使用更大的上下文窗口多得多的信息。这使得LLM能够为你提供更准确和有信息量的答案。'
- en: '*More personalized answers*—The LLM can also learn to personalize its answers
    based on your data. For example, if you chat with an LLM that has been fine-tuned
    on your customer data, it can learn to provide more relevant answers to your specific
    customers and their needs. For example, we can use a retrieval engine to index
    its customer data and then connect the retrieval engine to an LLM. This would
    allow the company to chat with its customers in a more personalized and informative
    way.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更个性化的答案*——LLM还可以根据你的数据学习个性化其答案。例如，如果你与一个针对你的客户数据进行微调的LLM聊天，它可以学习提供更相关的答案来满足你的特定客户和他们的需求。例如，我们可以使用检索引擎来索引其客户数据，然后将检索引擎连接到LLM。这将使公司能够以更个性化和信息化的方式与客户聊天。'
- en: '*More creative answers*—The LLM can also use your data to generate more creative
    and interesting answers to your questions. For example, if you chat with an LLM
    that has fine-tuned your product data, the LLM can learn to generate new product
    ideas or marketing campaigns.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更具创造性的答案*——LLM还可以使用你的数据来生成更具创造性和有趣的问题答案。例如，如果你与一个针对你的产品数据进行微调的LLM聊天，LLM可以学习生成新的产品想法或营销活动。'
- en: Of course, LLMs with a larger context window have their own benefits, but they
    can be a double-edged sword with some limitations. Larger context windows allow
    us to pass in more information in one API call and worry less about chunking up
    the application. For example, the recently announced GPT-4.5 Turbo has a 128K
    context window, allowing for approximately 300 pages of text in a single prompt,
    compared to approximately 75 pages from the earlier GPT-4 32K model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，具有更大上下文窗口的LLM有其自身的好处，但它们也可能是一把双刃剑，存在一些限制。更大的上下文窗口允许我们在一次API调用中传递更多信息，并且不必过多担心应用程序的块化。例如，最近宣布的GPT-4.5
    Turbo具有128K的上下文窗口，允许在一个提示中包含大约300页的文本，相比之下，早期的GPT-4 32K模型大约只能包含75页。
- en: On the flip side, having a larger context window has its challenges. For example,
    larger context window LLMs can be more computationally expensive to train and
    deploy. They can also be more prone to generating hallucinations or incorrect
    answers, as large context windows increase the complexity and uncertainty of the
    model’s output. LLMs are trained on large, diverse datasets that may contain incomplete,
    contradictory, or noisy information. When the model is given a long context window,
    it must process more information and decide what to generate next, which can lead
    to errors, inconsistencies, or fabrications in the output, especially if the model
    relies on heuristics or memorization rather than reasoning or understanding.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，拥有更大的上下文窗口也带来挑战。例如，更大的上下文窗口的LLM在训练和部署时可能更昂贵。它们也可能更容易产生幻觉或错误的答案，因为大的上下文窗口增加了模型输出的复杂性和不确定性。LLM是在包含不完整、矛盾或噪声信息的大型、多样化数据集上训练的。当模型被赋予一个长的上下文窗口时，它必须处理更多信息并决定接下来要生成什么，这可能导致输出中的错误、不一致或虚构，尤其是如果模型依赖于启发式或记忆而不是推理或理解。
- en: In contrast, chatting with your data can be more efficient and less prone to
    errors, mainly because when chatting with our data, we are grounding on that data
    and steering the model to use. The LLM can access a wider range of information
    and learn to personalize its answers based on your data. Ultimately, the best
    way to choose between a larger context window LLM and chatting with your data
    will depend on your specific needs and resources.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，与您的数据聊天可能更有效率且更不容易出错，主要是因为当我们与我们的数据聊天时，我们是在基于那些数据，并引导模型去使用。LLM可以访问更广泛的信息，并学会根据您的数据个性化其答案。最终，选择更大的上下文窗口LLM与与您的数据聊天之间的最佳方式将取决于您的具体需求和资源。
- en: 8.1.2 Building a chat application using our data
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 使用我们的数据构建聊天应用程序
- en: We will build on the RAG implementation from the last chapter and build a chat
    application that we can use to chat our data. As we saw before, vector databases
    are key for enterprises, enabling them to manage, secure, and scale embeddings
    in a production environment. For many enterprises, vector databases for semantic
    search use cases solve the performance and security requirements needed for production
    systems. Figure 8.1 shows the approach at a high level for incorporating LLM on
    our data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于上一章的RAG实现构建一个聊天应用程序，我们可以用它来与我们的数据聊天。正如我们之前看到的，向量数据库对企业至关重要，它使企业能够在一个生产环境中管理、安全和扩展嵌入。对于许多企业来说，用于语义搜索的向量数据库解决了生产系统所需的性能和安全要求。图8.1展示了在数据上集成LLM的高层次方法。
- en: '![figure](../Images/CH08_F01_Bahree.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F01_Bahree.png)'
- en: Figure 8.1 Azure OpenAI on your data
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 Azure OpenAI在您的数据上
- en: 'For example, we will use my blog ([https://blog.desigeek.com](https://blog.desigeek.com))
    as the proprietary data source. It has posts going back 20 years across various
    topics and technologies. If for every question a user asks we go back to the blog,
    load up all the posts, create embeddings, search through those, and then use RAG
    to answer the question, the process will be very time-consuming and not scalable.
    In addition, there will be added costs, as we will be using many more tokens on
    each conversation turn or for the new set of conversations. A better approach
    would be to set the following four stages we will go through:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将使用我的博客([https://blog.desigeek.com](https://blog.desigeek.com))作为专有数据源。它包含20年以上的各种主题和技术帖子。如果我们对用户提出的每个问题都回到博客，加载所有帖子，创建嵌入，搜索这些帖子，然后使用RAG来回答问题，这个过程将非常耗时且不可扩展。此外，还将产生额外的成本，因为我们将使用更多的标记在每个对话回合或新的一组对话中。更好的方法是将以下四个阶段作为我们将要经历的：
- en: Reading and injecting the information (i.e., retrieval)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读和注入信息（即检索）
- en: Creating the embeddings and saving the details to Redis
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建嵌入并将细节保存到 Redis
- en: Searches against the saved details for a Q&A implementation using the blog posts
    (i.e., augmenting)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用博客文章的保存细节进行问答实现搜索（即增强）
- en: Plugging this into the LLM generation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此与 LLM 生成相结合
- en: Let’s start by setting up a vector database.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置一个向量数据库。
- en: 8.2 Using a vector database
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 使用向量数据库
- en: As we saw earlier, a vector database has been designed to operate on embedding
    vectors. For most enterprise use cases, they are a great addition to RAG implementations
    and allow us to use our data. Many vector databases are available today, and with
    the increasing popularity of LLMs and generative AI, there is more support for
    semantic search each day. Let’s see how we can implement this.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，向量数据库已被设计用于在嵌入向量上运行。对于大多数企业用例，它们是 RAG 实现的绝佳补充，并允许我们使用我们的数据。如今，许多向量数据库可供使用，随着
    LLM 和生成式 AI 的日益流行，每天对语义搜索的支持也在增加。让我们看看我们如何实现这一点。
- en: In our learning context, we want something quick and easy to set up and run,
    mainly to understand the different concepts and steps required to deploy a vector
    database for embeddings and how to integrate it into our RAG implementation. For
    this purpose, we will use Redis as a vector database and run it locally in a Docker
    container.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的学习环境中，我们希望有一个快速且易于设置和运行的工具，主要是为了理解部署向量数据库所需的不同概念和步骤，以及如何将其集成到我们的 RAG 实现中。为此，我们将使用
    Redis 作为向量数据库，并在本地 Docker 容器中运行它。
- en: Redis is an open source, in-memory, key–value data store that can be used as
    a database, cache, message broker, and more. It supports data structures such
    as strings, lists, sets, hashes, and streams. Redis is fast, scalable, and reliable,
    which makes it popular for many use cases that require low latency and high throughput.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 是一个开源的内存键值数据存储，可以用作数据库、缓存、消息代理等。它支持字符串、列表、集合、哈希和流等数据结构。Redis 快速、可扩展且可靠，这使得它在需要低延迟和高吞吐量的许多用例中非常受欢迎。
- en: Redis expands its core capabilities using the concept of modules. Redis Search
    is a module that extends Redis with powerful text search and secondary indexing
    capabilities. It lets you create indexes on your Redis data and query them using
    a rich query language. You can also use Redis Search for vector similarity search,
    which enables semantic search based on embeddings.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 通过模块的概念扩展其核心功能。Redis Search 是一个模块，它通过强大的文本搜索和二级索引功能扩展了 Redis。它允许你在 Redis
    数据上创建索引，并使用丰富的查询语言进行查询。你还可以使用 Redis Search 进行向量相似度搜索，这使基于嵌入的语义搜索成为可能。
- en: There are several ways to deploy Redis. For local development, the quickest
    method is to use the Redis Stack Docker container, which we will use. Redis Stack
    contains several Redis modules that, for our purpose, can be used together to
    create a fast, multimodel data store and query engine. More details on the Redis
    Stack Docker container are available at [https://hub.docker.com/r/redis/redis-stack](https://hub.docker.com/r/redis/redis-stack).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Redis 有几种方法。对于本地开发，最快的方法是使用 Redis Stack Docker 容器，我们将使用这种方法。Redis Stack 包含几个
    Redis 模块，对于我们来说，可以一起使用来创建一个快速的多模型数据存储和查询引擎。有关 Redis Stack Docker 容器的更多详细信息，请参阅
    [https://hub.docker.com/r/redis/redis-stack](https://hub.docker.com/r/redis/redis-stack)。
- en: NOTE  The main prerequisite here is that Docker should already be installed
    and configured for you to use. The details of Docker installations are outside
    the book’s scope, as there are books dedicated to Docker and its management. If
    you don’t have Docker installed, please see the documentation for installing Docker
    Desktop for a more manageable experience or, at a minimum, the Docker engine.
    More details can be found at [https://docs.docker.com/desktop/](https://docs.docker.com/desktop/).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这里的主要先决条件是 Docker 应已安装并配置好，以便你可以使用。Docker 安装的详细信息超出了本书的范围，因为有一些书籍专门介绍 Docker
    及其管理。如果你没有安装 Docker，请参阅 Docker Desktop 安装文档以获得更易于管理的体验，或者至少是 Docker 引擎。更多详细信息请参阅
    [https://docs.docker.com/desktop/](https://docs.docker.com/desktop/)。
- en: 'In addition to the OpenAI packages, the following prerequisites are needed
    for us to get Redis running:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 OpenAI 包之外，我们还需要以下先决条件来运行 Redis：
- en: Docker must be installed and running locally.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 必须已安装并本地运行。
- en: When using conda, the `redis-py` package can be installed with `conda` `install
    -c` `conda-forge` `redis-py`. If we are using pip, then use `pip` `install` `redis`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用 conda 时，可以使用 `conda install -c conda-forge redis-py` 安装 `redis-py` 包。如果我们使用
    pip，则使用 `pip install redis`。
- en: We will use a docker-compose file for Docker, as shown in listing 8.1\. We have
    not changed the default ports, but you can configure them as you see fit for your
    environment. In this example, we pull the latest `redis-stack` image from the
    Docker registry and expose two ports—6379 and 8001\. We also set up a data volume
    to persist the information populated in the database. And finally, we set up some
    initial health checks to check basic things, such as that the service is up and
    running and reachable at the configured ports. If you change the ports, ensure
    this is updated in the test as part of the health check.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 docker-compose 文件来配置 Docker，如列表 8.1 所示。我们没有更改默认端口，但您可以根据您的环境需要配置它们。在这个例子中，我们从
    Docker 仓库中拉取最新的 `redis-stack` 镜像，并暴露两个端口——6379 和 8001。我们还设置了一个数据卷来持久化数据库中填充的信息。最后，我们设置了一些初始的健康检查来检查基本事项，例如服务是否正在运行，并且可以通过配置的端口访问。如果您更改了端口，请确保在健康检查中更新测试。
- en: Listing 8.1 docker-compose file for `redis-stack`
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1 `redis-stack` 的 docker-compose 文件
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For Docker to run, as per convention, we must ensure this file is saved as
    a `docker-compose.yml` file. You can start this by entering the following commands
    from the same location where the file is saved: `docker compose up -d`. In our
    example, the container runs via the Docker Desktop GUI, as shown in figure 8.2.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Docker 的惯例，我们必须确保此文件以 `docker-compose.yml` 文件保存。您可以从保存文件的同一路径输入以下命令来启动它：`docker
    compose up -d`。在我们的例子中，容器通过 Docker Desktop GUI 运行，如图 8.2 所示。
- en: '![figure](../Images/CH08_F02_Bahree.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F02_Bahree.png)'
- en: Figure 8.2 Docker Desktop running Redis container
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2 Docker Desktop 运行 Redis 容器
- en: This also includes Redis Insight, a GUI for managing our Redis database. Once
    the Docker container runs, we can access it locally at `http://localhost:8001`.
    If everything is set up correctly, we can see the database and installed modules
    (figure 8.3).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这也包括 Redis Insight，它是管理我们的 Redis 数据库的图形用户界面。一旦 Docker 容器运行，我们可以在本地通过 `http://localhost:8001`
    访问它。如果一切设置正确，我们可以看到数据库和安装的模块（图 8.3）。
- en: '![figure](../Images/CH08_F03_Bahree.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F03_Bahree.png)'
- en: Figure 8.3 Redis database with search running locally in a container
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3 在容器中本地运行的带有搜索功能的 Redis 数据库
- en: Now that we have our vector database up and running, let us work through the
    next step of retrieving the information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动并运行了矢量数据库，让我们继续下一步，检索信息。
- en: Note  We use Redis as an example in this chapter, as it is relatively easy to
    run it locally in a container for enterprises to try out the concepts and get
    a handle on the associated complexities. Given that it runs locally in a container,
    it also helps alleviate any initial matters about data going into the cloud, which
    might be a concern, at least in the early days of development. In addition to
    Redis, a few other vector databases are becoming increasingly popular. Some of
    the more popular vector databases are Azure AI Search, Pinecone, and Milvus.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，我们以 Redis 为例，因为对于企业来说，在本地容器中运行它相对容易，可以尝试相关概念并掌握其复杂性。鉴于它是在本地容器中运行的，这也帮助缓解了关于数据进入云端的初始问题，至少在开发的早期阶段可能会引起关注。除了
    Redis 之外，还有一些其他矢量数据库变得越来越受欢迎。一些更受欢迎的矢量数据库包括 Azure AI Search、Pinecone 和 Milvus。
- en: Azure AI Search
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Azure AI Search
- en: Although we are using Redis running locally, enterprises that need to scale
    to a larger corpus of data, indexes, and product-scale workloads and availability
    start getting much more complex. For such scenarios, Azure AI Search is a good
    choice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用的是本地运行的 Redis，但对于需要扩展到更大数据集、索引和产品级工作负载以及可用性的企业来说，情况开始变得更加复杂。在这种情况下，Azure
    AI Search 是一个好的选择。
- en: Azure AI Search is a cloud-based service that provides various features for
    building search applications. In addition to offering a vector search, which allows
    you to find and retrieve data objects that are semantically similar to a given
    query based on their vector embeddings, it also supports hybrid search. Hybrid
    search combines full-text and vector queries that execute against a search index
    containing searchable plain text content and generated embeddings. In a single-search
    request, hybrid queries can use existing functionality, such as filtering, faceting,
    sorting, scoring profiles, and semantic ranking. The query response provides just
    one result set, using reciprocal rank fusion (RRF) to determine which matches
    are included.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AI Search 是一种基于云的服务，为构建搜索应用程序提供各种功能。除了提供向量搜索，允许您根据其向量嵌入找到并检索与给定查询在语义上相似的数据对象之外，它还支持混合搜索。混合搜索结合了全文和向量查询，这些查询针对包含可搜索纯文本内容和生成嵌入的搜索索引执行。在单个搜索请求中，混合查询可以使用现有功能，例如过滤、分面、排序、评分配置文件和语义排名。查询响应仅提供一个结果集，使用互逆排名融合（RRF）来确定哪些匹配项被包含。
- en: Azure AI Search offers several benefits over Redis for vector searches with
    LLMs. It is a fully managed search service that can index and search structured,
    semi-structured, and unstructured data. Azure AI Search is highly scalable and
    can easily handle large amounts of data. It supports more robust security features
    that enterprises require, such as rest and transit encryption, role-based access
    control (RBAC), and more. You can find more details at [https://learn.microsoft.com/en-us/azure/search/](https://learn.microsoft.com/en-us/azure/search/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AI Search 在使用LLM进行向量搜索方面比Redis提供更多优势。它是一个完全管理的搜索服务，可以索引和搜索结构化、半结构化和非结构化数据。Azure
    AI Search 具有高度的可扩展性，可以轻松处理大量数据。它支持企业所需的各种强大安全功能，例如端到端加密、基于角色的访问控制（RBAC）等。更多详细信息请参阅[https://learn.microsoft.com/en-us/azure/search/](https://learn.microsoft.com/en-us/azure/search/)。
- en: 8.3 Planning for retrieving the information
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 信息检索规划
- en: First, we must understand what we are trying to retrieve and index. This helps
    us formulate the approach and determine which pieces of information are essential
    and which are redundant and can be ignored. As part of this exercise, we also
    need to factor in the technical aspects, such as how we connect to the source
    system and any technical or practical limitations. We must also understand the
    data format and engineering requirements (including data cleaning and conversions).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须了解我们试图检索和索引的内容。这有助于我们制定方法并确定哪些信息是必要的，哪些是冗余的并且可以忽略。作为这项练习的一部分，我们还需要考虑技术方面，例如我们如何连接到源系统以及任何技术或实际限制。我们还必须了解数据格式和工程要求（包括数据清理和转换）。
- en: Before we get the data from the blog, take a look at the details outlined in
    table 8.1.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从博客获取数据之前，请查看表8.1中概述的详细信息。
- en: Table 8.1 Data items for blog posts we are interested in
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1 我们感兴趣的博客文章数据项
- en: '| Data | Description |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 描述 |'
- en: '| --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| URL  | The URL to the individual blog post  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| URL | 单个博客文章的URL |'
- en: '| Title  | Title of the blog post  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 博客文章的标题 |'
- en: '| Description  | A couple of sentences describing what that specific blog post
    is about  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 几句话描述该特定博客文章的内容 |'
- en: '| Publish date  | Date when the post was published  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 发布日期 | 文章发布的日期 |'
- en: '| Content  | The actual content of the blog post  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 内容 | 博客文章的实际内容 |'
- en: Although we are using a blog post as a source system, it is a holistic example
    representing most of the RAG aspects and helping us to understand the best practices
    and how to approach them. We are retrieving the information from a remote system
    to read the blog posts. This is fundamentally similar to enterprises reading information
    for various line-of-business systems. Depending on the source system, they read
    this via APIs, exported files, or connecting to various databases and data sources.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用博客文章作为源系统，但它是一个整体示例，代表了大多数RAG方面，并帮助我们了解最佳实践以及如何处理它们。我们从远程系统中检索信息以阅读博客文章。这与企业读取各种业务系统信息的基本原理相似。根据源系统，他们通过API、导出文件或连接到各种数据库和数据源来读取这些信息。
- en: In our example, we will read all the posts using the blog’s RSS feed. RSS stands
    for really simple syndication, a standard website content distribution method
    often used to publish changes. The blog can be found at [https://blog.desigeek.com/](https://blog.desigeek.com/),
    and the corresponding RSS feed is available at [https://blog.desigeek.com/index.xml](https://blog.desigeek.com/index.xml).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用博客的RSS源读取所有帖子。RSS代表“真正简单的聚合”，这是一种标准网站内容分发方法，常用于发布更改。博客可以在[https://blog.desigeek.com/](https://blog.desigeek.com/)找到，相应的RSS源在[https://blog.desigeek.com/index.xml](https://blog.desigeek.com/index.xml)。
- en: First, we assume Redis runs locally in a container, as shown earlier. We will
    connect to Redis and create a new index called `posts`. The schema for the index
    is shown in the next listing and represents the structure of our data that we
    saw earlier. In addition to the main content of the blog post, we also capture
    associated metadata that will help us answer questions or understand the context
    better.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设Redis像之前展示的那样在本地容器中运行。我们将连接到Redis并创建一个新的索引，称为`posts`。索引的模式在下一列表中显示，并代表我们之前看到的数据结构。除了博客文章的主要内容外，我们还捕获了相关的元数据，这将帮助我们回答问题或更好地理解上下文。
- en: Listing 8.2 Redis index schema
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.2 Redis索引模式
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This schema contains the following types of fields:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式包含以下类型的字段：
- en: '`TagField`—Used to store tags, which are short, descriptive keywords that can
    be employed to categorize and organize data. Tags are typically stored as a list
    of strings, and Redis search supports searching for tags with Boolean operators
    such as `AND`, `OR`, and `NOT`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TagField`—用于存储标签，这些是简短、描述性的关键词，可以用来分类和组织数据。标签通常存储为字符串列表，Redis搜索支持使用布尔运算符（如`AND`、`OR`和`NOT`）搜索标签。'
- en: '`TextField`—Used to store text data, such as the title, description, and content
    of a blog post. Redis search supports full-text search on `TextField`s, meaning
    you can search for words and phrases in the text.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextField`—用于存储文本数据，例如博客文章的标题、描述和内容。Redis搜索支持对`TextField`进行全文搜索，这意味着您可以在文本中搜索单词和短语。'
- en: '`VectorField`—Stores vectors’ mathematical representations of data that can
    be used to perform machine learning tasks, such as image classification and natural
    language processing. Redis search supports vector similarity search, meaning you
    can search for vectors similar to a given vector.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorField`—存储用于执行机器学习任务（如图像分类和自然语言处理）的数据的向量的数学表示。Redis搜索支持向量相似性搜索，这意味着您可以搜索与给定向量相似的向量。'
- en: Most of the field names are self-explanatory, except the field called `"embedding"`
    of the `VectorField` type, which is used to store high-dimensional vectors. Redis
    supports two similarity search algorithms, FLAT and HNSW; in our example, we use
    HNSW.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数字段名称都是不言自明的，除了`VectorField`类型的`"embedding"`字段，它用于存储高维向量。Redis支持两种相似性搜索算法，FLAT和HSNW；在我们的示例中，我们使用HSNW。
- en: HSNW stands for *hierarchical navigable small world*. It’s an algorithm used
    for nearest neighbor search in multidimensional spaces and is used here as the
    embedding type. The HNSW algorithm is particularly useful for tasks such as similarity
    search or clustering in high-dimensional spaces. It is known for its efficiency
    and accuracy with lower computational overhead. HNSW organizes vectors into a
    graph structure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: HSNW代表**分层可导航小世界**。它是一种用于多维空间中最近邻搜索的算法，在此用作嵌入类型。HSNW算法特别适用于诸如相似性搜索或高维空间中的聚类等任务。它以其效率、准确性以及较低的计算开销而闻名。HSNW将向量组织成图结构。
- en: FLAT stands for *fast linear approximation transformation*. It is a brute-force
    algorithm and straightforward approach in which all vectors are indexed in a single
    tree or list structure. Finding the nearest neighbors of a query point is typically
    a brute-force search implemented by computing the distance from the query point
    and other indexes. This makes it much more accurate but computationally intensive
    and slower.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: FLAT代表**快速线性近似变换**。它是一种暴力算法和直接的方法，其中所有向量都在单个树或列表结构中索引。查询点的最近邻通常是通过计算查询点与其他索引的距离来实现的暴力搜索。这使得它更加准确，但计算密集且较慢。
- en: The embeddings are float numbers, as denoted by FLOAT32\. We set the dimensions
    to match the Azure OpenAI models’ 1536 dimensions, which must match the LLM’s
    architecture. Finally, we use the COSINE distance metric to measure similarity.
    Redis supports the three types of distance metrics (see table 8.2).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是浮点数，如FLOAT32所示。我们将维度设置为与Azure OpenAI模型的1536维相匹配，这必须与LLM的架构相匹配。最后，我们使用余弦距离度量来衡量相似度。Redis支持三种类型的距离度量（见表8.2）。
- en: Table 8.2 HNSW distance metric options
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.2 HNSW距离度量选项
- en: '| HNSW distance metric | Description |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| HNSW距离度量 | 描述 |'
- en: '| --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| EUCLIDEAN  | The straight-line distance between two points in Euclidean space.
    It’s a good choice when all dimensions are similar (e.g., all distances measured
    in meters).  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 欧几里得距离 | 欧几里得空间中两点之间的直线距离。当所有维度相似时（例如，所有距离以米为单位测量），这是一个不错的选择。 |'
- en: '| DOTPRODUCT  | Calculates the dot product between two vectors. The dot product
    is the sum of the products of the corresponding entries of the two sequence numbers.  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 点积 | 计算两个向量之间的点积。点积是两个序列数对应项乘积之和。 |'
- en: '| COSINE  | Calculates the cosine of the angle between two vectors. Regardless
    of their magnitude, it measures how similar the vectors are. This is often used
    in text analysis, where the direction of the vector (the angle) is more important
    than the length of the vector.  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 余弦 | 计算两个向量之间角度的余弦值。无论它们的幅度如何，它衡量向量之间的相似度。这在文本分析中经常使用，其中向量的方向（角度）比向量的长度更重要。
    |'
- en: TagField vs. TextField
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标签字段与文本字段
- en: The `URL` field is a `TagField` instead of a `TextField`. While this might seem
    odd at first, there is a good reason. With `TagField`, the entire URL is treated
    as a single tag. This property is useful if you want to search for documents using
    the exact URL. However, searching for documents containing certain words in their
    URL would be useless because the URL is not tokenized.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`URL`字段是一个`TagField`而不是`TextField`。虽然一开始这可能看起来有些奇怪，但有一个很好的原因。使用`TagField`，整个URL被视为一个单独的标签。如果想要使用精确URL搜索文档，这个属性很有用。但是，如果您想搜索包含其URL中某些单词的文档，这将毫无用处，因为URL不会被分词。'
- en: In contrast, if the URL field were defined as a `TextField`, it would be tokenized,
    and each part of the URL would be indexed separately. This would be useful if
    you searched for documents containing certain words in their URL. However, it
    would not be useful if you wanted to search for documents by exact URL because
    the URL would be tokenized.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果将URL字段定义为`TextField`，它将被分词，URL的每个部分将分别索引。如果您搜索包含其URL中某些单词的文档，这将很有用。但是，如果您想通过精确URL搜索文档，这将没有用，因为URL将被分词。
- en: In this case, if we ran a search that required tokenization (i.e., searching
    for documents that contain a certain word in their URL), the search would not
    return the expected results. Similarly, if you define the `URL` as a `TextField`
    and then try to perform a search that requires exact matching (i.e., searching
    for documents by exact URL), the search will not return the expected results.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，如果我们运行了一个需要分词的搜索（即搜索包含特定单词的URL的文档），搜索将不会返回预期的结果。同样，如果您将`URL`定义为`TextField`，然后尝试执行一个需要精确匹配的搜索（即通过精确URL搜索文档），搜索也不会返回预期的结果。
- en: Now that we understand the data that we need and the associated schema, let’s
    create the index to begin within Redis. We start by connecting to the Redis database,
    which, in our case, is running locally on Docker and reachable over port 6379,
    as shown in listing 8.3.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了所需的数据和相关模式，让我们在Redis中创建索引以开始。我们首先连接到Redis数据库，在我们的例子中，它在本地的Docker上运行，可以通过端口6379访问，如列表8.3所示。
- en: 'We need the following environment variables pointing to the server host, the
    port, and the password to set, respectively:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以下环境变量指向服务器主机、端口和要设置的密码，分别如下：
- en: For Windows, use
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows，使用
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note  You must restart your terminal to read the new variables.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您必须重新启动您的终端以读取新变量。
- en: On Linux/Mac, use
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux/Mac上，使用
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We first need to establish a connection with the Redis server, which is quite
    straightforward:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要与Redis服务器建立连接，这相当简单：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because we already have our schema defined, as shown in listing 8.2, creating
    a vector index is straightforward. We call the function `create_index` and pass
    it a name, schema, and optional prefix. Only two indexes are supported—`HASH`
    (the default) or `JSON`—for which we need a separate module. In our case, we will
    use the default `HASH`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经定义了模式，如列表8.2所示，创建向量索引是直接的。我们调用`create_index`函数，并传递一个名称、模式和可选的前缀。只支持两种索引——`HASH`（默认）或`JSON`——我们需要一个单独的模块。在我们的情况下，我们将使用默认的`HASH`：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Of course, we can delete the index and view its details. The full code for this
    helper function is shown in the following listing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以删除索引并查看其详情。此辅助函数的完整代码如下所示。
- en: Listing 8.3 Redis search index operations
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3 Redis搜索索引操作
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Redis connection details'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Redis连接详情'
- en: '#2 Connects to the Redis server'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 连接到Redis服务器'
- en: '#3 Sets the dimensions to match the LLM design'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置维度以匹配LLM设计'
- en: '#4 Function to delete index'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 删除索引的函数'
- en: '#5 Function to delete the keys from the index'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从索引中删除键的函数'
- en: '#6 Function to create an index'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 创建索引的函数'
- en: '#7 Function to run the main loop'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 运行主循环的函数'
- en: Figure 8.4 shows this code running locally as an example. The index type is
    HASH, and the keys’ prefix starts with “post.”
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4展示了该代码在本地运行的示例。索引类型是HASH，键的前缀以“post.”开头。
- en: '![figure](../Images/CH08_F04_Bahree.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F04_Bahree.png)'
- en: Figure 8.4 Redis Insight running locally as an example
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4 Redis Insight在本地作为示例运行
- en: In our case, we already have the index populated, and when we execute this to
    see the index, we obtain an output similar to the following listing. Note that
    the output has been truncated for brevity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，索引已经填充，当我们执行此操作以查看索引时，我们获得类似于以下列表的输出。请注意，为了简洁起见，输出已被截断：
- en: Listing 8.4 Redis search index details
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4 Redis搜索索引详情
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Figure 8.5 shows the details of one of the index items using Redis Insight—the
    GUI that allows us to do some basic database management. We can see the fields
    we identified when setting up the index. The embeddings are a binary representation,
    so they appear to be gibberish.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5展示了使用Redis Insight（一个允许我们进行一些基本数据库管理的图形用户界面）查看索引项细节的情况。我们可以看到在设置索引时确定的字段。嵌入是二进制表示，因此看起来像是乱码。
- en: '![figure](../Images/CH08_F05_Bahree.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F05_Bahree.png)'
- en: Figure 8.5 Index details
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 索引详情
- en: Now that we have an index set, let’s see how we can retrieve the data (i.e.,
    the blog posts), chunk it, populate the vector database, and finally update the
    index we created.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了索引，让我们看看我们如何检索数据（即博客文章），将其分块，填充向量数据库，并最终更新我们创建的索引。
- en: 8.4 Retrieving the data
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 获取数据
- en: At a high level, the process is quite simple. We start loading RSS feeds using
    the `feedparser` library; then, we retrieve each blog post found, parse it for
    the content we are interested in, create the corresponding embedding, and save
    all the details in Redis. Listing 8.5 shows this flow.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，这个过程相当简单。我们开始使用`feedparser`库加载RSS源；然后，我们检索找到的每篇博客文章，解析我们感兴趣的内容，创建相应的嵌入，并将所有详细信息保存到Redis中。列表8.5显示了此流程。
- en: Because each blog post is an HTML page, we use `BeautifulSoup`, a Python library,
    to parse the HTML page, allowing us to select the content we need. As shown in
    listing 8.5, we need to clean up some things and parse the content by matching
    the style of the blog post and the HTML generated. The search for various attributes
    and classes (such as `post-title`, etc.) depends on the shape of the incoming
    data and the use case we are trying to solve. In this example, the code must be
    updated if the blog changes its theme or rendering.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每篇博客文章都是一个HTML页面，我们使用`BeautifulSoup`，一个Python库，来解析HTML页面，这样我们就可以选择所需的内容。如列表8.5所示，我们需要清理一些东西，并通过匹配博客文章的风格和生成的HTML来解析内容。对各种属性和类（如`post-title`等）的搜索取决于传入数据的形式和我们试图解决的问题的使用场景。在这个例子中，如果博客更改了主题或渲染方式，代码必须进行更新。
- en: Listing 8.5 Extracting content from HTML
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5 从HTML中提取内容
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For real-world enterprise use cases, the retriever must be aware of the source
    system’s content and structure, which can be quite complex and daunting. In most
    cases, this would need to run through a data pipeline. This data pipeline would
    help address any data engineering aspects needed—all in the context of the associated
    use cases. See section 8.4.1 for more details:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现实世界的企业用例，检索器必须了解源系统的内容和结构，这可能相当复杂且令人畏惧。在大多数情况下，这需要通过数据管道运行。此数据管道将帮助解决所需的所有数据工程方面——所有这些都是在相关用例的背景下进行的。有关更多详细信息，请参阅8.4.1节：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We create a new index hash, adding details of the information we are interested
    in as embeddings—URL, title, publish date, and blog post. We also correlate the
    different chunks that are created with the same context.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个新的索引哈希，添加我们感兴趣的信息的细节作为嵌入—URL、标题、发布日期和博客文章。我们还关联了具有相同上下文创建的不同块。
- en: Note that we only show the key aspect of the code in the next listing, and for
    severity, we avoid the helper functions we have seen before. The complete code
    samples are in the book’s GitHub code repository ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在接下来的列表中，我们只展示了代码的关键部分，并且为了避免严重性，我们避免使用之前见过的辅助函数。完整的代码示例可以在本书的 GitHub 代码仓库中找到（[https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)）。
- en: Listing 8.6 Retrieving blog posts and saving them in Redis
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.6 检索博客文章并将其保存到 Redis
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once we get the blog post’s content, we need to chunk it up, as discussed in
    the previous chapter. For this example, we use spaCy to chunk the blog post and
    also have some overlap between different chunks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获取到博客文章的内容，我们需要将其分块，正如前一章所讨论的。在这个例子中，我们使用 spaCy 对博客文章进行分块，并且不同块之间存在一些重叠。
- en: 8.4.1 Retriever pipeline best practices
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 检索器管道最佳实践
- en: When implementing a RAG pattern, it’s crucial to have a deep understanding of
    the source system’s content and structure. The success of a RAG model hinges on
    its ability to access and interpret the right data, which necessitates a well-architected
    data pipeline. This pipeline is not just a conduit for data flow, but a sophisticated
    framework that ensures data is extracted, transformed, indexed, and stored to
    align with the model’s requirements and the defined use case.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现 RAG 模式时，对源系统内容及其结构的深入了解至关重要。RAG 模型的成功取决于其访问和解释正确数据的能力，这需要构建良好的数据管道。这个管道不仅仅是数据流动的渠道，而是一个复杂的框架，确保数据被提取、转换、索引和存储，以符合模型的要求和定义的使用案例。
- en: The first step toward implementing GPTs and LLMs in enterprises is a deep understanding
    of the source system. This involves thoroughly analyzing the data structure, including
    entity-relationship diagrams, data types, and data distribution. Data profiling
    tools can be instrumental in understanding the nature of the content.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业中实施 GPTs 和 LLMs 的第一步是对源系统有深刻的理解。这包括彻底分析数据结构，包括实体关系图、数据类型和数据分布。数据概要分析工具在理解内容性质方面可以起到关键作用。
- en: Note  For RAG to work well, it is important to carefully plan the preprocessing
    one needs to do in the retriever pipeline and not just use everything without
    considering whether it is better. If not planned well, this will create problems
    when using search as part of a RAG implementation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了使 RAG 运作良好，仔细规划在检索器管道中需要进行的预处理非常重要，而不仅仅是使用一切，不考虑是否更好。如果没有良好规划，这在使用搜索作为
    RAG 实现的一部分时将造成问题。
- en: The next phase defines the use case, which entails creating a detailed requirement
    document outlining the problem, potential solutions, expected results, and success
    metrics. This document should also detail the users’ informational needs and the
    scenarios in which the RAG model will be applied.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个阶段定义了用例，这包括创建一个详细的需求文档，概述问题、潜在解决方案、预期结果和成功指标。此文档还应详细说明用户的信息需求以及 RAG 模型将被应用到的场景。
- en: Following this, the focus shifts to data extraction and transformation. This
    process involves using ETL (extract, transform, load) tools to extract data from
    the source system and transform it into a format the RAG model can understand.
    It may involve NLP techniques such as tokenization, stop-word removal, and lemmatization.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，重点转向数据提取和转换。这个过程涉及使用 ETL（提取、转换、加载）工具从源系统提取数据并将其转换为 RAG 模型可以理解的形式。这可能涉及自然语言处理技术，如分词、停用词去除和词形还原。
- en: Once the data has been transformed, it needs to be indexed for efficient retrieval.
    Azure AI Search, Elasticsearch, Solr, and Lucene are ideal for this purpose, as
    they provide full-text search capabilities and can handle large datasets effectively.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换完成后，需要对其进行索引以实现高效检索。Azure AI Search、Elasticsearch、Solr 和 Lucene 对于此目的非常理想，因为它们提供全文搜索功能，并且可以有效地处理大量数据集。
- en: Parallel to data indexing, selecting a suitable data storage solution is important.
    Depending on the specific needs of the data size, speed, and type, this could
    be a traditional SQL database, a NoSQL database such as Cosmos DB, or a distributed
    file system such as Hadoop HDFS.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据索引并行，选择合适的数据存储解决方案也很重要。根据数据大小、速度和类型的特定需求，这可能是一个传统的SQL数据库，如Cosmos DB，或一个分布式文件系统，如Hadoop
    HDFS。
- en: One of the most critical phases is preprocessing planning. This involves careful
    planning of preprocessing steps, which could involve techniques such as noise
    removal, normalization, and dimensionality reduction. The goal is to retain information
    relevant to the use case while reducing the model’s complexity.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最关键的阶段之一是预处理规划。这涉及到对预处理步骤的仔细规划，可能包括噪声消除、归一化和降维等技术。目标是保留与用例相关的信息，同时降低模型的复杂性。
- en: The next phase is model integration, which involves using APIs or SDKs provided
    by the AI model vendor to integrate the RAG model into the application. The retriever
    must be configured with the correct query parameters, and the generator should
    be set up with the desired output structure.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个阶段是模型集成，这涉及到使用AI模型供应商提供的API或SDK将RAG模型集成到应用程序中。检索器必须配置正确的查询参数，生成器应设置所需的输出结构。
- en: Fine-tuning and monitoring are crucial for enhancing the model’s performance
    and ensuring the system’s health. This involves using a validation dataset for
    fine-tuning and application performance management (APM) tools for monitoring.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和监控对于提高模型性能和确保系统健康至关重要。这涉及到使用验证数据集进行微调和应用程序性能管理（APM）工具进行监控。
- en: Regarding scalability and reliability, cloud platforms such as AWS, Google Cloud,
    or Azure should be used to scale the system as needed. Containerization platforms
    such as Docker and Kubernetes can assist in scaling and managing the application.
    Redundancy and failover strategies are crucial to ensuring system reliability.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可扩展性和可靠性，应使用AWS、Google Cloud或Azure等云平台按需扩展系统。Docker和Kubernetes等容器化平台可以帮助扩展和管理应用程序。冗余和故障转移策略对于确保系统可靠性至关重要。
- en: Furthermore, security and compliance cannot be overlooked. Implementing data
    encryption, user authentication, access control, and regular system audits can
    ensure data security and compliance with data protection regulations such as GDPR
    or CCPA.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，安全和合规性不容忽视。实施数据加密、用户身份验证、访问控制和定期系统审计可以确保数据安全并符合数据保护法规，如GDPR或CCPA。
- en: Before deployment, rigorous testing and validation are imperative to ensure
    that the pipeline and the RAG model meet the expectations outlined by the use
    case. Once the system is live, comprehensive documentation and technical training
    should be provided to the team for effective management, maintenance, and troubleshooting.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署之前，严格的测试和验证是必不可少的，以确保管道和RAG模型符合用例中概述的期望。一旦系统上线，应提供全面的文档和技术培训，以便团队进行有效的管理、维护和故障排除。
- en: Finally, it’s crucial to ensure the quality control of the retrieval corpus,
    implement measures for information security and privacy, regularly update the
    retrieval corpus, and efficiently allocate resources. By following these steps,
    enterprises can effectively build and maintain AI-powered applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保检索语料库的质量控制至关重要，实施信息安全和个人隐私措施，定期更新检索语料库，并有效地分配资源。通过遵循这些步骤，企业可以有效地构建和维护AI驱动的应用程序。
- en: 8.5 Search using Redis
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 使用Redis进行搜索
- en: Now that we have the data ingested and the index ready, we can search against
    it. We create a simple console app that accepts a user’s query, vectorizes it,
    and searches based on the top three similar posts to return to the user. This
    is a semantic search. The following listing shows the output generated as an example
    when we ask about “Longhorn.”
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经摄取了数据并准备好了索引，我们可以对其进行搜索。我们创建了一个简单的控制台应用程序，它接受用户的查询，将其矢量化，并根据返回给用户的三个最相似帖子进行搜索。这是一个语义搜索。以下列表显示了当我们询问“Longhorn”时生成的输出示例。
- en: Listing 8.7 Search results
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7 搜索结果
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note  Windows Longhorn used to be the codename for the operating system that
    eventually became Windows Vista.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Windows Longhorn曾经是最终成为Windows Vista的操作系统代号。
- en: Let’s check out the code for implementing the search using Redis. We first take
    a user query such as “Tell me about Longhorn,” create a vector, and use cosine
    similarity to obtain a list of comparable results.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看使用Redis实现搜索的代码。我们首先获取一个用户查询，例如“告诉我关于Longhorn的事情”，创建一个向量，并使用余弦相似度获取一组可比较的结果。
- en: Listing 8.8 Searching using Redis
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8 使用Redis进行搜索
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 A base query that prefilters fields and is implemented as a KNN search'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 一个基础查询，它通过KNN搜索预先过滤字段'
- en: '#2 Selects the different fields we are interested in searching'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 选择我们感兴趣的搜索的不同字段'
- en: '#3 Sorts by cosine similarity in descending order'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 按余弦相似度降序排序'
- en: '#4 Executes the query'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 执行查询'
- en: '#5 Captures the query from the user'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从用户那里捕获查询'
- en: '#6 Vectorizes the input'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 向量化输入'
- en: '#7 Converts the vector to a NumPy array'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将向量转换为NumPy数组'
- en: '#8 Performs the similarity search'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 执行相似度搜索'
- en: As the name suggests, the `hybrid_search()` function does the heavy lifting
    of running the hybrid search query. A hybrid search query combines multiple types
    of searches into a single query. This can include combining text-based searches
    with other types, such as numerical, categorical, or even vector-based searches.
    Note that the exact search type would depend on the information and the requirement.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`hybrid_search()`函数执行混合搜索查询的重头戏。混合搜索查询将多种类型的搜索组合成一个单一的查询。这可以包括将基于文本的搜索与其他类型相结合，例如数值、分类或甚至是基于向量的搜索。请注意，确切的搜索类型将取决于信息和需求。
- en: In our example, we combine a K-Nearest Neighbors (KNN) search on an embedding
    vector with other search fields. The KNN search finds the most related items to
    a given item, in this case, the most similar posts to a given query vector. The
    query results are sorted by vector score, which means a high to low ordering based
    on cosine similarity. In other words, the results with the highest similarity
    are shown first. We also restrict this to the top three items, as depicted by
    the `top_k` parameter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将嵌入向量上的K-Nearest Neighbors (KNN)搜索与其他搜索字段相结合。KNN搜索找到与给定项目最相关的项目，在这种情况下，与给定查询向量最相似的帖子。查询结果按向量分数排序，这意味着基于余弦相似度的高到低排序。换句话说，首先显示相似度最高的结果。我们还通过`top_k`参数将其限制为前三个项目。
- en: Note that the exact nature of the search and type also depends on the search
    engine and the data type. For more details on Redis search types and KNN, see
    the documentation at [https://mng.bz/o0Gp](https://mng.bz/o0Gp).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，搜索的确切性质和类型也取决于搜索引擎和数据类型。有关Redis搜索类型和KNN的更多详细信息，请参阅[https://mng.bz/o0Gp](https://mng.bz/o0Gp)上的文档。
- en: Now that we have seen the search, let’s combine all the dimensions and integrate
    them into a chat experience using an LLM.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了搜索，让我们将所有维度结合起来，并使用LLM将其集成到聊天体验中。
- en: 8.6 An end-to-end chat implementation powered by RAG
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 由RAG驱动的端到端聊天实现
- en: Throughout this and the previous chapter, we have discussed and examined all
    the pieces to help us understand some of the core concepts; now, we can bring
    it all together and build an end-to-end chat application. In the application,
    we can ask questions to get details about our data (i.e., the blog posts). Figure
    8.6 shows the application flow.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和上一章中，我们讨论和检查了所有这些部分，以帮助我们理解一些核心概念；现在，我们可以将它们全部结合起来，构建一个端到端的聊天应用。在应用中，我们可以提出问题以获取有关我们的数据（即博客文章）的详细信息。图8.6显示了应用流程。
- en: '![figure](../Images/CH08_F06_Bahree.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F06_Bahree.png)'
- en: Figure 8.6 End-to-end chat application
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6 端到端聊天应用
- en: The question the user asks first gets converted into embeddings and then searched
    in Redis using a hybrid search index to find similar chunks, which are returned
    as search results. As we saw earlier, the blog posts have already been injected
    into the Redis database and indexed. Once we have the results, we formulate the
    LLM prompt by combining the original questions and the chunks retrieved to answer
    from. These are passed into the prompt itself before finally calling the LLM to
    generate a response.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 用户首先提出的问题被转换为嵌入，然后在Redis中使用混合搜索索引进行搜索以找到类似的片段，这些片段作为搜索结果返回。正如我们之前看到的，博客文章已经注入到Redis数据库并进行了索引。一旦我们得到结果，我们就通过结合原始问题和检索到的片段来回答，形成LLM提示。这些被传递到提示本身，最后调用LLM生成响应。
- en: On the search front, we deployed Redis running locally and created a vector
    index. We read all the blog posts going back nearly 20 years. We created the relevant
    chunks for these posts and their corresponding embeddings and populated our vector
    database. We also implemented a vector search on those embeddings. The only piece
    left is to integrate all of this into our application and hook it up with an LLM
    to complete the last stage of our RAG implementation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索方面，我们在本地部署了Redis并创建了一个向量索引。我们读取了近20年的所有博客文章。我们为这些文章创建了相关的片段及其对应的嵌入，并填充了我们的向量数据库。我们还对这些嵌入实现了向量搜索。唯一剩下的事情是将所有这些集成到我们的应用程序中，并将其与LLM连接起来，以完成我们RAG实现的最后阶段。
- en: Listing 8.9 shows exactly how to do this. Several helper functions, such as
    `get_ search_results()`, take the user’s query, call another helper function to
    search Redis, and return any results found. The actual API call that calls the
    GPT is in the `ask_gpt()` function, and it is a `ChatCompletion()` API, just like
    we saw earlier.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9展示了如何实现这一点。几个辅助函数，例如`get_search_results()`，接收用户的查询，调用另一个辅助函数来搜索Redis，并返回找到的任何结果。调用GPT的实际API调用在`ask_gpt()`函数中，它是一个`ChatCompletion()`
    API，就像我们之前看到的。
- en: As with previous examples, we leave out the code’s helper functions and other
    aspects for brevity. The complete code samples are available in the GitHub code
    repository accompanying the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，为了简洁，我们省略了代码的辅助函数和其他方面。完整的代码示例可在本书配套的GitHub代码仓库中找到（[https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)）。
- en: Listing 8.9 End-to-end RAG-powered chat
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9端到端RAG驱动的聊天
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Vectorizes the query'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将查询向量化'
- en: '#2 Converts the vector to a numpy array'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将向量转换为numpy数组'
- en: '#3 Performs the similarity search'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 执行相似性搜索'
- en: '#4 Manages token budget'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 管理token预算'
- en: '#5 Loops through the results while still keeping within the token budget'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 在保持token预算的同时循环遍历结果'
- en: '#6 Runs a vector search to get embeddings'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 运行向量搜索以获取嵌入'
- en: '#7 Sets up the chat completion calls'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 设置聊天完成调用'
- en: '#8 Calls the LLM'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 调用LLM'
- en: We can see all this coming together when we run it and chat with the blog. It
    understands the query, creates embeddings, uses the vector database and the associated
    vector indexes to retrieve the top five matching results, adds that to the prompt,
    and uses the LLM to generate the response (figure 8.7).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行它并与博客进行聊天时，我们可以看到所有这些功能是如何结合在一起的。它理解查询，创建嵌入，使用向量数据库和相关向量索引来检索前五个匹配结果，将这些结果添加到提示中，并使用LLM来生成响应（图8.7）。
- en: In the example we have seen thus far, we are responsible for everything—from
    setting up the Docker containers to deploying Redis and ingesting the data. This
    is not enough for enterprises to go into production. More system engineering is
    required, such as setting up various clusters of machines, scaling them up or
    down as needed, managing Redis, security requirements, overall operations, and
    so forth. This takes a significant amount of time, effort, cost, and skills that
    not every organization might have. Another option is to use Azure OpenAI, which
    can do much of this out of the box and allows organizations a quicker time to
    market, potentially at a lower cost. Let’s see how Azure OpenAI can achieve the
    same result but much faster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止看到的示例中，我们负责一切——从设置Docker容器到部署Redis和摄取数据。这对企业来说不足以投入生产。还需要更多的系统工程，例如设置各种机器集群，根据需要扩展或缩减它们，管理Redis，安全要求，整体运营等等。这需要大量的时间、努力、成本和技能，并非每个组织都具备。另一种选择是使用Azure
    OpenAI，它可以做到许多事情，并且可以让组织更快地进入市场，可能成本更低。让我们看看Azure OpenAI如何以更快的速度实现相同的结果。
- en: '![figure](../Images/CH08_F07_Bahree.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F07_Bahree.png)'
- en: Figure 8.7 Q&A using blog data with GPT-3.5 Turbo
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7使用GPT-3.5 Turbo的博客数据问答
- en: 8.7 Using Azure OpenAI on your data
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 在您的数据上使用Azure OpenAI
- en: Many enterprises use Azure, and incorporating Azure OpenAI as part of their
    data strategy represents a pivotal step in employing the power of generative AI
    for business transformation. Azure OpenAI provides an enterprise-grade platform
    to integrate advanced AI models such as ChatGPT into your data workflows.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 许多企业使用Azure，将Azure OpenAI作为其数据策略的一部分，代表了在业务转型中利用生成式AI力量的关键一步。Azure OpenAI提供了一个企业级平台，将高级AI模型如ChatGPT集成到您的数据工作流程中。
- en: '“Azure OpenAI on your data” is the service that enables running these powerful
    chat models on your data and getting out-of-the-box features that enterprises
    require for production workloads: scalability, security, refreshes, and integration
    into others. You can connect your data source using Azure OpenAI Studio (figure
    8.8) or the REST API.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: “Azure OpenAI在您的数据上”是一项服务，它使您能够在您的数据上运行这些强大的聊天模型，并获得企业生产工作负载所需的即用型功能：可扩展性、安全性、更新和与其他系统的集成。您可以使用Azure
    OpenAI Studio（见图8.8）或REST API连接您的数据源。
- en: Note Azure AI Studio is a platform that combines capabilities across multiple
    Azure AI services. It is designed for developers to build generative AI applications
    on an enterprise-grade platform. You can first interact with a project code via
    the Azure AI SDK and Azure AI CLI and seamlessly explore, build, test, and deploy
    using cutting-edge AI tools and ML models.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Azure AI Studio是一个结合了多个Azure AI服务功能的平台。它旨在为开发者提供一个企业级平台，用于构建生成式AI应用。您可以通过Azure
    AI SDK和Azure AI CLI首先与项目代码进行交互，并使用最前沿的AI工具和ML模型无缝探索、构建、测试和部署。
- en: At the core of Azure OpenAI’s appeal is its seamless integration with the broader
    Azure ecosystem. Connecting these powerful AI models to your data repositories
    unlocks the potential for more sophisticated data analysis, natural language processing,
    and predictive insights. This integration is particularly beneficial for enterprises
    with a significant footprint in Azure, enabling them to enhance their existing
    infrastructure with minimal disruption.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI的吸引力在于其与更广泛的Azure生态系统的无缝集成。将这些强大的AI模型连接到您的数据存储库，可以释放更复杂的数据分析、自然语言处理和预测洞察的潜力。这种集成对于在Azure上拥有显著足迹的企业尤其有益，使他们能够在最小干扰的情况下增强现有基础设施。
- en: '![figure](../Images/CH08_F08_Bahree.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F08_Bahree.png)'
- en: Figure 8.8 Adding your data to Azure OpenAI
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8 将您的数据添加到Azure OpenAI
- en: Azure AI Studio supports multiple options from existing Azure AI Search indexes,
    Blob storage, Cosmos DB, and so forth. One of these options is a URL, which we
    will use to ingest blog posts (see figure 8.9). We can also save the RSS feed
    locally and upload it as a file. One of the advantages of using our own Azure
    AI Search index is that it does the heavy lifting of keeping the data ingestion
    up to date from the source systems. This replaces Redis and can be globally distributed
    to a cloud-scale if required.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AI Studio支持从现有的Azure AI搜索索引、Blob存储、Cosmos DB等选项。其中之一是一个URL，我们将用它来摄取博客文章（见图8.9）。我们还可以将RSS源本地保存并作为文件上传。使用我们自己的Azure
    AI搜索索引的一个优点是，它能够从源系统保持数据摄取的更新，这可以替代Redis，并在需要时全球分布到云规模。
- en: '![figure](../Images/CH08_F09_Bahree.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F09_Bahree.png)'
- en: 'Figure 8.9 Azure AI Studio: Adding a data source'
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9 Azure AI Studio：添加数据源
- en: We can configure and set up most things here, including a storage resource where
    this data will be saved, an Azure AI Search resource, the index details, embedding
    details, and so forth (see figure 8.10). With a few clicks, all of this is set
    up and ready for us to use.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里配置和设置大多数事情，包括将保存这些数据的存储资源、Azure AI搜索资源、索引详情、嵌入详情等等（见图8.10）。只需几点击，所有这些就设置好了，并准备好供我们使用。
- en: '![figure](../Images/CH08_F10_Bahree.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F10_Bahree.png)'
- en: Figure 8.10 Configure details for data ingestion
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10 配置数据摄取的详细信息
- en: 'On the information security front, this process is streamlined by Azure’s robust
    security and compliance framework, ensuring that your data remains protected throughout
    its interaction with AI models. Azure OpenAI supports two key features on your
    data: role-based and document-level access controls. This feature, working alongside
    Azure AI Search security filters, can be used to limit access to only those users
    who should have access based on their permitted groups and LDAP memberships, which
    is a critical requirement for many enterprises, especially in regulated industries.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息安全方面，Azure强大的安全和合规框架简化了这一过程，确保您的数据在与AI模型交互的整个过程中都得到保护。Azure OpenAI支持您数据上的两个关键功能：基于角色的和文档级别的访问控制。这个功能与Azure
    AI搜索安全过滤器协同工作，可以用来限制只有那些根据其允许的组和LDAP成员资格应该有权访问的用户，这对于许多企业，尤其是在受监管的行业中，是一个关键要求。
- en: Finally, Azure’s ability to process and analyze large cloud-scale volumes of
    unstructured data scalability is another significant advantage. For example, OpenAI’s
    ChatGPT internally uses Azure AI Search, and that workload is 100+ million users
    per day. Azure’s cloud infrastructure allows for the easy scaling of AI capabilities
    as your data needs grow. More details on Azure OpenAI can be found at [https://mng.bz/n022](https://mng.bz/n022).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Azure处理和分析大量云规模非结构化数据的能力的可扩展性是另一个显著优势。例如，OpenAI的ChatGPT内部使用Azure AI Search，并且每天的工作负载超过1亿用户。Azure的云基础设施允许随着数据需求的增长轻松扩展AI能力。有关Azure
    OpenAI的更多详细信息，请参阅[https://mng.bz/n022](https://mng.bz/n022)。
- en: 8.8 Benefits of bringing your data using RAG
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 使用RAG引入数据的优势
- en: Enterprises often struggle to extract meaningful insights from unstructured
    data sources such as emails, customer feedback, or social media interactions.
    When enterprises integrate their data using RAG in LLMs, they unlock many advantages,
    enhancing the functionality and applicability of these AI systems in their unique
    business contexts.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常难以从电子邮件、客户反馈或社交媒体互动等非结构化数据源中提取有意义的见解。当企业使用LLMs中的RAG整合他们的数据时，他们可以解锁许多优势，增强这些AI系统在其独特业务环境中的功能和适用性。
- en: This feature offers distinct advantages over merely expanding the context window
    of these models. The pattern enhances the relevance and accuracy of LLM outputs
    and provides strategic benefits that a larger context window alone cannot match.
    LLMs can analyze this data, interpret it in a human-like manner, and provide actionable
    insights, all in a fraction of the time it would take using traditional methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能在仅扩展这些模型的上下文窗口方面提供了独特的优势。这种模式增强了LLM输出的相关性和准确性，并提供了仅靠更大的上下文窗口无法比拟的战略优势。LLMs可以分析这些数据，以类似人类的方式解释它，并在极短的时间内提供可操作的见解。
- en: Integrating RAG with real-time enterprise data ensures that the information
    retrieved and included in responses is relevant and current, a critical factor
    in rapidly evolving industries. This customization leads to more precise and applicable
    answers, which is especially beneficial for sectors with specialized knowledge,
    such as legal, medical, or technical fields.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将RAG与实时企业数据进行整合确保了检索并包含在响应中的信息是相关且最新的，这在快速发展的行业中是一个关键因素。这种定制化导致更精确和适用的答案，这对于具有专门知识领域的行业，如法律、医疗或技术领域，尤其有益。
- en: The key advantage of using enterprise-specific data in conjunction with RAG
    models lies in the tailored accuracy and applicability of the model’s responses.
    LLMs with a larger context window can process more information in a single instance,
    but they may still lack the depth of knowledge in specialized domains. When enterprises
    introduce their data, the LLMs can generate responses intricately aligned with
    the organization’s specific industry, jargon, and operational intricacies. This
    specificity is crucial for industries where specialized knowledge is paramount
    and goes beyond the scope of what a larger context window can provide.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用企业特定数据结合RAG模型的关键优势在于模型响应的定制准确性和适用性。具有更大上下文窗口的LLMs可以在单个实例中处理更多信息，但它们可能仍然缺乏在专业领域的知识深度。当企业引入他们的数据时，LLMs可以生成与组织特定行业、术语和运营复杂性复杂对齐的响应。这种特定性对于专业知识至关重要的行业至关重要，并且超出了更大上下文窗口所能提供的范围。
- en: While a larger context window allows for a broader range of preexisting information
    to be considered in the model’s responses, it does not necessarily incorporate
    the most current or enterprise-specific data. In addition, the larger the context
    window, the more the model has to process and the slower it is.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然更大的上下文窗口允许在模型的响应中考虑更广泛的现有信息，但它并不一定包含最新或企业特定的数据。此外，上下文窗口越大，模型需要处理的数据就越多，速度也就越慢。
- en: Furthermore, integrating proprietary data enhances decision-making processes
    more effectively than simply expanding the context window. This integration enables
    LLMs to offer insights and analysis deeply rooted in the enterprise’s historical
    data and strategic objectives. In contrast, a larger context window might provide
    broader information but lacks precision and direct relevance to enterprises’ strategic
    questions and challenges.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，整合专有数据比仅仅扩展上下文窗口更有效地增强决策过程。这种整合使得大型语言模型（LLMs）能够提供深深植根于企业历史数据和战略目标的见解和分析。相比之下，更大的上下文窗口可能提供更广泛的信息，但缺乏精确性和对企业战略问题和挑战的直接相关性。
- en: Regarding data security and privacy, bringing proprietary data under enterprise
    control is more manageable than relying on public or generalized data that a larger
    context window might access. By controlling data inputs, enterprises can more
    effectively ensure compliance with data privacy regulations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据安全和隐私，将专有数据置于企业控制之下比依赖于可能通过更大的上下文窗口访问的公共或通用数据更容易管理。通过控制数据输入，企业可以更有效地确保符合数据隐私法规。
- en: Implementing RAG with your data offers significant advantages for AI safety
    in enterprise environments, primarily by enhancing the accuracy and reliability
    of information. This fusion of generative capabilities of LLMs with a comprehensive
    corpus of data allows the model to access up-to-date, factual data, crucial for
    enterprises dealing with time-sensitive and accuracy-critical information. Moreover,
    by retrieving from a diverse set of sources, RAG can mitigate biases inherent
    in the training data of LLMs, a vital feature for making unbiased, data-driven
    decisions. Enterprises can customize the retrieval corpus, ensuring alignment
    with industry regulations and internal policies. Furthermore, incorporating the
    latest information and providing sources for generated content offers improved
    transparency and decision-making support.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中实施RAG与您的数据相结合提供了显著的AI安全性优势，主要是通过提高信息的准确性和可靠性。这种将LLM的生成能力与全面的数据语料库相结合的方法，使模型能够访问最新的、事实性的数据，这对于处理时间敏感和准确性关键信息的企业至关重要。此外，通过从多样化的来源检索数据，RAG可以减轻LLM训练数据中固有的偏差，这对于做出无偏、数据驱动的决策至关重要。企业可以定制检索语料库，确保与行业法规和内部政策保持一致。此外，纳入最新信息和提供生成内容的来源，提供了改进的透明度和决策支持。
- en: While expanding the context window of LLMs offers certain benefits, integrating
    proprietary data with RAG models provides specificity, current relevance, strategic
    alignment, personalization, data security, and innovation potential that a mere
    increase in the context window cannot match. This approach enables enterprises
    to use LLMs more effectively for their unique business needs and objectives.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然扩大LLM（大型语言模型）的上下文窗口提供了一定的好处，但将专有数据与RAG模型集成提供了特定性、当前相关性、战略一致性、个性化、数据安全性和创新潜力，这是仅仅增加上下文窗口所无法比拟的。这种方法使企业能够更有效地利用LLM满足其独特的商业需求和目标。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The benefits of integrating proprietary data with RAG models are enhancing AI
    systems’ specificity, relevance, strategic alignment, personalization, data security,
    and innovation potential.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将专有数据与RAG模型集成的好处是增强了AI系统的特定性、相关性、战略一致性、个性化、数据安全性和创新潜力。
- en: Using proprietary data over merely expanding the context window of LLMs offers
    multiple advantages, as the former provides more accurate, relevant, and personalized
    answers.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与仅仅扩大LLM的上下文窗口相比，使用专有数据提供了多个优势，因为前者提供了更准确、相关和个性化的答案。
- en: In a production environment, using a vector database and vector index to manage,
    secure, and scale embeddings is crucial for performance and cost reasons.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中，使用向量数据库和向量索引来管理、安全和扩展嵌入对于性能和成本至关重要。
- en: The process of retrieving proprietary data, chunking it, creating embeddings,
    and saving the details in a vector database depends on the shape of the data at
    hand. It can require significant planning and data engineering effort.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取专有数据、将其分块、创建嵌入并将详细信息保存到向量数据库中的过程取决于现有数据的形状。这可能需要大量的规划和数据工程工作。
- en: Integration of a RAG pattern with a source system is complex, requiring planning,
    robust engineering, and an understanding of the data structure details.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RAG模式与源系统集成是复杂的，需要规划、稳健的工程和数据结构细节的理解。
- en: An end-to-end application using RAG, prompt engineering, embeddings, and search
    can be very powerful for organizations. Still, it is also complex, and if not
    designed properly, it will slow things down when deploying to production.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAG、提示工程、嵌入和搜索的端到端应用对于组织来说可能非常强大。然而，它也很复杂，如果不正确设计，在部署到生产环境中时可能会减慢速度。
- en: The chapter highlights how to conduct search using a vector database, retrieving
    the most similar items to a given item based on their vector embeddings. It also
    shows how incorporating the vector databases and RAG is key for implementing an
    end-to-end chat application.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章重点介绍了如何使用向量数据库进行搜索，根据给定项的向量嵌入检索与其最相似的项目。它还展示了将向量数据库和RAG（检索即生成）集成对于实现端到端聊天应用的重要性。
- en: “Azure OpenAI on your data” is a PaaS service that enables enterprises to run
    AI models on their data with out-of-the-box features such as scalability, security,
    and integration into other Azure services.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Azure OpenAI on your data”是一种PaaS服务，它使企业能够在他们的数据上运行AI模型，并具有诸如可扩展性、安全性和集成到其他Azure服务等功能。
