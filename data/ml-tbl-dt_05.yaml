- en: 4 Classical algorithms for tabular data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 表格数据的经典算法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to Scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn简介
- en: Exploring and processing features of the Airbnb NYC dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和处理Airbnb纽约市数据集的特征
- en: Some classic machine learning techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些经典的机器学习技术
- en: Depending on the problem, classic machine learning algorithms are often the
    most practical approach to working with tabular data. With decades of research
    and practice behind these tools and algorithms, there is a rich palette of solutions
    to choose from.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题不同，经典机器学习算法通常是处理表格数据的最实用方法。这些工具和算法背后有着数十年的研究和实践，提供了丰富的解决方案供选择。
- en: In this chapter, we’ll cover essential algorithms in classical machine learning
    for making predictions using tabular data. We have focused on the linear models
    because they are still the most common solutions for both a challenging baseline
    and a solid and robust model in production. In addition, discussing linear models
    helps us build concepts and ideas that we can find in deep learning architectures
    and in more advanced machine learning algorithms, such as gradient-boosting decision
    trees (which will be one of the topics of the next chapter).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍用于使用表格数据进行预测的经典机器学习中的基本算法。我们专注于线性模型，因为它们仍然是挑战性基线和生产中稳健模型的常见解决方案。此外，讨论线性模型有助于我们构建可以在深度学习架构和更高级的机器学习算法（如梯度提升决策树，这是下一章的主题之一）中找到的概念和思想。
- en: We’ll also give you a quick introduction to Scikit-learn, a powerful and versatile
    machine learning library that we’ll use to continue exploring the Airbnb NYC dataset.
    We’ll stay away from lengthy mathematical definitions and textbook details in
    favor of examples and practical recommendations for applying these models to tabular
    data problems.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为您快速介绍Scikit-learn，这是一个强大且多功能的机器学习库，我们将使用它继续探索Airbnb纽约市数据集。我们将避免冗长的数学定义和教科书细节，而是提供示例和实际建议，以将这些模型应用于表格数据问题。
- en: 4.1 Introducing Scikit-learn
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 Scikit-learn简介
- en: 'Scikit-learn is an open-source library for classic machine learning. It started
    in 2007 as a Google Summer of Code project by David Cournapeau, and it later became
    part of the SciKits (short for Scipy Toolkits: [https://projects.scipy.org/scikits.html](https://projects.scipy.org/scikits.html))
    until the INRIA (Institut National de Recherche en Informatique et en Automatique)
    and its foundation took the leadership of the project and of its development.
    We provide a short example of how Scikit-learn can quickly solve most machine
    learning problems. In our starting example,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn是一个用于经典机器学习的开源库。它始于2007年，是David Cournapeau的一个Google Summer of Code项目，后来成为SciKits（Scipy工具包的简称：[https://projects.scipy.org/scikits.html](https://projects.scipy.org/scikits.html)）的一部分，直到INRIA（法国国家信息与自动化研究所）及其基金会接管了项目及其开发。我们提供了一个简短的例子，说明Scikit-learn如何快速解决大多数机器学习问题。在我们的起始示例中，
- en: We create a synthetic dataset for a classification problem with a binary balanced
    target with half labels positive and half negative.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个用于分类问题的合成数据集，目标具有二进制平衡标签，一半为正标签，一半为负标签。
- en: We set up a pipeline standardizing the features and passing them to a logistic
    regression model, one of the simplest and most effective statistical-based machine
    learning algorithms for classification problems.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了一个管道，标准化特征并将它们传递给逻辑回归模型，这是分类问题中最简单和最有效的基于统计的机器学习算法之一。
- en: We evaluate its performance using cross-validation.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用交叉验证来评估其性能。
- en: Finally, assured by the cross-validation results that our work with the problem
    is fine, we train a model on all the available data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在交叉验证结果确保我们处理问题的方法正确后，我们在所有可用数据上训练了一个模型。
- en: Listing 4.1 shows the complete listing and most of the features offered by Scikit-learn
    applied in a simple classification problem based on synthetically generated data.
    After creating the data, we define a pipeline, putting together statistical standardization
    with a basic model, logistic regression, for classification. Everything is first
    sent into a function that automatically estimates its performance on an evaluation
    metric, the accuracy, and the time its predictions are correct. Finally, figuring
    that its evaluated performances are suitable, we refit the same machine learning
    algorithm with all the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1展示了基于合成数据的简单分类问题中Scikit-learn提供的完整列表和大多数功能。在创建数据后，我们定义了一个管道，将统计标准化与基本模型、逻辑回归相结合用于分类。所有内容首先被送入一个函数，该函数自动估计其在评估指标（准确率）上的性能以及预测正确的时间。最后，考虑到其评估的性能是合适的，我们使用所有数据重新拟合相同的机器学习算法。
- en: Listing 4.1 Example using Scikit-learn for a classification problem
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 使用Scikit-learn解决分类问题的示例
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Generates a synthetic dataset with specified characteristics
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成具有指定特性的合成数据集
- en: ② Creates an instance of the LogisticRegression model
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建LogisticRegression模型的实例
- en: ③ Creates a pipeline that sequentially applies standard scaling and the logistic
    regression model
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个管道，按顺序应用标准缩放和逻辑回归模型
- en: ④ Performs a five-fold cross-validation using the defined pipeline, calculating
    accuracy scores
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道执行五折交叉验证，计算准确率得分
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证测试准确率得分的平均值和标准差
- en: ⑥ Fits the logistic regression model to the entire dataset X with corresponding
    labels y
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将逻辑回归模型拟合到整个数据集X及其对应的标签y
- en: 'The resulting output reports the obtained cross-validation accuracy on the
    classification:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出报告了分类中获得的交叉验证准确率：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The key point here is not the model but the procedure of doing things, which
    is standard for all tabular problems, whether you work with classical machine
    learning models or cutting-edge deep learning algorithms. Scikit-learn perfectly
    embeds such a procedure in its API, thus demonstrating a versatile and indispensable
    tool for tabular data problems. In the following sections, we will explore its
    characteristics and workings since we will reuse its procedures multiple times
    in our examples in the book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里关键的不是模型，而是做事的程序，这对于所有表格问题都是标准的，无论你使用的是经典机器学习模型还是前沿的深度学习算法。Scikit-learn完美地将这样的程序嵌入到其API中，从而证明了它是表格数据问题的多才多艺且不可或缺的工具。在接下来的章节中，我们将探讨其特性和工作原理，因为我们将在本书的示例中多次重用其程序。
- en: 4.1.1 Common features of Scikit-learn packages
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 Scikit-learn包的常见特性
- en: The key characteristics of the Scikit-learn package are
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包的关键特性是
- en: It offers a wide range of models for classification and regression, as well
    as functions for clustering, dimensionality reduction, preprocessing, and model
    selection. Most models will work in-memory when data is processed in the computer
    memory and out-of-core when data cannot fit into memory and is accessed from disk,
    allowing learning from data that exceeds your available computer memory.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了广泛的分类和回归模型，以及聚类、降维、预处理和模型选择的功能。大多数模型在数据在计算机内存中处理时将在内存中工作，当数据无法适应内存并从磁盘访问时将在内存外工作，从而允许从超出可用计算机内存的数据中学习。
- en: Across its range of models, it presents a consistent API (class methods such
    as `fit`, `partial_fit`, `predict`, `predict_proba`, `transform`) that can be
    quickly learned and reused and that focuses exclusively on the transformations
    and processes necessary for a model to learn from data and predict from it. Scikit-learn’s
    API also offers automatic segregation of train and test data, the ability to chain
    and reuse its elements in a data pipeline, and accessibility of its parameters
    by simply inspecting the used class’s public attributes.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其模型范围内，它提供了一个一致的API（类方法如`fit`、`partial_fit`、`predict`、`predict_proba`、`transform`），这些API可以快速学习和重用，并且专注于模型从数据中学习和预测所必需的转换和处理过程。Scikit-learn的API还提供了自动分离训练数据和测试数据的功能，能够在数据管道中链式使用其元素，并通过检查使用的类的公共属性来访问其参数。
- en: Initially working on NumPy arrays and sparse matrices, Scikit-learn later extended
    to pandas DataFrames, enabling the practitioner to use them as inputs. In later
    versions (since version 1.1.3), you can retain key DataFrame characteristics,
    such as the name of columns and the transformations operated by Scikit-learn functions
    and classes. The support recently provided by Scikit-learn for pandas DataFrames
    has been long yearned for and is indeed essential for the topic of our book, tabular
    data.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初Scikit-learn专注于NumPy数组和无序矩阵，后来扩展到了pandas DataFrame，使得实践者可以将它们作为输入使用。在后续版本中（自1.1.3版本起），您可以保留关键DataFrame特征，例如列名以及Scikit-learn函数和类所进行的转换。Scikit-learn最近为pandas
    DataFrame提供的支持一直备受期待，确实对于本书的主题——表格数据，是至关重要的。
- en: To define the working parameters of each Scikit-learn class, you just use standard
    Python types and classes (strings, floats, lists). In addition, the default values
    of all such parameters are already set to a proper value for you to create a baseline
    to start with and improve.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要定义每个Scikit-learn类的工作参数，您只需使用标准的Python类型和类（字符串、浮点数、列表）。此外，所有此类参数的默认值已经设置为适当的值，以便您可以从一个基线开始创建，并在此基础上进行改进。
- en: Thanks to a core group of top contributors (such as Andreas Mueller, Oliver
    Grisel, Fabian Pedregosa, Gael Varoquaux, and Gilles Loupe), Scikit-learn is in
    continuous development. There is constant debugging, and new functionalities and
    new models are added every time or old ones are excluded based on their robustness
    and scalability.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感谢一群顶尖的贡献者（如Andreas Mueller、Oliver Grisel、Fabian Pedregosa、Gael Varoquaux和Gilles
    Loupe），Scikit-learn一直在持续发展中。团队不断进行调试，并会根据算法的稳健性和可扩展性，定期添加新功能和新模型，或淘汰旧模型。
- en: The package also presents extensive and easily accessible documentation with
    examples you can consult online ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html))
    or offline using the `help()` command.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该包还提供了广泛且易于访问的文档，其中包含您可以在线咨询的示例（[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)）或使用`help()`命令离线查看。
- en: Depending on your operating system and installation preferences, if you want
    to install Scikit-learn, you just need to follow the instructions at [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html).
    Together with pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    Scikit-learn is the core library for tabular data analysis and modeling. It offers
    a vast range of machine learning and statistical algorithms exclusively for structured
    data; in fact, the input has to be a pandas Dataframe, a NumPy array, or a sparse
    matrix to choose from. These algorithms are all well-established because the Scikit-learn
    team decided to include any algorithm in the package based on “at least three
    years since publication, 200+ citations, and wide use and usefulness” criteria.
    For more details on the algorithm inclusion requirements in Scikit-learn, see
    [https://mng.bz/8OMw](https://mng.bz/8OMw).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统和安装偏好，如果您想安装Scikit-learn，只需遵循[https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html)上的说明。与pandas
    ([https://pandas.pydata.org/](https://pandas.pydata.org/))一起，Scikit-learn是表格数据分析与建模的核心库。它提供了一系列专门针对结构化数据的机器学习和统计算法；实际上，输入必须是一个pandas
    DataFrame、NumPy数组或无序矩阵。这些算法都得到了良好的建立，因为Scikit-learn团队决定根据“自发表以来至少三年、200+引用、广泛使用和实用性”的标准将任何算法包含在包中。有关Scikit-learn中算法包含要求的更多详细信息，请参阅[https://mng.bz/8OMw](https://mng.bz/8OMw)。
- en: 4.1.2 Common Scikit-learn interface
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 常见Scikit-learn接口
- en: The other key aspect of Scikit-learn that makes it so apt for tabular data problems
    is its current estimator API, the *fit, predict/transform* interface. Such an
    estimator API is not just limited to Scikit-learn, and it is widely recognized
    as the most effective approach to handling training and test data. Many other
    projects have adopted it (see [https://mng.bz/EaWO](https://mng.bz/EaWO)). In
    fact, following Scikit-learn API, you automatically incorporate all the best practices
    in your data science project. In particular, you strictly separate training from
    validation and test data, an indispensable step for the success of any tabular
    data modeling, as we will demonstrate in the next section by reprising the Airbnb
    NYC dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的另一个关键特性是它非常适合表格数据问题，那就是它当前的估计器API，即*fit, predict/transform*接口。这样的估计器API不仅限于Scikit-learn，而且被广泛认为是处理训练和测试数据最有效的方法。许多其他项目都采用了它（见[https://mng.bz/EaWO](https://mng.bz/EaWO)）。实际上，遵循Scikit-learn
    API，你自动将数据科学项目中所有最佳实践融入其中。特别是，你严格区分训练数据、验证数据和测试数据，这是任何表格数据建模成功不可或缺的一步，我们将在下一节通过重新介绍Airbnb
    NYC数据集来展示这一点。
- en: 'Before delving into more practical examples, we provide some basics about Scikit-learn
    estimators. First, we distinguish four kinds of objects in Scikit-learn, each
    with a different interface. One class can implement multiple objects at the same
    time. Estimators are just one of them, though they are the most important ones
    because most of the Scikit-learn classes are estimators. In the following example,
    we define a machine learning estimator, a logistic regression (to be later discussed
    in this same chapter) for classification using the LogisticRegression class offered
    by Scikit-learn:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨更多实际例子之前，我们提供一些关于Scikit-learn估计器的基础知识。首先，我们在Scikit-learn中区分四种类型的对象，每种对象都有不同的接口。一个类可以同时实现多个对象。估计器只是其中之一，尽管它们是最重要的，因为Scikit-learn的大多数类都是估计器。在下面的例子中，我们定义了一个机器学习估计器，即使用Scikit-learn提供的LogisticRegression类进行分类的逻辑回归（将在本章稍后讨论）：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An *estimator* is an object focused on learning from data using the .fit method.
    It can be applied to supervised learning, relating data to a target, or to unsupervised
    learning where only data is involved:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*估计器*是一个对象，它通过`.fit`方法从数据中学习。它可以应用于监督学习，将数据与目标相关联，或者应用于仅涉及数据的无监督学习：'
- en: 'For supervised learning: `estimator` = `estimator.fit(data, targets)`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习：`estimator` = `estimator.fit(data, targets)`
- en: 'For unsupervised learning: `estimator` = `estimator.fit(data)`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习：`estimator` = `estimator.fit(data)`
- en: Under the hood, an estimator uses data to estimate some parameters that serve
    for later mapping back data to predictions or transforming it. The parameters
    and other information collected in the process are made available as object attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，估计器使用数据来估计一些参数，这些参数用于后续将数据映射回预测或转换它。在过程中收集的参数和其他信息作为对象属性提供。
- en: 'Other typical Scikit-learn objects include the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的其他典型对象包括以下内容：
- en: '*Transformer* is an object focused on mapping a transformation on data:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换器*是一个对象，它专注于对数据进行转换：'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Predictor* is an object focussed on mapping a predicted response given some
    data by the methods `.predict` (predicting a general outcome) and `.predict_proba`
    (predicting a probability):'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测器*是一个对象，它通过`.predict`方法（预测一般结果）和`.predict_proba`方法（预测概率）来映射给定数据的一些预测响应：'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Model* is an object focused on providing the goodness of fit in respect of
    some data, typical of many statistical methods, by the method `.score`:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是一个对象，它通过`.score`方法提供一些数据的拟合优度，这在许多统计方法中很典型：'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Whether you need an estimator or a transformer, each class is always instantiated
    by assigning it to a variable and specifying its parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你需要估计器还是转换器，每个类都是通过将其分配给一个变量并指定其参数来实例化的。
- en: 'Under the hood, all these classes store parameters for their task. Some parameters
    are learned directly from the data and are commonly referred to as the weights
    or parameters of the models. You can think of these as the coefficients in a mathematical
    formulation: unknown values to be determined by data and computations. Others
    are given by the user at instantiation and can be configuration or initialization
    settings or parameters that influence how the algorithm learns from data. We usually
    refer to the latter ones as *hyperparameters*. They tend to differ depending on
    the machine learning model; hence, we will discuss the most important ones when
    explaining each algorithm.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，所有这些类都存储着它们任务的参数。一些参数直接从数据中学习，通常被称为模型的权重或参数。你可以将这些视为数学公式中的系数：由数据和计算确定的未知值。其他的是用户在实例化时提供的，可以是配置或初始化设置，或者影响算法如何从数据中学习的参数。我们通常将后者称为*超参数*。它们往往根据机器学习模型的不同而有所不同；因此，当解释每个算法时，我们将讨论最重要的那些。
- en: Configuration and setting parameters are similar for all the algorithms. For
    instance, the `random_state` setting helps to define a random seed for replicating
    the exact behavior of the model when using the same data. The results won’t change
    in different runs thanks to setting a random seed. The configuration parameter
    `n_jobs` will allow you to set how many CPU processors you want to be used in
    the computations, thus speeding up the time necessary for the model to complete
    its work but preventing you from doing other computer operations simultaneously.
    Depending on the algorithm, other available settings of the same kind may define
    the tolerance or the memory cache used by the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有算法的配置和设置参数都是相似的。例如，`random_state`设置有助于定义一个随机种子，以便在用相同的数据使用模型时复制模型的确切行为。由于设置了随机种子，结果在不同的运行中不会改变。配置参数`n_jobs`将允许你设置在计算中想要使用的CPU处理器数量，从而加快模型完成工作所需的时间，但同时也防止你同时进行其他计算机操作。根据算法的不同，其他可用的类似设置可能定义模型使用的容限或内存缓存。
- en: 'As we mentioned, some of these hyperparameters affect how the model operates
    and others how it learns from data. Let’s reprise our previous example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，这些超参数中的一些影响模型的操作方式，而另一些则影响模型从数据中学习的方式。让我们回顾一下我们之前的例子：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Among the hyperparameters that affect how the model learns from data, in our
    example, we can quote the C parameter, which, by taking different values, instructs
    the machine learning algorithm to apply some constraints in elaborating patterns
    from the data. We will address all the parameters to be fixed for each machine
    learning algorithm as we present them. It is important to notice that you usually
    set the hyperparameters at the time when the class is instantiated.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在影响模型从数据中学习过程的超参数中，在我们的例子中，我们可以引用C参数，该参数通过取不同的值，指导机器学习算法在从数据中提炼模式时应用一些约束。当我们介绍每个机器学习算法时，我们将解决每个算法需要固定的所有参数。重要的是要注意，你通常在实例化类的时候设置超参数。
- en: 'After the class instantiation, you usually provide the data to learn from and
    some limited instruction on how to deal with it—for instance, by giving different
    weights to each data example. At this stage, we say you train or fit the class
    on data. This phase is commonly mentioned as “fitting an estimator,” and it is
    done by providing data as a NumPy array, a sparse matrix, or a pandas DataFrame
    to the `.fit` method:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在类实例化之后，你通常提供用于学习的数据以及一些关于如何处理它的有限指令——例如，通过给每个数据实例赋予不同的权重。在这个阶段，我们说你在数据上训练或调整类的参数。这个阶段通常被称为“调整估计器”，是通过将数据作为NumPy数组、稀疏矩阵或pandas
    DataFrame传递给`.fit`方法来完成的：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since training a model requires mapping an answer to some data, the `.fit`
    method inputs the data matrix and the answer vector. Such behavior is more than
    typical to models because some other Scikit-learn classes input data. The `.fit`
    method is also common to all transformative classes in Scikit-learn. For instance,
    fitting just data is typical of all the classes dealing with preprocessing, as
    you can check at [https://mng.bz/N161](https://mng.bz/N161), because transformations
    also require learning some information from features. For example, if you need
    to standardize data, you must first learn the standard deviation and the mean
    of each numeric feature in the data. The Scikit-learn’s StandardScaler ([https://mng.bz/DMgw](https://mng.bz/DMgw))
    does exactly this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练模型需要将答案映射到某些数据上，`.fit`方法需要输入数据矩阵和答案向量。这种行为对于模型来说是典型的，因为Scikit-learn中的某些其他类也会输入数据。`.fit`方法在Scikit-learn中的所有转换类中也很常见。例如，仅对数据进行拟合是所有处理预处理的类的典型做法，正如你可以在[https://mng.bz/N161](https://mng.bz/N161)中检查的那样，因为转换也需要从特征中学习一些信息。例如，如果你需要标准化数据，你必须首先学习数据中每个数值特征的均值和标准差。Scikit-learn的StandardScaler([https://mng.bz/DMgw](https://mng.bz/DMgw))正是这样做的：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our example, we instantiate the class necessary for standardizing the data
    (StandardScaler), and we immediately afterward fit the data itself. Since the
    `.fit` method returns the instantiated class we used for the fitting procedure,
    you can safely get in return the class with all the learned parameters by combining
    these two steps. Such an approach will be helpful when building data pipelines
    and training models because it helps you separate the activities that learn something
    from data from the actions that apply what they learned to new data. This way,
    you won’t mistake mixing information from training and validation or test data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们实例化了用于标准化数据的类（StandardScaler），然后立即拟合数据本身。由于`.fit`方法返回用于拟合过程的实例化类，你可以通过结合这两个步骤安全地获取包含所有学习参数的类。这种做法在构建数据管道和训练模型时非常有用，因为它帮助你将学习数据中的某些活动与将所学内容应用于新数据的行为分开。这样，你就不会混淆训练、验证或测试数据中的信息。
- en: Depending on the complexity of the underlying operations and the quantity of
    provided data, fitting a model or a function processing data may take some time.
    After the fitting has been completed, many more attributes will become available
    for you to use afterward, depending on the algorithm you used.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据底层操作的复杂性和提供的数据量，拟合模型或处理数据的函数可能需要一些时间。拟合完成后，将会有更多属性可供你使用，具体取决于你使用的算法。
- en: 'For a trained model, you will obtain a vector of responses of predictions based
    on any new data by applying the `.predict` method. This will work both for a classification
    or a regression problem:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个训练好的模型，你可以通过应用`.predict`方法，基于任何新数据获得预测的响应向量。这既适用于分类问题，也适用于回归问题：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Suppose you are working on a classification; instead, you must get the probability
    that a certain class is a correct prediction for a new sample. In that case, you
    need to use the `.predict_proba` method, which is available only to certain models:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理一个分类任务；相反，你必须获取一个特定类别对新样本进行正确预测的概率。在这种情况下，你需要使用`.predict_proba`方法，这个方法仅适用于某些模型：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Classes that process data, instead, do not have a `.predict` method. Still,
    they use the `.transform` one, which returns transformed data if the class has
    been previously instantiated and fitted with some training data for learning the
    key parameters necessary for the transformation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据的类没有`.predict`方法。然而，它们使用`.transform`方法，如果该类已经通过一些训练数据实例化和拟合以学习转换所需的关键参数，它将返回转换后的数据：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since the transformation is often applied on the very same data that provided
    the key parameters, the `.fit_transform` method, which concatenates the two fit
    and transform phases, will result in a handy shortcut:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换通常应用于提供关键参数的相同数据，`.fit_transform`方法，它结合了拟合和转换两个阶段，将导致一个方便的快捷方式：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 4.1.3 Introduction to Scikit-learn pipelines
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 Scikit-learn管道简介
- en: You can also wrap a sequence of transformations and then predictions, selectively
    deciding what to transform and joining different sequences of transformations
    by using utility functions offered by Scikit-learn, such as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用Scikit-learn提供的实用函数，如将一系列转换和预测包装起来，选择性决定要转换的内容，并通过这些函数将不同的转换序列连接起来：
- en: '*Pipeline* ([https://mng.bz/lYx8](https://mng.bz/lYx8))'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管道* ([https://mng.bz/lYx8](https://mng.bz/lYx8))'
- en: '*ColumnTransformer* ([https://mng.bz/BXM8](https://mng.bz/BXM8))'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ColumnTransformer* ([https://mng.bz/BXM8](https://mng.bz/BXM8))'
- en: '*FeatureUnion* ([https://mng.bz/dX2O](https://mng.bz/dX2O))'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FeatureUnion* ([https://mng.bz/dX2O](https://mng.bz/dX2O))'
- en: The Pipeline command allows you to create a sequence of Scikit-learn classes
    that results in a series of transformations of the data, and it can end up with
    a model and its predictions. In this way, you can integrate any model with the
    transformations it requires for the data and deal with all the involved parameters
    at once—those of the transformations and those of the model itself. The Pipeline
    command is the core command to move tabular data from source to predictions in
    the Scikit-learn package. To set it, at instantiation time, you just need to provide
    a list of tuples, each containing the name of the step in the pipeline and the
    Scikit-learn class or model to be executed. Once instantiated, you can use it
    following the common API specifications of Scikit-learn (fit, transform/predict).
    The pipeline will execute all the predefined steps in sequence, returning the
    final result. Naturally, you can access, inspect, and tune the single steps of
    the pipeline sequence for better results and performance, but you can handle the
    pipeline as a single macro command.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline 命令允许你创建一系列 Scikit-learn 类，这些类将导致数据的一系列转换，并且最终可以结束于一个模型及其预测。通过这种方式，你可以将任何模型与其所需的数据转换集成在一起，并一次性处理所有相关参数——转换的参数和模型本身的参数。Pipeline
    命令是 Scikit-learn 包中将表格数据从源头移动到预测的核心命令。在实例化时设置它，你只需要提供一个包含步骤名称和要执行的 Scikit-learn
    类或模型的元组的列表。一旦实例化，你可以按照 Scikit-learn 的常见 API 规范（fit，transform/predict）使用它。管道将按顺序执行所有预定义的步骤，返回最终结果。当然，你可以访问、检查和调整管道序列的单个步骤以获得更好的结果和性能，但你也可以将管道作为一个单独的宏命令来处理。
- en: However, tabular columns may have different types and require quite different
    transformation sequences, or you may have devised two different ways to process
    your data that you would like to combine. ColumnTrasformer and FeatureUnion are
    Scikit-learn commands that can help you in such occurrences. ColumnTrasformer
    allows you to apply a certain transformation or pipeline of transformations only
    on certain columns (which you can define by their name or position index in the
    columns’ sequence). The command takes a list of tuples, as the Pipeline command,
    but it requires a name for the transformation, a Scikit-learn class for executing
    it, and a list of column names or indexes to which the transformation should be
    applied. Since it is just a transformative command, its ideal usage is inside
    a pipeline, where its transformations can be part of the data feeding of a model.
    FeatureUnion, instead, is just an easy way to concatenate the results of two distinct
    pipelines. You may achieve the same result with a simple NumPy command such as
    `np.hstack` ([https://mng.bz/rKJD](https://mng.bz/rKJD)). However, when using
    FeatureUnion you have the advantage that the command can fit into a Scikit-learn
    pipeline and hence automatically be used as part of the data feeding to the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，表格列可能具有不同的类型，需要相当不同的转换序列，或者你可能已经设计了两种不同的数据处理方式，你希望将它们结合起来。ColumnTransformer
    和 FeatureUnion 是 Scikit-learn 命令，可以帮助你在这种情况下。ColumnTransformer 允许你仅对某些列（你可以通过它们的名称或列序列中的位置索引来定义）应用特定的转换或转换序列。该命令接受一个元组的列表，就像
    Pipeline 命令一样，但它需要一个转换的名称，一个执行它的 Scikit-learn 类，以及一个要应用转换的列名称或索引列表。由于它只是一个转换命令，其理想的使用方式是在管道内部，其中其转换可以是模型数据输入的一部分。FeatureUnion，相反，只是将两个不同管道的结果连接起来的简单方法。你可以使用简单的
    NumPy 命令，如 `np.hstack` ([https://mng.bz/rKJD](https://mng.bz/rKJD)) 来实现相同的结果。然而，当使用
    FeatureUnion 时，你有优势，即该命令可以适应 Scikit-learn 管道，因此可以自动作为模型数据输入的一部分使用。
- en: The modularity of operations and API consistency offered by Scikit-learn and
    its Pipeline, ColumnTrasformer, and FeatureUnion will allow you to easily create
    complex data transformations to be handled as a single command, thus making your
    code highly readable, compact, and easily maintainable. In the next section, we
    will return to the Airbnb NYC dataset we used. We will create a series of transforming
    sequences in Scikit-learn that will allow us to demonstrate how Scikit-learn and
    its pipeline functions are the right choices for tackling your tabular data problems.
    We will also point out how easily you can switch between the different options
    for machine learning with tabular data thanks to a well-defined pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn及其Pipeline、ColumnTransformer和FeatureUnion提供的操作模块化和API一致性将允许您轻松创建复杂的数据转换，将其作为一个单独的命令处理，从而使您的代码高度可读、紧凑且易于维护。在下一节中，我们将回到我们使用的Airbnb纽约市数据集。我们将创建一系列Scikit-learn中的转换序列，以展示Scikit-learn及其管道函数是如何正确处理您的表格数据问题的。我们还将指出，通过一个定义良好的管道，您如何轻松地在表格数据的机器学习不同选项之间切换。
- en: 4.2 Exploring and processing features of the Airbnb NYC dataset
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 探索和处理Airbnb纽约市数据集的特征
- en: The previously introduced Airbnb NYC dataset is a perfect example for demonstrative
    purposes because it is a dataset representative of a real-world problem and because
    of its various types of columns. We will have to create and combine different
    pipelines to handle the different features, and the following chapters will give
    us a chance to present even more advanced processing techniques than the ones
    you can find in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 之前介绍的Airbnb纽约市数据集是演示目的的一个完美例子，因为它是一个代表现实世界问题的数据集，并且由于其各种类型的列。我们将不得不创建和组合不同的管道来处理不同的特征，接下来的章节将给我们一个机会来展示比本章中可以找到的更高级的处理技术。
- en: 'For the moment, we will place the features we will deal with into a list named
    `excluding_list`. They are features, such as the latitude and longitude degrees
    or the data of the last review (`last_review`), which need special ad hoc processing.
    Also, the dataset presents a few possible columns that may act as targets: the
    price, the availability of the listed properties (`availability_365`), and the
    number of reviews (`number_of_reviews`). For our purposes, we prefer to use the
    price. Because it is a continuous set of values above zero, we can immediately
    use it as a regression target. In addition, by applying a split on the mean or
    the median, or binning the values into deciles, we can quickly turn the price
    variable into a binary or multiclass classification target. Apart from price,
    we use all the other features as predictive ones or for more advanced feature
    engineering.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将我们将要处理的特征放入一个名为`excluding_list`的列表中。它们是需要特殊处理的特征，例如纬度和经度度数或最后评论的日期（`last_review`）。此外，数据集还展示了一些可能作为目标的列：价格、列出物业的可用性（`availability_365`）和评论数量（`number_of_reviews`）。就我们的目的而言，我们更倾向于使用价格。因为它是一个大于零的连续值集合，我们可以立即将其用作回归目标。此外，通过在平均值或中位数上进行拆分，或将值分箱到十分位，我们可以快速将价格变量转换为二元或多类分类目标。除了价格之外，我们使用所有其他特征作为预测特征或进行更高级的特征工程。
- en: In the following subsection, we will demonstrate a step-by-step approach to
    exploring the dataset, filtering the dataset based on the useful columns, and
    setting up our target variables. In principle, we will follow the hints and examples
    provided in chapter 2 when discussing *exploratory data analysis* (EDA). In the
    next section, we will take advantage of our discoveries and prepare suitable data
    pipelines that will be reused in the following paragraphs when revising the different
    options for machine learning for tabular data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将展示一种逐步探索数据集的方法，基于有用的列过滤数据集，并设置我们的目标变量。原则上，我们将遵循第2章中讨论*探索性数据分析*（EDA）时提供的提示和示例。在下一节中，我们将利用我们的发现，准备合适的数据管道，这些管道将在以下段落中修订表格数据的机器学习不同选项时被重用。
- en: 4.2.1 Dataset exploration
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 数据集探索
- en: As a first step in exploring the dataset, we import the relevant packages (NumPy
    and pandas), define the list of excluded features as well as separate lists for
    categorical and continuous features based on our prior knowledge built in the
    previous chapter, and load the data from our current working directory. The code
    to be executed is
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据集的第一步中，我们导入相关的包（NumPy和pandas），定义要排除的特征列表，以及基于我们在上一章中建立的知识，为分类和连续特征定义单独的列表，并从当前工作目录加载数据。要执行的代码是
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① List of column names to be excluded from the analysis
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ① 列出要排除在分析之外的列名
- en: ② List of names of columns that likely represent categorical variables in the
    dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ② 列出数据集中可能代表分类变量的列名
- en: ③ List of names of columns that represent continuous numerical variables in
    the dataset
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 列出数据集中代表连续数值变量的列名
- en: 'Once the code snippet has completed the loading of the data, we first check
    how many rows and columns have been returned in the data frame:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码片段完成数据的加载，我们首先检查数据框中返回了多少行和列：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will get 48,895 rows available—a fair number for a tabular problem, allowing
    us to use any available learning algorithm—and 16 columns. Since we are interested
    only in some of the columns—the ones we defined in the variables named categorical
    and continuous—we start by refining the classification of categorical features
    into low cardinality and high cardinality ones based on the number of unique values
    they have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得48,895行可用——对于一个表格问题来说是一个合理的数量，允许我们使用任何可用的学习算法——以及16列。由于我们只对一些列感兴趣——那些我们在名为分类和连续变量的变量中定义的列——我们首先根据它们具有的唯一值数量对分类特征进行低基数和高基数的分类：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The command results in the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 命令产生了以下输出：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our standard approach when dealing with categorical features is to apply *one-hot
    encoding*, creating one binary variable for each unique value in the original
    feature. However, by using one-hot encoding, features presenting over 20 unique
    values will result in an excessive number of columns in the dataset and data sparsity.
    You have sparsity in your data when your data is predominantly of zero values,
    which is a problem, especially for neural networks and generally for online algorithms
    because learning becomes more difficult. In chapter 6, we will present techniques,
    such as target encoding, to deal with features with too many unique values, called
    *high cardinality categorical features*. For the examples in this chapter, we
    will separate the low from the high cardinality categorical features and process
    only the low cardinality ones:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理分类特征的标准方法是对其应用**独热编码**，为原始特征中的每个唯一值创建一个二进制变量。然而，使用独热编码，具有超过20个唯一值的特征会导致数据集中列数过多和数据稀疏。当你的数据主要是零值时，你就有数据稀疏的问题，这是一个问题，特别是对于神经网络和通常对于在线算法来说，因为学习变得更加困难。在第6章中，我们将介绍处理具有过多唯一值特征的技术，这些特征被称为**高基数分类特征**。对于本章的示例，我们将区分低基数和高基数分类特征，并仅处理低基数特征：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, having defined that (for the moment, we will be working only with numeric
    and low cardinality categorical features), we need to figure out if there are
    any missing cases in our data. The following command asks to flag true missing
    values and then computes a count of them across features:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在定义了这一点（目前，我们将只使用数值和低基数分类特征）之后，我们需要弄清楚我们的数据中是否存在任何缺失情况。以下命令要求标记真实缺失值，然后计算它们在特征中的数量：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We obtain the following result that points out a problem only with the `reviews_per_month`
    feature:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果，指出只有`reviews_per_month`特征存在问题：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we mentioned in chapter 2, dealing with missing values shouldn’t be an automated
    procedure; rather, it requires some reflection on the data scientist’s part to
    determine if there is some reason for them to be missing. In this case, it becomes
    evident that there is a processing problem with the data at the source because
    if you check the minimum value, this will result in a value above zero:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第2章中提到的，处理缺失值不应该是一个自动化的过程；相反，它需要数据科学家对数据进行一些反思，以确定是否存在某些原因导致它们缺失。在这种情况下，很明显，数据源存在处理问题，因为如果你检查最小值，这将导致一个大于零的值：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The minimum reported is 0.01\. Here we have a missing value when there are
    not enough reviews to make statistics. Hence, we could replace the missing value
    on this feature with a zero value. Having filtered our features to be used for
    predictions and having checked missing values because most machine learning algorithms
    won’t work in the presence of missing input data, apart from a few such as the
    gradient boosting implementations XGBoost or LightGBM (discussed in the next chapter),
    we can proceed to check about our target. This part of EDA, *target analysis*,
    is often overlooked, yet it is quite important because, in tabular problems, not
    all machine learning algorithms can handle the same kind of targets. For example,
    targets with many zeros, fat tails, and multiple mode values are difficult for
    certain models and result in your model underfitting. Let’s start by checking
    the distribution of the price feature. A histogram, plotting the frequency of
    values falling into ranges of values (called bins), is particularly helpful in
    figuring out how your data distributes. For instance, a histogram can tell you
    if your data resembles a known distribution, such as the normal distribution,
    or highlight at around what values there are peaks and where the data is denser
    (see figure 4.1). If you are working with a pandas DataFrame, the plot can be
    made just by calling the `hist` method that depicts data distribution by plotting
    the frequency of values falling into ranges of values (bins):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的最小值是0.01。在这里，当没有足够的评论来制作统计数据时，我们会遇到一个缺失值。因此，我们可以用零值来替换这个特征的缺失值。在过滤了用于预测的特征并检查了缺失值之后，因为大多数机器学习算法在存在缺失输入数据的情况下无法工作，除了少数如梯度提升实现XGBoost或LightGBM（将在下一章讨论）之外，我们可以继续检查我们的目标。EDA的这一部分，即*目标分析*，通常被忽视，但它是相当重要的，因为在表格问题中，并非所有机器学习算法都能处理相同类型的目标。例如，具有许多零值、厚尾和多个众数的目标对于某些模型来说很难处理，并导致你的模型欠拟合。让我们先检查价格特征的分布。直方图，即绘制值落在值范围（称为箱）中的频率，对于了解你的数据如何分布特别有帮助。例如，直方图可以告诉你你的数据是否类似于已知的分布，如正态分布，或者突出显示在哪些值附近有峰值以及数据在哪里更密集（见图4.1）。如果你使用的是pandas
    DataFrame，可以通过调用`hist`方法来绘制直方图，该方法通过绘制值落在值范围（箱）中的频率来描述数据分布：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/CH04_F01_Ryan2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F01_Ryan2.png)'
- en: Figure 4.1 A histogram describing how the Price feature is distributed
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 描述价格特征分布的直方图
- en: 'The distribution shown in figure 4.1 is extremely skewed to the right, with
    many outlying values because the plotted values range to 10,000\. However, just
    before 2,000, it is hard to distinguish any bar depicting frequencies. This becomes
    even more evident by plotting a boxplot, which is a very useful tool when one
    wants to visualize where the core part of the distribution of a variable lies.
    A *boxplot* for a variable is a plot where the key measurements of the distribution
    are depicted as a box with “whiskers”: two lines outside the box that stretch
    to the expected limits of the variables’ distribution. The box is delimited by
    the interquartile range (IQR), determined by the 25th and 75th percentiles, and
    split into two by the line of the median. The whiskers stretch up and down to
    values 1.5 times the IQR. Everything above or below the whiskers’ edges is considered
    an *outlier*: an unusual or unexpected value. Let’s plot a boxplot for the price
    variable, again using a built-in method in pandas DataFrame, the boxplot method
    (see figure 4.2):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1中显示的分布极度向右倾斜，因为有许多异常值，因为绘制的值范围达到10,000。然而，在2,000之前，很难区分任何表示频率的条形。通过绘制箱线图，这一点变得更加明显，箱线图是当一个人想要可视化一个变量的分布的核心部分所在位置时非常有用的工具。一个变量的*箱线图*是一个图表，其中分布的关键测量值被描绘为一个带有“胡须”的箱：两条延伸到变量分布预期极限之外的线条。箱由四分位数范围（IQR）界定，由第25和第75百分位数确定，并由中位数线分为两部分。胡须向上向下延伸到IQR的1.5倍。胡须边缘之上或之下的一切都被认为是*异常值*：一个不寻常或意外的值。让我们再次使用pandas
    DataFrame中内置的方法，即箱线图方法，绘制价格变量的箱线图（见图4.2）：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/CH04_F02_Ryan2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F02_Ryan2.png)'
- en: Figure 4.2 A boxplot highlighting the distribution of the Price feature and
    its right heavy tail on large price values
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 一个箱线图，突出了价格特征的分布及其在大价格值上的右重尾
- en: 'Not surprisingly, the box and whiskers are squeezed in the lower part of the
    chart and are almost indistinguishable from each other. A long queue of outliers
    elongates from the upper limit of the upper extremity of the boxplot. This is
    an evident case of a right-skewed distribution. In such cases, a standard solution
    to remediate the variable is to transform the target using a logarithm transformation.
    It is common practice to add a constant to offset the values into the positive
    number field for handling values of zero and below. In our case, it is unnecessary,
    since all the values are positive and above zero. In the following code snippet,
    we represent the transformed price feature by application of a logarithmic transformation
    (see figures 4.3 and 4.4):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，箱线和须线在图表的下半部分被挤压，几乎无法区分。箱线图上部的上限处延伸出一个长队列的异常值。这是一个明显的右偏分布案例。在这种情况下，一个标准的解决方案是使用对数变换来转换目标变量。通常的做法是添加一个常数以将值偏移到正数域，以处理零和以下的值。在我们的案例中，这是不必要的，因为所有值都是正数且大于零。在下面的代码片段中，我们通过应用对数变换来表示变换后的价格特征（参见图4.3和图4.4）：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH04_F03_Ryan2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F03_Ryan2.png)'
- en: Figure 4.3 A histogram of the Price feature being more symmetrical after log
    transformation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 对数变换后的价格特征的直方图，显示出更加对称
- en: '![](../Images/CH04_F04_Ryan2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F04_Ryan2.png)'
- en: Figure 4.4 A boxplot of the Price feature after log transformation signaling
    the persistence of extreme values at both the tails of the distribution
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 对数变换后的价格特征的箱线图，显示出分布两端的极端值持续存在
- en: 'Now the distribution, represented both by the new histogram and the boxplot,
    is more symmetric, though it is evident that there are outlying observations on
    both sides of the distribution. Since our aim is illustrative, we can ignore the
    original distribution and focus on a meaningful target representation. For instance,
    we can keep only the price values below 1,000 (see figure 4.5). In the following
    code snippet, we produce a histogram focused only on price values below 1,000:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，分布，由新的直方图和箱线图表示，更加对称，尽管很明显分布两侧都有异常观测值。由于我们的目标是说明性的，我们可以忽略原始分布，而专注于有意义的目标表示。例如，我们可以只保留低于1,000的价格值（参见图4.5）。在下面的代码片段中，我们生成一个仅关注低于1,000的价格值的直方图：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/CH04_F05_Ryan2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F05_Ryan2.png)'
- en: Figure 4.5 A histogram of the Price feature for values under 1,000 still showing
    a right-skewed long tail
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 对于低于1,000的价格特征的直方图，仍然显示出右偏的长尾
- en: 'Here the represented distribution is still right-skewed, but it resembles more
    common distributions found in e-commerce or other sales with long-tail products.
    In addition, if we focus on the range between 50 and 200, the distribution will
    appear more uniform (see figure 4.6). In the following code snippet, we restrict
    our focus further only to prices between 50 and 200 and plot the relative histogram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里表示的分布仍然是右偏的，但它更类似于在电子商务或其他长尾产品销售中常见的分布。此外，如果我们关注50到200的范围，分布将看起来更加均匀（参见图4.6）。在下面的代码片段中，我们进一步将焦点仅限于50到200之间的价格，并绘制相应的直方图：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH04_F06_Ryan2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F06_Ryan2.png)'
- en: Figure 4.6 A histogram of the Price feature for values between 50 and 200, showing
    distributed values across the range
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 对于50到200之间的价格特征的直方图，显示出在整个范围内的分布值
- en: 'Therefore, we can create two masking variables, made of booleans, that can
    help us filter the target according to the type of algorithm we would like to
    test. The `price_capped` variable will be instrumental when demonstrating how
    certain machine learning algorithms can handle long tails easily:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以创建两个由布尔值组成的掩码变量，这可以帮助我们根据我们想要测试的算法类型来过滤目标。`price_capped`变量在演示某些机器学习算法如何轻松处理长尾时将非常有用：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Figure 4.7 shows the boxplot relative to the capped price, which presents right-sided
    outliers, but at least the boxplot is visible.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 显示了相对于上限价格箱线图，呈现右侧异常值，但至少箱线图是可见的。
- en: '![](../Images/CH04_F07_Ryan2.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F07_Ryan2.png)'
- en: Figure 4.7 A boxplot of the Price feature for values under 1,000 showing a long
    tail of extreme values in its right tail
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 对于低于1,000的价格特征的箱线图，其右尾显示出极端值的长尾
- en: 'Figure 4.8 shows the boxplot relative to the windowed price, showing no sign
    of outliers:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 显示了相对于窗口价格箱线图，没有显示出异常值：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/CH04_F08_Ryan2.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F08_Ryan2.png)'
- en: Figure 4.8 A boxplot of the Price feature for values between 50 and 200 showing
    a slightly right-skewed distribution with no extreme values
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 展示了价格特征的箱线图，对于 50 到 200 之间的值，显示出略微右偏的分布，没有极端值
- en: 'After completing our exploration of the predictors and the target, we are ready
    to prepare four different targets that will be used along with our examples:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成我们对预测因子和目标以及一些基本特征选择的探索之后，我们准备四个不同的目标，这些目标将与我们的一些示例一起使用：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We prepared two binary targets, `target_mean` and `target_median`, and a multiclass
    target with five distinct classes based on percentiles for classification purposes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备了两个二元目标，`target_mean` 和 `target_median`，以及一个基于百分位数的多类目标，有五个不同的类别，用于分类目的。
- en: 'In particular, it is important to notice that our `target_median` is a binary
    balanced target. Hence, we can safely use accuracy as a good performance measurement.
    As a test, you get an almost equal number of cases for the positive and negative
    classes if you try to count the values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，重要的是要注意我们的 `target_median` 是一个二元平衡目标。因此，我们可以安全地使用准确率作为良好的性能度量。作为测试，如果你尝试计数值，你会得到正负类别的案例几乎相等：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You get the result
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到结果
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Instead, if you try doing the same on the `target_mean` target variable, you
    get
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果你尝试在 `target_mean` 目标变量上做同样的事情，你会得到
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You will obtain a distribution that is imbalanced toward the negative cases;
    that is, there are more cases below the mean because of the skewed distribution
    we previously observed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获得一个不平衡的分布，倾向于负案例；也就是说，由于我们之前观察到的偏斜分布，均值以下的案例更多：
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In such a case, when evaluating the results of a machine learning classifier,
    we prefer to use metrics such as the Receiver Operating Characteristic Area Under
    the Curve (ROC-AUC) or Average Precision—both quite sensible for ordering. Finally,
    as for the multiclass target, counting the cases for each one of the five classes
    reveals that they are also balanced in distribution:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当评估机器学习分类器的结果时，我们更倾向于使用如接收者操作特征曲线下面积（ROC-AUC）或平均精度等指标——两者都非常适合排序。最后，对于多类目标，计算五个类别中的每一个案例的数量显示，它们的分布也是平衡的：
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command returns the result of
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回以下结果
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As for the regression target, `target_regression` is the original target without
    transformations. However, we will use subsets of it and accordingly transform
    them based on the machine learning algorithm we will demonstrate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归目标，`target_regression` 是未经变换的原始目标。然而，我们将使用它的子集，并根据我们将要展示的机器学习算法相应地进行变换。
- en: Having completed our exploration of the data, the target, and some basic feature
    selection in the next paragraph, using a building blocks approach, we will prepare
    a few pipelines to accompany our discovery of different machine learning options
    for tabular data problems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成我们对数据、目标和下一段中的一些基本特征选择的探索之后，我们将使用积木方法准备几个管道，以伴随我们对表格数据问题不同机器学习选项的发现。
- en: 4.2.2 Pipelines preparation
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 管道准备
- en: We will use the previously seen Pipeline and ColumnTransformer classes from
    Scikit-learn to prepare the pipelines. In a building blocks approach, we first
    create the different operations to be applied to the other data types that characterize
    features in a tabular dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前看到的 Scikit-learn 中的 Pipeline 和 ColumnTransformer 类来准备管道。在积木方法中，我们首先创建应用于其他数据类型的不同操作，这些数据类型表征了表格数据集中的特征。
- en: 'The following code defines three core procedures that will be reused multiple
    times in this chapter:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了三个核心过程，这些过程将在本章中多次重复使用：
- en: '*Categorical one-hot encoding*—Categorical features are transformed into binary
    ones. If a value has never been seen before, it will be ignored.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类独热编码*——分类特征被转换为二进制形式。如果一个值之前从未见过，它将被忽略。'
- en: '*Numeric pass-through*—Numeric features are imputed using zero as a value.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数值直接通过*——使用零作为值来填充数值特征。'
- en: '*Numeric standardization*—After imputing missing values, numeric features are
    rescaled by subtracting their mean and dividing them by their standard deviation'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数值标准化*——在填充缺失值后，数值特征通过减去它们的平均值并除以它们的标准差进行缩放'
- en: The code defining these procedures is shown in the following listing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 定义这些过程的代码如下所示。
- en: Listing 4.2 Setting up building blocks for tabular learning pipelines
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 设置表格学习管道的积木
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① Converts categorical features into one-hot encoded format
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将分类特征转换为独热编码格式
- en: ② Replaces missing numeric values with zero
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ② 用零替换缺失的数值
- en: ③ Pipeline replaces missing numeric values with zero and standardizes the features
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 管道用零替换缺失的数值并标准化特征
- en: 'At this point, we can compose specific transformation pipelines that handle
    the data according to our needs for each machine learning algorithm. For instance,
    in this example, we set a pipeline that will one-hot encode low categorical features
    and just impute missing values as zero for numeric ones. Such a pipeline is made
    by the ColumnTransformer function, a glue function that combines operations applied
    on different sets of features simultaneously. This is an excellent transformative
    strategy suitable for most machine learning models:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以根据每个机器学习算法的需求，编写特定的转换管道来处理数据。例如，在这个例子中，我们设置了一个管道，该管道将低类别特征进行独热编码，并将数值特征的缺失值简单地填充为零。这样的管道是通过ColumnTransformer函数创建的，这是一个粘合函数，它同时结合了对不同特征集应用的操作。这是一种非常适合大多数机器学习模型的优秀转换策略：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '① First step of the pipeline: one-hot encoding categorical features'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ① 管道的第一步：独热编码类别特征
- en: '② Second step of the pipeline: handling numeric features'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ② 管道的第二步：处理数值特征
- en: ③ The features not processed by the pipeline are dropped from the result.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 管道未处理的特征将从结果中删除。
- en: ④ Names of the features are kept as they originally are.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 特征的名称保持与原始名称相同。
- en: ⑤ The result is always a dense matrix (i.e., a NumPy array)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 结果始终是一个密集矩阵（即NumPy数组）
- en: 'We can immediately run this code snippet and check how this pipeline transforms
    our Airbnb NYC data:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即运行此代码片段并检查该管道如何转换我们的Airbnb NYC数据：
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The result is that the output is now a NumPy array made of floats and that
    the shape has increased to 13 columns. In fact, because of one-hot encoding, each
    value in the categorical features has turned into a separate feature:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是输出现在是一个由浮点数组成的NumPy数组，其形状增加到13列。实际上，由于独热编码，类别特征中的每个值都变成了一个单独的特征：
- en: '[PRE38]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The following section will explore the main machine learning techniques for
    tabular data. Each will be accompanied by its column transforming class, which
    will be integrated into the pipeline containing the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨表格数据的主要机器学习技术。每个技术都将伴随其列转换类，这些类将被集成到包含模型的管道中。
- en: 4.3 Classical machine learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 经典机器学习
- en: 'To explain the different models from the classic machine learning techniques
    for tabular data, we will first introduce the core characteristics of the algorithm,
    and then demonstrate a code snippet, seeing it at work on our reference tabular
    problem, the Airbnb NYC dataset. The following are some best practices that we
    will use in our examples to allow reproducibility and comparability of the different
    approaches:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释经典机器学习技术对于表格数据的差异，我们首先将介绍算法的核心特性，然后通过一个代码片段展示其工作，在参考的表格问题上，即Airbnb NYC数据集上运行。以下是我们将在示例中使用的一些最佳实践，以允许不同方法的可重复性和可比性：
- en: We define a pipeline incorporating both data transformation and modeling.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个包含数据转换和建模的管道。
- en: We set an error measure, such as root mean squared error (RMSE) for regression
    or accuracy for classification, and measure it using the same cross-validation
    strategy.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设置一个错误度量，例如回归的均方根误差（RMSE）或分类的准确度，并使用相同的交叉验证策略进行测量。
- en: We report the average and standard deviation—crucial to figure out if the model
    has a constant performance across different data samples—of the cross-validated
    estimate of the error.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们报告交叉验证估计误差的平均值和标准差——这对于确定模型在不同数据样本上是否具有恒定的性能至关重要。
- en: In the previous section, we introduced the different tools Scikit-learn offers
    for building data pipelines integrating feature processing and machine learning
    models. In this section, we will introduce the recommended evaluation measures
    and how the cross-validation estimate by the Scikit-learn `cross_validate` command
    works.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了Scikit-learn提供的不同工具，用于构建集成特征处理和机器学习模型的管道。在本节中，我们将介绍推荐的评估指标以及Scikit-learn
    `cross_validate`命令如何进行交叉验证估计。
- en: Let’s review *evaluation metrics* first. We decided to use RMSE, a common measure
    for regression tasks, and accuracy, another standard measure for balanced binary
    and multiclass classification problems when the classes have approximately the
    same sample sizes. In subsequent chapters, we will also use metrics suitable for
    unbalanced classification problems, such as ROC-AUC and average precision.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回顾一下*评估指标*。我们决定使用RMSE，这是回归任务中常用的一个度量，以及准确率，这是当类别具有大致相同的样本大小时，平衡二元和多类分类问题的另一个标准度量。在随后的章节中，我们还将使用适合不平衡分类问题的度量，例如ROC-AUC和平均精度。
- en: '*Cross-validation* is the de facto standard in data science when you intend
    to estimate the expected performance of a machine learning model on any data different
    from the training data but drawn from the same data distribution. It is important
    to note that cross-validation estimates the future performance of your model based
    on the idea that your data may change in the future but won’t be radically different.
    To work correctly, the model expects that you will use the same features in the
    future and that they will have the same unique values (if a categorical feature)
    with similar distributions (both for categorical and numeric features) and, most
    importantly, that features will be in the same relation with your target variable.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证*是数据科学中当您打算估计机器学习模型在除训练数据外，从相同数据分布中抽取的任何数据上的预期性能时的实际标准。需要注意的是，交叉验证基于这样的想法，即您的数据可能会在未来发生变化，但不会发生根本性的变化，来估计您模型未来的性能。为了正确工作，模型期望您未来将使用相同的特征，并且它们将具有相同的唯一值（如果是一个分类特征），具有相似的分布（对于分类和数值特征），最重要的是，特征将与您的目标变量保持相同的关系。'
- en: The assumption that data distributions will remain consistent in the future
    is frequently not true because economic dynamics, consumer markets, and social
    and political situations change rapidly in the real world. In the real world,
    your model may experience concept drifting, when the modeled relationships between
    features and targets no longer represent reality. Hence, your model will underperform
    when dealing with new data. Cross-validation is the best tool to evaluate your
    models at the time of their creation because it is based on your available information
    at that moment and because, if well designed, it is not influenced by the ability
    of your machine learning model to overfit the training data. Its usefulness stays
    true even after cross-validated results are disproved compared to future performances,
    usually because the underlying data distribution has changed. In addition, alternative
    methods, such as leave-one-out or bootstrapping, offer better estimates with increasing
    computational costs, whereas more straightforward methods, such as train/test
    split, are less reliable in their estimates.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据分布在未来将保持一致这一假设通常并不成立，因为在现实世界中，经济动态、消费者市场以及社会和政治情况变化非常快。在现实世界中，您的模型可能会遇到概念漂移，即特征与目标之间的建模关系不再代表现实。因此，当处理新数据时，您的模型将表现不佳。交叉验证是评估模型在创建时的最佳工具，因为它基于您当时可用的信息，并且因为，如果设计得当，它不会受到您的机器学习模型过度拟合训练数据的能力的影响。即使与未来的性能相比，交叉验证的结果被证明是错误的，其有用性仍然成立，通常是因为潜在的数据分布已经发生变化。此外，如留一法或自助法等替代方法，随着计算成本的提高，提供了更好的估计，而如训练/测试集分割等更直接的方法在估计上则不太可靠。
- en: 'In its most uncomplicated flavor, the *k-fold cross-validation* (implemented
    in Scikit-learn with the KFold function: [https://mng.bz/VVM0](https://mng.bz/VVM0))
    is based on the splitting of your available training data into k partitions and
    the building of k versions of your model fed each time by different sets of k-1
    partitions and then tested on the remaining left out partition (the out-of-sample
    performance). The average and standard deviation of the resulting k scores will
    provide an estimate and a quantification of its uncertainty level to be used as
    a model estimate for expected performance on future unseen data. Figure 4.9 illustrates
    the k-fold validation when k is set to 5: each row represents the data partitioning
    at each fold. The validation part of a fold is always distinct from the others,
    and the training part is always differently composed.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，*k 折交叉验证*（在 Scikit-learn 中通过 KFold 函数实现：[https://mng.bz/VVM0](https://mng.bz/VVM0)）基于将你的可用训练数据分成
    k 个分区，并构建 k 个版本的你的模型，每次由不同的 k-1 个分区提供数据，然后在剩下的一个未使用的分区上进行测试（样本外性能）。k 个分数的平均值和标准差将提供一个估计及其不确定性水平的量化，用作对未来未见数据的预期性能的模型估计。图
    4.9 展示了当 k 设置为 5 时的 k 折验证：每一行代表每个折叠中的数据分区。一个折叠的验证部分总是与其他的不同，而训练部分总是不同地组成。
- en: '![](../Images/CH04_F09_Ryan2.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F09_Ryan2.png)'
- en: Figure 4.9 How data is distributed between train and validation across the folds
    of a five-fold cross-validation strategy
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 五折交叉验证策略中数据在训练和验证之间的分布
- en: Setting the correct value to k is a matter of how much training data you have
    available, how computationally costly it is to train your model on it, how the
    sample you received catches all the possible variations of the data distribution
    you want to model, and for what purpose you intend to get a performance estimate.
    As a general rule of thumb, values of k such as 5 or 10 are optimal choices, with
    k = 10 being more suitable for precise performance evaluation and k = 5 a good
    value compromising precision and computation costs for activities such as model,
    features, and hyperparameters evaluation (hence it will be used for our examples).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 k 的正确值取决于你有多少可用训练数据，在它上训练模型需要多少计算成本，你收到的样本是否捕捉到了你想要建模的数据分布的所有可能变化，以及你打算为什么目的获取性能估计。作为一个一般性的经验法则，k
    的值如 5 或 10 是最佳选择，其中 k = 10 更适合精确的性能评估，而 k = 5 是一个在模型、特征和超参数评估等活动中的精度和计算成本之间取得良好平衡的好值（因此它将用于我们的示例）。
- en: 'To get a general performance estimation for your model, you can build the necessary
    cross-validation iterations using a series of iterations on the KFold function
    (or its variations, offering sample stratification or control on the time dimension:
    [https://mng.bz/xKne](https://mng.bz/xKne)) or rely on the `cross_validate` procedure
    ([https://mng.bz/AQyK](https://mng.bz/AQyK)) that will handle everything for you
    and just return the results. For our purposes of testing different algorithms,
    `cross_validate` is quite handy because, given the proper parameters, it will
    produce a series of metrics:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对你的模型进行一般性能估计，你可以通过在 KFold 函数（或其变体，提供样本分层或对时间维度的控制：[https://mng.bz/xKne](https://mng.bz/xKne)）上进行一系列迭代来构建必要的交叉验证迭代，或者依赖
    `cross_validate` 过程（[https://mng.bz/AQyK](https://mng.bz/AQyK)），它会为你处理所有事情并只返回结果。对于测试不同算法的目的，`cross_validate`
    非常方便，因为，给定适当的参数，它将生成一系列指标：
- en: Cross-validation test scores (out-of-sample performance)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证测试分数（样本外性能）
- en: Cross-validation train scores (in-sample performance)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证训练分数（样本内性能）
- en: Fit time and predict time (to evaluate the computational cost)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应时间和预测时间（用于评估计算成本）
- en: The trained estimators on the different cross-validation folds
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同交叉验证折叠上的训练估计器
- en: All we have to do is provide an estimator, which can be any Scikit-learn object
    with a fit method, predictors and target, a cross-validation strategy, and a single
    or multiple scoring functions in a list. This estimator should be provided in
    the form of a callable to be created using the `make_scorer` command ([https://mng.bz/ZlAO](https://mng.bz/ZlAO)).
    In the next section, we will start seeing how we can get cross-validated performance
    estimates using such inputs, starting with classical machine learning algorithms
    such as linear regression and logistic regression.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是提供一个估计器，它可以是一个具有fit方法的Scikit-learn对象，包括预测器和目标，交叉验证策略，以及列表中的单个或多个评分函数。这个估计器应以可调用的形式提供，使用`make_scorer`命令创建（[https://mng.bz/ZlAO](https://mng.bz/ZlAO)）。在下一节中，我们将开始看到如何使用这些输入获取交叉验证的性能估计，从经典的机器学习算法如线性回归和逻辑回归开始。
- en: 4.3.1 Linear and logistic regression
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 线性回归和逻辑回归
- en: In *linear regression*, a statistical method that models the relationship between
    a dependent variable and one or more independent variables by fitting a linear
    equation to observed data, you first have all your features converted to numeric
    ones and put them into a matrix, including one-hot encoded categorical features.
    The algorithm’s goal is to optimally find the weight values in a column vector
    (the coefficients) so that, when multiplied against the matrix of features, you
    get a vector of results best approximating your targets (the predictions). In
    other words, the algorithm strives to minimize the residual sum of squares between
    the targets and the predictions obtained by multiplying features with the weight
    vector. In the process, you can consider using a prediction baseline (the so-called
    intercept or bias) or placing constraints on the weight values for them to be
    only positive.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，这是一种统计方法，通过拟合线性方程到观测数据来模拟因变量与一个或多个自变量之间的关系。你首先需要将所有特征转换为数值型，并将它们放入一个矩阵中，包括独热编码的类别特征。算法的目标是在列向量（系数）中找到最优的权重值，这样当乘以特征矩阵时，可以得到一个结果向量，最好地近似你的目标（预测）。换句话说，算法试图最小化目标与通过乘以权重向量得到的预测之间的残差平方和。在这个过程中，你可以考虑使用预测基线（所谓的截距或偏差）或对权重值施加约束，使它们只能是正数。
- en: 'Since the linear regression algorithm is just a weighted summation, you have
    to take care of three key aspects:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性回归算法只是一个加权求和，你必须注意三个关键方面：
- en: Ensure there are no missing values since they cannot be used for multiplications
    or additions unless you have imputed them to some value.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保没有缺失值，因为除非你将它们填充为某个值，否则它们不能用于乘法或加法运算。
- en: Ensure you have handled outliers because they can affect the algorithm’s work
    both in training and prediction.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经处理了异常值，因为它们会影响算法在训练和预测中的工作。
- en: 'Validate that the features and the target are linearly related as much as possible
    (i.e., they have a good Pearson correlation): features weakly related to the target
    tend just to add noise to the model, and they tend to make it underfit or even,
    when in high numbers, overfit.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证特征和目标尽可能线性相关（即，它们有良好的皮尔逊相关系数）：与目标弱相关的特征往往会给模型增加噪声，并且它们往往会使其欠拟合，甚至在数量多时，甚至过拟合。
- en: Since a summation of your weighted features gives the prediction, it is easy
    to determine the most significant effect on the predicted output and how each
    feature contributes to it. Observing the coefficients relative to each feature
    gives you insight into how the algorithm behaves. Such understanding can prove
    valuable when you have to explain how the model works to regulatory authorities
    or stakeholders and when you want to check if the predictions are justifiable
    from the point of view of a hypothesis or expert knowledge of the domain.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于加权特征的求和给出了预测，因此很容易确定对预测输出影响最大的因素以及每个特征如何对其做出贡献。观察每个特征的系数可以让你了解算法的行为。这种理解在你必须向监管机构或利益相关者解释模型如何工作时非常有价值，以及当你想从假设或领域专家知识的角度检查预测是否合理时。
- en: However, there are also hidden perils in the easy way that a regression model
    shows how it works under the hood. When two or more features in the data are highly
    correlated, a condition known as “multicollinearity” in statistics, the interpretation
    in a regression model can be much more complicated, even if both features effectively
    contribute to the prediction. Usually, only one of many takes a notable coefficient,
    whereas the others take small values as if they were unrelated to the target.
    In reality, the opposite is often true, and the relative ease in understanding
    the role of a feature in a regression prediction can lead to important conceptual
    misunderstanding.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，回归模型在幕后如何工作的简单方式也存在隐藏的陷阱。当数据中的两个或多个特征高度相关时，这在统计学上被称为“多重共线性”，回归模型中的解释可能会变得非常复杂，即使这两个特征都有效地对预测做出了贡献。通常，只有一个许多中的显著系数，而其他则取很小的值，好像它们与目标无关。实际上，情况往往相反，理解特征在回归预测中的作用相对容易，这可能导致重要的概念误解。
- en: Another great advantage of the linear regression algorithm is that, since it
    is just some multiplications and summations, it is a breeze to implement it on
    any software platform, even by hand-coding it in a script. Other machine learning
    algorithms are more complex to replicate, and hence, implementation from scratch
    of algorithms more complicated than a linear regression may be susceptible to
    errors and bugs. However, though unfeasible for delivering your projects, we have
    to note that hand-coding any machine learning model can be a valuable learning
    experience, allowing you to gain a deeper understanding of the inner workings
    of the algorithm and making yourself more equipped to troubleshoot and optimize
    the performance of the similar models in the future. We present some manageable
    from-scratch implementations of some algorithms for learning purposes in chapter
    5.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法的另一个巨大优势是，由于它只是些乘法和加法，因此在任何软件平台上实现它都非常容易，甚至可以通过在脚本中手动编码来实现。其他机器学习算法更复杂，因此比线性回归更复杂的算法从头开始实现可能会容易出错和出现bug。然而，尽管手动编码任何机器学习模型对于交付项目来说是不切实际的，但我们必须指出，手动编码任何机器学习模型可以是一种宝贵的学习经历，让你更深入地理解算法的内部工作原理，并使你更有能力在未来解决和优化类似模型的性能。我们在第5章中展示了某些算法的一些可管理的从头开始实现，以供学习之用。
- en: We will start with an example of a linear regression model applied end to end
    to our Airbnb NYC data. The example follows the schema proposed in figure 4.10,
    a schema that we will replicate for every classical machine learning algorithm
    we will present and that is based on Scikit-learn’s pipelines and cross-validation
    evaluation functions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个线性回归模型的例子开始，该模型从头到尾应用于我们的Airbnb纽约市数据。这个例子遵循图4.10中提出的方案，这个方案我们将为每个经典机器学习算法进行复制，它基于Scikit-learn的管道和交叉验证评估函数。
- en: '![](../Images/CH04_F10_Ryan2.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F10_Ryan2.png)'
- en: Figure 4.10 Schema of how we will organize the examples for classical machine
    learning algorithms
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10经典机器学习算法示例组织方案
- en: The schema is quite linear. The input from a comma-separated values file first
    goes through a ColumnTransformer, which constitutes the data preparation part,
    which applies transformation on data, discards data, or lets it pass as it is,
    based on column names and then a machine learning model. Both are wrapped into
    a pipeline tested by a `cross_validate` function that executes cross-validation
    and records computation times, trained models, and performances on a certain number
    of folds. Finally, the results are selected to demonstrate how the model worked.
    In addition, we can access, passing by the pipeline, the model coefficients and
    weights to get more insights into the functionalities of the algorithm we tested.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该方案相当线性。来自逗号分隔值文件的输入首先通过ColumnTransformer，这是数据准备部分，它根据列名对数据进行转换、丢弃数据或让它原样通过，然后是一个机器学习模型。这两个部分都被一个由`cross_validate`函数测试的管道所包裹，该函数执行交叉验证并记录计算时间、训练模型和一定数量的折叠上的性能。最后，选择结果来展示模型是如何工作的。此外，我们可以通过管道访问模型系数和权重，以获得更多关于我们测试的算法功能的见解。
- en: Applying such a schema, we just use a vanilla linear regression model in listing
    4.3 since this algorithm usually does not need to specify any parameter. For special
    applications related to model interpretability, you could have specified the `fit_intercept`
    to be false to remove the intercept from the model and derive all the predictions
    from the features only or the positive parameter to be true to get only positive
    coefficients.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 应用此架构，我们只需在列表 4.3 中使用纯线性回归模型，因为此算法通常不需要指定任何参数。对于与模型可解释性相关的特殊应用，你可以指定 `fit_intercept`
    为 false 以从模型中移除截距并仅从特征中得出所有预测，或者将正参数设置为 true 以仅获得正系数。
- en: Listing 4.3 Linear regression
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 线性回归
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① ColumnTransformer, transforming data into numeric features and imputing missing
    data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ① ColumnTransformer，将数据转换为数值特征并填充缺失数据
- en: ② Vanilla linear regression model
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ② 纯线性回归模型
- en: ③ Pipeline assembling ColumnTransformer and model
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 组装 ColumnTransformer 和模型的管道
- en: ④ Cross-validation strategy based on five folds and random sampling
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 基于 5 折交叉验证和随机抽样的交叉验证策略
- en: ⑤ Function for evaluation metric derived from mean squared error
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从均方误差派生的评估指标函数
- en: ⑥ Automated cross-validate procedure
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 自动交叉验证过程
- en: ⑦ Reports the results in terms of evaluation metric, standard deviation, fitting,
    and prediction time
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 以评估指标、标准差、拟合度和预测时间为单位报告结果
- en: 'Running the listed code will produce the following RMSE results:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列出的代码将产生以下 RMSE 结果：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: That’s a good result, obtained in a minimal time (using a standard Google Colab
    instance or a Kaggle notebook), and can act as a baseline for more sophisticated
    attempts. For example, if you try to run the code in listing 4.4, you will realize
    that you can get similar results with fewer but accurately prepared features.
    That’s called *feature engineering,* and the interesting point of doing it is
    that you can get better results or the same results but with fewer features meaningful
    for domain or business experts. For example, we create various new features in
    the code listing by generating binary features relative to specific values, combining
    features, and transforming them using a logarithmic function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的结果，在极短的时间内获得（使用标准的 Google Colab 实例或 Kaggle 笔记本），可以作为更复杂尝试的基准。例如，如果你尝试运行列表
    4.4 中的代码，你会意识到你可以通过更少但准确准备的特征获得类似的结果。这被称为 *特征工程*，做这件事的有趣之处在于你可以获得更好的结果，或者用更少的特征获得相同的结果，这些特征对领域或商业专家来说更有意义。例如，我们在代码列表中通过生成与特定值相关的二进制特征、组合特征和使用对数函数转换它们来创建各种新的特征。
- en: Listing 4.4 Customized data preparation for linear regression
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 线性回归的定制数据准备
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ① Creates an empty DataFrame
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个空的 DataFrame
- en: ② A binary column indicating whether the 'neighbourhood_group' is 'Manhattan'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一个二进制列，指示 'neighbourhood_group' 是否为 'Manhattan'
- en: ③ A binary column indicating whether the 'neighbourhood_group' is 'Queens'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个二进制列，指示 'neighbourhood_group' 是否为 'Queens'
- en: ④ A binary column indicating whether the 'room_type' is 'Entire home/apt'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个二进制列，指示 'room_type' 是否为 'Entire home/apt'
- en: ⑤ A column containing the natural logarithm of the values in the 'minimum_nights'
    column plus 1
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 包含 'minimum_nights' 列中值自然对数加 1 的列
- en: ⑥ A column containing the natural logarithm of the values in the 'number_of_reviews'
    column plus 1
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 包含 'number_of_reviews' 列中值自然对数加 1 的列
- en: ⑦ A product of the binary 'neighbourhood_group_Manhattan' and 'room_type_Entire
    home/apt' columns
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 二进制 'neighbourhood_group_Manhattan' 和 'room_type_Entire home/apt' 列的乘积
- en: ⑧ A product of 'availability_365' and the binary 'neighbourhood_group_Manhattan'
    column
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 'availability_365' 与二进制 'neighbourhood_group_Manhattan' 列的乘积
- en: ⑨ A product of 'availability_365' and the binary 'room_type_Entire home/apt'
    column
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 'availability_365' 与二进制 'room_type_Entire home/apt' 列的乘积
- en: The resulting RMSE is
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 RMSE 为
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Though the result is comparable to the previous experiment, this time you are
    using a dataset with fewer features that have been created by specific transformations,
    such as the one-hot encoding of categorical features, the transformations applied
    to numeric ones by specific functions (i.e., cubed, squared, logarithm, or square
    root transformation) and by multiplying features together. In our experience,
    a model presenting fewer, more meaningful features generated by reasoned feature
    engineering and domain expertise is usually more accepted by business users, even
    if it has comparable or even less predictive performance than a purely data-driven
    one.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果与之前的实验相当，但这次你使用的是具有更少特征的数据集，这些特征是通过特定的转换创建的，例如分类特征的one-hot编码，对数值特征应用特定的函数（即立方、平方、对数或平方根转换）以及通过将特征相乘。根据我们的经验，由合理的特征工程和领域专业知识生成的具有更少、更有意义的特征的模型通常更受商业用户接受，即使它的预测性能与纯粹的数据驱动模型相当甚至更差。
- en: Multiplying features together is an operation that you find only when working
    with linear regression models; the obtained result is called interactions between
    features. Interactions work by multiplying two or more features to get a new one.
    All such transformations on the features are intended to render the relationship
    between each feature and the target as linear as possible. Good results can be
    obtained automatically or based on your knowledge of the data and the problem.
    Applying such transformations to the features is typical of the family of linear
    regression models. They have little or no effect on the more complex algorithms
    we will explore later in this chapter and subsequent chapters. Investing time
    in defining how the features should be expressed is both an advantage and a disadvantage
    of linear regression models. However, there are ways to automatically perform
    it using regularization, as we will propose in the next section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征相乘是一个仅在处理线性回归模型时才会遇到的操作；得到的结果被称为特征之间的交互。交互通过将两个或多个特征相乘得到一个新的特征来实现。所有这些对特征的转换都是为了尽可能地将每个特征与目标之间的关系线性化。好的结果可以通过自动方法或基于你对数据和问题的了解来获得。将此类转换应用于特征是线性回归模型家族的典型特征。它们对我们在本章和随后的章节中将要探索的更复杂的算法影响很小或没有影响。在定义特征应该如何表达上投入时间，是线性回归模型的一个优点也是一个缺点。然而，我们可以通过正则化自动执行它，正如我们将在下一节中提出的。
- en: The next section will discuss regularization in linear models (linear regression
    and logistic regression). Regularization is the best solution to implement when
    you have many features and their reciprocal multicollinearity (you have multicollinearity
    when two predictors are highly correlated with one another) doesn’t allow the
    linear regression model to find the best coefficients for the prediction because
    they are unstable and unreliable—for instance, showing a coefficient you didn’t
    expect in terms of sign and size.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '下一节将讨论线性模型（线性回归和逻辑回归）中的正则化。当你有很多特征并且它们的相互多重共线性（当你有两个预测变量高度相关时）不允许线性回归模型找到预测的最佳系数，因为它们不稳定且不可靠——例如，在符号和大小方面显示出你未曾预期的系数时，正则化是最佳解决方案。 '
- en: 4.3.2 Regularized methods
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 正则化方法
- en: Linear regression models are usually simple enough for humans to understand
    directly as formulas of coefficients applied to features. This means that, when
    applied to a real-world problem, they can turn out to be a rough approximation
    of complex dynamics and thus systematically miss correct predictions. Technically,
    they are models with a high bias. A remedy for this is to make their formulations
    more complex by adding more and more features and their transformations (logarithmic,
    squared, root transformations, and so on) and by making features interact with
    many others (through multiplication). In this way, a linear regression model can
    diminish its bias and become a better predictor. At the same time, however, the
    variance of the model will also increase, and it can start overfitting.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型通常足够简单，以至于人类可以直接将其理解为应用于特征的系数公式。这意味着，当应用于现实世界问题时，它们可能只是复杂动态的粗略近似，从而系统地错过正确的预测。技术上，它们是具有高偏差的模型。对此的补救措施是通过添加越来越多的特征及其转换（对数、平方、根转换等）以及通过使特征与许多其他特征（通过乘法）相互作用，使它们的公式更加复杂。这样，线性回归模型可以减少其偏差，成为一个更好的预测器。然而，与此同时，模型的方差也会增加，它可能会开始过度拟合。
- en: 'Occam’s razor principle, which states that among competing hypotheses, the
    one with the fewest assumptions should be selected ([https://mng.bz/RV40](https://mng.bz/RV40)),
    works perfectly for linear models, whereas it doesn’t matter for neural networks
    applied to tabular data where the more complex, the better. Hence, linear models
    should be as simple as possible to meet the needs of the problem. Here is where
    regularization enters the scene, helping you reduce the complexity of a linear
    model until it fits the problem. Regularization is a technique used to reduce
    overfitting in machine learning by limiting the complexity of the model, thus
    effectively improving its generalization performance. Regularization works because
    the linear regression model is penalized as it looks for the best coefficients
    for its predictions. The used penalization is based on the summation of the coefficients.
    Therefore, the regression model is incentivized to keep them as small as possible,
    if not to set them to zero. Constraining regression coefficients to limit their
    magnitude has two significant effects:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 奥卡姆剃刀原则，即认为在竞争假设中，应该选择假设最少的那个（[https://mng.bz/RV40](https://mng.bz/RV40)），对于线性模型来说效果完美，而对于应用于表格数据的神经网络来说则无关紧要，因为越复杂越好。因此，线性模型应该尽可能简单，以满足问题的需求。正是在这里，正则化登场，帮助你降低线性模型的复杂性，直到它适合问题。正则化是一种通过限制模型的复杂性来减少机器学习中的过拟合的技术，从而有效地提高其泛化性能。正则化之所以有效，是因为线性回归模型在寻找最佳预测系数时会受到惩罚。所使用的惩罚基于系数的求和。因此，回归模型被激励保持系数尽可能小，如果不是将其设置为零。将回归系数约束在限制其幅度有两个显著的影响：
- en: It avoids any form of data memorization and overfitting (i.e., certain specific
    coefficient values to be taken when there is a large number of features compared
    to the available examples).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它避免了任何形式的数据记忆和过拟合（即，当特征数量远大于可用示例时，需要采取某些特定的系数值）。
- en: As coefficient shrinking happens, estimates are stabilized because multicollinear
    features will have the values of their coefficients resized or concentrated on
    only one of the features.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着系数收缩的发生，估计值会稳定下来，因为多重共线性特征将调整其系数的值或集中在仅一个特征上。
- en: 'In the optimization process, coefficients are updated multiple times, and these
    steps are called iterations. At each step, each regression coefficient incorporates
    a correction toward its optimal value. The optimal value is determined by the
    gradient, which can be intended as a number representing the direction that greatly
    improves the coefficient at that step. A more detailed explanation closes this
    chapter. Penalization is a form of constraint that forces the weights deriving
    from the optimization of the model to have specific characteristics. We have two
    variants of regularization:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化过程中，系数会多次更新，这些步骤被称为迭代。在每一步中，每个回归系数都会朝着其最优值进行修正。最优值由梯度决定，可以理解为表示在该步骤中极大改善系数方向的数字。更详细的解释将在本章结束。惩罚是一种约束形式，迫使模型优化得到的权重具有特定的特征。我们有两种正则化的变体：
- en: 'The first variant is where the penalization is computed by summing the absolute
    values of the coefficients: this is called L1 regularization. It makes the coefficients
    sparse because it can push some coefficients to zero, making their related features
    irrelevant.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种变体是通过对系数的绝对值求和来计算惩罚：这被称为L1正则化。它使系数变得稀疏，因为它可以将某些系数推到零，使相关的特征变得无关紧要。
- en: 'The second option is where the penalization is computed by summing the squared
    coefficients: this is called L2 regularization, and its effect is generally to
    reduce the size of the coefficients (it is also relatively fast to compute).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种选项是通过求和系数的平方来计算惩罚：这被称为L2正则化，其效果通常是减小系数的大小（计算速度也相对较快）。
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus operating an implicit selection of the useful features (setting a coefficient
    to zero means that a feature doesn’t play any role in prediction). In addition,
    coefficients are always pushed toward zero with the same strength (technically,
    the gradients toward the solution are always +1 or –1). Hence, through the optimization
    steps, the features less associated with the target tend quickly to be assigned
    a zero coefficient and become totally irrelevant regarding the predictions. In
    short, if two or more features are multicollinear and all quite predictive, by
    applying L1 regularization, you will have only one of them with a coefficient
    different from zero.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化（或Lasso回归）将许多系数推向零值，从而隐式地选择了有用的特征（将系数设为零意味着该特征在预测中不起任何作用）。此外，系数总是以相同的强度推向零（技术上，指向解的梯度总是+1或-1）。因此，通过优化步骤，与目标关联度较低的特征会迅速被分配一个零系数，并在预测中变得完全无关。简而言之，如果有两个或更多特征是多重共线的，并且都具有很强的预测性，通过应用L1正则化，你将只有一个特征的系数与零不同。
- en: Instead, in L2 regularization (or Ridge regression), the fact that coefficients
    are squared prevents negative and positive values from canceling each other in
    the penalization and puts more weight on larger coefficients. The result is a
    set of generally smaller coefficients, and multicollinear features tend to have
    similar coefficient values. All the features involved are included in the summation.
    You can notice better important features because, contrary to what happens with
    standard regression, the role of a feature in the prediction is not hidden by
    its correlation with other features. L2 regularization tends to attenuate the
    coefficients. It does so proportionally during the optimization steps; technically,
    the gradients toward the solution tend to be smaller and smaller. Hence, coefficients
    can reach the zero value or be near it. Still, even if the feature must be completely
    irrelevant to the prediction, it takes many optimization iterations and is quite
    time-consuming. Consequently, reprising the previous example of two or more multicollinear
    features in L2 regularization, instead of L1 regression that keeps only one non-zero
    coefficient, all the features would have a non-zero, similar size coefficient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在L2正则化（或Ridge回归）中，系数平方的事实阻止了负值和正值在惩罚中相互抵消，并使较大的系数具有更大的权重。结果是系数集通常较小，多重共线特征往往具有相似的系数值。所有涉及的特征都包含在求和中。你可以注意到更重要的特征，因为与标准回归不同，特征在预测中的作用不会被其与其他特征的关联所隐藏。L2正则化倾向于衰减系数。它在优化步骤中按比例进行；技术上，指向解的梯度倾向于越来越小。因此，系数可以达到零值或接近零。尽管如此，即使特征必须与预测完全无关，也需要许多优化迭代，并且相当耗时。因此，在L2正则化中重新考虑两个或更多多重共线特征的先例，与只保留一个非零系数的L1回归不同，所有特征都将具有非零、相似大小的系数。
- en: 'In our example, we first try to create new features through systematic interactions
    between our available features and then perform an L2 and L1 penalized regression
    to compare their results and resulting coefficients. PolynomialFeatures is a Scikit-learn
    function ([https://mng.bz/2ynd](https://mng.bz/2ynd)) that automatically creates
    multiplications between features by multiplying them many times with other features
    and by themselves. The process is reminiscent of the mathematical *polynomial
    expansion* where a power of sums is expressed into its single terms:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们首先尝试通过我们可用特征之间的系统交互来创建新的特征，然后执行L2和L1惩罚回归来比较它们的结果和结果系数。PolynomialFeatures是Scikit-learn函数（[https://mng.bz/2ynd](https://mng.bz/2ynd)），它通过将特征与其他特征以及自身多次相乘来自动创建特征之间的乘积。这个过程让人联想到数学中的*多项式展开*，其中幂的和被表达为其单个项：
- en: (*a* + *b*)² = *a*² + 2*ab* + *b*²
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: (*a* + *b*)² = *a*² + 2*ab* + *b*²
- en: Scikit-learn makes it easier because when you state a degree, the function automatically
    creates the polynomial expansions up to that degree. You can decide whether to
    keep only the interactions. Such a process is interesting for a regression model
    because
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn使这个过程变得更容易，因为当你声明一个度数时，函数会自动创建到该度数的多项式展开。你可以决定是否只保留交互项。这个过程对于回归模型来说很有趣，
- en: '*Interactions* help the regression model to better take into account the conjoint
    values of more features since features usually do not relate to the target in
    isolation but in synergy with others.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交互作用*有助于回归模型更好地考虑更多特征的联合值，因为特征通常不会单独与目标相关，而是在与其他特征的协同作用下相关。'
- en: The set of *powers* of a feature helps to model it as a curve. For instance,
    a + a² is a curve in the shape of a parabola.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特征的*幂*集合有助于将其建模为曲线。例如，a + a²是一个抛物线形状的曲线。
- en: Though using polynomial expansion can avoid the heavy task of creating specific
    features for your problem, it has a downside because it dramatically increases
    the number of features your model uses. More features usually provide more predictive
    power, but they also mean more noise, more multicollinearity, and more chances
    that the model has just to memorize examples and overfit the problem. Applying
    penalties can help us fix this problem with the L2 penalty and select only the
    features to be kept with the L1 penalty.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用多项式展开可以避免为您的特定问题创建特定特征的繁重任务，但它有一个缺点，因为它会显著增加模型使用的特征数量。更多的特征通常提供更多的预测能力，但它们也意味着更多的噪声、更多的多重共线性以及模型只是记住示例并过度拟合问题的更多机会。应用惩罚可以帮助我们使用L2惩罚解决这个问题，并使用L1惩罚选择要保留的特征。
- en: In the code in listing 4.5, we test applying L2 and, successively in listing
    4.6, L1 regularization to the same polynomial expansion. It is important to note
    the effect of each kind of regularization. In this first example, we apply L2
    regularization (Ridge). Since regularization makes sense if you have plenty of
    features for your prediction, we create new features from the old ones using a
    polynomial expansion. Our ridge model is then set to a high alpha value to handle
    the increased number of collinear features.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.5中的代码中，我们测试了应用L2正则化，并在列表4.6中，依次应用L1正则化到相同的多项式展开。注意每种正则化的影响很重要。在这个第一个例子中，我们应用L2正则化（Ridge）。由于正则化在您有大量预测特征时才有意义，我们使用多项式展开从旧特征中创建新特征。然后，我们将ridge模型设置为高alpha值以处理增加的共线性特征数量。
- en: Listing 4.5 L2 regularized linear regression
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 L2正则化线性回归
- en: '[PRE43]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ① PolynomialFeatures instance performing second-degree polynomial expansion
    on the features
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对特征进行二次多项式展开的PolynomialFeatures实例
- en: ② A Ridge regression model instance with a regularization strength (alpha) of
    2,500
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一个具有正则化强度（alpha）为2,500的Ridge回归模型实例
- en: ③ Pipeline for column transformation, polynomial expansion, standardization,
    and Ridge regression modeling
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 用于列转换、多项式展开、标准化和Ridge回归建模的管道
- en: ④ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行五折交叉验证并计算RMSE分数
- en: ⑤ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证测试RMSE分数的均值和标准差
- en: 'The script results in the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本产生以下输出：
- en: '[PRE44]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If we count the number of non-zero coefficients (after rounding to five decimals
    to exclude extremely small values), we get
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算非零系数的数量（四舍五入到小数点后五位以排除极小的值），我们得到
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Ninety-one coefficients out of 105 have non-zero values.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在105个系数中有91个具有非零值。
- en: In the next example, we apply an L1 regularization and compare the results with
    the previous example. The procedure is the same as the last code listing, though
    we resort to a lasso model this time.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们应用L1正则化，并将结果与上一个例子进行比较。虽然程序与最后一个代码列表相同，但我们这次求助于lasso模型。
- en: Listing 4.6 L1 regularized linear regression
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 L1正则化线性回归
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① A Lasso regression model instance with a regularization strength (alpha) of
    0.1
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个具有正则化强度（alpha）为0.1的Lasso回归模型实例
- en: ② Pipeline applying column transformation, polynomial expansion, standardization,
    and Lasso regression modeling
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ② 应用列转换、多项式展开、标准化和Lasso回归建模的管道
- en: ③ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用定义的管道进行五折交叉验证并计算RMSE分数
- en: ④ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印交叉验证测试RMSE分数的均值和标准差
- en: The resulting output is
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出为
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If we check how many coefficients have non-zero-values by taking the first
    model built by the cross-validation cycle, this time we have fewer:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过检查由交叉验证周期构建的第一个模型中的非零系数数量，这次我们得到的更少：
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: With 53 non-zero coefficients, the number of working coefficients has been halved.
    By increasing the alpha parameter of the Lasso call, we can obtain an even sharper
    reduction of used coefficients, albeit at the price of a higher computation time.
    There’s a sweet spot after which applying a higher L1 penalty doesn’t improve
    the prediction results. For prediction purposes, you have to find the correct
    alpha by trial and error or using convenient automatic functions such as LassoCV
    ([https://mng.bz/1XoV](https://mng.bz/1XoV)) or RidgeCV ([https://mng.bz/Pdn9](https://mng.bz/Pdn9))
    that will do the experimentation for you.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 有53个非零系数，工作系数的数量已经减半。通过增加Lasso调用的alpha参数，我们可以获得使用系数的更大幅度的减少，尽管代价是更高的计算时间。在某个点上，应用更高的L1惩罚并不会改善预测结果。为了预测目的，你必须通过试错或使用方便的自动函数（如LassoCV
    [https://mng.bz/1XoV](https://mng.bz/1XoV) 或 RidgeCV [https://mng.bz/Pdn9](https://mng.bz/Pdn9)）来找到正确的alpha，这些函数将为你进行实验。
- en: Interestingly, regularization is also used in neural networks. Neural networks
    use sequential matrix multiplications based on matrices of coefficients to transit
    from features to predictions, which is an extension of the working of linear regression.
    Neural networks have more complexities, though; yet in such an aspect of matrix
    multiplication, they resemble a regression model. Based on similar workings, you
    may find it beneficial for your tabular data problem to fit a deep learning architecture
    and, in doing so, to apply an L2 penalty, so the coefficients of the network are
    attenuated and distributed, and/or an L1 penalty, so coefficients are instead
    sparse with many of them set to zeros. In the next section, we will continue our
    discussion of linear models by discovering how to solve a classification problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，正则化也被用于神经网络中。神经网络使用基于系数矩阵的顺序矩阵乘法从特征过渡到预测，这是线性回归工作的扩展。尽管神经网络有更多的复杂性；然而，在矩阵乘法的这个方面，它们与回归模型相似。基于类似的工作原理，你可能发现为你的表格数据问题拟合深度学习架构并在此过程中应用L2惩罚是有益的，这样网络的系数就会衰减并分布，或者应用L1惩罚，这样系数就会变得稀疏，其中许多被设置为0。在下一节中，我们将继续讨论线性模型，通过发现如何解决分类问题来继续我们的讨论。
- en: 4.3.3 Logistic regression
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 逻辑回归
- en: The linear regression model can be effectively extended to classification. In
    a binary classification problem, where you have two classes (a positive one and
    a negative one), you use the same approach as in a regression (feature matrix,
    vector of coefficients, bias). Still, you transform the target using the logit
    function (for details about this statistical distribution, see [https://mng.bz/JY20](https://mng.bz/JY20)).
    The transformative function is called the *link function*. On the optimization
    side, the algorithm uses as a reference the Bernoulli conditional distribution
    (for revising this distribution, see [https://mng.bz/wJoq](https://mng.bz/wJoq))
    instead of the normal distribution. As a result, you get output values ranging
    from 0 to 1, representing the probability that the sample belongs to the positive
    class. This is called logistic regression. Logistic regression is quite an intuitive
    and practical approach to solving binary classification problems and multiclass
    and multilabel ones.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型可以有效地扩展到分类。在二元分类问题中，你拥有两个类别（一个正类和一个负类），你使用与回归相同的方法（特征矩阵、系数向量、偏置）。然而，你使用对数几率函数（关于这种统计分布的详细信息，请参阅[https://mng.bz/JY20](https://mng.bz/JY20)）来转换目标。这种转换函数被称为*链接函数*。在优化方面，算法使用伯努利条件分布（关于修订这种分布，请参阅[https://mng.bz/wJoq](https://mng.bz/wJoq)）作为参考，而不是正态分布。因此，你得到从0到1的范围内的输出值，表示样本属于正类的概率。这被称为逻辑回归。逻辑回归是解决二元分类问题以及多类和多标签问题的相当直观和实用的方法。
- en: In listing 4.7, we replicate the same approach as seen with linear regression—this
    time trying to build a model to guess if an example has a target value above the
    median. Please note that transformations are the same, though we use a logistic
    regression model this time. Our target is a class that tells if the target value
    is above the median. Such a target is a binary balanced outcome, where half of
    the labels are positive and half are negative.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.7中，我们复制了与线性回归相同的方法——这次尝试构建一个模型来猜测一个示例的目标值是否高于中位数。请注意，变换是相同的，尽管这次我们使用了逻辑回归模型。我们的目标是判断目标值是否高于中位数的类别。这样的目标是一个二元平衡结果，其中标签的一半是正的，一半是负的。
- en: Listing 4.7 Logistic regression
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.7 逻辑回归
- en: '[PRE49]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ① A logistic regression model instance with the “saga” solver, no penalty, and
    a maximum of 1,000 iterations
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用“saga”求解器、无惩罚和最多1,000次迭代的逻辑回归模型实例
- en: ② A column transformer applying one-hot encoding to categorical features and
    standardization to numeric features
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ② 应用独热编码到分类特征的列转换器，并对数值特征进行标准化
- en: ③ A pipeline that sequentially applies column transformation and logistic regression
    modeling
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 依次应用列转换和逻辑回归建模的管道
- en: ④ Five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行五折交叉验证并计算准确率分数
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证测试准确率分数的均值和标准差
- en: 'The script results in the following scores:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本产生以下分数：
- en: '[PRE50]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As the feature processing is the same, we just focus on noticing how the logistic
    regression has some specific parameters with respect to linear regression. In
    particular, you can set the penalty directly without changing the algorithm and
    decide what optimizer will be used (using the parameter solver). Each optimizer
    allows specific penalties, and it can be more or less efficient based on the characteristics
    of your data:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征处理相同，我们只需关注注意逻辑回归相对于线性回归有一些特定的参数。特别是，你可以直接设置惩罚而不改变算法，并决定将使用什么优化器（使用参数求解器）。每个优化器允许特定的惩罚，并且它可以根据你的数据特性更有效或更无效：
- en: lbfgs for L2 or no penalty.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lbfgs用于L2或无惩罚。
- en: liblinear for L1 and L2 penalties—better for smaller datasets, limited to one-versus-rest
    schemes for multiclass problems.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: liblinear用于L1和L2惩罚——更适合小型数据集，对于多类问题仅限于一对一方案。
- en: newton-cg for L2 or no penalty.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: newton-cg用于L2或无惩罚。
- en: newton-cholesky for L2 or no penalty.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: newton-cholesky用于L2或无惩罚。
- en: sag for L2 or no penalty—ideal for large datasets. It requires standardized
    features (or features all with similar scale/standard deviation).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: saga用于L2或无惩罚——非常适合大型数据集。它需要标准化特征（或所有特征都具有相似尺度/标准差）。
- en: saga for no penalty, L1, L2, elasticnet (a mix of L1 and L2) penalties—ideal
    for large datasets, it requires standardized features (or features all with similar
    scale/standard deviation).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: saga无惩罚，L1，L2，弹性网络（L1和L2惩罚的混合）——非常适合大型数据集，它需要标准化特征（或所有特征都具有相似尺度/标准差）。
- en: In listing 4.8, we use an L2 penalty on the multiclass target to test how multiple
    targets are easily dealt with using the `multi_class` parameter set to “ovr” (one-versus-rest),
    a solution that takes a multiclass problem and builds a binary model for each
    of the classes to be predicted. At prediction time, prediction probabilities across
    all the classes are normalized to sum 1.0, and the class corresponding to the
    highest probability is taken and the predicted class. Such an approach is analogous
    to the softmax function approach used in neural networks where a vector of arbitrary
    real values is turned into a probability distribution, where the sum of all elements
    is 1 (for a more detailed explanation of softmax, see [https://mng.bz/qxYw](https://mng.bz/qxYw)).
    The alternative to the one-versus-rest approach is the multinomial option, where
    a single regression model directly models the probability distribution across
    all classes simultaneously.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.8中，我们使用L2惩罚对多类目标进行测试，以查看如何使用`multi_class`参数设置为“ovr”（一对一）轻松处理多个目标，这是一个将多类问题转换为为每个要预测的类构建二进制模型的解决方案。在预测时，所有类的预测概率被归一化以总和为1.0，并取对应最高概率的类作为预测类。这种方法类似于神经网络中使用的softmax函数方法，其中任意实数值向量被转换为概率分布，其中所有元素的总和为1（有关softmax的更详细解释，请参阅[https://mng.bz/qxYw](https://mng.bz/qxYw)）。一对一方法的替代方案是多项式选项，其中单个回归模型直接对所有类的概率分布进行建模。
- en: The multinomial approach is preferred when inter-class relationships are important
    (e.g., for ranking or confidence-based decisions) or when a compact, single-model
    solution is desired.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 当类间关系很重要时（例如，用于排名或基于置信度的决策）或当需要紧凑的单模型解决方案时，多项式方法更受欢迎。
- en: Listing 4.8 L2 regularized multiclass linear regression
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.8 L2正则化的多类线性回归
- en: '[PRE51]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ① Logistic regression model instance with L2 penalty, regularization C=0.1,
    “sag” solver, “ovr” multiclass strategy
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用L2惩罚、正则化C=0.1、“sag”求解器和“ovr”多类策略的逻辑回归模型实例
- en: ② Column transformer that applies one-hot encoding to categorical features and
    standardization to numeric features
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ② 应用独热编码到分类特征的列转换器，并对数值特征进行标准化
- en: ③ Pipeline that sequentially applies column transformation and logistic regression
    modeling
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 依次应用列转换和逻辑回归建模的管道
- en: ④ Cross-validation using the defined pipeline and calculating accuracy scores
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道进行交叉验证并计算准确度分数
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证测试准确度分数的平均值和标准差
- en: 'Predicting the target as a class is certainly more complicated than guessing
    if the target price is over a threshold value or not:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 将目标预测为类别肯定比猜测目标价格是否超过阈值更复杂：
- en: '[PRE52]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: From this output, it is important to notice how the training time for a cross-validation
    fold has skyrocketed 10 times more. The reason is that applying a penalty to the
    coefficients involves more iterations of the algorithm’s optimization processes
    before reaching a stable result and because now a model for each class is being
    built. As a general rule, consider that penalization requires longer computations
    for the L2 penalty and even longer for the L1 penalty. By setting the `max_iter`
    parameter, you can impose a limit to the algorithm’s iterations, but be aware
    that the result you obtain by cutting off the time required for the algorithm
    to converge won’t be assured to be the best.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，重要的是要注意交叉验证折叠的训练时间已经飙升了10倍。原因是应用惩罚系数涉及到算法优化过程更多的迭代，以达到一个稳定的结果，并且现在正在为每个类别构建一个模型。一般来说，考虑惩罚需要更长的L2惩罚计算时间，甚至L1惩罚需要更长的时间。通过设置`max_iter`参数，你可以对算法的迭代次数进行限制，但请注意，通过截断算法收敛所需的时间所得到的结果不一定是最优的。
- en: 4.3.4 Generalized linear methods
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 广义线性方法
- en: 'The idea of extending linear regression to binary classification by logit transformation
    can be applied to distributions other than Bernoulli conditional distribution.
    This is dictated by the target that may represent categorical data, count data,
    or other data whose distribution is known not to be from a normal distribution.
    As we have seen in the previous paragraph, multiclass problems can be modeled
    using the Bernoulli distribution (the one-versus-rest strategy of fitting multiple
    logistic regressions) and the multinomial one. Other problems, more typical of
    domains such as finance or insurance, require different approaches. For instance,
    the Scikit-learn package mentions a few real-world applications and their best-fitting
    distributions (for reference, see [https://mng.bz/7py9](https://mng.bz/7py9)):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对数变换将线性回归扩展到二元分类的想法可以应用于除了伯努利条件分布之外的分布。这是由目标决定的，目标可能代表分类数据、计数数据或其他已知分布不是正态分布的数据。正如我们在上一段中看到的，多类问题可以使用伯努利分布（多个逻辑回归的one-versus-rest策略）和多项式分布进行建模。其他问题，如金融或保险等领域的典型问题，需要不同的方法。例如，Scikit-learn包提到了一些现实世界的应用及其最佳拟合分布（参考，见[https://mng.bz/7py9](https://mng.bz/7py9)）：
- en: '*Climate modeling*—Number of rain events per year (Poisson distribution for
    count data and discrete events). The Poisson distribution is used for modeling
    events such as the number of calls to a call center or the number of customers
    visiting a restaurant), amount of rainfall per event (using Gamma distribution,
    a theoretical distribution useful for modeling because of its skewness and long
    tail), or total rain precipitation per year (Tweedie distribution, a distribution
    which is a compound of Poisson and Gamma distributions).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*气候建模*—每年降雨事件的数量（计数数据的泊松分布和离散事件）。泊松分布用于建模事件，如呼叫中心的呼叫次数或餐厅的顾客数量），每次事件的降雨量（使用伽马分布，一个具有偏斜和长尾的理论分布，对于建模很有用），或每年的总降雨量（Tweedie分布，这是一个泊松分布和伽马分布的复合分布）。'
- en: '*Risk modeling or insurance policy pricing*—Number of claim events or policyholder
    per year (Poisson), cost per event (Gamma), the total cost per policyholder per
    year (Tweedie).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*风险建模或保险政策定价*—每年索赔事件的数量或保单持有人数量（泊松分布），每次事件的成本（伽马分布），每年每个保单持有人的总成本（Tweedie分布）。'
- en: '*Predictive maintenance*—Number of production interruption events per year
    (Poisson), the duration of interruption (Gamma), and the total interruption time
    per year (Tweedie).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测性维护*—每年生产中断事件的数量（泊松分布），中断的持续时间（伽马分布），以及每年的总中断时间（Tweedie分布）。'
- en: Figure 4.11 shows the three distributions—Poisson, Tweedie, and Gamma—for different
    averages. The Tweedie distribution is calculated for power equal to 1.5, a blend
    between the Poisson and Gamma distributions.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11显示了三种分布——泊松、Tweedie和伽马——在不同平均值下的情况。Tweedie分布的幂等于1.5，是泊松和伽马分布的混合。
- en: '![](../Images/CH04_F11_Ryan2.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F11_Ryan2.png)'
- en: Figure 4.11 Comparing Poisson, Tweedie, and Gamma distributions at different
    mean values (mu) of the distribution
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 比较泊松、Tweedie和伽马分布在不同的分布平均值（mu）下
- en: Of course, you may try any distribution you want—even a plain regression model—for
    any such situation. However, approaching each of them using the appropriate generalized
    linear model that optimizes that specific distribution assures the best result
    in most cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以尝试任何你想要的分布——甚至是一个普通的回归模型——对于任何这种情况。然而，使用适合优化特定分布的适当广义线性模型来处理每个分布，通常能保证最佳结果。
- en: We don’t enter into the specifics of each distribution; you just need to know
    that the Swiss Army Knife of general linear models is the `TweedieRegressor` ([https://mng.bz/mGOr](https://mng.bz/mGOr)).
    This Scikit-learn implementation, depending on the power parameter, can allow
    you to quickly test normal distribution (a regular regression), Poisson distribution
    ([https://mng.bz/4a4w](https://mng.bz/4a4w)), Gamma distribution ([https://mng.bz/QDvG](https://mng.bz/QDvG)),
    Inverse Gaussian distribution (for nonnegative positively skewed data), and a
    blend of Gamma and Poisson (the Tweedie distribution) (see table 4.1).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不深入探讨每种分布的细节；你只需要知道，通用线性模型的瑞士军刀是`TweedieRegressor` ([https://mng.bz/mGOr](https://mng.bz/mGOr))。这个Scikit-learn实现，根据幂参数的不同，可以让你快速测试正态分布（常规回归）、泊松分布
    ([https://mng.bz/4a4w](https://mng.bz/4a4w))、伽马分布 ([https://mng.bz/QDvG](https://mng.bz/QDvG))、逆高斯分布（用于非负正偏斜数据），以及伽马和泊松的混合（Tweedie分布）（见表4.1）。
- en: Table 4.1 Power values and their corresponding statistical distributions
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 幂值及其对应的统计分布
- en: '| Power | Distribution |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 功率 | 分布 |'
- en: '| --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | Normal |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 正态 |'
- en: '| 1 | Poisson |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 泊松 |'
- en: '| (1,2) | Compound Poisson Gamma |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| (1,2) | 复合泊松伽马 |'
- en: '| 2 | Gamma |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 伽马 |'
- en: '| 3 | Inverse Gaussian |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 逆高斯 |'
- en: 'In listing 4.9, we test the different distributions offered by the TweedieRegressor
    on the entire distribution of prices of the Airbnb NYC dataset, a model fitting
    that we previously avoided because of the heavy distribution tails revealed by
    the EDA. We do so by testing each of these distributions one by one on the full
    range of price values since we are confident that using such specialized distribution
    will solve our problem of a target with heavy tails. It is important to remember
    that such distributions have limitations due to their formulations:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.9中，我们测试了TweedieRegressor提供的不同分布在整个Airbnb纽约市数据集价格分布上的效果，这是一个我们之前因为EDA显示的重尾分布而避免的模型拟合。我们通过逐个测试这些分布在整个价格值范围内来完成，因为我们相信使用这种专业分布将解决我们的目标重尾问题。重要的是要记住，由于它们的公式，这些分布存在局限性：
- en: '*Norma**l*—Any kind of value'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正常*——任何类型的值'
- en: '*Poisso**n*—Zero or positive values'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*泊松*——零或正值'
- en: '*Tweedie, Gamma, Inverse Gaussia**n*—Only non-zero positive values'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tweedie、伽马、逆高斯*——只有非零正值'
- en: This implies that you must adapt your data if you have negative or zero values
    by adding an offset value. Hence, depending on the modeled distribution, we clip
    the target values to a lower bound based on the aforementioned limitations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果你有负值或零值，你必须通过添加偏移值来调整你的数据。因此，根据所建模的分布，我们根据上述限制将目标值裁剪到下限。
- en: Listing 4.9 Tweedie regression
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.9 Tweedie回归
- en: '[PRE53]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ① A list of experiments, made of a distribution name, power parameter, and minimum
    target value
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个由分布名称、幂参数和最小目标值组成的实验列表
- en: ② Loops through the experiments list with distribution names and power parameters
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历实验列表中的分布名称和幂参数
- en: ③ Instance of the TweedieRegressor model with the specified power parameter
    for the current experiment
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 当前实验指定幂参数的TweedieRegressor模型实例
- en: ④ Clips the target regression data to a minimum value, according to the used
    distribution
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 根据使用的分布将目标回归数据裁剪到最小值
- en: ⑤ Prints the experiment name along with the results from cross-validation
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印实验名称以及交叉验证的结果
- en: 'The resulting best-fitting are the Poisson and Tweedie with power 1.5 distributions:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的最佳拟合是泊松和幂为1.5的Tweedie分布：
- en: '[PRE54]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: It is crucial to remember that the secret of the performances of the generalized
    linear models lies in the specific distribution they strive to model during the
    optimization phase. When faced with similar problems, we could resort to similar
    distributions on some more advanced algorithms than generalized linear models,
    particularly on gradient boosting implementations such as XGBoost or LightGBM,
    which will be discussed in the next chapter. In the next section, we will deal
    with a different approach related to large datasets.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，广义线性模型性能的秘密在于它们在优化阶段努力模拟的特定分布。当面临类似问题时，我们可以在比广义线性模型更先进的算法上使用类似的分布，尤其是在梯度提升实现上，如XGBoost或LightGBM，这些将在下一章中讨论。在下一节中，我们将处理与大数据集相关的一种不同方法。
- en: 4.3.5 Handling large datasets with stochastic gradient descent
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 使用随机梯度下降处理大型数据集
- en: 'When your tabular dataset cannot fit into your system’s memory, whether it
    is a cloud instance or your desktop computer, your options in modeling shrink.
    Apart from deep learning solutions, which will be discussed in the third part
    of this book, one other option, using classical machine learning, is to resort
    to out-of-core learning. In out-of-core learning, you keep your data in its storage
    (for instance, your data warehouse), and you have your model learn from it bit
    by bit, using small samples extracted from your data, called *batches*. This is
    practically feasible because modern data storage allows for the picking of specific
    data samples at a certain cost in terms of latency: the time interval between
    the initiation of a data-related operation and its completion or response. In
    addition, there are also tools for handling and processing data on the fly (for
    instance, Apache Kafka or Amazon Kinetics) that can redirect the data to out-of-core
    learning algorithms.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的表格数据集无法适应你的系统内存时，无论是云实例还是你的台式电脑，你在建模方面的选择就会减少。除了将在本书第三部分讨论的深度学习解决方案之外，另一个选择，即使用经典机器学习，是求助于离核学习。在离核学习中，你将数据保留在其存储中（例如，你的数据仓库），并且让模型逐步从中学习，使用从你的数据中提取的小样本，称为*批次*。这在实际中是可行的，因为现代数据存储允许以一定的延迟成本选择特定的数据样本：即从数据相关操作开始到完成或响应的时间间隔。此外，还有处理和实时处理数据的工具（例如，Apache
    Kafka或Amazon Kinetics），可以将数据重定向到离核学习算法。
- en: It is also algorithmically feasible because of linear/logistic regression models.
    Both models are made up of additions of coefficients relative to the features
    you use for learning. Out-of-core learning involves first estimating these coefficients
    using some small samples from the data and then updating such coefficients using
    more and more batches extracted from your data. In the end, though the process
    is particularly long, your final estimated coefficient would not be much different
    from the ones you would have obtained if you could have fit all the data into
    the memory.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是由于线性/逻辑回归模型算法上可行的。这两个模型都是由你用于学习的特征的相关系数的加法组成的。离核学习首先使用数据的一些小样本来估计这些系数，然后使用从你的数据中提取的越来越多的批次来更新这些系数。最终，尽管这个过程特别长，但你最终估计的系数不会与你能够将所有数据拟合到内存中时获得的系数有很大不同。
- en: 'How many such batches you have to use for your out-of-core modeling, and if
    you have to reuse them multiple times, is a matter of empirical experimentation:
    it depends on the problem and the data you are using. Though providing new batches
    of unseen data may simply prolong your training phase, having the algorithm see
    the same batches again may cause it to overfit. Unfortunately, in most situations,
    you need to reiterate the same batches multiple times because out-of-core learning
    is not as straightforward as when optimizing; it takes a long time, and you may
    need more passes on the same data, even if we are talking about massive amounts
    of it. Fortunately, you can rely on regularization techniques, such as L1 and
    L2 regularization, to avoid overfitting.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你的离核建模需要使用多少这样的批次，以及你是否需要多次重复使用它们，这是一个经验实验的问题：它取决于问题和你要使用的数据。尽管提供新的未见数据批次可能会简单地延长你的训练阶段，但让算法再次看到相同的批次可能会导致它过拟合。不幸的是，在大多数情况下，你需要多次重复相同的批次，因为离核学习不像优化那样简单；它需要很长时间，即使我们谈论的是大量数据，你也可能需要在同一数据上做更多的遍历。幸运的是，你可以依靠正则化技术，如L1和L2正则化，来避免过拟合。
- en: In listing 4.10, we reprise our logistic regression example and make it out-of-core.
    First, we split our data into a training set and a test set since it is complicated
    to create a cross-validation procedure when using out-of-core learning strategies.
    In real out-of-core learning settings, cross-validation is not just complicated
    but often infeasible because, in such settings, you often handle examples a single
    time. After all, they are streamed from sources and often discarded. The usual
    validation strategy is to collect a list of examples for testing purposes or to
    use a batch of every n-one as an out-of-sample testing batch. In our example,
    we prefer reserving a test set.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.10中，我们重新审视了我们的逻辑回归示例，并将其转换为离线处理。首先，我们将数据分割成训练集和测试集，因为在使用离线学习策略时创建交叉验证过程很复杂。在实际的离线学习设置中，交叉验证不仅复杂，而且往往不可行，因为在这样的设置中，你通常只处理一次示例。毕竟，它们是从源中流出的，并且通常被丢弃。通常的验证策略是收集一个用于测试目的的示例列表，或者使用每n个中的一个批次作为离样本测试批次。在我们的例子中，我们更喜欢保留一个测试集。
- en: Listing 4.10 Out-of-core logistic regression with L2 regularization
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.10 带L2正则化的离线逻辑回归
- en: '[PRE55]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ① Defines a function to generate batches of data for training
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个函数以生成用于训练的数据批次
- en: ② Generates batches of data indices for processing
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成用于处理的数据索引批次
- en: ③ Shuffles the sequence of examples if a random state is provided
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果提供了随机状态，则对示例序列进行洗牌
- en: ④ Yields batches of input features and corresponding labels
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 生成输入特征及其对应标签的批次
- en: ⑤ Creates an instance of the SGDClassifier model with logistic loss, averaging,
    L2 penalty, and alpha regularization
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个具有逻辑损失、平均、L2惩罚和alpha正则化的SGDClassifier模型实例
- en: ⑥ Splits the data and target into training and testing sets using an 80-20 ratio
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用80-20的比例将数据集和目标划分为训练集和测试集
- en: ⑦ Iterates through training data batches, fitting the column transformer on
    the first batch
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 遍历训练数据批次，在第一个批次上拟合列转换器
- en: ⑧ Uses partial fitting to train the model on the first batch, specifying the
    classes
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 使用部分拟合在第一个批次上训练模型，指定类别
- en: ⑨ Uses partial fitting to further train the model on subsequent batches
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用部分拟合在后续批次上进一步训练模型
- en: ⑩ Prints accuracy score of test data predictions
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 打印测试数据预测的准确度得分
- en: The train data is then split into multiple batches, and each batch is proposed
    for learning to the *stochastic gradient descent* (SGD) algorithm. SGD is not
    a stand-alone algorithm but an optimization procedure for linear models, optimizing
    the model weights by iteratively learning them from small batches of the data
    or even just single examples taken alone. It is based on the *gradient descent*
    optimization procedure and is also used in deep learning. Gradient descent starts
    with an initial guess for the model weights and computes the error. The next step
    involves computing the gradient of the error, which is obtained by taking the
    negative of the vector that contains the partial derivatives of the error with
    respect to the model weights. Since the gradient can be interpreted as taking
    the steepest descent on an error surface, a common example for gradient descent
    is always to figure it as descending from highs in the mountains to the lowest
    valley by taking the steepest path downward. The “mountains” in this analogy represent
    the error surface, and the “lowest valley” represents the minimum of the error
    function. Figure 4.12 visually represents this process by the progressive descent
    from a random high place to the lowest point in a bowl-shaped error curve.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将训练数据分割成多个批次，并将每个批次提交给*随机梯度下降*（SGD）算法进行学习。SGD不是一个独立的算法，而是线性模型的优化过程，通过迭代地从数据的小批次或甚至单独的示例中学习模型权重来优化模型权重。它基于*梯度下降*优化过程，也用于深度学习。梯度下降从模型权重的初始猜测开始，计算误差。下一步涉及计算误差的梯度，该梯度是通过取误差相对于模型权重的偏导数的向量的负值获得的。由于梯度可以解释为在误差表面上取最陡下降，梯度下降的一个常见例子就是将其想象为从山的高处沿着最陡的下坡路下降到最低的山谷。在这个类比中，“山脉”代表误差表面，“最低的山谷”代表误差函数的最小值。图4.12通过从随机的高点逐渐下降到碗形误差曲线的最低点来直观地表示这个过程。
- en: Besides the analogy, it is important to remember that the gradient determines
    how the weights should be adjusted to reduce the error at that step. Through repeated
    iterations, the error can be minimized by adjusting the model’s weights. However,
    how the weights are updated can significantly affect the outcome. If the updates
    are too large and decisive, the algorithm may take overly wide steps, potentially
    causing the model to overshoot the target and climb the error curve. In the worst-case
    scenario, this can result in a continuous worsening of the error, with no possibility
    of recovery. Conversely, taking smaller steps is generally safer but may be computationally
    burdensome. The size of such steps is decided by the learning rate, a parameter
    that regulates how the updates are done.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 除了类比之外，重要的是要记住，梯度决定了在这一步应该如何调整权重以减少误差。通过重复迭代，可以通过调整模型的权重来最小化误差。然而，权重的更新方式可能会显著影响结果。如果更新过大且果断，算法可能会采取过宽的步骤，可能导致模型超过目标并爬上误差曲线。在最坏的情况下，这可能导致误差持续恶化，没有恢复的可能。相反，采取较小的步骤通常更安全，但可能计算负担较重。这种步骤的大小由学习率决定，这是一个调节更新如何进行的参数。
- en: '![](../Images/CH04_F12_Ryan2.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F12_Ryan2.png)'
- en: Figure 4.12 Gradient descent optimization in action in a simple optimization
    landscape
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 在简单优化景观中的梯度下降优化过程
- en: Linear models can be optimized easily using gradient descent because their error
    surface is simple and bowl-shaped. However, more complex models like gradient
    boosting (which will be discussed in the next chapter) and deep learning architectures
    may encounter challenges in optimization due to their higher complexity, with
    interrelated parameters and a more complex error landscape. Depending on the starting
    point, as illustrated in figure 4.13, these models may become stuck in a local
    minimum or plateau during optimization, leading to suboptimal results.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型可以通过梯度下降法轻松优化，因为它们的误差表面简单且呈碗状。然而，像梯度提升（将在下一章讨论）和深度学习架构等更复杂的模型，由于它们的复杂性更高，参数相互关联且误差景观更复杂，可能在优化过程中遇到挑战。根据起始点，如图4.13所示，这些模型在优化过程中可能会陷入局部最小值或平台期，导致次优结果。
- en: '![](../Images/CH04_F13_Ryan2.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F13_Ryan2.png)'
- en: Figure 4.13 Gradient descent in a complex error landscape showing how local
    minima and plateaus can lead to less-than-optimal solutions
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 在复杂误差景观中的梯度下降，展示了局部最小值和平台期如何导致次优解
- en: Learning a linear model by SGD is made possible using Scikit-learn’s method
    `partial_fit`, which, after an informed start (the algorithm needs to know the
    target labels), can learn by partially fitting one batch after the other. The
    same procedure is repeated multiple times, called iterations or epochs, to consolidate
    and improve the learning, though repeating the same examples too often may also
    cause overfitting. The algorithm will see, though in a different order, the same
    examples multiple times and update its coefficients every time. To avoid abrupt
    changes in the coefficients, which frequently occur when an outlier is present
    in the batch, the updated coefficients are not substituted for the existing ones.
    Instead, they are averaged together, allowing a more gradual transition.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scikit-learn的`partial_fit`方法可以通过SGD学习线性模型，该方法在经过有信息量的起始（算法需要知道目标标签）后，可以通过逐批部分拟合来学习。相同的程序会重复多次，称为迭代或周期，以巩固和改进学习，尽管重复相同的例子太多次也可能导致过拟合。算法会看到相同的例子，尽管顺序不同，每次都会更新其系数。为了避免系数发生突然变化，这在批次中出现异常值时经常发生，更新的系数不会替换现有的系数。相反，它们会被平均在一起，允许更渐进的过渡。
- en: 'After all the learning process is completed, you will get the following result:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有学习过程后，你将得到以下结果：
- en: '[PRE56]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The result is quite comparable with in-core learning logistic regression. Out-of-core
    learning, though limited to only the simplest machine learning algorithms such
    as linear or logistic regression, is an effective way to train on your tabular
    data when too many samples cannot fit into memory. All deep learning solutions
    also use the idea of a stream of batches, and it will be discussed again in the
    chapters devoted to deep neural network methods for tabular data, together with
    strategies such as *early stopping*, a technique interrupting the iterations over
    data when necessary to avoid overfitting the data because of an excessive exposition
    of the algorithm to examples seen in previous iterations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与核心学习逻辑回归相当。虽然离核学习仅限于最简单的机器学习算法，如线性或逻辑回归，但当样本太多无法装入内存时，它是一种在表格数据上训练的有效方法。所有深度学习解决方案也使用批次的流式处理思想，这将在关于表格数据的深度神经网络方法章节中再次讨论，包括如*早期停止*这样的策略，这是一种在必要时中断数据迭代的迭代技术，以避免由于算法对先前迭代中看到的示例过度暴露而导致数据过拟合。
- en: We can now anticipate that a fundamental recipe of such learning strategies
    is the randomization of the order of the examples. Since the optimization is progressive,
    if your data is ordered in a specific way, it will cause a biased optimization,
    which may result in suboptimal learning. Repeating the same batches in the same
    order can negatively influence your results. Hence, randomizing the order is critical
    for a better-trained algorithm. Another important point with SGD, however, is
    the data preparation phase. In such a phase, you should include all feature rescaling
    operations, because the optimization process is sensible to the scale of features
    and all the feature engineering and feature interaction computations, and set
    it in a way as deterministic as possible since it could be difficult to use global
    parameters, such as the maximum/minimum or average and the standard deviation
    of a feature, when your data is split into multiple batches.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以预见，这种学习策略的基本配方是示例顺序的随机化。由于优化是渐进的，如果你的数据以特定的方式排序，它将导致偏差优化，这可能导致次优学习。以相同顺序重复相同的批次可能会对你的结果产生负面影响。因此，随机化顺序对于训练更好的算法至关重要。然而，另一个与随机梯度下降（SGD）相关的重要点是数据准备阶段。在这个阶段，你应该包括所有特征缩放操作，因为优化过程对特征的规模敏感，以及所有特征工程和特征交互计算，并且尽可能将其设置为确定性，因为当你的数据被分成多个批次时，使用全局参数，如特征的最大/最小值或平均值和标准差可能很困难。
- en: 4.3.6 Choosing your algorithm
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 选择你的算法
- en: As a general rule of thumb, you should first consider that machine learning
    algorithms scale differently based on how many rows and columns you have. Starting
    from the number of available rows, you must strictly resort to simple rule-based
    or statistical-based algorithms when operating with about or fewer than 10² rows
    of data. For up to 10³ rows, models based on linear combinations, such as linear
    and logistic regression, are best suited because they tend not to overfit the
    little data available. You usually cannot tell what algorithm will work better
    from about 10³ to 10⁴ – 10⁵ rows. Hence, it is all a matter of testing and experimenting.
    Here, deep learning solutions may outrun other choices only if there is some structure
    to exploit, such as an ordered series of information or a hierarchical structure.
    Up to 10⁹ rows, solutions from the gradient boosting family are likely the most
    effective. Again, you may find that something like out-of-core learning is a much
    better solution for specific problems, such as in the advertisement industry,
    where you have many fixed interactions that you need to estimate—for instance,
    between display devices, websites, and advertisements.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般规则，你应该首先考虑机器学习算法根据你拥有的行数和列数以不同的方式缩放。从可用的行数开始，当操作大约或少于10²行数据时，你必须严格使用基于规则或基于统计的简单算法。对于多达10³行，基于线性组合的模型，如线性回归和逻辑回归，是最适合的，因为它们往往不会过拟合少量数据。从大约10³行到10⁴-10⁵行，通常无法判断哪种算法效果更好。因此，这完全取决于测试和实验。在这里，深度学习解决方案只有在存在可以利用的结构时才能超越其他选择，例如有序的信息系列或层次结构。多达10⁹行，梯度提升家族的解决方案可能是最有效的。再次强调，你可能会发现，对于特定问题，如广告行业，其中你需要估计许多固定的交互，例如显示设备、网站和广告之间的交互，离核学习可能是一个更好的解决方案。
- en: 'Out-of-core learning refers to a learning strategy that certain machine learning
    algorithms can adopt when learning from data: instead of learning all at once
    from the data, they learn bit by bit from smaller samples of the data, the batches,
    or even from single examples, one by one, which is also mentioned as online learning.
    Finally, in our experience, in situations with datasets above 10⁹ rows, deep learning
    solutions, and some out-of-core learning algorithms tend to perform better because
    they can effectively deal with such an amount of data, whereas other machine learning
    algorithms may be forced to learn from subsamples from the data or find other
    suboptimal solutions.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 核外学习是指某些机器学习算法在从数据学习时可以采用的学习策略：不是一次性地从数据中学习，而是从较小的数据样本、批次或甚至从单个示例中一点一点地学习，这也被称作在线学习。最后，根据我们的经验，在数据集超过10⁹行的情况下，深度学习解决方案和一些核外学习算法往往表现更好，因为它们可以有效地处理如此大量的数据，而其他机器学习算法可能被迫从数据的子样本中学习或找到其他次优解。
- en: Regarding columns, we find that some algorithms need to scale better with datasets
    characterized by multiple columns, especially if they present sparse information—that
    is, many binary features. The sparser the datasets, which can be measured by the
    percentage of zeros values in relation to the total numeric values in the dataset,
    the earlier you may need to apply online learning algorithms or deep learning.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 关于列，我们发现一些算法需要更好地与具有多个列的数据集进行缩放，特别是如果它们呈现稀疏信息——也就是说，许多二元特征。数据集越稀疏，可以通过数据集中总数值中零值的百分比来衡量，你可能就越早需要应用在线学习算法或深度学习。
- en: However, apart from scalability reasons, which relate to memory and computational
    complexity, each machine learning solution also suits different needs in terms
    of model control, openness, and understandability of the solution. In such a way,
    the variety of needs in tabular problems and of models in machine learning defies
    the notion of one best algorithm that is all you need for your work. In other
    words, it is not just that you need to try more machine learning models because
    “there’s no free lunch,” as stated in the well-known theorem by David Wolpert
    and William Macready (see [http://www.no-free-lunch.org](http://www.no-free-lunch.org)
    for more details). More often than expected, there are cases where the underdog
    algorithm surprisingly beats the best-in-class algorithm. The necessity of more
    algorithms is mostly dictated because, as you change perspective on your problem
    as an artisan/artist creating their work from different angles, you may need different
    tools for the task.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了可扩展性原因，这与内存和计算复杂度相关之外，每个机器学习解决方案在模型控制、开放性和解决方案的可理解性方面也满足不同的需求。因此，表格问题中的需求多样性和机器学习中的模型多样性反驳了“一个最好的算法就足够了”的观念，这个观念是你工作所需的全部。换句话说，你之所以需要尝试更多的机器学习模型，不仅仅是因为“没有免费的午餐”，正如大卫·沃尔珀特和威廉·麦克雷德提出的著名定理所述（更多详情请见[http://www.no-free-lunch.org](http://www.no-free-lunch.org)）。出乎意料的是，往往存在一些情况下，劣势算法意外地击败了同类最佳算法。需要更多算法的原因主要是，当你作为一个工匠/艺术家从不同的角度看待你的问题时，你可能需要不同的工具来完成这项任务。
- en: In the next chapter, we will present a more powerful class of machine learning
    algorithms, the ensembles, and finally, the gradient boosting family and its successful
    and popular implementations, such as XGBoost and LightGBM.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一类更强大的机器学习算法，即集成算法，最后，梯度提升家族及其成功且流行的实现，如XGBoost和LightGBM。
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Determining what machine learning algorithm involves several factors: the number
    of examples and features, the expected performances, speed at prediction time,
    and interpretability. As a general rule of thumb,'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定机器学习算法涉及多个因素：示例和特征的数量、预期的性能、预测速度和可解释性。作为一个一般性的规则，
- en: Statistical machine learning is suitable for datasets with few cases.
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计机器学习适用于案例数量较少的数据集。
- en: Classical machine learning is suitable for datasets with a moderate number of
    cases.
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典机器学习适用于具有适度案例数量的数据集。
- en: Gradient boosting algorithms are particularly effective for datasets with a
    moderate to large number of cases.
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升算法对于具有适度到大量案例的数据集特别有效。
- en: Deep learning solutions are the most feasible and effective for datasets with
    massive amounts of data.
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大量数据的数据集，深度学习解决方案是最可行和最有效的。
- en: 'Scikit-learn is an open-source library for machine learning that offers a wide
    range of models for classification and regression, as well as functions for clustering,
    dimensionality reduction, preprocessing, and model selection. We can summarize
    its core advantages as follows:'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn是一个开源的机器学习库，提供了广泛的分类和回归模型，以及聚类、降维、预处理和模型选择等功能。我们可以将其核心优势总结如下：
- en: Consistent API across models
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型间一致的API
- en: Supports in-memory and out-of-core learning
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持内存和离核学习
- en: Supports working with pandas DataFrames
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持与pandas DataFrame一起工作
- en: Ideal for tabular problems
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于表格问题
- en: Easy to install
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于安装
- en: Extensive documentation
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丰富的文档
- en: 'Linear regression is the summation of weighted features that have been converted
    to numeric values (one-hot encoding for categorical features):'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归是转换为数值（对于分类特征使用one-hot编码）的加权特征的加权和：
- en: The algorithm finds optimal weight values (coefficients) to minimize the residual
    sum of squares between targets and predictions.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法找到最优的权重值（系数），以最小化目标值和预测值之间的残差平方和。
- en: Linear regression is easy to explain and understand how each feature contributes
    to the final result.
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归易于解释和理解每个特征如何对最终结果产生影响。
- en: A high correlation between features (multicollinearity) can cause conceptual
    misunderstanding.
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征之间的高相关性（多重共线性）可能导致概念上的误解。
- en: Linear regression is computationally simple and easy to implement.
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归在计算上简单且易于实现。
- en: Linear regression is limited in its ability to fit complex problems with nonlinear
    data unless features are carefully prepared beforehand with feature engineering,
    such as creating polynomial features, which can help linear regression capture
    nonlinear relationships.
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归在拟合具有非线性数据的复杂问题方面有限，除非在事先通过特征工程（如创建多项式特征）仔细准备特征，否则无法捕捉非线性关系。
- en: 'Regularization is used to prevent overfitting by reducing the complexity of
    a regression model and improving its generalization performance. There are two
    types of regularization:'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化通过降低回归模型的复杂性和提高其泛化性能来防止过拟合。有两种类型的正则化：
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus making some features irrelevant in the model.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1正则化（或Lasso回归）将许多系数推向零值，从而使得模型中的一些特征变得无关紧要。
- en: L2 regularization generally reduces the size of coefficients.
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化通常减小系数的大小。
- en: L1 regularization can be helpful for feature selection, while L2 regularization
    reduces overfitting when using many features while being faster to compute.
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1正则化对于特征选择有帮助，而L2正则化在许多特征的情况下减少过拟合，同时计算速度更快。
- en: Linear regression can be extended to classification problems using the logit
    function to transform the target and the Bernoulli conditional distribution to
    optimize the algorithm. This results in a logistic regression model that can be
    used for binary classification, multiclass, and multilabel problems. Logistic
    regression is easy to implement and understand but has the same limitations as
    linear regression.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归可以通过使用logit函数转换目标值和伯努利条件分布来扩展到分类问题，从而优化算法。这导致了一个逻辑回归模型，可用于二元分类、多类和多标签问题。逻辑回归易于实现和理解，但与线性回归具有相同的局限性。
- en: The same approach of transforming the target can be applied to other distributions
    as well, such as Poisson and Gamma, depending on the nature of the data. The resulting
    generalized linear models can be used for various real-world applications, such
    as climate modeling, risk modeling, and predictive maintenance. However, it’s
    important to note that the results may not be optimal without a proper understanding
    of the specific distribution applied to each situation.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将目标值转换的方法也可以应用于其他分布，例如泊松和伽马分布，具体取决于数据的性质。由此产生的广义线性模型可用于各种现实世界的应用，如气候建模、风险评估和预测性维护。然而，需要注意的是，如果没有正确理解每个情况下所应用的特定分布，结果可能不是最优的。
