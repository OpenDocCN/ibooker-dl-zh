- en: '11 Deploying an LLM on a Raspberry Pi: How low can you go?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 在树莓派上部署一个LLM：你能做到多低？
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Setting up a Raspberry Pi server on your local network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的本地网络上设置树莓派服务器
- en: Converting and quantizing a model to GGUF format
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型转换为GGUF格式并进行量化
- en: Serving your model as a drop-in replacement to the OpenAI GPT model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的模型作为OpenAI GPT模型的直接替代品提供服务
- en: What to do next and how to make it better
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来要做什么以及如何让它变得更好
- en: '*The bitterness of poor quality remains long after the sweetness of low price
    is forgotten.*—Benjamin Franklin'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*低价格带来的甜蜜很快就会忘记，而低质量带来的苦涩却会久久萦绕。* ——本杰明·富兰克林'
- en: 'Welcome to one of our favorite projects on this list: serving an LLM on a device
    smaller than it should ever be served on. In this project, we will be pushing
    to the edge of this technology. By following along, you’ll be able to really flex
    everything you’ve learned in this book. In this project, we’ll deploy an LLM to
    a Raspberry Pi, which we will set up as an LLM Service you can query from any
    device on your home network. For all the hackers out there, this exercise should
    open the doors to many home projects. For everyone else, it’s a chance to solidify
    your understanding of the limitations of using LLMs and appreciate the community
    that has made this possible.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们在这个列表中最喜欢的项目之一：在一个比它原本应该服务的设备还要小的设备上运行一个LLM。在这个项目中，我们将推动这项技术的极限。通过跟随这个项目，你将能够真正地运用在这本书中学到的所有知识。在这个项目中，我们将把一个LLM部署到树莓派上，我们将将其设置为LLM服务，你可以从家中网络上的任何设备查询。对于所有黑客来说，这个练习应该会打开许多家庭项目的门。对于其他人来说，这是一个巩固你对使用LLMs局限性的理解并欣赏使这一切成为可能的社会的机会。
- en: This is a practical project. In this chapter, we’ll dive into much more than
    LLMs, and there won’t be any model training or data focusing, so it is our first
    truly production-only project. What we’ll create will be significantly slower,
    less efficient, and less accurate than what you’re probably expecting, and that’s
    fine. Actually, it’s a wonderful learning experience. Understanding the difference
    between possible and useful is something many never learn until it smacks them
    across the face. An LLM running on a Raspberry Pi isn’t something you’ll want
    to deploy in an enterprise production system, but we will help you learn the principles
    behind it so you can eventually scale up to however large you’d like down the
    line.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实用的项目。在本章中，我们将深入探讨的内容远不止LLMs，而且不会有模型训练或数据聚焦，因此这是我们第一个真正只用于生产的项目。我们创建的内容将比你可能预期的要慢得多、效率低得多、准确性低得多，但这没关系。实际上，这是一个极好的学习经历。理解可能性和有用性之间的区别是许多人直到它狠狠地打在他们脸上之前都不会学到的东西。在树莓派上运行的LLM不是你想要在企业生产系统中部署的东西，但我们将帮助你学习其背后的原理，这样你最终可以扩展到你想要的任何大小。
- en: 11.1 Setting up your Raspberry Pi
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 设置你的树莓派
- en: Serving and inferencing on a Raspberry Pi despite all odds is doable, although
    we generally don’t recommend doing so other than to show that you can, which is
    the type of warning that is the telltale sign of a fun project, like figuring
    out how many marshmallows you can fit in your younger brother’s mouth. Messing
    with Raspberry Pis by themselves is pretty fun in general, and we hope that this
    isn’t the first time you’ve played with one. Raspberry Pis make great, cheap servers
    for your home. You can use them for ad blocking (Pi-Hole is a popular library)
    or media streaming your own personal library with services like Plex and Jellyfin.
    There are lots of fun projects. Because it’s fully customizable, if you can write
    a functional Python script, you can likely run it on a Raspberry Pi server for
    your local network to consume, which is what we are going to do for our LLM server.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管困难重重，但在树莓派上提供服务和推理是可行的，尽管我们通常不推荐这样做，除非是为了展示你可以做到，这是警告的一个典型迹象，表明这是一个有趣的项目，就像找出你能把多少棉花糖塞进你弟弟的嘴里一样。仅仅玩弄树莓派本身在一般情况下就很有趣，我们希望这并不是你第一次玩。树莓派是家庭中出色的、便宜的伺服器。你可以用它们来阻止广告（Pi-Hole是一个流行的库）或使用Plex和Jellyfin等服务流式传输你自己的个人媒体库。有很多有趣的项目。因为它完全可定制，如果你能写一个功能性的Python脚本，你很可能可以在你的本地网络服务器上运行它，这就是我们要为我们的LLM服务器所做的事情。
- en: 'You’ll just need three things to do this project: a Raspberry Pi with 8 GB
    of RAM, a MicroSD (at least 32 GB, but more is better), and a power supply. At
    the time of this writing, we could find several MicroSD cards with 1 TB of memory
    for $20, so hopefully, you get something much bigger than 32 GB. Anything else
    you purchase is just icing on the cake—for example, a case for your Pi. If you
    don’t have Wi-Fi, you’ll also need an ethernet cable to connect your Pi to your
    home network. We’ll show you how to remote into your Pi from your laptop once
    we get it up. In addition, if your laptop doesn’t come with a MicroSD slot, you’ll
    need some sort of adapter to connect it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要三样东西来完成这个项目：一个8 GB RAM的Raspberry Pi，一个至少32 GB的MicroSD卡（更多更好），以及一个电源。在撰写本文时，我们找到了一些1
    TB内存的MicroSD卡，价格为20美元，所以希望您能得到比32 GB更大的存储空间。您购买的其他任何东西都只是锦上添花——例如，为您的Pi购买一个外壳。如果您没有Wi-Fi，您还需要一根以太网线将Pi连接到您的家庭网络。一旦我们设置好，我们将向您展示如何从笔记本电脑远程连接到Pi。此外，如果您的笔记本电脑没有MicroSD插槽，您可能需要一个适配器来连接它。
- en: 'For the Raspberry Pi itself, we will be using the Raspberry Pi 5 8 GB model
    for this project. If you’d like to follow along, the exact model we’re using can
    be found here: [https://mng.bz/KDZg](https://mng.bz/KDZg). For the model we’ll
    deploy, you’ll need a single-board computer with at least 8 GB of RAM to follow
    along. As a fun fact, we have been successful in deploying models to smaller Pis
    with only 4GB of RAM, and plenty of other single-board alternatives to the Raspberry
    Pi are available. If you choose a different board, though, it might be more difficult
    to follow along exactly, so do so only if you trust the company. Some alternatives
    we recommend include Orange Pi, Zima Board, and Jetson, but we won’t go over how
    to set these up.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Raspberry Pi本身，我们将在这个项目中使用Raspberry Pi 5 8 GB型号。如果您想跟随操作，我们使用的确切型号可以在以下链接找到：[https://mng.bz/KDZg](https://mng.bz/KDZg)。对于我们将部署的型号，您需要一个至少8
    GB RAM的单板计算机来跟随操作。有趣的是，我们已经成功将模型部署到只有4GB RAM的小型Pi上，并且还有许多其他单板计算机作为Raspberry Pi的替代品可供选择。如果您选择不同的板子，可能更难精确跟随，所以只有当您信任该公司时才这样做。我们推荐的一些替代品包括Orange
    Pi、Zima Board和Jetson，但我们不会详细介绍如何设置这些设备。
- en: You won’t need to already know how to set up a Pi. We will walk you through
    all the steps, assuming this is your first Raspberry Pi project. A Pi is literally
    just hardware and an open sandbox for lots of projects, so we will first have
    to install an operating system (OS). After that, we’ll install the necessary packages
    and libraries, prepare our LLM, and finally serve it as a service you can ping
    from any computer in your home network and get generated text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要事先知道如何设置Pi。我们将引导您完成所有步骤，假设这是您的第一个Raspberry Pi项目。Pi实际上只是硬件和众多项目的开放沙盒，因此我们首先需要安装一个操作系统（OS）。之后，我们将安装必要的软件包和库，准备我们的LLM，并将其最终作为一项服务提供，您可以从家庭网络中的任何计算机上ping它并获取生成的文本。
- en: 11.1.1 Pi Imager
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 Pi Imager
- en: 'To start off, Pis don’t usually come with an OS installed, and even if yours
    did, we’re going to change it. Common distributions like Rasbian OS or Ubuntu
    are too large and take too much RAM to run models at their fastest. To help us
    with this limitation, Raspberry Pi’s makers have released a free imaging software
    called the Pi Imager that you can download on your laptop from here: [https://www.raspberrypi.com/software/](https://www.raspberrypi.com/software/).
    If you already have the imager, we recommend updating it to a version higher than
    1.8 since we are using a Pi 5.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Pi通常不预装操作系统，即使您的设备预装了，我们也会更换它。常见的发行版，如Rasbian OS或Ubuntu，体积太大，占用太多RAM，无法以最快速度运行模型。为了帮助我们克服这一限制，Raspberry
    Pi的制造商发布了一款名为Pi Imager的免费镜像软件，您可以从以下链接在您的笔记本电脑上下载：[https://www.raspberrypi.com/software/](https://www.raspberrypi.com/software/)。如果您已经有了镜像软件，我们建议将其更新到1.8版本以上，因为我们正在使用Pi
    5。
- en: Once you have it, plug the microSD into the computer where you’ve downloaded
    the Pi Imager program. (If you aren’t sure how to do this, search online for the
    USB 3.0 microSD Card Reader.) Open the imager and select the device; for us, that’s
    Raspberry Pi 5\. This selection will limit the OS options to those available for
    the Pi 5\. Then you can select the Raspberry Pi OS Lite 64-bit for your operating
    system. *Lite* is the keyword you are looking for, and you will likely have to
    find it in the Raspberry Pi OS (Other) subsection. Then select your microSD as
    your storage device. The actual name will vary depending on your setup. Figure
    11.1 shows an example of the Imager software with the correct settings. As a note,
    the Ubuntu Server is also a good operating system that would work for our project,
    and we’d recommend it. It’ll have a slightly different setup, so if you want to
    follow along, stick with a Raspberry Pi OS Lite.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了它，将 microSD 插入您下载 Pi Imager 程序的电脑。如果您不确定如何操作，请在网上搜索 USB 3.0 microSD 卡读卡器。打开镜像程序，选择设备；对我们来说，那就是
    Raspberry Pi 5。这个选择将限制操作系统选项仅限于适用于 Pi 5 的选项。然后您可以选择 Raspberry Pi OS Lite 64 位作为您的操作系统。“Lite”是您要找的关键词，您可能需要在
    Raspberry Pi OS（其他）子部分中找到它。然后选择您的 microSD 作为存储设备。实际名称将根据您的设置而有所不同。图 11.1 展示了带有正确设置的镜像软件示例。作为备注，Ubuntu
    Server 也是一个适合我们项目的良好操作系统，我们推荐它。它的设置会有所不同，所以如果您想跟上来，请坚持使用 Raspberry Pi OS Lite。
- en: '![figure](../Images/11-1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-1.png)'
- en: Figure 11.1 Raspberry Pi Imager set to the correct device, with the headless
    (Lite) operating system and the correct USB storage device selected
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1 将 Raspberry Pi Imager 设置为正确的设备，选择了无头（Lite）操作系统和正确的 USB 存储设备
- en: WARNING  And as a warning, make sure that you’ve selected the microSD to image
    the OS—please do not select your main hard drive.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 警告  并且作为警告，请确保您已选择了 microSD 来镜像操作系统——请勿选择您的主硬盘。
- en: Once you are ready, navigate forward by selecting the Next button, and you should
    see a prompt asking about OS customizations, as shown in figure 11.2\. We will
    set this up, so click the Edit Settings button, and you should see a settings
    page.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 准备就绪后，通过选择“下一步”按钮进行导航，您应该会看到一个关于操作系统定制的提示，如图 11.2 所示。我们将设置它，所以点击“编辑设置”按钮，您应该会看到一个设置页面。
- en: '![figure](../Images/11-2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-2.png)'
- en: Figure 11.2 Customizing our Raspberry Pi OS settings. Select Edit Settings.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.2 自定义我们的 Raspberry Pi OS 设置。选择“编辑设置”。
- en: Figure 11.3 shows an example of the settings page. We’ll give the Pi server
    a hostname after the project, llmpi. We’ll set a username and password and configure
    the Wi-Fi settings to connect to our home network. This is probably the most important
    step, so make sure that you’re set up for the internet, either by setting up your
    Wi-Fi connection in settings or via ethernet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 展示了设置页面的示例。我们将在项目名称之后为 Pi 服务器设置一个主机名，llmpi。我们将设置用户名和密码，并配置 Wi-Fi 设置以连接到我们的家庭网络。这可能是最重要的步骤，所以请确保您已为互联网设置好，无论是通过在设置中设置
    Wi-Fi 连接还是通过以太网。
- en: '![figure](../Images/11-3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-3.png)'
- en: Figure 11.3 Example screenshot of the settings page with correct and relevant
    information
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.3 设置页面截图示例，包含正确和相关信息
- en: Just as important as setting up the internet, we want to enable SSH, or none
    of the subsequent steps will work. To do this, go to the Services tab and select
    Enable SSH, as seen in figure 11.4\. We will use password authentication, so make
    sure you’ve set an appropriate username and password and are not leaving it to
    the default settings. You don’t want anyone with bad intentions to have super
    easy access to your Pi.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与设置互联网一样重要的是，我们希望启用 SSH，否则后续的步骤将无法工作。为此，转到“服务”选项卡并选择“启用 SSH”，如图 11.4 所示。我们将使用密码认证，所以请确保您已设置了合适的用户名和密码，并且没有使用默认设置。您不希望任何有恶意的人轻易访问您的
    Pi。
- en: At this point, we are ready to image. Move forward through the prompts, and
    the imager will install the OS onto your SD card. This process can take a few
    minutes but is usually over pretty quickly. Once your SD has your OS on it, you
    can remove it safely from your laptop. Put the microSD card in your Pi, and turn
    it on! If everything was done correctly, your Pi should automatically boot up
    and connect to your Wi-Fi.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好创建镜像。按照提示操作，镜像程序将把操作系统安装到您的 SD 卡上。这个过程可能需要几分钟，但通常很快就会完成。一旦您的 SD
    卡上有了操作系统，您就可以安全地从笔记本电脑上取下它。将 microSD 卡插入您的 Pi 中，并打开它！如果一切操作正确，您的 Pi 应该会自动启动并连接到您的
    Wi-Fi。
- en: '![figure](../Images/11-4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-4.png)'
- en: Figure 11.4 Make sure you select Enable SSH.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4 确保你选择了启用SSH。
- en: 11.1.2 Connecting to Pi
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 连接到树莓派
- en: We will use our little Pi like a small server. What’s nice about our setup is
    that you won’t need to find an extra monitor or keyboard to plug into your Pi.
    Of course, this setup comes with the obvious drawback that we can’t see what the
    Pi is doing, nor do we have an obvious way to interact with it. Don’t worry; that’s
    why we set up SSH. Now we’ll show you how to connect to your Pi from your laptop.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的小树莓派作为一个小型服务器。我们设置的好处是，你不需要为树莓派找到额外的显示器或键盘。当然，这种设置有一个明显的缺点，那就是我们无法看到树莓派在做什么，也没有明显的方法与之交互。别担心；这就是我们设置SSH的原因。现在我们将向你展示如何从你的笔记本电脑连接到你的树莓派。
- en: The first thing we’ll need to do is find the Raspberry Pi’s IP address. An IP
    address is a numerical label to identify a computer on a network. The easiest
    way to see new devices that have connected to the internet you’re using is through
    the router’s software. See figure 11.5\. If you can access your router, you can
    go to its IP address in a browser. The IP address is typically 192.168.86.1 or
    192.168.0.1; the type of router usually sets this number and can often be found
    on the router itself. You’ll then need to log in to your router, where you can
    see all devices connected to your network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要做的是找到树莓派的IP地址。IP地址是一个数字标签，用于在网络中标识一台计算机。查看你使用的互联网上连接的新设备的最简单方法是通过路由器的软件。见图11.5。如果你可以访问你的路由器，你可以在浏览器中访问其IP地址。IP地址通常是192.168.86.1或192.168.0.1；路由器的类型通常设置这个数字，并且通常可以在路由器本身上找到。然后你需要登录到你的路由器，在那里你可以看到连接到你的网络的所有设备。
- en: '![figure](../Images/11-5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-5.png)'
- en: Figure 11.5 Example Google Home router interface with several devices listed
    to discover their IP addresses
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5 示例谷歌智能家居路由器界面，列出了几个设备以发现它们的IP地址
- en: If you don’t have access to your router, which many people don’t, you’re not
    out of luck. The next easiest way is to ignore everything we said in the previous
    paragraph and connect your Pi to a monitor and keyboard. Run `$` `ifconfig` or
    `$` `ip` `a`, and then look for the `inet` parameter. These commands will output
    devices on your local network and their IP addresses. Figures 11.6 and 11.7 demonstrate
    running these commands and highlight what you are looking for. If you don’t have
    access to an extra monitor, well, things will get a bit tricky, but it’s still
    possible. However, we don’t recommend going down this path if you can avoid it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法访问你的路由器，很多人都是这种情况，你并不倒霉。下一个最简单的方法就是忽略我们之前段落中说的所有内容，并将你的树莓派连接到显示器和键盘上。运行
    `$` `ifconfig` 或 `$` `ip` `a`，然后查找 `inet` 参数。这些命令将输出你本地网络上的设备及其IP地址。图11.6和11.7展示了运行这些命令并突出显示你要找的内容。如果你没有额外的显示器，那么事情会变得有点棘手，但仍然可行。然而，如果你能避免，我们不推荐走这条路。
- en: '![figure](../Images/11-6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-6.png)'
- en: Figure 11.6 Example of running `ifconfig`. The IP address of our Pi (`inet`)
    is highlighted for clarity.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6 运行 `ifconfig` 的示例。为了清晰起见，我们的树莓派的IP地址（`inet`）被突出显示。
- en: '![figure](../Images/11-7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-7.png)'
- en: Figure 11.7 Example of running `ip` `a`. The IP address of our Pi (`inet`) is
    highlighted for clarity.
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.7 运行 `ip` `a` 的示例。为了清晰起见，我们的树莓派的IP地址（`inet`）被突出显示。
- en: To scan your local network for IP addresses, open a terminal on your laptop,
    and run that same command (`$` `ifconfig`), or if you are on a Windows, `$` `ipconfig`.
    If you don’t have `ifconfig`, you can install it with `$` `sudo` `apt` `install`
    `net-tools`. We didn’t mention this step before because it should have already
    been installed on your Pi.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要扫描你的本地网络以查找IP地址，在你的笔记本电脑上打开一个终端，并运行相同的命令 (`$` `ifconfig`)，如果你使用的是Windows，则运行
    `$` `ipconfig`。如果你没有 `ifconfig`，你可以使用 `$` `sudo` `apt` `install` `net-tools` 来安装它。我们之前没有提到这一步，因为它应该已经安装在你的树莓派上了。
- en: If you already recognize which device the Pi is, that’s awesome! Just grab the
    `inet` parameter for that device. More likely, though, you won’t, and there are
    a few useful commands you can use if you know how. Use the command `$` `arp` `-a`
    to view the list of all IP addresses connected to your network and the command
    `$` `nslookup` `$IP_ADDRESS` to get the hostname for the computer at the IP address
    you pass in—you’d be looking for the hostname `raspberry`, but we’ll skip all
    that. We trust that if you know how to use these commands, you won’t be reading
    this section of the book. Instead, we’ll use caveman problem-solving, which means
    we’ll simply turn off the Pi, run our `$` `ifconfig` command again, and see what
    changes, specifically what disappears. When you turn it back on, your router might
    assign it a different IP address than last time, but you should still be able
    to `diff` the difference and find it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经知道 Pi 是哪个设备，那太棒了！只需获取该设备的 `inet` 参数。然而，更有可能的是你不知道，如果你知道如何使用，这里有一些有用的命令。使用命令
    `$` `arp` `-a` 来查看连接到你的网络的所有 IP 地址，使用命令 `$` `nslookup` `$IP_ADDRESS` 来获取你传入的 IP
    地址对应的计算机的主机名——你将寻找主机名 `raspberry`，但我们将跳过所有这些。我们相信如果你知道如何使用这些命令，你不会阅读这本书的这一部分。相反，我们将使用原始人问题解决方法，这意味着我们将简单地关闭
    Pi，再次运行 `$` `ifconfig` 命令，看看有什么变化，特别是什么消失了。当你重新启动它时，你的路由器可能会分配一个与上次不同的 IP 地址，但你仍然应该能够
    `diff` 差异并找到它。
- en: 'Alright, we know that was potentially a lot just to get the IP address, but
    once you have it, the next step is easy. To SSH into it, you can run the `ssh`
    command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们知道为了获取 IP 地址可能需要做很多工作，但一旦你有了它，下一步就简单了。要 SSH 连接到它，你可以运行 `ssh` 命令：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Replace `username` with the username you created (it should be `pi` if you are
    following along with us), and replace the `0`s with the IP address of your Pi.
    Since this is the first time connecting to a brand-new device, you’ll be prompted
    to fingerprint to establish the connection and authenticity of the host. Then
    you’ll be prompted to put in a password. Enter the password you set in the imager
    before. If you didn’t set a password, it’s `pi` by default, but we trust you didn’t
    do that, right?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `username` 替换为你创建的用户名（如果你在跟随我们操作，应该是 `pi`），并将 `0`s 替换为你的 Pi 的 IP 地址。由于这是第一次连接到一个全新的设备，你将需要指纹验证以建立连接和主机真实性。然后你将被提示输入密码。输入你在
    imager 中设置的密码。如果你没有设置密码，默认密码是 `pi`，但我们相信你没有这么做，对吧？
- en: With that, you should be remotely connected to your Pi and see the Pi’s terminal
    reflected in your computer’s terminal, as shown in figure 11.8\. Nice job!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，你应该已经远程连接到你的 Pi，并在你的计算机终端中看到 Pi 的终端，如图 11.8 所示。做得好！
- en: '![figure](../Images/11-8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-8.png)'
- en: Figure 11.8 Terminal after a successfully secure shell into your Raspberry Pi.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.8 成功安全壳连接到 Raspberry Pi 的终端。
- en: 11.1.3 Software installations and updates
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 软件安装和更新
- en: 'Now that our Pi is up and we’ve connected to it, we can start the installation.
    The first command is well known and will simply update our system:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了 Pi 并连接到了它，我们可以开始安装。第一个命令是众所周知的，它将简单地更新我们的系统：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It can take a minute, but once that finishes running, congratulations! You
    now have a Raspberry Pi server on which you can run anything you want to at this
    point. It’s still a blank slate, so let’s change that and prepare it to run our
    LLM server. We first want to install any dependencies we need. Depending on your
    installation, this may include `g++` or `build-essentials`. We need just two:
    `git` and `pip`. Let’s start by installing them, which will make this whole process
    so much easier:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一分钟的时间，但一旦完成，恭喜！你现在有一个 Raspberry Pi 服务器，你可以在上面运行任何你想要的东西。它仍然是一张白纸，所以让我们改变它，准备运行我们的
    LLM 服务器。我们首先想要安装我们需要的任何依赖项。根据你的安装，这可能包括 `g++` 或 `build-essentials`。我们只需要两个：`git`
    和 `pip`。让我们先安装它们，这将使整个过程变得容易得多：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we can clone the repo that will be doing the majority of the work here:
    Llama.cpp. Let’s clone the project into your Pi and build the project. To do that,
    run the following commands:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以克隆将在这里做大部分工作的 repo：Llama.cpp。让我们将项目克隆到你的 Pi 上并构建项目。为此，运行以下命令：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A note on llama.cpp
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于 llama.cpp 的注意事项
- en: Llama.cpp, like many open source projects, is a project that is much more interested
    in making things work than necessarily following best engineering practices. Since
    you are cloning the repo in its current state, but we wrote these instructions
    in a previous state, you may run into problems we can’t prepare you for. Llama.cpp
    doesn’t have any form of versioning either. After cloning the repo, we recommend
    you run
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Llama.cpp，就像许多开源项目一样，是一个更关注让事情工作而不是必然遵循最佳工程实践的项目。由于你正在以当前状态克隆仓库，但我们是在之前的状态下编写这些说明的，你可能会遇到我们无法为你准备的问题。Llama.cpp也没有任何形式的版本控制。在克隆仓库后，我们建议你运行
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This command will checkout the exact `git` `commit` we used so you can run everything
    in the exact same version of llama.cpp. We tested this on Mac, Windows 10, Ubuntu,
    Debian, and, of course, both a Raspberry Pi 4 and 5\. We don’t expect any problems
    on most systems with this version.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将检出我们使用的确切`git` `commit`，这样你就可以在llama.cpp的相同版本上运行一切。我们在Mac、Windows 10、Ubuntu、Debian以及当然，Raspberry
    Pi 4和5上进行了测试。我们预计大多数系统使用这个版本不会出现任何问题。
- en: 'Now that we have the repo, we must complete a couple of tasks to prepare it.
    First, to keep our Pi clean, let’s create a virtual environment for our repo and
    activate it. Once we have our Python environment ready, we’ll install all the
    requirements. We can do so with the following commands:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了仓库，我们必须完成几个任务来准备它。首先，为了保持我们的Pi整洁，让我们为我们的仓库创建一个虚拟环境并激活它。一旦我们的Python环境准备就绪，我们将安装所有必需的依赖项。我们可以使用以下命令来完成：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Llama.cpp is written in C++, which is a compiled language. That means we have
    to compile all the dependencies to run on our hardware and architecture. Let’s
    go ahead and build it. We do that with one simple command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Llama.cpp是用C++编写的，这是一种编译型语言。这意味着我们必须编译所有依赖项以在硬件和架构上运行。让我们继续构建它。我们只需一个简单的命令就可以做到这一点：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A note on setting up
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于设置的注意事项
- en: 'If you’re performing this setup in even a slightly different environment, using
    CMake instead of Make can make all the difference! For example, even running on
    Ubuntu, we needed to use CMake to specify the compatible version of CudaToolkit
    and where that nvcc binary was stored in order to use CuBLAS instead of vanilla
    CPU to make use of a CUDA-integrated GPU. The original creator (Georgi Gerganov,
    aka ggerganov) uses CMake when building for tests because it requires more specifications
    than Make. For reference, here’s the CMake build command ggerganov currently uses;
    you can modify it as needed:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个略有不同的环境中执行此设置，使用CMake而不是Make可能会产生很大的差异！例如，即使在Ubuntu上运行，我们也需要使用CMake来指定兼容的CudaToolkit版本以及nvcc二进制文件存储的位置，以便使用CuBLAS而不是普通的CPU来利用CUDA集成的GPU。原始创建者（Georgi
    Gerganov，又名ggerganov）在构建测试时使用CMake，因为它需要比Make更多的指定。为了参考，以下是ggerganov目前使用的CMake构建命令；你可以根据需要修改它：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we just need to get our model, and we’ll be ready to move forward. The
    model we’ve picked for this project is Llava-v1.6-Mistral-7B, which we will download
    using the `huggingface-cli`, like we’ve done in other chapters. Go ahead and run
    the following command to pull the LLaVA model, its accompanying tokenizer, and
    the config files:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需要获取我们的模型，然后我们就可以继续前进。我们为这个项目选择的模型是Llava-v1.6-Mistral-7B，我们将使用`huggingface-cli`来下载它，就像我们在其他章节中所做的那样。现在运行以下命令来拉取LLaVA模型、其伴随的标记器以及配置文件：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have our model and tokenizer information, we’re ready to turn our
    LLM into something usable for devices as small as an Android phone or Raspberry
    Pi.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型和标记器信息，我们就可以将我们的LLM转换成适用于像Android手机或Raspberry Pi这样小型的设备了。
- en: 11.2 Preparing the model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 准备模型
- en: Now that we have a model, we need to standardize it so that the C++ code in
    the repo can interface with it in the best way. We will convert the model from
    the safetensor format, which we downloaded into .gguf. We’ve used GGUF models
    before, as they are extensible, quick to load, and contain all of the information
    about the model in a single model file. We also download the tokenizer information,
    which goes into our .gguf model file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型，我们需要将其标准化，以便仓库中的C++代码可以以最佳方式与之交互。我们将模型从safetensor格式转换为.gguf格式。我们之前已经使用过GGUF模型，因为它们是可扩展的、加载速度快，并且在一个模型文件中包含了关于模型的所有信息。我们还下载了标记器信息，这些信息将包含在我们的.gguf模型文件中。
- en: 'Once ready, we can convert our safetensor model to GGUF with the `convert.py`
    script:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备就绪，我们可以使用`convert.py`脚本来将我们的safetensor模型转换为GGUF：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This code will convert all the weights into one .gguf checkpoint that is the
    same size on disk as all of the .safetensors files we downloaded combined. That’s
    now two copies of whatever we’ve downloaded, which is likely one too many if your
    microSD card is rather small. Once you have the .gguf checkpoint, we recommend
    you either delete or migrate the original model files somewhere off of the Pi
    to reclaim that memory, which could look like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会将所有权重转换成一个与下载的所有.safetensors文件大小相同的.gguf检查点。现在我们有两个我们下载的副本，如果你的microSD卡相当小，这可能是多余的。一旦你有了.gguf检查点，我们建议你删除或迁移原始模型文件到Pi之外的地方以回收内存，这可能看起来像这样：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once our model is in the correct single-file format, we can make it smaller.
    Now memory constraints come into play. One reason we picked a 7B parameter model
    is that in the quantized `q4_K_M` format (we’ll talk about different llama.cpp-supported
    quantized formats later), it’s a little over 4 GB on disk, which is more than
    enough for the 8 GB Raspberry Pi to run effectively. Run the following command
    to quantize the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型以正确的单文件格式就绪后，我们可以将其变小。现在内存限制开始发挥作用。我们选择7B参数模型的一个原因是，在量化`q4_K_M`格式中（我们稍后会讨论llama.cpp支持的量化格式），它在磁盘上略大于4GB，这对于8GB的Raspberry
    Pi来说已经足够有效。运行以下命令来量化模型：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We won’t lie: it’ll be a bit of a waiting game while the quantization methodology
    is applied to all of the model weights, but when it’s finished, you’ll have a
    fresh quantized model ready to be served.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会撒谎：在将量化方法应用于所有模型权重时，这将是一场等待游戏，但一旦完成，你将有一个全新的量化模型准备好提供服务。
- en: Having trouble?
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 遇到麻烦了吗？
- en: 'While we’ve tested these instructions in a multitude of environments and hardware,
    you might still find yourself stuck. Here’s some troubleshooting advice you can
    try that has helped us out:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经在这许多环境和硬件上测试了这些说明，但你可能仍然会遇到困难。以下是一些你可以尝试的故障排除建议，这些建议对我们有所帮助：
- en: '*Redownload the model*. These models are large, and if your Pi had any internet
    connection problems during the download, you may have a corrupted model. You may
    try connecting with an ethernet cable instead of Wi-Fi if your connection is spotty.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重新下载模型*。这些模型很大，如果你的Pi在下载过程中有任何互联网连接问题，你可能有一个损坏的模型。如果你的连接不稳定，你可以尝试使用以太网线而不是Wi-Fi。'
- en: '*Recompile your dependencies*. The easiest way to recomplie your dependencies
    is to run `make` `clean` and then `make` again. You might try using `cmake` or
    checking out different options.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重新编译你的依赖项*。重新编译你的依赖项最简单的方法是运行`make clean`然后再次运行`make`。你也可以尝试使用`cmake`或检查不同的选项。'
- en: '*Reboot your Pi*. Rebooting is a classic but tried-and-true solution, especially
    if you are dealing with memory problems (which we don’t have a lot of for the
    task at hand.). You can reboot while in SSH with `sudo` `reboot.`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重启你的Pi*。重启是一个经典且经过验证的解决方案，特别是如果你正在处理内存问题（对于手头的任务，我们并没有太多内存问题）。你可以在SSH中使用`sudo
    reboot`来重启。'
- en: '*Run through these steps on your computer*. You’re likely to run into fewer
    problems on better hardware, and it can be useful to know what an easy path looks
    like before trying to make it work on an edge device.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在你的计算机上运行这些步骤*。你可能在更好的硬件上遇到的问题更少，在尝试在边缘设备上工作之前，了解一条简单路径是什么样的可能很有用。'
- en: '*Download an already prepared model*. While we encourage you to go through
    the steps of converting and quantizing yourself, you can usually find most open
    source models already quantized to any and every format. So if you aren’t worried
    about finetuning it, you should be in luck. For us, we are in said luck.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载一个已准备好的模型*。虽然我们鼓励你自己进行转换和量化，但通常你可以在任何格式中找到大多数已经量化的开源模型。所以如果你不担心微调它，你应该会很幸运。对我们来说，我们就是这种情况。'
- en: 'If you get stuck but want to keep moving forward, you can download a quantized
    version of the model with the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到了困难但还想继续前进，你可以使用以下命令下载模型的量化版本：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 11.3 Serving the model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 服务器模型
- en: 'We’re finally here, serving the model! With llama.cpp, creating a service for
    the model is incredibly easy, and we’ll get into some slightly more complex tricks
    in a bit, but for now, revel in what you’ve done:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于到了这里，开始提供服务！使用llama.cpp，为模型创建服务非常简单，我们稍后会详细介绍一些稍微复杂一些的技巧，但现在，享受你所做的一切：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Be sure to use your Pi’s IP address, and the API key can be any random string
    to provide a small layer of security. That’s it! You now have an LLM running on
    a Raspberry Pi that can be queried from any computer on your local network. Note
    that the server can take a long time to boot up on your Pi, as it loads in the
    model. Don’t worry too much; give it time. Once ready, let’s test it out with
    a quick demo.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要使用你的 Pi 的 IP 地址，API 密钥可以是任何随机的字符串，以提供一层小的安全性。就这样！你现在有一个在 Raspberry Pi 上运行的
    LLM，可以从你的本地网络上的任何计算机查询。请注意，服务器在你的 Pi 上启动可能需要很长时间，因为它正在加载模型。不要过于担心；给它一些时间。一旦准备好，让我们通过一个快速演示来测试它。
- en: For this demo, let’s say you’ve already integrated an app pretty deeply with
    OpenAI’s Python package. In listing 11.1, we show you how to point this app to
    your Pi LLM service instead. We’ll continue to use OpenAI’s Python bindings and
    point it to our service instead. We do this by updating the `base_url` to our
    Pi’s IP address and using the same API key we set when we created the server.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，假设你已经将一个应用程序与 OpenAI 的 Python 包深度集成。在列表 11.1 中，我们向你展示了如何将这个应用程序指向你的 Pi
    LLM 服务。我们将继续使用 OpenAI 的 Python 绑定，并将其指向我们的服务。我们通过更新 `base_url` 到我们的 Pi 的 IP 地址，并使用我们创建服务器时设置的相同的
    API 密钥来做这件事。
- en: Also, notice that we’re calling the `gpt-3.5-turbo` model. OpenAI has different
    processes for calling different models. You can easily change that if you don’t
    like typing those letters, but it doesn’t really matter. You’ll just have to figure
    out how to change the script for whichever model you want to feel like you’re
    calling (again, you’re not actually calling ChatGPT).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意我们正在调用 `gpt-3.5-turbo` 模型。OpenAI 对不同模型的调用有不同的流程。如果你不喜欢输入这些字母，你可以轻松地更改它，但这并不重要。你只需要弄清楚如何更改脚本，以便你感觉像是在调用（再次强调，你实际上并没有调用
    ChatGPT）。
- en: Listing 11.1 OpenAI but not ChatGPT
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.1 OpenAI 但不是 ChatGPT
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You don’t need code to interact with your server. The server script comes with
    a built-in minimal GUI, and you can access it on your local network with a phone
    or your laptop by pointing a browser to your Pi’s IP address. Be sure to include
    the port 8080\. You can see an example of this in figure 11.9\.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要代码来与你的服务器交互。服务器脚本自带内置的最小 GUI，你可以通过将浏览器指向你的 Pi 的 IP 地址在你的本地网络上访问它。务必包括端口
    8080。你可以在图 11.9 中看到这个示例。
- en: This process will allow you to interface with the LLM API you’re running in
    a simple chat window. We encourage you to play around with it a bit. Since you’re
    running on a Raspberry Pi, the fastest you can expect this to go is about five
    tokens per second, and the slowest is, well, SLOW. You’ll immediately understand
    why normal people don’t put LLMs on edge devices.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程将允许你通过简单的聊天窗口与运行的 LLM API 进行接口。我们鼓励你稍微玩一下。由于你运行在 Raspberry Pi 上，你期望的最快速度大约是每秒五个标记，而最慢的速度，嗯，很慢。你将立即理解为什么普通人不会在边缘设备上放置
    LLM。
- en: '![figure](../Images/11-9.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-9.png)'
- en: Figure 11.9 Running an LLM on your Pi and interacting with it through the llama.cpp
    server
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.9 在你的 Pi 上运行 LLM 并通过 llama.cpp 服务器与之交互
- en: At this point, you may be wondering why we were so excited about this project.
    We made a bunch of promises about what you’d learn, but this chapter is the shortest
    in the book, and the majority of what we did here was download other people’s
    repos and models. *Welcome to production.*
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道为什么我们对这个项目如此兴奋。我们承诺你会学到很多东西，但这一章是书中最短的，我们在这里做的绝大多数工作都是下载其他人的代码库和模型。*欢迎来到生产阶段*。
- en: 'This is ultimately what most companies will ask you to do: download some model
    that someone heard about from a friend and put it on hardware that’s way too small
    and isn’t meant to run it. You should now be ready to hack together a prototype
    of exactly what they asked for within about 20 to 30 minutes. Being able to iterate
    quickly will allow you to go back and negotiate with more leverage, demonstrating
    why you need more hardware, data to train on, RAG, or any other system to make
    the project work. Building a rapid proof of concept and then scaling up to fit
    the project’s needs should be a key workflow for data scientists and ML engineers.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是大多数公司会要求你做的事情：下载一些从朋友那里听说过的模型，并将其放在远小于其运行所需的硬件上。你现在应该准备好在约 20 到 30 分钟内拼凑出他们要求的原型。能够快速迭代将允许你回去进行更有力的谈判，展示你需要更多硬件、用于训练的数据、RAG
    或任何其他系统来使项目成功。构建快速的概念验证，然后扩展以满足项目需求，应该是数据科学家和机器学习工程师的关键工作流程。
- en: One huge advantage of following the rapid proof of concept workflow demo-ed
    here is visibility. You can show that you can throw something amazing together
    extremely fast, which (if your product managers are good) should add a degree
    of trust when other goals are taking longer than expected. They’ve seen that if
    you want something bad in production, you can do that in a heartbeat. The good
    stuff that attracts and retains customers takes time with real investment into
    data and research.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随这里演示的快速原型验证工作流程的一个巨大优势是可见性。你可以展示你能够极快地组合出令人惊叹的东西，如果你们的产品经理表现良好，这应该会在其他目标比预期花费更多时间时增加一定的信任度。他们已经看到，如果你在生产中急需某样东西，你可以在一瞬间就做到。吸引并留住顾客的优质内容需要时间，并且需要对数据和研究的真实投资。
- en: 11.4 Improvements
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 改进
- en: Now that we’ve walked through the project once, let’s talk about ways to modify
    this project. For clarity, we chose to hold your hand and tell you exactly what
    commands to run so you could get your feet wet with guided assistance. Tutorials
    often end here, but real learning, especially projects in production, always goes
    a step further. So we want to give you ideas about how you can make this project
    your own, from choosing a different model to using different tooling.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对这个项目进行了一次全面了解，让我们来谈谈修改这个项目的方法。为了清晰起见，我们选择手把手地告诉你确切要运行的命令，这样你就可以在指导帮助下开始接触实际操作。教程通常在这里结束，但真正的学习，尤其是在生产中的项目，总是更进一步。因此，我们想要给你一些想法，告诉你如何使这个项目成为你自己的，从选择不同的模型到使用不同的工具。
- en: 11.4.1 Using a better interface
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 使用更好的界面
- en: Learning a new tool is one of the most common tasks for someone in this field—and
    by that, we mean everything from data science to MLOps. While we’ve chosen to
    focus on some of the most popular and battle-tested tooling in this book—tools
    we’ve actually used in production— your company has likely chosen different tools.
    Even more likely, a new tool came out that everyone is talking about, and you
    want to try it out.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 学习新工具是这个领域的人最常见任务之一——我们这里指的是从数据科学到MLOps的所有事情。虽然我们选择在本书中专注于一些最受欢迎和经过实战考验的工具——我们在生产中实际使用过的工具——但你的公司可能选择了不同的工具。更有可能的是，出现了一个新工具，每个人都正在谈论，你想要尝试它。
- en: We’ve talked a lot about llama.cpp and used it for pretty much everything in
    this project, including compiling, quantizing, serving, and even creating a frontend
    for our project. While the tool shines on the compiling and quantizing side, the
    other stuff was mostly added out of convenience. Let’s consider some other tools
    that can help give your project that extra pop or pizzazz.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经就 llama.cpp 谈了很多，并在本项目中几乎用它做了所有事情，包括编译、量化、服务，甚至为我们的项目创建前端。虽然这个工具在编译和量化方面表现突出，但其他功能主要是出于方便而添加的。让我们考虑一些其他可以帮助你的项目增添额外活力或魅力的工具。
- en: To improve your project instantly, you might consider installing a frontend
    for the server like SillyTavern (not necessarily recommended; it’s just popular).
    A great frontend will turn “querying an LLM” into “chatting with an AI best friend,”
    shifting from a placid task to an exciting experience. Some tools we like for
    the job are KoboldCpp and Ollama, which were built to extend llama.cpp and make
    the interface simpler or more extensible. So they are perfect to extend this particular
    project. Oobabooga is another great web UI for text generation. All these tools
    offer lots of customization and ways to provide your users with unique experiences.
    They generally provide both a frontend and a server.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了立即改善你的项目，你可能考虑安装一个类似 SillyTavern（不一定推荐；它只是很受欢迎）的服务器前端。一个优秀的前端会将“查询一个LLM”转变为“与一个AI最佳朋友聊天”，将一个平静的任务转变为一个令人兴奋的体验。我们喜欢用于这项工作的工具包括
    KoboldCpp 和 Ollama，它们是为了扩展 llama.cpp 并使界面更简单或更可扩展而构建的。因此，它们非常适合扩展这个特定的项目。Oobabooga
    是另一个出色的文本生成网页UI。所有这些工具都提供了大量的定制选项和为用户提供独特体验的方式。它们通常提供前端和服务器。
- en: 11.4.2 Changing quantization
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 更改量化
- en: You might consider doing this same project but on an older Pi with only 4 GB
    of memory, so you’ll need a smaller model. Maybe you want to do more than just
    serve an LLM with your Pi, so you need to shrink the model a bit more, or maybe
    you want to switch up the model entirely. Either way, you’ll need to dive a bit
    deeper down the quantization rabbit hole. Before, we quantized the model using
    `q4_K_M` format with the promise we’d explain it later. Well, now it’s later.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能考虑在只有4GB内存的旧Pi上做同样的项目，这样你将需要一个更小的模型。也许你不仅想用Pi来服务LLM，所以你需要进一步缩小模型，或者也许你完全想更换模型。无论如何，你都需要深入探索量化这个兔子洞。之前，我们使用`q4_K_M`格式量化模型，承诺稍后解释。好吧，现在就是时候了。
- en: Llama.cpp offers many different quantization formats. To simplify the discussion,
    table 11.1 highlights a few of the more common quantization methods, along with
    how many bits each converts down to, the size of the resulting model, and the
    RAM required to run it for a 7B parameter model. This table should act as a quick
    reference to help you determine what size and level of performance you can expect.
    The general rule is that smaller quantization equals lower-quality performance
    and higher perplexity.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Llama.cpp提供了许多不同的量化格式。为了简化讨论，表11.1突出了一些更常见的量化方法，包括每个方法转换下来的比特数，结果的模型大小，以及运行7B参数模型所需的RAM。这张表应作为快速参考，帮助你确定可以期望的大小和性能水平。一般规则是，量化越小，性能越低，困惑度越高。
- en: Table 11.1 Comparison of key attributes for different llama.cpp quantization
    methods for a 7B parameter model
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.1 不同 llama.cpp 量化方法对7B参数模型的键属性比较
- en: '| Quant method | Bits | Size (GB) | Max RAM required (GB) | Use case | Params
    (billions) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 量化方法 | 比特数 | 大小 (GB) | 最大RAM需求 (GB) | 用例 | 参数 (十亿) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `Q2_K`  | 2  | 2.72  | 5.22  | Significant quality loss; not recommended
    for most purposes  | 7  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `Q2_K`  | 2  | 2.72  | 5.22  | 质量损失显著；不建议用于大多数目的  | 7  |'
- en: '| `Q3_K_S`  | 3  | 3.16  | 5.66  | Very small, high loss of quality  | 7  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `Q3_K_S`  | 3  | 3.16  | 5.66  | 非常小，质量损失高  | 7  |'
- en: '| `Q3_K_M`  | 3  | 3.52  | 6.02  | Very small, high loss of quality  | 7  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `Q3_K_M`  | 3  | 3.52  | 6.02  | 非常小，质量损失高  | 7  |'
- en: '| `Q3_K_L`  | 3  | 3.82  | 6.32  | Small, substantial quality loss  | 7  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `Q3_K_L`  | 3  | 3.82  | 6.32  | 小，质量损失较大  | 7  |'
- en: '| `Q4_0`  | 4  | 4.11  | 6.61  | Legacy; small, very high loss of quality;
    prefer using `Q3_K_M`  | 7  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `Q4_0`  | 4  | 4.11  | 6.61  | 传统；小，质量损失非常高；建议使用`Q3_K_M`  | 7  |'
- en: '| `Q4_K_S`  | 4  | 4.14  | 6.64  | Small, greater quality loss  | 7  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `Q4_K_S`  | 4  | 4.14  | 6.64  | 小，质量损失更大  | 7  |'
- en: '| `Q4_K_M`  | 4  | 4.37  | 6.87  | Medium, balanced quality; recommended  |
    7  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `Q4_K_M`  | 4  | 4.37  | 6.87  | 中等，平衡质量；推荐  | 7  |'
- en: '| `Q5_0`  | 5  | 5.00  | 7.50  | Legacy; medium, balanced quality; prefer using
    `Q4_K_M`  | 7  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `Q5_0`  | 5  | 5.00  | 7.50  | 传统；中等，平衡质量；建议使用`Q4_K_M`  | 7  |'
- en: '| `Q5_K_S`  | 5  | 5.00  | 7.50  | Large, low loss of quality; recommended  |
    7  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `Q5_K_S`  | 5  | 5.00  | 7.50  | 大，质量损失低；推荐  | 7  |'
- en: '| `Q5_K_M`  | 5  | 5.13  | 7.63  | large, very low loss of quality; recommended  |
    7  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `Q5_K_M`  | 5  | 5.13  | 7.63  | 大，质量损失非常低；推荐  | 7  |'
- en: '| `Q6_K`  | 6  | 5.94  | 8.44  | Very large, extremely low loss of quality  |
    7  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `Q6_K`  | 6  | 5.94  | 8.44  | 非常大，质量损失极低  | 7  |'
- en: '| `Q8_0`  | 8  | 7.70  | 10.20  | Very large, extremely low loss of quality;
    not recommended  | 7  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `Q8_0`  | 8  | 7.70  | 10.20  | 非常大，质量损失极低；不推荐  | 7  |'
- en: If you only have a 4 or 6 GB Pi, you’re probably looking at this table thinking,
    “Nope, time to give up.” But you’re not completely out of luck; your model will
    likely just run slower, and you’ll either need a smaller model than one of these
    7Bs—something with only, say, 1B or 3B parameters—or to quantize smaller to run.
    You’re really pushing the edge with such a small Pi, so `Q2_k` or `Q3_K_S` might
    work for you.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有4GB或6GB的Pi，你可能会看着这张表格想，“不，是时候放弃了。”但你还不是完全没有机会；你的模型可能只是运行得慢一些，你可能需要比这些7B参数更小的模型——比如只有1B或3B参数的模型——或者量化得更小以运行。你实际上是在用这么小的Pi来推动边缘，所以`Q2_k`或`Q3_K_S`可能适合你。
- en: 'A friendly note: we’ve been pushing the limits on the edge with this project,
    but it is a useful experience for more funded projects. When working on similar
    projects with better hardware, that better hardware will have its limits as to
    how large an LLM it can run. After all, there’s always a bigger model. Keep in
    mind that if you’re running with cuBLAS or any framework for utilizing a GPU,
    you’re constrained by the VRAM in addition to the RAM. For example, running with
    cuBLAS on a 3090 constrains you to 24 GB of VRAM. Using clever memory management
    (such as a headless OS to take up less RAM), you can load bigger models onto smaller
    devices and push the boundaries of what feels like it should be possible.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 友好的提醒：我们一直在用这个项目挑战边缘，但对于更有资金的项目来说，这是一个有用的经验。当使用更好的硬件进行类似项目时，更好的硬件在运行大型 LLM 方面也有其限制。毕竟，总是有更大的模型。请记住，如果你使用
    cuBLAS 或任何用于利用 GPU 的框架运行，你不仅受限于 RAM，还受限于 VRAM。例如，在 3090 上使用 cuBLAS 运行时，你受限于 24
    GB 的 VRAM。通过巧妙的内存管理（例如使用无头操作系统以占用更少的 RAM），你可以在较小的设备上加载更大的模型，并推动看似可能实现的功能的边界。
- en: 11.4.3 Adding multimodality
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.3 添加多模态
- en: 'There’s an entire dimension that we initially ignored so that it wouldn’t distract,
    but let’s talk about it now: LLaVA is actually multimodal! A multimodal model
    allows us to expand out from NLP to other sources like images, audio, and video.
    Pretty much every multimodal model is also an LLM at heart, as datasets of different
    modalities are labeled with natural language—for example, a text description of
    what is seen in an image. In particular, LLaVA, which stands for Large Language
    and Vision Assistant, allows us to give the model an input image and ask questions
    about it.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初忽略了一个完整的维度，以免分散注意力，但现在让我们来谈谈它：LLaVA 实际上是一个多模态模型！多模态模型使我们能够从 NLP 扩展到其他来源，如图像、音频和视频。几乎每个多模态模型在本质上也是一个大型语言模型（LLM），因为不同模态的数据集都使用自然语言进行标记——例如，图像中看到的内容的文本描述。特别是，LLaVA（大型语言和视觉助手）允许我们向模型输入一张图像并对其提问。
- en: A note about the llama server
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于 llama 服务器的一个注意事项
- en: Remember when we said the llama.cpp project doesn’t follow many engineering
    best practices? Well, multimodality is one of them. The llama.cpp server at first
    supported multimodality, but many issues were soon added to the project. Instead
    of adding a feature and incrementing on it, the creator felt the original implementation
    was hacky and decided to remove it. One day, everything was working, and the next,
    it just disappeared altogether.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们说过 llama.cpp 项目并不遵循许多工程最佳实践吗？嗯，多模态就是其中之一。最初，llama.cpp 服务器支持多模态，但很快项目中就出现了许多问题。创建者觉得原始实现很糟糕，决定将其移除。有一天，一切正常，第二天，它就完全消失了。
- en: This change happened while we were writing this chapter—which was a headache
    in itself—but imagine what damage it could have caused when trying to run things
    in production. Unfortunately, this sudden change is par for the course when working
    on LLMs at this point in time, as there are very few stable dependencies you can
    rely on that are currently available. To reproduce what’s here and minimize debugging,
    we hope you check out the `git` `commit` mentioned earlier. The good news is that
    llama.cpp plans to continue to support multimodality, and another implementation
    will likely be ready to go soon—possibly by the time you read this chapter
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变化发生在我们编写本章的时候——这本身就是一个头疼的问题——但想象一下，当试图在生产中运行时，它可能造成的损害。不幸的是，在当前这个时间点上，当我们在
    LLM 上工作时，这种突然的变化是家常便饭，因为目前可用的稳定依赖项非常少。为了重现这里的内容并最小化调试，我们希望您查看之前提到的 `git` `commit`。好消息是，llama.cpp
    计划继续支持多模态，另一个实现可能很快就会准备好——可能在你阅读这一章的时候。
- en: We haven’t really talked about multimodality at all in this book, as many lessons
    from learning how to make LLMs work in production should transfer over to multimodal
    models. Regardless, we thought it’d be fun to show you how to deploy one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们几乎没有讨论过多模态，因为从学习如何使 LLM 在生产中工作获得的经验教训应该可以转移到多模态模型上。无论如何，我们认为展示如何部署一个模型会很有趣。
- en: Updating the model
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新模型
- en: We’ve done most of the work already; however, llama.cpp has only converted the
    llama portion of the LLaVA model to .gguf. We need to add the vision portion back
    in. To test this, go to the GUI for your served model, and you’ll see an option
    to upload an image. If you do, you’ll get a helpful error, shown in figure 11.10,
    indicating that the server isn’t ready for multimodal serving.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经做了大部分工作；然而，llama.cpp只将LLaVA模型的llama部分转换为.gguf。我们需要将视觉部分重新添加进去。为了测试这一点，前往你提供的服务器模型的GUI，你会看到一个上传图像的选项。如果你这样做，你会得到一个有用的错误，如图11.10所示，表明服务器还没有准备好进行多模态服务。
- en: '![figure](../Images/11-10.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-10.png)'
- en: Figure 11.10 Our model isn’t ready yet; we need to provide a model projector.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.10 我们的模式还没有准备好；我们需要提供一个模型投影器。
- en: 'The first step to converting our model is downloading a multimodal projection
    file, similar to CLIP, for you to encode images. Once we can encode the images,
    the model will know what to do with them since it’s already been trained for multimodal
    tasks. We aren’t going to go into the details of preparing the projection file;
    instead, we’ll show you where you can find it. Run the following command to download
    this file and then move it:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的模型转换为第一步是下载一个与CLIP类似的多模态投影文件，以便你编码图像。一旦我们可以编码图像，模型就会知道如何处理它们，因为它已经训练过多模态任务。我们不会深入介绍准备投影文件的细节；相反，我们将向你展示在哪里可以找到它。运行以下命令下载此文件，然后移动它：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you are using a different model or a homebrew, make sure you find or create
    a multimodal projection model to perform that function for you. It should feel
    intuitive as to why you’d need it: language models only read language. You can
    try finetuning and serializing images to strings instead of using a multimodal
    projection model; however, we don’t recommend doing so, as we haven’t seen good
    results from it. It increases the total amount of RAM needed to run these models,
    but not very much.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用不同的模型或自制模型，确保你找到或创建一个多模态投影模型来为你执行该功能。它应该直观地告诉你为什么你需要它：语言模型只读取语言。你可以尝试微调和将图像序列化为字符串，而不是使用多模态投影模型；然而，我们不推荐这样做，因为我们没有看到它带来良好的结果。这会增加运行这些模型所需的RAM总量，但增加的量并不大。
- en: Serving the model
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提供模型服务
- en: 'Once you have your model converted and quantized, the command to start the
    server is the same, except you must add `--MMPROJ` `path/to/mmproj.gguf` to the
    end. This code will allow you to submit images to the model for tasks like performing
    optical character recognition (OCR), where we convert text in the image to actual
    text. Let’s do that now:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型被转换和量化，启动服务器的命令是相同的，但你必须在末尾添加`--MMPROJ` `path/to/mmproj.gguf`。这段代码将允许你向模型提交图像以执行任务，如进行光学字符识别（OCR），我们将图像中的文本转换为实际文本。现在让我们来做这件事：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that our server knows what to do with images, let’s send it in a request.
    In line with the OpenAI API we used to chat with the language-only model before,
    another version shows you how to call a multimodal chat. The code is very similar
    to listing 11.1 since all we are doing is adding some image support. Like the
    last listing, we use the OpenAI API to access our LLM backend, but we will change
    the base URL to our model. The main difference is that we are serializing the
    image into a string so that it can be sent in the object with a couple of imports
    to facilitate that using the `encode_image` function. The only other big change
    is adding the encoded image to the content section of the messages we send.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务器知道如何处理图像了，让我们发送一个请求。与之前我们用来与仅语言模型聊天的OpenAI API一致，另一个版本展示了如何调用多模态聊天。代码与列表11.1非常相似，因为我们所做的只是添加了一些图像支持。像上一个列表一样，我们使用OpenAI
    API访问我们的LLM后端，但我们将基本URL更改为我们的模型。主要区别在于我们将图像序列化为字符串，以便它可以包含在对象中，并使用`encode_image`函数添加一些导入以简化这个过程。唯一的另一个重大变化是将编码后的图像添加到我们发送的消息的内容部分。
- en: Listing 11.2 OpenAI but multimodal GPT-4
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2 OpenAI的多模态GPT-4
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Replace with your server’s IP address and port.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 替换为你的服务器的IP地址和端口。'
- en: '#2 Set to the maximum dimension to allow (512=1 tile, 2048=max).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 设置为允许的最大维度（512=1个瓦片，2048=最大）。'
- en: Nothing too fancy or all that different from the many other times we’ve sent
    requests to servers. One little gotcha with this code that you should keep in
    mind is that the API will throw an error if you don’t use an API key, but if you
    don’t set one on the server, you can pass anything, and it won’t error out.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么太花哨或与其他许多次向服务器发送请求有什么不同。你应该记住这个代码的一个小陷阱是，如果没有使用API密钥，API将抛出错误；但如果你在服务器上没有设置一个，你可以传递任何东西，它不会出错。
- en: And that’s it! We’ve now turned our language model into one that can also take
    images as input, and we have served it onto a Raspberry Pi and even queried it.
    At least, we hope you queried it because if you didn’t, let us tell you, it is
    very *slow*! When you run the multimodal server on the Pi, it will take dozens
    of minutes to encode and represent the image before even getting to the tokens
    per second that people generally use to measure the speed of generation. Once
    again, just because we can deploy these models to small devices doesn’t mean you’ll
    want to. This is the point where we’re going to recommend again that you should
    not actually be running this on a Pi, even in your house, if you want to actually
    get good use out of it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止！我们已经将我们的语言模型转变为可以接受图像作为输入的模型，并且我们已经将其部署到 Raspberry Pi 上，甚至对其进行了查询。至少，我们希望您已经对其进行了查询，因为如果您没有，让我们告诉您，它非常
    *慢*！当您在 Pi 上运行多模态服务器时，它将花费数十分钟来编码和表示图像，甚至达到人们通常用来衡量生成速度的每秒令牌数。再次强调，仅仅因为我们可以将这些模型部署到小型设备上，并不意味着您会想要这样做。这就是我们再次建议您不应该在
    Pi 上实际运行此程序，即使是在您的家中，如果您想要真正地充分利用它。
- en: 11.4.4 Serving the model on Google Colab
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.4 在 Google Colab 上提供模型服务
- en: 'Now that we’ve done a couple of these exercises, how can we improve and extend
    this project for your production environment? The first improvement is obvious:
    hardware. Single-board RAM compute isn’t incredibly helpful when you have hundreds
    of customers; however, it is incredibly useful for testing, especially when you
    don’t want to waste money debugging production for your on-prem deployment. Other
    options for GPU support also exist, and luckily, all the previously discussed
    steps, minus the RPi setup, work on Google Colab’s free tier. Here are all of
    the setup steps that are different:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了一些这些练习，我们如何改进和扩展这个项目以适应您的生产环境？第一个改进很明显：硬件。当您有数百名客户时，单板 RAM 计算 并不是非常有帮助；然而，它在测试时非常有用，尤其是当您不想浪费钱调试生产环境时。还有其他支持
    GPU 的选项，幸运的是，除了 RPi 设置之外，之前讨论的所有步骤都在 Google Colab 的免费层上工作。以下是所有不同的设置步骤：
- en: 'Setting up llama.cpp:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 llama.cpp：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '2\. Downloading from Hugging Face:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 从 Hugging Face 下载：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '3\. Server command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 服务器命令：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '4\. Accessing the server:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 访问服务器：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see, the steps are mostly the same, but because we are working in
    a Jupyter environment, some slight changes are necessary, as it’s often easier
    to run code directly instead of running a CLI command. We didn’t go into it, but
    Raspberry Pis can use `docker.io` and other packages to create docker images that
    you can use for responsible CI/CD. It’s a bit harder in a Google Colab environment.
    Also, keep in mind that Google won’t give you unlimited GPU time, and it goes
    so far as to monitor whether you have Colab open to turn off your free GPU “efficiently,”
    so make sure you’re only using those free resources for testing and debugging.
    No matter how you look at it, free GPUs are a gift, and we should be responsible
    with them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，步骤大多相同，但由于我们在 Jupyter 环境中工作，一些细微的调整是必要的，因为通常直接运行代码比运行 CLI 命令要容易。我们没有深入探讨，但
    Raspberry Pi 可以使用 `docker.io` 和其他软件包来创建可用于负责任的 CI/CD 的 docker 镜像。在 Google Colab
    环境中这要困难一些。此外，请记住，Google 不会给您无限的 GPU 时间，甚至到了监控您是否打开了 Colab 以“高效”地关闭您的免费 GPU 的程度，所以请确保您只将这些免费资源用于测试和调试。无论如何看，免费的
    GPU 是一份礼物，我们应该对它们负责。
- en: 'You can also skip downloading the whole repo and running Make every time. You
    can use the llama.cpp Python bindings. And you can `pip` `install` with cuBLAS
    or NEON (for Mac GeForce Mx cards) to use hardware acceleration when pip installing
    with this command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以跳过下载整个仓库并每次运行 Make 的步骤。您可以使用 llama.cpp 的 Python 绑定。并且您可以使用 cuBLAS 或 NEON（适用于
    Mac GeForce Mx 卡）来使用硬件加速，在执行以下命令时进行 pip 安装：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This command abstracts most of the code in llama.cpp into easy-to-use Python
    bindings. Let’s now go through an example of how to use the Python bindings to
    make something easy to dockerize and deploy. Working with an API is slightly different
    from working with an LLM by itself, but luckily, LangChain comes in handy. Its
    whole library is built around working with the OpenAI API, and we use that API
    to access our own model!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将 llama.cpp 中的大部分代码抽象为易于使用的 Python 绑定。现在让我们通过一个示例来了解如何使用 Python 绑定来制作易于 docker
    化和部署的内容。与 API 一起工作与单独使用 LLM 略有不同，但幸运的是，LangChain 非常方便。其整个库都是围绕使用 OpenAI API 构建的，我们使用该
    API 来访问我们自己的模型！
- en: In listing 11.3, we’ll combine what we know about the OpenAI API, llama.cpp
    Python bindings, and LangChain. We’ll start by setting up our environment variables,
    and then we’ll use the LangChain `ChatOpenAI` class and pretend that our server
    is GPT-3.5-turbo. Once we have those two things, we could be done, but we’ll extend
    by adding a sentence transformer and a prompt ready for RAG. If you have a dataset
    you’d like to use for RAG, now is the time to embed it and create a FAISS index.
    We’ll load your FAISS index and use it to help the model at inference time. Then,
    tokenize it with tiktoken to make sure we don’t overload our context length.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表11.3中，我们将结合我们对OpenAI API、llama.cpp Python绑定和LangChain的了解。我们将首先设置我们的环境变量，然后我们将使用LangChain的`ChatOpenAI`类，并假装我们的服务器是GPT-3.5-turbo。一旦我们有了这两样东西，我们就可以完成了，但我们将通过添加一个句子转换器和为RAG准备好的提示来扩展它。如果您有一个想要用于RAG的数据集，现在是嵌入它并创建FAISS索引的时候了。我们将加载您的FAISS索引，并在推理时使用它来帮助模型。然后，使用tiktoken对其进行标记化，以确保我们不会超载我们的上下文长度。
- en: Listing 11.3 OpenAI but not multimodal GPT-4
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3 OpenAI但不是多模态GPT-4
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Replace with your server’s address and port.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用您的服务器地址和端口替换。'
- en: '#2 Replace with your host IP.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 用您的主机IP替换。'
- en: '#3 This can be anything.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这可以是任何内容。'
- en: '#4 Again'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 再次'
- en: '#5 Embeddings for RAG'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 为RAG嵌入'
- en: '#6 Tokenization for checking context length quickly'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 快速检查上下文长度的标记化'
- en: '#7 Change the prompt to be whatever you want.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将提示更改为您想要的任何内容。'
- en: '#8 Here’s a vectorΔB; feel free to drop in a replacement.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 这里有一个向量ΔB；请随意替换。'
- en: '#9 If you haven’t created a faiss or elasticsearch or usearch index, do it.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 如果您还没有创建faiss或elasticsearch或usearch索引，请创建。'
- en: '#10 To keep track of chat history'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 为了跟踪聊天历史'
- en: '#11 Searches the Vector ΔB'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 搜索向量ΔB'
- en: '#12 Formats the prompt'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 格式化提示'
- en: '#13 Sets up the actual LLM chain'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 设置实际的LLM链'
- en: '#14 Δon’t overload your context length.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#14 不要超载您的上下文长度。'
- en: '#15 Runs RAG with your API'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#15 使用您的API运行RAG'
- en: '#16 We’re just printing; do whatever you need to here.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#16 我们只是打印；在这里做您需要做的任何事情。'
- en: So here’s where many of our concepts really come together. The amazing thing
    is that you really can perform this inference and RAG on a Raspberry Pi; you don’t
    need a gigantic computer to get good, repeatable results. Compute layered on top
    of this helps immensely until you get to about 48 GB and can fit full versions
    of 7B and quantized versions of everything above that; all compute after that
    ends up getting only marginal gains currently. This field is advancing quickly,
    so look for new, quicker methods of inferencing larger models on smaller hardware.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '所以，这里是我们许多概念真正汇聚的地方。令人惊讶的是，您真的可以在Raspberry Pi上执行这个推理和RAG；您不需要一台巨大的计算机来获得好、可重复的结果。在您达到大约48
    GB并可以容纳7B的全版本以及所有超过那的量化版本之前，这种计算层非常有帮助；目前，之后的所有计算只能带来边际的收益。这个领域正在快速发展，所以寻找在更小的硬件上推理更大模型的新、更快的方法。 '
- en: With that, we’ve got our prototype project up and running. It’s easily extensible
    in pretty much any direction you’d like, and it conforms to industry standards
    and uses popular libraries. Add to this, make it better, and if you have expertise
    you feel isn’t being represented here, share it! This field is new, and interdisciplinary
    knowledge is how it will be pushed forward.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的原型项目就启动并运行了。它在几乎任何您想要的方向上都非常容易扩展，并且符合行业标准并使用流行的库。添加到这，让它变得更好，如果您有您认为这里没有得到体现的专业知识，请分享它！这个领域是新的，跨学科的知识将推动它向前发展。
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Running the largest models on the smallest devices demands utilizing every memory-saving
    technique you can think of, like running a Lite operating system.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最小的设备上运行最大的模型需要利用您能想到的所有内存节省技术，比如运行一个轻量级操作系统。
- en: The hardest part of setting up a remote Pi for the first time is finding its
    IP address.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置远程Pi第一次最困难的部分是找到它的IP地址。
- en: For compute-limited hardware without an accelerator, you will need to compile
    the model to run on your architecture with a tool like llama.cpp.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于没有加速器的计算受限硬件，您需要使用像llama.cpp这样的工具将模型编译到您的架构上运行。
- en: In a memory-limited environment, quantization will be required for inference.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存受限的环境中，推理将需要量化。
- en: Even taking advantage of everything available, running LLMs on edge devices
    will often result in slower inference than desired. Just because something is
    possible doesn’t make it practical.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使利用所有可用的资源，在边缘设备上运行LLM通常会导致比期望的更慢的推理。仅仅因为某件事是可能的，并不意味着它是实用的。
- en: OpenAI’s API, along with all wrappers, can be used to access other models by
    pointing to a custom endpoint.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过指向自定义端点来使用OpenAI的API及其所有包装器，以访问其他模型。
- en: Many open source tools are available to improve both the serving of models and
    the user interface.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多开源工具可用于改进模型的提供和用户界面。
- en: Lower quantization equals higher perplexity, even with larger models.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化程度越低，即使模型规模更大，困惑度也越高。
- en: Running multimodal models is also possible on a Raspberry Pi.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Raspberry Pi上运行多模态模型也是可能的。
- en: The same commands we ran on the Pi can be used to develop in Google Collab or
    another cloud provider with only slight modifications, making these projects more
    accessible than ever.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在Pi上运行的相同命令，只需稍作修改就可以用于Google Collab或其他云服务提供商进行开发，这使得这些项目比以往任何时候都更容易获得。
- en: Setup and deployment are often much larger pieces to a successful project than
    preparing the model.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置和部署通常比准备模型对成功项目来说更为重要。
