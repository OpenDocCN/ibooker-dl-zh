- en: Chapter 7\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 降维
- en: Many machine learning problems involve thousands or even millions of features
    for each training instance. Not only do all these features make training extremely
    slow, but they can also make it much harder to find a good solution, as you will
    see. This problem is often referred to as the *curse of dimensionality*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题涉及每个训练实例数千甚至数百万个特征。所有这些特征不仅使训练变得极其缓慢，而且如你所见，它们也会使找到好的解决方案变得困难得多。这个问题通常被称为*维度灾难*。
- en: 'Fortunately, in real-world problems, it is often possible to reduce the number
    of features considerably, turning an intractable problem into a tractable one.
    For example, consider the MNIST images (introduced in [Chapter 3](ch03.html#classification_chapter)):
    the pixels on the image borders are almost always white, so you could completely
    drop these pixels from the training set without losing much information. As we
    saw in the previous chapter, [Figure 6-6](ch06.html#mnist_feature_importance_plot)
    confirms that these pixels are utterly unimportant for the classification task.
    Additionally, two neighboring pixels are often highly correlated: if you merge
    them into a single pixel (e.g., by taking the mean of the two pixel intensities),
    you will not lose much information, removing redundancy and sometimes even noise.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在现实世界的问题中，通常可以显著减少特征的数量，将一个难以处理的问题转化为一个可处理的问题。例如，考虑MNIST图像（在第3章中介绍）：图像边界的像素几乎总是白色，因此你可以完全从训练集中删除这些像素而不会丢失太多信息。正如我们在上一章中看到的，[图6-6](ch06.html#mnist_feature_importance_plot)证实了这些像素对于分类任务来说完全不重要。此外，相邻的像素通常高度相关：如果你将它们合并成一个像素（例如，通过取两个像素强度的平均值），你不会丢失太多信息，从而去除冗余，有时甚至去除噪声。
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Reducing dimensionality can also drop some useful information, just like compressing
    an image to JPEG can degrade its quality: it can make your system perform slightly
    worse, especially if you reduce dimensionality too much. Moreover, some models—such
    as neural networks—can handle high-dimensional data efficiently and learn to reduce
    its dimensionality while preserving the useful information for the task at hand.
    In short, adding an extra preprocessing step for dimensionality reduction will
    not always help.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 降维也可能丢失一些有用的信息，就像将图像压缩成JPEG格式可能会降低其质量一样：它可能会使你的系统性能略有下降，尤其是如果你过度降维的话。此外，一些模型——例如神经网络——可以有效地处理高维数据，并学会在保留任务所需的有用信息的同时降低其维度。简而言之，添加一个额外的预处理步骤进行降维并不总是有帮助。
- en: Apart from speeding up training and possibly improving your model’s performance,
    dimensionality reduction is also extremely useful for data visualization. Reducing
    the number of dimensions down to two (or three) makes it possible to plot a condensed
    view of a high-dimensional training set on a graph and often gain some important
    insights by visually detecting patterns, such as clusters. Moreover, data visualization
    is essential to communicate your conclusions to people who are not data scientists—in
    particular, decision makers who will use your results.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 除了加速训练和可能提高你的模型性能之外，降维对于数据可视化也非常有用。将维度数量减少到两个（或三个）使得在图上绘制高维训练集的浓缩视图成为可能，并且通过视觉检测模式，如簇，通常可以获得一些重要的见解。此外，数据可视化对于向非数据科学家的人们传达你的结论至关重要——特别是那些将使用你的结果做决策的人。
- en: 'In this chapter we will first discuss the curse of dimensionality and get a
    sense of what goes on in high-dimensional space. Then we will consider the two
    main approaches to dimensionality reduction (projection and manifold learning),
    and we will go through three of the most popular dimensionality reduction techniques:
    PCA, random projection, and locally linear embedding (LLE).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论维度灾难，并了解高维空间中发生的情况。然后我们将考虑降维的两种主要方法（投影和流形学习），并介绍三种最流行的降维技术：主成分分析（PCA）、随机投影和局部线性嵌入（LLE）。
- en: The Curse of Dimensionality
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度灾难
- en: We are so used to living in three dimensions⁠^([1](ch07.html#id1866)) that our
    intuition fails us when we try to imagine a high-dimensional space. Even a basic
    4D hypercube is incredibly hard to picture in our minds (see [Figure 7-1](#hypercube_wikipedia)),
    let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经习惯了生活在三维空间中⁠^([1](ch07.html#id1866))，以至于当我们试图想象高维空间时，直觉就会失效。即使是一个基本的4维超立方体在我们心中也很难想象（参见[图7-1](#hypercube_wikipedia)），更不用说在一个1000维空间中弯曲的200维椭球体了。
- en: '![Diagram illustrating the progression from a point to a tesseract, demonstrating
    the concept of hypercubes from 0D to 4D.](assets/hmls_0701.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![从点到四维超立方体的演变图，展示了从 0D 到 4D 超立方体的概念。](assets/hmls_0701.png)'
- en: Figure 7-1\. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)⁠^([2](ch07.html#id1867))
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 点、线段、正方形、立方体和超立方体（0D 到 4D 超立方体）⁠^([2](ch07.html#id1867))
- en: It turns out that many things behave very differently in high-dimensional space.
    For example, if you pick a random point in a unit square (a 1 × 1 square), it
    will have only about a 0.4% chance of being located less than 0.001 from a border
    (in other words, it is very unlikely that a random point will be “extreme” along
    any dimension). But in a 10,000-dimensional unit hypercube, this probability is
    greater than 99.999999%. Most points in a high-dimensional hypercube are very
    close to the border.⁠^([3](ch07.html#id1868))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，许多事物在高维空间中的行为非常不同。例如，如果你在一个单位正方形（1 × 1 的正方形）中随机选择一个点，它位于距离边界小于 0.001 的概率只有大约
    0.4%（换句话说，随机点在任意维度上“极端”的可能性非常低）。但在一个 10,000 维的单位超立方体中，这个概率超过 99.999999%。一个高维超立方体中的大多数点都非常接近边界。⁠^([3](ch07.html#id1868))
- en: 'Here is a more troublesome difference: if you pick two points randomly in a
    unit square, the distance between these two points will be, on average, roughly
    0.52\. If you pick two random points in a 3D unit cube, the average distance will
    be roughly 0.66\. But what about two points picked randomly in a 1,000,000-dimensional
    unit hypercube? The average distance, believe it or not, will be about 408.25
    (roughly $StartRoot StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$
    )! This is counterintuitive: how can two points be so far apart when they both
    lie within the same unit hypercube? Well, there’s just plenty of space in high
    dimensions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个更麻烦的差异：如果你在一个单位正方形中随机选择两个点，这两个点之间的距离平均大约是 0.52。如果你在一个 3D 单位立方体中随机选择两个点，平均距离大约是
    0.66。那么，如果你在一个 1,000,000 维的单位超立方体中随机选择两个点呢？平均距离，信不信由你，大约是 408.25（大约是 $StartRoot
    StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$ ）！这是反直觉的：当两个点都位于同一个单位超立方体中时，它们怎么可能相距如此之远？好吧，高维度中确实有大量的空间。
- en: 'As a result, high-dimensional datasets are often very sparse: most training
    instances are likely to be far away from each other, so training methods based
    on distance or similarity (such as *k*-nearest neighbors) will be much less effective.
    And some types of models will not be usable at all because they scale poorly with
    the dataset’s dimensionality (e.g., SVMs or dense neural networks). And new instances
    will likely be far away from any training instance, making predictions much less
    reliable than in lower dimensions since they will be based on much larger extrapolations.
    Since patterns in the data will become harder to identify, models will tend to
    fit the noise more frequently than in lower dimensions; regularization will become
    all the more important. Lastly, models will become even harder to interpret.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，高维数据集通常非常稀疏：大多数训练实例可能彼此相距甚远，因此基于距离或相似度的训练方法（如 *k* 近邻）将远不如有效。而且，某些类型的模型可能根本无法使用，因为它们与数据集的维度扩展不佳（例如，SVMs
    或密集神经网络）。新实例可能远离任何训练实例，这使得预测在低维度的可靠性大大降低，因为它们将基于更大的外推。由于数据中的模式更难识别，模型将比低维度更频繁地拟合噪声；正则化将变得更加重要。最后，模型将变得更加难以解释。
- en: In theory, some of these issues can be resolved by increasing the size of the
    training set to reach a sufficient density of training instances. Unfortunately,
    in practice, the number of training instances required to reach a given density
    grows exponentially with the number of dimensions. With just 100 features—significantly
    fewer than in the MNIST problem—all ranging from 0 to 1, you would need more training
    instances than atoms in the observable universe in order for training instances
    to be within 0.1 of each other on average, assuming they were spread out uniformly
    across all dimensions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，通过增加训练集的大小以达到足够的训练实例密度，可以解决这些问题。不幸的是，在实践中，达到给定密度所需的训练实例数量会随着维数的增加而呈指数增长。在只有
    100 个特征的情况下——这比 MNIST 问题中的特征少得多——所有特征都介于 0 到 1 之间，你需要比可观测宇宙中的原子还多的训练实例，才能使训练实例的平均距离在
    0.1 以内，假设它们在所有维度上均匀分布。
- en: Main Approaches for Dimensionality Reduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度约简的主要方法
- en: 'Before diving into specific dimensionality reduction algorithms, let’s look
    at the two main approaches to reducing dimensionality: projection and manifold
    learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究特定的降维算法之前，让我们看看降低维度的两种主要方法：投影和流形学习。
- en: Projection
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影
- en: In most real-world problems, training instances are *not* spread out uniformly
    across all dimensions. Many features are almost constant, while others are highly
    correlated (as discussed earlier for MNIST). As a result, all training instances
    lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional
    space. This sounds abstract, so let’s look at an example. In [Figure 7-2](#dataset_3d_plot),
    a 3D dataset is represented by small spheres (I will refer to this figure several
    times in the following sections).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际问题中，训练实例并不是均匀分布在所有维度上。许多特征几乎恒定不变，而其他特征高度相关（如前面讨论的 MNIST 所述）。因此，所有训练实例都位于（或接近）高维空间中一个低得多的
    *子空间* 内。这听起来很抽象，所以让我们看一个例子。在[图 7-2](#dataset_3d_plot)中，一个 3D 数据集由小圆球表示（我将在接下来的几节中多次提到这个图）。
- en: 'Notice that all training instances lie close to a plane: this is a lower-dimensional
    (2D) subspace of the higher-dimensional (3D) space. If we project every training
    instance perpendicularly onto this subspace (as represented by the short dashed
    lines connecting the instances to the plane), we get the new 2D dataset shown
    in [Figure 7-3](#dataset_2d_plot). Ta-da! We have just reduced the dataset’s dimensionality
    from 3D to 2D. Note that the axes correspond to new features *z*[1] and *z*[2]:
    they are the coordinates of the projections on the plane.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到所有训练实例都靠近一个平面：这是更高维（3D）空间中的低维（2D）子空间。如果我们将每个训练实例垂直投影到这个子空间上（如图中连接实例和平面的短虚线所示），我们就会得到[图
    7-3](#dataset_2d_plot)中显示的新 2D 数据集。哇！我们刚刚将数据集的维度从 3D 降低到 2D。注意，轴对应于新的特征 *z*[1]
    和 *z*[2]：它们是投影到平面上的坐标。
- en: '![A 3D scatter plot showing data points clustered near a 2D plane, illustrating
    a lower-dimensional subspace in higher-dimensional space.](assets/hmls_0702.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![一个 3D 散点图，显示数据点聚集在二维平面上，说明在更高维空间中的低维子空间。](assets/hmls_0702.png)'
- en: Figure 7-2\. A 3D dataset lying close to a 2D subspace
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 一个靠近二维子空间的 3D 数据集
- en: '![Scatter plot showing the 2D dataset with axes labeled as new features z1
    and z2, illustrating dimensionality reduction from 3D to 2D.](assets/hmls_0703.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示带有轴标签为新的特征 z1 和 z2 的 2D 数据集，说明从 3D 到 2D 的降维。](assets/hmls_0703.png)'
- en: Figure 7-3\. The new 2D dataset after projection
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 投影后的新 2D 数据集
- en: Manifold Learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习
- en: 'Although projection is fast and often works well, it’s not always the best
    approach to dimensionality reduction. In many cases the subspace may twist and
    turn, such as in the Swiss roll dataset represented in [Figure 7-4](#swiss_roll_plot):
    this is a toy dataset containing 3D points in the shape of a Swiss roll.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然投影速度快且通常效果良好，但它并不是降维的最佳方法。在许多情况下，子空间可能会扭曲和旋转，例如在[图 7-4](#swiss_roll_plot)中展示的瑞士卷数据集中：这是一个包含瑞士卷形状
    3D 点的玩具数据集。
- en: '![3D scatter plot of the Swiss roll dataset, illustrating points arranged in
    a spiral shape, used to demonstrate challenges in dimensionality reduction.](assets/hmls_0704.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![瑞士卷数据集的 3D 散点图，展示点以螺旋形状排列，用于演示降维的挑战。](assets/hmls_0704.png)'
- en: Figure 7-4\. Swiss roll dataset
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 瑞士卷数据集
- en: Simply projecting onto a plane (e.g., by dropping *x*[3]) would squash different
    layers of the Swiss roll together, as shown on the left side of [Figure 7-5](#squished_swiss_roll_plot).
    What you probably want instead is to unroll the Swiss roll to obtain the 2D dataset
    on the righthand side of [Figure 7-5](#squished_swiss_roll_plot).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地投影到一个平面上（例如，通过丢弃 *x*[3]）会将瑞士卷的不同层压在一起，如[图 7-5](#squished_swiss_roll_plot)的左侧所示。你可能更希望将瑞士卷展开，以获得[图
    7-5](#squished_swiss_roll_plot)右侧的 2D 数据集。
- en: '![Diagram showing a squashed Swiss roll dataset on the left, where layers overlap,
    versus an unrolled version on the right, where the data is spread out in two dimensions.](assets/hmls_0705.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![左侧显示压扁的瑞士卷数据集，层重叠，右侧显示展开版本，数据在两个维度上展开。](assets/hmls_0705.png)'
- en: Figure 7-5\. Squashing by projecting onto a plane (left) versus unrolling the
    Swiss roll (right)
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 通过投影到平面（左侧）压扁与展开瑞士卷（右侧）的比较
- en: 'The Swiss roll is an example of a 2D *manifold*. Put simply, a 2D manifold
    is a 2D shape that can be bent and twisted in a higher-dimensional space. More
    generally, a *d*-dimensional manifold is a part of an *n*-dimensional space (where
    *d* < *n*) that locally resembles a *d*-dimensional hyperplane. In the case of
    the Swiss roll, *d* = 2 and *n* = 3: it locally resembles a 2D plane, but it is
    rolled in the third dimension.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 瑞士卷是2D *流形*的一个例子。简单来说，2D流形是在更高维空间中可以弯曲和扭曲的2D形状。更普遍地说，*d*-维流形是*n*-维空间（其中*d* <
    *n*）的一部分，它在局部上类似于*d*-维超平面。在瑞士卷的情况下，*d* = 2，*n* = 3：它在局部上类似于2D平面，但在第三维上卷曲。
- en: Many dimensionality reduction algorithms (e.g., LLE, Isomap, t-SNE, or UMAP),
    work by modeling the manifold on which the training instances lie; this is called
    *manifold learning*. It relies on the *manifold assumption*, also called the *manifold
    hypothesis*, which holds that most real-world high-dimensional datasets lie close
    to a much lower-dimensional manifold. This assumption is very often empirically
    observed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多降维算法（例如，LLE、Isomap、t-SNE或UMAP）通过在训练实例所在的流形上建模来工作；这被称为*流形学习*。它依赖于*流形假设*，也称为*流形假设*，即大多数现实世界的高维数据集都靠近一个低维流形。这个假设通常在经验观察中是成立的。
- en: 'Once again, think about the MNIST dataset: all handwritten digit images have
    some similarities. They are made of connected lines, the borders are white, and
    they are more or less centered. If you randomly generated images, only a ridiculously
    tiny fraction of them would look like handwritten digits. In other words, the
    degrees of freedom available to you if you try to create a digit image are dramatically
    lower than the degrees of freedom you have if you are allowed to generate any
    image you want. These constraints tend to squeeze the dataset into a lower-dimensional
    manifold.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次思考MNIST数据集：所有手写数字图像都有一些相似之处。它们由连接的线条组成，边缘是白色的，并且它们或多或少地居中。如果你随机生成图像，只有极小的一部分看起来像手写数字。换句话说，如果你尝试创建数字图像，可用的自由度比你被允许生成任何想要的图像的自由度要低得多。这些约束往往会将数据集挤压到一个低维流形中。
- en: 'The manifold assumption is often accompanied by another implicit assumption:
    that the task at hand (e.g., classification or regression) will be simpler if
    expressed in the lower-dimensional space of the manifold. For example, in the
    top row of [Figure 7-6](#manifold_decision_boundary_plot) the Swiss roll is split
    into two classes: in the 3D space (on the left) the decision boundary would be
    fairly complex, but in the 2D unrolled manifold space (on the right) the decision
    boundary is a straight line.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设通常伴随着另一个隐含的假设：如果用流形的低维空间表达，手头的任务（例如，分类或回归）将更简单。例如，在[图7-6](#manifold_decision_boundary_plot)的顶部行中，瑞士卷被分为两类：在3D空间（左侧）中，决策边界将相当复杂，但在2D展开流形空间（右侧）中，决策边界是一条直线。
- en: However, this implicit assumption does not always hold. For example, in the
    bottom row of [Figure 7-6](#manifold_decision_boundary_plot), the decision boundary
    is located at *x*[1] = 5\. This decision boundary looks very simple in the original
    3D space (a vertical plane), but it looks more complex in the unrolled manifold
    (a collection of four independent line segments).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个隐含的假设并不总是成立。例如，在[图7-6](#manifold_decision_boundary_plot)的底部行中，决策边界位于 *x*[1]
    = 5。这个决策边界在原始的3D空间（一个垂直平面）中看起来非常简单，但在展开的流形（由四个独立的线段组成的集合）中看起来更复杂。
- en: In short, reducing the dimensionality of your training set before training a
    model will usually speed up training, but it may not always lead to a better or
    simpler solution; it all depends on the dataset. Dimensionality reduction is typically
    more effective when the dataset is small relative to the number of features, especially
    if it’s noisy, or many features are highly correlated to one another (i.e., redundant).
    And if you have domain knowledge about the process that generated the data, and
    you know it’s simple, then the manifold assumption certainly holds, and dimensionality
    reduction is likely to help.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在训练模型之前降低训练集的维度通常会加快训练速度，但它并不总是导致更好的或更简单的解决方案；这完全取决于数据集。当数据集相对于特征数量较小时，降维通常更有效，特别是如果它是嘈杂的，或者许多特征彼此高度相关（即冗余）。如果你对生成数据的流程有领域知识，并且你知道它是简单的，那么流形假设肯定成立，降维很可能会有所帮助。
- en: Hopefully, you now have a good sense of what the curse of dimensionality is
    and how dimensionality reduction algorithms can fight it, especially when the
    manifold assumption holds. The rest of this chapter will go through some of the
    most popular algorithms for dimensionality reduction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对维度灾难的概念以及降维算法如何与之抗争有了很好的理解，尤其是在流形假设成立的情况下。本章的其余部分将介绍一些最流行的降维算法。
- en: '![Diagrams illustrating how dimensionality reduction affects decision boundaries,
    showing a complex spiral structure on the left and its simplified lower-dimensional
    projections on the right.](assets/hmls_0706.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![说明降维如何影响决策边界的图解，展示左侧的复杂螺旋结构和右侧简化后的低维投影。](assets/hmls_0706.png)'
- en: Figure 7-6\. The decision boundary may not always be simpler with lower dimensions
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. 决策边界不一定总是随着维度降低而简化
- en: PCA
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA
- en: '*Principal component analysis* (PCA) is by far the most popular dimensionality
    reduction algorithm. First it identifies the hyperplane that lies closest to the
    data, and then it projects the data onto it, as shown back in [Figure 7-2](#dataset_3d_plot).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析*（PCA）无疑是降维算法中最受欢迎的。首先，它识别出离数据最近的超平面，然后将数据投影到该平面上，如图[图7-2](#dataset_3d_plot)所示。'
- en: Preserving the Variance
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留方差
- en: Before you can project the training set onto a lower-dimensional hyperplane,
    you first need to choose the right hyperplane. For example, a simple 2D dataset
    is represented on the left in [Figure 7-7](#pca_best_projection_plot), along with
    three different axes (i.e., 1D hyperplanes). On the right is the result of the
    projection of the dataset onto each of these axes. As you can see, the projection
    onto the solid line preserves the maximum variance (top), while the projection
    onto the dotted line preserves very little variance (bottom), and the projection
    onto the dashed line preserves an intermediate amount of variance (middle).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '在你能够将训练集投影到低维超平面之前，你首先需要选择正确的超平面。例如，一个简单的二维数据集在[图7-7](#pca_best_projection_plot)的左侧表示，包括三个不同的轴（即1D超平面）。右侧是数据集在每个这些轴上的投影结果。正如你所看到的，投影到实线上保留了最大的方差（顶部），而投影到点状线上保留了非常少的方差（底部），投影到虚线上保留了中等数量的方差（中间）。 '
- en: '![Diagram of PCA showing a 2D dataset on the left projected onto three different
    1D axes. The solid line preserves the most variance, the dashed line an intermediate
    amount, and the dotted line the least, as depicted by the spread of points on
    the right.](assets/hmls_0707.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PCA示意图，展示左侧的二维数据集投影到三个不同的1D轴上。实线保留了最多的方差，虚线保留了中等数量的方差，点状线保留了最少的方差，如右侧点分布所示。](assets/hmls_0707.png)'
- en: Figure 7-7\. Selecting the subspace on which to project
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. 选择投影的子空间
- en: 'It seems reasonable to select the axis that preserves the maximum amount of
    variance, as it will most likely lose less information than the other projections.
    Consider your shadow on the ground when the sun is directly overhead: it’s a small
    blob that doesn’t look anything like you. But your shadow on a wall at sunrise
    is much larger and it *does* look like you. Another way to justify choosing the
    axis that maximizes the variance is that it is also the axis that minimizes the
    mean squared distance between the original dataset and its projection onto that
    axis. This is the rather simple idea behind PCA, introduced way back [in 1901](https://homl.info/pca)!⁠^([4](ch07.html#id1877))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择保留最大方差量的轴似乎是合理的，因为它与其他投影相比很可能会丢失更少的信息。考虑当太阳正午直射时你在地面的影子：它是一个小而模糊的形状，看起来根本不像你。但你在日出时墙上的影子要大得多，它*确实*像你。另一种证明选择最大化方差轴的理由是，它也是最小化原始数据集与其在该轴上投影之间平均平方距离的轴。这正是主成分分析（PCA）背后的简单想法，这一概念早在[1901年](https://homl.info/pca)就被提出了！⁠^([4](ch07.html#id1877))
- en: Principal Components
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分
- en: 'PCA identifies the axis that accounts for the largest amount of variance in
    the training set. In [Figure 7-7](#pca_best_projection_plot), it is the solid
    line. It also finds a second axis, orthogonal to the first one, that accounts
    for the largest amount of the remaining variance. In this 2D example there is
    no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA
    would also find a third axis, orthogonal to both previous axes, and a fourth,
    a fifth, and so on—as many axes as the number of dimensions in the dataset.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PCA确定了在训练集中解释最大方差量的轴。在[图7-7](#pca_best_projection_plot)中，它是实线。它还找到一个第二个轴，垂直于第一个轴，解释了剩余的最大方差量。在这个2D示例中，没有选择：它是虚线。如果是一个更高维度的数据集，PCA还会找到一个第三个轴，垂直于前两个轴，第四个，第五个，等等——与数据集的维度数量一样多的轴。
- en: The *i*^(th) axis is called the *i*^(th) *principal component* (PC) of the data.
    In [Figure 7-7](#pca_best_projection_plot), the first PC is the axis on which
    vector **c**[**1**] lies, and the second PC is the axis on which vector **c**[**2**]
    lies. In [Figure 7-2](#dataset_3d_plot), the first two PCs are on the projection
    plane, and the third PC is the axis orthogonal to that plane. After the projection,
    back in [Figure 7-3](#dataset_2d_plot), the first PC corresponds to the *z*[1]
    axis, and the second PC corresponds to the *z*[2] axis.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第*i*轴被称为数据的第*i*个*主成分*（PC）。在[图7-7](#pca_best_projection_plot)中，第一个PC是向量**c**[**1**]所在的轴，第二个PC是向量**c**[**2**]所在的轴。在[图7-2](#dataset_3d_plot)中，前两个PC位于投影平面上，第三个PC是垂直于该平面的轴。投影后，回到[图7-3](#dataset_2d_plot)，第一个PC对应于*z*[1]轴，第二个PC对应于*z*[2]轴。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For each principal component, PCA finds a zero-centered unit vector pointing
    along the direction of the PC. Unfortunately, its direction is not guaranteed:
    if you perturb the training set slightly and run PCA again, the unit vector may
    point in the opposite direction. In fact, a pair of unit vectors may even rotate
    or swap if the variances along these two axes are very close. So if you use PCA
    as a preprocessing step before a model, make sure you always retrain the model
    entirely every time you update the PCA transformer: if you don’t and if the PCA’s
    output doesn’t align with the previous version, the model will be very confused.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主成分，PCA找到一个以零为中心的单位向量，指向PC的方向。不幸的是，其方向没有保证：如果你稍微扰动训练集并再次运行PCA，单位向量可能会指向相反的方向。实际上，如果这两个轴上的方差非常接近，一对单位向量甚至可能会旋转或交换。因此，如果你在模型之前使用PCA作为预处理步骤，确保每次更新PCA转换器时都完全重新训练模型：如果你不这样做，并且PCA的输出与之前版本不匹配，模型将会非常困惑。
- en: So how can you find the principal components of a training set? Luckily, there
    is a standard matrix factorization technique called *singular value decomposition*
    (SVD) that can decompose the training set matrix **X** into the product of three
    matrices **U** **Σ** **V**^⊺, where **V** contains the unit vectors that define
    all the principal components that you are looking for, in the correct order, as
    shown in [Equation 7-1](#principal_components_matrix).⁠^([5](ch07.html#id1880))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你如何找到训练集的主成分呢？幸运的是，有一个标准的矩阵分解技术叫做*奇异值分解*（SVD），可以将训练集矩阵**X**分解为三个矩阵**U** **Σ**
    **V**^⊺的乘积，其中**V**包含定义所有要查找的主成分的单位向量，按正确的顺序排列，如[方程7-1](#principal_components_matrix)所示。⁠^([5](ch07.html#id1880))
- en: Equation 7-1\. Principal components matrix
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-1\. 主成分矩阵
- en: $bold upper V equals Start 3 By 4 Matrix 1st Row 1st Column bar 2nd Column bar
    3rd Column Blank 4th Column bar 2nd Row 1st Column bold c 1 2nd Column bold c
    2 3rd Column midline-horizontal-ellipsis 4th Column bold c Subscript n Baseline
    3rd Row 1st Column bar 2nd Column bar 3rd Column Blank 4th Column bar EndMatrix$
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体大写V等于3x4矩阵，第一行第一列是2，第二列是2，第三列是空，第四列是2；第二行第一列是粗体c1，第二列是粗体c2，第三列是中划线省略号，第四列是粗体c的下标n基线；第三行第一列是2，第二列是2，第三列是空，第四列是2**。'
- en: 'The following Python code uses NumPy’s `svd()` function to obtain all the principal
    components of the 3D training set represented back in [Figure 7-2](#dataset_3d_plot),
    then it extracts the two unit vectors that define the first two PCs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码使用NumPy的`svd()`函数获取[图7-2](#dataset_3d_plot)中代表的所有3D训练集的主成分，然后它提取定义前两个PC的两个单位向量：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: PCA assumes that the dataset is centered around the origin. As you will see,
    Scikit-Learn’s PCA classes take care of centering the data for you. If you implement
    PCA yourself (as in the preceding example), or if you use other libraries, don’t
    forget to center the data first.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PCA假设数据集围绕原点中心化。正如你将看到的，Scikit-Learn的PCA类会为你处理数据中心化。如果你自己实现PCA（如前例所示），或者如果你使用其他库，别忘了首先中心化数据。
- en: Projecting Down to d Dimensions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影到*d*维
- en: Once you have identified all the principal components, you can reduce the dimensionality
    of the dataset down to *d* dimensions by projecting it onto the hyperplane defined
    by the first *d* principal components (we will discuss how to choose the number
    of dimensions *d* shortly). Selecting this hyperplane ensures that the projection
    will preserve as much variance as possible. For example, in [Figure 7-2](#dataset_3d_plot)
    the 3D dataset is projected down to the 2D plane defined by the first two principal
    components, preserving a large part of the dataset’s variance. As a result, the
    2D projection looks very much like the original 3D dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了所有的主要成分，你可以通过将其投影到由前*d*个主要成分定义的超平面上来将数据集的维度降低到*d*维。我们将在稍后讨论如何选择维度*d*的数量。选择这个超平面确保投影尽可能多地保留方差。例如，在[图7-2](#dataset_3d_plot)中，3D数据集被投影到由前两个主要成分定义的2D平面上，保留了数据集的大部分方差。因此，2D投影看起来非常像原始的3D数据集。
- en: To project the training set onto the hyperplane and obtain a reduced dataset
    **X**[*d*-proj] of dimensionality *d*, compute the matrix multiplication of the
    training set matrix **X** by the matrix **W**[*d*], defined as the matrix containing
    the first *d* columns of **V**, as shown in [Equation 7-2](#pca_projection).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要将训练集投影到超平面并获得一个维度为*d*的降低数据集**X**[*d*-proj]，计算训练集矩阵**X**与矩阵**W**[*d*]的矩阵乘法，该矩阵定义为包含**V**的前*d*列的矩阵，如[方程7-2](#pca_projection)所示。
- en: Equation 7-2\. Projecting the training set down to *d* dimensions
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-2\. 将训练集投影到*d*维
- en: $bold upper X Subscript d hyphen proj Baseline equals bold upper X bold upper
    W Subscript d$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**X**[*d*] - proj 基线等于 **X**[*d*] **W**[*d*]'
- en: 'The following Python code projects the training set onto the plane defined
    by the first two principal components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码将训练集投影到由前两个主要成分定义的平面上：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There you have it! You now know how to reduce the dimensionality of any dataset
    by projecting it down to any number of dimensions, while preserving as much variance
    as possible.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在知道如何通过将数据集投影到任意数量的维度来降低其维度，同时尽可能多地保留方差。
- en: Using Scikit-Learn
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn
- en: 'Scikit-Learn’s `PCA` class uses SVD to implement PCA, just like we did earlier
    in this chapter. The following code applies PCA to reduce the dimensionality of
    the dataset down to two dimensions (note that it automatically takes care of centering
    the data):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的`PCA`类使用SVD来实现PCA，就像我们在本章前面所做的那样。以下代码将PCA应用于将数据集的维度降低到两个维度（请注意，它自动处理数据中心化）：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After fitting the `PCA` transformer to the dataset, its `components_` attribute
    holds the transpose of **W**[*d*]: it contains one row for each of the first *d*
    principal components.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在将`PCA`转换器拟合到数据集后，其`components_`属性包含**W**[*d*]的转置：它包含每个前*d*个主要成分的一行。
- en: Explained Variance Ratio
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释方差比
- en: 'Another useful piece of information is the *explained variance ratio* of each
    principal component, available via the `explained_variance_ratio_` variable. The
    ratio indicates the proportion of the dataset’s variance that lies along each
    principal component. For example, let’s look at the explained variance ratios
    of the first two components of the 3D dataset represented in [Figure 7-2](#dataset_3d_plot):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的信息是每个主要成分的*解释方差比*，可以通过`explained_variance_ratio_`变量获得。这个比率表示数据集的方差中有多少比例沿着每个主要成分。例如，让我们看看[图7-2](#dataset_3d_plot)中3D数据集的前两个成分的解释方差比：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This output tells us that about 82% of the dataset’s variance lies along the
    first PC, and about 11% lies along the second PC. This leaves about 7% for the
    third PC, so it is reasonable to assume that the third PC probably carries little
    information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出告诉我们，大约82%的数据集方差沿着第一个主成分，大约11%沿着第二个主成分。这留下了大约7%给第三个主成分，因此可以合理地假设第三个主成分可能携带很少的信息。
- en: Choosing the Right Number of Dimensions
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的维度数量
- en: Instead of arbitrarily choosing the number of dimensions to reduce down to,
    it is simpler to choose the number of dimensions that add up to a sufficiently
    large portion of the variance—say, 95%. (An exception to this rule, of course,
    is if you are reducing dimensionality for data visualization, in which case you
    will want to reduce the dimensionality down to 2 or 3.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与随意选择要减少的维度数相比，选择一个总和足够大的方差比例的维度数更简单——比如说，95%。（当然，这个规则的例外是，如果你是为了数据可视化而减少维度，在这种情况下，你将希望将维度减少到2或3。）
- en: 'The following code loads and splits the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and performs PCA without reducing dimensionality, then computes the minimum number
    of dimensions required to preserve 95% of the training set’s variance:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载并拆分MNIST数据集（在第3章[分类章节](ch03.html#classification_chapter)中介绍），然后执行PCA而不减少维度，然后计算保留训练集95%方差所需的最小维度数：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You could then set `n_components=d` and run PCA again, but there’s a better
    option. Instead of specifying the number of principal components you want to preserve,
    you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio
    of variance you wish to preserve:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将`n_components=d`设置为再次运行PCA，但有一个更好的选择。与其指定要保留的主成分数量，你可以将`n_components`设置为介于0.0和1.0之间的浮点数，表示你希望保留的方差比例：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The actual number of components is determined during training, and it is stored
    in the `n_components_` attribute:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的组件数量是在训练过程中确定的，并存储在`n_components_`属性中：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Yet another option is to plot the explained variance as a function of the number
    of dimensions (simply plot `cumsum`; see [Figure 7-8](#explained_variance_plot)).
    There will usually be an elbow in the curve, where the explained variance stops
    growing fast. In this case, you can see that reducing the dimensionality down
    to about 100 dimensions wouldn’t lose too much explained variance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是将解释方差作为维度数量的函数进行绘图（简单地绘制`cumsum`；参见[图7-8](#explained_variance_plot))。通常曲线中会有一个肘部，表示解释方差停止快速增长。在这种情况下，你可以看到将维度减少到大约100维不会损失太多解释方差。
- en: '![Graph showing explained variance as a function of dimensions, with an elbow
    indicating diminishing returns beyond 100 dimensions.](assets/hmls_0708.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![展示解释方差作为维度函数的图表，肘部表示超过100维度的递减回报。](assets/hmls_0708.png)'
- en: Figure 7-8\. Explained variance as a function of the number of dimensions
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. 解释方差作为维度数量的函数
- en: 'Alternatively, if you are using dimensionality reduction as a preprocessing
    step for a supervised learning task (e.g., classification), then you can tune
    the number of dimensions as you would any other hyperparameter (see [Chapter 2](ch02.html#project_chapter)).
    For example, the following code example creates a two-step pipeline, first reducing
    dimensionality using PCA, then classifying using a random forest. Next, it uses
    `RandomizedSearchCV` to find a good combination of hyperparameters for both PCA
    and the random forest classifier. This example does a quick search, tuning only
    2 hyperparameters, training on just 1,000 instances, and running for just 10 iterations,
    but feel free to do a more thorough search if you have the time:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你将降维作为监督学习任务（例如，分类）的预处理步骤，那么你可以像调整任何其他超参数一样调整维度数（参见第2章[项目章节](ch02.html#project_chapter)）。例如，以下代码示例创建了一个两步管道，首先使用PCA减少维度，然后使用随机森林进行分类。接下来，它使用`RandomizedSearchCV`找到PCA和随机森林分类器的好超参数组合。这个例子进行了快速搜索，仅调整了2个超参数，在仅1,000个实例上进行训练，并且只运行了10次迭代，但如果你有时间，可以自由地进行更彻底的搜索：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s look at the best hyperparameters found:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看找到的最佳超参数：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s interesting to note how low the optimal number of components is: we reduced
    a 784-dimensional dataset to just 57 dimensions! This is tied to the fact that
    we used a random forest, which is a pretty powerful model. If we used a linear
    model instead, such as an `SGDClassifier`, the search would find that we need
    to preserve more dimensions (about 75).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，最优组件数量如此之低：我们将一个784维数据集减少到了仅仅57维！这与我们使用了随机森林这一相当强大的模型有关。如果我们使用线性模型，例如`SGDClassifier`，搜索将发现我们需要保留更多的维度（大约75）。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may also care about the model’s size and speed, not just it’s performance.
    The fewer dimensions, the smaller the model, and the faster training and inference
    will be. But if you shrink the data too much, then you will lose too much signal
    and your model will underfit. You need to choose the right balance of speed, size,
    and performance for your particular use case.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还关心模型的大小和速度，而不仅仅是它的性能。维度越少，模型越小，训练和推理速度越快。但是，如果你过度压缩数据，那么你会丢失太多信号，你的模型将欠拟合。你需要为特定的用例选择速度、大小和性能之间的正确平衡。
- en: PCA for Compression
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA用于压缩
- en: After dimensionality reduction, the training set takes up much less space. For
    example, after applying PCA to the MNIST dataset while preserving 95% of its variance,
    we are left with 154 features, instead of the original 784 features. So the dataset
    is now less than 20% of its original size, and we only lost 5% of its variance!
    This is a reasonable compression ratio, and it’s easy to see how such a size reduction
    would speed up a classification algorithm tremendously.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在降维之后，训练集占用的空间要小得多。例如，在将PCA应用于MNIST数据集并保留其95%方差的情况下，我们剩下154个特征，而不是原始的784个特征。因此，数据集现在的大小不到原始大小的20%，而我们只丢失了5%的方差！这是一个合理的压缩比率，很容易看出这种尺寸减小将极大地加快分类算法的速度。
- en: It is also possible to decompress the reduced dataset back to 784 dimensions
    by applying the inverse transformation of the PCA projection. This won’t give
    you back the original data, since the projection lost a bit of information (within
    the 5% variance that was dropped), but it will likely be close to the original
    data. The mean squared distance between the original data and the reconstructed
    data (compressed and then decompressed) is called the *reconstruction error*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用PCA投影的逆变换，也可以将降维后的数据集还原到784维。这不会给你回原始数据，因为投影丢失了一些信息（在丢弃的5%方差内），但它可能会接近原始数据。原始数据与重建数据（压缩后再解压缩）之间的平均平方距离被称为*重建误差*。
- en: 'The `inverse_transform()` method lets us decompress the reduced MNIST dataset
    back to 784 dimensions:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`inverse_transform()`方法使我们能够将降维的MNIST数据集解压缩回784维：'
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Figure 7-9](#mnist_compression_plot) shows a few digits from the original
    training set (on the left), and the corresponding digits after compression and
    decompression. You can see that there is a slight image quality loss, but the
    digits are still mostly intact.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#mnist_compression_plot)显示了原始训练集中的几个数字（在左侧），以及压缩和解压缩后的相应数字。你可以看到图像质量略有损失，但数字仍然大部分完好。'
- en: '![Comparison of original MNIST digits with their compressed and decompressed
    versions, showing slight quality loss while preserving 95% of the variance.](assets/hmls_0709.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![比较原始MNIST数字与其压缩和解压缩版本，显示在保留95%方差的同时略有质量损失。](assets/hmls_0709.png)'
- en: Figure 7-9\. MNIST compression that preserves 95% of the variance
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. 保留95%方差的MNIST压缩
- en: The equation for the inverse transformation is shown in [Equation 7-3](#inverse_pca).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 逆变换的方程式显示在[方程7-3](#inverse_pca)中。
- en: Equation 7-3\. PCA inverse transformation, back to the original number of dimensions
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-3\. PCA逆变换，回到原始的维度数
- en: $bold upper X Subscript recovered Baseline equals bold upper X Subscript d hyphen
    proj Baseline bold upper W Subscript d Baseline Superscript upper T$
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $ \text{bold upper X Subscript recovered Baseline} = \text{bold upper X Subscript
    d hyphen proj Baseline} \text{bold upper W Subscript d Baseline} \text{Superscript
    upper T} $
- en: Randomized PCA
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机PCA
- en: 'If you set the `svd_solver` hyperparameter to `"randomized"`, Scikit-Learn
    uses a stochastic algorithm called *randomized PCA* that quickly finds an approximation
    of the first *d* principal components. Its computational complexity is *O*(*m*
    × *d*²) + *O*(*d*³), instead of *O*(*m* × *n*²) + *O*(*n*³) for the full SVD approach,
    so it is dramatically faster than full SVD when *d* is much smaller than *n*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将`svd_solver`超参数设置为`"randomized"`，Scikit-Learn将使用一种称为*随机PCA*的随机算法，该算法快速找到前*d*个主成分的近似值。其计算复杂度为*O*(*m*
    × *d*²) + *O*(*d*³)，而不是全SVD方法的*O*(*m* × *n*²) + *O*(*n*³)，因此当*d*远小于*n*时，它比全SVD快得多：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tip
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'By default, `svd_solver` is set to `"auto"`: if the input data has few features
    (*n* < 1,000) and at least 10 times more samples (*m* > 10*n* ), then the `"covariance_eigh"`
    solver is used, which is very fast in these conditions. Otherwise, if max(*m*,
    *n*) > 500 and `n_components` is an integer smaller than 80% of min(*m*, *n*),
    it uses the `"randomized"` solver. In other cases, it uses the full SVD approach.
    If you want to force Scikit-Learn to use full SVD, trading compute time for a
    slightly more precise result, you can set `svd_solver="full"`.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`svd_solver`设置为`"auto"`：如果输入数据具有少量特征（*n* < 1,000）并且样本数至少是特征数的10倍（*m* >
    10*n*），则使用`"covariance_eigh"`求解器，在这些条件下非常快。否则，如果max(*m*, *n*) > 500且`n_components`是一个小于min(*m*,
    *n*)的80%的整数，则使用`"randomized"`求解器。在其他情况下，它使用完整的SVD方法。如果您想强制Scikit-Learn使用完整的SVD，以牺牲计算时间为代价换取略微更精确的结果，可以将`svd_solver="full"`。
- en: Incremental PCA
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量PCA
- en: One problem with the preceding implementations of PCA is that they require the
    whole training set to fit in memory in order for the algorithm to run. Fortunately,
    *incremental PCA* (IPCA) algorithms have been developed that allow you to split
    the training set into mini-batches and feed these in one mini-batch at a time.
    This is useful for large training sets and for applying PCA online (i.e., on the
    fly, as new instances arrive).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的PCA实现的一个问题是，它们要求整个训练集都适合内存，以便算法可以运行。幸运的是，已经开发出了允许您将训练集分成小批量并一次喂入一个小批量的*增量PCA*（IPCA）算法。这对于大型训练集和在在线（即实时，当新实例到达时）应用PCA非常有用。
- en: 'The following code splits the MNIST training set into 100 mini-batches (using
    NumPy’s `array_split()` function) and feeds them to Scikit-Learn’s `IncrementalPCA`
    class⁠^([6](ch07.html#id1905)) to reduce the dimensionality of the MNIST dataset
    down to 154 dimensions, just like before. Note that you must call the `partial_fit()`
    method with each mini-batch, rather than the `fit()` method with the whole training
    set:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将MNIST训练集分成100个小批量（使用NumPy的`array_split()`函数），并将它们喂给Scikit-Learn的`IncrementalPCA`类⁠^([6](ch07.html#id1905))，将MNIST数据集的维度降低到154维，就像之前一样。请注意，您必须使用每个小批量调用`partial_fit()`方法，而不是使用整个训练集的`fit()`方法：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, you can use NumPy’s `memmap` class, which allows you to manipulate
    a large array stored in a binary file on disk as if it were entirely in memory;
    the class loads only the data it needs in memory, when it needs it. To demonstrate
    this, let’s first create a memory-mapped (memmap) file and copy the MNIST training
    set to it, then call `flush()` to ensure that any data still in the cache gets
    saved to disk. In real life, `X_train` would typically not fit in memory, so you
    would load it chunk by chunk and save each chunk to the right part of the memmap
    array:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用NumPy的`memmap`类，它允许您像在内存中完全一样地操作存储在磁盘上的二进制文件中的大数组；该类仅在需要时在内存中加载所需的数据。为了演示这一点，我们首先创建一个内存映射（memmap）文件，并将MNIST训练集复制到其中，然后调用`flush()`以确保任何仍在缓存中的数据都保存到磁盘。在现实生活中，`X_train`通常不会适合内存，因此您需要分块加载它，并将每个块保存到memmap数组的正确部分：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we can load the memmap file and use it like a regular NumPy array. Let’s
    use the `IncrementalPCA` class to reduce its dimensionality. Since this algorithm
    uses only a small part of the array at any given time, memory usage remains under
    control. This makes it possible to call the usual `fit()` method instead of `partial_fit()`,
    which is quite convenient:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以加载memmap文件并像使用常规NumPy数组一样使用它。让我们使用`IncrementalPCA`类来降低其维度。由于该算法在任何给定时间只使用数组的一小部分，内存使用量保持可控。这使得可以调用通常的`fit()`方法而不是`partial_fit()`，这非常方便：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Warning
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Only the raw binary data is saved to disk, so you need to specify the data type
    and shape of the array when you load it. If you omit the shape, `np.memmap()`
    returns a 1D array.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只保存到磁盘的是原始二进制数据，因此当您加载它时，需要指定数组的类型和形状。如果您省略了形状，`np.memmap()`将返回一个一维数组。
- en: 'For very high-dimensional datasets, PCA can be too slow. As you saw earlier,
    even if you use randomized PCA, its computational complexity is still *O*(*m*
    × *d*²) + *O*(*d*³), so the target number of dimensions *d* must not be too large.
    If you are dealing with a dataset with tens of thousands of features or more (e.g.,
    images), then training may become much too slow: in this case, you should consider
    using random projection instead.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常高维的数据集，PCA可能太慢。如您之前所见，即使您使用随机PCA，其计算复杂度仍然是*O*(*m* × *d*²) + *O*(*d*³)，因此目标维度数*d*不能太大。如果您正在处理具有数万个特征或更多（例如，图像）的数据集，则训练可能会变得非常慢：在这种情况下，您应该考虑使用随机投影。
- en: Random Projection
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机投影
- en: As its name suggests, the random projection algorithm projects the data to a
    lower-dimensional space using a random linear projection. This may sound crazy,
    but it turns out that such a random projection is actually very likely to preserve
    distances fairly well, as was demonstrated mathematically by William B. Johnson
    and Joram Lindenstrauss in a famous lemma. So, two similar instances will remain
    similar after the projection, and two very different instances will remain very
    different.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，随机投影算法通过随机线性投影将数据投影到低维空间。这听起来可能有些疯狂，但事实证明，这种随机投影实际上很可能很好地保留距离，正如William
    B. Johnson和Joram Lindenstrauss在著名的引理中通过数学证明的那样。因此，两个相似的实例在投影后仍然相似，两个非常不同的实例在投影后仍然非常不同。
- en: 'Obviously, the more dimensions you drop, the more information is lost, and
    the more distances get distorted. So how can you choose the optimal number of
    dimensions? Well, Johnson and Lindenstrauss came up with an equation that determines
    the minimum number of dimensions to preserve in order to ensure—with high probability—that
    distances won’t change by more than a given tolerance. For example, if you have
    a dataset containing *m* = 5,000 instances with *n* = 20,000 features each, and
    you don’t want the squared distance between any two instances to change by more
    than *ε* = 10%,^([7](ch07.html#id1908)) then you should project the data down
    to *d* dimensions, with *d* ≥ 4 log(*m*) / (½ *ε*² - ⅓ *ε*³), which is 7,300 dimensions.
    That’s quite a significant dimensionality reduction! Notice that the equation
    does not use *n*, it only relies on *m* and *ε*. This equation is implemented
    by the `johnson_lindenstrauss_min_dim()` function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你丢弃的维度越多，损失的信息就越多，距离的扭曲也就越严重。那么，你如何选择最优的维度数呢？嗯，Johnson和Lindenstrauss提出了一个方程，该方程确定了为了确保——以高概率——距离不会超过给定的容差而需要保留的最小维度数。例如，如果你有一个包含*m*
    = 5,000个实例的数据集，每个实例有*n* = 20,000个特征，并且你不想任何两个实例之间的平方距离变化超过*ε* = 10%，^([7](ch07.html#id1908))，那么你应该将数据投影到*d*维度，其中*d*
    ≥ 4 log(*m*) / (½ *ε*² - ⅓ *ε*³)，即7,300个维度。这是一个相当显著的降维！请注意，该方程没有使用*n*，它只依赖于*m*和*ε*。这个方程通过`johnson_lindenstrauss_min_dim()`函数实现：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can just generate a random matrix **P** of shape [*d*, *n*], where each
    item is sampled randomly from a Gaussian distribution with mean 0 and variance
    1 / *d*, and use it to project a dataset from *n* dimensions down to *d*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需生成一个形状为[*d*, *n*]的随机矩阵**P**，其中每个元素都是从均值为0，方差为1/*d*的高斯分布中随机抽取的，并使用它将数据集从*n*维度投影到*d*：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That’s all there is to it! It’s simple and efficient, and training is almost
    instantaneous: the only thing the algorithm needs to create the random matrix
    is the dataset’s shape. The data itself is not used at all. This makes random
    projection particularly well suited for very high-dimensional data such as text
    or genomics with millions of features, or very sparse data, for which even randomized
    PCA may take too long to train and require too much memory. At inference time,
    random projection is just as fast as PCA (i.e., one matrix multiplication). That
    said, random projection is not a silver bullet: it loses a bit more signal than
    PCA, so there’s a trade-off between training speed and performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部！它简单高效，训练几乎瞬间完成：算法创建随机矩阵所需的所有信息仅仅是数据集的形状。数据本身根本不使用。这使得随机投影特别适合处理非常高维度的数据，例如具有数百万个特征的文本或基因组学数据，或者非常稀疏的数据，对于这些数据，即使是随机PCA也可能需要太长时间来训练，并且需要太多的内存。在推理时间，随机投影与PCA一样快（即，一次矩阵乘法）。但话虽如此，随机投影并不是万能的：它比PCA丢失了更多的信号，因此在训练速度和性能之间有一个权衡。
- en: 'Scikit-Learn offers a `GaussianRandomProjection` class to do exactly what we
    just did: when you call its `fit()` method, it uses `johnson_lindenstrauss_min_dim()`
    to determine the output dimensionality, then it generates a random matrix, which
    it stores in the `components_` attribute. Then when you call `transform()`, it
    uses this matrix to perform the projection. When creating the transformer, you
    can set `eps` to tweak *ε* (it defaults to 0.1), and `n_components` to force a
    specific target dimensionality *d* (you will probably want to fine-tune these
    hyperparameters using cross-validation). The following code example gives the
    same result as the preceding code (you can also verify that `gaussian_rnd_proj.components_`
    is equal to `P`):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了一个 `GaussianRandomProjection` 类来执行我们刚才所做的操作：当您调用其 `fit()` 方法时，它使用
    `johnson_lindenstrauss_min_dim()` 确定输出维度，然后生成一个随机矩阵，并将其存储在 `components_` 属性中。然后当您调用
    `transform()` 时，它使用这个矩阵进行投影。在创建转换器时，您可以设置 `eps` 来调整 *ε*（默认为 0.1），并设置 `n_components`
    来强制特定的目标维度 *d*（您可能希望使用交叉验证来微调这些超参数）。以下代码示例给出了与前面代码相同的结果（您还可以验证 `gaussian_rnd_proj.components_`
    是否等于 `P`）：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Scikit-Learn also provides a second random projection transformer, known as
    `SparseRandomProjection`. It determines the target dimensionality in the same
    way, generates a random matrix of the same shape, and performs the projection
    identically. The main difference is that the random matrix is sparse. This means
    it uses much less memory: about 25 MB instead of almost 1.2 GB in the preceding
    example! And it’s also much faster, both to generate the random matrix and to
    reduce dimensionality: about 50% faster in this case. Moreover, if the input is
    sparse, the transformation keeps it sparse (unless you set `dense_output=True`).
    Lastly, it enjoys the same distance-preserving property as the previous approach,
    and the quality of the dimensionality reduction is comparable (only very slightly
    less accurate). In short, it’s usually preferable to use this transformer instead
    of the first one, especially for large or sparse datasets.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 还提供了一个名为 `SparseRandomProjection` 的第二个随机投影转换器。它以相同的方式确定目标维度，生成相同形状的随机矩阵，并执行相同的投影。主要区别在于随机矩阵是稀疏的。这意味着它使用的内存要少得多：大约
    25 MB，而前面的例子中接近 1.2 GB！而且它也更快，无论是生成随机矩阵还是降低维度：在这种情况下大约快 50%。此外，如果输入是稀疏的，转换会保持其稀疏性（除非您设置
    `dense_output=True`）。最后，它还享有与先前方法相同的距离保持属性，降维的质量也相当（仅略低一些精度）。简而言之，通常更倾向于使用这个转换器而不是第一个，特别是对于大型或稀疏数据集。
- en: 'The ratio *r* of nonzero items in the sparse random matrix is called its *density*.
    By default, it is equal to $StartFraction 1 Over StartRoot n EndRoot EndFraction$
    . With 20,000 features, this means that only 1 in ~141 cells in the random matrix
    is nonzero: that’s quite sparse! You can set the `density` hyperparameter to another
    value if you prefer. Each cell in the sparse random matrix has a probability *r*
    of being nonzero, and each nonzero value is either –*v* or +*v* (both equally
    likely), where *v* = $StartFraction 1 Over StartRoot d r EndRoot EndFraction$
    .'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏随机矩阵中非零项的比率 *r* 被称为其 *密度*。默认情况下，它等于 $StartFraction 1 Over StartRoot n EndRoot
    EndFraction$ 。对于 20,000 个特征，这意味着在随机矩阵中只有大约 1/141 的单元格是非零的：这相当稀疏！如果您愿意，可以将 `density`
    超参数设置为另一个值。稀疏随机矩阵中的每个单元格都有 *r* 的概率是非零的，每个非零值要么是 –*v* 或 +*v*（两者同样可能），其中 *v* = $StartFraction
    1 Over StartRoot d r EndRoot EndFraction$ 。
- en: 'If you want to perform the inverse transform, you first need to compute the
    pseudoinverse of the components matrix using SciPy’s `pinv()` function, then multiply
    the reduced data by the transpose of the pseudoinverse:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想执行逆变换，您首先需要使用 SciPy 的 `pinv()` 函数计算成分矩阵的伪逆，然后将降维数据乘以伪逆的转置：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Warning
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Computing the pseudoinverse may take a very long time if the components matrix
    is large, as the computational complexity of `pinv()` is *O*(*dn*²) if *d* < *n*,
    or *O*(*nd*²) otherwise.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成分矩阵很大，计算伪逆可能需要非常长的时间，因为 `pinv()` 的计算复杂度为 *O*(*dn*²) 如果 *d* < *n*，否则为 *O*(*nd*²)。
- en: In summary, random projection is a simple, fast, memory-efficient, and surprisingly
    powerful dimensionality reduction algorithm that you should keep in mind, especially
    when you deal with high-dimensional datasets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，随机投影是一种简单、快速、内存高效且出奇强大的降维算法，您应该记住它，尤其是在处理高维数据集时。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Random projection is not always used to reduce the dimensionality of large
    datasets. For example, a [2017 paper](https://homl.info/flies)⁠^([8](ch07.html#id1916))
    by Sanjoy Dasgupta et al. showed that the brain of a fruit fly implements an analog
    of random projection to map dense low-dimensional olfactory inputs to sparse high-dimensional
    binary outputs: for each odor, only a small fraction of the output neurons get
    activated, but similar odors activate many of the same neurons. This is similar
    to a well-known algorithm called *locality sensitive hashing* (LSH), which is
    typically used in search engines to group similar documents (see [Chapter 17](ch17.html#speedup_chapter)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机投影并不总是用于降低大型数据集的维度。例如，Sanjoy Dasgupta等人于2017年发表的一篇论文[8](ch07.html#id1916)表明，果蝇的大脑实现了一种随机投影的类似物，将密集的低维嗅觉输入映射到稀疏的高维二进制输出：对于每种气味，只有一小部分输出神经元被激活，但相似的气味会激活许多相同的神经元。这与一个众所周知的算法*局部敏感哈希*
    (LSH) 类似，通常用于搜索引擎中相似文档的分组（参见第17章[17](ch17.html#speedup_chapter)）。
- en: LLE
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLE
- en: '[*Locally linear embedding* (LLE)](https://homl.info/lle)⁠^([9](ch07.html#id1921))
    is a *nonlinear dimensionality reduction* (NLDR) technique. It is a manifold learning
    technique that does not rely on projections, unlike PCA and random projection.
    In a nutshell, LLE first determines how each training instance linearly relates
    to its nearest neighbors, then it looks for a low-dimensional representation of
    the training set where these local relationships are best preserved (more details
    shortly). This approach makes it particularly good at unrolling twisted manifolds,
    especially when there is not too much noise. However, it does not scale well so
    it is mostly for small or medium sized datasets.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[*局部线性嵌入* (LLE)](https://homl.info/lle)⁠^([9](ch07.html#id1921))是一种*非线性降维*
    (NLDR) 技术。它是一种流形学习方法，与PCA和随机投影不同，它不依赖于投影。简而言之，LLE首先确定每个训练实例如何线性地与其最近邻相关联，然后它寻找一个低维表示的训练集，在这些局部关系中最好地保留了这些关系（更多细节稍后提供）。这种方法使其特别擅长展开扭曲的流形，尤其是在噪声不是太多的情况下。然而，它的扩展性不好，所以它主要用于小型或中型数据集。'
- en: 'The following code makes a Swiss roll, then uses Scikit-Learn’s `LocallyLinearEmbedding`
    class to unroll it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个瑞士卷，然后使用Scikit-Learn的`LocallyLinearEmbedding`类将其展开：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The variable `t` is a 1D NumPy array containing the position of each instance
    along the rolled axis of the Swiss roll. We don’t use it in this example, but
    it can be used as a target for a nonlinear regression task. The resulting 2D dataset
    is shown in [Figure 7-10](#lle_unrolling_plot).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`t`是一个包含瑞士卷卷轴上每个实例位置的1D NumPy数组。在这个例子中我们没有使用它，但它可以用作非线性回归任务的目标。得到的二维数据集在[图7-10](#lle_unrolling_plot)中显示。
- en: '![Visualization of an unrolled Swiss roll dataset using Locally Linear Embedding
    (LLE), showing the transformation of data points into a stretched band with preserved
    local distances.](assets/hmls_0710.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![使用局部线性嵌入 (LLE) 展开的瑞士卷数据集的可视化，显示了数据点转换为具有保留局部距离的拉伸带状物的转换。](assets/hmls_0710.png)'
- en: Figure 7-10\. Unrolled Swiss roll using LLE
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10\. 使用LLE展开的瑞士卷
- en: 'As you can see, the Swiss roll is completely unrolled, and the distances between
    instances are locally well preserved. However, distances are not preserved on
    a larger scale: the unrolled Swiss roll should be a rectangle, not this kind of
    stretched and twisted band. Nevertheless, LLE did a pretty good job of modeling
    the manifold.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，瑞士卷已经被完全展开，实例之间的距离在局部得到了很好的保留。然而，在更大的尺度上，距离并没有得到保留：展开的瑞士卷应该是一个矩形，而不是这种拉伸和扭曲的带状物。尽管如此，LLE在建模流形方面做得相当不错。
- en: 'Here’s how LLE works: for each training instance **x**^((*i*)), the algorithm
    identifies its *k*-nearest neighbors (in the preceding code *k* = 10), then tries
    to reconstruct **x**^((*i*)) as a linear function of these neighbors. More specifically,
    it tries to find the weights *w*[*i,j*] such that the squared distance between
    **x**^((*i*)) and $sigma-summation Underscript j equals 1 Overscript m Endscripts
    w Subscript i comma j Baseline bold x Superscript left-parenthesis j right-parenthesis$
    is as small as possible, assuming *w*[*i,j*] = 0 if **x**^((*j*)) is not one of
    the *k*-nearest neighbors of **x**^((*i*)). Thus the first step of LLE is the
    constrained optimization problem described in [Equation 7-4](#lle_first_step),
    where **W** is the weight matrix containing all the weights *w*[*i,j*]. The second
    constraint simply normalizes the weights for each training instance **x**^((*i*)).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LLE（局部线性嵌入）的工作原理：对于每个训练实例 **x**^((*i*)), 算法会识别其 *k*-近邻（在先前的代码中 *k* = 10），然后尝试将这些近邻作为线性函数来重建
    **x**^((*i*)). 更具体地说，它试图找到权重 *w*[*i,j*]，使得 **x**^((*i*)) 与 $sigma-summation Underscript
    j equals 1 Overscript m Endscripts w Subscript i comma j Baseline bold x Superscript
    left-parenthesis j right-parenthesis$ 之间的平方距离尽可能小，假设如果 **x**^((*j*)) 不是 **x**^((*i*))
    的 *k*-近邻之一，则 *w*[*i,j*] = 0。因此，LLE的第一步是[方程7-4](#lle_first_step)中描述的约束优化问题，其中 **W**
    是包含所有权重 *w*[*i,j*] 的权重矩阵。第二个约束简单地规范化了每个训练实例 **x**^((*i*)*) 的权重。
- en: 'Equation 7-4\. LLE step 1: linearly modeling local relationships'
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-4\. LLE步骤1：线性建模局部关系
- en: <mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover accent="true"><mi
    mathvariant="bold">W</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo form="prefix">argmin</mo>
    <mi mathvariant="bold">W</mi></munder> <mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover></mstyle> <msup><mfenced
    separators="" open="(" close=")"><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>-</mo><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext>
    <mtext>to</mtext> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>w</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub> <mo>=</mo>
    <mn>0</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mtext>is</mtext>
    <mtext>not</mtext> <mtext>one</mtext> <mtext>of</mtext> <mtext>the</mtext> <mi>k</mi>
    <mtext>n.n.</mtext> <mtext>of</mtext> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo lspace="0%" rspace="0%">,</mo> <mn>2</mn>
    <mo lspace="0%" rspace="0%">,</mo> <mo>⋯</mo> <mo lspace="0%" rspace="0%">,</mo>
    <mi>m</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover accent="true"><mi
    mathvariant="bold">W</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo form="prefix">argmin</mo>
    <mi mathvariant="bold">W</mi></munder> <mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover></mstyle> <msup><mfenced
    separators="" open="(" close=")"><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>-</mo><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext>
    <mtext>to</mtext> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>w</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub> <mo>=</mo>
    <mn>0</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mtext>is</mtext>
    <mtext>not</mtext> <mtext>one</mtext> <mtext>of</mtext> <mtext>the</mtext> <mi>k</mi>
    <mtext>n.n.</mtext> <mtext>of</mtext> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo lspace="0%" rspace="0%">,</mo> <mn>2</mn>
    <mo lspace="0%" rspace="0%">,</mo> <mo>⋯</mo> <mo lspace="0%" rspace="0%">,</mo>
    <mi>m</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
- en: 'After this step, the weight matrix $ModifyingAbove bold upper W With caret$
    (containing the weights $ModifyingAbove w With caret Subscript i comma j$ ) encodes
    the local linear relationships between the training instances. The second step
    is to map the training instances into a *d*-dimensional space (where *d* < *n*)
    while preserving these local relationships as much as possible. If **z**^((*i*))
    is the image of **x**^((*i*)) in this *d*-dimensional space, then we want the
    squared distance between **z**^((*i*)) and $sigma-summation Underscript j equals
    1 Overscript m Endscripts ModifyingAbove w With caret Subscript i comma j Baseline
    bold z Superscript left-parenthesis j right-parenthesis$ to be as small as possible.
    This idea leads to the unconstrained optimization problem described in [Equation
    7-5](#lle_second_step). It looks very similar to the first step, but instead of
    keeping the instances fixed and finding the optimal weights, we are doing the
    reverse: keeping the weights fixed and finding the optimal position of the instances’
    images in the low-dimensional space. Note that **Z** is the matrix containing
    all **z**^((*i*)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，权重矩阵 $通过上标加粗大写W的求和$（包含权重 $通过上标加粗w下标i，j的求和$）编码了训练实例之间的局部线性关系。第二步是将训练实例映射到一个
    *d*- 维空间（其中 *d* < *n*），尽可能多地保留这些局部关系。如果 **z**^((*i*)) 是 **x**^((*i*)) 在这个 *d*-
    维空间中的像，那么我们希望 **z**^((*i*)) 与 $sigma-summation下标j等于1上标m的求和，通过上标加粗w下标i，j基线加粗z上标左括号j右括号$
    之间的平方距离尽可能小。这个想法导致了[方程7-5](#lle_second_step)中描述的无约束优化问题。它看起来非常类似于第一步，但不是保持实例固定并找到最优权重，而是相反：保持权重固定并找到实例在低维空间中的最优位置。请注意，**Z**
    是包含所有 **z**^((*i*)*) 的矩阵。
- en: 'Equation 7-5\. LLE step 2: reducing dimensionality while preserving relationships'
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-5\. LLE步骤2：在保留关系的同时降低维度
- en: $ModifyingAbove bold upper Z With caret equals argmin Underscript bold upper
    Z Endscripts sigma-summation Underscript i equals 1 Overscript m Endscripts left-parenthesis
    bold z Superscript left-parenthesis i right-parenthesis Baseline minus sigma-summation
    Underscript j equals 1 Overscript m Endscripts ModifyingAbove w With caret Subscript
    i comma j Baseline bold z Superscript left-parenthesis j right-parenthesis Baseline
    right-parenthesis squared$
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: $通过上标加粗大写Z与上标加粗大写Z的求和等于argmin下标加粗大写Z的求和，下标i等于1上标m的求和，左括号加粗z上标左括号i右括号基线减去下标j等于1上标m的求和，通过上标加粗w下标i，j基线加粗z上标左括号j右括号基线右括号平方$
- en: 'Scikit-Learn’s LLE implementation has the following computational complexity:
    *O*(*m* log(*m*)*n* log(*k*)) for finding the *k*-nearest neighbors, *O*(*mnk*³)
    for optimizing the weights, and *O*(*dm*²) for constructing the low-dimensional
    representations. Unfortunately, the *m*² in the last term makes this algorithm
    scale poorly to very large datasets.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的LLE实现具有以下计算复杂度：*O*(*m* log(*m*)*n* log(*k*))用于找到 *k*- 个最近邻，*O*(*mnk*³)用于优化权重，以及
    *O*(*dm*²)用于构建低维表示。不幸的是，最后一个项中的 *m*² 使得该算法在非常大的数据集上扩展性不佳。
- en: As you can see, LLE is quite different from the projection techniques, and it’s
    significantly more complex, but it can also construct much better low-dimensional
    representations, especially if the data is nonlinear.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，局部线性嵌入 (LLE) 与投影技术相当不同，并且它更加复杂，但它也能构建更好的低维表示，尤其是当数据是非线性的。
- en: Other Dimensionality Reduction Techniques
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他降维技术
- en: 'Before we conclude this chapter, let’s take a quick look at a few other popular
    dimensionality reduction techniques available in Scikit-Learn:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这一章之前，让我们快速浏览一下Scikit-Learn中可用的其他一些流行的降维技术：
- en: '`sklearn.manifold.MDS`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.MDS`'
- en: '*Multidimensional scaling* (MDS) reduces dimensionality while trying to preserve
    the distances between the instances. Random projection does that for high-dimensional
    data, but it doesn’t work well on low-dimensional data.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*多维尺度分析* (MDS) 在尝试保留实例之间的距离的同时降低维度。随机投影适用于高维数据，但在低维数据上效果不佳。'
- en: '`sklearn.manifold.Isomap`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.Isomap`'
- en: '*Isomap* creates a graph by connecting each instance to its nearest neighbors,
    then reduces dimensionality while trying to preserve the *geodesic distances*
    between the instances. The geodesic distance between two nodes in a graph is the
    number of nodes on the shortest path between these nodes. This approach works
    best when the data lies on a fairly smooth and low-dimensional manifold with a
    single global structure (e.g., the Swiss roll).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*Isomap* 通过连接每个实例与其最近邻来创建一个图，然后在尝试保留实例之间的*测地距离*的同时降低维度。图中两个节点之间的测地距离是这两个节点之间最短路径上的节点数。这种方法在数据位于相当平滑且低维的流形上，并且具有单一全局结构（例如，瑞士卷）时效果最佳。'
- en: '`sklearn.manifold.TSNE`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.TSNE`'
- en: '*t-distributed stochastic neighbor embedding* (t-SNE) reduces dimensionality
    while trying to keep similar instances close and dissimilar instances apart. It
    is mostly used for visualization, in particular to visualize clusters of instances
    in high-dimensional space. For example, in the exercises at the end of this chapter
    you will use t-SNE to visualize a 2D map of the MNIST images. However, it is not
    meant to be used as a preprocessing stage for an ML model.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*t-分布随机邻域嵌入* (t-SNE) 在尝试保持相似实例靠近和不同实例分离的同时降低维度。它主要用于可视化，特别是用于可视化高维空间中的实例簇。例如，在本章末尾的练习中，您将使用t-SNE来可视化MNIST图像的2D地图。然而，它不打算用作ML模型的预处理阶段。'
- en: '`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`'
- en: '*Linear discriminant analysis* (LDA) is a linear classification algorithm that,
    during training, learns the most discriminative axes between the classes. These
    axes can then be used to define a hyperplane onto which to project the data. The
    benefit of this approach is that the projection will keep classes as far apart
    as possible, so LDA is a good technique to reduce dimensionality before running
    another classification algorithm (unless LDA alone is sufficient).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性判别分析* (LDA) 是一种线性分类算法，在训练过程中，学习类之间的最具判别性的轴。然后可以使用这些轴来定义一个超平面，将数据投影到该平面上。这种方法的好处是投影将尽可能地将类分开，因此LDA是在运行另一个分类算法之前降低维度的良好技术（除非LDA本身就足够了）。'
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '*Uniform Manifold Approximation and Projection* (UMAP) is another popular dimensionality
    reduction technique for visualization. While t-SNE is better at preserving the
    local structure, especially clusters, UMAP tries to preserve both the local and
    global structures. Moreover, it scales better to large datasets. Sadly, it is
    not available in Scikit-Learn, but there’s a good implementation in the [umap-learn
    package](https://umap-learn.readthedocs.io).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*均匀流形近似和投影* (UMAP) 是另一种流行的用于可视化的降维技术。虽然t-SNE在保留局部结构，尤其是簇方面表现更好，但UMAP试图保留局部和全局结构。此外，它更适合大规模数据集。遗憾的是，它不在Scikit-Learn中可用，但在
    [umap-learn包](https://umap-learn.readthedocs.io) 中有一个很好的实现。'
- en: '[Figure 7-11](#other_dim_reduction_plot) shows the results of MDS, Isomap,
    and t-SNE on the Swiss roll. MDS manages to flatten the Swiss roll without losing
    its global curvature, while Isomap drops it entirely. Depending on the downstream
    task, preserving the large-scale structure may be good or bad. t-SNE does a reasonable
    job of flattening the Swiss roll, preserving a bit of curvature, and it also amplifies
    clusters, tearing the roll apart. Again, this might be good or bad, depending
    on the downstream task.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-11](#other_dim_reduction_plot) 展示了MDS、Isomap和t-SNE在瑞士卷上的结果。MDS设法将瑞士卷展平，而没有失去其全局曲率，而Isomap则完全将其丢弃。根据下游任务，保留大规模结构可能是好是坏。t-SNE在将瑞士卷展平、保留一点曲率以及放大簇方面做得相当合理，它还撕裂了卷，再次，这可能是好是坏，取决于下游任务。'
- en: '![The diagram compares MDS, Isomap, and t-SNE techniques for reducing the Swiss
    roll dataset to two dimensions, illustrating different ways the global and local
    structures are preserved.](assets/hmls_0711.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![该图比较了MDS、Isomap和t-SNE技术在将瑞士卷数据集降至二维时的技术，展示了全局和局部结构的不同保留方式。](assets/hmls_0711.png)'
- en: Figure 7-11\. Using various techniques to reduce the Swiss roll to 2D
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. 使用各种技术将瑞士卷降至2D
- en: Exercises
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the main motivations for reducing a dataset’s dimensionality? What
    are the main drawbacks?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低数据集维度的主要动机是什么？主要缺点是什么？
- en: What is the curse of dimensionality?
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是维度诅咒？
- en: Once a dataset’s dimensionality has been reduced, is it possible to reverse
    the operation? If so, how? If not, why?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦降低了数据集的维度，是否可以逆转操作？如果是的话，如何？如果不是，为什么？
- en: Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA能否用于降低高度非线性数据集的维度？
- en: Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained
    variance ratio to 95%. How many dimensions will the resulting dataset have?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你对一个1,000维度的数据集进行PCA分析，将解释方差比设置为95%，那么结果数据集将有多少维度？
- en: In what cases would you use regular PCA, incremental PCA, randomized PCA, or
    random projection?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在什么情况下你会使用常规PCA、增量PCA、随机PCA或随机投影？
- en: How can you evaluate the performance of a dimensionality reduction algorithm
    on your dataset?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何评估降维算法在你数据集上的性能？
- en: Does it make any sense to chain two different dimensionality reduction algorithms?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个不同的降维算法串联起来是否有意义？
- en: Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and split it into a training set and a test set (take the first 60,000 instances
    for training, and the remaining 10,000 for testing). Train a random forest classifier
    on the dataset and time how long it takes, then evaluate the resulting model on
    the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained
    variance ratio of 95%. Train a new random forest classifier on the reduced dataset
    and see how long it takes. Was training much faster? Next, evaluate the classifier
    on the test set. How does it compare to the previous classifier? Try again with
    an `SGDClassifier`. How much does PCA help now?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集（在第3章中介绍），并将其分为训练集和测试集（取前60,000个实例用于训练，剩余的10,000个实例用于测试）。在数据集上训练一个随机森林分类器，并计时所需时间，然后评估测试集上的结果模型。接下来，使用PCA将数据集的维度降低，解释方差比为95%。在降低维度的数据集上训练一个新的随机森林分类器，并查看所需时间。训练是否更快？接下来，在测试集上评估分类器。它与之前的分类器相比如何？再次尝试使用`SGDClassifier`。PCA现在帮助了多少？
- en: Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to 2 dimensions
    and plot the result using Matplotlib. You can use a scatterplot using 10 different
    colors to represent each image’s target class. Alternatively, you can replace
    each dot in the scatterplot with the corresponding instance’s class (a digit from
    0 to 9), or even plot scaled-down versions of the digit images themselves (if
    you plot all digits the visualization will be too cluttered, so you should either
    draw a random sample or plot an instance only if no other instance has already
    been plotted at a close distance). You should get a nice visualization with well-separated
    clusters of digits. Try using other dimensionality reduction algorithms, such
    as PCA, LLE, or MDS, and compare the resulting visualizations.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用t-SNE将MNIST数据集的前5,000个图像降低到2维，并使用Matplotlib绘制结果。你可以使用10种不同的颜色来表示每个图像的目标类别。或者，你可以用相应的实例类别（一个从0到9的数字）替换散点图中的每个点，或者甚至绘制数字图像的缩小版（如果你绘制所有数字，可视化将过于杂乱，因此你应该绘制一个随机样本，或者只在没有其他实例在近距离被绘制的情况下绘制一个实例）。你应该得到一个具有数字良好分离簇的漂亮可视化。尝试使用其他降维算法，如PCA、LLE或MDS，并比较结果的可视化。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，请访问 [*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: ^([1](ch07.html#id1866-marker)) Well, four dimensions if you count time, and
    a few more if you are a string theorist.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#id1866-marker)) 嗯，如果你计算时间，是四个维度，如果你是弦论家，可能还有更多。
- en: ^([2](ch07.html#id1867-marker)) Watch a rotating tesseract projected into 3D
    space at [*https://homl.info/30*](https://homl.info/30). Image by Wikipedia user
    NerdBoy1392 ([Creative Commons BY-SA 3.0](https://oreil.ly/pMbrK)). Reproduced
    from [*https://en.wikipedia.org/wiki/Tesseract*](https://en.wikipedia.org/wiki/Tesseract).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#id1867-marker)) 观看将旋转的四面体投影到三维空间中的视频，请访问 [*https://homl.info/30*](https://homl.info/30)。图片由维基百科用户NerdBoy1392提供
    ([Creative Commons BY-SA 3.0](https://oreil.ly/pMbrK))。图片来源于 [*https://en.wikipedia.org/wiki/Tesseract*](https://en.wikipedia.org/wiki/Tesseract)。
- en: '^([3](ch07.html#id1868-marker)) Fun fact: anyone you know is probably an extremist
    in at least one dimension (e.g., how much sugar they put in their coffee), if
    you consider enough dimensions.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#id1868-marker)) 有趣的事实：如果你考虑足够的维度，你认识的人可能至少在一个维度上是极端主义者（例如，他们在咖啡中放多少糖）。
- en: '^([4](ch07.html#id1877-marker)) Karl Pearson, “On Lines and Planes of Closest
    Fit to Systems of Points in Space”, *The London, Edinburgh, and Dublin Philosophical
    Magazine and Journal of Science* 2, no. 11 (1901): 559–572.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.html#id1877-marker)) Karl Pearson，"关于空间中点集的最近拟合线和面"，*The London,
    Edinburgh, and Dublin Philosophical Magazine and Journal of Science* 2, no. 11
    (1901): 559–572。'
- en: ^([5](ch07.html#id1880-marker)) The proof that SVD happens to give us exactly
    the principal components we need for PCA requires some prerequisite math knowledge,
    such as eigenvectors and covariance matrices. If you are curious, you will find
    all the details in this [2014 paper by Jonathon Shlens](https://homl.info/pca2).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.html#id1880-marker)) SVD 确实能给我们提供 PCA 所需的恰好是主成分，这需要一些先验的数学知识，例如特征向量和协方差矩阵。如果你对此感兴趣，你可以在
    Jonathon Shlens 的这篇 [2014 年论文](https://homl.info/pca2) 中找到所有细节。
- en: '^([6](ch07.html#id1905-marker)) Scikit-Learn uses the [algorithm](https://homl.info/32)
    described in David A. Ross et al., “Incremental Learning for Robust Visual Tracking”,
    *International Journal of Computer Vision* 77, no. 1–3 (2008): 125–141.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch07.html#id1905-marker)) Scikit-Learn 使用了 David A. Ross 等人描述的 [算法](https://homl.info/32)，该算法发表在
    *International Journal of Computer Vision* 77, no. 1–3 (2008): 125–141。'
- en: ^([7](ch07.html#id1908-marker)) *ε* is the Greek letter epsilon, often used
    for tiny values.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#id1908-marker)) *ε* 是希腊字母 epsilon，常用于表示极小的值。
- en: '^([8](ch07.html#id1916-marker)) Sanjoy Dasgupta et al., “A neural algorithm
    for a fundamental computing problem”, *Science* 358, no. 6364 (2017): 793–796.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.html#id1916-marker)) Sanjoy Dasgupta 等人，"一个基本计算问题的神经网络算法"，*Science*
    358, no. 6364 (2017): 793–796。'
- en: '^([9](ch07.html#id1921-marker)) Sam T. Roweis and Lawrence K. Saul, “Nonlinear
    Dimensionality Reduction by Locally Linear Embedding”, *Science* 290, no. 5500
    (2000): 2323–2326.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch07.html#id1921-marker)) Sam T. Roweis 和 Lawrence K. Saul，"通过局部线性嵌入进行非线性降维"，*Science*
    290, no. 5500 (2000): 2323–2326。'
