- en: Chapter 7\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章\. 维度降低
- en: Many machine learning problems involve thousands or even millions of features
    for each training instance. Not only do all these features make training extremely
    slow, but they can also make it much harder to find a good solution, as you will
    see. This problem is often referred to as the *curse of dimensionality*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题涉及每个训练实例成千上万个特征。不仅所有这些特征使得训练变得极其缓慢，而且它们还可以使找到好的解决方案变得更加困难，正如你将看到的。这个问题通常被称为*维度诅咒*。
- en: 'Fortunately, in real-world problems, it is often possible to reduce the number
    of features considerably, turning an intractable problem into a tractable one.
    For example, consider the MNIST images (introduced in [Chapter 3](ch03.html#classification_chapter)):
    the pixels on the image borders are almost always white, so you could completely
    drop these pixels from the training set without losing much information. As we
    saw in the previous chapter, [Figure 6-6](ch06.html#mnist_feature_importance_plot)
    confirms that these pixels are utterly unimportant for the classification task.
    Additionally, two neighboring pixels are often highly correlated: if you merge
    them into a single pixel (e.g., by taking the mean of the two pixel intensities),
    you will not lose much information, removing redundancy and sometimes even noise.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在现实世界的问题中，通常可以显著减少特征的数量，将一个不可解的问题转化为可解的问题。例如，考虑MNIST图像（在第3章中介绍）：图像边界的像素几乎总是白色，因此你可以完全从训练集中删除这些像素，而不会丢失太多信息。正如我们在上一章中看到的，[图6-6](ch06.html#mnist_feature_importance_plot)证实了这些像素对于分类任务来说完全不重要。此外，相邻的两个像素通常高度相关：如果你将它们合并成一个像素（例如，通过取两个像素强度的平均值），你不会丢失太多信息，从而去除冗余，有时甚至去除噪声。
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Reducing dimensionality can also drop some useful information, just like compressing
    an image to JPEG can degrade its quality: it can make your system perform slightly
    worse, especially if you reduce dimensionality too much. Moreover, some models—such
    as neural networks—can handle high-dimensional data efficiently and learn to reduce
    its dimensionality while preserving the useful information for the task at hand.
    In short, adding an extra preprocessing step for dimensionality reduction will
    not always help.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 降低维度也可能丢失一些有用的信息，就像将图像压缩成JPEG会降低其质量一样：它可能会使你的系统性能略有下降，尤其是如果你过度降低维度。此外，一些模型——如神经网络——可以有效地处理高维数据，并学习在保留任务所需的有用信息的同时降低其维度。简而言之，添加一个额外的预处理步骤进行维度降低并不总是有帮助。
- en: Apart from speeding up training and possibly improving your model’s performance,
    dimensionality reduction is also extremely useful for data visualization. Reducing
    the number of dimensions down to two (or three) makes it possible to plot a condensed
    view of a high-dimensional training set on a graph and often gain some important
    insights by visually detecting patterns, such as clusters. Moreover, data visualization
    is essential to communicate your conclusions to people who are not data scientists—in
    particular, decision makers who will use your results.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 除了加快训练速度并可能提高你的模型性能之外，维度降低对于数据可视化也非常有用。将维度降低到两个（或三个）使得在图上绘制高维训练集的浓缩视图成为可能，并且通过视觉检测模式，如簇，通常可以获得一些重要的见解。此外，数据可视化对于向非数据科学家的人传达你的结论至关重要——特别是那些将使用你的结果做决策的人。
- en: 'In this chapter we will first discuss the curse of dimensionality and get a
    sense of what goes on in high-dimensional space. Then we will consider the two
    main approaches to dimensionality reduction (projection and manifold learning),
    and we will go through three of the most popular dimensionality reduction techniques:
    PCA, random projection, and locally linear embedding (LLE).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论维度诅咒，并了解高维空间中发生的情况。然后我们将考虑维度降低的两种主要方法（投影和流形学习），并介绍三种最流行的维度降低技术：主成分分析（PCA）、随机投影和局部线性嵌入（LLE）。
- en: The Curse of Dimensionality
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: We are so used to living in three dimensions⁠^([1](ch07.html#id1866)) that our
    intuition fails us when we try to imagine a high-dimensional space. Even a basic
    4D hypercube is incredibly hard to picture in our minds (see [Figure 7-1](#hypercube_wikipedia)),
    let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经习惯了生活在三维空间中⁠^([1](ch07.html#id1866))，以至于当我们试图想象高维空间时，直觉就会失效。即使是一个基本的4维超立方体在我们心中也很难想象（参见[图7-1](#hypercube_wikipedia)），更不用说在一个1000维空间中弯曲的200维椭球体了。
- en: '![Diagram illustrating the progression from a point to a tesseract, demonstrating
    the concept of hypercubes from 0D to 4D.](assets/hmls_0701.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![展示从点到四维体的演变，说明从0D到4D的超立方体概念的图](assets/hmls_0701.png)'
- en: Figure 7-1\. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)⁠^([2](ch07.html#id1867))
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 点、线段、正方形、立方体和四维体（从0D到4D的超立方体）⁠^([2](ch07.html#id1867))
- en: It turns out that many things behave very differently in high-dimensional space.
    For example, if you pick a random point in a unit square (a 1 × 1 square), it
    will have only about a 0.4% chance of being located less than 0.001 from a border
    (in other words, it is very unlikely that a random point will be “extreme” along
    any dimension). But in a 10,000-dimensional unit hypercube, this probability is
    greater than 99.999999%. Most points in a high-dimensional hypercube are very
    close to the border.⁠^([3](ch07.html#id1868))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，许多事物在高维空间中的表现非常不同。例如，如果你在单位正方形（1 × 1的正方形）中随机选择一个点，它位于距离边界小于0.001的概率只有大约0.4%（换句话说，随机点在任何维度上“极端”的可能性非常小）。但在一个10,000维的单位超立方体中，这个概率超过99.999999%。一个高维超立方体中的大多数点都非常接近边界。⁠^([3](ch07.html#id1868))
- en: 'Here is a more troublesome difference: if you pick two points randomly in a
    unit square, the distance between these two points will be, on average, roughly
    0.52\. If you pick two random points in a 3D unit cube, the average distance will
    be roughly 0.66\. But what about two points picked randomly in a 1,000,000-dimensional
    unit hypercube? The average distance, believe it or not, will be about 408.25
    (roughly $StartRoot StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$
    )! This is counterintuitive: how can two points be so far apart when they both
    lie within the same unit hypercube? Well, there’s just plenty of space in high
    dimensions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个更麻烦的差异：如果你在单位正方形中随机选择两个点，这两个点之间的距离平均大约是0.52。如果你在3D单位立方体中随机选择两个点，平均距离大约是0.66。那么，在1,000,000维单位超立方体中随机选择的两个点呢？平均距离，信不信由你，大约是408.25（大约是$StartRoot
    StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$）！这是反直觉的：当两个点都位于同一个单位超立方体中时，它们怎么可能相距如此之远？好吧，高维度中空间很大。
- en: 'As a result, high-dimensional datasets are often very sparse: most training
    instances are likely to be far away from each other, so training methods based
    on distance or similarity (such as *k*-nearest neighbors) will be much less effective.
    And some types of models will not be usable at all because they scale poorly with
    the dataset’s dimensionality (e.g., SVMs or dense neural networks). And new instances
    will likely be far away from any training instance, making predictions much less
    reliable than in lower dimensions since they will be based on much larger extrapolations.
    Since patterns in the data will become harder to identify, models will tend to
    fit the noise more frequently than in lower dimensions; regularization will become
    all the more important. Lastly, models will become even harder to interpret.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，高维数据集通常非常稀疏：大多数训练实例可能彼此相距很远，因此基于距离或相似性的训练方法（如*k*-最近邻）将不太有效。而且，某些类型的模型可能根本无法使用，因为它们与数据集的维度扩展不佳（例如，SVMs或密集神经网络）。新实例可能远离任何训练实例，这使得预测在低维度的可靠性大大降低，因为它们将基于更大的外推。由于数据中的模式将变得更加难以识别，模型将倾向于比低维度更频繁地拟合噪声；正则化将变得更加重要。最后，模型将变得更加难以解释。
- en: In theory, some of these issues can be resolved by increasing the size of the
    training set to reach a sufficient density of training instances. Unfortunately,
    in practice, the number of training instances required to reach a given density
    grows exponentially with the number of dimensions. With just 100 features—significantly
    fewer than in the MNIST problem—all ranging from 0 to 1, you would need more training
    instances than atoms in the observable universe in order for training instances
    to be within 0.1 of each other on average, assuming they were spread out uniformly
    across all dimensions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，通过增加训练集的大小以达到足够的训练实例密度，可以解决这些问题。不幸的是，在实践中，达到给定密度所需的训练实例数量会随着维数的增加而呈指数增长。在只有100个特征的情况下——这比MNIST问题中的特征少得多——所有特征的范围都在0到1之间，你需要比可观测宇宙中的原子还多的训练实例，才能使训练实例平均距离在0.1以内，假设它们在所有维度上均匀分布。
- en: Main Approaches for Dimensionality Reduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度约简的主要方法
- en: 'Before diving into specific dimensionality reduction algorithms, let’s look
    at the two main approaches to reducing dimensionality: projection and manifold
    learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究特定的降维算法之前，让我们看看降低维度的两种主要方法：投影和流形学习。
- en: Projection
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影
- en: In most real-world problems, training instances are *not* spread out uniformly
    across all dimensions. Many features are almost constant, while others are highly
    correlated (as discussed earlier for MNIST). As a result, all training instances
    lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional
    space. This sounds abstract, so let’s look at an example. In [Figure 7-2](#dataset_3d_plot),
    a 3D dataset is represented by small spheres (I will refer to this figure several
    times in the following sections).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际问题中，训练实例并不是均匀地分布在所有维度上。许多特征几乎恒定不变，而其他特征高度相关（如前面讨论的MNIST）。因此，所有训练实例都位于（或接近）高维空间中一个低得多的*子空间*内。这听起来很抽象，所以让我们看一个例子。在[图7-2](#dataset_3d_plot)中，一个3D数据集由小圆球表示（我将在接下来的几节中多次提到这个图）。
- en: 'Notice that all training instances lie close to a plane: this is a lower-dimensional
    (2D) subspace of the higher-dimensional (3D) space. If we project every training
    instance perpendicularly onto this subspace (as represented by the short dashed
    lines connecting the instances to the plane), we get the new 2D dataset shown
    in [Figure 7-3](#dataset_2d_plot). Ta-da! We have just reduced the dataset’s dimensionality
    from 3D to 2D. Note that the axes correspond to new features *z*[1] and *z*[2]:
    they are the coordinates of the projections on the plane.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到所有训练实例都靠近一个平面：这是高维（3D）空间中的低维（2D）子空间。如果我们将每个训练实例垂直投影到这个子空间上（如通过连接实例和平面的短虚线所示），我们就会得到[图7-3](#dataset_2d_plot)中显示的新2D数据集。哇！我们刚刚将数据集的维度从3D降低到2D。注意，轴对应于新的特征*z*[1]和*z*[2]：它们是平面上的投影坐标。
- en: '![A 3D scatter plot showing data points clustered near a 2D plane, illustrating
    a lower-dimensional subspace in higher-dimensional space.](assets/hmls_0702.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![一个3D散点图，显示数据点聚集在2D平面上附近，说明了高维空间中的低维子空间。](assets/hmls_0702.png)'
- en: Figure 7-2\. A 3D dataset lying close to a 2D subspace
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 一个靠近2D子空间的3D数据集
- en: '![Scatter plot showing the 2D dataset with axes labeled as new features z1
    and z2, illustrating dimensionality reduction from 3D to 2D.](assets/hmls_0703.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示2D数据集，轴标注为新的特征z1和z2，说明了从3D到2D的降维。](assets/hmls_0703.png)'
- en: Figure 7-3\. The new 2D dataset after projection
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 投影后的新2D数据集
- en: Manifold Learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习
- en: 'Although projection is fast and often works well, it’s not always the best
    approach to dimensionality reduction. In many cases the subspace may twist and
    turn, such as in the Swiss roll dataset represented in [Figure 7-4](#swiss_roll_plot):
    this is a toy dataset containing 3D points in the shape of a Swiss roll.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然投影速度快且通常效果良好，但并非总是降维的最佳方法。在许多情况下，子空间可能会扭曲和旋转，例如在[图7-4](#swiss_roll_plot)中展示的瑞士卷数据集中：这是一个包含瑞士卷形状3D点的玩具数据集。
- en: '![3D scatter plot of the Swiss roll dataset, illustrating points arranged in
    a spiral shape, used to demonstrate challenges in dimensionality reduction.](assets/hmls_0704.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![瑞士卷数据集的3D散点图，显示点以螺旋形状排列，用于说明降维中的挑战。](assets/hmls_0704.png)'
- en: Figure 7-4\. Swiss roll dataset
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 瑞士卷数据集
- en: Simply projecting onto a plane (e.g., by dropping *x*[3]) would squash different
    layers of the Swiss roll together, as shown on the left side of [Figure 7-5](#squished_swiss_roll_plot).
    What you probably want instead is to unroll the Swiss roll to obtain the 2D dataset
    on the righthand side of [Figure 7-5](#squished_swiss_roll_plot).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地将投影到平面上（例如，通过丢弃*x*[3]）会将瑞士卷的不同层挤压在一起，如[图7-5](#squished_swiss_roll_plot)左侧所示。你可能更希望将瑞士卷展开，以获得[图7-5](#squished_swiss_roll_plot)右侧的2D数据集。
- en: '![Diagram showing a squashed Swiss roll dataset on the left, where layers overlap,
    versus an unrolled version on the right, where the data is spread out in two dimensions.](assets/hmls_0705.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![显示左侧挤压的瑞士卷数据集，层重叠，与右侧展开版本对比，数据在两个维度上展开的示意图。](assets/hmls_0705.png)'
- en: Figure 7-5\. Squashing by projecting onto a plane (left) versus unrolling the
    Swiss roll (right)
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. 将投影到平面上（左侧）与展开瑞士卷（右侧）的挤压
- en: 'The Swiss roll is an example of a 2D *manifold*. Put simply, a 2D manifold
    is a 2D shape that can be bent and twisted in a higher-dimensional space. More
    generally, a *d*-dimensional manifold is a part of an *n*-dimensional space (where
    *d* < *n*) that locally resembles a *d*-dimensional hyperplane. In the case of
    the Swiss roll, *d* = 2 and *n* = 3: it locally resembles a 2D plane, but it is
    rolled in the third dimension.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 瑞士卷是2D *流形*的一个例子。简单来说，一个2D流形是在更高维空间中可以弯曲和扭曲的2D形状。更普遍地说，*d*-维流形是*n*-维空间（其中*d*
    < *n*）的一部分，它在局部类似于*d*-维超平面。在瑞士卷的情况下，*d* = 2，*n* = 3：它在局部类似于2D平面，但在第三维上被卷曲。
- en: Many dimensionality reduction algorithms (e.g., LLE, Isomap, t-SNE, or UMAP),
    work by modeling the manifold on which the training instances lie; this is called
    *manifold learning*. It relies on the *manifold assumption*, also called the *manifold
    hypothesis*, which holds that most real-world high-dimensional datasets lie close
    to a much lower-dimensional manifold. This assumption is very often empirically
    observed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多降维算法（例如，LLE、Isomap、t-SNE或UMAP）通过在训练实例所在的流形上建模来工作；这被称为*流形学习*。它依赖于*流形假设*，也称为*流形假设*，该假设认为大多数现实世界的高维数据集都靠近一个低维流形。这个假设通常在经验观察中是成立的。
- en: 'Once again, think about the MNIST dataset: all handwritten digit images have
    some similarities. They are made of connected lines, the borders are white, and
    they are more or less centered. If you randomly generated images, only a ridiculously
    tiny fraction of them would look like handwritten digits. In other words, the
    degrees of freedom available to you if you try to create a digit image are dramatically
    lower than the degrees of freedom you have if you are allowed to generate any
    image you want. These constraints tend to squeeze the dataset into a lower-dimensional
    manifold.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次思考MNIST数据集：所有手写数字图像都有一些相似之处。它们由连接的线条组成，边缘是白色的，并且它们或多或少是居中的。如果你随机生成图像，只有极小的一部分看起来像手写数字。换句话说，如果你尝试创建数字图像，可用的自由度比你被允许生成任何想要的图像时的自由度要低得多。这些约束往往会将数据集挤压到一个低维流形中。
- en: 'The manifold assumption is often accompanied by another implicit assumption:
    that the task at hand (e.g., classification or regression) will be simpler if
    expressed in the lower-dimensional space of the manifold. For example, in the
    top row of [Figure 7-6](#manifold_decision_boundary_plot) the Swiss roll is split
    into two classes: in the 3D space (on the left) the decision boundary would be
    fairly complex, but in the 2D unrolled manifold space (on the right) the decision
    boundary is a straight line.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设通常伴随着另一个隐含的假设：如果用流形的低维空间表达，手头的任务（例如，分类或回归）将变得更加简单。例如，在[图7-6](#manifold_decision_boundary_plot)的顶部行中，瑞士卷被分为两类：在3D空间（左侧）中，决策边界将相当复杂，但在2D展开流形空间（右侧）中，决策边界是一条直线。
- en: However, this implicit assumption does not always hold. For example, in the
    bottom row of [Figure 7-6](#manifold_decision_boundary_plot), the decision boundary
    is located at *x*[1] = 5\. This decision boundary looks very simple in the original
    3D space (a vertical plane), but it looks more complex in the unrolled manifold
    (a collection of four independent line segments).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个隐含的假设并不总是成立。例如，在[图7-6](#manifold_decision_boundary_plot)的底部行中，决策边界位于 *x*[1]
    = 5。这个决策边界在原始的3D空间（一个垂直平面）中看起来非常简单，但在展开的流形（由四个独立的线段组成）中看起来更复杂。
- en: In short, reducing the dimensionality of your training set before training a
    model will usually speed up training, but it may not always lead to a better or
    simpler solution; it all depends on the dataset. Dimensionality reduction is typically
    more effective when the dataset is small relative to the number of features, especially
    if it’s noisy, or many features are highly correlated to one another (i.e., redundant).
    And if you have domain knowledge about the process that generated the data, and
    you know it’s simple, then the manifold assumption certainly holds, and dimensionality
    reduction is likely to help.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在训练模型之前降低训练集的维度通常会加快训练速度，但它并不总是导致更好的或更简单的解决方案；这完全取决于数据集。当数据集相对于特征数量较小时，降维通常更有效，特别是如果它是嘈杂的，或者许多特征彼此高度相关（即冗余）。如果你对生成数据的流程有领域知识，并且你知道它是简单的，那么流形假设肯定成立，降维很可能会有所帮助。
- en: Hopefully, you now have a good sense of what the curse of dimensionality is
    and how dimensionality reduction algorithms can fight it, especially when the
    manifold assumption holds. The rest of this chapter will go through some of the
    most popular algorithms for dimensionality reduction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对维度灾难的概念以及降维算法如何对抗它有了很好的理解，尤其是在流形假设成立的情况下。本章的其余部分将介绍一些最流行的降维算法。
- en: '![Diagrams illustrating how dimensionality reduction affects decision boundaries,
    showing a complex spiral structure on the left and its simplified lower-dimensional
    projections on the right.](assets/hmls_0706.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图表说明降维如何影响决策边界，左侧显示一个复杂的螺旋结构，右侧显示其简化的低维投影。](assets/hmls_0706.png)'
- en: Figure 7-6\. The decision boundary may not always be simpler with lower dimensions
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 决策边界在低维情况下不一定更简单
- en: PCA
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA
- en: '*Principal component analysis* (PCA) is by far the most popular dimensionality
    reduction algorithm. First it identifies the hyperplane that lies closest to the
    data, and then it projects the data onto it, as shown back in [Figure 7-2](#dataset_3d_plot).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析*（PCA）是迄今为止最流行的降维算法。首先，它确定离数据最近的超平面，然后将数据投影到该平面上，如图 7-2 所示。'
- en: Preserving the Variance
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留方差
- en: Before you can project the training set onto a lower-dimensional hyperplane,
    you first need to choose the right hyperplane. For example, a simple 2D dataset
    is represented on the left in [Figure 7-7](#pca_best_projection_plot), along with
    three different axes (i.e., 1D hyperplanes). On the right is the result of the
    projection of the dataset onto each of these axes. As you can see, the projection
    onto the solid line preserves the maximum variance (top), while the projection
    onto the dotted line preserves very little variance (bottom), and the projection
    onto the dashed line preserves an intermediate amount of variance (middle).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够将训练集投影到低维超平面之前，你首先需要选择正确的超平面。例如，一个简单的二维数据集在 [图 7-7](#pca_best_projection_plot)
    的左侧表示，包括三个不同的轴（即 1D 超平面）。右侧是数据集在每个这些轴上的投影结果。正如你所看到的，实线上的投影保留了最大的方差（顶部），而点线上的投影保留了非常少的方差（底部），虚线上的投影保留了中等数量的方差（中间）。
- en: '![Diagram of PCA showing a 2D dataset on the left projected onto three different
    1D axes. The solid line preserves the most variance, the dashed line an intermediate
    amount, and the dotted line the least, as depicted by the spread of points on
    the right.](assets/hmls_0707.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 图表显示左侧的二维数据集投影到三个不同的 1D 轴上。实线保留了最多的方差，虚线保留了中等数量的方差，点线保留了最少的方差，如右侧点云的分布所示。](assets/hmls_0707.png)'
- en: Figure 7-7\. Selecting the subspace on which to project
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 选择投影的子空间
- en: 'It seems reasonable to select the axis that preserves the maximum amount of
    variance, as it will most likely lose less information than the other projections.
    Consider your shadow on the ground when the sun is directly overhead: it’s a small
    blob that doesn’t look anything like you. But your shadow on a wall at sunrise
    is much larger and it *does* look like you. Another way to justify choosing the
    axis that maximizes the variance is that it is also the axis that minimizes the
    mean squared distance between the original dataset and its projection onto that
    axis. This is the rather simple idea behind PCA, introduced way back [in 1901](https://homl.info/pca)!⁠^([4](ch07.html#id1877))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择保留最大方差量的轴似乎是合理的，因为它与其他投影相比可能丢失的信息更少。考虑当太阳正午直射时你在地面的影子：它是一个小而模糊的形状，看起来根本不像你。但你在日出时墙上的影子要大得多，*确实*像你。选择最大化方差轴的另一种理由是，它也是最小化原始数据集与其在该轴上投影之间的平均平方距离的轴。这是主成分分析（PCA）背后的简单想法，PCA
    早在 [1901 年](https://homl.info/pca)!⁠^([4](ch07.html#id1877)) 就被提出了！
- en: Principal Components
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分
- en: 'PCA identifies the axis that accounts for the largest amount of variance in
    the training set. In [Figure 7-7](#pca_best_projection_plot), it is the solid
    line. It also finds a second axis, orthogonal to the first one, that accounts
    for the largest amount of the remaining variance. In this 2D example there is
    no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA
    would also find a third axis, orthogonal to both previous axes, and a fourth,
    a fifth, and so on—as many axes as the number of dimensions in the dataset.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PCA识别出在训练集中解释最大方差量的轴。在[图7-7](#pca_best_projection_plot)中，它是实线。它还找到一个与第一个轴正交的第二个轴，解释了剩余的最大方差量。在这个2D示例中，没有选择：它是虚线。如果是一个更高维度的数据集，PCA还会找到一个与前面两个轴都正交的第三个轴，第四个轴，第五个轴，等等——与数据集维度数量一样多的轴。
- en: The *i*^(th) axis is called the *i*^(th) *principal component* (PC) of the data.
    In [Figure 7-7](#pca_best_projection_plot), the first PC is the axis on which
    vector **c**[**1**] lies, and the second PC is the axis on which vector **c**[**2**]
    lies. In [Figure 7-2](#dataset_3d_plot), the first two PCs are on the projection
    plane, and the third PC is the axis orthogonal to that plane. After the projection,
    back in [Figure 7-3](#dataset_2d_plot), the first PC corresponds to the *z*[1]
    axis, and the second PC corresponds to the *z*[2] axis.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第*i*轴被称为数据的第*i*个**主成分**（PC）。在[图7-7](#pca_best_projection_plot)中，第一个PC是向量**c**[**1**]所在的轴，第二个PC是向量**c**[**2**]所在的轴。在[图7-2](#dataset_3d_plot)中，前两个PC位于投影平面上，第三个PC是垂直于该平面的轴。在投影后，回到[图7-3](#dataset_2d_plot)，第一个PC对应于*z*[1]轴，第二个PC对应于*z*[2]轴。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For each principal component, PCA finds a zero-centered unit vector pointing
    along the direction of the PC. Unfortunately, its direction is not guaranteed:
    if you perturb the training set slightly and run PCA again, the unit vector may
    point in the opposite direction. In fact, a pair of unit vectors may even rotate
    or swap if the variances along these two axes are very close. So if you use PCA
    as a preprocessing step before a model, make sure you always retrain the model
    entirely every time you update the PCA transformer: if you don’t and if the PCA’s
    output doesn’t align with the previous version, the model will be very confused.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主成分，PCA找到一个以零为中心的单位向量，指向PC的方向。不幸的是，它的方向没有保证：如果你稍微扰动训练集并再次运行PCA，单位向量可能会指向相反的方向。事实上，如果这两个轴上的方差非常接近，一对单位向量甚至可能会旋转或交换。所以如果你在模型之前使用PCA作为预处理步骤，确保每次更新PCA转换器时都完全重新训练模型：如果你不这样做，并且PCA的输出与之前版本不匹配，模型会非常困惑。
- en: So how can you find the principal components of a training set? Luckily, there
    is a standard matrix factorization technique called *singular value decomposition*
    (SVD) that can decompose the training set matrix **X** into the product of three
    matrices **U** **Σ** **V**^⊺, where **V** contains the unit vectors that define
    all the principal components that you are looking for, in the correct order, as
    shown in [Equation 7-1](#principal_components_matrix).⁠^([5](ch07.html#id1880))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你如何找到训练集的主成分呢？幸运的是，有一个标准的矩阵分解技术叫做**奇异值分解**（SVD），可以将训练集矩阵**X**分解为三个矩阵**U**
    **Σ** **V**^⊺的乘积，其中**V**包含了定义你正在寻找的所有主成分的单位向量，按照正确的顺序排列，如[方程7-1](#principal_components_matrix)所示。⁠^([5](ch07.html#id1880))
- en: Equation 7-1\. Principal components matrix
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-1\. 主成分矩阵
- en: $bold upper V equals Start 3 By 4 Matrix 1st Row 1st Column bar 2nd Column bar
    3rd Column Blank 4th Column bar 2nd Row 1st Column bold c 1 2nd Column bold c
    2 3rd Column midline-horizontal-ellipsis 4th Column bold c Subscript n Baseline
    3rd Row 1st Column bar 2nd Column bar 3rd Column Blank 4th Column bar EndMatrix$
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体上标V等于3x4矩阵，第一行第一列是2，第二列是2，第三列是空，第四列是2，第二行第一列是粗体c，第二列是粗体c，第三列是中横线省略号，第四列是粗体c下标n基线，第三行第一列是2，第二列是2，第三列是空，第四列是2**。'
- en: 'The following Python code uses NumPy’s `svd()` function to obtain all the principal
    components of the 3D training set represented back in [Figure 7-2](#dataset_3d_plot),
    then it extracts the two unit vectors that define the first two PCs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码使用NumPy的`svd()`函数获取[图7-2](#dataset_3d_plot)中代表的所有3D训练集的主成分，然后它提取定义前两个PC的两个单位向量：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: PCA assumes that the dataset is centered around the origin. As you will see,
    Scikit-Learn’s PCA classes take care of centering the data for you. If you implement
    PCA yourself (as in the preceding example), or if you use other libraries, don’t
    forget to center the data first.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 假设数据集围绕原点中心化。正如你将看到的，Scikit-Learn 的 PCA 类会为你处理数据的中心化。如果你自己实现 PCA（如前例所示），或者如果你使用其他库，别忘了首先中心化数据。
- en: Projecting Down to d Dimensions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影到 d 维
- en: Once you have identified all the principal components, you can reduce the dimensionality
    of the dataset down to *d* dimensions by projecting it onto the hyperplane defined
    by the first *d* principal components (we will discuss how to choose the number
    of dimensions *d* shortly). Selecting this hyperplane ensures that the projection
    will preserve as much variance as possible. For example, in [Figure 7-2](#dataset_3d_plot)
    the 3D dataset is projected down to the 2D plane defined by the first two principal
    components, preserving a large part of the dataset’s variance. As a result, the
    2D projection looks very much like the original 3D dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了所有主成分，就可以通过将其投影到由前 *d* 个主成分定义的超平面上来将数据集的维度降低到 *d* 维（我们将在稍后讨论如何选择维度数 *d*）。选择这个超平面确保投影将尽可能保留方差。例如，在
    [图 7-2](#dataset_3d_plot) 中，3D 数据集被投影到由前两个主成分定义的 2D 平面上，保留了数据集的大部分方差。因此，2D 投影看起来非常像原始的
    3D 数据集。
- en: To project the training set onto the hyperplane and obtain a reduced dataset
    **X**[*d*-proj] of dimensionality *d*, compute the matrix multiplication of the
    training set matrix **X** by the matrix **W**[*d*], defined as the matrix containing
    the first *d* columns of **V**, as shown in [Equation 7-2](#pca_projection).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要将训练集投影到超平面并获取一个维度为 *d* 的降低数据集 **X**[*d*-proj]，计算训练集矩阵 **X** 与矩阵 **W**[*d*]
    的矩阵乘法，其中 **W**[*d*] 是包含 **V** 的前 *d* 列的矩阵，如 [方程式 7-2](#pca_projection) 所示。
- en: Equation 7-2\. Projecting the training set down to *d* dimensions
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 7-2\. 将训练集投影到 *d* 维
- en: $bold upper X Subscript d hyphen proj Baseline equals bold upper X bold upper
    W Subscript d$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $ \mathbf{X}^{d-\text{proj}}_{\text{Baseline}} = \mathbf{X} \mathbf{W}^{d} $
- en: 'The following Python code projects the training set onto the plane defined
    by the first two principal components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码将训练集投影到由前两个主成分定义的平面上：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There you have it! You now know how to reduce the dimensionality of any dataset
    by projecting it down to any number of dimensions, while preserving as much variance
    as possible.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在知道如何通过将数据集投影到任意数量的维度来降低其维度，同时尽可能保留方差。
- en: Using Scikit-Learn
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn
- en: 'Scikit-Learn’s `PCA` class uses SVD to implement PCA, just like we did earlier
    in this chapter. The following code applies PCA to reduce the dimensionality of
    the dataset down to two dimensions (note that it automatically takes care of centering
    the data):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 的 `PCA` 类使用 SVD 来实现 PCA，就像我们在本章前面所做的那样。以下代码将 PCA 应用到数据集上，将其维度降低到二维（请注意，它自动处理了数据的中心化）：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After fitting the `PCA` transformer to the dataset, its `components_` attribute
    holds the transpose of **W**[*d*]: it contains one row for each of the first *d*
    principal components.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 `PCA` 转换器拟合到数据集后，其 `components_` 属性包含 **W**[*d*] 的转置：它包含每个前 *d* 个主成分的一行。
- en: Explained Variance Ratio
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释方差比
- en: 'Another useful piece of information is the *explained variance ratio* of each
    principal component, available via the `explained_variance_ratio_` variable. The
    ratio indicates the proportion of the dataset’s variance that lies along each
    principal component. For example, let’s look at the explained variance ratios
    of the first two components of the 3D dataset represented in [Figure 7-2](#dataset_3d_plot):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的信息是每个主成分的解释方差比，通过 `explained_variance_ratio_` 变量提供。该比率表示数据集方差中沿每个主成分的比例。例如，让我们看看
    [图 7-2](#dataset_3d_plot) 中表示的 3D 数据集的前两个成分的解释方差比：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`This output tells us that about 82% of the dataset’s variance lies along the
    first PC, and about 11% lies along the second PC. This leaves about 7% for the
    third PC, so it is reasonable to assume that the third PC probably carries little
    information.`  [PRE4] ## Choosing the Right Number of Dimensions    Instead of
    arbitrarily choosing the number of dimensions to reduce down to, it is simpler
    to choose the number of dimensions that add up to a sufficiently large portion
    of the variance—say, 95%. (An exception to this rule, of course, is if you are
    reducing dimensionality for data visualization, in which case you will want to
    reduce the dimensionality down to 2 or 3.)    The following code loads and splits
    the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and performs PCA without reducing dimensionality, then computes the minimum number
    of dimensions required to preserve 95% of the training set’s variance:    [PRE5]py    You
    could then set `n_components=d` and run PCA again, but there’s a better option.
    Instead of specifying the number of principal components you want to preserve,
    you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio
    of variance you wish to preserve:    [PRE6]py    The actual number of components
    is determined during training, and it is stored in the `n_components_` attribute:    [PRE7]py   [PRE8]`
    [PRE9] X_recovered = pca.inverse_transform(X_reduced) [PRE10] rnd_pca = PCA(n_components=154,
    svd_solver="randomized", random_state=42) X_reduced = rnd_pca.fit_transform(X_train)
    [PRE11] from sklearn.decomposition import IncrementalPCA  n_batches = 100 inc_pca
    = IncrementalPCA(n_components=154) for X_batch in np.array_split(X_train, n_batches):     inc_pca.partial_fit(X_batch)  X_reduced
    = inc_pca.transform(X_train) [PRE12] filename = "my_mnist.mmap" X_mmap = np.memmap(filename,
    dtype=''float32'', mode=''write'', shape=X_train.shape) X_mmap[:] = X_train  #
    could be a loop instead, saving the data chunk by chunk X_mmap.flush() [PRE13]
    X_mmap = np.memmap(filename, dtype="float32", mode="readonly").reshape(-1, 784)
    batch_size = X_mmap.shape[0] // n_batches inc_pca = IncrementalPCA(n_components=154,
    batch_size=batch_size) inc_pca.fit(X_mmap) [PRE14]` [PRE15][PRE16][PRE17] [PRE18]py`
    # Random Projection    As its name suggests, the random projection algorithm projects
    the data to a lower-dimensional space using a random linear projection. This may
    sound crazy, but it turns out that such a random projection is actually very likely
    to preserve distances fairly well, as was demonstrated mathematically by William
    B. Johnson and Joram Lindenstrauss in a famous lemma. So, two similar instances
    will remain similar after the projection, and two very different instances will
    remain very different.    Obviously, the more dimensions you drop, the more information
    is lost, and the more distances get distorted. So how can you choose the optimal
    number of dimensions? Well, Johnson and Lindenstrauss came up with an equation
    that determines the minimum number of dimensions to preserve in order to ensure—with
    high probability—that distances won’t change by more than a given tolerance. For
    example, if you have a dataset containing *m* = 5,000 instances with *n* = 20,000
    features each, and you don’t want the squared distance between any two instances
    to change by more than *ε* = 10%,^([7](ch07.html#id1908)) then you should project
    the data down to *d* dimensions, with *d* ≥ 4 log(*m*) / (½ *ε*² - ⅓ *ε*³), which
    is 7,300 dimensions. That’s quite a significant dimensionality reduction! Notice
    that the equation does not use *n*, it only relies on *m* and *ε*. This equation
    is implemented by the `johnson_lindenstrauss_min_dim()` function:    [PRE19]py`
    `>>>` `d` `=` `johnson_lindenstrauss_min_dim``(``m``,` `eps``=``ε``)` [PRE20]py
    [PRE21]``py [PRE22]`py  [PRE23]py [PRE24]py`` [PRE25]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
