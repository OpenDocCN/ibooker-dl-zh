- en: Chapter 11\. Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。迁移学习
- en: “Learn from the mistakes of others. You can’t live long enough to make them
    all yourself.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “向他人的错误学习。你活不到足够长的时间来犯所有的错误。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Eleanor Roosevelt
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —埃莉诺·罗斯福
- en: It can be challenging to have an extensive collection of data, battle-tested
    model structure, and processing power. Wouldn’t it be nice to cut a corner? That
    nifty trick in [Chapter 7](ch07.html#the_chapter_7) where you could use Teachable
    Machine to transfer the qualities of a trained model to a novel one was pretty
    useful. In fact, this is a common trick in the machine learning world. While Teachable
    Machine hid the specifics and offered you only a single model, you can understand
    the mechanics of this trick and use it on all kinds of cool tasks. In this chapter,
    we will reveal the magic behind this process. While we’ll be focused on the example
    of MobileNet for simplicity, this can be applied to all kinds of models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大量数据、经过实战检验的模型结构和处理能力可能是具有挑战性的。能不能简单点？在第7章中，您可以使用 Teachable Machine 将训练好的模型的特质转移到新模型中，这是非常有用的。事实上，这是机器学习世界中的一个常见技巧。虽然
    Teachable Machine 隐藏了具体细节，只提供了一个模型，但您可以理解这个技巧的原理，并将其应用于各种酷炫的任务。在本章中，我们将揭示这个过程背后的魔法。虽然我们将简单地以
    MobileNet 为例，但这可以应用于各种模型。
- en: Transfer learning is the act of taking a trained model and repurposing it for
    a second related task.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是指将训练好的模型重新用于第二个相关任务。
- en: 'There are a few repeatable benefits to using transfer learning for your machine
    learning solution. Most projects utilize some amount of transfer learning for
    these reasons:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习为您的机器学习解决方案带来一些可重复的好处。大多数项目出于以下原因利用一定程度的迁移学习：
- en: Reutilizing a battle-tested model structure
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复使用经过实战检验的模型结构
- en: Getting a solution faster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快地获得解决方案
- en: Getting a solution via less data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过较少的数据获得解决方案
- en: In this chapter, you’ll learn several strategies for transfer learning. You
    will focus on MobileNet as a fundamental example that can be reused to identify
    a myriad of new classes in various ways.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习几种迁移学习策略。您将专注于 MobileNet 作为一个基本示例，可以以各种方式重复使用来识别各种新类别。
- en: 'We will:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将：
- en: Review how transfer learning works
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾迁移学习的工作原理
- en: See how to reuse feature vectors
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看如何重复使用特征向量
- en: Cut into Layers models and reconstruct new models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型切割成层并重构新模型
- en: Learn about KNN and deferred classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 KNN 和延迟分类
- en: When you finish this chapter, you’ll be able to take models that have been trained
    for a long time with lots of data and apply them to your own needs with smaller
    datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够将长时间训练并具有大量数据的模型应用于您自己的需求，即使只有较小的数据集。
- en: How Does Transfer Learning Work?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习是如何工作的？
- en: How does a model that has been trained on different data suddenly work well
    for your *new* data? It sounds miraculous, but it happens in humans every day.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经过不同数据训练的模型如何突然对您的*新*数据起作用？听起来像奇迹，但这在人类中每天都发生。
- en: You’ve spent years identifying animals, and you’ve probably seen hundreds of
    camels, guinea pigs, and beavers from cartoons, zoos, and commercials. Now I’m
    going to show you an animal you’ve probably not seen often, or even at all. The
    animal in [Figure 11-1](#capybara) is called a capybara (*Hydrochoerus hydrochaeris*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您花了多年时间识别动物，可能看过数百只骆驼、天竺鼠和海狸的卡通、动物园和广告。现在我将向您展示一种您可能很少见到甚至从未见过的动物。[图11-1](#capybara)中的动物被称为水豚（*Hydrochoerus
    hydrochaeris*）。
- en: '![profile of a capybara](assets/ltjs_1101.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![水豚的侧面](assets/ltjs_1101.png)'
- en: Figure 11-1\. The capybara
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1。水豚
- en: For some of you, this is the first time (or one of the few times) you’ve seen
    a photo of a capybara. Now, take a look at the lineup in [Figure 11-2](#transfer_quiz).
    Can you find the capybara?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你们中的一些人，这可能是第一次（或者是少数几次）看到水豚的照片。现在，看看[图11-2](#transfer_quiz)中的阵容。你能找到水豚吗？
- en: '![three mammals quiz](assets/ltjs_1102.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![三种哺乳动物测验](assets/ltjs_1102.png)'
- en: Figure 11-2\. Which one is the capybara?
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。哪一个是水豚？
- en: The training set of a single photo was enough for you to make a choice because
    you’ve been distinguishing between animals your entire life. With a novel color,
    angle, and photo size, your brain probably detected with absolute certainty that
    animal C was another capybara. The features learned by your years of experience
    have helped you make an educated decision. In that same way, powerful models that
    have significant experience can be taught to learn new things from small amounts
    of new data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一张单独的照片的训练集足以让您做出选择，因为您一生中一直在区分动物。即使是新的颜色、角度和照片尺寸，您的大脑可能也能绝对确定地检测到动物 C 是另一只水豚。您多年的经验学习到的特征帮助您做出了明智的决定。同样地，具有丰富经验的强大模型可以被教导从少量新数据中学习新事物。
- en: Transfer Learning Neural Networks
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习神经网络
- en: Let’s bring things back to MobileNet for a moment. The MobileNet model was trained
    to identify features that distinguish a thousand items from each other. That means
    there are convolutions to detect fur, metal, round things, ears, and all kinds
    of crucial differential features. All these features are chewed up and simplified
    before they are flattened into a neural network, where the combination of various
    features creates a classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到 MobileNet。MobileNet 模型经过训练，可以识别区分一千种物品之间的特征。这意味着有卷积来检测毛发、金属、圆形物体、耳朵以及各种关键的差异特征。所有这些特征在被压缩和简化之前都被吸收到一个神经网络中，各种特征的组合形成了分类。
- en: The MobileNet model can identify different breeds of dogs, and even distinguish
    a Maltese terrier from a Tibetan terrier. If you were to make a [“dog or cat”](https://oreil.ly/i9Xxm)
    classifier, it makes sense that a majority of those advanced features would be
    reusable in your simpler model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet 模型可以识别不同品种的狗，甚至可以区分马耳他犬和西藏犬。如果您要制作一个[“狗还是猫”](https://oreil.ly/i9Xxm)分类器，那么在您更简单的模型中，大多数这些高级特征是可以重复使用的。
- en: The previously learned convolutional filters would be extremely useful in identifying
    key features for brand-new classifications, like our capybara example in [Figure 11-2](#transfer_quiz).
    The trick is to take the feature identification portion of the model and apply
    your own neural network to the convolutional output, as illustrated in [Figure 11-3](#transfer_visual).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 先前学习的卷积滤波器在识别全新分类的关键特征方面非常有用，就像我们在[图11-2](#transfer_quiz)中的水豚示例一样。关键是将模型的特征识别部分提取出来，并将自己的神经网络应用于卷积输出，如[图11-3](#transfer_visual)所示。
- en: '![changing the NN flowchart](assets/ltjs_1103.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![更改NN流程图](assets/ltjs_1103.png)'
- en: Figure 11-3\. CNN transfer learning
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3。CNN迁移学习
- en: So how do you separate and recombine these sections of previously trained models?
    You’ve got lots of options. Again, we’ll learn a bit more about Graph and Layers
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何分离和重新组合先前训练模型的这些部分呢？您有很多选择。再次，我们将学习更多关于图和层模型的知识。
- en: Easy MobileNet Transfer Learning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的MobileNet迁移学习
- en: Fortunately, [TensorFlow Hub](https://tfhub.dev) already has a MobileNet model
    that is disconnected from any neural network. It offers half a model for you to
    use for transfer learning. Half a model means it hasn’t been tied down into a
    final softmax layer for what it’s meant to classify. This allows us to let MobileNet
    derive the features of an image and then provide us with tensors that we can then
    pass to our own trained network for classification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，[TensorFlow Hub](https://tfhub.dev)已经有一个与任何神经网络断开连接的MobileNet模型。它为您提供了一半的模型用于迁移学习。一半的模型意味着它还没有被绑定到最终的softmax层来进行分类。这使我们可以让MobileNet推导出图像的特征，然后为我们提供张量，然后我们可以将这些张量传递给我们自己训练的网络进行分类。
- en: TFHub calls these models *image feature vector* models. [You refine your search
    to show only these models](https://oreil.ly/BkokR) or identify them by looking
    at the problem domain tags, as illustrated in [Figure 11-4](#image_feature_vector).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TFHub将这些模型称为*图像特征向量*模型。[您可以缩小搜索范围，只显示这些模型](https://oreil.ly/BkokR)，或者通过查看问题域标签来识别它们，如[图11-4](#image_feature_vector)所示。
- en: '![screenshot of proper tags](assets/ltjs_1104.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![正确标签的截图](assets/ltjs_1104.png)'
- en: Figure 11-4\. Problem domain tags for image feature vectors
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4。图像特征向量的问题域标签
- en: You might notice small variations of MobileNet and wonder what the differences
    are. Once you learn a few sneaky terms, each of these model descriptions becomes
    quite readable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到MobileNet的小变化，并想知道差异是什么。一旦您了解了一些诡计术语，每个模型描述都会变得非常可读。
- en: For instance, we’ll use [Example 11-1](#example_mobile_net).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将使用[示例11-1](#example_mobile_net)。
- en: Example 11-1\. One of the image feature vector models
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-1。图像特征向量模型之一
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: imagenet
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: imagenet
- en: This model was trained on the ImageNet dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是在ImageNet数据集上训练的。
- en: mobilenet_v2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: mobilenet_v2
- en: The model’s architecture is MobileNet v2.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的架构是MobileNet v2。
- en: '130'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '130'
- en: The model’s depth multiplier was 1.30\. This results in more features. If you
    want to speed things up, you can choose “05,” which would have less than half
    the feature output with a boost in speed. This is a fine-tuning option when you’re
    ready to modify speed versus depth.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的深度乘数为1.30。这会产生更多的特征。如果您想加快速度，可以选择“05”，这将减少一半以下的特征输出并提高速度。这是一个微调选项，当您准备好修改速度与深度时可以使用。
- en: '224'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '224'
- en: The model’s expected input size is 224 x 224 images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的预期输入尺寸为224 x 224像素的图像。
- en: feature_vector
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: feature_vector
- en: We already know from the tag, but this model outputs tensors meant to be features
    of the image for a second model to interpret.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从标签中了解到，但这个模型输出的张量是为了作为图像特征的第二个模型来解释。
- en: Now that we have a trained model that can identify features in an image, we
    will run our training data through the MobileNet image feature vector model and
    then train a model on the output from that. In other words, the training images
    will turn into a feature vector, and we’ll train a model to interpret that feature
    vector.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个经过训练的模型，可以识别图像中的特征，我们将通过MobileNet图像特征向量模型运行我们的训练数据，然后在输出上训练一个模型。换句话说，训练图像将变成一个特征向量，我们将训练一个模型来解释该特征向量。
- en: The benefit of this strategy is that it’s straightforward to implement. The
    major drawback is that you’ll have to load two models when you’re ready to use
    the newly trained model (one to generate features and one to interpret them).
    Creatively, there might be some cases where it’s quite useful to “featurize” an
    image and then run that through multiple neural networks. Regardless, let’s see
    it in action.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的好处是实现起来很简单。主要缺点是当您准备使用新训练的模型时，您将不得不加载两个模型（一个用于生成特征，一个用于解释）。创造性地，可能有一些情况下“特征化”图像然后通过多个神经网络运行可能非常有用。无论如何，让我们看看它的实际效果。
- en: TensorFlow Hub Check, Mate!
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Hub检查，对手！
- en: We’re going to use transfer learning with MobileNet to identify chess pieces
    like the one shown in [Figure 11-5](#chess_promo).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MobileNet进行迁移学习，以识别像[图11-5](#chess_promo)中所示的国际象棋棋子。
- en: '![image of a chess knight on a table](assets/ltjs_1105.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![桌子上的国际象棋骑士的图像](assets/ltjs_1105.png)'
- en: Figure 11-5\. Simple chess pieces classifier
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5。简单的国际象棋棋子分类器
- en: You’ll only have a few images of each chess piece. That’s not normally enough,
    but with the magic of transfer learning, you’ll get an efficient model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您只会有每个国际象棋棋子的几张图像。通常这是不够的，但通过迁移学习的魔力，您将得到一个高效的模型。
- en: Loading chess images
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载国际象棋图像
- en: For this exercise, I’ve compiled a collection of 150 images and loaded them
    into a CSV file for quick use. This isn’t something I’d recommend doing in most
    cases because it’s inefficient for processing and disk space, but it serves as
    a simple vector for some quick in-browser training. The code to load these images
    is now trivial.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个练习，我已经编译了一个包含150张图像的集合，并将它们加载到CSV文件中以便快速使用。在大多数情况下，我不建议这样做，因为这对于处理和磁盘空间是低效的，但它可以作为一种简单的向量，用于一些快速的浏览器训练。现在加载这些图像的代码是微不足道的。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can access the chess images and the code that converted them into a CSV
    file in the [*chapter11/extra/node-make-csvs*](https://oreil.ly/INWAN) folder.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问象棋图像和将它们转换为 CSV 文件的代码在[*chapter11/extra/node-make-csvs*](https://oreil.ly/INWAN)文件夹中。
- en: The files *chess_labels.csv* and *chess_images.csv* can be found in the [*chess_data.zip*](https://oreil.ly/bcFop)
    file in code associated with this lesson. Unzip this file and use Danfo.js to
    load the contents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 *chess_labels.csv* 和 *chess_images.csv* 可以在与本课程相关的[*chess_data.zip*](https://oreil.ly/bcFop)文件中找到。解压这个文件并使用
    Danfo.js 加载内容。
- en: Many browsers may have issues with concurrently reading all 150 images, so I’ve
    limited the demo to process only 130 images. Working against concurrent data limitations
    is a common issue with machine learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 许多浏览器可能会在同时读取所有150个图像时出现问题，所以我限制了演示只处理130个图像。与并发数据限制作斗争是机器学习中常见的问题。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Once the image has been featurized, it takes up a lot less space. Feel free
    to experiment with creating features in batches, but that’s outside the scope
    of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被提取特征，它所占用的空间就会少得多。随意尝试批量创建特征，但这超出了本章的范围。
- en: 'The images are already 224 x 224, so you can load them with the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已经是224 x 224，所以你可以用以下代码加载它们：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO1-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO1-1)'
- en: The second parameter to `read_csv` limits the row count to the specified number.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`的第二个参数限制了行数到指定的数字。'
- en: '[![2](assets/2.png)](#co_transfer_learning_CO1-2)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO1-2)'
- en: The DataFrames are then converted to tensors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将 DataFrames 转换为张量。
- en: '[![3](assets/3.png)](#co_transfer_learning_CO1-3)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_transfer_learning_CO1-3)'
- en: The images were flattened to become serialized but are now reshaped into a rank-four
    batch of RGB images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被展平以变成序列化，但现在被重新塑造成一个四维的 RGB 图像批次。
- en: 'After a bit of time, this code prints out the X and Y shapes of 130 ready-to-go
    images and encodings:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间，这段代码会打印出130个准备好的图像和编码的 X 和 Y 形状：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If your computer is unable to handle the 130 images, you can lower the `numImages`
    variable and still play along. However, the load time for the CSV file is always
    constant because the entire file must be processed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机无法处理130个图像，你可以降低`numImages`变量，仍然可以继续。然而，CSV文件的加载时间始终是恒定的，因为整个文件必须被处理。
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Images like chess pieces are perfect for image augmentation because skewing
    chess pieces would never cause one piece to be confused with another. If you ever
    need more images, you can mirror the entire set to effectively double your data.
    [Entire libraries exist to mirror, tilt, and skew images](https://oreil.ly/tCMTN)
    so you can create more data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 象棋棋子这样的图像非常适合进行图像增强，因为扭曲棋子永远不会导致一个棋子被误认为是另一个。如果你需要更多的图像，你可以镜像整个集合，有效地将你的数据翻倍。[存在整个库来镜像、倾斜和扭曲图像](https://oreil.ly/tCMTN)，这样你就可以创建更多数据。
- en: Loading the feature model
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载特征模型
- en: You can load the feature model just like you’d load any model from TensorFlow
    Hub. You can pass the code through the model for prediction, and it will result
    in `numImages` predictions. The code looks like [Example 11-2](#example_load_tfhub_feature).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像加载 TensorFlow Hub 中的任何模型一样加载特征模型。你可以通过模型进行预测，结果将是`numImages`个预测。代码看起来像[示例
    11-2](#example_load_tfhub_feature)。
- en: Example 11-2\. Loading and using the feature vector model
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 加载和使用特征向量模型
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of the console log is
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台日志的输出是
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each of the 130 images has become a set of 1,664 floating-point values that
    are sensitive to features of the image. If you change the model to use a different
    depth, the number of features will change. The number 1,664 is unique to the 1.30
    depth version of MobileNet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个130个图像已经变成了一组1,664个浮点值，这些值对图像的特征敏感。如果你改变模型以使用不同的深度，特征的数量也会改变。1,664这个数字是 MobileNet
    1.30深度版本独有的。
- en: As previously mentioned, the 1,664 `Float32` feature set is significantly smaller
    than the `224*224*3 = 150,528` `Float32` input of each image. This will speed
    up training and be kinder to your computer memory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，1,664个`Float32`特征集比每个图像的`224*224*3 = 150,528`个`Float32`输入要小得多。这将加快训练速度，并对计算机内存更友好。
- en: Creating your neural network
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建你自己的神经网络
- en: Now that you have a collection of features, you can create a new and utterly
    untrained model that fits those 1,664 features to your labels.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一组特征，你可以创建一个新的完全未经训练的模型，将这1,664个特征与你的标签相匹配。
- en: Example 11-3\. A small 64-layer model with a final layer of 6
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. 一个包含64层的小模型，最后一层是6
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO2-1)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO2-1)'
- en: This Layers model is using a slightly different syntax than you’re used to.
    Rather than calling `.add`, all the layers are being presented in an array of
    the initial configuration. This syntax is nice for a small model like this.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Layers 模型使用了一个与你习惯的略有不同的语法。而不是调用`.add`，所有的层都被呈现在初始配置的数组中。这种语法对于像这样的小模型很好。
- en: '[![2](assets/2.png)](#co_transfer_learning_CO2-2)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO2-2)'
- en: The `inputShape` of the model is set to `1,664` dynamically, in the case that
    you’d like to change the model’s depth multiplier by updating the model URL.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的`inputShape`被动态设置为`1,664`，以防你想通过更新模型 URL 来改变模型的深度乘数。
- en: Training results
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练结果
- en: Nothing is new in the training code. The model trains based on the feature output.
    Because the feature output is so small compared to the original image tensor,
    the training happens extremely quickly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练代码中没有什么新的。模型基于特征输出进行训练。由于特征输出与原始图像张量相比非常小，训练速度非常快。
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Within a few epochs, the model has outstanding accuracy. Take a look at [Figure 11-6](#hub_transfer_results).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 几个周期后，模型的准确率就会非常出色。查看[图 11-6](#hub_transfer_results)。
- en: '![transfer learning results](assets/ltjs_1106.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习结果](assets/ltjs_1106.png)'
- en: Figure 11-6\. From 50% to 96% validation accuracy in 20 epochs
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 从50%到96%的验证准确率在20个周期内
- en: Transfer learning using an existing model on TensorFlow Hub relieves you of
    architectural headaches and rewards you with high accuracy. But it’s not the only
    way you can implement transfer learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow Hub上使用现有模型进行迁移学习可以减轻架构方面的困扰，并为你带来高准确性。但这并不是你实现迁移学习的唯一方式。
- en: Utilizing Layers Models for Transfer Learning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Layers模型进行迁移学习
- en: There are some obvious and not-so-obvious limitations to the previous method.
    First, the feature model cannot be trained. All your training was on a new model
    that consumed the features of the Graph model, but the convolutional layers and
    size were fixed. You have small variations of the convolutional network model
    available but no way to update or fine-tune it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法存在一些明显和不那么明显的限制。首先，特征模型无法进行训练。你所有的训练都是在一个消耗图模型特征的新模型上进行的，但卷积层和大小是固定的。你可以使用卷积网络模型的小变体，但无法更新或微调它。
- en: The previous model from TensorFlow Hub was a Graph model. The Graph model was
    optimized for speed and, as you know, cannot be modified or trained. On the other
    side, Layers models are primed for modification, so you can rewire them for transfer
    learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 之前来自TensorFlow Hub的模型是一个图模型。图模型被优化用于速度，并且无法修改或训练。另一方面，Layers模型则适用于修改，因此你可以将它们重新连接以进行迁移学习。
- en: Also, in the previous example, you were essentially dealing with two models
    every time you would need to classify an image. You would have to load two JSON
    models and run your image through the feature model and then the new model to
    categorize your images. It’s not the end of the world, but a single model is possible
    via combining Layers models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在之前的示例中，每次需要对图像进行分类时，实际上都在处理两个模型。你需要加载两个JSON模型，并将图像通过特征模型和新模型以对图像进行分类。这并不是世界末日，但通过组合Layers模型可以实现单一模型。
- en: Let’s solve the same chess problem again, but with a Layers version of MobileNet
    so we can inspect the difference.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次解决同样的国际象棋问题，但这次使用Layers版本的MobileNet，这样我们可以检查差异。
- en: Shaving Layers on MobileNet
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在MobileNet上修剪层
- en: For this exercise, you will use a version of MobileNet v1.0 that is set up for
    being a Layers model. This is the model Teachable Machine uses, and while it’s
    sufficient for small exploratory projects, you’ll notice it’s not as accurate
    as the MobileNet v2 with 1.30 depth. You’re already well versed in converting
    models with the wizard, as you learned in [Chapter 7](ch07.html#the_chapter_7),
    so you can create a larger, newer Layers model when needed. Accuracy is an important
    metric, but it’s far from the only metric you should evaluate when shopping for
    a transfer model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将使用一个设置为Layers模型的MobileNet v1.0版本。这是Teachable Machine使用的模型，虽然对于小型探索性项目来说已经足够了，但你会注意到它不如深度为1.30的MobileNet
    v2准确。你已经熟悉了使用向导转换模型的方法，就像你在[第7章](ch07.html#the_chapter_7)中学到的那样，所以在需要时你可以创建一个更大、更新的Layers模型。准确性是一个重要的指标，但在寻找迁移模型时，它远非唯一的评估指标。
- en: 'MobileNet has a vast collection of layers, and some of these are layers you’ve
    never seen before. Let’s take a look. Load the MobileNet model associated with
    this chapter and review the summary of layers with `model.summary()`. This prints
    a huge list of layers. Don’t feel overwhelmed. When you read from the bottom to
    the top, the last two convolutional layers with activations are called `conv_preds`
    and `conv_pw_13_relu`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet有大量的层，其中一些是你以前从未见过的。让我们来看一下。加载与本章相关联的MobileNet模型，并使用`model.summary()`来查看层的摘要。这会打印出一个庞大的层列表。不要感到不知所措。当你从底部向上阅读时，最后两个带有激活的卷积层被称为`conv_preds`和`conv_pw_13_relu`：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The last convolution, `conv_preds`, serves as a `flatten` layer of the features
    to the 1,000 possible classes. This is somewhat specific to the model’s trained
    classes, so because of that, we’ll jump up to the second convolution (`conv_pw_13_relu`)
    and cut there.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个卷积层`conv_preds`作为将特征展平到1,000个可能类别的`flatten`层。这在一定程度上是特定于模型训练的类别，因此因此我们将跳到第二个卷积层(`conv_pw_13_relu`)并在那里裁剪。
- en: MobileNet is a complex model, and even though you don’t have to understand all
    the layers to use it for transfer learning, there’s a bit of art in deciding what
    to remove. In simpler models, like the one for the upcoming Chapter Challenge,
    it’s common to keep the entire convolutional workflow and cut at the flatten layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet是一个复杂的模型，即使你不必理解所有的层来用它进行迁移学习，但在决定移除哪些部分时还是需要一些技巧。在更简单的模型中，比如即将到来的章节挑战中的模型，通常会保留整个卷积工作流程，并在flatten层进行裁剪。
- en: You can cut to a layer by knowing its unique name. The code shown in [Example 11-4](#printing_shaved_layers)
    is [available on GitHub](https://oreil.ly/KfhNb).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过知道其唯一名称来裁剪到一个层。[示例11-4](#printing_shaved_layers)中显示的代码在[GitHub上可用](https://oreil.ly/KfhNb)。
- en: Example 11-4\.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-4。
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code from [Example 11-4](#printing_shaved_layers) prints out two large models,
    but the key difference is that the second model suddenly stops at `conv_pw_13_relu`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例11-4](#printing_shaved_layers)中的代码打印出两个大模型，但关键区别在于第二个模型突然在`conv_pw_13_relu`处停止。'
- en: The last layer is now the one we identified. When you review the summary of
    the shaved-down model, it’s like a feature extractor. There is a key difference
    that should be noted. The final layer is a convolution, so the first layer of
    your constructed transfer model should flatten the convolutional input so it can
    be densely connected to a neural network.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最后一层是我们确定的那一层。当你查看修剪后模型的摘要时，它就像一个特征提取器。有一个应该注意的关键区别。最后一层是一个卷积层，因此你构建的迁移模型的第一层应该将卷积输入展平，以便可以与神经网络进行密集连接。
- en: Layers Feature Model
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层特征模型
- en: 'Now you can use the shaved model as a features model. This gets you the same
    two-model system you had from TFHub. Your second model will need to read the output
    of `conv_pw_13_relu`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以将修剪后的模型用作特征模型。这将为你带来与TFHub相同的双模型系统。你的第二个模型需要读取`conv_pw_13_relu`的输出：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are setting the shape as defined by the intermediate features. This could
    also be directly tied to the shaved model’s output shape (`shavedModel.outputs[0].shape.slice(1)`).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在设置由中间特征定义的形状。这也可以直接与修剪模型的输出形状相关联（`shavedModel.outputs[0].shape.slice(1)`）。
- en: From here, you’re right back to where you were in the TFHub model. The base
    model creates features, and the second model interprets those features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，您又回到了TFHub模型的起点。基础模型创建特征，第二个模型解释这些特征。
- en: Training with these two layers achieves around 80%+ accuracy. Keep in mind we’re
    using a completely different model architecture (this is MobileNet v1) and a lower
    depth multiplier. Getting at least 80% from this rough model is good.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个层进行训练可以实现大约80%以上的准确率。请记住，我们使用的是完全不同的模型架构（这是MobileNet v1）和较低的深度乘数。从这个粗糙模型中至少获得80%是不错的。
- en: A Unified Model
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统一模型
- en: Just as with the feature vector model, your training only has access to a few
    layers and does not update the convolutional layers. Now that you’ve trained two
    models, you can unify their layers again into a single model. You might be wondering
    why you’re combining the model after training instead of before. It’s a common
    practice to train your new layers with your feature layers locked or “frozen”
    to their original weights.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像特征向量模型一样，您的训练只能访问几层，并且不会更新卷积层。现在您已经训练了两个模型，可以将它们的层再次统一到一个单一模型中。您可能想知道为什么在训练后而不是之前将模型合并。在训练新层时，将您的特征层锁定或“冻结”到其原始权重是一种常见做法。
- en: Once the new layers have gotten trained up, you can generally “unfreeze” more
    layers and train the new and the old together. This phase is often called *fine-tuning*
    the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦新层得到训练，通常可以“解冻”更多层，并一起训练新的和旧的。这个阶段通常被称为*微调*模型。
- en: 'So how do you unify these two models now? The answer is surprisingly simple.
    Create a third sequential model and add the two models with `model.add`. The code
    looks like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在如何统一这两个模型呢？答案出奇地简单。创建第三个顺序模型，并使用`model.add`添加两个模型。代码如下：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The new `combo` model can be downloaded or trained further.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`combo`模型可以下载或进一步训练。
- en: If you had joined the models before training the new layers, you’d likely see
    your model overfit the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练新层之前将模型合并，您可能会看到您的模型过度拟合数据。
- en: No Training Needed
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无需训练
- en: It’s worth noting that there’s a witty way to use two models for transfer learning
    with zero training. The trick is to use a second model that identifies distances
    in similarity.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，有一种巧妙的方法可以使用两个模型进行零训练的迁移学习。诀窍是使用第二个模型来识别相似性中的距离。
- en: The second model is called K-Nearest Neighbors (KNN)^([1](ch11.html#idm45049236839224))
    model, and it groups a data element with K of the most similar data elements in
    a feature space. The idiom “birds of a feather flock together” is the premise
    for KNN.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型称为K-最近邻（KNN）^([1](ch11.html#idm45049236839224))模型，它将数据元素与特征空间中K个最相似的数据元素分组在一起。成语“物以类聚”是KNN的前提。
- en: In [Figure 11-7](#knn_graph), X would be identified as a bunny because the three
    nearest examples in features are also bunnies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11-7](#knn_graph)中，X将被识别为兔子，因为特征中的三个最近示例也是兔子。
- en: '![feature distance](assets/ltjs_1107.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![特征距离](assets/ltjs_1107.png)'
- en: Figure 11-7\. Identify with neighbors in feature space
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7。在特征空间中识别邻居
- en: KNN is sometimes called *instance-based learning* or *lazy learning* because
    you’re moving all the necessary processing to the moment of classification of
    the data around it. This differed model is straightforward to update. You can
    always add more images and classes dynamically to define edge cases or new categories
    without retraining. The cost comes from the fact that the feature graph grows
    with each example you add, unlike the fixed space of a single trained model. The
    more data points you add to a KNN solution, the larger the feature set that accompanies
    the models will become.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: KNN有时被称为*基于实例的学习*或*惰性学习*，因为你将所有必要的处理移动到数据分类的时刻。这种不同的模型很容易更新。您可以始终动态地添加更多图像和类别，以定义边缘情况或新类别，而无需重新训练。成本在于特征图随着添加的每个示例而增长，而不像单个训练模型的固定空间。您向KNN解决方案添加的数据点越多，伴随模型的特征集就会变得越大。
- en: Additionally, since there is no training, similarity is the *only* metric. This
    makes this system nonideal for some problems. For instance, if you were trying
    to train a model to see if people were wearing face masks or not, then you’re
    looking for a model to focus on a single feature rather than the collection of
    several features. Two people who are dressed the same might share more similarities
    and therefore be placed in the same category with KNN. For KNN to work on masks,
    your feature vector model would have to be face-specific, where trained models
    can learn differentiating patterns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于没有训练，相似性是*唯一*的度量标准。这使得这个系统对于某些问题来说并不理想。例如，如果您试图训练一个模型来判断人们是否戴着口罩，那么您需要一个模型专注于单个特征而不是多个特征的集合。穿着相同的两个人可能具有更多相似之处，因此可能会被放在KNN中的同一类别中。要使KNN在口罩上起作用，您的特征向量模型必须是面部特定的，训练模型可以学习区分模式。
- en: 'Easy KNN: Bunnies Versus Sports Cars'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的KNN：兔子对运动汽车
- en: KNN, like MobileNet, has a JS wrapper provided by Google. We can implement KNN
    transfer learning quickly by hiding all the complexity details use MobileNet and
    KNN NPM packages to make a quick transfer learning demo.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: KNN，就像MobileNet一样，由Google提供了一个JS包装器。我们可以通过隐藏所有复杂细节，使用MobileNet和KNN NPM包快速实现KNN迁移学习，以制作一个快速的迁移学习演示。
- en: Not only are we going to avoid running any training, but we’ll also use existing
    libraries to avoid any deep dive into TensorFlow.js. We’ll be doing this for a
    flashy demo, but if you decide to build something more robust with these models,
    you should probably evaluate avoiding abstract packages that you don’t control.
    You already understand all the inner workings of transfer learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅要避免运行任何训练，还要使用现有库来避免深入研究TensorFlow.js。我们将为一个引人注目的演示而这样做，但如果您决定使用这些模型构建更健壮的东西，您可能应该考虑避免使用您无法控制的抽象包。您已经了解了迁移学习的所有内部工作原理。
- en: 'To do this quick demo, you’ll import the three NPM modules:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个快速演示，您将导入三个NPM模块：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For simplicity, the example code from this chapter has all the images on the
    page, so you can directly reference them. Now you can load MobileNet with `mobileNet
    = await mobilenet.load();` and the KNN classifier with `knnClassifier.create();`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，本章的示例代码中所有的图像都在页面上，因此您可以直接引用它们。现在您可以使用`mobileNet = await mobilenet.load();`加载MobileNet，并使用`knnClassifier.create();`加载KNN分类器。
- en: 'The KNN classifier needs examples of each class. To simplify this process I’ve
    created the following helper function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: KNN分类器需要每个类别的示例。为了简化这个过程，我创建了以下辅助函数：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO3-1)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO3-1)'
- en: The `infer` method returns values rather than the rich JavaScript object of
    detections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`infer`方法返回值，而不是富JavaScript对象的检测。'
- en: '[![2](assets/2.png)](#co_transfer_learning_CO3-2)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO3-2)'
- en: The image `id` on the page will tell MobileNet what image to resize and process.
    The tensor logic is hidden by JavaScript, but many chapters in this book have
    explained what is actually happening.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上的图像`id`将告诉MobileNet要调整大小和处理哪个图像。张量逻辑被JavaScript隐藏，但本书的许多章节已经解释了实际发生的事情。
- en: '[![3](assets/3.png)](#co_transfer_learning_CO3-3)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_transfer_learning_CO3-3)'
- en: The MobileNet model returns the features (sometimes called *embeddings*) of
    the image. If this is not set, then the tensor of 1,000 raw values is returned
    (sometimes called *logits*).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet模型返回图像的特征（有时称为*嵌入*）。如果未设置，则返回1,000个原始值的张量（有时称为*对数*）。
- en: 'Now you can add examples of each class with this helper method. You just name
    the image element’s unique DOM ID and what class it should be associated with.
    Adding three examples of each is as simple as this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用这个辅助方法为每个类别添加示例。您只需命名图像元素的唯一DOM ID以及应与之关联的类别。添加三个示例就像这样简单：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Lastly, it’s the same system to predict. Get the features of an image, and ask
    the classifier to identify which class it believes the input is based on KNN.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，预测的系统是相同的。获取图像的特征，并要求分类器根据KNN识别输入基于哪个类。
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO4-1)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO4-1)'
- en: The `classIndex` is the number as passed in `addExample`. If a third class is
    added, that new index would be a possible output.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`classIndex`是作为`addExample`中传递的数字。如果添加第三个类别，那么新的索引将成为可能的输出。'
- en: '[![2](assets/2.png)](#co_transfer_learning_CO4-2)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO4-2)'
- en: The web page text is changed from “???” to the result.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 网页文本从“???”更改为结果。
- en: The result is that the AI can identify the correct class for a new image by
    comparing against six examples, as shown in [Figure 11-8](#bunnyvscars).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是AI可以通过与六个示例进行比较来识别新图像的正确类别，如[图11-8](#bunnyvscars)所示。
- en: '![screenshot of AI page](assets/ltjs_1108.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![AI页面的截图](assets/ltjs_1108.png)'
- en: Figure 11-8\. With only three images of each class, the KNN model predicts correctly
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. 仅有每个类别三张图像，KNN模型预测正确
- en: You can dynamically add more and more classes. KNN is an exciting and expandable
    way to utilize the experience of advanced models through transfer learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以动态地添加更多类别。KNN是一种令人兴奋且可扩展的方式，通过迁移学习利用先进模型的经验。
- en: Chapter Review
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节回顾
- en: Because this chapter has explained the mystery of transfer learning with MobileNet,
    you now have the ability to apply this power-up to any preexisting model you can
    somewhat comprehend. Perhaps you want to adjust the pet’s faces model to find
    cartoon or human faces. You don’t have to start from scratch!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因为本章已经解释了使用MobileNet进行迁移学习的神秘，现在您可以将这种增强功能应用于您可以在一定程度上理解的任何现有模型。也许您想调整宠物面孔模型以查找卡通或人脸。您不必从头开始！
- en: Transfer learning adds a new utility to your toolbelt of AI. Now when you find
    a new model in the wild, you can ask yourself how you could use it directly *and*
    how you can use it in transfer learning for something similar.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习为您的AI工具箱增加了新的实用功能。现在，当您在野外找到一个新模型时，您可以问自己如何直接使用它，以及如何将其用于类似的迁移学习。
- en: 'Chapter Challenge: Warp-Speed Learning'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 章节挑战：光速学习
- en: The Hogwarts sorting model from the previous chapter has thousands of black-and-white
    drawing images of experience in the convolutional layers. Unfortunately, those
    thousands of images were limited to animals and skulls. They all have nothing
    to do with *Star Trek*. Don’t fret; with only 50 or so new images, you can re-train
    the model from the previous chapter to identify the three *Star Trek* symbols
    shown in [Figure 11-9](#transfer_trek_logos).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中的霍格沃茨分选模型在卷积层中有数千张黑白绘画图像的经验。不幸的是，这些数千张图像仅限于动物和头骨。它们与*星际迷航*无关。不要担心；只需使用大约50张新图像，您就可以重新训练上一章的模型，以识别[图11-9](#transfer_trek_logos)中显示的三个*星际迷航*符号。
- en: '![Perfect validation accuracy in a few epochs](assets/ltjs_1109.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![几个时代内的完美验证准确性](assets/ltjs_1109.png)'
- en: Figure 11-9\. Star Trek symbols
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9\. 星际迷航符号
- en: Set phasers to fun and use the methods you learned in this chapter to take the
    Layers model you trained in [Chapter 10](ch10.html#the_chapter_10) (or download
    the trained one from the associated [book source code](https://oreil.ly/v3tvg)),
    and train a new model that can identify these images from a mere few examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将相位设置为有趣，并使用本章学到的方法来获取您在[第10章](ch10.html#the_chapter_10)中训练的Layers模型（或从相关的[书源代码](https://oreil.ly/v3tvg)下载已训练的模型），并训练一个新模型，可以从仅有几个示例中识别这些图像。
- en: The new training image data can be found in CSV form in [the associated book
    source code](https://oreil.ly/3dqcq). The training image data has been put in
    a CSV so you can easily import it with Danfo.js. The files are *images.csv* and
    *labels.csv*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 新的训练图像数据可以在[相关书籍源代码](https://oreil.ly/3dqcq)中以CSV形式找到。训练图像数据已经放在CSV中，因此您可以使用Danfo.js轻松导入它。文件是*images.csv*和*labels.csv*。
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[附录B](app02.html#appendix_b)中找到这个挑战的答案。
- en: Review Questions
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复习问题
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下您在本章编写的代码中学到的教训。花点时间回答以下问题：
- en: What does KNN stand for?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KNN代表什么？
- en: Whenever you have a small training set, there’s a danger of what?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当您有一个小的训练集时，存在什么危险？
- en: When you’re looking for the convolutional half of a CNN model on TensorFlow
    Hub, what tag are you looking for?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在TensorFlow Hub上寻找CNN模型的卷积部分时，您要寻找哪个标签？
- en: Which depth multiplier will have a more extensive feature output, 0.50 or 1.00?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个深度乘数会产生更广泛的特征输出，0.50还是1.00？
- en: What method can you call on the MobileNet NPM module to gather the feature embeddings
    of an image?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以调用MobileNet NPM模块的哪种方法来收集图像的特征嵌入？
- en: Should you combine your transfer model parts and then train, or train and then
    combine your models?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该先组合您的转移模型部分然后训练，还是先训练然后组合您的模型？
- en: When you cut a model at the convolutional layer, what do you have to do before
    importing that information to a neural network’s dense layers?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在卷积层上切割模型时，在将该信息导入神经网络的密集层之前，您需要做什么？
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在[附录A](app01.html#book_appendix)中找到。
- en: ^([1](ch11.html#idm45049236839224-marker)) KNN was developed by Evelyn Fix and
    Joseph Hodges in 1951.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.html#idm45049236839224-marker)) KNN是由Evelyn Fix和Joseph Hodges于1951年开发的。
