- en: 14 Question answering with a fine-tuned large language model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 使用微调的大型语言模型进行问答
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building a question-answering application using an LLM
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM构建问答应用程序
- en: Curating a question-answering dataset for training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练创建问答数据集
- en: Fine-tuning a Transformer-based LLM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调基于Transformer的LLM
- en: Integrating a deep-learning-based NLP pipeline to extract and rank answers from
    search results
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将基于深度学习的NLP管道集成到搜索结果中提取和排序答案
- en: 'We covered the basics of semantic search using Transformers in chapter 13,
    so we’re now ready to attempt one of the hardest problems in search: question
    answering.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第13章中介绍了使用Transformers进行语义搜索的基本知识，因此我们现在准备尝试搜索中最难的问题之一：问答。
- en: '*Question answering* is the process of returning an answer for a searcher’s
    query, rather than just a list of search results. There are two types of question-answering
    approaches: extractive and abstractive. *Extractive question answering* is the
    process of finding exact answers to questions from your documents. It returns
    snippets of your documents containing the likely answer to the user’s question
    so they don’t need to sift through search results. In contrast, *abstractive question
    answering* is the process of generating responses to a user’s question either
    as a summary of multiple documents or directly from an LLM with no source documents.
    In this chapter, we’ll focus primarily on extractive question answering, saving
    abstractive question answering for chapter 15.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*问答*是返回搜索者查询答案的过程，而不仅仅是搜索结果列表。问答方法有两种类型：抽取式和抽象式。*抽取式问答*是从你的文档中找到问题的确切答案的过程。它返回包含用户问题可能答案的文档片段，这样用户就不需要筛选搜索结果。相比之下，*抽象式问答*是生成用户问题响应的过程，可以是多份文档的摘要，也可以直接从没有源文档的LLM生成。在本章中，我们将主要关注抽取式问答，将抽象式问答留到第15章。'
- en: 'By solving the question-answering problem, you will accomplish three things:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决问答问题，你将完成三件事：
- en: You’ll better understand the Transformer tooling and ecosystem that you started
    learning about in chapter 13\.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将更好地理解你在第13章开始学习的Transformer工具和生态系统。
- en: You’ll learn how to fine-tune a large language model to a specific task.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习如何将大型语言模型微调到特定任务。
- en: You’ll merge your search engine with advanced natural language techniques.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将把你的搜索引擎与高级自然语言技术合并。
- en: In this chapter, we’ll show you how to provide direct answers to questions and
    produce a working question-answering application. The query types we’ll address
    are single clause *who*, *what*, *when*, *where*, *why*, and *how* questions.
    We’ll also continue using the Stack Exchange outdoors dataset from the last chapter.
    Our goal is to enable users to ask a *previously unseen question* and get a short
    answer in response, eliminating the need for users to read through multiple search
    results to find the answer themselves.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向你展示如何直接回答问题并生成一个可工作的问答应用程序。我们将处理的查询类型包括单句的*谁*、*什么*、*何时*、*何地*、*为什么*和*如何*问题。我们还将继续使用上一章的Stack
    Exchange户外数据集。我们的目标是使用户能够提出*以前未见过的*问题，并得到简短的答案作为回应，从而消除用户需要阅读多个搜索结果以找到答案的必要性。
- en: 14.1 Question-answering overview
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 问答概述
- en: Traditional search returns lists of documents or pages in response to a query,
    but people may often be looking for a quick answer to their question. In this
    case, we want to save people from having to dig for an answer in blocks of text
    when a straightforward one exists in our content.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统搜索在查询响应时返回文档或页面的列表，但人们可能经常在寻找问题的快速答案。在这种情况下，我们希望避免人们不得不在文本块中挖掘答案，而答案在我们的内容中已经直接存在。
- en: In this section, we’ll introduce the question-answering task and then define
    the retriever-reader pattern for implementing question-answering.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍问答任务，然后定义用于实现问答的检索器-阅读器模式。
- en: 14.1.1 How a question-answering model works
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 问答模型的工作原理
- en: 'Let’s look at how a question-answering model works in practice. Specifically,
    we’re implementing *extractive question answering*, which finds the best answer
    to a question in a given text. For example, take the following question:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个问答模型在实际中是如何工作的。具体来说，我们正在实现*抽取式问答*，它能在给定的文本中找到问题的最佳答案。例如，考虑以下问题：
- en: 'Q: `What are minimalist shoes?`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: `什么是极简主义鞋？`'
- en: Extractive question answering works by looking through a large document that
    probably contains the answer, and it identifies the answer for you.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 提取式问题回答通过查看可能包含答案的大文档来工作，并为你识别答案。
- en: 'Let’s look at a document that might contain the answer to our question. We
    provide the question `What` `are` `minimalist` `shoes?` and the following document
    text (the context) to a model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一份可能包含我们问题答案的文档。我们向模型提供了一个问题 `什么是` `极简主义` `鞋？` 以及以下文档文本（上下文）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The document can be broken into many small parts, known as *spans*, and the
    model extracts the best span as the answer. A working question-answering model
    will evaluate the question and context and may produce this span as the answer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文档可以被分解成许多小的部分，称为 *span*，模型会提取最佳span作为答案。一个工作的问题回答模型会评估问题和上下文，并可能产生这个span作为答案：
- en: 'A: `shoes intended to closely approximate barefoot running conditions`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: `旨在紧密模拟赤脚跑步条件的鞋`'
- en: But how does the model know the probability of whether any given span is the
    answer? We could try to look at different spans and see if they all somehow represent
    the answer, but that would be very complicated. Instead, the problem can be simplified
    by first learning the probability of whether each token in the context is the
    start of the answer and also the probability of whether each token in the context
    is the end of the answer. Since we are only looking at the probability for one
    token to represent the start, and another the end, the problem is easier to understand
    and solve. Our tokens are treated as discrete values, and the extractive question-answering
    model is trained to learn a *probability mass function* (PMF), which is a function
    that gives the probability that a discrete random variable is exactly equal to
    some value. This is different than measuring values that are continuous and are
    used in probability distributions, as we discussed with the continuous beta distribution
    in chapter 11\. The main difference between the two is that our tokens are discrete
    values.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但是模型是如何知道任何给定的span是否是答案的概率呢？我们可以尝试查看不同的span，看看它们是否以某种方式代表答案，但这会非常复杂。相反，通过首先学习上下文中每个标记是否是答案的开始以及每个标记是否是答案的结束的概率，可以简化这个问题。因为我们只关注一个标记代表开始的概率，另一个标记代表结束的概率，所以问题更容易理解和解决。我们的标记被视为离散值，提取式问题回答模型被训练来学习一个
    *概率质量函数*（PMF），这是一个函数，它给出了离散随机变量恰好等于某个值的概率。这与我们在第11章中讨论的连续beta分布中测量的连续值不同，这些连续值用于概率分布。这两个之间的主要区别在于我们的标记是离散值。
- en: Using this strategy, we can train one model that will learn two probability
    mass functions—one for the starting token of the answer span, and one for the
    ending token of the answer span. You may have caught before that we said the model
    “*may* produce” the previous answer. Since models trained with different data
    and hyperparameters will give different results, the specific answer provided
    for a question can vary based on the model’s training parameters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，我们可以训练一个模型，该模型将学习两个概率质量函数——一个用于答案span的起始标记，另一个用于答案span的结束标记。你可能会注意到我们之前说模型“*可能*”产生之前的答案。由于使用不同数据和超参数训练的模型会给出不同的结果，因此针对特定问题的具体答案可以基于模型的训练参数而变化。
- en: To illustrate how this works, we’ll start with a model that someone has already
    trained for the extractive question-answering task. The model will output the
    likelihood of whether the token is the start or the end of an answer span. When
    we identify the most likely start and end of the answer, that’s our answer span.
    The pretrained model we’ll use is `deepset/roberta-base-squad2`, available from
    the Hugging Face organization and trained by the Deepset team. We will pass the
    question and context through this model and pipeline in listings 14.1 through
    14.3 to determine the answer span start and end probabilities, as well as the
    final answer. Figure 14.1 demonstrates how this process works by *tokenizing*
    the question and context input, *encoding* that input, and *predicting* the most
    appropriate answer span.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这是如何工作的，我们将从一个已经为提取式问答任务训练好的模型开始。该模型将输出分词是答案跨度开始或结束的概率。当我们确定最可能的答案开始和结束位置时，那就是我们的答案跨度。我们将使用的预训练模型是
    `deepset/roberta-base-squad2`，由 Hugging Face 组织提供并由 Deepset 团队训练。我们将通过列表 14.1
    到 14.3 中的模型和管道将问题和上下文传递给这个模型，以确定答案跨度开始和结束的概率，以及最终的答案。图 14.1 通过对问题和上下文输入进行 *分词*、*编码*
    和 *预测* 最合适的答案跨度来演示此过程。
- en: '![figure](../Images/CH14_F01_Grainger.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH14_F01_Grainger.png)'
- en: Figure 14.1 Extractive question-answering prediction process
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.1 提取式问答预测过程
- en: 'In the figure, you can see that the question and context are first combined
    into a pair for tokenization. Tokenization is then performed on the pair, obtaining
    token inputs for the model. The model accepts those inputs and then outputs two
    sequences: the first sequence is the probabilities for whether each token in the
    context is the start of an answer, and the second sequence is the probabilities
    for whether each token in the context is the end of an answer. The start and end
    probability sequences are then combined to get the most likely answer span.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你可以看到问题和上下文首先组合成一对以进行分词。然后对这个对进行分词，获得模型的分词输入。模型接受这些输入，然后输出两个序列：第一个序列是上下文中每个分词是答案开始的概率，第二个序列是上下文中每个分词是答案结束的概率。然后开始和结束概率序列被组合起来以获得最可能的答案跨度。
- en: 'The following listing walks through the first step of this process: tokenization.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表介绍了此过程的第一个步骤：分词。
- en: Listing 14.1 Loading the tokenizer and model
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.1 加载分词器和模型
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The model name in listing 14.1 is a public model specifically pretrained for
    extractive question answering. With the model and tokenizer ready, we can now
    pass in a question and answer pair, shown in the following listing. The response
    will show that the number of tokens is equal to the number of start and end probabilities.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.1 中的模型名称是一个专门为提取式问答预训练的公开模型。有了模型和分词器准备就绪，我们现在可以传入一个问题和答案对，如下所示列表。响应将显示标记的数量等于开始和结束概率的数量。
- en: Listing 14.2 Tokenizing a question and context
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.2 分词问题及其上下文
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Response:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The inputs are obtained by tokenizing the question and context together. The
    outputs are obtained by making a forward pass with the inputs through the model.
    The `outputs` variable is a two-item list. The first item contains the start probabilities,
    and the second item contains the end probabilities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是通过将问题和上下文一起分词获得的。输出是通过将输入通过模型进行正向传递获得的。`outputs` 变量是一个包含两个元素的列表。第一个元素包含开始概率，第二个元素包含结束概率。
- en: Figure 14.2 visually demonstrates the probabilities for whether each token in
    the context is likely the start of an answer span, and figure 14.3 likewise demonstrates
    whether each token is likely the end of an answer span (darker highlighting indicates
    a higher probability).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 通过视觉演示了上下文中每个分词是否可能是答案跨度开始的概率，而图 14.3 同样演示了每个分词是否可能是答案跨度结束的概率（较暗的突出显示表示更高的概率）。
- en: '![figure](../Images/CH14_F02_Grainger.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH14_F02_Grainger.png)'
- en: Figure 14.2 Probabilities for whether the token is the start of an answer span
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.2 分词是否为答案跨度开始的概率
- en: '![figure](../Images/CH14_F03_Grainger.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH14_F03_Grainger.png)'
- en: Figure 14.3 Probabilities for whether the token is the end of an answer span
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.3 分词是否为答案跨度结束的概率
- en: Note that each token has a start probability (in figure 14.2) and an end probability
    (in figure 14.3) at its respective index. We also normalized the start and end
    probabilities at each index to be between `0.0` and `1.0`, which makes them easier
    to think about and calculate. We call these start and end probability lists *logits*,
    since they are lists of statistical probabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个标记在其各自的索引处都有一个起始概率（在图14.2中）和一个结束概率（在图14.3中）。我们还对每个索引处的起始和结束概率进行了归一化，使其介于`0.0`和`1.0`之间，这使得它们更容易思考和计算。我们称这些起始和结束概率列表为*logits*，因为它们是统计概率的列表。
- en: For example, for the 17th token (`_definition`), the probability of this token
    being the start of the answer is ~`0.37`, and the probability of it being the
    end of the answer is ~`0.20`. Since we’ve normalized both lists, the start of
    our answer span is the token where `start_logits_norm` `==` `1.0`, and the end
    of the answer span is where `end_logits_norm == 1.0`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于第17个标记（`_definition`），该标记作为答案起始的概率约为`0.37`，而作为答案结束的概率约为`0.20`。由于我们已经对这两个列表进行了归一化，因此答案跨度的起始是`start_logits_norm`等于`1.0`的标记，而答案跨度的结束是`end_logits_norm`等于`1.0`的位置。
- en: The following listing demonstrates both how to generate the token lists in figures
    14.2 and 14.3, as well as how to extract the final answer span.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了如何在图14.2和14.3中生成标记列表，以及如何提取最终的答案跨度。
- en: Listing 14.3 Identifying an answer span from a tokenized context
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.3 从标记化上下文中识别答案跨度
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Extracting the answer span, shown in the following output'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在以下输出中显示的提取答案跨度'
- en: '#2 The start probabilities, shown in figure 14.2'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 图14.2中显示的起始概率'
- en: '#3 The end probabilities, shown in figure 14.3'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 图14.3中显示的结束概率'
- en: 'Output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By fitting the start and end probability mass functions during training to a
    dataset of question, context, and answer triples, we create a model that can provide
    probabilities for the most likely answers to new questions and contexts. We then
    use this model in listing 14.3 to perform a probability search to identify the
    most likely span in the text that answers the question.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在训练过程中将起始和结束概率质量函数拟合到问答/上下文/答案三元组的集合中，我们创建了一个模型，该模型可以为新的问题和上下文提供最可能答案的概率。然后我们使用这个模型在列表14.3中进行概率搜索，以识别文本中最可能的答案跨度。
- en: 'In practice, it works as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，它的工作方式如下：
- en: We pick a minimum and maximum span size—where a span is a set of continuous
    words. For example, the answer might be one word long or, like the previous answer,
    it could be eight words long. We need to set those span sizes up front.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择一个最小和最大的跨度大小——跨度是一组连续的单词。例如，答案可能只有一个单词长，或者像之前的答案一样，它可能长达八个单词。我们需要提前设置这些跨度大小。
- en: For each span, we check the probability of whether or not the span is the correct
    answer. The answer is the span with the highest probability.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个跨度，我们检查该跨度是否是正确答案的概率。答案是具有最高概率的跨度。
- en: When we’re done checking all the spans, we present the correct answer.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们检查完所有跨度后，我们展示正确的答案。
- en: Building the model requires lots of question/context/answer triples and a way
    to provide these triples to the model so that calculations can be performed. Enter
    the Transformer encoder, which you should already be familiar with from chapter
    13\. We first encode lots of training data using an LLM that produces dense vectors.
    Then we train a neural network to learn the probability mass function of whether
    a given span’s encoding answers the question using positive and negative training
    examples.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型需要大量的问答/上下文/答案三元组和一种将这些三元组提供给模型以便进行计算的方法。这时就出现了Transformer编码器，你应该已经在第13章中熟悉了它。我们首先使用一个产生密集向量的LLM对大量训练数据进行编码。然后我们训练一个神经网络来学习给定跨度编码回答问题的概率质量函数，使用正负训练示例。
- en: 'We’ll see how to fine-tune a question-answering model later in the chapter,
    but first we need to address a very important detail: When someone asks a question,
    where do we get the context?'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面部分看到如何微调问答模型，但首先我们需要解决一个非常重要的细节：当有人提问时，我们从哪里获取上下文？
- en: 14.1.2 The retriever-reader pattern
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 检索器-阅读器模式
- en: 'In reading about how extractive question answering works, you may have thought
    “so, for every question query, I need to check the probabilities of every span
    in the entire corpus?” No! That would be extremely slow and unnecessary, since
    we already have a really fast and accurate way of getting relevant documents that
    probably contain the answer: search.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读关于抽取式问答如何工作的时候，你可能想过“所以，对于每个问题查询，我需要检查整个语料库中每个片段的概率吗？”不！那会非常慢且不必要，因为我们已经有一种非常快且准确的方法来获取可能包含答案的相关文档：搜索。
- en: What we’re really going to make is a very powerful text highlighter. Think of
    the entire question-answering system as an automatic reference librarian of sorts.
    It knows what document contains your answer, and then it reads that document’s
    text so it can point the exact answer out to you.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正要制作的是一个功能强大的文本高亮器。将整个问答系统想象成一种自动的参考图书馆。它知道哪个文档包含你的答案，然后阅读该文档的文本，以便能够指出确切的答案给你。
- en: 'This is known as the retriever-reader pattern. This pattern uses one component
    to retrieve and rank the candidate documents (running a query against the search
    engine) and another component to read the spans of the most relevant documents
    and extract the appropriate answer. This is very similar to how highlighting works
    in Lucene-based search engines like Solr, OpenSearch, or Elasticsearch: the unified
    highlighter will find the best passage(s) containing the analyzed query terms
    and use them as the context. It then identifies the exact location of the queried
    keywords in that context to show the end user the surrounding context.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为检索器-阅读器模式。这个模式使用一个组件来检索和排序候选文档（对搜索引擎运行查询）以及另一个组件来阅读最相关文档的片段并提取适当的答案。这与基于Lucene的搜索引擎（如Solr、OpenSearch或Elasticsearch）中的高亮功能非常相似：统一的高亮器将找到包含分析查询词的最佳段落，并使用它们作为上下文。然后，它确定查询关键词在该上下文中的确切位置，以便向最终用户显示周围上下文。
- en: 'We’re going to build something similar to a highlighter, but instead of showing
    the context containing the user’s queried keywords, our question-answering highlighter
    will answer questions like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建一个类似于荧光笔的工具，但与显示包含用户查询关键词的上下文不同，我们的问答荧光笔将回答类似这样的问题：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s look at the full context. When we ask `What are minimalist shoes?` we
    first use the *retriever* to get the document that is the most likely to contain
    the answer. In this case, this document (abridged here, but shown in full in section
    14.1.1) was returned:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看完整的背景。当我们问“什么是极简主义鞋？”时，我们首先使用*检索器*来获取最有可能包含答案的文档。在这种情况下，这个文档（在这里进行了摘要，但在14.1.1节中展示了全文）被返回：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the document in hand, the *reader* then scans it and finds the text that
    is most likely to answer the question.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 拿到文档后，*阅读器*会扫描它，并找到最有可能回答问题的文本。
- en: Aside from using fancy Transformers to find the correct answer, we’re taking
    a step beyond basic search in that we’ll actually use question/answer reader confidence
    as a reranker. So if we’re not sure which document is the most likely to contain
    the answer during the retriever step, we’ll have the reader look through a bunch
    of documents and see which is the best answer from all of them. The “bunch of
    documents” will be our reranking window, which we can set to any size.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用花哨的Transformer来找到正确的答案之外，我们在基本搜索之外又迈出一步，我们将实际上使用问题/答案阅读器的置信度作为重新排序器。所以，如果我们不确定在检索步骤中哪个文档最有可能包含答案，我们将让读者浏览一大堆文档，并从中找出最好的答案。这“一大堆文档”将是我们重新排序的窗口，我们可以将其设置为任何大小。
- en: Keep in mind, though, that analyzing documents in real time is not efficient.
    We shouldn’t ask the reader to look at 100 documents—that’s going to take too
    long. We’ll limit it to a much smaller number, like 3 or 5\. Limiting the reader
    window forces us to make sure our search engine is very accurate. The results
    must be relevant, as a retriever that doesn’t get relevant candidates in the top
    5 window size won’t give the reader anything useful to work with.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，实时分析文档并不高效。我们不应该要求阅读器查看100个文档——这将花费太多时间。我们将将其限制在一个更小的数字，比如3或5。限制阅读器窗口迫使我们确保我们的搜索引擎非常准确。结果必须是相关的，因为如果一个检索器在顶部5个窗口大小内找不到相关候选者，它将不会给阅读者提供任何有用的东西来工作。
- en: The retriever-reader has two separated concerns, so we can even replace our
    retriever with something else. We’ve shown how you can use a lexical search engine
    with out-of-the-box ranking (BM25, as covered in chapter 3), but you can also
    try it with a dense vector index of embeddings, as covered in chapter 13.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器-读者有两个独立的问题，因此我们可以用其他东西替换我们的检索器。我们已经展示了如何使用具有开箱即用的排名（第3章中介绍的BM25）的词汇搜索引擎，但你也可以尝试使用第13章中介绍的密集向量索引。
- en: 'Before we can take live user questions, we’ll also need to train a question-answering
    model to predict the best answer from a context. The full set of steps we’ll walk
    through for building our question-answering application is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以处理实时用户问题之前，我们还需要训练一个问答模型来从上下文中预测最佳答案。我们将逐步构建我们的问答应用程序的完整步骤如下：
- en: '*Set up a retriever using our search engine*—We’ll use a simple high-recall
    query for candidate answers for our example.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用我们的搜索引擎设置检索器*—我们将为我们的示例使用一个简单的具有高召回率的查询作为候选答案。'
- en: '*Wrangle and label the data appropriately*—This includes getting the data into
    the correct format, and getting a first pass at our answers from the base pretrained
    model. Then we’ll take that first pass and manually fix it and mark what examples
    to use for training and testing.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*适当整理和标记数据*—这包括将数据放入正确的格式，并从基础预训练模型中获得我们答案的第一遍。然后我们将进行第一遍手动修正，并标记用于训练和测试的示例。'
- en: '*Understand the nuances of the data structures*—We’ll use an existing data
    structure that will represent our training and test data in the correct format
    for a fine-tuning task.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*理解数据结构的细微差别*—我们将使用一个现有的数据结构，它将以正确的格式表示我们的训练和测试数据，以便进行微调任务。'
- en: '*Fine-tune the model*—Using the corrections we made in the previous step, we’ll
    train a fine-tuned question-answering model for better accuracy than the baseline.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*微调模型*—使用我们在上一步中进行的修正，我们将训练一个微调的问答模型，以获得比基线更好的准确性。'
- en: '*Use the model as the reader at query time*—We’ll put everything together that
    lets us request a query, get candidates from the search engine, read/rerank the
    answers from the model, and show them as a response.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在查询时使用模型作为读者*—我们将把所有这些放在一起，让我们可以请求查询，从搜索引擎获取候选答案，从模型中读取/重新排序答案，并将它们作为响应展示。'
- en: 'Figure 14.4 shows the flow of the entire architecture for our retriever, reader,
    and reranker:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4显示了我们的检索器、读者和重新排序器整个架构的流程：
- en: A question is asked by a user, and documents are queried in the retriever (search
    engine) using the question.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户提出一个问题，并使用该问题在检索器（搜索引擎）中查询文档。
- en: The search engine matches and ranks to get the top-*k* most relevant documents
    for the reader.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索引擎匹配和排序以获取对读者最相关的最高-*k*个文档。
- en: The original question is paired with each top-*k* retrieved context and sent
    into the QA pipeline.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原问题与每个检索到的最高-*k*上下文配对，并送入问答管道。
- en: The question/context pairs are tokenized and encoded into spans by the reader,
    which then predicts the top-*n* most likely answer spans, with their probabilities
    for likeliness as the score.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读者将问题/上下文对进行分词并编码成跨度，然后预测最可能的前-*n*个答案跨度，其概率作为评分。
- en: '![figure](../Images/CH14_F04_Grainger.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F04_Grainger.png)'
- en: Figure 14.4 Retriever-reader pattern for extractive question answering
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.4 提取式问答的检索器-读者模式
- en: 5\. The reranker sorts the score for each top-*n* answer span in descending
    order.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 重新排序器按降序排列每个最高-*n*个答案跨度的得分。
- en: 6\. The top ranked answer from the reranker is the accepted answer and is shown
    to the user.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 重新排序器排序的得分最高的答案被接受，并展示给用户。
- en: To accomplish all of this, we need to tune the retriever, wrangle data to train
    the reader model, fine-tune the reader model using that data, and build a reranker.
    Strategies for tuning the retriever (the search engine) have already been covered
    thoroughly in this book, so for our next step, we’ll wrangle the data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成所有这些，我们需要调整检索器，整理数据以训练读者模型，使用该数据微调读者模型，并构建一个重新排序器。关于调整检索器（搜索引擎）的策略，本书已经进行了详细的介绍，所以接下来，我们将整理数据。
- en: 14.2 Constructing a question-answering training dataset
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 构建问答训练数据集
- en: 'In this section, we’ll create a dataset we can use to train our question-answering
    model. This involves several steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个我们可以用来训练我们的问答模型的数据库。这涉及几个步骤：
- en: Gathering and cleaning a dataset to fit the question-answering problem space
    to our content
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集和清理数据集以适应我们的问答问题空间的内容
- en: Automatically creating a silver set (a semi-refined dataset that needs further
    labeling) from an existing model and corpus
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有的模型和语料库自动创建银集（一个需要进一步标记的半精炼数据集）
- en: Manually correcting the silver set to produce the golden set (a trustworthy
    dataset we can use for training)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动纠正银集以生成金集（一个我们可以用于训练的可信数据集）
- en: Splitting the dataset for training, testing, and validation of a fine-tuned
    model
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分割用于训练、测试和验证微调模型
- en: We’ll start with our Stack Exchange outdoors dataset because its data is already
    well-suited to a question-and-answer application. We need question/answer pairs
    to use when fine-tuning a base model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Stack Exchange户外数据集开始，因为它的数据已经非常适合问答应用。我们需要用于微调基础模型的问题/答案对。
- en: The outdoors dataset is already well-formatted and in bite-size question-and-answer
    chunks. With the power of Transformers, we can take off-the-shelf tools and models
    and construct the solution relatively quickly. This is far easier than trying
    to construct a question/answer dataset from something else, like the book *Great
    Expectations*. If you’re working with long-form text, such as a book or lengthy
    documents, you’d need to first split the text into paragraphs and manually devise
    questions for the paragraphs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 户外数据集已经格式良好，并且以小型的问答块形式呈现。借助Transformers的力量，我们可以快速地使用现成的工具和模型构建解决方案。这比尝试从其他内容（如《伟大的期望》这本书）中构建问答数据集要容易得多。如果你正在处理长篇文本，例如书籍或长篇文档，你首先需要将文本分割成段落，并手动为这些段落设计问题。
- en: Golden sets and silver sets
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金集和银集
- en: In machine learning, a *golden set* is an accurately labeled dataset that is
    used to train, test, and validate models. We treat golden sets as highly valuable
    assets, since gathering them often requires significant manual effort. The accuracy
    and usability of a trained model are limited by the accuracy and breadth of the
    golden set. Thus, the longer you spend growing and verifying your golden set,
    the better the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*金集*是一个准确标记的数据集，用于训练、测试和验证模型。我们将金集视为高度宝贵的资产，因为收集它们通常需要大量的手动工作。训练模型的准确性和可用性受限于金集的准确性和广度。因此，你花费更多时间来增长和验证你的金集，模型就会越好。
- en: To reduce some of the effort required in labeling data, we can save time by
    letting a machine try to generate a labeled dataset for us. This automatically
    generated set of labeled data is called a *silver set*, and it prevents us from
    having to start from scratch.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少在标记数据时所需的一些努力，我们可以通过让机器尝试为我们生成标记数据集来节省时间。这个自动生成的标记数据集被称为*银集*，它可以防止我们必须从头开始。
- en: A silver set is not as trustworthy as a golden set. Since we automatically obtain
    a silver set through machine-automated processes that aren’t as accurate as humans,
    there will be mistakes. Thus, a silver set should ideally be improved with a manual
    audit and corrections to increase its accuracy. Using silver sets to bootstrap
    your training dataset can save a lot of time and mental effort in the long term,
    and it can help you scale your training data curation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 银集不如金集可信。由于我们通过机器自动化的过程自动获得银集，而这些过程不如人类准确，因此会有错误。因此，理想情况下，应该通过手动审计和纠正来提高其准确性。使用银集来启动你的训练数据集可以在长期内节省大量时间和精神努力，并有助于你扩展你的训练数据整理。
- en: 14.2.1 Gathering and cleaning a question-answering dataset
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.1 收集和清理问答数据集
- en: 'On to our first step: let’s construct a dataset we can label and use to train
    a model. For this dataset, we need questions with associated contexts that contain
    their answers. Listing 14.4 shows how to get the questions and the contexts that
    contain answers in rows of a pandas dataframe. We need to construct two queries:
    one to get the community questions, and one to get the accepted community answers
    to those questions. We will only be using question/answer pairs that have an accepted
    answer. We will execute the two queries separately and join the two together.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的第一步：让我们构建一个可以标记并用于训练模型的数据库集。对于这个数据集，我们需要包含答案的相关上下文的问题。列表14.4展示了如何在pandas数据框的行中获取问题和包含答案的上下文。我们需要构建两个查询：一个用于获取社区问题，另一个用于获取这些问题的被接受社区答案。我们只将使用有被接受答案的问题/答案对。我们将分别执行这两个查询并将它们合并在一起。
- en: The model that we are using refers to the content from which we pluck our answer
    as a context. Remember, we’re not generating answers, we’re just finding the most
    appropriate answer inside a body of text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所使用的模型指的是从中提取答案作为上下文的内容。记住，我们不是在生成答案，我们只是在文本体中找到最合适的答案。
- en: Listing 14.4 Pulling training questions from Solr
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.4 从Solr提取训练问题
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Narrows the scope of question types that we retrieve'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 窄化我们检索的问题类型范围'
- en: '#2 Only retrieves questions that have accepted answers'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 只检索有已接受答案的问题'
- en: '#3 Only uses titles starting with a question type'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 只使用以问题类型开头的标题'
- en: 'With the list of questions from listing 14.4, we next need to get contexts
    associated with each question. Listing 14.5 returns a dataframe with the following
    columns: `id`, `url`, `question`, and `context`. We’ll use the `question` and
    `context` to generate the training and evaluation data for our question-answering
    model in the coming sections.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表14.4中的问题列表之后，我们接下来需要获取与每个问题相关联的上下文。列表14.5返回一个包含以下列的数据框：`id`、`url`、`question`和`context`。我们将使用`question`和`context`在接下来的章节中为我们的问答模型生成训练和评估数据。
- en: Listing 14.5 Searching for accepted answer contexts
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.5 搜索已接受答案的上下文
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Gets a list of all distinct answer ids'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取所有不同答案ID的列表'
- en: '#2 Calculates the number of search requests that need to be made'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算需要进行的搜索请求数量'
- en: '#3 Aggregates all answers'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 聚合所有答案'
- en: '#4 Retrieves all answer data for questions'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取所有问题的答案数据'
- en: '#5 Load the questions from listing 14.4.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 加载列表14.4中的问题。'
- en: '#6 Load the contexts for each question.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 加载每个问题的上下文。'
- en: 'Output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We encourage you to examine the full output for the question and context pairs
    to appreciate the variety of input and language used. We also included the original
    URL if you’d like to visit the Stack Exchange outdoors website and explore the
    source data yourself in the Jupyter notebook.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您检查问题和上下文对的完整输出，以欣赏所使用的各种输入和语言。如果您想访问Stack Exchange户外网站并自行在Jupyter笔记本中探索源数据，我们还包含了原始URL。
- en: '14.2.2 Creating the silver set: Automatically labeling data from a pretrained
    model'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.2 创建银集：从预训练模型自动标记数据
- en: 'Now that we have our dataset, we have to label it. For the training to work,
    we need to tell the model what the correct answer is inside of the context (document),
    given the question. An LLM exists that already does a decent job at selecting
    answers: `deepset/roberta-base-squad2`. This model was pretrained by the Deepset
    company using the SQuAD2 dataset and is freely available on their Hugging Face
    page ([https://huggingface.co/deepset](https://huggingface.co/deepset)). SQuAD
    is the *Stanford Question Answering Dataset*, which is a large public dataset
    made up of thousands of question and answer pairs. The Deepset team started with
    the RoBERTa architecture (covered in chapter 13) and fine-tuned a model based
    on this dataset for the task of question answering.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的数据集，我们必须对其进行标记。为了使训练工作，我们需要告诉模型在给定问题的上下文（文档）中正确的答案是什么。存在一个LLM（大型语言模型）已经在这方面做得相当不错：`deepset/roberta-base-squad2`。这个模型是由Deepset公司使用SQuAD2数据集预训练的，并在他们的Hugging
    Face页面上免费提供（[https://huggingface.co/deepset](https://huggingface.co/deepset)）。SQuAD是*斯坦福问答数据集*，这是一个由成千上万的问答对组成的大型公共数据集。Deepset团队从第13章中介绍的RoBERTa架构开始，并基于此数据集对模型进行微调，以完成问答任务。
- en: NOTE  It’s a good idea to familiarize yourself with the Hugging Face website
    ([https://huggingface.co](https://huggingface.co)). The Hugging Face community
    is very active and has provided thousands of free pretrained models, available
    for use by anyone.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：熟悉Hugging Face网站（[https://huggingface.co](https://huggingface.co)）是个好主意。Hugging
    Face社区非常活跃，已经提供了数千个免费预训练模型，任何人都可以使用。
- en: Our strategy is to use the best pretrained model available to attempt to answer
    all the questions first. We’ll call these answers “guesses” and the entire automatically
    labeled dataset the “silver set”. Then we will go through the silver set guesses
    and correct them ourselves, to get a “golden set”.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略是使用可用的最佳预训练模型首先尝试回答所有问题。我们将这些答案称为“猜测”，整个自动标记的数据集称为“银集”。然后我们将遍历银集猜测并自行纠正它们，以获得“金集”。
- en: Listing 14.6 shows our question-answering function, which uses a Transformers
    pipeline type of `question-answering` and the `deepset/roberta-base-squad2` model.
    We use these to construct a pipeline with an appropriate tokenizer and target
    device (either CPU or GPU). This gives us everything we need to pass in raw data
    and obtain the silver set, as illustrated in figure 14.5.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.6 展示了我们的问答函数，该函数使用 `question-answering` 类型的 Transformers 流程和 `deepset/roberta-base-squad2`
    模型。我们使用这些来构建一个包含适当的分词器和目标设备（CPU 或 GPU）的流程。这为我们提供了将原始数据传入并获取银牌集合所需的一切，如图 14.5 所示。
- en: '![figure](../Images/CH14_F05_Grainger.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F05_Grainger.png)'
- en: Figure 14.5 Obtaining the silver set and golden set from a prepared dataframe
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.5 从准备好的数据框中获取银牌集合和金牌集合。
- en: In Python, we create a function called `answer_questions` that accepts the list
    of contexts that we extracted from our retriever. This function runs each question
    and context through the pipeline to generate the answer and appends it to the
    list. We won’t presume that they’re actually answers at this point, because many
    of them will be incorrect (as you’ll see when you open the file). We will only
    count something as an *answer* when it’s been vetted by a person. This is the
    nature of upgrading a silver set to a golden set.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们创建了一个名为 `answer_questions` 的函数，该函数接受我们从检索器中提取的上下文列表。该函数将每个问题和上下文通过流程运行以生成答案，并将其追加到列表中。我们不会假设它们实际上是答案，因为其中许多将是错误的（正如你打开文件时将看到的）。我们只有在经过人工审核后才会将某些内容计为
    *答案*。这是将银牌集合升级为金牌集合的本质。
- en: The `device` (CPU or GPU) will be chosen automatically based on whether you
    have a GPU available to your Docker environment or not. Now is a good time to
    mention that if you’re running or training these models on a CPU-only home computer
    or Docker configuration, you may be waiting a while for the inference of all the
    data to be complete. If you’re not using a GPU, feel free to skip running listings
    14.6–14.7, as we have already provided the output needed to run later listings
    in this notebook’s dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`device`（CPU 或 GPU）将根据你的 Docker 环境中是否有 GPU 自动选择。现在是一个好时机来提到，如果你在仅使用 CPU 的家用电脑或
    Docker 配置上运行或训练这些模型，你可能需要等待一段时间才能完成所有数据的推理。如果你没有使用 GPU，你可以自由地跳过运行列表 14.6–14.7，因为我们已经提供了运行此笔记本数据集中后续列表所需的输出。'
- en: Listing 14.6 generates the silver set to extract out the most likely answers
    for our pairs of question and accepted answer contexts loaded previously in listing
    14.5.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.6 生成银牌集合，以提取出我们之前在列表 14.5 中加载的问题和接受答案上下文对中最可能的答案。
- en: Listing 14.6 Generating answers given question/context pairs
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.6 根据问题/上下文对生成答案。
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 This is the pipeline that we illustrated in figure 14.1.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这是我们在图 14.1 中展示的流程。'
- en: '#2 tqdm prints the progress of the operation as a progress bar.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 tqdm 以进度条的形式打印操作进度。'
- en: '#3 Process with GPU (CUDA) if available; otherwise use the CPU.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果可用，使用 GPU (CUDA) 处理；否则使用 CPU。'
- en: '#4 This is the pipeline that we illustrated in figure 14.1.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这是我们在图 14.1 中展示的流程。'
- en: '#5 tqdm prints the progress of the operation as a progress bar.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 tqdm 以进度条的形式打印操作进度。'
- en: '#6 Gets the answer (and confidence score) for every question/context pair'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 为每个问题/上下文对获取答案（和置信度分数）。'
- en: '#7 Process with GPU (CUDA) if available; otherwise use the CPU.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 如果可用，使用 GPU (CUDA) 处理；否则使用 CPU。'
- en: 'Output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Congratulations, we have now obtained the silver set! In the next section, we’ll
    improve it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们现在已经获得了银牌集合！在下一节中，我们将对其进行改进。
- en: GPU recommended
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推荐使用 GPU
- en: Feel free to run these listings on your personal computer, but be warned—some
    of them take a while on a CPU. The total execution time for listing 14.6, for
    example, was reduced by about 20 times when running on a GPU versus a mid-market
    CPU during our tests.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随意将这些列表运行在你的个人电脑上，但请注意——其中一些在 CPU 上可能需要一段时间。例如，列表 14.6 的总执行时间在我们的测试中，当在 GPU
    上运行时比在中端 CPU 上运行减少了大约 20 倍。
- en: Note that the fine-tuning examples later in the chapter will benefit significantly
    from having a GPU available. If you cannot access a GPU for those listings, that’s
    okay—we’ve trained the model and already included it for you as part of the outdoors
    dataset. You can follow along in the listings to see how the model is trained,
    and if you don’t have a GPU available, you can just skip running them. You can
    also use free services such as Google Colab or rent a server with a GPU from a
    cloud provider, which is typically a few US dollars per hour.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章后面的微调示例将显著受益于拥有GPU。如果你无法访问GPU来运行这些列表，那没关系——我们已经训练了模型，并将其作为户外数据集的一部分包含在内。你可以跟随列表来了解模型是如何训练的，如果你没有GPU可用，你可以直接跳过运行它们。你也可以使用像Google
    Colab这样的免费服务，或者从云服务提供商那里租用带有GPU的服务器，这通常每小时只需几美元。
- en: If you’re interested in learning more about GPUs and why they are better suited
    to tasks such as training models, we recommend checking out *Parallel and High
    Performance Computing* by Robert Robey and Yuliana Zamora (Manning, 2021).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要了解更多关于GPU以及为什么它们更适合像训练模型这样的任务，我们推荐阅读Robert Robey和Yuliana Zamora的《并行与高性能计算》（Manning,
    2021）。
- en: '14.2.3 Human-in-the-loop training: Manually correcting the silver set to produce
    a golden set'
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.3 人机交互训练：手动纠正银色集以生成黄金集
- en: The silver set CSV file (question-answering-squad2-guesses.csv) is used as a
    first pass at attempting to answer the questions. We’ll use this with human-in-the-loop
    manual correction and labeling of the data to refine the silver set into the golden
    set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 银色集CSV文件（question-answering-squad2-guesses.csv）被用作尝试回答问题的初步尝试。我们将使用它，结合人工介入的手动纠正和标记数据，将银色集精炼成黄金集。
- en: NOTE  No Python code can generate a golden set for you. The data *must* be labeled
    by an intelligent person (or perhaps one day an AI model highly optimized for
    making relevance judgments) with an understanding of the domain. All further listings
    will use this golden set. We are giving you a break, though, since we already
    labeled the data for you. For reference, it took 4 to 6 hours to label about 200
    guesses produced by the `deepset/roberta-base -squad2` model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：没有任何Python代码可以为你生成黄金集。数据*必须*由一个了解该领域的人（或者也许有一天是一个高度优化于进行相关性判断的AI模型）进行标记。所有后续列表都将使用这个黄金集。不过，我们已经为你标记了数据，给你一个喘息的机会。为了参考，标记大约200个由`deepset/roberta-base
    -squad2`模型产生的猜测大约需要4到6个小时。
- en: Labeling data yourself will give you a deeper appreciation for the difficulty
    of this NLP task. We *highly* encourage you to label even more documents and rerun
    the fine-tuning tasks coming up. Having an appreciation for the effort needed
    to obtain quality data, and the effect it has on the model accuracy, is a lesson
    you can only learn through experience.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 自己标记数据将使你更深刻地体会到这个NLP任务的难度。我们**强烈**鼓励你标记更多文档并重新运行即将到来的微调任务。理解获取高质量数据所需的努力以及它对模型准确性的影响，这是只有通过经验才能学到的教训。
- en: Before diving in and just labeling data, however, we need to have a plan for
    how and what to label. For each row, we need to classify it and, if necessary,
    write the correct answer ourselves into another column.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在开始标记数据之前，我们需要有一个如何以及标记什么的计划。对于每一行，我们需要对其进行分类，并在必要时将正确的答案我们自己写入另一列。
- en: 'Here is the key, as shown in figure 14.6, which we used for all the labeled
    rows in the `class` field:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键，如图14.6所示，这是我们用于`class`字段中所有标记行的标签：
- en: '`-2` = This is a negative example (an example where we know the guess is wrong!).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-2` = 这是一个负面示例（一个我们知道猜测是错误的示例）。'
- en: '`-1` = Ignore this question, as it is too vague or we are missing some information.
    For example, `What is this bird?` We can’t answer without a picture of the bird,
    so we don’t even try.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-1` = 忽略这个问题，因为它太模糊或者我们缺少一些信息。例如，`这是什么鸟？`没有鸟的图片，我们无法回答，所以我们甚至不尝试。'
- en: '`0` = This is an example that has been corrected by a person to highlight a
    better answer span in the same context. The guess given by `deepset/roberta-base
    -squad2` was incorrect or incomplete, so we changed it.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` = 这是一个由人纠正的示例，以突出同一上下文中更好的答案范围。`deepset/roberta-base -squad2`给出的猜测是不正确或不完整的，所以我们更改了它。'
- en: '`1` = This is an example that was given a correct answer by `deepset/roberta
    -base-squad2`, so we did not change the answer.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` = 这是一个由`deepset/roberta-base -base-squad2`给出正确答案的示例，所以我们没有更改答案。'
- en: (blank) = We didn’t check this row, so we’ll ignore it.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （空白）= 我们没有检查这一行，所以我们将忽略它。
- en: '![figure](../Images/CH14_F06_Grainger.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F06_Grainger.png)'
- en: Figure 14.6 Legend of label classes
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.6标签类别的图例
- en: You should open the outdoors_golden_answers.csv file and look through the rows
    yourself. Understand the ratio of questions that we labeled as `0` and `1`. You
    can even try opening the file in pandas and doing a little bit of analysis to
    familiarize yourself with the golden set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该打开outdoors_golden_answers.csv文件，亲自查看行。理解我们标注为`0`和`1`的问题比例。你甚至可以尝试在pandas中打开文件，进行一些分析，以便熟悉黄金集。
- en: 14.2.4 Formatting the golden set for training, testing, and validation
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.4 格式化训练、测试和验证的黄金集
- en: Now that we have labeled data, we’re almost ready to train our model, but first
    we need to get the data into the right format for the training and evaluation
    pipeline. Once our data is in the right format, we’ll also need to split it into
    training, testing, and validation sets for use when training the model to ensure
    it does not overfit our data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了标注数据，我们几乎准备好训练我们的模型了，但首先我们需要将数据格式化为训练和评估流程的正确格式。一旦我们的数据格式正确，我们还需要将其分成训练、测试和验证集，以确保在训练模型时不会过拟合我们的数据。
- en: Converting the labeled data into a standardized data format
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将标注数据转换为标准化数据格式
- en: Hugging Face provides a library called `datasets`, which we’ll use to prepare
    our data. The `datasets` library can accept the names of many publicly available
    datasets and provide a standard interface for working with them. The SQuAD2 dataset
    is one of the available datasets, but since our golden set is in a custom format,
    we first need to convert it to the standardized `datasets` configuration format
    shown in the following listing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face提供了一个名为`datasets`的库，我们将使用它来准备我们的数据。`datasets`库可以接受许多公开可用的数据集的名称，并提供一个用于处理它们的标准化接口。SQuAD2数据集是可用的数据集之一，但由于我们的黄金集是自定义格式，我们首先需要将其转换为以下列表中所示的标准化`datasets`配置格式。
- en: Listing 14.7 Transforming a golden dataset into SQuAD formatting
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.7将黄金数据集转换为SQuAD格式
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Randomly sorts all the examples'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 随机排序所有示例'
- en: '#2 75% of the examples will be used for training. This will give us 125 training
    samples.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 75%的示例将用于训练。这将给我们125个训练样本。'
- en: '#3 20% of the examples will be used for testing. We subtract 1 from train_split
    to allow for 125/32/10 records on the three splits.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 20%的示例将用于测试。我们从train_split中减去1，以便在三个分割上有125/32/10条记录。'
- en: '#4 The remaining 5% of the examples will be used for validation holdout. This
    will be 10 samples.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 剩余的5%的示例将用于验证保留。这将是有10个样本。'
- en: '#5 SQuAD requires three groups of data: train, test, and validation'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 SQuAD需要三组数据：训练、测试和验证'
- en: The first part of the function in listing 14.7 loads the CSV into a pandas dataframe
    and does some preprocessing and formatting. Once formatted, the data is split
    up into three parts and converted.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 14.7列表中的函数的第一部分将CSV加载到pandas数据框中，并进行一些预处理和格式化。一旦格式化，数据就被分成三部分并转换。
- en: The object returned and saved from listing 14.7 is a dataset dictionary (a `datadict`)
    that contains our three slices for training, testing, and validation. For our
    data table, with the split defined in `get_training_data`, we have 125 training
    examples, 32 testing examples, and 10 validation examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从14.7列表返回并保存的对象是一个数据集字典（一个`datadict`），它包含我们的三个训练、测试和验证部分。对于我们的数据表，在`get_training_data`中定义的分割下，我们有125个训练示例、32个测试示例和10个验证示例。
- en: Avoiding overfitting with a test set and holdout validation set
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用测试集和保留的验证集避免过拟合
- en: '*Overfitting* a model means you trained it to only memorize the training examples
    provided. This means it won’t generalize well enough to handle previously unseen
    data.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*过拟合*一个模型意味着你训练它只记住提供的训练示例。这意味着它无法很好地泛化以处理之前未见过的数据。'
- en: To prevent overfitting, we needed to split our dataset into separate training,
    testing, and validation slices, as we did in listing 14.7\. The testing and the
    holdout validation sets are used to measure the success of the model after it
    is trained. After you’ve gone through the process from end to end, consider labeling
    some more data and doing different splits across the training, testing, and validation
    slices to see how the model performs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，我们需要将我们的数据集分成独立的训练、测试和验证部分，就像在14.7列表中所做的那样。测试和保留的验证集用于在模型训练后衡量其成功程度。当你从头到尾完成这个过程后，考虑标注更多数据，并在训练、测试和验证部分进行不同的分割，以查看模型的性能。
- en: We use a training/test split to give some data to the model training and some
    to test the outcome. We iteratively tune hyperparameters for the model training
    to achieve higher accuracy (measured using a loss function) when the model is
    applied to the test set.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练/测试分割来给模型训练提供一些数据，并给测试结果提供一些数据。我们迭代调整模型训练的超参数，以提高当模型应用于测试集时的准确性（使用损失函数来衡量）。
- en: The holdout validation set is the real-world proxy for unseen data, and it’s
    not checked until the end. After training and testing are completed, you then
    validate the final model version by applying it against the holdout examples.
    If this score is much lower than the final test accuracy, you’ve overfit your
    model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 保留验证集是未见数据的现实世界代理，它直到最后才被检查。在训练和测试完成后，你通过将最终模型版本应用于保留示例来验证最终模型版本。如果这个分数远低于最终测试精度，那么你的模型已经过拟合。
- en: NOTE  The number of examples we’re using is quite small (125 training examples,
    32 testing examples, and 10 holdout validation examples) compared to what you’d
    use for fine-tuning data for a customer-facing system. As a general rule of thumb,
    aim for about 500 to 2,000 labeled examples. Sometimes you can get away with less,
    but typically the more you have the better. This will take considerable time investment,
    but it’s well worth the effort.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：我们使用的示例数量相当少（125个训练示例，32个测试示例和10个保留验证示例），与用于客户系统数据微调相比。一般来说，目标是大约500到2000个标记的示例。有时你可以用更少的示例完成，但通常示例越多越好。这将需要相当多的时间投入，但这是值得的。
- en: 14.3 Fine-tuning the question-answering model
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 调整问答模型
- en: We’ll now walk through obtaining a better model by fine-tuning the existing
    `deepset/roberta-base-squad2` model with our golden set.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过使用我们的黄金集微调现有的`deepset/roberta-base-squad2`模型来获得更好的模型。
- en: Unfortunately, this next notebook can be quite slow to run on a CPU. If you
    are going through the listings on a machine that is CUDA-capable and can configure
    your Docker environment to use the GPUs, then you should be all set! Otherwise,
    we recommend you use a service like Google Colab, which offers easy running of
    Jupyter notebooks on GPUs at no cost, or another cloud computing or hosting provider
    that has a CUDA device ready to go. You can load the notebook directly from Google
    Colab and run it without any other dependencies aside from our dataset. A link
    is provided above listing 14.8 in the associated notebook.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个下一个笔记本在CPU上运行可能会相当慢。如果你正在使用一台具有CUDA功能并且可以配置Docker环境以使用GPU的机器上查看列表，那么你应该一切就绪！否则，我们建议你使用像Google
    Colab这样的服务，它提供免费运行Jupyter笔记本在GPU上的服务，或者另一个已经准备好CUDA设备的云计算或托管提供商。你可以直接从Google Colab加载笔记本并运行，除了我们的数据集外，无需其他依赖。在相关笔记本中14.8列表上方提供了一个链接。
- en: TIP  As we noted previously, if you don’t want to go through the hassle of setting
    up a GPU-compatible environment, you can also follow along with listings 14.8–14.13
    without running them, since we have already trained the model and included it
    for you to use. However, if you can, we do encourage you to go through the effort
    of getting GPU access and training the model yourself to see how the process works
    and to enable you to tinker with the hyperparameters. Figure 4.7 shows the kind
    of speedup that GPUs can provide for massively parallel computations like language
    model training.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：正如我们之前提到的，如果你不想麻烦设置一个兼容GPU的环境，你也可以在不运行它们的情况下跟随14.8-14.13列表，因为我们已经训练了模型并为你提供了使用。然而，如果你能的话，我们确实鼓励你努力获取GPU访问并自己训练模型，以便了解这个过程是如何工作的，并使你能够调整超参数。图4.7显示了GPU可以为语言模型训练等巨大并行计算提供什么样的加速。
- en: '![figure](../Images/CH14_F07_Grainger.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F07_Grainger.png)'
- en: Figure 14.7 A V100 GPU (commonly available with cloud providers) has 640 tensor
    compute cores, compared to a 4-core x86-64 CPU. A Tesla T4 has 2,560 tensor compute
    cores. Individually, CPU cores are more powerful, but most GPUs have two to three
    orders of magnitude more cores than a CPU. This is important when doing massively
    parallel computations for millions of model parameters.
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.7 一个V100 GPU（通常与云提供商一起提供）有640个张量计算核心，而一个4核心x86-64 CPU有4个核心。单个CPU核心更强大，但大多数GPU的核心数量比CPU多两个到三个数量级。在进行数百万个模型参数的巨大并行计算时，这一点很重要。
- en: The first thing we need to do is require access to the GPU device. The code
    in the following listing will initialize and return the device ID of the available
    processor. If a GPU is configured and available, we should see that device’s ID.
    If you are using Colab and having any problems with listing 14.8, you may need
    to change the runtime type to `GPU` in the settings.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是要求访问GPU设备。以下列表中的代码将初始化并返回可用处理器的设备ID。如果配置了GPU并且可用，我们应该看到该设备的ID。如果你在使用Colab并且有任何关于列表14.8的问题，你可能需要在设置中将运行时类型更改为`GPU`。
- en: Listing 14.8 Detecting and initializing a GPU device
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.8 检测和初始化GPU设备
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have a GPU (in this listing output, at least). In the response, `device(type='cuda',`
    `index=0)` is what we were looking for. If a GPU isn’t available when you run
    the listing, `device(type='cpu')` will be returned instead, indicating that the
    CPU will be used for processing. If you have more than one capable device available
    to the notebook, it will list each of them with an incrementing numerical id.
    You can access the device later in training by specifying an `id` (in our case,
    `0`).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个GPU（在这个列表输出中至少有一个）。在响应中，`device(type='cuda', index=0)`是我们所寻找的。如果你在运行列表时没有可用的GPU，将返回`device(type='cpu')`，表示将使用CPU进行处理。如果你有多个可用的设备供笔记本使用，它将按递增的数字ID列出每个设备。你可以在训练过程中通过指定一个`id`（在我们的情况下，`0`）来访问设备。
- en: With our device ready to go, we will next load and tokenize our previously labeled
    dataset from listing 14.7.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设备准备就绪后，我们将加载并标记化我们在列表14.7中预先标记的数据集。
- en: 14.3.1 Tokenizing and shaping our labeled data
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 标记化和塑造我们的标记数据
- en: The model trainer doesn’t recognize words; it recognizes tokens that exist in
    the RoBERTa vocabulary. We covered tokenization in chapter 13, where we used it
    as an initial step when encoding documents and queries into dense vectors for
    semantic search. Similarly, we need to tokenize our question-answering dataset
    before we can use it to train the model. The model accepts token values as the
    input parameters, just like any other Transformer model.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练器不识别单词；它识别存在于RoBERTa词汇表中的标记。我们在第13章中介绍了标记化，当时我们将其用作将文档和查询编码为密集向量以进行语义搜索的初始步骤。同样，在我们使用它来训练模型之前，我们需要对问答数据集进行标记化。模型接受标记值作为输入参数，就像任何其他Transformer模型一样。
- en: The following listing shows how we’ll tokenize the data prior to model training.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了我们在模型训练之前如何标记数据。
- en: Listing 14.9 Tokenizing our training set
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.9 标记化我们的训练集
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Loads the datadict we created in listing 14.7 from our golden set'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从我们的黄金集加载我们在列表14.7中创建的datadict'
- en: '#2 Loads a pretrained tokenizer (roberta-base)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 加载预训练的标记器（roberta-base）'
- en: '#3 This will be the number of tokens in both the question and context.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这将是问题和上下文中标记的数量。'
- en: '#4 Sometimes we need to split the context into smaller chunks, so these chunks
    will overlap by this many tokens.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 有时我们需要将上下文拆分成更小的块，因此这些块将重叠这么多标记。'
- en: '#5 Add padding tokens to the end for question/context pairs that are shorter
    than the model input size.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 为比模型输入大小短的问答对添加填充标记。'
- en: '#6 Performs the tokenization for each of the examples'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 对每个示例执行标记化'
- en: '#7 Additional processing to identify start and end positions for questions
    and contexts. See the notebook for the full algorithm.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 对问题和中进行额外的处理以识别开始和结束位置。请参阅笔记本以获取完整的算法。'
- en: '#8 Invokes the tokenizer on each example in our golden dataset'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 在我们的黄金数据集的每个示例上调用标记器'
- en: We load a tokenizer (trained on the `roberta-base` model), load our `question
    -answering-training-set` golden set from disk (data/question-answering/question
    -answering-training-set/), and then run the examples from the golden set through
    the tokenizer to generate a `tokenized_datasets` object with the training and
    test datasets we’ll soon pass to the model trainer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载一个标记器（在`roberta-base`模型上训练），从磁盘加载我们的`question-answering-training-set`黄金集（data/question-answering/question-answering-training-set/），然后运行黄金集中的示例通过标记器以生成一个`tokenized_datasets`对象，我们将很快将其传递给模型训练器。
- en: For each context, we generate a list of tensors with a specific number of embeddings
    per tensor and a specific number of floats per embedding. The shape of the tensors
    containing the tokens must be the same for all the examples we provide to the
    trainer and evaluator. We accomplish this with a window-sliding technique.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个上下文，我们为每个张量生成一个具有特定数量的嵌入和每个嵌入特定数量的浮点数的张量列表。包含标记的张量的形状必须与我们提供给训练器和评估器的所有示例中的形状相同。我们通过窗口滑动技术实现这一点。
- en: '*Window sliding* is a technique that involves splitting a long list of tokens
    into many sublists of tokens, but where each sublist after the first shares an
    overlapping number of tokens with the previous sublist. In our case, `maximum_tokens`
    defines the size of each sublist, and `document_overlap` defines the overlap.
    This window-sliding process is demonstrated in figure 14.8.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*窗口滑动*是一种技术，它涉及将长列表的token分割成许多子列表，但每个子列表在第一个之后都与前一个子列表共享一定数量的重叠token。在我们的例子中，`maximum_tokens`定义了每个子列表的大小，而`document_overlap`定义了重叠。这种窗口滑动过程在图14.8中得到了展示。'
- en: Figure 14.8 demonstrates very small `maximum_tokens` (`24`) and `document_overlap`
    (`8`) numbers for illustration purposes, but the real tokenization process splits
    the contexts into tensors of `384` tokens with an overlap of `128`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8展示了为了说明目的而设置的非常小的`maximum_tokens`（`24`）和`document_overlap`（`8`）数值，但实际的分词过程将上下文分割成具有`128`重叠的`384`个token的张量。
- en: The window-sliding technique also makes use of *padding* to ensure that each
    tensor is the same length. If the number of tokens in the last tensor of the context
    is less than the maximum (`384`), then the rest of the positions in the tensor
    are filled with an empty marker token so that the final tensor size is also `384`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口滑动技术还利用*填充*来确保每个张量具有相同的长度。如果上下文最后一个张量中的token数量少于最大值（`384`），则将剩余的张量位置用空标记token填充，以确保最终张量的大小也是`384`。
- en: '![figure](../Images/CH14_F08_Grainger.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F08_Grainger.png)'
- en: Figure 14.8 Visualizing the sliding window technique that splits one context
    into tensors of the same shape
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.8 展示了将一个上下文分割成相同形状的张量的滑动窗口技术
- en: Knowing how the contexts are processed is important, as it can affect both accuracy
    and processing time. If we’re trying to identify answers in lengthy documents,
    the window-sliding process may reduce accuracy, particularly if the `maximum_tokens`
    and `document_overlap` are small and thus fragment the context too much. Long
    documents will also get sliced up into multiple tensors that collectively take
    longer to process. Most of the contexts in the outdoors dataset fit into the maximums
    we specified, but these trade-offs are important to consider in other datasets
    when choosing your `maximum_tokens` and `document_overlap` parameters.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 了解上下文是如何处理的很重要，因为它会影响准确性和处理时间。如果我们试图在长文档中识别答案，窗口滑动过程可能会降低准确性，尤其是如果`maximum_tokens`和`document_overlap`很小，从而过多地分割上下文。长文档也会被切割成多个张量，这些张量共同处理需要更长的时间。户外数据集中的大多数上下文都符合我们指定的最大值，但在选择`maximum_tokens`和`document_overlap`参数时，在其他数据集中考虑这些权衡是很重要的。
- en: 14.3.2 Configuring the model trainer
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.2 配置模型训练器
- en: 'We have one last step before we train our model: we need to specify *how* training
    and evaluation will happen.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之前，我们还需要进行最后一步：我们需要指定训练和评估将如何进行。
- en: 'When training our model, we need to specify the base model and training arguments
    (hyperparameters), as well as our training and testing datasets. You’ll want to
    understand the following key concepts when configuring the hyperparameters for
    the model trainer:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的模型时，我们需要指定基础模型和训练参数（超参数），以及我们的训练和测试数据集。在配置模型训练器的超参数时，您需要了解以下关键概念：
- en: '*Epochs*—The number of times the trainer will iterate over the dataset. More
    epochs help reinforce the context and reduce loss over time. Having too many epochs
    will likely overfit your model, though, and 3 epochs is a common choice when fine-tuning
    Transformers.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轮数*——训练器将遍历数据集的次数。更多的轮数有助于随着时间的推移加强上下文并减少损失。然而，过多的轮数可能会导致模型过拟合，当微调Transformers时，3轮是一个常见的选择。'
- en: '*Batch sizes*—The number of examples that will be trained/evaluated at once.
    A higher batch size might produce a better model. This setting is constrained
    by GPU core count and available memory, but common practice is to fit as much
    in a batch as possible to make the most of the available resources.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*批大小*——一次训练/评估的示例数量。较大的批大小可能会产生更好的模型。这个设置受GPU核心数量和可用内存的限制，但常见的做法是尽可能多地放入一个批次，以充分利用可用资源。'
- en: '*Warmups*—When training a model, it can be helpful to slowly tune the model
    initially, so that the early examples don’t have an undue influence on the model’s
    learned parameters. Warmup steps allow gradual improvements to the model (the
    *learning rate*), which helps prevent the trainer from overfitting on early examples.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预热*——在训练模型时，最初缓慢调整模型可能有所帮助，这样早期的示例就不会对模型学习到的参数产生过度影响。预热步骤允许模型（*学习率*）逐渐改进，这有助于防止训练器在早期示例上过度拟合。'
- en: '*Decay*—Weight decay is used to reduce overfitting by multiplying each weight
    by this constant value at each step. It is common to use 0.01 as the weight decay,
    but this can be changed to a higher value if the model is quickly overfitting
    or to a lower value if you don’t see improvement fast enough.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*衰减*——权重衰减通过在每一步将每个权重乘以这个常数值来减少过拟合。通常使用0.01作为权重衰减，但如果模型快速过拟合，可以将其更改为更高的值；如果看不到足够的改进，可以将其更改为更低的值。'
- en: Listing 4.10 demonstrates configuring the model trainer. The hyperparameters
    (`training_args`) we’ve specified in the listing are those used by SQuAD2 by default,
    but feel free to adjust any of them to see how it improves the quality of the
    question-answering model for your own questions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.10展示了配置模型训练器。列表中我们指定的超参数（`training_args`）是SQuAD2默认使用的，但你可以随意调整它们以查看它们如何提高你自己的问答模型的质量。
- en: When trying to choose the best settings, a common technique is to perform a
    grid search over these hyperparameters. A *grid search* is a process that automatically
    iterates over parameter values and tests how adjusting each of them in different
    combinations improves the quality of the trained models. We include a grid search
    example in the accompanying notebooks, should you wish to dive deeper into parameter
    tuning, but for now we’ll proceed with the hyperparameters specified in listing
    14.10\.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试选择最佳设置时，一种常见的技术是在这些超参数上执行网格搜索。*网格搜索*是一个自动遍历参数值并测试调整每个参数的不同组合如何提高训练模型质量的过程。如果你希望深入了解参数调整，我们提供了配套笔记本中的网格搜索示例，但现在我们将继续使用列表14.10中指定的超参数。
- en: Listing 14.10 Initializing the trainer and its hyperparameters
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.10初始化训练器和其超参数
- en: '[PRE17]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Evaluates loss per epoch'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 评估每个epoch的损失'
- en: '#2 Total number of training epochs'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 训练epoch的总数'
- en: '#3 Batch size per device during training'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练过程中每个设备的批大小'
- en: '#4 Batch size for evaluation'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 评估的批大小'
- en: '#5 Number of warmup steps for learning rate scheduler'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 学习率调度器的预热步数'
- en: '#6 Strength of weight decay'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 权重衰减的强度'
- en: '#7 The instantiated Hugging Face Transformers model to be trained'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 实例化的Hugging Face Transformers模型用于训练'
- en: '#8 Training arguments'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 训练参数'
- en: '#9 Specifies the training dataset'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 指定训练数据集'
- en: '#10 Specifies the evaluation dataset'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 指定评估数据集'
- en: 14.3.3 Performing training and evaluating loss
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.3 执行训练和评估损失
- en: With our hyperparameters all set, it’s now time to train the model. The following
    listing runs the previously configured trainer, returns the training output showing
    the model’s performance, and saves the model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好所有超参数后，现在是时候训练模型了。以下列表运行了之前配置的训练器，返回了显示模型性能的训练输出，并保存了模型。
- en: Listing 14.11 Training and saving the model
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.11训练和保存模型
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A *loss function* is a decision function that uses the error to give a quantified
    estimate of how bad a model is. Lower loss means a higher-quality model. What
    we’re looking for is a gradual reduction in loss with each epoch, which indicates
    that the model is continuing to get better with more training. We went from a
    validation loss of `2.178` to `2.012` to `1.939` on our testing set. The numbers
    are all getting smaller at a steady rate (no huge jumps), and that’s a good sign.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*是一个决策函数，它使用误差来给出一个量化的估计，说明模型有多糟糕。损失越低，模型质量越高。我们希望看到的是损失在每个epoch中逐渐减少，这表明模型随着更多训练而持续改进。我们在测试集上的验证损失从`2.178`降至`2.012`再降至`1.939`。这些数字都在以稳定的速度减小（没有大幅跳跃），这是一个好兆头。'
- en: The overall training loss for this freshly fine-tuned model is `2.532`, and
    the validation loss on our testing set is `1.939`. Given the constraints of our
    small fine-tuning dataset and hyperparameter configuration, a validation loss
    as small as `1.939` is quite good.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新微调模型的总体训练损失为`2.532`，而在我们的测试集上的验证损失为`1.939`。考虑到我们的小微调数据集和超参数配置限制，`1.939`这样的验证损失相当不错。
- en: 14.3.4 Holdout validation and confirmation
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.4 保留验证和确认
- en: How do we know if our trained model can be used successfully for real-world
    question answering? Well, we need to test the model against our holdout validation
    dataset. Recall that the holdout validation set is the third dataset (with only
    10 examples) in our `datadict` from listing 14.9\.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道我们训练好的模型能否成功用于现实世界的问答？嗯，我们需要将模型与我们的保留验证数据集进行测试。回想一下，保留验证集是我们列表14.9中的第三个数据集（只有10个示例）。
- en: Figure 14.9 underscores the purpose of the holdout validation set. We want the
    loss from the evaluation of our holdout set to be as good as our validation loss
    of `1.939` from listing 14.11\. If our holdout loss turns out higher, that would
    be a red flag that we may have overfit! Let’s see how our model performs in the
    following listing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9强调了保留验证集的目的。我们希望我们的保留集评估的损失与列表14.11中的`1.939`验证损失一样好。如果我们的保留损失结果更高，那将是一个红旗，表明我们可能有过拟合！让我们看看我们的模型在以下列表中的表现。
- en: '![figure](../Images/CH14_F09_Grainger.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH14_F09_Grainger.png)'
- en: 'Figure 14.9 Holdout set: answering previously unseen questions with our trained
    mode'
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.9 保留集：使用我们训练好的模型回答以前未见过的问答
- en: Listing 14.12 Evaluating the trained model on the holdout examples
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.12 在保留示例上评估训练好的模型
- en: '[PRE20]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `eval_loss` of `1.785` from testing our holdout validation set looks great.
    It’s even better than the training and testing loss. This means that our model
    is working well and is likely not overfitting the training or testing data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 测试我们的保留验证集的`eval_loss`为`1.785`看起来很棒。它甚至比训练和测试损失还要好。这意味着我们的模型运行良好，并且不太可能对训练或测试数据过拟合。
- en: Feel free to continue training and improving the model, but we’ll continue with
    this as the fully trained model that we’ll integrate into the reader for our question-answering
    system.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随意继续训练和改进模型，但我们将继续使用这个作为完全训练好的模型，并将其集成到我们的问答系统的读者中。
- en: 14.4 Building the reader with the new fine-tuned model
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 使用新的微调模型构建读者
- en: Now that our reader’s model training is completed, we’ll integrate it into a
    question-answering pipeline to produce our finalized reader that can extract answers
    from questions and contexts. The following listing demonstrates how we can load
    our model into a `question-answering` pipeline provided by the `transformers`
    library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们读者的模型训练已完成，我们将将其集成到问答管道中，以生成我们最终的读者，可以从问题和上下文中提取答案。以下列表展示了我们如何将模型加载到`transformers`库提供的`question-answering`管道中。
- en: Listing 14.13 Loading the fine-tuned outdoors question-answering model
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.13 加载微调后的户外问答模型
- en: '[PRE22]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With the question-answering pipeline loaded, we’ll extract some answers from
    some question/context pairs. Let’s use our 10-document holdout validation set
    used earlier in section 14.3.4\. The holdout examples were not used to train or
    to test the model, so they should be a good litmus test for how well our model
    works in practice.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载问答管道后，我们将从一些问题/上下文对中提取一些答案。让我们使用我们在14.3.4节中早期使用的10个文档的保留验证集。保留示例未用于训练或测试模型，因此它们应该是对我们模型在实际中表现如何的良好试金石。
- en: In the following listing, we test the accuracy of our question-answering model
    on the holdout validation set examples.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们测试了我们的问答模型在保留验证集示例上的准确性。
- en: Listing 14.14 Evaluating the fine-tuned question-answering model
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.14 评估微调的问答模型
- en: '[PRE23]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE24]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Successfully extracting 7 out of 10 correct answers is an impressive result.
    Congratulations, you’ve now fine-tuned an LLM for a real-world use case! This
    completes the `reader` component of our architecture, but we still need to combine
    it with a `retriever` that finds the initial candidate contexts to pass to the
    `reader`. In the next section, we’ll incorporate the retriever (our search engine)
    to finalize the end-to-end question-answering system.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 成功提取出10个正确答案中的7个是一个令人印象深刻的成果。恭喜你，你现在已经为现实世界的用例微调了一个LLM！这完成了我们架构中的`reader`组件，但我们仍然需要将其与一个用于找到传递给`reader`的初始候选上下文的`retriever`结合。在下一节中，我们将把检索器（我们的搜索引擎）纳入，以最终确定端到端问答系统。
- en: '14.5 Incorporating the retriever: Using the question-answering model with the
    search engine'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 纳入检索器：使用问答模型与搜索引擎
- en: 'Next, we’ll implement a rerank operation using the reader confidence score
    to rank the top answers. Here’s an outline of the steps we’ll go through in this
    exercise:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个重新排序操作，使用读者置信度分数来对顶级答案进行排序。以下是我们在这次练习中将经历的步骤概述：
- en: Query the outdoors index from a search collection tuned for high recall.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从针对高召回率调优的搜索集合中查询户外索引。
- en: Pair our question with the top-*K* document results and infer answers and scores
    with the question-answering NLP inference pipeline.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的问题与顶级-*K* 文档结果配对，并使用问答 NLP 推理管道推断答案和分数。
- en: Rerank the answer predictions by descending score.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按分数降序重新排序答案预测。
- en: Return the correct answer and top results using the parts created in steps 1
    through 3\.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤 1 到 3 中创建的部分，返回正确的答案和顶级结果。
- en: See figure 14.4 for a refresher on this application flow.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图 14.4 以刷新此应用程序流程。
- en: '14.5.1 Step 1: Querying the retriever'
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5.1 步骤 1：查询检索器
- en: Our goal in the first retrieval stage is recall. Specifically, what are all
    the possibly relevant documents that may contain our answer? We rely on the already-tuned
    search collection to give us that recall so that we can pass quality documents
    into our reranking stage.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段检索中，我们的目标是召回率。具体来说，所有可能包含我们答案的相关文档是什么？我们依靠已经微调的搜索集合来提供召回率，以便我们可以将高质量的文档传递到我们的重新排序阶段。
- en: The following listing implements our `retriever` function, which can accept
    a question and return an initial list of relevant documents to consider as potential
    contexts for the answer.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表实现了我们的 `retriever` 函数，该函数可以接受一个问题并返回一个初始的相关文档列表，这些文档可能作为答案的潜在上下文进行考虑。
- en: Listing 14.15 Retriever function that searches for relevant answers
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.15 检索器函数，用于搜索相关答案
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Uses the English spaCy NLP model'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用英语 spaCy NLP 模型'
- en: '#2 Converts the question to a query by removing stop words and focusing on
    important parts of speech (see the notebook for the implementation)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过移除停用词并关注重要词性将问题转换为查询（请参阅笔记本中的实现）'
- en: '#3 Only gets answer documents (not questions)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 仅获取答案文档（不是问题）'
- en: 'Response:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE26]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: One problem we face when using the question as the query is noise. There are
    lots of documents that have the terms “who”, “what”, “when”, “where”, “why”, and
    “how”, as well as other stop words and less important parts of speech. Although
    BM25 may do a good job of deprioritizing these terms in a ranking function, we
    know those are not the key terms a user is searching for, so we remove them in
    the `get_query_from_question` function to reduce noise. We covered part of speech
    tagging with spaCy previously in chapters 5 and 13, so we won’t repeat the implementation
    here (you can find it in the notebook).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将问题作为查询时，我们面临的一个问题是噪声。有很多文档包含“谁”、“什么”、“何时”、“何地”、“为什么”和“如何”等术语，以及其他停用词和不那么重要的词性。尽管
    BM25 在排名函数中可能很好地将这些术语降级，但我们知道这些并不是用户搜索的关键术语，所以我们通过 `get_query_from_question` 函数移除它们以减少噪声。我们之前在
    5 章和 13 章中介绍了使用 spaCy 进行词性标注，所以这里不再重复实现（你可以在笔记本中找到它）。
- en: With a good set of documents returned from the search engine that may contain
    the answers to the user’s question, we can now pass those documents as contexts
    to the `reader` model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索引擎返回了可能包含用户问题答案的良好文档集之后，我们现在可以将这些文档作为上下文传递给 `reader` 模型。
- en: '14.5.2 Step 2: Inferring answers from the reader model'
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5.2 步骤 2：从读者模型中推断答案
- en: We now can use the `reader` model to infer answers to the question from each
    of the top *N* contexts. Listing 14.16 implements our generic `reader` interface,
    which accepts the output from the `retriever` from step 1\. The model and pipeline
    loading for the `retriever` follow the same process as in listing 14.13, while
    the rest of the `reader` implementation specifically handles generating candidate
    answers (along with scores for each answer) from the passed-in contexts.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `reader` 模型从每个顶级 *N* 环境中推断问题的答案。列表 14.16 实现了我们的通用 `reader` 接口，该接口接受步骤
    1 中 `retriever` 的输出。`retriever` 的模型和管道加载过程与列表 14.13 中的过程相同，而 `reader` 的其余实现专门处理从传入的上下文中生成候选答案（以及每个答案的分数）。
- en: Listing 14.16 Reader function that incorporates our fine-tuned model
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.16 读者函数，结合我们的微调模型
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Creates a spaCy pipeline using our fine-tuned model'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用我们的微调模型创建 spaCy 管道'
- en: '#2 Invokes the reader pipeline to extract a candidate answer from each context'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用读者管道从每个上下文中提取一个候选答案'
- en: '#3 Returns additional metadata about where each answer was found'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 返回每个答案找到的附加元数据'
- en: The `reader` returns an answer from each context based upon our fine-tuned model,
    along with the `id`, `url`, and `score` for the answer.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`reader` 根据我们的微调模型从每个上下文中返回一个答案，以及答案的 `id`、`url` 和 `score`。'
- en: '14.5.3 Step 3: Reranking the answers'
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5.3 步骤 3：重新排序答案
- en: Listing 14.17 shows a straightforward function that reranks the answers by simply
    sorting them by the score (the probability mass function outputs) from the `reader`
    model. The top answer is the most likely and is therefore shown first. You can
    show one answer, or you can show all that are returned by the reader. Indeed,
    sometimes it might be useful to give the question-asker multiple options and let
    them decide. This increases the odds of showing a correct answer, but it also
    takes up more space in the browser or application presenting the answers, so it
    may require a user experience trade-off.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.17 展示了一个简单的函数，它通过按分数（来自“阅读器”模型的概率质量函数输出）对答案进行排序来重排答案。最上面的答案是可能性最高的，因此首先显示。你可以显示一个答案，或者显示阅读器返回的所有答案。实际上，有时提供多个选项给提问者并让他们决定可能是有用的。这增加了显示正确答案的机会，但同时也占据了浏览器或应用中展示答案的更多空间，因此可能需要用户体验上的权衡。
- en: Listing 14.17 The reranker sorts on scores from the reader
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.17 重排器根据阅读器的分数排序
- en: '[PRE28]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We should note that your reranker could be more sophisticated, potentially incorporating
    multiple conditional models or even attempting to combine multiple answers together
    (such as overlapping answers from multiple contexts). For our purposes, we’ll
    just rely on the top score.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该指出，你的重排器可能更复杂，可能包含多个条件模型，甚至尝试将多个答案组合在一起（例如，来自多个上下文的重叠答案）。就我们的目的而言，我们只需依赖最高分数。
- en: '14.5.4 Step 4: Returning results by combining the retriever, reader, and reranker'
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5.4 步骤 4：通过组合检索器、阅读器和重排器返回结果
- en: We’re now ready to assemble all the components of our question-answering (QA)
    system. The hard part is done, so we can put them in one function, aptly named
    `ask`, which will accept a query and print out the answer.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好组装我们问答（QA）系统的所有组件。困难的部分已经完成，因此我们可以将它们放入一个名为 `ask` 的函数中，该函数将接受一个查询并打印出答案。
- en: Listing 14.18 QA function combining retriever, reader, and reranker
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.18 组合检索器、阅读器和重排器的 QA 函数
- en: '[PRE29]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Response:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE30]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: These results look pretty good. Note that in some cases multiple contexts could
    return the same answer. Generally, this would be a strong signal of a correct
    answer, so it may be a signal to consider integrating into your reranking.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果看起来相当不错。请注意，在某些情况下，多个上下文可能会返回相同的答案。通常，这将是正确答案的强烈信号，因此这可能是一个考虑将其集成到您的重排中的信号。
- en: It’s amazing to see the quality of results possible using these out-of-the-box
    models with minimal retraining. Kudos to the NLP community for making these open
    source tools, techniques, models, and datasets freely available and straightforward
    to use!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 看到使用这些现成模型在最少重新训练的情况下所能达到的结果质量真是令人惊叹。向 NLP 社区致敬，他们使这些开源工具、技术、模型和数据集免费且易于使用！
- en: Congratulations, you’ve successfully implemented an end-to-end question-answering
    system that extracts answers from search results. You generated a silver set of
    answers, saw how to improve them into a golden set, loaded and fine-tuned a question-answering
    reader model, and implemented the retriever-reader pattern, using your trained
    model and the search engine.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你已经成功实现了一个端到端问答系统，该系统能够从搜索结果中提取答案。你生成了一个银色答案集，看到了如何将它们改进为金色答案集，加载并微调了一个问答阅读器模型，并实现了检索器-阅读器模式，使用你的训练模型和搜索引擎。
- en: With LLMs, we can do much more than just extract answers from search results,
    however. LLM’s can be fine-tuned to perform abstractive question answering to
    generate answers not seen in search results but synthesized from multiple sources.
    They can also be trained to summarize search results for users or even synthesize
    brand-new content (text, images, etc.) in response to user input. Many LLMs are
    trained on so much data across such a widespread amount of human knowledge (such
    as the majority of the known internet), that they can often perform a wide variety
    of tasks like this well out of the box. These foundation models, which we’ll cover
    in the next chapter, are paving the way for the next evolution of both AI and
    AI-powered search.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用大型语言模型（LLMs），我们能够做的不仅仅是从搜索结果中提取答案。LLMs 可以经过微调以执行抽象式问答，生成在搜索结果中未见过的答案，这些答案是从多个来源综合而成的。它们还可以被训练来为用户总结搜索结果，甚至根据用户输入生成全新的内容（文本、图像等）。许多
    LLMs 在涵盖如此广泛的人类知识（例如，已知互联网的大部分内容）的大量数据上进行了训练，因此它们通常能够出色地执行各种类似任务。这些基础模型，我们将在下一章中介绍，正在为
    AI 和 AI 驱动的搜索的下一阶段发展铺平道路。
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: An extractive question-answering system generally follows the retriever-reader
    pattern, where possible contexts (documents) are found by the retriever and are
    then analyzed using the reader model to extract the most likely answer.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取式问答系统通常遵循检索器-读者模式，其中检索器找到可能的上下文（文档），然后使用读者模型分析这些上下文以提取最可能的答案。
- en: A search engine serves as a great retriever, since it is specifically designed
    to take a query and return ranked documents that are likely to serve as relevant
    context for the query.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎充当了一个出色的检索器，因为它专门设计用来接收查询并返回一系列按相关性排序的文档，这些文档可能作为查询的相关上下文。
- en: A reader model analyzes spans of text to predict the most likely beginning and
    ending of the answer within each context, scoring all options to extract the most
    likely answer.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读者模型分析文本片段以预测每个上下文中答案最可能的开始和结束位置，对所有选项进行评分以提取最可能的答案。
- en: Curating a training dataset is time-intensive, but you can generate a silver
    set of training data automatically using a pretrained model. You can then tweak
    the answers in the silver set to save significant effort compared to creating
    the entire golden training dataset manually.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精心制作训练数据集是一项耗时的工作，但你可以使用预训练模型自动生成一组银色训练数据。然后，你可以调整银色数据集中的答案，与手动创建整个金色训练数据集相比，这样可以节省大量精力。
- en: You can fine-tune a pretrained model to your specific dataset using a training,
    testing, and holdout validation dataset and optimizing for a loss minimization
    function.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用训练、测试和保留验证数据集来微调预训练模型，并针对损失最小化函数进行优化。
