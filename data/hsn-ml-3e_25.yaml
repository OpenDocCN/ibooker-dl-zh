- en: Appendix D. TensorFlow Graphs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录D。TensorFlow图
- en: In this appendix, we will explore the graphs generated by TF functions (see
    [Chapter 12](ch12.html#tensorflow_chapter)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将探索由TF函数生成的图形（请参阅[第12章](ch12.html#tensorflow_chapter)）。
- en: TF Functions and Concrete Functions
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF函数和具体函数
- en: 'TF functions are polymorphic, meaning they support inputs of different types
    (and shapes). For example, consider the following `tf_cube()` function:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TF函数是多态的，意味着它们支持不同类型（和形状）的输入。例如，考虑以下`tf_cube()`函数：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Every time you call a TF function with a new combination of input types or
    shapes, it generates a new *concrete function*, with its own graph specialized
    for this particular combination. Such a combination of argument types and shapes
    is called an *input signature*. If you call the TF function with an input signature
    it has already seen before, it will reuse the concrete function it generated earlier.
    For example, if you call `tf_cube(tf.constant(3.0))`, the TF function will reuse
    the same concrete function it used for `tf_cube(tf.constant(2.0))` (for float32
    scalar tensors). But it will generate a new concrete function if you call `tf_cube(tf.constant([2.0]))`
    or `tf_cube(tf.constant([3.0]))` (for float32 tensors of shape [1]), and yet another
    for `tf_cube(tf.constant([[1.0, 2.0], [3.0, 4.0]]))` (for float32 tensors of shape
    [2, 2]). You can get the concrete function for a particular combination of inputs
    by calling the TF function’s `get_concrete_function()` method. It can then be
    called like a regular function, but it will only support one input signature (in
    this example, float32 scalar tensors):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 每次您调用一个TF函数并使用新的输入类型或形状组合时，它会生成一个新的*具体函数*，具有为这种特定组合专门优化的图形。这样的参数类型和形状组合被称为*输入签名*。如果您使用它之前已经见过的输入签名调用TF函数，它将重用之前生成的具体函数。例如，如果您调用`tf_cube(tf.constant(3.0))`，TF函数将重用用于`tf_cube(tf.constant(2.0))`（对于float32标量张量）的相同具体函数。但是，如果您调用`tf_cube(tf.constant([2.0]))`或`tf_cube(tf.constant([3.0]))`（对于形状为[1]的float32张量），它将生成一个新的具体函数，对于`tf_cube(tf.constant([[1.0,
    2.0], [3.0, 4.0]]))`（对于形状为[2, 2]的float32张量），它将生成另一个新的具体函数。您可以通过调用TF函数的`get_concrete_function()`方法来获取特定输入组合的具体函数。然后可以像普通函数一样调用它，但它只支持一个输入签名（在此示例中为float32标量张量）：
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Figure D-1](#tf_function_diagram) shows the `tf_cube()` TF function, after
    we called `tf_cube(2)` and `tf_cube(tf.constant(2.0))`: two concrete functions
    were generated, one for each signature, each with its own optimized *function
    graph* (`FuncGraph`) and its own *function definition* (`FunctionDef`). A function
    definition points to the parts of the graph that correspond to the function’s
    inputs and outputs. In each `FuncGraph`, the nodes (ovals) represent operations
    (e.g., power, constants, or placeholders for arguments like `x`), while the edges
    (the solid arrows between the operations) represent the tensors that will flow
    through the graph. The concrete function on the left is specialized for `x=2`,
    so TensorFlow managed to simplify it to just output 8 all the time (note that
    the function definition does not even have an input). The concrete function on
    the right is specialized for float32 scalar tensors, and it could not be simplified.
    If we call `tf_cube(tf.constant(5.0))`, the second concrete function will be called,
    the placeholder operation for `x` will output 5.0, then the power operation will
    compute `5.0 ** 3`, so the output will be 125.0.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图D-1](#tf_function_diagram)显示了`tf_cube()` TF函数，在我们调用`tf_cube(2)`和`tf_cube(tf.constant(2.0))`之后：生成了两个具体函数，每个签名一个，每个具有自己优化的*函数图*（`FuncGraph`）和自己的*函数定义*（`FunctionDef`）。函数定义指向与函数的输入和输出对应的图的部分。在每个`FuncGraph`中，节点（椭圆形）表示操作（例如，幂运算，常量，或用于参数的占位符如`x`），而边（操作之间的实箭头）表示将在图中流动的张量。左侧的具体函数专门用于`x=2`，因此TensorFlow成功将其简化为始终输出8（请注意，函数定义甚至没有输入）。右侧的具体函数专门用于float32标量张量，无法简化。如果我们调用`tf_cube(tf.constant(5.0))`，将调用第二个具体函数，`x`的占位符操作将输出5.0，然后幂运算将计算`5.0
    ** 3`，因此输出将为125.0。'
- en: '![mls3 ad01](assets/mls3_ad01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 ad01](assets/mls3_ad01.png)'
- en: Figure D-1\. The `tf_cube()` TF function, with its `ConcreteFunction`s and their
    `FuncGraph`s
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图D-1。`tf_cube()` TF函数，及其`ConcreteFunction`和它们的`FuncGraph`
- en: The tensors in these graphs are *symbolic tensors*, meaning they don’t have
    an actual value, just a data type, a shape, and a name. They represent the future
    tensors that will flow through the graph once an actual value is fed to the placeholder
    `x` and the graph is executed. Symbolic tensors make it possible to specify ahead
    of time how to connect operations, and they also allow TensorFlow to recursively
    infer the data types and shapes of all tensors, given the data types and shapes
    of their inputs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图中的张量是*符号张量*，意味着它们没有实际值，只有数据类型、形状和名称。它们代表将在实际值被馈送到占位符`x`并执行图形后流经图形的未来张量。符号张量使得可以预先指定如何连接操作，并且还允许TensorFlow递归推断所有张量的数据类型和形状，鉴于它们的输入的数据类型和形状。
- en: Now let’s continue to peek under the hood, and see how to access function definitions
    and function graphs and how to explore a graph’s operations and tensors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续窥探底层，并看看如何访问函数定义和函数图，以及如何探索图的操作和张量。
- en: Exploring Function Definitions and Graphs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索函数定义和图形
- en: 'You can access a concrete function’s computation graph using the `graph` attribute,
    and get the list of its operations by calling the graph’s `get_operations()` method:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`graph`属性访问具体函数的计算图，并通过调用图的`get_operations()`方法获取其操作列表：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, the first operation represents the input argument `x` (it
    is called a *placeholder*), the second “operation” represents the constant `3`,
    the third operation represents the power operation (`**`), and the final operation
    represents the output of this function (it is an identity operation, meaning it
    will do nothing more than copy the output of the power operation⁠^([1](app04.html#idm45720155973312))).
    Each operation has a list of input and output tensors that you can easily access
    using the operation’s `inputs` and `outputs` attributes. For example, let’s get
    the list of inputs and outputs of the power operation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第一个操作代表输入参数 `x`（它被称为 *占位符*），第二个“操作”代表常数 `3`，第三个操作代表幂运算（`**`），最后一个操作代表这个函数的输出（它是一个恒等操作，意味着它不会做任何比幂运算输出的更多的事情⁠^([1](app04.html#idm45720155973312)））。每个操作都有一个输入和输出张量的列表，您可以通过操作的
    `inputs` 和 `outputs` 属性轻松访问。例如，让我们获取幂运算的输入和输出列表：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This computation graph is represented in [Figure D-2](#computation_graph_diagram).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算图在 [图 D-2](#computation_graph_diagram) 中表示。
- en: '![mls3 ad02](assets/mls3_ad02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 ad02](assets/mls3_ad02.png)'
- en: Figure D-2\. Example of a computation graph
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 D-2\. 计算图示例
- en: 'Note that each operation has a name. It defaults to the name of the operation
    (e.g., `"pow"`), but you can define it manually when calling the operation (e.g.,
    `tf.pow(x, 3, name="other_name")`). If a name already exists, TensorFlow automatically
    adds a unique index (e.g., `"pow_1"`, `"pow_2"`, etc.). Each tensor also has a
    unique name: it is always the name of the operation that outputs this tensor,
    plus `:0` if it is the operation’s first output, or `:1` if it is the second output,
    and so on. You can fetch an operation or a tensor by name using the graph’s `get_operation_by_name()`
    or `get_tensor_by_name()` methods:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意每个操作都有一个名称。它默认为操作的名称（例如，`"pow"`），但当调用操作时您可以手动定义它（例如，`tf.pow(x, 3, name="other_name")`）。如果名称已经存在，TensorFlow
    会自动添加一个唯一的索引（例如，`"pow_1"`，`"pow_2"` 等）。每个张量也有一个唯一的名称：它总是输出该张量的操作的名称，如果它是操作的第一个输出，则为
    `:0`，如果它是第二个输出，则为 `:1`，依此类推。您可以使用图的 `get_operation_by_name()` 或 `get_tensor_by_name()`
    方法按名称获取操作或张量：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The concrete function also contains the function definition (represented as
    a protocol buffer⁠^([2](app04.html#idm45720155848416))), which includes the function’s
    signature. This signature allows the concrete function to know which placeholders
    to feed with the input values, and which tensors to return:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体函数还包含函数定义（表示为协议缓冲区⁠^([2](app04.html#idm45720155848416)）），其中包括函数的签名。这个签名允许具体函数知道要用输入值填充哪些占位符，以及要返回哪些张量：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now let’s look more closely at tracing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更仔细地看一下跟踪。
- en: A Closer Look at Tracing
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更仔细地看一下跟踪
- en: 'Let’s tweak the `tf_cube()` function to print its input:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调整 `tf_cube()` 函数以打印其输入：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s call it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调用它：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `result` looks good, but look at what was printed: `x` is a symbolic tensor!
    It has a shape and a data type, but no value. Plus it has a name (`"x:0"`). This
    is because the `print()` function is not a TensorFlow operation, so it will only
    run when the Python function is traced, which happens in graph mode, with arguments
    replaced with symbolic tensors (same type and shape, but no value). Since the
    `print()` function was not captured into the graph, the next times we call `tf_cube()`
    with float32 scalar tensors, nothing is printed:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`result` 看起来不错，但看看打印出来的内容：`x` 是一个符号张量！它有一个形状和数据类型，但没有值。而且它有一个名称（`"x:0"`）。这是因为
    `print()` 函数不是一个 TensorFlow 操作，所以它只会在 Python 函数被跟踪时运行，这发生在图模式下，参数被替换为符号张量（相同类型和形状，但没有值）。由于
    `print()` 函数没有被捕获到图中，所以下一次我们用 float32 标量张量调用 `tf_cube()` 时，什么也不会被打印：'
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'But if we call `tf_cube()` with a tensor of a different type or shape, or with
    a new Python value, the function will be traced again, so the `print()` function
    will be called:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们用不同类型或形状的张量，或者用一个新的 Python 值调用 `tf_cube()`，函数将再次被跟踪，因此 `print()` 函数将被调用：
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Warning
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If your function has Python side effects (e.g., it saves some logs to disk),
    be aware that this code will only run when the function is traced (i.e., every
    time the TF function is called with a new input signature). It’s best to assume
    that the function may be traced (or not) any time the TF function is called.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的函数具有 Python 副作用（例如，将一些日志保存到磁盘），请注意此代码只会在函数被跟踪时运行（即每次用新的输入签名调用 TF 函数时）。最好假设函数可能在调用
    TF 函数时随时被跟踪（或不被跟踪）。
- en: 'In some cases, you may want to restrict a TF function to a specific input signature.
    For example, suppose you know that you will only ever call a TF function with
    batches of 28 × 28–pixel images, but the batches will have very different sizes.
    You may not want TensorFlow to generate a different concrete function for each
    batch size, or count on it to figure out on its own when to use `None`. In this
    case, you can specify the input signature like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望将 TF 函数限制为特定的输入签名。例如，假设您知道您只会用 28 × 28 像素图像的批次调用 TF 函数，但是批次的大小会有很大的不同。您可能不希望
    TensorFlow 为每个批次大小生成不同的具体函数，或者依赖它自行决定何时使用 `None`。在这种情况下，您可以像这样指定输入签名：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This TF function will accept any float32 tensor of shape [*, 28, 28], and it
    will reuse the same concrete function every time:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 TF 函数将接受任何形状为 [*, 28, 28] 的 float32 张量，并且每次都会重用相同的具体函数：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'However, if you try to call this TF function with a Python value, or a tensor
    of an unexpected data type or shape, you will get an exception:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您尝试用 Python 值调用这个 TF 函数，或者用意外的数据类型或形状的张量调用它，您将会得到一个异常：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using AutoGraph to Capture Control Flow
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AutoGraph 捕获控制流
- en: 'If your function contains a simple `for` loop, what do you expect will happen?
    For example, let’s write a function that will add 10 to its input, by just adding
    1 10 times:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的函数包含一个简单的 `for` 循环，您期望会发生什么？例如，让我们编写一个函数，通过连续添加 1 来将 10 添加到其输入中：
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It works fine, but when we look at its graph, we find that it does not contain
    a loop: it just contains 10 addition operations!'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行正常，但当我们查看它的图时，我们发现它不包含循环：它只包含 10 个加法操作！
- en: '[PRE14]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This actually makes sense: when the function got traced, the loop ran 10 times,
    so the `x += 1` operation was run 10 times, and since it was in graph mode, it
    recorded this operation 10 times in the graph. You can think of this `for` loop
    as a “static” loop that gets unrolled when the graph is created.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上这是有道理的：当函数被跟踪时，循环运行了10次，因此`x += 1`操作运行了10次，并且由于它处于图模式下，它在图中记录了这个操作10次。您可以将这个`for`循环看作是一个在创建图表时被展开的“静态”循环。
- en: 'If you want the graph to contain a “dynamic” loop instead (i.e., one that runs
    when the graph is executed), you can create one manually using the `tf.while_loop()`
    operation, but it is not very intuitive (see the “Using AutoGraph to Capture Control
    Flow” section of the Chapter 12 notebook for an example). Instead, it is much
    simpler to use TensorFlow’s *AutoGraph* feature, discussed in [Chapter 12](ch12.html#tensorflow_chapter).
    AutoGraph is actually activated by default (if you ever need to turn it off, you
    can pass `autograph=False` to `tf.function()`). So if it is on, why didn’t it
    capture the `for` loop in the `add_10()` function? It only captures `for` loops
    that iterate over tensors of `tf.data.Dataset` objects, so you should use `tf.range()`,
    not `range()`. This is to give you the choice:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望图表包含一个“动态”循环（即在执行图表时运行的循环），您可以手动使用`tf.while_loop()`操作创建一个，但这并不直观（请参见第12章笔记本的“使用AutoGraph捕获控制流”部分以获取示例）。相反，使用TensorFlow的*AutoGraph*功能要简单得多，详见[第12章](ch12.html#tensorflow_chapter)。AutoGraph实际上是默认激活的（如果您需要关闭它，可以在`tf.function()`中传递`autograph=False`）。因此，如果它是开启的，为什么它没有捕获`add_10()`函数中的`for`循环呢？它只捕获对`tf.data.Dataset`对象的张量进行迭代的`for`循环，因此您应该使用`tf.range()`而不是`range()`。这是为了给您选择：
- en: If you use `range()`, the `for` loop will be static, meaning it will only be
    executed when the function is traced. The loop will be “unrolled” into a set of
    operations for each iteration, as we saw.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用`range()`，`for`循环将是静态的，这意味着仅在跟踪函数时才会执行。循环将被“展开”为每次迭代的一组操作，正如我们所见。
- en: If you use `tf.range()`, the loop will be dynamic, meaning that it will be included
    in the graph itself (but it will not run during tracing).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用`tf.range()`，循环将是动态的，这意味着它将包含在图表本身中（但在跟踪期间不会运行）。
- en: 'Let’s look at the graph that gets generated if we just replace `range()` with
    `tf.range()` in the `add_10()` function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果在`add_10()`函数中将`range()`替换为`tf.range()`时生成的图表：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the graph now contains a `While` loop operation, as if we had
    called the `tf.while_loop()` function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，图现在包含一个`While`循环操作，就好像我们调用了`tf.while_loop()`函数一样。
- en: Handling Variables and Other Resources in TF Functions
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TF函数中处理变量和其他资源
- en: 'In TensorFlow, variables and other stateful objects, such as queues or datasets,
    are called *resources*. TF functions treat them with special care: any operation
    that reads or updates a resource is considered stateful, and TF functions ensure
    that stateful operations are executed in the order they appear (as opposed to
    stateless operations, which may be run in parallel, so their order of execution
    is not guaranteed). Moreover, when you pass a resource as an argument to a TF
    function, it gets passed by reference, so the function may modify it. For example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，变量和其他有状态对象，如队列或数据集，被称为*资源*。TF函数对它们进行特殊处理：任何读取或更新资源的操作都被视为有状态的，并且TF函数确保有状态的操作按照它们出现的顺序执行（与无状态操作相反，后者可能并行运行，因此它们的执行顺序不被保证）。此外，当您将资源作为参数传递给TF函数时，它会通过引用传递，因此函数可能会对其进行修改。例如：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you peek at the function definition, the first argument is marked as a resource:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查看函数定义，第一个参数被标记为资源：
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It is also possible to use a `tf.Variable` defined outside of the function,
    without explicitly passing it as an argument:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在函数外部使用定义的`tf.Variable`，而无需显式将其作为参数传递：
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The TF function will treat this as an implicit first argument, so it will actually
    end up with the same signature (except for the name of the argument). However,
    using global variables can quickly become messy, so you should generally wrap
    variables (and other resources) inside classes. The good news is `@tf.function`
    works fine with methods too:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TF函数将将其视为隐式的第一个参数，因此实际上最终会具有相同的签名（除了参数的名称）。但是，使用全局变量可能会很快变得混乱，因此通常应该将变量（和其他资源）封装在类中。好消息是`@tf.function`也可以很好地与方法一起使用：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Warning
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Do not use `=`, `+=`, `-=`, or any other Python assignment operator with TF
    variables. Instead, you must use the `assign()`, `assign_add()`, or `assign_sub()`
    methods. If you try to use a Python assignment operator, you will get an exception
    when you call the method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用`=`、`+=`、`-=`或任何其他Python赋值运算符与TF变量。相反，您必须使用`assign()`、`assign_add()`或`assign_sub()`方法。如果尝试使用Python赋值运算符，当调用该方法时将会出现异常。
- en: A good example of this object-oriented approach is, of course, Keras. Let’s
    see how to use TF functions with Keras.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种面向对象的方法的一个很好的例子当然是Keras。让我们看看如何在Keras中使用TF函数。
- en: Using TF Functions with Keras (or Not)
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF函数与Keras（或不使用）
- en: 'By default, any custom function, layer, or model you use with Keras will automatically
    be converted to a TF function; you do not need to do anything at all! However,
    in some cases you may want to deactivate this automatic conversion—for example,
    if your custom code cannot be turned into a TF function, or if you just want to
    debug your code (which is much easier in eager mode). To do this, you can simply
    pass `dynamic=True` when creating the model or any of its layers:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，您在Keras中使用的任何自定义函数、层或模型都将自动转换为TF函数；您无需做任何事情！但是，在某些情况下，您可能希望停用此自动转换——例如，如果您的自定义代码无法转换为TF函数，或者如果您只想调试代码（在急切模式下更容易）。为此，您只需在创建模型或其任何层时传递`dynamic=True`：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If your custom model or layer will always be dynamic, you can instead call
    the base class’s constructor with `dynamic=True`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的自定义模型或层将始终是动态的，可以使用`dynamic=True`调用基类的构造函数：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Alternatively, you can pass `run_eagerly=True` when calling the `compile()`
    method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在调用`compile()`方法时传递`run_eagerly=True`：
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now you know how TF functions handle polymorphism (with multiple concrete functions),
    how graphs are automatically generated using AutoGraph and tracing, what graphs
    look like, how to explore their symbolic operations and tensors, how to handle
    variables and resources, and how to use TF functions with Keras.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了 TF 函数如何处理多态性（具有多个具体函数），如何使用 AutoGraph 和追踪自动生成图形，图形的样子，如何探索它们的符号操作和张量，如何处理变量和资源，以及如何在
    Keras 中使用 TF 函数。
- en: ^([1](app04.html#idm45720155973312-marker)) You can safely ignore it—it is only
    here for technical reasons, to ensure that TF functions don’t leak internal structures.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](app04.html#idm45720155973312-marker)) 你可以安全地忽略它 - 它只是为了技术原因而在这里，以确保 TF
    函数不会泄漏内部结构。
- en: ^([2](app04.html#idm45720155848416-marker)) A popular binary format discussed
    in [Chapter 13](ch13.html#data_chapter).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](app04.html#idm45720155848416-marker)) 在[第13章](ch13.html#data_chapter)中讨论的一种流行的二进制格式。
