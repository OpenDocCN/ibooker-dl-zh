- en: 1 Introduction to Bayesian optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 贝叶斯优化简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: What motivates Bayesian optimization and how it works
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是什么促使了贝叶斯优化以及它是如何工作的
- en: Real-life examples of Bayesian optimization problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化问题的实际例子
- en: A toy example of Bayesian optimization in action
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化的一个玩具示例
- en: You’ve made a wonderful choice in reading this book, and I’m excited for your
    upcoming journey! On a high level, *Bayesian optimization* is an optimization
    technique that may be applied when the function (or, in general, any process that
    generates an output when an input is passed in) one is trying to optimize is a
    black box and expensive to evaluate in terms of time, money, or other resources.
    This setup encompasses many important tasks, including hyperparameter tuning,
    which we define shortly. Using Bayesian optimization can accelerate this search
    procedure and help us locate the optimum of the function as quickly as possible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择阅读本书是一个很棒的选择，我对你即将开始的旅程感到兴奋！从高层次来看，*贝叶斯优化*是一种优化技术，当我们试图优化的函数（或者一般情况下，当输入一个值时产生输出的过程）是一个黑盒且评估起来时间、金钱或其他资源成本很高时，可以应用此技术。这个设置涵盖了许多重要的任务，包括超参数调优，我们将很快定义它。使用贝叶斯优化可以加速搜索过程，并帮助我们尽快找到函数的最优解。
- en: 'While Bayesian optimization has enjoyed enduring interest from the machine
    learning (ML) research community, it’s not as commonly used or talked about as
    other ML topics in practice. But why? Some might say Bayesian optimization has
    a steep learning curve: one needs to understand calculus, use some probability,
    and be an overall experienced ML researcher to use Bayesian optimization in an
    application. The goal of this book is to dispel the idea that Bayesian optimization
    is difficult to use and show that the technology is more intuitive and accessible
    than one would think.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管贝叶斯优化在机器学习研究界一直受到持久的关注，但在实践中，它并不像其他机器学习话题那样常用或广为人知。但为什么呢？有些人可能会说贝叶斯优化具有陡峭的学习曲线：使用者需要理解微积分、使用概率，并且需要是一个经验丰富的机器学习研究者，才能在应用中使用贝叶斯优化。本书的目标是打破贝叶斯优化难以使用的观念，并展示该技术比想象的更直观、更易用。
- en: Throughout this book, we encounter many illustrations, plots, and, of course,
    code, which aim to make the topic of discussion more straightforward and concrete.
    You learn how each component of Bayesian optimization works on a high level and
    how to implement them using state-of-the-art libraries in Python. The accompanying
    code also serves to help you hit the ground running with your own projects, as
    the Bayesian optimization framework is very general and “plug and play.” The exercises
    are also helpful in this regard.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们会遇到许多插图、图表和代码，旨在使讨论的主题更加简单明了和具体。你将了解贝叶斯优化的每个组成部分在高层次上是如何工作的，并学会如何使用Python中的最先进的库来实现它们。配套的代码还可帮助你快速上手你自己的项目，因为贝叶斯优化框架非常通用和“即插即用”。这些练习对此也非
- en: Generally, I hope this book is useful for your ML needs and is an overall fun
    read. Before we dive into the actual content, let’s take some time to discuss
    the problem that Bayesian optimization sets out to solve.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我希望这本书对你的机器学习需求有所帮助，并且是一本有趣的阅读。在我们深入讨论实际内容之前，让我们花点时间来讨论贝叶斯优化试图解决的问题。
- en: 1.1 Finding the optimum of an expensive black box function
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 寻找一个昂贵黑盒函数的最优解
- en: As mentioned previously, hyperparameter tuning in ML is one of the most common
    applications of Bayesian optimization. We explore this problem, as well as a couple
    of others, in this section as an example of the general problem of black box optimization.
    This will help us understand why Bayesian optimization is needed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，超参数调优是贝叶斯优化在机器学习中最常见的应用之一。我们在本节中探讨了这个问题以及其他一些问题，作为黑盒优化问题的一个例子。这将帮助我们理解为什么需要贝叶斯优化。
- en: 1.1.1 Hyperparameter tuning as an example of an expensive black box optimization
    problem
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 超参数调优作为昂贵黑盒优化问题的一个示例
- en: Say we want to train a neural network on a large dataset, but we are not sure
    how many layers this neural net should have. We know that the architecture of
    a neural net is a make-or-break factor in deep learning (DL), so we perform some
    initial testing and obtain the results shown in table 1.1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想在一个大数据集上训练神经网络，但我们不确定这个神经网络应该有多少层。我们知道神经网络的架构是深度学习中的一个成功因素，因此我们进行了一些初步测试，并得到了表格
    1.1 中显示的结果。
- en: Table 1.1 An example of a hyperparameter tuning task
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1.1 超参数调优任务的示例
- en: '| Number of layers | Accuracy on the test set |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 层数 | 测试集准确率 |'
- en: '| --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 5 | 0.72 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.72 |'
- en: '| 10 | 0.81 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.81 |'
- en: '| 20 | 0.75 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.75 |'
- en: Our task is to decide how many layers the neural network should have in the
    next trial in the search for the highest accuracy. It’s difficult to decide which
    number we should try next. The best accuracy we have found, 81%, is good, but
    we think we can do better with a different number of layers. Unfortunately, the
    boss has set a deadline to finish implementing the model. Since training a neural
    net on our large dataset takes several days, we only have a few trials remaining
    before we need to decide how many layers our network should have. With that in
    mind, we want to know what other values we should try so we can find the number
    of layers that provides the highest possible accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是决定神经网络在寻找最高准确率时应该有多少层。很难决定我们应该尝试下一个数字是多少。我们找到的最佳准确率为81%，虽然不错，但我们认为通过不同数量的层，我们可以做得更好。不幸的是，老板已经设定了完成模型实施的截止日期。由于在我们的大型数据集上训练神经网络需要几天时间，我们只剩下几次试验的机会，然后就需要决定我们的网络应该有多少层。考虑到这一点，我们想知道我们应该尝试哪些其他值，以便找到提供最高可能准确率的层数。
- en: This task, in which we want to find the best setting (hyperparameter values)
    for our model to optimize some performance metric, such as predictive accuracy,
    is typically called *hyperparameter tuning* in ML. In our example, the hyperparameter
    of our neural net is its depth (the number of layers). If we are working with
    a decision tree, common hyperparameters are the maximum depth, the minimum number
    of points per node, and the split criterion. With a support-vector machine, we
    could tune the regularization term and the kernel. Since the performance of a
    model very much depends on its hyperparameters, hyperparameter tuning is an important
    component of any ML pipeline.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务旨在找到最佳设置（超参数值），以优化模型的某些性能指标，如预测准确率，在机器学习中通常被称为*超参数调整*。在我们的示例中，神经网络的超参数是其深度（层数）。如果我们使用决策树，常见的超参数包括最大深度、每个节点的最小数据点数和分裂标准。对于支持向量机，我们可以调整正则化项和核函数。由于模型的性能很大程度上取决于其超参数，超参数调整是任何机器学习流水线的重要组成部分。
- en: If this were a typical real-world dataset, this process could take a lot of
    time and resources. Figure 1.1 from OpenAI ([https://openai.com/blog/ai-and-compute/](https://openai.com/blog/ai-and-compute/))
    shows that as neural networks keep getting larger and deeper, the amount of computation
    necessary (measured in petaflop/s-days) increases exponentially.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个典型的真实世界数据集，这个过程可能需要大量的时间和资源。来自OpenAI的图1.1（[https://openai.com/blog/ai-and-compute/](https://openai.com/blog/ai-and-compute/)）显示，随着神经网络变得越来越大和越来越深，所需的计算量（以petaflop/s-days为单位）呈指数增长。
- en: '![](../../OEBPS/Images/01-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-01.png)'
- en: Figure 1.1 The compute cost of training large neural networks has been steadily
    growing, making hyperparameter tuning increasingly difficult.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 训练大型神经网络的计算成本一直在稳步增长，使得超参数调整变得越来越困难。
- en: This is to say that training a model on a large dataset is quite involved and
    takes significant effort. Further, we want to identify the hyperparameter values
    that give the best accuracy, so training will have to be done many times. How
    should we go about choosing which values to use to parameterize our model so we
    can zero in on the best combination as quickly as possible? That is the central
    question of hyperparameter tuning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在大型数据集上训练模型是相当复杂的，并且需要大量的工作。此外，我们希望确定能够提供最佳准确率的超参数值，因此需要进行多次训练。我们应该如何选择数值来对我们的模型进行参数化，以便尽快找到最佳组合？这是超参数调整的核心问题。
- en: Getting back to our neural net example in section 1.1, what number of layers
    should we try next so we can find an accuracy greater than 81%? Some value between
    10 layers and 20 layers is promising, since at both 10 and 20, we have better
    performance than at 5 layers. But what exact value we should inspect next is still
    not obvious since there may still be a lot of variability in numbers between 10
    and 20\. When we say *variability*, we implicitly talk about our uncertainty regarding
    how the test accuracy of our model behaves as a function of the number of layers.
    Even though we know 10 layers lead to 81% and 20 layers lead to 75%, we cannot
    say for certain what value, say, 15 layers would yield. This is to say we need
    to account for our level of uncertainty when considering these values between
    10 and 20.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们在第1.1节中的神经网络示例，我们应该尝试多少层才能找到高于81%的准确度？在10层和20层之间的某个数值是有前途的，因为在10层和20层，我们的性能比在5层时更好。但我们应该检查哪个确切的数值仍然不明显，因为在10和20之间的数值仍然可能有很大变异性。当我们说*变异性*时，我们隐含地谈论了我们关于模型测试准确性如何随层数变化而变化的不确定性。即使我们知道10层导致81%，20层导致75%，我们仍然不能确定例如15层会产生什么值。这就是说，当我们考虑10和20之间的这些值时，我们需要考虑我们的不确定性水平。
- en: Further, what if some number greater than 20 gives us the highest accuracy possible?
    This is the case for many large datasets, where a sufficient depth is necessary
    for a neural net to learn anything useful. Or, though unlikely, what if a small
    number of layers (fewer than 5) is actually what we need?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果某个大于20的数值为我们提供了最高可能的准确度怎么办？这对于许多大型数据集来说是一种情况，其中足够的深度对于神经网络学习任何有用的东西都是必要的。或者，尽管不太可能，少于5层的小层数实际上是我们需要的吗？
- en: How should we explore these different options in a principled way so that when
    our time runs out and we have to report back to our boss, we can be sufficiently
    confident that we have arrived at the best number of layers for our model? This
    question is an example of the *expensive black box optimization* problem, which
    we discuss next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何以有原则的方式探索这些不同的选择，以便在时间耗尽和我们必须向老板汇报时，我们可以充分自信地认为我们已经找到了我们模型的最佳层数？该问题是*昂贵黑盒优化*问题的一个例子，我们接下来会讨论这个问题。
- en: 1.1.2 The problem of expensive black box optimization
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 昂贵黑盒优化问题
- en: In this subsection, we formally introduce the problem of expensive black box
    optimization, which is what Bayesian optimization aims to solve. Understanding
    why this is such a difficult problem will help us understand why Bayesian optimization
    is preferred over simpler, more naïve approaches, such as grid search (where we
    divide the search space into equal segments) or random search (where we use randomness
    to guide our search).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子章节中，我们正式介绍了昂贵黑盒优化问题，这是贝叶斯优化的目标。理解为什么这个问题很难有助于我们理解，为什么贝叶斯优化比更简单的、更天真的方法更受欢迎，比如网格搜索（我们将搜索空间分为相等的段）或随机搜索（我们使用随机性来指导我们的搜索）。
- en: In this problem, we have black box access to a function (some input–output mechanism),
    and our task is to find the input that maximizes the output of this function.
    The function is often called the *objective function*, as optimizing it is our
    objective, and we want to find the *optimum* of the objective function—the input
    that yields the highest function value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们可以黑匣子方式访问函数（一些输入-输出机制），我们的任务是找到最大化此函数输出的输入。该函数通常称为*目标函数*，因为优化它是我们的目标，并且我们希望找到目标函数的*最优解*——产生最高函数值的输入。
- en: Characteristics of the objective function
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数的特点
- en: The term *black box* means that we don’t know what the underlying formula of
    the objective is; all we have access to is the function output when we make an
    observation by computing the function value at some input. In our neural net example,
    we don’t know how the accuracy of our model will change if we increase the number
    of layers one by one (otherwise, we would just pick the best one).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*黑盒*意味着我们不知道目标的底层公式；我们唯一能够访问的是通过在某个输入处计算函数值进行观察时得到的函数输出。在我们的神经网络示例中，我们不知道如果我们逐层增加层数，我们的模型的准确性将如何变化（否则，我们将只选择最佳层）。
- en: The problem is expensive because in many cases, making an observation (evaluating
    the objective at some location) is prohibitively costly, rendering a naïve approach,
    such as an exhaustive search, intractable. In ML and, especially, DL, time is
    usually the main constraint, as we already discussed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题很昂贵，因为在许多情况下，做出观察（在某个位置评估目标）的成本非常高昂，使得像穷举搜索这样的天真方法难以处理。在机器学习和尤其是深度学习中，时间通常是主要的约束条件，正如我们之前讨论过的那样。
- en: Hyperparameter tuning belongs to this class of expensive black box optimization
    problems, but it is not the only one! Any procedure in which we are trying to
    find some settings or parameters to optimize a process without knowing how the
    different settings influence and control the result of the process qualifies as
    a black box optimization problem. Further, trying out a particular setting and
    observing its result on the target process (the objective function) is time-consuming,
    expensive, or costly in some other sense.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整属于这一类昂贵的黑盒优化问题，但不是唯一的！任何试图找到一些设置或参数来优化一个过程，而不知道不同设置如何影响和控制过程结果的程序都属于黑盒优化问题。此外，尝试特定设置并观察其对目标过程（目标函数）的结果是耗时的、昂贵的或在某种程度上成本高昂的。
- en: Definition The act of trying out a particular setting—that is, evaluating the
    value of the objective function at some input—is called *making a query* or *querying
    the objective function*. The entire procedure is summarized in figure 1.2.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 尝试特定设置的行为——即，在某个输入处评估目标函数的值——称为*发出查询*或*查询目标函数*。整个过程总结如图1.2。
- en: '![](../../OEBPS/Images/01-02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-02.png)'
- en: Figure 1.2 The framework of a black box optimization problem. We repeatedly
    query the function values at various locations to find the global optimum.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 黑盒优化问题的框架。我们反复查询不同位置的函数值以找到全局最优解。
- en: 1.1.3 Other real-world examples of expensive black box optimization problems
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 其他昂贵的黑盒优化问题的实际例子
- en: Now, let’s consider a few real-world examples that fall into the category of
    expensive black box optimization problems. We will see that such problems are
    common in the field; we often find ourselves with a function we’d like to optimize
    but that can only be evaluated a small number of times. In these cases, we’d like
    to find a way to intelligently choose where to evaluate the function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一些属于昂贵的黑盒优化问题类别的实际例子。我们会发现这样的问题在这个领域中很常见；我们经常会遇到想要优化但只能评估少数次的函数。在这些情况下，我们希望找到一种方法来智能地选择在哪里评估函数。
- en: The first example is drug discovery—the process in which scientists and chemists
    identify compounds with desirable chemical properties that may be synthesized
    into drugs. As you can imagine, the experimental process is quite involved and
    costs a lot of money. Another factor that makes this drug discovery task daunting
    is the decreasing trend in the productivity of drug discovery R&D that has been
    robustly observed in recent years. This phenomenon is known as *Eroom’s Law*—a
    reverse of *Moore’s Law*—which roughly states that the number of new drugs approved
    per billion US dollars halves over a fixed period of time. Eroom’s Law is visualized
    in figure 1 of the Nature paper “Diagnosing the Decline in Parmaceutical R&D Efficiency”
    by Jack W. Scannell, Alex Blanckley, Helen Boldon, and Brian Warrington ([https://www.nature.com/articles/nrd3681](https://www.nature.com/articles/nrd3681)).
    (Alternatively, you can simply search for images of “Eroom’s Law” on Google.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子是药物发现——科学家和化学家识别具有理想化学特性的化合物，可以合成成药物的过程。正如你可以想象的那样，实验过程非常复杂并且成本很高。使这项药物发现任务令人生畏的另一个因素是近年来已经观察到的药物发现研发生产力下降趋势。这种现象被称为*艾尔姆定律*——*摩尔定律*的反转——它大致说明了每十亿美元批准的新药物数量在固定时间内减半。艾尔姆定律在杰克·W·斯坎内尔（Jack
    W. Scannell）、亚历克斯·布兰克利（Alex Blanckley）、海伦·波尔登（Helen Boldon）和布莱恩·沃灵顿（Brian Warrington）撰写的自然杂志论文“诊断药物研发效率下降”（[https://www.nature.com/articles/nrd3681](https://www.nature.com/articles/nrd3681)）的第1张图片中有可视化。（或者，你也可以简单地在谷歌上搜索“艾尔姆定律”的图像。）
- en: Eroom’s Law shows that drug discovery throughput resulting from each billion-dollar
    investment in drug research and development (R&D) decreases linearly on the logarithmic
    scale over time. In other words, the decrease in drug discovery throughput for
    a fixed amount of R&D investment is exponential in recent years. Although there
    are ups and downs in the local trend throughout the years, the exponential decline
    is obvious going from 1950 to 2020.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔姆斯定律显示，每十亿美元的药物研究与开发（R&D）投资所得到的药物发现能力，经过对数尺度上的线性下降。换句话说，对于固定数量的R&D投资，药物发现能力在最近几年呈指数级下降。虽然在不同年份存在局部趋势的起伏，但从1950年到2020年，指数下降趋势是明显的。
- en: The same problem, in fact, applies to any scientific discovery task in which
    scientists search for new chemicals, materials, or designs that are rare, novel,
    and useful, with respect to some metric, using experiments that require top-of-the-line
    equipment and may take days or weeks to finish. In other words, they are trying
    to optimize for their respective objective functions where evaluations are extremely
    expensive.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，同样的问题适用于任何科学发现任务，其中科学家们通过使用需要顶级设备和可能需要几天或几周才能完成的实验，搜索罕见、新颖和有用的新化学品、材料或设计，用于某种度量标准。换句话说，他们试图针对极为昂贵的数据评估来优化它们各自的目标函数。
- en: As an illustration, table 1.2 shows a couple of data points in a real-life dataset
    from such a task. The objective is to find the alloy composition (from the four
    parent elements) with the lowest mixing temperature, which is a black box optimization
    problem. Here, materials scientists worked with compositions of alloys of lead
    (Pb), tin (Sn), germanium (Ge), and manganese (Mn). Each given combination of
    percentages of these compositions corresponds to a potential alloy that could
    be synthesized and experimented on in a laboratory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以表1.2为例，展示了真实数据集中的几个数据点。目标是找到具有最低混合温度的合金组成（来自四个母元素）。这是一个黑盒优化问题。在这里，材料科学家们研究了铅（Pb）、锡（Sn）、锗（Ge）和锰（Mn）的合金组成。每个给定的组合百分比对应于一个可以在实验室中合成和实验的潜在合金。
- en: Table 1.2 Data from a materials discovery task
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 来自材料发现任务的数据
- en: '| % of Pb | % of Sn | % of Ge | % of Mn | Mixing temp. (°F) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Pb的百分比 | Sn的百分比 | Ge的百分比 | Mn的百分比 | 混合温度（°F） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0.50 | 0.50 | 0.00 | 0.00 | 192.08 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 0.50 | 0.50 | 0.00 | 0.00 | 192.08 |'
- en: '| 0.33 | 0.33 | 0.33 | 0.00 | 258.30 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 0.33 | 0.33 | 0.33 | 0.00 | 258.30 |'
- en: '| 0.00 | 0.50 | 0.50 | 0.00 | 187.24 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | 0.50 | 0.50 | 0.00 | 187.24 |'
- en: '| 0.00 | 0.33 | 0.33 | 0.33 | 188.54 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | 0.33 | 0.33 | 0.33 | 188.54 |'
- en: '| *Source*: Author’s research work. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| *来源*: 作者的研究工作。 |'
- en: 'As a low temperature of mixing indicates a stable, valuable structure for the
    alloy, the objective is to find compositions whose mixing temperatures are as
    low as possible. But there is one bottleneck: determining this mixing temperature
    for a given alloy generally takes days. The question we set out to solve algorithmically
    is similar: Given the dataset we see, what is the next composition we should experiment
    with (in terms of how much lead, tin, germanium, and manganese should be present)
    to find the minimum temperature of mixing?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于低的混合温度表示合金结构稳定、有价值，目标是找到混合温度尽可能低的组成。但是有一个瓶颈：通常需要数天时间才能确定给定合金的混合温度。我们要算法地解决的问题是类似的：给定我们看到的数据集，我们应该尝试下一个组合（在铅、锡、锗和锰的含量上如何）以找到最低的混合温度？
- en: Another example is in mining and oil drilling, or, more specifically, finding
    the region within a big area that has the highest yield of valuable minerals or
    oil. This involves extensive planning, investment, and labor—again an expensive
    undertaking. As digging operations have significant negative effects on the environment,
    there are regulations in place to reduce mining activities, placing a limit on
    the number of *function evaluations* that may be done in this optimization problem.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是在采矿和石油钻探中，或者更具体地说，在一个大区域内找到具有最高价值矿物或石油产量的地区。这需要大量的规划、投资和劳动力，是一项昂贵的事业。由于挖掘作业对环境有重大负面影响，因此有相应的法规来减少采矿活动，在这个优化问题中对可以进行的*函数评估*数量设定了限制。
- en: 'The central question in expensive black box optimization is this: What is a
    good way to decide where to evaluate this objective function so its optimum may
    be found at the end of the search? As we see in a later example, simple heuristics—such
    as random or grid search, which are approaches implemented by popular Python packages
    like scikit-learn—may lead to wasteful evaluations of the objective function and,
    thus, overall poor optimization performance. This is where Bayesian optimization
    comes into play.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 昂贵的黑盒优化中的中心问题是：如何决定在哪里评估这个目标函数，以便在搜索结束时找到其最优值？正如我们在后面的例子中看到的，简单的启发式方法——如随机或网格搜索，这些方法是流行的Python包（如scikit-learn）实现的——可能会导致对目标函数的浪费性评估，从而导致整体优化性能较差。这就是贝叶斯优化发挥作用的地方。
- en: 1.2 Introducing Bayesian optimization
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 介绍贝叶斯优化
- en: With the problem of expensive black box optimization in mind, we now introduce
    Bayesian optimization as a solution to this problem. This gives us a high-level
    idea of what Bayesian optimization is and how it uses probabilistic ML to optimize
    expensive black box functions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到昂贵的黑盒优化问题，现在我们介绍贝叶斯优化作为这个问题的解决方案。这给了我们一个高层次的关于贝叶斯优化是什么以及它如何使用概率机器学习来优化昂贵的黑盒函数的概念。
- en: Definition Bayesian optimization (BayesOpt) is an ML technique that simultaneously
    maintains a predictive model to learn about the objective function *and* makes
    decisions about how to acquire new data to refine our knowledge about the objective,
    using Bayesian probability and decision theory.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化（BayesOpt）的定义是一种机器学习技术，它同时维护一个预测模型来学习关于目标函数的信息*并且*通过贝叶斯概率和决策理论决定如何获取新数据来完善我们对目标的知识。
- en: By *data*, we mean input–output pairs, each mapping an input value to the value
    of the objective function at that input. This data is different, in the specific
    case of hyperparameter tuning, from the training data for the ML model we aim
    to tune.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*数据*，我们指的是输入输出对，每个对应一个输入值到该输入处的目标函数值的映射。在超参数调优的具体案例中，这些数据与我们旨在调整的机器学习模型的训练数据不同。
- en: In a BayesOpt procedure, we make decisions based on the recommendation of a
    BayesOpt algorithm. Once we have taken the BayesOpt-recommended action, the BayesOpt
    model is updated based on the result of that action and proceeds to recommend
    the next action to take. This process repeats until we are confident we have zeroed
    in on the optimal action.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯优化过程中，我们根据贝叶斯优化算法的建议做出决定。一旦我们采取了贝叶斯优化建议的行动，贝叶斯优化模型将根据该行动的结果进行更新，并继续推荐下一步要采取的行动。这个过程重复进行，直到我们有信心找到了最优行动。
- en: 'There are two main components of this workflow:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流程有两个主要组成部分：
- en: An ML model that learns from the observations we make and makes predictions
    about the values of the objective functions on unseen data points
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从我们所做的观察中学习并对未见数据点上的目标函数值进行预测的机器学习模型
- en: An optimization policy that makes decisions about where to make the next observation
    by evaluating the objective to locate the optimum
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过评估目标以定位最优值的优化策略
- en: We introduce each of these components in the following subsection.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下小节中介绍每个组件。
- en: 1.2.1 Modeling with a Gaussian process
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 用高斯过程建模
- en: BayesOpt works by first fitting a predictive ML model on the objective function
    we are trying to optimize—sometimes, this is called the surrogate model, as it
    acts as a surrogate between what we believe the function to be from our observations
    and the function itself. The role of this predictive model is very important as
    its predictions inform the decisions made by a BayesOpt algorithm and, therefore,
    directly affect optimization performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化首先在我们试图优化的目标函数上拟合一个预测的机器学习模型——有时，这被称为替代模型，因为它充当我们从观察中相信的函数和函数本身之间的替代。这个预测模型的作用非常重要，因为它的预测结果会影响贝叶斯优化算法的决策，并且直接影响优化性能。
- en: 'In almost all cases, a Gaussian process (GP) is employed for this role of the
    predictive model, which we examine in this subsection. On a high level, a GP,
    like any other ML model, operates under the tenet that similar data points produce
    similar predictions. GPs might not be the most popular class of models, compared
    to, say, ridge regression, decision trees, support vector machines, or neural
    networks. However, as we see time and again throughout this book, GPs come with
    a unique and essential feature: they do not produce point estimate predictions
    like the other models mentioned; instead, their predictions are in the form of
    *probability distributions*. Predictions, in the form of probability distributions
    or probabilistic predictions, are key in BayesOpt, allowing us to quantify uncertainty
    in our predictions, which, in turn, improves our risk–reward tradeoff when making
    decisions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎在所有情况下，高斯过程（GP）被用于这种预测模型的角色，我们在本小节中对此进行了研究。在高层次上，与任何其他机器学习模型一样，高斯过程（GP）的运行原则是相似的数据点产生相似的预测。与岭回归、决策树、支持向量机或神经网络等其他模型相比，GPs
    可能不是最受欢迎的模型类别。然而，正如我们在本书中一再看到的那样，GPs 带有一个独特且至关重要的特性：它们不像其他提到的模型那样产生点估计预测；相反，它们的预测是以概率分布的形式。以概率分布或概率预测的形式进行的预测在贝叶斯优化中是关键的，它使我们能够量化我们的预测不确定性，进而改善我们在做出决策时的风险-回报折衷。
- en: Let’s first see what a GP looks like when we train it on a dataset. As an example,
    say we are interested in training a model to learn from the dataset in table 1.3,
    which is visualized as black *x*s in figure 1.3.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们看看当我们在数据集上训练一个 GP 时它是什么样子的。比如说，我们有兴趣训练一个模型来从表 1.3 的数据集中学习，该数据集在图 1.3 中被可视化为黑色的
    *x*。
- en: Table 1.3 An example regression dataset corresponding to figure 1.3
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 对应于图 1.3 的一个示例回归数据集
- en: '| Training data point | Label |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据点 | 标签 |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1.1470 | 1.8423 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1.1470 | 1.8423 |'
- en: '| -4.0712 | 0.7354 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| -4.0712 | 0.7354 |'
- en: '| 0.9627 | 0.9627 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 0.9627 | 0.9627 |'
- en: '| 1.2471 | 1.9859 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1.2471 | 1.9859 |'
- en: '![](../../OEBPS/Images/01-03.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-03.png)'
- en: Figure 1.3 Non-Bayesian models, such as ridge regressors, make pointwise estimates,
    while GPs produce probability distributions as predictions. GPs thus offer a calibrated
    quantification of uncertainty, which is an important factor when making high-risk
    decisions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 非贝叶斯模型，比如岭回归器，做出的是点估计，而高斯过程则产生概率分布作为预测。因此，高斯过程提供了一个校准的不确定性量化，这是在做出高风险决策时的一个重要因素。
- en: We first fit a ridge regression model on this dataset and make predictions within
    a range of –5 and 5; the top panel of figure 1.3 shows these predictions. A *ridge
    regression model* is a modified version of a linear regression model, where the
    weights of the model are regularized so that small values are preferred to prevent
    overfitting. Each prediction made by this model at a given test point is a single-valued
    number, which does not capture our level of uncertainty about how the function
    we are learning from behaves. For example, given the test point at 2, this model
    simply predicts 2.2.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在这个数据集上拟合了一个岭回归模型，并在 -5 和 5 的范围内做出预测；图 1.3 的顶部面板显示了这些预测。*岭回归模型*是线性回归模型的改进版本，其中模型的权重被正则化，以便更偏爱较小的值，以防止过拟合。该模型在给定测试点时所做的每个预测都是一个单值数字，它并没有捕获我们对所学习函数行为的不确定性水平的认识。例如，给定
    2 的测试点，该模型简单地预测为 2.2。
- en: We don’t need to go into too much detail about the inner workings of this model.
    The point here is that a ridge regressor produces point estimates without a measure
    of uncertainty, which is also the case for many ML models, such as support vector
    machines, decision trees, and neural networks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要过多地了解这个模型的内部工作原理。关键在于岭回归器产生没有不确定性度量的点估计，这也是许多机器学习模型的情况，比如支持向量机、决策树和神经网络。
- en: How, then, does a GP make its predictions? As shown on the bottom panel of figure
    1.3, predictions by a GP are in the form of probability distributions (specifically,
    normal distributions). This means that at each test point, we have a mean prediction
    (the solid line) as well as what’s called the 95% credible interval, or CI (the
    shaded region).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，高斯过程是如何做出预测的呢？如图 1.3 的底部面板所示，高斯过程的预测是以概率分布的形式（具体来说，是正态分布）进行的。这意味着在每个测试点，我们有一个平均预测（实线）以及所谓的
    95% 置信区间（CI）（阴影区域）。
- en: Note that the acronym *CI* is often used to abbreviate *confidence interval*
    in frequentist statistics; throughout this book, I use *CI* exclusively to denote
    *credible interval*. Many things can be said about the technical differences between
    the two concepts, but on a high level, we can still think of this CI as a range
    in which it’s likely that a quantity of interest (in this case, the true value
    of the function we’re predicting) falls.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，“CI”这个缩写词在统计学的频率主义中常用来缩写“置信区间”（confidence interval）；在本书中，我只使用“CI”来指代“可信区间”（credible
    interval）。虽然这两个概念在技术上有许多不同之处，但从高层次上来看，我们仍然可以将本书中的CI视为一个区间，这个区间内有可能包含一个感兴趣的数量（在这种情况下，就是我们正在预测的函数的真实值）。
- en: GP vs. ridge regression
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GP vs. 岭回归
- en: Interestingly, when using the same covariance function (also known as the *kernel*),
    a GP and a ridge regression model produce the same predictions (mean predictions
    for the GP), as illustrated in figure 1.3\. We discuss covariance functions in
    greater depth in chapter 3\. This is to say that a GP receives all the benefits
    a ridge regression model possesses, while offering the additional CI predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当使用相同的协方差函数（也称为“核”）时，“高斯过程”（GP）和“岭回归模型”产生了相同的预测结果（对于GP来说是均值预测），如图1.3所示。我们会在第3章更深入地讨论协方差函数。这意味着，GP具有岭回归模型所有的好处，同时还提供了额外的CI预测。
- en: '![](../../OEBPS/Images/01-03-unnumb.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-03-unnumb.png)'
- en: Effectively, this CI measures our level of uncertainty about the value at each
    test location. If a location has a large predictive CI (at –2 or 4 in figure 1.3,
    for example), then there is a wider range of values that are probable for this
    value. In other words, we have greater uncertainty about this value. If a location
    has a narrow CI (0 or 2, in figure 1.3), then we are more confident about the
    value at this location. A nice feature of the GP is that for each point in the
    training data, the predictive CI is close to 0, indicating we don’t have any uncertainty
    about its value. This makes sense; after all, we already know what that value
    is from the training set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际测试位置上，这个CI有效地度量了我们对每个测试位置的价值的不确定性水平。如果一个位置的预测CI较大（比如图1.3中的-2或4），则这个值的可能值范围更广。换句话说，我们对这个值的确定性更低。如果一个位置的CI较小（图1.3中的0或2），则我们对这个位置的值更有信心。GP的一个很好的特点是，对于训练数据的每个点，预测CI接近于0，这表示我们对其值没有任何不确定性。这是有道理的；毕竟，我们已经从训练集中知道了该值。
- en: Noisy function evaluations
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 带噪音的函数评估
- en: While not the case in figure 1.3, it’s possible that the labels of the data
    points in our dataset are noisy. It’s very possible that in many situations in
    the real world, the process of observing data can be corrupted by noise. In these
    cases, we can further specify the noise level with the GP, and the CI at the observed
    data points will not collapse to 0 but, instead, to the specified noise level.
    This goes to show the flexibility modeling with GPs offers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在图1.3中不是这种情况，但是在我们的数据集中，数据点的标签可能是有噪声的。在实际世界中，观察数据的过程很可能会受到噪声的干扰。在这种情况下，我们可以使用GP进一步指定噪声水平，观察数据点的CI将不会降为0，而是降至指定的噪声水平。这表明了使用GP建模所具有的灵活性。
- en: This ability to assign a number to our level of uncertainty, which is called
    *uncertainty quantification*, is quite useful in any high-risk decision-making
    procedure, such as BayesOpt. Imagine, again, the scenario in section 1.1, where
    we tune the number of layers in our neural net, and we only have time to try out
    one more model. Let’s say that after being trained on those data points, a GP
    predicts that 25 layers will give a mean accuracy of 0.85, and the corresponding
    95% CI is 0.81 to 0.89\. On the other hand, with 15 layers, the GP predicts our
    accuracy will also be 0.85 on average, but the 95% CI is 0.84 to 0.86\. Here,
    it’s quite reasonable to prefer 15 layers, even though both numbers have the same
    expected value. This is because we are more *certain* 15 will give us a good result.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将我们对不确定性的水平进行量化的能力（称为“不确定性量化”）在任何高风险的决策过程中都非常有用，比如贝叶斯优化。在1.1节中出现的情景再次设想一下，我们调整神经网络中的层数，并且只有时间尝试一个更多的模型。假设在那些数据点上训练之后，GP预测25层的平均精度将为0.85，相应的95%
    CI为0.81至0.89。另一方面，对于15层，GP预测我们的精度平均也是0.85，但是95% CI为0.84至0.86。在这种情况下，即使这两个数字具有相同的期望值，选择15层是相当合理的，因为我们更“确定”15层将给我们带来好的结果。
- en: 'To be clear, a GP does not make any decision for us, but it does offer us a
    means to do so with its probabilistic predictions. Decision-making is left to
    the second part of the BayesOpt framework: the policy.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 清楚地说，GP不会为我们做出任何决定，但它确实通过其概率预测为我们提供了一种方法。决策留给BayesOpt框架的第二部分：策略。
- en: 1.2.2 Making decisions with a BayesOpt policy
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用BayesOpt策略做决策
- en: In addition to a GP as a predictive model, in BayesOpt, we also need a decision-making
    procedure, which we explore in this subsection. This is the second component of
    BayesOpt, which takes in the predictions made by the GP model and reasons about
    how to best evaluate the objective function so the optimum may be located efficiently.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为预测模型的GP之外，在BayesOpt中，我们还需要一个决策过程，我们将在本小节中探讨这个问题。这是BayesOpt的第二个组成部分，它接受GP模型所做的预测，并推理如何最好地评估目标函数，以便有效地找到最优解。
- en: As mentioned previously, a prediction with a 95% CI of 0.84 to 0.86 is considered
    better than a 95% CI of 0.81 to 0.89, especially if we only have one more try.
    This is because the former is more of a sure thing, guaranteed to get us a good
    result. How should we make this decision in a more general case in which the two
    points might have different predictive means and predictive levels of uncertainty?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，95% CI为0.84至0.86的预测要比95% CI为0.81至0.89的预测更好，特别是如果我们只有一次尝试的机会。这是因为前者更像是一件确定的事情，保证为我们带来一个好结果。在两个点的预测均值和预测不确定性可能不同的更一般情况下，我们应该如何做出这个决定？
- en: 'This is exactly what a BayesOpt policy helps us do: quantify the usefulness
    of a point, given its predictive probability distribution. The job of a policy
    is to take in the GP model, which represents our belief about the objective function,
    and assign each data point with a score denoting how helpful that point is in
    helping us identify the global optimum. This score is sometimes called the *acquisition
    score*. Our job is then to pick out the point that maximizes this acquisition
    score and evaluate the objective function at that point.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是BayesOpt策略帮助我们做的事情：量化一个点的有用性，考虑到其预测概率分布。策略的工作是接受GP模型，该模型代表我们对目标函数的信念，并为每个数据点分配一个分数，表示该点在帮助我们识别全局最优解方面的帮助程度。这个分数有时被称为*获取分数*。我们的工作是选择最大化这个获取分数的点，并在该点评估目标函数。
- en: We see the same GP in figure 1.4 that we have in figure 1.3, where the bottom
    panel shows the plot of how a particular BayesOpt policy called *Expected Improvement*
    scores each point on the *x*-axis between –5 and 5 (which is our search space).
    We learn what this name means and how the policy scores data points in chapter
    4\. For now, let’s just keep in mind that if a point has a large acquisition score,
    this point is valuable for locating the global optimum.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图1.4中看到与图1.3中相同的GP，在底部面板中显示了一个名为*Expected Improvement*的特定BayesOpt策略如何在*x*-轴上的每个点（在我们的搜索空间内的-5到5之间）得分。我们将在第4章学习这个名称的含义以及该策略如何对数据点进行评分。现在，让我们先记住，如果一个点具有较大的获取分数，这个点对于定位全局最优解是有价值的。
- en: '![](../../OEBPS/Images/01-04.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-04.png)'
- en: Figure 1.4 A BayesOpt policy scores each individual data point by its usefulness
    in locating the global optimum. The policy prefers high predictive values (where
    the payoff is more likely) as well as high uncertainty (where the payoff may be
    large).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 BayesOpt策略通过其在定位全局最优解中的有用性对每个单独的数据点进行评分。该策略倾向于高预测值（其中回报更有可能）以及高不确定性（其中回报可能较大）。
- en: In figure 1.4, the best point is around 1.8, which makes sense, as according
    to our GP in the top panel, that’s also where we achieve the highest predictive
    mean. This means we will then pick this point at 1.8 to evaluate our objective,
    hoping to improve from the highest value we have collected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.4中，最佳点在1.8左右，这是有道理的，因为根据我们在顶部面板中的GP，在那里我们也实现了最高的预测均值。这意味着我们将选择在1.8处评估我们的目标，希望从我们收集到的最高值中得到改进。
- en: We should note that this is not a one-time procedure but, instead, a *learning
    loop*. At each iteration of the loop, we train a GP on the data we have observed
    from the objective, run a BayesOpt policy on this GP to obtain a recommendation
    that will hopefully help us identify the global optimum, make an observation at
    the recommended location, add the new point to our training data, and repeat the
    entire procedure until we reach some condition for terminating. Things might be
    getting a bit confusing, so it is time for us to take a step back and look at
    the bigger picture of BayesOpt.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，这不是一个一次性的过程，而是一个*学习循环*。在循环的每一次迭代中，我们根据我们从目标中观察到的数据训练一个高斯过程（GP），在这个高斯过程上运行贝叶斯优化策略，以得到一个希望帮助我们确定全局最优的建议，然后在推荐位置进行观察，将新点添加到我们的训练数据中，并重复整个过程，直到达到某个终止条件。事情可能变得有点混乱，所以是时候退后一步，看看贝叶斯优化的大局了。
- en: Connection to design of experiments
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与实验设计的联系
- en: The description of BayesOpt at this point might remind you of the concept of
    *design of experiments* (DoE) in statistics, which sets out to solve the same
    problem of optimizing an objective function by tweaking the controllable settings.
    Many connections exist between these two techniques, but BayesOpt can be seen
    as a more general approach than DoE that is powered by the ML model GP.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，贝叶斯优化的描述可能让你想起了统计学中的*实验设计*（DoE）的概念，它旨在通过调整可控设置来解决优化目标函数的问题。这两种技术之间存在许多联系，但是贝叶斯优化可以被看作是一种更一般的方法，它由机器学习模型高斯过程（GP）驱动。
- en: 1.2.3 Combining the GP and the optimization policy to form the optimization
    loop
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 组合高斯过程和优化策略形成优化循环
- en: In this subsection, we tie in everything we have described so far and make the
    procedure more concrete. We see the BayesOpt workflow as a whole and better understand
    how the various components work with each other.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们总结了我们迄今为止所描述的内容，并使过程更加具体。我们全面地看到了贝叶斯优化的工作流程，并更好地理解了各个组成部分是如何相互配合的。
- en: 'We start with an initial dataset, like those in tables 1.1, 1.2, and 1.3\.
    Then, the BayesOpt workflow is visualized in figure 1.5, which is summarized as
    follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个初始数据集开始，就像表1.1、1.2和1.3中的那样。然后，贝叶斯优化的工作流程在图1.5中进行了可视化，总结如下：
- en: We train a GP model on this set, which gives us a belief about what our objective
    looks like everywhere based on what we have observed from the training data. This
    belief is represented by the solid curve and shaded region, like those in figures
    1.3 and 1.4.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这个数据集上训练了一个高斯过程（GP）模型，根据我们从训练数据中观察到的内容给出了对我们的目标在每个地方的信念。这种信念由实线和阴影区域表示，就像图1.3和1.4中的那样。
- en: A BayesOpt policy then takes in this GP and scores each point in the domain
    in terms of how valuable the point is in helping us locate the global optimum.
    This is indicated by the bottom curve, as in figure 1.4.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，贝叶斯优化策略接收这个高斯过程，并根据该点在域中的价值对每个点进行评分，这如图1.4中的下曲线所示。
- en: The point that maximizes this score is the point we will choose to evaluate
    the objective at next and is then added to our training dataset.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大化该分数的点是我们选择下一个要评估目标的点，然后将其添加到我们的训练数据集中。
- en: The process repeats until we cannot afford to evaluate the objective anymore.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，直到我们无法再评估目标。
- en: '![](../../OEBPS/Images/01-05.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![BayesOpt循环](../../OEBPS/Images/01-05.png)'
- en: Figure 1.5 The BayesOpt loop, which combines a GP for modeling and a policy
    for decision-making. This complete workflow may now be used to optimize black
    box functions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 贝叶斯优化循环，结合了高斯过程（GP）建模和决策制定的策略。现在可以使用这个完整的工作流程来优化黑盒函数。
- en: Unlike a supervised learning task in which we just fit a predictive model on
    a training dataset and make predictions on a test set (which only encapsulates
    steps 1 and 2), a BayesOpt workflow is what is typically called *active learning*.
    Active learning is a subfield in ML in which we get to decide which data points
    our model learns from, and that decision-making process is, in turn, informed
    by the model itself.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习任务不同，我们只需在训练数据集上拟合一个预测模型并在测试集上进行预测（只包括步骤1和2），贝叶斯优化工作流程通常被称为*主动学习*。主动学习是机器学习中的一个子领域，我们可以决定我们的模型从哪些数据点中学习，而这个决策过程则由模型本身来决定。
- en: As we have said, the GP and the policy are the two main components of this BayesOpt
    procedure. If the GP does not model the objective well, then we will not be able
    to do a good job of informing the policy of the information contained in the training
    data. On the other hand, if the policy is not good at assigning high scores to
    “good” points and low scores to “bad” points (where *good* means helpful at locating
    the global optimum), then our subsequent decisions will be misguided and will
    most likely achieve bad results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，GP 和政策是这个 BayesOpt 过程的两个主要组成部分。如果 GP 没有很好地对客观函数进行建模，那么我们将无法很好地将训练数据中的信息通知给政策。另一方面，如果政策不能很好地给“好”点分配高分和给“坏”点分配低分（其中*好*意味着有助于找到全局最优解），那么我们的后续决策将是错误的，并且很可能会取得糟糕的结果。
- en: In other words, without a good predictive model, such as a GP, we won’t be able
    to make good predictions with calibrated uncertainty. Without a policy, we can
    make good *predictions*, but we won’t make good *decisions*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果没有一个好的预测模型，比如一个 GP，我们就无法通过校准的不确定性做出良好的预测。没有政策，我们可以做出良好的*预测*，但我们不会做出良好的*决策*。
- en: An example we consider multiple times throughout this book is weather forecasting.
    Imagine a scenario in which you are trying to decide whether to bring an umbrella
    with you before leaving the house to go to work, and you look at the weather forecasting
    app on your phone.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中多次考虑的一个例子是天气预报。想象一下这样一个情景，你要决定在离开家去上班前是否带伞，并查看手机上的天气预报应用程序。
- en: Needless to say, the predictions made by the app need to be accurate and reliable
    so you can confidently base your decisions on them. An app that always predicts
    sunny weather just won’t do. Further, you need a sensible way to make decisions
    based on these predictions. Never bringing an umbrella, regardless of how likely
    rainy weather is, is a bad decision-making policy and will get you in trouble
    when it does rain. On the other hand, always bringing an umbrella, even with a
    100% chance of sunny weather, is also not a smart decision. You want to *adaptively*
    decide to bring your umbrella, based on the weather forecast.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，应用程序的预测需要准确可靠，这样你就可以自信地根据它们做出决定。一个总是预测晴天的应用程序是不够的。此外，你需要一种明智的方式来根据这些预测做出决策。无论多么可能下雨，永远不带伞都是一个糟糕的决策政策，当下雨时会让你陷入麻烦。另一方面，即使天气预报有
    100% 的晴天可能性，也不是一个明智的决定。你希望根据天气预报*自适应地*决定是否带伞。
- en: Adaptively making decisions is what BayesOpt is all about, and to do it effectively,
    we need both a good predictive model and a good decision-making policy. Care needs
    to go into both components of the framework; this is why the two main parts of
    the book following this chapter cover modeling with GPs and decision-making with
    BayesOpt policies, respectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应地做出决策是 BayesOpt 的核心，为了有效地实现这一点，我们需要一个好的预测模型和一个好的决策政策。在这个框架的两个组成部分都需要注意；这就是为什么本章后面的两个主要部分分别涵盖了用
    GP 进行建模和用 BayesOpt 政策进行决策。
- en: 1.2.4 BayesOpt in action
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 BayesOpt 的实际应用
- en: At this point, you might be wondering whether all of this heavy machinery really
    works—or works better than some simple strategy like random sampling. To find
    out, let’s take a look at a “demo” of BayesOpt on a simple function. This will
    also be a good way for us to move away from the abstract to the concrete and tease
    out what we are able to do in future chapters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能想知道所有这些复杂的机器真的是否有效—或者是否比一些简单的策略如随机抽样更有效。为了找出答案，让我们看一下 BayesOpt 在一个简单函数上的“演示”。这也是我们从抽象到具体的好方法，并揭示了我们在后续章节中能做什么。
- en: Let’s say the black box objective function we are trying to optimize (specifically,
    in this case, maximize) is the one-dimensional function in figure 1.6, defined
    from –5 to 5\. Again, this picture is only for our reference; in black box optimization,
    we, in fact, do not know the shape of the objective. We see the objective has
    a couple of local maxima around –5 (roughly, –2.4 and 1.5) but the global maximum
    is on the right at approximately 4.3\. Let’s also assume we are allowed to evaluate
    the objective function a maximum of 10 times.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们试图优化的黑盒客观函数（特别是在这种情况下，最大化）是图 1.6 中的一维函数，从 -5 到 5 定义。同样，这个图片仅供参考；在黑盒优化中，我们实际上不知道客观函数的形状。我们看到客观函数在
    -5（大约 -2.4 和 1.5）附近有几个局部极大值，但全局最大值在右侧大约是 4.3。此外，假设我们被允许最多评估客观函数 10 次。
- en: '![](../../OEBPS/Images/01-06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-06.png)'
- en: Figure 1.6 The objective function that is to be maximized, where random search
    wastes resources on unpromising regions
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 要最大化的目标函数，在随机搜索中浪费资源于不利区域
- en: Before we see how BayesOpt solves this optimization problem, let’s look at two
    baseline strategies. The first is a random search, where we uniformly sample between
    –5 and 5; whatever points we end up with are the locations we will evaluate the
    objective at. Figure 1.6 is the result of one such scheme. The point with the
    highest value found here is at roughly *x* = 4, having the value of *f*(*x*) =
    3.38.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到贝叶斯优化如何解决这个优化问题之前，让我们看看两种基准策略。第一种是随机搜索，在–5到5之间均匀采样；我们得到的任何点都是我们将评估目标的位置。图1.6是这样一个方案的结果。这里找到的价值最高的点大约在*x*
    = 4处，其值为*f*(*x*) = 3.38。
- en: How random search works
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索的工作原理
- en: Random search involves choosing points uniformly at random within the domain
    of our objective function. That is, the probability that we end up at a point
    within the domain is equal to the probability that we end up at any other point.
    Instead of uniform sampling, we can draw these random samples from a non-uniform
    distribution if we believe there are important regions in the search space we
    should give more focus to. However, this non-uniform strategy requires knowing
    which regions are important before starting the search.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索涉及在我们的目标函数域内均匀随机选择点。也就是说，我们最终到达域内某点的概率等于我们最终到达其他任何点的概率。如果我们认为搜索空间中有重要区域应该更加关注，我们可以从非均匀分布中抽取这些随机样本，而不是均匀采样。然而，这种非均匀策略需要在开始搜索之前知道哪些区域是重要的。
- en: Something you might find unsatisfactory about these randomly sampled points
    is that many of them happen to fall into the region around 0\. Of course, it’s
    only by chance that many random samples cluster around 0, and in another instance
    of the search, we might find many samples in another area. However, the possibility
    remains that we could waste valuable resources inspecting a small region of the
    function with many evaluations. Intuitively, it is more beneficial to spread out
    our evaluations so we learn more about the objective function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得不满意的是，这些随机抽样的点中有许多恰好落入0周围的区域。当然，许多随机样本聚集在0周围只是偶然的，而在另一个搜索实例中，我们可能会发现在另一个区域有许多样本。然而，我们仍然可能浪费宝贵的资源来检查函数的一个小区域，其中包含许多评估。直觉上，扩展我们的评估更有利于我们了解目标函数。
- en: 'This idea of *spreading out* evaluations leads us to the second baseline: grid
    search. Here, we divide our search space into evenly spaced segments and evaluate
    at the endpoints of those segments, as in figure 1.7.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*扩散*评估的想法将我们带到了第二个基准：网格搜索。在这里，我们将搜索空间划分为均匀间隔的段，并在这些段的端点进行评估，就像图1.7所示。
- en: '![](../../OEBPS/Images/01-07.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-07.png)'
- en: Figure 1.7 Grid search is still inefficient at narrowing down a good region.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 网格搜索仍然无法有效缩小好的区域。
- en: The best point from this search is the very last point on the right at 5, evaluating
    at roughly 4.86\. This is better than random search but is still missing the actual
    global optimum.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这次搜索中的最佳点是最右边的最后一个点，在5处进行评估，大约为4.86。这比随机搜索更好，但仍然错过了实际的全局最优点。
- en: Now, we are ready to look at BayesOpt in action! BayesOpt starts off with a
    randomly sampled point, just like random search, shown in figure 1.8.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备看贝叶斯优化如何运作！贝叶斯优化从一个随机抽样的点开始，就像随机搜索一样，如图1.8所示。
- en: '![](../../OEBPS/Images/01-08.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/01-08.png)'
- en: Figure 1.8 The start of BayesOpt is similar to random search.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 贝叶斯优化的开始与随机搜索相似。
- en: The top panel of figure 1.8 represents the GP trained on the evaluated point,
    while the bottom panel shows the score computed by the Expected Improvement policy.
    Remember, this score tells us how much we should value each location in our search
    space, and we should pick the one that gives the highest score to evaluate next.
    Interestingly enough, our policy at this point tells us that almost the entire
    range between –5 and 5 we’re searching within is promising (except for the region
    around 1, where we have made a query). This should make intuitive sense, as we
    have only seen one data point, and we don’t yet know how the rest of the objective
    function looks in other areas. Our policy tells us we should explore more! Let’s
    now look at the state of our model from this first query to the fourth query in
    figure 1.9.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 的顶部面板表示对评估点进行训练的高斯过程，而底部面板显示了由期望改进策略计算的分数。请记住，这个分数告诉我们应该如何评价我们搜索空间中的每个位置，我们应该选择下一个评估分数最高的位置。有趣的是，在这一点上，我们的策略告诉我们，我们搜索的-5到5之间的几乎整个范围都很有前景（除了围绕1的区域，我们已经进行了一次查询）。这应该是直观的，因为我们只看到了一个数据点，而且我们还不知道其他区域的目标函数是什么样子的。我们的策略告诉我们我们应该探索更多！现在让我们看看从第一个查询到第四个查询时我们模型的状态如何在图
    1.9 中。
- en: '![](../../OEBPS/Images/01-09.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9](../../OEBPS/Images/01-09.png)'
- en: Figure 1.9 After four queries, we have identified the second-best optimum.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 在四次查询之后，我们确定了第二个最佳点。
- en: Three out of four queries are concentrating around the point 1, where there
    is a local optimum, and we also see that our policy is suggesting we query yet
    another point in this area next. At this point, you might be worried that we will
    get stuck in this locally optimal area and fail to break out to find the true
    optimum, but we will see that this is not the case. Let’s fast-forward to the
    next two iterations in figure 1.10.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 四次查询中有三次集中在点 1，这里有一个局部最优点，我们还看到我们的策略建议我们下一步查询这个区域的另一个点。此时，你可能担心我们会陷入这个局部最优区域无法摆脱，无法找到真正的最优点，但我们会看到情况并非如此。让我们快进到图
    1.10 中的接下来两次迭代。
- en: '![](../../OEBPS/Images/01-10.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10](../../OEBPS/Images/01-10.png)'
- en: Figure 1.10 After exploring a local optimum sufficiently, we are encouraged
    to look at other areas.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 在充分探索局部最优点后，我们被鼓励看看其他区域。
- en: After having five queries to scope out this locally optimal region, our policy
    decides there are other, more promising regions to explore—namely, the one to
    the left around –2 and the one to the right around 4\. This is very reassuring,
    as it shows that once we have explored a region enough, BayesOpt does not get
    stuck in that region. Let’s now see what happens after eight queries in figure
    1.11.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这个局部最优区域进行五次查询后，我们的策略决定有其他更有前景的区域可供探索——即左边约为-2和右边约为4的区域。这非常令人放心，因为它表明一旦我们足够探索一个区域，贝叶斯优化就不会陷入那个区域。现在让我们看看图
    1.11 中进行八次查询后会发生什么。
- en: '![](../../OEBPS/Images/01-11.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.11](../../OEBPS/Images/01-11.png)'
- en: Figure 1.11 BayesOpt successfully ignores the large region on the left.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 贝叶斯优化成功地忽略了左边的大区域。
- en: Here, we have observed two more points on the right, which update both our GP
    model and our policy. Looking at the mean function (the solid line, representing
    the most likely prediction), we see that it almost matches the true objective
    function from 4 to 5\. Further, our policy (the bottom curve) is now pointing
    very close to the global optimum and basically no other area. This is interesting
    because we have not thoroughly inspected the area on the left (we only have one
    observation to the left of 0), but our model believes that regardless of what
    the function looks like in that area, it is not worth investigating compared to
    the current region. This is, in fact, true in our case.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们观察到了右边的另外两个点，这些点更新了我们的高斯过程模型和我们的策略。观察均值函数（实线，表示最可能的预测），我们看到它几乎与真实目标函数从
    4 到 5 的情况完全匹配。此外，我们的策略（底部曲线）现在非常接近全局最优点，基本上没有其他区域。这很有趣，因为我们并没有彻底检查左边的区域（我们只有一个观察到左边的
    0），但我们的模型认为与当前区域相比，无论那个区域的函数长什么样子，都不值得调查。在我们的情况下，这实际上是正确的。
- en: Finally, at the end of the search with 10 queries, our workflow is now visualized
    in figure 1.12\. There is now little doubt that we have identified the global
    optimum around 4.3.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在进行了 10 次查询的搜索结束时，我们的工作流现在在图 1.12 中可视化。现在几乎没有疑问，我们已经确定了约为 4.3 的全局最优点。
- en: '![](../../OEBPS/Images/01-12.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12](../../OEBPS/Images/01-12.png)'
- en: Figure 1.12 BayesOpt has found the global optimum at the end of the search.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 贝叶斯优化在搜索结束时找到了全局最优点。
- en: This example has clearly shown us that BayesOpt can work a lot better than random
    search and grid search. This should be a very encouraging sign for us considering
    that the latter two strategies are what many ML practitioners use when faced with
    a hyperparameter tuning problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子清楚地向我们展示了贝叶斯优化比随机搜索和网格搜索要好得多。这对我们来说应该是一个非常鼓舞人心的迹象，因为后两种策略是许多机器学习实践者在面临超参数调优问题时使用的方法。
- en: For example, scikit-learn is one of the most popular ML libraries in Python,
    and it offers the `model_selection` module for various model selection tasks,
    including hyperparameter tuning. However, random search and grid search are the
    only hyperparameter tuning methods implemented in the module. In other words,
    if we are indeed tuning our hyperparameters with random or grid search, there
    is a lot of headroom to do better.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，scikit-learn是Python中最流行的机器学习库之一，它提供了`model_selection`模块用于各种模型选择任务，包括超参数调优。然而，随机搜索和网格搜索是该模块实现的唯一超参数调优方法。换句话说，如果我们确实使用随机或网格搜索调整超参数，我们有很大的提升空间。
- en: 'Overall, employing BayesOpt may result in a drastic improvement in optimization
    performance. We can take a quick look at a few real-world examples:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，使用BayesOpt可能会导致优化性能显著提高。我们可以快速看几个真实世界的例子：
- en: A 2020 research paper entitled “Bayesian Optimization is Superior to Random
    Search for Machine Learning Hyperparameter Tuning” ([https://arxiv.org/pdf/2104.10201.pdf](https://arxiv.org/pdf/2104.10201.pdf)),
    which was the result of a joint study by Facebook, Twitter, Intel, and others,
    found that BayesOpt was extremely successful across many hyperparameter tuning
    tasks.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一份名为“贝叶斯优化优于随机搜索的机器学习超参数调优”的2020年研究论文（[https://arxiv.org/pdf/2104.10201.pdf](https://arxiv.org/pdf/2104.10201.pdf)）是Facebook、Twitter、英特尔等公司的联合研究成果，发现BayesOpt在许多超参数调优任务中都非常成功。
- en: Frances Arnold, Nobel Prize winner in 2018 and professor at Caltech, uses BayesOpt
    in her research to guide the search for enzymes efficient at catalyzing desirable
    chemical reactions.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弗朗西斯·阿诺德（2018年诺贝尔奖获得者，加州理工学院教授）在她的研究中使用BayesOpt来引导寻找高效催化理想化学反应的酶。
- en: A study entitled “Design of Efficient Molecular Organic Light-Emitting Diodes
    by a High-Throughput Virtual Screening and Experimental Approach” ([https://www.nature.com/articles/nmat4717](https://www.nature.com/articles/nmat4717))
    published in *Nature* applied BayesOpt to the problem of screening for molecular
    organic light-emitting diodes (an important type of molecules) and observed a
    large improvement in efficiency.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一项名为“通过高通量虚拟筛选和实验方法设计高效的分子有机发光二极管”的研究（[https://www.nature.com/articles/nmat4717](https://www.nature.com/articles/nmat4717)）发表在*自然*上，将BayesOpt应用于分子有机发光二极管（一种重要类型的分子）的筛选问题，并观察到效率大幅提高。
- en: And there are many more of these examples out there.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多类似的例子。
- en: When not to use BayesOpt
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用BayesOpt的情况
- en: It’s also important to know when the problem setting isn’t appropriate and when
    *not* to use BayesOpt. As we have said, BayesOpt is useful when our limited resources
    prevent us from evaluating the objective function many times. If this is not the
    case and evaluating the objective is cheap, we have no reason to be frugal with
    how we observe the objective function.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要知道问题设置不合适的情况以及何时*不*使用BayesOpt。正如我们所说，当我们的有限资源阻止我们多次评估目标函数时，BayesOpt是有用的。如果不是这种情况，评估目标函数是廉价的，我们没有理由在观察目标函数时吝啬。
- en: Here, if we can inspect the objective thoroughly across a dense grid, that will
    ensure the global optimum is located. Otherwise, other strategies, such as the
    DIRECT algorithm or evolutionary algorithms, which are algorithms that often excel
    at optimization when evaluations are cheap, may be used. Further, if information
    about the gradient of the objective is available, gradient-based algorithms will
    be better suited.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够彻底检查密集网格上的目标，那将确保找到全局最优解。否则，可能会使用其他策略，例如DIRECT算法或进化算法，这些算法在评估成本较低时通常擅长优化。此外，如果目标梯度的信息可用，梯度算法将更适合。
- en: I hope this chapter was able to whet your appetite and get you excited for what’s
    to come. In the next section, we summarize the key skills you will be learning
    throughout the book.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一章能激发你的兴趣，让你对即将发生的事情感到兴奋。在下一节中，我们将总结你将在本书中学到的关键技能。
- en: 1.3 What will you learn in this book?
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 你将在本书中学到什么？
- en: 'This book provides a deep understanding of the GP model and the BayesOpt task.
    You will learn how to implement a BayesOpt pipeline in Python using state-of-the-art
    tools and libraries. You will also be exposed to a wide range of modeling and
    optimization strategies when approaching a BayesOpt task. By the end of the book,
    you will be able to do the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本书深入理解 GP 模型和 BayesOpt 任务。您将学习如何使用最先进的工具和库在 Python 中实现 BayesOpt 流水线。您还将接触到一系列建模和优化策略，当处理
    BayesOpt 任务时。到本书结束时，您将能够做到以下几点：
- en: Implement high-performance GP models using GPyTorch, the premiere GP modeling
    tool in Python; visualize and evaluate their predictions; choose appropriate parameters
    for these models; and implement extensions, such as variational GPs and Bayesian
    neural networks, to scale to big data
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPyTorch 实现高性能的 GP 模型，这是 Python 中的首选 GP 建模工具；可视化和评估它们的预测；为这些模型选择适当的参数；并实现扩展，例如变分
    GP 和贝叶斯神经网络，以适应大数据
- en: Implement a wide range of BayesOpt policies using the state-of-the-art BayesOpt
    library BoTorch, which integrates nicely with GPyTorch, and inspect as well as
    understand their decision-making strategies
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最先进的 BayesOpt 库 BoTorch 实现各种 BayesOpt 策略，并与 GPyTorch 很好地集成，并检查以及理解它们的决策策略
- en: Approach different specialized settings, such as batch, constrained, and multiobjective
    optimization, using the BayesOpt framework
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BayesOpt 框架处理不同的专业化设置，例如批处理、约束和多目标优化
- en: Apply BayesOpt to a real-life task, such as tuning the hyperparameters of an
    ML model
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 BayesOpt 应用于真实任务，例如调整机器学习模型的超参数
- en: Further, we use real-world examples and data in the exercises to consolidate
    what we learn in each chapter. Throughout the book, we run our algorithms on the
    same dataset in many different settings so we can compare and analyze the different
    approaches taken.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步地，我们在练习中使用真实世界的例子和数据来巩固我们在每一章学到的知识。在整本书中，我们在许多不同的环境中运行我们的算法在相同的数据集上，以便我们可以比较和分析不同的方法。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Many problems in the real world may be cast as expensive black box optimization
    problems. In these problems, we only observe the function values without any additional
    information. Further, observing one function value is expensive, rendering many
    cost-blind optimization algorithms unusable.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界中的许多问题可以被看作是昂贵的黑盒优化问题。在这些问题中，我们只观察到函数值，没有任何额外的信息。此外，观察一个函数值是昂贵的，使得许多盲目的优化算法无法使用。
- en: BayesOpt is an ML technique that solves this black box optimization problem
    by designing intelligent evaluations of the objective function so the optimum
    may be found as quickly as possible.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BayesOpt 是一种机器学习技术，通过设计目标函数的智能评估来解决这个黑盒优化问题，以便尽快找到最优解。
- en: In BayesOpt, a GP acts as a predictive model, predicting what the value of the
    objective function is at a given location. A GP produces not only a mean prediction
    but also a 95% CI, representing uncertainty via normal distributions.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BayesOpt 中，GP 充当预测模型，预测给定位置的目标函数的值。GP 不仅生成均值预测，还通过正态分布表示不确定性的 95% CI。
- en: To optimize a black box function, a BayesOpt policy iteratively makes decisions
    about where to evaluate the objective function. The policy does this by quantifying
    how helpful each data point is in terms of optimization.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要优化黑盒函数，BayesOpt 策略会迭代地决定在哪里评估目标函数。该策略通过量化每个数据点在优化方面的帮助程度来实现这一点。
- en: A GP and a policy go hand in hand in BayesOpt. The former is needed to make
    good predictions, and the latter is needed to make good decisions.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BayesOpt 中，GP 和策略是相辅相成的。前者用于进行良好的预测，后者用于做出良好的决策。
- en: By making decisions in an adaptive manner, BayesOpt is better at optimization
    than random search or grid search, which are often used as the default strategies
    in black box optimization problems.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以自适应的方式做出决策，BayesOpt 在优化方面比随机搜索或网格搜索更好，后者通常用作黑盒优化问题中的默认策略。
- en: BayesOpt has seen significant success in hyperparameter tuning in ML and other
    scientific applications, such as drug discovery.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BayesOpt 在机器学习和其他科学应用中，如药物发现中的超参数调优中取得了显著的成功。
