- en: 1 An introduction to deep learning systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 深度学习系统简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Defining a deep learning system
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义深度学习系统
- en: The product development cycle and how a deep learning system supports it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品开发周期及深度学习系统如何支持其
- en: An overview of a basic deep learning system and its components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本深度学习系统及其组成部分概述
- en: Differences between building a deep learning system and developing models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搭建深度学习系统与开发模型之间的差异
- en: This chapter will prepare you with a big-picture mental model of a deep learning
    system. We will review some definitions and provide a reference system architecture
    design and a complete sample implementation of the architecture. We hope this
    mental model will prime you to see how the rest of the chapters, which address
    each system component in detail, fit into the whole picture.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为您提供一个深度学习系统的全貌思维模型。我们将回顾一些定义，并提供一个参考系统架构设计和该架构的完整示例实现。我们希望这个思维模型能够让您看到其他章节如何详细介绍每个系统组件，并将其融入整体图景。
- en: 'To begin this chapter, we will discuss an even bigger picture beyond the deep
    learning system: something we call *the deep learning development cycle*. This
    cycle outlines the various roles and stages involved in bringing products based
    on deep learning to market. The model and the platform do not exist in a vacuum;
    they affect and are affected by product management, market research, production,
    and other stages. We believe that engineers design better systems when they understand
    this cycle and know what each team does and what it needs to do its job.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，我们将讨论一个超越深度学习系统的更大图景：我们称之为*深度学习开发周期*。这个周期概述了基于深度学习的产品推向市场所涉及的各种角色和阶段。模型和平台并不是孤立存在的；它们受产品管理、市场调研、生产和其他阶段的影响，也会影响这些阶段。我们相信，当工程师了解这个周期以及每个团队的工作内容和所需时，他们设计的系统会更好。
- en: In section 1.2, we start our discussion of deep learning system design with
    a sample architecture of a typical system that can be adapted for designing your
    own deep learning system. The components described in this section will be explored
    in greater detail in their own chapters. Finally, we will emphasize the differences
    between developing a model and developing a deep learning system. This distinction
    is often a point of confusion, so we want to clear it up right away.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.2节中，我们将从典型系统的示例架构开始讨论深度学习系统设计。本节描述的组件将在各自的章节中进行更详细的探讨。最后，我们将强调开发模型与开发深度学习系统之间的区别。这种区别常常是一个让人困惑的焦点，所以我们想要立即澄清。
- en: After reading this introductory chapter, you will have a solid understanding
    of the deep learning landscape. You will also be able to start creating your own
    deep learning system design, as well as understand existing designs and how to
    use and extend them, so you don’t have to build everything from scratch. As you
    continue reading this book, you will see how everything connects and works together
    as a deep learning system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完这个入门章节之后，您将对深度学习的概况有了扎实的理解。您还将能够开始创建自己的深度学习系统设计，并理解现有设计以及如何使用和扩展它们，这样您就不必从头开始构建一切。随着您继续阅读本书，您将看到一切是如何连接和共同作为一个深度学习系统运作的。
- en: Terminology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 术语
- en: Before we proceed with the rest of the chapter (and the rest of the book), let’s
    define and clarify a few terms that we use throughout the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续本章（以及本书的其余部分）之前，让我们定义和澄清本书中始终使用的一些术语。
- en: Deep learning vs. machine learning
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与机器学习的比较
- en: Deep learning is machine learning, but it is considered an evolution of machine
    learning. Machine learning, by definition, is an application of artificial intelligence
    that includes algorithms that parse data, learn from that data, and then apply
    what it has learned to make informed decisions. Deep learning is a special form
    of machine learning that uses a programmable neural network as the algorithm to
    learn from data and make accurate decisions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一种，但被认为是机器学习的一种演变。机器学习按定义是人工智能的一种应用，包括解析数据、从数据中学习，然后应用所学内容做出明智决策的算法。深度学习是机器学习的一种特殊形式，它使用可编程神经网络作为算法，从数据中学习并做出准确的决策。
- en: Although this book primarily focuses on teaching you how to build the system
    or infrastructure to facilitate deep learning development (all the examples are
    neural network algorithms), the design and project development concepts we discuss
    are all applicable to machine learning as well. So, in this book we use the terms
    *deep learning* and *machine learning* somewhat interchangeably. For example,
    the deep learning development cycle introduced in this chapter and the data management
    service introduced in chapter 2 work in the machine learning context, too.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书主要关注于教授如何构建系统或基础设施来促进深度学习开发（所有示例都是神经网络算法），但我们讨论的设计和项目开发概念在机器学习中也适用。因此，在本书中，我们有时将术语*深度学习*和*机器学习*互换使用。例如，本章介绍的深度学习开发周期和第2章介绍的数据管理服务也适用于机器学习上下文。
- en: Deep learning use case
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习用例
- en: A deep learning use case refers to a scenario that utilizes deep learning technology—in
    other words, a problem that you want to solve using deep learning. Examples include
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习用例是指利用深度学习技术解决问题的场景，换句话说，是您想使用深度学习解决的问题。例如：
- en: '*Chatbot*—A user can initiate a text-based conversation with a virtual agent
    on a customer support website. The virtual agent uses a deep learning model to
    understand sentences that the user enters and carries on a conversation with the
    user like a real human.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聊天机器人*—用户可以在客户支持网站上与虚拟代理进行基于文本的对话。虚拟代理使用深度学习模型理解用户输入的句子，并像真正的人类一样与用户进行对话。'
- en: '*Self-driving car*—A driver can put a car into an assistive driving mode that
    automatically steers itself according to road markings. Markings are captured
    by multiple cameras on board the car to form a perception of the road using deep
    learning–based computer vision technology.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动驾驶汽车*—驾驶员可以将汽车置于辅助驾驶模式，根据道路标线自动转向。车载多个摄像头捕捉标线，利用基于深度学习的计算机视觉技术形成对道路的感知。'
- en: Model, prediction and inference, and model serving
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、预测和推断，以及模型服务
- en: 'These three terms are described as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个术语的描述如下：
- en: '*Model*—A deep learning model can be seen as an executable program that contains
    an algorithm (model architecture) and required data to make a prediction.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*—深度学习模型可以被视为包含算法（模型架构）和进行预测所需数据的可执行程序。'
- en: '*Prediction and inference*—Both model prediction and model inference refer
    to executing the model with given data to get a set of outputs. As prediction
    and inference are used widely in the context of model serving, they are used interchangeably
    in this book.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测和推断*—模型预测和模型推断都是指使用给定数据执行模型以获得一组输出。由于在模型服务的上下文中广泛使用预测和推断，它们在本书中可以互换使用。'
- en: '*Model serving* (prediction service)—This book describes model serving as hosting
    machine learning models in a web application (on the cloud or on premises) and
    allowing deep learning applications to integrate the model functionality into
    their systems through an API. The model serving web program is usually referred
    to as the prediction service or model serving service.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型服务*（预测服务）—本书将模型服务描述为在Web应用程序（在云端或本地）中托管机器学习模型，并允许深度学习应用程序通过API将模型功能集成到其系统中。模型服务Web程序通常称为预测服务或模型服务。'
- en: Deep learning application
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用
- en: 'A deep learning application is a piece of software that utilizes deep learning
    technologies to solve problems. It usually does not perform any computationally
    intensive tasks, such as data crunching, deep learning model training, and model
    serving (with the exception of hosting models at the edge, such as an autonomous
    vehicle). Examples include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用是利用深度学习技术解决问题的软件。它通常不执行任何计算密集型任务，例如数据处理、深度学习模型训练和模型服务（除了在边缘托管模型，例如自动驾驶汽车）。例如：
- en: A *chatbot application* that provides a UI or APIs to take natural sentences
    as input from a user, interprets them, takes actions, and provides a meaningful
    response to the user. Based on the model output calculated in the deep learning
    system (from model serving service), the chatbot responds and takes action.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供UI或API以接受用户的自然句子作为输入，解释它们，采取行动并向用户提供有意义的响应的*聊天机器人应用程序*。基于深度学习系统中计算的模型输出（来自模型服务），聊天机器人做出响应并采取行动。
- en: '*Self-driving software* that takes input from multiple sensors, such as video
    cameras, proximity sensors, and LiDAR, to form a perception of a car’s surroundings
    with the help of deep learning models and drives the car accordingly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动驾驶软件*从多个传感器接收输入，如视频摄像头、接近传感器和激光雷达，借助深度学习模型形成对汽车周围环境的感知，并相应地驾驶汽车。'
- en: Platform vs. system vs. infrastructure
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平台 vs. 系统 vs. 基础设施
- en: 'In this book, the terms *deep learning platform*, *deep learning system*, and
    *deep learning infrastructure* all share the same meaning: an underlying system
    that provides all necessary support for building deep learning applications efficiently
    and to scale. We tend to use *system* most commonly, but in the context of this
    book, all three terms have the same meaning.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，术语 *深度学习平台*、*深度学习系统* 和 *深度学习基础设施* 具有相同的含义：为高效构建深度学习应用程序提供所有必要支持的基础系统。我们倾向于最常使用
    *系统*，但在本书的上下文中，所有三个术语都具有相同的含义。
- en: Now that we’re all on the same page about the terms, let’s get started!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对术语有了共识，让我们开始吧！
- en: 1.1 The deep learning development cycle
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 深度学习开发周期
- en: As we’ve said, deep learning systems are the infrastructure necessary for deep
    learning *project development* to progress efficiently. So, before we dive into
    the structure of a deep learning system, it’s prudent to look at the development
    paradigm that a deep learning system enables. We call this paradigm the *deep
    learning development cycle*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，深度学习系统是深度学习 *项目开发* 高效进行所必需的基础设施。因此，在深入探讨深度学习系统的结构之前，审视一下深度学习系统所启用的开发范式是明智的。我们称这个范式为
    *深度学习开发周期*。
- en: You may wonder why, in a technical book, we want to emphasize something that
    is as nontechnical as product development. The fact is that the goal of most deep
    learning work is, in the end, to bring a product or service to market. Yet many
    engineers are not familiar with the other stages of product development, just
    as many product developers do not know about engineering or modeling. From our
    experience in building deep learning systems, we have learned that persuading
    people in multiple roles in a company to adopt a system largely depends on whether
    the system will actually fix their particular problems. We believe that outlining
    the various stages and roles in the deep learning development cycle helps to articulate,
    address, communicate, and eventually solve everyone’s pain points.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，在一本技术书中，为什么我们要强调像产品开发这样与技术无关的东西。事实上，大多数深度学习工作的目标最终是将产品或服务推向市场。然而，许多工程师并不熟悉产品开发的其他阶段，就像许多产品开发者不了解工程或建模一样。从我们构建深度学习系统的经验中，我们已经了解到，说服公司中多个角色的人员采用系统主要取决于该系统是否实际上能解决他们的特定问题。我们相信，概述深度学习开发周期中的各个阶段和角色有助于表达、解决、沟通和最终解决每个人的痛点。
- en: Understanding this cycle solves a few other problems, as well. In the last decade,
    many new deep learning software packages have been developed to address different
    areas. Some of them tackle model training and serving, whereas others handle model
    performance tracking and experimentation. Data scientists and engineers would
    piece these tools together each time they needed to solve a specific application
    or use case; this is called MLOps (machine learning operations). As the number
    of these applications grows, piecing these tools together every time from scratch
    for a new application becomes repetitive and time-consuming. At the same time,
    as the importance of these applications grows, so do the expectations for their
    quality. Both of these concerns call for a consistent approach to developing and
    delivering deep learning features quickly and reliably. This consistent approach
    starts with everyone working under the same deep learning development paradigm
    or cycle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一周期也能解决其他一些问题。在过去的十年里，许多新的深度学习软件包已经被开发出来，以解决不同的领域。其中一些处理模型训练和服务，而另一些处理模型性能跟踪和实验。数据科学家和工程师每次需要解决特定应用程序或用例时都会组合这些工具；这被称为
    MLOps（机器学习运维）。随着这些应用程序数量的增长，为新的应用程序每次从头开始组合这些工具变得重复和耗时。同时，随着这些应用程序的重要性增长，对其质量的期望也在增加。这两个问题都需要一种一致的方法来快速可靠地开发和交付深度学习功能。这种一致的方法始于所有人都在同一深度学习开发范式或周期下工作。
- en: How does the deep learning *system* fit into the deep learning *cycle*? A well-built
    deep learning system would support the product development cycle and make performing
    the cycle easy, quick, and reliable. Ideally, data scientists can use a deep learning
    system as the infrastructure to complete the entire deep learning cycle without
    learning all the engineering details of the underlying complex systems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习*系统*如何适应深度学习*周期*？一个良好构建的深度学习系统应该支持产品开发周期，并使执行周期变得轻松、快速和可靠。理想情况下，数据科学家可以使用深度学习系统作为基础设施完成整个深度学习周期，而无需学习底层复杂系统的所有工程细节。
- en: Because every product and organization is unique, it is crucial for system builders
    to understand the unique requirements of the various roles to build a successful
    system. By “successful,” we mean one that helps stakeholders collaborate productively
    to deliver deep learning features quickly. Throughout this book, as we go through
    the design principles of deep learning systems and look at how each component
    works, your understanding of your stakeholder requirements will help you adapt
    this knowledge to form your own system design. As we discuss the technical details,
    we will point out when you need to pay attention to certain types of stakeholders
    during the design of the system. The deep learning development cycle will serve
    as the guiding framework when we consider the design requirements of each component
    of a deep learning system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个产品和组织都是独特的，对系统构建者来说，理解各种角色的独特需求以构建成功的系统至关重要。所谓的“成功”，是指帮助利益相关者高效协作，快速交付深度学习特性的系统。在本书中，当我们讨论深度学习系统的设计原则，并查看每个组件的工作方式时，你对利益相关者需求的理解将帮助你调整这些知识，形成自己的系统设计。在讨论技术细节时，我们将指出在设计系统时需要注意某些类型的利益相关者。深度学习开发周期将作为指导框架，帮助我们考虑深度学习系统的每个组件的设计要求。
- en: Let’s start with a picture. Figure 1.1 illustrates what a typical cycle looks
    like. It shows the machine learning (especially deep learning) development progress
    phase by phase. As you can see, cross-functional collaboration happens at almost
    every step. We will discuss each phase and role involved in this diagram in the
    following two sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一张图片开始。图1.1展示了典型周期的样貌。它展示了机器学习（特别是深度学习）的开发进度逐个阶段的过程。正如你所见，跨职能协作几乎在每一步都发生。我们将在接下来的两个部分讨论此图中涉及的每个阶段和角色。
- en: '![](../Images/01-01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-01.png)'
- en: Figure 1.1 A typical scenario to bring deep learning from research to a product.
    We call this *the deep learning development cycle*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 将深度学习从研究带入产品的典型场景。我们称之为*深度学习开发周期*。
- en: 1.1.1 Phases in the deep learning product development cycle
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 深度学习产品开发周期中的阶段
- en: 'The deep learning development cycle typically begins with a business opportunity
    and is driven by a product plan and its management. After that, the cycle normally
    goes through four phases: data exploration, prototyping, productionization (shipping
    to production), and application integration. Let’s look at these phases one at
    a time. Then we’ll look at all the roles involved (denoted by the icon of a person
    in figure 1.1).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习开发周期通常从一个业务机会开始，并由产品计划及其管理驱动。之后，周期通常经历四个阶段：数据探索、原型制作、产品化（投入生产）和应用集成。让我们逐一查看这些阶段。然后我们将查看所有涉及的角色（在图1.1中以人物图标表示）。
- en: Note The number in parentheses next to each following subsection corresponds
    to the same number in figure 1.1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：每个后续小节旁边括号中的数字与图1.1中的相同数字对应。
- en: Product initiation (1)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 产品启动（1）
- en: First, the business stakeholder (product owner or project manager) analyzes
    the business and identifies a potential business opportunity or problem that can
    be addressed with machine learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，业务利益相关者（产品所有者或项目经理）分析业务，并确定可以通过机器学习解决的潜在业务机会或问题。
- en: Data exploration (2)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索（2）
- en: When data scientists have a clear understanding of business requirements, they
    begin to work with data engineers to collect as much data as possible, label it,
    and build datasets. Data collection can include searching publicly available data
    and exploring internal sources. Data cleaning may also occur. Data labeling can
    either be outsourced or performed in-house.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家清楚了解业务需求时，他们开始与数据工程师合作，尽可能收集、标记数据并构建数据集。数据收集可以包括搜索公开可用数据和探索内部来源。数据清理也可能会发生。数据标记可以外包或在内部执行。
- en: Compared to the following phases, this early phase of data exploration is unstructured
    and often done casually. It might be a Python script or shell script, or even
    a manual copy of data. Data scientists often use web-based data analysis applications,
    such as Jupyter Notebook (open source; [https://jupyter.org](https://jupyter.org)),
    Amazon SageMaker Data Wrangler ([https://aws.amazon.com/sagemaker/data-wrangler](https://aws.amazon.com/sagemaker/data-wrangler)),
    and Databricks ([www.databricks.com](http://www.databricks.com)), to analyze data.
    There is no formal data collection pipeline that needs to be built.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下阶段相比，数据探索的早期阶段是非结构化的，通常是随意进行的。它可能是一个 Python 脚本或 shell 脚本，甚至是数据的手动复制。数据科学家经常使用基于
    Web 的数据分析应用程序，例如 Jupyter Notebook（开源；[https://jupyter.org](https://jupyter.org)）、Amazon
    SageMaker Data Wrangler（[https://aws.amazon.com/sagemaker/data-wrangler](https://aws.amazon.com/sagemaker/data-wrangler)）和
    Databricks（[www.databricks.com](http://www.databricks.com)）来分析数据。不需要构建正式的数据收集管道。
- en: Data exploration is not only important but also critical to the success of a
    deep learning project. The more relevant data is available, the higher the likelihood
    of building effective and efficient deep learning models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索不仅重要，而且对深度学习项目的成功至关重要。可用的相关数据越多，建立有效和高效深度学习模型的可能性就越高。
- en: Research and prototyping (3, 4)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究和原型设计（3, 4）
- en: The goal of prototyping is to find the most feasible algorithm/approach to address
    the business requirement (from product owner) with the given data. In this phase,
    data scientists can work with AI researchers to propose and evaluate different
    training algorithms with datasets built from the previous data exploration phase.
    Data scientists usually pilot multiple ideas in this phase and build proof-of-concept
    (POC) models to evaluate them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 原型设计的目标是找到最可行的算法/方法，以解决给定数据的业务需求（来自产品所有者）。在此阶段，数据科学家可以与 AI 研究人员合作，提出并评估来自前期数据探索阶段构建的不同训练算法。数据科学家通常在此阶段尝试多种想法，并构建概念验证（POC）模型来评估它们。
- en: Although newly published algorithms are often considered, most of them will
    not be adopted. The accuracy of an algorithm is not the only factor to be considered;
    one also must consider computing resource requirements, data volume, and algorithm
    implementation cost when evaluating an algorithm. The most practical approach
    is usually the winner.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管新发布的算法通常会受到考虑，但大多数算法都不会被采纳。算法的准确性不是唯一要考虑的因素；在评估算法时还必须考虑计算资源需求、数据量和算法实现成本。最实用的方法通常是获胜者。
- en: Note that due to resource constraints, researchers are not always involved in
    the prototyping phase. Frequently, data scientists do the research work as well
    as build the POC.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于资源限制，研究人员并不总是参与原型设计阶段。经常情况下，数据科学家既做研究工作，又构建 POC。
- en: 'You may also notice that in figure 1.1, there is an inner loop (loop A) in
    the big development cycle: Product Initiation > Data Exploration > Deep Learning
    Research > Prototyping > Model > Product Initiation. The purpose of this loop
    is to obtain product feedback in the early phase by building a POC model. We may
    run through this loop multiple times until all stakeholders (data scientists,
    product owners) arrive at a consensus on the algorithms and data that will be
    used to address the business requirement.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可能注意到，在图1.1中，大型开发周期中有一个内部循环（循环A）：产品启动 > 数据探索 > 深度学习研究 > 原型设计 > 模型 > 产品启动。该循环的目的是通过构建
    POC 模型在早期阶段获得产品反馈。我们可能会多次执行此循环，直到所有利益相关者（数据科学家、产品所有者）就将用于满足业务需求的算法和数据达成一致意见。
- en: 'Multiple hard lessons finally taught us that we must vet the solution with
    the product team or the customer (even better) before starting the expensive process
    of productionization—building production data and training pipelines and hosting
    models. The purpose of a deep learning project is no different from any other
    software development project: to solve a business need. Vetting the approach with
    the product team in the early stage will prevent the expensive and demoralizing
    process of reworking it in later stages.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 多次痛苦的教训最终教会了我们，在开始昂贵的生产过程——构建生产数据和训练管道以及托管模型之前，我们必须与产品团队或客户（甚至更好）审查解决方案。深度学习项目的目的与任何其他软件开发项目并无不同：解决业务需求。在早期阶段与产品团队审查方法将防止在后期重新制定方法的昂贵和令人沮丧的过程。
- en: Productionization aka MLOps (5)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化（也称为 MLOps）（5）
- en: Productionization, also called “shipping to production,” is the process of making
    a product production worthy and ready to be consumed by its users. Production
    worthiness is commonly defined as being able to serve customer requests, withstand
    a certain level of request load, and gracefully handle adverse situations such
    as malformed input and request overload. Production worthiness also includes postproduction
    efforts, such as continuous model metric monitoring and evaluation, feedback gathering,
    and model retraining.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化，也称为“投入生产”，是使产品具备生产价值并准备好被用户消费的过程。生产价值通常定义为能够服务客户请求，承受一定程度的请求负载，并优雅地处理诸如格式错误的输入和请求超载等不利情况。生产价值还包括后期工作，如持续的模型指标监控和评估、反馈收集和模型重新训练。
- en: Productionization is the most engineering-intensive part of the development
    cycle because we’ll be converting prototyping experiments into serious production
    processes. A nonexhaustive to-do list of productionization can include
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生产化是开发周期中最需要工程投入的部分，因为我们将把原型实验转化为严肃的生产流程。生产化的非详尽待办事项列表可以包括
- en: Building a data pipeline to pull data from different data sources repeatedly
    and keep the dataset versioned and updated.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个数据管道，重复从不同的数据源中提取数据，并使数据集版本化和更新。
- en: Building a data pipeline to preprocess dataset, such as data enhancement or
    enrichment and integrating with external labeling tools.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立数据管道对数据集进行预处理，例如数据增强或增强和与外部标记工具集成。
- en: Refactoring and dockerizing the prototyping code to production-quality model
    training code.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构和将原型代码docker化为生产质量的模型训练代码。
- en: Making the result of training and serving codes reproducible by versioning and
    tracking the inputs and outputs. For example, we could enable the training code
    to report the training metadata (training date and time, duration, hyperparameters)
    and model metadata (performance metrics, data, and code used) to ensure the full
    traceability of every model training run.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过版本控制和跟踪输入和输出使训练和服务代码的结果可再现。例如，我们可以使训练代码报告训练元数据（训练日期和时间、持续时间、超参数）和模型元数据（性能指标、使用的数据和代码），以确保对每次模型训练运行的完全可追溯性。
- en: Setting up continuous integration (Jenkins, GitLab CI) and continuous deployment
    to automate the code building, validation, and deployment.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置持续集成（Jenkins、GitLab CI）和持续部署以自动化代码构建、验证和部署。
- en: Building a continuous model training and evaluation pipeline so the model training
    can automatically consume the latest dataset and produce models in a repeatable,
    auditable, and reliable manner.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立连续的模型训练和评估管道，以便模型训练可以自动使用最新的数据集，并以可重复、可审计和可靠的方式生成模型。
- en: Building a model deployment pipeline that automatically releases models that
    have passed the quality gate, so the model serving component can access them;
    `async` or real-time model prediction can be performed depending on the business
    requirements. The model serving component hosts the model and exposes it via a
    web API.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个模型部署管道，自动发布通过质量门的模型，以便模型服务组件可以访问它们；根据业务需求可以执行`async`或实时模型预测。模型服务组件托管模型并通过Web
    API公开它。
- en: Building continuous-monitoring pipelines that periodically assess the dataset,
    model, and model serving performance to detect potential feature drift (data distribution
    change) in dataset or model performance degradation (concept drifting) and alert
    developers or retrain the model.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立持续监控管道，定期评估数据集、模型和模型服务性能，以检测数据集的潜在特征漂移（数据分布变化）或模型性能下降（概念漂移）并警告开发人员或重新训练模型。
- en: 'These days, the productionization step has a new alias with buzz: MLOps (machine
    learning operation), which is a vague term, and its definition is ambiguous for
    researchers and professionals. We interpret MLOps to mean bridging the gap between
    model development (experimentation) and operations in production environments
    (Ops) to facilitate productionization of machine learning projects. An example
    might be streamlining the process of taking machine learning models to production
    and then monitoring and maintaining them.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，生产化步骤有一个新的热门别名：MLOps（机器学习运营），这是一个模糊的术语，对研究人员和专业人员的定义模糊不清。我们解释MLOps的含义是弥合模型开发（实验）和生产环境运营（Ops）之间的鸿沟，以促进机器学习项目的生产化。例如，简化将机器学习模型推向生产的过程，然后对其进行监视和维护。
- en: 'MLOps is a paradigm rooted in the application of similar principles that DevOps
    has to software development. It leverages three disciplines: machine learning,
    software engineering (especially the operation), and data engineering. See figure
    1.2 for a look at deep learning through the lens of MLOps.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 是一种根植于 DevOps 原则的范式，应用了类似的原则到软件开发中。它利用了三个学科：机器学习、软件工程（特别是运维）和数据工程。查看图
    1.2，了解通过 MLOps 视角看深度学习。
- en: '![](../Images/01-02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-02.png)'
- en: 'Figure 1.2 MLOps applies DevOps approaches to deep learning for the productionization
    phase, when models get shipped to production. (Source: *Machine Learning Engineering
    in Action*, by Ben Wilson, Manning, 2022, figure 2.7)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 MLOps 在深度学习的产品化阶段应用了 DevOps 方法，当模型被推向生产时。（来源：*Machine Learning Engineering
    in Action*，作者 Ben Wilson，Manning 出版社，2022年，图 2.7）
- en: Because this book is about building machine learning systems that support ML
    operations, we won’t go into details about the practices shown in figure 1.2\.
    But, as you can see, the engineering effort that supports the development of machine
    learning models in production is huge. Compared to what data scientists used to
    do during the previous phase of data exploration and model prototyping, the tooling
    (software), engineering standards, and processes have dramatically changed and
    become much more complex.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这本书是关于构建支持 ML 运营的机器学习系统，所以我们不会详细介绍图 1.2 中所示的实践。但是，正如你所看到的，支持将机器学习模型开发到生产环境中的工程工作量是巨大的。与数据科学家在数据探索和模型原型阶段所做的工作相比，工具（软件）、工程标准和流程已经发生了巨大变化，并变得更加复杂。
- en: Why is shipping models to production difficult?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将模型部署到生产环境很困难？
- en: The massive underlying infrastructure (tools, services, servers) and heavy cross-team
    collaboration are the two biggest hurdles for shipping models to production. This
    section on productionization (aka MLOps) establishes that data scientists need
    to work with data engineers, platform developers, DevOps engineers, and machine
    learning engineers, as well as learn a massive infrastructure (deep learning system),
    to ship an algorithm/model from prototype to production. It's no wonder that productionizing
    a model takes so much time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 庞大的基础设施（工具、服务、服务器）和团队间的密集合作是将模型部署到生产环境的两个最大障碍。这个关于产品化（又称 MLOps）的部分建立在一个事实上，即数据科学家需要与数据工程师、平台开发人员、DevOps
    工程师和机器学习工程师一起工作，并且要了解庞大的基础设施（深度学习系统），才能将算法/模型从原型推向生产。难怪产品化模型需要花费如此多的时间。
- en: To solve these challenges, we need to abstract away the complexity from data
    scientists when designing and building a deep learning system. As with building
    a car, we want to put data scientists behind the wheel but without asking them
    to know much about the car itself.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们需要在设计和构建深度学习系统时，将复杂性从数据科学家那里抽象出来。就像建造汽车一样，我们希望让数据科学家坐在驾驶座上，但不要求他们对汽车本身了解太多。
- en: Now, returning to the development cycle, you may notice there is *another* inner
    loop (loop B) in figure 1.1 that goes from Productionization (box 5) and Model
    to Product Initiation (box 1). This is the second vet with the product team before
    we integrate the model inference with an AI application.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到开发周期，你可能会注意到图 1.1 中还有一个*另一个*内部循环（循环 B），从产品化（方框 5）到模型到产品启动（方框 1）。这是在我们将模型推理与
    AI 应用集成之前与产品团队进行的第二次审查。
- en: Our second vet (loop B) compares the model and data between prototyping and
    production. We want to ensure the model performance and scalability (for example,
    model serving capacity) match business requirements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二次审查（循环 B）在原型和生产之间比较模型和数据。我们要确保模型性能和可扩展性（例如，模型服务容量）符合业务需求。
- en: 'Note The following two papers are recommended; if you want to learn more about
    MLOps, they are great starting points: “Operationalizing Machine Learning: An
    Interview Study” (arXiv:2209.09125) and “Machine Learning Operations (MLOps):
    Overview, Definition, and Architecture” (arXiv:2205.02302).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：以下两篇论文是推荐的；如果你想了解更多关于 MLOps 的内容，它们是很好的起点：“Operationalizing Machine Learning:
    An Interview Study”（arXiv:2209.09125）和“Machine Learning Operations (MLOps): Overview,
    Definition, and Architecture”（arXiv:2205.02302）。'
- en: Application integration (6)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 应用集成（6）
- en: The last step of the product development cycle is to integrate the model prediction
    to the AI application. The common pattern is to host the models in the model serving
    service (which will be discussed in section 1.2.2) of the deep learning system
    and integrate the business application logic with the model by sending model prediction
    requests over the internet.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 产品开发周期的最后一步是将模型预测集成到AI应用中。常见的模式是将模型托管在深度学习系统的模型服务服务中，并通过互联网发送模型预测请求将业务应用逻辑与模型集成。
- en: As a sample user scenario, a chatbot user interacts with the chatbot user interface
    by typing or voicing questions. When the chatbot application receives input from
    the customer, it calls the remote model serving service to run a model prediction
    and then takes action or responds to the customer based on the model prediction
    result.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例用户场景，一个聊天机器人用户通过键入或发声问题与聊天机器人用户界面进行交互。当聊天机器人应用程序接收到来自客户的输入时，它调用远程模型服务服务来运行模型预测，然后根据模型预测结果采取行动或回应客户。
- en: Along with integrating model serving with application logic, this phase also
    involves evaluating metrics important to the product, such as clickthrough rate
    and churn rate. Nice ML-specific metrics (good precision–recall curve) do not
    always guarantee the business requirement is met. So the business stakeholders
    often perform customer interviews and product metric evaluation at this stage.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将模型服务与应用逻辑集成外，此阶段还涉及评估对产品重要的指标，如点击率和流失率。良好的ML特定指标（良好的精确度-召回率曲线）并不总是能保证满足业务需求。因此，业务利益相关者通常在此阶段进行客户访谈和产品指标评估。
- en: 1.1.2 Roles in the development cycle
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 开发周期中的角色
- en: Because you now have a clear idea of each step in a typical development cycle,
    let’s look at the key roles that collaborate in this cycle. The definitions, job
    titles, and responsibilities of each role may vary across organizations. So make
    sure you clarify who does what in your organization and adjust your system’s design
    appropriately.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您现在对典型开发周期中的每个步骤有了清晰的了解，让我们来看看在这个周期中合作的关键角色。每个角色的定义、职称和职责可能因组织而异。所以确保您澄清了您的组织中谁做什么，并相应调整您系统的设计。
- en: Business stakeholders (product owner)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 业务利益相关者（产品所有者）
- en: 'Many organizations assign the stakeholder role to multiple positions, such
    as product managers, engineering managers, and senior developers. Business stakeholders
    define the business goal of a product and are responsible for communication and
    execution of the product development cycle. The following are their responsibilities:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织将利益相关者角色分配给多个职位，如产品经理、工程经理和高级开发人员。业务利益相关者定义产品的业务目标，并负责产品开发周期的沟通和执行。以下是他们的责任：
- en: Getting inspiration from deep learning research, discussing potential application
    of deep learning features in products, and driving product requirements that in
    turn drive model development
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从深度学习研究中获得灵感，讨论在产品中应用深度学习特性的潜在应用，并驱动推动模型开发的产品需求
- en: Owning the product! Communicating with customers and making sure the engineering
    solution meets the business requirement and delivers the results
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有产品！与客户沟通，确保工程解决方案符合业务需求并产生结果
- en: Coordinating cross-functional collaborations between different roles and teams
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协调不同角色和团队之间的跨职能协作
- en: Running project development execution; providing guidance or feedback during
    the entire development cycle to ensure the deep learning features offer real value
    to the customers of the product
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行项目开发执行；在整个开发周期内提供指导或反馈，以确保深度学习特性为产品的客户提供真正的价值
- en: Evaluating the product metrics (such as user churn rate and feature usage)—not
    the model metrics (precision or accuracy)—and driving improvement of model development,
    productionization, or product integration
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估产品指标（如用户流失率和功能使用情况）—而不是模型指标（精度或准确性）—并推动模型开发、产品化或产品集成的改进
- en: Researchers
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员
- en: Machine learning researchers research and develop new and novel neural network
    architectures. They also develop techniques for improving model accuracy and efficiency
    in training models. These architectures and techniques can be used during model
    development.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究人员研究和开发新颖的神经网络架构。他们还开发提高模型准确性和训练模型效率的技术。这些架构和技术可以在模型开发过程中使用。
- en: Note The machine learning researcher role is often associated with big tech
    companies like Google, Microsoft, and Salesforce. In many other companies, data
    scientists fulfill the same role.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注：机器学习研究员角色通常与 Google、Microsoft 和 Salesforce 等大型科技公司相关联。在许多其他公司，数据科学家扮演相同的角色。
- en: Data scientists
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家
- en: Data scientists may wear a research hat, but most of the time, they translate
    a business problem into a machine learning problem and implement it using machine
    learning methods. Data scientists are motivated by the product’s need and apply
    research techniques to production data rather than standard benchmark datasets.
    Besides researching model algorithms, a data scientist’s responsibilities may
    also include
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可能会扮演研究员的角色，但大多数情况下，他们会将业务问题转化为机器学习问题，并使用机器学习方法来实现。数据科学家受产品需求的驱动，并将研究技术应用于生产数据，而不是标准基准数据集。除了研究模型算法外，数据科学家的职责还可能包括
- en: Combining multiple deep learning neural network architectures and/or techniques
    from different research into a solution. Sometimes they apply additional machine
    learning techniques besides pure deep learning.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同研究中的多个深度学习神经网络架构和/或技术结合到一个解决方案中。有时，他们除了纯深度学习外还应用其他机器学习技术。
- en: Exploring available data, determining what data is useful, and deciding on how
    to preprocess it before supplying it for training.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可用数据，确定哪些数据是有用的，并决定如何在供训练之前对其进行预处理。
- en: Prototyping different approaches (writing experimental code) to tackle the business
    problem.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原型不同方法（编写实验性代码）来解决业务问题。
- en: Converting model prototyping code into production code with workflow automation.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型原型代码转换为生产代码，并进行工作流自动化。
- en: Following the engineering process to ship models to production by using the
    deep learning system.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循工程流程，通过使用深度学习系统将模型部署到生产环境。
- en: Iterating on the need for any additional data that may help with model development.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要迭代可能有助于模型开发的任何额外数据。
- en: Continuously monitoring and evaluating data and model performance in production.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中持续监控和评估数据和模型性能。
- en: Troubleshooting model-related problems, such as model degradation.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查与模型相关的问题，如模型退化。
- en: Data engineers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师
- en: Data engineers help collect data and set up data pipelines for continuous data
    ingestion and processing, including data transformation, enrichment, and labeling.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师帮助收集数据，并建立连续数据摄入和处理的数据管道，包括数据转换、丰富和标记。
- en: MLOps engineer/ML engineer
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 工程师/ML 工程师
- en: An MLOps engineer fulfills a number of roles across multiple domains, including
    that of data engineer, DevOps (operation) engineer, data scientist, and platform
    engineer. As well as setting up and operating the machine learning infrastructure
    (both systems and hardware), they manage automation pipelines to create datasets
    and train and deploy models. ML infrastructures and user activities, such as training
    and serving, are also monitored by MLOps engineers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 工程师在多个领域扮演多种角色，包括数据工程师、DevOps（运维）工程师、数据科学家和平台工程师。除了设置和运行机器学习基础设施（系统和硬件），他们还管理自动化管道以创建数据集并训练和部署模型。MLOps
    工程师还监控 ML 基础设施和用户活动，如训练和服务。
- en: As you can see, MLOps is hard, because it requires people to master a set of
    practices across software development, operation, maintenance, and machine learning
    development. MLOps engineers’ goal is to ensure the creation, deployment, monitoring,
    and maintenance of machine learning models are efficient and reliable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，MLOps 很困难，因为它需要人们掌握一套跨越软件开发、运维、维护和机器学习开发的实践方法。MLOps 工程师的目标是确保机器学习模型的创建、部署、监控和维护高效可靠。
- en: Deep learning system/platform engineer
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统/平台工程师
- en: Deep learning system engineers build and maintain the general pieces of the
    machine learning infrastructure—the primary focus of this book—to support all
    the machine learning development activities for data scientists, data engineers,
    MLOps engineers, and AI applications. Among the components of the machine learning
    system are data warehouses, compute platforms, workflow orchestration services,
    model metadata and artifact stores, model training services, model serving services,
    and more.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统工程师构建和维护机器学习基础设施的主要组件——本书的主要关注点——以支持数据科学家、数据工程师、MLOps 工程师和 AI 应用的所有机器学习开发活动。机器学习系统的组成部分包括数据仓库、计算平台、工作流编排服务、模型元数据和工件存储、模型训练服务、模型服务等。
- en: Application engineer
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 应用工程师
- en: Application engineers build customer-facing applications (both frontend and
    backend) to address given business requirements. The application logic will make
    decisions or take actions based on the model prediction for a given customer request.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 应用工程师构建面向客户的应用程序（前端和后端），以满足给定的业务需求。应用程序逻辑将根据给定客户请求的模型预测做出决策或采取行动。
- en: Note In the future, as machine learning systems (infrastructure) mature, the
    roles involved in deep learning development cycle will merge into fewer and fewer.
    Eventually, data scientists will be able to complete the entire cycle on their
    own.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：未来，随着机器学习系统（基础设施）的成熟，深度学习开发周期中涉及的角色将合并为越来越少。最终，数据科学家将能够独自完成整个周期。
- en: 1.1.3 Deep learning development cycle walk-through
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 深度学习开发周期步骤详解
- en: 'By giving an example, we can demonstrate the roles and the process in a more
    concrete manner. Suppose you have been assigned the task of building a customer
    support system that answers questions automatically about the company’s product
    lines. The following steps will guide you through the process of bringing that
    product to market:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过给出一个例子，我们可以以更具体的方式展示角色和过程。假设你被分配了构建一个关于公司产品线自动回答问题的客户支持系统的任务。以下步骤将指导您完成将该产品推向市场的过程：
- en: The product requirement is to build a customer support application that presents
    a menu, so customers can navigate to find answers to commonly asked questions.
    As the number of questions grows, the menu becomes larger, with many layers of
    navigation. Analytics has shown that many customers are confused by the navigation
    system and drop off from navigating the menu while trying to find answers.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品要求是构建一个客户支持应用程序，提供一个菜单，让客户可以浏览以找到常见问题的答案。随着问题数量的增加，菜单变得越来越大，有许多层次的导航。分析显示，许多客户在尝试找到答案时对导航系统感到困惑，并放弃了浏览菜单。
- en: The product manager (PM) who owns the product is motivated to improve the user
    retention rate and experience (finding the answers quickly). After conducting
    some research with customers, the PM finds that a majority of customers would
    like to obtain answers without a complex menu system, preferably as simple as
    asking questions in their natural language.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有该产品的产品经理（PM）受到改善用户保留率和体验（快速找到答案）的动机。在与客户进行了一些研究后，产品经理发现，大多数客户希望在不复杂的菜单系统中获得答案，最好是像在他们的自然语言中提问一样简单。
- en: The PM reaches out to machine learning researchers for a potential solution.
    It turns out that deep learning may help. Experts think the technology is mature
    enough for this use case and suggest a few approaches based on deep learning models.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品经理联系机器学习研究人员寻求潜在解决方案。结果表明，深度学习可能会有所帮助。专家认为这项技术已经足够成熟，可以用于这个用例，并建议几种基于深度学习模型的方法。
- en: The PM writes a product spec indicating that the application should take one
    question from a customer at a time, recognize intent from the question, and match
    it with relevant answers.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品经理编写产品规格，指示应用程序一次从客户那里接收一个问题，从问题中识别意图，并与相关答案匹配。
- en: Data scientists receive product requirements and start to prototype deep learning
    models that fit the need. They first start data exploration to collect available
    training data and consult with researchers for the choices of algorithms. And
    then data scientists start to build prototyping code to produce experimental models.
    Eventually, they arrive at some datasets, a few training algorithms, and several
    models. After careful evaluation, one natural language process model is selected
    from various experiments.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家收到产品需求并开始原型化符合需求的深度学习模型。他们首先开始数据探索，收集可用的训练数据，并与研究人员商讨算法的选择。然后数据科学家开始编写原型代码以生成实验模型。最终，他们得到了一些数据集、几种训练算法和多个模型。经过仔细评估，从各种实验中选择了一个自然语言处理模型。
- en: Then the PM assembles a team of platform engineers, MLOps engineers, and data
    engineers to work with the data scientist to onboard the prototyping code, made
    in step 5, to production. The work includes building a continuous data processing
    pipeline and a continuous model training, deployment, and evaluation pipeline,
    as well as setting up the model serving functionality. The PM also specifies the
    number of predictions per second and the latency required.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，项目经理组建了一个平台工程师、MLOps工程师和数据工程师团队，与数据科学家一起工作，将在第5步中创建的原型代码投入生产。这项工作包括构建连续的数据处理管道和连续的模型训练、部署和评估管道，以及设置模型服务功能。项目经理还确定了每秒预测次数和所需的延迟。
- en: Once a production setup is complete, the application engineers integrate the
    customer support service’s backend with the model serving service (built in step
    6), so when a user types in a question, the service will return answers based
    on the model prediction. The PM also defines product metrics, such as average
    time spent finding an answer, to evaluate the end result and use it to drive the
    next round of improvement.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦生产设置完成，应用工程师将客户支持服务的后端与模型服务服务（在第6步中构建）集成起来，因此当用户输入问题时，服务将根据模型预测返回答案。项目经理还定义了产品指标，例如平均查找答案所花费的时间，以评估最终结果，并将其用于推动下一轮改进。
- en: 1.1.4 Scaling project development
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.4 项目开发的扩展
- en: As you saw in section 1.1.2, we need to fill seven different roles to complete
    a deep learning project. The cross-functional collaboration between these roles
    happens at almost every step. For example, data engineers, platform developers,
    and data scientists work together to productionize a project. Anyone who has been
    involved in a project that requires many stakeholders knows how much communication
    and coordination are required to keep a project like this moving forward.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在1.1.2节中所看到的，我们需要填补七种不同的角色才能完成一个深度学习项目。这些角色之间的跨职能协作几乎在每一个步骤都会发生。例如，数据工程师、平台开发人员和数据科学家共同致力于将项目投入生产。任何参与过需要许多利益相关方的项目的人都知道，为了推动这样一个项目前进，需要多少沟通和协调。
- en: These challenges make deep learning development hard to scale because we either
    don’t have the resources to fill all the required roles or we can’t meet the product
    timeline due to the communication costs and slowdowns. To reduce the enormous
    amount of operational work, communication, and cross-team coordination costs,
    companies are investing in machine learning infrastructure and reducing the number
    of people and the scope of knowledge required to build a machine learning project.
    The goal of a deep learning infrastructure stack is not only to automate model
    building and data processing but also to make it possible to merge the technical
    roles so that the data scientist is empowered to take care of all these functions
    autonomously within a project.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战使得深度学习开发难以扩展，因为我们要么没有资源来填补所有所需角色，要么由于沟通成本和减速而无法满足产品时间表。为了减少巨大的运营工作量、沟通和跨团队协调成本，公司正在投资于机器学习基础设施，并减少构建机器学习项目所需的人员数量和知识范围。深度学习基础设施堆栈的目标不仅是自动化模型构建和数据处理，还要使技术角色合并为可能，使数据科学家能够在项目中独立地处理所有这些功能。
- en: A key success indicator of a deep learning system is to see how smooth the model
    productionization process can be. With a good infrastructure, the data scientist,
    who is not expected to suddenly become an expert DevOps or data engineer, should
    be able to implement models in a scalable manner, set up data pipelines, and deploy
    and monitor models in production independently.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统的一个关键成功指标是看模型投产过程能否顺利进行。有了良好的基础设施，不会期望数据科学家突然成为专家级的DevOps或数据工程师，他们应该能够以可扩展的方式实现模型、建立数据管道，并独立地在生产环境中部署和监控模型。
- en: By using an efficient deep learning system, data scientists will be able to
    complete the development cycle with minimal additional overhead—less communication
    required and less time wasted waiting by others—and focus on the most important
    data science tasks, such as understanding the data and experimenting with algorithms.
    The ability to scale deep learning project development is the true value proposition
    of a deep learning system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用高效的深度学习系统，数据科学家将能够以最小的额外开销完成开发周期——需要较少的沟通和等待他人的时间，并专注于最重要的数据科学任务，如理解数据和尝试算法。扩展深度学习项目开发能力是深度学习系统的真正价值所在。
- en: 1.2 Deep learning system design overview
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 深度学习系统设计概述
- en: 'With the context of section 1.1 in mind, let’s dive into the focus of this
    book: the deep learning system itself. Designing a system—any system—is the art
    of achieving goals under a set of constraints that are unique to your situation.
    This is also true for deep learning systems. For instance, let’s say you have
    a few deep learning models that need to be served at the same time, but your budget
    does not allow you to operate a machine that has enough memory to fit all of them
    at the same time. You may need to design a caching mechanism to swap models between
    memory and disk. Swapping, however, will increase inference latency. Whether this
    solution is feasible will depend on latency requirements. Another possibility
    is to operate multiple smaller machines for each model, if your model sizes and
    budget will allow it.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑到第 1.1 节的背景下，让我们深入了解本书的重点：深度学习系统本身。设计一个系统——任何系统——都是在一组对你的情况独特的约束条件下实现目标的艺术。深度学习系统也不例外。例如，假设你有几个需要同时提供服务的深度学习模型，但是你的预算不允许你运行一台具有足够内存同时容纳所有模型的机器。你可能需要设计一个缓存机制来在内存和磁盘之间交换模型。然而，交换会增加推断延迟。这种解决方案是否可行将取决于延迟要求。另一个可能性是为每个模型运行多个较小的机器，如果你的模型大小和预算允许的话。
- en: Or, for another example, imagine your company’s product must comply with certain
    certification standards. It may mandate data access policies that pose significant
    limitations to anyone who wants to gain access to data collected by the company’s
    product. You may need to design a framework to allow data access in a compliant
    fashion so that researchers, data scientists, and data engineers can troubleshoot
    problems and develop new models that require such data access in your deep learning
    system.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，举个例子，想象一下你公司的产品必须符合某些认证标准。它可能会规定数据访问政策，对希望访问公司产品收集的数据的任何人施加重大限制。你可能需要设计一个框架，以符合标准地允许数据访问，以便研究人员、数据科学家和数据工程师可以在你的深度学习系统中解决问题并开发需要这种数据访问的新模型。
- en: As you can see, there are many knobs that can be turned. It will certainly be
    an iterative process to arrive at a design that will satisfy as many requirements
    as possible. But to shorten the iterative process, it is desirable to start with
    a design that is as close to the end state as possible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，有许多可以调整的旋钮。达到尽可能满足多个要求的设计肯定是一个迭代的过程。但为了缩短迭代过程，最好从尽可能接近最终状态的设计开始。
- en: In this section, we first propose a deep learning system design with only essential
    components and then explain the responsibility of each of the components and user
    workflows. In our experience of designing and tailoring deep learning systems,
    a few key components are common across different designs. We think that they can
    be used as a reasonable starting point for your design. We call this the *reference
    system architecture*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先提出了一个仅具有基本组件的深度学习系统设计，然后解释了每个组件的责任和用户工作流程。根据我们设计和定制深度学习系统的经验，几个关键组件在不同的设计中是共同的。我们认为它们可以作为你设计的合理起点。我们称之为
    *参考系统架构*。
- en: You can make a copy of this reference for your design project, go through your
    list of goals and constraints, and start by identifying knobs in each component
    that you can adjust to your needs. Because this isn’t an authoritative architecture,
    you should also assess whether all components are really necessary and add or
    remove components as you see fit.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为你的设计项目制作一份此参考副本，列出你的目标和约束条件，然后开始识别每个组件中可以根据需要调整的旋钮。因为这不是一个权威的体系结构，所以你还应该评估是否所有组件都真的是必需的，并根据需要添加或删除组件。
- en: 1.2.1 Reference system architecture
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 参考系统架构
- en: 'Figure 1.3 shows the high-level overview of the reference deep learning system.
    The deep learning system has two major parts. The first is the application programming
    interface (API; box A) for the system, located in the middle of the diagram. The
    second is the collection of components of the deep learning system, which is represented
    by all the rectangular boxes located within the large box, outlined in a dotted
    line and taking up the lower half of the diagram. These boxes each represent a
    system component:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 显示了参考深度学习系统的高层概述。深度学习系统有两个主要部分。第一个是系统的应用程序编程接口（API；盒子 A），位于图表中间。第二个是深度学习系统的组件集合，由所有矩形框表示，位于大框内，用虚线轮廓标出，占据图表的下半部分。这些框每个代表一个系统组件：
- en: API (box A)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API（框A）
- en: Dataset manager (box B)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理器（框B）
- en: Model trainer (box C)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练器（框C）
- en: Model serving (box D)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务（框D）
- en: Metadata and artifacts store (box E)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据和工件存储（框E）
- en: Workflow orchestration (box F)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流编排（框F）
- en: Interactive data science environment (box G)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式数据科学环境（框G）
- en: In this book, we assume that these system components are *microservices*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们假设这些系统组件是*微服务*。
- en: '![](../Images/01-03.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-03.png)'
- en: Figure 1.3 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored. In later chapters, we discuss
    each component in detail and explain how it fits into this big picture.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 典型深度学习系统的概览，包括支持深度学习开发周期的基本组件。这个参考架构可以作为一个起点，进行进一步的定制。在后面的章节中，我们将详细讨论每个组件，并解释它们如何融入这一大局。
- en: Definition There is no single definition for microservices. Here, we will use
    the term to mean processes that communicate over a network with the HTTP or the
    gRPC protocol.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：对于微服务，没有单一的定义。在这里，我们将使用该术语来指代使用HTTP或gRPC协议与网络通信的进程。
- en: This assumption means we can expect these components to reasonably support multiple
    users with different roles securely and are readily accessible over a network
    or the internet. (This book, however, will not cover all engineering aspects of
    how microservices are designed or built. We will focus our discussion on specifics
    that are relevant to deep learning systems.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这一假设意味着我们可以合理地期望这些组件能够安全地支持具有不同角色的多个用户，并且可以方便地通过网络或互联网访问。（然而，本书将不涵盖微服务的所有工程方面的设计或构建。我们将重点讨论与深度学习系统相关的具体内容。）
- en: NOTE You may wonder whether you need to design, build, and host all deep learning
    system components on your own. Indeed, there are open source (Kubeflow) and hosted
    alternatives (Amazon SageMaker) for them. We hope that after you have learned
    the fundamentals of each component, how they fit in the big picture, and how they
    are used by different roles, you will make the best decision for your use case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能会想知道，你是否需要自己设计、构建和托管所有深度学习系统组件。实际上，有开源（Kubeflow）和托管的替代方案（Amazon SageMaker）可供选择。我们希望在你学习了每个组件的基本知识、它们如何融入整体架构以及不同角色如何使用后，你能为你的使用场景做出最佳决策。
- en: 1.2.2 Key components
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 关键组件
- en: Now let’s walk through the key components that we consider essential to a basic
    deep learning system, as shown in figure 1.3\. You may want to add additional
    components or simplify them further as you see fit for your requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细介绍我们认为对基本深度学习系统至关重要的关键组件，如图1.3所示。你可能希望根据自己的需求添加其他组件或进一步简化。
- en: Application programming interface
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序编程接口
- en: The entry point (box A in figure 1.3) of our deep learning system is an API
    that is accessible over a network. We opted for an API because the system needs
    to support not only graphical user interfaces that will be used by researchers,
    data scientists, data engineers, and the like but also applications and possibly
    other systems—for example, a data warehouse from a partner organization.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深度学习系统的入口点（图1.3中的框A）是一个通过网络访问的API。我们选择API是因为系统不仅需要支持研究人员、数据科学家、数据工程师等使用的图形用户界面，还需要支持应用程序和可能来自合作伙伴组织的数据仓库等其他系统。
- en: Although conceptually the API is the single point of entry of the system, it
    is entirely possible that the API is defined as the sum of all APIs provided by
    each component, without an extra layer that aggregates everything under a single-service
    endpoint. Throughout this book, we will use the sum of all APIs provided by each
    component directly and skip the aggregation for simplicity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在概念上API是系统的唯一入口点，但完全有可能将API定义为每个组件提供的所有API的总和，而没有额外的层将所有内容聚合在单一服务端点下。在本书中，我们将直接使用每个组件提供的所有API的总和，并跳过聚合以简化问题。
- en: Note Should you use a centralized or distributed deep learning system API? In
    the reference architecture (figure 1.3), the deep learning system API is shown
    as a single box. It should be interpreted as a logical container for the complete
    set of your deep learning system API, regardless of whether it is implemented
    on single (e.g., an API gateway that proxies for all components) or multiple service
    endpoints (direct interaction with each component). Each implementation has its
    own merits and shortcomings, and you should work with your team to figure out
    what functions best. Direct interaction with each component may be easier if you
    start with a small use case and team.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意您应该使用集中式还是分布式深度学习系统API？在参考架构（图1.3）中，深度学习系统API显示为一个单独的框。应该将其解释为深度学习系统API的完整集合的逻辑容器，无论它是在单个（例如，代理所有组件的API网关）还是多个服务端点（直接与每个组件交互）上实现的。每种实现都有其优点和缺点，您应该与团队合作找出哪种方法最有效。如果从一个小的用例和团队开始，直接与每个组件交互可能会更容易。
- en: Dataset manager
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集管理器
- en: Deep learning is based on data. There is no doubt that the data management component
    is a central piece of a deep learning system. Every learning system is a garbage-in,
    garbage-out system, so ensuring good data quality for learning is of paramount
    importance. A good data management component should provide the solution to this
    problem. It enables collecting, organizing, describing, and storing data, which
    in turn makes it possible for data to be explored, labeled, and used for training
    models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是基于数据的。毫无疑问，数据管理组件是深度学习系统的核心组成部分。每个学习系统都是垃圾进，垃圾出的系统，因此确保良好的数据质量对于学习至关重要。良好的数据管理组件应该提供解决此问题的解决方案。它使收集、组织、描述和存储数据成为可能，从而使数据可以被探索、标记和用于训练模型。
- en: 'In figure 1.3, we can see at least four relationships of the dataset manager
    (box B) with other parties:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.3中，我们至少可以看到数据管理器（盒子B）与其他方面的四种关系：
- en: Data collectors push raw data to the dataset manager to create or update datasets.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集器将原始数据推送到数据集管理器以创建或更新数据集。
- en: The workflow orchestration service (box F) executes the data process pipeline,
    which pulls data from the dataset manager to enhance the training dataset or transform
    the data format and pushes the result back.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流编排服务（盒子F）执行数据处理管道，从数据管理器中提取数据以增强训练数据集或转换数据格式，并将结果推送回去。
- en: Data scientists, researchers, and data engineers use Jupyter Notebook (box G)
    to pull data from the data manager for data exploration and examination.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家、研究人员和数据工程师使用Jupyter Notebook（盒子G）从数据管理器中提取数据进行数据探索和检查。
- en: The model training service (box C) pulls training data from the data manger
    for model training.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练服务（盒子C）从数据管理器中提取训练数据进行模型训练。
- en: In chapter 2, we will discuss dataset management in depth. Throughout the book,
    we use the term *dataset* as a unit of collected data that may be related.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们将深入讨论数据集管理。在整本书中，我们使用术语*数据集*作为可能相关的收集数据的单位。
- en: Model trainer
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练器
- en: Model trainer (aka, model training service; box C) responds to provide foundational
    computation resources, such as CPUs, RAM, and GPUs, and job management logics
    to run the model training code and produce model files. In figure 1.3, we can
    see that the workflow orchestration service (box F) tells the model trainer to
    execute a model training code. The trainer takes input training data from the
    dataset manager (box B) and produces a model. Then it uploads the model with training
    metrics and metadata to the metadata and artifacts store (box E).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练器（又称模型训练服务；盒子C）响应以提供基础计算资源，如CPU、RAM和GPU，并提供作业管理逻辑来运行模型训练代码并生成模型文件。在图1.3中，我们可以看到工作流编排服务（盒子F）告诉模型训练器执行模型训练代码。训练器从数据集管理器（盒子B）获取输入训练数据并生成模型。然后，它将模型与训练指标和元数据一起上传到元数据和工件存储（盒子E）中。
- en: 'It is usually necessary to perform intense computation on a large dataset to
    produce high-quality deep learning models that can make accurate predictions.
    Adoption of new algorithms and training libraries/frameworks is also a critical
    requirement. These requirements produce challenges on several levels:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通常需要对大型数据集进行密集计算，以产生可以进行准确预测的高质量深度学习模型。采用新的算法和训练库/框架也是关键要求。这些要求在几个层面上产生挑战：
- en: '*Capability of reducing model training time*—Despite the growing size of training
    data and complexity of model architecture, training systems must keep training
    times reasonable.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*减少模型训练时间的能力*——尽管训练数据的规模和模型架构的复杂性不断增长，但训练系统必须保持训练时间合理。'
- en: '*Horizontal scalability*—An effective production training system should be
    able to support multiple training requests from different applications and users
    simultaneously.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*水平扩展性*——一个有效的生产训练系统应该能够同时支持来自不同应用程序和用户的多个训练请求。'
- en: '*Cost of adopting new technologies*—The deep learning community is a vigorous
    one, with constant updates and improvements to algorithms and tooling (SDK, framework).
    The training system should be flexible enough to accommodate new innovations easily
    without interfering with the existing workload.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*采用新技术的成本*——深度学习社区充满活力，不断更新和改进算法和工具（SDK、框架）。训练系统应该足够灵活，能够轻松地适应新的创新，而不会干扰现有的工作负载。'
- en: In chapter 3, we will investigate different approaches to solving the aforementioned
    problems. We will not go deep into the theoretical aspect of training algorithms
    in this book, as they do not affect how we design the system. In chapter 4, we
    will look at how we can distribute training to accelerate the process. In chapter
    5, we will explore a few different approaches for optimizing training hyperparameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们将研究解决上述问题的不同方法。我们不会在本书中深入探讨训练算法的理论方面，因为它们不会影响我们如何设计系统。在第 4 章中，我们将研究如何分发训练以加速该过程。在第
    5 章中，我们将探讨几种不同的方法来优化训练超参数。
- en: Model serving
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务
- en: 'Models can be used in various settings, such as online inference for real-time
    predictions or offline inference for batch predictions using large volumes of
    input data. This is where model serving surfaces—when a system hosts the model,
    takes input prediction requests, runs model prediction, and returns the prediction
    to users. There are a few key questions to be answered:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以在各种设置中使用，例如用于实时预测的在线推理或用于批量预测的离线推理，使用大量的输入数据。这就是模型服务的地方—当系统托管模型、接受输入预测请求、运行模型预测并将预测返回给用户时。有几个关键问题需要回答：
- en: Are your inference requests coming from over the network? Or are they coming
    from sensors that need to be served locally?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的推理请求是来自网络？还是来自需要本地服务的传感器？
- en: What is an acceptable latency? Are inference requests ad hoc or streaming?
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是可接受的延迟？推理请求是临时的还是流式的？
- en: How many models are being served? Is each model individually serving a type
    of inference request, or is an ensemble of models doing so?
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多少模型正在提供服务？每个模型是单独提供某种推理请求，还是一组模型这样做？
- en: How large are model sizes? How much memory capacity do you need to budget?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的大小有多大？您需要预算多少内存容量？
- en: What model architectures need to be supported? Does it require a GPU? How much
    computing resources do you need to produce inferences? Are there other supporting
    serving components—for example, embeddings, normalization, aggregation, etc.?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要支持哪些模型架构？是否需要 GPU？您需要多少计算资源才能生成推断？是否有其他支持服务的组件—例如嵌入、归一化、聚合等？
- en: Are there sufficient resources to keep models online? Or is a swapping (such
    as moving models between memory and disk) strategy needed?
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有足够的资源保持模型在线？还是需要一种置换策略（例如在内存和磁盘之间移动模型）？
- en: From figure 1.3, the main input and output of the model serving (box D) are
    inference requests and the prediction returned, respectively. To produce inferences,
    models are retrieved from the metadata and artifacts store (box E). Some requests
    and their responses may be logged and sent to the model monitoring and evaluation
    service (not shown in figure 1.3 or covered in this book), which detects anomalies
    from this data and produces alerts. In chapters 6 and 7, we will go deeper into
    model serving architecture, explore these key aspects, and discuss their solutions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 1.3 中，模型服务的主要输入和输出（方框 D）分别是推理请求和返回的预测。为了生成推理，模型是从元数据和工件存储中检索出来的（方框 E）。一些请求及其响应可能会被记录并发送到模型监控和评估服务（图
    1.3 中未显示或本书未涉及），该服务从这些数据中检测异常并生成警报。在第 6 章和第 7 章中，我们将深入探讨模型服务架构，探讨这些关键方面，并讨论它们的解决方案。
- en: Metadata and artifacts store
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据和工件存储
- en: Imagine working on a simple deep learning application as a one-person team,
    where you have to work with only a few datasets and train and deploy only one
    type of model. You can probably keep track of how datasets, training codes, models,
    inference codes, and inferences are related to one another. These relationships
    are essential for model development and troubleshooting as you need to be able
    to trace certain observations back to the cause.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下作为一个人的团队在一个简单的深度学习应用上工作，你只需处理几个数据集并训练并部署一种类型的模型。你可能可以追踪数据集、训练代码、模型、推理代码和推理之间的关系。这些关系对于模型开发和故障排除至关重要，因为你需要能够将某些观察追溯到原因。
- en: Now imagine adding more applications, more people, and more model types. The
    number of these relationships will grow exponentially. In a deep learning system
    that is designed to support multiple types of users working on multiple datasets,
    code, and models at various stages, there is a need for a component that keeps
    track of the web of relationships. The metadata and artifacts store in a deep
    learning system does just that. Artifacts include code that trains models and
    produces inferences, as well as any generated data such as trained models, inferences,
    and metrics. Metadata is any data that describes an artifact or the relationship
    between artifacts. Some concrete examples are
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象增加更多应用、更多人员和更多模型类型。这些关系的数量将呈指数级增长。在一个为多种用户服务的深度学习系统中，这些用户在不同阶段处理多个数据集、代码和模型，存在对一个跟踪关系网络的组件的需求。深度学习系统中的元数据和工件存储正是为此而设计。工件包括训练模型和生成推理的代码，以及任何生成的数据，如训练模型、推理和指标。元数据是描述工件或工件之间关系的任何数据。一些具体的例子是
- en: The author and version of a piece of training code
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练代码的作者和版本
- en: A reference of the input training dataset and the training environment of a
    trained model
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过训练的模型的输入训练数据集和训练环境的参考
- en: Training metrics of a trained model, such as training date and time, duration,
    and the owner of the training job
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过训练的模型的训练指标，例如训练日期和时间、持续时间以及训练任务的所有者
- en: Model-specific metrics, such as model version, model lineage (data and code
    used in training), and performance metrics
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定于模型的指标，如模型版本、模型血统（训练中使用的数据和代码）以及性能指标
- en: The model, request, and inference code that produce a certain inference
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于生成某一推断的模型、请求和推理代码
- en: Workflow history, tracking each step of the model training and data process
    pipeline runs
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流历史，跟踪模型训练和数据处理流水线的每个步骤
- en: These are just a few examples of what a baseline metadata and artifacts store
    would help track. You should tailor the component to the needs of your team or
    organization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是基准元数据和工件存储可以帮助跟踪的一些例子。你应该根据你的团队或组织的需求来定制这个组件。
- en: Every other component that generates metadata and artifacts in figure 1.3 would
    flow into the metadata and artifacts store (box E). The store also plays an important
    role in model serving because it provides model files and their metadata to the
    model serving service (box D). Although not shown in the figure, custom tools
    for trace lineage and troubleshooting are usually built at the user interface
    layer, powered by the metadata and artifacts store.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3中生成元数据和工件的每一个其他组件都会流入元数据和工件存储（箱体E）。该存储在模型服务中也扮演着重要角色，因为它提供模型文件及其元数据给模型服务服务（箱体D）。虽然图中未显示，但通常在用户界面层构建自定义工具来追踪血统和故障排除，这些工具由元数据和工件存储提供动力。
- en: As we proceed through chapter 8, we will look at a baseline metadata and artifacts
    store. This store is usually the central component of a deep learning system’s
    user interface.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第八章进行时，我们将会查看一个基准元数据和工件存储。这个存储通常是深度学习系统用户界面的核心组件。
- en: Workflow orchestration
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流协调
- en: Workflow orchestration (figure 1.3, box F) is ubiquitous in many systems, where
    it helps to automatically launch computation tasks triggered by programmatic conditions.
    In the context of a machine learning system, the workflow orchestration is the
    driving force behind all the automations running in a deep learning system. It
    allows people to define workflows or pipelines—directed acyclic graphs (DAGs)—to
    glue individual tasks together with an execution order. The workflow orchestration
    component orchestrates the task executions of these workflows. Some typical examples
    are
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排（图1.3，框 F）在许多系统中是无处不在的，它有助于根据编程条件自动启动计算任务。在机器学习系统的上下文中，工作流编排是所有在深度学习系统内运行的自动化的驱动力。它允许人们定义工作流程或管道—有向无环图（DAGs）—将单个任务以执行顺序粘合在一起。工作流编排组件编排这些工作流的任务执行。一些典型的示例是
- en: Launching model training whenever a new dataset is built
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建新数据集时启动模型训练
- en: Monitoring upstream data sources, augmenting new data, transferring its format,
    notifying external labelers, and merging the new data into existing datasets
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控上游数据源，增补新数据，转换其格式，通知外部标记者，并将新数据合并到现有数据集中
- en: Deploying the trained model to the model server if it passes some accepted criteria
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果通过了一些可接受的标准，则将训练好的模型部署到模型服务器
- en: Continually monitoring model performance metrics and alerting developers when
    degradation is detected
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续监控模型性能指标并在检测到性能下降时提醒开发人员
- en: You will learn how to build or set up a workflow orchestration system in chapter
    9.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习如何在第9章中构建或设置工作流编排系统。
- en: Interactive data science environment
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式数据科学环境
- en: Customer data and models cannot be downloaded to a local workstation from production
    for compliance and security reasons. For data scientists to interactively explore
    data, troubleshoot pipeline execution in workflow orchestration, and debug models,
    a remote interactive data science environment (figure 1.3, box G) located inside
    the deep learning system is required.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于合规性和安全性原因，无法从生产环境将客户数据和模型下载到本地工作站。为了让数据科学家交互式地探索数据，在工作流编排中排查管道执行问题以及调试模型，需要一个位于深度学习系统内的远程交互式数据科学环境（图1.3，框
    G）。
- en: It is common for companies to set up their own trusted data science environment
    by using open source Jupyter Notebooks ([https://jupyter.org/](https://jupyter.org/))
    or by utilizing cloud vendors’ JupyterLab-based solutions, such as Amazon SageMaker
    Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 公司通常会使用开源 Jupyter Notebooks ([https://jupyter.org/](https://jupyter.org/)) 或利用云供应商的基于
    JupyterLab 的解决方案，如 Amazon SageMaker Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/))
    来建立自己的可信数据科学环境。
- en: 'A typical interactive data science environment should provide the following
    functionalities:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的交互式数据科学环境应提供以下功能：
- en: '*Data exploration*—Offers data scientists easy access to customer data but
    keeps it secure and compliant; there are no data leaks, and any unauthorized data
    access will be rejected.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据探索*—为数据科学家提供对客户数据的便捷访问，但保持其安全和合规性；没有数据泄漏，并且任何未经授权的数据访问将被拒绝。'
- en: '*Model prototyping*—Provides the much-needed tooling for data scientists to
    develop quick POC models inside the deep learning system.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型原型*—为数据科学家提供了必要的工具，可以在深度学习系统内快速开发 POC 模型。'
- en: '*Troubleshooting*—Enables engineers to debug any activity happening inside
    the deep learning system, such as downloading the model and playing with it to
    analyze its behavior or checking all the input/output artifacts (intermediate
    datasets or configurations) from a failed pipeline.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*故障排除*—使工程师能够调试发生在深度学习系统内的任何活动，例如下载模型并对其行为进行分析，或者检查失败管道中的所有输入/输出工件（中间数据集或配置）。'
- en: 1.2.3 Key user scenarios
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 主要用户场景
- en: To better understand how deep learning systems can be used during the development
    cycle (figure 1.1), we prepared sample scenarios that illustrate how they could
    be used. Let’s start with programmatic consumers, shown in figure 1.4\. Data collectors
    that push data to the system will usually end up at the data management service
    via the API, which collects and organizes raw data for model training.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解深度学习系统在开发周期中的使用方式（图1.1），我们准备了说明它们如何被使用的示例场景。让我们从编程消费者开始，如图1.4所示。将数据推送到系统的数据收集器通常会通过API最终到达数据管理服务，该服务收集和组织原始数据用于模型训练。
- en: '![](../Images/01-04.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-04.png)'
- en: Figure 1.4 Data is pushed from sources or collectors, through the API to the
    data management service, where the data is further organized and stored in formats
    that are more friendly for model training.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 数据从来源或收集器推送，通过 API 到数据管理服务，数据在那里进一步组织和存储为更适合模型训练的格式。
- en: Deep learning applications will usually hit the model inference service to obtain
    inferences from a trained model, which is used to power deep learning features
    that end users will consume. Figure 1.5 shows the sequence of this interaction.
    Scripts, or even full-fledged management services, can be programmatic consumers,
    too. Because they are optional, we omitted them from the figure for simplicity.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用通常会访问模型推断服务，从训练模型中获取推理结果，这些结果被用于支持最终用户消费的深度学习功能。图1.5显示了这种交互的顺序。脚本，甚至是完整的管理服务，也可以是程序化的消费者。因为它们是可选的，我们简化了图表，没有将它们包括在内。
- en: '![](../Images/01-05.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-05.png)'
- en: Figure 1.5 Deep learning applications request inferences through the API. The
    model inference service accepts and processes these requests against trained models
    and produces inferences that are returned back to applications.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 深度学习应用通过 API 请求推理。模型推断服务接受并处理对训练模型的请求，并产生返回给应用程序的推理结果。
- en: Between human consumers and the API usually lies an extra layer—the user interface.
    The interface can be web based or command-line based. Some power users may even
    skip this interface and consume the API directly. Let’s walk through each persona
    one by one.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 人类消费者和 API 之间通常还有一个额外的层——用户界面。界面可以是基于 web 的，也可以是基于命令行的。一些高级用户甚至可以跳过这个界面直接使用
    API。让我们逐个角色讨论一下。
- en: A typical scenario of researchers using the system is illustrated in figure
    1.6\. Researchers can look up available data to try out their new modeling technique.
    They access the user interface and visit the data exploration and visualization
    section, which pulls data from the data management service. A great deal of manual
    data processing might be involved in massaging it into forms that can be consumed
    by new training techniques. Once researchers settle with a technique, they can
    package it as a library for others’ consumption.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用系统的典型场景如图1.6所示。研究人员可以查找可用数据来尝试他们的新建模技术。他们访问用户界面，并访问数据探索和可视化部分，从数据管理服务中提取数据。可能会涉及大量手动数据处理，将其处理成可以被新的训练技术使用的形式。一旦研究人员确定了一种技术，他们可以将其打包为库供其他人使用。
- en: '![](../Images/01-06.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-06.png)'
- en: Figure 1.6 A usage sequence of a researcher who is interested in seeing what
    data is available for researching and developing a new modeling technique. The
    researcher interacts with a user interface that is supported by the API and data
    management behind the scenes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 研究人员使用场景序列，他对查看可用于研究和开发新建模技术的数据感兴趣。研究人员与API和幕后的数据管理支持的用户界面进行交互。
- en: Data scientists and engineers can work on use cases by first looking at what
    data is available, similar to what researchers would initially do in the previous
    paragraph. This would be supported by the data management service. They make hypotheses
    and put together data processing and training techniques as code. These steps
    can be combined to form a workflow using the workflow management service.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和工程师可以通过首先查看可用数据来研究用例，类似于上一段中研究人员最初要做的事情。这将得到数据管理服务的支持。他们做出假设，并将数据处理和训练技术组合成代码。这些步骤可以结合成一个工作流，使用工作流管理服务。
- en: When the workflow management service performs a run of the workflow, it contacts
    the data management service and the model training service to perform actual duties
    and track their progress. Hyperparameters, code versions, model training metrics,
    and test results are all stored to the metadata and artifacts store by each service
    and training code.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作流管理服务执行工作流的运行时，它联系数据管理服务和模型训练服务来执行实际任务并跟踪它们的进展。每个服务和训练代码都将超参数、代码版本、模型训练度量和测试结果存储到元数据和工件存储中。
- en: Through the user interface, data scientists and engineers can compare experimental
    runs and deduce the best way to train models. This aforementioned scenario is
    shown in figure 1.7.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用户界面，数据科学家和工程师可以比较实验运行并推断出训练模型的最佳方法。前述场景如图1.7所示。
- en: '![](../Images/01-07.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-07.png)'
- en: Figure 1.7 A usage sequence of a data scientist defining model training workflow,
    running it, and reviewing results
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 数据科学家定义模型训练工作流程、运行它并审查结果的使用序列
- en: Product managers can also look at and query all kinds of metrics throughout
    the system through the user interface. The metrics data can be supplied by the
    metadata and artifacts store.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 产品经理还可以通过用户界面查看和查询整个系统的各种指标。指标数据可以由元数据和工件存储提供。
- en: 1.2.4 Derive your own design
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 推导您自己的设计
- en: Now that we have gone over all aspects of the reference system architecture,
    let’s discuss some guidelines for customizing your own version.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经讨论了参考系统架构的所有方面，让我们讨论一些定制您自己版本的指南。
- en: Gathering goals and requirements
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 收集目标和需求
- en: The first step to designing any successful system design is to have a set of
    clear goals and requirements with which to work. These should ideally come from
    users of your system, either directly or indirectly through the product management
    team or engineering management. This short list of goals and requirements will
    help you form a vision of what your system will look like. This vision, in turn,
    should be the guideline that drives you throughout the design and development
    phases of your system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 设计任何成功系统设计的第一步是具有一组清晰的目标和要求，以便进行工作。这些理想情况下应该来自您系统的用户，直接或间接通过产品管理团队或工程管理团队。这个简短的目标和要求清单将帮助您形成您的系统将会是什么样子的愿景。这个愿景，反过来又应该是您在系统设计和开发阶段的指导方针。
- en: Note Sometimes engineers are asked to develop a system to support one or more
    deep learning applications that already exist. In this case, you may instead start
    with identifying the set of common requirements among these applications and how
    your system can be designed to help bring innovation quickly to these applications.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 有时工程师被要求开发一个支持一个或多个已经存在的深度学习应用程序的系统。在这种情况下，您可以首先确定这些应用程序中的一组共同需求，以及您的系统如何设计来快速为这些应用程序带来创新。
- en: To collect the system goals and requirements, you need to identify the different
    types of users and stakeholders, or *personas*, of the system. (This is a general
    concept that can be applied to most system design problems.) It is the users,
    after all, who will help you articulate the goals and requirements of the system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要收集系统的目标和需求，您需要确定系统的不同类型的用户和利益相关者，或者系统的*人物角色*。（这是一个通用概念，可以应用于大多数系统设计问题。）毕竟，是用户将帮助您阐明系统的目标和需求。
- en: 'Our recommendation is to start with use cases or application requirements if
    you are not sure of a good starting point. Here are some example questions that
    you can ask your users:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的建议是，如果您不确定一个好的起点，请从用例或应用需求开始。以下是一些示例问题，您可以向用户提出：
- en: '*To data engineers and product managers*—Does the system allow applications
    to collect data for training? Does the system need to handle streaming input data?
    How much data is being collected?'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*给数据工程师和产品经理*—系统是否允许应用程序收集用于训练的数据？系统是否需要处理流输入数据？正在收集多少数据？'
- en: '*To data scientists and engineers*—How do we process and label the data? Does
    the system need to provide labeling tools for external vendors? How do we evaluate
    the model? How do we handle the test dataset? Is an interactive notebooking user
    interface needed for data science work?'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*给数据科学家和工程师*—我们如何处理和标记数据？系统是否需要为外部供应商提供标注工具？我们如何评估模型？我们如何处理测试数据集？数据科学工作是否需要交互式笔记本用户界面？'
- en: '*To researchers and data scientists*—How large of a volume of data is needed
    for training models? What’s the average time of model training runs? How much
    computing and data capacity is needed for research and data science? What kind
    of experiments should the system support? What metadata and metrics need to be
    collected to evaluate different experiments?'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*给研究人员和数据科学家*—模型训练需要多大量的数据？模型训练的平均时间是多少？研究和数据科学需要多少计算和数据容量？系统应该支持哪些实验？需要收集哪些元数据和指标来评估不同的实验？'
- en: '*To product managers and software engineers*—Is model serving done on the remote
    server or on the client? Is it a real-time model inference or offline batch prediction?
    Is there a latency requirement?'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*给产品经理和软件工程师*—模型服务是在远程服务器上完成还是在客户端上完成的？它是实时模型推断还是脱机批量预测？是否有延迟要求？'
- en: '*To product managers*—What problems are we trying to solve at our organization?
    What is our business model? How are we going to gauge the effectiveness of our
    implementations?'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对产品经理* ——我们在组织中试图解决什么问题？我们的商业模式是什么？我们将如何评估我们的实施效果？'
- en: '*To security teams*—What level of security is needed in your system? Is data
    access wide open or strictly restricted/isolated? Is there an audit requirement?
    Is there a certain level of compliance or certification (e.g., General Data Protection
    Regulation, System and Organization Controls 2, etc.) that the system needs to
    achieve?'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对安全团队* ——您的系统需要什么级别的安全性？数据访问是完全开放还是严格限制/隔离？是否有审计要求？是否有一定级别的合规性或认证（例如，通用数据保护条例，系统和组织控制2等）需要系统达到？'
- en: Customizing the reference architecture
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 定制参考架构
- en: Once the design requirement and scope are clear, we can start to customize the
    reference architecture in figure 1.3\. First, we can decide whether we need to
    add or remove any components. For example, if the requirement is purely managing
    model training in a remote server farm, we could remove the workflow management
    component. If data scientists want to evaluate model performance effectively with
    production data, they could also add an experiment management component. This
    component allows data scientists to perform training and validation using full-scale
    data that already exists in the system and conduct online A/B testing against
    production traffic with previously unseen data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 设计要求和范围明确后，我们可以开始定制图1.3中的参考架构。首先，我们可以决定是否需要添加或删除任何组件。例如，如果需求仅仅是在远程服务器群中管理模型训练，我们可以删除工作流管理组件。如果数据科学家想要有效评估生产数据的模型性能，他们也可以添加一个实验管理组件。这个组件允许数据科学家使用系统中已经存在的全量数据进行训练和验证，并对生产流量进行在线A/B测试，使用以前未见过的数据。
- en: The second step is to design and implement each key component suite to your
    specific needs. Depending on the requirement, you might exclude the data streaming
    API from the dataset management service and add distributed training support if
    training speed is a concern. You can either build each key component from scratch
    or use open source software. In the rest of the book, we cover both options for
    each key component to ensure you know what to do.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是根据您的特定需求设计和实现每个关键组件套件。根据要求，您可能会从数据集管理服务中排除数据流API，并添加分布式训练支持，如果训练速度是一个问题的话。您可以从头开始构建每个关键组件，也可以使用开源软件。在本书的其余部分，我们涵盖了每个关键组件的这两种选项，以确保您知道该做什么。
- en: TIP Keep the system design simple and user friendly. The purpose of creating
    such a large deep learning system is to improve the productivity of deep learning
    development, so please keep this in mind when designing it. We want to make it
    easy for data scientists to build high-quality models without the need to learn
    what’s going on in the underlying system.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 保持系统设计简单和用户友好。创建如此庞大的深度学习系统的目的是提高深度学习开发的生产力，所以请记住这一点。我们希望使数据科学家能够构建高质量的模型，而不需要了解底层系统的运行情况。
- en: 1.2.5 Building components on top of Kubernetes
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.5 在Kubernetes之上构建组件
- en: We have introduced a list of key components that are implemented as services.
    With this number of services, you may want to manage them with a sophisticated
    system at the infrastructure level, such as Kubernetes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了一系列实现为服务的关键组件。有了这么多服务，您可能希望在基础架构层面使用一个复杂的系统来管理它们，例如Kubernetes。
- en: Kubernetes is an open source system for automating deployment, scaling, and
    management of containerized applications, which are applications that run in isolated
    runtime environments—for example, docker containers. We have seen a number of
    deep learning systems that are built on top of Kubernetes. Some people learn how
    to use Kubernetes without ever knowing why it is used to run deep learning services,
    so we want to explain the thinking behind it. If you are familiar with Kubernetes,
    please feel free to skip this section.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个用于自动化部署、扩展和管理容器化应用程序的开源系统，这些应用程序在隔离的运行时环境中运行，例如docker容器。我们已经看到了一些构建在Kubernetes之上的深度学习系统。一些人学习如何使用Kubernetes，却从未知道为什么要用它来运行深度学习服务，所以我们想解释它背后的思想。如果您熟悉Kubernetes，请随意跳过本节。
- en: Note Kubernetes is a complex platform that would require a book-length of material
    to teach, so we are only discussing its merits for a deep learning system. If
    you want to learn Kubernetes, we highly recommend you check out *Kubernetes in
    Action* (Manning, 2018), by Marko Lukša.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Kubernetes 是一个复杂的平台，需要一本书的篇幅来进行教学，所以我们只讨论它在深度学习系统中的优点。如果你想学习 Kubernetes，我们强烈推荐你阅读
    *Kubernetes in Action*（Manning，2018），作者是 Marko Lukša。
- en: Challenges for managing computing resources
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 管理计算资源的挑战
- en: Executing one docker container on a remote server seems to be a simple task,
    but running 200 containers on 30 different servers is a different story. There
    are many challenges, such as monitoring all remote servers to determine on which
    one to run the container, needing to failover a container to a healthy server,
    restarting a container when it’s stuck, following up each container run and getting
    notified when it completes, etc. To address these challenges, we must monitor
    the hardware, OS processes, and networking ourselves. Not only is it technically
    challenging, but it is also a huge amount of work.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在远程服务器上执行一个 Docker 容器似乎是一个简单的任务，但在 30 个不同的服务器上运行 200 个容器就是另外一回事了。存在许多挑战，例如监视所有远程服务器以确定在哪个上运行容器，需要将容器故障转移到健康的服务器，当容器卡住时重新启动容器，跟踪每个容器运行并在完成时收到通知等。为了解决这些挑战，我们必须自己监视硬件、操作系统进程和网络。这不仅在技术上具有挑战性，而且工作量巨大。
- en: How Kubernetes helps
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 如何帮助
- en: 'Kubernetes is an open source container orchestration platform for scheduling
    and automating the deployment, management, and scaling of containerized applications.
    Once you set up a Kubernetes cluster, your server groups’ operation (deployment,
    patching, updates) and resources become manageable. Here is a deployment example:
    you can tell Kubernetes to run a docker image with 16 GB memory and 1 GPU with
    a command; Kubernetes will allocate the resource to run this docker image for
    you.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源的容器编排平台，用于调度和自动化部署、管理和扩展容器化应用程序。一旦你设置了 Kubernetes 集群，你的服务器组的操作（部署、打补丁、更新）和资源就变得可管理了。这里有一个部署示例：你可以告诉
    Kubernetes 运行一个带有 16GB 内存和 1 个 GPU 的 Docker 镜像，Kubernetes 将为你分配资源来运行这个 Docker
    镜像。
- en: This is a huge benefit for software developers because not every one of them
    has extensive experience with hardware and deployment. With Kubernetes, we only
    need to declare the end state of our cluster, and Kubernetes will do the actual
    job to meet our goals.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这对软件开发人员来说是一个巨大的好处，因为并不是每个人都有丰富的硬件和部署经验。通过 Kubernetes，我们只需要声明集群的最终状态，Kubernetes
    就会实际完成工作以达到我们的目标。
- en: 'Besides container deployment benefits, the following are some other key Kubernetes
    functionalities that are crucial for managing our training containers:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 除了容器部署的好处之外，以下是一些其他关键的 Kubernetes 功能，对于管理我们的训练容器至关重要：
- en: '*Autoscaling features*—Kubernetes automatically resizes the number of nodes
    in the cluster based on the workload. This means if there is a sudden burst of
    user requests, Kubernetes will add the capacity automatically, which is called
    *elastic compute management*.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动缩放功能* — 根据工作负载，Kubernetes 自动调整集群中节点的数量。这意味着如果有突然的用户请求增加，Kubernetes 将自动增加容量，这被称为
    *弹性计算管理*。'
- en: '*Self-healing capabilities*—Kubernetes restarts, replaces, or reschedules pods
    when they fail or when nodes die. It also kills pods that do not respond to user-defined
    health checks.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自愈能力* — 当 Pod 失败或节点死亡时，Kubernetes 会重新启动、替换或重新调度 Pod。它还会终止不响应用户定义的健康检查的 Pod。'
- en: '*Resource utilization and isolation*—Kubernetes takes care of computing resource
    saturation; it ensures every server is fully utilized. Internally, Kubernetes
    launches application containers in *pods*. Each pod is an isolated environment
    with a computing resource guarantee, and it runs a function unit. In Kubernetes,
    multiple pods can be in one node (server) as long as their combined resource requirements
    (CPU, memory, disk) don’t exceed the node’s limitations, so servers can be shared
    by different function units easily with guaranteed isolation.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*资源利用和隔离* — Kubernetes 负责计算资源饱和；它确保每个服务器都得到充分利用。在内部，Kubernetes 在 *Pod* 中启动应用程序容器。每个
    Pod 都是一个带有计算资源保证的隔离环境，并且运行一个功能单元。在 Kubernetes 中，只要多个 Pod 的组合资源需求（CPU、内存、磁盘）不超过节点的限制，多个
    Pod 就可以在一个节点（服务器）中，因此服务器可以轻松地被不同的功能单元共享，并保证隔离。'
- en: '*Namespaces*—Kubernetes supports splitting a physical cluster into different
    virtual clusters. These virtual clusters are called *namespaces*. You can define
    resource quota per namespace, which allows you to isolate resources for different
    teams by assigning them to different namespaces.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*命名空间*——Kubernetes支持将物理集群划分为不同的虚拟集群。这些虚拟集群称为*命名空间*。您可以为每个命名空间定义资源配额，这使您可以通过将它们分配给不同的命名空间来为不同的团队隔离资源。'
- en: On the flip side, these benefits come at a cost—they consume resources as well.
    When you run a Kubernetes pod, the pod itself takes some amount of system resources
    (CPU, memory). These resources are consumed on top of those that are needed to
    run containers inside pods. Kubernetes’s overhead seems reasonable in many situations;
    for example, from an experiment published in the article “How We Minimized the
    Overhead of Kubernetes in Our Job System” ([http://mng.bz/DZBV](http://mng.bz/DZBV))
    by Lally Singh and Ashwin Venkatesan (February 2021), the CPU overhead per pod
    was about 10 ms per second.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这些好处是有代价的——它们也会消耗资源。当您运行一个Kubernetes pod时，pod本身会占用一定量的系统资源（CPU、内存）。这些资源是在运行pod内部的容器所需资源之上消耗的。在许多情况下，Kubernetes的开销似乎是合理的；例如，根据Lally
    Singh和Ashwin Venkatesan（2021年2月）在文章“我们如何将Kubernetes的开销最小化在我们的作业系统中”中发表的实验，每个pod的CPU开销约为每秒10毫秒。
- en: Note We recommend you check out appendix B to see how existing deep learning
    systems relate to the concepts presented in this chapter. In that appendix, we
    compare the reference architecture described in section 1.2.1 with Amazon SageMaker,
    Google Vertex AI, Microsoft Azure Machine Learning, and Kubeflow.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们建议您查看附录B，了解现有深度学习系统与本章介绍的概念之间的关系。在该附录中，我们将1.2.1节描述的参考架构与Amazon SageMaker、Google
    Vertex AI、Microsoft Azure Machine Learning和Kubeflow进行了比较。
- en: 1.3 Building a deep learning system vs. developing a model
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 构建深度学习系统与开发模型
- en: 'A final piece of groundwork before we begin: we think it is crucial to call
    out the differences between *building a deep learning system* and *developing
    a deep learning model*. In this book, we define *the practice of developing a
    deep learning model* to solve a problem as the process of'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前的最后一项准备工作是：我们认为强调*构建深度学习系统*和*开发深度学习模型*之间的区别至关重要。在本书中，我们将*开发深度学习模型的实践*定义为解决问题的过程
- en: Exploring available data and how it can be transformed for training
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可用数据以及如何将其转换为训练数据
- en: Determining the effective training algorithm(s) to use for the problem
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定用于解决问题的有效训练算法
- en: Training models and developing inference code to test against unseen data
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型并开发推理代码以针对未见数据进行测试
- en: Recall that a deep learning system should support not only all tasks required
    by model development but also those that need to be performed by other roles and
    make collaboration between these roles seamless. When building a deep learning
    system, you are not developing deep learning models; you are building a system
    that supports the development of deep learning models, making that process more
    efficient and scalable.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，深度学习系统不仅应支持模型开发所需的所有任务，还应支持其他角色执行的任务，并使这些角色之间的协作无缝。在构建深度学习系统时，您不是在开发深度学习模型；您正在构建一个支持深度学习模型开发的系统，使该过程更加高效和可扩展。
- en: We have found an abundance of material published about building the models.
    But we have seen precious little written about designing and building the platforms
    or systems that support those models. And that is why we wrote this book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现已有大量关于构建模型的材料发布。但是，我们几乎没有看到有关设计和构建支持这些模型的平台或系统的资料。这就是为什么我们写了这本书。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A typical machine learning project development goes through the following cycle:
    product initiation, data exploration, model prototyping, productionization, and
    production integration.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的机器学习项目开发经历以下循环：产品启动、数据探索、模型原型制作、生产化和生产集成。
- en: 'There are seven different roles involved in deep learning project development:
    a product manager, researchers, data scientists, data engineers, MLOps engineers,
    machine learning system engineers, and application engineers.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习项目开发涉及七种不同的角色：产品经理、研究人员、数据科学家、数据工程师、MLOps工程师、机器学习系统工程师和应用工程师。
- en: A deep learning system should reduce complexity in the deep learning development
    cycle.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习系统应该降低深度学习开发周期中的复杂性。
- en: With the help of deep learning systems, the data scientist, who is not expected
    to suddenly become an expert DevOps or data engineer, should be able to implement
    models in a scalable manner, set up data pipelines, and deploy and monitor models
    in production independently.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习系统的帮助下，数据科学家不需要突然成为专业的DevOps或数据工程师，但应该能够以可扩展的方式实现模型，建立数据管道，独立地部署和监控模型。
- en: An efficient deep learning system should allow data scientists to focus on interesting
    and important data science tasks.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的深度学习系统应该让数据科学家专注于有趣且重要的数据科学任务。
- en: A high-level reference architecture like the one we present in figure 1.3 can
    help you quickly start a new design. First, make your own copy and then collect
    goals and requirements. Finally, add, modify, or subtract components and their
    relationships as you see fit.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高层次的参考架构（如图1.3所示）可以帮助您快速开始一个新的设计。首先，复制一份并收集目标和需求。最后，根据需要添加、修改或减少组件及其关系。
- en: 'A basic deep learning system consists of the following key components: dataset
    manager, model trainer, model serving, metadata and artifacts store, workflow
    orchestration, and data science environment.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础的深度学习系统由以下关键组件组成：数据集管理器、模型训练器、模型服务、元数据和存储容器、工作流编排和数据科学环境。
- en: The data management component helps collect, organize, describe, and store data
    as datasets that can be used as training input. It also supports data exploration
    activities and tracks lineage between datasets. Chapter 2 will discuss data management
    in detail.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理组件帮助收集、组织、描述和存储数据作为可用于训练的数据集。它还支持数据探索活动并跟踪数据集之间的血统。第2章将详细讨论数据管理。
- en: The model training component is responsible for handling multiple training requests
    and running them efficiently provided a given, limited set of computing resources.
    Chapters 3 and 4 will review the model training component.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练组件负责处理多个训练请求，并在给定有限的计算资源的情况下高效地运行它们。第3章和第4章将回顾模型训练组件。
- en: The model serving component handles incoming inference requests, produces inferences
    with models, and returns them to requesters. It will be covered in chapters 6
    and 7.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务组件处理传入的推断请求，使用模型生成推断结果，并将其返回给请求者。章节6和7将介绍这部分内容。
- en: The metadata and artifacts store component records metadata and stores artifacts
    from the rest of the system. Any data produced by the system can be treated as
    artifacts. Most of them would be models, which come with metadata that will be
    stored in the same component. This provides complete lineage information to support
    experimentation and troubleshooting. We will talk about this component in chapter
    8.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据和存储容器组件记录元数据并存储来自系统其余部分的工件。系统产生的任何数据都可以视为工件。其中大多数将是模型，其附带的元数据将存储在同一组件中。这提供了完整的血统信息，以支持实验和故障排除。我们将在第8章中讨论这个组件。
- en: The workflow management component stores workflow definitions that chain together
    different steps in data processing and model training. It is responsible for triggering
    periodic workflow runs and tracking the progress of each run step that is being
    executed on other components—for instance, a model training step being executed
    on the model training service. In chapter 9, we will provide a walk-through of
    this component.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流管理组件存储链式定义，连接数据处理和模型训练的不同步骤。它负责触发周期性工作流运行，并跟踪正在其他组件上执行的每个运行步骤的进度—例如，在模型训练服务上执行的模型训练步骤。在第9章中，我们将介绍该组件的实例。
- en: A deep learning system should support the deep learning development cycle and
    make collaboration between multiple roles easy.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习系统应支持深度学习开发周期，并使多个角色之间的协作变得简单。
- en: Building a deep learning system is different from developing a deep learning
    model. The system is the infrastructure to support deep learning model development.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度学习系统与开发深度学习模型是不同的。系统是支持深度学习模型开发的基础设施。
