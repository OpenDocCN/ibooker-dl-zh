- en: 3 Machine learning vs. deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 机器学习与深度学习对比
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A comparison of machine learning and deep learning as methods to tackle tabular
    data problems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将机器学习和深度学习作为解决表格数据问题的方法的比较
- en: A comparison of machine learning and deep learning in terms of simplicity
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从简洁性方面比较机器学习和深度学习
- en: A comparison of machine learning and deep learning in terms of transparency
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从透明度方面比较机器学习和深度学习
- en: A comparison of machine learning and deep learning in terms of efficacy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从效果方面比较机器学习和深度学习
- en: 'There’s an open debate in the data science community about the best machine
    learning approach for tabular data. Some assert that classic machine learning
    techniques, such as gradient boosting using tools like XGBoost or LightGBM, are
    superior for most tabular data problems. Others advocate for including deep learning
    in your analysis toolkit. In this chapter, we’ll examine these two approaches
    using two concrete examples:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学界，关于表格数据的最佳机器学习方法的公开辩论正在进行。有些人断言，像XGBoost或LightGBM这样的梯度提升技术对于大多数表格数据问题来说是优越的。其他人则主张在分析工具包中包含深度学习。在本章中，我们将通过两个具体的例子来检验这两种方法：
- en: Predict the price of Airbnb New York City listings. In this example, we use
    a real-world dataset of Airbnb listings to train models that predict whether a
    new listing will have a price above or below the average listing price. We’ll
    use this example to examine simplicity, transparency, and efficacy.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测纽约市Airbnb列表的价格。在这个例子中，我们使用一个真实的Airbnb列表数据集来训练模型，预测新的列表价格是否会高于或低于该市场的平均列表价格。我们将使用这个例子来检验简洁性、透明度和效果。
- en: Predict the length of time a property in a local real estate market is on the
    market before it is sold. In this example, we use a contrived real estate listing
    dataset to illustrate the explainability aspect of transparency.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一个地区房地产市场中的房产在售出之前在市场上的时间长度。在这个例子中，我们使用一个虚构的房地产列表数据集来说明透明度的可解释性方面。
- en: 'We will focus on three criteria that are of particular value in interpreting
    scientific and business data:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注三个在解释科学和商业数据方面具有特别价值的标准：
- en: '*Simplicity*—The simpler the solution in terms of the code for the application
    and the core API of the framework, the better.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简洁性*—在应用代码和框架核心API方面越简单，越好。'
- en: '*Transparency*—A solution that is interpretable and that can be explained easily
    to a business stakeholder is best.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*透明度*—一种可解释且能轻易向商业利益相关者解释的解决方案是最好的。'
- en: '*Efficacy*—The solution that provides the best results and takes less time
    to train and implement is preferable. Also, research interest can lead to more
    effective results as new approaches are discovered.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*效果*—提供最佳结果且训练和实施时间更短的解决方案更可取。此外，随着新方法被发现，研究兴趣可以导致更有效的结果。'
- en: 3.1 Predicting Airbnb prices in New York City
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 预测纽约市Airbnb的价格
- en: 'To compare the simplicity of machine learning and deep learning, we will contrast
    two solutions to a particular tabular data classification problem: predicting
    whether a New York City Airbnb listing will have a price that is greater than
    or less than the average price for Airbnb listings in that market. The two solutions
    we will compare are'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较机器学习和深度学习的简洁性，我们将对比两种针对特定表格数据分类问题的解决方案：预测纽约市Airbnb列表的价格是否会高于或低于该市场的平均价格。我们将比较的两种解决方案是
- en: '*Machine learning*—Represented by a solution that uses XGBoost, a popular gradient-based
    approach.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习*—由使用XGBoost（一种流行的基于梯度的方法）的解决方案表示。'
- en: '*Deep learning*—Represented by a solution that uses the Keras functional API.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习*—由使用Keras功能API的解决方案表示。'
- en: We will compare the code complexity of these solutions and review what these
    two solutions tell us about the overall question of the relative simplicity of
    machine learning and deep learning approaches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较这些解决方案的代码复杂性，并回顾这两个解决方案告诉我们关于机器学习和深度学习方法相对简洁性的整体问题的信息。
- en: 3.1.1 The Airbnb NYC dataset
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 纽约市Airbnb数据集
- en: 'To solve the problem of predicting whether a New York City Airbnb listing will
    have a price that is greater than or less than the average price, we use a tabular
    dataset with details about Airbnb listings in New York City. Figure 3.1 includes
    descriptions of the columns of the NYC Airbnb dataset along with the type of data
    in each column, and the dataset, which you can see a sample of in figure 3.2,
    has been shared in Kaggle: [https://mng.bz/avJ7](https://mng.bz/avJ7).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决预测纽约市Airbnb列表的价格是否高于或低于平均价格的问题，我们使用了一个包含纽约市Airbnb列表详细信息的表格数据集。图3.1包括了纽约市Airbnb数据集列的描述以及每列的数据类型，而您可以在图3.2中看到的数据集样本已在Kaggle上共享：[https://mng.bz/avJ7](https://mng.bz/avJ7)。
- en: Each row in this dataset has the information for a single listing, and each
    column in the dataset has the values for all listings for a given characteristic.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每一行都包含单个列表的信息，而数据集中的每一列都包含给定特征的列表的所有值。
- en: '![](../Images/CH03_F01_Ryan2b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F01_Ryan2b.png)'
- en: Figure 3.1 Details about the columns in the Airbnb NYC dataset
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 Airbnb纽约市数据集中列的详细信息
- en: '![](../Images/CH03_F02_Ryan2b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Ryan2b.png)'
- en: Figure 3.2 Sample of rows from the Airbnb NYC dataset
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 Airbnb纽约市数据集的行样本
- en: 'The Airbnb NYC dataset has characteristics that make it a good choice for comparing
    approaches to solving tabular data problems:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Airbnb纽约市数据集具有使其成为比较解决表格数据问题方法的好选择的特征：
- en: It has a convenient size. With around 49,000 records, it is big enough to be
    interesting but not so huge that it requires special “big data” tools such as
    Spark to handle it.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有方便的大小。大约有49,000条记录，它足够大以引起兴趣，但又不至于庞大到需要像Spark这样的特殊“大数据”工具来处理。
- en: It has a good number of columns for a comparison between machine learning and
    deep learning. As we will find out in subsequent chapters, a dataset with only
    three or four columns would immediately favor a classic machine learning approach.
    A dataset with hundreds of columns would be difficult to examine. The Airbnb NYC
    dataset has a “just right” number of columns—enough to give deep learning a chance
    to shine but not so many columns that the dataset is difficult for a human to
    comprehend.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它拥有足够多的列来进行机器学习和深度学习的比较。正如我们将在后续章节中发现的那样，只有三到四列的数据集会立即倾向于经典的机器学习方法。拥有数百列的数据集将很难进行审查。Airbnb纽约市数据集拥有“恰到好处”的列数——足够让深度学习有机会发光，但又不会让数据集对人类来说难以理解。
- en: Because the dataset has a reasonable number of rows and columns, it is easy
    to take a quick look at it in a spreadsheet, which means we don’t have to write
    Python code every time we want to answer a question about the dataset. With a
    spreadsheet, you can sort, filter, and count aspects of the dataset quickly and
    take advantage of scripting for Excel or Google Sheets to do a more detailed investigation.
    The Airbnb NYC dataset is amenable to being examined in a spreadsheet, which means
    it can be examined with minimal effort.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集有合理数量的行和列，因此很容易在电子表格中快速查看它，这意味着我们不必每次想要回答有关数据集的问题时都编写Python代码。使用电子表格，您可以快速排序、筛选和计数数据集的各个方面，并利用Excel或Google
    Sheets的脚本功能进行更详细的调查。Airbnb纽约市数据集适合在电子表格中进行审查，这意味着它可以以最小的努力进行审查。
- en: The dataset includes an interesting range of column types, including several
    kinds of continuous columns (`minimum_nights` is integer values, `price` and `reviews_per_month`
    are floating point values, and `latitude` and `longitude` are geospatial values),
    categorical columns (`neighbourhood_group`, `neighbourhood`, and `room_type`),
    and free-form text columns (`name` and `host_name`).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集包括一系列有趣的列类型，包括几种连续列（`minimum_nights`是整数值，`price`和`reviews_per_month`是浮点值，而`latitude`和`longitude`是地理空间值），分类列（`neighbourhood_group`、`neighbourhood`和`room_type`），以及自由文本列（`name`和`host_name`）。
- en: The dataset has some warts—for example, there are missing values in some of
    the columns—but it is not so messy that it requires massive cleanup before it
    can be used to train a model. This makes it convenient to build an application
    around this dataset without getting too distracted by cleanup.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集有一些瑕疵——例如，某些列中存在缺失值——但它并不那么混乱，以至于在使用它来训练模型之前需要进行大规模的清理。这使得围绕这个数据集构建应用程序变得方便，而不会因为清理而分心太多。
- en: It is both open sourced and based on real data from a real business. As we will
    see in subsequent chapters of this book, one of the challenges of exploring machine
    learning and deep learning with tabular data is the scarcity of substantial open
    source tabular datasets that represent real business problems. The Airbnb dataset
    is a rare example of a nontrivial tabular dataset that contains information from
    a working business.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是开源的，并且基于真实业务的真实数据。正如我们将在本书的后续章节中看到的那样，使用表格数据探索机器学习和深度学习的一个挑战是缺乏代表真实业务问题的实质性开源表格数据集。Airbnb数据集是非平凡表格数据集的罕见例子，其中包含来自运营业务的信息。
- en: 'With this dataset, the target for the model is evident: `price`. In our case,
    we are deriving a target based on `price`—namely whether a given listing will
    have a price that is above or below the median price for the listings in the dataset.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个数据集，模型的目标是显而易见的：`价格`。在我们的案例中，我们根据`价格`来推导目标——即给定的列表价格是否高于或低于数据集中列表的中位数价格。
- en: In this subsection we have taken a first look at the Airbnb NYC dataset. In
    the next subsection we will look at the code that ingests this dataset to train
    a model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们初步了解了Airbnb纽约数据集。在下一个小节中，我们将查看用于训练模型的该数据集的代码。
- en: 3.1.2 Introduction to the code
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 代码简介
- en: Now that we have introduced the Airbnb NYC dataset, let’s take a look at the
    code for the solutions. We won’t go into all the details of the code in this section,
    but it’s important to have an overview of how the pieces fit together.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了Airbnb纽约数据集，让我们来看看解决方案的代码。在本节中，我们不会深入探讨代码的所有细节，但了解各个部分如何组合在一起是很重要的。
- en: Figure 3.3 summarizes the files that make up the two solutions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3总结了构成两个解决方案的文件。
- en: '![](../Images/CH03_F03_Ryan2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F03_Ryan2.png)'
- en: Figure 3.3 Files that make up the Airbnb NYC solution
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 Airbnb纽约解决方案的构成文件
- en: 'The following are more details on the files that make up the solutions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对构成解决方案的文件的更多细节：
- en: '*Input CSV* ([https://mng.bz/avJ7](https://mng.bz/avJ7)).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入CSV* ([https://mng.bz/avJ7](https://mng.bz/avJ7))。'
- en: '*Data cleanup notebook* ([https://mng.bz/gawV](https://mng.bz/gawV)). Note
    that while the XGBoost and Keras versions of the code use the same cleanup notebook,
    XGBoost has built-in capabilities, such as handling missing values, which means
    an XGBoost-only version of the cleanup notebook could be simpler than the common
    version.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据清理笔记本* ([https://mng.bz/gawV](https://mng.bz/gawV))。请注意，虽然XGBoost和Keras版本的代码使用相同的清理笔记本，但XGBoost具有内置功能，例如处理缺失值，这意味着仅XGBoost版本的清理笔记本可能比通用版本更简单。'
- en: '*Cleanup config file* ([https://mng.bz/ey7Q](https://mng.bz/ey7Q)).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*清理配置文件* ([https://mng.bz/ey7Q](https://mng.bz/ey7Q))。'
- en: '*Training config file* for XGBoost ([https://mng.bz/pKOz](https://mng.bz/pKOz)).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*XGBoost训练配置文件* ([https://mng.bz/pKOz](https://mng.bz/pKOz))。'
- en: '*Training config file* for Keras ([https://mng.bz/vKpr](https://mng.bz/vKpr)).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Keras训练配置文件* ([https://mng.bz/vKpr](https://mng.bz/vKpr))。'
- en: '*XGBoost training notebook* ([https://mng.bz/YDGA](https://mng.bz/YDGA)). Among
    the gradient boosting solutions we chose XGBoost because it is very popular and
    there is plenty of guidance online about XGBoost if we happen to run into any
    problems.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*XGBoost训练笔记本* ([https://mng.bz/YDGA](https://mng.bz/YDGA))。在梯度提升解决方案中，我们选择了XGBoost，因为它非常受欢迎，而且如果遇到任何问题，网上关于XGBoost的指导资料非常丰富。'
- en: '*Keras training notebook* ([https://mng.bz/JYwP](https://mng.bz/JYwP)). We
    chose Keras as a representative deep learning approach because, unlike alternatives
    like PyTorch or fastai on top of PyTorch, Keras is, along with raw TensorFlow,
    most commonly used in business applications. We chose Keras over one of the tabular
    deep learning libraries introduced in the section “Comparing the research success
    of gradient boosted approaches with deep learning” because Keras is more widely
    used than any of those libraries and its APIs can be compared to the APIs of XGBoost
    in a way that is more “apples to apples” than a comparison between a deep learning
    library specifically designed for tabular data and the general capabilities of
    XGBoost.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Keras训练笔记本* ([https://mng.bz/JYwP](https://mng.bz/JYwP))。我们选择Keras作为代表性的深度学习方法，因为它与PyTorch或在其之上运行的fastai等替代方案不同，Keras与原始TensorFlow一样，在商业应用中最常用。我们选择Keras而不是在“比较梯度提升方法与深度学习的研究成果”部分介绍的表格深度学习库之一，因为Keras比任何这些库都更广泛使用，并且其API可以与XGBoost的API进行更“苹果对苹果”的比较，而不是将专门为表格数据设计的深度学习库与XGBoost的一般功能进行比较。'
- en: 'For your convenience, the XGBoost and Keras solutions are shared in two separate
    folders, but most of the code is common between the two solutions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了您的方便，XGBoost和Keras解决方案被共享在两个单独的文件夹中，但两个解决方案之间的大部分代码是通用的：
- en: You can find the code for the XGBoost solution at [https://mng.bz/GeEO](https://mng.bz/GeEO).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在[https://mng.bz/GeEO](https://mng.bz/GeEO)找到XGBoost解决方案的代码。
- en: You can find the code for the Keras solution at [https://mng.bz/zZ4Q](https://mng.bz/zZ4Q).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在[https://mng.bz/zZ4Q](https://mng.bz/zZ4Q)找到Keras解决方案的代码。
- en: The differences between these two repos are limited to the training notebooks
    and the training config files.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个仓库之间的差异仅限于训练笔记本和训练配置文件。
- en: 3.1.3 A deep learning solution using Keras
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 使用Keras的深度学习解决方案
- en: Before digging deeper into the details of the Airbnb dataset and the code used
    in the solutions, let’s put the Keras solution into context of its software stack.
    Figure 3.4 shows the stack for the Keras solution to the Airbnb price prediction
    problem.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入挖掘Airbnb数据集和解决方案中使用的代码的细节之前，让我们将Keras解决方案置于其软件堆栈的上下文中。图3.4显示了Keras解决方案解决Airbnb价格预测问题的堆栈。
- en: '![](../Images/CH03_F04_Ryan2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F04_Ryan2.png)'
- en: Figure 3.4 The stack for the Airbnb NYC Keras solution
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 Airbnb NYC Keras解决方案的堆栈
- en: In chapter 8 we will go into more detail about the stack layers shown in figure
    3.4\. For now, we can observe that Keras is the high-level deep learning API that
    we use to implement the deep learning solution to the Airbnb problem that we will
    examine in this chapter. There are two low-level deep learning frameworks, and
    the deep learning solution we will examine in this chapter depends on the TensorFlow
    low-level framework because that is the framework on which Keras is built.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章中，我们将更详细地介绍图3.4中显示的堆栈层。目前，我们可以观察到Keras是我们用来实现Airbnb问题深度学习解决方案的高级深度学习API。有两个低级深度学习框架，我们将在本章中考察的深度学习解决方案依赖于TensorFlow低级框架，因为Keras就是建立在它之上的。
- en: 3.1.4 Training features
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 训练特征
- en: 'The goal of both solutions is to predict whether a given Airbnb listing will
    have a price that is above or below the average price in the input dataset. To
    achieve this goal, both models are trained on the same set of features. The subset
    of features that we will use to train the model is defined in the config file
    for model training: [https://mng.bz/OBoE](https://mng.bz/OBoE). The following
    is the portion of the config file that specifies the features used to train the
    models:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 两个解决方案的目标都是预测给定的Airbnb列表的价格是否高于或低于输入数据集的平均价格。为了实现这个目标，两个模型都在同一组特征上进行了训练。我们将用于训练模型的特征子集在模型训练的配置文件中定义：[https://mng.bz/OBoE](https://mng.bz/OBoE)。以下是指定用于训练模型特征的配置文件的部分：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The config file also includes a list of features that are explicitly excluded
    from the training process:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件还包括一个列表，其中明确排除了从训练过程中排除的特征：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The reasons why these columns are not used as features to train the model are
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列没有被用作训练模型的特征的原因如下：
- en: '`price` is not included as a feature for training the model because it defines
    the target for the model, whether or not a listing has a price above or below
    the average price.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`price`没有被包括为训练模型的特征，因为它定义了模型的目标，即列表的价格是否高于或低于输入数据集的平均价格。'
- en: The two ID columns are not included as features because they don’t carry any
    signal about the price of a listing because they are just numeric IDs assigned
    to listings and hosts.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个ID列没有被包括为特征，因为它们不携带任何关于列表价格的信息，因为它们只是分配给列表和房东的数字ID。
- en: 'We don’t use `latitude` or `longitude` as features because the geographic location
    of listings is already encoded in the `neighbourhood_group` and `neighbourhood`
    features that are used to train the model. If we didn’t have these features to
    use for the geographic location of the listings, we could use the `latitude` and
    `longitude` values (or polar coordinates derived from them: [https://mng.bz/0Q66](https://mng.bz/0Q66))
    to cluster listings according to their locations or convert them into polar coordinates.
    Using the raw latitude and longitude for each listing as features could lead to
    overfitting because each listing would have a unique value for the pair (`latitude`,
    `longitude`).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不使用`latitude`或`longitude`作为特征，因为列表的地理位置已经编码在用于训练模型的`neighbourhood_group`和`neighbourhood`特征中。如果我们没有这些特征来用于列表的地理位置，我们可以使用`latitude`和`longitude`值（或从它们导出的极坐标：[https://mng.bz/0Q66](https://mng.bz/0Q66)）来根据位置对列表进行聚类或将它们转换为极坐标。使用每个列表的原始纬度和经度作为特征可能会导致过拟合，因为每个列表都会有一对独特的值（`latitude`，`longitude`）。
- en: The `name` and `host_name` columns are not included as features because they
    are somewhat arbitrary sets of tokens that allow human readers to identify the
    listings. An interesting exercise would be to include `host_name` as a feature
    to see if it provides some kind of signal related to the price of listings that
    have the same host.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`和`host_name`列不作为特征，因为它们是某种任意的标记集，允许人类读者识别列表。一个有趣的练习是将`host_name`作为特征，看看它是否提供了与具有相同主人的列表价格相关的某种信号。'
- en: We decided to not include `availabiltiy_365` in the feature set because the
    column is challenging to interpret.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们决定不将`availabiltiy_365`包含在特征集中，因为该列难以解释。
- en: We have examined the features that we will use to train the model to predict
    whether an Airbnb listing will have a price above or below the average price.
    In the next section, we will compare the simplicity of the code for the gradient
    boosting and deep learning models trained on this dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经检查了我们将用于训练模型以预测Airbnb列表价格是否高于或低于平均价格的特征。在下一节中，我们将比较基于此数据集训练的梯度提升和深度学习模型的代码简单性。
- en: 3.1.5 Comparing gradient boosting and deep learning solutions
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 比较梯度提升和深度学习解决方案
- en: As we mentioned previously, the two solutions for the Airbnb NYC problem only
    differ in a few places. Figure 3.5 shows the file structure of the solution again
    with the files that contain differences between the two approaches highlighted.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，解决Airbnb纽约问题的两种解决方案仅在少数地方有所不同。图3.5再次显示了解决方案的文件结构，其中突出显示了两种方法之间的差异文件。
- en: '![](../Images/CH03_F05_Ryan2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F05_Ryan2.png)'
- en: Figure 3.5 Airbnb solution files that differ between the XGBoost and Keras solutions
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 XGBoost和Keras解决方案之间不同的Airbnb解决方案文件
- en: If the solutions only differ in these four files, how can we use this example
    to contrast the simplicity of XGBoost vs Keras deep learning? Table 3.1 compares
    the code complexity across several aspects of the application.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果解决方案仅在四个文件中有所不同，我们如何使用这个例子来对比XGBoost与Keras深度学习的简单性？表3.1比较了应用程序的多个方面的代码复杂性。
- en: Table 3.1 Comparing the code complexity of XGBoost and Keras in three areas
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 比较XGBoost和Keras在三个领域的代码复杂性
- en: '| Aspect of code complexity | XGBoost | Keras deep learning model |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 代码复杂性方面 | XGBoost | Keras深度学习模型 |'
- en: '| --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Data preparation | Code block required to transform the list of numpy arrays
    into a numpy array of lists | NA—the data preparation was designed with the Keras
    deep learning solution in mind |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 数据准备 | 需要代码块将numpy数组列表转换为numpy数组列表 | NA——数据准备是为Keras深度学习解决方案设计的 |'
- en: '| Model definition | Single statement, consistent with the Scikit-learn pattern
    | Block of code required to define the layers of the model, with unique sets of
    layers for each column type: continuous, categorical, text |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 模型定义 | 单个语句，与Scikit-learn模式一致 | 需要代码块来定义模型的层，每个列类型（连续、分类、文本）都有独特的层集 |'
- en: '| Model training | Single statement, consistent with the Scikit-learn pattern
    | Block of code required to allow for callback control of the training process
    to avoid training iterations that provide no benefit and to ensure the training
    process outputs the trained model with the best performanceBlock of code to define
    the callback objects required to have an efficient Keras training cycle |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型训练 | 单个语句，与Scikit-learn模式一致 | 需要的代码块，以允许回调控制训练过程，避免提供无益的训练迭代，并确保训练过程输出性能最佳的训练模型
    | 定义回调对象所需的代码块，以实现高效的Keras训练周期 |'
- en: '| Model saving | Single statement, consistent with the Scikit-learn pattern
    | Included as part of model saving callback |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型保存 | 单个语句，与Scikit-learn模式一致 | 作为模型保存回调的一部分包含在内 |'
- en: '| Model loading | Block of code—requires installing the latest version of XGBoost
    or loaded model will fail with error:`AttributeError: ''XGBClassifier'' object
    has no attribute ''_le``''`  | Single statement |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 模型加载 | 代码块——需要安装最新版本的XGBoost或加载的模型将因错误`AttributeError: ''XGBClassifier''对象没有属性''_le''`而失败
    | 单个语句 |'
- en: Let’s look at the code in the Airbnb NYC solution for each of these aspects
    of code complexity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Airbnb纽约解决方案中这些代码复杂性的各个方面。
- en: The XGBoost solution has some additional data preparation code. The Airbnb NYC
    solution was originally coded using Keras deep learning and then the XGBoost solution
    was created using the Keras solution as a starting point. The original Keras model
    required training input in the form of a list of numpy arrays. XGBoost requires
    input in the form of a numpy array of lists. The following listing contains the
    code in the XGBoost training notebook that converts the original data format into
    the format required by XGBoost.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost解决方案有一些额外的数据准备代码。Airbnb纽约解决方案最初是用Keras深度学习编写的，然后使用Keras解决方案作为起点创建了XGBoost解决方案。原始的Keras模型需要以numpy数组列表的形式提供训练输入。XGBoost需要以列表的numpy数组形式提供输入。以下列表包含XGBoost训练笔记本中转换原始数据格式为XGBoost所需格式的代码。
- en: Listing 3.1 Data preparation code for XGBoost
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 XGBoost的数据准备代码
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Defines lists of lists for the training and test datasets (one list for each
    feature)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义训练和测试数据集的列表的列表（每个特征一个列表）
- en: ② Converts the train list of lists into a numpy array of lists
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将列表的列表转换为numpy数组的列表
- en: ③ Converts the test list of lists into a numpy array of lists
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将测试列表的列表转换为numpy数组的列表
- en: Note that while this is a genuine example of the XGBoost code having some additional
    complexity compared to the Keras code, this additional code is not intrinsically
    required by XGBoost but rather is needed because of the way that the XGBoost solution
    was adapted from the Keras solution.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然这是一个XGBoost代码相对于Keras代码具有一些额外复杂性的真实示例，但这种额外的代码并非XGBoost固有的需求，而是由于XGBoost解决方案是从Keras解决方案改编而来的方式所必需的。
- en: To get additional confirmation of what the XGBoost data preparation code does,
    go to Gemini ([https://gemini.google.com](https://gemini.google.com)) and paste
    the code into the entry field along with the prompt “what does this code do?”
    and click the Submit button, as shown in figure 3.6.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步确认XGBoost数据准备代码的作用，请访问Gemini（[https://gemini.google.com](https://gemini.google.com)），将代码粘贴到输入字段中，并附带提示“这段代码做什么？”然后点击提交按钮，如图3.6所示。
- en: '![](../Images/CH03_F06_Ryan2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F06_Ryan2.png)'
- en: Figure 3.6 Entering a request to interpret code in Gemini
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 在Gemini中输入解释代码的请求
- en: 'You will get a response that explains what the code does along with this summary:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您将获得一个解释代码功能以及以下摘要的响应：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have looked at the difference between XGBoost and Keras in terms
    of data preparation, let’s compare the model definition code for the two solutions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经比较了XGBoost和Keras在数据准备方面的差异，让我们比较两种解决方案的模型定义代码。
- en: 'The following is the XGBoost model definition for the Airbnb NYC problem—one
    line of code that follows the pattern of Scikit-learn:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为Airbnb纽约问题定义的XGBoost模型——遵循Scikit-learn模式的单行代码：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s look at what the model definition code looks like for the Keras solution.
    Figure 3.7 shows the initial part of the function that defines the deep learning
    model for the Airbnb NYC problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Keras解决方案的模型定义代码是什么样的。图3.7显示了定义Airbnb纽约问题深度学习模型的函数的初始部分。
- en: '![](../Images/CH03_F07_Ryan2.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Ryan2.png)'
- en: Figure 3.7 Model definition for the Keras deep learning solution for the Airbnb
    NYC problem (part 1)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 Airbnb纽约问题的Keras深度学习解决方案的模型定义（第一部分）
- en: Figure 3.8 shows the rest of the function that defines the deep learning model
    for the Airbnb NYC problem.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 展示了定义 Airbnb 纽约问题深度学习模型的其余函数。
- en: '![](../Images/CH03_F08_Ryan2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Ryan2.png)'
- en: Figure 3.8 Model definition for the Keras deep learning solution for the Airbnb
    NYC problem (part 2)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 Airbnb 纽约问题的 Keras 深度学习解决方案的模型定义（第二部分）
- en: Figure 3.9 shows the visualization for the model for the Airbnb problem.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 展示了 Airbnb 问题的模型可视化。
- en: '![](../Images/CH03_F09_Ryan2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F09_Ryan2.png)'
- en: Figure 3.9 Visualization of the Keras model for the Airbnb problem
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 Airbnb 问题的 Keras 模型可视化
- en: 'This is a bit of an extreme contrast—the deep learning model could have been
    defined more simply. This model definition specifies a different set of Keras
    layers for each column type (continuous, categorical, and text). This is not the
    minimal layer definition possible for a tabular data model, but it is very flexible.
    It will be able to deal with tabular datasets with various combinations of continuous,
    categorical, and text columns. Also, this model definition includes code that
    specifies the layers for text columns, and we did not choose any text columns
    to train the Airbnb model, so this code could have been omitted without affecting
    the Keras version of the Airbnb NYC solution. Nevertheless, the difference between
    the simplicity of the XGBoost model definition and the complexity of the Keras
    model definition underlines an advantage of XGBoost: model definition code is
    simpler in XGBoost than it is in Keras.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种极端的对比——深度学习模型本可以定义得更简单。这个模型定义指定了每个列类型（连续、分类和文本）的不同 Keras 层。这不是表格数据模型可能的最小层定义，但它非常灵活。它将能够处理具有各种连续、分类和文本列组合的表格数据集。此外，这个模型定义包括指定文本列层的代码，而我们没有选择任何文本列来训练
    Airbnb 模型，因此这段代码可以被省略，而不会影响 Airbnb 纽约解决方案的 Keras 版本。尽管如此，XGBoost 模型定义的简洁性与 Keras
    模型定义的复杂性之间的差异突出了 XGBoost 的一个优势：XGBoost 中的模型定义代码比 Keras 中的更简单。
- en: 'We have compared the data preparation code and the model definition code. Next,
    let’s compare the model training code for the XGBoost and Keras solutions. In
    the XGBoost solution, training is accomplished with a single line of code with
    defaults accepted for all parameters that have default values:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经比较了数据准备代码和模型定义代码。接下来，让我们比较 XGBoost 和 Keras 解决方案中的模型训练代码。在 XGBoost 解决方案中，通过一行代码完成训练，并接受所有具有默认值的参数的默认值：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The following listing shows that for the Keras solution there are two different
    versions of the fit statement and several additional parameters (including the
    batch size and the default number of epochs that will be run in the training process)
    need to be set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示，对于 Keras 解决方案，需要设置两种不同的拟合语句版本和几个额外的参数（包括批量大小和训练过程中将运行的默认epoch数）。
- en: Listing 3.2 Fit statements for Keras
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 Keras 的拟合语句
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Fit statement including a parameter that lists callbacks for early stopping
    and model saving
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ① 包含一个参数的拟合语句，该参数列出用于早期停止和模型保存的回调
- en: ② Fit statement without callbacks
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ② 不包含回调的拟合语句
- en: There isn’t a huge difference in the complexity of the training code between
    XGBoost and Keras. However, to make the Keras training process efficient, we need
    to use callbacks to avoid wasted training cycles and ending up with a suboptimal
    trained model at the end of the training process. See chapter 6 of *Deep Learning
    with Structured Data* ([https://mng.bz/KGx0](https://mng.bz/KGx0)) for details
    on using Keras callbacks to optimize the training process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 和 Keras 在训练代码的复杂性上没有太大的差异。然而，为了使 Keras 的训练过程高效，我们需要使用回调来避免浪费训练周期，并在训练过程结束时得到一个次优化的训练模型。有关使用
    Keras 回调优化训练过程的详细信息，请参阅《结构化数据深度学习》第 6 章 ([https://mng.bz/KGx0](https://mng.bz/KGx0))。
- en: 'Before taking a look at the callback statements for the Keras solution, let’s
    see what Gemini can tell us about the fit statements. Again, let’s submit this
    code to Gemini preceded with the prompt “what does this code do?” If you don’t
    get a satisfactory answer the first time, click Regenerate draft to get another
    answer. On the second try with Gemini, we got a detailed description of the code
    that included the following description of the parameters for the fit statement:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看Keras解决方案的回调语句之前，让我们看看Gemini能告诉我们关于fit语句的什么信息。同样，让我们将此代码提交给Gemini，并在前面加上提示“这段代码做什么？”如果你第一次没有得到满意的答案，点击“重新生成草稿”以获取另一个答案。在第二次使用Gemini时，我们得到了对代码的详细描述，包括以下对fit语句参数的描述：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Gemini also provides the following summary:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini还提供了以下摘要：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note how in the last line Gemini qualifies the limitations of its analysis by
    correctly noting that given only the training snippet, it cannot infer all the
    details about the complete solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在最后一行，Gemini通过正确地指出，仅给定训练片段，它无法推断出完整解决方案的所有细节，从而对其分析的限制进行了说明。
- en: Listing 3.3 shows the code that defines the callbacks that are used in the model
    training step for Keras. This code adds additional complexity in the Keras version
    of the Airbnb NYC solution.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3显示了定义用于Keras模型训练步骤的回调的代码。这段代码在Keras版本的Airbnb NYC解决方案中增加了额外的复杂性。
- en: Listing 3.3 Callback statements for Keras
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3 Keras的回调语句
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Defines an early stopping callback object to specify that the training process
    should stop once the performance stops improving
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义了一个早期停止回调对象，指定一旦性能停止提高，训练过程就应该停止
- en: ② Adds the first callback to the list of callbacks to be used in the training
    process
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将第一个回调添加到训练过程中要使用的回调列表中
- en: ③ Defines a model-saving callback object to ensure that the trained model that
    has the optimal performance in the whole training run is the model that is saved
    at the end of the training run
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义一个模型保存回调对象，以确保在整个训练运行中性能最优的模型是在训练运行结束时保存的模型
- en: ④ Adds the second callback to the list of callbacks to be used in the training
    process
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将第二个回调添加到训练过程中要使用的回调列表中
- en: Once we have trained the model, we want to save it to a file so that we can
    load it and exercise it in another session or as part of the model’s deployment.
    In our simple example, we save and reload the model in the same notebook as the
    one we use to train the model. The statement to save the model for XGBoost is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，我们希望将其保存到文件中，以便我们可以在另一个会话或作为模型部署的一部分加载和练习它。在我们的简单示例中，我们将模型保存和重新加载到用于训练模型的同一个笔记本中。保存XGBoost模型的语句是
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For Keras, we don’t need an explicit statement to save the model because the
    model is saved automatically with the model-saving callback.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Keras，我们不需要显式保存模型的语句，因为模型会自动随着模型保存回调一起保存。
- en: The following listing shows the code to load the model in XGBoost.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了在XGBoost中加载模型的代码。
- en: Listing 3.4 Model-loading statements for XGBoost
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 XGBoost的模型加载语句
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Defines a new XGBoost classifier object
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义了一个新的XGBoost分类器对象
- en: ② Loads the new XGBoost classifier object with the model you saved with the
    save_model statement
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用save_model语句保存的模型加载新的XGBoost分类器对象
- en: The statement to load a model for Keras is
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Keras模型的语句是
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'One additional difference between XGBoost and Keras is that if you try to load
    a saved XGBoost classifier model (like the one we trained for the Airbnb NYC problem)
    and run a prediction, you will get an error if you are not on a very current version
    of XGBoost. To get around this, the XGBoost model training notebook includes the
    following statement to ensure that the XGBoost is at the latest version:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost和Keras之间一个额外的区别是，如果你尝试加载一个保存的XGBoost分类器模型（例如我们为Airbnb NYC问题训练的模型）并运行预测，如果你不在非常新的XGBoost版本上，你会得到一个错误。为了解决这个问题，XGBoost模型训练笔记本中包含了以下语句，以确保XGBoost是最新版本：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this section, we have compared the simplicity of the XGBoost and Keras code
    for data preparation, model definition, model training, and model saving. Next,
    we will discuss the conclusions that we can draw from this comparison.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了XGBoost和Keras在数据准备、模型定义、模型训练和模型保存方面的代码简洁性。接下来，我们将讨论从这个比较中可以得出的结论。
- en: 3.1.6 Conclusions
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.6 结论
- en: One of the best pieces of advice for data science projects is to take the simplest
    approach possible. Apply Occam’s razor to a data science project to make the task
    easier. If there is more than one way to solve a problem, pick the simplest approach.
    If linear regression will solve the problem, then why use support vector machines?
    If a conventional coding approach will solve the problem, then why use machine
    learning at all? If you take the simplest approach, you will likely get initial
    results faster, complete development of the whole solution faster, and have an
    easier time maintaining the system once it has been deployed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学项目来说，最好的建议之一就是采取尽可能简单的方法。将奥卡姆剃刀原则应用于数据科学项目，以简化任务。如果解决问题有多种方法，请选择最简单的方法。如果线性回归可以解决问题，那么为什么还要使用支持向量机？如果传统的编码方法可以解决问题，那么为什么还要使用机器学习呢？如果你采取最简单的方法，你可能会更快地获得初步结果，更快地完成整个解决方案的开发，并且在部署后维护系统也会更加容易。
- en: 'To answer the question of how classic machine learning and deep learning compare
    in terms of simplicity, we compared solutions using each approach to a concrete
    problem: the Airbnb NYC price prediction problem. By answering this question,
    we can apply the dictum “keep it simple” to tabular data problems.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答经典机器学习和深度学习在简单性方面的比较问题，我们比较了使用每种方法解决一个具体问题的解决方案：Airbnb纽约市价格预测问题。通过回答这个问题，我们可以将“保持简单”的原则应用于表格数据问题。
- en: In the previous subsection we reviewed the Airbnb NYC tabular dataset and compared
    the complexity of two solutions trained with this dataset—one that uses a gradient
    boosting approach (XGBoost) and one that uses deep learning (Keras). The XGBoost
    solution required some additional data preparation code, but this additional code
    is an artifact of the XGBoost solution being adapted from the deep learning solution,
    not a direct requirement of XGBoost. For the XGBoost solution, only one line of
    code was needed for model definition and one line of code for model training.
    For Keras, on the other hand, model definition required many lines of code, and
    model training required more lines of code than XGBoost, in particular to take
    advantage of Keras callbacks to ensure an efficient training process. The complexity
    of the code to save and load the model was about the same in the XGBoost solution
    and the Keras solution. While the Airbnb NYC problem is not representative of
    all tabular data problems, it does give us the opportunity to make an apples to
    apples comparison between a gradient boosting approach and a deep learning approach,
    and the conclusion of this comparison is that the XGBoost code is simpler. The
    XGBoost solution has fewer lines of code, and the statements for model definition
    and model training are simpler and require fewer nondefault parameter values in
    XGBoost than they do in Keras.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的子节中，我们回顾了Airbnb纽约市表格数据集，并比较了使用该数据集训练的两个解决方案的复杂性——一个使用梯度提升方法（XGBoost），另一个使用深度学习（Keras）。XGBoost解决方案需要一些额外的数据准备代码，但这些额外的代码是XGBoost解决方案从深度学习解决方案中改编而来的产物，而不是XGBoost的直接需求。对于XGBoost解决方案，模型定义只需要一行代码，模型训练也只需要一行代码。而对于Keras来说，模型定义需要很多行代码，模型训练需要的代码行数比XGBoost多，特别是为了利用Keras回调确保高效的训练过程。保存和加载模型的代码复杂性在XGBoost解决方案和Keras解决方案中大致相同。虽然Airbnb纽约市问题不能代表所有表格数据问题，但它为我们提供了将梯度提升方法和深度学习方法进行苹果对苹果比较的机会，并且比较的结论是XGBoost代码更简单。XGBoost解决方案的代码行数更少，模型定义和模型训练的语句更简单，在XGBoost中比在Keras中需要的非默认参数值更少。
- en: Before closing our conclusion on the comparative simplicity of gradient boosting
    and deep learning, we need to note that Keras isn’t the only deep learning approach
    available. There are other deep learning approaches that deal with tabular data,
    and some of these approaches have code for defining and training models that is
    simpler than the Keras code that we examined for the Airbnb NYC problem. For example,
    with the fastai framework ([https://docs.fast.ai/](https://docs.fast.ai/)), which
    we will review in more detail in chapter 9, you can define a model to work with
    tabular data, train it, and use it to get predictions in less than 10 lines of
    code. Tensorflow canned estimators ([https://mng.bz/9Y51](https://mng.bz/9Y51))
    are another simple approach to deep learning with tabular data. With these canned
    estimators, you can train a model on a tabular dataset and get predictions from
    the model with an API that is as simple as the XGBoost API. These are just two
    examples of deep learning approaches with code that is simpler than Keras. The
    benefit of Keras is that it is very flexible, and its flexibility is one of the
    reasons that businesses frequently use Keras for production systems while simpler
    approaches, such as fastai, are rarely seen in production deployments.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得出关于梯度提升和深度学习比较简单性的结论之前，我们需要注意，Keras并不是唯一可用的深度学习方法。还有其他处理表格数据的深度学习方法，其中一些方法定义和训练模型的代码比我们为Airbnb纽约问题所检查的Keras代码更简单。例如，使用fastai框架（[https://docs.fast.ai/](https://docs.fast.ai/)），我们将在第9章中更详细地介绍，你可以定义一个用于处理表格数据的模型，训练它，并用不到10行代码来获取预测。Tensorflow预置估算器（[https://mng.bz/9Y51](https://mng.bz/9Y51)）是另一种简单的表格数据深度学习方法。使用这些预置估算器，你可以在表格数据集上训练一个模型，并通过与XGBoost
    API一样简单的API从模型中获取预测。这些只是两个代码比Keras更简单的深度学习方法的例子。Keras的优点是它非常灵活，其灵活性是企业在生产系统中经常使用Keras而较少在生产部署中看到简单方法（如fastai）的原因之一。
- en: Now that we have compared machine learning and deep learning in terms of simplicity,
    in the next section we will compare the two approaches in terms of transparency.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从简单性的角度比较了机器学习和深度学习，在下一节中，我们将从透明度的角度比较这两种方法。
- en: 3.2 Transparency
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 透明度
- en: 'There are two aspects of transparency that are relevant when comparing gradient
    boosting techniques with deep learning: explainability (that is, how easy it is
    to explain how the model works) and feature importance (that is, how easy it is
    to determine which feature has the biggest effect). In this section, we’ll compare
    gradient boosting with deep learning according to these two aspects of transparency.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较梯度提升技术与深度学习时，有两个与透明度相关的方面：可解释性（即解释模型工作原理的难易程度）和特征重要性（即确定哪个特征影响最大的难易程度）。在本节中，我们将根据这两个透明度方面比较梯度提升与深度学习。
- en: To compare the explainability of gradient boosting and deep learning, we will
    consider a simple, contrived dataset and how a model trained on this dataset could
    be explained.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较梯度提升和深度学习的可解释性，我们将考虑一个简单、人为构造的数据集，以及在这个数据集上训练的模型如何进行解释。
- en: The dataset that we will use contains information about houses in a particular
    real estate market, as shown table 3.2.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集包含特定房地产市场的房屋信息，如表3.2所示。
- en: Table 3.2 House time on the market dataset
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 房屋上市时间数据集
- en: '| Time on market (weeks) | City | Asking price (thousand $) | Distance to transit
    station (km) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 上市时间（周） | 城市 | 出售价格（千美元） | 到交通站距离（公里） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 6 | Kitchener | 600 | 10 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Kitchener | 600 | 10 |'
- en: '| 5 | Waterloo | 700 | 5 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Waterloo | 700 | 5 |'
- en: '| 12 | Kitchener | 900 | 20 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 12 | Kitchener | 900 | 20 |'
- en: '| 6 | Waterloo | 700 | 15 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Waterloo | 700 | 15 |'
- en: '| 1 | Waterloo | 500 | 5 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Waterloo | 500 | 5 |'
- en: '| 4 | Waterloo | 600 | 5 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Waterloo | 600 | 5 |'
- en: '| 8 | Waterloo | 750 | 5 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Waterloo | 750 | 5 |'
- en: '| 2 | Kitchener | 500 | 5 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Kitchener | 500 | 5 |'
- en: '| 9 | Kitchener | 1000 | 5 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Kitchener | 1000 | 5 |'
- en: '| 4 | Waterloo | 750 | 10 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Waterloo | 750 | 10 |'
- en: We will return to the Airbnb dataset in the next section. For now, this real
    estate dataset is simple enough to easily illustrate explainability.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节回到Airbnb数据集。目前，这个房地产数据集足够简单，可以很容易地说明可解释性。
- en: The dataset includes the city the house is located in, the asking price for
    the house, and the distance from the house to the closest transit station, along
    with the number of weeks the house was on the market before it was sold. We want
    to train a model using this dataset that will predict whether a house that is
    a fresh listing will be on the market for more or less than a month.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包括房屋所在的城镇、房屋的挂牌价格以及房屋到最近的交通站的距离，以及房屋售出前在市场上停留的周数。我们想要使用这个数据集训练一个模型，该模型将预测一个新挂牌的房屋将在市场上停留超过一个月还是不到一个月。
- en: 3.2.1 Explainability
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 可解释性
- en: Suppose that we wanted to give a nonspecialist, business audience an idea of
    how a decision tree model could be used to solve the “time in the market” problem
    for this dataset. We could create an illustration like the one in figure 3.10
    to give a rough idea of how such a decision tree works.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要给非专业人士、商业观众一个关于决策树模型如何用于解决这个数据集的“上市时间”问题的想法。我们可以创建一个如图3.10所示的插图，以给出这样一个决策树如何工作的粗略概念。
- en: '![](../Images/CH03_F10_Ryan2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F10_Ryan2.png)'
- en: Figure 3.10 Decision tree illustration
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 决策树插图
- en: Note that the illustration doesn’t contain any jargon or assume that the reader
    has any background in machine learning. The point of the decision tree is evident.
    Note this decision tree is a gross simplification, and there are important technical
    differences between a simple decision tree like this and a model that uses gradient
    boosting. XGBoost, for example, uses multiple decision trees, so this illustration
    by itself would not be sufficient to explain an XGBoost model. Nevertheless, it
    shows that, for some classic machine learning algorithms, it’s possible to give
    nonspecialists an intuition about how the algorithms work without forcing the
    nonspecialists to learn technical details about them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个插图不包含任何术语，也不假设读者有任何机器学习背景。决策树的目的很明显。注意这个决策树是一个粗略的简化，与这种简单的决策树和梯度提升模型之间存在重要的技术差异。例如，XGBoost使用多个决策树，所以这个插图本身不足以解释XGBoost模型。尽管如此，它表明，对于一些经典的机器学习算法，可以在不强迫非专业人士学习它们的详细技术信息的情况下，给非专业人士一个关于算法如何工作的直观理解。
- en: What if we wanted to give the same nonspecialist audience a general sense of
    how a deep learning model would be trained to solve the same “time on the market”
    problem? We could start with a generic neural network schematic like the one shown
    in figure 3.11.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要给同样是非专业人士的观众一个关于深度学习模型如何被训练来解决相同的“上市时间”问题的总体感觉呢？我们可以从一个如图3.11所示的通用神经网络示意图开始。
- en: '![](../Images/CH03_F11_Ryan2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F11_Ryan2.png)'
- en: Figure 3.11 Neural network illustration
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 神经网络插图
- en: Such an illustration may help to explain the “deep” in “deep learning,” but
    it provides no insight about how the model is actually trained. Would it help
    if we zoomed in to show how an individual node in the network works, as shown
    in figure 3.12?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的插图可能有助于解释“深度学习”中的“深度”，但它并没有提供关于模型实际是如何训练的任何见解。如果我们放大以展示网络中单个节点的工作，如图3.12所示，会有帮助吗？
- en: '![](../Images/CH03_F12_Ryan2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F12_Ryan2.png)'
- en: Figure 3.12 Simple illustration of a node in a neural network
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 神经网络中节点的简单插图
- en: Most nonspecialists would find it hard to interpret figure 3.12\. If zooming
    into details of the neural network doesn’t yield better explainability, what if
    we take a different approach and lean on the analogy between neural networks and
    biological neurons, as shown in figure 3.13? This illustration attempts to relate
    the overall neural network to the working of an individual node in the network
    (a “neuron”) and then relate that node to a biological neuron.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非专业人士会发现解读图3.12很困难。如果放大神经网络细节并不能提高可解释性，那么如果我们采取不同的方法，借鉴神经网络与生物神经元之间的类比，如图3.13所示，会怎么样呢？这个插图试图将整个神经网络与网络中单个节点（一个“神经元”）的工作联系起来，然后将该节点与生物神经元联系起来。
- en: '![](../Images/CH03_F13_Ryan2.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F13_Ryan2.png)'
- en: Figure 3.13 Relating a neural network to a biological neuron
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 将神经网络与生物神经元联系起来
- en: Appealing to the biological analogy raises two problems. First, the analogy
    itself is controversial. Some experts in the industry think that neural networks
    work nothing like biological neurons (e.g., [https://mng.bz/jp2P](https://mng.bz/jp2P)).
    Even if you accept that the analogy between neural networks and biological neurons
    is valid, using this analogy to explain a deep learning system can contribute
    to serious misunderstanding if nonspecialists infer from the analogy that simple
    deep learning systems have brain-like capabilities. Second, the analogy doesn’t
    really clarify how a deep learning model is trained. Most people know what a biological
    neuron is, but they don’t know how biological neurons actually work. An analogy
    is not helpful if the thing that is the source of your analogy is a bit of a mystery
    itself. In sum, the analogy between nodes of a neural network and biological neurons
    is, at best, an isolated curiosity that doesn’t help a nonspecialist understand
    how deep learning actually works.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 引用生物学类比提出了两个问题。首先，这个类比本身是有争议的。一些行业专家认为神经网络的工作方式与生物神经元完全不同（例如，[https://mng.bz/jp2P](https://mng.bz/jp2P)）。即使你接受神经网络与生物神经元之间的类比是有效的，使用这个类比来解释深度学习系统也可能导致严重的误解，如果非专业人士从类比中推断出简单的深度学习系统具有类似大脑的能力。其次，这个类比并没有真正阐明深度学习模型是如何训练的。大多数人知道什么是生物神经元，但他们不知道生物神经元实际上是如何工作的。如果类比的基础本身就是一个谜，那么类比是没有帮助的。总的来说，神经网络节点与生物神经元之间的类比，充其量是一个孤立的奇思妙想，并不能帮助非专业人士理解深度学习实际上是如何工作的。
- en: What can we conclude from this example? While millions of people have learned
    enough about deep learning to appreciate what it can and can’t do, deep learning
    still presents some formidable obstacles when you try to explain it to a business
    audience. Unlike decision trees, the basics of which can be explained in an easily
    understood illustration, deep learning does not lend itself to being explained
    in one easy picture. Even today, with accessible deep learning frameworks like
    fastai and hundreds of free online resources about deep learning, people with
    a decent background in linear algebra, calculus, and programming still need several
    months of study to get a solid intuition of how deep learning works. We assert
    that it’s not possible to pass on this intuition to a nonspecialist audience in
    one simple illustration, let alone create an instantly accessible explanation
    of how deep learning will work with a specific dataset.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中我们可以得出什么结论？尽管有成百上千万的人已经对深度学习有了足够的了解，能够欣赏其所能和不能做到的事情，但当试图向商业受众解释深度学习时，深度学习仍然存在一些难以克服的障碍。与可以简单用易懂的插图解释其基本原理的决策树不同，深度学习不适合用一张简单的图片来解释。即使今天，有了像fastai这样的易于访问的深度学习框架和数百个关于深度学习的免费在线资源，那些在线性代数、微积分和编程方面有相当背景的人仍然需要几个月的时间来获得对深度学习工作原理的深刻理解。我们断言，不可能通过一个简单的插图将这种直觉传递给非专业人士，更不用说创建一个能够立即解释特定数据集如何工作的深度学习工作原理的即时可访问的解释。
- en: 3.2.2 Feature importance
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 特征重要性
- en: 'In the previous section we compared machine learning approaches and deep learning
    according to one aspect of transparency: how easy it is to explain the approach
    to a nonspecialist audience. In this section we’ll look at the other aspect of
    transparency: how easy it is to determine the importance of a given feature to
    the performance of the model as a whole.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们根据透明度的一个方面比较了机器学习和深度学习方法：向非专业人士解释方法有多容易。在本节中，我们将探讨透明度的另一个方面：确定给定特征对整个模型性能的重要性有多容易。
- en: 'Returning to the Airbnb NYC example, we can see that the XGBoost solution uses
    XGBoost’s built-in API for determining feature importance:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Airbnb纽约市的例子，我们可以看到XGBoost解决方案使用了XGBoost内置的API来确定特征重要性：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: XGBoost offers several different options for calculating feature importance.
    The default is gain, which is the average gain across all splits the feature is
    used in, where gain means the degree to which a feature separates the input examples
    (in our case, Airbnb listings) according to the target (in our case, whether the
    price of the listing is above or below the median price).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost提供了几种不同的选项来计算特征重要性。默认的是增益，它是在特征被使用的所有分割中的平均增益，其中增益是指特征根据目标（在我们的案例中，是列表价格是否高于或低于中位数）将输入示例（在我们的案例中，是Airbnb列表）区分的程度。
- en: 'In the Airbnb NYC example, the output for this API shows the gain value for
    each of the features. The third feature used to train the model (`room_type`)
    has the biggest effect, followed by the first feature (`neighbourhood_group`):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airbnb纽约的例子中，该API的输出显示了每个特征的增益值。用于训练模型的第三个特征（`room_type`）影响最大，其次是第一个特征（`neighbourhood_group`）：
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can show the feature importance values in chart form using the following
    statement:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下语句以图表形式展示特征重要性值：
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output of this statement is a chart that shows the relative importance of
    each of the features, as shown in figure 3.14.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句的输出是一个图表，显示了每个特征的相对重要性，如图3.14所示。
- en: '![](../Images/CH03_F14_Ryan2.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F14_Ryan2.png)'
- en: Figure 3.14 Feature importance for the Airbnb NYC problem according to XGBoost
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 根据XGBoost的Airbnb纽约问题特征重要性
- en: This chart makes it clear that, according to XGBoost built-in feature importance,
    `room_type` is the most important feature, with `neighbourhood_group` a distant
    second and all the other features being relatively unimportant to the behavior
    of the model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表清楚地表明，根据XGBoost内置的特征重要性，`room_type`是最重要的特征，其次是`neighbourhood_group`，其他所有特征对模型行为的重要性相对较小。
- en: The values for the `room_type` feature are
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`room_type`特征值的取值范围是'
- en: Entire home/apartment
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个住宅/公寓
- en: Private room
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 私人房间
- en: Shared room
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享房间
- en: It intuitively makes sense that `room_type` would have a significant effect
    on the price of a listing. We would expect a large difference in price between
    a listing for an entire home and a listing for a shared room.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，`room_type`对列表价格的影响应该是显著的。我们预计整个住宅列表和共享房间列表之间的价格差异会很大。
- en: Now that we have looked at how we can get feature importance for the XGBoost
    model, let’s consider how we can get feature importance for the Keras solution
    for the Airbnb NYC problem. Unlike XGBoost, Keras (and deep learning frameworks
    in general) does not have a built-in way to determine feature importance. However,
    you can apply external methods to get feature importance for Keras similar to
    the built-in feature importance in XGBoost. For example, you can use a utility
    like lime ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime))
    or shap ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    to get feature importance for a Keras deep learning model. Examining these approaches
    to feature importance analysis is beyond the scope of this chapter. For now, we’ll
    note that with XGBoost you can get a basic idea of feature importance with a couple
    of lines of code while such a simple approach is not available for deep learning
    frameworks like Keras.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何获取XGBoost模型的特征重要性，让我们考虑如何获取Airbnb纽约问题的Keras解决方案的特征重要性。与XGBoost不同，Keras（以及深度学习框架）没有内置的确定特征重要性的方法。然而，你可以应用外部方法来获取与XGBoost内置特征重要性类似的特征重要性。例如，你可以使用像lime
    ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)) 或 shap
    ([https://github.com/slundberg/shap](https://github.com/slundberg/shap)) 这样的工具来获取Keras深度学习模型的特征重要性。探讨这些特征重要性分析方法超出了本章的范围。目前，我们只需注意，使用XGBoost，你可以通过几行代码就获得特征重要性的基本概念，而对于像Keras这样的深度学习框架，这种简单的方法是不可用的。
- en: 3.2.3 Conclusions
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 结论
- en: 'In this section we have compared machine learning and deep learning according
    to the following two aspects of transparency:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们根据以下两个方面的透明度比较了机器学习和深度学习：
- en: '*Explainability*—How easy is it to explain how the model works, particularly
    to business stakeholders or other people who are not data science specialists?
    Business stakeholders will have greater faith in a model that they can grasp in
    some intuitive way compared to a model that seems like a black box. More critically,
    for regulated industries like auto insurance, transparency isn’t just a question
    of reassuring business stakeholders with accessible abstractions of how the model
    works. Regulators in such industries expect to get detailed and comprehensible
    explanations of how models work and how the model’s behavior changes as new versions
    of the model are deployed externally.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性*——解释模型工作原理有多容易，尤其是向业务利益相关者或其他非数据科学专家解释？与看似黑盒的模型相比，业务利益相关者对可以通过某种直观方式理解的模型会更有信心。更重要的是，对于像汽车保险这样的监管行业，透明度不仅仅是用模型工作原理的易于理解的抽象来安慰业务利益相关者的问题。这些行业的监管者期望获得关于模型如何工作以及模型行为如何随着模型新版本的部署而变化的详细且易于理解的解释。'
- en: '*Feature importance*—How easy is it to determine which features have the biggest
    effect on the behavior of the model?'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征重要性*—确定哪些特征对模型行为影响最大的难易程度如何？'
- en: We have seen that machine learning is more explainable than deep learning and
    that XGBoost provides a built-in feature importance API while Keras does not have
    a built-in facility for determining feature importance. Now that we have compared
    machine learning and deep learning in terms of transparency, in the next section
    we will compare the two approaches in terms of their efficacy.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，机器学习比深度学习更具可解释性，XGBoost 提供了内置的特征重要性 API，而 Keras 没有内置的确定特征重要性的功能。现在，我们已经从透明度的角度比较了机器学习和深度学习，在下一节中，我们将从效果的角度比较这两种方法。
- en: 3.3 Efficacy
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 效率
- en: 'We have compared machine learning with deep learning in terms of simplicity
    and transparency. Now let’s look at how the two approaches compare to each other
    in terms of efficacy. We’ll look at two aspects of efficacy:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从简单性和透明度的角度比较了机器学习和深度学习。现在，让我们看看这两种方法在效率方面的比较。我们将查看效率的两个方面：
- en: '*Performance*—We will return to the Airbnb NYC example to compare the relative
    performance of the XGBoost version of the application with the Keras version of
    the application. In the Airbnb NYC problem, we train a model to predict whether
    a new listing will have a price above or below the average price. We will compare
    the accuracy of the predictions produced by each approach and the time required
    to run the code for each approach.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能*—我们将回到 Airbnb NYC 的例子，比较应用程序的 XGBoost 版本与 Keras 版本的相对性能。在 Airbnb NYC 问题中，我们训练一个模型来预测新的列表价格是否高于或低于平均价格。我们将比较每种方法产生的预测准确率以及运行每种方法代码所需的时间。'
- en: '*Research*—We will compare the amount of research that argues for and against
    the idea of applying deep learning to tabular data.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*研究*—我们将比较支持与反对将深度学习应用于表格数据这一想法的研究数量。'
- en: 3.3.1 Evaluating performance
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 评估性能
- en: First, we’ll look at the performance of the XGBoost and Keras versions of the
    Airbnb NYC application. We’ll compare the results we get “out of the box” for
    each approach.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看 Airbnb NYC 应用程序的 XGBoost 和 Keras 版本的性能。我们将比较每种方法“即开即用”的结果。
- en: 'Once we have trained the XGBoost model, we can get the accuracy of the trained
    model on the test dataset with the following statements:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了 XGBoost 模型，我们可以使用以下语句获取训练模型在测试数据集上的准确率：
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For a given run of the training notebook, we get the following result:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练笔记本的一次运行，我们得到以下结果：
- en: '[PRE18]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Running the code repeatedly, we get accuracy between 79% and 81% with the original
    parameter settings in the training configuration file ([https://mng.bz/pKOz](https://mng.bz/pKOz)).
    Elapsed time to run the notebook is between 3 and 4 seconds in a vanilla Colab
    environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练配置文件（[https://mng.bz/pKOz](https://mng.bz/pKOz)）中的原始参数设置下，重复运行代码，我们得到 79%
    到 81% 的准确率。在标准的 Colab 环境中运行笔记本的时间在 3 到 4 秒之间。
- en: 'For the Keras model, with the original parameter settings from the training
    configuration file ([https://mng.bz/vKpr](https://mng.bz/vKpr)) running in a vanilla
    Colab environment, the model has the following key performance characteristics:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Keras 模型，在标准的 Colab 环境中，使用训练配置文件（[https://mng.bz/vKpr](https://mng.bz/vKpr)）中的原始参数设置运行，模型具有以下关键性能特征：
- en: '*Accuracy*—Test accuracy between 80*%* and 81%'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确率*—测试准确率在 80% 到 81% 之间'
- en: '*Elapsed time to run the notebook*—Between 10 and 15 seconds. This is without
    using a GPU on Colab. As an exercise, you can try to run the Keras training notebook
    with and without a GPU on Colab and compare the time it takes to run the notebook.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*运行笔记本的耗时*—在 10 到 15 秒之间。这是在没有使用 Colab 上的 GPU 的情况下。作为一个练习，你可以尝试在 Colab 上使用和没有
    GPU 运行 Keras 训练笔记本，并比较运行笔记本所需的时间。'
- en: If we compare the performance of XGBoost and Keras for the Airbnb NYC problem,
    XGBoost comes out ahead in terms of speed of training. This one example doesn’t
    tell the whole story, and we shall see in later chapters that, with some patience
    and tuning, a deep learning solution can rival or, in some cases, exceed the performance
    of XGBoost. The point of this simple performance comparison is to demonstrate
    that XGBoost provides good performance “out of the box” without having to do a
    lot of tweaking and tuning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将XGBoost和Keras在Airbnb NYC问题上的性能进行比较，XGBoost在训练速度方面表现更优。这个例子并不能完全说明问题，我们将在后续章节中看到，通过一些耐心和调整，深度学习解决方案可以与XGBoost相媲美，甚至在某些情况下超越其性能。这个简单的性能比较的目的是为了说明XGBoost无需大量调整和微调就能提供良好的性能。
- en: 3.4 Digging Deeper
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 深入挖掘
- en: It’s beyond the scope of this book to provide a detailed survey of all the recent
    research advocating for and against deep learning with tabular data, but in this
    section we will take a closer look at the research and try to get some idea of
    which side of the argument is “winning” the research game.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本书范围之外，无法提供所有近期关于表格数据深度学习支持与反对的详细研究综述，但在这个部分，我们将更深入地研究这些研究，并试图了解哪一方在研究竞赛中“获胜”。
- en: The article “A Short Chronology of Deep Learning for Tabular Data” ([https://mng.bz/W2x1](https://mng.bz/W2x1))
    is a great summary of recent academic work, and it’s a good place to start a deeper
    look into research about deep learning and tabular data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 文章“表格数据深度学习简史”([https://mng.bz/W2x1](https://mng.bz/W2x1))是近期学术工作的优秀总结，也是深入了解深度学习和表格数据研究的良好起点。
- en: Following is a list of some current research that supports the use of deep learning
    with tabular data. Compared to the thousands of research papers published on deep
    learning with nontabular data, including text and images, only a tiny number of
    papers have been published on deep learning with tabular data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些当前支持使用深度学习进行表格数据研究的文献。与发表在非表格数据（包括文本和图像）上的数千篇深度学习研究论文相比，关于深度学习在表格数据上应用的研究论文数量非常少。
- en: In addition to making an argument for deep learning with tabular data, the following
    papers introduce libraries for deep learning with tabular data. These libraries
    offer convenient ways to apply deep learning to tabular datasets. We will be using
    some of these libraries in chapter 8 when we go through additional examples of
    applying deep learning to tabular data and examine alternatives to the Keras-based
    approach that we used in this chapter for the Airbnb NYC problem.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为表格数据的深度学习提出论点外，以下论文还介绍了用于表格数据深度学习的库。这些库提供了将深度学习应用于表格数据集的便捷方式。在第八章中，我们将通过更多示例应用深度学习于表格数据，并探讨本章用于Airbnb
    NYC问题的基于Keras方法的替代方案。
- en: 'SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive
    Pre-Training ([https://arxiv.org/abs/2106.01342](https://arxiv.org/abs/2106.01342)).
    We will examine this framework in more detail in a later chapter.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAINT：通过行注意力和对比预训练改进的表格数据神经网络 ([https://arxiv.org/abs/2106.01342](https://arxiv.org/abs/2106.01342))。我们将在后续章节中更详细地探讨这个框架。
- en: 'TabNet: Attentive Interpretable Tabular Learning ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442)).
    This paper introduces another framework for deep learning with tabular data that
    we will explore in more detail in a later chapter.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TabNet：通过行注意力和对比预训练改进的表格数据神经网络 ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442))。我们将在后续章节中更详细地探讨这个框架。
- en: 'PyTorch Tabular: A Framework for Deep Learning with Tabular Data ([https://arxiv.org/abs/2104.13638](https://arxiv.org/abs/2104.13638)).
    This paper introduces a library based on PyTorch, and it is another library that
    we will revisit in a later chapter.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Tabular：一个用于表格数据的深度学习框架 ([https://arxiv.org/abs/2104.13638](https://arxiv.org/abs/2104.13638))。本文介绍了一个基于PyTorch的库，这是我们将在后续章节中再次回顾的另一个库。
- en: 'fastai: A Layered API for Deep Learning ([https://arxiv.org/abs/2002.04688](https://arxiv.org/abs/2002.04688)).
    This paper introduces fastai, a high-level framework built on top of PyTorch.
    This framework includes explicit support for tabular data.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastai：一个基于PyTorch的深度学习分层API ([https://arxiv.org/abs/2002.04688](https://arxiv.org/abs/2002.04688))。本文介绍了fastai，这是一个构建在PyTorch之上的高级框架。该框架包括对表格数据的显式支持。
- en: Deep Tables ([https://deeptables.readthedocs.io/en/latest/](https://deeptables.readthedocs.io/en/latest/)).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deep Tables ([https://deeptables.readthedocs.io/en/latest/](https://deeptables.readthedocs.io/en/latest/))。
- en: 'DANets: Deep Abstract Networks for Tabular Data Classification and Regression
    ([https://arxiv.org/abs/2112.02962](https://arxiv.org/abs/2112.02962)).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DANets：用于表格数据分类和回归的深度抽象网络 ([https://arxiv.org/abs/2112.02962](https://arxiv.org/abs/2112.02962))。
- en: Each of these papers features code that we can exercise to validate the research
    results and, more importantly, determine the robustness of the libraries. If we
    want to use any of these libraries to solve real-world tabular data problems with
    deep learning, we need to assess whether the libraries are easy to use and work
    with current deep learning frameworks. Figure 3.15 shows the relative popularity
    of some of these libraries based on the number of citations their papers received
    along with the number of stars received by their repos.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文中的每一篇都包含我们可以用来验证研究结果的代码，更重要的是，可以确定库的鲁棒性。如果我们想使用这些库中的任何一个来解决使用深度学习的现实世界表格数据问题，我们需要评估这些库是否易于使用，并且是否与当前的深度学习框架兼容。图3.15显示了这些库中一些库的相对流行度，基于它们的论文获得的引用数量以及它们的仓库获得的星标数量。
- en: '![](../Images/CH03_F15_Ryan2b.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F15_Ryan2b.png)'
- en: Figure 3.15 Popularity of libraries for deep learning with tabular data
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 表格数据深度学习库的流行度
- en: The popularity of libraries matters. If a library is widely used, it is more
    likely to work in a variety of environments. We cannot take it for granted that
    a library will work in every environment. As we shall see in chapter 8, some libraries
    don’t work in Colab, for example, which means it is difficult to assess them.
    Also, if you use a library that hundreds or thousands of other machine learning
    practitioners are using, you are more likely to find answers to questions and
    resolutions to problems. If you are one of a handful of people using a library,
    you could end up being the first person who has encountered a given problem, and
    you will need to spend your time resolving it rather than simply finding an existing
    resolution on Stack Overflow.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 库的流行度很重要。如果一个库被广泛使用，它更有可能在各种环境中工作。我们不能想当然地认为一个库会在每个环境中工作。正如我们在第8章中将要看到的，一些库在Colab中无法工作，例如，这意味着评估它们很困难。此外，如果你使用的是数百或数千名其他机器学习实践者都在使用的库，你更有可能找到问题的答案和问题的解决方案。如果你是少数几个使用该库的人之一，你可能会成为第一个遇到给定问题的人，你需要花时间解决问题，而不是简单地找到Stack
    Overflow上的现有解决方案。
- en: 'The critics of deep learning with tabular data have contributed research to
    make their case, including the following papers:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对表格数据深度学习的批评者已经为他们的论点贡献了研究，包括以下论文：
- en: Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data? ([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815))
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么基于树的模型在表格数据上仍然优于深度学习？([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815))
- en: 'Tabular Data: Deep Learning Is Not All You Need ([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253))'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据：深度学习并非你所需要的一切 ([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253))
- en: 'Deep Neural Networks and Tabular Data: A Survey ([https://arxiv.org/abs/2110.01889](https://arxiv.org/abs/2110.01889))'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络和表格数据：综述 ([https://arxiv.org/abs/2110.01889](https://arxiv.org/abs/2110.01889))
- en: This list, and the preceding list of pro deep learning papers, is by no means
    exhaustive. However, it is fair to say that more research is published that advocates
    for deep learning with tabular data than research published to argue against deep
    learning with tabular data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表，以及之前列出的支持深度学习的论文列表，绝对不是详尽的。然而，可以说，支持使用表格数据深度学习的研究比反对使用表格数据深度学习的研究更多。
- en: Of all the research papers published on deep learning, what proportion deals
    with tabular data? It’s hard to get an exact ratio, but consider figure 3.16,
    which shows the number of papers published on deep learning for the decade and
    a half preceding 2018.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有关于深度学习的研究论文中，有多少比例涉及表格数据？很难得到一个确切的比率，但考虑图3.16，它显示了2018年之前十五年内发布的深度学习论文数量。
- en: '![](../Images/CH03_F16_Ryan2.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F16_Ryan2.png)'
- en: Figure 3.16 Number of published deep learning articles by year. The numbers
    of articles were obtained from the search results on Scopus and Google Scholar
    with the query “deep learning.”
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 按年份发布的深度学习文章数量。文章数量是通过在Scopus和Google Scholar上使用查询“深度学习”得到的搜索结果。
- en: This figure shows that recently tens of thousands of papers are published every
    year on the subject of deep learning. In the last few years, less than 100 papers
    have been published each year on deep learning with tabular data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示，近年来每年都有数以万计的论文发表在深度学习主题上。在过去的几年里，每年关于深度学习和表格数据的论文发表量不到100篇。
- en: 'One way of getting an idea of what proportion of deep learning research deals
    with tabular data is to do some searches on Google Scholar ([https://scholar.google.com/](https://scholar.google.com/)).
    Consider the number of search hits on Google Scholar that match the following
    search criteria:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 了解深度学习研究中处理表格数据比例的一种方法是在Google Scholar（[https://scholar.google.com/](https://scholar.google.com/)）上做一些搜索。考虑符合以下搜索条件的Google
    Scholar搜索结果数量：
- en: '“deep learning”: ~1.6 million'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度学习”：约1.6百万篇
- en: '“deep learning” along with “tabular data” or “structured data” and excluding
    “graph-structured”: ~34,500'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度学习”与“表格数据”或“结构化数据”相关，排除“图结构”：约34,500条
- en: '“deep neural networks”: ~530,000'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度神经网络”：约530,000篇
- en: '“deep neural networks” along with “tabular data” or “structured data” and excluding
    “graph-structured” and “deep learning”: ~1,500'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度神经网络”与“表格数据”或“结构化数据”相关，排除“图结构”和“深度学习”：约1,500篇
- en: 'Another way of determining what proportion of deep learning research deals
    with tabular data is to do some searches on arXiv ([https://arxiv.org/](https://arxiv.org/)).
    Consider the number of papers on arXiv that match the following search criteria:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 确定深度学习研究中处理表格数据比例的另一种方法是，在arXiv（[https://arxiv.org/](https://arxiv.org/)）上做一些搜索。考虑符合以下搜索条件的arXiv论文数量：
- en: '“deep learning” in the title: ~32,000'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题中包含“深度学习”：约32,000条
- en: '“deep learning” along with “tabular data” or “structured data”: ~200'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度学习”与“表格数据”或“结构化数据”相关：约200条
- en: '“deep neural networks” in the title: ~17,500'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题中包含“深度神经网络”：约17,500条
- en: '“deep neural networks” along with “tabular data” or “structured data”: ~11'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度神经网络”与“表格数据”或“结构化数据”相关：约11条
- en: From the searches on Google Scholar and arXiv, it’s clear that only a tiny proportion
    of the research published in deep learning deals with tabular data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从Google Scholar和arXiv的搜索结果来看，很清楚，在深度学习研究中，只有极小比例的研究涉及表格数据。
- en: 'To summarize, we can conclude the following points about research on deep learning
    with tabular data:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以得出以下关于深度学习与表格数据研究的结论：
- en: There are more publications that support deep learning with tabular data than
    there are publications that argue against deep learning with tabular data.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持深度学习与表格数据的出版物比反对的出版物要多。
- en: Several research papers on deep learning with tabular data include libraries
    that implement the approach described in the paper. So far, none of these libraries
    has emerged as a clear favorite for data scientists who are interested in deep
    learning with tabular data.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几篇关于深度学习与表格数据的研究论文包括了实现论文中描述方法的库。到目前为止，这些库还没有成为对深度学习与表格数据感兴趣的数据科学家们的明确首选。
- en: Of all the research done on deep learning, research dealing with deep learning
    and tabular data makes up only a tiny fraction.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有关于深度学习的研究中，处理深度学习和表格数据的研究仅占极小部分。
- en: With these conclusions, it’s clear that neither machine learning nor deep learning
    is the unambiguous winner in the area of research. In the argument for and against
    deep learning with tabular data, when it comes to research, the jury is still
    out.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结论，可以清楚地看出，在研究领域，机器学习和深度学习都不是明确的赢家。在关于深度学习与表格数据的辩论中，当涉及到研究时，陪审团仍然没有定论。
- en: 'We’ve compared machine learning with deep learning across three criteria: simplicity,
    transparency, and efficacy. Figure 3.17 figure summarizes how the two approaches
    compare with each other in each of these three criteria.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从三个标准：简单性、透明度和有效性，比较了机器学习和深度学习。图3.17总结了这两种方法在这三个标准上的比较情况。
- en: '![](../Images/CH03_F17_Ryan2.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F17_Ryan2.png)'
- en: Figure 3.17 Summary of the comparison of machine learning and deep learning
    with tabular data
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17机器学习和深度学习与表格数据比较的总结
- en: That wraps up our comparison of machine learning and deep learning. In the next
    chapter we will go beyond the simple Airbnb NYC example using XGBoost and dig
    into the details of machine learning with tabular data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对机器学习和深度学习的比较。在下一章中，我们将超越简单的Airbnb NYC示例，使用XGBoost深入探讨机器学习与表格数据的细节。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There are three characteristics that we can use to compare machine learning
    with deep learning: simplicity, transparency, and efficacy.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以用三个特征来比较机器学习和深度学习：简单性、透明性和有效性。
- en: The comparative simplicity of the code gives us an idea of which solution will
    be easier to build in the first place and easier to maintain in the long term.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码的相对简单性让我们可以预见到哪个解决方案最初更容易构建，长期来看也更容易维护。
- en: The transparency of the solution includes how easy it is to explain the model
    to a nonspecialist audience and how easy it is to assess the relative importance
    of the features used to train the model.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案的可透明性包括向非专业人士解释模型有多容易，以及评估用于训练模型的特征的相对重要性有多容易。
- en: Efficacy includes the success of each approach in commercial applications and
    in research.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效性包括每种方法在商业应用和研究中的成功。
- en: When comparing machine learning and deep learning in terms of code simplicity,
    machine learning comes out ahead.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从代码简单性方面比较机器学习和深度学习时，机器学习表现得更为出色。
- en: When comparing machine learning and deep learning in terms of transparency,
    machine learning comes out ahead.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从透明性方面比较机器学习和深度学习时，机器学习再次表现得更为出色。
- en: When comparing machine learning and deep learning in terms of efficacy, the
    two approaches are too close to call “out of the box,” though additional tuning
    could expose more differences between the results of the two approaches.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从有效性方面比较机器学习和深度学习时，两种方法非常接近，难以一概而论“出类拔萃”，尽管额外的调整可能会揭示两种方法结果之间的更多差异。
- en: The jury is still out for other measures of success, including success in Kaggle
    competitions, success in business, and research focus.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于其他成功指标，包括在Kaggle竞赛中的成功、商业上的成功以及研究重点，目前还没有定论。
