- en: 10 Agent reasoning and evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 代理推理和评估
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using various prompt engineering techniques to extend large language model functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种提示工程技术来扩展大型语言模型功能
- en: Engaging large language models with prompt engineering techniques that engage
    reasoning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用涉及推理的提示工程技术来参与大型语言模型
- en: Employing an evaluation prompt to narrow and identify the solution to an unknown
    problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用评估提示来缩小和识别未知问题的解决方案
- en: 'Now that we’ve examined the patterns of memory and retrieval that define the
    semantic memory component in agents, we can take a look at the last and most instrumental
    component in agents: planning. Planning encompasses many facets, from reasoning,
    understanding, and evaluation to feedback.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了定义代理中语义记忆组件的记忆和检索模式，我们可以看看代理中最后一个也是最关键的组件：规划。规划包括许多方面，从推理、理解、评估到反馈。
- en: To explore how LLMs can be prompted to reason, understand, and plan, we’ll demonstrate
    how to engage reasoning through prompt engineering and then expand that to planning.
    The planning solution provided by the Semantic Kernel (SK) encompasses multiple
    planning forms. We’ll finish the chapter by incorporating adaptive feedback into
    a new planner.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索如何通过提示工程来引导LLMs进行推理、理解和规划，我们将演示如何通过提示工程来参与推理，然后扩展到规划。语义内核（SK）提供的规划解决方案包含多种规划形式。我们将通过将自适应反馈纳入一个新的规划器来结束本章。
- en: Figure 10.1 demonstrates the high-level prompt engineering strategies we’ll
    cover in this chapter and how they relate to the various techniques we’ll cover.
    Each of the methods showcased in the figure will be explored in this chapter,
    from the basics of solution/direct prompting, shown in the top-left corner, to
    self-consistency and tree of thought (ToT) prompting, in the bottom right.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1展示了本章将涵盖的高级提示工程策略以及它们与我们将要介绍的各种技术之间的关系。图中的每种方法都将在本章中探讨，从左上角显示的解决方案/直接提示的基础，到右下角的自我一致性和思维树（ToT）提示。
- en: '![figure](../Images/10-1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-1.png)'
- en: Figure 10.1 How the two planning prompt engineering strategies align with the
    various techniques
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1 两种规划提示工程策略如何与各种技术相匹配
- en: 10.1 Understanding direct solution prompting
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 理解直接解决方案提示
- en: '*Direct solution prompting* is generally the first form of prompt engineering
    that users employ when asking LLMs questions or solving a particular problem.
    Given any LLM use, these techniques may seem apparent, but they are worth reviewing
    to establish the foundation of thought and planning. In the next section, we’ll
    start from the beginning, asking questions and expecting answers.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*直接解决方案提示*通常是用户在向LLMs提问或解决特定问题时采用的第一种提示工程形式。对于任何LLM的使用，这些技术可能看起来很明显，但它们值得回顾，以建立思考和规划的基础。在下一节中，我们将从提问和期待答案开始。'
- en: 10.1.1 Question-and-answer prompting
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 问答提示
- en: For the exercises in this chapter, we’ll employ prompt flow to build and evaluate
    the various techniques. (We already extensively covered this tool in chapter 9,
    so refer to that chapter if you need a review.) Prompt flow is an excellent tool
    for understanding how these techniques work and exploring the flow of the planning
    and reasoning process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的练习，我们将使用提示流来构建和评估各种技术。（我们已经在第9章中广泛介绍了这个工具，所以如果需要复习，请参考该章节。）提示流是一个理解这些技术如何工作以及探索规划和推理过程流程的绝佳工具。
- en: Open Visual Studio Code (VS Code) to the `chapter` `10` source folder. Create
    a new virtual environment for the folder, and install the `requirements.txt` file.
    If you need help setting up a chapter’s Python environment, refer to appendix
    B.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Visual Studio Code (VS Code)到`chapter` `10`源文件夹。为文件夹创建一个新的虚拟环境，并安装`requirements.txt`文件。如果您需要帮助设置章节的Python环境，请参阅附录B。
- en: We’ll look at the first flow in the `prompt_flow/question-answering-prompting`
    folder. Open the `flow.dag.yaml` file in the visual editor, as shown in figure
    10.2\. On the right side, you’ll see the flow of components. At the top is the
    `question_answer` LLM prompt, followed by two `Embedding` components and a final
    LLM prompt to do the evaluation called `evaluate`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看`prompt_flow/question-answering-prompting`文件夹中的第一个流程。在可视化编辑器中打开`flow.dag.yaml`文件，如图10.2所示。在右侧，您将看到组件的流程。顶部是`question_answer`
    LLM提示，后面跟着两个`Embedding`组件，最后是一个用于评估的最终LLM提示，称为`evaluate`。
- en: '![figure](../Images/10-2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-2.png)'
- en: Figure 10.2 The `flow.dag.yaml` file, open in the visual editor, highlighting
    the various components of the flow
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2 `flow.dag.yaml`文件，在可视化编辑器中打开，突出显示流程的各个组件
- en: The breakdown in listing 10.1 shows the structure and components of the flow
    in more detail using a sort of YAML-shortened pseudocode. You can also see the
    input and outputs to the various components and a sample output from running the
    flow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1中的分解详细展示了流程的结构和组件，使用了一种类似YAML简化的伪代码。您还可以看到各个组件的输入和输出，以及运行流程的示例输出。
- en: Listing 10.1 `question-answer-prompting` flow
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1 `question-answer-prompting`流程
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before running this flow, make sure your LLM block is configured correctly.
    This may require you to set up a connection to your chosen LLM. Again, refer to
    chapter 9 if you need a review on how to complete this. You’ll need to configure
    the LLM and `Embedding` blocks with your connection if you’re not using OpenAI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此流程之前，请确保您的LLM块配置正确。这可能需要您设置与所选LLM的连接。如果您需要复习如何完成此操作，请再次参考第9章。如果您不使用OpenAI，您需要配置LLM和`Embedding`块与您的连接。
- en: After configuring your LLM connection, run the flow by clicking the Play button
    from the visual editor or using the Test (Shift-F5) link in the YAML editor window.
    If everything is connected and configured correctly, you should see output like
    that in listing 10.1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置您的LLM连接后，通过从可视化编辑器中点击播放按钮或在YAML编辑器窗口中使用测试（Shift-F5）链接来运行流程。如果一切连接和配置正确，您应该会看到列表10.1中的输出。
- en: Open the `question_answer.jinja2` file in VS Code, as shown in listing 10.2\.
    This listing shows the basic question-and-answer-style prompt. In this style of
    prompt, the system message describes the basic rules and provides the context
    to answer the question. In chapter 4, we explored the retrieval augmented generation
    (RAG) pattern, and this prompt follows a similar pattern.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`question_answer.jinja2`文件，如图10.2所示。这个列表显示了基本的问答式提示。在这种提示风格中，系统消息描述了基本规则，并提供了解答问题的上下文。在第4章中，我们探讨了检索增强生成（RAG）模式，而这个提示遵循了类似的模式。
- en: Listing 10.2 `question_answer.jinja2`
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2 `question_answer.jinja2`
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Replace with the content LLM should answer the question about.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用LLM应该回答问题的内容替换。'
- en: '#2 Replace with the question.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 用问题替换。'
- en: This exercise shows the simple method of using an LLM to ask questions about
    a piece of content. Then, the question response is evaluated using a similarity
    matching score. We can see from the output in listing 10.1 that the LLM does a
    good job of answering a question about the context. In the next section, we’ll
    explore a similar technique that uses direct prompting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习展示了使用大型语言模型（LLM）对某段内容提问的简单方法。然后，使用相似度匹配分数来评估问题回答。我们可以从列表10.1的输出中看到，LLM在回答关于上下文的问题方面做得很好。在下一节中，我们将探讨一种类似的直接提示技术。
- en: 10.1.2 Implementing few-shot prompting
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 实现少样本提示
- en: '*Few-shot prompting* is like question-and-answer prompting, but the makeup
    of the prompt is more about providing a few examples than about facts or context.
    This allows the LLM to bend to patterns or content not previously seen. While
    this approach sounds like question and answer, the implementation is quite different,
    and the results can be powerful.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*少样本提示*类似于问答提示，但提示的构成更多地是提供几个示例，而不是事实或上下文。这允许LLM适应之前未见过的模式或内容。虽然这种方法听起来像是问答，但实现方式相当不同，结果可能非常强大。'
- en: Zero-shot, one-shot, and few-shot learning
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 零样本、单样本和少样本学习
- en: One holy grail of machine learning and AI is the ability to train a model on
    as few items as possible. For example, in traditional vision models, millions
    of images are fed into the model to help identify the differences between a cat
    and a dog.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和人工智能的一个圣杯是能够在尽可能少的样本上训练模型。例如，在传统的视觉模型中，数百万张图片被输入到模型中，以帮助识别猫和狗之间的差异。
- en: A *one-shot* model is a model that requires only a single image to train it.
    For example, a picture of a cat can be shown, and then the model can identify
    any cat image. A *few-shot* model requires only a few things to train the model.
    And, of course, *zero-shot* indicates the ability to identify something given
    no previous examples. LLMs are efficient learners and can do all three types of
    learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*单样本*模型是一个只需要单个图像来训练的模型。例如，可以展示一张猫的图片，然后模型可以识别任何猫的图片。一个*少样本*模型只需要少量东西来训练模型。当然，*零样本*表示在没有先前示例的情况下识别某物的能力。LLM是高效的学习者，可以完成这三种类型的学习。
- en: Open `prompt_flow/few-shot-prompting/flow.dag.yaml` in VS Code and the visual
    editor. Most of the flow looks like the one pictured earlier in figure 10.2, and
    the differences are highlighted in listing 10.3, which shows a YAML pseudocode
    representation. The main differences between this and the previous flow are the
    inputs and LLM prompt.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 和可视化编辑器中打开 `prompt_flow/few-shot-prompting/flow.dag.yaml`。大部分流程看起来像图
    10.2 中早些时候展示的那样，差异在列表 10.3 中突出显示，它展示了 YAML 伪代码表示。这个流程与之前的流程之间的主要区别是输入和 LLM 提示。
- en: Listing 10.3 `few-shot-prompting` flow
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.3 `few-shot-prompting` 流程
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Evaluation score represents the similarity between expected and predicted.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 评估分数表示预期和预测之间的相似度。'
- en: '#2 Uses sunner in a sentence'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在句子中使用 sunner'
- en: '#3 This is a false statement but the intent is to get the LLM to use the word
    as if it was real.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这是一个错误的陈述，但目的是让 LLM 使用这个单词，就像它是真实的一样。'
- en: Run the flow by pressing Shift-F5 or clicking the Play/Test button from the
    visual editor. You should see output like listing 10.3 where the LLM has used
    the word *sunner* (a made-up term) correctly in a sentence given the initial statement.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按 Shift-F5 或从可视化编辑器中点击播放/测试按钮来运行流程。你应该会看到类似于列表 10.3 的输出，其中 LLM 正确地在一个句子中使用了单词
    *sunner*（一个虚构的术语），这是基于初始语句的。
- en: This exercise demonstrates the ability to use a prompt to alter the behavior
    of the LLM to be contrary to what it has learned. We’re changing what the LLM
    understands to be accurate. Furthermore, we then use that modified perspective
    to elicit the use of a made-up word.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习展示了使用提示来改变 LLM 行为的能力，使其与它所学习的内容相反。我们正在改变 LLM 理解为准确的内容。此外，我们随后使用这种修改后的观点来引发对虚构词汇的使用。
- en: Open the `few_shot.jinja2` prompt in VS Code, shown in listing 10.4\. This listing
    demonstrates setting up a simple persona, that of an eccentric dictionary maker,
    and then providing examples of words it has defined and used before. The base
    of the prompt allows for the LLM to extend the examples and produce similar results
    using other words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 中打开 `few_shot.jinja2` 提示，如列表 10.4 所示。这个列表展示了设置一个简单的角色，即古怪的词典编纂者，然后提供它之前定义和使用的词汇的例子。提示的基础允许
    LLM 扩展例子并使用其他词汇产生类似的结果。
- en: Listing 10.4 `few_shot.jinja2`
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4 `few_shot.jinja2`
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Demonstrates an example defining a made-up word and using it in a sentence'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 展示了一个定义虚构词汇并在句子中使用它的例子'
- en: '#2 Demonstrates another example'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 展示了另一个例子'
- en: '#3 A rule to prevent the LLM from outputting extra information'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一条规则，用于防止 LLM 输出额外信息'
- en: '#4 The input statement defines a new word and asks for the use.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 输入语句定义了一个新词并要求使用。'
- en: You may say we’re forcing the LLM to hallucinate here, but this technique is
    the basis for modifying behavior. It allows prompts to be constructed to guide
    an LLM to do everything contrary to what it learned. This foundation of prompting
    also establishes techniques for other forms of altered behavior. From the ability
    to alter the perception and background of an LLM, we’ll move on to demonstrate
    a final example of a direct solution in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说我们在这里迫使 LLM 幻觉，但这项技术是修改行为的基础。它允许构建提示来引导 LLM 做出与它所学习的一切相反的事情。提示的基础还确立了其他形式改变行为的技术。从改变
    LLM 的感知和背景的能力，我们将继续在下一节展示一个直接解决方案的最终例子。
- en: 10.1.3 Extracting generalities with zero-shot prompting
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 使用零样本提示提取概括
- en: '*Zero-shot prompting or learning* is the ability to generate a prompt in such
    a manner that allows the LLM to generalize. This generalization is embedded within
    the LLM and demonstrated through zero-shot prompting, where no examples are given,
    but instead a set of guidelines or rules are given to guide the LLM.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*零样本提示或学习* 是以这种方式生成提示的能力，允许 LLM 进行泛化。这种泛化嵌入在 LLM 中，并通过零样本提示来展示，其中不提供示例，而是给出一系列指南或规则来引导
    LLM。'
- en: Employing this technique is simple and works well to guide the LLM to generate
    replies given its internal knowledge and no other contexts. It’s a subtle yet
    powerful technique that applies the knowledge of the LLM to other applications.
    This technique, combined with other prompting strategies, is proving effective
    at replacing other language classification models—models that identify the emotion
    or sentiment in text, for example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术很简单，并且很好地引导 LLM 根据其内部知识和没有其他上下文来生成回复。这是一种微妙而强大的技术，它将 LLM 的知识应用于其他应用。这种技术与其他提示策略相结合，正在证明在替代其他语言分类模型——例如识别文本中的情感或情绪的模型——方面是有效的。
- en: Open `prompt_flow/zero-shot-prompting/flow.dag.yaml` in the VS Code prompt flow
    visual editor. This flow is again almost identical to that shown earlier in figure
    10.1 but differs slightly in implementation, as shown in listing 10.5.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code提示流程可视化编辑器中打开 `prompt_flow/zero-shot-prompting/flow.dag.yaml`。这个流程与之前图10.1中显示的几乎相同，但在实现上略有不同，如列表10.5所示。
- en: Listing 10.5 `zero-shot-prompting` flow
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5 `zero-shot-prompting` 流程
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Shows a perfect evaluation score of 1.0'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 显示了完美的评估分数1.0'
- en: '#2 The statement we’re asking the LLM to classify'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们要求LLM进行分类的声明'
- en: Run the flow by pressing Shift-F5 within the VS Code prompt flow visual editor.
    You should see output similar to that shown in listing 10.5.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code提示流程可视化编辑器中按Shift-F5运行流程。你应该会看到类似于列表10.5所示的输出。
- en: Now open the `zero_shot.jinja2` prompt as shown in listing 10.6\. The prompt
    is simple and uses no examples to extract the sentiment from the text. What is
    especially interesting to note is that the prompt doesn’t even mention the phrase
    sentiment, and the LLM seems to understand the intent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开如列表10.6所示的 `zero_shot.jinja2` 提示。该提示简单，不使用示例来从文本中提取情感。特别值得注意的是，提示中甚至没有提到“情感”这个词，而LLM似乎理解了意图。
- en: Listing 10.6 `zero_shot.jinja2`
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6 `zero_shot.jinja2`
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Provides essential guidance on performing the classification'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提供了执行分类的基本指导'
- en: '#2 The statement of text to classify'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 文本分类的声明'
- en: Zero-shot prompt engineering is about using the ability of the LLM to generalize
    broadly based on its training material. This exercise demonstrates how knowledge
    within the LLM can be put to work for other tasks. The LLM’s ability to self-contextualize
    and apply knowledge can extend beyond its training. In the next section, we extend
    this concept further by looking at how LLMs can reason.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本提示工程是关于利用LLM基于其训练材料广泛概括的能力。这个练习展示了LLM中的知识如何用于其他任务。LLM自我情境化和应用知识的能力可以超越其训练范围。在下一节中，我们将进一步探讨LLM如何进行推理。
- en: 10.2 Reasoning in prompt engineering
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 提示工程中的推理
- en: LLMs like ChatGPT were developed to function as chat completion models, where
    text content is fed into the model, whose responses align with completing that
    request. LLMs were never trained to reason, plan, think, or have thoughts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ChatGPT这样的LLM被开发成作为聊天完成模型，其中文本内容被输入到模型中，其响应与完成该请求相一致。LLM从未被训练过推理、规划、思考或拥有思想。
- en: However, much like we demonstrated with the examples in the previous section,
    LLMs can be prompted to extract their generalities and be extended beyond their
    initial design. While an LLM isn’t designed to reason, the training material fed
    into the model provides an understanding of reasoning, planning, and thought.
    Therefore, by extension, an LLM understands what reasoning is and can employ the
    concept of reasoning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像我们在上一节中的示例所展示的那样，LLM可以被提示提取其概括性，并扩展到其初始设计之外。虽然LLM不是为推理而设计的，但输入到模型中的训练材料提供了对推理、规划和思维的理解。因此，通过扩展，LLM理解推理是什么，并且可以运用推理的概念。
- en: Reasoning and planning
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 理解和规划
- en: '*Reasoning* is the ability of an intellect, artificial or not, to understand
    the process of thought or thinking through a problem. An intellect can understand
    that actions have outcomes, and it can use this ability to reason through which
    action from a set of actions can be applied to solve a given task.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*推理* 是智力（无论是人工的还是非人工的）理解通过问题进行思考或思维过程的能力。智力可以理解行动有结果，并且可以利用这种能力通过从一系列行动中选择哪个行动可以应用于解决给定的任务来进行推理。'
- en: '*Planning* is the ability of the intellect to reason out the order of actions
    or tasks and apply the correct parameters to achieve a goal or outcome—the extent
    to which an intellectual plan depends on the scope of the problem. An intellect
    may combine multiple levels of planning, from strategic and tactical to operational
    and contingent.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*规划* 是智力推理出行动或任务的顺序并应用正确的参数以实现目标或结果的能力——智力计划依赖于问题范围的程度。智力可以结合多个层次的规划，从战略和战术到操作和应急。'
- en: We’ll look at another set of prompt engineering techniques that allow or mimic
    reasoning behavior to demonstrate this reasoning ability. Typically, when evaluating
    the application of reasoning, we look to having the LLM solve challenging problems
    it wasn’t designed to solve. A good source of such is based on logic, math, and
    word problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨另一组提示工程技术，这些技术允许或模拟推理行为，以展示这种推理能力。通常，在评估推理的应用时，我们会寻找 LLM 解决它未设计去解决的问题。这类问题的一个很好的来源是逻辑、数学和文字问题。
- en: Using the time travel theme, what class of unique problems could be better to
    solve than understanding time travel? Figure 10.3 depicts one example of a uniquely
    challenging time travel problem. Our goal is to acquire the ability to prompt
    the LLM in a manner that allows it to solve the problem correctly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时间旅行主题，理解时间旅行比解决哪一类独特问题更好？图 10.3 展示了一个独特且具有挑战性的时间旅行问题的例子。我们的目标是获得一种能力，能够以正确解决问题的方法提示
    LLM。
- en: '![figure](../Images/10-3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-3.png)'
- en: Figure 10.3 The complexity of the time travel problems we intend to solve using
    LLMs with reasoning and planning
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.3 我们打算使用具有推理和计划的 LLM 解决的时间旅行问题的复杂性
- en: Time travel problems are thought exercises that can be deceptively difficult
    to solve. The example in figure 10.3 is complicated to solve for an LLM, but the
    part it gets wrong may surprise you. The next section will use reasoning in prompts
    to solve these unique problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 时间旅行问题是一些看似难以解决的思维练习。图 10.3 中的例子对 LLM 来说很复杂，但它出错的部分可能会让你感到惊讶。下一节将使用提示中的推理来解决这些独特的问题。
- en: 10.2.1 Chain of thought prompting
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 思维链提示
- en: '*Chain of thought* (CoT)prompting is a prompt engineering technique that employs
    the one-shot or few-shot examples that describe the reasoning and the steps to
    accomplish a desired goal. Through the demonstration of reasoning, the LLM can
    generalize this principle and reason through similar problems and goals. While
    the LLM isn’t trained with the goal of reasoning, we can elicit the model to reason,
    using prompt engineering.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*思维链*（CoT）提示是一种提示工程技术，它使用一次或几次示例来描述推理和实现预期目标的步骤。通过推理的展示，LLM 可以推广这个原则，并通过类似的问题和目标进行推理。虽然
    LLM 没有以推理为目标进行训练，但我们可以通过提示工程技术来激发模型进行推理。'
- en: Open `prompt_flow/chain-of-thought-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. The elements of this flow are simple, as shown in figure 10.4\.
    With only two LLM blocks, the flow first uses a CoT prompt to solve a complex
    question; then, the second LLM prompt evaluates the answer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 提示流程可视化编辑器中打开 `prompt_flow/chain-of-thought-prompting/flow.dag.yaml`。如图
    10.4 所示，这个流程的元素很简单，只有两个 LLM 块。流程首先使用 CoT 提示来解决一个复杂问题；然后，第二个 LLM 提示评估答案。
- en: '![figure](../Images/10-4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-4.png)'
- en: Figure 10.4 The flow of the CoT
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.4 CoT 流程
- en: Listing 10.7 shows the YAML pseudocode that describes the blocks and the inputs/outputs
    of the flow in more detail. The default problem statement in this example isn’t
    the same as in figure 10.3.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.7 展示了描述流程中的块及其输入/输出的 YAML 伪代码。在这个例子中，默认的问题陈述与图 10.3 中的不同。
- en: Listing 10.7 `chain-of-thought-prompting` flow
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.7 `chain-of-thought-prompting` 流程
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 The evaluated score for the given solution'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 给定解决方案的评估分数'
- en: '#2 The expected answer for the problem'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 问题的预期答案'
- en: '#3 The predicted answer shows the reasoning steps and output.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 预测答案显示了推理步骤和输出。'
- en: Dig into the inputs and check the problem statement; try to evaluate the problem
    yourself. Then, run the flow by pressing Shift-F5\. You should see output similar
    to that shown in listing 10.7.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究输入并检查问题陈述；尝试自己评估这个问题。然后，通过按 Shift-F5 运行流程。你应该会看到类似于列表 10.7 中所示的结果。
- en: Open the `cot.jinja2` prompt file as shown in listing 10.8\. This prompt gives
    a few examples of time travel problems and then the thought-out and reasoned solution.
    The process of showing the LLM the steps to complete the problem provides the
    reasoning mechanism.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 按照列表 10.8 中的说明打开 `cot.jinja2` 提示文件。这个提示给出了一些时间旅行问题的例子，然后是经过思考和推理的解决方案。向 LLM
    展示完成问题的步骤提供了推理机制。
- en: Listing 10.8 `cot.jinja2`
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.8 `cot.jinja2`
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 A few example problem statements'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 几个示例问题陈述'
- en: '#2 The solution to the problem statement, output as a sequence of reasoning
    steps'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将问题陈述的解决方案输出为一系列推理步骤'
- en: '#3 A few example problem statements'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 几个示例问题陈述'
- en: '#4 The solution to the problem statement, output as a sequence of reasoning
    steps'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将问题陈述的解决方案输出为一系列推理步骤'
- en: '#5 The problem statement the LLM is directed to solve'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 LLM被指示解决的问题陈述'
- en: You may note that the solution to figure 10.3 is also provided as an example
    in listing 10.8\. It’s also helpful to go back and review listing 10.7 for the
    reply from the LLM about the problem. From this, you can see the reasoning steps
    the LLM applied to get its final answer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到图10.3的解决方案也作为例子在列表10.8中提供。回顾列表10.7中LLM对问题的回复也是很有帮助的。从这一点，你可以看到LLM应用了哪些推理步骤来得到其最终答案。
- en: Now, we can look at the prompt that evaluates how well the solution solved the
    problem. Open `evaluate_answer.jinja2`, shown in listing 10.9, to review the prompt
    used. The prompt is simple, uses zero-shot prompting, and allows the LLM to generalize
    how it should score the expected and predicted. We could provide examples and
    scores, thus changing this to an example of a few-shot classification.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看评估解决方案如何解决问题的提示。打开列表10.9中所示的`evaluate_answer.jinja2`以回顾所使用的提示。这个提示很简单，使用了零样本提示，并允许LLM泛化它应该如何评分预期和预测。我们可以提供示例和分数，从而将此转变为一个几样本分类的例子。
- en: Listing 10.9 `evaluate_answer.jinja2`
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.9 `evaluate_answer.jinja2`
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 The rules for evaluating the solution'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 评估解决方案的规则'
- en: '#2 Direction to only return the score and nothing else'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指示只返回分数，不返回其他内容'
- en: '#3 The initial problem statement'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 初始问题陈述'
- en: '#4 The expected or grounded answer'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 预期或基于事实的答案'
- en: '#5 The output from the CoT prompt earlier'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 之前CoT提示的输出'
- en: Looking at the LLM output shown earlier in listing 10.7, you can see why the
    evaluation step may get confusing. Perhaps a fix to this could be suggesting to
    the LLM to provide the final answer in a single statement. In the next section,
    we move on to another example of prompt reasoning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到在列表10.7中之前展示的LLM输出，你可以理解为什么评估步骤可能会变得令人困惑。或许解决这个问题的一个方法就是建议LLM以单个陈述的形式提供最终答案。在下一节中，我们将继续探讨另一个提示推理的例子。
- en: 10.2.2 Zero-shot CoT prompting
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 零样本CoT提示
- en: As our time travel demonstrates, CoT prompting can be expensive in terms of
    prompt generation for a specific class of problem. While not as effective, there
    are techniques similar to CoT that don’t use examples and can be more generalized.
    This section will examine a straightforward phrase employed to elicit reasoning
    in LLMs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们的时间旅行所展示的，CoT提示在特定类问题上的提示生成可能很昂贵。虽然效果不如前者，但有一些类似于CoT的技术不使用示例，并且可以更加通用。本节将检查用于在LLM中引发推理的简单短语。
- en: Open `prompt_flow/zero-shot-cot-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. This flow is very similar to the previous CoT, as shown in
    figure 10.4\. The next lsting shows the YAML pseudocode that describes the flow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code的提示流程视觉编辑器中打开`prompt_flow/zero-shot-cot-prompting/flow.dag.yaml`。这个流程与之前的CoT非常相似，如图10.4所示。下一个列表显示了描述该流程的YAML伪代码。
- en: Listing 10.10 `zero-shot-CoT-prompting` flow
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.10 `zero-shot-CoT-prompting`流程
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 The final evaluation score'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 最终评估分数'
- en: '#2 The expected answer'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 预期答案'
- en: '#3 The predicted answer (the steps have been omitted showing the final answer)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 预测答案（已省略步骤以显示最终答案）'
- en: '#4 The initial problem statement'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 初始问题陈述'
- en: Run/test the flow in VS Code by pressing Shift-F5 while in the visual editor.
    The flow will run, and you should see output similar to that shown in listing
    10.10\. This exercise example performs better than the previous example on the
    same problem.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code的视觉编辑器中按Shift-F5运行/测试流程。流程将运行，你应该会看到类似于列表10.10所示的输出。这个练习示例在相同问题上比之前的例子表现更好。
- en: Open the `cot.jinja2` prompt in VS Code, as shown in listing 10.11\. This is
    a much simpler prompt than the previous example because it only uses zero-shot.
    However, one key phrase turns this simple prompt into a powerful reasoning engine.
    The line in the prompt `Let’s` `think` `step` `by` `step` triggers the LLM to
    consider internal context showing reasoning. This, in turn, directs the LLM to
    reason out the problem in steps.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`cot.jinja2`提示，如列表10.11所示。这个提示比之前的例子简单得多，因为它只使用了零样本。然而，一个关键短语将这个简单的提示转换成了一个强大的推理引擎。提示中的这一行`让我们`
    `一步一步` `来思考`触发了LLM考虑内部上下文以展示推理。这反过来又指导LLM分步骤推理出问题。
- en: Listing 10.11 `cot.jinja2`
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.11 `cot.jinja2`
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 A magic line that formulates reasoning from the LLM'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 一行魔法代码，用于从LLM中构建推理'
- en: '#2 Asks the LLM to provide a final statement of the answer'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 要求LLM提供一个答案的最终陈述'
- en: '#3 The problem statement the LLM is asked to solve'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM被要求解决的问题陈述'
- en: Similar phrases asking the LLM to think about the steps or asking it to respond
    in steps also extract reasoning. We’ll demonstrate a similar but more elaborate
    technique in the next section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的短语要求LLM思考步骤或要求它以步骤的形式回答，也会提取推理。我们将在下一节中演示一个类似但更复杂的技术。
- en: 10.2.3 Step by step with prompt chaining
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 使用提示链分步骤
- en: We can extend the behavior of asking an LLM to think step by step into a chain
    of prompts that force the LLM to solve the problem in steps. In this section,
    we look at a technique called *prompt chaining* that forces an LLM to process
    problems in steps.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将向一个LLM逐步提问的行为扩展成一个链式提示，迫使LLM分步骤解决问题。在本节中，我们将探讨一种称为*提示链*的技术，该技术迫使LLM分步骤处理问题。
- en: Open the `prompt_flow/prompt-chaining/flow.dag.yaml` file in the visual editor,
    as shown in figure 10.5\. Prompt chaining breaks up the reasoning method used
    to solve a problem into chains of prompts. This technique forces the LLM to answer
    the problem in terms of steps.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 打开可视化编辑器中的`prompt_flow/prompt-chaining/flow.dag.yaml`文件，如图10.5所示。提示链将解决问题的推理方法分解成一系列提示。这项技术迫使LLM以步骤的形式回答问题。
- en: '![figure](../Images/10-5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10-5.png)'
- en: Figure 10.5 The prompt chaining flow
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5 提示链流程
- en: Listing 10.12 shows the YAML pseudocode that describes the flow in a few more
    details. This flow chains the output of the first LLM block into the second and
    then from the second into the third. Forcing the LLM to process the problem this
    way uncovers the reasoning pattern, but it can also be overly verbose.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.12展示了描述该流程的YAML伪代码，它提供了更多细节。这个流程将第一个LLM块的输出链式连接到第二个，然后从第二个连接到第三个。迫使LLM以这种方式处理问题揭示了推理模式，但它也可能过于冗长。
- en: Listing 10.12 `prompt-chaining` flow
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.12 `prompt-chaining`流程
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Start of the chain of prompts'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提示链的开始'
- en: '#2 Output from the previous step injected into this step'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将前一步的输出注入到这一步'
- en: '#3 Output from two previous steps injected into this step'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将前两个步骤的输出注入到这一步'
- en: '#4 The final solution statement, although wrong, is closer.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 最终的解决方案陈述，尽管是错误的，但更接近了。'
- en: Run the flow by pressing Shift-F5 from the visual editor, and you’ll see the
    output as shown in listing 10.12\. The answer is still not correct for the Alex
    problem, but we can see all the work the LLM is doing to reason out the problem.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从可视化编辑器按Shift-F5运行流程，你会看到如列表10.12所示的输出。对于Alex问题，答案仍然不正确，但我们可以看到LLM为了推理出问题所做的工作。
- en: 'Open up all three prompts: `decompose_steps.jinja2`, `calculate_steps.jinja2`,
    and `calculate_solution.jinja2` (see listings 10.13, 10.14, and 10.15, respectively).
    All three prompts shown in the listings can be compared to show how outputs chain
    together.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 打开所有三个提示：`decompose_steps.jinja2`、`calculate_steps.jinja2`和`calculate_solution.jinja2`（分别见列表10.13、10.14和10.15）。列表中展示的所有三个提示可以进行比较，以展示输出是如何链式连接的。
- en: Listing 10.13 `decompose_steps.jinja2`
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.13 `decompose_steps.jinja2`
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Forces the LLM to list only the steps and nothing else'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 迫使LLM只列出步骤，不列出其他任何内容'
- en: '#2 The initial problem statement'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始问题陈述'
- en: Listing 10.14 `calculate_steps.jinja2`
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.14 `calculate_steps.jinja2`
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Requests that the LLM not solve the whole problem, just the steps'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 要求LLM只解决步骤，而不是整个问题'
- en: '#2 Uses the magic statement to extract reasoning'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用魔法语句提取推理'
- en: '#3 Injects the steps produced by the decompose_steps step'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将分解步骤步骤产生的步骤注入其中'
- en: Listing 10.15 `calculate_solution.jinja2`
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.15 `calculate_solution.jinja2`
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Requests that the LLM output the final answer and not any steps'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 要求LLM输出最终答案，而不输出任何步骤'
- en: '#2 The decomposed steps'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 分解的步骤'
- en: '#3 The calculated steps'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算出的步骤'
- en: In this exercise example, we’re not performing any evaluation and scoring. Without
    the evaluation, we can see that this sequence of prompts still has problems solving
    our more challenging time travel problem shown earlier in figure 10.3\. However,
    that doesn’t mean this technique doesn’t have value, and this prompting format
    solves some complex problems well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习示例中，我们并没有进行任何评估和评分。没有评估，我们可以看到这个提示序列仍然存在一些问题，无法解决之前在图10.3中展示的更具挑战性的时间旅行问题。然而，这并不意味着这项技术没有价值，这种提示格式在解决一些复杂问题方面表现良好。
- en: What we want to find, however, is a reasoning and planning methodology that
    can solve such complex problems consistently. The following section moves from
    reasoning to evaluating the best solution.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们想要找到的是一种推理和规划方法，可以一致地解决如此复杂的问题。接下来的部分将从推理转向评估最佳解决方案。
- en: 10.3 Employing evaluation for consistent solutions
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 使用评估来实现一致性的解决方案
- en: In the previous section, we learned that even the best-reasoned plans may not
    always derive the correct solution. Furthermore, we may not always have the answer
    to confirm if that solution is correct. The reality is that we often want to use
    some form of evaluation to determine the efficacy of a solution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解到即使是经过最佳推理的计划也可能不会总是得出正确的解决方案。此外，我们可能并不总是有答案来确认该解决方案是否正确。现实情况是，我们经常想要使用某种形式的评估来确定解决方案的有效性。
- en: 'Figure 10.6 shows a comparison of the prompt engineering strategies that have
    been devised as a means of getting LLMs to reason and plan. We’ve already covered
    the two on the left: zero-shot direct prompting and CoT prompting. The following
    example exercises in this section will look at self-consistency with the CoT and
    ToT techniques.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6显示了作为使LLMs进行推理和规划手段而设计的提示工程策略的比较。我们已经涵盖了左侧的两个：零样本直接提示和CoT提示。本节下面的示例练习将探讨与CoT和ToT技术结合的自洽性。
- en: '![figure](../Images/10-6.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-6.png)'
- en: Figure 10.6 Comparing the various prompt engineering strategies to enable reasoning
    and planning from LLMs
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6 比较各种提示工程策略以实现从LLMs中进行推理和规划
- en: We’ll continue to focus on the complex time travel problem to compare these
    more advanced methods that expand on reasoning and planning with evaluation. In
    the next section, we’ll evaluate self-consistency.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续关注复杂的时间旅行问题，以比较这些更高级的方法，这些方法通过评估扩展了推理和规划。在下一节中，我们将评估自洽性。
- en: 10.3.1 Evaluating self-consistency prompting
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 评估自洽提示
- en: Consistency in prompting is more than just lowering the temperature parameter
    we send to an LLM. Often, we want to generate a consistent plan or solution and
    still use a high temperature to better evaluate all the variations to a plan.
    By evaluating multiple different plans, we can get a better sense of the overall
    value of a solution.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的一致性不仅仅是降低我们发送给LLM的温度参数。通常，我们希望生成一个一致的计划或解决方案，同时使用较高的温度来更好地评估计划的全部变体。通过评估多个不同的计划，我们可以更好地了解解决方案的整体价值。
- en: '*Self-consistent prompting* is the technique of generating multiple plans/solutions
    for a given problem. Then, those plans are evaluated, and the more frequent or
    consistent plan is accepted. Imagine three plans generated, where two are similar,
    but the third is different. Using self-consistency, we evaluate the first two
    plans as the more consistent answer.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*自洽提示*是针对给定问题生成多个计划/解决方案的技术。然后，评估这些计划，并接受出现频率更高或更一致的计划。想象一下生成了三个计划，其中两个相似，但第三个不同。使用自洽性，我们将前两个计划评估为更一致的答案。'
- en: Open `prompt_flow/self-consistency-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. The flow diagram shows the simplicity of the prompt generation
    flow in figure 10.7\. Next to it in the diagram is the self-consistency evaluation
    flow.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code提示流可视化编辑器中打开`prompt_flow/self-consistency-prompting/flow.dag.yaml`。流程图显示了图10.7中提示生成流程的简单性。在图旁边的是自洽评估流程。
- en: '![figure](../Images/10-7.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/10-7.png)'
- en: Figure 10.7 The self-consistency prompt generation beside the evaluation flow
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7 自洽提示生成与评估流程并置
- en: Prompt flow uses a direct acyclic graph (DAG) format to execute the flow logic.
    DAGs are an excellent way of demonstrating and executing flow logic, but because
    they are *acyclic,* meaning they can’t repeat, they can’t execute loops. However,
    because prompt flow provides a batch processing mechanism, we can use that to
    simulate loops or repetition in a flow.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 提示流使用直接无环图（DAG）格式来执行流程逻辑。DAGs是展示和执行流程逻辑的绝佳方式，但由于它们是无环的，意味着它们不能重复，因此不能执行循环。然而，由于提示流提供了一个批量处理机制，我们可以使用它来模拟流程中的循环或重复。
- en: Referring to figure 10.6, we can see that self-consistency processes the input
    three times before collecting the results and determining the best plan/reply.
    We can apply this same pattern but use batch processing to generate the outputs.
    Then, the evaluation flow will aggregate the results and determine the best answer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图10.6，我们可以看到自洽过程在收集结果并确定最佳计划/回复之前对输入进行了三次处理。我们可以应用相同的模式，但使用批量处理来生成输出。然后，评估流程将汇总结果并确定最佳答案。
- en: Open the `self-consistency-prompting/cot.jinja2` prompt template in VS Code
    (see listing 10.16). The listing was shortened, as we’ve seen parts before. This
    prompt uses two (few-shot prompt) examples of a CoT to demonstrate the thought
    reasoning to the LLM.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`self-consistency-prompting/cot.jinja2`提示模板（参见列表10.16）。列表已被缩短，因为我们之前已经看到了部分内容。此提示使用两个（少样本提示）CoT示例来向LLM展示推理过程。
- en: Listing 10.16 `self-consistency-prompting/cot.jinja2`
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.16 `self-consistency-prompting/cot.jinja2`
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 The Sarah time travel problem'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Sarah时间旅行问题'
- en: '#2 Sample CoT, cut for brevity'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 样本CoT，为了简洁已截断'
- en: '#3 The Max time travel problem'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 最大时间旅行问题'
- en: '#4 Sample CoT, cut for brevity'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 样本CoT，为了简洁已截断'
- en: '#5 Final guide and statement to constrain output'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 最终指南和声明以约束输出'
- en: 'Open the `self-consistency-prompting/flow.dag.yaml` file in VS Code. Run the
    example in batch mode by clicking Batch Run (the beaker icon) from the visual
    editor. Figure 10.8 shows the process step by step:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`self-consistency-prompting/flow.dag.yaml`文件。通过从可视化编辑器中点击批量运行（水壶图标）来以批量模式运行示例。图10.8展示了逐步过程：
- en: Click Batch Run.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击批量运行。
- en: Select the JSON Lines (JSONL) input.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择JSON Lines (JSONL)输入。
- en: Select `statements.jsonl`.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`statements.jsonl`。
- en: Click the Run link.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击运行链接。
- en: '![figure](../Images/10-8.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10-8.png)'
- en: Figure 10.8 The step-by-step process of launching a batch process
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8 启动批量过程的逐步过程
- en: TIP  If you need to review the process, refer to chapter 9, which covers this
    process in more detail.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您需要回顾过程，请参阅第9章，其中更详细地介绍了此过程。
- en: Listing 10.17 shows the JSON output from executing the flow in batch mode. The
    `statements.jsonl` file has five identical Alex time travel problem entries. Using
    identical entries allows us to simulate the prompt executing five times on the
    duplicate entry.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.17显示了批量模式下执行流程的JSON输出。`statements.jsonl`文件有五个相同的Alex时间旅行问题条目。使用相同的条目允许我们模拟在重复条目上执行提示五次。
- en: Listing 10.17 `self-consistency-prompting` batch execution output
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.17 `self-consistency-prompting` 批量执行输出
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 The path where the flow was executed from'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 流程执行的路径'
- en: '#2 The folder containing the outputs of the flow (note this path)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 包含流程输出的文件夹（注意此路径）'
- en: '#3 The data used to run the flow in batch'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 批量运行流程使用的数据'
- en: You can view the flow produced by pressing the Ctrl key and clicking the output
    link, highlighted in listing 10.17\. This will open another instance of VS Code,
    showing a folder with all the output from the run. We now want to check the most
    consistent answer. Fortunately, the evaluation feature in prompt flow can help
    us identify consistent answers using similarity matching.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过按Ctrl键并点击列表10.17中突出显示的输出链接来查看生成的流程。这将打开VS Code的另一个实例，显示包含运行所有输出的文件夹。我们现在想检查最一致的答案。幸运的是，提示流程中的评估功能可以帮助我们使用相似度匹配来识别一致的答案。
- en: Open `self-consistency-evaluation/flow.dag.yaml` in VS Code (see figure 10.7).
    This flow embeds the predicted answer and then uses an aggregation to determine
    the most consistent answer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`self-consistency-evaluation/flow.dag.yaml`（参见图10.7）。此流程将预测答案嵌入，然后使用聚合来确定最一致的答案。
- en: From the flow, open `consistency.py` in VS Code, as shown in listing 10.18\.
    The code for this tool function calculates the cosine similarity for all pairs
    of answers. Then, it finds the most similar answer, logs it, and outputs that
    as the answer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从流程中，在VS Code中打开`consistency.py`（如列表10.18所示）。此工具函数的代码计算所有答案对之间的余弦相似度。然后，它找到最相似的答案，记录它，并将其作为答案输出。
- en: Listing 10.18 `consistency.py`
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.18 `consistency.py`
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Calculates the mean of all the embeddings'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算所有嵌入的平均值'
- en: '#2 Calculates cosine similarity for each pair of embeddings'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算每对嵌入的余弦相似度'
- en: '#3 Finds the index of the most similar answer'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 找到最相似答案的索引'
- en: '#4 Logs the output as a metric'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将输出记录为指标'
- en: '#5 Returns the text for the most similar answer'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回最相似答案的文本'
- en: We need to run the evaluation flow in batch mode as well. Open `self-consistency-evaluation/flow.dag.yaml`
    in VS Code and run the flow in batch mode (beaker icon). Then, select Existing
    Run as the flow input, and when prompted, choose the top or the last run you just
    executed as input.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要以批量模式运行评估流程。在VS Code中打开`self-consistency-evaluation/flow.dag.yaml`，以批量模式（水壶图标）运行流程。然后，选择现有运行作为流程输入，当提示时，选择你刚刚执行的顶部或最后一个运行作为输入。
- en: Again, after the flow completes processing, you’ll see an output like that shown
    in listing 10.17\. Ctrl-click on the output folder link to open a new instance
    of VS Code showing the results. Locate and open the `metric.json` file in VS Code,
    as shown in figure 10.9.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，在流程完成处理之后，你会看到类似于列表10.17中所示的结果。在输出文件夹链接上Ctrl点击以打开一个新的VS Code实例，显示结果。在VS Code中定位并打开如图10.9所示的`metric.json`文件。
- en: '![figure](../Images/10-9.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10-9.png)'
- en: Figure 10.9 The VS Code is open to the batch run output folder. Highlighted
    are the `metrics.json` file and the output showing the most similar answer.
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9 VS Code打开到批量运行输出文件夹。突出显示的是`metrics.json`文件和显示最相似答案的输出。
- en: The answer shown in figure 10.9 is still incorrect for this run. You can continue
    a few more batch runs of the prompt and/or increase the number of runs in a batch
    and then evaluate flows to see if you get better answers. This technique is generally
    more helpful for more straightforward problems but still demonstrates an inability
    to reason out complex problems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9中显示的答案对于这次运行仍然是不正确的。你可以继续进行几个更多的提示批量运行，或者增加批量中的运行次数，然后评估流程以查看你是否能得到更好的答案。这种技术通常对更直接的问题更有帮助，但仍然展示了无法推理出复杂问题的能力。
- en: Self-consistency uses a reflective approach to evaluate the most likely thought.
    However, the most likely thing is certainly not always the best. Therefore, we
    must consider a more comprehensive approach in the next section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 自洽性使用反思方法来评估最可能的思想。然而，最可能的事情并不总是最好的。因此，在下一节中，我们必须考虑一个更全面的方法。
- en: 10.3.2 Evaluating tree of thought prompting
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 评估思维树提示
- en: As mentioned earlier, ToT prompting, as shown in figure 10.6, combines self-evaluation
    and prompt chaining techniques. As such, it breaks down the sequence of planning
    into a chain of prompts, but at each step in the chain, it provides for multiple
    evaluations. This creates a tree that can be executed and evaluated at each level,
    breadth-first, or from top to bottom, depth-first.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ToT提示，如图10.6所示，结合了自我评估和提示链技术。因此，它将规划序列分解为一系列提示，但在链的每个步骤中，它都提供了多次评估。这创建了一个可以在每个级别执行和评估的树，可以是广度优先，也可以是自顶向下的深度优先。
- en: Figure 10.10 shows the difference between executing a tree using breadth-first
    or depth-first. Unfortunately, due to the DAG execution pattern of prompt flow,
    we can’t quickly implement the depth-first method, but breadth-first works just
    fine.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10显示了使用广度优先或深度优先执行树之间的差异。遗憾的是，由于提示流的DAG执行模式，我们无法快速实现深度优先方法，但广度优先工作得很好。
- en: '![figure](../Images/10-10.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10-10.png)'
- en: Figure 10.10 Breadth-first vs. depth-first execution on a ToT pattern
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.10 ToT模式中的广度优先与深度优先执行
- en: Open `tree-of-thought-evaluation/flow.dag.yaml` in VS Code. The visual of the
    flow is shown in figure 10.11\. This flow functions like a breadth-first ToT pattern—the
    flow chains together a series of prompts asking the LLM to return multiple plans
    at each step.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`tree-of-thought-evaluation/flow.dag.yaml`。流程的可视化如图10.11所示。这个流程像一个广度优先的ToT模式——流程将一系列提示链在一起，要求LLM在每一步返回多个计划。
- en: '![figure](../Images/10-11.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10-11.png)'
- en: Figure 10.11 ToT pattern expressed and prompt flow
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11 表达的ToT模式和提示流程
- en: Because the flow executes in a breadth-first style, each level output of the
    nodes is also evaluated. Each node in the flow uses a pair of semantic functions—one
    to generate the answer and the other to evaluate the answer. The semantic function
    is a custom Python flow block that processes multiple inputs and generates multiple
    outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流程以广度优先的方式执行，每个节点的每个级别输出也会被评估。流程中的每个节点都使用一对语义函数——一个用于生成答案，另一个用于评估答案。语义函数是一个自定义的Python流程块，它处理多个输入并生成多个输出。
- en: Listing 10.19 shows the `semantic_function.py` tool. This general tool is reused
    for multiple blocks in this flow. It also demonstrates the embedding functionality
    from the SK for direct use within prompt flow.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.19展示了`semantic_function.py`工具。这个通用工具在本流程的多个块中被重复使用。它还展示了从SK直接用于提示流程的嵌入功能。
- en: Listing 10.19 `semantic_function.py`
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.19 `semantic_function.py`
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Uses a union to allow for different types of LLM connections'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用并集允许不同类型的LLM连接'
- en: '#2 Checks to see if the input is empty or None; if so, the function shouldn’t
    be executed.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检查输入是否为空或None；如果是，则不应执行该函数。'
- en: '#3 Sets up the generation function that creates a plan'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置生成函数，创建计划'
- en: '#4 Sets up the evaluation function'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 设置评估函数'
- en: '#5 Runs the evaluate function and determines if the input is good enough to
    continue'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 运行评估函数并确定输入是否足够好以继续'
- en: '#6 If the evaluation score is high enough, generates the next step'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 如果评估分数足够高，则生成下一步'
- en: The semantic function tool is used in the tree’s experts, nodes, and answer
    blocks. At each step, the function determines if any text is being input. If there
    is no text, the block returns with no execution. Passing no text to a block means
    that the previous block failed evaluation. By evaluating before each step, ToT
    short-circuits the execution of plans it deems as not being valid.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 语义函数工具用于树中的专家、节点和答案块。在每一步，该函数确定是否有文本输入。如果没有文本，则块返回而不执行。向块传递无文本意味着上一个块评估失败。通过在每一步之前评估，ToT
    会短路其认为无效的计划执行。
- en: This may be a complex pattern to grasp at first, so go ahead and run the flow
    in VS Code. Listing 10.20 shows just the answer node output of a run; these results
    may vary from what you see but should be similar. Nodes that return no text either
    failed evaluation or their parents did.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个一开始难以掌握的复杂模式，所以请继续在 VS Code 中运行该流程。列表 10.20 仅显示了运行中的答案节点输出；这些结果可能与您看到的不同，但应该是相似的。没有返回文本的节点要么评估失败，要么其父节点无效。
- en: Listing 10.20 Output from `tree-of-thought-evaluation` flow
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.20 `tree-of-thought-evaluation` 流程的输出
- en: '[PRE19]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Represents that the first node plans weren’t valid and not executed'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 表示第一个节点的计划无效且未执行'
- en: '#2 The plan for node 2 and answer 2 failed evaluation and wasn’t run.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 节点 2 的计划和答案 2 评估失败，未运行。'
- en: '#3 The plan for this node failed to evaluate and wasn’t run.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 此节点的计划评估失败，未运行。'
- en: The output in listing 10.20 shows how only a select set of nodes was evaluated.
    In most cases, the evaluated nodes returned an answer that could be valid. Where
    no output was produced, it means that the node itself or its parent wasn’t valid.
    When sibling nodes all return empty, the parent node fails to evaluate.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.20 中的输出显示了仅评估了一组选定的节点。在大多数情况下，评估的节点返回了一个可能有效的答案。没有输出产生，意味着该节点本身或其父节点无效。当所有兄弟节点都返回空值时，父节点评估失败。
- en: As we can see, ToT is valid for complex problems but perhaps not very practical.
    The execution of this flow can take up to 27 calls to an LLM to generate an output.
    In practice, it may only do half that many calls, but that’s still a dozen or
    more calls to answer a single problem.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，ToT 对复杂问题有效，但可能不太实用。此流程的执行可能需要多达 27 次调用 LLM 生成输出。在实践中，可能只需调用一半那么多，但那仍然是回答一个问题的十几次调用。
- en: 10.4 Exercises
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 练习
- en: 'Use the following exercises to improve your knowledge of the material:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高您对材料的了解：
- en: '*Exercise 1*—Create Direct Prompting, Few-Shot Prompting, and Zero-Shot Prompting'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 1*—创建直接提示、少量样本提示和零样本提示'
- en: '*Objective *—Create three different prompts for an LLM to summarize a recent
    scientific article: one using direct prompting, one with few-shot prompting, and
    the last employing zero-shot prompting.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—为 LLM 创建三个不同的提示，以总结最近的一篇科学文章：一个使用直接提示，一个使用少量样本提示，最后一个使用零样本提示。'
- en: '*Tasks:*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Compare the effectiveness of the summaries generated by each approach.
  id: totrans-237
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较每种方法生成的摘要的有效性。
- en: Compare the accuracy of the summaries generated by each approach.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较每种方法生成的摘要的准确性。
- en: '*Exercise 2*—Craft Reasoning Prompts'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 2*—制作推理提示'
- en: '*Objective *—Design a set of prompts that require the LLM to solve logical
    puzzles or riddles.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—设计一组提示，要求 LLM 解决逻辑谜题或谜语。'
- en: '*Tasks:*'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Focus on how the structure of your prompt can influence the LLM’s reasoning
    process.
  id: totrans-242
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注您的提示结构如何影响 LLM 的推理过程。
- en: Focus on how the same can influence the correctness of its answers.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注相同的方法如何影响其答案的正确性。
- en: '*Exercise 3*—Evaluation Prompt Techniques'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习 3*—评估提示技术'
- en: '*Objective *—Develop an evaluation prompt that asks the LLM to predict the
    outcome of a hypothetical experiment.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—开发一个评估提示，要求 LLM 预测一个假设实验的结果。'
- en: '*Task:*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Create a follow-up prompt that evaluates the LLM’s prediction for accuracy and
    provides feedback on its reasoning process.
  id: totrans-247
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个后续提示，以评估 LLM 的预测准确性，并提供对其推理过程的反馈。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Direct solution prompting is a foundational method of using prompts to direct
    LLMs toward solving specific problems or tasks, emphasizing the importance of
    clear question-and-answer structures.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接解决方案提示是使用提示引导 LLM 解决特定问题或任务的基础方法，强调清晰的问题和答案结构的重要性。
- en: Few-shot prompting provides LLMs with a few examples to guide them in handling
    new or unseen content, highlighting its power in enabling the model to adapt to
    unfamiliar patterns.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本提示为LLM提供少量示例，以指导他们处理新或未见过的内容，突出了其使模型能够适应未知模式的能力。
- en: Zero-shot learning and prompting demonstrate how LLMs can generalize from their
    training to solve problems without needing explicit examples, showcasing their
    inherent ability to understand and apply knowledge in new contexts.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零样本学习和提示展示了LLM如何从其训练中泛化以解决问题，而无需显式示例，展示了它们在理解并应用新情境中的知识的能力。
- en: Chain of thought prompting guides the LLMs through a reasoning process step
    by step to solve complex problems, illustrating how to elicit detailed reasoning
    from the model.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链提示引导LLM逐步通过推理过程解决复杂问题，说明了如何从模型中提取详细推理。
- en: Prompt chaining breaks down a problem into a series of prompts that build upon
    each other, showing how to structure complex problem-solving processes into manageable
    steps for LLMs.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示链将问题分解为一系列相互构建的提示，展示了如何将复杂的问题解决过程结构化为LLM可管理的步骤。
- en: Self-consistency is a prompt technique that generates multiple solutions to
    a problem and selects the most consistent answer through evaluation, emphasizing
    the importance of consistency in achieving reliable outcomes.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自洽性是一种提示技术，它为问题生成多个解决方案，并通过评估选择最一致的答案，强调了在实现可靠结果中一致性的重要性。
- en: Tree of thought prompting combines self-evaluation and prompt chaining to create
    a comprehensive strategy for tackling complex problems, allowing for a systematic
    exploration of multiple solution paths.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维树提示结合自我评估和提示链，为解决复杂问题创建了一种全面的策略，允许系统地探索多个解决方案路径。
- en: Advanced prompt engineering strategies provide insights into sophisticated techniques
    such as self-consistency with CoT and ToT, offering methods to increase the accuracy
    and reliability of LLM-generated solutions.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提示工程策略提供了对诸如与CoT和ToT的自洽性等复杂技术的见解，提供了提高LLM生成解决方案的准确性和可靠性的方法。
