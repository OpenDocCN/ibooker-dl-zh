- en: Chapter 6\. Ensemble Learning and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 集成学习和随机森林
- en: Suppose you pose a complex question to thousands of random people, then aggregate
    their answers. In many cases you will find that this aggregated answer is better
    than an expert’s answer. This is called the *wisdom of the crowd*. Similarly,
    if you aggregate the predictions of a group of predictors (such as classifiers
    or regressors), you will often get better predictions than with the best individual
    predictor. A group of predictors is called an *ensemble*; thus, this technique
    is called *ensemble learning*, and an ensemble learning algorithm is called an
    *ensemble method*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你向成千上万的随机人群提出一个复杂问题，然后汇总他们的答案。在许多情况下，你会发现这个汇总的答案比专家的答案更好。这被称为**群体智慧**。同样，如果你汇总了一组预测者的预测（例如分类器或回归器），你通常会得到比最佳单个预测者更好的预测。一组预测者被称为**集成**；因此，这种技术被称为**集成学习**，而集成学习算法被称为**集成方法**。
- en: As an example of an ensemble method, you can train a group of decision tree
    classifiers, each on a different random subset of the training set. You can then
    obtain the predictions of all the individual trees, and the class that gets the
    most votes is the ensemble’s prediction (see the last exercise in [Chapter 5](ch05.html#trees_chapter)).
    Such an ensemble of decision trees is called a *random forest*, and despite its
    simplicity, this is one of the most powerful machine learning algorithms available
    today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为集成方法的一个例子，你可以训练一组决策树分类器，每个分类器在不同的训练集随机子集上。然后你可以获得所有单个树的预测，得到投票最多的类别就是集成的预测（参见[第5章](ch05.html#trees_chapter)中的最后一个练习）。这样的决策树集成被称为**随机森林**，尽管它很简单，但这是今天最强大的机器学习算法之一。
- en: 'As discussed in [Chapter 2](ch02.html#project_chapter), you will often use
    ensemble methods near the end of a project, once you have already built a few
    good predictors, to combine them into an even better predictor. In fact, the winning
    solutions in machine learning competitions often involve several ensemble methods—most
    famously in the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
    There are some downsides, however: ensemble learning requires much more computing
    resources than using a single model (both for training and for inference), it
    can be more complex to deploy and manage, and the predictions are harder to interpret.
    But the pros often outweigh the cons.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch02.html#project_chapter)所述，你通常会在项目接近尾声时使用集成方法，一旦你已经构建了几个好的预测者，将它们组合成一个更好的预测者。事实上，机器学习竞赛中的获胜方案通常涉及几个集成方法——最著名的是在[Netflix
    Prize竞赛](https://en.wikipedia.org/wiki/Netflix_Prize)中。然而，也有一些缺点：集成学习比使用单个模型需要更多的计算资源（无论是训练还是推理），它可能更复杂，部署和管理起来更困难，而且预测结果更难解释。但优点通常大于缺点。
- en: In this chapter we will examine the most popular ensemble methods, including
    voting classifiers, bagging and pasting ensembles, random forests, boosting, and
    stacking ensembles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨最流行的集成方法，包括投票分类器、袋装和粘贴集成、随机森林、提升和堆叠集成。
- en: Voting Classifiers
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器
- en: Suppose you have trained a few classifiers, each one achieving about 80% accuracy.
    You may have a logistic regression classifier, an SVM classifier, a random forest
    classifier, a *k*-nearest neighbors classifier, and perhaps a few more (see [Figure 6-1](#voting_classifier_training_diagram)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经训练了几个分类器，每个分类器的准确率大约为80%。你可能有一个逻辑回归分类器、一个SVM分类器、一个随机森林分类器、一个**k**-最近邻分类器，可能还有更多（参见[图6-1](#voting_classifier_training_diagram)）。
- en: '![Diagram illustrating the training of diverse classifiers, including logistic
    regression, SVM, random forest, and others, highlighting their role in ensemble
    learning.](assets/hmls_0601.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![展示多种分类器训练图，包括逻辑回归、SVM、随机森林等，突出它们在集成学习中的作用](assets/hmls_0601.png)'
- en: Figure 6-1\. Training diverse classifiers
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1 训练多种分类器
- en: 'A very simple way to create an even better classifier is to aggregate the predictions
    of each classifier: the class that gets the most votes is the ensemble’s prediction.
    This majority-vote classifier is called a *hard voting* classifier (see [Figure 6-2](#voting_classifier_prediction_diagram)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个更好的分类器的一个非常简单的方法是汇总每个分类器的预测：得到最多投票的类别就是集成的预测。这种多数投票分类器被称为**硬投票**分类器（参见[图6-2](#voting_classifier_prediction_diagram)）。
- en: '![Diagram illustrating a hard voting classifier, where diverse predictors contribute
    their predictions, with the majority vote determining the ensemble''s final prediction
    for a new instance.](assets/hmls_0602.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![展示硬投票分类器的图解，其中多样化的预测因子贡献它们的预测，多数投票决定集成对新实例的最终预测。（图片链接：assets/hmls_0602.png）](assets/hmls_0602.png)'
- en: Figure 6-2\. Hard voting classifier predictions
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 硬投票分类器预测
- en: Somewhat surprisingly, this voting classifier often achieves a higher accuracy
    than the best classifier in the ensemble. In fact, even if each classifier is
    a *weak learner* (meaning it does only slightly better than random guessing),
    the ensemble can still be a *strong learner* (achieving high accuracy), provided
    there are a sufficient number of weak learners in the ensemble and they are sufficiently
    diverse (i.e., if they focus on different aspects of the data and make different
    kinds of errors).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这种投票分类器通常比集成中最好的分类器具有更高的准确率。实际上，即使每个分类器都是一个*弱学习器*（意味着它仅略好于随机猜测），只要集成中有足够数量的弱学习器，并且它们足够多样化（即，如果它们关注数据的不同方面并犯不同类型的错误），集成仍然可以成为一个*强学习器*（实现高准确率）。
- en: 'How is this possible? The following analogy can help shed some light on this
    mystery. Suppose you have a slightly biased coin that has a 51% chance of coming
    up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will
    generally get more or less 510 heads and 490 tails, and hence a majority of heads.
    If you do the math, you will find that the probability of obtaining a majority
    of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher
    the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This
    is due to the *law of large numbers*: as you keep tossing the coin, the ratio
    of heads gets closer and closer to the probability of heads (51%). [Figure 6-3](#law_of_large_numbers_plot)
    shows 10 series of biased coin tosses. You can see that as the number of tosses
    increases, the ratio of heads approaches 51%. Eventually all 10 series end up
    so close to 51% that they are consistently above 50%.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？以下类比可以帮助揭示这个谜团。假设您有一个略微偏颇的硬币，其出现正面的概率为51%，出现反面的概率为49%。如果您抛掷它1,000次，您通常会得到大约510次正面和490次反面，因此大多数是正面。如果您进行数学计算，您会发现1,000次抛掷后获得多数正面的概率接近75%。您抛掷硬币的次数越多，概率就越高（例如，抛掷10,000次，概率超过97%）。这是由于大数定律：随着您不断抛掷硬币，正面比率越来越接近正面的概率（51%）。[图6-3](#law_of_large_numbers_plot)展示了10次有偏硬币抛掷。您可以看到，随着抛掷次数的增加，正面比率接近51%。最终，所有10次抛掷的结果都接近51%，并且始终高于50%。
- en: '![A line chart illustrating 10 series of biased coin tosses, showing that as
    the number of tosses increases, the heads ratio approaches 51%, demonstrating
    the law of large numbers.](assets/hmls_0603.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![展示10次有偏硬币抛掷的折线图，显示随着抛掷次数的增加，正面比率接近51%，证明了大数定律。（图片链接：assets/hmls_0603.png）](assets/hmls_0603.png)'
- en: Figure 6-3\. The law of large numbers
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 大数定律
- en: Similarly, suppose you build an ensemble containing 1,000 classifiers that are
    individually correct only 51% of the time (barely better than random guessing).
    If you predict the majority voted class, you can hope for up to 75% accuracy!
    However, this is only true if all classifiers are perfectly independent, making
    uncorrelated errors, which is clearly not the case because they are trained on
    the same data. They are likely to make the same types of errors, so there will
    be many majority votes for the wrong class, reducing the ensemble’s accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设您构建了一个包含1,000个分类器的集成，这些分类器各自正确率仅为51%（略好于随机猜测）。如果您预测多数投票的类别，您可以期望达到高达75%的准确率！然而，这只在所有分类器完全独立、犯不相关的错误的情况下才成立，这显然是不可能的，因为它们是在相同的数据上训练的。它们很可能会犯相同类型的错误，因此会有许多多数投票选择错误的类别，从而降低集成方法的准确性。
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Ensemble methods work best when the predictors are as independent from one another
    as possible. One way to get diverse classifiers is to train them using very different
    algorithms. This increases the chance that they will make very different types
    of errors, improving the ensemble’s accuracy. You can also play with the model
    hyperparameters to get diverse models, or train the models on different subsets
    of the data, as we will see.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在预测因子尽可能相互独立时效果最佳。获得多样化分类器的一种方法是通过使用非常不同的算法来训练它们。这增加了它们犯不同类型错误的几率，从而提高了集成方法的准确性。您还可以通过调整模型超参数来获得多样化的模型，或者在不同的数据子集上训练模型，正如我们将看到的。
- en: 'Scikit-Learn provides a `VotingClassifier` class that’s quite easy to use:
    just give it a list of name/predictor pairs, and use it like a normal classifier.
    Let’s try it on the moons dataset (introduced in [Chapter 5](ch05.html#trees_chapter)).
    We will load and split the moons dataset into a training set and a test set, then
    we’ll create and train a voting classifier composed of three diverse classifiers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个非常容易使用的`VotingClassifier`类：只需给它一个名称/预测器对的列表，就像使用一个正常的分类器一样。让我们在月亮数据集（在第5章中介绍）上试试它。我们将加载并分割月亮数据集为训练集和测试集，然后我们将创建并训练一个由三个不同分类器组成的投票分类器：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you fit a `VotingClassifier`, it clones every estimator and fits the clones.
    The original estimators are available via the `estimators` attribute, while the
    fitted clones are available via the `estimators_` attribute. If you prefer a dict
    rather than a list, you can use `named_estimators` or `named_estimators_` instead.
    To begin, let’s look at each fitted classifier’s accuracy on the test set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拟合一个`VotingClassifier`时，它会克隆每一个估计器并拟合克隆。原始估计器可以通过`estimators`属性访问，而拟合的克隆可以通过`estimators_`属性访问。如果你更喜欢字典而不是列表，可以使用`named_estimators`或`named_estimators_`。首先，让我们看看每个拟合分类器在测试集上的准确率：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you call the voting classifier’s `predict()` method, it performs hard
    voting. For example, the voting classifier predicts class 1 for the first instance
    of the test set, because two out of three classifiers predict that class:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用投票分类器的`predict()`方法时，它执行硬投票。例如，投票分类器预测测试集的第一个实例为类别1，因为三个分类器中有两个预测这个类别：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let’s look at the performance of the voting classifier on the test set:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看投票分类器在测试集上的性能：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There you have it! The voting classifier outperforms all the individual classifiers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结果！投票分类器的性能超过了所有单个分类器。
- en: 'If all classifiers are able to estimate class probabilities (i.e., if they
    all have a `predict_proba()` method), then you should generally tell Scikit-Learn
    to predict the class with the highest class probability, averaged over all the
    individual classifiers. This is called *soft voting*. It often achieves higher
    performance than hard voting because it gives more weight to highly confident
    votes. All you need to do is set the voting classifier’s `voting` hyperparameter
    to `"soft"`, and ensure that all classifiers can estimate class probabilities.
    This is not the case for the `SVC` class by default, so you need to set its `probability`
    hyperparameter to `True` (this will make the `SVC` class use cross-validation
    to estimate class probabilities, slowing down training, and it will add a `predict_proba()`
    method). Let’s try that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有分类器都能够估计类概率（即，如果它们都有`predict_proba()`方法），那么你应该通常告诉Scikit-Learn预测所有单个分类器平均的最高类概率，这被称为*软投票*。它通常比硬投票有更高的性能，因为它给予高度自信的投票更多的权重。你只需要设置投票分类器的`voting`超参数为`"soft"`，并确保所有分类器都能估计类概率。默认情况下，`SVC`类并不是这样，所以你需要将其`probability`超参数设置为`True`（这将使`SVC`类使用交叉验证来估计类概率，从而减慢训练速度，并且它将添加一个`predict_proba()`方法）。让我们试试看：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We reach 92% accuracy simply by using soft voting—not bad!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅通过使用软投票就达到了92%的准确率——这已经很不错了！
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Soft voting works best when the estimated probabilities are well-calibrated.
    If they are not, you can use `sklearn.calibration.CalibratedClassifierCV` to calibrate
    them (see [Chapter 3](ch03.html#classification_chapter)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当估计的概率很好地校准时，软投票效果最好。如果不这样，可以使用`sklearn.calibration.CalibratedClassifierCV`来校准它们（参见第3章[分类章节](ch03.html#classification_chapter)）。
- en: Bagging and Pasting
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打包和粘贴
- en: One way to get a diverse set of classifiers is to use very different training
    algorithms, as just discussed. Another way is to use the same training algorithm
    for every predictor but train them on different random subsets of the training
    set. When sampling is performed *with* replacement,⁠^([1](ch06.html#id1744)) this
    method is called [*bagging*](https://homl.info/20)⁠^([2](ch06.html#id1745)) (short
    for *bootstrap aggregating*⁠^([3](ch06.html#id1746))). When sampling is performed
    *without* replacement, it is called [*pasting*](https://homl.info/21).⁠^([4](ch06.html#id1747))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一组不同分类器的一种方法就是使用非常不同的训练算法，正如刚才讨论的那样。另一种方法是使用相同的训练算法为每个预测器，但它们在不同的随机训练集子集上训练。当采样是*带有*替换时，这种方法被称为[*打包*](https://homl.info/20)（简称为*bootstrap
    aggregating*），^([1](ch06.html#id1744))。当采样是*不带*替换时，它被称为[*粘贴*](https://homl.info/21)。⁠^([4](ch06.html#id1747))
- en: In other words, both bagging and pasting allow training instances to be sampled
    several times across multiple predictors, but only bagging allows training instances
    to be sampled several times for the same predictor. This sampling and training
    process is represented in [Figure 6-4](#bagging_training_diagram).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，Bagging和Pasting都允许在多个预测器中对训练实例进行多次采样，但只有Bagging允许对同一个预测器进行多次采样。这个过程在[图6-4](#bagging_training_diagram)中表示。
- en: '![Diagram illustrating the bagging process, where multiple predictors are trained
    on random samples with replacement from the training set.](assets/hmls_0604.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![展示Bagging过程的图解，其中多个预测器在从训练集的随机样本中带有替换的训练。](assets/hmls_0604.png)'
- en: Figure 6-4\. Bagging and pasting involve training several predictors on different
    random samples of the training set
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. Bagging和Pasting涉及在训练集的不同随机样本上训练多个预测器
- en: Once all predictors are trained, the ensemble can make a prediction for a new
    instance by simply aggregating the predictions of all predictors. For classification,
    the aggregation function is typically the *statistical mode* (i.e., the most frequent
    prediction, just like with a hard voting classifier), and for regression it’s
    usually just the average. Each individual predictor has a higher bias than if
    it were trained on the original training set, but aggregation reduces both bias
    and variance.⁠^([5](ch06.html#id1749))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有预测器都经过训练，集成可以通过简单地聚合所有预测器的预测来对新实例进行预测。对于分类，聚合函数通常是*统计众数*（即最频繁的预测，就像在硬投票分类器中一样），而对于回归，通常是平均值。每个单独的预测器比在原始训练集上训练时的偏差更高，但聚合减少了偏差和方差。⁠^([5](ch06.html#id1749))
- en: 'To get an intuition of why this is the case, imagine that you trained two regressors
    to predict house prices. The first underestimates the prices by $40,000 on average,
    while the second overestimates them by $50,000 on average. Assuming these regressors
    are 100% independent and their predictions follow a normal distribution, if you
    compute the average of the two predictions, the result will overestimate the prices
    by only (–40,000 + 50,000)/2 = $5,000 on average: that’s a much lower bias! Similarly,
    if both predictors have a $10,000 standard deviation (i.e., a variance of 100,000,000),
    then the average prediction will have a variance of (10,000² + 10,000²)/2² = 50,000,000
    (i.e., the standard deviation will be $7,071). The variance is halved!'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么会出现这种情况，想象一下你训练了两个回归器来预测房价。第一个回归器平均低估了房价40,000美元，而第二个回归器平均高估了房价50,000美元。假设这些回归器是100%独立的，并且它们的预测遵循正态分布，如果你计算这两个预测的平均值，结果将平均高估房价（-40,000
    + 50,000）/2 = 5,000美元：这比偏差低得多！同样，如果两个预测器的标准差都是10,000美元（即方差为100,000,000），那么平均预测的方差将是（10,000²
    + 10,000²）/2² = 50,000,000（即标准差将是7,071美元）。方差减半了！
- en: In practice, the ensemble often ends up with a similar bias but a lower variance
    than a single predictor trained on the original training set. Therefore it works
    best with high-variance and low-bias models (e.g., ensembles of decision trees,
    not ensembles of linear regressors).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，集成通常具有与原始训练集上训练的单个预测器相似的偏差，但方差更低。因此，它最适合高方差和低偏差的模型（例如决策树集成，而不是线性回归器的集成）。
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Prefer bagging when your dataset is noisy or your model is prone to overfitting
    (e.g., deep decision tree). Otherwise, prefer pasting as it avoids redundancy
    during training, making it a bit more computationally efficient.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据集有噪声或你的模型容易过拟合（例如深度决策树）时，请优先选择Bagging。否则，请优先选择Pasting，因为它在训练过程中避免了冗余，从而使其在计算上更有效率。
- en: 'As you can see in [Figure 6-4](#bagging_training_diagram), predictors can all
    be trained in parallel, via different CPU cores or even different servers. Similarly,
    predictions can be made in parallel. This is one of the reasons bagging and pasting
    are such popular methods: they scale very well.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6-4](#bagging_training_diagram)所示，预测器可以通过不同的CPU核心甚至不同的服务器并行训练。同样，预测也可以并行进行。这也是Bagging和Pasting如此受欢迎的原因之一：它们的可扩展性非常好。
- en: Bagging and Pasting in Scikit-Learn
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn中的Bagging和Pasting
- en: 'Scikit-Learn offers a simple API for both bagging and pasting: the `BaggingClassifier`
    class (or `BaggingRegressor` for regression). The following code trains an ensemble
    of 500 decision tree classifiers:⁠^([6](ch06.html#id1752)) each is trained on
    100 training instances randomly sampled from the training set with replacement
    (this is an example of bagging, but if you want to use pasting instead, just set
    `bootstrap=False`). The `n_jobs` parameter tells Scikit-Learn the number of CPU
    cores to use for training and predictions, and `–1` tells Scikit-Learn to use
    all available cores:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了对bagging和pasting的简单API：`BaggingClassifier`类（或用于回归的`BaggingRegressor`）。以下代码训练了一个包含500个决策树分类器的集成：⁠^([6](ch06.html#id1752))
    每个分类器都是通过对训练集进行有放回的随机采样（每个分类器100个训练实例）进行训练的（这是一个bagging的例子，但如果你想使用pasting，只需设置`bootstrap=False`）。`n_jobs`参数告诉Scikit-Learn用于训练和预测的CPU核心数，`–1`告诉Scikit-Learn使用所有可用的核心：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A `BaggingClassifier` automatically performs soft voting instead of hard voting
    if the base classifier can estimate class probabilities (i.e., if it has a `predict_proba()`
    method), which is the case with decision tree classifiers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果基础分类器可以估计类概率（即，如果它有`predict_proba()`方法），则`BaggingClassifier`会自动执行软投票而不是硬投票，这是决策树分类器的情况。
- en: '[Figure 6-5](#decision_tree_without_and_with_bagging_plot) compares the decision
    boundary of a single decision tree with the decision boundary of a bagging ensemble
    of 500 trees (from the preceding code), both trained on the moons dataset. As
    you can see, the ensemble’s predictions will likely generalize much better than
    the single decision tree’s predictions: the ensemble has a comparable bias but
    a smaller variance (it makes roughly the same number of errors on the training
    set, but the decision boundary is less irregular).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-5](#decision_tree_without_and_with_bagging_plot)比较了单个决策树的决策边界与在月亮数据集上训练的包含500棵树的bagging集成决策边界（来自前面的代码）。如图所示，集成预测的泛化能力可能会比单个决策树的预测更好：集成具有可比的偏差但较小的方差（它在训练集上犯的错误数量大致相同，但决策边界不太规则）。'
- en: Bagging introduces a bit more diversity in the subsets that each predictor is
    trained on, so bagging ends up with a slightly higher bias than pasting; but the
    extra diversity also means that the predictors end up being less correlated, so
    the ensemble’s variance is reduced. Overall, bagging often results in better models,
    which explains why it’s generally preferred. But if you have spare time and CPU
    power, you can use cross-validation to evaluate both bagging and pasting, and
    select the one that works best.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging在训练每个预测器的子集时引入了更多的多样性，因此bagging最终具有比pasting略高的偏差；但额外的多样性也意味着预测器最终相关性较低，因此集成的方差减少。总的来说，bagging通常会产生更好的模型，这也是为什么它通常更受欢迎的原因。但如果你有额外的空闲时间和CPU能力，你可以使用交叉验证来评估bagging和pasting，并选择效果最好的一个。
- en: '![Comparison of a single decision tree''s decision boundaries versus those
    of a bagging ensemble with 500 trees, illustrating variance reduction through
    bagging.](assets/hmls_0605.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![单棵决策树的决策边界与包含500棵树的bagging集成决策边界的比较，展示了通过bagging实现的方差减少](assets/hmls_0605.png)'
- en: Figure 6-5\. A single decision tree (left) versus a bagging ensemble of 500
    trees (right)
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 单个决策树（左）与包含500棵树的bagging集成（右）
- en: Out-of-Bag Evaluation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外评估
- en: With bagging, some training instances may be sampled several times for any given
    predictor, while others may not be sampled at all. By default, a `BaggingClassifier`
    samples *m* training instances with replacement (`bootstrap=True`), where *m*
    is the size of the training set. With this process, it can be shown mathematically
    that only about 63% of the training instances are sampled on average for each
    predictor.⁠^([7](ch06.html#id1757)) The remaining 37% of the training instances
    that are not sampled are called *out-of-bag* (OOB) instances. Note that they are
    not the same 37% for all predictors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在bagging中，某些训练实例可能被多次采样，而其他实例可能根本不会被采样。默认情况下，`BaggingClassifier`使用有放回的方式采样*m*个训练实例（`bootstrap=True`），其中*m*是训练集的大小。通过这个过程，可以数学上证明每个预测器平均只有大约63%的训练实例被采样。⁠^([7](ch06.html#id1757))
    未被采样的剩余37%的训练实例被称为*袋外*（OOB）实例。请注意，它们不是所有预测器的37%都相同。
- en: 'A bagging ensemble can be evaluated using OOB instances, without the need for
    a separate validation set: indeed, if there are enough estimators, then each instance
    in the training set will likely be an OOB instance of several estimators, so these
    estimators can be used to make a fair ensemble prediction for that instance. Once
    you have a prediction for each instance, you can compute the ensemble’s prediction
    accuracy (or any other metric).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用OOB实例来评估袋装集成，无需单独的验证集：实际上，如果有足够的估计器，那么训练集中的每个实例很可能是几个估计器的OOB实例，因此可以使用这些估计器为该实例做出公平的集成预测。一旦为每个实例做出预测，就可以计算集成预测的准确率（或任何其他指标）。
- en: 'In Scikit-Learn, you can set `oob_score=True` when creating a `BaggingClassifier`
    to request an automatic OOB evaluation after training. The following code demonstrates
    this. The resulting evaluation score is available in the `oob_score_` attribute:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，当你创建`BaggingClassifier`时，可以将`oob_score=True`设置为请求训练后的自动OOB评估。以下代码演示了这一点。结果评估分数可在`oob_score_`属性中找到：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'According to this OOB evaluation, this `BaggingClassifier` is likely to achieve
    about 89.6% accuracy on the test set. Let’s verify this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种OOB评估，这个`BaggingClassifier`很可能在测试集中达到大约89.6%的准确率。让我们来验证这一点：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We get 92.0% accuracy on the test. The OOB evaluation was a bit too pessimistic.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试中达到了92.0%的准确率。OOB评估稍微有些悲观。
- en: 'The OOB decision function for each training instance is also available through
    the `oob_decision_function_` attribute. Since the base estimator has a `predict_proba()`
    method, the decision function returns the class probabilities for each training
    instance. For example, the OOB evaluation estimates that the first training instance
    has a 67.6% probability of belonging to the positive class and a 32.4% probability
    of belonging to the negative class:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练实例的OOB决策函数也通过`oob_decision_function_`属性提供。由于基本估计器有`predict_proba()`方法，决策函数返回每个训练实例的类概率。例如，OOB评估估计第一个训练实例有67.6%的概率属于正类，有32.4%的概率属于负类：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Random Patches and Random Subspaces
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机补丁和随机子空间
- en: 'The `BaggingClassifier` class supports sampling the features as well. Sampling
    is controlled by two hyperparameters: `max_features` and `bootstrap_features`.
    They work the same way as `max_samples` and `bootstrap`, but for feature sampling
    instead of instance sampling. Thus, each predictor will be trained on a random
    subset of the input features.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingClassifier`类支持采样特征。采样由两个超参数控制：`max_features`和`bootstrap_features`。它们与`max_samples`和`bootstrap`的工作方式相同，但用于特征采样而不是实例采样。因此，每个预测器将在输入特征的随机子集上训练。'
- en: This technique is particularly useful when you are dealing with high-dimensional
    inputs (such as images), as it can considerably speed up training. Sampling both
    training instances and features is called the [*random patches* method](https://homl.info/22).⁠^([8](ch06.html#id1765))
    Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`)
    but sampling features (by setting `bootstrap_features` to `True` and/or `max_features`
    to a value smaller than `1.0`) is called the [*random subspaces* method](https://homl.info/23).⁠^([9](ch06.html#id1767))
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理高维输入（如图像）时，这项技术特别有用，因为它可以显著加快训练速度。同时采样训练实例和特征被称为[*随机补丁*方法](https://homl.info/22)。⁠^([8](ch06.html#id1765))
    保持所有训练实例（通过设置`bootstrap=False`和`max_samples=1.0`），但采样特征（通过设置`bootstrap_features`为`True`和/或`max_features`为一个小于`1.0`的值）被称为[*随机子空间*方法](https://homl.info/23)。⁠^([9](ch06.html#id1767))
- en: Sampling features results in even more predictor diversity, trading a bit more
    bias for a lower variance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 采样特征导致预测器多样性更高，以略微增加的偏差换取更低的方差。
- en: Random Forests
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'As we have discussed, a [random forest](https://homl.info/24)⁠^([10](ch06.html#id1771))
    is an ensemble of decision trees, generally trained via the bagging method (or
    sometimes pasting), typically with `max_samples` set to the size of the training
    set. Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`,
    you can use the `RandomForestClassifier` class, which is more convenient and optimized
    for decision trees⁠^([11](ch06.html#id1772)) (similarly, there is a `RandomForestRegressor`
    class for regression tasks). The following code trains a random forest classifier
    with 500 trees, each limited to maximum 16 leaf nodes, using all available CPU
    cores:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，一个 [随机森林](https://homl.info/24)⁠^([10](ch06.html#id1771)) 是决策树的集成，通常通过
    bagging 方法（或有时是粘贴）训练，通常将 `max_samples` 设置为训练集的大小。您不需要构建一个 `BaggingClassifier`
    并传递一个 `DecisionTreeClassifier`，而是可以使用 `RandomForestClassifier` 类，它更方便且针对决策树进行了优化⁠^([11](ch06.html#id1772))（类似地，还有一个
    `RandomForestRegressor` 类用于回归任务）。以下代码使用所有可用的 CPU 核心，训练了一个包含 500 棵树、每棵树最多 16 个叶节点的随机森林分类器：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With a few exceptions, a `RandomForestClassifier` has all the hyperparameters
    of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters
    of a `BaggingClassifier` to control the ensemble itself.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了少数例外，`RandomForestClassifier` 具有与 `DecisionTreeClassifier` 相同的所有超参数（用于控制树的生长方式），以及
    `BaggingClassifier` 的所有超参数，用于控制集成本身。
- en: 'The `RandomForestClassifier` class introduces extra randomness when growing
    trees: instead of searching for the very best feature when splitting a node (see
    [Chapter 5](ch05.html#trees_chapter)), it searches for the best feature among
    a random subset of features. By default, it samples $StartRoot n EndRoot$ features
    (where *n* is the total number of features). The algorithm results in greater
    tree diversity, which (again) trades a higher bias for a lower variance, generally
    yielding an overall better model. So, the following `BaggingClassifier` is equivalent
    to the previous `RandomForestClassifier`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier` 类在生长树时引入了额外的随机性：在分割节点时，它不是搜索最佳特征（参见第 5 章），而是在特征的一个随机子集中搜索最佳特征。默认情况下，它采样
    $StartRoot n EndRoot$ 个特征（其中 *n* 是特征的总数）。该算法导致树多样性增加，这（再次）以更高的偏差换取了更低的方差，通常会产生更好的整体模型。因此，以下
    `BaggingClassifier` 等同于之前的 `RandomForestClassifier`：'
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Extra-Trees
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外树
- en: When you are growing a tree in a random forest, at each node only a random subset
    of the features is considered for splitting (as discussed earlier). It is possible
    to make trees even more random by also using random thresholds for each feature
    rather than searching for the best possible thresholds (like regular decision
    trees do). For this, simply set `splitter="random"` when creating a `DecisionTreeClassifier`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当在随机森林中生长一棵树时，在每个节点上仅考虑随机子集的特征进行分割（如前所述）。通过为每个特征使用随机阈值而不是搜索最佳可能的阈值（如常规决策树所做的那样），可以使树变得更加随机。为此，在创建
    `DecisionTreeClassifier` 时，只需将 `splitter="random"` 设置即可。
- en: A forest of such extremely random trees is called an [*extremely randomized
    trees*](https://homl.info/25)⁠^([12](ch06.html#id1782)) (or *extra-trees* for
    short) ensemble. Once again, this technique trades more bias for a lower variance,
    so they may perform better than regular random forests if you encounter overfitting,
    especially with noisy and/or high-dimensional datasets. Extra-trees classifiers
    are also much faster to train than regular random forests, because finding the
    best possible threshold for each feature at every node is one of the most time-consuming
    tasks of growing a tree in a random forest.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的极端随机树森林被称为 [*极端随机树*](https://homl.info/25)⁠^([12](ch06.html#id1782))（或简称为
    *extra-trees*）集成。再次强调，这种技术以更多的偏差换取了更低的方差，因此，如果您遇到过拟合，尤其是在有噪声和/或高维数据集的情况下，它们可能比常规随机森林表现更好。与常规随机森林相比，额外树分类器训练得也更快，因为找到每个节点上每个特征的最佳可能阈值是随机森林中生长树时最耗时的任务之一。
- en: You can create an extra-trees classifier using Scikit-Learn’s `ExtraTreesClassifier`
    class. Its API is identical to the `RandomForestClassifier` class, except `bootstrap`
    defaults to `False`. Similarly, the `ExtraTreesRegressor` class has the same API
    as the `RandomForestRegressor` class, except `bootstrap` defaults to `False`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Scikit-Learn 的 `ExtraTreesClassifier` 类创建一个额外的树分类器。它的 API 与 `RandomForestClassifier`
    类相同，只是 `bootstrap` 默认为 `False`。同样，`ExtraTreesRegressor` 类的 API 与 `RandomForestRegressor`
    类相同，只是 `bootstrap` 默认为 `False`。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Just like decision tree classes, random forest classes and extra-trees classes
    in recent Scikit-Learn versions support missing values natively, no need for an
    imputer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如决策树类别一样，最近Scikit-Learn版本的随机森林类别和额外树类别原生支持缺失值，无需使用填充器。
- en: Feature Importance
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Yet another great quality of random forests is that they make it easy to measure
    the relative importance of each feature. Scikit-Learn measures a feature’s importance
    by looking at how much the tree nodes that use that feature reduce impurity on
    average, across all trees in the forest. More precisely, it is a weighted average,
    where each node’s weight is equal to the number of training samples that are associated
    with it (see [Chapter 5](ch05.html#trees_chapter)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的另一个优点是它们使得测量每个特征的相对重要性变得容易。Scikit-Learn通过查看使用该特征的树节点在森林中所有树上的平均纯度降低程度来衡量一个特征的重要性。更精确地说，这是一个加权平均值，其中每个节点的权重等于与之关联的训练样本数量（参见第5章[Chapter 5](ch05.html#trees_chapter)）。
- en: 'Scikit-Learn computes this score automatically for each feature after training,
    then it scales the results so that the sum of all importances is equal to 1\.
    You can access the result using the `feature_importances_` variable. For example,
    the following code trains a `RandomForestClassifier` on the iris dataset (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)) and outputs each feature’s importance.
    It seems that the most important features are the petal length (44%) and width
    (42%), while sepal length and width are rather unimportant in comparison (11%
    and 2%, respectively):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn在训练后自动为每个特征计算这个分数，然后调整结果，使得所有重要性的总和等于1。你可以通过`feature_importances_`变量访问结果。例如，以下代码在鸢尾花数据集（在第4章[Chapter 4](ch04.html#linear_models_chapter)中介绍）上训练了一个`RandomForestClassifier`，并输出了每个特征的重要性。看起来最重要的特征是花瓣长度（44%）和宽度（42%），而与花瓣长度和宽度相比，萼片长度和宽度则相对不重要（分别为11%和2%）：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Similarly, if you train a random forest classifier on the MNIST dataset (introduced
    in [Chapter 3](ch03.html#classification_chapter)) and plot each pixel’s importance,
    you get the image represented in [Figure 6-6](#mnist_feature_importance_plot).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你在MNIST数据集（在第3章[Chapter 3](ch03.html#classification_chapter)中介绍）上训练一个随机森林分类器，并绘制每个像素的重要性，你将得到[图6-6](#mnist_feature_importance_plot)中所示的形象。
- en: '![Heatmap showing MNIST pixel importance with colors indicating feature significance
    according to a random forest classifier, where yellow denotes high importance
    and red denotes low importance.](assets/hmls_0606.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![显示MNIST像素重要性热图，颜色表示根据随机森林分类器特征显著性的颜色，其中黄色表示高重要性，红色表示低重要性。](assets/hmls_0606.png)'
- en: Figure 6-6\. MNIST pixel importance (according to a random forest classifier)
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. MNIST像素重要性（根据随机森林分类器）
- en: Random forests are very handy to get a quick understanding of what features
    actually matter, in particular if you need to perform feature selection.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林非常适合快速了解哪些特征真正重要，特别是如果你需要执行特征选择。
- en: Boosting
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: '*Boosting* (originally called *hypothesis boosting*) refers to any ensemble
    method that can combine several weak learners into a strong learner. The general
    idea of most boosting methods is to train predictors sequentially, each trying
    to correct its predecessor. There are many boosting methods available, but by
    far the most popular are [*AdaBoost*](https://homl.info/26)⁠^([13](ch06.html#id1792))
    (short for *adaptive boosting*) and *gradient boosting*. Let’s start with AdaBoost.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*提升*（最初称为*假设提升*）指的是任何可以将多个弱学习器组合成强学习器的集成方法。大多数提升方法的一般思想是顺序训练预测器，每个预测器都试图纠正其前驱。有许多提升方法可用，但最流行的是[*AdaBoost*](https://homl.info/26)⁠^([13](ch06.html#id1792))（即*自适应提升*）和*梯度提升*。让我们从AdaBoost开始。'
- en: AdaBoost
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: One way for a new predictor to correct its predecessor is to pay a bit more
    attention to the training instances that the predecessor underfit. This results
    in new predictors focusing more and more on the hard cases. This is the technique
    used by AdaBoost.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 新预测器纠正其前驱的一种方法是对前驱欠拟合的训练实例给予更多的关注。这导致新预测器越来越多地关注困难案例。这是AdaBoost使用的技术。
- en: For example, when training an AdaBoost classifier, the algorithm first trains
    a base classifier (such as a decision tree) and uses it to make predictions on
    the training set. The algorithm then increases the relative weight of misclassified
    training instances. Then it trains a second classifier, using the updated weights,
    and again makes predictions on the training set, updates the instance weights,
    and so on (see [Figure 6-7](#adaboost_training_diagram)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当训练一个 AdaBoost 分类器时，算法首先训练一个基分类器（例如决策树）并使用它对训练集进行预测。然后算法增加被错误分类的训练实例的相对权重。接着它使用更新后的权重训练第二个分类器，并在训练集上再次进行预测，更新实例权重，依此类推（参见
    [图 6-7](#adaboost_training_diagram)）。
- en: '![Diagram illustrating the sequential training process of AdaBoost, showing
    how instance weights and predictions are updated through stages.](assets/hmls_0607.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![说明 AdaBoost 顺序训练过程的图，展示了实例权重和预测如何通过阶段更新。](assets/hmls_0607.png)'
- en: Figure 6-7\. AdaBoost sequential training with instance weight updates
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. AdaBoost 顺序训练与实例权重更新
- en: '[Figure 6-8](#boosting_plot) shows the decision boundaries of five consecutive
    predictors on the moons dataset (in this example, each predictor is a highly regularized
    SVM classifier with an RBF kernel).⁠^([14](ch06.html#id1793)) The first classifier
    gets many instances wrong, so their weights get boosted. The second classifier
    therefore does a better job on these instances, and so on. The plot on the right
    represents the same sequence of predictors, except that the learning rate is halved
    (i.e., the misclassified instance weights are boosted much less at every iteration).
    As you can see, this sequential learning technique has some similarities with
    gradient descent, except that instead of tweaking a single predictor’s parameters
    to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually
    making it better.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-8](#boosting_plot) 展示了五个连续预测器在月亮数据集上的决策边界（在这个例子中，每个预测器是一个具有 RBF 内核的高度正则化的
    SVM 分类器）。⁠^([14](ch06.html#id1793)) 第一个分类器错误地分类了许多实例，因此它们的权重被提升。因此，第二个分类器在这些实例上做得更好，依此类推。右侧的图表示相同的预测器序列，除了学习率减半（即，在每次迭代中错误分类的实例权重提升较少）。如您所见，这种顺序学习技术与梯度下降有一些相似之处，除了
    AdaBoost 不是调整单个预测器的参数以最小化成本函数，而是将预测器添加到集成中，逐渐使其变得更好。'
- en: '![Two diagrams showing decision boundaries of five consecutive SVM classifiers
    on the moons dataset, with learning rates 1 and 0.5, illustrating the impact of
    learning rate on AdaBoost’s performance.](assets/hmls_0608.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![展示五个连续 SVM 分类器在月亮数据集上的决策边界的两个图，学习率分别为 1 和 0.5，说明了学习率对 AdaBoost 性能的影响。](assets/hmls_0608.png)'
- en: Figure 6-8\. Decision boundaries of consecutive predictors
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 连续预测器的决策边界
- en: Warning
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'There is one important drawback to this sequential learning technique: training
    cannot be parallelized since each predictor can only be trained after the previous
    predictor has been trained and evaluated. As a result, it does not scale as well
    as bagging or pasting.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种顺序学习技术有一个重要的缺点：由于每个预测器只能在之前的预测器被训练和评估之后才能进行训练，因此无法并行化。因此，它的扩展性不如 bagging 或
    pasting。
- en: Once all predictors are trained, the ensemble makes predictions very much like
    bagging or pasting, except that predictors have different weights depending on
    their overall accuracy on the weighted training set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有预测器都经过训练，集成进行预测的方式与 bagging 或 pasting 非常相似，除了预测器的权重取决于它们在加权训练集上的整体准确性。
- en: Let’s take a closer look at the AdaBoost algorithm. Each instance weight *w*^((*i*))
    is initially set to 1/*m*, so their sum is 1\. A first predictor is trained, and
    its weighted error rate *r*[1] is computed on the training set; see [Equation
    6-1](#weighted_error_rate).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看 AdaBoost 算法。每个实例权重 *w*^((*i*)) 初始设置为 1/*m*，因此它们的总和为 1。首先训练一个预测器，并在训练集上计算其加权错误率
    *r*[1]；参见 [方程式 6-1](#weighted_error_rate)。
- en: Equation 6-1\. Weighted error rate of the j^(th) predictor
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 6-1\. 第 j 个预测器的加权错误率
- en: <mrow><msub><mi>r</mi><mi>j</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>where</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mtext>is</mtext>
    <mtext>the</mtext> <msup><mi>j</mi><mtext>th</mtext></msup> <mtext>predictor’s</mtext>
    <mtext>prediction</mtext><mtext>for</mtext> <mtext>the</mtext> <msup><mi>i</mi>
    <mtext>th</mtext></msup> <mtext>instance</mtext></mrow>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>r</mi><mi>j</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>其中</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mtext>是</mtext> <mtext>第</mtext> <msup><mi>j</mi><mtext>个</mtext></msup> <mtext>预测器的</mtext>
    <mtext>预测结果</mtext><mtext>针对</mtext> <mtext>第</mtext> <msup><mi>i</mi> <mtext>个</mtext></msup>
    <mtext>实例</mtext></mrow>
- en: The predictor’s weight *α*[*j*] is then computed using [Equation 6-2](#predictor_weight),
    where *η* is the learning rate hyperparameter (defaults to 1).⁠^([15](ch06.html#id1795))
    The more accurate the predictor is, the higher its weight will be. If it is just
    guessing randomly, then its weight will be close to zero. However, if it is most
    often wrong (i.e., less accurate than random guessing), then its weight will be
    negative.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器的权重 *α*[*j*] 然后通过[方程6-2](#predictor_weight)计算，其中 *η* 是学习率超参数（默认为1）。⁠^([15](ch06.html#id1795))
    预测器越准确，其权重越高。如果只是随机猜测，那么其权重将接近零。然而，如果它通常错误（即，不如随机猜测准确），那么其权重将是负数。
- en: Equation 6-2\. Predictor weight
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程6-2\. 预测器权重
- en: $dollar-sign alpha Subscript j Baseline equals eta log StartFraction 1 minus
    r Subscript j Baseline Over r Subscript j Baseline EndFraction dollar-sign$
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: $dollar-sign alpha Subscript j Baseline equals eta log StartFraction 1 minus
    r Subscript j Baseline Over r Subscript j Baseline EndFraction dollar-sign$
- en: Next, the AdaBoost algorithm updates the instance weights, using [Equation 6-3](#instance_weight_update),
    which boosts the weights of the misclassified instances and encourages the next
    predictor to pay more attention to them.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，AdaBoost算法更新实例权重，使用[方程6-3](#instance_weight_update)，这增加了错误分类实例的权重，并鼓励下一个预测器更加关注它们。
- en: Equation 6-3\. Weight update rule
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程6-3\. 权重更新规则
- en: $dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column for i equals 1
    comma 2 comma period period period comma m 2nd Row 1st Column Blank 2nd Column
    w Superscript left-parenthesis i right-parenthesis Baseline left-arrow StartLayout
    Enlarged left-brace 1st Row  w Superscript left-parenthesis i right-parenthesis
    Baseline if ModifyingAbove y With caret Superscript left-parenthesis i right-parenthesis
    Baseline equals y Superscript left-parenthesis i right-parenthesis Baseline 2nd
    Row  w Superscript left-parenthesis i right-parenthesis Baseline exp left-parenthesis
    alpha Subscript j Baseline right-parenthesis if ModifyingAbove y With caret Superscript
    left-parenthesis i right-parenthesis Baseline not-equals y Superscript left-parenthesis
    i right-parenthesis EndLayout EndLayout dollar-sign$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column for i equals 1
    comma 2 comma period period period comma m 2nd Row 1st Column Blank 2nd Column
    w Superscript left-parenthesis i right-parenthesis Baseline left-arrow StartLayout
    Enlarged left-brace 1st Row  w Superscript left-parenthesis i right-parenthesis
    Baseline if ModifyingAbove y With caret Superscript left-parenthesis i right-parenthesis
    Baseline equals y Superscript left-parenthesis i right-parenthesis Baseline 2nd
    Row  w Superscript left-parenthesis i right-parenthesis Baseline exp left-parenthesis
    alpha Subscript j Baseline right-parenthesis if ModifyingAbove y With caret Superscript
    left-parenthesis i right-parenthesis Baseline not-equals y Superscript left-parenthesis
    i right-parenthesis EndLayout EndLayout dollar-sign$
- en: Then all the instance weights are normalized to ensure that their sum is once
    again 1 (i.e., they are divided by $sigma-summation Underscript i equals 1 Overscript
    m Endscripts w Superscript left-parenthesis i right-parenthesis$ ).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将所有实例权重归一化，以确保它们的总和再次为1（即，它们被除以 $sigma-summation Underscript i equals 1 Overscript
    m Endscripts w Superscript left-parenthesis i right-parenthesis$ )。
- en: 'Finally, a new predictor is trained using the updated weights, and the whole
    process is repeated: the new predictor’s weight is computed, the instance weights
    are updated, then another predictor is trained, and so on. The algorithm stops
    when the desired number of predictors is reached, or when a perfect predictor
    is found.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用更新后的权重训练一个新的预测器，整个过程重复进行：计算新预测器的权重，更新实例权重，然后训练另一个预测器，依此类推。算法在达到所需预测器数量或找到完美预测器时停止。
- en: To make predictions, AdaBoost simply computes the predictions of all the predictors
    and weighs them using the predictor weights *α*[*j*]. The predicted class is the
    one that receives the majority of weighted votes (see [Equation 6-4](#adaboost_prediction)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，AdaBoost简单地计算所有预测器的预测值，并使用预测器权重 *α*[*j*] 对其进行加权。预测类别是获得加权投票多数的类别（见[方程
    6-4](#adaboost_prediction)）。
- en: Equation 6-4\. AdaBoost predictions
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 6-4\. AdaBoost 预测
- en: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo form="prefix">argmax</mo>
    <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>where</mtext>
    <mi>N</mi> <mtext>is</mtext> <mtext>the</mtext> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>predictors</mtext></mrow>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mrow><mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo form="prefix">argmax</mo>
    <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>其中</mtext>
    <mi>N</mi> <mtext>是</mtext> <mtext>预测器</mtext> <mtext>的数量</mtext></mrow>
- en: Scikit-Learn uses a multiclass version of AdaBoost called [*SAMME*](https://homl.info/27)⁠^([16](ch06.html#id1797))
    (which stands for *Stagewise Additive Modeling using a Multiclass Exponential
    loss function*). When there are just two classes, SAMME is equivalent to AdaBoost.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 使用 AdaBoost 的多分类版本，称为 [*SAMME*](https://homl.info/27)⁠^([16](ch06.html#id1797))（代表
    *使用多类指数损失函数的阶跃式加性建模*）。当只有两个类别时，SAMME 等同于 AdaBoost。
- en: 'The following code trains an AdaBoost classifier based on 30 *decision stumps*
    using Scikit-Learn’s `AdaBoostClassifier` class (as you might expect, there is
    also an `AdaBoostRegressor` class). A decision stump is a decision tree with `max_depth=1`—in
    other words, a tree composed of a single decision node plus two leaf nodes. This
    is the default base estimator for the `AdaBoostClassifier` class:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用 Scikit-Learn 的 `AdaBoostClassifier` 类（正如你所期望的，还有一个 `AdaBoostRegressor`
    类）训练了一个基于 30 个 *决策树桩* 的 AdaBoost 分类器。决策树桩是一个 `max_depth=1` 的决策树——换句话说，由一个决策节点和两个叶节点组成的树。这是
    `AdaBoostClassifier` 类的默认基础估计器：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Tip
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your AdaBoost ensemble is overfitting the training set, you can try reducing
    the number of estimators or more strongly regularizing the base estimator.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 AdaBoost 集成对训练集过拟合，你可以尝试减少估计器的数量或更强烈地正则化基础估计器。
- en: Gradient Boosting
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Another very popular boosting algorithm is [*gradient boosting*](https://homl.info/28).⁠^([17](ch06.html#id1803))
    Just like AdaBoost, gradient boosting works by sequentially adding predictors
    to an ensemble, each one correcting its predecessor. However, instead of tweaking
    the instance weights at every iteration like AdaBoost does, this method tries
    to fit the new predictor to the *residual errors* made by the previous predictor.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的提升算法是[*梯度提升*](https://homl.info/28)。⁠^([17](ch06.html#id1803)) 就像 AdaBoost
    一样，梯度提升通过顺序地向集成中添加预测器来工作，每个预测器都纠正其前一个预测器。然而，与 AdaBoost 在每次迭代中调整实例权重不同，这种方法试图将新预测器拟合到前一个预测器产生的
    *残差误差*。
- en: 'Let’s go through a simple regression example, using decision trees as the base
    predictors; this is called *gradient tree boosting*, or *gradient boosted regression
    trees* (GBRT). First, let’s generate a noisy quadratic dataset and fit a `DecisionTreeRegressor`
    to it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的回归示例来了解，使用决策树作为基础预测器；这被称为 *梯度树提升*，或 *梯度提升回归树*（GBRT）。首先，让我们生成一个有噪声的二次数据集，并将其拟合到
    `DecisionTreeRegressor`：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we’ll train a second `DecisionTreeRegressor` on the residual errors made
    by the first predictor:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在第一个预测器的残差误差上训练第二个`DecisionTreeRegressor`：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And then we’ll train a third regressor on the residual errors made by the second
    predictor:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将在第二个预测器的残差误差上训练第三个回归器：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we have an ensemble containing three trees. It can make predictions on
    a new instance simply by adding up the predictions of all the trees:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含三棵树的集成。它可以通过简单地将所有树的预测相加来对新实例进行预测：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Figure 6-9](#gradient_boosting_plot) represents the predictions of these three
    trees in the left column, and the ensemble’s predictions in the right column.
    In the first row, the ensemble has just one tree, so its predictions are exactly
    the same as the first tree’s predictions. In the second row, a new tree is trained
    on the residual errors of the first tree. On the right you can see that the ensemble’s
    predictions are equal to the sum of the predictions of the first two trees. Similarly,
    in the third row another tree is trained on the residual errors of the second
    tree. You can see that the ensemble’s predictions gradually get better as trees
    are added to the ensemble.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-9](#gradient_boosting_plot)展示了左列中这三棵树的预测和右列中集成的预测。在第一行，集成只有一棵树，所以其预测与第一棵树的预测完全相同。在第二行，新树是在第一棵树的残差误差上训练的。在右边可以看到，集成的预测等于前两棵树的预测之和。同样，在第三行，另一棵树是在第二棵树的残差误差上训练的。您可以看到，随着树被添加到集成中，集成的预测逐渐变得更好。'
- en: 'You can use Scikit-Learn’s `GradientBoostingRegressor` class to train GBRT
    ensembles more easily (there’s also a `GradientBoostingClassifier` class for classification).
    Much like the `RandomForestRegressor` class, it has hyperparameters to control
    the growth of decision trees (e.g., `max_depth`, `min_samples_leaf`), as well
    as hyperparameters to control the ensemble training, such as the number of trees
    (`n_estimators`). The following code creates the same ensemble as the previous
    one:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Scikit-Learn的`GradientBoostingRegressor`类来更容易地训练GBRT集成（还有用于分类的`GradientBoostingClassifier`类）。与`RandomForestRegressor`类类似，它具有控制决策树增长的超参数（例如，`max_depth`、`min_samples_leaf`），以及控制集成训练的超参数，例如树的数量（`n_estimators`）。以下代码创建了与之前相同的集成：
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Diagram illustrating gradient boosting; the left column shows individual
    predictors trained on residuals, and the right column shows the ensemble''s progressive
    predictions.](assets/hmls_0609.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![说明梯度提升的图；左列显示在残差上训练的个别预测器，右列显示集成逐步预测。](assets/hmls_0609.png)'
- en: Figure 6-9\. In this depiction of gradient boosting, the first predictor (top
    left) is trained normally, then each consecutive predictor (middle left and lower
    left) is trained on the previous predictor’s residuals; the right column shows
    the resulting ensemble’s predictions
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9。在此梯度提升的描述中，第一个预测器（左上角）是正常训练的，然后每个连续的预测器（中间左和底部左）是在前一个预测器的残差上训练的；右列显示了结果集成的预测
- en: 'The `learning_rate` hyperparameter scales the contribution of each tree. If
    you set it to a low value, such as `0.05`, you will need more trees in the ensemble
    to fit the training set, but the predictions will usually generalize better. This
    is a regularization technique called *shrinkage*. [Figure 6-10](#gbrt_learning_rate_plot)
    shows two GBRT ensembles trained with different hyperparameters: the one on the
    left does not have enough trees to fit the training set, while the one on the
    right has about the right amount. If we added more trees, the GBRT would start
    to overfit the training set. As usual, you can use cross-validation to find the
    optimal learning rate, using `GridSearchCV` or `RandomizedSearchCV`.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`超参数缩放每个树的贡献。如果您将其设置为低值，例如`0.05`，则需要在集成中添加更多树来拟合训练集，但预测通常泛化得更好。这是一种称为*收缩*的正则化技术。[图6-10](#gbrt_learning_rate_plot)显示了使用不同超参数训练的两个GBRT集成：左边的那个没有足够的树来拟合训练集，而右边的那个数量大约是合适的。如果我们添加更多树，GBRT将开始过度拟合训练集。通常，您可以使用交叉验证来找到最佳的学习率，使用`GridSearchCV`或`RandomizedSearchCV`。'
- en: '![Comparison of GBRT ensemble predictions with insufficient predictors (left
    graph) versus sufficient predictors (right graph).](assets/hmls_0610.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![GBRT集成预测与不足预测因子（左图）与充分预测因子（右图）的比较](assets/hmls_0610.png)'
- en: Figure 6-10\. GBRT ensembles with not enough predictors (left) and just enough
    (right)
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10。GBRT集成：不足预测因子（左）和恰好足够的预测因子（右）
- en: 'To find the optimal number of trees, you could also perform cross-validation,
    but there’s a simpler way: if you set the `n_iter_no_change` hyperparameter to
    an integer value, say 10, then the `GradientBoostingRegressor` will automatically
    stop adding more trees during training if it sees that the last 10 trees didn’t
    help. This is simply early stopping (introduced in [Chapter 4](ch04.html#linear_models_chapter)),
    but with a little bit of patience: it tolerates having no progress for a few iterations
    before it stops. Let’s train the ensemble using early stopping:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳树的数量，你也可以执行交叉验证，但有一个更简单的方法：如果你将 `n_iter_no_change` 超参数设置为一个整数值，比如 10，那么
    `GradientBoostingRegressor` 将在训练过程中自动停止添加更多树，如果它看到最后 10 棵树没有帮助。这仅仅是提前停止（在第 4 章中介绍），但需要一点耐心：它在停止之前可以容忍几轮没有进展。让我们使用提前停止来训练集成：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you set `n_iter_no_change` too low, training may stop too early and the
    model will underfit. But if you set it too high, it will overfit instead. We also
    set a fairly small learning rate and a high number of estimators, but the actual
    number of estimators in the trained ensemble is much lower, thanks to early stopping:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 `n_iter_no_change` 设置得太低，训练可能会过早停止，模型将欠拟合。但如果你设置得太高，它将过拟合。我们还设置了一个相当小的学习率和高数量的估计器，但训练的集成中实际估计器的数量要低得多，这要归功于提前停止：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When `n_iter_no_change` is set, the `fit()` method automatically splits the
    training set into a smaller training set and a validation set: this allows it
    to evaluate the model’s performance each time it adds a new tree. The size of
    the validation set is controlled by the `validation_fraction` hyperparameter,
    which is 10% by default. The `tol` hyperparameter determines the maximum performance
    improvement that still counts as negligible. It defaults to 0.0001.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置 `n_iter_no_change` 时，`fit()` 方法会自动将训练集分成一个较小的训练集和一个验证集：这允许它在每次添加新树时评估模型的性能。验证集的大小由
    `validation_fraction` 超参数控制，默认为 10%。`tol` 超参数决定了仍然被视为可忽略的最大性能提升。默认值为 0.0001。
- en: The `GradientBoostingRegressor` class also supports a `subsample` hyperparameter,
    which specifies the fraction of training instances to be used for training each
    tree. For example, if `subsample=0.25`, then each tree is trained on 25% of the
    training instances, selected randomly. As you can probably guess by now, this
    technique trades a higher bias for a lower variance. It also speeds up training
    considerably. This is called *stochastic gradient boosting*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor` 类也支持一个 `subsample` 超参数，该参数指定了用于训练每棵树的训练实例的分数。例如，如果
    `subsample=0.25`，则每棵树将在随机选择的 25% 的训练实例上训练。你现在可能已经猜到了，这种技术以更高的偏差换取了更低的方差。它还大大加快了训练速度。这被称为
    *随机梯度提升*。'
- en: Histogram-Based Gradient Boosting
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于直方图的梯度提升
- en: 'Scikit-Learn also provides another GBRT implementation, optimized for large
    datasets: *histogram-based gradient boosting* (HGB). It works by binning the input
    features, replacing them with integers. The number of bins is controlled by the
    `max_bins` hyperparameter, which defaults to 255 and cannot be set any higher
    than this. Binning can greatly reduce the number of possible thresholds that the
    training algorithm needs to evaluate. Moreover, working with integers makes it
    possible to use faster and more memory-efficient data structures. And the way
    the bins are built removes the need for sorting the features when training each
    tree.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 还提供了一个针对大型数据集优化的 GBRT 实现：*基于直方图的梯度提升*（HGB）。它通过将输入特征分箱，用整数替换它们来实现。分箱的数量由
    `max_bins` 超参数控制，默认为 255，不能设置得比这更高。分箱可以大大减少训练算法需要评估的可能阈值数量。此外，使用整数使得可以使用更快、更节省内存的数据结构。而且，构建分箱的方式消除了在训练每棵树时对特征排序的需要。
- en: 'As a result, this implementation has a computational complexity of *O*(*b*×*m*)
    instead of *O*(*n*×*m*×log(*m*)), where *b* is the number of bins, *m* is the
    number of training instances, and *n* is the number of features. In practice,
    this means that HGB can train hundreds of times faster than regular GBRT on large
    datasets. However, binning causes a precision loss, which acts as a regularizer:
    depending on the dataset, this may help reduce overfitting, or it may cause underfitting.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个实现的计算复杂度为 *O*(*b*×*m*)，而不是 *O*(*n*×*m*×log(*m*))，其中 *b* 是分箱的数量，*m* 是训练实例的数量，*n*
    是特征的数量。在实践中，这意味着 HGB 可以在大型数据集上比常规 GBRT 快数百倍进行训练。然而，分箱会导致精度损失，这充当正则化器：根据数据集的不同，这可能会帮助减少过拟合，或者可能导致欠拟合。
- en: 'Scikit-Learn provides two classes for HGB: `HistGradientBoostingRegressor`
    and `HistGradientBoostingClassifier`. They’re similar to `GradientBoostingRegressor`
    and `GradientBoostingClassifier`, with a few notable differences:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn为HGB提供了两个类：`HistGradientBoostingRegressor`和`HistGradientBoostingClassifier`。它们与`GradientBoostingRegressor`和`GradientBoostingClassifier`类似，但有几个显著的区别：
- en: Early stopping is automatically activated if the number of instances is greater
    than 10,000\. You can turn early stopping always on or always off by setting the
    `early_stopping` hyperparameter to `True` or `False`.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果实例数量大于10,000，则会自动激活早期停止。您可以通过将`early_stopping`超参数设置为`True`或`False`来始终开启或始终关闭早期停止。
- en: Subsampling is not supported.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持子采样。
- en: '`n_estimators` is renamed to `max_iter`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`被重命名为`max_iter`。'
- en: The only decision tree hyperparameters that can be tweaked are `max_leaf_nodes`,
    `min_samples_leaf`, `max_depth`, and `max_features`.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以调整的唯一决策树超参数是`max_leaf_nodes`、`min_samples_leaf`、`max_depth`和`max_features`。
- en: 'The HGB classes support missing values natively, as well as categorical features.
    This simplifies preprocessing quite a bit. However, the categorical features must
    be represented as integers ranging from 0 to a number lower than `max_bins`. You
    can use an `OrdinalEncoder` for this. For example, here’s how to build and train
    a complete pipeline for the California housing dataset introduced in [Chapter 2](ch02.html#project_chapter):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: HGB类原生支持缺失值以及分类特征。这大大简化了预处理工作。然而，分类特征必须表示为从0到`max_bins`以下数字的整数。您可以使用`OrdinalEncoder`来完成这项工作。例如，以下是如何为[第2章](ch02.html#project_chapter)中介绍的加利福尼亚住房数据集构建和训练完整管道的方法：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The whole pipeline is almost as short as the imports! No need for an imputer,
    scaler, or a one-hot encoder, it’s really convenient. Note that `categorical_features`
    must be set to the categorical column indices (or a Boolean array). Without any
    hyperparameter tuning, this model yields an RMSE of about 47,600, which is not
    too bad.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道几乎与导入一样短！不需要填充器、缩放器或独热编码器，这非常方便。请注意，`categorical_features`必须设置为分类列的索引（或布尔数组）。在不进行任何超参数调整的情况下，此模型产生的RMSE约为47,600，这并不算太差。
- en: 'In short, HGB is a great choice when you have a fairly large dataset, especially
    when it contains categorical features and missing values: it performs well, doesn’t
    require much preprocessing work, and training is fast. However, it can be a bit
    less accurate than GBRT, due to the binning, so you might want to try both.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，当您拥有相当大的数据集时，HGB是一个很好的选择，尤其是当它包含分类特征和缺失值时：它表现良好，不需要太多的预处理工作，训练速度快。然而，由于分箱，它可能比GBRT略逊一筹，因此您可能想尝试两者。
- en: Tip
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Several other optimized implementations of gradient boosting are available
    in the Python ML ecosystem: in particular, [XGBoost](https://github.com/dmlc/xgboost),
    [CatBoost](https://catboost.ai), and [LightGBM](https://lightgbm.readthedocs.io).
    These libraries have been around for several years. They are all specialized for
    gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide
    many additional features, including hardware acceleration using GPUs; you should
    definitely check them out! Moreover, [Yggdrasil Decision Forests (YDF)](https://ydf.readthedocs.io)
    provides optimized implementations of a variety of random forest algorithms.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Python机器学习生态系统中提供了几种梯度提升的优化实现：特别是[XGBoost](https://github.com/dmlc/xgboost)、[CatBoost](https://catboost.ai)和[LightGBM](https://lightgbm.readthedocs.io)。这些库已经存在了几年。它们都是专门针对梯度提升的，它们的API与Scikit-Learn的非常相似，并且提供了许多附加功能，包括使用GPU的硬件加速；您绝对应该检查它们！此外，[Yggdrasil决策森林（YDF）](https://ydf.readthedocs.io)提供了各种随机森林算法的优化实现。
- en: Stacking
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: 'The last ensemble method we will discuss in this chapter is called *stacking*
    (short for [*stacked generalization*](https://homl.info/29)).⁠^([18](ch06.html#id1836))
    It is based on a simple idea: instead of using trivial functions (such as hard
    voting) to aggregate the predictions of all predictors in an ensemble, why don’t
    we train a model to perform this aggregation? [Figure 6-11](#blending_prediction_diagram)
    shows such an ensemble performing a regression task on a new instance. Each of
    the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and
    then the final predictor (called a *blender*, or a *meta learner*) takes these
    predictions as inputs and makes the final prediction (3.0).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的最后一种集成方法是称为*堆叠*（简称[*堆叠泛化*](https://homl.info/29)）。它基于一个简单的想法：我们为什么不训练一个模型来执行这个聚合，而不是使用简单的函数（如硬投票）来聚合集成中所有预测器的预测？[图6-11](#blending_prediction_diagram)展示了这样一个集成在新的实例上执行回归任务。底部的三个预测器预测不同的值（3.1、2.7和2.9），然后最终的预测器（称为*混合器*或*元学习器*）将这些预测作为输入并做出最终预测（3.0）。
- en: '![Diagram illustrating how blending combines predictions from multiple predictors
    to produce a singular prediction for a new instance.](assets/hmls_0611.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![展示如何将多个预测器的预测组合起来，为新的实例生成单个预测的示意图。](assets/hmls_0611.png)'
- en: Figure 6-11\. Aggregating predictions using a blending predictor
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 使用混合预测器聚合预测
- en: To train the blender, you first need to build the blending training set. You
    can use `cross_val_predict()` on every predictor in the ensemble to get out-of-sample
    predictions for each instance in the original training set ([Figure 6-12](#blending_layer_training_diagram)),
    and use these as the input features to train the blender; the targets can simply
    be copied from the original training set. Note that regardless of the number of
    features in the original training set (just one in this example), the blending
    training set will contain one input feature per predictor (three in this example).
    Once the blender is trained, the base predictors must be retrained one last time
    on the full original training set (since `cross_val_predict()` does not give access
    to the trained estimators).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练混合器，你首先需要构建混合训练集。你可以对集成中的每个预测器使用`cross_val_predict()`来获取原始训练集中每个实例的样本外预测（[图6-12](#blending_layer_training_diagram)），并将这些作为输入特征来训练混合器；目标可以直接从原始训练集中复制。请注意，无论原始训练集中的特征数量有多少（本例中只有一个），混合训练集将包含每个预测器的一个输入特征（本例中为三个）。一旦混合器被训练，基础预测器必须在完整的原始训练集上重新训练一次（因为`cross_val_predict()`无法访问已训练的估计器）。
- en: It is actually possible to train several different blenders this way (e.g.,
    one using linear regression, another using random forest regression) to get a
    whole layer of blenders, and then add another blender on top of that to produce
    the final prediction, as shown in [Figure 6-13](#multi_layer_blending_diagram).
    You may be able to squeeze out a few more drops of performance by doing this,
    but it will cost you in both training time and system complexity.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，可以通过这种方式训练几个不同的混合器（例如，一个使用线性回归，另一个使用随机森林回归）来获得一个完整的混合器层，然后在上面添加另一个混合器来生成最终预测，如图[图6-13](#multi_layer_blending_diagram)所示。通过这样做，你可能能够挤出更多性能，但这将增加训练时间和系统复杂性。
- en: '![Diagram illustrating the process of training a blender in a stacking ensemble,
    showing how predictions from multiple models form a blending training set for
    the final blending step.](assets/hmls_0612.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![展示在堆叠集成中训练混合器的过程，显示多个模型的预测如何形成一个混合训练集，用于最终的混合步骤的示意图。](assets/hmls_0612.png)'
- en: Figure 6-12\. Training the blender in a stacking ensemble
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. 在堆叠集成中训练混合器
- en: '![Diagram illustrating a multilayer stacking ensemble with three layers, showing
    how predictions are processed through interconnected blenders from a new instance
    input to produce a final prediction.](assets/hmls_0613.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![展示具有三个层的多层堆叠集成，显示预测是如何通过相互连接的混合器从新实例输入到最终预测的过程的示意图。](assets/hmls_0613.png)'
- en: Figure 6-13\. Predictions in a multilayer stacking ensemble
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13\. 多层堆叠集成中的预测
- en: 'Scikit-Learn provides two classes for stacking ensembles: `StackingClassifier`
    and `StackingRegressor`. For example, we can replace the `VotingClassifier` we
    used at the beginning of this chapter on the moons dataset with a `StackingClassifier`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了两个用于堆叠集成的类：`StackingClassifier`和`StackingRegressor`。例如，我们可以用`StackingClassifier`替换本章开头在moons数据集上使用的`VotingClassifier`：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For each predictor, the stacking classifier will call `predict_proba()` if available;
    if not it will fall back to `decision_function()` or, as a last resort, call `predict()`.
    If you don’t provide a final estimator, `StackingClassifier` will use `LogisticRegression`
    and `StackingRegressor` will use `RidgeCV`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个预测器，如果可用，堆叠分类器将调用 `predict_proba()`；如果不可用，它将回退到 `decision_function()`，或者作为最后的手段调用
    `predict()`。如果您不提供最终估计器，`StackingClassifier` 将使用 `LogisticRegression`，而 `StackingRegressor`
    将使用 `RidgeCV`。
- en: If you evaluate this stacking model on the test set, you will find 92.8% accuracy,
    which is a bit better than the voting classifier using soft voting, which got
    92%. Depending on your use case, this may or may not be worth the extra complexity
    and computational cost (since there’s an extra model to run after all the others).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在测试集上评估此堆叠模型，您将发现 92.8% 的准确率，这略好于使用软投票的投票分类器，其准确率为 92%。根据您的用例，这可能或可能不值得额外的复杂性和计算成本（因为所有其他模型之后还需要运行一个额外的模型）。
- en: In conclusion, ensemble methods are versatile, powerful, and fairly simple to
    use. They can overfit if you’re not careful, but that’s true of every powerful
    model. [Table 6-1](#ensemble_summary_table) summarizes all the techniques we discussed
    in this chapter, and when to use each one.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，集成方法非常灵活、强大且相对简单易用。如果您不小心，它们可能会过拟合，但这适用于每个强大的模型。[表 6-1](#ensemble_summary_table)
    总结了本章中讨论的所有技术及其使用时机。
- en: Table 6-1\. When to use each ensemble learning method
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. 何时使用每种集成学习方法
- en: '| Ensemble method | When to use it | Example use cases |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 集成方法 | 何时使用 | 示例用例 |'
- en: '| --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Hard voting | Balanced classification dataset with multiple strong but diverse
    classifiers. | Spam detection, sentiment analysis, disease classification |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Hard voting | 具有多个强大且多样化的分类器的平衡分类数据集。 | 垃圾邮件检测、情感分析、疾病分类 |'
- en: '| Soft voting | Classification dataset with probabilistic models, where confidence
    scores matter. | Medical diagnosis, credit risk analysis, fake news detection
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Soft voting | 具有概率模型的分类数据集，其中置信度分数很重要。 | 医疗诊断、信用风险分析、假新闻检测 |'
- en: '| Bagging | Structured or semi-structured dataset with high variance and overfitting-prone
    models. | Financial risk modeling, ecommerce recommendation |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Bagging | 具有高方差和容易过拟合模型的非结构化或半结构化数据集。 | 金融风险评估、电子商务推荐 |'
- en: '| Pasting | Structured or semi-structured dataset where more independent models
    are needed. | Customer segmentation, protein classification |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Pasting | 需要更多独立模型的结构化或半结构化数据集。 | 客户细分、蛋白质分类 |'
- en: '| Random forest | High-dimensional structured datasets with potentially noisy
    features. | Customer churn prediction, genetic data analysis, fraud detection
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Random forest | 具有潜在噪声特征的高维结构化数据集。 | 客户流失预测、遗传数据分析、欺诈检测 |'
- en: '| Extra-trees | Large structured datasets with many features, where speed is
    critical and reducing variance is important. | Real-time fraud detection, sensor
    data analysis |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Extra-trees | 具有众多特征且速度关键、降低方差重要的大型结构化数据集。 | 实时欺诈检测、传感器数据分析 |'
- en: '| AdaBoost | Small to medium-sized, low-noise, structured datasets with weak
    learners (e.g., decision stumps), where interpretability is helpful. | Credit
    scoring, anomaly detection, predictive maintenance |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost | 小型至中型、低噪声、结构化数据集，其中弱学习器（例如，决策树桩）有助于可解释性。 | 信用评分、异常检测、预测性维护 |'
- en: '| Gradient boosting | Medium to large structured datasets where high predictive
    power is required, even at the cost of extra tuning. | Housing price prediction,
    risk assessment, demand forecasting |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升 | 需要高预测能力的中型至大型结构化数据集，即使需要额外的调整。 | 房价预测、风险评估、需求预测 |'
- en: '| Histogram-based gradient boosting (HGB) | Large structured datasets where
    training speed and scalability are key. | Click-through rate prediction, ranking
    algorithms, real-time bidding in advertising |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 基于直方图的梯度提升（HGB） | 训练速度和可扩展性关键的大型结构化数据集。 | 点击率预测、排名算法、广告中的实时竞价 |'
- en: '| Stacking | Complex, high-dimensional datasets where combining multiple diverse
    models can maximize accuracy. | Recommendation engines, autonomous vehicle decision-making,
    Kaggle competitions |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Stacking | 复杂、高维数据集，其中结合多个多样化的模型可以最大化准确性。 | 推荐引擎、自动驾驶车辆决策、Kaggle竞赛 |'
- en: Tip
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Random forests, AdaBoost, GBRT, and HGB are among the first models you should
    test for most machine learning tasks, particularly with heterogeneous tabular
    data. Moreover, as they require very little preprocessing, they’re great for getting
    a prototype up and running quickly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林、AdaBoost、GBRT 和 HGB 是您应该首先测试的模型之一，尤其是在处理异构表格数据时。此外，由于它们需要的预处理很少，它们非常适合快速构建原型。
- en: 'So far, we have looked only at supervised learning techniques. In the next
    chapter, we will turn to the most common unsupervised learning task: dimensionality
    reduction.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了监督学习技术。在下一章中，我们将转向最常见的不监督学习任务：降维。
- en: Exercises
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: If you have trained five different models on the exact same training data, and
    they all achieve 95% precision, is there any chance that you can combine these
    models to get better results? If so, how? If not, why?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你已经在完全相同的训练数据上训练了五个不同的模型，并且它们都达到了 95% 的精确度，你有没有可能将这五个模型结合起来得到更好的结果？如果是这样，如何？如果不是，为什么？
- en: What is the difference between hard and soft voting classifiers?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬投票分类器和软投票分类器之间的区别是什么？
- en: Is it possible to speed up training of a bagging ensemble by distributing it
    across multiple servers? What about pasting ensembles, boosting ensembles, random
    forests, or stacking ensembles?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以通过将 bagging 集成方法分布到多个服务器上来加速其训练？对于粘贴集成、提升集成、随机森林或堆叠集成又如何？
- en: What is the benefit of out-of-bag evaluation?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 出袋评估有什么好处？
- en: What makes extra-trees ensembles more random than regular random forests? How
    can this extra randomness help? Are extra-trees classifiers slower or faster than
    regular random forests?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么使得 extra-trees 集成比常规随机森林更加随机？这种额外的随机性如何帮助？extra-trees 分类器比常规随机森林快还是慢？
- en: If your AdaBoost ensemble underfits the training data, which hyperparameters
    should you tweak, and how?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的 AdaBoost 集成欠拟合了训练数据，你应该调整哪些超参数，以及如何调整？
- en: If your gradient boosting ensemble overfits the training set, should you increase
    or decrease the learning rate?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的梯度提升集成过度拟合了训练集，你应该增加还是减少学习率？
- en: Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter)),
    and split it into a training set, a validation set, and a test set (e.g., use
    50,000 instances for training, 10,000 for validation, and 10,000 for testing).
    Then train various classifiers, such as a random forest classifier, an extra-trees
    classifier, and an SVM classifier. Next, try to combine them into an ensemble
    that outperforms each individual classifier on the validation set, using soft
    or hard voting. Once you have found one, try it on the test set. How much better
    does it perform compared to the individual classifiers?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集（在第 3 章中介绍），并将其分为训练集、验证集和测试集（例如，使用 50,000 个实例进行训练，10,000 个用于验证，10,000
    个用于测试）。然后训练各种分类器，例如随机森林分类器、extra-trees 分类器和 SVM 分类器。接下来，尝试将它们组合成一个集成，在验证集上优于每个单独的分类器，使用软投票或硬投票。一旦找到这样的集成，就在测试集上尝试它。与单独的分类器相比，它的表现提高了多少？
- en: 'Run the individual classifiers from the previous exercise to make predictions
    on the validation set, and create a new training set with the resulting predictions:
    each training instance is a vector containing the set of predictions from all
    your classifiers for an image, and the target is the image’s class. Train a classifier
    on this new training set. Congratulations—you have just trained a blender, and
    together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble
    on the test set. For each image in the test set, make predictions with all your
    classifiers, then feed the predictions to the blender to get the ensemble’s predictions.
    How does it compare to the voting classifier you trained earlier? Now try again
    using a `StackingClassifier` instead. Do you get better performance? If so, why?'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行之前练习中的单个分类器，在验证集上进行预测，并创建一个新的训练集，其中包含由此产生的预测：每个训练实例都是一个向量，包含所有分类器对一个图像的预测集，目标是图像的类别。在这个新的训练集上训练一个分类器。恭喜你——你刚刚训练了一个混合器，它与分类器一起形成了一个堆叠集成！现在在测试集上评估这个集成。对于测试集中的每个图像，使用所有分类器进行预测，然后将预测输入到混合器以获得集成的预测。它与您之前训练的投票分类器相比如何？现在尝试再次使用
    `StackingClassifier`。你得到更好的性能吗？如果是这样，为什么？
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch06.html#id1744-marker)) Imagine picking a card randomly from a deck
    of cards, writing it down, then placing it back in the deck before picking the
    next card: the same card could be sampled multiple times.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#id1744-marker)) 想象一下从一副扑克牌中随机抽取一张牌，写下它，然后将其放回牌堆中，再抽取下一张牌：同一张牌可能会被多次抽取。
- en: '^([2](ch06.html#id1745-marker)) Leo Breiman, “Bagging Predictors”, *Machine
    Learning* 24, no. 2 (1996): 123–140.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch06.html#id1745-marker)) Leo Breiman, “预测器的 Bagging”，*机器学习* 24, 第 2
    期 (1996): 123–140.'
- en: ^([3](ch06.html#id1746-marker)) In statistics, resampling with replacement is
    called *bootstrapping*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#id1746-marker)) 在统计学中，带替换的重新抽样称为 *bootstrapping*。
- en: '^([4](ch06.html#id1747-marker)) Leo Breiman, “Pasting Small Votes for Classification
    in Large Databases and On-Line”, *Machine Learning* 36, no. 1–2 (1999): 85–103.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch06.html#id1747-marker)) Leo Breiman, “在大型数据库和在线中对分类的小投票粘贴”，*机器学习* 36,
    第 1-2 期 (1999): 85–103。'
- en: ^([5](ch06.html#id1749-marker)) Bias and variance were introduced in [Chapter 4](ch04.html#linear_models_chapter).
    Recall that a high bias means that the average prediction is far off target, while
    a high variance means that the predictions are very scattered. We want both to
    be low.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#id1749-marker)) 偏差和方差在 [第 4 章](ch04.html#linear_models_chapter)
    中介绍。回想一下，高偏差意味着平均预测远低于目标，而高方差意味着预测非常分散。我们希望两者都低。
- en: ^([6](ch06.html#id1752-marker)) `max_samples` can alternatively be set to a
    float between 0.0 and 1.0, in which case the max number of sampled instances is
    equal to the size of the training set times `max_samples`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#id1752-marker)) `max_samples` 可以设置为 0.0 到 1.0 之间的浮点数，在这种情况下，最大抽样实例数等于训练集大小乘以
    `max_samples`。
- en: ^([7](ch06.html#id1757-marker)) As *m* grows, this ratio approaches 1 – exp(–1)
    ≈ 63%.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#id1757-marker)) 随着 *m* 的增长，这个比率趋近于 1 – exp(–1) ≈ 63%。
- en: '^([8](ch06.html#id1765-marker)) Gilles Louppe and Pierre Geurts, “Ensembles
    on Random Patches”, *Lecture Notes in Computer Science* 7523 (2012): 346–361.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch06.html#id1765-marker)) Gilles Louppe 和 Pierre Geurts, “基于随机补丁的集成”，*计算机科学讲义*
    7523 (2012): 346–361.'
- en: '^([9](ch06.html#id1767-marker)) Tin Kam Ho, “The Random Subspace Method for
    Constructing Decision Forests”, *IEEE Transactions on Pattern Analysis and Machine
    Intelligence* 20, no. 8 (1998): 832–844.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch06.html#id1767-marker)) Tin Kam Ho, “构建决策森林的随机子空间方法”，*IEEE 交易在模式分析与机器智能*
    20, 第 8 期 (1998): 832–844.'
- en: '^([10](ch06.html#id1771-marker)) Tin Kam Ho, “Random Decision Forests”, *Proceedings
    of the Third International Conference on Document Analysis and Recognition* 1
    (1995): 278.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '^([10](ch06.html#id1771-marker)) Tin Kam Ho, “随机决策森林”，*第三国际文档分析与识别会议论文集* 1
    (1995): 278.'
- en: ^([11](ch06.html#id1772-marker)) The `BaggingClassifier` class remains useful
    if you want a bag of something other than decision trees.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.html#id1772-marker)) 如果你想有一个除了决策树之外的其他东西的“袋”，`BaggingClassifier`
    类仍然是有用的。
- en: '^([12](ch06.html#id1782-marker)) Pierre Geurts et al., “Extremely Randomized
    Trees”, *Machine Learning* 63, no. 1 (2006): 3–42.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch06.html#id1782-marker)) Pierre Geurts 等人，“极端随机树”，*机器学习* 63, 第 1 期
    (2006): 3–42.'
- en: '^([13](ch06.html#id1792-marker)) Yoav Freund and Robert E. Schapire, “A Decision-Theoretic
    Generalization of On-Line Learning and an Application to Boosting”, *Journal of
    Computer and System Sciences* 55, no. 1 (1997): 119–139.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch06.html#id1792-marker)) Yoav Freund 和 Robert E. Schapire, “在线学习的决策理论推广及其在提升中的应用”，*计算机与系统科学杂志*
    55, 第 1 期 (1997): 119–139。'
- en: ^([14](ch06.html#id1793-marker)) This is just for illustrative purposes. SVMs
    are generally not good base predictors for AdaBoost; they are slow and tend to
    be unstable with it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch06.html#id1793-marker)) 这只是为了说明目的。SVMs 通常不是 AdaBoost 的好基预测器；它们速度慢，并且与它一起往往不稳定。
- en: ^([15](ch06.html#id1795-marker)) The original AdaBoost algorithm does not use
    a learning rate hyperparameter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch06.html#id1795-marker)) 原始的 AdaBoost 算法不使用学习率超参数。
- en: '^([16](ch06.html#id1797-marker)) For more details, see Ji Zhu et al., “Multi-Class
    AdaBoost”, *Statistics and Its Interface* 2, no. 3 (2009): 349–360.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '^([16](ch06.html#id1797-marker)) 更多细节，请参阅 Ji Zhu 等人，“多类 AdaBoost”，*统计学及其应用*
    2, 第 3 期 (2009): 349–360.'
- en: '^([17](ch06.html#id1803-marker)) Gradient boosting was first introduced in
    Leo Breiman’s [1997 paper](https://homl.info/arcing) “Arcing the Edge” and was
    further developed in the [1999 paper](https://homl.info/gradboost) “Greedy Function
    Approximation: A Gradient Boosting Machine” by Jerome H. Friedman.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '^([17](ch06.html#id1803-marker)) 梯度提升首先在 Leo Breiman 的 1997 年论文“Arcing the
    Edge”（https://homl.info/arcing）中提出，并在 Jerome H. Friedman 的 1999 年论文“Greedy Function
    Approximation: A Gradient Boosting Machine”（https://homl.info/gradboost）中进一步发展。'
- en: '^([18](ch06.html#id1836-marker)) David H. Wolpert, “Stacked Generalization”,
    *Neural Networks* 5, no. 2 (1992): 241–259.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch06.html#id1836-marker)) 大卫·H·沃尔珀特，“堆叠泛化”，*神经网络* 5, 第2期 (1992): 241–259.'
