- en: 3 Graph convolutional networks and GraphSAGE
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 图卷积网络和 GraphSAGE
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing GraphSAGE and graph convolutional networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 GraphSAGE 和图卷积网络
- en: Applying convolutional graph neural networks to generate product bundles from
    Amazon
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将卷积图神经网络应用于从 Amazon 生成产品捆绑
- en: Key parameters and settings for graph convolutional networks and GraphSAGE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图卷积网络和 GraphSAGE 的关键参数和设置
- en: More theoretical insights, including convolution and message passing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括卷积和消息传递在内的更多理论见解
- en: In the first two chapters of this book, we explored fundamental concepts related
    to graphs and graph representation learning. All of this served to set us up for
    part 2, where we’ll explore distinct types of graph neural network (GNN) architectures,
    including convolutional GNNs, graph attention networks (GATs), and graph autoencoders
    (GAEs).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前两章中，我们探讨了与图和图表示学习相关的根本概念。所有这些都为我们设置了第二部分的基础，我们将探索不同类型的图神经网络（GNN）架构，包括卷积
    GNNs、图注意力网络（GATs）和图自动编码器（GAEs）。
- en: In this chapter, our goal is to understand and apply graph convolutional networks
    (GCNs) and GraphSAGE [1, 2]. These two architectures are part of a larger class
    of GNNs that approach deep learning by applying convolutions to graph data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是理解和应用图卷积网络（GCNs）和 GraphSAGE [1, 2]。这两个架构是更大类别的 GNNs 的一部分，通过在图数据上应用卷积来接近深度学习。
- en: Convolutional operations are relatively common in deep learning models, particularly
    for image-based tasks that rely heavily on convolutional neural networks (CNNs).
    To learn more about CNNs and their application to computer vision, we recommend
    checking out *Deep Learning with Python* (Manning, 2024) or *Deep Learning with
    PyTorch* (Manning, 2023).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作在深度学习模型中相对常见，尤其是在依赖于卷积神经网络（CNNs）的图像任务中。要了解更多关于 CNNs 以及其在计算机视觉中的应用，我们建议查阅
    *使用 Python 进行深度学习*（Manning, 2024）或 *使用 PyTorch 进行深度学习*（Manning, 2023）。
- en: We provide a short primer on convolutions later in the chapter, but essentially
    convolutional operations can be understood as performing a spatial or local averaging
    across entities. For example, in images, CNN layers form representations at incrementally
    larger pixel subdomains. For GCNs, we’ll apply the same idea of a local averaging,
    but with neighborhoods of nodes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面提供关于卷积的简要介绍，但基本上，卷积操作可以理解为在实体上执行空间或局部平均。例如，在图像中，CNN 层在递增更大的像素子域中形成表示。对于
    GCNs，我们将应用相同的局部平均概念，但使用节点的邻域。
- en: 'In this chapter, you’ll learn how to apply convolutional GNNs to a node prediction
    problem, key parameters and settings for GCN and GraphSAGE, ways to optimize performance
    for convolutional GNNs, and relevant theoretical topics, including graph convolution
    and message passing. Additionally, we’ll explore the Amazon Products dataset.
    This chapter is structured as follows: first, we jump into the product category
    prediction problem and create baseline models (section 3.1); then we adjust our
    models using neighborhood aggregation (section 3.2); next, we optimize our models
    using general deep learning methods (section 3.3); following that, we explain
    relevant theory in more detail (section 3.4); and finally, we dig deeper into
    the Amazon Products dataset used in this chapter and later in the book (section
    3.5).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何将卷积 GNNs 应用于节点预测问题，GCN 和 GraphSAGE 的关键参数和设置，以及优化卷积 GNNs 性能的方法，以及相关的理论主题，包括图卷积和消息传递。此外，我们将探索
    Amazon 产品数据集。本章的结构如下：首先，我们跳转到产品类别预测问题并创建基线模型（第 3.1 节）；然后，我们使用邻域聚合调整我们的模型（第 3.2
    节）；接下来，我们使用通用深度学习方法优化我们的模型（第 3.3 节）；随后，我们更详细地解释相关理论（第 3.4 节）；最后，我们深入探讨本章以及书中后续章节使用的
    Amazon 产品数据集（第 3.5 节）。
- en: This chapter is designed to immerse you immediately in the application of convolutional
    GNNs, equipping you with the essential knowledge needed to deploy these models
    effectively. The initial sections provide you with the minimum toolkit for a functioning
    understanding of convolutional GNNs in practice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在让您立即沉浸在卷积图神经网络（convolutional GNNs）的应用中，为您提供部署这些模型所需的基本知识。最初的部分为您提供了在实践中对卷积
    GNNs 有功能理解的最小工具集。
- en: However, when facing challenging modeling problems, deeper comprehension becomes
    invaluable. The latter sections of the chapter cover underlying principles of
    the layers, settings, and parameters introduced earlier. They are crafted to enhance
    your conceptual grasp, ensuring that your practical skills are complemented by
    a thorough theoretical understanding. This holistic approach aims to not only
    enable you to apply GNNs but to innovate and adapt them to the nuanced demands
    of real-world problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，面对具有挑战性的建模问题，更深入的理解变得极为宝贵。本章的后半部分涵盖了之前引入的层、设置和参数的底层原理。它们旨在增强你的概念理解，确保你的实践技能与彻底的理论理解相辅相成。这种整体方法旨在不仅使你能够应用GNN，而且能够创新和适应现实世界问题的细微需求。
- en: Note  While *GraphSAGE* refers to a specific individual architecture, it may
    be confusing that *GCN* also refers to a specific architecture and not the entire
    class of GNNs based on convolutions. So, in this chapter, we’ll use *convolutional
    GNNs* to refer to this entire class of GNNs, which include GraphSAGE and GCN.
    We’ll use *GCN* to refer to the individual architecture introduced by Thomas Kipf
    and Max Welling [1].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然*GraphSAGE*指的是一个特定的个体架构，但可能会令人困惑的是*GCN*也指代一个特定的架构，而不是基于卷积的整个GNN类。因此，在本章中，我们将使用*卷积GNN*来指代这一整个GNN类，包括GraphSAGE和GCN。我们将使用*GCN*来指代由Thomas
    Kipf和Max Welling [1]引入的个体架构。
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/wJMW](https://mng.bz/wJMW)). Colab links and data from this chapter
    can be accessed in the same locations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的代码以笔记本形式存储在GitHub仓库中（[https://mng.bz/wJMW](https://mng.bz/wJMW)）。本章的Colab链接和数据可以在相同的位置访问。
- en: 3.1 Predicting consumer product categories
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 预测消费者产品类别
- en: Let’s start our exploration of convolutional GNNs with a product management
    problem using the Amazon Products dataset (see table 3.1). Imagine you’re a product
    manager aiming to enhance sales by identifying and promoting emerging trends in
    product bundles. You have a dataset derived from Amazon’s product co-purchasing
    network, containing a rich set of relationships between products based on customer
    buying behavior. Your task is to use insights about product categories and co-purchasing
    patterns to uncover hidden and appealing product bundles that resonate with your
    customers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以使用Amazon Products数据集（见表3.1）的产品管理问题开始我们对卷积GNN的探索。想象一下，你是一位产品经理，旨在通过识别和推广产品捆绑中的新兴趋势来提高销售额。你有一个来自亚马逊产品共购买网络的数据库，其中包含基于客户购买行为的丰富产品关系。你的任务是利用对产品类别和共购买模式的认识，揭示与客户产生共鸣的隐藏且吸引人的产品捆绑。
- en: Table 3.1 Overview of the Amazon Products dataset
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1 Amazon Products数据集概述
- en: '| Amazon co-purchases organized by product category |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 按产品类别组织的亚马逊共购买 |  |'
- en: '| --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Number of nodes (products)  | ~2,500,000  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 节点数量（产品） | ~2,500,000 |'
- en: '| Node features  | 100  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 节点特征 | 100 |'
- en: '| Node categories  | 47  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 节点类别 | 47 |'
- en: '| Total number of edges  | ~61,900,000  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 边的总数 | ~61,900,000 |'
- en: 'To tackle this, we introduce GCNs and GraphSAGE—two convolutional GNN architectures.
    This section will guide you through training these models on the Amazon Products
    dataset. We’ll focus on two tasks: identifying a product’s category and finding
    sets of product bundles by analyzing the similarity between product embeddings
    produced by the trained models.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一问题，我们引入了GCN和GraphSAGE——两种卷积GNN架构。本节将指导你如何在Amazon Products数据集上训练这些模型。我们将关注两个任务：通过分析训练模型产生的产品嵌入之间的相似性来识别产品的类别和找到产品捆绑的集合。
- en: Note  If you want to get deeper into the theory behind GCN and GraphSAGE, see
    section 3.4\. For details about the Amazon Products dataset, see section 3.5.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您想深入了解GCN和GraphSAGE背后的理论，请参阅第3.4节。有关Amazon Products数据集的详细信息，请参阅第3.5节。
- en: 'Following our model training process, in this section, we’ll do the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型训练过程之后，在本节中，我们将执行以下操作：
- en: '*Preprocess our dataset*—We’ll take the Amazon Products dataset and reduce
    its size to work with systems with minimal resources.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理我们的数据集*——我们将使用Amazon Products数据集并将其大小缩减，以便在资源最少的情况下工作。'
- en: '*Construct our model classes*—We’ll focus on two convolutional GNNs: GCN and
    GraphSAGE. We’ll initially create model classes and instantiate them with default
    parameters.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建我们的模型类*——我们将关注两种卷积GNN：GCN和GraphSAGE。我们最初将创建模型类，并用默认参数实例化它们。'
- en: '*Code our training and validation loops*—We’ll train the models with a validation
    step for each epoch. To compare the two models, we’ll train them simultaneously
    with the same batches.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编写训练和验证循环的代码*—我们将为每个epoch设置一个验证步骤来训练模型。为了比较两个模型，我们将使用相同的批次同时训练它们。'
- en: '*Assess model performance*—We’ll take a look at training curves. Then, we’ll
    use traditional classification metrics and observe the ability of the model to
    predict particular categories.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估模型性能*—我们将查看训练曲线。然后，我们将使用传统的分类指标来观察模型预测特定类别的能力。'
- en: Our immediate goal is to develop first passes of our trained models. So, at
    this point, the emphasis isn’t on performance optimization but on covering the
    essential steps to get a baseline model working. Subsequent sections will refine
    these approaches, enhancing performance and efficiency.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的目标是开发我们训练模型的初步版本。因此，在这个阶段，重点不在于性能优化，而在于覆盖使基线模型工作的基本步骤。随后的章节将细化这些方法，提高性能和效率。
- en: 3.1.1 Loading and processing the data
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 加载数据和处理
- en: We start by downloading the Amazon Products dataset from the Open Graph Benchmark
    (OGB) site ([https://ogb.stanford.edu/](https://ogb.stanford.edu/)). This dataset
    is large for a single machine, taking up 1.3 GB. This includes 2.5 million nodes
    (products) and 61.9 million edges (co-purchases).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从开放图基准（OGB）网站（[https://ogb.stanford.edu/](https://ogb.stanford.edu/)）下载亚马逊产品数据集。这个数据集对于一个单机来说很大，占用了1.3
    GB的空间。它包括250万个节点（产品）和6190万个边（共同购买）。
- en: To make working with this data manageable for systems with smaller memory capacity
    and less powerful processors, we’ll reduce its size. We simply take the nodes
    that have the first 10,000 node indices in the original graph and create a subgraph
    based on those. Depending on your problem, there are other strategies to create
    subgraphs. In chapter 8, we look at creating subgraphs in more depth.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使具有较小内存容量和较不强大的处理器的系统能够处理这些数据，我们将减小其大小。我们简单地取原始图中前10,000个节点索引的节点，并基于这些节点创建一个子图。根据你的问题，还有其他创建子图的战略。在第8章中，我们将更深入地探讨创建子图的方法。
- en: In creating a subset graph, there is often bookkeeping that must be done to
    ensure our node subset has a consistent and logical ordering and is connected
    to the correct labels and features. We must also filter out edges that are connected
    to nodes from outside the subset. Lastly, we want to make sure we can call back
    the original indices of the subset in case we want to call back useful information;
    for example, for the Amazon Products dataset, we can access the SKU (Amazon Standard
    Identification Number, ASIN) numbers and product categories of each node using
    their original indices.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建子图时，通常需要进行一些记录工作，以确保我们的节点子集具有一致性和逻辑顺序，并且与正确的标签和特征连接。我们还必须过滤掉连接到子集外节点的边。最后，我们还想确保在需要回溯有用信息时可以回溯子集的原始索引；例如，对于亚马逊产品数据集，我们可以使用原始索引访问每个节点的SKU（亚马逊标准识别号，ASIN）编号和产品类别。
- en: 'So, we relabel the nodes with a consistent ordering. Then, we reassign the
    respective node features and labels to correspond to the new indices. Even though
    we choose nodes with the first 10,000 indices, this may not be so in any particular
    case. Here’s how we’ll refine and prepare the data for modeling in four steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们以一致的顺序重新标记节点。然后，我们将相应的节点特征和标签重新分配以对应新的索引。尽管我们选择了前10,000个索引的节点，但这在任何特定情况下可能并不适用。以下是我们将如何通过四个步骤精炼和准备数据以进行建模的方法：
- en: '*Initialize the subset graph *—We create a new graph object that will store
    our subset of data. This graph will hold the edges, features, and labels of the
    nodes that have indices 0–9,999 in our original graph.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始化子图*—我们创建一个新的图对象，将存储我们的数据子集。这个图将包含原始图中索引为0–9,999的节点的边、特征和标签。'
- en: '*Relabel node indices *—To ensure consistency and avoid index mismatches, we
    relabel the node indices within our subset graph. This relabeling is crucial because
    operations within GNNs depend heavily on indexing to process node and edge information.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重新标记节点索引*—为了确保一致性和避免索引不匹配，我们在子图内部重新标记节点索引。这种重新标记至关重要，因为GNN中的操作在很大程度上依赖于索引来处理节点和边信息。'
- en: '*Feature and label assignment *—We assign node features (x) and labels (y)
    to our new graph object. These features and labels are sliced from the original
    dataset, corresponding to our specified subset indices.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*特征和标签分配*—我们将节点特征（x）和标签（y）分配给我们的新图对象。这些特征和标签是从原始数据集中切分出来的，对应于我们指定的子集索引。'
- en: '*Edge mask utilization *—The `return_edge_mask` option, used during the subgraph
    extraction, lets us identify which edges were selected during the subgraph creation.
    This is useful for tracing back to the original graph’s structure or for any structural
    analysis required later.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*边缘掩码利用* —在子图提取期间使用的 `return_edge_mask` 选项使我们能够识别在子图创建过程中选择的哪些边。这对于追踪到原始图的结构或进行后续所需的任何结构分析都很有用。'
- en: By restructuring the data in this manner, we not only make it manageable but
    also tailor it specifically for efficient processing in subsequent graph-based
    learning tasks. This setup is foundational as we proceed to construct and evaluate
    our GNN models in the following sections. The following listing shows the code
    for implementing that process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式重新组织数据，我们不仅使其易于管理，而且专门针对后续基于图的机器学习任务的高效处理进行了定制。这种设置是我们接下来在以下章节中构建和评估我们的
    GNN 模型的基础。以下列表显示了实现该过程的代码。
- en: Listing 3.1 Reading in data and creating a subgraph
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.1 读取数据并创建子图
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Loads dataset from the specified root directory and specifies ogbn-products
    to indicate which dataset is being loaded'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从指定的根目录加载数据集，并指定 ogbn-products 以指示正在加载哪个数据集'
- en: '#2 The first graph object from the dataset is selected for processing.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从数据集中选择第一个图对象进行处理。'
- en: '#3 Creates an array of indices for the first 10,000 nodes, which defines our
    subset for the experiment'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建一个包含前 10,000 个节点索引的数组，这定义了我们的实验子集'
- en: '#4 Calls the subgraph function with subset_indices to extract edges and attributes
    relevant to these indices. The nodes are relabeled to maintain a consistent zero-based
    index in the new graph.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 subset_indices 调用子图函数以提取与这些索引相关的边和属性。节点被重新标记以保持在新图中的零基索引的一致性。'
- en: '#5 Indexes node features from the original data according to subset_indices
    to ensure that only relevant features are transferred to the new graph'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 根据 subset_indices 从原始数据中索引节点特征，以确保只将相关特征转移到新图中'
- en: '#6 Similarly, indexes node labels to maintain correspondence with the subset
    features'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 类似地，索引节点标签以保持与子集特征的一致性'
- en: '#7 Creates a new instance of the data class to store our subset graph'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 创建一个新的数据类实例以存储我们的子集图'
- en: '#8 Assigns the edge index array created during the subgraph extraction to the
    new graph'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 将在子图提取期间创建的边缘索引数组分配给新图'
- en: '#9 Assigns subset features to the new graph’s node feature matrix'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 将子集特征分配给新图的节点特征矩阵'
- en: '#10 Assigns node labels corresponding to the subset to the new graph'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 将子集对应的节点标签分配给新图'
- en: 3.1.2 Creating our model classes
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 创建我们的模型类
- en: 'After setting up our dataset and preparing a manageable subgraph, we transition
    to the core of our graph machine learning pipeline: defining the models. In this
    section, we focus on two popular types of GNNs provided by the PyTorch Geometric
    (PyG) library: GCN and GraphSAGE.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置我们的数据集并准备一个可管理的子图后，我们转向图机器学习管道的核心：定义模型。在本节中，我们关注 PyTorch Geometric (PyG)
    库提供的两种流行的 GNN 类型：GCN 和 GraphSAGE。
- en: Understanding our model architectures
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解我们的模型架构
- en: PyG simplifies the construction of GNNs through modular layer objects, each
    encapsulating a specific type of graph convolution. These layers can be stacked
    and integrated with other PyTorch modules to build complex architectures tailored
    to various graph-based tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PyG 通过模块化层对象简化了 GNN 的构建，每个层封装了一种特定的图卷积类型。这些层可以堆叠并与其他 PyTorch 模块集成，以构建针对各种基于图的任务的复杂架构。
- en: GCN model
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GCN 模型
- en: The GCN model uses the `GCNConv` layer, which implements the graph convolution
    operation as described by Kipf and Welling in their seminal paper [1]. It takes
    advantage of the spectral properties of graphs to facilitate information flow
    between nodes, allowing the model to learn representations that embed both local
    graph structure and node features.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 模型使用 `GCNConv` 层，该层实现了 Kipf 和 Welling 在其开创性论文 [1] 中描述的图卷积操作。它利用图的谱性质来促进节点之间的信息流动，使模型能够学习嵌入局部图结构和节点特征的表示。
- en: In listing 3.2, the GCN class sets up a two-layer model. Each layer is represented
    by the `GCNConv` module, which processes graph data by applying a convolution
    operation that directly uses the graph’s structure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 3.2 中，GCN 类设置了一个两层模型。每一层都由 `GCNConv` 模块表示，该模块通过应用直接使用图结构的卷积操作来处理图数据。
- en: To summarize its workings, from an input set of node features and the graph
    structure (`edge_index`), the network will update node features by aggregating
    the neighborhood information from each respective node. After the first layer,
    we apply a rectified linear unit (ReLU) activation function, which adds nonlinearity
    to the model. The second layer refines these features further.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结其工作原理，从输入的节点特征集和图结构（`edge_index`），网络将通过聚合每个相应节点的邻域信息来更新节点特征。在第一层之后，我们应用一个修正线性单元（ReLU）激活函数，这为模型添加了非线性。第二层进一步细化这些特征。
- en: If we want to look at the node embeddings directly—for instance, to visualize
    them or to use them in some other analysis—we can just return them right after
    the second layer. Otherwise, we apply another activation function—in this case,
    a softmax function—to normalize the outputs for our classification problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想直接查看节点嵌入——例如，为了可视化或用于其他分析——我们可以在第二层之后直接返回它们。否则，我们应用另一个激活函数——在这种情况下，softmax函数——以对分类问题的输出进行归一化。
- en: Listing 3.2 GCN class
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 GCN类
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Initializes the first graph convolution layer that transforms input features
    (in_channels) into hidden features (hidden_channels)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化第一个图卷积层，将输入特征（in_channels）转换为隐藏特征（hidden_channels）'
- en: '#2 Forward method that dictates how data flows through the model from input
    to output'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 前向方法，规定了数据从输入到输出的流动方式'
- en: '#3 Applies the ReLU activation function after the first convolution to add
    nonlinearity to the model'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在第一次卷积后应用ReLU激活函数，以向模型添加非线性'
- en: '#4 Optionally returns the raw embeddings from the network, which can be useful
    for tasks that require raw node representations without classification, such as
    visualization or further processing'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 可选地返回网络的原始嵌入，这对于需要原始节点表示而不进行分类的任务很有用，例如可视化或进一步处理'
- en: '#5 Applies a log softmax activation to the final layer’s output'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 对最终层的输出应用log softmax激活函数'
- en: GraphSAGE model
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GraphSAGE模型
- en: Much like the GCN model, the GraphSAGE model class in our code also sets up
    a two-layer network, but with the `SAGEConv` layers. While structurally similar
    in code, GraphSAGE is a significant shift from GCN in theory. Unlike GCN’s full
    reliance on the entire graph’s adjacency matrix, GraphSAGE is designed to learn
    from randomly sampled neighborhood data, making it particularly well-suited for
    large graphs. This sampling approach allows GraphSAGE to scale effectively by
    focusing on localized regions of the graph.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与GCN模型类似，我们代码中的GraphSAGE模型类也设置了一个两层网络，但使用的是`SAGEConv`层。虽然在代码结构上相似，但GraphSAGE在理论上对GCN是一个重大的转变。与GCN完全依赖整个图的邻接矩阵不同，GraphSAGE旨在从随机采样的邻域数据中学习，这使得它特别适合大型图。这种采样方法允许GraphSAGE通过关注图的局部区域来有效地进行扩展。
- en: GraphSAGE uses the `SAGEConv` layer, which supports various aggregation functions—mean,
    pool, and long short-term memory (LSTM)—offering flexibility in how node features
    are aggregated. After each `SAGEConv` layer, similar to the GCN model, a nonlinearity
    is applied. If node embeddings are required directly for tasks such as visualization
    or further analysis, they can be returned immediately following the second layer.
    Otherwise, a softmax function is applied to normalize the outputs for classification
    tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE使用`SAGEConv`层，该层支持各种聚合函数——平均、池化和长短期记忆（LSTM），在节点特征聚合方面提供了灵活性。在每个`SAGEConv`层之后，类似于GCN模型，应用一个非线性函数。如果需要直接将节点嵌入用于任务，如可视化或进一步分析，它们可以在第二层之后立即返回。否则，应用softmax函数对输出进行归一化，以用于分类任务。
- en: The key difference in PyG’s implementation of these models lies in their efficiency
    and scalability with large datasets. Both models learn node representations, but
    GraphSAGE provides a significant advantage for practical applications involving
    very large graphs. Unlike GCN, which can operate on sparse data representations
    but still processes information from the entire graph structure, GraphSAGE doesn’t
    require the entire adjacency matrix. Instead, it samples local neighborhoods,
    which allows it to handle vast networks efficiently without overwhelming memory
    resources by having to load the entire graph representation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PyG对这些模型实现的显著差异在于它们的效率和在大数据集上的可扩展性。这两个模型都学习节点表示，但GraphSAGE在涉及非常大型图的实用应用中提供了显著的优点。与GCN不同，GCN可以在稀疏数据表示上操作，但仍然处理整个图结构的信息，GraphSAGE不需要整个邻接矩阵。相反，它采样局部邻域，这使得它能够有效地处理大型网络，而不会因为需要加载整个图表示而耗尽内存资源。
- en: Listing 3.3 GraphSAGE class
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3 GraphSAGE类
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Initializes the first graph convolution layer that transforms input features
    (in_channels) into hidden features (hidden_channels)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化第一个图卷积层，该层将输入特征（in_channels）转换为隐藏特征（hidden_channels）'
- en: '#2 Forward method that dictates how data flows through the model from input
    to output'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 前向方法，它决定了数据从输入到输出的流动方式'
- en: '#3 Applies the ReLU activation function after the first convolution to add
    nonlinearity to the model'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在第一次卷积后应用ReLU激活函数以向模型添加非线性'
- en: '#4 Optionally returns the raw embeddings from the network, which can be useful
    for tasks that require raw node representations without classification, such as
    visualization or further processing'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 可选地返回网络中的原始嵌入，这对于需要原始节点表示而不进行分类的任务很有用，例如可视化或进一步处理'
- en: '#5 Applies a log softmax activation to the final layer’s output'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将log softmax激活函数应用于最终层的输出'
- en: Integration and customization
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成和定制
- en: While we use default settings in this introductory example, both models are
    highly customizable. Parameters such as the number of layers, hidden dimensions,
    and types of aggregation functions (for GraphSAGE) can be adjusted to optimize
    performance for specific datasets or tasks. Next, we’ll train these models on
    our subset graph and evaluate their performance to demonstrate their practical
    applications and effectiveness.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个入门示例中我们使用默认设置，但这两个模型都是高度可定制的。可以调整诸如层数、隐藏维度和聚合函数类型（对于GraphSAGE）等参数，以优化特定数据集或任务的性能。接下来，我们将在我们的子图上训练这些模型并评估它们的性能，以展示它们的实际应用和有效性。
- en: 3.1.3 Model training
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 模型训练
- en: 'With our data ready and our models set up, let’s get into the training process.
    Training is relatively straightforward, as it follows typical machine learning
    routines but applied to graph data. We’ll be training two models simultaneously—GCN
    and GraphSAGE—by feeding them the same data each epoch. This parallel training
    allows us to directly compare the performance and efficiency of these two model
    types under identical conditions. Here’s a concise breakdown of the training loop:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪，模型设置完毕，让我们进入训练过程。训练相对简单，因为它遵循典型的机器学习流程，但应用于图数据。我们将通过在每个epoch向它们提供相同的数据来同时训练两个模型—GCN和GraphSAGE。这种并行训练使我们能够在相同条件下直接比较这两种模型类型的性能和效率。以下是训练循环的简要概述：
- en: '*Initialize optimizers*—Set up Adam optimizers with a learning rate of 0.01\.
    This helps us fine-tune the model weights effectively during training.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*初始化优化器*—设置学习率为0.01的Adam优化器。这有助于我们在训练期间有效地微调模型权重。'
- en: '*Training and validation loops*—For each epoch, run the training function,
    which processes the data through the model to compute losses and update weights.
    Concurrently, validate the model on unseen data to monitor overfitting and adjust
    training strategies accordingly.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练和验证循环*—对于每个epoch，运行训练函数，该函数通过模型处理数据以计算损失并更新权重。同时，在未见过的数据上验证模型，以监控过拟合并相应地调整训练策略。'
- en: '*Track progress*—Record losses for both training and validation phases to visualize
    the learning curve and adjust parameters if needed.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跟踪进度*—记录训练和验证阶段的损失，以便可视化学习曲线并在需要时调整参数。'
- en: '*Conclude with testing*—After training, the models are evaluated on a separate
    test set to gauge their generalization capabilities.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以测试结束*—在训练后，模型将在单独的测试集上评估，以衡量它们的泛化能力。'
- en: By maintaining a consistent training regimen for both models, we ensure that
    any differences in performance can be attributed to the models’ architectural
    differences rather than varied training conditions. The following listing contains
    the annotated code of our training logic.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为两个模型保持一致的训练计划，我们确保任何性能差异都可以归因于模型架构的差异，而不是不同的训练条件。以下列表包含我们训练逻辑的注释代码。
- en: Listing 3.4 Training loop
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4 训练循环
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Initializes models for the GCN and GraphSAGE'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化GCN和GraphSAGE模型'
- en: '#2 Initializes models for the GCN and GraphSAGE'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始化GCN和GraphSAGE模型'
- en: '#3 Sets up optimizers for the GCN and GraphSAGE models'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为GCN和GraphSAGE模型设置优化器'
- en: '#4 Sets up optimizers for the GCN and GraphSAGE models'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为GCN和GraphSAGE模型设置优化器'
- en: '#5 Initializes the cross-entropy loss function for the classification task'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 为分类任务初始化交叉熵损失函数'
- en: '#6 Train functions used every epoch'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 每个epoch使用的训练函数'
- en: '#7 Validation functions used every epoch'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 每个epoch使用的验证函数'
- en: '#8 Sets up arrays to capture losses for each model'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 设置数组以捕获每个模型的损失'
- en: '#9 Training and validation loop'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 训练和验证循环'
- en: Now that we’ve set up and trained our models, it’s time to see how well they
    perform. The next section will look at the training and validation loss curves
    to understand how the models learned over time. It will check out key metrics
    such as accuracy, precision, recall, and F1 scores to evaluate how well our models
    can predict product categories based on our graph data. All of this is to understand
    our models and to figure out where we can improve them in later sections.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了模型并进行了训练，是时候看看它们的性能如何了。下一节将分析训练和验证损失曲线，以了解模型随时间的学习情况。它将检查关键指标，如准确率、精确率、召回率和
    F1 分数，以评估我们的模型根据我们的图数据预测产品类别的能力。所有这些都是为了理解我们的模型，并在后面的章节中找出我们可以改进的地方。
- en: 3.1.4 Model performance analysis
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 模型性能分析
- en: For the next section, we’ll look at the model performance of the GCN and GraphSAGE
    models. We’ll first examine the training curves and point out that we have to
    improve the overfitting in a subsequent chapter. Then, we’ll look at the F1 and
    log loss scores, followed by examining accuracy for the product categories.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看 GCN 和 GraphSAGE 模型的模型性能。我们首先检查训练曲线，并指出我们将在下一章中改进过拟合问题。然后，我们将查看 F1
    分数和对数损失分数，接着检查产品类别的准确率。
- en: Training curves
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练曲线
- en: During the training process, we saved the losses for each model for every epoch.
    *Loss* is a measure of how well our model is able to make correct predictions,
    with lower values being better.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们为每个模型在每个时代保存了损失。*损失*是衡量我们的模型能够做出正确预测的能力的指标，数值越低越好。
- en: Using `Matplotlib`, we use this data to plot training loss and validation loss
    curves, shown in figure 3.1\. Such curves track the performance of the model on
    training and validation datasets over the course of the training process. Ideally,
    both losses should decline over time. However, in our curves, we see a divergence
    beginning near epoch 20\. The validation loss curves reach a nadir and then begin
    to climb. Meanwhile, the training loss continues to decline. Our models’ performance
    continues to improve on the training data but degrades on the validation data
    past some optimal point. This is the classic overfitting problem, which we’ll
    address later in the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Matplotlib`，我们使用这些数据绘制训练损失和验证损失曲线，如图 3.1 所示。此类曲线跟踪模型在训练过程中在训练和验证数据集上的性能。理想情况下，两个损失都应随时间下降。然而，在我们的曲线中，我们看到从第
    20 个时代开始出现偏差。验证损失曲线达到最低点后开始上升。同时，训练损失继续下降。我们的模型在训练数据上的性能继续提高，但在某个最佳点之后在验证数据上的性能下降。这是经典的过拟合问题，我们将在本章后面解决。
- en: '![figure](../Images/3-1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-1.png)'
- en: Figure 3.1 Training and validation loss curves for the GCN model (left) and
    GraphSAGE model (right) trained in this section. The divergence of the validation
    from the training curve signals over-fitting, where the model learns the training
    data too well and at the expense of generalizing to new data.
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.1 本节训练的 GCN 模型（左）和 GraphSAGE 模型（右）的训练和验证损失曲线。验证损失与训练曲线的偏差表明过拟合，即模型在训练数据上学习得太好，而以泛化到新数据为代价。
- en: 'In our training process, we’ve saved the instance of the model with the best
    performance, which is the instance with the lowest validation loss. Next, we look
    at two classification metrics to assess performance: log loss and F1 score.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练过程中，我们保存了表现最佳模型的实例，即具有最低验证损失的那个实例。接下来，我们查看两个分类指标以评估性能：对数损失和 F1 分数。
- en: 'Classification performance: F1 and log loss'
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类性能：F1 分数和对数损失
- en: Given the overfitting problems shown earlier, we turn to the classification
    performance of our models to establish a baseline for our improvement efforts.
    We use our validation sets to establish F1 and log loss scores, shown in table
    3.2\. (The F1 score is weighted, which measures F1 for each class separately,
    then averages them, weighing each class by its proportion of the total data.)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于前面显示的过拟合问题，我们转向我们的模型分类性能，以建立改进努力的基准。我们使用验证集来建立 F1 分数和对数损失分数，如表 3.2 所示。（F1
    分数是加权的，它分别衡量每个类的 F1 分数，然后取平均值，每个类按其在总数据中的比例进行加权。）
- en: The middling scores indicate that the models have much room for improvement.
    Our F1 scores don’t exceed 80%, while the log loss scores are no lower than 1.25\.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 中等的分数表明模型有很大的改进空间。我们的 F1 分数没有超过 80%，而对数损失分数不低于 1.25。
- en: Table 3.2 Classification performance of our models, by F1 score and log loss
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.2 模型按 F1 分数和对数损失的分类性能
- en: '|  | F1 Score | Log Loss |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | F1 分数 | 对数损失 |'
- en: '| --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GCN  | 0.781  | 1.25  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GCN  | 0.781  | 1.25  |'
- en: '| GraphSAGE  | 0.733  | 1.88  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE  | 0.733  | 1.88  |'
- en: In this case, GCN performs better for both metrics. To improve these scores
    for a multiclass problem, we could look more deeply at the model’s capability
    to predict individual classes and examine its performance for imbalanced classes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，GCN在两个指标上都表现得更好。为了提高多类问题的这些分数，我们可以更深入地研究模型预测单个类别的能力，并检查其在不平衡类别上的表现。
- en: Model performance at a class level
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型在类别层面的性能
- en: The Amazon Products dataset comes with two useful files that map each node with
    its class, and each node with its individual Amazon product number (ASIN). To
    evaluate the performance of our baseline models by class, we take the node class
    information and create a table, as shown in figure 3.2, summarizing prediction
    accuracy for the 25 classes containing the most items.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊产品数据集附带两个有用的文件，分别将每个节点与其类别以及每个节点与其单独的亚马逊产品编号（ASIN）映射。为了按类别评估我们基线模型的表现，我们取节点类别信息并创建一个表格，如图3.2所示，总结包含最多项目的25个类别的预测准确率。
- en: 'Along with accuracy, in this table, we examine the biggest mispredictions for
    each class. From this information, let’s make some high-level observations:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率之外，在这个表格中，我们还检查了每个类别的最大误判。从这个信息中，让我们做一些高级观察：
- en: '*Performance by category*—Both models show variability in their prediction
    accuracy across different product categories. The Books category and CDs & Vinyl
    category have high accuracy rates. This can be due to their relatively high number
    of samples. It could also indicate that these categories are more distinct or
    well-defined, making it easier for the model to distinguish them. The first factor,
    number of samples, is easy to adjust because we’re using 10,000 product nodes
    and can draw millions more from our dataset. You can give it a try by adjusting
    the size of the subset in the provided code.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*按类别性能*—两个模型在不同产品类别上的预测准确率都有所变化。书籍类别和CD与黑胶类别具有很高的准确率。这可能是由于它们的相对样本数量较多。这也可能表明这些类别更加独特或定义良好，使得模型更容易区分它们。第一个因素，样本数量，很容易调整，因为我们使用了10,000个产品节点，并可以从我们的数据集中提取数百万个。您可以通过调整提供的代码中子集的大小来尝试一下。'
- en: To improve less distinctive classes, we need to make a deeper exploration of
    the node features to determine how distinctive the classes are relative to each
    other and to brainstorm ways to enhance those features to bring out their novelty.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进不那么独特的类别，我们需要更深入地探索节点特征，以确定这些类别相对于彼此的独特性，并头脑风暴如何增强这些特征以突出其新颖性。
- en: '*Performance by model*—Looking over all the classes, GraphSAGE generally appears
    to perform better than GCN in most categories, as seen from the higher percentages
    of correct predictions. This suggests that GraphSAGE’s approach of aggregating
    features from a node’s neighborhood might be more effective for this dataset.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*按模型性能*—查看所有类别后，GraphSAGE在大多数类别中似乎比GCN表现得更好，正如更高的正确预测百分比所示。这表明GraphSAGE从节点的邻域聚合特征的方法可能更适用于这个数据集。'
- en: '*Misclassifications*—Common misclassifications tend to occur between categories
    that might share similar characteristics or be frequently purchased together.
    For example, the misclassification between Books and Movies & TV or between Electronics
    and Cell Phones & Accessories suggests that items in these categories may share
    overlapping features or are often bought by similar customer segments.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*误分类*—常见的误分类往往发生在可能具有相似特征或经常一起购买的类别之间。例如，书籍与电影和电视或电子产品与手机及配件之间的误分类表明，这些类别中的项目可能具有重叠的特征，或者经常被相似的客户群体购买。'
- en: '![figure](../Images/3-2.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-2.png)'
- en: Figure 3.2 Classification performance (accuracy) by product category, comparing
    GCN and GraphSAGE
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 按产品类别分类性能（准确率），比较GCN和GraphSAGE
- en: Though we generally don’t want misclassifications, observing the classes most
    likely to be misconstrued for another could inform us about common customer perceptions
    or confusions between product categories, highlighting potential areas for marketing
    and product placement strategies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们通常不希望出现误分类，但观察最有可能被误认为是另一个类别的类别，可能会告诉我们关于常见客户对产品类别的感知或混淆的信息，突出潜在的市场营销和产品定位策略。
- en: The next two sections will improve the models’ performance from these baseline
    results by taking advantage of the properties of our GNNs (section 3.2) and by
    using well-known deep learning methods (section 3.3). To end this section, let’s
    use our models to come up with a product bundle for our product manager.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下两个部分将通过利用我们的GNNs（第3.2节）和采用已知的深度学习方法（第3.3节）来提高模型性能，从而从这些基线结果中提升。为了结束本节，让我们使用我们的模型为我们的产品经理提出一个产品组合。
- en: 3.1.5 Our first product bundle
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 我们的第一个产品组合
- en: In the beginning of this section, we discussed our use case of a product manager
    who wants to enhance sales by introducing product bundles. Let’s use one of our
    newly trained models to suggest a bundle for a given product. We’ll group together
    the nodes whose embeddings are most similar to a selected node, forming a bundle
    based on their similarity. Later in the chapter, as we improve the models, we’ll
    come back to the exercise.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开始，我们讨论了我们的产品经理使用案例，他希望通过引入产品组合来提高销售额。让我们使用我们新训练的一个模型为给定产品提出一个组合。我们将把与所选节点嵌入最相似的节点分组在一起，根据它们的相似性形成一个组合。在章节的后面，当我们改进模型时，我们将回到这个练习。
- en: Note  The code won’t be reviewed extensively here but can be found in the repository.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在此处不会对代码进行详细审查，但可以在存储库中找到。
- en: Node ID to product number
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点ID到产品编号
- en: 'One key file provided in the Amazon Products dataset is a comma-separated values
    (CSV) file mapping for node index to Amazon product ID (ASIN). In the repository,
    this is used to create a Python dictionary of node ID (key) to ASIN (value). Using
    a node’s ASIN, we can access information about the product using a URL in this
    format: www.amazon.com/dp/{ASIN}. (Given the age of the dataset, a few ASINs don’t
    have web pages currently, but the vast majority we tested do at the time of writing.)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊产品数据集中提供的一个关键文件是一个逗号分隔值（CSV）文件，将节点索引映射到亚马逊产品ID（ASIN）。在存储库中，这被用来创建一个Python字典，将节点ID（键）映射到ASIN（值）。使用一个节点的ASIN，我们可以通过以下格式的URL访问有关产品的信息：www.amazon.com/dp/{ASIN}。（鉴于数据集的年龄，一些ASIN目前没有网页，但在撰写本文时，我们测试的大多数都有。）
- en: 'To create a product bundle, we work with node embeddings. We choose an individual
    product node and then find the six most similar products to it. This takes four
    steps:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个产品组合，我们与节点嵌入一起工作。我们选择一个单独的产品节点，然后找到与它最相似的六个产品。这需要四个步骤：
- en: Produce node embeddings by running our nodes through our trained GNN.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行我们的节点通过我们的训练好的GNN来产生节点嵌入。
- en: Create a similarity matrix using the node embeddings.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用节点嵌入创建一个相似度矩阵。
- en: Sort the top five embeddings by similarity to our chosen product.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按相似度对所选产品的顶级嵌入进行排序。
- en: Convert the node indices of these top embeddings to product IDs.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些顶级嵌入的节点索引转换为产品ID。
- en: A seed may be set to ensure reproducibility. Otherwise, your results will differ
    with every run of your program.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设置一个种子以确保可重复性。否则，每次运行程序的结果都会不同。
- en: Produce node embeddings
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 产生节点嵌入
- en: 'Like chapter 2, we run our nodes through the model to produce an embedding
    instead of a prediction. In contrast to chapter 2, we have a trained model for
    this purpose that has learned from the node features and the co-purchasing relationships
    of our dataset. To accomplish this, we put our model into evaluation mode (`eval()`),
    disable gradient computations that support backpropagation (`no_grad()`), and
    then run a forward pass of the graph data through the model. Earlier, when defining
    the model class, we enabled an option to return an embedding or a prediction (`return_embeds`):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与第2章类似，我们通过模型运行我们的节点以产生嵌入而不是预测。与第2章不同，我们有一个为此目的训练好的模型，它已经从我们的数据集的节点特征和共同购买关系中学到了知识。为了完成这个任务，我们将我们的模型置于评估模式（`eval()`），禁用支持反向传播的梯度计算（`no_grad()`），然后通过模型运行图数据的正向传递。在定义模型类的时候，我们启用了一个选项来返回嵌入或预测（`return_embeds`）：
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create a similarity matrix
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建一个相似度矩阵
- en: 'A similarity matrix is a set of data, usually in tabular form, that contains
    the similarities between all pairs of items in a set. In our case, we use cosine
    similarity, and compare the embeddings of all the nodes in our set. SciKit Learn’s
    `cosine_similarity` function accomplishes this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度矩阵是一组数据，通常以表格形式呈现，包含集合中所有项目对之间的相似性。在我们的案例中，我们使用余弦相似性，并比较我们集合中所有节点的嵌入。SciKit
    Learn的`cosine_similarity`函数实现了这一点：
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: List the items closest in similarity to a chosen node
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列出与所选节点最相似的条目
- en: 'To identify items most similar to a specific node, we begin by selecting a
    node—referred to by its index as `product_idx`. Using the cosine similarity matrix,
    we examine how closely related each node is to our chosen node by sorting the
    similarities in descending order. The top entries from this sorting (specifically,
    the first six, where `top_k` is set to `6`) represent the nodes most similar to
    our selected node. Notably, the list includes the selected node itself, So, for
    practical purposes, we consider the next five nodes to effectively create a bundle
    of similar items:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别与特定节点最相似的物品，我们首先选择一个节点——用其索引`product_idx`来指代。使用余弦相似度矩阵，我们通过降序排列相似度来检查每个节点与所选节点的紧密程度。排序中的前几项（具体来说，前六项，其中`top_k`设置为`6`）代表了与所选节点最相似的节点。值得注意的是，这个列表中包括了所选节点本身，因此，出于实用目的，我们考虑接下来的五个节点来有效地创建一个相似物品的捆绑包：
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Convert the node indices to product IDs
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将节点索引转换为产品ID
- en: From here, using the index-to-ASIN dictionary will identify the product bundle
    from the node indices. With this done, let’s pick a product node at random and
    generate a product bundle around it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，使用索引到ASIN字典，我们可以根据节点索引识别产品捆绑包。完成此操作后，让我们随机选择一个产品节点并围绕它生成一个产品捆绑包。
- en: Product Bundle Demo
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 产品捆绑包演示
- en: 'At random, we pick node #123\. Using our index-to-ASIN dictionary, we get ASIN:
    B00BV1P6GK. This ASIN belongs to the product Funko POP Television: Adventure Time
    Marceline Vinyl Figure, as shown in figure 3.3\. The category of this product
    is Toys & Games.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择节点#123。使用我们的索引到ASIN字典，我们得到ASIN：B00BV1P6GK。这个ASIN属于图3.3中所示的产品Funko POP电视：*冒险时间*马塞尔琳乙烯基人偶。该产品的类别是玩具与游戏。
- en: '![figure](../Images/3-3.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-3.png)'
- en: 'Figure 3.3 Our selected product, Funko POP Television: Adventure Time Marceline
    Vinyl Figure. In this section, a product bundle will be generated for this product.'
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 我们所选的产品，Funko POP电视：*冒险时间*马塞尔琳乙烯基人偶。在本节中，将为该产品生成一个产品捆绑包。
- en: Marceline, the hundreds-of-years-old Vampire Queen, is one of the main characters
    in the popular animated TV series *Adventure Time.* Marceline is known for her
    rock star persona, love of music, and playing her bass guitar, which is often
    a focal point in her appearances. Her persona is reflected in the figurine, which
    is smiling and has a relaxed but confident pose.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 马塞尔琳，这位数百岁的吸血鬼女王，是流行动画电视剧《*冒险时间*》中的主要角色之一。马塞尔琳以其摇滚明星形象、对音乐的热爱以及弹奏她的贝斯吉他而闻名，这在她的出场中经常成为焦点。她的形象在雕像中得到了体现，雕像面带微笑，姿势放松但自信。
- en: '*Adventure Time* is an animated series that follows the surreal and epic adventures
    of a boy named Finn and his magical dog Jake in the mystical Land of Ooo, filled
    with princesses, vampires, ice kings, and many other bizarre characters.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 《*冒险时间*》是一部动画系列，讲述了名叫芬恩的男孩和他的魔法狗杰克在神秘的土地奥兹的奇幻和史诗般的冒险故事，这里充满了公主、吸血鬼、冰王和许多其他奇怪的角色。
- en: For a collection based on the *Adventure Time* series, one may expect a variety
    of vinyl figures representing the show’s eclectic cast of characters. Let’s see
    what our system generates.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于《*冒险时间*》系列的收藏，人们可能会期待一系列代表该节目多元角色阵容的乙烯基人偶。让我们看看我们的系统会生成什么。
- en: 'Using the process outlined earlier, the bundle shown in figure 3.4, was generated.
    There is one *Adventure Time* vinyl figure included. The rest of the choices seem
    unrelated at first glance, but maybe this set is a nonintuitive bundle. Let’s
    take a closer look:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面概述的过程，生成了图3.4所示的捆绑包。其中包含一个*冒险时间*乙烯基人偶。乍一看，其余的选择似乎无关，但也许这个套装是一个非直观的捆绑包。让我们仔细看看：
- en: '*First ranked similarity: Funko POP Television: Adventure Time Finn with Accessories*—Finn
    is the central character from *Adventure Time*, a recommendation we expected.
    This suggests that fans of Marceline might also appreciate or collect merchandise
    related to other main characters from the same show.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*排名第一的相似度：Funko POP电视：*冒险时间*芬恩带配件*——芬恩是《*冒险时间*》的中心角色，这是我们预期的推荐。这表明，马塞尔琳的粉丝可能也会欣赏或收集与该节目其他主要角色相关的商品。'
- en: '*Second ranked similarity: Funko My Little Pony: DJ Pon-3 Vinyl Figure*—This
    item might seem out of context at first glance, but it may indicate a crossover
    interest in animated series. DJ Pon-3, or Vinyl Scratch, from *My Little Pony*
    is a musical character like Marceline, appealing to those who enjoy characters
    associated with music.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第二相似度排名：Funko My Little Pony：DJ Pon-3乙烯基人偶*——这个物品乍一看可能显得与上下文不符，但它可能表明对动画系列的跨界兴趣。来自《我的小马驹》的DJ
    Pon-3，或称乙烯基Scratch，是一个像马塞尔琳一样的音乐角色，吸引那些喜欢与音乐相关的角色。'
- en: '![figure](../Images/3-4.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-4.png)'
- en: Figure 3.4 A product bundle centered on the Marceline product. The recommendations
    are members of the Toy & Games category. The themes of these products connect
    loosely to the selected product.
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4 以 Marceline 产品为中心的产品组合。推荐的产品属于玩具与游戏类别。这些产品的主题与所选产品有松散的联系。
- en: '*Third ranked similarity: Funko My Little Pony: Twilight Sparkle Vinyl Figure*—Similar
    to DJ Pon-3, Twilight Sparkle from *My Little Pony* represents another connection
    to a popular animated series. This inclusion could appeal to collectors who enjoy
    fantasy themes and strong female characters.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第三位相似度：Funko 小马宝莉：暮光闪闪乙烯基人偶*—与 DJ Pon-3 一样，来自 *小马宝莉* 的暮光闪闪代表着与一部流行动画系列的另一种联系。这种包含可能会吸引那些喜欢奇幻主题和强大女性角色的收藏家。'
- en: '*Fourth and fifth ranked similarities: Pirate-themed accessories (Gold Coins,
    Tattoos, Handheld Brass Telescope with Wooden Box)*—These items are less directly
    related to “Adventure Time” or “My Little Pony”, but they enhance the theme of
    adventure and exploration, which is a significant element of both series.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第四和第五位相似度：海盗主题配饰（金币、纹身、手持黄铜望远镜带木盒）*—这些物品与“冒险时间”或“小马宝莉”的直接关联较少，但它们增强了探险和探索的主题，这是这两部系列的重要元素。'
- en: All in all, this is not a bad product bundle from our baseline models! Wrapping
    up this introductory section on model training and evaluation, we’ve now established
    a solid foundation for understanding and using GNNs. This understanding is crucial
    as we progress to section 3.2, where we’ll dive deeper into neighborhood aggregation,
    an effective tool to enhance performance. Then, in section 3.3, we’ll draw from
    general deep learning approaches to further optimize the models’ performances.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这是我们基线模型中不错的产品组合！总结本节关于模型训练和评估的介绍，我们现在已经为理解和使用 GNN 建立了一个坚实的基础。这种理解对于我们进入
    3.2 节至关重要，在那里我们将更深入地探讨邻域聚合，这是一种有效的工具，可以增强性能。然后，在 3.3 节中，我们将借鉴通用的深度学习方法来进一步优化模型的性能。
- en: 3.2 Aggregation methods
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 聚合方法
- en: In this section, we extend the product category analysis from the previous section
    and take a deeper look into the characteristics of GNNs that influence their performance
    on tasks such as product categorization. Specifically, we explore aggregation
    methods, techniques that have a large influence on the performance of convolutional
    GNNs. Neighborhood aggregation allows nodes to gather and integrate feature information
    from their local node neighborhoods, capturing contextual relevance within the
    larger network.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们扩展了上一节的产品类别分析，并深入研究了影响 GNN 在产品分类等任务上性能的特征。具体来说，我们探讨了聚合方法，这些方法对卷积 GNN
    的性能有重大影响。邻域聚合允许节点从其局部节点邻域收集和整合特征信息，捕捉更大网络中的上下文相关性。
- en: 'We start with the simple aggregations mean, sum, and max, each applied over
    all layers of a model. Then, we survey a few more advanced implementations in
    PyG: unique aggregations applied per layer, list aggregations, aggregation functions,
    and a layer-wise aggregation known as jumping knowledge networks (JK-Nets). Finally,
    we provide some guidelines on applying such methods.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的聚合方法开始，包括均值、总和和最大值，这些方法应用于模型的所有层。然后，我们在 PyG 中调查了几种更高级的实现：每层应用的独特聚合、列表聚合、聚合函数以及称为跳跃知识网络（JK-Nets）的层内聚合。最后，我们提供了一些应用这些方法的指导方针。
- en: 3.2.1 Neighborhood aggregation
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 邻域聚合
- en: One way graph data structures are different is that nodes are interconnected
    through edges, creating a network where nodes can be directly linked or separated
    by several degrees. This spatial arrangement means that any given node may be
    in close proximity to certain other nodes, forming what we call its *neighborhood*.
    The concept of a node’s neighborhood is critical as it often holds key insights
    into the node’s characteristics and that of the overall graph.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据结构的一种不同之处在于节点通过边相互连接，形成一个节点可以直接链接或由几个度数分隔的网络。这种空间排列意味着任何给定的节点可能与其他某些节点非常接近，形成我们所说的其
    *邻域*。节点邻域的概念至关重要，因为它通常包含关于节点特征和整个图的关键见解。
- en: In convolutional GNNs, node neighborhoods are used through a process known as
    *neighborhood aggregation*. This technique involves gathering and combining feature
    information from a node’s immediate neighbors to capture both their individual
    and collective properties. By doing so, a node’s representation is enriched with
    the contextual information provided by its surroundings, which enhances the model’s
    capability to learn more complex and nuanced patterns within the graph.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积图神经网络（GNN）中，节点邻域通过称为*邻域聚合*的过程来使用。这项技术涉及收集和组合节点直接邻居的特征信息，以捕捉它们的个体和集体属性。通过这样做，节点的表示被其周围环境提供的上下文信息丰富，这增强了模型在图中学习更复杂和细微模式的能力。
- en: Neighborhood aggregation operates under the premise that nodes in proximity
    to each other are likely to influence each other more significantly than those
    farther away. This is particularly advantageous for tasks where the relationship
    and interaction between nodes are predictive of their behaviors or properties.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域聚合基于这样一个前提：彼此靠近的节点可能比距离较远的节点更可能相互影响。这在节点之间的关系和交互可以预测其行为或属性的任务中特别有利。
- en: Neighborhood aggregation in PyG
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyG中的邻域聚合
- en: In the PyG layers GCN (`GCNconv`) and GraphSAGE (`SAGEConv`), neighborhood aggregation
    is implemented in different ways. In GCN, a weighted average aggregation is built
    into the layer; if you want to tweak it, you must create a customized version
    of this layer. In this section, we’ll mostly focus on GraphSAGE, which allows
    you to set an aggregation via a parameter. An upcoming section will examine a
    layer-wise aggregation used in GCN.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyG层GCN（`GCNconv`）和GraphSAGE（`SAGEConv`）中，邻域聚合以不同的方式实现。在GCN中，加权平均聚合内置到层中；如果你想要调整它，你必须创建这个层的自定义版本。在本节中，我们将主要关注GraphSAGE，它允许你通过参数设置聚合。下一节将检查GCN中使用的层级聚合。
- en: 'In `SAGEConv`, the `aggr` parameter specifies the type of aggregation. The
    options include, but are not limited to the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SAGEConv`中，`aggr`参数指定了聚合的类型。选项包括但不限于以下内容：
- en: '*Sum aggregation*—A simple aggregation that sums up all neighbor node features.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*求和聚合*——一种简单的聚合，将所有邻居节点的特征求和。'
- en: '*Mean aggregation*—Computes the mean of the neighbor node features. This is
    often used for its simplicity and effectiveness in averaging feature information,
    helping to smooth out anomalies in the data.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*均值聚合*——计算邻居节点特征的均值。这通常因其简单性和在平均特征信息方面的有效性而得到应用，有助于平滑数据中的异常值。'
- en: '*Max aggregation*—Takes the maximum feature value among all neighbors for each
    feature dimension. This can help when the most prominent features are more informative
    than average features, capturing the most significant signals from the neighbors.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最大值聚合*——对于每个特征维度，从所有邻居中取最大特征值。当最显著的特征比平均特征更有信息量时，这有助于捕捉来自邻居的最重要信号。'
- en: '*LSTM aggregation*—A relatively compute- and memory-intensive method that uses
    an LSTM network to process features of the ordered sequence of neighbor nodes.
    It considers the sequence of nodes, which can be crucial for tasks where the order
    of node processing affects the results. As such, special care must be taken to
    arrange a dataset’s nodes and edges for training.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LSTM聚合*——一种相对计算和内存密集的方法，使用LSTM网络处理邻居节点有序序列的特征。它考虑了节点的序列，这在节点处理顺序影响结果的任务中可能至关重要。因此，必须特别注意安排数据集的节点和边以进行训练。'
- en: Choosing among these types will depend on the characteristics of a given graph,
    and the prediction goals. If you don’t have a good feel for which method will
    be more effective for your graph and your use case, trial and error can suffice
    to choose the aggregation method. In addition, while some of the aggregation options
    can be applied out of the box, others—such as LSTM aggregation, which relies on
    a trained LSTM network—require some thought to be put into data preparation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类型中选择将取决于给定图的特性和预测目标。如果你对哪种方法对你的图和用例更有效没有很好的感觉，可以通过试错来选择聚合方法。此外，虽然一些聚合选项可以即插即用，但其他一些选项——例如依赖于训练好的LSTM网络的LSTM聚合——在数据准备上需要一些思考。
- en: To see the effect of different aggregations, we add the `aggr` parameter to
    our model class and then proceed to train as in section 3.1, swapping out the
    mean, sum, and max aggregations. It should be noted that the mean aggregation
    is the default for the `SAGEConv` layer, so it’s equivalent to our GraphSAGE baseline
    model. Creating a *GraphSAGE* class with aggregations would look like the following
    listing.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到不同聚合的效果，我们在模型类中添加了 `aggr` 参数，然后继续按照第 3.1 节中的步骤进行训练，用均值、求和和最大聚合替换。需要注意的是，均值聚合是
    `SAGEConv` 层的默认值，因此它与我们的 GraphSAGE 基线模型等效。创建具有聚合的 *GraphSAGE* 类的示例如下。
- en: Listing 3.5 GraphSAGE class with aggregation parameter
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.5 带聚合参数的 GraphSAGE 类
- en: '[PRE7]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Sets keyword parameter for aggregation'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置聚合的关键字参数'
- en: '#2 First GraphSAGE layer with specified aggregation'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指定聚合的第一层 GraphSAGE'
- en: '#3 Second GraphSAGE layer with specified aggregation'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 指定聚合的第二层 GraphSAGE'
- en: Results of using mean, max, and sum aggregations
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用均值、最大值和求和聚合的结果
- en: Table 3.3 compares the models using F1 score and log loss as performance metrics.
    The table shows that the model using max aggregation is the best under both measures.
    The results for the model using max aggregation shows the highest F1 score of
    0.7449 and the lowest log loss of 2.1039, suggesting that max aggregation is a
    little more capable at identifying and using the most influential features in
    the prediction task. The model that uses mean aggregation is equivalent to the
    model trained in section 3.1\. We observe that the max aggregation out-performs
    the other two. Overall, the performance using different aggregations is very similar
    to that of our baseline GraphSAGE model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.3 比较了使用 F1 分数和对数损失作为性能指标的不同模型。表中显示，使用最大聚合的模型在两个指标下都是最好的。使用最大聚合的模型结果显示 F1
    分数最高为 0.7449，对数损失最低为 2.1039，这表明最大聚合在识别和使用预测任务中最具影响力的特征方面略胜一筹。使用均值聚合的模型等同于第 3.1
    节中训练的模型。我们观察到最大聚合优于其他两种聚合。总体而言，使用不同聚合的性能与我们的基线 GraphSAGE 模型非常相似。
- en: Table 3.3 Classification performance for GraphSAGE models with different settings
    for neighborhood aggregation
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.3 不同邻域聚合设置下 GraphSAGE 模型的分类性能
- en: '| Aggregation Type | F1 Score | Log Loss |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 聚合类型 | F1 分数 | 对数损失 |'
- en: '| --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mean (default)  | 0.7406  | 2.1214  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 均值（默认） | 0.7406 | 2.1214 |'
- en: '| Sum  | 0.7384  | 2.2496  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 求和 | 0.7384 | 2.2496 |'
- en: '| Max  | 0.7449  | 2.1039  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 0.7449 | 2.1039 |'
- en: What model should be chosen if separate models had the highest F1 score and
    log loss? For example, what if the max aggregation model scored highest for F1,
    but the mean aggregation model took the highest for log loss? This will depend
    on the context of your application, your requirements for prediction, and the
    consequences of potential errors.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单独的模型在 F1 分数和对数损失上都有最高分，应该选择哪个模型？例如，如果最大聚合模型在 F1 分数上得分最高，而均值聚合模型在对数损失上得分最高，这将取决于你应用的上下文、预测的要求以及潜在错误的后果。
- en: In a healthcare situation, such as predicting patient readmissions within 30
    days of discharge, for example, the choice of model can significantly affect patient
    outcomes and resource allocation. A model with a high F1 score would have a more
    balanced precision and recall, making it better in situations where missing a
    readmission could be costly or dangerous. It would be expected to identify more
    patients at risk, allowing for timely interventions. However, this could also
    result in higher false positives, leading to unnecessary treatments and increased
    costs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健情况下，例如预测患者出院后 30 天内的再入院，模型的选择可以显著影响患者结果和资源分配。具有高 F1 分数的模型将具有更平衡的精确度和召回率，在漏诊再入院可能代价高昂或危险的情况下表现更好。它预计会识别出更多处于风险中的患者，从而允许及时干预。然而，这也可能导致更高的假阳性，导致不必要的治疗和成本增加。
- en: A model that exhibits low log loss, on the other hand, offers high confidence
    in its predictions, prioritizing the accuracy of each prediction over the number
    of positive cases detected. This model is useful when resource allocation needs
    to be precise or when treatments have substantial side effects.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，具有低对数损失的模型对其预测有很高的信心，它优先考虑每个预测的准确性，而不是检测到的阳性案例数量。这种模型在资源分配需要精确或治疗方案有显著副作用时非常有用。
- en: Coming back to our product manager who is deciding which products and product
    bundles to allocate marketing dollars to, having more confident predictions would
    be desirable in preventing wasted marketing efforts. The lower likelihood of false
    positives helps in efficiently using resources, but at the risk of missing some
    revenue-generating bundle configurations due to conservative predictions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的产品经理，他正在决定将营销资金分配给哪些产品和产品组合，更可靠的预测将有助于防止营销努力的浪费。降低假阳性的可能性有助于高效地使用资源，但同时也存在因保守预测而错过一些能带来收入的组合配置的风险。
- en: In this section, we used a simple string argument for the `aggr` parameter.
    PyG, however, has a wide set of tools to incorporate a variety of aggregation
    methods into your models. We explore these in the next section.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了简单的字符串参数`aggr`。然而，PyG有一套广泛的工具，可以将各种聚合方法纳入你的模型。我们将在下一节中探讨这些工具。
- en: 3.2.2 Advanced aggregation tools
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2高级聚合工具
- en: This section explores more advanced aggregation tools within PyG. We begin by
    assigning distinct aggregation methods to different layers within a multilayer
    architecture. Next, we explore the combination of various aggregation strategies—such
    as `'mean'`, `'max'`, and `'sum'`—within a single layer. Finally, we revisit GCNs
    to examine the jumping knowledge (JK) method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了PyG中更高级的聚合工具。我们首先将不同的聚合方法分配给多层架构中的不同层。接下来，我们探索了在单个层中组合各种聚合策略——如`'mean'`、`'max'`和`'sum'`——的可能性。最后，我们回顾GCNs以检查跳跃知识（JK）方法。
- en: Using multiple aggregations across layers
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在层间使用多个聚合
- en: In a multilayer GraphSAGE model, you can of course adjust the aggregation function
    at each layer independently. For example, you might use mean aggregation at the
    first layer to smooth features but switch to max aggregation at a subsequent layer
    to highlight the most significant of the resulting neighbor features.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层GraphSAGE模型中，你当然可以在每个层独立地调整聚合函数。例如，你可能会在第一层使用平均聚合来平滑特征，但在后续层切换到最大聚合以突出显示最显著的邻居特征。
- en: As a first exercise in our exploration, let’s apply a couple of permutations
    of aggregations to two layers and see if these configurations outperform our previous
    results. We use the code from before, swapping out the `aggr` settings for `conv1`
    and `conv2`. For one model, we use *mean* for the first layer and *max* at the
    second. For the other model, we use *sum* for the first layer and *max* at the
    second. Table 3.4 summarizes the results.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 作为探索的第一步，让我们将几种聚合排列应用于两层，看看这些配置是否优于我们之前的结果。我们使用之前的代码，将`aggr`设置从`conv1`和`conv2`中替换出来。对于一种模型，我们在第一层使用`mean`，在第二层使用`max`。对于另一种模型，我们在第一层使用`sum`，在第二层使用`max`。表3.4总结了结果。
- en: Table 3.4 Classification performance for GraphSAGE models with different settings
    for neighborhood aggregation
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.4不同邻域聚合设置下的GraphSAGE模型分类性能
- en: '| Aggregation Type | F1 Score | Log Loss |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 聚合类型 | F1分数 | 对数损失 |'
- en: '| --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mean (default)  | 0.7406  | 2.1214  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 平均值（默认） | 0.7406 | 2.1214 |'
- en: '| Sum  | 0.7384  | 2.2496  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 求和 | 0.7384 | 2.2496 |'
- en: '| Max  | 0.7449  | 2.1039  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 0.7449 | 2.1039 |'
- en: '| Layered: Mean → Max  | 0.7316  | 2.2041  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 层次化：平均值 → 最大值 | 0.7316 | 2.2041 |'
- en: '| Layered: Sum → Max  | 0.7344  | 2.345  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 层次化：求和 → 最大值 | 0.7344 | 2.345 |'
- en: We have middling results at best for our dataset. The model with only max aggregation
    outperforms the newer models. Let’s move on to combining several aggregations
    for each layer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据集，我们的结果最多是中等。只有最大聚合的模型优于新模型。让我们继续为每个层组合多个聚合。
- en: List aggregations and aggregation functions
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列表聚合和聚合函数
- en: In PyG, the concept of using a list for specifying aggregation functions allows
    you to customize your models with multiple aggregation strategies simultaneously.
    This feature is significant as it enables the model to use different aspects of
    graph data, potentially enhancing model performance by capturing various properties
    of the graph. In a way, you’re aggregating your aggregations. For instance, you
    could combine `'mean'`, `'max'`, and `'sum'` aggregations in a single layer to
    capture average, most significant, and summed structural properties of the neighborhood.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyG中，使用列表指定聚合函数的概念允许你同时使用多种聚合策略来定制你的模型。这个特性很重要，因为它使模型能够使用图数据的各个方面，通过捕捉图的多种属性来提高模型性能。从某种意义上说，你是在聚合你的聚合。例如，你可以在单个层中结合`'mean'`、`'max'`和`'sum'`聚合，以捕捉邻域的平均、最显著和总和的结构属性。
- en: This works in PyG by passing a list of aggregation functions, either as strings
    or as `Aggregation` module instances, into the `MessagePassing` class. PyG resolves
    these strings against a predefined set of aggregation functions or can directly
    use an aggregation function as the `aggr` argument. For example, using the keyword
    `'mean'` invokes the `MeanAggregation()` function.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这在PyG中通过将聚合函数的列表（可以是字符串或`Aggregation`模块实例）传递给`MessagePassing`类来实现。PyG将这些字符串与预定义的聚合函数集进行解析，或者可以直接将聚合函数作为`aggr`参数使用。例如，使用关键字`'mean'`将调用`MeanAggregation()`函数。
- en: 'There are a universe of combinations to try, but let’s try two examples to
    demonstrate, mixing familiar aggregations, `''`max`''`, `''`sum`''`, and `''`mean`''`;
    and a set of more exotic aggregations, `SoftmaxAggregation` and `StdAggregation`
    [3]. They can be applied to our `conv1` layer as follows (table 3.5 compares these
    results with previous results):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有无数种组合可以尝试，但让我们尝试两个示例来演示，混合熟悉的聚合，`'max'`，`'sum'`和`'mean'`；以及一组更奇特的聚合，`SoftmaxAggregation`和`StdAggregation`[3]。它们可以应用于我们的`conv1`层，如下所示（表3.5比较了这些结果与之前的结果）：
- en: '[PRE8]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Table 3.5 Classification performance for GraphSAGE models with list aggregations
    added
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.5添加列表聚合的GraphSAGE模型的分类性能
- en: '| Aggregation type | F1 score | Log loss |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 聚合类型 | F1分数 | 对数损失 |'
- en: '| --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mean (default)  | 0.7406  | 2.1214  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 平均（默认） | 0.7406  | 2.1214  |'
- en: '| Sum  | 0.7384  | 2.2496  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 求和  | 0.7384  | 2.2496  |'
- en: '| Max  | 0.7449  | 2.1039  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 最大值  | 0.7449  | 2.1039  |'
- en: '| Layered: Mean →Max  | 0.7316  | 2.2041  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 层次：平均→最大值  | 0.7316  | 2.2041  |'
- en: '| Layered: Sum →Max  | 0.7344  | 2.345  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 层次：求和→最大值 | 0.7344  | 2.345  |'
- en: '| List (standard)  | 0.7484  | 2.622  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 列（标准） | 0.7484  | 2.622  |'
- en: '| List (exotic)  | 0.745  | 2.156  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 列（奇特） | 0.745  | 2.156  |'
- en: Figure 3.5 visualizes the performance comparison from table 3.5\. While the
    F1 scores are very similar, there is a slight performance boost in F1 score from
    the “standard” list aggregation, though with the drawback of a much higher log
    loss.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5可视化了表3.5中的性能比较。虽然F1分数非常相似，但“标准”列表聚合在F1分数上略有提升，尽管代价是更高的对数损失。
- en: '![figure](../Images/3-5.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-5.png)'
- en: Figure 3.5 Performance comparison visualized from table 3.5\. While the F1 scores
    are very similar, the standard list aggregation performs slightly better with
    respect to log loss.
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5从表3.5中可视化的性能比较。虽然F1分数非常相似，但标准列表聚合在对数损失方面表现略好。
- en: Given the results of our quick survey of these aggregation methods applied to
    the GraphSAGE layer, you might conclude that sticking with the default setting
    is often the best option. However, the potential for performance improvements
    through tailored aggregation strategies suggests that further exploration could
    be beneficial.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们对这些聚合方法应用于GraphSAGE层的快速调查结果，你可能会得出结论，坚持默认设置通常是最佳选择。然而，通过定制聚合策略提高性能的潜力表明，进一步的探索可能是有益的。
- en: In the upcoming section 3.2.3, we’ll review some considerations in applying
    these aggregation methods. Before that, we’ll come back to the GCN layer to examine
    the JK aggregation method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的3.2.3节中，我们将回顾应用这些聚合方法时的一些考虑因素。在那之前，我们将回到GCN层来检查JK聚合方法。
- en: Jumping knowledge networks
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跳跃知识网络
- en: '*Jumping knowledge* (JK) is a novel approach for node representation learning
    on graphs that addresses limitations of existing models such as GCNs and GraphSAGE
    [4]. It focuses on overcoming a problem of neighborhood aggregation models in
    which models are sensitive to the graph’s structure, causing inconsistent learning
    quality across different graph parts.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*跳跃知识*（JK）是一种在图上进行节点表示学习的新方法，它解决了现有模型（如GCNs和GraphSAGE）的局限性[4]。它专注于克服邻域聚合模型的问题，即模型对图结构敏感，导致不同图部分的学习质量不一致。'
- en: Jumping knowledge networks (JK-Nets) allow flexible usage of different neighborhood
    ranges for each node, thereby adapting to local neighborhood properties and task-specific
    requirements. This adaptation results in improved node representations by enabling
    the model to selectively use information from various neighborhood depths based
    on the node and the subgraph’s context. JK has been implemented for the GCN layer
    in PyG, as shown in listing 3.6\.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃知识网络（JK-Nets）允许对每个节点灵活地使用不同的邻域范围，从而适应局部邻域属性和特定任务的要求。这种适应通过使模型能够根据节点和子图上下文有选择地使用不同邻域深度的信息，从而提高了节点表示。JK已在PyG的GCN层中实现，如列表3.6所示。
- en: 'Its main parameter, `mode`, specifies the aggregation scheme used to combine
    outputs from different layers. The options are listed here:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它的主要参数`mode`指定了用于组合不同层输出的聚合方案。选项如下：
- en: '`''cat''`—Concatenates the outputs from all layers along the feature dimension.
    This approach preserves all information from each layer but increases the dimensionality
    of the output.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''cat''`—沿着特征维度连接所有层的输出。这种方法保留了每一层的所有信息，但增加了输出的维度性。'
- en: '`''max''`—Applies max pooling across the layer outputs. This method takes the
    maximum value across all layers for each feature, which can help in capturing
    the most significant features from the graph while being robust against less informative
    signals.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max''`—在层输出上应用最大池化。这种方法对每个特征在所有层中取最大值，这有助于从图中捕获最重要的特征，同时对于不太有信息性的信号具有鲁棒性。'
- en: '`''lstm''`—Uses a bidirectional LSTM to learn attention scores for each layer’s
    output. The outputs are then combined based on these learned attention weights,
    allowing the model to focus on the most relevant layers dynamically based on the
    input graph structure.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''lstm''`—使用双向LSTM为每一层的输出学习注意力分数。然后根据这些学习到的注意力权重进行输出组合，允许模型根据输入图结构动态地关注最相关的层。'
- en: Listing 3.6 GCN class with `JumpingKnowledge` layer
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6带有`JumpingKnowledge`层的GCN类
- en: '[PRE9]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Initializes JK with concatenation mode'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用连接模式初始化JK'
- en: '#2 List to save outputs from each layer for JK'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 列表用于保存JK的每一层输出'
- en: '#3 Appends the layer outputs list'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将层输出列表附加到'
- en: '#4 Applies JK aggregation to the collected layer outputs'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对收集到的层输出应用JK聚合'
- en: In the listing, for the initialization, a `JumpingKnowledge` layer is initialized
    with the mode set to `'cat'` (concatenate), indicating that the features from
    each layer will be concatenated to form the final node representations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表中，对于初始化，`JumpingKnowledge`层被初始化为模式设置为`'cat'`（连接），表示每一层的特征将被连接以形成最终的节点表示。
- en: In the forward pass, `layer_outputs` is initialized as an empty list to store
    the outputs from each convolutional layer. This list will be used by the `JumpingKnowledge`
    layer.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递中，`layer_outputs`被初始化为一个空列表，用于存储每个卷积层的输出。这个列表将被`JumpingKnowledge`层使用。
- en: The first convolutional layer processes the input `x` and the graph structure
    `edge_index`, and applies a ReLU activation function to introduce nonlinearity.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个卷积层处理输入`x`和图结构`edge_index`，并应用ReLU激活函数以引入非线性。
- en: The output of the first layer (`x1`) is then added to the `layer_outputs` list.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首层的输出（`x1`）随后被添加到`layer_outputs`列表中。
- en: After the second convolutional layer, the second output (`x2`) is also added
    to the `layer_outputs` list.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二个卷积层之后，第二个输出（`x2`）也被添加到`layer_outputs`列表中。
- en: Then, the `JumpingKnowledge` layer takes the list of outputs from all the previous
    layers and aggregates them according to the specified mode (`'cat'`). In concatenation
    mode, the feature vectors from each layer are concatenated along the feature dimension.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，`JumpingKnowledge`层接受所有先前层的输出列表，并根据指定的模式（`'cat'`）进行聚合。在连接模式下，每个层的特征向量沿着特征维度进行连接。
- en: Table 3.6 compares the classification performance for GCN models. The baseline
    GCN model from section 3.1 is compared to a version using the `JumpingKnowledge`
    aggregation method. The baseline model has a better F1 score, while the JK model
    outperforms in log loss.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.6比较了GCN模型的分类性能。第3.1节中的基线GCN模型与使用`JumpingKnowledge`聚合方法的版本进行比较。基线模型具有更好的F1分数，而JK模型在log损失方面表现更优。
- en: Table 3.6 Classification performance for GCN models
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.6 GCN模型的分类性能
- en: '| Model | F1 Score | Log Loss |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | F1分数 | Log损失 |'
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Baseline GCN  | 0.781  | 1.42  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 基线GCN  | 0.781  | 1.42  |'
- en: '| JK (GCN)  | 0.699  | 1.36  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| JK (GCN)  | 0.699  | 1.36  |'
- en: The results show that choosing between the baseline and JK versions involves
    a tradeoff between higher recall/precision and higher prediction certainty. This
    tradeoff should be carefully considered based on the specific requirements and
    goals of the task at hand. Further exploration in section 3.2.3 will review some
    considerations in applying these aggregation methods effectively.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在基线版本和JK版本之间进行选择涉及在更高的召回率/精确度和更高的预测确定性之间进行权衡。这种权衡应根据任务的具体要求和目标仔细考虑。第3.2.3节将进一步探索有效应用这些聚合方法的考虑因素。
- en: 3.2.3 Practical considerations in applying aggregation
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 应用聚合的实际考虑因素
- en: Choosing the right aggregation method is a technical decision that should be
    informed by the specific characteristics and needs of the dataset at hand as well
    as the use case. For datasets where the local neighborhood structure is crucial,
    using mean or sum aggregations could potentially blur essential features. In contrast,
    max aggregation could help highlight critical attributes. For example, in a social
    network graph where influencer detection is key, max aggregation might be more
    effective. On the other hand, if what we want to do is represent typical features,
    max aggregation may overemphasize outliers. In a dataset of financial transactions,
    where we want to understand typical user behavior, a max aggregation could distort
    the common behavioral features in favor of one or two large but uncommon transactions.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的聚合方法是一个技术决策，应基于手头数据集的具体特征和需求以及用例。对于局部邻域结构至关重要的数据集，使用平均或求和聚合可能会模糊关键特征。相比之下，最大聚合可以帮助突出关键属性。例如，在一个关键人物检测至关重要的社交网络图中，最大聚合可能更有效。另一方面，如果我们想要表示典型特征，最大聚合可能会过分强调异常值。在一个我们想要了解典型用户行为的金融交易数据集中，最大聚合可能会扭曲常见的用户行为特征，以有利于一两个大但不太常见的交易。
- en: The task itself can dictate the choice of aggregation method. Tasks that require
    capturing the most influential features might benefit from max aggregation, while
    those needing a general representation may find mean aggregation sufficient. In
    a recommendation system for products, max aggregation could help identify the
    most important product features that drive purchases. Additionally, the nature
    of the graph’s topology should guide the aggregation method. Densely connected
    graphs might require different strategies compared to sparsely connected graphs
    to avoid over-smoothing or underrepresentation of node features. For instance,
    a transportation network graph with varying node connectivity might need different
    aggregations at different layers.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 任务本身可以决定聚合方法的选择。需要捕捉最有影响力特征的任务可能从最大聚合中受益，而需要一般表示的任务可能发现平均聚合足够。在一个产品推荐系统中，最大聚合可以帮助识别驱动购买的最重要产品特征。此外，图拓扑的性质应指导聚合方法。密集连接的图可能需要与稀疏连接的图不同的策略，以避免过度平滑或节点特征的欠表示。例如，具有不同节点连接性的交通网络图可能在不同层需要不同的聚合。
- en: Given the dataset’s complexity, empirical testing of different aggregation methods
    is essential. Experimentation can help identify which methods best capture the
    relational dynamics and feature distributions of the dataset. This is particularly
    important for more exotic aggregation methods, where intuition alone may not suffice
    to determine their effectiveness. The scalability of the chosen aggregation method
    to handle millions of nodes and edges efficiently is also crucial. It’s important
    to balance computational efficiency with the sophistication of the method, especially
    for real-time applications such as recommendation systems.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的复杂性，对不同的聚合方法进行实证测试是必不可少的。实验可以帮助确定哪些方法最能捕捉数据集的关系动态和特征分布。这对于更复杂的聚合方法尤为重要，在这些方法中，仅凭直觉可能不足以确定其有效性。所选聚合方法的可扩展性，以高效处理数百万个节点和边，也是至关重要的。在实时应用（如推荐系统）中，平衡计算效率与方法复杂性尤为重要。
- en: Aggregation methods should be considered alongside other model enhancements
    such as feature engineering, node embedding techniques, and regularization strategies
    to address overfitting and improve model generalization. For instance, combining
    effective aggregation methods with advanced embedding techniques (e.g., Node2Vec)
    or incorporating dropout for regularization could significantly boost model performance.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑其他模型增强方法（如特征工程、节点嵌入技术和正则化策略以解决过拟合和改善模型泛化）的同时，应考虑聚合方法。例如，将有效的聚合方法与高级嵌入技术（例如Node2Vec）相结合或引入dropout进行正则化，可以显著提高模型性能。
- en: While there is no one-size-fits-all aggregation method, a thoughtful combination
    of techniques, backed by empirical validation, can significantly enhance model
    performance and applicability. This strategic approach not only aids in accurate
    product categorization but also in crafting effective recommendation systems that
    are crucial in e-commerce settings.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有一种适合所有情况的聚合方法，但经过实证验证的深思熟虑的技术组合可以显著提高模型性能和适用性。这种战略方法不仅有助于准确的产品分类，还有助于构建有效的推荐系统，这在电子商务环境中至关重要。
- en: This section explored and applied different aggregations to our models. The
    next section will round out our exploration of convolutional GNNs by applying
    regularization and adjusting the depth of our models. We’ll consolidate our improvements
    into a final model, from which we’ll generate another product bundle based on
    the Marceline figurine to see if there is improvement.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了并应用了不同的聚合方法到我们的模型中。下一节将通过应用正则化和调整我们模型的深度来完善我们对卷积GNNs的探索。我们将把我们的改进整合到一个最终模型中，然后基于Marceline小雕像生成另一个产品捆绑，以查看是否有改进。
- en: 3.3 Further optimizations and refinements
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 进一步优化和改进
- en: 'Up to now, the GCN and GraphSAGE layers have been introduced via a product
    management example. We established a baseline using the default settings in section
    3.1\. In section 3.2, we examined the use of neighborhood and layer aggregation.
    In this section, we’ll consider other ways we can refine and improve our model.
    In the first subsections, we’ll introduce two other adjustments: the use of dropout
    and model depth. Dropout is a well-known regularization technique that can reduce
    overfitting, and model depth is an adjustment that has a unique meaning for GNNs.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，GCN和GraphSAGE层是通过产品管理示例引入的。我们在3.1节中使用了默认设置建立了基线。在3.2节中，我们考察了使用邻域和层聚合的方法。在本节中，我们将考虑其他我们可以用来改进和优化我们的模型的方法。在前几小节中，我们将介绍两种其他调整：使用dropout和模型深度。Dropout是一种众所周知的正则化技术，可以减少过拟合，而模型深度是对GNNs具有独特意义的调整。
- en: In section 3.3.3, we synthesize these insights to develop a model that incorporates
    multiple improvements and observe the cumulative performance uplift. Finally,
    in section 3.3.4, we revisit our product bundle problem. We create a new product
    bundle using the refined model of section 3.3.3 and compare its performance to
    the bundle created in section 3.1.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.3.3节中，我们综合这些见解来开发一个包含多个改进的模型，并观察累积的性能提升。最后，在3.3.4节中，我们重新审视我们的产品捆绑问题。我们使用3.3.3节中精炼的模型创建一个新的产品捆绑，并将其性能与3.1节中创建的捆绑进行比较。
- en: 3.3.1 Dropout
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 Dropout
- en: '*Dropout* is a regularization technique used to prevent overfitting in neural
    networks by randomly dropping units during training. This helps the model generalize
    better by reducing its dependency on specific neurons.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout* 是一种正则化技术，通过在训练过程中随机丢弃单元来防止神经网络过拟合。这有助于模型更好地泛化，因为它减少了模型对特定神经元的依赖。'
- en: In PyG, the `dropout` function works similarly to the standard PyTorch dropout,
    meaning it randomly sets some elements of the input tensor and the hidden-layer
    activations to `0` during the training process. During each forward pass in training,
    inputs and activations are set to `0` according to the specified dropout rate.
    This helps prevent overfitting by ensuring that the model doesn’t rely too heavily
    on any particular input or activation.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyG中，`dropout`函数的工作方式与标准的PyTorch dropout相似，这意味着在训练过程中，它会随机将输入张量和隐藏层激活的一些元素设置为`0`。在训练过程中的每次前向传递中，根据指定的dropout率，输入和激活被设置为`0`。这有助于通过确保模型不会过度依赖任何特定的输入或激活来防止过拟合。
- en: The structure of the graph, including its vertices (nodes) and edges, remains
    unchanged during dropout. The graph’s topology is preserved, and only the neural
    network’s activations are affected. This distinction is crucial as it maintains
    the integrity of the graph while still using dropout to improve model robustness.
    PyG does have functions that can drop nodes or drop edges during training, but
    the dropout built into `GCNConv` and `SAGEConv` refers to the traditional deep
    learning dropout.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图的结构，包括其顶点（节点）和边，在dropout过程中保持不变。图的拓扑结构得到保留，只有神经网络的激活受到影响。这种区别至关重要，因为它在仍然使用dropout来提高模型鲁棒性的同时，保持了图的完整性。PyG确实有函数可以在训练过程中删除节点或边，但内置在`GCNConv`和`SAGEConv`中的dropout指的是传统的深度学习dropout。
- en: In PyG, both the GraphSAGE and GCN layers use `dropout` rate as a parameter,
    with a default of 0\. Figure 3.6 illustrates the performance of GCN models with
    varying dropout rates (0%, 50%, and 85%). As shown, higher dropout rates can help
    mitigate overfitting, as indicated by the reduced gap between training and validation
    losses. For the 85% case, the higher dropout rate could be causing the model to
    converge more slowly, or it could be a sign of overfitting. More testing is warranted
    to find out.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyG 中，GraphSAGE 和 GCN 层都使用 `dropout` 率作为参数，默认值为 0。图 3.6 展示了具有不同 dropout 率（0%、50%
    和 85%）的 GCN 模型的性能。如图所示，更高的 dropout 率可以帮助减轻过拟合，这可以从训练损失和验证损失之间的差距减小中看出。对于 85% 的情况，更高的
    dropout 率可能会导致模型收敛速度变慢，或者这可能是一个过拟合的迹象。需要进行更多测试来确定。
- en: Next, let’s examine model depth and how it’s implemented for convolutional GNNs.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来探讨模型深度及其在卷积图神经网络（GNNs）中的实现方式。
- en: 3.3.2 Model depth
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 模型深度
- en: In GNNs, a *layer* refers to the number of hops or message-passing steps. Each
    layer allows nodes to aggregate information from their immediate neighbors, effectively
    increasing the receptive field by one hop per layer. A three-layer model, for
    example, would interrogate the neighborhood three hops away from each node. The
    *depth* of a GNN, then, refers to the number of layers in the network, analogous
    to the depth in traditional deep learning models but with key differences due
    to graph-structured data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GNNs 中，*层* 指的是跳跃或消息传递步骤的数量。每一层允许节点从其直接邻居中聚合信息，有效地通过每一层增加一个跳跃的感知场。例如，一个三层模型会查询每个节点三个跳跃之外的邻域。因此，GNN
    的 *深度* 指的是网络中的层数，类似于传统深度学习模型中的深度，但由于图结构数据的关键差异而有所不同。
- en: If a GNN has too few layers, it may not capture sufficient information from
    the graph, leading to poor representation learning, as each node can only aggregate
    information from a limited neighborhood. Conversely, increasing the number of
    layers can lead to *over-smoothing*, where node features become too similar, making
    it difficult to distinguish between different nodes. With each additional layer,
    nodes aggregate information from a larger neighborhood, diluting the unique features
    of individual nodes. Various metrics and methods have been proposed to measure
    and mitigate this effect.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个 GNN 层数过少，它可能无法从图中捕获足够的信息，导致较差的表示学习，因为每个节点只能从有限的邻域中聚合信息。相反，增加层数可能导致 *过度平滑*，节点特征变得过于相似，难以区分不同的节点。随着每增加一层，节点从更大的邻域中聚合信息，稀释了单个节点的独特特征。已经提出了各种指标和方法来衡量和减轻这种影响。
- en: '![figure](../Images/3-6.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-6.png)'
- en: Figure 3.6 Training curve comparisons of three models with different levels
    of dropout. The left has a dropout of 0%, the middle a dropout of 50%, and the
    right a dropout of 85%. For our model and dataset, adding dropout indeed ameliorates
    overfitting. The model with 85% dropout could show signs of either underfitting
    or slowly converging, requiring more experimentation.
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.6 三种不同 dropout 水平的模型训练曲线比较。左侧的 dropout 为 0%，中间为 50%，右侧为 85%。对于我们的模型和数据集，添加
    dropout 确实改善了过拟合。具有 85% dropout 的模型可能显示出欠拟合或收敛缓慢的迹象，需要更多的实验。
- en: The performance of GNNs with different depths can vary significantly. Typically,
    GNNs with 2 or 3 layers perform competitively on many tasks, balancing the need
    for sufficient neighborhood information without causing over-smoothing. While
    deeper GNNs can theoretically capture more complex patterns, they often suffer
    from over-smoothing and increased computational complexity. Very deep GNNs, such
    as those with 50 or more layers, can lead to higher validation loss, indicating
    over-fitting and/or over-smoothing.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 不同深度的 GNNs 的性能可能会有显著差异。通常，具有 2 或 3 层的 GNNs 在许多任务上表现竞争力，在满足足够邻域信息需求的同时，不会导致过度平滑。虽然更深层的
    GNNs 从理论上可以捕捉更复杂的模式，但它们通常会受到过度平滑和计算复杂度增加的影响。非常深的 GNNs，例如具有 50 层或更多层的，可能会导致更高的验证损失，这表明过拟合和/或过度平滑。
- en: Figure 3.7 compares the performance of GNNs with different depths (e.g., 2 layers,
    10 layers, and 50 layers). We see that the 2-layer model achieves a good balance
    between training and validation loss. In the 10-layer GNNs, we see some improvement
    in training loss but also signs of over-smoothing from the higher validation loss.
    The 50-layer model shows degraded training and validation loss, which indicates
    severe over-smoothing or over-fitting.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 比较了不同深度 GNN 的性能（例如，2 层、10 层和 50 层）。我们看到 2 层模型在训练和验证损失之间取得了良好的平衡。在 10 层
    GNN 中，我们看到了训练损失的改进，但也出现了来自更高验证损失的过平滑迹象。50 层模型显示训练和验证损失下降，这表明存在严重的过平滑或过拟合。
- en: '![figure](../Images/3-7.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-7.png)'
- en: 'Figure 3.7 Training curves for trained models of different depths: 2 layers
    (top), 10 layers (middle), 50 layers (bottom). The 2-layer model has the best
    profile with no signs of overfitting or performance degradation.'
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.7 不同深度训练模型的训练曲线：2 层（顶部），10 层（中间），50 层（底部）。2 层模型具有最佳性能，没有过拟合或性能下降的迹象。
- en: Balancing the depth of the model is critical to achieving optimal performance
    in GNNs. Too few layers may result in weak representation learning, while too
    many layers can lead to over-smoothing, where node features become indistinguishable.
    In the next section, we apply what we’ve learned about tuning our model in this
    chapter, resulting in a refined model that will outperform our baseline.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GNN 中实现最佳性能的关键在于平衡模型的深度。层数过少可能导致弱表示学习，而层数过多可能导致过平滑，节点特征变得难以区分。在下一节中，我们将应用本章中关于调整模型所学到的东西，从而得到一个优于基线的优化模型。
- en: 3.3.3 Improving the baseline model’s performance
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 提高基线模型的性能
- en: 'Given all the insights gained in this chapter, let’s train models that synthesize
    these learnings and compare them against the baseline. Some key takeaways from
    the previous sections that we’ll incorporate are listed here:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章获得的所有洞察的基础上，让我们训练合成这些学习的模型，并将它们与基线进行比较。以下是我们将在此处包含的一些关键要点：
- en: '*Model depth*—We’ll keep it low, at two layers.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型深度*—我们将将其保持在较低水平，即两层。'
- en: '*Neighborhood aggregation*—We’ll use max aggregation and experiment with two
    list aggregations. The same aggregation will be used on both layers.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*邻域聚合*—我们将使用最大聚合并尝试两种列表聚合。相同的聚合将应用于两层。'
- en: '*Dropout*—We’ll use 50% dropout on both layers.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout*—我们将对两层都使用 50% dropout。'
- en: The following listing shows a GraphSAGE class with adjustable dropout, layer
    depth, and aggregations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了一个具有可调整 dropout、层深度和聚合的 GraphSAGE 类。
- en: Listing 3.7 GraphSAGE class
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列 3.7 GraphSAGE 类
- en: '[PRE10]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The layer is initialized with number of layers, dropout rate, and aggregation
    type.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 层初始化时使用层数、dropout 率和聚合类型。'
- en: '#2 The loop applies the aggregation to each layer.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 循环将聚合应用于每一层。'
- en: 'We trained three models using the preceding class:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用前面的类训练了三个模型：
- en: '[PRE11]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Table 3.7 summarizes the performance of different GraphSAGE models with various
    aggregation methods and a baseline model using the default mean aggregation. The
    results indicate that all the improved models outperform the baseline in both
    F1 score and log loss. Notably, Model 2, which uses a combination of `'max'`,
    `'sum'`, and `'mean'` aggregations, achieved the highest F1 score of 0.8828\.
    Model 3, with a combination of `SoftmaxAggregation()` and `StdAggregation()`,
    shows the best log loss at 0.5764, suggesting it has the highest prediction certainty
    among the tested configurations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.7 总结了不同聚合方法和默认平均值聚合的 GraphSAGE 模型的性能。结果表明，所有改进的模型在 F1 分数和 log loss 方面都优于基线。值得注意的是，使用
    `'max'`、`'sum'` 和 `'mean'` 聚合组合的模型 2 实现了最高的 F1 分数 0.8828。模型 3 使用 `SoftmaxAggregation()`
    和 `StdAggregation()` 的组合，在 0.5764 的最佳 log loss 下表现最佳，这表明它在测试配置中具有最高的预测确定性。
- en: Table 3.7 Two-layer GraphSAGE models using 50% dropout and different aggregation
    types
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.7 使用 50% dropout 和不同聚合类型的两层 GraphSAGE 模型
- en: '| GraphSAGE Model | Aggregation Type | F1 Score | Log Loss |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE 模型 | 聚合类型 | F1 分数 | Log Loss |'
- en: '| --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Model 1  | `''max''`  | 0.8674  | 0.594  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1  | `''max''`  | 0.8674  | 0.594  |'
- en: '| Model 2  | `[''max'', ''sum'', ''mean'']`  | 0.8876  | 0.660  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2  | `[''max'', ''sum'', ''mean'']`  | 0.8876  | 0.660  |'
- en: '| Model 3  | `[SoftmaxAggregation(), StdAggregation()]`  | 0.8829  | 0.574  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 模型 3  | `[SoftmaxAggregation(), StdAggregation()]`  | 0.8829  | 0.574  |'
- en: '| Baseline model  | Mean (default)  | 0.7406  | 2.1214  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 基线模型 | 平均值（默认） | 0.7406  | 2.1214  |'
- en: The confusion matrix in figure 3.8 visualizes the classification performance
    of Model 1 with max aggregation. The majority of values are along the diagonal,
    indicating that the model is correctly classifying most instances. However, there
    are off-diagonal elements, representing misclassifications, for example, instances
    of class 0 being classified as class 1 or vice versa. The frequency and spread
    of these misclassifications highlight areas where the model struggles. Additionally,
    using the bar on the side indicating the count of members per class, the confusion
    matrix shows how these different classes are distributed. Some classes have higher
    counts, while others have significantly lower counts, suggesting class imbalance
    in the dataset.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8中的混淆矩阵可视化了使用最大聚合的模型1的分类性能。大多数值都在对角线上，表明模型正确分类了大多数实例。然而，也存在非对角线元素，代表错误分类，例如，将类别0的实例错误分类为类别1或反之亦然。这些错误分类的频率和分布突出了模型在哪些方面存在困难。此外，使用侧边栏上的条形图表示每个类别的成员数量，混淆矩阵显示了这些不同类别的分布情况。有些类别的计数较高，而其他类别的计数则显著较低，这表明数据集中存在类别不平衡。
- en: '![figure](../Images/3-8.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-8.png)'
- en: Figure 3.8 Confusion matrix from the two-layer GraphSAGE model with 50% dropout
    and max aggregation. The strong diagonal pattern indicates good classification
    performance. The sidebar gives a distribution of the classes, highlighting a class
    imbalance.
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8展示了具有50%丢弃率和最大聚合的两层GraphSAGE模型的混淆矩阵。强烈的对角线模式表明分类性能良好。侧边栏给出了类别的分布，突出了类别不平衡。
- en: Note that, in all of this, we’ve been using less than 1% of the dataset’s nodes,
    arbitrarily chosen by index order. Increasing the number of nodes would improve
    the performance of our models. Additionally, selecting a subgraph in a more meaningful
    way while keeping the number of nodes the same could also increase performance.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这整个过程中，我们只使用了数据集节点中不到1%的部分，这些节点是任意按照索引顺序选择的。增加节点的数量将提高我们模型的表现。此外，在保持节点数量不变的同时，以更有意义的方式选择子图也可以提高性能。
- en: While the current models show significant improvement, several other strategies
    can be considered to further enhance performance. Increasing the dataset size
    by using a larger subset can provide more training data, potentially improving
    model generalization. Refining subgraph selection based on domain knowledge or
    using graph sampling techniques can ensure that more meaningful data is used for
    training. Hyperparameter optimization, systematically tuning hyperparameters using
    tools such as Hyperopt, can help find the optimal settings for the model. Hyperopt
    allows for efficient searching of the hyperparameter space using algorithms such
    as Bayesian optimization. Exploring more sophisticated aggregation functions or
    custom aggregations tailored to the specific characteristics of the dataset can
    also yield improvements. Additionally, implementing regularization methods such
    as L2 regularization or gradient clipping can stabilize training and prevent over-fitting.
    Graph preprocessing techniques, such as normalization, feature engineering, and
    dimensionality reduction on graph features, can enhance the quality of input data,
    further boosting model performance. Next, we’ll select the model that performs
    highest on log loss to generate another product bundle.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然当前的模型显示出显著的改进，但还可以考虑其他几种策略来进一步提高性能。通过使用更大的子集来增加数据集的大小可以提供更多的训练数据，从而可能提高模型的泛化能力。根据领域知识细化子图选择或使用图采样技术可以确保使用更有意义的数据进行训练。使用Hyperopt等工具系统地调整超参数可以帮助找到模型的最佳设置。Hyperopt允许使用贝叶斯优化等算法高效地搜索超参数空间。探索更复杂的聚合函数或针对数据集特定特征的定制聚合也可以带来改进。此外，实现正则化方法，如L2正则化或梯度裁剪，可以稳定训练并防止过拟合。图预处理技术，如归一化、特征工程和图特征的降维，可以提高输入数据的质量，进一步提升模型性能。接下来，我们将选择在日志损失上表现最高的模型来生成另一个产品包。
- en: 3.3.4 Revisiting the Marcelina product bundle
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 重访Marcelina产品包
- en: The models have markedly improved from our baselines in section 3.1\. Let’s
    revisit the product bundling problem and recommend one for our product manager
    based on our refined GraphSAGE model from the earlier section. Using the process
    from section 3.1.5 results in the bundle in figure 3.9, which is displayed with
    the original bundle for comparison.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在3.1节中的基线模型上有了显著改进。让我们重新审视产品捆绑问题，并根据前面章节中改进的GraphSAGE模型为我们的产品经理推荐一个方案。使用3.1.5节中的过程得到的捆绑方案如图3.9所示，并与原始捆绑方案进行了比较。
- en: '![figure](../Images/3-9.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-9.png)'
- en: Figure 3.9 Product bundles centered on the Marceline product. The upper bundle
    is from the improved model from section 3.3.3, while the lower bundle is from
    the baseline model of section 3.1.5\. The new recommendations are members of the
    Toy & Games, Books, and Movies & TV categories.
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9以Marceline产品为中心的产品捆绑。上面的捆绑方案来自3.3.3节的改进模型，而下面的捆绑方案来自3.1.5节的基线模型。新的推荐方案是玩具与游戏、书籍和电影与电视类别的成员。
- en: 'What do you think of this new bundle? Is it an improvement, that is, more likely
    to drive purchases than the former bundle? This new bundle incorporates items
    from Toys & Games, Books, and Movies & TV categories, which is a diverse product
    selection. The introduction of the *Wild Kratts: Wildest Animal Adventures* DVD
    alongside the adventure book *The Sword of Shannara* and action figures reflects
    a pivot toward a more family-oriented and child-friendly product mix.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 您对这款新套餐有何看法？这是否是一个改进，也就是说，比之前的套餐更有可能推动购买？这个新套餐包含了玩具与游戏、书籍和电影与电视类别的商品，这是一个多样化的产品选择。在冒险书籍《沙纳拉之剑》和动作人偶的旁边引入《野生克鲁特：最野的动物冒险》DVD，反映了向更家庭化和儿童友好的产品组合的转变。
- en: This new bundle’s potential for driving purchases is grounded in the enhanced
    understanding of customer purchase behaviors and preferences captured by the updated
    model. The bundle seems well-suited for gifting purposes, catering to both the
    collectors of pop culture memorabilia (e.g., the Marceline figure and related
    collectibles) and young fans of fantasy and adventure narratives.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新套餐推动购买的可能性基于对客户购买行为和偏好的更新模型所捕捉到的更深入的理解。这个套餐似乎非常适合送礼目的，既满足了流行文化纪念品收藏者（例如Marceline人偶和相关收藏品）的需求，也满足了年轻奇幻和冒险叙事粉丝的需求。
- en: 'The shift from a more generic collection of toys to a focused, theme-oriented
    bundle could likely increase its attractiveness as a purchase. The inclusion of
    both entertainment (*Wild Kratts: Wildest Animal Adventures* DVD) and literary
    (*The Sword of Shannara*) elements, in addition to the collectible figures, provides
    a more comprehensive entertainment experience centered around popular themes of
    adventure and exploration. This could appeal to parents looking for engaging and
    themed gifts that also offer educational value, such as the animal- and nature-related
    content of Wild Kratts.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从更通用的玩具集合转向一个专注的、主题导向的捆绑方案可能会增加其作为购买吸引力的可能性。除了收藏品（如Marceline人偶和相关收藏品）之外，还包括娱乐（《野生克鲁特：最野的动物冒险》DVD）和文学（《沙纳拉之剑》）元素，提供了一个围绕流行的冒险和探索主题的更全面的娱乐体验。这可能吸引那些寻找有吸引力和主题礼物，同时也提供教育价值的父母，例如《野生克鲁特》中关于动物和自然的内容。
- en: It’s crucial to consider the psychological effect of a well-curated bundle.
    By aligning the products more closely with identified customer interests and cross-selling
    patterns, the bundle not only caters to existing demand but also encourages additional
    purchases by enhancing perceived value by how the bundled items complement each
    other.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一个精心策划的套餐的心理效应是至关重要的。通过将产品与已识别的客户兴趣和交叉销售模式更紧密地对齐，这个套餐不仅满足了现有需求，而且通过增强捆绑商品之间的互补性，提高了感知价值，从而鼓励了额外的购买。
- en: Ultimately, the decision on whether this new bundle is an improvement over the
    original should be validated through customer feedback and sales data. Tracking
    the sales performance of both bundles (as well as bundles suggested by human product
    managers) and gathering direct customer insights through surveys or A/B testing
    would be beneficial to quantitatively assess which bundle performs better in terms
    of sales and customer satisfaction. This data-driven approach will confirm the
    theoretical benefits of the advanced modeling techniques used in the new bundle’s
    creation.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，关于这个新捆绑包是否比原始版本有所改进的决定，应该通过客户反馈和销售数据进行验证。跟踪两个捆绑包（以及由人类产品经理建议的捆绑包）的销售表现，并通过调查或
    A/B 测试收集直接客户洞察，将有助于定量评估哪个捆绑包在销售和客户满意度方面表现更好。这种数据驱动的方法将证实新捆绑包创建中使用的先进建模技术的理论优势。
- en: With this, we conclude the hands-on product example of this chapter. The next
    two sections are optional as they dive deeper into the theory of convolutional
    GNNs and take a closer look at the Amazon Products dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们结束了本章的动手产品示例。接下来的两个部分是可选的，因为它们深入探讨了卷积 GNN 的理论，并更详细地研究了 Amazon 产品数据集。
- en: 3.4 Under the hood
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 内部机制
- en: Now that we’ve created and refined a working convolutional GNN, let’s dig deeper
    into the elements of a GNN to better understand how they work. Such knowledge
    can help when we want to design new GNNs or troubleshoot a GNN.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建并改进了一个工作的卷积 GNN，让我们更深入地研究 GNN 的元素，以更好地理解它们是如何工作的。这种知识有助于我们在设计新的 GNN
    或调试 GNN 时。
- en: In chapter 2, we introduced the idea of using GNN layers to produce a prediction
    or create embeddings using message passing. Here’s that architecture diagram again,
    reproduced in figure 3.10\.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们介绍了使用 GNN 层通过消息传递来生成预测或创建嵌入的想法。这是那个架构图再次呈现，如图 3.10 所示。
- en: Let’s get below the surface of a GNN layer and examine its elements. Then, we’ll
    tie this to the concept of aggregation functions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入到 GNN 层的表面之下，检查其元素。然后，我们将将其与聚合函数的概念联系起来。
- en: '![figure](../Images/3-10.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-10.png)'
- en: Figure 3.10 Node embedding architecture diagram from chapter 2
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.10 来自第二章的节点嵌入架构图
- en: 3.4.1 Convolution methods
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 卷积方法
- en: 'Let’s first consider one of the most popular architectures for deep learning,
    the convolutional neural network (CNN). CNNs are typically used for computer vision
    tasks such as segmentation or classification. A CNN layer can be thought of as
    having a sequence of operations that are applied to input data:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑深度学习中最受欢迎的架构之一，卷积神经网络（CNN）。CNN 通常用于计算机视觉任务，如分割或分类。可以将 CNN 层视为对输入数据应用一系列操作的序列：
- en: 'Layer: Filter → Activation function → Pooling'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 层：滤波器 → 激活函数 → 池化
- en: 'The output of each entire layer is some transformed data that makes some downstream
    task easier or more successful. These transformation operations include the following:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 每个整个层的输出是一些经过变换的数据，这使得某些下游任务更容易或更成功。这些变换操作包括以下内容：
- en: '*Filter (or kernel operation)*—A process that transforms the input data. The
    filter is used to highlight some specific features of the input data and consists
    of learnable weights that are optimized by an objective or loss function.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*滤波器（或核操作）*——一种变换输入数据的过程。滤波器用于突出输入数据的一些特定特征，并包含通过目标或损失函数优化的可学习权重。'
- en: '*Activation Function*—A nonlinear transformation applied to the filter output.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*激活函数*——应用于滤波器输出的非线性变换。'
- en: '*Pooling*—An operation that reduces the size of the filter output for subsequent
    learning tasks.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*池化*——一种减少后续学习任务中滤波器输出大小的操作。'
- en: 'CNNs and many GNNs share a common foundation: the concept of convolution. You
    read about the concept of convolution when we discussed the three operations used
    in a CNN. Convolution in both CNNs and GNNs is all about learning by establishing
    *hierarchies of localized patterns* in the data. For CNNs this might be used for
    image classification, whereas a convolutional GNN, such as a GCN, might use convolution
    to predict features of nodes. To emphasize this point, CNNs apply convolution
    to a fixed grid of pixels to identify patterns in the grid. GCN models apply convolution
    to graphs of nodes to identify patterns in the graph.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs和许多GNNs有一个共同的基础：卷积的概念。当讨论CNN中使用的三个操作时，你了解了卷积的概念。在CNNs和GNNs中，卷积都是通过在数据中建立*局部模式的层次结构*来学习的。对于CNNs，这可以用于图像分类，而卷积GNN，如GCN，可能使用卷积来预测节点的特征。为了强调这一点，CNNs将卷积应用于固定像素网格以识别网格中的模式。GCN模型将卷积应用于节点图以识别图中的模式。
- en: 'I referred to the *concept* of convolution in the previous paragraph because
    the convolution can be implemented in different ways. Theoretically, convolution
    relates to the mathematical convolution operator, which we’ll be discussing in
    more detail shortly. For GNNs, convolution can be separated into spatial and spectral
    methods [1, 5, 6]:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一个段落中提到了卷积的概念，因为卷积可以以不同的方式实现。从理论上讲，卷积与数学上的卷积算子相关，我们将在稍后更详细地讨论这一点。对于GNNs，卷积可以分为空间和频谱方法[1,
    5, 6]：
- en: '*Spatial*—Sliding a window (filter) across a graph.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*空间*—在图上滑动一个窗口（过滤器）。'
- en: '*Spectral*—Filtering a graph signal using spectral methods.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*频谱*—使用频谱方法过滤图信号。'
- en: Spatial methods
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 空间方法
- en: In traditional deep learning, convolutional processes learn data representations
    by applying a special filter called a *convolutional kernel* to input data. This
    kernel is smaller in size than the input data and is applied by moving it across
    the input data. This is shown in figure 3.11, where we apply our convolutional
    kernel (the matrix in the center) to an image of a lion. The resulting image has
    been inverted due to our convolutional kernel, which has negative values for all
    non-center elements. We can see that some of the features have been emphasized,
    such as the outline of the lion. This highlights the filtering aspect of convolutions.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的深度学习中，卷积过程通过将一个称为*卷积核*的特殊过滤器应用于输入数据来学习数据表示。这个内核的大小小于输入数据，并且通过在其上移动来应用。这如图3.11所示，我们应用我们的卷积核（中间的矩阵）到一只狮子的图像上。由于我们的卷积核的所有非中心元素都有负值，结果图像已经被反转。我们可以看到一些特征被强调，例如狮子的轮廓。这突出了卷积的过滤方面。
- en: '![figure](../Images/3-11.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-11.png)'
- en: Figure 3.11 A convolution of an input image (left). The kernel (middle) is passed
    over the image of an animal, resulting in a distinct representation (right) of
    the input image. In a deep learning process, the parameters of the filter (the
    numbers in the matrix) are learned parameters.
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11 输入图像的卷积（左侧）。内核（中间）在动物图像上移动，从而得到输入图像的特定表示（右侧）。在深度学习过程中，过滤器的参数（矩阵中的数字）是学习参数。
- en: This use of convolutional networks is particularly common in the computer vision
    domain. For example, when learning on 2D images, we can apply a simple CNN of
    a few layers. In each layer, we pass a 2D filter (kernel) over each image. The
    3 × 3 filter works on an image many times its size. We can produce learned representations
    of the input image by doing this over successive layers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这种卷积网络的使用在计算机视觉领域尤为常见。例如，当在2D图像上学习时，我们可以应用几层的简单CNN。在每一层中，我们通过每个图像传递一个2D过滤器（内核）。3
    × 3的过滤器在一个比其大得多的图像上多次工作。通过这种方式，我们可以通过连续的层产生输入图像的学习表示。
- en: For graphs, we want to apply this same idea of moving a window across our data,
    but now we need to make adjustments to account for the relational and non-Euclidian
    topology of our data. For images, we’re dealing with rigid 2D grids; for graphs,
    we’re dealing with data that has no fixed shape or order. Without a predefined
    ordering of the nodes in a graph, we use the concept of a *neighborhood*, consisting
    of a starting node, and all of its one-hop neighbors (i.e., all nodes within one
    hop from the central node). Then, our sliding window moves across a graph by moving
    across its node neighborhoods.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图，我们希望应用这种在数据上移动窗口的相同想法，但现在我们需要调整以考虑我们数据的关联和非欧几里得拓扑。对于图像，我们处理的是刚性的二维网格；对于图，我们处理的是没有固定形状或顺序的数据。在没有预定义图节点顺序的情况下，我们使用*邻域*的概念，包括一个起始节点及其所有一跳邻居（即从中心节点出发的一跳范围内的所有节点）。然后，我们的滑动窗口通过移动节点的邻域在图上移动。
- en: In figure 3.12, we see an illustration comparing convolution applied to grid
    data and applied to graph data. In the grid case, pixel values are filtered around
    the nine pixels immediately surrounding the central pixel (marked with a gray
    dot). However, for a graph, node attributes are filtered based on all nodes that
    can be connected by one edge. Once we have the nodes that we’ll be considering,
    we then need to perform some operation on the nodes. This is known as the *aggregation
    operation*; for example, all the node weights in a neighborhood might be averaged
    or summed, or we might take the max value. What is important for graphs is that
    this operation is *permutation invariant*. The order of the nodes shouldn’t matter.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.12中，我们看到一个比较卷积应用于网格数据和应用于图数据的插图。在网格情况下，像素值在围绕中心像素（用灰色点标记）的九个像素周围被过滤。然而，对于图，节点属性是基于可以通过一条边连接的所有节点进行过滤的。一旦我们确定了将要考虑的节点，我们然后需要对节点执行某些操作。这被称为*聚合操作*；例如，一个邻域中所有节点权重可能被平均或求和，或者我们可能取最大值。对于图来说，重要的是这个操作是*置换不变的*。节点的顺序不应该很重要。
- en: '![figure](../Images/3-12.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-12.png)'
- en: Figure 3.12 A comparison of convolution over grid data (left; e.g., a 2D image)
    and over a graph (right).
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12 在网格数据（左；例如，二维图像）和图（右）上应用卷积的比较。
- en: Spectral methods
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 频谱方法
- en: To introduce the second method of convolution, let’s examine the concept of
    a graph signal [6]. In the field of information processing, *signals* are sequences
    that can be examined in either time or frequency domains. When studying a signal
    in the time domain, we consider its dynamics, namely how it changes over time.
    From the frequency domain, we consider how much of the signal lies within each
    frequency band.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍卷积的第二种方法，让我们考察图信号[6]的概念。在信息处理领域，*信号*是可以考察时间或频率域的序列。当在时间域研究信号时，我们考虑其动态性，即它是如何随时间变化的。从频率域来看，我们考虑信号中有多少位于每个频率带内。
- en: We can also study the signal of a graph in an analogous way. To do this, we
    define a graph signal as a vector of node features. Thus, for a given graph, its
    set of node weights can be used to construct its signal. As a visual example,
    in figure 3.13, we have a graph with values associated with each node, where the
    height of each respective bar represents some node feature.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以以类似的方式研究图的信号。为此，我们定义图信号为节点特征的向量。因此，对于给定的图，其节点权重集可以用来构建其信号。作为一个视觉示例，在图3.13中，我们有一个与每个节点相关联的值的图，其中每个相应的条的高度代表某些节点特征。
- en: '![figure](../Images/3-13.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-13.png)'
- en: Figure 3.13 A random positive graph signal on the vertices of the graph. The
    height of each vertical bar represents the signal value at the node where the
    bar originates.
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13 图的顶点上的随机正图信号。每个垂直条的高度代表条形起始节点的信号值。
- en: To operate on this graph signal, we represent the graph signal as a matrix,
    where each row is a set of features associated with a particular node. We can
    then apply operations from signal processing on the graph matrix. One critical
    operation is that of the Fourier transform. The Fourier transform can express
    a graph signal, its set of node features, into a frequency representation. Conversely,
    an inverse Fourier transform will revert the frequency representation into a graph
    signal.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要操作这个图信号，我们将图信号表示为一个矩阵，其中每一行是与特定节点相关联的一组特征。然后，我们可以在图矩阵上应用信号处理操作。一个关键操作是傅里叶变换。傅里叶变换可以将图信号及其节点特征集表示为频率表示。相反，逆傅里叶变换将频率表示转换回图信号。
- en: 'Above and beyond: Limitations of traditional deep learning methods to graphs'
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 除此之外：传统深度学习方法在图上的局限性
- en: Why can’t we apply CNNs directly to a graph structure? The reason is because
    graph representations have an ambiguity that image representations don’t. CNNs,
    and traditional deep learning tools in general, can’t resolve this ambiguity.
    A neural network that can deal with this ambiguity is said to be *permutation
    equivariant* or *permutation invariant*.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们不能直接将CNN应用于图结构？原因是图表示具有图像表示所不具备的歧义。CNN以及传统的深度学习工具通常无法解决这种歧义。能够处理这种歧义的神经网络被称为**排列等变**或**排列不变**。
- en: 'Let’s illustrate the ambiguity of a graph versus an image by considering the
    image of the lion shown earlier. A simple representation of this set of pixels
    is as a 2D matrix (with dimensions for height and width). This representation
    will be unique: if we swap out two rows of the image, or two columns, we don’t
    have an equivalent image. Similarly, if we swap out two columns or rows of the
    matrix representation of the image (as shown in figure 3.14), we don’t have an
    equivalent matrix.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑之前展示的狮子图像来阐述图与图像之间的歧义。这个像素集的简单表示是一个二维矩阵（具有高度和宽度的维度）。这种表示将是唯一的：如果我们交换图像的两行或两列，我们不会得到一个等效的图像。同样，如果我们交换图像矩阵表示中的两列或两行（如图3.14所示），我们也不会得到一个等效的矩阵。
- en: '![figure](../Images/3-14.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-14.png)'
- en: Figure 3.14 The image of a lion is unique (left). If we swap out two columns
    (right), we end up with a distinct photo with respect to the original.
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14 狮子的图像是唯一的（左）。如果我们交换两列（右），我们最终得到一个与原始图像不同的独特照片。
- en: This isn’t the case with a graph. Graphs can be represented by adjacency matrices
    (described in chapter1 and appendix A), such that each row and column element
    stands for the relation between two nodes. If an element is nonzero, it means
    that the row node and column node are linked. Given such a matrix, we can repeat
    our previous experiment and swap out two rows as we did with the image. Unlike
    the case of the image, we end up with a matrix that represents the graph we started
    with. We can do any number of permutations or rows and columns and end up with
    a matrix that represents the same graph.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图来说，情况并非如此。图可以通过邻接矩阵（在第1章和附录A中描述）来表示，其中每一行和每一列的元素代表两个节点之间的关系。如果一个元素非零，这意味着行节点和列节点是相连的。给定这样一个矩阵，我们可以重复我们之前的实验，并像处理图像那样交换两行。与图像的情况不同，我们最终得到一个代表我们最初图的矩阵。我们可以进行任意数量的排列或交换行和列，并得到一个代表相同图的矩阵。
- en: Returning to the convolution operation, to successfully apply a convolutional
    filter or a CNN to the graph’s matrix representation, such an operation or layer
    would have to yield the same result no matter the ordering of the adjacency matrix
    (because every ordering describes the same thing). CNNs fail in this respect.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 回到卷积操作，为了成功地将卷积滤波器或CNN应用于图的矩阵表示，这样的操作或层必须无论邻接矩阵的顺序如何都能得到相同的结果（因为每种顺序都描述了相同的事物）。CNN在这方面失败了。
- en: 'Finding convolutional filters that can be applied to graphs has been solved
    in a variety of ways. In this chapter, we examined two ways this has been done:
    spatial and spectral methods. (For a deeper discussion and derivation of convolutional
    filters applied to graphs, see [7].)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 找到可以应用于图的卷积滤波器已经以多种方式解决了。在本章中，我们考察了两种实现方式：空间方法和频谱方法。（对于卷积滤波器应用于图的更深入讨论和推导，请参见[7]。）
- en: 3.4.2 Message passing
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 消息传递
- en: Both spatial and spectral approaches describe how we can combine data on our
    graph. Spatial methods look at the graph structure and combine data across spatial
    neighborhoods. Spectral methods look at the graph signal and use methods from
    signal processing, such as the Fourier transform to combine data across the graph.
    Implicit to both these methods is the idea of message passing.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 空间和频谱方法都描述了我们可以如何将我们的图上的数据结合起来。空间方法关注图的结构，并在空间邻域间组合数据。频谱方法关注图信号，并使用信号处理方法，例如傅里叶变换，来在图上组合数据。这两种方法都隐含了消息传递的概念。
- en: In chapter 3, we introduced message passing as a way to extract more information
    from our graphs. Let’s go step-by-step and consider what message passing does.
    First, the messages from each node or edge are collected from neighboring nodes.
    Second, we transform these messages to encode the data as feature vectors. Finally,
    we update the node or edge data to include these messages. The result is that
    each node or edge ends up containing individual data as well as data from the
    rest of the graph. The amount of data that becomes encoded in these nodes is reflected
    by the number of hops or message-passing steps. This is the same as the number
    of layers in a GNN. In figure 3.15, we show a mental model for message passing.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们介绍了消息传递作为一种从我们的图中提取更多信息的方法。让我们一步一步地考虑消息传递做了什么。首先，从每个节点或边收集消息。其次，我们将这些消息转换为特征向量来编码数据。最后，我们将节点或边数据更新以包含这些消息。结果是，每个节点或边最终包含个别数据以及图中其他部分的数据。编码在这些节点中的数据量反映了跳数或消息传递步骤的数量。这与GNN中的层数相同。在图3.15中，我们展示了消息传递的心理模型。
- en: '![figure](../Images/3-15.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-15.png)'
- en: Figure 3.15 Elements of our message-passing layer. Each message-passing layer
    consists of an aggregation, a transformation, and an update step.
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15 消息传递层的元素。每个消息传递层由一个聚合步骤、一个转换步骤和一个更新步骤组成。
- en: The output of each message-passing layer is a set of embeddings or features.
    In the aggregation step, we gather the messages from the graph neighborhoods.
    In the transformation step, we apply a neural network to the aggregated messages.
    Finally, in the update step, we alter the features of the nodes or edges to include
    the message passing data.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 每个消息传递层的输出是一组嵌入或特征。在聚合步骤中，我们从图邻域收集消息。在转换步骤中，我们将神经网络应用于聚合消息。最后，在更新步骤中，我们改变节点或边的特征以包含消息传递数据。
- en: 'In this way, a GNN layer is similar to a CNN layer. It can be interpreted as
    a sequence of operations that are applied to input data:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，一个GNN层类似于一个CNN层。它可以被解释为一系列应用于输入数据的操作：
- en: 'Layer: Aggregate → Transform → Update'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 层：聚合 → 转换 → 更新
- en: As we explore different GNNs in this book, we’ll return to this set of operations,
    as most types of GNNs can be seen as modifications of these elements. For example,
    in this chapter, you’re learning about GCNs as a specific type of aggregation.
    In the next chapter, you’ll learn about GATs, which combine both transformation
    and aggregation steps by learning how to aggregate messages using an attention
    mechanism.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索本书中的不同GNN时，我们将回到这一组操作，因为大多数类型的GNN都可以被视为这些元素的修改。例如，在本章中，你正在学习GCN作为一种特定的聚合类型。在下一章中，你将学习GATs，它通过学习如何使用注意力机制聚合消息来结合转换和聚合步骤。
- en: To build this message-passing step, let’s work through the preceding process,
    adding more detail. The first two steps can be understood as a type of filter,
    similar to the first step of a conventional neural network. First, we aggregate
    node or edge datausing our *aggregation operator*. For example, we might sum the
    features, average the features, or choose the maximum values. The most important
    thing is that the order of the nodes shouldn’t matter for the final representation.
    The reason that the order shouldn’t matter is that we want our models to be permutation
    equivariant, which means that subtraction or division wouldn’t be suitable.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个消息传递步骤，让我们逐步分析前面的过程，并添加更多细节。前两个步骤可以理解为一种过滤器，类似于传统神经网络的第一个步骤。首先，我们使用我们的**聚合算子**对节点或边数据进行聚合。例如，我们可能对特征求和、平均特征或选择最大值。最重要的是，节点的顺序对于最终表示不应该很重要。顺序不应该很重要的原因是，我们希望我们的模型是排列等变的，这意味着减法或除法可能不合适。
- en: Once we’ve aggregated information from all node or collected all the messages,
    we then transform them into *embeddings* by passing the new messages through a
    neural network and an activation function. Once we have these transformed embeddings,
    we apply an activation function and then combine them with the node or edge data
    and the previous embeddings.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从所有节点或收集了所有消息，我们就通过传递新的消息通过神经网络和激活函数将它们转换成**嵌入**。一旦我们有了这些转换后的嵌入，我们就应用激活函数，然后将它们与节点或边数据和之前的嵌入结合起来。
- en: The activation function is a nonlinear transformation that is applied to the
    transformed and aggregated messages. We need the function to be nonlinear; otherwise,
    the model would be linear, regardless of how many layers it has, similar to a
    linear (or logistic, in our case) regression model. These are standard activation
    functions used in artificial neural networks, such as the ReLU, which is the maximum
    value between zero and the input value. The pooling step then reduces the overall
    size of the filter output for any graph-level learning tasks. For node prediction,
    this can be omitted, which we’ll do here.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是对转换和聚合消息应用的非线性变换。我们需要函数是非线性的；否则，无论有多少层，模型都会是线性的，类似于线性（或在我们的情况下是逻辑回归）模型。这些是在人工神经网络中使用的标准激活函数，例如ReLU，它是零和输入值之间的最大值。池化步骤随后减少了任何图级学习任务中滤波器输出的总体大小。对于节点预测，这可以省略，我们在这里就是这样做的。
- en: 'We can combine the previous description into a single expression for the message-passing
    operation. First, let’s assume that we’re working with node embeddings as we’ll
    do in this chapter. We want to transform the data for node *n* into a node embedding.
    We can do so using the following formula:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的描述组合成一个消息传递操作的单一表达式。首先，让我们假设我们正在使用节点嵌入，正如我们将在本章中所做的那样。我们希望将节点 *n* 的数据转换成节点嵌入。我们可以使用以下公式来完成：
- en: (3.1)
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.1)
- en: '![figure](../Images/Equation-3-1.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-1.png)'
- en: Here, *u* represents the nodes. The learnable weights are given by *W*[a], which
    will be tuned based on the loss function, and *σ* is the activation function.
    To build the embeddings, we need to take all the node data and combine it into
    a single vector. This is where the aggregation function comes in. For GCNs, the
    aggregation operator is summation. Therefore,
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*u* 代表节点。可学习的权重由 *W*[a] 给出，这些权重将根据损失函数进行调整，而 *σ* 是激活函数。为了构建嵌入，我们需要将所有节点数据组合成一个单一的向量。这就是聚合函数发挥作用的地方。对于GCN，聚合操作符是求和。因此，
- en: (3.2)
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.2)
- en: '![figure](../Images/Equation-3-2.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-2.png)'
- en: 'where, for node *u,* *h*[v]is data from node *v* in the neighborhood of node
    *u*, *N(u)*. Combining both equations, we can construct a general formula for
    constructing node embeddings:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于节点 *u*，*h*[v]是节点 *u* 邻域中节点 *v* 的数据，*N(u)*。结合这两个方程，我们可以构建一个构建节点嵌入的一般公式：
- en: (3.3)
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.3)
- en: '![figure](../Images/Equation-3-3.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-3.png)'
- en: 'For the preceding formula, we see that a node and its neighborhood play a central
    part. Indeed, this is one of the main reasons that GNNs have proven to be so successful.
    We also see that we need to make a choice on both our activation function and
    our aggregation function. Finally, these are updated to include the previous data
    at each node:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的公式，我们看到节点及其邻域起着核心作用。确实，这是GNN证明非常成功的主要原因之一。我们还看到，我们需要在激活函数和聚合函数上做出选择。最后，这些更新包括每个节点上的先前数据：
- en: (3.4)
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.4)
- en: '![figure](../Images/Equation-3-4.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-4.png)'
- en: Here, we’re concatenating the messages together. It’s also possible to use other
    methods to update the message information, and the choice depends on the architecture
    used.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将消息连接在一起。也可以使用其他方法来更新消息信息，选择取决于所使用的架构。
- en: This update equation is the essence of message passing. For each layer, we’re
    *updating* all the node data using the *transformed* data that contains all *aggregated*
    messages. If we have only one layer, we perform this operation only once, we’re
    aggregating the information from the neighbors one hop away from our starting
    node. If we run these operations for multiple iterations, we aggregate nodes within
    two hops of our central node into the node feature data. Thus, the number of GNN
    layers is directly tied to the size of the neighborhoods we’re interrogating with
    our model.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新方程是消息传递的本质。对于每一层，我们使用包含所有 *聚合* 消息的 *转换* 数据来 *更新* 所有节点数据。如果我们只有一个层，我们只执行一次这个操作，我们正在从起始节点的一跳邻居中聚合信息。如果我们运行多次迭代这些操作，我们将中央节点两跳内的节点聚合到节点特征数据中。因此，GNN层的数量直接关联到我们用模型询问的邻域大小。
- en: These are the fundamentals for what operations are being performed during a
    message-passing step. The variations of things such as aggregation or activation
    functions highlight key differences in the architecture of a GNN.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是消息传递步骤中执行的操作的基本原则。聚合或激活函数等事物的变化突出了GNN架构中的关键差异。
- en: 3.4.3 GCN aggregation function
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 GCN聚合函数
- en: The key distinction between GCN and GraphSAGE is that they perform different
    aggregation operations. GCN is a spectral-based GNN, whereas GraphSAGE is a spatial
    method. To better understand the difference between the two, let’s look at implementing
    them both.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: GCN与GraphSAGE之间的关键区别在于它们执行不同的聚合操作。GCN是一种基于谱的GNN，而GraphSAGE是一种空间方法。为了更好地理解这两种方法之间的区别，让我们看看如何实现它们。
- en: First, we need to understand how to apply convolution to graphs. Mathematically,
    the convolutional operation can be expressed as the combination of two functions
    that produces a third function as
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要了解如何将卷积应用于图。从数学上讲，卷积操作可以表示为两个函数的组合，产生第三个函数
- en: (3.5)
  id: totrans-392
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.5)
- en: '![figure](../Images/Equation-3-5.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-5.png)'
- en: 'where *f*(*x*) and *h*(*x*) are functions, and the operator represents element-wise
    multiplication. In the context of CNNs, the image and the kernel matrices are
    the functions in equation 3.6:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f*(*x*) 和 *h*(*x*) 是函数，运算符表示逐元素乘法。在CNN的上下文中，图像和核矩阵是方程3.6中的函数：
- en: (3.6)
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.6)
- en: '![figure](../Images/Equation-3-6.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-6.png)'
- en: 'This mathematical operation is interpreted as the kernel sliding over the image,
    as in the sliding window method. We can convert the preceding description to matrices
    or tensors describing our data. To apply the convolution of equation 3.7 to graphs,
    we use the following ingredients:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学运算被解释为核在图像上滑动，就像滑动窗口方法一样。我们可以将前面的描述转换为描述我们数据的矩阵或张量。为了将方程3.7的卷积应用于图，我们使用以下成分：
- en: 'Matrix representations of the graph:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的矩阵表示：
- en: Vector **x** as the graph signal
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量 **x** 作为图信号
- en: Adjacency matrix **A**
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻接矩阵 **A**
- en: Laplacian matrix **L**
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵 **L**
- en: A matrix of eigenvectors of the Laplacian **U**
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉普拉斯算子的特征向量矩阵 **U**
- en: A parameterized matrix for the weights, **H**
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重的参数化矩阵 **H**
- en: 'Fourier transform based on the matrix operations: **U**^T**x**'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于矩阵操作的傅里叶变换：**U**^T**x**
- en: 'This leads to the expression for spectral convolution over a graph:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了在图上的频谱卷积的表达式：
- en: (3.7)
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.7)
- en: '![figure](../Images/Equation-3-7.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-7.png)'
- en: Because this operation isn’t a simple element-wise multiplication, we’re using
    the symbol *[G] to express this operation. Several convolutional-based GNNs build
    on equation 3.8; next, we’ll examine the GCN version.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个操作不是简单的逐元素乘法，所以我们使用符号 *[G] 来表达这个操作。几个基于卷积的GNN基于方程3.8；接下来，我们将检查GCN版本。
- en: GCNs introduced changes to the convolution equation (3.8) to simplify operations
    and to reduce computational cost. These changes include using a filter based on
    a polynomial rather than a set of matrices and limiting the number of hops to
    one. This reduces the computational complexity from quadratic to linear, which
    is significant. However, the key thing to note is that GCNs updated the aggregation
    function that we described earlier. This will still use a summation but includes
    a normalization term.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: GCN对卷积方程（3.8）进行了修改，以简化操作并降低计算成本。这些修改包括使用基于多项式的滤波器而不是一组矩阵，并限制跳数为一跳。这将从二次复杂度降低到线性复杂度，这是一个显著的改进。然而，需要注意的是，GCN更新了我们之前描述的聚合函数。这仍然使用求和，但包括一个归一化项。
- en: Previously, the aggregation operator was summation. This can lead to problems
    in graphs where the degree of nodes can have high variance. If a graph contains
    nodes whose degrees are high, those nodes will dominate. To solve this, one method
    is to replace summation with averaging. The aggregation function is then expressed
    as
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，聚合算子是求和。这可能导致节点度数变化大的图中的问题。如果一个图中包含度数高的节点，这些节点将占主导地位。为了解决这个问题，一种方法是用平均值代替求和。聚合函数随后表示为
- en: (3.8)
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.8)
- en: '![figure](../Images/Equation-3-8.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-8.png)'
- en: Therefore, for GCN message passing, we have
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于GCN消息传递，我们有
- en: (3.9)
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.9)
- en: '![figure](../Images/Equation-3-9.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-9.png)'
- en: where
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*h* is the updated node embedding.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h* 是更新的节点嵌入。'
- en: sigma, *σ*, is a nonlinearity (i.e., activation function) applied to every element.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sigma, *σ*, 是应用于每个元素的非线性（即激活函数）。
- en: W is a trained weight matrix.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W 是一个训练好的权重矩阵。
- en: '|*N *| denotes the count of the elements in the set of graph nodes'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|*N*| 表示图节点集合中元素的数量'
- en: The summed factor,
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 求和因子，
- en: (3.10)
  id: totrans-422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.10)
- en: '![figure](../Images/Equation-3-10.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-10.png)'
- en: 'is a special normalization called *symmetric normalization*. Additionally,
    GCNs include self-loops such that node embeddings include both neighborhood data
    and data from the starting node. So, to implement a GCN, the following operations
    must occur:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 是一种特殊的正则化，称为**对称正则化**。此外，GCN包括自环，使得节点嵌入包含邻域数据和起始节点的数据。因此，要实现GCN，必须发生以下操作：
- en: Graph nodes adjusted to contain self-loops
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整图节点以包含自环
- en: Matrix multiplication of the trained weight matrix and the node embeddings
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练权重矩阵和节点嵌入的矩阵乘法
- en: Normalization operations summing the terms of the symmetric normalization
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化操作是对称正则化项的总和
- en: In figure 3.16, we explain each of the terms used in a message-passing step
    in detail.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.16中，我们详细解释了消息传递步骤中使用的每个术语。
- en: So far this has all been theoretical. Let’s next look at how we implement this
    in PyG.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这些都是理论上的。接下来，让我们看看如何在PyG中实现这些操作。
- en: '![figure](../Images/3-16.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-16.png)'
- en: Figure 3.16 Mapping of key computational operations in the GCN embedding formula
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16 GCN嵌入公式中关键计算操作的映射
- en: 3.4.4 GCN in PyTorch Geometric
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 PyTorch Geometric中的GCN
- en: In the PyG documentation, you can find source code that implements the GCN layer,
    as well as a simplified implementation of the GCN layer. In the following, we’ll
    point out how the source code implements the preceding key operations.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyG文档中，您可以找到实现GCN层的源代码，以及GCN层的简化实现。以下，我们将指出源代码如何实现前面的关键操作。
- en: 'In table 3.8, we break down key steps in the computation of the GCN embeddings
    and tie them to functions in the source code. These operations are implemented
    using a class and a function:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在表3.8中，我们将GCN嵌入计算中的关键步骤分解，并将其与源代码中的函数关联。这些操作通过类和函数实现：
- en: Function `gcn_norm` performs normalization and add self-loops to the graph.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数`gcn_norm`执行正则化和向图中添加自环。
- en: Class `GCNConv` instantiates the GNN layer and performs matrix operations.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类`GCNConv`实例化GNN层并执行矩阵操作。
- en: Table 3.8 Mapping key computational operations in the GCN embedding formula
  id: totrans-437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.8 GCN嵌入公式中的关键计算操作映射
- en: '| Operation | Function/Method |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 函数/方法 |'
- en: '| --- | --- |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Add self-loops to nodes  | `gcn_norm()`, annotated in listing 3.8  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 向节点添加自环 | `gcn_norm()`，列表3.8中的注释 |'
- en: '| Multiply weights and embeddings *W*^(^(*k*)^)*h*[u]  | `GCNConv.__init__`;
    `GCNConv.forward`  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 乘以权重和嵌入 *W*^(^(*k*)^)*h*[u] | `GCNConv.__init__`; `GCNConv.forward` |'
- en: '| Symmetric normalization  | `gcn_norm()`, annotated in listing 3.8  |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 对称正则化 | `gcn_norm()`，列表3.8中的注释 |'
- en: 'In listing 3.8, we show the code in detail for the `gcn_norm` function and
    class and use annotation to highlight the key operations. This normalization function
    is a key aspect for the GCN architecture. The `gcn_norm` arguments are as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表3.8中，我们详细展示了`gcn_norm`函数和类的代码，并使用注释突出关键操作。这个正则化函数是GCN架构的关键方面。`gcn_norm`的参数如下：
- en: '`edge_index`—The node representations are in a tensor or sparse tensor form.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_index`—节点表示以张量或稀疏张量形式存在。'
- en: '`edge_weight`—An array of one-dimensional edge weights is optional.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_weight`—一个可选的一维边权重数组。'
- en: '`num_nodes`—This is a dimension of the input graph.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_nodes`—这是输入图的维度。'
- en: '`improved`—This introduces an alternative method to add self-loops from the
    Graph U-Nets paper [8].'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`improved`—这引入了从Graph U-Nets论文[8]中引入的添加自环的替代方法。'
- en: '`Add_self_loops`—Adding self-loops is the default, but it’s optional.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Add_self_loops`—添加自环是默认操作，但它是可选的。'
- en: Listing 3.8 The `gcn_norm` function
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8 `gcn_norm`函数
- en: '[PRE12]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Performs symmetric normalization of the input graph and adds a self-loop
    to the input graph'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 对输入图执行对称正则化，并在输入图中添加自环'
- en: '#2 The fill_value parameter is used in the alternative self-loop operation.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 fill_value参数用于替代自环操作。'
- en: '#3 If the graph input is a sparse tensor, the first block of code in the if-statement
    will apply. Otherwise, the second will apply.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果图输入是稀疏张量，则if语句中的第一块代码将应用。否则，将应用第二块。'
- en: In practice, we can simplify the implementation of the normalization considerably
    by using some functions from PyTorch and PyG. In listing 3.9, we show a shortened
    version of normalizing the adjacency matrix. First, we compute the in-degree for
    each node and then calculate the inverse square root. We then use this to create
    a new edge weighting and apply the degree-based inverse square root to this weighting.
    Finally, we create a sparse tensor that represents the adjacency matrix and assign
    this to our data.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以通过使用 PyTorch 和 PyG 的一些函数来显著简化归一化的实现。在列表 3.9 中，我们展示了归一化邻接矩阵的简短版本。首先，我们计算每个节点的入度，然后计算逆平方根。然后我们使用这个逆平方根来创建新的边权重，并将基于度数的逆平方根应用于这个权重。最后，我们创建一个表示邻接矩阵的稀疏张量，并将其分配给我们的数据。
- en: Listing 3.9 Normalizing using PyTorch and PyG
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9 使用 PyTorch 和 PyG 进行归一化
- en: '[PRE13]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Assumes node indices start from 0'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 假设节点索引从 0 开始'
- en: '#2 Computes in-degree for each node'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算每个节点的入度'
- en: '#3 Computes the degree-based inverse square'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算基于度的逆平方'
- en: '#4 Creates a new edge_weight tensor'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建一个新的边权重张量'
- en: '#5 Applies deg_inv_sqrt to edge weights'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将逆平方根应用于边权重'
- en: '#6 Assumes node indices start from 0'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 假设节点索引从 0 开始'
- en: '#7 Creates a sparse tensor and assigns to data'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 创建一个稀疏张量并将其分配给数据'
- en: In the following listing, we have excerpts of the `GCNConv` class, which calls
    on the `gcn_norm` function as well as the matrix operations.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们提供了 `GCNConv` 类的摘录，该类调用了 `gcn_norm` 函数以及矩阵操作。
- en: Listing 3.10 The `GCNConv` class
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.10 `GCNConv` 类
- en: '[PRE14]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 The forward propagation function performs symmetric normalization given
    one of two options: that the input graph is a tensor or a sparse tensor. The source
    code for a tensor input is included here.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 前向传播函数执行对称归一化，给定两种选择之一：输入图是一个张量或稀疏张量。这里包含了张量输入的源代码。'
- en: '#2 Linear transformation of the node feature matrix'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 节点特征矩阵的线性变换'
- en: '#3 Message propagation'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 消息传播'
- en: '#4 There is an optional additive bias to the output.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 输出可选的加性偏置。'
- en: 3.4.5 Spectral vs. spatial convolution
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 光谱卷积与空间卷积
- en: 'In the previous section, we talked about interpreting convolution in two ways:
    (1) via a thought experiment of sliding a window filter across part of a graph
    consisting of a local neighborhood of linked nodes, and (2) by processing graph
    signal data through a filter. We also discussed how these two interpretations
    highlight two branches of convolutional GNNs: the spatial method and the spectral
    method. Sliding window and other spatial methods rely on a graph’s geometrical
    structure to perform convolution. Spectral methods instead use graph signal filters.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了两种解释卷积的方式：(1) 通过在由链接节点局部邻域组成的图的一部分上滑动窗口滤波器的思维实验，以及(2) 通过滤波器处理图信号数据。我们还讨论了这两种解释如何突出了卷积
    GNN 的两个分支：空间方法和光谱方法。滑动窗口和其他空间方法依赖于图的几何结构来执行卷积。相反，光谱方法使用图信号滤波器。
- en: There is no clear demarcation between the spectral and spatial methods, and
    often one type can be interpreted as the other. For example, one contribution
    of GCN is the demonstration that its spectral derivation could be interpreted
    in a spatial way. However, at the time of writing, spatial methods are preferred
    because they have fewer restrictions and, in general, offer less computational
    complexity. We’ve highlighted additional aspects of both spectral and spatial
    methods in table 3.9.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 光谱方法和空间方法之间没有明确的界限，通常一种类型可以解释为另一种类型。例如，GCN 的一个贡献是证明了其光谱推导可以以空间方式解释。然而，在撰写本文时，空间方法更受欢迎，因为它们有更少的限制，并且通常具有更低的计算复杂度。我们在表
    3.9 中突出了光谱和空间方法的附加方面。
- en: Table 3.9 A comparison of spectral and spatial convolutional methods
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.9 光谱卷积与空间卷积方法的比较
- en: '| Spectral | Spatial |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 光谱 | 空间 |'
- en: '| --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Operation: performing a convolution using a graph’s eigenvalues  | Operation:
    aggregation of node features in node neighborhoods  |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 操作：使用图的特征值进行卷积 | 操作：在节点邻域中聚合节点特征 |'
- en: '| • Must be undirected • Operation dependent on node features'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 必须是无向的 • 操作依赖于节点特征'
- en: • Generally less computationally efficient
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: • 通常计算效率更低
- en: '| • Not required to be undirected • Operation not dependent on node features'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 不需要是无向的 • 操作不依赖于节点特征'
- en: • Generally more computationally efficient
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: • 通常计算效率更高
- en: '|'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.4.6 GraphSAGE aggregation function
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.6 GraphSAGE 聚合函数
- en: GraphSAGE improved on the computational cost of GCNs by limiting the number
    of neighboring nodes used in the aggregation operation. Instead, GraphSAGE aggregates
    from a randomly selected sample of the neighborhood. The aggregation operator
    is more flexible (e.g., it can be a summation or an average), but the messages
    that are considered are now only a subset of all messages. Mathematically, we
    can write this as
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE通过限制聚合操作中使用的邻居节点数量来改进GCN的计算成本。相反，GraphSAGE从邻居的随机样本中进行聚合。聚合算子更加灵活（例如，可以是求和或平均值），但现在考虑的消息只是所有消息的一个子集。从数学上讲，我们可以将其表示为
- en: (3.11)
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.11)
- en: '![figure](../Images/Equation-3-11.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-11.png)'
- en: where Ɐ*u* ϵ *S* denotes that the neighborhood is picked from a random sample,
    *S,* of the total neighborhood. From the GraphSAGE paper [2], we have the general
    embedding updating process, which the paper introduces as Algorithm 1, reproduced
    here in figure 3.17.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Ɐ*u* ϵ *S* 表示邻居是从总邻居的随机样本 *S* 中选择的。从GraphSAGE论文[2]中，我们有通用的嵌入更新过程，该过程在论文中作为算法1介绍，此处以图3.17的形式重现。
- en: '![figure](../Images/3-17.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-17.png)'
- en: Figure 3.17 Algorithm 1, the GraphSAGE embedding generation algorithm from the
    GraphSAGE paper [2]
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.17 算法1，GraphSAGE嵌入生成算法，来自GraphSAGE论文[2]
- en: 'The basics of this algorithm can be described as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的基本原理可以描述如下：
- en: 'For every layer/iteration *and* for every node:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一层/迭代以及每个节点：
- en: Aggregate the embeddings of the neighbors.
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚合邻居的嵌入。
- en: Concatenate neighbor embeddings with the central node.
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将邻居嵌入与中心节点连接起来。
- en: Matrix multiply that concatenation with the Weights matrix.
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该连接与权重矩阵相乘。
- en: Multiply that result with an activation function.
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该结果与激活函数相乘。
- en: Apply a normalization.
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用归一化。
- en: Update node features, z, with node embeddings, h.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用节点嵌入 h 更新节点特征 z。
- en: 'Let’s take a closer look at what this means for the message-passing step. We’ve
    defined message passing for GraphSAGE as follows:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这将对消息传递步骤意味着什么。我们已将GraphSAGE的消息传递定义为如下：
- en: (3.12)
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.12)
- en: '![figure](../Images/Equation-3-13.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-13.png)'
- en: If we choose the mean as the aggregation function, this becomes
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择平均值作为聚合函数，则变为
- en: (3.13)
  id: totrans-502
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.13)
- en: '![figure](../Images/Equation-3-14.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-14.png)'
- en: For implementation, we can further reduce this to
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实现，我们可以进一步将其简化为
- en: (3.14)
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.14)
- en: '![figure](../Images/Equation-3-15.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-3-15.png)'
- en: where *x'*[i] denotes the generated central node embeddings, and *x*[i] and
    *x*[j] are the input features of the central and neighboring nodes, respectively.
    The weight matrices are applied to both central nodes and neighboring nodes, as
    shown in figure 3.18, but only the neighboring nodes have an aggregation operator
    (in this case, the mean).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x'*[i] 表示生成的中心节点嵌入，而 *x*[i] 和 *x*[j] 分别是中心节点和邻居节点的输入特征。权重矩阵应用于中心节点和邻居节点，如图3.18所示，但只有邻居节点具有聚合算子（在这种情况下，平均值）。
- en: '![figure](../Images/3-18.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-18.png)'
- en: Figure 3.18 Mapping key computational operations in the GraphSAGE embedding
    formula
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.18 GraphSAGE嵌入公式中的关键计算操作映射
- en: We’ve now seen all the main features of the GraphSAGE algorithm. Let’s next
    look at how we implement this in PyG.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了GraphSAGE算法的所有主要特性。接下来，让我们看看如何在PyG中实现它。
- en: 3.4.7 GraphSAGE in PyTorch Geometric
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.7 PyTorch Geometric中的GraphSAGE
- en: In table 3.10, we break down the key operations and where they occur in PyG’s
    GraphSAGE class. The key operations are aggregation of the neighbor embeddings,
    concatenation of a node’s neighbors’ embeddings with that node’s embeddings, multiplication
    of weights with the concatenation, and application of an activation function.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在表3.10中，我们分解了PyG的GraphSAGE类中的关键操作及其发生的位置。关键操作包括邻居嵌入的聚合、节点邻居嵌入与节点嵌入的连接、权重与连接的乘法以及激活函数的应用。
- en: Table 3.10 Mapping key computational operations in the GCN embedding formula
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.10 GCN嵌入公式中的关键计算操作映射
- en: '| Operation | Function/Method |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 函数/方法 |'
- en: '| --- | --- |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Aggregate the embeddings of the neighbors (sum, mean, or other).  | `SAGEConv.message_and_aggregate`  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 聚合邻居的嵌入（求和、平均值或其他）。 | `SAGEConv.message_and_aggregate` |'
- en: '| Concatenate neighbor embeddings with that of the central node.  | `SAGEConv.forward`  |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 将邻居嵌入与中心节点的嵌入连接起来。 | `SAGEConv.forward` |'
- en: '| Matrix multiply that concatenation with the Weights matrix.  | `SAGEConv.message_and_aggregate`  |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 将该连接与权重矩阵相乘。 | `SAGEConv.message_and_aggregate` |'
- en: '| Apply an activation function.  | If the `project` parameter is set to `True`,
    done in `SAGEConv.forward`  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 应用激活函数。  | 如果`project`参数设置为`True`，则在`SAGEConv.forward`中完成  |'
- en: '| Apply a normalization.  | `SAGEConv.forward`  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 应用归一化。  | `SAGEConv.forward`  |'
- en: For GraphSAGE, PyG also has source code to implement this layer in the `SAGEConv`
    class, excerpts of which are shown in the following listing.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GraphSAGE，PyG还提供了在`SAGEConv`类中实现此层的源代码，以下列出其中的一些摘录。
- en: Listing 3.11 The GraphSAGE class
  id: totrans-522
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.11 GraphSAGE类
- en: '[PRE15]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 If the project parameter is set to True, this applies a linear transformation
    with an activation function (ReLU, in this case) to the neighbor nodes.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果project参数设置为True，则将对邻居节点应用带有激活函数（在这种情况下为ReLU）的线性变换。'
- en: '#2 Propagates messages and applies a linear transformation'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 传播消息并应用线性变换'
- en: '#3 Assigns the root node to a variable'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将根节点分配给一个变量'
- en: '#4 If the root_weight parameter is set to True and a root node exists, this
    will add (concatenate) the transformed root node features to the output.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 如果root_weight参数设置为True且存在根节点，则将转换后的根节点特征添加（连接）到输出中。'
- en: '#5 If the normalize parameter is set to True, L2 normalization will be applied
    to the output features.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果normalize参数设置为True，则将对输出特征应用L2归一化。'
- en: '#6 Matrix multiplication with an aggregation. Setting the aggr parameter establishes
    the aggregation scheme (e.g., mean, max, lstm; default is add). adj_t is the sparse
    matrix representation of the input; using such a representation speeds up calculations.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 使用聚合的矩阵乘法。设置aggr参数建立聚合方案（例如，平均值、最大值、lstm；默认为add）。adj_t是输入的稀疏矩阵表示；使用这种表示可以加快计算速度。'
- en: 3.5 Amazon Products dataset
  id: totrans-530
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 亚马逊产品数据集
- en: In both this chapter and chapter 5, we use the Amazon Products dataset [9].
    This dataset explores product relationships, particularly co-purchases, which
    are products purchased in the same transaction. This co-purchase data is a great
    dataset for benchmarking methods for predicting both nodes and edges. We give
    a bit more information about the dataset in this section.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和第5章中，我们使用亚马逊产品数据集[9]。该数据集探讨了产品关系，特别是共同购买，即在同一交易中购买的产品。这种共同购买数据是预测节点和边的方法的基准数据集。在本节中，我们提供了关于数据集的一些更多信息。
- en: To illustrate the concept of co-purchases, in figure 3.19, we show six example
    co-purchase images for an online customer. For each product, we include a picture,
    a plain text product label, and a bold text category label.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明共同购买的概念，在图3.19中，我们展示了六个在线客户的共同购买示例图像。对于每个产品，我们包括一张图片、一个纯文本产品标签和一个粗体文本类别标签。
- en: '![figure](../Images/3-19.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-19.png)'
- en: Figure 3.19 Examples of co-purchases on Amazon.com. Each product is represented
    by a picture, a plain text product title, and a bold text product category. We
    see that some co-purchases feature products that are obvious complements of one
    another, while other groupings are less so.
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19 亚马逊.com上的共同购买示例。每个产品由一张图片、一个纯文本产品标题和一个粗体文本产品类别表示。我们看到一些共同购买的产品是彼此的明显互补，而其他分组则不那么明显。
- en: Some of these co-purchase groups seem to fit together well, such as the book
    purchases or the clothing purchases. Other co-purchases are less explainable,
    such as an Apple iPod being purchased with instant meals, or beans being purchased
    with a wireless speaker. In those less obvious groupings, there may be some latent
    product relationship, or maybe it’s just mere coincidence. Examining the data
    at scale can provide clues.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些共同购买组似乎很好地结合在一起，例如书籍购买或服装购买。其他共同购买则不太容易解释，例如购买苹果iPod与即食餐食一起，或者购买豆类与无线扬声器一起。在这些不太明显的分组中，可能存在一些潜在的产品关系，或者可能只是纯粹的巧合。在规模上检查数据可以提供线索。
- en: To show how the co-purchasing graph would appear at a small scale, figure 3.20
    takes one of the images from the previous figure and represents the products as
    nodes, with the edges between them representing each co-purchase. For one customer
    and one purchase, this is a small graph, with only four nodes and six edges. But
    for the same customer over time, for a larger set of customers with the same tastes
    in food, or even all the customers, it’s easy to imagine how this graph can scale
    with more products and product connections branching from these few products.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示共同购买图在小规模上的外观，图3.20取了前一个图中的一个图像，并将产品表示为节点，它们之间的边代表每次共同购买。对于一个客户和一次购买，这是一个小图，只有四个节点和六条边。但对于同一个客户随着时间的推移，对于有相同口味的一组客户，甚至所有客户，很容易想象这个图如何随着更多产品和产品连接从这些少数产品中分支出来而扩展。
- en: The construction of this dataset is a long journey in itself, which is very
    much of interest to graph construction and the decisions that have to be made
    to get a meaningful and useful dataset. Put simply, this dataset was derived from
    purchasing log data from Amazon, which directly showed co-purchases, and from
    text data from product reviews, which was used to indirectly show product relationships.
    (For the in-depth story, see [8]).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个数据集本身就是一个漫长的旅程，这对于图构建以及为了得到一个有意义和有用的数据集所必须做出的决策非常感兴趣。简单来说，这个数据集是从亚马逊的购买日志数据中提取的，这些数据直接显示了共同购买的情况，以及从产品评论中的文本数据，这些数据被用来间接展示产品关系。（关于详细的故事，参见[8]）。
- en: '![figure](../Images/3-20.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-20.png)'
- en: Figure 3.20 A graph representation of one of the co-purchases from figure 3.19\.
    Each product’s picture is a node, and the co-purchases are the edges (shown as
    lines) between the products. For the four products shown here, this graph is only
    the co-purchasing graph of one customer. If we show the corresponding graph for
    all customers of Amazon, the number of products and edges could feature tens of
    thousands of product nodes and millions of co-purchasing edges.
  id: totrans-539
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.20 图3.19中一次共同购买的一个图表示。每个产品的图片是一个节点，共同购买是产品之间的边（以线条表示）。对于这里显示的四个产品，这个图只是单个客户的共同购买图。如果我们展示亚马逊所有客户的对应图，产品节点和共同购买边的数量可能达到数万个产品节点和数百万条共同购买边。
- en: To explore the product relationships, we can use the Amazon Products co-purchasing
    graph, a dataset of products that have been bought together in the same transaction
    (defined as a co-purchase). In this dataset, products are represented by nodes
    with both the type of product that was bought, which are the category labels,
    and some feature information. The feature information takes the product description
    and first applies a natural language processing (NLP) method, the bag-of-words
    algorithm, to convert the strings into numerical values. Then, to convert this
    into the same fixed length, the creators of the dataset used principal component
    analysis (PCA) to convert this into a vector of length 100\.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索产品关系，我们可以使用亚马逊产品共同购买图，这是一个包含在同一交易中一起购买的产品数据集（定义为共同购买）。在这个数据集中，产品由节点表示，这些节点既有购买的产品类型，即分类标签，还有一些特征信息。特征信息首先将产品描述应用自然语言处理（NLP）方法，即词袋算法，将字符串转换为数值。然后，为了将其转换为相同的固定长度，数据集的创建者使用了主成分分析（PCA）将其转换为长度为100的向量。
- en: Meanwhile, co-purchases are represented by edges, which refers to two products
    that were bought together. In total, the dataset, `ogbn-products`, consists of
    2.5 million nodes (products) and 61.9 million edges (co-purchases). The dataset
    is provided through the Open Graph Benchmark (OGB) dataset, mentioned at the beginning
    of the chapter, with a usage license from Amazon. Each node has 100 features.
    There are 47 categories that are used as targets in a classification task. We
    note that the edges here are undirected and unweighted.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，共同购买由边表示，这指的是一起购买的两个产品。总的来说，`ogbn-products`数据集包含250万个节点（产品）和6190万个边（共同购买）。这个数据集是通过本章开头提到的开放图基准（OGB）数据集提供的，并获得了亚马逊的使用许可。每个节点有100个特征。有47个类别被用作分类任务的目标。我们注意到这里的边是无向和无权的。
- en: In figure 3.21, we see that the categories with the highest counts of nodes
    are Books (668,950 nodes), CDs & Vinyl (172,199 nodes), and Toys & Games (158,771
    nodes). The lowest are Furniture and Decor (9 nodes), Digital Music (6 nodes),
    and an unknown category (#508510) with 1 node.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.21中，我们看到节点数量最多的类别是书籍（668,950个节点）、CD和黑胶（172,199个节点）以及玩具和游戏（158,771个节点）。最少的类别是家具和装饰（9个节点）、数字音乐（6个节点）以及一个未知类别（#508510）有1个节点。
- en: '![figure](../Images/3-21.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-21.png)'
- en: Figure 3.21 Distribution of node labels in the Amazon Products dataset
  id: totrans-544
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.21 亚马逊产品数据集中节点标签的分布
- en: We also observe that many categories have very low proportions in the dataset.
    The mean count of nodes per label/category is 52,107; the median count is 3,653\.
    This highlights that there is a strong class imbalance in our dataset. This can
    pose a challenge for typical tabular results.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到，在数据集中许多类别的比例非常低。每个标签/类别的节点平均计数为52,107；中位数为3,653。这突显出我们的数据集中存在强烈的类别不平衡。这可能会对典型的表格结果构成挑战。
- en: In this chapter, we explored the fundamentals of graph convolutional networks
    (GCNs) and GraphSAGE, two powerful architectures for learning on graph-structured
    data. We applied these models to a practical product categorization problem using
    the Amazon Products dataset, demonstrating how to implement, train, and refine
    GNNs. We also delved into the theoretical underpinnings of these models, examining
    concepts like neighborhood aggregation, message passing, and the distinctions
    between spectral and spatial convolution methods. By combining hands-on implementation
    with theoretical insights, this chapter has provided a comprehensive foundation
    for understanding and applying convolutional GNNs to real-world graph learning
    tasks. In the next chapter, we study a special convolutional GNN that uses the
    attention mechanism, the Graph Attention Network (GAT).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了图卷积网络（GCNs）和GraphSAGE的基本原理，这两种强大的架构用于在图结构数据上学习。我们使用亚马逊产品数据集将这些模型应用于实际的产品分类问题，展示了如何实现、训练和优化GNNs。我们还深入研究了这些模型的理论基础，考察了诸如邻域聚合、消息传递以及频谱和空间卷积方法之间的区别。通过将实际操作与理论洞察相结合，本章为理解并应用卷积GNNs到现实世界的图学习任务提供了一个全面的基石。在下一章中，我们将研究一种特殊的卷积GNN，它使用注意力机制，即图注意力网络（GAT）。
- en: Summary
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GCNs and GraphSAGE are GNNs that use convolution, done by spatial and spectral
    methods, respectively.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCNs和GraphSAGE是使用卷积的GNNs，分别通过空间和频谱方法进行。
- en: These GNNs can be used in supervised and semi-supervised learning problems.
    We applied them to the semi-supervised problem of predicting product categories.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些GNNs可以用于监督学习和半监督学习问题。我们将它们应用于预测产品类别的半监督问题。
- en: The Amazon Products dataset, `ogbn-products`, consists of a set of products
    (nodes) linked by being purchased in the same transaction (co-purchases). Each
    product node has a set of features, including its product-category. This dataset
    is a popular benchmark for graph classification problems. We can also study how
    it was constructed to get insights on graph creation methodology.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊产品数据集`ogbn-products`由一组产品（节点）组成，这些产品通过同一交易中的购买（共同购买）相互连接。每个产品节点都有一个特征集，包括其产品类别。这个数据集是图分类问题的流行基准。我们还可以研究它是如何构建的，以获得关于图创建方法的见解。
- en: Selecting subgraphs based on domain knowledge or using graph sampling techniques
    ensures more meaningful data is used for training. This can improve the performance
    of the models by focusing on relevant parts of the graph.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据领域知识选择子图或使用图采样技术可以确保使用更有意义的数据进行训练。这可以通过关注图的关联部分来提高模型的性能。
- en: Different aggregation methods, such as mean, max, and sum, have varied effects
    on model performance. Experimenting with multiple aggregation strategies can help
    capture various properties of the graph data, potentially enhancing model performance.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的聚合方法，如平均值、最大值和总和，对模型性能有不同的影响。通过实验多种聚合策略可以帮助捕捉图数据的各种属性，从而可能提高模型性能。
- en: Exploring more sophisticated aggregation functions or custom aggregations tailored
    to the specific characteristics of the dataset can yield performance improvements.
    Examples include `SoftmaxAggregation` and `StdAggregation`.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索更复杂的聚合函数或针对数据集特定特征的定制聚合可以带来性能提升。例如包括`SoftmaxAggregation`和`StdAggregation`。
- en: Depth in GNNs is analogous to the number of hops or message-passing steps. While
    deeper models can theoretically capture more complex patterns, they often suffer
    from over-smoothing, where node features become too similar, making it difficult
    to distinguish between different nodes.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图神经网络（GNNs）中，深度与跳跃次数或消息传递步骤的数量相当。虽然更深层次的模型在理论上可以捕捉更复杂的模式，但它们通常会受到过度平滑的影响，节点特征变得过于相似，这使得区分不同节点变得困难。
- en: Empirical testing of different aggregation methods and model configurations
    is essential. Experimentation helps determine which methods best capture the relational
    dynamics and feature distributions of the dataset.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不同的聚合方法和模型配置进行实证测试是至关重要的。实验有助于确定哪些方法最能捕捉数据集的关系动态和特征分布。
