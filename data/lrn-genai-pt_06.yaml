- en: 5 Selecting characteristics in generated images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 选择生成图像中的特征
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Building a conditional generative adversarial network to generate images with
    certain attributes (human faces with or without eyeglasses, for example)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建条件生成对抗网络以生成具有特定属性的图像（例如，戴眼镜或不戴眼镜的人类面孔）
- en: Implementing Wasserstein distance and gradient penalty to improve image quality
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Wasserstein距离和梯度惩罚以改善图像质量
- en: Selecting vectors associated with different features so that the trained GAN
    model generates images with certain characteristics (male or female faces, for
    example)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择与不同特征相关的向量，以便训练的生成对抗网络模型生成具有特定特征的图像（例如，男性或女性面孔）
- en: Combining conditional GAN with vector selection to specify two attributes simultaneously
    (female faces without glasses or male faces with glasses, for example)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将条件生成对抗网络与向量选择相结合，以同时指定两个属性（例如，无眼镜的女性面孔或戴眼镜的男性面孔）
- en: The anime faces we generated with deep convolutional GAN (DCGAN) in chapter
    4 look realistic. However, you may have noticed that each generated image has
    different attributes such as hair color, eye color, and whether the head tilts
    toward the left or right. You may be wondering if there is a way to tweak the
    model so that the generated images have certain characteristics (such as with
    black hair and tilting toward the left). It turns out you can.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第4章中用深度卷积生成对抗网络（DCGAN）生成的动漫面孔看起来很逼真。然而，你可能已经注意到，每个生成的图像都有不同的属性，例如发色、眼色以及头部是否向左或向右倾斜。你可能想知道是否有方法可以调整模型，使得生成的图像具有某些特征（例如，黑色头发且向左倾斜）。实际上，你可以做到。
- en: In this chapter, you’ll learn two different ways of selecting characteristics
    in the generated images and their respective advantages and disadvantages. The
    first method involves selecting specific vectors in the latent space. Different
    vectors correspond to different characteristics—for example, one vector might
    result in a male face and another in a female face. The second method uses a conditional
    GAN (cGAN), which involves training the model on labeled data. This allows us
    to prompt the model to generate images with a specified label, each representing
    a distinct characteristic—like faces with or without eyeglasses.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习两种不同的方法来选择生成图像中的特征，以及它们各自的优缺点。第一种方法涉及在潜在空间中选择特定的向量。不同的向量对应不同的特征——例如，一个向量可能导致男性面孔，另一个可能导致女性面孔。第二种方法使用条件生成对抗网络（cGAN），这涉及到在标记数据上训练模型。这允许我们提示模型生成具有指定标签的图像，每个标签代表一个独特的特征——如戴眼镜或不戴眼镜的面孔。
- en: 'In addition, you’ll learn to combine the two methods so that you can select
    two independent attributes of the images at the same time. As a result, you can
    generate four different groups of images: males with glasses, males without glasses,
    females with glasses, and females without glasses. To make things more interesting,
    you can use a weighted average of the labels or a weighted average of the input
    vectors to generate images that transition from one attribute to another. For
    example, you can generate a series of images so that the eyeglasses gradually
    fade out on the same person’s face (label arithmetic). Or you can generate a series
    of images so that the male features gradually fade out and a male face changes
    to a female face (vector arithmetic).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将学会结合这两种方法，以便你可以同时选择图像的两个独立属性。结果，你可以生成四组不同的图像：戴眼镜的男性、无眼镜的男性、戴眼镜的女性和无眼镜的女性。为了使事情更有趣，你可以使用标签的加权平均值或输入向量的加权平均值来生成从一种属性过渡到另一种属性的图像。例如，你可以生成一系列图像，使得同一人的脸上的眼镜逐渐消失（标签算术）。或者你可以生成一系列图像，使得男性特征逐渐消失，男性面孔变成女性面孔（向量算术）。
- en: 'Being able to conduct either vector arithmetic or label arithmetic alone feels
    like science fiction, let alone performing the two simultaneously. The whole experience
    reminds us of the quote by Arthur C. Clarke (author of *2001: A Space Odyssey*),
    “Any sufficiently advanced technology is indistinguishable from magic.”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 能够单独进行向量算术或标签算术就像科幻小说一样，更不用说同时进行这两种操作了。整个体验让我们想起了亚瑟·C·克拉克（*2001太空漫游*的作者）的名言：“任何足够先进的技术都与魔法无法区分。”
- en: Despite the realism of the anime faces generated in chapter 4, they were limited
    by low resolution. Training GAN models can be tricky and is often hampered by
    problems like small sample sizes or low-quality images. These challenges can prevent
    models from converging, resulting in poor image quality. To address this, we’ll
    discuss and implement an improved training technique using the Wasserstein distance
    with gradient penalty in our cGAN. This enhancement results in more realistic
    human faces and noticeably better image quality compared to the previous chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第4章生成的动漫面孔在现实感上有所提高，但它们受到低分辨率的限制。训练GAN模型可能很棘手，并且经常受到样本量小或图像质量低等问题的影响。这些挑战可能会阻止模型收敛，导致图像质量差。为了解决这个问题，我们将在我们的cGAN中讨论并实现使用Wasserstein距离和梯度惩罚的改进训练技术。这种增强使得生成的人脸更加逼真，与上一章相比图像质量明显更好。
- en: 5.1 The eyeglasses dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 眼镜数据集
- en: 'We’ll use the eyeglasses dataset in this chapter to train a cGAN model. In
    the next chapter, we’ll also use this dataset to train a CycleGAN model in one
    of the exercises: to convert an image with eyeglasses to an image without eyeglasses
    and vice versa. In this section, you’ll learn to download the dataset and preprocess
    images in it.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用眼镜数据集来训练一个cGAN模型。在下一章中，我们也将使用这个数据集在练习中训练一个CycleGAN模型：将戴眼镜的图片转换为不戴眼镜的图片，反之亦然。在本节中，你将学习如何下载数据集并预处理其中的图片。
- en: 'The Python programs in this chapter and the next are adapted from two excellent
    online open-source projects: the Kaggle project by Yashika Jain [https://mng.bz/JNVQ](https://mng.bz/JNVQ)
    and a GitHub repository by Aladdin Persson [https://mng.bz/w5yg](https://mng.bz/w5yg).
    I encourage you to look into these two projects while going through this chapter
    and the next.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和下一章中的Python程序改编自两个优秀的在线开源项目：Yashika Jain的Kaggle项目[https://mng.bz/JNVQ](https://mng.bz/JNVQ)和Aladdin
    Persson的GitHub仓库[https://mng.bz/w5yg](https://mng.bz/w5yg)。我鼓励你在阅读本章和下一章时查看这两个项目。
- en: 5.1.1 Downloading the eyeglasses dataset
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 下载眼镜数据集
- en: 'The eyeglasses dataset we use is from Kaggle. Log into Kaggle and go to the
    link [https://mng.bz/q0oz](https://mng.bz/q0oz) to download the image folder and
    the two CSV files on the right: `train.csv` and `test.csv`. There are 5,000 images
    in the folder /faces-spring-2020/. Once you have the data, place both the image
    folder and the two CSV files inside the folder /files/ on your computer.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的眼镜数据集来自Kaggle。登录Kaggle并访问链接[https://mng.bz/q0oz](https://mng.bz/q0oz)下载图片文件夹以及右侧的两个CSV文件：`train.csv`和`test.csv`。文件夹/faces-spring-2020/中有5,000张图片。一旦你有了数据，请将图片文件夹和两个CSV文件都放在你电脑上的/fles/文件夹内。
- en: 'Next, we’ll sort the photos into two subfolders: one containing only images
    with eyeglasses and another one with images without eyeglasses.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将照片分类到两个子文件夹中：一个只包含戴眼镜的图片，另一个包含不戴眼镜的图片。
- en: 'First, let’s look at the file train.csv:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看train.csv文件：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads the data in the file train.csv as a pandas DataFrame
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将train.csv文件中的数据加载为pandas DataFrame
- en: ② Sets the values in the id column as the indexes of observations
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将id列的值设置为观察的索引
- en: 'The previous code cell imports the file `train.csv` and sets the variable `id`
    as the index of each observation. The column `glasses` in the file has two values:
    0 or 1, indicating whether the image has eyeglasses in it or not (0 means no glasses;
    1 means with glasses).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码单元导入`train.csv`文件并将变量`id`设置为每个观察的索引。文件中的`glasses`列有两个值：0或1，表示图片中是否有眼镜（0表示没有眼镜；1表示有眼镜）。
- en: 'Next, we separate the images into two different folders: one containing images
    with eyeglasses and one containing images without eyeglasses.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图片分为两个不同的文件夹：一个包含戴眼镜的图片，另一个包含不戴眼镜的图片。
- en: Listing 5.1 Sorting images with and without eyeglasses
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 对戴眼镜和不戴眼镜的图片进行分类
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Creates a subfolder /files/glasses/G/ to contain images with eyeglasses
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个子文件夹/files/glasses/G/来存放戴眼镜的图片
- en: ② Creates a subfolder /files/glasses/NoG/ to contain images without eyeglasses
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个子文件夹/files/glasses/NoG/来存放不戴眼镜的图片
- en: ③ Moves images labeled 0 to folder NoG
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将标签为0的图片移动到文件夹NoG
- en: ④ Moves images labeled 1 to folder G
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将标签为1的图片移动到文件夹G
- en: In the preceding code cell, we first use the `os` library to create two subfolders
    /glasses/G/ and /glasses/NoG/ inside the folder /files/ on your computer. We then
    use the `shutil` library to move images to the two folders based on the label
    `glasses` in the file `train.csv`. Those labeled 1 are moved to folder G and those
    labeled 0 to folder NoG.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码单元中，我们首先使用 `os` 库在您的计算机上的 `/files/` 文件夹内创建两个子文件夹 `/glasses/G/` 和 `/glasses/NoG/`。然后，我们使用
    `shutil` 库根据文件 `train.csv` 中的标签 `glasses` 将图像移动到这两个文件夹。标签为 1 的图像被移动到文件夹 G，而标签为
    0 的图像被移动到文件夹 NoG。
- en: 5.1.2 Visualizing images in the eyeglasses dataset
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 在眼镜数据集中可视化图像
- en: 'The classification column `glasses` in the file `train.csv` is not perfect.
    If you go to the subfolder G on your computer, for example, you’ll see that most
    images have glasses, but about 10% have no glasses. Similarly, if you go to the
    subfolder NoG, you’ll see that about 10% actually have glasses. You need to manually
    correct this by moving images from one folder to the other. This is important
    for our training later so you should manually move images in the two folders so
    that one contains only images with glasses and the other images without glasses.
    Welcome to the life of a data scientist: fixing data problems is part of daily
    routine! Let’s first visualize some examples of images with eyeglasses.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 `train.csv` 中的分类列 `glasses` 并不完美。例如，如果您访问计算机上的子文件夹 G，您会看到大多数图像都有眼镜，但大约 10%
    的图像没有眼镜。同样，如果您访问子文件夹 NoG，您会发现大约 10% 的图像实际上有眼镜。您需要手动纠正这个问题，通过将图像从一个文件夹移动到另一个文件夹。这对我们后续的训练非常重要，因此您应该手动移动两个文件夹中的图像，使得一个文件夹只包含有眼镜的图像，另一个文件夹只包含无眼镜的图像。欢迎来到数据科学家的生活：修复数据问题是日常工作的一个部分！让我们首先可视化一些有眼镜的图像示例。
- en: Listing 5.2 Visualizing images with eyeglasses
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 使用眼镜可视化图像
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Randomly selects 16 images from folder G
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从文件夹 G 中随机选择 16 个图像
- en: ② Displays the 16 images in a 2 × 8 grid
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在 2 × 8 网格中显示 16 个图像
- en: If you have manually corrected the mislabeling of images in folder G, you’ll
    see 16 images with eyeglasses after running the code in listing 5.2\. The output
    is shown in figure 5.1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经手动纠正了文件夹 G 中图像的错误标记，运行列表 5.2 中的代码后，您将看到 16 个有眼镜的图像。输出结果如图 5.1 所示。
- en: '![](../../OEBPS/Images/CH05_F01_Liu.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F01_Liu.png)'
- en: Figure 5.1 Sample images with eyeglasses in the training dataset
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 训练数据集中有眼镜的样本图像
- en: You can change G to NoG in listing 5.2 to visualize 16 sample images without
    eyeglasses in the dataset. The complete code is in the book’s GitHub repository
    [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI). The output
    is shown in figure 5.2\.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将列表 5.2 中的 G 更改为 NoG 来可视化数据集中没有眼镜的 16 个样本图像。完整的代码在本书的 GitHub 仓库 [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    中。输出结果如图 5.2 所示。
- en: '![](../../OEBPS/Images/CH05_F02_Liu.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F02_Liu.png)'
- en: Figure 5.2 Sample images without eyeglasses in the training dataset
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 训练数据集中没有眼镜的样本图像
- en: 5.2 cGAN and Wasserstein distance
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 cGAN 和 Wasserstein 距离
- en: A cGAN is similar to the GAN models you have seen in chapters 3 and 4, with
    the exception that you attach a label to the input data. The labels correspond
    to different characteristics in the input data. Once the trained GAN model “learns”
    to associate a certain label with a characteristic, you can feed a random noise
    vector with a label to the model to generate output with the desired characteristic.^([1](#footnote-002))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: cGAN 与您在第 3 章和第 4 章中看到的 GAN 模型类似，不同之处在于您为输入数据附加了一个标签。这些标签对应于输入数据中的不同特征。一旦训练好的
    GAN 模型“学习”将某个标签与特征关联起来，您就可以向模型提供一个带有标签的随机噪声向量，以生成具有所需特征的输出。^([1](#footnote-002))
- en: 'GAN models often suffer from problems like mode collapse (the generator finds
    a certain type of output that is good at fooling the discriminator and then collapses
    its outputs to these few modes, ignoring other variations), vanishing gradients,
    and slow convergence. Wasserstein GAN (WGAN) introduces the Earth Mover’s (or
    Wasserstein-1) distance as the loss function, offering a smoother gradient flow
    and more stable training. It mitigates problems like mode collapse.^([2](#footnote-001))
    We’ll implement it in cGAN training in this chapter. Note that WGAN is a concept
    independent of cGAN: It uses the Wasserstein distance to improve the training
    process and can be applied to any GAN model (such as the ones we created in chapters
    3 and 4). We’ll combine both concepts in one setting to save space.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GAN模型通常会遇到诸如模式坍塌（生成器找到一种能够很好地欺骗判别器的输出类型，然后将输出坍塌到这些少数模式，忽略其他变化）、梯度消失和收敛缓慢等问题。Wasserstein
    GAN（WGAN）引入地球迁移距离（或Wasserstein-1距离）作为损失函数，提供更平滑的梯度流和更稳定的训练。它缓解了模式坍塌等问题.^([2](#footnote-001))
    我们将在本章中实现它，在cGAN训练中。请注意，WGAN是一个独立于cGAN的概念：它使用Wasserstein距离来改进训练过程，可以应用于任何GAN模型（例如我们在第3章和第4章中创建的模型）。我们将结合这两个概念在一个设置中，以节省空间。
- en: Other ways to stabilize GAN training
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定GAN训练的其他方法
- en: The problems with training GAN models are most common when generating high-resolution
    images. The model architecture is usually complex, with many neural layers. Other
    than WGAN, progressive GAN is another way to stabilize training. Progressive GANs
    enhance the stability of GAN training by breaking down the complex task of high-resolution
    image generation into manageable steps, allowing for more controlled and effective
    learning. For details, see “Progressive Growing of GANs for Improved Quality,
    Stability, and Variation.” by Karas et al., [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN模型时的问题在生成高分辨率图像时最为常见。模型架构通常很复杂，包含许多神经网络层。除了WGAN之外，渐进式GAN是另一种稳定训练的方法。渐进式GAN通过将高分辨率图像生成的复杂任务分解为可管理的步骤，从而增强了GAN训练的稳定性，允许更可控和有效的学习。有关详细信息，请参阅Karas等人撰写的“用于提高质量、稳定性和变化的GAN渐进式生长”，[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)。
- en: 5.2.1 WGAN with gradient penalty
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 带梯度惩罚的WGAN
- en: WGAN is a technique used to improve the training stability and performance of
    GAN models. Regular GANs (such as the ones you have seen in Chapters 3 and 4)
    have two components—a generator and a discriminator. The generator creates fake
    data, while the discriminator evaluates whether the data is real or fake. Training
    involves a competitive zero-sum game in which the generator tries to fool the
    discriminator, and the discriminator tries to accurately classify real and fake
    data instances.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN是一种用于提高GAN模型训练稳定性和性能的技术。常规GAN（例如你在第3章和第4章中看到的）有两个组件——生成器和判别器。生成器创建伪造数据，而判别器评估数据是否真实。训练涉及一个零和博弈的竞争，其中生成器试图欺骗判别器，而判别器试图准确分类真实和伪造数据实例。
- en: Researchers have proposed to use Wasserstein distance (a measure of dissimilarity
    between two distributions) instead of the binary cross-entropy as the loss function
    to stabilize training with a gradient penalty term.^([3](#footnote-000)) The technique
    offers a smoother gradient flow and mitigates problems like mode collapse. Figure
    5.3 provides a diagram of WGAN. As you can see on the right side of the figure,
    the losses associated with the real and fake images are Wasserstein loss instead
    of the regular binary cross-entropy loss.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了使用Wasserstein距离（两个分布之间差异的度量）而不是二元交叉熵作为损失函数，通过梯度惩罚项来稳定训练.^([3](#footnote-000))
    这种技术提供了更平滑的梯度流，并缓解了模式坍塌等问题。图5.3提供了WGAN的示意图。如图所示，与真实图像和伪造图像相关的损失是Wasserstein损失，而不是常规的二元交叉熵损失。
- en: '![](../../OEBPS/Images/CH05_F03_Liu.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F03_Liu.png)'
- en: 'Figure 5.3 WGAN with gradient penalty. The discriminator network in WGAN (which
    we call the critic) rates input images: it tries to assign a score of –∞ to a
    fake image (bottom left) and a score of ∞ to the real image (top middle). Further,
    an interpolated image of the real and fake images (top left) is presented to the
    critic, and the gradient penalty with respect to the critic’s rating on the interpolated
    image is added to the total loss in the training process.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3带有梯度惩罚的WGAN。WGAN中的判别器网络（我们称之为评论家）对输入图像进行评分：它试图将一个负无穷大的分数分配给一个假图像（左下角）和一个正无穷大的分数分配给真实图像（右上角）。此外，还向评论家展示了一个真实和假图像的插值图像（左上角），并在训练过程中将评论家对插值图像的梯度惩罚添加到总损失中。
- en: Further, for the Wasserstein distance to work correctly, the discriminator (called
    the critic in WGANs) must be 1-Lipschitz continuous, meaning the gradient norms
    of the critic’s function must be at most 1 everywhere. The original WGAN paper
    proposed weight clipping to enforce the Lipschitz constraint.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使Wasserstein距离能够正确工作，判别器（在WGAN中称为评论家）必须是1-Lipschitz连续的，这意味着评论家函数的梯度范数必须在任何地方都最多为1。原始的WGAN论文提出了权重裁剪来强制执行Lipschitz约束。
- en: To address weight clipping problems, the gradient penalty is added to the loss
    function to enforce the Lipschitz constraint more effectively. To implement WGAN
    with gradient penalty, we first randomly sample points along the straight line
    between real and generated data points (as indicated by the interpolated image
    in the top left of figure 5.3). Since both real and fake images have labels attached
    to them, the interpolated image also has a label attached to it, which is the
    interpolated value of the two original labels. We then compute the gradient of
    the critic’s output with respect to these sampled points. Finally, we add a penalty
    to the loss function proportional to the deviation of these gradient norms from
    1 (the penalty term is called gradient penalty). That is, gradient penalty in
    WGANs is a technique to improve training stability and sample quality by enforcing
    the Lipschitz constraint more effectively, addressing the limitations of the original
    WGAN model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决权重裁剪问题，梯度惩罚被添加到损失函数中，以更有效地强制执行Lipschitz约束。为了实现带有梯度惩罚的WGAN，我们首先在真实和生成数据点之间的直线上的点进行随机采样（如图5.3左上角的插值图像所示）。由于真实和假图像都附有标签，插值图像也附有标签，这是两个原始标签的插值值。然后，我们计算评论家输出对这些采样点的梯度。最后，我们将与1（梯度惩罚项称为梯度惩罚）的偏差成比例的惩罚添加到损失函数中。也就是说，WGAN中的梯度惩罚是一种通过更有效地强制执行Lipschitz约束来提高训练稳定性和样本质量的技术，解决了原始WGAN模型的局限性。
- en: 5.2.2 cGANs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 cGANs
- en: 'cGAN is an extension of the basic GAN framework. In a cGAN, both the generator
    and the discriminator (or the critic since we are implementing WGAN and cGAN in
    the same setting) are conditioned on some additional information. This could be
    anything, such as class labels, data from other modalities, or even textual descriptions.
    This conditioning is typically achieved by feeding this additional information
    into both the generator and discriminator. In our setting, we’ll add class labels
    to the inputs to both the generator and the critic: we attach one label to images
    with eyeglasses and another label to images without eyeglasses. Figure 5.4 provides
    a diagram of the training process for cGANs.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: cGAN是基本GAN框架的扩展。在cGAN中，生成器和判别器（或评论家，因为我们是在相同的设置下实现WGAN和cGAN）都基于一些额外的信息。这可能是一切，例如类别标签、来自其他模态的数据，甚至是文本描述。这种条件通常是通过将此附加信息输入到生成器和判别器中实现的。在我们的设置中，我们将类别标签添加到生成器和评论家的输入中：我们给戴眼镜的图像贴上标签，给不戴眼镜的图像贴上另一个标签。图5.4提供了cGANs训练过程的示意图。
- en: '![](../../OEBPS/Images/CH05_F04_Liu.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F04_Liu.png)'
- en: Figure 5.4 The training process for cGANs
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 cGANs的训练过程
- en: As you can see at the top left of figure 5.4, in a cGAN, the generator receives
    both a random noise vector and the conditional information (a label indicating
    whether the image has eyeglasses or not) as input. It uses this information to
    generate data that not only looks real but also aligns with the conditional input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图5.4的左上角所见，在cGAN中，生成器接收一个随机噪声向量和一个条件信息（一个标签，指示图像是否戴眼镜）作为输入。它使用这些信息生成看起来真实且与条件输入一致的数据。
- en: The critic receives either real data from the training set or fake data generated
    by the generator, along with the conditional information (a label indicating whether
    the image has eyeglasses or not in our setting). Its task is to determine whether
    the given data is real or fake, taking the conditional information into account
    (does the generated image have eyeglasses in it?). In figure 5.4, we use the critic
    network instead of the discriminator network since we implement both cGAN and
    WGAN simultaneously, but the concept of cGAN applies to traditional GANs as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家接收来自训练集的真实数据或生成器生成的伪造数据，以及条件信息（在我们的设置中，一个标签表示图像是否有眼镜）。其任务是确定给定数据是真实还是伪造，考虑条件信息（生成的图像中是否有眼镜？）。在图
    5.4 中，我们使用评论家网络而不是判别器网络，因为我们同时实现了 cGAN 和 WGAN，但 cGAN 的概念也适用于传统的 GAN。
- en: The main advantage of cGANs is their ability to select aspects of the generated
    data, making them more versatile and applicable in scenarios where the output
    needs to be directed or conditioned on certain input parameters. In our setting,
    we’ll train the cGAN so that we have the ability to select whether the generated
    images have eyeglasses or not.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: cGAN 的主要优势在于它们选择生成数据方面的能力，这使得它们更加灵活，适用于输出需要根据某些输入参数进行定向或条件化的场景。在我们的设置中，我们将训练
    cGAN，以便我们能够选择生成的图像是否带有眼镜。
- en: In summary, cGANs are a powerful extension of the basic GAN architecture, enabling
    targeted generation of synthetic data based on conditional inputs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，cGAN 是基本 GAN 架构的一个强大扩展，它能够根据条件输入有针对性地生成合成数据。
- en: 5.3 Create a cGAN
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 创建 cGAN
- en: In this section, you’ll learn to create a cGAN to generate human faces with
    or without eyeglasses. You’ll also learn to implement the WGAN with gradient penalty
    to stabilize training.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何创建一个 cGAN 来生成带有或不带有眼镜的人类面部图像。你还将学习如何实现带有梯度惩罚的 WGAN 以稳定训练。
- en: The generator in cGANs uses not only random noise vectors but also conditional
    information such as labels as inputs to create images either with or without eyeglasses.
    Further, a critic network in WGANs is different from the discriminator network
    in traditional GANs. You’ll also learn how to calculate the Wasserstein distance
    and the gradient penalty in this section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: cGAN 中的生成器不仅使用随机噪声向量，还使用条件信息（如标签）作为输入来创建带有或不带有眼镜的图像。此外，WGAN 中的评论家网络与传统 GAN 中的判别器网络不同。你还将学习如何在本节中计算
    Wasserstein 距离和梯度惩罚。
- en: 5.3.1 A critic in cGAN
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 cGAN 中的评论家
- en: In cGANs, the discriminator is a binary classifier to identify the input as
    either real or fake, conditional on the label. In WGAN, we call the discriminator
    network the critic. The critic evaluates the input and gives a score between −∞
    and ∞. The higher the score, the more likely that the input is from the training
    set (that is, real).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 cGAN 中，判别器是一个二元分类器，用于根据标签识别输入是真实还是伪造。在 WGAN 中，我们将判别器网络称为评论家。评论家评估输入并给出介于 −∞
    和 ∞ 之间的分数。分数越高，输入来自训练集（即真实）的可能性就越大。
- en: Listing 5.3 creates the critic network. The architecture is somewhat similar
    to the discriminator network we used in chapter 4 when generating color images
    of anime faces. In particular, we use seven `Conv2d` layers in PyTorch to gradually
    downsample the input so that the output is a single value between −∞ and ∞.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 创建了评论家网络。其架构与我们第四章在生成动漫面孔彩色图像时使用的判别器网络有些相似。特别是，我们使用 PyTorch 中的七个 `Conv2d`
    层逐步下采样输入，以便输出是一个介于 −∞ 和 ∞ 之间的单一值。
- en: Listing 5.3 A critic network in cGAN with Wasserstein distance
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3：具有 Wasserstein 距离的 cGAN 评论家网络
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① The critic network has two Conv2d layers plus five blocks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ① 评论家网络有两个 Conv2d 层加上五个块。
- en: ② The output has one feature, without activation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输出只有一个特征，没有激活。
- en: ③ Each block contains a Conv2d layer, an InstanceNorm2d layer, with LeakyReLU
    activation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 每个块包含一个 Conv2d 层，一个 InstanceNorm2d 层，以及 LeakyReLU 激活。
- en: The input to the critic network is a color image with a shape of 5 × 256 × 256.
    The first three channels are the color channels (colors red, green, and blue).
    The last two channels (the fourth and fifth channels) are label channels to tell
    the critic whether the image is with glasses or without glasses. We’ll discuss
    the exact mechanism to accomplish this in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家网络的输入是一个形状为 5 × 256 × 256 的彩色图像。前三个通道是颜色通道（红色、绿色和蓝色）。最后两个通道（第四和第五通道）是标签通道，用于告诉评论家图像是否带有眼镜。我们将在下一节中讨论实现这一机制的确切方法。
- en: The critic network consists of seven `Conv2d` layers. In chapter 4, we discussed
    in depth how these layers work. They are used for feature extraction by applying
    a set of learnable filters on the input images to detect patterns and features
    at different spatial scales, effectively capturing hierarchical representations
    of the input data. The critic then evaluates the input images based on these representations.
    The five `Conv2d` layers in the middle are all followed by an `InstanceNorm2d`
    layer and a `LeakyReLU` activation; hence, we define a `block()` method to streamline
    the critic network. The `InstanceNorm2d` layer is similar to the `BatchNorm2d`
    layer we discussed in chapter 4, except that we normalize each individual instance
    in the batch independently.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 批判网络由七个 `Conv2d` 层组成。在第4章中，我们深入讨论了这些层的工作原理。它们通过在输入图像上应用一组可学习的滤波器来提取特征，以检测不同空间尺度上的模式和特征，从而有效地捕获输入数据的层次表示。然后，批判器根据这些表示评估输入图像。中间的五
    `Conv2d` 层后面都跟着一个 `InstanceNorm2d` 层和一个 `LeakyReLU` 激活函数；因此，我们定义了一个 `block()`
    方法来简化批判网络。`InstanceNorm2d` 层与我们在第4章中讨论的 `BatchNorm2d` 层类似，不同之处在于我们独立地对批处理中的每个实例进行归一化。
- en: Another key point is that the output is no longer a value between 0 and 1 since
    we don’t use the sigmoid activation in the last layer in the critic network. Instead,
    the output is a value between −∞ and ∞ since we use the Wasserstein distance with
    gradient penalty in our cGAN.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键点是，输出不再是介于 0 和 1 之间的值，因为我们没有在批判网络的最后一层使用 sigmoid 激活函数。相反，由于我们在 cGAN 中使用
    Wasserstein 距离和梯度惩罚，输出是一个介于 −∞ 和 ∞ 之间的值。
- en: 5.3.2 A generator in cGAN
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 cGAN 中的生成器
- en: In WGANs, the generator’s job is to create data instances so that they can be
    evaluated at a high score by the critic. In cGANs, the generator must generate
    data instances with conditional information (with or without eyeglasses in our
    setting). Since we are implementing a cGAN with Wasserstein distance, we’ll tell
    the generator what type of images we want to generate by attaching a label to
    the random noise vector. We’ll discuss the exact mechanism in the next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 WGAN 中，生成器的任务是创建数据实例，以便它们可以被批判器以高分数评估。在 cGAN 中，生成器必须生成具有条件信息的数据实例（在我们的设置中是带有或不带有眼镜）。由于我们正在实现一个使用
    Wasserstein 距离的 cGAN，我们将通过将标签附加到随机噪声向量来告诉生成器我们想要生成哪种类型的图像。我们将在下一节中讨论具体的机制。
- en: We create the neural network shown in the following listing to represent the
    generator.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了以下列表中所示的神经网络来表示生成器。
- en: Listing 5.4 A generator in cGAN
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 cGAN 中的生成器
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The generator consists of seven ConvTranspose2d layers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成器由七个 ConvTranspose2d 层组成。
- en: ② Uses Tanh activation to squeeze values to the range [–1, 1], the same as images
    in the training set
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 Tanh 激活将值压缩到范围 [–1, 1]，与训练集中的图像相同
- en: ③ Each block consists of a ConvTranspose2d layer, a BatchNorm2d layer, and ReLU
    activation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 每个块由一个 ConvTranspose2d 层、一个 BatchNorm2d 层和 ReLU 激活函数组成。
- en: We’ll feed a random noise vector from a 100-dimensional latent space to the
    generator as input. We’ll also feed a 2-value one-hot encoded image label to the
    generator to tell it to generate an image either with or without eyeglasses. We’ll
    concatenate the two pieces of information together to form a 102-dimensional input
    variable to the generator. The generator then generates a color image based on
    the input from the latent space and the labeling information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从100维潜在空间中随机噪声向量输入到生成器中。我们还将一个2值的一热编码图像标签输入到生成器中，以告诉它生成带有或不带有眼镜的图像。我们将这两部分信息连接起来，形成一个102维的输入变量输入到生成器中。然后，生成器根据潜在空间和标签信息生成彩色图像。
- en: The generator network consists of seven `ConvTranspose2d` layers, and the idea
    is to mirror the steps in the critic network to conjure up images, as we discussed
    in chapter 4\. The first six `ConvTranspose2d` layers are all followed by a `BatchNorm2d`
    layer and a `ReLU` activation; hence, we define a `block()` method in the generator
    network to simplify the architecture. As we have done in chapter 4, we use the
    Tanh activation function at the output layer so the output pixels are all in the
    range of –1 and 1, the same as the images in the training set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络由七个 `ConvTranspose2d` 层组成，其思路是镜像批判网络中的步骤来生成图像，正如我们在第4章中讨论的那样。前六个 `ConvTranspose2d`
    层后面都跟着一个 `BatchNorm2d` 层和一个 `ReLU` 激活函数；因此，我们在生成器网络中定义了一个 `block()` 方法来简化架构。正如我们在第4章中所做的那样，我们在输出层使用
    Tanh 激活函数，以便输出像素都在范围 -1 和 1 之间，与训练集中的图像相同。
- en: 5.3.3 Weight initialization and the gradient penalty function
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 权重初始化和梯度惩罚函数
- en: In deep learning, the weights in neural networks are randomly initialized. When
    the network architecture is complicated, and there are many hidden layers (which
    is the case in our setting), how weights are initialized is crucial.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，神经网络中的权重是随机初始化的。当网络架构复杂，存在许多隐藏层（在我们的设置中就是这样）时，权重的初始化方式至关重要。
- en: 'We, therefore, define the following `weights_init()` function to initialize
    weights in both the generator and the critic networks:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们定义以下 `weights_init()` 函数来初始化生成器和评论家网络中的权重：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The function initializes weights in `Conv2d` and `ConvTranspose2d` layers with
    values drawn from a normal distribution with a mean of 0 and a standard deviation
    of 0.02\. It also initializes weights in `BatchNorm2d` layers with values drawn
    from a normal distribution with a mean of 1 and a standard deviation of 0.02\.
    We choose a small standard deviation in weight initializations to avoid exploding
    gradients.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 函数使用均值为0和标准差为0.02的正态分布值初始化 `Conv2d` 和 `ConvTranspose2d` 层的权重。它还使用均值为1和标准差为0.02的正态分布值初始化
    `BatchNorm2d` 层的权重。我们在权重初始化中选择了较小的标准差，以避免梯度爆炸。
- en: 'Next, we create a generator and a critic based on the `Generator()` and `Critic()`
    classes we defined in the last subsection. We then initialize the weights in them
    based on the `weights_init()` function defined earlier:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们根据在上一个子节中定义的 `Generator()` 和 `Critic()` 类创建一个生成器和评论家。然后，我们根据之前定义的 `weights_init()`
    函数初始化它们中的权重：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As usual, we’ll use the Adam optimizer for both the critic and the generator:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们将使用Adam优化器对评论家和生成器进行优化：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The generator tries to create images that are indistinguishable from those
    in the training set with the given label. It presents the images to the critic
    to obtain high ratings on the generated images. The critic, on the other hand,
    tries to assign high ratings to real images and low ratings to fake images, conditional
    on the given label. Specifically, the loss function for the critic has three components:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器试图创建与给定标签的训练集中图像无法区分的图像。它将图像展示给评论家以获得对生成图像的高评分。另一方面，评论家试图在给定标签的条件下，对真实图像给予高评分，对假图像给予低评分。具体来说，评论家的损失函数有三个组成部分：
- en: critic_value(fake) − critic_value(real) + weight × GradientPenalty
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: critic_value(fake) − critic_value(real) + weight × GradientPenalty
- en: The first term, *critic_value(fake)*, says that if an image is fake, the critic’s
    objective is to identify it as fake and give it a low evaluation. The second term,
    *− critic_value(real)*, indicates that if the image is real, the critic’s objective
    is to identify it as real and give it a high evaluation. Further, the critic wants
    to minimize the gradient penalty term, *weight* *× GradientPenalty*, where *weight*
    is a constant to determine how much penalty we want to assign to deviations of
    the gradient norms from the value 1\. The gradient penalty is calculated as shown
    in the following listing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项，*critic_value(fake)*，表示如果一幅图像是假的，评论家的目标是将其识别为假并给予低评价。第二个项，*− critic_value(real)*，表示如果图像是真实的，评论家的目标是将其识别为真实并给予高评价。此外，评论家还希望最小化梯度惩罚项，*weight*
    *× GradientPenalty*，其中*weight*是一个常数，用于确定我们希望分配给梯度范数偏差的惩罚程度。梯度惩罚的计算方法如下所示。
- en: Listing 5.5 Calculating gradient penalty
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5 计算梯度惩罚
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Creates an interpolated image of the real and the fake
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建真实图像和假图像的插值图像
- en: ② Obtains the critic value with respect to the interpolated image
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取关于插值图像的评论家值
- en: ③ Calculates the gradient of the critic value
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算评论家值的梯度
- en: ④ Gradient penalty is the squared deviation of the gradient norm from value
    1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 梯度惩罚是梯度范数与值1的平方偏差。
- en: 'In the function `GP()`, we first create interpolated images of real ones and
    fake ones. This is done by randomly sampling points along the straight line between
    real and generated images. Imagine a slider: at one end is the real image, and
    at the other is the fake image. As you move the slider, you see a continuous blend
    from the real to the fake, with the interpolated images representing the stages
    in between.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数 `GP()` 中，我们首先创建真实图像和假图像的插值图像。这是通过沿真实图像和生成图像之间直线随机采样点来完成的。想象一个滑块：一端是真实图像，另一端是假图像。当你移动滑块时，你会看到从真实到假的连续混合，插值图像代表中间的阶段。
- en: We then present interpolated images to the critic network to obtain ratings
    on them and calculate the gradient of the critic’s output with respect to the
    interpolated images. Finally, the gradient penalty is calculated as the squared
    deviation of the gradient norms from the target value of 1.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将插值图像呈现给评论网络以获取对其的评分，并计算评论网络输出相对于插值图像的梯度。最后，梯度惩罚被计算为梯度范数与目标值1的平方偏差。
- en: 5.4 Training the cGAN
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 训练cGAN
- en: As we mentioned in the last section, we need to find a way to tell both the
    critic and the generator what the image label is so they know if the image has
    eyeglasses or not.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中提到的，我们需要找到一种方法来告诉评论网络和生成网络图像标签是什么，以便它们知道图像是否有眼镜。
- en: In this section, you’ll first learn how to add labels to the inputs to the critic
    network and the inputs to the generator network so the generator knows what type
    of images to create while the critic can evaluate the images conditional on the
    labels. After that, you’ll learn how to train the cGAN with Wasserstein distance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将首先学习如何向评论网络和生成网络的输入添加标签，以便生成网络知道要创建哪种类型的图像，而评论网络可以基于标签评估图像。之后，你将学习如何使用Wasserstein距离训练cGAN。
- en: 5.4.1 Adding labels to inputs
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 为输入添加标签
- en: 'We first preprocess the data and convert the images to torch tensors:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先预处理数据并将图像转换为torch张量：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We set the batch size to 16 and the image size to 256 by 256 pixels. The pixel
    values are chosen so the generated images have higher resolutions than those in
    the last chapter (64 by 64 pixels). We choose a batch size of 16, smaller than
    the batch size in chapter 3, due to the larger image size. If the batch size is
    too large, your GPU (or even CPU) will run out of memory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将批处理大小设置为16，图像大小为256x256像素。像素值被选择，以便生成的图像分辨率高于上一章中的图像（64x64像素）。由于图像尺寸较大，我们选择批处理大小为16，小于第3章中的批处理大小。如果批处理大小太大，你的GPU（甚至CPU）可能会耗尽内存。
- en: tip If you are using GPU training and your GPU has a small memory (say, 6GB),
    consider reducing the batch size to a smaller number than 16, such as 10 or 8,
    so that your GPU doesn’t run out of memory. Alternatively, you can keep the batch
    size at 16 but switch to CPU training to address the GPU memory problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你使用GPU训练，并且你的GPU内存较小（比如6GB），考虑将批处理大小减少到小于16的较小数字，例如10或8，这样你的GPU就不会耗尽内存。或者，你可以保持批处理大小为16，但切换到CPU训练以解决GPU内存问题。
- en: Next, we’ll add labels to the training data. Since there are two types of images—images
    with eyeglasses and images without glasses—we’ll create two one-hot image labels.
    Images with glasses will have a one-hot label of [1, 0], and images without glasses
    will have a one-hot label of [0, 1].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将标签添加到训练数据中。由于有两种类型的图像——有眼镜的图像和无眼镜的图像——我们将创建两个one-hot图像标签。有眼镜的图像将有一个one-hot标签[1,
    0]，无眼镜的图像将有一个one-hot标签[0, 1]。
- en: 'The input to the generator is a 100-value random noise vector. We concatenate
    the one-hot label with the random noise vector and feed the 102-value input to
    the generator. The input to the critic network is a three-channel color image
    with a shape of 3 by 256 by 256 (PyTorch uses channel-first tensors to represent
    images). How do we attach a label with a shape of 1 by 2 to an image with a shape
    of 3 by 256 by 256? The solution is to add two channels to the input image so
    that the image shape changes from (3, 256, 256) to (5, 256, 256): the two additional
    channels are the one-hot labels. Specifically, if an image has eyeglasses in it,
    the fourth channel is filled with 1s and the fifth channel 0s; if the image has
    no eyeglasses in it, the fourth channel is filled with 0s and the fifth channel
    1s.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络的输入是一个100个值的随机噪声向量。我们将one-hot标签与随机噪声向量连接，并将102个值的输入馈送到生成网络。评论网络输入是一个形状为3x256x256的三通道彩色图像（PyTorch使用通道优先张量来表示图像）。我们如何将形状为1x2的标签附加到形状为3x256x256的图像上？解决方案是在输入图像中添加两个通道，使图像形状从(3,
    256, 256)变为(5, 256, 256)：这两个额外的通道是one-hot标签。具体来说，如果一个图像中有眼镜，第四个通道将填充1，第五个通道填充0；如果一个图像中没有眼镜，第四个通道将填充0，第五个通道填充1。
- en: Creating labels if there are more than two values in a characteristic
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征值多于两个的情况下创建标签
- en: You can easily extend the cGAN model to characteristics with more than two values.
    For example, if you create a model to generate images with the different hair
    colors black, blond, and white, the image labels you feed to the generator can
    have values [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively. You can attach
    three channels to the input image before you feed it to the discriminator or critic.
    For example, if an image has black hair, the fourth channel is filled with 1s
    and the fifth and sixth channels 0s.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地将 cGAN 模型扩展到具有两个以上值的特征。例如，如果您创建一个生成不同头发颜色（黑色、金色和白色）的图像的模型，您馈送到生成器的图像标签可以分别具有值
    [1, 0, 0]、[0, 1, 0] 和 [0, 0, 1]。在您将图像馈送到判别器或评论家之前，您可以附加三个通道到输入图像。例如，如果一个图像有黑色头发，第四个通道填充
    1s，第五和第六个通道填充 0s。
- en: 'Additionally, in the eyeglasses example, since there are only two values in
    the label, you can potentially use values 0 and 1 to indicate images with and
    without glasses when you feed the label to the generator. You can attach one channel
    to the input image before you feed it to the critic: if an image has eyeglasses,
    the fourth channel is filled with 1s; if the image has no eyeglasses, the fourth
    channel is filled with 0s. I’ll leave that as an exercise for you. The solution
    is provided in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在眼镜示例中，由于标签中只有两个值，当您将标签馈送到生成器时，您可以潜在地使用值 0 和 1 来表示戴眼镜和不戴眼镜的图像。在您将图像馈送到评论家之前，您可以附加一个通道到输入图像：如果一个图像有眼镜，第四个通道填充
    1s；如果一个图像没有眼镜，第四个通道填充 0s。我将把这个留给你作为练习。解决方案在本书的 GitHub 仓库中提供：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: We implement this change as shown in the following listing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下列表所示实现此更改。
- en: Listing 5.6 Attaching labels to input images
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 将标签附加到输入图像
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Creates two extra channels filled with 0s, each channel with a shape of 256
    by 256, the same as the dimension of each channel in the input image
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建两个填充为 0s 的额外通道，每个通道的形状为 256x256，与输入图像中每个通道的维度相同
- en: ② If the original image label is 0, fills the fourth channel with 1s
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果原始图像标签是 0，则将第四个通道填充为 1s
- en: ③ If the original image label is 1, fills the fifth channel with 1s
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果原始图像标签是 1，则将第五个通道填充为 1s
- en: ④ Adds the fourth and fifth channels to the original image to form a five-channel
    labeled image
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将第四和第五个通道添加到原始图像中，形成一个五通道标签图像
- en: tip Earlier when we load the images by using the `torchvision.datasets.ImageFolder()`
    method from the folder /files/glasses, PyTorch assigns labels to images in each
    subfolder in alphabetical order. Therefore, images in /files/glasses/G/ are assigned
    a label of 0, and those in /files/glasses/NoG/, a label of 1.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在我们之前使用 `torchvision.datasets.ImageFolder()` 方法从文件夹 /files/glasses 加载图像时，PyTorch
    按字母顺序将标签分配给每个子文件夹中的图像。因此，/files/glasses/G/ 中的图像被分配标签 0，而 /files/glasses/NoG/ 中的图像被分配标签
    1。
- en: We first create an empty list `newdata` to hold images with labels. We create
    a PyTorch tensor with a shape (2, 256, 256) to be attached to the original input
    image to form a new image with a shape of (5, 256, 256). If the original image
    label is 0 (this means images are from the folder /files/glasses/G/), we fill
    the fourth channel with 1s and the fifth channel with 0s so that the critic knows
    it’s an image with glasses. On the other hand, if the original image label is
    1 (this means images are from the folder /files/glasses/NoG/), we fill the fourth
    channel with 0s and the fifth channel with 1s so that the critic knows it’s an
    image without glasses.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个空的列表 `newdata` 来保存带有标签的图像。我们创建一个形状为 (2, 256, 256) 的 PyTorch 张量，将其附加到原始输入图像上，以形成一个形状为
    (5, 256, 256) 的新图像。如果原始图像标签是 0（这意味着图像来自文件夹 /files/glasses/G/），我们填充第四个通道为 1s，第五个通道为
    0s，以便评论家知道这是一张戴眼镜的图像。另一方面，如果原始图像标签是 1（这意味着图像来自文件夹 /files/glasses/NoG/），我们填充第四个通道为
    0s，第五个通道为 1s，以便评论家知道这是一张不带眼镜的图像。
- en: 'We create a data iterator with batches (to improve computational efficiency,
    memory usage, and optimization dynamics in the training process) as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个具有批次的迭代器（为了提高训练过程中的计算效率、内存使用和优化动态），如下所示：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 5.4.2 Training the cGAN
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 训练 cGAN
- en: Now that we have the training data and two networks, we’ll train the cGAN. We’ll
    use visual inspections to determine when the training should stop.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练数据和两个网络，我们将训练 cGAN。我们将使用视觉检查来确定何时停止训练。
- en: Once the model is trained, we’ll discard the critic network and use the generator
    to create images with a certain characteristic (with or without glasses, in our
    case).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们将丢弃评论家网络，并使用生成器创建具有特定特征（在我们的例子中是带眼镜或不带眼镜）的图像。
- en: We’ll create a function to test periodically what the generated images look
    like.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个函数来定期检查生成的图像的外观。
- en: Listing 5.7 Inspecting generated images
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7 检查生成的图像
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Creates a one-hot label for images with glasses
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为带眼镜的图像创建一个独热标签
- en: ② Feeds the concatenated noise vector and label to the generator to create images
    with glasses
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将拼接的噪声向量和标签输入生成器以创建带眼镜的图像
- en: ③ Plots the generated images with glasses
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 绘制带有眼镜的生成图像
- en: ④ Creates a one-hot label for images without glasses
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为不带眼镜的图像创建一个独热标签
- en: 'After each epoch of training, we’ll ask the generator to create a set of images
    with glasses and a set of images without glasses. We then plot the images so that
    we can inspect them visually. To create images with glasses, we first create one-hot
    labels [1, 0] and attach them to the random noise vectors before feeding the concatenated
    vector to the generator network. The generator creates images with glasses since
    the label is [1, 0] instead of [0, 1]. We then plot the generated images in four
    rows and eight columns and save the subplots on your computer. The process of
    creating images without glasses is similar, except that we use the one-hot label
    [0, 1] instead of [1, 0]. I skipped part of the code in listing 5.7, but you can
    find it in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练周期结束后，我们将要求生成器创建一组带眼镜的图像和一组不带眼镜的图像。然后我们绘制这些图像，以便我们可以直观地检查它们。要创建带眼镜的图像，我们首先创建一个独热标签[1,
    0]，并将其附加到随机噪声向量上，然后再将拼接的向量输入到生成器网络中。由于标签是[1, 0]而不是[0, 1]，生成器创建了带眼镜的图像。然后我们在四行八列中绘制生成的图像，并将子图保存在您的计算机上。创建不带眼镜的图像的过程类似，只是我们使用独热标签[0,
    1]而不是[1, 0]。我在列表5.7中省略了一部分代码，但您可以在本书的GitHub仓库中找到它：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: We define a `train_batch()` function to train the model with a batch of data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`train_batch()`函数，用于使用一批数据训练模型。
- en: Listing 5.8 Training the model with a batch of data
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 使用一批数据训练模型
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① A batch of real images with labels
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ① 带有标签的一批真实图像
- en: ② A batch of generated images with labels
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ② 带有标签的一批生成图像
- en: '③ The total loss for the critic has three components: loss from evaluating
    real images, loss from evaluating fake images, and the gradient penalty loss.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 评论家的总损失有三个组成部分：评估真实图像的损失、评估伪造图像的损失以及梯度惩罚损失。
- en: ④ Trains the generator with the Wasserstein loss
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用Wasserstein损失训练生成器
- en: In the `train_batch()` function, we first train the critic with real images.
    We also ask the generator to create a batch of fake data with the given label.
    We then train the critic with fake images. In the `train_batch()` function, we
    also train the generator with a batch of fake data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_batch()`函数中，我们首先使用真实图像训练评论家。我们还要求生成器根据给定的标签创建一批伪造数据。然后我们使用伪造图像训练评论家。在`train_batch()`函数中，我们还使用一批伪造数据训练生成器。
- en: 'NOTE The loss for the critic has three components: loss from evaluating real
    images, loss from evaluating fake images, and the gradient penalty loss.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：评论家的损失有三个组成部分：评估真实图像的损失、评估伪造图像的损失以及梯度惩罚损失。
- en: 'We now train the model for 100 epochs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对模型进行100个周期的训练：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Iterates through all batches in the training dataset
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历训练数据集中的所有批次
- en: ② Trains the model with a batch of data
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用一批数据训练模型
- en: ③ Saves the weights in the trained generator
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 保存训练生成器的权重
- en: After each epoch of training, we print out the critic loss and the generator
    loss to ensure that the losses are in a reasonable range. We also generate 32
    images of faces with glasses as well as 32 images without glasses by using the
    `plot_epoch()` function we defined earlier. We save the weights in the trained
    generator in the local folder after training is done so that later we can generate
    images using the trained model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练周期结束后，我们打印出评论家损失和生成器损失，以确保损失在合理的范围内。我们还使用我们之前定义的`plot_epoch()`函数生成32张带眼镜的面部图像以及32张不带眼镜的面部图像。训练完成后，我们在本地文件夹中保存训练生成器的权重，以便以后可以使用训练模型生成图像。
- en: 'This training takes about 30 minutes if you are using GPU training. Otherwise,
    it may take several hours, depending on the hardware configuration on your computer.
    Alternatively, you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip](https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip).
    Unzip the file after downloading.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 GPU 训练，这种训练大约需要 30 分钟。否则，可能需要几个小时，具体取决于你电脑的硬件配置。或者，你可以从我的网站下载训练好的模型：[https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip](https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip)。下载后请解压文件。
- en: 5.5 Selecting characteristics in generated images
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 在生成的图像中选择特征
- en: 'There are at least two ways to generate images with a certain characteristic.
    The first is to attach a label to a random noise vector before feeding it to the
    trained cGAN model. Different labels lead to different characteristics in the
    generated image (in our case, whether the image has eyeglasses). The second way
    is to select the noise vector you feed to the trained model: while one vector
    leads to an image with a male face, another leads to an image with a female face.
    Note that the second way works even in a traditional GAN such as the ones we trained
    in chapter 4\. It works in a cGAN as well.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 生成具有特定特征的图像至少有两种方法。第一种是在将随机噪声向量输入到训练好的 cGAN 模型之前为其附加标签。不同的标签会导致生成的图像具有不同的特征（在我们的例子中，图像是否包含眼镜）。第二种方法是选择输入到训练模型的噪声向量：一个向量会导致具有男性面部的图像，另一个向量会导致具有女性面部的图像。请注意，第二种方法即使在传统的
    GAN（如我们在第 4 章训练的 GAN）中也有效。它同样适用于 cGAN。
- en: 'Better yet, in this section, you’ll learn to combine these two methods so you
    can select two characteristics simultaneously: an image of a male face with eyeglasses
    or a female face without eyeglasses, and so on.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，在本节中，你将学习如何结合这两种方法，以便你可以同时选择两个特征：带有眼镜的男性面部或无眼镜的女性面部，等等。
- en: There are pros and cons for each one of these two methods in selecting a certain
    characteristic in generated images. The first way, the cGAN, requires labeled
    data to train the model. Sometimes, labeled data is costly to curate. However,
    once you have successfully trained a cGAN, you can generate a wide range of images
    with a certain characteristic. In our case, you can generate many different images
    with eyeglasses (or without eyeglasses); each one is different from the other.
    The second way, handpicking a noise vector, doesn’t need labeled data to train
    the model. However, each handpicked noise vector can only generate one image.
    If you want to generate many different images with the same characteristic as
    the cGAN, you’ll need to handpick many different noise vectors ex ante.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择生成图像中的特定特征时，这两种方法各有优缺点。第一种方法，cGAN，需要标记数据来训练模型。有时，标记数据可能难以整理。然而，一旦你成功训练了一个
    cGAN，你就可以生成具有特定特征的广泛图像。在我们的例子中，你可以生成许多不同的带有眼镜（或无眼镜）的图像；每张图像都与其他图像不同。第二种方法，手动选择噪声向量，不需要标记数据来训练模型。然而，每个手动选择的噪声向量只能生成一张图像。如果你想生成与
    cGAN 相同特征的许多不同图像，你将需要事先手动选择许多不同的噪声向量。
- en: 5.5.1 Selecting images with or without eyeglasses
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 选择是否带有眼镜的图像
- en: By attaching a label of either [1, 0] or [0, 1] to a random noise vector before
    you feed it to the trained cGAN model, you can select whether the generated image
    has eyeglasses.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在将随机噪声向量附加标签 [1, 0] 或 [0, 1] 并将其输入到训练好的 cGAN 模型之前，你可以选择生成的图像是否包含眼镜。
- en: First, we’ll use the trained model to generate 32 images with glasses and plot
    them in a 4 × 8 grid. To make results reproducible, we’ll fix the random state
    in PyTorch. Further, we’ll use the same set of random noise vectors so that we
    look at the same set of faces.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用训练好的模型生成 32 张带有眼镜的图像，并在 4 × 8 网格中绘制它们。为了使结果可重复，我们将在 PyTorch 中固定随机状态。此外，我们将使用相同的随机噪声向量集，以便我们查看相同的面部集合。
- en: We fix the random state at seed 0 and generate 32 images of faces with eyeglasses.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随机状态固定在种子 0，并生成 32 张带有眼镜的面部图像。
- en: Listing 5.9 Generating images of human faces with eyeglasses
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.9 生成带有眼镜的人类面部图像
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Fixes the random state so results are reproducible
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 固定随机状态以确保结果可重复
- en: ② Loads up the trained weights
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ② 加载训练好的权重
- en: ③ Generates a set of random noise vectors and saves it so we can select certain
    vectors from it to perform vector arithmetic
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 生成一组随机噪声向量并将其保存，以便我们可以从中选择某些向量进行向量运算
- en: ④ Creates a label to generate images with eyeglasses
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建标签以生成带有眼镜的图像
- en: 'We create another instance of the `Generator()` class and name it `generator`.
    We then load up the trained weights that we saved in the local folder in the last
    section (or you can download the weights from my website: [https://mng.bz/75Z4](https://mng.bz/75Z4)).
    To generate 32 images of human faces with eyeglasses; we first draw 32 random
    noise vectors in the latent space. We’ll also create a set of labels and name
    them `labels_g`, and they tell the generator to produce 32 images with eyeglasses.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`Generator()`类的另一个实例，并将其命名为`generator`。然后我们加载上一节中保存在本地文件夹中的训练好的权重（或者你也可以从我的网站上下载权重：[https://mng.bz/75Z4](https://mng.bz/75Z4)）。为了生成32张带眼镜的人类面部图像；我们首先在潜在空间中绘制32个随机噪声向量。我们还将创建一组标签，并将其命名为`labels_g`，它们告诉生成器生成32张带眼镜的图像。
- en: If you run the program in listing 5.9, you’ll see 32 images as shown in figure
    5.5.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行列表5.9中的程序，你会看到如图5.5所示的32张图像。
- en: '![](../../OEBPS/Images/CH05_F05_Liu.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F05_Liu.png)'
- en: Figure 5.5 Images of human faces with eyeglasses that are generated by the trained
    cGAN model
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 由训练好的cGAN模型生成的带眼镜的人类面部图像
- en: 'First, all 32 images do have eyeglasses in them. This indicates that the trained
    cGAN model is able to generate images conditional on the provided labels. You
    may have noticed that some images have male features while others have female
    features. To prepare us for vector arithmetic in the next subsection, we’ll select
    one random noise vector that leads to an image with male features and one that
    leads to female features. After inspecting the 32 images in figure 5.5, we select
    images with index values 0 and 14, like so:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有32张图像中确实都包含眼镜。这表明训练好的cGAN模型能够根据提供的标签生成图像。你可能已经注意到，有些图像具有男性特征，而有些图像具有女性特征。为了让我们为下一节中的向量运算做准备，我们将选择一个导致具有男性特征的图像和一个导致具有女性特征的图像的随机噪声向量。在检查图5.5中的32张图像后，我们选择了索引值为0和14的图像，如下所示：
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To generate 32 images without eyeglasses, we first produce another set of random
    noise vectors and labels:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成32张无眼镜的图像，我们首先生成另一组随机噪声向量和标签：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The new set of random noise vectors is named `noise_ng`, and the new set of
    labels `labels_ng`. Feed them to the generator and you should see 32 images without
    eyeglasses, as shown in figure 5.6.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 新的随机噪声向量集被命名为`noise_ng`，新的标签集被命名为`labels_ng`。将它们输入到生成器中，你应该会看到32张没有眼镜的图像，如图5.6所示。
- en: 'None of the 32 faces in figure 5.6 has eyeglasses in it: the trained cGAN model
    can generate images contingent upon the given label. We select images with indexes
    8 (male) and 31 (female) to prepare for vector arithmetic in the next subsection:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6中的32张面孔中没有任何一张带有眼镜：训练好的cGAN模型可以根据给定的标签生成图像。我们选择了索引为8（男性）和31（女性）的图像，为下一节中的向量运算做准备：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../../OEBPS/Images/CH05_F06_Liu.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F06_Liu.png)'
- en: Figure 5.6 Images of human faces without eyeglasses that are generated by the
    trained cGAN model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 由训练好的cGAN模型生成的无眼镜的人类面部图像
- en: Next, we’ll use label interpolation to perform label arithmetic. Recall that
    the two labels, `noise_g` and `noise_ng`, instruct the trained cGAN model to create
    images with and without eyeglasses, respectively. What if we feed an interpolated
    label (a weighted average of the two labels [1, 0] and [0, 1]) to the model? What
    type of images will the trained generator produce? Let’s find out.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用标签插值来执行标签运算。回想一下，两个标签`noise_g`和`noise_ng`分别指示训练好的cGAN模型创建带眼镜和不带眼镜的图像。如果我们向模型提供一个插值标签（两个标签[1,
    0]和[0, 1]的加权平均值）会怎样？训练好的生成器会生成什么类型的图像？让我们来看看。
- en: Listing 5.10 Label arithmetic in cGAN
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.10 cGAN中的标签运算
- en: '[PRE19]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Creates five weights
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建五个权重
- en: ② Creates a weighted average of the two labels
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建两个标签的加权平均值
- en: ③ Gives the new label to the trained model to create an image
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将新的标签给训练好的模型以创建图像
- en: 'We first create five weights (w): 0, 0.25, 0.5, 0.75, and 1, equally spaced
    between 0 and 1\. Each of these five values of w is the weight we put on the no
    eyeglasses label `labels_ng`. The complementary weight is put on the eyeglasses
    label `labels_g`. The interpolated label therefore has a value of `w*labels_ng+(1-w)*labels_g`.
    We then feed the interpolated label to the trained model, along with the random
    noise vector `z_female_g` that we saved earlier. The five generated images, based
    on the five values of w, are plotted in a 1 × 5 grid, as shown in figure 5.7\.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建了五个权重（w）：0，0.25，0.5，0.75和1，它们在0和1之间均匀分布。这五个w值中的每一个都是我们放在无眼镜标签 `labels_ng`
    上的权重。互补权重放在眼镜标签 `labels_g` 上。因此，插值标签的值为 `w*labels_ng+(1-w)*labels_g`。然后我们将插值标签输入到训练好的模型中，以及我们之前保存的随机噪声向量
    `z_female_g`。基于五个w值生成的五个图像被绘制在一个1×5的网格中，如图5.7所示。
- en: '![](../../OEBPS/Images/CH05_F07_Liu.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F07_Liu.png)'
- en: 'Figure 5.7 Label arithmetic in cGAN. We first create two labels: the no eyeglasses
    label `labels_ng` and the eyeglasses label `labels_g`. These two labels instruct
    the trained generator to produce images with and without eyeglasses, respectively.
    We then create five interpolated labels, each as a weighted average of the original
    two labels: `w*labels_ng+(1-w)*labels_g`, where the weight `w` takes five different
    values, 0, 0.25, 0.5, 0.75, and 1\. The five generated images based on the five
    interpolated labels are shown in the figure. The image on the far left has eyeglasses.
    As we move from the left to the right, the eyeglasses gradually fade away, until
    the image on the far right has no eyeglasses in it.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 cGAN中的标签算术。我们首先创建两个标签：无眼镜标签 `labels_ng` 和眼镜标签 `labels_g`。这两个标签指示训练好的生成器分别生成带眼镜和不带眼镜的图像。然后我们创建了五个插值标签，每个标签都是原始两个标签的加权平均值：`w*labels_ng+(1-w)*labels_g`，其中权重
    `w` 取五个不同的值，0，0.25，0.5，0.75和1。基于五个插值标签生成的五个图像显示在图中。最左边的图像有眼镜。当我们从左到右移动时，眼镜逐渐消失，直到最右边的图像中没有眼镜。
- en: When you look at the five generated images in figure 5.7 from the left to the
    right, you’ll notice that the eyeglasses gradually fade away. The image on the
    left has eyeglasses while the image on the right has no eyeglasses. The three
    images in the middle show some signs of eyeglasses, but the eyeglasses are not
    as conspicuous as those in the first image.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从左到右查看图5.7中的五个生成图像时，你会注意到眼镜逐渐消失。左边的图像有眼镜，而右边的图像没有眼镜。中间的三张图像显示了一些眼镜的迹象，但眼镜并不像第一张图像中那么显眼。
- en: Exercise 5.1
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.1
- en: Since we used the random noise vector `z_female_g` in listing 5.10, the images
    in figure 5.7 have a female face. Change the noise vector to `z_male_g` in listing
    5.10 and rerun the program; see what the images look like.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在列表5.10中使用了随机噪声向量 `z_female_g`，图5.7中的图像具有女性脸。将噪声向量更改为列表5.10中的 `z_male_g`
    并重新运行程序；看看图像是什么样的。
- en: 5.5.2 Vector arithmetic in latent space
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 潜在空间中的向量算术
- en: 'You may have noticed that some generated human face images have male features
    while others have female features. You may wonder: Can we select male or female
    features in generated images? The answer is yes. We can achieve this by selecting
    noise vectors in the latent space.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，一些生成的人脸图像具有男性特征，而另一些则具有女性特征。你可能想知道：我们能否在生成的图像中选择男性或女性特征？答案是肯定的。我们可以通过在潜在空间中选择噪声向量来实现这一点。
- en: In the last subsection, we have saved two random noise vectors, `z_male_ng`
    and `z_female_ng`, that lead to images of a male face and a female face, respectively.
    Next, we feed a weighted average of the two vectors (i.e., an interpolated vector)
    to the trained model and see what the generated images look like.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个子节中，我们已经保存了两个随机噪声向量，`z_male_ng` 和 `z_female_ng`，分别对应男性脸和女性脸的图像。接下来，我们将这两个向量的加权平均值（即插值向量）输入到训练好的模型中，看看生成的图像是什么样的。
- en: Listing 5.11 Vector arithmetic to select image characteristics
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.11 向量算术以选择图像特征
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Creates five weights
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建五个权重
- en: ② Creates a weighted average of the two random noise vectors
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建两个随机噪声向量的加权平均值
- en: ③ Feeds the new random noise vector to the trained model to create an image
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将新的随机噪声向量输入到训练好的模型中，以创建一个图像
- en: We have created five weights, 0, 0.25, 0.5, 0.75, and 1\. We iterate through
    the five weights and create five weighted averages of the two random noise vectors,
    `w*z_female_ng+(1-w)*z_male_ng`. We then feed the five vectors, along with the
    label, `labels_ng`, to the trained model to obtain five images, as shown in figure
    5.8.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了五个权重，0，0.25，0.5，0.75和1。我们遍历这五个权重，创建两个随机噪声向量的五个加权平均值，`w*z_female_ng+(1-w)*z_male_ng`。然后我们将这五个向量以及标签`labels_ng`输入到训练好的模型中，以获得五个图像，如图5.8所示。
- en: '![](../../OEBPS/Images/CH05_F08_Liu.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F08_Liu.png)'
- en: 'Figure 5.8 Vector arithmetic in GAN. We first save two random noise vectors
    `z_female_ng` and `z_male_ng`. The two vectors lead to images of female and male
    faces, respectively. We then create five interpolated vectors, each as a weighted
    average of the original two vectors: `w*z_female_ng+(1-w)*z_male_ng`, where the
    weight `w` takes five different values, 0, 0.25, 0.5, 0.75, and 1\. The five generated
    images based on the five interpolated vectors are shown in the figure. The image
    on the far left has male features. As we move from the left to the right, the
    male features gradually fade away and the female features gradually appear, until
    the image on the far right shows a female face.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 GAN中的向量算术。我们首先保存两个随机噪声向量`z_female_ng`和`z_male_ng`。这两个向量分别导致女性和男性面孔的图像。然后我们创建五个插值向量，每个向量都是原始两个向量的加权平均值：`w*z_female_ng+(1-w)*z_male_ng`，其中权重`w`取五个不同的值，0，0.25，0.5，0.75和1。基于五个插值向量的五个生成的图像如图所示。最左边的图像具有男性特征。当我们从左向右移动时，男性特征逐渐消失，女性特征逐渐出现，直到最右边的图像显示女性面孔。
- en: Vector arithmetic can transition from one instance of an image to another instance.
    Since we happen to have selected a male and a female image, when you look at the
    five generated images in figure 5.8 from the left to the right, you’ll notice
    that male features gradually fade away and female features gradually appear. The
    first image shows an image with a male face while the last image shows an image
    with a female face.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 向量算术可以从一个图像实例过渡到另一个图像实例。由于我们偶然选择了男性和女性图像，当你从左到右查看图5.8中的五个生成的图像时，你会注意到男性特征逐渐消失，女性特征逐渐出现。第一张图像显示男性面孔，而最后一张图像显示女性面孔。
- en: Exercise 5.2
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.2
- en: Since we used the label `labels_ng` in listing 5.11, the images in figure 5.8
    have no eyeglasses in them. Change the label to `labels_g` in listing 5.11 and
    rerun the program to see what the images look like.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在列表5.11中使用了标签`labels_ng`，图5.8中的图像中没有眼镜。在列表5.11中将标签更改为`labels_g`并重新运行程序，以查看图像的外观。
- en: 5.5.3 Selecting two characteristics simultaneously
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 同时选择两个特征
- en: So far, we have selected one characteristic at a time. By selecting the label,
    you have learned how to generate images with or without eyeglasses. By selecting
    a specific noise vector, you have learned how to select a specific instance of
    the generated image.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一次只选择一个特征。通过选择标签，你已经学会了如何生成带眼镜或不带眼镜的图像。通过选择特定的噪声向量，你已经学会了如何选择生成的图像的特定实例。
- en: 'What if you want to select two characteristics (glasses and gender, for example)
    at the same time? There are four possible combinations of the two independent
    characteristics: male faces with glasses, male faces without glasses, female faces
    with glasses, and female faces without glasses. Next we’ll generate an image of
    each type.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想同时选择两个特征（例如眼镜和性别），这两个独立特征的组合有四种可能：有眼镜的男性面孔、无眼镜的男性面孔、有眼镜的女性面孔和无眼镜的女性面孔。接下来我们将生成每种类型的图像。
- en: Listing 5.12 Selecting two characteristics simultaneously
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.12 同时选择两个特征
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Iterates through 0 to 3
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历0到3
- en: ② The value of p, which can be either 0 or 1, selects the random noise vector
    to generate a male or female face.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ② p的值可以是0或1，用于选择随机噪声向量以生成男性或女性面孔。
- en: ③ The value of q, which can be either 0 or 1, selects the label to determine
    whether the generated image has eyeglasses in it or not.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ③ q的值可以是0或1，用于选择标签以确定生成的图像中是否包含眼镜。
- en: ④ Combines the random noise vector with the label to select two characteristics
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将随机噪声向量与标签结合以选择两个特征
- en: 'To generate four images to cover the four different cases, we need to use one
    of the noise vectors as the input: `z_female_g` or `z_male_g`. We also need to
    attach to the input a label, which can be either `labels_ng` or `labels_g.` To
    use one single program to cover all four cases, we iterate through four values
    of i, 0 to 3, and create two values, p and q, which are the integer quotient and
    the remainder of the value i divided by 2\. Therefore, the values of p and q can
    be either 0 or 1\. By setting the value of the random noise vector to `z_female_g*p+z_male_g*(1-p)`,
    we can select a random noise vector to generate either a male or female face.
    Similarly, by setting the value of the label to `labels_ng[0]*q+labels_g[0]*(1-q)`,
    we can select a label to determine whether the generated image has eyeglasses
    in it or not. Once we combine the random noise vector with the label and feed
    them to the trained model, we can select two characteristics simultaneously.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成四个图像以覆盖四种不同的情况，我们需要使用其中一个噪声向量作为输入：`z_female_g` 或 `z_male_g`。我们还需要将一个标签附加到输入上，这个标签可以是
    `labels_ng` 或 `labels_g`。为了使用一个单独的程序覆盖所有四种情况，我们遍历 i 的四个值，0 到 3，并创建两个值，p 和 q，它们是
    i 除以 2 的整数商和余数。因此，p 和 q 的值可以是 0 或 1。通过将随机噪声向量的值设置为 `z_female_g*p+z_male_g*(1-p)`，我们可以选择一个随机噪声向量来生成男性或女性面孔。同样，通过将标签的值设置为
    `labels_ng[0]*q+labels_g[0]*(1-q)`，我们可以选择一个标签来决定生成的图像中是否包含眼镜。一旦我们将随机噪声向量与标签结合并输入到训练好的模型中，我们就可以同时选择两个特征。
- en: If you run the program in listing 5.12, you’ll see four images as shown in figure
    5.9.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行列表 5.12 中的程序，你会看到如图 5.9 所示的四个图像。
- en: '![](../../OEBPS/Images/CH05_F09_Liu.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F09_Liu.png)'
- en: 'Figure 5.9 Selecting two characteristics simultaneously in the generated image.
    We select a noise vector from the following two choices: `z_female_ng` and `z_male_ng`.
    We also select a label from the following two choices: `labels_ng` and `labels_g`.
    We then feed the noise vector and the label to the trained generator to create
    an image. Based on the values of the noise vector and the label, the trained model
    can create four types of images. By doing this, we effectively select two independent
    characteristics in the generated image: a male or a female face and whether the
    image has eyeglasses in it or not.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 同时在生成的图像中选择两个特征。我们从以下两个选项中选择一个噪声向量：`z_female_ng` 和 `z_male_ng`。我们还从以下两个选项中选择一个标签：`labels_ng`
    和 `labels_g`。然后我们将噪声向量和标签输入到训练好的生成器中，以创建一个图像。根据噪声向量和标签的值，训练好的模型可以创建四种类型的图像。通过这种方式，我们有效地在生成的图像中选择了两个独立特征：一个男性或女性面孔，以及图像中是否包含眼镜。
- en: 'The four generated images in figure 5.9 have two independent characteristics:
    a male or a female face and whether the image has eyeglasses in it or not. The
    first image shows an image of a male face with glasses; the second image is a
    male face without glasses. The third image is a female face with glasses, while
    the last image shows a female face without glasses.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 中生成的四个图像有两个独立特征：一个男性或女性面孔，以及图像中是否包含眼镜。第一幅图像显示了一个戴眼镜的男性面孔；第二幅图像是一个不戴眼镜的男性面孔。第三幅图像是一个戴眼镜的女性面孔，而最后一幅图像显示了一个不戴眼镜的女性面孔。
- en: Exercise 5.3
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: We used the two random noise vectors `z_female_g` and `z_male_g` in listing
    5.12\. Change the two random noise vectors to `z_female_ng` and `z_male_ng` instead
    and rerun the program to see what the images look like.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表 5.12 中使用了两个随机噪声向量 `z_female_g` 和 `z_male_g`。将这两个随机噪声向量改为 `z_female_ng`
    和 `z_male_ng`，然后重新运行程序，看看图像看起来像什么。
- en: 'Finally, we can conduct label arithmetic and vector arithmetic simultaneously.
    That is, we can feed an interpolated noise vector and an interpolated label to
    the trained cGAN model and see what the generated image looks like. You can achieve
    that by running the following code block:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以同时进行标签算术和向量算术。也就是说，我们可以将插值噪声向量和插值标签输入到训练好的 cGAN 模型中，看看生成的图像是什么样子。你可以通过运行以下代码块来实现：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The code is similar to that in listing 5.12, except that p and q each can take
    six different values: 0, 1, 2, 3, 4, and 5\. The random noise vector, `z_female_ng*p/5+z_male_ng*(1-p/5)`,
    takes six different values based on the value of p. The label, `labels_ng[0]*q/5+labels_g[0]*(1-q/5)`,
    takes six different values based on the value of q. We therefore have 36 different
    combinations of images based on the interpolated noise vector and the interpolated
    label. If you run the previous program, you’ll see 36 images as shown in figure
    5.10.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与列表 5.12 中的代码类似，除了 p 和 q 每个都可以取六个不同的值：0、1、2、3、4 和 5。随机噪声向量 `z_female_ng*p/5+z_male_ng*(1-p/5)`
    根据 p 的值取六个不同的值。标签 `labels_ng[0]*q/5+labels_g[0]*(1-q/5)` 根据 q 的值取六个不同的值。因此，基于插值噪声向量和插值标签，我们有
    36 种不同的图像组合。如果你运行前面的程序，你会看到如图 5.10 所示的 36 张图像。
- en: '![](../../OEBPS/Images/CH05_F10_Liu.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F10_Liu.png)'
- en: 'Figure 5.10 Conducting vector arithmetic and label arithmetic simultaneously.
    The value of i changes from 0 to 35; p and q are the integer quotient and remainder,
    respectively, when i is divided by 6\. Therefore, p and q each can take six different
    values: 0, 1, 2, 3, 4, and 5\. The interpolated noise vector, `z_female_ng*p/5+z_male_ng*(1-p/5)`,
    and the interpolated label, `labels_ng[0]*q/5+labels_g[0]*(1-q/5)`, can each take
    six different values. In each row, when you go from left to right, the eyeglasses
    gradually fade away. In each column, when you go from top to bottom, the image
    changes gradually from a male face to a female face.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 同时进行向量算术和标签算术。i 的值从 0 变化到 35；p 和 q 分别是 i 除以 6 的整数商和余数。因此，p 和 q 每个都可以取六个不同的值：0、1、2、3、4
    和 5。插值噪声向量 `z_female_ng*p/5+z_male_ng*(1-p/5)` 和插值标签 `labels_ng[0]*q/5+labels_g[0]*(1-q/5)`
    都可以取六个不同的值。在每一行中，从左到右，眼镜逐渐消失。在每一列中，从上到下，图像逐渐从男性面孔变为女性面孔。
- en: The are 36 images in figure 5.10\. The interpolated noise vector is a weighted
    average of the two random noise vectors, `z_female_ng` and `z_male_ng`, which
    generate a female face and a male face, respectively. The label is a weighted
    average of the two labels, `labels_ng` and `labels_g`, which determine whether
    the generated image has eyeglasses in it or not. The trained model generates 36
    different images based on the interpolated noise vector and the interpolated label.
    In each row, when you go from the left to the right, the eyeglasses gradually
    fade away. That is, we conduct label arithmetic in each row. In each column, when
    you go from the top to the bottom, the image changes gradually from a male face
    to a female face. That is, we conduct vector arithmetic in each column.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 中有 36 张图像。插值噪声向量是两个随机噪声向量 `z_female_ng` 和 `z_male_ng` 的加权平均值，分别生成女性面孔和男性面孔。标签是两个标签
    `labels_ng` 和 `labels_g` 的加权平均值，它们决定生成的图像中是否有眼镜。训练模型根据插值噪声向量和插值标签生成 36 张不同的图像。在每一行中，从左到右，眼镜逐渐消失。也就是说，我们在每一行中进行标签算术。在每一列中，从上到下，图像逐渐从男性面孔变为女性面孔。也就是说，我们在每一列中进行向量算术。
- en: Exercise 5.4
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: 'In this project, there are two values in the label: one indicates eyeglasses
    and one indicates no eyeglasses. Therefore, we can use a binary value instead
    of one-hot variables as labels. Change the programs in this chapter and use values
    1 and 0 (instead of [1, 0] and [0, 1]) to represent images with and without glasses.
    Attach 1 or 0 to the random noise vector so that you feed a 101-value vector to
    the generator. Attach one channel to the input image before you feed it to the
    critic: if an image has eyeglasses in it, the fourth channel is filled with 0s;
    if the image has no eyeglasses in it, the fourth channel is filled with 1s. Then
    create a generator and a critic; use the training dataset to train them. The solution
    is provided in the book’s GitHub repository, along with solutions to the other
    three exercises in this chapter.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，标签中有两个值：一个表示眼镜，一个表示没有眼镜。因此，我们可以使用二进制值而不是 one-hot 变量作为标签。更改本章的程序，并使用值
    1 和 0（而不是 [1, 0] 和 [0, 1]）来表示有眼镜和无眼镜的图像。将 1 或 0 附着到随机噪声向量上，以便向生成器提供 101 值的向量。在将图像输入到评论家之前，附加一个通道：如果图像中有眼镜，第四个通道填充
    0s；如果图像中没有眼镜，第四个通道填充 1s。然后创建一个生成器和评论家；使用训练数据集来训练它们。解决方案在本书的 GitHub 仓库中提供，包括本章其他三个练习的解决方案。
- en: 'Now that you have witnessed what GAN models are capable of, you’ll explore
    deeper in the next chapter by conducting style transfers with GANs. For example,
    you’ll learn how to build a CycleGAN model and train it using celebrity face images
    so that you can convert blond hair to black hair or black hair to blond hair in
    these images. The exact same model can be trained on other datasets: for example,
    you can train it on the human face dataset you used in this chapter so that you
    can add or remove eyeglasses in human face images.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经见证了 GAN 模型的能力，你将在下一章通过使用 GAN 进行风格迁移来进一步探索。例如，你将学习如何构建 CycleGAN 模型，并使用名人面部图像对其进行训练，以便在这些图像中将金发变为黑发或将黑发变为金发。相同的模型可以在其他数据集上训练：例如，你可以在本章中使用的人类面部数据集上训练它，以便在人类面部图像中添加或移除眼镜。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: By selecting a certain noise vector in the latent space and feeding it to the
    trained GAN model, we can select a certain characteristic in the generated image,
    such as whether the image has a male or female face in it.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在潜在空间中选择一个特定的噪声向量并将其输入到训练好的 GAN 模型中，我们可以选择生成图像中的某个特定特征，例如图像中是否有男性或女性面部。
- en: A cGAN is different from a traditional GAN. We train the model on labeled data
    and ask the trained model to generate data with a specific attribute. For example,
    one label tells the model to generate images of human faces with eyeglasses while
    another tells the model to create human faces without eyeglasses.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cGAN 与传统的 GAN 不同。我们在标记数据上训练模型，并要求训练好的模型生成具有特定属性的数据。例如，一个标签告诉模型生成戴眼镜的人类面部图像，而另一个标签则告诉模型创建不戴眼镜的人类面部图像。
- en: After a cGAN is trained, we can use a series of weighted averages of the labels
    to generate images that transition from an image represented by one label to an
    image represented by another label—for example, a series of images in which the
    eyeglasses gradually fade away on the same person’s face. We call this label arithmetic.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 cGAN 训练完成后，我们可以使用一系列标签的加权平均来生成从由一个标签表示的图像过渡到由另一个标签表示的图像的图像——例如，一系列同一人面部上的眼镜逐渐消失的图像。我们称这种算术为标签算术。
- en: We can also use a series of weighted averages of two different noise vectors
    to create images that transition from one attribute to another—for example, a
    series of images in which the male features gradually fade away, and female features
    gradually appear. We call this vector arithmetic.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以使用两个不同噪声向量的加权平均来创建从一种属性过渡到另一种属性的图像——例如，一系列男性特征逐渐消失，女性特征逐渐出现的图像。我们称这种向量为向量算术。
- en: Wasserstein GAN (WGAN) is a technique used to improve the training stability
    and performance of GAN models by using Wasserstein distance instead of the binary
    cross-entropy as the loss function. Further, for the Wasserstein distance to work
    correctly, the critic in WGANs must be 1-Lipschitz continuous, meaning the gradient
    norms of the critic’s function must be at most 1 everywhere. The gradient penalty
    in WGANs adds a regularization term to the loss function to enforce the Lipschitz
    constraint more effectively.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein GAN (WGAN) 是一种通过使用 Wasserstein 距离而不是二元交叉熵作为损失函数来提高 GAN 模型训练稳定性和性能的技术。此外，为了
    Wasserstein 距离能够正确工作，WGAN 中的评判器必须是 1-Lipschitz 连续的，这意味着评判器函数的梯度范数必须在任何地方都小于等于
    1。WGAN 中的梯度惩罚通过向损失函数添加正则化项来更有效地强制执行 Lipschitz 约束。
- en: '* * *'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-002-backlink))  Mehdi Mirza, Simon Osindero, 2014, “Conditional
    Generative Adversarial Nets.” [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-002-backlink))  Mehdi Mirza, Simon Osindero, 2014, “Conditional
    Generative Adversarial Nets.” [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
- en: ^([2](#footnote-001-backlink))  Martin Arjovsky, Soumith Chintala, and Léon
    Bottou, 2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](#footnote-001-backlink))  Martin Arjovsky, Soumith Chintala, 和 Léon Bottou,
    2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875).
- en: ^([3](#footnote-000-backlink))  Martin Arjovsky, Soumith Chintala, and Leon
    Bottou, 2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875);
    and Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron
    Courville, 2017, “Improved Training of Wasserstein GANs.” [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](#footnote-000-backlink))  Martin Arjovsky, Soumith Chintala, 和 Leon Bottou,
    2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875);
    以及 Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, 和 Aaron Courville,
    2017, “Improved Training of Wasserstein GANs.” [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028).
