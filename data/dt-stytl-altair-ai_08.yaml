- en: '7 From information to knowledge: Building textual context'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 从信息到知识：构建文本上下文
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Introducing context
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍上下文
- en: Calibrating the story to the audience
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整故事以适应受众
- en: Using ChatGPT for commentaries and annotations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ChatGPT进行评论和注释
- en: Using large language models for textual context
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大型语言模型进行文本上下文处理
- en: 'A case study: From information to knowledge (part 1)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究：从信息到知识（第一部分）
- en: Talking about knowledge in a computer science book might seem completely out
    of place. The word *knowledge* could inspire philosophical concepts or even intimidate.
    But in this chapter (and the next), we will not be talking about philosophical
    knowledge but, rather, the knowledge that helps the reader understand the context
    of a story. It is, therefore, knowledge applied to the context of our data story,
    rather than general knowledge. In these chapters, we will review the basic concepts
    behind context in a data story and how to adapt it based on the audience. First,
    we will focus on textual context in this chapter, and in the next one, we will
    cover images. We will introduce large language models (LLMs) and use ChatGPT as
    an example of LLM implementation for data storytelling. Finally, we will explore
    a practical example.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学书籍中谈论知识可能显得完全不合适。单词“知识”可能会激发哲学概念，甚至让人感到畏惧。但在这章（以及下一章），我们不会谈论哲学知识，而是谈论帮助读者理解故事背景的知识。因此，这是应用于我们的数据故事背景的知识，而不是一般知识。在这些章节中，我们将回顾数据故事中上下文的基本概念以及如何根据受众进行调整。首先，我们将在本章中关注文本上下文，而在下一章中，我们将涵盖图像。我们将介绍大型语言模型（LLMs），并以ChatGPT作为LLM在数据故事中的应用实例。最后，我们将探讨一个实际案例。
- en: 7.1 Introducing context
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 介绍上下文
- en: When I was a child, I often heard my parents discussing some topic and did not
    understand anything. Their words rang in my ears as meaningless until, eager to
    understand what they were talking about, I entered the conversation and asked
    for explanations. Then, my father or mother, very patiently, explained to me what
    they were talking about, adapting their adult reasoning to my child’s mind so
    that I, too, could understand. Years later, I found myself in the same situation
    as a mother. My children often ask me to explain more complex speech *in words
    they can understand*. And the satisfaction is enormous when I see their faces
    light up and understand what I’m saying.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我还是个孩子的时候，我经常听到我的父母讨论某个话题，却什么都不懂。他们的言语在我耳边回响，毫无意义，直到我渴望了解他们在谈论什么，我加入了对话并请求解释。然后，我的父亲或母亲，非常耐心地向我解释他们在谈论什么，调整他们的成人推理以适应我的孩子思维，这样我也能理解。多年以后，我发现我自己处于同样的情况，作为一个母亲。我的孩子们经常要求我用他们能理解的话来解释更复杂的言语。当我看到他们的脸上发光，理解我在说什么时，我感到无比的满足。
- en: The examples described show us the need to adapt the words we use according
    to the audience we are addressing. If we ignore who will receive our story, we
    risk talking in a way that may make perfect sense to ourselves but which excludes
    our audience from the message we want to communicate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的例子告诉我们，根据我们面向的受众调整我们使用的语言是必要的。如果我们忽视了谁将接收我们的故事，我们可能会以对我们自己来说完全合理的方式说话，但这样可能会排除我们的受众，使他们无法理解我们想要传达的信息。
- en: In the previous chapter, we looked at how to extract and represent an insight
    through a chart. The next step is to enrich the chart with context (text and images),
    making reading easier for the reference audience. *Context* refers to the surrounding
    elements allowing the audience to understand the displayed information, such as
    texts, images, and symbols. Data context should prepare the scene of your data
    story and raise interest in your audience. In this chapter, we’ll primarily be
    dealing with textual context, while in the next chapter, we’ll look more at visual
    context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何通过图表提取和表示洞察力。下一步是丰富图表的上下文（文本和图像），使参考受众的阅读更加容易。“上下文”指的是周围元素，使受众能够理解显示的信息，如文本、图像和符号。数据上下文应该为你的数据故事场景做好准备，并在受众中激发兴趣。在本章中，我们将主要处理文本上下文，而在下一章中，我们将更多地关注视觉上下文。
- en: Context depends on the type of audience you are addressing. For example, if
    you are talking with an adult about how much you paid for a product, you don’t
    need to explain how money works. On the other hand, if you are talking to your
    kids about the same topic, you probably need to explain the denominations of the
    different banknotes and how the monetary system works.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 情境取决于您要面对的受众类型。例如，如果您与成年人谈论您为产品支付了多少钱，您不需要解释金钱是如何运作的。另一方面，如果您与您的孩子们谈论相同的话题，您可能需要解释不同钞票的面额以及货币体系是如何运作的。
- en: You can use generative AI tools, such as ChatGPT for text and DALL-E for images,
    to ease context building. You have already learned the basic techniques for building
    context using generative AI tools. This chapter will focus on more advanced techniques
    to write an impactful context tailored to your audience.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用生成式AI工具，例如ChatGPT用于文本和DALL-E用于图像，以简化情境构建。您已经学习了使用生成式AI工具构建情境的基本技术。本章将重点介绍更高级的技术，以编写针对受众的具有影响力的情境。
- en: 'We will consider the following types of context:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑以下类型的情境：
- en: '*Commentary* —The text that precedes your insight. It includes the background
    that helps the audience to set the scene and understand the insight. In the example
    of the product cost explained to your kids, the commentary includes banknotes
    denominations, and how the monetary system works.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论* — 在您的见解之前的文本。它包括帮助观众设定场景和理解见解的背景。在向孩子们解释产品成本示例中，评论包括钞票面额和货币体系是如何运作的。'
- en: '*Annotation* —A short text that explains a detail of your chart, for example,
    an anomalous point or a trend line. Consider adding annotations only when necessary.
    Don’t overload your chart with unnecessary annotations.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注释* — 解释图表细节的简短文本，例如异常点或趋势线。仅在必要时添加注释。不要让图表因不必要的注释而负担过重。'
- en: '*Image* —A picture enforcing the commentary or the annotation. In the example
    of the product cost, you could add banknote images to help your kids understand
    the different denominations.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像* — 强调评论或注释的图片。在产品成本示例中，您可以添加钞票图片以帮助孩子们理解不同的面额。'
- en: '*Symbols* —Arrows, circles, lines, and so on, combined with annotations. They
    help the audience focus on particular points of your chart.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符号* — 箭头、圆圈、线条等，结合注释。它们有助于观众关注图表的特定点。'
- en: In the remainder of this chapter, we will use ChatGPT for commentaries and annotations.
    In the next chapter, we will focus on DALL-E for images and symbols. In addition,
    we will introduce LLMs and how to use them for commentaries and annotations. But
    first, let’s describe how to calibrate the story to our audience.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用ChatGPT进行评论和注释。在下一章中，我们将专注于DALL-E用于图像和符号。此外，我们将介绍LLMs以及如何使用它们进行评论和注释。但首先，让我们描述如何调整故事以适应我们的受众。
- en: 7.2 Calibrating the story to the audience
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 调整故事以适应受众
- en: A few years ago, I was invited to give a seminar to master’s students. The seminar
    topic concerned the implementation of web applications for the construction of
    data journalism projects. Unfortunately, I found myself faced with a somewhat
    embarrassing situation. My seminar topic was very technical, even commenting on
    some pieces of code. As I began to speak, I realized that the audience couldn’t
    follow me because they didn’t have the technical skills required to properly understand.
    My presentation was technically correct, but having spoken too technically to
    a non-technical audience, the result of my talk was that the audience learned
    very little. The experience I gained from that episode taught me to always learn
    about the audience I will be addressing before communicating any message.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我被邀请为研究生举办一次研讨会。研讨会的主题是关于构建数据新闻项目网络应用的实施。不幸的是，我发现我面临一个有些尴尬的情况。我的研讨会主题非常技术性，甚至评论了一些代码片段。当我开始讲话时，我意识到观众无法跟上我，因为他们没有理解所需的技术技能。我的演示在技术上正确，但由于对非技术受众讲得过于技术性，我的演讲结果是观众学到的很少。我从这次事件中获得的经验教会了我，在传达任何信息之前，一定要了解我将要面对的受众。
- en: The *audience* is the person or the group of persons reading your data story.
    Understanding the target audience is crucial to building data stories that convey
    information effectively. In the previous chapter, we saw that you can use multiple
    types of charts to convey information (table 6.4). Once you’ve chosen the set
    of charts that answer your question, you can refine your choice, tailoring the
    chart to your audience.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*受众* 是阅读您数据故事的个人或群体。理解目标受众对于构建能够有效传达信息的数据故事至关重要。在前一章中，我们了解到您可以使用多种类型的图表来传达信息（表6.4）。一旦您选择了回答您问题的图表集，您就可以进一步细化选择，根据受众调整图表。'
- en: 'In chapter 4, you learned that there are different types of audiences. For
    simplicity, in this chapter, we group them into three common types of audiences:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，您了解到有不同类型的受众。为了简单起见，在本章中，我们将它们分为三种常见的受众类型：
- en: General public
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通公众
- en: Executives
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高管
- en: Professionals
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专业人士
- en: 'Let’s investigate each type of audience separately. To explain how you can
    calibrate the chart to the target audience, we will use the case study described
    in chapter 4\. The objective of this case study was to understand which athletic
    disciplines our hypothetical team needed to continue to train to achieve the best
    possible results in the upcoming competitions. For convenience, figure 7.1 shows
    the complete data story we implemented: moving from data to wisdom.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别调查每种类型的受众。为了解释您如何调整图表以适应目标受众，我们将使用第4章中描述的案例研究。这个案例研究的目的是了解我们的假设团队需要继续训练哪些运动项目才能在即将到来的比赛中取得最佳成绩。为了方便起见，图7.1显示了我们所实施的数据故事的完整内容：从数据到智慧。
- en: '![figure](../Images/7-1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图表](../Images/7-1.png)'
- en: Figure 7.1 The use case described in chapter 4
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 第4章中描述的使用案例
- en: 7.2.1 General public
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 普通公众
- en: This audience includes individuals from various backgrounds and levels of knowledge.
    They may have little to no previous knowledge of your topic. When crafting data
    stories for the general public, use precise language, avoid overwhelming them
    with too much information, and focus on presenting the most relevant insights
    visually and engagingly. The general public could find the chart shown in figure
    7.1 complex, with an unnecessary baseline. As an alternative to the chart in figure
    7.1, you could draw the chart shown in figure 7.2, which some audiences may find
    more appealing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个受众包括来自不同背景和知识水平的个人。他们可能对您的话题知之甚少或一无所知。在为普通公众制作数据故事时，使用精确的语言，避免用过多信息使他们感到不知所措，并专注于以视觉和吸引人的方式呈现最相关的见解。普通公众可能会发现图7.1中的图表复杂，有不必要的基线。作为图7.1中图表的替代方案，您可以考虑绘制图7.2中的图表，某些受众可能会觉得它更有吸引力。
- en: '![figure](../Images/7-2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图表](../Images/7-2.png)'
- en: Figure 7.2 The use case adapted to the general public
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 适应普通公众的使用案例
- en: This chart is called a multi-layer donut chart. We could have placed the images
    close to the relevant bars, but in this case, there wasn’t enough space, so we
    placed them in the center of the chart. In other scenarios, you might consider
    placing images next to the bars. You can find the complete code to generate this
    chart in the GitHub repository for the book under 07/general-public.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表被称为多层饼图。我们本可以将图像放置在相关的条形旁边，但在这个案例中，空间不足，所以我们把它们放在图表的中心。在其他情况下，您可能考虑将图像放在条形旁边。您可以在本书的GitHub仓库下的07/general-public找到生成此图表的完整代码。
- en: 7.2.2 Executives
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 高管
- en: Executives are typically high-level decision makers in organizations, who rely
    on data-driven insights to make essential business choices. They often have limited
    time and need concise and actionable information. When creating data stories for
    executives, it is essential to present key findings, trends, and recommendations
    up front.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 高管通常是组织中高级决策者，他们依赖数据驱动的洞察来做出关键的商业选择。他们通常时间有限，需要简洁且可操作的信息。在为高管创建数据故事时，提前展示关键发现、趋势和建议是至关重要的。
- en: Use visualizations highlighting the most critical data points and providing
    a straightforward narrative linking the data to strategic goals. It can also be
    helpful to provide additional context or industry benchmarks to support your analysis.
    The chart shown in figure 7.1 could be great for executives because it does not
    contain many details and describes why we chose some sports, thanks to its baseline
    of 50%.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用突出显示最关键数据点的可视化，并直接将数据与战略目标联系起来。提供额外的背景或行业标准以支持你的分析也可能很有帮助。图7.1所示的图表对于高管来说可能很好，因为它不包含很多细节，并描述了我们为什么选择某些运动，这得益于其50%的基线。
- en: 7.2.3 Professionals
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 专业人士
- en: This audience consists of individuals with a specific domain expertise or professional
    background. They have a deeper understanding of data and require more analytical
    information. When creating data stories for professionals, explain the methodology,
    assumptions, and limitations of the data analysis. Consider including additional
    supporting data and references, allowing professionals to explore the data further.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个受众群体包括具有特定领域专业知识或专业背景的个人。他们对数据的理解更深入，需要更多分析信息。在为专业人士创建数据故事时，解释数据分析的方法、假设和局限性。考虑包括额外的支持数据和参考文献，使专业人士能够进一步探索数据。
- en: As an alternative to the chart in figure 7.1, you could draw the chart shown
    in figure 7.3, which some audiences may understand easily. The figure shows only
    the chart, without any annotation or context. You can find the complete code to
    generate this chart in the GitHub repository for the book under 07/professionals.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为7.1图中图表的替代方案，你可以绘制图7.3所示的图表，某些受众可能很容易理解。该图只显示了图表，没有任何注释或背景。你可以在本书的GitHub仓库下的07/professionals中找到生成此图表的完整代码。
- en: '![figure](../Images/7-3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-3.png)'
- en: Figure 7.3 The use case adapted to professionals
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 适用于专业人士的使用案例
- en: 'Table 7.1 summarizes what to represent in a chart based on the audience type.
    Now that you have learned how to adapt your chart based on the audience type,
    let’s move on to the next step: using ChatGPT for commentaries and annotations.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1总结了根据受众类型在图表中应表示的内容。现在你已经学会了如何根据受众类型调整你的图表，让我们继续下一步：使用ChatGPT进行评论和注释。
- en: Table 7.1 What to represent in a chart based on the audience type
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 根据受众类型在图表中应表示的内容
- en: '| Audience Type | Requirements | What to Represent |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 受众类型 | 需求 | 应表示的内容 |'
- en: '| --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| General public  | Understand data  | An appealing overview of insights  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 普通公众 | 理解数据 | 吸引人的见解概述 |'
- en: '| Executives  | High-level overview of data trends to aid strategic decision
    making  | Highlight critical metrics and trends influencing business outcomes.  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 高管 | 高层次的数据趋势概述，以辅助战略决策 | 突出关键指标和影响业务成果的趋势。 |'
- en: '| Professionals  | Detailed insights to understand the phenomenon behind data  |
    Add numbers, statistics, and useful information to understand insights deeply.  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 专业人士 | 深入理解数据背后的现象的详细见解 | 添加数字、统计和有用信息以深入理解见解。 |'
- en: 7.3 Using ChatGPT for commentaries and annotations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 使用ChatGPT进行评论和注释
- en: In his novella, *Metamorphosis*, Franz Kafka tells the story of Gregor Samsa,
    a traveling salesman who wakes up one morning transformed into a giant insect.
    Encased in this insect’s guise, Samsa cannot interact with his family or communicate
    his thoughts. Gregor’s family struggles to accept his transformation, leading
    to their relationship with Gregor deteriorating and Gregor becoming increasingly
    isolated. The novella unearths the fundamental isolation that emerges when one’s
    inner world remains inaccessible to others. Data analysts could find themselves
    in a situation quite similar to that experienced by Gregor Samsa in Kafka’s novella
    when they have to add text to a data visualization chart. The data analyst, by
    nature, is a technician and could encounter some difficulties in writing engaging
    text.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的小说《变形记》中，弗朗茨·卡夫卡讲述了格里高尔·萨姆沙的故事，他是一位旅行推销员，一天早上醒来发现自己变成了一个巨大的昆虫。萨姆沙被困在这个昆虫的外壳中，无法与家人互动或传达他的想法。萨姆沙的家人努力接受他的转变，导致他们与萨姆沙的关系恶化，萨姆沙变得越来越孤立。这部小说揭示了当一个人的内心世界对他人不可接近时出现的根本孤立。数据分析师可能会发现自己处于与卡夫卡小说中格里高尔·萨姆沙相似的情况，当他们不得不向数据可视化图表添加文本时。数据分析师本质上是一名技术人员，可能会在撰写引人入胜的文本时遇到一些困难。
- en: 'ChatGPT can assist you in adding textual context to your data visualization
    chart. You have already learned that a prompt’s basic structure for ChatGPT comprises
    three main elements: role, audience, and task.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT可以帮助你在你的数据可视化图表中添加文本上下文。你已经了解到，ChatGPT的提示的基本结构包括三个主要元素：角色、受众和任务。
- en: For example, you can write *Act as an entertainer* (role)*, writing for decision
    makers* (audience)*. Write 5 titles about <topic>* (task). The topic could be
    whatever you want. The main problem is structuring the topic so that ChatGPT produces
    the correct context. To include the topic in the ChatGPT prompt as well, we will
    generate context following the schema shown in figure 7.4.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以写上*扮演一个表演者*（角色）*，为决策者写作*（受众）*。为<主题>写5个标题*（任务）。主题可以是任何你想要的内容。主要问题是如何构建主题，以便ChatGPT产生正确的上下文。为了将主题包含在ChatGPT的提示中，我们将根据图7.4所示的方案生成上下文。
- en: '![figure](../Images/7-4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-4.png)'
- en: Figure 7.4 The schema used to generate context
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 生成上下文所使用的方案
- en: 'In a prompt, we specify the following four main elements:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中，我们指定以下四个主要元素：
- en: '*Role* —The role you want ChatGPT to take. You learned about many role types
    in chapter 4, including entertainer, educator, informer, inspirer, inviter to
    action, and relationship builder.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*角色* — 你希望ChatGPT扮演的角色。在第4章中，你已经学习了多种角色类型，包括表演者、教育者、信息提供者、激励者、行动邀请者和关系构建者。'
- en: '*Audience* —The audience of your chart. There are different types of audiences,
    such as the general public, executives, and professionals.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*受众* — 你图表的受众。有不同类型的受众，例如公众、高管和专业人士。'
- en: '*Topic* —The subject of your chart.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主题* — 你图表的主题。'
- en: '*Type* —The text type to generate, including annotations and commentaries.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*类型* — 要生成的文本类型，包括注释和评论。'
- en: The process of generating context is iterative, in the sense that you can generate
    the context multiple times if you are not satisfied with the produced result.
    For example, you can adjust one or more elements to make ChatGPT converge on the
    desired output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 生成上下文的过程是迭代的，这意味着如果你对产生的结果不满意，你可以多次生成上下文。例如，你可以调整一个或多个元素，使ChatGPT收敛到期望的输出。
- en: In the remainder of this section, we will focus on how to write the topic and
    type elements of the schema while keeping the role and the audience simple. However,
    you can adapt the strategies described for the topic and the audience to the other
    elements.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将关注如何编写方案的主题和类型元素，同时保持角色和受众简单。然而，你可以将描述主题和受众的策略适应到其他元素。
- en: 'As an example of how to build the context, we will focus on the case study
    described in chapter 4 and shown in figure 7.1\. The following text summarizes
    the scenario for convenience: *Imagine you work in a sports company. You are training
    a team of young athletes in various disciplines. For each discipline, you have
    noted the world record and recorded the best time achieved by your team for comparison.
    Unfortunately, your company has limited investment funds available. Your boss
    asks you to understand which disciplines are worth training in, hoping to achieve
    good results in the upcoming competitions.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建上下文的例子，我们将关注第4章中描述并在图7.1中展示的案例研究。以下文本为了方便起见总结了场景：*想象你在一个体育公司工作。你正在训练一支在各个学科领域的年轻运动员队伍。对于每个学科，你都记录了世界纪录和你的队伍达到的最佳时间以供比较。不幸的是，你的公司可用的投资资金有限。你的老板要求你了解哪些学科值得训练，希望在下一次比赛中取得好成绩。*
- en: 7.3.1 Describing the topic
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 描述主题
- en: '*Describing the topic* means composing simple words that precisely depict for
    ChatGPT what you have discovered and shown in your chart. The more precise you
    are, the better the output will be.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*描述主题*意味着组成简单的词汇，精确地描述给ChatGPT你在图表中发现和展示的内容。你越精确，输出结果就越好。'
- en: 'To describe the topic, focus on three aspects: scenario, data, and insight,
    as shown in figure 7.5\. Let’s go through each of those three aspects in a bit
    more detail.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 描述主题时，关注三个方面的内容：场景、数据和洞察，如图7.5所示。让我们更详细地探讨这三个方面。
- en: '![figure](../Images/7-5.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-5.png)'
- en: Figure 7.5 The elements used to describe the topic
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 描述主题所使用的元素
- en: Scenario
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景
- en: 'Describe an overview of your scenario, including the background and objective
    of the analysis. For the scenario in figure 7.1, we could write the following
    prompt for ChatGPT: *We are training a team of young athletes in various disciplines.
    For each discipline, we have calculated the percentage improvement of each discipline
    compared to the world record in that discipline. The objective is to search for
    the best two disciplines to fund.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 描述你场景的概述，包括分析的背景和目标。对于图7.1中的场景，我们可以为ChatGPT编写以下提示：*我们正在训练一支在各个运动项目中都有参与的年轻运动员队伍。对于每个项目，我们计算了每个项目与该项目世界纪录相比的百分比提升。目标是寻找两个最佳项目进行资助。*
- en: Data
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据
- en: Provide an overview of your data. This may include explaining the type of data,
    its source, and any manipulations you performed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 提供你数据的概述。这可能包括解释数据的类型、来源以及你进行的任何操作。
- en: Describe the data using your own words, providing a more personalized description.
    By manually describing the data, you can highlight important patterns, trends,
    or correlations that may not be apparent through automated methods alone. Additionally,
    through manual descriptions, you can incorporate domain expertise observations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你自己的话描述数据，提供更个性化的描述。通过手动描述数据，你可以突出通过自动化方法本身可能不明显的重要模式、趋势或相关性。此外，通过手动描述，你可以融入领域专业知识观察。
- en: 'For the scenario in figure 7.1, write the following prompt text: *There are
    five sports disciplines: Rowing (percentage improvement = 62.32%), Cycling (57.64%),
    Sprinting (42.69%), Long-distance running (18.31%), and Swimming (12.38%).* Now
    that you have learned how to describe the data, let’s move on to the last step:
    describing insights.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图7.1中的场景，编写以下提示文本：*有五种运动项目：划船（百分比提升=62.32%），自行车（57.64%），短跑（42.69%），长距离跑（18.31%），和游泳（12.38%）。现在你已经学会了如何描述数据，接下来让我们进入最后一步：描述见解。*
- en: Insights
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 见解
- en: 'Describe the central insights you have derived from the analysis, such as key
    patterns, trends, correlations, or relationships you have discovered. For the
    scenario in figure 7.2, include the following text: *Rowing and Cycling percentages
    are more significant than the baseline of 50%.* Before illustrating how we can
    build the audience description, let’s test the prompt built so far on ChatGPT.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 描述你从分析中得出的主要见解，例如你发现的关键模式、趋势、相关性或关系。对于图7.2中的场景，包括以下文本：*划船和自行车的百分比比50%的基线更为重要。在我们展示如何构建观众描述之前，让我们在ChatGPT上测试到目前为止构建的提示。
- en: Test
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试
- en: 'We write the following prompt: *Act as an inspirer. Write 5 titles for the
    following topic. There are five sports disciplines: Rowing (percentage improvement
    = 62.32%), Cycling (57.64%), Sprinting (42.69%), Long-distance running (18.31%),
    and Swimming (12.38%). Rowing and Cycling percentages are more significant than
    the baseline of 50%.* Figure 7.6 shows a possible output produced by ChatGPT.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写以下提示：*扮演一个启发者。为以下主题编写5个标题。有五种运动项目：划船（百分比提升=62.32%），自行车（57.64%），短跑（42.69%），长距离跑（18.31%），和游泳（12.38%）。划船和自行车的百分比高于50%的基线。图7.6显示了ChatGPT可能生成的输出。*
- en: '![figure](../Images/7-6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-6.png)'
- en: Figure 7.6 The five titles generated by ChatGPT
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 ChatGPT生成的五个标题
- en: 'As an alternative, you can ask ChatGPT to produce the context as follows: *Act
    as an inspirer. Write the context of a chart using 30 words for the following
    topic. There are five sports* *disciplines: Rowing (percentage improvement = 62.32%),
    Cycling (57.64%), Sprinting (42.69%),* *Long-distance running (18.31%), and Swimming
    (12.38%). Rowing and Cycling percentages are greater than the baseline of 50%.*
    Figure 7.7 shows a possible output produced by ChatGPT.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，你可以要求ChatGPT生成以下内容：*扮演一个启发者。使用30个词为以下主题编写图表的上下文。有五种运动项目：划船（百分比提升=62.32%），自行车（57.64%），短跑（42.69%），*长距离跑（18.31%），和游泳（12.38%）。划船和自行车的百分比高于50%的基线。图7.7显示了ChatGPT可能生成的输出。
- en: '![figure](../Images/7-7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-7.png)'
- en: Figure 7.7 The context generated by ChatGPT
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 ChatGPT生成的上下文
- en: 'Now that you have learned how to describe the topic, try to generate the context
    for the case studies described in chapter 1: the pets scenario. For example, you
    can act as an informer. For convenience, we summarize the scenario: *The organizers
    of an event dedicated to pets are collecting the type of pets that will participate.
    For each pet category, the organizers advertise the event on specific websites
    dedicated to that category. The organizers ask you to build a quick report about
    the current situation.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何描述主题，尝试为第1章中描述的案例研究生成上下文：宠物场景。例如，您可以扮演信息提供者的角色。为了方便，我们总结了场景：*一个专门为宠物举办的活动组织者正在收集将参加活动的宠物类型。对于每个宠物类别，组织者在专门针对该类别的网站上宣传活动。组织者要求您构建一份关于当前情况的快速报告。*
- en: 'For more details, please refer to chapter 1\. You can find a prompt with the
    generated context here: [https://mng.bz/EZvJ](https://mng.bz/EZvJ). For further
    practice, write the topic for the other scenarios described in the previous chapters.
    Now that you have learned how to describe the topic, let’s move on to the next
    element: describing the type.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详情请参阅第1章。您可以在此处找到生成的上下文的提示：[https://mng.bz/EZvJ](https://mng.bz/EZvJ)。为了进一步练习，请为前几章中描述的其他场景编写主题。现在您已经学会了如何描述主题，让我们继续到下一个元素：描述类型。
- en: 7.3.2 Describing the type
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 描述类型
- en: 'We consider the following types: commentary and annotations. In the previous
    sections, you have seen different ways to instruct ChatGPT to generate context
    types, such as writing the context of a chart using 30 words.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑以下类型：评论和注释。在前几节中，您已经看到了不同的方法来指导ChatGPT生成上下文类型，例如使用30个单词编写图表的上下文。
- en: 'When describing the type, be as precise as possible, specifying the following
    aspects:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述类型时，尽可能精确，指定以下方面：
- en: The type (commentary, annotation, or general text)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型（评论、注释或普通文本）
- en: The maximum number of words to generate
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成单词的最大数量
- en: My suggestion is to try different types and evaluate the results based on your
    needs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是尝试不同的类型，并根据您的需求评估结果。
- en: 7.3.3 Setting custom instructions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 设置自定义指令
- en: ChatGPT enables you to configure custom instructions for all your new chats.
    For example, if we build our charts for the same audience type and act with the
    same role, we can use this property as a default configuration for ChatGPT.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT使您可以为所有新聊天配置自定义指令。例如，如果我们为同一受众类型构建图表并扮演相同的角色，我们可以将此属性用作ChatGPT的默认配置。
- en: To enable custom instructions, access the ChatGPT web interface, click the three
    dots near your profile, and then click Custom Instructions. In the new window,
    write the custom instructions. For example, you can use the first box to configure
    your role as well as the target audience and the second box for more details,
    such as the number of words to generate, the tone, and the style, as shown in
    figure 7.8.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用自定义指令，访问ChatGPT网页界面，点击您个人资料附近的三个点，然后点击“自定义指令”。在新窗口中，编写自定义指令。例如，您可以使用第一个框来配置您的角色以及目标受众，第二个框用于更多详细信息，例如要生成的单词数、语气和风格，如图7.8所示。
- en: '![figure](../Images/7-8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-8.png)'
- en: Figure 7.8 The Custom Instructions dialog box
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8 自定义指令对话框
- en: Click the Save button to enable the custom instructions for new chats. If you
    want to disable this property, deselect the Enable for New Chats property in the
    dialog box. Then, click the Save button.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“保存”按钮以启用新聊天的自定义指令。如果您想禁用此属性，请在对话框中取消选中“为新聊天启用”属性。然后，点击“保存”按钮。
- en: 'Let’s try the custom instructions with the previous text (we have removed the
    number of words to generate, since we have configured them in the custom instructions):
    *Write the context of a chart for the following topic. There are five sports disciplines:
    Rowing (percentage improvement = 62.32%), Cycling (57.64%), Sprinting (42.69%),
    Long-distance running (18.31%), and Swimming (12.38%). Rowing and Cycling percentages
    are greater than the baseline of 50%.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用前面的文本（我们已移除要生成的单词数，因为我们已在自定义指令中进行了配置）：*为以下主题编写图表的上下文。有五种运动项目：划船（百分比提升=62.32%），自行车（57.64%），短跑（42.69%），长跑（18.31%）和游泳（12.38%）。划船和自行车的百分比高于50%的基线。*
- en: Figure 7.9 shows a possible output. Notice, for example, the informal tone we
    have set in the custom instructions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9展示了可能的一个输出。注意，例如，我们在自定义指令中设定的非正式语气。
- en: '![figure](../Images/7-9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-9.png)'
- en: Figure 7.9 A possible output produced when configuring custom instructions
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9 配置自定义指令时可能产生的输出
- en: For optimal usage of custom instructions in data storytelling, I suggest using
    them to configure the role and the audience. In addition, you can configure other
    specific details correlated to your job or data, as specified in the ChatGPT documentation
    ([https://mng.bz/8wAP](https://mng.bz/8wAP)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在数据叙事中最佳地使用自定义指令，我建议使用它们来配置角色和受众。此外，你可以配置与你的工作或数据相关的其他特定细节，如ChatGPT文档中所述（[https://mng.bz/8wAP](https://mng.bz/8wAP)）。
- en: 'Now that you have learned how to use ChatGPT for commentaries and annotations,
    let’s move on to the next step: using large language models in a different way.
    So far, you have used the web interface provided by OpenAI to write your prompt
    for ChatGPT. Now, we will cover an advanced use of LLMs, based on APIs calls.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何使用ChatGPT进行评论和注释，让我们继续下一步：以不同的方式使用大型语言模型。到目前为止，你已经使用了OpenAI提供的Web界面来为ChatGPT编写提示。现在，我们将介绍基于API调用的LLM的高级使用。
- en: 7.4 Using large language models for context
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用大型语言模型进行上下文处理
- en: 'So far, you have used three generative AI tools: Copilot for code generation,
    ChatGPT for text generation, and DALL-E for image generation. All these tools
    are examples of application usage of a *large language model* (LLM). An LLM is
    a machine learning (ML) model aimed at predicting plausible language. LLMs have
    exploded in popularity since 2017, when Google researchers introduced the concept
    of transformers, a revolutionary architecture that allowed the training of large
    language models, such as generative pretrained transformers (GPTs), on which ChatGPT
    is based, and bidirectional encoder representations from transformers (BERT).
    Transformers allowed for the training of LLMs on massive datasets, resulting in
    models with incredible language generation capabilities.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经使用了三个生成式人工智能工具：Copilot用于代码生成，ChatGPT用于文本生成，DALL-E用于图像生成。所有这些工具都是*大型语言模型*（LLM）应用使用的例子。LLM是一种旨在预测合理语言的机器学习（ML）模型。自2017年谷歌研究人员引入了transformers的概念以来，LLM的受欢迎程度急剧上升，这是一种革命性的架构，它允许在大型语言模型上进行训练，例如基于生成预训练的transformers（GPTs），ChatGPT就是基于此，以及双向编码器表示的transformers（BERT）。Transformers使得在大量数据集上训练LLM成为可能，从而产生了具有令人难以置信的语言生成能力的模型。
- en: In this book, we will not focus on how LLMs work. Instead, we aim to demonstrate
    how you can use them effectively for data storytelling. However, if you’re interested
    in delving deeper into the technical aspects, a vast bibliography is available
    on the topic (Vaswani, 2017; Koenigstein, 2024).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们不会关注LLM是如何工作的。相反，我们旨在展示你如何有效地使用它们进行数据叙事。然而，如果你对深入了解技术方面感兴趣，关于这个主题有大量的参考文献可供查阅（Vaswani，2017；Koenigstein，2024）。
- en: 'Before you embark on using LLM to build your data-driven story, it’s essential
    to ask yourself whether the model needs to know specific information related to
    your domain of work, as shown in figure 7.10\. If the answer is no, then you can
    safely continue using ChatGPT. If, however, your answer is yes, then you can apply
    one of the following techniques:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始使用大型语言模型（LLM）构建你的数据驱动故事之前，问自己模型是否需要了解与你工作领域相关的特定信息，如图7.10所示。如果答案是“否”，那么你可以安全地继续使用ChatGPT。然而，如果你的答案是“是”，那么你可以应用以下技术之一：
- en: '*Fine-tuning* —This technique adapts a pretrained LLM to a specific domain
    by updating its parameters on task-specific data, optimizing its performance for
    that domain.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微调* — 这种技术通过在特定任务数据上更新其参数，将预训练的LLM适应特定领域，优化其在该领域的性能。'
- en: '*Retrieval augmented generation* —This technique combines information retrieval
    and language generation, enabling LLMs to incorporate external knowledge sources
    during the generation process.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索增强生成* — 这种技术结合了信息检索和语言生成，使大型语言模型（LLMs）能够在生成过程中整合外部知识源。'
- en: '![figure](../Images/7-10.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-10.png)'
- en: Figure 7.10 A criterion to establish whether to extend an LLM or not
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10 建立是否扩展LLM的标准
- en: 'In the remainder of this section, we assume that your answer is yes and that
    you must tailor your LLM to your specific domain. A practical case where fine-tuning
    is useful could be when you must generate different data stories for the same
    type of audience or even the same audience. In this case, you could build your
    database with the same structure of annotations so that all new annotations have
    the same structure as the previous ones. This may generate some familiarity for
    your audience when they read your data stories. In other cases, you may need to
    use RAG, for example, when you have a long document and you want to build a short
    annotation for your data story based on it. Using RAG could help you to build
    textual summaries. Now that you have learned the potential benefits of extending
    the LLM, let’s start by analyzing the first strategy: fine-tuning.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们假设你的答案是肯定的，并且你必须根据你的特定领域定制你的大型语言模型。微调有用的实际案例可能包括当你必须为同一类型的受众或甚至同一受众生成不同的数据故事时。在这种情况下，你可以使用相同的注释结构构建你的数据库，以便所有新的注释都与之前的注释具有相同的结构。当你的受众阅读你的数据故事时，这可能会产生一些熟悉感。在其他情况下，你可能需要使用RAG，例如，当你有一个长文档，你想要基于它为你的数据故事构建一个简短的注释时。使用RAG可以帮助你构建文本摘要。现在你已经了解了扩展LLM的潜在好处，让我们首先分析第一种策略：微调。
- en: 7.4.1 Fine-tuning
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 微调
- en: GPT-3 was trained on 17 gigabytes of data, and GPT-4, the most recent model
    of OpenAI, has 45 gigabytes of training data. This means they contain a variety
    of information you can use in almost all cases. However, in some cases, fine-tuning
    your model could provide better results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3在17GB的数据上进行了训练，而OpenAI的最新模型GPT-4有45GB的训练数据。这意味着它们包含几乎在所有情况下都可以使用的信息。然而，在某些情况下，微调你的模型可能会提供更好的结果。
- en: Fine-tuning is the process of further training a pretrained language model on
    a specific dataset that is more relevant to your specific domain. During fine-tuning,
    you use a smaller dataset, which typically contains examples and specific input–output
    pairs relevant to your task. In practice, the dataset is a collection of samples,
    each containing the prompt and the suggested completion.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是在特定数据集上进一步训练预训练语言模型的过程，该数据集与你的特定领域更相关。在微调过程中，你使用一个较小的数据集，该数据集通常包含与你的任务相关的示例和特定的输入-输出对。在实践中，数据集是一组样本的集合，每个样本包含提示和建议的完成内容。
- en: When you apply fine-tuning to data storytelling, you can build a different dataset
    for each audience type, leading to better results. For example, you can build
    a dataset for the general public, one for professionals, and another for decision
    makers. You can even create a different dataset for each scenario you work with
    (products, topic, and so on) and for each type of text you want to generate (title,
    annotation, and commentary). The more specific your dataset is, the better your
    results will be.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将微调应用于数据叙事时，你可以为每种受众类型构建不同的数据集，从而获得更好的结果。例如，你可以为公众构建一个数据集，为专业人士构建一个，为决策者构建另一个。你甚至可以为每个你工作的场景（产品、主题等）以及你想要生成的每种文本类型（标题、注释和评论）创建不同的数据集。你的数据集越具体，你的结果就会越好。
- en: 'Preparing the dataset is the most significant effort during the process of
    fine-tuning. In the remainder of this section, we will describe two strategies
    to prepare the dataset: manual building and building from sources. In both cases,
    we will use the OpenAI API. For more details on the installation, refer to appendix
    A.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据集是微调过程中的最大努力。在本节的剩余部分，我们将描述两种准备数据集的策略：手动构建和从来源构建。在两种情况下，我们都会使用OpenAI API。有关安装的更多详细信息，请参阅附录A。
- en: Manual building
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 手动构建
- en: 'Manual building involves defining each pair (prompt, completion) manually.
    This solution enables you to obtain the best results since you specify the exact
    behavior of your model, given a specific input. Consider, for example, the following
    pair:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建涉及手动定义每个对（提示，完成）。这种解决方案使你能够获得最佳结果，因为你指定了模型在特定输入下的确切行为。例如，考虑以下对：
- en: '*Prompt* —Generate a title for the general public about topic X.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示* — 为关于主题X的公众生成一个标题。'
- en: '*Completion* —X revealed to you!'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完成* — X向你揭示了！'
- en: 'Now, imagine that you have fine-tuned your model with that pair and want to
    use your model to generate titles for the general public. If you give *the theory
    of relativity* topic as input to your fine-tuned model, it will probably generate
    a title similar to the following one: *The theory of relativity revealed to you!*
    The drawback of this strategy is that it is time consuming because you must write
    each pair manually.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设您已经使用这对数据对模型进行了微调，并想使用您的模型为公众生成标题。如果您将相对论主题作为输入提供给您的微调模型，它可能会生成以下类似的标题：*相对论揭秘给你！*
    这种策略的缺点是它耗时，因为您必须手动编写每一对。
- en: To start, you can define a minimum number of curated samples covering all your
    possible cases. The OpenAI model requires you to represent at least 10 samples.
    Next, train the model. After that, proceed with the model evaluation by considering
    the original model (i.e., without fine-tuning) as a reference. Provide the same
    prompt to the two models, original and fine-tuned, and compare the produced outputs.
    Use your new model if your fine-tuned model performs better than the original
    one. Instead, if it performs worse than or has the same behavior as the original
    model, try to add new samples or improve the existing ones. Repeat this procedure
    until you reach a good result.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可以定义一个最小数量的精选样本，涵盖所有可能的案例。OpenAI模型要求您至少表示10个样本。接下来，训练模型。然后，通过将原始模型（即，没有微调）作为参考，进行模型评估。向原始模型和微调模型提供相同的提示，并比较产生的输出。如果您的微调模型比原始模型表现更好，则使用您的新模型。相反，如果它表现不如或与原始模型表现相同，则尝试添加新的样本或改进现有的样本。重复此过程，直到达到良好的结果。
- en: To show how manual building works, we will build a dataset tailored for the
    general public and generate a commentary as an output. In the previous chapter,
    you saw that you should use a different chart based on the information you want
    to convey. Here, we will build a different output based on the information to
    convey. We will build one or more (prompt–completion) pairs for each type of information
    to convey. Table 7.2 shows a possible dataset representing the described scenario.
    You can find the code described in this section in the GitHub repository for the
    book under 07/manual-chatgpt-fine-tuning.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示手动构建的工作原理，我们将构建一个针对公众的定制数据集，并生成一个输出作为评论。在前一章中，您看到应根据您想要传达的信息使用不同的图表。在这里，我们将根据需要传达的信息构建不同的输出。我们将为每种类型的信息构建一个或多个（提示-完成）对。表7.2显示了表示所述场景的可能数据集。您可以在本书的GitHub仓库下的07/manual-ChatGPT-fine-tuning部分找到本节中描述的代码。
- en: Table 7.2 The samples based on the information to convey
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2 基于需要传达的信息的样本
- en: '| Information to Convey | Prompt | Completion ^a |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 需要传达的信息 | 提示 | 完成 ^a |'
- en: '| --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Parts of a whole  | Percentage of participants in the conference by type
    (researchers 5%, students 30%, professors 65%)  | More professors participated
    in the conference (65%). Researchers were not interested in the event (5%).  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 整体部分 | 会议参与者类型百分比（研究人员 5%，学生 30%，教授 65%） | 参与会议的教授比例较高（65%）。研究人员对此次活动不感兴趣（5%）。
    |'
- en: '| Comparison among entities  | Comparison between red (80%), yellow, and green
    performance  | Compared to yellow and green, red experienced an improvement in
    performance of 80%.  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 实体之间的比较 | 红色（80%）、黄色和绿色性能的比较 | 与黄色和绿色相比，红色在性能上提高了80%。 |'
- en: '| Trend  | Sales changed in the last 12 months (–12%), due to fewer subscribers
    and video views.  | Over the last 12 months, sales decreased by 12% based on various
    metrics, including subscribers and video views.  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 趋势 | 上一个12个月内的销售变化（–12%），由于订阅者和视频观看次数减少。 | 在过去12个月内，基于各种指标（包括订阅者和视频观看次数），销售下降了12%。
    |'
- en: '| Outcomes of a survey or a questionnaire  | Questionnaire: 3 positive answers,
    7 negative answers  | Three out of 10 people answered the questionnaire with a
    positive answer.  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 调查或问卷的结果 | 问卷：3个正面回答，7个负面回答 | 在10个人中，有3个人对问卷给出了正面回答。 |'
- en: '| Distribution  | Sales of product A (+30%) and product B over the last 12
    months  | Compared to product B, sales of product A have increased by 30% over
    the last 12 months.  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 分布 | 产品A（+30%）和产品B在过去12个月的销售 | 与产品B相比，产品A在过去12个月的销售增长了30%。 |'
- en: '| Spatial information  | Sales in North Europe (+23%) compared to South Europe  |
    Compared to South Europe, sales in North Europe increased by 23%.  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 空间信息 | 与南欧相比，北欧的销售（+23%） | 与南欧相比，北欧的销售增长了23%。 |'
- en: '| Relationship  | The sales trend line from 1990 to 2020 increased by 120%.  |
    Between 1990 and 2020, sales increased by 120%.  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 关系  | 1990 年至 2020 年的销售趋势线增长了 120%。  | 在 1990 年至 2020 年间，销售额增长了 120%。  |'
- en: '| Comparison among entities  | Top ingredients for our recipe: sugar and salt  |
    The chosen ingredients for our recipe are sugar and salt.  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 实体比较  | 我们食谱的主要成分：糖和盐  | 我们食谱所选的成分是糖和盐。  |'
- en: '| Comparison among entities  | Comparison between gold (30), silver (20), and
    bronze (40)  | Bronze beats silver and gold with 40\.  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 实体比较  | 金（30）、银（20）和铜（40）之间的比较  | 铜以 40% 的比例击败了银和金。  |'
- en: '| Distribution  | Distribution of household chores (cooking 35%, cleaning 30%,
    laundry 20%, and yard work 15%)  | Cooking takes up the most significant portion
    at 35%. Cleaning follows at 30%, while laundry and yard work account for 20% and
    15%, respectively.  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 分布  | 家庭事务分配（烹饪 35%，清洁 30%，洗衣 20%，和园艺工作 15%）  | 烹饪占据了最大的比例，达到 35%。清洁紧随其后，为
    30%，而洗衣和园艺工作分别占 20% 和 15%。  |'
- en: '| a. The word *completion* may be confusing, but it is used by the OpenAI API.
    *Completion* refers to the output produced by the model. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| a. 单词 *completion* 可能会令人困惑，但这是 OpenAI API 使用的术语。*Completion* 指的是模型产生的输出。
    |'
- en: Once you have built the dataset, you must format it as a JSONL file. This file
    contains a list of messages. Consider each message as a separate chat where you
    can specify a general configuration, the user prompt, and the assistant (model)
    output, as shown in the following listing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了数据集，你必须将其格式化为 JSONL 文件。此文件包含一条消息列表。将每条消息视为一个单独的聊天，你可以指定一个通用配置、用户提示和助手（模型）输出，如下所示。
- en: Listing 7.1 The structure of a JSONL file
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.1 JSONL 文件的结构
- en: '[PRE0]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note  Use the keyword `messages` to define the list of samples. Imagine each
    sample as a separate chat, where you can specify the model role: `system`, for
    general configuration; `user`, for user prompt; and `assistant`, for model output.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意  使用关键字 `messages` 定义样本列表。想象每个样本都是一个单独的聊天，你可以指定模型角色：`system` 用于通用配置；`user`
    用于用户提示；`assistant` 用于模型输出。
- en: If your dataset is saved as a CSV file, use the code shown in the following
    listing, which is also available in prepare-data.py, to convert it into JSONL.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集以 CSV 文件保存，请使用以下列表中显示的代码进行转换，该代码也包含在 prepare-data.py 中，以将其转换为 JSONL。
- en: Listing 7.2 How to convert the CSV file into JSONL
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.2 如何将 CSV 文件转换为 JSONL
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note  First, load the dataset as a pandas DataFrame. Next, format it in the
    JSONL format, as described in listing 7.1\. Finally, save the generated JSONL
    file.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意  首先，将数据集作为 pandas DataFrame 加载。接下来，按照列表 7.1 中描述的格式将其格式化为 JSONL 格式，最后保存生成的
    JSONL 文件。
- en: Now, we are ready to fine-tune our model. We need an `OPENAI_API_KEY`, as specified
    in appendix A. If you are transitioning from a free to a for-fee plan, you might
    need to generate a new API key because the initial key does not work after the
    switch to a for-fee plan. Open a terminal, and export your `OPENAI_API_KEY` as
    an environment variable (`export` `OPENAI_API_KEY='my` `key'`). Next, upload the
    produced file to the OpenAI server, and when the uploading process is complete,
    create a job for fine-tuning. The following listing shows the code to perform
    these operations. Alternatively, read the tune-model.py script in the GitHub repository
    for the book. Remember that this option is exclusively available with the for-fee
    version.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好微调我们的模型。我们需要一个 `OPENAI_API_KEY`，如附录 A 中所述。如果你从免费计划转换到付费计划，你可能需要生成一个新的
    API 密钥，因为初始密钥在切换到付费计划后不再有效。打开终端，并将你的 `OPENAI_API_KEY` 作为环境变量导出（`export OPENAI_API_KEY='my
    key'`）。接下来，将生成的文件上传到 OpenAI 服务器，上传过程完成后，创建一个用于微调的任务。以下列表显示了执行这些操作的代码。或者，阅读 GitHub
    仓库中的 tune-model.py 脚本。请记住，此选项仅适用于付费版本。
- en: Listing 7.3 How to fine-tune the model
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3 如何微调模型
- en: '[PRE2]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 An alternative way to get your key: openai.api_key = ‘MY_KEY’'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取密钥的另一种方法：openai.api_key = ''MY_KEY'''
- en: '#2 Creates a new dataset and uploads it to the OpenAI server'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建一个新的数据集并将其上传到 OpenAI 服务器'
- en: '#3 Enters into a loop until the model is fine-tuned'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 进入循环，直到模型微调完成'
- en: '#4 Create a new fine-tuning job'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建一个新的微调任务'
- en: '#5 Prints the model ID'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 打印模型 ID'
- en: Note  First, use the `openai.File.create()` method to create a new dataset and
    upload it to the OpenAI server. Next, use the `openai.FineTuningJob.create()`
    method to create a fine-tuning job using GPT-3.5 Turbo. Wait until the job is
    completed. This could take a long time, depending on the dataset size. Once the
    model is trained, use the `fine_tuned_model` variable to print the information
    associated with the fine-tuned model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：首先，使用`openai.File.create()`方法创建一个新的数据集并将其上传到OpenAI服务器。接下来，使用`openai.FineTuningJob.create()`方法创建一个使用GPT-3.5
    Turbo的微调作业。等待作业完成。这可能会花费很长时间，具体取决于数据集的大小。一旦模型训练完成，使用`fine_tuned_model`变量来打印与微调模型相关的信息。
- en: The following listing shows an example of information printed after the execution
    of the fine-tune-model.py script. This fine-tuning would cost around $0.05.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了在执行fine-tune-model.py脚本后的输出示例。这次微调大约需要0.05美元。
- en: Listing 7.4 An example of information associated with a fine-tuned model
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4 与微调模型相关的信息示例
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note  Some helpful information is provided, including the model type, the model
    ID, the hyperparameters used, and more.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：提供了一些有用的信息，包括模型类型、模型ID、使用的超参数等。
- en: Now, we can use the fine-tuned model to generate new commentaries tailored to
    the general public. Use the value corresponding to the `fine_tuned_model` key
    of the previous listing to refer to your model (`"ft:gpt-3.5-turbo-0613:personal::7t1Xuct5"`
    in the example).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用微调模型来生成针对普通公众的新评论。使用前一个列表中`fine_tuned_model`键对应的值来引用你的模型（例如，示例中的`"ft:gpt-3.5-turbo-0613:personal::7t1Xuct5"`）。
- en: To generate a new commentary, start a new chat session by using the `openai.ChatCompletion.create()`
    method, as shown in the following listing and in the generate-description.py script
    of the GitHub repository for the book. As a use case, consider again the example
    of figure 7.1.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成新的评论，请使用`openai.ChatCompletion.create()`方法开始一个新的聊天会话，如以下列表和GitHub仓库中该书的generate-description.py脚本所示。作为一个用例，再次考虑图7.1的例子。
- en: Listing 7.5 How to generate a new commentary
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5 如何生成新的评论
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note  Create a new `ChatCompletion` instance by specifying the model ID and
    the list of messages. The example defines only one message with the same system
    role as the fine-tuning dataset and the user role with a short description of
    our scenario.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通过指定模型ID和消息列表创建一个新的`ChatCompletion`实例。示例中定义了一个消息，其系统角色与微调数据集相同，用户角色有一个简短的情景描述。
- en: The following listing shows an example output.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了一个示例输出。
- en: Listing 7.6 An example output
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6 一个示例输出
- en: '[PRE5]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note  The output contains the role (assistant) and the content.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：输出包含角色（助手）和内容。
- en: Incorporate the produced content into your chart as a commentary. This example
    has demonstrated how you can perform model fine-tuning using a manual dataset.
    In the example, the output is straightforward. If you want your model to produce
    more complex outputs, you must complicate your fine-tuning dataset—for example,
    by adding new pairs (prompt–completion) specifically designed for your audience
    or your topic.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的内文作为评论纳入你的图表中。这个例子展示了如何使用手动数据集进行模型微调。在例子中，输出是直接的。如果你想让你的模型产生更复杂的输出，你必须使你的微调数据集更复杂——例如，通过添加专门为你的受众或主题设计的新对（提示-完成）。
- en: For comparison with the fine-tuned model, figure 7.11 shows the output produced
    by ChatGPT (without fine-tuning) with a similar input. Now that you have learned
    how to build your dataset manually, let’s move on to the next strategy, building
    from sources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与微调模型进行比较，图7.11显示了ChatGPT（未微调）在类似输入下的输出。现在你已经学会了如何手动构建你的数据集，让我们继续到下一个策略，从来源构建。
- en: '![figure](../Images/7-11.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-11.png)'
- en: Figure 7.11 The output produced by ChatGPT without fine-tuning
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11 ChatGPT未微调产生的输出
- en: Building from sources
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从来源构建
- en: This strategy involves building your dataset from external sources, such as
    your company website or a domain-specific blog. For example, if you work in the
    health field, you could download the title and abstracts of scientific papers
    about health. This enables you to build a dataset with a very domain-specific
    language. Or if you work in the ICT field, you can download the titles and subtitles
    of blog articles from feeds to build your technical dataset. Anyway, you must
    pay attention to the data license in all cases. If the license explicitly prohibits
    their usage, you cannot use those sources, and you must search for other data
    sources. In some cases, contacting the data author directly could be sufficient—for
    example, if you want to download their data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略涉及从外部来源构建您的数据集，例如您的公司网站或特定领域的博客。例如，如果您在医疗领域工作，您可以下载关于健康的科学论文的标题和摘要。这使您能够构建一个具有非常特定领域语言的数据集。或者，如果您在ICT领域工作，您可以从馈送中下载博客文章的标题和副标题来构建您的技术数据集。无论如何，您必须在所有情况下注意数据许可。如果许可证明确禁止使用，则不能使用这些来源，并且您必须寻找其他数据来源。在某些情况下，直接联系数据作者可能就足够了——例如，如果您想下载他们的数据。
- en: In the remainder of this section, we will build a dataset tailored to a technical
    audience by extracting feeds from my Medium blog. The objective is to build a
    chart title corresponding to the blog title, providing the blog subheading as
    an input. You can find the example in the GitHub repository for the book under
    07/from-source-chatgpt-fine-tuning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将通过从我的Medium博客中提取内容来构建一个针对技术受众的数据集。目标是构建与博客标题相对应的图表标题，并将博客副标题作为输入。您可以在本书的GitHub仓库下的07/from-source-ChatGPT-fine-tuning目录中找到示例。
- en: First, download the data. Ask Copilot to generate the code for you. The following
    listing shows the instructions for Copilot.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载数据。要求Copilot为您生成代码。以下列表显示了Copilot的指令。
- en: Listing 7.7 The instructions for Copilot
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.7 Copilot的指令
- en: '[PRE6]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note  Specify the feed URL and the information to extract for each item. Also,
    ask Copilot to generate the code to save the extracted items into a CSV file.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：指定每个条目的馈送URL和要提取的信息。还要求Copilot生成代码以将提取的项目保存到CSV文件中。
- en: Copilot will generate an output similar to that shown in listing 7.8\. Set the
    prompt to the subheading and the completion to the title. Save the script, and
    run it. You can find the code generated by Copilot in the GitHub repository for
    the book in the download-raw-data.py script. You should see the medium-articles.csv
    file in your working directory.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot将生成类似于列表7.8中所示的结果。将提示设置为副标题，将完成设置为标题。保存脚本，并运行它。您可以在本书的GitHub仓库中的download-raw-data.py脚本中找到Copilot生成的代码。您应该在您的当前工作目录中看到medium-articles.csv文件。
- en: Listing 7.8 How to extract data from feeds
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8 如何从馈送中提取数据
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note  Use the `feedparser`, `requests`, and `bs4` libraries. If you don’t have
    them in your environment, install them using the pip package manager.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用`feedparser`、`requests`和`bs4`库。如果您在环境中没有它们，请使用pip包管理器进行安装。
- en: Once you have built the dataset, follow the procedure described in section 7.4.1
    to fine-tune the dataset (listings 7.2–7.6). You can find the complete example
    in the GitHub repository for the book.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了数据集，请按照7.4.1节中描述的程序微调数据集（列表7.2–7.6）。您可以在本书的GitHub仓库中找到完整的示例。
- en: 'To test the fine-tuned model, provide the following prompt as input: *A chart
    on selecting the best sport to fund*. The model generates an output similar to
    the following: *How to Choose the Best Sport to Fund: A Data-Driven Approach*.
    Try a similar prompt with ChatGPT. Figure 7.12 shows a possible output. Since
    ChatGPT is not fine-tuned, you must specify more details in your prompt, as previously
    seen. Instead, for your fine-tuned model, describing the content in your prompt
    is sufficient. Now that you have learned how to perform fine-tuning for data storytelling,
    let’s move on to the next strategy for adapting your model to your specific context:
    retrieval augmented generation.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试微调后的模型，请提供以下提示作为输入：*选择最佳资助运动项目的图表*。模型生成的输出类似于以下内容：*如何选择最佳资助运动项目：一种数据驱动的方法*。尝试使用ChatGPT进行类似的提示。图7.12显示了可能的输出。由于ChatGPT未进行微调，您必须在提示中指定更多细节，如之前所见。相反，对于您的微调模型，在提示中描述内容就足够了。现在您已经学会了如何进行数据故事讲述的微调，让我们继续学习将模型适应特定上下文的下一个策略：增强检索生成。
- en: '![figure](../Images/7-12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-12.png)'
- en: Figure 7.12 The output produced by ChatGPT
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12 ChatGPT生成的输出
- en: 7.4.2 Retrieval augmented generation
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 增强检索生成
- en: So far, you have seen how to adapt an LLM to a context by building an ad hoc
    dataset. The effort, in this case, consists in preparing the dataset. Imagine
    how nice it would be to pass a text directly to the LLM without converting it
    to a specific format. Well, the good news is that this is possible, thanks to
    retrieval augmented generation (RAG).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了如何通过构建一个专用的数据集来适应一个LLM的上下文。在这种情况下，努力的工作在于准备数据集。想象一下，如果能够直接将文本传递给LLM而不需要将其转换为特定格式，那会多么方便。好消息是，这确实可能，多亏了检索增强生成（RAG）。
- en: RAG is an advanced natural language processing (NLP) technique that combines
    elements of information retrieval and text generation. First, RAG performs a retrieval
    step, which queries an external knowledge source, such as a vast text corpus or
    a structured database. Next, RAG uses this knowledge source to enhance its response
    generation. RAG integrates the retrieved facts into its generated text.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种高级自然语言处理（NLP）技术，它结合了信息检索和文本生成的元素。首先，RAG执行一个检索步骤，查询外部知识源，如庞大的文本语料库或结构化数据库。接下来，RAG使用这个知识源来增强其响应生成。RAG将检索到的事实整合到其生成的文本中。
- en: In the data storytelling domain, you can use RAG to adapt your LLM to your topic,
    such as a product, real-time data, customer reviews, or other relevant information.
    For instance, by querying the knowledge base for specific product details, you
    can generate ad hoc commentaries and annotations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据故事讲述领域，你可以使用RAG来适应你的LLM到你的主题，例如产品、实时数据、客户评论或其他相关信息。例如，通过查询知识库以获取特定产品的详细信息，你可以生成专用的评论和注释。
- en: Imagine you want to build a RAG-based system that retrieves information about
    a product from your website company. Figure 7.13 shows the architecture of the
    RAG system we will implement.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想构建一个基于RAG的系统，从你的公司网站上检索有关产品的信息。图7.13展示了我们将要实现的RAG系统的架构。
- en: '![figure](../Images/7-13.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-13.png)'
- en: Figure 7.13 A RAG-based system
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.13 基于RAG的系统
- en: First, we will download the text from a specified URL, split it, and represent
    it as vectors we store in a vector database. We will provide the vector database
    as an input to an LLM application, which can answer queries by querying the vector
    database. We will implement an example that generates commentaries for a specific
    smartphone, based on its description contained in an HTML page. In practice, we
    will load the HTML page into the vector database, and then we will implement an
    LLM application to query it. We will use LangChain to implement the LLM application,
    Chroma for the vector database, and OpenAI for the LLM to make everything work.
    For more details on how to install these tools, refer to appendix A.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从指定的URL下载文本，将其分割，并以我们存储在向量数据库中的向量形式表示。我们将向量数据库作为LLM应用程序的输入，该应用程序可以通过查询向量数据库来回答查询。我们将实现一个示例，根据包含在HTML页面中的描述生成特定智能手机的评论。在实践中，我们将加载HTML页面到向量数据库中，然后我们将实现一个LLM应用程序来查询它。我们将使用LangChain来实现LLM应用程序，使用Chroma作为向量数据库，使用OpenAI作为LLM，以便一切都能正常工作。有关如何安装这些工具的更多详细信息，请参阅附录A。
- en: In the remainder of this section, you will learn how to implement the described
    system. We will start by introducing LangChain. Next, we will see how to store
    data in Chroma. Finally, you will learn how to query the built system.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，你将学习如何实现所描述的系统。我们将首先介绍LangChain。接下来，我们将看到如何将数据存储在Chroma中。最后，你将学习如何查询构建的系统。
- en: Introducing LangChain
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LangChain介绍
- en: LangChain ([https://www.langchain.com/](https://www.langchain.com/)) is a framework
    that enables you to create applications that connect an LLM to other sources.
    LangChain supports several providers, including OpenAI, Google, Microsoft, Hugging
    Face, and many more. In this book, we will focus on the models provided by OpenAI.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain ([https://www.langchain.com/](https://www.langchain.com/)) 是一个框架，它使你能够创建将LLM连接到其他来源的应用程序。LangChain支持多个提供商，包括OpenAI、Google、Microsoft、Hugging
    Face以及更多。在这本书中，我们将专注于OpenAI提供的数据模型。
- en: 'The core idea behind LangChain is the concept of a chain, which consists of
    several components from different modules. There are three main components:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain背后的核心思想是链的概念，它由来自不同模块的几个组件组成。有三个主要组件：
- en: '*LLM wrappers* —Wrappers for LLMs provided by external providers, such as OpenAI
    and Hugging Face'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM包装器* — 由外部提供商提供的LLM包装器，例如OpenAI和Hugging Face'
- en: '*Prompt templates* —Templates for different prompts, such as chatbot and question
    answering'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示模板* — 不同提示的模板，例如聊天机器人和问答'
- en: '*Indexes* —External structures you can use to provide additional context to
    an LLM'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引* — 你可以使用的外部结构，为 LLM 提供额外的上下文'
- en: The LangChain-based applications are *context aware* because they connect LLM
    to external sources. Additionally, such applications are useful because they can
    answer questions based on the provided context, what actions to take, and so on.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LangChain 的应用程序是 *上下文感知* 的，因为它们将 LLM 连接到外部来源。此外，此类应用程序很有用，因为它们可以根据提供的上下文、采取的行动等问题进行回答。
- en: 'The most straightforward chain consists of just one LLM chained with a prompt
    that enables you to query the model. In the remainder of this section, we will
    implement a LangChain composed of the components shown in figure 7.14: the vector
    database (Chroma), the prompt template, the LLM model (GPT-3.5 Turbo by OpenAI),
    and the retrieval interface.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的链仅由一个 LLM 和一个允许你查询模型的提示组成。在本节的剩余部分，我们将实现一个由图 7.14 中显示的组件组成的 LangChain：向量数据库（Chroma）、提示模板、LLM
    模型（OpenAI 的 GPT-3.5 Turbo）和检索接口。
- en: '![figure](../Images/7-14.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-14.png)'
- en: Figure 7.14 The implemented architecture
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.14 实现的架构
- en: You can find the full code described in this example in the GitHub repository
    of this book under 07/rag. We need an `OPENAI_API_KEY`, as specified in appendix
    A. Open a terminal, and export your `OPENAI_API_KEY` as an environment variable
    (`export` `OPENAI_API_KEY='my` `key'`).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到本例中描述的完整代码，位于 07/rag 目录下。我们需要一个 `OPENAI_API_KEY`，如附录 A 中指定。打开终端，并将你的
    `OPENAI_API_KEY` 作为环境变量导出（`export OPENAI_API_KEY='my key'`）。
- en: Chroma ([https://www.trychroma.com/](https://www.trychroma.com/)) is an embedding
    database you can use as an indexer for your LangChain. To install and configure
    Chroma, refer to appendix A. An *embedding* is a numerical representation of data
    that is easy to index and retrieve, often for real-time tasks (Lane and Dyshel,
    2024). Before storing a text in Chroma, we must convert it into vector embeddings.
    For more details about embeddings, refer to the references section of this chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Chroma ([https://www.trychroma.com/](https://www.trychroma.com/)) 是一个嵌入数据库，你可以将其用作
    LangChain 的索引器。要安装和配置 Chroma，请参阅附录 A。*嵌入* 是数据的数值表示，易于索引和检索，通常用于实时任务（Lane 和 Dyshel，2024）。在将文本存储到
    Chroma 之前，我们必须将其转换为向量嵌入。有关嵌入的更多详细信息，请参阅本章的参考文献部分。
- en: Consider the product description available on a hypothetical website, as shown
    in figure 7.15\. The objective of our task is to store the product description
    shown in figure 7.15 in Chroma.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个假设网站上的产品描述，如图 7.15 所示。我们任务的目标是将图 7.15 中显示的产品描述存储在 Chroma 中。
- en: '![figure](../Images/7-15.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-15.png)'
- en: Figure 7.15 The HTML page with the product description
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.15 包含产品描述的 HTML 页面
- en: The first step involves loading the data from the URL, as shown in the following
    listing. Since Chroma is fully integrated with LangChain, we will use it to accomplish
    our task. LangChain supports multiple formats, including PDFs, URLs, and more.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及从 URL 加载数据，如下所示列表。由于 Chroma 完全集成于 LangChain，我们将使用它来完成我们的任务。LangChain 支持多种格式，包括
    PDF、URL 等。
- en: Listing 7.9 How to load the HTML document in LangChain
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.9 在 LangChain 中如何加载 HTML 文档
- en: '[PRE8]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Loads data'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载数据'
- en: Note  To load an HTML document in LangChain, build an `UnstructuredHTMLLoader()`
    object.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意  — 要在 LangChain 中加载 HTML 文档，请构建一个 `UnstructuredHTMLLoader()` 对象。
- en: Next, split the data into chunks of 20, as shown in the following listing. We
    could have chosen any number smaller than the total text size for the chunk size.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将数据分成 20 个块，如下所示列表。我们本可以选择小于总文本大小的任何数字作为块大小。
- en: Listing 7.10 How to split the text into chunks
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.10 如何将文本分割成块
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note  Create a `RecursiveCharacterTextSplitter()` object to split the text into
    chunks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意  — 创建一个 `RecursiveCharacterTextSplitter()` 对象来将文本分割成块。
- en: After that, convert the split text into embeddings and store them in Chroma.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将分割的文本转换为嵌入并将它们存储在 Chroma 中。
- en: Listing 7.11 How to generate embeddings in Chroma
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.11 如何在 Chroma 中生成嵌入
- en: '[PRE10]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note  First, create a new `OpenAIEmbeddings()` object. Next, create a Chroma
    store with the split data and the embeddings and associate it with the collection
    `Product-Info`. Finally, store the Chroma store on the filesystem, using the `persist()`
    method.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意  — 首先，创建一个新的 `OpenAIEmbeddings()` 对象。然后，创建一个包含分割数据和嵌入的 Chroma 存储，并将其与集合 `Product-Info`
    关联。最后，使用 `persist()` 方法将 Chroma 存储在文件系统中。
- en: 'Now, our vector store is ready, so we can move on to the next step: defining
    a prompt template.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的向量存储已准备就绪，因此我们可以继续下一步：定义一个提示模板。
- en: Defining a prompt template
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义提示模板
- en: A prompt template is a predefined text used for generating prompts for LLMs.
    A prompt template may include instructions, examples, context, and questions appropriate
    for your task. The following listing shows an example of a prompt we can provide
    as an input to our system.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板是用于生成 LLM 提示的预定义文本。提示模板可能包括适合您任务的说明、示例、上下文和问题。以下列表显示了一个我们可以作为系统输入的提示示例。
- en: Listing 7.12 How to structure a prompt template
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.12 如何构建提示模板
- en: '[PRE11]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note  First, define the structure of your template. Use brackets to define
    input variables. In the example, there are two variables: `context` and `question`.
    Next, create a new `PromptTemplate()` object, and pass it the template and the
    input variables as parameters.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：首先，定义你的模板结构。使用括号定义输入变量。在示例中，有两个变量：`context` 和 `question`。接下来，创建一个新的 `PromptTemplate()`
    对象，并将模板和输入变量作为参数传递给它。
- en: 'Once we have built the prompt template, we are ready to proceed with the last
    step: retrieval and query.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了提示模板，我们就准备好进行最后一步：检索和查询。
- en: Retrieval interface
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检索接口
- en: A *retrieval interface* is an interface that enables us to combine the data
    stored in the Chroma database and the OpenAI LLM. We can use a retrieval to query
    our system and generate commentaries and annotations to incorporate in our charts.
    The following listing shows an example of the usage of a retrieval.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索接口* 是一个使我们能够结合存储在 Chroma 数据库和 OpenAI LLM 中的数据的接口。我们可以使用检索来查询我们的系统，并生成评论和注释以包含在我们的图表中。以下列表显示了一个检索使用示例。'
- en: Listing 7.13 How to build a retrieval
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.13 如何构建检索
- en: '[PRE12]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note  First, create an LLM instance using `ChatOpenAI()`. Set the temperature
    to 0 for conservative output. The temperature spans from 0 (low creativity) to
    1 (high creativity). Set the model to `GPT-3.5-turbo`. Next, create a retrieval
    interface using `RetrievalQA()` by specifying the LLM, the vector store (`retriever`),
    the prompt, and other parameters. Set the `chain_type` to `stuff`, a prepackaged
    document chain that takes a list of documents and inserts them into the prompt,
    which is then passed to the LLM. Finally, ask the question.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：首先，使用 `ChatOpenAI()` 创建一个 LLM 实例。将温度设置为 0 以获得保守的输出。温度范围从 0（低创造力）到 1（高创造力）。将模型设置为
    `GPT-3.5-turbo`。接下来，通过指定 LLM、向量存储（`retriever`）、提示和其他参数，使用 `RetrievalQA()` 创建一个检索接口。将
    `chain_type` 设置为 `stuff`，这是一个预包装的文档链，它将文档列表插入到提示中，然后传递给 LLM。最后，提出问题。
- en: The following listing shows the produced output. You can insert the produced
    text (in bold) in your chart.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了生成的输出。您可以将生成的文本（粗体）插入到您的图表中。
- en: Listing 7.14 The produced output
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.14 生成的输出
- en: '[PRE13]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note  The output contains the text to insert in the chart (in bold) and other
    useful information, such as the original query and the source documents.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：输出包含要插入图表中的文本（粗体）和其他有用信息，例如原始查询和源文档。
- en: Now that you have learned how to apply LLMs to build context in data storytelling,
    let’s move on to a practical example.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何将 LLM 应用于构建数据故事中的上下文，让我们继续到一个实际例子。
- en: '7.5 Case study: From information to knowledge (part 1)'
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 案例研究：从信息到知识（第一部分）
- en: 'In the previous chapter, we analyzed how to turn data into information in the
    aquaculture case study. As a quick reminder, the case study involved building
    a story around the problem of safety in the salmon aquaculture in the United States.
    We decided to plot the salmon aquaculture sales trend line versus the other types
    of aquaculture. As an insight, we discovered that since 1998, there had been an
    increase in sales, following a period of decrease in sales from 1992 to 1998\.
    We discovered that the decreasing period was partially due to some health problems
    in the salmon aquaculture. Figure 7.16 shows the chart produced at the end of
    the first step of the DIKW pyramid: from data to information.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们分析了如何在水产养殖案例研究中将数据转化为信息。作为一个快速提醒，该案例研究涉及围绕美国鲑鱼养殖安全问题构建故事。我们决定将鲑鱼养殖的销售趋势线与其他类型的养殖进行比较。作为一个洞察，我们发现自
    1998 年以来，销售有所增加，而 1992 年至 1998 年期间销售下降。我们发现下降期部分是由于鲑鱼养殖中的一些健康问题。图 7.16 显示了 DIKW
    金字塔第一步结束时生成的图表：从数据到信息。
- en: '![figure](../Images/7-16.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-16.png)'
- en: Figure 7.16 The chart produced at the end of the data-to-information phase
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.16 数据到信息阶段结束时生成的图表
- en: 'To transform the chart into a data story, the next step involves turning information
    into knowledge. We will accomplish this by doing the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要将图表转化为数据故事，下一步涉及将信息转化为知识。我们将通过以下方式完成：
- en: Some design considerations to tailor the chart to the audience
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整图表以适应受众的一些设计考虑因素
- en: Adding a commentary describing the general situation regarding safety in aquaculture
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加描述水产养殖安全一般情况的注释
- en: Adding an annotation and a symbol to highlight the period of decrease in sales
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加注释和符号以突出销售下降的时期
- en: 'Let’s start with the first point: tailoring the chart to the audience.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个要点开始：根据受众调整图表。
- en: 7.5.1 Tailoring the chart to the audience
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 根据受众调整图表
- en: The scenario required us to present the data story to an audience of executives,
    which meant we needed a chart easy enough to understand that they could quickly
    make decisions based on its information. In general, executives are familiar with
    trend lines, so we do not need to modify the chart. In addition, the chart is
    neither too detailed nor too simple. It contains the right level of detail to
    allow the audience not to be overwhelmed by information. Additionally, the chart
    doesn’t give the impression of being sparse. Therefore, we can conclude that the
    chart type is perfect for our audience.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 该场景要求我们向高管受众展示数据故事，这意味着我们需要一个图表足够简单，他们可以快速根据其信息做出决策。一般来说，高管熟悉趋势线，因此我们不需要修改图表。此外，图表既不太详细也不太简单。它包含适当的信息量，使受众不会被信息淹没。此外，图表没有显得稀疏。因此，我们可以得出结论，图表类型非常适合我们的受众。
- en: We also suppose that our audience is familiar with the $ symbol on the y-axis
    and the Years label on the x-axis, so we do not need to add any further specifications.
    We can leave the comparison between the salmon trend line and the others because
    it is useful for our audience to understand how the salmon sales behave compared
    to the other categories. It is not necessary to add further details.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设我们的受众熟悉y轴上的$符号和x轴上的“年份”标签，因此我们不需要添加任何进一步的说明。我们可以保留鲑鱼趋势线与其他趋势线的比较，因为这有助于我们的受众了解鲑鱼销售与其他类别相比的行为。不需要添加更多细节。
- en: 'Challenge: How could you tailor the chart to the general public or to an audience
    of professionals?'
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战：你如何调整图表以适应普通公众或专业人士的受众？
- en: For the general public, you could consider simplifying the chart—for example,
    by reducing the number of years. You may also need to better explain the meaning
    of the y-axis. For professionals, you could add more details, such as points with
    values for each year, or you could even show the other aquaculture categories.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于普通公众，您可以考虑简化图表——例如，通过减少年份的数量。您可能还需要更好地解释y轴的含义。对于专业人士，您可以添加更多细节，例如每年都有值的点，或者甚至可以展示其他水产养殖类别。
- en: 'Now that we have discussed some design considerations to tailor the chart to
    the audience, let’s move on to the next step: adding a commentary. We will use
    RAG to generate the commentary.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了一些设计考虑因素，以根据受众调整图表，让我们继续下一步：添加注释。我们将使用RAG生成注释。
- en: 7.5.2 Using RAG to add a commentary
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 使用RAG添加注释
- en: We will add a commentary to the chart immediately under the title. Our commentary
    should explain how safety works in US aquaculture. We will base the commentary
    on “Aquacultured Seafood” ([https://mng.bz/WEW0](https://mng.bz/WEW0)), an official
    FDA fact sheet. This document describes, among other topics, the safety levels
    of aquaculture seafood.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在标题下方立即添加注释。我们的注释应该解释美国水产养殖中的安全机制。我们将基于“养殖海鲜”（[https://mng.bz/WEW0](https://mng.bz/WEW0)），一份官方FDA事实说明书。这份文件描述了包括水产养殖海鲜的安全水平在内的多个主题。
- en: 'You can implement a RAG-based system that builds required commentary using
    the code implemented in section 7.4.2\. You only need to provide this prompt:
    *Describe* *the safety of aquaculture seafood in the U.S*. The code of the implemented
    RAG system is also available in the GitHub repository for the book under CaseStudies/aquaculture/
    from-information-to-knowledge/rag.py. The following listing shows the produced
    output, containing the required commentary.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实施一个基于RAG的系统，使用第7.4.2节中实现的代码构建所需的注释。您只需要提供以下提示：“*描述*美国水产养殖海鲜的安全性”。实现的RAG系统代码也可在本书的GitHub仓库中找到，位于CaseStudies/aquaculture/from-information-to-knowledge/rag.py。以下列表显示了生成的输出，包含所需的注释。
- en: Listing 7.15 The produced output
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.15 生成的输出
- en: '[PRE14]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note  Use the produced output as a commentary for the chart.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用生成的输出作为图表的注释。
- en: Now, we can add this text as a commentary for our chart. Listing 7.16 shows
    only the modifications to our original chart including the commentary. You can
    find the complete code in the GitHub repository for the book under CaseStudies/aquaculture/
    from-information-to-knowledge/chart.py.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将此文本作为图表的注释添加。列表 7.16 仅显示了包括注释在内的原始图表的修改。您可以在 GitHub 仓库中找到完整的代码，位于 CaseStudies/aquaculture/
    从信息到知识/chart.py。
- en: Listing 7.16 The commentary
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.16 注释
- en: '[PRE15]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note  Use the `title` property to add commentary to the chart, immediately before
    the title. Also, add a provisory title to the chart.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用 `title` 属性在标题之前添加注释，并为图表添加一个临时标题。
- en: The next step involves highlighting the period of decrease in sales. So let’s
    proceed.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是突出显示销售额下降的时期。让我们继续。
- en: 7.5.3 Highlighting the period of decrease in sales
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 突出显示销售额下降的时期
- en: 'The period of decrease in sales ranges from 1992 to 1998\. We want to highlight
    it to let the audience know that during this period, there were health problems
    in the salmon aquaculture. This will prepare the audience to consider respecting
    the safety rules to avoid the same problems in the future. We will add two elements
    to highlight this decreasing period:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 销售额下降的时期从 1992 年到 1998 年。我们想突出显示它，让观众知道在这个时期，鲑鱼养殖业存在健康问题。这将让观众考虑尊重安全规则，以避免未来出现同样的问题。我们将添加两个元素来突出这个下降时期：
- en: A light-gray rectangle covering the decreasing period
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖下降时期的浅灰色矩形
- en: A textual annotation describing the health problems
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述健康问题的文本注释
- en: The following listing shows the code to build the rectangle.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了构建矩形的代码。
- en: Listing 7.17 The rectangle
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.17 矩形
- en: '[PRE16]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 A magic number to set the upper part of the chart'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 一个用于设置图表上部的魔法数字'
- en: Note  First, build a DataFrame with the rectangle’s coordinates. Next, draw
    the rectangle using `mark_rect()`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：首先，使用矩形的坐标构建一个 DataFrame。接下来，使用 `mark_rect()` 绘制矩形。
- en: 'Challenge: Which instructions could you write for Copilot to speed up the coding
    process?'
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战：您可以为 Copilot 编写哪些指令来加快编码过程？
- en: 'You could try adding the following instruction to generate the rectangle: `#`
    `Add` `a` `rectangle` `starting` `from` `1993` `to` `2000`. What output would
    you obtain?'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试添加以下指令来生成矩形：`#` `添加` `一个` `从` `1993` `到` `2000` 的 `矩形`。您会得到什么输出？
- en: The following listing shows the code to add the annotation. The decline in sales
    was partially due to fish health issues.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了添加注释的代码。销售额下降的部分原因是鱼类健康问题。
- en: Listing 7.18 The annotation
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.18 注释
- en: '[PRE17]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note  First, build a DataFrame with the annotation text and its position information.
    Next, draw the annotation using `mark_text()`. Finally, plot and save the chart.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：首先，使用注释文本及其位置信息构建一个 DataFrame。接下来，使用 `mark_text()` 绘制注释。最后，绘制并保存图表。
- en: 'Figure 7.17 shows the final chart, after adding context; we have turned information
    into knowledge. In the next chapter, we will further enrich context by adding
    some images, and in chapter 9, we will complete the story by adding the next step:
    wisdom.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 展示了添加上下文后的最终图表，我们将信息转化为知识。在下一章中，我们将通过添加一些图片来进一步丰富上下文，而在第 9 章中，我们将通过添加下一步：智慧来完成故事。
- en: '![figure](../Images/7-17.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-17.png)'
- en: Figure 7.17 The chart produced at the end of the information-to-knowledge phase
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.17 信息到知识阶段结束时的图表
- en: You have now implemented a practical example of turning information into knowledge.
    Before moving to the next chapter, let’s further solidify the concept by completing
    a practical exercise.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经实现了一个将信息转化为知识的实际例子。在进入下一章之前，让我们通过完成一个实际练习来进一步巩固这个概念。
- en: 7.5.4 Exercise
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.4 练习
- en: 'Modify the previous chart as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式修改之前的图表：
- en: Tailor the chart to an audience of professionals.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图表调整到专业人士的受众。
- en: 'Add points to the salmon line chart. Suggestion: Use `point=True` as a parameter
    of `mark_line().`'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向鲑鱼折线图添加点。建议：将 `point=True` 作为 `mark_line()` 的参数。
- en: 'Add values for each point. Suggestion: Use `mark_text()` to add values for
    each point.'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个点添加值。建议：使用 `mark_text()` 为每个点添加值。
- en: 'Implement a RAG-based system to extract an annotation for the decreasing period
    from the *Governor’s Task Force on the Planning and Development of Marine Aquaculture
    in Maine Report and Recommendations* (pp. 28–32, [https://mng.bz/jXqx](https://mng.bz/jXqx)).
    Suggestion: Use `PDFMinerLoader()` to extract data from PDF. You may need to install
    some additional Python packages, including pdf2image, pdfminer, and pdfminer.six.
    You can find the solution in the GitHub repository for the book under CaseStudies/aquaculture/from-information-to-knowledge/rag-annotation.py.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个基于RAG的系统，从*缅因州海洋水产养殖规划与发展委员会报告和建议*（第28-32页，[https://mng.bz/jXqx](https://mng.bz/jXqx)）中提取下降期的注释。建议：使用`PDFMinerLoader()`从PDF中提取数据。你可能需要安装一些额外的Python包，包括pdf2image、pdfminer和pdfminer.six。你可以在本书的GitHub仓库下的CaseStudies/aquaculture/from-information-to-knowledge/rag-annotation.py中找到解决方案。
- en: In the first part of this chapter, you learned how to turn information into
    knowledge by adding context to your data visualization chart. You saw that context
    depends on the audience reading your chart. For example, if your chart will be
    read by the general public, avoid technical details and use an appealing visualization.
    On the other hand, if your chart will be read by technical experts, add as many
    details as you can, while keeping the chart easy to read. In the second part of
    the chapter, you saw how to use generative AI tools as assistants to build your
    context. Finally, you learned where to put the textual context in your chart.
    In the next chapter, you will see how to add images to your chart to enrich context.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，你学习了如何通过为你的数据可视化图表添加上下文来将信息转化为知识。你看到上下文取决于阅读你图表的受众。例如，如果你的图表将由公众阅读，请避免技术细节并使用吸引人的可视化。另一方面，如果你的图表将由技术专家阅读，请尽可能添加细节，同时保持图表易于阅读。在章节的第二部分，你看到了如何使用生成式AI工具作为助手来构建你的上下文。最后，你学习了在哪里放置你的图表中的文本上下文。在下一章中，你将看到如何向你的图表添加图像以丰富上下文。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Adding context to your data visualization is crucial for turning information
    into knowledge. Textual context includes all the relevant facts and events useful
    for the audience to understand data.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的数据可视化中添加上下文对于将信息转化为知识至关重要。文本上下文包括所有对观众理解数据有用的相关事实和事件。
- en: 'When you build a chart, tailor it to the audience. In general, there are three
    types of audiences: the general public, professionals, and executives.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你构建图表时，请根据受众进行定制。一般来说，有三种类型的受众：公众、专业人士和行政人员。
- en: Use generative AI tools as assistants to help create context for your data.
    In particular, use ChatGPT to generate commentaries and annotations.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成式AI工具作为助手来为你的数据创建上下文。特别是，使用ChatGPT来生成评论和注释。
- en: If ChatGPT needs to know custom data or topics, extend your LLM with fine-tuning
    or RAG.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果ChatGPT需要了解自定义数据或主题，可以通过微调或RAG扩展你的LLM。
- en: Fine-tuning enables you to optimize a pretrained LLM based on a dataset of prompt–completion
    pairs.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调使你能够根据提示-完成对的数据集优化预训练的LLM。
- en: Retrieval augmented generation uses an external database, called a *vector database*,
    to extend the LLM knowledge with domain-specific topics.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成使用一个称为*向量数据库*的外部数据库，通过特定领域的主题扩展LLM的知识。
- en: References
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Embeddings
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入
- en: Lane, H. and Dyshel, M. (2024). *Natural Language Processing in Action* (2nd
    ed.). Manning Publications.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lane, H. 和 Dyshel, M. (2024). *《自然语言处理实战》（第2版）*。Manning Publications.
- en: OpenAI. (n.d.). *Embeddings*. [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI. (n.d.). *嵌入*。[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings).
- en: Fine-tuning
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: Bantilan, N. (2023). *Fine Tuning vs. Prompt Engineering Large Language Models.*
    [https://mlops.community/fine-tuning-vs-prompt-engineering-llms/](https://mlops.community/fine-tuning-vs-prompt-engineering-llms/).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bantilan, N. (2023). *微调与提示工程大型语言模型比较*。[https://mlops.community/fine-tuning-vs-prompt-engineering-llms/](https://mlops.community/fine-tuning-vs-prompt-engineering-llms/).
- en: Jolley, E. (2023). *Introduction to Retrieval Augmented Generation*. [https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/](https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jolley, E. (2023). *检索增强生成简介*。[https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/](https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/).
- en: Marcelo, X. (2023). *How to Fine-Tune OpenAI GPT*. [https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4](https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcelo, X. (2023). *如何微调OpenAI GPT*。[https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4](https://medium.com/@marceloax.br/how-to-fine-tune-openai-gpt-3-d06741f915f4).
- en: OpenAI. (n.d.). *Fine-Tuning.* [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI. (n.d.). *微调*。[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
- en: LangChain
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LangChain
- en: Biswas, A. (2023). *How to Work with LangChain Python Modules.* [https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules](https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biswas, A. (2023). *如何使用LangChain Python模块*。[https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules](https://www.packtpub.com/article-hub/how-to-work-with-langchain-python-modules).
- en: Geeks for Geeks. (2024). *Introduction to LangChain.* [https://www.geeksforgeeks.org/introduction-to-langchain/](https://www.geeksforgeeks.org/introduction-to-langchain/).
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geeks for Geeks. (2024). *LangChain简介*。[https://www.geeksforgeeks.org/introduction-to-langchain/](https://www.geeksforgeeks.org/introduction-to-langchain/).
- en: 'Pinecone. (n.d.). *LangChain: Introduction and Getting Started.* [https://www.pinecone.io/learn/series/langchain/langchain-intro/](https://www.pinecone.io/learn/series/langchain/langchain-intro/).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinecone. (n.d.). *LangChain：简介和入门*。[https://www.pinecone.io/learn/series/langchain/langchain-intro/](https://www.pinecone.io/learn/series/langchain/langchain-intro/).
- en: LLM
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM
- en: 'De Angelis, L., Baglivo, F., Arzilli, G., Privitera, G. P., Ferragina, P.,
    Tozzi, A. E., and Rizzo, C. (2023). *ChatGPT and the Rise of Large Language Models:
    The New AI-Driven Infodemic Threat in Public Health*. *Frontiers in Public Health*,
    *11*, 1166120\. [https://doi.org/10.3389/fpubh.2023.1166120](https://doi.org/10.3389/fpubh.2023.1166120).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Angelis, L.，Baglivo, F.，Arzilli, G.，Privitera, G. P.，Ferragina, P.，Tozzi,
    A. E.，和Rizzo, C. (2023). *ChatGPT和大型语言模型的兴起：公共卫生中的新AI驱动的信息流行病威胁*。《公共卫生前沿》，*11*，1166120。
    [https://doi.org/10.3389/fpubh.2023.1166120](https://doi.org/10.3389/fpubh.2023.1166120).
- en: Google Developers. (n.d.). *Introduction to Large Language Models.* [https://developers.google.com/machine-learning/resources/intro-llms?hl=en](https://developers.google.com/machine-learning/resources/intro-llms?hl=en).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Developers. (n.d.). *大型语言模型简介*。[https://developers.google.com/machine-learning/resources/intro-llms?hl=en](https://developers.google.com/machine-learning/resources/intro-llms?hl=en).
- en: RAG
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG
- en: Jolley, E. (2023). *Introduction to Retrieval Augmented Generation.* [https://arize.com/blog-course/introduction-to-retrieval-augmented-generation](https://arize.com/blog-course/introduction-to-retrieval-augmented-generation).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jolley, E. (2023). *检索增强生成简介*。[https://arize.com/blog-course/introduction-to-retrieval-augmented-generation](https://arize.com/blog-course/introduction-to-retrieval-augmented-generation).
- en: Needham, M. (2023). *Learn Data with Mark*. [https://github.com/mneedham/LearnDataWithMark/tree/main](https://github.com/mneedham/LearnDataWithMark/tree/main).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Needham, M. (2023). *用Mark学习数据*。[https://github.com/mneedham/LearnDataWithMark/tree/main](https://github.com/mneedham/LearnDataWithMark/tree/main).
- en: ———. (2023). *Retrieval Augmented Generation with OpenAI/GPT and Chrom**a.*
    [https://www.youtube.com/watch?v=Cim1lNXvCzY](https://www.youtube.com/watch?v=Cim1lNXvCzY).
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ———. (2023). *使用OpenAI/GPT和Chroma进行检索增强生成*。[https://www.youtube.com/watch?v=Cim1lNXvCzY](https://www.youtube.com/watch?v=Cim1lNXvCzY).
- en: 'Routu, V. (2023). *Answering with OpenAI and LangChain: Harnessing the Potential
    of Retrieval Augmented Generation (RAG).* [https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/](https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Routu, V. (2023). *使用OpenAI和LangChain回答问题：利用检索增强生成（RAG）的潜力*。[https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/](https://www.linkedin.com/pulse/transforming-question-answering-openai-langchain-harnessing-routu/).
- en: Schwaber-Cohen, R. (2023). *What Is a Vector Database?* [https://www.pinecone.io/learn/vector-database](https://www.pinecone.io/learn/vector-database).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwaber-Cohen, R. (2023). *什么是向量数据库？*。[https://www.pinecone.io/learn/vector-database](https://www.pinecone.io/learn/vector-database).
- en: Thinking for the audience
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为观众思考
- en: Bettes, S. (2019). *Technical and Professional Writing Genres.* [https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/](https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bettes, S. (2019). *技术和专业写作体裁*。[https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/](https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-2/).
- en: Emery, A. K. (2021). *Why “Know Your Audience” Is Terrible Dataviz Advice—And
    What to Do Instead.* [https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/](https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/).
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Emery, A. K. (2021). *为什么“了解您的受众”是糟糕的数据可视化建议——以及您应该做什么代替.* [https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/](https://depictdatastudio.com/why-know-your-audience-is-terrible-dataviz-advice-what-to-do-instead/).
- en: QuantHub. (2023). *How to Identify Your Audience for Impactful Data Storytelling.*
    [https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/](https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/).
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QuantHub. (2023). *如何确定您的受众以进行有影响力的数据故事讲述.* [https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/](https://www.quanthub.com/how-to-identify-your-audience-for-impactful-data-storytelling/).
- en: LinkedIn Community with AI. (n.d.). *How Do You Engage and Nurture Your Technical
    Audience and Build Trust and Authority?* [https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience](https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LinkedIn AI 社区. (n.d.). *您如何吸引和培养您的技术受众，建立信任和权威？* [https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience](https://www.linkedin.com/advice/0/how-do-you-engage-nurture-your-technical-audience).
- en: WirelessLAN Professionals. (n.d.). *How to Present to a Technical Audience.*
    [https://wlanprofessionals.com/how-to-present-to-a-technical-audience/](https://wlanprofessionals.com/how-to-present-to-a-technical-audience/).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无线局域网专业人士. (n.d.). *如何向技术受众进行展示.* [https://wlanprofessionals.com/how-to-present-to-a-technical-audience/](https://wlanprofessionals.com/how-to-present-to-a-technical-audience/).
- en: Transformers
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器
- en: Koenigstein, N. (2024). *Transformers in Action*. Manning Publications.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koenigstein, N. (2024). *变换器实战*. Manning 出版公司.
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł., and Polosukhin, I. (2017). “Attention Is All You Need.” *Advances
    in Neural Information Processing Systems*, *30*.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł., and Polosukhin, I. (2017). “Attention Is All You Need.” *Advances
    in Neural Information Processing Systems*, *30*.
