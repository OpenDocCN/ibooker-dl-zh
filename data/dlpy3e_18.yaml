- en: Best practices for the real world
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界的最佳实践
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world](https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world](https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world)'
- en: You’ve come quite far since the beginning of this book. You can now train image
    classification models, image segmentation models, models for classification or
    regression on vector data, timeseries forecasting models, text classification
    models, sequence-to-sequence models, and even generative models for text and images.
    You’ve got all the bases covered.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自从这本书的开头以来，你已经取得了很大的进步。你现在可以训练图像分类模型、图像分割模型、对向量数据进行分类或回归的模型、时间序列预测模型、文本分类模型、序列到序列模型，甚至是文本和图像的生成模型。你已经涵盖了所有的基础。
- en: However, your models so far have all been trained at a small scale — on small
    datasets, with a single GPU — and they generally haven’t reached the best achievable
    performance on each dataset we’ve looked at. This book is, after all, an introductory
    book. If you are to go out into the real world and achieve state-of-the-art results
    on brand new problems, there’s still a bit of a chasm that you’ll need to cross.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你迄今为止的所有模型都是在小规模上训练的——在小型数据集上，使用单个GPU——并且它们通常没有达到我们所查看的每个数据集上所能达到的最佳性能。毕竟，这本书是一本入门书。如果你要进入现实世界并在全新的问题上取得最先进的结果，你仍然需要跨越一段不小的鸿沟。
- en: 'This chapter is about bridging that gap and giving you the best practices you’ll
    need as you go from machine learning student to a fully fledged machine learning
    engineer. We’ll review essential techniques for systematically improving model
    performance: hyperparameter tuning and model ensembling. Then we’ll look at how
    you can speed up and scale up model training, with multi-GPU and TPU training,
    mixed precision, and quantization.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在弥合这一差距，并为你从机器学习学生成长为一名合格的机器学习工程师提供最佳实践。我们将回顾系统性地提高模型性能的必要技术：超参数调整和模型集成。然后，我们将探讨如何通过多GPU和TPU训练、混合精度和量化来加速和扩展模型训练。
- en: Getting the most out of your models
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 充分发挥你的模型潜力
- en: Blindly trying out different architecture configurations works well enough if
    you just need something that works okay. In this section, we’ll go beyond “works
    okay” to “works great and wins machine learning competitions” via a quick guide
    to a set of must-know techniques for building state-of-the-art deep learning models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只需要一个过得去的东西，那么盲目尝试不同的架构配置已经足够好了。在本节中，我们将通过一套必须掌握的技术的快速指南，从“过得去”提升到“表现卓越并赢得机器学习竞赛”。
- en: Hyperparameter optimization
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数优化
- en: 'When building a deep learning model, you have to make many seemingly arbitrary
    decisions: How many layers should you stack? How many units or filters should
    go in each layer? Should you use `relu` as an activation, or a different function?
    Should you use `BatchNormalization` after a given layer? How much dropout should
    you use? And so on. These architecture-level parameters are called *hyperparameters*
    to distinguish them from the *parameters* of a model, which are trained via backpropagation.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建一个深度学习模型时，你必须做出许多看似随意的决定：你应该堆叠多少层？每一层应该有多少个单元或过滤器？你应该使用`relu`作为激活函数，还是使用不同的函数？你应该在给定的层之后使用`BatchNormalization`吗？你应该使用多少dropout？等等。这些架构级别的参数被称为*超参数*，以区分它们与通过反向传播训练的模型的*参数*。
- en: In practice, experienced machine learning engineers and researchers build intuition
    over time as to what works and what doesn’t when it comes to these choices — they
    develop hyperparameter-tuning skills. But there are no formal rules. If you want
    to get to the very limit of what can be achieved on a given task, you can’t be
    content with such arbitrary choices. Your initial decisions are almost always
    suboptimal, even if you have very good intuition. You can refine your choices
    by tweaking them by hand and retraining the model repeatedly — that’s what machine
    learning engineers and researchers spend most of their time doing. But it shouldn’t
    be your job as a human to fiddle with hyperparameters all day — that is better
    left to a machine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，经验丰富的机器学习工程师和研究人员随着时间的推移对什么有效，什么无效有了直觉——他们发展了超参数调整技能。但没有正式的规则。如果你想达到给定任务可以实现的极限，你不能满足于这样的任意选择。你的初始决策几乎总是次优的，即使你非常有直觉。你可以通过手动调整并重复训练模型来细化你的选择——这就是机器学习工程师和研究人员大部分时间在做的事情。但整天调整超参数不应是你的工作——这最好留给机器。
- en: 'Thus, you need to explore the space of possible decisions automatically and
    systematically in a principled way. You need to search the architecture space
    and find the best-performing ones empirically. That’s what the field of automatic
    hyperparameter optimization is about: it’s an entire field of research, and an
    important one.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要以原则性的方式自动和系统地探索可能的决策空间。你需要搜索架构空间，并经验性地找到表现最佳的模型。这正是自动超参数优化领域的主题：这是一个完整的研究领域，也是一个重要的领域。
- en: 'The process of optimizing hyperparameters typically looks like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 优化超参数的过程通常如下所示：
- en: Choose a set of hyperparameters (automatically).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一组超参数（自动）。
- en: Build the corresponding model.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建相应的模型。
- en: Fit it to your training data, and measure performance on the validation data.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其拟合到你的训练数据，并在验证数据上衡量性能。
- en: Choose the next set of hyperparameters to try (automatically).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择下一组要尝试的超参数（自动）。
- en: Repeat.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复。
- en: Eventually, measure performance on your test data.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，在你的测试数据上衡量性能。
- en: 'The key to this process is the algorithm that analyzes the relationship between
    validation performance and various hyperparameter values to choose the next set
    of hyperparameters to evaluate. Many different techniques are possible: Bayesian
    optimization, genetic algorithms, simple random search, and so on.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的关键是分析验证性能与各种超参数值之间关系的算法，以选择下一组要评估的超参数。可能有许多不同的技术：贝叶斯优化、遗传算法、简单的随机搜索等等。
- en: 'Training the weights of a model is relatively easy: you compute a loss function
    on a mini-batch of data and then use backpropagation to move the weights in the
    right direction. Updating hyperparameters, on the other hand, presents unique
    challenges. Consider that'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的权重相对容易：你在一个数据的小批量上计算损失函数，然后使用反向传播将权重移动到正确的方向。另一方面，更新超参数则提出了独特的挑战。考虑以下情况：
- en: The hyperparameter space is typically made of discrete decisions and thus isn’t
    continuous or differentiable. Hence, you typically can’t do gradient descent in
    hyperparameter space. Instead, you must rely on gradient-free optimization techniques,
    which, naturally, are far less efficient than gradient descent.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数空间通常由离散决策组成，因此不是连续的或可微分的。因此，你通常不能在超参数空间中进行梯度下降。相反，你必须依赖无梯度优化技术，这些技术自然比梯度下降效率低得多。
- en: 'Computing the feedback signal of this optimization process (does this set of
    hyperparameters lead to a high-performing model on this task?) can be extremely
    expensive: it requires creating and training a new model from scratch on your
    dataset.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算这个优化过程的反馈信号（这组超参数是否会导致在这个任务上表现良好的模型？）可能非常昂贵：它需要从头开始创建和训练一个新的模型。
- en: 'The feedback signal may be noisy: if a training run performs 0.2% better, is
    that because of a better model configuration or because you got lucky with the
    initial weight values?'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈信号可能存在噪声：如果一次训练运行提高了0.2%，这是否是因为更好的模型配置，还是因为你初始权重值运气好？
- en: 'Thankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner.
    Let’s check it out.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个工具可以使超参数调整更简单：KerasTuner。让我们来看看它。
- en: Using KerasTuner
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用KerasTuner
- en: 'Let’s start by installing KerasTuner:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装KerasTuner开始：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The key idea that KerasTuner is built upon is to let you replace hardcoded hyperparameter
    values, such as `units=32`, with a range of possible choices, such as `Int(name="units",
    min_value=16, max_value=64, step=16)`. The set of such choices in a given model
    is called the *search space* of the hyperparameter tuning process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KerasTuner 建立在其核心思想之上，即让你用一系列可能的选择，如 `Int(name="units", min_value=16, max_value=64,
    step=16)`，来替换硬编码的超参数值，如 `units=32`。在给定模型中，这样的选择集合被称为超参数调优过程的 *搜索空间*。
- en: To specify a search space, define a model-building function (see the next listing).
    It takes an `hp` argument, from which you can sample hyperparameter ranges, and
    it returns a compiled Keras model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定搜索空间，定义一个模型构建函数（参见下一列表）。它接受一个 `hp` 参数，你可以从中采样超参数范围，并返回一个编译好的 Keras 模型。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 18.1](#listing-18-1): A KerasTuner model-building function'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 18.1](#listing-18-1)：一个 KerasTuner 模型构建函数'
- en: If you want to adopt a more modular and configurable approach to model-building,
    you can also subclass the `HyperModel` class and define a `build` method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要采用更模块化和可配置的方法来构建模型，你也可以从 `HyperModel` 类中派生出一个子类，并定义一个 `build` 方法。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 18.2](#listing-18-2): A KerasTuner `HyperModel`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 18.2](#listing-18-2)：一个 KerasTuner `HyperModel`'
- en: The next step is to define a “tuner.” Schematically, you can think of a tuner
    as a `for` loop, which will repeatedly
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义一个“调谐器”。从概念上讲，你可以将调谐器视为一个 `for` 循环，它将反复
- en: Pick a set of hyperparameter values
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一组超参数值
- en: Call the model-building function with these values to create a model
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些值调用模型构建函数以创建一个模型
- en: Train the model and record its metrics
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型并记录其指标
- en: 'KerasTuner has several built-in tuners available — `RandomSearch`, `BayesianOptimization`,
    and `Hyperband`. Let’s try `BayesianOptimization`, a tuner that attempts to make
    smart predictions for which new hyperparameter values are likely to perform best
    given the outcome of previous choices:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KerasTuner 有几个内置的调谐器可用——`RandomSearch`、`BayesianOptimization` 和 `Hyperband`。让我们尝试
    `BayesianOptimization`，这是一个尝试根据先前选择的成果来智能预测哪些新的超参数值可能表现最佳的调谐器：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can display an overview of the search space via `search_space_summary()`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `search_space_summary()` 显示搜索空间的概述：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, let’s launch the search. Don’t forget to pass validation data and
    make sure not to use your test set as validation data — otherwise, you’d quickly
    start overfitting to your test data, and you wouldn’t be able to trust your test
    metrics anymore:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们启动搜索。别忘了传递验证数据，并确保不要使用测试集作为验证数据——否则，你很快就会开始对测试数据过拟合，而且你将无法再信任你的测试指标：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The previous example will run in just a few minutes since we’re only looking
    at a few possible choices and we’re training on MNIST. However, with a typical
    search space and dataset, you’ll often find yourself letting the hyperparameter
    search run overnight or even over several days. If your search process crashes,
    you can always restart it — just specify `overwrite=False` in the tuner so that
    it can resume from the trial logs stored on disk.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只查看几个可能的选择，并且在 MNIST 上进行训练，所以前面的例子只需几分钟就能运行。然而，在典型的搜索空间和数据集上，你通常会发现自己让超参数搜索运行一整夜，甚至几天。如果你的搜索过程崩溃，你总是可以重新启动它——只需在调谐器中指定
    `overwrite=False`，这样它就可以从存储在磁盘上的试验日志中恢复。
- en: Once the search is complete, you can query the best hyperparameter configurations,
    which you can use to create high-performing models that you can then retrain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦搜索完成，你可以查询最佳超参数配置，这些配置可以用来创建高性能模型，然后你可以重新训练这些模型。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 18.3](#listing-18-3): Querying the best hyperparameter configurations'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 18.3](#listing-18-3)：查询最佳超参数配置'
- en: Usually, when retraining these models, you may want to include the validation
    data as part of the training data since you won’t be making any further hyperparameter
    changes, and thus you will no longer be evaluating performance on the validation
    data. In our example, we’d train these final models on the totality of the original
    MNIST training data, without reserving a validation set.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你重新训练这些模型时，你可能希望将验证数据作为训练数据的一部分，因为你不会进行任何进一步的超参数更改，因此你将不再在验证数据上评估性能。在我们的例子中，我们将使用原始
    MNIST 训练数据的全部来训练这些最终模型，而不保留验证集。
- en: 'Before we can train on the full training data, though, there’s one last parameter
    we need to settle: the optimal number of epochs to train for. Typically, you’ll
    want to train the new models for longer than you did during the search: using
    an aggressive `patience` value in the `EarlyStopping` callback saves time during
    the search, but may lead to underfitted models. Just use the validation set to
    find the best epoch:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够对全部训练数据进行训练之前，尽管如此，我们还需要解决最后一个参数：训练的最佳轮数。通常，你希望对新模型进行比搜索期间更长时间的训练：在`EarlyStopping`回调中使用激进的`patience`值可以在搜索过程中节省时间，但可能会导致模型欠拟合。只需使用验证集来找到最佳的轮数：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And finally, train on the full dataset for just a bit longer than this epoch
    count, since you’re training on more data — 20% more, in this case:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于你正在训练更多的数据——在这个例子中是20%更多的数据——所以你需要在这个轮数的基础上再训练一段时间：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you’re not worried about slightly underperforming, there’s a shortcut you
    can take: just use the tuner to reload the top-performing models with the best
    weights saved during the hyperparameter search, without retraining new models
    from scratch:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太担心稍微低一点的性能，你可以采取一个捷径：只需使用调整器重新加载在超参数搜索期间保存的最佳权重下的顶级性能模型，而无需从头开始重新训练新模型：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The art of crafting the right search space
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建正确搜索空间的艺术
- en: 'Overall, hyperparameter optimization is a powerful technique that is an absolute
    requirement to get to state-of-the-art models on any task or to win machine learning
    competitions. Think about it: once upon a time, people handcrafted the features
    that went into shallow machine learning models. That was very suboptimal. Now
    deep learning automates the task of hierarchical feature engineering — features
    are learned using a feedback signal, not hand-tuned, and that’s the way it should
    be. In the same way, you shouldn’t handcraft your model architectures; you should
    optimize them in a principled way.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，超参数优化是一种强大的技术，对于在任何任务上达到最先进的模型或赢得机器学习竞赛来说是绝对必要的。想想看：曾经，人们手工制作了浅层机器学习模型中的特征。那是非常低效的。现在深度学习自动化了层次特征工程的任务——特征是通过反馈信号学习的，而不是手工调整的，这就是应该的方式。同样，你不应该手工制作你的模型架构；你应该以原则性的方式优化它们。
- en: 'However, doing hyperparameter tuning is not a replacement for being familiar
    with model architecture best practices: search spaces grow combinatorially with
    the number of choices, so it would be far too expensive to turn everything into
    a hyperparameter and let the tuner sort it out. You need to be smart about designing
    the right search space. Hyperparameter tuning is automation, not magic: you use
    it to automate experiments that you would otherwise have run by hand, but you
    still need to handpick experiment configurations that have the potential to yield
    good metrics.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，进行超参数调整并不能取代熟悉模型架构的最佳实践：随着选择数量的增加，搜索空间呈组合式增长，因此将所有内容都变成超参数并让调整器处理它将非常昂贵。你需要聪明地设计合适的搜索空间。超参数调整是自动化，而不是魔法：你使用它来自动化你本应手动运行的实验，但你仍然需要手动选择有潜力产生良好指标的实验配置。
- en: 'The good news: by using hyperparameter tuning, the configuration decisions
    you have to make graduate from micro-decisions (What number of units do I pick
    for this layer?) to higher-level architecture decisions (Should I use residual
    connections throughout this model?). And while micro-decisions are specific to
    a certain model and a certain dataset, higher-level decisions generalize better
    across different tasks and datasets: for instance, pretty much every image classification
    problem can be solved via the same sort of search space template.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是：通过使用超参数调整，你必须做出的配置决策从微观决策（我该为这一层选择多少个单元？）升级到更高层次的架构决策（我是否应该在整个模型中使用残差连接？）。虽然微观决策特定于某个模型和某个数据集，但更高层次的决策在不同任务和数据集上具有更好的泛化能力：例如，几乎每个图像分类问题都可以通过相同类型的搜索空间模板来解决。
- en: Following this logic, KerasTuner attempts to provide *premade search spaces*
    that are relevant to broad categories of problems — such as image classification.
    Just add data, run the search, and get a pretty good model. You can try the hypermodels
    `kt.applications.HyperXception` and `kt.applications.HyperResNet`, which are effectively
    tunable versions of Keras Applications models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个逻辑，KerasTuner试图提供与广泛问题类别相关的*预先准备好的搜索空间*——例如图像分类。只需添加数据，运行搜索，就能得到一个相当不错的模型。你可以尝试超模型`kt.applications.HyperXception`和`kt.applications.HyperResNet`，它们实际上是Keras
    Applications模型的可调整版本。
- en: Model ensembling
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型集成
- en: Another powerful technique for obtaining the best possible results on a task
    is *model ensembling*. Ensembling consists of pooling together the predictions
    of a set of different models to produce better predictions. If you look at machine
    learning competitions — in particular, on Kaggle — you’ll see that the winners
    use very large ensembles of models that inevitably beat any single model, no matter
    how good.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个任务上获得最佳可能结果的一种强大技术是*模型集成*。集成包括将一组不同模型的预测汇总起来以产生更好的预测。如果你查看机器学习竞赛——特别是在Kaggle上——你会发现赢家使用非常大的模型集成，这不可避免地会击败任何单个模型，无论其多么优秀。
- en: 'Ensembling relies on the assumption that different well-performing models trained
    independently are likely to be good for different reasons: each model looks at
    slightly different aspects of the data to make its predictions, getting part of
    the “truth” but not all of it. You may be familiar with the ancient parable of
    the blind men and the elephant: a group of blind men come across an elephant for
    the first time and try to understand what the elephant is by touching it. Each
    man touches a different part of the elephant’s body — just one part, such as the
    trunk or a leg. Then the men describe to each other what an elephant is: “It’s
    like a snake,” “Like a pillar or a tree,” and so on. The blind men are essentially
    machine learning models trying to understand the manifold of the training data,
    each from its own perspective, using its own assumptions (provided by the unique
    architecture of the model and the unique random weight initialization). Each of
    them gets part of the truth of the data, but not the whole truth. By pooling their
    perspectives together, you can get a far more accurate description of the data.
    The elephant is a combination of parts: no single blind man gets it quite right,
    but interviewed together, they can tell a fairly accurate story.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 集成依赖于这样的假设：独立训练的不同表现良好的模型可能因不同的原因而表现良好：每个模型查看数据的不同方面来做出预测，获得部分“真相”但不是全部。你可能熟悉古老的寓言《盲人摸象》：一群盲人第一次遇到大象，试图通过触摸它来理解大象是什么。每个盲人都触摸到大象身体的不同部分——只触摸一部分，比如鼻子或腿。然后盲人们向彼此描述大象是什么：“它像一条蛇”，“像一根柱子或一棵树”，等等。盲人们本质上是在尝试从自己的视角、使用自己的假设（由模型的独特架构和独特的随机权重初始化提供）来理解训练数据的机器学习模型。每个模型都获得了数据的一部分真相，但不是全部真相。通过汇总他们的视角，你可以得到对数据的更准确描述。大象是各个部分的组合：没有哪个盲人能完全正确地描述它，但当他们一起接受采访时，他们可以讲述一个相当准确的故事。
- en: 'Let’s use classification as an example. The easiest way to pool the predictions
    of a set of classifiers (to *ensemble the classifiers*) is to average their predictions
    at inference time:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以分类为例。将一组分类器的预测（即*集成分类器*）进行汇总的最简单方法是在推理时平均它们的预测：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, this will work only if the classifiers are more or less equally good.
    If one of them is significantly worse than the others, the final predictions may
    not be as good as the best classifier of the group.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只有在分类器大致相同的情况下才会有效。如果其中之一明显比其他分类器差得多，最终的预测可能不会像该组中最好的分类器那样好。
- en: 'A smarter way to ensemble classifiers is to do a weighted average, where the
    weights are learned on the validation data — typically, the better classifiers
    are given a higher weight, and the worse classifiers are given a lower weight.
    To search for a good set of ensembling weights, you can use random search or a
    simple optimization algorithm, such as the Nelder-Mead algorithm:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更智能的集成分类器的方式是进行加权平均，其中权重是在验证数据上学习的——通常，表现更好的分类器会得到更高的权重，而表现较差的分类器会得到较低的权重。为了寻找一组好的集成权重，你可以使用随机搜索或简单的优化算法，例如Nelder-Mead算法：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There are many possible variants: you can do an average of an exponential of
    the predictions, for instance. In general, a simple weighted average with weights
    optimized on the validation data provides a very strong baseline.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可能的变体：例如，你可以对预测的指数进行平均。一般来说，在验证数据上优化的简单加权平均提供了一个非常强大的基线。
- en: The key to making ensembling work is the *diversity* of the set of classifiers.
    Diversity is strength. If all the blind men only touched the elephant’s trunk,
    they would agree that elephants are like snakes, and they would forever stay ignorant
    of the truth of the elephant. Diversity is what makes ensembling work. In machine
    learning terms, if all of your models are biased in the same way, then your ensemble
    will retain this same bias. If your models are *biased in different ways*, the
    biases will cancel each other out, and the ensemble will be more robust and more
    accurate.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使集成有效果的关键是分类器集的*多样性*。多样性是力量。如果所有盲人只触摸大象的鼻子，他们会同意大象像蛇一样，他们将永远不知道大象的真实情况。多样性是使集成工作起来的因素。在机器学习的术语中，如果你的所有模型都以相同的方式有偏差，那么你的集成将保留这种相同的偏差。如果你的模型以*不同的方式*有偏差，偏差将相互抵消，集成将更加稳健和准确。
- en: For this reason, you should ensemble models that are *as good as possible* while
    being *as different as possible*. This typically means using very different architectures
    or even different brands of machine learning approaches. One thing that is largely
    not worth doing is ensembling the same network trained several times independently,
    from different random initializations. If the only difference between your models
    is their random initialization and the order in which they were exposed to the
    training data, then your ensemble will be low in diversity and will provide only
    a tiny improvement over any single model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你应该集成尽可能好且尽可能不同的模型。这通常意味着使用非常不同的架构，甚至使用不同的机器学习方法的品牌。一件在很大程度上不值得做的事情是集成多次独立训练的相同网络，从不同的随机初始化开始。如果你的模型之间唯一的区别是它们的随机初始化以及它们接触训练数据的顺序，那么你的集成将缺乏多样性，并且只会对任何单个模型提供微小的改进。
- en: 'One thing I have found to work well in practice — but that doesn’t generalize
    to every problem domain — is the use of an ensemble of tree-based methods (such
    as random forests or gradient-boosted trees) and deep neural networks. In 2014,
    Andrei Kolev and I took fourth place in the Higgs Boson decay detection challenge
    on Kaggle (www.kaggle.com/c/higgs-boson) using an ensemble of various tree models
    and deep neural networks. Remarkably, one of the models in the ensemble originated
    from a different method than the others (it was a regularized greedy forest) and
    had a significantly worse score than the others. Unsurprisingly, it was assigned
    a small weight in the ensemble. But to our surprise, it turned out to improve
    the overall ensemble by a large factor because it was so different from every
    other model: it provided information that the other models didn’t have access
    to. That’s precisely the point of ensembling. It’s not so much about how good
    your best model is; it’s about the diversity of your set of candidate models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我在实践中发现了一件事效果很好——但这并不适用于每个问题领域——那就是使用基于树的多种方法的集成（例如随机森林或梯度提升树）和深度神经网络。2014年，我和安德烈·科列夫在Kaggle（www.kaggle.com/c/higgs-boson）的希格斯玻色子衰变检测挑战中获得了第四名，我们使用了各种基于树的模型和深度神经网络的集成。令人惊讶的是，集成中的一个模型来自与其他不同的方法（它是一个正则化贪婪森林），并且比其他模型得分显著更低。不出所料，它在集成中被分配了很小的权重。但令我们惊讶的是，它最终通过其与其他模型如此不同而大幅提高了整体集成的性能：它提供了其他模型无法获得的信息。这正是集成的目的。这不仅仅关乎你的最佳模型有多好；这关乎你候选模型集的多样性。
- en: Scaling up model training with multiple devices
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多设备扩展模型训练
- en: 'Recall the “loop of progress” concept we introduced in chapter 7: the *quality*
    of your ideas is a function of how many refinement cycles they’ve been through
    (figure 18.1). And the speed at which you can iterate on an idea is a function
    of how fast you can set up an experiment, how fast you can run that experiment,
    and, finally, how well you can analyze the resulting data.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在第7章中介绍的“进步循环”概念：你想法的质量是它们经过多少次精炼循环的函数（见图18.1）。而你对一个想法进行迭代的速度取决于你设置实验的速度、运行实验的速度，以及最终分析结果数据的能力。
- en: '![](../Images/55c7b82f43c4187d22ac7ddef68ddd0d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55c7b82f43c4187d22ac7ddef68ddd0d.png)'
- en: '[Figure 18.1](#figure-18-1): The loop of progress'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18.1](#figure-18-1)：进步循环'
- en: As you develop your expertise in the Keras API, how fast you can code up your
    deep learning experiments will cease to be the bottleneck of this progress cycle.
    The next bottleneck will become the speed at which you can train your models.
    Fast training infrastructure means that you can get your results back in 10 or
    15 minutes and, hence, that you can go through dozens of iterations every day.
    Faster training directly improves the *quality* of your deep learning solutions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你对Keras API的专长发展，你编写深度学习实验的速度将不再是这个进步周期的瓶颈。下一个瓶颈将变成你训练模型的速度。快速训练基础设施意味着你可以在10或15分钟内得到结果，因此你可以每天进行数十次迭代。更快的训练直接提高了你的深度学习解决方案的*质量*。
- en: In this section, you’ll learn about how to scale up your training runs by using
    multiple GPUs or TPUs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解如何通过使用多个GPU或TPU来扩展你的训练运行。
- en: Multi-GPU training
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多GPU训练
- en: While GPUs are getting more powerful every year, deep learning models are also
    getting increasingly larger, requiring ever more computational resources. Training
    on a single GPU puts a hard bound on how fast you can move. The solution? You
    could simply add more GPUs and start doing *multi-GPU distributed training*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPU每年都在变得更强大，但深度学习模型也在变得越来越庞大，需要更多的计算资源。在单个GPU上训练对移动速度的速度设定了一个硬限制。解决方案？你可以简单地添加更多的GPU并开始进行*多GPU分布式训练*。
- en: 'There are two ways to distribute computation across multiple devices: *data
    parallelism* and *model parallelism*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个设备上分配计算有两种方式：*数据并行*和*模型并行*。
- en: With data parallelism, a single model gets replicated on multiple devices or
    multiple machines. Each of the model replicas processes different batches of data,
    and then they merge their results.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据并行，单个模型在多个设备或多个机器上被复制。每个模型副本处理不同的数据批次，然后合并他们的结果。
- en: 'With model parallelism, different parts of a single model run on different
    devices, processing a single batch of data together at the same time. This works
    best with models that have a naturally parallel architecture, such as models that
    feature multiple branches. In practice, model parallelism is only used in the
    case of models that are too large to fit on any single device: it isn’t used as
    a way to speed up training of regular models but as a way to train larger models.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型并行，单个模型的不同部分在不同的设备上运行，同时处理单个数据批次。这对于具有自然并行架构的模型效果最好，例如具有多个分支的模型。在实践中，模型并行仅在模型太大而无法适应任何单个设备的情况下使用：它不是用作加速常规模型训练的方法，而是用作训练更大模型的方法。
- en: 'Then, of course, you can also mix both data parallelism and model parallelism:
    a single model can be split across multiple devices (e.g., 4), and that split
    model can be replicated across multiple groups of devices (e.g., twice, for a
    total of 2 * 4 = 8 devices used).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当然，你也可以混合使用数据并行和模型并行：单个模型可以跨多个设备（例如，4个）分割，并且分割后的模型可以在多个设备组（例如，两次，总共使用2 *
    4 = 8个设备）。
- en: Let’s see how that works in detail.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看它是如何工作的。
- en: 'Data parallelism: Replicating your model on each GPU'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据并行：在每个GPU上复制你的模型
- en: 'Data parallelism is the most common form of distributed training. It operates
    on a simple principle: divide and conquer. Each GPU receives a copy of the entire
    model, called a *replica*. Incoming batches of data are split into *N* sub-batches,
    which are processed by one model replica each, in parallel. This is why it’s called
    *data parallelism*: different samples (data points) are processed in parallel.
    For instance, with two GPUs, a batch of size 128 would be split into two sub-batches
    of size 64, which would be processed by two model replicas. Then'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行是分布式训练中最常见的形式。它基于一个简单的原则：分而治之。每个GPU接收整个模型的副本，称为*副本*。传入的数据批次被分成*N*个子批次，每个子批次由一个模型副本并行处理。这就是为什么它被称为*数据并行*：不同的样本（数据点）是并行处理的。例如，如果有两个GPU，大小为128的批次将被分成两个大小为64的子批次，由两个模型副本处理。然后
- en: '*In inference* — We would retrieve the predictions for each sub-batch and concatenate
    them to obtain the predictions for the full batch.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在推理* — 我们将检索每个子批次的预测并将它们连接起来以获得整个批次的预测。'
- en: '*In training* — We would retrieve the gradients for each sub-batch, average
    them, and update all model replicas based on the gradient average. The state of
    the model would then be the same as if you had trained it on the full batch of
    128 samples. This is called *synchronous* training, since all replicas are kept
    in sync — their weights have the same value at all times. Nonsynchronous alternatives
    exist, but they are less efficient and aren’t used anymore in practice.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练过程中*——我们会检索每个子批次的梯度，计算平均值，并根据梯度平均值更新所有模型副本。然后，模型的状态将与你在128个样本的全批次上训练时相同。这被称为*同步*训练，因为所有副本都保持同步——它们的权重在所有时间点都有相同的值。存在非同步的替代方案，但它们效率较低，并且在实践中不再使用。'
- en: 'Data parallelism is a simple and highly scalable way to train your models faster.
    If you get more devices, just increase your batch size, and your training throughput
    increases accordingly. It has one limitation, though: it requires your model to
    be able to fit into one of your devices. However, it is now common to train foundation
    models that have tens of billions of parameters, which wouldn’t fit on any single
    GPU.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行是一种简单且高度可扩展的方式来加速你的模型训练。如果你获得更多设备，只需增加你的批次大小，你的训练吞吐量就会相应增加。然而，它有一个限制：它要求你的模型能够适应你的某个设备。然而，现在训练具有数十亿参数的基础模型是很常见的，这些模型无法适应任何单个GPU。
- en: 'Model parallelism: Splitting your model across multiple GPUs'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型并行：将你的模型分割到多个GPU上
- en: That’s where *model parallelism* comes in. While data parallelism works by splitting
    your batches of data into sub-batches and processing the sub-batches in parallel,
    model parallelism works by splitting your model into submodels and running each
    one on a different device — in parallel. For instance, consider the following
    model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*模型并行*的作用所在。虽然数据并行是通过将你的数据批次分割成子批次并在并行处理子批次来工作的，但模型并行是通过将你的模型分割成子模型，并在不同的设备上并行运行每个子模型来工作的。例如，考虑以下模型。
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 18.4](#listing-18-4): A large densely connected model'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表18.4](#listing-18-4)：一个大型密集连接模型'
- en: 'Each sample has 16,000 features and gets classified into 8,000 potentially
    overlapping categories by two `Dense` layers. Those are large layers — the first
    one has about 1 billion parameters, and the last one has about 512 million parameters.
    If you’re working with two small devices, you won’t be able to use data parallelism,
    since you can’t fit the model on a single device. What you can do is *split* a
    single instance of the model across multiple devices. This is often called *sharding*
    or *partitioning* a model. There are two main ways to split a model across devices:
    horizontal partitioning and vertical partitioning.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本有16,000个特征，并通过两个`Dense`层被分类到8,000个可能重叠的类别中。这些层很大——第一个大约有10亿个参数，最后一个大约有5.12亿个参数。如果你使用的是两个小设备，你将无法使用数据并行，因为你无法将模型拟合到单个设备上。你可以做的是将单个模型实例分割到多个设备上。这通常被称为*分片*或*分区*模型。在设备间分割模型主要有两种方式：水平分区和垂直分区。
- en: In horizontal partitioning, each device processes different layers of the model.
    For example, in the previous model, one GPU would handle the first `Dense` layer,
    and the other one would handle the second `Dense` layer. The main drawback of
    this approach is that it can introduce communication overhead. For example, the
    output of the first layer needs to be copied to the second device before it can
    be processed by the second layer. This can become a bottleneck, especially if
    the output of the first layer is large — you’d risk keeping your GPUs idle.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在水平分区中，每个设备处理模型的不同层。例如，在之前的模型中，一个GPU会处理第一个`Dense`层，而另一个GPU会处理第二个`Dense`层。这种方法的缺点主要是可能会引入通信开销。例如，第一层的输出需要在第二层处理之前被复制到第二个设备上。这可能会成为瓶颈，尤其是如果第一层的输出很大——你可能会冒着让GPU闲置的风险。
- en: 'In vertical partitioning, each layer is split across all available devices.
    Since layers are usually implemented in terms of `matmul` or `convolution` operations,
    which are highly parallelizable, this strategy is easy to implement in practice
    and is almost always the best fit for large models. For example, in the previous
    model, you could split the kernel and bias of the first `Dense` layer into two
    halves so that each device only receives a kernel of shape `(16000, 32000)` (split
    along its last axis) and a bias of shape `(32000,)`. You’d compute `matmul(inputs,
    kernel) + bias` with this half-kernel and half-bias for each device, and you’d
    merge the two outputs by concatenating them like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在垂直分区中，每一层都分布在所有可用设备上。由于层通常是用 `matmul` 或 `convolution` 操作实现的，这些操作高度可并行化，因此在实践中实现这种策略很容易，并且几乎总是大型模型的最佳选择。例如，在先前的模型中，你可以将第一个
    `Dense` 层的内核和偏置分成两半，这样每个设备只接收形状为 `(16000, 32000)` 的内核（沿其最后一个轴分割）和形状为 `(32000,)`
    的偏置。你将使用这个半内核和半偏置对每个设备进行 `matmul(inputs, kernel) + bias` 计算，然后通过以下方式连接两个输出：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In reality, you will want to mix data parallelism and model parallelism. You
    will split your model across, say, four devices, and you will replicate that split
    model across multiple groups of two devices — let’s say two — each processing
    one sub-batch of data in parallel. You will then have two replicas, each running
    on four devices, for a total of eight devices used (figure 18.2).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你将想要混合数据并行性和模型并行性。你将把你的模型分布在，比如说，四个设备上，然后你将在多个两组设备中复制那个分割的模型——比如说两组——每组并行处理一个子批次的数据。然后你将有两个副本，每个副本在四个设备上运行，总共使用八个设备（图18.2）。
- en: '![](../Images/83dcf3712374590e5c60bc5c084d67b9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83dcf3712374590e5c60bc5c084d67b9.png)'
- en: '[Figure 18.2](#figure-18-2): Distributing a model across eight devices: two
    model replicas, each handled by a group of four devices'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18.2](#figure-18-2)：将模型分布在八个设备上：两个模型副本，每个由四组设备处理'
- en: Distributed training in practice
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实践中的分布式训练
- en: Now let’s see how to implement these concepts in practice. We will only cover
    the JAX backend, as it is the most performant and most scalable of the various
    Keras backends, by a mile. If you’re doing any kind of large-scale distributed
    training and you aren’t using JAX, you’re making a mistake — and wasting your
    dollars burning way more compute than you actually need.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何在实践中实现这些概念。我们只会涵盖JAX后端，因为它是各种Keras后端中最高效和最可扩展的，远远超过其他后端。如果你在进行任何类型的大规模分布式训练，而你没有使用JAX，那么你正在犯一个错误——并且浪费你的美元，消耗了比你实际需要的计算资源多得多的计算。
- en: Getting your hands on two or more GPUs
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取两个或更多GPU
- en: 'First, you need to get access to several GPUs. As of now, Google Colab only
    lets you use a single GPU, so you will need to do one of two things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要获取几个GPU的访问权限。到目前为止，Google Colab只允许你使用单个GPU，所以你需要做以下两件事之一：
- en: Acquire two to eight GPUs, mount them on a single machine (it will require a
    beefy power supply), and install CUDA drivers, cuDNN, etc. For most people, this
    isn’t the best option.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取两个到八个GPU，将它们安装在一台机器上（这将需要一个强大的电源），并安装CUDA驱动程序、cuDNN等。对大多数人来说，这不是最佳选择。
- en: Rent a multi-GPU virtual machine (VM) on Google Cloud, Azure, or AWS. You’ll
    be able to use VM images with pre-installed drivers and software, and you’ll have
    very little setup overhead. This is likely the best option for anyone who isn’t
    training models 24/7.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Google Cloud、Azure或AWS上租用多GPU虚拟机（VM）。你将能够使用预安装驱动程序和软件的VM镜像，并且设置开销非常小。这可能是对那些不是全天候训练模型的人来说的最佳选择。
- en: We won’t cover the details of how to spin up multi-GPU cloud VMs because such
    instructions would be relatively short-lived, and this information is readily
    available online.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细介绍如何启动多GPU云虚拟机，因为这样的说明相对短暂，并且这些信息在网上很容易找到。
- en: Using data parallelism with JAX
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在JAX中使用数据并行性
- en: 'Using data parallelism with Keras and JAX is very simple: before building your
    model, just add the following line of code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras和JAX中使用数据并行性非常简单：在构建你的模型之前，只需添加以下代码行：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That’s it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些。
- en: If you want more granular control, you can specify which devices you want to
    use. You can list available devices via
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更细粒度的控制，你可以指定你想要使用的设备。你可以通过以下方式列出可用设备：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It will return a list of strings — the names of your devices, such as `"gpu:0"`,
    `"gpu:1"`, and so on. You can then pass these to the `DataParallel` constructor:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回一个字符串列表——你的设备名称，例如 `"gpu:0"`、`"gpu:1"` 等等。然后你可以将这些传递给 `DataParallel` 构造函数：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In an ideal world, training on *N* GPUs would result in a speedup of factor
    *N*. In practice, however, distribution introduces some overhead — in particular,
    merging the weight deltas originating from different devices takes some time.
    The effective speedup you get is a function of the number of GPUs used:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，在*N*个GPU上训练将导致加速因子为*N*。然而，在实践中，分布引入了一些开销——特别是，合并来自不同设备的权重变化需要一些时间。你获得的有效加速是所用GPU数量的函数：
- en: With two GPUs, the speedup stays close to 2×.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个GPU时，加速保持在2×左右。
- en: With four, the speedup is around 3.8×.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用四个时，加速大约为3.8×。
- en: With eight, it’s around 7.3×.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用八个时，大约为7.3×。
- en: This assumes that you’re using a large-enough global batch size to keep each
    GPU utilized at full capacity. If your batch size is too small, the local batch
    size won’t be enough to keep your GPUs busy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设你使用足够大的全局批次大小，以保持每个GPU的满负荷运行。如果你的批次大小太小，本地批次大小将不足以保持GPU忙碌。
- en: Using model parallelism with JAX
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用JAX进行模型并行
- en: Keras also provides powerful tools for fully customizing how you want to do
    distributed training, including model parallel training and any mixture of data
    parallel and model parallel training you can imagine. Let’s dive in.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Keras还提供了强大的工具，可以完全自定义你想要如何进行分布式训练，包括模型并行训练以及你可以想象的数据并行和模型并行训练的任何混合。让我们深入了解。
- en: The DeviceMesh API
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DeviceMesh API
- en: 'First, you need to understand the concept of a *device mesh*. A device mesh
    is simply a grid of devices. Consider this example, with eight GPUs:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要理解**设备网格**的概念。设备网格简单地说就是设备网格。考虑以下示例，有八个GPU：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The big idea is to separate devices into groups, organized along axes. Typically,
    one axis will be responsible for data parallelism, and one axis will be responsible
    for model parallelism (like in figure 18.2, your devices form a grid, where the
    horizontal axis handles data parallelism and the vertical axis handles model parallelism).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想是将设备分为组，沿着轴组织。通常，一个轴将负责数据并行，另一个轴将负责模型并行（如图18.2所示，你的设备形成一个网格，其中水平轴处理数据并行，垂直轴处理模型并行）。
- en: A device mesh doesn’t have to be 2D — it could be any shape you want. In practice,
    however, you will only ever see 1D and 2D meshes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 设备网格不必是二维的——它可以是你想要的任何形状。然而，在实践中，你只会看到一维和二维网格。
- en: 'Let’s make a 2 × 4 device mesh in Keras:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Keras中创建一个2×4的设备网格：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Mind you, you can also explicitly specify the devices you want to use:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你也可以明确指定你想要使用的设备：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you may have guessed from the `axis_names` argument, we intend to use the
    devices along axis 0 for data parallelism and the devices along axis 1 for model
    parallelism. Since there are two devices along axis 0 and four along axis 1, we’ll
    split our model’s computation across four GPUs, and we’ll make two copies of our
    split model, running each copy on a different sub-batch of data in parallel.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从`axis_names`参数中猜到的，我们打算使用轴0上的设备进行数据并行，轴1上的设备进行模型并行。由于轴0上有两个设备，轴1上有四个，我们将把模型计算分配到四个GPU上，并将我们的分割模型复制两次，并行地在不同子批次的数据上运行。
- en: Now that we have our mesh, we need to tell Keras how to split different pieces
    of computation across our devices. For that, we’ll use the `LayoutMap` API.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了网格，我们需要告诉Keras如何将不同的计算部分分配到我们的设备上。为此，我们将使用`LayoutMap` API。
- en: The LayoutMap API
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LayoutMap API
- en: To specify where different bits of computation should take place, we use *variables*
    as our frame of reference. We will split or replicate variables across our devices,
    and we will let the compiler move all computation associated with that part of
    the variable to the corresponding device.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定不同的计算部分应该在何处进行，我们使用**变量**作为我们的参考框架。我们将分割或复制变量到我们的设备上，并将所有与该变量部分相关的计算移动到相应的设备上。
- en: 'Consider a variable. Its shape is, let’s say, `(32, 64)`. There are two things
    you could do with this variable:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个变量。它的形状是，比如说，`(32, 64)`。你可以对这个变量做两件事：
- en: You could *replicate it* (copy it) across an axis of your mesh so each device
    along that axis sees the same value.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在网格的轴上**复制**（复制）它，这样沿着该轴的每个设备都能看到相同的值。
- en: You could *shard it* (split it) across an axis of your mesh — for instance,
    you could shard it into four chunks of shape `(32, 16)` — so that each device
    along that axis sees one different chunk.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在网格的轴上**分割**（拆分）它——例如，你可以将其分割成四个形状为`(32, 16)`的块——这样沿着该轴的每个设备都能看到不同的块。
- en: Now, do note that our variable has two dimensions. Importantly, “sharding” or
    “replicating” are decisions that you can make independently for each dimension
    of the variable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意我们的变量有两个维度。重要的是，“分片”或“复制”是你可以独立为变量的每个维度做出的决定。
- en: 'The API you will use to tell Keras about such decisions is the `LayoutMap`
    class. A `LayoutMap` is similar to a dictionary. It maps model variables (for
    instance, the kernel variable of the first dense layer in your model) to a bit
    of information about how that variable should be replicated or sharded over a
    device mesh. Specifically, it maps a *variable path* to a tuple that has as many
    entries as your variable has dimensions, where each entry specifies what to do
    with that variable dimension. It looks like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你将用来告诉Keras这些决定的API是`LayoutMap`类。`LayoutMap`类似于字典。它将模型变量（例如，你模型中第一个密集层的核变量）映射到关于该变量如何在设备网格上复制或分片的一些信息。具体来说，它将一个*变量路径*映射到一个元组，该元组有与变量维度一样多的条目，其中每个条目指定了对该变量维度的操作。它看起来像这样：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is the first time you encountered the concept of a *variable path* — it
    is simply a string identifier that looks like `"sequential/dense_1/kernel"`. It’s
    a useful way to refer to a variable without keeping a handle on the actual variable
    instance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你第一次遇到*变量路径*的概念——它只是一个看起来像`"sequential/dense_1/kernel"`的字符串标识符。这是一个在不保留实际变量实例的情况下引用变量的有用方式。
- en: 'Here’s how you can print the paths for all variables in a model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可以打印模型中所有变量路径的方法：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'On the example model from listing 18.4, here’s what we get:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表18.4的示例模型中，我们得到的结果如下：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let’s shard and replicate these variables. In the case of a simple model
    like this one, your go-to rule of thumb for variable sharding should be as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分片和复制这些变量。对于这样一个简单的模型，你的变量分片的基本规则应该是这样的：
- en: Shard the last dimension of the variable along the `"model"` mesh axis.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着`"model"`网格轴分片变量的最后一个维度。
- en: Leave all other dimensions as replicated.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有其他维度保持为复制状态。
- en: 'Simple enough, right? Like this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 简单吧？就像这样：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we tell Keras to refer to this sharding layout when instantiating
    the variables by setting the distribution configuration like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过设置分布配置来告诉Keras在实例化变量时参考这个分片布局，如下所示：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once the distribution configuration is set, you can create your model and `fit()`
    it. No other part of your code changes — your model definition code is the same,
    and your training code is the same. That’s true whether you’re using built-in
    APIs like `fit()` and `evaluate()` or your own training logic. Assuming that you
    have the right `LayoutMap` for your variables, the little code snippets you just
    saw are enough to distribute computation for any large language model training
    run — it scales to as many devices as you have available and arbitrary model sizes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了分布配置，你就可以创建你的模型并使用`fit()`函数来拟合它。你的代码的其他部分不会改变——你的模型定义代码和训练代码都是相同的。这适用于你使用内置的API如`fit()`和`evaluate()`，或者使用你自己的训练逻辑。假设你为你的变量有了正确的`LayoutMap`，你刚才看到的简短代码片段就足以分配任何大型语言模型训练运行的计算——它可以扩展到你拥有的任何数量的设备，以及任意大小的模型。
- en: 'To check how your variables were sharded, you can inspect the `variable.value.sharding`
    property, like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查你的变量是如何分片的，你可以检查`variable.value.sharding`属性，如下所示：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can even visualize it via the JAX utility `jax.debug.visualize_sharding`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以通过JAX实用工具`jax.debug.visualize_sharding`来可视化它：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: TPU training
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPU训练
- en: Beyond just GPUs, there is generally a trend in the deep learning world toward
    moving workflows to increasingly specialized hardware designed specifically for
    deep learning workflows; such single-purpose chips are known as ASICs (application-specific
    integrated circuits). Various companies big and small are working on new chips,
    but today the most prominent effort along these lines is Google’s Tensor Processing
    Unit (TPU), which is available on Google Cloud and via Google Colab.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GPU之外，在深度学习领域，通常有一个趋势是将工作流程转移到为深度学习工作流程专门设计的越来越专业的硬件上；这样的专用芯片被称为ASIC（应用特定集成电路）。大小各种公司都在开发新的芯片，但今天在这一领域最突出的努力是谷歌的Tensor
    Processing Unit (TPU)，它可在谷歌云和谷歌Colab上使用。
- en: 'Training on TPU does involve jumping through some hoops. But it’s worth the
    extra work: TPUs are really, really fast. Training on a TPU v2 (available on Colab)
    will typically be 15× faster than training a NVIDIA P100 GPU. For most models,
    TPU training ends up being 3× more cost-effective than GPU training on average.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上训练确实需要跳过一些障碍。但这是值得的：TPU真的非常快。在Colab上可用的TPU v2（NVIDIA P100 GPU）上训练通常比训练快15倍。对于大多数模型，TPU训练的平均成本效益比GPU训练高3倍。
- en: You can actually use TPU v2 for free in Colab. In the Colab menu, under the
    Runtime tab, in the Change Runtime Type option, you’ll notice that you have access
    to a TPU runtime in addition to the GPU runtime. For more serious training runs,
    Google Cloud also makes available TPU v3 through v5, which are even faster.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以在Colab中免费使用TPU v2。在Colab菜单中，在“Runtime”选项卡下，在“Change Runtime Type”选项中，你会注意到除了GPU运行时之外，你还可以访问TPU运行时。对于更严肃的训练运行，Google
    Cloud还提供了从v3到v5的TPU，它们甚至更快。
- en: When running Keras code with the JAX backend on a TPU-enabled notebook, you
    don’t need anything more than calling `keras.distribution.set_distribution(distribution)`
    with a `DataParallel` or `ModelParallel` distribution instance to start using
    your TPU cores. Make sure to call it before creating your model!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当在启用TPU的笔记本上使用JAX后端运行Keras代码时，你不需要做任何更多的事情，只需调用`keras.distribution.set_distribution(distribution)`并传入一个`DataParallel`或`ModelParallel`分布实例即可开始使用你的TPU核心。确保在创建模型之前调用它！
- en: Using step fusing to improve TPU utilization
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用步骤融合来提高TPU利用率
- en: 'Because a TPU has a lot of compute power available, you need to train with
    very large batches to keep the TPU cores busy. For small models, the batch size
    required can get extraordinarily large — upward of 10,000 samples per batch. When
    working with enormous batches, you should make sure to increase your optimizer
    learning rate accordingly: you’re going to be making fewer updates to your weights,
    but each update will be more accurate (since the gradients are computed using
    more data points); hence, you should move the weights by a greater magnitude with
    each update.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TPU拥有大量的计算能力，你需要使用非常大的批次来保持TPU核心忙碌。对于小型模型，所需的批次大小可能会非常大——每个批次超过10,000个样本。当处理巨大的批次时，你应该确保相应地增加你的优化器学习率：你将进行的权重更新会更少，但每次更新将更准确（因为梯度是使用更多的数据点计算的）；因此，你应该在每次更新中通过更大的幅度移动权重。
- en: 'There is, however, a simple trick you can use to keep reasonably sized batches
    while maintaining full TPU utilization: *step fusing*. The idea is to run multiple
    steps of training during each TPU execution step. Basically, do more work in between
    two roundtrips from the virtual machine memory to the TPU. To do this, simply
    specify the `steps_per_execution` argument in `compile()` — for instance, `steps_per_execution=8`
    to run eight steps of training during each TPU execution. For small models that
    are underutilizing the TPU, this can result in a dramatic speedup:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个简单的技巧可以在保持合理大小的批次的同时，维持TPU的完全利用率：*步骤融合*。其思路是在每个TPU执行步骤中运行多个训练步骤。基本上，在虚拟机内存与TPU之间往返两次之间做更多的工作。要做到这一点，只需在`compile()`函数中指定`steps_per_execution`参数——例如，`steps_per_execution=8`表示在每个TPU执行中运行八个训练步骤。对于未充分利用TPU的小型模型，这可以带来显著的加速：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Speeding up training and inference with lower-precision computation
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用低精度计算加速训练和推理
- en: What if I told you there’s a simple technique you could use to speed up training
    and inference of almost any model by up to 2×, basically for free? It seems too
    good to be true, and yet, such a trick does exist. To understand how it works,
    first, we need to take a look at the notion of “precision” in computer science.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉你有一个简单的技巧，你可以用它来加速几乎任何模型的训练和推理，速度提高高达2倍，基本上是免费的？这听起来太好了，但确实存在这样的技巧。要了解它是如何工作的，首先，我们需要看看计算机科学中“精度”的概念。
- en: Understanding floating-point precision
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解浮点精度
- en: 'Precision is to numbers what resolution is to images. Because computers can
    only process 1s and 0s, any number seen by a computer has to be encoded as a binary
    string. For instance, you may be familiar with `uint8` integers, which are integers
    encoded on eight bits: `00000000` represents `0` in `uint8`, and `11111111` represents
    255\. To represent integers beyond 255, you’d need to add more bits — eight isn’t
    enough. Most integers are stored on 32 bits, with which we can represent signed
    integers ranging from −2147483648 to 2147483647.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 精度对数字的重要性就像分辨率对图像的重要性。因为计算机只能处理1和0，所以任何被计算机看到的数字都必须被编码为二进制字符串。例如，你可能熟悉`uint8`整数，这些整数是八位编码的整数：`00000000`在`uint8`中表示`0`，而`11111111`表示255。要表示超过255的整数，你需要添加更多的位——八位是不够的。大多数整数都是32位存储的，我们可以用它们来表示从-2147483648到2147483647的带符号整数。
- en: 'Floating-point numbers are the same. In mathematics, real numbers form a continuous
    axis: there’s an infinite number of points in between any two numbers. You can
    always zoom in on the axis of reals. In computer science, this isn’t true: there’s
    only a finite number of intermediate points between 3 and 4, for instance. How
    many? Well, it depends on the *precision* you’re working with: the number of bits
    you’re using to store a number. You can only zoom up to a certain resolution.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数也是一样。在数学中，实数构成一个连续的轴：在任意两个数字之间有无限多个点。你总是可以放大实数轴。在计算机科学中，这并不成立：例如，在3和4之间只有有限数量的中间点。有多少个？这取决于你工作的**精度**：存储数字所使用的位数。你只能放大到一定的分辨率。
- en: 'There are three levels of precision you’d typically use:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常会使用三个级别的精度：
- en: Half precision, or `float16`, where numbers are stored on 16 bits
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半精度，或`float16`，其中数字存储在16位上
- en: Single precision, or `float32`, where numbers are stored on 32 bits
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单精度，或`float32`，其中数字存储在32位上
- en: Double precision, or `float64`, where numbers are stored on 64 bits
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双精度，或`float64`，其中数字存储在64位上
- en: You could even go up to `float8`, as you’ll see in a bit.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以提高到`float8`，稍后你将看到。
- en: The way to think about the resolution of floating-point numbers is in terms
    of the smallest distance between two arbitrary numbers that you’ll be able to
    safely process. In single precision, that’s around 1e-7\. In double precision,
    that’s around 1e-16\. And in half precision, it’s only 1e-3.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑浮点数的分辨率的方法是考虑两个任意数字之间你可以安全处理的最小距离。在单精度中，这大约是1e-7。在双精度中，这大约是1e-16。而在半精度中，只有1e-3。
- en: Float16 inference
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Float16推理
- en: 'Every model you’ve seen in this book so far has used single-precision numbers:
    it stored its state as `float32` weight variables and ran its computations on
    `float32` inputs. That’s enough precision to run the forward and backwards pass
    of a model without losing any information — in particular when it comes to small
    gradient updates (recall that the typically learning rate is 1e-3, and it’s pretty
    common to see weight updates on the order of 1e-6).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中你迄今为止看到的每一个模型都使用了单精度数字：它将状态存储为`float32`权重变量，并在`float32`输入上运行其计算。这种精度足以运行模型的正向和反向传播而不会丢失任何信息——特别是在涉及小的梯度更新时（回想一下，通常的学习率是1e-3，权重更新的量级通常为1e-6）。
- en: Modern GPUs and TPUs feature specialized hardware that can run 16-bit operations
    much faster and using less memory than equivalent 32-bit operations. By using
    these lower-precision operations whenever possible, you can speed up training
    on those devices by a significant factor. You can set the default floating point
    precision to `float16` in Keras via
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU和TPU具有专门的硬件，可以比等效的32位操作更快地运行16位操作，并且使用更少的内存。通过尽可能使用这些低精度操作，你可以显著加快这些设备上的训练速度。你可以在Keras中将默认的浮点精度设置为`float16`：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that this should be done before you define your model. Doing this will
    net you a nice speedup for model inference, for instance, via `model.predict()`.
    You should expect a nearly 2× speed boost on GPU and TPU.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这应该在定义你的模型之前完成。这样做将为模型推理（例如，通过`model.predict()`）带来很好的加速效果。你应该期望在GPU和TPU上获得近2倍的速度提升。
- en: 'There’s also an alternative to `float16` that works better on some devices,
    in particular TPUs: `bfloat16`. `bfloat16` is also a 16-bit precision floating-point
    type, but it differs from `float16` in its structure: it uses 8 exponent bits
    instead of 5, and 7 mantissa bits instead of 10 (see table 18.1). This means it
    can cover a much wider range of values, but it has a lower “resolution” over this
    range. Some devices are better optimized for `bfloat16` compared to `float16`,
    so it can be a good idea to try both before settling for the option that turns
    out to be the fastest.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些设备，特别是TPU，还有`float16`的替代方案：`bfloat16`。`bfloat16`也是一种16位精度的浮点数类型，但它与`float16`在结构上有所不同：它使用8位指数位而不是5位，7位尾数位而不是10位（见表18.1）。这意味着它可以覆盖更广泛的值范围，但在这个范围内的“分辨率”较低。与`float16`相比，一些设备对`bfloat16`进行了更好的优化，所以尝试两者之前选择最快的选项可能是个好主意。
- en: '| dtype | `float16` | `bfloat16` |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | `float16` | `bfloat16` |'
- en: '| --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Exponent bits | 5 | 8 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 指数位 | 5 | 8 |'
- en: '| Mantissa bits | 10 | 7 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 尾数位 | 10 | 7 |'
- en: '| Sign bits | 1 | 1 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 符号位 | 1 | 1 |'
- en: '[Table 18.1](#table-18-1): Difference between `float16` and `bfloat16`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[表18.1](#table-18-1)：`float16`和`bfloat16`之间的差异'
- en: Mixed-precision training
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: Setting your default float precision to 16 bits is a great way to speed up inference.
    Now, when it comes to training, there’s a significant complication. The gradient
    descent process wouldn’t run smoothly in `float16` or `bfloa16`, since we couldn’t
    represent small gradient updates of around 1e-5 or 1e-6, which are quite common.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将默认浮点精度设置为16位是加快推理速度的好方法。现在，当涉及到训练时，有一个显著的复杂性。梯度下降过程在`float16`或`bfloa16`中不会顺利运行，因为我们无法表示大约1e-5或1e-6的小梯度更新，这在实践中相当常见。
- en: 'You can, however, use a hybrid approach: that’s what *mixed-precision training*
    is about. The idea is to use 16-bit computation in places where precision isn’t
    an issue, while working with 32-bit values in other places to maintain numerical
    stability — in particular, when handling gradients and variable updates. By maintaining
    the precision-sensitive parts of the model in full precision, you can get most
    of the speed benefits of 16-bit computation without meaningfully impacting model
    quality.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以使用一种混合方法：这就是*混合精度训练*的含义。其思路是在精度不是问题的地方使用16位计算，而在其他地方使用32位值以保持数值稳定性——特别是在处理梯度变量更新时。通过在完全精度下保持模型中精度敏感的部分，你可以在不显著影响模型质量的情况下，获得16位计算的大部分速度优势。
- en: 'You can turn on mixed precision like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样开启混合精度：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Typically, most of the forward pass of the model will be done in `float16` (with
    the exception of numerically unstable operations like softmax), while the weights
    of the model will be stored and updated in `float32`. Your `float16` gradients
    will be cast to `float32` before updating the `float32` variables.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的正向传播的大部分操作将在`float16`（除了像softmax这样的数值不稳定操作）中完成，而模型的权重将存储和更新在`float32`中。你的`float16`梯度在更新`float32`变量之前将被转换为`float32`。
- en: Keras layers have a `variable_dtype` and a `compute_dtype` attribute. By default,
    both of these are set to `float32`. When you turn on mixed precision, the `compute_dtype`
    of most layers switches to `float16`. As a result, those layer will cast their
    inputs to `float16` and will perform their computation in `float16` (using half-precision
    copies of the weights). However, since their `variable_dtype` is still `float32`,
    their weights will be able to receive accurate `float32` updates from the optimizer,
    as opposed to half-precision updates.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Keras层有`variable_dtype`和`compute_dtype`属性。默认情况下，这两个都设置为`float32`。当你开启混合精度时，大多数层的`compute_dtype`将切换到`float16`。因此，这些层将把它们的输入转换为`float16`并在`float16`（使用半精度权重副本）中进行计算。然而，由于它们的`variable_dtype`仍然是`float32`，它们的权重将能够从优化器接收准确的`float32`更新，而不是半精度更新。
- en: Some operations may be numerically unstable in `float16` (in particular, softmax
    and crossentropy). If you need to opt out of mixed precision for a specific layer,
    just pass the argument `dtype="float32"` to the constructor of this layer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作在`float16`中可能数值不稳定（特别是softmax和交叉熵）。如果你需要为特定层退出混合精度，只需将`dtype="float32"`参数传递给该层的构造函数即可。
- en: Using loss scaling with mixed precision
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用混合精度进行损失缩放
- en: During training, gradients can become very small. When using mixed precision,
    your gradients remain in `float16` (same as the forward pass). As a result, the
    limited range of representable numbers can cause small gradients to be rounded
    down to zero. This prevents the model from learning effectively.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，梯度可能会变得非常小。当使用混合精度时，你的梯度保持在`float16`（与前向传递相同）。因此，可表示数字的有限范围可能导致小梯度被舍入为零。这会阻止模型有效地学习。
- en: Gradient values are proportional to the loss value, so to encourage gradients
    to be larger, a simple trick is to multiply the loss by a large scalar factor.
    Your gradients will then be much less likely to get rounded to zero.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度值与损失值成比例，因此为了鼓励梯度更大，一个简单的技巧是将损失乘以一个大的标量因子。你的梯度将不太可能被舍入为零。
- en: 'Keras makes this easy. If you want to use a fixed loss scaling factor, you
    can simply pass a `loss_scale_factor` argument to your optimizer like this:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使这变得简单。如果你想使用一个固定的损失缩放因子，你只需像这样将`loss_scale_factor`参数传递给你的优化器即可：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If you would like for the optimizer to automatically figure out the right scaling
    factor, you can also use the `LossScaleOptimizer` wrapper:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望优化器自动确定正确的缩放因子，你也可以使用`LossScaleOptimizer`包装器：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Using `LossScaleOptimizer` is usually your best option: the right scaling value
    can change over the course of training!'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`LossScaleOptimizer`通常是你的最佳选择：正确的缩放值可以在训练过程中改变！
- en: 'Beyond mixed precision: float8 training'
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超越混合精度：`float8`训练
- en: 'If running your forward pass in 16-bit precision yields such neat performance
    benefits, you might want to ask: Could we go even lower? What about 8-bit precision?
    Four bits, maybe? Two bits? The answer is, it’s complicated.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以16位精度运行前向传递会产生如此整洁的性能优势，你可能想知道：我们能更低吗？8位精度怎么样？四位，也许？两位？答案是，这很复杂。
- en: 'Mixed precision training using `float16` in the forward pass is that last level
    of precision that “just works” — `float16` precision has enough bits to represent
    all intermediate tensors (except for gradient updates, which is why we use `float32`
    for those). This is no longer true if you go down to `float8` precision: you are
    simply losing too much information. It is still possible to use `float8` in some
    computations, but this requires you to make considerable modifications to your
    forward pass. You will *not* be able to simply set your `compute_dtype` to `float8`
    and run.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`float16`进行前向传递的混合精度训练是“刚好工作”的最后一个精度级别——`float16`精度有足够的位来表示所有中间张量（除了梯度更新，这就是为什么我们为那些使用`float32`）。如果你降低到`float8`精度，这就不再成立了：你只是丢失了太多的信息。在某些计算中使用`float8`仍然是可能的，但这需要你对前向传递进行相当大的修改。你将**不能**简单地设置你的`compute_dtype`为`float8`并运行。
- en: 'The Keras framework provides a built-in implementation for `float8` training.
    Because it specifically targets Transformer use cases, it only covers a restricted
    set of layers: `Dense`, `EinsumDense` (the version of `Dense` that is used by
    the `MultiHeadAttention` layer), and `Embedding` layers. The way it works is not
    simple — it keeps track of past activation values to rescale activations at each
    step so as to utilize the full range of values representable in `float8`. It also
    needs to override part of the backward pass to do the same with gradient values.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Keras框架为`float8`训练提供了内置实现。因为它专门针对Transformer用例，所以它只覆盖了一组受限的层：`Dense`、`EinsumDense`（`MultiHeadAttention`层使用的`Dense`版本）和`Embedding`层。它的工作方式并不简单——它跟踪过去的激活值，以便在每一步重新缩放激活值，以便利用`float8`可表示值的全部范围。它还需要覆盖反向传播的一部分，以便对梯度值做同样的处理。
- en: Importantly, this added overhead has a computational cost. If your model is
    too small or if your GPU isn’t powerful enough, that cost will exceed the benefits
    of doing certain operations in `float8`, and you will see a slowdown instead of
    a speedup. `float8` training is only viable for very large models (typically over
    5B parameters) and large, recent GPUs such as the NVDIA H100\. `float8` is rarely
    used in practice, except in foundation model training runs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这种额外的开销有计算成本。如果你的模型太小或如果你的GPU不够强大，这种成本将超过在`float8`中执行某些操作的好处，你将看到减速而不是加速。`float8`训练仅适用于非常大的模型（通常超过50亿参数）和大型、最新的GPU，如NVIDIA
    H100。在实践中，`float8`很少使用，除了在基础模型训练运行中。
- en: Faster inference with quantization
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用量化的更快推理
- en: 'Running inference in `float16` — or even `float8` — will result in a nice speedup
    for your models. But there’s also another trick you can use: *`int8` quantization*.
    The big idea is to take an already trained model with weights in `float32` and
    convert these weights to a lower-precision dtype (typically `int8`) while preserving
    the numerical correctness of the forward pass as much as possible.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在`float16`——甚至`float8`——下运行推理将给你的模型带来不错的加速效果。但还有一个你可以使用的技巧：*`int8`量化*。主要思路是将一个已经训练好的、权重为`float32`的模型转换为更低精度的数据类型（通常是`int8`），同时尽可能保留前向传递的数值正确性。
- en: 'If you want to implement quantization from scratch, the math is simple: the
    general idea is to scale all `matmul` input tensors by a certain factor so that
    their coefficients fit in the range representable with `int8`, which is `[-127,
    127]` — a total of 256 possible values. After scaling the inputs, you cast them
    to `int8` and perform the `matmul` operation in `int8` precision, which should
    be quite a bit faster than `float16`. Finally, you cast the output back to `float32`,
    and you divide it by the product of the input scaling factors. Since `matmul`
    is a linear operation, this final unscaling cancels out the initial scaling, and
    you should get the same output as if you used the original values — any loss of
    accuracy only comes from the value rounding that happens when you cast the inputs
    to `int8`.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从头实现量化，数学原理很简单：一般思路是将所有`matmul`输入张量按某个因子缩放，使得它们的系数适合用`int8`表示的范围，即`[-127,
    127]`——总共256个可能的值。在缩放输入后，将它们转换为`int8`，并以`int8`精度执行`matmul`操作，这应该比`float16`快得多。最后，将输出转换回`float32`，并除以输入缩放因子的乘积。由于`matmul`是线性操作，最终的取消缩放会抵消初始缩放，你应该得到与使用原始值相同的结果——任何精度损失仅来自将输入转换为`int8`时的值舍入。
- en: 'Let’s make this concrete with an example. Let’s say you want to perform `matmul(x,
    kernel)`, with the following values:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来具体说明。假设你想执行`matmul(x, kernel)`，以下是一些值：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If you were to naively cast these values to `int8` without scaling first, that
    would be very destructive — for instance, your `x` would become `[[0, 0], [1,
    0]]`. So let’s apply the “abs-max” scaling scheme, which spreads out the values
    of each tensor across the `[-127, 127]` range:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你未经缩放就天真地将这些值转换为`int8`，那将会非常破坏性——例如，你的`x`会变成`[[0, 0], [1, 0]]`。所以让我们应用“绝对最大值”缩放方案，该方案将每个张量的值分散到`[-127,
    127]`范围内：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can perform a faster `matmul` and unscale the output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以执行更快的`matmul`操作并取消输出缩放：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'How accurate is it? Let’s compare our `y` with the output of the `float32`
    `matmul`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 它有多准确？让我们将我们的`y`与`float32` `matmul`的输出进行比较：
- en: '[PRE35]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Pretty accurate! For a large `matmul`, doing this will save you a lot of compute,
    since `int8` computation can be considerably faster than even `float16` computation,
    and you only had to add fairly fast elementwise ops to the computation graphs
    — `abs`, `max`, `clip`, `cast`, `divide`, `multiply`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 非常准确！对于大的`matmul`，这样做可以为你节省大量的计算，因为`int8`计算可以比`float16`计算快得多，而你只需向计算图中添加相对快速的逐元素操作——`abs`、`max`、`clip`、`cast`、`divide`、`multiply`。
- en: 'Now, of course, I don’t expect you to ever implement quantization by hand —
    that would be tremendously impractical. Similarly to `float8`, `int8` quantization
    is built directly into specific Keras layers: `Dense`, `EinsumDense`, and `Embedding`.
    This unlocks `int8` inference support for any Transformer-based model. Here’s
    how to use it with any Keras model that includes such layers:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当然，我不期望你手动实现量化——那将非常不切实际。与`float8`类似，`int8`量化直接集成到特定的Keras层中：`Dense`、`EinsumDense`和`Embedding`。这为基于Transformer的任何模型解锁了`int8`推理支持。以下是使用任何包含此类层的Keras模型的方法：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can use hyperparameter tuning and KerasTuner to automate the tedium out
    of finding the best model configuration. But be mindful of validation-set overfitting!
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用超参数调整和KerasTuner来自动化寻找最佳模型配置的繁琐工作。但要注意验证集过拟合！
- en: An ensemble of diverse models can often significantly improve the quality of
    your predictions.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组多样化的模型通常可以显著提高你预测的质量。
- en: To further scale your workflows, you can use *data parallelism* to train a model
    on multiple devices, as long as the model is small enough to fit on a single device.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了进一步扩展你的工作流程，你可以使用*数据并行*在多个设备上训练模型，只要模型足够小，可以适应单个设备。
- en: For larger models, you can also use *model parallelism* to split your model’s
    variables and computation across several devices.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更大的模型，您还可以使用*模型并行*来将您的模型变量和计算分散到多个设备上。
- en: You can speed up model training on GPUs or TPUs by turning on mixed precision
    — you’ll generally get a nice speed boost at virtually no cost.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过开启混合精度来加速在GPU或TPU上的模型训练——您通常可以在几乎不付出任何代价的情况下获得不错的速度提升。
- en: You can also speed up inference by using `float16` precision or even `int8`
    quantization.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您也可以通过使用`float16`精度或甚至`int8`量化来加速推理。
