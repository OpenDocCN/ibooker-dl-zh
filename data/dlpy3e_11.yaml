- en: Image segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter11_image-segmentation](https://deeplearningwithpython.io/chapters/chapter11_image-segmentation)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter11_image-segmentation](https://deeplearningwithpython.io/chapters/chapter11_image-segmentation)
- en: 'Chapter 8 gave you a first introduction to deep learning for computer vision
    via a simple use case: binary image classification. But there’s more to computer
    vision than image classification! This chapter dives deeper into another essential
    computer vision application — image segmentation.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章通过一个简单的用例——二值图像分类，首次介绍了计算机视觉中的深度学习。但计算机视觉不仅仅是图像分类！本章将进一步深入探讨另一个重要的计算机视觉应用——图像分割。
- en: Computer vision tasks
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉任务
- en: 'So far, we’ve focused on image classification models: an image goes in, a label
    comes out. “This image likely contains a cat; this other one likely contains a
    dog.” But image classification is only one of several possible applications of
    deep learning in computer vision. In general, there are three essential computer
    vision tasks you need to know about:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于图像分类模型：图像输入，标签输出。“这张图像可能包含一只猫；另一张可能包含一只狗。”但图像分类只是深度学习在计算机视觉中可能应用的几种可能性之一。一般来说，有三个基本的计算机视觉任务你需要了解：
- en: '*Image classification*, where the goal is to assign one or more labels to an
    image. It may be either single-label classification (meaning categories are mutually
    exclusive) or multilabel classification (tagging all categories that an image
    belongs to, as shown in figure 11.1). For example, when you search for a keyword
    on the Google Photos app, behind the scenes you’re querying a very large multilabel
    classification model — one with over 20,000 different classes, trained on millions
    of images.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分类*，其目标是给图像分配一个或多个标签。这可能是单标签分类（意味着类别是互斥的）或多标签分类（标记图像所属的所有类别，如图11.1所示）。例如，当你在Google
    Photos应用中搜索关键词时，在幕后你正在查询一个非常大的多标签分类模型——一个拥有超过20,000个不同类别，并在数百万张图像上训练的模型。'
- en: '*Image segmentation*, where the goal is to “segment” or “partition” an image
    into different areas, with each area usually representing a category (as shown
    in figure 11.1). For instance, when Zoom or Google Meet displays a custom background
    behind you in a video call, it’s using an image segmentation model to distinguish
    your face from what’s behind it, with pixel-level precision.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分割*，其目标是“分割”或“划分”图像为不同的区域，每个区域通常代表一个类别（如图11.1所示）。例如，当Zoom或Google Meet在视频通话中显示你背后的自定义背景时，它正在使用图像分割模型以像素级的精度区分你的面部和其后的内容。'
- en: '*Object detection*, where the goal is to draw rectangles (called *bounding
    boxes*) around objects of interest in an image and associate each rectangle with
    a class. A self-driving car could use an object detection model to monitor cars,
    pedestrians, and signs in view of its cameras, for instance.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标检测*，其目标是围绕图像中感兴趣的对象绘制矩形（称为*边界框*），并将每个矩形与一个类别关联。例如，自动驾驶汽车可以使用目标检测模型来监控其摄像头视野中的车辆、行人和标志。'
- en: '![](../Images/2045275007f5b1bc39eac7d6965c23da.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/2045275007f5b1bc39eac7d6965c23da.png)'
- en: '[Figure 11.1](#figure-11-1): The three main computer vision tasks: classification,
    segmentation, and detection'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1](#figure-11-1)：三种主要的计算机视觉任务：分类、分割和检测'
- en: Deep learning for computer vision also encompasses a number of somewhat more
    niche tasks besides these three, such as image similarity scoring (estimating
    how visually similar two images are), keypoint detection (pinpointing attributes
    of interest in an image, such as facial features), pose estimation, 3D mesh estimation,
    depth estimation, and so on. But to start with, image classification, image segmentation,
    and object detection form the foundation that every machine learning engineer
    should be familiar with. Almost all computer vision applications boil down to
    one of these three.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三个任务之外，计算机视觉的深度学习还包括一些相对较窄的任务，例如图像相似度评分（估计两张图像在视觉上的相似程度）、关键点检测（在图像中定位感兴趣的特征，如面部特征）、姿态估计、3D网格估计、深度估计等等。但首先，图像分类、图像分割和目标检测构成了每个机器学习工程师都应该熟悉的基石。几乎所有的计算机视觉应用都可以归结为这三个中的某一个。
- en: You’ve seen image classification in action in Chapter 8. Next, let’s dive into
    image segmentation. It’s a very useful and very versatile technique, and you can
    straightforwardly approach it with what you’ve already learned so far. Then, in
    the next chapter, you’ll learn about object detection in detail.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第八章中已经看到了图像分类的实际应用。接下来，让我们深入探讨图像分割。这是一个非常有用且非常通用的技术，你可以直接运用你到目前为止所学到的知识来接近它。然后，在下一章中，你将详细了解目标检测。
- en: Types of image segmentation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像分割类型
- en: Image segmentation with deep learning is about using a model to assign a class
    to each pixel in an image, thus *segmenting* the image into different zones (such
    as “background” and “foreground” or “road,” “car,” and “sidewalk”). This general
    category of techniques can be used to power a considerable variety of valuable
    applications in image and video editing, autonomous driving, robotics, medical
    imaging, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行图像分割是关于使用模型为图像中的每个像素分配一个类别，从而将图像分割成不同的区域（如“背景”和“前景”或“道路”、“汽车”和“人行道”）。这类技术可以用于支持图像和视频编辑、自动驾驶、机器人技术、医学成像等多种有价值的应用。
- en: 'There are three different flavors of image segmentation that you should know
    about:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该了解三种不同的图像分割类型：
- en: '*Semantic segmentation*, where each pixel is independently classified into
    a semantic category, like “cat.” If there are two cats in the image, the corresponding
    pixels are all mapped to the same generic “cat” category (see figure 11.2).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义分割*，其中每个像素都被独立地分类到语义类别，如“猫”。如果图像中有两只猫，相应的像素都将映射到相同的通用“猫”类别（见图11.2）。'
- en: '*Instance segmentation*, which seeks to parse out individual object instances.
    In an image with two cats in it, instance segmentation would distinguish between
    pixels belonging to “cat 1” and pixels belonging to “cat 2” (see figure 11.2).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例分割*，旨在解析出单个对象实例。在一个有两只猫的图像中，实例分割将区分属于“猫1”的像素和属于“猫2”的像素（见图11.2）。'
- en: '*Panoptic segmentation*, which combines semantic segmentation and instance
    segmentation by assigning to each pixel in an image both a semantic label (like
    “cat”) and an instance label (like “cat 2”). This is the most informative of all
    three segmentation types.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全景分割*，通过为图像中的每个像素分配语义标签（如“猫”）和实例标签（如“猫2”）来结合语义分割和实例分割。这是三种分割类型中最具信息量的。'
- en: '![](../Images/7fd2d264b2190f8be9fac2be63b59e45.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7fd2d264b2190f8be9fac2be63b59e45.png)'
- en: '[Figure 11.2](#figure-11-2): Semantic segmentation vs. instance segmentation'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.2](#figure-11-2)：语义分割与实例分割的比较'
- en: To get more familiar with segmentation, let’s get started with training a small
    segmentation model from scratch on your own data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更熟悉分割，让我们从从头开始在您自己的数据上训练一个小型分割模型开始。
- en: Training a segmentation model from scratch
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始训练分割模型
- en: In this first example, we’ll focus on semantic segmentation. We’ll be looking
    once again at images of cats and dogs, and this time we’ll be learning to tell
    apart the main subject and its background.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个例子中，我们将专注于语义分割。我们将再次查看猫和狗的图像，这次我们将学习区分主要主题及其背景。
- en: Downloading a segmentation dataset
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载分割数据集
- en: 'We’ll work with the Oxford-IIIT Pets dataset ([https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)),
    which contains 7,390 pictures of various breeds of cats and dogs, together with
    foreground-background *segmentation masks* for each picture. A segmentation mask
    is the image segmentation equivalent of a label: it’s an image the same size as
    the input image, with a single color channel where each integer value corresponds
    to the class of the corresponding pixel in the input image. In our case, the pixels
    of our segmentation masks can take one of three integer values:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用牛津-IIIT宠物数据集（[https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)），该数据集包含7,390张各种品种的猫和狗的图片，以及每张图片的前景-背景*分割掩码*。分割掩码是图像分割的等价物：它是一个与输入图像大小相同的图像，具有单个颜色通道，其中每个整数值对应于输入图像中相应像素的类别。在我们的情况下，我们的分割掩码的像素可以取三个整数值之一：
- en: 1 (foreground)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 (前景)
- en: 2 (background)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 (背景)
- en: 3 (contour)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 (轮廓)
- en: 'Let’s start by downloading and uncompressing our dataset, using the `wget`
    and `tar` shell utilities:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过使用`wget`和`tar`shell工具下载和解压我们的数据集：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input pictures are stored as JPG files in the `images/` folder (such as
    `images/Abyssinian_1.jpg`), and the corresponding segmentation mask is stored
    as a PNG file with the same name in the `annotations/trimaps/` folder (such as
    `annotations/trimaps/Abyssinian_1.png`).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图片存储在`images/`文件夹中作为JPG文件（例如`images/Abyssinian_1.jpg`），相应的分割掩码存储在`annotations/trimaps/`文件夹中，文件名与图片相同，为PNG文件（例如`annotations/trimaps/Abyssinian_1.png`）。
- en: 'Let’s prepare the list of input file paths, as well as the list of the corresponding
    mask file paths:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准备输入文件路径的列表，以及相应的掩码文件路径列表：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, what does one of these inputs and its mask look like? Let’s take a quick
    look (see figure 11.3).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些输入及其掩码看起来是什么样子？让我们快速看一下（见图11.3）。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/539e2f264128e5b1ac6b451aa3d2e9a4.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/539e2f264128e5b1ac6b451aa3d2e9a4.png)'
- en: '[Figure 11.3](#figure-11-3): An example image'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.3](#figure-11-3)：一个示例图像'
- en: 'Let’s look at its target mask as well (see figure 11.4):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的目标掩码（见图11.4）：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2686b58935fc0c0e5d28d781497c7142.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2686b58935fc0c0e5d28d781497c7142.png)'
- en: '[Figure 11.4](#figure-11-4): The corresponding target mask'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.4](#figure-11-4)：相应的目标掩码'
- en: 'Next, let’s load our inputs and targets into two NumPy arrays. Since the dataset
    is very small, we can load everything into memory:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将我们的输入和目标加载到两个NumPy数组中。由于数据集非常小，我们可以将所有内容加载到内存中：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As always, let’s split the arrays into a training and a validation set:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们将数组分成训练集和验证集：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Building and training the segmentation model
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建和训练分割模型
- en: 'Now, it’s time to define our model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候定义我们的模型了：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first half of the model closely resembles the kind of ConvNet you’d use
    for image classification: a stack of `Conv2D` layers, with gradually increasing
    filter sizes. We downsample our images three times by a factor of two each — ending
    up with activations of size `(25, 25, 256)`. The purpose of this first half is
    to encode the images into smaller feature maps, where each spatial location (or
    “pixel”) contains information about a large spatial chunk of the original image.
    You can understand it as a kind of compression.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前半部分与您用于图像分类的ConvNet非常相似：一系列`Conv2D`层，滤波器大小逐渐增加。我们通过每次减半的因子将图像下采样三次——最终得到大小为`(25,
    25, 256)`的激活。这一半的目的是将图像编码成较小的特征图，其中每个空间位置（或“像素”）包含有关原始图像中较大空间块的信息。你可以将其理解为一种压缩。
- en: 'One important difference between the first half of this model and the classification
    models you’ve seen before is the way we do downsampling: in the classification
    ConvNets from chapter 8, we used `MaxPooling2D` layers to downsample feature maps.
    Here, we downsample by adding *strides* to every other convolution layer (if you
    don’t remember the details of how convolution strides work, see chapter 8, section
    8.1.1). We do this because, in the case of image segmentation, we care a lot about
    the spatial location of information in the image since we need to produce per-pixel
    target masks as output of the model. When you do 2 × 2 max pooling, you are completely
    destroying location information within each pooling window: you return one scalar
    value per window, with zero knowledge of which of the four locations in the windows
    the value came from.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的前半部分与您之前看到的分类模型的一个重要区别在于我们进行下采样的方式：在第8章的分类ConvNets中，我们使用了`MaxPooling2D`层来下采样特征图。在这里，我们通过在每个卷积层中添加*步长*来下采样（如果你不记得卷积步长的细节，请参阅第8章的8.1.1节）。我们这样做是因为，在图像分割的情况下，我们非常关注图像中信息的空间位置，因为我们需要将每个像素的目标掩码作为模型的输出。当你进行2×2最大池化时，你完全破坏了每个池化窗口内的位置信息：你为每个窗口返回一个标量值，而对四个位置中的哪一个值来自窗口一无所知。
- en: So, while max pooling layers perform well for classification tasks, they would
    hurt us quite a bit for a segmentation task. Meanwhile, strided convolutions do
    a better job at downsampling feature maps while retaining location information.
    Throughout this book, you’ll notice that we tend to use strides instead of max
    pooling in any model that cares about feature location, such as the generative
    models in chapter 17.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然最大池化层在分类任务中表现良好，但对于分割任务，它们可能会给我们带来相当大的伤害。同时，步长卷积在降采样特征图的同时，更好地保留了位置信息。在这本书的整个过程中，你会发现我们倾向于在关注特征位置的任何模型中使用步长而不是最大池化，例如第17章中的生成模型。
- en: 'The second half of the model is a stack of `Conv2DTranspose` layers. What are
    those? Well, the output of the first half of the model is a feature map of shape
    `(25, 25, 256)`, but we want our final output to predict a class for each pixel,
    matching the original spatial dimensions. The final model output will have shape
    `(200, 200, num_classes)`, which is `(200, 200, 3)` here. Therefore, we need to
    apply a kind of *inverse* of the transformations we’ve applied so far, something
    that will *upsample* the feature maps instead of downsampling them. That’s the
    purpose of the `Conv2DTranspose` layer: you can think of it as a kind of convolution
    layer that *learns to upsample*. If you have an input of shape `(100, 100, 64)`
    and you run it through the layer `Conv2D(128, 3, strides=2, padding="same")`,
    you get an output of shape `(50, 50, 128)`. If you run this output through the
    layer `Conv2DTranspose(64, 3, strides=2, padding="same")`, you get back an output
    of shape `(100, 100, 64)`, the same as the original. So after compressing our
    inputs into feature maps of shape `(25, 25, 256)` via a stack of `Conv2D` layers,
    we can simply apply the corresponding sequence of `Conv2DTranspose` layers followed
    by a final `Conv2D` layer to produce outputs of shape `(200, 200, 3)`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的后半部分是一系列 `Conv2DTranspose` 层。那些是什么？嗯，模型前半部分的输出是一个形状为 `(25, 25, 256)` 的特征图，但我们希望最终的输出为每个像素预测一个类别，匹配原始的空间维度。最终的模型输出将具有形状
    `(200, 200, num_classes)`，在这里是 `(200, 200, 3)`。因此，我们需要应用一种 *逆* 变换，即 *上采样* 特征图而不是下采样它们。这就是
    `Conv2DTranspose` 层的目的：你可以把它想象成一种 *学习上采样* 的卷积层。如果你有一个形状为 `(100, 100, 64)` 的输入，并且通过
    `Conv2D(128, 3, strides=2, padding="same")` 层运行它，你会得到一个形状为 `(50, 50, 128)` 的输出。如果你将这个输出通过
    `Conv2DTranspose(64, 3, strides=2, padding="same")` 层运行，你会得到一个形状为 `(100, 100,
    64)` 的输出，与原始输入相同。因此，通过一系列 `Conv2D` 层将我们的输入压缩成形状为 `(25, 25, 256)` 的特征图后，我们可以简单地应用相应的
    `Conv2DTranspose` 层序列，然后是一个最终的 `Conv2D` 层，以产生形状为 `(200, 200, 3)` 的输出。
- en: 'To evaluate the model, we’ll use a metric named *Intersection over Union* (IoU).
    It’s a measure of the match between the ground truth segmentation masks and the
    predicted masks. It can be computed separately for each class or averaged over
    multiple classes. Here’s how it works:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型，我们将使用一个名为 *交并比* (IoU) 的指标。它是真实分割掩码与预测掩码之间匹配程度的度量。它可以针对每个类别单独计算，也可以在多个类别上平均计算。以下是它是如何工作的：
- en: Compute the *intersection* between the masks, the area where the prediction
    and ground truth overlap.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算掩码之间的 *交集*，即预测和真实重叠的区域。
- en: Compute the *union* of the masks, the total area covered by both masks combined.
    This is the whole space we’re interested in — the target object and any extra
    bits your model might have included by mistake.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算掩码的 *并集*，即两个掩码共同覆盖的总区域。这是我们感兴趣的全部空间——目标对象以及你的模型可能错误包含的任何额外部分。
- en: Divide the intersection area by the union area to get the IoU. It’s a number
    between 0 and 1, where 1 denotes a perfect match, and 0 denotes a complete miss.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将交集区域除以并集区域以获得 IoU。它是一个介于 0 和 1 之间的数字，其中 1 表示完美匹配，0 表示完全未命中。
- en: 'We can simply use a built-in Keras metric rather than building this ourselves:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用内置的 Keras 指标，而不是自己构建：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now compile and fit our model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编译和拟合我们的模型：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s display our training and validation loss (see figure 11.5):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示我们的训练和验证损失（见图 11.5）：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/9f64353f124760b76a5383248fac2a7f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f64353f124760b76a5383248fac2a7f.png)'
- en: '[Figure 11.5](#figure-11-5): Displaying training and validation loss curves'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.5](#figure-11-5)：显示训练和验证损失曲线'
- en: 'You can see that we start overfitting midway, around epoch 25\. Let’s reload
    our best-performing model according to validation loss and demonstrate how to
    use it to predict a segmentation mask (see figure 11.6):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们在大约第 25 个 epoch 时开始过拟合。让我们根据验证损失重新加载我们表现最好的模型，并展示如何使用它来预测一个分割掩码（见图 11.6）：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/5df726de6b0d2524f74f492e8d42c2d1.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5df726de6b0d2524f74f492e8d42c2d1.png)'
- en: '[Figure 11.6](#figure-11-6): A test image and its predicted segmentation mask'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.6](#figure-11-6)：一个测试图像及其预测的分割掩码'
- en: There are a couple of small artifacts in our predicted mask, caused by geometric
    shapes in the foreground and background. Nevertheless, our model appears to work
    nicely.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测掩码中存在一些小的伪影，这是由前景和背景中的几何形状造成的。尽管如此，我们的模型看起来工作得很好。
- en: Using a pretrained segmentation model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的分割模型
- en: In the image classification example from chapter 8, you saw how using a pretrained
    model could significantly boost your accuracy — especially when you only have
    a few samples to train on. Image segmentation is no different.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章的图像分类示例中，你看到了使用预训练模型如何显著提高你的准确率——尤其是在你只有少量样本进行训练时。图像分割也不例外。
- en: The *Segment Anything Model*,^([[1]](#footnote-1)) or SAM for short, is a powerful
    pretrained segmentation model you can use for, well, almost anything. It was developed
    by Meta AI and released in April 2023\. It was trained on 11 million images and
    their segmentation masks, covering over 1 billion object instances. This massive
    amount of training data provides the model with built-in knowledge of virtually
    any object that appears in natural images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*Segment Anything模型*，或简称SAM，是一个强大的预训练分割模型，你可以用它来做几乎所有的事情。它由Meta AI开发并于2023年4月发布。它是在1100万张图像及其分割掩码上训练的，覆盖了超过10亿个对象实例。如此大量的训练数据为模型提供了对自然图像中几乎任何出现的对象的内置知识。'
- en: The main innovation of SAM is that it’s not limited to a predefined set of object
    classes. You can use it for segmenting new objects simply by providing an example
    of what you’re looking for. You don’t even need to fine-tune the model first.
    Let’s see how that works.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SAM的主要创新在于它不仅限于预定义的对象类别集合。你可以通过提供一个你正在寻找的示例来简单地用它来分割新的对象。你甚至不需要先微调模型。让我们看看它是如何工作的。
- en: Downloading the Segment Anything Model
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载Segment Anything模型
- en: First, let’s instantiate SAM and download its weights. Once again, we can use
    the KerasHub package to use this pretrained model without needing to implement
    it ourselves from scratch.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化SAM并下载其权重。同样，我们可以使用KerasHub包来使用这个预训练模型，而无需从头开始实现它。
- en: 'Remember the `ImageClassifier` task we used in the previous chapter? We can
    use another KerasHub task `ImageSegmenter` for wrapping pretrained image segmentation
    models into a high-level model with standard inputs and outputs. Here, we’ll use
    the `sam_huge_sa1b` pretrained model, where `sam` stands for the model, `huge`
    refers to the number of parameters in the model, and `sa1b` stands for the SA-1B
    dataset released along with the model, with 1 billion annotated masks. Let’s download
    it now:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在上一章中使用的`ImageClassifier`任务吗？我们可以使用另一个KerasHub任务`ImageSegmenter`来将预训练的图像分割模型包装成一个具有标准输入和输出的高级模型。在这里，我们将使用`sam_huge_sa1b`预训练模型，其中`sam`代表模型，`huge`指的是模型中的参数数量，而`sa1b`代表与模型一起发布的SA-1B数据集，包含10亿个注释过的掩码。现在让我们下载它：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'One thing we can note off the bat is that our model is, indeed, huge:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即注意到的是，我们的模型确实是巨大的：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At 641 million parameters, SAM is the largest model we have used so far in this
    book. The trend of pretrained models getting larger and larger and using more
    and more data will be discussed in more detail in chapter 16.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在6410万个参数的情况下，SAM是我们在这本书中使用的最大的模型。预训练模型越来越大，使用的数据越来越多这一趋势将在第16章中更详细地讨论。
- en: How Segment Anything works
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Segment Anything是如何工作的
- en: Before we try running some segmentation with the model, let’s talk a little
    more about how SAM works. Much of the capability of the model comes from the scale
    of the pretraining dataset. Meta developed the SA-1B dataset along with the model,
    where the partially trained model was used to assist with the data labeling process.
    That is, the dataset and model were developed together in a feedback loop of sorts.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试使用该模型进行一些分割之前，让我们更多地谈谈SAM是如何工作的。模型的大部分能力都来自于预训练数据集的规模。Meta与模型一起开发了SA-1B数据集，其中部分训练的模型被用来辅助数据标注过程。也就是说，数据集和模型以一种反馈循环的方式共同开发。
- en: The goal with the SA-1B dataset is to create fully segmented images, where every
    object in an image is given a unique segmentation mask. See figure 11.7 as an
    example. Each image in the dataset has ~100 masks on average, and some images
    have over 500 individually masked objects. This was done through a pipeline of
    increasingly automated data collection. At first, human experts manually segmented
    a small example dataset of images, which was used to train an initial model. This
    model was used to help drive a semiautomated stage of data collection, where images
    were first segmented by SAM and improved by human correction and further annotation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SA-1B数据集的目标是创建完全分割的图像，其中图像中的每个对象都分配了一个唯一的分割遮罩。见图11.7作为示例。数据集中的每张图像平均有约100个遮罩，有些图像有超过500个单独遮罩的对象。这是通过一个越来越自动化的数据收集流程完成的。最初，人类专家手动分割了一个小型的图像示例数据集，该数据集用于训练初始模型。该模型被用来帮助推动数据收集的半自动化阶段，在这一阶段，图像首先由SAM分割，然后通过人工校正和进一步标注进行改进。
- en: '![](../Images/f7fee6b67ad6bf846f8c26fdfbde4e3e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/f7fee6b67ad6bf846f8c26fdfbde4e3e.png)'
- en: '[Figure 11.7](#figure-11-7): An example image from the SA-1B dataset'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.7](#figure-11-7)：SA-1B数据集的一个示例图像'
- en: 'The model is trained on `(image, prompt, mask)` triples. `image` and `prompt`
    are the model inputs. The image can be any input image, and the prompt can take
    a couple of forms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在`(图像, 提示, 遮罩)`三元组上进行训练。`图像`和`提示`是模型的输入。图像可以是任何输入图像，而提示可以采取几种形式：
- en: A point inside the object to mask
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮罩对象内部的一个点
- en: A box around the object to mask
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 围绕遮罩对象的一个框
- en: Given the `image` and `prompt` input, the model is expected to produce an accurate
    predicted mask for the object indicated by the prompt, which is compared with
    a ground truth `mask` label.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 给定`图像`和`提示`输入，模型预计将产生一个准确的预测遮罩，该遮罩对应于提示中指示的对象，并将其与地面真实`遮罩`标签进行比较。
- en: The model consists of a few separate components. An image encoder similar to
    the Xception model we used in previous chapters, will take an input image and
    output a much smaller image embedding. This is something we already know how to
    build.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由几个独立的组件组成。一个类似于我们在前几章中使用的Xception模型的图像编码器，将输入图像转换为更小的图像嵌入。这是我们已知如何构建的。
- en: Next, we add a prompt encoder, which is responsible for mapping prompts in any
    of the previously mentioned forms to an embedded vector, and a mask decoder, which
    takes in both the image embedding and prompt embedding and outputs a few possible
    predicted masks. We won’t get into the details of the prompt encoder and mask
    decoder here, as they use some modeling techniques we won’t see until later chapters.
    We can compare these predicted masks with our ground truth mask much like we did
    in the earlier section of this chapter (see figure 11.8).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个提示编码器，它负责将之前提到的任何形式的提示映射到一个嵌入向量，以及一个遮罩解码器，它接收图像嵌入和提示嵌入，并输出几个可能的预测遮罩。我们不会在这里详细介绍提示编码器和遮罩解码器的细节，因为它们使用了我们在后面的章节中才会看到的建模技术。我们可以将这些预测遮罩与我们的地面真实遮罩进行比较，就像我们在本章早期部分所做的那样（见图11.8）。
- en: '![](../Images/02a451fa68c4d16b5f76ddfbd8d7e7e4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02a451fa68c4d16b5f76ddfbd8d7e7e4.png)'
- en: '[Figure 11.8](#figure-11-8): The Segment Anything high-level architecture overview'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.8](#figure-11-8)：Segment Anything高级架构概述'
- en: All of these subcomponents are trained simultaneously by forming batches of
    new `(image, prompt, mask)` triples to train on from the SA-1B image and mask
    data. The process here is actually quite simple. For a given input image, choose
    a random mask in the input. Next, randomly choose whether to create a box prompt
    or a point prompt. To create a point prompt, choose a random pixel inside the
    mask label. To create a box prompt, draw a box around all points inside the mask
    label. We can repeat this process indefinitely, sampling a number of `(image,
    prompt, mask)` triples from each image input.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些子组件都是通过形成新的`(图像, 提示, 遮罩)`三元组批次来同时训练的，这些批次是从SA-1B图像和遮罩数据中训练的。这里的流程实际上相当简单。对于给定的输入图像，选择输入中的一个随机遮罩。接下来，随机选择是否创建一个框提示或一个点提示。要创建一个点提示，选择遮罩标签内的一个随机像素。要创建一个框提示，围绕遮罩标签内的所有点绘制一个框。我们可以无限重复这个过程，从每个图像输入中采样一定数量的`(图像,
    提示, 遮罩)`三元组。
- en: Preparing a test image
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备测试图像
- en: 'Let’s make this a little more concrete by trying the model out. We can start
    by loading a test image for our segmentation work. We’ll use a picture of a bowl
    of fruits (see figure 11.9):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过尝试模型来使这个例子更具体。我们可以从加载用于分割工作的测试图像开始。我们将使用一个水果碗的图片（见图11.9）：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/cb3ddcfe0d2fd3d54cc924ac0bb4ea5c.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/cb3ddcfe0d2fd3d54cc924ac0bb4ea5c.png)'
- en: '[Figure 11.9](#figure-11-9): Our test image'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.9](#figure-11-9)：我们的测试图像'
- en: 'SAM expects inputs that are 1024 × 1024\. However, forcibly resizing arbitrary
    images to 1024 × 1024 would distort their aspect ratio — for instance, our image
    isn’t square. It’s better to first resize the image so that its longest side becomes
    1,024 pixels long and then pad the remaining pixels with a filler value, such
    as 0\. We can achieve this with the `pad_to_aspect_ratio` argument in the `keras.ops.image.resize()`
    operation, like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 预期输入的尺寸为 1024 × 1024。然而，强制将任意图像调整到 1024 × 1024 的大小会扭曲其宽高比——例如，我们的图像不是正方形。更好的做法是首先将图像调整到其最长边为
    1,024 像素，然后用填充值（如 0）填充剩余的像素。我们可以通过在 `keras.ops.image.resize()` 操作中使用 `pad_to_aspect_ratio`
    参数来实现这一点，如下所示：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, let’s define a few utilities that will come in handy when using the model.
    We’re going to need to
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一些在使用模型时将很有用的实用工具。我们需要做的是
- en: Display images.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示图像。
- en: Display segmentation masks overlaid on an image.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像上显示叠加的分割掩码。
- en: Highlight specific points on an image.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像上突出显示特定的点。
- en: Display boxes overlaid on an image.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像上显示叠加的框。
- en: 'All our utilities take a Matplotlib `axis` object (noted `ax`) so that they
    can all write to the same figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有工具都接受一个 Matplotlib `axis` 对象（记作 `ax`），这样它们就可以写入同一个图像：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Prompting the model with a target point
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用目标点提示模型
- en: 'To use SAM, you need to prompt it. This means we need one of the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 SAM，你需要提示它。这意味着我们需要以下之一：
- en: '*Point prompts* — Select a point in an image and let the model segment the
    object that the point belongs to.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*点提示*——在图像中选择一个点，并让模型分割该点所属的对象。'
- en: '*Box prompts* — Draw an approximate box around an object (it does not need
    to be particularly precise) and let the model segment the object in the box.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*框提示*——在对象周围画一个大致的框（不需要特别精确），然后让模型在框内分割对象。'
- en: Let’s start with a point prompt. Points are labeled, with 1 indicating the foreground
    (the object you want to segment) and 0 indicating the background (everything around
    the object). In ambiguous cases, to improve your results, you could pass multiple
    labeled points, instead of a single point, to refine your definition of what should
    be included (points labeled 1) and what should be excluded (points labeled 0).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从点提示开始。点被标记，其中 1 表示前景（你想要分割的对象），0 表示背景（对象周围的一切）。在模糊的情况下，为了提高你的结果，你可以传递多个标记的点，而不是单个点，以细化你想要包含（标记为
    1 的点）和排除（标记为 0 的点）的定义。
- en: 'We try a single foreground point (see figure 11.10). Here’s a test point:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试一个单独的前景点（见图 11.10）。这是一个测试点：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/1fb3211965623f1112b05761214c02f1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fb3211965623f1112b05761214c02f1.png)'
- en: '[Figure 11.10](#figure-11-10): A prompt point, landing on a peach'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.10](#figure-11-10)：一个提示点，落在桃子上'
- en: 'Let’s prompt SAM with it:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个图像提示 SAM：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The return value `outputs` has a `"masks"` field which contains four 256 ×
    256 candidate masks for the target object, ranked by decreasing match quality.
    The quality scores of the masks are available under the `"iou_pred"` field as
    part of the model’s output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值 `outputs` 有一个 `"masks"` 字段，它包含四个 256 × 256 的候选掩码，按降低的匹配质量排序。掩码的质量分数作为模型输出的
    `"iou_pred"` 字段的一部分提供：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s overlay the first mask on the image (see figure 11.11):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在图像上叠加第一个掩码（见图 11.11）：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/9a04a54d693d52d5d46a0779a32ef31d.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a04a54d693d52d5d46a0779a32ef31d.png)'
- en: '[Figure 11.11](#figure-11-11): Segmented peach'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.11](#figure-11-11)：分割的桃子'
- en: Pretty good!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 很不错！
- en: 'Next, let’s try a banana. We’ll prompt the model with coordinates `(300, 550)`,
    which land on the second banana from the left (see figure 11.12):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试一个香蕉。我们将用坐标 `(300, 550)` 来提示模型，这个坐标落在从左数第二个香蕉上（见图 11.12）：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/d57b293d144b43710f13bcb5dc407813.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d57b293d144b43710f13bcb5dc407813.png)'
- en: '[Figure 11.12](#figure-11-12): Segmented banana'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.12](#figure-11-12)：分割的香蕉'
- en: 'Now, what about the other mask candidates? Those can come in handy for ambiguous
    prompts. Let’s try to plot the other three masks (see figure 11.13):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于其他掩码候选者呢？它们对于模糊提示很有用。让我们尝试绘制其他三个掩码（见图 11.13）：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/2825d36b86ac06b70d4c188bdeeb0383.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2825d36b86ac06b70d4c188bdeeb0383.png)'
- en: '[Figure 11.13](#figure-11-13): Alternative segmentation masks for the banana
    prompt'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.13](#figure-11-13)：香蕉提示的替代分割掩码'
- en: As you can see here, an alternative segmentation found by the model includes
    both bananas.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型找到的替代分割包括两个香蕉。
- en: Prompting the model with a target box
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用目标框提示模型
- en: 'Besides providing one or more target points, you can also provide boxes approximating
    the location of the object to segment. These boxes should be passed via the coordinates
    of their top-left and bottom-right corners. Here’s a box around the mango (see
    figure 11.14):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供一个或多个目标点之外，您还可以提供近似分割对象位置的框。这些框应通过其左上角和右下角的坐标传递。这里有一个围绕芒果的框（见图11.14）：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/611f443ab648ad48cdc642f1c265fd6e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/611f443ab648ad48cdc642f1c265fd6e.png)'
- en: '[Figure 11.14](#figure-11-14): Box prompt around the mango'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.14](#figure-11-14)：围绕芒果的框提示'
- en: 'Let’s prompt SAM with it (see figure 11.15):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用它来提示SAM（见图11.15）：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/afe8dd8fc5b03bf9c605eb1d054321d6.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/afe8dd8fc5b03bf9c605eb1d054321d6.png)'
- en: '[Figure 11.15](#figure-11-15): Segmented mango'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.15](#figure-11-15)：分割的芒果'
- en: SAM can be a powerful tool to quickly create large datasets of images annotated
    with segmentation masks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: SAM可以是一个强大的工具，快速创建带有分割掩码的大图像数据集。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Image segmentation is one of the main categories of computer vision tasks. It
    consists of computing segmentation masks that describe the contents of an image
    at the pixel level.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割是计算机视觉任务的主要类别之一。它包括计算分割掩码，这些掩码描述了图像在像素级别的内容。
- en: To build your own segmentation model, use a stack of strided `Conv2D` layers
    to “compress” the input image into a smaller feature map, followed by a stack
    of corresponding `Conv2DTranspose` layers to “expand” the feature map into a segmentation
    mask the same size as the input image.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要构建自己的分割模型，使用一系列步进`Conv2D`层来“压缩”输入图像到一个较小的特征图，然后使用相应的`Conv2DTranspose`层堆叠来“扩展”特征图，使其大小与输入图像相同的分割掩码。
- en: You can also use a pretrained segmentation model. Segment Anything, included
    in KerasHub, is a powerful model that supports image prompting, text prompting,
    point prompting, and box prompting.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以使用预训练的分割模型。KerasHub中包含的Segment Anything是一个支持图像提示、文本提示、点提示和框提示的强大模型。
- en: Footnotes
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Kirillov et al., “Segment Anything,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, arXiv (2023), [https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643).
    [[↩]](#footnote-link-1)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kirillov等人，“Segment Anything”，在*IEEE/CVF国际计算机视觉会议论文集*，arXiv (2023)，[https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643)。[[↩]](#footnote-link-1)
