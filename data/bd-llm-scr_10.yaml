- en: appendix D Adding bells and whistles to the training loop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录D：为训练循环添加铃声和哨声
- en: In this appendix, we enhance the training function for the pretraining and fine-tuning
    processes covered in chapters 5 to 7\. In particular, it covers *learning rate
    warmup*, *cosine decay*, and *gradient clipping*. We then incorporate these techniques
    into the training function and pretrain an LLM.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个附录中，我们增强了第5到7章中涵盖的预训练和微调过程的训练函数。特别是，它涵盖了*学习率预热*、*余弦衰减*和*梯度裁剪*。然后我们将这些技术纳入训练函数并预训练一个LLM。
- en: 'To make the code self-contained, we reinitialize the model we trained in chapter
    5:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码自包含，我们重新初始化在第5章中训练的模型：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Vocabulary size'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 词汇量大小'
- en: '#2 Shortened context length (orig: 1024)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 缩短上下文长度（原：1024）'
- en: '#3 Embedding dimension'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 嵌入维度'
- en: '#4 Number of attention heads'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 注意力头数量'
- en: '#5 Number of layers'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 层数数量'
- en: '#6 Dropout rate'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 Dropout率'
- en: '#7 Query-key-value bias'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 查询-键-值偏差'
- en: 'After initializing the model, we need to initialize the data loaders. First,
    we load the “The Verdict” short story:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化模型后，我们需要初始化数据加载器。首先，我们加载“The Verdict”短篇小说：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we load the `text_data` into the data loaders:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`text_data`加载到数据加载器中：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: D.1 Learning rate warmup
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.1 学习率预热
- en: Implementing a learning rate warmup can stabilize the training of complex models
    such as LLMs. This process involves gradually increasing the learning rate from
    a very low initial value (`initial_lr`) to a maximum value specified by the user
    (`peak_lr`). Starting the training with smaller weight updates decreases the risk
    of the model encountering large, destabilizing updates during its training phase.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实现学习率预热可以稳定复杂模型（如LLM）的训练。这个过程涉及将学习率从非常低的初始值（`initial_lr`）逐渐增加到用户指定的最大值（`peak_lr`）。以较小的权重更新开始训练可以降低模型在训练阶段遇到大而破坏性的更新的风险。
- en: 'Suppose we plan to train an LLM for 15 epochs, starting with an initial learning
    rate of 0.0001 and increasing it to a maximum learning rate of 0.01:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们计划训练一个LLM 15个epoch，初始学习率为0.0001，增加到最大学习率0.01：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The number of warmup steps is usually set between 0.1% and 20% of the total
    number of steps, which we can calculate as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 预热步骤的数量通常设置在总步骤数的0.1%到20%之间，我们可以按以下方式计算：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 20% warmup'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 20%预热'
- en: This prints `27`, meaning that we have 20 warmup steps to increase the initial
    learning rate from 0.0001 to 0.01 in the first 27 training steps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出`27`，意味着我们有20个预热步骤，在最初的27个训练步骤中将初始学习率从0.0001增加到0.01。
- en: 'Next, we implement a simple training loop template to illustrate this warmup
    process:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个简单的训练循环模板来展示这个预热过程：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 This increment is determined by how much we increase the inital_lr in each
    of the 20 warmup steps.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这个增量由我们在每个20个预热步骤中增加的`initial_lr`的量决定。'
- en: '#2 Executes a typical training loop iterating over the batches in the training
    loader in each epoch'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在每个epoch中遍历训练加载器中的批次执行典型的训练循环'
- en: '#3 Updates the learning rate if we are still in the warmup phase'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果我们仍在预热阶段，则更新学习率'
- en: '#4 Applies the calculated learning rate to the optimizer'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将计算出的学习率应用于优化器'
- en: '#5 In a complete training loop, the loss and the model updates would be calculated,
    which are omitted here for simplicity.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 在完整的训练循环中，会计算损失和模型更新，这里为了简单起见省略了这些内容。'
- en: 'After running the preceding code, we visualize how the learning rate was changed
    by the training loop to verify that the learning rate warmup works as intended:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们通过可视化训练循环如何改变学习率来验证学习率预热是否按预期工作：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The resulting plot shows that the learning rate starts with a low value and
    increases for 20 steps until it reaches the maximum value after 20 steps (figure
    D.1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示，学习率从低值开始，在20步后增加到最大值，然后在20步后达到最大值（图D.1）。
- en: '![figure](../Images/D-1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/D-1.png)'
- en: Figure D.1 The learning rate warmup increases the learning rate for the first
    20 training steps. After 20 steps, the learning rate reaches the peak of 0.01
    and remains constant for the rest of the training.
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图D.1 学习率预热在最初的20个训练步骤中增加学习率。在20步之后，学习率达到峰值0.01，并在剩余的训练中保持恒定。
- en: Next, we will modify the learning rate further so that it decreases after reaching
    the maximum learning rate, which further helps improve the model training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步修改学习率，使其在达到最大学习率后降低，这有助于进一步提高模型训练。
- en: D.2 Cosine decay
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.2 余弦衰减
- en: Another widely adopted technique for training complex deep neural networks and
    LLMs is *cosine decay*. This method modulates the learning rate throughout the
    training epochs, making it follow a cosine curve after the warmup stage.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛采用的训练复杂深度神经网络和LLMs的技术是*余弦衰减*。这种方法在整个训练周期中调节学习率，使它在预热阶段之后遵循余弦曲线。
- en: In its popular variant, cosine decay reduces (or decays) the learning rate to
    nearly zero, mimicking the trajectory of a half-cosine cycle. The gradual learning
    decrease in cosine decay aims to decelerate the pace at which the model updates
    its weights. This is particularly important because it helps minimize the risk
    of overshooting the loss minima during the training process, which is essential
    for ensuring the stability of the training during its later phases.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在其流行的变体中，余弦衰减将学习率降低（或衰减）到几乎为零，模仿半余弦周期的轨迹。余弦衰减中学习率的逐渐降低旨在减缓模型更新其权重的速度。这尤其重要，因为它有助于在训练过程中最小化超过损失最小值的风险，这对于确保训练后期阶段的稳定性至关重要。
- en: 'We can modify the training loop template by adding cosine decay:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加余弦衰减来修改训练循环模板：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Applies linear warmup'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 应用线性预热'
- en: '#2 Uses cosine annealing after warmup'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在预热后使用余弦退火'
- en: 'Again, to verify that the learning rate has changed as intended, we plot the
    learning rate:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了验证学习率是否按预期改变，我们绘制了学习率图：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resulting learning rate plot shows that the learning rate starts with a
    linear warmup phase, which increases for 20 steps until it reaches the maximum
    value after 20 steps. After the 20 steps of linear warmup, cosine decay kicks
    in, reducing the learning rate gradually until it reaches its minimum (figure
    D.2).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的学习率图显示，学习率开始于线性预热阶段，在20步内增加，直到20步后达到最大值。在20步的线性预热之后，余弦衰减开始，逐渐降低学习率，直到达到其最小值（图D.2）。
- en: '![figure](../Images/D-2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/D-2.png)'
- en: Figure D.2 The first 20 steps of linear learning rate warmup are followed by
    a cosine decay, which reduces the learning rate in a half-cosine cycle until it
    reaches its minimum point at the end of training.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图D.2 线性学习率预热的前20步之后是余弦衰减，它以半余弦周期降低学习率，直到训练结束时达到其最小点。
- en: D.3 Gradient clipping
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.3 梯度裁剪
- en: '*Gradient clipping* is another important technique for enhancing stability
    during LLM training. This method involves setting a threshold above which gradients
    are downscaled to a predetermined maximum magnitude. This process ensures that
    the updates to the model’s parameters during backpropagation stay within a manageable
    range.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度裁剪*是增强LLM训练稳定性的另一种重要技术。这种方法涉及设置一个阈值，超过该阈值梯度将被缩放到预定的最大幅度。这个过程确保在反向传播过程中模型参数的更新保持在可管理的范围内。'
- en: For example, applying the `max_norm=1.0` setting within PyTorch’s `clip_grad_`
    `norm_` function ensures that the norm of the gradients does not surpass 1.0\.
    Here, the term “norm” signifies the measure of the gradient vector’s length, or
    magnitude, within the model’s parameter space, specifically referring to the L2
    norm, also known as the Euclidean norm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在PyTorch的`clip_grad_` `norm_`函数中应用`max_norm=1.0`设置确保梯度的范数不超过1.0。在这里，“范数”表示梯度向量在模型参数空间中的长度或大小，具体指的是L2范数，也称为欧几里得范数。
- en: In mathematical terms, for a vector ***v*** composed of components ***v*** =
    [*v*[1], *v*[2], ..., *v**n*], the L2 norm is
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学术语来说，对于一个由分量***v*** = [*v*[1], *v*[2], ..., *v**n*]组成的向量***v***，L2范数是
- en: '![figure](../Images/Equation-eqs-D-1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-D-1.png)'
- en: This calculation method is also applied to matrices. For instance, consider
    a gradient matrix given by
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算方法也适用于矩阵。例如，考虑一个由以下给出的梯度矩阵
- en: '![figure](../Images/Equation-eqs-D-2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-D-2.png)'
- en: If we want to clip these gradients to a `max_norm` of 1, we first compute the
    L2 norm of these gradients, which is
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将这些梯度裁剪到`max_norm`为1，我们首先计算这些梯度的L2范数，它是
- en: '![figure](../Images/Equation-eqs-D-3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-D-3.png)'
- en: Given that |**G**|[2] = 5 exceeds our `max_norm` of 1, we scale down the gradients
    to ensure their norm equals exactly 1\. This is achieved through a scaling factor,
    calculated as `max_norm`/|**G**|[2] = 1/5\. Consequently, the adjusted gradient
    matrix **G'** becomes
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于|**G**|[2] = 5超过了我们的`max_norm`，我们将梯度缩放到其范数正好等于1。这是通过一个缩放因子实现的，计算为`max_norm`/|**G**|[2]
    = 1/5。因此，调整后的梯度矩阵**G'**变为
- en: '![figure](../Images/Equation-eqs-D-4.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-D-4.png)'
- en: 'To illustrate this gradient clipping process, we begin by initializing a new
    model and calculating the loss for a training batch, similar to the procedure
    in a standard training loop:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个梯度裁剪过程，我们首先初始化一个新的模型并计算一个训练批次的损失，类似于标准训练循环中的过程：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Upon calling the `.backward()` method, PyTorch calculates the loss gradients
    and stores them in a `.grad` attribute for each model weight (parameter) tensor.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`.backward()`方法后，PyTorch计算损失梯度并将它们存储在每个模型权重（参数）张量的`.grad`属性中。
- en: 'To clarify the point, we can define the following `find_highest_gradient` utility
    function to identify the highest gradient value by scanning all the `.grad` attributes
    of the model’s weight tensors after calling `.backward()`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这一点，我们可以定义以下`find_highest_gradient`实用函数，通过扫描模型权重张量的所有`.grad`属性来识别最高的梯度值，在调用`.backward()`之后：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The largest gradient value identified by the preceding code is
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码确定的最大梯度值是
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s now apply gradient clipping and see how this affects the largest gradient
    value:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应用梯度裁剪并看看这如何影响最大梯度值：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The largest gradient value after applying the gradient clipping with the max
    norm of 1 is substantially smaller than before:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 应用梯度裁剪（最大范数为1）后的最大梯度值比之前小得多：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: D.4 The modified training function
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.4 修改后的训练函数
- en: 'Finally, we improve the `train_model_simple` training function (see chapter
    5) by adding the three concepts introduced herein: linear warmup, cosine decay,
    and gradient clipping. Together, these methods help stabilize LLM training.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过添加本文中介绍的三种概念来改进`train_model_simple`训练函数（见第5章）：线性预热、余弦衰减和梯度裁剪。这些方法共同帮助稳定LLM的训练。
- en: 'The code, with the changes compared to the `train_model_simple` annotated,
    is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与`train_model_simple`相比，代码的变化如下所示：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Retrieves the initial learning rate from the optimizer, assuming we use
    it as the peak learning rate'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从优化器中检索初始学习率，假设我们将其用作峰值学习率'
- en: '#2 Calculates the total number of iterations in the training process'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算训练过程中的总迭代次数'
- en: '#3 Calculates the learning rate increment during the warmup phase'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在预热阶段计算学习率增量'
- en: '#4 Adjusts the learning rate based on the current phase (warmup or cosine annealing)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 根据当前阶段（预热或余弦退火）调整学习率'
- en: '#5 **Applies the calculated learning rate to the optimizer**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 **将计算出的学习率应用于优化器**'
- en: '#6 Applies gradient clipping after the warmup phase to avoid exploding gradients'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 在预热阶段之后应用梯度裁剪以避免梯度爆炸'
- en: '#7 Everything below here remains unchanged compared to the train_model_simple
    function used in chapter 5.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 以下内容与第5章中使用的train_model_simple函数相比保持不变。'
- en: 'After defining the `train_model` function, we can use it in a similar fashion
    to train the model compared to the `train_model_simple` method we used for pretraining:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`train_model`函数后，我们可以像使用`train_model_simple`方法进行预训练一样使用它来训练模型：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The training will take about 5 minutes to complete on a MacBook Air or similar
    laptop and prints the following outputs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在MacBook Air或类似笔记本电脑上完成训练大约需要5分钟，并打印以下输出：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Like pretraining, the model begins to overfit after a few epochs since it is
    a very small dataset, and we iterate over it multiple times. Nonetheless, we can
    see that the function is working since it minimizes the training set loss.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与预训练类似，由于数据集非常小，模型在几个epoch后开始过拟合，我们多次迭代它。尽管如此，我们可以看到函数正在起作用，因为它最小化了训练集损失。
- en: Readers are encouraged to train the model on a larger text dataset and compare
    the results obtained with this more sophisticated training function to the results
    that can be obtained with the `train_model_simple` function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者在更大的文本数据集上训练模型，并将使用此更复杂的训练函数获得的结果与使用`train_model_simple`函数获得的结果进行比较。
