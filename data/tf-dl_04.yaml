- en: Chapter 4\. Fully Connected Deep Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。全连接的深度网络
- en: This chapter will introduce you to fully connected deep networks. Fully connected
    networks are the workhorses of deep learning, used for thousands of applications.
    The major advantage of fully connected networks is that they are “structure agnostic.”
    That is, no special assumptions need to be made about the input (for example,
    that the input consists of images or videos). We will make use of this generality
    to use fully connected deep networks to address a problem in chemical modeling
    later in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您介绍全连接的深度网络。全连接网络是深度学习的主力军，用于成千上万的应用。全连接网络的主要优势在于它们是“结构不可知的”。也就是说，不需要对输入做出特殊的假设（例如，输入由图像或视频组成）。我们将利用这种通用性，使用全连接的深度网络来解决本章后面的化学建模问题。
- en: We delve briefly into the mathematical theory underpinning fully connected networks.
    In particular, we explore the concept that fully connected architectures are “universal
    approximators” capable of learning any function. This concept provides an explanation
    of the generality of fully connected architectures, but comes with many caveats
    that we discuss at some depth.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要探讨支撑全连接网络的数学理论。特别是，我们探讨全连接架构是“通用逼近器”，能够学习任何函数的概念。这个概念解释了全连接架构的通用性，但也伴随着我们深入讨论的许多注意事项。
- en: While being structure agnostic makes fully connected networks very broadly applicable,
    such networks do tend to have weaker performance than special-purpose networks
    tuned to the structure of a problem space. We will discuss some of the limitations
    of fully connected architectures later in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然结构不可知使全连接网络非常广泛适用，但这种网络的性能往往比针对问题空间结构调整的专用网络要弱。我们将在本章后面讨论全连接架构的一些限制。
- en: What Is a Fully Connected Deep Network?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是全连接的深度网络？
- en: A fully connected neural network consists of a series of fully connected layers.
    A fully connected layer is a function from <math alttext="double-struck upper
    R Superscript m"><msup><mi>ℝ</mi> <mi>m</mi></msup></math> to <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> . Each output
    dimension depends on each input dimension. Pictorially, a fully connected layer
    is represented as follows in [Figure 4-1](#ch4-fclayer).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接神经网络由一系列全连接层组成。全连接层是从<math alttext="double-struck upper R Superscript m"><msup><mi>ℝ</mi>
    <mi>m</mi></msup></math>到<math alttext="double-struck upper R Superscript n"><msup><mi>ℝ</mi>
    <mi>n</mi></msup></math>的函数。每个输出维度都依赖于每个输入维度。在图4-1中，全连接层的图示如下。
- en: '![FCLayer.png](assets/tfdl_0401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![FCLayer.png](assets/tfdl_0401.png)'
- en: Figure 4-1\. A fully connected layer in a deep network.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. 深度网络中的全连接层。
- en: 'Let’s dig a little deeper into what the mathematical form of a fully connected
    network is. Let <math><mrow><mi>x</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>m</mi></msup></mrow></math>
    represent the input to a fully connected layer. Let <math alttext="y Subscript
    i Baseline element-of double-struck upper R"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∈</mo> <mi>ℝ</mi></mrow></math> be the <math alttext="i"><mi>i</mi></math>
    -th output from the fully connected layer. Then <math><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>∈</mo> <mi>ℝ</mi></mrow></math> is computed as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解全连接网络的数学形式。让<math><mrow><mi>x</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>m</mi></msup></mrow></math>表示全连接层的输入。让<math
    alttext="y Subscript i Baseline element-of double-struck upper R"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>∈</mo> <mi>ℝ</mi></mrow></math>是全连接层的第i个输出。那么<math><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>∈</mo> <mi>ℝ</mi></mrow></math>的计算如下：
- en: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>w</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi> <mi>m</mi></msub> <msub><mi>x</mi>
    <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>w</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi> <mi>m</mi></msub> <msub><mi>x</mi>
    <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Here, <math alttext="sigma"><mi>σ</mi></math> is a nonlinear function (for now,
    think of <math><mi>σ</mi></math> as the sigmoid function introduced in the previous
    chapter), and the <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    are learnable parameters in the network. The full output *y* is then
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="sigma"><mi>σ</mi></math> 是一个非线性函数（暂时将<math><mi>σ</mi></math>视为前一章介绍的Sigmoid函数），<math
    alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> 是网络中可学习的参数。完整的输出*y*如下：
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mi>n</mi><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mi>σ</mi>
    <mo>(</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>w</mi>
    <mrow><mi>n</mi><mo>,</mo><mi>m</mi></mrow></msub> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Note that it’s directly possible to stack fully connected networks. A network
    with multiple fully connected networks is often called a “deep” network as depicted
    in [Figure 4-2](#ch4-multifcnet).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以直接堆叠全连接网络。具有多个全连接网络的网络通常被称为“深度”网络，如图4-2所示。
- en: '![multilayer_fcnet.png](assets/tfdl_0402.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![multilayer_fcnet.png](assets/tfdl_0402.png)'
- en: Figure 4-2\. A multilayer deep fully connected network.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。一个多层深度全连接网络。
- en: 'As a quick implementation note, note that the equation for a single neuron
    looks very similar to a dot-product of two vectors (recall the discussion of tensor
    basics). For a layer of neurons, it is often convenient for efficiency purposes
    to compute *y* as a matrix multiply:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速实现的注意事项，注意单个神经元的方程看起来非常类似于两个向量的点积（回想一下张量基础的讨论）。对于一层神经元，通常为了效率目的，将*y*计算为矩阵乘积是很方便的：
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>σ</mi> <mo>(</mo> <mi>w</mi>
    <mi>x</mi> <mo>)</mo></mrow></math>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mi>σ</mi> <mo>(</mo> <mi>w</mi>
    <mi>x</mi> <mo>)</mo></mrow></math>
- en: where sigma is a matrix in <math alttext="double-struck upper R Superscript
    n times m"><msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></math>
    and the nonlinearity <math alttext="sigma"><mi>σ</mi></math> is applied componentwise.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中sigma是一个矩阵在<math alttext="双击上R上标n次m"><msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></math>，非线性<math
    alttext="sigma"><mi>σ</mi></math>是逐分量应用的。
- en: “Neurons” in Fully Connected Networks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接网络中的“神经元”
- en: The nodes in fully connected networks are commonly referred to as “neurons.”
    Consequently, elsewhere in the literature, fully connected networks will commonly
    be referred to as “neural networks.” This nomenclature is largely a historical
    accident.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接网络中的节点通常被称为“神经元”。因此，在文献中，全连接网络通常被称为“神经网络”。这种命名方式在很大程度上是历史的偶然。
- en: In the 1940s, Warren S. McCulloch and Walter Pitts published a first mathematical
    model of the brain that argued that neurons were capable of computing arbitrary
    functions on Boolean quantities. Successors to this work slightly refined this
    logical model by making mathematical “neurons” continuous functions that varied
    between zero and one. If the inputs of these functions grew large enough, the
    neuron “fired” (took on the value one), else was quiescent. With the addition
    of adjustable weights, this description matches the previous equations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在1940年代，沃伦·S·麦卡洛克和沃尔特·皮茨发表了一篇关于大脑的第一个数学模型，认为神经元能够计算布尔量上的任意函数。这项工作的后继者稍微完善了这个逻辑模型，通过使数学“神经元”成为在零和一之间变化的连续函数。如果这些函数的输入足够大，神经元就会“发射”（取值为一），否则就是静止的。通过可调权重的添加，这个描述与之前的方程匹配。
- en: Is this how a real neuron behaves? Of course not! A real neuron ([Figure 4-3](#ch4-neuron))
    is an exceedingly complex engine, with over 100 trillion atoms, and tens of thousands
    of different signaling proteins capable of responding to varying signals. A microprocessor
    is a better analogy for a neuron than a one-line equation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这才是真正的神经元行为吗？当然不是！一个真实的神经元（[图4-3](#ch4-neuron)）是一个极其复杂的引擎，拥有超过100万亿个原子，以及数以万计的不同信号蛋白质，能够对不同信号做出反应。微处理器比一个一行方程更好地类比于神经元。
- en: '![neuron.png](assets/tfdl_0403.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![neuron.png](assets/tfdl_0403.png)'
- en: Figure 4-3\. A more biologically accurate representation of a neuron.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3。神经元的更生物学准确的表示。
- en: In many ways, this disconnect between biological neurons and artificial neurons
    is quite unfortunate. Uninitiated experts read breathless press releases claiming
    artificial neural networks with billions of “neurons” have been created (while
    the brain has only 100 billion biological neurons) and reasonably come away believing
    scientists are close to creating human-level intelligences. Needless to say, state
    of the art in deep learning is decades (or centuries) away from such an achievement.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，生物神经元和人工神经元之间的这种脱节是非常不幸的。未经培训的专家读到令人激动的新闻稿，声称已经创建了拥有数十亿“神经元”的人工神经网络（而大脑只有1000亿个生物神经元），并且合理地认为科学家们已经接近创造人类水平的智能。不用说，深度学习的最新技术距离这样的成就还有几十年（甚至几个世纪）的距离。
- en: As you read further about deep learning, you may come across overhyped claims
    about artificial intelligence. Don’t be afraid to call out these statements. Deep
    learning in its current form is a set of techniques for solving calculus problems
    on fast hardware. It is not a precursor to *Terminator* ([Figure 4-4](#ch4-terminator)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当您进一步了解深度学习时，您可能会遇到关于人工智能的夸大宣传。不要害怕指出这些声明。目前的深度学习是一套在快速硬件上解决微积分问题的技术。它不是*终结者*的前身（[图4-4](#ch4-terminator)）。
- en: '![terminator.png](assets/tfdl_0404.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![terminator.png](assets/tfdl_0404.png)'
- en: Figure 4-4\. Unfortunately (or perhaps fortunately), this book won’t teach you
    to build a Terminator!
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4。不幸的是（或者也许是幸运的），这本书不会教你如何构建一个终结者！
- en: AI Winters
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI寒冬
- en: Artificial intelligence has gone through multiple rounds of boom-and-bust development.
    This cyclical development is characteristic of the field. Each new advance in
    learning spawns a wave of optimism in which prophets claim that human-level (or
    superhuman) intelligences are incipient. After a few years, no such intelligences
    manifest, and disappointed funders pull out. The resulting period is called an
    AI winter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能经历了多轮繁荣和衰退的发展。这种循环性的发展是该领域的特点。每一次学习的新进展都会引发一波乐观情绪，其中预言家声称人类水平（或超人类）的智能即将出现。几年后，没有这样的智能体现出来，失望的资助者退出。由此产生的时期被称为AI寒冬。
- en: There have been multiple AI winters so far. As a thought exercise, we encourage
    you to consider when the next AI winter will happen. The current wave of deep
    learning progress has solved many more practical problems than any previous wave
    of advances. Is it possible AI has finally taken off and exited the boom-and-bust
    cycle or do you think we’re in for the “Great Depression” of AI soon?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止已经有多次AI寒冬。作为一种思考练习，我们鼓励您考虑下一次AI寒冬将在何时发生。当前的深度学习进展解决了比以往任何一波进步更多的实际问题。AI是否可能最终脱颖而出，摆脱繁荣和衰退的周期，或者您认为我们很快就会迎来AI的“大萧条”？
- en: Learning Fully Connected Networks with Backpropagation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向传播学习全连接网络
- en: The first version of a fully connected neural network was the Perceptron, ([Figure 4-5](#ch4-perceptron)),
    created by Frank Rosenblatt in the 1950s. These perceptrons are identical to the
    “neurons” we introduced in the previous equations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的神经网络的第一个版本是感知器（[图4-5](#ch4-perceptron)），由Frank Rosenblatt在1950年代创建。这些感知器与我们在前面的方程中介绍的“神经元”是相同的。
- en: '![perceptron.png](assets/tfdl_0405.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![perceptron.png](assets/tfdl_0405.png)'
- en: Figure 4-5\. A diagrammatic representation of the perceptron.
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。感知器的示意图。
- en: Perceptrons were trained by a custom “perceptron” rule. While they were moderately
    useful solving simple problems, perceptrons were fundamentally limited. The book
    *Perceptrons* by Marvin Minsky and Seymour Papert from the end of the 1960s proved
    that simple perceptrons were incapable of learning the XOR function. [Figure 4-6](#ch4-perceptron2)
    illustrates the proof of this statement.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是通过自定义的“感知器”规则进行训练的。虽然它们在解决简单问题时有一定用处，但感知器在根本上受到限制。1960年代末Marvin Minsky和Seymour
    Papert的书《感知器》证明了简单感知器无法学习XOR函数。[图4-6](#ch4-perceptron2)说明了这个说法的证明。
- en: '![xor2.gif](assets/tfdl_0406.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![xor2.gif](assets/tfdl_0406.png)'
- en: Figure 4-6\. The perceptron’s linear rule can’t learn the perceptron.
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。感知器的线性规则无法学习感知器。
- en: This problem was overcome with the invention of the multilayer perceptron (another
    name for a deep fully connected network). This invention was a formidable achievement,
    since earlier simple learning algorithms couldn’t learn deep networks effectively.
    The “credit assignment” problem stumped them; how does an algorithm decide which
    neuron learns what?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通过多层感知器（另一个称为深度全连接网络的名称）的发明得以解决。这一发明是一个巨大的成就，因为早期的简单学习算法无法有效地学习深度网络。 “信用分配”问题困扰着它们；算法如何决定哪个神经元学习什么？
- en: The full solution to this problem requires backpropagation. Backpropagation
    is a generalized rule for learning the weights of neural networks. Unfortunately,
    complicated explanations of backpropagation are epidemic in the literature. This
    situation is unfortunate since backpropagation is simply another word for automatic
    differentiation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的完整方法需要反向传播。反向传播是学习神经网络权重的通用规则。不幸的是，关于反向传播的复杂解释在文献中泛滥。这种情况很不幸，因为反向传播只是自动微分的另一个说法。
- en: Let’s suppose that <math alttext="f left-parenthesis theta comma x right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math> is a function
    that represents a deep fully connected network. Here <math alttext="x"><mi>x</mi></math>
    is the inputs to the fully connected network and <math alttext="theta"><mi>θ</mi></math>
    is the learnable weights. Then the backpropagation algorithm simply computes <math
    alttext="StartFraction normal partial-differential f Over normal partial-differential
    theta EndFraction"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>θ</mi></mrow></mfrac></math>
    . The practical complexities arise in implementing backpropagation for all possible
    functions *f* that arise in practice. Luckily for us, TensorFlow takes care of
    this already!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设<math alttext="f left-parenthesis theta comma x right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math>是代表深度全连接网络的函数。这里<math
    alttext="x"><mi>x</mi></math>是完全连接网络的输入，<math alttext="theta"><mi>θ</mi></math>是可学习的权重。然后，反向传播算法简单地计算<math
    alttext="StartFraction normal partial-differential f Over normal partial-differential
    theta EndFraction"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>θ</mi></mrow></mfrac></math>。在实践中，实现反向传播以处理所有可能出现的*f*函数的复杂性。幸运的是，TensorFlow已经为我们处理了这一点！
- en: Universal Convergence Theorem
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用收敛定理
- en: The preceding discussion has touched on the ideas that deep fully connected
    networks are powerful approximations. McCulloch and Pitts showed that logical
    networks can code (almost) any Boolean function. Rosenblatt’s Perceptron was the
    continuous analog of McCulloch and Pitt’s logical functions, but was shown to
    be fundamentally limited by Minsky and Papert. Multilayer perceptrons looked to
    solve the limitations of simple perceptrons and empirically seemed capable of
    learning complex functions. However, it wasn’t theoretically clear whether this
    empirical ability had undiscovered limitations. In 1989, George Cybenko demonstrated
    that multilayer perceptrons were capable of representing arbitrary functions.
    This demonstration provided a considerable boost to the claims of generality for
    fully connected networks as a learning architecture, partially explaining their
    continued popularity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的讨论涉及到了深度全连接网络是强大逼近的想法。McCulloch和Pitts表明逻辑网络可以编码（几乎）任何布尔函数。Rosenblatt的感知器是McCulloch和Pitt的逻辑函数的连续模拟，但被Minsky和Papert证明在根本上受到限制。多层感知器试图解决简单感知器的限制，并且在经验上似乎能够学习复杂函数。然而，从理论上讲，尚不清楚这种经验能力是否存在未被发现的限制。
    1989年，George Cybenko证明了多层感知器能够表示任意函数。这一演示为全连接网络作为学习架构的普遍性主张提供了相当大的支持，部分解释了它们持续受欢迎的原因。
- en: However, if both backpropagation and fully connected network theory were understood
    in the late 1980s, why didn’t “deep” learning become more popular earlier? A large
    part of this failure was due to computational limitations; learning fully connected
    networks took an exorbitant amount of computing power. In addition, deep networks
    were very difficult to train due to lack of understanding about good hyperparameters.
    As a result, alternative learning algorithms such as SVMs that had lower computational
    requirements became more popular. The recent surge in popularity in deep learning
    is partly due to the increased availability of better computing hardware that
    enables faster computing, and partly due to increased understanding of good training
    regimens that enable stable learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在上世纪80年代后期人们已经理解了反向传播和全连接网络理论，为什么“深度”学习没有更早变得更受欢迎呢？这种失败的很大一部分是由于计算能力的限制；学习全连接网络需要大量的计算能力。此外，由于对好的超参数缺乏理解，深度网络非常难以训练。因此，计算要求较低的替代学习算法，如SVM，变得更受欢迎。深度学习近年来的流行部分原因是更好的计算硬件的增加可用性，使计算速度更快，另一部分原因是对能够实现稳定学习的良好训练方案的增加理解。
- en: Is Universal Approximation That Surprising?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用逼近是否令人惊讶？
- en: Universal approximation properties are more common in mathematics than one might
    expect. For example, the Stone-Weierstrass theorem proves that any continuous
    function on a closed interval can be a suitable polynomial function. Loosening
    our criteria further, Taylor series and Fourier series themselves offer some universal
    approximation capabilities (within their domains of convergence). The fact that
    universal convergence is fairly common in mathematics provides partial justification
    for the empirical observation that there are many slight variants of fully connected
    networks that seem to share a universal approximation property.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近性质在数学中比人们可能期望的更常见。例如，Stone-Weierstrass定理证明了在闭区间上的任何连续函数都可以是一个合适的多项式函数。进一步放宽我们的标准，泰勒级数和傅里叶级数本身提供了一些通用逼近能力（在它们的收敛域内）。通用收敛在数学中相当常见的事实部分地为经验观察提供了部分理由，即许多略有不同的全连接网络变体似乎具有通用逼近性质。
- en: Universal Approximation Doesn’t Mean Universal Learning!
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用逼近并不意味着通用学习！
- en: A critical subtlety exists in the universal approximation theorem. The fact
    that a fully connected network can represent any function doesn’t mean that backpropagation
    can learn any function! One of the major limitations of backpropagation is that
    there is no guarantee the fully connected network “converges”; that is, finds
    the best available solution of a learning problem. This critical theoretical gap
    has left generations of computer scientists queasy with neural networks. Even
    today, many academics will prefer to work with alternative algorithms that have
    stronger theoretical guarantees.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近定理中存在一个关键的微妙之处。全连接网络可以表示任何函数并不意味着反向传播可以学习任何函数！反向传播的一个主要限制是没有保证全连接网络“收敛”；也就是说，找到学习问题的最佳可用解决方案。这个关键的理论差距让几代计算机科学家对神经网络感到不安。即使在今天，许多学者仍然更愿意使用具有更强理论保证的替代算法。
- en: Empirical research has yielded many practical tricks that allow backpropagation
    to find good solutions for problems. We will go into many of these tricks in significant
    depth in the remainder of this chapter. For the practicing data scientist, the
    universal approximation theorem isn’t something to take too seriously. It’s reassuring,
    but the art of deep learning lies in mastering the practical hacks that make learning
    work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 经验研究已经产生了许多实用技巧，使反向传播能够为问题找到好的解决方案。在本章的其余部分中，我们将深入探讨许多这些技巧。对于实践数据科学家来说，通用逼近定理并不是什么需要太认真对待的东西。这是令人放心的，但深度学习的艺术在于掌握使学习有效的实用技巧。
- en: Why Deep Networks?
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要使用深度网络？
- en: A subtlety in the universal approximation theorem is that it in fact holds true
    for fully connected networks with only one fully connected layer. What then is
    the use of “deep” learning with multiple fully connected layers? It turns out
    that this question is still quite controversial in academic and practical circles.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近定理中的一个微妙之处是，事实上它对只有一个全连接层的全连接网络也成立。那么，具有多个全连接层的“深度”学习有什么用呢？事实证明，这个问题在学术和实践领域仍然颇具争议。
- en: In practice, it seems that deeper networks can sometimes learn richer models
    on large datasets. (This is only a rule of thumb, however; every practitioner
    has a bevy of examples where deep fully connected networks don’t do well.) This
    observation has led researchers to hypothesize that deeper networks can represent
    complex functions “more efficiently.” That is, a deeper network may be able to
    learn more complex functions than shallower networks with the same number of neurons.
    For example, the ResNet architecture mentioned briefly in the first chapter, with
    130 layers, seems to outperform its shallower competitors such as AlexNet. In
    general, for a fixed neuron budget, stacking deeper leads to better results.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，似乎更深层的网络有时可以在大型数据集上学习更丰富的模型。（然而，这只是一个经验法则；每个实践者都有许多例子，深度全连接网络表现不佳。）这一观察结果导致研究人员假设更深层的网络可以更有效地表示复杂函数。也就是说，相比具有相同数量的神经元的较浅网络，更深的网络可能能够学习更复杂的函数。例如，在第一章中简要提到的ResNet架构，具有130层，似乎胜过其较浅的竞争对手，如AlexNet。一般来说，对于固定的神经元预算，堆叠更深层次会产生更好的结果。
- en: A number of erroneous “proofs” for this “fact” have been given in the literature,
    but all of them have holes. It seems the question of depth versus width touches
    on profound concepts in complexity theory (which studies the minimal amount of
    resources required to solve given computational problems). At present day, it
    looks like theoretically demonstrating (or disproving) the superiority of deep
    networks is far outside the ability of our mathematicians.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了一些关于深度网络优势的错误“证明”，但它们都有漏洞。深度与宽度的问题似乎涉及到复杂性理论中的深刻概念（研究解决给定计算问题所需的最小资源量）。目前看来，理论上证明（或否定）深度网络的优越性远远超出了我们数学家的能力范围。
- en: Training Fully Connected Neural Networks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练全连接神经网络
- en: As we mentioned previously, the theory of fully connected networks falls short
    of practice. In this section, we will introduce you to a number of empirical observations
    about fully connected networks that aid practitioners. We strongly encourage you
    to use our code (introduced later in the chapter) to check our claims for yourself.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，全连接网络的理论与实践有所不同。在本节中，我们将向您介绍一些关于全连接网络的经验观察，这些观察有助于从业者。我们强烈建议您使用我们的代码（在本章后面介绍）来验证我们的说法。
- en: Learnable Representations
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可学习表示
- en: One way of thinking about fully connected networks is that each fully connected
    layer effects a transformation of the feature space in which the problem resides.
    The idea of transforming the representation of a problem to render it more malleable
    is a very old one in engineering and physics. It follows that deep learning methods
    are sometimes called “representation learning.” (An interesting factoid is that
    one of the major conferences for deep learning is called the “International Conference
    on Learning Representations.”)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一种思考全连接网络的方式是，每个全连接层都会对问题所在的特征空间进行转换。在工程和物理学中，将问题的表示转换为更易处理的形式的想法是非常古老的。因此，深度学习方法有时被称为“表示学习”。（有趣的事实是，深度学习的一个主要会议被称为“国际学习表示会议”。）
- en: Generations of analysts have used Fourier transforms, Legendre transforms, Laplace
    transforms, and so on in order to simplify complicated equations and functions
    to forms more suitable for handwritten analysis. One way of thinking about deep
    learning networks is that they effect a data-driven transform suited to the problem
    at hand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 几代分析师已经使用傅立叶变换、勒让德变换、拉普拉斯变换等方法，将复杂的方程和函数简化为更适合手工分析的形式。一种思考深度学习网络的方式是，它们实现了一个适合手头问题的数据驱动转换。
- en: 'The ability to perform problem-specific transformations can be immensely powerful.
    Standard transformation techniques couldn’t solve problems of image or speech
    analysis, while deep networks are capable of solving these problems with relative
    ease due to the inherent flexibility of the learned representations. This flexibility
    comes with a price: the transformations learned by deep architectures tend to
    be much less general than mathematical transforms such as the Fourier transform.
    Nonetheless, having deep transforms in an analytic toolkit can be a powerful problem-solving
    tool.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 执行特定于问题的转换能力可能非常强大。标准的转换技术无法解决图像或语音分析的问题，而深度网络能够相对轻松地解决这些问题，这是由于学习表示的固有灵活性。这种灵活性是有代价的：深度架构学习到的转换通常比傅立叶变换等数学变换要不那么通用。尽管如此，将深度变换纳入分析工具包中可以成为一个强大的问题解决工具。
- en: There’s a reasonable argument that deep learning is simply the first representation
    learning method that works. In the future, there may well be alternative representation
    learning methods that supplant deep learning methods.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个合理的观点认为，深度学习只是第一个有效的表示学习方法。将来，可能会有替代的表示学习方法取代深度学习方法。
- en: Activations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: We previously introduced the nonlinear function <math alttext="sigma"><mi>σ</mi></math>
    as the sigmoidal function. While the sigmoidal is the classical nonlinearity in
    fully connected networks, in recent years researchers have found that other activations,
    notably the rectified linear activation (commonly abbreviated ReLU or relu) <math
    alttext="sigma left-parenthesis x right-parenthesis equals max left-parenthesis
    x comma 0 right-parenthesis"><mrow><mi>σ</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>max</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math>
    work better than the sigmoidal unit. This empirical observation may be due to
    the *vanishing gradient* problem in deep networks. For the sigmoidal function,
    the slope is zero for almost all values of its input. As a result, for deeper
    networks, the gradient would tend to zero. For the ReLU function, the slope is
    nonzero for a much greater part of input space, allowing nonzero gradients to
    propagate. [Figure 4-7](#ch4-activation) illustrates sigmoidal and ReLU activations
    side by side.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍了非线性函数<math alttext="sigma"><mi>σ</mi></math>作为S形函数。虽然S形函数是全连接网络中的经典非线性，但近年来研究人员发现其他激活函数，特别是修正线性激活（通常缩写为ReLU或relu）<math
    alttext="sigma left-parenthesis x right-parenthesis equals max left-parenthesis
    x comma 0 right-parenthesis"><mrow><mi>σ</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>max</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math>比S形单元效果更好。这种经验观察可能是由于深度网络中的*梯度消失*问题。对于S形函数，几乎所有输入值的斜率都为零。因此，对于更深的网络，梯度会趋近于零。对于ReLU函数，输入空间的大部分部分斜率都不为零，允许非零梯度传播。[图4-7](#ch4-activation)展示了S形和ReLU激活函数并排的情况。
- en: '![activation-functions.png](assets/tfdl_0407.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![activation-functions.png](assets/tfdl_0407.png)'
- en: Figure 4-7\. Sigmoidal and ReLU activation functions.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。S形和ReLU激活函数。
- en: Fully Connected Networks Memorize
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接网络记忆
- en: One of the striking aspects about fully connected networks is that they tend
    to memorize training data entirely given enough time. As a result, training a
    fully connected network to “convergence” isn’t really a meaningful metric. The
    network will keep training and learning as long as the user is willing to wait.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接网络的一个显著特点是，给定足够的时间，它们倾向于完全记住训练数据。因此，将全连接网络训练到“收敛”实际上并不是一个有意义的度量。只要用户愿意等待，网络将继续训练和学习。
- en: For large enough networks, it is quite common for training loss to trend all
    the way to zero. This empirical observation is one the most practical demonstrations
    of the universal approximation capabilities of fully connected networks. Note
    however, that training loss trending to zero does not mean that the network has
    learned a more powerful model. It’s rather likely that the model has started to
    memorize peculiarities of the training set that aren’t applicable to any other
    datapoints.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于足够大的网络，训练损失趋向于零是非常常见的。这一经验观察是全连接网络的通用逼近能力最实用的证明之一。然而，请注意，训练损失趋向于零并不意味着网络已经学会了一个更强大的模型。相反，模型很可能已经开始记忆训练集的怪癖，这些怪癖并不适用于任何其他数据点。
- en: It’s worth digging into what we mean by peculiarities here. One of the interesting
    properties of high-dimensional statistics is that given a large enough dataset,
    there will be plenty of spurious correlations and patterns available for the picking.
    In practice, fully connected networks are entirely capable of finding and utilizing
    these spurious correlations. Controlling networks and preventing them from misbehaving
    in this fashion is critical for modeling success.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 值得深入探讨这里我们所说的奇特之处。高维统计学的一个有趣特性是，给定足够大的数据集，将有大量的虚假相关性和模式可供选择。在实践中，全连接网络完全有能力找到并利用这些虚假相关性。控制网络并防止它们以这种方式行为不端对于建模成功至关重要。
- en: Regularization
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is the general statistical term for a mathematical operation
    that limits memorization while promoting generalizable learning. There are many
    different types of regularization available, which we will cover in the next few
    sections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一个通用的统计术语，用于限制记忆化，同时促进可泛化的学习。有许多不同类型的正则化可用，我们将在接下来的几节中介绍。
- en: Not Your Statistician’s Regularization
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不是您的统计学家的正则化
- en: Regularization has a long history in the statistical literature, with entire
    sheaves of papers written on the topic. Unfortunately, only some of this classical
    analysis carries over to deep networks. The linear models used widely in statistics
    can behave very differently from deep networks, and many of the intuitions built
    in that setting can be downright wrong for deep networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化在统计文献中有着悠久的历史，有许多关于这个主题的论文。不幸的是，只有一部分经典分析适用于深度网络。在统计学中广泛使用的线性模型可能与深度网络表现出截然不同，而在那种情况下建立的许多直觉对于深度网络来说可能是错误的。
- en: The first rule for working with deep networks, especially for readers with prior
    statistical modeling experience, is to trust empirical results over past intuition.
    Don’t assume that past knowledge about techniques such as LASSO has much meaning
    for modeling deep architectures. Rather, set up an experiment to methodically
    test your proposed idea. We will return at greater depth to this methodical experimentation
    process in the next chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度网络一起工作的第一条规则，特别是对于具有先前统计建模经验的读者，是相信经验结果胜过过去的直觉。不要假设对于建模深度架构等技术的过去知识有太多意义。相反，建立一个实验来系统地测试您提出的想法。我们将在下一章更深入地讨论这种系统化实验过程。
- en: Dropout
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: Dropout is a form of regularization that randomly drops some proportion of the
    nodes that feed into a fully connected layer ([Figure 4-8](#ch4-dropout)). Here,
    dropping a node means that its contribution to the corresponding activation function
    is set to 0\. Since there is no activation contribution, the gradients for dropped
    nodes drop to zero as well.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种正则化形式，它随机地删除一些输入到全连接层的节点的比例（[图4-8](#ch4-dropout)）。在这里，删除一个节点意味着其对应激活函数的贡献被设置为0。由于没有激活贡献，被删除节点的梯度也降为零。
- en: '![dropout.png](assets/tfdl_0408.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![dropout.png](assets/tfdl_0408.png)'
- en: Figure 4-8\. Dropout randomly drops neurons from a network while training. Empirically,
    this technique often provides powerful regularization for network training.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8。在训练时，Dropout随机删除网络中的神经元。从经验上看，这种技术通常为网络训练提供强大的正则化。
- en: The nodes to be dropped are chosen at random during each step of gradient descent.
    The underlying design principle is that the network will be forced to avoid “co-adaptation.”
    Briefly, we will explain what co-adaptation is and how it arises in non-regularized
    deep architectures. Suppose that one neuron in a deep network has learned a useful
    representation. Then other neurons deeper in the network will rapidly learn to
    depend on that particular neuron for information. This process will render the
    network brittle since the network will depend excessively on the features learned
    by that neuron, which might represent a quirk of the dataset, instead of learning
    a general rule.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除的节点是在梯度下降的每一步中随机选择的。底层的设计原则是网络将被迫避免“共适应”。简而言之，我们将解释什么是共适应以及它如何在非正则化的深度架构中出现。假设深度网络中的一个神经元学习了一个有用的表示。那么网络中更深层的其他神经元将迅速学会依赖于该特定神经元获取信息。这个过程将使网络变得脆弱，因为网络将过度依赖于该神经元学到的特征，而这些特征可能代表数据集的一个怪癖，而不是学习一个普遍规则。
- en: Dropout prevents this type of co-adaptation because it will no longer be possible
    to depend on the presence of single powerful neurons (since that neuron might
    drop randomly during training). As a result, other neurons will be forced to “pick
    up the slack” and learn useful representations as well. The theoretical argument
    follows that this process should result in stronger learned models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout可以防止这种协同适应，因为不再可能依赖于单个强大的神经元的存在（因为该神经元在训练期间可能会随机消失）。因此，其他神经元将被迫“弥补空缺”并学习到有用的表示。理论上的论点是，这个过程应该会产生更强大的学习模型。
- en: In practice, dropout has a pair of empirical effects. First, it prevents the
    network from memorizing the training data; with dropout, training loss will no
    longer tend rapidly toward 0, even for very large deep networks. Next, dropout
    tends to slightly boost the predictive power of the model on new data. This effect
    often holds for a wide range of datasets, part of the reason that dropout is recognized
    as a powerful invention, and not just a simple statistical hack.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，dropout有一对经验效果。首先，它防止网络记忆训练数据；使用dropout后，即使对于非常大的深度网络，训练损失也不会迅速趋向于0。其次，dropout倾向于略微提升模型对新数据的预测能力。这种效果通常适用于各种数据集，这也是dropout被认为是一种强大的发明而不仅仅是一个简单的统计技巧的部分原因。
- en: You should note that dropout should be turned off when making predictions. Forgetting
    to turn off dropout can cause predictions to be much noisier and less useful than
    they would be otherwise. We discuss how to handle dropout for training and predictions
    correctly later in the chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意，在进行预测时应关闭dropout。忘记关闭dropout可能导致预测比原本更加嘈杂和无用。我们将在本章后面正确讨论如何处理训练和预测中的dropout。
- en: How Can Big Networks Not Overfit?
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型网络如何避免过拟合？
- en: One of the most jarring points for classically trained statisticians is that
    deep networks may routinely have more internal degrees of freedom than are present
    in the training data. In classical statistics, the presence of these extra degrees
    of freedom would render the model useless, since there will no longer exist a
    guarantee that the model learned is “real” in the classical sense.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传统训练有素的统计学家来说，最令人震惊的一点是，深度网络可能经常具有比训练数据中存在的内部自由度更多的内部自由度。在传统统计学中，这些额外的自由度的存在会使模型变得无用，因为不再存在一个保证模型学到的是“真实”的经典意义上的保证。
- en: How then can a deep network with millions of parameters learn meaningful results
    on datasets with only thousands of exemplars? Dropout can make a big difference
    here and prevent brute memorization. But, there’s also a deeper unexplained mystery
    in that deep networks will tend to learn useful facts even in the absence of dropout.
    This tendency might be due to some quirk of backpropagation or fully connected
    network structure that we don’t yet understand.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一个拥有数百万参数的深度网络如何能够在只有数千个示例的数据集上学习到有意义的结果呢？Dropout可以在这里起到很大的作用，防止蛮力记忆。但是，即使没有使用dropout，深度网络也会倾向于学习到有用的事实，这种倾向可能是由于反向传播或全连接网络结构的某种特殊性质，我们尚不理解。
- en: Early stopping
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早停止
- en: As mentioned, fully connected networks tend to memorize whatever is put before
    them. As a result, it’s often useful in practice to track the performance of the
    network on a held-out “validation” set and stop the network when performance on
    this validation set starts to go down. This simple technique is known as early
    stopping.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，全连接网络往往会记住放在它们面前的任何东西。因此，在实践中，跟踪网络在一个保留的“验证”集上的表现，并在该验证集上的表现开始下降时停止网络，通常是很有用的。这种简单的技术被称为早停止。
- en: In practice, early stopping can be quite tricky to implement. As you will see,
    loss curves for deep networks can vary quite a bit in the course of normal training.
    Devising a rule that separates healthy variation from a marked downward trend
    can take significant effort. In practice, many practitioners just train models
    with differing (fixed) numbers of epochs, and choose the model that does best
    on the validation set. [Figure 4-9](#ch4-traintest) illustrates how training and
    test set accuracy typically change as training proceeds.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，早停止可能会很棘手。正如你将看到的，深度网络的损失曲线在正常训练过程中可能会有很大的变化。制定一个能够区分健康变化和明显下降趋势的规则可能需要很大的努力。在实践中，许多从业者只是训练具有不同（固定）时代数量的模型，并选择在验证集上表现最好的模型。[图4-9](#ch4-traintest)展示了训练和测试集准确率随着训练进行而通常变化的情况。
- en: '![earlystopping.png](assets/tfdl_0409.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![earlystopping.png](assets/tfdl_0409.png)'
- en: Figure 4-9\. Model accuracy on training and test sets as training proceeds.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9. 训练和测试集的模型准确率随着训练进行而变化。
- en: We will dig more into proper methods for working with validation sets in the
    following chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中更深入地探讨与验证集一起工作的正确方法。
- en: Weight regularization
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重正则化
- en: A classical regularization technique drawn from the statistical literature penalizes
    learned weights that grow large. Following notation from the previous chapter,
    let <math alttext="script upper L left-parenthesis x comma y right-parenthesis"><mrow><mi>ℒ</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> denote the
    loss function for a particular model and let <math><mi>θ</mi></math> denote the
    learnable parameters of this model. Then the regularized loss function is defined
    by
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学文献中借鉴的一种经典正则化技术惩罚那些权重增长较大的学习权重。根据前一章的符号表示，让<math alttext="script upper L
    left-parenthesis x comma y right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>表示特定模型的损失函数，让<math><mi>θ</mi></math>表示该模型的可学习参数。那么正则化的损失函数定义如下
- en: <math display="block"><mrow><msup><mi>ℒ</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>ℒ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo>
    <mi>θ</mi> <mo>∥</mo></mrow></mrow></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>ℒ</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>ℒ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo>
    <mi>θ</mi> <mo>∥</mo></mrow></mrow></math>
- en: where <math alttext="parallel-to theta parallel-to"><mrow><mo>∥</mo> <mi>θ</mi>
    <mo>∥</mo></mrow></math> is the weight penalty and <math alttext="alpha"><mi>α</mi></math>
    is a tunable parameter. The two common choices for penalty are the *L*¹ and *L*²
    penalties
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="parallel-to theta parallel-to"><mrow><mo>∥</mo> <mi>θ</mi>
    <mo>∥</mo></mrow></math>是权重惩罚，<math alttext="alpha"><mi>α</mi></math>是一个可调参数。惩罚的两种常见选择是*L*¹和*L*²惩罚
- en: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>θ</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math><math
    display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow> <mn>1</mn></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mrow><mo>|</mo> <msub><mi>θ</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi>θ</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math><math
    display="block"><mrow><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow> <mn>1</mn></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <mrow><mo>|</mo> <msub><mi>θ</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>
- en: where <math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></math> and <math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>1</mn></msub></math> denote the *L*¹ and *L*² penalties, respectively. From
    personal experience, these penalties tend to be less useful for deep models than
    dropout and early stopping. Some practitioners still make use of weight regularization,
    so it’s worth understanding how to apply these penalties when tuning deep networks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></math>和<math alttext="parallel-to theta parallel-to"><msub><mrow><mo>∥</mo><mi>θ</mi><mo>∥</mo></mrow>
    <mn>1</mn></msub></math>分别表示*L*¹和*L*²的惩罚。从个人经验来看，这些惩罚对于深度模型来说往往不如dropout和早停止有用。一些从业者仍然使用权重正则化，因此值得了解如何在调整深度网络时应用这些惩罚。
- en: Training Fully Connected Networks
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练全连接网络
- en: Training fully connected networks requires a few tricks beyond those you have
    seen so far in this book. First, unlike in the previous chapters, we will train
    models on larger datasets. For these datasets, we will show you how to use minibatches
    to speed up gradient descent. Second, we will return to the topic of tuning learning
    rates.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练全连接网络需要一些技巧，超出了您在本书中迄今为止看到的内容。首先，与之前的章节不同，我们将在更大的数据集上训练模型。对于这些数据集，我们将向您展示如何使用minibatches来加速梯度下降。其次，我们将回到调整学习率的话题。
- en: Minibatching
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Minibatching
- en: For large datasets (which may not even fit in memory), it isn’t feasible to
    compute gradients on the full dataset at each step. Rather, practitioners often
    select a small chunk of data (typically 50–500 datapoints) and compute the gradient
    on these datapoints. This small chunk of data is traditionally called a minibatch.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集（甚至可能无法完全装入内存），在每一步计算梯度时无法在整个数据集上进行。相反，从业者通常选择一小部分数据（通常是50-500个数据点）并在这些数据点上计算梯度。这小部分数据传统上被称为一个minibatch。
- en: In practice, minibatching seems to help convergence since more gradient descent
    steps can be taken with the same amount of compute. The correct size for a minibatch
    is an empirical question often set with hyperparameter tuning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，minibatching似乎有助于收敛，因为可以在相同的计算量下进行更多的梯度下降步骤。minibatch的正确大小是一个经验性问题，通常通过超参数调整来设置。
- en: Learning rates
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate dictates the amount of importance to give to each gradient
    descent step. Setting a correct learning rate can be tricky. Many beginning deep-learners
    set learning rates incorrectly and are surprised to find that their models don’t
    learn or start returning NaNs. This situation has improved significantly with
    the development of methods such as ADAM that simplify choice of learning rate
    significantly, but it’s worth tweaking the learning rate if models aren’t learning
    anything.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率决定了每个梯度下降步骤的重要性。设置正确的学习率可能会有些棘手。许多初学者设置学习率不正确，然后惊讶地发现他们的模型无法学习或开始返回NaN。随着ADAM等方法的发展，这种情况已经得到了显著改善，但如果模型没有学到任何东西，调整学习率仍然是值得的。
- en: Implementation in TensorFlow
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中的实现
- en: In this section, we will show you how to implement a fully connected network
    in TensorFlow. We won’t need to introduce many new TensorFlow primitives in this
    section since we have already covered most of the required basics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将向您展示如何在TensorFlow中实现一个全连接网络。在这一部分中，我们不需要引入太多新的TensorFlow原语，因为我们已经涵盖了大部分所需的基础知识。
- en: Installing DeepChem
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装DeepChem
- en: 'In this section, you will use the DeepChem machine learning toolchain for your
    experiments (full disclosure: one of the authors was the creator of DeepChem).
    [Detailed installation directions](https://deepchem.io) for DeepChem can be found
    online, but briefly the Anaconda installation via the `conda` tool will likely
    be most convenient.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，您将使用DeepChem机器学习工具链进行实验（完整披露：其中一位作者是DeepChem的创始人）。有关DeepChem的[详细安装说明](https://deepchem.io)可以在线找到，但简要地说，通过`conda`工具进行的Anaconda安装可能是最方便的。
- en: Tox21 Dataset
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tox21数据集
- en: For our modeling case study, we will use a chemical dataset. Toxicologists are
    very interested in the task of using machine learning to predict whether a given
    compound will be toxic or not. This task is extremely complicated, since today’s
    science has only a limited understanding of the metabolic processes that happen
    in a human body. However, biologists and chemists have worked out a limited set
    of experiments that provide indications of toxicity. If a compound is a “hit”
    in one of these experiments, it will likely be toxic for a human to ingest. However,
    these experiments are often costly to run, so data scientists aim to build machine
    learning models that can predict the outcomes of these experiments on new molecules.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的建模案例研究中，我们将使用一个化学数据集。毒理学家对使用机器学习来预测给定化合物是否有毒非常感兴趣。这个任务非常复杂，因为当今的科学只对人体内发生的代谢过程有有限的了解。然而，生物学家和化学家已经研究出一套有限的实验，可以提供毒性的指示。如果一个化合物在这些实验中是“命中”的，那么人类摄入后可能会有毒。然而，这些实验通常成本很高，因此数据科学家旨在构建能够预测这些实验结果的机器学习模型，用于新分子。
- en: One of the most important toxicological dataset collections is called Tox21\.
    It was released by the NIH and EPA as part of a data science initiative and was
    used as the dataset in a model building challenge. The winner of this challenge
    used multitask fully connected networks (a variant of fully connected networks
    where each network predicts multiple quantities for each datapoint). We will analyze
    one of the datasets from the Tox21 collection. This dataset consists of a set
    of 10,000 molecules tested for interaction with the androgen receptor. The data
    science challenge is to predict whether new molecules will interact with the androgen
    receptor.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的毒理学数据集之一称为 Tox21。它由 NIH 和 EPA 发布，作为数据科学倡议的一部分，并被用作模型构建挑战中的数据集。这个挑战的获胜者使用了多任务全连接网络（全连接网络的一种变体，其中每个网络为每个数据点预测多个数量）。我们将分析来自
    Tox21 集合中的一个数据集。该数据集包含一组经过测试与雄激素受体相互作用的 10,000 种分子。数据科学挑战是预测新分子是否会与雄激素受体相互作用。
- en: Processing this dataset can be tricky, so we will make use of the MoleculeNet
    dataset collection curated as part of DeepChem. Each molecule in Tox21 is processed
    into a bit-vector of length 1024 by DeepChem. Loading the dataset is then a few
    simple calls into DeepChem ([Example 4-1](#ch4-tox21load)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个数据集可能有些棘手，因此我们将利用 DeepChem 部分作为 MoleculeNet 数据集收集。DeepChem 将 Tox21 中的每个分子处理为长度为
    1024 的比特向量。然后加载数据集只需几个简单的调用到 DeepChem 中（[示例 4-1](#ch4-tox21load)）。
- en: Example 4-1\. Load the Tox21 dataset
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 加载 Tox21 数据集
- en: '[PRE0]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here the `X` variables hold processed feature vectors, `y` holds labels, and
    `w` holds example weights. The labels are binary 1/0 for compounds that interact
    or don’t interact with the androgen receptor. Tox21 holds *imbalanced* datasets,
    where there are far fewer positive examples than negative examples. `w` holds
    recommended per-example weights that give more emphasis to positive examples (increasing
    the importance of rare examples is a common technique for handling imbalanced
    datasets). We won’t use these weights during training for simplicity. All of these
    variables are NumPy arrays.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `X` 变量保存处理过的特征向量，`y` 保存标签，`w` 保存示例权重。标签是与雄激素受体相互作用或不相互作用的化合物的二进制 1/0。Tox21
    拥有*不平衡*数据集，其中正例远远少于负例。`w` 保存建议的每个示例权重，给予正例更多的重视（增加罕见示例的重要性是处理不平衡数据集的常见技术）。为简单起见，我们在训练过程中不使用这些权重。所有这些变量都是
    NumPy 数组。
- en: Tox21 has more datasets than we will analyze here, so we need to remove the
    labels associated with these extra datasets ([Example 4-2](#ch4-tox21trim)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Tox21 拥有比我们这里将要分析的更多数据集，因此我们需要删除与这些额外数据集相关联的标签（[示例 4-2](#ch4-tox21trim)）。
- en: Example 4-2\. Remove extra datasets from Tox21
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 从 Tox21 中删除额外的数据集
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Accepting Minibatches of Placeholders
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接受占位符的小批量
- en: 'In the previous chapters, we created placeholders that accepted arguments of
    fixed size. When dealing with minibatched data, it is often convenient to be able
    to feed batches of variable size. Suppose that a dataset has 947 elements. Then
    with a minibatch size of 50, the last batch will have 47 elements. This would
    cause the code in [Chapter 3](ch03.html#linear_and_logistic_regression_with_tensorflow)
    to crash. Luckily, TensorFlow has a simple fix to the situation: using `None`
    as a dimensional argument to a placeholder allows the placeholder to accept tensors
    with arbitrary size in that dimension ([Example 4-3](#ch4-tox21place)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们创建了接受固定大小参数的占位符。在处理小批量数据时，能够输入不同大小的批次通常很方便。假设一个数据集有 947 个元素。那么以小批量大小为
    50，最后一个批次将有 47 个元素。这将导致 [第 3 章](ch03.html#linear_and_logistic_regression_with_tensorflow)
    中的代码崩溃。幸运的是，TensorFlow 对这种情况有一个简单的解决方法：使用 `None` 作为占位符的维度参数允许占位符在该维度上接受任意大小的张量（[示例 4-3](#ch4-tox21place)）。
- en: Example 4-3\. Defining placeholders that accept minibatches of different sizes
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. 定义接受不同大小小批量的占位符
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note `d` is 1024, the dimensionality of our feature vectors.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `d` 是 1024，即我们特征向量的维度。
- en: Implementing a Hidden Layer
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现隐藏层
- en: The code to implement a hidden layer is very similar to code we’ve seen in the
    last chapter for implementing logistic regression, as shown in [Example 4-4](#ch4-tox21hidden).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 实现隐藏层的代码与我们在上一章中看到的用于实现逻辑回归的代码非常相似，如 [示例 4-4](#ch4-tox21hidden) 所示。
- en: Example 4-4\. Defining a hidden layer
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. 定义一个隐藏层
- en: '[PRE3]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use a `tf.name_scope` to group together introduced variables. Note that we
    use the matricial form of the fully connected layer. We use the form *xW* instead
    of *Wx* in order to deal more conveniently with a minibatch of input at a time.
    (As an exercise, try working out the dimensions involved to see why this is so.)
    Finally, we apply the ReLU nonlinearity with the built-in `tf.nn.relu` activation
    function.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.name_scope` 将引入的变量分组在一起。请注意，我们使用全连接层的矩阵形式。我们使用形式 *xW* 而不是 *Wx*，以便更方便地处理一次输入的小批量。（作为练习，尝试计算涉及的维度，看看为什么会这样。）最后，我们使用内置的
    `tf.nn.relu` 激活函数应用 ReLU 非线性。
- en: The remainder of the code for the fully connected layer is quite similar to
    that used for the logistic regression in the previous chapter. For completeness,
    we display the full code used to specify the network in [Example 4-5](#ch4-tox21fcnet).
    As a quick reminder, the full code for all models covered is available in the
    GitHub repo associated with this book. We strongly encourage you to try running
    the code for yourself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接层的其余代码与上一章中用于逻辑回归的代码非常相似。为了完整起见，我们展示了用于指定网络的完整代码在[例4-5](#ch4-tox21fcnet)中使用。作为一个快速提醒，所有模型的完整代码都可以在与本书相关的GitHub存储库中找到。我们强烈建议您尝试运行代码。
- en: Example 4-5\. Defining the fully connected architecture
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例4-5。定义完全连接的架构
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Adding Dropout to a Hidden Layer
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向隐藏层添加dropout
- en: TensorFlow takes care of implementing dropout for us in the built-in primitive
    `tf.nn.dropout(x, keep_prob)`, where `keep_prob` is the probability that any given
    node is kept. Recall from our earlier discussion that we want to turn on dropout
    when training and turn off dropout when making predictions. To handle this correctly,
    we will introduce a new placeholder for `keep_prob`, as shown in [Example 4-6](#ch4-tox21keep).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow负责为我们实现dropout，内置原语`tf.nn.dropout(x, keep_prob)`，其中`keep_prob`是保留任何给定节点的概率。回想一下我们之前的讨论，我们希望在训练时打开dropout，在进行预测时关闭dropout。为了正确处理这一点，我们将引入一个新的占位符`keep_prob`，如[例4-6](#ch4-tox21keep)所示。
- en: Example 4-6\. Add a placeholder for dropout probability
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例4-6。为丢失概率添加一个占位符
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: During training, we pass in the desired value, often 0.5, but at test time we
    set `keep_prob` to 1.0 since we want predictions made with all learned nodes.
    With this setup, adding dropout to the fully connected network specified in the
    previous section is simply a single extra line of code ([Example 4-7](#ch4-tox21drophidden)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们传入所需的值，通常为0.5，但在测试时，我们将`keep_prob`设置为1.0，因为我们希望使用所有学习节点进行预测。通过这种设置，在前一节中指定的完全连接网络中添加dropout只是一行额外的代码（[例4-7](#ch4-tox21drophidden)）。
- en: Example 4-7\. Defining a hidden layer with dropout
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例4-7。定义一个带有dropout的隐藏层
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Implementing Minibatching
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现小批量处理
- en: To implement minibatching, we need to pull out a minibatch’s worth of data each
    time we call `sess.run`. Luckily for us, our features and labels are already in
    NumPy arrays, and we can make use of NumPy’s convenient syntax for slicing portions
    of arrays ([Example 4-8](#ch4-tox21minibatch)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现小批量处理，我们需要在每次调用`sess.run`时提取一个小批量的数据。幸运的是，我们的特征和标签已经是NumPy数组，我们可以利用NumPy对数组的方便语法来切片数组的部分（[例4-8](#ch4-tox21minibatch)）。
- en: Example 4-8\. Training on minibatches
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例4-8。在小批量上进行训练
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Evaluating Model Accuracy
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型准确性
- en: To evaluate model accuracy, standard practice requires measuring the accuracy
    of the model on data not used for training (namely the validation set). However,
    the fact that the data is imbalanced makes this tricky. The classification accuracy
    metric we used in the previous chapter simply measures the fraction of datapoints
    that were labeled correctly. However, 95% of data in our dataset is labeled 0
    and only 5% are labeled 1\. As a result the all-0 model (which labels everything
    negative) would achieve 95% accuracy! This isn’t what we want.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的准确性，标准做法要求在未用于训练的数据上测量模型的准确性（即验证集）。然而，数据不平衡使这一点变得棘手。我们在上一章中使用的分类准确度指标简单地衡量了被正确标记的数据点的比例。然而，我们数据集中95%的数据被标记为0，只有5%被标记为1。因此，全0模型（将所有内容标记为负面的模型）将实现95%的准确性！这不是我们想要的。
- en: A better choice would be to increase the weights of positive examples so that
    they count for more. For this purpose, we use the recommended per-example weights
    from MoleculeNet to compute a weighted classification accuracy where positive
    samples are weighted 19 times the weight of negative samples. Under this weighted
    accuracy, the all-0 model would have 50% accuracy, which seems much more reasonable.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的选择是增加正例的权重，使其更重要。为此，我们使用MoleculeNet推荐的每个示例权重来计算加权分类准确性，其中正样本的权重是负样本的19倍。在这种加权准确性下，全0模型的准确率将达到50%，这似乎更为合理。
- en: ForI computing the weighted accuracy, we use the function `accuracy_score(true,
    pred, sample_weight=given_sample_weight)` from `sklearn.metrics`. This function
    has a keyword argument `sample_weight`, which lets us specify the desired weight
    for each datapoint. We use this function to compute the weighted metric on both
    the training and validation sets ([Example 4-9](#ch4-tox21weightaccuracy)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算加权准确性，我们使用`sklearn.metrics`中的函数`accuracy_score(true, pred, sample_weight=given_sample_weight`。这个函数有一个关键字参数`sample_weight`，让我们可以为每个数据点指定所需的权重。我们使用这个函数在训练集和验证集上计算加权指标（[例4-9](#ch4-tox21weightaccuracy)）。
- en: Example 4-9\. Computing a weighted accuracy
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例4-9。计算加权准确性
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'While we could reimplement this function ourselves, sometimes it’s easier (and
    less error prone) to use standard functions from the Python data science infrastructure.
    Learning about this infrastructure and available functions is part of being a
    practicing data scientist. Now, we can train the model (for 10 epochs in the default
    setting) and gauge its accuracy:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以自己重新实现这个函数，但有时使用Python数据科学基础设施中的标准函数会更容易（并且更少出错）。了解这种基础设施和可用函数是作为一名实践数据科学家的一部分。现在，我们可以训练模型（在默认设置下进行10个时期）并评估其准确性：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [Chapter 5](ch05.html#hyperparameter_optimization), we will show you methods
    to systematically improve this accuracy and tune our fully connected model more
    carefully.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#hyperparameter_optimization)中，我们将向您展示系统地提高这种准确性的方法，并更仔细地调整我们的完全连接模型。
- en: Using TensorBoard to Track Model Convergence
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard跟踪模型收敛
- en: Now that we have specified our model, let’s use TensorBoard to inspect the model.
    Let’s first check the graph structure in TensorBoard ([Figure 4-10](#ch4-tensorboardfcnet)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经指定了我们的模型，让我们使用TensorBoard来检查模型。让我们首先在TensorBoard中检查图结构（[图4-10](#ch4-tensorboardfcnet)）。
- en: The graph looks similar to that for logistic regression, with the addition of
    a new hidden layer. Let’s expand the hidden layer to see what’s inside ([Figure 4-11](#ch4-tensorboardfcnetexp)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该图与逻辑回归的图类似，只是增加了一个新的隐藏层。让我们扩展隐藏层，看看里面有什么（[图4-11](#ch4-tensorboardfcnetexp)）。
- en: '![fcgraph.png](assets/tfdl_0410.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![fcgraph.png](assets/tfdl_0410.png)'
- en: Figure 4-10\. Visualizing the computation graph for a fully connected network.
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。可视化全连接网络的计算图。
- en: '![hidden_expand.png](assets/tfdl_0411.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![hidden_expand.png](assets/tfdl_0411.png)'
- en: Figure 4-11\. Visualizing the expanded computation graph for a fully connected
    network.
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。可视化全连接网络的扩展计算图。
- en: You can see how the new trainable variables and the dropout operation are represented
    here. Everything looks to be in the right place. Let’s end now by looking at the
    loss curve over time ([Figure 4-12](#ch4-tensorboardfcnetloss)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这里如何表示新的可训练变量和dropout操作。一切看起来都在正确的位置。让我们通过查看随时间变化的损失曲线来结束（[图4-12](#ch4-tensorboardfcnetloss)）。
- en: '![fcnet_loss_curve.png](assets/tfdl_0412.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![fcnet_loss_curve.png](assets/tfdl_0412.png)'
- en: Figure 4-12\. Visualizing the loss curve for a fully connected network.
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。可视化全连接网络的损失曲线。
- en: The loss curve trends down as we saw in the previous section. But, let’s zoom
    in to see what this loss looks like up close ([Figure 4-13](#ch4-tensorboardfcnetlosszoom)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的那样，损失曲线呈下降趋势。但是，让我们放大一下，看看这个损失在近距离下是什么样子的（[图4-13](#ch4-tensorboardfcnetlosszoom)）。
- en: '![fcnet_zoomed_loss.png](assets/tfdl_0413.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![fcnet_zoomed_loss.png](assets/tfdl_0413.png)'
- en: Figure 4-13\. Zooming in on a section of the loss curve.
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13。放大损失曲线的一部分。
- en: Note that loss looks much bumpier! This is one of the prices of using minibatch
    training. We no longer have the beautiful, smooth loss curves that we saw in the
    previous sections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，损失看起来更加崎岖！这是使用小批量训练的代价之一。我们不再拥有在前几节中看到的漂亮、平滑的损失曲线。
- en: Review
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: In this chapter, we’ve introduced you to fully connected deep networks. We delved
    into the mathematical theory of these networks, and explored the concept of “universal
    approximation,” which partially explains the learning power of fully connected
    networks. We ended with a case study, where you trained a deep fully connected
    architecture on the Tox21 dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了全连接深度网络。我们深入研究了这些网络的数学理论，并探讨了“通用逼近”的概念，这在一定程度上解释了全连接网络的学习能力。我们以一个案例研究结束，您在该案例中训练了一个深度全连接架构的Tox21数据集。
- en: In this chapter, we haven’t yet shown you how to tune the fully connected network
    to achieve good predictive performance. In [Chapter 5](ch05.html#hyperparameter_optimization),
    we will discuss “hyperparameter optimization,” the process of tuning network parameters,
    and have you tune the parameters of the Tox21 network introduced in this chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还没有向您展示如何调整全连接网络以实现良好的预测性能。在[第5章](ch05.html#hyperparameter_optimization)中，我们将讨论“超参数优化”，即调整网络参数的过程，并让您调整本章介绍的Tox21网络的参数。
