- en: Chapter 6\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。卷积神经网络
- en: Convolutional neural networks allow deep networks to learn functions on structured
    spatial data such as images, video, and text. Mathematically, convolutional networks
    provide tools for exploiting the local structure of data effectively. Images satisfy
    certain natural statistical properties. Let’s assume we represent an image as
    a two-dimensional grid of pixels. Parts of an image that are close to one other
    in the pixel grid are likely to vary together (for example, all pixels corresponding
    to a table in the image are probably brown). Convolutional networks learn to exploit
    this natural covariance structure in order to learn effectively.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络允许深度网络学习结构化空间数据（如图像、视频和文本）上的函数。从数学上讲，卷积网络提供了有效利用数据局部结构的工具。图像满足某些自然的统计特性。让我们假设将图像表示为像素的二维网格。在像素网格中彼此接近的图像部分很可能一起变化（例如，图像中对应桌子的所有像素可能都是棕色的）。卷积网络学会利用这种自然的协方差结构以有效地学习。
- en: Convolutional networks are a relatively old invention. Versions of convolutional
    networks have been proposed in the literature dating back to the 1980s. While
    the designs of these older convolutional networks were often quite sound, they
    required resources that exceeded hardware available at the time. As a result,
    convolutional networks languished in relative obscurity in the research literature.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络是一个相对古老的发明。卷积网络的版本早在上世纪80年代就在文献中提出过。虽然这些旧卷积网络的设计通常相当合理，但它们需要超过当时可用硬件的资源。因此，卷积网络在研究文献中相对默默无闻。
- en: This trend reversed dramatically following the 2012 ILSVRC challenge for object
    detection in images, where the convolutional AlexNet achieved error rates half
    that of its nearest competitors. AlexNet was able to use GPUs to train old convolutional
    architectures on dramatically larger datasets. This combination of old architectures
    with new hardware allowed AlexNet to dramatically outperform the state of the
    art in image object detection. This trend has only continued, with convolutional
    neural networks achieving tremendous boosts over other technologies for processing
    images. It isn’t an exaggeration to say that nearly all modern image processing
    pipelines are now powered by convolutional neural networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一趋势在2012年ILSVRC挑战赛中戏剧性地逆转，该挑战赛是关于图像中物体检测的，卷积网络AlexNet实现的错误率是其最近竞争对手的一半。AlexNet能够利用GPU在大规模数据集上训练旧的卷积架构。这种旧架构与新硬件的结合使得AlexNet能够在图像物体检测领域显著超越现有技术。这一趋势仅在继续，卷积神经网络在处理图像方面取得了巨大的提升。几乎可以说，现代几乎所有的图像处理流程现在都由卷积神经网络驱动。
- en: There has also been a renaissance in convolutional network design that has moved
    convolutional networks well past the basic models from the 1980s. For one, networks
    have been getting much deeper with powerful state-of-the-art networks reaching
    hundreds of layers deep. Another broad trend has been toward generalizing convolutional
    architectures to work on new datatypes. For example, graph convolutional architectures
    allow convolutional networks to be applied to molecular data such as the Tox21
    dataset we encountered a few chapters ago! Convolutional architectures are also
    making a mark in genomics and in text processing and even language translation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络设计也经历了复兴，将卷积网络推进到远远超过上世纪80年代基本模型的水平。首先，网络变得更加深层，强大的最新网络达到了数百层深度。另一个广泛的趋势是将卷积架构泛化到适用于新数据类型。例如，图卷积架构允许卷积网络应用于分子数据，如我们在前几章中遇到的Tox21数据集！卷积架构还在基因组学、文本处理甚至语言翻译中留下了痕迹。
- en: In this chapter, we will introduce the basic concepts of convolutional networks.
    These will include the basic network components that constitute convolutional
    architectures and an introduction to the design principles that guide how these
    pieces are joined together. We will also provide an in-depth example that demonstrates
    how to use TensorFlow to train a convolutional network. The example code for this
    chapter was adapted from the TensorFlow documentation tutorial on convolutional
    neural networks. We encourage you to access [the original tutorial](https://www.tensorflow.org/tutorials/deep_cnn)
    on the TensorFlow website if you’re curious about the changes we’ve made. As always,
    we encourage you to work through the scripts for this chapter in the associated
    [GitHub repo for this book](https://github.com/matroid/dlwithtf).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍卷积网络的基本概念。这些将包括构成卷积架构的基本网络组件，以及指导这些组件如何连接的设计原则的介绍。我们还将提供一个深入的示例，演示如何使用TensorFlow训练卷积网络。本章的示例代码改编自TensorFlow文档中有关卷积神经网络的教程。如果您对我们所做的更改感兴趣，请访问TensorFlow网站上的[原始教程](https://www.tensorflow.org/tutorials/deep_cnn)。与往常一样，我们鼓励您在本书的相关[GitHub存储库](https://github.com/matroid/dlwithtf)中逐步完成本章的脚本。
- en: Introduction to Convolutional Architectures
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积架构简介
- en: Most convolutional architectures are made up of a number of basic primitives.
    These primitives include layers such as convolutional layers and pooling layers.
    There’s also a set of associated vocabulary including local receptive field size,
    stride size, and number of filters. In this section, we will give you a brief
    introduction to the basic vocabulary and concepts underlying convolutional networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数卷积架构由许多基本原语组成。这些原语包括卷积层和池化层等层。还有一组相关的词汇，包括局部感受野大小、步幅大小和滤波器数量。在本节中，我们将简要介绍卷积网络基本词汇和概念的基础。
- en: Local Receptive Fields
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部感受野
- en: The local receptive field concept originates in neuroscience, where the receptive
    field of a neuron is the part of the body’s sensory perception that affects the
    neuron’s firing. Neurons have a certain field of “view” as they process sensory
    input that the brain sees. This field of view is traditionally called the local
    receptive field. This “field of view” could correspond to a patch of skin or to
    a segment of a person’s visual field. [Figure 6-1](#ch6-localrec) illustrates
    a neuron’s local receptive field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 局部感受野的概念源自神经科学，神经元的感受野是影响神经元放电的身体感知部分。神经元在处理大脑看到的感官输入时有一定的“视野”。这个视野传统上被称为局部感受野。这个“视野”可以对应于皮肤的一小块或者一个人的视野的一部分。[图6-1](#ch6-localrec)展示了一个神经元的局部感受野。
- en: '![neuron_receptive_field.png](assets/tfdl_0601.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![neuron_receptive_field.png](assets/tfdl_0601.png)'
- en: Figure 6-1\. An illustration of a neuron’s local receptive field.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 一个神经元的局部感受野的插图。
- en: Convolutional architectures borrow this latter concept with the computational
    notion of “local receptive fields.” [Figure 6-2](#ch6-localrecconv) provides a
    pictorial representation of the local receptive field concept applied to image
    data. Each local receptive field corresponds to a patch of pixels in the image
    and is handled by a separate “neuron.” These “neurons” are directly analogous
    to those in fully connected layers. As with fully connected layers, a nonlinear
    transformation is applied to incoming data (which originates from the local receptive
    image patch).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积架构借用了这个概念，计算概念上的“局部感受野”。[图6-2](#ch6-localrecconv)提供了应用于图像数据的局部感受野概念的图示表示。每个局部感受野对应于图像中的一组像素，并由一个单独的“神经元”处理。这些“神经元”与全连接层中的神经元直接类似。与全连接层一样，对传入数据（源自局部感受图像补丁）应用非线性变换。
- en: '![local_receiptive_input.png](assets/tfdl_0602.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![local_receiptive_input.png](assets/tfdl_0602.png)'
- en: Figure 6-2\. The local receptive field (RF) of a “neuron” in a convolutional
    network.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 卷积网络中“神经元”的局部感受野（RF）。
- en: A layer of such “convolutional neurons” can be combined into a convolutional
    layer. This layer can viewed as a transformation of one spatial region into another.
    In the case of images, one batch of images is transformed into another by a convolutional
    layer. [Figure 6-3](#ch6-convlayer) illustrates such a transformation. In the
    next section, we will show you more details about how a convolutional layer is
    constructed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的“卷积神经元”层可以组合成一个卷积层。这一层可以被看作是一个空间区域到另一个空间区域的转换。在图像的情况下，一个批次的图像通过卷积层被转换成另一个。[图6-3](#ch6-convlayer)展示了这样的转换。在接下来的部分，我们将向您展示卷积层是如何构建的更多细节。
- en: '![conv_receptive.png](assets/tfdl_0603.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![conv_receptive.png](assets/tfdl_0603.png)'
- en: Figure 6-3\. A convolutional layer performs an image transformation.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 一个卷积层执行图像转换。
- en: It’s worth emphasizing that local receptive fields don’t have to be limited
    to image data. For example, in stacked convolutional architectures, where the
    output of one convolutional layer feeds into the input of the next, the local
    receptive field will correspond to a “patch” of processed feature data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，局部感受野不一定局限于图像数据。例如，在堆叠的卷积架构中，其中一个卷积层的输出馈送到下一个卷积层的输入，局部感受野将对应于处理过的特征数据的“补丁”。
- en: Convolutional Kernels
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积核
- en: In the last section, we mentioned that a convolutional layer applies nonlinear
    function to a local receptive field in its input. This locally applied nonlinearity
    is at the heart of convolutional architectures, but it’s not the only piece. The
    second part of the convolution is what’s called a “convolutional kernel.” A convolutional
    kernel is just a matrix of weights, much like the weights associated with a fully
    connected layer. [Figure 6-4](#ch6-convkernel) diagrammatically illustrates how
    a convolutional kernel is applied to inputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们提到卷积层对其输入中的局部感受野应用非线性函数。这种局部应用的非线性是卷积架构的核心，但不是唯一的部分。卷积的第二部分是所谓的“卷积核”。卷积核只是一个权重矩阵，类似于与全连接层相关联的权重。[图6-4](#ch6-convkernel)以图解的方式展示了卷积核如何应用到输入上。
- en: '![sliding_kernal.png](assets/tfdl_0604.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![sliding_kernal.png](assets/tfdl_0604.png)'
- en: Figure 6-4\. A convolutional kernel is applied to inputs. The kernel weights
    are multiplied elementwise with the corresponding numbers in the local receptive
    field and the multiplied numbers are summed. Note that this corresponds to a convolutional
    layer without a nonlinearity.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 一个卷积核被应用到输入上。卷积核的权重与局部感受野中对应的数字逐元素相乘，相乘的数字相加。请注意，这对应于一个没有非线性的卷积层。
- en: The key idea behind convolutional networks is that the same (nonlinear) transformation
    is applied to every local receptive field in the image. Visually, picture the
    local receptive field as a sliding window dragged over the image. At each positioning
    of the local receptive field, the nonlinear function is applied to return a single
    number corresponding to that image patch. As [Figure 6-4](#ch6-convkernel) demonstrates,
    this transformation turns one grid of numbers into another grid of numbers. For
    image data, it’s common to label the size of the local receptive field in terms
    of the number of pixels on each size of the receptive field. For example, 5 ×
    5 and 7 × 7 local receptive field sizes are commonly seen in convolutional networks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络背后的关键思想是相同的（非线性）转换应用于图像中的每个局部感受野。在视觉上，将局部感受野想象成在图像上拖动的滑动窗口。在每个局部感受野的位置，非线性函数被应用以返回与该图像补丁对应的单个数字。正如[图6-4](#ch6-convkernel)所示，这种转换将一个数字网格转换为另一个数字网格。对于图像数据，通常以每个感受野大小的像素数来标记局部感受野的大小。例如，在卷积网络中经常看到5×5和7×7的局部感受野大小。
- en: What if we want to specify that local receptive fields should not overlap? The
    way to do this is to alter the *stride size* of the convolutional kernel. The
    stride size controls how the receptive field is moved over the input. [Figure 6-4](#ch6-convkernel)
    demonstrates a one-dimensional convolutional kernel, with stride sizes 1 and 2,
    respectively. [Figure 6-5](#ch6-stride) illustrates how altering the stride size
    changes how the receptive field is moved over the input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要指定局部感受野不重叠怎么办？这样做的方法是改变卷积核的*步幅大小*。步幅大小控制感受野在输入上的移动方式。[图6-4](#ch6-convkernel)展示了一个一维卷积核，分别具有步幅大小1和2。[图6-5](#ch6-stride)说明了改变步幅大小如何改变感受野在输入上的移动方式。
- en: '![stride_size.png](assets/tfdl_0605.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![stride_size.png](assets/tfdl_0605.png)'
- en: Figure 6-5\. The stride size controls how the local receptive field “slides”
    over the input. This is easiest to visualize on a one-dimensional input. The network
    on the left has stride 1, while that on the right has stride 2\. Note that each
    local receptive field computes the maximum of its inputs.
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。步幅大小控制局部感受野在输入上的“滑动”。这在一维输入上最容易可视化。左侧的网络步幅为1，而右侧的网络步幅为2。请注意，每个局部感受野计算其输入的最大值。
- en: Now, note that the convolutional kernel we have defined transforms a grid of
    numbers into another grid of numbers. What if we want more than one grid of numbers
    output? It’s easy enough; we simply need to add more convolutional kernels for
    processing the image. Convolutional kernels are also called *filters*, so the
    number of filters in a convolutional layer controls the number of transformed
    grids we obtain. A collection of convolutional kernels forms a *convolutional
    layer*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意我们定义的卷积核将一个数字网格转换为另一个数字网格。如果我们想要输出多个数字网格怎么办？这很容易；我们只需要添加更多的卷积核来处理图像。卷积核也称为*滤波器*，因此卷积层中的滤波器数量控制我们获得的转换网格数量。一组卷积核形成一个*卷积层*。
- en: Convolutional Kernels on Multidimensional Inputs
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维输入上的卷积核
- en: In this section, we primarily described convolutional kernels as transforming
    grids of numbers into other grids of numbers. Recalling our tensorial language
    from earlier chapters, convolutions transform matrices into matrices.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要将卷积核描述为将数字网格转换为其他数字网格。回想一下我们在前几章中使用的张量语言，卷积将矩阵转换为矩阵。
- en: What if your input has more dimensions? For example, an RGB image typically
    has three color channels, so an RGB image is rightfully a rank-3 tensor. The simplest
    way to handle RGB data is to dictate that each local receptive field includes
    all the color channels associated with pixels in that patch. You might then say
    that the local receptive field is of size 5 × 5 × 3 for a local receptive field
    of size 5 × 5 pixels with three color channels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的输入具有更多维度怎么办？例如，RGB图像通常具有三个颜色通道，因此RGB图像在正确的情况下是一个秩为3的张量。处理RGB数据的最简单方法是规定每个局部感受野包括与该补丁中的像素相关联的所有颜色通道。然后，您可以说局部感受野的大小为5×5×3，对于一个大小为5×5像素且具有三个颜色通道的局部感受野。
- en: In general, you can generalize to tensors of higher dimension by expanding the
    dimensionality of the local receptive field correspondingly. This may also necessitate
    having multidimensional strides, especially if different dimensions are to be
    handled separately. The details are straightforward to work out, and we leave
    exploration of multidimensional convolutional kernels as an exercise for you to
    undertake.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您可以通过相应地扩展局部感受野的维度来将高维张量推广到更高维度的张量。这可能还需要具有多维步幅，特别是如果要分别处理不同维度。细节很容易解决，我们将探索多维卷积核作为您要进行的练习。
- en: Pooling Layers
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: In the previous section, we introduced the notion of convolutional kernels.
    These kernels apply learnable nonlinear transformations to local patches of inputs.
    These transformations are learnable, and by the universal approximation theorem,
    capable of learning arbitrarily complex input transformations on local patches.
    This flexibility gives convolutional kernels much of their power. But at the same
    time, having many learnable weights in a deep convolutional network can slow training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了卷积核的概念。这些核将可学习的非线性变换应用于输入的局部补丁。这些变换是可学习的，并且根据通用逼近定理，能够学习局部补丁上任意复杂的输入变换。这种灵活性赋予了卷积核很大的能力。但同时，在深度卷积网络中具有许多可学习权重可能会减慢训练速度。
- en: Instead of using a learnable transformation, it’s possible to instead use a
    fixed nonlinear transformation in order to reduce the computational cost of training
    a convolutional network. A popular fixed nonlinearity is “max pooling.” Such layers
    select and output the maximally activating input within each local receptive patch.
    [Figure 6-6](#ch6-pool) demonstrates this process. Pooling layers are useful for
    reducing the dimensionality of input data in a structured fashion. More mathematically,
    they take a local receptive field and replace the nonlinear activation function
    at each portion of the field with the max (or min or average) function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用可学习变换不同，可以使用固定的非线性变换来减少训练卷积网络的计算成本。一种流行的固定非线性是“最大池化”。这样的层选择并输出每个局部感受补丁中激活最大的输入。[图6-6](#ch6-pool)展示了这个过程。池化层有助于以结构化方式减少输入数据的维度。更具体地说，它们采用局部感受野，并用最大（或最小或平均）函数替换字段的每个部分的非线性激活函数。
- en: '![maxpool.jpeg](assets/tfdl_0606.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![maxpool.jpeg](assets/tfdl_0606.png)'
- en: Figure 6-6\. An illustration of a max pooling layer. Notice how the maximal
    value in each colored region (each local receptive field) is reported in the output.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。最大池化层的示例。请注意，每个彩色区域（每个局部感受野）中的最大值报告在输出中。
- en: Pooling layers have become less useful as hardware has improved. While pooling
    is still useful as a dimensionality reduction technique, recent research tends
    to avoid using pooling layers due to their inherent lossiness (it’s not possible
    to back out of pooled data which pixel in the input originated the reported activation).
    Nonetheless, pooling appears in many standard convolutional architectures so it’s
    worth understanding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着硬件的改进，池化层变得不那么有用。虽然池化仍然作为一种降维技术很有用，但最近的研究倾向于避免使用池化层，因为它们固有的丢失性（无法从池化数据中推断出输入中的哪个像素产生了报告的激活）。尽管如此，池化出现在许多标准卷积架构中，因此值得理解。
- en: Constructing Convolutional Networks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建卷积网络
- en: A simple convolutional architecture applies a series of convolutional layers
    and pooling layers to its input to learn a complex function on the input image
    data. There are a lot of details in forming these networks, but at its heart,
    architecture design is simply an elaborate form of Lego stacking. [Figure 6-7](#ch6-convarch)
    demonstrates how a convolutional architecture might be built up out of constituent
    blocks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的卷积架构将一系列卷积层和池化层应用于其输入，以学习输入图像数据上的复杂函数。在构建这些网络时有很多细节，但在其核心，架构设计只是一种复杂的乐高堆叠形式。[图6-7](#ch6-convarch)展示了一个卷积架构可能是如何由组成块构建起来的。
- en: '![cnnimage.png](assets/tfdl_0607.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![cnnimage.png](assets/tfdl_0607.png)'
- en: Figure 6-7\. An illustration of a simple convolutional architecture constructed
    out of stacked convolutional and pooling layers.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。一个简单的卷积架构的示例，由堆叠的卷积和池化层构成。
- en: Dilated Convolutions
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 膨胀卷积
- en: Dilated or atrous convolutions are a newly popular form of convolutional layer.
    The insight here is to leave gaps in the local receptive field for each neuron
    (atrous means *a trous*, or “with holes” in French). The basic concept is an old
    one in signal processing that has recently found some good traction in the convolutional
    literature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积或空洞卷积是一种新近流行的卷积层形式。这里的见解是为每个神经元在局部感受野中留下间隙（atrous意味着*a trous*，即法语中的“带孔”）。这个基本概念在信号处理中是一个古老的概念，最近在卷积文献中找到了一些好的应用。
- en: The core advantage to the atrous convolution is the increase in visible area
    for each neuron. Let’s consider a convolution architecture whose first layer is
    a vanilla convolutional with 3 × 3 local receptive fields. Then a neuron one layer
    deeper in the architecture in a second vanilla convolutional layer has receptive
    depth 5 × 5 (each neuron in a local receptive field of the second layer itself
    has a local receptive field in the first layer). Then, a neuron two layers deeper
    has receptive view 7 × 7\. In general, a neuron *N* layers within the convolutional
    architecture has receptive view of size (2*N* + 1) × (2*N* + 1). This linear growth
    in receptive view is fine for smaller images, but quickly becomes a liability
    for large images.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 空洞卷积的核心优势是每个神经元的可见区域增加。让我们考虑一个卷积架构，其第一层是具有3×3局部感受野的普通卷积。然后，在架构中更深一层的第二个普通卷积层中的神经元具有5×5的感受野深度（第二层中局部感受野中的每个神经元本身在第一层中具有局部感受野）。然后，更深的两层的神经元具有7×7的感受视图。一般来说，卷积架构中第*N*层的神经元具有大小为(2*N*
    + 1) × (2*N* + 1)的感受视图。这种感受视图的线性增长对于较小的图像是可以接受的，但对于大型图像很快就会成为一个负担。
- en: The atrous convolution enables exponential growth in the visible receptive field
    by leaving gaps in its local receptive fields. A “1-dilated” convolution leaves
    no gaps, while a “2-dilated” convolution leaves one gap between each local receptive
    field element. Stacking dilated layers leads to exponentially increasing local
    receptive field sizes. [Figure 6-8](#ch6-convdil) illustrates this exponential
    increase.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 空洞卷积通过在其局部感受野中留下间隙实现了可见感受野的指数增长。一个“1-膨胀”卷积不留下间隙，而一个“2-膨胀”卷积在每个局部感受野元素之间留下一个间隙。堆叠膨胀层会导致局部感受野大小呈指数增长。[图6-8](#ch6-convdil)说明了这种指数增长。
- en: Dilated convolutions can be very useful for large images. For example, medical
    images can stretch thousands of pixels in every dimension. Creating vanilla convolutional
    networks that have global understanding could require unreasonably deep networks.
    Using dilated convolutions could enable networks to better understand the global
    structure of such images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积对于大型图像非常有用。例如，医学图像在每个维度上可以延伸到数千个像素。创建具有全局理解的普通卷积网络可能需要不合理深的网络。使用膨胀卷积可以使网络更好地理解这些图像的全局结构。
- en: '![dilated_convolution.png](assets/tfdl_0608.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![dilated_convolution.png](assets/tfdl_0608.png)'
- en: Figure 6-8\. A dilated (or atrous) convolution. Gaps are left in the local receptive
    field for each neuron. Diagram (a) depicts a 1-dilated 3 × 3 convolution. Diagram
    (b) depicts the application of a 2-dilated 3 × 3 convolution to (a). Diagram (c)
    depicts the application of a 4-dilated 3 × 3 convolution to (b). Notice that the
    (a) layer has receptive field of width 3, the (b) layer has receptive field of
    width 7, and the (c) layer has receptive field of width 15.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8。一个膨胀（或空洞）卷积。为每个神经元在局部感受野中留下间隙。图（a）描述了一个1-膨胀的3×3卷积。图（b）描述了将一个2-膨胀的3×3卷积应用于（a）。图（c）描述了将一个4-膨胀的3×3卷积应用于（b）。注意，（a）层的感受野宽度为3，（b）层的感受野宽度为7，（c）层的感受野宽度为15。
- en: Applications of Convolutional Networks
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络的应用
- en: In the previous section, we covered the mechanics of convolutional networks
    and introduced you to many of the components that make up these networks. In this
    section, we describe some applications that convolutional architectures enable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了卷积网络的机制，并向您介绍了构成这些网络的许多组件。在本节中，我们描述了一些卷积架构可以实现的应用。
- en: Object Detection and Localization
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测和定位
- en: Object detection is the task of detecting the objects (or entities) present
    in a photograph. Object localization is the task of identifying where in the image
    the objects exist and drawing a “bounding box” around each occurrence. [Figure 6-9](#ch6-objdet)
    demonstrates what detection and localization on standard images looks like.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是检测照片中存在的对象（或实体）的任务。目标定位是识别图像中对象存在的位置，并在每个出现的位置周围绘制“边界框”的任务。[图6-9](#ch6-objdet)展示了标准图像上检测和定位的样子。
- en: '![detection_and_localization.jpg](assets/tfdl_0609.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![detection_and_localization.jpg](assets/tfdl_0609.png)'
- en: Figure 6-9\. Objects detected and localized with bounding boxes in some example
    images.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9。在一些示例图像中检测和定位的对象，并用边界框标出。
- en: Why is detection and localization important? One very useful localization task
    is detecting pedestrians in images taken from a self-driving car. Needless to
    say, it’s extremely important that a self-driving car be able to identify all
    nearby pedestrians. Other applications of object detection could be used to find
    all instances of friends in photos uploaded to a social network. Yet another application
    could be to identify potential collision dangers from a drone.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么检测和定位很重要？一个非常有用的定位任务是从自动驾驶汽车拍摄的图像中检测行人。不用说，自动驾驶汽车能够识别所有附近的行人是非常重要的。目标检测的其他应用可能用于在上传到社交网络的照片中找到所有朋友的实例。另一个应用可能是从无人机中识别潜在的碰撞危险。
- en: This wealth of applications has made detection and localization the focus of
    tremendous amounts of research activity. The ILSVRC challenge mentioned multiple
    times in this book focused on detecting and localizing objects found in the ImagetNet
    collection.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些丰富的应用使得检测和定位成为大量研究活动的焦点。本书中多次提到的ILSVRC挑战专注于检测和定位在ImagetNet集合中找到的对象。
- en: Image Segmentation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分割
- en: Image segmentation is the task of labeling each pixel in an image with the object
    it belongs to. Segmentation is related to object localization, but is significantly
    harder since it requires precisely understanding the boundaries between objects
    in images. Until recently, image segmentation was often done with graphical models,
    an alternate form of machine learning (as opposed to deep networks), but recently
    convolutional segmentations have risen to prominence and allowed image segmentation
    algorithms to achieve new accuracy and speed records. [Figure 6-10](#ch6-objseg)
    displays an example of image segmentation applied to data for self-driving car
    imagery.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是将图像中的每个像素标记为其所属对象的任务。分割与目标定位相关，但要困难得多，因为它需要准确理解图像中对象之间的边界。直到最近，图像分割通常是通过图形模型完成的，这是一种与深度网络不同的机器学习形式，但最近卷积分割已经崭露头角，并使图像分割算法取得了新的准确性和速度记录。[图6-10](#ch6-objseg)显示了应用于自动驾驶汽车图像数据的图像分割的示例。
- en: '![nvidia_digits.png](assets/tfdl_0610.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![nvidia_digits.png](assets/tfdl_0610.png)'
- en: Figure 6-10\. Objects in an image are “segmented” into various categories. Image
    segmentation is expected to prove very useful for applications such as self-driving
    cars and robotics since it will enable fine-grained scene understanding.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10。图像中的对象被“分割”为各种类别。图像分割预计将对自动驾驶汽车和机器人等应用非常有用，因为它将实现对场景的细粒度理解。
- en: Graph Convolutions
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积
- en: The convolutional algorithms we’ve shown you thus far expect rectangular tensors
    as their inputs. Such inputs could come in the form of images, videos, or even
    sentences. Is it possible to generalize convolutions to apply to irregular inputs?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们向您展示的卷积算法期望其输入为矩形张量。这样的输入可以是图像、视频，甚至句子。是否可能将卷积推广到不规则输入？
- en: The fundamental idea underlying convolutional layers is the notion of a local
    receptive field. Each neuron computes upon the inputs in its local receptive field,
    which typically constitute adjacent pixels in an image input. For irregular inputs,
    such as the undirected graph in [Figure 6-11](#ch6-undgraph), this simple notion
    of a local receptive field doesn’t make sense; there are no adjacent pixels. If
    we can define a more general local receptive field for an undirected graph, it
    stands to reason that we should be able to define convolutional layers that accept
    undirected graphs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层背后的基本思想是局部感受野的概念。每个神经元计算其局部感受野中的输入，这些输入通常构成图像输入中相邻的像素。对于不规则输入，例如[图6-11](#ch6-undgraph)中的无向图，这种简单的局部感受野的概念是没有意义的；没有相邻的像素。如果我们可以为无向图定义一个更一般的局部感受野，那么我们应该能够定义接受无向图的卷积层。
- en: '![graph_example.png](assets/tfdl_0611.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![graph_example.png](assets/tfdl_0611.png)'
- en: Figure 6-11\. An illustration of an undirected graph consisting of nodes connected
    by edges.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11。由边连接的节点组成的无向图的示例。
- en: As [Figure 6-11](#ch6-undgraph) shows, a graph is made up of a collection of
    nodes connected by edges. One potential definition of a local receptive field
    might be to define it to constitute a node and its collection of neighbors (where
    two nodes are considered neighbors if they are connected by an edge). Using this
    definition of local receptive fields, it’s possible to define generalized notions
    of convolutional and pooling layers. These layers can be assembled into graph
    convolutional architectures.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6-11](#ch6-undgraph)所示，图由一组由边连接的节点组成。一个潜在的局部感受野的定义可能是将其定义为一个节点及其邻居的集合（如果两个节点通过边连接，则被认为是邻居）。使用这种局部感受野的定义，可以定义卷积和池化层的广义概念。这些层可以组装成图卷积架构。
- en: Where might such graph convolutional architectures prove useful? In chemistry,
    it turns out molecules can be modeled as undirected graphs where atoms form nodes
    and chemical bonds form edges. As a result, graph convolutional architectures
    are particularly useful in chemical machine learning. For example, [Figure 6-12](#ch6-molgraph)
    demonstrates how graph convolutional architectures can be applied to process molecular
    inputs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种图卷积架构可能在哪些地方证明有用？在化学中，分子可以被建模为原子形成节点，化学键形成边缘的无向图。因此，图卷积架构在化学机器学习中特别有用。例如，[图6-12](#ch6-molgraph)展示了图卷积架构如何应用于处理分子输入。
- en: '![graphconv_graphic_v2.png](assets/tfdl_0612.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![graphconv_graphic_v2.png](assets/tfdl_0612.png)'
- en: Figure 6-12\. An illustration of a graph convolutional architecture processing
    a molecular input. The molecule is modeled as an undirected graph with atoms forming
    nodes and chemical bond edges. The “graph topology” is the undirected graph corresponding
    to the molecule. “Atom features” are vectors, one per atom, summarizing local
    chemistry. Adapted from “Low Data Drug Discovery with One-Shot Learning.”
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。展示了一个图卷积架构处理分子输入的示意图。分子被建模为一个无向图，其中原子形成节点，化学键形成边缘。"图拓扑"是对应于分子的无向图。"原子特征"是向量，每个原子一个，总结了局部化学信息。改编自“一次性学习的低数据药物发现”。
- en: Generating Images with Variational Autoencoders
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变分自动编码器生成图像
- en: The applications we’ve described thus far are all supervised learning problems.
    There are well-defined inputs and outputs, and the task remains (using a convolutional
    network) to learn a sophisticated function mapping input to output. Are there
    unsupervised learning problems that can be solved with convolutional networks?
    Recall that unsupervised learning requires “understanding” the structure of input
    datapoints. For image modeling, a good measure of understanding the structure
    of input images is being able to “sample” new images that come from the input
    distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述的应用都是监督学习问题。有明确定义的输入和输出，任务仍然是（使用卷积网络）学习一个将输入映射到输出的复杂函数。有没有无监督学习问题可以用卷积网络解决？回想一下，无监督学习需要“理解”输入数据点的结构。对于图像建模，理解输入图像结构的一个好的衡量标准是能够“采样”来自输入分布的新图像。
- en: What does “sampling” an image mean? To explain, let’s suppose we have a dataset
    of dog images. Sampling a new dog image requires the generation of a new image
    of a dog that *is not in the training data*! The idea is that we would like a
    picture of a dog that could have reasonably been included with the training data,
    but was not. How could we solve this task with convolutional networks?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是“采样”图像的意思？为了解释，假设我们有一组狗的图像数据集。采样一个新的狗图像需要生成一张*不在训练数据中*的新狗图像！这个想法是，我们希望得到一张狗的图片，这张图片可能已经被包含在训练数据中，但实际上并没有。我们如何用卷积网络解决这个任务？
- en: Perhaps we could train a model to take in word labels like “dog” and predict
    dog images. We might possibly be able to train a supervised model to solve this
    prediction problem, but the issue remains that our model could generate only one
    dog picture given the input label “dog.” Suppose now that we could attach a random
    tag to each dog—say “dog3422” or “dog9879.” Then all we’d need to do to get a
    new dog image would be to attach a new random tag, say “dog2221,” to get out a
    new picture of a dog.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以训练一个模型，输入词标签如“狗”，并预测狗的图像。我们可能能够训练一个监督模型来解决这个预测问题，但问题在于我们的模型只能在输入标签“狗”时生成一张狗的图片。现在假设我们可以给每只狗附加一个随机标签，比如“dog3422”或“dog9879”。那么我们只需要给一只新的狗附加一个新的随机标签，比如“dog2221”，就可以得到一张新的狗的图片。
- en: 'Variational autoencoders formalize these intuitions. Variational autoencoders
    consist of two convolutional networks: the encoder and decoder network. The encoder
    network is used to transform an image into a flat “embedded” vector. The decoder
    network is responsible for transforming the embedded vector into images. Noise
    is added to ensure that different images can be sampled by the decoder. [Figure 6-13](#ch6-varencod)
    illustrates a variational autoencoder.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器形式化了这些直觉。变分自动编码器由两个卷积网络组成：编码器网络和解码器网络。编码器网络用于将图像转换为一个平坦的“嵌入”向量。解码器网络负责将嵌入向量转换为图像。为了确保解码器可以生成不同的图像，会添加噪音。[图6-13](#ch6-varencod)展示了一个变分自动编码器。
- en: '![variational_autoencoder.png](assets/tfdl_0613.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![variational_autoencoder.png](assets/tfdl_0613.png)'
- en: Figure 6-13\. A diagrammatic illustration of a variational autoencoder. A variational
    autoencoder consists of two convolutional networks, the encoder and decoder.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13。变分自动编码器的示意图。变分自动编码器由两个卷积网络组成，编码器和解码器。
- en: There are more details involved in an actual implementation, but variational
    autoencoders are capable of sampling images. However, naive variational encoders
    seem to generate blurry image samples, as [Figure 6-14](#ch6-varencodsamp) demonstrates.
    This blurriness may be because the *L*² loss doesn’t penalize image blurriness
    sharply (recall our discussion about *L*² not penalizing small deviations). To
    generate crisp image samples, we will need other architectures.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中涉及更多细节，但变分自动编码器能够对图像进行采样。然而，朴素的变分编码器似乎生成模糊的图像样本，正如[图6-14](#ch6-varencodsamp)所示。这种模糊可能是因为*L*²损失不会严厉惩罚图像的模糊（回想我们关于*L*²不惩罚小偏差的讨论）。为了生成清晰的图像样本，我们将需要其他架构。
- en: '![variational-autoencoder-faces.jpg](assets/tfdl_0614.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![variational-autoencoder-faces.jpg](assets/tfdl_0614.png)'
- en: Figure 6-14\. Images sampled from a variational autoencoder trained on a dataset
    of faces. Note that sampled images are quite blurry.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。从一个训练有素的人脸数据集上训练的变分自动编码器中采样的图像。请注意，采样的图像非常模糊。
- en: Adversarial models
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗模型
- en: The L2 loss sharply penalizes large local deviations, but doesn’t severely penalize
    many small local deviations, causing blurriness. How could we design an alternate
    loss function that penalizes blurriness in images more sharply? It turns out that
    it’s quite challenging to write down a loss function that does the trick. While
    our eyes can quickly spot blurriness, our analytical tools aren’t quite so fast
    to capture the problem.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: L2损失会严厉惩罚大的局部偏差，但不会严重惩罚许多小的局部偏差，导致模糊。我们如何设计一个替代的损失函数，更严厉地惩罚图像中的模糊？事实证明，编写一个能够解决问题的损失函数是相当具有挑战性的。虽然我们的眼睛可以很快发现模糊，但我们的分析工具并不那么快捕捉到这个问题。
- en: What if we could somehow “learn” a loss function? This idea sounds a little
    nonsensical at first; where would we get training data? But it turns out that
    there’s a clever idea that makes it feasible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够“学习”一个损失函数会怎样？这个想法起初听起来有点荒谬；我们从哪里获取训练数据呢？但事实证明，有一个聪明的想法使这变得可行。
- en: Suppose we could train a separate network that learns the loss. Let’s call this
    network the discriminator. Let’s call the network that makes the images the generator.
    The generator can be set to duel against the discriminator until the generator
    is capable of producing images that are photorealistic. This form of architecture
    is commonly called a generative adversarial network, or GAN.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以训练一个单独的网络来学习损失。让我们称这个网络为鉴别器。让我们称制作图像的网络为生成器。生成器可以与鉴别器对抗，直到生成器能够产生逼真的图像。这种架构通常被称为生成对抗网络，或GAN。
- en: Faces generated by a GAN ([Figure 6-15](#ch6-gansamp)) are considerably crisper
    than those generated by the naive variational autoencoder ([Figure 6-14](#ch6-varencodsamp))!
    There are a number of other promising results that have been achieved by GANs.
    The CycleGAN, for example, appears capable of learning complex image transformations
    such as transmuting horses into zebras and vice versa. [Figure 6-16](#ch6-cycgan)
    shows some CycleGAN image transformations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由GAN生成的面部图像（[图6-15](#ch6-gansamp)）比朴素变分自动编码器生成的图像要清晰得多（[图6-14](#ch6-varencodsamp)）！GAN已经取得了许多其他有希望的成果。例如，CycleGAN似乎能够学习复杂的图像转换，例如将马转变为斑马，反之亦然。[图6-16](#ch6-cycgan)展示了一些CycleGAN图像转换。
- en: '![GAN_faces.png](assets/tfdl_0615.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![GAN_faces.png](assets/tfdl_0615.png)'
- en: Figure 6-15\. Images sampled from a generative adversarial network (GAN) trained
    on a dataset of faces. Note that sampled images are less blurry than those from
    the variational autoencoder.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。从一个在面部数据集上训练的生成对抗网络（GAN）中采样的图像。请注意，采样的图像比变分自动编码器生成的图像更清晰。
- en: '![CycleGAN.jpg](assets/tfdl_0616.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN.jpg](assets/tfdl_0616.png)'
- en: Figure 6-16\. The CycleGAN is capable of performing complex image transformations,
    such as transforming images of horses into those of zebras (and vice versa).
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-16。CycleGAN能够执行复杂的图像转换，例如将马的图像转换为斑马的图像（反之亦然）。
- en: Unfortunately, generative adversarial networks are still challenging to train
    in practice. Making generators and discriminators learn reasonable functions requires
    a deep bag of tricks. As a result, while there have been many exciting GAN demonstrations,
    GANs have not yet matured into a state where they can be widely deployed in industrial
    applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，生成对抗网络在实践中仍然具有挑战性。使生成器和鉴别器学习合理的函数需要许多技巧。因此，虽然有许多令人兴奋的GAN演示，但GAN尚未发展到可以广泛部署在工业应用中的阶段。
- en: Training a Convolutional Network in TensorFlow
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中训练卷积网络
- en: In this section we consider a code sample for training a simple convolutional
    neural network. In particular, our code sample will demonstrate how to train a
    LeNet-5 convolutional architecture on the MNIST dataset using TensorFlow. As always,
    we recommend that you follow along by running the full code sample from the [GitHub
    repo associated with the book](https://github.com/matroid/dlwithtf).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们考虑了一个用于训练简单卷积神经网络的代码示例。具体来说，我们的代码示例将演示如何使用TensorFlow在MNIST数据集上训练LeNet-5卷积架构。和往常一样，我们建议您通过运行与本书相关的[GitHub存储库](https://github.com/matroid/dlwithtf)中的完整代码示例来跟随。
- en: The MNIST Dataset
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST数据集
- en: The MNIST dataset consists of images of handwritten digits. The machine learning
    challenge associated with MNIST consists of creating a model trained on the training
    set of digits that generalizes to the validation set. [Figure 6-17](#ch6-mnist)
    shows some images drawn from the MNIST dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含手写数字的图像。与MNIST相关的机器学习挑战包括创建一个在数字训练集上训练并推广到验证集的模型。[图6-17](#ch6-mnist)展示了从MNIST数据集中绘制的一些图像。
- en: '![minst_images.png](assets/tfdl_0617.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![minst_images.png](assets/tfdl_0617.png)'
- en: Figure 6-17\. Some images of handwritten digits from the MNIST dataset. The
    learning challenge is to predict the digit from the image.
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-17。来自MNIST数据集的一些手写数字图像。学习挑战是从图像中预测数字。
- en: MNIST was a very important dataset for the development of machine learning methods
    for computer vision. The dataset is challenging enough that obvious, non-learning
    methods don’t tend to do well. At the same time, MNIST is small enough that experimenting
    with new architectures doesn’t require very large amounts of computing power.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机视觉的机器学习方法的发展，MNIST是一个非常重要的数据集。该数据集足够具有挑战性，以至于明显的非学习方法往往表现不佳。与此同时，MNIST数据集足够小，以至于尝试新架构不需要非常大量的计算资源。
- en: However, the MNIST dataset has mostly become obsolete. The best models achieve
    near one hundred percent test accuracy. Note that this fact doesn’t mean that
    the problem of handwritten digit recognition is solved! Rather, it is likely that
    human scientists have overfit architectures to the MNIST dataset and capitalized
    on its quirks to achieve very high predictive accuracies. As a result, it’s no
    longer good practice to use MNIST to design new deep architectures. That said,
    MNIST is still a superb dataset for pedagogical purposes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MNIST数据集大多已经过时。最佳模型实现了接近百分之百的测试准确率。请注意，这并不意味着手写数字识别问题已经解决！相反，很可能是人类科学家已经过度拟合了MNIST数据集的架构，并利用其特点实现了非常高的预测准确性。因此，不再建议使用MNIST来设计新的深度架构。尽管如此，MNIST仍然是一个非常好的用于教学目的的数据集。
- en: Loading MNIST
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载MNIST
- en: The MNIST codebase is located online on [Yann LeCun’s website](http://yann.lecun.com/exdb/mnist/).
    The download script pulls down the raw file from the website. Notice how the script
    caches the download so repeated calls to `download()` won’t waste effort.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST代码库位于[Yann LeCun的网站](http://yann.lecun.com/exdb/mnist/)上。下载脚本从网站下载原始文件。请注意脚本如何缓存下载，因此重复调用`download()`不会浪费精力。
- en: As a more general note, it’s quite common to store ML datasets in the cloud
    and have user code retrieve it before processing for input into a learning algorithm.
    The Tox21 dataset we accessed via the DeepChem library in [Chapter 4](ch04.html#fully_connected_networks)
    followed this same design pattern. In general, if you would like to host a large
    dataset for analysis, hosting on the cloud and downloading to a local machine
    for processing as necessary seems good practice. (This breaks down for very large
    datasets however, where network transfer times become exorbitantly expensive.)
    See [Example 6-1](#ch6-mnistdownload).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个更一般的说明，将机器学习数据集存储在云中，并让用户代码在处理之前检索数据，然后输入到学习算法中是非常常见的。我们在[第4章](ch04.html#fully_connected_networks)中通过DeepChem库访问的Tox21数据集遵循了相同的设计模式。一般来说，如果您想要托管一个大型数据集进行分析，将其托管在云端并根据需要下载到本地机器进行处理似乎是一个不错的做法。（然而，对于非常大的数据集，网络传输时间变得非常昂贵。）请参见[示例6-1](#ch6-mnistdownload)。
- en: Example 6-1\. This function downloads the MNIST dataset
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-1。这个函数下载MNIST数据集
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This download checks for the existence of `WORK_DIRECTORY`. If this directory
    exists, it assumes that the MNIST dataset has already been downloaded. Else, the
    script uses the `urllib` Python library to perform the download and prints the
    number of bytes downloaded.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此下载检查`WORK_DIRECTORY`的存在。如果该目录存在，则假定MNIST数据集已经被下载。否则，脚本使用`urllib` Python库执行下载并打印下载的字节数。
- en: The MNIST dataset is stored as a raw string of bytes encoding pixel values.
    In order to easily process this data, we need to convert it into a NumPy array.
    The function `np.frombuffer` provides a convenience that allows the conversion
    of a raw byte buffer into a numerical array ([Example 6-2](#ch6-mnistdownload2)).
    As we have noted elsewhere in this book, deep networks can be destabilized by
    input data that occupies wide ranges. For stable gradient descent, it is often
    necessary to constrain inputs to span a bounded range. The original MNIST dataset
    contains pixel values ranging from 0 to 255\. For stability, this range needs
    to be shifted to have mean zero and unit range (from –0.5 to +0.5).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集以字节编码的原始字符串形式存储像素值。为了方便处理这些数据，我们需要将其转换为NumPy数组。函数`np.frombuffer`提供了一个方便的方法，允许将原始字节缓冲区转换为数值数组（[示例6-2](#ch6-mnistdownload2)）。正如我们在本书的其他地方所指出的，深度网络可能会被占据广泛范围的输入数据破坏。为了稳定的梯度下降，通常需要将输入限制在一个有界范围内。原始的MNIST数据集包含从0到255的像素值。为了稳定性，这个范围需要被移动，使其均值为零，范围为单位（从-0.5到+0.5）。
- en: Example 6-2\. Extracting images from a downloaded dataset into NumPy arrays
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-2。从下载的数据集中提取图像到NumPy数组
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The labels are stored in a simple file as a string of bytes. There is a header
    consisting of 8 bytes, with the remainder of the data containing labels ([Example 6-3](#ch6-mnistlabels)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 标签以简单的文件形式存储为字节字符串。有一个包含8个字节的标头，其余的数据包含标签（[示例6-3](#ch6-mnistlabels)）。
- en: Example 6-3\. This function extracts labels from the downloaded dataset into
    an array of labels
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-3。这个函数将从下载的数据集中提取标签到一个标签数组中
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Given the functions defined in the previous examples, it is now feasible to
    download and process the MNIST training and test dataset ([Example 6-4](#ch6-mnisttogether)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面示例中定义的函数的基础上，现在可以下载并处理MNIST训练和测试数据集（[示例6-4](#ch6-mnisttogether)）。
- en: Example 6-4\. Using the functions defined in the previous examples, this code
    snippet downloads and processes the MNIST train and test datasets
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-4。使用前面示例中定义的函数，此代码片段下载并处理MNIST训练和测试数据集
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The MNIST dataset doesn’t explicitly define a validation dataset for hyperparameter
    tuning. Consequently, we manually designate the final 5,000 datapoints of the
    training dataset as validation data ([Example 6-5](#ch6-mnistval)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集并没有明确定义用于超参数调整的验证数据集。因此，我们手动将训练数据集的最后5,000个数据点指定为验证数据（[示例6-5](#ch6-mnistval)）。
- en: Example 6-5\. Extract the final 5,000 datasets of the training data for hyperparameter
    validation
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-5。提取训练数据的最后5,000个数据集用于超参数验证
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Choosing the Correct Validation Set
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的验证集
- en: In [Example 6-5](#ch6-mnistval), we use the final fragment of training data
    as a validation set to gauge the progress of our learning methods. In this case,
    this method is relatively harmless. The distribution of data in the test set is
    well represented by the distribution of data in the validation set.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例6-5](#ch6-mnistval)中，我们使用训练数据的最后一部分作为验证集来评估我们学习方法的进展。在这种情况下，这种方法相对无害。测试集中的数据分布在验证集中得到了很好的代表。
- en: However, in other situations, this type of simple validation set selection can
    be disastrous. In molecular machine learning (the use of machine learning to predict
    properties of molecules), it is almost always the case that the test distribution
    is dramatically different from the training distribution. Scientists are most
    interested in *prospective* prediction. That is, scientists would like to predict
    the properties of molecules that have never been tested for the property at hand.
    In this case, using the last fragment of training data for validation, or even
    a random subsample of the training data, will lead to misleadingly high accuracies.
    It’s quite common for a molecular machine learning model to have 90% accuracy
    on validation and, say, 60% on test.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在其他情况下，这种简单的验证集选择可能是灾难性的。在分子机器学习中（使用机器学习来预测分子的性质），测试分布几乎总是与训练分布截然不同。科学家最感兴趣的是*前瞻性*预测。也就是说，科学家希望预测从未针对该属性进行测试的分子的性质。在这种情况下，使用最后一部分训练数据进行验证，甚至使用训练数据的随机子样本，都会导致误导性地高准确率。分子机器学习模型在验证时具有90%的准确率，而在测试时可能只有60%是非常常见的。
- en: To correct for this error, it becomes necessary to design validation set selection
    methods that take pains to make the validation dissimilar from the training set.
    A variety of such algorithms exist for molecular machine learning, most of which
    use various mathematical estimates of graph dissimilarity (treating a molecule
    as a mathematical graph with atoms as nodes and chemical bonds as edges).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正这个错误，有必要设计验证集选择方法，这些方法要尽力使验证集与训练集不同。对于分子机器学习，存在各种算法，大多数使用各种数学估计图的不相似性（将分子视为具有原子节点和化学键边的数学图）。
- en: This issue crops up in many other areas of machine learning as well. In medical
    machine learning or in financial machine learning, relying on historical data
    to make forecasts can be disastrous. For each application, it’s important to critically
    reason about whether performance on the selected validation set is actually a
    good proxy for true performance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在许多其他机器学习领域也会出现。在医学机器学习或金融机器学习中，依靠历史数据进行预测可能是灾难性的。对于每个应用程序，重要的是要批判性地思考所选验证集上的性能是否实际上是真实性能的良好代理。
- en: TensorFlow Convolutional Primitives
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow卷积原语
- en: We start by introducing the TensorFlow primitives that are used to construct
    our convolutional networks ([Example 6-6](#ch6-tfconv2d)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍用于构建我们的卷积网络的TensorFlow原语（[示例6-6](#ch6-tfconv2d)）。
- en: Example 6-6\. Defining a 2D convolution in TensorFlow
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-6。在TensorFlow中定义2D卷积
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The function `tf.nn.conv2d` is the built-in TensorFlow function that defines
    convolutional layers. Here, `input` is assumed to be a tensor of shape `(batch,
    height, width, channels)` where `batch` is the number of images in a minibatch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`tf.nn.conv2d`是内置的TensorFlow函数，用于定义卷积层。这里，`input`被假定为形状为`(batch, height, width,
    channels)`的张量，其中`batch`是一个小批量中的图像数量。
- en: Note that the conversion functions defined previously read the MNIST data into
    this format. The argument `filter` is a tensor of shape `(filter_height, filter_width,
    channels, out_channels)` that specifies the learnable weights for the nonlinear
    transformation learned in the convolutional kernel. `strides` contains the filter
    strides and is a list of length 4 (one for each input dimension).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，先前定义的转换函数将MNIST数据读入此格式。参数`filter`是形状为`(filter_height, filter_width, channels,
    out_channels)`的张量，指定了在卷积核中学习的非线性变换的可学习权重。`strides`包含滤波器步幅，是长度为4的列表（每个输入维度一个）。
- en: '`padding` controls whether the input tensors are padded (with extra zeros as
    in [Figure 6-18](#ch6-convpad)) to guarantee that output from the convolutional
    layer has the same shape as the input. If `padding="SAME"`, then `input` is padded
    to ensure that the convolutional layer outputs an image tensor of the same shape
    as the original input image tensor. If `padding="VALID"` then extra padding is
    not added.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`padding`控制输入张量是否被填充（如[图6-18](#ch6-convpad)中的额外零）以确保卷积层的输出与输入具有相同的形状。如果`padding="SAME"`，则填充`input`以确保卷积层输出与原始输入图像张量具有相同形状的图像张量。如果`padding="VALID"`，则不添加额外填充。'
- en: '![conv_padding.png](assets/tfdl_0618.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![conv_padding.png](assets/tfdl_0618.png)'
- en: Figure 6-18\. Padding for convolutional layers ensures that the output image
    has the same shape as the input image.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-18。卷积层的填充确保输出图像具有与输入图像相同的形状。
- en: The code in [Example 6-7](#ch6-tfconv2d2) defines max pooling in TensorFlow.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例6-7](#ch6-tfconv2d2)中的代码定义了TensorFlow中的最大池化。'
- en: Example 6-7\. Defining max pooling in TensorFlow
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-7。在TensorFlow中定义最大池化
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `tf.nn.max_pool` function performs max pooling. Here `value` has the same
    shape as `input` for `tf.nn.conv2d`, `(batch, height, width, channels)`. `ksize`
    is the size of the pooling window and is a list of length 4\. `strides` and `padding`
    behave as for `tf.nn.conv2d`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.nn.max_pool`函数执行最大池化。这里`value`与`tf.nn.conv2d`的`input`具有相同的形状，即`(batch,
    height, width, channels)`。`ksize`是池化窗口的大小，是长度为4的列表。`strides`和`padding`的行为与`tf.nn.conv2d`相同。'
- en: The Convolutional Architecture
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积架构
- en: The architecture defined in this section will closely resemble LeNet-5, the
    original architecture used to train convolutional neural networks on the MNIST
    dataset. At the time the LeNet-5 architecture was invented, it was exorbitantly
    expensive computationally, requiring multiple weeks of compute to complete training.
    Today’s laptops thankfully are more than sufficient to train LeNet-5 models. [Figure 6-19](#ch6-lenet5)
    illustrates the structure of the LeNet-5 architecture.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中定义的架构将与LeNet-5非常相似，LeNet-5是最初用于在MNIST数据集上训练卷积神经网络的原始架构。在LeNet-5架构被发明时，计算成本非常昂贵，需要多周的计算才能完成训练。如今的笔记本电脑幸运地足以训练LeNet-5模型。[图6-19](#ch6-lenet5)展示了LeNet-5架构的结构。
- en: '![lenet5.png](assets/tfdl_0619.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![lenet5.png](assets/tfdl_0619.png)'
- en: Figure 6-19\. An illustration of the LeNet-5 convolutional architecture.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-19。LeNet-5卷积架构的示意图。
- en: Where Would More Compute Make a Difference?
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多计算会有什么不同？
- en: The LeNet-5 architecture is decades old, but is essentially the right architecture
    for the problem of digit recognition. However, its computational requirements
    forced the architecture into relative obscurity for decades. It’s interesting
    to ask, then, what research problems today are similarly solved but limited solely
    by lack of adequate computational power?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5架构已有几十年历史，但实质上是解决数字识别问题的正确架构。然而，它的计算需求使得这种架构在几十年来相对默默无闻。因此，有趣的是，今天有哪些研究问题同样被解决，但仅仅受限于缺乏足够的计算能力？
- en: One good contender is video processing. Convolutional models are quite good
    at processing video. However, it is unwieldy to store and train models on large
    video datasets, so most academic papers don’t report results on video datasets.
    As a result, it’s not so easy to hack together a good video processing system.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的应用是视频处理。卷积模型在处理视频方面非常出色。然而，在大型视频数据集上存储和训练模型是不方便的，因此大多数学术论文不会报告视频数据集的结果。因此，要拼凑出一个良好的视频处理系统并不容易。
- en: This situation will likely change as computing capabilities increase and it’s
    likely that video processing systems will become much more commonplace. However,
    there’s one critical difference between today’s hardware improvements and those
    of past decades. Unlike in years past, Moore’s law has slowed dramatically. As
    a result, improvements in hardware require more than natural transistor shrinkage
    and often require considerable ingenuity in architecture design. We will return
    to this topic in later chapters and discuss the architectural needs of deep networks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的增强，这种情况可能会发生变化，视频处理系统可能会变得更加普遍。然而，今天的硬件改进与过去几十年的硬件改进之间存在一个关键区别。与过去几年不同，摩尔定律的放缓明显。因此，硬件的改进需要更多的比自然晶体管缩小更多的东西，通常需要在架构设计上付出相当大的智慧。我们将在后面的章节中回到这个话题，并讨论深度网络的架构需求。
- en: Let’s define the weights needed to train our LeNet-5 network. We start by defining
    some basic constants that are used to define our weight tensors ([Example 6-8](#ch6-lenetconsts)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义训练LeNet-5网络所需的权重。我们首先定义一些用于定义权重张量的基本常量（[示例6-8](#ch6-lenetconsts)）。
- en: Example 6-8\. Defining basic constants for the LeNet-5 model
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-8。为LeNet-5模型定义基本常量
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The architecture we define will use two convolutional layers interspersed with
    two pooling layers, topped off by two fully connected layers. Recall that pooling
    requires no learnable weights, so we simply need to create weights for the convolutional
    and fully connected layers. For each `tf.nn.conv2d`, we need to create a learnable
    weight tensor corresponding to the `filter` argument for `tf.nn.conv2d`. In this
    particular architecture, we will also add a convolutional bias, one for each output
    channel ([Example 6-9](#ch6-lenetweights)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的架构将使用两个卷积层交替使用两个池化层，最后是两个完全连接的层。请记住，池化不需要可学习的权重，因此我们只需要为卷积和完全连接的层创建权重。对于每个`tf.nn.conv2d`，我们需要创建一个与`tf.nn.conv2d`的`filter`参数对应的可学习权重张量。在这种特定的架构中，我们还将添加一个卷积偏置，每个输出通道一个（[示例6-9](#ch6-lenetweights)）。
- en: Example 6-9\. Defining learnable weights for the convolutional layers
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-9。为卷积层定义可学习的权重
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the convolutional weights are 4-tensors, while the biases are 1-tensors.
    The first fully connected layer converts the outputs of the convolutional layer
    to a vector of size 512\. The input images start with size `IMAGE_SIZE=28`. After
    the two pooling layers (each of which reduces the input by a factor of 2), we
    end with images of size `IMAGE_SIZE//4`. We create the shape of the fully connected
    weights accordingly.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，卷积权重是4维张量，而偏置是1维张量。第一个完全连接的层将卷积层的输出转换为大小为512的向量。输入图像从大小`IMAGE_SIZE=28`开始。经过两个池化层（每个将输入减少2倍），我们最终得到大小为`IMAGE_SIZE//4`的图像。我们相应地创建完全连接权重的形状。
- en: The second fully connected layer is used to provide the 10-way classification
    output, so it has weight shape `(512,10)` and bias shape `(10)`, shown in [Example 6-10](#ch6-lenetfcweights).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个完全连接的层用于提供10路分类输出，因此其权重形状为`(512,10)`，偏置形状为`(10)`，如[示例6-10](#ch6-lenetfcweights)所示。
- en: Example 6-10\. Defining learnable weights for the fully connected layers
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-10。为完全连接的层定义可学习的权重
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With all the weights defined, we are now free to define the architecture of
    the network. The architecture has six layers in the pattern conv-pool-conv-pool-full-full
    ([Example 6-11](#ch6-lenetdef)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有权重定义完成后，我们现在可以自由定义网络的架构。该架构有六层，模式为conv-pool-conv-pool-full-full（[示例6-11](#ch6-lenetdef)）。
- en: Example 6-11\. Defining the LeNet-5 architecture. Calling the function defined
    in this example will instantiate the architecture.
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-11。定义LeNet-5架构。调用此示例中定义的函数将实例化架构。
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As noted previously, the basic architecture of the network intersperses `tf.nn.conv2d`,
    `tf.nn.max_pool`, with nonlinearities, and a final fully connected layer. For
    regularization, a dropout layer is applied after the final fully connected layer,
    but only during training. Note that we pass in the input as an argument `data`
    to the function `model()`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，网络的基本架构交替使用`tf.nn.conv2d`、`tf.nn.max_pool`和非线性，以及最后一个完全连接的层。为了正则化，在最后一个完全连接的层之后应用一个dropout层，但只在训练期间。请注意，我们将输入作为参数`data`传递给函数`model()`。
- en: The only part of the network that remains to be defined are the placeholders
    ([Example 6-12](#ch6-lenetplace)). We need to define two placeholders for inputting
    the training images and the training labels. In this particular network, we also
    define a separate placeholder for evaluation that allows us to input larger batches
    when evaluating.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中仍需定义的唯一部分是占位符（[示例6-12](#ch6-lenetplace)）。我们需要定义两个占位符，用于输入训练图像和训练标签。在这个特定的网络中，我们还定义了一个用于评估的单独占位符，允许我们在评估时输入更大的批次。
- en: Example 6-12\. Define placeholders for the architecture
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-12。为架构定义占位符
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With these definitions in place, we now have the data processed, inputs and
    weights specified, and the model constructed. We are now prepared to train the
    network ([Example 6-13](#ch6-lenettrain)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些定义，我们现在已经处理了数据，指定了输入和权重，并构建了模型。我们现在准备训练网络（[示例6-13](#ch6-lenettrain)）。
- en: Example 6-13\. Training the LeNet-5 architecture
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-13。训练LeNet-5架构
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The structure of this fitting code looks quite similar to other code for fitting
    we’ve seen so far in this book. In each step, we construct a feed dictionary,
    and then run a step of the optimizer. Note that we use minibatch training as before.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个拟合代码的结构看起来与本书迄今为止看到的其他拟合代码非常相似。在每一步中，我们构建一个feed字典，然后运行优化器的一步。请注意，我们仍然使用小批量训练。
- en: Evaluating Trained Models
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估经过训练的模型
- en: We now have a model training. How can we evaluate the accuracy of the trained
    model? A simple method is to define an error metric. As in previous chapters,
    we shall use a simple classification metric to gauge accuracy ([Example 6-14](#ch6-leneterror)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个正在训练的模型。我们如何评估训练模型的准确性？一个简单的方法是定义一个错误度量。与前几章一样，我们将使用一个简单的分类度量来衡量准确性（[示例6-14](#ch6-leneterror)）。
- en: Example 6-14\. Evaluating the error of trained architectures
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-14。评估经过训练的架构的错误
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can use this function to evaluate the error of the network as we train. Let’s
    introduce an additional convenience function that evaluates predictions on any
    given dataset in batches ([Example 6-15](#ch6-leneterrorbatch)). This convenience
    is necessary since our network can only handle inputs with fixed batch sizes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数来评估网络在训练过程中的错误。让我们引入一个额外的方便函数，以批处理的方式评估任何给定数据集上的预测（[示例6-15](#ch6-leneterrorbatch)）。这种便利是必要的，因为我们的网络只能处理固定批量大小的输入。
- en: Example 6-15\. Evaluating a batch of data at a time
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-15。一次评估一批数据
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can now add a little instrumentation (in the inner `for`-loop of training)
    that periodically evaluates the model’s accuracy on the validation set. We can
    end training by scoring test accuracy. [Example 6-16](#ch6-lenetfullfit) shows
    the full fitting code with instrumentation added in.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在训练过程中的内部`for`循环中添加一些仪器（instrumentation），定期评估模型在验证集上的准确性。我们可以通过评分测试准确性来结束训练。[示例6-16](#ch6-lenetfullfit)展示了添加了仪器的完整拟合代码。
- en: Example 6-16\. The full code for training the network, with instrumentation
    added
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-16。训练网络的完整代码，添加了仪器
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Challenge for the Reader
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读者的挑战
- en: Try training the network yourself. You should be able to achieve test error
    of < 1%!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试自己训练网络。您应该能够达到<1%的测试错误！
- en: Review
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: In this chapter, we have shown you the basic concepts of convolutional network
    design. These concepts include convolutional and pooling layers that constitute
    core building blocks of convolutional networks. We then discussed applications
    of convolutional architectures such as object detection, image segmentation, and
    image generation. We ended the chapter with an in-depth case study that showed
    you how to train a convolutional architecture on the MNIST handwritten digit dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们向您展示了卷积网络设计的基本概念。这些概念包括构成卷积网络核心构建模块的卷积和池化层。然后我们讨论了卷积架构的应用，如目标检测、图像分割和图像生成。我们以一个深入的案例研究结束了这一章，向您展示了如何在MNIST手写数字数据集上训练卷积架构。
- en: In [Chapter 7](ch07.html#recurrent_neural_networks), we will cover recurrent
    neural networks, another core deep learning architecture. Unlike convolutional
    networks, which were designed for image processing, recurrent architectures are
    powerfully suited to handling sequential data such as natural language datasets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.html#recurrent_neural_networks)中，我们将介绍循环神经网络，另一个核心深度学习架构。与为图像处理而设计的卷积网络不同，循环架构非常适合处理顺序数据，如自然语言数据集。
