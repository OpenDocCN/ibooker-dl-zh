- en: 12 Fine-tuning LLMs with business domain knowledge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 使用商业领域知识微调LLMs
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The fine-tuning process for an LLM
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的微调过程
- en: Preparing a data set to use for fine-tuning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于微调的数据集
- en: Using fine-tuning tools to better understand the process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调工具更好地理解过程
- en: Although the effects of large language models (LLMs) in various industries have
    been covered extensively in the mainstream media, the ubiquity and popularity
    of LLMs have contributed to a quiet revolution in the AI open source community.
    Through the spirit of open collaboration and the support of big technology companies,
    the ability to fine-tune AI models has become increasingly more accessible to
    AI enthusiasts. This opportunity has resulted in a vibrant community that is experimenting
    and sharing a wide range of processes and tools that can be used to better understand
    how fine-tuning works and how we can tune models ourselves or in teams.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在各个行业中的影响在主流媒体中得到了广泛的报道，但LLMs的普遍性和受欢迎程度已经为AI开源社区带来了一场静悄悄的革命。通过开放协作的精神和大型科技公司的支持，微调AI模型的能力越来越容易为AI爱好者所获得。这个机会产生了一个充满活力的社区，该社区正在尝试和分享各种过程和工具，这些过程和工具可以更好地理解微调是如何工作的，以及我们如何自己或团队合作来调整模型。
- en: The topic of fine-tuning is vast, and getting into each important detail would
    require an entire book of its own. However, by taking advantage of the models,
    data sets, platforms, and tools created by and for the open source community,
    we can establish an appreciation for the fine-tuning process. These open source
    resources can prepare us for a future in which we might find ourselves fine-tuning
    our models within our organizations to help build context-based LLMs. However,
    fine-tuning is as much about the approach we take as it is about the tools we
    use. So, in this chapter, we’ll go through each important part of the fine-tuning
    process and learn how to fine-tune our model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的主题非常广泛，深入探讨每个重要细节可能需要一本整本书。然而，通过利用开源社区创建和为开源社区创建的模型、数据集、平台和工具，我们可以对微调过程有一个欣赏。这些开源资源可以为我们准备一个未来，在那个未来中，我们可能会在我们的组织中微调我们的模型，以帮助构建基于上下文的LLMs。然而，微调不仅关乎我们采取的方法，也关乎我们使用的工具。因此，在本章中，我们将详细了解微调过程的每个重要部分，并学习如何微调我们的模型。
- en: 12.1 Exploring the fine-tuning process
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 探索微调过程
- en: Before we learn more about fine-tuning tools, we should first discuss what fine-tuning
    entails and reflect on what we hope to achieve. As we’ll see, fine-tuning involves
    a series of steps that play an important role in the wider process. Knowing what
    we hope to achieve helps us not only with the evaluation of a model once it’s
    been tuned, but also guides us in our tuning approach. Each step of the fine-tuning
    process includes distinct activities and challenges. And although we might not
    be able to cover all the details, we’ll learn enough to understand what happens
    as we tune models and the challenges that we might face.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习更多关于微调工具之前，我们首先应该讨论微调包含的内容，并反思我们希望实现的目标。正如我们将看到的，微调涉及一系列步骤，这些步骤在更广泛的过程中发挥着重要作用。了解我们希望实现的目标不仅有助于我们评估微调后的模型，而且还能指导我们的微调方法。微调过程的每一步都包括不同的活动和挑战。尽管我们可能无法涵盖所有细节，但我们将学习到足够多的知识，以了解我们在微调模型时会发生什么，以及我们可能面临的挑战。
- en: 12.1.1 A map of the fine-tuning process
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 微调过程的地图
- en: As we discovered in chapter 10, fine-tuning is the process of taking an existing
    model that has already gone through some sort of training, known as a foundational
    model, and training it further with additional data sets to
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第10章中发现的，微调是将已经经过某种训练的现有模型（称为基础模型）进一步使用额外的数据集进行训练的过程
- en: Make a model more attuned to contextual information
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型更能适应上下文信息
- en: Change the tone of a model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变模型的声音
- en: Help it respond to specific queries or instructions
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助它响应特定的查询或指令
- en: Just as a foundational model goes through a series of steps to be trained, there
    is a series of steps to go through during a fine-tuning session. Figure 12.1 summarizes
    these steps and how they feed into one another.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像基础模型需要经过一系列步骤进行训练一样，在微调会话期间也有一系列步骤需要遵循。图12.1总结了这些步骤以及它们是如何相互关联的。
- en: '![](../../OEBPS/Images/CH12_F01_Winteringham2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F01_Winteringham2.png)'
- en: Figure 12.1 A visual representation of the different steps taken during fine-tuning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 精调过程中所采取的不同步骤的视觉表示
- en: Figure 12.1 doesn’t necessarily cover every nuance of fine-tuning, but it captures
    the core steps we would typically expect to go through to successfully create
    a tuned model. Throughout this chapter, we’ll examine each step in more detail,
    but let’s first reflect on what might be the most important part of the process,
    identifying what we hope to achieve from fine-tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1不一定涵盖微调的每一个细微之处，但它捕捉了我们通常期望成功创建一个微调模型的核心步骤。在本章中，我们将更详细地检查每个步骤，但首先让我们反思一下这个过程可能最重要的部分，即确定我们希望通过微调实现的目标。
- en: 12.1.2 Goal setting
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 目标设定
- en: One of the biggest mistakes that we can make when tuning LLMs is not having
    a clear idea of what we want our tuned LLM to do. Failing to set out clear goals
    for what problems a tuned model is supposed to help with can affect every part
    of the fine-tuning process, from data preparation to testing. Given the indeterministic
    nature of LLMs, this doesn’t mean setting goals around specific information we
    expect to get from an LLM. But we do have to ask ourselves what type of behavior
    we want and how it fits into our wider context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调LLM时，我们可能犯的最大错误之一就是没有明确的想法，不知道我们希望微调后的LLM做什么。未能为微调模型应该帮助解决的问题设定明确的目标可能会影响微调过程的各个方面，从数据准备到测试。鉴于LLM的不确定性，这并不意味着围绕我们期望从LLM中获得的具体信息来设定目标。但我们必须问自己我们想要什么类型的行为，以及它如何融入更广泛的背景。
- en: 'To illustrate this, let’s consider two different goals. One is to create a
    code-completion tool that has been fine-tuned on our code base to use an LLM’s
    generative capabilities, without revealing intellectual property to a third party.
    Another is a Q&A/chat-based LLM that offers support to users, which has been tuned
    on support documentation and customer data to help answer questions. Depending
    on which goal we want to pursue, we would need to consider details such as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们考虑两个不同的目标。一个是创建一个代码补全工具，该工具已经在我们的代码库上微调，以使用LLM的生成能力，同时不向第三方泄露知识产权。另一个是一个基于问答/聊天的LLM，它为用户提供支持，该LLM已经在支持文档和客户数据上微调，以帮助回答问题。根据我们想要追求的目标，我们需要考虑以下细节：
- en: '*What datasets to use?* For our code completion scenario, we likely want to
    create a data set that consists of code broken down into logical sections to tune
    our model with. We may also be interested in using other data sets that include
    open source code. This would differ from the Q&A chat scenario in which we would
    create a corpus of data that includes help guides and documentation.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用什么数据集？* 对于我们的代码补全场景，我们可能希望创建一个由逻辑部分组成的代码数据集，以微调我们的模型。我们也可能对使用包含开源代码的其他数据集感兴趣。这与问答聊天场景不同，在问答聊天场景中，我们会创建一个包括帮助指南和文档的数据集。'
- en: '*What model to use?* At the time of writing, Hugging Face, an AI open source
    community site, is currently hosting 500,000+ different models, all of which are
    designed to serve different purposes such as generation, classification, and translation.
    When considering our fine-tuning goals, we will need to select models that suit
    our needs. In our code scenario, we would likely pick a model that has already
    been trained on a large corpus of code data, making it easier to further tune
    with our code base. In our Q&A/chat LLM scenario, we would likely want a model
    that has already been trained to act as a capable chat-based LLM.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用什么模型？* 在撰写本文时，Hugging Face，一个AI开源社区网站，目前托管了500,000+个不同的模型，这些模型都旨在服务于不同的目的，如生成、分类和翻译。在考虑我们的微调目标时，我们需要选择适合我们需求的模型。在我们的代码场景中，我们可能会选择一个已经在大量代码数据集上训练过的模型，这使得它更容易与我们的代码库进一步微调。在我们的问答/聊天LLM场景中，我们可能会想要一个已经训练成能够作为有能力的基于聊天的LLM的模型。'
- en: '*How big a model to use?* Another question to ask ourselves is how large a
    model do we need? Depending on the size of the model we want, usually defined
    by its parameter size, will also determine our hardware requirements. There is
    a tradeoff that must be considered. If we want our models to be accurate in their
    responses, then a large amount of hardware needs to be dedicated to the hosting
    and running of a larger model. If our budget is limited, or the location we have
    to deploy our model isn’t performant, then we may need to consider a smaller model
    that might not be as accurate or respond as quickly to our requests.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用多大的模型？* 我们需要问自己的另一个问题是我们需要多大型的模型？根据我们想要的模型大小，通常由其参数大小定义，也将决定我们的硬件需求。这里有一个必须考虑的权衡。如果我们希望我们的模型在响应时准确，那么就需要大量的硬件来托管和运行更大的模型。如果我们预算有限，或者我们部署模型的位置性能不佳，那么我们可能需要考虑一个可能不那么准确或对我们的请求响应不那么快的较小模型。'
- en: What is Hugging Face?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是Hugging Face？
- en: Hugging Face is a platform for the AI open source community that allows members
    to host, deploy, and collaborate on their AI work. Notably, Hugging Face offers
    a place to host data sets and models, as well as the opportunity to let others
    deploy and interact with AI applications through their space features. There are
    also other paid-for features such as auto training, which is designed to make
    fine-tuning easier, as well as increased hardware space to deploy more complex
    and resource-demanding models. Similar to how GitHub enables teams to collaborate
    in addition to offering the ability to host code, Hugging Face offers a place
    to share and learn from AI community members and work together on future AI projects.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一个AI开源社区的平台，允许成员托管、部署和协作他们的AI工作。值得注意的是，Hugging Face提供了一个托管数据集和模型的地方，以及通过其空间功能让其他人部署和交互AI应用的机会。还有其他付费功能，如自动训练，旨在使微调更容易，以及增加硬件空间以部署更复杂和资源密集型模型。类似于GitHub除了提供托管代码的能力外，还允许团队协作，Hugging
    Face提供了一个分享和从AI社区成员那里学习的地方，并共同参与未来的AI项目。
- en: This is by no means an exhaustive list of considerations, and as we explore
    fine-tuning more, we’ll learn how there are different options we need to decide
    on. Fortunately (if you have budget), many tools have been created and put in
    place to make the experimentation with fine-tuning models easier and faster. Therefore,
    although it’s good to have a goal in mind when setting out on a fine-tuning journey,
    we are free to switch out models, data sets, and more to learn how to create the
    most optimal fine-tuned model for a given context.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对不是考虑因素的详尽列表，随着我们进一步探索微调，我们将了解到我们需要决定的不同选项。幸运的是（如果你有预算），已经创建并实施了许多工具，使微调模型的实验更容易、更快。因此，尽管在开始微调之旅时有一个目标是很不错的，但我们仍然可以自由地更换模型、数据集等，以了解如何为特定情境创建最优化微调模型。
- en: 12.2 Executing a fine-tuning session
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 执行微调会话
- en: Keeping in mind the importance of a goal for a fine-tuned model, in this chapter,
    we’re going to attempt to fine-tune a model using the code base of a project to
    create a model that can support future analysis. In real terms, this means having
    a model that, when asked questions about our code base, can give answers that
    are context-sensitive to our project. To do this, we’ll be using the code from
    the open source project restful-booker-platform ([https://mng.bz/vJRJ](https://mng.bz/vJRJ)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到微调模型目标的重要性，在本章中，我们将尝试使用项目的代码库来微调一个模型，以创建一个可以支持未来分析的支持模型。在实际情况中，这意味着拥有一个模型，当被问及我们的代码库时，可以给出与我们的项目相关的上下文敏感的回答。为此，我们将使用开源项目restful-booker-platform的代码（[https://mng.bz/vJRJ](https://mng.bz/vJRJ)）。
- en: 12.2.1 Preparing data for training
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 准备训练数据
- en: 'Although fine-tuning doesn’t require the same volumes of data as a pretrained
    model, the success of fine-tuning relies heavily on the size and quality of the
    data we use, which brings us to the question of what sort of data we need. To
    help us understand the size, scope, and formatting of data sets, we can learn
    a lot from sites such as Hugging Face, where open source data sets are stored
    ([https://huggingface.co/datasets](https://huggingface.co/datasets)). Some notable
    data sets used at the time of writing include:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微调不需要像预训练模型那样大量的数据，但微调的成功在很大程度上依赖于我们使用的数据的大小和质量，这引出了我们需要什么类型的数据的问题。为了帮助我们理解数据集的大小、范围和格式，我们可以从像Hugging
    Face这样的网站中学到很多，那里存储着开源数据集（[https://huggingface.co/datasets](https://huggingface.co/datasets)）。在撰写本文时，一些值得注意的数据集包括：
- en: '*The Stack*—546 million rows of code examples that have been scraped from open
    source projects on sites such as GitHub ([https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack))'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*堆栈*—从GitHub等开源项目上抓取的5.46亿行代码示例（[https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack)）'
- en: '*Alpaca*—52,000 rows of synthetic data that have been generated using an existing
    LLM ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca))'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*羊驼*—使用现有的大型语言模型（LLM）生成的52,000行合成数据（[https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)）'
- en: '*OpenOrca*—2.91 million rows of question and response data ([https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca))'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenOrca*—290万行问题和回答数据（[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)）'
- en: Looking through each of these data sets, we can see they contain different types
    of information, from code examples to questions and responses, created in different
    ways. Data sets such as The Stack are based on real information scraped off the
    internet, whereas Alpaca has been synthetically generated by an AI. We can also
    see that Alpaca is a much smaller data set compared with the others in the list,
    but that doesn’t mean it’s not useful.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些数据集的每一个，我们可以看到它们包含不同类型的信息，从代码示例到问题和回答，以不同的方式创建。例如，The Stack这样的数据集是基于从互联网上抓取的真实信息，而Alpaca是由人工智能合成的。我们还可以看到，与列表中的其他数据集相比，Alpaca是一个规模较小的数据集，但这并不意味着它没有用。
- en: What is synthetic data?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是合成数据？
- en: In the context of AI training and fine-tuning, synthetic data is the process
    of artificially generating data that, while looking like real user data, is not
    based on real-life data. Using synthetic data is a useful technique for training
    and fine-tuning AIs because it can provide the necessary data required for carrying
    out training or fine-tuning. There are side effects to using synthetic data, though.
    First, there is a cost in generating the test data. Tools such as gretel.ai, mostly.ai,
    and tonic.ai offer data-generation tools, but they come at a price. Second, and
    perhaps more important, studies have shown that training models on purely synthetic
    data can impact the quality of a model’s responses. This makes sense because real
    data will have variances and randomness that are hard to simulate in AI-generated
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能训练和微调的背景下，合成数据是指人工生成看起来像真实用户数据，但实际上并非基于真实生活数据的过程。使用合成数据是训练和微调人工智能的有用技术，因为它可以提供进行训练或微调所需的数据。尽管如此，使用合成数据也存在副作用。首先，生成测试数据会有成本。例如，gretel.ai、mostly.ai和tonic.ai等工具提供数据生成工具，但它们是有价格的。其次，也许更重要的是，研究表明，在纯合成数据上训练模型可能会影响模型响应的质量。这很有道理，因为真实数据将具有难以在人工智能生成数据中模拟的变异性随机性。
- en: So, when setting out our requirements, we need to consider what we want, what
    is currently available, and what we might need to build ourselves. Let’s return
    to our code assistant and Q&A model examples. For our code assistant LLM, we are
    likely going to want a corpus of data that is mostly code based, whereas with
    our Q&A model, we would need data written in natural language and containing questions
    and answers in a key–value format (the question being the key and the answer being
    the value). As we can see, our goals inform our decisions regarding the type of
    data we require, but with it come additional questions, such as where is the data
    going to come from and how are we going to format it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在制定我们的要求时，我们需要考虑我们想要什么，目前有什么可用，以及我们可能需要自己构建什么。让我们回到我们的代码助手和问答模型示例。对于我们的代码助手LLM，我们可能需要一个主要以代码为基础的数据集，而我们的问答模型则需要以自然语言编写的数据，并包含以键值格式（问题作为键，答案作为值）编写的问题和答案。正如我们所看到的，我们的目标指导我们关于所需数据类型的决策，但随之而来的是更多的问题，例如数据从何而来以及我们将如何格式化它。
- en: 'We’ve already seen that there are lots of publicly available data sets from
    sites such as Hugging Face (Kaggle available at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
    is also a great source of data sets). But if we’re attempting to fine-tune a model
    so it is more aligned with our context, chances are we will want to tune it using
    data that belongs to our organization. Therefore, an exercise in what data is
    available, its quality, and how can we access it is required before we decide
    how we’re going to convert it into a format that is suitable for training. Consider
    the format in which the Alpaca data set has been structured ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)).
    The data set consists of the following four columns: instruction, input, output,
    and text. As we’ll learn in the next step, depending on the way we are fine-tuning
    a model, we will require different aspects of a data set. For example, if we wanted
    to fine-tune a Q&A model, we would require, as a minimum, the instruction and
    output columns to help tune it toward what type of questions to expect and what
    types of answers to respond with.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，来自Hugging Face（Kaggle的网址[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)也是一个优秀的数据集来源）等网站上有许多公开的数据集。但如果我们试图微调一个模型，使其更符合我们的上下文，那么我们很可能会希望使用属于我们组织的数据来调整它。因此，在决定如何将其转换为适合训练的格式之前，我们需要进行一项关于可用数据、其质量以及我们如何获取它的练习。考虑Alpaca数据集的结构格式([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca))。该数据集包括以下四个列：指令、输入、输出和文本。正如我们将在下一步学习的那样，根据我们微调模型的方式，我们将需要数据集的不同方面。例如，如果我们想微调一个问答模型，我们至少需要指令和输出列，以帮助调整它，使其能够回答预期的问题类型和回答类型。
- en: The challenge is getting raw data into a structured format like the one we saw
    in the Alpaca data set. For example, if we consider our Q&A model scenario, we
    would likely want to train it on our documentation and support documents. Some
    of this raw data might be in a Q&A format, such as FAQs, but the majority of our
    data wouldn’t be so straightforward. Therefore, we would need to work out a way
    to parse our data in a way that fits our data set structure. To make matters more
    complicated, we would also need to do this automatically to generate enough of
    a corpus of data so that it would be useful. Doing it manually is also an option,
    but it could be a costly one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于将原始数据转换为像我们在Alpaca数据集中看到的那种结构化格式。例如，如果我们考虑我们的问答模型场景，我们可能希望用我们的文档和支持文档来训练它。其中一些原始数据可能以问答格式存在，例如常见问题解答（FAQs），但我们的大部分数据可能不会这么直接。因此，我们需要找出一种方法来解析我们的数据，使其适合我们的数据集结构。更复杂的是，我们还需要自动完成这项工作，以生成足够的数据量，使其有用。手动完成也是一项选择，但可能会很昂贵。
- en: A case study in data preparation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备案例研究
- en: 'The data set we are going to use for our fine-tuning session presents a good
    example of the challenges we may encounter when building even small data sets.
    For our fine-tuning session, we’ll be using a previously created data set I built
    that can be found on Hugging Face at [https://mng.bz/4pXa](https://mng.bz/4pXa).
    The data set is a JSONL-formatted document that contains parsed sections of the
    Java portion of restful-booker-platform (RBP), paired with generated instructions,
    a sample of which is provided here (line breaks have been added for readability):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要用于微调会话的数据集展示了我们在构建小型数据集时可能遇到的挑战。在我们的微调会话中，我们将使用一个之前创建的数据集，该数据集可以在Hugging
    Face上找到，网址为[https://mng.bz/4pXa](https://mng.bz/4pXa)。该数据集是一个JSONL格式的文档，包含restful-booker-platform
    (RBP)的Java部分的解析部分，以及生成的指令，其中一部分如下所示（为了便于阅读，已添加换行符）：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This snippet will give us everything we need data-wise for tuning, but before
    we begin, let’s explore how it was made. Consider a class like the following and
    ask yourself how would you break this code up for fine-tuning in a logical fashion?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段将为我们提供调整所需的所有数据，但在我们开始之前，让我们探索一下它是如何制作的。考虑以下类，然后问问自己你会如何以逻辑的方式将这段代码拆分以进行微调？
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Would you fine-tune on a file-by-file basis, a line-by-line basis, or some
    other way? On a first attempt at testing out fine-tuning of a model on the RBP
    code base, I opted for the line-by-line approach. It created a script that would
    iterate through each file in a project and add each line of the file into its
    row resulting in a table of data that looked similar to the following example
    table:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你会选择按文件逐个微调、按行逐个微调，还是其他方式？在尝试测试基于RBP代码库的模型微调的第一尝试中，我选择了逐行的方法。它创建了一个脚本，该脚本将遍历项目中的每个文件，并将文件的每一行添加到其行中，从而生成一个类似于以下示例表的数据表：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The problem with this approach was that, although it was easy to parse and store
    the data, I ended up with entries that were lacking context, meaning I was tuning
    with entries such as `}` or `@Autowired`. These don’t provide a lot of context
    or detail about the RBP project, and they raised another question. What type of
    instruction can be paired with these types of entries? If you recall, during instruction-based
    fine-tuning, we send an instruction (sometimes with additional input data) and
    then compare the response with our expected output. The type of instruction we
    would add to an entry such as `}` would contain no hints toward our context and
    potentially contribute to a fine-tuned model that responds in unusual and undesired
    ways. This is exactly what happened when I attempted to tune a model based on
    line-by-line material.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，尽管解析和存储数据很容易，但我最终得到的条目缺乏上下文，这意味着我调整的是像`}`或`@Autowired`这样的条目。这些条目并没有提供很多关于RBP项目的上下文或细节，并且引发了一个新的问题。可以与这些类型的条目配对的指令类型是什么？如果你还记得，在基于指令的微调过程中，我们会发送一个指令（有时附带额外的输入数据），然后比较响应与我们的预期输出。我们添加到像`}`这样的条目中的指令类型将不包含任何关于我们上下文的提示，并可能对微调模型产生响应异常和不希望的方式。这正是我在尝试基于逐行材料调整模型时发生的情况。
- en: 'Instead, what I opted for (and what can be found in the dataset) is an approach
    that breaks down the code into logical sections. This means that rather than slicing
    things line by line, a file would be sliced up based on different attributes within
    a Java class. For example, a selection of slices from the class I shared earlier
    would look like the following in an example table:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我选择的方法（并在数据集中可以找到）是将代码分解成逻辑部分。这意味着，而不是逐行切割事物，一个文件将基于Java类中的不同属性进行切割。例如，从之前分享的类中选择的切片在示例表中看起来如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Instead of each entry in the data set being a line of code, each entry might
    contain details of how the class was declared, what variables are being declared
    in the class, each method in the class, and its containing code. The goal was
    to keep it detailed enough so the fine-tune would result in a model that had a
    greater awareness of the code base, but not so granular to lose the context completely.
    The result was a more successful tune, but the process of parsing became more
    complex. It required the creation of additional code that would iterate through
    each file, using JavaParser ([https://javaparser.org/](https://javaparser.org/))
    to ingest the code, build a semantic tree, and then query said tree to extract
    the information required for the data set (the code of which can be found at [https://mng.bz/QVpw](https://mng.bz/QVpw)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集中的每个条目都是一行代码不同，每个条目可能包含如何声明类、在类中声明的变量、类中的每个方法及其包含的代码的详细信息。目标是保持足够详细，以便微调的结果是一个对代码库有更高意识的模型，但又不至于过于细化而完全失去上下文。结果是更成功的调整，但解析过程变得更加复杂。这需要创建额外的代码，该代码将遍历每个文件，使用JavaParser
    ([https://javaparser.org/](https://javaparser.org/)) 读取代码，构建语义树，然后查询该树以提取数据集所需的信息（代码可以在[https://mng.bz/QVpw](https://mng.bz/QVpw)找到）。
- en: This example is a very basic one when it comes to preparing a data set for tuning
    (or training). However, after reflecting on it, it becomes clear that organizing
    and preparing even a simple data set from scratch has its complexities and challenges.
    The raw data for this data set was easily parsable with the right tools, but how
    do we manage data that is diverse in structure or has no discernible structure
    in the first place? This exploration into data sets highlights that the identification
    and creation of a data set is a complicated process. How we structure our data
    and what we put in it is critical for the success of fine-tuned models, and it’s
    where most of the work of fine-tuning and experimenting with LLMs can be found.
    Therefore, it’s important to have the necessary processes and tooling in place
    so that we can rapidly experiment with different data sets to see how they impact
    the result of a fine-tuned model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到为微调（或训练）准备数据集时，这个例子是一个非常基础的例子。然而，经过反思，我们可以清楚地看到，从头开始组织和准备即使是简单数据集也有其复杂性和挑战。这个数据集的原始数据可以用合适的工具轻松解析，但我们如何管理结构多样或一开始就没有明显结构的数据呢？对数据集的这种探索突出了识别和创建数据集是一个复杂的过程。我们如何组织数据以及我们放入其中的内容对于微调模型的成功至关重要，这也是微调和实验LLMs的大部分工作所在。因此，确保我们有必要的流程和工具非常重要，这样我们就可以快速实验不同的数据集，看看它们如何影响微调模型的结果。
- en: 12.2.2 Preprocessing and setup
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 预处理和设置
- en: With our data set in place, we next need to preprocess our data before tuning
    and get our tuning tools in place. We’ll get to tool setup soon, but first, to
    understand the preprocessing activities for fine-tuning, we need to skip ahead
    a little and talk about what happens during a fine-tuning session. Perhaps not
    surprisingly, given the size of data sets, a fine-tuning session consists of a
    specific loop, visualized in figure 12.2, which is run multiple times during fine-tuning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集就绪后，我们接下来需要在微调之前预处理我们的数据，并准备好我们的微调工具。我们将很快介绍工具设置，但首先，为了了解微调的预处理活动，我们需要稍微跳一下，谈谈微调会话期间发生的事情。考虑到数据集的大小，微调会话由一个特定的循环组成，如图12.2所示，这个循环在微调过程中会多次运行。
- en: Stepping through the visualization, we start with our data set. Assuming it’s
    structured in a way similar to the `_Alpaca_ dataset` we looked at earlier (it
    contains a column of instructions, inputs, and outputs), the data stored in the
    instruction and input columns are added to a prompt. That prompt is then sent
    to the model we are fine-tuning and a response is sent back from the model. We
    then compare the response from the model with the output stored in our data set
    to determine the *sentiment*. The sentiment indicates how closely aligned the
    response is to our expected output. The sentiment score is then used to inform
    what tweaks need to be made to the model’s parameters to fine-tune it toward how
    we want our model to respond. If the sentiment score indicates it’s responding
    desirably, then changes will be minimal. On the other hand, if the sentiment score
    indicates an undesirable response, then bigger changes will be made.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化过程中，我们首先从我们的数据集开始。假设它以与我们之前查看的 `_Alpaca_` 数据集类似的方式组织（它包含一个指令、输入和输出的列），指令和输入列中存储的数据将被添加到一个提示中。然后，这个提示被发送到我们正在微调的模型，模型会返回一个响应。我们随后将模型的响应与数据集中存储的输出进行比较，以确定*情感*。情感表示响应与我们的预期输出之间的匹配程度。然后，情感分数被用来告知需要调整模型参数以微调模型以实现我们希望模型如何响应。如果情感分数表明响应是可取的，那么变化将很小。另一方面，如果情感分数表明响应不可取，那么将进行更大的调整。
- en: '![](../../OEBPS/Images/CH12_F02_Winteringham2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F02_Winteringham2.png)'
- en: Figure 12.2 A visualization of what happens during fine-tuning
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 微调过程中的可视化
- en: This whole process is done programmatically using different tools and is run
    multiple times across each entry in a data set. The data set itself is also usually
    iterated through multiple times known as an *epoch*. It’s through iterating across
    multiple epochs in the fine-tuning process that the model is tuned toward how
    we want it to respond. This approach to tuning is known as instruction-based fine-tuning,
    and it’s important to understand how it works before we execute our fine-tuning
    because there are steps we need to take before the tuning can begin. First, we
    need to design the type of prompt we want to send to our model. Second, we need
    to determine how we are going to codify our prompt so that our model can read
    it. Similar to curating our data set, the choices we make for these two steps
    can also have a dramatic effect on the result of our fine-tuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程都是通过不同的工具以编程方式完成的，并在数据集中的每个条目上多次运行。数据集本身通常也会多次迭代，这被称为一个*epoch*。通过在微调过程中迭代多个epoch，模型被调整到我们希望它如何响应的方式。这种调整方法被称为基于指令的微调，在我们执行微调之前理解它的工作方式非常重要，因为我们需要在调整开始之前采取一些步骤。首先，我们需要设计我们想要发送给模型的提示类型。其次，我们需要确定我们如何将我们的提示编码化，以便我们的模型可以读取它。类似于我们选择数据集，我们在这两个步骤中做出的选择也可能对我们的微调结果产生重大影响。
- en: Prompt design
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计
- en: 'Once we know the format of our data set, we need to create an instructional
    prompt that works with the data within the data set and any additional instructions
    we want to add. For example, consider these two instruction prompts from the deeplearning.ai
    course *Finetuning Large Language Models* ([https://mng.bz/XV9G](https://mng.bz/XV9G)).
    First is a prompt that takes an instruction and an input:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了数据集的格式，我们需要创建一个指导提示，使其能够与数据集中的数据以及我们想要添加的任何附加指令一起工作。例如，考虑deeplearning.ai课程《微调大型语言模型》中的这两个指导提示（[https://mng.bz/XV9G](https://mng.bz/XV9G)）。第一个提示接受一个指令和一个输入：
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction that describes a task, paired with an input that
    provides further context. Write a response that appropriately completes the request.###
    Instruction:{instruction}### Input:{input}### Response: |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个描述任务的指令，配有一个提供更多上下文的输入。请编写一个响应，以适当地完成请求。### Instruction:{instruction}###
    Input:{input}### Response: |'
- en: 'The second is a prompt that contains just an instruction:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个提示只包含一个指令：
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:{instruction}### Response: |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个描述任务的指令。请编写一个响应，以适当地完成请求。### Instruction:{instruction}### Response:
    |'
- en: 'Notice how each prompt contains a combination of static, instructional text
    that contextualizes what information is being sent, and then tags such as `{instruction}`
    to inject data from the data set. Based on the RBP data set we want to use, we
    could configure our prompt for fine-tuning like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到每个提示都包含静态的、指导性的文本，它为发送的信息提供了上下文，然后是像`{instruction}`这样的标签来注入数据集中的数据。根据我们想要使用的RBP数据集，我们可以这样配置我们的提示以进行微调：
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###{instruction}### |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个由三个井号分隔的指令，它询问关于restful booker平台代码库的问题。请提供必要的代码来回答问题。在输出之前检查代码是否正确编译。###{instruction}###
    |'
- en: 'The prompt follows some of the prompting tactics we’ve explored in earlier
    chapters. We can use those to help us instruct the model clearly in what to expect
    in our prompt and what we want to see it responds with. To help us better understand
    the fine-tuning loop, let’s imagine we have the following entry in a data set:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示遵循我们在前面章节中探讨的一些提示策略。我们可以使用这些策略来帮助我们清楚地指导模型在提示中期待什么，以及我们希望它做出什么响应。为了更好地理解微调循环，让我们想象我们在数据集中有以下条目：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: During fine-tuning, the instruction portion of this data set would be injected
    into the prompt to create the following prompt
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，这个数据集的指令部分将被注入到提示中，以创建以下提示
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###How does the method i`nitialiseMocks` work for `BrandingServiceTest`?###
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个由三个井号分隔的指令，询问关于restful booker平台代码库的问题。请提供必要的代码来回答问题。在输出之前检查代码是否正确编译。###`initialiseMocks`方法对`BrandingServiceTest`是如何工作的？###
    |'
- en: 'This might result in the model responding with a code example written like
    this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会导致模型以如下方式响应一个代码示例：
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI Logo](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'However, our output in our data set is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们数据集中的输出如下：
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI Logo](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: This means that the sentiment score between the two sets of data is a middling
    one because, although the response is code and it has some similarities to our
    expected output in solution, the code isn’t entirely the same. This sentiment
    score would then be factored into what tweaks need to be made to the model’s parameters
    so that when this specific row in our data set comes around again, the results
    are more closely aligned. The prompt template we use affects the results of a
    fine-tuned model, and the instructions we add make a difference. However, we need
    to be mindful that what we add to a prompt template doesn’t affect just the result
    of fine-tuning but also has an impact on what is sent to the model in the first
    place. This brings us to how we turn a text-based prompt into a language that
    a model understands.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着两组数据之间的情感得分是中等水平，因为尽管响应是代码，并且它与我们的预期输出在解决方案上有些相似，但代码并不完全相同。这个情感得分将随后被考虑进需要调整模型参数以使当我们的数据集中的特定行再次出现时，结果更接近预期。我们使用的提示模板会影响微调模型的结果，我们添加的指令也会产生影响。然而，我们需要意识到我们添加到提示模板中的内容不仅会影响微调的结果，还会影响最初发送给模型的内容。这带我们来到了如何将基于文本的提示转换为模型理解的语言。
- en: Tokenization
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenization
- en: A *token* is a numerical representation of a word, phrase, or character. We
    covered the topic of tokens in chapter 10\. So why do we need to be aware of tokenization
    as part of the fine-tuning process? First, there are many different tokenizers
    available that can be utilized during data preprocessing that will tokenize the
    text in different ways. The type of model we are using will influence the type
    of tokenizer. Choosing one that doesn’t align with the model we’re tuning would
    result in our prompts being converted into token identifiers that don’t align
    with the parameters that exist inside the model we’re tuning. Roughly speaking,
    it would be like being taught a course by a teacher who was speaking in a different
    or completely made-up language.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *token* 是一个单词、短语或字符的数值表示。我们在第10章中讨论了token的概念。那么，为什么我们需要在微调过程中意识到tokenization的重要性呢？首先，在数据预处理阶段，有许多不同的tokenizer可以使用，它们将以不同的方式对文本进行tokenization。我们使用的模型类型将影响tokenizer的类型。选择一个与我们正在微调的模型不匹配的tokenizer会导致我们的提示被转换成与我们在微调的模型内部参数不匹配的token标识符。简单来说，这就像是一个用不同或完全虚构的语言授课的老师在教你课程。
- en: The second reason, which relates to our fine-tuning prompt and our data set,
    is context length. Context length is the total amount of tokens a model can process
    at once. This is important because if we create a prompt that has a large number
    of tokens within it or attempt to tune using data containing a large number of
    tokens within each entry, then our prompts risk breaching the context length,
    meaning our prompt will be truncated. Every token that is over the context length
    limit would simply be discarded or ignored, and the result would be a model being
    fine-tuned on partially complete prompts, which might create unexpected or undesired
    side effects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因与我们的微调提示和数据集相关，是上下文长度。上下文长度是指模型一次可以处理的token总数。这很重要，因为如果我们创建了一个包含大量token的提示，或者尝试使用每个条目中包含大量token的数据进行微调，那么我们的提示可能会超出上下文长度限制，这意味着我们的提示将被截断。超过上下文长度限制的每个token将简单地被丢弃或忽略，结果将是基于部分完成的提示进行微调的模型，这可能会产生意外或不希望出现的副作用。
- en: Therefore, we need to keep our context length in mind both when curating our
    dataset and designing our prompt for tuning. This might mean removing any entries
    from our data set that have the potential to overflow our context length, writing
    a prompt that has clear instructions but doesn’t overflow the token count, or
    looking for a new model that contains a larger context length.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在整理我们的数据集和设计微调提示时，我们需要考虑我们的上下文长度。这可能意味着从数据集中移除任何可能超出我们上下文长度的条目，编写具有明确指令但不会超出令牌计数的提示，或者寻找包含更大上下文长度的新的模型。
- en: Tooling and hardware
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 工具和硬件
- en: Given the many steps required for processing data and executing the fine-tuning
    process, necessary tooling needs to be in place to execute each phase. Fortunately,
    tooling for fine-tuning has progressed greatly recently. Originally, it would
    require extensive experience with tools such as Python and libraries such as PyTorch,
    Tensorflow, or Keras. And though these tools are designed to be as easy to use
    as possible, the learning curve could be quite steep and require us to build our
    fine-tuning frameworks from the ground up. If we are comfortable with this type
    of approach or are working with those who have experience with these types of
    tools, then it’s worthwhile using them. However, as interest has grown around
    fine-tuning, new tooling built on the aforementioned tools has begun to appear
    to make fine-tuning more accessible. Frameworks such as Axolotl and platforms
    such as Hugging Face let us set up fine-tuning quickly in a way that requires
    minimal tool development. The tradeoff is that these frameworks are either opinionated,
    for example selecting what tokenizers we should use, or they come at a cost.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理数据和执行微调过程需要许多步骤，因此必须具备必要的工具来执行每个阶段。幸运的是，微调工具最近取得了很大的进步。最初，这可能需要使用Python等工具以及PyTorch、Tensorflow或Keras等库的丰富经验。尽管这些工具旨在尽可能易于使用，但学习曲线可能相当陡峭，需要我们从零开始构建微调框架。如果我们对这种类型的方法感到舒适，或者与有这些类型工具经验的人一起工作，那么使用它们是值得的。然而，随着微调兴趣的增长，基于上述工具的新工具已经开始出现，使微调更加容易访问。例如Axolotl这样的框架和Hugging
    Face这样的平台允许我们快速设置微调，而无需进行大量的工具开发。权衡的是，这些框架要么具有偏见，例如选择我们应该使用的标记化器，要么是有成本的。
- en: It is not only the tooling around AI tuning that has seen growth, but also the
    infrastructure that supports it. Training models is a hardware-intensive exercise
    and requires access to graphical processing units (GPUs). This means either purchasing
    hardware that has a generous amount of CPU, RAM, and GPU for tuning, or provisioning
    computing from cloud providers. The latter is the popular option for many teams
    and a massive area of growth as it keeps the costs down for hardware requirements,
    ensuring access to newer, updated GPUs. Unsurprisingly, the big cloud computing
    companies such as Google, Microsoft, and Amazon all offer access to dedicated
    services that are designed specifically for tuning and hosting of LLMs. But some
    alternatives have begun to appear, such as RunPod, Latitude.sh, and Lambda Labs,
    which are specialist GPU cloud providers. These are options that can be used in
    conjunction with the tools we select for fine-tuning, but some services provide
    both the frameworks for fine-tuning and the computing resources to run the tuning
    for you.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅围绕AI微调的工具看到了增长，支持它的基础设施也是如此。训练模型是一项硬件密集型的工作，需要访问图形处理单元（GPU）。这意味着要么购买具有大量CPU、RAM和GPU的硬件用于微调，要么从云服务提供商那里获取计算资源。对于许多团队来说，后者是更受欢迎的选择，并且是一个快速增长的领域，因为它降低了硬件需求成本，确保了访问更新的GPU。不出所料，像谷歌、微软和亚马逊这样的大型云计算公司都提供了专门的服务，这些服务是为微调和LLM托管而设计的。但一些替代方案已经开始出现，例如RunPod、Latitude.sh和Lambda
    Labs，它们是专业的GPU云服务提供商。这些是可以与我们所选择的微调工具结合使用的选项，但一些服务同时提供微调框架和运行微调的计算资源。
- en: The market landscape for what we can use for and where we can run our fine-tuning
    is a fast-growing space. But what it highlights is that research is required to
    determine what tooling and infrastructure best suits the experience of our team
    and the type of budget we have available for tuning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用于微调的市场环境和可以运行微调的地方是一个快速发展的领域。但这也突显了需要进行研究，以确定哪些工具和基础设施最适合我们团队的经验和可用于微调的预算类型。
- en: Setting up our fine-tuning tooling
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们的微调工具
- en: There are many tools available for fine-tuning that offer different levels of
    control of the tuning process, PyTorch being a popular choice. But as mentioned
    before, there is a learning curve to setting and using these tools. If we were
    working in a context in which we wanted full control of our prompts, tokenizers,
    and tuning tools, we might choose these more granular tools. But for new starters,
    like us, who are happy using tools that trade opinions on approaches for ease
    of use, we can once again look to the AI open source community. Therefore, for
    our tuning session, we’ll be running our fine-tuning with Axolotl, a tool *designed
    to streamline the fine-tuning of various AI models, offering support for multiple
    configurations and architectures.*
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多微调工具可供选择，它们提供了不同级别的微调过程控制，PyTorch是一个流行的选择。但如前所述，设置和使用这些工具存在一个学习曲线。如果我们在一个希望对提示、分词器和微调工具有完全控制的上下文中工作，我们可能会选择这些更细粒度的工具。但对于像我们这样的新手，我们乐于使用那些为了易用性而牺牲意见的工具，我们再次可以转向AI开源社区。因此，对于我们的微调会话，我们将使用Axolotl，这是一个*专门设计用于简化各种AI模型微调的工具，提供对多种配置和架构的支持*。
- en: We can think of Axolotl as a framework for fine-tuning that contains all the
    necessary tools and processes to carry out our tuning. That means for our fine-tuning
    session, the prompting approach and tokenizer have been taken care of for us,
    allowing us to get into our fine-tuning quickly and without a massive learning
    curve.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Axolotl视为一个微调框架，其中包含执行微调所需的所有工具和流程。这意味着对于我们的微调会话，提示方法和分词器已经为我们处理好了，使我们能够快速进入微调，而不需要巨大的学习曲线。
- en: Hardware requirements for using Axolotl
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Axolotl的硬件要求
- en: 'Before we begin, it’s important to note that to carry out a fine-tuning session,
    you will need a system that has access to a GPU. If, however, you don’t have access
    to a GPU, there are cost-effective cloud platforms designed to support AI fine-tuning.
    Axolotl’s ReadMe contains links to two providers: RunPod and Latitude.sh.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，重要的是要注意，为了进行微调会话，你需要一个可以访问GPU的系统。然而，如果你没有访问GPU的权限，有一些成本效益高的云平台被设计用来支持AI微调。Axolotl的ReadMe文件包含了两个提供商的链接：RunPod和Latitude.sh。
- en: 'As someone who doesn’t have access to a GPU, I found RunPod easy to get set
    up with and reasonably priced to run multiple training sessions for less than
    $10\. If this is the approach you want to take, here are some steps to follow
    to get set up:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作为没有访问GPU的人，我发现RunPod很容易设置，并且价格合理，每次训练会话不到10美元。如果你想采取这种方法，以下是一些设置步骤：
- en: Create an account and add credit to it via [https://www.runpod.io/console/user/billing](https://www.runpod.io/console/user/billing).
    I have found the minimum transaction of $10 to be enough.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个账户并通过[https://www.runpod.io/console/user/billing](https://www.runpod.io/console/user/billing)为其添加信用。我发现最低交易额10美元就足够了。
- en: Head to GPU cloud [https://www.runpod.io/console/gpu-cloud](https://www.runpod.io/console/gpu-cloud),
    click `Choose Template` at the top of the page, find the `winglian/axolotl-runpod:main-latest`
    Docker image, and select it.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往GPU云[https://www.runpod.io/console/gpu-cloud](https://www.runpod.io/console/gpu-cloud)，点击页面顶部的“选择模板”，找到`winglian/axolotl-runpod:main-latest`
    Docker镜像，并选择它。
- en: Next, choose a pod to deploy. Depending on the time of day and demand, you will
    see which ones can be deployed and which can’t. At the time of writing, a 1x RTX
    4090 will suffice for our tuning exercise. However, if we want the tuning to go
    faster, we can choose more GPUs or a larger box.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择一个要部署的Pod。根据一天中的时间和需求，你会看到哪些可以部署，哪些不能。在撰写本文时，1x RTX 4090足以满足我们的微调练习。然而，如果我们希望微调更快，我们可以选择更多的GPU或更大的盒子。
- en: Click deploy and go through the setup wizard to fire off the creation of your
    pod. Head to [https://www.runpod.io/console/pods](https://www.runpod.io/console/pods)
    and wait for your pod to deploy.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“部署”并完成设置向导以启动Pod的创建。前往[https://www.runpod.io/console/pods](https://www.runpod.io/console/pods)并等待Pod部署。
- en: 'Once the pod is deployed, click on connect to reveal the details to SSH into
    your pod (This will require you to add an SSH public key before connecting, which
    can be done here: [https://www.runpod.io/console/user/settings](https://www.runpod.io/console/user/settings).)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦部署了Pod，点击“连接”以显示详情，然后通过SSH进入你的Pod（这需要你在连接之前添加一个SSH公钥，这可以在以下链接完成：[https://www.runpod.io/console/user/settings](https://www.runpod.io/console/user/settings)。）
- en: Once you are logged into the pod, you’ll find Axolotl is installed and ready
    to use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你登录到Pod，你会发现Axolotl已经安装并准备好使用。
- en: We’ll start by setting up Axolotl on our machine of choice (if you have chosen
    the RunPod option this can be skipped). The documentation and code for Axolotl
    can be found at [https://mng.bz/yoRG](https://mng.bz/yoRG), and it contains comprehensive
    instructions on how to install the application, offering the option of installing
    it directly on our machine or via Docker.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在我们的选择机器上设置Axolotl（如果您选择了RunPod选项，则可以跳过这一步）。Axolotl的文档和代码可在[https://mng.bz/yoRG](https://mng.bz/yoRG)找到，其中包含关于如何安装应用程序的全面说明，提供直接在我们的机器上或通过Docker安装的选项。
- en: 12.2.3 Working with fine-tuning tools
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 使用微调工具
- en: Once Axolotl is set up, we can begin configuring our session. As mentioned before,
    we’ll be using the RBP data set, which can be found on Hugging Face ([https://mng.bz/M1M7](https://mng.bz/M1M7)).
    For our model, we’ll also be using a version of Meta’s Llama-2 model, which contains
    7 billion parameters and a context window of 4k. Much like the prompts and tokenizer,
    the model settings have been taken care of in an example file that can be found
    inside the Axolotl project `examples/llama-2/lora.yml.` However, to train the
    model on our data set, we need to update the `dataset.path` in the YAML file to
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Axolotl设置完成，我们就可以开始配置我们的会话。如前所述，我们将使用RBP数据集，该数据集可在Hugging Face上找到（[https://mng.bz/M1M7](https://mng.bz/M1M7)）。对于我们的模型，我们还将使用Meta的Llama-2模型版本，该模型包含70亿个参数和4k的上下文窗口。与提示和分词器类似，模型设置已在Axolotl项目`examples/llama-2/lora.yml`中的示例文件中处理完毕。然而，为了在我们的数据集上训练模型，我们需要在YAML文件中更新`dataset.path`：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'with `path` dictating where to find the data set on Hugging Face (that is,
    where it will be downloaded from) and `type` setting out which template prompt
    we want to use. Looking at the top of the YAML file, we can also see references
    to the model and the tokenizer that will be used. Again, these can be modified
    if we want to experiment with other approaches:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`path`指定在Hugging Face上找到数据集的位置（即从哪里下载）以及`type`设置我们想要使用的模板提示。查看YAML文件顶部，我们还可以看到将要使用的模型和分词器的引用。同样，如果我们想尝试其他方法，这些也可以进行修改：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Several other settings can be found in the file and are beyond the scope of
    this chapter, but there are two we do want to highlight: `sample_packing` and
    `num_epochs`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中还有其他一些设置，但超出了本章的范围，但有两个我们想要强调：`sample_packing`和`num_epochs`。
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As for `sample_packing,` we set it to `false` because the data set isn’t large
    enough for splitting into training and testing sets (more on that later). `num_epochs`
    determines how many times we want to iterate through our data set. The default
    is `4`, meaning the fine-tuning process will loop through the whole data set four
    times before it completes. With these changes made to the YAML file, we can save,
    quit, and begin our fine-tuning.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`sample_packing`，我们将其设置为`false`，因为数据集不够大，不足以将其分为训练集和测试集（关于这一点稍后会有更多说明）。`num_epochs`决定了我们想要遍历数据集的次数。默认值为`4`，这意味着微调过程将在完成之前将整个数据集循环四次。在YAML文件中做出这些更改后，我们可以保存、退出并开始我们的微调。
- en: 12.2.4 Setting off a fine-tuning run
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 启动微调运行
- en: 'With our configuration in place, we can begin fine-tuning. To do this, we’ll
    be following the steps found in Axolotl’s ReadMe. To start, we trigger a preprocessing
    step:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的配置就绪后，我们可以开始微调。为此，我们将遵循Axolotl的ReadMe中找到的步骤。首先，我们将触发一个预处理步骤：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preprocessing step downloads the data set and runs it through the tokenizer,
    translating the data from text to tokens, ready for fine-tuning.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理步骤会下载数据集并通过分词器进行处理，将数据从文本转换为标记，以便进行微调。
- en: What is LORA?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LORA是什么？
- en: You may have noticed that the YAML file we’ve been configuring and are now using
    to start our fine-tuning session is named lora.yml. LORA is an approach to fine-tuning
    in which instead of attempting to directly tune the parameters within a model,
    a smaller subset of parameters that approximate the parameters of the model are
    created and fine-tuned, creating a LORA adapter. This means that when we deploy
    the model once it’s fine-tuned, the model is loaded with the LORA adapter inside
    it to give us the tuned behaviors we are looking for. It has become popular because
    it speeds up the fine-tuning process and allows communities and teams to share
    their adapters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们一直在配置并现在用于启动微调会话的 YAML 文件名为 lora.yml。LORA 是一种微调方法，它不是直接尝试调整模型内的参数，而是创建一个较小的参数子集来近似模型的参数，并进行微调，从而创建一个
    LORA 适配器。这意味着当我们部署微调后的模型时，模型内部将加载 LORA 适配器，以提供我们期望的微调行为。它之所以变得流行，是因为它加快了微调过程，并允许社区和团队共享他们的适配器。
- en: 'Once the preprocessing is complete, we are ready to start the fine-tuning process.
    Keep in mind that depending on the hardware being used, this process can take
    anywhere from 30 minutes to 4 hours or more, so pick a time in which the fine-tune
    can be left to run while you work on other tasks. To trigger the fine-tuning,
    we run the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理完成后，我们就准备好开始微调过程了。请注意，根据所使用的硬件，这个过程可能需要从 30 分钟到 4 小时或更长时间，所以请选择一个可以在你处理其他任务时让微调运行的时间。要触发微调，我们运行以下命令：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will pull in the YAML file that we’ve edited and kick-start the fine-tuning
    process. As the tuning begins, we’ll start to see details on its progress of the
    tune like the following example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导入我们编辑过的 YAML 文件并启动微调过程。随着微调的开始，我们将开始看到关于微调进度的详细信息，如下例所示：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each entry in the console details the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中的每个条目都详细说明了以下内容：
- en: '*Loss*—This score indicates the alignment between the expected output in our
    data set and the output of the model. The lower the score, the better aligned
    the expected and actual responses. In this example, the loss score is relatively
    high because it was taken at the start of a tuning. As the tuning progresses,
    we would hope to see the loss score reduce.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失*——这个分数表示我们数据集中预期输出与模型输出的对齐程度。分数越低，预期和实际响应的对齐程度越好。在这个例子中，损失分数相对较高，因为它是微调开始时取的。随着微调的进行，我们希望看到损失分数降低。'
- en: '*Learning rate*—This is a numerical representation of the step change made
    to the parameters in the model. Smaller steps mean more gradual changes in tuning.
    How big a step is made is determined by the sentiment score, as well as the learning
    rate hyper-parameter. In the context of AI training, a hyper-parameter is a configuration
    option that we can set before the tuning begins, which impacts the outcome of
    training or tuning. So, in the case of learning rate, we can increase the step
    range, which can result in more dramatic tunes to a model. This again may or may
    not result in a more optimal tune.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习率*——这是对模型参数所做的步长变化的数值表示。较小的步长意味着微调中的变化更渐进。步长的大小由情感分数以及学习率超参数决定。在人工智能训练的背景下，超参数是在微调开始之前可以设置的配置选项，它会影响训练或微调的结果。因此，在学习率的情况下，我们可以增加步长范围，这可能导致模型有更显著的变化。这又可能或可能不会导致更优的微调。'
- en: '*Epoch*—Earlier, we learned how we can iterate through a data set multiple
    times during fine-tuning and that each iteration is known as an epoch. The epoch
    value shown in the console output simply informs us how far through a given epoch
    we are.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*纪元*——之前，我们学习了如何在微调期间多次迭代数据集，并且每次迭代都被称为一个纪元。控制台输出中显示的纪元值只是简单地告诉我们我们处于给定纪元的哪个阶段。'
- en: These metrics are useful to give us an indication of the quality and the progress
    of a fine-tuning session. Depending on the size of the data set, the model and
    hardware supporting the tune will determine how long a tune might take. However,
    it’s not uncommon for a tune to take multiple hours, given the volume of work
    required for tuning. This is why more experienced model tuners will set up processes
    and tooling so that multiple models can be tuned at once for comparison once the
    tuning is complete.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标有助于我们了解微调会话的质量和进度。根据数据集的大小，支持微调的模型和硬件将决定微调可能需要多长时间。然而，由于微调所需的工作量很大，微调可能需要多个小时并不罕见。这就是为什么经验丰富的模型微调员会设置流程和工具，以便在微调完成后可以同时微调多个模型以进行比较。
- en: 12.2.5 Testing the results of a fine-tune
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.5 测试微调的结果
- en: 'Once our fine-tuning has been completed, we’ll want to check whether the changes
    we’ve made align with the goal that we set at the start of the fine-tuning process.
    If you recall, during the fine-tuning process, an instruction is sent to a model,
    and it returns an output. Sentiment analysis then determines how closely aligned
    the model’s output and our expected output are, which informs what tuning to the
    model’s parameters takes place. Consequently, the parameters within a model should
    now be more biased toward our context. Therefore, to test whether a model has
    been successfully tuned, we want to check what happens when we ask the model for
    new instructions that differ from the ones it was tuned on. We can do this in
    one of the two ways: inference and/or human validation.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的微调完成，我们将想要检查我们所做的更改是否与我们在微调过程开始时设定的目标一致。如果你还记得，在微调过程中，一条指令被发送给模型，然后它返回一个输出。情感分析随后确定模型的输出和我们的预期输出之间的匹配程度，这有助于确定对模型参数的调整。因此，模型内部的参数现在应该更多地偏向我们的上下文。因此，为了测试模型是否已被成功微调，我们想要检查当我们要求模型提供与微调时不同的新指令时会发生什么。我们可以通过以下两种方式之一来完成这项工作：推理和/或人工验证。
- en: Inference
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 推理
- en: Given the numerous parameters within a model and the options in instructions
    to send and outputs to receive, inference employs an automated approach to testing
    the output of a model. Inference works very similar to fine-tuning. We either
    take a slice out of a larger data set or employ a new data set that follows the
    same structure as the data set we used for fine-tuning that contains instructions
    and outputs that are different to the ones in our original tuning data. Then we
    send each of these sets of instructions to a model, capture the response, and
    then use sentiment analysis to compare what we expect the model to respond against
    what it responded. (The key difference between tuning and inference comes after
    the sentiment analysis. Tuning will make changes to the model, whereas in case
    of inference, the model is left alone). If the returned sentiment score is high,
    we can assume the model has been tuned in a way that meets our goals. If it doesn’t,
    then we can start to consider what our next steps are for a future fine-tuning
    session.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型内部有众多参数和指令发送选项以及接收输出的选项，推理采用了一种自动化的方法来测试模型的输出。推理的工作方式与微调非常相似。我们要么从更大的数据集中取出一部分，要么使用一个新的数据集，该数据集的结构与用于微调的数据集相同，包含与原始微调数据不同的指令和输出。然后我们将这些指令集发送给模型，捕获响应，然后使用情感分析来比较我们期望模型做出的响应与它实际做出的响应。（调整和推理之间的关键区别在于情感分析之后。调整会改变模型，而在推理的情况下，模型保持不变）。如果返回的情感分数很高，我们可以假设模型已被调整以满足我们的目标。如果它没有，那么我们可以开始考虑在未来的微调会话中下一步应该做什么。
- en: Human validation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 人工验证
- en: Although sentiment analysis is useful, it is based on mathematical models to
    determine alignment. Therefore, it’s also sensible to explore the outputs of a
    model manually, through human validation. This might be through using prompts
    saved in the inference data set and evaluating responses, or by testing out different
    responses by generating new prompts to see how the model reacts. To get the same
    level of scope and coverage as with the inference testing would be too expensive
    for a team, but it can provide a more human perspective that can spot discrepancies
    and/or hallucinations that inference wouldn’t.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管情感分析很有用，但它基于数学模型来确定一致性。因此，手动探索模型输出，通过人工验证也是合理的。这可能通过使用推理数据集中保存的提示并评估响应来完成，或者通过生成新的提示来测试不同的响应，以查看模型如何反应。为了获得与推理测试相同的范围和覆盖范围，对于团队来说可能过于昂贵，但它可以提供更符合人类视角的视角，可以识别推理无法识别的差异和/或幻觉。
- en: Depending on what we learn from both, the automated processes of inference and
    our experimentation will inform our next decisions. Perhaps the fine-tuning resulted
    in a model that is acceptable, and we can release it for wider use. However, we
    will often likely conclude that the resulting tuned model isn’t right for us.
    This would trigger further analysis to determine what our next steps are. Perhaps
    our data set needs changing, or we want to modify our prompt further, or perhaps
    the tokenizer could be replaced. Regardless of the choice—again, a decision-making
    process that is beyond the scope of this chapter—this highlights that fine-tuning
    projects require to experiment many times over to discover an optimal result.
    This is why teams that have a mature process around fine-tuning (or training in
    general) will run multiple fine-tuning experiments at once, tuning multiple models
    at once, with slightly different parameters to compare the results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '| 根据我们从两方面学到的知识，推理的自动化流程和我们的实验将指导我们的下一步决策。也许微调的结果产生了一个可接受的模型，我们可以将其发布以供更广泛的使用。然而，我们通常会得出结论，微调后的模型不适合我们。这将触发进一步的分析，以确定我们的下一步行动。也许我们的数据集需要改变，或者我们想要进一步修改我们的提示，或者也许可以将分词器替换掉。无论如何——再次强调，这是一个超出本章范围的决策过程——这表明微调项目需要多次实验以发现最佳结果。这就是为什么那些在微调（或一般训练）方面有成熟流程的团队会同时运行多个微调实验，同时调整多个模型，使用略有不同的参数来比较结果。|'
- en: Testing out our fine-tuning session
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 测试我们的微调会话
- en: 'Returning to our fine-tuned model, although we don’t have enough data to run
    an inference testing session, we can launch our model to test it out manually.
    To do this, we run the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的微调模型，尽管我们没有足够的数据来运行推理测试会话，但我们可以手动测试我们的模型。为此，我们运行以下命令：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this command, we are loading up the Llama-2 model and loading in the LORA
    adapter we have created as part of our tuning. The command `--gradio` allows us
    to host a user interface for our model using the gradio library ([https://www.gradio.app/](https://www.gradio.app/))
    so that we can start to test out our model via a web browser.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，我们正在加载 Llama-2 模型，并加载我们作为微调的一部分创建的 LORA 适配器。命令 `--gradio` 允许我们使用 gradio
    库（[https://www.gradio.app/](https://www.gradio.app/））托管我们的模型用户界面，这样我们就可以通过网页浏览器开始测试我们的模型。
- en: 'With our model running, we can begin to check the results of our fine-tuning
    session. First, we can check the tuning by selecting an instruction and output
    set from our data set such as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型运行时，我们可以开始检查微调会话的结果。首先，我们可以通过从我们的数据集中选择一个指令和输出集来检查微调，例如：
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| Instruction:How does the method `initialiseMocks` work for `BrandingServiceTest`?Output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '| 说明：`initialiseMocks` 方法在 `BrandingServiceTest` 中是如何工作的？输出：|'
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Using the prompt from the tuning session, we can send the following request
    to our tuned model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调会话中的提示，我们可以向我们的微调模型发送以下请求：
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response: |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 以下是一个描述任务的说明。写一个适当的响应来完成请求。### 说明：`initialiseMocks` 方法在 `BrandingServiceTest`
    中是如何工作的？### 响应：|'
- en: 'Upon sending this, we would receive a response such as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 发送此请求后，我们会收到如下响应：
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-openai.png)'
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '| 以下是一个描述任务的说明。写一个适当的响应来完成请求。### 说明：`initialiseMocks` 方法在 `BrandingServiceTest`
    中是如何工作的？### 响应：|'
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'As we can see, the model has been tuned to the point that it is returning a
    strongly aligned response based on the instructions we’ve sent. Now we can turn
    our attention to how the model copes with new untested prompts, such as this basic,
    example prompt:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型已经微调到根据我们发送的说明返回一个强烈对齐的响应。现在我们可以关注模型如何处理新的未经测试的提示，例如这个基本的示例提示：
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| What are the annotations found in the `BrandingResult` java class? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `BrandingResult` java 类中发现了哪些注释？|'
- en: 'Sending this to an example tuned model returns a response like this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到示例微调模型会返回如下响应：
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-openai.png)'
- en: '| What are the annotations found in the `BrandingResult` java class?'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在`BrandingResult` java类中发现了哪些注释？'
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Comparing this response to the original code on which the model was tuned (which
    can be found at [https://mng.bz/aVlz](https://mng.bz/aVlz)), we can see that the
    model has demonstrated some success in listing aspects of the `BrandingResult`
    class such as the variables used and the getter/setter methods. However, it is
    also missing details such as the class constructor, and it got the names of variables
    wrong (although it is at least consistent across the code). It could also be argued
    that the prompt wasn’t answered correctly as we were requesting details on annotations
    and not on the class as a whole.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将此响应与微调模型所基于的原始代码进行比较（可以在[https://mng.bz/aVlz](https://mng.bz/aVlz)找到），我们可以看到模型在列出`BrandingResult`类的某些方面（如使用的变量和getter/setter方法）方面取得了一些成功。然而，它也遗漏了诸如类构造函数等细节，并且错误地得到了变量的名称（尽管它在代码中至少是一致的）。也可以认为提示没有正确回答，因为我们要求的是关于注释的细节，而不是整个类的信息。
- en: So, in conclusion, we have seen some success with this tuning session, but more
    work is required. The tuning process has rebalanced the parameters within the
    model in a way that our context has become more dominant within it. However, the
    missing items and incorrect details mean that further tweaking is required for
    the tuning process. Perhaps we could look to improve the quality of the instructions
    in the data set or reconsider the prompt we use for tuning. Equally, we could
    look at more technical aspects of the tuning, such as choosing a model with a
    larger parameter count, or tweak hyper-parameters such as the amount of epochs
    we use for training or the learning rate.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，我们在这次调整会话中取得了一些成功，但还需要更多的工作。调整过程以某种方式重新平衡了模型内部的参数，使得我们的上下文在其中变得更加突出。然而，缺失的项目和错误细节意味着需要进一步调整微调过程。也许我们可以考虑提高数据集中指令的质量，或者重新考虑我们用于调整的提示。同样，我们可以关注调整的更多技术方面，例如选择具有更多参数的模型，或者调整超参数，如我们用于训练的epoch数量或学习率。
- en: 12.2.6 Lessons learned with fine-tuning
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.6 通过微调学到的经验
- en: This chapter gave us a taste of how the fine-tuning process works. At first,
    it may seem like an overwhelming activity. But although there is specific tooling
    and terminology to understand, by taking the fine-tuning process step by step,
    we can tackle each challenge as it comes. Ultimately, fine-tuning is very much
    about experimentation. What data we use, models we tune, tooling we employ, and
    hyper-parameters we set, they all affect the result.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本章让我们领略了微调过程的工作方式。起初，这可能看起来是一项令人望而生畏的活动。但尽管存在特定的工具和术语需要理解，通过逐步进行微调过程，我们可以逐个应对出现的挑战。最终，微调在很大程度上是关于实验。我们使用的数据、调整的模型、使用的工具以及设置的超参数，它们都会影响结果。
- en: At the time of writing, the cost of experimentation is not something that can
    be dismissed. Teams that want to carry out fine-tuning sessions require substantial
    financial backing for resources and experience. But as both private companies
    and the open source community grow, fine-tuning will become more accessible, and
    the price of hardware will likely decrease, making this a growing space in organizations
    and a challenge for teams to deliver high-quality models that can assist us and
    our organizations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，实验的成本并不是可以忽视的事情。想要进行微调会话的团队需要大量的财务支持以获取资源和经验。但随着私营公司和开源社区的不断发展，微调将变得更加容易获得，硬件的价格可能会降低，这使得微调在组织内成为一个不断增长的空间，并为团队提供高质量模型带来挑战，这些模型可以帮助我们和我们的组织。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning is the process of training a pre-existing model further, which is
    sometimes known as a foundational model.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调是进一步训练现有模型的过程，有时也称为基础模型。
- en: The fine-tuning process involves multiple steps such as goal setting, data preparation,
    processing, tuning, and testing.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调过程涉及多个步骤，如目标设定、数据准备、处理、调整和测试。
- en: Setting clear goals around what we want a fine-tuned LLM to do informs how we
    approach the fine-tuning process.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确设定我们希望微调的LLM要完成的任务的目标，这指导我们如何进行微调过程。
- en: Fine-tuned models require specifying and preparation of data.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型需要指定和准备数据。
- en: Data sets massively influence the results of a fine-tuned model. This means
    finding data relevant to our goals and formatting it in a way that helps maximize
    the output of a model after fine-tuning.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集极大地影响了微调模型的结果。这意味着找到与我们目标相关的数据，并以有助于最大化微调后模型输出的方式对其进行格式化。
- en: Fine-tuning relies on repeatedly sending prompts embedded with training data
    to get a response that we want to bias toward aligning with an expected output.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调依赖于反复发送嵌入训练数据的提示，以获得我们希望偏向于与预期输出对齐的响应。
- en: Models need prompts to be converted into a machine-readable language. This is
    achieved through the tokenization process.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要提示才能转换为机器可读的语言。这是通过分词过程实现的。
- en: Tokenization is the process in which data is sliced into smaller tokens.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词是将数据切割成更小令牌的过程。
- en: Models have a context length, which is the maximum number of tokens it can process
    at once. Send too many tokens at once, and some will be discarded, affecting the
    fine-tuning process.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型有一个上下文长度，即它一次可以处理的令牌的最大数量。一次性发送太多令牌，其中一些将被丢弃，影响微调过程。
- en: When fine-tuning, we can either build our frameworks, which require experience,
    or utilize existing frameworks, which are opinionated and/or cost money to use.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微调时，我们既可以构建自己的框架，这需要经验，也可以利用现有的框架，这些框架可能具有主观性，或者使用它们需要付费。
- en: Axolotl is a great framework for fine-tuning that is accessible to those with
    limited experience and looking to get started.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鳄鱼是针对微调的出色框架，对于经验有限且希望开始的人来说易于访问。
- en: Testing of fine-tuned models can be done in an automated fashion, using inference,
    or manually.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型的测试可以通过自动化的方式完成，使用推理或手动进行。
- en: Fine-tuning is becoming increasingly accessible to teams for use in the AI assistant
    tooling.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调正变得越来越容易为团队使用，用于人工智能助手工具。
