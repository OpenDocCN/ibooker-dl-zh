- en: 9 GPT-ing on the go
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 在路上GPT-ing
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Running a large language model locally
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行大型语言模型
- en: Comparing the results of two locally hosted large language models against those
    of ChatGPT
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较两个本地托管的大型语言模型的输出结果与ChatGPT的输出结果
- en: Determining when using offline models is appropriate
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定何时使用离线模型是合适的
- en: Imagine you are on your way to an AI conference halfway around the world. You
    are on a plane, cruising at 35,000 feet above the ground, and you want to prototype
    a new feature for your application. The airplane’s Wi-Fi is prohibitively slow
    and expensive. What if instead of paying all that money for a broken and borderline
    unusable GPT, you have one running right there on your laptop, offline? This chapter
    will review developers’ options to run a large language model (LLM) locally.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在前往世界另一端的人工智能会议的路上。你正在飞机上，飞行在离地面35,000英尺的高度，你想要为你的应用程序原型设计一个新功能。飞机的Wi-Fi既慢又贵。如果你不是为那个破旧的、几乎无法使用的GPT支付所有这些钱，而是在你的笔记本电脑上有一个离线的运行，那会怎么样？本章将回顾开发者运行本地大型语言模型（LLM）的选项。
- en: 9.1 Motivating theory
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 动机理论
- en: The introductory scenario is not too far a stretch. Although the ubiquity of
    high-speed internet is increasing, it has not yet achieved total coverage. You
    will find yourself in areas without broadband, whether at home, on the road, at
    school, or in the office. Hopefully, this book has successfully made the case
    that you should be using LLMs as a tool in your developer toolbelt. For this reason,
    you need to take precautions to ensure that you always have an LLM available to
    you in some capacity. As you use it, the more you will get from it. Like your
    dependency on an integrated development environment, without it, you are still
    a good developer; with it, however, you are much more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍性场景并不算太夸张。尽管高速互联网的普及率在增加，但它尚未实现全面覆盖。你可能会发现自己身处没有宽带的地方，无论是家里、路上、学校还是办公室。希望这本书已经成功地论证了，你应该将LLM作为你的开发者工具包中的工具使用。因此，你需要采取预防措施，确保你始终以某种形式拥有LLM。随着你的使用，你将从中获得更多。就像你对集成开发环境的依赖一样，没有它，你仍然是一个优秀的开发者；然而，有了它，你将更加出色。
- en: But fear not. Many options are available to you. This chapter will present two,
    neither requiring a complex and pained installation process. You will not need
    to memorize the APIs from a specific vendor. These approaches are not all that
    different from using ChatGPT. Your prompting skills will be fully portable. Ready?
    Let’s get started.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但不必担心。有许多选项可供你选择。本章将介绍两种，都不需要复杂和痛苦的安装过程。你不需要记住特定供应商的API。这些方法与使用ChatGPT并没有太大的不同。你的提示技巧将完全可移植。准备好了吗？让我们开始吧。
- en: 9.2 Hosting your own LLM
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 自主托管LLM
- en: 'When we look to run an LLM on our local machine, we immediately encounter a
    couple of problems: the first is that LLMs generally require significant computational
    resources. High-performance GPUs are typically necessary to run these models.
    The cost of such hardware can be prohibitive. The large size of these models means
    they require substantial memory to load and run. This can be a challenge even
    for systems with high-end GPUs, as they may not have enough VRAM to accommodate
    the model. The second problem we need to consider is the quality of the output
    of these models relative to managed LLMs like ChatGPT.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑在我们的本地机器上运行LLM时，我们立即会遇到几个问题：第一个问题是LLM通常需要大量的计算资源。运行这些模型通常需要高性能的GPU。这种硬件的成本可能很高。这些模型的大尺寸意味着它们需要大量的内存来加载和运行。即使对于高端GPU系统来说，这也可能是一个挑战，因为它们可能没有足够的VRAM来容纳模型。我们需要考虑的第二个问题是这些模型的输出质量相对于ChatGPT等托管LLM的质量。
- en: This chapter examines two instances of models explicitly selected because they
    do not require costly hardware. These models run on modest commodity hardware,
    such as the Apple MacBook Pro M2 silicon chip on which this book was written.
    We will start with Llama 2, an LLM developer by Meta and trained on 2 trillion
    tokens and offering 7 billion, 13 billion, and 70 billion parameter options. Llama
    2 can present difficulties in installing and running locally; fortunately, there
    is a Dockerized version called Ollama, which we will use in the first section
    of this chapter. In the second half of the chapter, we use GPT-4All.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了两个显式选择的模型实例，因为它们不需要昂贵的硬件。这些模型运行在普通的商用硬件上，例如这本书所写的Apple MacBook Pro M2硅芯片。我们将从Llama
    2开始，这是Meta的LLM开发者，在2000亿个标记上训练，并提供70亿、130亿和700亿参数选项。Llama 2在本地安装和运行时可能会遇到困难；幸运的是，有一个名为Ollama的Docker版本，我们将在本章的第一部分中使用。在章节的后半部分，我们将使用GPT-4All。
- en: This leaves the second problem to contend with. To this end, we will use the
    output generated by ChatGPT as the baseline against which to measure these local
    models. It should not come as a surprise, but these models perform very well relative
    to the baseline.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就留下了第二个需要解决的问题。为此，我们将使用ChatGPT生成的输出作为基准，来衡量这些本地模型。这并不令人惊讶，但这些模型相对于基准表现非常好。
- en: 9.2.1 Baselining with ChatGPT
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 使用ChatGPT进行基准测试
- en: 'In this chapter, we will use a novel (at least in this book) problem: calculating
    the standard deviation of a list of integers. The standard deviation measures
    the variation in a set of values. Throughout the chapter, we will use the same
    prompt and present it to each model:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个新颖的问题（至少在这本书中是这样）：计算整数列表的标准差。标准差衡量一组值的变异。在整个章节中，我们将使用相同的提示并将其呈现给每个模型：
- en: '|'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code you would use and walk me through
    it step by step. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 作为一名数学家，您正在尝试使用纯Python计算列表的标准差。请向我展示您将使用的代码，并一步一步地引导我。 |'
- en: ChatGPT provides the following explanation of the steps involved and the method.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT提供了以下关于涉及步骤和方法的解释。
- en: Listing 9.1 ChatGPTs explanation of calculating standard deviation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 ChatGPT对计算标准差的解释
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If we write a small `main` function to sum the list of integers from 1 to 4,
    we get the value 1.4142135623730951 or a close approximation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们编写一个小的`main`函数来计算从1到4的整数列表的总和，我们得到1.4142135623730951或其近似值。
- en: Listing 9.2 `main` function to drive our standard deviation calculation
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 `main`函数驱动我们的标准差计算
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Feel free to use your favorite calculator, financial modeling program, or other
    mechanism to verify the result. You will find that this value is more or less
    correct. Now that we have a baseline against which to compare, we can begin our
    comparison.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随意使用您喜欢的计算器、财务建模程序或其他机制来验证结果。您会发现这个值或多或少是正确的。现在我们已经有了可以比较的基准，我们可以开始我们的比较了。
- en: 9.2.2 Asking Llama 2 to spit out an answer
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 让Llama 2输出答案
- en: 'It’s time to introduce our first locally running LLM, Llama 2\. As previously
    mentioned, this model can require considerable effort to install (at least at
    the time of writing). To make it easier to start evaluating this model, we will
    use the Dockerized version: Ollama. The following two Docker commands will run
    our model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候介绍我们的第一个本地运行的LLM（大型语言模型），Llama 2了。正如之前提到的，这个模型可能需要相当大的努力才能安装（至少在撰写本文时是这样）。为了使评估这个模型更容易，我们将使用Docker版本：Ollama。以下两个Docker命令将运行我们的模型。
- en: Listing 9.3 Starting and running Ollama
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 启动和运行Ollama
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If this works correctly, you will (eventually) see a prompt of three greater-than
    symbols (`>`). The first time you run this command, Ollama will need to download
    the Llama 2 model, which is several gigabytes. This will likely take a while and
    needs to be done with a stable internet connection. However, you will not need
    an internet connection once this has been completed. Therefore, ensure that you
    run this command before using Ollama in offline mode.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个操作正确，您最终会看到一个由三个大于号（`>`）组成的提示。第一次运行此命令时，Ollama需要下载Llama 2模型，该模型有数GB大小。这可能需要一段时间，并且需要稳定的互联网连接来完成。然而，一旦完成，您将不再需要互联网连接。因此，请确保在离线模式下使用Ollama之前运行此命令。
- en: 'With Ollama installed and running, we can now ask Llama 2 to introduce itself:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装并运行Ollama之后，我们现在可以要求Llama 2自我介绍：
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| Imagine that you are the author of a book on Generative AI. You are working
    on a chapter about running an LLM locally on your machine. How might you describe
    Llama? |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 假设你是《生成式AI》一书的作者。你正在编写一个关于在本地机器上运行LLM的章节。你将如何描述Llama？|'
- en: Listing 9.4 Llama 2 provides some background on itself
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 Llama 2对自己的一些背景介绍
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With introductions out of the way, we can begin our assessment of the output
    from Llama 2, comparing it to the output produced by ChatGPT. We have a baseline
    answer that we know is correct, so we can accurately assess the correctness of
    the code that Llama 2 delivers. Let’s ask Llama 2 to calculate the standard deviation
    of a list of numbers using the same prompt as before:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍完毕之后，我们可以开始评估Llama 2的输出，将其与ChatGPT产生的输出进行比较。我们有一个已知是正确的基线答案，因此我们可以准确地评估Llama
    2提供的代码的正确性。让我们用之前的提示让Llama 2计算一个数字列表的标准差：
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code that you would use and walk
    me through it step by step. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 作为一名数学家，你正在尝试使用纯Python计算一个列表的标准差。请向我展示你会使用的代码，并一步一步地引导我。|'
- en: Listing 9.5 Llama 2’s first attempt to calculate standard deviation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 Llama 2第一次尝试计算标准差
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Although this may be the way we would do it in the real world (kudos, Llama
    2), it does not provide us with an apples-to-apples comparison. Let’s reframe
    and refine the question:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能是我们在现实世界中会这样做的方式（恭喜，Llama 2），但它并没有提供给我们一个苹果对苹果的比较。让我们重新构建并细化问题：
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| Do not use any libraries, built in Python functions. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 不要使用任何库，内置的Python函数。|'
- en: Okay, technically, it’s not a question, but hopefully, it will get the job done.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，技术上这并不是一个问题，但希望它能完成这项任务。
- en: Listing 9.6 Llama 2’s second attempt to calculate standard deviation
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 Llama 2第二次尝试计算标准差
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Attentive readers (and do not worry if you missed this at first) may notice
    that Llama 2 subtracted one from the number of items in the list. This is curious.
    It does not match ChatGPT’s response. Let us see if we can figure out why. Let’s
    ask it:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者（如果你一开始错过了这个，不要担心）可能会注意到Llama 2从列表中的项目数量中减去了一个。这很奇怪。它与ChatGPT的响应不匹配。让我们看看我们能否弄清楚原因。让我们询问它：
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| What is the formula for the standard deviation? |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 标准差的公式是什么？|'
- en: Listing 9.7 Llama 2’s understanding of the standard deviation formula
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 Llama 2对标准差公式的理解
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is consistent with the code that Llama 2 generated, so that’s good. However,
    it does not explain the apparent discrepancy in the output between this and ChatGPT.
    Let’s ask it directly why it subtracted the one:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Llama 2生成的代码一致，所以这是好的。然而，它并没有解释为什么这个输出和ChatGPT的输出之间有明显的差异。让我们直接询问它为什么减去了一个：
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| When you subtract 1 from the number of numbers in the list is that to add
    a degree of freedom? If so, why? |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 当你从列表中的数字数量减去1时，这是否意味着增加一个自由度？如果是，为什么？|'
- en: If you have taken statistics in the past, you may see what is coming. If not,
    Llama 2 will provide a useful explanation for what is meant by *degrees of freedom*
    and why it is used.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前学过统计学，你可能已经看到了接下来会发生什么。如果没有，Llama 2将提供一个有用的解释，说明*自由度*的含义以及为什么它被使用。
- en: Listing 9.8 Llama 2’s explanation of degrees of freedom
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 Llama 2对自由度的解释
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s take a quick detour to answer this question: What is the formula for
    the standard deviation? The answer is, which one? There are actually two: sample
    and population. The sample standard deviation refers to the standard deviation
    calculated from a subset, or *sample*, of data points taken from a larger population.
    This formula is shown and explained in figure 9.1 This is used when working with
    limited information and cannot be applied to the entire dataset (population).
    This is contrasted to the *population* standard deviation, which is how much the
    individual data points deviate from the average value of the population. This
    formula is shown in figure 9.2'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速绕道回答这个问题：标准差的公式是什么？答案是，哪一个？实际上有两个：样本和总体。样本标准差是指从更大的总体中抽取的子集或*样本*中的数据点计算出的标准差。这个公式在图9.1中展示并解释了。这用于处理有限信息时，不能应用于整个数据集（总体）。这与*总体*标准差形成对比，总体标准差是指个别数据点与总体平均值偏离的程度。这个公式在图9.2中展示。
- en: '![](../Images/CH09_F01_Crocker2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F01_Crocker2.png)'
- en: Figure 9.1 The formula for the sample standard deviation
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 样本标准差的公式
- en: Both formulas involve squaring the difference between each data point and the
    mean, summing these squares, and then taking the square root of the sum. This
    provides a measure of the spread of values around the mean.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个公式都涉及将每个数据点与平均值的差值平方，然后将这些平方值相加，最后取和的平方根。这提供了围绕平均值值分布的度量。
- en: '![](../Images/CH09_F02_Crocker2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Crocker2.png)'
- en: Figure 9.2 The formula for the population standard deviation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 总体标准差的公式
- en: Therefore, we can conclude that Llama 2 generated a function to calculate the
    sample standard deviation rather than the population standard deviation (the baseline).
    Given that this is in the realm of statistics specifically, we can ask,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论，Llama 2 生成了一个用于计算样本标准差的函数，而不是总体标准差（基准）。鉴于这属于特定于统计学的领域，我们可以问，
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| How would you have changed your response if I had asked you to assume the
    role of a statistician rather than a mathematician? |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 如果我要求你扮演统计学家角色而不是数学家角色，你会如何改变你的回答？|'
- en: Listing 9.9 Llama 2’s response in the role of a statistician
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9 Llama 2 扮演统计学家角色的回答
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Llama 2 changes its response if we ask it to assume the statistician role. Unfortunately,
    it does not divide by the length of the list of numbers. Once this is fixed, we
    will get the correct answer for the population standard deviation. Let this be
    a reminder that LLMs can confidently produce incorrect answers. Always double-check
    the results against your knowledge or that of experts. Now, type **`/bye`** to
    end your session. (Typing `/bye` in Llama 2 signals that you wish to terminate
    the session.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要求 Llama 2 扮演统计学家角色，它会改变其回答。不幸的是，它没有将结果除以数字列表的长度。一旦这个问题得到解决，我们将得到正确的总体标准差。让我们记住，LLMs
    可以自信地产生错误的答案。始终将结果与您的知识或专家的知识进行双重检查。现在，键入 **`/bye`** 以结束您的会话。（在 Llama 2 中键入 `/bye`
    表示您希望终止会话。）
- en: 'Let’s focus on another LLM we can run locally: GPT-4All.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注另一个我们可以在本地运行的 LLM：GPT-4All。
- en: 9.2.3 Democratizing answers with GPT-4All
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 使用 GPT-4All 民主化答案
- en: GPT-4All is open source software developed by Anthropic that allows users to
    train and operate their own LLMs. It is based on GPT-3 and therefore may not operate
    as effectively as a GPT-4-based model; however, it can be run directly on a personal
    computer without the need for an internet connection. Despite the similarity in
    name, it is not related to GPT-4 at all.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4All 是由 Anthropic 开发的一款开源软件，它允许用户训练和操作他们自己的大型语言模型（LLMs）。它基于 GPT-3，因此可能不如基于
    GPT-4 的模型有效；然而，它可以在没有互联网连接的个人电脑上直接运行。尽管名称相似，但它与 GPT-4 完全无关。
- en: 'Before we dive in and use it, let’s have GPT4-All introduce itself, using the
    following prompt:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入使用它之前，让我们让 GPT4-All 自我介绍，使用以下提示：
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| Imagine that you are the author of a book on Generative AI. You are working
    on a chapter about running an LLM locally. How might you describe GPT-4All? |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 假设你是《生成式人工智能》一书的作者。你正在编写关于在本地运行 LLM 的章节。你将如何描述 GPT-4All？|'
- en: Listing 9.10 GPT-4All’s description of itself
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 GPT-4All 对自身的描述
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Unlike Ollama, GPT-4All requires installation. Fortunately, the process is
    relatively quick and painless: navigate to [https://gpt4all.io/](https://gpt4all.io/),
    download the appropriate installer for your computer, and follow the installation
    instructions. Once you have installed the application, you will receive instructions
    to download a model, as shown in figure 9.3.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Ollama 不同，GPT-4All 需要安装。幸运的是，这个过程相对快速且痛苦不大：导航至 [https://gpt4all.io/](https://gpt4all.io/)，下载适合您电脑的安装程序，并按照安装说明进行操作。安装应用程序后，您将收到下载模型的说明，如图
    9.3 所示。
- en: '![](../Images/CH09_F03_Crocker2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F03_Crocker2.png)'
- en: Figure 9.3 GPT-4All requires that you download models to be run.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 GPT-4All 要求您下载模型才能运行。
- en: I downloaded and used Mistral OpenOrca, a high-performance parallel and distributed
    programming framework designed to simplify the development of large-scale, data-intensive
    applications on high-performance computing clusters or cloud environments. It’s
    particularly well suited for handling big data processing tasks, scientific simulations,
    machine learning algorithms, and other compute-intensive workloads that require
    efficient resource utilization and scalability across multiple nodes. Mistral
    OpenOrca provides a set of tools and libraries to manage job scheduling, communication,
    fault tolerance, and load balancing in distributed environments, making it an
    ideal choice for developers working on complex projects requiring high performance
    and parallelism. Both the GPT-4All introduction and the majority of this paragraph
    were generated by Mistral OpenOrca.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我下载并使用了Mistral OpenOrca，这是一个高性能的并行和分布式编程框架，旨在简化在高性能计算集群或云环境中开发大规模、数据密集型应用程序的过程。它特别适合处理大数据处理任务、科学模拟、机器学习算法以及其他需要高效资源利用和跨多个节点可扩展性的计算密集型工作负载。Mistral
    OpenOrca提供了一套工具和库来管理分布式环境中的作业调度、通信、容错和负载均衡，使其成为开发复杂项目、需要高性能和并行性的开发者的理想选择。GPT-4All的介绍和本段的大部分内容都是由Mistral
    OpenOrca生成的。
- en: If you click the Downloads button from Settings, you will see the downloaded
    model, as shown in figure 9.4\. You will also find the complete chat history in
    the menu, as shown in figure 9.5.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从设置中点击下载按钮，你会看到如图9.4所示的下载模型。你还可以在菜单中找到完整的聊天历史，如图9.5所示。
- en: '![](../Images/CH09_F04_Crocker2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Crocker2](../Images/CH09_F04_Crocker2.png)'
- en: Figure 9.4 The downloaded models in GPT-4All
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 GPT-4All中下载的模型
- en: '![](../Images/CH09_F05_Crocker2.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F05_Crocker2](../Images/CH09_F05_Crocker2.png)'
- en: Figure 9.5 The list of chats you’ve had with the selected GPT-4All model
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 与所选GPT-4All模型进行的聊天列表
- en: 'Let’s get one thing out of the way by using the following prompt:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下提示来消除一些疑虑：
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![logo-NC](../Images/logo-NC.png)'
- en: '| Is an active internet connection required to generate output? |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 生成输出是否需要活跃的互联网连接？ |'
- en: Listing 9.11 GPT-4All’s assurance that internet is not required
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 GPT-4All保证不需要互联网
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can move on to compare GPT4-All/Mistral OpenOrca (from here on simply
    referred to as GPT-4All) with the following prompt:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续比较GPT4-All/Mistral OpenOrca（以下简称为GPT-4All）与以下提示：
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![logo-NC](../Images/logo-NC.png)'
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code that you would use. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 作为一名数学家，你正在尝试使用纯Python计算列表的标准差。请向我展示你会使用的代码。 |'
- en: Listing 9.12 GPT-4All’s attempt to calculate standard deviation
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.12 GPT-4All尝试计算标准差
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A couple of things to note. First, GPT-4All generates the code for the population
    standard deviation. Second, the text and code are generated very quickly (on my
    computer, four to five times more quickly than by Ollama). Third, the code is
    exactly right! You are encouraged to download different models, ask what each
    is good at, and compare the results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意。首先，GPT-4All生成了计算总体标准差的代码。其次，文本和代码生成得非常快（在我的电脑上，比Ollama快四到五倍）。第三，代码完全正确！鼓励你下载不同的模型，询问每个模型擅长什么，并比较结果。
- en: You may wonder when you would want to use Llama 2 versus GPT-4All versus ChatGPT.
    Excellent question! Take a look at figure 9.6\. Llama 2 is a great, general model.
    It excels at summarizing large bodies of text and writing contextually appropriate
    passages of text. GPT-4All’s use cases are as diverse as the available models.
    For example, Mistral OpenOrca is ideal when you need a multilingual model that
    can handle various languages effectively. ChatGPT is the best option if your primary
    goal is to have natural-language conversations with the AI model and receive the
    most accurate responses based on input (which really should be what you want).
    An obvious limitation of ChatGPT is that it requires a persistent internet connection.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道何时使用Llama 2与GPT-4All和ChatGPT。这是一个很好的问题！看看图9.6。Llama 2是一个优秀的通用模型。它擅长总结大量文本并撰写上下文适当的文本段落。GPT-4All的使用案例与可用的模型一样多样化。例如，Mistral
    OpenOrca在你需要一个能够有效处理各种语言的多元语言模型时非常理想。如果你的主要目标是与AI模型进行自然语言对话并获得基于输入的最准确响应（这确实应该是你想要的），ChatGPT是最佳选择。ChatGPT的一个明显限制是它需要一个持续的互联网连接。
- en: '![](../Images/CH09_F06_Crocker2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F06_Crocker2](../Images/CH09_F06_Crocker2.png)'
- en: Figure 9.6 A comparison of the models that we used in this chapter
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 本章中使用的模型比较
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Local LLMs require significant computational resources and costly hardware for
    optimal performance; however, alternatives like Llama 2 run on modest commodity
    hardware with varying parameter options. These models can produce output that
    is generally high quality, but not quite of the quality of the responses of managed
    LLMs like ChatGPT (at least at the time of writing).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地 LLM 需要大量的计算资源和昂贵的硬件才能实现最佳性能；然而，像 Llama 2 这样的替代方案可以在普通的商用硬件上运行，并提供不同的参数选项。这些模型可以生成通常质量较高的输出，但并不完全达到像
    ChatGPT 这样的托管 LLM 的响应质量（至少在撰写本文时是这样）。
- en: Both population and sample standard deviations measure variability in datasets.
    They differ in terms of the entire population being considered versus a smaller
    subset or sample; this means the former provides an exact measurement for the
    whole group, whereas the latter is an estimate based on a portion of it.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体标准差和样本标准差都用于衡量数据集中的变异性。它们在考虑整个总体还是较小的子集或样本方面有所不同；这意味着前者为整个群体提供了一种精确的测量，而后者是基于其一部分的估计。
- en: Llama 2 excels at diverse text handling, such as generating summaries or writing
    coherent passages of text and code, GPT-4All offers various use cases, including
    multilingual support; and ChatGPT shines in natural language conversations with
    accurate responses (but it requires an internet connection).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama 2 在处理各种文本方面表现出色，例如生成摘要或编写连贯的文本和代码段落，GPT-4All 提供了各种用例，包括多语言支持；而 ChatGPT
    在与自然语言进行对话并给出准确响应方面表现出色（但需要互联网连接）。
- en: 'In addition to offline availability, there are various situations in which
    using an offline version of an LLM such as Llama 2 or GPT-4All makes sense:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了离线可用性之外，还有各种情况，使用像 Llama 2 或 GPT-4All 这样的 LLM 的离线版本是有意义的：
- en: '*Privacy and security concerns—*Offline models eliminate the need to transmit
    sensitive data over the internet, reducing privacy risks and potential cybersecurity
    threats.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐私和安全问题—*离线模型消除了在互联网上传输敏感数据的需要，降低了隐私风险和潜在的网络安全威胁。'
- en: '*Cost savings—*Running a local model on your own hardware may reduce cloud
    computing costs associated with using an online service like ChatGPT or OpenAI
    API.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本节约—*在自己的硬件上运行本地模型可能会降低使用 ChatGPT 或 OpenAI API 等在线服务相关的云计算成本。'
