- en: Chapter 18\. Introduction to RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章. RAG简介
- en: Remember that first time you chatted with an LLM like ChatGPT—and how it was
    extremely insightful about things you didn’t expect it to know? I had worked with
    LLMs before the release of ChatGPT and on projects that highlighted LLM abilities,
    and I *still* was surprised by what they could do. Remember the famous on-stage
    demonstration by Google, where the CEO had a conversation with the planet Pluto?
    It was one of those fundamental mind shifts in the possibilities of AI that we’re
    *still* exploring as it continues to evolve.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 记得你第一次与像ChatGPT这样的LLM（大型语言模型）聊天，以及它对你意想不到的事情有多么深刻的见解吗？在ChatGPT发布之前，我就已经与LLM合作过，并在一些突出LLM能力的项目上工作过，但我仍然对他们能做什么感到惊讶。记得谷歌在舞台上进行的著名演示吗？CEO与冥王星进行了对话？这是我们仍在探索的AI可能性中的基本思维转变之一，随着AI的持续发展，我们仍在探索。
- en: 'But, despite all that brilliance, there were still limitations, and the more
    I and others worked with LLMs, the more we encountered them. The transformer-based
    architecture that we discussed in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580)
    was brilliant at snarfing up text data, creating QKV mappings from it, and learning
    how to artificially understand the semantics of the text as a result. But despite
    the volume of text used to build those mappings, there was—and always is—one blind
    spot: private data. In particular, if there is data that you want to work with
    that the model was not trained on, you’re at a major risk of hallucination!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，尽管有所有这些辉煌，仍然存在局限性，我和其他人越是用LLM工作，就越会遇到它们。我们在第15章（ch15.html#ch15_transformers_and_transformers_1748549808974580）中讨论的基于transformer的架构在抓取文本数据、从中创建QKV映射以及学习如何人工理解文本的语义方面非常出色。但尽管用于构建这些映射的文本量很大，仍然存在——并且始终存在的一个盲点：私人数据。特别是，如果你想要使用模型未训练过的数据，你面临的主要风险是胡编乱造！
- en: Gaining skills to help mitigate this blind spot could potentially be the *most*
    valuable thing you can do as a software developer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 获得帮助减轻这种盲点的技能可能是在作为软件开发者你能做的最有价值的事情。
- en: For this chapter, I want you to think about AI models and in particular large
    generative models like LLMs *differently*. Stop seeing them as intelligent and
    knowledgeable and start seeing them as *utilities* to help you parse your data
    better. Think of everything they have learned not as a knowledge base in and of
    itself but as a way that they have generalized understanding of language by being
    extensively well read.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一章，我希望你以不同的方式思考AI模型，特别是像LLM这样的大型生成模型。停止将它们视为聪明和有知识的，开始将它们视为*工具*，帮助你更好地解析数据。将它们所学到的一切不是作为一个知识库本身，而是它们通过广泛阅读而获得的语言泛化理解的方式。
- en: I call this *artificial understanding,* as a complementary technology to AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为*人工理解*，作为AI的补充技术。
- en: Then, once you treat your favorite LLM as an engine for artificial understanding,
    you can start having it understand your private text—stuff that wasn’t in its
    training set—and through that understanding, process your text in new and interesting
    ways.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦你将你最喜欢的LLM视为人工理解的一个引擎，你就可以开始让它理解你的私人文本——那些不在其训练集中的内容——并通过这种理解，以新的和有趣的方式处理你的文本。
- en: Let’s explore this with a scenario. Imagine you’re discussing your favorite
    sci-fi novel with an AI model. You want to ask about characters, plot, theme,
    and stuff like that, but the model struggles with the specifics, offering only
    general responses—or worse, hallucinating them. For example, take a look at [Figure 18-1](#ch18_figure_1_1748550073457538),
    which shows the results I got when I was chatting with ChatGPT about a character
    from a novel called *Space Cadets.*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个场景来探讨这一点。想象你正在与一个AI模型讨论你最喜欢的科幻小说。你想要询问关于角色、情节、主题等内容，但模型在具体细节上挣扎，只能提供一般性的回答——或者更糟糕的是，胡编乱造。例如，看看[图18-1](#ch18_figure_1_1748550073457538)，它展示了我在与ChatGPT讨论一本名为*太空战士*的小说中的角色时得到的结果。
- en: '![](assets/aiml_1801.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1801.png)'
- en: Figure 18-1\. Chatting with GPT about a character
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-1. 与GPT讨论一个角色
- en: This is all very interesting—except that it’s wrong. First of all, the character
    is from *North* Korea, not *South* Korea.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都非常有趣——除了它是错误的。首先，这个角色来自*朝鲜*，而不是*韩国*。
- en: GPT is being confidently incorrect. Why? Because this novel isn’t in the training
    set! I wrote it in 2014, and it was published by a small press that folded just
    a few months afterward. As such, it’s relatively obscure and the perfect fodder
    for us to use to explore RAG. By the end of this chapter, you’ll have used your
    PyTorch skills to create an application that is much smarter at understanding
    this novel and, indeed, the character in question. And yes, you’ll have the full
    novel to work with!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 正在自信地犯错。为什么？因为这个小说不在训练集中！我是在 2014 年写的，它由一家小型出版社出版，几个月后就倒闭了。因此，它相对较为冷门，是我们用来探索
    RAG 的完美素材。到本章结束时，你将使用你的 PyTorch 技能创建一个能够更智能地理解这部小说以及所讨论角色的应用程序。而且，你将拥有整部小说来工作！
- en: 'A small aside: when I first used an LLM for tasks like this, my mind was blown.
    Its ability to *artificially understand* the contents and context of my own writing
    was like having a partner beside me to critique my work and to help me dig deep
    into the characters and themes. The book ends on a cliffhanger, and I never came
    back to write any sequels. Having conversations with an LLM about the character
    arcs, etc., gave me a whole new fount of wisdom about where it could go.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小插曲：当我第一次使用 LLM 来完成这类任务时，我感到震惊。它能够 *人工理解* 我自己的写作内容和上下文，就像身边有一个伙伴来批评我的作品，并帮助我深入挖掘角色和主题。这本书以悬念结束，我从未回来写任何续集。与
    LLM 讨论角色弧线等问题，为我提供了关于它可能走向的全新智慧源泉。
- en: And of course, you aren’t limited to works of fiction. Almost every business
    has a trove of internal intelligence that’s locked up in documents that would
    take a human too much time to read, index, cross-correlate, and understand to
    be able to answer queries—so the ability of an LLM to artificially understand
    them to help you mine the text for knowledge is second-to-none.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你不仅限于文学作品。几乎每个企业都有大量内部智力资源被锁在文档中，这些文档需要人类花费大量时间阅读、索引、交叉关联和理解，才能回答查询——因此，LLM
    能够人工理解这些文档以帮助您挖掘知识的能力是无与伦比的。
- en: That’s why I’m excited about RAG. And I hope you will be, too, after you finish
    this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，我对 RAG 感到兴奋。希望你在完成本章后也会如此。
- en: What Is RAG?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 RAG？
- en: The acronym *RAG* stands for *retrieval augmented generation*, which works to
    bridge the knowledge gap between what an LLM has been trained on and private data
    you own that it doesn’t have mappings for. At query time, as well as with a prompt
    like “Tell me about the character…,” we’ll also feed it information snippets from
    the local datastore. So, for example, if we’re querying about a character from
    a novel, local data might include things like her hometown, her favorite food,
    her values, and how she speaks. When we pass *that* data along with the query,
    a lot of it *is* in the training set for the LLM, and as such, the LLM can have
    a much more informed opinion about her. Not least, the mistake the LLM made in
    [Figure 18-1](#ch18_figure_1_1748550073457538) can be mitigated—when the LLM is
    given her hometown, it can at least get the country right!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写 *RAG* 代表 *retrieval augmented generation*，其目的是弥合 LLM 训练时所依据的知识与您拥有的、LLM 没有映射的私有数据之间的差距。在查询时间以及像“告诉我关于这个角色的……”这样的提示下，我们还会向其提供来自本地数据存储的信息片段。例如，如果我们正在查询小说中的一个角色，本地数据可能包括她的家乡、她最喜欢的食物、她的价值观以及她的说话方式。当我们把
    *这些* 数据与查询一起传递时，其中很多都是 LLM 训练集中的，因此 LLM 可以对她的了解更加深入。至少，LLM 在 [图 18-1](#ch18_figure_1_1748550073457538)
    中犯的错误可以得到缓解——当 LLM 被提供她的家乡时，它至少可以正确地确定她的国籍！
- en: '[Figure 18-2](#ch18_figure_2_1748550073457600) shows the flow of a typical
    query to an LLM. It’s quite basic: you pass in a prompt and the transformers do
    their magic by going through the knowledge that the LLM learned to produce QKV
    values to generate a response.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-2](#ch18_figure_2_1748550073457600) 展示了向 LLM 发起典型查询的流程。它相当基础：你传递一个提示，然后变压器通过遍历
    LLM 学到的知识来产生 QKV 值，从而生成一个响应。'
- en: '![](assets/aiml_1802.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1802.png](assets/aiml_1802.png)'
- en: Figure 18-2\. Typical flow of a query to an LLM
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-2\. 向 LLM 发起查询的典型流程
- en: As we’ve demonstrated , if the LLM doesn’t have much knowledge of the specifics,
    it will fill in the gaps—and it does a pretty good job. For example, even though
    it got her nationality wrong in the example shown in [Figure 18-1](#ch18_figure_1_1748550073457538),
    it was at least able to infer that her name is Korean!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所展示的，如果 LLM 对具体知识了解不多，它会填补这些空白——而且它做得相当不错。例如，尽管它在 [图 18-1](#ch18_figure_1_1748550073457538)
    中展示的例子中错误地推断出她的国籍，但它至少能够推断出她的名字是韩国式的！
- en: With RAG, we change this flow to augment the query with extra information that
    we bundle in (see [Figure 18-3](#ch18_figure_3_1748550073457629)). We do this
    by having a local database of the content of the book, and then we search that
    for things that are *similar* to the query. You’ll see the details of how that
    works shortly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RAG，我们改变了这个流程，通过添加我们捆绑的额外信息来增强查询（参见[图18-3](#ch18_figure_3_1748550073457629)）。我们通过拥有一个包含书籍内容的本地数据库来实现这一点，然后在该数据库中搜索与查询相似的内容。你很快就会看到它是如何工作的细节。
- en: '![](assets/aiml_1803.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1803.png)'
- en: Figure 18-3\. Typical flow of a RAG query with an LLM
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-3\. 使用LLM的RAG查询的典型流程
- en: The goal here is to enhance the initial prompt with a lot of additional context.
    So, scenes in the book might have her mention her hometown, her family history,
    favorite foods, why she likes people or things, etc. When that is passed to the
    LLM along with the query, the LLM has a lot more to work with—including things
    that it *has* learned about, so its interpretation of the character becomes a
    lot more intelligent. It therefore *artificially understands* the content better.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是增强初始提示，添加大量额外的上下文。因此，书中的场景可能会提到她的家乡、家族历史、最喜欢的食物、她为什么喜欢人或事物等。当这些内容与查询一起传递给LLM时，LLM有更多的工作内容——包括它已经学习到的东西，因此其对角色的解读变得更加智能。因此，它能够*人为地理解*内容得更好。
- en: The key to all of this, of course, is in being able to retrieve the best information
    to bundle with the prompt to make the most of the LLM. You can achieve this by
    storing content from the source material (in this case, the book) in a way that
    lets you do searches for things that are semantically relevant. To that end, you’ll
    use a vector store. We’ll explore that next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这一切的关键在于能够检索出最佳信息，并将其与提示结合，以充分利用LLM（大型语言模型）。你可以通过将源材料（在这种情况下，是书籍）的内容以允许你进行语义相关事物搜索的方式存储来实现这一点。为此，你将使用一个向量存储库。我们将在下一节中探讨这一点。
- en: Getting Started with RAG
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG入门
- en: To get started, let’s first explore how to create a vector database. To do this,
    you’ll use a database engine that supports vectors and similarity search.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们首先探索如何创建一个向量数据库。为此，你需要使用支持向量和相似性搜索的数据库引擎。
- en: These work with the idea of storing text as vectors that represent it by using
    embeddings. We saw these in action in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    For simplicity, you’ll start by using a pre-built, pre-learned set of embeddings
    from OpenAI with an API provided by LangChain. These will be combined with a vector
    store database called Chroma that is free and open source.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法基于将文本存储为表示它的向量的想法。我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中看到了这些方法的应用。为了简单起见，你将首先使用OpenAI提供的预构建、预学习的嵌入集，并通过LangChain提供的API来使用它们。这些嵌入将与一个名为Chroma的向量存储数据库相结合，该数据库是免费且开源的。
- en: 'Let’s include the following imports:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们包含以下导入：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `PyPDFLoader`, as its name suggests, is used for managing PDF files in Python.
    I’m providing the book as a PDF, so we’ll need this.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`PyPDFLoader`用于在Python中管理PDF文件。我提供这本书作为PDF格式，因此我们需要这个工具。
- en: The `RecursiveCharacterTextSplitter` is a really useful class for slicing the
    book up into text chunks. It provides flexibility on the size of the chunk and
    the overlap between chunks. We’ll explore that in detail a little later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter`是一个非常有用的类，可以将书籍分割成文本块。它提供了对块大小和块之间重叠的灵活性。我们将在稍后详细探讨这一点。'
- en: The `OpenAIEmbeddings` class gives us access to the embeddings learned by Open
    AI while training GPT, and it’s a nice shortcut to make things quicker for us.
    We don’t need to learn our own embeddings for this application—as long as our
    text is encoded in a set of embeddings and our prompt uses the same ones, we can
    use them for similarity search. There are lots of options for this, and Hugging
    Face is a great repository where you can look for the latest and greatest.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`OpenAIEmbeddings`类为我们提供了访问OpenAI在训练GPT时学习的嵌入的能力，并且这是一个让我们更快完成工作的便捷途径。对于这个应用，我们不需要学习自己的嵌入——只要我们的文本被编码在一系列嵌入中，并且我们的提示使用相同的嵌入，我们就可以用于相似性搜索。有很多这样的选项，Hugging
    Face是一个很好的仓库，你可以在这里找到最新和最好的资源。'
- en: Finally, the `Chroma` database provides us with the ability to store and search
    text based on similarity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`Chroma`数据库为我们提供了基于相似性存储和搜索文本的能力。
- en: Understanding Similarity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解相似性
- en: We’ve mentioned similarity a few times now, and it’s important for you to understand
    where it can be useful for you. Recall that in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    we discussed how embeddings can be used to turn words into vectors. A simple representation
    of this is shown in [Figure 18-4](#ch18_figure_4_1748550073457653).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次提到了相似度，了解它在您哪里有用是很重要的。回想一下，在 [第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)，我们讨论了如何使用嵌入将单词转换为向量。这种简单表示如图
    [18-4](#ch18_figure_4_1748550073457653) 所示。
- en: '![](assets/aiml_1804.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1804.png)'
- en: Figure 18-4\. Words as vectors
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-4\. 作为向量的单词
- en: Here, we plot the words *Awesome*, *Great*, and *Terrible* based on their learned
    vectors. It’s an oversimplification in two dimensions, but hopefully it’s enough
    to demonstrate the concept. In this case, we can visualize that *Awesome* and
    *Great* are similar because they’re close to each other, but we can quantify that
    by looking at the angle of the vectors between them. Taking a function of that
    angle, like its *cosine,* can give us a great indication of how close the vectors
    are to each other. Similarly, if we look at the word *Terrible*, the angle between
    *Awesome* and *Terrible* is very large, indicating that the two words aren’t similar.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据它们学习到的向量绘制了单词 *Awesome*、*Great* 和 *Terrible*。在二维空间中这是一个过于简化的表示，但希望这足以演示这个概念。在这种情况下，我们可以可视化
    *Awesome* 和 *Great* 是相似的，因为它们彼此很近，但我们可以通过观察它们之间向量的角度来量化这一点。通过取该角度的函数，如其 *余弦值*，我们可以得到一个很好的指示，说明这两个向量有多接近。同样，如果我们观察单词
    *Terrible*，*Awesome* 和 *Terrible* 之间的角度非常大，这表明这两个单词不相似。
- en: This process is called *cosine similarity*, and we’ll be using it as we create
    our RAG. We’ll split the book into chunks, calculate the embedding for those chunks,
    and store them in the database. Then, by using a store (ChromaDB, in this case)
    that provides a search based on cosine similarity, we’ll have the key to our RAG.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为 *余弦相似度*，我们将在创建我们的 RAG 时使用它。我们将把书籍分割成块，计算这些块的嵌入，并将它们存储在数据库中。然后，通过使用提供基于余弦相似度搜索的存储（在这种情况下是
    ChromaDB），我们将拥有我们 RAG 的关键。
- en: There are many different ways to calculate similarity, with cosine similarity
    being one of them. It’s worth looking into these other ways to fine-tune your
    RAG solution, but for the rest of this chapter, I’ll use cosine similarity because
    of its simplicity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以计算相似度，余弦相似度是其中之一。值得研究这些其他方法以微调您的 RAG 解决方案，但在这章的其余部分，我将使用余弦相似度，因为它简单易用。
- en: Creating the Database
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据库
- en: To create the vector store, we’ll go through the process of loading the PDF
    file, splitting it into chunks, calculating the chunks’ embeddings, and then storing
    them. Let’s look at this step-by-step.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建向量存储，我们将通过加载 PDF 文件、将其分割成块、计算块的嵌入以及然后存储它们的过程来完成。让我们一步步来看。
- en: 'First, we’ll load the PDF file by using `PyPDFLoader`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `PyPDFLoader` 加载 PDF 文件：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we’ll set up a text splitter that reads what we’ll use to chunk the text.
    An important part of your application will be establishing the appropriate sizes
    of chunks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设置一个文本分割器，它将读取我们将用于分割文本的内容。您应用程序的一个重要部分将是确定合适的块大小：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this case, the code will split the text into chunks of one thousand characters.
    But it uses a recursive strategy to calculate the split, in which it tries to
    do it on the natural boundaries in the text, rather than making hard cuts at exactly
    one thousand characters. It tries to split on newlines first, then on sentences,
    then on punctuation, and then on spaces. As a last resort, it will split in the
    middle of a word.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，代码将文本分割成一千个字符的块。但它使用递归策略来计算分割，其中它试图在文本的自然边界上执行分割，而不是在正好一千个字符处进行硬切割。它首先尝试在新行上分割，然后是句子、标点符号，然后是空格。作为最后的手段，它将在单词的中间进行分割。
- en: The overlap means that the next chunk won’t start at the immediate next character
    but around two hundred characters back. If we have these overlaps, some text will
    be included twice in the data—and that’s OK. It means that we won’t lose content
    by splitting in the middle of a sentence, etc. You should explore the size of
    the chunk and overlap based on what suits your scenario. Larger chunks like this
    will be faster to search because there will be fewer chunks than if they were
    smaller, but it also lowers the likelihood of the chunks being very similar to
    your prompt if the prompt is shorter than the chunk size.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重叠意味着下一个块不会从下一个字符立即开始，而是在大约两百个字符之前开始。如果我们有这些重叠，一些文本会在数据中出现两次——这是可以的。这意味着我们不会在句子中间分割时丢失内容。你应该根据你的场景探索块的大小和重叠。像这样的较大块将更快地搜索，因为块的数量将比它们更小，但这也降低了如果提示比块大小短，块与提示非常相似的可能性。
- en: The splitter provides the ability for you to specify your own length function
    if you want to measure length differently. In this case, I’m just using Python’s
    default `len` function. Typically, for a RAG like this, you may not need to override
    the `len` function, but the idea is that different models and encoders may count
    tokens in different ways. For example, GPT 3.5 recognizes a phrase like `lol`
    as a single token, but an emoji can be four tokens.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分割器提供了让你指定自己的长度函数的能力，如果你想要以不同的方式测量长度。在这种情况下，我只是在使用 Python 的默认 `len` 函数。通常，对于像这样的
    RAG，你可能不需要覆盖 `len` 函数，但这个想法是不同的模型和编码器可能以不同的方式计算标记。例如，GPT 3.5 将 `lol` 这样的短语识别为一个单独的标记，但一个表情符号可以是四个标记。
- en: The `add_start_index` parameter adds metadata to each chunk, indicating where
    it was located in the original text. This is useful for debugging, in which you
    can trace back where each chunk came from or provide things like citations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_start_index` 参数为每个块添加元数据，指示它在原始文本中的位置。这对于调试很有用，你可以回溯每个块来自哪里，或者提供类似引用的东西。'
- en: 'Once you’ve specified the text, you can use it to split the PDF into multiple
    texts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你指定了文本，你可以用它来将 PDF 分割成多个文本：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that you have the texts, you can turn them into embeddings by using the
    `OpenAIEmbeddings` class, and you can also specify that you want a vector store
    using Chroma by passing it the documents:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了文本，你可以使用 `OpenAIEmbeddings` 类将它们转换为嵌入，你也可以通过传递文档来指定你想要使用 Chroma 的向量存储：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As shown, you then simply pass the texts and embeddings you specified and a
    directory to store the embeddings. Then save the vector store to disk with this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，你只需简单地传递你指定的文本和嵌入以及存储嵌入的目录。然后使用以下命令将向量存储保存到磁盘：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `OpenAIEmbeddings` requires an `OPENAI_API_KEY` environment variable. You
    can get one at the [Open AIPlatform website](https://oreil.ly/41hwI) and then
    follow the instructions for your operating system by setting one. Make sure you
    name it exactly as shown.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`OpenAIEmbeddings` 需要一个 `OPENAI_API_KEY` 环境变量。你可以在 [Open AIPlatform 网站](https://oreil.ly/41hwI)
    上获取一个，然后根据你的操作系统设置一个，确保你将其命名为显示的确切名称。'
- en: The underlying database is an SQLite3 one (see [Figure 18-5](#ch18_figure_5_1748550073457676)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基础数据库是一个 SQLite3 数据库（见[图 18-5](#ch18_figure_5_1748550073457676)）。
- en: '![](assets/aiml_1805.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1805.png)'
- en: Figure 18-5\. The directory containing the ChromaDB content
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-5\. 包含 ChromaDB 内容的目录
- en: This gives you the ability to browse and inspect the database by using any tools
    that work with SQLite. So, for example, you can use the free [DB Browser for SQLite](https://sqlitebrowser.org)
    to access the data (see [Figure 18-6](#ch18_figure_6_1748550073457698)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这使你能够使用任何与 SQLite 兼容的工具浏览和检查数据库。例如，你可以使用免费的 [DB Browser for SQLite](https://sqlitebrowser.org)
    访问数据（见[图 18-6](#ch18_figure_6_1748550073457698)）。
- en: '![](assets/aiml_1806.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1806.png)'
- en: Figure 18-6\. Browsing data in the SQLite browser
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-6\. 在 SQLite 浏览器中浏览数据
- en: Now that we have the vector store, let’s explore what happens when we want to
    search it for similar text.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了向量存储，让我们来看看当我们想要搜索相似文本时会发生什么。
- en: Performing a Similarity Search
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行相似性搜索
- en: Once you have the vector store set up, it’s easy to search it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了向量存储，搜索它就变得很容易了。
- en: 'Here’s a function you can use to perform a similarity search with the vector
    store:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个你可以用来使用向量存储执行相似性搜索的函数：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, it’s pretty straightforward! You can override or extend some
    of the functionality if you like with optional parameters, including the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这很简单！如果你喜欢，可以使用可选参数覆盖或扩展一些功能，包括以下内容：
- en: Search_type
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索类型
- en: This defaults to `similarity` but can also be `mmr` for *maximum marginal relevance*
    (MMR), which is worth experimenting with as you build out production systems.
    MMR is particularly useful when you want to avoid redundant results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认为`similarity`，但也可以是`mmr`，代表*最大边际相关性*（MMR），在构建生产系统时值得尝试。MMR在你想避免冗余结果时特别有用。
- en: Distance_metric
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量
- en: This defaults to `cosine`, as we saw earlier, but it can also be `l2`, which
    is the *distance*—effectively, the straight-line distance between the two vectors
    in the embedding space. Alternatively, it can be `ip` for *inner product*, which
    provides a very fast calculation but at the cost of lower accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 默认为`cosine`，正如我们之前看到的，但也可以是`l2`，代表*距离*——实际上是嵌入空间中两个向量之间的直线距离。或者，它可以是`ip`，代表*内积*，这提供了一个非常快速的计算，但代价是精度较低。
- en: Lambda_mult
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda_mult
- en: This is an optional value between 0 and 1 that you use to control the strictness
    of the distance measurement. A value of 1.0 will give highly relevant scores,
    and a value of 0.0 will give much more diverse scores.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个介于0和1之间的可选值，你用它来控制距离测量的严格性。1.0的值将给出高度相关的分数，而0.0的值将给出更多样化的分数。
- en: As you build systems, I recommend that you try multiple approaches to see which
    works best for your scenario.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你构建系统，我建议你尝试多种方法，看看哪种最适合你的场景。
- en: Putting It All Together
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Now, you can use code like the following to take your PDF, slice and store
    it as vectors in the store, and run a query against it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用以下代码来处理你的PDF文件，将其切割并存储为库中的向量，并对它进行查询：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When running this, I got detailed results about her character. Here are some
    snippets:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此操作时，我得到了关于她性格的详细信息。以下是一些片段：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, when we’re making a query about the character to an LLM, we have all this
    extra content. We’ll explore that next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们向LLM查询角色时，我们还有所有这些额外内容。我们将在下一节探讨这一点。
- en: Using RAG Content with an LLM
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM与RAG内容结合使用
- en: Now that you’ve created a vector store and stored the book in it, let’s explore
    how you would read snippets back from the store, add them to a prompt, and get
    data back. We’ll use a local Ollama server to keep things simple. For more on
    Ollama, see [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了一个向量存储并将书籍存储在其中，让我们探讨如何从存储中读取片段，将其添加到提示中，并获取数据。我们将使用本地Ollama服务器以保持简单。有关Ollama的更多信息，请参阅[第17章](ch17.html#ch17_serving_llms_with_ollama_1748550058915113)。
- en: 'First, let’s load the vector store that we created in the previous step:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载我们在上一步创建的向量存储：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You *must* use the same embeddings as those you used when you created the vector
    store. Otherwise, there will be a mismatch when you try to encode your prompt
    and search for stuff similar to it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你*必须*使用与你在创建向量存储时使用的相同嵌入。否则，当你尝试编码提示并搜索类似内容时，将出现不匹配。
- en: In this case, I’m using the OpenAIEmbeddings, but it’s entirely up to you how
    to approach this. There are many embeddings available in open source on Hugging
    Face, or you could use things like the GLoVE embeddings we explored in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用的是OpenAIEmbeddings，但如何处理完全取决于你。Hugging Face上有很多开源的嵌入，或者你可以使用我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中探讨的GLoVE嵌入。
- en: ChromaDB persisted the embeddings in an SQLite database at a specific directory.
    Make sure you embed that, and then all you have to do is pass this and your embedding
    function to Chroma to get a reference to your database.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ChromaDB将嵌入持久化存储在特定目录的SQLite数据库中。确保你嵌入它，然后你只需要传递这个和你的嵌入函数到Chroma，以获取你的数据库引用。
- en: 'To search the vector store, you’ll use the same code as earlier:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要搜索向量存储，你将使用与之前相同的代码：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, input a query. For example, input this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，输入一个查询。例如，输入以下内容：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, you have all the pieces you need to do a RAG query, which you
    can do like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经拥有了进行RAG查询所需的所有组件，你可以这样操作：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, you create a helper function that will pass the query and the vector store,
    and you also have a parameter with the number of items to find in the vector store.
    The app will return the answer (from the LLM) as well as a list of sources from
    the data that it used to augment the query.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你创建一个辅助函数，该函数将传递查询和向量存储，你还有一个参数，用于指定在向量存储中要查找的项目数量。应用将返回答案（来自LLM）以及它用于增强查询的数据源列表。
- en: 'Let’s explore this function in depth:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这个函数：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You’ll start by searching the vector store with the code provided earlier. This
    will give you the decoded chunks from the datastore as strings, and you should
    call these `relevant_docs`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先使用之前提供的代码在向量存储中进行搜索。这将给出从数据存储中解码的块作为字符串，你应该将这些称为`relevant_docs`。
- en: You’ll then create the context string by joining the chunks together with some
    new line characters to separate them. It’s as simple as that.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过使用一些新行字符将块连接起来创建上下文字符串，以分隔它们。就这么简单。
- en: Now, the query and the context will be used in a call to Ollama. Let’s see how
    that will work.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，查询和上下文将被用于对Ollama的调用中。让我们看看这将如何工作。
- en: 'Start by defining the function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义函数：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, you can set the function to accept the prompt and context. I’ve added
    a couple of optional parameters that, if they’re not set, will use the defaults.
    The first is the model. To get a list of available models on your server, you
    can just use “ollama list” from the command prompt. The `temperature` parameter
    indicates how deterministic your response will be: the smaller the number, the
    more deterministic the answer, and the higher the number, the more creative the
    answer. I set a default of 0.7, which gives some flexibility to the model to make
    it natural sounding while staying relevant. But when you use smaller models in
    Ollama (like `llama3.1`, as shown), it does make hallucination more likely.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以设置函数以接受提示和上下文。我已经添加了一些可选参数，如果未设置，将使用默认值。第一个是模型。要获取服务器上可用的模型列表，你只需在命令提示符中使用“ollama
    list”。`temperature`参数表示你的响应将有多大的确定性：数字越小，答案越确定，数字越大，答案越有创意。我将其设置为默认值0.7，这给模型提供了一些灵活性，使其听起来自然同时保持相关性。但当你使用Ollama中的较小模型（如`llama3.1`，如图所示）时，确实会使幻觉的可能性增加。
- en: You’ll also want to specify the `ollama_url` endpoint, as shown in [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你还想要指定`ollama_url`端点，如[第17章](ch17.html#ch17_serving_llms_with_ollama_1748550058915113)中所示。
- en: Next, you create the messages that will be used to interact with the model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建用于与模型交互的消息。
- en: The structure of conversations with a model typically looks like the one in
    [Figure 18-7](#ch18_figure_7_1748550073457720). The model will optionally be primed
    with a system message that gives it instructions on how to behave. It will then
    have an initial message that it emits to the user, like, “Welcome to the Chat.
    How can I help?” The user will then respond with a prompt asking the model to
    do something, to which the model will respond, and so on.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型对话的结构通常看起来像[图18-7](#ch18_figure_7_1748550073457720)中的那样。模型可以选择性地通过一个系统消息来引导，该消息给出它如何行为的指示。然后，它将向用户发出一个初始消息，例如，“欢迎来到聊天。我能帮什么忙？”用户将随后用一个提示来要求模型做某事，模型将做出回应，依此类推。
- en: '![](assets/aiml_1807.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1807.png)'
- en: Figure 18-7\. Anatomy of a conversation with a model
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-7\. 与模型对话的解剖结构
- en: The *memory* of the conversation will be a JSON document with each of the roles
    prefixed by a `role` value. The initial message will have the `system` role, the
    model messages will have the `model` role, and the user messages will have the
    `user` role.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对话的*记忆*将是一个带有每个角色前缀的`role`值的JSON文档。初始消息将具有`system`角色，模型消息将具有`model`角色，用户消息将具有`user`角色。
- en: 'So, for the simple RAG app we’re creating, we can create an instance of a conversation
    like this—passing the system message and the user message, which will be composed
    of the prompt and the context, like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们正在创建的简单RAG应用，我们可以创建一个这样的对话实例——传递系统消息和用户消息，这些消息将由提示和上下文组成，如下所示：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Depending on how you set up the system role, you’ll get very different behavior.
    In this case, I used a prompt that gets it to heavily focus on the provided context.
    You don’t *need* to do this, and by working with this prompt, you might get much
    better results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你如何设置系统角色，你会得到非常不同的行为。在这种情况下，我使用了一个提示，使其高度关注提供的上下文。你*不需要*这样做，通过使用这个提示，你可能会得到更好的结果。
- en: Within the user role, this is just as simple as creating a string with `Context:`
    and `Question:` content that you paste the context and prompt into.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户角色中，这就像创建一个包含`Context:`和`Question:`内容的字符串，你将上下文和提示粘贴进去一样简单。
- en: 'From this, you can now create a JSON payload to pass to Ollama that contains
    the desired model, the messages, the temperature, and the stream (which must be
    set to `False` if you want to get a single answer back):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，你现在可以创建一个JSON负载，将其传递给Ollama，其中包含所需的模型、消息、温度和流（如果你想要得到单个答案，必须将流设置为`False`）：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note also that the desired model must be installed in Ollama or you’ll get an
    error, so see [Chapter 17](ch17.html#ch17_serving_llms_with_ollama_1748550058915113)
    for adding models to Ollama.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所需的模型必须安装到Ollama中，否则你会得到一个错误，所以查看[第17章](ch17.html#ch17_serving_llms_with_ollama_1748550058915113)以了解如何将模型添加到Ollama。
- en: Then, you simply have to use an HTTP post to the Ollama URL, passing it the
    payload. When you get the response, you can query the returned message—where there’ll
    now be new content added by the model. This content will contain your answer!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你只需使用HTTP POST到Ollama URL，传递它负载。当你得到响应时，你可以查询返回的消息——其中现在将添加由模型生成的新内容。这个内容将包含你的答案！
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case, I used Llama 3.1 and got some excellent answers. Here’s an example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了Llama 3.1并得到了一些出色的答案。以下是一个例子：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Your results will vary, based on the temperature, the slicing size for the chunks,
    and various other factors.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果将根据温度、块切片大小以及各种其他因素而有所不同。
- en: One thing to note is that you can also use a *really* small model like Gemma2b
    and still get really good results. However, the context window of a model this
    small could have issues when you’re retrieving and augmenting your query with
    lots of information. As you saw earlier in this chapter, we were using one-thousand-character
    chunks, and we’re retrieving the 10 closest ones to the prompt. This is already
    in order of 10 k characters, and depending on the tokenization strategy, that
    could be more than 10 k tokens. Given that the context window for that model is
    only 2 k tokens, you could hit a problem. Watch out for that!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个需要注意的事情是，你也可以使用一个非常小的模型如Gemma2b，并且仍然可以得到非常好的结果。然而，如此小的模型的上下文窗口在检索和增强你的查询时可能会出现问题。正如你在这章前面看到的，我们使用了一千字符的块，并检索了与提示最接近的10个。这已经是10
    k字符的顺序，根据分词策略，这可能会超过10 k个标记。鉴于该模型的上下文窗口仅为2 k个标记，你可能会遇到问题。注意这一点！
- en: Extending to Hosted Models
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展到托管模型
- en: In the example we just walked through, we used smaller models like Llama and
    Gemma to perform RAG on a local Ollama server. If you want to use larger, hosted
    models like GPT, the process is exactly the same. One change I would make, though,
    is with the system prompt. Given that these models have huge amounts of parameters
    that have learned a lot, it’s good to unshackle them a bit and not expect them
    to be limited solely to the context provided!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚演示的例子中，我们使用了像Llama和Gemma这样的较小模型在本地Ollama服务器上执行RAG操作。如果你想使用像GPT这样的大型托管模型，过程完全相同。不过，我会做的一个改变是关于系统提示。鉴于这些模型具有大量参数并且学到了很多知识，最好稍微放松一下，不要期望它们仅限于提供的上下文！
- en: 'For example, for GPT, you can import classes that support OpenAI’s GPT models
    like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于GPT，你可以这样导入支持OpenAI GPT模型的类：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can then instantiate this class like this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样实例化这个类：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The model value is a string containing the name of the model you want to use.
    For example, you could use `gpt-3.5-turbo` or `gpt-4`. Check the [OpenAI API documentation
    for model versions](https://oreil.ly/SVBXr) available at the time you’re reading
    this.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型值是一个包含你想要使用的模型名称的字符串。例如，你可以使用`gpt-3.5-turbo`或`gpt-4`。检查你阅读此内容时的[OpenAI API文档中的模型版本](https://oreil.ly/SVBXr)。
- en: 'Then, you can create the prompt very simply. First, create a prompt template
    to hold the system and user prompts:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以非常简单地创建提示。首先，创建一个提示模板来保存系统和用户提示：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, you can make the formatted prompt with the details of the context and
    prompt:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用上下文和提示的详细信息来制作格式化的提示：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, you can invoke the GPT chat with the formatted prompt and get the
    response:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用格式化的提示调用GPT聊天并获取响应：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, as long as you ensure that you have an `OPENAI_API_KEY` environment variable,
    as discussed earlier, you’re RAGging against GPT! Please pay attention to the
    pricing on OpenAI for using the available models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只要确保你有一个之前讨论过的`OPENAI_API_KEY`环境变量，你就是在使用GPT进行RAG操作！请注意OpenAI使用可用模型的定价。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you dipped your toes into the RAG waters, where you learned
    a powerful technique that enhances the capabilities of LLMs by combining their
    general understanding skills with local, private data. You saw how RAG works by
    creating a vector database with the contents of a book, and then you searched
    that database for information that was relevant to your given prompts.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你尝试了RAG的领域，在那里你学习了一种强大的技术，通过结合LLMs的一般理解技能与本地、私有数据来增强其能力。你通过创建包含一本书内容的向量数据库来了解RAG的工作原理，然后你搜索了这个数据库以找到与你的给定提示相关的信息。
- en: We also explored querying a character from the book to learn more about her—and
    despite models like Llama and GPT not being trained on content about her, they
    were able to artificially understand the text and provide great information and
    analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探索了查询书中的人物以了解更多关于她的信息——尽管Llama和GPT等模型并未在关于她的内容上进行训练，但它们能够人为地理解文本并提供出色的信息和分析。
- en: 'You also explored tools like ChromaDB (for vector storage) and pretrained embeddings
    (such as OpenAIs for vector encoding of text allowing similarity searches). You
    also explored various models that could be enhanced by using RAG, both small and
    local ones (like Llama and Gemma with Ollama) and large hosted models (like GPT
    via the OpenAI API). This took you through the process end to end: slicing text,
    encoding it, storing it, searching it based on similarity, and bundling it with
    a prompt to a model to perform RAG.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你还探索了像ChromaDB（用于向量存储）和预训练嵌入（如OpenAIs用于文本的向量编码，允许进行相似度搜索）这样的工具。你也探索了可以通过使用RAG增强的各种模型，包括小型和本地模型（如Llama和Gemma与Ollama）以及大型托管模型（如通过OpenAI
    API的GPT）。这个过程涵盖了从端到端：切片文本，编码它，存储它，基于相似度进行搜索，以及将提示与模型捆绑以执行RAG。
- en: 'In the next chapter, we’ll shift gears a bit to another exciting aspect of
    AI: generative image models. We’ll explore a number of different models that provide
    images from text prompts, and we’ll dig down a little into how they work.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将稍微转换一下方向，探讨AI的另一个令人兴奋的方面：生成图像模型。我们将探索提供图像的多种不同模型，这些模型来自文本提示，并且我们将深入探讨它们的工作原理。
