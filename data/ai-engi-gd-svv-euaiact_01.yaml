- en: Chapter 1\. Understanding the AI Regulations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：理解人工智能法规
- en: As people, organizations, and the public sector increasingly rely on AI to drive
    decision making, the technology must be trustworthy. The [EU AI Act](https://oreil.ly/-8wI7)
    aims to provide a legal framework for developing, deploying, and using AI technologies
    within the European Union, emphasizing safety, transparency, and ethical considerations.
    It is a regulatory framework for artificial intelligence that includes specific
    requirements for AI systems of different risk categories within the EU. This book
    is focused on understanding and implementing the regulatory requirements set by
    the European Union’s legislation on artificial intelligence. Please note that
    the guidance it provides is not intended as a substitute for obtaining professional
    legal advice.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人们、组织以及公共部门越来越依赖人工智能来驱动决策，这项技术必须值得信赖。《欧盟人工智能法案》（[EU AI Act](https://oreil.ly/-8wI7)）旨在为在欧洲联盟内开发、部署和使用人工智能技术提供一个法律框架，强调安全性、透明度和伦理考量。这是一个涵盖欧盟内不同风险类别人工智能系统的具体要求的人工智能监管框架。本书专注于理解和实施欧盟人工智能立法设定的监管要求。请注意，本书提供的指导不旨在替代获取专业法律建议。
- en: This chapter begins by outlining the motivation behind the EU AI Act, emphasizing
    the idea of “trustworthy AI,” and identifying the essential requirements for ensuring
    AI is trustworthy. It then describes the structure of the EU AI Act, including
    its definitions, key stakeholders, risk classifications (prohibited, high risk,
    limited risk, and minimal risk), and the implementation timeline. Finally, it
    briefly compares the EU AI Act with other international regulations and standards
    related to AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先概述了欧盟人工智能法案背后的动机，强调“可信赖的人工智能”这一理念，并确定了确保人工智能值得信赖的基本要求。然后，它描述了欧盟人工智能法案的结构，包括其定义、关键利益相关者、风险分类（禁止的、高风险、有限风险和低风险）以及实施时间表。最后，它简要比较了欧盟人工智能法案与其他与人工智能相关的国际法规和标准。
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The author is not a lawyer, and this book does not provide legal advice. The
    intersection of law and artificial intelligence is a complex subject that requires
    expertise beyond the scope of AI, data science, and machine learning. Legal considerations
    surrounding AI systems can be complex and far-reaching. If you have any legal
    concerns related to the AI systems you are working on, seek professional legal
    advice from qualified experts in the field.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作者并非律师，本书不提供法律建议。法律与人工智能的交叉是一个复杂的主题，需要超越人工智能、数据科学和机器学习范畴的专业知识。围绕人工智能系统的法律考量可能非常复杂且影响深远。如果您对正在开发的人工智能系统有任何法律方面的疑问，请寻求该领域合格专家的专业法律建议。
- en: 'The Motivation for the EU AI Act: Trustworthy AI'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案的动机：可信赖的人工智能
- en: As AI becomes increasingly intertwined with our daily lives, one of the challenges
    we face is learning to navigate the uncertainty that comes with it. This uncertainty
    is inherent to AI. AI models’ predictive accuracy has long been considered a core
    evaluation criterion when building an AI system. However, with the widespread
    use of AI in critical areas such as human resources, transportation, finance,
    medicine, and security, there is a growing need for these systems to be trustworthy—and
    traditional predictive accuracy alone is not sufficient to build trustworthy AI
    applications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能与我们的日常生活日益紧密相连，我们面临的一个挑战是学会应对随之而来的不确定性。这种不确定性是人工智能固有的。长期以来，人工智能模型的预测准确性一直被视为构建人工智能系统时的核心评估标准。然而，随着人工智能在人力资源、交通、金融、医疗和安全等关键领域的广泛应用，对这些系统建立信任的需求日益增长——仅仅依靠传统的预测准确性是不足以构建可信赖的人工智能应用的。
- en: To better understand trustworthy AI, let’s start with its definition. *Trustworthy
    AI* is an umbrella term that refers to artificial intelligence systems that are
    designed and developed with principles such as fairness, privacy, and non-discrimination
    in mind, and with robust mechanisms to ensure reliability, security, and resilience.
    Within the AI community, this term is used interchangeably with *responsible AI*,
    *ethical AI*, *reliable AI*, and *values-driven AI*. Trustworthy AI systems must
    be adaptable to diverse and changing environments and robust against various types
    of disruptions, including cyber threats, data variability, and operational changes.
    They should operate transparently and be held accountable, with continuous monitoring
    and evaluation to respect human rights, including privacy and freedom from discrimination,
    and to ensure adherence to democratic values.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解可信人工智能，让我们从其定义开始。*可信人工智能*是一个总称，指的是设计和发展时考虑到公平、隐私和非歧视等原则的人工智能系统，并具有确保可靠性、安全性和弹性的强大机制。在人工智能社区中，这个术语与*负责任的人工智能*、*伦理人工智能*、*可靠人工智能*和*价值观驱动的人工智能*等术语互换使用。可信人工智能系统必须能够适应多样化和不断变化的环境，并能够抵御各种类型的干扰，包括网络威胁、数据变异和运营变化。它们应透明地运行，并受到问责，进行持续监测和评估，以尊重人权，包括隐私和免于歧视的权利，并确保遵守民主价值观。
- en: Trustworthy AI is a complex term that incorporates a long list of concepts and
    principles, which are visualized in [Figure 1-1](#chapter_1_figure_1_1748539916811473).
    These concepts lay the foundation for understanding the EU AI Act.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可信的人工智能是一个复杂的术语，它包含了一系列的概念和原则，这些概念和原则在[图1-1](#chapter_1_figure_1_1748539916811473)中得到了展示。这些概念为理解欧盟人工智能法案奠定了基础。
- en: Trustworthiness in AI is grounded in the three pillars of *lawfulness*, *ethics*,
    and *robustness*. First, AI systems should be lawful, meaning they must comply
    with all relevant regulations to ensure a fair market, promote economic benefits,
    and protect citizens’ rights. Second, they must be built on ethical principles
    and values, incorporating input from all stakeholders and establishing appropriate
    feedback mechanisms. Finally, they must be robust, accounting for potential risks
    and ensuring safety at every stage of development and deployment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的可信性建立在*合法性*、*伦理*和*鲁棒性*三个支柱之上。首先，人工智能系统应该是合法的，这意味着它们必须遵守所有相关法规，以确保公平的市场、促进经济效益和保护公民权利。其次，它们必须建立在伦理原则和价值之上，吸收所有利益相关者的意见，并建立适当的反馈机制。最后，它们必须具有鲁棒性，考虑到潜在的风险，并在开发和部署的每个阶段确保安全性。
- en: '![](assets/taie_0101.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0101.png)'
- en: Figure 1-1\. The foundation and seven requirements of trustworthy AI
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 可信人工智能的基础和七个要求
- en: 'Standing on these three pillars are seven key requirements that AI systems
    must implement to be deemed trustworthy:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在三个支柱之上的七个关键要求是人工智能系统必须实施的，以便被认为是可信的：
- en: Human agency and oversight
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类代理和监督
- en: Technical robustness and safety
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 技术鲁棒性和安全性
- en: Privacy and data governance
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐私和数据治理
- en: Transparency
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 透明度
- en: Diversity, non-discrimination, and fairness
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多样性、非歧视和公平
- en: Societal and environmental well-being
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社会和环境保护
- en: Accountability
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 责任制
- en: These come directly from the requirements outlined in the [Ethics Guidelines
    for Trustworthy AI](https://oreil.ly/nBhE6) developed by the European Commission’s
    High-Level Expert Group on AI (AI HLEG). In the following sections, I provide
    an explanation of each.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些要求直接来源于欧洲委员会人工智能高级专家小组（AI HLEG）制定的[可信人工智能伦理指南](https://oreil.ly/nBhE6)。在接下来的章节中，我将解释每个要求。
- en: Human Agency and Oversight
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类代理和监督
- en: Human agency and oversight are crucial in developing and operating AI systems.
    *Human agency* refers to the ability of individuals to make informed decisions
    and maintain control. AI systems should support this by providing transparency,
    interpretability, and mechanisms for control and intervention that enable humans
    to understand and influence the system’s decisions and actions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类代理和监督在开发和运营人工智能系统中至关重要。*人类代理*指的是个人做出知情决策并保持控制的能力。人工智能系统应通过提供透明度、可解释性和控制与干预机制来支持这一点，这些机制使人类能够理解和影响系统的决策和行为。
- en: '*Human oversight* involves establishing governance processes and mechanisms
    that allow for human monitoring, evaluation, and intervention in the operation
    of AI systems. This includes ensuring transparency and interpretability of the
    systems’ decision-making processes based on the results produced by the AI models.
    Additionally, AI systems should provide human control mechanisms (e.g., the ability
    to override, adjust, or shut down the system when necessary). As depicted in [Figure 1-2](#chapter_1_figure_2_1748539916811507),
    I distinguish between four different modes of human oversight:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类监督* 涉及建立治理流程和机制，允许人类对 AI 系统的运行进行监控、评估和干预。这包括确保基于 AI 模型产生的结果的系统决策过程的透明度和可解释性。此外，AI
    系统应提供人类控制机制（例如，在必要时能够覆盖、调整或关闭系统的能力）。如图 [图 1-2](#chapter_1_figure_2_1748539916811507)
    所示，我区分了四种不同的人类监督模式：'
- en: Human-in-command
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人类指挥
- en: This mode represents the highest level of human control, where humans maintain
    ultimate authority and responsibility over the AI system. Humans must explicitly
    authorize any AI system action. They are at the core of decision-making processes
    and always make the final decisions. In this mode, humans oversee the AI system’s
    overall activity and can decide when and how to use the system in specific situations.
    For example, the [“AI cockpit” concept](https://oreil.ly/HFlbo) demonstrates the
    human-in-command approach by providing a central user interface that enables operators
    to monitor AI systems’ effects on different user groups, evaluate results against
    technical and ethical criteria, and exercise the authority to disable the systems
    when necessary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式代表了人类控制的最高水平，人类保持对 AI 系统的最终权威和责任。人类必须明确授权任何 AI 系统的操作。他们是决策过程的核心，并且总是做出最终决策。在这种模式下，人类监督
    AI 系统的整体活动，并可以决定在特定情况下何时以及如何使用系统。例如，[“AI 驾驶舱”概念](https://oreil.ly/HFlbo) 通过提供一个中央用户界面来展示人类指挥方法，使操作员能够监控
    AI 系统对不同用户群体的影响，根据技术和伦理标准评估结果，并在必要时行使关闭系统的权力。
- en: Human-in-the-loop
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中人类
- en: AI systems require human interaction at critical points in the decision-making
    process to add a layer of human experience and contextual understanding that AI
    currently lacks. Often, this involvement entails guiding the direction of decisions
    based on AI-generated predictions. The level of intervention and control might
    depend on the risk level of the system. Medical AI applications, for example,
    will typically require more intensive human intervention and control than less
    critical AI applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统在决策过程中的关键点需要人类交互，以增加人类经验和情境理解，这是 AI 目前所缺乏的。通常，这种参与包括根据 AI 生成的预测来引导决策方向。干预和控制程度可能取决于系统的风险水平。例如，医疗
    AI 应用通常需要比不那么关键的 AI 应用更密集的人类干预和控制。
- en: Human-on-the-loop
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中人类
- en: The human plays an observer role, monitoring the actions of the AI system and
    intervening if necessary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人类扮演观察者的角色，监控 AI 系统的行为，并在必要时进行干预。
- en: Human-out-of-the-loop
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 环境外人类
- en: The AI system operates independently, without human intervention or real-time
    supervision. This scenario is possible when decisions based on the system’s predictions
    can be programmatically implemented. Often, this mode is applied in trading applications
    that require data processing at speed, where human oversight is not realistic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统独立运行，没有人类干预或实时监督。当基于系统的预测的决策可以程序化实施时，这种情况是可能的。通常，这种模式应用于需要快速数据处理的应用程序，例如交易应用，其中人类监督不切实际。
- en: '![](assets/taie_0102.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0102.png)'
- en: Figure 1-2\. The different levels of human oversight in AI
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 人工智能中人类监督的不同层级
- en: The modes described here are each suitable for different AI use cases. Regardless
    of the one you use, proper implementation of the human agency and oversight requirement
    is crucial for designing and developing ethical AI systems that benefit from the
    strengths of AI while mitigating potential risks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的模式适用于不同的 AI 应用场景。无论使用哪一种，正确实施人类代理和监督要求对于设计和开发能够从 AI 的优势中获益同时减轻潜在风险的道德 AI
    系统至关重要。
- en: Technical Robustness and Safety
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术稳健性和安全性
- en: To be trustworthy, AI systems need to be accurate, reliable, and able to repeat
    their results. They should also have a “backup plan” if something goes wrong.
    In computer science, *robustness* means a system can keep working correctly even
    when there are mistakes, unusual inputs, or unexpected situations. For AI, robustness
    means that a machine learning (ML) model can keep making good, reliable predictions
    despite facing different conditions, tricky inputs, or changes in the type of
    data it sees.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为值得信赖的，人工智能系统需要准确、可靠，并且能够重复其结果。它们还应该有一个“备用计划”，以防万一出现问题。在计算机科学中，“鲁棒性”意味着系统即使在出现错误、异常输入或意外情况时也能保持正确运行。对于人工智能来说，鲁棒性意味着机器学习（ML）模型能够在面对不同的条件、棘手的输入或其看到的数据类型变化时，仍然能够持续做出良好、可靠的预测。
- en: In simpler terms, a robust ML model can still give you the correct answers even
    if the information it receives is somewhat noisy, messy, or comes from a different
    source than the model was originally trained on. If these differences cause its
    performance to drop suddenly, then it’s not considered robust anymore. Achieving
    robustness is essential for building trust in AI—especially in areas like self-driving
    cars, cybersecurity systems, and healthcare, where a single mistake can have serious
    consequences.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个鲁棒的机器学习模型即使在接收到的信息有些嘈杂、混乱，或来自模型最初训练的不同来源时，仍然可以给出正确的答案。如果这些差异导致其性能突然下降，那么它就不再被认为是鲁棒的。实现鲁棒性对于在人工智能中建立信任至关重要——特别是在自动驾驶汽车、网络安全系统和医疗保健等领域，在这些领域，一个错误可能会产生严重的后果。
- en: 'There are three core layers to an AI system: the data it’s trained on, the
    algorithm it uses, and the software that puts them to use. Correspondingly, there
    are three levels of robustness:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统有三个核心层：它所训练的数据、它所使用的算法以及将其付诸使用的软件。相应地，有三个级别的鲁棒性：
- en: '*Data robustness* requires training the system on a wide variety of data types
    and scenarios. This allows the model to “learn” to handle different situations
    and ensures it is less likely to fail when confronted with new or unexpected conditions.
    For example, a self-driving car’s AI should be trained on data from different
    weather conditions to handle rain, snow, and fog. In cybersecurity, an intrusion
    detection system should learn from many different types of network traffic and
    attack methods, so it can catch new threats as they emerge.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据鲁棒性*需要训练系统处理广泛的数据类型和场景。这允许模型“学习”如何处理不同的情况，并确保它在面对新的或意外的情况时不太可能失败。例如，自动驾驶汽车的AI应该接受来自不同天气条件的数据训练，以处理雨、雪和雾。在网络安全中，入侵检测系统应该从许多不同类型的网络流量和攻击方法中学习，以便能够捕捉到新出现的威胁。'
- en: '*Algorithmic robustness* focuses on defending against attacks that try to trick
    the AI model itself. Some attacks occur at the moment of decision making by slightly
    altering the input to confuse the model. Others are executed during the training
    phase by introducing “poisoned” data that causes the model to make incorrect predictions
    later. A robust algorithm should be able to handle noisy or altered inputs, changes
    in data types, and even deliberately manipulated inputs designed to fool it. For
    example, a robust language model can deal with misspellings, slang, or strange
    grammar without losing accuracy.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*算法鲁棒性*关注于防御试图欺骗AI模型的攻击。一些攻击发生在决策时刻，通过轻微改变输入来混淆模型。其他攻击则在训练阶段通过引入“有毒”数据执行，这些数据会导致模型后来做出错误的预测。一个鲁棒的算法应该能够处理嘈杂或改变后的输入、数据类型的变化，甚至故意设计的旨在欺骗它的输入。例如，一个鲁棒的语言模型可以处理拼写错误、俚语或奇怪的语法，而不会失去准确性。'
- en: '*System-level robustness* looks at the bigger picture—the entire lifecycle
    of the AI product, from how data is collected to how the model is built, tested,
    and finally used in the real world. By examining every step, we can spot risks
    before they lead to serious problems. This holistic approach ensures the entire
    AI pipeline, and its interaction with other systems, remains steady and secure.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统级鲁棒性*着眼于更大的图景——AI产品的整个生命周期，从数据收集到模型构建、测试，最后在现实世界中的应用。通过检查每个步骤，我们可以在它们导致严重问题之前发现风险。这种整体方法确保整个AI管道及其与其他系统的交互保持稳定和安全。'
- en: 'In research, we often differentiate two types of robustness:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究中，我们通常区分两种鲁棒性：
- en: Non-adversarial robustness
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 非对抗性鲁棒性
- en: Can the model handle everyday challenges, like noisy inputs or slight changes
    in the environment, without losing accuracy?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能否处理日常挑战，如嘈杂的输入或环境中的微小变化，而不会失去准确性？
- en: Adversarial robustness
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性鲁棒性
- en: Can the model resist “tricks” that attackers use intentionally to confuse it,
    such as subtly modified inputs designed to produce the wrong answer?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能否抵抗攻击者故意使用的“诡计”，例如精心修改的输入，以产生错误答案？
- en: 'To measure how robust an AI model is, practitioners and researchers look at
    a variety of measurements:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量人工智能模型的鲁棒性，实践者和研究人员会查看各种测量指标：
- en: Accuracy
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率
- en: How many predictions the model gets right.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测正确的次数。
- en: Error rate
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率
- en: How many incorrect predictions the model makes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型做出的错误预测次数。
- en: Sensitivity (recall)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性（召回率）
- en: The model’s ability to correctly identify items that have a given condition
    (true positives). For example, if we are implementing a binary classifier, of
    all the cases that should be labeled “positive,” how many are correctly identified?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正确识别具有给定条件的项目（真阳性）的能力。例如，如果我们正在实施二元分类器，在所有应标记为“正”的案例中，有多少被正确识别？
- en: Specificity
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 灵敏度（召回率）
- en: The model’s ability to correctly identify items that do not have a given condition
    (true negatives). For example, of all the cases that should be labeled “negative,”
    how many are correctly rejected?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正确识别不具有给定条件的项目（真阴性）的能力。例如，在所有应标记为“负”的案例中，有多少被正确拒绝？
- en: Robustness curves
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性曲线
- en: Graphs that show how the model’s performance changes when we introduce challenges,
    like more noise, missing information, or unusual inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示当引入挑战时（如更多噪声、缺失信息或异常输入），模型性能如何变化。
- en: All of these measurements can help us understand the model’s strengths and weaknesses.
    However, the best way to measure robustness typically depends on the specific
    problem and requirements of the AI system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些测量都可以帮助我们了解模型的优势和劣势。然而，通常测量鲁棒性的最佳方式取决于特定问题和人工智能系统的要求。
- en: Privacy and Data Governance
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私和数据治理
- en: The second requirement of trustworthy AI systems is that they must ensure full
    respect for privacy and personal data protection, in line with Articles 7 and
    8 of the [Charter of Fundamental Rights of the European Union](https://oreil.ly/nPDrM).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可信人工智能系统的第二个要求是，它们必须确保充分尊重隐私和个人数据保护，符合《欧洲联盟基本权利宪章》第7条和第8条的规定。[欧洲联盟基本权利宪章](https://oreil.ly/nPDrM)。
- en: Many AI systems, such as recommender systems and systems that provide personalized
    predictions, require some amount of personally identifiable information (PII).
    Handling of PII is a subject to regulations within the EU and the US, under the
    General Data Protection Regulation (GDPR) and the California Consumer Privacy
    Act (CCPA).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人工智能系统，如推荐系统和提供个性化预测的系统，需要一定量的个人可识别信息（PII）。PII的处理受欧盟和美国法规的约束，根据通用数据保护条例（GDPR）和加利福尼亚消费者隐私法案（CCPA）。
- en: By examining how an AI system collects, stores, processes, and shares personal
    or sensitive data, *p**rivacy* *i**mpact* *a**ssessments* (PIAs) help organizations
    spot areas where privacy might be at risk and guide them in modifying the system,
    implementing safeguards, and complying with relevant privacy laws and regulations.
    PIAs should ideally be conducted early in the design process, to proactively identify,
    assess, and mitigate potential privacy risks. This is an example of *privacy by
    design*, an engineering philosophy that involves embedding privacy protections
    into every phase of an AI system’s lifecycle—from data collection and preprocessing
    to model training, deployment, and maintenance. This ensures that privacy isn’t
    just treated as a legal afterthought or a check-the-box compliance step at the
    end of the development process, but as a fundamental design principle that helps
    organizations maintain user trust and meet regulatory requirements.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查人工智能系统如何收集、存储、处理和共享个人或敏感数据，*隐私影响评估*（PIAs）帮助组织发现隐私可能存在风险的区域，并指导它们修改系统、实施安全措施，并遵守相关的隐私法律和法规。PIAs理想情况下应在设计过程的早期进行，以主动识别、评估和减轻潜在的隐私风险。这是一个*隐私设计*的例子，这是一种工程哲学，它涉及将隐私保护嵌入人工智能系统生命周期的每个阶段——从数据收集和预处理到模型训练、部署和维护。这确保了隐私不仅仅被视为法律上的事后考虑或开发过程结束时的合规步骤，而是一个基本的设计原则，有助于组织维护用户信任并满足监管要求。
- en: Privacy by design incorporates privacy considerations into the foundational
    components of AI systems. For AI engineers, this might involve using techniques
    such as minimal data collection, deidentification, and anonymization (encryption,
    differential privacy, other methods to protect sensitive information before it
    even reaches the model); adapting model architecture choices (for example, using
    federated learning); and continuous evaluation and monitoring. The related concept
    of *data governance* ensures that privacy principles are consistently applied,
    maintained, and improved within a structured organizational framework.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 设计中的隐私将隐私考虑纳入AI系统的基本组件。对于AI工程师来说，这可能包括使用最小数据收集、去标识化、匿名化（加密、差分隐私、其他在信息甚至到达模型之前保护敏感信息的方法）；调整模型架构选择（例如，使用联邦学习）；以及持续评估和监控。相关的概念*数据治理*确保在结构化的组织框架内持续应用、维护和改进隐私原则。
- en: Data governance is a data management function that guarantees the availability,
    usability, integrity, and security of the data collected and used in an organization.
    Its key elements are visualized in [Figure 1-3](#chapter_1_figure_3_1748539916811532).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理是一个数据管理功能，它保证了组织收集和使用的数据的可用性、可用性、完整性和安全性。其关键要素在[图1-3](#chapter_1_figure_3_1748539916811532)中得到了可视化。
- en: '![](assets/taie_0103.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0103.png)'
- en: 'Figure 1-3\. Key elements of data governance (adapted from [Data Governance:
    The Definitive Guide](https://oreil.ly/4hIZp) by Evren Eryurek et al. [O’Reilly])'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3. 数据治理的关键要素（改编自Evren Eryurek等人的《数据治理： definitive guide》（https://oreil.ly/4hIZp）[O’Reilly]）
- en: 'Data governance has become increasingly important as data volumes have grown,
    organizations have become more data-driven, and access to data has expanded. The
    ultimate goal of data governance is to enhance the trustworthiness of data, which
    is fundamental to trustworthy AI. There are three key aspects to this: discoverability,
    security, and accountability.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加、组织变得更加数据驱动以及数据访问的扩展，数据治理变得越来越重要。数据治理的最终目标是提高数据的可信度，这是可信AI的基础。这有三个关键方面：可发现性、安全性和问责制。
- en: '*Discoverability* refers to the availability of the dataset’s metadata, data
    provenance (lineage), and glossary of domain entities. It’s essential for ensuring
    users and AI systems can easily access the data they need. Data governance establishes
    procedures that guarantee that the right data is accessed by the appropriate people
    in the organization and determine what data AI systems can access. *D**ata quality*
    is a related concept that is crucial in building trust in data: data should be
    correct, complete, timely, and integral.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*可发现性* 指的是数据集元数据、数据来源（血缘）和领域实体词汇表的可用性。这对于确保用户和AI系统可以轻松访问他们所需的数据至关重要。数据治理建立程序，确保组织内适当的人员可以访问正确的数据，并确定AI系统可以访问哪些数据。*数据质量*
    是一个相关的概念，在建立数据信任方面至关重要：数据应该是正确的、完整的、及时的、整体的。'
- en: In the context of AI engineering, data*security* means protecting the data used
    to train, validate, and run models from unauthorized access, theft, tampering,
    or loss. Together with privacy, data security is an essential aspect of protecting
    data and ensuring adherence to regulations such as the GDPR  (or CCPA).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI工程背景下，*数据安全* 指的是保护用于训练、验证和运行模型的从未经授权的访问、盗窃、篡改或丢失中。与隐私一起，数据安全是保护数据并确保遵守GDPR（或CCPA）等法规的一个基本方面。
- en: The third aspect of data governance, *accountability*, involves ensuring that
    everyone involved with an AI system, from data suppliers to model developers and
    operators, knows their role, can explain their choices, and can be held responsible
    for the system working ethically, legally, and safely.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理的第三个方面是*问责制*，它涉及确保与AI系统相关的每个人，从数据供应商到模型开发者和操作者，都知道他们的角色，可以解释他们的选择，并且可以因系统在道德、法律和安全方面的工作而承担责任。
- en: AI development should respect the fundamental right to privacy at all points
    of the AI application lifecycle, including data collection, processing, and storage
    and model design, development, and deployment. Privacy issues can arise in various
    aspects of AI systems, such as when social media platforms using AI to analyze
    user behaviors and preferences inadvertently expose sensitive information through
    targeted advertising or data breaches, or when AI is used to enhance the capabilities
    of surveillance systems, leading to potential overreach in monitoring activities.
    This can result in a loss of anonymity and freedom, as every action can be watched
    and recorded. For example, governmental structures using facial recognition technologies
    in public spaces can track individuals without their consent, potentially leading
    to misuse of power and privacy violations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: AI开发应在AI应用生命周期的所有环节尊重隐私的基本权利，包括数据收集、处理和存储以及模型设计、开发和部署。AI系统的各个方面都可能引发隐私问题，例如，使用AI分析用户行为和偏好的社交媒体平台可能通过定向广告或数据泄露无意中泄露敏感信息，或者当AI用于增强监控系统功能时，可能导致监控活动的过度扩展。这可能导致匿名性和自由的丧失，因为每个动作都可能被监视和记录。例如，在公共场所使用面部识别技术的政府机构可以在未经个人同意的情况下追踪个人，可能导致权力滥用和隐私侵犯。
- en: AI systems can also amplify biases present in their training data. When these
    biases affect how data is collected, processed, or used, they can disproportionately
    impact the privacy of certain groups. For instance, AI-driven credit scoring models
    might use biased data that discriminates against certain racial or gender groups,
    raising concerns about fairness and privacy in relation to financial data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统还可以放大其训练数据中存在的偏见。当这些偏见影响数据的收集、处理或使用时，它们可能不成比例地影响某些群体的隐私。例如，由AI驱动的信用评分模型可能使用具有歧视性的数据，针对某些种族或性别群体，引发关于财务数据公平性和隐私的担忧。
- en: 'Example metrics and mechanisms to define and track privacy and data governance
    include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 定义和跟踪隐私和数据治理的示例指标和机制包括：
- en: Data encryption levels
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加密级别
- en: These determine the degree of data protection during transmission and at rest.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些决定了数据在传输和静止状态下的保护程度。
- en: Access controls
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制
- en: These are policies and tools that manage who can access or alter data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是管理和控制谁可以访问或更改数据的政策和工具。
- en: Data retention and deletion policies
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据保留和删除政策
- en: These ensure compliance with data minimization principles and regulations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些确保符合数据最小化原则和法规。
- en: Transparency
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度
- en: Data, AI models, and software systems that include AI components must be transparent
    and provide traceability. Regardless of the industry or use case, transparency
    is the [most commonly cited ethical principle](https://oreil.ly/_pir3) in existing
    AI guidelines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据、包含人工智能组件的AI模型和软件系统必须透明并提供可追溯性。无论行业或用例如何，透明度是现有AI指南中[最常引用的伦理原则](https://oreil.ly/_pir3)。
- en: 'Transparency in AI systems refers to the ability to understand how an AI system
    works internally and makes decisions. It provides comprehensible explanations
    about the AI system’s components, algorithms, decision-making process, and overall
    functioning to stakeholders such as users, developers, and regulators. Key dimensions
    of AI transparency include:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统中的透明度指的是理解AI系统内部工作方式和做出决策的能力。它为用户、开发者、监管者等利益相关者提供了关于AI系统组件、算法、决策过程和整体运作的易懂解释。AI透明度的关键维度包括：
- en: Explainability
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性
- en: The ability to clearly articulate in human-understandable terms how and why
    an AI system, encompassing its data sources, model components, and decision-making
    processes, arrived at a specific output or decision for a given case. This is
    important for building trust and accountability. As depicted in [Figure 1-4](#chapter_1_figure_4_1748539916811556),
    explainable AI (XAI) models might not be the models with the highest accuracy,
    so depending on the use case requirements, engineers might select an explainable
    but less accurate AI algorithm. Techniques for XAI are used at every stage of
    the AI lifecycle, including analyzing data for model development, incorporating
    interpretability into the system architecture, and producing post-hoc explanations
    of system behavior.^([1](ch01.html#id333))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 能够用人类可理解的语言清楚地阐述人工智能系统（包括其数据来源、模型组件和决策过程）如何以及为什么针对特定案例得出特定的输出或决策。这对于建立信任和问责制很重要。如图
    1-4 所示，可解释人工智能（XAI）模型可能不是最精确的模型，因此根据用例要求，工程师可能会选择一个可解释但不太精确的人工智能算法。XAI 的技术被用于人工智能生命周期的每个阶段，包括分析数据以进行模型开发、将可解释性纳入系统架构以及产生系统行为的后验解释。（^([1](ch01.html#id333)))
- en: '![](assets/taie_0104.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0104.png)'
- en: Figure 1-4\. Balancing explainability and accuracy is a common challenge in
    AI systems
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-4。在人工智能系统中平衡可解释性和准确性是一个常见的挑战
- en: Data transparency
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据透明度
- en: Openness about the training data used to build an AI model, including its sources,
    characteristics, and potential biases or limitations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用于构建人工智能模型的训练数据的开放性，包括其来源、特征以及潜在的偏差或局限性。
- en: Algorithmic transparency
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 算法透明度
- en: Visibility into the AI algorithms and how they process input data to generate
    outputs or decisions. This includes understanding the features, weights, and logic
    the model uses.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 了解人工智能算法及其如何处理输入数据以生成输出或决策。这包括理解模型使用的特征、权重和逻辑。
- en: Governance transparency
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 治理透明度
- en: Documenting key decisions made during an AI system’s development process, establishing
    clear protocols and responsibilities, and ensuring organizational oversight. This
    aspect is particularly relevant to compliance with the EU AI Act.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 记录人工智能系统开发过程中做出的关键决策，建立明确的协议和责任，并确保组织监督。这一方面尤其与遵守欧盟人工智能法案相关。
- en: Communication transparency
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通信透明度
- en: Sharing information about the AI system’s purpose, capabilities, and limitations
    with relevant stakeholders in a timely, clear, and accessible manner.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 及时、清晰、易于访问地与相关利益相关者分享关于人工智能系统目的、能力和局限性的信息。
- en: AI transparency aims to open the “black box” representing the internal operations
    of AI systems, which are often complex and opaque. This enables humans to understand
    the inner workings of the systems and audit them for errors or biases, fostering
    trust in their use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能透明度旨在打开代表人工智能系统内部操作的“黑盒”，这些系统通常复杂且不透明。这使人类能够理解系统的内部运作，并对其错误或偏差进行审计，从而促进对其使用的信任。
- en: 'Examples of metrics to define and track transparency in AI systems include
    the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 定义和跟踪人工智能系统透明度的指标示例包括以下内容：
- en: Explainability index
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性指数
- en: How easily the system’s decisions can be explained to users. This depends on
    the availability of explanations throughout the system’s lifecycle, the types
    of explanations provided (e.g., feature importance, counterfactual examples, visual
    aids), and the scope of those explanations (whether they address global model
    behavior or local, individual predictions).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 系统决策如何容易地向用户解释。这取决于系统生命周期中解释的可用性、提供的解释类型（例如，特征重要性、反事实示例、视觉辅助）以及解释的范围（它们是否解决全局模型行为或局部、个别预测）。
- en: Documentation completeness
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 文档完整性
- en: Availability and clarity of documentation on the AI system’s purpose, functionality,
    limitations, model training details, and feature engineering process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统目的、功能、局限性、模型训练细节和特征工程过程的文档的可用性和清晰度。
- en: Algorithmic auditability
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可审计性
- en: Ease of auditing AI algorithms for compliance and performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 审计人工智能算法以符合规范和性能的简便性。
- en: Diversity, Non-Discrimination, and Fairness
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性、非歧视和公平性
- en: Discussion about bias and fairness in machine learning has become a hot topic
    over the past decade. Because data collected in an unequal manner and processed
    by non-diverse teams could potentially cause harm, incorporating diversity and
    inclusion throughout the entire AI system lifecycle is a clear requirement for
    trustworthy AI. This includes accessibility, a user-centric approach that guarantees
    that the usability of the AI system takes everyone into account—especially people
    with disabilities.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，关于机器学习中的偏见和公平性的讨论已成为热门话题。由于收集数据的方式不平等，且由非多元化的团队处理，可能会造成潜在的危害，因此在整个人工智能系统生命周期中融入多样性和包容性是建立可信人工智能的明确要求。这包括可访问性，一种以用户为中心的方法，确保人工智能系统的可用性考虑到了所有人，特别是残疾人士。
- en: '[Inclusive engineering](https://oreil.ly/n3qlT) is defined as “the process
    of ensuring that engineering products and services are accessible and inclusive
    of all users, and are as free as possible from discrimination and bias, throughout
    their lifecycle.” This approach is crucial in the development of AI systems, extending
    beyond just technical engineering to encompass broader social considerations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[包容性工程](https://oreil.ly/n3qlT)被定义为“确保工程产品和服务的可访问性和包容性，对所有用户一视同仁，并在其整个生命周期中尽可能减少歧视和偏见的过程。”这种方法在人工智能系统的开发中至关重要，它不仅超越了技术工程，还涵盖了更广泛的社会考量。'
- en: Potential biases should be identified and addressed at every stage of AI system
    development. It is vital to establish a well-defined strategy or set of procedures
    to mitigate bias and promote fairness, both in the collection and use of input
    data and in the algorithm design. In this book, I focus on the [CRISP-ML(Q)](https://oreil.ly/srSRa)
    development process model to specify a fair and effective AI system development
    strategy. This model requires establishing processes for testing and monitoring
    for potential biases and detecting non-representativeness in data across all phases
    of development.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 应在人工智能系统开发的每个阶段识别和解决潜在的偏见。建立一套明确的策略或程序来减轻偏见并促进公平，无论是在收集和使用输入数据时，还是在算法设计中，都是至关重要的。在这本书中，我专注于[CRISP-ML(Q)](https://oreil.ly/srSRa)开发流程模型，以指定一个公平且有效的人工智能系统开发策略。该模型要求在所有开发阶段建立测试和监控潜在偏见的流程，以及检测数据非代表性的流程。
- en: Evaluating the complete end-to-end AI development workflow for fairness is important
    for building successful AI systems. Improving diversity and representativeness
    in AI systems is a step toward compliance with the EU AI Act and providing value
    for all users of those systems. Establishing a robust mechanism for flagging issues
    related to bias, discrimination, or poor performance—such as through bias detection
    tools, categorized reporting systems, and clear reporting guidance for affected
    individuals—helps developers and end users become aware of and address these issues.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 评估完整的端到端人工智能开发工作流程的公平性对于构建成功的人工智能系统至关重要。提高人工智能系统中的多样性和代表性是符合欧盟人工智能法案并为所有系统用户创造价值的一步。建立一种强大的机制来标记与偏见、歧视或性能不佳相关的问题——例如通过偏见检测工具、分类报告系统以及为受影响个人提供的明确报告指南——有助于开发人员和最终用户意识到并解决这些问题。
- en: An often underestimated and less technical aspect of an organization’s data
    culture is educational and awareness initiatives for bias and fairness in AI.
    These initiatives are intended to help AI product managers, designers, and engineers
    become more aware of the possible bias they can inject while designing and developing
    AI systems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 组织数据文化的一个常被低估且不太技术性的方面是关于人工智能中的偏见和公平性的教育和意识倡议。这些倡议旨在帮助人工智能产品经理、设计师和工程师在设计和发展人工智能系统时更加意识到可能引入的偏见。
- en: The EU AI Act ensures that every entity along the AI system’s value chain, from
    producer to deployer, is responsible for providing users with a fair and ethical
    experience. Several metrics are used to evaluate fairness in AI systems, and technical
    approaches for fairness improvement and bias mitigation can be applied before
    modeling (preprocessing), at the point of modeling (in-processing), or after modeling
    (post-processing), as illustrated in [Figure 1-5](#chapter_1_figure_5_1748539916811583).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案确保人工智能系统价值链上的每个实体，从生产者到部署者，都有责任为用户提供公平和道德的体验。使用多种指标来评估人工智能系统的公平性，并且可以在建模（预处理）之前、建模点（处理中）或建模之后（后处理）应用公平性改进和偏见缓解的技术方法，如图[图1-5](#chapter_1_figure_5_1748539916811583)所示。
- en: '![](assets/taie_0105.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0105.png)'
- en: 'Figure 1-5\. Various metrics and approaches can be implemented at different
    stages of the modeling process to ensure equitable outcomes and reduce bias (source:
    [*https://oreil.ly/CGjLK*](https://oreil.ly/CGjLK)*)*'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 在建模过程的各个阶段可以实施不同的指标和方法，以确保公平的结果并减少偏见（来源：[*https://oreil.ly/CGjLK*](https://oreil.ly/CGjLK)*)*
- en: '[Table 1-1](#chapter_1_table_1_1748539916818522) lists some metrics that are
    commonly used to evaluate fairness and non-discrimination. These metrics quantify
    potential biases or disparities in the AI model’s predictions across different
    demographic groups.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1-1](#chapter_1_table_1_1748539916818522) 列出了一些常用于评估公平性和非歧视的指标。这些指标量化了人工智能模型在不同人口群体预测中的潜在偏见或差异。'
- en: Table 1-1\. Preprocessing, in-processing, and post-processing fairness metrics
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. 预处理、内处理和后处理公平性度量
- en: '| Metric category/name | Definition |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 度量类别/名称 | 定义 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Preprocessing |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 预处理 |'
- en: '| Statistical/demographic parity | Ensures equal probability of being classified
    with positive labels across groups |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 统计/人口统计均衡 | 确保在各个群体中被分类为正标签的概率相等 |'
- en: '| Disparate impact | Measures the ratio of positive classification rates between
    unprivileged and privileged groups |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 差异影响 | 衡量无特权群体和特权群体之间正分类率的比率 |'
- en: '| Individual fairness | Ensures similar individuals receive similar treatment
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 个人公平性 | 确保相似的个人受到相似对待 |'
- en: '| In-processing |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 内处理 |'
- en: '| Equal opportunity | Ensures equal true positive rates across different groups
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 平等机会 | 确保不同群体之间具有相等的真正阳性率 |'
- en: '| Equalized odds | Requires equal true positive and false positive rates across
    protected groups |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 平等机会 | 要求在受保护群体中具有相等的真正阳性率和假阳性率 |'
- en: '| Overall accuracy equality | Compares relative accuracy rates between different
    groups |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 总体准确度平等 | 比较不同群体之间的相对准确率 |'
- en: '| Treatment equality | Considers the ratio of false negatives to false positives
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 处理平等 | 考虑假阴性率与假阳性率的比率 |'
- en: '| Post-processing |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 后处理 |'
- en: '| Test fairness/calibration | Ensures equal probability of a positive outcome
    given a particular score across groups |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 测试公平性/校准 | 确保在特定分数下，各个群体获得正结果的概率相等 |'
- en: '| Well calibration | Requires predicted probabilities to match actual probabilities
    across groups |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 良好校准 | 需要预测概率在各个群体中与实际概率相匹配 |'
- en: '| Balance for positive/negative class | Ensures equal expected scores for positive
    and negative classes across groups |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 正负类平衡 | 确保在各个群体中正类和负类的预期分数相等 |'
- en: '| Generalized entropy index | Measures individual-level fairness impacts of
    classification outcomes |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 广义熵指数 | 衡量分类结果对个人层面的公平性影响 |'
- en: Societal and Environmental Well-Being
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会和环境福祉
- en: AI can have positive or negative impacts depending on how it is used. The EU
    AI Act addresses the societal and environmental implications of trustworthy AI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的使用方式决定了它可能产生积极或消极的影响。欧盟人工智能法案关注了可信人工智能的社会和环境影响。
- en: AI technology has proven negative impacts on the environment. The primary environmental
    concerns associated with AI include energy consumption, carbon footprint, e-waste,
    and indirect environmental impacts. AI and machine learning models, especially
    large ones, require significant computational power (as visualized in [Figure 1-6](#chapter_1_figure_6_1748539916811608)).
    The data centers housing these processors thus require vast amounts of energy
    to run and to cool, which means large-scale AI systems have a significant carbon
    footprint. To mitigate this concern, efforts are being made to power data centers
    with renewable energy sources. The production of AI hardware also requires precious
    metals and rare earth elements, which can be environmentally damaging (in terms
    of extraction, resource depletion, and recycling challenges). Additionally, hardware
    quickly becomes obsolete, contributing to electronic waste.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能技术已被证明对环境有负面影响。与人工智能相关的首要环境问题包括能源消耗、碳足迹、电子废物和间接的环境影响。人工智能和机器学习模型，尤其是大型模型，需要大量的计算能力（如图
    1-6 所示）。因此，容纳这些处理器的数据中心需要消耗大量的能源来运行和冷却，这意味着大规模人工智能系统具有显著的碳足迹。为了减轻这一担忧，正在努力使用可再生能源为数据中心供电。人工智能硬件的生产还需要贵重金属和稀土元素，这可能会对环境造成损害（在提取、资源耗竭和回收挑战方面）。此外，硬件很快就会过时，导致电子废物。
- en: '![](assets/taie_0106.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0106.png)'
- en: 'Figure 1-6\. The amount of computing resources used to train deep learning
    models increased 300,000-fold between 2012 and 2018 (source: [*https://oreil.ly/WJOOw*](https://oreil.ly/WJOOw))'
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 用于训练深度学习模型的计算资源量在2012年至2018年之间增加了300,000倍（来源：[*https://oreil.ly/WJOOw*](https://oreil.ly/WJOOw)）
- en: Depending on its applications, AI can have indirect effects on the environment
    as well. For instance, AI-driven automation can lead to increased production capacities
    and potentially increased resource consumption. Conversely, AI can optimize systems
    to be more energy-efficient, reduce waste, or enhance resource management, thereby
    potentially having a positive impact.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其应用，人工智能也可能对环境产生间接影响。例如，人工智能驱动的自动化可能导致生产能力增加和资源消耗增加。相反，人工智能可以优化系统以更节能、减少浪费或提高资源管理，从而可能产生积极影响。
- en: As well as model accuracy, the proposed [“Green AI” approach](https://oreil.ly/jGAsk)
    considers *efficiency*, measured as the number of floating-point operations (flops)
    required to generate a result, as a key evaluation criterion. Financial operations
    (FinOps), a cloud financial management practice that helps organizations efficiently
    manage their cloud spending, can also be leveraged to address the environmental
    and financial impacts of AI.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型准确性之外，提出的[“绿色人工智能”方法](https://oreil.ly/jGAsk)还考虑了*效率*，作为关键评估标准，即生成结果所需的浮点运算次数（flops）。金融操作（FinOps），一种帮助组织有效管理其云支出的云财务管理实践，也可以被利用来应对人工智能的环境和财务影响。
- en: 'With regard to the societal aspect, AI is changing the way we work in three
    key ways: by automating tasks, reshaping work processes, and affecting required
    job skills. Job displacement is an important concern, as AI has the potential
    to automate many roles—especially those that involve [repetitive tasks](https://oreil.ly/7Pdjg).
    At the same time, AI technology has the potential to create [new job roles](https://oreil.ly/zFqNJ)
    that demand advanced technological and analytical skills, such as machine learning
    engineers, data scientists, and AI ethics specialists. Additionally, AI can drive
    the development of innovative products and services, opening up new career opportunities
    in emerging sectors like AI-driven digital assistants and smart devices.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 关于社会方面，人工智能正在以三种关键方式改变我们的工作方式：通过自动化任务、重塑工作流程以及影响所需的工作技能。工作替代是一个重要的担忧，因为人工智能有潜力自动化许多角色——特别是那些涉及[重复性任务](https://oreil.ly/7Pdjg)的。同时，人工智能技术有潜力创造[新的工作角色](https://oreil.ly/zFqNJ)，这些角色需要高级技术和分析技能，例如机器学习工程师、数据科学家和人工智能伦理专家。此外，人工智能可以推动创新产品和服务的开发，为人工智能驱动的数字助手和智能设备等新兴行业开辟新的职业机会。
- en: A related concern is that the rise of AI is leading to increased [job polarization](https://oreil.ly/hW0gZ),
    with demand for high-skilled workers growing and low-skilled positions facing
    obsolescence. This worsens income inequality and creates challenges for those
    without access to education and training. As a result, AI in the workplace can
    lead to increased stress, anxiety, and job insecurity due to the fear of job loss
    and uncertainty about the future.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的担忧是，人工智能的兴起正在导致[工作两极化](https://oreil.ly/hW0gZ)加剧，对高技能劳动力的需求增长，而低技能职位面临淘汰。这加剧了收入不平等，并为那些无法获得教育和培训的人创造了挑战。因此，由于对失业的恐惧和对未来的不确定性，工作场所的人工智能可能导致压力、焦虑和就业不安全感增加。
- en: AI systems also have the potential to negatively impact society at large through
    misinformation. AI—particularly generative AI—can produce misinformation and disinformation
    at scale. This can harm democracy by contributing to misinformation in elections,
    spreading propaganda, and influencing voter behavior. Tools like ChatGPT, pi.ai,
    and perplexity.ai can easily create realistic but false content, which can be
    used to spread misinformation. AI-generated “deepfakes” (convincing but fake videos
    and images) can be used to manipulate public opinion, and AI-driven content personalization
    can create “information bubbles” where individuals are exposed only to information
    that reinforces their existing beliefs, leading to increased polarization and
    social fragmentation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统还有可能通过虚假信息对整个社会产生负面影响。人工智能——尤其是生成式人工智能——可以大规模地产生虚假信息和错误信息。这可能会通过在选举中造成虚假信息、传播宣传和影响选民行为来损害民主。像ChatGPT、pi.ai和perplexity.ai这样的工具可以轻松创建逼真的但虚假的内容，这些内容可以用来传播虚假信息。人工智能生成的“深度伪造”（令人信服但虚假的视频和图像）可以用来操纵公众舆论，而人工智能驱动的内容个性化可以创建“信息泡沫”，其中个人只能接触到加强他们现有信念的信息，从而导致两极化和社会分裂加剧。
- en: The use of AI for surveillance is also an area of concern with regard to societal
    well-being and privacy. Research has shown that at least 75 countries actively
    utilize AI technologies for surveillance (see [Figure 1-6](#chapter_1_figure_6_1748539916811608))*.*
    This can lead to unauthorized data collection, privacy violations and unauthorized
    access to sensitive data, and potential misuse of personal information. In addition,
    use of AI in law enforcement, such as for predictive policing, can result in biased
    outcomes and discrimination against certain demographic groups.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人工智能进行监控也是社会福祉和隐私方面值得关注的一个领域。研究表明，至少有75个国家积极利用人工智能技术进行监控（见[图1-6](#chapter_1_figure_6_1748539916811608))*.*
    这可能导致未经授权的数据收集、隐私侵犯以及敏感数据的未经授权访问，以及个人信息潜在的滥用。此外，在执法领域使用人工智能，例如用于预测警务，可能导致偏见结果和对某些人口群体的歧视。
- en: '![](assets/taie_0107.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0107.png)'
- en: 'Figure 1-7\. AI surveillance technology is being adopted by a larger number
    of countries and at a faster pace than experts typically believe (source: [*https://oreil.ly/9_iB7*](https://oreil.ly/9_iB7))'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. 人工智能监控技术被越来越多的国家采用，其速度比专家通常认为的要快（来源：[*https://oreil.ly/9_iB7*](https://oreil.ly/9_iB7))
- en: Addressing these challenges requires robust ethical guidelines, transparent
    regulatory frameworks, and ongoing public dialogue to ensure that AI is developed
    and deployed to support democratic values and societal well-being.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要强有力的伦理指南、透明的监管框架以及持续的公众对话，以确保人工智能的开发和部署支持民主价值观和社会福祉。
- en: Accountability
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问责制
- en: AI systems must be developed and operated responsibly. *AI accountability* involves
    establishing mechanisms for holding the developers and users of AI systems responsible
    for the impacts of those systems throughout the entire development cycle.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统必须负责任地开发和运营。*人工智能问责制*涉及在整个开发周期内建立机制，以确保人工智能系统的开发者和使用者对其影响负责。
- en: Accountability implies that information about the system’s purpose, design,
    data, and processes is available to internal and external auditors. As an example,
    we can refer to Google’s Responsible Generative AI Toolkit, which covers risk
    and mitigation techniques to address safety, privacy, fairness, and accountability
    (see [Figure 1-8](#chapter_1_figure_8_1748539916811664)). This includes maintaining
    detailed documentation and records of the AI development process, decision making,
    and outcomes to enable traceability. Logging and recordkeeping are essential for
    accountability. Additionally, accountability requires that an AI system can explain
    or justify its decisions. Finally, accountability ensures that there are mechanisms
    and processes for redress in place to minimize or correct negative impacts or
    unfair outcomes caused by AI systems. AI systems should provide appropriate opportunities
    for feedback, relevant explanations, and defined procedures for escalating concerns.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 问责制意味着系统目的、设计、数据和处理过程的信息对内部和外部审计员是可用的。例如，我们可以参考谷歌的负责任生成式人工智能工具包，它涵盖了风险和缓解技术，以解决安全性、隐私、公平性和问责制（见[图1-8](#chapter_1_figure_8_1748539916811664))。这包括维护详细的开发过程、决策和结果记录，以实现可追溯性。日志记录和记录保存对于问责制至关重要。此外，问责制还要求人工智能系统能够解释或证明其决策。最后，问责制确保有机制和流程来减少或纠正由人工智能系统造成的负面影响或不公平结果。人工智能系统应提供适当的反馈机会、相关解释和定义的程序，以升级关注的问题。
- en: '![](assets/taie_0108.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0108.png)'
- en: 'Figure 1-8\. Google’s Responsible Generative AI Toolkit takes a holistic approach
    to accountability (source: [*https://oreil.ly/maEI5*](https://oreil.ly/maEI5))'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. 谷歌的负责任生成式人工智能工具包采用了一种全面的责任方法（来源：[*https://oreil.ly/maEI5*](https://oreil.ly/maEI5))
- en: 'In the context of trustworthy AI, key mechanisms that should be implemented
    to create accountability include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在可信赖的人工智能的背景下，应实施以下关键机制以创建问责制：
- en: Clear responsibility guidelines and processes
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的责任指南和流程
- en: Establish guidelines and clear responsibilities for various stakeholders involved
    in the AI system lifecycle, including developers, deployers, and users.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为人工智能系统生命周期中涉及的各方建立指南和明确的责任，包括开发者、部署者和用户。
- en: Transparency and explainability
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度和可解释性
- en: Maintain detailed documentation of the AI development process, training data,
    algorithms used, and decision-making criteria to enable traceability. Ensure that
    AI systems are transparent about their capabilities, limitations, and decision-making
    processes. Provide clear explanations for decisions made based on the AI predictions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 维护AI开发过程、训练数据、使用的算法和决策标准的详细文档，以实现可追溯性。确保AI系统对其能力、局限性和决策过程透明。对基于AI预测做出的决策提供清晰的解释。
- en: Human oversight and intervention
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 人类监督和干预
- en: Implement human checks and oversight, especially for high-stakes AI decisions,
    with the ability to override the AI when needed.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实施人类检查和监督，特别是在需要时能够覆盖AI决策的高风险情况下。
- en: Auditing and evaluation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 审计和评估
- en: Conduct regular internal and third-party audits to identify and eliminate biases,
    ensuring compliance with regulations and ethical standards. By systematically
    auditing their AI systems using the CRISP-ML(Q) process model, organizations can
    assess the systems’ quality, reliability, and trustworthiness. The audit depth
    and focus areas can be tailored based on the risk and criticality of the use case.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 定期进行内部和第三方审计，以识别和消除偏见，确保符合法规和伦理标准。通过系统地使用CRISP-ML(Q)过程模型对AI系统进行审计，组织可以评估系统的质量、可靠性和可信度。审计深度和重点领域可以根据用例的风险和关键性进行调整。
- en: Redress and complaint mechanisms
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 补救和投诉机制
- en: Establish user-friendly channels for submitting complaints, feedback, or requests
    for explanations about AI decisions. Provide clear processes for affected individuals
    to challenge decisions and seek remedy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 建立用户友好的渠道，用于提交投诉、反馈或请求对AI决策的解释。为受影响个人提供明确的流程，以挑战决策并寻求补救。
- en: Trustworthy AI is the foundational concept behind the legislation of the EU
    AI Act. You should now have a general understanding of the seven key requirements
    of trustworthy AI systems. Next, we’ll take a look at how the Act itself is structured.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可信AI是欧盟AI法案立法背后的基础概念。你现在应该对可信AI系统的七个关键要求有一个一般性的了解。接下来，我们将看看该法案本身的架构。
- en: The EU AI Act in a Nutshell
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧盟AI法案概览
- en: The EU AI Act  focuses  on promoting human-centered and trustworthy artificial
    intelligence. Its primary objective is to foster innovation while ensuring a high
    level of protection for health, safety, and fundamental rights as outlined in
    the Charter—including democracy, the rule of law, and environmental protection—and
    mitigating the potential harmful effects of AI systems in use in the European
    Union. To fully understand the EU AI Act, it is essential to grasp its scope,
    who it applies to, and the timeline for compliance with the AI system requirements.
    This section provides a high-level overview of the EU AI Act. I’ll explain the
    detailed requirements for different risk categories later in the book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟AI法案 侧重于促进以人为本且值得信赖的人工智能。其主要目标是促进创新，同时确保健康、安全和基本权利的高水平保护，这些权利在宪章中概述——包括民主、法治和环境保护——并减轻欧盟使用中的AI系统的潜在有害影响。要全面理解欧盟AI法案，必须掌握其范围、适用对象以及符合AI系统要求的时间表。本节提供了欧盟AI法案的高级概述。我将在本书的后面部分解释不同风险类别的详细要求。
- en: 'Per the [General Provisions](https://oreil.ly/6MMz1), the Act establishes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[一般规定](https://oreil.ly/6MMz1)，该法案确立了：
- en: “Harmonized rules for the placing on the market, the putting into service, and
    the use of AI systems in the Union;
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “在联盟内投放市场、投入使用和使用AI系统的统一规则；
- en: Prohibitions of certain AI practices;
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁止某些AI实践；
- en: Specific requirements for high-risk AI systems and obligations for operators
    of such systems;
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对高风险AI系统的具体要求以及此类系统运营商的义务；
- en: Harmonized transparency rules for certain AI systems;
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 某些AI系统的统一透明度规则；
- en: Harmonized rules for the placing on the market of general-purpose AI models;
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对通用AI模型投放市场的统一规则；
- en: Rules on market monitoring, market surveillance, governance and enforcement;
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 市场监控、市场监管、治理和执法规则；
- en: Measures to support innovation, with a particular focus on [small and medium-sized
    enterprises], including startups.”
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持创新的措施，特别是关注[中小企业]，包括初创企业。”
- en: 'It is structured into the following main sections:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案结构如下：
- en: Chapters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 章节
- en: The Act is divided into 13 chapters covering different aspects, such as general
    provisions, prohibited AI practices, requirements for high-risk AI systems, governance,
    etc.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案分为13章，涵盖了不同的方面，如一般规定、禁止的人工智能实践、高风险人工智能系统的要求、治理等。
- en: Articles
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 文章
- en: Each chapter contains one or more articles laying out the specific rules and
    obligations. The articles are numbered sequentially throughout the Act and may
    be grouped into sections within each chapter. For example, Article 6 under Section
    1 of Chapter III covers the “Classification Rules for High-Risk AI Systems.”
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每章包含一个或多个文章，阐述了具体的规则和义务。文章在法案中按顺序编号，并且可以在每个章节内分组。例如，第三章第一节下的第6条涵盖了“高风险人工智能系统的分类规则。”
- en: Annexes
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 附件
- en: The Act has 13 annexes that provide supplementary information such as lists,
    definitions, and procedures. You can navigate to a specific Annex by its number
    or title, e.g., “Annex III” or “High-Risk AI Systems Referred to in Article 6(2).”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案包含13个附件，提供了补充信息，如列表、定义和程序。您可以通过编号或标题导航到特定的附件，例如“附件III”或“第6(2)条中提到的“高风险人工智能系统”。”
- en: Recitals
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理由部分
- en: The recitals are numbered paragraphs that explain the rationale and context
    behind the Act’s provisions. They can help readers interpret the articles but
    are not binding.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 理由部分是编号段落，解释了法案条款背后的理由和背景。它们可以帮助读者解释文章，但并非具有约束力。
- en: Definitions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: 'Chapter I, Article 3 of the EU AI Act provides a set of definitions for terms
    used in the legislation. Perhaps most importantly for the purposes of this book,
    it defines an *AI system* as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案第一章第三条为立法中使用的术语提供了一套定义。也许对于本书的目的来说最重要的是，它将*人工智能系统*定义为：
- en: A machine-based system that is designed to operate with varying levels of autonomy
    and that may exhibit adaptiveness after deployment, and that, for explicit or
    implicit objectives, infers, from the input it receives, how to generate outputs
    such as predictions, content, recommendations, or decisions that can influence
    physical or virtual environments.
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种基于机器的系统，旨在以不同的自主程度运行，并在部署后可能表现出适应性，并且，为了明确或隐含的目标，从它接收的输入中推断出如何生成输出，如预测、内容、推荐或决策，这些输出可以影响物理或虚拟环境。
- en: Note, however, that given the rapid pace and unpredictability of technological
    and AI development, this definition is not entirely static, and a dynamic regulatory
    tool has been integrated into the Act to allow it to adapt over time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，鉴于技术和人工智能发展的快速节奏和不可预测性，这个定义并非完全静止，法案中已经整合了一个动态监管工具，以便其能够随着时间的推移而适应。
- en: 'The definition covers a wide range of AI techniques and approaches, including:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该定义涵盖了广泛的人工智能技术和方法，包括：
- en: Machine learning methods (such as supervised, unsupervised, and semi-supervised
    machine learning)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习方法（例如监督学习、无监督学习和半监督学习）
- en: Deep learning methods
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习方法
- en: Reinforcement learning
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Logic- and knowledge-based methods (such as logic programming, expert systems,
    inference and deductive engines, reasoning engines)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于逻辑和知识的方法（例如逻辑编程、专家系统、推理和演绎引擎、推理引擎）
- en: Statistical approaches
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计方法
- en: Bayesian methods
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯方法
- en: Search and optimization approaches
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索和优化方法
- en: 'Pretrained generative models such as BERT, DALL·E, Claude, Mistral, and GPT
    have become increasingly popular in recent years. They are developed using large-scale
    self-supervised learning on massive datasets and can be adjusted to various downstream
    tasks. These are commonly referred to as *general-purpose AI (GPAI) model**s*,
    which the Act defines as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练生成模型如BERT、DALL·E、Claude、Mistral和GPT变得越来越受欢迎。它们是通过在大量数据集上进行大规模自监督学习开发的，并且可以调整以适应各种下游任务。这些通常被称为*通用人工智能（GPAI）模型**，法案如下定义：
- en: An AI model . . . trained with a large amount of data using self-supervision
    at scale, that displays significant generality and is capable of competently performing
    a wide range of distinct tasks regardless of the way the model is placed on the
    market and that can be integrated into a variety of downstream systems or applications,
    except AI models that are used for research, development or prototyping activities
    before they are placed on the market
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个使用大量数据通过大规模自监督训练的AI模型，显示出显著的泛化能力，并且能够胜任各种不同的任务，无论该模型如何投放市场，并且可以集成到各种下游系统或应用中，但前提是这些AI模型在投放市场之前用于研究、开发或原型制作活动
- en: The EU AI Act aims to ensure that the previously mentioned techniques used in
    digital and physical products, services, or systems are safe and respect existing
    laws on fundamental rights and European Union values.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案旨在确保之前提到的用于数字和物理产品、服务或系统中的技术是安全的，并尊重现有的关于基本权利和欧盟价值观的法律。
- en: Key Players from Creation to Market Operation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从创建到市场运营的关键参与者
- en: The EU AI Act is a legal framework for developing, distributing, and using AI
    in the EU. This regulatory framework applies to companies and persons that make,
    bring in, or distribute AI systems or general-purpose AI models in the EU, even
    if those entities are located in a country outside the EU. [Figure 1-9](#chapter_1_figure_10_1748539916811704)
    visualizes the key players who are affected by the EU AI Act throughout the complete
    AI system lifecycle.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案是欧盟开发、分销和使用人工智能的法律框架。这一监管框架适用于在欧盟制造、引进或分销人工智能系统或通用人工智能模型的公司和个人，即使这些实体位于欧盟以外的国家。[图1-9](#chapter_1_figure_10_1748539916811704)展示了在整个人工智能系统生命周期中受欧盟人工智能法案影响的关键参与者。
- en: '![](assets/taie_0109.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/taie_0109.png)'
- en: Figure 1-9\. Key players
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9. 关键参与者
- en: 'Article 3 defines these roles as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第三条如下定义了这些角色：
- en: A *provider* is a natural or legal person (company, organization, or other body)
    that develops an AI system or GPAI model (or has one developed) and puts it on
    the market or uses it, whether for payment or for free, under their own name or
    brand.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *提供者* 是指开发人工智能系统或通用人工智能模型（或由其开发）并将其投放市场或使用的自然人或法人（公司、组织或其他实体），无论是有偿还是免费，以自己的名义或品牌。
- en: An *importer* is a natural or legal person in the EU who places on the market
    an AI system bearing the name or trademark of an entity in a non-EU country.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *进口商* 是指在欧盟内的自然人或法人，该实体将带有非欧盟国家实体名称或商标的人工智能系统投放市场。
- en: A *distributor* is a natural or legal person in the supply chain, other than
    the provider or the importer, that makes an AI system available on the EU market.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *分销商* 是指供应链中的自然人或法人，除了提供者或进口商之外，使人工智能系统在欧盟市场上可用。
- en: An *authorized representative* is a natural or legal person in the EU who has
    been given written permission by the provider of an AI system or GPAI model to
    carry out the responsibilities and procedures outlined in the Act on their behalf.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *授权代表* 是指在欧盟内，由人工智能系统或通用人工智能模型提供者书面授权的自然人或法人，代表其执行法案中概述的职责和程序。
- en: A *deployer* is a natural or legal person using an AI system for professional
    activities.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *部署者* 是指使用人工智能系统进行专业活动的自然人或法人。
- en: A *user* is considered to be any natural person or group of persons who use
    or are otherwise affected by an AI system.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *用户* 被认为是任何使用或受人工智能系统影响的自然人或人员群体。
- en: Classification of AI Systems by Risk Levels
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能系统按风险等级分类
- en: 'Another important aspect of the EU AI Act is its risk-based approach. According
    to the regulation’s classification system, AI systems are categorized into four
    risk levels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案的另一个重要方面是其基于风险的方法。根据该法规的分类系统，人工智能系统被分为四个风险等级：
- en: Prohibited
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁止
- en: High risk
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高风险
- en: Limited risk
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限风险
- en: Minimal risk
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 极小风险
- en: It is also important to note that GPAI models are specifically regulated and
    classified under this Act. In [Chapter 7](ch07.html#chapter_7_toward_trustworthy_general_purpose_ai_and_generati_1748539924538638),
    I will lay out the foundation for understanding the requirements for compliance
    with the Act for GPAI models, and their relation to the currently popular generative
    AI.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，通用人工智能模型（GPAI）受到特别监管和分类，并纳入本法案。在[第七章](ch07.html#chapter_7_toward_trustworthy_general_purpose_ai_and_generati_1748539924538638)中，我将阐述理解符合该法案对GPAI模型要求的基础，以及它们与目前流行的生成式人工智能的关系。
- en: The EU AI Act prohibits certain AI systems that are considered to pose an unacceptable
    risk. The bans are aimed at AI systems that could heavily influence or harm people’s
    decision making or infringe upon their rights. Specifically, the Act prohibits
    AI practices such as using manipulative subliminal techniques, exploiting vulnerabilities
    based on age, disability, or social circumstances, and making high-stakes assessments
    based on profiling or predictive traits without sufficient human oversight. The
    legislation also prohibits the unregulated use of “real-time” biometric identification
    in public spaces, unless under strict conditions for law enforcement purposes
    related to significant public safety concerns. (See [Chapter II, Article 5](https://oreil.ly/0KGOH),
    for more information.)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟AI法案禁止某些被认为存在不可接受风险的AI系统。这些禁令针对的是可能严重影响或伤害人们决策或侵犯他们权利的AI系统。具体来说，法案禁止使用操纵性潜意识技术、基于年龄、残疾或社会状况的漏洞利用，以及在没有足够人类监督的情况下，基于个人资料或预测特征进行高风险评估的AI实践。该法规还禁止在公共场合未经监管使用“实时”生物识别识别，除非在执法目的相关的重要公共安全担忧的严格条件下。（更多信息请参见[第二章，第五条](https://oreil.ly/0KGOH)。）
- en: '[Figure 1-10](#chapter_1_figure_11_1748539916811723) illustrates the risk categories
    and their expected distribution. Roughly 20% of all AI systems are expected to
    be classified as high risk.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-10](#chapter_1_figure_11_1748539916811723) 展示了风险类别及其预期分布。预计所有AI系统中大约有20%将被归类为高风险。'
- en: '![](assets/taie_0110.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0110.png)'
- en: Figure 1-10\. AI system risk categories
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. AI系统风险类别
- en: High-risk AI systems, as defined by the EU AI Act, either are intended to be
    used as safety components of products or are products themselves (see [Chapter
    III](https://oreil.ly/_o2pw)). These AI systems are subject to specific legislation
    and require third-party conformity assessments. In addition, high-risk AI systems
    specified in a designated list (Annex III) are subject to stringent compliance
    requirements due to their potential impact on the health, safety, or fundamental
    rights of individuals. These include systems in areas such as biometrics, critical
    infrastructure, education, etc.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 根据欧盟AI法案定义，高风险AI系统要么是作为产品安全组件的预期用途，要么本身就是产品（参见[第三章](https://oreil.ly/_o2pw)）。这些AI系统受特定法规的约束，并需要第三方合格评定。此外，指定清单（附件III）中列出的高风险AI系统由于可能对个人健康、安全或基本权利产生影响，因此受到严格的合规要求。这些包括生物识别、关键基础设施、教育等领域。
- en: Like many products traded on the extended Single Market in the European Economic
    Area (EEA), AI systems classified as high risk must receive the CE marking ([Figure 1-11](#chapter_1_figure_12_1748539916811741))
    to be certified within the EU. This mark indicates that products sold in the EEA
    have been evaluated to meet stringent safety, health, and environmental protection
    requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与在欧洲经济区（EEA）扩展单一市场上交易的大多数产品一样，被归类为高风险的AI系统必须在欧盟内获得CE标志([图1-11](#chapter_1_figure_12_1748539916811741))以获得认证。此标志表明在EEA销售的产品的安全、健康和环境保护要求已得到评估。
- en: '![](assets/taie_0111.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0111.png)'
- en: Figure 1-11\. The CE mark
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11\. CE标志
- en: Limited-risk AI systems pose lower risks, mostly in the form of manipulation,
    deception, or impersonation. This category includes systems like chatbots and
    generative AI systems capable of producing deepfakes. For limited-risk AI systems,
    the main obligation is transparency—providers must disclose that the output is
    AI-generated and users must be made aware they are interacting with an AI system.
    There are also requirements to label deepfakes clearly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 低风险AI系统带来的风险较低，主要表现为操纵、欺骗或模仿。这一类别包括聊天机器人和能够生成深度伪造的生成式AI系统。对于低风险AI系统，主要义务是透明度——提供商必须披露输出是AI生成的，并且用户必须意识到他们正在与AI系统互动。还需要明确标记深度伪造。
- en: The last category is minimal-risk AI systems. According to the EU AI Act, these
    are systems that pose little to no risk to people’s safety, fundamental rights,
    or privacy. They include AI applications like video games, spam filters, or simple
    image editing tools that perform narrow tasks with limited decision-making capabilities.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个类别是最小风险AI系统。根据欧盟AI法案，这些系统对人们的安全、基本权利或隐私几乎或没有风险。它们包括视频游戏、垃圾邮件过滤器或执行有限任务且决策能力有限的简单图像编辑工具等AI应用。
- en: It is important to understand that the EU AI Act classifies use cases (AI systems
    and GPAI models) and not the AI/ML technologies or algorithms themselves. Proper
    classification has an impact on the estimation of the AI system’s requirements,
    because different risk categories imply different governance and MLOps architectural
    decisions and obligations. However, determining the risk level of an AI system
    can be challenging as it relies on various factors and involves classifying how
    the capabilities of a nondeterministic system will affect users and systems that
    may interact with it in the future. In [Chapter 4](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657),
    I outline a risk classification framework for AI systems.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，欧盟人工智能法案对用例（人工智能系统和GPAI模型）进行分类，而不是对人工智能/机器学习技术或算法本身进行分类。正确的分类会影响对人工智能系统要求的估计，因为不同的风险类别意味着不同的治理和MLOps架构决策和义务。然而，确定人工智能系统的风险水平可能具有挑战性，因为它依赖于各种因素，并涉及如何对非确定性系统的能力进行分类，以及这些能力将如何影响未来可能与之交互的用户和系统。在[第4章](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657)中，我概述了一个人工智能系统风险分类框架。
- en: Note that scientific research and military applications of AI fall outside the
    scope of the EU AI Act. However, so-called “dual use” technologies—systems that
    can be used for both civilian and military purposes—may still be subject to the
    Act if used in civilian contexts.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，人工智能的科学研究和军事应用不在欧盟人工智能法案的范围内。然而，所谓的“双重用途”技术——既可用于民用也可用于军事目的的系统——如果在民用环境中使用，可能仍然受该法案的约束。
- en: Enforcement and Implementation
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执法和实施
- en: 'The implementation of the EU AI Act follows a structured timeline. Here are
    the key milestones and deadlines:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案的实施遵循一个结构化的时间表。以下是关键里程碑和截止日期：
- en: EU AI Act enters into force (August 2024)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案生效（2024年8月）
- en: The Act is effective 20 days after it is published in the Official Journal of
    the EU.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案自公布于欧盟官方期刊之日起20天后生效。
- en: Six months after entry into force (Q4 2024–Q1 2025)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起六个月后（2024年第四季度至2025年第一季度）
- en: Prohibitions on unacceptable-risk AI systems are effective.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对不可接受风险的人工智能系统禁令生效。
- en: Nine months after entry into force (Q1 2025)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起九个月后（2025年第一季度）
- en: Codes of practice for GPAI models must be finalized.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: GPAI模型的行为准则必须最终确定。
- en: Twelve months after entry into force (Q2–Q3 2025)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起十二个月后（2025年第二季度至第三季度）
- en: Obligations on providers of GPAI models are effective.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: GPAI模型提供者的义务开始适用。
- en: Eighteen months after entry into force (Q4 2025–Q1 2026)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起十八个月后（2025年第四季度至2026年第一季度）
- en: The Commission implements acts on post-market monitoring for high-risk AI systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 委员会实施针对高风险人工智能系统的上市后监测法规。
- en: Twenty-four months after entry into force (Q2–Q3 2026)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起二十四个月后（2026年第二季度至第三季度）
- en: Obligations on high-risk AI systems listed in Annex III become applicable. Member
    states must have implemented rules on penalties and established AI regulatory
    sandboxes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 附件III中列出的高风险人工智能系统义务开始适用。成员国必须实施处罚规则并建立人工智能监管沙盒。
- en: Thirty-six months after entry into force (Q4 2026–Q1 2027)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 自生效之日起三十六个月后（2026年第四季度至2027年第一季度）
- en: Obligations for high-risk AI systems not prescribed in Annex III but subject
    to existing EU product safety laws are effective.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 未在附件III中规定但受现有欧盟产品安全法约束的高风险人工智能系统义务生效。
- en: By the end of 2030
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 到2030年底
- en: Obligations for specific AI systems that are components of large-scale EU IT
    systems in areas like security and justice become applicable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全和司法等领域的欧盟大型IT系统中的特定人工智能系统义务开始适用。
- en: This timeline, depicted in [Figure 1-12](#chapter_1_figure_13_1748539916811763),
    implies that organizations that create high-risk AI systems today have a grace
    period to prepare their internal processes. They are responsible for complying
    fully with the provisions of the Act by the end of the grace period (June 2026).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此时间线，如图[图1-12](#chapter_1_figure_13_1748539916811763)所示，意味着今天创建高风险人工智能系统的组织有一个宽限期来准备其内部流程。它们有责任在宽限期结束前（2026年6月）完全遵守法案的规定。
- en: '![](assets/taie_0112.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/taie_0112.png)'
- en: Figure 1-12\. Timeline outlining the key milestones and deadlines for the implementation
    of the EU AI Act
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-12\. 概述欧盟人工智能法案实施关键里程碑和截止日期的时间线
- en: The Full Picture of Compliance
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遵守的全面图景
- en: 'Many organizations using and embedding AI technology into their products are
    asking the same question: “What does the EU AI Act mean for us?” The Act aims
    to ensure that all digital and physical products that incorporate AI—whether as
    a feature or as a core function—are used in a safe and ethical manner, in line
    with EU fundamental rights. To this end, the Act treats AI systems as regulated
    products. Like other products in the EU market, AI systems that are considered
    high risk must be CE marked to indicate compliance, as described earlier, or they
    are not permitted for use. [Table 1-2](#chapter_1_table_2_1748539916818552) briefly
    outlines the process steps needed to meet the EU AI Act’s requirements for this
    category of AI systems.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 许多使用并将人工智能技术嵌入其产品的组织都在问同样的问题：“欧盟人工智能法案对我们意味着什么？”该法案旨在确保所有包含人工智能的数字和物理产品——无论作为功能还是核心功能——都安全且合乎道德地使用，符合欧盟的基本权利。为此，法案将人工智能系统视为受监管的产品。与欧盟市场上的其他产品一样，被认为高风险的人工智能系统必须按照前面所述进行CE标志以表明合规，否则不得使用。[表1-2](#chapter_1_table_2_1748539916818552)简要概述了满足欧盟人工智能法案对这类人工智能系统要求的必要步骤。
- en: Warning
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Again, the author is not a lawyer. This book does not provide legal advice.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，作者不是律师。本书不提供法律建议。
- en: Table 1-2\. The end-to-end process for EU AI Act compliance
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-2. 欧盟人工智能法案端到端合规流程
- en: '| EU AI Act compliance step | Guiding questions |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 欧盟人工智能法案合规步骤 | 指导问题 |'
- en: '| --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AI system inventory and risk classification |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能系统清单和风险分类 |'
- en: How many AI systems are in place already or are intended to be put in production?
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经部署或计划投入生产的人工智能系统有多少？
- en: What risk categories do each of these AI systems belong to?
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些人工智能系统分别属于哪些风险类别？
- en: Do any of them utilize models that are classified as systemic risk GPAI models?
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是否使用了被归类为系统性风险GPAI模型的模型？
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Identification of compliance requirements |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 合规要求识别 |'
- en: What requirements do we need to fulfill?
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要满足哪些要求？
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Compliance operationalization |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 合规实施 |'
- en: What processes, structures, engineering practices, and roles need to be established
    to comply with the Act?
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要建立哪些流程、结构、工程实践和角色才能符合法案？
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pre-market compliance verification |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 市场前合规验证 |'
- en: What has to be done before placing AI systems on the market and putting them
    into service?
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将人工智能系统投放市场并投入使用之前，需要做些什么？
- en: What internal and external conformity assessments are required for compliance
    verification?
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要进行哪些内部和外部一致性评估以进行合规验证？
- en: How do we CE-mark our AI systems?
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何为我们的AI系统进行CE标志？
- en: Where should our AI systems be registered?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的人工智能系统应该在何处注册？
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Post-market continuous compliance |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 市场后持续合规 |'
- en: What has to be done to ensure compliance after putting the AI systems into service?
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在人工智能系统投入使用后，需要做些什么来确保合规？
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Let’s take a closer look at these essential steps for providers and deployers
    of AI systems  to ensure conformity with the EU AI Act:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看人工智能系统提供者和部署者必须遵循的这些基本步骤，以确保符合欧盟人工智能法案：
- en: 1\. AI system inventory and risk classification
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 人工智能系统清单和风险分类
- en: Before setting up the technical and organizational processes for achieving compliance,
    you must determine the required scope of compliance measures. Start with an inventory
    of all AI systems that are currently in use or in development. Then assign a risk
    level to each of those systems (I discuss risk classification for AI systems in
    detail in [Chapter 4](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657)).
    Any AI use cases that fall into the prohibited category must be discontinued.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立实现合规的技术和组织流程之前，您必须确定合规措施所需的范围。从当前使用或正在开发的所有人工智能系统清单开始。然后，为每个系统分配风险等级（我在[第4章](ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657)中详细讨论了人工智能系统的风险分类）。任何属于禁止类别的AI用例都必须停止使用。
- en: 2\. Identification of compliance requirements
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 合规要求识别
- en: The compliance measures to implement will depend on the results of the risk
    classification. It’s important to distinguish between obligations for AI systems
    and for GPAI models used in those systems, which may be subject to transparency
    obligations or systemic risk requirements. The Act lays out concrete requirements
    for both categories.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 将要实施的合规措施将取决于风险分类的结果。区分人工智能系统的义务和用于这些系统的GPAI模型的义务很重要，这些模型可能受到透明度义务或系统性风险要求的约束。法案为这两类都规定了具体要求。
- en: 3\. Compliance operationalization
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 合规实施
- en: Identifying compliance requirements allows you to define the scope of the technical
    and organizational processes needed for conformity with the EU AI Act. This includes
    establishing governance structures, engineering practices, and clearly defined
    roles. The Act outlines specific obligations, such as implementing risk management
    measures, maintaining technical documentation, ensuring human oversight, and guaranteeing
    the accuracy, robustness, and security of AI systems. This phase, which is the
    focus of this book, also involves setting up a quality management system that
    covers testing, incident reporting, data governance, record retention, and logging.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 确定合规要求使您能够定义符合欧盟人工智能法案所需的技术和组织流程的范围。这包括建立治理结构、工程实践和明确定义的角色。法案概述了具体的义务，例如实施风险管理措施、维护技术文档、确保人工监督以及保证人工智能系统的准确性、鲁棒性和安全性。本书的重点阶段还涉及建立一个涵盖测试、事件报告、数据治理、记录保留和日志记录的质量管理体系。
- en: 4\. Pre-market compliance verification
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 市场前合规验证
- en: The next step is to demonstrate compliance through internal and, where necessary,
    external conformity assessments. Once these are completed, the AI systems must
    be CE marked and registered in the EU database. These steps are mandatory before
    the systems can be placed on the market or put into service.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过内部和必要时外部的一致性评估来证明合规性。一旦这些评估完成，人工智能系统必须获得CE标志并在欧盟数据库中注册。在系统可以投放市场或投入使用之前，这些步骤是强制性的。
- en: 5\. Post-market continuous compliance
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 市场后持续合规
- en: The final phase focuses on ensuring that the AI systems continue to meet the
    requirements of the Act throughout their lifecycle. This involves demonstrating
    continued adherence to compliance standards, regardless of changes to the system,
    through continuous monitoring of system performance and addressing any issues
    that arise to maintain alignment with regulatory requirements.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后阶段的重点是确保人工智能系统在其整个生命周期内继续满足法案的要求。这涉及通过持续监控系统性能和解决出现的任何问题来证明持续遵守合规标准，以保持与监管要求的对齐。
- en: Penalties for EU AI Act Violation
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案违规罚款
- en: The EU AI Act introduces significant penalties and fines for companies that
    violate its rules and requirements. [Table 1-3](#chapter_1_table_3_1748539916818575)
    provides an overview.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案对违反其规则和要求的公司引入了重大罚款。[表1-3](#chapter_1_table_3_1748539916818575) 提供了概述。
- en: Table 1-3\. Penalties for violating the terms of EU AI Act
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-3. 违反欧盟人工智能法案条款的罚款
- en: '| Violation | Penalties |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 违规行为 | 罚款 |'
- en: '| --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Prohibited AI practices (AI practices listed in Article 5, such as exploiting
    vulnerabilities, social scoring, real-time biometric identification in public
    spaces, etc.) | Administrative fines of up to €35 million or 7% of total worldwide
    annual turnover for the preceding financial year, whichever is higher |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 禁止的人工智能实践（如第5条中列出的人工智能实践，例如利用漏洞、社会评分、公共场所实时生物识别等） | 行政罚款最高可达3500万欧元或前一年度全球年度总营业额的7%，以较高者为准
    |'
- en: '| Noncompliance with requirements for high-risk AI systems under Article 10
    | Administrative fines of up to €30 million or 6% of total worldwide annual turnover,
    whichever is higher |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 第10条关于高风险人工智能系统合规要求不符合 | 行政罚款最高可达3000万欧元或全球年度总营业额的6%，以较高者为准 |'
- en: '| Noncompliance with other obligations (apart from Articles 5 and 10) | Administrative
    fines of up to €20 million or 4% of total worldwide annual turnover, whichever
    is higher |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 不符合其他义务（除第5条和第10条外） | 行政罚款最高可达2000万欧元或全球年度总营业额的4%，以较高者为准 |'
- en: '| Providing incorrect, incomplete, or misleading information to authorities
    | Administrative fines of up to €10 million or 2% of total worldwide annual turnover,
    whichever is higher |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 向当局提供不正确、不完整或误导性信息 | 行政罚款最高可达1000万欧元或全球年度总营业额的2%，以较高者为准 |'
- en: Existing AI Regulations and Standards
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有人工智能法规和标准
- en: 'The EU AI Act serves as a comprehensive regulatory model for non-EU nations,
    underscoring the balance between innovation and trustworthy AI. Its strict regulations
    on biometric systems and high-risk AI applications set a high standard for AI
    governance globally. Let’s briefly review the existing landscape of AI regulations:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案为非欧盟国家提供了一个全面的监管模型，强调了创新与可信人工智能之间的平衡。其对生物识别系统和高风险人工智能应用的严格规定，为全球人工智能治理设定了高标准。让我们简要回顾一下现有的人工智能监管格局：
- en: UNESCO AI ethics recommendations
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 联合国教科文组织人工智能伦理建议
- en: The [Recommendation on the Ethics of Artificial Intelligence](https://oreil.ly/Qr8eM),
    adopted in November 2021 by UNESCO’s 193 Member States, provides a framework for
    ethical AI development. It outlines core values and principles for the ethical
    development and deployment of AI, including respect for human rights, inclusion
    and diversity, fairness and non-discrimination, transparency and explainability,
    accountability, safety and security, and sustainability. The recommendation aims
    to influence ethical AI practices globally and includes innovative tools and methodologies
    to translate ethical principles into practice.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 《关于人工智能伦理的建议》（[https://oreil.ly/Qr8eM](https://oreil.ly/Qr8eM)），由联合国教科文组织193个成员国于2021年11月通过，为伦理人工智能发展提供了一个框架。它概述了伦理人工智能开发和部署的核心价值观和原则，包括尊重人权、包容性和多样性、公平和非歧视、透明度和可解释性、问责制、安全性和可持续性。该建议旨在影响全球的伦理人工智能实践，并包括将伦理原则转化为实践的创新工具和方法。
- en: US executive order on trustworthy AI
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 美国关于可信人工智能的行政命令
- en: The [Executive Order on the Safe, Secure, and Trustworthy Development and Use
    of Artificial Intelligence](https://oreil.ly/zRfHi), issued in October 2023, emphasizes
    the potential benefits and risks of AI. It highlights the importance of responsible
    AI governance to address societal issues and prevent negative consequences such
    as fraud, bias, and threats to national security. The order stresses the need
    for collaboration across government, industry, academia, and civil society to
    ensure the safe and responsible development and application of AI.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 《关于人工智能安全、可靠和可信发展及使用的行政命令》（[https://oreil.ly/zRfHi](https://oreil.ly/zRfHi)），于2023年10月发布，强调了人工智能的潜在利益和风险。它突出了负责任的人工智能治理对于解决社会问题以及防止欺诈、偏见和国家安全威胁等负面后果的重要性。该命令强调了政府、行业、学术界和民间社会之间合作的需求，以确保人工智能的安全和负责任的发展与应用。
- en: China generative AI services law
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 中国生成式人工智能服务法
- en: The [Interim Measures for the Administration of Generative Artificial Intelligence
    Services](https://oreil.ly/Axc-r), jointly issued by the Cyberspace Administration
    of China (CAC) and six other government agencies in July 2023, signal China’s
    proactive approach to regulating generative AI services (including models and
    related technologies that produce text, graphics, audio, and video). The regulation
    applies to all entities offering these services to the general Chinese population.
    The Interim Measures recognize the potential for foreign investment while also
    promoting innovation and research. Future artificial intelligence laws are expected
    to expand the regulation’s scope beyond generative AI. Given the potential penalties
    or shutdowns for noncompliant services operating in China, it is essential to
    ensure compliance.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 《生成式人工智能服务管理暂行办法》（[https://oreil.ly/Axc-r](https://oreil.ly/Axc-r)），由中国国家互联网信息办公室（CAC）和六个其他政府部门于2023年7月联合发布，标志着中国对生成式人工智能服务（包括生成文本、图形、音频和视频的模型和相关技术）监管的积极态度。该法规适用于向中国普通民众提供这些服务的所有实体。《暂行办法》承认外国投资的可能性，同时也促进创新和研究。预计未来的人工智能法律将扩大该法规的适用范围，超越生成式人工智能。鉴于在中国运营的不合规服务可能面临处罚或关闭的风险，确保合规至关重要。
- en: National Institute of Standards and Technology (NIST) AI Risk Management Framework
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 国家标准与技术研究院（NIST）人工智能风险管理框架
- en: 'The NIST [Artificial Intelligence Risk Management Framework: Generative Artificial
    Intelligence Profile](https://oreil.ly/sISXu) released in July 2024 identifies
    12 key risks unique to or exacerbated by GenAI technologies that organizations
    should take care to identify and mitigate. These include risks related to chemical,
    biological, radiological, or nuclear information, dangerous or violent recommendations,
    data privacy, environmental impacts, information integrity and security, and intellectual
    property. The framework proposes over 400 actions organizations can take to manage
    these risks.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 美国国家标准与技术研究院（NIST）发布的《人工智能风险管理框架：生成式人工智能配置文件》（[https://oreil.ly/sISXu](https://oreil.ly/sISXu)），于2024年7月发布，确定了12个与生成式人工智能技术独特相关或加剧的关键风险，组织应小心识别和缓解这些风险。这些风险包括与化学、生物、辐射或核信息相关的风险，危险或暴力的建议，数据隐私，环境影响，信息完整性和安全性，以及知识产权。该框架提出了组织可以采取的400多项行动来管理这些风险。
- en: Institute of Electrical and Electronics Engineers Standards Association (IEEE
    SA) recommended practice
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 电气和电子工程师协会标准协会（IEEE SA）推荐实践
- en: The IEEE’s [Recommended Practice for Organizational Governance of Artificial
    Intelligence](https://oreil.ly/Pd2qj) outlines governance criteria for AI development
    and use within organizations, including safety, transparency, accountability,
    responsibility, and bias minimization. It also provides steps for implementation,
    auditing, training, and compliance. The document aims to promote responsible AI
    practices by integrating internal governance structures with external AI governance
    instruments.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE的[《人工智能组织治理推荐实践》](https://oreil.ly/Pd2qj)概述了组织内部AI开发和使用的治理标准，包括安全性、透明度、问责制、责任和偏见最小化。它还提供了实施、审计、培训和合规的步骤。该文件旨在通过将内部治理结构与外部AI治理工具相结合，促进负责任的AI实践。
- en: These laws and frameworks collectively aim to regulate AI in a way that promotes
    innovation while ensuring safety, transparency, and respect for human rights,
    similar to the objectives of the EU AI Act.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这些法律和框架共同旨在以促进创新的同时确保安全、透明度和尊重人权的方式规范AI，类似于欧盟AI法案的目标。
- en: Conclusion
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'The EU AI Act focuses on promoting the development and use of human-centered
    and trustworthy artificial intelligence, to safeguard health, safety, fundamental
    rights, democracy, the rule of law, and environmental protection within the European
    Union. In this first chapter, we examined the concept of trustworthiness in AI,
    which is based on three pillars: lawfulness, ethics, and robustness. I outlined
    the seven key requirements for trustworthy AI systems, which include human agency
    and oversight; technical robustness and safety; privacy and data governance; transparency;
    diversity, non-discrimination, and fairness; societal and environmental well-being;
    and accountability.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟AI法案侧重于促进以人为本和值得信赖的人工智能的开发和使用，以保障欧盟内的健康、安全、基本权利、民主、法治和环境保护。在本章的第一部分，我们探讨了AI中的可信度概念，该概念基于三个支柱：合法性、伦理和稳健性。我概述了可信AI系统的七个关键要求，包括人类代理和监督；技术稳健性和安全性；隐私和数据治理；透明度；多样性、非歧视和公平；社会和环境福祉；以及问责制。
- en: Understanding the Act requires knowledge of its scope, who it applies to, and
    the established timeline for AI system compliance. The EU AI Act is a comprehensive
    risk-based legal framework that establishes rules for the development, distribution,
    and use of AI systems and GPAI models. It applies to businesses and individuals
    both within and outside the European Union, including providers, importers, distributors,
    authorized representatives, deployers, and users of AI systems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 理解该法案需要了解其范围、适用对象以及AI系统合规的既定时间表。欧盟AI法案是一个基于风险的全面法律框架，为AI系统和GPAI模型的开发、分销和使用制定了规则。它适用于欧盟内外企业和个人，包括AI系统提供商、进口商、分销商、授权代表、部署者和用户。
- en: The Act classifies AI systems into four categories based on risk and establishes
    corresponding obligations. It prohibits AI systems that pose unacceptable risks,
    imposes strict requirements on high-risk systems, and mandates transparency and
    labeling for limited-risk systems. The Act also lays out a specific set of obligations
    for GPAI models, with additional rules for models deemed to pose systemic risk.
    These regulations apply not only to the AI systems themselves but to all organizations
    involved in their development and deployment across the AI value chain.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案根据风险将AI系统分为四个类别，并建立相应的义务。它禁止存在不可接受风险的AI系统，对高风险系统提出严格的要求，并要求对低风险系统进行透明度和标签化。该法案还针对GPAI模型制定了具体的义务，对被认为可能造成系统性风险的模型制定了额外规则。这些规定不仅适用于AI系统本身，还适用于所有参与其开发和部署在AI价值链上的组织。
- en: Classifying the AI systems in use in your organization and identifying the corresponding
    compliance requirements will help determine the scope of required data and AI
    governance measures, as well as the necessary AI engineering practices. In the
    next chapter, I’ll discuss the CRISP-ML(Q) structured AI system development process
    and MLOps, which provides technical and organizational best practices for AI system
    operationalization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对您组织中使用的AI系统进行分类并确定相应的合规要求，将有助于确定所需数据和AI治理措施的范围，以及必要的AI工程实践。在下一章中，我将讨论CRISP-ML(Q)结构化AI系统开发流程和MLOps，这些提供了AI系统运营化的技术和组织最佳实践。
- en: Further Reading
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Ali, Sajid, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, José M. Alonso-Moral,
    Roberto Confalonieri, Riccardo Guidotti, et al. “Explainable Artificial Intelligence
    (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.”
    *Information Fusion* 99 (November 2023): 101805\. [*https://oreil.ly/WERsh*](https://oreil.ly/WERsh).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ali, Sajid, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, José M. Alonso-Moral,
    Roberto Confalonieri, Riccardo Guidotti, et al. “可解释人工智能（XAI）：我们所知与实现可信赖人工智能所剩无几。”
    *信息融合* 99 (November 2023): 101805\. [*https://oreil.ly/WERsh*](https://oreil.ly/WERsh).'
- en: 'Braiek, Houssem Ben, and Foutse Khomh. “Machine Learning Robustness: A Primer.”
    arXiv preprint arXiv:2404.00897, May 2024\. [*https://oreil.ly/6iJoo*](https://oreil.ly/6iJoo).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Braiek, Houssem Ben, and Foutse Khomh. “机器学习鲁棒性：入门指南。” arXiv预印本 arXiv:2404.00897，2024年5月\.
    [*https://oreil.ly/6iJoo*](https://oreil.ly/6iJoo).
- en: 'Caton, Simon, and Christian Haas. “Fairness in Machine Learning: A Survey.”
    *ACM Computing Surveys* 56, no. 7 (2024): 166\. [*https://oreil.ly/ykP9I*](https://oreil.ly/ykP9I).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'Caton, Simon, and Christian Haas. “机器学习中的公平性：综述。” *ACM计算调查* 56, no. 7 (2024):
    166\. [*https://oreil.ly/ykP9I*](https://oreil.ly/ykP9I).'
- en: High-Level Expert Group on AI (AI HLEG). *The Assessment List for Trustworthy
    Artificial Intelligence (ALTAI) for Self-Assessment*. European Commission, July
    2020\. [*https://oreil.ly/QNzCW*](https://oreil.ly/QNzCW).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 高级人工智能专家小组（AI HLEG）。*可信赖人工智能评估清单（ALTAI）自我评估指南*。欧洲委员会，2020年7月\. [*https://oreil.ly/QNzCW*](https://oreil.ly/QNzCW).
- en: 'Li, Bo, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and
    Bowen Zhou. “Trustworthy AI: From Principles to Practices.” *ACM Computing Surveys*
    55, no. 9 (2023): 177\. [*https://oreil.ly/sAQ99*](https://oreil.ly/sAQ99).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 'Li, Bo, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and
    Bowen Zhou. “可信赖人工智能：从原则到实践。” *ACM计算调查* 55, no. 9 (2023): 177\. [*https://oreil.ly/sAQ99*](https://oreil.ly/sAQ99).'
- en: Malhotra, Tanya. “Top Artificial Intelligence (AI) Governance Laws and Frameworks.”
    *Marktechpost*, May 2, 2024\. [*https://oreil.ly/OvYAI*](https://oreil.ly/OvYAI).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Malhotra, Tanya. “顶级人工智能（AI）治理法律和框架。” *Marktechpost*，2024年5月2日\. [*https://oreil.ly/OvYAI*](https://oreil.ly/OvYAI).
- en: 'Monteith, Scott, Tasha Glenn, John R. Geddes, Peter C. Whybrow, Eric Achtyes,
    and Micheal Bauer. “Artificial Intelligence and Increasing Misinformation.” *The
    British Journal of Psychiatry* 224, no. 2 (2024): 33–35\. [*https://oreil.ly/KgesS*](https://oreil.ly/KgesS).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 'Monteith, Scott, Tasha Glenn, John R. Geddes, Peter C. Whybrow, Eric Achtyes,
    and Micheal Bauer. “人工智能与虚假信息的增加。” *英国精神病学杂志* 224, no. 2 (2024): 33–35\. [*https://oreil.ly/KgesS*](https://oreil.ly/KgesS).'
- en: 'Novelli, Claudio, Mariarosaria Taddeo, and Luciano Floridi. “Accountability
    in Artificial Intelligence: What It Is and How It Works.” *AI & SOCIETY* 39 (2023):
    1871–82. [*https://oreil.ly/yEJ1b*](https://oreil.ly/yEJ1b).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 'Novelli, Claudio, Mariarosaria Taddeo, and Luciano Floridi. “人工智能中的问责制：它是什么以及它是如何运作的。”
    *人工智能与社会* 39 (2023): 1871–82. [*https://oreil.ly/yEJ1b*](https://oreil.ly/yEJ1b).'
- en: 'Radclyffe, Charless, Mafalda Ribeiro, and Robert H. Wortham. “The Assessment
    List for Trustworthy Artificial Intelligence: A Review and Recommendations.” *Frontiers
    in Artificial Intelligence* 6 (2023): 1020592\. [*https://oreil.ly/bmSIN*](https://oreil.ly/bmSIN).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 'Radclyffe, Charless, Mafalda Ribeiro, and Robert H. Wortham. “可信赖人工智能评估清单：综述和建议。”
    *人工智能前沿* 6 (2023): 1020592\. [*https://oreil.ly/bmSIN*](https://oreil.ly/bmSIN).'
- en: 'Reinsel, David, John Gantz, and John Rydning. *Data Age 2025: The Evolution
    of Data to Life-Critical*. Seagate, April 2017\. [*https://oreil.ly/PYxM2*](https://oreil.ly/PYxM2).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Reinsel, David, John Gantz, and John Rydning. *数据时代2025：数据向生命关键性的演变*。希捷，2017年4月\.
    [*https://oreil.ly/PYxM2*](https://oreil.ly/PYxM2).
- en: 'Repetto, Marco. “Fostering Robust AI: Understanding Its Importance and Navigating
    the EU Artificial Intelligence Act.” CertX, June 12, 2023\. [*https://oreil.ly/U2WUK*](https://oreil.ly/U2WUK).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Repetto, Marco. “培养鲁棒人工智能：理解其重要性并导航欧盟人工智能法案。” CertX，2023年6月12日\. [*https://oreil.ly/U2WUK*](https://oreil.ly/U2WUK).
- en: 'Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” *Communications
    of the ACM* 63, no. 12 (2020): 54–63\. [*https://oreil.ly/S7AJT*](https://oreil.ly/S7AJT).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 'Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “绿色人工智能。” *ACM通讯*
    63, no. 12 (2020): 54–63\. [*https://oreil.ly/S7AJT*](https://oreil.ly/S7AJT).'
- en: 'Studer, Stefan, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig
    Winkler, Steven Peters, and Klaus-Robert Mueller. “Towards CRISP-ML(Q): A Machine
    Learning Process Model with Quality Assurance Methodology.” arXiv preprint arXiv:2003.05155,
    February 2021\. [*https://oreil.ly/srSRa*](https://oreil.ly/srSRa).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Studer, Stefan, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig
    Winkler, Steven Peters, and Klaus-Robert Mueller. “迈向CRISP-ML(Q)：一个带有质量保证方法的机器学习流程模型。”
    arXiv预印本 arXiv:2003.05155，2021年2月\. [*https://oreil.ly/srSRa*](https://oreil.ly/srSRa).
- en: Turri, Violet. “What Is Explainable AI?” *SEI Blog* (Carnegie Mellon University
    Software Engineering Institute), January 17, 2022\. [*https://oreil.ly/pciy_*](https://oreil.ly/pciy_).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Turri, Violet. “什么是可解释人工智能？” *SEI 博客* (卡内基梅隆大学软件工程研究所), 2022年1月17日\. [*https://oreil.ly/pciy_*](https://oreil.ly/pciy_).
- en: 'Wu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
    Kiwan Maeng, Gloria Chang, et al. “Sustainable AI: Environmental Implications,
    Challenges and Opportunities.” arXiv preprint arXiv:2111.00364, January 2022\.
    [*https://oreil.ly/vplRJ*](https://oreil.ly/vplRJ).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Wu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
    Kiwan Maeng, Gloria Chang, 等人. “可持续人工智能：环境影响、挑战与机遇。” arXiv 预印本 arXiv:2111.00364,
    2022年1月\. [*https://oreil.ly/vplRJ*](https://oreil.ly/vplRJ).
- en: ^([1](ch01.html#id333-marker)) For further reading on this and other topics
    introduced in this chapter, see the list of references at the end of the chapter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#id333-marker)) 关于本章介绍的其他主题的进一步阅读，请参阅章节末尾的参考文献列表。
