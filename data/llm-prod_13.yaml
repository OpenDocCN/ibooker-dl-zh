- en: appendix B Reinforcement learning with human feedback
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B：带有人类反馈的强化学习
- en: Reinforcement learning with human feedback (RLHF) is a variation of traditional
    reinforcement learning (RL), which typically involves solving the k-armed bandit
    problem. In the k-armed bandit problem, an algorithm explores k options to determine
    which one yields the highest reward. However, RLHF takes a different approach.
    Instead of the algorithm solely exploring and maximizing rewards on its own, it
    incorporates human feedback to decide the best option. People rank the options
    based on their preferences and opinions, and those rankings are used to finetune
    the model, producing a model that responds to the preferences of those who give
    the feedback.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 带有人类反馈的强化学习（RLHF）是传统强化学习（RL）的一种变体，通常涉及解决k臂老虎机问题。在k臂老虎机问题中，算法探索k个选项以确定哪个能产生最高的奖励。然而，RLHF采取了一种不同的方法。不是算法完全独立探索并最大化奖励，而是结合人类反馈来决定最佳选项。人们根据他们的偏好和观点对选项进行排名，这些排名被用来微调模型，产生一个能够响应提供反馈的人的偏好的模型。
- en: In listing B.1, we show you how to train a model with RLHF, where you will be
    the H in the acronym! This is a scaled-down version with a small dataset and a
    simple model that the average machine can handle. Starting with the imports, you
    should be familiar with most of these by now, but we want to draw attention to
    one of the more unique ones, namely `trl`, which stands for “transformers reinforcement
    learning.” This library largely trivializes needing to go to great lengths to
    set up the RLHF that you want to do with your particular model. It also integrates
    very well with the Hugging Face ecosystem, including Accelerate and PEFT (Parameter-Efficient
    Fine-Tuning) if you want to RLHF LoRAs for different tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表B.1中，我们向您展示如何使用RLHF训练一个模型，其中你将是这个缩写词中的H！这是一个缩小版的版本，包含小数据集和简单模型，普通机器可以处理。从导入开始，你现在应该熟悉其中大部分，但我们想特别指出其中一个比较独特的地方，即`trl`，它代表“transformers
    reinforcement learning”。这个库在很大程度上简化了设置你想要用特定模型进行的RLHF的复杂过程。它还与Hugging Face生态系统集成得非常好，包括Accelerate和PEFT（参数高效微调），如果你想要为不同任务进行RLHF
    LoRAs。
- en: Listing B.1 Example RLHF training
  id: totrans-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表B.1示例RLHF训练
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we’re going to pull a dataset to train on. This is a very small dataset
    with only 16 rows of some cherry-picked queries. We won’t be able to really tune
    in any model off of such a small dataset, but we aren’t too concerned; we’re really
    just going through the motions right now to get a feel for how to do RLHF:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将拉取一个数据集进行训练。这是一个非常小的数据集，只有16行精心挑选的查询。我们无法从如此小的数据集中真正调整任何模型，但我们并不太关心；我们真正只是走走过场，以了解如何进行RLHF：
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output is
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE2]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll load in our model. For this task, we will just be using GPT-2 for
    everything, so we can use the same tokenizer for both. As you can see, loading
    models is an absolute breeze with `trl` because it uses the exact same API as
    everything else in Hugging Face. As a note, GPT-2 doesn’t have a `pad_token,`
    so we’ll give it one:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载我们的模型。对于这个任务，我们将使用GPT-2来完成所有事情，因此我们可以使用相同的分词器。正如你所见，使用`trl`加载模型非常简单，因为它使用与Hugging
    Face中所有其他内容完全相同的API。作为备注，GPT-2没有`pad_token`，所以我们将给它一个：
- en: '[PRE3]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For this task, we will be using proximal policy optimization (PPO), which is
    a very popular optimization algorithm for reinforcement learning tasks. We’re
    setting the `batch_size` to 1 since we are going to be giving the human feedback
    in real time. We’ll also define some parameters for text generation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用近端策略优化（PPO），这是强化学习任务中非常流行的优化算法。我们将`batch_size`设置为1，因为我们将在实时中提供人类反馈。我们还将定义一些用于文本生成的参数：
- en: '[PRE4]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we are ready to train our model! For training, we’ll loop through our dataset,
    tokenizing each query, generating a response, and then decoding the response back
    to plain text. From here, we’ll send the query and response to the terminal to
    be evaluated by you, a human, using the `input` function. You can respond to the
    prompt with an integer to give it a reward. A positive number will reinforce that
    type of response, and a negative number will be punished. Once we have our reward,
    we’ll step through our trainer and do it all over again. Lastly, we’ll save our
    model when we are done:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练我们的模型了！对于训练，我们将遍历我们的数据集，对每个查询进行标记化，生成一个响应，然后将响应解码回普通文本。从这里，我们将查询和响应发送到终端，由你，一个人类，使用`input`函数进行评估。你可以用一个整数来回应提示以给予它奖励。正数将加强那种类型的响应，而负数将被惩罚。一旦我们有了奖励，我们将通过我们的训练器再次进行操作。最后，当我们完成时，我们将保存我们的模型：
- en: '[PRE5]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Gets response from model'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从模型获取响应'
- en: '#2 Gets reward score from the user'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从用户获取奖励分数'
- en: '#3 Runs PPO step'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 运行PPO步骤'
- en: '#4 Saves model'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 保存模型'
- en: While this works for demonstration purposes, this isn’t how you’ll run RLHF
    for production workloads. Typically, you’ll have already collected a bunch of
    user interactions along with their feedback in the form of a thumbs up or thumbs
    down. Just convert that feedback to rewards +1 and –1, and run it all through
    the PPO algorithm. Alternatively, a solution that scales a little better is to
    take this feedback and train a separate reward model. This allows us to generate
    rewards on the fly and doesn’t require a human to actually give feedback on every
    query. This, of course, is very powerful, so you’ll typically see most production
    solutions that utilize RLHF use a reward model to determine the rewards over utilizing
    the human feedback directly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这适用于演示目的，但这并不是你将用于生产工作负载的RLHF运行方式。通常，你已经在用户交互中收集了大量数据，以及他们以点赞或点踩形式提供的反馈。只需将这种反馈转换为奖励+1和-1，然后通过PPO算法运行所有这些。或者，一个稍微好一点的解决方案是，将这种反馈用于训练一个单独的奖励模型。这允许我们即时生成奖励，并且不需要人类对每个查询实际提供反馈。当然，这非常强大，所以你通常会看到大多数利用RLHF的生产解决方案都使用奖励模型来确定奖励，而不是直接使用人类反馈。
- en: If this example piques your interest, we highly recommend checking out other
    examples and docs for the trl library, which you can find at [https://github.com/huggingface/trl](https://github.com/huggingface/trl).
    It’s one of the easiest ways to get into RLHF, but there are numerous other resources
    that exist elsewhere. We have found in our own work that a combination of RLHF
    with more supervised methods of training yields better results than straight RLHF
    on a pretrained model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个例子激起了你的兴趣，我们强烈推荐查看trl库的其他示例和文档，你可以在[https://github.com/huggingface/trl](https://github.com/huggingface/trl)找到它们。这是进入强化学习与人类反馈（RLHF）的最简单方法之一，但还有许多其他资源存在于其他地方。我们在自己的工作中发现，将RLHF与更多的监督训练方法相结合，比在预训练模型上直接使用RLHF能产生更好的结果。
