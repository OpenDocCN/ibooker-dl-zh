- en: Chapter 2\. How AI Coding Technology Works
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章\. AI编码技术是如何工作的
- en: In this chapter, we’ll crack open the hood of AI-assisted programming tools
    and take a peek at what makes them tick. We’ll briefly wade through the history,
    take a whirl with transformer models and LLMs, and demo the OpenAI Playground.
    Then we’ll get some advice on how to evaluate LLMs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将揭开AI辅助编程工具的面纱，看看是什么让它们运转。我们将简要回顾历史，体验transformer模型和LLMs，并演示OpenAI游乐场。然后，我们将提供有关如何评估LLMs的建议。
- en: Grasping what this powerful technology can and can’t do will pave the way for
    smarter use of AI-assisted programming tools for real-world software projects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这项强大技术能做什么和不能做什么将为在现实世界的软件项目中更智能地使用AI辅助编程工具铺平道路。
- en: Key Features
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键特性
- en: The market has been buzzing about AI-assisted programming tools such as GitHub
    Copilot, Tabnine, CodiumAI, and Amazon CodeWhisperer. The makers of each product
    attempt to flaunt their own set of bells and whistles. But there’s a good chunk
    of capabilities these tools share. [Table 2-1](#common_functions_of_aihyphenassisted_pro)
    summarizes some of the main features.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上关于GitHub Copilot、Tabnine、CodiumAI和Amazon CodeWhisperer等AI辅助编程工具的讨论不绝于耳。每个产品的制作者都试图展示他们自己的一套特色功能。但这些工具共享了许多功能。[表格2-1](#common_functions_of_aihyphenassisted_pro)总结了其中的一些主要功能。
- en: Table 2-1\. Common functions of AI-assisted programming tools
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 表格2-1\. AI辅助编程工具的常见功能
- en: '| Feature | Description |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 描述 |'
- en: '| --- | --- |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Code suggestions | Provides code suggestions based on comments and file context;
    recommends individual lines or whole functions. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 代码建议 | 根据注释和文件上下文提供代码建议；推荐单个行或整个函数。 |'
- en: '| Context-aware completions | Offers context-aware code completions based on
    all or a part of the code base, as well as suggestions to aid in coding. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 上下文感知补全 | 基于整个或部分代码库提供上下文感知的代码补全，以及辅助编码的建议。 |'
- en: '| Test generation | Analyzes code to generate meaningful tests, map code behaviors,
    and surface edge cases to ensure software reliability before shipping. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 测试生成 | 分析代码以生成有意义的测试，映射代码行为，并在发货前暴露边缘情况，以确保软件可靠性。 |'
- en: '| User–IDE interaction | Automatically activates and provides guidance as users
    type code in the IDE; users can interact with the code through chat. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 用户-IDE交互 | 当用户在IDE中输入代码时自动激活并提供指导；用户可以通过聊天与代码交互。 |'
- en: '| Code analysis | Analyzes code snippets, docstrings, and comments to provide
    reliable code predictions and tag suspicious code. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 代码分析 | 分析代码片段、文档字符串和注释，以提供可靠的代码预测并标记可疑代码。 |'
- en: '| Bug detection and fixing | Identifies potential bugs in code and suggests
    ways to fix them. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 错误检测和修复 | 识别代码中的潜在错误并建议修复方法。 |'
- en: '| Code autodocumentation | Automatically adds docstrings and enhances code
    documentation. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 代码自动文档化 | 自动添加文档字符串并增强代码文档。 |'
- en: '| Routine task automation | Helps with code creation for routine or time-consuming
    tasks, unfamiliar APIs or SDKs, and other common coding scenarios like file operations
    and image processing. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 常规任务自动化 | 帮助创建常规或耗时任务、不熟悉的API或SDK以及其他常见编码场景（如文件操作和图像处理）的代码。 |'
- en: '| API and SDK usage optimization | Aids in making correct and effective use
    of APIs and SDKs. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| API和SDK使用优化 | 帮助正确有效地使用API和SDK。 |'
- en: '| Open source discovery and attribution | Facilitates discovery and attribution
    of open source code and libraries. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 开源代码发现和归属 | 促进开源代码和库的发现和归属。 |'
- en: The list in [Table 2-1](#common_functions_of_aihyphenassisted_pro) isn’t the
    be-all and end-all; innovation has been moving at a rapid clip. Clearly, these
    systems can give developers a big leg up, in large part by providing code suggestions
    and context-aware completions. We’ll take a closer look at these in the next section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表格2-1中的列表并非尽善尽美；创新正在以快速的速度发展。显然，这些系统可以为开发者提供很大的帮助，在很大程度上是通过提供代码建议和上下文感知的补全。我们将在下一节中更详细地探讨这些内容。
- en: Code Suggestions and Context-Aware Completions Versus Smart Code Completion
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码建议和上下文感知补全与智能代码补全
- en: The magic of *smart code completion*, also known as autocompletion or Microsoft’s
    term IntelliSense, is something many IDEs bring to the table. They lend developers
    a hand by suggesting, filling in, and spotlighting bits of code as the humans
    hammer away at the keyboard. This technology has actually been around since the
    late 1950s with the inception of spellcheckers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*智能代码补全*的魔力，也称为自动补全或微软的术语IntelliSense，是许多IDE提供的一项功能。它们通过建议、填充和突出显示代码片段来帮助开发者，当人类在键盘上敲击时。这项技术实际上自1950年代末拼写检查器的出现以来就已经存在。'
- en: The breakthrough came in the mid-1990s. Microsoft’s Microsoft Visual Basic 5.0
    provided real-time suggestions and completions, with an emphasis on basic syntax
    and function signatures. This greatly improved productivity and reduced errors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 突破发生在1990年代中期。微软的Microsoft Visual Basic 5.0提供了实时建议和补全功能，重点在于基本语法和函数签名。这大大提高了生产率并减少了错误。
- en: 'So you might be wondering: How does something like IntelliSense stack up against
    AI-assisted programming tools? After all, IntelliSense has a smattering of AI
    and machine learning under its belt.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可能想知道：像IntelliSense这样的东西与AI辅助编程工具相比如何？毕竟，IntelliSense在其背后包含了一些AI和机器学习的元素。
- en: Yet, there’s an important distinction to be made. AI-assisted tools are powered
    by generative AI. They serve up not just code but a buffet of documentation, planning
    documents, and helpful guides among other things. Thanks to generative AI, these
    tools get the knack of churning out, tweaking, and understanding human-like text
    based on the given context, making them champs at translation, summarization,
    text analytics, topic modeling, and answering queries. Engaging with these tools
    can sometimes be like having a casual chat with your code. With an LLM at their
    core, they can catch the drift of the context and intent from your input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个重要的区别。AI辅助工具由生成式AI驱动。它们不仅提供代码，还提供文档、规划文档和有用的指南等多种内容。多亏了生成式AI，这些工具掌握了根据给定上下文生成、调整和理解类似人类文本的技巧，使它们在翻译、摘要、文本分析、主题建模和回答查询方面成为佼佼者。与这些工具互动有时就像与你的代码进行闲聊一样。由于它们的核心是LLM，它们可以捕捉到你的输入中的上下文和意图。
- en: Compilers Versus AI-Assisted Programming Tools
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译器与AI辅助编程工具的比较
- en: 'To get a better understanding of AI-assisted programming tools, it helps to
    understand what compilers do. Here are the main steps that a compiler performs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解AI辅助编程工具，了解编译器的作用很有帮助。以下是编译器执行的主要步骤：
- en: Lexical analysis (tokenization)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 词法分析（标记化）
- en: The compiler acts like a language teacher, breaking your code into tokens.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器的作用就像是一位语言老师，将你的代码分解成标记。
- en: Syntax analysis
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 语法分析
- en: Here, the compiler checks how your tokens are grouped. It makes sure your coding
    has the right structure, not just the right commands.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，编译器检查你的标记是如何分组的。它确保你的编码结构正确，而不仅仅是命令正确。
- en: Semantic analysis (error checks)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分析（错误检查）
- en: The compiler ensures that your code makes sense in the context of the programming
    language. It’s not just about correct syntax. It’s about correct meaning, too.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器确保你的代码在编程语言的上下文中是有意义的。这不仅仅是关于正确的语法。还关于正确的意义。
- en: Intermediate code generation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 中间代码生成
- en: This is where your code starts its transformation journey. The compiler translates
    your high-level code into an intermediate form. It’s not quite machine language
    yet, but it’s getting there.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你代码开始其转换之旅的地方。编译器将你的高级代码转换为中间形式。这还不是机器语言，但正在朝这个方向发展。
- en: Code optimization
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代码优化
- en: In this step, the compiler is like a personal trainer for your code, making
    it leaner and more efficient. It tweaks the intermediate code to run faster and
    take up less space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，编译器就像是你代码的个人教练，使其更加精简和高效。它调整中间代码以使其运行更快并占用更少的空间。
- en: Code generation
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成
- en: This is the final transformation. The compiler converts the optimized intermediate
    code into machine code or assembly language that your CPU can understand.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终的转换。编译器将优化的中间代码转换为CPU可以理解的机器代码或汇编语言。
- en: 'Linking and loading:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 链接和加载：
- en: Sometimes considered a part of the compilation process, *linking* involves combining
    various pieces of code and libraries into a single executable program. *Loading*
    is the process of placing the program into memory for execution.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时被认为是编译过程的一部分，*链接*涉及将各种代码和库组合成一个单一的可执行程序。*加载*是将程序放入内存以执行的过程。
- en: As for AI-assisted programming tools like Copilot, they’re a different beast.
    They don’t really “get” programming languages like compilers do. This is fine.
    The compiler does this. Instead, they use AI to guess and suggest bits of code
    based on tons of code that’s already out there. Since the tools are playing the
    odds, the suggestions can vary a lot. The compiler will then take this code and
    make it so the machine can run the program.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 至于像Copilot这样的AI辅助编程工具，它们是另一种生物。它们并不真正“理解”编程语言，就像编译器那样。这没关系。编译器会这样做。相反，它们使用AI来猜测并基于大量已有的代码建议代码片段。由于这些工具是在玩概率，因此建议可能会有很大差异。然后编译器将这段代码转换成机器可以运行的程序。
- en: Sometimes, AI tools might miss something simple like a bracket, which a human
    coder or a compiler would spot in a heartbeat. That’s because the LLMs are based
    on predicting patterns, not a compiler engine. If something’s not common in the
    training, they might not catch it. Also, these tools might get fancy and suggest
    complex code based on the situation. Yes, AI-assisted programming tools can get
    carried away.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，AI工具可能会错过一些简单的东西，比如括号，而人类程序员或编译器会立刻发现。这是因为LLMs基于预测模式，而不是编译器引擎。如果训练中不常见，它们可能不会捕捉到。此外，这些工具可能会变得复杂，并根据情况建议复杂的代码。是的，AI辅助编程工具可能会走火入魔。
- en: When it comes to spotting errors, AI-assisted programming tools are generally
    effective but still do not quite match up to a compiler’s ninja-like error-checking
    skills. Yet the tools are still powerful. For example, they can help catch pesky
    syntax errors—missing semicolons, typos in function names, mismatched brackets—and
    swiftly suggest the right fix. They also shine in steering you clear of common
    coding pitfalls. Whether it’s reminding you to properly close a file after opening
    it or suggesting more efficient ways to loop through an array, this tool has your
    back. And when it comes to logical errors, AI-assisted programming tools can be
    surprisingly insightful. They may not solve every complex problem, but they can
    often propose alternative approaches or solutions you might not have considered,
    nudging your problem-solving journey in the right direction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到发现错误时，AI辅助编程工具通常很有效，但仍然无法与编译器的忍者般的错误检查技能相媲美。然而，这些工具仍然很强大。例如，它们可以帮助捕捉讨厌的语法错误——缺少分号、函数名中的拼写错误、不匹配的括号——并迅速提出正确的修复建议。它们在引导你避开常见的编码陷阱方面也表现出色。无论是提醒你在打开文件后正确关闭文件，还是建议更高效地遍历数组的方法，这个工具都会为你提供支持。至于逻辑错误，AI辅助编程工具可能会出人意料地有洞察力。它们可能无法解决每个复杂问题，但它们经常可以提出你未曾考虑过的替代方法或解决方案，引导你的问题解决之旅走向正确的方向。
- en: This all means that while AI tools are helpful for making coding smoother, they’re
    not a replacement for the thorough checks a compiler does or the keen eye of a
    human coder.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切意味着，尽管人工智能工具有助于使编码更加流畅，但它们并不能取代编译器所做的彻底检查或人类程序员敏锐的视角。
- en: These drawbacks really underline how crucial it is to blend the smarts of AI-assisted
    tools with the thoroughness of compiler checks and a human touch. After all, you
    want to make sure your code is not just good but spot-on accurate and correct.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点实际上突显了将人工智能辅助工具的智慧与编译器检查的彻底性和人文关怀相结合是多么关键。毕竟，你希望确保你的代码不仅质量好，而且精确无误。
- en: Levels of Capability
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 能力层级
- en: In October 2023, Quinn Slack, the CEO and cofounder of Sourcegraph, shared an
    insightful [blog post](https://oreil.ly/SoDqc). He dived into the world of AI-assisted
    programming tools like Copilot and came up with an interesting way to think about
    them, which he called “levels of code AI.” His step-by-step framework makes it
    easier for everyone to get what these AI tools can do and check if the boastful
    claims by the companies selling them actually hold water. [Figure 2-1](#programming_systems_have_different_level)
    shows the levels of code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年10月，Sourcegraph的首席执行官和联合创始人Quinn Slack分享了一篇有见地的[博客文章](https://oreil.ly/SoDqc)。他深入探讨了像Copilot这样的AI辅助编程工具的世界，并提出了一种有趣的方式来思考它们，他称之为“代码AI的层级”。他的逐步框架使每个人都能更容易地了解这些AI工具能做什么，并检查销售这些工具的公司所宣称的夸大其词是否站得住脚。[图2-1](#programming_systems_have_different_level)展示了代码的层级。
- en: The first three levels focus on human-led coding, where the developer is the
    main player. Starting off, Level 0 is where there is no AI assistance, which is
    old-school coding. Developers do everything by hand with no AI in sight. It’s
    the baseline that sets the stage for AI to step in later on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个级别侧重于以人为中心的编码，其中开发者是主要角色。一开始，第0级是没有AI辅助的阶段，这是传统的编码方式。开发者手动完成所有工作，没有AI的参与。这是为AI后来介入设定的基础。
- en: Then there’s Level 1, code completion. This is where AI starts to chip in and
    helps to whip up single lines or chunks of code based on what’s going on around
    it. At this stage, the developer is still in the driver’s seat, directing the
    overall program and using AI as a shortcut for typical coding tasks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是第1级，代码补全。在这个阶段，AI开始发挥作用，根据周围环境帮助生成单行代码或代码块。在这个阶段，开发者仍然是主导者，指导整体程序，并将AI作为典型编码任务的捷径。
- en: '![](assets/aiap_0201.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiap_0201.png)'
- en: Figure 2-1\. Programming systems have different levels of AI capability
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 编程系统具有不同级别的AI能力
- en: Level 2, code creation, ramps up the AI. Here, it gets more hands-on and crafts
    longer code sections. The AI can, for example, design APIs and even fix existing
    code. Of course, it’s all happening with a human keeping an eye on things. This
    level needs the AI to get the codebase and the context around it so it can come
    up with code that’s not just correct but also fits in nicely.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第2级，代码创建，增加了AI的参与。在这里，它更加亲自动手，编写更长的代码段。例如，AI可以设计API甚至修复现有代码。当然，这一切都是在人类的监督下进行的。这个级别需要AI了解代码库及其上下文，以便提出不仅正确而且与现有代码很好地融合的代码。
- en: Starting with Level 3, supervised automation, we see a shift toward AI taking
    the lead in coding. In this stage, the AI tackles several tasks to meet broader
    goals set by humans, and it doesn’t need a check-in at every turn. Working at
    this level is like delegating work to a junior developer. The AI at this level
    is savvy enough to sort out bugs, toss in new features, and mesh systems together,
    reaching out to its human counterpart for any clarifications along the way.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从第3级开始，监督自动化，我们看到AI在编码中开始占据主导地位。在这个阶段，AI完成多项任务以满足人类设定的更广泛目标，并且不需要在每个转折点进行检查。在这个级别工作就像委托给初级开发者一样。这个级别的AI足够聪明，可以解决错误、添加新功能，并将系统结合起来，在过程中向人类同事寻求任何澄清。
- en: At Level 4, full automation, the AI really steps up its game. Here, it handles
    complex tasks all on its own, without needing humans to give the final thumbs-up
    on the code. Imagine the trust you’d have in a top-notch engineer if you were
    a CEO or product manager. This is the kind of relationship this level aims for.
    The AI isn’t just reacting. It’s proactively keeping an eye on the code, spotting
    and sorting out issues as they come up.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到了第4级，实现全面自动化，AI真正提升了它的表现。在这里，它能够独立处理复杂任务，无需人类对代码进行最终确认。想象一下，如果你是首席执行官或产品经理，你会对顶级工程师有多少信任。这个级别正是追求这种关系的。AI不仅仅是反应，它主动关注代码，及时发现并解决问题。
- en: Finally, there’s Level 5, AI-led full autonomy. This level is a whole different
    ball game, where AI isn’t just following human instructions but setting its own
    objectives. It’s about AI working off a core reward function. Think of it as playing
    its own game in a world where it faces off against other agents. Sure, this level
    sounds a bit like sci-fi, but given how fast things are moving, it’s not too wild
    to think we might see this level become a reality in our lifetimes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是第5级，AI主导的完全自主。这个级别完全不同，AI不仅仅是遵循人类指令，而是设定自己的目标。这是关于AI基于核心奖励函数工作的。想象一下，在一个它与其他代理对抗的世界里，AI在玩自己的游戏。当然，这个级别听起来有点像科幻，但鉴于发展速度如此之快，我们在这个时代看到这一级别成为现实并不太离谱。
- en: 'Right now, tools like Copilot are hovering around Level 3, give or take. Pinning
    down the exact level can be tricky, but Quinn Slack’s framework does a pretty
    solid job of making sense of the technology and its key interactions. And one
    thing’s for sure: the technology isn’t slowing down—it’s moving forward really
    fast.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，像Copilot这样的工具大致处于第3级，上下浮动。精确确定级别可能有些棘手，但Quinn Slack的框架在理解这项技术和其关键交互方面做得相当不错。有一点是肯定的：这项技术并没有放缓脚步——它正在快速发展。
- en: Generative AI and Large Language Models (LLMs)
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI和大型语言模型（LLMs）
- en: Using AI-assisted programming tools doesn’t require you to be a whiz at the
    nitty-gritty of generative AI technology. However, having a bird’s-eye view of
    the technology can be quite handy. You’ll be able to evaluate the responses, capabilities,
    and limitations of these tools in a sharper way.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人工智能辅助编程工具不需要你对生成式人工智能技术的细节了如指掌。然而，对这项技术的全局了解可以非常有帮助。你将能够更敏锐地评估这些工具的响应、功能和局限性。
- en: '*Transparency* isn’t just a buzzword here. For a new technology to really catch
    on, having a clear picture of what’s under the hood is crucial. Adoption is all
    about trust. In the coding world, reliability and accountability aren’t just fancy
    extras—they’re the bread and butter.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*透明度*不仅仅是一个流行词。对于一项新技术要真正流行起来，了解其内部运作至关重要。采用率关乎信任。在编码世界中，可靠性和问责制不仅仅是花哨的额外功能——它们是基础。'
- en: As we venture into the upcoming sections, we’ll skim the surface of generative
    AI and LLMs to give you a clearer picture.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入接下来的部分，我们将简要介绍生成式人工智能和大型语言模型，以便给你一个更清晰的画面。
- en: Evolution
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进化
- en: The story of generative AI has its roots stretching back several decades, with
    one of its earliest examples being ELIZA, the pioneering chatbot brought to life
    by Massachusetts Institute of Technology professor Joseph Weizenbaum in the mid-60s.
    ELIZA was crafted to mimic chats with a psychotherapist ([you can still find it
    online](https://oreil.ly/MbLP8)). Sure, it was basic, running on a rule-based
    algorithm and mostly parroting back user input.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能的故事可以追溯到几十年前，其中一个最早的例子是20世纪60年代中期由麻省理工学院教授约瑟夫·魏岑鲍姆（Joseph Weizenbaum）带来的先驱聊天机器人ELIZA。ELIZA被设计成模仿与心理治疗师聊天（[你仍然可以在网上找到它](https://oreil.ly/MbLP8)）。当然，它很简单，基于规则算法运行，主要只是重复用户输入。
- en: Yet many folks found ELIZA more pleasant to chat with than a real therapist,
    and some were even fooled into thinking they were communicating with a human.
    This curious occurrence, dubbed the “ELIZA effect,” showcased how easily people
    can imagine human-like understanding on the part of a computer program.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多人发现与ELIZA聊天比与真正的治疗师聊天更愉快，有些人甚至被骗以为他们正在与人类交流。这种奇特的现象被称为“ELIZA效应”，展示了人们如何轻易地想象计算机程序具有类似人类的理解能力。
- en: 'However, the journey of generative AI wasn’t exactly a sprint. The tech gears
    at its core were quite basic, and progress was more of a slow crawl. But come
    the 2010s, the scene hit a turning point. The technology world was now boasting
    hefty compute power, flashy hardware systems like GPUs (graphics processing units),
    a treasure trove of data, and the fine-tuning of sophisticated models like deep
    learning. And just like that, generative AI was back in the fast lane. As it developed,
    different methods emerged:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生成式人工智能的旅程并不完全是短跑。其核心的技术齿轮相当基础，进步更多的是一种缓慢的爬行。但到了2010年代，情况发生了转折点。技术世界现在拥有强大的计算能力，像GPU（图形处理单元）这样的闪亮硬件系统，大量数据，以及深度学习等复杂模型的微调。就这样，生成式人工智能回到了快车道。随着其发展，出现了不同的方法：
- en: Variational autoencoders (VAEs)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）
- en: This technology made its debut in 2013, thanks to Diederik P. Kingma and Max
    Welling and their paper [“Auto-Encoding Variational Bayes”](https://arxiv.org/abs/1312.6114).
    Their VAE model consists of lower-dimensional latent space from more complex,
    higher-dimensional data, all without supervision. It also includes an encoder–decoder
    structure. When we say *higher-dimensional data*, we’re talking about data with
    many features, each being a dimension—think of a 28 × 28 pixel image in a 784-dimension
    space. The lower-dimensional latent space is like a compact version of this data,
    holding onto the crucial information while shedding the extra dimensions. This
    is important because it lightens the computational load, fights off the curse
    of dimensionality, and makes the data easier to visualize and interpret. This
    leap from a higher- to a lower-dimensional space is called *dimensionality reduction*,
    and it simplifies the data to its bare essentials. Unlike their cousins, the traditional
    autoencoders, that spit out a single value for each latent attribute, the encoder
    in a VAE gives you a probability distribution. The decoder then picks samples
    from this distribution to rebuild the data. This neat trick of offering a range
    of data in the latent space rather than a single value opens the door to create
    new data or images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术首次亮相于2013年，归功于Diederik P. Kingma和Max Welling以及他们发表的论文[“Auto-Encoding Variational
    Bayes”](https://arxiv.org/abs/1312.6114)。他们的VAE模型由从更复杂、更高维度的数据中提取的低维潜在空间组成，这一切都无需监督。它还包括一个编码器-解码器结构。当我们说*高维数据*时，我们指的是具有许多特征的数据，每个特征都是一个维度——想象一下在784维空间中的一个28
    × 28像素的图像。低维潜在空间就像这种数据的紧凑版本，保留着关键信息，同时摒弃了额外的维度。这一点很重要，因为它减轻了计算负担，抵抗了维度灾难，并使得数据更容易可视化和解释。从高维到低维空间的这种飞跃被称为*降维*，它将数据简化到其基本要素。与它们的表亲，传统的自动编码器不同，后者为每个潜在属性输出一个单一值，VAE中的编码器则提供了一个概率分布。解码器随后从这个分布中选取样本来重建数据。这种在潜在空间中提供一系列数据而不是单一值的小技巧，为创建新数据或图像打开了大门。
- en: Generative adversarial networks (GANs)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: Introduced by [Ian Goodfellow and his colleagues in 2014](https://arxiv.org/abs/1406.2661),
    generative adversarial networks are a class of AI algorithms used in unsupervised
    machine learning. At the heart of GANs are two neural networks, dubbed the *generator*
    and the *discriminator*, that go head-to-head in a game-like showdown. The generator
    churns out new data nuggets, while the discriminator plays the judge, distinguishing
    the real from the fake data. With each round, the generator ups its game, crafting
    data that’s eerily similar to real instances. This clever setup has swung open
    doors to new possibilities, leading to AI that creates realistic images, voice
    recordings, and a whole lot more.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由Ian Goodfellow及其同事于2014年[提出](https://arxiv.org/abs/1406.2661)，生成对抗网络是一类用于无监督机器学习的AI算法。GANs的核心是两个神经网络，被称为*生成器*和*判别器*，它们在一场类似游戏的对抗中一决高下。生成器不断产出新的数据片段，而判别器扮演着裁判的角色，区分真实数据和伪造数据。每一轮，生成器都会提升其技能，制作出与真实实例惊人相似的数据。这种巧妙的设置打开了通往新可能性的大门，引领了能够创建逼真图像、声音录音以及更多AI的诞生。
- en: These types of generative AI would be important building blocks for the transformer
    model, a real breakthrough that has made the power of LLMs a reality.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这类生成式AI将是Transformer模型的重要构建块，这是一个真正的突破，使得LLMs的力量成为现实。
- en: The Transformer Model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer模型
- en: Before transformers made a splash, the go-to method for natural language processing
    (NLP) was the recurrent neural network (RNN). RNNs were crafted to tackle sequential
    or time-series data. They would keep tabs on a hidden state to remember bits from
    previous steps in a sequence—a handy feature for things like language modeling,
    speech recognition, and sentiment analysis. The RNNs take it step-by-step, processing
    one piece of the sequence at a time, updating their hidden state based on the
    current input and what’s been processed before—hence the term *recurrent*. But
    they hit a snag when faced with long sequences, getting tripped up by the vanishing
    or exploding gradient problem. This made it hard for them to keep track of long-term
    relationships in the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模型引起轰动之前，自然语言处理（NLP）的常用方法是循环神经网络（RNN）。RNNs被设计来处理序列或时间序列数据。它们会跟踪一个隐藏状态来记住序列中之前步骤的片段——这对于语言建模、语音识别和情感分析等任务来说是一个实用的功能。RNNs按步骤进行，一次处理序列中的一段，根据当前输入和之前处理过的内容更新它们的隐藏状态——这就是“循环”一词的由来。但当面对长序列时，它们会遇到梯度消失或爆炸的问题，这使得它们难以追踪数据中的长期关系。
- en: Enter the transformer, flipping the script entirely. Instead of taking the step-by-step
    approach of RNNs, transformers breeze through data in parallel and tap into attention
    mechanisms to keep tabs on relationships between different bits in the input sequence,
    no matter where they’re placed. This switch in the architectural blueprint lets
    transformers handle both short and long sequences with ease. It also sidesteps
    the gradient woes. Plus, their parallel processing capabilities mesh nicely with
    sophisticated chip architectures like graphics processing units (GPUs) or tensor
    processing units (TPUs).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了变换器，完全改变了游戏规则。变换器不是像RNNs那样按步骤处理数据，而是并行地快速处理数据，并利用注意力机制来跟踪输入序列中不同部分之间的关系，无论它们的位置如何。这种架构蓝图上的转变使得变换器能够轻松处理短序列和长序列。它还避免了梯度问题。此外，它们的并行处理能力与图形处理单元（GPUs）或张量处理单元（TPUs）等复杂的芯片架构很好地结合在一起。
- en: Ashish Vaswani and his fellow researchers at Google created the transformer
    and published the core architecture in the pathbreaking paper [“Attention Is All
    You Need”](https://arxiv.org/abs/1706.03762) in 2017\. [Figure 2-2](#the_architecture_of_the_transformer_mode)
    illustrates the main parts of the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Ashish Vaswani和他在谷歌的研究团队创造了变换器，并在2017年发表了开创性的论文[“Attention Is All You Need”](https://arxiv.org/abs/1706.03762)。[图2-2](#the_architecture_of_the_transformer_mode)展示了模型的主要部分。
- en: 'The transformer model is like a brilliant linguist, adept at unraveling the
    intricacies of language. Its magic unfolds in two primary stages: encoding and
    decoding. Each is composed of its own set of layers. During the *encoding* stage,
    the model reads and comprehends the input text similar to how a linguist would
    understand a sentence in a foreign language. Then in the *decoding* stage, the
    model generates a new piece of text or translation based on the understanding
    acquired in the encoding stage, much like a linguist translating that sentence
    into your native language.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型就像一位杰出的语言学家，擅长解开语言的复杂性。它的魔力在两个主要阶段展开：编码和解码。每个阶段都由其自身的层级组成。在*编码*阶段，模型读取并理解输入文本，就像语言学家理解外语中的句子一样。然后在*解码*阶段，模型根据编码阶段获得的理解生成新的文本或翻译，就像语言学家将那个句子翻译成你的母语一样。
- en: At the heart of the transformer is a mechanism called *attention*, which allows
    it to assess the relevance of each word in a sentence to the other words. It assigns
    an attention score to each. For example, take the sentence “The cat sat on the
    mat.” When the model focuses on the word *sat*, the words *cat* and *mat* might
    receive higher attention scores due to their direct relationship to the action
    of sitting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的核心是一个称为*注意力*的机制，它允许模型评估句子中每个词与其它词的相关性。它为每个词分配一个注意力分数。例如，以句子“The cat sat
    on the mat.”为例，当模型关注到单词*sat*时，与坐的动作有直接关系的单词*cat*和*mat*可能会获得更高的注意力分数。
- en: One notable feature of this model is the *self-attention mechanism*. This allows
    it to look at an entire sentence, understand the relationships between words,
    and retain these relationships over long stretches of text. This grants the transformer
    a form of long-term memory by enabling it to focus on all the words or *tokens*
    (whole words or parts of a word) that have appeared so far, thereby understanding
    the broader context.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的一个显著特点是**自注意力机制**。这允许它审视整个句子，理解词语之间的关系，并在长段文本中保持这些关系。这使得变换器通过能够关注到目前为止出现的所有词语或**标记**（整个词语或词语的一部分）来获得一种长期记忆，从而理解更广泛的上下文。
- en: However, despite these capabilities, the transformer initially lacks the ability
    to recognize the order of words in a sentence, which is crucial for understanding
    the meaning. Here, *positional encoding* steps in. It acts like a GPS to provide
    the model with the information about the position of each word within the sentence
    and aids in making sense of clauses like “The cat chases the mouse” versus “The
    mouse chases the cat.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管具有这些能力，变换器最初缺乏识别句子中词语顺序的能力，这对于理解意义至关重要。在这里，**位置编码**步骤介入。它就像一个GPS，为模型提供关于句子中每个词语位置的信息，并有助于理解“猫追逐老鼠”与“老鼠追逐猫”这样的从句。
- en: '![](assets/aiap_0202.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiap_0202.png)'
- en: Figure 2-2\. The architecture of the transformer model is at the heart of LLMs
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2. 变换器模型的架构是LLM的核心
- en: Adding to the sophistication, the transformer employs a *multi-head attention
    mechanism*. Envision the model having multiple pairs of eyes, each pair examining
    the sentence from a unique angle and focusing on different aspects or relationships
    between the words. For instance, one pair might focus on understanding actions,
    another on identifying characters, and yet another on recognizing locations. This
    multi-view approach enables the transformer to grasp a richer understanding of
    the text.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂性上更进一步，变换器采用了**多头注意力机制**。想象一下模型拥有多对眼睛，每一对眼睛从独特的角度审视句子，关注不同的词语方面或关系。例如，一对眼睛可能专注于理解动作，另一对专注于识别人物，还有一对专注于识别地点。这种多视角方法使变换器能够更深入地理解文本。
- en: Furthermore, each stage of the transformer encompasses layers of a *feedforward
    neural network*, a straightforward network that aids in processing relationships
    between words. This further enhances the understanding and generation of text.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，变换器（Transformer）的每个阶段都包含**前馈神经网络**的层，这是一个简单的网络，有助于处理词语之间的关系。这进一步增强了文本理解和生成的能力。
- en: A transformer is in the form of a pretrained model. It has already been trained
    on an enormous amount of data and is ready for use or further fine-tuning. Once
    pretrained, the model can be accessed as an API, allowing for its immediate use
    in various language-processing tasks. Companies or individuals can rapidly integrate
    this model into their systems, such as AI-assisted programming applications. Moreover,
    the pretrained LLM can be further honed to excel in specialized domains, such
    as medical or legal text analysis, by fine-tuning it on domain-specific data.
    This eliminates the need for developing a complex language model from the ground
    up, saving a substantial amount of time, effort, and resources. The pretrained
    model, with its foundational language understanding, acts as a springboard for
    the development of generative AI applications.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是以预训练模型的形式存在的。它已经在大量数据上进行了训练，并准备好使用或进一步微调。一旦预训练完成，该模型可以通过API访问，允许其在各种语言处理任务中立即使用。公司或个人可以快速将此模型集成到他们的系统中，例如AI辅助编程应用。此外，通过在特定领域的数据上进行微调，预训练的大型语言模型（LLM）可以进一步优化，以在医疗或法律文本分析等特定领域表现出色。这消除了从头开始开发复杂语言模型的需求，节省了大量时间、精力和资源。具有基础语言理解的预训练模型，为生成式AI应用的发展提供了一个跳板。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Building and operating an LLM is costly. During early 2023, GitHub Copilot was
    [losing an average of more than $20 a month per user](https://oreil.ly/D2NiB),
    according to the *Wall Street Journal*. In some cases, some users were losing
    the company $80 per month. However, as the infrastructure is scaled for generative
    AI in the coming years, per-user costs should decrease.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和运营一个LLM成本高昂。根据《华尔街日报》的报道，截至2023年初，GitHub Copilot每月每个用户平均亏损超过20美元（[链接](https://oreil.ly/D2NiB)）。在某些情况下，一些用户每月使公司亏损80美元。然而，随着未来几年生成式AI基础设施的扩展，每个用户的成本应该会降低。
- en: The two main types of transformer systems are *generative pretrained transformer*
    (GPT) and *bidirectional encoder representations from transformers* (BERT). GPT
    is a tool from OpenAI that is ideal for creating text, summarizing information,
    and translating languages. It is based on an autoregressive LLM architecture.
    This means that it crafts text by carefully considering each word based on what
    it’s already output, much like a storyteller building a narrative one word at
    a time. Its skills come from being trained on a colossal amount of text data.
    GPT uses the decoder for generating content.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 两种主要的transformer系统是*生成预训练transformer*（GPT）和*双向encoder表示的transformer*（BERT）。GPT是OpenAI的一个工具，非常适合创建文本、总结信息和翻译语言。它基于自回归LLM架构。这意味着它通过仔细考虑每个单词（基于它已经输出的内容）来构建文本，就像讲故事的人逐字构建叙事一样。它的技能来自于在大量文本数据上的训练。GPT使用解码器来生成内容。
- en: BERT, on the other hand, uses an autoencoding approach. This design enables
    it to deeply understand the context of words in a sentence, making it adept at
    deciphering the nuances and meanings of language. Google developed BERT in 2018
    as an open source project. Since then, many variations and enhancements to the
    core model have emerged.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，BERT使用自动编码方法。这种设计使它能够深入理解句子中单词的上下文，使其擅长解读语言的细微差别和含义。Google于2018年开发了BERT作为一个开源项目。从那时起，许多基于核心模型的变体和改进已经出现。
- en: As for AI-assisted programming applications, the main type of transformer model
    is GPT. It has been shown to predict and autocomplete code efficiently, based
    on the context provided by the programmer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AI辅助编程应用，主要的transformer模型类型是GPT。它已被证明能够根据程序员提供的上下文高效地预测和自动补全代码。
- en: OpenAI Playground
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI游乐场
- en: The [OpenAI Playground](https://platform.openai.com) is a generative AI sandbox
    that provides access to various models developed by OpenAI. It allows for model
    customization via an intuitive graphical interface.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI游乐场](https://platform.openai.com)是一个生成式AI沙盒，提供了访问OpenAI开发的多种模型。它允许通过直观的图形界面进行模型定制。'
- en: The OpenAI Playground makes it easier to understand the strengths and weaknesses
    of the various LLMs. Moreover, it enables real-time testing and adjustments of
    models in response to different inputs, like temperature.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的游乐场使得理解各种大型语言模型（LLM）的优缺点变得更加容易。此外，它还允许根据不同的输入，如温度，对模型进行实时测试和调整。
- en: However, OpenAI charges for use of the platform. Fees are based on the number
    of tokens used, as seen in [Table 2-2](#the_costs_of_openai_llms). Keep in mind
    that prices change periodically. The good news is that all changes as of this
    writing have been reductions in price.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OpenAI对平台的使用收费。费用基于使用的token数量，如[表2-2](#the_costs_of_openai_llms)所示。请注意，价格会定期变动。好消息是，截至本文撰写时，所有变动都是价格下降。
- en: Table 2-2\. The costs of OpenAI LLMs
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2\. OpenAI LLM的成本
- en: '| Model | Input | Output |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 输入 | 输出 |'
- en: '| --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4/8K context | $0.03/1K tokens | $0.06/1K tokens |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4/8K上下文 | $0.03/1K个token | $0.06/1K个token |'
- en: '| GPT-4/32K context | $0.06/1K tokens | $0.12/1K tokens |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4/32K上下文 | $0.06/1K个token | $0.12/1K个token |'
- en: '| GPT-3.5-Turbo/4K context | $0.0015/1K tokens | $0.002/1K tokens |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo/4K上下文 | $0.0015/1K个token | $0.002/1K个token |'
- en: '| GPT-3.5-Turbo/16K context | $0.003/1K tokens | $0.004/1K tokens |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo/16K上下文 | $0.003/1K个token | $0.004/1K个token |'
- en: For example, suppose you are using the GPT-4/8K context LLM. You have a prompt
    with 1,000 tokens, and the response to this from the model is 2,000 tokens. Then
    the cost will be 3 cents for the input and 12 cents for the output.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你使用的是GPT-4/8K上下文LLM。你的提示包含1,000个token，模型对此的响应是2,000个token。那么成本将是输入3美分，输出12美分。
- en: When you first sign up for an OpenAI account, you will get a $5 credit that
    can be used for the OpenAI Playground. This can be used for calls to the API.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你首次注册OpenAI账户时，你将获得5美元的信用额度，可用于OpenAI游乐场。这可以用于API调用。
- en: Tokens
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Token
- en: 'Let’s take a more detailed look at tokens. OpenAI has a tool called the [Tokenizer](https://platform.openai.com/tokenizer),
    shown in [Figure 2-3](#the_openai_tokenizer_displays_the_tokens) where I have
    entered the following for analysis:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看token。OpenAI有一个名为[Tokenizer](https://platform.openai.com/tokenizer)的工具，如图2-3所示，我在其中输入以下内容进行分析：
- en: '*Input:* ChatGPT is unbelievable! 🎉 I love it.'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*输入:* ChatGPT太不可思议了！🎉我喜欢它。'
- en: '![](assets/aiap_0203.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiap_0203.png)'
- en: Figure 2-3\. The OpenAI Tokenizer displays the tokens for an excerpt of text
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. OpenAI Tokenizer显示文本摘录的token
- en: In the tokenization—which is highlighted with colors—the word *ChatGPT* is composed
    of three tokens. The breakdown is Chat, G, and PT. The word *unbelievable* and
    its following exclamation point have two tokens, one for the word and one for
    the punctuation. As for the emoji, it consists of three tokens. Each punctuation
    mark is a token. Spaces are included with an adjacent word.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词中——用颜色突出显示——单词*ChatGPT*由三个标记组成。分解为Chat、G和PT。单词*unbelievable*及其随后的感叹号有两个标记，一个用于单词，一个用于标点符号。至于表情符号，它由三个标记组成。每个标点符号都是一个标记。空格包括与相邻单词一起。
- en: The Tokenizer is for GPT-3, GPT-3.5, and GPT-4\. Keep in mind that tokenization
    is often different among the LLMs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器适用于GPT-3、GPT-3.5和GPT-4。请注意，LLM之间的分词通常不同。
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As a rule of thumb, 1,000 tokens is roughly equivalent to 750 words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为经验法则，1,000个标记大约相当于750个单词。
- en: Using the Platform
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用平台
- en: When you go to the OpenAI Playground, you get access to a dashboard, shown in
    [Figure 2-4](#the_openai_playground_has_a_dashboard_wi).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当您访问OpenAI游乐场时，您将获得访问仪表板的权限，如图[图2-4](#the_openai_playground_has_a_dashboard_wi)所示。
- en: '![](assets/aiap_0204.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiap_0204.png)'
- en: Figure 2-4\. The OpenAI Playground has a dashboard with tips, resources, and
    interaction areas
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4. OpenAI游乐场有一个包含提示、资源和交互区域的仪表板
- en: 'The middle of the screen has the main workflow for the interactions with an
    LLM:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕中间是用于与LLM交互的主要工作流程：
- en: System
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 系统
- en: This is where you provide some context for the LLM, for example, “You are an
    expert in Python programming.” The system prompt is the first message in a session
    and sets the stage for the interaction. Customizing the system prompt allows for
    greater control over how the model behaves in the conversation, which can be particularly
    useful to ensure that it stays within desired parameters or contexts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您为LLM提供一些背景信息的地方，例如，“您是Python编程的专家。”系统提示是会话中的第一条消息，为交互设定了基调。定制系统提示可以更好地控制模型在对话中的行为，这特别有用，可以确保它保持在期望的参数或上下文中。
- en: User
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 用户
- en: This is the main instruction of the prompt. For example, this is where you can
    ask the LLM to carry out a coding task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提示的主要指令。例如，这是您要求LLM执行编码任务的地方。
- en: Add message
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 添加消息
- en: This allows you to have an ongoing chat with the LLM.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许您与LLM进行持续的聊天。
- en: 'Let’s try an example. Suppose you’re working on a Python project and you’re
    having trouble understanding how to implement the Tkinter library to get user
    input. You can enter the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个例子。假设您正在处理一个Python项目，并且您在理解如何实现Tkinter库以获取用户输入方面遇到困难。您可以输入以下内容：
- en: '*System message:* You are a Python expert specialized in Tkinter.'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*系统消息*：您是一位专注于Tkinter的Python专家。'
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User message:* I want to create a simple GUI using Tkinter to get a user’s
    name and age. How can I do that?'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*用户消息*：我想使用Tkinter创建一个简单的GUI来获取用户的姓名和年龄。我该如何操作？'
- en: The LLM will generate the code listing. But suppose you want to add validation
    for the input. You can press the Add button and enter “How can I ensure the age
    entered is a number and not text?”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LLM将生成代码列表。但假设您想添加对输入的验证。您可以点击添加按钮并输入“我该如何确保输入的年龄是一个数字而不是文本？”
- en: The LLM will respond with the code for this, using a `try-except` block to convert
    the age input to an integer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LLM将使用`try-except`块将年龄输入转换为整数来对此做出响应。
- en: 'Granted, this is like using ChatGPT—but with more structure. Also, the real
    power is the ability for customization. You’ll find these features on the right
    side of the screen:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这就像使用ChatGPT一样——但结构更严谨。真正的力量在于定制的功能。您将在屏幕右侧找到这些功能：
- en: Model
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: You can select from a variety of models and can even use your own fine-tuned
    LLMs to ensure the model is focused on the unique needs of your coding. You can
    find more information about fine-tuning a model in the [OpenAI API documentation](https://oreil.ly/L3y09).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从各种模型中进行选择，甚至可以使用您自己的微调LLM来确保模型专注于您编码的独特需求。您可以在[OpenAI API文档](https://oreil.ly/L3y09)中找到有关微调模型的更多信息。
- en: Temperature
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 温度
- en: This adjusts the randomness or creativity of the generated content. The range
    of values is from 0 to 2\. The lower the value, the more deterministic and focused
    are the responses. [Table 2-3](#suggested_temperature_levels_for_certain) shows
    suggested temperature levels for different types of development tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这调整了生成内容的随机性或创造性。值的范围是从0到2。值越低，响应越确定性和专注。[表2-3](#suggested_temperature_levels_for_certain)显示了不同类型开发任务的建议温度级别。
- en: Table 2-3\. Suggested temperature levels for certain types of programming tasks
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-3。建议的某些类型编程任务的温度级别
- en: '| Task category | Temperature value | Description |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 任务类别 | 温度值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Code generation | 0.2–0.3 | Ensures more deterministic, accurate code adhering
    to common conventions for reliable and understandable outcomes. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | 0.2–0.3 | 确保更确定性的、准确的代码，遵循常见的约定，以实现可靠和可理解的结果。 |'
- en: '| Code review | 0.2 or less | Focuses on well-established best practices and
    standards for precise feedback. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 代码审查 | 0.2或以下 | 专注于已建立的最佳实践和标准，以提供精确的反馈。 |'
- en: '| Bug fixing | 0.2 or less | Produces more accurate and straightforward solutions
    to identified issues. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 错误修复 | 0.2或以下 | 产生更准确和直接的解决方案，以解决识别出的问题。 |'
- en: '| Creative problem solving | 0.7–1.0 | Explores a broader range of possible
    solutions, useful in brainstorming or innovative problem solving. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 创造性问题解决 | 0.7–1.0 | 探索更广泛的可能解决方案，在头脑风暴或创新问题解决中很有用。 |'
- en: '| Learning and experimentation | 0.7–1.0 | Provides a wider variety of examples
    and solutions for understanding different approaches to problem solving. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 学习和实验 | 0.7–1.0 | 提供更多样化的示例和解决方案，以理解解决问题的不同方法。 |'
- en: '| Data analysis and visualization | 0.2 or less | Generates accurate and meaningful
    visualizations or analyses. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 数据分析和可视化 | 0.2或以下 | 生成准确且具有意义的可视化或分析。 |'
- en: '| Optimization tasks | Varied | Permits striking a balance between exploration
    (higher temperature) and exploitation (lower temperature) for efficient solutions.
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 优化任务 | 多样化 | 允许在探索（较高温度）和利用（较低温度）之间取得平衡，以实现高效解决方案。 |'
- en: 'However, if you use a fairly high value for the temperature, the results can
    be nonsensical. Here’s a sample prompt when using a value of 2:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您为温度设置一个相当高的值，结果可能会变得毫无意义。这里是一个使用值为2时的示例提示：
- en: '*Prompt:* In Python, what are the steps to migrate data from a CSV file to
    a MySQL database?'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 在Python中，将数据从CSV文件迁移到MySQL数据库的步骤是什么？'
- en: '[Figure 2-5](#when_using_a_temperature_of_twocomma_the) shows the output. As
    you can see, this makes little sense!'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-5](#when_using_a_temperature_of_twocomma_the)显示了输出。正如您所看到的，这几乎没有任何意义！'
- en: '![](assets/aiap_0205.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiap_0205.png)'
- en: Figure 2-5\. When using a temperature of 2, the LLM’s results are mostly nonsensical
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。当使用温度为2时，LLM的结果大多是毫无意义的
- en: 'Now, let’s look at the other features you can adjust:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看您可以调整的其他功能：
- en: Maximum length
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最大长度
- en: This is the maximum number of tokens to use to generate content. The number
    includes usage for both the prompt and response. The ratio of tokens to content
    depends on the model you use.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于生成内容的最大标记数。该数字包括提示和响应的使用。标记与内容的比例取决于您使用的模型。
- en: Stop sequence
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列
- en: This indicates a point at which the LLM should stop creating further text. You
    can specify a particular string or sequence of characters that, when detected
    in the generated text, will signal the model to halt the process.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示LLM应该停止创建更多文本的点。您可以指定一个特定的字符串或字符序列，当检测到生成的文本中时，将信号模型停止该过程。
- en: Top p
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Top p
- en: Also known as nucleus sampling, this technique selects words based on a cumulative
    probability threshold, denoted by *p*, which can range from 0 to 1\. In simpler
    terms, instead of always choosing from the top few most likely next words, the
    model considers a broader or narrower range of possible next words based on the
    specified *p*-value. A lower *p*-value results in a smaller, more focused set
    of words to choose from, leading to more predictable and coherent text. A higher
    *p*-value, on the other hand, allows for a wider set of possible next words, leading
    to more diverse and creative text generation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为核采样，该技术根据累积概率阈值选择单词，该阈值用*p*表示，其范围可以从0到1。简单来说，模型不会总是从最有可能的几个下一个单词中选择，而是根据指定的*p*值考虑更广泛或更窄的下一个可能单词的范围。较低的*p*值会导致选择范围更小、更集中的单词集，从而产生更可预测和连贯的文本。另一方面，较高的*p*值允许选择更广泛的下一个可能单词，从而产生更多样化和富有创造性的文本生成。
- en: Frequency penalty
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚
- en: This helps to tackle a common problem with LLMs, which is repetitive phrases
    or sentences. The value ranges from 0 to 2\. The higher the value, the less repetition.
    However, at values greater than 1, text generation can get unpredictable and even
    nonsensical.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于解决LLM中常见的重复短语或句子的问题。值范围从0到2。值越高，重复越少。然而，在值大于1时，文本生成可能会变得不可预测，甚至毫无意义。
- en: Presence penalty
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚
- en: This also has a value of 0 to 2\. A higher value will allow the LLM to include
    a wider variety of tokens, which means using a more diverse vocabulary or broader
    universe of concepts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这也有一个0到2的值。更高的值将允许LLM包含更广泛的标记，这意味着使用更丰富的词汇或更广泛的概念宇宙。
- en: With the frequency penalty, presence penalty, and top *p*, OpenAI recommends
    selecting one approach to adjust for your task. But don’t shy away from experimentation.
    The path to optimizing LLMs isn’t paved with strict rules, thanks to the intricate
    dance of the complexities involved.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在频率惩罚、存在惩罚和top *p*的情况下，OpenAI建议选择一种方法来调整您的任务。但不要害怕实验。由于涉及到的复杂性的复杂舞蹈，优化LLM的道路并非由严格的规则铺就。
- en: Evaluating LLMs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLM
- en: Assessing LLMs is a hefty task. These behemoths are often so opaque that they
    can seem impossible to understand. The competition among AI firms only worsens
    this. It’s become par for the course to see scant details on the datasets these
    models are trained on, the number of parameters used to fine-tune their behavior,
    and the hardware that powers them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM是一项艰巨的任务。这些巨兽通常如此不透明，以至于它们似乎难以理解。AI公司之间的竞争使这个问题更加严重。对于这些模型所训练的数据集、用于微调其行为的参数数量以及驱动它们的硬件的细节，我们通常只能看到很少的信息。
- en: But there is some good news, thanks to some researchers at Stanford. They’ve
    created a [scoring system](https://oreil.ly/FoVAr) dubbed the Foundation Model
    Transparency Index to size up the openness of LLMs. This yardstick, shaped by
    a hundred criteria, is a bid to usher some clarity into the murky waters of LLM
    transparency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有一些好消息，多亏了斯坦福的一些研究人员。他们创建了一个[评分系统](https://oreil.ly/FoVAr)，称为基础模型透明度指数，用于评估LLM的开放性。这个由一百个标准构成的指标，旨在为LLM透明度的模糊水域带来一些清晰。
- en: The ranking is based on a percentage scale. [Table 2-4](#rankings_of_top_llms_in_terms_of_transpa)
    shows the rankings. Unfortunately, the results are far from encouraging. No major
    LLM is close to achieving “adequate transparency,” according to the researchers,
    and the mean score is only 37%.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 排名基于百分比尺度。[表2-4](#rankings_of_top_llms_in_terms_of_transpa)展示了排名。遗憾的是，结果远非鼓舞人心。研究人员表示，没有主要的大型语言模型接近实现“足够的透明度”，平均得分仅为37%。
- en: Table 2-4\. Rankings of top LLMs in terms of transparency of their models^([a](ch02.html#id399))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-4\. 按模型透明度排名的顶级LLM^([a](ch02.html#id399))
- en: '| Company | Model | Rank |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 公司 | 模型 | 排名 |'
- en: '| --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Meta | LLaMA 2 | 54% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Meta | LLaMA 2 | 54% |'
- en: '| BigScience | BLOOMZ | 53% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| BigScience | BLOOMZ | 53% |'
- en: '| OpenAI | GPT-4 | 48% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | GPT-4 | 48% |'
- en: '| Stability.ai | Stable Diffusion 2 | 47% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Stability.ai | Stable Diffusion 2 | 47% |'
- en: '| Google | PaLM 2 | 40% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Google | PaLM 2 | 40% |'
- en: '| Anthropic | Claude 2 | 36% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Anthropic | Claude 2 | 36% |'
- en: '| Cohere | Command | 34% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Cohere | Command | 34% |'
- en: '| AI21Labs | Jurassic-2 | 25% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| AI21Labs | Jurassic-2 | 25% |'
- en: '| Inflection | Inflection-1 | 21% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Inflection | Inflection-1 | 21% |'
- en: '| Amazon | Titan Text | 12% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Amazon | Titan Text | 12% |'
- en: '| ^([a](ch02.html#id399-marker)) Center for Research on Foundation Models,
    Foundation Model Transparency Index Total Scores 2023, [*https://crfm.stanford.edu/fmti*](https://crfm.stanford.edu/fmti)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch02.html#id399-marker)) 基础模型研究中心，基础模型透明度指数总得分2023，[*https://crfm.stanford.edu/fmti*](https://crfm.stanford.edu/fmti)
    |'
- en: The flexibility of LLMs to handle various domains and tasks, such as software
    development, is a notable advantage. However, it also complicates the evaluation
    process, as it requires domain-specific evaluation metrics and benchmarks to ensure
    the model’s effectiveness and safety in each particular application.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LLM处理各种领域和任务（如软件开发）的灵活性是一个显著的优势。然而，这也使得评估过程变得复杂，因为它需要特定领域的评估指标和基准，以确保模型在每个特定应用中的有效性和安全性。
- en: 'Despite all this, there are some metrics to consider when evaluating LLMs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在评估LLM时，还有一些指标需要考虑：
- en: BERTScore
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore
- en: This metric is designed to evaluate text generation models by comparing generated
    text to reference text using BERT embeddings. Although primarily used for natural
    language text, it can be extended or adapted for code generation tasks, especially
    when the code is annotated or commented in natural language.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标旨在通过比较生成的文本与参考文本（使用BERT嵌入）来评估文本生成模型。尽管主要用于自然语言文本，但它可以扩展或适应代码生成任务，尤其是在代码用自然语言注释或注释时。
- en: Perplexity
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂度
- en: This is a common metric for evaluating probabilistic models like LLMs. It quantifies
    how well the probability distribution predicted by the model aligns with the actual
    distribution of the data. In the context of code generation, lower perplexity
    values indicate that the model is better at predicting the next token in a sequence
    of code.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种评估概率模型（如 LLMs）的常用度量标准。它量化模型预测的概率分布与实际数据分布之间的匹配程度。在代码生成的背景下，较低的困惑度值表示模型在预测代码序列中的下一个标记方面表现得更好。
- en: BLEU (bilingual evaluation understudy)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评估辅助研究）
- en: Originally developed for machine translation, BLEU is also used in code generation
    to compare the generated code with reference code. It computes *n*-gram precision
    scores to quantify the similarity between the generated and reference texts, which
    can help in evaluating the syntactic correctness of the generated code. A higher
    *n*-gram precision score indicates better agreement between the generated and
    reference text for that specific sequence of *n* words.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最初是为机器翻译开发的，但 BLEU 也用于代码生成，以比较生成代码与参考代码。它计算 *n*-gram 精确度分数来量化生成文本和参考文本之间的相似度，这有助于评估生成代码的语法正确性。更高的
    *n*-gram 精确度分数表示在特定的 *n* 词序列中，生成文本与参考文本之间的协议更好。
- en: ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE（面向摘要评估的召回率辅助研究）
- en: This is another metric borrowed from NLP that can be used to evaluate code generation
    models. It calculates the overlap of *n*-grams between the generated and reference
    texts, providing insights into how well the generated code aligns with the expected
    output.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从自然语言处理（NLP）中借用的一种度量标准，可用于评估代码生成模型。它计算生成文本和参考文本之间 *n*-gram 的重叠，从而提供有关生成代码与预期输出对齐程度的见解。
- en: MBXP (most basic X programming problems)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: MBXP（最基础的 X 编程问题）
- en: This benchmark is designed specifically for evaluating code generation models
    across multiple programming languages. It uses a scalable conversion framework
    to transpile prompts and test cases from original datasets into target languages,
    thereby facilitating a comprehensive multilingual evaluation of code generation
    models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准专门设计用于评估跨多种编程语言的代码生成模型。它使用可扩展的转换框架将提示和测试用例从原始数据集转换为目标语言，从而促进对代码生成模型进行全面的跨语言评估。
- en: HumanEval
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval
- en: This is a benchmark to evaluate the code generation capabilities of LLMs by
    measuring their functional correctness in synthesizing programs from docstrings.
    This benchmark is crucial for the continuous development and enhancement of AI
    models in code generation. While different models display varying levels of proficiency
    on HumanEval, an extended version called HUMANEVAL+ has been key in identifying
    previously undetected incorrect code generated by popular LLMs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基准，通过测量 LLMs 从文档字符串中合成程序的功能正确性来评估它们的代码生成能力。这个基准对于代码生成中 AI 模型的持续发展和改进至关重要。虽然不同的模型在
    HumanEval 上的熟练程度不同，但一个被称为 HUMANEVAL+ 的扩展版本在识别由流行的 LLMs 生成的先前未检测到的错误代码方面发挥了关键作用。
- en: Multilingual HumanEval (HumanEval-X)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言 HumanEval（HumanEval-X）
- en: This is an extension of the original HumanEval benchmark. Multilingual HumanEval
    evaluates LLMs’ code generation and translation capabilities across more than
    10 programming languages. It employs a conversion framework to transpile prompts
    and test cases from Python into corresponding data in target languages, creating
    a more comprehensive benchmark for multilingual code generation and translation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是原始 HumanEval 基准的扩展。多语言 HumanEval 评估 LLMs 在超过 10 种编程语言中的代码生成和翻译能力。它采用转换框架将提示和测试用例从
    Python 转换为目标语言中的相应数据，为多语言代码生成和翻译创建了一个更全面的基准。
- en: Another way to evaluate an LLM is to look at the number of parameters—which
    can be in the hundreds of billions. So the more, the better, right? Not necessarily.
    Evaluation should take a more nuanced approach. First of all, the costs of scaling
    the parameters can be enormous, in terms of compute power and energy usage. This
    could make an LLM uneconomical for monetizing applications. Next, as the parameter
    counts balloon, so does the complexity of the model, which could potentially lead
    to overfitting. *Overfitting* occurs when the model learns to perform exceedingly
    well on the training data but fumbles when exposed to unseen data. This dilutes
    its generalization capability.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个大型语言模型（LLM）的另一种方法是查看其参数数量——这可能是数百亿个。所以参数越多越好，对吧？不一定。评估应该采取更细致的方法。首先，扩展参数的成本可能非常巨大，从计算能力和能源消耗的角度来看。这可能会使LLM在货币化应用上变得不经济。接下来，随着参数数量的激增，模型的复杂性也随之增加，这可能导致过度拟合。*过度拟合*发生在模型在训练数据上表现出色，但在面对未见过的数据时却表现不佳。这削弱了其泛化能力。
- en: Another issue is the need for vast and diverse training datasets to feed the
    insatiable appetite of these models for data. However, obtaining and curating
    such extensive datasets not only is resource intensive but also poses challenges
    pertaining to data privacy and bias. What’s more, the evaluation of these behemoths
    becomes increasingly intricate with the surge in parameters. The evaluation metrics
    need to be more comprehensive and diverse to accurately gauge the model’s performance
    across a myriad of tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是需要大量且多样化的训练数据集来满足这些模型对数据的无尽渴望。然而，获取和整理如此庞大的数据集不仅资源密集，而且也提出了与数据隐私和偏差相关的问题。更重要的是，随着参数数量的激增，这些巨头的评估变得越来越复杂。评估指标需要更加全面和多样化，才能准确衡量模型在众多任务上的性能。
- en: Finally, fine-tuning can be a better way to get more out of models without the
    need for large increases in the parameter size of the underlying LLM.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，微调可能是在不增加底层LLM参数规模的情况下，从模型中获得更多的一种更好的方法。
- en: Types of LLMs
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的类型
- en: There are various types of LLMs, and one prominent category is open source LLMs.
    Anyone can use, tweak, or share them. Their transparency means you can see how
    these models tick. Plus, open source LLMs allow developers to collaborate on innovation
    as well as develop add-ons and, of course, fix pesky bugs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种类型的LLM，其中一种突出的类别是开源LLM。任何人都可以使用、调整或分享它们。它们的透明度意味着你可以看到这些模型是如何运作的。此外，开源LLM允许开发者合作创新，以及开发附加组件，当然，修复烦人的错误。
- en: And the best part? They don’t come with a price tag.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 而最好的部分？它们没有价格标签。
- en: But open source LLMs are not all rainbows and unicorns. There’s usually no dedicated
    team to swoop in and fix issues or roll out regular updates. So, if you hit a
    snag, you might have to roll up your sleeves and dive into the forums for some
    help.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但开源LLM并非全是彩虹和独角兽。通常没有专门的团队来迅速解决问题或推出常规更新。所以，如果你遇到问题，你可能不得不卷起袖子，深入论坛寻求帮助。
- en: The quality and performance of open source models can sometimes feel like a
    rollercoaster. Then there are the nagging security issues. Since everything is
    available, hackers are more likely to find ways to insert nefarious code. Caution
    is advised.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型的质量和性能有时感觉像过山车。然后还有那些烦人的安全问题。由于一切都可以访问，黑客更有可能找到方法插入恶意代码。建议谨慎行事。
- en: Lastly, when it comes to user guides and documentation, open source LLMs might
    have you wishing for more. The guides can sometimes feel like they were written
    in hieroglyphics.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当涉及到用户指南和文档时，开源LLM可能会让你希望有更多。有时，这些指南感觉像是用象形文字写的。
- en: '[Table 2-5](#top_open_source_llms) shows some of the top open source LLMs.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-5](#top_open_source_llms) 展示了一些最受欢迎的开源LLM。'
- en: Table 2-5\. Top open source LLMs
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-5\. 最受欢迎的开源LLM
- en: '| Model | Developer | Parameters (B = billion) | Noteworthy features |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 开发者 | 参数（B = 十亿） | 值得注意的功能 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-NeoX-20B | EleutherAI | 20B | Trained on “The Pile” dataset; capable
    of various NLP tasks such as story generation, chatbots, and summarization |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| GPT-NeoX-20B | EleutherAI | 20B | 在“The Pile”数据集上训练；能够执行各种NLP任务，如故事生成、聊天机器人和摘要
    |'
- en: '| LLaMA 2 | Meta | 7B to 70B | Trained on 2 trillion tokens; double the context
    length of LLaMA 1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2 | Meta | 7B到70B | 在2000亿个标记上训练；LLaMA 1的上下文长度翻倍 |'
- en: '| OPT-175B | Meta | 175B | Part of a suite of models; trained with a lower
    carbon footprint than GPT-3 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175B | Meta | 175B | 作为一系列模型的一部分；训练时比GPT-3的碳足迹低 |'
- en: '| BLOOM | BigScience | 176B | Trained on ROOTS^([a](ch02.html#id417)) corpus;
    designed for transparency with disclosed training data details and evaluation
    methods |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | BigScience | 176B | 在 ROOTS^([a](ch02.html#id417)) 语料库上训练；设计时考虑透明度，公开了训练数据细节和评估方法
    |'
- en: '| Falcon-40B | Technology Innovation Institute (TII) | 40B | Trained on 1,000B
    tokens |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Falcon-40B | 技术创新研究所 (TII) | 40B | 在 1,000B 个标记上训练 |'
- en: '| Dolly 2.0 | Databricks | 12B | Based on EleutherAI’s Pythia model family;
    delivers ChatGPT-like instruction-following interactivity |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Dolly 2.0 | Databricks | 12B | 基于 EleutherAI 的 Pythia 模型系列；提供类似 ChatGPT 的指令跟随交互性
    |'
- en: '| Mistral 7B | Mistral AI | 7.3B | Uses grouped-query and sliding window attention;
    trained on a vast dataset and excels in longer sequence handling |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | Mistral AI | 7.3B | 使用分组查询和滑动窗口注意力；在庞大的数据集上训练，擅长处理长序列 |'
- en: '| Mixtral 8X7B | Mistral AI | 46.7B | Sparse mixture of experts model; performs
    inference like a 12.9B model, supports multiple languages, and excels in various
    tasks including code generation and reasoning |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral 8X7B | Mistral AI | 46.7B | 稀疏专家混合模型；性能如同 12.9B 模型，支持多种语言，在代码生成和推理等各项任务中表现出色
    |'
- en: '| ^([a](ch02.html#id417-marker)) Responsible Open-science Open-collaboration
    Text Sources |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch02.html#id417-marker)) 负责的开放科学 开放协作 文本来源 |'
- en: Closed-source or proprietary LLMs, on the other hand, are much more secretive.
    They mostly keep their code, training data, and model structures under tight wraps.
    However, the companies that develop these complex systems usually have enormous
    amounts of capital. [Table 2-6](#venture_capital_raised_by_top_llm_develo) shows
    the capital raised by these firms in 2023.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，闭源或专有 LLM 要保密得多。它们通常将代码、训练数据和模型结构严格保密。然而，开发这些复杂系统的公司通常拥有巨额资金。[表 2-6](#venture_capital_raised_by_top_llm_develo)
    显示了这些公司在 2023 年筹集的资金。
- en: Table 2-6\. Venture capital raised by top LLM developers
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-6\. 顶级 LLM 开发者筹集的风险资本
- en: '| Company | Funding |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 公司 | 资金 |'
- en: '| --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Anthropic | $1.25 billion |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Anthropic | 1.25 亿美元 |'
- en: '| OpenAI | $10 billion |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 10 亿美元 |'
- en: '| Cohere | $270 million |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Cohere | 2.7 亿美元 |'
- en: '| Inflection AI | $1.3 billion |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Inflection AI | 1.3 亿美元 |'
- en: With such resources, these companies can hire the world’s best data scientists
    and build sophisticated infrastructure. The result is that the LLMs are often
    state-of-the-art in terms of performance. They are also built for scale and the
    rigorous needs of enterprises, such as for security and privacy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有如此资源，这些公司可以聘请世界上最优秀的数据科学家并建立复杂的基础设施。结果是，LLM 在性能方面通常是尖端技术。它们也针对规模和企业的严格需求而构建，例如安全性、隐私性。
- en: As for the downsides, there is the problem with trust. How do these models come
    up with their responses? What about hallucinations and bias? Answers to these
    questions can be lacking in detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 至于缺点，存在信任问题。这些模型是如何得出它们的回答的？关于幻觉和偏见呢？对这些问题的答案可能缺乏详细性。
- en: Then there is the risk that these mega AI operators will become a monopoly.
    This could mean that a customer would be locked into an ecosystem. Lastly, closed-source
    LLMs might be more prone to stagnation than open source projects, as they might
    not benefit from the diverse input and scrutiny that open source projects usually
    enjoy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些大型 AI 运营商可能成为垄断者的风险。这可能意味着客户会被锁定在一个生态系统中。最后，闭源 LLM 可能比开源项目更容易停滞不前，因为它们可能无法从开源项目通常享有的多样化和审查中受益。
- en: Evaluation of AI-Assisted Programming Tools
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 辅助编程工具评估
- en: Figuring out which AI-assisted programming tool to go for can be a head-scratcher.
    You’ve got to weigh many factors like its precision, chat features, security,
    speed, and user-friendliness. Sometimes, it boils down to what feels right to
    work with. But then again, your hands might be tied if your employer insists on
    a specific system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 确定选择哪种 AI 辅助编程工具可能是一个令人头疼的问题。您必须权衡许多因素，如其精确度、聊天功能、安全性、速度和用户友好性。有时，这归结为感觉适合工作的事情。但另一方面，如果您的雇主坚持使用特定的系统，您的选择可能受到限制。
- en: To get a sense of what’s hot right now, [Stack Overflow’s 2023 Developer Survey](https://oreil.ly/nvqKY)
    is a handy resource. Stack Overflow gathered insights from nearly 90,000 coders
    on the most popular tools, which you can see in [Table 2-7](#the_ranking_of_popular_aihyphenassisted).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解当前的热门趋势，[Stack Overflow 2023 开发者调查](https://oreil.ly/nvqKY) 是一个实用的资源。Stack
    Overflow 从近 90,000 名程序员那里收集了关于最受欢迎的工具的见解，您可以在[表 2-7](#the_ranking_of_popular_aihyphenassisted)中看到。
- en: Table 2-7\. The ranking of popular AI-assisted programming tools^([a](ch02.html#id424))
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-7\. 流行AI辅助编程工具排名^([a](ch02.html#id424))
- en: '| AI-assisted developer tool | Percentage |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| AI辅助开发者工具 | 百分比 |'
- en: '| --- | --- |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GitHub Copilot | 54.77% |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| GitHub Copilot | 54.77% |'
- en: '| Tabnine | 12.88% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Tabnine | 12.88% |'
- en: '| Amazon CodeWhisperer | 5.14% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Amazon CodeWhisperer | 5.14% |'
- en: '| Snyk Code | 1.33% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Snyk Code | 1.33% |'
- en: '| Codeium | 1.25% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Codeium | 1.25% |'
- en: '| Wispr AI | 1.13% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Wispr AI | 1.13% |'
- en: '| Replit Ghostwriter | 0.83% |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Replit Ghostwriter | 0.83% |'
- en: '| Mintlify | 0.52% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Mintlify | 0.52% |'
- en: '| Adrenaline | 0.43% |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Adrenaline | 0.43% |'
- en: '| Rubberduck AI | 0.37% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Rubberduck AI | 0.37% |'
- en: '| ^([a](ch02.html#id424-marker)) [Stack Overflow, 2023 Developer Survey](https://oreil.ly/0u7WZ)
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch02.html#id424-marker)) [Stack Overflow, 2023开发者调查](https://oreil.ly/0u7WZ)
    |'
- en: This chart gives you a glimpse of the numerous tools available. When you’re
    looking to pick one, a smart move is to get recommendations from other developers.
    Plus, it’s a good idea to test drive a few yourself. Luckily, most of these tools
    offer free trials, so you can give them a whirl without committing right off the
    bat.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表向您展示了众多可用的工具。当您在寻找合适的工具时，从其他开发者那里获取推荐是一个明智的选择。此外，亲自试用几个工具也是一个好主意。幸运的是，大多数这些工具都提供免费试用，这样您可以在不立即做出承诺的情况下尝试它们。
- en: Another key aspect to consider is the company’s financial backing. Does it have
    venture capital funding? Without this, a company might struggle not just to grow
    but also to keep its platform innovative. Already, several AI-assisted programming
    firms have had to pull the plug on their services, and that can really throw a
    wrench in the works for developers. Take Kite, for instance. It was one of the
    early players in this field, starting up in 2014\. However, by 2022, the company
    decided to [call it quits on the project](https://oreil.ly/Bnz9U). The silver
    lining? It open sourced most of the tool’s codebase.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的关键方面是公司的财务支持。它是否有风险投资？如果没有，公司可能不仅难以增长，还可能难以保持其平台创新。已经，几家AI辅助编程公司不得不关闭他们的服务，这对开发者来说可能真的会打乱工作。以Kite为例。它是这个领域的早期参与者之一，于2014年成立。然而，到2022年，公司决定[停止该项目](https://oreil.ly/Bnz9U)。一线希望？它开源了工具的大部分代码库。
- en: Conclusion
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, we pulled back the curtain on generative AI and LLMs. We got
    a glimpse of some of the fascinating history, such as with ELIZA, and then focused
    on one of the biggest breakthroughs in AI: the transformer model. We also tried
    out the OpenAI Playground and showed how to customize the LLM.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们揭开了生成式AI和LLM的神秘面纱。我们瞥见了其中一些迷人的历史，例如ELIZA，然后聚焦于AI领域的一项重大突破：Transformer模型。我们还尝试了OpenAI游乐场，并展示了如何定制LLM。
- en: Some of the key nuggets in this chapter include tokens, the advantages of piggybacking
    on pretrained models, the dos and don’ts of sizing up LLMs, metrics like perplexity
    and BLEU scores, and open source versus proprietary models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一些关键要点包括标记、利用预训练模型的优点、评估LLM的注意事项、如困惑度和BLEU分数等指标，以及开源模型与专有模型之间的比较。
