- en: Chapter 16\. Vision and Multimodal Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章. 视觉和多模态转换器
- en: In the previous chapter, we implemented a transformer from scratch and turned
    it into a translation system, then we explored encoder-only models for NLU, decoder-only
    models for NLG, and we even built a little chatbot—that was quite a journey! Yet,
    there’s still a lot more to say about transformers. In particular, we have only
    dealt with text so far, but transformers actually turned out to be exceptionally
    good at processing all sorts of inputs. In this chapter we will cover *vision
    transformers* (ViTs), capable of processing images, followed by *multimodal transformers*,
    capable of handling multiple modalities, including text, images, audio, videos,
    robot sensors and actuators, and really any kind of data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们从零开始实现了一个转换器并将其转变为翻译系统，然后我们探讨了仅编码器模型用于 NLU，仅解码器模型用于 NLG，我们甚至构建了一个小型的聊天机器人——这是一段相当漫长的旅程！然而，关于转换器还有很多话要说。特别是，我们迄今为止只处理了文本，但转换器实际上在处理各种输入方面表现出色。在本章中，我们将介绍能够处理图像的
    *视觉转换器*（ViTs），然后是能够处理包括文本、图像、音频、视频、机器人传感器和执行器以及任何类型数据的 *多模态转换器*。
- en: 'In the first part of this chapter, we will discuss some of the most influential
    pure-vision transformers:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将讨论一些最有影响力的纯视觉转换器：
- en: DETR (Detection Transformer)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: DETR (检测转换器)
- en: An early encoder-decoder transformer for object detection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于目标检测的早期编码器-解码器转换器。
- en: The original ViT (Vision Transformer)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 ViT (视觉转换器)
- en: This landmark encoder-only transformer treats image patches like word tokens
    and reaches the state of the art if trained on a large dataset.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个里程碑式的仅编码器转换器将图像块视为词标记，如果在大数据集上训练，可以达到最先进的状态。
- en: DeiT (Data-Efficient Image Transformer)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DeiT (数据高效图像转换器)
- en: A more data-efficient ViT trained at scale using distillation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒸馏在规模上训练的更具数据效率的 ViT。
- en: PVT (Pyramid Vision Transformer)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PVT (金字塔视觉转换器)
- en: A hierarchical model that can produce multiscale feature maps for semantic segmentation
    and other dense prediction tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以生成多尺度特征图用于语义分割和其他密集预测任务分层模型。
- en: Swin Transformer (Shifted Windows Transformer)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Swin Transformer (移动窗口转换器)
- en: A much faster hierarchical model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个速度更快的分层模型。
- en: DINO (self-Distillation with NO labels)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DINO (无标签的自蒸馏)
- en: This introduced a novel self-supervised technique for visual representation
    learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了一种新颖的自监督视觉表示学习方法。
- en: 'In the second part of this chapter, we will dive into multimodal transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将深入探讨多模态转换器：
- en: VideoBERT
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: VideoBERT
- en: A BERT model trained to process both text and video tokens.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一种训练用于处理文本和视频标记的 BERT 模型。
- en: ViLBERT (Visio-Linguistic BERT)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ViLBERT (视觉语言BERT)
- en: A dual-encoder model for image plus text, which introduced co-attention (i.e.,
    two-way cross-attention).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于图像加文本的双编码器模型，它引入了共注意力（即双向交叉注意力）。
- en: CLIP (Contrastive Language–Image Pretraining)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP (对比语言-图像预训练)
- en: This is another image plus text dual-encoder model trained using contrastive
    pretraining.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个使用对比预训练训练的图像加文本双编码器模型。
- en: DALL·E (a pun on the names of the artist Salvador Dali and the Pixar character
    Wall-E)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DALL·E （对艺术家萨尔瓦多·达利和皮克斯角色瓦力名字的恶搞）
- en: A model capable of generating images from text prompts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一种能够从文本提示生成图像的模型。
- en: Perceiver
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver
- en: This efficiently compresses any high-resolution modality into a short sequence
    using a cross-attention trick.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用交叉注意力技巧有效地将任何高分辨率模态压缩成短序列。
- en: Perceiver IO (Input/Output)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver IO (输入/输出)
- en: Adds a flexible output mechanism to the Perceiver, using a similar cross-attention
    trick.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Perceiver 添加了一个灵活的输出机制，使用类似的交叉注意力技巧。
- en: Flamingo
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo
- en: Rather than starting from scratch, it reuses two large pretrained models—one
    for vision and one for language (both frozen)—and connects them using a Perceiver-style
    adapter named a Resampler. This architecture enables open-ended visual dialogue.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 它不是从头开始，而是重用了两个大型预训练模型——一个用于视觉，一个用于语言（两者都冻结）——并使用名为 Resampler 的 Perceiver 风格适配器将它们连接起来。这种架构使得开放式的视觉对话成为可能。
- en: BLIP-2 (Bootstrapping Language-Image Pretraining)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-2 (自举语言-图像预训练)
- en: This is another open-ended visual dialogue model that reuses two large pretrained
    models, connects them using a lightweight querying transformer (Q-Former), and
    uses a powerful two-stage training approach with multiple training objectives.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个开放式的视觉对话模型，它重用了两个大型预训练模型，使用轻量级的查询转换器（Q-Former）将它们连接起来，并采用了一种强大的两阶段训练方法，具有多个训练目标。
- en: So turn on the lights, transformers are about to open their eyes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，打开灯光，变换器即将睁开眼睛。
- en: Vision Transformers
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变换器
- en: 'Vision transformers didn’t pop out of a vacuum: before they were invented,
    there were RNNs with visual attention, and hybrid CNN-Transformer models. Let’s
    take a look at these ViT ancestors before we dive into some of the most influential
    ViTs.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器并非凭空出现：在它们被发明之前，已经有了带视觉注意力的RNN和混合CNN-Transformer模型。在我们深入研究一些最有影响力的ViT之前，让我们看看这些ViT的祖先。
- en: RNNs with Visual Attention
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带视觉注意力的RNN
- en: One of the first applications of attention mechanisms beyond NLP was to generate
    image captions using [visual attention](https://homl.info/visualattention).⁠^([1](ch16.html#id3722))
    Here a convolutional neural network first processes the image and outputs some
    feature maps, then a decoder RNN equipped with an attention mechanism generates
    the caption, one token at a time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在NLP之外的第一项应用之一是使用[视觉注意力](https://homl.info/visualattention)生成图像标题。⁠^([1](ch16.html#id3722))
    在这里，卷积神经网络首先处理图像并输出一些特征图，然后一个配备注意力机制的解码器RNN逐个生成标题。
- en: 'The decoder uses an attention layer at each decoding step to focus on just
    the right part of the image. For example, in [Figure 16-1](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a Frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “Frisbee”: clearly, most of its attention
    was focused on the Frisbee.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器在每个解码步骤使用一个注意力层来专注于图像的恰当部分。例如，在[图16-1](#visual_attention_diagram)中，模型生成了标题“一位女士在公园里扔飞盘”，你可以看到当解码器即将输出“飞盘”这个词时，它关注了输入图像的哪个部分：很明显，大部分注意力都集中在飞盘上。
- en: '![A woman in green throws a red Frisbee in a park, and the attention layer
    highlights the Frisbee when producing the word "Frisbee" in a caption.](assets/hmls_1601.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![一位穿绿色衣服的女士在公园里扔一个红色的飞盘，当在标题中生成“飞盘”这个词时，注意力层突出了飞盘。](assets/hmls_1601.png)'
- en: 'Figure 16-1\. Visual attention: an input image (left) and the model’s focus
    before producing the word “Frisbee” (right)⁠^([2](ch16.html#id3724))'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1\. 视觉注意力：输入图像（左）和模型在输出“飞盘”这个词之前关注的焦点（右）⁠^([2](ch16.html#id3724))
- en: Once transformers were invented, they were quickly applied to visual tasks,
    generally by replacing RNNs in existing architectures (e.g., for image captioning).
    However, the bulk of the visual work was still performed by a CNN, so although
    they were transformers used for visual tasks, we usually don’t consider them as
    ViTs. The *detection transformer* (DETR) is a good example of this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器被发明后，它们很快就被应用于视觉任务，通常是通过替换现有架构中的RNN（例如，用于图像标题）。然而，大部分视觉工作仍然由CNN完成，因此尽管它们是用于视觉任务的变换器，我们通常不把它们视为ViT。*检测变换器*（DETR）是这种类型的典型例子。
- en: 'DETR: A CNN-Transformer Hybrid for Object Detection'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETR：用于目标检测的CNN-Transformer混合模型
- en: In May 2020, a team of Facebook researchers proposed a hybrid CNN–transformer
    architecture for object detection, named [*detection transformer*](https://homl.info/detr)
    (DETR, see [Figure 16-2](#detr_diagram)).⁠^([4](ch16.html#id3740)) The CNN first
    processes the input images and outputs a set of feature maps, then these feature
    maps are turned into a sequence of visual tokens that are fed to an encoder-decoder
    transformer, and finally the transformer outputs a sequence of bounding box predictions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年5月，一组Facebook研究人员提出了一种用于目标检测的混合CNN-变换器架构，命名为[*检测变换器*](https://homl.info/detr)（DETR，见[图16-2](#detr_diagram)）。⁠^([4](ch16.html#id3740))
    CNN首先处理输入图像并输出一系列特征图，然后这些特征图被转换成一系列视觉标记，这些标记被输入到编码器-解码器变换器中，最后变换器输出一系列边界框预测。
- en: 'At one point, someone was bound to wonder whether we could get rid of the CNN
    entirely. After all, attention is all you need, right? This happened a few months
    after DETR: the original ViT was born.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，肯定有人会想知道我们是否可以完全去掉CNN。毕竟，注意力就是一切，对吧？在DETR之后几个月，原始的ViT诞生了。
- en: '![Diagram of the detection transformer (DETR) process illustrating how visual
    tokens and positional encoding are used to identify objects like a temple and
    a tree with confidence levels.](assets/hmls_1602.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![检测变换器（DETR）过程图解，展示了如何使用视觉标记和位置编码来以置信度识别寺庙和树木等物体。](assets/hmls_1602.png)'
- en: Figure 16-2\. The detection transformer (DETR) for object detection
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2\. 用于目标检测的检测变换器（DETR）
- en: The Original ViT
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始ViT
- en: 'In October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([5](ch16.html#id3741))
    that introduced the first vision transformer without a CNN (see [Figure 16-3](#vit_diagram)).
    It was simply named the *vision transformer* (ViT). The idea is surprisingly simple:
    chop the image into little 16 × 16 patches, and treat the sequence of patches
    as if it is a sequence of word representations. In fact, the paper’s title is
    “An Image Is Worth 16 × 16 Words”.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年10月，一组谷歌研究人员发布了一篇[论文](https://homl.info/vit)⁠^([5](ch16.html#id3741))，介绍了第一个没有CNN的视觉变换器（参见[图16-3](#vit_diagram))。它简单地被命名为*视觉变换器*（ViT）。这个想法非常简单：将图像切割成小的16
    × 16补丁，并将补丁序列视为一个词表示序列。实际上，这篇论文的标题是“一张图片值16 × 16个词”。
- en: 'To be more precise, the patches are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors (the 3 is for the RGB color channels). For example, a 224 × 224 image
    gets chopped into 14 × 14 = 196 patches, so we get 196 vectors of 768 dimensions
    each. These vectors then go through a linear layer that projects the vectors to
    the transformer’s embedding size. The resulting sequence of vectors can then be
    treated just like a sequence of word embeddings: add learnable positional embeddings,
    and pass the result to the transformer, which is a regular encoder-only model.
    A class token with a trainable representation is inserted at the start of the
    sequence, and a classification head is added on top of the corresponding output
    (i.e., this is BERT-style classification).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要更精确地说，这些补丁首先被展平成16 × 16 × 3 = 768维度的向量（3代表RGB颜色通道）。例如，一个224 × 224的图像被切割成14
    × 14 = 196个补丁，因此我们得到了196个每个维度为768的向量。这些向量随后通过一个线性层，将向量投影到变换器的嵌入大小。得到的向量序列可以像处理词嵌入序列一样处理：添加可学习的位置嵌入，然后将结果传递给变换器，这是一个常规的仅编码器模型。在序列的开始插入一个具有可训练表示的类别标记，并在相应的输出上方添加一个分类头（即这是BERT风格的分类）。
- en: And that’s it! This model beat the state of the art on ImageNet image classification,
    but to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这个模型在ImageNet图像分类上击败了当时的最佳水平，但公平地说，作者们不得不使用超过3亿张额外的图像进行训练。这很合理，因为变换器不像卷积神经网络那样具有许多*归纳偏差*，因此它们需要额外的数据来学习CNN隐含假设的东西。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    are translation invariant, so they implicitly assume that patterns learned in
    one location will likely be useful in other locations as well. They also have
    a strong bias toward locality. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳偏差是模型由于其架构而做出的隐含假设。例如，线性模型隐含地假设数据是线性的。CNN具有平移不变性，因此它们隐含地假设在一个位置学习到的模式在其他位置也可能有用。它们还强烈倾向于局部性。RNN隐含地假设输入是有序的，并且最近的标记比旧的标记更重要。一个模型具有的归纳偏差越多，假设它们是正确的，那么模型所需的训练数据就越少。但如果隐含的假设是错误的，那么即使在大数据集上训练，模型也可能表现不佳。
- en: '![Diagram illustrating the Vision Transformer (ViT) model, showing the process
    of converting image patches into token embeddings for classification.](assets/hmls_1603.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![说明视觉变换器（ViT）模型图，展示了将图像补丁转换为分类标记嵌入的过程。](assets/hmls_1603.png)'
- en: Figure 16-3\. Vision transformer (ViT) for classification
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-3\. 用于分类的视觉变换器（ViT）
- en: Now you know everything you need to implement a ViT from scratch!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了一切，可以从头开始实现一个ViT了！
- en: Implementing a ViT from scratch using PyTorch
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch从头开始实现ViT
- en: We will start by implementing a custom module to take care of patch embedding.
    For this, we can actually use an `nn.Conv2d` module with `kernel_size` and `stride`
    both set to the patch size (16). This is equivalent to chopping the image into
    patches, flattening them, and passing them through a linear layer (then reshaping
    the result). Just what we need!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现一个自定义模块来处理补丁嵌入。为此，我们可以实际使用一个 `nn.Conv2d` 模块，将 `kernel_size` 和 `stride`
    都设置为补丁大小（16）。这相当于将图像切割成补丁，展平它们，并通过一个线性层（然后重塑结果）。这正是我们所需要的！
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the convolutional layer, we must flatten the spatial dimensions and transpose
    the last two dimensions to ensure the embedding dimension ends up last, which
    is what the `nn.TransformerEncoder` module expects. Now we’re ready to implement
    our ViT model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层之后，我们必须展平空间维度并转置最后两个维度，以确保嵌入维度最终位于最后，这是 `nn.TransformerEncoder` 模块所期望的。现在我们已准备好实现我们的
    ViT 模型：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s go through this code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: The constructor starts by creating the `PatchEmbedding` module.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数首先创建 `PatchEmbedding` 模块。
- en: Then it creates the class token’s trainable embedding, initialized using a normal
    distribution with a small standard deviation (0.02 is common). Its shape is [1,
    1, *E*], where *E* is the embedding dimension.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它创建类别标记的可训练嵌入，使用具有小标准差（0.02 是常见的）的正态分布进行初始化。其形状为 [1, 1, *E*]，其中 *E* 是嵌入维度。
- en: Next, we initialize the positional embeddings, of shape [1, 1 + *L*, *E*], where
    *L* is the number of patch tokens. We need one more positional embedding for the
    class token, hence the 1 + *L*. Again, we initialize it using a normal distribution
    with a small standard deviation.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化位置嵌入，形状为 [1, 1 + *L*, *E*]，其中 *L* 是补丁标记的数量。我们需要一个额外的位置嵌入来处理类别标记，因此是
    1 + *L*。同样，我们使用具有小标准差的正态分布来初始化它。
- en: 'Next, we create the other modules: `nn.Dropout`, `nn.TransformerEncoder` (based
    on an `nn.TransformerEncoderLayer`), `nn.LayerNorm`, and the output linear layer
    that we will use as a classification head.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建其他模块：`nn.Dropout`、`nn.TransformerEncoder`（基于 `nn.TransformerEncoderLayer`）、`nn.LayerNorm`
    以及我们将用作分类头的输出线性层。
- en: In the `forward()` method, we start by creating the patch tokens.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `forward()` 方法中，我们首先创建补丁标记。
- en: Then we replicate the class token along the batch axis, using the `expand()`
    method, and we concatenate the patch tokens. This ensures that each sequence of
    patch tokens starts with the class token.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们使用 `expand()` 方法在批处理轴上复制类别标记，并将补丁标记连接起来。这确保了每个补丁标记序列都以类别标记开头。
- en: 'The rest is straightforward: we add the positional embeddings, apply some dropout,
    run the encoder, keep only the class token’s output (`Z[:, 0]`) and normalize
    it, and lastly pass it through the output layer, which produces the logits.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余部分都很直接：我们添加位置嵌入，应用一些 dropout，运行编码器，只保留类别标记的输出 (`Z[:, 0]`) 并对其进行归一化，最后通过输出层，该层产生
    logits。
- en: 'You can create the model and test it with a random batch of images, like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建模型并使用随机的一批图像进行测试，如下所示：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can then train this model using the `nn.CrossEntropyLoss`, as usual. This
    would take quite a while, however, so unless your image dataset is very domain-specific,
    you’re usually better off downloading a pretrained ViT using the Transformers
    library and then fine-tuning it on your dataset. Let’s see how.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `nn.CrossEntropyLoss` 训练这个模型，就像通常一样。然而，这会花费相当长的时间，所以除非您的图像数据集非常具有领域特定性，否则您通常更好的做法是使用
    Transformers 库下载预训练的 ViT，然后在您的数据集上对其进行微调。让我们看看如何操作。
- en: Fine-tuning a pretrained ViT using the Transformers library
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Transformers 库微调预训练的 ViT
- en: 'Let’s download a small pretrained ViT and fine-tune it on the Oxford-IIIT Pet
    dataset, which contains over 7,000 pictures of pets grouped into 37 different
    classes. First, let’s download the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载一个小型的预训练的 ViT 并在牛津-IIIT Pet 数据集上对其进行微调，该数据集包含超过 7,000 张宠物图片，分为 37 个不同的类别。首先，让我们下载数据集：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s download the ViT:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们下载 ViT：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’re loading a base ViT model that was pretrained on the ImageNet-21k dataset.
    This dataset contains roughly 14 million images across over 21,800 classes. We’re
    using the `ViTForImageClassification` class which automatically replaces the original
    classification head with a new one (untrained) for the desired number of classes.
    That’s the part we now need to train.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在加载一个在 ImageNet-21k 数据集上预训练的基础 ViT 模型。这个数据集包含大约 1400 万张图片，跨越 21,800 多个类别。我们使用
    `ViTForImageClassification` 类，该类会自动用一个新的（未训练的）分类头替换原始的分类头，以适应所需的类别数量。这就是我们现在需要训练的部分。
- en: 'We also loaded the image processor for this model. We will use it to preprocess
    each image as the model expects: it will be rescaled to 224 × 224, pixel values
    will be normalized to range between –1 and 1, and the channel dimension will be
    moved in front of the spatial dimensions. We also set `use_fast=True` because
    a fast implementation of the image processor is available, so we might as well
    use it. The processor takes an image as input and returns a dictionary containing
    a “pixel_values” entry equal to the preprocessed image.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还加载了该模型的图像处理器。我们将使用它来预处理每个图像，正如模型所期望的那样：它将被缩放到224 × 224，像素值将被归一化到-1和1之间，通道维度将被移动到空间维度之前。我们还设置了`use_fast=True`，因为有一个快速的图像处理器实现可用，所以我们不妨使用它。处理器接收一个图像作为输入，并返回一个包含“pixel_values”条目的字典，该条目等于预处理后的图像。
- en: 'Next, we need a data collator that will preprocess all the images in a batch
    and return the images and labels as PyTorch tensors:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个数据收集器，它将预处理一批中的所有图像，并将图像和标签作为PyTorch张量返回：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We set `do_convert_rgb=True` because the model expects RGB images, but some
    images in the dataset are RGBA (i.e., they have an extra transparency channel),
    so we must force the conversion to RGB to avoid an error in the middle of training.
    And now we’re ready to train our model using the familiar Hugging Face training
    API:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了`do_convert_rgb=True`，因为模型期望RGB图像，但数据集中的一些图像是RGBA（即它们有一个额外的透明度通道），因此我们必须强制转换为RGB以避免训练过程中出现错误。现在我们准备好使用熟悉的Hugging
    Face训练API来训练我们的模型：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'By default, the trainer will automatically remove input attributes that are
    not used by the `forward()` method: our model expects `pixel_values` and optionally
    `labels`, but anything else will be dropped, including the `"image"` attribute.
    Since the unused attributes are dropped before the `collate_fn()` function is
    called, the code `example["image"]` will cause an error. This is why we must set
    `remove_unused_columns=False`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练器会自动移除`forward()`方法未使用的输入属性：我们的模型期望`pixel_values`和可选的`labels`，但任何其他内容都将被丢弃，包括`"image"`属性。由于未使用的属性在调用`collate_fn()`函数之前被丢弃，因此`example["image"]`代码将导致错误。这就是为什么我们必须设置`remove_unused_columns=False`。
- en: 'After just 3 epochs, our ViT model reaches about 91.8% accuracy. With some
    data augmentation and more training, you could reach 93 to 95% accuracy, which
    is close to the state of the art. Great! But we’re just getting started: ViTs
    have been improved in many ways since 2020\. In particular, it’s possible to train
    them from scratch in a much more efficient way using distillation. Let’s see how.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仅经过3个epoch，我们的ViT模型就达到了约91.8%的准确率。通过一些数据增强和更多的训练，你可能会达到93%到95%的准确率，这接近了当前的最佳水平。太棒了！但我们才刚刚开始：自2020年以来，ViT在许多方面都得到了改进。特别是，现在可以使用蒸馏以更高效的方式从头开始训练它们。让我们看看如何。
- en: Data-Efficient Image Transformer
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据高效图像转换器
- en: Just two months after Google’s ViT paper was published, a team of Facebook researchers
    released [*data-efficient image transformers*](https://homl.info/deit) (DeiT).⁠^([6](ch16.html#id3760))
    Their DeiT model achieved competitive results on ImageNet without requiring any
    additional data for training. The model’s architecture is virtually the same as
    the original ViT (see [Figure 16-4](#deit_diagram)), but the authors used a distillation
    technique to transfer knowledge from a teacher model to their student ViT model
    (distillation was introduced in [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌的 ViT 论文发布仅仅两个月后，一支Facebook的研究团队发布了[*数据高效图像转换器*](https://homl.info/deit)
    (DeiT)。⁠^([6](ch16.html#id3760)) 他们的DeiT模型在ImageNet上取得了有竞争力的结果，而无需为训练提供任何额外的数据。该模型的架构几乎与原始ViT相同（参见[图16-4](#deit_diagram)），但作者们使用了一种蒸馏技术，将知识从教师模型传递到他们的学生ViT模型（蒸馏技术在[第15章](ch15.html#transformer_chapter)中介绍）。
- en: The authors used a frozen, state-of-the-art CNN as the teacher model. During
    training, they added a special distillation token to the student ViT model. Just
    like the class token, the distillation token representation is trainable, and
    its output goes through a dedicated classification head. Both classification heads
    (for the class token and for the distillation token) are trained simultaneously,
    both using the cross-entropy loss, but the class token’s classification head is
    trained using the normal hard targets (i.e., one-hot vectors), while the distillation
    head is trained using soft targets output by a teacher model. The final loss is
    a weighted sum of both classification losses (typically with equal weights). At
    inference time, the distillation token is dropped, along with its classification
    head. And that’s all there is to it! If you fine-tune a DeiT model on the same
    pets dataset, using `model_id = "facebook/deit-base-distilled-patch16-224"` and
    `DeiTForImageClassification`, you should get around 94.4% validation accuracy
    after just three epochs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了一个冻结的、最先进的CNN作为教师模型。在训练过程中，他们向学生ViT模型添加了一个特殊的蒸馏标记。就像类别标记一样，蒸馏标记的表示是可训练的，其输出通过一个专门的分类头。两个分类头（类别标记和蒸馏标记）同时训练，都使用交叉熵损失，但类别标记的分类头使用正常的硬目标（即one-hot向量）进行训练，而蒸馏头使用教师模型输出的软目标进行训练。最终的损失是两个分类损失（通常权重相等）的加权总和。在推理时间，蒸馏标记及其分类头被丢弃。这就是全部内容！如果你在相同的pets数据集上微调一个DeiT模型，使用`model_id
    = "facebook/deit-base-distilled-patch16-224"`和`DeiTForImageClassification`，你只需经过三个epoch就应该能达到大约94.4%的验证准确率。
- en: '![Diagram illustrating the data-efficient image transformer (DeiT) process,
    showing the flow from image patches through linear transformation, encoding with
    position and class tokens, distillation token inclusion, to classifier heads with
    hard and soft target outputs from a CNN.](assets/hmls_1604.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![说明数据高效图像Transformer (DeiT)过程的图，展示了从图像块通过线性变换、编码位置和类别标记、蒸馏标记包含，到具有来自CNN的硬和软目标输出的分类头的过程。](assets/hmls_1604.png)'
- en: Figure 16-4\. Data-efficient image transformer (DeiT) = ViT + distillation
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4\. 数据高效图像Transformer (DeiT) = ViT + 蒸馏
- en: So far, we have only used ViTs for classification tasks, but what about dense
    prediction tasks such as object detection or semantic segmentation (introduced
    in [Chapter 12](ch12.html#cnn_chapter))? For this, the ViT architecture needs
    to be tweaked a bit; welcome to hierarchical vision transformers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了ViTs进行分类任务，那么对于密集预测任务，如目标检测或语义分割（在第12章中介绍）怎么办呢？为此，ViT架构需要稍作调整；欢迎来到分层视觉Transformer。
- en: Pyramid Vision Transformer for Dense Prediction Tasks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于密集预测任务的Pyramid Vision Transformer
- en: 'The year 2021 was a year of plenty for ViTs: new models advanced the state
    of the art almost every other week. An important milestone was the release of
    the [Pyramid Vision Transformer (PVT)](https://homl.info/pvt) in February 2021,⁠^([7](ch16.html#id3765))
    developed by a team of researchers from Nanjing University, HKU, IIAI, and SenseTime
    Research. They pointed out that the original ViT architecture was good at classification
    tasks, but not so much at dense prediction tasks, where fine-grained resolution
    is needed. To solve this issue, they proposed a pyramidal architecture in which
    the image is processed into a gradually smaller but deeper image (i.e., semantically
    richer), much like in a CNN. [Figure 16-5](#pvt_diagram) shows how a 256 × 192
    image with 3 channels (RGB) is first turned into a 64 × 48 image with 64 channels,
    then into a 32 × 24 image with 128 channels, then a 16 × 12 image with 320 channels,
    and lastly an 8 × 6 image with 512 channels.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年是ViTs（视觉Transformer）丰收的一年：几乎每隔一周就有新的模型推动了技术的进步。一个重要的里程碑是2021年2月发布的[Pyramid
    Vision Transformer (PVT)](https://homl.info/pvt)，由南京大学、香港大学、IIAI和SenseTime Research的研究团队开发。他们指出，原始的ViT架构在分类任务上表现良好，但在需要精细分辨率的大规模预测任务上表现不佳。为了解决这个问题，他们提出了一种金字塔架构，其中图像被处理成越来越小但越来越深的图像（即语义上更丰富），这与CNN非常相似。[图16-5](#pvt_diagram)展示了如何将一个256
    × 192像素、3个通道（RGB）的图像首先转换成一个64 × 48像素、64个通道的图像，然后转换成一个32 × 24像素、128个通道的图像，接着是一个16
    × 12像素、320个通道的图像，最后是一个8 × 6像素、512个通道的图像。
- en: '![Diagram illustrating the Pyramid Vision Transformer process, showing how
    an image is transformed through multiple stages with increasing channels and reduced
    spatial resolution, ultimately used for dense prediction tasks.](assets/hmls_1605.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![说明金字塔视觉Transformer过程的图解，展示了图像如何通过多个阶段进行转换，通道数增加，空间分辨率降低，最终用于密集预测任务。](assets/hmls_1605.png)'
- en: Figure 16-5\. Pyramid Vision Transformer for dense prediction tasks
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-5\. 用于密集预测任务的金字塔视觉Transformer
- en: At each pyramid level, the input image is processed very much like in a regular
    ViT. It is first chopped into patches and turned into a sequence of patch tokens,
    then trainable positional embeddings are added, and the resulting tokens are passed
    through an encoder-only transformer, composed of multiple encoder layers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在金字塔的每一层，输入图像的处理方式与常规 ViT 非常相似。它首先被切割成补丁，并转换成一系列补丁标记，然后添加可训练的位置嵌入，最后将得到的标记通过一个仅编码器的
    transformer，由多个编码器层组成。
- en: Since the encoder outputs a sequence of vectors (i.e., contextualized embeddings),
    this sequence must be reshaped into a *grid* of vectors, which can then be treated
    as an image (with many channels) and passed on to the next level of the pyramid.
    For example, the encoder at the first level receives a sequence of 3,072 patch
    tokens, since the image was chopped into a 64 × 48 grid of 4 × 4 patches (and
    64 × 48 = 3,072). Each patch token is represented as a 64-dimensional vector.
    The encoder also outputs 3,072 vectors (i.e., contextualized embeddings), each
    64-dimensional, and they are organized into a 64 × 48 grid once again. This gives
    us a 64 × 48 image with 64 channels, which can be passed on to the next level.
    In levels 2, 3, and 4 of the pyramid, the patch tokens are 128-, 320-, and 512-dimensional,
    respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码器输出一系列向量（即，上下文化的嵌入），这个序列必须被重塑成一个 *网格* 的向量，然后可以将其视为一个图像（具有许多通道）并传递到金字塔的下一层。例如，第一层的编码器接收一个包含
    3,072 个补丁标记的序列，因为图像被切割成一个 64 × 48 的 4 × 4 补丁网格（64 × 48 = 3,072）。每个补丁标记表示为一个 64
    维的向量。编码器还输出 3,072 个向量（即，上下文化的嵌入），每个 64 维，并且它们被组织成一个 64 × 48 的网格。这给我们提供了一个 64 ×
    48 的图像，具有 64 个通道，可以传递到下一层。在金字塔的第二、三、四层，补丁标记分别是 128 维、320 维和 512 维。
- en: 'Importantly, the patches are much smaller than in the original ViT: instead
    of 16 × 16, they are just 4 × 4 at level 1, and 2 × 2 at levels 2, 3, and 4\.
    These tiny patches offer a much higher spatial resolution, which is crucial for
    dense prediction tasks. However, this comes at a cost: smaller patches means many
    more of them, and since multi-head attention has quadratic complexity, a naive
    adaptation of ViT would require vastly more computation. This is why the PVT authors
    introduced *spatial reduction attention* (SRA): it’s just like MHA except that
    the keys and values are first spatially reduced (but not the queries). For this,
    the authors proposed a sequence of operations that is usually implemented as a
    strided convolutional layer, followed by layer norm (although some implementations
    use an average pooling layer instead).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这些补丁的大小比原始的 ViT 小得多：在第一层，它们只是 4 × 4，而在第二、三、四层则是 2 × 2。这些微小的补丁提供了更高的空间分辨率，这对于密集预测任务至关重要。然而，这也带来了代价：较小的补丁意味着需要更多的它们，而且由于多头注意力的复杂度是二次的，对
    ViT 的简单适配将需要大量的计算。这就是为什么 PVT 作者引入了 *空间减少注意力*（SRA）：它就像 MHA 一样，只是键和值首先在空间上进行了减少（但不是查询）。为此，作者提出了一系列操作，通常实现为一个步长卷积层，随后是层归一化（尽管一些实现使用平均池化层代替）。
- en: 'Let’s look at the impact of SRA at the first level of the pyramid. There are
    3,072 patch tokens. In regular MHA, each of these tokens would attend to every
    token, so we would have to compute 3,072² attention scores: that’s over 9 million
    scores! In SRA, the query is unchanged so it still involves 3,072 tokens, but
    the keys and values are reduced spatially by a factor of 8, both horizontally
    and vertically (in levels 2, 3, and 4 of the pyramid, the reduction factor is
    4, 2, and 1, respectively). So instead of 3,072 tokens representing a 64 × 48
    grid, the keys and values are only composed of 48 tokens representing an 8 × 6
    grid (because 64 / 8 = 8 and 48 / 8 = 6). So we only need to compute 3,072 × 48
    = 147,456 attention scores: that’s 64 times less computationally expensive. And
    the good news is that this doesn’t affect the output resolution since we didn’t
    reduce the query at all: the encoder still output 3,072 tokens, representing a
    64 × 48 image.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看SRA在金字塔第一层的影响。这里有3,072个补丁标记。在常规MHA中，每个标记都会关注每个标记，因此我们需要计算3,072²个注意力分数：这超过900万个分数！在SRA中，查询保持不变，因此仍然涉及3,072个标记，但键和值在空间上减少了8倍，水平和垂直方向都是如此（在金字塔的2、3和4层，减少因子分别是4、2和1）。因此，不再是3,072个标记代表一个64
    × 48的网格，键和值仅由48个标记组成，代表一个8 × 6的网格（因为64 / 8 = 8和48 / 8 = 6）。因此，我们只需要计算3,072 × 48
    = 147,456个注意力分数：这比计算成本降低了64倍。而且好消息是，这不会影响输出分辨率，因为我们根本就没有减少查询：编码器仍然输出3,072个标记，代表一个64
    × 48的图像。
- en: 'OK, so the PVT model takes an image and outputs four gradually smaller and
    deeper images. Now what? How do we use these multiscale feature maps to implement
    object detection or other dense prediction tasks? Well, no need to reinvent the
    wheel: existing solutions generally involve a CNN backbone that produces multiscale
    feature maps, so we can simply swap out this backbone for a PVT (often pretrained
    on ImageNet). For example, we can use an FCN approach for semantic segmentation
    (introduced at the end of [Chapter 12](ch12.html#cnn_chapter)) by upscaling and
    combining the multiscale feature maps output by the PVT, and add a final classification
    head to output one class per pixel. Similarly, we can use a Mask R-CNN for object
    detection and instance segmentation, replacing its CNN backbone with a PVT.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以PVT模型接收一个图像并输出四个逐渐变小且更深的图像。那么接下来呢？我们如何使用这些多尺度特征图来实现目标检测或其他密集预测任务呢？嗯，没有必要重新发明轮子：现有的解决方案通常涉及一个生成多尺度特征图的CNN骨干网络，因此我们可以简单地将其替换为PVT（通常在ImageNet上预训练）。例如，我们可以使用FCN方法进行语义分割（在第12章末尾介绍），通过上采样和组合PVT输出的多尺度特征图，并添加一个最终的分类头以输出每个像素的一个类别。同样，我们可以使用Mask
    R-CNN进行目标检测和实例分割，用PVT替换其CNN骨干网络。
- en: In short, the PVT’s hierarchical structure was a big milestone for vision transformers,
    but despite spatial reduction attention, it’s still computationally expensive.
    The Swin Transformer, released one month later, is much more scalable. Let’s see
    why.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，PVT的层次结构是视觉Transformer的一个重大里程碑，尽管有空间减少注意力，但它仍然计算成本高昂。Swin Transformer在一个月后发布，具有更高的可扩展性。让我们看看原因。
- en: 'The Swin Transformer: A Fast and Versatile ViT'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swin Transformer：一种快速且通用的ViT
- en: 'In March 2021, a team of Microsoft researchers released the [Swin Transformer](https://homl.info/swin).⁠^([8](ch16.html#id3776))
    Just like PVT, it has a hierarchical structure, producing multiscale feature maps
    which can be used for dense prediction tasks. But Swin uses a very different variant
    of multi-head attention: each patch only attends to patches located within the
    same window. This is called *window-based multi-head self-attention* (W-MSA),
    and it allows the cost of self-attention to scale linearly with the image size
    (meaning its area), instead of quadratically.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年3月，一支微软研究团队发布了[Swin Transformer](https://homl.info/swin)。⁠^([8](ch16.html#id3776))
    就像PVT一样，它具有层次结构，生成可用于密集预测任务的多尺度特征图。但Swin使用了一种非常不同的多头注意力变体：每个补丁只关注同一窗口内的补丁。这被称为*基于窗口的多头自注意力*（W-MSA），它允许自注意力的成本与图像大小（即面积）线性缩放，而不是平方缩放。
- en: For example, on the lefthand side of [Figure 16-6](#swin_diagram), the image
    is chopped into a 28 × 28 grid of patches, and these patches are grouped into
    nonoverlapping windows. At the first level of the Swin pyramid, the patches are
    usually 4 × 4 pixels, and each window contains a 7 × 7 grid of patches. So there’s
    a total of 784 patch tokens (28 × 28), but each token only attends to 49 tokens
    (7 × 7), so the W-MSA layer only needs to compute 784 × 49 = 38,416 attention
    scores, instead of 784² = 614,656 scores for regular MHA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图16-6](#swin_diagram)的左侧，图像被切割成28 × 28的补丁网格，这些补丁被分组到非重叠的窗口中。在Swin金字塔的第一层，补丁通常是4
    × 4像素，每个窗口包含一个7 × 7的补丁网格。因此总共有784个补丁标记（28 × 28），但每个标记只关注49个标记（7 × 7），所以W-MSA层只需要计算784
    × 49 = 38,416个关注分数，而不是常规MHA的784² = 614,656个分数。
- en: 'Most importantly, if we double the width and the height of the image, we quadruple
    the number of patch tokens, but each token still attends to only 49 tokens, so
    we just need to compute 4 times more attention scores: the Swin Transformer’s
    computational cost scales linearly with the image’s area, so it can handle large
    images. Conversely, ViT, DeiT, and PVT all scale quadratically: if you double
    the image width and height, the area is quadrupled, and the computational cost
    is multiplied by 16! As a result, these models are way too slow for very large
    images, meaning you must first downsample the image, which may hurt the model’s
    accuracy, especially for dense prediction tasks.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，如果我们将图像的宽度和高度加倍，补丁标记的数量将增加到四倍，但每个标记仍然只关注49个标记，所以我们只需要计算4倍的关注分数：Swin Transformer的计算成本与图像面积成线性关系，因此它可以处理大图像。相反，ViT、DeiT和PVT都呈二次方增长：如果你将图像的宽度和高度加倍，面积将增加到四倍，计算成本将乘以16！因此，这些模型对于非常大的图像来说速度太慢，这意味着你必须首先下采样图像，这可能会损害模型的准确性，尤其是在密集预测任务中。
- en: '![Diagram of the Swin Transformer architecture showing regular multi-head self-attention
    (S-MSA), shifted window multi-head self-attention (SW-MSA), and optimized SW-MSA,
    highlighting how different window configurations cover the same image.](assets/hmls_1606.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Swin Transformer架构图，展示了常规多头自注意力（S-MSA）、移位窗口多头自注意力（SW-MSA）和优化SW-MSA，突出显示了不同的窗口配置如何覆盖相同的图像。](assets/hmls_1606.png)'
- en: 'Figure 16-6\. Swin Transformer: alternates W-MSA (left) and SW-MSA (center);
    SW-MSA can be optimized to require the same number of windows as W-MSA (right)'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-6. Swin Transformer：交替使用W-MSA（左）和SW-MSA（中）；SW-MSA可以被优化为需要与W-MSA相同数量的窗口（右）
- en: 'But wait a minute! If each token only attends to patches within the same window,
    how can we hope to capture long-range patterns? The answer is in the name of the
    architecture, Swin, which stands for *shifted windows*: every other encoder layer
    uses *shifted W-MSA* (SW-MSA), which is just like W-MSA except the windows are
    offset by half a window size. As you can see in the middle of [Figure 16-6](#swin_diagram),
    the windows are shifted by 3 patches toward the bottom right (because half of
    7 is 3.5, which we round down to 3). Why does this help? Well, nearby patches
    that were in separate windows in the previous layer are now in the same window,
    so they can see each other. By alternating W-MSA and SW-MSA, information from
    any part of the image can gradually propagate throughout the whole image. Moreover,
    since the architecture is hierarchical, the patches get coarser and coarser as
    we go up the pyramid, so the information can propagate faster and faster.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！如果每个标记只关注同一窗口内的补丁，我们如何希望捕捉到长距离模式呢？答案就在架构的名字中，Swin，代表*移位窗口*：每隔一个编码器层使用*移位W-MSA*（SW-MSA），这就像W-MSA一样，只是窗口偏移了半个窗口大小。正如你在[图16-6](#swin_diagram)的中间部分可以看到的，窗口向右下角偏移了3个补丁（因为7的一半是3.5，我们将其四舍五入到3）。这有什么帮助呢？好吧，在上一层中分别位于不同窗口的邻近补丁现在在同一窗口中，因此它们可以相互看到。通过交替使用W-MSA和SW-MSA，图像任何部分的信息可以逐渐传播到整个图像。此外，由于架构是分层的，随着我们向上金字塔移动，补丁变得越来越粗糙，因此信息可以传播得越来越快。
- en: 'A naive implementation of SW-MSA would require handling many extra windows.
    For example, if you compare W-MSA and SW-MSA in [Figure 16-6](#swin_diagram),
    you can see that W-MSA uses 16 windows, while SW-MSA uses 25 (at least in this
    example). To avoid this extra cost, the authors proposed an optimized implementation:
    instead of shifting the windows, we shift the image itself and wrap it around
    the borders, as shown in the righthand side of [Figure 16-6](#swin_diagram). This
    way, we’re back to 16 windows. However, this requires careful masking for the
    border windows that contain the wrapped patches; for example, the regions labeled
    ①, ②, ③, ④ should not see each other, even though they are within the same window,
    so an appropriate attention mask must be applied.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SW-MSA的简单实现需要处理许多额外的窗口。例如，如果你比较[图16-6](#swin_diagram)中的W-MSA和SW-MSA，你可以看到W-MSA使用16个窗口，而SW-MSA使用25个（至少在这个例子中）。为了避免这种额外成本，作者提出了一个优化的实现：不是移动窗口，而是移动图像本身并将其包裹在边缘，如图[图16-6](#swin_diagram)的右侧所示。这样，我们回到了16个窗口。然而，这需要对包含包裹补丁的边缘窗口进行仔细的掩码处理；例如，标记为①、②、③、④的区域不应该相互看到，即使它们位于同一个窗口内，因此必须应用适当的注意力掩码。
- en: 'Overall, Swin is harder to implement than PVT, but its linear scaling and excellent
    performance make it one of the best vision transformers out there. But the year
    2021 wasn’t over: [Swin v2](https://homl.info) was released in November 2021.⁠^([9](ch16.html#id3781))
    It improved Swin across the board: more stable training for large ViTs, easier
    to fine-tune on large images, reduced need for labeled data, and more. Swin v2
    is still widely used in vision tasks today.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，Swin比PVT更难实现，但它的线性扩展和优秀性能使其成为最好的视觉Transformer之一。但2021年还没有结束：[Swin v2](https://homl.info)于2021年11月发布。⁠^([9](ch16.html#id3781))它在各个方面都改进了Swin：对大型ViT的更稳定训练，更容易在大图像上进行微调，减少了对标记数据的需要，等等。Swin
    v2至今仍在视觉任务中得到广泛应用。
- en: Our toolbox now contains vision transformers for classification (e.g., ViT and
    DeiT) and for dense prediction tasks (e.g., PVT and Swin). Let’s now explore one
    last pure-vision transformer, DINO, which introduced a revolutionary self-supervision
    technique for visual representation learning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工具箱现在包含了用于分类（例如，ViT 和 DeiT）和密集预测任务（例如，PVT 和 Swin）的视觉Transformer。现在让我们探索最后一个纯视觉Transformer，DINO，它引入了一种革命性的自监督技术，用于视觉表示学习。
- en: 'DINO: Self-Supervised Visual Representation Learning'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DINO：自监督视觉表示学习
- en: In April 2021, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([10](ch16.html#id3788))
    an impressive self-supervised training technique that produces models capable
    of generating excellent image representations. These representations can then
    be used for classification and other tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年4月，Mathilde Caron等人介绍了[DINO](https://homl.info/dino)，^([10](ch16.html#id3788))一种令人印象深刻的自监督训练技术，能够生成能够生成优秀图像表示的模型。这些表示可以用于分类和其他任务。
- en: 'Here’s how it works: the model is duplicated during training, with one network
    acting as a teacher and the other acting as a student (see [Figure 16-7](#dino_diagram)).
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average (EMA) of the student’s weights. This is called a
    *momentum teacher*. The student is trained to match the teacher’s predictions:
    since they’re almost the same model, this is called *self-distillation* (hence
    the name of the model: self-**di**stillation with **no** labels).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的工作原理：在训练过程中，模型被复制，一个网络充当教师，另一个网络充当学生（参见[图16-7](#dino_diagram)）。梯度下降只影响学生，而教师的权重只是学生权重的指数移动平均（EMA）。这被称为*动量教师*。学生被训练以匹配教师的预测：由于它们几乎是同一个模型，这被称为*自蒸馏*（因此模型的名称是self-**di**stillation
    with **no** labels）。
- en: 'At each training step, the input images are augmented in various ways: color
    jitter, grayscale, Gaussian blur, horizontal flipping, and more. Importantly,
    they are augmented in different ways for the teacher and the student: the teacher
    always sees the full image, only slightly augmented, while the student often sees
    only a zoomed-in section of the image, with stronger augmentations. In short,
    the teacher and the student don’t see the same variant of the original image,
    yet their predictions must still match. This forces them to agree on high-level
    representations.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练步骤中，输入图像以各种方式进行增强：颜色抖动、灰度、高斯模糊、水平翻转等。重要的是，它们以不同的方式增强教师和学生：教师总是看到完整图像，仅略有增强，而学生通常只看到图像的放大部分，增强更强烈。简而言之，教师和学生看到的原始图像的变体不同，但他们的预测必须仍然匹配。这迫使他们就高级表示达成一致。
- en: With this mechanism, however, there’s a strong risk of *mode collapse*. This
    is when both the student and the teacher always output the exact same thing, completely
    ignoring the input images. To prevent this, DINO keeps track of a moving average
    of the teacher’s predicted logits, and it subtracts this average from the predicted
    logits. This is called *centering*, forcing the teacher to distribute its predictions
    evenly across all classes (on average, over time).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种机制存在*模式坍塌*的强烈风险。这是当学生和教师始终输出完全相同的东西，完全忽略输入图像时。为了防止这种情况，DINO 跟踪教师预测对数的移动平均值，并从预测对数中减去这个平均值。这被称为*居中*，迫使教师将其预测均匀地分布在所有类别上（平均而言，随着时间的推移）。
- en: 'But centering alone might cause the teacher to simply output the same probability
    for every class, all the time, still ignoring the image. To avoid this, DINO also
    forces the teacher to have high confidence in its highest predictions: this is
    called *sharpening*. It’s implemented by applying a low temperature to the teacher’s
    logits (i.e., dividing them by a temperature smaller than 1). Together, centering
    and sharpening preserve the diversity in the teacher’s outputs; this leaves no
    easy shortcut for the model. It must base its predictions on the actual content
    of the image.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅居中可能会导致教师始终为每个类别输出相同的概率，始终忽略图像。为了避免这种情况，DINO 还迫使教师对其最高预测有很高的信心：这被称为*锐化*。它是通过将教师的对数几率（即，除以小于
    1 的温度）应用低温度来实现的。居中和锐化共同保留了教师输出的多样性；这给模型留下了没有捷径。它必须基于图像的实际内容进行预测。
- en: '![Diagram illustrating the DINO model''s use of teacher and student Vision
    Transformers (ViTs) with centering and sharpening to enhance predictive accuracy
    without labels.](assets/hmls_1607.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![说明 DINO 模型使用教师和学生视觉 Transformer (ViTs) 的图，通过居中和锐化来增强无标签的预测准确性。](assets/hmls_1607.png)'
- en: Figure 16-7\. DINO, or self-distillation with no labels
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-7\. DINO，或无标签的自蒸馏
- en: 'After training, you can drop the teacher: the student is the final DINO model.
    If you feed it a new image, it will output a sequence of contextualized patch
    embeddings. These can be used in various ways. For example, you can train a classifier
    head on top of the class token’s output embedding. In fact, you don’t even need
    a new classifier head: you can run DINO on every training image to get their representation
    (i.e., the output of the class token), then compute the mean representation per
    class. Then, when given a new image, use DINO to compute its representation and
    look for the class with the nearest mean representation. This simple approach
    reaches 78.3% top-1 accuracy on ImageNet, which is pretty impressive.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你可以丢弃教师：学生就是最终的 DINO 模型。如果你给它一个新图像，它将输出一系列上下文化的补丁嵌入。这些嵌入可以用各种方式使用。例如，你可以在类别标记的输出嵌入之上训练一个分类头。实际上，你甚至不需要一个新的分类头：你可以在每个训练图像上运行
    DINO 来获取它们的表示（即类别标记的输出），然后计算每个类别的平均表示。然后，当给定一个新图像时，使用 DINO 来计算其表示，并寻找与最近平均表示最接近的类别。这种方法在
    ImageNet 上达到了 78.3% 的 top-1 准确率，相当令人印象深刻。
- en: But it’s not just about classification! Interestingly, the DINO authors noticed
    that the class token’s attention maps in the last layer often focus on the main
    object of interest in the image, even though they were trained entirely without
    labels! In fact, each attention head seems to focus on a different part of the
    object, as you can see in [Figure 16-8](#unsupervised_segmentation_diagram).⁠^([11](ch16.html#id3797))
    See the notebook for a code example that uses DINO to plot a similar attention
    map.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不仅仅是关于分类！有趣的是，DINO的作者们注意到，在最后一层的类别标记的注意力图中，通常关注图像中感兴趣的主要对象，尽管它们完全是未经标签训练的！实际上，每个注意力头似乎都关注对象的不同部分，正如你在[图16-8](#unsupervised_segmentation_diagram)中可以看到。⁠^([11](ch16.html#id3797))
    请参阅笔记本，其中包含使用DINO绘制类似注意力图的代码示例。
- en: '![Diagram showing unsupervised image segmentation using DINO, with different
    attention heads highlighting various parts of objects like vegetables, a zebra,
    and a truck.](assets/hmls_1608.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![使用DINO进行无监督图像分割的示意图，不同的注意力头突出显示蔬菜、斑马和卡车等对象的不同部分。](assets/hmls_1608.png)'
- en: Figure 16-8\. Unsupervised image segmentation using DINO—different attention
    heads attend to different parts of the main object
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-8\. 使用DINO进行无监督图像分割——不同的注意力头关注主对象的不同部分
- en: Later techniques such as [TokenCut](https://homl.info/tokencut)⁠^([12](ch16.html#id3799))
    built upon DINO to detect and segment objects in images and videos. Then, in April
    2023, Meta released [DINOv2](https://homl.info/dino2),⁠^([13](ch16.html#id3801))
    which was trained on a curated and much larger dataset, and was tweaked to output
    per-patch features, making it a great foundation model not just for classification,
    but also for dense prediction tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 后续技术，如[TokenCut](https://homl.info/tokencut)⁠^([12](ch16.html#id3799))，在DINO的基础上检测和分割图像和视频中的对象。然后在2023年4月，Meta发布了[DINOv2](https://homl.info/dino2)⁠^([13](ch16.html#id3801))，它在精心策划的更大数据集上进行了训练，并调整以输出每个块的特性，使其不仅成为分类的强大基础模型，而且对于密集预测任务也非常有用。
- en: 'Let’s step back: we’ve covered CNN-based transformers such as DETR, followed
    by the original ViT (image patches through an encoder), DeiT (a distilled ViT),
    PVT (a hierarchical ViT with spatial reduction attention), Swin (a hierarchical
    ViT with window-based attention), and DINO (self-distillation with no labels).
    Before we move on to multimodal transformers, let’s quickly go through a few more
    pure-vision transformer models and techniques.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下：我们已经介绍了基于CNN的transformers，如DETR，接着是原始的ViT（通过编码器处理图像块），DeiT（蒸馏ViT），PVT（具有空间减少注意力的分层ViT），Swin（基于窗口的注意力的分层ViT），以及DINO（无标签的自蒸馏）。在我们继续到多模态transformers之前，让我们快速浏览一些更多的纯视觉transformer模型和技术。
- en: Other Major Vision Models and Techniques
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他主要视觉模型和技术
- en: 'Progress in vision transformers has continued steadily to this day. Here is
    a brief overview of some landmark papers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉transformers的进展一直持续到今天。以下是关于一些里程碑论文的简要概述：
- en: '[“Scaling Vision Transformers”](https://homl.info/scalingvits),⁠^([14](ch16.html#id3806))
    June 2021'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[“扩展视觉Transformer”](https://homl.info/scalingvits),⁠^([14](ch16.html#id3806))
    2021年6月'
- en: 'Google researchers showed how to scale ViTs up or down, depending on the amount
    of available data. They managed to create a huge 2 billion parameter model that
    reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a
    scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only
    10,000 images: that’s just 10 images per class!'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Google研究人员展示了如何根据可用数据量扩展或缩小ViT。他们成功地创建了一个包含20亿个参数的巨大模型，在ImageNet上达到了超过90.4%的top-1准确率。相反，他们还训练了一个缩小后的模型，在ImageNet上达到了超过84.8%的top-1准确率，仅使用了10,000张图片：每个类别的图片只有10张！
- en: '[“BEiT: BERT Pre-Training of Image Transformers”](https://homl.info/beit),⁠^([15](ch16.html#id3808))
    June 2021'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[“BEiT：图像Transformer的BERT预训练”](https://homl.info/beit),⁠^([15](ch16.html#id3808))
    2021年6月'
- en: Hangbo Bao et al. proposed a *masked image modeling* (MIM) approach inspired
    from BERT’s masked language modeling (MLM). BEiT is pretrained to reconstruct
    masked image patches from the visible ones. This pretraining technique significantly
    improves downstream tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Hangbo Bao等人提出了一种受BERT的掩码语言建模（MLM）启发的*掩码图像建模*（MIM）方法。BEiT被预训练以从可见图像块重建掩码图像块。这种预训练技术显著提高了下游任务。
- en: Note that BEiT is not trained to predict the raw pixels of the masked patches;
    instead, it must predict the masked token IDs. But where do these token IDs come
    from? Well, the original image is passed through a *discrete variational autoencoder*
    (dVAE, see [Chapter 18](ch18.html#autoencoders_chapter)) which encodes each patch
    into a visual token ID (an integer), from a fixed vocabulary. These are the IDs
    that BEiT tries to predict. The goal is to avoid wasting the model’s capacity
    on unnecessary details.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，BEiT 并未训练来预测遮挡补丁的原始像素；相反，它必须预测遮挡标记 ID。但这些标记 ID 从何而来？嗯，原始图像通过一个 *离散变分自动编码器*（dVAE，见[第
    18 章](ch18.html#autoencoders_chapter)）传递，将每个补丁编码成一个视觉标记 ID（一个整数），来自一个固定词汇表。这些就是
    BEiT 尝试预测的 ID。目标是避免在不必要的细节上浪费模型的能力。
- en: '[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae),⁠^([16](ch16.html#id3816))
    November 2021'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae)，⁠^([16](ch16.html#id3816))
    2021 年 11 月'
- en: 'This paper by a team of Facebook researchers (led by the prolific Kaiming He)
    also proposes a pretraining technique based on masked image modeling, but it removes
    the complexity of BEiT’s dVAE: masked autoencoder (MAE) directly predicts raw
    pixel values. Crucially, it uses an asymmetric encoder-decoder architecture: a
    large encoder processes only the visible patches, while a lightweight decoder
    reconstructs the entire image. Since 75% of patches are masked, this design dramatically
    reduces computational cost and allows MAE to be pretrained on very large datasets.
    This leads to strong performance on downstream tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇由 Facebook 研究团队（由多产的 Kaiming He 领导）撰写的论文也提出了一种基于遮挡图像建模的预训练技术，但它去除了 BEiT 的
    dVAE 的复杂性：遮挡自动编码器（MAE）直接预测原始像素值。关键的是，它使用非对称的编码器-解码器架构：一个大型编码器仅处理可见补丁，而一个轻量级的解码器重建整个图像。由于
    75% 的补丁被遮挡，这种设计大大降低了计算成本，并允许 MAE 在非常大的数据集上进行预训练。这导致了在下游任务上的强大性能。
- en: '[“Model Soups”](https://homl.info/modelsoups),⁠^([17](ch16.html#id3820)) March
    2022'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Model Soups”](https://homl.info/modelsoups)，⁠^([17](ch16.html#id3820)) 2022
    年 3 月'
- en: This paper demonstrated that it’s possible to first train multiple transformers,
    then average their weights to create a new and improved model. This is similar
    to an ensemble (see [Chapter 6](ch06.html#ensembles_chapter)), except there’s
    just one model in the end, which means there’s no inference cost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本文证明了首先训练多个变压器，然后平均它们的权重以创建一个新且改进的模型是可能的。这与集成（见[第 6 章](ch06.html#ensembles_chapter)）类似，但最终只有一个模型，这意味着没有推理成本。
- en: '[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva),⁠^([18](ch16.html#id3821))
    May 2022'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva)，⁠^([18](ch16.html#id3821))
    2022 年 5 月'
- en: EVA is a family of large ViTs pretrained at scale, using enhanced MAE and strong
    augmentations. It’s one of the leading foundation models for ViTs. EVA-02, released
    in March 2023, does just as well or better despite having fewer parameters. The
    large variant has 304M parameters and reaches an impressive 90.0% on ImageNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: EVA 是一系列在规模上预训练的大型 ViT，使用增强的 MAE 和强大的增强。它是 ViT 的领先基础模型之一。2023 年 3 月发布的 EVA-02，尽管参数较少，但表现同样出色甚至更好。大型变体有
    304M 个参数，在 ImageNet 上达到了令人印象深刻的 90.0%。
- en: '[I-JEPA](https://homl.info/ijepa),⁠^([19](ch16.html#id3823)) January 2023'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[I-JEPA](https://homl.info/ijepa)，⁠^([19](ch16.html#id3823)) 2023 年 1 月'
- en: Yann LeCun proposed the joint-embedding predictive architecture (JEPA) in a
    [2022 paper](https://homl.info/jepa),⁠^([20](ch16.html#id3826)) as part of his
    world-model framework, which aims to deepen AI’s understanding of the world and
    improve the reliability of its predictions. I-JEPA is an implementation of JEPA
    for images. It was soon followed by [V-JEPA](https://homl.info/vjepa) in 2024,
    and [V-JEPA 2](https://homl.info/vjepa2) in 2025, both of which process videos.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun 在一篇 2022 年的论文中提出了联合嵌入预测架构（JEPA），⁠^([20](ch16.html#id3826)) 作为他世界模型框架的一部分，旨在加深
    AI 对世界的理解并提高其预测的可靠性。I-JEPA 是 JEPA 的图像实现。2024 年很快推出了 [V-JEPA](https://homl.info/vjepa)，2025
    年推出了 [V-JEPA 2](https://homl.info/vjepa2)，两者都处理视频。
- en: 'During training, JEPA involves two encoders and a predictor: the teacher encoder
    sees the full input (e.g., a photo of a cat) while the student encoder sees only
    part of the input (e.g., the same cat photo but without the ears). Both encoders
    convert their inputs to embeddings, then the predictor tries to predict the teacher
    embedding for the missing part (e.g., the ears) given the student embeddings for
    the rest of the input (e.g., the cat without ears). The student encoder and the
    predictor are trained jointly, while the teacher encoder is just a moving average
    of the student encoder (much like in DINO). JEPA mostly works in embedding space
    rather than pixel space, which makes it fast, parameter efficient, and more semantic.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，JEPA涉及两个编码器和预测器：教师编码器看到完整的输入（例如，一张猫的照片），而学生编码器只看到输入的一部分（例如，同一张猫的照片但没有耳朵）。两个编码器都将它们的输入转换为嵌入，然后预测器试图根据输入其余部分的学生嵌入（例如，没有耳朵的猫）预测缺失部分（例如，耳朵）的教师嵌入。学生编码器和预测器联合训练，而教师编码器只是学生编码器的一个移动平均值（类似于DINO）。JEPA主要在嵌入空间而不是像素空间中工作，这使得它快速、参数高效且更具语义性。
- en: After training, the teacher encoder and the predictor are no longer needed,
    but the student encoder can be used to generate excellent, meaningful representations
    for downstream tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，教师编码器和预测器不再需要，但学生编码器可以用来为下游任务生成优秀且具有意义的表示。
- en: 'The list could go on and on:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表可以一直继续：
- en: NesT or DeiT-III for image classification
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NesT或DeiT-III用于图像分类
- en: MobileViT, EfficientFormer, EfficientViT, or TinyViT for small and efficient
    image classification models (e.g., for mobile devices)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileViT、EfficientFormer、EfficientViT或TinyViT，用于小型高效的图像分类模型（例如，用于移动设备）
- en: Hierarchical transformers like Twins-SVT, FocalNet, MaxViT, and InternImage,
    often used as backbones for dense prediction tasks
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Twins-SVT、FocalNet、MaxViT和InternImage这样的分层Transformer，常被用作密集预测任务的主干网络
- en: Mask2Former or OneFormer for general-purpose segmentation, SEEM for universal
    segmentation, and SAM or MobileSAM for interactive segmentation
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mask2Former或OneFormer用于通用分割，SEEM用于通用分割，SAM或MobileSAM用于交互式分割
- en: ViTDet or RT-DETR for object detection
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViTDet或RT-DETR用于目标检测
- en: TimeSformer, VideoMAE, or OmniMAE for video understanding
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TimeSformer、VideoMAE或OmniMAE用于视频理解
- en: There are also techniques like *token merging* (ToMe) which speeds up inference
    by merging similar tokens on the fly, *token pruning* to drop unimportant tokens
    during processing (i.e., with low attention scores), *early exiting* to only compute
    deep layers for the most important tokens, *patch selection* to select only the
    most informative patches for processing, and self-supervised training techniques
    like SimMIM, iBOT, CAE, or DINOv2, and more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些技术，如*token merging*（ToMe），通过在运行时合并相似标记来加速推理，*token pruning*在处理过程中去除不重要的标记（即，具有低注意力分数的标记），*early
    exiting*只计算最重要的标记的深层，*patch selection*只选择最具有信息量的块进行处理，以及像SimMIM、iBOT、CAE或DINOv2这样的自监督训练技术，等等。
- en: Hopefully we’ve covered a wide enough variety of models and techniques for you
    to be able to explore further on your own.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们已经涵盖了足够多的模型和技术，以便您能够进一步探索。
- en: Tip
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Some of these vision-only models were pretrained on multimodal data (e.g.,
    image-text pairs or input prompts): OmniMAE, SEEM, SAM, MobileSAM, and DINOv2\.
    Which leads us nicely to the second part of this chapter.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些仅用于视觉的模型是在多模态数据上预训练的（例如，图像-文本对或输入提示）：OmniMAE、SEEM、SAM、MobileSAM和DINOv2。这很自然地引出了本章的第二部分。
- en: We already had transformers that could read and write (and chat!), and now we
    have vision transformers that can see. It’s time to build transformers that can
    handle both text and images at the same time, as well as other modalities.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了能够阅读和写作（以及聊天！）的Transformer，现在我们有了能够看到的视觉Transformer。是时候构建能够同时处理文本和图像，以及其他模态的Transformer了。
- en: Multimodal Transformers
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态Transformer
- en: 'Humans are multimodal creatures: we perceive the world through multiple senses—sight,
    hearing, smell, taste, touch, sense of balance, proprioception (i.e., sense of
    body position), and several others—and we act upon the world through movement,
    speech, writing, etc. Each of these modalities can be considered at a very low
    level (e.g., sound waves) or at a higher level (e.g., words, intonations, melody).
    Importantly, modalities are heterogeneous: one modality may be continuous while
    another is discrete, one may be temporal while the other is spatial, one may be
    high-resolution (e.g., 48 kHz audio) while the other is not (e.g., text), one
    may be noisy while the other is clean, and so on.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是多模态生物：我们通过多种感官感知世界——视觉、听觉、嗅觉、味觉、触觉、平衡感、本体感觉（即身体位置感）以及其他几种——并通过运动、言语、书写等方式作用于世界。这些模态可以被视为非常低级（例如，声波）或高级（例如，单词、语调、旋律）。重要的是，模态是异质的：一个模态可能是连续的，而另一个是离散的；一个可能是时间的，而另一个是空间的；一个可能是高分辨率的（例如，48
    kHz音频），而另一个则不是（例如，文本）；一个可能是嘈杂的，而另一个是干净的，等等。
- en: 'Moreover, modalities may interact in various ways. For example, when we chat
    with someone, we may listen to their voice while also watching the movement of
    their lips: these two modalities (auditory and visual) carry overlapping information,
    which helps our brain better parse words. But multimodality is not just about
    improving the signal/noise ratio: facial expressions may carry their own meaning
    (e.g., smiles and frowns), and different modalities may combine to produce a new
    meaning. For example, if you say “he’s an expert” while rolling your eyes or gesturing
    air quotes, you’re clearly being ironic, which inverts the meaning of your sentence
    and conveys extra information (e.g., humor or disdain) which neither modality
    possesses on its own.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模态可能以各种方式相互作用。例如，当我们与人交谈时，我们可能同时听到他们的声音并观察他们嘴唇的运动：这两种模态（听觉和视觉）携带重叠的信息，这有助于我们的大脑更好地解析单词。但多模态不仅仅是提高信号/噪声比：面部表情可能携带它们自己的意义（例如，微笑和皱眉），不同的模态可能结合产生新的意义。例如，如果你说“他是个专家”同时翻白眼或做出引号手势，你显然是在讽刺，这颠倒了你的句子的意义并传达了额外信息（例如，幽默或轻蔑），这些信息任何一个模态单独都不具备。
- en: So multimodal machine learning requires designing models that can handle very
    heterogeneous data and capture their interactions. There are two main challenges
    for this. The first is called *fusion*, and it’s about finding a way to combine
    different modalities, for example, by encoding them into the same representation
    space. The second is called *alignment*, where the goal is to discover the relationships
    between modalities. For example, perhaps you have a recording of a speech, as
    well as a text transcription, and you want to find the timestamp of each word.
    Or you want to find the most relevant object in an image given a text query such
    as “the dog next to the tree” (this is called *visual grounding*). Many other
    common tasks involve two or more modalities, such as image captioning, image search,
    visual question answering (VQA), speech-to-text (STT), text-to-speech (TTS), embodied
    AI (i.e., a model capable of physically interacting with the environment), and
    much more.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，多模态机器学习需要设计能够处理非常异质数据并捕捉它们之间相互作用的模型。这有两个主要挑战。第一个被称为**融合**，它涉及到找到一种方法来结合不同的模态，例如，通过将它们编码到同一个表示空间中。第二个被称为**对齐**，其目标是发现模态之间的关系。例如，你可能有一个语音的录音，以及相应的文本转录，你想要找到每个单词的时间戳。或者，你想要根据文本查询“树旁的狗”找到图像中最相关的对象（这被称为**视觉定位**）。许多其他常见任务涉及两个或更多模态，例如图像标题、图像搜索、视觉问答（VQA）、语音转文本（STT）、文本转语音（TTS）、具身人工智能（即能够与物理环境进行物理交互的模型）等等。
- en: 'Multimodal machine learning has been around for decades, but progress has recently
    accelerated thanks to deep learning, and particularly since the rise of transformers.
    Indeed, transformers can ingest pretty much any modality, as long as you can chop
    it into a sequence of meaningful tokens (e.g., text into words, images into small
    patches, audio or video into short clips, etc.). Once you have prepared a sequence
    of token embeddings, you’re ready to feed it to a transformer. Embeddings from
    different modalities can be fused in various ways: summed up, concatenated, passed
    through a fusion encoder, and more. This can take care of the fusion problem.
    And transformers also have multi-head attention, which is a powerful tool to detect
    and exploit complex patterns, both within and across modalities. This can take
    care of the alignment problem.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态机器学习已经存在了几十年，但由于深度学习，尤其是自变压器兴起以来，最近进展加速。确实，只要你能将其切割成一系列有意义的标记（例如，将文本切割成单词，将图像切割成小块，将音频或视频切割成短剪辑等），变压器几乎可以处理任何模态。一旦你准备好了标记嵌入的序列，你就可以将其输入到变压器中。不同模态的嵌入可以通过各种方式融合：相加、连接、通过融合编码器传递，等等。这可以解决融合问题。而且，变压器还有多头注意力，这是一个强大的工具，可以检测和利用复杂模式，无论是模态内部还是跨模态。这可以解决对齐问题。
- en: Researchers quickly understood the potential of transformers for multimodal
    architectures. The first multimodal transformers were released just months after
    the original Transformer paper was released in early 2018 with image captioning,
    video captioning, and more. Let’s look at some of the most impactful multimodal
    transformer architectures, starting with VideoBERT.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员很快理解了变压器在多模态架构中的潜力。第一个多模态变压器是在2018年初原Transformer论文发布后的几个月内发布的，包括图像字幕、视频字幕等。让我们看看一些最有影响力的多模态变压器架构，从VideoBERT开始。
- en: 'VideoBERT: A BERT Variant for Text plus Video'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoBERT：一种用于文本加视频的BERT变体
- en: 'In April 2019, Google researchers released [VideoBERT](https://homl.info/videobert).⁠^([21](ch16.html#id3840))
    As its name suggests, this model is very similar to BERT, except it can handle
    both text and videos. In fact, the authors just took a pretrained BERT-large model,
    extended its embedding matrix to allow for extra video tokens (more on this shortly),
    and continued training the model using self-supervision on a text plus video training
    set. This dataset was built from a large collection of instructional YouTube videos,
    particularly cooking videos. These videos typically involve someone describing
    a sequence of actions while performing them (e.g., “Cut the tomatoes into thin
    slices like this”). To feed these videos to VideoBERT, the authors had to encode
    the videos into both text and visual sequences (see [Figure 16-9](#videobert_encoding_diagram)):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年4月，谷歌研究人员发布了[VideoBERT](https://homl.info/videobert)。⁠^([21](ch16.html#id3840))
    如其名所示，这个模型与BERT非常相似，但它可以处理文本和视频。事实上，作者只是将预训练的BERT-large模型扩展到允许额外的视频标记（稍后会有更多介绍），并继续使用文本加视频训练集上的自监督训练模型。这个数据集是从大量教学YouTube视频中构建的，特别是烹饪视频。这些视频通常涉及某人描述执行一系列动作的同时进行（例如，“像这样将番茄切成薄片”）。为了将这些视频输入到VideoBERT中，作者必须将视频编码成文本和视觉序列（参见[图16-9](#videobert_encoding_diagram))）：
- en: For the visual modality, they extracted nonoverlapping 1.5-second clips at 20
    frames per second (i.e., 30 frames each), and they passed these clips through
    a 3D CNN named S3D. This CNN is based on Inception modules and separable convolutions
    (see [Chapter 12](ch12.html#cnn_chapter)), and it was pretrained on the Kinetics
    dataset composed of many YouTube videos of people performing a wide range of actions.
    The authors added a 3D average pooling layer on top of S3D to get a 1,024-dimensional
    vector for each video clip. Each vector encodes fairly high-level information
    about the video clip.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于视觉模态，他们提取了非重叠的1.5秒剪辑，每秒20帧（即每个剪辑30帧），并将这些剪辑通过一个名为S3D的3D CNN。这个CNN基于Inception模块和可分离卷积（参见[第12章](ch12.html#cnn_chapter))，并在包含许多YouTube视频中人们执行各种动作的Kinetics数据集上进行了预训练。作者在S3D之上添加了一个3D平均池化层，为每个视频剪辑得到一个1,024维的向量。每个向量编码了关于视频剪辑相当高级的信息。
- en: To extract the text from the videos, the authors used YouTube’s internal speech-to-text
    software, after which they dropped the audio tracks from the videos. Then they
    separated the text into sentences by adding punctuation using an off-the-shelf
    LSTM model. Finally, they preprocessed and tokenized the text just like for BERT.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了从视频中提取文本，作者使用了YouTube的内部语音到文本软件，之后他们从视频中删除了音频轨道。然后，他们通过添加标点符号使用现成的LSTM模型将文本分离成句子。最后，他们像对BERT一样预处理和标记化文本。
- en: '![Diagram illustrating the VideoBERT process, showing how actions in a video
    are converted into text and visual tokens using S3D for video clips and speech-to-text
    for audio.](assets/hmls_1609.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![说明VideoBERT过程的图解，展示了如何使用S3D对视频剪辑进行视频到文本转换，以及使用语音到文本对音频进行转换，将视频中的动作转换为文本和视觉标记。](assets/hmls_1609.png)'
- en: Figure 16-9\. VideoBERT—encoding a video into a text sequence and a visual sequence
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-9\. VideoBERT—将视频编码为文本序列和视觉序列
- en: 'Great! We now have a text token sequence describing some actions, and a sequence
    of vectors representing video clips of these actions. However, we have a problem.
    Recall that BERT is pretrained using MLM, where the model must predict masked
    tokens from a fixed vocabulary. We do have a fixed vocabulary for the text tokens,
    but not for the video tokens. So let’s build one! For this, the authors gathered
    all the visual vectors produced by S3D over their training set, and they clustered
    these vectors into *k* = 12 clusters using *k*-means (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
    Then they used *k*-means again on each cluster to get 12² = 144 clusters, then
    again and again to get 12⁴ = 20,736 clusters. This process is called *hierarchical
    k-means*, and it’s much faster than running *k*-means just once using *k* = 20,736,
    plus it typically produces much better clusters. Now each vector can be replaced
    with its cluster ID: this way, each video clip is represented by a single ID from
    a fixed visual vocabulary, so the whole video is now represented as one sequence
    of visual token IDs (e.g., 194, 3912, …​), exactly like tokenized text. In short,
    we’ve gone from a continuous 1,024-dimensional space down to a discrete space
    with just 20,736 possible values. There’s a lot of information loss at this step,
    but VideoBERT’s excellent performance suggests that much of the important information
    remains.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们现在有一个描述一些动作的文本标记序列，以及代表这些动作视频剪辑的向量序列。然而，我们遇到了一个问题。回想一下，BERT是使用MLM进行预训练的，其中模型必须从固定词汇表中预测被掩码的标记。我们确实有一个用于文本标记的固定词汇表，但没有用于视频标记的。所以，让我们构建一个吧！为此，作者收集了S3D在他们的训练集中产生的所有视觉向量，并使用k-均值（见第8章[第8章](ch08.html#unsupervised_learning_chapter)）将这些向量聚类成k
    = 12个簇。然后，他们在每个簇上再次使用k-均值，得到12² = 144个簇，然后又再次这样做，得到12⁴ = 20,736个簇。这个过程被称为**层次k-均值**，它比只使用k
    = 20,736运行k-均值一次要快得多，而且通常会产生更好的簇。现在，每个向量都可以用其簇ID来替换：这样，每个视频剪辑都由固定视觉词汇中的一个单一ID来表示，因此整个视频现在由一个视觉标记ID序列（例如，194，3912，…）来表示，就像标记化文本一样。简而言之，我们已经从连续的1,024维空间下降到只有20,736个可能值的离散空间。在这个步骤中会有大量的信息损失，但VideoBERT出色的性能表明，大部分重要信息仍然保留。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since the authors used a pretrained BERT-large model, the text token embeddings
    were already excellent before VideoBERT’s additional training even started. For
    the visual token embeddings, rather than using trainable embeddings initialized
    from scratch, the authors used frozen embeddings initialized using the 1,024-dimensional
    vector representations of the *k*-means cluster centroids.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于作者使用了预训练的BERT-large模型，因此在VideoBERT的额外训练开始之前，文本标记嵌入已经非常优秀了。对于视觉标记嵌入，作者没有使用从零开始初始化的可训练嵌入，而是使用了使用k-均值簇质心的1,024维向量表示初始化的冻结嵌入。
- en: 'The authors used three different training regimes: text-only, video-only, and
    text plus video. In text-only and video-only modes, VideoBERT was fed a single
    modality and trained to predict masked tokens (either text tokens or video tokens).
    For text plus video, the model was fed both text tokens and video tokens, simply
    concatenated (plus an unimportant separation token in between), and it had to
    predict whether the text tokens and video tokens came from the same part of the
    original video. This is called *linguistic-visual alignment*. For this, the authors
    added a binary classification head on top of the class token’s output (this replaces
    BERT’s next sentence prediction head). For negative examples, the authors just
    sampled random sentences and video segments. [Figure 16-10](#videobert_pretraining_diagram)
    shows all three modes at once, but keep in mind that they are actually separate.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了三种不同的训练模式：仅文本、仅视频和文本加视频。在仅文本和仅视频模式下，VideoBERT只接受单一模态输入，并训练预测掩码标记（文本标记或视频标记）。对于文本加视频，模型同时接受文本标记和视频标记，简单拼接（中间加上一个不重要的分隔标记），并需要预测文本标记和视频标记是否来自原始视频的同一部分。这被称为*语言-视觉对齐*。为此，作者在类别标记输出之上添加了一个二分类头（这取代了BERT的下一句预测头）。对于负例，作者只是随机采样句子和视频片段。[图16-10](#videobert_pretraining_diagram)同时显示了所有三种模式，但请注意，它们实际上是分开的。
- en: '![Diagram illustrating the VideoBERT model, highlighting masked token prediction
    and linguistic-visual alignment processes.](assets/hmls_1610.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![说明VideoBERT模型，突出掩码标记预测和语言-视觉对齐过程的图。](assets/hmls_1610.png)'
- en: Figure 16-10\. VideoBERT—pretraining using masked token prediction and linguistic-visual
    alignment (shown together but actually separate)
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-10\. VideoBERT—使用掩码标记预测和语言-视觉对齐进行预训练（显示在一起但实际上是分开的）
- en: Linguistic-visual alignment is a noisy task since the cook may explain something
    that they have already finished or will do later, so the authors concatenated
    random neighboring sentences to give the model more context. The authors had a
    few more tricks up their sleeves, such as randomly changing the video sampling
    rate to make the model more robust to different action speeds, since some cooks
    are faster than others; see the paper for more details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 语言-视觉对齐是一个有噪声的任务，因为厨师可能会解释他们已经完成或将要做的某事，因此作者将随机相邻的句子拼接起来，以给模型提供更多上下文。作者还有一些其他的技巧，例如随机改变视频采样率，使模型对不同的动作速度更加鲁棒，因为有些厨师比其他人更快；更多细节请参阅论文。
- en: 'This was a lot of work, but the authors were finally done: they had a fully
    trained VideoBERT model. To demonstrate its effectiveness, they evaluated VideoBERT
    on some downstream tasks, including:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项大量工作，但作者最终完成了：他们拥有一个完全训练好的VideoBERT模型。为了展示其有效性，他们在一些下游任务上评估了VideoBERT，包括：
- en: Zero-shot action classification
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本动作分类
- en: 'Given a video clip, figure out which action is performed, without fine-tuning
    VideoBERT. The authors achieved this by feeding the video to VideoBERT, along
    with the following masked sentence: “Now let me show you how to [MASK] the [MASK]”.
    Then they looked at the output probabilities for both masked tokens, for each
    possible pair of verb and noun. If the video shows a cook slicing some tomatoes,
    then the probability of “slice” and “tomatoes” will be much higher than “bake”
    and “cake” or “boil” and “egg”.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个视频片段，确定执行了哪个动作，而不需要微调VideoBERT。作者通过将视频输入到VideoBERT中，并附带以下掩码句子：“现在让我给你演示如何
- en: Video captioning
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 视频字幕
- en: 'Given a video clip, generate a caption. To do this, the authors used the earliest
    [video-captioning transformer architecture](https://homl.info/videocaption),⁠^([22](ch16.html#id3846))
    but they replaced the input to the encoder with visual features output by VideoBERT.
    More specifically, they took an average of VideoBERT’s final output representations,
    including the representations of all of the visual tokens and the masked-out text
    tokens. The masked sentence they used was: “now let’s [MASK] the [MASK] to the
    [MASK], and then [MASK] the [MASK]”. After fine-tuning this new model, they obtained
    improved results over the original captioning model.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个视频片段，生成一个字幕。为此，作者使用了最早的[视频字幕Transformer架构](https://homl.info/videocaption)⁠^([22](ch16.html#id3846))，但他们用VideoBERT输出的视觉特征替换了编码器的输入。更具体地说，他们取了VideoBERT最终输出表示的平均值，包括所有视觉标记的表示和被遮蔽的文本标记的表示。他们使用的遮蔽句子是：“现在让我们把刀切向番茄，然后切向土豆，最后切向洋葱”。在微调这个新模型后，他们获得了比原始字幕模型更好的结果。
- en: 'Using similar approaches, VideoBERT can be adapted for many other tasks, such
    as multiple-choice visual question answering: given an image, a question, and
    multiple possible answers, the model must find the correct answer. For example:
    “What is the cook doing?” → “Slicing tomatoes”. For this, one approach is to simply
    run VideoBERT on each possible answer, along with the video, and compare the linguistic-visual
    alignment scores: the correct answer should have the highest score.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的方法，VideoBERT可以适应许多其他任务，例如多选题视觉问答：给定一个图像、一个问题以及多个可能的答案，模型必须找到正确的答案。例如：“厨师在做什么？”→“切番茄”。为此，一种方法是在每个可能的答案上简单地运行VideoBERT，包括视频，并比较语言-视觉对齐得分：正确的答案应该有最高的得分。
- en: 'The success of VideoBERT inspired many other BERT-based multimodal transformers,
    many of which were released in August and September 2019: ViLBERT, VisualBERT,
    Unicoder-VL, LXMERT, VL-BERT, and UNITER. Most of these are single-stream models
    like VideoBERT, meaning that the modalities are fused very early in the network,
    typically by simply concatenating the sequences. However, ViLBERT and LXMERT are
    dual-stream transformers, meaning that each modality is processed by its own encoder,
    with a mechanism allowing the encoders to influence each other. This lets the
    model better understand each modality before trying to make sense of the interactions
    between them. VilBERT was particularly influential, so let’s look at it more closely.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: VideoBERT的成功激励了许多基于BERT的多模态Transformer，其中许多在2019年8月和9月发布：ViLBERT、VisualBERT、Unicoder-VL、LXMERT、VL-BERT和UNITER。其中大多数是单流模型，类似于VideoBERT，意味着模态在网络的早期就融合在一起，通常是通过简单地连接序列。然而，ViLBERT和LXMERT是双流Transformer，意味着每个模态都由自己的编码器处理，并有一个机制允许编码器相互影响。这使得模型在尝试理解它们之间的相互作用之前，能更好地理解每个模态。ViLBERT尤其有影响力，因此让我们更详细地看看它。
- en: 'ViLBERT: A Dual-Stream Transformer for Text plus Image'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViLBERT：一种用于文本加图像的双流Transformer
- en: ViLBERT was [proposed in August 2019](https://homl.info/vilbert)⁠^([23](ch16.html#id3852))
    by a team of researchers from the Georgia Institute of Technology, Facebook AI
    Research, and Oregon State University. They pointed out that the single-stream
    approach (used by VideoBERT and many others) treats both modalities identically,
    even though they may require different levels of processing. For example, if the
    visual features come from a deep CNN, then we already have good high-level visual
    features, whereas the text will need much more processing before the model has
    access to high-level text features. Moreover, the researchers hypothesized that
    “image regions may have weaker relations than words in a sentence”.⁠^([24](ch16.html#id3853))
    Lastly, BERT was initially pretrained using text only, so forcing it to process
    other modalities may give suboptimal results and even damage its weights during
    multimodal training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ViLBERT是由乔治亚理工学院、Facebook AI Research和俄勒冈州立大学的研究团队在2019年8月[提出](https://homl.info/vilbert)⁠^([23](ch16.html#id3852))的。他们指出，单流方法（VideoBERT和许多其他方法所使用）对两种模态的处理是相同的，尽管它们可能需要不同级别的处理。例如，如果视觉特征来自深度CNN，那么我们已经有很好的高级视觉特征，而文本在模型能够访问高级文本特征之前需要更多的处理。此外，研究人员假设“图像区域可能比句子中的单词关系更弱”。⁠^([24](ch16.html#id3853))
    最后，BERT最初仅使用文本进行预训练，因此强迫它处理其他模态可能会得到次优结果，甚至可能在多模态训练期间损坏其权重。
- en: 'So the authors chose a dual-stream approach instead: each modality goes through
    its own encoder, and in the upper layers the two encoders are connected and exchange
    information through a new bidirectional cross-attention mechanism called *co-attention*
    (see [Figure 16-11](#co_attention_diagram)). Specifically, in each pair of connected
    encoder layers, the MHA query of one encoder is used as the MHA key/value by the
    other encoder.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者选择了一种双流方法：每个模态都通过自己的编码器，在上层，两个编码器通过一个新的双向交叉注意力机制连接并交换信息，称为*共注意力*（见[图16-11](#co_attention_diagram)）。具体来说，在每一对连接的编码层中，一个编码器的MHA查询被另一个编码器用作MHA键/值。
- en: '![Diagram of connected vision and text co-encoder layers using co-attention,
    where the multi-head attention (MHA) query from one encoder is used as the key/value
    by the other encoder.](assets/hmls_1611.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![使用共注意力连接的视觉和文本共编码层图，其中一个编码器的多头注意力（MHA）查询被另一个编码器用作键/值。](assets/hmls_1611.png)'
- en: 'Figure 16-11\. Two encoder layers connected through co-attention: the MHA query
    of one encoder is used as the MHA key/value by the other encoder'
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-11\. 通过共注意力连接的两个编码层：一个编码器的MHA查询被另一个编码器用作MHA键/值
- en: 'The lower layers of the text encoder are initialized with BERT’s weights (the
    authors used a BERT base, which has 12 layers), and 6 co-attention layers sit
    on top (see the lower-right quadrant of [Figure 16-12](#vilbert_diagram)). The
    visual features are produced by a pretrained and frozen Faster R-CNN model, and
    it is assumed that these features are sufficiently high level so no further processing
    is needed; therefore, the visual encoder is exclusively composed of six co-attention
    layers, paired up with the text encoder’s six co-attention layers (see the lower-left
    quadrant of the figure). The Faster R-CNN model’s outputs go through a mean pooling
    layer for each detected region, so we get one feature vector per region, and low-confidence
    regions are dropped: each image ends up represented by 10 to 36 vectors.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器的底层使用BERT的权重初始化（作者使用了一个BERT基础模型，它有12层），并在其上方放置了6个共注意力层（见[图16-12](#vilbert_diagram)的右下象限）。视觉特征由一个预训练并冻结的Faster
    R-CNN模型产生，并假设这些特征足够高级，因此不需要进一步处理；因此，视觉编码器仅由六个共注意力层组成，与文本编码器的六个共注意力层配对（见图的左下象限）。Faster
    R-CNN模型的输出为每个检测到的区域通过一个均值池化层，因此我们得到每个区域的特征向量，并且低置信度区域被丢弃：每个图像最终由10到36个向量表示。
- en: '![Diagram illustrating the ViLBERT model''s pretraining process for masked
    token prediction and linguistic-visual alignment, highlighting the interaction
    between the visual and text encoders and their integration for classification
    tasks.](assets/hmls_1612.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![说明ViLBERT模型使用掩码标记预测和语言-视觉对齐进行预训练的图，突出视觉编码器和文本编码器之间的交互以及它们在分类任务中的集成。](assets/hmls_1612.png)'
- en: Figure 16-12\. ViLBert pretraining using masked token prediction and linguistic-visual
    alignment (again, shown together but actually separate)
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-12\. ViLBert使用掩码标记预测和语言-视觉对齐进行预训练（再次，一起显示但实际上是分开的）
- en: 'Since regions don’t have a natural order like words do, the visual encoder
    does not use positional encoding. Instead, it uses spatial encodings that are
    computed like this: each region’s bounding box is encoded as a 5D vector containing
    the normalized upper-left and lower-right coordinates, and the ratio of the image
    covered by the bounding box. Then this 5D vector is linearly projected up to the
    same dimensionality as the visual vector, and simply added to it.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于区域没有像单词那样的自然顺序，视觉编码器不使用位置编码。相反，它使用如下计算的空间编码：每个区域的边界框被编码为一个包含归一化左上角和右下角坐标以及边界框覆盖的图像比例的5D向量。然后，这个5D向量被线性投影到与视觉向量相同的维度，并简单地添加到它上面。
- en: 'Lastly, a special [IMG] token is prepended to the visual sequence: it serves
    the same purpose as the class token (i.e., to produce a representation of the
    whole sequence), but instead of being a trainable embedding, it’s computed as
    the average of the feature vectors (before spatial encoding), plus the spatial
    encoding for a bounding box covering the whole image.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个特殊的[IMG]标记被添加到视觉序列的开头：它具有与类别标记相同的作用（即，生成整个序列的表示），但它不是一个可训练的嵌入，而是计算为特征向量的平均值（在空间编码之前），加上覆盖整个图像的边界框的空间编码。
- en: 'Now on to training! Similar to VideoBERT, the authors used masked token prediction
    and linguistic-visual alignment:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到训练！与VideoBERT类似，作者使用了掩码标记预测和语言-视觉对齐：
- en: 'For masked token prediction, the authors used regular BERT-like MLM for the
    text encoder. However, for the visual encoder, since ViLBERT does not use a fixed-size
    visual vocabulary (there’s no clustering step), the model is trained to predict
    the class distribution that the CNN predicts for the given image region (this
    is a soft target). The authors chose this task rather than predicting raw pixels
    because the regions can be quite large and there’s typically not enough information
    in the surrounding regions and in the text to reconstruct the masked region correctly:
    it’s better to aim for a higher-level target.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于掩码标记预测，作者在文本编码器中使用了类似于BERT的常规MLM。然而，对于视觉编码器，由于ViLBERT不使用固定大小的视觉词汇表（没有聚类步骤），模型被训练来预测CNN对给定图像区域预测的类别分布（这是一个软目标）。作者选择这个任务而不是预测原始像素，因为区域可能相当大，并且通常周围区域和文本中的信息不足以正确重建掩码区域：目标是追求更高层次的目标。
- en: For linguistic-visual alignment, the model takes the outputs of the [IMG] and
    [CLS] tokens, then computes their itemwise product and passes the result to a
    binary classification head that must predict whether the text and image match.
    Multiplication is preferred over addition because it amplifies features that are
    strong in both representations (a bit like a logical AND gate), so it better captures
    alignment.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语言-视觉对齐，模型取[IMG]和[CLS]标记的输出，然后计算它们的逐项乘积，并将结果传递给必须预测文本和图像是否匹配的二分类头。乘法优于加法，因为它放大了在两种表示中都很强的特征（有点像逻辑AND门），因此更好地捕捉了对齐。
- en: 'And that’s it. This model significantly beat the state of the art for several
    downstream tasks, including image grounding, caption-based image retrieval (even
    zero-shot), visual question answering, and *visual commonsense reasoning* (VCR)
    which involves answering a multiple-choice question about an image (like VQA),
    then selecting the appropriate justification. For example, given an image of a
    waiter serving some pancakes at a table, along with the question “Why is person
    #4 pointing at person #1”, the model must choose the correct answer “He is telling
    person #3 that person #1 ordered the pancakes”, then it must choose the justification
    “Person #3 is serving food and they might not know whose order is whose”.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。该模型在包括图像定位、基于描述的图像检索（甚至零样本）、视觉问答和*视觉常识推理*（VCR）在内的多个下游任务上显著超越了现有技术。VCR涉及回答关于图像的多项选择题（如VQA），然后选择适当的理由。例如，给定一张服务员在桌子上为一些煎饼服务的图像，以及问题“为什么第4个人指着第1个人”，模型必须选择正确的答案“他正在告诉第3个人第1个人点了煎饼”，然后它必须选择理由“第3个人正在上菜，他们可能不知道谁的订单是谁的”。
- en: 'ViLBERT had a strong influence on the field of multimodal machine learning
    thanks to its dual-stream architecture, the invention of co-attention, and its
    excellent results on many downstream tasks. It was another great demonstration
    of the power of large-scale self-supervised pretraining using transformers. The
    next major milestone came in 2021, and it approached the problem very differently,
    using contrastive pretraining: meet CLIP.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ViLBERT由于其双流架构、协同注意力的发明以及在许多下游任务上的出色结果，对多模态机器学习领域产生了强烈影响。这是大规模自监督预训练使用transformers的又一伟大展示。下一个重大里程碑出现在2021年，它以非常不同的方式处理这个问题，使用了对比预训练：遇见CLIP。
- en: 'CLIP: A Dual-Encoder Text plus Image Model Trained with Contrastive Pretraining'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP：使用对比预训练训练的双编码器文本加图像模型
- en: 'OpenAI’s January 2021 release of [contrastive language–image pretraining (CLIP)](https://homl.info/clip)⁠^([25](ch16.html#id3870))
    was a major breakthrough, not just for its astounding capabilities, but also because
    of its surprisingly straightforward approach based on *contrastive learning*:
    the model learns to encode text and images into vector representations that are
    similar when the text and image match, and dissimilar when they don’t match.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2021年1月发布的[对比语言-图像预训练（CLIP）](https://homl.info/clip)⁠^([25](ch16.html#id3870))是一个重大突破，不仅因为其惊人的能力，还因为其基于*对比学习*的出人意料简单的方法：模型学习将文本和图像编码成向量表示，当文本和图像匹配时相似，不匹配时不相似。
- en: 'Once trained, the model can be used for many tasks, particularly zero-shot
    image classification. For example, CLIP can be used as an insect classifier without
    any additional training: just start by feeding all the possible class names to
    CLIP, such as “cricket”, “ladybug”, “spider”, and so on, to get one vector representation
    for each class name. Then, whenever you want to classify an image, feed it to
    CLIP to get a vector representation, and find the most similar class name representation
    using cosine similarity. This usually works even better if the text resembles
    typical image captions found on the web, since this is what CLIP was trained on,
    for example, “This is a photo of a ladybug” instead of just “ladybug”. A bit of
    prompt engineering can help (i.e., experimenting with various prompt templates).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，该模型可以用于许多任务，尤其是零样本图像分类。例如，CLIP可以用作昆虫分类器，而无需任何额外训练：只需将所有可能的类名输入到CLIP中，例如“蟋蟀”、“瓢虫”、“蜘蛛”等等，为每个类名获得一个向量表示。然后，无论何时你想对图像进行分类，只需将其输入到CLIP中，以获得一个向量表示，然后使用余弦相似度找到最相似类名表示。这通常在文本类似于网络上常见的典型图像标题时效果更好，因为CLIP就是这样训练的，例如，“这是一张瓢虫的照片”而不是仅仅“瓢虫”。一点提示工程可以帮助（即，尝试各种提示模板）。
- en: 'The good news is that CLIP is fully open source,⁠^([26](ch16.html#id3873)),
    several pretrained models are available on the Hugging Face Hub, and the Transformers
    library provides a convenient pipeline for zero-shot image classification:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，CLIP是完全开源的⁠^([26](ch16.html#id3873))，Hugging Face Hub上有几个预训练模型可用，Transformers库提供了一个方便的零样本图像分类管道：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we provided a prompt template, so the model will actually encode
    “This is a photo of a ladybug”, not just “ladybug” (if you don’t provide any template,
    the pipeline actually defaults to “This is a photo of a {}”.). Now let’s look
    at the results, which are sorted by score:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们提供了一个提示模板，因此模型实际上会编码“这是一张瓢虫的照片”，而不仅仅是“瓢虫”（如果你没有提供任何模板，该管道实际上默认为“这是一张{}的照片”。）现在让我们看看结果，这些结果按分数排序：
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Great! CLIP predicts ladybug with over 99.7% confidence. Now if you want a flower
    classifier instead, just replace the candidate labels with names of flowers. If
    you include “dandelion” in the list and classify the same image, the model should
    choose “dandelion” with high confidence (ignoring the ladybug). Impressive!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！CLIP以超过99.7%的置信度预测了瓢虫。现在，如果你想有一个花卉分类器，只需将候选标签替换为花卉的名称。如果你在列表中包含“蒲公英”并分类相同的图像，模型应该以高置信度选择“蒲公英”（忽略瓢虫）。令人印象深刻！
- en: 'So how does this magic work? Well, CLIP’s architecture is based on a regular
    text encoder and a regular vision encoder, no co-attention or anything fancy (see
    [Figure 16-13](#clip_diagram)). You can actually use pretty much any text and
    vision encoders you want, as long as they can produce a vector representation
    of the text or image. The authors experimented with various encoders, including
    several ResNet and ViT models for vision, and a GPT-2-like model for text, all
    trained from scratch. What’s that I hear you say, GPT-2 is not an encoder? That’s
    true, it’s a decoder-only model, but we’re not pretraining it for next token prediction,
    so the last token’s output is free to be used as a representation of the entire
    input sequence, which is what CLIP does. You may wonder why we’re not using a
    regular text encoder like BERT? Well, we could, but OpenAI created GPT—Alex Radford
    is the lead author of both GPT and CLIP—so that’s most likely why GPT-2 was chosen:
    the authors simply had more experience with this model and a good training infrastructure
    already in place. Using a causal encoder also makes it possible to cache the intermediate
    state of the model when multiple texts start in the same way; for example, “This
    is a photo of a”.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个魔法是如何工作的呢？好吧，CLIP的架构基于一个常规文本编码器和常规视觉编码器，没有共注意力或任何花哨的东西（见[图16-13](#clip_diagram)）。实际上，你可以使用几乎任何你想要的文本和视觉编码器，只要它们可以产生文本或图像的向量表示。作者尝试了各种编码器，包括几个ResNet和ViT模型用于视觉，以及一个类似GPT-2的模型用于文本，所有这些都是从零开始训练的。你听到我说GPT-2不是一个编码器吗？这是真的，它是一个仅解码器模型，但我们不是为下一个标记预测进行预训练，所以最后一个标记的输出可以自由地用作整个输入序列的表示，这正是CLIP所做的事情。你可能想知道为什么我们不使用像BERT这样的常规文本编码器？好吧，我们可以，但OpenAI创建了GPT——Alex
    Radford是GPT和CLIP的首席作者——所以这很可能是选择GPT-2的原因：作者只是对这个模型有更多的经验，并且已经建立了一个良好的训练基础设施。使用因果编码器还使得在多个文本以相同方式开始时缓存模型的中间状态成为可能；例如，“这是一张瓢虫的照片”。
- en: '![Diagram illustrating the CLIP model with text and vision encoders processing
    image-caption pairs into vectors to match corresponding pairs and differentiate
    mismatched ones.](assets/hmls_1613.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![说明CLIP模型中文本和视觉编码器处理图像-文本对以将它们转换为向量以匹配对应对并区分不匹配对。](assets/hmls_1613.png)'
- en: 'Figure 16-13\. CLIP: a batch of image-caption pairs is encoded as vectors,
    then matching pairs are pulled closer while mismatched pairs are pushed away'
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-13\. CLIP：将一批图像-文本对编码为向量，然后匹配的向量被拉近，不匹配的向量被推开
- en: Also note that a pooling layer is added on top of the vision encoder to ensure
    it outputs a single vector for the whole image instead of feature maps. Moreover,
    a linear layer is added on top of each encoder to project the final representation
    into the same output space (i.e., with the same number of dimensions). So given
    a batch of *m* image-caption pairs, we get *m* vector representations for the
    images and *m* vector representations for the captions, and all vectors have the
    same number of dimensions. [Figure 16-13](#clip_diagram) shows *m* = 4, but the
    authors used a shockingly large batch size of *m* = 2^(15) = 32,768 during training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在视觉编码器之上添加了一个池化层，以确保它输出整个图像的单个向量，而不是特征图。此外，在每个编码器之上添加了一个线性层，将最终表示投影到相同的输出空间（即具有相同数量的维度）。因此，给定*m*个图像-文本对批次，我们得到*m*个图像的向量表示和*m*个文本的向量表示，所有向量具有相同数量的维度。[图16-13](#clip_diagram)显示了*m*
    = 4，但在训练期间，作者使用了令人震惊的大批次大小*m* = 2^(15) = 32,768。
- en: 'The model was then pretrained on a large dataset of 400 million image-caption
    pairs scraped from the internet, using a contrastive loss⁠^([27](ch16.html#id3874))
    that pulls together the representations of matching pairs, while also pushing
    apart representations of mismatched pairs. Here’s how it works:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型随后在从互联网上抓取的包含4亿个图像-文本对的庞大数据集上进行了预训练，使用对比损失⁠^([27](ch16.html#id3874))将匹配对的表示拉近，同时将不匹配对的表示推开。以下是它是如何工作的：
- en: 'All vectors are first ℓ[2] normalized, meaning they are rescaled to unit vectors:
    we only care about their orientation, not their length.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有向量首先进行ℓ[2]归一化，这意味着它们被缩放为单位向量：我们只关心它们的方向，而不是它们的长度。
- en: Next, we compute the cosine similarity of the image representation and the text
    representation for every possible image-caption pair. The result is an *m* × *m*
    matrix containing numbers between –1 for opposite vectors, and +1 for identical
    vectors. In [Figure 16-13](#clip_diagram), this matrix is represented by the 4
    × 4 grid (black is +1, white is –1). Each column measures how much each image
    in the batch matches a given caption in the same batch, while each row measures
    how much each caption matches a given image.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个可能的图像-文本对图像表示和文本表示之间的余弦相似度。结果是包含介于-1（对于相反向量）和+1（对于相同向量）之间的数字的*m* ×
    *m*矩阵。在[图16-13](#clip_diagram)中，这个矩阵由4 × 4网格表示（黑色为+1，白色为-1）。每一列衡量每个批次中的图像与同一批次中给定文本的匹配程度，而每一行衡量每个文本与给定图像的匹配程度。
- en: Since the *i*^(th) image corresponds to the *i*^(th) caption, we want the main
    diagonal of this matrix to contain similarity scores close to +1, while all other
    scores should be close to 0\. Why not close to –1? Well, if an image and a text
    are totally unrelated, we can think of their representations as two random vectors.
    Recall that two random high-dimensional vectors are highly likely to be close
    to orthogonal (as discussed in [Chapter 7](ch07.html#dimensionality_chapter)),
    so their cosine similarity will be close to 0, not –1\. In other words, it makes
    sense to assume that the text and image representations of a mismatched pair are
    unrelated (score close to 0), not opposite (score close to –1).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于第*i*个图像对应于第*i*个文本，我们希望这个矩阵的主对角线包含接近+1的相似度分数，而所有其他分数应接近0。为什么不是接近-1呢？好吧，如果一个图像和文本完全无关，我们可以将它们的表示视为两个随机向量。回想一下，两个随机的高维向量很可能接近正交（如第7章所述），因此它们的余弦相似度将接近0，而不是-1。换句话说，假设不匹配对的文本和图像表示无关（分数接近0），而不是相反（分数接近-1）是有意义的。
- en: In the *i*^(th) row, we know that the matching caption is in the *i*^(th) column,
    so we want the model to produce a high similarity score in that column, and a
    low score elsewhere. This resembles a classification task where the target class
    is the *i*^(th) class. Indeed, we can treat each similarity score as class logit
    and simply compute the cross-entropy loss for that row with *i* as the target.
    We can follow the exact same rationale for each column. If we compute the cross-entropy
    loss for each row and each column (using class *i* as the target for the *i*^(th)
    row and the *i*^(th) column), and evaluate the mean, we get the final loss.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 *i*^(th) 行中，我们知道匹配的标题在第 *i*^(th) 列，因此我们希望模型在该列产生高相似度分数，在其他地方产生低分数。这类似于一个分类任务，其中目标类别是第
    *i*^(th) 类。实际上，我们可以将每个相似度分数视为类别 logit，并简单地计算该行的交叉熵损失，其中 *i* 作为目标。我们可以对每一列采用完全相同的推理。如果我们为每一行和每一列计算交叉熵损失（使用类别
    *i* 作为第 *i*^(th) 行和第 *i*^(th) 列的目标），并计算平均值，我们得到最终的损失。
- en: 'There’s just one extra technical detail: the similarity scores range between
    –1 and +1, which is unlikely to be the ideal logit scale for the task, so CLIP
    divides all the similarity scores by a trainable temperature (a scalar) before
    computing the loss.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个额外的技术细节：相似度分数的范围在 –1 和 +1 之间，这不太可能是任务的理想 logit 尺度，因此 CLIP 在计算损失之前将所有相似度分数除以一个可训练的温度（一个标量）。
- en: Warning
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This loss requires a large batch size to ensure the model sees enough negative
    examples to contrast with the positive examples, or else it could overfit details
    in the positive examples. CLIP’s success is due in part to the gigantic batch
    size that the authors were able to implement.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失需要一个大的批量大小以确保模型看到足够的负例来与正例进行对比，否则它可能会过度拟合正例的细节。CLIP 的成功部分归因于作者能够实现的巨大的批量大小。
- en: The authors evaluated CLIP on many image classification datasets, and for roughly
    60% of these, it performed better without any extra training (i.e., zero-shot)
    than a *linear probe* trained on ResNet-50 features (that’s a linear classifier
    trained on features output by a pretrained and frozen ResNet-50 model), including
    on ImageNet, despite the fact that the ResNet-50 model was actually pretrained
    on ImageNet. CLIP is particularly strong on datasets with few examples per class,
    with pictures of everyday scenes (i.e., the kind of pictures you find on the web).
    In fact, CLIP even beat the state of the art on the Stanford Cars dataset, ahead
    of the best ViTs specifically trained on this dataset, because pictures of cars
    are very common on the web and the dataset doesn’t have many examples per class.
    However, CLIP doesn’t perform as well on domain-specific images, such as satellite
    or medical images.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在许多图像分类数据集上评估了 CLIP，对于其中大约 60% 的数据集，它在没有任何额外训练（即零样本）的情况下表现优于在 ResNet-50 特征上训练的
    *线性探针*（这是一个在预训练并冻结的 ResNet-50 模型输出的特征上训练的线性分类器），包括在 ImageNet 上，尽管 ResNet-50 模型实际上是在
    ImageNet 上预训练的。CLIP 在每个类别只有少量示例的数据集上特别强大，例如日常场景的图片（即你在网上找到的那种图片）。事实上，CLIP 甚至在斯坦福汽车数据集上击败了当时最先进的
    ViTs，因为汽车图片在网络上非常常见，而且数据集每个类别的示例并不多。然而，CLIP 在特定领域的图像上表现不佳，例如卫星或医学图像。
- en: 'Importantly, the visual features output by CLIP are also highly robust to perturbations,
    making them excellent for downstream tasks, such as image retrieval: if you store
    images in a vector database, indexing them by their CLIP-encoded visual features,
    you can then search for them given either a text query or an image query. For
    this, just run the query through CLIP to get a vector representation, then search
    the database for images with a similar representation.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，CLIP 输出的视觉特征对扰动也非常鲁棒，这使得它们非常适合下游任务，例如图像检索：如果你将图像存储在向量数据库中，通过它们的 CLIP 编码的视觉特征进行索引，那么你可以根据文本查询或图像查询来搜索它们。为此，只需将查询通过
    CLIP 获取向量表示，然后在具有相似表示的图像中进行数据库搜索。
- en: 'To get the text and visual features using the Transformers library, you must
    run the CLIP model directly, without going through a pipeline:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Transformers 库获取文本和视觉特征，您必须直接运行 CLIP 模型，而不是通过管道：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tip
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you need to encode the images and text separately, you can use the CLIP model’s
    `get_image_features()` and `get_text_features()` methods. You must first tokenize
    the text using a `CLIPTokenizer` and process the images using a `CLIPImageProcessor`.
    The resulting features are not ℓ[2] normalized, so you must divide them by `features.norm(dim=1,
    keepdim=True)` (see the notebook for a code example).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要分别对图像和文本进行编码，可以使用CLIP模型的`get_image_features()`和`get_text_features()`方法。你必须首先使用`CLIPTokenizer`对文本进行标记化，并使用`CLIPImageProcessor`处理图像。生成的特征没有进行ℓ[2]归一化，因此你必须将它们除以`features.norm(dim=1,
    keepdim=True)`（请参阅笔记本中的代码示例）。
- en: 'The features are already ℓ[2] normalized, so if you want to compute similarity
    scores, a single matrix multiplication is all you need:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 特征已经进行了ℓ[2]归一化，因此如果你要计算相似度分数，只需要一次矩阵乘法即可：
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This works because matrix multiplication computes the dot products of every
    row vector in the first matrix with every column vector in the second, and each
    dot product is equal to the cosine of the angle between the vectors multiplied
    by the norms of the vectors. Since the vectors have been ℓ[2] normalized in this
    case, the norms are equal to 1, so the result is just the cosine of the angle,
    which is the similarity score we’re after. As you can see, the most similar representation
    is the second one, for the ladybug class. If you prefer estimated probabilities
    rather than similarity scores, you must first rescale the similarities using the
    model’s learned temperature, then pass the result through the softmax function
    (it’s nice to see that we get the same result as the pipeline):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效，是因为矩阵乘法计算了第一个矩阵中每一行向量与第二个矩阵中每一列向量的点积，每个点积等于向量之间角度的余弦值乘以向量的范数。由于在这种情况下向量已经进行了ℓ[2]归一化，范数等于1，所以结果只是角度的余弦值，这正是我们想要的相似度分数。正如你所见，最相似的表现是第二个，对于瓢虫类别。如果你更喜欢估计概率而不是相似度分数，你必须首先使用模型学习到的温度对相似度进行缩放，然后将结果通过softmax函数（很高兴看到我们得到了与管道相同的结果）：
- en: '[PRE11]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: CLIP wasn’t the only surprise OpenAI had in stock in 2021\. Just the following
    month, OpenAI announced DALL·E, which can generate impressive images given a text
    description. Let’s discuss it now.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP并不是OpenAI在2021年唯一的惊喜。就在下一个月，OpenAI宣布了DALL·E，它可以根据文本描述生成令人印象深刻的图像。现在让我们来讨论它。
- en: 'DALL·E: Generating Images from Text Prompts'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DALL·E：从文本提示生成图像
- en: 'OpenAI [DALL·E](https://homl.info/dalle),⁠^([28](ch16.html#id3883)) released
    in February 2021, is a model capable of generating images based on text prompts,
    such as “an armchair in the shape of an avocado”. Its architecture is quite simple
    (see the lefthand side of [Figure 16-14](#dalle_diagram)): a GPT-like model trained
    to predict the next token, but unlike GPT, it was pretrained on millions of image-caption
    pairs, and fed input sequences composed on text tokens followed by visual tokens.
    At inference time, you only feed it the text tokens, and the model then generates
    the visual tokens, one at a time, until you get the full image. The visual tokens
    are generated by a dVAE model, which takes an image and outputs a sequence of
    tokens from a fixed vocabulary. Sadly, the model was never released to the public,
    but the paper was detailed enough so some open source replications are available,
    such as [DALL·E mini](https://huggingface.co/dalle-mini), also known as Craiyon.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2021年2月发布的[DALL·E](https://homl.info/dalle),⁠^([28](ch16.html#id3883))是一个能够根据文本提示生成图像的模型，例如“一个形状像鳄梨的扶手椅”。其架构相当简单（参见[图16-14](#dalle_diagram)的左侧）：一个类似于GPT的模型，经过训练以预测下一个标记，但与GPT不同，它是在数百万个图像-标题对上预训练的，并输入由文本标记后跟视觉标记组成的序列。在推理时，你只需提供文本标记，然后模型逐个生成视觉标记，直到生成完整的图像。视觉标记是由一个dVAE模型生成的，它接受一个图像并输出一个来自固定词汇表的标记序列。遗憾的是，该模型从未向公众发布，但论文描述得足够详细，因此有一些开源复制品可用，例如[DALL·E
    mini](https://huggingface.co/dalle-mini)，也称为Craiyon。
- en: 'One year later, in April 2022, OpenAI released [DALL·E 2](https://homl.info/dalle2),⁠^([29](ch16.html#id3886))
    able to generate even higher quality images. Its architecture is actually very
    different: the text is fed to a CLIP model which outputs a text embedding, then
    this text embedding is fed to a *diffusion model* which uses it to guide its image
    generation process (we will discuss diffusion models in [Chapter 18](ch18.html#autoencoders_chapter)).
    The model is not open source, but it’s available through a paid API, and via some
    products such as Microsoft Designer, Bing Image Creator, Canva, ChatGPT, and more.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，即2022年4月，OpenAI发布了[DALL·E 2](https://homl.info/dalle2)，⁠^([29](ch16.html#id3886))，能够生成更高质量的图像。其架构实际上非常不同：文本被输入到CLIP模型中，该模型输出文本嵌入，然后这个文本嵌入被输入到一个*扩散模型*中，该模型使用它来指导其图像生成过程（我们将在[第18章](ch18.html#autoencoders_chapter)中讨论扩散模型）。该模型不是开源的，但可以通过付费API获取，并通过一些产品，如Microsoft
    Designer、Bing Image Creator、Canva、ChatGPT等。
- en: '![Diagram comparing the architectures of DALL·E and DALL·E 2, illustrating
    the transition from text embedding via a decoder and dVAE (for DALL·E) to CLIP
    and diffusion models (for DALL·E 2) in generating the image of a Chinese tower.](assets/hmls_1614.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![比较DALL·E和DALL·E 2架构的图表，展示了从通过解码器和dVAE（对于DALL·E）到CLIP和扩散模型（对于DALL·E 2）生成中国塔图像的文本嵌入的过渡](assets/hmls_1614.png)'
- en: Figure 16-14\. DALL·E (left) and DALL·E 2 (right)
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-14。DALL·E（左）和DALL·E 2（右）
- en: 'DALL·E 3 was released in October 2023\. Sadly, by then OpenAI had fully shifted
    away from its initial openness: there was no peer-reviewed paper, no code, no
    weights, no data. Like the previous version, DALL·E 3 is available through an
    API and via some products. We know it’s diffusion-based, it doesn’t use CLIP,
    and it’s tightly integrated with GPT-4, which rewrites the prompt before generating
    the image. It works impressively well: it outputs stunning images which match
    the prompts much more precisely than previous versions. The difference is particularly
    striking for *compositional prompts* (e.g., “A fluffy white cat sitting on a red
    velvet cushion, with a vase of sunflowers behind it, bathed in golden hour light.
    The cat is looking directly at the viewer”.). DALL·E 1 and 2 would generally follow
    only one or two elements of such prompts, whereas DALL·E 3 follows instructions
    much more closely. The image quality, realism, artistic style, and consistency
    are astounding. Lastly, DALL·E 3 also integrates some moderation capabilities.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: DALL·E 3于2023年10月发布。遗憾的是，到那时，OpenAI已经完全转变了其最初的开源策略：没有经过同行评审的论文，没有代码，没有权重，没有数据。与上一版本一样，DALL·E
    3可以通过API和一些产品获取。我们知道它是基于扩散的，不使用CLIP，并且与GPT-4紧密集成，GPT-4在生成图像之前会重写提示。它的工作效果令人印象深刻：它输出的图像令人惊叹，与提示的匹配程度比之前的版本更加精确。这种差异对于*组合提示*（例如，“一只毛茸茸的白色猫坐在一个红色的天鹅绒垫子上，后面是一个盛满向日葵的花瓶，沐浴在金色时光中。猫正直视观众”。）尤其明显。DALL·E
    1和2通常只会遵循此类提示中的一个或两个元素，而DALL·E 3则更紧密地遵循指令。图像质量、逼真度、艺术风格和一致性令人惊叹。最后，DALL·E 3还集成了某些监管能力。
- en: 'The next landmark in our multimodal journey came one month after the first
    DALL·E model: the Perceiver.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多模态旅程的下一个里程碑是在第一个DALL·E模型发布后一个月出现的：Perceiver。
- en: 'Perceiver: Bridging High-Resolution Modalities with Latent Spaces'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Perceiver：通过潜在空间连接高分辨率模态
- en: 'Every transformer so far has required chopping the inputs into meaningful tokens.
    In the case of text, tokens represent words or subwords. In the case of ViTs,
    they represent 16 × 16 pixel patches. In VideoBERT, it’s short 1.5-second clips.
    In audio transformers, it’s short audio clips. If we fed individual characters,
    pixels, or audio frames directly into a transformer, the input sequence would
    be extremely long, and we would run into the quadratic attention problem. Also,
    we would lose important inductive biases: for example, by chopping an image into
    patches, we enforce a strong inductive bias toward proximity (i.e., nearby pixels
    are assumed to be more strongly correlated than distant pixels).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个Transformer都需要将输入切割成有意义的标记。在文本的情况下，标记代表单词或子单词。在ViTs的情况下，它们代表16×16像素的补丁。在VideoBERT中，是短1.5秒的剪辑。在音频Transformer中，是短音频剪辑。如果我们直接将单个字符、像素或音频帧输入到Transformer中，输入序列会非常长，我们会遇到二次注意力问题。此外，我们还会失去重要的归纳偏差：例如，通过将图像切割成补丁，我们强制执行了强烈的归纳偏差，即邻近性（即，附近的像素被认为比远处的像素具有更强的相关性）。
- en: However, such tokenization is modality-specific, which makes it harder to deal
    with new modalities or mix them in the model. Moreover, inductive biases are great
    when you don’t have a lot of training data (assuming the biases are correct),
    but if your dataset is large, you will often get better performance by using unbiased
    models with very few implicit assumptions. Sure, the model will have to figure
    out on its own that nearby pixels are generally related, but on the other hand,
    it will be flexible enough to discover patterns that might otherwise go unnoticed.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种标记化是模态特定的，这使得处理新的模态或将其混合到模型中变得更加困难。此外，当您没有大量训练数据时（假设偏差是正确的），归纳偏差非常有用，但如果您的数据集很大，您通常会通过使用具有非常少隐含假设的无偏模型获得更好的性能。当然，模型将不得不自己找出附近的像素通常相关，但另一方面，它将足够灵活，可以发现可能被忽视的模式。
- en: 'This is why DeepMind introduced the [*Perceiver*](https://homl.info/perceiver)⁠^([30](ch16.html#id3898))
    in March 2021\. This architecture is capable of directly handling any modality
    at the lowest level: characters, pixels, audio frames, and more. Moreover, it
    does so with a modality-agnostic design, so the same model can handle different
    modalities. The Perceiver architecture is shown in [Figure 16-15](#perceiver_diagram).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，DeepMind 在 2021 年 3 月推出了 [*Perceiver*](https://homl.info/perceiver)⁠^([30](ch16.html#id3898))。这种架构能够在最低级别直接处理任何模态：字符、像素、音频帧等。此外，它以模态无关的设计来实现这一点，因此相同的模型可以处理不同的模态。Perceiver
    架构在 [图 16-15](#perceiver_diagram) 中展示。
- en: '![Diagram of the Perceiver architecture showing the flow of input pixels through
    Fourier positional encoding, linear encoding, and conversion into pixel tokens,
    which interact with latent tokens through cross-attention layers, leading to a
    classification head.](assets/hmls_1615.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![Perceiver 架构图，展示了输入像素通过傅里叶位置编码、线性编码和转换为像素标记的流程，这些标记通过交叉注意力层与潜在标记交互，最终导致分类头。](assets/hmls_1615.png)'
- en: 'Figure 16-15\. Perceiver architecture: inputs are ingested through cross-attention
    layers, while the main input is a sequence of learned latent tokens'
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-15\. Perceiver 架构：输入通过交叉注意力层摄取，而主要输入是一系列学习到的潜在标记
- en: 'Let’s walk through this architecture:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个架构：
- en: 'The input is first chopped into its smallest constituents. In this example,
    the input is an image, so it is chopped into individual pixels: we now have a
    sequence of 3D vectors (red, green, blue).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入首先被分割成其最小的组成部分。在这个例子中，输入是一个图像，因此它被分割成单个像素：我们现在有一个 3D 向量序列（红色、绿色、蓝色）。
- en: Positional encodings are concatenated to these feature vectors. Perceiver uses
    Fourier positional encodings, which are very similar to the sinusoidal positional
    encodings of the original Transformer, except they encode all of the input’s dimensions.
    Since an image is 2D, each pixel’s horizontal and vertical coordinates are encoded;
    for example, if a pixel is located at coordinates *x* and *y* (normalized between
    –1 and 1), then the positional encoding vector will include *x* and *y*, followed
    by sin(π_fx_), sin(π_fy_), and cos(π_fx_), cos(π_fy_) repeated *K* times (typically
    6) with the frequency *f* starting at 1 and going up to *μ* / 2 (spaced equally),
    where *μ* is the target resolution (e.g., if the image is 224 × 224 pixels, then
    *μ* = 224).⁠^([31](ch16.html#id3901)) The dimensionality of the positional encoding
    vector is *d*(2_K_ + 1), where *d* is the number of input dimensions (i.e., 1
    for audio, 2 for images, 3 for videos, etc.).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码被连接到这些特征向量中。Perceiver 使用傅里叶位置编码，这与原始 Transformer 的正弦位置编码非常相似，但它们编码了输入的所有维度。由于图像是二维的，每个像素的水平坐标和垂直坐标都会被编码；例如，如果一个像素位于坐标
    *x* 和 *y*（归一化在 –1 和 1 之间），那么位置编码向量将包括 *x* 和 *y*，然后是 sin(π_fx_)，sin(π_fy_)，cos(π_fx_)，cos(π_fy_)，重复
    *K* 次（通常是 6 次）频率 *f* 从 1 开始，增加到 *μ* / 2（均匀分布），其中 *μ* 是目标分辨率（例如，如果图像是 224 × 224
    像素，则 *μ* = 224）。⁠^([31](ch16.html#id3901)) 位置编码向量的维度是 *d*(2^K_ + 1)，其中 *d* 是输入维度的数量（即音频为
    1，图像为 2，视频为 3 等）。
- en: The pixel tokens now have 3 + 2 × (2 × 6 + 1) = 29 dimensions. We then pass
    them through a linear layer to project them to the Perceiver’s dimensionality
    (e.g., 512).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在像素标记有 3 + 2 × (2 × 6 + 1) = 29 维。然后我们通过一个线性层将它们投影到 Perceiver 的维度（例如，512）。
- en: The Perceiver’s architecture itself is composed of repeated processing blocks
    (e.g., eight), where each block is composed of a single cross-attention multi-head
    attention layer (MHA) followed by a regular transformer encoder (e.g., with six
    encoder layers). The final block is composed of a single cross-attention MHA layer
    and an average pooling layer to reduce the input sequence into a single vector,
    which is then fed to a classification head (i.e., linear plus softmax).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器的架构本身由重复的处理块（例如，八个）组成，其中每个块由一个交叉注意力多头注意力层（MHA）和一个常规的变换器编码器（例如，具有六个编码器层）组成。最后的块由一个交叉注意力MHA层和一个平均池化层组成，以将输入序列减少到一个单一的向量，然后将其输入到分类头（即线性加softmax）。
- en: The pixel tokens are fed to the Perceiver exclusively through the MHA layers,
    and they play the role of the keys and values. In other words, the Perceiver attends
    to the pixel tokens through cross-attention only.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素标记仅通过MHA层输入到感知器中，并扮演键和值的角色。换句话说，感知器仅通过交叉注意力关注像素标记。
- en: 'Crucially, the Perceiver’s main input is a fairly short sequence of *latent
    tokens* (e.g., 512). These tokens are similar to an RNN’s hidden state: an initial
    sequence (learned during training) is fed to the Perceiver, and it gradually gets
    updated as the model learns more and more about the pixel tokens via cross-attention.
    Since it’s a short sequence, it doesn’t suffer much from the quadratic attention
    problem. This is called the *latent bottleneck trick*, and is the key to the success
    of the Perceiver.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的是，感知器的主要输入是一个相当短的潜在标记序列（例如，512）。这些标记类似于RNN的隐藏状态：一个初始序列（在训练期间学习）被输入到感知器中，并且随着模型通过交叉注意力学习越来越多的关于像素标记的信息，它逐渐得到更新。由于这是一个短序列，它不会受到二次注意力问题的很大影响。这被称为“潜在瓶颈技巧”，是感知器成功的关键。
- en: The authors experimented sharing weights across processing blocks (excluding
    the first cross-attention layer), and they got good results. When the processing
    blocks share the same weights, the Perceiver is effectively a recurrent neural
    network, and the latent tokens really are its hidden state.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们尝试了在处理块之间共享权重（不包括第一个交叉注意力层），并且取得了良好的结果。当处理块共享相同的权重时，感知器实际上是一个循环神经网络，而潜在标记确实是其隐藏状态。
- en: Note
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As we saw in [Chapter 7](ch07.html#dimensionality_chapter), the manifold hypothesis
    states that most real-world data lives near a low-dimensional manifold, much like
    a rolled piece of paper lives in 3D but is essentially a 2D object. This 2D space
    is latent (i.e., hidden, potential) until we unroll the paper. Similarly, the
    Perceiver’s goal is to “unroll” its high-dimensional inputs so the model can work
    in the latent space, using low-dimensional representations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第7章](ch07.html#dimensionality_chapter)中看到的，流形假设指出，大多数现实世界的数据都生活在一个低维流形附近，就像一张卷起的纸张生活在三维空间中，但本质上是一个二维对象。这个二维空间是潜在的（即隐藏的、可能的），直到我们展开纸张。同样，感知器的目标是将其高维输入“展开”，以便模型可以在潜在空间中工作，使用低维表示。
- en: Importantly, this architecture can efficiently process high-resolution inputs.
    For example, a 224 × 224 image has 50,176 pixels, so if we tried to feed such
    a long sequence of pixel tokens directly to a regular encoder, each self-attention
    layer would have to compute 50,176² ≈ 2.5 billion attention scores! But since
    the Perceiver only attends to the pixel tokens through cross-attention, it just
    needs to compute 50,176 times the number of latent tokens. Even for the biggest
    Perceiver variant, that’s just a total of 50,176 × 512 ≈ 25.7 million attention
    scores, which is roughly 100 times less compute.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这种架构可以有效地处理高分辨率输入。例如，一个224 × 224的图像有50,176个像素，如果我们尝试直接将如此长的像素标记序列输入到常规编码器中，每个自注意力层将不得不计算50,176²
    ≈ 25亿个注意力分数！但是，由于感知器仅通过交叉注意力关注像素标记，它只需要计算50,176乘以潜在标记的数量。即使是最大的感知器变体，这也只是总共50,176
    × 512 ≈ 2570万个注意力分数，这大约是100倍的计算量。
- en: Note
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Thanks to the latent bottleneck, the Perceiver scales linearly with the number
    of pixel tokens, instead of quadratically.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了潜在瓶颈，感知器的扩展与像素标记的数量呈线性关系，而不是二次关系。
- en: The authors trained the Perceiver using regular supervised learning on various
    classification tasks across several modalities, including image-only (ImageNet),
    audio plus video (AudioSet),⁠^([32](ch16.html#id3910)) or point clouds (ModelNet40),⁠^([33](ch16.html#id3911))
    all using the same model architecture. They got competitive results, in some cases
    even reaching the state of the art.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们使用常规的监督学习在多个模态的分类任务上训练了Perceiver，包括仅图像（ImageNet）、音频加视频（AudioSet）⁠^([32](ch16.html#id3910))
    或点云（ModelNet40）⁠^([33](ch16.html#id3911))，所有这些都使用了相同的模型架构。他们得到了有竞争力的结果，在某些情况下甚至达到了现有技术的水平。
- en: The videos in the AudioSet dataset were downsampled to 224 × 224 pixels at 25
    frames per second (fps), with a 48 kHz audio sample rate. You could theoretically
    feed each pixel and each audio frame individually to the Perceiver, but this would
    be a bit extreme, as each 10s video would be represented as a sequence of 224
    × 224 × 25 × 10 ≈ 12.5 million pixel tokens, and 48,000 × 10 = 480,000 audio tokens.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: AudioSet数据集中的视频被下采样到每秒25帧的224 × 224像素，音频采样率为48 kHz。理论上，你可以单独将每个像素和每个音频帧输入到Perceiver中，但这会有些极端，因为每个10秒的视频将表示为一个由224
    × 224 × 25 × 10 ≈ 12.5百万像素标记组成的序列，以及48,000 × 10 = 480,000音频标记。
- en: So the authors had to compromise. They trained on 32-frame clips (at 25 fps,
    that’s 1.28s each, instead of 10s) and they chopped the video into 2 × 8 × 8 patches
    (i.e., 2 frames × 8 × 8 pixels), resulting in 224 × 224 × 32 / (2 × 8 × 8) = 12,544
    video tokens of 128 RGB pixels each (plus the position encoding). They also chopped
    the audio into clips of 128 frames each, resulting in 480 audio tokens. They also
    tried converting the audio to a mel spectrogram (which resulted in 4,800 audio
    tokens). Using a spectrogram instead of raw audio is a standard practice in audio
    processing, but it made very little difference to the model’s performance, which
    shows that the Perceiver is able to extract useful features from the raw data
    without any help.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者们不得不妥协。他们在32帧剪辑（每秒25帧，每段1.28秒，而不是10秒）上进行了训练，并将视频切割成2 × 8 × 8个块（即2帧 × 8
    × 8像素），从而得到每个包含128个RGB像素的12,544个视频标记（加上位置编码）。他们还将音频切割成每段128帧的剪辑，从而得到480个音频标记。他们还尝试将音频转换为梅尔频谱图（这产生了4,800个音频标记）。使用频谱图而不是原始音频是音频处理中的标准做法，但这几乎没有对模型性能产生影响，这表明Perceiver能够在没有任何帮助的情况下从原始数据中提取有用的特征。
- en: Then they simply concatenated the video and audio token sequences (after positional
    encoding), and also concatenated a modality embedding to help the model distinguish
    the modalities.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后他们简单地连接了视频和音频标记序列（在位置编码之后），还连接了一个模态嵌入，以帮助模型区分模态。
- en: One limitation of the Perceiver architecture is that it was only designed for
    multimodal classification. That said, instead of averaging the latent tokens and
    feeding them to a classification head, we could try to use them for other downstream
    tasks. Of course, the DeepMind researchers thought of that, and just a few months
    later they published the Perceiver IO architecture.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver架构的一个局限性是它仅设计用于多模态分类。话虽如此，我们不是简单地平均潜在标记并将它们输入到分类头中，而是可以尝试将它们用于其他下游任务。当然，DeepMind的研究人员想到了这一点，仅仅几个月后，他们就发布了Perceiver
    IO架构。
- en: 'Perceiver IO: A Flexible Output Mechanism for the Perceiver'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Perceiver IO：Perceiver的灵活输出机制
- en: DeepMind released [Perceiver IO](https://homl.info/perceiverio) in July 2021.⁠^([34](ch16.html#id3915))
    It can perform classification tasks like the Perceiver, but also many other tasks
    such as masked language modeling (MLM) better than BERT, *optical flow* (i.e.,
    predicting where each pixel will move in the next video frame), actually beating
    the state of the art, and even playing StarCraft II.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind于2021年7月发布了[Perceiver IO](https://homl.info/perceiverio)。它能够执行与Perceiver类似的分类任务，但比BERT更好地执行许多其他任务，如掩码语言建模（MLM）、*光流*（即预测每个像素在下一个视频帧中的移动位置），实际上超越了现有技术，甚至可以玩星际争霸II。
- en: 'The model is identical to Perceiver up to the output latent tokens, but the
    pooling layer and the classification head are replaced by a very flexible output
    mechanism (see [Figure 16-16](#perceiverio_diagram)):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在输出潜在标记之前与Perceiver相同，但池化层和分类头被一个非常灵活的输出机制所取代（见[图16-16](#perceiverio_diagram)）：
- en: A new cross-attention layer is added, which acts as a decoder by attending to
    the output latent tokens and producing the final output representations. These
    output representations can then go through a task-specific head, or even multiple
    heads if we’re doing multitask learning.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number and nature of the output tokens is task-specific:'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For classification, we only need one output vector, which we can feed to a classification
    head. Therefore, we need one output query token, which can just be a learned embedding.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For masked language modeling, we can use one output query token per masked
    token, and add a classification head on top of the output representations (i.e.,
    linear plus softmax) to get one estimated token probability for each masked token.
    To help the model locate each masked token, the output query tokens are learnable
    positional embeddings based on the masked token’s position. For example, given
    the masked sentence “The dog [MASK] the [MASK]”, the masked tokens are located
    at positions #2 and #4, so we use the positional embedding #2 as the first output
    query token, and #4 as the second output query token. This same approach works
    for any other modality: just predict the masked tokens. It can also be extended
    to multiple modalities at once, typically by adding a modality embedding to the
    output query token before feeding it to the output cross-attention layer.'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For optical flow, the authors actually used one output token per pixel, using
    the same pixel representations both as the inputs to the Perceiver and as the
    output query tokens. This representation includes a Fourier positional encoding.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram of Perceiver IO architecture showing input tokens processed into
    latent tokens, which connect through a cross-attention layer to output query tokens,
    leading to task-specific head(s).](assets/hmls_1616.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-16\. Perceiver IO architecture: one output query token per desired
    output token is fed to a cross-attention layer that attends to the Perceiver’s
    output latent tokens'
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because the output query tokens only ever attend to the latent tokens, the Perceiver
    IO can handle a very large number of output query tokens. The latent bottleneck
    allows the model to scale linearly for both the inputs and outputs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The Perceiver IO is a bidirectional architecture; there’s no causal masking,
    so it’s not well suited for autoregressive tasks. In particular, it cannot efficiently
    perform next token prediction, so it’s not well suited for text generation tasks
    such as image captioning. Sure, you could feed it an image and some text with
    a mask token at the end, and make it predict which token was masked, then start
    over to get the next token, and so on, but it would be horribly inefficient compared
    to a causal model (which can cache the previous state).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, Google and DeepMind researchers released the [Perceiver AR
    architecture](https://homl.info/perceiverar) in February 2022 to address this
    limitation (AR stands for autoregressive). The model works very much like the
    Perceiver, except the last tokens of the input sequence are used as the latent
    tokens, the model is causal over these latent tokens, and it is trained using
    next token prediction. Perceiver AR didn’t quite have the same impact as Perceiver
    and Perceiver IO, but it got excellent results on very long input sequences, thanks
    to its linear scaling capability.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Google和DeepMind的研究人员在2022年2月发布了[Perceiver AR架构](https://homl.info/perceiverar)来解决这一限制（AR代表自回归）。该模型的工作方式与感知器非常相似，除了输入序列的最后几个标记被用作潜在标记，模型对这些潜在标记是因果的，并且使用下一个标记预测进行训练。Perceiver
    AR并没有像Perceiver和Perceiver IO那样产生同样大的影响，但它由于其线性扩展能力，在非常长的输入序列上取得了出色的结果。
- en: 'But DeepMind researchers weren’t done with multimodal ML; they soon released
    yet another amazing multimodal model, partly based on the Perceiver: Flamingo.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 但DeepMind的研究人员并没有停止多模态机器学习；他们很快又发布了一个基于感知器的另一个令人惊叹的多模态模型：Flamingo。
- en: 'Flamingo: Open-Ended Visual Dialogue'
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flamingo：开放式视觉对话
- en: DeepMind’s [Flamingo paper](https://homl.info/flamingo), published in April
    2022, introduced a visual-language model (VLM) that can take arbitrary sequences
    of text and images as input and generate coherent free-form text. Most importantly,
    its few-shot performance is excellent on a wide variety of tasks.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind于2022年4月发布的[Flamingo论文](https://homl.info/flamingo)，介绍了一种视觉语言模型（VLM），它可以接受任意序列的文本和图像作为输入，并生成连贯的自由形式文本。最重要的是，它在各种任务上的少样本性能都非常出色。
- en: 'For example, suppose you want to build a model that takes a picture and outputs
    a poem about that image: no need to train a new model; you can just feed a few
    examples to Flamingo, add the new image at the end, and it will happily generate
    a poem about this new image. If you want it to detect license plate numbers on
    car photos, just give it a few photos along with the corresponding license plate
    numbers (as text), then add a new car photo, and Flamingo will output its license
    plate number. You can just as easily use Flamingo for image captioning. Or visual
    question answering. Or you can ask it to compare two images. In fact, you can
    even give the model several frames from a video and ask it to describe the action.
    It’s an incredibly versatile and powerful model out of the box, without any fine-tuning.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想构建一个模型，该模型接受一张图片并输出关于该图片的诗篇：无需训练新的模型；你只需向Flamingo提供几个示例，并在最后添加新的图像，它就会愉快地生成关于这张新图像的诗篇。如果你想让它检测汽车照片上的车牌号码，只需提供几张带有相应车牌号码（作为文本）的照片，然后添加一张新的汽车照片，Flamingo就会输出车牌号码。你同样可以轻松地使用Flamingo进行图像标题生成。或者视觉问答。或者你可以要求它比较两张图像。实际上，你甚至可以给模型提供视频的几个帧，并要求它描述动作。这是一个功能极其强大且无需微调的模型。
- en: 'Let’s look at Flamingo’s architecture (see [Figure 16-17](#flamingo_diagram)):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Flamingo的架构（见[图16-17](#flamingo_diagram)）：
- en: 'Instead of starting from scratch, Flamingo is based on two large pretrained
    models, which are both frozen: a vision model and a decoder-only language model.
    The authors used Chinchilla and CLIP, respectively, but many other powerful models
    would work fine too.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flamingo不是从头开始，而是基于两个大型预训练模型，这两个模型都是冻结的：一个是视觉模型，另一个是仅解码的语言模型。作者分别使用了Chinchilla和CLIP，但许多其他强大的模型也能很好地工作。
- en: Each input image is fed to the vision model, and the outputs go through a Perceiver
    model, called a *Resampler*, which produces a sequence of latent token representations.
    This ensures that every image gets represented as a fairly short sequence of latent
    representations (typically much shorter than the output of the vision model).
    This works around the quadratic attention problem.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入图像都会被输入到视觉模型中，其输出经过一个名为 *Resampler* 的感知器模型，该模型生成一系列潜在标记表示。这确保了每个图像都能以相当短的潜在表示序列（通常比视觉模型的输出短得多）来表示。这解决了二次注意力问题。
- en: 'The sequences output by the Resampler are fed as the keys/values to many *gated
    xattn-dense* modules, which are inserted before every block in the frozen LLM:'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Resampler输出的序列被用作许多 *gated xattn-dense* 模块的键/值，这些模块被插入到冻结的LLM中的每个块之前：
- en: Each gated xattn-dense module is composed of a masked multi-head attention layer
    followed by a feedforward module, with a skip connection each, just like the cross-attention
    half of a vanilla Transformer’s decoder layer.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个门控xattn-dense模块由一个掩码多头注意力层和一个前馈模块组成，每个都有跳跃连接，就像标准Transformer解码器层的前馈交叉注意力部分一样。
- en: However, both the masked MHA layer and the feedforward module are followed by
    a *tanh gate*. These gates multiply their input by tanh(*α*), where *α* is a learnable
    scalar parameter initialized to 0 (one per gate). Since tan(0) = 0, training starts
    with all gates closed, so the inputs can only flow through the skip connections,
    and the gated xattn-dense modules have no impact on the LLM. But as training progresses,
    the model gradually learns to open the gates, allowing the gated modules to influence
    the LLM’s outputs.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，掩码MHA层和前馈模块后面都跟着一个*tanh门控器*。这些门控器将它们的输入乘以tanh(*α*)，其中*α*是一个可学习的标量参数，初始化为0（每个门控器一个）。由于tanh(0)
    = 0，训练开始时所有门控器都是关闭的，因此输入只能通过跳跃连接流动，门控的xattn-dense模块对LLM没有影响。但随着训练的进行，模型逐渐学会打开门控器，允许门控模块影响LLM的输出。
- en: In the gated xattn-dense module, each text token can only attend to visual tokens
    from the closest image located before it; visual tokens from all other images
    are masked. For example, the last text token (“is”) can only attend to the Chinese
    tower photo, it cannot directly attend to the flower photo. However, since previous
    text tokens have information about the flower photo, the last token does have
    indirect access to the flower photo via the frozen LLM’s self-attention layers.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在门控xattn-dense模块中，每个文本标记只能关注位于其之前的最近图像中的视觉标记；来自所有其他图像的视觉标记都被掩码了。例如，最后一个文本标记（“是”）只能关注中国塔的照片，它不能直接关注花朵照片。然而，由于前面的文本标记有关于花朵照片的信息，最后一个标记确实通过冻结的LLM的自注意力层间接访问了花朵照片。
- en: The text is tokenized as the LLM expects (e.g., Chinchilla expects start-of-sequence
    and end-of-sequence tokens, which I denoted as <s> and </s>), but a couple new
    special tokens are added. Each image-text chunk ends with an end-of-chunk token
    (which I denoted as </c>), and each image is replaced with an image token (which
    I denoted as <i>). Both are represented using trainable embeddings.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本被标记化成LLM期望的形式（例如，Chinchilla期望序列开始和结束标记，我标记为<s>和</s>
- en: '![Diagram illustrating the Flamingo model architecture, showing the flow of
    image and text inputs through a vision encoder, Resampler, and gated xattn-dense
    modules, which integrate into the LLM blocks.](assets/hmls_1617.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![说明Flamingo模型架构的图解，展示了图像和文本输入通过视觉编码器、重采样器和门控xattn-dense模块的流动，这些模块集成到LLM块中。](assets/hmls_1617.png)'
- en: Figure 16-17\. Flamingo takes any sequence of text and images, and outputs coherent
    free-form text
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-17\. Flamingo可以接受任何文本和图像的序列，并输出连贯的自由形式文本
- en: 'The bad news is that DeepMind did not release Flamingo to the public. The good
    news is that open source replications and variants are available:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是DeepMind没有将Flamingo公开。好消息是开源复制品和变体是可用的：
- en: '[OpenFlamingo](https://homl.info/openflamingo), created by the MLFoundations
    team, which is part of the non-profit organization LAION. It is fully open source
    and available on the Hugging Face Hub (e.g., openflamingo/OpenFlamingo-9B-vitl-mpt7b,
    based on a CLIP ViT-L/14 vision encoder and a MPT-7B LLM).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenFlamingo](https://homl.info/openflamingo)，由MLFoundations团队创建，该团队是非营利组织LAION的一部分。它是完全开源的，可在Hugging
    Face Hub上获得（例如，openflamingo/OpenFlamingo-9B-vitl-mpt7b，基于CLIP ViT-L/14视觉编码器和MPT-7B
    LLM）。'
- en: '[IDEFICS](https://homl.info/idefics) by Hugging Face, trained on a huge dataset
    named OBELICS,⁠^([35](ch16.html#id3931)) composed of 141 million interleaved text-image
    documents gathered from Common Crawl (including 350 million images and 115 billion
    text tokens). Both IDEFICS and OBELICS are available on the hub (e.g., Idefics3-8B-Llama3
    and OBELICS by HuggingFaceM4). The architecture includes a few improvements over
    Flamingo; for example, you can more easily swap in different LLMs or vision encoders.
    IDEFICS itself is open source, but the models it is based on may have licensing
    limitations. In particular, IDEFICS 1 and 3 are based on Llama, which has some
    limitations for commercial use, while IDEFICS 2 is based on Mistral, which is
    fully open source.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face的[IDEFICS](https://homl.info/idefics)，在名为OBELICS的巨大数据集上训练，该数据集由来自Common
    Crawl的1.41亿个交错文本-图像文档组成（包括3.5亿张图像和1150亿个文本标记）。IDEFICS和OBELICS都可在平台上找到（例如，Idefics3-8B-Llama3和HuggingFaceM4的OBELICS）。该架构在Flamingo的基础上进行了一些改进；例如，你可以更容易地替换不同的LLM或视觉编码器。IDEFICS本身是开源的，但基于它的模型可能存在许可限制。特别是，IDEFICS
    1和3基于Llama，它在商业使用上存在一些限制，而IDEFICS 2基于Mistral，它是完全开源的。
- en: '[AudioFlamingo](https://homl.info/audioflamingo) by Nvidia, which is very similar
    to Flamingo but handles audio instead of images.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia的[AudioFlamingo](https://homl.info/audioflamingo)，与Flamingo非常相似，但处理的是音频而不是图像。
- en: Other variants are available, such as domain-specific models like [Med-Flamingo](https://homl.info/medflamingo),
    an OpenFlamingo model trained on medical documents.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他变体也很有用，例如针对特定领域的模型，如[Med-Flamingo](https://homl.info/medflamingo)，这是一个在医疗文档上训练的OpenFlamingo模型。
- en: The last multimodal architecture we will discuss is bootstrapping language-image
    pretraining, or BLIP, by Salesforce. Its second version, BLIP-2, also successfully
    reuses two large pretrained models—a vision model and an LLM—to create a VLM that
    can ingest both images and text, and generate free-form text. Let’s see how.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一种多模态架构是由Salesforce提出的语言-图像预训练，或称BLIP。它的第二个版本，BLIP-2，也成功地重用了两个大型预训练模型——一个视觉模型和一个LLM——来创建一个能够同时处理图像和文本的VLM，并生成自由形式的文本。让我们看看它是如何做到的。
- en: BLIP and BLIP-2
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLIP和BLIP-2
- en: 'The original [BLIP model](https://homl.info/blip) is an excellent visual-language
    model released by Salesforce in January 2022.⁠^([36](ch16.html#id3938)) Its architecture
    is a *mixture of encoder-decoder* (MED) composed of a text-only encoder, a vision-only
    encoder, an image-grounded text encoder, and an image-grounded text decoder, sharing
    many layers. This flexible architecture made it possible to train the model simultaneously
    on three distinct objectives: *image-text matching* (ITM), an *image-text contrastive*
    (ITC) loss to align image and text representations (similar to CLIP), and language
    modeling (LM) where the model must try to generate the caption using next token
    prediction.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Salesforce在2022年1月发布的原始[BLIP模型](https://homl.info/blip)是一个优秀的视觉-语言模型。其架构是一个由纯文本编码器、纯视觉编码器、基于图像的文本编码器和基于图像的文本解码器组成的*编码器-解码器混合体*（MED），它们共享许多层。这种灵活的架构使得模型能够同时针对三个不同的目标进行训练：*图像-文本匹配*（ITM），一个*图像-文本对比损失*（ITC）来对齐图像和文本表示（类似于CLIP），以及语言建模（LM），其中模型必须尝试使用下一个标记预测来生成标题。
- en: 'Another important reason for BLIP’s success is the fact that it was pretrained
    on a very large and clean dataset. To build this dataset, the authors simultaneously
    trained a *captioning module* to generate synthetic captions for images, and a
    *filtering module* to remove noisy data. This approach, named *CapFilt*, removed
    poor quality captions from the original web-scraped dataset, and added many new
    high-quality synthetic captions. After this bootstrapping stage, the authors trained
    the final model on the large and clean dataset they had just built. It’s a two-stage
    process, hence the name BLIP: *bootstrapping language-image pretraining*.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP成功的一个重要原因是它在一个非常大且干净的数据集上进行了预训练。为了构建这个数据集，作者同时训练了一个*标题生成模块*来为图像生成合成标题，以及一个*过滤模块*来移除噪声数据。这种方法被称为*CapFilt*，它从原始的网页抓取数据集中移除了低质量的标题，并添加了许多新的高质量合成标题。在这个自举阶段之后，作者在刚刚构建的大且干净的数据集上训练了最终的模型。这是一个两阶段的过程，因此得名BLIP：*自举语言-图像预训练*。
- en: One year later, in January 2023, Salesforce released [BLIP-2](https://homl.info/blip2),⁠^([37](ch16.html#id3951))
    which is based on the same core ideas but greatly improves the model’s performance
    by reusing two large pretrained models, one vision model and one language model,
    both frozen. BLIP-2 even outperformed Flamingo with a much smaller model.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，2023年1月，Salesforce发布了[BLIP-2](https://homl.info/blip2)，⁠^([37](ch16.html#id3951))，它基于相同的核心思想，但通过重用两个大型预训练模型（一个视觉模型和一个语言模型，两者都冻结）大大提高了模型性能。BLIP-2甚至以更小的模型超越了Flamingo。
- en: Training is split in two stages. BLIP-2’s architecture during the first stage
    is shown in [Figure 16-18](#blip2_stage1_diagram).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分为两个阶段。BLIP-2在第一阶段的结构如图16-18所示。
- en: '![Diagram illustrating BLIP-2''s Stage 1 pretraining architecture, focusing
    on the Q-Former module integrating vision and text sequences through various attention
    and processing layers.](assets/hmls_1618.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图示BLIP-2第一阶段预训练架构，重点关注通过各种注意力和处理层整合视觉和文本序列的Q-Former模块。](assets/hmls_1618.png)'
- en: 'Figure 16-18\. BLIP-2 pretraining, Stage 1: training the Q-Former'
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-18\. BLIP-2预训练，第一阶段：训练Q-Former
- en: The central component is called the *Q-Former* (querying transformer). Its architecture
    is the same as BERT-base, and in fact it’s even initialized using BERT-base’s
    pretrained weights, but it also has some extra cross-attention layers that let
    it attend to visual tokens produced by the pretrained visual encoder. The cross-attention
    layers are inserted in every other encoder layer, between the self-attention layer
    and the feedforward module, and they are initialized randomly.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心组件被称为*Q-Former*（查询转换器）。其架构与BERT-base相同，实际上它甚至使用BERT-base的预训练权重进行初始化，但它也有一些额外的交叉注意力层，使其能够关注由预训练视觉编码器产生的视觉标记。交叉注意力层插入到每个编码器层的每隔一层，在自注意力层和前馈模块之间，并且它们是随机初始化的。
- en: 'The Q-Former processes three sequences: a sequence of text tokens (using BERT
    tokenization and token embeddings), a sequence of visual tokens produced by the
    pretrained vision encoder, and lastly a sequence of trainable Perceiver-style
    latent tokens. In BLIP-2, the latent tokens are called *query tokens* because
    their output representations will later be used to query the pretrained LLM.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-Former处理三个序列：一个文本标记序列（使用BERT标记化和标记嵌入），一个由预训练视觉编码器产生的视觉标记序列，最后是一个可训练的Perceiver风格的潜在标记序列。在BLIP-2中，这些潜在标记被称为*查询标记*，因为它们的输出表示将后来用于查询预训练的LLM。
- en: 'The Q-Former is trained with the same three objectives as BLIP: ITM, ITC, and
    LM. For each objective, a different mask is used:'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-Former使用与BLIP相同的三个目标进行训练：ITM、ITC和LM。对于每个目标，使用不同的掩码：
- en: For ITM, query tokens and text tokens can attend to each other. In other words,
    the output representations for the query tokens represent text-grounded visual
    features, and the output representations for the text tokens represent image-grounded
    text features. The query token outputs go through a linear layer which produces
    two logits per query token (image-text match or mismatch), and the model computes
    the mean logits across all query tokens, then computes the binary cross-entropy.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于ITM，查询标记和文本标记可以相互关注。换句话说，查询标记的输出表示代表基于文本的视觉特征，而文本标记的输出表示代表基于图像的文本特征。查询标记的输出通过一个线性层，每个查询标记产生两个logits（图像-文本匹配或不匹配），然后模型计算所有查询标记的平均logits，然后计算二元交叉熵。
- en: For ITC, query tokens and text tokens cannot attend to each other. In other
    words, the Q-Former’s outputs represent visual-only features and text-only features.
    For each possible image/caption pair in the batch, the model computes the maximum
    similarity between the query token outputs and the class token output. We get
    a matrix of maximum similarities, and the loss pushes the values toward +1 on
    the main diagonal, and pushes the other values toward 0, much like CLIP.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于ITC，查询标记和文本标记不能相互关注。换句话说，Q-Former的输出代表仅视觉特征和仅文本特征。对于批次中每个可能的图像/标题对，模型计算查询标记输出和类别标记输出之间的最大相似度。我们得到一个最大相似度的矩阵，损失函数推动主对角线上的值趋向于+1，推动其他值趋向于0，类似于CLIP。
- en: 'For LM, text tokens can only attend previous tokens (i.e., we use a causal
    mask), but they can attend all query tokens. However, query tokens cannot attend
    any text token. In other words, the query token outputs represent visual-only
    features, while text token outputs represent image-grounded causal text features.
    The model is trained using next token prediction: each text token’s output goes
    through a classification head which must predict the next token in the caption.'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于LM，文本标记只能关注前面的标记（即，我们使用因果掩码），但它们可以关注所有查询标记。然而，查询标记不能关注任何文本标记。换句话说，查询标记输出代表仅视觉特征，而文本标记输出代表基于图像的因果文本特征。该模型使用下一个标记预测进行训练：每个文本标记的输出通过一个分类头，该头必须预测标题中的下一个标记。
- en: You may be surprised that the Q-Former is used to encode text (for ITM and ITC)
    and also to generate text (for LM). Since the Q-Former is initialized using the
    weights of a pretrained BERT-base model, it’s pretty good at text encoding right
    from the start of training, but it initially doesn’t know that it has to predict
    the next token for the LM task. Luckily, it can learn fairly fast since it’s not
    starting from scratch; it has good BERT features to work with. However, we need
    to tell it whether we want it to encode the text or predict the next token. For
    this, we replace the class token with a *decode token* during LM.⁠^([38](ch16.html#id3959))
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会惊讶，Q-Former既用于编码文本（用于ITM和ITC），也用于生成文本（用于LM）。由于Q-Former是使用预训练的BERT-base模型的权重初始化的，因此它从一开始就非常擅长文本编码，但它最初并不知道它必须预测LM任务中的下一个标记。幸运的是，它可以相当快地学习，因为它不是从零开始；它有很好的BERT特征可以工作。然而，我们需要告诉它我们想要它编码文本还是预测下一个标记。为此，我们在LM期间用*解码标记*替换了类别标记。⁠^([38](ch16.html#id3959))
- en: 'Once stage 1 is finished, the Q-Former is already a powerful model that can
    encode images and text into the same space, so a photo of a chimpanzee produces
    a very similar output representation as the caption “A photo of a chimpanzee”.
    But it’s even better than that: the query token outputs were trained to be most
    helpful for next token prediction.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦第一阶段完成，Q-Former已经是一个强大的模型，可以将图像和文本编码到同一个空间，因此一只黑猩猩的照片会产生与标题“一只黑猩猩的照片”非常相似的结果表示。但它甚至更好：查询标记输出被训练为对下一个标记预测最有帮助。
- en: Tip
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To produce negative examples for ITM, one strategy is to randomly pick a caption
    in the same batch, excluding the image’s true caption. However, this makes the
    task too easy, so the model doesn’t learn much. Instead, the authors used a *hard
    negative mining* strategy, where difficult captions are more likely to be sampled.
    For example, given a photo of a chimpanzee, the caption “A gorilla” is more likely
    to be sampled than “A spacecraft”. To find difficult captions, the algorithm uses
    the similarity scores from the ITC task.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了产生ITM的负例，一种策略是从同一个批次中随机选择一个标题，排除图像的真实标题。然而，这使得任务过于简单，因此模型学不到很多东西。相反，作者使用了一种*硬负例挖掘*策略，其中困难的标题更有可能被采样。例如，给定一只黑猩猩的照片，标题“一只大猩猩”比“一艘宇宙飞船”更有可能被采样。为了找到困难的标题，算法使用ITC任务的相似度分数。
- en: 'So it’s time for the second stage of training (see [Figure 16-19](#blip2_stage2_diagram)):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在是训练的第二阶段（见[图16-19](#blip2_stage2_diagram)）：
- en: We keep the vision transformer and the Q-Former, but we drop the rest and we
    add a new linear layer, initialized randomly, on top of the Q-Former.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保留了视觉转换器和Q-Former，但丢弃了其余部分，并在Q-Former之上添加了一个新的随机初始化的线性层。
- en: For each image/caption pair, the Q-Former attends to the visual features produced
    by the pretrained vision encoder, and the outputs go through the linear layer
    to produce a sequence of visual query tokens.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个图像/标题对，Q-Former关注由预训练的视觉编码器产生的视觉特征，输出通过线性层产生一系列视觉查询标记。
- en: The visual query tokens and the text token representations are concatenated
    and fed to the (frozen) pretrained LLM. We train BLIP-2 to predict the next caption
    token.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将视觉查询标记和文本标记表示连接起来，然后输入到（冻结的）预训练的LLM中。我们训练BLIP-2来预测下一个标题标记。
- en: During stage 2, the model learns to properly map the visual query tokens to
    the LLM’s input space. Once trained, the model can be used like in stage 2, generating
    visual-grounded text.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，模型学习将视觉查询标记正确映射到LLM的输入空间。一旦训练完成，该模型就可以像第二阶段一样使用，生成视觉基础文本。
- en: '![Diagram illustrating the BLIP-2 pretraining process, showing how visual features
    from a vision encoder are processed by a Q-former and a linear layer to generate
    visual query tokens, which are then combined with text tokens and fed to a large
    language model (LLM).](assets/hmls_1619.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![BLIP-2预训练过程的示意图，展示了视觉编码器中的视觉特征如何被Q-former和线性层处理以生成视觉查询标记，然后这些标记与文本标记结合并输入到大型语言模型（LLM）中。](assets/hmls_1619.png)'
- en: 'Figure 16-19\. BLIP-2 pretraining, Stage 2: training the linear layer to map
    the query tokens to the LLM’s input space'
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-19。BLIP-2预训练，阶段2：训练线性层将查询标记映射到LLM的输入空间
- en: 'Let’s use BLIP-2 to generate a caption for an image:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用BLIP-2为一张图片生成标题：
- en: '[PRE12]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: What did BLIP-2 see?
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-2看到了什么？
- en: '[PRE13]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It’s a good description of the photo, but it would nicer without the special
    tokens, so let’s get rid of them when decoding the model’s output:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对照片的良好描述，但没有特殊标记会更好，所以让我们在解码模型输出时去掉它们：
- en: '[PRE14]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Perfect!
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！
- en: Tip
  id: totrans-322
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Also check out InstructBLIP, a BLIP-2 model with vision-language instruction
    tuning.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以查看InstructBLIP，这是一个具有视觉语言指令调整的BLIP-2模型。
- en: Other Multimodal Models
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他多模态模型
- en: 'We’ve covered quite a few multimodal models, with very different architectures
    and pretraining techniques, but of course there are many others. Here is a quick
    overview of some of the most notable ones:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了很多多模态模型，它们具有非常不同的架构和预训练技术，但当然还有很多其他模型。以下是一些最引人注目的模型的快速概述：
- en: LayoutLM (Microsoft, Dec. 2019)
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLM (Microsoft, Dec. 2019)
- en: Document understanding based on text, vision, and document layout. Version 3
    was released in April 2022.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本、视觉和文档布局的文档理解。第3版于2022年4月发布。
- en: GLIP (Microsoft, Dec. 2021)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: GLIP (Microsoft, Dec. 2021)
- en: A vision-language model for visual grounding and object detection. GLIP-2 was
    released in 2022.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 用于视觉定位和目标检测的视觉语言模型。GLIP-2于2022年发布。
- en: Stable Diffusion (Stability AI, Dec. 2021)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion (Stability AI, Dec. 2021)
- en: A powerful text-to-image model.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强大的文本到图像模型。
- en: OFA (Microsoft, Feb. 2022)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: OFA (Microsoft, Feb. 2022)
- en: Unified (one for all) vision-language pretraining framework handling various
    vision-language tasks.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 统一（适用于所有）视觉语言预训练框架，处理各种视觉语言任务。
- en: CoCa (Google, May 2022)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: CoCa (Google, May 2022)
- en: A vision-language model pretrained using contrastive and captioning objectives.
    CoCa influenced later models like PaLI-X and Flamingo-2.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对比和标题目标预训练的视觉语言模型。CoCa影响了后来的模型，如PaLI-X和Flamingo-2。
- en: PaLI (Google, Sep. 2022)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: PaLI (Google, Sep. 2022)
- en: Multilingual multimodal models for vision-language tasks like VQA and captioning,
    with strong zero-shot performance. The next versions, PaLI-X and PaLI-3, were
    released in 2023, and PaliGemma in May 2024.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 用于视觉语言任务（如VQA和标题）的多语言多模态模型，具有强大的零样本性能。下一版本PaLI-X和PaLI-3于2023年发布，PaliGemma于2024年5月发布。
- en: Kosmos-1 (Microsoft, Feb. 2023)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Kosmos-1 (Microsoft, Feb. 2023)
- en: A vision-language model with strong support for visual grounding. Kosmos-2 and
    Kosmos-2.5 came out in 2023.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有强大视觉定位支持的视觉语言模型。Kosmos-2和Kosmos-2.5于2023年发布。
- en: PaLM-E (Google, Mar. 2023)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM-E (Google, Mar. 2023)
- en: PaLM-E extends Google’s PaLM series with visual inputs and embodied sensor data.
    A decoder-only LLM generates text commands like “grab the hammer”, which are interpreted
    and executed by a robot via a downstream system.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM-E通过视觉输入和具身传感器数据扩展了Google的PaLM系列。一个仅解码器的大型语言模型生成文本命令，如“拿起锤子”，这些命令通过下游系统被解释并由机器人执行。
- en: LLaVA (H. Liu et al., Apr. 2023)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA (H. Liu et al., Apr. 2023)
- en: Among the best open source vision-language chat models.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在最好的开源视觉语言聊天模型中。
- en: ImageBind (Meta, May 2023)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ImageBind (Meta, May 2023)
- en: A CLIP-style model extended to six modalities (image, text, audio, IMU,⁠^([39](ch16.html#id3976))
    depth, and thermal).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一种扩展到六个模态（图像、文本、音频、IMU、深度和热成像）的CLIP风格模型。
- en: RT-2 (DeepMind, Jul. 2023)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: RT-2 (DeepMind, Jul. 2023)
- en: A vision-language model capable of robotic control as well, trained on a large-scale
    instruction-following dataset.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够进行机器人控制的视觉语言模型，在大型指令遵循数据集上训练。
- en: SeamlessM4T (Meta, Aug. 2023)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: SeamlessM4T (Meta, Aug. 2023)
- en: A single model that can perform speech-to-text, speech-to-speech, text-to-speech,
    and text-to-text translation across close to 100 languages.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以执行语音转文本、语音转语音、文本转语音和文本转文本翻译的单一模型，支持近100种语言。
- en: Qwen-VL (Alibaba, Sep. 2023)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen-VL (Alibaba, Sep. 2023)
- en: Open vision-language family (7B to 72B) that became one of the strongest open
    multimodal baselines. Led to Qwen2-VL (Aug. 2024) and Qwen3-Omni (Sep. 2025),
    which expanded to video and audio and reached trillion-parameter scale.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 开放视觉语言家族（7B到72B），成为最强的开源多模态基线之一。导致了Qwen2-VL（2024年8月）和Qwen3-Omni（2025年9月），扩展到视频和音频并达到万亿参数规模。
- en: Fuyu (Adept AI, Oct. 2023)
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Fuyu（Adept AI，2023年10月）
- en: Processes interleaved image and text in real time with a unified transformer.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用统一的Transformer实时处理交织的图像和文本。
- en: EMO (Alibaba, Feb. 2024)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: EMO（阿里巴巴，2024年2月）
- en: Takes an image of a person, plus an audio recording of someone speaking or singing,
    and the model generates a video of that person, matching the audio. EMO-2 was
    released in January 2025.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉一个人的图像，加上某人说话或唱歌的音频记录，模型生成匹配音频的该人的视频。EMO-2于2025年1月发布。
- en: GLaMM (H. Rasheed et al., Jun. 2024)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: GLaMM（H. Rasheed等人，2024年6月）
- en: A visual dialogue model which generates text responses mixed with object segmentation
    masks.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 一种视觉对话模型，生成包含对象分割掩码的文本响应。
- en: LaViDa (UCLA, Panasonic, Adobe, Salesforce, May 2025)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: LaViDa（加州大学洛杉矶分校，松下，Adobe，Salesforce，2025年5月）
- en: A family of open, diffusion-based vision-language models.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列基于扩散的开源视觉-语言模型。
- en: Tip
  id: totrans-360
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: I’ve created homl.info short links for all the models discussed in this chapter;
    just use the lowercase name without hyphens, for example, [*https://homl.info/qwen2vl*](https://homl.info/qwen2vl).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这一章中讨论的所有模型创建了homl.info短链接；只需使用不带连字符的小写名称，例如，[*https://homl.info/qwen2vl*](https://homl.info/qwen2vl)。
- en: There are also several commercial multimodal models whose detailed architectures
    were not disclosed, such as GPT-4.1 and Sora by OpenAI, Gemini 2.5 Pro by Google,
    Veo-3 by DeepMind, and Claude 4 Opus by Anthropic. To access these models, you
    first need to create an account and get a subscription (or use the free tier),
    then you can either use the provided apps (e.g., Google AI Studio, [*https://aistudio.google.com*](https://aistudio.google.com)),
    or query the model via an API. For example, following is a short code example
    showing how to query Gemini 2.5 Pro via the API. You first need to get an API
    key in Google AI Studio, then you can use any secret management method you prefer
    to store it and load it in your code (e.g., if you are using Colab, I recommend
    you use Colab’s secret manager, as we saw in [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几个商业多模态模型，其详细架构尚未公开，例如OpenAI的GPT-4.1和Sora，Google的Gemini 2.5 Pro，DeepMind的Veo-3，以及Anthropic的Claude
    4 Opus。要访问这些模型，你首先需要创建一个账户并获取订阅（或使用免费层），然后你可以使用提供的应用程序（例如，Google AI Studio，[*https://aistudio.google.com*](https://aistudio.google.com))，或者通过API查询模型。以下是一个简短的代码示例，展示如何通过API查询Gemini
    2.5 Pro。你首先需要在Google AI Studio中获取一个API密钥，然后你可以使用你喜欢的任何秘密管理方法来存储它并在你的代码中加载它（例如，如果你正在使用Colab，我建议你使用Colab的秘密管理器，正如我们在[第15章](ch15.html#transformer_chapter)中看到的）。
- en: '[PRE15]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code uses the `google-genai` library, which is already installed on Colab.
    It also assumes that a file named *my_cats_photo.jpg* is present in the same directory
    as the notebook.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了已安装在Colab上的`google-genai`库。它还假设在笔记本所在的目录中存在一个名为*my_cats_photo.jpg*的文件。
- en: 'This wraps up this chapter; I hope you enjoyed it. Transformers can now see,
    hear, touch, and more! In the next chapter, we will explore some fairly advanced
    techniques designed to speed up and scale transformers. As Daft Punk put it: harder,
    better, faster, stronger.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了这一章；希望你喜欢。Transformer现在可以看、听、触摸等等！在下一章中，我们将探讨一些旨在加快和扩展Transformer的相当高级的技术。正如Daft
    Punk所说：更难、更好、更快、更强。
- en: Exercises
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you describe the original ViT’s architecture? Why does it matter?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能描述一下原始ViT的架构吗？为什么这很重要？
- en: What tasks are regular ViTs (meaning nonhierarchical) best used for? What are
    their limitations?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 常规ViT（即非分层）最适合哪些任务？它们的局限性是什么？
- en: What is the main innovation in DeiT? Is this idea generalizable to other architectures?
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeiT的主要创新是什么？这个想法是否可以推广到其他架构？
- en: What are some examples of hierarchical ViTs? What kind of tasks are they good
    for?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些分层ViT的例子？它们适合哪些任务？
- en: How do PVTs and Swin Transformers reduce the computational cost of processing
    high-resolution images?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何通过PVTs和Swin Transformers降低处理高分辨率图像的计算成本？
- en: How does DINO work? What changed in DINOv2? When would you want to use DINOv2?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DINO是如何工作的？DINOv2中有什么变化？你会在什么情况下想使用DINOv2？
- en: What is the objective of the JEPA architecture? How does it work?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JEPA架构的目标是什么？它是如何工作的？
- en: What is a multimodal model? Can you give five examples of multimodal tasks?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是多模态模型？你能给出五个多模态任务的例子吗？
- en: Explain what the fusion and alignment problems are in multimodal learning. Why
    are transformers well suited to tackle them?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释在多模态学习中的融合和对齐问题是什么。为什么Transformer非常适合解决这些问题？
- en: Can you write a one-line summary of the main ideas in VideoBERT, ViLBERT, CLIP,
    DALL·E, Perceiver IO, Flamingo, and BLIP-2?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能为一行总结VideoBERT、ViLBERT、CLIP、DALL·E、Perceiver IO、Flamingo和BLIP-2的主要思想吗？
- en: If you are using a Perceiver IO model and you double the length of the inputs
    and the outputs, approximately how much more computation will be required?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用Perceiver IO模型，并且将输入和输出的长度加倍，大约需要多少额外的计算？
- en: Try fine-tuning a pretrained ViT model on the [Food 101 dataset](https://homl.info/food101)
    (`torchvision.datasets.Food101`). What accuracy can you reach? How about using
    a CLIP model, zero-shot?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在[Food 101数据集](https://homl.info/food101)（`torchvision.datasets.Food101`）上微调预训练的ViT模型。您能达到多少准确率？使用CLIP模型，零样本呢？
- en: 'Create a simple search engine for your own photos: first, write a function
    that uses a CLIP model to embed all of your photos and saves the resulting vectors.
    Next, write a function that takes a search query (text or image), embeds it using
    CLIP, then finds the most similar photo embeddings and displays the corresponding
    photos. You can manually implement the similarity search algorithm, or a dedicated
    library such as the [FAISS library](https://github.com/facebookresearch/faiss)
    or even a full-blown vector database.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您自己的照片创建一个简单的搜索引擎：首先，编写一个函数，使用CLIP模型嵌入所有照片并保存结果向量。接下来，编写一个函数，它接受一个搜索查询（文本或图像），使用CLIP进行嵌入，然后找到最相似的图像嵌入并显示相应的照片。您可以手动实现相似性搜索算法，或者使用专门的库，如[FAISS库](https://github.com/facebookresearch/faiss)，甚至是一个完整的向量数据库。
- en: Use BLIP-2 to automatically caption all of your photos.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用BLIP-2自动为您的所有照片添加标题。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch16.html#id3722-marker)) Kelvin Xu et al., “Show, Attend and Tell: Neural
    Image Caption Generation with Visual Attention”, *Proceedings of the 32nd International
    Conference on Machine Learning* (2015): 2048–2057.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch16.html#id3722-marker)) 凯文·许等，“展示、关注和讲述：具有视觉注意力的神经图像标题生成”，*第32届国际机器学习会议论文集*（2015年）：2048–2057。
- en: ^([2](ch16.html#id3724-marker)) This is a part of Figure 3 from the paper. It
    is reproduced with the kind authorization of the authors.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch16.html#id3724-marker)) 这部分内容来自论文中的图3。它是在作者们的友好授权下复制的。
- en: '^([3](ch16.html#id3728-marker)) Marco Tulio Ribeiro et al., “‘Why Should I
    Trust You?’: Explaining the Predictions of Any Classifier”, *Proceedings of the
    22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*
    (2016): 1135–1144.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch16.html#id3728-marker)) 马可·图利奥·里贝罗等，“‘我为什么要相信你？’：解释任何分类器的预测”，*第22届ACM
    SIGKDD国际知识发现和数据挖掘会议论文集*（2016年）：1135–1144。
- en: ^([4](ch16.html#id3740-marker)) Nicolas Carion et al., “End-to-End Object Detection
    with Transformers”, arXiv preprint arXiv:2005.12872 (2020).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch16.html#id3740-marker)) 尼古拉斯·卡里翁等，“使用Transformer进行端到端目标检测”，arXiv预印本arXiv:2005.12872（2020年）。
- en: '^([5](ch16.html#id3741-marker)) Alexey Dosovitskiy et al., “An Image Is Worth
    16x16 Words: Transformers for Image Recognition at Scale”, arXiv preprint arXiv:2010.11929
    (2020).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch16.html#id3741-marker)) 亚历克谢·多索夫斯基等，“一张图片等于16x16个单词：用于大规模图像识别的Transformer”，arXiv预印本arXiv:2010.11929（2020年）。
- en: ^([6](ch16.html#id3760-marker)) Hugo Touvron et al., “Training Data-Efficient
    Image Transformers & Distillation Through Attention”, arXiv preprint arXiv:2012.12877
    (2020).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch16.html#id3760-marker)) 雨果·图尔弗朗等，“训练数据高效图像Transformer与通过注意力进行蒸馏”，arXiv预印本arXiv:2012.12877（2020年）。
- en: '^([7](ch16.html#id3765-marker)) Wenhai Wang et al., “Pyramid Vision Transformer:
    A Versatile Backbone for Dense Prediction without Convolutions”, arXiv preprint
    arXiv:2102.12122 (2021).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch16.html#id3765-marker)) 王文海等，“金字塔视觉Transformer：无需卷积的密集预测的多功能骨干网络”，arXiv预印本arXiv:2102.12122（2021年）。
- en: '^([8](ch16.html#id3776-marker)) Ze Liu et al., “Swin Transformer: Hierarchical
    Vision Transformer using Shifted Windows”, arXiv preprint arXiv:2103.14030 (2021).'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch16.html#id3776-marker)) 刘泽等，“Swin Transformer：使用平移窗口的层次视觉Transformer”，arXiv预印本arXiv:2103.14030（2021年）。
- en: '^([9](ch16.html#id3781-marker)) Ze Liu et al., “Swin Transformer V2: Scaling
    Up Capacity and Resolution”, arXiv preprint arXiv:2111.09883 (2021).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch16.html#id3781-marker)) 刘泽等，“Swin Transformer V2：提升容量和分辨率”，arXiv预印本arXiv:2111.09883（2021年）。
- en: ^([10](ch16.html#id3788-marker)) Mathilde Caron et al., “Emerging Properties
    in Self-Supervised Vision Transformers”, arXiv preprint arXiv:2104.14294 (2021).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch16.html#id3788-marker)) 玛蒂尔德·卡隆等，“自监督视觉Transformer中的新兴特性”，arXiv预印本arXiv:2104.14294（2021年）。
- en: ^([11](ch16.html#id3797-marker)) This is the righthand part of Figure 3 of the
    DINO paper, reproduced with the kind authorization of the authors.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch16.html#id3797-marker)) 这是DINO论文图3的右侧部分，在作者们的友好授权下复制。
- en: '^([12](ch16.html#id3799-marker)) Yangtao Wang et al., “TokenCut: Segmenting
    Objects in Images and Videos with Self-supervised Transformer and Normalized Cut”,
    arXiv preprint arXiv:2209.00383 (2022).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch16.html#id3799-marker)) 王扬涛等人，“TokenCut: 使用自监督Transformer和归一化Cut在图像和视频中分割对象”，arXiv预印本
    arXiv:2209.00383 (2022)。'
- en: '^([13](ch16.html#id3801-marker)) “DINOv2: Learning Robust Visual Features without
    Supervision”, arXiv preprint arXiv:2304.07193 (2023).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch16.html#id3801-marker)) “DINOv2: 无监督学习鲁棒视觉特征”，arXiv预印本 arXiv:2304.07193
    (2023)。'
- en: ^([14](ch16.html#id3806-marker)) Xiaohua Zhai et al., “Scaling Vision Transformers”,
    arXiv preprint arXiv:2106.04560 (2021).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch16.html#id3806-marker)) 赵晓华等人，“扩展视觉Transformer”，arXiv预印本 arXiv:2106.04560
    (2021)。
- en: '^([15](ch16.html#id3808-marker)) Hangbo Bao et al., “BEiT: BERT Pre-Training
    of Image Transformers”, arXiv preprint arXiv:2106.08254 (2021).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch16.html#id3808-marker)) 包航波等人，“BEiT：图像Transformer的BERT预训练”，arXiv预印本
    arXiv:2106.08254 (2021)。
- en: ^([16](ch16.html#id3816-marker)) Kaiming He et al., “Masked Autoencoders Are
    Scalable Vision Learners”, arXiv preprint arXiv:2111.06377 (2021).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch16.html#id3816-marker)) 何凯明等人，“掩码自编码器是可扩展的视觉学习者”，arXiv预印本 arXiv:2111.06377
    (2021)。
- en: '^([17](ch16.html#id3820-marker)) Mitchell Wortsman et al., “Model Soups: Averaging
    Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference
    Time”, arXiv preprint arXiv:2203.05482 (2022).'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch16.html#id3820-marker)) Mitchell Wortsman等人，“模型汤：平均多个微调模型的权重可以提高准确率，而不会增加推理时间”，arXiv预印本
    arXiv:2203.05482 (2022)。
- en: '^([18](ch16.html#id3821-marker)) Yuxin Fang et al., “EVA: Exploring the Limits
    of Masked Visual Representation Learning at Scale”, arXiv preprint arXiv:2211.07636
    (2022).'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch16.html#id3821-marker)) 方宇新等人，“EVA: 在大规模上探索掩码视觉表示学习的极限”，arXiv预印本 arXiv:2211.07636
    (2022)。'
- en: ^([19](ch16.html#id3823-marker)) “Self-Supervised Learning from Images with
    a Joint-Embedding Predictive Architecture”, arXiv preprint arXiv:2301.08243 (2023).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch16.html#id3823-marker)) “使用联合嵌入预测架构从图像中进行自监督学习”，arXiv预印本 arXiv:2301.08243
    (2023)。
- en: ^([20](ch16.html#id3826-marker)) Yann LeCun, “A Path Towards Autonomous Machine
    Intelligence” (2022).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch16.html#id3826-marker)) Yann LeCun，“通往自主机器智能之路”（2022）。
- en: '^([21](ch16.html#id3840-marker)) Chen Sun et al., “VideoBERT: A Joint Model
    for Video and Language Representation Learning”, arXiv preprint arXiv:1904.01766
    (2019).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch16.html#id3840-marker)) 陈孙等人，“VideoBERT：视频和语言表示学习的联合模型”，arXiv预印本 arXiv:1904.01766
    (2019)。
- en: ^([22](ch16.html#id3846-marker)) L. Zhou, Y. Zhou et al., “End-to-End Dense
    Video Captioning with Masked Transformer”, *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition* (2018).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch16.html#id3846-marker)) 周磊，周勇等人，“使用掩码Transformer进行端到端密集视频字幕生成”，*IEEE计算机视觉与模式识别会议论文集*
    (2018)。
- en: '^([23](ch16.html#id3852-marker)) Jiasen Lu et al., “ViLBERT: Pretraining Task-Agnostic
    Visiolinguistic Representations for Vision-and-Language Tasks”, *Advances in Neural
    Information Processing Systems* 32 (2019).'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '^([23](ch16.html#id3852-marker)) 陆嘉森等人，“ViLBERT: 针对视觉和语言任务的预训练任务无关的视语言表示”，*神经信息处理系统进展*
    32 (2019)。'
- en: '^([24](ch16.html#id3853-marker)) Jize Cao et al. later provided some empirical
    evidence supporting this claim in their paper [“Behind the Scene: Revealing the
    Secrets of Pre-trained Vision-and-Language Models”](https://homl.info/probing):
    in particular, they found that more attention heads focus on the text modality
    than on the visual modality.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch16.html#id3853-marker)) 曹继泽等人后来在他们的论文[“幕后：揭示预训练视觉和语言模型之谜”](https://homl.info/probing)中提供了一些支持这一观点的经验证据：特别是，他们发现更多的注意力头集中在文本模态上，而不是视觉模态上。
- en: ^([25](ch16.html#id3870-marker)) Alec Radford et al., “Learning Transferable
    Visual Models From Natural Language Supervision”, arXiv preprint arXiv:2103.00020
    (2021).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch16.html#id3870-marker)) Alec Radford等人，“从自然语言监督中学习可迁移的视觉模型”，arXiv预印本
    arXiv:2103.00020 (2021)。
- en: ^([26](ch16.html#id3873-marker)) The training code and data were not released
    by OpenAI, but Gabriel Ilharco et al. created [OpenCLIP](https://homl.info/openclip)
    which is a flexible open source replication of CLIP with the full training code
    and data.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch16.html#id3873-marker)) 训练代码和数据并未由OpenAI发布，但Gabriel Ilharco等人创建了[OpenCLIP](https://homl.info/openclip)，这是一个具有完整训练代码和数据的灵活开源CLIP复制品。
- en: ^([27](ch16.html#id3874-marker)) This contrastive loss was first introduced
    as the *multiclass n-pair loss* in a [2016 paper by Kihyuk Sohn](https://homl.info/npairloss),
    then used for contrastive representation learning and renamed to *InfoNCE* (information
    noise-contrastive estimation) in a [2018 paper by Aaron van den Oord et al](https://homl.info/infonce).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch16.html#id3874-marker)) 这种对比损失最初是在 2016 年由 Kihyuk Sohn 的 [一篇论文](https://homl.info/npairloss)
    中作为 *多类 n 对损失* 介绍，然后用于对比表示学习，并在 2018 年由 Aaron van den Oord 等人发表的 [一篇论文](https://homl.info/infonce)
    中更名为 *InfoNCE*（信息噪声对比估计）。
- en: ^([28](ch16.html#id3883-marker)) Aditya Ramesh et al., “Zero-Shot Text-to-Image
    Generation”, arXiv preprint arXiv:2102.12092 (2021).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch16.html#id3883-marker)) 阿迪亚·拉梅什等人，“零样本文本到图像生成”，arXiv 预印本 arXiv:2102.12092（2021）。
- en: ^([29](ch16.html#id3886-marker)) Aditya Ramesh et al., “Hierarchical Text-Conditional
    Image Generation with CLIP Latents”, arXiv preprint arXiv:2204.06125 (2022).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch16.html#id3886-marker)) 阿迪亚·拉梅什等人，“使用 CLIP 潜在的分层文本条件图像生成”，arXiv 预印本
    arXiv:2204.06125（2022）。
- en: '^([30](ch16.html#id3898-marker)) Andrew Jaegle et al., “Perceiver: General
    Perception with Iterative Attention”, arXiv preprint arXiv:2103.03206 (2021).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch16.html#id3898-marker)) 安德鲁·耶格尔等人，“Perceiver：具有迭代注意力的通用感知”，arXiv 预印本
    arXiv:2103.03206（2021）。
- en: '^([31](ch16.html#id3901-marker)) If Δ is the spacing between samples, then
    the Nyquist–Shannon sampling theorem tells us that the maximum frequency we can
    measure is *f* = 1 / 2Δ. This is why *f* stops at *μ* / 2 rather than *μ*: sampling
    at a higher resolution would not add any information, and it might introduce aliasing
    artifacts.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch16.html#id3901-marker)) 如果 Δ 是样本之间的间距，那么奈奎斯特-香农采样定理告诉我们，我们可以测量的最大频率是
    *f* = 1 / 2Δ。这就是为什么 *f* 停在 *μ* / 2 而不是 *μ*：以更高的分辨率采样不会增加任何信息，还可能引入混叠伪影。
- en: ^([32](ch16.html#id3910-marker)) [AudioSet](https://homl.info/audioset) contains
    over 2 million video segments of 10s each, sorted into over 500 classes.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch16.html#id3910-marker)) [AudioSet](https://homl.info/audioset) 包含超过
    200 万个 10 秒的视频片段，分为超过 500 个类别。
- en: ^([33](ch16.html#id3911-marker)) [ModelNet40](https://homl.info/modelnet) is
    a synthetic dataset of 3D point clouds of various shapes, such as airplanes or
    cars. A common source of point clouds in real life is LiDAR sensors.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch16.html#id3911-marker)) [ModelNet40](https://homl.info/modelnet) 是一个包含各种形状的
    3D 点云合成数据集，例如飞机或汽车。现实生活中点云的常见来源是激光雷达传感器。
- en: '^([34](ch16.html#id3915-marker)) Andrew Jaegle et al., “Perceiver IO: A General
    Architecture for Structured Inputs & Outputs”, arXiv preprint arXiv:2107.14795
    (2021).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch16.html#id3915-marker)) 安德鲁·耶格尔等人，“Perceiver IO：一种用于结构化输入和输出的通用架构”，arXiv
    预印本 arXiv:2107.14795（2021）。
- en: ^([35](ch16.html#id3931-marker)) In the French comic series *Astérix*, Obélix
    is a big and friendly Gaul, and Idéfix is his clever little dog.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch16.html#id3931-marker)) 在法国漫画系列《阿斯特rix》中，奥贝利克斯是一个高大友善的高卢人，而伊德菲克斯是他的聪明小狗。
- en: '^([36](ch16.html#id3938-marker)) Junnan Li et al., “BLIP: Bootstrapping Language-Image
    Pre-training for Unified Vision-Language Understanding and Generation”, arXiv
    preprint arXiv:2201.12086 (2022).'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch16.html#id3938-marker)) 李俊南等人，“BLIP：用于统一视觉语言理解和生成的自举语言-图像预训练”，arXiv
    预印本 arXiv:2201.12086（2022）。
- en: '^([37](ch16.html#id3951-marker)) Junnan Li et al., “BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language Models”, arXiv preprint
    arXiv:2301.12597 (2023).'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch16.html#id3951-marker)) 李俊南等人，“BLIP-2：使用冻结图像编码器和大型语言模型的自举语言-图像预训练”，arXiv
    预印本 arXiv:2301.12597（2023）。
- en: ^([38](ch16.html#id3959-marker)) The idea of training a single model capable
    of both encoding and generating text was introduced in 2019 by Microsoft researchers
    Li Dong et al. with their [UniLM model](https://homl.info/unilm).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ^([38](ch16.html#id3959-marker)) 能够同时编码和生成文本的单个模型的想法是在 2019 年由微软研究人员李东等人提出的，他们推出了
    [UniLM 模型](https://homl.info/unilm)。
- en: '^([39](ch16.html#id3976-marker)) Most modern smartphones contain an inertial
    measurement unit (IMU) sensor: it measures acceleration, angular velocity, and
    often the magnetic field strength.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch16.html#id3976-marker)) 大多数现代智能手机都包含一个惯性测量单元（IMU）传感器：它测量加速度、角速度，以及通常的磁场强度。
