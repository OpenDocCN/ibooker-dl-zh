- en: 12 Evaluations and benchmarks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 评估和基准
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the significance of benchmarking and evaluating LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基准测试和评估LLMs的重要性
- en: Learning different evaluation metrics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习不同的评估指标
- en: Benchmarking model performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试模型性能
- en: Implementing comprehensive evaluation strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施全面的评估策略
- en: Best practices for evaluation benchmarks and key evaluation criteria to consider
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估基准的最佳实践和需要考虑的关键评估标准
- en: Taking into account the recent surge of interest in GenAI and specifically in
    large language models (LLMs), it’s crucial to approach these novel and uncertain
    features cautiously and responsibly. Many leaderboards and studies have shown
    that LLMs can match human performance in various tasks, such as taking standardized
    tests or creating art, sparking enthusiasm and attention. However, their novelty
    and uncertainties necessitate careful handling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到近期对通用人工智能（GenAI）以及大型语言模型（LLMs）的兴趣激增，谨慎和负责任地处理这些新颖且不确定的特性至关重要。许多排行榜和研究表明，LLMs在各种任务中可以匹配人类的表现，例如参加标准化测试或创作艺术，引发了热情和关注。然而，它们的创新性和不确定性需要谨慎处理。
- en: The role of benchmarking LLMs in production deployment cannot be overstated.
    It involves evaluating performance, comparing models, guiding improvements, accelerating
    technological advancement, managing costs and latency, and ensuring efficient
    task flow for real-world applications. While evaluations are part of LLMOps, their
    criticality in ensuring LLMs meet the demands of various applications warrants
    a separate discussion in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产部署中基准测试大型语言模型（LLMs）的作用不容小觑。它涉及评估性能、比较模型、指导改进、加速技术进步、管理成本和延迟，并确保现实应用中任务流的效率。虽然评估是LLMOps的一部分，但它们在确保LLMs满足各种应用需求中的关键性，使得本章需要单独讨论。
- en: Evaluating LLMs is not a simple task but a complex and multifaceted process
    that demands quantitative and qualitative approaches. When evaluating LLMs, comprehensive
    assessment methods covering various aspects of model performance and effect must
    be employed. Stanford University’s Human-Centered Artificial Intelligence (HAI)
    publishes an annual AI Index report [1] that aims to collate and track different
    data points related to AI. One of the most significant challenges we face is the
    lack of standardized evaluations, which makes a systematic comparison between
    different models incredibly difficult when it comes to capabilities and potential
    risks and harms. This means we don’t have an objective measure of how good or
    smart any of these specific models are, which underscores the complexity and importance
    of the evaluation process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs并非一项简单任务，而是一个复杂且多方面的过程，需要定量和定性的方法。在评估LLMs时，必须采用涵盖模型性能和效果的各个方面进行全面评估的方法。斯坦福大学的人本人工智能（HAI）每年发布一份AI指数报告[1]，旨在收集和跟踪与AI相关的不同数据点。我们面临的最具挑战性的问题之一是缺乏标准化的评估，这使得在能力和潜在风险及危害方面对不同模型进行系统比较变得极其困难。这意味着我们没有客观的衡量标准来衡量这些特定模型的好坏或智能程度，这突显了评估过程的复杂性和重要性。
- en: When we discuss GenAI evaluations in this initial stage, most discussions concern
    accuracy and performance evaluations that assess how well a language model can
    comprehend and produce text that resembles human language. This aspect is very
    important for applications that rely on the quality and relevance of the content
    they generate, such as chatbots, content creation, and summarization tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这一初始阶段讨论通用人工智能（GenAI）评估时，大多数讨论都集中在准确性和性能评估上，这些评估旨在衡量语言模型理解并产生类似人类语言文本的能力。这一方面对于依赖于生成内容质量和相关性的应用非常重要，例如聊天机器人、内容创作和摘要任务。
- en: 'There are three general types of evaluations that can measure accuracy and
    performance: traditional evaluation metrics that judge language quality, LLM task-specific
    benchmarks for assessing specific tasks, and human evaluations. Let’s start by
    understanding what LLM evaluations are and learn about some of the best practices
    associated with evaluations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种一般类型的评估可以衡量准确性和性能：判断语言质量的传统评估指标、评估特定任务的LLM特定基准，以及人工评估。让我们首先了解LLM评估是什么，并了解一些与评估相关的最佳实践。
- en: 12.1 LLM evaluations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 LLM评估
- en: 'It is essential to evaluate LLMs to ensure they are reliable and appropriate
    for real-world applications. A strong evaluation strategy covers performance metrics
    such as accuracy, fluency, coherence, and relevance. These metrics help us to
    understand the model’s advantages and disadvantages across different contexts.
    I summarize here a few areas as best practices to consider when evaluating LLMs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM至关重要，以确保它们可靠且适用于实际应用。一个强大的评估策略涵盖了性能指标，如准确性、流畅性、连贯性和相关性。这些指标帮助我们了解模型在不同情境下的优缺点。以下是我总结的几个作为最佳实践考虑的领域：
- en: To evaluate the LLM meaningfully, it must be tested on the use cases it is designed
    for, meaning using the model on various natural language processing (NLP) tasks,
    such as summarization, question-answering, and translation. The evaluation process
    should use standard metrics such as ROUGE (Recall-Oriented Understudy for Gisting
    Evaluation) for summarization to maintain reliability and comparability.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了对LLM进行有意义的评估，必须在其设计的使用案例上进行测试，这意味着在多种自然语言处理（NLP）任务上使用模型，例如摘要、问答和翻译。评估过程应使用标准指标，如ROUGE（Recall-Oriented
    Understudy for Gisting Evaluation）进行摘要，以保持可靠性和可比性。
- en: Another important aspect of LLM evaluation is the creation of prompts. Prompts
    must be unambiguous and fair, providing a valid assessment of the model’s abilities.
    This ensures that the evaluation outcomes reflect the model’s actual performance.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM评估的另一个重要方面是提示（prompts）的创建。提示必须明确且公平，以提供对模型能力的有效评估。这确保了评估结果反映了模型的实际性能。
- en: Benchmarking is a crucial practice that enables evaluating an LLM’s performance
    based on existing criteria and other models. This not only tracks progress but
    also identifies areas requiring improvement. A continuous evaluation process,
    combined with constant development practices, allows for periodic assessment and
    refinement of the LLM.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试（benchmarking）是一项关键实践，它使人们能够根据现有标准和其它模型评估LLM的性能。这不仅跟踪了进展，还确定了需要改进的领域。结合持续的开发实践，持续的评估过程允许定期评估和改进LLM。
- en: The evaluation of LLMs must involve ethical considerations at every step. The
    process must check the model for biases, fairness, and ethical problems, looking
    at the training data and the outputs. Moreover, the user experience should be
    a key part of the evaluation, ensuring that the model’s outputs match user needs
    and expectations.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的评估必须在每个步骤都涉及伦理考量。该过程必须检查模型是否存在偏见、公平性和伦理问题，并关注训练数据和输出。此外，用户体验应该是评估的关键部分，确保模型输出符合用户需求和期望。
- en: The evaluation must be transparent at every stage. Recording the criteria, methods,
    and results allows for independent verification and increases confidence in the
    LLM’s abilities. Finally, the evaluation outcomes should inform a continuous improvement
    cycle, improving the model, training data, and the evaluation process based on
    performance measures and feedback.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估必须在每个阶段都是透明的。记录标准、方法和结果允许独立验证，并增加对LLM能力的信心。最后，评估结果应指导持续改进周期，根据性能指标和反馈改进模型、训练数据和评估过程。
- en: These practices underscore the importance of a rigorous and systematic approach
    to evaluating LLMs, ensuring that they are accurate but also fair, ethical, and
    suitable for various applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实践强调了以严格和系统的方法评估LLM的重要性，确保它们准确但同时也公平、道德且适用于各种应用。
- en: By following these practices, enterprises can conduct reliable and effective
    evaluations, developing trustworthy and helpful LLMs for different uses. Now that
    we know what evaluations are, let’s take a look at some metrics we should use.
    They can be categorized into traditional and newer LLM-specific evaluation metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些实践，企业可以开展可靠有效的评估，开发出值得信赖且有助于不同用途的LLM。既然我们已经了解了评估是什么，让我们来看看我们应该使用的某些指标。它们可以分为传统和较新的LLM特定评估指标。
- en: 12.2 Traditional evaluation metrics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 传统评估指标
- en: BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for
    Gisting Evaluation), and BERTScore (BERT Similarity Score) are some of the more
    standardized metrics. These metrics help quantify the linguistic quality of model
    outputs against reference texts and are used to evaluate text quality in tasks
    such as machine translation or text summarization. Still, they differ in their
    approaches and focus on different aspects of the text. Table 12.1 shows a detailed
    explanation of what each of the three scores indicates. We will show how to compute
    these in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评估助手）、ROUGE（基于回忆的摘要评估助手）和BERTScore（BERT相似度得分）是一些较为标准化的度量标准。这些度量标准有助于量化模型输出与参考文本之间的语言质量，并用于评估机器翻译或文本摘要等任务中的文本质量。尽管如此，它们在方法和关注的文本方面有所不同。表12.1展示了这三个分数的详细解释。我们将在下一节中展示如何计算这些度量。
- en: Table 12.1 Traditional evaluation metrics
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.1 传统评估度量
- en: '| Metric | Focus | Method | Limitations |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 度量标准 | 重点关注 | 方法 | 局限性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BLEU  | It primarily measures precision, the percentage of words in the machine-
    generated text that appear in the reference text.  | It compares n-grams (word
    sequences) of the candidate translation with the reference translation and counts
    the matches.  | It can miss the mark on semantic meaning because it doesn’t account
    for synonyms or the context of words. It also doesn’t handle word reordering well.  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| BLEU  | 它主要衡量精确度，即机器生成文本中出现在参考文本中的单词百分比。  | 它将候选翻译的n-gram（单词序列）与参考翻译进行比较，并计算匹配项。  |
    由于它没有考虑同义词或单词的上下文，因此它可能会错过语义意义。它还处理不好单词重排。  |'
- en: '| ROUGE  | It is more recall oriented, focusing on the percentage of words
    from the reference text that appear in the generated text.  | It has several variants,
    such as ROUGE-N, which compares n-grams, and ROUGE-L, which looks at the longest
    common subsequence.  | Like BLEU, ROUGE can overlook semantic similarities and
    paraphrasing because it’s based on exact word matches.  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE  | 它更侧重于回忆，关注参考文本中出现在生成文本中的单词百分比。  | 它有几个变体，如ROUGE-N，它比较n-gram，以及ROUGE-L，它查看最长公共子序列。  |
    与BLEU一样，ROUGE可能会忽略语义相似性和释义，因为它基于精确的词匹配。  |'
- en: '| BERTScore  | It evaluates semantic similarity rather than relying on exact
    word matches.  | It uses contextual embeddings from models such as BERT to represent
    the text and calculates the cosine similarity between these embeddings.  | It
    can capture paraphrasing and semantic meaning better than BLEU and ROUGE because
    it considers each word’s context.  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore  | 它评估语义相似性，而不是依赖于精确的词匹配。  | 它使用BERT等模型生成的上下文嵌入来表示文本，并计算这些嵌入之间的余弦相似度。  |
    由于它考虑了每个词的上下文，因此它比BLEU和ROUGE更好地捕捉到释义和语义意义。  |'
- en: Metrics such as ROUGE, BLEU, and BERTScore compare the similarities between
    text generated by an LLM and reference text written by humans. They are commonly
    used for evaluating tasks such as summarization and machine translation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 比如ROUGE、BLEU和BERTScore等度量标准，它们比较由LLM生成的文本和人类编写的参考文本之间的相似性。它们通常用于评估摘要和机器翻译等任务。
- en: 12.2.1 BLEU
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 BLEU
- en: BLEU (Bilingual Evaluation Understudy) [2] is an algorithm used to evaluate
    the quality of machine-translated text from one natural language to another. Its
    central idea is to measure the correspondence between a machine’s output and that
    of a human translator. In other words, according to BLEU, the closer a machine
    translation is to a professional human translation, the better it is. BLEU does
    not consider intelligibility or grammatical correctness; it focuses on content
    overlap.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评估助手）[2]是一种用于评估从一种自然语言到另一种自然语言的机器翻译质量的算法。其核心思想是衡量机器输出与人类翻译之间的对应关系。换句话说，根据BLEU，机器翻译越接近专业的人类翻译，质量就越好。BLEU不考虑可理解性或语法正确性；它关注内容重叠。
- en: 12.2.2 ROUGE
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 ROUGE
- en: 'ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [3] is a set of measures
    used in NLP to assess how well automatic text summarization and machine translation
    perform. Its main goal is to contrast summaries or translations produced by machines
    with human reference summaries. It evaluates the following aspects:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE（基于回忆的摘要评估助手）[3]是一组用于NLP中评估自动文本摘要和机器翻译性能的度量标准。其主要目标是对比机器生成的摘要或翻译与人类参考摘要。它评估以下方面：
- en: '*Recall*—ROUGE measures how much of the reference summary the system summary
    captures. It evaluates how well the system recovers or captures content from the
    reference.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率*——ROUGE衡量系统摘要捕获了多少参考摘要的内容。它评估系统恢复或捕获参考内容的效果。'
- en: '*Precision*—It also assesses how much of the system summary is relevant, needed,
    or useful.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确度*——它还评估系统摘要中有多少是相关、必要或有用的。'
- en: '*F-measure*—It combines precision and recall to provide a balanced view of
    system performance.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F度量*——它结合精确度和召回率，提供一个平衡的系统性能视图。'
- en: ROUGE has different versions, such as ROUGE-N (which uses n-grams) and ROUGE-L
    (based on the Longest Common Subsequence algorithm). By looking at single words
    and sequences, ROUGE helps us measure the effectiveness of NLP algorithms in summarization
    and translation tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE有不同的版本，如ROUGE-N（使用n-gram）和ROUGE-L（基于最长公共子序列算法）。通过观察单个单词和序列，ROUGE帮助我们衡量NLP算法在摘要和翻译任务中的有效性。
- en: However, ROUGE has limitations. It relies solely on surface-level overlap and
    doesn’t account for semantic meaning or fluency. Sensitivity to stop words, stemming,
    and word order can affect scores. While ROUGE provides valuable insights, it’s
    essential to consider other evaluation metrics and human judgment to assess summary
    quality comprehensively. Researchers often use a combination of metrics to evaluate
    summarization models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ROUGE存在局限性。它仅依赖于表面层的重叠，不考虑语义意义或流畅性。对停用词、词干提取和词序的敏感性会影响分数。虽然ROUGE提供了有价值的见解，但考虑其他评估指标和人类判断来全面评估摘要质量是至关重要的。研究人员通常使用多种指标来评估摘要模型。
- en: 12.2.3 BERTScore
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 BERTScore
- en: 'BERTScore [4] is a measure of how good text generation is. It uses pretrained
    BERT model embeddings to compare candidate and reference sentences. The idea is
    to find similar words in the candidate and reference sentences based on cosine
    similarity. This metric agrees with human opinion in sentence- and system-level
    evaluations. It has the following elements:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore [4] 是衡量文本生成质量的一个指标。它使用预训练的BERT模型嵌入来比较候选句和参考句。其思路是基于余弦相似性，在候选句和参考句中找到相似单词。这个指标在句子级和系统级评估中与人类意见一致。它包含以下要素：
- en: '*Contextual embeddings*—BERTScore represents both the candidate and reference
    sentences with embeddings that consider each word’s context.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文嵌入*——BERTScore使用考虑每个单词上下文的嵌入来表示候选句和参考句。'
- en: '*Cosine similarity*—It calculates the cosine similarity between the embeddings
    of the candidate and reference texts.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*余弦相似度*——它计算候选文本和参考文本嵌入之间的余弦相似度。'
- en: '*Token matching*—To compute precision and recall scores, each token in the
    candidate text matches the most similar token in the reference text.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记匹配*——为了计算精确度和召回率分数，候选文本中的每个标记都与参考文本中最相似的标记进行匹配。'
- en: '*F1 score*—The precision and recall are combined to calculate the F1 score,
    providing a single quality measure.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F1分数*——将精确度和召回率结合起来计算F1分数，提供一个单一的质量度量。'
- en: The key advantage of BERTScore over traditional metrics such as BLEU is its
    ability to capture semantic similarity. This means it can recognize when different
    words have similar meanings and when the same words are used in different contexts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore相对于传统的指标如BLEU的关键优势在于其捕捉语义相似性的能力。这意味着它能够识别不同单词具有相似含义的情况，以及相同单词在不同语境中的使用。
- en: 12.2.4 An example of traditional metric evaluation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 传统指标评估的示例
- en: Let’s bring it all together and make it real through a simple example. Here
    we have two information summaries and can evaluate which one might be better.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子将所有这些内容结合起来，使其变得真实。这里我们有两个信息摘要，可以评估哪一个可能更好。
- en: For this example, we take the AI development principles of the Bill and Melinda
    Gates Foundation as the article we want to analyze and understand. This article
    is available at [https://mng.bz/vJe4](https://mng.bz/vJe4). From the article,
    we create two summaries that we’ll compare. In this case, one is created by NLTK
    and the other by another LLM (GPT-3.5). This could also be two different human-written
    versions or any other combination. We use the `newspaper3K` and `bert_score` packages
    to download the article and the Hugging Face Evaluate package for the evaluations.
    These can be installed in conda using `conda` `install` `-c` `conda-forge` `newspaper3k`
    `evaluate` `bert_score`. In pip, use `pip install evaluate newspaper3k bert_score`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将比尔及梅琳达·盖茨基金会的AI开发原则作为我们想要分析和理解的文章。这篇文章可在[https://mng.bz/vJe4](https://mng.bz/vJe4)找到。从文章中，我们创建了两个摘要进行比较。在这种情况下，一个是NLTK创建的，另一个是由另一个LLM（GPT-3.5）创建的。这也可以是两个不同的人写版本或任何其他组合。我们使用`newspaper3K`和`bert_score`包下载文章，并使用Hugging
    Face Evaluate包进行评估。这些可以在conda中使用`conda install -c conda-forge newspaper3k evaluate
    bert_score`安装。在pip中，使用`pip install evaluate newspaper3k bert_score`。
- en: We use `newspaper3k` to download and parse the article first. Then we apply
    the `nlp()` function to process the article and get the summary from the summary
    property. We must ensure the article is downloaded and parsed before using NLP;
    note that this only works for Western languages. We use the summary created by
    NLP as our reference summary and the `Evaluate` library to calculate the specific
    metrics. The listing shows the code to implement this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`newspaper3k`下载并解析文章。然后，我们应用`nlp()`函数处理文章，并从摘要属性中获取摘要。我们必须确保在应用NLP之前文章已被下载并解析；请注意，这仅适用于西方语言。我们将NLP创建的摘要作为我们的参考摘要，并使用`Evaluate`库来计算特定指标。代码示例展示了如何实现这一点。
- en: Listing 12.1 Automated evaluation metrics
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.1 自动评估指标
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Sets up the OpenAI details'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置OpenAI详细信息'
- en: '#2 Function to download and parse the article; returns both the article text
    and a summary'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 下载和解析文章的函数；返回文章文本和摘要'
- en: '#3 Summarizes the article using OpenAI'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用OpenAI总结文章'
- en: '#4 Function to calculate metrics (BLEU, ROUGE, etc.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算指标（BLEU、ROUGE等）的函数'
- en: '#5 Configures newspaper3k to allow downloading articles'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 配置newspaper3k以允许下载文章'
- en: 'This is the output we can observe when executing the code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是执行代码时我们可以观察到的输出：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we have seen, the BLEU score is composed of several components that collectively
    assess the quality of a machine-generated translation against a set of reference
    translations. Let’s examine each component and see what it means, starting with
    the BLEU score outlined in table 12.2\. Tables 12.3 and 12.4 show the results
    for the ROUGE score and the BERT score, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，BLEU分数由几个组成部分构成，这些组成部分共同评估机器翻译与一组参考翻译之间的质量。让我们逐一检查每个组成部分，并了解其含义，从表12.2中概述的BLEU分数开始。表12.3和12.4分别显示了ROUGE分数和BERT分数的结果。
- en: Table 12.2 BLEU score
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.2 BLEU分数
- en: '| Component value | Meaning |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 组件值 | 含义 |'
- en: '| --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| BLEU: 0.047 (4.7%)  | This is the overall BLEU score, which is quite low.
    BLEU scores range from 0 to 1 (or 0% to 100%), with higher scores indicating better
    translation quality. A score below 10% is generally considered poor.  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| BLEU：0.047（4.7%） | 这是整体BLEU分数，相当低。BLEU分数的范围从0到1（或0%到100%），分数越高表示翻译质量越好。分数低于10%通常被认为很差。
    |'
- en: '| Precisions  | These are the n-gram precision scores for 1-gram, 2-gram, 3-gram,
    and 4-gram matches. Our scores indicate a decent number of 1-gram matches but
    few longer matches, suggesting that the translation has some correct words but
    lacks coherent phrases and sentences.  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 这些是1-gram、2-gram、3-gram和4-gram匹配的n-gram精确度分数。我们的分数表明有相当数量的1-gram匹配，但较长的匹配很少，这表明翻译中有一些正确的单词，但缺乏连贯的短语和句子。
    |'
- en: '| Brevity penalty: 1.0  | This means there was no penalty for brevity; the
    translation length was appropriate compared to the reference length.  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 简洁性惩罚：1.0 | 这意味着没有简洁性惩罚；翻译长度与参考长度相比是合适的。 |'
- en: '| Length ratio: 1.27  | The translation is 27% longer than the reference, which
    might suggest some verbosity.  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 长度比：1.27 | 翻译比参考文本长27%，这可能表明有些冗长。 |'
- en: '| Translation length: 140  | The length of the machine-translated text  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 翻译长度：140 | 机器翻译文本的长度 |'
- en: '| Reference length: 110  | The length of the reference text  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 参考长度：110 | 参考文本的长度 |'
- en: Table 12.3 ROUGE score
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.3 ROUGE分数
- en: '| Component value | Meaning |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 组件值 | 含义 |'
- en: '| --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ROUGE-1: 0.3463 (34.63%)  | It measures the overlap of 1-gram between the
    system output and the reference summary. A moderate score indicates a fair amount
    of overlap.  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1：0.3463（34.63%） | 它衡量系统输出和参考摘要之间1-gram的重叠。一个适中的分数表明有相当多的重叠。 |'
- en: '| ROUGE-2 : 0.0961 (9.61%)  | It measures the overlap of bigrams and is a stricter
    metric than ROUGE-1\. A low score suggests that the system struggles to form accurate
    phrases.  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2：0.0961（9.61%） | 它衡量bigram的重叠，比ROUGE-1更严格。低分数表明系统在形成准确的短语方面有困难。 |'
- en: '| ROUGE-L: 0.1645 (16.45%)  | It measures the longest common subsequence, indicating
    the fluency and order of the words. The score suggests limited fluency.  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L：0.1645（16.45%） | 它衡量最长公共子序列，表明单词的流畅性和顺序。该分数表明流畅性有限。 |'
- en: '| ROUGE-Lsum: 0.2684 (26.84%)  | It is similar to ROUGE-L but considers the
    sum of the longest common subsequences, indicating a slightly better grasp of
    the content structure.  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-Lsum：0.2684（26.84%） | 它类似于ROUGE-L，但考虑了最长公共子序列的总和，表明对内容结构的理解略好。 |'
- en: Table 12.4 BERT Score
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.4 BERT分数
- en: '| Component value | Meaning |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 组件值 | 含义 |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Precision: 0.8524 (85.24%)  | It measures how many words in the candidate
    text are relevant or needed.  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 精确率：0.8524（85.24%） | 它衡量候选文本中有多少单词是相关或必要的。 |'
- en: '| Recall: 0.8710 (87.10%)  | It measures how much of the candidate text captures
    the reference content.  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 召回率：0.8710（87.10%） | 它衡量候选文本捕捉参考内容多少。 |'
- en: '| F1 Score: 0.8616 (86.16%)  | This harmonic mean of precision and recall provides
    a single score that balances both. An F1 score closer to 1 indicates better performance.  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| F1分数：0.8616（86.16%） | 这是精确率和召回率的调和平均值，提供了一个平衡两者的单一分数。F1分数接近1表示更好的性能。 |'
- en: The BLEU and ROUGE scores suggest that the translation or summary has room for
    improvement, particularly in forming coherent phrases and sentences. However,
    the BERT score is quite high, indicating that the candidate text is semantically
    similar to the reference text and captures most of its content. Thus, while the
    translation may not match the reference text word for word, it does convey the
    same overall meaning quite well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU和ROUGE分数表明，翻译或摘要还有改进的空间，尤其是在形成连贯的短语和句子方面。然而，BERT分数相当高，表明候选文本在语义上与参考文本相似，并捕捉了其大部分内容。因此，尽管翻译可能不会逐词与参考文本匹配，但它确实很好地传达了相同的基本意义。
- en: Even though metrics such as BERTScore, ROUGE, and BLEU help compare similar
    text, they primarily focus on surface-level similarity. They may not capture semantic
    equivalence or the overall quality of the generated text. These more traditional
    metrics often penalize LLMs, which can produce coherent and fluent generations.
    For these, we need LLM task-specific benchmarks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管BERTScore、ROUGE和BLEU等指标有助于比较相似文本，但它们主要关注表面相似性。它们可能无法捕捉语义等价性或生成文本的整体质量。这些更传统的指标通常会对能够生成连贯流畅文本的LLM（大型语言模型）进行惩罚。对于这些情况，我们需要LLM特定任务的基准。
- en: 12.3 LLM task-specific benchmarks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 LLM特定任务的基准
- en: Measuring the performance of LLMs across various NLP tasks requires task-specific
    benchmarks. They are created to test how well the models can understand, reason,
    and generate natural language for specific domains or tasks, providing a clear
    way to compare different models. These benchmarks can reveal a model’s abilities
    and limitations, enabling focused improvements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测量LLM在各种NLP任务上的性能需要特定任务的基准。它们是为了测试模型在特定领域或任务中理解、推理和生成自然语言的能力而创建的，为比较不同模型提供了明确的方式。这些基准可以揭示模型的能力和局限性，从而实现有针对性的改进。
- en: Task-specific benchmarks assess LLMs on specific NLP tasks such as text classification,
    sentiment analysis, question answering, summarization, and more. These benchmarks
    usually consist of datasets with predefined inputs and expected outputs, allowing
    for quantitative assessment of model performance through metrics such as accuracy,
    F1 score, or BLEU score, depending on the task. Some key LLM benchmarks are groundedness,
    relevance, coherence, fluency, and GPT similarity; these evaluation metrics are
    outlined in table 12.5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 特定任务的基准评估LLM在特定NLP任务（如文本分类、情感分析、问答、摘要等）上的表现。这些基准通常由具有预定义输入和预期输出的数据集组成，允许通过准确率、F1分数或BLEU分数等指标对模型性能进行定量评估，具体取决于任务。一些关键的LLM基准包括基础性、相关性、连贯性、流畅性和GPT相似性；这些评估指标在表12.5中概述。
- en: Table 12.5 LLM evaluation metrics
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.5 LLM评估指标
- en: '| Metric | Focus | Method | When to use? |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 侧重 | 方法 | 何时使用？ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Groundedness  | It evaluates how well the answers the model produces match
    the information in the source data (context that the user provides). This metric
    ensures that the context backs up the answers generated by AI.  | It evaluates
    how well the statements in an AI-generated answer match the source context, ensuring
    that the context supports these statements. It is rated from 1 (bad) to 5 (good).  |
    It is used when we want to check that the AI responses match and are confirmed
    by the given context. It is also used when being factually correct and contextually
    precise is important, such as when finding information, answering questions, and
    summarizing content.  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 基础性 | 它评估模型生成的答案与源数据（用户提供的上下文）中的信息匹配得有多好。这个指标确保 AI 生成的答案有上下文支持。 | 它评估 AI
    生成的答案中的陈述与源上下文匹配得有多好，确保上下文支持这些陈述。评分从 1（差）到 5（好）。 | 当我们想要检查 AI 响应与给定上下文匹配并得到确认时使用它。当在查找信息、回答问题和总结内容时，确保事实正确和上下文精确很重要时也使用它。
    |'
- en: '| Coherence  | It evaluates the model’s ability to generate coherent, natural
    output similar to human language.  | It evaluates how well the generation is structured
    and connected. This is rated from 1 (bad) to 5 (good).  | Use it when evaluating
    how easy and user-friendly your model’s generated responses are in real-world
    situations.  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 连贯性 | 它评估模型生成与人类语言相似、连贯的自然输出的能力。 | 它评估生成的结构化和连接性如何。这个评分从 1（差）到 5（好）。 | 在评估模型在实际场景中生成的响应的易用性和用户友好性时使用它。
    |'
- en: '| Fluency  | It measures the grammar proficiency and readability of a model’s
    generated response.  | The fluency measure evaluates how well the generated text
    follows grammatical rules, syntactic structures, and suitable word choices. It
    is scored from 1 (bad) to 5 (good).  | This tool assesses the linguistic accuracy
    of the generated text, ensuring that it follows appropriate grammar rules, syntax
    structures, and word choices.  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 流畅性 | 它衡量模型生成响应的语法熟练度和可读性。 | 流畅性衡量评估生成的文本如何遵循语法规则、句法结构和合适的词汇选择。评分从 1（差）到
    5（好）。 | 该工具评估生成文本的语言准确性，确保其遵循适当的语法规则、句法结构和词汇选择。 |'
- en: '| GPT similarity  | It compares how similar a source data (ground truth) sentence
    is to the output from an AI model.  | This assessment involves creating sentence-level
    embeddings for both the ground truth and the model’s prediction, which are high-dimensional
    vector representations that encode the semantic meaning and context of the sentences.  |
    Use it to get an unbiased measure of a model’s performance, especially in text
    generation tasks where we have the correct responses available. This lets us check
    how closely the generated text matches the intended content, which helps us evaluate
    the model’s quality and accuracy.  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GPT 相似度 | 它比较源数据（真实情况）句子与 AI 模型输出句子的相似度。 | 这种评估涉及为真实情况和模型预测创建句子级嵌入，这些是高维向量表示，编码了句子的语义意义和上下文。
    | 使用它来获得模型性能的无偏测量，特别是在我们有正确响应的文本生成任务中。这让我们可以检查生成的文本与预期内容匹配的紧密程度，有助于我们评估模型的质量和准确性。
    |'
- en: To illustrate how this works, we will apply a reference-free evaluation method
    based on the G-Eval method. Reference-free means that we do not depend on comparing
    a generated summary to a preexisting reference summary. Let’s start with understanding
    G-Eval.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明其工作原理，我们将应用基于 G-Eval 方法的无参考评估方法。无参考意味着我们不需要将生成的摘要与现有的参考摘要进行比较。让我们先了解 G-Eval。
- en: '12.3.1 G-Eval: A measuring approach for NLG evaluation'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 G-Eval：NLG 评估的测量方法
- en: G-Eval [5] introduces a new framework for measuring the quality of text produced
    by NLG systems. Using LLMs, G-Eval combines a chain-of-thought–prompting method
    with a form-filling technique to examine different aspects of the NLG output,
    such as coherence, consistency, and relevance. G-Eval judges the quality of the
    generated content based on the input prompt and text alone, without any reference
    texts, and is thus considered reference free.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: G-Eval [5] 提出了一种用于衡量自然语言生成（NLG）系统生成文本质量的新框架。利用大型语言模型（LLMs），G-Eval 结合了思维链提示方法与表格填写技术，以检查
    NLG 输出的不同方面，例如连贯性、一致性和相关性。G-Eval 仅根据输入提示和文本本身来判断生成内容的品质，无需任何参考文本，因此被认为是无参考的。
- en: 'The method is particularly useful for novel datasets and tasks with few human
    references available. This flexibility makes G-Eval suitable for various innovative
    applications, especially in fields where data is continuously evolving or is highly
    specific. Here are a few scenarios where G-Eval would be beneficial:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法特别适用于新颖的数据集和人类参考较少的任务。这种灵活性使 G-Eval 适用于各种创新应用，尤其是在数据持续演变或高度具体的领域。以下是 G-Eval
    将有益的几个场景：
- en: '*Medical report generation*—In the medical domain, where automated systems
    produce customized reports from various patient data, G-Eval can evaluate the
    reports for correctness, consistency, and medical relevance. As patient scenarios
    differ a lot, conventional reference-based metrics might not always work, making
    G-Eval a more adaptable and appropriate option that guarantees the quality and
    dependability of medical reports.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医疗报告生成*—在医疗领域，自动化的系统从各种患者数据中生成定制报告时，G-Eval 可以评估报告的正确性、一致性和医学相关性。由于患者情况差异很大，基于参考的常规指标可能并不总是适用，这使得
    G-Eval 成为一个更适应性和合适的选项，保证了医疗报告的质量和可靠性。'
- en: '*Legal document writing*—When AI creates legal documents that suit particular
    cases, G-Eval assesses how well the documents meet legal requirements, how clear
    and coherent they are, and how well they follow the rules. This is important in
    legal situations where having precise reference texts for every situation is not
    feasible, but accuracy and conformity to legal standards are vital.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*法律文件撰写*—当AI创建适合特定案例的法律文件时，G-Eval 评估这些文件在满足法律要求、清晰性和连贯性以及遵循规则方面的表现。在无法为每种情况都提供精确参考文本的法律情况下，这一点尤为重要，但准确性和符合法律标准至关重要。'
- en: '*Creative content evaluation*—Novelty is essential in fields that require creativity,
    such as advertising or video game storytelling. G-Eval helps assess the novelty,
    appeal, and target audience suitability of such content, providing a way to gauge
    the quality of creativity that is more than just word or phrase similarity.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创意内容评估*—在需要创造力的领域，如广告或视频游戏叙事，新颖性至关重要。G-Eval 帮助评估此类内容的创新性、吸引力和目标受众的适用性，提供了一种衡量创造力的质量的方法，这不仅仅是词或短语相似性的衡量。'
- en: '*AI-based content moderation*—G-Eval can help verify that moderation actions
    are suitable and successful, even when there is no reliable reference data, by
    using AI systems to moderate changing online content. This is especially important
    in online settings where context and sensitivity matter.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于AI的内容审核*—G-Eval 可以通过使用AI系统来审核不断变化的在线内容，即使在没有可靠参考数据的情况下，也能帮助验证审核措施是否合适且成功。这在需要考虑上下文和敏感性的在线环境中尤为重要。'
- en: 'These examples show how G-Eval can assess the quality of AI-generated text
    with human-like standards and flexibility to meet different needs. This is important
    for GenAI applications where conventional metrics are insufficient. G-Eval has
    many advantages for businesses that want to develop and use effective NLG solutions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了 G-Eval 如何以类似人类的标准和灵活性评估人工智能生成文本的质量，这对于传统指标不足的 GenAI 应用来说非常重要。G-Eval
    对于希望开发和使用有效自然语言生成解决方案的企业具有许多优势：
- en: G-Eval shows much better agreement with human evaluation than conventional metrics
    such as BLEU and ROUGE. This is especially clear in open-ended and creative NLG
    tasks, where conventional metrics often fail. By giving a more precise measurement
    of NLG system quality, G-EVAL helps enterprises make smart choices about their
    development and deployment.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G-Eval 与人类评估的协议度比传统的指标如 BLEU 和 ROUGE 要好得多。这在开放式和创造性的自然语言生成任务中尤为明显，因为传统的指标往往失效。通过更精确地衡量自然语言生成系统的质量，G-EVAL
    帮助企业就其开发和部署做出明智的选择。
- en: G-Eval uses the probabilities of output tokens from LLMs to produce fine-grained
    continuous scores. This enables the capture of slight differences between generated
    texts, giving more detailed feedback than traditional metrics that often depend
    on discrete scoring. Such precise feedback can be very helpful for enterprises
    as they adjust their NLG systems for the best performance.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G-Eval 使用来自大型语言模型（LLM）的输出标记的概率来生成细粒度的连续分数。这能够捕捉到生成文本之间的细微差异，比传统指标提供更详细的反馈，而传统指标往往依赖于离散评分。这种精确的反馈对于企业在调整其自然语言生成系统以获得最佳性能时非常有帮助。
- en: An interesting feature of G-Eval is that it can be customized to evaluate different
    NLG tasks by changing the prompt and evaluation criteria. This flexibility removes
    the need for task-specific evaluators, making the evaluation process easier for
    enterprises working with various NLG applications.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G-Eval的一个有趣特性是它可以被定制以通过改变提示和评估标准来评估不同的自然语言生成（NLG）任务。这种灵活性消除了对特定任务评估者的需求，使得企业在使用各种NLG应用时的评估过程更加简便。
- en: However, a possible problem with LLM-based evaluators is that they may prefer
    text generated by LLMs. This problem needs more research and solutions to ensure
    reliable and correct evaluation, especially when using LLM-based metrics to enhance
    NLG systems.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于LLM的评估者可能存在的一个问题是，它们可能更喜欢由LLM生成的文本。这个问题需要更多的研究和解决方案，以确保可靠的正确评估，尤其是在使用基于LLM的指标来增强NLG系统时。
- en: G-Eval provides a potential method for NLG evaluation in enterprises, which
    can help create and use more efficient and dependable NLG systems for different
    purposes. Let’s see how we can use this.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: G-Eval为企业提供了一种潜在的NLG评估方法，可以帮助创建和使用更高效、更可靠的NLG系统，用于不同的目的。让我们看看我们如何使用它。
- en: 'We can demonstrate how G-Eval can be very helpful with a simple example. Imagine
    an enterprise that wants to evaluate customer service chatbots. These chatbots
    are usually trained to deal with many kinds of customer questions and problems,
    and their performance is essential for maintaining customer satisfaction and loyalty.
    For example, let’s think about a customer complaint about a service. Suppose a
    customer writes the following complaint in an email:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的例子来展示G-Eval如何非常有帮助。想象一家企业想要评估客户服务聊天机器人。这些聊天机器人通常被训练来处理各种客户问题和疑问，它们的性能对于维持客户满意度和忠诚度至关重要。例如，让我们考虑一个关于服务的客户投诉。假设一位客户在电子邮件中写下以下投诉：
- en: I am extremely disappointed with the delay in service. I was promised a two-day
    delivery, and it’s already been a week without any updates. This is unacceptable.
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我对服务延迟感到极其失望。我被告知两天内交付，但现在已经过去一周了，没有任何更新。这是不可接受的。
- en: 'Now imagine two different automated responses generated by customer service
    bots:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象两个由客户服务机器人生成的不同自动响应：
- en: '*Response A (more literal and generic)*—“We apologize for any inconvenience
    caused. Your complaint has been registered. We will update you shortly.”'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*响应A（更直接和通用）*——“我们对造成的任何不便表示歉意。您的投诉已登记。我们将尽快更新您。”'
- en: '*Response B (more empathetic and specific)*—“We’re really sorry to hear about
    this delay and completely understand your frustration. It’s not the experience
    we want to provide. Our team is looking into this as a priority, and we’ll reach
    out with an update on your delivery by tomorrow morning.”'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*响应B（更具同理心和具体性）*——“我们非常抱歉听到这个延迟，并完全理解您的挫折感。这不是我们想要提供的服务体验。我们的团队正在优先处理这个问题，我们将在明天早上之前与您联系，更新您的交付情况。”'
- en: Conventional metrics such as BLEU and ROUGE would likely assess these responses
    based on how closely certain words or phrases match a set of predefined correct
    responses. Response A might score reasonably well if the reference responses favor
    generic acknowledgments. However, these metrics might miss nuances in tone and
    specificity crucial for customer satisfaction. When evaluating with G-Eval, it
    would be more likely to assess the content, tone, empathy, and relevance of the
    response to the specific complaint. It would consider how effectively the response
    addresses the customer’s emotional state and the problem raised. In our example,
    response B would likely score higher on G-Eval because it acknowledges the customer’s
    feelings, provides a specific promise, and sets clear expectations—all of which
    are important to human judges (i.e., customers) in evaluating the quality of customer
    service.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的指标，如BLEU和ROUGE，可能会根据某些单词或短语与一组预定义的正确响应的匹配程度来评估这些响应。如果参考响应倾向于通用认可，那么响应A可能会得到合理的分数。然而，这些指标可能会错过对客户满意度至关重要的语气和具体性的细微差别。当使用G-Eval进行评估时，更有可能评估响应的内容、语气、同理心和针对具体投诉的相关性。它将考虑响应如何有效地解决客户的情绪状态和提出的问题。在我们的例子中，响应B在G-Eval上可能会得到更高的分数，因为它承认了客户的感受，提供了具体的承诺，并设定了明确的期望——这些都是对人类评判者（即客户）在评估客户服务质量时非常重要的。
- en: For enterprises, particularly in areas such as customer service, the effectiveness
    of automated responses can significantly affect customer satisfaction and loyalty.
    G-Eval aligns better with human evaluation because it captures the qualitative
    aspects of communication that are important in real-life interactions—such as
    empathy, specificity, and reassurance—but that are often overlooked by traditional
    metrics such as BLEU and ROUGE.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业来说，尤其是在客户服务等领域，自动响应的有效性可以显著影响客户满意度和忠诚度。G-Eval与人类评估更一致，因为它捕捉了现实生活中互动中重要的定性方面——如同理心、具体性和保证——但这些通常被传统的指标如BLEU和ROUGE所忽视。
- en: 12.3.2 An example of LLM-based evaluation metrics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 基于LLM的评估指标示例
- en: 'In this example, we implement a G-Eval approach using Azure OpenAI’s GPT-4
    model to measure how good text summaries are. It uses the following four criteria:
    relevance, coherence, consistency, and fluency. We have an article and two summaries
    that are based on it. In addition, we use the code to score each summary on the
    four criteria and show which is better. As an example, we use the AI principles
    of the Bill and Melinda Gates Foundation, which are listed as “The first principles
    guiding our work with AI” and can be accessed online at [https://mng.bz/vJe4](https://mng.bz/vJe4).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用Azure OpenAI的GPT-4模型实现G-Eval方法来衡量文本摘要的好坏。它使用了以下四个标准：相关性、连贯性、一致性和流畅性。我们有一篇文章和两个基于它的摘要。此外，我们使用代码对每个摘要在这四个标准上进行评分，并展示哪个更好。作为一个例子，我们使用了比尔及梅琳达·盖茨基金会的AI原则，这些原则被列为“指导我们与AI合作的第一原则”，并且可以在[https://mng.bz/vJe4](https://mng.bz/vJe4)在线访问。
- en: We have two summaries made from this source article that we want to compare
    with the article. The NLP library makes one summary, and another is made by LLM
    (Google’s Gemini Pro 1.5). We have saved all these locally for easy access and
    are reading them from there. The full code where we download the article and create
    the summaries is shown in the book’s GitHub repository.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个从这篇文章中制作的摘要，我们想要将其与文章进行比较。NLP库制作了一个摘要，另一个是由LLM（谷歌的Gemini Pro 1.5）制作的。我们已经将这些内容保存在本地以便于访问，并且从那里读取它们。下载文章并创建摘要的完整代码可以在本书的GitHub仓库中找到。
- en: Listings 12.2 and 12.3 show the key areas of this example with the full code.
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)). We define the evaluation
    metrics and their criteria and steps using prompt engineering and RAG. Each describes
    the scoring criteria and the steps to follow when evaluating a summary. Note that
    we don’t show all the code for brevity reasons.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.2和12.3展示了本例的关键区域以及完整代码。([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))。我们使用提示工程和RAG定义了评估指标及其标准和方法。每个都描述了评分标准以及在评估摘要时应遵循的步骤。请注意，为了简洁起见，我们没有展示所有代码。
- en: Listing 12.2 LLM-based evaluation metrics
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.2 基于LLM的评估指标
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Evaluation prompt template based on G-Eval'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 基于G-Eval的评估提示模板'
- en: '#2 Defines the relevance metric as outlined by G-Eval'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义了G-Eval概述的相关性指标'
- en: '#3 Outlines the rules of the relevance metrics and how to measure'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 概述了相关性指标的规则以及如何进行测量'
- en: '#4 Defines the coherence metric as outlined by G-Eval'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 定义了G-Eval概述的连贯性指标'
- en: '#5 Outlines the rules of the coherence metric and how to measure'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 概述了连贯性指标的规则以及如何进行测量'
- en: '#6 Defines the consistency metric as outlined by G-Eval'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 定义了一致性指标，如G-Eval所述'
- en: '#7 Outlines the rules of the consistency metrics and how to measure'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 概述了一致性指标的规则以及如何进行测量'
- en: '#8 Defines the fluency metric as outlined by G-Eval'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 定义了G-Eval概述的流畅性指标'
- en: '#9 Outlines the rules of the fluency metrics and how to measure'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 概述了流畅性指标的规则以及如何进行测量'
- en: 'We have already explained the prompts that define the metrics and the rules
    for computing them. Now look at the rest of the code in listing 12.3\. This is
    simple, and we do the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了定义指标和计算规则的提示。现在看看列表12.3中的其余代码。这很简单，我们做以下操作：
- en: Use the `get_article()` function to get the article and summaries.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`get_article()`函数获取文章和摘要。
- en: Use the `get_geval_score()` function, loop over the evaluation metrics and summaries,
    generate a G-Eval score for each combination, and store the results in a dictionary.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`get_geval_score()`函数，遍历评估指标和摘要，为每个组合生成一个G-Eval评分，并将结果存储在字典中。
- en: Finally, convert the dictionary to a data frame so we can pivot it and print
    it to the console.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，将字典转换为数据框，以便我们可以进行转置并打印到控制台。
- en: Note  The parameters for the Azure OpenAI are quite strict, with `max_ tokens`
    set to 5, `temperature` set to 0, and `top_p` set to 1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Azure OpenAI 的参数设置相当严格，`max_ tokens` 设置为 5，`temperature` 设置为 0，`top_p` 设置为
    1。
- en: Listing 12.3 LLM-based evaluation metrics
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.3 基于 LLM 的评估指标
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Function to load the files from disk'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 函数用于从磁盘加载文件'
- en: '#2 Function to calculate various evaluation metrics'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 函数用于计算各种评估指标'
- en: '#3 Sets up the prompt for G-Eval'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置 G-Eval 的提示'
- en: '#4 Completion API to run the evaluation'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 完成API以运行评估'
- en: '#5 Dictionary to store the evaluation results'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 字典用于存储评估结果'
- en: '#6 Loops over the evaluation metrics and summaries'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 遍历评估指标和摘要'
- en: '#7 Checks if result is not empty and if it is a number'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 检查结果是否为空以及是否为数字'
- en: '#8 Evaluation result stored in a dictionary'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 评估结果存储在字典中'
- en: '#9 Converts the dictionary to a Pandas DataFrame'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 将字典转换为 Pandas DataFrame'
- en: '#10 Pivots the DataFrame to allow easy visualization'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 将 DataFrame 旋转以方便可视化'
- en: 'The output of this is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出的结果如下：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s see how we interpret these results and what these scores mean:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何解释这些结果以及这些分数的含义：
- en: Coherence measures how logically and smoothly ideas transition from one sentence
    to another. A score of 5 for the LLM summary indicates it presents information
    logically, is well-organized, and is easy for readers to follow. The traditional
    NLP summary, with a score of 1, likely struggles with disjointed ideas or lacks
    logical flow, which makes it difficult for readers to understand the sequence
    or connection of thoughts.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性衡量的是文本中是否存在矛盾以及在整个摘要中保持相同标准。LLM 的 5 分表明它保持了一致的语气、风格和事实准确性。虽然很好，但传统的 NLP 的
    4 分表明在保持这些元素一致性方面存在一些小问题。
- en: Consistency relates to the absence of contradictions within the text and maintaining
    the same standards throughout the summary. The LLM’s high score of 5 suggests
    it maintains a uniform tone, style, and factual accuracy. While good, the traditional
    NLP’s score of 4 indicates minor problems with maintaining these elements uniformly.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性关系到文本中不存在矛盾以及在整个摘要中保持相同标准。LLM 的高分为 5，表明它保持了一致的语气、风格和事实准确性。虽然很好，但传统的 NLP 的
    4 分表明在保持这些元素一致性方面存在一些小问题。
- en: Fluency assesses the text’s smoothness and the language’s naturalness. A score
    of 3 for the LLM indicates moderate fluency; the language is generally clear but
    might have some awkward phrasing or complexity that could impede readability.
    The traditional NLP, scoring lower, might exhibit more significant problems such
    as grammatical errors or unnatural sentence structures.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流畅性评估文本的流畅性和语言的自然性。LLM 的 3 分表明中等流畅性；语言通常清晰，但可能有一些笨拙的表达或复杂性，可能会妨碍可读性。传统的 NLP，得分较低，可能表现出更严重的问题，如语法错误或不自然的句子结构。
- en: Relevance measures how well the summary addresses the main points and purpose
    of the original content. The LLM’s score of 5 suggests that it effectively captures
    and focuses on the key elements of the original text, providing a summary that
    meets the informational needs of the reader. The traditional NLP, with a score
    of 2, likely includes some relevant information but misses important details or
    includes irrelevant content.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性衡量摘要如何很好地反映原始内容的主要观点和目的。LLM 的 5 分表明它有效地捕捉并专注于原始文本的关键要素，提供了一个满足读者信息需求的摘要。传统的
    NLP，得分为 2，可能包含一些相关信息，但可能遗漏重要细节或包含不相关内容。
- en: As we have seen, assessing LLMs requires more than traditional tasks and includes
    more difficult benchmarks that measure higher-level understanding, logic, and
    adaptation skills. Some of these benchmarks, such as HELM, HEIM, HellaSWAG, and
    MMLU (Massive Multitask Language Understanding), are notable for their difficulty
    and scope.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，评估 LLM 需要的不仅仅是传统任务，还包括更困难的基准，这些基准衡量更高层次的理解、逻辑和适应能力。其中一些基准，如 HELM、HEIM、HellaSWAG
    和 MMLU（大规模多任务语言理解），因其难度和范围而引人注目。
- en: 12.3.3 HELM
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 HELM
- en: HELM (Holistic Evaluation of Language Models) [6] is a holistic framework for
    evaluating foundational models introduced by Stanford University. The framework
    aims to comprehensively assess language models, focusing on their abilities, limitations,
    and associated risks. Developed to enhance model transparency, HELM offers a more
    detailed understanding of model performance across diverse scenarios. It categorizes
    the extensive range of potential scenarios and metrics relevant to language models.
    A subset of these scenarios and metrics is then evaluated based on their coverage
    and practicality, ensuring that HELM is a practical and useful tool for enterprises.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: HELM（语言模型全面评估）[6]是斯坦福大学引入的用于评估基础模型的全面框架。该框架旨在全面评估语言模型，重点关注其能力、局限性和相关风险。旨在提高模型透明度，HELM提供了对模型在不同场景下性能的更深入理解。它将大量与语言模型相关的潜在场景和指标进行分类。然后根据其覆盖范围和实用性，对这些场景和指标进行评估，确保HELM是一个实用且有用的企业工具。
- en: The HELM approach uses a multimetric evaluation, assessing various factors such
    as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency
    for each chosen scenario. It consists of large-scale evaluations of different
    language models performed under standardized conditions to guarantee comparability.
    Moreover, HELM promotes openness by sharing all model prompts and completions
    with the public for further analysis. This is supplemented by a modular toolkit
    that facilitates continuous benchmarking within the community.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: HELM方法采用多指标评估，评估每个选定场景的准确性、校准、鲁棒性、公平性、偏差、毒性和效率等各个方面。它包括在不同标准化条件下进行的大规模语言模型评估，以确保可比性。此外，HELM通过向公众共享所有模型提示和完成内容来促进开放性。这还辅以一个模块化工具包，便于社区内持续基准测试。
- en: For enterprises considering the use of GenAI language models, HELM provides
    valuable information to make informed choices. It allows for the comparison of
    different models on various metrics, aiding in the selection of the most suitable
    model. Moreover, HELM helps mitigate risks by assessing potential harms such as
    bias and toxicity, enabling enterprises to identify and address problems before
    real-world application. The transparency and trust promoted by HELM through access
    to raw model predictions and evaluation data further enhance understanding and
    confidence in using LMs within an organization.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于考虑使用生成式人工智能语言模型的企业，HELM提供了有价值的信息，以做出明智的选择。它允许在不同指标上比较不同的模型，有助于选择最合适的模型。此外，HELM通过评估潜在的危害，如偏差和毒性，帮助降低风险，使企业能够在实际应用之前识别和解决问题。通过访问原始模型预测和评估数据，HELM促进的透明度和信任进一步增强了组织内部使用语言模型的理解和信心。
- en: 12.3.4 HEIM
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.4 HEIM
- en: Stanford University introduced a benchmark called Holistic Evaluation of Text-To-Image
    Models (HEIM) [7] to provide a complete quantitative analysis of the strengths
    and weaknesses of text-to-image models. Unlike other evaluations measuring text–image
    alignment and image quality, HEIM examines 12 important aspects of using models
    in the real world. Some of these aspects are aesthetics, originality, reasoning,
    knowledge, bias, toxicity, and efficiency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学引入了一个名为全面评估文本到图像模型（HEIM）[7]的基准，以提供对文本到图像模型优缺点进行全面定量分析。与其他评估文本-图像对齐和图像质量的方法不同，HEIM检查了12个在现实世界中使用模型的重要方面。其中一些方面包括美学、原创性、推理、知识、偏差、毒性和效率。
- en: HEIM takes a holistic approach to evaluating the text-to-image models by curating
    62 scenarios. This holistic approach reveals that no single model excels in all
    areas, highlighting different strengths and weaknesses among various models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: HEIM通过精心策划62个场景，对文本到图像模型进行全面评估。这种全面的方法揭示了没有哪个模型在所有领域都表现出色，突出了不同模型之间的不同优势和劣势。
- en: 'HEIM should be a key evaluation criterion for enterprises, as it provides a
    transparent and standardized way to assess text-to-image models. By understanding
    the strengths and limitations of these models, enterprises can make informed decisions
    about which models to use for specific tasks or services. Moreover, the evaluation
    helps identify potential risks such as bias or toxicity, which could have legal
    and reputational implications for businesses. Consequently, for enterprises building
    and deploying GenAI applications to production, the HEIM benchmark offers valuable
    insights across the following four dimensions:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: HEIM应成为企业的一个关键评估标准，因为它提供了一个透明和标准化的方式来评估文本到图像模型。通过了解这些模型的优点和局限性，企业可以就特定任务或服务使用哪些模型做出明智的决定。此外，评估有助于识别潜在风险，如偏见或毒性，这可能会对企业的法律和声誉产生影响。因此，对于构建和部署生成式人工智能应用的企业来说，HEIM基准在以下四个维度提供了宝贵的见解：
- en: '*Model selection*—HEIM highlights that no single model excels in all aspects.
    Enterprises must carefully evaluate and select models based on their application’s
    specific requirements. For example, applications focused on artistic creation
    might prioritize aesthetics and originality, while those requiring factual accuracy
    might focus on alignment and knowledge.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型选择*—HEIM强调没有哪个模型在所有方面都出色。企业必须根据其应用的具体需求仔细评估和选择模型。例如，专注于艺术创作的应用可能会优先考虑美学和原创性，而需要事实准确性的应用可能会侧重于对齐和知识。'
- en: '*Risk mitigation*—HEIM emphasizes evaluating bias, toxicity, and fairness.
    Enterprises must ensure their applications are ethically sound and avoid perpetuating
    harmful stereotypes or generating inappropriate content. This necessitates careful
    model selection, fine-tuning, and implementation of safety measures.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*风险缓解*—HEIM强调评估偏见、毒性和公平性。企业必须确保其应用在道德上是合理的，避免延续有害的刻板印象或生成不适当的内容。这需要谨慎的模型选择、微调和实施安全措施。'
- en: '*Performance optimization*—Evaluating reasoning, robustness, and multilinguality
    is crucial for ensuring application reliability and user satisfaction. Enterprises
    must select models that perform well across diverse scenarios and user inputs.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能优化*—评估推理、鲁棒性和多语言能力对于确保应用可靠性和用户满意度至关重要。企业必须选择在多种场景和用户输入下表现良好的模型。'
- en: '*Efficiency considerations*—Image generation efficiency affects user experience
    and operational costs. Enterprises should consider the tradeoffs between model
    size, speed, and resource requirements when selecting and deploying models.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*效率考量*—图像生成效率影响用户体验和运营成本。企业在选择和部署模型时，应考虑模型大小、速度和资源需求之间的权衡。'
- en: 'We will show an example and explain how we can apply HEIM to think about models.
    As mentioned before, HEIM assesses text-to-image models by creating scenarios
    that cover the 12 aspects it measures. If we wanted to test one of those 12 aspects—the
    text–image alignment aspect—HEIM might use a scenario where the model gets a complicated
    textual prompt and then checks how well the image it generates matches the prompt
    details and context. Here’s a possible evaluation scenario:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示一个示例并解释如何应用HEIM来思考模型。如前所述，HEIM通过创建涵盖其12个评估方面的场景来评估文本到图像模型。如果我们想测试这12个方面之一——文本-图像对齐方面——HEIM可能会使用一个场景，其中模型接收到一个复杂的文本提示，然后检查生成的图像与提示细节和背景的匹配程度。以下是一个可能的评估场景：
- en: '*Prompt*—“A futuristic cityscape at dusk with flying cars and neon signs reflecting
    in the water below.”'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示*—“黄昏时分未来派城市景观，飞车和霓虹灯在下面的水中反射。”'
- en: '*Model generation*—The model generates an image based on the prompt.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型生成*—模型根据提示生成图像。'
- en: '*Evaluation*—Human evaluators or automated metrics assess the image on various
    factors:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估*—人工评估者或自动化指标会从多个因素评估图像：'
- en: Does the image accurately depict a cityscape at dusk?
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像是否准确描绘了黄昏时的城市景观？
- en: Are there flying cars and neon signs as described?
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有描述中的飞车和霓虹灯？
- en: Is there a reflection in the water, and how realistic does it look?
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水中有反射吗？看起来有多逼真？
- en: Overall, how well does the image align with the prompt?
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，图像与提示的匹配程度如何？
- en: The model’s performance in this scenario would contribute to its overall score
    in text-image alignment. Similar scenarios would be created for other aspects,
    such as image quality, originality, reasoning, and so forth. The comprehensive
    evaluation across all 12 aspects provides insights into the model’s capabilities
    and limitations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种场景下，该模型的表现将对其在文本-图像对齐方面的总体评分做出贡献。将创建类似场景以评估其他方面，如图像质量、原创性、推理等。对所有12个方面的综合评估可以深入了解模型的能力和局限性。
- en: HEIM’s approach ensures that models are evaluated on their ability to generate
    visually appealing images and their understanding of the text, creativity, and
    potential biases or ethical concerns. This holistic evaluation is crucial for
    enterprises, as it helps them choose models that align with their values and needs,
    while being aware of the risks involved.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: HEIM的方法确保模型在生成视觉上吸引人的图像和对其文本、创造力以及潜在偏见或伦理问题的理解能力上得到评估。这种全面评估对于企业至关重要，因为它帮助他们选择与其价值观和需求相一致、同时意识到所涉及风险的模型。
- en: More details on HEIM, including the leaderboard, dataset, and other dependencies
    such as model access unified APIs, can be found at [https://crfm.stanford.edu/helm/heim/latest](https://crfm.stanford.edu/helm/heim/latest).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 关于HEIM的更多细节，包括排行榜、数据集以及其他依赖项，如模型访问统一API，可以在[https://crfm.stanford.edu/helm/heim/latest](https://crfm.stanford.edu/helm/heim/latest)找到。
- en: 12.3.5 HellaSWAG
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.5 HellaSWAG
- en: HellaSWAG [8] is a challenging benchmark that tests AI models’ common-sense
    reasoning skills. It improves on its previous version, SWAG, by adding a more
    diverse and complex set of multiple-choice questions that require the models to
    do more than just language processing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSWAG [8] 是一个具有挑战性的基准，用于测试AI模型的常识推理能力。它通过添加更多样化和复杂的多项选择题集来改进其上一个版本SWAG，这些题目要求模型进行更多语言处理以外的操作。
- en: 'HellaSWAG is a task in which each question presents a scenario with four possible
    endings, and the LLM must choose the most fitting ending among the options. For
    instance, the following question is from HellaSWAG’s dataset. The question consists
    of the context given to the LLM and the four options, which are the possible endings
    for that context. Only one of these options makes sense with common-sense reasoning.
    In this example, option C is highlighted:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSWAG是一个任务，其中每个问题都提供了一个带有四个可能结局的场景，LLM必须在选项中选择最合适的结局。例如，以下问题是来自HellaSWAG数据集的。问题由LLM给出的上下文和四个选项组成，这些选项是该上下文的可能结局。只有一个选项与常识推理相符。在这个例子中，选项C被突出显示：
- en: A woman is outside with a bucket and a dog. The dog is running around trying
    to avoid a bath. She…
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个女人在外面拿着桶和狗。狗在四处跑动，试图避免洗澡。她……
- en: A. rinses the bucket off with soap and blow-dries the dog’s head.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: A. 用肥皂冲洗桶并吹干狗的头。
- en: B. uses a hose to keep it from getting soapy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: B. 用水管防止它变得太滑。
- en: C. gets the dog wet, then it runs away again.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: C. 把狗弄湿后，它又跑开了。
- en: D. gets into a bathtub with the dog.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: D. 狗和狗一起进入浴缸。
- en: The model’s selections are compared to the correct answers to measure its performance.
    This testing method evaluates the LLM’s knowledge of language nuances and its
    deeper comprehension of common-sense logic and the complexities of real-world
    situations. Doing well in HellaSWAG means that a model has a nuanced understanding
    and reasoning ability, which is essential for applications that need a sophisticated
    grasp of context and logic.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的选择与正确答案进行比较，以衡量其性能。这种测试方法评估了LLM对语言细微差别和常识逻辑的深入理解，以及对现实世界情况复杂性的理解。在HellaSWAG中表现良好意味着模型具有细微的理解和推理能力，这对于需要复杂语境和逻辑掌握的应用至关重要。
- en: If interested, the HellaSWAG’s dataset is available online via HuggingFace at
    [https://huggingface.co/datasets/Rowan/hellaswag](https://huggingface.co/datasets/Rowan/hellaswag).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感兴趣，HellaSWAG的数据集可以通过HuggingFace在线获取，网址为[https://huggingface.co/datasets/Rowan/hellaswag](https://huggingface.co/datasets/Rowan/hellaswag)。
- en: 12.3.6 Massive Multitask Language Understanding
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.6 大规模多任务语言理解
- en: The Massive Multitask Language Understanding (MMLU) [9] benchmark assesses the
    breadth and depth of an LLM’s knowledge on various topics and domains. MMLU stands
    out by covering hundreds of tasks linked to different knowledge areas, from science
    and literature to history and social sciences.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模多任务语言理解（MMLU）[9]基准评估了LLM在各个主题和领域的知识广度和深度。MMLU因其涵盖数百个与不同知识领域相关的任务而脱颖而出，从科学和文学到历史和社会科学。
- en: MMLU was developed in response to the observation that while many language models
    excelled at NLP tasks, they struggled with natural language understanding (NLU).
    Previous benchmarks such as GLUE and SuperGLUE were quickly mastered by LLMs,
    indicating a need for more rigorous testing. MMLU aimed to fill this gap by testing
    language understanding and problem-solving abilities using knowledge encountered
    during training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU是在观察到尽管许多语言模型在NLP任务上表现出色，但它们在自然语言理解（NLU）方面却存在困难的情况下开发的。之前的基准如GLUE和SuperGLUE很快就被LLMs掌握，这表明需要更严格的测试。MMLU旨在通过使用训练期间遇到的知识来测试语言理解和问题解决能力，以填补这一空白。
- en: The benchmark includes questions from various subjects, including humanities,
    social sciences, hard sciences, and other specialized areas, varying from elementary
    to advanced professional levels. This approach was unique because most NLU benchmarks
    at the time focused on elementary knowledge. MMLU sought to push the boundaries
    by testing specialized knowledge, a new challenge for LLMs2.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准涵盖了包括人文、社会科学、自然科学和其他专业领域在内的各种学科的问题，难度从基础到高级专业水平不等。这种方法的独特之处在于，当时大多数自然语言理解（NLU）基准都集中在基础知识上。MMLU试图通过测试专业知识来突破界限，这对大型语言模型（LLMs2）来说是一个新的挑战。
- en: Like HellaSWAG, MMLU often uses a multiple-choice format where the model must
    identify the right answer from a set of options. The overall accuracy across these
    diverse tasks shows the model’s general performance, comprehensively measuring
    its language understanding and knowledge application across domains. High performance
    on MMLU means that a model has a large amount of information and is skilled at
    using this knowledge to answer questions and problems correctly. This wide range
    of understanding is essential for developing LLMs that can handle the complexities
    of human knowledge and language subtly and informatively.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与HellaSWAG类似，MMLU通常采用多选题格式，其中模型必须从一组选项中识别正确答案。这些不同任务的整体准确率显示了模型的总体性能，全面衡量了其在各个领域的语言理解和知识应用。在MMLU上的高性能意味着模型拥有大量信息，并且擅长使用这些知识正确回答问题和解决问题。这种广泛的理解能力对于开发能够微妙且信息丰富地处理人类知识和语言的LLMs至关重要。
- en: While comprehensive, the MMLU faces several limitations. First, the performance
    of language models on this test can be constrained by the diversity and quality
    of their training data. If certain topics are underrepresented, models may underperform
    in those areas. Additionally, MMLU primarily assesses whether models can generate
    correct answers but does not evaluate how they arrive at these conclusions. It
    is crucial in applications where the reasoning process is as important as the
    outcome.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MMLU非常全面，但它面临着几个限制。首先，语言模型在这项测试上的表现可能受到其训练数据多样性和质量的影响。如果某些主题代表性不足，模型在这些领域可能会表现不佳。此外，MMLU主要评估模型是否能够生成正确答案，但并不评估它们如何得出这些结论。在推理过程与结果同样重要的应用中，这一点至关重要。
- en: Another significant concern is the potential for bias within the test. Because
    MMLU is constructed from various sources, it may inadvertently include biases
    from these materials, affecting the fairness of model assessments, especially
    on sensitive topics. Furthermore, there is a risk that models could be overfitting
    to the specific MMLU format and style, optimizing for test performance rather
    than genuine understanding and applicability in real-world scenarios.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的关注点是测试中可能存在的偏见。由于MMLU由各种来源构建，它可能无意中包含这些材料中的偏见，影响模型评估的公平性，尤其是在敏感话题上。此外，存在模型可能过度拟合特定MMLU格式和风格的风险，优化测试性能而不是真正的理解和实际应用的风险。
- en: Moreover, the logistical demands of running such a comprehensive test are substantial,
    requiring significant computational resources that might not be available to all
    researchers. This limitation can restrict the range of insights gleaned from the
    test. Finally, the scalability of knowledge poses a challenge; as fields evolve,
    the test must be updated regularly to stay relevant, necessitating ongoing resource
    investment. These factors highlight the complexities of using MMLU as a benchmark
    and underscore the need for continuous refinement to maintain its efficacy and
    relevance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，运行如此全面的测试在后勤方面需求巨大，需要大量的计算资源，这些资源可能不是所有研究人员都能获得的。这种限制可能会限制从测试中获得见解的范围。最后，知识的可扩展性提出了挑战；随着领域的演变，测试必须定期更新以保持相关性，这需要持续的资源投入。这些因素突出了使用MMLU作为基准的复杂性，并强调了持续改进以保持其效力和相关性的必要性。
- en: 12.3.7 Using Azure AI Studio for evaluations
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.7 使用Azure AI Studio进行评估
- en: As an Azure customer, you can easily use these evaluations, as they are already
    integrated into Azure AI Studio. It includes tools for recording, seeing, and
    exploring detailed evaluation metrics, with the option to use custom evaluation
    flows and batch runs without evaluation. With AI Studio, we can make an evaluation
    run from a test dataset or flow with ready-made evaluation metrics. For more adaptability,
    we can make our evaluation flow.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Azure客户，您可以使用这些评估，因为它们已经集成到Azure AI Studio中。它包括记录、查看和探索详细评估指标的工具，您可以选择使用自定义评估流程和批量运行而不进行评估。使用AI
    Studio，我们可以从测试数据集或流程中运行带有现成评估指标的评估。为了提高适应性，我们还可以创建自己的评估流程。
- en: 'Before you can use AI-assisted metrics to evaluate your model, make sure you
    have these things prepared:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以使用AI辅助指标评估您的模型之前，请确保您已准备好以下事项：
- en: A test dataset in either CSV or JSONL format. If you don’t have a dataset ready,
    you can also enter data by hand from the UI.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV或JSONL格式的测试数据集。如果您没有准备好的数据集，您也可以通过UI手动输入数据。
- en: 'One of these models deployed: GPT-3.5 models, GPT-4 models, or Davinci models.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的这些模型之一：GPT-3.5模型、GPT-4模型或Davinci模型。
- en: A compute instance runtime to run the evaluation.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于运行评估的计算实例运行时。
- en: Figure 12.1 shows an example of how to set up the various tests. More details
    can be found at [https://mng.bz/4pMj](https://mng.bz/4pMj).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1展示了如何设置各种测试的示例。更多详细信息可在[https://mng.bz/4pMj](https://mng.bz/4pMj)找到。
- en: If you are not using Azure, other similar options exist, such as DeepEval (see
    the next section). This open source LLM evaluation framework allows running multiple
    LLM metrics and makes this process quite easy.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不使用Azure，其他类似选项也存在，例如DeepEval（见下一节）。这个开源LLM评估框架允许运行多个LLM指标，并使这个过程变得相当简单。
- en: '![figure](../Images/CH12_F01_Bahree.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F01_Bahree.png)'
- en: Figure 12.1 Azure AI Studio evaluations
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 Azure AI Studio评估
- en: '12.3.8 DeepEval: An LLM evaluation framework'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.8 DeepEval：一个LLM评估框架
- en: DeepEval is a free LLM evaluation framework that works like `pytest` (a popular
    testing framework for Python) but focuses on unit-testing LLM outputs. DeepEval
    uses the newest research to assess LLM outputs based on metrics such as hallucination,
    answer relevancy, and RAGAS (Retrieval Augmented Generation Assessment). These
    metrics rely on LLMs and other NLP models that run on your computer locally for
    evaluation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: DeepEval是一个免费的LLM评估框架，它的工作方式类似于`pytest`（Python的一个流行测试框架），但专注于单元测试LLM输出。DeepEval使用最新的研究，根据诸如幻觉、答案相关性以及RAGAS（检索增强生成评估）等指标来评估LLM输出。这些指标依赖于LLM和运行在您计算机本地的其他NLP模型进行评估。
- en: DeepEval supports many useful features for enterprise applications. It can evaluate
    whole datasets simultaneously, create custom metrics, compare any LLM to popular
    benchmarks, and evaluate in real-time in production. In addition, it works with
    tools such as LlamaIndex and Hugging Face and is automatically connected to Confident
    AI for ongoing evaluation of your LLM application throughout its lifetime.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: DeepEval支持许多适用于企业应用的有用功能。它可以同时评估整个数据集，创建自定义指标，比较任何LLM与流行基准，并在生产中实时评估。此外，它与LlamaIndex和Hugging
    Face等工具兼容，并自动连接到Confident AI，以在整个LLM应用程序的生命周期内进行持续评估。
- en: 'The framework also offers a platform for recording test outcomes, measuring
    metrics’ passes/fails, selecting and comparing the best hyperparameters, organizing
    evaluation test cases/datasets, and monitoring live LLM responses in production.
    This book does not cover DeepEval in detail; more details are available at their
    GitHub repository: [https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval).
    Figure 12.2 shows a simple example of a test metric.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架还提供了一个平台用于记录测试结果，测量指标通过/失败情况，选择和比较最佳超参数，组织评估测试用例/数据集，以及监控生产中的实时LLM响应。本书不详细涵盖DeepEval；更多详细信息可在他们的GitHub仓库中找到：[https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval)。图12.2展示了测试指标的一个简单示例。
- en: '![figure](../Images/CH12_F02_Bahree.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH12_F02_Bahree.png)'
- en: Figure 12.2 DeepEval test session example
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2 DeepEval测试会话示例
- en: 12.4 New evaluation benchmarks
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 新评估基准
- en: Over the last 12 to 18 months, we have seen that AI models have reached performance
    saturation on established industry benchmarks such as ImageNet, SQuAD, and SuperGLUE,
    to name a few. This has spurred the industry to develop more challenging benchmarks.
    Some of the newer ones are SWE-bench for coding, MMMU for general reasoning, MoCa
    for moral reasoning, and HaluEval for hallucinations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去12到18个月中，我们注意到AI模型在ImageNet、SQuAD和SuperGLUE等既定行业基准上的性能已经达到饱和，仅举几个例子。这促使行业开发更具挑战性的基准。其中一些较新的基准包括用于编码的SWE-bench、用于通用推理的MMMU、用于道德推理的MoCa和用于幻觉的HaluEval。
- en: 12.4.1 SWE-bench
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 SWE-bench
- en: To measure the progress of GenAI systems that can code, we need more difficult
    tasks to evaluate them. SWE-bench [10] is a dataset containing nearly hundreds
    of software engineering problems from real-world GitHub and Python repositories.
    It poses a harder challenge for AI coding skills, requiring that systems make
    changes across multiple functions, deal with different execution environments,
    and perform complex reasoning.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量能够编码的GenAI系统的进展，我们需要更困难的任务来评估它们。SWE-bench [10] 是一个包含近数百个来自现实世界GitHub和Python仓库的软件工程问题的数据集。它对AI编码技能提出了更严峻的挑战，要求系统在多个功能之间进行更改，处理不同的执行环境，并执行复杂的推理。
- en: The SWE-bench dataset evaluates systems’ abilities to solve GitHub problems
    automatically. It collects 2,294 problem-pull request pairs from 12 popular Python
    repositories. The evaluation is performed by verifying the proposed solutions
    using unit tests and comparing them to the post-PR behavior as the reference solution.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SWE-bench数据集评估系统自动解决GitHub问题的能力。它收集了来自12个流行Python仓库的2,294个问题-问题请求对。评估是通过验证提出的解决方案使用单元测试，并将它们与PR后的行为作为参考解决方案进行比较来进行的。
- en: The primary evaluation metric for SWE-bench is the *percentage of resolved task
    instances.* In other words, it measures how effectively a model can address the
    given problems—the higher the percentage of resolved instances, the better the
    model’s performance. More details on SWE-bench can be found at [https://www.swebench.com](https://www.swebench.com).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: SWE-bench的主要评估指标是*解决任务实例的百分比*。换句话说，它衡量模型解决给定问题的有效性——解决实例的百分比越高，模型的性能越好。有关SWE-bench的更多详细信息，请参阅[https://www.swebench.com](https://www.swebench.com)。
- en: 12.4.2 MMMU
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 MMMU
- en: 'MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark)
    [9] is a new benchmark designed to evaluate multimodal models’ capabilities on
    tasks requiring college-level subject knowledge and expert-level reasoning across
    multiple disciplines. It includes 11.5K multimodal questions from college exams,
    quizzes, and textbooks, covering six core disciplines: art and design, business,
    science, health and medicine, humanities and social science, and tech and engineering.
    These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous
    image types, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: MMMU（大规模多学科多模态理解和推理基准）[9] 是一个新基准，旨在评估多模态模型在需要大学水平学科知识和多学科专家级推理的任务上的能力。它包括来自大学考试、测验和教科书的11.5K多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文社会科学和科技与工程。这些问题涵盖了30个主题和183个子领域，包括30种高度异质化的图像类型，如图表、图表、地图、表格、乐谱和化学结构。
- en: 'MMMU is unique because it focuses on advanced perception and reasoning with
    domain-specific knowledge and challenging models to perform tasks that experts
    face. The benchmark has been used to evaluate several open source LLMs and proprietary
    models such as GPT-4V, highlighting the substantial challenges MMMU poses. Even
    the advanced models only achieve accuracies between 56% and 59%, indicating significant
    room for improvement. It operates by assessing LLMs’ ability to perceive, understand,
    and reason across different disciplines and subfields using various image types.
    The benchmark focuses on three essential skills in LLMs: perception, knowledge,
    and reasoning.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: MMMU的独特之处在于它专注于使用特定领域的知识和具有挑战性的模型进行高级感知和推理，以执行专家面临的任务。该基准已被用于评估多个开源LLM和专有模型，如GPT-4V，突显了MMMU带来的重大挑战。即使是高级模型也仅实现了56%至59%的准确率，表明仍有很大的改进空间。它通过评估LLM在不同学科和子领域使用各种图像类型进行感知、理解和推理的能力来运作。该基准关注LLM中的三个基本技能：感知、知识和推理。
- en: Note that it might seem that the MMLU discussed earlier is the same as MMMU;
    however, they are different. MMLU evaluates language models on a wide range of
    text-based tasks across various domains, focusing solely on language understanding.
    In contrast, MMMU assesses multimodal models, requiring both visual and textual
    comprehension across specialized disciplines, thus challenging models with complex,
    domain-specific multimodal content.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，之前讨论的MMLU可能看起来与MMMU相同；然而，它们是不同的。MMLU评估语言模型在各个领域广泛基于文本的任务上的表现，专注于语言理解。相比之下，MMMU评估多模态模型，要求模型在多个专业学科中具备视觉和文本理解能力，从而挑战模型处理复杂、特定领域的多模态内容。
- en: The MMMU benchmark presents several key challenges for multimodal models, which
    include
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: MMMU基准为多模态模型提出了几个关键挑战，包括
- en: '*Comprehensiveness*—Since the benchmark includes a wide array of 11.5K college-level
    problems across broad disciplines, the models must have a broad knowledge base
    and understanding across multiple fields.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全面性*——由于基准包括广泛学科中的11.5K个大学水平问题，模型必须具备广泛的知识基础和理解多个领域的知识。'
- en: '*Highly heterogeneous image types*—The questions involve 30 different types
    of images, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures. This means the models must be able to interpret and understand various
    visual information.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高度异质化的图像类型*——问题涉及30种不同的图像类型，如图表、图表、地图、表格、乐谱和化学结构。这意味着模型必须能够解释和理解各种视觉信息。'
- en: '*Interleaved text and images*—Many questions feature a mix of text and images,
    requiring models to process and integrate information from both modalities to
    arrive at the correct answer.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交织的文本和图像*——许多问题都包含文本和图像的混合，要求模型处理和整合来自两种模态的信息，以得出正确答案。'
- en: '*Expert-level perception and reasoning*—The tasks demand deep subject knowledge
    and expert-level reasoning, akin to the challenges faced by human experts in their
    respective fields.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*专家级感知和推理*——这些任务需要深厚的专业知识以及专家级的推理，类似于人类专家在其各自领域所面临的挑战。'
- en: These challenges aim to stretch the limits of existing multimodal models, testing
    their capacity to do sophisticated perception, analytical thinking, and domain-specific
    reasoning. The questions demand a profound understanding of the topic and the
    ability to use knowledge in intricate scenarios, even for human experts.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战旨在拉伸现有多模态模型的极限，测试它们进行复杂感知、分析思维和特定领域推理的能力。这些问题要求对主题有深刻的理解，以及在复杂场景中使用知识的能力，即使对于人类专家也是如此。
- en: 12.4.3 MoCa
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.3 MoCa
- en: 'The MoCa (Measuring Human-Language Model Alignment on Causal and Moral Judgment
    Tasks) [11] framework evaluates how well LLMs align with human participants in
    making causal and moral judgments about text-based scenarios. AI models can perform
    well in language and vision tasks, but their ability to make moral decisions,
    especially those that match human opinions, is unclear. To investigate this topic,
    a group of Stanford researchers created a new dataset (MoCa) of human stories
    with moral aspects. Here, we will look at the details of each:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: MoCa（在因果和道德判断任务上衡量人类-语言模型对齐的框架）[11]评估了LLMs在做出关于基于文本场景的因果和道德判断时与人类参与者对齐的程度。AI模型在语言和视觉任务上可以表现良好，但它们做出道德决策的能力，尤其是与人类观点相匹配的决策能力，尚不清楚。为了研究这个主题，斯坦福大学的研究人员创建了一个包含道德方面的人类故事数据集（MoCa）。在这里，我们将查看每个细节：
- en: '*Causal judgments*—Humans intuitively understand events, people, and the world
    around them by organizing their understanding into intuitive theories. These theories
    help us reason about how objects and agents interact with one another, including
    concepts related to causality. The MoCa framework collects a dataset of stories
    from cognitive science papers and annotates each story with the factors they investigate.
    It then tests whether LLMs make causal judgments about text scenarios that align
    with those of humans. On an aggregate level, alignment has improved with more
    recent LLMs. However, statistical analyses reveal that LLMs weigh various factors
    in a different way than human participants.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*因果判断*——人类通过将他们的理解组织成直观的理论，本能地理解事件、人物以及他们周围的世界。这些理论帮助我们推理物体和代理之间如何相互作用，包括与因果关系相关的概念。MoCa框架收集了认知科学论文中的故事数据集，并为每个故事标注了他们所研究的因素。然后，它测试LLMs是否对文本场景做出与人类一致的因果判断。在总体层面上，随着更近期的LLMs的出现，一致性有所提高。然而，统计分析显示，LLMs在权衡各种因素的方式上与人类参与者不同。'
- en: '*Moral judgments*—Tasks evaluate agents in narrative-like text for moral reasoning.
    These tasks and datasets vary in structure, ranging from free-form anecdotes to
    more structured inputs. The MoCa framework assesses how well LLMs align with human
    moral intuitions in these scenarios.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*道德判断*—这些任务评估代理在类似叙事的文本中进行道德推理。这些任务和数据集在结构上有所不同，从自由形式的轶事到更结构化的输入。MoCa 框架评估LLM在这些场景中与人类道德直觉的契合程度。'
- en: The main metric used in MoCa is the Area under the Receiver Operating Characteristic
    (AuROC) curve, which measures the alignment between LLMs and human judgments.
    Furthermore, accuracy serves as a secondary metric for comparison between models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: MoCa 使用的最主要指标是接收者操作特征（ROC）曲线下的面积（AuROC），它衡量LLM与人类判断之间的契合度。此外，准确率作为比较模型之间的次要指标。
- en: A higher score indicates closer alignment with human moral judgment. The study
    yielded intriguing results. No model perfectly matches human moral systems. However,
    newer, larger models such as GPT-4 and Claude show greater alignment with human
    moral sentiments than smaller models such as GPT-3, suggesting that as AI models
    scale, they are gradually becoming more morally aligned with humans.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 分数越高，与人类道德判断的契合度越接近。这项研究产生了有趣的结果。没有模型能够完美匹配人类的道德体系。然而，像GPT-4和Claude这样的新、大型模型与像GPT-3这样的小型模型相比，在人类道德情感上的契合度更高，这表明随着AI模型规模的扩大，它们逐渐与人类在道德上更加契合。
- en: In summary, MoCa provides insights into how LLMs handle causal and moral reasoning,
    shedding light on their implicit tendencies and alignment (or lack thereof) with
    human intuitions. We can get more details on MoCa at [https://moca-llm.github.io](https://moca-llm.github.io).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，MoCa 提供了LLM如何处理因果和道德推理的见解，揭示了它们的隐含倾向和与人类直觉的契合（或缺乏契合）。我们可以在[https://moca-llm.github.io](https://moca-llm.github.io)上了解更多关于MoCa的详细信息。
- en: 12.4.4 HaluEval
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.4 HaluEval
- en: HaluEval [12] benchmark is a large-scale evaluation framework designed to assess
    LLMs’ performance in recognizing hallucinations. In this context, hallucinations
    refer to LLM-generated content that conflicts with the source or cannot be verified
    by factual knowledge. The benchmark includes a collection of generated and human-annotated
    hallucinated samples.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: HaluEval [12] 基准是一个大规模评估框架，旨在评估大型语言模型（LLM）在识别幻觉方面的性能。在此背景下，幻觉指的是与来源冲突或无法通过事实知识验证的LLM生成内容。该基准包括一组生成和人工标注的幻觉样本。
- en: A two-step framework involving sampling-then-filtering is used to create these
    samples, often based on responses from models such as ChatGPT. Human labelers
    also contribute by annotating hallucinations in the responses. The empirical results
    from HaluEval suggest that LLMs, including ChatGPT, can generate hallucinated
    content, particularly on specific topics, by fabricating unverifiable information.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用采样-过滤的两步框架来创建这些样本，通常基于ChatGPT等模型的响应。人类标注员还通过在响应中标注幻觉来做出贡献。HaluEval的实证结果表明，包括ChatGPT在内的LLM可以通过制造不可验证的信息在特定主题上生成幻觉内容。
- en: The study also explores how good current LLMs are at finding hallucinations.
    It can lead LLMs to spot hallucinations in tasks such as question-answering knowledge-grounded
    dialogue and text summarization. The results show that many LLMs have difficulties
    with these tasks, emphasizing that hallucination is a serious, persistent problem.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究还探讨了当前LLM在寻找幻觉方面的能力。它可以使LLM在问答、基于知识的对话和文本摘要等任务中识别出幻觉。结果显示，许多LLM在这些任务上存在困难，强调了幻觉是一个严重且持续存在的问题。
- en: 'The HaluEval benchmark includes 5,000 general user queries with ChatGPT responses
    and 30,000 task-specific examples from three tasks: question answering, knowledge-grounded
    dialogue, and text summarization. It''s a significant step toward understanding
    and improving the reliability of LLMs in generating accurate and verifiable content.
    HaluEval’s GitHub repository at [https://github.com/RUCAIBox/HaluEval](https://github.com/RUCAIBox/HaluEval)
    provides more details.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: HaluEval 基准包括5,000个来自ChatGPT的通用用户查询和30,000个来自三个任务（问答、基于知识的对话和文本摘要）的特定示例。这是理解并提高LLM生成准确和可验证内容可靠性的重要一步。HaluEval的GitHub仓库[https://github.com/RUCAIBox/HaluEval](https://github.com/RUCAIBox/HaluEval)提供了更多详细信息。
- en: 12.5 Human evaluation
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 人工评估
- en: Human evaluation plays a crucial role in understanding the quality of LLMs,
    as it captures nuances, context, and potential biases that automated metrics might
    overlook. For enterprises to conduct effective human evaluations, they need to
    start by defining clear criteria to guide the assessment of LLM outputs. These
    criteria should cover aspects such as accuracy, fluency, relevance, and the presence
    of any biases. To ensure consistency and objectivity, enterprises should develop
    comprehensive guidelines and rubrics for evaluators to follow.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估在理解LLMs的质量方面发挥着关键作用，因为它捕捉到了自动化指标可能忽略的细微差别、上下文和潜在的偏见。为了企业能够进行有效的人工评估，他们需要首先定义明确的准则来指导LLMs输出的评估。这些准则应涵盖准确性、流畅性、相关性以及是否存在任何偏见等方面。为确保一致性和客观性，企业应为评估者制定全面的指南和评分标准。
- en: When choosing the right evaluation methods, enterprises have several options.
    They can either engage domain experts or trained annotators for detailed assessments
    or opt for crowdsourcing platforms such as Amazon Mechanical Turk ([https://www.mturk.com](https://www.mturk.com))
    to access a wider pool of evaluators. The next step involves data collection and
    annotation, which requires user-friendly interfaces and clear instructions to
    ensure quality and consistency. It’s also important to collect enough data to
    yield statistically significant results.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择合适的评估方法时，企业有多种选择。他们可以聘请领域专家或经过培训的标注者进行详细评估，或者选择众包平台，如 Amazon Mechanical Turk
    ([https://www.mturk.com](https://www.mturk.com))，以访问更广泛的评估者群体。下一步涉及数据收集和标注，这需要用户友好的界面和清晰的指示以确保质量和一致性。收集足够的数据以产生具有统计意义的成果也同样重要。
- en: After collecting data, a thorough analysis is necessary. Enterprises should
    employ statistical methods to measure interrater agreement and confirm the reliability
    of the evaluations. The insights derived from this process should then be used
    to make iterative improvements to LLMs, including adjustments to the training
    data, model architecture, and prompt engineering. Regular human evaluations are
    essential for monitoring progress and pinpointing areas that need further improvement.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据后，进行彻底的分析是必要的。企业应采用统计方法来衡量评分者间的一致性并确认评估的可靠性。从这个过程中得出的见解应随后用于对LLMs进行迭代改进，包括调整训练数据、模型架构和提示工程。定期进行人工评估对于监控进度和确定需要进一步改进的领域至关重要。
- en: While human evaluation is invaluable, it does come with its challenges. It can
    be expensive and time-consuming, especially when dealing with large datasets.
    There’s also the risk of subjectivity and bias in human judgments. However, these
    problems can be mitigated by providing clear guidelines, adequate training for
    evaluators, and employing proper aggregation methods.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工评估非常有价值，但它也带来了一些挑战。它可能成本高昂且耗时，尤其是在处理大量数据集时。还存在人类判断中主观性和偏见的风险。然而，通过提供明确的指南、为评估者提供充分的培训以及采用适当的聚合方法，这些问题可以得到缓解。
- en: Several tools and platforms are available to help streamline the human evaluation
    process. Crowdsourcing platforms provide access to a diverse workforce, while
    annotation tools offer efficient data-labeling features. Evaluation frameworks
    are also available, including libraries of metrics and scripts designed specifically
    for LLM evaluation and to support human evaluation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具和平台可以帮助简化人工评估流程。众包平台提供对多样化工作力的访问，而标注工具提供高效的数据标注功能。还有可用的评估框架，包括为LLM评估和人工评估专门设计的指标库和脚本。
- en: Some examples of tools that assist with annotations are Label Studio ([https://labelstud.io](https://labelstud.io)),
    which offers both open source and enterprise offerings. Prodigy ([https://prodi.gy](https://prodi.gy))
    is another annotation tool that supports text, images, videos, and audio. Text-only
    annotation tools also exist, such as Labellerr ([https://www.labellerr.com](https://www.labellerr.com)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一些辅助标注的工具示例包括Label Studio ([https://labelstud.io](https://labelstud.io))，它提供开源和商业版本。Prodigy
    ([https://prodi.gy](https://prodi.gy)) 是另一个支持文本、图像、视频和音频的标注工具。也存在仅支持文本的标注工具，例如
    Labellerr ([https://www.labellerr.com](https://www.labellerr.com))。
- en: Some companies specialize in LLM evaluation, offering robust testing frameworks
    and various resources to assist the evaluation process. For enterprises that are
    not comfortable implementing their evaluation frameworks using tools such as PromptFlow,
    which we saw earlier in the previous chapter, Weights and Biases ([https://wandb.ai](https://wandb.ai)),
    and so forth, there are new companies that specialize in LLM evaluations, such
    as Giskard ([https://www.giskard.ai](https://www.giskard.ai)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一些公司专注于LLM评估，提供强大的测试框架和各种资源以协助评估过程。对于不习惯使用PromptFlow等工具实施自己的评估框架的企业，如我们在上一章中看到的，以及Weights
    and Biases ([https://wandb.ai](https://wandb.ai))等，现在有新的公司专注于LLM评估，例如Giskard ([https://www.giskard.ai](https://www.giskard.ai))。
- en: By following these steps and utilizing the available resources, enterprises
    can implement a structured and effective human evaluation process for LLMs. It’s
    important to remember that this is an evolving field, and staying up to date on
    the latest developments and training is crucial for maintaining the quality of
    evaluations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤并利用现有资源，企业可以为LLM实施结构化和有效的评估流程。重要的是要记住，这是一个不断发展的领域，跟上最新发展和培训对于保持评估质量至关重要。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Benchmarking systems are essential for verifying the performance of GenAI and
    LLMs, directing enhancements, and confirming real-world suitability. They assist
    us in evaluating the efficiency and preparedness of generative AI and LLMs for
    deployment in production environments.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试系统对于验证GenAI和LLM的性能、指导改进和确认实际适用性至关重要。它们帮助我们评估生成AI和LLM在部署到生产环境中的效率和准备情况。
- en: The correlation between evaluations and LLMs is a new and emerging area. We
    should use traditional metrics, LLM task-specific benchmarks, and human evaluation
    to assess LLM performance and ensure its suitability for real-world applications.
    G-Eval is a reference-free evaluation method using LLMs to assess the generated
    text’s coherence, consistency, and relevance.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估与LLM之间的相关性是一个新兴的领域。我们应该使用传统指标、LLM特定任务的基准和人工评估来评估LLM的性能，并确保其适用于实际应用。G-Eval是一种无参考评估方法，使用LLM来评估生成文本的连贯性、一致性和相关性。
- en: Conventional metrics such as BLEU, ROUGE, and BERTScore help measure text generation
    quality and evaluate text numerically based on n-gram matching or semantic similarity.
    They do face some challenges in fully representing contextual meaning and paraphrasing.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的指标如BLEU、ROUGE和BERTScore有助于衡量文本生成质量，并基于n-gram匹配或语义相似性对文本进行数值评估。它们在完全代表上下文意义和释义方面面临一些挑战。
- en: LLM-specific benchmarks measure how well LLMs perform tasks such as text classification,
    sentiment analysis, and question answering. They introduce new metrics such as
    groundedness, coherence, fluency, and GPT similarity that help assess the quality
    of LLM outputs and how close they are to human-like standards.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM特定基准衡量LLM在文本分类、情感分析和问答等任务上的表现。它们引入了新的指标，如扎根性、连贯性、流畅性和GPT相似度，有助于评估LLM输出的质量以及它们与人类标准接近的程度。
- en: Effective evaluation methods for meaningful LLM evaluations include testing
    in relevant settings, creating fair prompts, conducting ethical reviews, and assessing
    the user experience. These include advanced benchmarks such as HELM, HEIM, HellaSWAG,
    and MMLU, which test LLMs against various scenarios and capabilities.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的LLM评估方法包括在相关环境中进行测试、创建公平的提示、进行伦理审查以及评估用户体验。这些包括如HELMA、HEIM、HellaSWAG和MMLU等高级基准，它们测试LLM在各种场景和功能上的表现。
- en: Tools such as Azure AI Studio and the DeepEval framework enable effective LLM
    evaluations in an enterprise context. These tools allow the development of customized
    evaluation workflows, batch executions, and the incorporation of real-time evaluations
    into production settings.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具如Azure AI Studio和DeepEval框架使企业环境中的LLM评估变得有效。这些工具允许开发定制的评估工作流程、批量执行并将实时评估纳入生产环境。
