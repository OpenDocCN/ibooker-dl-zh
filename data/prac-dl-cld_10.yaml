- en: Chapter 10\. AI in the Browser with TensorFlow.js and ml5.js
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章. 在浏览器中使用 TensorFlow.js 和 ml5.js 的人工智能
- en: 'Written in collaboration with guest author: Zaid Alyafeai'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与客座作者 Zaid Alyafeai 合作撰写
- en: You’re a developer who dreams big. You have a kickass AI model that you would
    like a lot of people to try. How many is a lot? Ten thousand? A million? No, silly.
    You like to dream big. How about 100 million people? That’s a nice round number.
    Now convincing 100 million people to download and install an app and make space
    for it on their phones is not an easy sell. But what if we told you that they
    all have an app already installed, just for you. No downloads. No installs. No
    app stores. What kind of black magic is this!? Of course, it’s the web browser.
    And as a bonus, it also runs on your PC.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个有远大梦想的开发者。你有一个了不起的 AI 模型，你希望很多人尝试。多少人算是很多？一万人？一百万人？不，傻瓜。你喜欢远大梦想。一亿人怎么样？这是一个很好的圆数。现在说服一亿人下载并安装一个应用程序，并为其在他们的手机上腾出空间并不容易。但如果我们告诉你，他们已经为你安装了一个应用程序。无需下载。无需安装。无需应用商店。这是什么黑魔法！当然，这就是
    Web 浏览器。而且作为一个奖励，它也可以在你的 PC 上运行。
- en: This is what Google did with its home page when it decided to launch its first-ever
    AI doodle to their billions of users ([Figure 10-1](part0012.html#the_bach_music_harmonizer_doodle_from_go)).
    And what better theme to pick for it than the music of J.S. Bach. (Bach’s parents
    wanted to call him J.S. Bach, 310 years before JavaScript was even created. They
    had quite the foresight!)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是谷歌在决定向数十亿用户推出其首个 AI 涂鸦时在其主页上所做的事情。而选择的主题比 J.S. Bach 的音乐更好的是什么呢。（Bach 的父母想要叫他
    J.S. Bach，比 JavaScript 诞生还早了 310 年。他们有相当远见！）
- en: To explain briefly, the doodle allowed anyone to write one line (voice) of random
    notes for two measures using mouse clicks. When the user clicked a button labeled
    Harmonize, the input would then be processed against hundreds of musical pieces
    written by Bach that contain between two and four lines (voices) of music. The
    system would figure out which notes would sound best along with the user’s input
    to create a much richer Bach-like sounding musical piece. The entire process ran
    in the browser, so Google would not need to scale up its machine learning prediction
    infrastructure at all.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这个涂鸦允许任何人使用鼠标点击写下两小节的随机音符的一行（声音）。当用户点击一个标有“和谐”的按钮时，输入将被处理，与 Bach 写的数百首包含两到四行（声音）音乐的音乐相比较。系统会找出哪些音符与用户的输入最搭配，以创造出更丰富的
    Bach 风格的音乐作品。整个过程在浏览器中运行，因此谷歌不需要扩展其机器学习预测基础设施。
- en: '![The Bach music harmonizer doodle from Google](../images/00222.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![谷歌的 Bach 音乐和谐器涂鸦](../images/00222.jpeg)'
- en: Figure 10-1\. The Bach music harmonizer [doodle](https://oreil.ly/BYFfg) from
    Google
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1. 谷歌的 Bach 音乐和谐器涂鸦
- en: In addition to the cost savings and the ability to run on any platform, with
    a browser we can provide users with a richer, more interactive experience because
    network latency is not a factor. And of course, because everything can be run
    locally after the model is downloaded, the end user can benefit from the privacy
    of their data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了成本节省和在任何平台上运行的能力外，通过浏览器，我们可以为用户提供更丰富、更交互式的体验，因为网络延迟不是一个因素。当然，因为一旦模型被下载后，一切都可以在本地运行，最终用户可以从他们数据的隐私中受益。
- en: Given that JavaScript is the language of the web browser, it’s useful for us
    to delve into JavaScript-based deep learning libraries that can run our trained
    model within users’ browsers. And that’s exactly what we do in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 JavaScript 是 Web 浏览器的语言，深入研究可以在用户浏览器中运行我们训练好的模型的基于 JavaScript 的深度学习库对我们来说是有用的。这正是我们在本章中要做的事情。
- en: Here, we focus on implementing deep learning models in the browser. First, we
    look at a brief history of different JavaScript-based deep learning frameworks
    before moving on to TensorFlow.js and eventually a higher-level abstraction for
    it called ml5.js. We also examine a few complex browser-based applications such
    as detecting the body pose of a person or converting a hand-drawn doodle to a
    photograph (using GANs). Finally, we talk about some practical considerations
    and showcase some real-world case studies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们专注于在浏览器中实现深度学习模型。首先，我们看一下不同基于 JavaScript 的深度学习框架的简要历史，然后转向 TensorFlow.js，最终是一个称为
    ml5.js 的更高级抽象。我们还会检查一些复杂的基于浏览器的应用程序，比如检测一个人的身体姿势或将手绘涂鸦转换为照片（使用 GANs）。最后，我们会谈论一些实际考虑因素，并展示一些真实案例研究。
- en: 'JavaScript-Based Machine Learning Libraries: A Brief History'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 JavaScript 的机器学习库：简要历史
- en: Since the breakthrough of deep learning in recent years, many attempts have
    been made to make AI accessible to a wider range of people in the form of web-based
    libraries. [Table 10-1](part0012.html#historical_overview_of_different_javascr)
    offers a brief overview of the different libraries in the order in which they
    were first released.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来深度学习的突破，许多尝试都是为了让更多人以 Web 库的形式访问 AI。[表 10-1](part0012.html#historical_overview_of_different_javascr)
    提供了不同库的简要概述，按照它们首次发布的顺序。
- en: Table 10-1\. Historical overview of different JavaScript-based deep learning
    libraries (data captured as of August 2019)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1. 不同基于 JavaScript 的深度学习库的历史概述（截至 2019 年 8 月的数据）
- en: '|  | **Active years** | **★ on GitHub** | **Known for** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  | **活跃年份** | **GitHub 上的★** | **以其闻名** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| brain.js | 2015–present | 9,856 | Neural networks, RNNs, LSTMs, and GRUs
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| brain.js | 2015–至今 | 9,856 | 神经网络，RNNs，LSTMs 和 GRUs |'
- en: '| ConvNetJS | 2014–2016 | 9,735 | Neural networks, CNNs |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ConvNetJS | 2014–2016 | 9,735 | 神经网络，CNNs |'
- en: '| Synaptic | 2014–present | 6,571 | Neural networks, LSTMs |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Synaptic | 2014–至今 | 6,571 | 神经网络，LSTMs |'
- en: '| MXNetJS | 2015–2017 | 420 | Running MXNet models |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| MXNetJS | 2015–2017 | 420 | 运行 MXNet 模型 |'
- en: '| Keras.js | 2016–2017 | 4,562 | Running Keras models |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Keras.js | 2016–2017 | 4,562 | 运行 Keras 模型 |'
- en: '| CaffeJS | 2016–2017 | 115 | Running Caffe models |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| CaffeJS | 2016–2017 | 115 | 运行 Caffe 模型 |'
- en: '| TensorFlow.js (formerly known as deeplearn.js) | 2017–present | 11,282 |
    Running TensorFlow models on GPU |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow.js（以前称为deeplearn.js）| 2017至今 | 11,282 | 在GPU上运行TensorFlow模型 |'
- en: '| ml5.js | 2017–present | 2,818 | Easy to use on top of TF.js. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ml5.js | 2017至今 | 2,818 | 在TF.js之上易于使用。 |'
- en: '| ONNX.js | 2018–present | 853 | Speed, running ONNX models |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ONNX.js | 2018至今 | 853 | 速度，运行ONNX模型 |'
- en: Let’s go through a few of these libraries in more detail and see how they evolved.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一些这些库，并看看它们是如何发展的。
- en: ConvNetJS
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNetJS
- en: '[ConvNetJS](https://oreil.ly/URdv9) is a JavaScript library that was designed
    in 2014 by Andrej Karpathy as part of a course during his Ph.D. at Stanford University.
    It trained CNNs in the browser, an exciting proposition, especially in 2014, considering
    the AI hype was starting to take off, and a developer wouldn’t have had to go
    through an elaborate and painful setup process to get running. ConvNetJS helped
    introduce AI to so many people for the first time with interactive training demonstrations
    in the browser.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNetJS](https://oreil.ly/URdv9)是由Andrej Karpathy于2014年设计的JavaScript库，作为他在斯坦福大学博士期间的课程的一部分。它在浏览器中训练CNN，这是一个令人兴奋的提议，特别是在2014年，考虑到AI热潮开始兴起，开发人员不必经历繁琐和痛苦的设置过程即可运行。ConvNetJS通过在浏览器中进行交互式训练演示，帮助许多人第一次接触AI。'
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In fact, when MIT scientist Lex Fridman taught his popular self-driving course
    in 2017, he challenged students worldwide to train a simulated autonomous car
    using reinforcement learning—in the browser using ConvNetJS—as shown in [Figure 10-2](part0012.html#screenshot_of_deeptrafficcomma_training).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当麻省理工学院的科学家Lex Fridman在2017年教授他的热门自动驾驶课程时，他挑战全球学生使用强化学习在浏览器中使用ConvNetJS训练模拟自动驾驶汽车，如[图10-2](part0012.html#screenshot_of_deeptrafficcomma_training)所示。
- en: '![Screenshot of DeepTraffic, training a car with reinforcement learning using
    ConvNetJS](../images/00290.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![DeepTraffic的截图，使用ConvNetJS进行强化学习训练](../images/00290.jpeg)'
- en: Figure 10-2\. Screenshot of DeepTraffic training a car with reinforcement learning
    using ConvNetJS
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。使用ConvNetJS进行强化学习训练汽车的DeepTraffic截图
- en: Keras.js
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras.js
- en: Keras.js was introduced in 2016 by Leon Chen. It was a Keras port made to work
    in the browser by using JavaScript. Keras.js used WebGL to run computations on
    the GPU. It used shaders (special operations for pixel rendering) to run inferences,
    which made them run much faster than using just the CPU. Additionally, Keras.js
    could run on a Node.js server on a CPU to provide server-based inferences. Keras.js
    implemented a handful of convolutional, dense, pooling, activation, and RNN layers.
    It is no longer under active development.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Keras.js是由Leon Chen于2016年推出的。它是一个在浏览器中使用JavaScript的Keras端口。Keras.js使用WebGL在GPU上运行计算。它使用着色器（用于像素渲染的特殊操作）来运行推断，这使得它们比仅使用CPU运行要快得多。此外，Keras.js可以在CPU上的Node.js服务器上运行以提供基于服务器的推断。Keras.js实现了一些卷积、密集、池化、激活和RNN层。它已不再处于活跃开发阶段。
- en: ONNX.js
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ONNX.js
- en: 'Created by Microsoft in 2018, ONNX.js is a JavaScript library for running ONNX
    models in browsers and on Node.js. ONNX is an open standard for representing machine
    learning models that is a collaboration between Microsoft, Facebook, Amazon, and
    others. ONNX.js is surprisingly fast. In fact, faster than even TensorFlow.js
    (discussed in the next section) in early benchmarks, as shown in [Figure 10-3](part0012.html#benchmarking_data_for_resnet-50-id00001)
    and [Figure 10-4](part0012.html#benchmarking_data_for_resnet-50_on_diffe). This
    could be attributed to the following reasons:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由Microsoft于2018年创建，ONNX.js是一个JavaScript库，用于在浏览器和Node.js中运行ONNX模型。ONNX是一个代表机器学习模型的开放标准，是Microsoft、Facebook、Amazon等公司的合作。ONNX.js速度惊人。事实上，在早期基准测试中，甚至比TensorFlow.js（下一节讨论）更快，如[图10-3](part0012.html#benchmarking_data_for_resnet-50-id00001)和[图10-4](part0012.html#benchmarking_data_for_resnet-50_on_diffe)所示。这可能归因于以下原因：
- en: ONNX.js utilizes WebAssembly (from Mozilla) for execution on the CPU and WebGL
    on the GPU.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ONNX.js利用WebAssembly（来自Mozilla）在CPU上执行，利用WebGL在GPU上执行。
- en: WebAssembly allows it to run C/C++ and Rust programs in the web browser while
    providing near-native performance.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WebAssembly允许在Web浏览器中运行C/C++和Rust程序，同时提供接近本机性能。
- en: WebGL provides GPU-accelerated computations like image processing within the
    browser.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WebGL在浏览器中提供像图像处理这样的GPU加速计算。
- en: Although browsers tend to be single-threaded, ONNX.js uses Web Workers to provide
    a multithreaded environment in the background for parallelizing data operations.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管浏览器往往是单线程的，但ONNX.js使用Web Workers在后台提供多线程环境以并行化数据操作。
- en: '![Benchmarking data for ResNet-50 on different JavaScript machine learning
    libraries on CPU (data source: https://github.com/microsoft/onnxjs)](../images/00297.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![在不同JavaScript机器学习库上对ResNet-50进行基准测试的数据（数据来源：https://github.com/microsoft/onnxjs）](../images/00297.jpeg)'
- en: Figure 10-3\. Benchmarking data for ResNet-50 on different JavaScript machine
    learning libraries on CPU ([data source](https://github.com/microsoft/onnxjs))
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3。在不同JavaScript机器学习库上对ResNet-50进行基准测试的数据（数据来源）
- en: '![Benchmarking data for ResNet-50 on different JavaScript machine learning
    libraries on GPU (data source: https://github.com/microsoft/onnxjs)](../images/00061.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![在不同JavaScript机器学习库上对ResNet-50进行基准测试的数据（数据来源：https://github.com/microsoft/onnxjs）](../images/00061.jpeg)'
- en: Figure 10-4\. Benchmarking data for ResNet-50 on different JavaScript machine
    learning libraries on GPU ([data source](https://github.com/microsoft/onnxjs))
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4。在不同JavaScript机器学习库上对ResNet-50进行基准测试的数据（数据来源）
- en: TensorFlow.js
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow.js
- en: Some libraries offered the ability to train within the browser (e.g., ConvNetJS),
    whereas other libraries offered blazing-fast performance (e.g., the now-defunct
    TensorFire). deeplearn.js from Google was the first library that supported fast
    GPU accelerated operations using WebGL while also providing the ability to define,
    train, and infer within the browser. It offered both an immediate execution model
    (for inference) as well as a delayed execution model for training (like in TensorFlow
    1.x). Originally released in 2017, this project became the core of TensorFlow.js
    (released in 2018). It is considered an integral part of the TensorFlow ecosystem,
    and as a result, it is currently the most actively developed JavaScript deep learning
    library. Considering this fact, we focus on TensorFlow.js in this chapter. To
    make TensorFlow.js even simpler to use, we also look at ml5.js, which is built
    on top of TensorFlow.js and abstracts away its complexities, exposing a simple
    API with ready-to-use models from GANs to PoseNet.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一些库提供了在浏览器内训练的能力（例如ConvNetJS），而其他库提供了极快的性能（例如现已停用的TensorFire）。来自Google的deeplearn.js是第一个支持使用WebGL进行快速GPU加速操作的库，同时还提供了在浏览器内定义、训练和推理的能力。它提供了即时执行模型（用于推理）以及延迟执行模型进行训练（类似于TensorFlow
    1.x）。这个项目最初于2017年发布，成为了TensorFlow.js（2018年发布）的核心。它被认为是TensorFlow生态系统的一个重要组成部分，因此目前是最活跃开发的JavaScript深度学习库。考虑到这一事实，我们在本章中专注于TensorFlow.js。为了使TensorFlow.js更加简单易用，我们还看看ml5.js，它构建在TensorFlow.js之上，抽象出其复杂性，提供了一个简单的API，其中包含从GAN到PoseNet的现成模型。
- en: TensorFlow.js Architecture
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow.js架构
- en: First, let’s take a look at the high-level architecture of TensorFlow.js (see
    [Figure 10-5](part0012.html#a_high-level_overview_of_the_tensorflowd)). TensorFlow.js
    runs directly in the browser on desktop and mobile. It utilizes WebGL for GPU
    acceleration, but also can fall back to the browser’s runtime for execution on
    the CPU.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下TensorFlow.js的高级架构（请参见[图10-5](part0012.html#a_high-level_overview_of_the_tensorflowd)）。TensorFlow.js直接在桌面和移动设备的浏览器中运行。它利用WebGL进行GPU加速，但也可以回退到浏览器的运行时以在CPU上执行。
- en: 'It consists of two APIs: the Operations API and the Layers API. The Operations
    API provides access to lower-level operations such as tensor arithmetic and other
    mathematical operations. The Layers API builds on top of the Operations API to
    provide layers such as convolution, ReLU, and so on.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它由两个API组成：操作API和层API。操作API提供对低级操作（如张量算术和其他数学运算）的访问。层API建立在操作API之上，提供卷积、ReLU等层。
- en: '![A high-level overview of the TensorFlow.js and ml5.js ecosystem](../images/00037.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow.js和ml5.js生态系统的高级概述](../images/00037.jpeg)'
- en: Figure 10-5\. A high-level overview of the TensorFlow.js and ml5.js ecosystem
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。TensorFlow.js和ml5.js生态系统的高级概述
- en: Beyond the browser, TensorFlow.js can also run on a Node.js server. Additionally,
    ml5.js uses TensorFlow.js to provide an even higher-level API along with several
    prebuilt models. Having access to all of these APIs at different levels of abstraction
    allows us to build web apps, not only to do simple inference, but also to train
    models within the browser itself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了浏览器，TensorFlow.js还可以在Node.js服务器上运行。此外，ml5.js使用TensorFlow.js提供了一个更高级别的API以及几个预构建的模型。在不同抽象级别上都可以访问所有这些API，使我们能够构建Web应用程序，不仅可以进行简单的推理，还可以在浏览器内部训练模型。
- en: 'Following are some common questions that come up during the development life
    cycle for browser-based AI:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在基于浏览器的AI开发生命周期中经常出现的一些常见问题：
- en: How do I run pretrained models in the browser? Can I use my webcam feed for
    real-time interactivity?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何在浏览器中运行预训练模型？我可以使用我的网络摄像头实时交互吗？
- en: How can I create models for the browser from my TensorFlow trained models?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何从我的TensorFlow训练模型为浏览器创建模型？
- en: Can I even train a model in the browser?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能在浏览器中训练模型吗？
- en: How do different hardware and browsers affect performance?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的硬件和浏览器如何影响性能？
- en: We answer each of these questions in this chapter, starting with TensorFlow.js
    before moving on to ml5.js. We explore some rich built-in functionality contributed
    by the ml5.js community, which would otherwise take a lot of effort and expertise
    to implement directly on TensorFlow.js. We also look at approaches to benchmarking
    before looking at some motivating examples built by creative developers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中回答了这些问题，首先是TensorFlow.js，然后是ml5.js。我们探索了ml5.js社区贡献的一些丰富的内置功能，否则直接在TensorFlow.js上实现将需要大量的工作和专业知识。我们还看看了一些创意开发者构建的激励示例之前的基准方法。
- en: Now let’s take a look at how to take advantage of pretrained models to make
    inferences within the browser.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何利用预训练模型在浏览器中进行推理。
- en: Running Pretrained Models Using TensorFlow.js
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow.js运行预训练模型
- en: TensorFlow.js offers lots of pretrained models that we can directly run in the
    browser. Some examples include MobileNet, SSD, and PoseNet. In the following example,
    we load a pretrained MobileNet model. The full code is located on the book’s GitHub
    repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-10/mobilenet-example/*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js提供了许多预训练模型，我们可以直接在浏览器中运行。一些示例包括MobileNet、SSD和PoseNet。在下面的示例中，我们加载了一个预训练的MobileNet模型。完整的代码位于本书的GitHub存储库（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）中的*code/chapter-10/mobilenet-example/*。
- en: 'First, we import the latest bundle of the library:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入库的最新捆绑包：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then import the MobileNet model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们导入MobileNet模型：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we can make predictions using the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下代码进行预测：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 10-6](part0012.html#class_prediction_output_in_the_browser) shows a
    sample output.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-6](part0012.html#class_prediction_output_in_the_browser)显示了一个示例输出。'
- en: '![Class prediction output in the browser](../images/00285.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![浏览器中的类别预测输出](../images/00285.jpeg)'
- en: Figure 10-6\. Class prediction output in the browser
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6。浏览器中的类别预测输出
- en: 'We can alternatively load a model using a JSON file URL in the following manner:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式使用JSON文件URL加载模型：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The JSON file contains the architecture, the parameter names of the model, and
    the paths to the smaller sharded weight files. The sharding allows the files to
    be small enough to be cached by web browsers, which would make loading faster
    for any subsequent time the model would be needed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件包含模型的架构、参数名称和较小的分片权重文件的路径。分片使文件足够小，可以被Web浏览器缓存，这将使加载对于之后需要模型的任何时间更快。
- en: Model Conversion for the Browser
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于浏览器的模型转换
- en: In the previous section, we examined how to load a pretrained model that is
    already in the JSON format. In this section, we learn how to convert a pretrained
    Keras model (.`h5` format) to JSON that is compatible with TensorFlow.js. To do
    this, we need to install the conversion tool using `pip`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看了如何加载一个已经以JSON格式存在的预训练模型。在本节中，我们将学习如何将一个预训练的Keras模型（`.h5`格式）转换为与TensorFlow.js兼容的JSON格式。为此，我们需要使用`pip`安装转换工具。
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Assuming that our trained Keras model is stored in a folder named *keras_model*,
    we would be able to use the following command to convert it:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们训练的Keras模型存储在名为*keras_model*的文件夹中，我们可以使用以下命令进行转换：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now the `web_model` directory will contain the `.json` and `.shard` files that
    we can easily load using the `tf.loadLayersModel` method:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`web_model`目录将包含我们可以使用`tf.loadLayersModel`方法轻松加载的`.json`和`.shard`文件：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That’s it! Bringing our trained model to the browser is an easy task. For cases
    in which we don’t already have an existing trained model, TensorFlow.js also allows
    us to train models directly in the browser. In the next section, we explore this
    by creating an end-to-end example of training a model using a webcam feed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！将我们训练好的模型带到浏览器中是一项简单的任务。对于我们没有现成训练好的模型的情况，TensorFlow.js也允许我们直接在浏览器中训练模型。在下一节中，我们将通过创建一个使用网络摄像头视频流训练模型的端到端示例来探索这一点。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Loading a model locally requires running a web server. There are tons of options
    that we can use, ranging from the LAMP (Linux, Apache, MySQL, PHP) stack to installing
    `http-server` using npm, to even running Internet Information Services (IIS) on
    Windows to test model loading locally. Even Python 3 can run a simple web server:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地加载模型需要运行一个Web服务器。我们可以使用很多选项，从LAMP（Linux, Apache, MySQL, PHP）堆栈到使用npm安装`http-server`，甚至在Windows上运行Internet
    Information Services（IIS）来本地测试模型加载。甚至Python 3也可以运行一个简单的Web服务器：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Training in the Browser
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在浏览器中训练
- en: The previous example used a pretrained model. Let’s take it up a notch and train
    our own models directly in the browser using input from the webcam. Like in some
    of the previous chapters, we look at a simple example in which we exploit transfer
    learning to make the training process faster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子使用了一个预训练模型。让我们提高一下，直接在浏览器中使用来自网络摄像头的输入训练我们自己的模型。就像在之前的一些章节中一样，我们看一个简单的例子，利用迁移学习来加快训练过程。
- en: Adapted from Google’s Teachable Machine, we use transfer learning to construct
    a simple binary classification model, which will be trained using the webcam feed.
    To build this, we need a feature extractor (converts input images into features
    or embeddings), and then attach a network that converts these features into a
    prediction. Finally, we can train it with webcam inputs. The code is referenced
    in the book’s GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-10/teachable-machine*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从Google的Teachable Machine改编，我们使用迁移学习构建一个简单的二元分类模型，将使用网络摄像头的视频流进行训练。为了构建这个模型，我们需要一个特征提取器（将输入图像转换为特征或嵌入），然后连接一个网络将这些特征转换为预测。最后，我们可以使用网络摄像头的输入进行训练。代码可以在书的GitHub存储库中找到（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))，路径为*code/chapter-10/teachable-machine*。
- en: Note
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Google Creative Lab built a fun interactive website called [Teachable Machine](https://oreil.ly/jkM6W)
    where the user can train three classes on any type of classification problem by
    simply showing those objects in front of the webcam. The three classes are given
    simple labels of green, purple, and orange. During prediction, rather than showing
    bland-looking class probabilities in text on a web page (or even worse, in the
    console), Teachable Machine shows GIFs of cute animals or plays different sounds
    based on the class that is being predicted. As you can imagine, this would be
    a fun and engaging experience for kids in a classroom, and it would serve as a
    wonderful tool to introduce them to AI.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Google Creative Lab建立了一个有趣的互动网站叫做[Teachable Machine](https://oreil.ly/jkM6W)，用户可以通过简单地在网络摄像头前展示这些对象来训练任何类型的分类问题的三个类别。这三个类别被简单地标记为绿色、紫色和橙色。在预测时，Teachable
    Machine不会在网页上以文本形式显示单调的类别概率（甚至更糟的是在控制台中显示），而是显示可爱动物的GIF或根据被预测的类别播放不同的声音。可以想象，这对于课堂上的孩子来说将是一次有趣和引人入胜的体验，也将作为一个很好的工具来介绍他们AI。
- en: '![Training live in the browser with Teachable Machine](../images/00267.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![在Teachable Machine中在浏览器中实时训练](../images/00267.jpeg)'
- en: Figure 10-7\. Training live in the browser with Teachable Machine
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. 在Teachable Machine中在浏览器中实时训练
- en: Feature Extraction
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: As we explored in some of the early chapters in this book, training a large
    model from scratch is a slow process. It’s a lot cheaper and quicker to use a
    pretrained model and use transfer learning to customize the model for our use
    case. We’ll use that model to extract high-level features (embeddings) from the
    input image, and use those features to train our custom model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书的一些早期章节中所探讨的，从头开始训练一个大模型是一个缓慢的过程。使用一个预训练模型并利用迁移学习来定制模型以适应我们的用例是更便宜和更快速的。我们将使用该模型从输入图像中提取高级特征（嵌入），并使用这些特征来训练我们的自定义模型。
- en: 'For extracting the features, we load and use a pretrained MobileNet model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取特征，我们加载并使用一个预训练的MobileNet模型：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s inspect the inputs and outputs of the model. We know that the model was
    trained on ImageNet and the final layer predicts a probability of each one of
    the 1,000 classes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查模型的输入和输出。我们知道该模型是在ImageNet上训练的，最后一层预测了1,000个类别中的每一个的概率：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To extract the features, we select a layer closer to the output. Here we select
    `conv_pw_13_relu` and make it the output of the model; that is, remove the dense
    layers at the end. The model we created is called a feature extraction model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 提取特征时，我们选择一个靠近输出的层。在这里，我们选择`conv_pw_13_relu`并将其作为模型的输出；也就是说，移除末尾的密集层。我们创建的模型称为特征提取模型：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We’ll keep the feature extraction model unmodified during our training process.
    Instead, we add a trainable set of layers on top of it to build our classifier:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将保持特征提取模型不变。相反，我们在其顶部添加一组可训练的层来构建我们的分类器：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Data Collection
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Here, we collect images using the webcam feed and process them for feature
    extraction. The `capture()` function in the Teachable Machine is responsible for
    setting up the `webcamImage` for storing the captured images from the webcam in
    memory. Now, let’s preprocess them to make them applicable to the feature extraction
    model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用网络摄像头捕获图像并进行特征提取处理。Teachable Machine中的`capture()`函数负责设置`webcamImage`以存储从网络摄像头捕获的图像。现在，让我们对它们进行预处理，使其适用于特征提取模型：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After we capture the images, we can add the image and label to the training
    data:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕获图像后，我们可以将图像和标签添加到训练数据中：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Next, we train the model, much as we trained in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b).
    Just like in Keras and TensorFlow, we add an optimizer and define the loss function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型，就像我们在[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中训练的那样。就像在Keras和TensorFlow中一样，我们添加一个优化器并定义损失函数：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'One important thing to keep in mind is that on the GPU, memory allocated by
    TensorFlow.js is not released when a `tf.tensor` object goes out of scope. One
    solution is to call the `dispose()` method on every single object created. However,
    that would make the code more difficult to read, particularly when chaining is
    involved. Consider the following example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一件重要事情是，在GPU上，由TensorFlow.js分配的内存在`tf.tensor`对象超出范围时不会被释放。一个解决方案是在每个创建的对象上调用`dispose()`方法。然而，这会使代码更难阅读，特别是涉及到链式调用时。考虑以下示例：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To cleanly dispose of all memory, we’d need to break it down into the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清理所有内存，我们需要将其分解为以下步骤：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Instead, we could simply use `tf.tidy()` to do the memory management for us,
    while keeping our code clean and readable. Our first line simply needs to be wrapped
    inside the `tf.tidy()` block, as shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以简单地使用`tf.tidy()`来进行内存管理，同时保持我们的代码整洁和可读。我们的第一行代码只需要包裹在`tf.tidy()`块中，如下所示：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With a CPU backend, the objects are automatically garbage collected by the browser.
    Calling `.dispose()` there does not have any effect.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CPU后端时，对象会被浏览器自动垃圾回收。在那里调用`.dispose()`没有任何效果。
- en: 'As a simple use case, let’s train the model to detect emotions. To do so, we
    need to simply add training examples belonging to either one of two classes: happy
    or sad. Using this data, we can start training. [Figure 10-8](part0012.html#predictions_by_our_model_on_a_webcam_fee)
    shows the final result after training the model on 30 images per each class.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的用例，让我们训练模型来检测情绪。为此，我们只需添加属于两个类别之一的训练示例：快乐或悲伤。使用这些数据，我们可以开始训练。在[图10-8](part0012.html#predictions_by_our_model_on_a_webcam_fee)中，展示了在每个类别的30张图像上训练模型后的最终结果。
- en: '![Predictions by our model on a webcam feed within a browser](../images/00245.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![我们模型在浏览器中的网络摄像头视频中的预测](../images/00245.jpeg)'
- en: Figure 10-8\. Predictions by our model on a webcam feed within a browser
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8\. 我们模型在浏览器中的网络摄像头视频中的预测
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Usually, when using a webcam for predictions, the UI tends to freeze. This is
    because the computation happens on the same thread as the UI rendering. Calling
    `await` `tf.nextFrame()` will release the UI thread, which will make the web page
    responsive and prevent the tab/browser from freezing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在使用网络摄像头进行预测时，UI会冻结。这是因为计算发生在与UI渲染相同的线程上。调用`await` `tf.nextFrame()`将释放UI线程，使网页响应并防止标签/浏览器冻结。
- en: GPU Utilization
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU利用率
- en: We can see the CPU/GPU utilization during training and inference using the Chrome
    profiler. In the previous example, we recorded the utilization for 30 seconds
    and observed the GPU usage. In [Figure 10-9](part0012.html#gpu_utilization_shown_in_the_google_chro),
    we see that the GPU is used a quarter of the time.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Chrome分析器查看训练和推断期间的CPU/GPU利用率。在前面的示例中，我们记录了30秒的利用率并观察了GPU的使用情况。在[图10-9](part0012.html#gpu_utilization_shown_in_the_google_chro)中，我们看到GPU使用了四分之一的时间。
- en: '![GPU utilization shown in the Google Chrome profiler view](../images/00129.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![在Google Chrome分析器视图中显示的GPU利用率](../images/00129.jpeg)'
- en: Figure 10-9\. GPU utilization shown in the Google Chrome profiler view
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-9\. 在Google Chrome分析器视图中显示的GPU利用率
- en: So far, we’ve discussed how to do everything from scratch, including loading
    the model, capturing video from the webcam, collecting the training data, training
    the model, and running an inference. Wouldn’t it be great if all these steps could
    be taken care of under the hood and we could just focus on what to do with the
    results of the inference? In the next section, we discuss exactly that with ml5.js.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何从头开始做所有事情，包括加载模型、从网络摄像头捕获视频、收集训练数据、训练模型和运行推断。如果所有这些步骤都可以在幕后处理，我们只需专注于如何处理推断结果，那不是很好吗？在下一节中，我们将使用ml5.js讨论这一点。
- en: ml5.js
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ml5.js
- en: ml5.js is a higher abstraction of TensorFlow.js that makes it easy to use existing
    pretrained deep learning models in a unified way, with a minimal number of lines
    of code. The package comes with a wide range of built-in models, ranging from
    image segmentation to sound classification to text generation, as shown in [Table 10-2](part0012.html#selected_built-in_models_in_ml5dotjscomm).
    Further, ml5.js reduces the steps related to preprocessing, postprocessing, and
    so on, letting us concentrate on building the application that we want with these
    models. For each of these functionalities, ml5js comes with a [demonstration](https://ml5js.org/reference)
    and reference code.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ml5.js 是 TensorFlow.js 的一个更高级抽象，可以轻松地以统一的方式使用现有的预训练深度学习模型，只需很少的代码行数。该软件包配备了各种内置模型，从图像分割到声音分类再到文本生成，如
    [表 10-2](part0012.html#selected_built-in_models_in_ml5dotjscomm) 所示。此外，ml5.js
    简化了与预处理、后处理等相关的步骤，让我们可以专注于使用这些模型构建我们想要的应用程序。对于这些功能，ml5js 都配备了一个 [演示](https://ml5js.org/reference)
    和参考代码。
- en: Table 10-2\. Selected built-in models in ml5.js, showing the range of functionalities
    in text, image, and sound
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-2\. ml5.js 中的选定内置模型，显示文本、图像和声音的功能范围
- en: '| **Functionality** | **Description** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **描述** |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| PoseNet | Detect the location of human joints |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| PoseNet | 检测人体关节的位置 |'
- en: '| U-Net | Object segmentation; e.g., removing object background |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| U-Net | 对象分割；例如，去除对象背景 |'
- en: '| Style Transfer | Transfers style of one image to another |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 风格迁移 | 将一幅图像的风格转移到另一幅图像 |'
- en: '| Pix2Pix | Image-to-image translation; e.g., black-and-white to color |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Pix2Pix | 图像到图像的转换；例如，黑白到彩色 |'
- en: '| Sketch RNN | Creates doodles based on an incomplete sketch |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Sketch RNN | 根据不完整的草图创建涂鸦 |'
- en: '| YOLO | Object detection; e.g., locates faces with bounding boxes |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| YOLO | 目标检测；例如，定位带有边界框的人脸 |'
- en: '| Sound Classifier | Recognizes audio; e.g., whistle, clap, “one,” “stop,”
    etc. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 声音分类器 | 识别音频；例如，口哨、拍手、“一”、“停止”等 |'
- en: '| Pitch Detector | Estimates pitch of sound |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 音高检测器 | 估计声音的音高 |'
- en: '| Char RNN | Generates new text based on training on a large corpus of text
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Char RNN | 基于大量文本训练生成新文本 |'
- en: '| Sentiment Classifier | Detects sentiment of a sentence |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 情感分类器 | 检测句子的情感 |'
- en: '| Word2Vec | Produces word embeddings to identify word relations |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Word2Vec | 生成词嵌入以识别词之间的关系 |'
- en: '| Feature Extractor | Generates features or embeddings from input |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 特征提取器 | 从输入生成特征或嵌入 |'
- en: '| kNN Classifier | Creates a fast classifier using *k*-Nearest Neighbor |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| kNN 分类器 | 使用 *k*-最近邻创建快速分类器 |'
- en: 'Let’s see it in action. First, we import the latest bundle of ml5.js, which
    is similar to TensorFlow.js:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的运行情况。首先，我们导入最新的 ml5.js 捆绑包，这类似于 TensorFlow.js：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that we no longer need to import anything related to TensorFlow.js, because
    it already comes included with ml5.js. We create a simple example where we use
    the same MobileNet scenario as earlier:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不再需要导入与 TensorFlow.js 相关的任何内容，因为它已经包含在 ml5.js 中。我们创建一个简单的示例，其中使用与之前相同的
    MobileNet 场景：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Done! In effectively three lines, a pretrained model is running in our browser.
    Now, let’s open the browser’s console to inspect the output presented in [Figure 10-10](part0012.html#the_top_predicted_classes_with_the_proba).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！在三行代码中，一个预训练模型就在我们的浏览器中运行了。现在，让我们打开浏览器的控制台，检查 [图 10-10](part0012.html#the_top_predicted_classes_with_the_proba)
    中呈现的输出。
- en: '![The top predicted classes with the probability of each class](../images/00151.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![预测类别及其概率](../images/00151.jpeg)'
- en: Figure 10-10\. The top predicted classes with the probability of each class
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 预测类别及其概率
- en: Tip
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are unfamiliar with the browser console, you can simply access it by
    right-clicking anywhere within the browser window and selecting “Inspect element.”
    A separate window opens with the console inside it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉浏览器控制台，可以通过在浏览器窗口中的任何位置右键单击并选择“检查元素”来简单访问它。一个单独的窗口将打开，其中包含控制台。
- en: We can find the full source code for the previous example at *code/chapter-10/ml5js*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 *code/chapter-10/ml5js* 找到前面示例的完整源代码。
- en: Note that ml5.js uses callbacks to manage asynchronous calls of the models.
    A callback is a function that is executed after the accompanying call is finished.
    For instance, in the last code snippet, after the model is loaded, the `modelLoaded`
    function is called, indicating that the model is loaded into memory.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，ml5.js 使用回调来管理模型的异步调用。回调是在相应调用完成后执行的函数。例如，在最后的代码片段中，模型加载完成后会调用 `modelLoaded`
    函数，表示模型已加载到内存中。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: p5.js is a library that works nicely in conjunction with ml5.js and makes it
    super easy to make model predictions in real time using a live video stream. You
    can find a code snippet demonstrating the power of p5.js at *code/chapter-10/p5js-webcam/*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: p5.js 是一个与 ml5.js 配合得很好的库，可以通过实时视频流轻松进行模型预测。您可以在 *code/chapter-10/p5js-webcam/*
    找到一个演示 p5.js 强大功能的代码片段。
- en: ml5.js natively supports p5.js elements and objects. You can use p5.js elements
    for drawing objects, capturing webcam feeds, and more. Then, you can easily use
    such elements as input to the ml5.js callback functions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ml5.js 原生支持 p5.js 元素和对象。您可以使用 p5.js 元素来绘制对象、捕获网络摄像头视频等。然后，您可以轻松地将这些元素作为输入传递给
    ml5.js 回调函数。
- en: PoseNet
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PoseNet
- en: 'So far in this book, we have primarily explored image classification problems.
    In later chapters, we take a look at object detection and segmentation problems.
    These types of problems form a majority of computer-vision literature. In this
    section, however, we choose to take a break from the usual and deal with a different
    kind of problem: keypoint detection. This has significant applications in a variety
    of areas including health care, fitness, security, gaming, augmented reality,
    and robotics. For instance, to encourage a healthy lifestyle through exercise,
    Mexico City installed kiosks that detect the squat pose and offer free subway
    tickets to passengers who can do at least 10 squats. In this section, we explore
    how to run something so powerful in our humble web browser.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们主要探讨了图像分类问题。在后面的章节中，我们将研究目标检测和分割问题。这些问题类型构成了计算机视觉文献的大部分。然而，在本节中，我们选择暂时离开常规，处理一种不同类型的问题：关键点检测。这在包括医疗保健、健身、安全、游戏、增强现实和机器人技术等各种领域都有重要应用。例如，为了通过锻炼鼓励健康生活方式，墨西哥城安装了可以检测深蹲姿势并向至少做10个深蹲的乘客提供免费地铁票的亭子。在本节中，我们将探讨如何在我们谦卑的网络浏览器中运行如此强大的东西。
- en: The PoseNet model offers real-time pose estimation in the browser. A “pose”
    consists of the position of different keypoints (including joints) in the human
    body such as the top of the head, eyes, nose, neck, wrists, elbows, knees, ankles,
    shoulders, and hips. You can use PoseNet for single or multiple poses that might
    exist in the same frame.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PoseNet模型在浏览器中提供实时姿势估计。一个“姿势”包括人体不同关键点（包括关节）的位置，如头顶、眼睛、鼻子、颈部、手腕、肘部、膝盖、脚踝、肩膀和臀部。您可以使用PoseNet来检测同一帧中可能存在的单个或多个姿势。
- en: Let’s build an example using PoseNet, which is readily available in ml5.js,
    to detect and draw keypoints (with the help of p5.js).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PoseNet构建一个示例，ml5.js中已经提供了它，可以用来检测和绘制关键点（借助p5.js的帮助）。
- en: '![Keypoints drawn using PoseNet on a picture of former President Obama in a
    snowball fight](../images/00125.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![使用PoseNet在前总统奥巴马参与雪仗的照片上绘制的关键点](../images/00125.jpeg)'
- en: Figure 10-11\. Keypoints drawn using PoseNet on a picture of former President
    Obama in a snowball fight
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-11\. 使用PoseNet在前总统奥巴马参与雪仗的照片上绘制的关键点
- en: 'You can find the code to detect keypoints for a still image at *code/chapter-10/posenet/single.html*:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在*code/chapter-10/posenet/single.html*找到检测静态图像关键点的代码：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can also run a similar script (at *code/chapter-10/posenet/webcam.html*)
    on our webcam.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在网络摄像头上运行类似的脚本（在*code/chapter-10/posenet/webcam.html*）。
- en: Now let us look at another example that is supported by ml5.js.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看另一个由ml5.js支持的示例。
- en: pix2pix
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pix2pix
- en: “Hasta la vista, baby!”
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: “Hasta la vista, baby!”
- en: 'This is one of the most memorable lines in the history of film. Coincidentally,
    it was uttered by an AI cyborg in the 1991 classic *Terminator 2: Judgment Day*.
    By the way, its translation is “Goodbye, baby!” Language translation technology
    has come a long way since then. It used to be that language translation was built
    on phrase substitution rules. And now, it’s replaced by much better performing
    deep learning systems that understand the context of the sentence to convert it
    into a similar meaning sentence in the target language.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是电影史上最令人难忘的台词之一。巧合的是，这是1991年经典电影《终结者2：审判日》中一个AI机器人说的。顺便说一句，它的翻译是“再见，宝贝！”自那时以来，语言翻译技术已经取得了长足的进步。过去，语言翻译建立在短语替换规则上。现在，它被表现更好的深度学习系统所取代，这些系统能够理解句子的上下文，将其转换为目标语言中具有类似含义的句子。
- en: 'Here’s a thought: if we can translate from sentence one to sentence two, could
    we translate a picture from one setting to another? Could we do the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想：如果我们可以从句子一翻译到句子二，那么我们是否可以将一幅图片从一个环境转换到另一个环境？我们可以做到以下这些吗：
- en: Convert an image from low resolution to higher resolution?
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一幅图像从低分辨率转换为高分辨率？
- en: Convert an image from black and white to color?
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一幅图像从黑白转换为彩色？
- en: Convert an image from daytime to nighttime view?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一幅图像从白天转换为夜晚视图？
- en: Convert a satellite image of the earth into a map view?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将地球的卫星图像转换为地图视图？
- en: Convert an image from a hand-drawn sketch into a photograph?
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一幅手绘草图转换为照片？
- en: Well, image translation is not science fiction anymore. In 2017, [Philip Isola
    et al.](https://oreil.ly/g5R60) developed a way to convert a picture into another
    picture, conveniently naming it pix2pix. By learning from several pairs of before
    and after pictures, the pix2pix model is able to generate highly realistic renderings
    based on the input image. For example, as demonstrated in [Figure 10-12](part0012.html#example_of_input_and_output_pairs_on_pix),
    given a pencil sketch of a bag, it can recreate the photo of a bag. Additional
    applications include image segmentation, synthesizing artistic imagery, and more.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，图像翻译不再是科幻。2017年，[Philip Isola等人](https://oreil.ly/g5R60)开发了一种将一幅图片转换为另一幅图片的方法，方便地命名为pix2pix。通过学习几对之前和之后的图片，pix2pix模型能够基于输入图片生成高度逼真的渲染。例如，如[图10-12](part0012.html#example_of_input_and_output_pairs_on_pix)所示，给定一个包的铅笔素描，它可以重新创建包的照片。其他应用包括图像分割、合成艺术形象等。
- en: '![Example of input and output pairs on pix2pix](../images/00135.jpeg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![pix2pix上输入和输出对的示例](../images/00135.jpeg)'
- en: Figure 10-12\. Example of input and output pairs on pix2pix
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12\. pix2pix上输入和输出对的示例
- en: Note
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Imagine a scenario with a bank teller and a currency counterfeiter. The job
    of the bank teller is to spot fake currency bills, whereas the counterfeiter’s
    goal is to make it as difficult as possible for the bank teller to identify the
    fakes. They are clearly in an adversarial situation. Each time the cop spots the
    fake bills, the counterfeiter learns his mistake, takes it as an opportunity to
    improve (growth mindset after all), and tries to make it even more difficult for
    the bank teller to thwart him next time. This forces the bank teller to get better
    at recognizing fakes over time. This feedback cycle forces both of them to get
    better at what they do. This is the underlying principle driving GANs.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情景，有一个银行出纳员和一个货币伪造者。银行出纳员的工作是发现假币，而伪造者的目标是尽可能地让银行出纳员难以识别假币。他们显然处于对抗性的情况。每当警察发现假钞时，伪造者都会从中学习错误，将其视为改进的机会（毕竟是成长思维），并试图让银行出纳员下次更难阻止他。这迫使银行出纳员随着时间的推移更善于识别假币。这种反馈循环迫使他们两个在自己的工作上变得更好。这是推动GAN的基本原理。
- en: As [Figure 10-13](part0012.html#a_flowchart_for_a_gan) illustrates, GANs consist
    of two networks, a Generator and a Discriminator, which have the same adversarial
    relationship as the counterfeiter and the bank teller. The Generator’s job is
    to generate realistic-looking output, very similar to the training data. The Discriminator’s
    responsibility is to identify whether the data passed to it by the Generator was
    real or fake. The output of the Discriminator is fed back into the Generator to
    begin the next cycle. Each time the Discriminator correctly identifies a generated
    output as a fake, it forces the Generator to get better in the next cycle.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图10-13](part0012.html#a_flowchart_for_a_gan)所示，GAN由两个网络组成，一个生成器和一个鉴别器，它们与伪造者和银行出纳员具有相同的对抗关系。生成器的工作是生成看起来逼真的输出，与训练数据非常相似。鉴别器的责任是识别由生成器传递给它的数据是真实还是伪造的。鉴别器的输出被反馳回生成器以开始下一个周期。每当鉴别器正确地将生成的输出识别为伪造时，它会迫使生成器在下一个周期变得更好。
- en: It is worthwhile to note that GANs typically do not have control over the data
    to be generated. However, there are variants of GANs, such as *conditional GANs*,
    that allow for labels to be part of the input, providing more control over the
    output generation; that is, conditioning the output. pix2pix is an example of
    a conditional GAN.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，GAN通常无法控制要生成的数据。然而，有一些GAN的变体，如*条件GAN*，允许标签成为输入的一部分，从而更好地控制输出生成；也就是说，调节输出。pix2pix是条件GAN的一个例子。
- en: '![A flowchart for a GAN](../images/00024.jpeg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![GAN的流程图](../images/00024.jpeg)'
- en: Figure 10-13\. A flowchart for a GAN
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13。GAN的流程图
- en: We used pix2pix to create a simple sketching app that works in the browser.
    The output images are really interesting to look at. Consider the examples shown
    in [Figure 10-14](part0012.html#sketches_to_image_example) and [Figure 10-15](part0012.html#we_can_create_colored_blueprints_left_pa).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用pix2pix创建了一个在浏览器中运行的简单素描应用程序。输出图像非常有趣。请考虑[图10-14](part0012.html#sketches_to_image_example)和[图10-15](part0012.html#we_can_create_colored_blueprints_left_pa)中显示的示例。
- en: '![Sketches to image example](../images/00004.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![素描到图像示例](../images/00004.jpeg)'
- en: Figure 10-14\. Sketch-to-image example
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-14。素描到图像示例
- en: '![We can create colored blueprints (left) and it will convert them to realistic-looking
    human faces (right)](../images/00251.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![我们可以创建彩色蓝图（左），它将把它们转换为逼真的人脸（右）](../images/00251.jpeg)'
- en: Figure 10-15\. We can create colored blueprints (left) and pix2pix will convert
    them to realistic-looking human faces (right)
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-15。我们可以创建彩色蓝图（左）和pix2pix将其转换为逼真的人脸（右）
- en: Note
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Fun fact: Ian Goodfellow came up with the idea for GANs while at a bar. This
    adds yet another item to the list of inventions, organizations, and companies
    whose ideas originated over drinks, including the creation of the RSA Algorithm,
    Southwest Airlines, and the game of Quidditch.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实：Ian Goodfellow在酒吧时想出了GAN的想法。这为发明、组织和公司的想法起源于饮料的列表中又增加了一项，包括RSA算法、西南航空和魁地奇游戏的创造。
- en: pix2pix works by training on pairs of images. In [Figure 10-16](part0012.html#training_pairs_for_pix2pixcolon_a_bamper),
    the image on the left is the input image or the conditional input. The image on
    the right is the target image, the realistic output that we want to generate (if
    you are reading the print version, you will not see the color image on the right).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix通过训练图像对来工作。在[图10-16](part0012.html#training_pairs_for_pix2pixcolon_a_bamper)中，左侧的图像是输入图像或条件输入。右侧的图像是目标图像，我们想要生成的逼真输出（如果您正在阅读印刷版本，则不会看到右侧的彩色图像）。
- en: '![Training pairs for pix2pix: a B&W image and its original color image](../images/00230.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![pix2pix的训练对：一幅黑白图像及其原始彩色图像](../images/00230.jpeg)'
- en: 'Figure 10-16\. Training pairs for pix2pix: a B&W image and its original color
    image'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。pix2pix的训练对：一幅黑白图像及其原始彩色图像
- en: 'One of the easier ports for training pix2pix is the TensorFlow-based implementation
    by [Christopher Hesse](https://oreil.ly/r-d1l). We can use a very simple script
    to train our own model:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 训练pix2pix的一个更简单的端口是由[Christopher Hesse](https://oreil.ly/r-d1l)基于TensorFlow的实现。我们可以使用一个非常简单的脚本来训练我们自己的模型：
- en: '[PRE21]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After training has finished, we can save the model using the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以使用以下命令保存模型：
- en: '[PRE22]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After that, we can use this simple code to load the saved weights to ml5.js.
    Note the transfer function that is used to retrieve the output in a canvas:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以使用这段简单的代码将保存的权重加载到ml5.js中。请注意用于在画布中检索输出的转移函数：
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can also draw strokes and allow real-time sketching. For instance, [Figure 10-17](part0012.html#pix2pixcolon_edges_to_pikachu_by_yining)
    shows an example that draws Pikachu.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制笔画并允许实时素描。例如，[图10-17](part0012.html#pix2pixcolon_edges_to_pikachu_by_yining)展示了一个绘制皮卡丘的示例。
- en: '![Pix2Pix: Edges to Pikachu by Yining Shi, built on ml5.js](../images/00311.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Pix2Pix：由Yining Shi基于ml5.js制作的Pikachu边缘](../images/00311.jpeg)'
- en: 'Figure 10-17\. [Pix2Pix: Edges to Pikachu](https://oreil.ly/HIaSy) by Yining
    Shi, built on ml5.js'
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17。[Pix2Pix：从边缘到皮卡丘](https://oreil.ly/HIaSy)由Yining Shi制作，基于ml5.js
- en: Benchmarking and Practical Considerations
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试和实际考虑
- en: 'As people who care deeply about how our end users perceive our product, it’s
    important for us to treat them right. Two factors play a large role in how users
    experience our product: the model size, and the inference time based on the hardware.
    Let’s take a closer look at each factor.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 作为非常关心我们的最终用户如何看待我们的产品的人，对待他们是很重要的。两个因素在用户体验我们的产品时起着重要作用：模型大小和基于硬件的推理时间。让我们更仔细地看看每个因素。
- en: Model Size
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小
- en: 'A typical MobileNet model is 16 MB. Loading this on a standard home or office
    network might just take a few seconds. Loading the same model on a mobile network
    would take even longer. The clock is ticking, and the user is becoming impatient.
    And this is before the model even gets a chance to start inference. Waiting for
    big models to load is more detrimental to the UX than their runtime, especially
    where internet speeds are not as fast as a broadband paradise like Singapore.
    There are a few strategies that can help:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的MobileNet模型为16 MB。在标准家庭或办公网络上加载这个可能只需要几秒钟。在移动网络上加载相同的模型会花更长时间。时间在流逝，用户变得不耐烦。而这还是在模型开始推理之前。等待大型模型加载对用户体验的影响比它们的运行时间更为有害，尤其是在互联网速度不如新加坡这样的宽带天堂的地方。有一些策略可以帮助：
- en: Pick the smallest model for the job
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最适合工作的最小模型
- en: Among pretrained networks, EfficientNet, MobileNet, or SqueezeNet tend to be
    the smallest (in order of decreasing accuracy).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练网络中，EfficientNet、MobileNet或SqueezeNet往往是最小的（按准确性递减顺序）。
- en: Quantize the model
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 量化模型
- en: Reduce the model size using the TensorFlow Model Optimization Toolkit before
    exporting to TensorFlow.js.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在导出到TensorFlow.js之前，使用TensorFlow模型优化工具包减小模型大小。
- en: Build our own tiny model architecture
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 构建我们自己的微型模型架构
- en: If the final product does not need heavy ImageNet-level classification, we could
    build our own smaller model. When Google made the J.S. Bach doodle on the Google
    home page, its model was only 400 KB, loading almost instantly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最终产品不需要重量级的ImageNet级别分类，我们可以构建自己的较小模型。当谷歌在谷歌主页上制作J.S. Bach涂鸦时，其模型仅为400 KB，几乎瞬间加载。
- en: Inference Time
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理时间
- en: Considering our model is accessible in a browser running on a PC or a mobile
    phone, we would want to pay careful attention to the UX, especially on the slowest
    hardware. During our benchmarking process, we ran *chapter10/code/benchmark.html*
    within various browsers on different devices. [Figure 10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom)
    presents the results of these experiments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的模型可以在运行在PC或手机上的浏览器中访问，我们希望特别注意用户体验，尤其是在最慢的硬件上。在我们的基准测试过程中，我们在不同设备上的各种浏览器中运行*chapter10/code/benchmark.html*。[图10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom)展示了这些实验的结果。
- en: '![Inference time for MobileNetV1 in Chrome on different devices](../images/00233.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![在不同设备上Chrome上MobileNetV1的推理时间](../images/00233.jpeg)'
- en: Figure 10-18\. Inference time for MobileNetV1 in Chrome on different devices
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-18。在Chrome上不同设备上MobileNetV1的推理时间
- en: '[Figure 10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom) implies
    the faster the hardware, the faster the model inference. Apple appears to be outdoing
    Android in terms of GPU performance. Though, clearly, it is not an “apples-to-apples
    comparison.”'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom)暗示硬件越快，模型推理速度越快。苹果在GPU性能方面似乎胜过安卓。尽管，显然，这不是一个“不折不扣的比较”。'
- en: Out of curiosity, do different browsers run inference at the same speed? Let’s
    find that out on an iPhone X; [Figure 10-19](part0012.html#inference_time_in_different_browsers_on)
    shows the results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，不同浏览器是否以相同的速度运行推理？让我们在iPhone X上找出答案；[图10-19](part0012.html#inference_time_in_different_browsers_on)展示了结果。
- en: '![Inference time in different browsers on iPhone X](../images/00107.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![iPhone X上不同浏览器的推理时间](../images/00107.jpeg)'
- en: Figure 10-19\. Inference time in different browsers on iPhone X
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-19。iPhone X上不同浏览器的推理时间
- en: '[Figure 10-19](part0012.html#inference_time_in_different_browsers_on) shows
    us the same speed in all browsers on an iPhone. This shouldn’t be surprising,
    because all of these browsers use iPhone’s WebKit-based built-in browser control
    called `WKWebView`. How about on a MacBook Pro? Take a look at [Figure 10-20](part0012.html#inference_time_in_different_bro-id00001)
    to see.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-19](part0012.html#inference_time_in_different_browsers_on)向我们展示了iPhone上所有浏览器的相同速度。这并不令人惊讶，因为所有这些浏览器都使用iPhone的基于WebKit的内置浏览器控件称为`WKWebView`。在MacBook
    Pro上呢？看看[图10-20](part0012.html#inference_time_in_different_bro-id00001)。'
- en: '![Inference time in different browsers on an i7 @ 2.6 GHz macOS 10.14 machine](../images/00090.jpeg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![i7 @ 2.6 GHz macOS 10.14机器上不同浏览器的推理时间](../images/00090.jpeg)'
- en: Figure 10-20\. Inference time in different browsers on an i7 @ 2.6 GHz macOS
    10.14 machine
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-20。在i7 @ 2.6 GHz macOS 10.14机器上不同浏览器的推理时间
- en: The results might be surprising. Chrome is almost double the speed of Firefox
    in this example. Why is that? Opening a GPU monitor showed that Chrome had much
    higher GPU utilization compared to Firefox and slightly higher than Safari. The
    higher the utilization, the faster the inference. What this means is that depending
    on the operating system, browsers might have different optimizations to speed
    up the inference on the GPU, leading to different running times.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会令人惊讶。在这个例子中，Chrome的速度几乎是Firefox的两倍。为什么？打开GPU监视器显示，与Firefox相比，Chrome的GPU利用率要高得多，稍高于Safari。利用率越高，推理速度越快。这意味着根据操作系统的不同，浏览器可能具有不同的优化来加速GPU上的推理，从而导致不同的运行时间。
- en: One key point to note is that these tests were performed on top-of-the-line
    devices. They do not necessarily reflect the kind of device an average user might
    have. This also has implications for battery usage, if run for prolonged periods
    of time. Accordingly, we need to set appropriate expectations regarding performance,
    particularly for real-time user experiences.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个关键点是这些测试是在顶级设备上进行的。它们不一定反映普通用户可能拥有的设备类型。这也对电池使用有影响，如果长时间运行。因此，我们需要对性能设置适当的期望，特别是对于实时用户体验。
- en: Case Studies
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Now that we know all the ingredients for deep learning on the browser, let’s
    see what the industry is cooking.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了浏览器上深度学习的所有要素，让我们看看这个行业正在做些什么。
- en: Semi-Conductor
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Semi-Conductor
- en: Have you ever dreamed of conducting the New York Philharmonic Orchestra? With
    [Semi-Conductor](https://oreil.ly/sNOFg), your dream is half-way fulfilled. Open
    the website, stand in front of the webcam, wave your arms, and watch the entire
    orchestra perform Mozart’s Eine Kleine Nachtmusik at your whim! As you might have
    guessed, it’s using PoseNet to track the arm movements and using those movements
    to set the tempo, volume, and the section of instruments ([Figure 10-21](part0012.html#control_an_orchestra_by_waving_your_arms))
    playing the music (including violins, violas, cellos, and double bass). Built
    by the Google Creative Lab in Sydney, Australia, it uses a prerecorded musical
    piece broken up into tiny fragments, and each fragment is played at the scored
    speed and volume depending on the arm movements. Moving hands up increases volume,
    moving faster increases tempo. This interactive experience is possible only because
    PoseNet is able to run inferences at several frames a second (on a regular laptop).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾梦想指挥纽约爱乐乐团？通过[Semi-Conductor](https://oreil.ly/sNOFg)，你的梦想已经实现了一半。打开网站，站在网络摄像头前，挥动手臂，看着整个管弦乐团随你的意愿演奏莫扎特的《小夜曲》！正如你可能猜到的那样，它使用PoseNet来跟踪手臂运动，并利用这些运动来设置速度、音量和演奏音乐的乐器部分（包括小提琴、中提琴、大提琴和低音提琴）（[图10-21](part0012.html#control_an_orchestra_by_waving_your_arms)）。这个由谷歌悉尼创意实验室在澳大利亚悉尼建立的项目，使用了一个预先录制的音乐片段，将其分解成微小片段，每个片段根据手臂运动以得分速度和音量播放。手向上移动增加音量，移动更快增加速度。这种互动体验只有因为PoseNet能够在每秒几帧的速度下进行推理（在普通笔记本电脑上）。
- en: '![Control an orchestra by waving your arms on the Semi-Conductor demonstrator](../images/00010.jpeg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![通过在半导体演示器上挥动手臂来控制管弦乐团](../images/00010.jpeg)'
- en: Figure 10-21\. Control an orchestra by waving your arms on the Semi-Conductor
    demonstrator
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-21。通过在半导体演示器上挥动手臂来控制管弦乐团
- en: TensorSpace
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorSpace
- en: CNNs can often feel…well, convoluted. Often treated as a black box, they can
    be difficult to understand. What do the filters look like? What activates them?
    Why did they make a certain prediction? They are shrouded in mystery. As with
    anything complex, visualizations can help open this black box and make it easier
    to understand. And that’s where [TensorSpace](https://tensorspace.org), the library
    that “presents tensors in space,” comes in.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs经常感觉...嗯，复杂。通常被视为黑匣子，很难理解。滤波器是什么样的？是什么激活了它们？为什么它们做出了某种预测？它们笼罩在神秘之中。与任何复杂的事物一样，可视化可以帮助打开这个黑匣子，使其更容易理解。这就是[TensorSpace](https://tensorspace.org)的作用，这个“在空间中呈现张量的库”。
- en: It allows us to load models in 3D space, explore their structures in the browser,
    zoom and rotate through them, feed inputs, and understand how the image is processed
    and passed layer by layer all the way to the final prediction layer. The filters
    can finally be opened up to manual inspection without the need for any installation.
    And, as [Figure 10-22](part0012.html#lenet_model_visualized_inside_tensorspac)
    teases, if you’re feeling savvy, you could even load this in virtual reality against
    any background!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许我们在3D空间中加载模型，探索它们在浏览器中的结构，通过它们进行缩放和旋转，输入数据，并了解图像是如何逐层处理并传递到最终预测层的。滤波器最终可以被手动检查，而无需任何安装。而且，正如[图10-22](part0012.html#lenet_model_visualized_inside_tensorspac)所暗示的，如果你感觉很有见识，你甚至可以加载到虚拟现实中，并与任何背景进行对比！
- en: '![LeNet model visualized inside TensorSpace](../images/00316.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![在TensorSpace内可视化的LeNet模型](../images/00316.jpeg)'
- en: Figure 10-22\. LeNet model visualized inside TensorSpace
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-22。在TensorSpace内可视化的LeNet模型
- en: Metacar
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Metacar
- en: Self-driving cars are a complex beast. And using reinforcement learning to train
    them can take a lot of time, money, and monkeywrenching (not counting the crashes,
    initially). What if we could train them in the browser itself? [Metacar](https://metacar-project.com)
    solves this by providing a simulated 2D environment to train toy cars with reinforcement
    learning, all in the browser, as depicted in [Figure 10-23](part0012.html#metacar_environment_for_training_with_re).
    Just as with video games you progress to ever-more difficult levels, Metacar allows
    building multiple levels to improve the performance of your car. Utilizing TensorFlow.js,
    this is aimed at making reinforcement learning more accessible (in which we dive
    into greater detail in [Chapter 17](part0020.html#J2B83-13fa565533764549a6f0ab7f11eed62b)
    while building a small-scale autonomous car).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车是一个复杂的怪物。使用强化学习来训练它们可能需要很多时间、金钱和猴子扳手（不包括最初的碰撞）。如果我们能在浏览器中训练它们会怎样？[Metacar](https://metacar-project.com)通过提供一个模拟的2D环境来训练玩具汽车，使用强化学习，全部在浏览器中进行，如[图10-23](part0012.html#metacar_environment_for_training_with_re)所示。就像玩视频游戏一样，你可以逐渐进入更难的关卡，Metacar允许构建多个关卡来提高你的汽车性能。利用TensorFlow.js，这旨在使强化学习更易于访问（我们将在[第17章](part0020.html#J2B83-13fa565533764549a6f0ab7f11eed62b)中更详细地探讨这个问题，同时构建一个小型自动驾驶汽车）。
- en: '![Metacar environment for training with reinforcement learning](../images/00084.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![用强化学习进行训练的Metacar环境](../images/00084.jpeg)'
- en: Figure 10-23\. Metacar environment for training with reinforcement learning
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-23。用强化学习进行训练的Metacar环境
- en: Airbnb’s Photo Classification
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airbnb的照片分类
- en: Airbnb, the online property rental company, requires homeowners and renters
    to upload pictures of themselves for their profiles. Unfortunately, a few people
    try to find the most readily available picture they have—their driver’s license
    or passport. Given the confidential nature of the information, Airbnb uses a neural
    network running on TensorFlow.js to detect sensitive images and prevent their
    upload to the server.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在线房屋租赁公司Airbnb要求房主和租客上传他们的照片以完善个人资料。不幸的是，一些人试图上传他们最容易获得的照片——他们的驾照或护照。考虑到信息的机密性，Airbnb使用在TensorFlow.js上运行的神经网络来检测敏感图像，并阻止它们上传到服务器。
- en: GAN Lab
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN实验室
- en: Similar to [TensorFlow Playground](https://oreil.ly/vTpmu) (an in-browser neural
    network visualization tool), [GAN Lab](https://oreil.ly/aQgga) ([Figure 10-24](part0012.html#screenshot_of_gan_lab))
    is an elegant visualization tool for understanding GANs using TensorFlow.js. Visualizing
    GANs is a difficult process, so to simplify it, GAN Lab attempts to learn simple
    distributions and visualizes the generator and discriminator network output. For
    instance, the real distribution could be points representing a circle in 2D space.
    The generator starts from a random Gaussian distribution and gradually tries to
    generate the original distribution. This project is a collaboration between Georgia
    Tech and Google Brain/PAIR (People + AI Research).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[TensorFlow Playground](https://oreil.ly/vTpmu)（一个基于浏览器的神经网络可视化工具），[GAN实验室](https://oreil.ly/aQgga)（[图10-24](part0012.html#screenshot_of_gan_lab)）是一个优雅的可视化工具，用于理解使用TensorFlow.js的GAN。可视化GAN是一个困难的过程，因此为了简化它，GAN实验室尝试学习简单的分布并可视化生成器和鉴别器网络的输出。例如，真实分布可以是在2D空间中表示一个圆的点。生成器从一个随机的高斯分布开始，并逐渐尝试生成原始分布。这个项目是乔治亚理工学院和谷歌Brain/PAIR（People
    + AI Research）之间的合作。
- en: '![Screenshot of GAN Lab](../images/00105.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![GAN实验室截图](../images/00105.jpeg)'
- en: Figure 10-24\. Screenshot of GAN Lab
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-24\. GAN实验室截图
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We first examined the evolution of JavaScript-based deep learning libraries
    and chose TensorFlow.js as the candidate to focus on. We ran pretrained models
    in real time on the webcam feed, and then even trained models in the browser.
    Tools like Chrome’s profiler gave us insight into GPU usage. Then, to simplify
    development further, we used ml5.js, which enabled us to build demos like PoseNet
    and pix2pix in just a few lines of code. Finally, we benchmarked the performance
    of these models and libraries in the real world, ending with some interesting
    case studies.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先研究了基于JavaScript的深度学习库的发展，并选择了TensorFlow.js作为重点关注的候选。我们在网络摄像头上实时运行预训练模型，甚至在浏览器中训练模型。像Chrome的分析器这样的工具让我们了解GPU的使用情况。然后，为了进一步简化开发，我们使用了ml5.js，这使我们能够仅用几行代码构建像PoseNet和pix2pix这样的演示。最后，我们在现实世界中对这些模型和库的性能进行了基准测试，最终得出了一些有趣的案例研究。
- en: One huge benefit of running neural networks in the browser is the vast reach
    that browsers have compared to any smartphone platform. Add to that the advantage
    of not having to overcome the user’s reluctance to install yet another app. This
    also makes for a quick prototyping platform that enables inexpensive validation
    of hypotheses before investing significant amounts of time and money to build
    native experiences. TensorFlow.js, in conjunction with ml5.js, has accelerated
    the process of bringing the power of AI to the browser and broaden its reach to
    the masses.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中运行神经网络的一个巨大好处是浏览器相比于任何智能手机平台具有更广泛的覆盖范围。再加上不必克服用户不愿安装另一个应用的优势。这也为快速原型设计提供了一个平台，使得在投入大量时间和金钱构建本地体验之前，可以廉价地验证假设。TensorFlow.js与ml5.js结合使用，加速了将人工智能的力量带入浏览器的过程，并将其覆盖范围扩大到大众。
