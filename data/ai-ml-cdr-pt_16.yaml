- en: Chapter 15\. Transformers and transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章\. Transformers和transformers
- en: 'With the paper [“Attention Is All You Need” by Ashish Vaswani et al.](https://oreil.ly/R7og7)
    in 2017, the field of AI was changed forever. While the abstract of the paper
    indicates something lightweight and simple—an evolution of the architecture of
    convolutions and recurrence (see Chapters [4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)
    through [9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    of this book)—the impact of the work was, if you’ll forgive the pun, transformative.
    It utterly revolutionized AI, beginning with NLP. Despite the authors’ claim of
    the simplicity of the approach, implementing it in code is and was inherently
    complex. At its core was a new approach to ML architecture: *Transformers* (which
    we capitalize to indicate that we’re referring to them as a concept).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Ashish Vaswani等人发表的论文[“Attention Is All You Need”](https://oreil.ly/R7og7)彻底改变了AI领域。虽然论文的摘要表明这是一种轻量级和简单的方法——卷积和递归架构的演变（参见本书的第[4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)章到第[9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)章）——但这项工作的影响，如果你不介意这个双关语，是变革性的。它彻底颠覆了AI，从NLP开始。尽管作者声称这种方法简单，但在代码中实现它本质上是非常复杂的。其核心是一种新的ML架构方法：*Transformers*（我们用大写字母来表示我们指的是它们作为一个概念）。
- en: 'In this chapter we’ll explore the ideas behind Transformers at a high level,
    demonstrating the three main architectures: encoder, decoder and encoder-decoder.
    Please note that we will just be exploring at a very high level, giving an overview
    of how these architectures work. To go deep into these would require several books,
    not just a single chapter!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从高层次探讨Transformers背后的思想，展示三种主要架构：编码器、解码器和编码器-解码器。请注意，我们只是从非常高的层次进行探讨，概述这些架构是如何工作的。要深入了解这些，需要几本书，而不仅仅是单独的一章！
- en: We’ll then explore *transformers*, which we lowercase to indicate that they
    are the APIs and libraries from Hugging Face that are designed to make using Transformer-based
    models easy to use. Before transformers, you had to read the papers and figure
    out how to implement the details for yourself for the most part. So, the Hugging
    Face transformers library has widened access to models created using the Transformer
    architecture and has become the de facto standard for using the many models that
    have been created using the transformer-based architecture.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨*transformers*，我们将其小写以表明它们是Hugging Face的API和库，旨在使使用基于Transformer的模型变得容易。在transformers之前，你大部分时间都需要阅读论文并自己弄清楚如何实现细节。因此，Hugging
    Face的transformers库扩大了使用Transformer架构创建的模型的可访问性，并已成为使用基于transformer架构创建的许多模型的实际标准。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Just to clarify, for the rest of this chapter, I’ll refer to the architecture,
    models, and concepts as Transformers (with a capital *T*) and the Hugging Face
    libraries as transformers (with a lowercase *t*) to prevent confusion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，在本章的剩余部分，我将把架构、模型和概念称为Transformers（首字母大写的*T*），而将Hugging Face库称为transformers（首字母小写的*t*），以避免混淆。
- en: Understanding Transformers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Transformers
- en: Since the publication of the original paper mentioned in the introduction to
    this chapter, the field of Transformers has evolved and grown, but its underlying
    basis has remained pretty much the same. In this section, we’ll explore this.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自从本章引言中提到的原始论文发表以来，Transformers领域已经发展和成长，但其基础仍然基本保持不变。在本节中，我们将探讨这一点。
- en: When working with LLMs anywhere (not just with Hugging Face), you’ll hear of
    the terms *encoder*, *decoder*, and *encoder-decoder*. Therefore, I think it’s
    a good idea for you to get a high-level understanding of them. Each of these architectures
    represents a different approach to text management—be it processing, classification,
    or generation. They have specific strengths for particular scenarios, and to optimize
    for your scenario, it’s good to understand them so you can choose the appropriate
    ones.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在任何地方（而不仅仅是与Hugging Face一起）处理LLMs时，你会听到*编码器*、*解码器*和*编码器-解码器*这些术语。因此，我认为你了解它们的高级概念是个好主意。这些架构中的每一个都代表了处理文本的不同方法——无论是处理、分类还是生成。它们在特定场景下具有特定的优势，为了优化你的场景，了解它们是很好的，这样你可以选择合适的架构。
- en: Encoder Architectures
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器架构
- en: Encoder-only architectures (e.g., BERT, RoBERTa) generally excel at *understanding*
    text because of how rigorous they are in processing it. They’re bidirectional
    in nature, being able to “see” the entire input sequence at once. With that nature
    of understanding, they’re particularly effective for tasks that require a deep
    understanding and comprehension of the text and its semantics. So, they’re particularly
    suited for tasks such as classification, named-entity recognition, and extraction
    of meaning for things like question answering. Their strength is transforming
    text into rich, contextual representations, but they’re not designed to *generate*
    new text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 仅编码器架构（例如BERT、RoBERTa）通常在*理解*文本方面表现出色，因为它们在处理文本时非常严谨。它们本质上是双向的，能够一次性“看到”整个输入序列。有了这种理解能力，它们特别适用于需要深入理解和理解文本及其语义的任务。因此，它们特别适合分类、命名实体识别和问答等任务。它们的优势是将文本转换为丰富的、上下文相关的表示，但它们并不是为了*生成*新文本而设计的。
- en: You can see the encoder-based architecture in [Figure 15-1](#ch15_figure_1_1748549808951859).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图15-1](#ch15_figure_1_1748549808951859)中看到基于编码器的架构。
- en: '![](assets/aiml_1501.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1501.png)'
- en: Figure 15-1\. Encoder-based architecture
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-1\. 基于编码器的架构
- en: Let’s explore this architecture in a little more detail. It begins with the
    tokenized inputs, which are then passed to the self-attention layer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨这个架构。它从标记化输入开始，然后传递到自注意力层。
- en: The self-attention layer
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力层
- en: '*Self-attention* is the core mechanism that allows tokens to “pay attention”
    to other tokens in the input sequence. So, for example, consider the sentence
    “I went to high school in Ireland, so I had to study how to speak Gaelic.” The
    last word in this sentence is *Gaelic*, and it’s effectively triggered by the
    word *Ireland* earlier in the sentence. If a model pays attention to the entire
    sentence, it can predict the word *Gaelic* to be the next word. On the other hand,
    if the model didn’t pay attention to the entire sentence, then it might interpret
    from the sentence something more appropriate to “how to speak,” such as *politely*
    or another adjective.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注意力*是允许标记“关注”输入序列中其他标记的核心机制。例如，考虑句子“我在爱尔兰上高中，所以我不得不学习如何说盖尔语。”这个句子的最后一个词是*盖尔语*，它实际上是由句子中较早的词*爱尔兰*触发的。如果一个模型关注整个句子，它就可以预测下一个词是*盖尔语*。另一方面，如果一个模型没有关注整个句子，那么它可能从句子中解读出更合适的“如何说”的内容，比如*礼貌地*或其他形容词。'
- en: However, the self-attention mechanism—by considering the entire sentence—can
    understand context like that more granularly. It works by having each token in
    the sentence get three vectors associated with it. These are the query (Q) vector
    (aka “What am I looking for that’s relevant to this token?”), the key (K) vector
    (aka “What tokens might reference me?”), and the value (V) vector (aka “What type
    of information do I carry?”). The representations in these vectors are learned
    over time, in much the same process as we saw in earlier chapters of this book.
    An attention score is then calculated as a function of these, and using Softmax,
    the embeddings for the words will be updated with the attention details, bending
    word embeddings closer to one another when there are similarities learned between
    them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自注意力机制——通过考虑整个句子——可以更细致地理解上下文。它是通过让句子中的每个标记与三个向量相关联来工作的。这些是查询（Q）向量（即“我在寻找什么与这个标记相关的信息？”）、键（K）向量（即“哪些标记可能引用我？”）和值（V）向量（即“我携带什么类型的信息？”）。这些向量中的表示随着时间的推移而学习，与我们在本书早期章节中看到的类似过程。然后根据这些向量计算一个注意力分数，并使用Softmax，将单词的嵌入更新为注意力细节，当它们之间学习到相似性时，将单词嵌入弯曲得更近。
- en: Do note that self-attention is generally bidirectional, so the order of the
    words doesn’t matter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，自注意力通常是双向的，所以单词的顺序并不重要。
- en: The self-attention mechanism usually also has the context of *heads*, which
    are effectively multiple, parallel instances of the three vectors (Q, K, and V)
    that we saw previously, which can learn different representations and effectively
    specialize in different aspects of the input. A high-level representation of these
    heads is shown in [Figure 15-2](#ch15_figure_2_1748549808951907).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制通常也有“头”的概念，这些头实际上是之前我们看到的三种向量（Q、K和V）的多个并行实例，它们可以学习不同的表示并有效地专门处理输入的不同方面。这些头的的高级表示如图[图15-2](#ch15_figure_2_1748549808951907)所示。
- en: Thus, each head has its own set of learned weights for Q, K, and V vectors.
    The processing and learning for these vectors is done in parallel, with their
    results concatenated, and their final output projection is then a combination
    of information from each of the heads. As models have grown larger over time,
    one of the factors for this growth is the number of heads. For example, BERT-base
    has 12 heads and BERT-large has 16 heads. Architectures that also use a decoder,
    such as GPT, have grown similarly. GPT-2 had 12 attention heads, whereas GPT-3
    grew to 96!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个头都有其自己的Q、K和V向量的学习权重集。这些向量的处理和学习是并行进行的，它们的最终输出投影是每个头的组合信息。随着时间的推移，随着模型规模的增大，导致这种增长的因素之一是头的数量。例如，BERT-base有12个头，BERT-large有16个头。也使用解码器的架构，如GPT，也有类似的增长。GPT-2有12个注意力头，而GPT-3增长到96个！
- en: Returning to [Figure 15-1](#ch15_figure_1_1748549808951859), the self-attention
    instance (in which [Figure 15-2](#ch15_figure_2_1748549808951907) can be encapsulated
    into the self-attention box from [Figure 15-1](#ch15_figure_1_1748549808951859))
    then outputs to a *feedforward network* (FFN), which is often more accurately
    referred to as a *position-wise feedforward network*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图15-1](#ch15_figure_1_1748549808951859)，自注意力实例（其中[图15-2](#ch15_figure_2_1748549808951907)可以封装到[图15-1](#ch15_figure_1_1748549808951859)中的自注意力框中）然后输出到一个*前馈网络*（FFN），这通常更准确地被称为*位置前馈网络*。
- en: '![](assets/aiml_1502.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1502.png)'
- en: Figure 15-2\. Multihead self-attention
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2. 多头自注意力
- en: The feedforward network layer
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前馈网络层
- en: The FFN layer is vital in supporting the model’s capacity to learn complex patterns
    in the text. It does this by introducing nonlinearity into the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: FFN层对于支持模型学习文本中的复杂模式至关重要。它是通过向模型中引入非线性来做到这一点的。
- en: Why is that important? First of all, let’s understand the difference between
    linearity and nonlinearity. A *linear equation* is one for which the value is
    relatively easy to predict. For example, consider an equation that determines
    a house price. A linear version of this might be the cost of the land plus a particular
    dollar amount per square foot, and every house would follow the same formula.
    But as we know, house prices are far more complex than this—they don’t (unfortunately)
    follow a simple linear equation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？首先，让我们了解线性和非线性之间的区别。一个*线性方程*是指其值相对容易预测的方程。例如，考虑一个确定房价的方程。这个方程的线性版本可能是土地成本加上每平方英尺特定的美元金额，并且每栋房子都会遵循相同的公式。但正如我们所知，房价远比这复杂得多——它们（不幸的是）并不遵循简单的线性方程。
- en: Understanding sentiment can be the same. So, for example, if you were to assign
    coordinates on a graph (like we did when explaining embeddings in [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648))
    to the words *good* and *not*, where *good* might be +1 and *not* might be –1,
    then a linear relationship between these for *not good* would give us 0, which
    is neutral, whereas *not good* is clearly negative. So, we need equations that
    are more nuanced (i.e., nonlinear) when capturing sentiment and effectively understanding
    our text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理解情感可以是一样的。所以，例如，如果你要在图表上分配坐标（就像我们在[第7章](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)解释嵌入时做的那样）给单词*good*和*not*，其中*good*可能是+1，而*not*可能是–1，那么*not
    good*之间的线性关系会给我们0，这是中性的，而*not good*显然是负面的。因此，在捕捉情感和有效理解我们的文本时，我们需要更细微的方程（即非线性）。
- en: That’s the job of the FFN. It achieves this nonlinearity by expanding the dimensions
    of its input vector, applying a transformation to that, and using ReLU to “remove”
    the negative values (and thus remove the linearity) before restoring the vector
    back to its original dimensions. You can see this in [Figure 15-3](#ch15_figure_3_1748549808951933).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是FFN的工作。它通过扩展其输入向量的维度，对该向量进行变换，并使用ReLU“移除”负值（从而移除线性）来达到非线性，然后再将向量恢复到其原始维度。你可以在[图15-3](#ch15_figure_3_1748549808951933)中看到这一点。
- en: '![](assets/aiml_1503.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1503.png)'
- en: Figure 15-3\. A feedforward network
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3. 前馈网络
- en: 'The underlying math and logic behind how it works are a little beyond the scope
    of this book, but let’s explore them with a simple example. Consider this code,
    which simulates what’s happening in [Figure 15-3](#ch15_figure_3_1748549808951933):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它如何工作的底层数学和逻辑略超出了本书的范围，但让我们用一个简单的例子来探讨一下。考虑以下代码，它模拟了[图15-3](#ch15_figure_3_1748549808951933)中发生的情况：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We start with a simple 2D tensor: `[–1.0, 2.0].`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的2D张量开始：`[–1.0, 2.0]`。
- en: 'The first linear layer has the following weights and biases:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层线性层的权重和偏差如下：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When we pass our 2D tensor through this layer to get `layer1_out`, the matrix
    multiplication gives us a 4D output that looks like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将我们的2D张量通过这一层得到`layer1_out`时，矩阵乘法给我们一个看起来是这样的4D输出：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are two negative values in this layer (–3 and –0.5), so when we pass
    it through the ReLU, they are set to zero and our matrix becomes this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一层有两个负值（-3和-0.5），所以当我们通过ReLU传递时，它们被设置为0，我们的矩阵变成这样：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This process is called *bending*. By taking these values out, we’re now introducing
    nonlinearity into the equation. The relationships between the values have become
    much more complex, so a process that attempts to learn the parameters of those
    relationships will have to deal with that complexity, and if it succeeds in doing
    so, it will avoid the linearity trap.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*弯曲*。通过移除这些值，我们现在将非线性引入方程中。值之间的关系变得更加复杂，因此试图学习这些关系参数的过程将不得不处理这种复杂性，如果它成功地做到了这一点，它将避免线性陷阱。
- en: 'Next, we’ll convert back to a 2D tensor by going through another layer, with
    weights and biases like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过另一个具有类似权重和偏差的层将其转换回2D张量：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the layer will then look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 层的输出将看起来像这样：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, the effect of the FFN is to take in a vector and output a vector of the
    same dimension, with linearity removed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，FFN的效果是接收一个向量并输出一个相同维度的向量，去除了线性。
- en: 'We can explore this with our simple code. Consider what happens when we take
    our input and apply simple, linear changes to it. So, if we take [–1.0, 2.0] and
    double it to [–2.0, 4.0], the nonlinearity introduced by the FFN will mean that
    the output won’t be a simple doubling. And similarly, if we negate it, the output
    again won’t be a simple negation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用我们的简单代码来探索这一点。考虑当我们对我们的输入应用简单的线性变化时会发生什么。所以，如果我们取[-1.0, 2.0]并将其翻倍为[-2.0,
    4.0]，FFN引入的非线性意味着输出不会是简单的翻倍。同样，如果我们取反，输出也不会是简单的取反：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Over time, the parameters that are learned for the weights and biases should
    maintain the relationships between the tokens and allow the network to learn more
    nuanced, nonlinear equations that define the overall relationships between them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，为权重和偏差学习到的参数应该保持标记之间的关系，并允许网络学习更细微、非线性的方程，这些方程定义了它们之间的整体关系。
- en: Layer normalization
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层归一化
- en: Referring back to [Figure 15-1](#ch15_figure_1_1748549808951859), the next step
    in the process is called *layer normalization*. At this point, the goal is to
    stabilize the data flowing through the neural network by removing outliers and
    high variance. Layer normalization does this by calculating the mean and variance
    of the input features, which it then normalizes and scales/shifts before outputting
    (see [Figure 15-4](#ch15_figure_4_1748549808951956)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图15-1](#ch15_figure_1_1748549808951859)，过程的下一步被称为*层归一化*。在这个阶段，目标是通过对异常值和高方差进行移除来稳定通过神经网络的流动数据。层归一化通过计算输入特征的均值和方差来实现这一点，然后对其进行归一化并输出之前进行缩放/平移（参见[图15-4](#ch15_figure_4_1748549808951956))。
- en: '![](assets/aiml_1504.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1504.png)'
- en: Figure 15-4\. Layer normalization
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-4\. 层归一化
- en: From a statistical perspective, the idea of removing the outliers from using
    mean and variance and then normalizing them is quite straightforward. I won’t
    go into details on the statistics here, but that’s generally the goal of doing
    these types of calculations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学的角度来看，从均值和方差中移除异常值并对其进行归一化的想法相当直接。在这里我不会深入细节，但那通常是进行这些类型计算的一般目标。
- en: The *Scale and Shift* box then becomes a mystery. Why would you want to do this?
    Well, if you dig a little bit into the logic, the idea here is that the process
    of *normalization* will drive the mean of a set of values to 0 and the standard
    deviation to 1\. The process itself can destroy distinctiveness in our input features
    by making them too alike. So, if there’s a process that we can use to return some
    level of variance to them with parameters that are learned, we can clean up the
    data without destroying it—meaning we won’t throw the baby out with the bathwater!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*缩放和偏移*框变得神秘。你为什么要这样做呢？好吧，如果你稍微深入逻辑，这里的想法是，*归一化*的过程会将一组值的均值驱动到0，标准差驱动到1。这个过程本身可能会通过使它们过于相似而破坏我们输入特征的独特性。所以，如果我们有一个可以使用学习到的参数来恢复它们某些程度方差的过程，我们可以在不破坏它们的情况下清理数据——这意味着我们不会把婴儿和洗澡水一起倒掉！
- en: Therefore, multiplying our outputs by values with offsets can change this. These
    values are typically called *gamma* and *beta* values, and they act a little like
    weights and biases. It’s probably easiest to show this in code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将我们的输出乘以具有偏移量的值可以改变这一点。这些值通常被称为*gamma*和*beta*值，它们的作用有点像权重和偏差。可能通过代码来展示这一点最简单。
- en: 'So, consider this example, in which we’ll take an input feature containing
    some values and then normalize them. We’ll see that the normalized values will
    have a mean of 0 and a standard deviation of 1:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑以下示例，我们将取一个包含一些值的输入特征，然后对其进行归一化。我们会看到归一化后的值将具有0的均值和1的标准差：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'But when we move the values through the scale and shift by using gamma and
    beta values, we get a new set of parameters that maintain a closer relationship
    to the originals but with massive variance (aka noise) removed. The output of
    this code should look like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当我们通过gamma和beta值对值进行缩放和平移时，我们得到一组新的参数，这些参数与原始参数保持更紧密的关系，但去除了大量的方差（即噪声）。此代码的输出应如下所示：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Like the FFN, this is effectively destroying and then reconstructing the features
    in a clever way. In this case, it’s designed to do it to remove variance, which
    has the effect of amplifying or dampening features as needed by shifting the overall
    distribution to better ranges for activation functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像FFN一样，这实际上是以一种巧妙的方式破坏并重新构建特征。在这种情况下，它被设计用来去除方差，这会根据需要通过将整体分布平移到更好的激活函数范围来放大或抑制特征。
- en: I like to think of this as what you do with your TV to get a better image—sometimes,
    you adjust the contrast or the brightness. By finding the optimal values of these
    settings, you can see the important details of a particular image better. Think
    of the contrast as the scale and the brightness as the shift. If a network can
    learn these for your input features, it will improve its ability to understand
    them!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将这想象成你对电视所做的以获得更好的图像——有时，你会调整对比度或亮度。通过找到这些设置的优化值，你可以更好地看到特定图像的重要细节。将对比度视为比例，将亮度视为平移。如果一个网络能够学习你的输入特征，它将提高理解它们的能力！
- en: Repeated encoder layers
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复的编码器层
- en: Referring back to [Figure 15-1](#ch15_figure_1_1748549808951859), you’ll see
    that the self-attention, feedforward, and layer normalization layers can be repeated
    N times. Typically, smaller models will have 12 instances and larger ones will
    have 24\. The deeper the model, the more computational resources are required.
    More layers provide more capacity to learn complex patterns, but of course, this
    means longer training time, more GPU memory overhead, and potentially greater
    risks of overfitting. Additionally, larger models can impact the corresponding
    data requirements, leading to a possibly negative knock-on effect for complexity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图15-1](#ch15_figure_1_1748549808951859)，你会看到自注意力、前馈和层归一化层可以重复N次。通常，较小的模型将有12个实例，较大的模型将有24个实例。模型越深，所需的计算资源就越多。更多的层提供了更多的学习复杂模式的能力，但当然，这也意味着更长的训练时间、更多的GPU内存开销，以及潜在的过拟合风险。此外，较大的模型可能会影响对应的数据需求，导致复杂性的可能负面影响。
- en: For the most part, there’s a trade-off between the depth of the model (the number
    of layers) and the width (the size of each layer, including the number of heads).
    In some cases, models can reuse the same layer multiple times to reduce the overall
    parameter count—and ALBERT is an example of this.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，模型深度（层数）和宽度（每层的大小，包括头的数量）之间存在权衡。在某些情况下，模型可以通过多次重用相同的层来减少整体参数数量——ALBERT就是这样一个例子。
- en: The Decoder Architecture
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器架构
- en: While the encoder architecture specializes in understanding text by having attention
    across the entire context of the input sequence, the decoder architecture serves
    as the generative powerhouse. It is designed to produce sequential outputs one
    element at a time. While the encoder processes all inputs simultaneously (or as
    many of them as it can, based on the parallelization of the system), the decoder
    operates autoregressively. It generates each output token while considering both
    the encoded input representations *and* the previously generated outputs. The
    goal is to maintain coherence and contextual relevance through the process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然编码器架构通过在整个输入序列的上下文中具有注意力来专门理解文本，但解码器架构作为生成动力源。它被设计成一次生成一个元素顺序输出。虽然编码器同时处理所有输入（或尽可能多的输入，基于系统的并行化），但解码器是自回归的。它在生成每个输出标记时，同时考虑编码的输入表示和之前生成的输出。目标是保持过程的一致性和上下文相关性。
- en: You can see a diagram of the encoder architecture in [Figure 15-5](#ch15_figure_5_1748549808951977).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图15-5](#ch15_figure_5_1748549808951977)中看到编码器架构的示意图。
- en: '![](assets/aiml_1505.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1505.png)'
- en: Figure 15-5\. The decoder architecture
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-5\. 解码器架构
- en: 'Let’s explain this from the top down. The first box is the previously generated
    tokens. In a pure decoder architecture, this is the set of tokens that have already
    been generated or provided. When they’re provided, they’re typically called the
    prompt. So, say you provide the following tokens:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上往下解释这个结构。第一个框是之前生成的标记。在一个纯解码器架构中，这是已经生成或提供的标记集合。当它们被提供时，通常被称为提示。所以，假设你提供了以下标记：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'after one is run through the decoder, the token for “clap” will be generated.
    You will now have these:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器处理后，将生成“clap”的标记。现在你将拥有以下内容：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These tokens will flow into the box for token embedding + positional encoding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记将流入标记嵌入 + 位置编码的框中。
- en: Understanding token and positional encoding
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解标记和位置编码
- en: This transforms each token into a vector representation called an *embedding*
    (as explained in [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)),
    which clusters words of similar semantic meaning in a similar vector space. Remember
    that these embeddings are learned over time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将每个标记转换为一个称为*嵌入*的向量表示（如第5章[第5章自然语言处理简介](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中所述），在相似的向量空间中聚类具有相似语义意义的单词。请记住，这些嵌入是随着时间的推移而学习的。
- en: 'So, for example, if we think about the words *it* and *cla*p at the end of
    the aforementioned token list, they may have token embeddings that look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑上述标记列表末尾的单词*it*和*clap*，它们可能具有如下所示的标记嵌入：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I have simplified the embedding to just three dimensions for readability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我为了可读性简化了嵌入到只有三个维度。
- en: The next step is to perform the *positional* encoding, which is a huge innovation
    in Transformers. In addition to an encoding in a vector space for the semantics
    and meaning of the word, an innovative method using sine and cosine waves is performed
    to encode the word’s position and the impact of this position on neighbors.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是执行*位置编码*，这是Transformers中的一个巨大创新。除了在向量空间中对单词的语义和意义进行编码外，还使用正弦和余弦波进行创新方法，以编码单词的位置及其对邻居的影响。
- en: 'So, for example, given that we have 3D vectors for our encodings, we’ll create
    a 3D vector using sine waves for the odd-numbered indices and cosine waves for
    the even-numbered ones, like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，鉴于我们为编码使用了三维向量，我们将使用正弦波为奇数索引创建一个三维向量，为偶数索引使用余弦波，如下所示：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then add these together to get this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些向量相加以得到以下结果：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'While this may seem arbitrary, the positional encodings actually come from
    a specific formula. These are shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来可能是任意的，但位置编码实际上来自一个特定的公式。这些公式如下所示：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you plot these values as a table, they’ll look like [Table 15-1](#ch15_table_1_1748549808960051).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这些值作为表格绘制，它们将看起来像[表15-1](#ch15_table_1_1748549808960051)。
- en: Table 15-1\. Positional encodings
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-1\. 位置编码
- en: '| Position | Dimension 0 | Dimension 1 | Dimension 2 | Dimension 3 | Dimension
    4 | Dimension 5 | Dimension 6 | Dimension 7 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 维度 0 | 维度 1 | 维度 2 | 维度 3 | 维度 4 | 维度 5 | 维度 6 | 维度 7 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 |'
- en: '| **1** | 0.841 | 0.540 | 0.100 | 0.995 | 0.010 | 1.000 | 0.001 | 1.000 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.841 | 0.540 | 0.100 | 0.995 | 0.010 | 1.000 | 0.001 | 1.000 |'
- en: '| **2** | 0.909 | –0.416 | 0.199 | 0.980 | 0.020 | 1.000 | 0.002 | 1.000 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.909 | –0.416 | 0.199 | 0.980 | 0.020 | 1.000 | 0.002 | 1.000 |'
- en: '| **3** | 0.141 | –0.990 | 0.296 | 0.955 | 0.030 | 1.000 | 0.003 | 1.000 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0.141 | –0.990 | 0.296 | 0.955 | 0.030 | 1.000 | 0.003 | 1.000 |'
- en: The ultimate goal here is to have a relationship between each position on the
    list and every other position that, in some dimensions, has tokens that are typically
    far apart being a little closer and those that are typically closer being a little
    further apart!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最终目标是列表上的每个位置与其他每个位置之间都有一个关系，在某些维度上，通常距离较远的标记会稍微靠近一些，而通常距离较近的标记会稍微远离一些！
- en: So, imagine you have an input sequence of four tokens in positions 0 through
    3, as charted in the table. The token in position 3 is as far away as possible
    from the token in position 0, so they are at extreme ends of the sequence.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，想象一下你有一个输入序列，包含位置0到3的四个标记，如表格所示。位置3的标记与位置0的标记尽可能远，因此它们位于序列的两端。
- en: With a positional encoding like this, we are given the possibility that in some
    dimensions, they are closer together. You can see in the first column that the
    values for dimension 0 places position 3 closer to position 0 than either of the
    others, whereas in the column for dimension 2, they are further apart. By using
    these positional encodings, we’re opening up the possibility that words can be
    clustered, even if they’re far apart in a sentence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种位置编码，我们得到了在某些维度上它们可能更靠近彼此的可能性。你可以在第一列中看到，维度0的值将位置3与位置0的距离比其他任何位置都要近，而在维度2的列中，它们则更远。通过使用这些位置编码，我们打开了单词可以聚集的可能性，即使它们在句子中的距离很远。
- en: Think back to the sentence “I went to high school in Ireland, so I had to study
    how to speak Gaelic.” In that case, the final token “Gaelic” was most accurate
    because it described a language in “Ireland,” which was earlier in the sentence.
    Without positional encoding, this would have been missed!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 想想那个句子“我在爱尔兰上高中，所以我不得不学习如何说盖尔语。”在这种情况下，最后的标记“盖尔语”是最准确的，因为它描述了“爱尔兰”的语言，这在句子中是早先提到的。如果没有位置编码，这就会被遗漏！
- en: Also, the positional encodings are *added* to the token embeddings, so they
    provide a sort of pressure to keep together the words that might be semantically
    related in different parts of the sentence, but they don’t completely override
    the embeddings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，位置编码被**添加**到标记嵌入中，因此它们为可能在不同句子部分语义相关的单词提供了一种保持在一起的压力，但它们并不完全覆盖嵌入。
- en: This is then fed into the multihead masked attention. We’ll look at that next.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将这些输入到多头掩码注意力中。我们将在下一节中探讨这一点。
- en: Understanding multihead masked attention
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解多头掩码注意力
- en: Earlier in this chapter, in the section on attention, we saw how the Q, K, and
    V vectors for each token are learned and used to update the embeddings of the
    word with attention to the other words. The idea behind *masked attention* updates
    this to ignore words that we shouldn’t be paying attention to. In other words
    (sic), the goal is that we should only pay attention to *previous* positions in
    the sequence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面的注意力部分，我们看到了每个标记的Q、K和V向量是如何学习和使用的，以及如何使用注意力来更新单词的嵌入。**掩码注意力**背后的想法是将这个更新扩展到忽略我们不应该关注的单词。换句话说（即），目标是我们应该只关注序列中的**先前**位置。
- en: 'So, imagine you have a sequence of eight words and you want to predict the
    ninth. When you’re processing the third word in the sentence, it should only pay
    attention to the second and first word but nothing after them. You can achieve
    this with a triangular matrix like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，想象一下你有一串由八个单词组成的序列，你想预测第九个单词。当你处理句子中的第三个单词时，它应该只关注第二个和第一个单词，而不要关注它们之后的任何内容。你可以通过这样的三角形矩阵来实现这一点：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So, imagine this for a set of words like *the*, *cat*, and *sat* (see [Table 15-2](#ch15_table_2_1748549808960104)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，想象一下对于像*the*、*cat*和*sat*这样的单词集合（见[表15-2](#ch15_table_2_1748549808960104)）。
- en: Table 15-2\. Simple masked attention
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-2\. 简单掩码注意力
- en: '|   | the | cat | sat |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|   | the | cat | sat |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| the | 1 | 0 | 0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| the | 1 | 0 | 0 |'
- en: '| cat | 1 | 1 | 0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| cat | 1 | 1 | 0 |'
- en: '| sat | 1 | 1 | 1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| sat | 1 | 1 | 1 |'
- en: When processing *the*, using this method means we can only pay attention to
    *The* itself. When processing *cat*, we can pay attention to both *the* and *cat*.
    When processing sat, we can pay attention to *the*, *cat*, and *sat*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理*the*时，使用这种方法意味着我们只能关注*The*本身。当处理*cat*时，我们可以关注*the*和*cat*。当处理sat时，我们可以关注*the*、*cat*和*sat*。
- en: So, recalling that the K, Q, and V vectors will amend the embeddings for the
    word in a way that bends them closer together for instances where the words may
    not have close syntactical meaning but are impacting one another through attention
    (like *Ireland* and *Gaelic* in the earlier example), the goal of the masked attention
    layer will only do this bending for words that we’re allowed to pay attention
    to.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，回忆一下，K、Q和V向量将根据情况调整单词的嵌入，使它们在可能没有紧密的句法意义但通过注意力相互影响（如早期示例中的*Ireland*和*Gaelic*）的情况下更靠近。因此，掩码注意力层的目的是只为我们可以关注到的单词进行这种弯曲。
- en: When this is performed multiple times, in parallel, across multiple heads, and
    aggregated together, we get an attention adjustment of the embeddings that’s very
    similar to the one we did with the encoder—except that the masking prevents any
    amendment of the token from words *after* it in the sequence, particularly those
    that are generated.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当多次并行地在多个头部执行此操作，并将结果汇总在一起时，我们得到一个与编码器中使用的非常相似的嵌入注意力调整——只不过掩码防止了序列中任何单词之后（尤其是生成的单词）的标记被修改。
- en: Adding and normalizing
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加和归一化
- en: Next, we take the attention output from the masked attention layer and add it
    to the original input. This is called making a *residual connection*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将掩码注意力层的注意力输出添加到原始输入中。这被称为建立*残差连接*。
- en: 'So, for example, the process might look like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个过程可能看起来像这样：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we think about this, we’ll see that we don’t *replace* the original information
    with the attention mechanism but instead we *add* the new learned embeddings from
    the attention mechanism, thus preserving the original information. Over time,
    this will have the effect of helping the network learn. If some attention updates
    aren’t useful, then the network will just make them close to zero through the
    backpropagation of gradients.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考这个问题时，我们会发现我们并没有用注意力机制*替换*原始信息，而是*添加*了从注意力机制中学习的新嵌入，从而保留了原始信息。随着时间的推移，这将有助于网络学习。如果某些注意力更新没有用，那么网络将通过梯度回传将它们调整为接近零。
- en: This is beyond the scope of this book, and it’s generally found in the papers
    behind the creation of these models. But ultimately, the goal here is to get rid
    of a problem called the *vanishing gradient problem*—in which, if the original
    input was *not* maintained, then the gradients of the attention layer can get
    smaller and smaller with successive layers, thus limiting the number of layers
    you can use. But if you always add the gradients to the original input, then there
    will be a floor—such as the [0.5, –0.3, 0.7, 0.1] for the cat gradient previously
    mentioned—so the small changes from the attention gradients won’t push these values
    close to zero and cause the overall gradients to vanish.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这超出了本书的范围，通常可以在创建这些模型的论文中找到。但最终，这里的目的是解决被称为*梯度消失问题*的问题——在这种情况下，如果原始输入没有被保持，那么注意力层的梯度会随着后续层的增加而变得越来越小，从而限制了你可以使用的层数。但是，如果你总是将梯度添加到原始输入中，那么就会有一个下限——例如，之前提到的猫的梯度[0.5,
    –0.3, 0.7, 0.1]——因此，注意力梯度的小变化不会将这些值推向零，从而导致整体梯度消失。
- en: This is then pushed through a layer normalization, as described in the encoder
    chapter, to remove outliers while keeping the knowledge of the sequences intact.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其通过层归一化，如编码器章节所述，以去除异常值同时保持序列的知识。
- en: The feedforward layer
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前馈层
- en: The feedforward layer operates in exactly the same way as those layers used
    in encoders (see earlier in this chapter), with the goal of reducing any linear
    dependencies in the token sequence. The output from this is again added to the
    original data and then normalized, the logic being that the process of removing
    the outliers with the FFN should also prevent the gradients from vanishing and
    thus preserve important information. The normalization also keeps the values in
    a stable range, as repeatedly adding as we’re doing here might push some values
    far above 1.0 or below –1.0, and normalized values in these ranges tend to be
    better for matrix calculation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈层的工作方式与编码器中使用的那些层完全相同（参见本章前面的内容），目的是减少标记序列中的任何线性依赖。从这个输出再次添加到原始数据中，然后进行归一化，其逻辑是，移除FFN中的异常值的过程也应该防止梯度消失，从而保留重要信息。归一化还保持了值的稳定范围，因为我们反复添加，可能会将一些值推到远高于1.0或低于–1.0，而这些范围内的归一化值通常更适合矩阵计算。
- en: We can repeat this process of masked attention -> add and normalize -> feedforward
    -> add and normalize multiple times before we get to the next layer, where we’ll
    use the learned values to predict the next token.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到达下一层之前，我们可以重复这个过程多次，即掩码注意力 -> 添加和归一化 -> 前馈 -> 添加和归一化，在下一层中，我们将使用学习到的值来预测下一个标记。
- en: The linear and Softmax layers
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性和Softmax层
- en: The linear and Softmax layers are responsible for turning the decoder’s representations
    into probabilities for the next token.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和Softmax层负责将解码器的表示转换为下一个标记的概率。
- en: The linear layer will learn representations for each of the words in the dictionary
    with the transposed size of the decoder’s representations. This is a bit of a
    mouthful, so let’s explore it with an example.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层将学习字典中每个单词的表示，其大小与解码器表示的转置大小相同。这有点难以理解，所以让我们用一个例子来探讨一下。
- en: Say our decoder output, having flowed through all the layers, is a 4D representation,
    like in [Table 15-3](#ch15_table_3_1748549808960134).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的解码器输出，经过所有层后，是一个4D表示，就像[表15-3](#ch15_table_3_1748549808960134)中那样。
- en: Table 15-3\. Simulated decoder output
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-3\. 模拟解码器输出
- en: '|   |   |   |   |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   |   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.2 | –0.5 | 0.8 | –0.3 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | –0.5 | 0.8 | –0.3 |'
- en: We now have a weights matrix for each word in our vocabulary that is learned
    during training, and that matrix might look like the one in [Table 15-4](#ch15_table_4_1748549808960160).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为词汇表中的每个词都有一个在训练过程中学习的权重矩阵，这个矩阵可能看起来像[表15-4](#ch15_table_4_1748549808960160)中的那个。
- en: Table 15-4\. Weights matrix for words in our vocab
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-4\. 词汇中单词的权重矩阵
- en: '| cat | dog | sat | mat | the |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| cat | dog | sat | mat | the |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1.0 | 0.5 | 2.0 | 0.3 | 0.7 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 0.5 | 2.0 | 0.3 | 0.7 |'
- en: '| -0.3 | 0.8 | 1.5 | 0.4 | 0.2 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| -0.3 | 0.8 | 1.5 | 0.4 | 0.2 |'
- en: '| 2.0 | 0.3 | 2.1 | 0.5 | 0.8 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 0.3 | 2.1 | 0.5 | 0.8 |'
- en: '| 0.4 | 0.6 | 0.9 | 0.2 | 0.5 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 0.4 | 0.6 | 0.9 | 0.2 | 0.5 |'
- en: Note that the decoder representation is 1 × 4 and that each matrix for each
    word is 4 × 1. That’s the transposition, and multiplying them out is now easy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，解码器的表示是1 × 4，而每个单词的矩阵是4 × 1。这就是转置，现在乘法变得容易了。
- en: 'So, for *cat*, our final score will be this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于*cat*，我们的最终得分将是这个：
- en: (0.2 × 1.0) + (–.5 × –.3) + (0.8 × 2.0) + (–0.3 × 0.4) = 1.8
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (0.2 × 1.0) + (–.5 × –.3) + (0.8 × 2.0) + (–0.3 × 0.4) = 1.8
- en: 'We can then get final scores for each word, as in [Table 15-5](#ch15_table_5_1748549808960183):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以得到每个单词的最终得分，就像[表15-5](#ch15_table_5_1748549808960183)中那样：
- en: Table 15-5\. Final scores for each word
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-5\. 每个单词的最终得分
- en: '| cat | dog | sat | mat | the |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| cat | dog | sat | mat | the |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1.8 | -0.2 | 1.1 | 0.2 | 0.5 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 1.8 | -0.2 | 1.1 | 0.2 | 0.5 |'
- en: Using the Softmax function, these are then turned into probabilities, as in
    [Table 15-6](#ch15_table_6_1748549808960204).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Softmax函数，这些值随后被转换为概率，就像[表15-6](#ch15_table_6_1748549808960204)中那样。
- en: Table 15-6\. Probabilities from Softmax function
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表15-6\. Softmax函数的概率
- en: '| cat | dog | sat | mat | the |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| cat | dog | sat | mat | the |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 47.5% | 6.4% | 23.6% | 9.6% | 12.9% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 47.5% | 6.4% | 23.6% | 9.6% | 12.9% |'
- en: And then we can take the highest-probability word as the next token, in a process
    called *greedy decoding*. Alternatively, we can take 1 from *k* possible top values,
    in a process called *top-k decoding*, in which we pick, for example, the top three
    probabilities and choose one value from there.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将概率最高的单词作为下一个标记，这个过程称为*贪婪解码*。或者，我们可以从*k*个可能的前k个值中取1个，这个过程称为*top-k解码*，例如，我们选择前三个概率，并从那里选择一个值。
- en: This is then fed back into the top box as the new token list so that the process
    can continue to predict the next token.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些输出被反馈回顶部框作为新的标记列表，以便过程可以继续预测下一个标记。
- en: And that’s pretty much how decoders work, at least from a high level. In the
    next section, we’ll look at how they can be combined in the encoder-decoder architecture.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 至少从高层次来看，这就是解码器的工作方式。在下一节中，我们将探讨它们如何在编码器-解码器架构中结合在一起。
- en: The Encoder-Decoder Architecture
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: The *encoder-decoder architecture*, also known as *sequence-to-sequence*, combines
    the two aforementioned architecture types. It does this to tackle tasks that require
    transformation between input and output sequences of varying lengths. It’s proven
    to be very effective for machine translation in particular, but it also can be
    used in models for text summarization and question answering.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码器-解码器架构*，也称为*序列到序列*，结合了上述两种架构类型。这样做是为了处理需要输入和输出序列之间转换的任务。特别是对于机器翻译，它已被证明非常有效，但它也可以用于文本摘要和问答模型的模型中。'
- en: As you can see in [Figure 15-6](#ch15_figure_6_1748549808951997), it’s very
    similar to the decoder architecture, for the most part. The difference is the
    addition of a cross-attention layer that takes in the output from the encoder
    and injects it into the middle of the workflow.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图15-6](#ch15_figure_6_1748549808951997)中看到的，它的大部分结构与解码器架构非常相似。不同之处在于增加了一个交叉注意力层，该层接收来自编码器的输出并将其注入到工作流程的中间部分。
- en: The encoder will process the entire input sequence, creating a rich contextual
    representation that captures the full meaning of the input. The decoder layer
    can then query this, combining it with its representations to allow the decoder
    to focus additionally on relevant parts of the input when generating each new
    token.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将处理整个输入序列，创建一个丰富的上下文表示，捕捉输入的全部意义。解码器层可以查询这个表示，将其与其表示相结合，允许解码器在生成每个新标记时，额外关注输入的相关部分。
- en: '![](assets/aiml_1506.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1506.png)'
- en: Figure 15-6\. The encoder-decoder architecture
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-6\. 编码器-解码器架构
- en: 'Now, you might wonder at this point why the encoder-decoder architecture needs
    the encoder’s output, which it merges with cross-attention. Why can’t it just
    unmask in its own self-attention block? The fundamental reason boils down to the
    fact that this is more powerful for the following reasons:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能会想知道为什么编码器-解码器架构需要编码器的输出，并将其与交叉注意力合并。为什么它不能在自己的自注意力块中解除掩码？基本原因归结为以下事实：这有以下几个更强大的原因：
- en: Separation of concerns and parameter focus
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关注点的分离和参数聚焦
- en: If the decoder self-attention block were to be unmasked, it would have to handle
    the tasks of both understanding the input *and* generating the output simultaneously.
    That could lead to issues with learning because there’s a poor target. But if
    we separate them, each can focus on its own specialized role.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果解码器的自注意力块被解除掩码，它将不得不同时处理理解输入*和*生成输出的任务。这可能导致学习问题，因为目标不够明确。但如果我们将它们分开，每个都可以专注于自己的专业角色。
- en: Quality
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 质量
- en: If we separate concerns, each role can build up a rich representation that’s
    suitable for its task. In particular with the encoder, we have a well-known, battle-tested
    architecture for artificial understanding that we know works for that task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分离关注点，每个角色都可以构建一个适合其任务的丰富表示。特别是对于编码器，我们有一个众所周知、经过实战考验的人工理解架构，我们知道它适用于该任务。
- en: The major innovation here is the *cross-attention block*. We can demonstrate
    the intuition behind this with the analogy of a human language translator. When
    a person translates a sentence from French to English, they don’t just memorize
    the entire French corpus and then write English. Instead, while writing the English
    words, they actively look at different parts of the French sentence, focusing
    on the most relevant parts of the sentence for the word they are currently writing.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要创新是*交叉注意力块*。我们可以通过类比人类语言翻译者来展示这一直觉。当一个人将句子从法语翻译成英语时，他们不会只是记住整个法语语料库然后写英语。相反，在写英语单词的同时，他们会积极查看法语句子的不同部分，专注于当前正在写的单词最相关的句子部分。
- en: In French, the sentence “Le chat noir” translates to “The black cat,” but the
    noun and adjective are reversed. A straight translation would be “The cat black.”
    The human translator, when paying attention, would know this and would focus on
    other words in the French sentence. Cross-attention does the same thing. As the
    decoder generates each new word, it needs to refer to the source material to figure
    out what word to generate next. The words that have gone before in its output
    may not be enough.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在法语中，句子“Le chat noir”翻译成“The black cat”，但名词和形容词的顺序是颠倒的。直译将是“The cat black”。当人类翻译员注意这一点时，就会知道这一点，并会关注法语句子中的其他单词。交叉注意力做的是同样的事情。随着解码器生成每个新单词，它需要参考源材料来确定下一个要生成的单词。它输出中之前出现的单词可能不足以做到这一点。
- en: You can see this mechanism in action in [Figure 15-7](#ch15_figure_7_1748549808952017).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图15-7](#ch15_figure_7_1748549808952017)中看到这个机制的实际应用。
- en: Ultimately, an encoder creates a rich, contextual representation of the input
    because it artificially understands the source sentence. Cross-attention is a
    selective spotlight that highlights what the model believes to be the most relevant
    parts of this understanding for each word it generates, and this highlighting
    makes the model more effective at generating the correct words. You can see how
    this is very effective for machine translation, but it’s not much of a stretch
    to understand how it might be useful for other tasks!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，编码器通过人工理解源句子，创建了一个丰富、上下文相关的输入表示。交叉注意力是一个选择性的聚光灯，它突出了模型认为对每个生成的单词来说最相关的理解部分，这种突出使得模型在生成正确单词方面更加有效。你可以看到这对于机器翻译非常有效，但理解它可能对其他任务也有用并不需要太多的想象！
- en: The mechanism for cross-attention works with the K, Q, and V vectors as before,
    but the innovation here is that the Q vector will code from the decoder, while
    the K and V vectors will come from the encoder.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉注意力的机制与之前的K、Q和V向量相同，但这里的创新在于Q向量将从解码器编码，而K和V向量将来自编码器。
- en: This concludes a very high-level look at Transformers and how they work. There’s
    lots more detail—in particular, about *how* they learn things like the Q, K, and
    V vectors—that’s beyond the scope of this chapter, and I’d recommend reading the
    original paper to learn more.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对Transformer及其工作原理的一个非常高级的概述。还有很多更详细的细节——特别是关于它们如何学习Q、K和V向量等内容的细节——这些内容超出了本章的范围，我建议阅读原始论文以了解更多信息。
- en: Now, let’s switch gears to the transformers (with a lowercase *t*) API from
    Hugging Face, which makes it really easy for you to use Transformer-based models
    in code.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转换一下话题，转向Hugging Face的transformers（小写*t*）API，它使得你在代码中使用基于Transformer的模型变得非常容易。
- en: '![](assets/aiml_1507.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1507.png)'
- en: Figure 15-7\. Cross-attention
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-7\. 交叉注意力
- en: The transformers API
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: The transformers API
- en: 'At their core, transformers provide an API for working with pretrained models
    that use Python and PyTorch. The library’s success stems from three key innovations:
    a simple interface for using pretrained models, an extensive collection of pretrained
    models (as we explored in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)),
    and a vibrant community that frequently contributes improvements and new models.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，transformers提供了一个API，用于使用Python和PyTorch与预训练模型一起工作。该库的成功源于三个关键创新：一个用于使用预训练模型简单接口、一个广泛的预训练模型集合（正如我们在[第14章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)中探讨的），以及一个活跃的社区，该社区经常贡献改进和新模型。
- en: The library has evolved beyond its NLP roots to support multiple types of models,
    including those that support computer vision, audio processing, and multimodal
    tasks. Originally, the goal of the transformer-based architecture was to be effective
    in learning how a sequence of tokens is followed by another sequence of tokens.
    Then, innovative models built on this idea to allow concepts such as sound to
    be expressed as a sequence of tokens, and as a result, transformers could then
    learn sound patterns.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该库已经超越了其NLP根源，支持多种类型的模型，包括支持计算机视觉、音频处理和多模态任务的模型。最初，基于transformer的架构的目标是有效地学习一个标记序列是如何被另一个标记序列跟随的。然后，基于这个想法的创新模型允许诸如声音这样的概念被表示为一个标记序列，因此，transformers可以学习声音模式。
- en: For developers, transformers offer multiple abstraction levels. The high-level
    pipeline API, which we explored in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    enables immediate use of models for common tasks, while lower-level interfaces
    provide fine-grained control for custom implementations. Also, the library’s modular
    design allows developers to mix and match components like tokenizers, models,
    and optimizers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者来说，transformers 提供了多个抽象级别。我们在 [第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)
    中探讨了的高级管道 API，使得模型可以立即用于常见任务，而较低级别的接口则提供了对自定义实现的精细控制。此外，库的模块化设计允许开发者混合和匹配分词器、模型和优化器等组件。
- en: Perhaps most powerfully, transformers emphasize transparency and reproducibility.
    All model implementations are open source, well documented, and accompanied by
    model cards describing their capabilities, limitations, and ethical considerations.
    It’s a wonderful learning process to crack open the transformers library on GitHub
    and explore the source code for common models like GPT and Gemma.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最有力的，transformers 强调透明性和可重复性。所有模型实现都是开源的，有良好的文档，并附有描述其功能、限制和伦理考虑的模型卡片。在 GitHub
    上打开 transformers 库并探索 GPT 和 Gemma 等常见模型的源代码是一个美妙的学习过程。
- en: This commitment to openness has made the transformers API an invaluable tool
    for you, and it’s something that’s well worth investing your time to learn!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对开放性的承诺使得 transformers API 成为您宝贵的工具，这是值得您投入时间去学习的！
- en: Getting Started with transformers
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 transformers
- en: 'In [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    we explored how to access a model from the Hugging Face Hub, and we saw how easy
    it was to instantiate one and then use it with the various pipelines. We’ll go
    a little deeper into that in this chapter, but let’s begin by installing the requisite
    libraries:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)
    中，我们探讨了如何从 Hugging Face Hub 访问模型，并看到了如何轻松地实例化一个模型然后使用各种管道来使用它。在本章中，我们将更深入地探讨这一点，但让我们首先安装所需的库：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Many of the models in the Hugging Face Hub, which are accessible via transformers,
    need you to have an authentication token on Hugging Face, along with permission
    to use that model. In [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    we showed you how to get an access token. After that, depending on the model you
    choose to use, you may need to ask permission on its landing page if you want
    access to it. You also learned how to use the transformers library in Google Colab,
    but there are, of course, many other ways to use transformers other than in Colab!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub 中的许多模型，通过 transformers 可访问，需要您在 Hugging Face 上拥有一个身份验证令牌，以及使用该模型的权限。在
    [第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797) 中，我们向您展示了如何获取访问令牌。之后，根据您选择的模型，如果您想访问它，可能需要在它的着陆页上请求权限。您还学习了如何在
    Google Colab 中使用 transformers 库，但当然，除了 Colab 之外，还有许多其他使用 transformers 的方法！
- en: 'Once you have a token, you can use it in your Python code like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了令牌，您就可以像这样在您的 Python 代码中使用它：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Or, if you prefer, you can set an environment variable:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您愿意，可以设置一个环境变量：
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With that, your development environment can now support development with transformers.
    Next, we’ll look at some core concepts within the library.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，您的发展环境现在可以支持使用 transformers 进行开发。接下来，我们将查看库中的某些核心概念。
- en: Core Concepts
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: There are a number of important core concepts of using transformers that you
    can take advantage of. The simplicity of the transformers design hides the core
    concepts from you until you need them, but let’s take a look at some of them here
    in a little more detail.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 transformers 有许多重要的核心概念，您可以加以利用。transformers 设计的简单性将核心概念隐藏起来，直到您需要它们，但让我们在这里更详细地看看其中的一些。
- en: We’ll start with the pipelines that you learned about in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从您在 [第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)
    中学到的管道开始。
- en: Pipelines
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: The `pipeline` class implements transformers’ most useful abstraction, with
    a goal of making complex transformer operations accessible with minimal code.
    You can get the core default functionality of the model by using the appropriate
    pipeline with its defaults, and you can also override the defaults to create custom
    functionality.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline` 类实现了转换器最有用的抽象，目标是使复杂的转换器操作可以通过最少的代码来访问。您可以通过使用具有默认值的适当管道来获取模型的核心默认功能，也可以覆盖默认值以创建自定义功能。'
- en: Pipelines encapsulate all of ML processing—from data preprocessing to model
    inference to result formatting—in a single method.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 管道封装了所有机器学习处理过程——从数据预处理到模型推理再到结果格式化——在一个单一的方法中。
- en: 'To see the power of pipelines in action, let’s look at an example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到管道的实际作用，让我们来看一个例子：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this case, we didn’t specify a model but rather a scenario (`sentiment-analysis`),
    and the pipeline configuration chose the default model for that scenario and did
    all of the initialization for us. As you saw in Chapters [5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and [6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    when you’re dealing with text, you need to tokenize and sequence it. You also
    need to know which tokenizer was used by a particular model so you can ensure
    that your text is encoded correctly and then converted to tensors in the correct,
    normalized format. You also need to parse and potentially detokenize the output,
    yet none of that code is present here. Instead, you simply say that you want sentiment
    analysis and then point the pipeline toward the text you want to classify.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们没有指定一个模型，而是指定了一个场景（`sentiment-analysis`），管道配置选择了该场景的默认模型，并为我们完成了所有初始化。正如你在第[5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)章和第[6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)章中看到的，当你处理文本时，你需要对其进行分词和序列化。你还需要知道特定模型使用的分词器，以确保你的文本被正确编码，然后转换为正确的、归一化的张量格式。你还需要解析输出，并可能进行去分词，但这里没有出现任何这样的代码。相反，你只需说明你想要进行情感分析，然后将管道指向你想要分类的文本。
- en: We’re not just limited to text sentiment analysis, of course. We can also do
    text classification, generation, summarization, and translation. Other scenarios
    include entity recognition and question answering. As more multimodal transformer-based
    models come online, there’s also image classification and segmentation, object
    detection, image generation, and audio scenarios including speech recognition
    and generation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于文本情感分析，当然。我们还可以进行文本分类、生成、摘要和翻译。其他场景包括实体识别和问答。随着更多多模态基于transformer的模型上线，还包括图像分类和分割、目标检测、图像生成以及包括语音识别和生成的音频场景。
- en: While there are default models for a scenario, like we saw just now, there’s
    also the ability to override the defaults and send custom parameters to a model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于某个场景有默认模型，就像我们刚才看到的，但也可以覆盖默认设置并向模型发送自定义参数。
- en: 'So, for example, with text generation, we can use the default experience like
    this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在文本生成中，我们可以使用默认体验如下：
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Or we can customize it further by passing parameters for things like the number
    of tokens to generate (`max_length`), the temperature (how creative it will be),
    and the number of tokens to evaluate when outputting a new one (`top_k`):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以进一步自定义它，通过传递参数来指定生成令牌的数量（`max_length`）、温度（这将有多有创意），以及输出新令牌时评估的令牌数量（`top_k`）：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This gives you the flexibility you need to have fine control of any particular
    model, meaning it lets you set your desired parameters to override the default
    behavior. Importantly, the pipeline’s abstraction handles the crucial steps of
    using the model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这为你提供了对任何特定模型进行精细控制的灵活性，这意味着它允许你设置所需的参数以覆盖默认行为。重要的是，管道的抽象处理了使用模型的关键步骤。
- en: 'To reiterate: when you’re using a pipeline, you’re getting more than just the
    model download. Depending on your scenario, you’ll typically get the following
    steps:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调：当你使用管道时，你得到的不仅仅是模型下载。根据你的场景，你通常会得到以下步骤：
- en: Model loading
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 模型加载
- en: When you specify a model name, the pipeline API automatically downloads and
    caches the model and any associated tools, such as the tokenizer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你指定一个模型名称时，管道API会自动下载并缓存模型及其相关工具，如分词器。
- en: Preprocessing
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理
- en: This takes your raw inputs in their typical data format (strings, bitmaps, wave
    files, etc.) and turns them into model-compatible formats, even when multiple
    steps are needed. For example, it can tokenize a string and then turn the resulting
    tokens into embeddings or tensors.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将你的原始输入（通常是字符串、位图、波形文件等）转换为模型兼容的格式，即使需要多个步骤。例如，它可以对字符串进行分词，然后将生成的令牌转换为嵌入或张量。
- en: Tokenization
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 分词
- en: As mentioned previously, this helps you with the specific tokenization strategy
    that a particular model needs. One tokenization scheme does not fit all!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这有助于你获得特定模型所需的特定分词策略。一种分词方案并不适合所有情况！
- en: Batching
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理
- en: This abstracts away the need for you to calculate optimal batch sizes. It will
    efficiently handle batching for you while respecting memory constraints.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这抽象掉了你需要计算最佳批量大小的需求。它将高效地为你处理批处理，同时尊重内存限制。
- en: Post-processing
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理
- en: Models output tensors of probabilities, and the pipeline will turn them into
    a human-readable format that you or your code can work with.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出概率张量，流水线将它们转换为人类可读的格式，你可以或你的代码可以与之工作。
- en: Now that we’ve had a quick look at pipelines and what they are (and you’ll be
    using them a lot in this book), let’s continue our tour of the core concepts and
    switch to tokenizers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经快速浏览了流水线及其内容（你将在本书中大量使用它们），让我们继续我们的核心概念之旅，转向分词器。
- en: Tokenizers
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器
- en: We’ve spoken about tokenizers a lot, and we’ve even built our own in [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    But prebuilt tokenizers can be very powerful and useful tools as you create your
    own apps. Hugging Face transformers give you the `AutoTokenizer` class, and they
    make your life a lot easier when you’re dealing with tokenizers by handling many
    of the complex scenarios for you. Even if you are creating your own models and
    training them from scratch, using an AutoTokenizer is probably a much smarter
    way of handling this task, rather than rolling your own.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了很多关于分词器的内容，我们甚至在[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)和[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中构建了自己的分词器。但是，预构建的分词器可以作为你创建自己的应用程序时非常强大和有用的工具。Hugging
    Face transformers提供了`AutoTokenizer`类，当你处理分词器时，它们为你处理了许多复杂的场景，使你的生活变得更加容易。即使你正在创建自己的模型并从头开始训练它们，使用AutoTokenizer可能也是处理这项任务的一种更明智的方式，而不是自己从头开始。
- en: To go a little deeper, the tokenizer is fundamental to how transformer models
    will process text. It’s the first step in converting your raw text into a format
    that the model can work with. It’s also often overlooked in discussions of building
    models, and that’s a big mistake. The tokenization strategy is vital in the design
    of any system, and a badly designed one can negatively impact your overall model
    performance. Therefore, it’s important to have a well-designed tokenizer for the
    task at hand.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入一点，分词器对于transformer模型如何处理文本是至关重要的。它是将原始文本转换为模型可以处理格式的第一步。它也常常在构建模型的讨论中被忽视，这是一个很大的错误。分词策略在任何系统设计中都是至关重要的，一个设计不良的分词器可能会对你的整体模型性能产生负面影响。因此，对于手头的任务，拥有一个设计良好的分词器是非常重要的。
- en: At its core, the tokenizer’s job is to break down text into smaller, numeric
    units called tokens. They can be words, parts of words (aka *subwords*), or even
    individual characters. To choose the right tokenization strategy, you’ll need
    to make trade-offs among vocabulary size, the length of the sequences of tokens
    you will use in the model architecture, and your desire and requirements to handle
    words that are uncommon.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，分词器的任务是分解文本为更小的、称为标记的数值单元。它们可以是单词、单词的部分（即*子词*），甚至是单个字符。为了选择正确的分词策略，你需要在词汇量大小、你将在模型架构中使用的标记序列长度以及你处理不常见单词的愿望和需求之间做出权衡。
- en: The transformers library supports multiple approaches, with subword tokenization
    being the most common one. It’s a nice balance between character- and word-level
    tokenization that allows for less frequent words to still be in the corpus, because
    they’re made of more common subwords. For example, the word *antidisestablishmentarianism*
    isn’t a frequently used term, but it is made up of the letter combinations *anti*,
    *di*s, *est*, *ab*, *lish*, *ment*, *ari*, *an,* and *ism*, which are! It’s also
    a fun word to use with AI models that interpret your speech, to see if it can
    complete the word before you do!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库支持多种方法，其中子词分词是最常见的一种。它在字符级和词级分词之间提供了一个很好的平衡，允许不常见的单词仍然存在于语料库中，因为它们由更常见的子词组成。例如，单词*antidisestablishmentarianism*不是一个常用术语，但它由字母组合*anti*、*di*s、*est*、*ab*、*lish*、*ment*、*ari*、*an,*
    和 *ism* 组成，这些都是！它也是一个与AI模型一起使用的好词，这些模型可以解释你的语音，看看它是否能在你之前完成单词！
- en: As you can see, this can give you terrific *vocabulary efficiency*, which is
    the ability to maintain a manageable vocabulary size while still being able to
    capture meaningful semantics. It can also handle *out-of-vocabulary* effectively,
    which means being effective and handling unseen words by breaking them down into
    known subwords.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这可以提供极好的*词汇效率*，即在保持可管理词汇量的同时，仍然能够捕捉到有意义的语义。它还可以有效地处理*词汇表外的*词，这意味着通过将它们分解成已知的子词来处理未见过的词。
- en: Here’s a really interesting example of this. In the very early days of transformers
    (pre-GPT), I worked on a project in which I created a transformer that I trained
    with the scripts of the TV show *Stargate,* and then I worked with the producers
    and actors on the show to do a table read of an AI-generated script. Given that
    the TV show is science fiction, with a lot of made-up words (aka technobabble),
    I used a subtoken tokenizer, following the logic that it could make up new words
    too. However, it ended up getting a little too creative! You can see the actors
    struggling with the new words in [this video of the table read](https://oreil.ly/A42Ko).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个非常有趣的例子。在transformers的早期（GPT之前），我参与了一个项目，我创建了一个使用电视剧《星际之门》的脚本进行训练的transformer，然后我与该节目的制作人和演员一起对AI生成的剧本进行了朗读。鉴于该电视剧是科幻剧，有很多虚构的词（即技术术语），我使用了一个子标记分词器，按照它可以创造新词的逻辑。然而，它最终变得有点过于创意了！你可以在[这个朗读视频](https://oreil.ly/A42Ko)中看到演员们对新词的挣扎。
- en: So now, let’s explore some common tokenizers in the ecosystem and see how they
    work. Note that tokenizers are associated with their model type, so when you explore
    models in the Hugging Face Hub (see [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)),
    you’ll be able to find their tokenizer. You must use the correct tokenizer with
    each model, or it won’t be able to understand your input. Depending on the licenses
    associated with the tokenizers, you could also use them to train or tune your
    own models, instead of rolling your own as we did in Chapters [4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)
    and [5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索生态系统中的一些常见分词器，看看它们是如何工作的。请注意，分词器与它们的模型类型相关联，所以当你探索Hugging Face Hub中的模型（见第14章[14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)），你将能够找到它们的分词器。你必须使用与每个模型相应的正确分词器，否则它将无法理解你的输入。根据与分词器相关的许可证，你也可以使用它们来训练或调整你自己的模型，而不是像我们在第4章[4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)和第5章[5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中所做的那样从头开始构建。
- en: The WordPiece tokenizer
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WordPiece分词器
- en: The WordPiece tokenizer, which is associated with the BERT model, is a common
    tokenizer that’s highly efficient at managing subwords. It starts with a basic
    vocabulary and then iteratively adds the most frequent combinations it sees. The
    subwords are denoted by *##*. While generally created for English, it also works
    well for similar languages that have clear word boundaries denoted by spaces and
    other punctuation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT模型关联的WordPiece分词器是一种常见的分词器，它在管理子词方面非常高效。它从一个基本词汇表开始，然后迭代地添加它看到的频率最高的组合。子词由*##*表示。虽然它主要是为英语设计的，但它也适用于有清晰词边界（由空格和其他标点符号表示）的类似语言。
- en: 'So now, let’s consider an example sentence that contains complex words:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个包含复杂词的示例句子：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To load the tokenizer, you instantiate an instance of AutoTokenizer and call
    the `from_pretrained` method, passing the name of the tokenizer. For Bert’s WordPiece,
    you can use `bert-base-uncased` as the tokenizer name:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载分词器，你需要实例化一个AutoTokenizer对象并调用`from_pretrained`方法，传入分词器的名称。对于BERT的WordPiece，你可以使用`bert-base-uncased`作为分词器名称：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, to tokenize the text, all you need to do is call the `tokenize` method
    and pass it your string. This will output the list of tokens:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了对文本进行分词，你只需要调用`tokenize`方法并传入你的字符串。这将输出一个包含标记的列表：
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will then output the list of tokens from the sentence, which looks like
    this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出句子中的标记列表，看起来像这样：
- en: '[PRE26]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Long words that aren’t commonly used (like *marathoner* and *immunohistochemistry*)
    are broken into subwords, whereas others (like *conference* and *neuroscience*)
    are kept as whole words. This is based on the training corpus used in BERT, and
    the decisions about which ones are common enough and which ones are not (for example,
    I would have expected *qualified* to be quite common) were made by the original
    researcher.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不常用的长词（如 *marathoner* 和 *immunohistochemistry*）会被拆分成子词，而其他词（如 *conference* 和
    *neuroscience*）则保持为整个单词。这是基于BERT使用的训练语料库，关于哪些词足够常见以及哪些词不够常见（例如，我原本预期 *qualified*
    应该相当常见）的决定是由原始研究者做出的。
- en: 'If you want to see the IDs for these tokens, you can get them with the encode
    method, like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要查看这些令牌的ID，你可以使用encode方法来获取，如下所示：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The list of IDs is shown here:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ID列表如下所示：
- en: '[PRE28]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that the first and last tokens are `101` and `102`, which are special tokens
    for the start and end of the sentence that the tokenizer inserted and which are
    expected by the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一个和最后一个令牌是 `101` 和 `102`，这是分词器插入的特殊令牌，用于句子的开始和结束，模型期望这些令牌。
- en: 'Now, say that you decode the list of token IDs back into a string, like this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你将令牌ID列表解码回字符串，如下所示：
- en: '[PRE29]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, you’ll see how the sentence has these special tokens inserted:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你会看到句子中插入了这些特殊令牌：
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: I would recommend that you continue to experiment with the tokenizer to understand
    how it manages text by turning it into tokens and special characters. Having this
    knowledge is often important when you’re debugging model behavior or if you’re
    doing some kind of fine-tuning to manage how your vocabulary will be used.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你继续实验tokenizer，以了解它是如何通过将其转换为令牌和特殊字符来管理文本的。当你调试模型行为或进行某种微调以管理词汇的使用时，这种知识通常很重要。
- en: Byte-pair encoding
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字节对编码
- en: The GPT family uses a format called *byte-pair encoding* (BPE), which is a data
    compression algorithm as well as a tokenization one. It starts with the vocabulary
    of individual characters, which progressively learns common byte or character
    pairs to merge into new tokens.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: GPT系列使用一种称为 *字节对编码*（BPE）的格式，它是一种数据压缩算法，也是一种分词算法。它从单个字符的词汇表开始，逐渐学习常见的字节或字符对，将它们合并成新的令牌。
- en: The algorithm initially splits the training corpus into characters, assigning
    tokens to each. It then iteratively merges the most frequent pairs into new tokens,
    adding them to the vocabulary. The process continues for a predetermined number
    of merges. So, for example, over time, common patterns in words become their own
    tokens. This tends to veer toward the beginnings of the words with common prefixes
    (like *inter*) or the ends of words with common suffixes (like *er*) ending up
    having their own token. Instead of using the `##` string to determine the beginning
    of a subword, BPE uses a special character (usually *Ġ*).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 算法最初将训练语料库拆分为字符，并为每个字符分配令牌。然后，它迭代地将最频繁出现的对合并成新的令牌，并将它们添加到词汇表中。这个过程会持续进行预定的合并次数。例如，随着时间的推移，单词中的常见模式会变成它们自己的令牌。这往往倾向于具有常见前缀（如
    *inter*）的单词的开头或具有常见后缀（如 *er*）的单词的结尾，最终拥有它们自己的令牌。BPE不是使用 `##` 字符串来确定子词的开始，而是使用一个特殊字符（通常是
    *Ġ*）。
- en: 'Here’s the code you can use to tokenize the same sentence. You’ll use the `gpt2`
    AutoTokenizer from OpenAI:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你可以用来对相同句子进行分词的代码。你将使用来自OpenAI的`gpt2` AutoTokenizer：
- en: '[PRE31]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is shown here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE32]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Over time, you’ll see that the splits will be slightly different, which reflects
    the difference between the training sets. BERT was trained on Wikipedia and the
    Toronto BookCorpus, while GPT-2 was trained on web text.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，你会发现拆分会有细微的差异，这反映了训练集之间的差异。BERT是在维基百科和Toronto BookCorpus上训练的，而GPT-2是在网络文本上训练的。
- en: SentencePiece
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SentencePiece
- en: SentencePiece, which is used by the T5 model, is a unique tokenizer. It treats
    all input text as a raw sequence of Unicode characters, which gives it strong
    support for non-English languages. As part of this, it treats whitespaces like
    any other characters. That makes it effective for languages like Japanese and
    Chinese that don’t always have clear word boundaries, and it also removes the
    need for language-specific preprocessing. In fact, while it was being built, this
    tokenizer learned its subword units directly from raw sentences in multiple languages.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: SentencePiece，T5 模型所使用的独特分词器。它将所有输入文本视为原始的 Unicode 字符序列，这为非英语语言提供了强大的支持。作为其中的一部分，它将空白符视为其他任何字符。这使得它对像日语和中文这样的语言非常有效，这些语言并不总是有清晰的单词边界，并且它还消除了对特定语言预处理的需求。实际上，在构建过程中，这个分词器直接从多种语言的原始句子中学习其子词单元。
- en: 'Here’s how to use it:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何使用它的方法：
- en: '[PRE33]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This produces the following set of tokens:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下一组标记：
- en: '[PRE34]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As mentioned, where it’s particularly powerful is with non-English languages.
    So, for example, consider this code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，它在非英语语言中尤其强大。例如，考虑以下代码：
- en: '[PRE35]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It will output the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出以下内容：
- en: '[PRE36]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note how the Japanese characters for Tokyo were split into multiple tokens and
    the numbers were kept whole (i.e., `123` and `45`).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，东京的日语字符被分割成多个标记，而数字保持完整（即 `123` 和 `45`）。
- en: Given that transformers were initially designed to improve machine translation,
    you can see that SentencePiece, which predates generative AI like GPT, was designed
    with internationalization in mind!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Transformer 最初是为了改进机器翻译而设计的，你可以看到，在 GPT 等生成式 AI 之前出现的 SentencePiece，在设计时就已经考虑了国际化！
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at Transformers, the architecture that underpins
    modern LLMs, and transformers, the library from Hugging Face that makes Transformers
    easy to use.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 Transformer 架构，它是现代大型语言模型的基础，以及 Hugging Face 的 Transformer 库，它使得
    Transformer 的使用变得简单易行。
- en: We explored how the original Transformer architecture revolutionized AI through
    its use of attention mechanisms, in which context vectors for words were amended
    based on learned details of where the sequence might be paying appropriate attention
    to other parts of the sequence. We also looked at encoders that excel at artificial
    understanding of text, decoders that can intelligently generate text, and encoder-decoders
    that bring the best of both for sequence-to-sequence models. We also double-clicked
    into their architecture to understand how the mechanisms such as attention, feedforward,
    normalization, and many other parts of the architecture work.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了原始 Transformer 架构如何通过使用注意力机制来革命性地改变 AI，在这些机制中，基于学习到的细节，单词的上下文向量会根据序列可能适当关注的其他部分进行修改。我们还研究了擅长人工理解文本的编码器，能够智能生成文本的解码器，以及将两者优点结合起来的序列到序列模型。我们还深入研究了它们的架构，以了解注意力、前馈、归一化等机制以及架构的其他部分是如何工作的。
- en: We then looked into transformers, which form the library from Hugging Face that
    makes downloading and instantiation of Transformer-based models (including the
    entire inference pipeline) very easy. There’s a whole lot more still in there,
    and hopefully, this gave you a good head start.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们研究了 Transformer，它构成了 Hugging Face 库的一部分，使得下载和实例化基于 Transformer 的模型（包括整个推理流程）变得非常简单。其中还有很多其他内容，希望这为你提供了一个良好的起点。
- en: In the next chapter, you’re going to go a little further and explore how to
    adapt LLM models to your specific needs, taking custom data and using it to fine-tune
    or prompt tune models to make them work for your specific use cases. Get ready
    to turn theory into practice!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将更进一步，探索如何将 LLM 模型适应你的特定需求，使用自定义数据微调或提示调整模型，使其适用于你的特定用例。准备好将理论转化为实践吧！
