- en: 9 Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 自动编码器
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing autoencoders
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的介绍
- en: Training of autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的训练
- en: Types of autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的类型
- en: Python code using TensorFlow and Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow和Keras的Python代码
- en: Out of intense complexities, intense simplicities emerge.—Winston Churchill
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从复杂的混乱中，简单的真理浮现出来。——温斯顿·丘吉尔
- en: In the preceding chapter, we explored the concepts of deep learning. In this
    chapter, we start with unsupervised deep learning. Autoencoders are the very first
    topic. We will first cover the basics of autoencoders, what are they, and how
    we train them. We then get into the different types of autoencoders followed by
    a Python code on the implementation. Welcome to the ninth chapter, and all the
    very best!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了深度学习的概念。在这一章中，我们将从无监督深度学习开始。自动编码器是第一个主题。我们将首先介绍自动编码器的基础知识，它们是什么，以及我们如何训练它们。然后我们将探讨不同类型的自动编码器，并随后提供一段Python代码以展示其实现。欢迎来到第九章，祝大家一切顺利！
- en: 9.1 Technical toolkit
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 技术工具箱
- en: 'We will continue to use the same version of Python and Jupyter Notebook as
    we have used so far. The codes and datasets used in this chapter have been checked
    in at the GitHub location. You need to install a couple of Python libraries in
    this chapter: `tensorflow` and `keras`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止所使用的相同版本的Python和Jupyter Notebook。本章中使用的代码和数据集已在GitHub位置进行检查。您需要在本章中安装几个Python库：`tensorflow`和`keras`。
- en: 9.2 Feature learning
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 特征学习
- en: Predictive modeling is quite an interesting topic. Across various domains and
    business functions, predictive modeling is used for various purposes like predicting
    the sales for a business in the next year, the amount of rainfall expected, whether
    the incoming credit card transaction is fraud or not, whether the customer will
    make a purchase or not, and so on. The use cases are many, and all the aforementioned
    use cases fall under supervised learning algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模是一个非常有趣的话题。在各个领域和商业功能中，预测建模被用于各种目的，如预测下一年企业的销售额、预期的降雨量、即将到来的信用卡交易是否为欺诈、客户是否会进行购买等。用例很多，上述所有用例都属于监督学习算法。
- en: NOTE  The datasets that we use have variables or attributes. They are also called
    characteristics or features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们使用的数据集具有变量或属性。它们也被称为特征或属性。
- en: While we wish to create these predictive models, we are also interested in understanding
    the variables that are useful for making the prediction. Let’s consider a case
    where a bank wants to predict if an incoming transaction is fraudulent or not.
    In such a scenario, the bank will wish to know which factors are significant to
    identify an incoming transaction as fraud. Factors that might be considered include
    the amount of the transaction, the time of the transaction, the origin/source
    of the transaction, etc. The variables that are important for making a prediction
    are called *significant variables*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望创建这些预测模型时，我们也对理解对预测有用的变量感兴趣。让我们考虑一个案例，其中一家银行想要预测一笔即将到来的交易是否为欺诈。在这种情况下，银行将希望知道哪些因素对于识别一笔即将到来的交易是否为欺诈是重要的。可能考虑的因素包括交易金额、交易时间、交易的来源/来源等。对于做出预测的重要变量被称为*显著变量*。
- en: To create a machine learning–based predictive model, *feature engineering* is
    used. Feature engineering, otherwise known as feature extraction, is the process
    of extracting features from the raw data to improve the overall quality of the
    model and enhance the accuracy as compared to a model where only raw data is fed
    to the machine learning model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建基于机器学习的预测模型，需要使用*特征工程*。特征工程，也称为特征提取，是从原始数据中提取特征以改善模型整体质量并提高准确性的过程，与仅向机器学习模型提供原始数据的模型相比。
- en: Feature engineering can be done using domain understanding, various manual methods,
    and a few automated methods too. One such method is known as feature learning.
    Feature learning is the set of techniques that help a solution automatically discover
    the representations required for feature detection. With the help of feature learning,
    manual feature engineering is not required. The effect of feature learning is
    much more relevant for datasets where images, text, audio, and video are being
    used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以通过领域理解、各种手动方法以及一些自动化方法来完成。其中一种方法被称为特征学习。特征学习是一组帮助解决方案自动发现特征检测所需表示的技术。借助特征学习，不需要手动特征工程。特征学习的效果对于使用图像、文本、音频和视频的数据集来说更为相关。
- en: Feature learning can be both supervised and unsupervised. For supervised feature
    learning, neural networks are the best example. For unsupervised feature learning,
    we have examples like matrix factorization, clustering algorithms, and autoencoders.
    We have already covered clustering and matrix factorization. In this chapter,
    we start with an introduction to autoencoders.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习可以是监督的也可以是无监督的。对于监督特征学习，神经网络是最好的例子。对于无监督特征学习，我们有矩阵分解、聚类算法和自编码器等例子。我们已经涵盖了聚类和矩阵分解。在本章中，我们从自编码器的介绍开始。
- en: 9.3 Introducing autoencoders
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 自编码器的介绍
- en: When we start with any data science problem, data plays the most significant
    role. A dataset that has a lot of noise is one of the biggest challenges in data
    science and machine learning. There are quite a few solutions available now, and
    autoencoders are one of them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始处理任何数据科学问题时，数据扮演着最显著的角色。一个包含大量噪声的数据集是数据科学和机器学习中最具挑战性的问题之一。现在有相当多的解决方案，其中之一就是自编码器。
- en: Simply put, an autoencoder is a type of artificial neural network, and it is
    used to learn the data encodings. Autoencoders are typically used for dimensionality
    reduction methods. They can also be used as generative models, which can create
    synthetic data that is like the old data. For example, if we do not have a good
    amount of data to train machine learning, we can use generated synthetic data
    to train the models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，自编码器是一种人工神经网络，用于学习数据编码。自编码器通常用于降维方法。它们也可以用作生成模型，可以创建类似于旧数据的合成数据。例如，如果我们没有足够的数据来训练机器学习，我们可以使用生成的合成数据来训练模型。
- en: Autoencoders are feed-forward neural networks, and they compress the input into
    a lower dimensional code and then try to reconstruct the output from this representation.
    The objective of an autoencoder is to learn the lower dimensional representation
    (also sometimes known as encoding) for a high-dimensional dataset. Recall from
    the previous chapters principal component analysis (PCA). Autoencoders can be
    thought of as a generalization for PCA. PCA is a linear method whereas autoencoders
    can learn nonlinear relationships as well. Hence, autoencoders are required for
    dimensionality reduction solutions wherein they capture the most significant attributes
    from the input data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是前馈神经网络，它们将输入压缩成一个低维代码，然后尝试从这个表示中重建输出。自编码器的目标是学习高维数据集的低维表示（有时也称为编码）。回想一下前几章中的主成分分析（PCA）。自编码器可以被视为PCA的推广。PCA是一种线性方法，而自编码器可以学习非线性关系。因此，自编码器对于需要捕获输入数据中最显著属性的低维化解决方案是必需的。
- en: 9.4 Components of autoencoders
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 自编码器的组成部分
- en: 'The architecture of an autoencoder is quite simple to understand. An autoencoder
    consists of three parts: an encoder, a bottleneck or a code, and a decoder, as
    shown in figure 9.1\. In simple terms, an encoder compresses the input data, a
    bottleneck or code contains this compressed information, and the decoder decompresses
    the knowledge and hence reconstructs this data back to its original form. Once
    the decompression has been done and the data has been reconstructed to its encoded
    form, the input and output can be compared.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的架构非常简单易懂。自编码器由三部分组成：编码器、瓶颈或代码和解码器，如图9.1所示。简单来说，编码器压缩输入数据，瓶颈或代码包含这些压缩信息，解码器解压缩知识，从而将数据重建回其原始形式。一旦完成解压缩并将数据重建为编码形式，就可以比较输入和输出。
- en: '![figure](../Images/CH09_F01_Verdhan.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH09_F01_Verdhan.png)'
- en: Figure 9.1 Structure of an autoencoder with an encoder, a bottleneck, and a
    decoder
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 自编码器的结构，包括编码器、瓶颈和解码器
- en: 'Let’s study these components in more detail:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地研究这些组件：
- en: '*Encoder*—The input data passes through the encoder. An encoder is nothing
    but a fully connected artificial neural network. It compresses the input data
    into an encoded representation, and in the process the output generated is reduced
    in size. An encoder compresses the input data into a compressed module known as
    a bottleneck.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器*—输入数据通过编码器。编码器不过是一个全连接的人工神经网络。它将输入数据压缩成一个编码表示，在这个过程中生成的输出被减小了尺寸。编码器将输入数据压缩成一个称为瓶颈的压缩模块。'
- en: '*Bottleneck*—The bottleneck can be considered the brain of the encoder. It
    contains the compressed information representations, and it is the job of the
    bottleneck to allow only the most important information to pass through.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*窄带*—窄带可以被认为是编码器的“大脑”。它包含压缩的信息表示，并且窄带的任务是只允许最重要的信息通过。'
- en: '*Decoder*—The information received from the bottleneck is decompressed by a
    decoder. It re-creates the data back to its original or encoded form. Once the
    job of the decoder is done, the actual values are compared with the decompressed
    values created by the decoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解码器*—从窄带接收到的信息由解码器解压缩。它将数据重新创建成其原始或编码形式。一旦解码器的任务完成，实际值与解码器创建的解压缩值进行比较。'
- en: 'There are a few important points about autoencoders to consider:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自动编码器有几个重要点需要考虑：
- en: There is a loss of information in autoencoders when the decompression is done
    as compared to the original inputs. So when the compressed data is decompressed,
    there is a loss as compared to the original data.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始输入相比，在自动编码器中，当进行解压缩时会有信息损失。因此，当压缩数据被解压缩时，与原始数据相比会有损失。
- en: Autoencoders are specific to datasets. This means that an algorithm that is
    trained on images of flowers will not work on images of traffic signals and vice
    versa. This is because the features the autoencoder learned will be specific to
    flowers only. So we can say that autoencoders are only able to compress the data
    similar to the one used for training.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器是针对数据集特定的。这意味着在花卉图像上训练的算法不会在交通信号图像上工作，反之亦然。这是因为自动编码器学习的特征将仅限于花卉。因此，我们可以说自动编码器只能压缩与训练数据相似的数据。
- en: It is relatively easier to train specialized instances of algorithms to perform
    well on specific types of inputs. We just need representative training datasets
    to train the autoencoder.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练专门实例的算法以在特定类型的输入上表现良好相对容易。我们只需要代表性的训练数据集来训练自动编码器。
- en: 9.5 Training of autoencoders
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 自动编码器的训练
- en: It is important to note that if there is no correlation between the variables
    in the data, then it is really difficult to compress and subsequently decompress
    the input data. For us to create a meaningful solution, there should be some level
    of relationship or correlation between the variables in the input data. To create
    an autoencoder, we require an encoding method, a decoding method, and a loss function
    to compare the actual versus decompressed values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果数据中的变量之间没有相关性，那么压缩和随后解压缩输入数据将非常困难。为了创建一个有意义的解决方案，输入数据中的变量之间应该存在某种程度的关系或相关性。要创建自动编码器，我们需要编码方法、解码方法以及一个损失函数来比较实际值和解压缩值。
- en: 'The process is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: The input data passes through the encoder module.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据通过编码器模块。
- en: The encoder compresses the input of a model into a compact bottleneck.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器将模型的输入压缩成一个紧凑的窄带。
- en: The bottleneck restricts the flow of information and allows only important information
    to pass through; hence, a bottleneck is sometimes referred to as *knowledge-representation*.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 窄带限制了信息的流动，只允许重要信息通过；因此，窄带有时被称为*知识表示*。
- en: The decoder decompresses the information and re-creates the data back to its
    original or encoded form. This encoder-decoder architecture is quite efficient
    in getting the most significant attributes from the input data.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器解压缩信息，并将数据重新创建成其原始或编码形式。这种编码器-解码器架构在从输入数据中获取最重要的属性方面非常高效。
- en: The objective of the solution is to generate an output identical to the input.
    Generally, the decoder architecture is a mirror image of the coder architecture.
    This is not mandatory but is generally followed. We ensure that the dimensionality
    of the input and outputs are the same.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的目标是生成与输入相同的输出。通常，解码器架构是编码器架构的镜像。这并非强制性的，但通常是遵循的。我们确保输入和输出的维度相同。
- en: NOTE  If you do not know the meaning of hyperparameter, refer to the appendix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您不知道超参数的含义，请参阅附录。
- en: 'We need to define four hyperparameters for training an autoencoder:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义四个超参数来训练自动编码器：
- en: '*Code size*—This is perhaps the most significant hyperparameter. It represents
    the number of nodes in the middle layer. This decides the compression of the data
    and can also act as a regularization term. The less the value of code size, the
    more compressed the data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码大小*—这可能是最重要的超参数。它表示中间层的节点数。这决定了数据的压缩程度，也可以作为正则化项。代码大小的值越小，数据就越压缩。'
- en: '*Parameter*—This denotes the depth of the autoencoder. A model that has more
    depth is obviously more complex and will have a longer processing time.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参数*—这表示自动编码器的深度。具有更多深度的模型显然更复杂，并且处理时间会更长。'
- en: '*Number of nodes per layer*—This is the weight used per layer. It generally
    decreases with every subsequent layer as the input becomes smaller across the
    layers. It increases back in the decoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每层的节点数*—这是每层使用的权重。随着输入在层间变得越来越小，它通常在后续的每一层中都会减少。在解码器中又重新增加。'
- en: '*Loss function used*—If the input values are in the [0,1] range, binary cross-entropy
    is preferred; otherwise, mean squared error is used.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用的损失函数*—如果输入值在[0,1]范围内，则首选二元交叉熵；否则，使用均方误差。'
- en: We have covered the hyperparameters used in training autoencoders. The training
    process is similar to backpropagation, which we have already covered.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了在训练自动编码器时使用的超参数。训练过程与反向传播类似，我们已经讨论过了。
- en: 9.6 Application of autoencoders
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 自动编码器的应用
- en: Autoencoders are capable of solving a number of problems inherent to unsupervised
    learning. Major applications for autoencoders include
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器能够解决无监督学习中的一些固有问题的解决方案。自动编码器的主要应用包括
- en: '*Dimensionality reduction*—Sometimes autoencoders can learn more complex data
    projections than PCA and other techniques.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降维*—有时自动编码器可以学习比PCA和其他技术更复杂的数据投影。'
- en: '*Anomaly detection*—The error or the reconstruction error (error between the
    actual data and the reconstructed data) can be used to detect the anomalies.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测*—错误或重建误差（实际数据与重建数据之间的误差）可以用来检测异常。'
- en: '*Data compression*—It is difficult to beat the basic solutions like JPEG by
    training the algorithm. Moreover, since autoencoders are data specific, they can
    use only the types of datasets they have been trained upon. If we wish to enhance
    the capacity to include more data types and make it more general, then the amount
    of the training data required will be too high, and obviously, the time required
    will be high too.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据压缩*—通过训练算法来击败像JPEG这样的基本解决方案是困难的。此外，由于自动编码器是数据特定的，它们只能使用它们被训练过的数据集类型。如果我们希望提高包括更多数据类型的容量并使其更通用，那么所需的训练数据量将非常高，显然，所需的时间也会很高。'
- en: '*Other applications*—These include drug discovery, machine translation, image
    denoising, etc.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*其他应用*—这些包括药物发现、机器翻译、图像去噪等。'
- en: There are still not a lot of practical implementations of autoencoders in the
    real world. This is due to a multitude of reasons like the nonavailability of
    datasets, infrastructure, readiness of various systems, etc.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，自动编码器的实际应用仍然不多。这有多种原因，如数据集不可用、基础设施、各种系统的准备就绪等。
- en: 9.7 Types of autoencoders
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 自动编码器的类型
- en: 'There are five main types of autoencoders. A brief description of the different
    types of encoders is given next. We have kept the section mathematically light
    and skipped the math behind the scenes as it is quite complex to understand. For
    curious readers, the papers listed in section 9.10 can explain the mathematics:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器主要有五种类型。接下来将简要介绍不同类型的编码器。我们尽量使这一部分数学简单，并跳过了背后的数学，因为它相当复杂，难以理解。对于好奇的读者，第9.10节中列出的论文可以解释数学原理：
- en: '*Undercomplete autoencoders*—An undercomplete autoencoder is the simplest form
    of an autoencoder. It simply takes an input dataset and then reconstructs the
    same dataset again from the compressed bottleneck region. By penalizing the neural
    network as per the reconstruction error, the model will learn the most significant
    attributes of the data. By learning the most important attributes, the model will
    be able to reconstruct the original data from the compressed state. As we know,
    there is a loss when the compressed data is reconstructed; this loss is called
    *reconstruction* loss.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欠完备自编码器*——欠完备自编码器是自编码器最简单的形式。它只是接受一个输入数据集，然后从压缩的瓶颈区域重新构建相同的数据集。通过根据重建误差惩罚神经网络，模型将学习数据的最显著属性。通过学习最重要的属性，模型将能够从压缩状态重建原始数据。正如我们所知，当压缩数据重建时会有损失；这种损失被称为*重建*损失。'
- en: Undercomplete autoencoders are unsupervised in nature as they do not have any
    target label to train. Such types of autoencoders are used for dimensionality
    reduction. Recall in chapter 2 we discussed dimensionality reduction (PCA), and
    in chapter 6, we discussed the advanced dimensionality reduction algorithms (t-distributed
    stochastic neighbor embedding and multidimensional scaling). See figure 9.2.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 欠完备自编码器本质上是无监督的，因为它们没有目标标签进行训练。这类自编码器用于降维。回想在第二章中我们讨论了降维（PCA），在第六章中我们讨论了高级降维算法（t分布随机邻域嵌入和多维尺度）。见图9.2。
- en: '![figure](../Images/CH09_F02_Verdhan.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F02_Verdhan.png)'
- en: Figure 9.2 The performance starts to improve with more dimensions but decreases
    after some time. The curse of dimensionality is a real problem when it comes to
    creating sound data science solutions.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 随着维度的增加，性能开始提高，但过了一段时间后开始下降。当涉及到创建良好的数据科学解决方案时，维度诅咒是一个真正的问题。
- en: Dimensionality reduction is possible using undercomplete autoencoders as the
    bottleneck is created, which is the compressed form of the input data. This compressed
    data can be decompressed back with the aid of the network. Recall in chapter 3
    we explained that PCA provides a linear combination of the input variables. For
    more details and to refresh your memory on PCA, please refer to chapter 3\. We
    know that PCA tries to get a low-dimensional hyperplane to describe the original
    dataset; undercomplete autoencoders can also learn nonlinear relationships. The
    difference is shown in figure 9.3.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欠完备自编码器作为瓶颈，可以降低维度，因为瓶颈是输入数据的压缩形式。这些压缩数据可以在网络的辅助下重新解压缩。回想在第三章中我们解释了PCA提供了输入变量的线性组合。有关更多细节和PCA的复习，请参阅第三章。我们知道PCA试图通过一个低维超平面来描述原始数据集；欠完备自编码器也可以学习非线性关系。这种差异在图9.3中显示。
- en: '![figure](../Images/CH09_F03_Verdhan.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F03_Verdhan.png)'
- en: Figure 9.3 PCA is linear in nature while autoencoders are nonlinear. This is
    the core difference between the two algorithms.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 PCA本质上是线性的，而自编码器是非线性的。这是两种算法之间的核心区别。
- en: Interestingly, if all the nonlinear activation functions are removed from the
    undercomplete autoencoder and only linear layers are used, the autoencoder is
    equivalent to a PCA only. To make the autoencoder generalize and not memorize
    the training data, an undercomplete autoencoder is regulated and fine-tuned by
    the size of the bottleneck. It allows the solution to not memorize the training
    data and generalize very well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果从欠完备自编码器中移除所有非线性激活函数，只使用线性层，那么自编码器就相当于PCA。为了使自编码器泛化并不过度记忆训练数据，欠完备自编码器通过瓶颈的大小进行调节和微调。这允许解决方案不记忆训练数据，并且泛化得非常好。
- en: NOTE  If a machine learning model works very well on the training data but does
    not work on the unseen test data, it is called overfitting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：如果一个机器学习模型在训练数据上表现非常好，但在未见过的测试数据上表现不佳，那么它被称为过拟合。
- en: '*Sparse autoencoders*—Sparse autoencoders are similar to undercomplete autoencoders
    except they use a different methodology to tackle overfitting. Conceptually, a
    sparse autoencoder changes the number of nodes at each of the hidden layer and
    keeps it flexible. Since it is not possible to have a neural network capable of
    a flexible number of neurons, the loss function is customized for it. In the loss
    function, a term is introduced that captures the number of activated neurons.
    The penalty term is proportional to the number of activated neurons. The higher
    the number of activated neurons, the higher the penalty. This penalty is called
    the *sparsity function*. Using the penalty, it is possible to reduce the number
    of activated neurons; hence the penalty is lower, and the network is able to tackle
    the problem of overfitting.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稀疏自编码器*——稀疏自编码器与欠完备自编码器类似，但它们使用不同的方法来处理过拟合问题。从概念上讲，稀疏自编码器在每个隐藏层改变节点的数量，并保持其灵活性。由于不可能有一个能够灵活调整神经元数量的神经网络，因此为它定制了损失函数。在损失函数中，引入了一个捕捉激活神经元数量的项。惩罚项与激活神经元数量成正比。激活神经元数量越多，惩罚越高。这个惩罚被称为*稀疏函数*。使用这个惩罚，可以减少激活神经元的数量；因此，惩罚较低，网络能够处理过拟合问题。'
- en: '*Contractive autoencoders*—Contractive autoencoders work on a similar concept
    as other autoencoders. They consider that the inputs that are quite similar should
    be encoded the same. Hence, they should have the same latent space representation.
    It means that there should not be much difference between the input data and the
    latent space.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*收缩自编码器*——收缩自编码器与其他自编码器的工作原理类似。它们认为非常相似的输入应该被编码成相同的。因此，它们应该有相同的潜在空间表示。这意味着输入数据和潜在空间之间不应该有太大的差异。'
- en: '*Denoizing autoencoders**—*Denoizing means removing the noise, and that is
    the precise task of denoizing autoencoders. They do not take an image as an input;
    instead they take a noisy version of an image as an input as shown in figure 9.4.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*去噪自编码器*——去噪意味着去除噪声，这正是去噪自编码器的精确任务。它们不接收图像作为输入；相反，它们接收如图9.4所示的图像的噪声版本作为输入。'
- en: '![figure](../Images/CH09_F04_Verdhan.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F04_Verdhan.png)'
- en: Figure 9.4 An original image, noisy output, and the outputs from the autoencoder
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4 原始图像、带噪声的输出和自编码器的输出
- en: The process of denoizing the autoencoder is depicted in figure 9.5\. The original
    image is changed by adding noise to it. This noisy image is fed to the encoder-decoder
    architecture and the output received is compared to the original image. The autoencoder
    learns the representation of the image, which is used to remove the noise; this
    is achieved by mapping the input image into a lower dimensional manifold.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器去噪的过程在图9.5中展示。原始图像通过添加噪声而改变。这个带噪声的图像被输入到编码器-解码器架构中，接收到的输出与原始图像进行比较。自编码器学习图像的表示，用于去除噪声；这是通过将输入图像映射到一个低维流形上实现的。
- en: '![figure](../Images/CH09_F05_Verdhan.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F05_Verdhan.png)'
- en: Figure 9.5 The process of denoizing in an autoencoder. It starts with the original
    image; noise is added, which results in a noisy image, and then it is fed to the
    autoencoder.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5 自编码器中的去噪过程。它从原始图像开始；添加噪声，结果产生一个带噪声的图像，然后将其输入到自编码器中。
- en: We can use denoizing autoencoders for nonlinear dimensionality reduction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用去噪自编码器进行非线性降维。
- en: '*Variational autoencoders*—A standard autoencoder model represents the input
    in a compressed form using the bottleneck. A variation is probabilistic generative
    models (usually Gaussian) over latent variables, which only need neural networks
    as a part of their overall structure. They are trained using expectation-maximization
    meta-algorithms. The mathematical details are beyond the scope of this book.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分自编码器*——标准自编码器模型使用瓶颈以压缩形式表示输入。变分是概率生成模型（通常是高斯分布）在潜在变量上，它们只需要神经网络作为其整体结构的一部分。它们使用期望最大化元算法进行训练。数学细节超出了本书的范围。'
- en: 9.8 Python implementation of autoencoders
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 自编码器的Python实现
- en: 'Let’s create two versions of an autoencoder. The code has been taken from the
    official source at the Keras website ([https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html))
    and has been modified for our usage. The steps are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个自编码器的版本。代码是从Keras网站（[https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html)）的官方来源中取出的，并为了我们的使用进行了修改。步骤如下：
- en: 'Import the necessary libraries:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Create our network architecture:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 创建我们的网络架构：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. Add more details to the model:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 向模型添加更多细节：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '4\. Load the datasets:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 加载数据集：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5\. Create the train and test the datasets:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 创建训练和测试数据集：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '6\. Fit the model (see figure 9.6):'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 拟合模型（见图9.6）：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![figure](../Images/CH09_F06_Verdhan.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F06_Verdhan.png)'
- en: Figure 9.6 Fitting the model
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6 模型拟合
- en: '7\. Test it on the test dataset:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 在测试数据集上测试它：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '8\. Plot the results. You can see the original image and final output (see
    figure 9.7):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 绘制结果。您可以看到原始图像和最终输出（见图9.7）：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![figure](../Images/CH09_F07_Verdhan.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F07_Verdhan.png)'
- en: Figure 9.7 The original image (bottom) and the final outcome (top)
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7 原始图像（底部）和最终结果（顶部）
- en: 9.9 Concluding thoughts
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 总结性思考
- en: Deep learning is a powerful tool. With a sound business problem and a quality
    dataset, we can create a lot of innovative solutions. Autoencoders are only one
    type of such solutions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个强大的工具。有了合理的企业问题和高质量的数据集，我们可以创造许多创新解决方案。自动编码器只是这类解决方案中的一种。
- en: In this chapter, we started with feature engineering, which allows us to extract
    the most significant features from a dataset. Then we moved to autoencoders. Autoencoders
    are a type of neural network only used to learn efficient coding of unlabeled
    datasets. Autoencoders can be applied to many business problems like facial recognition,
    anomaly detection, image recognition, drug discovery, machine translation, and
    so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从特征工程开始，这使我们能够从数据集中提取最重要的特征。然后我们转向自动编码器。自动编码器是一种仅用于学习未标记数据集有效编码的神经网络。自动编码器可以应用于许多商业问题，如人脸识别、异常检测、图像识别、药物发现、机器翻译等。
- en: 9.10 Practical next steps and suggested readings
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.10 实践步骤和推荐阅读
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一些下一步行动的建议和一些有用的阅读材料：
- en: Read the blog at [https://mng.bz/qxaw](https://mng.bz/qxaw).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读博客[https://mng.bz/qxaw](https://mng.bz/qxaw)。
- en: 'Study the following papers:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究以下论文：
- en: Hinton, G. E., Krizhevsky, A., and Wang, S. D. (2011). Transforming Auto-encoders.
    [https://mng.bz/7p99](https://mng.bz/7p99)
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G. E., Krizhevsky, A., and Wang, S. D. (2011). Transforming Auto-encoders.
    [https://mng.bz/7p99](https://mng.bz/7p99)
- en: Bank, D., Koenigstein, N., and Giryes, R. (2020). Autoencoders. [https://arxiv.org/abs/2003.05991](https://arxiv.org/abs/2003.05991)
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bank, D., Koenigstein, N., and Giryes, R. (2020). Autoencoders. [https://arxiv.org/abs/2003.05991](https://arxiv.org/abs/2003.05991)
- en: Michelucci, U. (2020). An Introduction to Autoencoders. [https://arxiv.org/abs/2201.03898](https://arxiv.org/abs/2201.03898)
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michelucci, U. (2020). An Introduction to Autoencoders. [https://arxiv.org/abs/2201.03898](https://arxiv.org/abs/2201.03898)
- en: See the good code and dataset available on the TensorFlow official page. [https://mng.bz/mGQr](https://mng.bz/mGQr).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参阅TensorFlow官方页面上的良好代码和数据集。[https://mng.bz/mGQr](https://mng.bz/mGQr)。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Predictive modeling is used in various domains to make future predictions using
    supervised learning algorithms.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测建模在各个领域使用，通过监督学习算法进行未来预测。
- en: Key aspects of predictive modeling involve identifying significant variables
    or features for accurate predictions.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测建模的关键方面包括识别对准确预测有重要影响的变量或特征。
- en: Feature engineering enhances model accuracy by extracting useful features from
    raw data.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程通过从原始数据中提取有用特征来提高模型精度。
- en: Feature learning automates feature detection, suitable for datasets like images,
    text, and audio.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征学习自动化特征检测，适用于图像、文本和音频等数据集。
- en: Autoencoders are a type of neural network used for data encoding, dimensionality
    reduction, and generating synthetic data.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器是一种用于数据编码、降维和生成合成数据的神经网络。
- en: The architecture of autoencoders includes encoder, bottleneck, and decoder components
    for data compression and reconstruction.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的架构包括编码器、瓶颈和解码器组件，用于数据压缩和重建。
- en: Autoencoders face information loss, are dataset-specific, and are suitable for
    precise applications.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器面临信息损失、数据集特定，适用于精确应用。
- en: Training autoencoders requires encoding, decoding, and defining hyperparameters
    such as code size and loss function.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练自动编码器需要编码、解码以及定义超参数，如代码大小和损失函数。
- en: Major applications include dimensionality reduction, anomaly detection, and
    data compression, among others.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要应用包括降维、异常检测和数据压缩等。
- en: Types of autoencoders include undercomplete, sparse, contractive, denoizing,
    and variational.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的类型包括欠完备、稀疏、收缩、去噪和变分。
- en: Sparse and contractive autoencoders address overfitting using different methodologies.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏和收缩自动编码器通过不同的方法来解决过拟合问题。
- en: A Python implementation of basic autoencoder architecture involves the Keras
    library for encoding and decoding data.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python实现基本自动编码器架构涉及使用Keras库进行编码和解码数据。
