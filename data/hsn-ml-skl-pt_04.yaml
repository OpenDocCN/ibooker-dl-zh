- en: Chapter 3\. Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 分类
- en: In [Chapter 1](ch01.html#landscape_chapter) I mentioned that the most common
    supervised learning tasks are regression (predicting values) and classification
    (predicting classes). In [Chapter 2](ch02.html#project_chapter) we explored a
    regression task, predicting housing values, using various algorithms such as linear
    regression, decision trees, and random forests (which will be explained in further
    detail in later chapters). Now we will turn our attention to classification systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第1章](ch01.html#landscape_chapter) 中，我提到最常见的监督学习任务是回归（预测值）和分类（预测类别）。在 [第2章](ch02.html#project_chapter)
    中，我们探讨了回归任务，使用各种算法（如线性回归、决策树和随机森林，这些将在后面的章节中详细解释）预测房价。现在，我们将把注意力转向分类系统。
- en: MNIST
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST
- en: 'In this chapter we will be using the MNIST dataset, which is a set of 70,000
    small images of digits handwritten by high school students and employees of the
    US Census Bureau. Each image is labeled with the digit it represents. This set
    has been studied so much that it is often called the “hello world” of machine
    learning: whenever people come up with a new classification algorithm they are
    curious to see how it will perform on MNIST, and anyone who learns machine learning
    tackles this dataset sooner or later.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用MNIST数据集，这是一个由美国人口普查局的高中生和员工手写的70,000个小数字图像集合。每个图像都标有它所代表的数字。这个集合已经被研究得如此之多，以至于它经常被称为机器学习的“hello
    world”：每当人们提出一个新的分类算法时，他们都会好奇它在MNIST上的表现如何，任何学习机器学习的人迟早都会处理这个数据集。
- en: Scikit-Learn provides many helper functions to download popular datasets. MNIST
    is one of them. The following code fetches the MNIST dataset from OpenML.org:⁠^([1](ch03.html#id1283))
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了许多辅助函数来下载流行的数据集。MNIST就是其中之一。以下代码从OpenML.org获取MNIST数据集：⁠^([1](ch03.html#id1283))
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `sklearn.datasets` package contains mostly three types of functions: `fetch_*`
    functions such as `fetch_openml()` to download real-life datasets, `load_*` functions
    to load small toy datasets bundled with Scikit-Learn (so they don’t need to be
    downloaded over the internet), and `make_*` functions to generate fake datasets,
    useful for tests. Generated datasets are usually returned as an `(X, y)` tuple
    containing the input data and the targets, both as NumPy arrays. Other datasets
    are returned as `sklearn.utils.Bunch` objects, which are dictionaries whose entries
    can also be accessed as attributes. They generally contain the following entries:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.datasets` 包主要包含三种类型的函数：`fetch_*` 函数，例如 `fetch_openml()` 用于下载真实数据集，`load_*`
    函数用于加载与Scikit-Learn捆绑的小型玩具数据集（因此不需要从互联网上下载），以及 `make_*` 函数用于生成假数据集，这对于测试很有用。生成的数据集通常以
    `(X, y)` 元组的形式返回，包含输入数据和目标，两者均为NumPy数组。其他数据集以 `sklearn.utils.Bunch` 对象的形式返回，这些对象是字典，其条目也可以作为属性访问。它们通常包含以下条目：'
- en: '`"DESCR"`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`"DESCR"`'
- en: A description of the dataset
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的描述
- en: '`"data"`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`"数据"`'
- en: The input data, usually as a 2D NumPy array
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据，通常是一个二维NumPy数组
- en: '`"target"`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`"目标"`'
- en: The labels, usually as a 1D NumPy array
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标签，通常是一个一维NumPy数组
- en: 'The `fetch_openml()` function is a bit unusual since by default it returns
    the inputs as a Pandas DataFrame and the labels as a Pandas Series (unless the
    dataset is sparse). But the MNIST dataset contains images, and DataFrames aren’t
    ideal for that, so it’s preferable to set `as_frame=False` to get the data as
    NumPy arrays instead. Let’s look at these arrays:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetch_openml()` 函数有点不寻常，因为它默认返回输入作为Pandas DataFrame，标签作为Pandas Series（除非数据集是稀疏的）。但MNIST数据集包含图像，DataFrame并不适合这种情况，因此最好设置
    `as_frame=False` 以获取NumPy数组形式的数据。让我们看看这些数组：'
- en: '[PRE1]`` `array([[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0, 0, 0],`  `[0,
    0, 0, ..., 0, 0, 0],`  `...,`  `[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0,
    0, 0],`  `[0, 0, 0, ..., 0, 0, 0]])` `>>>` `X``.``shape` [PRE2] `array([''5'',
    ''0'', ''4'', ..., ''4'', ''5'', ''6''], dtype=object)` `>>>` `y``.``shape` ``
    `(70000,)` `` [PRE3]` [PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]`` `array([[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0, 0, 0],`  `[0,
    0, 0, ..., 0, 0, 0],`  `...,`  `[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0,
    0, 0],`  `[0, 0, 0, ..., 0, 0, 0]])` `>>>` `X``.``shape` [PRE2] `array([''5'',
    ''0'', ''4'', ..., ''4'', ''5'', ''6''], dtype=object)` `>>>` `y``.``shape` ``
    `(70000,)` `` [PRE3]` [PRE4]'
- en: '[PRE5]` [PRE6] [PRE7]`py [PRE8]py[PRE9]py[PRE10]`py[PRE11]py` [PRE12]`py[PRE13]py[PRE14]py[PRE15]py[PRE16]py[PRE17]py[PRE18]`py[PRE19]py[PRE20]`
    [PRE21] >>> from sklearn.metrics import f1_score `>>>` `f1_score``(``y_train_5``,`
    `y_train_pred``)` `` `0.7325171197343846` `` [PRE22]` [PRE23][PRE24][PRE25][PRE26]``py[PRE27]py`
    ## The Precision/Recall Trade-Off    To understand this trade-off, let’s look
    at how the `SGDClassifier` makes its classification decisions. For each instance,
    it computes a score based on a *decision function*. If that score is greater than
    a threshold, it assigns the instance to the positive class; otherwise it assigns
    it to the negative class. [Figure 3-4](#decision_threshold_diagram) shows a few
    digits positioned from the lowest score on the left to the highest score on the
    right. Suppose the *decision threshold* is positioned at the central arrow (between
    the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold,
    and 1 false positive (actually a 6). Therefore, with that threshold, the precision
    is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so
    the recall is 67% (4 out of 6). If you raise the threshold (move it to the arrow
    on the right), the false positive (the 6) becomes a true negative, thereby increasing
    the precision (up to 100% in this case), but one true positive becomes a false
    negative, decreasing recall down to 50%. Conversely, lowering the threshold increases
    recall and reduces precision.  ![Diagram illustrating the precision/recall trade-off
    with different decision thresholds; as the threshold increases, precision generally
    increases while recall decreases.](assets/hmls_0304.png)  ###### Figure 3-4\.
    The precision/recall trade-off: images are ranked by their classifier score, and
    those above the chosen decision threshold are considered positive; the higher
    the threshold, the lower the recall, but (in general) the higher the precision    Instead
    of calling the classifier’s `predict()` method, you can call its `decision_function()`
    method, which returns a score for each instance. You can then use any threshold
    you want to make predictions based on those scores:    [PRE28]py`` `array([2164.22030239])`
    `>>>` `threshold` `=` `0` [PRE29]` [PRE30][PRE31][PRE32]``py[PRE33]`py` The `SGDClassifier`
    uses a threshold equal to 0, so the preceding code returns the same result as
    the `predict()` method (i.e., `True`). Let’s raise the threshold:    [PRE34]py
    `>>>` `y_some_digit_pred` `` `array([False])` `` [PRE35]py   [PRE36] [PRE37]``
    This confirms that raising the threshold decreases recall. The image actually
    represents a 5, and the classifier detects it when the threshold is 0, but it
    misses it when the threshold is increased to 3,000.    How do you decide which
    threshold to use? One option is to use the `cross_val_predict()` function to get
    the scores of all instances in the training set, but this time specify that you
    want to return decision scores instead of predictions:    [PRE38]    With these
    scores, use the `precision_recall_curve()` function to compute precision and recall
    for all possible thresholds (the function adds a last precision of 1 and a last
    recall of 0, corresponding to an infinite threshold):    [PRE39]    Finally, use
    Matplotlib to plot precision and recall as functions of the threshold value ([Figure 3-5](#precision_recall_vs_threshold_plot)).
    Let’s show the threshold of 3,000 we selected:    [PRE40]  ![Graph showing precision
    and recall curves as functions of decision threshold, illustrating their inverse
    relationship.](assets/hmls_0305.png)  ###### Figure 3-5\. Precision and recall
    versus the decision threshold    ###### Note    You may wonder why the precision
    curve is bumpier than the recall curve in [Figure 3-5](#precision_recall_vs_threshold_plot).
    The reason is that precision may sometimes go down when you raise the threshold
    (although in general it will go up). To understand why, look back at [Figure 3-4](#decision_threshold_diagram)
    and notice what happens when you start from the central threshold and move it
    just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%).
    On the other hand, recall can only go down when the threshold is increased, which
    explains why its curve looks smooth.    At this threshold value, precision is
    near 90% and recall is around 50%. Another way to select a good precision/recall
    trade-off is to plot precision directly against recall, as shown in [Figure 3-6](#precision_vs_recall_plot)
    (the same threshold is shown):    [PRE41]  ![Graph showing the precision-recall
    curve with a marked point where precision drops sharply as recall approaches 80%.](assets/hmls_0306.png)  ######
    Figure 3-6\. Precision versus recall    You can see that precision really starts
    to fall sharply at around 80% recall. You will probably want to select a precision/recall
    trade-off just before that drop—for example, at around 60% recall. But of course,
    the choice depends on your project.    Suppose you decide to aim for 90% precision.
    You could use the first plot to find the threshold you need to use, but that’s
    not very precise. Alternatively, you can search for the lowest threshold that
    gives you at least 90% precision. For this, you can use the NumPy array’s `argmax()`
    method. This returns the first index of the maximum value, which in this case
    means the first `True` value:    [PRE42] `>>>` `threshold_for_90_precision` ``
    `np.float64(3370.0194991439557)` `` [PRE43]   [PRE44]` [PRE45] [PRE46]`py [PRE47]py``
    [PRE48]py[PRE49][PRE50][PRE51][PRE52]py[PRE53]py` [PRE54]`py`` [PRE55]`py[PRE56][PRE57][PRE58]
    [PRE59][PRE60][PRE61][PRE62][PRE63]`` [PRE64][PRE65][PRE66] [PRE67]`py[PRE68]py[PRE69]py[PRE70]py[PRE71]py[PRE72]`py[PRE73]py[PRE74][PRE75][PRE76]py
    >>> class_id = some_digit_scores.argmax() `>>>` `class_id` `` `np.int64(5)` ``
    [PRE77]py[PRE78]py[PRE79][PRE80] [PRE81] If you want to force Scikit-Learn to
    use one-versus-one or one-versus-the-rest, you can use the `OneVsOneClassifier`
    or `OneVsRestClassifier` classes. Simply create an instance and pass a classifier
    to its constructor (it doesn’t even have to be a binary classifier). For example,
    this code creates a multiclass classifier using the OvR strategy, based on an
    `SVC`:    [PRE82]    Let’s make a prediction, and check the number of trained
    classifiers:    [PRE83]   [PRE84][PRE85]``py[PRE86]`` [PRE87]`` [PRE88]` Oops,
    that’s incorrect. Prediction errors do happen! This time Scikit-Learn used the
    OvR strategy under the hood: since there are 10 classes, it trained 10 binary
    classifiers. The `decision_function()` method now returns one value per class.
    Let’s look at the scores that the SGD classifier assigned to each class:    [PRE89]   [PRE90]
    You can see that the classifier is not very confident about its prediction: almost
    all scores are very negative, while class 3 has a score of +1,824, and class 5
    is not too far behind at –1,386\. Of course, you’ll want to evaluate this classifier
    on more than one image. Since there are roughly the same number of images in each
    class, the accuracy metric is fine. As usual, you can use the `cross_val_score()`
    function to evaluate the model:    [PRE91]py   [PRE92]`py [PRE93]py`` [PRE94]py[PRE95][PRE96][PRE97][PRE98]py[PRE99]py`
    [PRE100]`py`` [PRE101]`py[PRE102][PRE103][PRE104] [PRE105][PRE106][PRE107][PRE108]
    [PRE109]py` # Error Analysis    If this were a real project, you would now follow
    the steps in your machine learning project checklist (see [*https://homl.info/checklist*](https://homl.info/checklist)).
    You’d explore data preparation options, try out multiple models, shortlist the
    best ones, fine-tune their hyperparameters using `GridSearchCV`, and automate
    as much as possible. Here, we will assume that you have found a promising model
    and you want to find ways to improve it. One way to do this is to analyze the
    types of errors it makes.    First, look at the confusion matrix. For this, you
    first need to make predictions using the `cross_val_predict()` function; then
    you can pass the labels and predictions to the `confusion_matrix()` function,
    just like you did earlier. However, since there are now 10 classes instead of
    2, the confusion matrix will contain quite a lot of numbers, and it may be hard
    to read.    A colored diagram of the confusion matrix is much easier to analyze.
    To plot such a diagram, use the `ConfusionMatrixDisplay.from_predictions()` function
    like this:    [PRE110]py    This produces the left diagram in [Figure 3-9](#confusion_matrix_plot_1).
    This confusion matrix looks pretty good: most images are on the main diagonal,
    which means that they were classified correctly. Notice that the cell on the diagonal
    in row #5 and column #5 looks slightly darker than the other digits. This could
    be because the model made more errors on 5s, or because there are fewer 5s in
    the dataset than the other digits. That’s why it’s important to normalize the
    confusion matrix by dividing each value by the total number of images in the corresponding
    (true) class (i.e., divide by the row’s sum). This can be done simply by setting
    `normalize="true"`. We can also specify the `values_format=".0%"` argument to
    show percentages with no decimals. The following code produces the diagram on
    the right in [Figure 3-9](#confusion_matrix_plot_1):    [PRE111]py  ![Confusion
    matrices displaying model predictions, with the left showing raw counts and the
    right normalized by row to percentage accuracy for each class.](assets/hmls_0309.png)  ######
    Figure 3-9\. Confusion matrix (left) and the same CM normalized by row (right)    Now
    we can easily see that only 82% of the images of 5s were classified correctly.
    The most common error the model made with images of 5s was to misclassify them
    as 8s: this happened for 10% of all 5s. But only 2% of 8s got misclassified as
    5s; confusion matrices are generally not symmetrical! If you look carefully, you
    will notice that many digits have been misclassified as 8s, but this is not immediately
    obvious from this diagram. If you want to make the errors stand out more, you
    can try putting zero weight on the correct predictions. The following code does
    just that and produces the diagram on the left in [Figure 3-10](#confusion_matrix_plot_2):    [PRE112]py  ![Confusion
    matrix diagrams showing error rates normalized by row on the left and by column
    on the right, highlighting common misclassification patterns.](assets/hmls_0310.png)  ######
    Figure 3-10\. Confusion matrix with errors only, normalized by row (left) and
    by column (right)    Now you can see much more clearly the kinds of errors the
    classifier makes. The column for class 8 is now really bright, which confirms
    that many images got misclassified as 8s. In fact this is the most common misclassification
    for almost all classes. But be careful how you interpret the percentages in this
    diagram: remember that we’ve excluded the correct predictions. For example, the
    36% in row #7, column #9 in the left grid does *not* mean that 36% of all images
    of 7s were misclassified as 9s. It means that 36% of the *errors* the model made
    on images of 7s were misclassifications as 9s. In reality, only 3% of images of
    7s were misclassified as 9s, as you can see in the diagram on the right in [Figure 3-9](#confusion_matrix_plot_1).    It
    is also possible to normalize the confusion matrix by column rather than by row:
    if you set `normalize="pred"`, you get the diagram on the right in [Figure 3-10](#confusion_matrix_plot_2).
    For example, you can see that 56% of misclassified 7s are actually 9s.    Analyzing
    the confusion matrix often gives you insights into ways to improve your classifier.
    Looking at these plots, it seems that your efforts should be spent on reducing
    the false 8s. For example, you could try to gather more training data for digits
    that look like 8s (but are not) so that the classifier can learn to distinguish
    them from real 8s. Or you could engineer new features that would help the classifier—for
    example, writing an algorithm to count the number of closed loops (e.g., 8 has
    two, 6 has one, 5 has none). Or you could preprocess the images (e.g., using Scikit-Image,
    Pillow, or OpenCV) to make some patterns, such as closed loops, stand out more.    Analyzing
    individual errors can also be a good way to gain insights into what your classifier
    is doing and why it is failing. For example, let’s plot examples of 3s and 5s
    in a confusion matrix style ([Figure 3-11](#error_analysis_digits_plot)):    [PRE113]py  ![Confusion
    matrix showing images of handwritten digits 3 and 5, highlighting misclassifications
    by a simple classifier.](assets/hmls_0311.png)  ###### Figure 3-11\. Some images
    of 3s and 5s organized like a confusion matrix    As you can see, some of the
    digits that the classifier gets wrong (i.e., in the bottom-left and top-right
    blocks) are so badly written that even a human would have trouble classifying
    them. However, most misclassified images seem like obvious errors to us. It may
    be hard to understand why the classifier made the mistakes it did, but remember
    that the human brain is a fantastic pattern recognition system, and our visual
    system does a lot of complex preprocessing before any information even reaches
    our consciousness. So, the fact that this task feels simple does not mean that
    it is. Recall that we used a simple `SGDClassifier`, which is just a linear model:
    all it does is assign a weight per class to each pixel, and when it sees a new
    image it just sums up the weighted pixel intensities to get a score for each class.
    Since 3s and 5s differ by only a few pixels, this model will easily confuse them.    The
    main difference between 3s and 5s is the position of the small line that joins
    the top line to the bottom arc. If you draw a 3 with the junction slightly shifted
    to the left, the classifier might classify it as a 5, and vice versa. In other
    words, this classifier is quite sensitive to image shifting and rotation. One
    way to reduce the 3/5 confusion is to preprocess the images to ensure that they
    are well centered and not too rotated. However, this may not be easy since it
    requires predicting the correct rotation of each image. A much simpler approach
    consists of augmenting the training set with slightly shifted and rotated variants
    of the training images. This will force the model to learn to be more tolerant
    to such variations. This is called *data augmentation* (we’ll cover this in [Chapter 12](ch12.html#cnn_chapter);
    also see exercise 2 at the end of this chapter).    # Multilabel Classification    Until
    now, each instance has always been assigned to just one class. But in some cases
    you may want your classifier to output multiple classes for each instance. Consider
    a face-recognition classifier: what should it do if it recognizes several people
    in the same picture? It should attach one tag per person it recognizes. Say the
    classifier has been trained to recognize three faces: Alice, Bob, and Charlie.
    Then when the classifier is shown a picture of Alice and Charlie, it should output
    `[True, False, True]` (meaning “Alice yes, Bob no, Charlie yes”). Such a classification
    system that outputs multiple binary tags is called a *multilabel classification*
    system.    We won’t go into face recognition just yet, but let’s look at a simpler
    example, just for illustration purposes:    [PRE114]py    This code creates a
    `y_multilabel` array containing two target labels for each digit image: the first
    indicates whether the digit is large (7, 8, or 9), and the second indicates whether
    it is odd. Then the code creates a `KNeighborsClassifier` instance, which supports
    multilabel classification (not all classifiers do), and trains this model using
    the multiple targets array. Now you can make a prediction, and notice that it
    outputs two labels:    [PRE115]py   [PRE116]py  [PRE117]py [PRE118]py`` [PRE119]py[PRE120]py[PRE121]py[PRE122]py[PRE123]py[PRE124]py[PRE125]py`
    [PRE126]`py[PRE127]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5]` [PRE6] [PRE7]`py [PRE8]py[PRE9]py[PRE10]`py[PRE11]py` [PRE12]`py[PRE13]py[PRE14]py[PRE15]py[PRE16]py[PRE17]py[PRE18]`py[PRE19]py[PRE20]`
    [PRE21] >>> from sklearn.metrics import f1_score `>>>` `f1_score``(``y_train_5``,`
    `y_train_pred``)` `` `0.7325171197343846` `` [PRE22]` [PRE23][PRE24][PRE25][PRE26]``py[PRE27]py`
    ## 精确率/召回率权衡    为了理解这种权衡，让我们看看`SGDClassifier`是如何做出分类决定的。对于每个实例，它根据一个*决策函数*计算一个分数。如果这个分数大于一个阈值，它将实例分配到正类；否则，它将实例分配到负类。[图3-4](#decision_threshold_diagram)显示了从左到右从最低分数到最高分数的一些数字。假设*决策阈值*位于中央箭头（两个5之间）：你会在阈值右侧找到4个真正的正例（实际的5），以及1个假正例（实际上是一个6）。因此，使用这个阈值，精确率为80%（5个中的4个）。但是，在6个实际的5中，分类器只检测到4个，因此召回率为67%（6个中的4个）。如果你提高阈值（将其移动到右侧的箭头），假正例（6）变成真负例，从而提高了精确率（在本例中达到100%），但一个真正的正例变成假负例，召回率下降到50%。相反，降低阈值会增加召回率并降低精确率。  ![不同决策阈值下说明精确率/召回率权衡的图；随着阈值的增加，精确率通常增加，而召回率降低。](assets/hmls_0304.png)  ######
    图3-4. 精确率/召回率权衡：图像按其分类器分数排序，高于所选决策阈值的被认为是正例；阈值越高，召回率越低，但（通常）精确率越高    你可以调用分类器的`predict()`方法，而不是调用其`decision_function()`方法，后者为每个实例返回一个分数。然后你可以使用任何你想要的阈值来根据这些分数做出预测：    [PRE28]py``
    `array([2164.22030239])` `>>>` `threshold` `=` `0` [PRE29]` [PRE30][PRE31][PRE32]``py[PRE33]`py`
    `SGDClassifier`使用等于0的阈值，因此前面的代码返回的结果与`predict()`方法相同（即`True`）。让我们提高阈值：    [PRE34]py
    `>>>` `y_some_digit_pred` `` `array([False])` `` [PRE35]py   [PRE36] [PRE37]``
    这证实了提高阈值会降低召回率。实际上，图像代表一个5，当阈值是0时，分类器可以检测到它，但当阈值增加到3,000时，它就错过了它。    你如何决定使用哪个阈值？一个选项是使用`cross_val_predict()`函数来获取训练集中所有实例的分数，但这次指定你想要返回决策分数而不是预测：    [PRE38]    使用这些分数，使用`precision_recall_curve()`函数计算所有可能阈值下的精确率和召回率（该函数添加一个最后的精确率为1和一个最后的召回率为0，对应于无限阈值）：    [PRE39]    最后，使用Matplotlib将精确率和召回率作为阈值值的函数绘制出来([图3-5](#precision_recall_vs_threshold_plot))。让我们展示我们选择的3,000个阈值：    [PRE40]  ![显示精确率和召回率曲线作为决策阈值的函数的图表，说明了它们之间的反向关系。](assets/hmls_0305.png)  ######
    图3-5. 精确率和召回率与决策阈值    ###### 注意    你可能会想知道为什么[图3-5](#precision_recall_vs_threshold_plot)中的精确率曲线比召回率曲线更崎岖。原因是，当你提高阈值时，精确率有时会下降（尽管在一般情况下它会上升）。要了解原因，请回顾[图3-4](#decision_threshold_diagram)，并注意当你从中央阈值开始并将其向右移动一个数字时会发生什么：精确率从4/5（80%）下降到3/4（75%）。另一方面，召回率只有在阈值增加时才会下降，这解释了为什么它的曲线看起来很平滑。    在这个阈值值下，精确率接近90%，召回率约为50%。选择良好的精确率/召回率权衡的另一种方法是直接将精确率与召回率绘制在一起，如图[图3-6](#precision_vs_recall_plot)所示（显示相同的阈值）：    [PRE41]  ![显示精确率-召回率曲线的图表，其中标记了一个点，其中精确率随着召回率接近80%而急剧下降。](assets/hmls_0306.png)  ######
    图3-6. 精确率与召回率    你可以看到，精确率在约80%的召回率时真正开始急剧下降。你可能想在下降之前选择一个精确率/召回率权衡——例如，在约60%的召回率时。但当然，选择取决于你的项目。    假设你决定以90%的精确率为目标。你可以使用第一个图表来找到你需要使用的阈值，但这不是很精确。或者，你可以搜索至少给你90%精确率的最小阈值。为此，你可以使用NumPy数组的`argmax()`方法。这返回最大值的第一个索引，在这种情况下意味着第一个`True`值：    [PRE42]
    `>>>` `threshold_for_90_precision` `` `np.float64(3370.0194991439557)` `` [PRE43]   [PRE44]`
    [PRE45] [PRE46]`py [PRE47]py`` [PRE48]py[PRE49][PRE50][PRE51][PRE52]py[PRE53]py`
    [PRE54]`py`` [PRE55]`py[PRE56][PRE57][PRE58] [PRE59][PRE60][PRE61][PRE62][PRE63]``
    [PRE64][PRE65][PRE66] [PRE67]`py[PRE68]py[PRE69]py[PRE70]py[PRE71]py[PRE72]`py[PRE73]py[PRE74][PRE75][PRE76]py
    >>> class_id = some_digit_scores.argmax() `>>>` `class_id` `` `np.int64(5)` ``
    [PRE77]py[PRE78]py[PRE79][PRE80] [PRE81] 如果你想要强制Scikit-Learn使用一对一或一对多，你可以使用`OneVsOneClassifier`或`OneVsRestClassifier`类。只需创建一个实例并将其传递给构造函数（它甚至不需要是二元分类器）。例如，此代码创建了一个基于`SVC`的OvR策略的多类分类器：    [PRE82]    让我们进行预测，并检查训练的分类器数量：    [PRE83]   [PRE84][PRE85]``py[PRE86]``
    [PRE87]`` [PRE88]` 哎呀，这是错误的。预测错误确实会发生！这次Scikit-Learn在底层使用了OvR策略：由于有10个类别，它训练了10个二元分类器。`decision_function()`方法现在为每个类别返回一个值。让我们看看SGD分类器分配给每个类别的分数：    [PRE89]   [PRE90]
    你可以看到，分类器对其预测并不太自信：几乎所有的分数都非常负，而类别3的分数为+1,824，类别5的分数也不太远，为-1,386。当然，你将想要在多个图像上评估这个分类器。由于每个类别中的图像数量大致相同，准确率指标是合适的。像往常一样，你可以使用`cross_val_score()`函数来评估模型：    [PRE91]py   [PRE92]`py
    [PRE93]py`` [PRE94]py[PRE95][PRE96][PRE97][PRE98]py[PRE99]py` [PRE100]`py`` [PRE101]`py[PRE102][PRE103][PRE104]
    [PRE105][PRE106][PRE107][PRE108] [PRE109]py` # 错误分析    如果这是一个真实的项目，你现在将遵循你的机器学习项目清单中的步骤（见[*https://homl.info/checklist*](https://homl.info/checklist)）。你会探索数据准备选项，尝试多个模型，筛选出最好的模型，使用`GridSearchCV`调整它们的超参数，并尽可能自动化。在这里，我们假设你已经找到了一个有前途的模型，并且你想要找到改进它的方法。一种方法是通过分析它所犯的错误类型。    首先，看看混淆矩阵。为此，你首先需要使用`cross_val_predict()`函数进行预测；然后你可以将标签和预测传递给`confusion_matrix()`函数，就像你之前做的那样。但是，由于现在有10个类别而不是2个，混淆矩阵将包含相当多的数字，并且可能难以阅读。    混淆矩阵的彩色图更容易分析。要绘制这样的图表，使用`ConfusionMatrixDisplay.from_predictions()`函数，如下所示：    [PRE110]py    这产生了[图3-9](#confusion_matrix_plot_1)中的左侧图表。这个混淆矩阵看起来相当不错：大多数图像都在主对角线上，这意味着它们被正确分类。注意，第5行和第5列的对角线单元格看起来比其他数字略暗。这可能是因为模型在5上犯了更多的错误，或者因为数据集中5的数量比其他数字少。这就是为什么通过将每个值除以对应（真实）类别的总图像数（即除以行的总和）来规范化混淆矩阵很重要。这可以通过简单地设置`normalize="true"`来完成。我们还可以指定`values_format=".0%"`参数来显示没有小数的百分比。以下代码产生了[图3-9](#confusion_matrix_plot_1)右侧的图表：    [PRE111]py  ![显示模型预测的混淆矩阵，左侧显示原始计数，右侧按行规范化为每个类别的百分比准确率。](assets/hmls_0309.png)  ######
    图3-9. 混淆矩阵（左侧）和按行规范化的相同CM（右侧）    现在，我们可以很容易地看到只有82%的5的图像被正确分类。模型对5的图像犯的最常见的错误是将它们错误地分类为8：这发生在所有5的10%中。但只有2%的8被错误地分类为5；混淆矩阵通常不是对称的！如果你仔细观察，你会注意到许多数字被错误地分类为8，但这从这个图表中并不明显。如果你想使错误更加突出，你可以尝试对正确的预测不赋予任何权重。以下代码就是这样做的，并产生了[图3-10](#confusion_matrix_plot_2)左侧的图表：    [PRE112]py  ![显示错误率按行规范化（左侧）和按列规范化（右侧）的混淆矩阵图表，突出显示常见的错误分类模式。](assets/hmls_0310.png)  ######
    图3-10. 仅包含错误的混淆矩阵，按行规范化（左侧）和按列规范化（右侧）    现在，你可以更清楚地看到分类器犯的错误类型。现在，类别8的列真的很亮，这证实了许多图像被错误地分类为8。事实上，这是几乎所有类别的最常见错误分类。但请注意如何解释这个图表中的百分比：记住，我们已经排除了正确的预测。例如，左侧网格中第7行第9列的36%并不意味着36%的所有7的图像都被错误地分类为9。这意味着模型在7的图像上犯的错误中有36%被错误地分类为9。实际上，只有3%的7的图像被错误地分类为9，正如你在[图3-9](#confusion_matrix_plot_1)右侧的图表中可以看到的。    还有可能通过列而不是行来规范化混淆矩阵：如果你设置`normalize="pred"`，你将得到[图3-10](#confusion_matrix_plot_2)右侧的图表。例如，你可以看到56%的错误分类的7实际上是9。    分析混淆矩阵通常可以让你了解如何改进你的分类器。查看这些图表，似乎你应该把精力放在减少错误的8上。例如，你可以尝试收集更多看起来像8但不是的数字的训练数据，以便分类器可以学会将它们与真正的8区分开来。或者，你可以设计新的特征来帮助分类器——例如，编写一个算法来计算封闭环的数量（例如，8有两个，6有一个，5没有）。或者，你可以对图像进行预处理（例如，使用Scikit-Image、Pillow或OpenCV）来使某些模式（例如，封闭环）更加突出。    分析单个错误也可以是深入了解你的分类器正在做什么以及为什么它失败的好方法。例如，让我们以混淆矩阵的方式绘制3和5的示例([图3-11](#error_analysis_digits_plot))：    [PRE113]py  ![显示手写数字3和5的图像，以混淆矩阵的方式组织，突出显示一个简单分类器犯的错误。](assets/hmls_0311.png)  ######
    图3-11. 一些3和5的图像，以混淆矩阵的方式组织    如你所见，一些分类器犯错的数字（即左下角和右上角块中的数字）写得非常糟糕，即使是人类也很难分类它们。然而，大多数被错误分类的图像对我们来说似乎是明显的错误。虽然很难理解分类器为什么会犯错误，但请记住，人脑是一个了不起的模式识别系统，我们的视觉系统在信息甚至达到我们的意识之前会进行大量的复杂预处理。因此，这项任务感觉很简单并不意味着它很简单。回想一下，我们使用了一个简单的`SGDClassifier`，它只是一个线性模型：它所做的只是为每个类别分配每个像素的权重，当它看到一个新的图像时，它只是将加权像素强度相加以获得每个类别的分数。由于3和5只相差几个像素，这个模型很容易将它们混淆。    3和5之间的主要区别是连接顶部行和底部弧的小线的位置。如果你将3画得连接点稍微向左偏移，分类器可能会将其分类为5，反之亦然。换句话说，这个分类器对图像的平移和旋转非常敏感。减少3/5混淆的一种方法是对图像进行预处理，确保它们很好地居中并且不太旋转。然而，这可能不容易做到，因为它需要预测每个图像的正确旋转。一个更简单的方法是在训练集中添加稍微平移和旋转的训练图像的变体。这将迫使模型学会对这种变化更加宽容。这被称为*数据增强*（我们将在[第12章](ch12.html#cnn_chapter)中介绍；也请参阅本章末尾的练习2）。    #
    多标签分类    到目前为止，每个实例都始终被分配到只有一个类别。但在某些情况下，你可能希望你的分类器为每个实例输出多个类别。考虑一个面部识别分类器：如果它在同一张图片中识别出几个人，它应该怎么办？它应该为它识别的每个人分配一个标签。假设分类器已经训练好了识别三个面部：Alice、Bob和Charlie。然后当分类器被展示一张Alice和Charlie的图片时，它应该输出`[True,
    False, True]`（表示“Alice是，Bob不是，Charlie是”）。这种输出多个二进制标签的分类系统称为*多标签分类*系统。    我们现在不会深入探讨面部识别，但让我们看看一个更简单的例子，只是为了说明目的：    [PRE114]py    此代码创建了一个`y_multilabel`数组，其中包含每个数字图像的两个目标标签：第一个表示数字是否很大（7、8或9），第二个表示它是否为奇数。然后代码创建了一个`KNeighborsClassifier`实例，该实例支持多标签分类（并非所有分类器都支持），并使用多个目标数组训练此模型。现在你可以进行预测，并注意它输出了两个标签：    [PRE115]py   [PRE116]py  [PRE117]py
    [PRE118]py`` [PRE119]py[PRE120]py[PRE121]py[PRE122]py[PRE123]py[PRE124]py[PRE125]py`
    [PRE126]`py[PRE127]'
