- en: Chapter 7\. Deploying PyTorch to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 将PyTorch部署到生产环境
- en: Most of this book so far has focused on model design and training. Earlier chapters
    showed you how to use the built-in capabilities of PyTorch to design your models
    and create custom NN modules, loss functions, optimizers, and other algorithms.
    In the previous chapter, we looked at how to use distributed training and model
    optimizations to accelerate your model training times and minimize the resources
    needed for running your models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书大部分内容都集中在模型设计和训练上。早期章节向您展示了如何使用PyTorch的内置能力设计您的模型，并创建自定义的NN模块、损失函数、优化器和其他算法。在上一章中，我们看到了如何使用分布式训练和模型优化来加速模型训练时间，并最大程度地减少运行模型所需的资源。
- en: At this point, you have everything you need to create some well-trained, cutting-edge
    NN models—but don’t let your innovations sit in isolation. Now it’s time to deploy
    your models into the world through applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经拥有创建一些经过良好训练、尖端的NN模型所需的一切，但不要让您的创新孤立存在。现在是时候通过应用程序将您的模型部署到世界上了。
- en: In the past, going from research to production was a challenging task that required
    a team of software engineers to move PyTorch models to a framework and integrate
    them nto a (often non-Python) production environment. Today, PyTorch includes
    built-in tools and external libraries to support rapid deployment to a variety
    of production environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，从研究到生产是一项具有挑战性的任务，需要一个软件工程团队将PyTorch模型移至一个框架并将其集成到（通常非Python）生产环境中。如今，PyTorch包括内置工具和外部库，支持快速部署到各种生产环境。
- en: In this chapter, we focus on deploying your model for inference, not training,
    and we’ll explore how to deploy your trained PyTorch models into a variety of
    applications. First, I’ll describe the various built-in capabilities and tools
    within PyTorch that you can use for deployment. Tools like TorchServe and TorchScript
    allow you to easily deploy your PyTorch models to the cloud and to mobile or edge
    devices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于将您的模型部署到推理而非训练，并探讨如何将经过训练的PyTorch模型部署到各种应用程序中。首先，我将描述PyTorch内置的各种能力和工具，供您用于部署。像TorchServe和TorchScript这样的工具使您能够轻松地将PyTorch模型部署到云端、移动设备或边缘设备。
- en: Depending on the application and environment, you may have several options for
    deployment, each with its own trade-offs. I’ll show you examples of how you can
    deploy your PyTorch models in multiple cloud and edge environments. You’ll learn
    how to deploy to web servers for development and production at scale, to iOS and
    Android mobile devices, and to Internet of Things (IoT) devices based on ARM processors,
    GPUs, and field-programmable gate array (FPGA) hardware.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序和环境，您可能有多种部署选项，每种选项都有其自身的权衡。我将向您展示如何在多个云端和边缘环境中部署您的PyTorch模型的示例。您将学习如何部署到用于开发和生产规模的Web服务器，iOS和Android移动设备，以及基于ARM处理器、GPU和现场可编程门阵列（FPGA）硬件的物联网（IoT）设备。
- en: The chapter will also provide reference code, including references to the key
    APIs and libraries we use, to make getting started easy. When it comes time to
    deploy your models, you can refer back to this chapter for a quick reference so
    you can demonstrate your applications in cloud or mobile environments.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将提供参考代码，包括我们使用的关键API和库的引用，以便轻松入门。当您需要部署模型时，您可以参考本章进行快速查阅，以便在云端或移动环境中展示您的应用程序。
- en: Let’s begin by reviewing the resources that PyTorch provides to assist you in
    deploying your models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始回顾PyTorch提供的资源，以帮助您部署您的模型。
- en: PyTorch Deployment Tools and Libraries
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch部署工具和库
- en: PyTorch includes built-in tools and capabilities to facilitate deploying your
    model to production environments and edge devices. In this section, we’ll explore
    those tools, and in the rest of the chapter we’ll apply them to various environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch包括内置工具和能力，以便将您的模型部署到生产环境和边缘设备。在本节中，我们将探索这些工具，并在本章的其余部分将它们应用于各种环境中。
- en: PyTorch’s deployment capabilities include its natural Python API, as well as
    the TorchServe, TorchScript, ONNX, and mobile libraries. Since PyTorch’s natural
    API is Python-based, PyTorch models can be deployed as is in any environment that
    supports Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的部署能力包括其自然的Python API，以及TorchServe、TorchScript、ONNX和移动库。由于PyTorch的自然API基于Python，PyTorch模型可以在任何支持Python的环境中直接部署。
- en: '[Table 7-1](#table_deployment_resources) summarizes the various resources available
    for deployment and indicates how to appropriately use each one.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7-1](#table_deployment_resources) 总结了可用于部署的各种资源，并指示如何适当地使用每种资源。'
- en: Table 7-1\. PyTorch resources for deployment
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-1. 部署PyTorch资源
- en: '| Resource | Use |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 使用 |'
- en: '| --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Python API | Perform fast prototyping, training, and experimentation; program
    Python runtimes. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Python API | 进行快速原型设计、训练和实验；编写Python运行时程序。 |'
- en: '| TorchScript | Improve performance and portability (e.g., load and run a model
    in C++); program non-Python runtimes or strict latency and performance requirements.
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| TorchScript | 提高性能和可移植性（例如，在C++中加载和运行模型）；编写非Python运行时或严格的延迟和性能要求。 |'
- en: '| TorchServe | A fast production environment tool with model store, A/B testing,
    monitoring, and RESTful API. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| TorchServe | 一个快速的生产环境工具，具有模型存储、A/B测试、监控和RESTful API。 |'
- en: '| ONNX | Deploy to systems with ONNX runtimes or FPGA devices. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ONNX | 部署到具有ONNX运行时或FPGA设备的系统。 |'
- en: '| Mobile libraries | Deploy to iOS and Android devices. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 移动库 | 部署到iOS和Android设备。 |'
- en: The following sections provide a reference and some sample code for each deployment
    resource. In each case, we’ll use the same example model, described next.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节为每个部署资源提供参考和一些示例代码。在每种情况下，我们将使用相同的示例模型，接下来将对其进行描述。
- en: Common Example Model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见示例模型
- en: For each of the deployment resource examples and applications, as well as the
    reference code provided in this chapter, we will use the same model. For our examples,
    we’ll deploy an image classifier using a VGG16 model pretrained with ImageNet
    data. That way, each section can focus on the deployment approach used and not
    the model itself. For each approach, you can replace the VGG16 model with one
    of your own and follow the same workflow to achieve results with your own designs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章提供的每个部署资源示例和应用程序，以及参考代码，我们将使用相同的模型。对于我们的示例，我们将使用一个使用ImageNet数据预训练的VGG16模型来部署一个图像分类器。这样，每个部分都可以专注于使用的部署方法，而不是模型本身。对于每种方法，您可以用自己的模型替换VGG16模型，并按照相同的工作流程来实现您自己设计的结果。
- en: 'The following code instantiates the model for use throughout this chapter:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实例化了本章中将要使用的模型：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’ve used the VGG16 model before. To give you an idea of the model’s complexity,
    let’s print out the number of trainable parameters using the following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用过VGG16模型。为了让您了解模型的复杂性，让我们使用以下代码打印出可训练参数的数量：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The VGG16 model has 138,357,544 trainable parameters. As we go through each
    approach, keep in mind the performance at this level of complexity. You can use
    this as a rough benchmark when comparing the complexity of your models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16模型有138,357,544个可训练参数。当我们逐步进行每种方法时，请记住在这个复杂性水平上的性能。您可以将其用作比较您模型复杂性的粗略基准。
- en: After we instantiate the VGG16 model, it requires minimal effort to deploy it
    in a Python application. In fact, we’ve already done this when we tested our models
    in previous chapters. Let’s review the process one more time before we jump into
    other approaches.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化VGG16模型后，将其部署到Python应用程序中需要很少的工作。事实上，在之前的章节中测试我们的模型时，我们已经这样做了。在我们进入其他方法之前，让我们再次回顾一下这个过程。
- en: Python API
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python API
- en: 'The Python API is not a new resource. It’s the same one we’ve been using throughout
    the book. I mention it here to point out that you can deploy your PyTorch models
    without any changes to your code. In this case, you simply call your model in
    evaluation mode with your desired inputs from any Python application, as shown
    in the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Python API并不是一个新资源。这是我们在整本书中一直在使用的资源。我在这里提到它是为了指出您可以在不更改代码的情况下部署您的PyTorch模型。在这种情况下，您只需从任何Python应用程序中以评估模式调用您的模型，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code loads the model, passes in the input, and prints out the output. This
    is a simple standalone Python application. You’ll see how to deploy a model to
    a Python web server using a RESTful API and Flask later in this chapter. Using
    a Flask web server, you can build a quick browser application that demonstrates
    your model’s capability.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码加载模型，传入输入，并打印输出。这是一个简单的独立Python应用程序。您将看到如何在本章后面使用RESTful API和Flask将模型部署到Python
    Web服务器。使用Flask Web服务器，您可以构建一个快速的浏览器应用程序，展示您模型的能力。
- en: Python is not always used in production environments due to its slower performance
    and lack of true multithreading. If your production environment uses another language
    (e.g., C++, Java, Rust, or Go), you can convert your models to TorchScript code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python并不总是在生产环境中使用，因为它的性能较慢，而且缺乏真正的多线程。如果您的生产环境使用另一种语言（例如C++、Java、Rust或Go），您可以将您的模型转换为TorchScript代码。
- en: TorchScript
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TorchScript
- en: TorchScript is a way to serialize and optimize your PyTorch model code so that
    your PyTorch models can be saved and executed in non-Python runtime environments
    with no dependency on Python. TorchScript is commonly used to run PyTorch models
    in C++ and with any language that supports C++ bindings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript是一种序列化和优化PyTorch模型代码的方式，使得PyTorch模型可以在非Python运行环境中保存和执行，而不依赖于Python。TorchScript通常用于在C++中运行PyTorch模型，并与支持C++绑定的任何语言一起使用。
- en: TorchScript represents a PyTorch model in a format that can be understood, compiled,
    and serialized by the TorchScript compiler. The TorchScript compiler creates a
    serialized, optimized version of your model that can be used in C++ applications.
    To load your TorchScript model in C++, you would use the PyTorch C++ API library
    called *LibTorch*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript代表了一个PyTorch模型的格式，可以被TorchScript编译器理解、编译和序列化。TorchScript编译器创建了一个序列化的优化版本的您的模型，可以在C++应用程序中使用。要在C++中加载您的TorchScript模型，您将使用名为*LibTorch*的PyTorch
    C++ API库。
- en: There are two ways to convert your PyTorch models to TorchScript. The first
    one is called *tracing*, which is a process in which you pass in an example input
    and perform the conversion with one line of code. It’s used in most cases. The
    second is called *scripting*, and it’s used when your model has more complex control
    code. For example, if your model has conditional `if` statements that depend on
    the input itself, you’ll want to use scripting. Let’s take a look at some reference
    code for each case.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以将您的PyTorch模型转换为TorchScript。第一种称为*tracing*，这是一个过程，在这个过程中，您传入一个示例输入并使用一行代码进行转换。在大多数情况下使用。第二种称为*scripting*，当您的模型具有更复杂的控制代码时使用。例如，如果您的模型具有依赖于输入本身的条件`if`语句，您将需要使用scripting。让我们看一下每种情况的一些参考代码。
- en: 'Since our VGG16 example model does not have any control flow, we can use tracing
    to convert our model to TorchScript, as shown in the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的VGG16示例模型没有任何控制流，我们可以使用跟踪来将我们的模型转换为TorchScript，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code creates a Python callable model, `torchscript_model`, that can be evaluated
    using a normal PyTorch approach such as `output = torchscript_model(inputs)`.
    Once we save the model, we can use it in a C++ application.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码创建了一个Python可调用模型`torchscript_model`，可以使用正常的PyTorch方法进行评估，例如`output = torchscript_model(inputs)`。一旦我们保存了模型，就可以在C++应用程序中使用它。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The “normal” method of evaluating a model in PyTorch is often called `eager
    mode` since it’s the quickest way to evaluate your models for development.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中评估模型的“正常”方法通常被称为`eager mode`，因为这是开发中评估模型的最快方法。
- en: 'If our model used control flow, we would need to use the annotation method
    to convert it to TorchScript. Let’s consider the following model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型使用了控制流，我们将需要使用注释方法将其转换为TorchScript。让我们考虑以下模型：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this example, the `ControlFlowModel` outputs and weights depend on the input
    values. In this case, we need to use `torch.jit.script()`, and then we can save
    the model to TorchScript just like we did with tracing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`ControlFlowModel`的输出和权重取决于输入值。在这种情况下，我们需要使用`torch.jit.script()`，然后我们可以像跟踪一样将模型保存到TorchScript中。
- en: 'Now we can use our model in a C++ application, as shown in the following C++
    code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在C++应用程序中使用我们的模型，如下所示的C++代码：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We pass in the filename of the TorchScript module to the program and load the
    model using `torch::jit::load()`. Then we create a sample input vector, run it
    through our TorchScript model, and convert the outputs to tensors, printing them
    to `stdout`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将TorchScript模块的文件名传递给程序，并使用`torch::jit::load()`加载模型。然后我们创建一个示例输入向量，将其通过我们的TorchScript模型运行，并将输出转换为张量，打印到`stdout`。
- en: The TorchScript API provides additional functions to support converting your
    models to TorchScript. [Table 7-2](#tableTorchScriptapi) lists the supported functions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript API提供了额外的函数来支持将模型转换为TorchScript。[表7-2](#tableTorchScriptapi)列出了支持的函数。
- en: Table 7-2\. TorchScript API functions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-2\. TorchScript API函数
- en: '| Function | Description |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `script(`*`obj[, optimize, _frames_up, _rcb]`*`)` | Inspects the source code,
    compiles it as *TorchScript* code using the *TorchScript* compiler, and returns
    a `ScriptModule` or `ScriptFunction` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `script(`*`obj[, optimize, _frames_up, _rcb]`*`)` | 检查源代码，使用*TorchScript*编译器将其编译为*TorchScript*代码，并返回一个`ScriptModule`或`ScriptFunction`
    |'
- en: '| `trace(`*`func, example_inputs[, optimize, ...]`*`)` | Traces a function
    and returns an executable or `ScriptFunction` that will be optimized using just-in-time
    compilation |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `trace(`*`func, example_inputs[, optimize, ...]`*`)` | 跟踪一个函数并返回一个可执行的或者`ScriptFunction`，将使用即时编译进行优化
    |'
- en: '| `script_if_tracing(`*`fn`*`)` | Compiles `fn` when it is first called during
    tracing |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `script_if_tracing(`*`fn`*`)` | 在跟踪期间首次调用`fn`时编译 |'
- en: '| `trace_module(`*`mod, inputs[, optimize, ...]`*`)` | Traces a module and
    returns an executable `ScriptModule` that will be optimized using just-in-time
    compilation |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `trace_module(`*`mod, inputs[, optimize, ...]`*`)` | 跟踪一个模块并返回一个可执行的`ScriptModule`，将使用即时编译进行优化
    |'
- en: '| `fork(`*`func, *args, **kwargs`*`)` | Creates an asynchronous task executing
    `func` and a reference to the value of the result of this execution |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `fork(`*`func, *args, **kwargs`*`)` | 创建一个异步任务，执行`func`并引用此执行结果的值 |'
- en: '| `wait(`*`future`*`)` | Forces the completion of a `torch.jit.Future[T]` asynchronous
    task, returning the result of the task |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `wait(`*`future`*`)` | 强制完成一个`torch.jit.Future[T]`异步任务，返回任务的结果 |'
- en: '| `ScriptModule()` | Wraps a script into a C++ `torch::jit::Module` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `ScriptModule()` | 将脚本封装成C++ `torch::jit::Module` |'
- en: '| `ScriptFunction()` | Works the same as `ScriptModule()` but represents a
    single function and does not have any attributes or parameters |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `ScriptFunction()` | 与`ScriptModule()`相同，但表示一个单独的函数，没有任何属性或参数 |'
- en: '| `freeze(`*`mod[, preserved_attrs]`*`)` | Clones a `ScriptModule` and attempts
    to inline the cloned module’s submodules, parameters, and attributes as constants
    in the *TorchScript* IR Graph |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `freeze(`*`mod[, preserved_attrs]`*`)` | 克隆一个`ScriptModule`，并尝试将克隆模块的子模块、参数和属性内联为*TorchScript*
    IR图中的常量 |'
- en: '| `save(`*`m, f[, _extra_files]`*`)` | Saves an offline version of the module
    for use in a separate process |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `save(`*`m, f[, _extra_files]`*`)` | 保存模块的离线版本，用于在单独的进程中使用 |'
- en: '| `load(`*`f[, map_location, _extra_files]`*`)` | Loads a `ScriptModule` or
    `ScriptFunction` previously saved with `torch.jit.save()` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `load(`*`f[, map_location, _extra_files]`*`)` | 加载之前使用`torch.jit.save()`保存的`ScriptModule`或`ScriptFunction`
    |'
- en: '| `ignore(`*`[drop]`*`)` | Indicates to the compiler that a function or method
    should be ignored and left as a Python function |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `ignore(`*`[drop]`*`)` | 指示编译器忽略一个函数或方法，并保留为Python函数 |'
- en: '| `unused(`*`fn`*`)` | Indicates to the compiler that a function or method
    should be ignored and replaced with the raising of an exception |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `unused(`*`fn`*`)` | 指示编译器忽略一个函数或方法，并替换为引发异常 |'
- en: '| `isinstance(`*`obj, target_type`*`)` | Provides for container-type refinement
    in TorchScript |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `isinstance(`*`obj, target_type`*`)` | 在TorchScript中提供容器类型细化 |'
- en: In this section, we used TorchScript to increase the performance of our model
    when it’s used in a C++ application or in a language that binds to C++. However,
    deploying PyTorch models at scale requires additional capabilities, like packaging
    models, configuring runtime environments, exposing API endpoints, logging and
    monitoring, and managing multiple model versions. Fortunately, PyTorch provides
    a tool called TorchServe to facilitate these tasks and rapidly deploy your models
    for inference at scale.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用TorchScript来提高模型在C++应用程序或绑定到C++的语言中的性能。然而，规模化部署PyTorch模型需要额外的功能，比如打包模型、配置运行环境、暴露API端点、日志和监控，以及管理多个模型版本。幸运的是，PyTorch提供了一个名为TorchServe的工具，以便于这些任务，并快速部署您的模型进行规模推理。
- en: TorchServe
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TorchServe
- en: TorchServe is an open-source model-serving framework that makes it easy to deploy
    trained PyTorch models. It was developed by AWS engineers and jointly released
    with Facebook in April 2020, and it is actively maintained by AWS. TorchServe
    supports all the features needed to deploy models to production at scale, including
    multimodel serving, model versioning for A/B testing, logging and metrics for
    monitoring, and a RESTful API for integration with other systems. [Figure 7-1](#fig_torchserve)
    illustrates how TorchServe works.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe是一个开源的模型服务框架，可以轻松部署训练好的PyTorch模型。它由AWS工程师开发，并于2020年4月与Facebook联合发布，目前由AWS积极维护。TorchServe支持部署模型到生产环境所需的所有功能，包括多模型服务、模型版本控制用于A/B测试、日志和监控指标、以及与其他系统集成的RESTful
    API。[图7-1](#fig_torchserve)展示了TorchServe的工作原理。
- en: '![“TorchServe Architecture”](Images/ptpr_0701.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![“TorchServe Architecture”](Images/ptpr_0701.png)'
- en: Figure 7-1\. TorchServe architecture
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. TorchServe架构
- en: The client application interfaces with TorchServe through multiple APIs. The
    Inference API provides the main inference requests and predictions. The client
    application sends input data through the RESTful API request and receives the
    prediction results. The Management API allows you to register and manage your
    deployed models. You can register, unregister, set default models, configure A/B
    testing, check status, and specify the number of workers for a model. The Metrics
    API allows you to monitor each model’s performance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端应用程序通过多个 API 与 TorchServe 进行交互。推理 API 提供主要的推理请求和预测。客户端应用程序通过 RESTful API
    请求发送输入数据，并接收预测结果。管理 API 允许您注册和管理已部署的模型。您可以注册、注销、设置默认模型、配置 A/B 测试、检查状态，并为模型指定工作人员数量。指标
    API 允许您监视每个模型的性能。
- en: TorchServe runs all model instances and captures server logs. It processes the
    frontend APIs and manages the model storage to disk. TorchServe also provides
    a number of default handlers for common applications like object detection and
    text classification. The handlers take care of converting data from the API into
    a format that your model will process. This helps speed up deployment since you
    don’t have to write custom code for these common applications.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 运行所有模型实例并捕获服务器日志。它处理前端 API 并管理模型存储到磁盘。TorchServe 还为常见应用程序提供了许多默认处理程序，如目标检测和文本分类。处理程序负责将
    API 中的数据转换为模型将处理的格式。这有助于加快部署速度，因为您不必为这些常见应用程序编写自定义代码。
- en: Warning
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: TorchServe is experimental and subject to change.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 是实验性的，可能会发生变化。
- en: To deploy your models via TorchServe, you will need to follow a few steps. First
    you need to install TorchServe’s tools. Then you’ll package your model using the
    model archiver tool. Once your models are archived, you’ll then run the TorchServe
    web server. Once the web server is running, you can use its APIs to request predictions,
    manage your models, perform monitoring, or access server logs. Let’s take a look
    at how to perform each step.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 TorchServe 部署您的模型，您需要遵循几个步骤。首先，您需要安装 TorchServe 的工具。然后，您将使用模型存档工具打包您的模型。一旦您的模型被存档，您将运行
    TorchServe Web 服务器。一旦 Web 服务器运行，您可以使用其 API 请求预测，管理您的模型，执行监控，或访问服务器日志。让我们看看如何执行每个步骤。
- en: Install TorchServe and torch-model-archiver
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 TorchServe 和 torch-model-archiver
- en: AWS provides preinstalled machines with TorchServe in Amazon SageMaker or Amazon
    EC2 instances. If you’re using a different cloud provider, check with them to
    see if preinstalled instances exist before getting started. If you’re using a
    local server or need to install TorchServe, see the [TorchServe installation instructions](https://pytorch.tips/torchserve-install).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 在 Amazon SageMaker 或 Amazon EC2 实例中提供了预安装的 TorchServe 机器。如果您使用不同的云提供商，请在开始之前检查是否存在预安装实例。如果您使用本地服务器或需要安装
    TorchServe，请参阅[TorchServe安装说明](https://pytorch.tips/torchserve-install)。
- en: 'A simple approach to try is to install with `conda` or `pip`, as shown in the
    following command lines:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试的一个简单方法是使用 `conda` 或 `pip` 进行安装，如下所示：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you run into issues, refer to the TorchServe installation instructions at
    the preceding link.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到问题，请参考上述链接中的 TorchServe 安装说明。
- en: Package a model archive
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打包模型存档
- en: TorchServe has the ability to package all model artifacts into a single-model
    archive file. To do so, we will use the `torch-model-archiver` command-line tool
    that we installed in the previous step. It packages model checkpoints as well
    as the `state_dict` into a *.mar* file that the TorchServe server uses to serve
    the model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 有能力将所有模型工件打包到单个模型存档文件中。为此，我们将使用我们在上一步中安装的 `torch-model-archiver`
    命令行工具。它将模型检查点以及 `state_dict` 打包到一个 *.mar* 文件中，TorchServe 服务器将使用该文件来提供模型服务。
- en: You can use the `torch-model-archiver` to archive your TorchScript models as
    well as the standard “eager-mode” implementations, as shown in the following code.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `torch-model-archiver` 来存档您的 TorchScript 模型以及标准的 “eager 模式” 实现，如下所示。
- en: 'For a TorchScript moel, the command line is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TorchScript 模型，命令行如下：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We set the model as our example VGG16 model and use the saved serialized file,
    *model.pt*. In this case, we can use the default `image_classifier` handler as
    well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型设置为我们的示例 VGG16 模型，并使用保存的序列化文件 *model.pt*。在这种情况下，我们也可以使用默认的 `image_classifier`
    处理程序。
- en: 'For the standard eager-mode model we would use the following command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准的 eager 模式模型，我们将使用以下命令：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is similar to the previous command, but we also need to specify the model
    file, *model.py*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的命令类似，但我们还需要指定模型文件 *model.py*。
- en: 'The complete set of options for the `torch-model-archiver` tool is shown in
    the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch-model-archiver` 工具的完整选项集如下所示：'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Table 7-3\. Model archiver tool options
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-3\. 模型存档工具选项
- en: '| Options | Description |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 描述 |'
- en: '| --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `-h`, `--help` | Help message. After the help message is displayed, the program
    will exit. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `-h`, `--help` | 帮助消息。显示帮助消息后，程序将退出。|'
- en: '| `--model-name *MODEL_NAME*` | Exported model name. Exported file will be
    named as *<model-name>.mar* and saved in the current working directory if no `--export-path`
    is specified, else it will be saved under the export path. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `--model-name *MODEL_NAME*` | 导出的模型名称。导出的文件将命名为 *<model-name>.mar*，如果未指定
    `--export-path`，则将保存在当前工作目录中，否则将保存在导出路径下。|'
- en: '| `--serialized-file` `*SERIALIZED_FILE*` | Path to a _.pt_ or _.pth_ file
    containing `state_dict` in case of eager mode or an executable `ScriptModule`
    in case of TorchScript. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `--serialized-file` `*SERIALIZED_FILE*` | 指向包含 `state_dict` 的 _.pt_ 或 _.pth_
    文件的路径，对于 eager 模式，或者包含可执行 `ScriptModule` 的路径，对于 TorchScript。|'
- en: '| `--model-file *MODEL_FILE*` | Path to Python file containing the model architecture.
    This parameter is mandatory for eager-mode models. The model architecture file
    must contain only one class definition extended from `torch.nn.modules`. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `--model-file *MODEL_FILE*` | 指向包含模型架构的 Python 文件的路径。对于 eager 模式模型，此参数是必需的。模型架构文件必须只包含一个从
    `torch.nn.modules` 扩展的类定义。|'
- en: '| `--handler *HANDLER*` | *TorchServe*’s default handler name or Python file
    path to handle custom *TorchServe* inference logic. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `--handler *HANDLER*` | *TorchServe*的默认处理程序名称或处理自定义*TorchServe*推理逻辑的Python文件路径。
    |'
- en: '| `--extra-files *EXTRA_FILES*` | Comma-separated path to extra dependency
    files. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `--extra-files *EXTRA_FILES*` | 逗号分隔的额外依赖文件的路径。 |'
- en: '| `--runtime *{python, python2, python3}*` | The runtime specifies which language
    to run your inference code on. The default runtime is `RuntimeType.PYTHON`, but
    at present the following runtimes are supported: `python`, `python2`, and `python3`.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `--runtime *{python, python2, python3}*` | 运行时指定要在其上运行推理代码的语言。默认运行时为`RuntimeType.PYTHON`，但目前支持以下运行时：`python`，`python2`和`python3`。
    |'
- en: '| `--export-path *EXPORT_PATH*` | Path where the exported _.mar_ file will
    be saved. This is an optional parameter. If `--export-path` is not specified,
    the file will be saved in the current working directory. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `--export-path *EXPORT_PATH*` | 导出的_.mar_文件将保存在的路径。这是一个可选参数。如果未指定`--export-path`，文件将保存在当前工作目录中。
    |'
- en: '| `--archive-format *{tgz, no-archive, default}*` | The format in which the
    model artifacts are archived. `tgz` creates the model archive in *<model-name>.tar.gz*
    format. If platform hosting requires model artifacts to be in *.tar.gz*, use this
    option. `no-archive` creates a nonarchived version of model artifacts at *<export-path>*/*<model-name>*.
    As a result of this choice, a MANIFEST file will be created at that location without
    archiving these model files. `default` creates the model archive in *<model-name>.mar*
    format. This is the default archiving format. Models archived in this format will
    be readily hostable on *TorchServe*. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `--archive-format *{tgz, no-archive, default}*` | 存档模型工件的格式。`tgz`以*<model-name>.tar.gz*格式创建模型存档。如果平台托管需要模型工件为*.tar.gz*，请使用此选项。`no-archive`在*<export-path>*/*<model-name>*处创建模型工件的非存档版本。由于此选择，将在该位置创建一个MANIFEST文件，而不会对这些模型文件进行归档。`default`以*<model-name>.mar*格式创建模型存档。这是默认的归档格式。以此格式归档的模型将可以轻松托管在*TorchServe*上。
    |'
- en: '| `-f`,`--force` | When the `-f` or `--force` flag is specified, an existing
    *.mar* file with the same name as that provided in `--model-name` in the path
    specified by `--export-path` will be overwritten. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `-f`,`--force` | 当指定`-f`或`--force`标志时，将覆盖具有与`--model-name`中提供的相同名称的现有*.mar*文件，该文件位于`--export-path`指定的路径中。
    |'
- en: '| `-v`, `--version` | Model’s version. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `-v`, `--version` | 模型的版本。 |'
- en: '| `-r`, `--requirements-file` | Path to a *requirements.txt* file containing
    a list of model-specific Python packages to be installed by *TorchServe* for seamless
    model serving. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `-r`, `--requirements-file` | 指定包含要由*TorchServe*安装的模型特定Python包列表的*requirements.txt*文件的路径，以实现无缝模型服务。
    |'
- en: We can save our model archive *.mar* file in the */models* folder. We’ll use
    this as our model store. Next, let’s run the TorchServe web server.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型存档*.mar*文件保存在*/models*文件夹中。我们将使用这个作为我们的模型存储。接下来，让我们运行TorchServe Web服务器。
- en: Run TorchServe
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行TorchServe
- en: 'TorchServe includes a built-in web server that is run from the command line.
    It wraps one or more PyTorch models in a set of REST APIs and provides controls
    for configuring the port, host, and logging. The following command starts the
    web server with all models in the model store located in the */models* folder:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe包括一个从命令行运行的内置Web服务器。它将一个或多个PyTorch模型包装在一组REST API中，并提供控件以配置端口、主机和日志记录。以下命令启动Web服务器，其中所有模型都位于*/models*文件夹中的模型存储中：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A complete set of options is shown in [Table 7-4](#table_torchserve_options).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的选项集显示在[表7-4](#table_torchserve_options)中。
- en: Table 7-4\. TorchServe options
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-4\. TorchServe选项
- en: '| Options | Description |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 描述 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `--model-store` *+MODEL_STORE+* *+(mandatory)+* | Specifies the model store
    location where models can be loaded |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `--model-store` *+MODEL_STORE+* *+(mandatory)+* | 指定模型存储位置，可以加载模型 |'
- en: '| `-h`, `--help` | Shows the help message and exits |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `-h`, `--help` | 显示帮助消息并退出 |'
- en: '| `-v`, `--version` | Returns the TorchServe version |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `-v`, `--version` | 返回TorchServe的版本 |'
- en: '| `--start` | Starts the model server |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `--start` | 启动模型服务器 |'
- en: '| `--stop` | Stops the model server |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `--stop` | 停止模型服务器 |'
- en: '| `--ts-config` *+TS_CONFIG+* | Indicates the configuration file for TorchServe
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `--ts-config` *+TS_CONFIG+* | 指示TorchServe的配置文件 |'
- en: '| `--models` *+MODEL_PATH1 MODEL_NAME=MODEL_PATH2… [MODEL_PATH1 MODEL_NAME=MODEL_PATH2…
    …]+* | Indicates the models to be loaded using *`[model_name=]model_location`*
    format; locations can be an HTTP URL, a model archive file, or a directory that
    contains model archive files in `MODEL_STORE` |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `--models` *+MODEL_PATH1 MODEL_NAME=MODEL_PATH2… [MODEL_PATH1 MODEL_NAME=MODEL_PATH2…
    …]+* | 指示要使用*`[model_name=]model_location`*格式加载的模型；位置可以是HTTP URL、模型存档文件或包含模型存档文件的目录在`MODEL_STORE`中
    |'
- en: '| `--log-config` *+LOG_CONFIG+* | Indicates the log4j configuration file for
    TorchServe |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `--log-config` *+LOG_CONFIG+* | 指示TorchServe的log4j配置文件 |'
- en: '| `--ncs`, `--no-config-snapshots` | Disables the snapshot feature |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `--ncs`, `--no-config-snapshots` | 禁用快照功能 |'
- en: Now that the TorchServe web server is running, you can use the Inference API
    to send data and request predictions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在TorchServe Web服务器正在运行，您可以使用推理API发送数据并请求预测。
- en: Request predictions
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求预测
- en: 'You use the Inference API to pass data and request predictions. The Inference
    API listens on port 8080 and is only accessible from localhost by default. To
    change the default setting, refer to the [TorchServe documentation](https://pytorch.org/serve/configuration.html).
    To get predictions from the server, we use the Inference API’s `Service.Predictions`
    gRPC API and make a REST call to */predictions/<model_name>*, as shown using `curl`
    in the following command line:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以使用推理API传递数据并请求预测。推理API在端口8080上侦听，默认情况下仅从本地主机访问。要更改默认设置，请参阅[TorchServe文档](https://pytorch.org/serve/configuration.html)。要从服务器获取预测，我们使用推理API的`Service.Predictions`
    gRPC API，并通过REST调用到*/predictions/<model_name>*，如下面的命令行中使用`curl`所示： '
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code assumes we have an image file, *hot_dog.jpg.* The JSON-formatted response
    would look something like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码假设我们有一个图像文件*hot_dog.jpg.* JSON格式的响应看起来像这样：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can also use the Inference API to do a health check using the following
    request:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用推理API进行健康检查，使用以下请求：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The response will look like the following if the server is running:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务器正在运行，响应将如下所示：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For a full list of inference APIs use the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看推理API的完整列表，请使用以下命令：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Logging and monitoring
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志记录和监控
- en: 'You can configure metrics using the Metrics API and monitor and log your models’
    performance when deployed. The Metrics API listens on port 8082 and is only accessible
    from localhost by default, but you can change the default when configuring your
    TorchServe server. The following command illustrates how to access metrics:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Metrics API配置指标，并在部署时监视和记录模型的性能。Metrics API监听端口8082，默认情况下仅从本地主机访问，但在配置TorchServe服务器时可以更改默认设置。以下命令说明如何访问指标：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The default metrics endpoint returns Prometheus-formatted metrics. Prometheus
    is a free software application used for event monitoring and alerting that records
    real-time metrics in a time series database built using an HTTP pull model. You
    can query metrics using `curl` requests or point a Prometheus Server to the endpoint
    and use Grafana for dashboards. See the [Metrics API documentation](https://pytorch.tips/serve-metrics)
    for more details.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的指标端点返回Prometheus格式的指标。Prometheus是一个免费软件应用程序，用于事件监控和警报，它使用HTTP拉模型记录实时指标到时间序列数据库中。您可以使用`curl`请求查询指标，或者将Prometheus服务器指向端点并使用Grafana进行仪表板。有关更多详细信息，请参阅[Metrics
    API文档](https://pytorch.tips/serve-metrics)。
- en: Metrics are logged to a file. TorchServe also supports other types of server
    logging, including access logs and TorchServe logs. Access logs record the inference
    requests and the time it takes to complete the requests. As defined in the *properties*
    file, the access logs are collected in the *<log_location>/access_log.log* file.
    TorchServe logs collect all the logs from TorchServe and its backend workers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 指标被记录到文件中。TorchServe还支持其他类型的服务器日志记录，包括访问日志和TorchServe日志。访问日志记录推理请求以及完成请求所需的时间。根据*properties*文件的定义，访问日志被收集在*<log_location>/access_log.log*文件中。TorchServe日志收集来自TorchServe及其后端工作人员的所有日志。
- en: TorchServe supports capabilities beyond the default settings for metrics and
    logging. Metrics and logging can be configured in many different ways. In addition,
    you can create custom logs. For more information on metric and logging customization
    and other advanced features of TorchServe, refer to the [TorchServe documentation](https://pytorch.tips/torchserve).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe支持超出默认设置的指标和日志记录功能。指标和日志可以以许多不同的方式进行配置。此外，您可以创建自定义日志。有关TorchServe的指标和日志自定义以及其他高级功能的更多信息，请参阅[TorchServe文档](https://pytorch.tips/torchserve)。
- en: Note
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The *NVIDIA Triton Inference Server* is becoming more popular and is also used
    to deploy AI models at scale in production. Although not part of the PyTorch project,
    you may want to consider the Triton Inference Server as an alternative to TorchServe,
    especially when deploying to NVIDIA GPUs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*NVIDIA Triton推理服务器*变得越来越受欢迎，也用于在生产环境中规模部署AI模型。尽管不是PyTorch项目的一部分，但在部署到NVIDIA
    GPU时，您可能希望考虑Triton推理服务器作为TorchServe的替代方案。'
- en: 'The Triton Inference Server is open source software and can load models from
    local storage, GCP, or AWS S3\. Triton supports running multiple models on single
    or multiple GPUs, low latency and shared memory, and model ensembles. Some possible
    advantages of Triton over TorchServe include:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器是开源软件，可以从本地存储、GCP或AWS S3加载模型。Triton支持在单个或多个GPU上运行多个模型，低延迟和共享内存，以及模型集成。Triton相对于TorchServe的一些可能优势包括：
- en: Triton is out of beta.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Triton已经不再是测试版。
- en: It is the fastest way to infer on NVIDIA hardware (common).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是在NVIDIA硬件上进行推理的最快方式（常见）。
- en: It can use `int4` quantization.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以使用`int4`量化。
- en: You can port directly from PyTorch without ONNX.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以直接从PyTorch转换而无需ONNX。
- en: Available as a Docker container, Triton Inference Server also integrates with
    Kubernetes for orchestration, metrics, and auto-scaling. For more information,
    visit the [NVIDIA Triton Inference Server documentation](https://pytorch.tips/triton).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Docker容器提供，Triton推理服务器还与Kubernetes集成，用于编排、指标和自动扩展。有关更多信息，请访问[NVIDIA Triton推理服务器文档](https://pytorch.tips/triton)。
- en: ONNX
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ONNX
- en: If your platform doesn’t support PyTorch and you cannot use TorchScript/C++
    or TorchServe for your deployment, it may be possible that your deployment platform
    supports the Open Neural Network Exchange (ONNX) format. The ONNX format defines
    a common set of operators and a common file format so that deep learning engineers
    can use models across a variety of frameworks, tools, runtimes, and compilers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的平台不支持PyTorch，并且无法在部署中使用TorchScript/C++或TorchServe，那么您的部署平台可能支持Open Neural
    Network Exchange（ONNX）格式。ONNX格式定义了一组通用操作符和通用文件格式，以便深度学习工程师可以在各种框架、工具、运行时和编译器之间使用模型。
- en: ONNX was developed by Facebook and Microsoft to allow model interoperability
    between PyTorch and other frameworks, such as Caffe2 and Microsoft Cognitive Toolkit
    (CTK). ONNX is currently supported by inference runtimes from a number of providers,
    including Cadence Systems, Habana, Intel AI, NVIDIA, Qualcomm, Tencent, Windows,
    and Xilinx.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX是由Facebook和Microsoft开发的，旨在允许PyTorch和其他框架（如Caffe2和Microsoft认知工具包（CTK））之间的模型互操作性。目前，ONNX由多家供应商的推理运行时支持，包括Cadence
    Systems、Habana、Intel AI、NVIDIA、Qualcomm、腾讯、Windows和Xilinx。
- en: An example use case is edge deployment on a Xilinx FPGA device. FPGA devices
    are custom chips that can be programmed with specific logic. They are used by
    edge devices for low-latency or high-performance applications, like video. If
    you want to deploy your new innovative model to an FPGA device, you would first
    convert it to ONNX format and then use the Xilinx FPGA development tools to generate
    an FPGA image with your model’s implementation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例用例是在Xilinx FPGA设备上进行边缘部署。FPGA设备是可以使用特定逻辑编程的定制芯片。它们被边缘设备用于低延迟或高性能应用，如视频。如果您想将您的新创新模型部署到FPGA设备上，您首先需要将其转换为ONNX格式，然后使用Xilinx
    FPGA开发工具生成带有您模型实现的FPGA图像。
- en: 'Let’s take a look at an example of how to export a model to ONNX, again using
    our VGG16 model. The ONNX exporter can use tracing or scripting. We learned about
    tracing and scripting, described in the earlier section on TorchScript. We can
    use tracing by simply providing the model and an example input. The following
    code shows how we’d export our VGG16 model to ONNX using tracing:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个如何将模型导出为ONNX的示例，再次使用我们的VGG16模型。ONNX导出器可以使用追踪或脚本。我们在TorchScript的早期部分学习了关于追踪和脚本的内容。我们可以通过简单地提供模型和一个示例输入来使用追踪。以下代码显示了我们如何使用追踪将我们的VGG16模型导出为ONNX：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We define an example input and call `torch.onnx.export()`. The resulting file,
    *vgg16.onnx*, is a binary protobuf file that contains both the network structure
    and the parameters of the VGG16 model we exported.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个示例输入并调用`torch.onnx.export()`。生成的文件*vgg16.onnx*是一个包含我们导出的VGG16模型的网络结构和参数的二进制protobuf文件。
- en: 'If we want to verify that our model was converted to ONNX properly, we can
    use the ONNX checker, as shown in the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要验证我们的模型是否正确转换为ONNX，我们可以使用ONNX检查器，如下所示：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This code uses the Python ONNX library to load the model, run the checker, and
    print out a human-readable version of the model. You may need to install the ONNX
    library before running the code, using `conda` or `pip`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用Python ONNX库加载模型，运行检查器，并打印出模型的可读版本。在运行代码之前，您可能需要安装ONNX库，使用`conda`或`pip`。
- en: To learn more about converting to ONNX or running in an ONNX runtime, check
    out the [ONNX tutorial](https://pytorch.tips/onnx-tutorial) on the PyTorch website.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于转换为ONNX或在ONNX运行时中运行的信息，请查看PyTorch网站上的[ONNX教程](https://pytorch.tips/onnx-tutorial)。
- en: In addition to TorchScript, TorchServe, and ONNX, more tools are being developed
    to support PyTorch model deployment. Let’s consider some tools used to deploy
    models to mobile platforms.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 除了TorchScript、TorchServe和ONNX之外，还正在开发更多工具来支持PyTorch模型部署。让我们考虑一些用于将模型部署到移动平台的工具。
- en: Mobile Libraries
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移动库
- en: Android and iPhone devices are continuously evolving and adding native support
    for deep learning acceleration in their custom chipsets. In addition, there is
    a growing need to reduce latency, preserve privacy, and interact seamlessly with
    deep learning models in applications such as augmented reality (AR). Deployment
    to mobile devices is further complicated due to mobile runtimes that can significantly
    differ from the training environments used by developers, leading to errors and
    challenges during mobile deployment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Android和iPhone设备不断发展，并在其定制芯片组中添加对深度学习加速的本机支持。此外，由于需要减少延迟、保护隐私以及在应用程序中与深度学习模型无缝交互的增长需求，部署到移动设备变得更加复杂。这是因为移动运行时可能与开发人员使用的训练环境有显著不同，导致在移动部署过程中出现错误和挑战。
- en: PyTorch Mobile addresses these challenges and provides an end-to-end workflow
    to go from training to mobile deployment. PyTorch Mobile is available for iOS,
    Android, and Linux and provides APIs for the preprocessing and integration tasks
    needed for mobile applications. The basic workflow is shown in [Figure 7-2](#fig_pytorch_mobile).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Mobile解决了这些挑战，并提供了一个从训练到移动部署的端到端工作流程。PyTorch Mobile可用于iOS、Android和Linux，并提供用于移动应用程序所需的预处理和集成任务的API。基本工作流程如[图7-2](#fig_pytorch_mobile)所示。
- en: '![“PyTorch Mobile Workflow”](Images/ptpr_0702.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![“PyTorch Mobile Workflow”](Images/ptpr_0702.png)'
- en: Figure 7-2\. PyTorch mobile workflow
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. PyTorch移动工作流程
- en: You start by designing your model in PyTorch as you normally would. Then you
    may quantize your model to reduce its complexity with minimal degradation in performance.
    Subsequently, you would use tracing or scripting to convert to TorchScript and
    optimize your model for mobile devices using `torch.utils`. Next, save your model
    and use the appropriate mobile library for deployment. Android uses the Maven
    PyTorch library and iOS uses CocoPods with the LibTorch pod.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像通常在PyTorch中设计模型一样开始。然后，您可以对模型进行量化，以减少其复杂性，同时最小程度地降低性能。随后，您可以使用追踪或脚本转换为TorchScript，并使用`torch.utils`优化模型以适用于移动设备。接下来，保存您的模型并使用适当的移动库进行部署。Android使用Maven
    PyTorch库，iOS使用CocoPods与LibTorch pod。
- en: Warning
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: PyTorch Mobile is still being developed and is subject to change.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Mobile仍在开发中，可能会发生变化。
- en: For the latest details on PyTorch Mobile, refer to the [PyTorch Mobile documentation](https://pytorch.tips/mobile).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PyTorch Mobile的最新详细信息，请参考[PyTorch Mobile文档](https://pytorch.tips/mobile)。
- en: Now that we’ve explored some PyTorch tools available for deploying our models,
    let’s take a look at some reference applications and code for deployment to the
    cloud and to edge devices. First I’ll show you how to build a web server for development
    using Flask.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了一些PyTorch工具，用于部署我们的模型，让我们看一些参考应用程序和代码，用于部署到云端和边缘设备。首先，我将向您展示如何使用Flask构建开发Web服务器。
- en: Deploying to a Flask App
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署到Flask应用程序
- en: Before deploying to full-scale production, you may want to deploy your models
    to a development web server. This enables you to integrate your deep learning
    algorithms with other systems and quickly build prototypes to demonstrate your
    new models. One of the easiest ways to build a development server is with Python
    using Flask.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署到全面生产之前，您可能希望将模型部署到开发Web服务器。这使您能够将深度学习算法与其他系统集成，并快速构建原型以演示您的新模型。使用Python使用Flask构建开发服务器的最简单方法之一。
- en: Flask is a simple micro web framework written in Python. It is called a “micro”
    framework because it does not include a database abstraction layer, form validation,
    upload handling, various authentication technologies, or anything else that might
    be provided with other libraries. We won’t cover Flask in depth in this book,
    but I’ll show you how to use Flask to deploy your models in Python.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Flask是一个用Python编写的简单微型Web框架。它被称为“微”框架，因为它不包括数据库抽象层、表单验证、上传处理、各种身份验证技术或其他可能由其他库提供的内容。我们不会在本书中深入讨论Flask，但我会向您展示如何使用Flask在Python中部署您的模型。
- en: We’ll also expose a REST API so that other applications can pass in data and
    receive predictions. In the following examples, we’ll deploy our pretrained VGG16
    model and classify images. First we’ll define our API endpoints, request types,
    and response types. Our API endpoint will be at */predict*, which takes in POST
    requests (including the image file). The response will be in JSON format and contain
    a `class_id` and `class_name` from the ImageNet dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将公开一个REST API，以便其他应用程序可以传入数据并接收预测。在以下示例中，我们将部署我们预训练的VGG16模型并对图像进行分类。首先，我们将定义我们的API端点、请求类型和响应类型。我们的API端点将位于*/predict*，接受POST请求（包括图像文件）。响应将以JSON格式返回，并包含来自ImageNet数据集的`class_id`和`class_name`。
- en: 'Let’s create our main Flask file, called *app.py*. First we’ll import the required
    packages:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的主要Flask文件，称为*app.py*。首先我们将导入所需的包：
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ll be using `io` to convert bytes to an image and `json` to handle JSON-formatted
    data. We’ll be using `torchvision` to create our VGG16 model and transform the
    image data into the appropriate format for our model. Finally, we import `Flask`,
    `jsonnify`, and `request` to handle the API requests and responses.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`io`将字节转换为图像，使用`json`处理JSON格式数据。我们将使用`torchvision`创建我们的VGG16模型，并将图像数据转换为适合我们模型的格式。最后，我们导入`Flask`、`jsonnify`和`request`来处理API请求和响应。
- en: 'Before we create our web server, let’s define a `get_prediction()` function
    that reads in image data, preprocesses it, passes it into our model, and returns
    the image class:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的Web服务器之前，让我们定义一个`get_prediction()`函数，该函数读取图像数据，预处理它，将其传递到我们的模型，并返回图像类别：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Since our model will return a number indicating the class, we’ll need a lookup
    table to convert this number to a class name. We create a dictionary called `imagenet_class_index`
    by reading in the JSON conversion file. We then instantiate our VGG16 model and
    define our image transforms to preprocess a PIL image by resizing it, center-cropping
    it, converting it to a tensor, and normalizing it. These steps are required prior
    to sending the image into our model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型将返回一个表示类别的数字，我们需要一个查找表将此数字转换为类别名称。我们通过读取JSON转换文件创建一个名为`imagenet_class_index`的字典。然后，我们实例化我们的VGG16模型，并定义我们的图像转换，以预处理一个PIL图像，将其调整大小、中心裁剪、转换为张量并进行归一化。在将图像发送到我们的模型之前，这些步骤是必需的。
- en: Our `get_prediction()` function creates a PIL image object based on the received
    bytes and applies the required image transforms to create an input tensor. Next,
    we perform the forward pass (or model inference) and find the class with highest
    probability, `y`. Last, we look up the class name using the output class value.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`get_prediction()`函数基于接收到的字节创建一个PIL图像对象，并应用所需的图像转换以创建输入张量。接下来，我们执行前向传递（或模型推断），并找到具有最高概率的类别`y`。最后，我们使用输出类值查找类名。
- en: 'Now that we have code that preprocesses an image, passes it through our model,
    and returns the predicted class, we can create our Flask web server and endpoints
    and deploy our model, as shown in the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了预处理图像、通过我们的模型传递图像并返回预测类别的代码，我们可以创建我们的Flask Web服务器和端点，并部署我们的模型，如下所示的代码：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our web server object is called `app`. We’ve created it, but it’s not running
    yet. We set our endpoint to */predict* and configured it to handle POST requests.
    When the web server receives the POST, it will execute the `predict()` function
    that reads the image, gets the prediction, and returns the image class in JSON
    format.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Web服务器对象称为`app`。我们已经创建了它，但它还没有运行。我们将我们的端点设置为*/predict*，并配置它以处理POST请求。当Web服务器接收到POST请求时，它将执行`predict()`函数，读取图像，获取预测，并以JSON格式返回图像类别。
- en: 'That’s it! Now we just need to add the following code so that the web server
    runs when we execute *app.py*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在我们只需要添加以下代码，以便在执行*app.py*时运行Web服务器：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To test our web server, we can run it as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试我们的Web服务器，可以按照以下方式运行它：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can send an image using a simple Python file and the `requests` library:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个简单的Python文件和`requests`库发送图像：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this example, we’re running a web server on our local machine on port 5000
    (`localhost:5000`). You may want to run your development web server in Google
    Colab to take advantage of cloud GPUs. I’ll show you how to do so next.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在本地机器的端口5000（`localhost:5000`）上运行一个Web服务器。您可能希望在Google Colab中运行开发Web服务器，以利用云GPU。接下来我将向您展示如何做到这一点。
- en: Colab Flask App
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Colab Flask应用程序
- en: Perhaps you’ve been developing your PyTorch models in Colab to take advantage
    of its rapid development or its GPUs. Colab provides a virtual machine (VM) which
    routes its `localhost` to our machine’s local host. To expose it to a public URL,
    we can use a library called `ngrok`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 也许您一直在Colab中开发您的PyTorch模型，以利用其快速开发或其GPU。Colab提供了一个虚拟机（VM），它将其`localhost`路由到我们机器的本地主机。要将其暴露到公共URL，我们可以使用一个名为`ngrok`的库。
- en: 'First install `ngrok` in Colab:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在Colab中安装`ngrok`：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To run our Flask app with `ngrok`, all we need to do is add two lines of code,
    as shown in the following annotations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`ngrok`运行我们的Flask应用程序，我们只需要添加两行代码，如下注释所示：
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO1-1)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO1-1)'
- en: Import the `ngrok` library.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`ngrok`库。
- en: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO1-2)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO1-2)'
- en: Starts `ngrok` when the app is run.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序运行时启动`ngrok`。
- en: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO1-3)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO1-3)'
- en: Since we’re running in Colab, we don’t need to check for `main`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在Colab中运行，我们不需要检查`main`。
- en: 'I’ve omitted the other imports and the `get_prediction()` function as they
    do not change. Now you can run your development web server in Colab for even faster
    prototyping. The `ngrok` library provides a secure URL for the server running
    in Colab; you’ll find the URL in the Colab notebook output when running the Flask
    app. For example, the following output shows that the URL is *[*http://c0c97117ba27.ngrok.io*](http://c0c97117ba27.ngrok.io)*:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经省略了其他导入和`get_prediction()`函数，因为它们没有改变。现在您可以在Colab中运行开发Web服务器，以便更快地进行原型设计。`ngrok`库为在Colab中运行的服务器提供了安全的URL；当运行Flask应用程序时，您将在Colab笔记本输出中找到URL。例如，以下输出显示URL为[*http://c0c97117ba27.ngrok.io*](http://c0c97117ba27.ngrok.io)：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once again, you can send a POST request with an image to test the web server.
    You can run the following code locally or in another Colab notebook:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，您可以发送带有图像的POST请求来测试Web服务器。您可以在本地或另一个Colab笔记本中运行以下代码：
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice the URL has changed. Deploying your model in a Flask app is a good way
    to quickly test it and the `get_prediction()` function with a REST API. However,
    our Flask app here is used as a development web server, not for production deployment.
    When deploying your models at scale, you will need to address things like model
    management, A/B testing, monitoring, logging, and other tasks to ensure your model
    server is working properly. To deploy to production at scale, we’ll use TorchServe.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意URL已更改。在Flask应用程序中部署您的模型是快速测试和使用REST API的`get_prediction()`函数的好方法。但是，在这里我们的Flask应用程序用作开发Web服务器，而不是用于生产部署。在大规模部署模型时，您将需要解决模型管理、A/B测试、监控、日志记录和其他任务等问题，以确保您的模型服务器正常工作。要在大规模生产环境中部署，我们将使用TorchServe。
- en: Deploying to the Cloud with TorchServe
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TorchServe部署到云端
- en: In this example, we’ll deploy our VGG16 image classifier to a production environment.
    Let’s pretend our company makes a software tool that will sort collections of
    retail product images into categories depending on which objects appear in the
    images. The company is growing rapidly and now supports millions of small businesses
    that use the tool daily.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将将VGG16图像分类器部署到生产环境。假设我们的公司制作了一个软件工具，将根据图像中出现的对象对零售产品图像集进行分类。该公司正在迅速发展，现在支持数百万家每天使用该工具的小型企业。
- en: As part of the machine learning engineering team, you’ll need to deploy your
    model to production and provide a simple REST API that the software tool will
    use to classify its images. Because we want to deploy something as quickly as
    possible, we’ll use a Docker container in an AWS EC2 instance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习工程团队的一部分，您需要将模型部署到生产环境，并提供一个简单的REST API，软件工具将使用该API对其图像进行分类。因为我们希望尽快部署某些东西，所以我们将在AWS
    EC2实例中使用Docker容器。
- en: Quick Start with Docker
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker快速入门
- en: TorchServe provides scripts to create Docker images based on a variety of platforms
    and options. Running a Docker container eliminates the need to reinstall all the
    dependencies required to run TorchServe. In addition, we can scale our model inference
    by spinning multiple Docker containers using Kubernetes. First we must create
    the Docker image according to the resources we have on our EC2 instance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe提供了脚本，可以基于各种平台和选项创建Docker镜像。运行Docker容器可以消除重新安装运行TorchServe所需的所有依赖项的需要。此外，我们可以通过使用Kubernetes旋转多个Docker容器来扩展我们的模型推理。首先，我们必须根据我们在EC2实例上拥有的资源创建Docker镜像。
- en: 'The first step is to clone the TorchServe repository and navigate to the *Docker*
    folder, using the following commands:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是克隆TorchServe存储库并导航到*Docker*文件夹，使用以下命令：
- en: '[PRE29]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next we’ll need to add our model archive for VGG16 into the Docker image. We
    do this by adding the following line to the Dockerfile that downloads the archived
    model file and saves it within the */home/model-server/* directory:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将VGG16模型归档添加到Docker镜像中。我们通过向下载存档模型文件并将其保存在*/home/model-server/*目录中的Dockerfile添加以下行来实现这一点：
- en: '[PRE30]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now run the *build_image.sh* script to create a Docker image with the
    public binaries installed. Since we’re running on an EC2 instance with a GPU,
    we’ll use the `-g` flag, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行*build_image.sh*脚本，创建一个安装了公共二进制文件的Docker镜像。由于我们在带有GPU的EC2实例上运行，我们将使用`-g`标志，如下所示：
- en: '[PRE31]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can run **`./build_image.sh -h`** to see additional options.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以运行**`./build_image.sh -h`**查看其他选项。
- en: 'Once our Docker image is created, we can run the container with the following
    command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了Docker镜像，我们可以使用以下命令运行容器：
- en: '[PRE32]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This command will start the container with the 8080/81/82 and 7070/71 ports
    exposed to the outer world’s localhost. It uses one GPU with the latest CUDA version.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将启动容器，将8080/81/82和7070/71端口暴露给外部世界的本地主机。它使用一个带有最新CUDA版本的GPU。
- en: Now our TorchServe Docker container is running. Our company’s software tool
    can send inference requests by sending the image file to *ourip.com/predict* and
    can receive image classifications via JSON.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的TorchServe Docker容器正在运行。我们公司的软件工具可以通过将图像文件发送到*ourip.com/predict*来发送推理请求，并可以通过JSON接收图像分类。
- en: For more details on running TorchServe in Docker, refer to the [TorchServe Docker
    documentation](https://pytorch.tips/torchserve-docker). To learn more about TorchServe,
    visit the [TorchServe repository](https://pytorch.tips/torchserve-github).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Docker中运行TorchServe的更多详细信息，请参阅[TorchServe Docker文档](https://pytorch.tips/torchserve-docker)。要了解有关TorchServe的更多信息，请访问[TorchServe存储库](https://pytorch.tips/torchserve-github)。
- en: 'Now you can deploy your models to your local machine and cloud servers using
    Flask for development or TorchServe for production. This is useful for prototyping
    and integrating with other applications through a REST API. Next, you’ll expand
    your deployment capabilities outside of the cloud: in the following section we’ll
    explore how you would deploy models to mobile devices and other edge devices.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用Flask进行开发或使用TorchServe进行生产，将模型部署到本地计算机和云服务器。这对于原型设计和通过REST API与其他应用程序集成非常有用。接下来，您将扩展部署能力，将模型部署到云之外：在下一节中，我们将探讨如何将模型部署到移动设备和其他边缘设备。
- en: Deploying to Mobile and Edge
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署到移动和边缘
- en: Edge devices are (usually small) hardware systems that interface directly with
    the user or environment and run machine learning computations directly on the
    device instead of on a centralized server in the cloud. Some examples of edge
    devices include mobile phones and tablets, wearables like smart watches and heart
    rate monitors, and other IoT devices such as industrial sensors and home thermostats.
    There’s a growing need to run deep learning algorithms on edge devices in order
    to maintain privacy, reduce data transfer, minimize latency, and support new interactive
    use cases in real time.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘设备通常是与用户或环境直接交互并在设备上直接运行机器学习计算的（通常是小型）硬件系统，而不是在云中的集中服务器上运行。一些边缘设备的例子包括手机和平板电脑，智能手表和心率监测器等可穿戴设备，以及工业传感器和家用恒温器等其他物联网设备。有一个越来越大的需求在边缘设备上运行深度学习算法，以保护隐私，减少数据传输，最小化延迟，并支持实时的新交互式用例。
- en: First we’ll explore how to deploy your PyTorch models on mobile devices with
    iOS and Android, then we’ll cover other edge devices. PyTorch’s support for edge
    deployment is limited but growing. These sections will provide some reference
    code to help you get started using PyTorch Mobile.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将探讨如何在iOS和Android移动设备上部署PyTorch模型，然后我们将涵盖其他边缘设备。PyTorch对边缘部署的支持有限但在不断增长。这些部分将提供一些参考代码，帮助您开始使用PyTorch
    Mobile。
- en: iOS
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iOS
- en: According to Apple, as of January 2021 there were over 1.65 billion active iOS
    devices in the world. The support for machine learning hardware acceleration continues
    to grow with each new model and custom processing unit. Learning how to deploy
    your PyTorch models to iOS opens the doors for many opportunities to create an
    iOS app based on deep learning.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 根据苹果公司的数据，截至2021年1月，全球有超过16.5亿活跃的iOS设备。随着每个新模型和定制处理单元的推出，对机器学习硬件加速的支持不断增长。学习如何将PyTorch模型部署到iOS为您打开了许多机会，可以基于深度学习创建iOS应用程序。
- en: To deploy your model to an iOS device, you’ll need to learn how to create an
    iOS application using development tools like Xcode. We won’t cover iOS development
    in this book, but you can find a “Hello, World” program and sample code to help
    you build your app at the [PyTorch iOS Example Apps GitHub repository](https://pytorch.tips/ios-demo).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型部署到iOS设备，您需要学习如何使用Xcode等开发工具创建iOS应用程序。我们不会在本书中涵盖iOS开发，但您可以在[PyTorch iOS示例应用GitHub存储库](https://pytorch.tips/ios-demo)中找到“Hello,
    World”程序和示例代码，以帮助您构建您的应用程序。
- en: Let’s describe the workflow for deploying our VGG16 network to an iOS application.
    iOS will use the PyTorch C++ API to interface with our model, so we’ll need to
    convert and save our model to TorchScript first. Then we’ll wrap the C++ function
    in Objective-C so iOS Swift code can access the API. We’ll use Swift to load and
    preprocess an image, and then we’ll pass the image data into our model to predict
    its class.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述将我们的VGG16网络部署到iOS应用程序的工作流程。iOS将使用PyTorch C++ API与我们的模型进行接口，因此我们需要首先将我们的模型转换并保存为TorchScript。然后我们将在Objective-C中包装C++函数，以便iOS
    Swift代码可以访问API。我们将使用Swift加载和预处理图像，然后将图像数据传入我们的模型以预测其类别。
- en: 'First we will convert our VGG16 model to TorchScript using tracing and save
    it as *model.pt*, as shown in the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将使用追踪将我们的VGG16模型转换为TorchScript并保存为*model.pt*，如下面的代码所示：
- en: '[PRE33]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO2-1)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_deploying_pytorch_to_production_CO2-1)'
- en: Define `example` using random data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机数据定义`example`。
- en: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO2-2)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_deploying_pytorch_to_production_CO2-2)'
- en: Convert `model` to TorchScript.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 将`model`转换为TorchScript。
- en: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO2-3)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_deploying_pytorch_to_production_CO2-3)'
- en: New step to optimize the code.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 优化代码的新步骤。
- en: '[![4](Images/4.png)](#co_deploying_pytorch_to_production_CO2-4)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_deploying_pytorch_to_production_CO2-4)'
- en: Save the model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型。
- en: As described earlier, using tracing requires defining an example input, and
    we do so using random data. Then we convert the model to TorchScript using `torch.jit.trace()`.
    We then add a new step to optimize the TorchScript code for mobile platforms using
    the `torch.utils.mobile_optimizer` package. Finally, we save the model to a file
    named *model.pt*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用追踪需要定义一个示例输入，我们使用随机数据来做到这一点。然后我们使用`torch.jit.trace()`将模型转换为TorchScript。然后我们添加一个新步骤，使用`torch.utils.mobile_optimizer`包为移动平台优化TorchScript代码。最后，我们将模型保存到名为*model.pt*的文件中。
- en: 'Now we’ll need to write our Swift iOS application. Our iOS app will use the
    PyTorch C++ library, which we can install via CocoaPods as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要编写我们的Swift iOS应用程序。我们的iOS应用程序将使用PyTorch C++库，我们可以通过CocoaPods安装如下：
- en: '[PRE34]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then we need to write some Swift code to load a sample image. You can improve
    this in the future by accessing the camera or photos on the device, but for now
    we’ll keep it simple:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要编写一些Swift代码来加载一个示例图像。您可以在将来通过访问设备上的相机或照片来改进这一点，但现在我们将保持简单：
- en: '[PRE35]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here we resize the image to 224 × 224 pixels and run a function to normalize
    the image data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将图像调整大小为224×224像素，并运行一个函数来规范化图像数据。
- en: 'Next we load and instantiate our model into our iOS app, as shown in the following
    code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载并实例化我们的模型到我们的iOS应用程序中，如下面的代码所示：
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: iOS is written in Swift, and Swift cannot interface to C++, so we need to use
    an Objective-C class, `TorchModule`, as a wrapper for `torch::jit::script::Module`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: iOS是用Swift编写的，Swift无法与C++进行接口，因此我们需要使用一个Objective-C类`TorchModule`作为`torch::jit::script::Module`的包装器。
- en: 'Now that our model is loaded, we can predict an image’s class by passing the
    preprocessed image data into our model and running a prediction, as shown in the
    following code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已加载，我们可以通过将预处理的图像数据传入我们的模型并运行预测来预测图像的类别，如下面的代码所示：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Under the hood, the `predict()` Objective-C wrapper calls the C++ `forward()`
    function as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`predict()` Objective-C包装器调用C++ `forward()`函数如下：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: When you run the sample app, you should see output similar to [Figure 7-3](#fig_ios_example)
    for the sample image file.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行示例应用程序时，您应该看到类似于[图7-3](#fig_ios_example)的输出，用于示例图像文件。
- en: 'This image classification example is only a small representation of the capabilities
    of coding for iOS devices. For more advanced use cases, you can still follow the
    same process: convert and save to TorchScript, create an Objective-C wrapper,
    preprocess the input, and call your `predict()` function. Next, we’ll follow a
    similar process for deploying PyTorch to Android mobile devices.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像分类示例只是对为iOS设备编码能力的一个小表示。对于更高级的用例，您仍然可以遵循相同的流程：转换并保存为TorchScript，创建一个Objective-C包装器，预处理输入，并调用您的`predict()`函数。接下来，我们将为部署PyTorch到Android移动设备遵循类似的流程。
- en: '![“iOS Example”](Images/ptpr_0703.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![“iOS示例”](Images/ptpr_0703.png)'
- en: Figure 7-3\. iOS example
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. iOS示例
- en: Android
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Android
- en: Android mobile devices are also abundantly used throughout the world, with the
    OS estimated to have a market share of over 70% in mobile devices at the start
    of 2021\. This means there is also a huge opportunity to deploy PyTorch models
    to Android devices.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Android移动设备在全球范围内也被广泛使用，据估计，2021年初移动设备的市场份额超过70%。这意味着也有巨大的机会将PyTorch模型部署到Android设备上。
- en: Android uses the PyTorch Android API, and you will need to install the Android
    development tools to build a sample app. Using Android Studio, you will be able
    to install the Android native development kit (NDK) and software development kit
    (SDK). We won’t cover Android development in the book, but you can find a “Hello,
    World” program and sample code to help you build your app at the [PyTorch Android
    Example GitHub repository](https://pytorch.tips/android-demo).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Android使用PyTorch Android API，您需要安装Android开发工具来构建示例应用程序。使用Android Studio，您将能够安装Android本机开发工具包（NDK）和软件开发工具包（SDK）。我们不会在本书中涵盖Android开发，但您可以在[PyTorch
    Android示例GitHub存储库](https://pytorch.tips/android-demo)中找到“Hello, World”程序和示例代码，以帮助您构建您的应用程序。
- en: The workflow for deploying a PyTorch model on an Android device is very similar
    to the process we used for iOS. We’ll still need to convert our model to TorchScript
    to use it with the PyTorch Android API. However, since the API natively supports
    loading and running our TorchScript model, we do not need to wrap it in C++ code
    as we did with iOS. Instead, we’ll use Java to write an Android app that loads
    and preprocesses an image file, passes it to our model for inference, and returns
    the results.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在Android设备上部署PyTorch模型的工作流程与我们用于iOS的过程非常相似。我们仍然需要将我们的模型转换为TorchScript以便与PyTorch
    Android API一起使用。然而，由于API本身支持加载和运行我们的TorchScript模型，我们无需像在iOS中那样将其包装在C++代码中。相反，我们将使用Java编写一个Android应用程序，该应用程序加载和预处理图像文件，将其传递给我们的模型进行推理，并返回结果。
- en: 'Let’s deploy our VGG16 model to Android. First we convert the model to TorchScript
    just like we did for iOS, as shown in the following code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将VGG16模型部署到Android。首先，我们将模型转换为TorchScript，就像我们为iOS所做的那样，如下面的代码所示：
- en: '[PRE39]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We convert the model to TorchScript using tracing with `torch.jit.trace()`.
    We then add a new step to optimize the TorchScript code for mobile platforms using
    the `torch.utils.mobile_optimizer` package. Finally, we save the model to a file
    named *model.pt*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`torch.jit.trace()`进行跟踪将模型转换为TorchScript。然后，我们添加一个新步骤，使用`torch.utils.mobile_optimizer`包为移动平台优化TorchScript代码。最后，我们将模型保存到名为*model.pt*的文件中。
- en: 'Next, we create our Android app using Java. We add the PyTorch Android API
    to our app as a Gradle dependency by adding the following code to *build.gradle*:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Java创建我们的Android应用程序。我们通过将以下代码添加到*build.gradle*将PyTorch Android API添加到我们的应用程序作为Gradle依赖项：
- en: '[PRE40]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we write our Android app. We start by loading an image and preprocessing
    it with the following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写我们的Android应用程序。我们首先加载图像并使用以下代码对其进行预处理：
- en: '[PRE41]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now that we have our image, we can predict its class, but first we must load
    our model, as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的图像，我们可以预测它的类别，但首先我们必须加载我们的模型，如下所示：
- en: '[PRE42]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then we can run inference to predict the image’s class and process the results
    with the following code:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以运行推理来预测图像的类别，并使用以下代码处理结果：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This workflow can be used for more advanced use cases. You can use the camera
    or photos on the device or other Android sensors to create more complex apps.
    For more PyTorch Android demo applications, visit the [PyTorch Android Demo App
    GitHub repository](https://pytorch.tips/android-demo-repo).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流程可以用于更高级的用例。您可以使用设备上的相机或照片或其他Android传感器来创建更复杂的应用程序。有关更多PyTorch Android演示应用程序，请访问[PyTorch
    Android演示应用程序GitHub存储库](https://pytorch.tips/android-demo-repo)。
- en: Other Edge Devices
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他边缘设备
- en: Mobile devices running iOS or Android represent one type of edge device, but
    there are many more that can execute deep learning algorithms. Edge devices are
    often built using custom hardware for a specific application. Examples of other
    edge devices include sensors, video equipment, medical monitors, software-defined
    radios, thermostats, farming machines, and manufacturing sensors to detect defects.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 运行iOS或Android的移动设备代表一种边缘设备，但还有许多可以执行深度学习算法的设备。边缘设备通常使用定制硬件构建用于特定应用。其他边缘设备的示例包括传感器、视频设备、医疗监视器、软件定义无线电、恒温器、农业机械和制造传感器以检测缺陷。
- en: 'Most edge devices include computer processors, GPUs, FPGAs, or other custom
    ASIC computer chips that are capable of running deep learning models. So how do
    you deploy your PyTorch models to these edge devices? Well, it depends on what
    processing components are used on the device. Let’s explore some ideas for commonly
    used chips:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数边缘设备包括计算机处理器、GPU、FPGA或其他定制ASIC计算机芯片，能够运行深度学习模型。那么，如何将PyTorch模型部署到这些边缘设备呢？这取决于设备上使用了哪些处理组件。让我们探讨一些常用芯片的想法：
- en: CPUs
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: CPU
- en: If your edge device uses a CPU, such as an Intel or AMD processor, PyTorch can
    be deployed in Python and C++ using both TorchScript and the C++ frontend API.
    Mobile and edge CPU chipsets are usually optimized to minimize power, and memory
    may be more limited on an edge device. It may be worthwhile to optimize your models
    using pruning or quantization prior to deployment to minimize the power and memory
    required to run inference.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的边缘设备使用CPU，如英特尔或AMD处理器，PyTorch可以在Python和C++中使用TorchScript和C++前端API部署。移动和边缘CPU芯片组通常经过优化以最小化功耗，并且边缘设备上的内存可能更有限。在部署之前通过修剪或量化优化您的模型以最小化运行推理所需的功耗和内存可能是值得的。
- en: ARMs
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ARMs
- en: ARM processors are a family of computer processors with a reduced set of instructions.
    They typically run at lower power and clock speeds than Intel or AMD CPUs and
    can be included within Systems on a Chip (SoCs). In addition to the processor,
    SoCs chips usually include other electronics such as programmable FPGA logic or
    GPUs. Running PyTorch in Linux on ARM devices is currently under development.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ARM处理器是一类具有简化指令集的计算机处理器。它们通常以比英特尔或AMD CPU更低的功耗和时钟速度运行，并可以包含在片上系统（SoCs）中。除了处理器，SoCs芯片通常还包括其他电子设备，如可编程FPGA逻辑或GPU。目前正在开发在ARM设备上运行PyTorch的Linux。
- en: Microcontrollers
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 微控制器
- en: Microcontrollers are very limited processors that are usually aimed at very
    simple control tasks. Some popular microcontrollers include Arduino and Beaglebone
    processors. Support for microcontrollers is limited due to the few resources available.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 微控制器是通常用于非常简单控制任务的非常有限的处理器。一些流行的微控制器包括Arduino和Beaglebone处理器。由于可用资源有限，对微控制器的支持有限。
- en: GPUs
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: GPUs
- en: Edge devices may include GPU chips. NVIDIA GPUs, are the most widely supported
    GPUs, but other companies (such as AMD and Intel) manufacture GPU chips as well.
    NVIDIA supports PyTorch in its GPU development kits, including its Jetson Nano,
    Xavier, and NX boards.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘设备可能包括GPU芯片。NVIDIA GPU是最广泛支持的GPU，但其他公司（如AMD和英特尔）也制造GPU芯片。NVIDIA在其GPU开发套件中支持PyTorch，包括其Jetson
    Nano、Xavier和NX板。
- en: FPGAs
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: FPGAs
- en: PyTorch models can be deployed to many FPGA devices, including Xilinx (recently
    acquired by AMD) and Intel FPGA device families. Neither platform supports direct
    PyTorch deployment; however, they do support the ONNX format. The typical approach
    is to convert PyTorch models to ONNX and use the FPGA development tools to create
    FPGA logic from the ONNX model.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch模型可以部署到许多FPGA设备，包括赛灵思（最近被AMD收购）和英特尔FPGA设备系列。这两个平台都不支持直接的PyTorch部署；但是它们支持ONNX格式。典型的方法是将PyTorch模型转换为ONNX，并使用FPGA开发工具从ONNX模型创建FPGA逻辑。
- en: TPUs
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs
- en: Google’s TPU chips are being deployed across edge devices as well. PyTorch is
    supported via the XLA library, as described in [“PyTorch on a TPU”](ch06.xhtml#onatpu).
    Deploying your models to edge devices that utilize TPUs can enable you to run
    inference using the XLA library.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的TPU芯片也正在部署到边缘设备上。PyTorch通过XLA库支持，如[“在TPU上的PyTorch”](ch06.xhtml#onatpu)中所述。将模型部署到使用TPU的边缘设备可以使您使用XLA库进行推理。
- en: ASICs
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ASICs
- en: Many companies are developing their own custom chips or ASICs that implement
    model designs in a highly optimized and efficient manner. The ability to deploy
    your PyTorch models will depend heavily on the capabilities supported by the custom
    ASIC chip designs and development tools. In some cases, you may be able to use
    the PyTorch/XLA library if the ASIC supports it.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司正在开发自己的定制芯片或ASIC，以高度优化和高效的方式实现模型设计。部署PyTorch模型的能力将严重依赖于定制ASIC芯片设计和开发工具所支持的功能。在某些情况下，如果ASIC支持，您可以使用PyTorch/XLA库。
- en: When it comes time to deploy your PyTorch models to an edge device, consider
    the processing components available on the system. Depending on the chips available,
    investigate your options to utilize the C++ frontend API, leverage TorchScript,
    convert your models to ONNX format, or access the PyTorch XLA library to deploy
    your models.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署PyTorch模型到边缘设备时，请考虑系统上可用的处理组件。根据可用的芯片，研究利用C++前端API、利用TorchScript、将模型转换为ONNX格式，或访问PyTorch
    XLA库以部署模型的选项。
- en: In this chapter, you learned how to use the standard Python API, TorchScript/C++,
    TorchServe, ONNX, and the PyTorch mobile libraries to deploy your models for inference.
    The chapter also provided reference code to deploy your PyTorch models to local
    development servers or production environments in the cloud using Flask and TorchServe,
    as well as to iOS and Android devices.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用标准的Python API、TorchScript/C++、TorchServe、ONNX和PyTorch移动库来部署您的模型进行推理。本章还提供了参考代码，以在本地开发服务器或云中的生产环境中使用Flask和TorchServe，以及在iOS和Android设备上部署PyTorch模型。
- en: PyTorch supports a large, active ecosystem of useful tools for model development
    and deployment. We’ll explore this ecosystem in the next chapter, which also provides
    reference code for some of the most popular PyTorch tools.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch支持一个庞大、活跃的有用工具生态系统，用于模型开发和部署。我们将在下一章中探讨这个生态系统，该章还提供了一些最受欢迎的PyTorch工具的参考代码。
