- en: Language models and the Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型和Transformer
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer](https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer](https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer)
- en: With the basics of text preprocessing and modeling covered in the previous chapter,
    this chapter will tackle some more involved language problems such as machine
    translation. We will build up a solid intuition for the Transformer model that
    powers products like ChatGPT and has helped trigger a wave of investment in natural
    language processing (NLP).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中介绍了文本预处理和建模的基础之后，本章将探讨一些更复杂的语言问题，例如机器翻译。我们将为驱动ChatGPT等产品并帮助引发自然语言处理（NLP）投资热潮的Transformer模型建立起坚实的直觉。
- en: The language model
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: In the previous chapter, we learned how to convert text data to numeric inputs,
    and we used this numeric representation to classify movie reviews. However, text
    classification is, in many ways, a uniquely simple problem. We only need to output
    a single floating-point number for binary classification and, at worst, *N* numbers
    for *N*-way classification.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何将文本数据转换为数值输入，并使用这种数值表示来对电影评论进行分类。然而，文本分类在很多方面都是一个独特而简单的问题。对于二元分类，我们只需要输出一个单精度浮点数，而对于N路分类，最坏的情况是输出*N*个数字。
- en: What about other text-based tasks like question answering or translation? For
    many real-world problems, we are interested in a model that can generate a text
    output for a given input. Just like we needed tokenizers and embeddings to help
    us handle text on the *way in* to a model, we must build up some techniques before
    we can produce text on the *way out*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于其他基于文本的任务，如问答或翻译呢？对于许多现实世界的问题，我们感兴趣的是能够为给定输入生成文本输出的模型。就像我们需要标记化器和嵌入层来帮助我们处理模型输入路径上的文本一样，我们必须在能够生成模型输出路径上的文本之前构建一些技术。
- en: We don’t need to start from scratch here; we can continue to use the idea of
    an integer sequence as a natural numeric representation for text. In the previous
    chapter, we covered *tokenizing* a string, where we split inputs into tokens and
    map each token to an int. We can *detokenize* a sequence by proceeding in reverse
    — map ints back to string tokens and join them together. With this approach, our
    problem becomes building a model that can predict an integer sequence of tokens.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们不需要从头开始；我们可以继续使用整数序列作为文本的自然数值表示的想法。在上一章中，我们学习了如何将字符串**标记化**，即把输入分割成标记并将每个标记映射到一个整数。我们可以通过相反的过程**反标记化**一个序列——将整数映射回字符串标记并将它们连接起来。采用这种方法，我们的问题变成了构建一个能够预测标记整数序列的模型。
- en: The simplest option to consider might be to train a direct classifier over the
    space of all possible output integer sequences, but some back-of-the-envelope
    math will quickly show this is intractable. With a vocabulary of 20,000 words,
    there are 20,000 ^ 4, or 160 quadrillion possible 4-word sequences, and fewer
    atoms in the universe than possible 20-word sequences. Attempting to represent
    every output sequence as a unique classifier output would overwhelm compute resources
    no matter how we design our model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑的最简单选项可能是训练一个直接分类器，覆盖所有可能的输出整数序列空间，但一些简单的数学计算很快就会显示这是不可行的。以20,000个单词的词汇量为例，有20,000^4，即160万亿可能的4词序列，而宇宙中的原子数量少于可能的20词序列。试图将每个输出序列表示为唯一的分类器输出将无论我们如何设计模型都会耗尽计算资源。
- en: 'A practical approach for making such a prediction problem feasible is to build
    a model that only predicts a single token output at a time. A *language model*
    is a model that, in its simplest form, learns a straightforward but deep probability
    distribution: `p(token|past tokens)`. Given a sequence of all tokens observed
    up to a point, a language model will attempt to output a probability distribution
    over all possible tokens that could come next. A 20,000-word vocabulary means
    the model needs only predict 20,000 outputs, but by *repeatedly* predicting the
    next token, we will have built a model that can generate a long sequence of text.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这样的预测问题可行的一个实际方法是构建一个一次只预测单个标记输出的模型。一个**语言模型**在其最简单的形式下，学习一个简单但深入的概率分布：`p(token|past
    tokens)`。给定一个观察到某个点的所有标记的序列，语言模型将尝试输出一个概率分布，覆盖所有可能的下一个标记。一个20,000词的词汇表意味着模型只需要预测20,000个输出，但通过**重复**预测下一个标记，我们将构建一个能够生成长序列文本的模型。
- en: Let’s make this more concrete by building a simple language model that predicts
    the next character in a sequence of characters. We will train a small model that
    can output Shakespeare-like text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过构建一个简单的语言模型来预测字符序列中的下一个字符来使这个问题更具体。我们将训练一个能够输出类似莎士比亚文本的小型模型。
- en: Training a Shakespeare language model
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练莎士比亚语言模型
- en: To begin, we can download a collection of some of Shakespeare’s plays and sonnets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以下载一些莎士比亚的戏剧和十四行诗的集合。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 15.1](#listing-15-1): Downloading an abbreviated collection of Shakespeare’s
    work'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.1](#listing-15-1)：下载莎士比亚作品的缩略版集合'
- en: 'Let’s take a look at some of the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些数据：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To build a *language model* from this input, we will need to massage our source
    text. First, we will split our data into equal-length chunks that we can batch
    and use for model training, much as we did for weather measurements in the timeseries
    chapter. Because we will be using a character-level tokenizer here, we can do
    this chunking directly on the string input. A 100-character string will map to
    a 100-integer sequence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要从这个输入构建一个*语言模型*，我们需要对我们的源文本进行整理。首先，我们将数据分割成等长块，我们可以批量使用这些块进行模型训练，就像我们在时间序列章节中为天气测量所做的那样。因为我们在这里将使用字符级分词器，所以我们可以直接在字符串输入上进行这种分割。一个100字符的字符串将映射到一个100个整数的序列。
- en: We will also split each input into two separate feature and label sequences,
    with each label sequence simply being the input sequence offset by a single character.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将每个输入分割成两个单独的特征和标签序列，每个标签序列只是输入序列偏移一个字符。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 15.2](#listing-15-2): Splitting text into chunks for language model
    training'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.2](#listing-15-2)：将文本分割成块以进行语言模型训练'
- en: 'Let’s look at an `(x, y)` input sample. Our label at each position in the sequence
    is the next character in the sequence:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个`(x, y)`输入样本。序列中每个位置上的标签是序列中的下一个字符：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To map this input to a sequence of integers, we can again use the `TextVectorization`
    layer we saw in the last chapter. To learn a character-level vocabulary instead
    of a word-level vocabulary, we can change our `split` argument. Rather than the
    default `"whitespace"` splitting, we instead split by `"character"`. We will do
    no standardization here — to keep things simple, we will preserve case and pass
    punctuation through unaltered.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个输入映射到整数序列，我们又可以再次使用我们在上一章中看到的`TextVectorization`层。为了学习字符级词汇表而不是词级词汇表，我们可以更改我们的`split`参数。而不是默认的`"whitespace"`分割，我们改为按`"character"`分割。在这里我们不会进行标准化——为了简单起见，我们将保留大小写并直接传递标点符号。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 15.3](#listing-15-3): Learning a character-level vocabulary with the
    `TextVectorization` layer'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.3](#listing-15-3)：使用`TextVectorization`层学习字符级词汇表'
- en: 'Let’s inspect the vocabulary:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下词汇表：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We need only 67 characters to handle the full source text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要67个字符来处理完整的源文本。
- en: 'Next, we can apply our tokenization layer to our input text. And finally, we
    can shuffle, batch, and cache our dataset so we don’t need to recompute it every
    epoch:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将我们的分词层应用于我们的输入文本。最后，我们可以打乱、批量并缓存我们的数据集，这样我们就不需要在每个epoch重新计算它：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With that, we are ready to start modeling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经准备好开始建模。
- en: To build our simple language model, we want to predict the probability of a
    character given all past characters. Of all the modeling possibilities we have
    seen so far in this book, an RNN is the most natural fit, as the recurrent state
    of each cell allows the model to propagate information about past characters when
    predicting the label of the current character. We can also use an `Embedding`,
    as we saw in the previous chapter, to embed each input character as a unique 256-dimensional
    vector.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的简单语言模型，我们想要预测给定所有过去字符的字符概率。在我们这本书中迄今为止看到的所有建模可能性中，RNN是最自然的选择，因为每个单元的循环状态允许模型在预测当前字符的标签时传播关于过去字符的信息。我们也可以使用`Embedding`，正如我们在上一章中看到的，将每个输入字符嵌入为唯一的256维向量。
- en: We will use only a single recurrent layer to keep this model small and easy
    to train. Any recurrent layer would do here, but to keep things simple, we will
    use a `GRU`, which is fast and has a simpler internal state than an `LSTM`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用单个循环层来保持这个模型小巧且易于训练。任何循环层都可以在这里使用，但为了简单起见，我们将使用`GRU`，它速度快且内部状态比`LSTM`简单。
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 15.4](#listing-15-4): Building a miniature language model'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.4](#listing-15-4)：构建微型语言模型'
- en: 'Let’s take a look at our model summary:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的模型摘要：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This model outputs a softmax probability for every possible character in our
    vocabulary, and we will `compile()` it with a crossentropy loss. Note that our
    model is still training on a classification problem, it’s just that we will make
    one classification prediction for every token in our sequence. For our batch of
    64 samples with 100 characters each, we will predict 6,400 individual labels.
    Loss and accuracy metrics reported by Keras during training will be averaged first
    across each sequence and, second, across each batch.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型为词汇表中的每个可能字符输出一个softmax概率，我们将使用交叉熵损失`compile()`它。请注意，我们的模型仍在进行分类问题的训练，只是我们将为序列中的每个符号进行一次分类预测。对于我们的每个包含100个字符的64个样本批次，我们将预测6,400个单独的标签。Keras在训练期间报告的损失和准确度指标将首先在每个序列中平均，然后在每个批次中平均。
- en: Let’s go ahead and train our language model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续训练我们的语言模型。
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 15.5](#listing-15-5): Training a miniature language model'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.5](#listing-15-5)：训练微型语言模型'
- en: After 20 epochs, our model can eventually predict the next character in our
    input sequences around 70% of the time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 经过20个epoch后，我们的模型最终可以大约70%的时间预测输入序列中的下一个字符。
- en: Generating Shakespeare
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成莎士比亚
- en: Now that we have trained a model that can predict the next *individual* tokens
    with some accuracy, we would like to use it to extrapolate an entire predicted
    sequence. We can do this by calling the model in a loop, where the model’s predicted
    output at one time step becomes the model’s input at the next time step. A model
    built for this kind of feedback loop is sometimes called an *autoregressive* model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个可以以一定精度预测下一个 *单个* 符号的模型，我们希望用它来预测整个序列。我们可以通过循环调用模型来实现这一点，其中模型在某一时间步的预测输出成为下一时间步的模型输入。为这种反馈循环构建的模型有时被称为
    *自回归* 模型。
- en: To run such a loop, we need to perform a slight surgery on the model we just
    trained. During training, our model handled only a fixed sequence length of 100
    tokens, and the `GRU` cell’s state was handled implicitly when calling the layer.
    During generation, we would like to predict a single output token at a time and
    explicitly output the state of the `GRU`’s cell. We need to propagate that state,
    which contains all information the model has encoded about past input characters,
    the next time we call the model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这样的循环，我们需要对我们刚刚训练的模型进行轻微的手术。在训练过程中，我们的模型只处理固定长度的100个符号序列，并且当调用层时，`GRU` 单元的状
    态被隐式处理。在生成过程中，我们希望一次预测一个输出符号并显式输出 `GRU` 的单元状态。我们需要传播这个状态，它包含模型关于过去输入字符的所有编码信息，在下一次调用模型时。
- en: Let’s make a model that handles a single input character at a time and allows
    explicitly passing the RNN state. Because this model will have the same computational
    structure, with slightly modified inputs and outputs, we can assign weights from
    one model to another.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个一次处理一个输入字符并允许显式传递RNN状态的模型。因为这个模型将具有相同的计算结构，只是输入和输出略有修改，我们可以将一个模型的权重分配给另一个模型。
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 15.6](#listing-15-6): Modifying the language model for autoregressive
    inference'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.6](#listing-15-6)：修改语言模型以进行自回归推理'
- en: 'With this, we can call the model to predict an output sequence in a loop. Before
    we do, we will make explicit lookup tables so we switch from characters to integers
    and choose a *prompt* — a snippet of text we will feed as input to the model before
    we begin predicting new tokens:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以循环调用模型来预测输出序列。在我们这样做之前，我们将创建显式查找表，以便从字符切换到整数并选择一个 *提示* —— 一段文本，在我们开始预测新符号之前作为输入提供给模型：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To begin generation, we first need to “prime” the internal state of the GRU
    with our prompt. To do this, we will feed the prompt into the model one token
    at a time. This will compute the exact RNN state the model would see if this prompt
    had been encountered during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始生成，我们首先需要用我们的提示“初始化”GRU的内部状态。为此，我们将提示逐个符号地输入到模型中。这将计算出模型在训练过程中遇到此提示时的确切RNN状态。
- en: When we feed the very last character of the prompt into the model, our state
    output will capture information about the entire prompt sequence. We can save
    the final output prediction to later select the first character of our generated
    response.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将提示的最后一个字符输入到模型中时，我们的状态输出将捕获关于整个提示序列的信息。我们可以将最终的输出预测保存下来，以便稍后选择我们生成响应的第一个字符。
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 15.7](#listing-15-7): Using a fixed prompt to compute a language model’s
    starting state'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.7](#listing-15-7)：使用固定提示计算语言模型的起始状态'
- en: Now we are ready to let the model predict a new output sequence. In a loop,
    up to a desired length, we will continually select the most likely next character
    predicted by the model, feed that to the model, and persist the new RNN state.
    In this way, we can predict an entire sequence, a token at time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好让模型预测一个新的输出序列。在一个循环中，直到达到期望的长度，我们将不断地选择模型预测的最可能的下一个字符，将其输入到模型中，并保持新的RNN状态。这样，我们可以预测整个序列，一次一个标记。
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 15.8](#listing-15-8): Predicting with the language model a token at
    a time'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.8](#listing-15-8)：使用语言模型逐个预测标记'
- en: 'Let’s convert our output integer sequence to a string to see what the model
    predicted. To *detokenize* our input, we simply map all token IDs to strings and
    join them together:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的输出整数序列转换为字符串，看看模型预测了什么。为了*去标记化*我们的输入，我们只需将所有标记ID映射到字符串并将它们连接起来：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We get the following output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have yet to produce the next great tragedy, but this is not terrible for
    two minutes of training on a minimal dataset. The goal of this toy example is
    to show the power of the language model setup. We trained the model on the narrow
    problem of guessing a single character at a time but still use it for a much broader
    problem, generating an open-ended, Shakespearean-like text response.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有创造出下一个伟大的悲剧，但这对于在最小数据集上训练两分钟来说并不糟糕。这个玩具示例的目标是展示语言模型设置的力量。我们在猜测单个字符的狭窄问题上训练了模型，但仍然用它来解决一个更广泛的问题，生成一个开放式的、莎士比亚式的文本响应。
- en: It’s important to notice that this training setup only works because a recurrent
    neural network only passes information forward in the sequence. If you’d like,
    try replacing the `GRU` layer with a `Bidirectional(GRU(...))`. The training accuracy
    will zoom to above 99% immediately, and generation will stop working entirely.
    During training, our model sees the entire sequence each train step. If we “cheat”
    by letting information from the next token in the sequence affect the current
    token’s prediction, we’ve made our problem trivially easy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这种训练设置之所以有效，仅仅是因为循环神经网络只在前向传递信息。如果你想的话，可以尝试将`GRU`层替换为`Bidirectional(GRU(...))`。训练准确率将立即飙升到99%以上，而生成将完全停止工作。在训练过程中，我们的模型在每个训练步骤中都会看到整个序列。如果我们通过让序列中下一个标记的信息影响当前标记的预测来“作弊”，我们就使我们的问题变得极其简单。
- en: This *language modeling* setup is fundamental to countless problems in the text
    domain. It is also somewhat unique compared to other modeling problems we have
    seen so far in this book. We cannot simply call `model.predict()` to get the desired
    output. There is an entire loop, and a nontrivial amount of logic, that exists
    only at inference time! The looping of state in the RNN cell happens for both
    training and inference, but at no point during training do we feed a model’s predicted
    labels back into itself as input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*语言建模*设置对于文本领域的无数问题至关重要。与其他我们在本书中看到的建模问题相比，它也有些独特。我们无法简单地调用`model.predict()`来获取所需的输出。在推理时间存在一个完整的循环和一个非平凡的逻辑！RNN单元中的状态循环在训练和推理时都会发生，但在训练过程中，我们从未将模型的预测标签作为输入反馈给模型。
- en: Sequence-to-sequence learning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列到序列学习
- en: Let’s take the language model idea and extend it to tackle an important problem
    — machine translation. Translation belongs to a class of modeling problems often
    called *sequence-to-sequence* modeling (or *seq2seq* if you are trying to save
    keystrokes). We seek to build a model that can take in a source text as a fixed
    input sequence and generate the translated text sequence as a result. Question
    answering is another classic sequence-to-sequence problem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将语言模型的想法扩展到解决一个重要的问题——机器翻译。翻译属于一类通常称为*序列到序列*建模（如果你试图节省按键，可以称为*seq2seq*）。我们寻求构建一个模型，它可以接受源文本作为固定输入序列，并生成翻译文本序列作为结果。问答是另一个经典的序列到序列问题。
- en: 'The general template behind sequence-to-sequence models is described in figure
    15.1\. During training, the following happens:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型背后的通用模板在图15.1中描述。在训练过程中，以下情况发生：
- en: An encoder model turns the source sequence into an intermediate representation.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器模型将源序列转换为中间表示。
- en: A decoder is trained using the language modeling setup we saw previously. It
    will recursively predict the next token in the target sequence by looking at all
    previous target tokens *and* our encoder’s representation of the source sequence.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们之前看到的语言建模设置来训练解码器。它将通过查看所有先前的目标标记以及我们编码器对源序列的表示来递归地预测目标序列中的下一个标记。
- en: 'During inference, we don’t have access to the target sequence — we’re trying
    to predict it from scratch. We will generate it one token at a time, just as we
    did with our Shakespeare generator:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们无法访问目标序列——我们正在从头开始尝试预测它。我们将逐个生成标记，就像我们使用我们的莎士比亚生成器一样：
- en: We obtain the encoded source sequence from the encoder.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从编码器中获取编码后的源序列。
- en: The decoder starts by looking at the encoded source sequence as well as an initial
    “seed” token (such as the string `"[start]"`) and uses them to predict the first
    real token in the sequence.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器首先查看编码后的源序列以及一个初始的“种子”标记（例如字符串`"[start]"`），并使用它们来预测序列中的第一个真实标记。
- en: The predicted sequence so far is fed back into the decoder, in a loop, until
    it generates a stop token (such as the string `"[end]"`).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止预测的序列被反馈到解码器中，在一个循环中，直到生成一个停止标记（例如字符串`"[end]"`）。
- en: '![](../Images/9a9039d4015f58ca855e14986b7f53df.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a9039d4015f58ca855e14986b7f53df.png)'
- en: '[Figure 15.1](#figure-15-1): Sequence-to-sequence learning: the source sequence
    is processed by the encoder and is then sent to the decoder. The decoder looks
    at the target sequence so far and predicts the target sequence offset by one step
    in the future. During inference, we generate one target token at a time and feed
    it back into the decoder.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.1](#figure-15-1)：序列到序列学习：源序列由编码器处理，然后发送到解码器。解码器查看到目前为止的目标序列，并预测未来一步的目标序列。在推理过程中，我们逐个生成目标标记，并将其反馈到解码器中。'
- en: Let’s build a sequence-to-sequence translation model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个序列到序列的翻译模型。
- en: English-to-Spanish translation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 英语到西班牙语的翻译
- en: 'We’ll be working with an English-to-Spanish translation dataset. Let’s download
    it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个英语到西班牙语的翻译数据集。让我们下载它：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The text file contains one example per line: an English sentence, followed
    by a tab character, followed by the corresponding Spanish sentence. Let’s parse
    this file:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件每行包含一个示例：一个英语句子，后面跟着一个制表符，然后是相应的西班牙语句子。让我们解析这个文件：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our `text_pairs` look like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`text_pairs`看起来像这样：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s shuffle them and split them into the usual training, validation, and
    test sets:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打乱它们并将它们分成通常的训练、验证和测试集：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let’s prepare two separate `TextVectorization` layers: one for English
    and one for Spanish. We’re going to need to customize the way strings are preprocessed:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们准备两个独立的`TextVectorization`层：一个用于英语，一个用于西班牙语。我们需要自定义字符串的预处理方式：
- en: We need to preserve the `"[start]"` and `"[end]"` tokens that we’ve inserted.
    By default, the characters `[` and `]` would be stripped, but we want to keep
    them around so we can distinguish the word `"start"` from the start token `"[start]"`.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要保留我们插入的`"[start]"`和`"[end]"`标记。默认情况下，字符`[`和`]`会被去除，但我们要保留它们，以便我们可以区分单词`"start"`和起始标记`"[start]"`。
- en: Punctuation is different from language to language! In the Spanish `TextVectorization`
    layer, if we’re going to strip punctuation characters, we need to also strip the
    character `¿`.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号在不同的语言中是不同的！在西班牙语的`TextVectorization`层中，如果我们打算去除标点符号，我们还需要去除字符`¿`。
- en: Note that for a non-toy translation model, we would treat punctuation characters
    as separate tokens rather than stripping them since we would want to be able to
    generate correctly punctuated sentences. In our case, for simplicity, we’ll get
    rid of all punctuation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于一个非玩具翻译模型，我们会将标点符号视为单独的标记，而不是去除它们，因为我们希望能够生成正确标点的句子。在我们的情况下，为了简单起见，我们将去除所有标点。
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 15.9](#listing-15-9): Learning token vocabularies for English and
    Spanish text'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.9](#listing-15-9)：为英语和西班牙语文本学习标记词汇表'
- en: Finally, we can turn our data into a `tf.data` pipeline. We want it to return
    a tuple `(inputs, target, sample_weights)` where `inputs` is a dict with two keys,
    `"english"` (the tokenized English sentence) and `"spanish"` (the tokenized Spanish
    sentence), and `target` is the Spanish sentence offset by one step ahead. `sample_weights`
    here will be used to tell Keras which labels to use when calculating our loss
    and metrics. Our output translations are not all equal in length, and some of
    our label sequences will be padded with zeros. We only care about predictions
    for non-zero labels that represent actual translated text.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的数据转换为`tf.data`管道。我们希望它返回一个元组`(inputs, target, sample_weights)`，其中`inputs`是一个字典，包含两个键，`"english"`（标记化的英语句子）和`"spanish"`（标记化的西班牙语句子），而`target`是提前一步的西班牙语句子。`sample_weights`在这里将用于告诉Keras在计算我们的损失和度量时使用哪些标签。我们的输出翻译长度并不相同，我们的一些标签序列将用零填充。我们只关心非零标签的预测，这些标签代表实际的翻译文本。
- en: This matches the “off by one” label set up in the generation model we just built,
    with the addition of the fixed encoder inputs, which will be handled separately
    in our model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在刚刚构建的生成模型中设置的“偏移一个”标签设置相匹配，增加了固定的编码器输入，这些输入将在我们的模型中单独处理。
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 15.10](#listing-15-10): Tokenizing and preparing the Translation data'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.10](#listing-15-10)：标记化和准备翻译数据'
- en: 'Here’s what our dataset outputs look like:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的数据集输出看起来像什么：
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The data is now ready — time to build some models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在准备好了——是时候构建一些模型了。
- en: Sequence-to-sequence learning with RNNs
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RNN进行序列到序列学习
- en: 'Before we try the twin encoder/decoder setup we previously mentioned, let’s
    think through simpler options. The easiest, naive way to use RNNs to turn one
    sequence into another is to keep the output of the RNN at each time step and predict
    an output token from it. In Keras, it would look like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试我们之前提到的双编码器/解码器设置之前，让我们先考虑一些更简单的选项。使用RNN将一个序列转换为另一个序列的最简单、最直接的方法是保留RNN在每个时间步的输出，并从中预测一个输出标记。在Keras中，它看起来像这样：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: However, there is a critical issue with this approach. Due to the step-by-step
    nature of RNNs, the model will only look at tokens `0...N` in the source sequence
    to predict token `N` in the target sequence. Consider translating the sentence,
    “I will bring the bag to you.” In Spanish, that would be “Te traeré la bolsa,”
    where “Te,” the first word of the translation, corresponds to “you” in the English
    source text. There’s simply no way to output the first word of the translation
    without seeing the last word of the source English text!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一个关键问题。由于RNN的逐步性质，模型将只查看源序列中的`0...N`标记来预测目标序列中的标记`N`。考虑翻译句子，“我将把包带到你那里。”在西班牙语中，这将变成“Te
    traeré la bolsa”，其中“Te”，翻译中的第一个词，对应于英语源文本中的“you”。没有看到源英语文本的最后单词，就根本无法输出翻译的第一个单词！
- en: If you’re a human translator, you’d start by reading the entire source sentence
    before beginning to translate it. This is especially important if you’re dealing
    with languages with wildly different word ordering. And that’s precisely what
    standard sequence-to-sequence models do. In a proper sequence-to-sequence setup
    (see figure 15.2), you would first use an encoder RNN to turn the entire source
    sequence into a single representation of the source text. This could be the last
    output of the RNN or, alternatively, its final internal state vectors. We can
    use this representation as the initial state of a decoder RNN in the language
    model setup instead of an initial state of zeros, which we used in our Shakespeare
    generator. This decoder learns to predict the next word of the Spanish translation
    given the current word of the translation, with all information about the English
    sequence coming from that initial RNN state.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名人工翻译员，你将首先阅读整个源句子，然后再开始翻译。如果你处理的是具有截然不同单词顺序的语言，这一点尤为重要。这正是标准序列到序列模型所做的事情。在一个合适的序列到序列设置中（见图15.2），你将首先使用编码器RNN将整个源序列转换成源文本的单个表示。这可以是RNN的最后一个输出，或者，作为替代，其最终内部状态向量。我们可以使用这个表示作为语言模型设置中解码器RNN的初始状态，而不是我们用于莎士比亚生成器的零初始状态。这个解码器学习根据翻译中的当前单词预测西班牙语翻译的下一个单词，所有关于英语序列的信息都来自那个初始RNN状态。
- en: '![](../Images/ed3f977b6d8a998c69ff33ac7cad5bc2.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ed3f977b6d8a998c69ff33ac7cad5bc2.png)'
- en: '[Figure 15.2](#figure-15-2): A sequence-to-sequence RNN: an RNN encoder is
    used to produce a vector that encodes the entire source sequence, which is used
    as the initial state for an RNN decoder.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15.2](#figure-15-2)：序列到序列 RNN：一个 RNN 编码器用于生成一个编码整个源序列的向量，该向量用作 RNN 解码器的初始状态。'
- en: Let’s implement this in Keras, with GRU-based encoders and decoders. We can
    start with just the encoder. Since we will not actually be predicting tokens in
    the encoder sequence, we don’t have to worry about “cheating” by letting the model
    pass information from the end of the sequence to positions at the beginning. In
    fact, this is a good idea, as we want a rich representation of the source sequence.
    We can achieve this with a `Bidirectional` layer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Keras 中实现这个模型，使用基于 GRU 的编码器和解码器。我们可以先从编码器开始。由于我们实际上不会在编码器序列中预测标记，所以我们不必担心通过让模型从序列的末尾传递信息到序列的开头位置来“作弊”。实际上，这是一个好主意，因为我们希望有一个丰富的源序列表示。我们可以通过一个`双向`层来实现这一点。
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 15.11](#listing-15-11): Building a sequence-to-sequence encoder'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.11](#listing-15-11)：构建序列到序列编码器'
- en: Next, let’s add the decoder — a simple `GRU` layer that takes as its initial
    state the encoded source sentence. On top of it, we add a `Dense` layer that produces
    a probability distribution over the Spanish vocabulary for each output step. Here,
    we want to predict the next tokens based only on what came before, so a `Bidirectional`
    RNN would break training by making the loss function trivially easy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加解码器——一个简单的 `GRU` 层，它将编码的源句子作为其初始状态。在其之上，我们添加一个 `Dense` 层，它为每个输出步骤生成西班牙语词汇表上的概率分布。在这里，我们希望仅基于之前的内容来预测下一个标记，所以一个
    `双向` RNN 会通过使损失函数变得过于简单而破坏训练。
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 15.12](#listing-15-12): Building a sequence-to-sequence decoder'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.12](#listing-15-12)：构建序列到序列解码器'
- en: 'Let’s take a look at the seq2seq model in full:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们全面看看 seq2seq 模型：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Our model and data are both ready. We can now begin training our translation
    model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式和数据都已经准备好了。现在我们可以开始训练我们的翻译模型了：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We picked accuracy as a crude way to monitor validation set performance during
    training. We get to 65% accuracy: on average, the model correctly predicts the
    next word in the Spanish sentence 65% of the time. However, in practice, next-token
    accuracy isn’t a great metric for machine translation models, in particular because
    it makes the assumption that the correct target tokens from `0` to `N` are already
    known when predicting token `N + 1`. In reality, during inference, you’re generating
    the target sentence from scratch, and you can’t rely on previously generated tokens
    being 100% correct. When working on a real-world machine translation system, metrics
    must be more carefully designed. There are standard metrics, such as a BLEU score,
    that measure the similarity of the machine-translated text to a set of high-quality
    reference translations and can tolerate slightly misaligned sequences.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择准确率作为监控训练过程中验证集性能的粗略方法。我们达到了 65% 的准确率：平均而言，模型在西班牙语句子中正确预测下一个单词的频率为 65%。然而，在实践中，下一个标记的准确率并不是机器翻译模型的一个很好的指标，特别是因为它假设在预测标记
    `N + 1` 时，从 `0` 到 `N` 的正确目标标记已经为人所知。实际上，在推理过程中，你是从头开始生成目标句子，你不能依赖之前生成的标记是 100%
    正确的。在处理实际的机器翻译系统时，指标必须更加精心设计。有一些标准指标，如 BLEU 分数，可以衡量机器翻译文本与一组高质量参考翻译的相似性，并且可以容忍略微错位的序列。
- en: At last, let’s use our model for inference. We’ll pick a few sentences in the
    test set and check how our model translates them. We’ll start from the seed token
    `"[start]"` and feed it into the decoder model, together with the encoded English
    source sentence. We’ll retrieve a next-token prediction, and we’ll re-inject it
    into the decoder repeatedly, sampling one new target token at each iteration,
    until we get to `"[end]"` or reach the maximum sentence length.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用我们的模型进行推理。我们将从测试集中的几个句子中选择，并检查我们的模型如何翻译它们。我们将从种子标记 `"[start]"` 开始，将其输入到解码器模型中，同时输入编码的英语源句子。我们将检索下一个标记的预测，并将其反复重新注入到解码器中，在每次迭代中采样一个新的目标标记，直到我们到达
    `"[end]"` 或达到最大句子长度。
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 15.13](#listing-15-13): Generating translations with a seq2seq RNN'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.13](#listing-15-13)：使用 seq2seq RNN 生成翻译'
- en: 'The exact translations will vary from run to run, as the final model weights
    will depend on the random initializations of our weights and the random shuffling
    of our input data. Here’s what we got:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的翻译会因运行而异，因为最终的模型权重将取决于我们权重的随机初始化以及输入数据的随机洗牌。以下是我们的结果：
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Our model works decently well for a toy model, although it still makes many
    basic mistakes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型作为一个玩具模型表现相当不错，尽管它仍然犯了许多基本错误。
- en: Note that this inference setup, while very simple, is inefficient, since we
    reprocess the entire source sentence and the entire generated target sentence
    every time we sample a new word. In a practical application, you’d want to be
    careful not to recompute any state that has not changed. All we really need to
    predict a new token in the decoder is the current token and the previous RNN state,
    which we could cache before each loop iteration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个推理设置虽然非常简单，但效率低下，因为每次我们采样一个新词时，都需要重新处理整个源句子和整个生成的目标句子。在实际应用中，你想要小心不要重新计算任何没有改变的状态。我们真正需要预测解码器中的新标记的只是当前标记和之前的
    RNN 状态，我们可以在每次循环迭代之前将其缓存。
- en: 'There are many ways this toy model could be improved. We could use a deep stack
    of recurrent layers for both the encoder and the decoder, we could try other RNN
    layers like `LSTM`, and so on. Beyond such tweaks, however, the RNN approach to
    sequence-to-sequence learning has a few fundamental limitations:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以改进这个玩具模型。我们可以为编码器和解码器都使用深层循环层堆叠，我们可以尝试其他 RNN 层，如 `LSTM`，等等。然而，除了这些微调之外，RNN
    方法在序列到序列学习中的几个基本局限性：
- en: The source sequence representation has to be held entirely in the encoder state
    vector, which significantly limits the size and complexity of the sentences you
    can translate.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源序列表示必须完全存储在编码器状态向量中，这显著限制了你可以翻译的句子的规模和复杂性。
- en: RNNs have trouble dealing with very long sequences since they tend to progressively
    forget about the past — by the time you’ve reached the 100th token in either sequence,
    little information remains about the start of the sequence.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 在处理非常长的序列时遇到困难，因为它们往往会逐渐忘记过去——当你到达任意序列中的第100个标记时，关于序列开始的信息就所剩无几了。
- en: Recurrent neural networks dominated sequence-to-sequence learning in the mid-2010s.
    Google Translate circa 2017 was powered by a stack of seven large `LSTM` layers
    in a setup similar to what we just created. However, these limitations of RNNs
    eventually led to researchers developing a new style of sequence model, called
    the Transformer.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络在2010年代中期主导了序列到序列学习。2017年左右的 Google Translate 是由七个大型 `LSTM` 层堆叠而成的，其设置类似于我们刚刚创建的。然而，这些
    RNN 的局限性最终导致了研究人员开发了一种新的序列模型风格，称为 Transformer。
- en: The Transformer architecture
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: In 2017, Vaswani et al. introduced the Transformer architecture in the seminal
    paper “Attention Is All You Need.”^([[1]](#footnote-1)) The authors were working
    on translation systems like the one we just built, and the critical discovery
    is right in the title. As it turned out, a simple mechanism called *attention*
    can be used to construct powerful sequence models that don’t feature recurrent
    layers at all. The idea of attention was not new and had been used in NLP systems
    for a couple of years when they published. But the idea that attention was so
    useful it could be the *only* mechanism used to pass information across a sequence
    was quite surprising at the time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Vaswani等人发表了开创性的论文“Attention Is All You Need.”^([[1]](#footnote-1))，在其中介绍了
    Transformer 架构。作者们正在研究类似于我们刚刚构建的翻译系统，而关键发现就体现在标题中。结果证明，一种简单的机制称为 *注意力* 可以用来构建不包含循环层的强大序列模型。注意力的想法并不新颖，在他们发表时，该想法已经在自然语言处理系统中使用了几年。但当时，注意力如此有用，以至于它可以成为传递序列信息的
    *唯一* 机制，这一点相当令人惊讶。
- en: This finding unleashed nothing short of a revolution in natural language processing
    — and beyond. Attention has fast become one of the most influential ideas in deep
    learning. In this section, you’ll get an in-depth explanation of how it works
    and why it has proven so effective for sequence modeling. We’ll then use attention
    to rebuild our English-to-Spanish translation model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现引发了自然语言处理——乃至更广泛的领域的革命。注意力迅速成为深度学习中最有影响力的想法之一。在本节中，你将深入了解它是如何工作的以及为什么它在序列建模中证明如此有效。然后我们将使用注意力来重建我们的英语到西班牙语的翻译模型。
- en: So, with all that as build-up, what exactly is attention? And how does it offer
    a replacement for the recurrent neural networks we have used so far?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在所有这些铺垫之后，什么是注意力？它又是如何为我们迄今为止使用的循环神经网络提供替代方案的？
- en: Attention was actually developed as a way to augment an RNN model like the one
    we just built. Researchers noticed that while RNNs excelled at modeling dependencies
    in a local neighborhood, they struggled with recall as sequences got longer. Say
    you were building a system to answer questions about a source document. If the
    document length got too long, RNN results would get plain bad, a far cry from
    human performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力实际上是为了增强我们刚刚构建的类似RNN模型而开发的。研究人员注意到，虽然RNNs在建模局部邻域中的依赖关系方面表现出色，但随着序列变长，它们在回忆方面遇到了困难。比如说，你正在构建一个回答关于源文档问题的系统。如果文档长度过长，RNN的结果会变得非常糟糕，远远低于人类的表现。
- en: As a thought experiment, imagine using this book to build a weather prediction
    model. If you had enough time, you might read the entire book cover to cover,
    but when you actually implemented your model, you would pay special attention
    to just the timeseries chapters. Even within a chapter, you might find specific
    code samples and explanations you would refer to often. On the other hand, you
    would not be particularly worried about the details of image convolutions as you
    worked on your code. The overall word count of this book is well over 100,000,
    far beyond any sequence length we have tackled, but humans can be *selective*
    and *contextual* in how we pull information from text.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种思想实验，想象使用这本书来构建一个天气预报模型。如果你有足够的时间，你可能会从头到尾阅读整本书，但在实际实现你的模型时，你会特别关注时间序列章节。即使在章节中，你也可能会找到你经常参考的特定代码示例和解释。另一方面，当你编写代码时，你不会特别担心图像卷积的细节。这本书的总字数远远超过10万，远超我们处理过的任何序列长度，但人类在从文本中提取信息时可以是*选择性的*和*情境性的*。
- en: RNNs, on the other hand, lack any mechanism to refer back to a previous section
    of a sequence directly. All information must, by design, be passed through an
    RNN cell’s internal state in a loop, through *every* position in a sequence. It’s
    a bit like finishing this book, closing it, and trying to implement that weather
    prediction model entirely from memory. The idea with attention is to build a mechanism
    by which a neural network can give more weight to some part of a sequence and
    less weight to others contextually, depending on the current input being processed
    (figure 15.3).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，RNNs缺乏直接回溯序列先前部分的机制。所有信息必须通过RNN单元的内部状态在循环中传递，通过序列中的每个位置。这有点像完成这本书，合上它，然后试图完全从记忆中实现那个天气预报模型。注意力的想法是通过构建一种机制，使神经网络可以根据当前正在处理的输入，在序列的某些部分给予更多权重，而在其他部分给予较少权重（图15.3）。
- en: '![](../Images/3129120895f1bf9c96b397c648057a6f.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3129120895f1bf9c96b397c648057a6f.png)'
- en: '[Figure 15.3](#figure-15-3): The general concept of attention in deep learning:
    input features get assigned attention scores, which can be used to inform the
    next representation of the input.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.3](#figure-15-3)：深度学习中注意力的通用概念：输入特征被分配注意力分数，这些分数可以用来告知输入的下一个表示。'
- en: Dot-product attention
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 点积注意力
- en: Let’s revisit our translation RNN and try to add the notion of selective attention.
    Consider predicting just a single token. After passing the `source` and `target`
    sequences through our `GRU` layers, we will have a vector representing the target
    token we are about to predict and a sequence of vectors representing each word
    in the source text.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的翻译RNN，并尝试添加选择性注意的概念。考虑预测单个标记。在通过我们的`GRU`层传递`source`和`target`序列之后，我们将有一个代表即将预测的目标标记的向量，以及代表源文本中每个单词的向量序列。
- en: With attention, our goal is to give the model a way to *score* every single
    vector in our source sequence based on its *relevance* to the current word we
    are trying to predict (figure 15.4). If the vector representation of a source
    token has a high score, we consider it particularly important; if not, we care
    less about it. For now, let’s assume we have this function `score(target_vector,
    source_vector)`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用注意力，我们的目标是给模型一种方法，根据当前试图预测的单词的相关性，为我们的源序列中的每个向量评分（图15.4）。如果一个源标记的向量表示具有高分数，我们认为它特别重要；如果不是，我们就不太关心它。目前，让我们假设我们有一个这个函数
    `score(target_vector, source_vector)`。
- en: '![](../Images/00096f30f6b13d63d087c6f71161bab2.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00096f30f6b13d63d087c6f71161bab2.png)'
- en: '[Figure 15.4](#figure-15-4): Attention assigns a relevance score to each vector
    in a source for each vector in a target sequence.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.4](#figure-15-4)：注意力为源序列中的每个向量以及目标序列中的每个向量分配一个相关性分数。'
- en: 'For attention to work well, we want to avoid passing information about important
    tokens through a loop potentially as long as our combined source and target sequence
    length — this is where RNNs start to fail. A simple way to do this is to take
    a weighted sum of all the source vectors based on this score we will compute.
    It would also be convenient if the sum of all attention scores for a given target
    were 1, as this would give our weighted sum a predictable magnitude. We can achieve
    this by running the scores through a `softmax` function — something like this,
    in NumPy pseudocode:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使注意力机制工作得更好，我们希望避免通过可能长达我们组合的源和目标序列长度之长的循环传递关于重要标记的信息——这就是RNN开始失败的地方。一个简单的方法是根据我们将要计算的分数对所有的源向量进行加权求和。如果给定目标的所有注意力分数之和为1，这将使我们的加权求和具有可预测的幅度，这将是方便的。我们可以通过运行`softmax`函数来实现这一点——在NumPy伪代码中可能像这样：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: But how should we compute this relevance score? When researchers first worked
    with attention, this question was a big topic of inquiry. It turns out that one
    of the most straightforward approaches is best. We can use a dot-product as a
    simple measure of the distance between target and source vectors. If the source
    and target vectors are close together, we assume that means the source token is
    relevant to our prediction. At the end of this chapter, we will examine why this
    assumption makes intuitive sense.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们应该如何计算这个相关性分数呢？当研究人员最初开始使用注意力机制时，这个问题成为了研究的重点。事实证明，最直接的方法是最好的。我们可以使用点积作为目标向量和源向量之间距离的简单度量。如果源向量和目标向量很接近，我们假设这意味着源标记与我们的预测相关。在本章结束时，我们将探讨为什么这个假设具有直观的意义。
- en: 'Let’s update our pseudocode. We can make our snippet more complete by handling
    the entire target sequence at once — it will be equivalent to running our previous
    snippet in a loop for each token in the target sequence. When both `target` and
    `source` are sequences, the attention scores will be a matrix. Each row represents
    how much a target word will value a source word in the weighted sum (see figure
    15.5). We will use the Einsum notation as a convenient way to write the dot-product
    and weighted sum:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的伪代码。我们可以通过一次处理整个目标序列来使我们的片段更加完整——它将相当于在目标序列中的每个标记上运行我们之前的片段。当`target`和`source`都是序列时，注意力分数将是一个矩阵。每一行表示在加权求和中，目标单词将如何评价源单词（参见图15.5）。我们将使用Einsum符号作为方便地编写点积和加权求和的方法：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](../Images/5a894dfe04e69c356b3b2f38c25b9e54.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a894dfe04e69c356b3b2f38c25b9e54.png)'
- en: '[Figure 15.5](#figure-15-5): When both target and source are sequences, attention
    scores are a 2D matrix. Each row shows the attention scores for the word we are
    trying to predict (in green).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.5](#figure-15-5)：当目标和源都是序列时，注意力分数是一个二维矩阵。每一行显示了我们要预测的单词的注意力分数（以绿色显示）。'
- en: We can make the *hypothesis space* of this attention mechanism much richer if
    we give the model parameters to control the attention score. If we project both
    source and target vectors with `Dense` layers, the model can find a good shared
    space where source vectors are close to target vectors if they help the overall
    prediction quality. Similarly, we should allow the model to project the source
    vectors into an entirely separate space before they are combined and once again
    after the summation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给模型参数以控制注意力分数，我们可以使这个注意力机制的**假设空间**变得更加丰富。如果我们用`Dense`层将源和目标向量都投影，模型可以在一个良好的共享空间中找到源向量，如果它们有助于整体预测质量，源向量就会靠近目标向量。同样，我们应该允许模型在将源向量组合之前和求和之后将源向量投影到完全不同的空间中。
- en: We can also adopt a slightly different naming for inputs that has become standard
    in the field. What we just wrote is roughly summarized as `sum(score(target, source)
    * source)`. We will write this equivalently with different input names as `sum(score(query,
    key) * value)`. This three-argument version is more general — in rare cases, you
    might not want to use the same vector to score your source inputs as you use to
    sum your source inputs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以采用在领域内已成为标准的稍有不同的输入命名。我们刚才写的内容大致可以总结为`sum(score(target, source) * source)`。我们将用不同的输入名称以相同的方式写出这个等价表达式，即`sum(score(query,
    key) * value)`。这个三个参数的版本更通用——在罕见的情况下，你可能不想使用与你的源输入相同的向量来评分你的源输入，就像你用来求和源输入的向量一样。
- en: The terminology comes from search engines and recommender systems. Imagine a
    search tool to look up photos in a database — the “query” is your search term,
    the “keys” are photo tags you use to match with the query, and finally, the “values”
    are the photos themselves (figure 15.6). The attention mechanism we are building
    is roughly analogous to this sort of lookup.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 术语来自搜索引擎和推荐系统。想象一个在数据库中查找照片的工具——“查询”是你的搜索词，“键”是你用来与查询匹配的照片标签，最后，“值”是照片本身（图15.6）。我们构建的注意力机制大致相当于这种查找方式。
- en: '![](../Images/06f4a4bfcfd1563e8cf33f6be461e375.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06f4a4bfcfd1563e8cf33f6be461e375.png)'
- en: '[Figure 15.6](#figure-15-6): Retrieving images from a database: the *query*
    is compared to a set of *keys*, and the match scores are used to rank *values*
    (images).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.6](#figure-15-6)：从数据库检索图像：*查询*与一组*键*进行比较，并使用匹配分数对*值*（图像）进行排序。'
- en: 'Let’s update our pseudocode, so we have a parameterized attention using our
    new terminology:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的伪代码，以便我们有一个使用新术语的参数化注意力：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This block is a perfectly functional attention mechanism! We just wrote a function
    that will allow the model to pull information from anywhere in the source sequence,
    contextually, depending on the target word we are decoding.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块是一个功能齐全的注意力机制！我们刚刚编写了一个函数，允许模型根据我们正在解码的目标词，从源序列的任何位置提取信息，上下文相关。
- en: 'The “Attention is all you need” authors made two more changes to our mechanism
    through trial and error. The first is a simple scaling factor. When input vectors
    get long, the dot-product scores can get quite large, which can affect the stability
    of our softmax gradients. The fix is simple: we can scale down our softmax scores
    slightly. Scaling by the square root of the vector length works well for any vector
    size.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: “注意力即一切”的作者通过试错法对我们的机制进行了两项更多更改。第一项是一个简单的缩放因子。当输入向量变长时，点积分数可能会变得相当大，这可能会影响softmax梯度的稳定性。解决方案很简单：我们可以稍微降低我们的softmax分数。通过向量长度的平方根进行缩放对任何向量大小都有效。
- en: 'The other has to do with the expressivity of the attention mechanism. The softmax
    sum we are doing is powerful — it allows a direct connection across distant parts
    of a sequence. But the summation is also blunt: if the model tries to attend to
    too many tokens at once, the interesting features of individual source tokens
    will get “washed out” in the combined representation. A simple trick that works
    well is to do this attention operation several times for the same sequence, with
    several different attention *heads* running the same computation with different
    parameters:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面与注意力机制的表意性有关。我们进行的softmax求和非常强大——它允许在序列的遥远部分之间建立直接连接。但求和操作也很直接：如果模型试图一次性关注太多标记，单个源标记的有趣特征会在组合表示中被“冲淡”。一个有效的小技巧是对同一序列进行多次注意力操作，使用几个不同的注意力*头*以不同的参数运行相同的计算：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: By projecting the query and key differently, one head might learn to match the
    subject of the source sentence, while another head might attend to punctuation.
    This multi-headed attention avoids the limitation of needing to combine the entire
    source sequence with a single softmax sum (figure 15.7).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不同地投影查询和键，一个头可能学会匹配源句子的主题，而另一个头可能关注标点符号。这种多头注意力避免了需要将整个源序列与单个softmax求和结合的限制（图15.7）。
- en: '![](../Images/304293112b9495ea5eb0e67c73ae1138.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/304293112b9495ea5eb0e67c73ae1138.png)'
- en: '[Figure 15.7](#figure-15-7): Multi-headed attention allows each target word
    to attend to different parts of the source sequence in separate partitions of
    the eventual output vector.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.7](#figure-15-7)：多头注意力允许每个目标词在最终输出向量的不同分区中关注源序列的不同部分。'
- en: 'Of course, in practice, you would want to write this code as a reusable layer.
    Here, Keras has you covered. We can recreate our previous code with the `MultiHeadAttention`
    layer as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在实践中，你希望将这段代码编写成一个可重用的层。在这里，Keras为你提供了支持。我们可以使用`MultiHeadAttention`层重新创建之前的代码，如下所示：
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Transformer encoder block
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer编码器块
- en: One way to use the `MultiHeadAttention` layer would be to add it to our existing
    RNN translation model. We could pass the sequence output from our encoder and
    decoder into an attention layer and use its output to update our target sequence
    before prediction. Attention would allow the model to handle long-range dependencies
    in text that the `GRU` layer will struggle with. This does, in fact, improve an
    RNN model’s capabilities and is how attention was first used in the mid-2010s.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `MultiHeadAttention` 层的一种方法是将它添加到我们现有的 RNN 翻译模型中。我们可以将编码器和解码器的序列输出传递到一个注意力层，并使用其输出在预测之前更新我们的目标序列。注意力可以使模型处理文本中的长距离依赖关系，而
    `GRU` 层将难以处理。这实际上提高了 RNN 模型的能力，并且是注意力在2010年代中期首次被使用的方式。
- en: 'However, the authors of “Attention is all you need” realized you could go further
    and use attention as a general mechanism for handling all sequence data in a model.
    Although so far we have only looked at attention as a way to handle information
    passing between two sequences, you could also use attention as a way to let a
    sequence attend to itself:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，“Attention is all you need”的作者意识到你可以更进一步，将注意力作为处理模型中所有序列数据的一般机制。尽管到目前为止我们只将注意力视为处理两个序列之间信息传递的方式，但你也可以将注意力作为让序列关注自身的方式：
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is called *self-attention*, and it is quite powerful. With self-attention,
    each token can attend to every token in its own sequence, including itself, allowing
    the model to learn a representation of the word in context.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为 *自注意力*，它非常强大。使用自注意力，每个标记可以关注其自身序列中的每个标记，包括自身，这使得模型能够学习到上下文中的词表示。
- en: 'Consider an example sentence: “The train left the station on time.” Now, consider
    one word in the sentence: “station.” What kind of station are we talking about?
    Could it be a radio station? Maybe the International Space Station? With self-attention,
    the model could learn to give a high attention score to the pair of “station”
    and “train,” summing the vector used to represent “train” into the representation
    of the word “station.”'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个例子句子：“火车准时离开了车站。”现在，考虑句子中的一个词：“车站。”我们谈论的是哪种车站？可能是一个广播电台？也许是一个国际空间站？使用自注意力，模型可以学习给“车站”和“火车”这对词赋予高注意力得分，将表示“火车”的向量加到“车站”这个词的表示中。
- en: Self-attention gives the model an effective way to go from representing a word
    in a vacuum to representing a word conditioned on all other tokens that appear
    in the sequence. This sounds a lot like what an RNN is supposed to do. Can we
    just go ahead and replace our RNN layers with `MultiHeadAttention`?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力为模型提供了一种有效的方法，从表示一个真空中的词到表示一个基于序列中所有其他标记的词。这听起来很像 RNN 应该做的事情。我们是否可以直接用 `MultiHeadAttention`
    替换我们的 RNN 层？
- en: Almost! But not quite; we still need an essential ingredient for any deep neural
    network — a nonlinear activation function. The `MultiHeadAttention` layer combines
    linear projections of every element in a source sequence, but that’s it. In a
    sense, it’s a very expressive pooling operation. Consider, in the extreme case,
    a token length of one. In this case, the attention score matrix is always a single
    one, and the entire layer boils down to a linear projection of the source sequence,
    with no nonlinearities. You could stack 100 attention layers together and still
    be able to simplify the entire computation to a single matrix multiplication!
    That’s a real problem with the expressiveness of our model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎是了！但还不完全；我们仍然需要一个深度神经网络的基本成分——一个非线性激活函数。`MultiHeadAttention` 层结合了源序列中每个元素的线性投影，仅此而已。从某种意义上说，它是一个非常表达性的池化操作。考虑一个极端情况，一个标记长度为1。在这种情况下，注意力得分矩阵始终是一个单一值，整个层简化为源序列的线性投影，没有任何非线性。你可以堆叠100个注意力层，仍然可以将整个计算简化为单次矩阵乘法！这是我们模型表达力的一个真正问题。
- en: At some point, all recurrent cells pass the input vector for each token through
    a dense projection and apply an activation function; we need a plan for something
    similar. The authors of “Attention is all you need” decided to add this back in
    the simplest way possible — stacking a feedforward network of two dense layers
    with an activation in the middle. Attention passes information across the sequence,
    and the feedforward network updates the representation of individual sequence
    items.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个点上，所有循环单元都会将每个标记的输入向量通过密集投影，并应用激活函数；我们需要一个类似的计划。在“Attention is all you need”的作者决定以最简单的方式添加这个功能——堆叠一个由两个密集层和一个中间激活函数组成的前馈网络。注意力在序列间传递信息，前馈网络更新单个序列项的表示。
- en: We are ready to start building a Transformer model. Let’s start by replacing
    the encoder of our translation model. We will use self-attention to pass information
    along the source sequence of English words. We will also add in two things we
    learned to be particularly important when building ConvNets back in chapter 9,
    *normalization* and _residual connections.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好开始构建一个 Transformer 模型。让我们从替换我们的翻译模型的编码器开始。我们将使用自注意力机制来在英语单词的源序列中传递信息。我们还将加入在第
    9 章中构建 ConvNets 时发现特别重要的两个东西，*归一化*和_residual connections_。
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[Listing 15.14](#listing-15-14): A Transformer encoder block'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.14](#listing-15-14)：一个 Transformer 编码器块'
- en: 'You’ll note that the normalization layers we’re using here aren’t `BatchNormalization`
    layers like those we’ve used in image models. That’s because `BatchNormalization`
    doesn’t work well for sequence data. Instead, we’re using the `LayerNormalization`
    layer, which normalizes each sequence independently from other sequences in the
    batch — like this, in NumPy-like pseudocode:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们在这里使用的归一化层并不是像我们在图像模型中使用的那种 `BatchNormalization` 层。这是因为 `BatchNormalization`
    在序列数据上效果不佳。相反，我们使用的是 `LayerNormalization` 层，它独立于批次中的其他序列对每个序列进行归一化——就像这样，在类似 NumPy
    的伪代码中：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Compare to `BatchNormalization` (during training):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `BatchNormalization`（在训练期间）比较：
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: While `BatchNormalization` collects information from many samples to obtain
    accurate statistics for the feature means and variances, `LayerNormalization`
    pools data within each sequence separately, which is more appropriate for sequence
    data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `BatchNormalization` 从许多样本中收集信息以获得特征均值和方差的准确统计数据，但 `LayerNormalization` 在每个序列内部单独汇总数据，这更适合序列数据。
- en: We also pass a new input to the `MultiHeadAttention` layer called `attention_mask`.
    This Boolean tensor input will be broadcast to the same shape as our attention
    scores `(batch_size, target_length, source_length)`. When set, it will zero the
    attention score in specific locations, stopping the source tokens at these locations
    from being used in the attention calculation. We will use this to prevent any
    token in the sequence from attending to padding tokens, which contain no information.
    Our encoder layer takes a `source_mask` input that will mark all the non-padding
    tokens in our inputs and upranks it to shape `(batch_size, 1, source_length)`
    to use as an `attention_mask`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还向 `MultiHeadAttention` 层传递一个新的输入，称为 `attention_mask`。这个布尔张量输入将被广播到与我们的注意力分数相同的形状
    `(batch_size, target_length, source_length)`。当设置时，它将在特定位置将注意力分数置零，阻止这些位置的源标记在注意力计算中使用。我们将使用这个来防止序列中的任何标记关注到填充标记，这些标记不包含任何信息。我们的编码器层接受一个
    `source_mask` 输入，它将标记我们输入中的所有非填充标记，并将其提升到形状 `(batch_size, 1, source_length)` 以用作
    `attention_mask`。
- en: Note that the input and outputs of this layer have the same shape, so encoder
    blocks can be stacked on top of each other, building a progressively more expressive
    representation of the input English sentence.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个层的输入和输出具有相同的形状，因此编码器块可以堆叠在一起，构建对输入英语句子表达越来越丰富的表示。
- en: Transformer decoder block
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer 解码器块
- en: Next up is the decoder block. This layer will be almost identical to the encoder
    block, except we want the decoder to use the encoder output sequence as an input.
    To do this, we can use attention twice. We first apply a self-attention layer
    like our encoder, which allows each position in the target sequence to use information
    from other target positions. We then add another `MultiHeadAttention` layer, which
    receives both the source and target sequence as input. We will call this attention
    layer *cross-attention* as it brings information across the encoder and decoder.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是解码器块。这个层将几乎与编码器块相同，但我们希望解码器使用编码器的输出序列作为输入。为此，我们可以使用两次注意力。我们首先应用一个类似于编码器的自注意力层，它允许目标序列中的每个位置使用来自其他目标位置的信息。然后我们添加另一个
    `MultiHeadAttention` 层，它接收源序列和目标序列作为输入。我们将这个注意力层称为 *交叉注意力*，因为它在编码器和解码器之间传递信息。
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[Listing 15.15](#listing-15-15): A Transformer decoder block'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.15](#listing-15-15)：一个 Transformer 解码器块'
- en: Our decoder layer takes in both a `target` and `source`. Like with the `TransformerEncoder`,
    we take in a `source_mask` that marks the location of all padding in the source
    input (`True` for non-padding, `False` for padding) and use it as an `attention_mask`
    for the cross-attention layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器层接受一个 `target` 和一个 `source`。与 `TransformerEncoder` 一样，我们接受一个 `source_mask`，它标记源输入中所有填充的位置（`True`
    表示非填充，`False` 表示填充），并将其用作交叉注意力层的 `attention_mask`。
- en: For the decoder’s self-attention layer, we need a different type of attention
    mask. Recall that when we built our RNN decoder, we avoided using a `Bidirectional`
    RNN. If we had used one, the model would be able to cheat by seeing the label
    it was trying to predict as a feature! Attention is inherently bidirectional;
    in self-attention, any token position in the target sequence can attend to any
    other position. Without special care, our model will learn to pass the next token
    in the sequence as the current label and will have no ability to generate novel
    translations.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器的自注意力层，我们需要不同类型的注意力掩码。回想一下，当我们构建我们的 RNN 解码器时，我们避免了使用 `Bidirectional` RNN。如果我们使用了双向
    RNN，模型将能够通过看到它试图预测的标签作为特征来作弊！注意力本质上是双向的；在自注意力中，目标序列中的任何标记位置都可以关注任何其他位置。如果不特别小心，我们的模型将学会将序列中的下一个标记作为当前标签，并且将没有能力生成新颖的翻译。
- en: 'We can achieve one-directional information flow with a special “causal” attention
    mask. Let’s say we pass an attention mask with ones in the lower-triangular section
    like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用特殊的“因果”注意力掩码来实现单向信息流。假设我们传递一个在下半三角部分为 1 的注意力掩码，如下所示：
- en: '[PRE40]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Each row `i` can be read as a mask for attention for the target token at position
    `i`. In the first row, the first token can only attend to itself. In the second
    row, the second token can attend to both the first and second tokens, and so forth.
    This gives us the same effect as our RNN layer, where information can only propagate
    forward in the sequence, not backward. In Keras, you can specify this lower-triangular
    mask simply by passing `use_casual_mask` to the `MultiHeadAttention` layer when
    calling it. Figure 15.8 shows a visual representation of the layers in both the
    encoder and decoder layers, when stacked into a Transformer model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行 `i` 可以被读取为目标位置 `i` 的目标标记的注意力掩码。在第一行中，第一个标记只能关注自身。在第二行中，第二个标记可以关注第一个和第二个标记，以此类推。这给我们带来了与我们的
    RNN 层相同的效果，其中信息只能在序列中向前传播，而不能向后传播。在 Keras 中，您可以通过在调用时将 `use_casual_mask` 传递给 `MultiHeadAttention`
    层来简单地指定这个下三角掩码。图 15.8 展示了当堆叠到 Transformer 模型中时，编码器和解码器层中的层的一个视觉表示。
- en: '![](../Images/1e1ad371ac9fa40b33c1c0ac497e8fe9.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e1ad371ac9fa40b33c1c0ac497e8fe9.png)'
- en: '[Figure 15.8](#figure-15-8): A visual representation of the computations for
    both `TransformerEncoder` and `TransformerDecoder` blocks'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15.8](#figure-15-8)：`TransformerEncoder` 和 `TransformerDecoder` 块的计算的视觉表示'
- en: Sequence-to-sequence learning with a Transformer
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Transformer 进行序列到序列学习
- en: Let’s try putting this all together. We will use the same basic setup as our
    RNN model, replacing the `GRU` layers with our `TransformerEncoder` and `TransformerDecoder`.
    We will use `256` as the embedding size throughout the model, except in the feedforward
    block. In the feedforward block, we scale up the embedding size to `2048` before
    nonlinearity and scale back to the model’s hidden size afterward. This large intermediate
    dimension works well in practice.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将这些全部放在一起。我们将使用与我们的 RNN 模型相同的基本设置，用我们的 `TransformerEncoder` 和 `TransformerDecoder`
    替换 `GRU` 层。在整个模型中，我们将使用 `256` 作为嵌入大小，除了在前馈块中。在前馈块中，我们在非线性之前将嵌入大小扩展到 `2048`，然后在之后将其缩放回模型的隐藏大小。这个大的中间维度在实践中效果很好。
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[Listing 15.16](#listing-15-16): Building a Transformer model'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.16](#listing-15-16)：构建 Transformer 模型'
- en: 'Let’s take a look at the summary of our Transformer model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们 Transformer 模型的摘要：
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Our model has almost exactly the same structure as the `GRU` translation model
    we trained earlier, with attention now substituting for recurrent layers as the
    mechanism to pass information across the sequence. Let’s try training the model:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式几乎与我们在前面训练的 `GRU` 翻译模型具有相同的结构，现在注意力取代了循环层作为在序列间传递信息的机制。让我们尝试训练这个模型：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'After training, we get to about 58% accuracy: on average, the model correctly
    predicts the next word in the Spanish sentence 58% of the time. Something is off
    here. Training is worse than the RNN model by 7 percentage points. Either this
    Transformer architecture is not what it was hyped up to be, or we missed something
    in our implementation. Can you spot what it is?'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们达到了大约 58% 的准确率：平均而言，模型正确预测西班牙语句子中下一个单词的准确率为 58%。这里有些不对劲。训练效果比 RNN 模型差
    7 个百分点。要么这个 Transformer 架构并不是人们吹嘘的那样，要么我们在实现中遗漏了某些东西。你能找出是什么吗？
- en: This section is ostensibly about sequence models. In the previous chapter, we
    saw how vital word order could be to meaning. And yet, the Transformer we just
    built wasn’t a sequence model at all. Did you notice? It’s composed of dense layers
    that process sequence tokens independently of each other and an attention layer
    that looks at the tokens as a set. You could change the order of the tokens in
    a sequence, and you’d get identical pairwise attention scores and the same context-aware
    representations. If you were to rearrange every word in every English source sentence
    completely, the model wouldn’t notice, and you’d still get the same accuracy.
    Attention is a set-processing mechanism, focused on the relationships between
    pairs of sequence elements — it’s blind to whether these elements occur at the
    beginning, at the end, or in the middle of a sequence. So why do we say that Transformer
    is a sequence model? And how could it possibly be suitable for machine translation
    if it doesn’t look at word order?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节表面上关于序列模型。在前一章中，我们看到了单词顺序对于意义的重要性。然而，我们刚刚构建的Transformer根本不是一个序列模型。你注意到了吗？它由密集层组成，这些层独立地处理序列标记，还有一个关注层，它将标记视为一组。如果你改变序列中标记的顺序，你会得到相同的成对注意力分数和相同的有上下文感知的表示。如果你将每个英语源句子中的每个单词完全重新排列，模型不会注意到，你仍然会得到相同的准确度。注意力是一种集合处理机制，专注于序列元素对之间的关系——它对元素是否出现在序列的开始、结束或中间是盲目的。那么我们为什么说Transformer是一个序列模型呢？如果它不查看单词顺序，它怎么可能适合机器翻译呢？
- en: For RNNs, we relied on the layer’s *computation* to be order aware. In the case
    of the Transformer, we instead inject positional information directly into our
    embedded sequence itself. This is called a *positional embedding.* Let’s take
    a look.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RNNs，我们依赖于层的**计算**来确保顺序感知。在Transformer的情况下，我们直接将位置信息注入到我们的嵌入序列本身。这被称为**位置嵌入**。让我们看看。
- en: Embedding positional information
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入位置信息
- en: 'The idea behind a positional embedding is very simple: to give the model access
    to word order information, we will add the word’s position in the sentence to
    each word embedding. Our input word embeddings will have two components: the usual
    word vector, which represents the word independently of any specific context,
    and a position vector, which represents the position of the word in the current
    sentence. Hopefully, the model will then figure out how to best use this additional
    information.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入背后的思想非常简单：为了使模型能够访问单词顺序信息，我们将单词在句子中的位置添加到每个单词嵌入中。我们的输入单词嵌入将有两个组成部分：通常的单词向量，它代表单词，不受任何特定上下文的影响，以及一个位置向量，它代表单词在当前句子中的位置。希望模型能够然后找出如何最好地使用这些附加信息。
- en: The most straightforward scheme to add position information would be concatenating
    each word’s position to its embedding vector. You’d add a “position” axis to the
    vector and fill it with `0` for the first word in the sequence, `1` for the second,
    and so on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 添加位置信息最直接的方法是将每个单词的位置连接到其嵌入向量中。你会在向量中添加一个“位置”轴，并用`0`填充序列中的第一个单词，`1`填充第二个，以此类推。
- en: However, that may not be ideal because the positions can potentially be very
    large integers, which will disrupt the range of values in the embedding vector.
    As you know, neural networks don’t like very large input values or discrete input
    distributions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能并不理想，因为位置可能潜在地是非常大的整数，这将干扰嵌入向量的值域。正如你所知，神经网络不喜欢非常大的输入值或离散的输入分布。
- en: 'The “Attention is all you need” authors used an interesting trick to encode
    word positions: they added to the word embeddings a vector containing values in
    the range `[-1, 1]` that varied cyclically depending on the position (they used
    cosine functions to achieve this). This trick offers a way to uniquely characterize
    any integer in a large range via a vector of small values. It’s clever, but it
    turns out we can do something simpler and more effective: we’ll learn positional
    embedding vectors the same way we learn to embed word indices. We’ll then add
    our positional embeddings to the corresponding word embeddings to obtain a position-aware
    word embedding. This is called a *positional embedding*. Let’s implement it.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: “注意力即一切”的作者使用了一个有趣的技巧来编码单词位置：他们向单词嵌入中添加了一个包含`[-1, 1]`范围内值的向量，这些值根据位置周期性变化（他们使用余弦函数来实现这一点）。这个技巧提供了一种通过一个小值向量唯一表征大范围内任何整数的方法。这很聪明，但结果证明我们可以做更简单、更有效的事情：我们将以学习单词索引相同的方式学习位置嵌入向量。然后我们将我们的位置嵌入添加到相应的单词嵌入中，以获得一个位置感知的单词嵌入。这被称为*位置嵌入*。让我们来实现它。
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[Listing 15.17](#listing-15-17): A learned position embedding layer'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.17](#listing-15-17)：一个学习到的位置嵌入层'
- en: We would use this `PositionalEmbedding` layer just like a regular `Embedding`
    layer. Let’s see it in action as we try training our Transformer for a second
    time.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像使用常规`Embedding`层一样使用这个`PositionalEmbedding`层。让我们在尝试第二次训练我们的Transformer时看看它的实际效果。
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[Listing 15.18](#listing-15-18): Building a Transformer model with positional
    embeddings'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.18](#listing-15-18)：使用位置嵌入构建Transformer模型'
- en: 'With the positional embedding now added to our model, let’s try training again:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将位置嵌入添加到我们的模型中，让我们再次尝试训练：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: With positional information back in the model, things went much better. We achieved
    a 67% accuracy when guessing the next word. It’s a noticeable improvement from
    the `GRU` model, and that’s all the more impressive when you consider that this
    model has half the parameters of the GRU counterpart.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中重新引入位置信息后，情况变得好多了。我们在猜测下一个单词时达到了67%的准确率。与`GRU`模型相比，这是一个明显的改进，而且当你考虑到这个模型只有`GRU`模型参数的一半时，这更加令人印象深刻。
- en: There’s one other important thing to notice about this training run. Training
    is noticeably faster than the RNN — each epoch takes about a third of the time.
    This would be true even if we matched parameter count with the RNN model, and
    it is a side effect of getting rid of the looped state passing of our `GRU` layers.
    With attention, there is no looping computation to handle during training, meaning
    that on a GPU or TPU, we can handle the entire attention computation in one go.
    This makes the `Transformer` quicker to train on accelerators.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这次训练运行，还有一件其他重要的事情需要注意。训练速度明显快于RNN——每个epoch大约需要三分之一的时间。即使我们与RNN模型的参数数量相匹配，这也是真的，这是去掉我们的`GRU`层循环状态传递的副作用。有了注意力，训练过程中没有循环计算要处理，这意味着在GPU或TPU上，我们可以一次性处理整个注意力计算。这使得`Transformer`在加速器上的训练更快。
- en: Let’s rerun generation with our newly trained `Transformer`. We can use the
    same code as we did for our RNN sampling.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用新训练的`Transformer`重新运行生成。我们可以使用与我们的RNN采样相同的代码。
- en: '[PRE47]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[Listing 15.19](#listing-15-19): Generating translations with a Transformer'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.19](#listing-15-19)：使用Transformer生成翻译'
- en: 'Running the generation code, we get the following output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 运行生成代码，我们得到以下输出：
- en: '[PRE48]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Subjectively, the Transformer performs significantly better than the GRU-based
    translation model. It’s still a toy model, but it’s a better toy model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 主观来说，Transformer的性能明显优于基于GRU的翻译模型。它仍然是一个玩具模型，但是一个更好的玩具模型。
- en: The Transformer is a powerful architecture that has laid the basis for an explosion
    of interest in text-processing models. It’s also fairly complex, as deep learning
    models go. After seeing all of these implementation details, one might reasonably
    protest that this all seems quite arbitrary. There are so many small details to
    take on faith. How could we possibly know this choice and configuration of layers
    is optimal?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种强大的架构，为文本处理模型兴趣的爆发奠定了基础。就深度学习模型而言，它也相当复杂。在看到所有这些实现细节后，人们可能会合理地抗议，这一切似乎都很随意。有如此多的细节需要我们相信。我们怎么可能知道这个层的选择和配置是最优的？
- en: The answer is simple — it’s not. Over the years, a number of improvements have
    been proposed to the Transformer architecture by making changes to attention,
    normalization, and positional embeddings. Many new models in research today are
    replacing attention altogether with something less computationally complex as
    sequence lengths get very long. Eventually, perhaps by the time you are reading
    this book, something will have supplanted the Transformer as the dominant architecture
    used for language modeling.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 答案很简单——它不是。多年来，人们通过改变注意力、归一化和位置嵌入等方式对Transformer架构进行了多项改进。随着序列长度变得非常长，今天的研究中许多新模型都在用某种计算上更简单的机制来完全取代注意力。最终，也许在你阅读这本书的时候，某种东西将取代Transformer，成为语言建模中占主导地位的架构。
- en: There’s a lot we can learn from the Transformer that will stand the test of
    time. At the end of this chapter, we will discuss what makes the Transformer so
    effective. But it’s worth remembering that, as a whole, the field of machine learning
    moves empirically. Attention grew out of an attempt to augment RNNs, and after
    years of guessing and checking by a ton of people, it gave rise to the Transformer.
    There is little reason to think this process is done playing out.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从Transformer中学到很多经得起时间考验的东西。在本章结束时，我们将讨论是什么使得Transformer如此有效。但值得记住的是，作为整体，机器学习领域是经验性的。注意力是从增强RNNs的尝试中产生的，经过大量人员的多年猜测和检查，它导致了Transformer的诞生。几乎没有理由认为这个过程已经结束。
- en: Classification with a pretrained Transformer
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练Transformer进行分类
- en: After “Attention is all you need,” people started to notice how far Transformer
    training could scale, especially compared to models that had come before. As we
    just mentioned, one big plus was that the model is faster to train than RNNs.
    No more loops during training, which is always good when working with a GPU or
    TPU.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在“注意力即一切”之后，人们开始注意到Transformer训练可以扩展到何种程度，尤其是与之前的模型相比。正如我们刚才提到的，一个很大的优点是，该模型比RNNs训练得更快。在用GPU或TPU工作时，没有更多的循环，这总是件好事。
- en: It is also a very data hungry model architecture. We actually got a little taste
    of this in the last section. While our RNN translation model plateaued in validation
    performance after 5 or so epochs, the Transformer model was still improving its
    validation score after 30 epochs of training.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 它也是一个非常“数据饥渴”的模型架构。我们实际上在上一个部分中已经尝到了一点。虽然我们的RNN翻译模型在5个或更多个epoch后验证性能达到了平台期，但Transformer模型在训练了30个epoch后仍在提高其验证分数。
- en: These observations prompted many to try scaling up the Transformer with more
    data, layers, and parameters — with great results. This caused a distinctive shift
    in the field toward large pretrained models that can cost millions to train but
    perform noticeably better on a wide range of problems in the text domain.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察促使许多人尝试通过更多数据、层和参数来扩展Transformer，结果非常显著。这导致该领域向大型预训练模型发生了显著转变，这些模型训练成本可能高达数百万美元，但在文本领域的广泛问题上表现明显更好。
- en: For our last code example in the text section, we will revisit our IMDb text-classification
    problem, this time with a pretrained Transformer model.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本部分的最后一个代码示例中，我们将重新审视我们的IMDb文本分类问题，这次使用预训练的Transformer模型。
- en: Pretraining a Transformer encoder
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练Transformer编码器
- en: One of the first pretrained Transformers to become popular in NLP was called
    BERT, short for Bidirectional Encoder Representations from Transformers^([[2]](#footnote-2)).
    The paper and model were released a year after “Attention Is All You Need.” The
    model structure was exactly the same as the encoder part of the translation Transformer
    we just built. This encoder model is *bidirectional* in that every position in
    the sequence can attend to positions in front of and behind it. This means it’s
    a good model for computing a rich representation of input text, but not a model
    meant to run generation in a loop.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中第一个流行的预训练Transformer被称为BERT，即“来自Transformer的双向编码器表示”，缩写为BERT^([[2]](#footnote-2))。该论文和模型在“注意力即一切”发布后一年发布。模型结构与我们所构建的翻译Transformer的编码器部分完全相同。这个编码器模型是*双向的*，即序列中的每个位置都可以关注其前后的位置。这意味着它是一个计算输入文本丰富表示的好模型，但不是一个旨在循环运行生成的模型。
- en: BERT was trained in sizes between 100 million and 300 million parameters, much
    bigger than the 14 million parameter Transformer we just trained. This meant the
    model needed a lot of training data to perform well. To achieve this, the authors
    used a riff on the classic language modeling setup called *masked language modeling*.
    To pretrain the model, we take a sequence of text and replace about 15% of the
    tokens with a special `[MASK]` token. The model will attempt to predict the original
    masked token values during training. While the classic language model, sometimes
    called a *causal language model*, attempts to predict `p(token|past tokens)`,
    the masked language model attempts to predict `p(token|surrounding tokens)`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的训练规模在1亿到3亿参数之间，比我们刚刚训练的1400万个参数的Transformer大得多。这意味着模型需要大量的训练数据才能表现良好。为了实现这一点，作者们使用了一种经典的称为*遮蔽语言模型*的语言模型设置。为了预训练模型，我们取一个文本序列，并替换大约15%的标记为一个特殊的`
- en: This training setup is unsupervised. You don’t need any labels about the text
    you feed in; for any text sequence, you can easily choose some random tokens and
    mask them out. That made it easy for the authors to find a large amount of text
    data needed to train models of this size. For the most part, they pulled from
    Wikipedia as a source.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练设置是无监督的。你不需要任何关于你输入文本的标签；对于任何文本序列，你都可以轻松地选择一些随机标记并对其进行遮蔽。这使得作者们能够找到大量训练这种规模模型所需的文本数据。大部分数据是从维基百科作为来源获取的。
- en: Using pretrained word embeddings was already common practice when BERT was released
    — we saw this ourselves in the last chapter. But pretraining an entire Transformer
    gave something much more powerful — the ability to compute a word embedding for
    a word in the *context* of the words around it. And the Transformer allowed doing
    this at a scale and quality that were unheard of at the time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT发布时，使用预训练的词嵌入已经是常见的做法了——我们在上一章中自己就看到了这一点。但是，预训练整个Transformer带来了更强大的功能——能够在周围词语的*上下文*中计算一个词语的嵌入。Transformer允许以当时前所未有的规模和质量来做这件事。
- en: The authors of BERT took this model, pretrained on a huge amount of text, and
    specialized it to achieve state-of-the-art results on several NLP benchmarks at
    the time. This marked a distinctive shift in the field toward using very large,
    pretrained models, often with only a small amount of fine-tuning. Let’s try this
    out.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者将这个模型在大量文本上进行了预训练，并将其专门化，以在当时的几个NLP基准测试中取得最先进的成果。这标志着该领域向使用非常大的预训练模型转变，通常只需要少量微调。让我们试试看。
- en: Loading a pretrained Transformer
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载预训练的Transformer
- en: Instead of using BERT here, let’s use a follow-up model called RoBERTa^([[3]](#footnote-3)),
    short for Robustly Optimized BERT. RoBERTa made some minor simplifications to
    BERT’s architecture, but most notably used more training data to improve performance.
    BERT used 16 GB of English language text, mainly from Wikipedia. The RoBERTa authors
    used 160 GB of text from all over the web. It’s estimated that RoBERTa cost a
    few hundred thousand dollars to train at the time. Because of this extra training
    data, the model performs noticeably better for an equivalent overall parameter
    count.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里不使用BERT，让我们使用一个后续模型，称为RoBERTa^([[3]](#footnote-3))，简称Robustly Optimized BERT。RoBERTa对BERT的架构进行了一些小的简化，但最值得注意的是使用了更多的训练数据来提高性能。BERT使用了16
    GB的英语文本，主要来自维基百科。RoBERTa的作者使用了来自整个网络的160 GB文本。当时估计RoBERTa的训练成本为几万美元。由于这个额外的训练数据，模型在等效的总参数数量下表现明显更好。
- en: 'To use a pretrained model we will need a few things:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用预训练模型，我们需要一些东西：
- en: '*A matching tokenizer* — Used with the pretrained model itself. Any text must
    be tokenized in the same way as during pretraining. If the words of our IMDb reviews
    map to different token indices than they would have during pretraining, we cannot
    use the learned representations of each token in the model.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*匹配的分词器*——与预训练模型本身一起使用。任何文本都必须以与预训练期间相同的方式进行分词。如果我们的IMDb评论中的词语映射到与预训练期间不同的标记索引，我们就不能使用模型中每个标记学习到的表示。'
- en: '*A matching model architecture* — To use the pretrained model, we need to recreate
    the math used internally by the model for pretraining exactly.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*匹配的模型架构*——为了使用预训练模型，我们需要精确地重建模型在预训练过程中内部使用的数学公式。'
- en: '*The pretrained weights* — These weights were created by training the model
    for about a day on 1,024 GPUs and billions of input words.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预训练权重*——这些权重是通过在1,024个GPU上训练模型大约一天，并在数十亿个输入词上创建的。'
- en: Recreating the tokenizer and architecture code ourselves would not be too hard.
    The model internals almost exactly match the `TransformerEncoder` we built previously.
    However, matching a model implementation is a time-consuming process, and as we
    have done earlier in this book, we can instead use the KerasHub library to access
    pretrained model implementations for Keras.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自己重新创建分词器和架构代码并不会太难。模型的内部结构几乎与我们之前构建的`TransformerEncoder`完全匹配。然而，匹配模型实现是一个耗时的过程，正如我们在本书前面所做的那样，我们可以使用KerasHub库来访问Keras的预训练模型实现。
- en: Let’s use KerasHub to load a RoBERTa tokenizer and model. We can use the special
    constructor `from_preset()` to load a pretrained model’s weights, configuration,
    and tokenizer assets from disk. We will load RoBERTa’s base model, which is the
    smallest of the few pretrained checkpoints released with the RoBERTa paper.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用KerasHub来加载RoBERTa分词器和模型。我们可以使用特殊的构造函数`from_preset()`从磁盘加载预训练模型的权重、配置和分词器资产。我们将加载RoBERTa的基础模型，这是与RoBERTa论文一起发布的几个预训练检查点中最小的一个。
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[Listing 15.20](#listing-15-20): Loading the RoBERTa pretrained model with
    KerasHub'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.20](#listing-15-20)：使用KerasHub加载RoBERTa预训练模型'
- en: The `Tokenizer` maps from text to sequences of integers, as we would expect.
    Remember the `SubWordTokenizer` we built in the last chapter? RoBERTa’s tokenizer
    is almost the same as that tokenizer, with minor tweaks to handle Unicode characters
    from any language.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer`将文本映射到整数序列，正如我们所期望的那样。还记得我们在上一章中构建的`SubWordTokenizer`吗？RoBERTa的分词器几乎与那个分词器相同，只是对处理来自任何语言的Unicode字符进行了一些小的调整。'
- en: 'Given the size of RoBERTa’s pretraining dataset, subword tokenization is a
    must. Using a character-level tokenizer would make input sequences way too long,
    making the model much more expensive to train. Using a word-level tokenizer would
    require a massive vocabulary to attempt to cover all the distinct words in the
    millions of documents of text used from across the web. Getting good coverage
    of words would blow up our vocabulary size and make the `Embedding` layer at the
    front of the Transformer unworkably large. Using a subword tokenizer allows the
    model to handle any word with only a 50,000-term vocabulary:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到RoBERTa预训练数据集的大小，子词分词是必须的。使用字符级分词器会使输入序列变得非常长，从而使模型训练成本大大增加。使用词级分词器则需要一个庞大的词汇表来尝试覆盖来自网络上的数百万个文档中的所有不同单词。要获得良好的单词覆盖范围，将会使我们的词汇表大小爆炸，并使Transformer前面的`Embedding`层变得无法使用。使用子词分词器允许模型仅使用50,000个术语的词汇表来处理任何单词：
- en: '[PRE50]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: What is this `Backbone` we just loaded? We saw in chapter 8 that a `backbone`
    is a term often used in computer vision for a network that maps from input images
    to a latent space — basically a vision model without a head for making predictions.
    In KerasHub, a backbone refers to any pretrained model that is not yet specialized
    for a task. The model we just loaded takes in an input sequence and embeds it
    to an output sequence with shape `(batch_size, sequence_length, 768)`, but it’s
    not set up for a particular loss function. You could use it for any number of
    downstream tasks — classifying sentences, identifying text spans with certain
    information, identifying parts of speech, etc.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚加载的这个`Backbone`是什么？在第8章中，我们看到`backbone`是一个在计算机视觉中常用的术语，指的是从输入图像映射到潜在空间（basically
    a vision model without a head for making predictions）的网络——基本上是一个没有预测头部的视觉模型。在KerasHub中，`backbone`指的是任何尚未针对特定任务进行优化的预训练模型。我们刚刚加载的模型接受一个输入序列，并将其嵌入到一个形状为`(batch_size,
    sequence_length, 768)`的输出序列中，但它还没有设置特定的损失函数。你可以用它来完成任何数量的下游任务——例如分类句子、识别包含特定信息的文本片段、识别词性等。
- en: 'Next, we will attach a classification head to this backbone that specializes
    it for our IMDb review classification fine-tuning. You can think of this as attaching
    different heads to a screwdriver: a Phillips head for one task, a flat head for
    another.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为这个`backbone`附加一个分类头，使其专门用于我们的IMDb评论分类微调。你可以将这想象为给螺丝刀附加不同的头部：一个用于一个任务，一个用于另一个任务。
- en: 'Let’s take a look at our backbone. We loaded the *smallest* variant of RoBERTa
    here, but it still has 124 million parameters, which is the biggest model we have
    used in this book:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的`backbone`。在这里，我们加载了RoBERTa的最小变体，但它仍然有1.24亿个参数，这是我们在这本书中使用过的最大的模型：
- en: '[PRE51]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: RoBERTa uses 12 Transformer encoder layers stacked on top of each other. That’s
    a big step up from our translation model!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 使用了12个堆叠在一起的 Transformer 编码器层。这比我们的翻译模型有了很大的提升！
- en: Preprocessing IMDb movie reviews
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理 IMDb 电影评论
- en: 'We can reuse the IMDb loading code we used in chapter 14 unchanged. This will
    download the movie review data to a `train_dir` and `test_dir` and split a validation
    dataset into a `val_dir`:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不加修改地重用第14章中使用的 IMDb 加载代码。这将下载电影评论数据到 `train_dir` 和 `test_dir`，并将验证数据集分割到
    `val_dir`：
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: After loading, we once again have a training set of 20,000 movie reviews and
    a validation set of 5,000 movie reviews.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 加载后，我们再次拥有一个包含20,000条电影评论的训练集和一个包含5,000条电影评论的验证集。
- en: 'Before fine-tuning our classification model, we must tokenize our movie reviews
    with the RoBERTa tokenizer we loaded. During pretraining, RoBERTa used a specific
    form of “packing” tokens into a sequence, similar to what we did for our translation
    model. Each sequence would begin with an `<s>` token, end with an `</s>` token,
    and be followed by any number of `<pad>` tokens like this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调我们的分类模型之前，我们必须使用我们加载的 RoBERTa 分词器对电影评论进行标记。在预训练期间，RoBERTa 使用了一种类似于我们为翻译模型所做的“打包”标记到序列的特定形式。每个序列将以
    `<s>` 标记开始，以 `</s>` 标记结束，并跟随着任意数量的 `<pad>` 标记，如下所示：
- en: '[PRE53]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: It’s important to match the token ordering used for pretraining as closely as
    possible; the model will train more quickly and accurately if we do. KerasHub
    provides a layer for this type of token packing called the `StartEndPacker`. The
    layer appends start, end, and padding tokens, trimming long sequences to a given
    sequence length if necessary. Let’s use it along with our tokenizer.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是尽可能匹配预训练中使用的标记顺序；如果我们这样做，模型将训练得更快、更准确。KerasHub 提供了一个用于此类标记打包的层，称为 `StartEndPacker`。该层附加起始、结束和填充标记，如果需要，将长序列修剪到给定的序列长度。让我们使用它以及我们的分词器。
- en: '[PRE54]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[Listing 15.21](#listing-15-21): Preprocessing IMDb movie reviews with RoBERTa’s
    tokenizer'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.21](#listing-15-21)：使用 RoBERTa 的分词器预处理 IMDb 电影评论'
- en: 'Let’s take a look at a single preprocessed batch:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下单个预处理的批次：
- en: '[PRE55]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: With our inputs preprocessed, we are ready to start fine-tuning our model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的输入预处理完成后，我们就可以开始微调我们的模型了。
- en: Fine-tuning a pretrained Transformer
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调预训练的 Transformer
- en: Before we fine-tune our backbone to predict movie reviews, we need to update
    it so it outputs a binary classification label. The backbone outputs an entire
    sequence with shape `(batch_size, sequence_length, 768)`, where each 768-dimensional
    vector represents an input word in the context of its surrounding words. Before
    predicting a label, we must condense this sequence to a single vector per sample.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将骨干微调以预测电影评论之前，我们需要更新它，使其输出二进制分类标签。骨干输出一个形状为 `(batch_size, sequence_length,
    768)` 的整个序列，其中每个768维向量代表一个在其周围词语上下文中的输入词。在预测标签之前，我们必须将这个序列压缩为每个样本的单个向量。
- en: One option would be to do mean pooling or max pooling across the whole sequence,
    computing an average of all token vectors. What works slightly better is simply
    using the first token’s representation as the pooled value. This is due to the
    nature of the attention in our model — the first position in the final encoder
    layer will be able to attend to all other positions in the sequence and pull information
    from them. So rather than pooling information with something coarse, like taking
    an average, attention allows us to pool information *contextually* across the
    sequence.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选择是在整个序列上进行平均池化或最大池化，计算所有标记向量的平均值。稍微好一点的方法是简单地使用第一个标记的表示作为池化值。这是由于我们模型中注意力的性质——最终编码层的第一个位置将能够关注序列中的所有其他位置并从中提取信息。因此，而不是用像平均这样的粗糙方法来池化信息，注意力使我们能够在整个序列中*上下文相关地*池化信息。
- en: Let’s now add a classification head to our backbone. We will also add one final
    `Dense` projection with a nonlinearity before generating an output prediction.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向我们的骨干添加一个分类头。我们还会在生成输出预测之前添加一个具有非线性性的最终 `Dense` 投影。
- en: '[PRE56]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[Listing 15.22](#listing-15-22): Extending the base RoBERTa model for classification'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.22](#listing-15-22)：扩展基础 RoBERTa 模型以进行分类'
- en: With that, we are ready to fine-tune and evaluate the model on the IMDb dataset.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们就可以在 IMDb 数据集上微调和评估模型了。
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[Listing 15.23](#listing-15-23): Training the RoBERTa classification model'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表15.23](#listing-15-23)：训练 RoBERTa 分类模型'
- en: 'Finally, let’s evaluate the trained model:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们评估训练好的模型：
- en: '[PRE58]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In just a single epoch of training, our model reached 93%, a noticeable improvement
    from the 90% ceiling we hit in our last chapter. Of course, this is a far more
    expensive model to use than the simple bigram classifier we built previously,
    but there are clear benefits to using such a large model. And this is all with
    the smaller size of the RoBERTa model. Using the larger 300 million parameter
    model, we could achieve an accuracy of over 95%.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅仅一个训练周期后，我们的模型达到了 93%，相较于我们上一章中达到的 90% 的上限，这是一个明显的提升。当然，这个模型比我们之前构建的简单二元分类器要昂贵得多，但使用这样一个大型模型确实有明显的优势。而且这还是在
    RoBERTa 模型较小规模的情况下。使用更大规模的 3 亿参数模型，我们能够达到超过 95% 的准确率。
- en: What makes the Transformer effective?
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是让 Transformer 变得有效的因素？
- en: 'In 2013, at Google, Tomas Mikolov and his colleagues noticed something remarkable.
    They were building a pretrained embedding called “Word2Vec,” similar to the Continuous
    Bag of Words (CBOW) embedding we built in the last chapter. Much like our CBOW
    model, their training objective sought to turn correlation relationships between
    words into distance relationships in the embedding space: a vector was associated
    with each word in a vocabulary, and the vectors were optimized so that the dot-product
    (cosine proximity) between vectors representing frequently co-occurring words
    would be closer to 1, while the dot-product between vectors representing rarely
    co-occurring words would be closer to 0.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 2013 年，在谷歌，Tomas Mikolov 和他的同事们注意到了一个显著的现象。他们正在构建一个名为“Word2Vec”的预训练嵌入，这与我们在上一章中构建的连续词袋（CBOW）嵌入类似。与我们的
    CBOW 模型一样，他们的训练目标旨在将单词之间的相关性关系转换为嵌入空间中的距离关系：每个词汇中的每个单词都与一个向量相关联，并且这些向量被优化，使得代表频繁共现单词的向量之间的点积（余弦距离）更接近
    1，而代表很少共现单词的向量之间的点积更接近 0。
- en: They found that the resulting embedding space did much more than capture semantic
    similarity. It featured some form of emergent learning — a sort of “word arithmetic.”
    A vector existed in the space that you could add to many male nouns to obtain
    a point that would land close to its female equivalent, as in `V(king) - V(man)
    + V(woman) = V(queen)`, a gender vector. This was quite surprising; the model
    had not been trained for this in any explicit way. There seemed to be dozens of
    such magic vectors — a plural vector, a vector to go from wild animals’ names
    to their closest pet equivalent, etc.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现，生成的嵌入空间不仅能够捕捉语义相似性，还包含某种形式的自发学习——一种“词算术”。在这个空间中存在一个向量，你可以将其添加到许多男性名词中，以获得一个接近其女性对应词的点，例如
    `V(king) - V(man) + V(woman) = V(queen)`，这是一个性别向量。这相当令人惊讶；模型在没有任何明确训练的情况下做到了这一点。似乎有数十个这样的神奇向量——一个复数向量，一个从野生动物名称到其最接近的宠物对应词的向量等。
- en: Fast-forward about 10 years — we are now in the age of large, pretrained Transformer
    models. On the surface, these models couldn’t seem any further from the primitive
    Word2Vec model. A Transformer can generate perfectly fluent language — a feat
    Word2Vec was entirely incapable of. As we will see in the next chapter, such models
    can seem knowledgeable about virtually any topic. And yet, they actually have
    a lot in common with good old Word2Vec.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 快进大约 10 年——我们现在处于大型预训练 Transformer 模型的时代。从表面上看，这些模型似乎与原始的 Word2Vec 模型相去甚远。Transformer
    可以生成完全流畅的语言——这是 Word2Vec 完全无法做到的壮举。正如我们将在下一章中看到的，这些模型似乎对几乎所有主题都了如指掌。然而，它们实际上与古老的
    Word2Vec 有很多共同之处。
- en: 'Both models seek to embed tokens (words or subwords) in a vector space. Both
    rely on the same fundamental principle to learn this space: tokens that appear
    together end up close in the embedding space. The distance function used to compare
    tokens is cosine distance in both cases. Even the dimensionality of the embedding
    space is similar: a vector with somewhere between 1,000 and 10,000 dimensions
    to represent each word.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型都试图在向量空间中嵌入标记（单词或子词）。它们都依赖于相同的基本原理来学习这个空间：一起出现的标记最终会在嵌入空间中靠近。在两种情况下，用于比较标记的距离函数都是余弦距离。甚至嵌入空间的维度也相似：一个具有
    1,000 到 10,000 维的向量来表示每个单词。
- en: 'At this point, you might interject: A Transformer is trained to predict missing
    words in a sequence, not to group tokens in an embedding space. How does the language
    model loss function relate to Word2Vec’s objective of maximizing the dot-product
    between co-occurring tokens? The answer is the attention mechanism.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会打断我：Transformer 是训练用来预测序列中缺失的单词，而不是在嵌入空间中对标记进行分组。语言模型损失函数如何与 Word2Vec
    的目标相关，即最大化共现标记之间的点积？答案是注意力机制。
- en: Attention is, by far, the most critical component in the Transformer architecture.
    It’s a mechanism for learning a new token embedding space by linearly recombining
    token embeddings from some prior space, in weighted combinations that give greater
    importance to tokens that are already “closer” to each other (i.e., that have
    a higher dot-product). It will tend to pull together the vectors of already close
    tokens, resulting over time in a space where token correlation relationships turn
    into embedding proximity relationships (in terms of cosine distance). Transformers
    work by learning a series of incrementally refined embedding spaces, each based
    on recombining elements from the previous one.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是 Transformer 架构中迄今为止最重要的组件。它是一种通过线性重新组合先前空间中的标记嵌入，以加权组合的形式学习新的标记嵌入空间，赋予已经“更接近”彼此的标记（即具有更高点积的标记）更高重要性的机制。它倾向于将已经接近的标记的向量拉在一起，随着时间的推移，在空间中，标记的相关关系转变为嵌入的邻近关系（从余弦距离的角度来看）。Transformer
    通过学习一系列增量优化的嵌入空间来工作，每个嵌入空间都是基于重新组合前一个嵌入空间中的元素。
- en: 'Attention provides Transformers with two crucial properties:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力为 Transformer 提供了两个关键属性：
- en: The embedding spaces they learn are semantically continuous — that is, moving
    a bit in an embedding space only changes the human-facing meaning of the corresponding
    tokens by a bit. The Word2Vec space also exhibited this property.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们学习的嵌入空间是语义连续的——也就是说，在嵌入空间中稍微移动只会略微改变对应标记的人类面向意义。Word2Vec 空间也表现出这一特性。
- en: The embedding spaces they learn are semantically interpolative — that is, taking
    the intermediate point between two points in an embedding space produces a point
    representing the “intermediate meaning” between the corresponding tokens. This
    comes from the fact that each new embedding space is built by interpolating between
    vectors from the previous space.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们学习的嵌入空间是语义插值的——也就是说，在嵌入空间中取两个点的中间点会产生一个表示对应标记之间“中间意义”的点。这源于每个新的嵌入空间都是通过在先前空间中的向量之间进行插值来构建的。
- en: This is not entirely unlike the way the brain learns. The key learning principle
    in the brain is Hebbian learning — in short, “neurons that fire together, wire
    together.” Correlation relationships between neural firing events (which may represent
    actions or perceptual inputs) are turned into proximity relationships in the brain
    network, just like the Transformer and Word2Vec turn correlation relationships
    into vector proximity relationships. Both are maps of a space of information.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这与大脑学习的方式并不完全不同。大脑中的关键学习原理是赫布学习——简而言之，“一起放电的神经元，一起连接。”神经放电事件（可能代表动作或感知输入）之间的相关性关系在脑网络中转变为邻近关系，就像
    Transformer 和 Word2Vec 将相关性关系转变为向量邻近关系一样。两者都是信息空间的一幅图。
- en: 'Of course, there are significant differences between Word2Vec and the Transformer.
    Word2Vec was not designed for generative text sampling. A Transformer can get
    far bigger and can encode vastly more complex transformations. The thing is, Word2Vec
    is very much a toy model: it is to today’s language models as a logistic regression
    on MNIST pixels is to state-of-the-art computer vision models. The fundamental
    principles are mostly the same, but the toy model lacks any meaningful representation
    power. Word2Vec wasn’t even a deep neural network — it had a shallow, single-layer
    architecture. Meanwhile, today’s Transformer models have the highest representation
    power of any model anyone has ever trained — they feature dozens of stacked attention
    and feedforward layers, and their parameter count ranges in the billions.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Word2Vec 和 Transformer 之间存在显著差异。Word2Vec 并非为生成文本采样而设计。Transformer 可以更大，并且可以编码更复杂的转换。问题是，Word2Vec
    非常像一个玩具模型：它对于今天的语言模型来说，就像 MNIST 像素上的逻辑回归对于最先进的计算机视觉模型一样。基本原理大多相同，但玩具模型缺乏任何有意义的表示能力。Word2Vec
    甚至不是一个深度神经网络——它有一个浅层、单层架构。与此同时，今天的 Transformer 模型具有任何人所训练过的模型中最高的表示能力——它们具有数十个堆叠的注意力和前馈层，其参数数量在数十亿级别。
- en: Like with Word2Vec, the Transformer learns useful semantic functions as a by-product
    of organizing tokens into a vector space. But thanks to this increased representation
    power and a much more refined autoregressive optimization objective, we’re no
    longer confined to linear transformations like a gender vector or a plural vector.
    Transformers can store arbitrarily complex vector functions — so complex, in fact,
    that it would be more accurate to refer to them as vector programs rather than
    functions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Word2Vec 一样，Transformer 通过将标记组织到向量空间中作为副产品学习有用的语义函数。但得益于这种增强的表示能力和一个更加精细的自回归优化目标，我们不再局限于像性别向量或复数向量这样的线性变换。Transformers
    可以存储任意复杂的向量函数——实际上，它们如此复杂，以至于更准确地称其为向量程序而不是函数。
- en: Word2Vec enabled you to do basic things like `plural(cat) → cats` or `male_to_female(king)
    → queen`. Meanwhile, a large Transformer model can do pure magic — things like
    `write_this_in_style_of_shakespeare("...your poem...") → "...new poem..."`. And
    a single model can contain millions of such programs.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 使你可以做基本的事情，比如 `plural(cat) → cats` 或 `male_to_female(king) → queen`。与此同时，一个大型
    Transformer 模型可以做到纯粹的魔法——比如 `write_this_in_style_of_shakespeare("...your poem...")
    → "...new poem..."`。而且一个模型可以包含数百万这样的程序。
- en: 'You can see a Transformer as analogous to a database: it stores information
    you can retrieve via the tokens you pass in. But there are two important differences
    between a Transformer and a database.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 Transformer 视为一个数据库的类似物：它存储的信息可以通过你传递的标记检索。但 Transformer 和数据库之间有两个重要的区别。
- en: The first difference is that a Transformer is a continuous, interpolative kind
    of database. Instead of being stored as a set of discrete entries, your data is
    stored as a vector space — a curve. You can move around on the curve (it’s semantically
    continuous, as we discussed) to explore nearby, related points. And you can interpolate
    on the curve between different data points to find their in-between. This means
    that you can retrieve a lot more from your database than you put into it — although
    not all of it will be accurate or meaningful. Interpolation can lead to generalization,
    but it can also lead to hallucinations — a significant problem facing the generative
    language models trained today.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别在于，Transformer 是一种连续的、插值型的数据库。与存储为一系列离散条目不同，你的数据以向量空间的形式存储——一条曲线。你可以在曲线上移动（正如我们讨论的，它在语义上是连续的）来探索附近的、相关的点。你还可以在曲线上对不同的数据点进行插值，以找到它们之间的中间值。这意味着你可以从数据库中检索出比输入更多的信息——尽管其中并非所有信息都是准确或有意义的。插值可能导致泛化，但也可能导致幻觉——这是今天训练的生成语言模型面临的一个重大问题。
- en: 'The second difference is that a Transformer doesn’t just contain data. For
    models like RoBERTa, trained on hundreds of thousands of documents scraped from
    the internet, there is a lot of data: facts, places, people, dates, things, and
    relationships. But it’s also — perhaps primarily — a database of programs.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个区别是，Transformer 不仅包含数据。对于像 RoBERTa 这样的模型，它是在从互联网上抓取的数十万份文档上训练的，因此有大量的数据：事实、地点、人物、日期、事物和关系。但它还——也许主要是——一个程序数据库。
- en: They’re different from the kind of programs you’re used to dealing with, mind
    you. These are not like Python programs — series of symbolic statements processing
    data step by step. Instead, these vector programs are highly nonlinear functions
    that map the latent embedding space unto itself, analogous to Word2Vec’s magic
    vectors, but far more complex.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它们与您习惯处理的程序不同。这些不是像 Python 程序那样的符号语句序列，逐步处理数据。相反，这些向量程序是高度非线性的函数，将潜在嵌入空间映射到自身，类似于
    Word2Vec 的魔法向量，但更加复杂。
- en: In the next chapter, we will push Transformer models to an entirely new scale.
    Models will use billions of parameters and train on trillions of words. Output
    from these models can often feel like magic — like an intelligent operator sitting
    inside our model and pulling the strings. But it’s important to remember that
    these models are fundamentally interpolative — thanks to attention, they learn
    an interpolative embedding space for a significant chunk of all text written in
    the English language. Wandering this embedding space can lead to interesting,
    unexpected generalizations, but it cannot synthesize something fundamentally new
    with anything close to genuine, human-level intelligence.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把 Transformer 模型推向一个全新的规模。这些模型将使用数十亿个参数，并在万亿个单词上进行训练。这些模型的输出常常让人感觉像是魔法——就像一个智能操作员坐在我们的模型内部，操纵着一切。但重要的是要记住，这些模型在本质上都是插值的——多亏了注意力机制，它们学习了一个插值嵌入空间，这个空间涵盖了英语语言中大量文本的嵌入。在这个嵌入空间中漫步可能会产生有趣、意外的泛化，但它无法用任何接近真正、人类水平智能的方式合成全新的东西。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A *language model* is a model that learns a specific probability distribution
    — `p(token|past tokens)`:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *语言模型* 是一个学习特定概率分布——`p(token|past tokens)` 的模型：
- en: Language models have broad applications, but the most important is that you
    can generate text by calling them in a loop, where the output token at one time
    step becomes the input token in the next.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型有广泛的应用，但最重要的是，你可以通过循环调用它们来生成文本，其中某一时间步的输出标记成为下一时间步的输入标记。
- en: A *masked language model* learns a related probability distribution `p(tokens|surrounding
    tokens)` and can be helpful for classifying text and individual tokens.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *掩码语言模型* 学习一个相关的概率分布 `p(tokens|surrounding tokens)`，并且对于文本分类和单个标记的分类可能很有帮助。
- en: A *sequence-to-sequence language model* learns to predict the next token given
    both past tokens in a target sequence and an entirely separate, fixed source sequence.
    Sequence-to-sequence models are useful for problems like translation and question
    answering.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *序列到序列语言模型* 学习在给定目标序列中的过去标记和一个完全独立的、固定的源序列的情况下预测下一个标记。序列到序列模型对于翻译和问答等问题非常有用。
- en: A sequence-to-sequence model usually has two separate components. An *encoder*
    computes a representation of the source sequence, and a *decoder* takes this representation
    as input and predicts the next token in a target sequence based on past tokens.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个序列到序列模型通常有两个独立的组件。一个 *编码器* 计算源序列的表示，而一个 *解码器* 将这个表示作为输入，并根据过去的标记预测目标序列中的下一个标记。
- en: '*Attention* is a mechanism that allows a model to pull information from anywhere
    in a sequence selectively based on the context of the token currently being processed:'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力* 是一种机制，允许模型根据当前正在处理的标记的上下文，从序列的任何位置有选择地提取信息：'
- en: Attention avoids the problems RNNs have with long-range dependencies in text.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力避免了 RNN 在文本中的长距离依赖问题。
- en: Attention works by taking the dot-product of two vectors to compute an attention
    score. Vectors near each other in an embedding space will be summed together in
    the attention mechanism.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力通过计算两个向量的点积来计算一个注意力分数。在嵌入空间中彼此靠近的向量将在注意力机制中被相加。
- en: 'The *Transformer* is a sequence modeling architecture that uses attention as
    the only mechanism to pass information across a sequence:'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Transformer* 是一种序列建模架构，它使用注意力作为唯一机制在序列中传递信息：'
- en: The Transformer works by stacking blocks of alternating attention and two-layer
    feedforward networks.
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 通过堆叠交替的注意力块和两层前馈网络来工作。
- en: The Transformer can scale to many parameters and lots of training data while
    still improving accuracy in the language modeling problem.
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 可以扩展到许多参数和大量训练数据，同时在语言建模问题上提高准确性。
- en: Unlike RNNs, the Transformer involves no sequence-length loops at training time,
    making the model much easier to train in parallel across many machines.
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 RNN 不同，Transformer 在训练时没有序列长度循环，这使得模型在多台机器上并行训练变得更加容易。
- en: A Transformer encoder uses bidirectional attention to build a rich representation
    of a sequence.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 编码器使用双向注意力来构建序列的丰富表示。
- en: A Transformer decoder uses causal attention to predict the next word in a language
    model setup.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 解码器在语言模型设置中使用因果注意力来预测下一个单词。
- en: Footnotes
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Vaswani et al., “Attention Is All You Need,” Proceedings of the 31st International
    Conference on Neural Information Processing Systems (2017), [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
    [[↩]](#footnote-link-1)
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 瓦斯瓦尼等人，“Attention Is All You Need”，第31届国际神经网络信息处理系统会议论文集（2017），[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
    [[↩]](#footnote-link-1)
- en: 'Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding,” Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (2019), [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).
    [[↩]](#footnote-link-2)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 德夫林等人，“BERT：用于语言理解的深度双向变换器预训练”，北美计算语言学协会第2019年会议：人机语言技术，第1卷（2019），[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).
    [[↩]](#footnote-link-2)
- en: 'Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019),
    [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692). [[↩]](#footnote-link-3)'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刘等人，“RoBERTa：一种鲁棒优化的BERT预训练方法”（2019），[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692).
    [[↩]](#footnote-link-3)
