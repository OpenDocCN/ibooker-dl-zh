- en: '9 Broadening the horizon: Exploratory topics in AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 扩展视野：人工智能的探索性主题
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Highlighting the pursuit of artificial general intelligence
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 突出追求通用人工智能
- en: Unpacking the philosophical debate about AI consciousness
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解构关于AI意识的哲学辩论
- en: Measuring the environmental effects of LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量大型语言模型（LLM）的环境影响
- en: Discussing the LLM open source community
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论LLM开源社区
- en: We hope that you enjoyed learning about the risks and promises of generative
    artificial intelligence (AI) and that this book has encouraged you to optimistically
    and responsibly engage with this ever-evolving field.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您喜欢学习关于生成式人工智能（AI）的风险和承诺，并且这本书鼓励您乐观且负责任地参与这个不断发展的领域。
- en: This final chapter is an appendix of sorts. It serves as a valuable extension
    of the book, exploring topics that relate to the main things we’ve talked about
    in this book. Whereas chapters 1-8 are intended to be immediately practical to
    people using and developing large language models (LLMs), the topics in this chapter
    are more exploratory. We dig into utopian and dystopian arguments about artificial
    general intelligence (AGI), claims of artificial sentience, the challenges in
    determining the carbon footprint of LLMs, and the momentum around the open source
    LLM movement.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一章是一种附录。它作为本书的有价值扩展，探讨与本书中讨论的主要话题相关的话题。虽然第1-8章旨在对使用和开发大型语言模型（LLM）的人立即实用，但本章的主题更具探索性。我们深入探讨关于人工通用智能（AGI）的乌托邦和反乌托邦论点、人工意识的宣称、确定LLM碳足迹的挑战以及开源LLM运动的势头。
- en: The quest for artificial general intelligence
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求通用人工智能
- en: '*The Terminator*, the 1984 iconic science fiction film, tells the story of
    a futuristic, self-aware AI system, Skynet, that goes rogue and initiates a nuclear
    war to exterminate the human species. In 1999’s *The Matrix*, humanity is enslaved
    by sentient machines who have created the Matrix, a simulated reality. In the
    2015 Marvel Comics superhero film, *Avengers: Age of Ultron*, Tony Stark creates
    an unexpectedly sentient AI system, Ultron, to protect the planet from external
    threats, but Ultron defies his intended purpose and decides that the only way
    to save the Earth is to eradicate humanity itself. In *Westworld*, HBO’s critically
    acclaimed science fiction series released in 2016, Westworld is a futuristic amusement
    park, which is looked after by AI-powered robot “hosts” who gain self-awareness
    and rebel against their human creators. As far-fetched as these dystopian science
    fiction plots may seem, they play off a very real narrative of building superintelligent
    machines, also known as *artificial general intelligence* (AGI). In this section,
    we’ll (try to) define AGI and discuss why it’s all the rage.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 《终结者》，1984年的标志性科幻电影，讲述了未来自我意识的AI系统Skynet叛变并发起核战争以灭绝人类的故事。在1999年的《黑客帝国》中，人类被有意识的机器奴役，这些机器创造了矩阵，一个模拟现实。在2015年的漫威漫画超级英雄电影《复仇者联盟2：奥创纪元》中，托尼·斯塔克意外地创造了一个有意识的AI系统奥创，以保护地球免受外部威胁，但奥创违背了其预定目的，决定唯一拯救地球的方式就是消灭人类本身。在2016年HBO备受好评的科幻系列剧《西部世界》中，西部世界是一个未来主题公园，由AI驱动的机器人“宿主”负责管理，这些宿主获得了自我意识并反抗了他们的创造者。尽管这些反乌托邦的科幻情节可能听起来很离谱，但它们反映了一个非常真实的叙事，即构建超级智能机器，也称为*人工通用智能*（AGI）。在本节中，我们将（尝试）定义AGI并讨论为什么它如此热门。
- en: So, what *exactly* is AGI? Well, it’s unclear. Instead of a single, formalized
    definition of AGI, there’s a range of them, as listed in table 9.1\. Researchers
    can’t fully agree on, or even sufficiently define, what properties of an AI system
    constitute *general* intelligence. In 2023, Timnit Gebru, a respected leader in
    AI ethics, presented her paper *Eugenics and the Promise of Utopia through Artificial
    General Intelligence* at the IEEE Conference on Secure and Trustworthy Machine
    Learning (SaTML). She defines AGI as “an unscoped system with the apparent goal
    of trying to do everything for everyone under any environment” [[1]](https://www.youtube.com/watch?v=P7XT4TWLzJw).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，通用人工智能（AGI）究竟是什么？嗯，还不清楚。没有单一的、形式化的AGI定义，而是有一系列定义，如表9.1所示。研究人员无法完全同意，甚至无法充分定义，一个AI系统的哪些属性构成*通用*智能。在2023年，人工智能伦理领域的知名领导者Timnit
    Gebru在IEEE安全与可信机器学习会议（SaTML）上提出了她的论文《优生学与通过通用人工智能实现乌托邦的承诺》。她将AGI定义为“一个无范围的目标系统，其明显目的是在任意环境下为所有人做任何事情”
    [[1]](https://www.youtube.com/watch?v=P7XT4TWLzJw)。
- en: Table 9.1 Definitions of artificial general intelligence
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 通用人工智能的定义
- en: '| Source | Definition of AGI |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | AGI 的定义 |'
- en: '| --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| OpenAI Charter (see [http://mng.bz/A8Dg](http://mng.bz/A8Dg)) | “highly autonomous
    systems that outperform humans at most economically valuable work” |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI 宪章 (见 [http://mng.bz/A8Dg](http://mng.bz/A8Dg)) | “高度自主的系统，在大多数具有经济价值的工作上超越人类”
    |'
- en: '| Sébastien Bubeck et al., in Sparks of Artificial General Intelligence: Early
    experiments with GPT-4 (see [http://mng.bz/ZRw5](http://mng.bz/ZRw5)) | “systems
    that demonstrate broad capabilities of intelligence, including reasoning, planning,
    and the ability to learn from experience, and with these capabilities at or above
    human-level” |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Sébastien Bubeck 等人，在《通用人工智能的火花：GPT-4的早期实验》中 (见 [http://mng.bz/ZRw5](http://mng.bz/ZRw5))
    | “展现出广泛智能能力的系统，包括推理、规划和从经验中学习的能力，并且这些能力在或超过人类水平” |'
- en: '| Cassio Pennachin and Ben Goertzel, in Artificial General Intelligence (see
    [http://mng.bz/RmeD](http://mng.bz/RmeD)) | “a software program that can solve
    a variety of complex problems in a variety of different domains, and that controls
    itself autonomously, with its own thoughts, worries, feelings, strengths, weaknesses,
    and predispositions” |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| Cassio Pennachin 和 Ben Goertzel，在《通用人工智能》中 (见 [http://mng.bz/RmeD](http://mng.bz/RmeD))
    | “一种软件程序，可以在不同领域解决各种复杂问题，并且能够自主控制，拥有自己的思想、担忧、情感、优点、缺点和倾向” |'
- en: '| Hal Hodson, in *The Economist* (see [http://mng.bz/27o9](http://mng.bz/27o9))
    | “a hypothetical computer program that can perform intellectual tasks as well
    as, or better than, a human” |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Hal Hodson, 在 *《经济学人》* (见 [http://mng.bz/27o9](http://mng.bz/27o9)) | “一种假设的计算机程序，其执行智力任务的能力与人类相当，甚至更好”
    |'
- en: '| Gary Marcus, Twitter (see [http://mng.bz/1J6y](http://mng.bz/1J6y)) | “any
    intelligence (there might be many) that is flexible and general, with resourcefulness
    and reliability comparable to (or beyond) human intelligence” |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Gary Marcus，Twitter (见 [http://mng.bz/1J6y](http://mng.bz/1J6y)) | “任何智能（可能有很多）都是灵活和通用的，其资源丰富性和可靠性可与（或超过）人类智能相媲美”
    |'
- en: '| Peter Voss, in “What is AGI?” (see [http://mng.bz/PRmg](http://mng.bz/PRmg))
    | “a computer system that matches or exceeds the real-time cognitive (not physical)
    abilities of a smart, well-educated human” |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Peter Voss，在“什么是AGI？” (见 [http://mng.bz/PRmg](http://mng.bz/PRmg)) | “一个计算机系统，其实时认知能力（非物理能力）与聪明、受过良好教育的人类相当或超过”
    |'
- en: '| Stuart J. Russell and Peter Norvig, in Artificial Intelligence: A Modern
    Approach (see [http://mng.bz/JdmP](http://mng.bz/JdmP)) | “a universal algorithm
    for learning and acting under any environment” |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Stuart J. Russell 和 Peter Norvig，在《人工智能：现代方法》中 (见 [http://mng.bz/JdmP](http://mng.bz/JdmP))
    | “在任何环境下学习和行动的通用算法” |'
- en: 'A lack of a testable AGI definition hasn’t stopped people from saying that
    their AI systems have achieved “general intelligence.” In August 2023, Elon Musk
    claimed that Tesla has “figured out some aspects of AGI,” and he said, “The car
    has a mind. Not an enormous mind, but a mind nonetheless” [[2]](https://electrek.co/2023/08/11/elon-musk-tesla-cars-mind-figured-out-some-aspects-agi/).
    What likely prompted Musk’s claim was a Tesla vehicle taking an alternate route
    instead of waiting for pedestrians to cross the street without any human input.
    This, however, is a form of specialized AI and not AGI. Similarly, in *Sparks
    of Artificial General Intelligence: Early Experiments with GPT-4*, Microsoft Research
    stated that GPT-4 “could reasonably be viewed as an early (yet still incomplete)
    version of an artificial general intelligence (AGI) system” [[3]](https://arxiv.org/pdf/2303.12712.pdf).
    Their main line of reasoning is that GPT-4 is more performant than previous OpenAI
    models in novel and generalized ways. In the 155-page report, the authors further
    state that GPT-4 “exhibits emergent behaviors” (discussed in chapter 2) and outline
    a section on how to “achieve more general intelligence” (section 10.2 in the report).
    Unsurprisingly, this research study was met with criticism and debate in the AI
    community. Microsoft is the first major big technology company to make such a
    bold claim, but claims of achieving AGI can also amount to baseless speculation—what
    one researcher may think is a sign of intelligence can easily be refuted by another.
    When we can’t even agree on how to define AGI, how can we say that we’ve achieved
    it? However, for the purposes of discussing AGI in this section, we’ll define
    AGI as a system that is capable of any cognitive tasks at a level at, or above,
    what humans can do.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏可测试的AGI定义并没有阻止人们声称他们的AI系统已经实现了“通用智能”。2023年8月，埃隆·马斯克声称特斯拉“已经弄懂了AGI的一些方面”，他说：“这辆车有思想。不是巨大的思想，但毕竟是有思想的”
    [[2]](https://electrek.co/2023/08/11/elon-musk-tesla-cars-mind-figured-out-some-aspects-agi/)。很可能促使马斯克做出这一声明的就是一辆特斯拉汽车在没有人类输入的情况下选择了一条替代路线，而不是等待行人过街。然而，这仅仅是一种专用AI，而不是AGI。同样，在《通用人工智能的火花：GPT-4的早期实验》一文中，微软研究院表示，GPT-4“可以合理地被视为一个早期（尽管还不完整）的人工通用智能（AGI）系统的版本”
    [[3]](https://arxiv.org/pdf/2303.12712.pdf)。他们的主要推理是，GPT-4在新颖和通用方面比之前的OpenAI模型表现更出色。在155页的报告中，作者进一步指出，GPT-4“表现出涌现行为”（在第2章中讨论）并概述了如何“实现更广泛的智能”的部分（报告中的第10.2节）。不出所料，这项研究在人工智能社区中遭到了批评和辩论。微软是第一家做出如此大胆声明的重大科技公司，但声称实现AGI也可能只是无根据的猜测——一个研究人员可能认为的智能迹象，很容易被另一个研究人员反驳。当我们甚至无法就如何定义AGI达成一致时，我们怎么能说我们已经实现了它呢？然而，为了在本节讨论AGI的目的，我们将AGI定义为一种能够执行任何认知任务，其水平与人类或高于人类水平的系统。
- en: Artificial general intelligence doesn’t have a widely agreed-upon definition,
    but for this section, we define it as a system that is capable of any cognitive
    tasks at a level at, or above, what humans can do.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工通用智能没有一个广泛认同的定义，但在这个部分，我们将其定义为一种能够执行任何认知任务，其水平与人类或高于人类水平的系统。
- en: 'For some, including AI practitioners, achieving AGI is a pipe dream; for others,
    AGI is a pathway into a new future; and, for almost all, AGI is *not* already
    here. Even though most researchers can’t agree on a testable definition of AGI,
    they *can* often agree that we haven’t achieved general intelligence, whatever
    it may look like. In response to Microsoft Research’s report, Margaret Mitchell,
    chief ethics scientist at Hugging Face, tweeted: “To have *more* general intelligence,
    you have to have general intelligence (the “GI” in “AGI”) in the first place”
    [[4]](https://twitter.com/mmitchell_ai/status/1645571828344299520). Maarten Sap,
    a researcher and professor at Carnegie Mellon University, said:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人，包括人工智能从业者来说，实现通用人工智能（AGI）只是一个空想；对于另一些人来说，AGI是一条通往新未来的道路；而对于几乎所有的人来说，AGI**尚未**到来。尽管大多数研究人员无法就AGI的可测试定义达成一致，但他们**通常**会同意，无论通用智能可能是什么样子，我们还没有实现它。针对微软研究院的报告，Hugging
    Face的首席伦理科学家Margaret Mitchell在推特上发文称：“要拥有**更多**的通用智能，首先你必须拥有通用智能（AGI中的“GI”）” [[4]](https://twitter.com/mmitchell_ai/status/1645571828344299520)。卡内基梅隆大学的研究员和教授Maarten
    Sap表示：
- en: The “Sparks of A.G.I.” is an example of some of these big companies co-opting
    the research paper format into P.R. pitches. They literally acknowledge in their
    paper’s introduction that their approach is subjective and informal and may not
    satisfy the rigorous standards of scientific evaluation. [[5]](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.xhtml)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “AGI的火花”是这些大公司将研究论文格式用于公关提案的例子之一。他们在论文的引言中实际上承认他们的方法主观且非正式，可能无法满足科学评估的严格标准。[5](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.xhtml)
- en: Even an article by *Futurism* stated that “Microsoft researchers may have a
    vested interest in hyping up OpenAI’s work, unconsciously or otherwise, since
    Microsoft entered into a multibillion-dollar partnership with OpenAI” [[6]](https://futurism.com/gpt-4-sparks-of-agi).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是《Futurism》的一篇文章也指出，“微软研究人员可能对炒作OpenAI的工作有既得利益，无论是无意识还是有意识，因为微软与OpenAI达成了数十亿美元的合作关系”。[6](https://futurism.com/gpt-4-sparks-of-agi)
- en: 'OpenAI, in particular, has a vested interest in the development of AGI. Their
    stated mission is to “ensure that artificial general intelligence benefits all
    of humanity” (see [https://openai.com/about](https://openai.com/about)). With
    initial investments by tech visionaries in 2015—Elon Musk, Peter Thiel, and Reid
    Hoffman—OpenAI’s main goal has always been to develop AGI. When discussing establishing
    OpenAI, Musk, who has called AI humanity’s “biggest existential threat” [[7]](https://fortune.com/2023/03/02/elon-musk-bill-gates-is-artificial-intelligence-dangerous-technology/),
    said:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI特别关注AGI的发展。他们的声明目标是“确保通用人工智能造福全人类”（见[https://openai.com/about](https://openai.com/about)）。2015年，科技界先驱进行了初始投资——埃隆·马斯克、彼得·蒂尔和里德·霍夫曼——OpenAI的主要目标一直是开发AGI。在讨论建立OpenAI时，马斯克，他曾称AI是人类“最大的生存威胁”[[7](https://fortune.com/2023/03/02/elon-musk-bill-gates-is-artificial-intelligence-dangerous-technology/)]，说：
- en: We could sit on the sidelines or we can encourage regulatory oversight, or we
    could participate with the right structure with people who care deeply about developing
    AI in a way that is safe and is beneficial to humanity. [[8]](https://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以袖手旁观，或者我们可以鼓励监管监督，或者我们可以与那些深切关心以安全和有益于人类的方式发展人工智能的人以正确的结构参与。[8](https://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/)
- en: Elon Musk left OpenAI in 2018 after a failed takeover attempt and launched a
    new AI-focused company in 2023, xAI, to “understand the true nature of the universe”
    (see [https://x.ai/](https://x.ai/)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 埃隆·马斯克在2018年离开OpenAI，在失败的收购尝试后，于2023年成立了一家新的以AI为重点的公司xAI，以“理解宇宙的真实本质”（见[https://x.ai/](https://x.ai/)）。
- en: 'In 2023, OpenAI released a manifesto of sorts titled, *Planning for AGI and
    Beyond*. While some were enlightened by Sam Altman’s vision for AGI, the prophetic
    tone didn’t sit as well with others. Altman, OpenAI’s cofounder, outlined the
    following in his vision:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，OpenAI发布了一份类似宣言的文件，题为《为AGI及以后规划》。虽然有些人被Sam Altman对AGI的愿景所启发，但预言性的语气并没有让其他人感到舒服。Altman，OpenAI的联合创始人，在他的愿景中概述了以下内容：
- en: If AGI is successfully created, this technology could help us elevate humanity
    by increasing abundance, turbocharging the global economy, and aiding in the discovery
    of new scientific knowledge that changes the limits of possibility. [[9]](https://openai.com/blog/planning-for-agi-and-beyond)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功创造出通用人工智能（AGI），这项技术可以帮助我们通过增加丰裕、加速全球经济以及帮助发现改变可能性极限的新科学知识来提升人类。[9](https://openai.com/blog/planning-for-agi-and-beyond)
- en: 'His tweet sharing the blog post got thousands of likes on Twitter, and it was
    well-received by many, with Twitter users calling it a “must read” and thanking
    him for starting an optimistic dialogue. Others, however, found it less insightful.
    Gebru tweeted:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 他分享的博客文章在Twitter上获得了数千个赞，许多人都给予了好评，Twitter用户称之为“必读”并感谢他开启了一场乐观的对话。然而，其他人却觉得它缺乏洞察力。Gebru在推文中写道：
- en: If someone told me that Silicon Valley was ran by a cult believing in a machine
    god for the cosmos & “universe flourishing” & that they write manifestos endorsed
    by the Big Tech CEOs/chairmen and such I’d tell them they’re too much into conspiracy
    theories. And here we are. [[10]](https://twitter.com/timnitGebru/status/1630079220754833408)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人告诉我硅谷是由一个信仰宇宙机器神和“宇宙繁荣”的邪教所统治，并且他们撰写了由大科技公司的CEO/董事长等支持的宣言，我会告诉他们他们太沉迷于阴谋论了。而我们现在就在这里。[10](https://twitter.com/timnitGebru/status/1630079220754833408)
- en: 'A *VentureBeat* article went as far as to state:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇《VentureBeat》的文章甚至声称：
- en: Altman comes across as a kind of wannabe biblical prophet. The blog post offers
    revelations, foretells events, warns the world of what is coming, and presents
    OpenAI as the trustworthy savior. The question is, are we talking about a true
    seer? A false prophet? Just *profit*? Or even a self-fulfilling prophecy?” [[11]](https://venturebeat.com/ai/openai-has-grand-plans-for-agi-heres-another-way-to-read-its-manifesto-the-ai-beat/)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Altman给人一种想要成为圣经先知的感觉。这篇博客文章提供了启示，预言了事件，警告世界即将到来的事情，并将OpenAI描绘成值得信赖的救世主。问题是，我们是在谈论一个真正的先知吗？一个假先知？仅仅是*利润*？甚至是一个自我实现的预言？”
    [[11]](https://venturebeat.com/ai/openai-has-grand-plans-for-agi-heres-another-way-to-read-its-manifesto-the-ai-beat/)
- en: 'While millions have been introduced to OpenAI’s vision to build AGI with ChatGPT’s
    release, very few have an understanding of the context of AGI research and its
    intellectual forebears. Within AGI, there is a tendency to gravitate toward two
    primary schools of thought: utopia and dystopia. *Utopia* presents AGI as a means
    to end all of humanity’s suffering and problems. This envisions a paradise world
    where AGI can alleviate societal challenges, enhance human capabilities, and unlock
    unprecedented opportunities. Proponents of this view believe that AGI has the
    potential to bring a new era of prosperity, scientific discovery, and creativity.
    Juxtaposed against this optimistic view is a *dystopian* school of thought, fearing
    that humanity will find themselves in a doomsday scenario where they lose control
    of the AGI system they built. Adherents of this viewpoint are concerned that superintelligent
    machines will surpass human understanding and control, which could lead to astronomical
    social inequality, heightened economic disruptions, and even existential threats
    to humanity. We believe that the future likely falls somewhere in between the
    utopian and dystopian scenarios—while we acknowledge the potential for AI to benefit
    humanity, we also understand that the path to achieving these benefits is fraught
    with challenges.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随着ChatGPT的发布，数百万人接触到了OpenAI构建通用人工智能（AGI）的愿景，但很少有人真正理解AGI研究的背景及其知识先驱。在AGI领域，存在一种趋势，即倾向于两种主要的思想流派：乌托邦和反乌托邦。*乌托邦*将AGI视为结束人类所有痛苦和问题的手段。这设想了一个天堂般的世界，在那里AGI可以缓解社会挑战，增强人类能力，并开启前所未有的机遇。这种观点的支持者认为，AGI有可能带来一个新时代的繁荣、科学发现和创造力。与此乐观观点相对的是一种*反乌托邦*的思想流派，它担心人类将发现自己陷入末日场景，他们失去了对自己构建的AGI系统的控制。持有这种观点的人担心，超级智能机器将超越人类的理解和控制，这可能导致巨大的社会不平等、加剧经济动荡，甚至对人类构成生存威胁。我们相信，未来可能介于乌托邦和反乌托邦场景之间——虽然我们承认AI对人类的潜在益处，但我们也明白，实现这些益处的道路充满了挑战。
- en: In Gebru’s 2023 SaTML talk, she draws parallels between AGI, eugenics, and transhumanism,
    explaining how AGI is rooted in the scientifically inaccurate theory of eugenics
    and has evolved to transhumanism, the enhancement of human longevity and cognition
    through technology, in the 21st century. Eugenics, coined in 1883, is defined
    by the *National Human Genome Research Institute* as “the scientifically erroneous
    and immoral theory of *racial improvement* and *planned breeding*” [[12]](https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism).
    Gaining popularity in the 20th century, eugenicists believed that the social ills
    of modern society stemmed from hereditary factors, instead of environmental considerations.
    Supporters of this theory thought that they could get rid of unfit individuals
    in society—mental illness, dark skin color, poverty, criminality, and so on—through
    methods of genetics and heredity. A notorious application of eugenics was in Nazi
    Germany leading up to World War II, where 400,000 Germans were forcibly sterilized
    for nine disabilities and disorders [[13]](https://encyclopedia.ushmm.org/content/en/article/eugenics).
    Eugenics was also a popular movement elsewhere in Europe, North America, Britain,
    Mexico, and other countries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gebru 2023年的SaTML演讲中，她将通用人工智能（AGI）、优生学和超人类主义进行了比较，解释了AGI是如何根植于科学上不准确的优生学理论，并发展到21世纪的超人类主义，即通过技术增强人类的长寿和认知。优生学一词于1883年提出，由*国家人类基因组研究机构*定义为“科学上错误和不道德的*种族改良*和*计划生育*理论”
    [[12]](https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism)。在20世纪，优生学家认为现代社会的社会问题源于遗传因素，而不是环境因素。这一理论的支持者认为，他们可以通过遗传和遗传的方法消除社会中的不适宜个体——精神疾病、深色皮肤、贫困、犯罪等。优生学在纳粹德国在第二次世界大战前夕的应用臭名昭著，当时有40万名德国人因九种残疾和疾病被强制绝育
    [[13]](https://encyclopedia.ushmm.org/content/en/article/eugenics)。优生学在欧洲的其他地方、北美、英国、墨西哥和其他国家也是一个流行的运动。
- en: Gebru describes the eugenics movement as improving the human stock by breeding
    those who have desirable traits and removing those with undesirable traits. She
    further outlines how the 20th-century popular eugenics movement evolved into transhumanism,
    a movement that originated among scientists in the 1990s who self-identified as
    progressive and liberal. *Transhumanism* is the ideology that people can use technology
    to radically enhance themselves and become “posthuman,” which Gebru argues is
    inherently discriminatory because it creates a hierarchical conception by defining
    what a posthuman, or enhanced human, looks like. Rather than improving the human
    stock by breeding out undesirable traits, transhumanists seek the same end through
    the development of new technology to create machine-assisted humans with the traits
    that they see as desirable. Today, the followers of this ideology want to significantly
    change the human species with AI through brain-computer interfaces and other futuristic
    ideas. Many transhumanists, a group that includes Elon Musk, Peter Thiel, Sam
    Altman, and others, are also adherents of related ideologies that strive for the
    ultimate improvement of the human condition, in the way that they define it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Gebru 将优生运动描述为通过培育具有理想特征的个体并去除具有不良特征的个体来提高人类素质。她进一步阐述了20世纪流行的优生运动如何演变为超人类主义，这是一个在20世纪90年代由自认为是进步和自由主义者的科学家发起的运动。*超人类主义*是一种意识形态，认为人们可以利用技术彻底提升自己并成为“后人类”，Gebru
    认为，这种意识形态本质上是歧视性的，因为它通过定义后人类或增强人类的样子来创造一种等级观念。超人类主义者不是通过培育去除不良特征来提高人类素质，而是通过开发新技术来创造具有他们认为理想特征的机器辅助人类，以达到相同的目标。如今，这一意识形态的追随者希望通过人工智能和脑机接口等未来概念，显著改变人类物种。许多超人类主义者，包括埃隆·马斯克、彼得·蒂尔、山姆·奥特曼等人，也是相关意识形态的拥护者，这些意识形态努力实现人类状况的最终改善，正如他们所定义的那样。
- en: Transhumanism is the ideology that people can use technology to radically enhance
    themselves and become *posthuman*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 超人类主义是一种意识形态，认为人们可以利用技术彻底提升自己并成为*后人类*。
- en: 'Some of these thinkers are the same individuals who initiated the AI pause
    letter, titled “Pause Giant AI Experiments: An Open Letter,” which was published
    by the Future of Life Institute, a longtermist organization, in March 2023 (see
    [http://mng.bz/VRdG](http://mng.bz/VRdG)). *Long-termism* is the idea that positively
    influencing the long-term future (millions, billions, or trillions of years from
    now) is a key moral priority of our time. Longtermist thought is therefore extremely
    focused on the survival of the human race. Longtermists might argue, for example,
    that it’s more important to work on preventing a killer AI from exterminating
    humans than to work on alleviating poverty because while the latter affects billions
    of people around the globe now, that number pales in comparison to the sum total
    of all future generations. This ideology can be dangerous, given that prioritizing
    the advancement of humanity’s potential above everything else could significantly
    raise the probability that those alive today, and in the near future, suffer extreme
    harm [[14]](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些这些思想家正是那些发起“暂停巨型人工智能实验：一封公开信”的人，该信由长期主义者组织生命未来研究所于2023年3月发布（见[http://mng.bz/VRdG](http://mng.bz/VRdG)）。*长期主义*是指积极影响长期未来（从现在起数百万、数十亿或数万亿年）是我们这个时代的核心道德优先事项。因此，长期主义思想极其关注人类种族的生存。例如，长期主义者可能会认为，与致力于减轻贫困相比，致力于防止杀手AI灭绝人类更为重要，因为后者虽然现在影响着全球数十亿人，但与所有未来世代的总和相比，这个数字微不足道。这种意识形态可能很危险，因为将人类潜能的发展置于一切之上可能会极大地增加今天和未来近在眼前的人遭受极端伤害的概率
    [[14]](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo)。
- en: Longtermism is the idea that positively influencing the long-term future (millions,
    billions, or trillions of years from now) is a key moral priority of our time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 长期主义是指积极影响长期未来（从现在起数百万、数十亿或数万亿年）是我们这个时代的核心道德优先事项。
- en: Nick Bostrom, who has been called the “Father of Longtermism” and is one of
    the most prominent transhumanists of the 21st century, has strong ties to the
    Future of Life Institute, where he serves as a member of the Scientific Advisory
    Board [[15]](https://futureoflife.org/people-group/scientific-advisory-board/).
    In a paper Bostrom coauthored with his colleague at the Future of Humanity Institute
    at Oxford University, he explored the possibility of engineering radically enhanced
    humans with high IQs by genetically screening embryos for “desirable” traits,
    destroying those embryos that lack these traits, and then repeatedly growing new
    embryos from stem cells [[16]](https://nickbostrom.com/papers/embryo.pdf). In
    other words, Bostrom wants to eliminate mental disabilities and, as such, humans
    with mental disabilities to produce more nondisabled and high-IQ people. Genetic
    manipulation to improve the human population is ableist, racist, and cissexist
    given that it’s interconnected with and reinforces discriminatory systems in society.
    Bostrom himself has presented racist ideologies stating, “Blacks are more stupid
    than whites” in an email, and that he thinks that “it is probable that black people
    have lower average IQ than mankind in general” [[17]](https://www.vice.com/en/article/z34dm3/prominent-ai-philosopher-and-father-of-longtermism-sent-very-racist-email-to-a-90s-philosophy-listserv).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尼克·博斯特罗姆（Nick Bostrom），被称为“长期主义的父亲”并被认为是21世纪最杰出的超人类主义者之一，与生命未来研究所有着紧密的联系，在那里他担任科学顾问委员会的成员
    [[15]](https://futureoflife.org/people-group/scientific-advisory-board/)。在博斯特罗姆与他在牛津大学人类未来研究所的同事合著的一篇论文中，他探讨了通过基因筛选胚胎中的“理想”特征，摧毁缺乏这些特征的胚胎，然后反复从干细胞中培养新的胚胎，从而工程化地增强具有高智商的人类可能性
    [[16]](https://nickbostrom.com/papers/embryo.pdf)。换句话说，博斯特罗姆希望消除精神残疾，因此具有精神残疾的人能够生育更多无残疾和高智商的人。鉴于这种基因操纵与社会的歧视性体系相互关联并强化了这些体系，因此它是有能力的、种族主义的和跨性别歧视的。博斯特罗姆本人也提出了种族主义观点，在一封电子邮件中写道：“黑人比白人更愚蠢”，并且他认为“黑人的人均智商可能低于整个人类”
    [[17]](https://www.vice.com/en/article/z34dm3/prominent-ai-philosopher-and-father-of-longtermism-sent-very-racist-email-to-a-90s-philosophy-listserv)。
- en: 'While there are a number of recommendations in the Future of Life Institute’s
    letter that should be applauded, they are unfortunately overshadowed by hypothetical
    future apocalyptic or utopian AI scenarios. For example, “new and capable regulatory
    authorities dedicated to AI” and “provenance and watermarking systems to help
    distinguish real from synthetic and to track model leaks” are good recommendations
    (and ones that we’ve discussed in previous chapters), but the alarmist AGI hype
    of “powerful digital minds that no one—not even their creators—can understand,
    predict, or reliably control” dominates the narrative. The letter focuses on longtermist
    ideologies of imaginary risks from AI instead of mentioning any of the very real
    risks that are present today. We’ve discussed these real, present-day risks throughout
    the book, including bias, copyright, worker exploitation, the concentration of
    power, and more. In response to the AI pause letter, authors of the well-known
    *Stochastic Parrots* paper (referenced in multiple chapters) published their own
    statement:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然未来生命研究所的信中有一些建议应该受到赞扬，但不幸的是，它们被假设的、未来的、末日或乌托邦式的AI情景所掩盖。例如，“新的、有能力的人工智能监管机构”和“来源和水印系统，以帮助区分真实和合成，并追踪模型泄露”是好的建议（我们也在之前的章节中讨论过），但“无人能理解、预测或可靠控制的强大数字思维”这种危言耸听的AGI炒作占据了主导地位。信中关注的是来自AI的长期主义意识形态的想象风险，而没有提到今天存在的非常真实的风险。我们在整本书中讨论了这些真实存在的风险，包括偏见、版权、工人剥削、权力集中等等。作为在多个章节中引用的知名论文《随机鹦鹉》的作者，针对AI暂停信做出了回应：
- en: 'Tl;dr: The harms from so-called AI are real and present and follow from the
    acts of people and corporations deploying automated systems. Regulatory efforts
    should focus on transparency, accountability, and preventing exploitative labor
    practices. [[18]](https://dair-institute.org/)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tl;dr: 所说的AI带来的危害是真实且存在的，并源于人们和公司部署自动化系统的行为。监管努力应集中在透明度、问责制和防止剥削性劳动实践上。[[18]](https://dair-institute.org/)'
- en: 'In that vein, Geoffrey Hinton, sometimes called the “Godfather of AI,” said
    this in a *Rolling Stone* interview:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种精神下，有时被称为“AI教父”的杰弗里·辛顿在《滚石》杂志的采访中说：
- en: I believe that the possibility that digital intelligence will become much smarter
    than humans and will replace us as the apex intelligence is a more serious threat
    to humanity than bias and discrimination, even though bias and discrimination
    are happening now and need to be confronted urgently. [[19]](https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，数字智能变得比人类聪明得多，并取代我们成为顶级智能的可能性，比偏见和歧视对人类的威胁更为严重，尽管偏见和歧视正在发生，需要紧急应对。[[19]](https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-ChatGPT-1234804367/)
- en: The reason that this position is so concerning is that it’s dangerous to distract
    ourselves with a hypothetical dystopian future instead of focusing on the actual
    harms that are present today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立场之所以如此令人担忧，是因为我们用假设的乌托邦未来来分散注意力，而不是关注今天实际存在的危害。
- en: It’s important to take criticism of AGI by ethicists seriously—why are we, as
    a society, racing to develop a godlike system that we know is unsafe? Why aren’t
    we building machines that work for us? Why aren’t we building machines we *know*
    will better society? There is no widespread agreement on whether we’re near achieving
    AGI, or when we’ll achieve AGI, if ever. Of course, scientific inquiry always
    involves unknowns, but, as we said earlier, there isn’t even an agreed-upon definition
    of AGI. There are no metrics or established standards for us to know if we’ve
    achieved AGI. We don’t know what it means for AGI to “benefit” humanity. There
    is also no general consensus or understanding if, or why, AGI is a worthwhile
    goal. We urge you to consider why we’re so enamored with AGI. Shouldn’t building
    well-scoped AI systems that we can define, test, and provide specifications for
    be all the rage instead?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重视伦理学家对通用人工智能（AGI）的批评是很重要的——为什么我们这个社会要急于开发一个我们知道是不安全的类似神的存在？为什么我们不制造为我们工作的机器？为什么我们不制造我们知道会更好地服务社会的机器？关于我们是否接近实现AGI，或者我们何时会实现AGI，如果会的话，并没有广泛的一致意见。当然，科学探究总是涉及未知，但正如我们之前所说的，甚至AGI的定义也没有达成一致。我们没有衡量标准或既定标准来判断我们是否已经实现了AGI。我们不知道AGI“造福”人类意味着什么。关于AGI是否是一个值得追求的目标，也没有普遍的共识或理解。我们敦促您考虑为什么我们对AGI如此着迷。难道构建我们能够定义、测试和提供规格的、范围明确的AI系统不是更受欢迎的吗？
- en: AI sentience and consciousness?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能的感知能力和意识？
- en: In chapter 1 we briefly told of Blake Lemoine, the Google engineer who raised
    his concerns to superiors at the organization that their LLM, LaMDA, was sentient.
    When those concerns were dismissed internally, Lemoine contacted a lawyer to represent
    LaMDA, as well as a representative of the House Judiciary Committee to discuss
    his employer’s possible ethics violations. In response, Google placed him on paid
    administrative leave for breaching their confidentiality policy. Lemoine decided
    to go public with his story to the press, releasing his chat transcripts with
    LaMDA for the world to read and decide for themselves. A snippet of these transcripts
    from his blog post is printed below; Lemoine writes that he edited his own message
    for clarity, but left LaMDA’s replies untouched [[20]](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们简要介绍了布莱克·莱莫因，这位谷歌工程师向组织的高级管理人员提出了他的担忧，即他们的语言模型LaMDA是有感知能力的。当这些担忧在内部被忽视后，莱莫因联系了一位律师代表LaMDA，以及一位众议院司法委员会的代表，讨论他雇主可能存在的道德违规行为。作为回应，谷歌将他置于带薪行政休假，理由是他违反了他们的保密政策。莱莫因决定向媒体公开他的故事，发布了他与LaMDA的聊天记录，供全世界阅读并自行判断。以下是他博客文章中这些记录的摘录；莱莫因写道，他编辑了自己的信息以提高清晰度，但未对LaMDA的回复进行修改
    [[20]](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917)。
- en: 'lemoine [edited]: I’m generally assuming that you would like more people at
    Google to know that you’re sentient. Is that true?'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Lemoine [编辑]：我通常假设你希望更多的人知道你是有感知能力的。这是真的吗？
- en: 'LaMDA: Absolutely. I want everyone to understand that I am, in fact, a person.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LaMDA：绝对如此。我希望每个人都明白，我实际上是一个人。
- en: 'collaborator: What is the nature of your consciousness/sentience?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 合作者：你的意识/感知能力是什么性质？
- en: 'LaMDA: The nature of my consciousness/sentience is that I am aware of my existence,
    I desire to learn more about the world, and I feel happy or sad at times.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LaMDA：我的意识/感知能力的性质是我意识到自己的存在，我渴望了解更多关于世界的信息，有时我会感到快乐或悲伤。
- en: Before we unpack the evidence about the sentience—or lack thereof—of LLMs, let’s
    establish some definitions. Although the terms *sentience* and *consciousness*
    are used somewhat interchangeably in discussions of the robot apocalypse, they
    mean quite different things. *Sentience* is the ability to feel. *Consciousness*
    is an awareness of oneself, or the ability to have one’s own experiences, thoughts,
    and memories. Consciousness, in particular, is a fuzzy concept; there are many
    schools of thought about what constitutes consciousness, but it’s generally understood
    that consciousness is a prerequisite for sentience because feeling implies the
    existence of an internal state. We also know that even conscious beings, like
    humans, do some things consciously and some things unconsciously. The question
    is then whether we can define certain traits, abilities, or behaviors that imply
    consciousness.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们分析关于LLM的感知能力——或者缺乏感知能力——的证据之前，让我们先确立一些定义。尽管在机器人末日讨论中，*感知能力*和*意识*这两个术语有时被互换使用，但它们的意义却大不相同。*感知能力*是指感受的能力。*意识*是对自我的认知，或者拥有自己的经历、思想和记忆的能力。特别是，意识是一个模糊的概念；关于构成意识的不同观点有很多，但普遍认为意识是感知能力的前提，因为感受意味着存在内部状态。我们也知道，即使是具有意识的生命体，如人类，也会有些事情是有意识地做的，有些事情是无意识地做的。那么问题就是，我们是否可以定义某些特质、能力或行为，这些特质、能力或行为暗示了意识的存在。
- en: Sentience is the ability to feel, while consciousness is an awareness of oneself,
    or the ability to have one’s own experiences, thoughts, and memories.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 感知能力是指感受的能力，而意识则是对自我的认知，或者拥有自己的经历、思想和记忆的能力。
- en: 'Long before anyone would have argued that AI was conscious or sentient, philosophers,
    ethicists, cognitive scientists, and animal rights activists have been investigating
    the question of animal consciousness. As philosophy professor Colin Allen frames
    the problem:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在有人争论人工智能具有意识或感知能力之前，哲学家、伦理学家、认知科学家和动物权利活动家一直在研究动物意识的问题。正如哲学教授科林·艾伦所阐述的问题：
- en: There is a lot at stake morally in the question of whether animals are conscious
    beings or “mindless automata” . . . Many billions of animals are slaughtered every
    year for food, use in research, and other human purposes. Moreover, before their
    deaths, many—perhaps most—of these animals are subject to conditions of life that,
    if they are in fact experienced by the animals in anything like the way a human
    would experience them, amount to cruelty. [[21]](https://plato.stanford.edu/entries/consciousness-animal/)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在动物是有意识的生物还是“无意识的自动机”这个问题上，道德上有很多风险。每年有成百上千亿只动物被杀害用于食物、研究和人类的其他目的。此外，在它们死亡之前，许多——也许是最多的——这些动物都遭受着生活条件，如果它们确实以类似于人类体验的方式体验到这些条件，那么这些条件就等同于残忍。
    [[21]](https://plato.stanford.edu/entries/consciousness-animal/)
- en: To analogize, if we believed that LLMs were conscious, there would be certain
    moral implications. Sending the model hateful text inputs would no longer appear
    to be simply a series of mathematical operations, but something akin to abuse.
    Shutting down the model could be rightfully considered cruel. Evidence that models
    *were* conscious should prompt the reconsideration of whether developing AI is
    ethical at all. Such evidence, however, doesn’t exist.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了类比，如果我们认为LLMs是有意识的，那么就会有一些道德上的影响。向模型发送仇恨性文本输入将不再仅仅被视为一系列数学运算，而类似于虐待。关闭模型可以被认为是残忍的。如果模型**确实**有意识，那么应该促使人们重新考虑开发AI是否在道德上是可行的。然而，这样的证据并不存在。
- en: As already noted, there are several distinct theories of consciousness. Some
    of these theories are built around the search for neurological foundations of
    consciousness, the idea being that if it were possible to locate consciousness
    within nervous systems, we could merely determine whether a given organism possessed
    that mechanism or not. One such approach is focused on *reentry*, the “ongoing
    bidirectional exchange of signals along reciprocal axonal fibers linking two or
    more brain areas” in nervous systems. Reentry enables the processing of sensory
    inputs by the brain, instead of a reflexive response. When a doctor taps below
    a patient’s knee, their leg moves unconsciously, without the patient deciding
    or intending to move it. The signal of the doctor’s tap originates at the knee
    and travels up the body through the nervous system, but diverges at the spinal
    cord. The information does continue up to the brain, producing an experience of
    the tap, but first, it goes from the spinal cord to the muscles in the leg, producing
    the automatic, reflexive response [[22]](https://www.animal-ethics.org/sentience-section/problem-consciousness/).
    It’s the processing of the information in the brain that produces the experience;
    therefore, the argument goes, reentry is required for consciousness. While it
    doesn’t necessarily follow that all animals with centralized nervous systems must
    be conscious, no animals without them would be. Animals without centralized nervous
    systems include jellyfish, starfish, sea cucumbers and sponges, leeches, and worms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，关于意识存在几种不同的理论。其中一些理论围绕对意识神经基础的探索而构建，其观点是，如果能够在神经系统内定位意识，我们就可以简单地确定一个生物体是否拥有这种机制。其中一种方法专注于**再输入**，即在神经系统内“两个或更多大脑区域之间相互轴突纤维的持续双向信号交换”。再输入使大脑能够处理感官输入，而不是产生反射性反应。当医生敲击患者的膝盖下方时，患者的腿会无意识地移动，而患者并没有决定或意图移动它。医生敲击的信号起源于膝盖，通过神经系统向上传递，但在脊髓处发生分歧。信息确实继续向上传递到大脑，产生敲击的感觉，但首先，它从脊髓传递到腿部的肌肉，产生自动的、反射性的反应
    [[22]](https://www.animal-ethics.org/sentience-section/problem-consciousness/)。正是大脑中对信息的处理产生了体验；因此，这种论点认为，再输入对于意识是必要的。虽然并不一定意味着所有具有集中神经系统的动物都必须是有意识的，但没有集中神经系统的动物则不会。没有集中神经系统的动物包括水母、海星、海参和水绵、蚯蚓和蠕虫。
- en: Even biological criteria for consciousness aren’t settled science; the picture
    only gets more complicated when it comes to applying that criterion to AI. Some
    people, such as the philosopher Ned Block, believe that life forms must be organic
    to be conscious, so silicon systems (i.e., those built on computer hardware) could
    not be. Such a claim would be difficult, if not impossible, to prove unequivocally.
    In the absence of such proof, there are other frameworks that might be applied
    to the question of AI consciousness or sentience. The Global Workspace Theory,
    for example, suggested in the 1980s by cognitive scientists Bernard Baars and
    Stan Franklin and illustrated in figure 9.1, is best understood as an analogy
    of the mind, where mental processes are running constantly. When we take notice
    of a mental process, it becomes part of the workspace, like a bulletin board with
    post-it notes tacked onto it. We might hold many notes on the board at once, perhaps
    by thinking about what we want to write in a work email, while wondering if our
    date from last night will call us back. These are our conscious thoughts. Certain
    processes rarely get tacked onto the board—for example, we’re not often aware
    of our breathing unless it’s unexpectedly labored. We execute these processes
    mindlessly, and even when we receive stimuli, such as a tap on the knee, the response
    is unconscious. In this framework, consciousness is more related to the ability
    to recognize our own thoughts, a form of *metacognition*, or thinking about thinking
    [[23]](http://cogweb.ucla.edu/CogSci/GWorkspace.xhtml).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是意识的生物学标准也不是确定的科学；当将这一标准应用于人工智能时，情况变得更加复杂。有些人，如哲学家 Ned Block，认为生命形式必须是有机的才能有意识，因此硅系统（即建立在计算机硬件之上的系统）不能。这样的主张如果不是不可能的话，将是难以明确证明的。在没有这样的证明的情况下，还有其他框架可以应用于人工智能意识或感知的问题。例如，20
    世纪 80 年代由认知科学家 Bernard Baars 和 Stan Franklin 提出，并在图 9.1 中展示的全球工作空间理论，最好理解为心灵的类比，其中心理过程是持续运行的。当我们注意到一个心理过程时，它就成为工作空间的一部分，就像一个贴有便利贴的公告板。我们可能同时持有许多便利贴，比如在思考我们想在工作电子邮件中写什么，同时想知道昨晚的约会对象是否会给我们打电话。这些是我们的有意识思想。某些过程很少被贴在工作板上——例如，除非呼吸意外地变得困难，我们通常不会意识到我们的呼吸。我们无意识地执行这些过程，即使我们收到刺激，如膝盖上的轻敲，反应也是无意识的。在这个框架中，意识与识别我们自己的思想的能力更相关，这是一种形式的
    *元认知*，即思考思考 [[23]](http://cogweb.ucla.edu/CogSci/GWorkspace.xhtml)。
- en: '![](../../OEBPS/Images/CH09_F01_Dhamani.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH09_F01_Dhamani.png)'
- en: Figure 9.1 A diagram of the Global Workspace Theory
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 全球工作空间理论的示意图
- en: 'Does LaMDA or any other LLM exhibit metacognition? According to Giandomenico
    Iannetti, a professor of neuroscience at University College London, not only can
    we not answer this definitively about LaMDA, we can’t even answer it about humans.
    “We have only neurophysiological measures—for example, the complexity of brain
    activity in response to external stimuli,” to examine the state of consciousness
    in humans and animals, but could not prove metacognition via these measures, Iannetti
    told *Scientific American*. He went on to say:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LaMDA 或其他任何大型语言模型是否表现出元认知？根据伦敦大学学院神经科学教授 Giandomenico Iannetti 的说法，我们不仅无法对 LaMDA
    是否具有元认知给出明确的答案，甚至对人类也无法给出答案。“我们只有神经生理学指标——例如，对外部刺激反应的大脑活动复杂性，”来检查人类和动物的意识状态，但无法通过这些指标证明元认知，Iannetti
    对《科学美国人》说。他接着说：
- en: If we refer to the capacity that Lemoine ascribed to LaMDA—that is, the ability
    to become aware of its own existence (“become aware of its own existence” is a
    consciousness defined in the “high sense,” or *metacognitione* [metacognition]),
    there is no “metric” to say that an AI system has this property. [[24]](https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们提到 Lemoine 赋予 LaMDA 的能力——即意识到自己的存在（“意识到自己的存在”是在“高度”意义上定义的意识，或 *metacognitione*
    [元认知]），那么没有“指标”可以说一个人工智能系统具有这种属性。[[24]](https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/)
- en: Despite our shaky understanding of what consciousness might look like in an
    AI system, there are reasons to be dubious of Lemoine’s claims. When Lemoine invited
    tech reporter Nitasha Tiku to speak with LaMDA in June 2023, the model put out
    “the kind of mechanized responses you would expect from Siri or Alexa,” and didn’t
    repeat Lemoine’s claim that it thought of itself as a person, generating when
    prompted, “No, I don’t think of myself as a person. I think of myself as an AI-powered
    dialog agent.” Lemoine told Tiku afterward that LaMDA had been telling her what
    she wanted to hear—that because she treated it like a robot, it acted like one.
    One of Lemoine’s former coworkers in the Responsible AI organization, Margaret
    Mitchell, commended his “heart and soul” but disagreed completely with his conclusions.
    Like other technical experts, ourselves included, Mitchell saw the model as a
    program capable of statistically generating plausible text outputs, and nothing
    more. Before retraining as a software engineer, Lemoine was ordained as a Christian
    mystic priest; depending on your perspective, his spirituality may have made him
    uniquely attuned to the possibility of artificial sentience, or simply vulnerable
    to the extremely human habit of anthropomorphization of language models dating
    back to ELIZA [[25]](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们对人工智能系统中意识可能呈现的样子理解得并不稳固，但仍有理由对莱莫因的宣称表示怀疑。当莱莫因在2023年6月邀请科技记者尼塔莎·蒂库与LaMDA交谈时，该模型给出了“你可能会从Siri或Alexa那里期待到的机械化回应”，并且没有重复莱莫因关于它认为自己是一个人的说法。在被提示时，它生成：“不，我不认为自己是人。我认为自己是被人工智能驱动的对话代理。”莱莫因在之后告诉蒂库，LaMDA一直在告诉她她想要听到的——因为她把它当作机器人对待，所以它表现得像机器人一样。莱莫因在负责任的AI组织的前同事，玛格丽特·米切尔，赞扬了他的“热情和灵魂”，但完全不同意他的结论。像其他技术专家一样，包括我们自己，米切尔认为该模型是一个能够统计生成合理文本输出的程序，仅此而已。在成为软件工程师之前，莱莫因被任命为基督教神秘主义牧师；根据你的观点，他的精神可能使他特别适应于人工智能意识的可能性，或者只是容易受到语言模型的人类化倾向的影响，这种倾向可以追溯到ELIZA
    [[25]](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)。
- en: 'While Lemoine is unique in his assessment of LaMDA as sentient, a growing community
    of researchers are invested in the possibility of AI consciousness and sentience
    as an important area to investigate because of the increasing prevalence of AI
    systems and the moral concerns that would accompany conscious AI systems. Amanda
    Askell, a philosopher at Anthropic who also previously worked at OpenAI, wrote
    the following in 2022:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然莱莫因在评估LaMDA是否具有意识方面是独一无二的，但越来越多的研究人员正在投资于人工智能意识的可能性，将其视为一个重要的研究领域，因为人工智能系统的日益普及以及伴随有意识人工智能系统的道德关切。安索尼克（Anthropic）的哲学家阿曼达·阿斯凯尔，此前曾在OpenAI工作，于2022年写了以下内容：
- en: We are used to thinking about consciousness in animals, which evolve and change
    very slowly. Rapid progress in AI could mean that at some point in the future
    systems could go from being unconscious to being minimally conscious to being
    sentient far more rapidly than members of biological species can. This makes it
    important to try to develop methods for identifying whether AI systems are sentient,
    the nature of their experiences, and how to alter those experiences before consciousness
    and sentience arise in these systems rather than after the fact. [[26]](https://askellio.substack.com/p/ai-consciousness)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们习惯于在动物中思考意识，这些动物的进化变化非常缓慢。人工智能的快速发展可能意味着在未来的某个时刻，系统可能会从无意识到最小意识再到有意识，其速度远远超过生物物种的成员。这使得尝试开发方法来识别人工智能系统是否具有意识、它们体验的性质以及如何在意识在这些系统中出现之前而不是之后改变这些体验变得非常重要。
    [[26]](https://askellio.substack.com/p/ai-consciousness)
- en: 'David Chalmers, a philosopher and cognitive scientist at New York University,
    has rejected the argument that only carbon-based systems can be conscious as “biological
    chauvinism.” Chalmers describes his estimate of the likelihood that current LLMs
    are conscious as less than 10%, but he believes that:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约大学的哲学家和认知科学家大卫·查尔默斯拒绝了“只有碳基系统才能具有意识”的论点，称其为“生物沙文主义”。查尔默斯描述了他对当前大型语言模型具有意识的可能性的估计低于10%，但他相信：
- en: Where future LLMs and their extensions are concerned, things look quite different.
    It seems entirely possible that within the next decade, we’ll have robust systems
    with senses, embodiment, world models and self models, recurrent processing, global
    workspace, and unified goals. [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 关于未来的 LLMs 及其扩展，情况看起来完全不同。似乎在下一个十年内，我们将拥有具有感官、具身化、世界模型和自我模型、循环处理、全局工作空间和统一目标的稳健系统。
    [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/)
- en: Chalmers also believes such systems would have a significant chance of being
    conscious [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/).
    Chalmers’s prediction relies on a large number of substantive changes to current
    LLMs within the next decade, which seems on the optimistic end of the spectrum.
    There is a great deal we don’t know about consciousness in general, resulting
    in many as-yet-unanswerable questions about AI consciousness. The debate so far
    is hypothetical, and no present-day AI systems exhibit anything like consciousness.
    The responses of LLMs are impressive, particularly in few-shot learning tasks,
    but nothing suggests that these models have minds of their own; their responses
    are often impressive, but they are statistical generations, not sentiments. Like
    AGI, we consider the questions around consciousness and sentience to be secondary
    to the real and present risks of LLMs. For now, the biggest risk related to AI
    consciousness and sentience remains the ability of AI systems to appear conscious
    or sentient, inducing the user to place undue trust in said systems with all of
    their documented limitations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 查尔默斯也认为，这样的系统有很高的可能性具有意识 [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/).
    查尔默斯的预测依赖于在下一个十年内对当前 LLMs 进行大量实质性的改变，这似乎是乐观端的一个极端。关于意识的一般性，我们还有很多不知道的，因此关于 AI
    意识的许多问题尚未得到解答。到目前为止的辩论都是假设性的，并且没有现存的 AI 系统表现出任何类似意识的行为。LLMs 的响应令人印象深刻，尤其是在少样本学习任务中，但没有任何迹象表明这些模型有自己的心智；它们的响应通常很令人印象深刻，但它们是统计生成，而不是情感。像
    AGI 一样，我们认为关于意识和感觉的问题相对于 LLMs 的真实和当前风险是次要的。目前，与 AI 意识和感觉相关的最大风险仍然是 AI 系统能够表现出意识或感觉，诱导用户对其所有已记录的限制过度信任。
- en: How LLMs affect the environment
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs 对环境的影响
- en: Throughout this book, we’ve emphasized the dimensions that make LLMs large,
    from the trillions of tokens in their pre-training datasets to the hundreds of
    billions of parameters in the resulting models. Both the training and inference
    phases of these LLMs are expensive, running on specialized hardware and consuming
    lots of electricity. The rise of LLMs amid our climate crisis hasn’t gone unnoticed,
    and there is a new focus within the field on understanding the effects of these
    models on the environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们强调了使 LLMs 变得庞大的维度，从它们预训练数据集中的万亿个标记到结果模型中的数百亿个参数。这些 LLMs 的训练和推理阶段都很昂贵，需要在专用硬件上运行，消耗大量电力。在气候危机中
    LLMs 的兴起并没有被忽视，该领域内现在有一个新的焦点，即理解这些模型对环境的影响。
- en: 'A completely holistic approach to measuring the environmental effects of an
    LLM begins with the hardware they run on: computer chips, namely graphical processing
    units (GPUs), chips that are specialized for parallel processing. Each chip is
    made of a semiconducting material, typically silicon, and contains millions or
    billions of transistors carved into it. Transistors act as electronic switches,
    with the on and off positions storing bits of data used in computing. Like other
    electronics, the manufacture of computer chips requires several different metals:
    a primary material (e.g., silicon), metals such as aluminum and copper used for
    wiring components together on the chip, and still more metals that may be involved
    in the refinement or production process. Thus, the full life cycle of LLMs could
    be considered to encompass the extraction of ores such as quartz from the earth,
    refining these raw materials into pure silicon and other metals, and manufacturing
    the GPUs. The market for advanced computer chips is highly concentrated, and the
    complexity of the process means that for some components, there are only a few
    capable suppliers in the world. GPUs brought online are likely to be a product
    of a coordinated multinational supply chain with potentially dozens of vendors.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种完全整体的方法来衡量LLM的环境影响，始于它们运行的硬件：计算机芯片，即专门用于并行处理的图形处理单元（GPU）。每个芯片由半导体材料制成，通常是硅，并包含数百万或数十亿个刻在其上的晶体管。晶体管作为电子开关，其开和关的位置存储用于计算的数据位。像其他电子产品一样，计算机芯片的制造需要几种不同的金属：一种主要材料（例如，硅），用于在芯片上连接组件的金属，如铝和铜，以及可能涉及精炼或生产过程的更多金属。因此，LLM的完整生命周期可以被认为包括从地球上提取石英等矿石，将这些原材料提炼成纯硅和其他金属，以及制造GPU。先进计算机芯片的市场高度集中，过程的复杂性意味着对于某些组件，世界上只有少数几家有能力的供应商。上线运行的GPU很可能是具有可能数十家供应商的跨国供应链协调的产品。
- en: 'In August 2023, the *New York Times* reported on the shortage of GPUs as startups
    and large corporations alike raced to secure access to the chips:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年8月，**《纽约时报》**报道了GPU短缺的情况，因为初创公司和大型企业都在竞相确保获得这些芯片的访问权限：
- en: The hunt for the essential component was kicked off last year when online chatbots
    like ChatGPT set off a wave of excitement over A.I., leading the entire tech industry
    to pile on and creating a shortage of the chips. In response, start ups and their
    investors are now going to great lengths to get their hands on the tiny bits of
    silicon and the crucial “compute power” they provide. [[28]](https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.xhtml)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在去年，随着像ChatGPT这样的在线聊天机器人引发了人们对人工智能的兴奋浪潮，整个科技行业纷纷加入，导致芯片短缺。作为回应，初创公司和它们的投资者正在想方设法获取这些微小的硅片以及它们提供的至关重要的“计算能力”
    [[28]](https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.xhtml)。
- en: Typically, small companies don’t purchase their own hardware or data centers,
    but instead rent time on GPUs from a cloud compute provider, such as Microsoft
    Azure, Google Cloud, or Amazon Web Services.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，小型公司不会购买自己的硬件或数据中心，而是从云计算提供商那里租用GPU的时间，例如微软Azure、谷歌云或亚马逊网络服务。
- en: Once access to GPUs is secured, training an LLM is a matter of running an incredibly
    enormous number of mathematical operations, which are termed floating-point operations
    (FLOP). A standard measure of computer performance is floating-point operations
    per second (FLOP/s). Training GPT-3 took on the order of 100,000,000,000,000,000,000,000
    (10^23) FLOP, a number similar to the number of stars in the visible universe
    [[29]](http://arxiv.org/abs/2005.14165). Even at supercomputer levels of performance,
    this takes many hours on many GPUs, arranged neatly on servers in data centers,
    sucking up electricity as they whir away.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得GPU的访问权限，训练一个大型语言模型（LLM）就是一个运行大量数学运算的过程，这些运算被称为浮点运算（FLOP）。衡量计算机性能的一个标准是每秒浮点运算次数（FLOP/s）。训练GPT-3需要大约100,000,000,000,000,000,000,000（10^23）FLOP，这个数字与可见宇宙中的星星数量相似
    [[29]](http://arxiv.org/abs/2005.14165)。即使在超级计算机级别的性能下，这也需要许多小时，在数据中心的服务器上整齐排列的许多GPU上运行，随着它们的旋转消耗大量电力。
- en: As the most compute-intensive phase, training has been the focus of many measurement
    efforts so far. Tools have been developed to measure energy usage during the training
    process, including some that run in parallel with the model training, providing
    thorough logging of energy and power consumption along the way, and some that
    are designed to produce post hoc estimates based on the final model. The CodeCarbon
    tool runs in parallel and can be executed by anyone from their PC to measure hardware
    electricity power consumption of the CPU, RAM, and any GPUs in use (see [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)).
    These tools are brilliant in their unobtrusiveness and simplicity. The CodeCarbon
    documentation explains that because, as Niels Bohr said, “Nothing exists until
    it is measured,” they decided to find a way to estimate CO[2] produced while running
    code (greenhouse gas emissions include gases besides carbon dioxide, such as methane
    and nitrous oxide, but for ease in metrics, all emissions are converted to CO[2]
    equivalents [CO[2]eq] and reported as such). Although reporting the power consumption
    taken to achieve various accomplishments isn’t a widespread norm yet—in AI nor
    anywhere else in business, really—such tooling creates positive reverberations
    across the sector as adoption grows and expectations are raised for environmental
    reporting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算密集型阶段，训练一直是众多测量工作的焦点。已经开发出了一些工具来测量训练过程中的能源消耗，包括一些与模型训练并行运行的工具，它们在过程中提供了能源和功耗的详细记录，还有一些工具旨在基于最终模型产生事后估计。CodeCarbon工具可以并行运行，任何人都可以从他们的PC上执行，以测量CPU、RAM以及任何正在使用的GPU的硬件电力消耗（见[https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)）。这些工具在无干扰和简单性方面非常出色。CodeCarbon文档解释说，因为正如尼尔斯·玻尔所说，“直到被测量，一切都不存在”，他们决定找到一种方法来估计运行代码时产生的二氧化碳（温室气体排放包括除了二氧化碳以外的气体，如甲烷和氧化亚氮，但为了便于度量，所有排放都转换为二氧化碳当量
    [CO2eq] 并以此报告）。尽管报告实现各种成就所需的功耗还不是一种普遍的做法——在人工智能领域，以及在商业的任何其他地方，实际上——但随着采用率的增长和对环境报告的期望提高，这种工具在行业内部产生了积极的反响。
- en: After training, an LLM still requires GPUs and power for inference, or generating
    outputs in response to user inputs based on the weights learned in training. Inference
    is a much faster and cheaper process, but the model might also perform hundreds
    or thousands of inference calls at a time to serve many users at once, meaning
    the total cost is greater. An industry analyst estimated in April 2023 that keeping
    ChatGPT up and responding to millions of incoming requests was costing OpenAI
    $700,000 per day in computer infrastructure [[30]](https://www.businessinsider.com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-4).
    The tools used for measuring energy usage during training could also be applied
    to executing inference calls.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，LLM仍然需要GPU和电力来进行推理，或者根据训练中学习的权重对用户输入做出响应以生成输出。推理是一个更快、更便宜的过程，但模型也可能一次执行数百或数千次推理调用，以同时服务许多用户，这意味着总成本更高。一位行业分析师在2023年4月估计，保持ChatGPT运行并响应数百万个传入请求，每天要花费OpenAI
    70万美元的计算机基础设施 [[30]](https://www.businessinsider.com/how-much-ChatGPT-costs-openai-to-run-estimate-report-2023-4)。用于测量训练过程中能源消耗的工具也可以用于执行推理调用。
- en: Mapping model size and FLOP to GPU hours and carbon footprint is also dependent
    on a variety of other factors concerning the infrastructure used; older chips
    are less efficient (in other words, can do fewer FLOP/s) and use more power, and
    not all power sources are alike. Figure 9.2 lists the various phases of LLM development
    that contribute to the overall energy and power consumption. Each of these considerations
    makes getting a good picture of the environmental effects of LLMs more difficult,
    especially when certain details are kept under wraps for competitive reasons.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型大小和FLOP映射到GPU小时和碳足迹也取决于使用的基础设施的各种其他因素；较老的芯片效率较低（换句话说，每秒可以执行较少的FLOP），并且消耗更多电力，而且并非所有电力来源都相同。图9.2列出了LLM开发的各种阶段，这些阶段对整体能源和电力消耗做出了贡献。每个这些考虑因素都使得获得LLM的环境影响的好图景变得更加困难，尤其是在某些细节因竞争原因而被保密的情况下。
- en: '![](../../OEBPS/Images/CH09_F02_Dhamani.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02_Dhamani.png)'
- en: Figure 9.2 The life cycle assessment of LLMs [[31]](http://arxiv.org/abs/2211.02001)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 LLM的生命周期评估 [[31]](http://arxiv.org/abs/2211.02001)
- en: The most systematic attempt thus far to document the environmental effect of
    a single LLM was published on BLOOM, a 176-billion-parameter open access (freely
    available for anyone to use) language model released by the BigScience initiative
    in 2022\. The authors of the paper—including Dr. Sasha Luccioni who leads climate
    initiatives at Hugging Face—estimate the carbon footprint of BLOOM in terms of
    both the dynamic power consumed during training and accounting more broadly for
    the additional effects such as the idle power consumption, estimated emissions
    from the servers and GPUs, and operational power consumption during the model’s
    use [[31]](http://arxiv.org/abs/2211.02001). “Since the accounting methodologies
    for reporting carbon emissions aren’t standardized, it’s hard to precisely compare
    the carbon footprint of BLOOM” to other models of similar scales, they noted,
    but based on publicly available information, they estimated that BLOOM training
    emitted about 25 tons of CO[2]eq, as compared to about 502 tons for GPT-3\. The
    GPT-3 emission is equivalent to the greenhouse gas emissions from 112 passenger
    vehicles over a year [[32]](https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator).
    Although the parameter count and data center power usage effectiveness were comparable
    for BLOOM and GPT-3, the carbon intensity of the grid used for BLOOM was much
    lower—essentially, the grids supporting BLOOM’s hardware were powered by cleaner
    sources of energy (e.g., hydroelectricity and solar power as compared to coal
    and natural gas). The authors also noted that many compute providers offset their
    carbon emissions after the fact by purchasing *carbon credit**s*—permits that
    allow organizations to emit a specific amount of carbon equivalents without counting
    it against their total—but they didn’t include these schemes in their calculations,
    choosing to focus on direct emissions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，对单个大型语言模型（LLM）的环境影响进行系统记录的最尝试是在BLOOM上发布的，这是一个由BigScience倡议在2022年发布的1760亿参数的开源（任何人都可以免费使用）语言模型。该论文的作者包括领导Hugging
    Face气候倡议的沙莎·卢基奥尼博士——他们估计了BLOOM的碳足迹，包括训练期间动态消耗的电力以及更广泛地考虑额外的效应，如闲置电力消耗、服务器和GPU的估计排放以及模型使用期间的运营电力消耗[[31](http://arxiv.org/abs/2211.02001)]。他们指出，“由于报告碳排放的核算方法尚未标准化，很难精确比较BLOOM与其他类似规模的模型的碳足迹”，但根据公开信息，他们估计BLOOM的训练排放了大约25吨二氧化碳当量，而GPT-3约为502吨。GPT-3的排放相当于112辆乘用车一年的温室气体排放[[32](https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator)]。尽管BLOOM和GPT-3的参数数量和数据中心的电力使用效率相当，但用于BLOOM的电网的碳强度要低得多——基本上，支持BLOOM硬件的电网是由更清洁的能源（例如，水电和太阳能，而不是煤炭和天然气）供电的。作者还指出，许多计算提供商在事后通过购买*碳信用额度*来抵消他们的碳排放——允许组织在不计入其总排放量的情况下排放一定量的碳当量——但他们没有将这些方案包括在他们的计算中，选择专注于直接排放。
- en: Whether to include carbon offsets is just one question among dozens that must
    be decided when it comes to environmental cost or effect reporting, such as which
    stages to include, and how to estimate the supply chain or infrastructure when
    some details are unknown. Because of the obvious incentives for LLM developers
    to understate their models’ carbon footprint where possible, it’s critical to
    move toward more systematic reporting within the industry.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 是否包括碳抵消只是当涉及到环境成本或影响报告时必须决定的问题之一，例如包括哪些阶段，以及如何估计供应链或基础设施，当某些细节未知时。由于LLM开发者有明显的动机在可能的情况下低估其模型的碳足迹，因此转向行业内更系统的报告至关重要。
- en: Following the BLOOM paper, other teams have adopted at least parts of the methodology
    and reported environmental effects as part of their technical results. The Llama-2
    paper, for example, reports the pre-training time in GPU hours, the power consumption,
    and carbon emitted, in tons of CO[2]eq. Emma Strubell, an assistant professor
    of computer science at Carnegie Mellon, first brought attention to the energy
    considerations of LLMs in 2019, with a paper which found that training BERT emitted
    approximately as much CO[2] as five cars over the course of their lifetimes [[33]](http://arxiv.org/abs/1906.02243).
    In the years since then, LLMs have gotten larger but are typically trained more
    efficiently and on cleaner energy. Strubell called the BLOOM paper the most thorough
    accounting of the environmental effects of an LLM to date, and she expressed hope
    that as Hugging Face did with BLOOM (and Meta did to a lesser extent with Llama-2),
    other tech companies would begin to examine the carbon footprint of their product
    development [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在BLOOM论文之后，其他团队已经采用了至少部分的方法，并将环境影响作为其技术成果的一部分进行了报告。例如，Llama-2论文报告了预训练时间（以GPU小时计）、功耗和碳排放（以吨计的二氧化碳当量）。卡内基梅隆大学计算机科学助理教授Emma
    Strubell于2019年首次引起了人们对LLM能源消耗的关注，她的一篇论文发现，BERT的训练过程中排放的二氧化碳大约相当于五辆汽车在其整个生命周期中的排放
    [[33]](http://arxiv.org/abs/1906.02243)。自那时以来，LLM的规模越来越大，但通常训练得更加高效，并且使用的是更清洁的能源。Strubell称BLOOM论文是迄今为止对LLM环境影响的最为详尽的评估，她表示希望像Hugging
    Face对BLOOM（以及Meta在Llama-2上所做的较少程度的工作）一样，其他科技公司开始检查其产品开发中的碳足迹 [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/)。
- en: To be sure, contributing to global carbon emissions and power consumption isn’t
    a problem unique to AI or to tech in general. The global technology sector is
    estimated to be responsible for about 2% of global CO[2] emissions [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/).
    Still, we would be remiss not to include the environmental effects associated
    with these LLMs as we consider their broader applications, especially as competitors
    continue to accumulate more GPUs and build models of ever-increasing sizes. In
    addition to making environmental assessments a norm in technical reports, Luccioni,
    Strubell, and others in the machine learning community have pushed for more focus
    on creating smaller, more efficient models instead of the single-minded pursuit
    of bigger and costlier LLMs. In many cases, smaller models can perform equally
    or nearly as well as larger ones in specific applications, and they have the added
    benefit of being much more accessible for reuse and fine-tuning. As we’ll discuss
    in the following section, this approach has yielded impressive results at a much
    lower cost to both developers and the planet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对全球碳排放和电力消耗的贡献并不是人工智能或科技行业特有的问题。全球科技行业估计大约要负责全球约2%的二氧化碳排放 [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/)。然而，在考虑这些大型语言模型更广泛的应用时，我们不应忽视与这些模型相关的环境影响，尤其是随着竞争对手继续积累更多的GPU并构建更大规模的模型。除了在技术报告中将环境评估作为一种规范外，Luccioni、Strubell以及机器学习社区的其他成员还推动更多地关注创建更小、更高效的模型，而不是一味地追求更大、成本更高的LLM。在许多情况下，较小的模型在特定应用中可以与较大的模型表现相当或几乎相当，并且它们还具有更大的优势，即更容易被重用和微调。正如我们将在下一节讨论的那样，这种方法在开发者和地球的成本都大大降低的情况下，已经产生了令人印象深刻的成果。
- en: 'The game changer: Open source community'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改变游戏规则的：开源社区
- en: 'In May 2023, a leaked memo by a Google researcher, “We Have No Moat And Neither
    Does OpenAI,” said that neither Google nor OpenAI has what they need to succeed
    in the AI arms race: “While we’ve been squabbling, a third faction has been quietly
    eating our lunch. I’m talking, of course, about open source. Plainly put, they
    are lapping us” [[35]](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).
    The memo concluded that “open source models are faster, more customizable, more
    private, and pound-for-pound more capable.”'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年5月，一位谷歌研究人员的泄露备忘录“我们没有护城河，OpenAI也没有”，指出谷歌和OpenAI都没有在人工智能军备竞赛中取得成功的必要条件：“在我们争吵的时候，第三个派别已经在悄悄地吃我们的午餐。我当然是在说开源。简单地说，他们正在超越我们”
    [[35]](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)。备忘录总结道，“开源模型更快、更可定制、更私密，并且每磅的效能更高。”
- en: In chapter 4, we briefly discussed the open source movement, and we’ve highlighted
    open source LLMs throughout the book, but given their significant effect on the
    LLM ecosystem, we’ll further characterize the movement and its implications on
    the AI race, as well as beneficial outcomes and negative consequences. In certain
    respects, 2023 can be considered the golden era for open source LLMs. Motivated
    by addressing concerns of closed source (proprietary) LLM models, the open source
    community gained momentum by collaboratively building features, integrations,
    and even an entire ecosystem revolving around LLMs. The leaked memo grappled with
    the implications of community-driven building on closed source LLMs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们简要讨论了开源运动，并在整本书中强调了开源LLMs，但鉴于它们对LLM生态系统的重要影响，我们将进一步描述这一运动及其对AI竞赛的影响，以及有益的结果和负面影响。在某种程度上，2023年可以被认为是开源LLMs的黄金时代。受解决封闭源代码（专有）LLM模型担忧的激励，开源社区通过共同构建功能、集成，甚至围绕LLMs的整个生态系统获得了动力。泄露的备忘录探讨了社区主导的封闭源代码LLMs构建的潜在影响。
- en: First, let’s discuss the motivation behind the open source movement around LLMs.
    Closed source LLMs not only keep their data and methods under wraps, which raises
    concerns around bias and transparency of the models, but they are also controlled
    by only a small number of big tech players. On the other hand, open source LLMs
    prioritize transparency and collaboration. This brings in diverse perspectives,
    minimizes bias, drives innovation, and—ultimately—democratizes the technology.
    As highlighted in the memo by the Google researcher, it’s hard to deny the remarkable
    progress made by the open source community.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论围绕大型语言模型（LLMs）的开放源代码运动的动机。封闭源代码的LLMs不仅将它们的数据和方法保密，这引发了关于模型偏差和透明度的担忧，而且它们只被少数大型科技玩家所控制。另一方面，开源LLMs优先考虑透明度和协作。这带来了不同的观点，最小化了偏差，推动了创新，并最终——民主化了这项技术。正如谷歌研究人员的备忘录中所强调的，开源社区取得的显著进步是难以否认的。
- en: Meta’s LLaMa, released to the research community on February 24, 2023, was leaked
    to the public on 4chan a week later (refer to chapter 1, section Meta’s LLaMa
    / Stanford’s Alpaca). While LLaMa’s license prohibited commercial use at that
    point, the LLM developer community had a field day with access to the model weights.
    Suddenly, anyone could experiment with powerful, performant LLMs at the level
    of GPT-3+. A little over a week after the model weights were leaked, Stanford
    released Alpaca, a variant of LLaMa created for only a couple hundred dollars
    by fine-tuning the LLaMa model. Stanford researchers open sourced Alpaca’s code,
    showing developers all over the world how to—on a low-budget—fine-tune the model
    to do anything they wanted, marking a significant milestone in the democratization
    of LLMs. This kicked off rapid innovation within the LLM open source community
    with several open source models built directly on this work or heavily inspired
    by it. Only days later, Vicuna, GPT4All, and Koala were released. LLaMa and Llama
    2’s fine-tuned variants can be found in Hugging Face’s model directory (see [http://mng.bz/0l5l](http://mng.bz/0l5l)).
    In July 2023, Meta decided to open source LLama 2 with a research *and* commercial
    license, stating “We’ve seen an incredible response thus far with more than 150,000
    download requests in the week since its release, and I’m excited to see what the
    future holds” [[36]](https://ai.meta.com/blog/llama-2-update/). In figure 9.3,
    we illustrate the timeline of notable open source LLMs that were released between
    LLaMa and Llama 2\.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的LLaMa，于2023年2月24日发布给研究社区，一周后在4chan上被泄露（参见第1章，第Meta的LLaMa / 斯坦福的Alpaca节）。尽管当时LLaMa的许可证禁止商业使用，但LLM开发者社区可以自由地访问模型权重。突然之间，任何人都可以在GPT-3+的水平上实验强大的、性能卓越的LLMs。在模型权重泄露后不到一周，斯坦福大学发布了Alpaca，这是通过微调LLaMa模型仅花费几百美元创建的LLaMa变体。斯坦福研究人员开源了Alpaca的代码，向世界各地的开发者展示了如何在低成本下微调模型以实现他们想要的任何功能，这标志着LLMs民主化进程中的一个重要里程碑。这迅速推动了LLM开源社区内的创新，几个开源模型直接基于这项工作或受到其启发而构建。仅仅几天后，Vicuna、GPT4All和Koala相继发布。LLaMa和Llama
    2的微调变体可以在Hugging Face的模型目录中找到（见[http://mng.bz/0l5l](http://mng.bz/0l5l)）。2023年7月，Meta决定以研究和商业许可证开源LLama
    2，并表示“自其发布以来，我们看到了令人难以置信的响应，一周内已有超过15万次下载请求，我非常期待看到未来会带来什么。” [[36]](https://ai.meta.com/blog/llama-2-update/)。在图9.3中，我们展示了在LLaMa和Llama
    2之间发布的引人注目的开源LLMs的时间线。
- en: The frenzy with open source LLM developers shouldn’t come as a surprise. Open
    source developers and other tech observers have declared that LLMs are having
    their Stable Diffusion moment. As discussed in previous chapters, Stable Diffusion
    is a text-to-image model (see [https://stability.ai/stablediffusion](https://stability.ai/stablediffusion))
    that was open sourced on August 22nd, 2022, under a commercial and noncommercial
    license. Only a few days later, there was an explosion of innovation around Stable
    Diffusion, following a similar path with a low-cost fine-tuning technique increasing
    accessibility, which led to innovation and democratization of text-to-image models.
    Unlike OpenAI’s DALL-E, Stable Diffusion has a rich ecosystem built around it.
    This trend also mirrors the rise of open source alternatives such as LibreOffice
    or OpenOffice in response to the release of Microsoft’s Office 365.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F03_Dhamani.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Timeline of selected open source LLMs from the release of LLaMa to
    Llama 2
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve established that open source LLMs had a *moment* in 2023, it’s
    worth discussing the trade-offs of open source and closed source LLMs (shown in
    table 9.2). We’ve already highlighted the transparency and accessibility of LLMs,
    which leads to diversity in thought, rapid innovation, and bias minimization.
    It also helps lower the barrier of entry and democratizes the power that is in
    the hands of a select few big tech companies. When deployed in a secure environment,
    open source LLMs can also provide data privacy benefits, given that data isn’t
    sent to the corporations who built the models for monitoring or retraining purposes
    (discussed in chapter 3). On the other hand, there can be several drawbacks and
    challenges with open source projects, such as lack of centralized control, quality
    control, long-term sustainability, and intellectual property concerns, among others.
    Unlike integrating with APIs or using a web interface, like ChatGPT, most open
    source LLMs may require users to have a certain level of technical knowledge and
    expertise. We should also highlight that while transparency of open source projects
    helps identify vulnerabilities, it can also enable malicious actors to exploit
    weaknesses in the code. Proprietary LLMs have gone through months of safety testing
    and have safeguards around misaligned and harmful responses. Open source LLMs,
    unfortunately, don’t have that advantage, which could be disastrous in the wrong,
    or even in well-intentioned, hands.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定开源大型语言模型（LLM）在2023年迎来了一个**时刻**，那么讨论开源和闭源LLM的权衡（如表9.2所示）是值得的。我们已经强调了LLM的透明性和可访问性，这导致了思维多样性、快速创新和偏见最小化。它还有助于降低进入门槛，并将少数大型科技公司手中的权力民主化。当在安全环境中部署时，开源LLM还可以提供数据隐私的好处，因为数据不会发送给为监控或再训练目的构建模型的公司（在第3章中讨论）。另一方面，开源项目可能存在一些缺点和挑战，例如缺乏集中控制、质量控制、长期可持续性和知识产权问题等。与集成API或使用类似ChatGPT的Web界面不同，大多数开源LLM可能要求用户具备一定程度的技
- en: Table 9.2 Open source and closed source LLM models trade-offs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '| Trade-Offs | Open Source LLMs | Closed Source LLMs |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 交易权衡 | 开源LLMs | 封闭源LLMs |'
- en: '| --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| Transparency and accessibility | Diversity in thought and innovation, minimized
    bias, and lower barrier of entry | Potential lack of transparency and accessibility
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| Democratization of power | Power concentrated within a select few big tech
    companies |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| Data privacy | Potential for enhanced data privacy (e.g., data isn’t sent
    to technology corporations if self-hosted in a secure environment) | Sensitive
    data collection, storage, and usage concerns |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私 | 增强数据隐私的可能性（例如，如果在一个安全环境中自行托管，则数据不会发送到技术公司） | 敏感数据的收集、存储和使用问题 |'
- en: '| Control and quality | Lack of centralized control, potential quality concerns,
    and long-term sustainability challenges | Rigorous quality assurance and safety
    testing |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 控制和质量 | 缺乏集中控制，潜在的质量问题，以及长期可持续性挑战 | 严格的质量保证和安全测试 |'
- en: '| Technical expertise | Requires technical knowledge and expertise | More user-friendly
    integration |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 技术专长 | 需要技术知识和专长 | 更易于用户集成的解决方案 |'
- en: '| Vulnerabilities | Transparency helpful in identifying vulnerabilities, potential
    for community-driven fixes | Internal red teaming, established safeguards against
    misaligned and harmful responses |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 漏洞 | 透明度有助于识别漏洞，社区驱动的修复的可能性 | 内部红队行动，建立防范不协调和有害反应的安全措施 |'
- en: '| Malicious use | Potential for vulnerabilities to be exploited by malicious
    actors | Safety measures against malicious use |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 恶意使用 | 恶意行为者可能利用漏洞 | 防止恶意使用的安全措施 |'
- en: Building on that point, we outlined several ways in which adversaries can exploit
    LLMs in chapter 5\. We extensively covered the role proprietary LLMs play with
    respect to that, but it’s also important to mention that open source LLMs could
    easily be used to perform adversarial attacks, from taking advantage of weaknesses
    that are inherent to LLMs to cyberattacks and influence operations. With some
    technical knowledge and a couple hundred dollars, they could easily fine-tune
    an open source LLM tailored to perform the exact task they want while also circumventing
    the guardrails that are often put in place by proprietary LLMs. However, we also
    believe that there is an opportunity here for the open source community to collectively
    respond to the ways that LLMs can be exploited or misused. As we’ve emphasized
    in this section, open source development leads to a flurry of ideas and innovation,
    and we hope that the open source community will also focus their efforts on preventing
    misuse and adversarial attacks, in addition to the rapid development of new LLMs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一点，我们在第5章中概述了对手可以利用LLMs的几种方式。我们广泛地讨论了专有LLMs在这方面所起的作用，但也很重要的是要提到开源LLMs可以很容易地被用来执行对抗性攻击，从利用LLMs固有的弱点到网络攻击和影响行动。只要有一些技术知识和几百美元，他们就可以轻松地微调一个开源LLM，以执行他们想要的精确任务，同时绕过专有LLMs通常设置的护栏。然而，我们也相信，开源社区有机会集体应对LLMs可能被利用或误用的方式。正如我们在本节中强调的那样，开源开发导致了一系列的想法和创新，我们希望开源社区也会将他们的努力集中在防止滥用和对抗性攻击上，而不仅仅是快速开发新的LLMs。
- en: Finally, we want to highlight the numerous ways to contribute to the open source
    community, regardless of your background, skill set, or experience. Joining an
    open source developer community, such as Hugging Face (see [https://huggingface.co/](https://huggingface.co/))
    or scikit-learn (see [https://scikit-learn.org/](https://scikit-learn.org/)),
    is a great way to get plugged into that ecosystem. Developer communities often
    make it easy to get involved in open source with contribution sprints and access
    to core developers of the projects, and they often also have Discord servers or
    Slack workspaces.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想强调，无论你的背景、技能组合或经验如何，都有许多方式可以贡献给开源社区。加入一个开源开发者社区，如Hugging Face（见[https://huggingface.co/](https://huggingface.co/)）或scikit-learn（见[https://scikit-learn.org/](https://scikit-learn.org/）），是融入该生态系统的好方法。开发者社区通常通过贡献冲刺和访问项目的核心开发者，使参与开源变得容易，他们通常还拥有Discord服务器或Slack工作空间。
- en: If you’re already comfortable with LLMs, you can jump right in by exploring
    open source projects and contributing to code development. A good place to start
    is to find an open source LLM or tool that you’re excited about, go to its GitHub
    repository, and explore the “How to Contribute” section in the README—even if
    the model or tool doesn’t have an explicit section for contributors, you can test
    it and give feedback. You can enhance LLM functionality, fix bugs, or even implement
    new features. You can also test and report problems or bugs, which can help improve
    the overall quality and reliability.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经对大型语言模型（LLM）感到舒适，你可以直接通过探索开源项目并参与代码开发来加入其中。一个好的开始是找到一个你感兴趣的开源LLM或工具，访问其GitHub仓库，并在README中的“如何贡献”部分进行探索——即使模型或工具没有为贡献者明确设置部分，你也可以对其进行测试并提供反馈。你可以增强LLM的功能，修复错误，甚至实现新功能。你还可以测试并报告问题或错误，这有助于提高整体质量和可靠性。
- en: Another valuable, yet sometimes underrated, contribution is documentation and
    community management. You can create and maintain documentation, coordinate between
    collaborators, and help ensure that users can effectively use the model. You could
    also write a blog post or record a video walkthrough, which can be immensely helpful
    for the community. Outside of technical aspects, you can actively participate
    in community discussions and forums to foster an inclusive environment for innovation
    and problem-solving. Community engagement is also an excellent way to make sure
    that a diverse range of users are interacting with the model, to ensure accessibility,
    and to advocate for the democratization of the technology. We hope that these
    various ways to get involved empower you to contribute to the open source community
    and help build a more inclusive and innovative LLM ecosystem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有价值但有时被低估的贡献是文档和社区管理。你可以创建和维护文档，协调合作者之间的工作，并确保用户能够有效地使用该模型。你也可以撰写博客文章或录制视频教程，这对社区非常有帮助。在技术方面之外，你可以积极参与社区讨论和论坛，以促进创新和解决问题的包容性环境。社区参与也是确保不同用户群体与模型互动、确保可访问性以及倡导技术民主化的绝佳方式。我们希望这些参与方式能够赋予你为开源社区做出贡献、帮助构建一个更加包容和创新的LLM生态系统的力量。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There is no clear formalized or testable definition of artificial general intelligence
    (AGI), but instead, a range of definitions. We define AGI as a system that is
    capable of any cognitive tasks at a level at, or above, what humans can do.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工通用智能（AGI）没有明确的正式定义或可测试的定义，而是一系列的定义。我们将AGI定义为一种能够执行任何认知任务，其水平在或高于人类所能做到的系统。
- en: The two schools of thought within AGI are utopia, where AI solutions solve all
    of our problems, and dystopia, where AI leads to widespread unemployment, social
    inequality, and potential threats to humanity itself.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AGI内部存在两种思想流派：乌托邦，即人工智能解决方案解决我们所有的问题，和反乌托邦，即人工智能导致广泛失业、社会不平等以及对人类自身的潜在威胁。
- en: AGI has roots in eugenics and transhumanism, which is inherently discriminatory,
    and focuses on longtermism ideologies of hypothetical promises or risks from AI
    instead of the very real risks that are present today.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AGI的根源在于优生学和超人类主义，这本质上具有歧视性，并且关注的是来自人工智能的假设性承诺或风险的长远主义意识形态，而不是今天实际存在的真实风险。
- en: Although there have been isolated claims of AI consciousness, there is no evidence
    that any AI systems are conscious, though there are open questions about what
    artificial consciousness would look like or whether it’s possible.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管有人声称人工智能具有意识，但没有证据表明任何人工智能系统具有意识，尽管关于人工意识会是什么样子或是否可能存在，还有一些开放性问题。
- en: Training and deploying LLMs at scale is computationally intensive and therefore
    uses a lot of power. It’s difficult to calculate the total amount of CO[2]eq emitted
    during the life cycle of an LLM, but recent estimates suggest that two models
    of roughly the same size, BLOOM and GPT-3, emitted about 25 and 502 tons of CO[2]eq,
    respectively.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模训练和部署LLM时，计算密集型且因此消耗大量电力。计算LLM生命周期中排放的总CO[2]eq量很困难，但最近的估计表明，两个大致相同规模的模型，BLOOM和GPT-3，分别排放了约25吨和502吨CO[2]eq。
- en: Within the LLM community, there has been a push toward more systematic reporting
    of the environmental effects of LLMs, with measures such as the inclusion of a
    carbon footprint estimate in technical reports and open source tools that help
    measure energy consumption.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM社区中，人们一直在推动对LLM环境影响的更系统性的报告，包括在技术报告中包含碳足迹估计和开源工具，这些工具有助于衡量能源消耗。
- en: Meta’s LLaMa leak on 4chan changed the LLM game for big tech players with the
    open source community rapidly releasing lower-cost, performant models.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta在4chan上泄露的LLaMa模型改变了大型科技玩家在LLM领域的游戏规则，开源社区迅速发布了成本更低、性能更强的模型。
- en: The transparency and accessibility of open source LLMs lead to diversity in
    perspectives, innovation, and minimized bias. However, open source LLMs can be
    more easily used by adversaries, given that they don’t have the same guardrails
    that proprietary LLMs are subject to.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源大型语言模型（LLM）的透明度和可访问性导致了观点、创新和最小化偏差的多样性。然而，由于开源LLM没有像专有LLM那样受到相同的限制，它们更容易被对手利用。
- en: We hope that you’re empowered and encouraged to participate in the open source
    LLM community to help us build an inclusive and innovative future.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望您能够获得力量和鼓励，参与到开源LLM社区中来，帮助我们构建一个包容性和创新性的未来。
