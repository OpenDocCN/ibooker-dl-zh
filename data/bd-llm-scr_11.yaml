- en: appendix E Parameter-efficient fine-tuning with LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录E：使用LoRA进行参数高效的微调
- en: '*Low-rank adaptation* (*LoRA*) is one of the most widely used techniques for
    *parameter-efficient fine-tuning*. The following discussion is based on the spam
    classification fine-tuning example given in chapter 6\. However, LoRA fine-tuning
    is also applicable to the supervised *instruction fine-tuning* discussed in chapter
    7.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*低秩适应*（LoRA）是参数高效微调中最广泛使用的技术之一。以下讨论基于第6章中给出的垃圾邮件分类微调示例。然而，LoRA微调也适用于第7章中讨论的监督*指令微调*。'
- en: E.1 Introduction to LoRA
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.1 LoRA简介
- en: LoRA is a technique that adapts a pretrained model to better suit a specific,
    often smaller dataset by adjusting only a small subset of the model’s weight parameters.
    The “low-rank” aspect refers to the mathematical concept of limiting model adjustments
    to a smaller dimensional subspace of the total weight parameter space. This effectively
    captures the most influential directions of the weight parameter changes during
    training. The LoRA method is useful and popular because it enables efficient fine-tuning
    of large models on task-specific data, significantly cutting down on the computational
    costs and resources usually required for fine-tuning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是一种技术，通过仅调整模型权重参数的小子集，将预训练模型调整以更好地适应特定（通常是较小的）数据集。其中“低秩”方面指的是将模型调整限制在总权重参数空间的一个较小维度的子空间中的数学概念。这有效地捕捉了训练过程中权重参数变化的最有影响力的方向。LoRA方法因其能够使大型模型在特定任务数据上高效微调而有用且受欢迎，显著降低了微调通常所需的计算成本和资源。
- en: Suppose a large weight matrix *W* is associated with a specific layer. LoRA
    can be applied to all linear layers in an LLM. However, we focus on a single layer
    for illustration purposes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个大的权重矩阵*W*与一个特定的层相关联。LoRA可以应用于LLM中的所有线性层。然而，为了说明目的，我们专注于单个层。
- en: When training deep neural networks, during backpropagation, we learn a D*W*
    matrix, which contains information on how much we want to update the original
    weight parameters to minimize the loss function during training. Hereafter, I
    use the term “weight” as shorthand for the model’s weight parameters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，在反向传播过程中，我们学习一个D*W*矩阵，它包含了我们在训练过程中希望更新原始权重参数以最小化损失函数的信息。此后，我使用“权重”一词作为模型权重参数的简称。
- en: 'In regular training and fine-tuning, the weight update is defined as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规训练和微调中，权重更新定义为如下：
- en: '![figure](../Images/Equation-eqs-E-1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-E-1.png)'
- en: 'The LoRA method, proposed by Hu et al. ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)),
    offers a more efficient alternative to computing the weight updates D*W* by learning
    an approximation of it:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由胡等人提出的LoRA方法([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685))，为计算权重更新D*W*提供了一个更高效的替代方案，通过学习其近似值：
- en: '![figure](../Images/Equation-eqs-E-2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-E-2.png)'
- en: where *A* and *B* are two matrices much smaller than *W*, and *AB* represents
    the matrix multiplication product between *A* and *B*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*A*和*B*是两个比*W*小得多的矩阵，*AB*表示*A*和*B*之间的矩阵乘积。
- en: 'Using LoRA, we can then reformulate the weight update we defined earlier:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA，我们可以重新表述我们之前定义的权重更新：
- en: '![figure](../Images/Equation-eqs-E-3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-E-3.png)'
- en: Figure E.1 illustrates the weight update formulas for full fine-tuning and LoRA
    side by side.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.1展示了全微调和LoRA的权重更新公式并排比较。
- en: '![figure](../Images/E-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/E-1.png)'
- en: 'Figure E.1 A comparison between weight update methods: regular fine-tuning
    and LoRA. Regular fine-tuning involves updating the pretrained weight matrix W
    directly with DW (left). LoRA uses two smaller matrices, A and B, to approximate
    DW, where the product AB is added to W, and r denotes the inner dimension, a tunable
    hyperparameter (right).'
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图E.1 比较了权重更新方法：常规微调和LoRA。常规微调直接使用DW更新预训练权重矩阵W（左）。LoRA使用两个较小的矩阵A和B来近似DW，其中AB的乘积被加到W上，r表示内维，一个可调的超参数（右）。
- en: If you paid close attention, you might have noticed that the visual representations
    of full fine-tuning and LoRA in figure E.1 differ slightly from the earlier presented
    formulas. This variation is attributed to the distributive law of matrix multiplication,
    which allows us to separate the original and updated weights rather than combine
    them. For example, in the case of regular fine-tuning with *x* as the input data,
    we can express the computation as
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你可能会注意到图 E.1 中全微调和 LoRA 的视觉表示与之前展示的公式略有不同。这种变化归因于矩阵乘法的分配律，它允许我们分离原始和更新的权重而不是将它们组合。例如，在以
    *x* 作为输入数据的常规微调情况下，我们可以将计算表示为
- en: '![figure](../Images/Equation-eqs-E-4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/Equation-eqs-E-4.png)'
- en: 'Similarly, we can write the following for LoRA:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以为 LoRA 编写以下内容：
- en: '![figure](../Images/Equation-eqs-E-5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/Equation-eqs-E-5.png)'
- en: Besides reducing the number of weights to update during training, the ability
    to keep the LoRA weight matrices separate from the original model weights makes
    LoRA even more useful in practice. Practically, this allows for the pretrained
    model weights to remain unchanged, with the LoRA matrices being applied dynamically
    after training when using the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少训练期间需要更新的权重数量外，将 LoRA 权重矩阵与原始模型权重分开的能力使 LoRA 在实践中更加有用。实际上，这允许预训练模型权重保持不变，在训练后使用模型时，LoRA
    矩阵会动态应用。
- en: Keeping the LoRA weights separate is very useful in practice because it enables
    model customization without needing to store multiple complete versions of an
    LLM. This reduces storage requirements and improves scalability, as only the smaller
    LoRA matrices need to be adjusted and saved when we customize LLMs for each specific
    customer or application.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中保持 LoRA 权重分离非常有用，因为它允许在不存储多个完整的 LLM 版本的情况下进行模型定制。这减少了存储需求并提高了可扩展性，因为当我们为每个特定的客户或应用程序定制
    LLM 时，只需要调整和保存较小的 LoRA 矩阵。
- en: Next, let’s see how LoRA can be used to fine-tune an LLM for spam classification,
    similar to the fine-tuning example in chapter 6.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看 LoRA 如何用于微调 LLM 以进行垃圾邮件分类，类似于第 6 章中的微调示例。
- en: E.2 Preparing the dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.2 准备数据集
- en: Before applying LoRA to the spam classification example, we must load the dataset
    and pretrained model we will work with. The code here repeats the data preparation
    from chapter 6\. (Instead of repeating the code, we could open and run the chapter
    6 notebook and insert the LoRA code from section E.4 there.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 LoRA 到垃圾邮件分类示例之前，我们必须加载我们将要使用的数据集和预训练模型。这里的代码重复了第 6 章的数据准备。（我们也可以打开并运行第
    6 章的笔记本，并在其中插入 E.4 节的 LoRA 代码。）
- en: First, we download the dataset and save it as CSV files.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们下载数据集并将其保存为 CSV 文件。
- en: Listing E.1 Downloading and preparing the dataset
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 E.1 下载和准备数据集
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we create the `SpamDataset` instances.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建 `SpamDataset` 实例。
- en: Listing E.2 Instantiating PyTorch datasets
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 E.2 实例化 PyTorch 数据集
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After creating the PyTorch dataset objects, we instantiate the data loaders.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 PyTorch 数据集对象之后，我们实例化数据加载器。
- en: Listing E.3 Creating PyTorch data loaders
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 E.3 创建 PyTorch 数据加载器
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a verification step, we iterate through the data loaders and check that
    the batches contain eight training examples each, where each training example
    consists of 120 tokens:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为验证步骤，我们遍历数据加载器，并检查每个批次包含八个训练示例，每个训练示例由 120 个标记组成：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Lastly, we print the total number of batches in each dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印每个数据集的总批次数：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, we have the following number of batches per dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们每个数据集有以下批次数：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: E.3 Initializing the model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.3 初始化模型
- en: We repeat the code from chapter 6 to load and prepare the pretrained GPT model.
    We begin by downloading the model weights and loading them into the `GPTModel`
    class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复第 6 章的代码来加载和准备预训练的 GPT 模型。我们首先下载模型权重并将它们加载到 `GPTModel` 类中。
- en: Listing E.4 Loading a pretrained GPT model
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 E.4 加载预训练的 GPT 模型
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Vocabulary size'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 词汇量大小'
- en: '#2 Context length'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 上下文长度'
- en: '#3 Dropout rate'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Dropout 率'
- en: '#4 Query-key-value bias'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 查询键值偏差'
- en: 'To ensure that the model was loaded corrected, let’s double-check that it generates
    coherent text:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保模型正确加载，让我们再次检查它是否生成连贯的文本：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following output shows that the model generates coherent text, which is
    an indicator that the model weights are loaded correctly:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示，模型生成了连贯的文本，这是模型权重加载正确的指标：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we prepare the model for classification fine-tuning, similar to chapter
    6, where we replace the output layer:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为分类微调准备模型，类似于第6章，我们替换输出层：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lastly, we calculate the initial classification accuracy of the not-fine-tuned
    model (we expect this to be around 50%, which means that the model is not able
    to distinguish between spam and nonspam messages yet reliably):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算未微调模型的初始分类准确率（我们预计这个值约为50%，这意味着模型还不能可靠地区分垃圾邮件和非垃圾邮件）：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The initial prediction accuracies are
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 初始预测准确率如下
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: E.4 Parameter-efficient fine-tuning with LoRA
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.4 使用LoRA进行参数高效的微调
- en: Next, we modify and fine-tune the LLM using LoRA. We begin by initializing a
    LoRALayer that creates the matrices *A* and *B*, along with the `alpha` scaling
    factor and the `rank` (*r*) setting. This layer can accept an input and compute
    the corresponding output, as illustrated in figure E.2.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用LoRA修改和微调LLM。我们首先初始化一个LoRALayer，它创建矩阵*A*和*B*，以及`alpha`缩放因子和`rank`（*r*）设置。这个层可以接受输入并计算相应的输出，如图E.2所示。
- en: '![figure](../Images/E-2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/E-2.png)'
- en: Figure E.2 The LoRA matrices A and B are applied to the layer inputs and are
    involved in computing the model outputs. The inner dimension r of these matrices
    serves as a setting that adjusts the number of trainable parameters by varying
    the sizes of A and B.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图E.2 LoRA矩阵A和B应用于层输入，并参与计算模型输出。这些矩阵的内维r作为一个设置，通过改变A和B的大小来调整可训练参数的数量。
- en: In code, this LoRA layer can be implemented as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这个LoRA层可以如下实现。
- en: Listing E.5 Implementing a LoRA layer
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表E.5 实现LoRA层
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 The same initialization used for Linear layers in PyTorch'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与PyTorch中线性层的相同初始化'
- en: The `rank` governs the inner dimension of matrices *A* and *B*. Essentially,
    this setting determines the number of extra parameters introduced by LoRA, which
    creates balance between the adaptability of the model and its efficiency via the
    number of parameters used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`rank`控制矩阵*A*和*B*的内维。本质上，这个设置决定了LoRA引入的额外参数数量，通过参数数量来平衡模型的适应性和效率。'
- en: The other important setting, `alpha`, functions as a scaling factor for the
    output from the low-rank adaptation. It primarily dictates the degree to which
    the output from the adapted layer can affect the original layer’s output. This
    can be seen as a way to regulate the effect of the low-rank adaptation on the
    layer’s output. The `LoRALayer` class we have implemented so far enables us to
    transform the inputs of a layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的设置，`alpha`，作为低秩适应输出的缩放因子。它主要决定了适应层输出对原始层输出的影响程度。这可以看作是一种调节低秩适应对层输出影响的方法。我们迄今为止实现的`LoRALayer`类使我们能够转换层的输入。
- en: In LoRA, the typical goal is to substitute existing `Linear` layers, allowing
    weight updates to be applied directly to the pre-existing pretrained weights,
    as illustrated in figure E.3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoRA中，典型的目标是用现有的`线性`层进行替换，允许直接将权重更新应用于预训练的现有权重，如图E.3所示。
- en: '![figure](../Images/E-3.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/E-3.png)'
- en: Figure E.3 The integration of LoRA into a model layer. The original pretrained
    weights (W) of a layer are combined with the outputs from LoRA matrices (A and
    B), which approximate the weight update matrix (DW). The final output is calculated
    by adding the output of the adapted layer (using LoRA weights) to the original
    output.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图E.3 LoRA集成到模型层中。层的原始预训练权重（W）与LoRA矩阵（A和B）的输出相结合，这些输出近似权重更新矩阵（DW）。最终输出是通过将适应层（使用LoRA权重）的输出与原始输出相加来计算的。
- en: To integrate the original `Linear` layer weights, we now create a `LinearWithLoRA`
    layer. This layer utilizes the previously implemented `LoRALayer` and is designed
    to replace existing `Linear` layers within a neural network, such as the self-attention
    modules or feed-forward modules in the `GPTModel`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了整合原始的`线性`层权重，我们现在创建一个`LinearWithLoRA`层。这个层利用之前实现的`LoRALayer`，并设计用来替换神经网络中的现有`线性`层，例如`GPTModel`中的自注意力模块或前馈模块。
- en: Listing E.6 Replacing `Linear` layers with `LinearWithLora` layers
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表E.6 使用`LinearWithLora`层替换`线性`层
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code combines a standard `Linear` layer with the `LoRALayer`. The `forward`
    method computes the output by adding the results from the original linear layer
    and the LoRA layer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码结合了一个标准的`线性`层和`LoRALayer`。`forward`方法通过添加原始线性层和LoRA层的结果来计算输出。
- en: Since the weight matrix *B* (`self.B` in `LoRALayer`) is initialized with zero
    values, the product of matrices *A* and *B* results in a zero matrix. This ensures
    that the multiplication does not alter the original weights, as adding zero does
    not change them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重矩阵*B*（`LoRALayer`中的`self.B`）是用零值初始化的，矩阵*A*和*B*的乘积结果是一个零矩阵。这确保了乘法不会改变原始权重，因为添加零不会改变它们。
- en: 'To apply LoRA to the earlier defined `GPTModel`, we introduce a `replace_linear_
    with_lora` function. This function will swap all existing `Linear` layers in the
    model with the newly created `LinearWithLoRA` layers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将LoRA应用于先前定义的`GPTModel`，我们引入了一个`replace_linear_with_lora`函数。这个函数将模型中所有现有的`Linear`层与新创建的`LinearWithLoRA`层进行交换：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Replaces the Linear layer with LinearWithLoRA'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将线性层替换为LinearWithLoRA'
- en: '#2 Recursively applies the same function to child modules'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 递归地应用于子模块'
- en: We have now implemented all the necessary code to replace the `Linear` layers
    in the `GPTModel` with the newly developed `LinearWithLoRA` layers for parameter-efficient
    fine-tuning. Next, we will apply the `LinearWithLoRA` upgrade to all `Linear`
    layers found in the multihead attention, feed-forward modules, and the output
    layer of the `GPTModel`, as shown in figure E.4.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经实现了替换`GPTModel`中`Linear`层为新开发的`LinearWithLoRA`层以进行参数高效微调的所有必要代码。接下来，我们将应用`LinearWithLoRA`升级到`GPTModel`中所有`Linear`层，包括多头注意力、前馈模块和输出层，如图E.4所示。
- en: '![figure](../Images/E-4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/E-4.png)'
- en: Figure E.4 The architecture of the GPT model. It highlights the parts of the
    model where `Linear` layers are upgraded to `LinearWithLoRA` layers for parameter-efficient
    fine-tuning.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图E.4 GPT模型的架构。它突出了模型中`Linear`层升级为`LinearWithLoRA`层以进行参数高效微调的部分。
- en: 'Before we apply the `LinearWithLoRA` layer upgrades, we first freeze the original
    model parameters:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用`LinearWithLoRA`层升级之前，我们首先冻结原始模型参数：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can see that none of the 124 million model parameters are trainable:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到，在1.24亿个模型参数中，没有任何一个是可训练的：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we use the `replace_linear_with_lora` to replace the `Linear` layers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`replace_linear_with_lora`来替换`Linear`层：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After adding the LoRA layers, the number of trainable parameters is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 添加LoRA层后，可训练参数的数量如下：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we can see, we reduced the number of trainable parameters by almost 50× when
    using LoRA. A `rank` and `alpha` of 16 are good default choices, but it is also
    common to increase the rank parameter, which in turn increases the number of trainable
    parameters. Alpha is usually chosen to be half, double, or equal to the rank.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用LoRA时，我们将可训练参数的数量减少了近50倍。16的`rank`和`alpha`是良好的默认选择，但也很常见增加rank参数，这反过来又增加了可训练参数的数量。`alpha`通常选择为rank的一半、两倍或与rank相等。
- en: 'Let’s verify that the layers have been modified as intended by printing the
    model architecture:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打印模型架构来验证层是否已按预期修改：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output is
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The model now includes the new `LinearWithLoRA` layers, which themselves consist
    of the original `Linear` layers, set to nontrainable, and the new LoRA layers,
    which we will fine-tune.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在包括了新的`LinearWithLoRA`层，这些层本身由设置为不可训练的原始`Linear`层和新LoRA层组成，我们将对这些层进行微调。
- en: 'Before we begin fine-tuning the model, let’s calculate the initial classification
    accuracy:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始微调模型之前，让我们计算初始分类准确度：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The resulting accuracy values are
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的准确度值是
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These accuracy values are identical to the values from chapter 6\. This result
    occurs because we initialized the LoRA matrix *B* with zeros. Consequently, the
    product of matrices *AB* results in a zero matrix. This ensures that the multiplication
    does not alter the original weights since adding zero does not change them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些准确度值与第6章中的值相同。这种结果发生是因为我们用零初始化了LoRA矩阵*B*。因此，矩阵*A*和*B*的乘积结果是一个零矩阵。这确保了乘法不会改变原始权重，因为添加零不会改变它们。
- en: Now let’s move on to the exciting part—fine-tuning the model using the training
    function from chapter 6\. The training takes about 15 minutes on an M3 MacBook
    Air laptop and less than half a minute on a V100 or A100 GPU.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续到令人兴奋的部分——使用第6章中的训练函数微调模型。在M3 MacBook Air笔记本电脑上训练大约需要15分钟，在V100或A100
    GPU上不到半分钟。
- en: Listing E.7 Fine-tuning a model with LoRA layers
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表E.7 使用LoRA层微调模型
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output we see during the training is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中我们看到的输出是
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training the model with LoRA took longer than training it without LoRA (see
    chapter 6) because the LoRA layers introduce an additional computation during
    the forward pass. However, for larger models, where backpropagation becomes more
    costly, models typically train faster with LoRA than without it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA训练模型比不使用LoRA训练模型花费的时间更长（见第6章），因为LoRA层在正向传播过程中引入了额外的计算。然而，对于更大的模型，反向传播变得成本更高，模型在有LoRA的情况下通常比没有LoRA时训练得更快。
- en: 'As we can see, the model received perfect training and very high validation
    accuracy. Let’s also visualize the loss curves to better see whether the training
    has converged:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该模型接受了完美的训练，并且验证准确度非常高。让我们也可视化损失曲线，以便更好地观察训练是否收敛：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Figure E.5 plots the results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.5绘制了结果。
- en: '![figure](../Images/E-5.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/E-5.png)'
- en: Figure E.5 The training and validation loss curves over six epochs for a machine
    learning model. Initially, both training and validation loss decrease sharply
    and then they level off, indicating the model is converging, which means that
    it is not expected to improve noticeably with further training.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图E.5展示了机器学习模型在六个epoch上的训练和验证损失曲线。最初，训练和验证损失急剧下降，然后趋于平稳，表明模型正在收敛，这意味着它不会因为进一步的训练而有明显改进。
- en: 'In addition to evaluating the model based on the loss curves, let’s also calculate
    the accuracies on the full training, validation, and test set (during the training,
    we approximated the training and validation set accuracies from five batches via
    the `eval_iter=5` setting):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了根据损失曲线评估模型外，我们还可以计算整个训练、验证和测试集上的准确率（在训练过程中，我们通过`eval_iter=5`设置从五个批次中近似训练和验证集准确率）：
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The resulting accuracy values are
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的准确率值是
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These results show that the model performs well across training, validation,
    and test datasets. With a training accuracy of 100%, the model has perfectly learned
    the training data. However, the slightly lower validation and test accuracies
    (96.64% and 97.33%, respectively) suggest a small degree of overfitting, as the
    model does not generalize quite as well on unseen data compared to the training
    set. Overall, the results are very impressive, considering we fine-tuned only
    a relatively small number of model weights (2.7 million LoRA weights instead of
    the original 124 million model weights).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，该模型在训练、验证和测试数据集上表现良好。训练准确率达到100%，模型完美地学习了训练数据。然而，验证和测试准确度（分别为96.64%和97.33%）略低，表明模型存在一定程度过拟合，因为与训练集相比，模型在未见数据上的泛化能力较差。总体而言，考虑到我们仅微调了相对较少的模型权重（270万LoRA权重而不是原始的1240万模型权重），结果非常令人印象深刻。
- en: index
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引
- en: SYMBOLS
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 符号
- en: '[124M parameter](../Text/chapter-5.html#p263)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[124M参数](../Text/chapter-5.html#p263)'
- en: '[\[EOS] (end of sequence) token](../Text/chapter-2.html#p126)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[\[EOS\] (序列结束)标记](../Text/chapter-2.html#p126)'
- en: '[.reshape method](../Text/appendix-a.html#p93), [2nd](../Text/appendix-a.html#p101)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[reshape方法](../Text/appendix-a.html#p93), [第2次](../Text/appendix-a.html#p101)'
- en: '[.to() method](../Text/appendix-a.html#p78), [2nd](../Text/appendix-a.html#p293)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[.to()方法](../Text/appendix-a.html#p78), [第2次](../Text/appendix-a.html#p293)'
- en: '[.weight attribute](../Text/chapter-5.html#p254)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[.weight属性](../Text/chapter-5.html#p254)'
- en: '[.eval() mode](../Text/chapter-4.html#p221)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[.eval()模式](../Text/chapter-4.html#p221)'
- en: '[__getitem__ method](../Text/appendix-a.html#p201)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[__getitem__方法](../Text/appendix-a.html#p201)'
- en: '[\[PAD] (padding) token](../Text/chapter-2.html#p127)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[\[PAD\] (填充)标记](../Text/chapter-2.html#p127)'
- en: '[.T method](../Text/appendix-a.html#p102)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[.T方法](../Text/appendix-a.html#p102)'
- en: '[.backward() method](../Text/chapter-4.html#p130), [2nd](../Text/appendix-d.html#p45)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[.backward()方法](../Text/chapter-4.html#p130), [第2次](../Text/appendix-d.html#p45)'
- en: '[%timeit command](../Text/appendix-a.html#p317)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[%timeit命令](../Text/appendix-a.html#p317)'
- en: '[.matmul method](../Text/appendix-a.html#p106)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[.matmul方法](../Text/appendix-a.html#p106)'
- en: '[04_preference-tuning-with-dpo folder](../Text/chapter-7.html#p264)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[04_preference-tuning-with-dpo文件夹](../Text/chapter-7.html#p264)'
- en: '[355M parameter](../Text/chapter-7.html#p131)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[355M参数](../Text/chapter-7.html#p131)'
- en: '[\[BOS] (beginning of sequence) token](../Text/chapter-2.html#p125)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[\[BOS\] (序列开始)标记](../Text/chapter-2.html#p125)'
- en: '[\<|unk|> tokens](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p98),
    [3rd](../Text/chapter-2.html#p100), [4th](../Text/chapter-2.html#p109), [5th](../Text/chapter-2.html#p147)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[\</unk\>标记](../Text/chapter-2.html#p96), [第2次](../Text/chapter-2.html#p98),
    [第3次](../Text/chapter-2.html#p100), [第4次](../Text/chapter-2.html#p109), [第5次](../Text/chapter-2.html#p147)'
- en: '[.view method](../Text/chapter-3.html#p293), [2nd](../Text/chapter-3.html#p294)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[.view方法](../Text/chapter-3.html#p293), [第2次](../Text/chapter-3.html#p294)'
- en: '[__init__ constructor](../Text/chapter-3.html#p172), [2nd](../Text/chapter-4.html#p170),
    [3rd](../Text/appendix-a.html#p144)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[__init__ 构造函数](../Text/chapter-3.html#p172), [第2次](../Text/chapter-4.html#p170),
    [第3次](../Text/appendix-a.html#p144)'
- en: '[.shape attribute](../Text/appendix-a.html#p89)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[.shape 属性](../Text/appendix-a.html#p89)'
- en: '[@ operator](../Text/appendix-a.html#p110)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[@ 操作符](../Text/appendix-a.html#p110)'
- en: '[__len__ method](../Text/appendix-a.html#p202)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[__len__ 方法](../Text/appendix-a.html#p202)'
- en: '[\<|endoftext|> token](../Text/chapter-2.html#p146)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[\<|endoftext|> 标记](../Text/chapter-2.html#p146)'
- en: '[.pth extension](../Text/chapter-5.html#p242)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[.pth 扩展名](../Text/chapter-5.html#p242)'
- en: <i>Dolma\
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <i>多尔玛</i>
- en: '[An Open Corpus of Three Trillion Tokens for LLM Pretraining Research</> (Soldaini
    et al.)](../Text/chapter-1.html#p64)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[《用于LLM预训练研究的三十万亿词开放语料库》（Soldaini等人）](../Text/chapter-1.html#p64)'
- en: '[== comparison operator](../Text/appendix-a.html#p253)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[== 比较运算符](../Text/appendix-a.html#p253)'
- en: A
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: A
- en: '[arXiv](../Text/chapter-7.html#p267)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[arXiv](../Text/chapter-7.html#p267)'
- en: '[Alpaca dataset](../Text/chapter-7.html#p172), [2nd](../Text/appendix-b.html#p88)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[Alpaca 数据集](../Text/chapter-7.html#p172), [第2次](../Text/appendix-b.html#p88)'
- en: '[argmax function](../Text/chapter-5.html#p43), [2nd](../Text/chapter-5.html#p45),
    [3rd](../Text/chapter-5.html#p189), [4th](../Text/chapter-5.html#p195), [5th](../Text/chapter-6.html#p146),
    [6th](../Text/appendix-a.html#p245), [7th](../Text/appendix-a.html#p249)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[argmax 函数](../Text/chapter-5.html#p43), [第2次](../Text/chapter-5.html#p45),
    [第3次](../Text/chapter-5.html#p189), [第4次](../Text/chapter-6.html#p146), [第5次](../Text/appendix-a.html#p245),
    [第6次](../Text/appendix-a.html#p249)'
- en: attention mechanisms
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制
- en: '[coding](../Text/chapter-3.html#p1), [2nd](../Text/chapter-3.html#p23)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码](../Text/chapter-3.html#p1), [第2次](../Text/chapter-3.html#p23)'
- en: '[problem with modeling long sequences](../Text/chapter-3.html#p13)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[建模长序列的问题](../Text/chapter-3.html#p13)'
- en: '[attention scores](../Text/chapter-3.html#p46)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意力分数](../Text/chapter-3.html#p46)'
- en: '[AI (artificial intelligence)](../Text/appendix-a.html#p14)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[AI（人工智能）](../Text/appendix-a.html#p14)'
- en: '[autograd engine](../Text/appendix-a.html#p129)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[autograd 引擎](../Text/appendix-a.html#p129)'
- en: '[alpha scaling factor](../Text/appendix-e.html#p54)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[alpha 缩放因子](../Text/appendix-e.html#p54)'
- en: '[autoregressive model](../Text/chapter-1.html#p72)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[自回归模型](../Text/chapter-1.html#p72)'
- en: '[attention weights, computing step by step](../Text/chapter-3.html#p116), [2nd](../Text/chapter-3.html#p163)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意力权重，逐步计算](../Text/chapter-3.html#p116), [第2次](../Text/chapter-3.html#p163)'
- en: '[attn_scores](../Text/chapter-3.html#p173)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[attn_scores](../Text/chapter-3.html#p173)'
- en: '[Axolotl](../Text/chapter-7.html#p270)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[Axolotl](../Text/chapter-7.html#p270)'
- en: '[allowed_max_length](../Text/appendix-c.html#p114)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[allowed_max_length](../Text/appendix-c.html#p114)'
- en: '[AdamW optimizer](../Text/chapter-5.html#p156), [2nd](../Text/appendix-b.html#p59)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdamW 优化器](../Text/chapter-5.html#p156), [第2次](../Text/appendix-b.html#p59)'
- en: B
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: B
- en: '[Bahdanau attention mechanism](../Text/chapter-3.html#p24)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bahdanau 注意力机制](../Text/chapter-3.html#p24)'
- en: '[backpropagation](../Text/chapter-5.html#p63)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[反向传播](../Text/chapter-5.html#p63)'
- en: '[BERT (bidirectional encoder representations from transformers)](../Text/chapter-1.html#p48)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT（双向编码器表示从转换器）](../Text/chapter-1.html#p48)'
- en: '[BPE (byte pair encoding)](../Text/chapter-2.html#p129)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[BPE（字节对编码）](../Text/chapter-2.html#p129)'
- en: '[batch_size](../Text/chapter-7.html#p173)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[batch_size](../Text/chapter-7.html#p173)'
- en: C
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: C
- en: '[compute_accuracy function](../Text/appendix-a.html#p262), [2nd](../Text/appendix-a.html#p264)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[compute_accuracy 函数](../Text/appendix-a.html#p262), [第2次](../Text/appendix-a.html#p264)'
- en: '[causal attention mask](../Text/chapter-6.html#p137)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[因果注意力掩码](../Text/chapter-6.html#p137)'
- en: '[clip_grad_norm_ function](../Text/appendix-d.html#p34)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[clip_grad_norm_ 函数](../Text/appendix-d.html#p34)'
- en: '[calc_loss_loader function](../Text/chapter-5.html#p130)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[calc_loss_loader 函数](../Text/chapter-5.html#p130)'
- en: '[cross_entropy function](../Text/chapter-5.html#p77), [2nd](../Text/chapter-5.html#p80),
    [3rd](../Text/chapter-5.html#p91)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[交叉熵函数](../Text/chapter-5.html#p77), [第2次](../Text/chapter-5.html#p80), [第3次](../Text/chapter-5.html#p91)'
- en: '[conversational performance](../Text/chapter-7.html#p187)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[对话表现](../Text/chapter-7.html#p187)'
- en: '[custom_collate_draft_1](../Text/chapter-7.html#p67)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[custom_collate_draft_1](../Text/chapter-7.html#p67)'
- en: '[custom_collate_draft_2](../Text/chapter-7.html#p78)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[custom_collate_draft_2](../Text/chapter-7.html#p78)'
- en: '[calc_accuracy_loader function](../Text/chapter-6.html#p157)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[calc_accuracy_loader 函数](../Text/chapter-6.html#p157)'
- en: '[calc_loss_batch function](../Text/chapter-5.html#p132), [2nd](../Text/chapter-6.html#p165),
    [3rd](../Text/chapter-6.html#p167)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[calc_loss_batch 函数](../Text/chapter-5.html#p132), [第2次](../Text/chapter-6.html#p165),
    [第3次](../Text/chapter-6.html#p167)'
- en: classification
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 分类
- en: '[tasks](../Text/chapter-1.html#p41)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[任务](../Text/chapter-1.html#p41)'
- en: '[custom_collate_fn function](../Text/chapter-7.html#p112), [2nd](../Text/appendix-c.html#p104)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[custom_collate_fn 函数](../Text/chapter-7.html#p112), [第2次](../Text/appendix-c.html#p104)'
- en: '[classify_review function](../Text/chapter-6.html#p202)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[classify_review 函数](../Text/chapter-6.html#p202)'
- en: '[context_length](../Text/chapter-4.html#p23)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[context_length](../Text/chapter-4.html#p23)'
- en: '[cfg dictionary](../Text/chapter-4.html#p150)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[cfg 字典](../Text/chapter-4.html#p150)'
- en: '[computing gradients](../Text/appendix-a.html#p62)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算梯度](../Text/appendix-a.html#p62)'
- en: '[context vectors](../Text/chapter-3.html#p41), [2nd](../Text/chapter-3.html#p106),
    [3rd](../Text/chapter-3.html#p280)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[上下文向量](../Text/chapter-3.html#p41), [第2次](../Text/chapter-3.html#p106), [第3次](../Text/chapter-3.html#p280)'
- en: '[CausalAttention class](../Text/chapter-3.html#p250), [2nd](../Text/chapter-3.html#p258)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[CausalAttention类](../Text/chapter-3.html#p250), [第2次](../Text/chapter-3.html#p258)'
- en: D
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: D
- en: '[DistributedSampler](../Text/appendix-a.html#p325)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[DistributedSampler](../Text/appendix-a.html#p325)'
- en: '[dim parameter](../Text/chapter-4.html#p65), [2nd](../Text/chapter-4.html#p65)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[dim参数](../Text/chapter-4.html#p65), [第2次](../Text/chapter-4.html#p65)'
- en: '[Dataset class](../Text/chapter-2.html#p181), [2nd](../Text/chapter-6.html#p59),
    [3rd](../Text/appendix-a.html#p193), [4th](../Text/appendix-a.html#p194), [5th](../Text/appendix-a.html#p200),
    [6th](../Text/appendix-a.html#p206), [7th](../Text/appendix-a.html#p223)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset类](../Text/chapter-2.html#p181), [第2次](../Text/chapter-6.html#p59),
    [第3次](../Text/appendix-a.html#p193), [第4次](../Text/appendix-a.html#p194), [第5次](../Text/appendix-a.html#p200),
    [第6次](../Text/appendix-a.html#p206), [第7次](../Text/appendix-a.html#p223)'
- en: '[DataLoader class](../Text/chapter-7.html#p53), [2nd](../Text/chapter-7.html#p118)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[DataLoader类](../Text/chapter-7.html#p53), [第2次](../Text/chapter-7.html#p118)'
- en: datasets
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: '[downloading](../Text/chapter-7.html#p19)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[下载](../Text/chapter-7.html#p19)'
- en: '[download_and_load_gpt2 function](../Text/chapter-5.html#p261), [2nd](../Text/chapter-5.html#p274),
    [3rd](../Text/chapter-6.html#p88)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[download_and_load_gpt2函数](../Text/chapter-5.html#p261), [第2次](../Text/chapter-5.html#p274),
    [第3次](../Text/chapter-6.html#p88)'
- en: '[DummyGPTClass](../Text/chapter-4.html#p37)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[DummyGPTClass](../Text/chapter-4.html#p37)'
- en: '[DistributedDataParallel class](../Text/appendix-a.html#p333)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[DistributedDataParallel类](../Text/appendix-a.html#p333)'
- en: '[DummyLayerNorm](../Text/chapter-4.html#p35), [2nd](../Text/chapter-4.html#p48)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[DummyLayerNorm](../Text/chapter-4.html#p35), [第2次](../Text/chapter-4.html#p48)'
- en: '[placeholder](../Text/chapter-4.html#p52)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[占位符](../Text/chapter-4.html#p52)'
- en: '[DummyGPTModel](../Text/chapter-4.html#p29), [2nd](../Text/chapter-4.html#p31),
    [3rd](../Text/chapter-4.html#p33), [4th](../Text/chapter-4.html#p42)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[DummyGPTModel](../Text/chapter-4.html#p29), [第2次](../Text/chapter-4.html#p31),
    [第3次](../Text/chapter-4.html#p33), [第4次](../Text/chapter-4.html#p42)'
- en: '[deep learning](../Text/appendix-a.html#p18)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习](../Text/appendix-a.html#p18)'
- en: '[dot products](../Text/chapter-3.html#p53)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[点积](../Text/chapter-3.html#p53)'
- en: '[DDP (DistributedDataParallel) strategy](../Text/appendix-a.html#p322)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[DDP (DistributedDataParallel)策略](../Text/appendix-a.html#p322)'
- en: '[device variable](../Text/chapter-7.html#p115)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[设备变量](../Text/chapter-7.html#p115)'
- en: '[decode method](../Text/chapter-2.html#p142), [2nd](../Text/chapter-2.html#p152)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[decode方法](../Text/chapter-2.html#p142), [第2次](../Text/chapter-2.html#p152)'
- en: '[data loaders](../Text/chapter-6.html#p49), [2nd](../Text/chapter-6.html#p82)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据加载器](../Text/chapter-6.html#p49), [第2次](../Text/chapter-6.html#p82)'
- en: '[code for](../Text/appendix-c.html#p14)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码示例](../Text/appendix-c.html#p14)'
- en: dropout
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: dropout
- en: '[defined](../Text/chapter-3.html#p234)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[定义](../Text/chapter-3.html#p234)'
- en: '[drop_rate](../Text/chapter-4.html#p27)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[drop_rate](../Text/chapter-4.html#p27)'
- en: '[drop_last parameter](../Text/appendix-a.html#p213)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[drop_last参数](../Text/appendix-a.html#p213)'
- en: '[DummyTransformerBlock](../Text/chapter-4.html#p163)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[DummyTransformerBlock](../Text/chapter-4.html#p163)'
- en: '[data list](../Text/chapter-7.html#p22)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据列表](../Text/chapter-7.html#p22)'
- en: '[ddp_setup function](../Text/appendix-a.html#p340)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[ddp_setup函数](../Text/appendix-a.html#p340)'
- en: '[d_out argument](../Text/chapter-3.html#p312), [2nd](../Text/appendix-c.html#p27)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[d_out参数](../Text/chapter-3.html#p312), [第2次](../Text/appendix-c.html#p27)'
- en: '[DataFrame](../Text/chapter-6.html#p26)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[DataFrame](../Text/chapter-6.html#p26)'
- en: E
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: E
- en: '[eps variable](../Text/chapter-4.html#p78)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[eps变量](../Text/chapter-4.html#p78)'
- en: '[evaluate_model function](../Text/chapter-5.html#p150), [2nd](../Text/chapter-5.html#p150),
    [3rd](../Text/chapter-5.html#p154), [4th](../Text/chapter-6.html#p179)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[evaluate_model函数](../Text/chapter-5.html#p150), [第2次](../Text/chapter-5.html#p150),
    [第3次](../Text/chapter-5.html#p154), [第4次](../Text/chapter-6.html#p179)'
- en: '[embedding size](../Text/chapter-2.html#p239)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[嵌入大小](../Text/chapter-2.html#p239)'
- en: '[emergent behavior](../Text/chapter-1.html#p77)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[涌现行为](../Text/chapter-1.html#p77)'
- en: '[encoder](../Text/chapter-3.html#p15)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码器](../Text/chapter-3.html#p15)'
- en: '[encode method](../Text/chapter-2.html#p76), [2nd](../Text/chapter-2.html#p138),
    [3rd](../Text/chapter-2.html#p175)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[encode方法](../Text/chapter-2.html#p76), [第2次](../Text/chapter-2.html#p138),
    [第3次](../Text/chapter-2.html#p175)'
- en: '[emb_dim](../Text/chapter-4.html#p24)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[emb_dim](../Text/chapter-4.html#p24)'
- en: '[eval_iter value](../Text/chapter-6.html#p196)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[eval_iter值](../Text/chapter-6.html#p196)'
- en: F
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: F
- en: '[find_highest_gradient function](../Text/appendix-d.html#p46)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[find_highest_gradient函数](../Text/appendix-d.html#p46)'
- en: '[first_batch variable](../Text/chapter-2.html#p188)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[first_batch变量](../Text/chapter-2.html#p188)'
- en: '[FeedForward module](../Text/chapter-4.html#p106), [2nd](../Text/chapter-4.html#p108),
    [3rd](../Text/chapter-4.html#p112), [4th](../Text/chapter-4.html#p146)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[FeedForward模块](../Text/chapter-4.html#p106), [第2次](../Text/chapter-4.html#p108),
    [第3次](../Text/chapter-4.html#p112), [第4次](../Text/chapter-4.html#p146)'
- en: '[format_input function](../Text/chapter-7.html#p35), [2nd](../Text/chapter-7.html#p37),
    [3rd](../Text/chapter-7.html#p41), [4th](../Text/chapter-7.html#p230), [5th](../Text/appendix-c.html#p94),
    [6th](../Text/appendix-c.html#p96)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[format_input函数](../Text/chapter-7.html#p35), [第2次](../Text/chapter-7.html#p37),
    [第3次](../Text/chapter-7.html#p41), [第4次](../Text/chapter-7.html#p230), [第5次](../Text/appendix-c.html#p94),
    [第6次](../Text/appendix-c.html#p96)'
- en: fine-tuning
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 微调
- en: '[LLMs, to follow instructions](../Text/chapter-7.html#p1)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLMs，遵循指令](../Text/chapter-7.html#p1)'
- en: '[categories of](../Text/chapter-6.html#p11)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[类别](../Text/chapter-6.html#p11)'
- en: '[for classification](../Text/chapter-6.html#p1)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[用于分类](../Text/chapter-6.html#p1)'
- en: '[forward method](../Text/chapter-4.html#p34), [2nd](../Text/chapter-4.html#p121)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[前向方法](../Text/chapter-4.html#p34), [第2次](../Text/chapter-4.html#p121)'
- en: G
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: G
- en: '[generate_and_print_sample function](../Text/chapter-5.html#p152)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate_and_print_sample函数](../Text/chapter-5.html#p152)'
- en: '[GELU (Gaussian error linear unit)](../Text/appendix-b.html#p41)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[GELU (高斯误差线性单元)](../Text/appendix-b.html#p41)'
- en: '[activation function](../Text/chapter-4.html#p85), [2nd](../Text/chapter-4.html#p123)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[激活函数](../Text/chapter-4.html#p85), [第2次](../Text/chapter-4.html#p123)'
- en: '[GPTModel](../Text/chapter-4.html#p169), [2nd](../Text/chapter-4.html#p192),
    [3rd](../Text/chapter-4.html#p195), [4th](../Text/chapter-4.html#p199), [5th](../Text/chapter-5.html#p31),
    [6th](../Text/appendix-e.html#p66)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTModel](../Text/chapter-4.html#p169), [第2次](../Text/chapter-4.html#p192),
    [第3次](../Text/chapter-4.html#p195), [第4次](../Text/chapter-4.html#p199), [第5次](../Text/chapter-5.html#p31),
    [第6次](../Text/appendix-e.html#p66)'
- en: '[class](../Text/chapter-4.html#p202), [2nd](../Text/chapter-6.html#p90)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[类](../Text/chapter-4.html#p202), [第2次](../Text/chapter-6.html#p90)'
- en: '[code](../Text/chapter-5.html#p114)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码](../Text/chapter-5.html#p114)'
- en: '[instance](../Text/chapter-5.html#p18), [2nd](../Text/chapter-5.html#p238),
    [3rd](../Text/chapter-5.html#p286), [4th](../Text/chapter-5.html#p300)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[实例](../Text/chapter-5.html#p18), [第2次](../Text/chapter-5.html#p238), [第3次](../Text/chapter-5.html#p286),
    [第4次](../Text/chapter-5.html#p300)'
- en: '[GPT (Generative Pre-trained Transformer)](../Text/chapter-1.html#p48)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT (生成预训练变换器)](../Text/chapter-1.html#p48)'
- en: '[architecture](../Text/chapter-1.html#p69)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[架构](../Text/chapter-1.html#p69)'
- en: '[coding](../Text/chapter-4.html#p162), [2nd](../Text/chapter-4.html#p200)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码](../Text/chapter-4.html#p162), [第2次](../Text/chapter-4.html#p200)'
- en: '[implementing from scratch to generate text](../Text/chapter-4.html#p1)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[从头开始实现生成文本](../Text/chapter-4.html#p1)'
- en: '[grad_fn value](../Text/appendix-a.html#p180)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[grad_fn值](../Text/appendix-a.html#p180)'
- en: '[gpt_download.py Python module](../Text/chapter-5.html#p258)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[gpt_download.py Python模块](../Text/chapter-5.html#p258)'
- en: '[GPT_CONFIG_124M dictionary](../Text/chapter-4.html#p21), [2nd](../Text/chapter-4.html#p153),
    [3rd](../Text/chapter-4.html#p164), [4th](../Text/chapter-4.html#p173), [5th](../Text/chapter-4.html#p231),
    [6th](../Text/chapter-5.html#p16)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT_CONFIG_124M字典](../Text/chapter-4.html#p21), [第2次](../Text/chapter-4.html#p153),
    [第3次](../Text/chapter-4.html#p164), [第4次](../Text/chapter-4.html#p173), [第5次](../Text/chapter-4.html#p231),
    [第6次](../Text/chapter-5.html#p16)'
- en: '[generative text models, evaluating](../Text/chapter-5.html#p11)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[生成文本模型，评估](../Text/chapter-5.html#p11)'
- en: '[GenAI (generative AI)](../Text/chapter-1.html#p16)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[GenAI (生成式AI)](../Text/chapter-1.html#p16)'
- en: '[gpt2-medium355M-sft.pth file](../Text/chapter-7.html#p200)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[gpt2-medium355M-sft.pth文件](../Text/chapter-7.html#p200)'
- en: '[GPTDatasetV1 class](../Text/chapter-2.html#p180), [2nd](../Text/chapter-2.html#p182),
    [3rd](../Text/chapter-2.html#p184)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTDatasetV1类](../Text/chapter-2.html#p180), [第2次](../Text/chapter-2.html#p182),
    [第3次](../Text/chapter-2.html#p184)'
- en: '[generate_text_simple function](../Text/chapter-4.html#p213), [2nd](../Text/chapter-4.html#p215),
    [3rd](../Text/chapter-4.html#p217), [4th](../Text/chapter-5.html#p42), [5th](../Text/chapter-5.html#p181),
    [6th](../Text/chapter-5.html#p186)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate_text_simple函数](../Text/chapter-4.html#p213), [第2次](../Text/chapter-4.html#p215),
    [第3次](../Text/chapter-4.html#p217), [第4次](../Text/chapter-5.html#p42), [第5次](../Text/chapter-5.html#p181),
    [第6次](../Text/chapter-5.html#p186)'
- en: '[GPT-4](../Text/chapter-7.html#p209)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-4](../Text/chapter-7.html#p209)'
- en: '[GPT-2](../Text/chapter-4.html#p18)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-2](../Text/chapter-4.html#p18)'
- en: '[model](../Text/chapter-7.html#p158)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型](../Text/chapter-7.html#p158)'
- en: '[tokenizer](../Text/chapter-6.html#p54)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[分词器](../Text/chapter-6.html#p54)'
- en: '[GPT-3](../Text/chapter-1.html#p62)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-3](../Text/chapter-1.html#p62)'
- en: '[generate_model_scores function](../Text/chapter-7.html#p244), [2nd](../Text/chapter-7.html#p246)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate_model_scores函数](../Text/chapter-7.html#p244), [第2次](../Text/chapter-7.html#p246)'
- en: '[Google Colab](../Text/appendix-a.html#p48)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Colab](../Text/appendix-a.html#p48)'
- en: '[generate function](../Text/chapter-5.html#p235), [2nd](../Text/chapter-5.html#p294),
    [3rd](../Text/chapter-7.html#p142), [4th](../Text/chapter-7.html#p177), [5th](../Text/chapter-7.html#p179),
    [6th](../Text/chapter-7.html#p192), [7th](../Text/appendix-c.html#p60)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate函数](../Text/chapter-5.html#p235), [第2次](../Text/chapter-5.html#p294),
    [第3次](../Text/chapter-7.html#p142), [第4次](../Text/chapter-7.html#p177), [第5次](../Text/chapter-7.html#p179),
    [第6次](../Text/chapter-7.html#p192), [第7次](../Text/appendix-c.html#p60)'
- en: I
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: I
- en: '[init_process_group function](../Text/appendix-a.html#p335)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[init_process_group函数](../Text/appendix-a.html#p335)'
- en: '[instruction dataset](../Text/chapter-7.html#p10)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[指令数据集](../Text/chapter-7.html#p10)'
- en: '[information leakage](../Text/chapter-3.html#p216)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[信息泄露](../Text/chapter-3.html#p216)'
- en: '[input_embeddings](../Text/chapter-2.html#p261)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[输入嵌入](../Text/chapter-2.html#p261)'
- en: '[InstructionDataset class](../Text/chapter-7.html#p55), [2nd](../Text/appendix-c.html#p102)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructionDataset类](../Text/chapter-7.html#p55), [第2次](../Text/appendix-c.html#p102)'
- en: '[instruction fine-tuning](../Text/chapter-1.html#p41)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[指令微调](../Text/chapter-1.html#p41)'
- en: '[instruction following, creating data loaders for instruction dataset](../Text/chapter-7.html#p110),
    [2nd](../Text/chapter-7.html#p126)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[遵循指令，为指令数据集创建数据加载器](../Text/chapter-7.html#p110), [第2次](../Text/chapter-7.html#p126)'
- en: '[overview](../Text/chapter-7.html#p12)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[概述](../Text/chapter-7.html#p12)'
- en: '[’instruction’ object](../Text/chapter-7.html#p26)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[‘指令’对象](../Text/chapter-7.html#p26)'
- en: K
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: K
- en: '[keepdim parameter](../Text/chapter-4.html#p64)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[keepdim参数](../Text/chapter-4.html#p64)'
- en: L
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: L
- en: '[logits tensor](../Text/chapter-5.html#p85)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[logits张量](../Text/chapter-5.html#p85)'
- en: '[LinearWithLoRA layer](../Text/appendix-e.html#p70), [2nd](../Text/appendix-e.html#p83)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[带有LoRA层的线性层](../Text/appendix-e.html#p70), [第2次](../Text/appendix-e.html#p83)'
- en: '[LoRALayer class](../Text/appendix-e.html#p59), [2nd](../Text/appendix-e.html#p64)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[LoRALayer类](../Text/appendix-e.html#p59), [第2次](../Text/appendix-e.html#p64)'
- en: '[loss metric](../Text/chapter-5.html#p26)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[损失度量](../Text/chapter-5.html#p26)'
- en: '[LLMs (large language models)](../Text/chapter-2.html#p7), [2nd](../Text/chapter-2.html#p10)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLMs（大型语言模型）](../Text/chapter-2.html#p7), [第2次](../Text/chapter-2.html#p10)'
- en: '[applications of](../Text/chapter-1.html#p24)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[应用](../Text/chapter-1.html#p24)'
- en: '[building and using](../Text/chapter-1.html#p31), [2nd](../Text/chapter-1.html#p42),
    [3rd](../Text/chapter-1.html#p79)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[构建和使用](../Text/chapter-1.html#p31), [第2次](../Text/chapter-1.html#p42), [第3次](../Text/chapter-1.html#p79)'
- en: '[coding architecture](../Text/chapter-4.html#p11)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码架构](../Text/chapter-4.html#p11)'
- en: '[coding attention mechanisms, causal attention mechanism](../Text/chapter-3.html#p194),
    [2nd](../Text/chapter-3.html#p262)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码注意力机制，因果注意力机制](../Text/chapter-3.html#p194), [第2次](../Text/chapter-3.html#p262)'
- en: '[fine-tuning](../Text/chapter-7.html#p149), [2nd](../Text/chapter-7.html#p204),
    [3rd](../Text/appendix-b.html#p72)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[微调](../Text/chapter-7.html#p149), [第2次](../Text/chapter-7.html#p204), [第3次](../Text/appendix-b.html#p72)'
- en: '[fine-tuning for classification](../Text/chapter-6.html#p100), [2nd](../Text/chapter-6.html#p140),
    [3rd](../Text/chapter-6.html#p144), [4th](../Text/chapter-6.html#p173), [5th](../Text/chapter-6.html#p202)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[用于分类的微调](../Text/chapter-6.html#p100), [第2次](../Text/chapter-6.html#p140),
    [第3次](../Text/chapter-6.html#p144), [第4次](../Text/chapter-6.html#p173), [第5次](../Text/chapter-6.html#p202)'
- en: '[instruction fine-tuning, loading pretrained LLMs](../Text/chapter-7.html#p128),
    [2nd](../Text/chapter-7.html#p147)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[指令微调，加载预训练的LLMs](../Text/chapter-7.html#p128), [第2次](../Text/chapter-7.html#p147)'
- en: '[overview of](../Text/chapter-1.html#p1), [2nd](../Text/chapter-1.html#p13),
    [3rd](../Text/chapter-1.html#p22)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[概述](../Text/chapter-1.html#p1), [第2次](../Text/chapter-1.html#p13), [第3次](../Text/chapter-1.html#p22)'
- en: '[utilizing large datasets](../Text/chapter-1.html#p57)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[利用大型数据集](../Text/chapter-1.html#p57)'
- en: '[Linear layers](../Text/appendix-e.html#p60), [2nd](../Text/appendix-e.html#p68)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[线性层](../Text/appendix-e.html#p60), [第2次](../Text/appendix-e.html#p68)'
- en: '[LayerNorm](../Text/chapter-4.html#p81), [2nd](../Text/chapter-4.html#p151),
    [3rd](../Text/chapter-4.html#p171)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayerNorm](../Text/chapter-4.html#p81), [第2次](../Text/chapter-4.html#p151),
    [第3次](../Text/chapter-4.html#p171)'
- en: '[LIMA dataset](../Text/appendix-b.html#p90)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[LIMA数据集](../Text/appendix-b.html#p90)'
- en: '[layer normalization](../Text/chapter-4.html#p51), [2nd](../Text/chapter-4.html#p88)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[层归一化](../Text/chapter-4.html#p51), [第2次](../Text/chapter-4.html#p88)'
- en: '[load_state_dict method](../Text/chapter-5.html#p248)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[加载状态字典方法](../Text/chapter-5.html#p248)'
- en: '[load_weights_into_gpt function](../Text/chapter-5.html#p288), [2nd](../Text/chapter-5.html#p290),
    [3rd](../Text/chapter-5.html#p291), [4th](../Text/chapter-5.html#p292)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[将权重加载到gpt函数](../Text/chapter-5.html#p288), [第2次](../Text/chapter-5.html#p290),
    [第3次](../Text/chapter-5.html#p291), [第4次](../Text/chapter-5.html#p292)'
- en: '[loss.backward() function](../Text/chapter-4.html#p128)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[loss.backward()函数](../Text/chapter-4.html#p128)'
- en: '[Linear layer weights](../Text/appendix-e.html#p62)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[线性层权重](../Text/appendix-e.html#p62)'
- en: '[Llama 3 model](../Text/chapter-7.html#p206)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama 3模型](../Text/chapter-7.html#p206)'
- en: '[LLama 2 model](../Text/chapter-5.html#p107)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLama 2模型](../Text/chapter-5.html#p107)'
- en: '[LoRA (low-rank adaptation)](../Text/chapter-7.html#p259), [2nd](../Text/appendix-e.html#p2),
    [3rd](../Text/appendix-e.html#p4)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[LoRA（低秩自适应）](../Text/chapter-7.html#p259), [第2次](../Text/appendix-e.html#p2),
    [第3次](../Text/appendix-e.html#p4)'
- en: '[parameter-efficient fine-tuning](../Text/appendix-e.html#p25), [2nd](../Text/appendix-e.html#p41)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[参数高效的微调](../Text/appendix-e.html#p25), [第2次](../Text/appendix-e.html#p41)'
- en: M
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: M
- en: '[main function](../Text/appendix-a.html#p338)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[主函数](../Text/appendix-a.html#p338)'
- en: '[max_length](../Text/chapter-5.html#p115), [2nd](../Text/chapter-6.html#p62),
    [3rd](../Text/appendix-c.html#p82)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[最大长度](../Text/chapter-5.html#p115), [第2次](../Text/chapter-6.html#p62), [第3次](../Text/appendix-c.html#p82)'
- en: '[model.eval() function](../Text/chapter-5.html#p245)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[model.eval() 函数](../Text/chapter-5.html#p245)'
- en: '[MultiHeadAttention class](../Text/chapter-3.html#p290), [2nd](../Text/chapter-3.html#p309),
    [3rd](../Text/chapter-3.html#p310), [4th](../Text/chapter-3.html#p314), [5th](../Text/chapter-3.html#p317),
    [6th](../Text/appendix-b.html#p32)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiHeadAttention 类](../Text/chapter-3.html#p290)，[第2次](../Text/chapter-3.html#p309)，[第3次](../Text/chapter-3.html#p310)，[第4次](../Text/chapter-3.html#p314)，[第5次](../Text/chapter-3.html#p317)，[第6次](../Text/appendix-b.html#p32)'
- en: '[model.train() setting](../Text/appendix-a.html#p233)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[model.train() 设置](../Text/appendix-a.html#p233)'
- en: '[MultiHeadAttentionWrapper class](../Text/chapter-3.html#p272), [2nd](../Text/chapter-3.html#p274),
    [3rd](../Text/chapter-3.html#p282), [4th](../Text/chapter-3.html#p283), [5th](../Text/chapter-3.html#p286),
    [6th](../Text/chapter-3.html#p287), [7th](../Text/chapter-3.html#p291)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiHeadAttentionWrapper 类](../Text/chapter-3.html#p272)，[第2次](../Text/chapter-3.html#p274)，[第3次](../Text/chapter-3.html#p282)，[第4次](../Text/chapter-3.html#p283)，[第5次](../Text/chapter-3.html#p286)，[第6次](../Text/chapter-3.html#p287)，[第7次](../Text/chapter-3.html#p291)'
- en: '[machine learning](../Text/appendix-a.html#p15)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[机器学习](../Text/appendix-a.html#p15)'
- en: '[multi-head attention](../Text/chapter-3.html#p249), [2nd](../Text/chapter-3.html#p265)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[多头注意力](../Text/chapter-3.html#p249)，[第2次](../Text/chapter-3.html#p265)'
- en: '[Module base class](../Text/appendix-a.html#p143)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[模块基类](../Text/appendix-a.html#p143)'
- en: '[multiprocessing submodule](../Text/appendix-a.html#p334)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[多进程子模块](../Text/appendix-a.html#p334)'
- en: '[masked attention](../Text/chapter-3.html#p194)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[掩码注意力](../Text/chapter-3.html#p194)'
- en: '[multinomial function](../Text/chapter-5.html#p191), [2nd](../Text/chapter-5.html#p202),
    [3rd](../Text/chapter-5.html#p203)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[多项式函数](../Text/chapter-5.html#p191)，[第2次](../Text/chapter-5.html#p202)，[第3次](../Text/chapter-5.html#p203)'
- en: '[macOS](../Text/appendix-a.html#p312)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[macOS](../Text/appendix-a.html#p312)'
- en: '[model_response](../Text/chapter-7.html#p198)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型响应](../Text/chapter-7.html#p198)'
- en: '[minbpe repository](../Text/appendix-b.html#p21)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[minbpe 仓库](../Text/appendix-b.html#p21)'
- en: '[model_configs table](../Text/chapter-5.html#p278)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型配置表](../Text/chapter-5.html#p278)'
- en: '[mps device](../Text/chapter-7.html#p113)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[mps 设备](../Text/chapter-7.html#p113)'
- en: N
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: N
- en: '[NEW_CONFIG dictionary](../Text/chapter-5.html#p284)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[NEW_CONFIG 字典](../Text/chapter-5.html#p284)'
- en: neural networks
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络
- en: '[implementing feed forward network with GELU activations](../Text/chapter-4.html#p92),
    [2nd](../Text/chapter-4.html#p115)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 GELU 激活实现前馈网络](../Text/chapter-4.html#p92)，[第2次](../Text/chapter-4.html#p115)'
- en: '[nn.Linear layers](../Text/chapter-3.html#p181)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[nn.Linear 层](../Text/chapter-3.html#p181)'
- en: '[n_heads](../Text/chapter-4.html#p25)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[n_heads](../Text/chapter-4.html#p25)'
- en: '[numel() method](../Text/chapter-4.html#p178)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[numel() 方法](../Text/chapter-4.html#p178)'
- en: '[num_heads dimension](../Text/chapter-3.html#p295)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[num_heads 维度](../Text/chapter-3.html#p295)'
- en: O
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: O
- en: '[output layer nodes](../Text/chapter-6.html#p103)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出层节点](../Text/chapter-6.html#p103)'
- en: '[ollama run llama3 command](../Text/chapter-7.html#p216), [2nd](../Text/chapter-7.html#p220),
    [3rd](../Text/chapter-7.html#p225)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[运行 llama3 命令](../Text/chapter-7.html#p216)，[第2次](../Text/chapter-7.html#p220)，[第3次](../Text/chapter-7.html#p225)'
- en: '[ollama serve command](../Text/chapter-7.html#p213), [2nd](../Text/chapter-7.html#p214),
    [3rd](../Text/chapter-7.html#p215), [4th](../Text/chapter-7.html#p228)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[ollama serve 命令](../Text/chapter-7.html#p213)，[第2次](../Text/chapter-7.html#p214)，[第3次](../Text/chapter-7.html#p215)，[第4次](../Text/chapter-7.html#p228)'
- en: '[optimizer.zero_grad() method](../Text/appendix-a.html#p235)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[optimizer.zero_grad() 方法](../Text/appendix-a.html#p235)'
- en: '[Ollama application](../Text/chapter-7.html#p207), [2nd](../Text/chapter-7.html#p219)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama 应用](../Text/chapter-7.html#p207)，[第2次](../Text/chapter-7.html#p219)'
- en: '[Ollama Llama 3 method](../Text/appendix-c.html#p106)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama Llama 3 方法](../Text/appendix-c.html#p106)'
- en: '[ollama run command](../Text/chapter-7.html#p232)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[ollama run 命令](../Text/chapter-7.html#p232)'
- en: P
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: P
- en: PyTorch
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch
- en: '[and Torch](../Text/appendix-a.html#p41)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[和火炬](../Text/appendix-a.html#p41)'
- en: '[automatic differentiation](../Text/appendix-a.html#p124), [2nd](../Text/appendix-a.html#p139)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动微分](../Text/appendix-a.html#p124)，[第2次](../Text/appendix-a.html#p139)'
- en: '[computation graphs](../Text/appendix-a.html#p116)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算图](../Text/appendix-a.html#p116)'
- en: '[data loaders](../Text/chapter-7.html#p45)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据加载器](../Text/chapter-7.html#p45)'
- en: '[dataset objects](../Text/appendix-e.html#p30)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据集对象](../Text/appendix-e.html#p30)'
- en: '[efficient data loaders](../Text/appendix-a.html#p192)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[高效数据加载器](../Text/appendix-a.html#p192)'
- en: '[implementing multilayer neural networks](../Text/appendix-a.html#p141), [2nd](../Text/appendix-a.html#p190)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[实现多层神经网络](../Text/appendix-a.html#p141)，[第2次](../Text/appendix-a.html#p190)'
- en: '[installing](../Text/appendix-a.html#p24), [2nd](../Text/appendix-a.html#p51)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[安装](../Text/appendix-a.html#p24)，[第2次](../Text/appendix-a.html#p51)'
- en: '[loading and saving model weights in](../Text/chapter-5.html#p237)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[在](../Text/chapter-5.html#p237) 加载和保存模型权重'
- en: '[optimizing training performance with GPUs](../Text/appendix-a.html#p282)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 GPU 优化训练性能](../Text/appendix-a.html#p282)'
- en: '[overview](../Text/appendix-a.html#p2), [2nd](../Text/appendix-a.html#p6)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[概述](../Text/appendix-a.html#p2)，[第2次](../Text/appendix-a.html#p6)'
- en: '[with a NumPy-like API](../Text/appendix-a.html#p59)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[具有类似 NumPy 的 API](../Text/appendix-a.html#p59)'
- en: '[pip installer](../Text/chapter-2.html#p132)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[pip 安装程序](../Text/chapter-2.html#p132)'
- en: '[Phi-3 model](../Text/appendix-b.html#p93)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[Phi-3 模型](../Text/appendix-b.html#p93)'
- en: '[print_gradients function](../Text/chapter-4.html#p131), [2nd](../Text/chapter-4.html#p135)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[print_gradients 函数](../Text/chapter-4.html#p131)，[第2次](../Text/chapter-4.html#p135)'
- en: '[plot_values function](../Text/chapter-6.html#p192)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[plot_values 函数](../Text/chapter-6.html#p192)'
- en: parameters
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '[calculating](../Text/appendix-c.html#p34)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算](../Text/appendix-c.html#p34)'
- en: '[perplexity](../Text/chapter-5.html#p96)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[困惑度](../Text/chapter-5.html#p96)'
- en: '[partial derivatives](../Text/appendix-a.html#p127)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[偏导数](../Text/appendix-a.html#p127)'
- en: '[print statement](../Text/chapter-2.html#p59)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[打印语句](../Text/chapter-2.html#p59)'
- en: '[plot_losses function](../Text/chapter-7.html#p166)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[plot_losses 函数](../Text/chapter-7.html#p166)'
- en: '[Python version](../Text/appendix-a.html#p26)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[Python 版本](../Text/appendix-a.html#p26)'
- en: '[Prometheus model](../Text/appendix-b.html#p96)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prometheus 模型](../Text/appendix-b.html#p96)'
- en: '[prompt styles](../Text/chapter-7.html#p30)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示样式](../Text/chapter-7.html#p30)'
- en: '[pretraining](../Text/chapter-1.html#p39)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[预训练](../Text/chapter-1.html#p39)'
- en: '[calculating training and validation set losses](../Text/chapter-5.html#p102)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算训练集和验证集损失](../Text/chapter-5.html#p102)'
- en: '[on unlabeled data](../Text/chapter-5.html#p1)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[在未标记数据上](../Text/chapter-5.html#p1)'
- en: '[training LLMs](../Text/chapter-5.html#p143), [2nd](../Text/chapter-5.html#p170)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[训练 LLMs](../Text/chapter-5.html#p143)，[第2次](../Text/chapter-5.html#p170)'
- en: '[print_sampled_tokens function](../Text/chapter-5.html#p205), [2nd](../Text/appendix-c.html#p51)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[print_sampled_tokens 函数](../Text/chapter-5.html#p205)，[第2次](../Text/appendix-c.html#p51)'
- en: '[pos_embeddings](../Text/chapter-2.html#p254), [2nd](../Text/chapter-2.html#p257)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[pos_embeddings](../Text/chapter-2.html#p254)，[第2次](../Text/chapter-2.html#p257)'
- en: '[preference fine-tuning](../Text/appendix-b.html#p101)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[偏好微调](../Text/appendix-b.html#p101)'
- en: Q
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Q
- en: '[qkv_bias](../Text/chapter-4.html#p28)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[qkv_bias](../Text/chapter-4.html#p28)'
- en: '[query_llama function](../Text/chapter-7.html#p235)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[query_llama 函数](../Text/chapter-7.html#p235)'
- en: '[query_model function](../Text/chapter-7.html#p239)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[query_model 函数](../Text/chapter-7.html#p239)'
- en: R
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: R
- en: '[responses, extracting and saving](../Text/chapter-7.html#p175), [2nd](../Text/chapter-7.html#p202)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[提取和保存响应](../Text/chapter-7.html#p175)，[第2次](../Text/chapter-7.html#p202)'
- en: '[re library](../Text/chapter-2.html#p36)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[re 库](../Text/chapter-2.html#p36)'
- en: '[RMSNorm](../Text/appendix-b.html#p40)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '[RMSNorm](../Text/appendix-b.html#p40)'
- en: '[ReLU (rectified linear unit)](../Text/chapter-4.html#p58), [2nd](../Text/chapter-4.html#p93)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[ReLU (修正线性单元)](../Text/chapter-4.html#p58)，[第2次](../Text/chapter-4.html#p93)'
- en: '[re.split command](../Text/chapter-2.html#p37)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '[re.split 命令](../Text/chapter-2.html#p37)'
- en: '[replace_linear_with_lora function](../Text/appendix-e.html#p74)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[replace_linear_with_lora 函数](../Text/appendix-e.html#p74)'
- en: '[raw text](../Text/chapter-1.html#p37)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始文本](../Text/chapter-1.html#p37)'
- en: '[retrieval-augmented generation](../Text/chapter-2.html#p17)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[检索增强生成](../Text/chapter-2.html#p17)'
- en: '[RNNs (recurrent neural networks)](../Text/chapter-3.html#p16)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[RNNs (循环神经网络)](../Text/chapter-3.html#p16)'
- en: '[random_split function](../Text/chapter-6.html#p43)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[random_split 函数](../Text/chapter-6.html#p43)'
- en: S
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: S
- en: '[shortcut connections](../Text/chapter-4.html#p118), [2nd](../Text/chapter-4.html#p142)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '[捷径连接](../Text/chapter-4.html#p118)，[第2次](../Text/chapter-4.html#p142)'
- en: '[saving and loading models](../Text/appendix-a.html#p274)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[保存和加载模型](../Text/appendix-a.html#p274)'
- en: '[SimpleTokenizerV1 class](../Text/chapter-2.html#p78), [2nd](../Text/chapter-2.html#p80)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[SimpleTokenizerV1 类](../Text/chapter-2.html#p78)，[第2次](../Text/chapter-2.html#p80)'
- en: '[spawn function](../Text/appendix-a.html#p337)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '[spawn 函数](../Text/appendix-a.html#p337)'
- en: '[Sequential class](../Text/appendix-a.html#p152)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sequential 类](../Text/appendix-a.html#p152)'
- en: '[SelfAttention_v2 class](../Text/chapter-3.html#p183), [2nd](../Text/chapter-3.html#p189),
    [3rd](../Text/chapter-3.html#p190)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '[SelfAttention_v2 类](../Text/chapter-3.html#p183)，[第2次](../Text/chapter-3.html#p189)，[第3次](../Text/chapter-3.html#p190)'
- en: '[softmax_naive function](../Text/chapter-3.html#p68), [2nd](../Text/chapter-3.html#p70)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[softmax_naive 函数](../Text/chapter-3.html#p68)，[第2次](../Text/chapter-3.html#p70)'
- en: '[sci_mode parameter](../Text/chapter-4.html#p72)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[sci_mode 参数](../Text/chapter-4.html#p72)'
- en: '[set_printoptions method](../Text/appendix-a.html#p244)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[set_printoptions 方法](../Text/appendix-a.html#p244)'
- en: '[SGD (stochastic gradient descent)](../Text/appendix-a.html#p229)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[SGD (随机梯度下降)](../Text/appendix-a.html#p229)'
- en: '[SelfAttention_v1 class](../Text/chapter-3.html#p171), [2nd](../Text/chapter-3.html#p187)'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[SelfAttention_v1 类](../Text/chapter-3.html#p171)，[第2次](../Text/chapter-3.html#p187)'
- en: '[softmax function](../Text/appendix-a.html#p186), [2nd](../Text/appendix-a.html#p234),
    [3rd](../Text/appendix-a.html#p240)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[softmax 函数](../Text/appendix-a.html#p186)，[第2次](../Text/appendix-a.html#p234)，[第3次](../Text/appendix-a.html#p240)'
- en: '[self.register_buffer() call](../Text/chapter-3.html#p257)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[self.register_buffer() 调用](../Text/chapter-3.html#p257)'
- en: '[state_dict](../Text/chapter-5.html#p246), [2nd](../Text/appendix-a.html#p276)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[state_dict](../Text/chapter-5.html#p246)，[第2次](../Text/appendix-a.html#p276)'
- en: '[SpamDataset class](../Text/chapter-6.html#p58), [2nd](../Text/chapter-6.html#p60),
    [3rd](../Text/chapter-6.html#p65)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[SpamDataset类](../Text/chapter-6.html#p58), [2nd](../Text/chapter-6.html#p60),
    [3rd](../Text/chapter-6.html#p65)'
- en: '[special context tokens](../Text/chapter-2.html#p118)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[特殊上下文标记](../Text/chapter-2.html#p118)'
- en: '[stride setting](../Text/chapter-2.html#p193)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[步长设置](../Text/chapter-2.html#p193)'
- en: '[self.out_proj layer](../Text/chapter-3.html#p308)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '[self.out_proj层](../Text/chapter-3.html#p308)'
- en: '[supervised learning](../Text/appendix-a.html#p19)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[监督学习](../Text/appendix-a.html#p19)'
- en: '[supervised data, fine-tuning model on](../Text/chapter-6.html#p175)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '[在监督数据上微调模型](../Text/chapter-6.html#p175)'
- en: '[strip() function](../Text/chapter-7.html#p145)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[strip()函数](../Text/chapter-7.html#p145)'
- en: supervised instruction fine-tuning
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 监督指令微调
- en: '[preparing dataset for](../Text/chapter-7.html#p17), [2nd](../Text/chapter-7.html#p49)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '[为准备数据集](../Text/chapter-7.html#p17), [2nd](../Text/chapter-7.html#p49)'
- en: '[settings dictionary](../Text/chapter-5.html#p270), [2nd](../Text/chapter-5.html#p276)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[设置字典](../Text/chapter-5.html#p270), [2nd](../Text/chapter-5.html#p276)'
- en: '[self-attention mechanism](../Text/chapter-3.html#p31)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[自注意力机制](../Text/chapter-3.html#p31)'
- en: '[computing attention weights for all input tokens](../Text/chapter-3.html#p79),
    [2nd](../Text/chapter-3.html#p108)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[计算所有输入标记的注意力权重](../Text/chapter-3.html#p79), [2nd](../Text/chapter-3.html#p108)'
- en: '[implementing with trainable weights](../Text/chapter-3.html#p110), [2nd](../Text/chapter-3.html#p192)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用可训练权重实现](../Text/chapter-3.html#p110), [2nd](../Text/chapter-3.html#p192)'
- en: '[without trainable weights](../Text/chapter-3.html#p37), [2nd](../Text/chapter-3.html#p77)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[无训练权重](../Text/chapter-3.html#p37), [2nd](../Text/chapter-3.html#p77)'
- en: '[single-head attention, stacking multiple layers](../Text/chapter-3.html#p269)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[单头注意力，堆叠多层](../Text/chapter-3.html#p269)'
- en: '[SimpleTokenizerV2 class](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p109)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[SimpleTokenizerV2类](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p109)'
- en: T
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: T
- en: '[text generation function, modifying](../Text/chapter-5.html#p225)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[修改文本生成函数](../Text/chapter-5.html#p225)'
- en: '[train_ratio](../Text/chapter-5.html#p118)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_ratio](../Text/chapter-5.html#p118)'
- en: '[text data](../Text/chapter-2.html#p1)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本数据](../Text/chapter-2.html#p1)'
- en: '[adding special context tokens](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p128)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[添加特殊上下文标记](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p128)'
- en: '[creating token embeddings](../Text/chapter-2.html#p208)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[创建标记嵌入](../Text/chapter-2.html#p208)'
- en: '[sliding window](../Text/chapter-2.html#p155), [2nd](../Text/chapter-2.html#p202)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[滑动窗口](../Text/chapter-2.html#p155), [2nd](../Text/chapter-2.html#p202)'
- en: '[tokenization, byte pair encoding](../Text/chapter-2.html#p131), [2nd](../Text/chapter-2.html#p153)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[分词，字节对编码](../Text/chapter-2.html#p131), [2nd](../Text/chapter-2.html#p153)'
- en: '[torch.save function](../Text/chapter-5.html#p240)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[torch.save函数](../Text/chapter-5.html#p240)'
- en: '[token IDs](../Text/chapter-2.html#p64), [2nd](../Text/chapter-2.html#p94)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[标记ID](../Text/chapter-2.html#p64), [2nd](../Text/chapter-2.html#p94)'
- en: '[tensor library](../Text/appendix-a.html#p11)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[张量库](../Text/appendix-a.html#p11)'
- en: '[TransformerBlock class](../Text/chapter-4.html#p149)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[TransformerBlock类](../Text/chapter-4.html#p149)'
- en: '[token_embedding_layer](../Text/chapter-2.html#p241), [2nd](../Text/chapter-2.html#p252)'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[标记嵌入层](../Text/chapter-2.html#p241), [2nd](../Text/chapter-2.html#p252)'
- en: '[token embeddings](../Text/chapter-2.html#p204), [2nd](../Text/chapter-2.html#p229)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[标记嵌入](../Text/chapter-2.html#p204), [2nd](../Text/chapter-2.html#p229)'
- en: '[train_simple_function](../Text/appendix-c.html#p66)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_simple_function](../Text/appendix-c.html#p66)'
- en: '[ToyDataset class](../Text/appendix-a.html#p197), [2nd](../Text/appendix-a.html#p199)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[ToyDataset类](../Text/appendix-a.html#p197), [2nd](../Text/appendix-a.html#p199)'
- en: training function
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 训练函数
- en: '[enhancing](../Text/appendix-d.html#p2)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[增强](../Text/appendix-d.html#p2)'
- en: '[modified](../Text/appendix-d.html#p55), [2nd](../Text/appendix-d.html#p63)'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '[修改](../Text/appendix-d.html#p55), [2nd](../Text/appendix-d.html#p63)'
- en: '[train_data subset](../Text/chapter-5.html#p120)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_data子集](../Text/chapter-5.html#p120)'
- en: '[tril function](../Text/chapter-3.html#p204)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '[tril函数](../Text/chapter-3.html#p204)'
- en: '[tokenizing text](../Text/chapter-2.html#p25), [2nd](../Text/chapter-2.html#p61)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本分词](../Text/chapter-2.html#p25), [2nd](../Text/chapter-2.html#p61)'
- en: training, optimizing performance with GPUs
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU优化性能的训练
- en: '[PyTorch computations on GPU devices](../Text/appendix-a.html#p284)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch在GPU设备上的计算](../Text/appendix-a.html#p284)'
- en: '[selecting available GPUs on multi-GPU machine](../Text/appendix-a.html#p342),
    [2nd](../Text/appendix-a.html#p357)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[在多GPU机器上选择可用的GPU](../Text/appendix-a.html#p342), [2nd](../Text/appendix-a.html#p357)'
- en: '[single-GPU training](../Text/appendix-a.html#p304)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[单GPU训练](../Text/appendix-a.html#p304)'
- en: '[training with multiple GPUs](../Text/appendix-a.html#p320)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用多个GPU进行训练](../Text/appendix-a.html#p320)'
- en: '[test_loader](../Text/appendix-a.html#p208)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '[test_loader](../Text/appendix-a.html#p208)'
- en: '[train_loader](../Text/appendix-a.html#p212)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_loader](../Text/appendix-a.html#p212)'
- en: '[torch.sum method](../Text/appendix-a.html#p257)'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[torch.sum方法](../Text/appendix-a.html#p257)'
- en: '[training loops](../Text/appendix-a.html#p225), [2nd](../Text/appendix-a.html#p271)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[训练循环](../Text/appendix-a.html#p225)，[第2次](../Text/appendix-a.html#p271)'
- en: '[cosine decay](../Text/appendix-d.html#p24)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[余弦衰减](../Text/appendix-d.html#p24)'
- en: '[gradient clipping](../Text/appendix-d.html#p33)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[梯度裁剪](../Text/appendix-d.html#p33)'
- en: '[learning rate warmup](../Text/appendix-d.html#p10)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[学习率预热](../Text/appendix-d.html#p10)'
- en: '[train_classifier_simple function](../Text/chapter-6.html#p181), [2nd](../Text/chapter-6.html#p194)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_classifier_simple函数](../Text/chapter-6.html#p181)，[第2次](../Text/chapter-6.html#p194)'
- en: '[training batches, organizing data into](../Text/chapter-7.html#p51), [2nd](../Text/chapter-7.html#p108)'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '[训练批次，组织数据](../Text/chapter-7.html#p51)，[第2次](../Text/chapter-7.html#p108)'
- en: '[text generation](../Text/chapter-4.html#p204)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本生成](../Text/chapter-4.html#p204)'
- en: '[using GPT to generate text](../Text/chapter-5.html#p14)'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用GPT生成文本](../Text/chapter-5.html#p14)'
- en: '[top-k sampling](../Text/chapter-5.html#p207), [2nd](../Text/chapter-5.html#p208)'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '[top-k采样](../Text/chapter-5.html#p207)，[第2次](../Text/chapter-5.html#p208)'
- en: '[text_data](../Text/appendix-d.html#p7)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本数据](../Text/appendix-d.html#p7)'
- en: '[transformer architecture](../Text/chapter-1.html#p15), [2nd](../Text/chapter-1.html#p44),
    [3rd](../Text/chapter-1.html#p55), [4th](../Text/chapter-3.html#p26)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformer架构](../Text/chapter-1.html#p15)，[第2次](../Text/chapter-1.html#p44)，[第3次](../Text/chapter-1.html#p55)，[第4次](../Text/chapter-3.html#p26)'
- en: '[temperature scaling](../Text/chapter-5.html#p181), [2nd](../Text/chapter-5.html#p196)'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '[温度缩放](../Text/chapter-5.html#p181)，[第2次](../Text/chapter-5.html#p196)'
- en: '[train_model_simple function](../Text/chapter-5.html#p147), [2nd](../Text/chapter-5.html#p149),
    [3rd](../Text/chapter-5.html#p157), [4th](../Text/chapter-5.html#p245), [5th](../Text/chapter-5.html#p251),
    [6th](../Text/chapter-6.html#p177)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[train_model_simple函数](../Text/chapter-5.html#p147)，[第2次](../Text/chapter-5.html#p149)，[第3次](../Text/chapter-5.html#p157)，[第4次](../Text/chapter-5.html#p245)，[第5次](../Text/chapter-5.html#p251)，[第6次](../Text/chapter-6.html#p177)'
- en: '[tensor2d](../Text/appendix-a.html#p67)'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[tensor2d](../Text/appendix-a.html#p67)'
- en: '[tensor3d](../Text/appendix-a.html#p67)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[tensor3d](../Text/appendix-a.html#p67)'
- en: '[torch.no_grad() context manager](../Text/appendix-a.html#p182)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '[torch.no_grad()上下文管理器](../Text/appendix-a.html#p182)'
- en: '[test_set dictionary](../Text/chapter-7.html#p191), [2nd](../Text/chapter-7.html#p196)'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '[测试集字典](../Text/chapter-7.html#p191)，[第2次](../Text/chapter-7.html#p196)'
- en: '[tensors](../Text/appendix-a.html#p59)'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[张量](../Text/appendix-a.html#p59)'
- en: '[common tensor operations](../Text/appendix-a.html#p84)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[常见张量操作](../Text/appendix-a.html#p84)'
- en: '[tensor data types](../Text/appendix-a.html#p69)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[张量数据类型](../Text/appendix-a.html#p69)'
- en: '[torch.nn.Linear layers](../Text/appendix-a.html#p158)'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[torch.nn.Linear层](../Text/appendix-a.html#p158)'
- en: '[transformer blocks](../Text/chapter-4.html#p13), [2nd](../Text/chapter-6.html#p106)'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformer块](../Text/chapter-4.html#p13)，[第2次](../Text/chapter-6.html#p106)'
- en: '[connecting attention and linear layers in](../Text/chapter-4.html#p144), [2nd](../Text/chapter-4.html#p159)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[在连接注意力层和线性层中](../Text/chapter-4.html#p144)，[第2次](../Text/chapter-4.html#p159)'
- en: '[text generation loss](../Text/chapter-5.html#p29)'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本生成损失](../Text/chapter-5.html#p29)'
- en: '[torchvision library](../Text/appendix-a.html#p31)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '[torchvision库](../Text/appendix-a.html#p31)'
- en: U
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: U
- en: '[unbiased parameter](../Text/chapter-4.html#p80)'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '[无偏参数](../Text/chapter-4.html#p80)'
- en: '[unlabeled data, decoding strategies to control randomness](../Text/chapter-5.html#p172)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[未标记数据，解码策略以控制随机性](../Text/chapter-5.html#p172)'
- en: V
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: V
- en: '[variable-length inputs](../Text/chapter-5.html#p117)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '[变长输入](../Text/chapter-5.html#p117)'
- en: '[vocab_size](../Text/chapter-4.html#p22)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '[vocab_size](../Text/chapter-4.html#p22)'
- en: '[v vector](../Text/appendix-d.html#p35)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[v向量](../Text/appendix-d.html#p35)'
- en: '[vectors](../Text/appendix-a.html#p66), [2nd](../Text/appendix-a.html#p114)'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '[向量](../Text/appendix-a.html#p66)，[第2次](../Text/appendix-a.html#p114)'
- en: W
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: W
- en: '[W<.Subscript>q</> matrix](../Text/chapter-3.html#p292)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[W<.Subscript>q</>矩阵](../Text/chapter-3.html#p292)'
- en: '[weight parameters](../Text/chapter-3.html#p131), [2nd](../Text/chapter-5.html#p9)'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[权重参数](../Text/chapter-3.html#p131)，[第2次](../Text/chapter-5.html#p9)'
- en: '[word embeddings](../Text/chapter-2.html#p13), [2nd](../Text/chapter-2.html#p23)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[单词嵌入](../Text/chapter-2.html#p13)，[第2次](../Text/chapter-2.html#p23)'
- en: '[weight_decay parameter](../Text/chapter-6.html#p200)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[权重衰减参数](../Text/chapter-6.html#p200)'
- en: '[Word2Vec](../Text/chapter-2.html#p18)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '[Word2Vec](../Text/chapter-2.html#p18)'
- en: weights
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 权重
- en: '[initializing model with pretrained weights](../Text/chapter-6.html#p84)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用预训练权重初始化模型](../Text/chapter-6.html#p84)'
- en: '[loading pretrained weights from OpenAI](../Text/chapter-5.html#p253), [2nd](../Text/chapter-5.html#p302)'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '[从OpenAI加载预训练权重](../Text/chapter-5.html#p253)，[第2次](../Text/chapter-5.html#p302)'
- en: '[word positions, encoding](../Text/chapter-2.html#p231)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[单词位置，编码](../Text/chapter-2.html#p231)'
- en: '[weight splits](../Text/chapter-3.html#p285)'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[权重拆分](../Text/chapter-3.html#p285)'
- en: X
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: X
- en: '[X training example](../Text/appendix-a.html#p178)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '[X训练示例](../Text/appendix-a.html#p178)'
