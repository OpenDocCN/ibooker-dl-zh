- en: 'Chapter 4\. The “Hello World” of TinyML: Building and Training a Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。TinyML的“Hello World”：构建和训练模型
- en: In [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), we learned the basic concepts
    of machine learning and the general workflow that machine learning projects follow.
    In this chapter and the next, we’ll start putting our knowledge into practice.
    We’re going to build and train a model from scratch and then integrate it into
    a simple microcontroller program.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml#chapter_get_up_to_speed)中，我们学习了机器学习的基本概念以及机器学习项目遵循的一般工作流程。在本章和下一章中，我们将开始将我们的知识付诸实践。我们将从头开始构建和训练一个模型，然后将其集成到一个简单的微控制器程序中。
- en: In the process, you’ll get your hands dirty with some powerful developer tools
    that are used every day by cutting-edge machine learning practitioners. You’ll
    also learn how to integrate a machine learning model into a C++ program and deploy
    it to a microcontroller to control current flowing in a circuit. This might be
    your first taste of mixing hardware and ML, and it should be fun!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，您将通过一些强大的开发者工具亲自动手，这些工具每天都被尖端机器学习从业者使用。您还将学习如何将机器学习模型集成到C++程序中，并将其部署到微控制器以控制电路中的电流。这可能是您第一次尝试混合硬件和机器学习，应该很有趣！
- en: 'You can test the code that we write in these chapters on your Mac, Linux, or
    Windows machine, but for the full experience, you’ll need one of the embedded
    devices mentioned in [“What Hardware Do You Need?”](ch02.xhtml#getting_started_hardware_requirements):'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Mac、Linux或Windows机器上测试我们在这些章节中编写的代码，但要获得完整的体验，您需要其中一个嵌入式设备，如[“需要哪些硬件？”](ch02.xhtml#getting_started_hardware_requirements)中提到的。
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SparkFun Edge](https://oreil.ly/-hoL-)'
- en: '[ST Microelectronics STM32F746G Discovery kit](https://oreil.ly/cvm4J)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ST Microelectronics STM32F746G Discovery kit](https://oreil.ly/cvm4J)'
- en: To create our machine learning model, we’ll use Python, TensorFlow, and Google’s
    Colaboratory, which is a cloud-based interactive notebook for experimenting with
    Python code. These are some of the most important tools for real-world machine
    learning engineers, and they’re all free to use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们的机器学习模型，我们将使用Python、TensorFlow和Google的Colaboratory，这是一个基于云的交互式笔记本，用于尝试Python代码。这些是真实世界中机器学习工程师最重要的工具之一，而且它们都是免费使用的。
- en: Note
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Wondering about the title of this chapter? It’s a tradition in programming that
    new technologies are introduced with example code that demonstrates how to do
    something very simple. Often, the simple task is to make a program output the
    words, [“Hello, world.”](https://oreil.ly/zK06G) There’s no clear equivalent in
    ML, but we’re using the term “hello world” to refer to a simple, easy-to-read
    example of an end-to-end TinyML application.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想知道本章标题的含义吗？在编程中，引入新技术通常会附带演示如何做一些非常简单的事情的示例代码。通常，这个简单的任务是使程序输出“Hello, world.”这些词。在机器学习中没有明确的等价物，但我们使用术语“hello
    world”来指代一个简单、易于阅读的端到端TinyML应用程序的示例。
- en: 'Over the course of this chapter, we will do the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们将执行以下操作：
- en: Obtain a simple dataset.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个简单的数据集。
- en: Train a deep learning model.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个深度学习模型。
- en: Evaluate the model’s performance.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的性能。
- en: Convert the model to run on-device.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型转换为在设备上运行。
- en: Write code to perform on-device inference.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码执行设备推断。
- en: Build the code into a binary.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码构建成二进制文件。
- en: Deploy the binary to a microcontroller.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二进制部署到微控制器。
- en: All the code that we will use is available in [TensorFlow’s GitHub repository](https://oreil.ly/TQ4CC).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的所有代码都可以在[TensorFlow的GitHub存储库](https://oreil.ly/TQ4CC)中找到。
- en: We recommend that you walk through each part of this chapter and then try running
    the code. There are instructions on how to do this along the way. But before we
    start, let’s discuss exactly what we’re going to build.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您逐步阅读本章的每个部分，然后尝试运行代码。沿途会有如何操作的说明。但在我们开始之前，让我们讨论一下我们要构建的内容。
- en: What We’re Building
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们正在构建什么
- en: In [Chapter 3](ch03.xhtml#chapter_get_up_to_speed), we discussed how deep learning
    networks learn to model patterns in their training data so they can make predictions.
    We’re now going to train a network to model some very simple data. You’ve probably
    heard of the [sine](https://oreil.ly/jxAmF) function. It’s used in trigonometry
    to help describe the properties of right-angled triangles. The data we’ll be training
    with is a [sine wave](https://oreil.ly/XDvJu), which is the graph obtained by
    plotting the result of the sine function over time (see [Figure 4-1](#sine_wave)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml#chapter_get_up_to_speed)中，我们讨论了深度学习网络如何学习模拟其训练数据中的模式，以便进行预测。现在我们将训练一个网络来模拟一些非常简单的数据。您可能听说过正弦函数。它在三角学中用于帮助描述直角三角形的性质。我们将使用的数据是正弦波，这是通过绘制随时间变化的正弦函数的结果得到的图形（请参见[图4-1](#sine_wave)）。
- en: Our goal is to train a model that can take a value, `x`, and predict its sine,
    `y`. In a real-world application, if you needed the sine of `x`, you could just
    calculate it directly. However, by training a model to approximate the result,
    we can demonstrate the basics of machine learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个模型，可以接受一个值`x`，并预测其正弦值`y`。在实际应用中，如果您需要`x`的正弦值，您可以直接计算它。然而，通过训练一个模型来近似结果，我们可以演示机器学习的基础知识。
- en: The second part of our project will be to run this model on a hardware device.
    Visually, the sine wave is a pleasant curve that runs smoothly from –1 to 1 and
    back. This makes it perfect for controlling a visually pleasing light show! We’ll
    be using the output of our model to control the timing of either some flashing
    LEDs or a graphical animation, depending on the capabilities of the device.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的第二部分将是在硬件设备上运行这个模型。从视觉上看，正弦波是一个愉悦的曲线，从-1平稳地运行到1，然后返回。这使得它非常适合控制一个视觉上令人愉悦的灯光秀！我们将使用我们模型的输出来控制一些闪烁的LED或图形动画的时间，具体取决于设备的功能。
- en: '![Graph of a sine function over time](Images/timl_0401.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![随时间变化的正弦函数图](Images/timl_0401.png)'
- en: Figure 4-1\. A sine wave
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. 正弦波
- en: Online, you can see an [animated GIF](https://oreil.ly/XhqG9) of this code flashing
    the LEDs of a SparkFun Edge. [Figure 4-2](#sparkfun_edge_hello_world) is a still
    from this animation, showing a couple of the device’s LEDs lit. This may not be
    a particularly useful application of machine learning, but in the spirit of a
    “hello world” example, it’s simple, fun, and will help demonstrate the basic principles
    you need to know.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在线，您可以看到这段代码闪烁SparkFun Edge的LED的[动画GIF](https://oreil.ly/XhqG9)。[图4-2](#sparkfun_edge_hello_world)是来自此动画的静止图像，显示了设备的几个LED灯亮起。这可能不是机器学习的特别有用的应用，但在“hello
    world”示例的精神中，它简单，有趣，并将有助于演示您需要了解的基本原则。
- en: 'After we get our basic code working, we’ll be deploying it to three different
    devices: the SparkFun Edge, an Arduino Nano 33 BLE Sense, and an ST Microelectronics
    STM32F746G Discovery kit.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基本代码运行后，我们将部署到三种不同的设备：SparkFun Edge，Arduino Nano 33 BLE Sense和ST Microelectronics
    STM32F746G Discovery套件。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since TensorFlow is an actively developed open source project that is continually
    evolving, you might notice some slight differences between the code printed here
    and the code hosted online. Don’t worry—even if a few lines of code change, the
    basic principles remain the same.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow是一个不断发展的积极开发的开源项目，您可能会注意到此处打印的代码与在线托管的代码之间存在一些细微差异。不用担心，即使有几行代码发生变化，基本原则仍然保持不变。
- en: '![A still from a video showing the SparkFun Edge with two LEDs lit](Images/timl_0402.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![显示SparkFun Edge带有两个LED灯亮起的视频静止图像](Images/timl_0402.png)'
- en: Figure 4-2\. The code running on a SparkFun Edge
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。在SparkFun Edge上运行的代码
- en: Our Machine Learning Toolchain
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的机器学习工具链
- en: To build the machine learning parts of this project, we’re using the same tools
    used by real-world machine learning practitioners. This section introduces them
    to you.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建这个项目的机器学习部分，我们正在使用真实世界机器学习从业者使用的相同工具。本节向您介绍这些工具。
- en: Python and Jupyter Notebooks
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python和Jupyter笔记本
- en: Python is the favorite programming language of machine learning scientists and
    engineers. It’s easy to learn, works well for many different applications, and
    has a ton of libraries for useful tasks involving data and mathematics. The vast
    majority of deep learning research is done using Python, and researchers often
    release the Python source code for the models they create.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Python是机器学习科学家和工程师最喜欢的编程语言。它易于学习，适用于许多不同的应用程序，并且有大量用于涉及数据和数学的有用任务的库。绝大多数深度学习研究都是使用Python进行的，研究人员经常发布他们创建的模型的Python源代码。
- en: Python is especially great when combined with something called [*Jupyter Notebooks*](https://jupyter.org/).
    This is a special document format that allows you to mix writing, graphics, and
    code that can be run at the click of a button. Jupyter notebooks are widely used
    as a way to describe, explain, and explore machine learning code and problems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Python与一种称为[*Jupyter笔记本*](https://jupyter.org/)结合使用时特别好。这是一种特殊的文档格式，允许您混合编写、图形和代码，可以在点击按钮时运行。Jupyter笔记本被广泛用作描述、解释和探索机器学习代码和问题的一种方式。
- en: We’ll be creating our model inside of a Jupyter notebook, which permits us to
    do awesome things to visualize our data during development. This includes displaying
    graphs that show our model’s accuracy and convergence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Jupyter笔记本中创建我们的模型，这使我们能够在开发过程中对我们的数据进行可视化。这包括显示显示我们模型准确性和收敛性的图形。
- en: If you have some programming experience, Python is easy to read and learn. You
    should be able to follow this tutorial without any trouble.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一些编程经验，Python易于阅读和学习。您应该能够在没有任何困难的情况下跟随本教程。
- en: Google Colaboratory
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌Colaboratory
- en: To run our notebook we’ll use a tool called [Colaboratory](https://oreil.ly/ZV7NK),
    or *Colab* for short. Colab is made by Google, and it provides an online environment
    for running Jupyter notebooks. It’s provided for free as a tool to encourage research
    and development in machine learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的笔记本，我们将使用一个名为[Colaboratory](https://oreil.ly/ZV7NK)的工具，简称为*Colab*。Colab由谷歌制作，它提供了一个在线环境来运行Jupyter笔记本。它作为一个免费工具提供，以鼓励机器学习中的研究和开发。
- en: Traditionally, you needed to create a notebook on your own computer. This required
    installing a lot of dependencies, such as Python libraries, which can be a headache.
    It was also difficult to share the resulting notebook with other people, since
    they might have different versions of the dependencies, meaning the notebook might
    not run as expected. In addition, machine learning can be computationally intensive,
    so training models might be slow on your development computer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，您需要在自己的计算机上创建一个笔记本。这需要安装许多依赖项，如Python库，这可能会让人头疼。与其他人分享结果笔记本也很困难，因为他们可能有不同版本的依赖项，这意味着笔记本可能无法按预期运行。此外，机器学习可能需要大量计算，因此在开发计算机上训练模型可能会很慢。
- en: Colab allows you to run notebooks on Google’s powerful hardware, at zero cost.
    You can edit and view your notebooks from any web browser, and you can share them
    with other people, who are guaranteed to get the same results when they run them.
    You can even configure Colab to run your code on specially accelerated hardware
    that can perform training more quickly than a normal computer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Colab允许您在谷歌强大的硬件上免费运行笔记本。您可以从任何网络浏览器编辑和查看您的笔记本，并与其他人分享，他们在运行时保证获得相同的结果。您甚至可以配置Colab在专门加速的硬件上运行您的代码，这样可以比普通计算机更快地进行训练。
- en: TensorFlow and Keras
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow和Keras
- en: '[TensorFlow](https://tensorflow.org) is a set of tools for building, training,
    evaluating, and deploying machine learning models. Originally developed at Google,
    TensorFlow is now an open source project built and maintained by thousands of
    contributors across the world. It is the most popular and widely used framework
    for machine learning. Most developers interact with TensorFlow via its Python
    library.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow](https://tensorflow.org)是一套用于构建、训练、评估和部署机器学习模型的工具。最初由谷歌开发，TensorFlow现在是一个由全球数千名贡献者构建和维护的开源项目。它是最受欢迎和广泛使用的机器学习框架。大多数开发人员通过其Python库与TensorFlow进行交互。'
- en: TensorFlow does many different things. In this chapter we’ll use [Keras](https://oreil.ly/JgNtS),
    TensorFlow’s high-level API that makes it easy to build and train deep learning
    networks. We’ll also use [TensorFlow Lite](https://oreil.ly/LbDBK), a set of tools
    for deploying TensorFlow models to mobile and embedded devices, to run our model
    on-device.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow可以做很多不同的事情。在本章中，我们将使用[Keras](https://oreil.ly/JgNtS)，这是TensorFlow的高级API，使构建和训练深度学习网络变得容易。我们还将使用[TensorFlow
    Lite](https://oreil.ly/LbDBK)，这是一组用于在移动和嵌入式设备上部署TensorFlow模型的工具，以在设备上运行我们的模型。
- en: '[Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers) will
    cover TensorFlow in much more detail. For now, just know that it is an extremely
    powerful and industry-standard tool that will continue to serve your needs as
    you go from beginner to deep learning expert.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)将更详细地介绍TensorFlow。现在，只需知道它是一个非常强大和行业标准的工具，将在您从初学者到深度学习专家的过程中继续满足您的需求。'
- en: Building Our Model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的模型
- en: We’re now going to walk through the process of building, training, and converting
    our model. We include all of the code in this chapter, but you can also follow
    along in Colab and run the code as you go.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将逐步介绍构建、训练和转换模型的过程。我们在本章中包含了所有的代码，但您也可以在Colab中跟着进行并运行代码。
- en: First, [load the notebook](https://oreil.ly/NN6Mj). After the page loads, at
    the top, click the “Run in Google Colab” button, as shown in [Figure 4-3](#run_in_google_colab).
    This copies the notebook from GitHub into Colab, allowing you to run it and make
    edits.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，[加载笔记本](https://oreil.ly/NN6Mj)。页面加载后，在顶部，单击“在Google Colab中运行”按钮，如[图4-3](#run_in_google_colab)所示。这将把笔记本从GitHub复制到Colab，允许您运行它并进行编辑。
- en: '![The ''Run in Google Colab'' button](Images/timl_0403.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![“在Google Colab中运行”按钮](Images/timl_0403.png)'
- en: Figure 4-3\. The “Run in Google Colab” button
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. “在Google Colab中运行”按钮
- en: By default, in addition to the code, the notebook contains a sample of the output
    you should expect to see when the code is run. Since we’ll be running through
    the code in this chapter, let’s clear this output so the notebook is in a pristine
    state. To do this, in Colab’s menu, click Edit and then select “Clear all outputs,”
    as shown in [Figure 4-4](#clear_all_outputs).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，除了代码外，笔记本还包含您在运行代码时应该看到的输出样本。由于我们将在本章中运行代码，让我们清除这些输出，使笔记本处于原始状态。要做到这一点，在Colab的菜单中，单击“编辑”，然后选择“清除所有输出”，如[图4-4](#clear_all_outputs)所示。
- en: '![The ''Clear all outputs'' option](Images/timl_0404.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![“清除所有输出”选项](Images/timl_0404.png)'
- en: Figure 4-4\. The “Clear all outputs” option
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. “清除所有输出”选项
- en: Nice work. Our notebook is now ready to go!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好。我们的笔记本现在已经准备好了！
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re already familiar with machine learning, TensorFlow, and Keras, you
    might want to skip ahead to the part where we convert our model to use with TensorFlow
    Lite. In the book, jump to [“Converting the Model for TensorFlow Lite”](#converting_the_model).
    In Colab, scroll down to the heading “Convert to TensorFlow Lite.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟悉机器学习、TensorFlow和Keras，您可能想直接跳到我们将模型转换为TensorFlow Lite使用的部分。在书中，跳到[“将模型转换为TensorFlow
    Lite”](#converting_the_model)。在Colab中，滚动到“转换为TensorFlow Lite”标题下。
- en: Importing Dependencies
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入依赖项
- en: Our first task is to import the dependencies we need. In Jupyter notebooks,
    code and text are arranged in *cells*. There are *code* cells, which contain executable
    Python code, and *text* cells, which contain formatted text.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个任务是导入我们需要的依赖项。在Jupyter笔记本中，代码和文本被安排在*单元格*中。有*代码*单元格，其中包含可执行的Python代码，以及*文本*单元格，其中包含格式化的文本。
- en: 'Our first code cell is located under “Import dependencies.” It sets up all
    of the libraries that we need to train and convert our model. Here’s the code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个代码单元格位于“导入依赖项”下面。它设置了我们需要训练和转换模型的所有库。以下是代码：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In Python, the `import` statement loads a library so that it can be used from
    our code. You can see from the code and comments that this cell does the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，`import`语句加载一个库，以便我们的代码可以使用它。您可以从代码和注释中看到，这个单元格执行以下操作：
- en: Installs the TensorFlow 2.0 library using `pip`, a package manager for Python
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pip`安装TensorFlow 2.0库，`pip`是Python的软件包管理器
- en: Imports TensorFlow, NumPy, Matplotlib, and Python’s `math` library
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入TensorFlow、NumPy、Matplotlib和Python的`math`库
- en: When we import a library, we can give it an alias so that it’s easy to refer
    to later. For example, in the preceding code, we use `import numpy as np` to import
    NumPy and give it the alias `np`. When we use it in our code, we can refer to
    it as `np`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们导入一个库时，我们可以给它一个别名，以便以后容易引用。例如，在前面的代码中，我们使用`import numpy as np`导入NumPy，并给它别名`np`。当我们在代码中使用它时，可以将其称为`np`。
- en: The code in code cells can be run by clicking the button that appears at the
    upper left when the cell is selected. In the “Import dependencies” section, click
    anywhere in the first code cell so that it becomes selected. [Figure 4-5](#import_dependencies)
    shows what a selected cell looks like.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代码单元格中的代码可以通过单击出现在左上角的按钮来运行，当单元格被选中时会出现该按钮。在“导入依赖项”部分，单击第一个代码单元格的任何位置，使其被选中。[图4-5](#import_dependencies)显示了选定单元格的外观。
- en: '![The ''Import dependencies'' cell in its selected state](Images/timl_0405.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![“导入依赖项”单元格处于选定状态](Images/timl_0405.png)'
- en: Figure 4-5\. The “Import dependencies” cell in its selected state
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. “导入依赖项”单元格处于选定状态
- en: To run the code, click the button that appears in the upper left. As the code
    is being run, the button will animate with a circle as depicted in [Figure 4-6](#import_dependencies_running).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行代码，请单击左上角出现的按钮。当代码正在运行时，按钮将以圆圈的形式显示动画，如[图4-6](#import_dependencies_running)所示。
- en: 'The dependencies will begin to be installed, and you’ll see some output appearing.
    You should eventually see the following line, meaning that the library was installed
    successfully:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项将开始安装，并会看到一些输出。最终您应该看到以下行，表示库已成功安装：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![The ''Import dependencies'' cell in its running state](Images/timl_0406.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![“导入依赖项”单元格处于运行状态](Images/timl_0406.png)'
- en: Figure 4-6\. The “Import dependencies” cell in its running state
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. “导入依赖项”单元格处于运行状态
- en: After a cell has been run in Colab, you’ll see that a `1` is now displayed in
    the upper-left corner when it is no longer selected, as illustrated in [Figure 4-7](#import_dependencies_counter).
    This number is a counter that is incremented each time the cell is run.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中运行一个单元格后，当它不再被选中时，您会看到左上角显示一个`1`，如[图4-7](#import_dependencies_counter)所示。这个数字是一个计数器，每次运行单元格时都会递增。
- en: '![The cell run counter in the upper-left corner](Images/timl_0407.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![左上角的单元格运行计数器](Images/timl_0407.png)'
- en: Figure 4-7\. The cell run counter in the upper-left corner
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. 左上角的单元格运行计数器
- en: You can use this to understand which cells have been run, and how many times.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这个来了解哪些单元格已经运行过，以及运行了多少次。
- en: Generating Data
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成数据
- en: Deep learning networks learn to model patterns in underlying data. As we mentioned
    earlier, we’re going to train a network to model data generated by a sine function.
    This will result in a model that can take a value, `x`, and predict its sine,
    `y`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络学习对底层数据的模式进行建模。正如我们之前提到的，我们将训练一个网络来模拟由正弦函数生成的数据。这将导致一个模型，可以接受一个值`x`，并预测它的正弦值`y`。
- en: Before we go any further, we need some data. In a real-world situation, we might
    be collecting data from sensors and production logs. For this example, however,
    we’re using some simple code to generate a dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要一些数据。在现实世界的情况下，我们可能会从传感器和生产日志中收集数据。然而，在这个例子中，我们使用一些简单的代码来生成数据集。
- en: The next cell is where this will happen. Our plan is to generate 1,000 values
    that represent random points along a sine wave. Let’s take a look at [Figure 4-8](#sine_wave_2)
    to remind ourselves what a sine wave looks like.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的单元格就是这样的。我们的计划是生成1,000个代表正弦波上随机点的值。让我们看一下[图4-8](#sine_wave_2)来提醒自己正弦波是什么样子的。
- en: Each full cycle of a wave is called its *period*. From the graph, we can see
    that a full cycle is completed approximately every six units on the `x`-axis.
    In fact, the period of a sine wave is 2 × π, or 2π.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 波的每个完整周期称为它的*周期*。从图中，我们可以看到每隔大约六个单位在`x`轴上完成一个完整周期。事实上，正弦波的周期是2 × π，或2π。
- en: So that we have a full sine wave worth of data to train on, our code will generate
    random `x` values from 0 to 2π. It will then calculate the sine for each of these
    values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练完整的正弦波数据，我们的代码将生成从0到2π的随机`x`值。然后将计算每个这些值的正弦值。
- en: '![Graph of a sine function over time](Images/timl_0401.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![随时间变化的正弦函数图表](Images/timl_0401.png)'
- en: Figure 4-8\. A sine wave
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. 一个正弦波
- en: 'Here’s the full code for this cell, which uses NumPy (`np`, which we imported
    earlier) to generate random numbers and calculate their sine:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个单元格的完整代码，它使用NumPy（我们之前导入的`np`）生成随机数并计算它们的正弦值：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In addition to what we discussed earlier, there are a few things worth pointing
    out in this code. First, you’ll see that we use `np.random.uniform()` to generate
    our `x` values. This method returns an array of random numbers in the specified
    range. NumPy contains a lot of useful methods that operate on entire arrays of
    values, which is very convenient when dealing with data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前讨论的内容，这段代码中还有一些值得指出的地方。首先，您会看到我们使用`np.random.uniform()`来生成我们的`x`值。这个方法返回指定范围内的随机数数组。NumPy包含许多有用的方法，可以操作整个值数组，这在处理数据时非常方便。
- en: Second, after generating the data, we shuffle it. This is important because
    the training process used in deep learning depends on data being fed to it in
    a truly random order. If the data were in order, the resulting model would be
    less accurate.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在生成数据后，我们对数据进行了洗牌。这很重要，因为深度学习中使用的训练过程取决于以真正随机的顺序提供数据。如果数据是有序的，那么生成的模型将不够准确。
- en: Next, notice that we use NumPy’s `sin()` method to calculate our sine values.
    NumPy can do this for all of our `x` values at once, returning an array. NumPy
    is great!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，请注意我们使用NumPy的`sin()`方法来计算正弦值。NumPy可以一次为所有`x`值执行此操作，返回一个数组。NumPy太棒了！
- en: 'Finally, you’ll see some mysterious code invoking `plt`, which is our alias
    for Matplotlib:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您会看到一些神秘的代码调用`plt`，这是我们对Matplotlib的别名：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What does this code do? It plots a graph of our data. One of the best things
    about Jupyter notebooks is their ability to display graphics that are output by
    the code you run. Matplotlib is an excellent tool for creating graphs from data.
    Since visualizing data is a crucial part of the machine learning workflow, this
    will be incredibly helpful as we train our model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是做什么的？它绘制了我们数据的图表。Jupyter笔记本的一个最好的地方是它们能够显示代码运行输出的图形。Matplotlib是一个从数据创建图表的优秀工具。由于可视化数据是机器学习工作流程的重要部分，这将在我们训练模型时非常有帮助。
- en: To generate the data and render it as a graph, run the code in the cell. After
    the code cell finishes running, you should see a beautiful graph appear underneath,
    like the one shown in [Figure 4-9](#smooth_data_graph).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成数据并将其呈现为图表，请运行单元格中的代码。代码单元格运行完成后，您应该会看到一个漂亮的图表出现在下面，就像[图4-9](#smooth_data_graph)中显示的那样。
- en: '![A graph of our generated data](Images/timl_0409.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![我们生成数据的图表](Images/timl_0409.png)'
- en: Figure 4-9\. A graph of our generated data
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. 我们生成数据的图表
- en: 'This is our data! It is a selection of random points along a nice, smooth sine
    curve. We could use this to train our model. However, this would be too easy.
    One of the exciting things about deep learning networks is their ability to sift
    patterns from noise. This allows them to make predictions even when trained on
    messy, real-world data. To show this off, let’s add some random noise to our datapoints
    and draw another graph:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的数据！这是沿着一个漂亮、平滑的正弦曲线的随机点的选择。我们可以使用这个来训练我们的模型。然而，这样做太容易了。深度学习网络的一个令人兴奋的地方是它们能够从噪音中提取模式。这使它们能够在训练混乱的真实世界数据时进行预测。为了展示这一点，让我们向我们的数据点添加一些随机噪音并绘制另一个图表：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Run this cell and take a look at the results, as shown in [Figure 4-10](#noisy_data_graph).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个单元格，看看结果，如[图4-10](#noisy_data_graph)所示。
- en: Much better! Our points are now randomized, so they represent a distribution
    around a sine wave instead of a smooth, perfect curve. This is much more reflective
    of a real-world situation, in which data is generally quite messy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更好了！我们的点现在已经随机化，因此它们代表了围绕正弦波的分布，而不是平滑的完美曲线。这更加反映了现实世界的情况，其中数据通常相当混乱。
- en: '![A graph of our data with noise added](Images/timl_0410.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![我们的数据添加了噪声的图](Images/timl_0410.png)'
- en: Figure 4-10\. A graph of our data with noise added
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。我们的数据添加了噪声
- en: Splitting the Data
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割数据
- en: 'From the previous chapter, you might remember that a dataset is often split
    into three parts: *training*, *validation*, and *test*. To evaluate the accuracy
    of the model we train, we need to compare its predictions to real data and check
    how well they match up.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一章，您可能记得数据集通常分为三部分：*训练*、*验证*和*测试*。为了评估我们训练的模型的准确性，我们需要将其预测与真实数据进行比较，并检查它们的匹配程度。
- en: This evaluation happens during training (where it is referred to as validation)
    and after training (referred to as testing). It’s important in each case that
    we use fresh data that was not already used to train the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估发生在训练期间（称为验证）和训练之后（称为测试）。在每种情况下，使用的数据都必须是新鲜的，不能已经用于训练模型。
- en: To ensure that we have data to use for evaluation, we’ll set some aside before
    we begin training. Let’s reserve 20% of our data for validation, and another 20%
    for testing. We’ll use the remaining 60% to train the model. This is a typical
    split used when training models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们有数据用于评估，我们将在开始训练之前留出一些数据。让我们将我们的数据的20%保留用于验证，另外20%用于测试。我们将使用剩下的60%来训练模型。这是训练模型时常用的典型分割。
- en: 'The following code splits our data and then plots each set as a different color:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码分割我们的数据，然后将每个集合绘制为不同的颜色：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To split our data, we use another handy NumPy method: `split()`. This method
    takes an array of data and an array of indices and then chops the data into parts
    at the indices provided.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分割我们的数据，我们使用另一个方便的NumPy方法：`split()`。这个方法接受一个数据数组和一个索引数组，然后在提供的索引处将数据分割成部分。
- en: Run this cell to see the results of our split. Each type of data will be represented
    by a different color (or shade, if you’re reading the print version of this book),
    as demonstrated in [Figure 4-11](#split_data_graph).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此单元格以查看我们分割的结果。每种类型的数据将由不同的颜色表示（或者如果您正在阅读本书的打印版本，则为不同的阴影），如[图4-11](#split_data_graph)所示。
- en: '![A graph of our data split into training, validation, and test sets](Images/timl_0411.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![我们的数据分为训练、验证和测试集的图](Images/timl_0411.png)'
- en: Figure 4-11\. A graph of our data split into training, validation, and test
    sets
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。我们的数据分为训练、验证和测试集
- en: Defining a Basic Model
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义基本模型
- en: Now that we have our data, it’s time to create the model that we’ll train to
    fit it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，是时候创建我们将训练以适应它的模型了。
- en: We’re going to build a model that will take an input value (in this case, `x`)
    and use it to predict a numeric output value (the sine of `x`). This type of problem
    is called a *regression*. We can use regression models for all sorts of tasks
    that require a numeric output. For example, a regression model could attempt to
    predict a person’s running speed in miles per hour based on data from an accelerometer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个模型，该模型将接受一个输入值（在本例中为`x`）并使用它来预测一个数值输出值（`x`的正弦）。这种类型的问题称为*回归*。我们可以使用回归模型来处理各种需要数值输出的任务。例如，回归模型可以尝试根据来自加速度计的数据预测一个人的每小时英里数。
- en: To create our model, we’re going to design a simple neural network. It uses
    layers of neurons to attempt to learn any patterns underlying the training data
    so that it can make predictions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的模型，我们将设计一个简单的神经网络。它使用神经元层来尝试学习训练数据中的任何模式，以便进行预测。
- en: 'The code to do this is actually quite straightforward. It uses [*Keras*](https://oreil.ly/IpFqC),
    TensorFlow’s high-level API for creating deep learning networks:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，执行此操作的代码非常简单。它使用*Keras*，TensorFlow的用于创建深度学习网络的高级API：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, we create a `Sequential` model using Keras, which just means a model
    in which each layer of neurons is stacked on top of the next, as we saw in [Figure 3-1](ch03.xhtml#network_layers).
    We then define two layers. Here’s where the first layer is defined:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用Keras创建一个`Sequential`模型，这意味着每个神经元层都堆叠在下一个层上，就像我们在[图3-1](ch03.xhtml#network_layers)中看到的那样。然后我们定义两个层。这是第一层的定义：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first layer has a single input—our `x` value—and 16 neurons. It’s a `Dense`
    layer (also known as a *fully connected* layer), meaning the input will be fed
    into every single one of its neurons during inference, when we’re making predictions.
    Each neuron will then become *activated* to a certain degree. The amount of activation
    for each neuron is based on both its *weight* and *bias* values, learned during
    training, and its *activation function*. The neuron’s activation is output as
    a number.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层有一个单一的输入—我们的`x`值—和16个神经元。这是一个`Dense`层（也称为*全连接*层），意味着在推断时，当我们进行预测时，输入将被馈送到每一个神经元中。然后每个神经元将以一定程度被*激活*。每个神经元的激活程度基于其在训练期间学习到的*权重*和*偏差*值，以及其*激活函数*。神经元的激活作为一个数字输出。
- en: 'Activation is calculated by a simple formula, shown in Python. We won’t ever
    need to code this ourselves, since it is handled by Keras and TensorFlow, but
    it will be helpful to know as we go further into deep learning:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 激活是通过一个简单的公式计算的，用Python显示。我们永远不需要自己编写这个代码，因为它由Keras和TensorFlow处理，但随着我们深入学习，了解这个公式将会很有帮助：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To calculate the neuron’s activation, its input is multiplied by the weight,
    and the bias is added to the result. The calculated value is passed into the activation
    function. The resulting number is the neuron’s activation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算神经元的激活，它的输入被权重相乘，偏差被加到结果中。计算出的值被传递到激活函数中。得到的数字是神经元的激活。
- en: The activation function is a mathematical function used to shape the output
    of the neuron. In our network, we’re using an activation function called *rectified
    linear unit*, or *ReLU* for short. This is specified in Keras by the argument
    `activation=relu`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是用于塑造神经元输出的数学函数。在我们的网络中，我们使用了一个称为*修正线性单元*或*ReLU*的激活函数。这在Keras中由参数`activation=relu`指定。
- en: 'ReLU is a simple function, shown here in Python:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个简单的函数，在Python中显示如下：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'ReLU returns whichever is the larger value: its input, or zero. If its input
    value is negative, ReLU returns zero. If its input value is above zero, ReLU returns
    it unchanged.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU返回较大的值：它的输入或零。如果输入值为负，则ReLU返回零。如果输入值大于零，则ReLU返回不变。
- en: '[Figure 4-12](#relu) shows the output of ReLU for a range of input values.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-12](#relu)显示了一系列输入值的ReLU输出。'
- en: '![A graph of ReLU for inputs from –10 to 10](Images/timl_0412.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![从-10到10的输入的ReLU图](Images/timl_0412.png)'
- en: Figure 4-12\. A graph of ReLU for inputs from –10 to 10
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。从-10到10的输入的ReLU图
- en: Without an activation function, the neuron’s output would always be a linear
    function of its input. This would mean that the network could model only linear
    relationships in which the ratio between `x` and `y` remains the same across the
    entire range of values. This would prevent a network from modeling our sine wave,
    because a sine wave is nonlinear.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 没有激活函数，神经元的输出将始终是其输入的线性函数。这意味着网络只能模拟`x`和`y`之间的比率在整个值范围内保持不变的线性关系。这将阻止网络对我们的正弦波进行建模，因为正弦波是非线性的。
- en: Since ReLU is nonlinear, it allows multiple layers of neurons to join forces
    and model complex nonlinear relationships, in which the `y` value doesn’t increase
    by the same amount for every increment of `x`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ReLU是非线性的，它允许多层神经元联合起来模拟复杂的非线性关系，其中`y`值并不是每个`x`增量都增加相同的量。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are other activation functions, but ReLU is the most commonly used. You
    can see some of the other options in the [Wikipedia article on activation functions](https://oreil.ly/Yxe-N).
    Each activation function has different trade-offs, and machine learning engineers
    experiment to find which options work best for a given architecture.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他激活函数，但ReLU是最常用的。您可以在[Wikipedia关于激活函数的文章](https://oreil.ly/Yxe-N)中看到其他选项。每个激活函数都有不同的权衡，机器学习工程师会进行实验，找出哪些选项对于给定的架构最有效。
- en: 'The activation numbers from our first layer will be fed as inputs to our second
    layer, which is defined in the following line:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 来自我们第一层的激活数字将作为输入传递给我们的第二层，该层在以下行中定义：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Because this layer is a single neuron, it will receive 16 inputs, one for each
    of the neurons in the previous layer. Its purpose is to combine all of the activations
    from the previous layer into a single output value. Since this is our output layer,
    we don’t specify an activation function—we just want the raw result.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这一层是一个单个神经元，它将接收16个输入，每个输入对应前一层中的一个神经元。它的目的是将前一层的所有激活组合成一个单一的输出值。由于这是我们的输出层，我们不指定激活函数，我们只想要原始结果。
- en: 'Because this neuron has multiple inputs, it has a corresponding weight value
    for each. The neuron’s output is calculated by the following formula, shown in
    Python:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个神经元有多个输入，所以它有对应的每个输入的权重值。神经元的输出是通过以下公式计算的，如Python中所示：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output value is obtained by multiplying each input with its corresponding
    weight, summing the results, and then adding the neuron’s bias.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值是通过将每个输入与其对应的权重相乘，对结果求和，然后加上神经元的偏差来获得的。
- en: 'The network’s weights and biases are learned during training. The `compile()`
    step in the code shown earlier in the chapter configures some important arguments
    used in the training process, and prepares the model to be trained:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的权重和偏差在训练期间学习。在本章前面显示的代码中的`compile()`步骤配置了一些在训练过程中使用的重要参数，并准备好模型进行训练：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `optimizer` argument specifies the algorithm that will adjust the network
    to model its input during training. There are several choices, and finding the
    best one often comes down to experimentation. You can read about the options in
    the [Keras documentation](https://oreil.ly/oT-pU).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimizer`参数指定了在训练期间调整网络以模拟其输入的算法。有几种选择，找到最佳选择通常归结为实验。您可以在[Keras文档](https://oreil.ly/oT-pU)中了解选项。'
- en: The `loss` argument specifies the method used during training to calculate how
    far the network’s predictions are from reality. This method is called a *loss
    function*. Here, we’re using `mse`, or *mean squared error*. This loss function
    is used in the case of regression problems, for which we’re trying to predict
    a number. There are various loss functions available in Keras. You can see some
    of the options listed in the [Keras docs](https://keras.io/losses).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`参数指定了在训练期间使用的方法，用于计算网络预测与现实之间的距离。这种方法称为*损失函数*。在这里，我们使用`mse`，或*均方误差*。这种损失函数用于回归问题，我们试图预测一个数字。Keras中有各种损失函数可用。您可以在[Keras文档](https://keras.io/losses)中看到一些选项。'
- en: The `metrics` argument allows us to specify some additional functions that are
    used to judge the performance of our model. We specify `mae`, or *mean absolute
    error*, which is a helpful function for measuring the performance of a regression
    model. This metric will be measured during training, and we’ll have access to
    the results after training is done.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`metrics`参数允许我们指定一些额外的函数，用于评估我们模型的性能。我们指定`mae`，或*平均绝对误差*，这是一个有用的函数，用于衡量回归模型的性能。这个度量将在训练期间进行测量，我们将在训练结束后获得结果。'
- en: 'After we compile our model, we can use the following line to print some summary
    information about its architecture:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译模型后，我们可以使用以下行打印关于其架构的一些摘要信息：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Run the cell in Colab to define the model. You’ll see the following output
    printed:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中运行单元格以定义模型。您将看到以下输出打印：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This table shows the layers of the network, their output shapes, and their numbers
    of *parameters*. The size of a network⁠—how much memory it takes up—depends mostly
    on its number of parameters, meaning its total number of weights and biases. This
    can be a useful metric when discussing model size and complexity.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格显示了网络的层、它们的输出形状以及它们的*参数*数量。网络的大小——它占用的内存量——主要取决于它的参数数量，即其总权重和偏差的数量。在讨论模型大小和复杂性时，这可能是一个有用的指标。
- en: For simple models like ours, the number of weights can be determined by calculating
    the number of connections between neurons in the model, given that each connection
    has a weight.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们这样简单的模型，权重的数量可以通过计算模型中神经元之间的连接数来确定，假设每个连接都有一个权重。
- en: The network we’ve just designed consists of two layers. Our first layer has
    16 connections—one between its input and each of its neurons. Our second layer
    has a single neuron, which also has 16 connections—one to each neuron in the first
    layer. This makes the total number of connections 32.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚设计的网络由两层组成。我们的第一层有16个连接——一个连接到每个神经元的输入。我们的第二层有一个神经元，也有16个连接——一个连接到第一层的每个神经元。这使得连接的总数为32。
- en: Since every neuron has a bias, the network has 17 biases, meaning it has a total
    of 32 + 17 = 49 parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个神经元都有一个偏差，网络有17个偏差，这意味着它总共有32 + 17 = 49个参数。
- en: We’ve now walked through the code that defines our model. Next, we’ll begin
    the training process.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经走完了定义我们模型的代码。接下来，我们将开始训练过程。
- en: Training Our Model
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的模型
- en: After we define our model, it’s time to train it and then evaluate its performance
    to see how well it works. When we see the metrics, we can decide if it’s good
    enough, or if we should make changes to our design and train it again.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了我们的模型之后，就是训练它，然后评估其性能，看看它的工作效果如何。当我们看到指标时，我们可以决定是否足够好，或者是否应该对设计进行更改并重新训练。
- en: 'To train a model in Keras we just call its `fit()` method, passing all of our
    data and some other important arguments. The code in the next cell shows how:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中训练模型，我们只需调用其`fit()`方法，传递所有数据和一些其他重要参数。下一个单元格中的代码显示了如何：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the code in the cell to begin training. You’ll see some logs start to appear:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格中的代码开始训练。您将看到一些日志开始出现：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our model is now training. This will take a little while, so while we wait
    let’s walk through the details of our call to `fit()`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在正在训练。这将需要一些时间，所以在等待时，让我们详细了解我们对`fit()`的调用：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, you’ll notice that we assign the return value of our `fit()` call to
    a variable named `history_1`. This variable contains a ton of information about
    our training run, and we’ll use it later to investigate how things went.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您会注意到我们将`fit()`调用的返回值分配给一个名为`history_1`的变量。这个变量包含了关于我们训练运行的大量信息，我们稍后将使用它来调查事情的进展。
- en: 'Next, let’s take a look at the `fit()` function’s arguments:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下`fit()`函数的参数：
- en: '`x_train`, `y_train`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_train`，`y_train`'
- en: The first two arguments to `fit()` are the `x` and `y` values of our training
    data. Remember that parts of our data are kept aside for validation and testing,
    so only the training set is used to train the network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`的前两个参数是我们训练数据的`x`和`y`值。请记住，我们的数据的部分被保留用于验证和测试，因此只有训练集用于训练网络。'
- en: '`epochs`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`epochs`'
- en: The next argument specifies how many times our entire training set will be run
    through the network during training. The more epochs, the more training will occur.
    You might think that the more training happens, the better the network will be.
    However, some networks will start to overfit their training data after a certain
    number of epochs, so we might want to limit the amount of training we do.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个参数指定在训练期间整个训练集将通过网络运行多少次。时期越多，训练就越多。您可能会认为训练次数越多，网络就会越好。然而，一些网络在一定数量的时期后会开始过拟合其训练数据，因此我们可能希望限制我们进行的训练量。
- en: In addition, even if there’s no overfitting, a network will stop improving after
    a certain amount of training. Since training costs time and computational resources,
    it’s best not to train if the network isn’t getting better!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使没有过拟合，网络在一定数量的训练后也会停止改进。由于训练需要时间和计算资源，最好不要在网络没有变得更好的情况下进行训练！
- en: We’re starting out with 1,000 epochs of training. When training is complete,
    we can dig into our metrics to discover whether this is the correct number.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始使用1,000个时期进行训练。训练完成后，我们可以深入研究我们的指标，以发现这是否是正确的数量。
- en: '`batch_size`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size`'
- en: The `batch_size` argument specifies how many pieces of training data to feed
    into the network before measuring its accuracy and updating its weights and biases.
    If we wanted, we could specify a `batch_size` of `1`, meaning we’d run inference
    on a single datapoint, measure the loss of the network’s prediction, update the
    weights and biases to make the prediction more accurate next time, and then continue
    this cycle for the rest of the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size`参数指定在测量准确性并更新权重和偏差之前要向网络提供多少训练数据。如果需要，我们可以指定`batch_size`为`1`，这意味着我们将在单个数据点上运行推断，测量网络预测的损失，更新权重和偏差以使下次预测更准确，然后继续这个循环直到处理完所有数据。'
- en: Because we have 600 datapoints, each epoch would result in 600 updates to the
    network. This is a lot of computation, so our training would take ages! An alternative
    might be to select and run inference on multiple datapoints, measure the loss
    in aggregate, and then updating the network accordingly.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有600个数据点，每个时期会导致网络更新600次。这是很多计算量，所以我们的训练会花费很长时间！另一种选择可能是选择并对多个数据点运行推断，测量总体损失，然后相应地更新网络。
- en: If we set `batch_size` to `600`, each batch would include all of our training
    data. We’d now have to make only one update to the network every epoch—much quicker.
    The problem is, this results in less accurate models. Research has shown that
    models trained with large batch sizes have less ability to generalize to new data—they
    are more likely to overfit.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将`batch_size`设置为`600`，每个批次将包括所有训练数据。现在，我们每个时代只需要对网络进行一次更新，速度更快。问题是，这会导致模型的准确性降低。研究表明，使用大批量大小训练的模型对新数据的泛化能力较差，更容易过拟合。
- en: The compromise is to use a batch size that is somewhere in the middle. In our
    training code, we use a batch size of 16\. This means that we’ll choose 16 datapoints
    at random, run inference on them, calculate the loss in aggregate, and update
    the network once per batch. If we have 600 points of training data, the network
    will be updated around 38 times per epoch, which is far better than 600.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 妥协的方法是使用一个介于中间的批量大小。在我们的训练代码中，我们使用批量大小为16。这意味着我们会随机选择16个数据点，对它们进行推断，计算总体损失，并每批次更新一次网络。如果我们有600个训练数据点，网络将在每个时代更新大约38次，这比600次要好得多。
- en: When choosing a batch size, we’re making a compromise between training efficiency
    and model accuracy. The ideal batch size will vary from model to model. It’s a
    good idea to start with a batch size of 16 or 32 and experiment to see what works
    best.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择批量大小时，我们在训练效率和模型准确性之间做出妥协。理想的批量大小会因模型而异。最好从批量大小为16或32开始，并进行实验以找出最佳工作方式。
- en: '`validation_data`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`验证数据`'
- en: This is where we specify our validation dataset. Data from this dataset will
    be run through the network throughout the training process, and the network’s
    predictions will be compared with the expected values. We’ll see the results of
    validation in the logs and as part of the `history_1` object.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们指定验证数据集的地方。来自该数据集的数据将在整个训练过程中通过网络运行，并且网络的预测将与预期值进行比较。我们将在日志中看到验证结果，并作为`history_1`对象的一部分。
- en: Training Metrics
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练指标
- en: Hopefully, by now, training has finished. If not, wait a few moments for it
    to complete.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到目前为止，培训已经结束。如果没有，请等待一段时间以完成培训。
- en: We’re now going to check various metrics to see how well our network has learned.
    To begin, let’s look at the logs written during training. This will show how the
    network has improved during training from its random initial state.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查各种指标，以查看我们的网络学习情况如何。首先，让我们查看训练期间编写的日志。这将显示网络如何从其随机初始状态改进。
- en: 'Here are the logs for our first and last epochs:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一个和最后一个时代的日志：
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `loss`, `mae`, `val_loss`, and `val_mae` tell us various things:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`损失`，`mae`，`val_loss`和`val_mae`告诉我们各种事情：'
- en: '`loss`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`损失`'
- en: This is the output of our loss function. We’re using mean squared error, which
    is expressed as a positive number. Generally, the smaller the loss value, the
    better, so this is a good thing to watch as we evaluate our network.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们损失函数的输出。我们使用均方误差，它表示为正数。通常，损失值越小，越好，因此在评估网络时观察这一点是一个好方法。
- en: Comparing the first and last epochs, the network has clearly improved during
    training, going from a loss of ~0.7 to a smaller value of ~0.15\. Let’s look at
    the other numbers to see whether this improvement is enough!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 比较第一个和最后一个时代，网络在训练过程中显然有所改进，从约0.7的损失到更小的约0.15。让我们看看其他数字，以确定这种改进是否足够！
- en: '`mae`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`mae`'
- en: This is the mean absolute error of our training data. It shows the average difference
    between the network’s predictions and the expected `y` values from the training
    data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们训练数据的平均绝对误差。它显示了网络预测值与训练数据中预期`y`值之间的平均差异。
- en: 'We can expect our initial error to be pretty dismal, given that it’s based
    on an untrained network. This is certainly the case: the network’s predictions
    are off by an average of ~0.78, which is a large number when the range of acceptable
    values is only from –1 to 1!'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 可以预期我们的初始误差会非常糟糕，因为它基于未经训练的网络。这当然是事实：网络的预测平均偏差约为0.78，这是一个很大的数字，当可接受值的范围仅为-1到1时！
- en: However, even after training, our mean absolute error is ~0.30\. This means
    that our predictions are off by an average of ~0.30, which is still quite awful.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在训练之后，我们的平均绝对误差仍然约为0.30。这意味着我们的预测平均偏差约为0.30，这仍然相当糟糕。
- en: '`val_loss`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`val_loss`'
- en: This is the output of our loss function on our validation data. In our final
    epoch, the training loss (~0.15) is slightly lower than the validation loss (~0.17).
    This is a hint that our network might be overfitting, because it is performing
    worse on data it has not seen before.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们验证数据上损失函数的输出。在我们的最后一个时代中，训练损失（约0.15）略低于验证损失（约0.17）。这暗示我们的网络可能存在过拟合问题，因为它在未见过的数据上表现更差。
- en: '`val_mae`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`val_mae`'
- en: This is the mean absolute error for our validation data. With a value of ~0.32,
    it’s worse than the mean absolute error on our training set, which is another
    sign that the network might be overfitting.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们验证数据的平均绝对误差。值为约0.32，比我们训练集上的平均绝对误差更糟糕，这是网络可能存在过拟合的另一个迹象。
- en: Graphing the History
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制历史数据
- en: So far, it’s clear that our model is not doing a great job of making accurate
    predictions. Our task now is to figure out why. To do so, let’s make use of the
    data collected in our `history_1` object.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，很明显我们的模型并没有做出准确的预测。我们现在的任务是找出原因。为此，让我们利用我们`history_1`对象中收集的数据。
- en: 'The next cell extracts the training and validation loss data from the history
    object and plots it on a chart:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格从历史对象中提取训练和验证损失数据，并将其绘制在图表上：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `history_1` object contains an attribute called, `history_1.history`, which
    is a dictionary recording metric values during training and validation. We use
    this to collect the data we’re going to plot. For our x-axis we use the epoch
    number, which we determine by looking at the number of loss datapoints. Run the
    cell and you’ll see the graph in [Figure 4-13](#training_validation_loss).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`history_1`对象包含一个名为`history_1.history`的属性，这是一个记录训练和验证期间指标值的字典。我们使用这个来收集我们要绘制的数据。对于我们的x轴，我们使用时期数，通过查看损失数据点的数量来确定。运行单元格，您将在[图4-13](#training_validation_loss)中看到图形。'
- en: '![A graph of training and validation loss](Images/timl_0413.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证损失的图形](Images/timl_0413.png)'
- en: Figure 4-13\. A graph of training and validation loss
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13。训练和验证损失的图形
- en: As you can see, the amount of loss rapidly decreases over the first 50 epochs,
    before flattening out. This means that the model is improving and producing more
    accurate predictions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，损失量在前50个时期内迅速减少，然后趋于稳定。这意味着模型正在改进并产生更准确的预测。
- en: Our goal is to stop training when either the model is no longer improving or
    the training loss is less than the validation loss, which would mean that the
    model has learned to predict the training data so well that it can no longer generalize
    to new data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在模型不再改进或训练损失小于验证损失时停止训练，这意味着模型已经学会如此好地预测训练数据，以至于无法推广到新数据。
- en: 'The loss drops precipitously in the first few epochs, which makes the rest
    of the graph quite difficult to read. Let’s skip the first 100 epochs by running
    the next cell:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 损失在最初几个时期急剧下降，这使得其余的图表非常难以阅读。让我们通过运行下一个单元格来跳过前100个时期：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Figure 4-14](#training_validation_loss_skip) presents the graph produced by
    this cell.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-14](#training_validation_loss_skip)展示了此单元格生成的图形。'
- en: '![A graph of training and validation loss, skipping the first 100 epochs](Images/timl_0414.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![跳过前100个时期的训练和验证损失图](Images/timl_0414.png)'
- en: Figure 4-14\. A graph of training and validation loss, skipping the first 100
    epochs
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14。跳过前100个时期的训练和验证损失图
- en: Now that we’ve zoomed in, you can see that loss continues to reduce until around
    600 epochs, at which point it is mostly stable. This means that there’s probably
    no need to train our network for so long.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经放大了，您可以看到损失继续减少直到大约600个时期，此时它基本稳定。这意味着可能没有必要训练我们的网络那么长时间。
- en: However, you can also see that the lowest loss value is still around 0.15\.
    This seems relatively high. In addition, the validation loss values are consistently
    even higher.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您还可以看到最低的损失值仍然约为0.15。这似乎相对较高。此外，验证损失值始终更高。
- en: 'To gain more insight into our model’s performance we can plot some more data.
    This time, let’s plot the mean absolute error. Run the next cell to do so:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解我们模型的性能，我们可以绘制更多数据。这次，让我们绘制平均绝对误差。运行下一个单元格来执行：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Figure 4-15](#training_validation_mae) shows the resulting graph.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-15](#training_validation_mae)显示了结果图形。'
- en: '![A graph of mean absolute error during training and validation](Images/timl_0415.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证期间的平均绝对误差图](Images/timl_0415.png)'
- en: Figure 4-15\. A graph of mean absolute error during training and validation
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15。训练和验证期间的平均绝对误差图
- en: This graph of mean absolute error gives us some further clues. We can see that
    on average, the training data shows lower error than the validation data, which
    means that the network might have overfit, or learned the training data so rigidly
    that it can’t make effective predictions about new data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个平均绝对误差图给了我们一些进一步的线索。我们可以看到，平均而言，训练数据显示的误差比验证数据低，这意味着网络可能已经过拟合，或者学习了训练数据，以至于无法对新数据做出有效预测。
- en: In addition, the mean absolute error values are quite high, around ~0.31, which
    means that some of the model’s predictions are wrong by at least 0.31\. Since
    our expected values only range in size from –1 to +1, an error of 0.31 means we
    are very far from accurately modeling the sine wave.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，平均绝对误差值相当高，约为0.31左右，这意味着模型的一些预测至少有0.31的错误。由于我们的预期值的范围仅为-1到+1，0.31的误差意味着我们离准确建模正弦波还有很大距离。
- en: To get more insight into what is happening, we can plot our network’s predictions
    for the training data against the expected values.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入了解发生了什么，我们可以将网络对训练数据的预测与预期值绘制在一起。
- en: 'This happens in the following cell:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在以下单元格中：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By calling `model_1.predict(x_train)`, we run inference on all of the `x` values
    from the training data. The method returns an array of predictions. Let’s plot
    this on the graph alongside the actual `y` values from our training set. Run the
    cell to see the graph in [Figure 4-16](#training_predicted_actual).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`model_1.predict(x_train)`，我们对训练数据中的所有`x`值进行推断。该方法返回一个预测数组。让我们将这个绘制在图上，与我们训练集中的实际`y`值一起。运行单元格，您将在[图4-16](#training_predicted_actual)中看到图形。
- en: '![A graph of predicted versus actual values for our training data](Images/timl_0416.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![我们训练数据的预测与实际值的图形](Images/timl_0416.png)'
- en: Figure 4-16\. A graph of predicted versus actual values for our training data
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16。我们训练数据的预测与实际值的图形
- en: Oh, dear! The graph makes it clear that our network has learned to approximate
    the sine function in a very limited way. The predictions are highly linear, and
    only very roughly fit the data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，亲爱的！图表清楚地表明我们的网络已经学会以非常有限的方式逼近正弦函数。预测非常线性，只是非常粗略地拟合数据。
- en: The rigidity of this fit suggests that the model does not have enough capacity
    to learn the full complexity of the sine wave function, so it’s able to approximate
    it only in an overly simplistic way. By making our model bigger, we should be
    able to improve its performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这种拟合的刚性表明模型没有足够的容量来学习正弦波函数的全部复杂性，因此它只能以过于简单的方式逼近它。通过使我们的模型更大，我们应该能够提高其性能。
- en: Improving Our Model
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进我们的模型
- en: 'Armed with the knowledge that our original model was too small to learn the
    complexity of our data, we can try to make it better. This is a normal part of
    the machine learning workflow: design a model, evaluate its performance, and make
    changes in the hope of seeing improvement.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借我们的原始模型太小无法学习数据的复杂性的知识，我们可以尝试改进它。这是机器学习工作流程的正常部分：设计模型，评估其性能，并进行更改，希望看到改进。
- en: An easy way to make the network bigger is to add another layer of neurons. Each
    layer of neurons represents a transformation of the input that will hopefully
    get it closer to the expected output. The more layers of neurons a network has,
    the more complex these transformations can be.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 扩大网络的简单方法是添加另一层神经元。每一层神经元代表输入的转换，希望能使其更接近预期的输出。网络有更多层神经元，这些转换就可以更复杂。
- en: 'Run the following cell to redefine our model in the same way as earlier, but
    with an additional layer of 16 neurons in the middle:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下单元格以重新定义我们的模型，方式与之前相同，但在中间增加了16个神经元的额外层：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As you can see, the code is basically the same as for our first model, but
    with an additional `Dense` layer. Let’s run the cell to see the `summary()` results:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，代码基本上与我们第一个模型相同，但增加了一个`Dense`层。让我们运行这个单元格来查看`summary()`结果：
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With two layers of 16 neurons, our new model is a lot larger. It has (1 * 16)
    + (16 * 16) + (16 * 1) = 288 weights, plus 16 + 16 + 1 = 33 biases, for a total
    of 288 + 33 = 321 parameters. Our original model had only 49 total parameters,
    so this is a 555% increase in model size. Hopefully, this extra capacity will
    help represent the complexity of our data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 有了16个神经元的两层，我们的新模型要大得多。它有(1 * 16) + (16 * 16) + (16 * 1) = 288个权重，加上16 + 16
    + 1 = 33个偏差，总共是288 + 33 = 321个参数。我们的原始模型只有49个总参数，因此模型大小增加了555%。希望这种额外的容量将有助于表示数据的复杂性。
- en: 'The following cell will train our new model. Since our first model stopped
    improving so quickly, let’s train for fewer epochs this time—only 600\. Run this
    cell to begin training:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的单元格将训练我们的新模型。由于我们的第一个模型改进得太快，这次让我们训练更少的时代——只有600个。运行这个单元格开始训练：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When training is complete, we can take a look at the final log to get a quick
    feel for whether things have improved:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以查看最终日志，快速了解事情是否有所改善：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Wow! You can see that we’ve already achieved a huge improvement—validation loss
    has dropped from 0.17 to 0.01, and validation mean absolute error has dropped
    from 0.32 to 0.08\. This looks very promising.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！您可以看到我们已经取得了巨大的进步——验证损失从0.17降至0.01，验证平均绝对误差从0.32降至0.08。这看起来非常有希望。
- en: 'To see how things are going, let’s run the next cell. It’s set up to generate
    the same graphs we used last time. First, we draw a graph of the loss:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解情况如何，让我们运行下一个单元格。它设置为生成我们上次使用的相同图表。首先，我们绘制损失的图表：
- en: '[PRE28]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Figure 4-17](#training_validation_loss_2) shows the result.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-17](#training_validation_loss_2)显示了结果。'
- en: 'Next, we draw the same loss graph but with the first 100 epochs skipped so
    that we can better see the detail:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制相同的损失图，但跳过前100个时代，以便更好地看到细节：
- en: '[PRE29]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![A graph of training and validation loss](Images/timl_0417.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证损失的图表](Images/timl_0417.png)'
- en: Figure 4-17\. A graph of training and validation loss
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-17。训练和验证损失的图表
- en: '[Figure 4-18](#training_validation_loss_skip_2) presents the output.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-18](#training_validation_loss_skip_2)展示了输出。'
- en: 'Finally, we plot the mean absolute error for the same set of epochs:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制相同一组时代的平均绝对误差：
- en: '[PRE30]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![A graph of training and validation loss, skipping the first 100 epochs](Images/timl_0418.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证损失的图表，跳过前100个时代](Images/timl_0418.png)'
- en: Figure 4-18\. A graph of training and validation loss, skipping the first 100
    epochs
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-18。训练和验证损失的图表，跳过前100个时代
- en: '[Figure 4-19](#training_validation_mae_2) depicts the graph.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-19](#training_validation_mae_2)描述了图表。'
- en: '![A graph of mean absolute error during training and validation](Images/timl_0419.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证期间的平均绝对误差图](Images/timl_0419.png)'
- en: Figure 4-19\. A graph of mean absolute error during training and validation
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-19。训练和验证期间的平均绝对误差图
- en: 'Great results! From these graphs, we can see two exciting things:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒的结果！从这些图表中，我们可以看到两个令人兴奋的事情：
- en: The metrics are broadly better for validation than training, which means the
    network is not overfitting.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证的指标比训练的要好，这意味着网络没有过拟合。
- en: The overall loss and mean absolute error are much better than in our previous
    network.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体损失和平均绝对误差比我们之前的网络要好得多。
- en: You might be wondering why the metrics for validation are better than those
    for training, and not merely identical. The reason is that validation metrics
    are calculated at the end of each epoch, meanwhile training metrics are calculated
    while the epoch of training is still in progress. This means validation happens
    on a model that has been trained for slightly longer.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么验证的指标比训练的好，而不仅仅是相同的。原因是验证指标是在每个时代结束时计算的，而训练指标是在训练时代仍在进行时计算的。这意味着验证是在一个训练时间稍长的模型上进行的。
- en: Based on our validation data, our model seems to be performing great. However,
    to be sure of this, we need to run one final test.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的验证数据，我们的模型似乎表现很好。然而，为了确保这一点，我们需要进行最后一次测试。
- en: Testing
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: Earlier, we set aside 20% of our data to use for testing. As we discussed, it’s
    very important to have separate validation and test data. Since we fine-tune our
    network based on its validation performance, there’s a risk that we might accidentally
    tune the model to overfit its validation set and that it might not be able to
    generalize to new data. By retaining some fresh data and using it for a final
    test of our model, we can make sure that this has not happened.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们留出了20%的数据用于测试。正如我们讨论过的，拥有单独的验证和测试数据非常重要。由于我们根据验证性能微调我们的网络，存在一个风险，即我们可能会意外地调整模型以过度拟合其验证集，并且可能无法推广到新数据。通过保留一些新鲜数据并将其用于对模型的最终测试，我们可以确保这种情况没有发生。
- en: After we’ve used our test data, we need to resist the urge to tune our model
    further. If we did make changes with the goal of improving test performance, we
    might cause it to overfit our test set. If we did this, we wouldn’t be able to
    know, because we’d have no fresh data left to test with.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用了我们的测试数据之后，我们需要抵制进一步调整模型的冲动。如果我们为了提高测试性能而进行更改，可能会导致过拟合测试集。如果这样做了，我们将无法知道，因为我们没有剩余的新数据来进行测试。
- en: This means that if our model performs badly on our test data, it’s time to go
    back to the drawing board. We’ll need to stop optimizing the current model and
    come up with a brand new architecture.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果我们的模型在测试数据上表现不佳，那么是时候重新考虑了。我们需要停止优化当前模型，并提出全新的架构。
- en: 'With that in mind, the following cell will evaluate our model against our test
    data:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，接下来的单元将评估我们的模型与测试数据的表现：
- en: '[PRE31]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: First, we call the model’s `evaluate()` method with the test data. This will
    calculate and print the loss and mean absolute error metrics, informing us as
    to how far the model’s predictions deviate from the actual values. Next, we make
    a set of predictions and plot them on a graph alongside the actual values.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用测试数据调用模型的`evaluate()`方法。这将计算并打印损失和平均绝对误差指标，告诉我们模型的预测与实际值的偏差有多大。接下来，我们进行一组预测，并将其与实际值一起绘制在图表上。
- en: 'Now we can run the cell to learn how our model is performing! First, let’s
    see the results of `evaluate()`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行单元，了解我们的模型表现如何！首先，让我们看看`evaluate()`的结果：
- en: '[PRE32]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This shows that 200 datapoints were evaluated, which is our entire test set.
    The model took 71 microseconds to make each prediction. The loss metric was 0.0103,
    which is excellent, and very close to our validation loss of 0.0104\. Our mean
    absolute error, 0.0718, is also very small and fairly close to its equivalent
    in validation, 0.0806.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示有200个数据点被评估，这是我们整个测试集。模型每次预测需要71微秒。损失指标为0.0103，非常出色，并且非常接近我们的验证损失0.0104。我们的平均绝对误差为0.0718，也非常小，与验证中的0.0806相当接近。
- en: This means that our model is working great, and it isn’t overfitting! If the
    model had overfit our validation data, we could expect that the metrics on our
    test set would be significantly worse than those resulting from validation.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的模型运行良好，没有过拟合！如果模型过拟合了验证数据，我们可以预期测试集上的指标会明显比验证结果差。
- en: The graph of our predictions against our actual values, shown in [Figure 4-20](#test_predicted_actual),
    makes it clear how well our model is performing.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测与实际值的图表，显示在[图4-20](#test_predicted_actual)中，清楚地展示了我们的模型表现如何。
- en: '![A graph of predicted versus actual values for our test data](Images/timl_0420.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![我们的测试数据的预测与实际值的图表](Images/timl_0420.png)'
- en: Figure 4-20\. A graph of predicted versus actual values for our test data
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-20。我们的测试数据的预测与实际值的图表
- en: You can see that, for the most part, the dots representing *predicted* values
    form a smooth curve along the center of the distribution of *actual* values. Our
    network has learned to approximate a sine curve, even though the dataset was noisy!
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，大部分情况下，代表*预测*值的点形成了一个平滑的曲线，沿着*实际*值的分布中心。我们的网络已经学会了近似正弦曲线，即使数据集很嘈杂！
- en: 'If you look closely, however, you’ll see that there are some imperfections.
    The peak and trough of our predicted sine wave are not perfectly smooth, like
    a real sine wave would be. Variations in our training data, which is randomly
    distributed, have been learned by our model. This is a mild case of overfitting:
    instead of learning the smooth sine function, our model has learned to replicate
    the exact shape of our data.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仔细观察，你会发现一些不完美之处。我们预测的正弦波的峰值和谷值并不完全平滑，像真正的正弦波那样。我们模型学习了训练数据的变化，这些数据是随机分布的。这是过拟合的轻微情况：我们的模型没有学习到平滑的正弦函数，而是学会了复制数据的确切形状。
- en: For our purposes, this overfitting isn’t a major problem. Our goal is for this
    model to gently fade an LED on and off, and it doesn’t need to be perfectly smooth
    to achieve this. If we thought the level of overfitting was problematic, we could
    attempt to address it through regularization techniques or by obtaining more training
    data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，这种过拟合并不是一个主要问题。我们的目标是让这个模型轻轻地控制LED的亮度，不需要完全平滑才能实现这一目标。如果我们认为过拟合的程度有问题，我们可以尝试通过正则化技术或获取更多的训练数据来解决。
- en: Now that we’re happy with our model, let’s get it ready to deploy on-device!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对模型满意了，让我们准备在设备上部署它！
- en: Converting the Model for TensorFlow Lite
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型转换为TensorFlow Lite
- en: At the beginning of this chapter we briefly touched on TensorFlow Lite, which
    is a set of tools for running TensorFlow models on “edge devices”—meaning everything
    from mobile phones down to microcontroller boards.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们简要提到了TensorFlow Lite，这是一组用于在“边缘设备”上运行TensorFlow模型的工具。
- en: '[Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers) goes
    into detail on TensorFlow Lite for Microcontrollers. For now, we can think of
    it as having two main components:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)详细介绍了用于微控制器的TensorFlow
    Lite。目前，我们可以将其视为具有两个主要组件：'
- en: TensorFlow Lite Converter
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite转换器
- en: This converts TensorFlow models into a special, space-efficient format for use
    on memory-constrained devices, and it can apply optimizations that further reduce
    the model size and make it run faster on small devices.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这将TensorFlow模型转换为一种特殊的、节省空间的格式，以便在内存受限设备上使用，并且可以应用优化，进一步减小模型大小，并使其在小型设备上运行更快。
- en: TensorFlow Lite Interpreter
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite解释器
- en: This runs an appropriately converted TensorFlow Lite model using the most efficient
    operations for a given device.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用给定设备的最有效操作来运行适当转换为TensorFlow Lite模型。
- en: Before we use our model with TensorFlow Lite, we need to convert it. We use
    the TensorFlow Lite Converter’s Python API to do this. It takes our Keras model
    and writes it to disk in the form of a *FlatBuffer*, which is a special file format
    designed to be space-efficient. Because we’re deploying to devices with limited
    memory, this will come in handy! We’ll look at FlatBuffers in more detail in [Chapter 12](ch12.xhtml#chapter_magic_wand_training).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用TensorFlow Lite之前，我们需要将模型转换。我们使用TensorFlow Lite转换器的Python API来完成这个任务。它将我们的Keras模型写入磁盘，以*FlatBuffer*的形式，这是一种专门设计的节省空间的文件格式。由于我们要部署到内存有限的设备，这将非常有用！我们将在[第12章](ch12.xhtml#chapter_magic_wand_training)中更详细地了解FlatBuffers。
- en: In addition to creating a FlatBuffer, the TensorFlow Lite Converter can also
    apply optimizations to the model. These optimizations generally reduce the size
    of the model, the time it takes to run, or both. This can come at the cost of
    a reduction in accuracy, but the reduction is often small enough that it’s worthwhile.
    You can read more about optimizations in [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建FlatBuffer外，TensorFlow Lite转换器还可以对模型应用优化。这些优化通常会减小模型的大小、运行时间，或者两者兼而有之。这可能会导致准确度降低，但降低通常是小到足以值得的。您可以在[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)中了解更多关于优化的信息。
- en: One of the most useful optimizations is *quantization*. By default, the weights
    and biases in a model are stored as 32-bit floating-point numbers so that high-precision
    calculations can occur during training. Quantization allows you to reduce the
    precision of these numbers so that they fit into 8-bit integers—a four times reduction
    in size. Even better, because it’s easier for a CPU to perform math with integers
    than with floats, a quantized model will run faster.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最有用的优化之一是*量化*。默认情况下，模型中的权重和偏置以32位浮点数存储，以便在训练期间进行高精度计算。量化允许您减少这些数字的精度，使其适合于8位整数——大小减小四倍。更好的是，因为CPU更容易使用整数而不是浮点数进行数学运算，量化模型将运行得更快。
- en: The coolest thing about quantization is that it often results in minimal loss
    in accuracy. This means that when deploying to low-memory devices, it is nearly
    always worthwhile.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 量化最酷的一点是，它通常会导致准确度的最小损失。这意味着在部署到低内存设备时，几乎总是值得的。
- en: In the following cell, we use the converter to create and save two new versions
    of our model. The first is converted to the TensorFlow Lite FlatBuffer format,
    but without any optimizations. The second is quantized.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元格中，我们使用转换器创建并保存我们模型的两个新版本。第一个转换为TensorFlow Lite FlatBuffer格式，但没有任何优化。第二个是量化的。
- en: 'Run the cell to convert the model into these two variants:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格将模型转换为这两种变体：
- en: '[PRE33]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To create a quantized model that runs as efficiently as possible, we need to
    provide a *representative dataset*—a set of numbers that represent the full range
    of input values of the dataset on which the model was trained.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个尽可能高效运行的量化模型，我们需要提供一个*代表性数据集*——一组数字，代表了模型训练时数据集的全部输入值范围。
- en: In the preceding cell, we can use our test dataset’s `x` values as a representative
    dataset. We define a function, `representative_dataset_generator()`, that uses
    the `yield` operator to return them one by one.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的单元格中，我们可以使用测试数据集的`x`值作为代表性数据集。我们定义一个函数`representative_dataset_generator()`，使用`yield`操作符逐个返回这些值。
- en: To prove these models are still accurate after conversion and quantization,
    we use both of them to make predictions and compare these against our test results.
    Given that these are TensorFlow Lite models, we need to use the TensorFlow Lite
    interpreter to do so.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这些模型在转换和量化后仍然准确，我们使用它们进行预测，并将结果与我们的测试结果进行比较。鉴于这些是TensorFlow Lite模型，我们需要使用TensorFlow
    Lite解释器来执行此操作。
- en: 'Because it’s designed primarily for efficiency, the TensorFlow Lite interpreter
    is slightly more complicated to use than the Keras API. To make predictions with
    our Keras model, we could just call the `predict()` method, passing an array of
    inputs. With TensorFlow Lite, we need to do the following:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow Lite解释器主要设计用于效率，因此使用起来比Keras API稍微复杂一些。要使用我们的Keras模型进行预测，我们只需调用`predict()`方法，传递一个输入数组即可。而对于TensorFlow
    Lite，我们需要执行以下操作：
- en: Instantiate an `Interpreter` object.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`Interpreter`对象。
- en: Call some methods that allocate memory for the model.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用一些为模型分配内存的方法。
- en: Write the input to the input tensor.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入写入输入张量。
- en: Invoke the model.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用模型。
- en: Read the output from the output tensor.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输出张量中读取输出。
- en: 'This sounds like a lot, but don’t worry about it too much for now; we’ll walk
    through it in detail in [Chapter 5](ch05.xhtml#chapter_building_an_application).
    For now, run the following cell to make predictions with both models and plot
    them on a graph, alongside the results from our original, unconverted model:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很多，但现在不要太担心；我们将在[第5章](ch05.xhtml#chapter_building_an_application)中详细介绍。现在，运行以下单元格，使用两个模型进行预测，并将它们与原始未转换的模型的结果一起绘制在图表上：
- en: '[PRE34]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Running this cell yields the graph in [Figure 4-21](#test_predicted_actual_converted).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此单元格将产生[图4-21](#test_predicted_actual_converted)中的图表。
- en: '![A graph comparing models'' predictions against the actual values](Images/timl_0421.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![比较模型预测与实际值的图表](Images/timl_0421.png)'
- en: Figure 4-21\. A graph comparing models’ predictions against the actual values
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-21。比较模型预测与实际值的图表
- en: We can see from the graph that the predictions for the original model, the converted
    model, and the quantized model are all close enough to be indistinguishable. Things
    are looking good!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们可以看到，原始模型、转换模型和量化模型的预测都非常接近，几乎无法区分。情况看起来很不错！
- en: 'Since quantization makes models smaller, let’s compare both converted models
    to see the difference in size. Run the following cell to calculate their sizes
    and compare them:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 由于量化使模型变小，让我们比较两个转换后的模型，看看大小上的差异。运行以下单元格计算它们的大小并进行比较：
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should see the following output:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE36]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Our quantized model is 224 bytes smaller than the original version, which is
    great—but it’s only a minor reduction in size. At around 2.4 KB, this model is
    already so small that the weights and biases make up only a fraction of the overall
    size. In addition to weights, the model contains all the logic that makes up the
    architecture of our deep learning network, known as its *computation graph*. For
    truly tiny models, this can add up to more size than the model’s weights, meaning
    quantization has little effect.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的量化模型比原始版本小224字节，这很好，但大小只有轻微减小。在约2.4 KB左右，这个模型已经非常小，权重和偏差只占整体大小的一小部分。除了权重，模型还包含构成我们深度学习网络架构的所有逻辑，称为*计算图*。对于真正微小的模型，这可能比模型的权重占用更多的空间，这意味着量化几乎没有效果。
- en: More complex models have many more weights, meaning the space saving from quantization
    will be much higher. It can be expected to approach four times for most sophisticated
    models.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的模型有更多的权重，这意味着量化带来的空间节省将更高。对于大多数复杂模型，可以预期接近四倍。
- en: Regardless of its exact size, our quantized model will take less time to execute
    than the original version, which is important on a tiny microcontroller.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 无论其确切大小如何，我们的量化模型执行起来都比原始版本快，这对于微小微控制器非常重要。
- en: Converting to a C File
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换为C文件
- en: The final step in preparing our model for use with TensorFlow Lite for Microcontrollers
    is to convert it into a C source file that can be included in our application.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的模型能够与TensorFlow Lite for Microcontrollers一起使用的最后一步是将其转换为一个可以包含在我们应用程序中的C源文件。
- en: So far during this chapter, we’ve been using TensorFlow Lite’s Python API. This
    means that we’ve been able to use the `Interpreter` constructor to load our model
    files from disk.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们一直在使用TensorFlow Lite的Python API。这意味着我们可以使用`Interpreter`构造函数从磁盘加载我们的模型文件。
- en: However, most microcontrollers don’t have a filesystem, and even if they did,
    the extra code required to load a model from disk would be wasteful given our
    limited space. Instead, as an elegant solution, we provide the model in a C source
    file that can be included in our binary and loaded directly into memory.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数微控制器没有文件系统，即使有，从磁盘加载模型所需的额外代码也会在有限的空间下是浪费的。相反，作为一个优雅的解决方案，我们提供了一个可以包含在我们的二进制文件中并直接加载到内存中的C源文件中的模型。
- en: In the file, the model is defined as an array of bytes. Fortunately, there’s
    a convenient Unix tool named `xxd` that is able to convert a given file into the
    required format.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件中，模型被定义为一个字节数组。幸运的是，有一个方便的Unix工具名为`xxd`，能够将给定文件转换为所需的格式。
- en: 'The following cell runs `xxd` on our quantized model, writes the output to
    a file called *sine_model_quantized.cc*, and prints it to the screen:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格在我们的量化模型上运行`xxd`，将输出写入名为*sine_model_quantized.cc*的文件，并将其打印到屏幕上：
- en: '[PRE37]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is very long, so we won’t reproduce it all here, but here’s a snippet
    that includes just the beginning and end:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出非常长，所以我们不会在这里全部复制，但这里有一个片段，包括开头和结尾：
- en: '[PRE38]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: To use this model in a project, you could either copy and paste the source or
    download the file from the notebook.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要在项目中使用这个模型，您可以复制粘贴源代码，或者从笔记本中下载文件。
- en: Wrapping Up
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: And with that, we’re done building our model. We’ve trained, evaluated, and
    converted a TensorFlow deep learning network that can take a number between 0
    and 2π and output a good-enough approximation of its sine.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们构建我们的模型就完成了。我们已经训练、评估并转换了一个TensorFlow深度学习网络，可以接收0到2π之间的数字，并输出其正弦的良好近似值。
- en: This was our first taste of using Keras to train a tiny model. In future projects,
    we’ll be training models that are still tiny, but *far* more sophisticated.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次使用Keras训练微小模型。在未来的项目中，我们将训练仍然微小但*远远*更复杂的模型。
- en: For now, let’s move on to [Chapter 5](ch05.xhtml#chapter_building_an_application),
    where we’ll write code to run our model on microcontrollers.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续[第5章](ch05.xhtml#chapter_building_an_application)，在那里我们将编写代码在微控制器上运行我们的模型。
