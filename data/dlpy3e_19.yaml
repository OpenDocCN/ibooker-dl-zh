- en: The future of AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的未来
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter19_future_of_ai](https://deeplearningwithpython.io/chapters/chapter19_future_of_ai)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[深度学习Python教程](https://deeplearningwithpython.io/chapters/chapter19_future_of_ai)'
- en: To use a tool appropriately, you should not only understand what it *can* do
    but also be aware of what it *can’t* do. I’m going to present an overview of some
    key limitations of deep learning. Then, I’ll offer some speculative thoughts about
    the future evolution of AI and what it would take to get to human-level general
    intelligence. This should be especially interesting to you if you’d like to get
    into fundamental research.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要恰当地使用一个工具，你不仅应该了解它能做什么，还应该知道它不能做什么。我将概述一些深度学习的关键局限性。然后，我将提供一些关于人工智能未来演化和达到人类水平通用智能所需条件的推测性思考。如果你对基础研究感兴趣，这应该特别吸引你。
- en: The limitations of deep learning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的局限性
- en: There are infinitely many things you can do with deep learning. But deep learning
    can’t do *everything*. To use a tool well, you should be aware of its limitations,
    not just its strengths. So where does deep learning fall short?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用深度学习做无数的事情。但深度学习并不能做*一切*。要很好地使用一个工具，你应该了解它的局限性，而不仅仅是它的优势。那么，深度学习在哪里不足？
- en: Deep learning models struggle to adapt to novelty
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型难以适应新事物
- en: Deep learning models are big, parametric curves fitted to large datasets. That’s
    the source of their power — they’re easy to train, and they scale really well,
    both in terms of model size and dataset size. But that’s also a source of significant
    weaknesses. Curve fitting has inherent limitations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的特性是大型、参数化的曲线，这些曲线拟合到大量数据集上。这是它们力量的来源——它们容易训练，并且在模型大小和数据集大小方面都能很好地扩展。但这也是一个显著弱点的来源。曲线拟合有固有的局限性。
- en: First and foremost, a parametric curve is only capable of information storage
    — it’s a kind of *database*. Recall our discussion of Transformers as an “interpolative
    database” from chapter 15? Second, crucially, this database is *static*. The model’s
    parameters are determined during a distinct “training time” phase. Afterward,
    these parameters are frozen, and this fixed version is used during “inference
    time” for making predictions on new data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，参数化曲线只能用于信息存储——它是一种*数据库*。回想一下我们在第15章中关于Transformer作为“插值数据库”的讨论？其次，关键的是，这个数据库是*静态的*。模型的参数在特定的“训练时间”阶段确定。之后，这些参数被冻结，这个固定版本在“推理时间”用于对新数据进行预测。
- en: 'The only thing you can do with a static database is information retrieval.
    And that’s exactly what deep learning models excel at: recognizing or generating
    patterns highly similar to those encountered during training. The flip side is
    that they are inherently poor at *adaptation*. The database is backward-looking
    — it fits past data but can’t handle a changing future. At inference time, you’d
    better hope that the situations the model faces are part of the training data
    distribution, because otherwise, the model will break down. A model trained on
    ImageNet will classify a leopard-print sofa as an actual leopard, for instance
    — sofas were not part of its training data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用静态数据库唯一能做的就是信息检索。这正是深度学习模型擅长的：识别或生成与训练过程中遇到的模式高度相似的图案。另一方面，它们在*适应性*方面天生不足。数据库是向后看的——它适合过去的数据，但无法处理不断变化的未来。在推理时间，你最好希望模型面对的情况是训练数据分布的一部分，否则模型将会崩溃。例如，在ImageNet上训练的模型会将豹纹沙发分类为真正的豹子——沙发不是其训练数据的一部分。
- en: This also applies to the largest of generative models. In recent years, the
    rise of large language models (LLMs) and their application to programming assistance
    and reasoning-like problems has provided extensive empirical proof of this. Despite
    frequent claims that LLMs can perform *in-context learning* to pick up new skills
    from just a few examples, there is overwhelming evidence that what they’re actually
    doing is fetching vector functions they’ve memorized during training and reapplying
    them to the task at hand. By learning to do next-token prediction across a web-sized
    text dataset, an LLM has collected millions of potentially useful mini text-processing
    programs, and it can easily be prompted into reusing them on a new problem. But
    show it something that has no direct equivalent in its training data, and it’s
    helpless.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于最大的生成模型。近年来，大型语言模型（LLMs）的兴起及其在编程辅助和类似推理问题中的应用，为这一观点提供了广泛的实证证据。尽管经常有人声称LLMs可以通过*上下文学习*从几个例子中获取新技能，但大量证据表明，他们实际上正在执行的是在训练期间记忆的向量函数的检索和重新应用。通过学习在网页大小的文本数据集上进行下一标记预测，LLM收集了数百万个可能有用的迷你文本处理程序，并且可以很容易地提示它们在新问题上的重复使用。但是，如果展示给它的是其训练数据中没有直接对应的内容，它就无能为力了。
- en: Take a look at the puzzle in figure 19.1. Did you figure out the solution? Good.
    It’s not very hard, is it? But today, no state-of-the-art LLM or vision-language
    model can do this because this particular problem doesn’t directly map to anything
    they’ve seen at training time — even after having been trained on the entire internet
    and then some. An LLM’s ability to solve a given problem has nothing to do with
    problem complexity, and everything to do with *familiarity* — they will break
    their teeth on any sufficiently novel problem, no matter how simple.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图19.1中的谜题。你找到解决方案了吗？很好。这并不难，对吧？但是今天，没有任何最先进的LLM或视觉语言模型能够做到这一点，因为这个问题并没有直接映射到他们在训练时间看到的任何东西——即使是在整个互联网上训练过之后。LLM解决特定问题的能力与问题复杂性无关，而与*熟悉度*有关——他们会在任何足够新颖的问题上咬紧牙关，无论这个问题多么简单。
- en: '![](../Images/b23cd5277a88b93d0a3bf331695f5c36.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b23cd5277a88b93d0a3bf331695f5c36.png)'
- en: '[Figure 19.1](#figure-19-1): An easy yet novel puzzle'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.1](#figure-19-1)：一个简单而新颖的谜题'
- en: This failure mode even applies to tiny variations of a pattern that an LLM encountered
    many times in its training data. For instance, for a few months after the release
    of ChatGPT, if you asked it, “What’s heavier, 10 kilos of steel or one kilo of
    feathers?,” it would answer that they weigh the same. That’s because the question
    “What’s heavier, one kilo of steel or one kilo of feathers?” is found many times
    on the internet — as a trick question. The right answer, of course, is that they
    both weigh the same, so the GPT model would just repeat the answer it had memorized
    without paying any attention to the actual numbers in the query, or what the query
    really *meant*. Similarly, LLMs struggle to adapt to variations of the Monty Hall
    problem (see figure 19.2) and will tend to always output the canonical answer
    to the puzzle, which they’ve seen many times during training, regardless of whether
    it makes sense in context.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种失败模式甚至适用于LLM在训练数据中多次遇到的模式的微小变化。例如，在ChatGPT发布后的几个月里，如果你问它，“10公斤的钢铁和1公斤的羽毛哪个更重？”，它会回答说它们重量相同。这是因为“10公斤的钢铁和1公斤的羽毛哪个更重？”这个问题在互联网上多次出现——作为一个陷阱问题。当然，正确的答案是它们重量相同，所以GPT模型只是重复它记忆中的答案，而没有注意到查询中的实际数字，或者查询真正*意味着*什么。同样，LLM在适应蒙提霍尔问题的变体（见图19.2）方面也遇到困难，并且倾向于总是输出他们在训练期间多次看到的经典答案，而不管这在上下文中是否有意义。
- en: '![](../Images/bbb2a6ce26449fc1e118892bfb73a357.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbb2a6ce26449fc1e118892bfb73a357.png)'
- en: '[Figure 19.2](#figure-19-2): A variation of the Monty Hall problem'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.2](#figure-19-2)：蒙提霍尔问题的变体'
- en: To note, these specific prompts were patched later on by special-casing them.
    Today, there are over 25,000 people who are employed full time to provide training
    data for LLMs by reviewing failure cases and suggesting better answers. LLM maintenance
    is a constant game of whack-a-mole where failing prompts are patched one at a
    time, without addressing the more general underlying issue. Even already patched
    prompts will still fail if you make small changes to them!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这些特定的提示后来通过特殊处理得到了修复。如今，有超过25,000人全职工作，通过审查失败案例和提出更好的答案来为LLMs提供训练数据。LLMs的维护是一场持续的打地鼠游戏，一次修复一个失败的提示，而没有解决更普遍的根本问题。即使已经修复的提示，如果你对其做出小的修改，它们仍然会失败！
- en: Deep learning models are highly sensitive to phrasing and other distractors
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型对措辞和其他干扰因素非常敏感
- en: A closely related problem is the extreme sensitivity of deep learning models
    to how their input is presented. For instance, image models are affected by *adversarial
    examples*, which are samples fed to a deep learning network that are designed
    to trick the model into misclassifying them. You’re already aware that it’s possible
    to do gradient ascent in input space to generate inputs that maximize the activation
    of some ConvNet filter — this is the basis of the filter visualization technique
    introduced in chapter 10.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与之密切相关的问题是深度学习模型对输入呈现方式的极端敏感性。例如，图像模型会受到*对抗样本*的影响，这些样本被输入到深度学习网络中，旨在欺骗模型将其错误分类。你已经知道，在输入空间中可以进行梯度上升来生成最大化某些卷积神经网络（ConvNet）滤波器激活的输入——这是第10章中引入的滤波器可视化技术的基础。
- en: Similarly, through gradient ascent, you can slightly modify an image to maximize
    the class prediction for a given class. By taking a picture of a panda and adding
    to it a gibbon gradient, we can get a neural network to classify the panda as
    a gibbon (see figure 19.3). This evidences both the brittleness of these models
    and the deep difference between their input-to-output mapping and our human perception.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，通过梯度上升，你可以稍微修改一个图像，以最大化给定类别的预测。通过拍摄一只熊猫的照片并添加长臂猿的梯度，我们可以让神经网络将熊猫分类为长臂猿（见图19.3）。这既证明了这些模型的脆弱性，也证明了它们的输入到输出映射与我们的人类感知之间的深刻差异。
- en: '![](../Images/31f0c0132cc718d30238b6ec0e944f54.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31f0c0132cc718d30238b6ec0e944f54.png)'
- en: '[Figure 19.3](#figure-19-3): An adversarial example: imperceptible changes
    in an image can upend a model’s classification of the image.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.3](#figure-19-3)：一个对抗样本：图像中的细微变化可能会颠覆模型对图像的分类。'
- en: 'Similarly, LLMs suffer from an extremely high sensitivity to minor details
    in their prompts. Innocuous prompt modifications, such as changing place and people’s
    names in a text paragraph or variable names in a block of code, can significantly
    degrade LLM performance. Consider the well-known *Alice in Wonderland* riddle^([[1]](#footnote-1)):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，大型语言模型（LLMs）对提示中的细微细节具有极高的敏感性。无害的提示修改，如更改文本段落中的地点和人物名称或代码块中的变量名称，可能会显著降低LLMs的性能。考虑一下著名的*爱丽丝梦游仙境*谜题^([[1]](#footnote-1))：
- en: “Alice has N brothers and she also has M sisters. How many sisters does Alice’s
    brother have?”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “爱丽丝有N个兄弟，她还有M个姐妹。爱丽丝的兄弟有多少个姐妹？”
- en: The answer, of course, is *M* + 1 (Alice’s sisters plus Alice herself). For
    an LLM, asking the question with values commonly found in online instances of
    the riddle (like *N* = 3 and *M* = 2) will generally result in the correct answer,
    but try tweaking the values of *M* and *N*, and you will quickly get incorrect
    answers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 答案当然是*M* + 1（爱丽丝的姐妹加上爱丽丝自己）。对于一个LLM来说，用在线实例中常见的值（如*N* = 3和*M* = 2）提问通常会得到正确答案，但如果你调整*M*和*N*的值，你很快就会得到错误的答案。
- en: 'This oversensitivity to phrasing has given rise to the concept of *prompt engineering*.
    Prompt engineering is the art of formulating LLM prompts in a way that maximizes
    performance on a task. For instance, it turns out that adding the instruction
    “Please think step by step” to a prompt that involves reasoning can significantly
    boost performance. The term *prompt engineering* is a very optimistic framing
    of the underlying issue: “Your models are better than you know! You just need
    to use them right!” A more negative framing would be to point out that for any
    query that seems to work, there’s a range of minor changes that have the potential
    to tank performance. To what extent do LLMs understand something if you can break
    their understanding with simple rewordings?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对措辞的过度敏感催生了**提示工程**的概念。提示工程是制定LLM提示的艺术，以最大限度地提高任务性能。例如，将“请逐步思考”这样的指令添加到涉及推理的提示中，可以显著提高性能。术语**提示工程**是对潜在问题的非常乐观的表述：“你的模型比你想象的要好！你只需要正确使用它们！”更消极的表述可能是指出，对于任何看似有效的查询，都有一系列微小的变化可能会严重影响性能。如果你可以通过简单的改写来破坏LLMs的理解，那么LLMs对某事的理解程度有多大？
- en: 'What’s behind this phenomenon is that an LLM is a big parametric curve — a
    medium for storing knowledge and programs where you can interpolate between any
    two objects to produce infinitely many intermediate objects. Your prompt is a
    way to address a particular location of the database: if you ask, “How do you
    sort a list in Python? Answer like a pirate,” that’s a kind of database lookup,
    where you first retrieve a piece of knowledge (how to sort a list in Python) and
    then retrieve and execute a style transfer program (“Answer like a pirate”).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象背后的原因是，LLM是一个大的参数曲线——一个存储知识和程序的中介，你可以在这两个对象之间进行插值，以产生无限多的中间对象。你的提示是一种指向数据库特定位置的方式：如果你问，“如何在Python中排序一个列表？像海盗一样回答”，那是一种数据库查找，你首先检索一段知识（如何在Python中排序列表），然后检索并执行一个风格转换程序（“像海盗一样回答”）。
- en: Since the knowledge and programs indexed by the LLM are interpolative, you can
    *move around in latent space* to explore nearby locations. A slightly different
    prompt, like “Explain Python list sorting, but answer like a buccaneer” would
    still have pointed to a very similar location in the database, resulting in an
    answer that would be pretty close but not quite identical. There are thousands
    of variations you could have used, each resulting in a similar yet slightly different
    answer. And that’s why prompt engineering is needed. There is no a priori reason
    for your first, naive prompt to be optimal for your task. The LLM is not going
    to understand what you meant and then perform it in the best possible way — it’s
    merely going to fetch the program that your prompt points to, among many possible
    locations you could have landed on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM索引的知识和程序是插值的，你可以在潜在空间中*移动*来探索附近的区域。一个略有不同的提示，比如“解释Python列表排序，但像海盗一样回答”，仍然会指向数据库中非常相似的位置，从而得到一个相当接近但并不完全相同的答案。你可以使用数千种不同的变体，每种变体都会得到一个相似但略有不同的答案。这就是为什么需要提示工程的原因。你的第一个、天真的提示没有先验理由是针对你的任务的优化。LLM不会理解你的意思，然后以最佳方式执行它——它只是会在许多可能的着陆点中检索你的提示所指向的程序。
- en: Prompt engineering is the process of searching through latent space to find
    the lookup query that seems to perform best on your target task by trial and error.
    It’s no different from trying different keywords when doing a Google search. If
    LLMs actually understood what you asked them, there would be no need for this
    search process, since the amount of information conveyed about your target task
    does not change whether your prompt uses the word “rewrite” instead of “rephrase”
    or whether you prefix your prompt with “Think step by step.” Never assume that
    the LLM “gets it” the first time — keep in mind that your prompt is but an address
    in an infinite ocean of programs, all memorized as a by-product of learning to
    complete an enormous amount of token sequences.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是通过试错法在潜在空间中搜索，以找到在目标任务上似乎表现最好的查找查询的过程。这与在谷歌搜索时尝试不同的关键词没有区别。如果大型语言模型（LLMs）实际上理解了你所问的内容，那么这个搜索过程就没有必要了，因为关于你的目标任务所传达的信息量不会因为你的提示使用“重写”而不是“改写”或者是否在提示前加上“逐步思考”而改变。永远不要假设LLM第一次就能“理解”你的意思——记住，你的提示只是无限程序海洋中的一个地址，所有这些程序都是作为学习完成大量标记序列的副产品而被记忆的。
- en: Deep learning models struggle to learn generalizable programs
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型在学习可泛化程序方面存在困难
- en: 'The problem with deep learning models isn’t just that they’re limited to blindly
    reapplying patterns they’ve memorized at training time or that they’re highly
    sensitive to how their input is presented. Even if you just need to query and
    apply a well-known program, and you know exactly how to address this program in
    latent space, you still face a major issue: the programs memorized by deep learning
    models often don’t generalize well. They will work for some input values and fail
    for some other input values. This is especially true for programs that encode
    any kind of discrete logic.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的问题不仅仅是它们局限于盲目地重新应用在训练时记住的模式，或者它们对输入的呈现方式高度敏感。即使你只需要查询和应用一个已知的程序，并且你知道如何在潜在空间中精确地定位这个程序，你仍然面临一个主要问题：深度学习模型记住的程序通常泛化能力不好。它们对某些输入值有效，但对其他输入值无效。这对于编码任何类型的离散逻辑的程序尤其如此。
- en: 'Consider the problem of adding two numbers, represented as character sequences
    — like “4 3 5 7 + 8 9 3 6.” Try training a Transformer on hundreds of thousands
    of such digit pairs: you will reach a very high accuracy. Very high, but not 100%
    — you will keep regularly seeing incorrect answers, because the Transformer doesn’t
    manage to encode the exact addition algorithm (you know, the one you learned in
    primary school). It is instead guessing the output by interpolating between the
    data points it has seen at training time.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下加法的问题，即以字符序列表示的两个数字的相加——例如“4 3 5 7 + 8 9 3 6”。尝试训练一个Transformer在成千上万的此类数字对上：你将达到非常高的准确率。非常高，但不是100%——你将经常看到错误的答案，因为Transformer无法成功编码精确的加法算法（你知道的，你在小学学过的那个）。相反，它通过在训练时看到的各个数据点之间进行插值来猜测输出。
- en: This applies to state-of-the-art LLMs, too — at least those that weren’t explicitly
    hardcoded to execute snippets like “4357 + 8936” in Python to provide the right
    answer. They’ve seen enough examples of digit addition that they can add numbers,
    but they only have about 70% accuracy — quite underwhelming. Further, their accuracy
    is strongly dependent on *which* digits are being added, with more common digits
    leading to higher accuracy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于最先进的LLMs——至少对于那些没有明确硬编码在Python中执行“4357 + 8936”等片段以提供正确答案的LLMs。它们已经看到了足够多的数字加法示例，可以加法，但它们的准确率只有大约70%——相当令人失望。此外，它们的准确率强烈依赖于*哪些*数字正在被加，更常见的数字会导致更高的准确率。
- en: 'The reason why a deep learning model does not end up learning an exact addition
    algorithm even after seeing millions of examples is that it is just *a static
    chain of simple, continuous geometric transformations* mapping one vector space
    into another. That is a good fit for perceptual pattern recognition, but it’s
    a very poor fit for encoding any sort of step-by-step discrete logic, such as
    concepts like place value or carrying over. All it can do is map one data manifold
    X into another manifold Y, assuming the existence of a learnable continuous transform
    from X to Y. A deep learning model can be interpreted as a kind of program, but
    inversely, *most programs can’t be expressed as deep-learning models*. For most
    tasks, either there exists no corresponding neural network of reasonable size
    that solves the task or, even if one exists, it may not be *learnable*: the corresponding
    geometric transform may be far too complex, or there may not be appropriate data
    available to learn it.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 即使看到数百万个示例，深度学习模型也无法最终学习到精确的加法算法的原因是，它只是一个静态的简单、连续的几何变换链，将一个向量空间映射到另一个向量空间。这对于感知模式识别来说是一个很好的匹配，但对于编码任何类型的逐步离散逻辑（例如，如数位或进位这样的概念）来说，匹配得非常差。它所能做的就是将一个数据流形X映射到另一个流形Y，假设存在一个可学习的连续变换从X到Y。深度学习模型可以被解释为一种程序，但反过来，*大多数程序都不能表示为深度学习模型*。对于大多数任务来说，要么不存在合理大小的相应神经网络来解决该任务，要么即使存在，也可能不可学习：相应的几何变换可能过于复杂，或者可能没有适当的数据来学习它。
- en: The risk of anthropomorphizing machine-learning models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将机器学习模型拟人化的风险
- en: Our own understanding of images, sounds, and language is grounded in our sensorimotor
    experience as humans. Machine learning models have no access to such experiences
    and thus can’t understand their inputs in a human-relatable way. By feeding a
    large number of training examples into our models, we get them to learn a geometric
    transform that maps data to human concepts on a specific set of examples, but
    this mapping is a simplistic sketch of the original model in our minds — the one
    developed from our experience as embodied agents. It’s like a dim image in a mirror
    (see figure 19.4). The models you create will take any shortcut available to fit
    their training data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对图像、声音和语言的理解根植于我们作为人类的感觉运动经验。机器学习模型无法访问这样的经验，因此无法以人类相关的方式理解它们的输入。通过向我们的模型输入大量训练示例，我们让它们学习一种几何变换，将数据映射到特定示例集上的人类概念，但这种映射只是我们心中原始模型——即从我们作为具身代理的经验中发展出来的模型——的简单草图。这就像镜子中的模糊图像（见图19.4）。你创建的模型将采取任何可用的捷径来适应它们的训练数据。
- en: '![](../Images/b7716b74d6114efd798acee9fa22164e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7716b74d6114efd798acee9fa22164e.png)'
- en: '[Figure 19.4](#figure-19-4): Current machine-learning models: like a dim image
    in a mirror'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.4](#figure-19-4)：当前机器学习模型：就像镜子中的模糊图像'
- en: 'One real risk with contemporary AI is misinterpreting what deep learning models
    do and overestimating their abilities. A fundamental feature of humans is our
    *theory of mind*: our tendency to project intentions, beliefs, and knowledge on
    the things around us. Drawing a smiley face on a rock suddenly makes it “happy”
    — in our minds. Applied to deep learning, this means that when we train models
    capable of using language, we’re led to believe that the model “understands” the
    contents of the word sequences they generate just the way we do. Then we’re surprised
    when any slight departure from the patterns present in the training data causes
    the model to generate completely absurd answers.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当代人工智能的一个真实风险是误解深度学习模型所做的事情，并高估它们的能力。人类的一个基本特征是我们的**心智理论**：我们倾向于将意图、信念和知识投射到周围的事物上。在石头上画一个笑脸突然让它在我们心中变得“快乐”。应用于深度学习，这意味着当我们训练能够使用语言的模型时，我们会认为模型“理解”它们生成的单词序列的内容，就像我们一样。然后，当我们发现任何与训练数据中存在的模式略有偏离时，模型就会产生完全荒谬的答案，我们会感到惊讶。
- en: 'As a machine learning practitioner, always be mindful of this and never fall
    into the trap of believing that neural networks understand the task they perform
    — they don’t, at least not in a way that would make sense to us. They were trained
    on a different, far narrower task than the one we wanted to teach them: that of
    mapping training inputs to training targets, point by point. Show them anything
    that deviates from their training data, and they will break in absurd ways.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习从业者，始终要意识到这一点，并永远不要陷入相信神经网络理解它们所执行的任务的陷阱——它们不理解，至少不是以对我们有意义的方式。它们被训练在一个与我们想要教给它们的任务截然不同、范围更窄的任务上：将训练输入映射到训练目标，一点一点地。向它们展示任何与它们的训练数据不符的东西，它们就会以荒谬的方式崩溃。
- en: Scale isn’t all you need
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模并不是一切
- en: 'Could we just keep scaling our models to overcome the limitations of deep learning?
    Is *scale* all we need? This has long been the prevailing narrative in the field,
    one that was especially prominent in early 2023, during peak LLM hype. Back then,
    GPT-4 had just been released, and it was essentially a scaled-up version of GPT-3:
    more parameters, more training data. Its significantly improved performance seemed
    to suggest that you could just keep going — that there could be a GPT-5 that would
    simply be more of the same and from which artificial general intelligence (AGI)
    would spontaneously emerge.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以仅仅通过扩大模型规模来克服深度学习的局限性？**规模**是我们所需要的全部吗？这长期以来一直是该领域的普遍观点，特别是在2023年初，在LLM炒作的高峰期尤为突出。当时，GPT-4刚刚发布，本质上只是GPT-3的扩大版：更多的参数，更多的训练数据。其显著提高的性能似乎表明，你可以继续前进——可能会有一个GPT-5，它只是更多相同的东西，并且从它那里会自然地出现通用人工智能（AGI）。
- en: Proponents of this view would point to “scaling laws” as evidence. Scaling laws
    are an empirical relationship observed between the size of a deep learning model
    (as well as the size of its training dataset) and its performance on specific
    tasks. They suggest that increasing the size of a model reliably leads to better
    performance in a predictable manner. But the key thing that scaling law enthusiasts
    are missing is that the benchmarks they’re using to measure “performance” are
    effectively memorization tests, the kind we like to give university students.
    LLMs perform well on these tests by memorizing the answers, and naturally, cramming
    more questions and more answers into the models improves their performance accordingly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种观点的支持者会指出“缩放定律”作为证据。缩放定律是观察到的经验关系，即深度学习模型的大小（以及其训练数据集的大小）与其在特定任务上的性能之间的关系。它们表明，增加模型的大小可以可靠地以可预测的方式提高性能。但缩放定律的爱好者们忽略的关键问题是，他们用来衡量“性能”的基准实际上是记忆测试，这是我们喜欢给大学生出的测试。LLMs通过记忆答案在这些测试中表现良好，而且自然地，将更多问题和答案塞入模型相应地提高了它们的性能。
- en: The reality is that scaling up our models hasn’t led to any progress on the
    issues I’ve listed so far in these pages — inability to adapt to novelty, oversensitivity
    to phrasing, and the inability to infer generalizable programs for reasoning problems
    — because these issues are inherent to curve fitting, the paradigm of deep learning.
    I started pointing out these problems in 2017, and we’re still struggling with
    them today — with models that are now four or five orders of magnitude larger
    and more knowledgeable. We have not made any progress on these problems because
    *the models we’re using are still the same*. They’ve been the same for over seven
    years — they’re still parametric curves fitted to a dataset via gradient descent,
    and they’re still using the Transformer architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，扩大我们的模型并没有解决我在这些页面中列出的任何问题——无法适应新颖性、对措辞过度敏感以及无法推理出推理问题的通用程序——因为这些问题是曲线拟合，深度学习范式的固有属性。我从2017年开始指出这些问题，我们至今仍在努力解决这些问题——现在模型的大小已经增加了四到五个数量级，并且知识更加丰富。我们没有在这些问题上取得任何进展，因为*我们使用的模型仍然是相同的*。它们已经保持了七年多——它们仍然是通过对数据集进行梯度下降拟合的参数曲线，并且它们仍在使用Transformer架构。
- en: 'Scaling up current deep learning techniques by stacking more layers and using
    more training data won’t solve the fundamental problems of deep learning:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠更多层和使用更多训练数据来扩展当前的深度学习技术并不能解决深度学习的根本问题：
- en: Deep-learning models are limited to using interpolative programs they memorize
    at training time. They are not able, on their own, to synthesize brand-new programs
    at inference time to adapt to substantially novel situations.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型局限于使用它们在训练时记忆的内插程序。它们无法在推理时独立地合成全新的程序以适应大量新颖的情况。
- en: Even within known situations, these interpolative programs suffer from generalization
    issues, which lead to oversensitivity to phrasing and confounder features.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使在已知的情况下，这些内插程序也存在着泛化问题，这导致了对措辞的过度敏感和混淆特征。
- en: Deep learning models are limited in what they can represent, and most of the
    programs you may wish to learn can’t be expressed as a continuous geometric morphing
    of a data manifold. This is true in particular of algorithmic reasoning tasks.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型在它们能表示的内容上有所局限，你可能会希望学习的多数程序不能被表示为数据流形连续几何变形。这在算法推理任务中尤其如此。
- en: Let’s take a closer look at what separates biological intelligence from the
    deep learning approach.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看是什么将生物智能与深度学习方法区分开来。
- en: Automatons vs. intelligent agents
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动机与智能体
- en: There are fundamental differences between the straightforward geometric morphing
    from input to output that deep learning models do and the way humans think and
    learn. It isn’t just the fact that humans learn by themselves from embodied experience
    instead of being presented with explicit training examples. The human brain is
    an entirely different beast compared to a differentiable parametric function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型从输入到输出的直接几何变形与人类思考和学习的差异是根本性的。这不仅仅是人类通过具身经验自学，而不是被提供明确的训练示例。与可微分的参数函数相比，人脑是完全不同的生物。
- en: 'Let’s zoom out a little bit and ask, What’s the purpose of intelligence? Why
    did it arise in the first place? We can only speculate, but we can make fairly
    informed speculations. We can start by looking at brains — the organ that produces
    intelligence. Brains are an evolutionary adaptation — a mechanism developed incrementally
    over hundreds of millions of years, via random trial and error guided by natural
    selection, that dramatically expanded the ability of organisms to adapt to their
    environment. Brains originally appeared more than half a billion years ago as
    a way to *store and execute behavioral programs*. Behavioral programs are just
    sets of instructions that make an organism reactive to its environment: “If this
    happens, then do that.” They link the organism’s sensory inputs to its motor controls.
    In the beginning, brains would have served to hardcode behavioral programs (as
    neural connectivity patterns), which would allow an organism to react appropriately
    to its sensory input. This is the way insect brains still work — flies, ants,
    *C. elegans* (see figure 19.5), etc. Because the original “source code” of these
    programs was DNA, which would get decoded as neural connectivity patterns, evolution
    was suddenly able to *search over behavior space* in a largely unbounded way —
    a major evolutionary shift.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微放大视角，问一问，智能的目的是什么？它最初为什么会出现？我们只能进行推测，但我们可以做出相当有根据的推测。我们可以从大脑开始看起——产生智能的器官。大脑是一种进化适应——一种在数亿年间通过自然选择引导的随机尝试和错误逐步发展起来的机制，它极大地扩展了生物适应其环境的能力。大脑最初在超过五十亿年前出现，作为一种存储和执行行为程序的方式。行为程序只是一系列指令，使生物对其环境产生反应：“如果发生这种情况，那么就做那件事。”它们将生物的感觉输入与其运动控制联系起来。最初，大脑的作用可能是硬编码行为程序（作为神经网络连接模式），这样生物就能对其感觉输入做出适当的反应。这就是昆虫大脑仍然工作的方式——苍蝇、蚂蚁、*C.
    elegans*（见图19.5）等。因为这些程序的原初“源代码”是DNA，它会被解码为神经网络连接模式，进化突然能够以在很大程度上不受限制的方式在行为空间中进行搜索——这是一个主要的进化转变。
- en: '![](../Images/bb8c04b7246b281dc75a4e71a9e4adf1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/bb8c04b7246b281dc75a4e71a9e4adf1.png)'
- en: '[Figure 19.5](#figure-19-5): The brain network of the *C. elegans* worm: a
    behavioral automaton “programmed” by natural evolution. Figure created by Emma
    Towlson (from “Network control principles predict neuron function in the Caenorhabditis
    elegans connectome,” Yan et al., *Nature*, Oct. 2017).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.5](#figure-19-5)：秀丽线虫（*C. elegans*）的大脑网络：一个由自然进化“编程”的行为自动机。图由Emma Towlson创作（来自“网络控制原理预测秀丽隐杆线虫连接组中的神经元功能”，Yan等人，《自然》，2017年10月）。'
- en: Evolution was the programmer, and brains were computers carefully executing
    the code evolution gave them. Because neural connectivity is a very general computing
    substrate, the sensorimotor space of all brain-enabled species could suddenly
    start undergoing a dramatic expansion. Eyes, ears, mandibles, 4 legs, 24 legs
    — as long as you have a brain, evolution will kindly figure out for you behavioral
    programs that make good use of these. Brains can handle any modality, or combination
    of modalities, you throw at them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 进化是程序员，大脑是执行进化所提供代码的计算机。由于神经网络连接是一个非常通用的计算基础，所有大脑启用物种的感官运动空间突然开始经历巨大的扩张。眼睛、耳朵、颚、4条腿、24条腿——只要你有大脑，进化就会为你找到利用这些特性的行为程序。大脑可以处理你扔给它的任何模态，或模态的组合。
- en: 'Now, mind you, these early brains weren’t exactly intelligent per se. They
    were very much *automatons*: they would merely execute behavioral programs hardcoded
    in the organism’s DNA. They could only be described as intelligent in the same
    sense that a thermostat is “intelligent.” Or a list-sorting program. Or a trained
    deep neural network (of the artificial kind). This is an important distinction,
    so let’s look at it carefully: What’s the difference between automatons and actual
    intelligent agents?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意，这些早期的头脑本身并不一定具有智能。它们非常像自动机：它们只会执行生物DNA中硬编码的行为程序。它们只能被描述为具有与恒温器“智能”相同的智能。或者是一个列表排序程序。或者是一个训练有素的深度神经网络（人工的那种）。这是一个重要的区别，让我们仔细看看：自动机和真正的智能代理之间有什么区别？
- en: Local generalization vs. extreme generalization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部泛化与极端泛化
- en: The field of AI has long suffered from conflating the notions of *intelligence*
    and *automation*. An automation system (or automaton) is static, crafted to accomplish
    specific things in a specific context — “If this, then that” — while an intelligent
    agent can adapt on the fly to novel, unexpected situations. When an automaton
    is exposed to something that doesn’t match what it was “programmed” to do (whether
    we’re talking about human-written programs, evolution-generated programs, or the
    implicit programming process of fitting a model on a training dataset), it will
    fail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能领域长期以来一直受到将*智能*和*自动化*概念混淆的困扰。一个自动化系统（或自动机）是静态的，被设计在特定环境中完成特定任务——“如果这个，那么那个”——而智能代理可以即时适应新颖、意外的情况。当一个自动机遇到不符合其“编程”去做的事情时（无论是关于人类编写的程序、进化生成的程序，还是将模型拟合到训练数据集上的隐式编程过程），它将失败。
- en: Meanwhile, intelligent agents, like us humans, will use their fluid intelligence
    to find a way forward. How do you tell the difference between a student who has
    memorized the past three years of exam questions but has no understanding of the
    subject and one who actually understands the material? You give them a brand-new
    problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，像我们人类这样的智能代理将利用他们的流动智能找到前进的道路。你如何区分一个只是记住了过去三年考试问题但没有理解学科的学生和一个真正理解材料的学生？你给他们一个全新的问题。
- en: Humans are capable of far more than mapping immediate stimuli to immediate responses
    as a deep network or an insect would. We can assemble on-the-fly complex, abstract
    models of our current situation, of ourselves, and of other people, and can use
    these models to anticipate different possible futures and perform long-term planning.
    We can quickly adapt to unexpected situations and pick up new skills after just
    a little bit of practice.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的能力远不止像深度网络或昆虫那样将即时刺激映射到即时反应。我们能够即时构建关于我们当前情况、我们自己以及他人的复杂、抽象模型，并可以使用这些模型来预测不同的可能未来并执行长期规划。我们能够快速适应意外情况，并在少量练习后掌握新技能。
- en: 'This ability to use *abstraction* and *reasoning* to handle experiences we
    weren’t prepared for is the defining characteristic of human cognition. I call
    it *extreme generalization*: an ability to adapt to novel, never-before-experienced
    situations using little data or even no new data at all. This capability is key
    to the intelligence displayed by humans and advanced animals.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使用*抽象*和*推理*来处理我们没有准备好的经验的能力是人类认知的标志性特征。我称之为*极端泛化*：一种使用少量数据甚至没有新数据就能适应新颖、从未经历过的情境的能力。这种能力是人类和高级动物所展现的智能的关键。
- en: 'This stands in sharp contrast with what automaton-like systems do. A very rigid
    automaton wouldn’t feature any generalization at all; it would be incapable of
    handling anything that it wasn’t precisely told about in advance. A Python dict,
    or a basic question-answering program implemented as hardcoded if-then-else statements
    would fall into this category. Deep nets do slightly better: they can successfully
    process inputs that deviate a bit from what they’re familiar with, which is precisely
    what makes them useful. Our dogs-versus-cats model from chapter 8 could classify
    cat or dog pictures it had not seen before, as long as they were close enough
    to what it was trained on. However, deep nets are limited to what I call *local
    generalization* (see figure 19.6): the mapping from inputs to outputs performed
    by a deep net quickly stops making sense as inputs start deviating from what the
    net saw at training time. Deep nets can only generalize to *known unknowns*, to
    factors of variation that were anticipated during model development and that are
    extensively featured in the training data, such as different camera angles or
    lighting conditions for pet pictures. That’s because deep nets generalize via
    interpolation on a manifold (remember chapter 5): any factor of variation in their
    input space needs to be captured by the manifold they learn. That’s why basic
    data augmentation is so helpful in improving deep net generalization. Unlike humans,
    these models have no ability to improvise in the face of situations for which
    little or no data is available.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这与类似自动机系统的行为形成了鲜明的对比。一个非常僵化的自动机根本不具备任何泛化能力；它无法处理任何它事先没有精确告知的事情。Python字典，或者作为硬编码的if-then-else语句实现的简单问答程序就会属于这一类。深度网络做得稍微好一些：它们可以成功处理与它们熟悉的情况略有偏差的输入，这正是它们有用的地方。我们第8章中的狗与猫模型可以分类它之前没有见过的猫或狗图片，只要它们足够接近训练时的样本。然而，深度网络局限于我所说的*局部泛化*（见图19.6）：当输入开始偏离网络在训练时看到的输入时，深度网络从输入到输出的映射很快就会变得没有意义。深度网络只能泛化到*已知未知*，到在模型开发期间预测到的变化因素，这些因素在训练数据中广泛出现，例如宠物图片的不同角度或光照条件。这是因为深度网络通过在流形上的插值进行泛化（记得第5章）：它们输入空间中的任何变化因素都需要被它们学习的流形所捕捉。这就是为什么基本的数据增强在提高深度网络泛化能力方面非常有帮助。与人类不同，这些模型在面对数据很少或没有数据的情况时，没有任何即兴发挥的能力。
- en: '![](../Images/d7476909b13b29de39c7b82a79798be3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7476909b13b29de39c7b82a79798be3.png)'
- en: '[Figure 19.6](#figure-19-6): Local generalization vs. extreme generalization'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.6](#figure-19-6)：局部泛化与极端泛化'
- en: 'Consider, for instance, the problem of learning the appropriate launch parameters
    to get a rocket to land on the moon. If you used a deep net for this task and
    trained it using supervised learning or reinforcement learning, you’d have to
    feed it tens of thousands or even millions of launch trials: you’d need to expose
    it to a *dense sampling* of the input space for it to learn a reliable mapping
    from input space to output space. In contrast, as humans, we can use our power
    of abstraction to come up with physical models — rocket science — and derive an
    exact solution that will land the rocket on the moon in one or a few trials. Similarly,
    if you developed a deep net controlling a human body and you wanted it to learn
    to safely navigate a city without getting hit by cars, the net would have to die
    many thousands of times in various situations until it could infer that cars are
    dangerous and develop appropriate avoidance behaviors. Dropped into a new city,
    the net would have to relearn most of what it knows. On the other hand, humans
    are able to learn safe behaviors without having to die even once — again, thanks
    to our power of abstract modeling of novel situations.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这样一个问题：学习适当的发射参数，使火箭能够成功着陆在月球上。如果你为这个任务使用深度网络，并使用监督学习或强化学习来训练它，你将不得不给它提供成千上万甚至数百万次的发射试验：你需要让它接触到输入空间的密集采样，以便它能够学习从输入空间到输出空间的可靠映射。相比之下，作为人类，我们可以利用我们的抽象能力来提出物理模型——火箭科学——并推导出一个精确的解决方案，这个方案将使火箭在一次性或几次试验中成功着陆在月球上。同样，如果你开发了一个控制人类身体的深度网络，并且你希望它学会在不会撞到汽车的情况下安全地导航城市，网络将不得不在各种情况下死亡成千上万次，直到它能够推断出汽车是危险的，并发展出适当的避免行为。被投入到一个新的城市中，网络将不得不重新学习它所知道的大部分内容。另一方面，人类能够在不死亡的情况下学会安全的行为——这再次得益于我们抽象建模新情况的能力。
- en: The purpose of intelligence
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能的目的
- en: This distinction between highly adaptable intelligent agents and rigid automatons
    leads us back to brain evolution. Why did brains — originally a mere medium for
    natural evolution to develop behavioral automatons — eventually turn intelligent?
    Like every significant evolutionary milestone, it happened because natural selection
    constraints encouraged it to happen.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高度适应性的智能代理和僵化的自动机之间的区别让我们回到了大脑进化的主题。为什么大脑——最初只是自然进化发展行为自动机的媒介——最终变得智能？像每一个重要的进化里程碑一样，这是由于自然选择的限制鼓励了这一变化的发生。
- en: 'Brains are responsible for behavior generation. If the set of situations an
    organism had to face was mostly static and known in advance, behavior generation
    would be an easy problem: evolution would just figure out the correct behaviors
    via random trial and error and hardcode them into the organism’s DNA. This first
    stage of brain evolution — brains as automatons — would already be optimal. However,
    crucially, as organism complexity — and alongside it, environmental complexity
    — kept increasing, the situations animals had to deal with became much more dynamic
    and more unpredictable. A day in your life, if you look closely, is unlike any
    day you’ve ever experienced and unlike any day ever experienced by any of your
    evolutionary ancestors. You need to be able to face unknown and surprising situations
    constantly. There is no way for evolution to find and hardcode as DNA the sequence
    of behaviors you’ve been executing to successfully navigate your day since you
    woke up a few hours ago. It has to be generated on the fly every day.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑负责行为生成。如果一个生物体必须面对的情况大多是静态的且提前已知，那么行为生成将是一个简单的问题：进化只需通过随机尝试和错误找出正确的行为，并将它们硬编码到生物体的DNA中。大脑进化的这个第一阶段——作为自动机的头脑——就已经是最佳的。然而，关键的是，随着生物体复杂性和与之相伴的环境复杂性的持续增加，动物必须应对的情况变得更加动态和不可预测。如果你仔细观察，你生活中的每一天都不像你以前经历过的任何一天，也不像你的进化祖先们经历过的任何一天。你需要能够不断面对未知和意外的情况。进化无法找到并将你从几个小时前醒来以来成功导航你一天的行为序列硬编码到DNA中。它必须每天即时生成。
- en: The brain, as a good behavior-generation engine, simply adapted to fit this
    need. It optimized for adaptability and generality themselves, rather than merely
    optimizing for fitness to a fixed set of situations. This shift likely occurred
    multiple times throughout evolutionary history, resulting in highly intelligent
    animals in very distant evolutionary branches — apes, octopuses, ravens, and more.
    Intelligence is an answer to challenges presented by complex, dynamic ecosystems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑，作为一个良好的行为生成引擎，只是适应了这一需求。它优化了适应性和通用性本身，而不仅仅是优化对一组固定情况的适应性。这种转变很可能在进化历史的多个时期发生，导致了在非常遥远的进化分支上的高度智能动物——猿类、章鱼、乌鸦等等。智能是对复杂、动态生态系统提出的挑战的回答。
- en: 'That’s the nature of intelligence: it is the ability to efficiently use the
    information at your disposal to produce successful behavior in the face of an
    uncertain, ever-changing future. What Descartes calls “understanding” is the key
    to this remarkable capability: the power to mine your past experience to develop
    modular, reusable abstractions that can be quickly repurposed to handle novel
    situations and achieve extreme generalization.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是智能的本质：它是在不确定、不断变化的未来中，有效地利用你拥有的信息来产生成功行为的能力。笛卡尔所说的“理解”是这种非凡能力的关键：挖掘你过去经验的能力，以开发模块化、可重用的抽象，这些抽象可以快速重新用于处理新情况并实现极端的泛化。
- en: Climbing the spectrum of generalization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 爬升泛化谱系
- en: As a crude caricature, you could summarize the evolutionary history of biological
    intelligence as a slow climb up the *spectrum of generalization*. It started with
    automaton-like brains that could only perform local generalization. Over time,
    evolution started producing organisms capable of increasingly broader generalization
    that could thrive in ever-more complex and variable environments. Eventually,
    in the past few million years — an instant in evolutionary terms — certain hominin
    species started trending toward an implementation of biological intelligence capable
    of extreme generalization, precipitating the start of the Anthropocene and forever
    changing the history of life on Earth.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用一种粗略的讽刺画来说，你可以将生物智能的进化历史总结为一种缓慢的沿着*泛化光谱*的攀登。它始于只能执行局部泛化的类似自动机的头脑。随着时间的推移，进化开始产生能够进行越来越广泛泛化的生物体，这些生物体能够在越来越复杂和多变的环境中生存。最终，在过去的几百万年——在进化术语中是一瞬间——某些人属物种开始趋向于实现能够进行极端泛化的生物智能的实施，从而引发了人类世的开端，永远改变了地球上生命的历史。
- en: The progress of AI over the past 70 years bears striking similarities to this
    evolution. Early AI systems were pure automatons, like the ELIZA chat program
    from the 1960s, or SHRDLU:^([[2]](#footnote-2)), a 1970 AI capable of manipulating
    simple objects from natural language commands. In the 1990s and 2000s, we saw
    the rise of machine learning systems capable of local generalization that could
    deal with some level of uncertainty and novelty. In the 2010s, deep learning further
    expanded the local generalization power of these systems by enabling engineers
    to use much larger datasets and much more expressive models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 过去70年间人工智能的进步与这一演变过程有着惊人的相似之处。早期的AI系统是纯粹的自动机，例如20世纪60年代的ELIZA聊天程序，或者SHRDLU：^([[2]](#footnote-2))，这是一种1970年能够通过自然语言命令操纵简单物体的AI。在1990年代和2000年代，我们见证了能够处理一定程度的不确定性和新颖性的局部泛化机器学习系统的兴起。在2010年代，深度学习通过使工程师能够使用更大的数据集和更具有表现力的模型，进一步扩展了这些系统的局部泛化能力。
- en: Today, we may be on the cusp of the next evolutionary step. We are moving toward
    systems that achieve *broad generalization*, which I define as the ability to
    deal with *unknown unknowns* within a single broad domain of tasks (including
    situations the system was not trained to handle and that its creators could not
    have anticipated). Examples are a self-driving car capable of safely dealing with
    any situation you throw at it or a domestic robot that could pass the “Woz test
    of intelligence” — entering a random kitchen and making a cup of coffee:^([[3]](#footnote-3)).
    By combining deep learning and painstakingly handcrafted abstract models of the
    world, we’re already making visible progress toward these goals.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们可能正处于下一个进化步骤的边缘。我们正在走向能够实现*广泛泛化*的系统，我将这定义为在单个广泛的任务领域内处理*未知未知*的能力（包括系统未接受过训练的情况以及其创造者无法预见的情况）。例如，一辆能够安全应对任何情况的自动驾驶汽车或能够通过“沃兹智力测试”的家庭机器人——进入一个随机的厨房并煮一杯咖啡：^([[3]](#footnote-3))。通过结合深度学习和精心手工制作的抽象模型，我们已经在朝着这些目标取得明显的进展。
- en: 'However, the deep learning paradigm has remained limited to cognitive automation:
    The “intelligence” label in “artificial intelligence” has been a category error.
    It would be more accurate to call our field “artificial cognition,” with “cognitive
    automation” and “artificial intelligence” being two nearly independent subfields
    within it. In this subdivision, AI would be a greenfield where almost everything
    remains to be discovered.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度学习范式仍然局限于认知自动化：在“人工智能”中的“智能”标签是一个分类错误。更准确地说，我们应该称我们的领域为“人工认知”，其中“认知自动化”和“人工智能”是其中的两个几乎独立的子领域。在这个细分中，AI将是一个绿洲，其中几乎一切都还有待发现。
- en: 'Now, I don’t mean to diminish the achievements of deep learning. Cognitive
    automation is incredibly useful, and the way deep learning models are capable
    of automating tasks from exposure to data alone represents an especially powerful
    form of cognitive automation, far more practical and versatile than explicit programming.
    Doing this well is a game changer for essentially every industry. But it’s still
    a long way from human (or animal) intelligence. Our models, so far, can only perform
    local generalization: they map space X to space Y via a smooth geometric transform
    learned from a dense sampling of X-to-Y data points, and any disruption within
    spaces X or Y invalidates this mapping. They can only generalize to new situations
    that stay similar to past data, whereas human cognition is capable of extreme
    generalization, quickly adapting to radically novel situations and planning for
    long-term future situations.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我不打算贬低深度学习的成就。认知自动化非常有用，深度学习模型仅通过数据暴露就能自动化任务的能力，代表了一种特别强大的认知自动化形式，比显式编程更实用、更灵活。做好这一点对于几乎所有行业来说都是一场变革。但它离人类（或动物）的智能还差得很远。到目前为止，我们的模型只能进行局部泛化：它们通过从X到Y的数据点的密集采样中学习到的平滑几何变换，将空间X映射到空间Y，而X或Y中的任何干扰都会使这种映射无效。它们只能泛化到与过去数据相似的新情况，而人类的认知能够进行极端的泛化，快速适应极端新颖的情况，并为长期未来的情况做出规划。
- en: How to build intelligence
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建智能
- en: So far, you’ve learned that there’s a lot more to intelligence than the sort
    of latent manifold interpolation that deep learning does. But what, then, do we
    need to start building real intelligence? What are the core pieces that are currently
    eluding us?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解到，智能远不止深度学习所进行的这种潜在流形插值。那么，我们究竟需要从哪里开始构建真正的智能？目前我们还在逃避的核心要素是什么？
- en: The kaleidoscope hypothesis
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 万花筒假说
- en: Intelligence is the ability to use your past experience (and innate prior knowledge)
    to face novel, unexpected future situations. Now, if the future you had to face
    was *truly novel* — sharing no common ground with anything you’ve seen before
    — you’d be unable to react to it, no matter how intelligent you are.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 智能是利用你的过去经验（以及先天的先验知识）来面对新颖、意外的未来情况的能力。现在，如果你必须面对的未来是*真正新颖的*——与之前所见的一切都没有共同点——无论你多么聪明，你都无法对它做出反应。
- en: Intelligence works because nothing is ever truly without precedent. When we
    encounter something new, we’re able to make sense of it by drawing analogies to
    our past experience and articulating it in terms of the abstract concepts we’ve
    collected over time. A person from the 17th century seeing a jet plane for the
    first time might describe it as a large, loud metal bird that doesn’t flap its
    wings. A car? That’s a horseless carriage. If you’re trying to teach physics to
    a grade schooler, you can explain how electricity is like water in a pipe or how
    spacetime is like a rubber sheet getting distorted by heavy objects.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 智能之所以有效，是因为没有任何事物是完全没有先例的。当我们遇到新事物时，我们能够通过将它们与过去的经验进行类比，并使用我们收集多年的抽象概念来阐述它们，从而理解它们。一个17世纪的人第一次看到喷气式飞机时，可能会描述它为一只大而响亮的金属鸟，它不会拍打翅膀。汽车？那是一种无马的马车。如果你试图向小学生讲解物理，你可以解释电就像水管中的水，或者时空就像被重物扭曲的橡皮膜。
- en: Besides such clear-cut, explicit analogies, we’re constantly making smaller,
    implicit analogies — every second, with every thought. Analogies are how we navigate
    life. Shopping at a new supermarket? You’ll find your way by relating it to similar
    stores you’ve been to. Talking to someone new? They’ll remind you of a few people
    you’ve met before. Even seemingly random patterns, like the shape of clouds, instantly
    evoke in us vivid images — an elephant, a ship, a fish.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这样明确、显式的类比之外，我们还在不断地做出更小、更隐晦的类比——每秒钟，每思考一次。类比是我们导航生活的方式。在一家新的超市购物？你会通过将其与你去过的类似商店联系起来找到自己的路。与新人交谈？他们会让你想起你之前遇到的一些人。甚至看似随机的模式，如云的形状，也会立刻在我们脑海中唤起生动的图像——一头大象，一艘船，一条鱼。
- en: 'These analogies aren’t just in our minds, either: physical reality itself is
    full of isomorphisms. Electromagnetism is analogous to gravity. Animals are all
    structurally similar to each other, due to shared origins. Silica crystals are
    similar to ice crystals. And so on.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类比不仅存在于我们的脑海中：物理现实本身充满了同构。电磁学与重力相似。由于共同的起源，动物在结构上彼此相似。二氧化硅晶体与冰晶体相似。等等。
- en: 'I call this the *kaleidoscope hypothesis*: our experience of the world seems
    to feature incredible complexity and never-ending novelty, but everything in this
    sea of complexity is similar to everything else. The number of *unique atoms of
    meaning* that you need to describe the universe you live in is relatively small,
    and everything around you is a recombination of these atoms: a few seeds, endless
    variation, much like what goes on inside a kaleidoscope, where a few glass beads
    are reflected by a system of mirrors to produce rich, seemingly endless patterns
    (see figure 19.7).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我把这个称为*万花筒假说*：我们对世界的体验似乎具有难以置信的复杂性和永无止境的新颖性，但这个复杂性海洋中的每一件事都与其他事物相似。你需要描述你所处的宇宙的独特*意义原子*的数量相对较小，而你周围的一切都是这些原子的重组：一些种子，无尽的变异，就像万花筒内部发生的事情一样，其中一些玻璃珠被一组镜子反射，产生丰富、看似无尽的图案（见图19.7）。
- en: '![](../Images/70a8b1d2b9cca867dc91b790b56e9ae2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/70a8b1d2b9cca867dc91b790b56e9ae2.png)'
- en: '[Figure 19.7](#figure-19-7): A kaleidoscope produces rich (yet repetitive)
    patterns from just a few beads of colored glass.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.7](#figure-19-7)：万花筒仅从几颗彩色玻璃珠中产生丰富（但重复）的图案。'
- en: 'The essence of intelligence: Abstraction acquisition and recombination'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智力的本质：抽象获取和重组
- en: Intelligence is the ability to mine your experience to identify these atoms
    of meaning that can seemingly be reused across many different situations — the
    core beads of the kaleidoscope. Once extracted, they’re called *abstractions*.
    Whenever you encounter a new situation, you make sense of it by recombining on
    the fly abstractions from your collections, to weave a brand new “model” adapted
    to the situation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 智力是挖掘你的经验以识别这些可以看似在不同情况下重复使用的意义原子——万花筒的核心珠子。一旦提取出来，它们就被称为*抽象*。无论何时你遇到一个新情况，你都会通过即时重新组合你收藏中的抽象来理解它，以编织一个全新的“模型”，适应该情况。
- en: 'This process consists of two key parts:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程包括两个关键部分：
- en: '*Abstraction acquisition* — Efficiently extracting compact, reusable abstractions
    from a stream of experience or data. This involves identifying underlying structures,
    principles, or invariants.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*抽象获取*——有效地从一系列经验或数据中提取紧凑、可重用的抽象。这涉及到识别潜在的结构、原则或不变量。'
- en: '*On-the-fly recombination* — Efficiently selecting and recombining these abstractions
    in novel ways to model new problems and situations, even ones far removed from
    past experience.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*即时重组*——以新颖的方式高效地选择和重组这些抽象来模拟新的问题和情况，甚至那些与以往经验截然不同的情况。'
- en: The emphasis on *efficiency* is crucial. How intelligent you are is determined
    by how efficiently you can acquire good abstractions from limited experience and
    how efficiently you can recombine them to navigate uncertainty and novelty. If
    you need hundreds of thousands of hours of practice to acquire a skill, you are
    not very intelligent. If you need to enumerate every possible move on the chess
    board to find the best one, you are not very intelligent.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对*效率*的强调至关重要。你的智能取决于你从有限的经验中获取良好抽象的效率，以及你如何有效地重组它们以导航不确定性和新颖性。如果你需要数万小时的练习来掌握一项技能，那么你并不聪明。如果你需要列出棋盘上所有可能的走法来找到最佳走法，那么你也不聪明。
- en: 'And that’s the source of the two main issues with the classic deep learning
    paradigm:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是经典深度学习范式中的两个主要问题的来源：
- en: These models are completely missing on-the-fly recombination. They do a decent
    job at acquiring abstractions at training time, via gradient descent, but by design
    they have zero ability to recombine what they know at test time. They behave like
    a static abstract database, limited purely to retrieval. They’re missing half
    of the picture — the most important half.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型完全缺乏即时重组。它们在训练时间通过梯度下降在获取抽象方面做得相当不错，但按照设计，它们在测试时间没有能力重组它们所知道的内容。它们的行为就像一个静态的抽象数据库，仅限于检索。它们遗漏了整个画面的一半——最重要的那一半。
- en: They’re terribly inefficient. Gradient descent requires vast amounts of data
    to distill neat abstractions — many orders of magnitude more data than humans.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们效率极低。梯度下降需要大量的数据来提炼整洁的抽象——比人类多出许多数量级的更多数据。
- en: So how can we move beyond these limitations?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何超越这些限制？
- en: The importance of setting the right target
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置正确目标的重要性
- en: Biological intelligence was the answer to a question asked by nature. Likewise,
    if we want to develop true AI, first, we need to be asking the right questions.
    Ultimately, the capabilities of AI systems reflect the objectives they were designed
    and optimized for.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 生物智能是自然界提出的问题的答案。同样，如果我们想开发真正的AI，首先，我们需要提出正确的问题。最终，AI系统的能力反映了它们被设计和优化的目标。
- en: 'An effect you see constantly in systems design is the *shortcut rule*: if you
    focus on optimizing one success metric, you will achieve your goal, but at the
    expense of everything in the system that wasn’t covered by your success metric.
    You end up taking every available shortcut toward the goal. Your creations are
    shaped by the incentives you give yourself.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统设计中，你经常会看到一种效应，即*捷径规则*：如果你专注于优化一个成功指标，你将实现你的目标，但要以牺牲系统内所有未涵盖在你成功指标中的东西为代价。你最终会采取通往目标的每一个可用的捷径。你的创造物是由你给予自己的激励所塑造的。
- en: 'You see this often in machine learning competitions. In 2009, Netflix ran a
    challenge that promised a $1 million prize to the team that would achieve the
    highest score on a movie recommendation task. It ended up never using the system
    created by the winning team because it was way too complex and compute intensive.
    The winners had optimized for prediction accuracy alone — what they were incentivized
    to achieve — at the expense of every other desirable characteristic of the system:
    inference cost, maintainability, explainability. The shortcut rule holds true
    in most Kaggle competitions as well — the models produced by Kaggle winners can
    rarely, if ever, be used in production.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你在机器学习竞赛中经常看到这种情况。2009年，Netflix举办了一场挑战赛，承诺向实现电影推荐任务最高分数的团队提供100万美元的奖金。结果，他们从未使用获胜团队创建的系统，因为它过于复杂且计算密集。获胜者只优化了预测准确性——他们被激励去实现的目标——而牺牲了系统其他所有可取的特性：推理成本、可维护性、可解释性。在大多数Kaggle竞赛中，捷径规则也是成立的——Kaggle获胜者产生的模型很少，如果有的话，可以在生产中使用。
- en: 'The shortcut rule has been everywhere in AI over the past few decades. In the
    1970s, psychologist and computer science pioneer Allen Newell, concerned that
    his field wasn’t making any meaningful progress toward a proper theory of cognition,
    proposed a new grand goal for AI: chess playing. The rationale was that playing
    chess, in humans, seemed to involve — perhaps even require — capabilities such
    as perception, reasoning and analysis, memory and study from books, and so on.
    Surely, if we could build a chess-playing machine, it would have to feature these
    attributes as well. Right?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，*捷径规则*在AI领域无处不在。在20世纪70年代，心理学家和计算机科学先驱艾伦·纽厄尔担心他的领域没有在向正确的认知理论取得任何有意义的进展，因此为AI提出了一个新的宏伟目标：下棋。其理由是，在人类中，下棋似乎涉及到——也许甚至需要——诸如感知、推理和分析、记忆和从书籍中学习等能力。当然，如果我们能构建一个下棋的机器，它也必须具备这些属性。对吧？
- en: 'Over two decades later, the dream came true: in 1997, IBM’s Deep Blue beat
    Gary Kasparov, the best chess player in the world. Researchers had then to contend
    with the fact that creating a chess-champion AI had taught them little about human
    intelligence. The A-star algorithm at the heart of Deep Blue wasn’t a model of
    the human brain and couldn’t generalize to tasks other than similar board games.
    It turned out it was easier to build an AI that could only play chess than to
    build an artificial mind — so that’s the shortcut researchers took.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 二十多年后，梦想成真：1997年，IBM的Deep Blue击败了世界最佳棋手加里·卡斯帕罗夫。那时，研究人员不得不面对这样一个事实：创建一个棋类冠军AI并没有让他们对人类智能有太多了解。Deep
    Blue的核心算法A*并不是人类大脑的模型，也不能推广到除类似棋类游戏之外的任务。结果证明，构建一个只能下棋的AI比构建一个人工智能大脑要容易得多——这就是研究人员采取的捷径。
- en: So far, *the driving success metric of the field of AI has been to solve specific
    tasks*, from chess to Go, from MNIST classification to ImageNet, from high school
    math tests to the bar exam. Consequently, the history of the field has been defined
    by a series of “successes” where *we figured out how to solve these tasks without
    featuring any intelligence*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，AI领域的*驱动成功指标一直是解决特定任务*，从棋类到围棋，从MNIST分类到ImageNet，从高中数学考试到律师资格考试。因此，该领域的历史是由一系列“成功”定义的，在这些“成功”中，*我们找到了解决这些任务的方法，而没有涉及任何智能*。
- en: If that sounds like a surprising statement, keep in mind that human-like intelligence
    isn’t characterized by skill at any particular task — rather, it is the ability
    to adapt to novelty to efficiently acquire new skills and master never-before-seen
    tasks. By fixing the task, you make it possible to provide an arbitrarily precise
    description of what needs to be done — either via hardcoding human-provided knowledge
    or by supplying humongous amounts of data. You make it possible for engineers
    to “buy” more skill for their AI by just adding data or adding hardcoded knowledge,
    without increasing the generalization power of the AI (see figure 19.8). If you
    have near-infinite training data, even a very crude algorithm like nearest-neighbor
    search can play video games with superhuman skill. Likewise, if you have a near-infinite
    amount of human-written if-then-else statements — that is, until you make a small
    change to the rules of the game, the kind a human could adapt to instantly — that
    will require the unintelligent system to be retrained or rebuilt from scratch.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来像是一个令人惊讶的陈述，请记住，类似人类的智能并不是由任何特定任务的技能所定义——相反，它是指适应新事物以高效获取新技能和掌握从未见过的任务的能力。通过固定任务，你使得提供对需要完成的事情的任意精确描述成为可能——无论是通过硬编码人类提供的知识，还是通过提供巨大的数据量。你使得工程师只需添加数据或添加硬编码的知识，就能“购买”更多技能给他们的AI，而不需要增加AI的泛化能力（见图19.8）。如果你有近乎无限的训练数据，即使是像最近邻搜索这样非常粗糙的算法也能以超人的技能玩电子游戏。同样，如果你有近乎无限的人类编写的if-then-else语句——也就是说，直到你对游戏规则进行微小改变，这种改变是人类可以立即适应的——这将要求无智能的系统重新训练或从头开始重建。
- en: '![](../Images/888660ebed8c3c6535cba344ae288fb5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/888660ebed8c3c6535cba344ae288fb5.png)'
- en: '[Figure 19.8](#figure-19-8): A low-generalization system can achieve arbitrary
    skill at a fixed task given unlimited task-specific information.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.8](#figure-19-8)：一个低泛化能力的系统在给定无限的任务特定信息的情况下，可以在固定任务上实现任意技能。'
- en: In short, by fixing the task, you remove the need to handle uncertainty and
    novelty, and since the nature of intelligence is the ability to handle uncertainty
    and novelty, you’re effectively removing the need for intelligence. And because
    it’s always easier to find a unintelligent solution to a specific task than to
    solve the general problem of intelligence, that’s the shortcut you will take 100%
    of the time. Humans can use their general intelligence to acquire skills at any
    new task, but in reverse, there is no path from a collection of task-specific
    skills to general intelligence.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，通过固定任务，你消除了处理不确定性和新事物的需求，而智能的本质就是处理不确定性和新事物，因此你实际上消除了对智能的需求。而且，由于找到特定任务的低智能解决方案总是比解决智能的普遍问题更容易，所以这就是你100%会采取的捷径。人类可以用他们的通用智能来学习任何新任务的技能，但反过来，从一系列特定任务的技能到通用智能没有路径。
- en: 'A new target: On-the-fly adaptation'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新的目标：即时适应
- en: 'To make AI actually intelligent and give it the ability to deal with the incredible
    variability and ever-changing nature of the real world, first, we need to move
    away from seeking to achieve *task-specific skill* and, instead, start targeting
    generalization power itself. We need new metrics of progress that will help us
    develop increasingly intelligent systems: metrics that will point in the right
    direction and that will give us an actionable feedback signal. As long as we set
    our goal to be “create a model that solves task X,” the shortcut rule will apply,
    and we’ll end up with a model that does X, period.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要使AI真正智能并赋予它处理现实世界难以置信的多样性和不断变化性质的能力，首先，我们需要摆脱寻求实现*特定任务技能*的愿望，转而开始针对泛化能力本身。我们需要新的进展指标，这将帮助我们开发越来越智能的系统：指标将指引正确的方向，并给我们一个可操作的反馈信号。只要我们设定的目标是“创建一个解决任务X的模型”，捷径规则就会适用，我们最终会得到一个只做X的模型。
- en: 'In my view, intelligence can be precisely quantified as an *efficiency ratio*:
    the conversion ratio between the *amount of relevant information* you have available
    about the world (which could be either past experience or innate prior knowledge)
    and your *future operating area*, the set of novel situations where you will be
    able to produce appropriate behavior (you can view this as your *skill set*).
    A more intelligent agent will be able to handle a broader set of future tasks
    and situations using a smaller amount of past experience. To measure such a ratio,
    you just need to fix the information available to your system — its experience
    and its prior knowledge — and measure its performance on a set of reference situations
    or tasks that are known to be sufficiently different from what the system has
    had access to. Trying to maximize this ratio should lead you toward intelligence.
    Crucially, to avoid cheating, you’re going to need to make sure to test the system
    only on tasks it wasn’t programmed or trained to handle — in fact, you need tasks
    that the *creators of the system could not have anticipated*.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，智能可以精确地量化为一个*效率比率*：你关于世界的相关信息量（这可能是过去的经验或先天的先验知识）与你的*未来操作区域*之间的转换比率，即你将能够产生适当行为的新的情境集合（你可以将其视为你的*技能集*）。一个更智能的代理将能够使用更少的过去经验处理更广泛的未来任务和情境。为了测量这个比率，你只需要固定你系统可用的信息——它的经验和它的先验知识——并测量它在一系列已知与系统所接触到的内容足够不同的参考情境或任务上的表现。试图最大化这个比率应该会引导你走向智能。关键的是，为了避免作弊，你需要确保只在系统没有被编程或训练来处理的任务上对其进行测试——实际上，你需要的是系统*创造者无法预料的*任务。
- en: In 2018 and 2019, I developed a benchmark dataset called the *Abstraction &
    Reasoning Corpus for Artificial General Intelligence (ARC-AGI)*:^([[4]](#footnote-4))
    that seeks to capture this definition of intelligence. ARC-AGI is meant to be
    approachable by both machines and humans, and it looks very similar to human IQ
    tests, such as Raven’s progressive matrices. At test time, you’ll see a series
    of “tasks.” Each task is explained via three or four “examples” that take the
    form of an input grid and a corresponding output grid (see figure 19.9). You’ll
    then be given a brand-new input grid, and you’ll have three tries to produce the
    correct output grid, before moving on to the next task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年和2019年，我开发了一个名为*抽象与推理语料库用于通用人工智能（ARC-AGI）*的基准数据集（^[[4]](#footnote-4)），旨在捕捉这种智能的定义。ARC-AGI旨在使机器和人类都能接近，它看起来非常类似于人类智商测试，例如拉文渐进矩阵测试。在测试时，你会看到一系列“任务”。每个任务都通过三到四个“示例”进行解释，这些示例以输入网格和相应的输出网格的形式出现（见图19.9）。然后你会得到一个全新的输入网格，你将有三次机会产生正确的输出网格，之后才能进行下一个任务。
- en: '![](../Images/9659a2add458e3a593bbcf376baaf32f.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9659a2add458e3a593bbcf376baaf32f.png)'
- en: '[Figure 19.9](#figure-19-9): An ARC-AGI task: the nature of the task is demonstrated
    by a couple of input-output pair examples. Provided with a new input, you must
    construct the corresponding output.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.9](#figure-19-9)：一个ARC-AGI任务：任务性质通过几个输入-输出对示例进行展示。给定一个新的输入，你必须构建相应的输出。'
- en: 'Compared to IQ tests, two things are unique about ARC-AGI. First, ARC seeks
    to measure generalization power by only testing you on tasks you’ve never seen
    before. That means that ARC-AGI is *a game you can’t practice for*, at least in
    theory: the tasks you will get tested on will have their own unique logic that
    you will have to understand on the fly. You can’t just memorize specific strategies
    from past tasks.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与智商测试相比，ARC-AGI有两个独特之处。首先，ARC通过只测试你从未见过的任务来寻求衡量泛化能力。这意味着ARC-AGI是一个你无法练习的游戏，至少在理论上是这样：你将要接受测试的任务将具有其独特的逻辑，你必须即时理解。你不能只是记住过去任务中的特定策略。
- en: In addition, ARC-AGI tries to control for the *prior knowledge* that you bring
    to the test. You never approach a new problem entirely from scratch — you bring
    to it preexisting skills and information. ARC-AGI makes the assumption that all
    test takers should start from the set of knowledge priors, called *Core Knowledge
    priors*, which represent the knowledge systems humans are born with. Unlike an
    IQ test, ARC-AGI tasks will never involve acquired knowledge, like English sentences,
    for instance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ARC-AGI试图控制你在测试中带来的*先验知识*。你永远不会完全从头开始接近一个新问题——你带着先前的技能和信息接近它。ARC-AGI假设所有测试者都应该从一组称为*核心知识先验*的知识集合开始，这代表了人类天生具有的知识系统。与智商测试不同，ARC-AGI任务永远不会涉及获得的知识，例如英语句子等。
- en: ARC Prize
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ARC奖
- en: In 2024, to accelerate progress toward AI systems capable of the kind of fluid
    abstraction and reasoning measured by ARC-AGI, I partnered with Mike Knoop to
    establish the nonprofit ARC Prize Foundation. The foundation runs a yearly competition,
    with a substantial prize pool (over $1 million in its 2024 iteration) to incentivize
    researchers to develop AI that can solve ARC-AGI and thus display genuine fluid
    intelligence.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年，为了加速向能够进行类似ARC-AGI所测量的那种流畅抽象和推理的人工智能系统迈进，我与Mike Knoop合作成立了非营利性的ARC Prize基金会。该基金会每年举办一次竞赛，奖金池丰厚（2024年的版本超过100万美元）以激励研究人员开发能够解决ARC-AGI并因此展现真正流畅智能的人工智能。
- en: The ARC-AGI benchmark has proven remarkably resistant to the prevailing deep
    learning scaling paradigm. Most other benchmarks have saturated quickly in the
    age of LLMs. That’s because they can be hacked via memorization, whereas ARC-AGI
    is designed to be resistant to it. From 2019, when ARC-AGI was first released,
    to 2025, base LLMs underwent a roughly 50,000× scale-up — from, say, GPT-2 (2019)
    to GPT-4.5 (2025), but their performance on the 2019 version of ARC-AGI only went
    from 0% to around 10%. Given that you, reader, would easily score above 95%, that
    isn’t very good.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ARC-AGI基准对主流的深度学习扩展范式表现出惊人的抵抗力。在LLM时代，大多数其他基准很快就已经饱和。那是因为它们可以通过记忆来破解，而ARC-AGI被设计成对此具有抵抗力。从2019年ARC-AGI首次发布到2025年，基础LLM经历了大约50,000倍的扩展——从GPT-2（2019）到GPT-4.5（2025），但它们在2019版ARC-AGI上的表现仅从0%上升到大约10%。考虑到读者您很容易就能得到95%以上的分数，这并不好。
- en: If you scale up your system by 50,000× and you’re still not making meaningful
    progress, that’s like a big warning sign telling you that you need to try new
    ideas. Simply making models bigger or training them on more data has not unlocked
    the kind of fluid intelligence that ARC-AGI requires. ARC-AGI was clearly showing
    that on-the-fly recombination capabilities are necessary to tackle reasoning.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将你的系统扩展了50,000倍，但你仍然没有取得有意义的进展，那就像一个巨大的警告信号告诉你需要尝试新的想法。仅仅使模型更大或用更多数据训练它们并没有解锁ARC-AGI所需的流畅智能。ARC-AGI明显表明，即时重组能力是解决推理所必需的。
- en: The test-time adaptation era
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试时自适应时代
- en: 'In 2024, everything changed. That year saw a major narrative shift — one that
    was partly catalyzed by ARC Prize. The prevailing “Scale is all you need” story
    that was a bedrock dogma of 2023 started giving way to “Actually, we need on-the-fly
    recombination.” The results of the competition, announced in December 2024, were
    illuminating: the leading solutions did not emerge from simply scaling existing
    deep learning architectures. They all used some form of test-time adaptation (TTA)
    — either test-time search or test-time training.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年，一切发生了改变。那一年见证了主要叙事的转折——部分是由ARC Prize催化的。2023年的主流“规模就是一切”的故事，曾是那个时代的基石教条，开始让位于“实际上，我们需要即时重组”。2024年12月宣布的竞赛结果令人启迪：领先的解决方案并非仅仅通过扩展现有的深度学习架构产生。它们都使用了某种形式的测试时自适应（TTA）——要么是测试时搜索，要么是测试时训练。
- en: TTA refers to methods where the AI system performs active reasoning or learning
    during the test itself, using the specific problem information provided — the
    key component that was missing from the classic deep learning paradigm.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TTA指的是AI系统在测试过程中进行主动推理或学习的方法，使用特定问题信息——这是经典深度学习范式所缺失的关键组件。
- en: 'There are several ways to implement test-time adaptation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以实现测试时自适应：
- en: '*Test-time training* — The model adjusts some of its parameters based on the
    examples given in the test task, using gradient descent.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试时训练*——模型根据测试任务中给出的示例调整其部分参数，使用梯度下降。'
- en: '*Search methods* — The system searches through many possible reasoning steps
    or potential solutions at test time to find the best one. This could be done in
    natural language (chain-of-thought synthesis) or in a space of symbolic, verifiable
    programs (program synthesis).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索方法*——系统在测试时搜索许多可能的推理步骤或潜在解决方案，以找到最佳方案。这可以是自然语言（思维链合成）或在一个符号、可验证的程序空间中（程序综合）。'
- en: These TTA approaches allow AI systems to be more flexible and handle novelty
    better than static models. Every single top entry in ARC Prize 2024 used them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些TTA方法使AI系统更加灵活，并能比静态模型更好地处理新颖性。ARC Prize 2024的每个顶级参赛作品都使用了它们。
- en: Shortly following the competition’s conclusion, in late December 2024, OpenAI
    previewed its o3 test-time reasoning model and used ARC-AGI to showcase its unprecedented
    capabilities. Using considerable test-time compute resources, this model achieved
    scores of 76% at a cost of about $200 per task, and 88% at a cost of over $20,000
    per task, surpassing the nominal human baseline. For the very first time, we were
    seeing an AI model that showed signs of genuine fluid intelligence. This breakthrough
    opened the floodgates of a new wave of interest and investment in similar techniques
    — the test-time adaptation era had begun. Importantly, ARC-AGI was one of the
    only benchmarks at the time that provided a clear signal that a major paradigm
    shift was underway.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛结束不久，2024年12月底，OpenAI预览了其o3测试时推理模型，并使用ARC-AGI展示了其前所未有的能力。利用大量的测试时计算资源，该模型以每项任务约200美元的成本实现了76%的得分，以每项任务超过20,000美元的成本实现了88%的得分，超过了名义上的人类基准。我们第一次看到了一个显示出真正流畅智能迹象的人工智能模型。这一突破打开了类似技术的新一波兴趣和投资的闸门——测试时适应时代开始了。重要的是，ARC-AGI是当时唯一提供明确信号的基准之一，表明正在进行一场重大的范式转变。
- en: ARC-AGI 2
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ARC-AGI 2
- en: Does that mean AGI is solved? Was o3 as intelligent as a human?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着AGI已经被解决了？o3的智能是否与人类相当？
- en: Not quite. First, while o3’s performance was a landmark achievement, it came
    at a tremendous cost — tens of thousands of dollars of compute per ARC-AGI puzzle.
    Intelligence isn’t just about capability; it’s fundamentally about efficiency.
    Brute-forcing the solution space given enormous compute is a shortcut that makes
    all kinds of tasks possible without requiring intelligence. In principle, you
    could even solve ARC-AGI by simply walking down the tree of every possible solution
    program and testing each one until you find one that works on the demonstration
    pairs. The o3 results, impressive as they were, felt more like cracking a code
    with a supercomputer than a display of nimble, human-like fluid reasoning. The
    entire point of intelligence is to achieve results with the least amount of resources
    possible.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还不尽然。首先，虽然o3的表现是一个里程碑式的成就，但它付出了巨大的代价——每个ARC-AGI谜题的计算成本高达数万美元。智能不仅仅是关于能力；它本质上关乎效率。在巨大的计算能力下强行搜索解决方案空间是一种捷径，它使得各种任务成为可能，而不需要智能。原则上，你甚至可以通过简单地遍历每个可能的解决方案程序树并测试每一个，直到找到在演示对中能工作的一个来解决问题ARC-AGI。尽管o3的结果令人印象深刻，但它们更像是用超级计算机破解代码，而不是展示灵活、类似人类的流畅推理。智能的整个目的就是以尽可能少的资源实现结果。
- en: 'Second, we found that o3 was still stumped by many tasks that humans found
    very easy (like the one in figure 19.10). This strongly suggests that o3 wasn’t
    quite human-level yet. Here’s the thing — the 2019 version of ARC-AGI was intended
    to be easy. It was essentially a binary test of fluid intelligence: either you
    have no fluid intelligence, like all base LLMs, in which case you score near zero,
    or you do display some genuine fluid intelligence, in which case you immediately
    score extremely high, like any human — or o3. There wasn’t much room in between.
    It was clear that the benchmark needed to evolve alongside the AI capabilities
    it was designed to measure. There was a need for a new ARC-AGI version that was
    less brute-forcible and that could better differentiate between systems possessing
    varying levels of fluid reasoning ability, up to human-level fluid intelligence.
    Good news: we had been working on one since 2022.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们发现o3仍然被许多人类认为非常简单（如图19.10所示）的任务难住了。这强烈表明o3还没有达到人类水平。这里的关键是，2019版本的ARC-AGI旨在简单。它本质上是对流体智能的二进制测试：要么你没有流体智能，就像所有基础LLM一样，在这种情况下你的得分接近零，要么你确实表现出一些真正的流体智能，在这种情况下你将立即获得极高的分数，就像任何人类——或者o3一样。中间没有太多空间。很明显，基准需要随着它旨在衡量的AI能力的发展而发展。需要一个新的ARC-AGI版本，它不那么容易强行解决，并且能够更好地区分具有不同水平流体推理能力的系统，直至达到人类水平的流体智能。好消息是：我们自2022年以来一直在研究一个。
- en: '![](../Images/9077c542d7115d9d9680d5a0e76a3f51.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9077c542d7115d9d9680d5a0e76a3f51.png)'
- en: '[Figure 19.10](#figure-19-10): Example of a task that couldn’t be solved by
    o3 on the highest compute settings (over $20,000 per task)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.10](#figure-19-10)：o3在最高计算设置下（每项任务超过20,000美元）无法解决的问题示例'
- en: And so, in March 2025, the ARC Prize Foundation introduced ARC-AGI-2\. It kept
    the exact same format as the first version but significantly improved the task
    content. The new iteration was designed to raise the bar, incorporating tasks
    that demand more complex reasoning chains and are inherently more resistant to
    exhaustive search methods. The goal was to create a benchmark where computational
    efficiency becomes a more critical factor for success, pushing systems toward
    more genuinely intelligent, efficient strategies rather than simply exploring
    billions of possibilities. While most ARC-AGI-1 tasks could be solved almost instantaneously
    by a human without requiring much cognitive effort, all tasks in ARC-AGI-2 require
    some amount of deliberate thinking (see figure 19.11) — for instance, the average
    time for task completion among human test takers in our experiments was 5 minutes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在2025年3月，ARC Prize Foundation推出了ARC-AGI-2。它保留了与第一版完全相同的格式，但显著提高了任务内容。新的迭代旨在提高标准，包括需要更复杂推理链和本质上更难以穷举搜索方法的任务。目标是创建一个基准，其中计算效率成为成功的关键因素，推动系统走向更真实、更有效的策略，而不仅仅是探索数十亿种可能性。虽然大多数ARC-AGI-1任务几乎可以立即由人类解决，而不需要太多认知努力，但ARC-AGI-2中的所有任务都需要一定程度的深思熟虑（见图19.11）——例如，在我们实验中人类测试者完成任务的平均时间是5分钟。
- en: '![](../Images/b57cb6381b4a41c9cf47ade9836d89bc.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b57cb6381b4a41c9cf47ade9836d89bc.png)'
- en: '[Figure 19.11](#figure-19-11): Typical ARC-AGI-1 task (left) vs. typical ARC-AGI-2
    task (right)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.11](#figure-19-11)：典型的ARC-AGI-1任务（左）与典型的ARC-AGI-2任务（右）'
- en: 'The initial AI testing results on ARC-AGI 2 were sobering: even o3 struggled
    significantly with this new set of challenges, its scores plummeting back into
    the low double digits when constrained to reasonable computational budgets. As
    for base LLMs? Their performance on ARC-AGI-2 was effectively back at 0% — fitting,
    as base LLMs don’t possess fluid intelligence. The challenge of building AI with
    truly efficient, human-like fluid intelligence is still far from solved. We’re
    going to need something beyond current TTA techniques.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在ARC-AGI 2上的初始AI测试结果令人沮丧：即使是o3在与这一新挑战的斗争中也遇到了重大困难，其分数在合理计算预算限制下暴跌至个位数以下。至于基础LLM呢？它们在ARC-AGI-2上的表现实际上回到了0%——这是合适的，因为基础LLM没有流体智力。构建具有真正高效、类似人类流体智力的AI的挑战仍然远未解决。我们需要超越当前TTA技术的某种东西。
- en: 'The missing ingredients: Search and symbols'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺少的要素：搜索和符号
- en: 'What would it take to fully solve ARC-AGI, in particular version 2? Hopefully,
    this challenge will get you thinking. That’s the entire point of ARC-AGI: to give
    you a goal of a different kind, that will nudge you in a new direction — hopefully,
    a productive direction. Now, let’s take a quick look at the key ingredients you’re
    going to need if you want to answer the call.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全解决ARC-AGI，特别是版本2，需要什么？希望这个挑战能让你思考。这正是ARC-AGI的全部目的：给你一个不同类型的目标，这将推动你走向新的方向——希望是一个富有成效的方向。现在，让我们快速看一下如果你想要回应这个召唤，你需要的关键要素。
- en: 'I’ve said that intelligence consists of two components: *abstraction acquisition*
    and *abstraction recombination*. They are tightly coupled — what *kind* of abstractions
    you manipulate determines how, and how well, you can recombine them. Deep learning
    models only manipulate abstractions stored via parametric curves, fitted via gradient
    descent. Could there be a better way?'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我说过，智力由两个组成部分组成：**抽象获取**和**抽象重组**。它们紧密相连——你操作的是哪种类型的抽象决定了你如何以及如何有效地重组它们。深度学习模型只操作通过参数曲线存储的抽象，通过梯度下降拟合。是否可能有更好的方法？
- en: The two poles of abstraction
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽象的两个极端
- en: Abstraction acquisition starts with *comparing things to one another*. Crucially,
    there are two distinct ways to compare things, from which arise two different
    kinds of abstraction and two modes of thinking, each better suited to a different
    kind of problem. Together, these two poles of abstraction form the basis for all
    of our thoughts.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象获取始于**相互比较**。关键的是，有两种不同的比较方式，由此产生了两种不同的抽象类型和两种不同的思维方式，每种方式更适合解决不同类型的问题。这两个抽象的极端共同构成了我们所有思想的基础。
- en: The first way to relate things to each other is *similarity comparison*, which
    gives rise to *value-centric analogies*. The second way is *exact structural match*,
    which gives rise to *program-centric analogies* (or structure-centric analogies).
    In both cases, you start from *instances* of a thing, and you merge together related
    instances to produce an *abstraction* that captures the common elements of the
    underlying instances. What varies is how you tell that two instances are related
    and how you merge instances into abstractions. Let’s take a close look at each
    type.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将事物相互关联的第一种方式是*相似性比较*，这产生了以*价值为中心的类比*。第二种方式是*精确的结构匹配*，这产生了以*程序为中心的类比*（或以结构为中心的类比）。在这两种情况下，你都是从某个事物的一个*实例*开始，并将相关的实例合并在一起，以产生一个*抽象*，这个抽象能够捕捉到潜在实例的共同元素。变化的是你如何判断两个实例之间的关系，以及你如何将实例合并到抽象中。让我们逐一仔细看看每种类型。
- en: Value-centric analogy
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 价值中心类比
- en: 'Let’s say you come across a number of different beetles in your backyard, belonging
    to multiple species. You’ll notice similarities between them. Some will be more
    similar to one another, and some will be less similar: the notion of similarity
    is implicitly a smooth, continuous *distance function* that defines a latent manifold
    where your instances live. Once you’ve seen enough beetles, you can start clustering
    more similar instances together and merging them into a set of *prototypes* that
    captures the shared visual features of each cluster (figure 19.12). These prototypes
    are abstract: they don’t look like any specific instance you’ve seen, although
    they encodes properties that are common across all of them. When you encounter
    a new beetle, you won’t need to compare it to every single beetle you’ve seen
    before to know what to do with it. You can simply compare it to your handful of
    prototypes to find the closest prototype — the beetle’s *category* — and use it
    to make useful predictions: Is the beetle likely to bite you? Will it eat your
    apples?'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在后院看到许多不同的甲虫，属于多个物种。你会注意到它们之间的相似性。有些会彼此更相似，有些则不那么相似：相似性的概念隐含着一个平滑的、连续的*距离函数*，它定义了一个潜在流形，你的实例就生活在这个流形上。一旦你看过足够的甲虫，你就可以开始将更相似的实例聚在一起，并将它们合并成一个*原型*集合，这个集合能够捕捉到每个集群共享的视觉特征（图19.12）。这些原型是抽象的：它们看起来不像你见过的任何特定实例，尽管它们编码了它们共有的属性。当你遇到一个新的甲虫时，你不需要将它与你之前看到的每一个甲虫进行比较，就知道如何处理它。你只需将它与你手中的几个原型进行比较，找到最接近的原型——甲虫的*类别*——然后用它做出有用的预测：甲虫可能会咬你吗？它会吃你的苹果吗？
- en: '![](../Images/3d23d2d57a0d3e7fe97a45187ff3ca68.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3d23d2d57a0d3e7fe97a45187ff3ca68.png)'
- en: '[Figure 19.12](#figure-19-12): Value-centric analogy relates instances via
    a continuous notion of similarity to obtain abstract prototypes.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.12](#figure-19-12)：价值中心类比通过连续的相似性概念将实例联系起来，以获得抽象原型。'
- en: Does this sound familiar? It’s pretty much a description of what unsupervised
    machine learning (such as the K-means clustering algorithm) does. In general,
    all of modern machine learning, unsupervised or not, works by learning latent
    manifolds that describe a space of instances, encoded via prototypes. (Remember
    the ConvNet features you visualized in chapter 10? They were visual prototypes.)
    Value-centric analogy is the kind of analogy-making that enables deep learning
    models to perform local generalization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来熟悉吗？这几乎是对无监督机器学习（如K-means聚类算法）的描述。一般来说，所有现代机器学习，无论是监督还是无监督，都是通过学习潜在流形来工作的，这些流形描述了一个实例空间，并通过原型进行编码。（还记得第10章中可视化的ConvNet特征吗？它们是视觉原型。）以价值为中心的类比是那种能够使深度学习模型进行局部泛化的类比制作方式。
- en: It’s also what many of your own cognitive abilities run on. As a human, you
    perform value-centric analogies all the time. It’s the type of abstraction that
    underlies *pattern recognition*, *perception*, and *intuition*. If you can do
    a task without thinking about it, you’re heavily relying on value-centric analogies.
    If you’re watching a movie and you start subconsciously categorizing the different
    characters into “types,” that’s value-centric abstraction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是许多你自己的认知能力所依赖的。作为一个人类，你一直在进行以价值为中心的类比。这是*模式识别*、*感知*和*直觉*背后的抽象类型。如果你能不思考就能完成任务，你很大程度上是在依赖以价值为中心的类比。如果你在看电影，并开始无意识地将不同的角色分类为“类型”，那是以价值为中心的抽象。
- en: Program-centric analogy
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序中心类比
- en: 'Crucially, there’s more to cognition than the kind of immediate, approximate,
    intuitive categorization that value-centric analogy enables. There’s another type
    of abstraction-generation mechanism, slower, exact, deliberate: program-centric
    (or structure-centric) analogy.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，认知不仅仅是价值中心类比所允许的那种直接、近似、直觉的分类。还有一种抽象生成机制，较慢、精确、深思熟虑：程序中心（或结构中心）类比。
- en: 'In software engineering, you often write different functions or classes that
    seem to have a lot in common. When you notice these redundancies, you start asking,
    Could there be a more abstract function that performs the same job that could
    be reused twice? Could there be an abstract base class that both of your classes
    could inherit from? The definition of abstraction you’re using here corresponds
    to program-centric analogy. You’re not trying to compare your classes and functions
    by *how similar* they look, the way you’d compare two human faces, via an implicit
    distance function. Rather, you’re interested in whether there are *parts* of them
    that have *exactly the same structure*. You’re looking for what is called a *subgraph
    isomorphism* (see figure 19.13): programs can be represented as graphs of operators,
    and you’re trying to find subgraphs (program subsets) that are exactly shared
    across your different programs.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，你经常编写看起来有很多共同点的不同函数或类。当你注意到这些冗余时，你开始思考，是否有一个更抽象的函数可以执行相同的工作，并且可以被重复使用两次？是否有一个抽象的基类，你的两个类都可以从中继承？你在这里使用的抽象定义对应于程序中心类比。你并不是试图通过它们看起来**多么相似**来比较你的类和函数，就像你通过隐含的距离函数比较两个人的脸一样。相反，你感兴趣的是它们是否有**部分**具有**完全相同的结构**。你正在寻找所谓的**子图同构**（见图19.13）：程序可以用操作符的图来表示，你正在尝试找到在不同程序中完全共享的子图（程序子集）。
- en: '![](../Images/d1ce12ea5f821fdbae5cc53e4a36dcfe.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d1ce12ea5f821fdbae5cc53e4a36dcfe.png)'
- en: '[Figure 19.13](#figure-19-13): Program-centric analogy identifies and isolates
    isomorphic substructures across different instances.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.13](#figure-19-13)：程序中心类比识别并隔离不同实例间的同构子结构。'
- en: This kind of analogy-making via exact structural match within different discrete
    structures isn’t at all exclusive to specialized fields like computer science,
    or mathematics — you’re constantly using it without noticing. It underlies *reasoning*,
    *planning*, and the general concept of *rigor* (as opposed to intuition). Any
    time you’re thinking about objects connected to each other by a discrete network
    of relationships (rather than a continuous similarity function), you’re using
    program-centric analogies.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在不同离散结构内部通过精确结构匹配进行类比制作的过程，并不局限于像计算机科学或数学这样的专业领域——你一直在不知不觉中使用它。它构成了**推理**、**规划**以及**严谨**（相对于直觉）的一般概念。每当你思考通过离散关系网络相互关联的对象（而不是连续相似性函数）时，你就是在使用以程序为中心的类比。
- en: Cognition as a combination of both kinds of abstraction
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认知作为两种抽象的结合
- en: Table 19.1 compares these two poles of abstraction side by side.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表19.1并排比较了这两个抽象的极端。
- en: '| Value-centric abstraction | Program-centric abstraction |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 价值中心抽象 | 程序中心抽象 |'
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Relates things by distance | Relates things by exact structural match |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 通过距离关联事物 | 通过精确的结构匹配关联事物 |'
- en: '| Continuous, grounded in geometry. | Discrete, grounded in topology |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 连续的，基于几何的。 | 离散的，基于拓扑的 |'
- en: '| Produces abstractions by “averaging” instances into “prototypes” | Produces
    abstractions by isolating isomorphic substructures across instances |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 通过“平均”实例到“原型”产生抽象 | 通过隔离实例间的同构子结构产生抽象 |'
- en: '| Underlies perception and intuition | Underlies reasoning and planning |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 基于感知和直觉 | 基于推理和规划 |'
- en: '| Immediate, fuzzy, approximative | Slow, exact, rigorous |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 立即的，模糊的，近似的 | 慢速的，精确的，严谨的 |'
- en: '| Requires a lot of experience to produce reliable results | Experience efficient:
    can operate on as few as two instances |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 产生可靠结果需要大量经验 | 经验高效：可以操作最少的两个实例 |'
- en: '[Table 19.1](#table-19-1): The two poles of abstraction'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[表19.1](#table-19-1)：抽象的两个极端'
- en: Everything we do, everything we think, is a combination of these two types of
    abstraction. You’d be hard pressed to find tasks that only involve one of the
    two. Even a seemingly “pure perception” task, like recognizing objects in a scene,
    involves a fair amount of implicit reasoning about the relationships between the
    objects you’re looking at. And even a seemingly “pure reasoning” task, like finding
    the proof of a mathematical theorem, involves a good amount of intuition. When
    a mathematician puts their pen to the paper, they’ve already got a fuzzy vision
    of the direction in which they’re going. The discrete reasoning steps they take
    to get to the destination are guided by high-level intuition.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的一切，我们思考的一切，都是这两种抽象类型的组合。你很难找到只涉及其中一种的任务。即使是看似“纯粹感知”的任务，比如在场景中识别物体，也涉及到大量关于你所观察到的物体之间关系的隐含推理。而即使是看似“纯粹推理”的任务，比如寻找数学定理的证明，也涉及到大量的直觉。当数学家把笔放在纸上时，他们已经对将要走的方向有一个模糊的愿景。他们为了到达目的地所采取的离散推理步骤是由高级直觉引导的。
- en: These two poles are complementary, and it’s their interleaving that enables
    extreme generalization. No mind could be complete without both of them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个极端是互补的，正是它们的交织使得极端泛化成为可能。没有哪一个心智可以没有它们而完整。
- en: Why deep learning isn’t a complete answer to abstraction generation
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么深度学习不是抽象生成完整答案
- en: Deep learning is very good at encoding value-centric abstraction, but it has
    basically no ability to generate program-centric abstraction. Human-like intelligence
    is a tight interleaving of both types, so we’re literally missing half of what
    we need — arguably the most important half.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在编码以价值为中心的抽象方面非常出色，但它基本上没有能力生成以程序为中心的抽象。类似人类的智能是这两种类型的紧密交织，所以我们实际上缺少了我们需要的另一半——可以说是最重要的另一半。
- en: 'Now, here’s a caveat. So far, I’ve presented each type of abstraction as entirely
    separate from the other — opposite, even. In practice, however, they’re more of
    a spectrum: to an extent, you could do reasoning by embedding discrete programs
    in continuous manifolds — just like you may fit a polynomial function through
    any set of discrete points, as long as you have enough coefficients. And inversely,
    you could use discrete programs to emulate continuous distance functions — after
    all, when you’re doing linear algebra on a computer, you’re working with continuous
    spaces, entirely via discrete programs that operate on 1s and 0s.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有一个警告。到目前为止，我把每种类型的抽象都描述为完全独立于另一种——甚至是相反的。然而，在实践中，它们更像是一个光谱：在一定程度上，你可以通过在连续流形中嵌入离散程序来进行推理——就像你可能通过任何一组离散点拟合多项式函数一样，只要你有多达足够的系数。反过来，你可以使用离散程序来模拟连续的距离函数——毕竟，当你用计算机进行线性代数时，你是在连续空间中工作，完全是通过在1和0上操作的离散程序来实现的。
- en: 'However, there are clearly types of problems that are better suited to one
    or the other. Try to train a deep learning model to sort a list of five numbers,
    for instance. With the right architecture, it’s not impossible, but it’s an exercise
    in frustration. You’ll need a massive amount of training data to make it happen
    — and even then, the model will still make occasional mistakes when presented
    with new numbers. And if you want to start sorting lists of 10 numbers instead,
    you’ll need to completely retrain the model — on even more data. Meanwhile, writing
    a sorting algorithm in Python takes just a few lines, and the resulting program,
    once validated on a couple more examples, will work every time on lists of any
    size. That’s pretty strong generalization: going from a couple of demonstration
    examples and test examples to a program that can successfully process literally
    any list of numbers.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显然有一些问题更适合其中一种类型。比如，尝试训练一个深度学习模型来排序五个数字的列表。有了正确的架构，这并非不可能，但这是一个令人沮丧的练习。你需要大量的训练数据才能让它发生——即使如此，当面对新的数字时，模型仍然会偶尔犯错。而如果你想要开始排序十个数字的列表，你需要完全重新训练模型——在更多的数据上。与此同时，用Python编写一个排序算法只需要几行代码，一旦在几个额外的例子上验证通过，它就可以在任何大小的列表上工作。这相当强的泛化能力：从几个演示示例和测试示例到可以成功处理任何数字列表的程序。
- en: 'In reverse, perception problems are a terrible fit for discrete reasoning processes.
    Try to write a pure-Python program to classify MNIST digits, without using any
    machine learning technique: you’re in for a ride. You’ll find yourself painstakingly
    coding functions that can detect the number of closed loops in a digit, the coordinates
    of the center of mass of a digit, and so on. After thousands of lines of code,
    you might achieve 90% test accuracy. In this case, fitting a parametric model
    is much simpler; it can better utilize the large amount of data that’s available,
    and it achieves much more robust results. If you have lots of data and you’re
    faced with a problem where the manifold hypothesis applies, go with deep learning.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，感知问题与离散推理过程非常不匹配。尝试编写一个纯Python程序来分类MNIST数字，不使用任何机器学习技术：你将面临一场挑战。你会发现自己在费力地编写可以检测数字中闭合环数的函数、数字质心的坐标等等。在编写数千行代码之后，你可能会达到90%的测试准确率。在这种情况下，拟合参数模型要简单得多；它可以更好地利用大量可用数据，并实现更稳健的结果。如果你有很多数据，并且面临一个适用于流形假设的问题，那么选择深度学习。
- en: For this reason, it’s unlikely that we’ll see the rise of an approach that would
    reduce reasoning problems to manifold interpolation or that would reduce perception
    problems to discrete reasoning. The way forward in AI is to develop a unified
    framework that incorporates *both* types of abstraction generation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不太可能看到一种将推理问题简化为流形插值或把感知问题简化为离散推理的方法的出现。在人工智能领域的前进方向是开发一个统一的框架，该框架包含**两种**抽象生成类型。
- en: 'An alternative approach to AI: Program synthesis'
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能的另一种方法：程序综合
- en: Until 2024, AI systems capable of genuine discrete reasoning were all hardcoded
    by human programmers — for instance, software that relies on search algorithms,
    graph manipulation, and formal logic. In the test-time adaptation (TTA) era, this
    is finally starting to change. A branch of TTA that is especially promising is
    *program synthesis* — a field that is still very niche today, but that I expect
    to take off in a big way over the next few decades.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2024年，能够进行真正离散推理的人工智能系统都是由人类程序员硬编码的——例如，依赖于搜索算法、图操作和形式逻辑的软件。在测试时自适应（TTA）时代，这种情况终于开始改变。特别有前途的TTA分支之一是**程序综合**——一个今天仍然非常小众的领域，但我预计在接下来的几十年里它将迎来大发展。
- en: 'Program synthesis consists of automatically generating simple programs by using
    a search algorithm (possibly genetic search, as in *genetic programming*) to explore
    a large space of possible programs (see figure 19.14). The search stops when a
    program is found that matches the required specifications, often provided as a
    set of input-output pairs. This is highly reminiscent of machine learning: given
    training data provided as input-output pairs, we find a program that matches inputs
    to outputs and can generalize to new inputs. The difference is that instead of
    learning parameter values in a hardcoded program (a neural network), we generate
    source code via a discrete search process (see table 19.2).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 程序综合是通过使用搜索算法（可能是遗传搜索，如**遗传编程**）来探索大量可能的程序空间（见图19.14）来自动生成简单的程序。当找到一个符合所需规格的程序时，搜索停止，这些规格通常以一组输入输出对的形式提供。这非常类似于机器学习：给定作为输入输出对提供的训练数据，我们找到一个匹配输入到输出的程序，并且可以推广到新的输入。区别在于，我们不是在硬编码的程序（神经网络）中学习参数值，而是通过离散搜索过程生成源代码（见表19.2）。
- en: '![](../Images/7d0d57c1aff590444ccd30601e61c047.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d0d57c1aff590444ccd30601e61c047.png)'
- en: '[Figure 19.14](#figure-19-14): A schematic view of program synthesis: given
    a program specification and a set of building blocks, a search process assembles
    the building blocks into candidate programs, which are then tested against the
    specification. The search continues until a valid program is found.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.14](#figure-19-14)：程序综合的示意图：给定一个程序规范和一组构建块，搜索过程将构建块组装成候选程序，然后对这些候选程序进行测试，以符合规范。搜索会继续进行，直到找到一个有效的程序。'
- en: '| Machine learning | Program synthesis |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习 | 程序综合 |'
- en: '| --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model: differentiable parametric function | Model: graph of operators from
    a programming language |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 模型：可微分的参数函数 | 模型：编程语言操作符的图 |'
- en: '| Engine: gradient descent | Engine: discrete search (such as genetic search)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 引擎：梯度下降 | 引擎：离散搜索（如遗传搜索） |'
- en: '| Requires a lot of data to produce reliable results | Data efficient: can
    work with a couple of training examples |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 需要大量数据来产生可靠的结果 | 数据高效：可以用几个训练示例工作 |'
- en: '[Table 19.2](#table-19-2): Machine learning vs. program synthesis'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[表19.2](#table-19-2)：机器学习与程序综合的比较'
- en: Program synthesis is how we’re going to add program-centric abstraction capabilities
    to our AI systems. It’s the missing piece of the puzzle.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 程序综合是我们将向我们的AI系统添加以程序为中心的抽象能力的方法。它是拼图缺失的一块。
- en: Blending deep learning and program synthesis
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合深度学习和程序综合
- en: 'Of course, deep learning isn’t going anywhere. Program synthesis isn’t its
    replacement; it is its complement. It’s the hemisphere that has been so far missing
    from our artificial brains. We’re going to be using both, in combination. There
    are two major ways this will take place:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，深度学习不会消失。程序综合不是它的替代品；它是它的补充。这是我们人工大脑迄今为止缺失的半球。我们将两者结合使用。这种结合将主要有两种方式：
- en: Developing systems that integrate both deep learning modules and discrete algorithmic
    modules
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发集成深度学习模块和离散算法模块的系统
- en: Using deep learning to make the program search process itself more efficient
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习使程序搜索过程本身更高效
- en: Let’s review each of these possible avenues.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些可能的途径。
- en: Integrating deep learning modules and algorithmic modules into hybrid systems
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将深度学习模块和算法模块集成到混合系统中
- en: 'Today, many of the most powerful AI systems are hybrid: they use both deep
    learning models and handcrafted symbol-manipulation programs. In DeepMind’s AlphaGo,
    for example, most of the intelligence on display is designed and hardcoded by
    human programmers (such as Monte Carlo Tree Search). Learning from data happens
    only in specialized submodules (value networks and policy networks). Or consider
    the Waymo self-driving car: it’s able to handle a large variety of situations
    because it maintains a model of the world around it — a literal 3D model — full
    of assumptions hardcoded by human engineers. This model is constantly updated
    via deep learning perception modules (powered by Keras) that interface it with
    the surroundings of the car.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，许多最强大的AI系统都是混合型的：它们既使用深度学习模型，也使用手工编写的符号操作程序。例如，在DeepMind的AlphaGo中，展示的大部分智能都是由人类程序员（如蒙特卡洛树搜索）设计和硬编码的。数据学习仅发生在专门的子模块（价值网络和政策网络）中。或者考虑Waymo自动驾驶汽车：它能够处理大量不同的场景，因为它维护着周围世界的模型——一个字面上的3D模型——其中充满了由人类工程师硬编码的假设。这个模型通过深度学习感知模块（由Keras提供动力）不断更新，这些模块将其与汽车周围的环境接口。
- en: For both of these systems — AlphaGo and self-driving vehicles — the combination
    of human-created discrete programs and learned continuous models is what unlocks
    a level of performance that would be impossible with either approach in isolation,
    such as an end-to-end deep net or a piece of software without machine learning
    elements. So far, the discrete algorithmic elements of such hybrid systems are
    painstakingly hardcoded by human engineers. But in the future, such systems may
    be fully learned, with no human involvement.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个系统——AlphaGo和自动驾驶汽车——人类创建的离散程序和学习的连续模型相结合，才能解锁一种单独使用任何一种方法都无法达到的性能水平，例如端到端的深度网络或不含机器学习元素的软件。到目前为止，这种混合系统的离散算法元素都是人类工程师费力地硬编码的。但在未来，这样的系统可能完全是通过学习实现的，没有任何人类参与。
- en: 'What will this look like? Consider a well-known type of network: recurrent
    neural networks (RNNs). It’s important to note that RNNs have slightly fewer limitations
    than feedforward networks. That’s because RNNs are a bit more than mere geometric
    transformations: they’re geometric transformations *repeatedly applied inside
    a `for` loop*. The temporal `for` loop is itself hardcoded by human developers:
    it’s a built-in assumption of the network. Naturally, RNNs are still extremely
    limited in what they can represent, primarily because each step they perform is
    a differentiable geometric transformation, and they carry information from step
    to step via points in a continuous geometric space (state vectors). Now imagine
    a neural network that’s augmented in a similar way with programming primitives
    but instead of a single hardcoded `for` loop with hardcoded continuous-space memory,
    the network includes a large set of programming primitives that the model is free
    to manipulate to expand its processing function, such as `if` branches, `while`
    statements, variable creation, disk storage for long-term memory, sorting operators,
    advanced data structures (such as lists, graphs, and hash tables), and many more.
    The space of programs that such a network could represent would be far broader
    than what can be represented with current deep learning models, and some of these
    programs could achieve superior generalization power. Importantly, such programs
    will not be differentiable end to end, although specific modules will remain differentiable,
    and thus will need to be generated via a combination of discrete program search
    and gradient descent.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这会是什么样子呢？考虑一种著名的网络类型：循环神经网络（RNNs）。需要注意的是，RNNs比前馈网络有稍微少的限制。这是因为RNNs不仅仅是几何变换：它们是在`for`循环中反复应用的几何变换。时间`for`循环本身是由人类开发者硬编码的：这是网络的内置假设。自然地，RNNs在它们能表示的内容上仍然极为有限，主要是因为它们每一步执行的都是可微的几何变换，并且它们通过连续几何空间中的点（状态向量）从一步传递到下一步携带信息。现在想象一个以类似方式增强的神经网络，它使用编程原语，但不是单个硬编码的`for`循环和硬编码的连续空间内存，而是包含了一个大型的编程原语集合，模型可以自由地操作以扩展其处理功能，例如`if`分支，`while`语句，变量创建，用于长期记忆的磁盘存储，排序运算符，高级数据结构（如列表、图和哈希表）等等。这样一个网络可以表示的程序空间将比当前深度学习模型所能表示的更广泛，其中一些程序可以实现更优越的泛化能力。重要的是，这样的程序将不是端到端可微的，尽管特定的模块仍然可微，因此需要通过离散程序搜索和梯度下降的组合来生成。
- en: We’ll move away from having, on one hand, hardcoded algorithmic intelligence
    (handcrafted software) and, on the other hand, learned geometric intelligence
    (deep learning). Instead, we’ll have a blend of formal algorithmic modules that
    provide reasoning and abstraction capabilities and geometric modules that provide
    informal intuition and pattern-recognition capabilities (figure 19.15). The entire
    system will be learned with little or no human involvement. This should dramatically
    expand the scope of problems that can be solved with machine learning — the space
    of programs that we can generate automatically, given appropriate training data.
    Systems like AlphaGo — or even RNNs — can be seen as a prehistoric ancestor of
    such hybrid algorithmic-geometric models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再仅仅拥有硬编码的算法智能（手工软件）和学习的几何智能（深度学习）。相反，我们将拥有一个结合了提供推理和抽象能力的正式算法模块以及提供非正式直觉和模式识别能力的几何模块（图19.15）。整个系统将几乎不需要人工参与进行学习。这应该会极大地扩展可以用机器学习解决的问题的范围——给定适当的训练数据，我们可以自动生成的程序空间。像AlphaGo这样的系统——甚至RNNs——可以被视为这种混合算法-几何模型的史前祖先。
- en: '![](../Images/cb571dcdb0fc9974cabe50470bf1ab16.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/cb571dcdb0fc9974cabe50470bf1ab16.png)'
- en: '[Figure 19.15](#figure-19-15): A learned program relying on both geometric
    primitives (pattern recognition, intuition) and algorithmic primitives (reasoning,
    search, memory)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.15](#figure-19-15)：一个既依赖几何原语（模式识别，直觉）又依赖算法原语（推理，搜索，记忆）的学习程序'
- en: Using deep learning to guide program search
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用深度学习引导程序搜索
- en: 'Today, program synthesis faces a major obstacle: it’s tremendously inefficient.
    To caricature, typical program synthesis techniques work by trying every possible
    program in a search space until it finds one that matches the specification provided.
    As the complexity of a program specification increases, or as the vocabulary of
    primitives used to write programs expands, the program search process runs into
    what’s known as *combinatorial explosion*: the set of possible programs to consider
    grows very fast, in fact, much faster than merely exponentially fast. As a result,
    today, program synthesis can only be used to generate very short programs. You’re
    not going to be generating a new OS for your computer anytime soon.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，程序综合面临一个主要障碍：它效率极低。为了夸张，典型的程序综合技术是通过在搜索空间中尝试每一个可能程序，直到找到一个与提供的规范相匹配的程序。随着程序规范复杂性的增加，或者用于编写程序的原始词汇的扩展，程序搜索过程会遇到所谓的*组合爆炸*：要考虑的可能程序集增长非常快，实际上，比仅仅指数级增长还要快。因此，今天，程序综合只能用来生成非常短小的程序。你不会很快为你的电脑生成一个新的操作系统。
- en: 'To move forward, we’re going to need to make program synthesis efficient by
    bringing it closer to the way humans write software. When you open your editor
    to code up a script, you’re not thinking about every possible program you could
    potentially write. You only have in mind a handful of possible approaches: you
    can use your understanding of the problem and your past experience to drastically
    cut through the space of possible options to consider.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了前进，我们需要通过使其更接近人类编写软件的方式，使程序综合变得高效。当你打开你的编辑器来编写脚本时，你不会考虑每一个可能编写程序。你只会在心中考虑几种可能的方法：你可以利用你对问题的理解和以往的经验，大大减少要考虑的可能选项的空间。
- en: 'Deep learning can help program synthesis do the same: although each specific
    program we’d like to generate might be a fundamentally discrete object that performs
    non-interpolative data manipulation, evidence so far indicates that *the space
    of all useful programs* may look a lot like a continuous manifold. That means
    that a deep learning model that has been trained on millions of successful program-generation
    episodes might start to develop solid *intuition* about the *path through program
    space* that the search process should take to go from a specification to the corresponding
    program — just like a software engineer might have immediate intuition about the
    overall architecture of the script they’re about to write and about the intermediate
    functions and classes they should use as stepping stones on the way to the goal.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以帮助程序综合做到同样的事情：尽管我们希望生成的每个特定程序可能是一个基本上离散的对象，执行非插值数据操作，但到目前为止的证据表明，*所有有用程序的集合*可能看起来非常像一个连续流形。这意味着一个在数百万个成功的程序生成场景上训练过的深度学习模型可能会开始发展出关于搜索过程应该采取的*程序空间路径*的*直觉*——就像软件工程师可能立即对即将编写的脚本的总体架构以及他们应该用作通往目标途径的垫脚石的中间函数和类有直觉一样。
- en: Remember that human reasoning is heavily guided by value-centric abstraction
    — that is, by pattern recognition and intuition. The same should be true of program
    synthesis. I expect the general approach of guiding program search via learned
    heuristics to see increasing research interest over the next 10 to 20 years.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，人类的推理很大程度上是由以价值为中心的抽象所指导的——也就是说，通过模式识别和直觉。程序综合也应该如此。我预计，在接下来的10到20年里，通过学习启发式方法引导程序搜索的一般方法将越来越受到研究兴趣的关注。
- en: Modular component recombination and lifelong learning
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模块化组件重组和终身学习
- en: If models become more complex and are built on top of richer algorithmic primitives,
    then this increased complexity will require higher reuse between tasks, rather
    than training a new model from scratch every time we have a new task or a new
    dataset. Many datasets don’t contain enough information for us to develop a new,
    complex model from scratch, and it will be necessary to use information from previously
    encountered datasets (much as you don’t learn English from scratch every time
    you open a new book — that would be impossible). Training models from scratch
    on every new task is also inefficient due to the large overlap between the current
    tasks and previously encountered tasks.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型变得更加复杂，并且建立在更丰富的算法原语之上，那么这种增加的复杂性将需要任务之间更高的重用率，而不是每次遇到新任务或新数据集时从头开始训练新模型。许多数据集不包含足够的信息，使我们能够从头开始开发新的复杂模型，因此有必要使用先前遇到的数据集中的信息（就像你每次打开新书时不会从头开始学习英语一样——那是不可能的）。由于当前任务与先前遇到的任务之间有大量重叠，因此在每个新任务上从头开始训练模型也是低效的。
- en: 'With modern foundation models, we’re starting to move closer to a world where
    AI systems possess enormous amounts of acquired knowledge and skills and can bring
    them to bear on whatever comes their way. But LLMs are missing a key ingredient:
    recombination. LLMs are very good at fetching and reapplying memorized functions,
    but they’re not yet able to recombine those functions on the fly into brand-new
    programs adapted to the situation at hand. They are, in fact, entirely incapable
    of performing function composition, as investigated in a recent paper by Dziri
    et al.^([[5]](#footnote-5)). What’s more, the kind of functions they learn aren’t
    sufficiently abstract or modular, making them a poor fit for recombination in
    the first place. Remember how we pointed out that LLMs have low accuracy in adding
    large integers? You probably wouldn’t want to build your next codebase on top
    of such brittle functions.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代基础模型的发展，我们正逐渐接近一个世界，其中AI系统拥有大量的知识和技能，并将它们应用于任何他们遇到的情况。但是，LLMs缺少一个关键成分：重组。LLMs非常擅长检索和重新应用记忆中的函数，但它们还无法将这些函数即时重新组合成适应当前情况的新程序。实际上，正如Dziri等人最近的研究论文所调查的，它们完全无法执行函数组合。更重要的是，它们学习的函数类型既不够抽象也不够模块化，这使得它们一开始就不适合重组。记得我们指出LLMs在添加大整数时准确性较低吗？你可能不会想在如此脆弱的函数之上构建你的下一个代码库。
- en: To solve *compositional generalization*, we’re going to need to reuse robust
    *program components* like the functions and classes found in human programming
    languages. These components will be evolved specifically for modular reuse in
    a new context — unlike the patterns that LLMs memorize. And our AIs will recombine
    them on the fly to synthesize new programs adapted to the current task. Crucially,
    libraries of such reusable components will be built through the cumulative experience
    of all instances of our AIs and will then be accessible by all in perpetuity.
    Any single problem encountered by our AIs would only need to be solved once —
    making them constantly self-improving.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决*组合泛化*问题，我们需要重用像人类编程语言中找到的函数和类这样的强大*程序组件*。这些组件将专门为在新的环境中模块化重用而进化——与LLMs记忆的模式不同。我们的AI将在运行时重新组合它们，以合成适应当前任务的新的程序。关键的是，这样的可重用组件库将通过我们AI所有实例的累积经验来构建，并将永久对所有用户开放。我们的AI遇到的任何单个问题只需解决一次——使它们不断自我改进。
- en: 'Think of the process of software development today: once an engineer solves
    a specific problem (HTTP queries in Python, for instance), they package it as
    an abstract, reusable library, accessible by anyone on the planet. Engineers who
    face a similar problem in the future will be able to search for existing libraries,
    download one, and use it in their own project. In a similar way, in the future,
    meta-learning systems will be able to assemble new programs by sifting through
    a global library of high-level reusable blocks. When the system finds itself developing
    similar program subroutines for several different tasks, it can come up with an
    *abstract*, reusable version of the subroutine and store it in the global library
    (see figure 19.16). These subroutines can be either geometric (deep learning modules
    with pretrained representations) or algorithmic (closer to the libraries that
    contemporary software engineers manipulate).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下今天的软件开发过程：一旦工程师解决了特定问题（例如Python中的HTTP查询），他们就会将其打包成一个抽象的、可重用的库，任何人都可以在地球上访问。未来面临类似问题的工程师将能够搜索现有库，下载一个，并在自己的项目中使用它。以类似的方式，在未来，元学习系统将通过筛选全球高级可重用块库来组装新的程序。当系统发现自己为几个不同的任务开发类似的程序子例程时，它可以提出一个**抽象的**、可重用的子例程版本，并将其存储在全局库中（见图19.16）。这些子例程可以是几何的（具有预训练表示的深度学习模块）或算法的（更接近当代软件工程师操作的库）。
- en: '![](../Images/5c408e5c057c76442835fd1ab119022e.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c408e5c057c76442835fd1ab119022e.png)'
- en: '[Figure 19.16](#figure-19-16): A meta-learner capable of quickly developing
    task-specific models using reusable primitives (both algorithmic and geometric),
    thus achieving extreme generalization'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19.16](#figure-19-16)：一个能够快速开发特定任务模型的元学习器，使用可重用的原语（既包括算法也包括几何），从而实现极端的泛化'
- en: The long-term vision
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长期愿景
- en: 'In short, here’s my long-term vision for AI:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这是我对于人工智能的长期愿景：
- en: Models will be more like programs and will have capabilities that go far beyond
    the continuous geometric transformations of the input data we currently work with.
    These programs will arguably be much closer to the abstract mental models that
    humans maintain about their surroundings and themselves, and they will be capable
    of stronger generalization due to their rich algorithmic nature.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型将更像程序，并将具有超越我们目前工作的输入数据的连续几何变换的能力。这些程序将无庸置疑地更接近人类对其周围环境和自身的抽象心理模型，并且由于它们丰富的算法性质，它们将能够实现更强的泛化。
- en: In particular, models will blend *algorithmic modules* providing formal reasoning,
    search, and abstraction capabilities with *geometric modules* providing informal
    intuition and pattern-recognition capabilities. This will achieve a blend of value-centric
    and program-centric abstraction. AlphaGo or self-driving cars (systems that required
    a lot of manual software engineering and human-made design decisions) provide
    an early example of what such a blend of symbolic and geometric AI could look
    like.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尤其是模型将融合提供形式推理、搜索和抽象能力的**算法模块**，以及提供非正式直觉和模式识别能力的**几何模块**。这将实现以价值为中心和以程序为中心的抽象的融合。AlphaGo或自动驾驶汽车（需要大量手动软件工程和人工设计决策的系统）是这种符号和几何人工智能融合的早期例子。
- en: Such models will be *grown* automatically rather than hardcoded by human engineers,
    using modular parts stored in a global library of reusable subroutines — a library
    evolved by learning high-performing models on thousands of previous tasks and
    datasets. As frequent problem-solving patterns are identified by the meta-learning
    system, they will be turned into reusable subroutines — much like functions and
    classes in software engineering — and added to the global library.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样的模型将通过自动**生长**而不是由人类工程师硬编码，使用存储在全局可重用子例程库中的模块化部分——这个库是通过在数千个先前任务和数据集上学习高性能模型而演化的。随着元学习系统识别出频繁的问题解决模式，它们将被转化为可重用的子例程——就像软件工程中的函数和类一样——并添加到全局库中。
- en: The process that searches over possible combinations of subroutines to grow
    new models will be a discrete search process (program synthesis), but it will
    be heavily guided by a form of *program-space intuition* provided by deep learning.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索可能的子例程组合以增长新模型的流程将是一个离散搜索过程（程序综合），但它将受到由深度学习提供的**程序空间直觉**的强烈指导。
- en: 'This global subroutine library and associated model-growing system will be
    able to achieve some form of human-like *extreme generalization*: given a new
    task or situation, the system will be able to assemble a new working model appropriate
    for the task using very little data, thanks to rich program-like primitives that
    generalize well, and extensive experience with similar tasks. In the same way,
    humans can quickly learn to play a complex new video game if they have experience
    with many previous games because the models derived from this previous experience
    are abstract and program-like, rather than a basic mapping between stimuli and
    action.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个全球子程序库及其相关的模型增长系统将能够实现某种形式的人类似*极端泛化*：给定一个新的任务或情况，系统将能够使用非常少的数据组装一个新的适用于该任务的模型，这得益于丰富的程序化原语，这些原语具有很好的泛化能力，以及丰富的类似任务的经验。同样，如果人们有大量先前游戏的经验，他们可以快速学会玩复杂的全新电子游戏，因为从这种先前经验中得出的模型是抽象的、程序化的，而不是刺激和行动之间基本映射。
- en: 'This perpetually learning model-growing system can be interpreted as an *artificial
    general intelligence* (AGI). But don’t expect any singularitarian robot apocalypse
    to ensue: that’s pure fantasy, coming from a long series of profound misunderstandings
    of both intelligence and technology. Such a critique, however, doesn’t belong
    in this book.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个持续学习且模型不断增长的系统可以被解释为一种*通用人工智能*（AGI）。但不要期待随之而来的任何单一化机器人末日：那只是纯粹的幻想，源于对智能和技术的长期深刻误解。然而，这样的批评并不属于这本书的内容。
- en: Footnotes
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: 'See also Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev,
    “Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art
    Large Language Models,” arXiv, [https://arxiv.org/abs/2406.02061](https://arxiv.org/abs/2406.02061).
    [[↩]](#footnote-link-1)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参见玛丽安娜·涅祖里娜、卢西亚·奇波利娜-库恩、梅赫迪·查尔蒂和叶尼娅·吉特谢夫，《爱丽丝梦游仙境：简单任务显示最先进大型语言模型中完全推理崩溃》，arXiv，[https://arxiv.org/abs/2406.02061](https://arxiv.org/abs/2406.02061)。[[↩]](#footnote-link-1)
- en: Terry Winograd, “Procedures as a Representation for Data in a Computer Program
    for Understanding Natural Language,” 1971. [[↩]](#footnote-link-2)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特里·温格罗德，《作为计算机程序中理解自然语言数据表示的过程》，1971年。[[↩]](#footnote-link-2)
- en: 'Michael Shick, “Wozniak: Could a Computer Make a Cup of Coffee?” Fast Company,
    March 2010. [[↩]](#footnote-link-3)'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迈克尔·希克，《沃兹尼亚克：电脑能煮一杯咖啡吗？》快公司，2010年3月。[[↩]](#footnote-link-3)
- en: François Chollet, “On the Measure of Intelligence,” 2019. [[↩]](#footnote-link-4)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 弗朗索瓦·肖莱，《关于智能的度量》，2019年。[[↩]](#footnote-link-4)
- en: 'Nouha Dziri et. al., “Faith and Fate: Limits of Transformers on Compositionality,”
    Proceedings of the 37th International Conference on Neural Information Processing
    Systems (2023), [https://arxiv.org/abs/2305.18654](https://arxiv.org/abs/2305.18654).
    [[↩]](#footnote-link-5)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nouha Dziri 等人，《信仰与命运：Transformer在组合性上的局限性》，第37届国际神经网络信息处理系统会议论文集（2023年），[https://arxiv.org/abs/2305.18654](https://arxiv.org/abs/2305.18654)。[[↩]](#footnote-link-5)
