- en: Chapter 3\. LLM-Based Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 基于LLM的应用
- en: As of early 2025, only a few companies offer large multimodal models that can
    understand and generate text, images, and other media, like sound and video. For
    brevity, we will call these AI models. The most well-known examples are the GPT
    models created by OpenAI, but a few other popular examples are the Gemini models
    created by Google, the Claude Sonnet and Haiku models created by Anthropic, and
    the Llama models created by Meta.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2025年初，只有少数公司提供能够理解和生成文本、图像以及其他媒体，如声音和视频的大型多模态模型。为了简洁，我们将这些AI模型称为AI模型。最著名的例子是由OpenAI创建的GPT模型，但还有一些其他流行的例子，如由Google创建的Gemini模型，由Anthropic创建的Claude
    Sonnet和Haiku模型，以及由Meta创建的Llama模型。
- en: In many cases, these companies partner with other companies to offer these models
    as a cloud service. For example, OpenAI has a partnership with Microsoft, which
    provides the infrastructure to host OpenAI’s models in cloud services that can
    be accessed via APIs. Other companies, like Meta, provide a model snapshot, a
    large binary file containing the weights of a pretrained model, which users can
    install in their own infrastructure. This infrastructure can be “bare metal,”
    meaning physical machines the companies own, or cloud infrastructure they purchase
    from other providers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这些公司与其他公司合作，将这些模型作为云服务提供。例如，OpenAI与微软合作，为在可以通过API访问的云服务中托管OpenAI的模型提供基础设施。其他公司，如Meta，提供模型快照，这是一个包含预训练模型权重大型二进制文件，用户可以将其安装在自己的基础设施中。这种基础设施可以是“裸机”，即公司拥有的物理机器，或者他们从其他提供商购买的云基础设施。
- en: 'Model-building companies also offer user-facing applications. In many cases,
    the name of the model and the name of the user-facing application are the same
    or very similar, making it easy to confuse the two. For example, the Google Gemini
    application uses the Google Gemini model, and the Claude application uses the
    Anthropic Claude Sonnet and Haiku models. OpenAI’s names are slightly different:
    its user-facing application, ChatGPT, allows users to interact with the GPT-4o
    and GPT-4o-mini models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建公司也提供面向用户的应用。在许多情况下，模型名称和面向用户的应用名称相同或非常相似，这很容易让人混淆。例如，Google Gemini应用使用Google
    Gemini模型，Claude应用使用Anthropic的Claude Sonnet和Haiku模型。OpenAI的名称略有不同：其面向用户的应用ChatGPT允许用户与GPT-4o和GPT-4o-mini模型进行交互。
- en: These applications can have different levels of sophistication. The simplest
    type of application is a chat-like web interface that allows users to send prompts
    directly to the model and returns the response. These days, most of the applications
    provided by large companies are more sophisticated than that. Instead of simply
    passing prompts directly, they add several layers of their own instructions to
    the user input, keep track of what the user asked earlier in that conversation
    (and sometimes in previous sessions), modify the user-submitted prompt to increase
    the chance of getting a better response, and ensure that their answers are safe
    and polite.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用可以具有不同复杂程度。最简单类型的应用是一种类似聊天的网络界面，允许用户直接向模型发送提示并返回响应。如今，大多数由大型公司提供的应用比这更复杂。它们不仅仅直接传递提示，还在用户输入中添加了几层自己的指令，跟踪用户在对话中之前提出的问题（有时在之前的会话中），修改用户提交的提示以增加获得更好响应的机会，并确保他们的回答安全且礼貌。
- en: Given these additional prompts and safeguards, users get different answers when
    interacting with models through the API and through the default web application.
    For example, when a user is interacting with ChatGPT on the web through chatgpt.com,
    they are likely to get different answers than if they were to submit the same
    prompt directly to the model using an API. The answer from ChatGPT may use data
    from previous chats and will add some additional safeguards and instructions to
    the user-provided prompt. For example, when you ask a question using ChatGPT’s
    website, it now usually finishes the response with a question inviting the user
    to continue the conversation, like “Would you like to explore more?” If you use
    the model directly from the API, it will not include this conversational phrase.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些额外的提示和保障措施下，用户通过API和默认的Web应用与模型交互时得到不同的答案。例如，当用户通过chatgpt.com在Web上与ChatGPT交互时，他们可能会得到与直接通过API提交相同提示时不同的答案。ChatGPT的回答可能会使用之前聊天的数据，并添加一些额外的保障措施和指令到用户提供的提示中。例如，当您使用ChatGPT的网站提问时，现在通常以一个问题结束回答，邀请用户继续对话，比如“您想了解更多吗？”如果您直接从API使用模型，它将不会包含这个对话短语。
- en: Companies don’t have to develop and train an AI model in order to create an
    application that uses AI. They can license and integrate existing models such
    as Gemini, Claude, or GPT-4o into their user-facing applications. Since 2023,
    a large proportion of repositories in GitHub have been importing code that allows
    use of the OpenAI APIs, indicating that many developers are using the GPT cloud
    services to add AI features to their own applications, as shown in [Figure 3-1](#ch03_figure_1_1748895493826708).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 公司不需要开发和训练AI模型来创建使用AI的应用程序。他们可以将Gemini、Claude或GPT-4o等现有模型许可并集成到他们的面向用户的应用程序中。自2023年以来，GitHub中很大比例的仓库都在导入允许使用OpenAI
    API的代码，这表明许多开发者正在使用GPT云服务将AI功能添加到自己的应用程序中，如图3-1所示。
- en: This chapter discusses the operational considerations for using AI models in
    user-facing applications.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在面向用户的应用中使用AI模型的操作考虑因素。
- en: '![](assets/llmo_0301.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0301.png)'
- en: 'Figure 3-1\. Generative AI growth (source: [Microsoft | AI for Good Lab](https://oreil.ly/NJnTq))'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 生成式AI增长（来源：[Microsoft | AI for Good Lab](https://oreil.ly/NJnTq))
- en: Using AI Models in Applications
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在应用中使用AI模型
- en: Many application tasks that were formerly assisted by automation and machine
    learning are now using AI models. The difference between implementing automation
    yourself and using a foundation model is subtle but consequential, especially
    for LLMOps.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多以前由自动化和机器学习辅助的应用任务现在正在使用AI模型。自己实现自动化和使用基础模型之间的区别微妙但意义重大，尤其是对于LLMOps。
- en: Before machine learning and foundation models, if you wanted to automate a task,
    you would need to code it yourself, programming all the possible inputs and corresponding
    outputs for the application. In the last decade, a lot of this automation code
    could be replaced by training a model using machine learning. When a new input
    was submitted, the ML model would generate an appropriate output, even if that
    input was not explicitly programmed or previously seen by the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和基础模型之前，如果您想自动化一个任务，您需要自己编写代码，为应用编程所有可能的输入和相应的输出。在过去十年中，大量这种自动化代码可以通过使用机器学习训练的模型来替代。当提交新的输入时，ML模型会生成适当的输出，即使该输入没有明确编程或之前被模型看到过。
- en: 'Here are some examples of popular consumer-facing applications that incorporate
    third-party foundation models:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些流行的面向消费者的应用示例，它们集成了第三方基础模型：
- en: BeMyEyes
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: BeMyEyes
- en: This [OpenAI application](https://oreil.ly/YxiJF) helps people who are blind
    or have low vision to navigate the world better by using their phones. They can
    point their phone cameras at things and hear rich descriptions. They can use the
    app to count money, to identify products in the supermarket, and to assist with
    using automated teller machines. They can even point the app at their computer
    screens to get technical support.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[OpenAI应用](https://oreil.ly/YxiJF)帮助视障人士或视力低下的人通过使用手机更好地导航世界。他们可以将手机摄像头对准物体，并听到丰富的描述。他们可以使用该应用来数钱，识别超市中的产品，以及协助使用自动柜员机。他们甚至可以将应用对准他们的电脑屏幕以获得技术支持。
- en: Duolingo
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Duolingo
- en: This foreign language–learning application [uses LLMs to create lessons](https://oreil.ly/CNNpM)
    faster and with more variety. The models are used to generate more versions of
    dialogues that adhere to difficulty standards, making lessons less repetitive
    and more enjoyable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个外语学习应用[使用LLM来创建课程](https://oreil.ly/CNNpM)更快，且种类更多。这些模型用于生成更多符合难度标准的对话版本，使课程更少重复，更有趣。
- en: Khan Academy
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可汗学院
- en: 'This educational website is used by hundreds of millions of people to learn
    academic subjects, mainly those taught in grades K–12\. Khan Academy offers [Khanmigo](https://oreil.ly/bM6vS)
    (a portmanteau of *Khan* and *amigo*, Spanish and Portuguese for “friend”), which
    serves as a study buddy and personal tutor. Students can ask Khanmigo questions
    about careers and why the lesson they’re taking is useful for their lives (a favorite
    question among teenagers: “Ugh, why do I have to learn *this?*”). Khanmigo can
    also generate quizzes to assess learning, provide feedback on the student’s writing
    and answers, and generally help a student without providing the answers directly.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教育网站被数亿人用来学习学术科目，主要是K-12年级教授的科目。可汗学院提供[Khanmigo](https://oreil.ly/bM6vS)（由“Khan”和“amigo”组合而成，西班牙语和葡萄牙语中意为“朋友”），它充当学习伙伴和个人导师。学生可以向Khanmigo询问关于职业的问题以及他们正在学习的课程为什么对他们生活有用（青少年中一个受欢迎的问题：“呃，我为什么要学*这个*？”）。Khanmigo还可以生成测验来评估学习，对学生的写作和答案提供反馈，并通常在不直接提供答案的情况下帮助学生。
- en: Microsoft Copilot
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Copilot
- en: The [Copilot stack](https://oreil.ly/julAQ) accounts for perhaps the widest
    deployment of AI to date. Available in several popular Microsoft products, including
    Office, Windows, and Bing, the Copilot products help users accomplish common tasks
    quicker. For example, you can open Word and ask Copilot to “write a letter to
    Bank of America asking to close my checking account.” A letter automatically populates
    with the appropriate language, and you just need to fill in a few blanks. Another
    frequently used example is converting Word documents to PowerPoint presentations
    and vice versa.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Copilot 堆栈](https://oreil.ly/julAQ)可能是迄今为止AI应用最广泛的例子。它可在多个流行的微软产品中使用，包括Office、Windows和Bing，Copilot产品帮助用户更快地完成常见任务。例如，您可以在Word中打开Copilot并要求“写一封给美国银行要求关闭我的支票账户的信。”信件会自动填充适当的语言，您只需填写几个空白即可。另一个常用的例子是将Word文档转换为PowerPoint演示文稿，反之亦然。'
- en: Infrastructure Applications
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施应用
- en: 'While the first wave of LLM applications was mostly focused on user-facing
    tools for tasks like writing and summarization, the current wave of LLM-based
    applications is largely focused on infrastructure applications that make LLMs
    faster, more programmable, and more modular. They redefine what an LLM is and
    how it can be used. LLM applications are no longer limited to chatbots: they have
    become a new layer of code in software applications. (This is what we referred
    to as *Software 3.0* in [Chapter 2](ch02.html#ch02_introduction_to_llmops_1748895480208948).)
    Let’s look at some of these uses, one by one.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一波LLM应用主要集中在面向用户的工具上，如写作和摘要任务，但当前基于LLM的应用主要是关注基础设施应用，这些应用使LLM更快、更可编程、更模块化。它们重新定义了LLM是什么以及如何使用。LLM应用不再局限于聊天机器人：它们已成为软件应用中新的代码层。（这就是我们在[第2章](ch02.html#ch02_introduction_to_llmops_1748895480208948)中提到的*软件3.0*。）让我们逐一看看这些用途。
- en: Agentic Workflows
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理工作流程
- en: A single prompt can take you far. But for anything beyond a surface-level task,
    you need more than one-shot queries. You need memory. You need planning. You need
    tools. And eventually, you need agents that can *act*—not just complete a prompt
    but *choose* what to do next. This is where the shift from language models to
    agentic systems comes in.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提示就可以走得很远。但对于任何超出表面任务的事情，你需要不止一次的查询。你需要记忆。你需要规划。你需要工具。最终，你需要能够*行动*的代理——不仅完成提示，还要*选择*下一步做什么。这就是从语言模型到代理系统的转变所在。
- en: At its core, an agent is just a loop. It observes, decides, and acts—over and
    over. Those acts might be to read an instruction, check its current state, fetch
    a resource, call a tool, or break a task into smaller ones. Each of those actions
    requires reasoning, and each decision affects what happens next. In an agentic
    system, the model is no longer passive. It’s running code, managing steps, and
    adapting as it goes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，一个代理只是一个循环。它观察、决定并行动——一次又一次。这些行为可能是读取指令、检查当前状态、获取资源、调用工具或将任务分解成更小的部分。每个这些动作都需要推理，每个决定都会影响接下来发生的事情。在代理系统中，模型不再是被动的。它在运行代码、管理步骤并在进行中适应。
- en: This approach unlocks more complex workflows. Instead of a user stitching together
    model calls by hand, an agent handles the logic. It can retry failures, store
    intermediate state, track objectives, and even call other agents. This isn’t prompt
    engineering anymore—it’s system design.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法解锁了更复杂的流程。不再是用户手动拼接模型调用，而是代理处理逻辑。它可以重试失败，存储中间状态，跟踪目标，甚至调用其他代理。这不再是提示工程，而是系统设计。
- en: 'Not all agents operate the same way. Some follow a fixed plan; others adapt
    in real time. Some agents think once, act once, and stop. Others operate in loops,
    revisiting goals and shifting strategy as needed. Understanding these differences
    helps in designing systems that are robust, interpretable, and efficient. Major
    types of agents include:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有代理都以相同的方式运作。有些遵循固定的计划；有些则实时调整。有些代理思考一次，行动一次，然后停止。其他代理则循环操作，根据需要重新审视目标和调整策略。理解这些差异有助于设计出稳健、可解释且高效的系统。主要的代理类型包括：
- en: Single-step agents
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 单步代理
- en: The simplest form of an agent is little more than a wrapped prompt. It takes
    an input, does some local reasoning, returns an output, and exits. There’s no
    memory, no iteration, no feedback loop. These are useful when the task is bounded,
    like generating a SQL query, converting a paragraph to a tweet, or answering a
    direct question. But single-step agents are brittle. They assume everything is
    known up front. They can’t handle surprises or partial failures. You’ll quickly
    outgrow them when tasks involve multiple actions or require state tracking.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的最简单形式不过是一个包装好的提示。它接受输入，进行一些局部推理，返回输出，然后退出。没有记忆，没有迭代，没有反馈循环。当任务有限时，如生成SQL查询、将段落转换为推文或回答直接问题时，这些很有用。但单步代理是脆弱的。它们假设所有事情在开始时都是已知的。它们无法处理意外或部分失败。当任务涉及多个动作或需要状态跟踪时，你会很快超越它们。
- en: Chain-of-thought agents
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链代理
- en: 'Here, the agent reasons step-by-step—often in the same prompt. Instead of jumping
    straight to the answer, it explains its logic first. This internal decomposition
    improves reasoning and often leads to better performance on multi-hop problems.
    However, the limitation is clear: it’s still all happening within a single model
    call. There’s no memory of what was done before. If the chain is too long or the
    context window too short, the agent breaks.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，代理逐步推理——通常在同一提示中。它不会直接跳到答案，而是首先解释其逻辑。这种内部分解提高了推理能力，并往往在多跳问题上的表现更佳。然而，局限性也很明显：所有这些都在单个模型调用中发生。没有对之前所做事情的记录。如果链太长或上下文窗口太短，代理就会崩溃。
- en: Plan-and-act agents
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 计划并行动的代理
- en: This is where things get more interesting. These agents first generate a high-level
    plan, then execute it step-by-step. For example, if you tasked a plan-and-ask
    agent with writing a blog post, it would start by drafting an outline. Then it
    would write each section separately, check for coherence, and edit the result.
    The planning and acting can happen in the same model or across separate agents.
    This structure introduces what has been done, what’s left, and where things went
    wrong. It also allows for retries and self-correction. If a step fails, the agent
    can replan or fall back to an alternative.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里事情变得更有趣。这些代理首先生成一个高级计划，然后逐步执行它。例如，如果你让一个计划并询问的代理写博客文章，它将首先起草大纲。然后它会分别撰写每个部分，检查连贯性，并编辑结果。计划和行动可以在同一模型中发生，也可以在不同的代理之间发生。这种结构引入了已经完成的事情、剩下的事情以及出错的地方。它还允许重试和自我纠正。如果某个步骤失败，代理可以重新规划或回退到替代方案。
- en: Reflective or self-improving agents
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 反思或自我改进的代理
- en: These agents don’t just act—they reflect on their performance. After completing
    a task, they might score their own output, compare it with a ground truth, or
    even consult another model to improve their reasoning. This creates a feedback
    loop where the agent learns from its past actions—not in the ML training sense
    but in the runtime sense. Reflection is costly but powerful. It adds robustness,
    especially in open-ended domains where correctness is hard to define in advance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代理不仅行动，还会反思其表现。完成任务后，它们可能会给自己评分，将其与真实情况比较，甚至咨询另一个模型以改进推理。这创建了一个反馈循环，代理从其过去的行为中学习——不是在机器学习训练的意义上，而是在运行时意义上。反思是昂贵的，但也是强大的。它增加了稳健性，特别是在开放领域，预先定义正确性很困难。
- en: Recursive decomposition agents
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 递归分解代理
- en: 'In this pattern, an agent tackles a task by recursively breaking it down. It
    might see a top-level goal, decide it’s too big, and generate subtasks. It then
    becomes a manager—delegating each subtask to a new instance of itself or to other
    specialized agents. This pattern is used in systems like AutoGPT and BabyAGI.
    It allows for dynamic depth: tasks get decomposed until they’re small enough to
    solve directly. The challenge is keeping the recursion from spiraling out of control,
    especially in the absence of tight constraints or time limits.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模式中，一个代理通过递归分解任务来处理任务。它可能会看到顶层目标，认为它太大，然后生成子任务。然后它成为管理者——将每个子任务委派给自己或给其他专业代理的新实例。这个模式用于AutoGPT和BabyAGI等系统。它允许动态深度：任务被分解，直到足够小，可以直接解决。挑战是保持递归不会失控，尤其是在没有严格约束或时间限制的情况下。
- en: Multi-agent collaborators
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理协作
- en: 'Rather than using recursion, some systems *distribute* responsibility: one
    agent writes, another edits. One gathers data, another analyzes it. These agents
    have defined roles, often with isolated tools and memories. Communication happens
    through shared messages or task queues.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与递归不同，一些系统*分配*责任：一个代理写作，另一个编辑。一个收集数据，另一个分析它。这些代理有定义的角色，通常有隔离的工具和记忆。通信通过共享消息或任务队列进行。
- en: 'This is effective when tasks are parallelizable or when each agent has domain-specific
    expertise. It also forces clear interfaces: each agent must expose how to talk
    to it and what kind of input it expects.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务可并行化或每个代理具有特定领域的专业知识时，这种方法是有效的。它还强制明确了接口：每个代理都必须暴露如何与之交流以及它期望什么样的输入。
- en: Each of these patterns serves a different purpose. Some are simple, designed
    for speed. Others are more expressive, built for complexity. As models improve
    and infrastructure grows, we’ll see these workflows combine—agents that reflect
    and recurse in combination with systems that plan, act, and then hand off to a
    team. As these systems scale, coordination becomes the challenge. One agent might
    specialize in math. Another might handle API queries. A third might focus on summarization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式中的每一个都服务于不同的目的。有些很简单，设计用于速度。其他则更具有表现力，用于复杂性。随着模型改进和基础设施增长，我们将看到这些工作流程的结合——反映和递归的代理与计划、行动然后转交给团队的系统相结合。随着这些系统规模的扩大，协调成为挑战。一个代理可能专注于数学。另一个可能处理API查询。第三个可能专注于总结。
- en: Instead of building a single monolithic agent that does everything, it makes
    more sense to compose smaller, focused agents that work together. This is where
    multi-agent systems enter. A *multi-agent system* isn’t just a collection of bots.
    It’s an architecture in which each agent operates semi-independently, often with
    its own tools, memory, and goals. The agents talk, delegate, and collaborate.
    For instance, one agent might take a user request and break it into subgoals.
    Another might execute a subtask and pass the result back. Over time, agents can
    even evolve internal protocols—figuring out how best to share information or resolve
    conflicts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与构建一个能够做所有事情的单一单体代理相比，构建更小、更专注的代理，这些代理可以协同工作，更有意义。这就是多代理系统介入的地方。*多代理系统*不仅仅是一系列机器人。它是一种架构，其中每个代理以半独立的方式操作，通常拥有自己的工具、记忆和目标。代理之间进行交流、委派和协作。例如，一个代理可能会接收一个用户请求并将其分解为子目标。另一个代理可能会执行子任务并将结果返回。随着时间的推移，代理甚至可以进化内部协议——找出如何最好地共享信息或解决冲突。
- en: Designing these workflows is not trivial. The more agents you add, the more
    coordination overhead you introduce. You have to define roles, communication boundaries,
    and fallback plans. You need logging, observability, and memory management. You
    also need clarity around failure; for example, what happens when one agent goes
    silent or another returns an error?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 设计这些工作流程并不简单。你添加的代理越多，你引入的协调开销就越大。你必须定义角色、通信边界和后备计划。你需要日志记录、可观察性和内存管理。你还需要对失败有清晰的了解；例如，当一个代理沉默或另一个代理返回错误时会发生什么？
- en: That’s why agentic design is not just about the model—it requires *control flow*.
    What decisions should happen inside the model versus outside of it? What should
    be handled by logic, and what should be learned? These are architectural questions,
    not just engineering ones.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代理设计不仅仅是关于模型——它需要*控制流*。模型内部和外部应该发生什么决策？逻辑应该处理什么，应该学习什么？这些问题是架构问题，而不仅仅是工程问题。
- en: As of now, we’re still early in this transition. The industry is moving from
    LLMs as isolated prompt responders toward full-blown systems that can think in
    steps, delegate across agents, and operate over time. To make this possible, it
    needs shared protocols like MCP and A2A, which we discuss next. These protocols
    provide conventions that let agents talk, reason, and act as a ​team.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在这个过渡中仍处于早期阶段。行业正在从作为孤立提示响应者的LLM转向能够逐步思考、跨代理委派并在时间上运行的完整系统。为了实现这一点，它需要像MCP和A2A这样的共享协议，我们将在下一节讨论。这些协议提供了让代理能够交谈、推理并以团队形式行动的规范。
- en: Model Context Protocol
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型上下文协议
- en: There is a quiet shift happening in how we build intelligent systems today.
    In the early days of language models, most applications were monoliths—self-contained,
    brittle, and tightly coupled to the tools they used. Every integration was handcrafted.
    If a model needed to talk to a database, spreadsheet, calendar, or code repository,
    someone had to write a custom connection for each pairing. As the number of models
    and tools grew, the web of integrations became unmanageable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们在构建智能系统的方式上正在发生一场静悄悄的转变。在语言模型早期，大多数应用都是单体——自包含、脆弱且紧密耦合到它们使用的工具上。每个集成都是手工制作的。如果一个模型需要与数据库、电子表格、日历或代码库通信，就必须为每一对编写定制的连接。随着模型和工具数量的增加，集成网络变得难以管理。
- en: What emerged out of that chaos was a new design principle—simple, but transformative.
    Instead of trying to make every tool speak the model’s language or forcing every
    model to understand every tool, AI engineers split the problem, giving each part
    a role. That principle now lives in the form of the *Model Context Protocol* (MCP),
    [first introduced by Anthropic](https://oreil.ly/MLp_L) in late 2024 ([Figure 3-2](#ch03_figure_2_1748895493826745)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从那个混乱中浮现出一种新的设计原则——简单但具有变革性。与其试图让每个工具都说模型的“语言”或强迫每个模型理解每个工具，AI工程师将问题分解，为每个部分分配角色。这个原则现在以**模型上下文协议**（MCP）的形式存在，由Anthropic在2024年底首次引入（[图3-2](#ch03_figure_2_1748895493826745)）。
- en: '![](assets/llmo_0302.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_0302.png)'
- en: 'Figure 3-2\. Model Context Protocol (source: [Phil Schmid](https://oreil.ly/SqaNo))'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 模型上下文协议（来源：[Phil Schmid](https://oreil.ly/SqaNo))
- en: At its core, MCP is a contract among three moving parts. First, there’s the
    *host*—your AI application, like a desktop assistant or a chatbot. Then there’s
    the *server*—any external system or tool that exposes capabilities to the model.
    Finally, the *client* is the messenger that connects the two. What makes MCP powerful
    is not just that it splits the parts cleanly but that it gives them a common language;
    i.e., a standard way to describe what they can do, what they know, and what they
    can provide to the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，MCP是三个动态部分之间的合同。首先，是**主机**——你的AI应用，如桌面助手或聊天机器人。然后是**服务器**——任何向模型公开功能的系统或工具。最后，**客户端**是连接两者的信使。使MCP强大的不仅仅是它清晰地将部分分开，还在于它为它们提供了一个共同的语言；即一个标准的方式来描述它们能做什么，知道什么，以及能为模型提供什么。
- en: With MCP, a model no longer has to guess what’s possible. Instead, it can discover
    tools, query data sources, and select prompts—all in real time, all through a
    shared protocol. This means a model doesn’t just generate responses; it acts,
    it calls tools, it gathers context, and it learns how to interact with the outside
    world in a modular, controlled way.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MCP，模型不再需要猜测可能发生的事情。相反，它可以发现工具、查询数据源并选择提示——所有这些都在实时进行，所有这些通过一个共享协议。这意味着模型不仅仅是生成响应；它采取行动，调用工具，收集上下文，并学习如何以模块化和受控的方式与外界互动。
- en: In practice, working with MCP feels less like magic and more like plumbing.
    You can plug in a GitHub integration, a Slack connector, or a calendar interface,
    and the model can learn to use it—without needing a new integration each time.
    Each server exposes tools and resources. The client mediates. The host orchestrates.
    And the model flows through it all, aware of the tools at its disposal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用MCP的感觉更像是管道工的工作，而不是魔法。你可以接入GitHub集成、Slack连接器或日历界面，模型可以学会使用它们——无需每次都添加新的集成。每个服务器都公开工具和资源。客户端进行调解。主机进行协调。模型则贯穿其中，了解可用的工具。
- en: MCP components
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MCP组件
- en: 'At the time of writing, MCP defines three key components that external systems
    can expose:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，MCP定义了外部系统可以公开的三个关键组件：
- en: Tools
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 工具
- en: These model-controlled functions are callable operations that the language model
    can invoke during a session. Think of tools as function endpoints—when a model
    determines that it needs to take an action, such as retrieving a document, querying
    an API, or triggering a workflow, it does so by calling a tool. Tools are defined
    with a clear input/output schema and are registered with the host application
    at runtime.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些由模型控制的函数是语言模型在会话期间可以调用的可调用操作。将工具视为函数端点——当模型确定需要采取行动时，例如检索文档、查询API或触发工作流，它通过调用工具来完成。工具使用清晰的输入/输出模式定义，并在运行时注册到主机应用程序。
- en: Resources
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 资源
- en: These application-controlled data endpoints are read-only data sources that
    the model can reference to enrich its context. Unlike tools, resources do not
    execute logic. Instead, they expose structured data—such as a list of files, user
    profiles, or metadata—that the model can access via lookups. These are useful
    when the application wants to give the model direct access to information that
    doesn’t require active computation or external side effects.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些由应用程序控制的数据端点是模型可以引用以丰富其上下文的只读数据源。与工具不同，资源不执行逻辑。相反，它们暴露结构化数据——例如文件列表、用户配置文件或元数据——模型可以通过查找访问这些数据。当应用程序希望模型直接访问不需要主动计算或外部副作用的信息时，这些数据非常有用。
- en: Prompts
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: These user-controlled templates are pre-engineered prompt structures that can
    be presented to the model as part of the context or decision-making path. Prompts
    help guide the model’s behavior using predefined instructions, formats, or strategies.
    They can encapsulate common workflows or suggest best practices for using tools
    and resources effectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些由用户控制的模板是预先设计的提示结构，可以作为上下文或决策路径的一部分呈现给模型。提示通过预定义的指令、格式或策略帮助指导模型的行为。它们可以封装常见的流程或建议有效使用工具和资源的最佳实践。
- en: This is the new system architecture design for Software 3.0\. It’s how we move
    from fragile, one-off agents to systems that scale. With MCP, you can build once
    and use what you build everywhere. The protocol acts like a universal adapter,
    abstracting away the messy details of each tool and giving the model a consistent
    way to act.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是软件3.0的新系统架构设计。这是我们如何从脆弱的、一次性的代理过渡到可扩展的系统。有了MCP，您可以一次构建，然后在任何地方使用您所构建的内容。该协议就像一个通用适配器，抽象出每个工具的杂乱细节，并为模型提供一个一致的行为方式。
- en: MCP implementation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MCP实现
- en: Now, let’s look into how MCP works. MCP is implemented as a client–server protocol.
    The host application—which could be a desktop assistant, IDE plugin, or custom
    agent—runs one or more MCP clients. Each client establishes a one-to-one connection
    with an *MCP server*, which is an external system exposing tools, resources, and
    prompts.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看MCP是如何工作的。MCP作为客户端-服务器协议实现。主机应用程序——可能是桌面助手、IDE插件或自定义代理——运行一个或多个MCP客户端。每个客户端与一个**MCP服务器**建立一对一的连接，该服务器是一个外部系统，它公开工具、资源和提示。
- en: The protocol begins with a *handshake phase*, where the client and server exchange
    version and capability metadata. This ensures compatibility and allows the client
    to dynamically understand what the server offers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 协议从**握手阶段**开始，客户端和服务器交换版本和能力元数据。这确保了兼容性，并允许客户端动态理解服务器提供的内容。
- en: Once the connection is established, the *discovery phase* begins. The client
    queries the server to enumerate all available tools, resources, and prompts. The
    server responds with structured metadata that the client can then serialize and
    expose to the model through the host application.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立连接，就进入**发现阶段**。客户端查询服务器以列出所有可用的工具、资源和提示。服务器响应以结构化元数据，客户端可以将其序列化并通过主机应用程序暴露给模型。
- en: During the *interaction phase*, when a model identifies a need to call a tool,
    it emits a structured function call (using JSON or a similar schema). The host
    routes this request through the client to the server, which executes the tool
    logic and returns the result. The host application can then inject the output
    back into the model’s context, allowing the LLM to incorporate external data into
    its next reasoning step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在**交互阶段**，当模型确定需要调用工具时，它发出一个结构化的函数调用（使用JSON或类似模式）。主机通过客户端将此请求路由到服务器，服务器执行工具逻辑并返回结果。然后，主机应用程序可以将输出注入到模型的上下文中，允许LLM将其外部数据纳入其下一步推理步骤。
- en: The same pattern applies for resource lookups—the model can request a resource
    by ID or query, and the host will retrieve the data from the MCP server via the
    client. Similarly, when using prompts, the host can offer predefined templates
    to the model based on the MCP server’s response during discovery.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源查找，相同的模式适用——模型可以通过 ID 或查询请求资源，主机将通过客户端从 MCP 服务器检索数据。同样，当使用提示时，主机可以根据 MCP
    服务器在发现过程中的响应向模型提供预定义的模板。
- en: All of this happens asynchronously and incrementally. The model doesn’t need
    to preload every tool or piece of data. It queries what it needs in real time,
    based on the evolving conversation or task state. This architecture introduces
    a high degree of modularity. A single host can connect to multiple MCP servers
    simultaneously. Each server needs to implement the protocol only once, and it
    becomes compatible with any number of model-based clients or applications that
    follow ​MCP.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作都是异步和增量进行的。模型不需要预先加载每个工具或数据。它根据不断发展的对话或任务状态实时查询所需的内容。这种架构引入了高度模块化。单个主机可以同时连接到多个
    MCP 服务器。每个服务器只需实现一次协议，即可与任何数量的基于模型的客户端或应用程序兼容，这些客户端或应用程序遵循 MCP。
- en: Example MCP project
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 MCP 项目
- en: 'Here is a quick example of how to build a simple server with Python to fetch
    weather data using MCP. This code connects to an MCP server running a weather
    data service. It sends a request for weather data about Lisbon, Portugal, including
    temperature in Celsius. It lists available resources and tools on the server.
    It calls a tool (`weather-tool`) to fetch the weather data. Optionally, it reads
    a file resource containing a weather report. Finally, it prints out the weather
    data received from the server:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用 Python 构建简单服务器的快速示例，以使用 MCP 获取天气数据。此代码连接到一个运行天气数据服务的 MCP 服务器。它发送关于葡萄牙里斯本的天气数据请求，包括摄氏度温度。它列出服务器上的可用资源和工具。它调用一个工具（`weather-tool`）来获取天气数据。可选地，它读取包含天气报告的文件资源。最后，它打印出从服务器接收到的天气数据：
- en: 'Step 1: Set up the server parameters'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步：设置服务器参数
- en: 'First, define the parameters required to connect to the MCP server, including
    the executable, server script, and any optional environment variables:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义连接到 MCP 服务器所需的参数，包括可执行文件、服务器脚本以及任何可选的环境变量：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Define sampling callback'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步：定义采样回调
- en: 'Create an optional callback function that handles incoming data from the server.
    In this case, it processes weather-related messages:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个可选的回调函数来处理来自服务器的数据。在这种情况下，它处理与天气相关的消息：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 3: Establish a connection to the server'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步：建立与服务器的连接
- en: 'Here we use `stdio_client` to connect to the MCP server:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `stdio_client` 连接到 MCP 服务器：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code has `stdio_client(server_params)` open a connection to the MCP server
    using the parameters you defined earlier. It returns two objects, `read` and `write`,
    which are the communication channels for reading from and writing to the server.
    `ClientSession` is used to ​create a session that will handle all communication
    with the server. It is passed the `read` and `write` channels and the `sampling_callback`
    function to process messages from the server.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码通过 `stdio_client(server_params)` 使用您之前定义的参数打开与 MCP 服务器的连接。它返回两个对象，`read`
    和 `write`，它们是从服务器读取和写入的通信通道。`ClientSession` 用于创建一个将处理与服务器所有通信的会话。它传递 `read` 和
    `write` 通道以及 `sampling_callback` 函数来处理来自服务器的消息。
- en: Finally, `async with`ensures that the connection is automatically closed once
    the block of code is done executing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`async with` 确保一旦代码块执行完毕，连接将自动关闭。
- en: 'Step 4: Initialize the session'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步：初始化会话
- en: 'This line initializes the session, setting up necessary configurations or authentication
    with the server. It must be called before performing any actions like listing
    prompts or using tools:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码初始化会话，设置必要的配置或与服务器进行身份验证。在执行任何操作（如列出提示或使用工具）之前必须调用此代码：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 5: List the available prompts'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 步：列出可用的提示
- en: 'You can request a list of available prompts from the server. In this case,
    you need to retrieve a specific prompt from the server, passing any required arguments,
    such as a city name for the weather data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从服务器请求可用提示的列表。在这种情况下，您需要从服务器检索特定的提示，传递任何所需的参数，例如用于天气数据的城市名称：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 6: List the available resources and tools'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 步：列出可用的资源和工具
- en: 'Next, you need to fetch two lists from the server: one of available resources
    and one of available tools. *Resources* might include files, API keys, or data
    that the server can access. *Tools* are functions or services that the server
    can call to perform specific tasks, like fetching weather data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要从服务器获取两个列表：一个是可用的资源列表，另一个是可用的工具列表。*资源*可能包括文件、API密钥或服务器可以访问的数据。"工具"是服务器可以调用的函数或服务，以执行特定任务，例如获取天气数据：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 7: Call a tool'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：调用工具
- en: 'This will call a specific tool on the server, again passing any necessary arguments
    (city name, temperature units). The tool will process the request and return the
    result:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在服务器上调用一个特定的工具，再次传递任何必要的参数（城市名称、温度单位）。该工具将处理请求并返回结果：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Optionally, you can also fetch and read a resource from the ​server:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您还可以从服务器获取并读取一个资源：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Step 8: Display the result'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：显示结果
- en: 'Print or process the result from the tool call (in this case, the weather data):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 打印或处理工具调用的结果（在这种情况下，天气数据）：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Step 9: Run the code'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：运行代码
- en: 'Finally, use an event loop (`asyncio.run()`) to run the asynchronous function
    and complete the entire process:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用事件循环（`asyncio.run()`）来运行异步函数并完成整个流程：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: MCP and the future of large language models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MCP与大型语言模型的未来
- en: 'What’s most important about MCP is that it opens the door to something we couldn’t
    do before: true agentic reasoning across systems. Instead of loading up a model
    with all the knowledge in the world, we give it the power to seek, to ask, to
    call upon the right tool at the right time. That’s how intelligence works in the
    real world: not by knowing everything but by knowing where to look and how to
    act.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: MCP最重要的地方在于它为我们打开了一扇以前无法打开的门：跨系统真正的代理推理。我们不是将世界上所有的知识都加载到模型中，而是赋予它寻求、提问、在正确的时间调用正确工具的能力。这就是现实世界中智能工作的方式：不是通过知道一切，而是通过知道在哪里寻找和如何行动。
- en: As the concept of MCP becomes more established, a new class of frameworks and
    tools has begun to emerge. These frameworks, while not always explicitly labeled
    as MCP based, follow similar principles of modularity and structured interaction
    between language models and external systems. Tools like LangChain, DSPy, and
    Gorilla exemplify this shift. They enable developers to build more efficient programs
    by managing execution across context windows, scaling interactions with language
    models, and integrating various tools in a consistent, predictable manner. The
    core logic is shared; i.e., modularize the architecture, separate concerns, and
    treat the language model as a flexible tool rather than a monolithic entity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着MCP概念的日益确立，一类新的框架和工具已经开始出现。这些框架，尽管并不总是明确标记为基于MCP，但遵循类似的模块化和语言模型与外部系统之间结构化交互的原则。LangChain、DSPy和Gorilla等工具体现了这一转变。它们使开发者能够通过管理跨上下文窗口的执行、扩展与语言模型的交互以及以一致、可预测的方式集成各种工具来构建更高效的程序。核心逻辑是共享的；即模块化架构、分离关注点，并将语言模型视为一个灵活的工具而不是一个整体实体。
- en: This approach is more than just a trend; it’s the beginning of a broader shift
    in how we interact with language models. As these frameworks mature, the interaction
    with models will evolve from simple prompt-based queries to more complex programmatic
    workflows. We will build layers of logic and structure around models, much like
    we would with any backend system. As the capabilities of language models expand,
    the MCP layer itself will evolve to handle more advanced functionalities, such
    as composable functions, modular logic, conditional execution, tool invocation,
    and memory manipulation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅仅是一个趋势，它标志着我们与语言模型互动方式的更广泛转变的开始。随着这些框架的成熟，与模型的交互将从简单的基于提示的查询演变为更复杂的程序化工作流程。我们将围绕模型构建逻辑和结构，就像我们会对任何后端系统做的那样。随着语言模型能力的扩展，MCP层本身也将演变为处理更高级的功能，例如可组合函数、模块化逻辑、条件执行、工具调用和内存操作。
- en: Looking ahead, the context  window will continue to grow. Tools like virtualized
    LLMs (vLLMs) will enable persistence across sessions, allowing a model’s state
    to carry over from one interaction to the next. This will enable more sophisticated
    workflows, where a user no longer just “prompts” a model. Instead, they will interact
    with a fully integrated system—an LLM-native environment that includes memory,
    a task stack, logs, and capabilities. The line between the language model and
    the broader infrastructure will blur, creating an environment where the model
    can be treated as a dynamic, stateful component within a larger system. In the
    future, this protocol may be invisible to users, just as HTTP is invisible when
    you browse the web. But it will shape the way every AI application is built. It
    will become the backbone of multi-agent systems, agentic workflows, and the infrastructure
    that supports open-ended intelligence. It doesn’t just make LLMs smarter—it gives
    them hands, eyes, and a map of the world they operate in.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，上下文窗口将继续扩大。像虚拟化LLM（vLLM）这样的工具将允许会话间的持久性，使得模型的状态可以从一次交互延续到下一次。这将使更复杂的流程成为可能，其中用户不再只是“提示”模型。相反，他们将与一个完全集成的系统进行交互——一个包含记忆、任务堆栈、日志和能力的LLM原生环境。语言模型和更广泛的基础设施之间的界限将变得模糊，创造出一个模型可以被视为更大系统中动态、有状态的组件的环境。在未来，这个协议可能对用户来说是不可见的，就像浏览网页时HTTP不可见一样。但它将塑造每个AI应用程序的构建方式。它将成为多代理系统、代理工作流程和支持开放性智能的基础设施的核心。它不仅使LLM更智能，还给了它们手、眼和它们运作的世界地图。
- en: This evolution of MCP will lead to environments where tasks are not limited
    to short, isolated interactions but can span multiple steps and contexts, with
    the model acting as a true agent capable of interacting with a wider range of
    tools and resources. This shift is already happening, albeit incrementally, and
    will become a fundamental part of how we build AI-driven applications in the future.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: MCP的这一演变将导致环境中的任务不再局限于短期的、孤立的交互，而是可以跨越多个步骤和上下文，模型作为一个真正的代理，能够与更广泛的工具和资源进行交互。这种转变已经在发生，尽管是渐进式的，并将成为我们未来构建AI驱动应用程序的基本组成部分。
- en: Agent-to-Agent Protocol
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理到代理协议
- en: MCP brought structure to how language models interface with tools, memory, and
    external logic, giving us a formal way to treat models like programmable systems.
    But it assumes there is a single actor—a single agent querying tools, running
    logic, and managing state. What happens when there’s more than one agent?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: MCP为语言模型如何与工具、记忆和外部逻辑接口提供了结构，为我们提供了一种正式的方式来处理模型，就像可编程系统一样。但它假设存在一个单一的参与者——一个单一的代理查询工具、运行逻辑和管理状态。当存在多个代理时会发生什么？
- en: As agentic systems grow in complexity, so does the need for coordination. An
    agent that schedules meetings might need to talk to another that handles email
    summaries, which in turn might call a tool that fetches flight times. So the next
    wave of model-based systems doesn’t live in isolation—it lives in a swarm. Imagine
    multiple agents with specialized roles, distributed across platforms and services,
    all trying to talk to one another. That’s where Agent2Agent protocol (A2A) steps
    in.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代理系统复杂性的增加，协调的需求也在增加。一个安排会议的代理可能需要与处理电子邮件摘要的另一个代理交谈，而这个代理可能需要调用一个获取航班时间的工具。因此，下一波基于模型的系统不会孤立存在——它生活在蜂群中。想象一下，多个具有专门角色的代理，分布在平台和服务中，都在试图相互交谈。这就是代理到代理协议（A2A）介入的地方。
- en: A2A, [introduced by Google as an open standard](https://oreil.ly/PuDm-), is
    like a foundational layer for interoperability. It defines how AI agents identify
    each other, communicate, negotiate tasks, and share results. It picks up where
    MCP stops. MCP gives one agent a structure; A2A gives a group of agents a shared
    language.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: A2A，由谷歌作为开放标准[引入](https://oreil.ly/PuDm-)，就像是一个互操作性基础层。它定义了AI代理如何识别彼此、通信、协商任务和共享结果。它从MCP停止的地方开始。MCP给一个代理提供了一个结构；A2A给一组代理提供了一个共享的语言。
- en: Without a shared protocol, these connections are brittle, with custom integrations,
    hard-coded dependencies, and vendor lock-in. Every agent has its own dialect,
    its own handshake. A2A solves this by offering a standard. It doesn’t just manage
    centralized orchestration—it enables decentralized collaboration. One agent doesn’t
    need to know how another works under the hood. It just needs to know what that
    agent can do and how to call it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 没有共享协议，这些连接是脆弱的，具有定制的集成、硬编码的依赖关系和供应商锁定。每个代理都有自己的方言，自己的握手。A2A 通过提供标准来解决这一问题。它不仅管理集中式编排，还促进了去中心化协作。一个代理不需要知道另一个代理在底层是如何工作的。它只需要知道那个代理能做什么以及如何调用它。
- en: At its core, A2A is a communication protocol for autonomous agents. It defines
    a set of conventions for secure, structured, and extensible agent interactions.
    It [abstracts away](https://oreil.ly/Ri5XT) the vendor-specific details and focuses
    on capability discovery, message exchange, task delegation, and identity verification.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，A2A 是一种用于自主代理的通信协议。它定义了一套用于安全、结构化和可扩展的代理交互的约定。它[抽象出](https://oreil.ly/Ri5XT)了供应商特定的细节，并专注于能力发现、消息交换、任务委派和身份验证。
- en: 'Agent cards form the basis of discovery and negotiation. These are JSON-based
    documents that advertise who the agents are, what they can do, how to contact
    them, and what security policies they follow. Two agents can expose their cards,
    find each other, assess their mutual compatibility, and begin collaborating—all
    without tight coupling. The  core components of A2A include:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 代理卡是发现和协商的基础。这些是基于 JSON 的文档，用于宣传代理是谁、能做什么、如何联系他们以及他们遵循的安全策略。两个代理可以公开他们的卡，找到彼此，评估他们的互操作性，并开始协作——所有这些都不需要紧密耦合。A2A
    的核心组件包括：
- en: Agent identity
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 代理身份
- en: Each  agent signs its messages cryptographically, so you know who you’re talking
    to.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理都对其消息进行加密签名，这样您就知道您在跟谁说话。
- en: Agent cards
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 代理卡
- en: These  include structured metadata that defines an agent’s capabilities, interfaces,
    and protocols.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括定义代理能力、接口和协议的结构化元数据。
- en: Capability discovery
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 能力发现
- en: Agents query each other to find out what functions or tasks the other supports.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 代理相互查询以找出对方支持的功能或任务。
- en: Task negotiation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 任务协商
- en: Agents can delegate work, propose plans, or asynchronously coordinate workflows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以委派工作、提出计划或异步协调工作流程。
- en: Secure messaging
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 安全消息
- en: All communication is authenticated, encrypted, and audit friendly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通信都是经过身份验证的、加密的，并且便于审计。
- en: Extensibility
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: A2A is built to evolve. You can extend the schema, define your own agent roles,
    and create domain-specific logic.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: A2A 是为了进化而构建的。您可以扩展模式，定义自己的代理角色，并创建特定领域的逻辑。
- en: A typical A2A interaction follows a few predictable steps, as shown in [Figure 3-3](#fig0303).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的 A2A 交互遵循几个可预测的步骤，如图 [图 3-3](#fig0303) 所示。
- en: '![](assets/llmo_0303.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0303.png)'
- en: Figure 3-3\. Typical A2A interactions
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 典型的 A2A 交互
- en: 'Let’s look at each step in more detail:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看每一步：
- en: 'Step 1: Discovery'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步：发现
- en: An agent looks up another agent’s card, either from a registry or via direct
    request.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 代理查找另一个代理的卡，无论是从注册表还是通过直接请求。
- en: 'Step 2: Validation'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步：验证
- en: The calling agent checks the other agent’s identity and credentials via its
    agent card.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 调用代理通过其代理卡检查其他代理的身份和凭证。
- en: 'Step 3: Capability matching'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步：能力匹配
- en: The calling agent examines the other agent’s listed functions and decides how
    to delegate a task.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 调用代理检查其他代理列出的功能，并决定如何委派任务。
- en: 'Step 4: Task delegation'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步：任务委派
- en: The calling agent sends a structured request. The called agent may accept it,
    reject it, or propose an alternative.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 调用代理发送一个结构化请求。被调用代理可能接受它、拒绝它或提出替代方案。
- en: 'Step 5: Execution'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 步：执行
- en: If it accepts the request, the called agent performs the task, optionally invoking
    its own tools or subagents.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它接受请求，被调用代理执行任务，可选地调用其自己的工具或子代理。
- en: 'Step 6: Response'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 步：响应
- en: The called agent returns a result to the calling agent, along with any relevant
    logs, metrics, or follow-up capabilities.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 被调用代理将结果返回给调用代理，并附带任何相关的日志、指标或后续能力。
- en: Each of these steps is modular. You can plug them into any system—whether you’re
    building agents on top of LangChain, Haystack, or your own platform. And because
    it’s an open spec, you don’t have to rely on Google—or any one vendor—to make
    it work.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤中的每一个都是模块化的。您可以将它们插入到任何系统中——无论您是在 LangChain、Haystack 或您自己的平台上构建代理。而且因为它是一个开放规范，您不必依赖
    Google 或任何单一供应商来使其工作。
- en: The Rise of vLLMs and Multimodal LLMs
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: vLLMs 和多模态 LLMs 的兴起
- en: So far, most of the LLM workflows we’ve discussed in this book revolve around
    text. There are tokens in and tokens out; i.e., the model interprets language,
    transforms it, and spits back structured thoughts. But the world isn’t made of
    words alone. We see. We hear. We act in spaces where text is just one part of
    the information stream.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在这本书中讨论的大多数LLM工作流程都是围绕文本的。有输入标记和输出标记；即，模型解释语言，将其转换，并吐出结构化的思想。但世界不仅仅由文字组成。我们看到。我们听到。我们在有文本只是信息流一部分的空间中行动。
- en: 'Multimodal models are the next step. These models don’t just read but also
    see, listen, describe, and even generate across modes. These systems are built
    to handle input and output in multiple modalities: text, image, audio, video,
    and sometimes even tabular data or code. A model is multimodal if it accepts more
    than one modality as input or produces outputs in more than one. The most common
    form of multimodal model today is the vision–language model (VLM). VLMs accept
    both text and images as input and generate text as output. Some also generate
    images or annotate images with bounding boxes and captions.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型是下一步。这些模型不仅阅读，还能看、听、描述，甚至能在多个模态之间生成。这些系统被构建来处理多种模态的输入和输出：文本、图像、音频、视频，有时甚至是表格数据或代码。一个模型是多模态的，如果它接受多种模态作为输入或产生多种模态的输出。今天最常见的多模态模型形式是视觉-语言模型（VLM）。VLM接受文本和图像作为输入，并生成文本作为输出。其中一些也生成图像或用边界框和标题注释图像。
- en: 'This shift unlocks new kinds of capabilities that were previously impossible
    in pure language systems. Three things in particular make multimodal LLMs viable
    now:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转变解锁了在纯语言系统中之前不可能的新能力。有三件事特别使得多模态LLM现在可行：
- en: Transformer generalization
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer泛化
- en: Transformers, the architecture behind LLMs, don’t care if a token is a word
    or a pixel. Once data is embedded into the right format, it becomes just another
    stream to process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器，LLM背后的架构，不在乎一个标记是单词还是像素。一旦数据被嵌入到正确的格式，它就变成了另一个要处理的流。
- en: Training scale
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练规模
- en: Pretraining now happens on massive corpora that include image–text pairs (like
    LAION or COCO), enabling vision–language alignment at scale.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练现在在包含图像-文本对（如LAION或COCO）的庞大语料库上进行，这使大规模的视觉-语言对齐成为可能。
- en: Tooling and open access
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 工具和开放访问
- en: Projects like [CLIP](https://oreil.ly/CZ0KA), [Flamingo](https://oreil.ly/cq0iR),
    [BLIP](https://oreil.ly/fK3gC), and [LLaVA](https://oreil.ly/ZeNRn) have created
    reusable architectures and checkpoints. You no longer need to be OpenAI or Google
    to train or fine-tune one.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 像CLIP（[CLIP](https://oreil.ly/CZ0KA)）、Flamingo（[Flamingo](https://oreil.ly/cq0iR)）、BLIP（[BLIP](https://oreil.ly/fK3gC)）和LLaVA（[LLaVA](https://oreil.ly/ZeNRn)）这样的项目已经创建了可重用的架构和检查点。您不再需要是OpenAI或Google才能训练或微调一个模型。
- en: 'Here’s how the pipeline typically works:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是管道通常的工作方式：
- en: 1\. Input embeddings
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 输入嵌入
- en: The text is tokenized. Images are passed through a visual encoder (often a ViT).
    Both get embedded into the same vector space.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 文本被分词。图像通过视觉编码器（通常是ViT）传递。两者都被嵌入到相同的向量空间中。
- en: 2\. Fusion
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 融合
- en: The model combines visual and textual embeddings, attending over both. This
    is where reasoning happens, as the model aligns what’s seen with what’s said.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 模型结合视觉和文本嵌入，关注两者。这就是推理发生的地方，因为模型将所见与所说对齐。
- en: 3\. Output
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 输出
- en: The model produces a text output (like a caption, description, or answer) conditioned
    on both modalities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 模型根据两种模态生成文本输出（如标题、描述或答案）。
- en: 'Some models, like CLIP, don’t generate text at all. Instead, they match image
    embeddings with text embeddings. Others, like LLaVA or MiniGPT-4, are chat based;
    i.e., they can answer visual questions, describe images, or interpret charts and
    graphs through conversation. Some of the most well-known multimodal models, as
    of mid-2025, include:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型，如CLIP，根本不生成文本。相反，它们将图像嵌入与文本嵌入相匹配。其他模型，如LLaVA或MiniGPT-4，是基于聊天的；即，它们可以回答视觉问题、描述图像或通过对话解释图表和图形。截至2025年中，一些最知名的多模态模型包括：
- en: CLIP
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP
- en: CLIP is a model from OpenAI that learns joint vision–text embeddings. It’s not
    generative—it’s matching based. You can find the image that matches a sentence
    or the sentence that describes an image.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是来自OpenAI的一个模型，它学习联合视觉-文本嵌入。它不是生成性的——它是基于匹配的。您可以找到与句子匹配的图像或描述图像的句子。
- en: BLIP and BLIP-2
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP和BLIP-2
- en: These are bootstrapped vision–language models, pretrained to describe and reason
    about images in natural language. BLIP-2 uses a frozen image encoder with a lightweight
    query transformer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是自举视觉-语言模型，预训练用于用自然语言描述和推理图像。BLIP-2使用冻结的图像编码器和一个轻量级的查询变换器。
- en: MiniGPT-4
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: MiniGPT-4
- en: MiniGPT-4 combines a visual encoder with a frozen LLM, aligning their representations
    with minimal training. It basically acts like a visual chatbot.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: MiniGPT-4 将视觉编码器与冻结的 LLM 结合起来，通过最小训练使它们的表示对齐。它基本上就像一个视觉聊天机器人。
- en: LLaVA
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA
- en: LLaVA, which is built on LLaMA and CLIP-style vision encoders, allows for interactive
    visual dialogue and visual question answering (VQA).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLaMA 和 CLIP 风格视觉编码器的 LLaVA，允许进行交互式视觉对话和视觉问答（VQA）。
- en: Flamingo
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo
- en: Flamingo, from DeepMind, is a powerful closed-source model that has set new
    benchmarks for few-shot multimodal reasoning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 DeepMind 的 Flamingo 是一个强大的闭源模型，它为少量多模态推理设定了新的基准。
- en: BentoML and LLM Foundry
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: BentoML 和 LLM Foundry
- en: BentoML and LLM Foundry, while not models themselves, provide deployment, serving,
    and training infrastructure to fine-tune and run VLMs on your own stack.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: BentoML 和 LLM Foundry，虽然不是模型本身，但提供了部署、服务和训练基础设施，以在您的堆栈上微调和运行 VLM。
- en: 'Multimodal agents unlock a huge new surface area for ways to use AI. They can:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态代理解锁了使用 AI 的新巨大领域。它们可以：
- en: Answer questions about an image, video frame, or document screenshot
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答关于图像、视频帧或文档截图的问题
- en: Parse charts, tables, or handwritten text
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析图表、表格或手写文本
- en: Summarize slides
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概括幻灯片
- en: Transcribe whiteboards
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转录白板
- en: Describe UI layouts
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述用户界面布局
- en: Navigate the physical world in robotics or assistive agents
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器人或辅助代理中导航物理世界
- en: Build more “human-like” interfaces that feel less robotic and more perceptual
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建更“人性化”的界面，感觉上不那么机械，更具有感知性
- en: It’s about reasoning across modalities, treating language and vision as two
    views of the same world. Just like in language, the shift is from pattern recognition
    to *contextual reasoning*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这关乎于跨模态推理，将语言和视觉视为同一世界的两种视角。就像在语言中，转变是从模式识别到*情境推理*。
- en: 'The trajectory is clear: modality boundaries are blurring. Future systems will
    handle vision, language, audio, code, and interaction as part of the same cognitive
    loop. Some already do. The open frontier is about building agentic systems that
    *see, decide, and act*—not in separate blocks but as integrated flows.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹是清晰的：模态边界正在模糊。未来的系统将处理视觉、语言、音频、代码和交互，作为同一认知循环的一部分。一些系统已经做到了。开放的前沿是构建能够*看到、决定和行动*的代理系统——不是在独立的模块中，而是在集成流程中。
- en: This changes the architecture of everything. Prompts become interfaces, not
    just instructions. Inputs are multimodal, such as a voice command plus a camera
    feed. Outputs are also multimodal, often including a generated summary and a visual
    markup.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这改变了所有事物的架构。提示成为接口，而不仅仅是指令。输入是多模态的，例如语音命令加上摄像头视频流。输出也是多模态的，通常包括生成的摘要和视觉标记。
- en: Multimodal LLMs thus aren’t just a feature upgrade—they introduce a structural
    change in what language models are. They represent a shift from abstract dialogue
    machines to embodied agents that understand and operate in the real world.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态 LLM 不仅仅是功能升级——它们在语言模型的结构上引入了变革。它们代表了一种从抽象对话机器到理解和在现实世界中操作的具身代理的转变。
- en: The LLMOps Question
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps 问题
- en: The main question that LLMOps teams need to answer is “Does the application
    perform well at a reasonable cost?” Once they can answer yes to this question,
    they can start working on optimizations, trying to extract maximum performance
    at the minimum possible cost. For LLM applications, there are several dimensions
    of performance, but let’s talk about cost first.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps 团队需要回答的主要问题是“应用程序在合理的成本下表现良好吗？”一旦他们可以对这个问题的回答是肯定的，他们就可以开始进行优化，试图以最低的成本获得最大的性能。对于
    LLM 应用程序，有多个性能维度，但让我们先谈谈成本。
- en: For companies that choose to buy LLM services from a cloud provider, costs can
    be measured directly in financial terms (you can use [LLM Price Check](https://oreil.ly/-Ez0q)
    for comparisons). While it’s not trivial to define performance and measure it,
    let’s assume for a moment that the app is already deployed in production and that
    the performance is at the desired level. In that case, if the main goal of the
    company is to maximize profits, the LLMOps team should choose the cloud LLM that
    provides that performance at the lowest cost.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于选择从云服务提供商购买 LLM 服务的公司，成本可以直接用财务术语衡量（您可以使用 [LLM 价格检查](https://oreil.ly/-Ez0q)
    进行比较）。虽然定义性能并衡量它并不简单，但让我们暂时假设应用程序已经部署在生产中，并且性能达到了期望的水平。在这种情况下，如果公司的主要目标是最大化利润，LLMOps
    团队应该选择提供该性能且成本最低的云 LLM。
- en: 'For companies that choose to build LLMs or run LLM applications in their own
    hardware, there’s an additional consideration: the *opportunity cost of building
    versus buying*. (*Opportunity cost* here means the money, time, and market lead
    lost if the organization chooses to build and manage the hardware itself rather
    than outsourcing it using model APIs.) The demand for graphics processing units
    (GPUs) capable of running LLMs is very high, helping to propel the stock prices
    of GPU manufacturers like Nvidia to record levels. In addition to determining
    the cost of running an application, companies that acquire GPUs must answer one
    surprising question: would they make more money simply renting those GPUs? Spheron
    has an excellent [in-depth blog post](https://oreil.ly/6VeFD) on the economics
    and drawbacks of buying versus renting GPUs. In some cases, renting your GPUs
    is more profitable than running your own operations in-house.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于选择在自己的硬件上构建LLM或运行LLM应用的公司，还有一个额外的考虑因素：构建与购买的机会成本。（在这里，“机会成本”指的是如果组织选择自己构建和管理硬件而不是使用模型API外包，所失去的钱、时间和市场领先。）能够运行LLM的图形处理单元（GPU）的需求非常高，这有助于推动像Nvidia这样的GPU制造商的股价达到历史最高水平。除了确定运行应用程序的成本之外，购买GPU的公司还必须回答一个令人惊讶的问题：他们仅仅出租这些GPU就能赚更多的钱吗？Spheron有一个关于购买与租赁GPU的经济效益和缺点的深入[博客文章](https://oreil.ly/6VeFD)。在某些情况下，租赁GPU比内部运行自己的操作更有利可图。
- en: Monitoring Application Performance
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控应用性能
- en: The field of MLOps existed for several years before LLMs, and for many application
    classes, the performance metrics for MLOps and LLMOps are the same or very similar.
    This is because most application performance metrics are domain dependent. For
    example, for an application that uses LLMs to support the sales process, a key
    metric might be the sales conversion rate, which answers the question “Are we
    selling more now that we’ve started using LLMs?” For a human resources application
    that uses LLMs to match candidate resumes to job descriptions, a key metric may
    be the interview-screening success rate, which answers the question “Are we getting
    better candidates in our pipeline now that we’ve started using LLMs?”
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps领域在LLM出现之前就已经存在了几年，对于许多应用类别，MLOps和LLMOps的性能指标是相同的或非常相似。这是因为大多数应用性能指标都是领域相关的。例如，对于一个使用LLM来支持销售流程的应用，一个关键指标可能是销售转化率，它回答了“自从我们开始使用LLM以来，我们的销售是否增加了？”的问题。对于一个使用LLM将候选人简历与职位描述相匹配的人力资源应用，一个关键指标可能是面试筛选成功率，它回答了“自从我们开始使用LLM以来，我们的人才库中的候选人是否更好？”的问题。
- en: 'Calculating these metrics doesn’t depend on the underlying technology. Whether
    the application is using ML, an LLM, or pen and paper to determine which customers
    or candidates to call, these evaluation metrics are calculated the same way. LLMs
    can still provide additional challenges, mainly because they are less deterministic
    and more susceptible to a class of changes called *drift* than ML models are.
    We will discuss drift in detail in [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823).
    For now, it helps to think of it this way: applications that use LLMs frequently
    behave more like processes executed by humans than like computer-based applications,
    especially in terms of output variability. Your application metrics should accommodate
    this variability.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这些指标并不取决于底层技术。无论应用是使用机器学习、LLM还是笔和纸来确定哪些客户或候选人需要联系，这些评估指标的计算方式都是相同的。LLM仍然会带来额外的挑战，主要是因为它们比机器学习模型更不可预测，更容易受到一种称为“漂移”的变化类别的干扰。我们将在[第7章](ch07.html#ch07_evaluation_for_llms_1748896751667823)中详细讨论漂移。目前，可以这样考虑：频繁使用LLM的应用在行为上更类似于由人类执行的过程，而不是基于计算机的应用，特别是在输出可变性方面。您的应用指标应该适应这种可变性。
- en: Measuring a Consumer LLM Application’s Performance
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量消费者LLM应用性能
- en: 'To better illustrate the differences, let’s use an example that ML has solved:
    classifying email as spam and not spam. Let’s assume that the ML version of this
    application uses a popular model like XGBoost and that the LLM version of this
    application uses a popular model like GPT-4o. For now, let’s assume we’re using
    a simple prompt in our LLM-based application. For example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明差异，让我们用一个机器学习已经解决的问题作为例子：将电子邮件分类为垃圾邮件和非垃圾邮件。假设这个应用的机器学习版本使用了一个流行的模型，比如XGBoost，而该应用的LLM版本使用了一个流行的模型，比如GPT-4o。目前，我们假设我们在基于LLM的应用中使用了一个简单的提示。例如：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Spam detection is a classic binary classification problem for which the metrics
    of accuracy, precision, and recall are typically used:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件检测是一个经典的二元分类问题，通常使用准确率、精确率和召回率等指标：
- en: Accuracy
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度
- en: 'Accuracy measures the proportion of correctly classified instances out of the
    total instances. In this case, it indicates the overall percentage of emails that
    the model correctly labeled, either as spam (true positives) or not spam (true
    negatives). The formula for accuracy is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率衡量的是在所有实例中正确分类实例的比例。在这种情况下，它表示模型正确标记的邮件的整体百分比，无论是垃圾邮件（真正阳性）还是非垃圾邮件（真正阴性）。准确率的公式是：
- en: $upper A c c u r a c y equals StartFraction upper T r u e upper P o s i t i
    v e s plus upper T r u e upper N e g a t i v e s Over upper T o t a l upper I
    n s t a n c e s EndFraction$
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: $upper A c c u r a c y equals StartFraction upper T r u e upper P o s i t i
    v e s plus upper T r u e upper N e g a t i v e s Over upper T o t a l upper I
    n s t a n c e s EndFraction$
- en: While accuracy is a helpful general measure, it can be less informative when
    the data is imbalanced—for example, if most emails are not spam, which is a typical
    case.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然准确率是一个有用的总体指标，但当数据不平衡时，它可能不那么具有信息量——例如，如果大多数邮件不是垃圾邮件，这是一个典型的情况。
- en: Precision
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度
- en: 'Precision measures the proportion of true positive predictions (correct spam
    classifications) out of all the instances that the model classified as positive
    (spam). Precision answers the question “Of all emails classified as spam, how
    many actually were spam?” The formula for precision is:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度衡量的是模型将实例分类为正例（垃圾邮件）中真正阳性预测的比例。精确度回答的问题是：“在所有被分类为垃圾邮件的邮件中，有多少实际上是垃圾邮件？” 精确度的公式是：
- en: $upper P r e c i s i o n equals StartFraction upper T r u e upper P o s i t
    i v e s Over upper T r u e upper P o s i t i v e s plus upper F a l s e upper
    P o s i t i v e s EndFraction$
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: $upper P r e c i s i o n equals StartFraction upper T r u e upper P o s i t
    i v e s Over upper T r u e upper P o s i t i v e s plus upper F a l s e upper
    P o s i t i v e s EndFraction$
- en: High precision indicates that when the model classifies an email as spam, it’s
    likely correct. However, high precision may come at the expense of missing some
    actual spam emails, which would lower recall.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 高精确度表明，当模型将邮件分类为垃圾邮件时，它很可能是正确的。然而，高精确度可能以错过一些实际垃圾邮件为代价，这会降低召回率。
- en: Recall
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆
- en: 'Recall (also called *sensitivity* or *true positive rate*) measures the proportion
    of true positive predictions out of all actual positive instances (all actual
    spam emails). Recall answers the question “Of all actual spam emails, how many
    did the model correctly identify as spam?” The formula for recall is:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率（也称为 *灵敏度* 或 *真正阳性率*）衡量的是所有实际阳性实例（所有实际垃圾邮件）中真正阳性预测的比例。召回率回答的问题是：“在所有实际垃圾邮件中，模型正确识别出多少是垃圾邮件？”
    召回率的公式是：
- en: $upper R e c a l l equals StartFraction upper T r u e upper P o s i t i v e
    s Over upper T r u e upper P o s i t i v e s plus upper F a l s e upper N e g
    a t i v e s EndFraction$
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: $upper R e c a l l equals StartFraction upper T r u e upper P o s i t i v e
    s Over upper T r u e upper P o s i t i v e s plus upper F a l s e upper N e g
    a t i v e s EndFraction$
- en: High recall means the model successfully identifies most spam emails, but this
    can sometimes lead to more false positives, which can reduce precision.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率意味着模型成功识别出大多数垃圾邮件，但有时这可能导致更多的误报，从而降低精确度。
- en: Accuracy provides an overall rate of correct classifications, precision shows
    accuracy within the positive predictions, and recall reflects the model’s ability
    to capture actual positives. Together, these metrics offer a well-rounded evaluation
    of the model’s effectiveness in distinguishing between spam and non-spam emails.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率提供了一个总体正确的分类率，精确度显示了正预测中的准确度，而召回率反映了模型捕捉实际正例的能力。这三个指标共同为模型在区分垃圾邮件和非垃圾邮件方面的有效性提供了一个全面的评估。
- en: The choice of whether to prioritize precision or recall depends on the application.
    For instance, in spam filtering, users typically prefer higher recall—that is,
    catching as much spam as possible—even at the risk of flagging some non-spam emails
    as spam. To mitigate the problem of false positives, users have been trained over
    the years to look into their spam folders from time to time. In other classification
    tasks, such as medical diagnoses or fraud detection, your application may prefer
    to prioritize precision to minimize false positives and the associated expenses
    and stress.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 是否优先考虑精确度或召回率的选择取决于应用场景。例如，在垃圾邮件过滤中，用户通常更倾向于较高的召回率——也就是说，尽可能多地捕获垃圾邮件，即使这意味着可能会将一些非垃圾邮件标记为垃圾邮件。为了减轻误报的问题，多年来用户已经被训练定期查看他们的垃圾邮件文件夹。在其他分类任务中，如医学诊断或欺诈检测，您的应用程序可能更倾向于优先考虑精确度，以最小化误报以及相关的费用和压力。
- en: 'In this example, regardless of whether you are using LLMs or classic ML for
    your application, as long as the application produces an output of “spam” or “ham,”
    you can calculate accuracy, precision, and recall and compare the models using
    the metrics above. To do so, you’d use a test dataset for which you have prelabeled
    correct answers: the *ground truth*. You can use this test dataset with your model
    to calculate the metrics above.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，无论您是使用LLMs还是经典ML来构建您的应用程序，只要应用程序产生“垃圾邮件”或“非垃圾邮件”的输出，您就可以计算准确度、精确度和召回率，并使用上述指标比较模型。为此，您需要使用一个具有预标记正确答案的测试数据集：即*真实情况*。您可以使用此测试数据集与您的模型一起计算上述指标。
- en: 'In a machine learning setting, the model output has a meaning: the “spamminess”
    level of the email. Higher numbers mean that the email is more likely to be spam.
    Engineers can increase this application’s precision, at the expense of recall,
    by setting a higher threshold for classifying an email as spam. This reduces the
    chance of misclassifying non-spam emails as spam, which in turn results in fewer
    false positives and raises precision (fewer non-spam emails are flagged as spam).
    However, it may also mean that more spam emails go undetected, which lowers recall.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习环境中，模型输出具有意义：电子邮件的“垃圾邮件程度”。数值越高，电子邮件越有可能被识别为垃圾邮件。工程师可以通过为将电子邮件分类为垃圾邮件设定更高的阈值来提高此应用的精确度，从而牺牲召回率。这减少了将非垃圾邮件错误分类为垃圾邮件的机会，从而减少了误报并提高了精确度（更少的非垃圾邮件被标记为垃圾邮件）。然而，这也可能意味着更多的垃圾邮件未被检测到，从而降低了召回率。
- en: The engineers can also go in the opposite direction, setting a lower threshold.
    Now the model would classify more emails as spam, catching more true spam emails
    and increasing recall, but increasing the likelihood of false positives (non-spam
    emails incorrectly classified as spam) and reducing precision.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师们也可以选择相反的方向，设定一个较低的阈值。现在模型会将更多的电子邮件分类为垃圾邮件，捕获更多的真实垃圾邮件，从而提高召回率，但同时也增加了误报（将非垃圾邮件错误地分类为垃圾邮件）的可能性，并降低了精确度。
- en: By using this process in several values of the model output, you can plot a
    *precision–recall curve*, which is a shortcut to calculate the performance of
    the model under several different settings. The precision–recall curve shows precision
    on the y-axis and recall on the x-axis at different threshold levels. This curve
    allows an MLOps team to visualize how the precision and recall change as the threshold
    varies, showing the trade-offs between them. Since “spamminess” is the default
    output of ML models, you can create this chart easily in an ML setting by simply
    using your test dataset with different classification thresholds, calculating
    precision and recall, and plotting the results.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在模型输出的多个值上使用此过程，您可以绘制一个**精确度-召回率曲线**，这是计算模型在不同设置下的性能的快捷方式。精确度-召回率曲线在y轴上显示不同阈值水平下的精确度，在x轴上显示召回率。这条曲线允许MLOps团队可视化精确度和召回率如何随着阈值的改变而变化，展示了它们之间的权衡。由于“垃圾邮件程度”是机器学习模型的默认输出，您可以通过简单地使用具有不同分类阈值的测试数据集，计算精确度和召回率，并绘制结果，在机器学习环境中轻松创建此图表。
- en: In general, a high-quality model will have a curve that reaches higher precision
    and recall levels, and a low-quality model will have a curve that stays near the
    origin (low precision and recall). You can measure the *area under the curve for
    precision and recall* (AUC-PR) in the graphs in [Figure 3-4](#ch03_figure_3_1748895493826770).
    Higher AUC-PR values indicate a better model, as they suggest that the model maintains
    both high precision and high recall over a range of threshold values. That is,
    a model with a larger area is better overall.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个高质量的模型将有一个曲线，达到更高的精确率和召回率水平，而一个低质量的模型将有一个曲线，保持在原点附近（低精确率和召回率）。你可以在[图3-4](#ch03_figure_3_1748895493826770)中的图表中测量*精确率和召回率的曲线下面积*（AUC-PR）。更高的AUC-PR值表示更好的模型，因为它们表明模型在一系列阈值值上保持了高精确率和高召回率。也就是说，面积更大的模型整体上更好。
- en: '![](assets/llmo_0304.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_0304.png)'
- en: 'Figure 3-4\. An example case where AUR-PC is a better predictor for model performance,
    even though the ROC curve may tell a different story (source: [Fabio Sigrist](https://oreil.ly/InmMu))'
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 这是一个例子，其中AUR-PC是模型性能更好的预测器，尽管ROC曲线可能讲述了一个不同的故事（来源：[Fabio Sigrist](https://oreil.ly/InmMu)）
- en: 'When using an LLM as the engine of your application, you can’t easily calculate
    the area under the curve. The first practical problem is that the output of the
    model does not contain the probability that a given email is spam. One way to
    solve this problem is to modify the prompt and ask for the probability instead
    of the output, as proposed in the 2024 paper [“Calibrating Verbalized Probabilities
    for Large Language Models”](https://oreil.ly/dfHos):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用LLM作为你应用程序的引擎时，你无法轻松地计算曲线下的面积。第一个实际问题就是模型的输出不包含给定电子邮件是垃圾邮件的概率。解决这个问题的方法之一是修改提示并请求概率而不是输出，如2024年论文[“Calibrating
    Verbalized Probabilities for Large Language Models”](https://oreil.ly/dfHos)中提出的：
- en: '[PRE11]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using such a prompt would let you create the graph, but here is the problem
    with LLMs not being deterministic: even at a temperature of zero (as little randomness
    as possible), an LLM may produce wildly different probability numbers for the
    same input email. Temperature, along with several other parameters, such as `frequency_penalty`
    and `presence_penalty`, allows us to define the randomness (or creativity) of
    the model output. A standard OpenAI request with our parameters could look something
    like the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的提示会让你创建图表，但这里的问题是LLMs的非确定性：即使在零温度（尽可能少的随机性）下，LLM也可能为相同的输入电子邮件产生截然不同的概率数字。温度，以及`frequency_penalty`和`presence_penalty`等几个其他参数，使我们能够定义模型输出的随机性（或创造力）。一个标准的OpenAI请求，使用我们的参数可能看起来像以下这样：
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let’s take an example where I use the prompt to classify an email informing
    me of the delivery of components for installing a solar panel.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们举一个例子，其中我使用提示来分类一封通知我太阳能板安装组件交付的电子邮件。
- en: '[PRE13]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I sent requests twice in a row with GPT-4o at temperature 0, and I obtained
    two different probabilities, 0.05 and 0.1\. A third submission resulted in 0.05\.
    Although setting the temperature to 0 is supposed to make the model deterministic,
    clearly this flip-flop in outputs means that you can never assume perfect consistency
    in LLMs. Therefore, you can’t reliably use this probability-plotting method to
    generate a curve and then choose the model with the best area. The typical compute
    metrics, like receiver operating characteristic (ROC) or recall curve, area under
    the curve (AUC), and area under the precision recall curve (AUC-PR),​ assume *stable
    scoring*; that is, the same input gives the same score every time. However, for
    LLMs, these curves become noisy, and the standard methods of evaluation become
    unreliable. The evaluation system can falsely assert that one “version” of the
    model is better than the other, simply because of internal randomness.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我连续两次使用GPT-4o在温度0的情况下发送请求，并获得了两个不同的概率，0.05和0.1。第三次提交的结果是0.05。尽管将温度设置为0应该使模型确定性，但显然这种输出上的波动意味着你永远不能假设LLMs（大型语言模型）具有完美的稳定性。因此，你不能可靠地使用这种概率绘图方法来生成曲线，然后选择具有最佳面积的模型。典型的计算指标，如接收者操作特征（ROC）或召回率曲线、曲线下面积（AUC）和精确率召回率曲线下的面积（AUC-PR），假设*稳定的评分*；也就是说，相同的输入每次都会得到相同的分数。然而，对于LLMs，这些曲线变得嘈杂，标准评估方法变得不可靠。评估系统可能会错误地断言一个“版本”的模型比另一个更好，仅仅是因为内部随机性。
- en: Choosing the Best Model for Your Application
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为您的应用程序选择最佳模型
- en: One of the most common tasks in both MLOps and LLMOps is to decide whether a
    new version of a model is better than an existing version. This is sometimes called
    the *champion/challenger test* or *A/A test* in which the model that is currently
    in production is called the “champion,” and the new model is called the “challenger.”
    If the challenger proves better than the champion, it replaces the champion and
    takes its place in production. The proof is usually done by evaluating both models
    by a metric or a collection of metrics.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLOps和LLMOps中，最常见的任务之一是决定一个模型的新版本是否优于现有版本。这有时被称为*冠军/挑战者测试*或*A/A测试*，其中当前在生产中的模型被称为“冠军”，而新模型被称为“挑战者”。如果挑战者证明优于冠军，它将取代冠军并在生产中占据其位置。证明通常是通过评估两个模型的一个或多个指标来完成的。
- en: To handle the inherent variability of LLM outputs, you’ll need to calculate
    distributions over multiple samples rather than rely on single-point metrics like
    AUC. In practice, this means running each test dataset several times to obtain
    a range of outcomes, allowing you to calculate statistical distributions such
    as the mean, standard deviation, and confidence intervals for the performance
    metric you’re evaluating.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理LLM输出的固有变异性，你需要计算多个样本的分布，而不是依赖于AUC等单一指标。在实践中，这意味着运行每个测试数据集多次以获得一系列结果，从而允许你计算你正在评估的性能指标的统计分布，如均值、标准差和置信区间。
- en: By gathering these distributions, you can gain insights into the stability and
    reliability of the model’s responses. These metrics help determine whether the
    challenger genuinely outperforms the champion or if differences are simply due
    to inherent model variability.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集这些分布，你可以深入了解模型响应的稳定性和可靠性。这些指标有助于确定挑战者是否真正优于冠军，或者差异仅仅是由于模型固有的变异性。
- en: For example, rather than using a single precision or recall figure, you would
    calculate the average precision and recall for both models over a large number
    of tests, recording the variance in these scores. The resulting distributions
    allow you to perform significance testing, or a [*t*-test](https://oreil.ly/s3VHJ),
    which can help you determine if the difference between the two models is statistically
    significant. This approach accounts for the model’s variability, offering a clearer
    picture of whether the challenger consistently performs better than the champion
    across a range of scenarios and inputs. You can use the A/A test to see if your
    process works. Running your decision framework on the same model twice might produce
    results that have different point estimates, but the difference between them should
    not be statistically significant.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，而不是使用单一的精确度或召回率数字，你会在大量测试中计算两个模型的平均精确度和召回率，并记录这些分数的方差。这些结果分布允许你执行显著性测试，或[*t*-检验](https://oreil.ly/s3VHJ)，这可以帮助你确定两个模型之间的差异是否具有统计学意义。这种方法考虑了模型的变异性，为你提供了一个更清晰的图景，即挑战者是否在一系列场景和输入中始终优于冠军。你可以使用A/A测试来查看你的流程是否有效。在同一个模型上运行你的决策框架两次可能会产生具有不同点估计的结果，但它们之间的差异不应具有统计学意义。
- en: Ultimately,  calculating distributions (see the code in [Example 3-1](#ch03_example_1_1748895493838650))
    rather than relying on single metrics provides a more robust framework for comparing
    LLM-based applications, helping mitigate the challenges posed by the nondeterministic
    nature of LLMs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，计算分布（参见[示例 3-1](#ch03_example_1_1748895493838650)中的代码）而不是依赖于单一指标，为比较基于LLM的应用提供了一个更稳健的框架，有助于减轻LLM非确定性本质带来的挑战。
- en: Example 3-1\. Calculating two models’ distributions of precision and recall
    over a large number of tests
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1\. 在大量测试中计算两个模型的精确度和召回率分布
- en: '[PRE14]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By evaluating performance over large test sets and using statistical tools to
    interpret these results, you can better determine if the challenger model is a
    worthwhile replacement for the champion.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估大量测试集的性能并使用统计工具来解释这些结果，你可以更好地确定挑战者模型是否是冠军模型的值得替换的版本。
- en: Other Application Metrics
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他应用指标
- en: Although we used precision and recall in the preceding example for simplicity,
    many other metrics are used in practice. Applications like recommendation systems
    and ranked-choice applications (such as search algorithms) often use *mean average
    precision* (MAP) to evaluate ranking quality. MAP calculates the average precision
    for each query or user and emphasizes placing relevant items higher in the results
    list. MAP gives higher scores to models that rank relevant items at the top, making
    it particularly useful for applications where the order of results is critical.
    This is essential in contexts like ecommerce search, where users are more likely
    to click on the first few results, and high MAP scores indicate that relevant
    items are being effectively prioritized.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们为了简单起见在先前的例子中使用了精确率和召回率，但在实践中还使用了许多其他指标。像推荐系统和排序选择应用（如搜索算法）这样的应用通常使用*平均平均精度*（MAP）来评估排序质量。MAP计算每个查询或用户的平均精度，并强调将相关项目放置在结果列表的较高位置。MAP对将相关项目排在顶部的模型给予更高的评分，这使得它在结果顺序至关重要的应用中特别有用。这在电子商务搜索等环境中至关重要，因为用户更有可能点击前几个结果，而高MAP分数表明相关项目正在被有效地优先考虑。
- en: Another widely used metric for ranked applications is *normalized discounted
    cumulative gain* (NDCG), which evaluates the relevance of results while accounting
    for their positions. NDCG applies a discount factor that reduces the importance
    of relevant items that appear lower in the ranking, making it ideal for systems
    that need to surface relevant content at the very top. For example, in a news
    recommendation app, users are likely to click only on the first few articles,
    so high NDCG scores indicate that the most relevant articles are being given prime
    spots.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的排序应用指标是*归一化折现累积收益*（NDCG），它评估结果的相关性，同时考虑它们的排名。NDCG应用一个折现因子，降低排名中较低位置的相关项目的重要性，使其非常适合需要将相关内容置于顶部的系统。例如，在新闻推荐应用中，用户可能只会点击前几篇文章，因此高NDCG分数表明最相关的文章被给予了首要位置。
- en: Other key metrics in ranking and recommendation applications are *hit rate*
    (also called *top-k accuracy*) and *coverage*. Hit rate measures how often at
    least one relevant item appears in the top *k* results, indicating the model’s
    consistency in recommending relevant items within the top ranks. For instance,
    in a streaming service, a high hit rate ensures that users frequently see relevant
    shows or movies in the top suggestions. Coverage, on the other hand, measures
    the proportion of items in the catalog that are recommended. This helps determine
    whether the model provides a wide range of options and helps it avoid repeatedly
    recommending popular items. High coverage in recommendation systems is desirable
    because it exposes users to a broader variety of content.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 排序和推荐应用中的其他关键指标是*命中率*（也称为*top-k准确率*）和*覆盖率*。命中率衡量至少一个相关项目出现在前*k*个结果中的频率，这表明模型在推荐相关项目方面的连贯性。例如，在流媒体服务中，高命中率确保用户经常在顶部建议中看到相关的节目或电影。另一方面，覆盖率衡量目录中推荐项目的比例。这有助于确定模型是否提供了广泛的选择，并帮助它避免反复推荐热门项目。在推荐系统中，高覆盖率是可取的，因为它使用户接触到更广泛的内容种类。
- en: In many LLM-based applications, the ground truth is very subjective. For example,
    although you can often test whether code generated by an LLM works for a given
    test set, this is usually not enough to judge code quality—after all, there’s
    a lot of code that works but is inefficient or hard to understand. In these cases,
    teams tend to use customer-focused metrics like user engagement metrics, including
    *click-through rate* (CTR) and *conversion rate*. CTR measures the percentage
    of recommended items that users click, while conversion rate tracks recommendations
    that lead to actions like purchases or sign-ups.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '在许多基于LLM的应用中，真实情况非常主观。例如，尽管你经常可以测试由LLM生成的代码是否适用于给定的测试集，但这通常不足以判断代码质量——毕竟，有很多代码虽然能工作但效率低下或难以理解。在这些情况下，团队往往会使用以客户为中心的指标，如用户参与度指标，包括*点击率*（CTR）和*转化率*。CTR衡量用户点击推荐项目的百分比，而转化率跟踪导致行动（如购买或注册）的推荐。 '
- en: In the code generation example, one way to figure out whether users like the
    content is whether they click “accept” for the code that is offered. Another way
    is to check how much they modify the code once it is incorporated in their own
    code. In an application that sends emails, you can see whether the user who generated
    the email sent it as provided and monitor the read and response rate of the emails.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码生成示例中，确定用户是否喜欢内容的一种方法是他们是否点击“接受”提供的代码。另一种方法是检查他们一旦将代码纳入自己的代码中，对其进行了多少修改。在一个发送电子邮件的应用程序中，您可以查看生成电子邮件的用户是否按提供的版本发送了电子邮件，并监控电子邮件的阅读和回复率。
- en: What Can You Control in an LLM-Based Application?
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在基于 LLM 的应用程序中您可以控制什么？
- en: When implementing an application with an LLM as the core component, several
    parameters allow you to shape the model’s responses while balancing creativity,
    coherence, and efficiency. The most well-known setting is *temperature*, a setting
    that controls the randomness of the model’s output by adjusting how deterministic
    or variable the response should be. This is a number between 0 and 1\. Lower numbers,
    such as 0.2, make the model focus on the most likely answer, which is useful for
    factual, consistent responses. Conversely, a higher temperature setting, like
    0.8, introduces more randomness, allowing the model to generate diverse and creative
    content, which can be advantageous in applications like storytelling or brainstorming.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 LLM 作为核心组件实现应用程序时，一些参数允许您在平衡创造力、连贯性和效率的同时塑造模型的响应。最著名的设置是 *温度*，这是一个通过调整响应的确定性或变量程度来控制模型输出随机性的设置。这是一个介于
    0 和 1 之间的数字。较低的数字，如 0.2，使模型专注于最可能的答案，这对于事实性、一致的响应很有用。相反，较高的温度设置，如 0.8，引入了更多的随机性，允许模型生成多样化和创造性的内容，这在讲故事或头脑风暴等应用中可能是有利的。
- en: Two other parameters, *top-*k and *top-*p*,* control the diversity of responses
    by setting probabilistic limits. Top-*k* sampling restricts the model to only
    the top *k* most probable tokens at each step, thus guiding it toward a focused
    range of words that are statistically most likely, with lower values ensuring
    more coherent responses. Top-*p* sampling, on the other hand, is a *cumulative*
    probability threshold, where the model selects from the smallest set of tokens
    with a combined probability above a given value (e.g., 0.9). This allows it to
    consider more word options for creative responses, while still ignoring highly
    unlikely terms. You can use the top-*k* and top-*p* parameters alongside temperature
    to strike a balance between response diversity and coherence, adjusting them to
    the specific requirements of your application.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 两个其他参数，*top-k* 和 *top-p*，通过设置概率限制来控制响应的多样性。*top-k* 样本限制模型在每个步骤中只选择最可能的 *k* 个标记，从而引导它向一个统计上最有可能的单词范围集中，较低的值确保了更连贯的响应。另一方面，*top-p*
    样本是一个 *累积* 概率阈值，模型从概率总和超过给定值（例如，0.9）的最小标记集中进行选择。这允许它考虑更多的单词选项以生成创造性的响应，同时仍然忽略高度不可能的术语。您可以使用
    *top-k* 和 *top-p* 参数与温度一起使用，在响应多样性和连贯性之间取得平衡，调整它们以满足您应用程序的具体要求。
- en: Another parameter that influences variety is *frequency penalty*, which applies
    a penalty based on how many times a token (word or phrase) has already appeared
    in the generated text. This means that each additional occurrence of a previously
    used word is penalized progressively, making it less likely that the model will
    repeat specific words frequently. This parameter is particularly useful in creative
    applications, where avoiding repetition can make the output more pleasant, ensuring
    that common words or phrases are not repeated too often.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 影响多样性的另一个参数是 *频率惩罚*，它根据标记（单词或短语）在生成文本中出现的次数应用惩罚。这意味着之前使用的单词的每次额外出现都会逐渐受到惩罚，使得模型频繁重复特定单词的可能性降低。此参数在创意应用中特别有用，避免重复可以使输出更加愉悦，确保常用单词或短语不会过于频繁地重复。
- en: In contrast, another parameter called *presence penalty* applies a more general
    penalty to any token that has already appeared at least once, regardless of how
    frequently it has been used. This means that the model is discouraged from reusing
    *any* word or phrase that it has already used, even if it has only appeared once.
    Presence penalty is less about reducing high-frequency words and more about encouraging
    the model to introduce entirely new vocabulary throughout the response. This can
    be beneficial in applications like content generation, where avoiding even mild
    repetition helps keep the language fresh and varied.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，另一个名为“存在惩罚”的参数会对至少出现一次的任何令牌施加更一般的惩罚。这意味着模型被鼓励不要重复使用它已经使用过的任何单词或短语，即使它只出现了一次。存在惩罚与其说是减少高频词，不如说是鼓励模型在整个响应中引入全新的词汇。这在内容生成等应用中可能是有益的，避免即使是轻微的重复也有助于保持语言的清新和多样性。
- en: To manage response length and prevent excessive generation, most models allow
    you to set a token limit with a parameter like `max tokens`, determining the maximum
    number of tokens the model will generate for each response. This is crucial for
    both controlling costs and tailoring the response length to suit different scenarios;
    concise answers might require a low token limit, while longer, detailed outputs
    may need a higher limit. In our spam detection example, the `max tokens` can be
    set to a low number. In some models, the `max tokens` parameter represents both
    the inputs and the output tokens. The final parameter that you can easily control
    is the prompt, and this is where a lot of the real-world optimizations happen—through
    prompt engineering.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理响应长度并防止过度生成，大多数模型允许你通过设置一个参数，如`max tokens`，来限制每个响应生成的最大令牌数。这对于控制成本和调整响应长度以适应不同场景至关重要；简洁的答案可能需要较低的令牌限制，而较长的、详细的输出可能需要较高的限制。在我们的垃圾邮件检测示例中，`max
    tokens`可以设置为较低的数值。在某些模型中，`max tokens`参数既代表输入也代表输出令牌。你可以轻松控制的最后一个参数是提示，这也是许多现实世界优化发生的地方——通过提示工程。
- en: Prompt Engineering Is “Hard”
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程“很难”
- en: 'In our spam detector example, we started with a very simple prompt:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的垃圾邮件检测器示例中，我们从一个非常简单的提示开始：
- en: '[PRE15]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that we’re back to the case in which the prompt is expected to produce
    answers that are only *spam* or *ham* and will not provide a “spamminess” value.
    If you use the preceding prompt, you may be surprised to find that there’s no
    guarantee that the model will actually follow the instructions as you expect and
    output only *spam* or *ham*. Some other potential outputs I’ve obtained from GPT-4o
    are as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们又回到了这种情况，即提示预期产生只包含“垃圾邮件”或“非垃圾邮件”的答案，而不会提供“垃圾邮件程度”的值。如果你使用前面的提示，你可能会惊讶地发现，模型实际上并不保证会按照你的预期执行指令，只输出“垃圾邮件”或“非垃圾邮件”。我从GPT-4o获得的一些其他潜在输出如下：
- en: '[PRE16]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Although the last answer may be surprising, it is a very common message from
    LLMs and may be triggered unexpectedly, often if the spam email has content that
    is deemed grossly offensive.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后一个答案可能令人惊讶，但这却是LLM非常常见的信息，并且可能意外触发，通常如果垃圾邮件的内容被认为极其冒犯性。
- en: This raises another consideration when using LLMs instead of machine learning
    in applications. When integrating an LLM into an application, you have to decide
    what to do when the answer doesn’t conform to the expected output. A typical solution
    is to create a new class, such as “unknown.” But even then, you could simply coerce
    all outputs into *spam* or *not spam*, for example by classifying all outputs
    that are not *N* into *spam*. Creating the “unknown” category prevents the out-of-specification
    errors from being hidden from you. In this case, you’ll want to add a metric for
    out-of-specification error percentage and set a low target.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当在应用中使用大型语言模型（LLM）而不是机器学习时，这又引发了一个新的考虑。当将LLM集成到应用中时，你必须决定当答案不符合预期输出时该做什么。一个典型的解决方案是创建一个新的类别，例如“未知”。但即便如此，你也可以简单地将所有输出强制归类为“垃圾邮件”或“非垃圾邮件”，例如通过将所有非*N*的输出归类为“垃圾邮件”。创建“未知”类别可以防止超出规格的错误从你这里隐藏。在这种情况下，你可能会想添加一个超出规格错误百分比的指标，并设置一个低目标。
- en: Making adjustments to the prompt is called *prompt engineering*. One of the
    simplest things you can do to improve the prompt is to add “Use only spam or ham
    as the answers, nothing else.” This should lower the proportion of answers that
    fall under “unknown.”
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 调整提示被称为“提示工程”。你可以做的最简单的事情之一是添加“只使用垃圾邮件或非垃圾邮件作为答案，不要使用其他任何内容。”这应该会降低“未知”类别下答案的比例。
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s say someone in management reads in an article that models tend to perform
    better if you ask them to think carefully. They ask you to test whether that change
    makes your spam detection application better. How can you test whether the new
    prompt shown next performs better or worse than the existing prompt?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 假设管理层中有人在一篇文章中读到，如果要求模型仔细思考，它们的表现往往会更好。他们要求你测试这种变化是否会使你的垃圾邮件检测应用变得更好。你该如何测试下一个显示的新提示是否比现有的提示表现更好或更差？
- en: '[PRE18]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We do know one thing: using a more detailed prompt will increase costs, because
    models charge by the length of inputs and outputs combined.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实知道一件事：使用更详细的提示会增加成本，因为模型按输入和输出的总长度收费。
- en: Did Our Prompt Engineering Produce Better Results?
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的提示工程是否产生了更好的结果？
- en: To figure out whether the additional cost comes with additional performance,
    you’ll have to go through a decision process. [Example 3-2](#ex-3-2) shows how
    to do this using Python. In the example, we use the *enron_spam_data.csv* file,
    a public labeled dataset of approximately 30,000 emails, exactly half of which
    are spam.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定额外的成本是否伴随着额外的性能，你必须经过一个决策过程。[示例3-2](#ex-3-2)展示了如何使用Python来完成这项工作。在示例中，我们使用了*enron_spam_data.csv*文件，这是一个大约有30,000封邮件的公共标记数据集，其中正好有一半是垃圾邮件。
- en: The code tests just a few emails, running 10 experiments with 30 spam and 30
    ham emails each. In a real setting, you would want to use your own, much larger
    labeled dataset (because of drift, as we will explain in [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823))
    and do more experiments.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 代码仅测试了几封邮件，运行了10次实验，每次实验有30封垃圾邮件和30封正常邮件。在实际设置中，你希望使用自己的、规模更大的标记数据集（因为漂移，我们将在第7章中解释）并进行更多实验。
- en: Example 3-2\. Prompt engineering test
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-2\. 提示工程测试
- en: '[PRE19]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The results are below:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can see that the LLM is very good at finding spam in this dataset, with
    a recall of 100% with both prompts, but the precision is around 84% for the original
    prompt and 78% for the longer prompt. Is the new prompt better than the existing
    prompt? We can use a *t*-test to make that determination. Since the recall is
    the same for both models, we only need to do the *t*-test for the precision metric.
    First, we calculate the *t*-statistic:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在这个数据集中，LLM在发现垃圾邮件方面非常出色，两个提示都实现了100%的召回率，但原始提示的精确度约为84%，而较长的提示的精确度约为78%。新的提示是否比现有的提示更好？我们可以使用*t*-检验来做出这个判断。由于两个模型的召回率相同，我们只需要对精确度指标进行*t*-检验。首先，我们计算*t*-统计量：
- en: $t equals StartFraction x overbar Subscript upper A Baseline minus x overbar
    Subscript upper B Baseline Over StartRoot StartFraction s Subscript upper A Superscript
    2 Baseline Over n Subscript upper A Baseline EndFraction plus StartFraction s
    Subscript upper B Superscript 2 Baseline Over n Subscript upper B Baseline EndFraction
    EndRoot EndFraction$
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $t等于\frac{x̄_{A_{\text{Baseline}}} - x̄_{B_{\text{Baseline}}}}{\sqrt{\frac{s_{A_{\text{Baseline}}}^2}{n_{A_{\text{Baseline}}}}
    + \frac{s_{B_{\text{Baseline}}}^2}{n_{B_{\text{Baseline}}}}}}$
- en: In this case, the *t*-statistic is 6.93\. We can convert the *t*-statistic to
    a *p*-value, which in this case is approximately 2.13 × 10^(–9), far smaller than
    a typical significance threshold of 0.05.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*t*-统计量为6.93。我们可以将*t*-统计量转换为*p*-值，在这种情况下大约为2.13 × 10^(–9)，远小于典型的显著性阈值0.05。
- en: Since the *p*-value is extremely small, we can confidently conclude that *Prompt
    A performs significantly better than Prompt B* in terms of precision. This result
    indicates that the observed difference in mean precision is highly unlikely to
    be due to random variation. ​Since Prompt B is also more expensive, we would not
    update the model to use Prompt B.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*p*-值极小，我们可以自信地得出结论，*提示A在精确度方面显著优于提示B*。这一结果表明，观察到的平均精确度差异极不可能是由随机变化引起的。由于提示B也更为昂贵，我们不会更新模型以使用提示B。
- en: LLM-Based Infrastructure Systems Are “Harder”
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的基础设施系统“更难”
- en: Once you get past calling an LLM with just a prompt and start orchestrating
    it into a live system—with memory, tools, feedback, and goals—then you are no
    longer dealing with the nondeterministic nature of an LLM. You’re dealing with
    a complex operating system with its own language, state, dependencies, and failure
    modes, each awaiting a failure anytime. Here, both the agentic systems as well
    as infrastructure-level LLM applications have their own operational or LLMOps
    problems.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你不再只是通过提示调用LLM，而是开始将其编排成一个具有记忆、工具、反馈和目标的实时系统，那么你就不再是在处理LLM的非确定性本质。你正在处理一个具有自己的语言、状态、依赖和故障模式的复杂操作系统，每个部分都在等待随时可能发生的故障。在这里，无论是代理系统还是基础设施级别的LLM应用，它们都有自己的操作或LLMOps问题。
- en: While the agentic systems promise flexibility, reasoning, and automated decision-making,
    they also introduce complex control flows that can be very hard to debug and even
    harder to trust. As we saw in the example prompt in the previous section, LLM
    outputs can vary from one to another. This can make debugging very difficult.
    If your agent fails at step 6 in a 10-step task, rerunning it might make it fail
    at step 3 or succeed entirely. Thus, there’s no clear “stack trace.” Although
    companies like W&B have offerings like Weave (its tracing tool), perfect reproducibility
    in agentic workflows can be incredibly hard, if not impossible.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然代理系统承诺提供灵活性、推理和自动化决策，但它们也引入了复杂的控制流程，这些流程可能非常难以调试，甚至更难以信任。正如我们在上一节中的示例提示中看到的，LLM的输出可能各不相同。这可能会使调试变得非常困难。如果你的代理在10步任务的第6步失败，重新运行它可能会使其在第3步失败或完全成功。因此，没有清晰的“堆栈跟踪”。尽管像W&B这样的公司提供了Weave（其跟踪工具）等产品，但在代理工作流程中实现完美的可重复性可能非常困难，如果不是不可能的话。
- en: Another issue is that agents need context. They remember facts and refer to
    earlier steps to plan ahead. But storing and retrieving this memory (whether vectorized
    or tokenized) becomes a bottleneck in terms of both latency and accuracy. Most
    memory systems are brittle, leaky, and misaligned with the model’s representation
    space. Additionally, very few agents, if any to date, are good at “planning.”
    This has been documented on several Twitter/X spaces where agents often skip steps,
    repeat tasks, or pursue irrelevant paths. This becomes ever harder when calling
    tools, because when the tools fail due to API errors or empty results, the agent
    must recover without getting stuck in a loop or hallucinating. Again, very few,
    if any, agents handle these edge cases gracefully without being preprogrammed
    using if-else conditions. Moreover, there’s no equivalent of “unit tests” or “assertions”
    that works well for agentic workflows yet.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题在于代理需要上下文。它们记住事实，并参考早期步骤来规划未来。但是，无论是以向量化还是分词的形式存储和检索这种记忆，都会在延迟和准确性方面成为瓶颈。大多数记忆系统都很脆弱、泄漏，并且与模型的表示空间不一致。此外，到目前为止，很少有代理擅长“规划”。这一点已经在几个Twitter/X空间中得到记录，其中代理经常跳过步骤、重复任务或追求无关路径。当调用工具时，这变得更加困难，因为当工具由于API错误或空结果而失败时，代理必须恢复，而不会陷入循环或产生幻觉。再次强调，非常少有代理能够优雅地处理这些边缘情况，而不需要预先使用if-else条件编程。此外，目前还没有适用于代理工作流程的“单元测试”或“断言”能够很好地工作。
- en: Agentic workflows break when the logic is messy—if, say, the plans don’t decompose
    or memory is poorly structured. However, infrastructure-level LLM applications
    introduce even more failure points and complexity. If the protocols don’t sync
    with each other, or the data flows start leaking, or the model boundaries are
    unclear. . .there are far too many failure points to count. While most people
    have been jumping on the bandwagon to adopt MCPs or A2A, very few are equipped
    to handle the LLMOps issues these tools introduce.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当逻辑混乱时，例如，如果计划没有分解或记忆结构不佳，代理工作流程就会中断。然而，基础设施级别的LLM应用引入了更多的故障点和复杂性。如果协议之间不同步，或者数据流开始泄漏，或者模型边界不明确……有太多的故障点无法计数。虽然大多数人都在争先恐后地采用MCP或A2A，但很少有人能够处理这些工具引入的LLMOps问题。
- en: First, MCP assumes that the memory and tools are abstracted and callable. But
    that couldn’t be further from the truth. Memory updates go out of sync pretty
    often. Different agents have different scores, and the tools might update shared
    state without coordination. You need memory versioning, namespacing, and syncing—none
    of which actually come “out of the box.”
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，MCP假设记忆和工具是抽象的并且可以调用的。但事实远非如此。记忆更新经常出现不同步。不同的代理有不同的分数，工具可能在没有协调的情况下更新共享状态。你需要记忆版本控制、命名空间和同步——这些实际上都没有“开箱即用”。
- en: Then, say, if a model call takes time, wrapping the model call in an MCP session
    adds orchestration overhead. That includes setup, prompt retrieval, and tool registration
    issues, all of which can compound. This is where you need to consider the opportunity
    cost. Similarly, A2A will add network latency, serialization, and agent discovery
    issues, and again, for complex tasks, all of these sources of overhead compound
    very quickly. When a prompt fails in LangChain, you see a clear trace. When an
    A2A agent fails, it might return an invalid response, break the schema, or even
    time out. It’s never clear where it failed. Was it at the agent stage or transport
    or the memory layer or the tooling? You need to stack up layers and layers of
    observability tools and structured logging across agents and sessions.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，比如说，如果模型调用需要时间，将模型调用包裹在MCP会话中会增加编排开销。这包括设置、提示检索和工具注册问题，所有这些问题都可能累积。这就是你需要考虑机会成本的地方。同样，A2A会增加网络延迟、序列化和代理发现问题，而且，对于复杂任务来说，所有这些开销来源都会非常快地累积。当LangChain中的提示失败时，你会看到清晰的跟踪。当A2A代理失败时，它可能会返回无效的响应，破坏模式，甚至超时。它永远不会清楚失败在哪里。是在代理阶段、传输、内存层还是工具层？你需要堆叠一层又一层的可观察性工具和结构化日志，跨越代理和会话。
- en: If your workflow includes five agents, each calling three tools, and every interaction
    is mediated through A2A or MCP layers, then the user ends up waiting for quite
    a few seconds or even minutes. This ruins the user experience. Also, most of the
    security issues haven’t even yet been documented as of this writing. Agents might
    hijack each other’s memories, triggering unintended tool actions—the attack surface
    becomes incredibly wide, especially when you let LLMs act autonomously on the
    user data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的工作流程包括五个代理，每个代理调用三个工具，并且每次交互都通过A2A或MCP层进行中介，那么用户最终可能需要等待几秒钟甚至几分钟。这会破坏用户体验。此外，截至本文撰写时，大多数安全问题甚至还没有被记录。代理可能会劫持彼此的记忆，触发意外的工具操作——攻击面变得极其广泛，尤其是当你让LLMs在用户数据上自主行动时。
- en: 'Agentic intelligence feels incredibly powerful in demos but breaks in production.
    Indeed, it is very fragile without solid infrastructure. Every day, I personally
    see tons of clever orchestrations around dumb prompt chains tied up in a brittle,
    underused LLMOps infrastructure. But building this infrastructure means acknowledging
    the costs: performance overhead, strict interface contracts, and state complexity,
    as well as a need for more LLMOps engineers to create the best practices, tooling,
    and frameworks to run these systems reliably, safely, and robustly.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示中，代理智能感觉非常强大，但在生产中却会崩溃。确实，如果没有坚实的基础设施，它非常脆弱。每天，我都亲自看到许多围绕笨拙的提示链的巧妙编排，这些编排被绑定在脆弱、利用率低的LLMOps基础设施中。但建立这种基础设施意味着承认成本：性能开销、严格的接口合同、状态复杂性，以及需要更多LLMOps工程师来创建最佳实践、工具和框架，以可靠、安全、稳健地运行这些系统。
- en: Conclusion
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we’ve covered the key considerations for integrating LLMs into
    applications, from measuring performance to improving models through parameter
    adjustment, prompt engineering, and agentic and infrastructure applications. Using
    LLMs successfully in enterprise applications requires defining clear performance
    metrics, monitoring them, and continuously improving the models.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了将LLMs集成到应用程序中的关键考虑因素，从衡量性能到通过参数调整、提示工程和代理及基础设施应用来改进模型。在企业应用程序中成功使用LLMs需要定义明确的性能指标，监控它们，并持续改进模型。
- en: References
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alayrac, Jean-Baptiste et al. [“Tackling Multiple Tasks with a Single Visual
    Language Model”](https://oreil.ly/hTMjp), Google DeepMind, April 28, 2022.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Alayrac, Jean-Baptiste等. [“使用单个视觉语言模型处理多个任务”](https://oreil.ly/hTMjp)，Google
    DeepMind，2022年4月28日。
- en: Anthropic. [“Introducing the Model Context Protocol”](https://oreil.ly/-UjTz),
    November 25, 2024.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic. [“介绍模型上下文协议”](https://oreil.ly/-UjTz)，2024年11月25日。
- en: 'Bevans, Rebecca. [“An Introduction to *t* Tests: Definitions, Formula, and
    Examples”](https://oreil.ly/snP0y), Scribbr, June 22, 2023.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Bevans, Rebecca. [“*t*检验简介：定义、公式和示例”](https://oreil.ly/snP0y)，Scribbr，2023年6月22日。
- en: 'Fiore, Steven. [“Inside Microsoft’s Copilot Stack: Building Smarter AI Assistants”](https://oreil.ly/aDxV5),
    *Lantern*, August 2, 2024.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Fiore, Steven. [“微软Copilot堆栈内部：构建更智能的AI助手”](https://oreil.ly/aDxV5)，*Lantern*，2024年8月2日。
- en: Henry, Parker. [“How Duolingo Uses AI to Create Lessons Faster”](https://oreil.ly/_2C7G),
    Duolingo Blog, June 22, 2023.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Henry, Parker. [“如何使用AI更快地创建课程”](https://oreil.ly/_2C7G)，Duolingo博客，2023年6月22日。
- en: 'Li, Junnan, et al. [“BLIP: Bootstrapping Language-Image Pre-training for Unified
    Vision-Language Un](https://oreil.ly/1YOME)[derstanding and Generation”](https://oreil.ly/1YOME),
    arXiv, February 15, 2022.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Li, Junnan等. [“BLIP：为统一视觉-语言理解和生成启动语言-图像预训练”](https://oreil.ly/1YOME)[和](https://oreil.ly/1YOME),
    arXiv, 2022年2月15日.
- en: Microsoft | AI for Good Lab. [“When AI Became a General-Purpose Technology”](https://oreil.ly/2YnWU)
    [figure]. LinkedIn post by Brad Smith, Microsoft, September 2024.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 微软 | AI for Good Lab. [“当AI成为通用技术”](https://oreil.ly/2YnWU) [图]. Brad Smith在LinkedIn上的帖子，微软，2024年9月.
- en: Microsoft Research. n.d. [“Building Next-Gen Multimodal Foundation Models for
    General-Purpose Assistants”](https://oreil.ly/1ZUKe), accessed May 21, 2025.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究院. n.d. [“为通用助手构建下一代多模态基础模型”](https://oreil.ly/1ZUKe), 访问日期：2025年5月21日.
- en: OpenAI. n.d. [“Be My Eyes”](https://oreil.ly/CZcE_), accessed May 21, 2025.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI. n.d. [“Be My Eyes”](https://oreil.ly/CZcE_), 访问日期：2025年5月21日.
- en: 'OpenAI. [“CLIP: Connecting Text and Images”](https://oreil.ly/rbuEV), OpenAI,
    January 5, 2021.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI. [“CLIP：连接文本和图像”](https://oreil.ly/rbuEV), OpenAI, 2021年1月5日.
- en: OpenL. n.d. [“LLM Price Check”](https://oreil.ly/-Ez0q), accessed May 21, 2025.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: OpenL. n.d. [“LLM价格检查”](https://oreil.ly/-Ez0q), 访问日期：2025年5月21日.
- en: Schmid, Phil. [“Model Context Protocol (MCP) an Overview”](https://oreil.ly/LGDUG),
    personal blog, April 3, 2025.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Schmid, Phil. [“模型上下文协议（MCP）概述”](https://oreil.ly/LGDUG), 个人博客，2025年4月3日.
- en: Sigrist, Fabio. [“Demystifying ROC and Precision-Recall Curves”](https://oreil.ly/eWU7R),
    *Medium*, January 25, 2022.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Sigrist, Fabio. [“揭秘ROC和精确率-召回率曲线”](https://oreil.ly/eWU7R), *Medium*, 2022年1月25日.
- en: South, Tobin et al. [“Authenticated Delegation and Authorized AI Agents”](https://oreil.ly/cQyyw),
    arXiv, January 16, 2025.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: South, Tobin等. [“认证委托和授权AI代理”](https://oreil.ly/cQyyw), arXiv, 2025年1月16日.
- en: 'Spheron Network. [“The Economics of Renting Cloud GPUs: A Comprehensive Breakdown”](https://oreil.ly/t70mb),
    March 13, 2025.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Spheron Network. [“租赁云GPU的经济效益：全面分析”](https://oreil.ly/t70mb), 2025年3月13日.
- en: Wang, Cheng, et al. [“Calibrating Verbalized Probabilities for Large Language
    Models”](https://oreil.ly/lGJuU), arXiv, October 9, 2024.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Wang, Cheng等. [“校准大型语言模型的可言说概率”](https://oreil.ly/lGJuU), arXiv, 2024年10月9日.
