- en: Chapter 7\. Monitoring the Training Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。监控训练过程
- en: In the last chapter, you learned how to launch the model training process. In
    this chapter, we’ll cover the process itself.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何启动模型训练过程。在本章中，我们将介绍过程本身。
- en: I’ve used fairly straightforward examples in this book to help you grasp each
    concept. When you’re running a real training process in TensorFlow, however, things
    can be more complicated. When problems arise, for example, you need to think about
    how to determine whether your model is *overfitting* the training data. (Overfitting
    occurs when the model learns and memorizes the training data and the noise in
    training data so well that it negatively affects its ability to learn new data.)
    If it is, you’ll need to set up cross validation. If not, you can take steps to
    prevent overfitting.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我使用了相当直接的例子来帮助您理解每个概念。然而，在TensorFlow中运行真实的训练过程时，事情可能会更加复杂。例如，当出现问题时，您需要考虑如何确定您的模型是否*过拟合*训练数据（当模型学习并记忆训练数据和训练数据中的噪声以至于负面影响其学习新数据时就会发生过拟合）。如果是，您需要设置交叉验证。如果不是，您可以采取措施防止过拟合。
- en: 'Other questions that often arise during the training process include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中经常出现的其他问题包括：
- en: How often should I save the model during the training process?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中应该多久保存一次模型？
- en: How should I determine which epoch gives the best model before overfitting occurs?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过拟合发生之前，我应该如何确定哪个时期给出了最佳模型？
- en: How can I track model performance?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何跟踪模型性能？
- en: Can I stop training if the model is not improving or is overfitting?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型没有改进或出现过拟合，我可以停止训练吗？
- en: Is there a way to visualize the model training process?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有没有一种方法可以可视化模型训练过程？
- en: 'TensorFlow provides a very easy way to address these questions: callback functions.
    In this chapter, you will learn how to make quick use of callback functions as
    you monitor the training process. The first half of the chapter discusses `ModelCheckpoint`
    and `EarlyStopping`, while the second half focuses on TensorBoard and shows you
    several techniques for invoking TensorBoard and using it for visualization.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了一种非常简单的方法来解决这些问题：回调函数。在本章中，您将学习如何快速使用回调函数来监视训练过程。本章的前半部分讨论了`ModelCheckpoint`和`EarlyStopping`，而后半部分侧重于TensorBoard，并向您展示了几种调用TensorBoard和使用它进行可视化的技巧。
- en: Callback Objects
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回调对象
- en: A TensorFlow *callback object* is an object that can perform a group of built-in
    functions provided by `tf.keras`. When certain events occur during training, a
    callback object will execute specific code or functions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的*回调对象*是一个可以执行由`tf.keras`提供的一组内置函数的对象。当训练过程中发生某些事件时，回调对象将执行特定的代码或函数。
- en: 'Using callbacks is optional, so you don’t need to implement any callback objects
    to train a model. We’ll be looking at three of the most frequently used classes:
    `ModelCheckpoint`, `EarlyStopping`, and TensorBoard.^([1](ch07.xhtml#ch06fn01))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回调是可选的，因此您不需要实现任何回调对象来训练模型。我们将看一下最常用的三个类：`ModelCheckpoint`、`EarlyStopping`和TensorBoard。^([1](ch07.xhtml#ch06fn01))
- en: ModelCheckpoint
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ModelCheckpoint
- en: The `ModelCheckpoint` class enables you to save your model regularly throughout
    the training process. By default, at the end of each training epoch, model weights
    and biases are finalized and saved as a weight file. Typically, when you launch
    a training process, the model learns from the training data in that epoch and
    updates the weights and biases, which are saved in a directory you specify before
    beginning the training process. However, sometimes you’ll want to save the model
    only if it has improved from the previous epoch, so that the last saved model
    is always the best model. To do this, you can use the `ModelCheckpoint` class.
    In this section, you are going to see how to leverage this class in your model
    training process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelCheckpoint`类使您能够在训练过程中定期保存模型。默认情况下，在每个训练时期结束时，模型的权重和偏差会被最终确定并保存为权重文件。通常，当您启动训练过程时，模型会从该时期的训练数据中学习并更新权重和偏差，这些权重和偏差会保存在您在开始训练过程之前指定的目录中。然而，有时您只想在模型从上一个时期改进时保存模型，以便最后保存的模型始终是最佳模型。为此，您可以使用`ModelCheckpoint`类。在本节中，您将看到如何在模型训练过程中利用这个类。'
- en: 'Let’s try it out using the CIFAR-10 image classification dataset that we used
    in [Chapter 6](ch06.xhtml#model_creation_styles). As usual, we start by importing
    the necessary libraries, and then we read the CIFAR-10 data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在[第6章](ch06.xhtml#model_creation_styles)中使用的CIFAR-10图像分类数据集中进行。像往常一样，我们首先导入必要的库，然后读取CIFAR-10数据：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, normalize the pixel values in the images to be in a range of 0 to 1:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将图像中的像素值归一化为0到1的范围：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The image labels in this dataset consist of integers. Verify this using a NumPy
    command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集中的图像标签由整数组成。使用NumPy命令验证这一点：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This shows the values to be:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示的值为：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can map these integers to the plain-text labels. The [labels here](https://oreil.ly/hYg5R)
    (provided by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton) are in alphabetical
    order. Thus, `airplane` maps to a value of 0 in `train_labels`, while `truck`
    maps to 9:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将这些整数映射到纯文本标签。这里提供的标签（由Alex Krizhevsky、Vinod Nair和Geoffrey Hinton提供）按字母顺序排列。因此，`airplane`在`train_labels`中映射为0，而`truck`映射为9：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since there is a separate partition for `test_images`, extract the first 500
    images from `test_images` to use for cross validation and name it `validation_images`.
    You’ll use the remaining images for testing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`test_images`有一个单独的分区，从`test_images`中提取前500张图像用于交叉验证，并将其命名为`validation_images`。剩下的图像将用于测试。
- en: 'To use your compute resources more efficiently, convert the `test_images` images
    and labels from their native NumPy array format to dataset format:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地利用计算资源，将`test_images`的图像和标签从其原生NumPy数组格式转换为数据集格式：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After executing these commands, you should have all of the images in three
    datasets: a training dataset (`train_dataset`), a validation dataset (`validation_dataset`),
    and a testing dataset (`test_dataset`).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些命令后，您应该在三个数据集中拥有所有图像：一个训练数据集（`train_dataset`）、一个验证数据集（`validation_dataset`）和一个测试数据集（`test_dataset`）。
- en: 'It would be nice to know the sizes of these datasets. To find the sample size
    of a TensorFlow dataset, convert it to a list, and then find the length of the
    list using the `len` function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这些数据集的大小会很有帮助。要找到TensorFlow数据集的样本大小，将其转换为列表，然后使用`len`函数找到列表的长度：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can expect these results:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以期待以下结果：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, shuffle and batch the three datasets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对这三个数据集进行洗牌和分批处理：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that `train_dataset` will be split into multiple batches. Each batch
    will contain `TRAIN_BATCH_SIZE` samples (in this case, 128). Each training batch
    is fed to the model during the training process to enable incremental updates
    to weights and biases. There is no need to create multiple batches for validation
    and testing. These will be used as one batch, but only for the purposes of logging
    metrics and testing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`train_dataset`将被分成多个批次。每个批次将包含`TRAIN_BATCH_SIZE`个样本（在本例中为128）。每个训练批次在训练过程中被馈送到模型中，以实现对权重和偏差的增量更新。对于验证和测试，不需要创建多个批次。它们将作为一个批次使用，但仅用于记录指标和测试。
- en: 'Next, specify how often to update weights and validate:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，指定多久更新权重和验证一次：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code means that after the model has seen the number of batches
    of training data specified by `STEPS_PER_EPOCH`, it’s time to test the model with
    the validation dataset (used as one batch).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码意味着在模型看到由`STEPS_PER_EPOCH`指定的训练数据批次数量后，是时候使用验证数据集（作为一个批次）测试模型了。
- en: 'To do this, you’ll first define the model architecture:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您首先要定义模型架构：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, compile the model to make sure it’s set up properly:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编译模型以确保它设置正确：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, name the folders to which TensorFlow should save the model at each checkpoint.
    Usually, you will rerun the training routine multiple times, and you may find
    it tedious to create a unique folder name each time. A simple and frequently used
    approach is to append a timestamp to the model name:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，命名TensorFlow应在每个检查点保存模型的文件夹。通常，您会多次重新运行训练例程，可能会觉得每次创建一个唯一的文件夹名称很烦琐。一个简单且经常使用的方法是在模型名称后附加一个时间戳：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding command yields a name such as *myCIFAR10-20210123-212138*. You
    can use this name for the checkpoint directory:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令会产生一个名为*myCIFAR10-20210123-212138*的名称。您可以将此名称用于检查点目录：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding command specifies the directory path to be *./myCIFAR10-20210123-212138/ckpt-{epoch}*.
    This directory is located one level below your current directory. *{epoch}* will
    be encoded with the epoch number during training. Now define the `myCheckPoint`
    object:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令指定了目录路径为*./myCIFAR10-20210123-212138/ckpt-{epoch}*。该目录位于当前目录的下一级。*{epoch}*将在训练期间用epoch号进行编码。现在定义`myCheckPoint`对象：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here you specified the file path where TensorFlow will save the model at each
    epoch. You also set it up to monitor validation accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您指定了TensorFlow将在每个epoch保存模型的文件路径。您还设置了监视验证准确性。
- en: 'When you launch the training process with a callback, the callback will expect
    a Python list. So, let’s put the `myCheckPoint` object into a Python list:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用回调启动训练过程时，回调将期望一个Python列表。因此，让我们将`myCheckPoint`对象放入Python列表中：
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now launch the training process. This command assigns the entire model training
    history to the object `hist`, which is a Python dictionary:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在启动训练过程。此命令将整个模型训练历史分配给对象`hist`，这是一个Python字典：
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can view the cross-validation accuracy from the first epoch of training
    to the last using the command `hist[''val_accuracy'']`. The display should look
    something like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用命令`hist['val_accuracy']`查看从训练的第一个epoch到最后一个epoch的交叉验证准确性。显示应该类似于这样：
- en: '[PRE17]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case, cross-validation accuracy improved for a number of epochs, then
    gradually degraded. This degradation is a typical sign of overfitting. The best
    model here is the one with the highest validation accuracy (the highest value
    in the array). To determine its position (or index) in the array, use this code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，交叉验证准确性在一些epoch中有所提高，然后逐渐下降。这种下降是过拟合的典型迹象。这里最好的模型是具有最高验证准确性（数组中最高值）的模型。要确定其在数组中的位置（或索引），请使用此代码：
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Remember to add 1 to `max_index` because the epoch starts at 1, not 0 (unlike
    a NumPy array index). The output is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住将`max_index`加1，因为epoch从1开始，而不是0（与NumPy数组索引不同）。输出是：
- en: '[PRE19]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, take a look at the checkpoint directories by running the following Linux
    command in your Jupyter Notebook cell:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过在Jupyter Notebook单元格中运行以下Linux命令来查看检查点目录：
- en: '[PRE20]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You will see the contents of this directory (shown in [Figure 7-1](#model_saved_at_each_checkpoint)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到此目录的内容（如[图7-1](#model_saved_at_each_checkpoint)所示）。
- en: '![Model saved at each checkpoint](Images/t2pr_0701.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![在每个检查点保存的模型](Images/t2pr_0701.png)'
- en: Figure 7-1\. Model saved at each checkpoint
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。在每个检查点保存的模型
- en: 'You can rerun this command and specify a specific directory to see the model
    built by a particular epoch (as shown in [Figure 7-2](#model_files_saved_at_checkpoint_eight)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以重新运行此命令并指定特定目录，以查看特定epoch构建的模型（如[图7-2](#model_files_saved_at_checkpoint_eight)所示）：
- en: '[PRE21]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Model files saved at checkpoint 8](Images/t2pr_0702.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![在检查点8保存的模型文件](Images/t2pr_0702.png)'
- en: Figure 7-2\. Model files saved at checkpoint 8
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。在检查点8保存的模型文件
- en: 'So far, you have seen how to use `CheckPoint` to save the model at each epoch.
    If you wish to save only the best model, specify `save_best_only = True`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到如何使用`CheckPoint`在每个epoch保存模型。如果您只希望保存最佳模型，请指定`save_best_only = True`：
- en: '[PRE22]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then put `bestCheckPoint` in a callback list:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`bestCheckPoint`放入回调列表中：
- en: '[PRE23]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After that, you can launch the training process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您可以启动训练过程：
- en: '[PRE24]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this training, rather than saving all checkpoints, `bestCallbacks` causes
    the model to be saved only if it has a better validation accuracy than the previous
    epoch. The `save_best_only` option lets you save checkpoints after the first epoch
    *only* if there is an incremental improvement to the model metric of your choice
    (specified with `monitor`), so that the last checkpoint saved is the best model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个训练中，而不是保存所有检查点，`bestCallbacks`只有在验证准确率比上一个epoch更好时才保存模型。`save_best_only`选项允许您在第一个epoch之后*仅*在模型指标有增量改进时保存检查点（使用`monitor`指定），因此最后保存的检查点是最佳模型。
- en: 'To look at what you’ve saved, run the following command in a Jupyter Notebook
    cell:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看您保存的内容，请在Jupyter Notebook单元格中运行以下命令：
- en: '[PRE25]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The saved models with incremental improvement in validation accuracy are shown
    in [Figure 7-3](#models_saved_with_save_best_only_set_to).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7-3](#models_saved_with_save_best_only_set_to)中显示了验证准确率逐渐提高的保存模型。
- en: '![Models saved with save_best_only set to True](Images/t2pr_0703.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![设置save_best_only为True保存的模型](Images/t2pr_0703.png)'
- en: Figure 7-3\. Models saved with save_best_only set to True
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。设置save_best_only为True的模型
- en: The model from the first epoch is always saved. In the third epoch, the model
    shows improvement in validation accuracy, so the third checkpoint model is saved.
    The training continues. Validation accuracy improves in the ninth epoch, so the
    ninth checkpoint model is the last directory that is saved. The training continues
    through the 12th epoch without any further incremental improvement in validation
    accuracy. This means that the ninth checkpoint directory contains the model with
    the best validation accuracy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个epoch的模型始终会被保存。在第三个epoch，模型在验证准确率上有所改进，因此第三个检查点模型被保存。训练继续进行。第九个epoch中验证准确率提高，因此第九个检查点模型是最后一个被保存的目录。训练持续到第12个epoch，没有进一步增加验证准确率的改进。这意味着第九个检查点目录包含了最佳验证准确率的模型。
- en: 'Now that you’re familiar with `ModelCheckpoint`, let’s examine another callback
    object: `EarlyStopping`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您熟悉了`ModelCheckpoint`，让我们来看看另一个回调对象：`EarlyStopping`。
- en: EarlyStopping
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EarlyStopping
- en: The `EarlyStopping` callback object enables you to stop the training process
    before it reaches the final epoch. Usually, you’d do this to save training time
    if the model isn’t improving.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStopping`回调对象使您能够在达到最终epoch之前停止训练过程。通常，如果模型没有改进，您会这样做以节省训练时间。'
- en: The object lets you specify a model metric—for example, validation accuracy—to
    monitor through all epochs. If the specified metric does not improve after a certain
    number of epochs, the training will stop.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该对象允许您指定一个模型指标，例如验证准确率，通过所有epoch进行监视。如果指定的指标在一定数量的epoch后没有改进，训练将停止。
- en: 'To define an `EarlyStopping` object, use this command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个`EarlyStopping`对象，请使用以下命令：
- en: '[PRE26]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this case, you are monitoring validation accuracy at each epoch. You set
    the `patience` parameter to 4, which means that if validation accuracy does not
    improve within four epochs, the training stops.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您在每个epoch监视验证准确率。您将`patience`参数设置为4，这意味着如果验证准确率在四个epoch内没有改进，训练将停止。
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: To learn more ways to customize early stopping, see the [TensorFlow 2 documentation](https://oreil.ly/UDpaA).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多自定义提前停止的方法，请参阅[TensorFlow 2文档](https://oreil.ly/UDpaA)。
- en: 'To implement early stopping with a `ModelCheckpoint` object in a callback,
    you need to put it into a list:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要在回调中使用`ModelCheckpoint`对象实现提前停止，需要将其放入列表中：
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The training process is the same, but you designate `callbacks=myCallbacks`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程是相同的，但您指定了`callbacks=myCallbacks`：
- en: '[PRE28]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once you launch the preceding training command, the output should resemble [Figure 7-4](#early_stop_during_training).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您启动了前面的训练命令，输出应该类似于[图7-4](#early_stop_during_training)。
- en: '![Early stop during training](Images/t2pr_0704.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![训练过程中的提前停止](Images/t2pr_0704.png)'
- en: Figure 7-4\. Early stop during training
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。训练过程中的提前停止
- en: In the training shown in [Figure 7-4](#early_stop_during_training), the best
    validation accuracy appears in epoch 15, with a value of 0.7220\. After four more
    epochs, validation accuracy has not improved beyond that value, and so the training
    stops after epoch 19.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7-4](#early_stop_during_training)中显示的训练中，最佳验证准确率出现在第15个epoch，值为0.7220。再经过四个epoch，验证准确率没有超过该值，因此在第19个epoch后停止训练。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: While the `ModelCheckpoint` class lets you set a condition or cadence to save
    the model during training, the `EarlyStopping` class lets you terminate training
    early if the model is showing no improvement to the metric of your choice. Together,
    these classes are specified in a Python list, and this list is passed into the
    training routine as a callback.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelCheckpoint`类允许您设置条件或频率以在训练期间保存模型，而`EarlyStopping`类允许您在模型没有改进到您选择的指标时提前终止训练。这些类一起在Python列表中指定，并将此列表作为回调传递到训练例程中。'
- en: Many other functions are available for monitoring training progress (see [tf.keras.callbacks.Callback](https://oreil.ly/1nIE6)
    and the [Keras Callbacks API](https://oreil.ly/BeJBW)), but `ModelCheckpoint`
    and `EarlyStopping` are two of the most frequently used ones.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他用于监控训练进度的功能可用（请参阅[tf.keras.callbacks.Callback](https://oreil.ly/1nIE6)和[Keras
    Callbacks API](https://oreil.ly/BeJBW)），但`ModelCheckpoint`和`EarlyStopping`是最常用的两个。
- en: The remainder of this chapter will dive deep into the popular callback class
    known as `TensorBoard`, which provides a visual representation of your training
    progress and results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将深入探讨被称为`TensorBoard`的流行回调类，它提供了您的训练进度和结果的可视化表示。
- en: TensorBoard
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard
- en: If you wish to visualize your model and training process, TensorBoard is the
    tool for you. TensorBoard provides a visual representation of how your model parameters
    and metrics evolve from the beginning to the end of your training process. It’s
    frequently used for tracking model accuracy over training epochs. It can also
    let you see how weights and biases evolve in each model layer. And just like `ModelCheckpoint`
    and `EarlyStopping`, TensorBoard is applied to the training process via the callbacks
    module. You create an object that represents a `Tensorboard`, then pass that object
    as a member of a callback list.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望可视化您的模型和训练过程，TensorBoard是您的工具。TensorBoard提供了一个视觉表示，展示了您的模型参数和指标在训练过程中如何演变。它经常用于跟踪训练周期中的模型准确性。它还可以让您看到每个模型层中的权重和偏差如何演变。就像`ModelCheckpoint`和`EarlyStopping`一样，TensorBoard通过回调模块应用于训练过程。您创建一个代表`Tensorboard`的对象，然后将该对象作为回调列表的成员传递。
- en: 'Let’s try building a model that classifies CIFAR-10 images. As usual, start
    by importing libraries, loading the CIFAR-10 images, and normalizing pixel values
    to a range of 0 to 1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个对CIFAR-10图像进行分类的模型。像往常一样，首先导入库，加载CIFAR-10图像，并将像素值归一化到0到1的范围内：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define your plain-text labels:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 定义您的纯文本标签：
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now convert the images to a dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将图像转换为数据集：
- en: '[PRE31]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then determine the data size for training, validation, and test partitions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后确定用于训练、验证和测试分区的数据大小：
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Your results should look like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果应该如下所示：
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now you can shuffle and batch the data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以对数据进行洗牌和分批处理：
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And then specify parameters for setting up a cadence to update model weights:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后指定参数以设置更新模型权重的节奏：
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`STEPS_PER_EPOCH` is an integer, rounded down from the division between `train_dataset_size`
    and `TRAIN_BATCH_SIZE`. (The double forward slashes indicate division and rounding
    down to the nearest integer.)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`STEPS_PER_EPOCH`是一个整数，从`train_dataset_size`和`TRAIN_BATCH_SIZE`之间的除法向下取整得到。（双斜杠表示除法并向下取整到最接近的整数。）'
- en: 'We’ll reuse the model architecture we built in [“ModelCheckpoint”](#modelcheckpoint):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用我们在[“ModelCheckpoint”](#modelcheckpoint)中构建的模型架构：
- en: '[PRE36]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Notice that this time, each layer has a name. Designating a name for each layer
    helps you know which layer you are inspecting. This is not required, but it’s
    a good practice for visualization in TensorBoard.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次每个层都有一个名称。为每个层指定一个名称有助于您知道您正在检查哪个层。这不是必需的，但对于在TensorBoard中进行可视化是一个好的实践。
- en: 'Now compile the model to make sure the model architecture is valid, and designate
    a loss function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在编译模型以确保模型架构有效，并指定损失函数：
- en: '[PRE37]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Setting a model name will be helpful later, when TensorBoard lets you pick
    a model (or models) and inspect the visualization of its training results. You
    can append a timestamp to the model name as we did when using `ModelCheckpoint`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 设置模型名称将在以后有所帮助，当TensorBoard让您选择一个模型（或多个模型）并检查其训练结果的可视化时。您可以像在使用`ModelCheckpoint`时那样在模型名称后附加一个时间戳：
- en: '[PRE38]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this example, `MODEL_NAME` is `myCIFAR10-20210124-135804`. Yours will be
    similar.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`MODEL_NAME`是`myCIFAR10-20210124-135804`。您的将类似。
- en: 'Next, set up the checkpoint directory:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置检查点目录：
- en: '[PRE39]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*./myCIFAR10-20210124-135804/ckpt-{epoch}* is the name of this checkpoint directory.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*./myCIFAR10-20210124-135804/ckpt-{epoch}*是这个检查点目录的名称。'
- en: 'Define the model checkpoint:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型检查点：
- en: '[PRE40]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next you will define a TensorBoard, and then we’ll take a closer look at this
    code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将定义一个TensorBoard，然后我们将更仔细地查看这段代码：
- en: '[PRE41]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The first argument specified here is `log_dir`. It is the path where you want
    to save the training logs. As indicated, it is in a directory below the current
    level, named *tensorboardlogs*, which is followed by a subdirectory named `MODEL_NAME`.
    As training progresses, your logs will be generated and stored here so that TensorBoard
    can parse them for visualization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里指定的第一个参数是`log_dir`。这是您想要保存训练日志的路径。如指示，它在当前级别下的一个名为*tensorboardlogs*的目录中，后面跟着一个名为`MODEL_NAME`的子目录。随着训练的进行，您的日志将在这里生成和存储，以便TensorBoard可以解析它们进行可视化。
- en: The parameter `write_graph` is set to True, so that the model graph will be
    visualized. Another parameter, `write_images`, is also set to True. This ensures
    that model weights will be written in the log, so you can visualize how they change
    throughout training.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`write_graph`设置为True，这样模型图将被可视化。另一个参数`write_images`也设置为True。这确保模型权重将被写入日志，这样您可以可视化它们在训练过程中的变化。
- en: 'Finally, `histogram_freq` is set to 1\. This tells TensorBoard when to create
    a visualization by epoch: 1 means a visualization is created for each epoch. For
    more parameters, see the TensorBoard [documentation](https://oreil.ly/k1Pd2).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`histogram_freq`设置为1。这告诉TensorBoard何时按epoch创建可视化：1表示每个epoch创建一个可视化。有关更多参数，请参阅TensorBoard的[文档](https://oreil.ly/k1Pd2)。
- en: 'Finally, you have two callback objects to set up: `myCheckPoint` and `myTensorBoard`.
    To put both in a Python list, you only have to do this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您有两个回调对象要设置：`myCheckPoint`和`myTensorBoard`。要将两者放入Python列表中，您只需执行以下操作：
- en: '[PRE42]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then pass your `myCallbacks` list into the training routine:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将您的`myCallbacks`列表传递到训练例程中：
- en: '[PRE43]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Once the training process is done, there are three ways to invoke TensorBoard.
    You can run it from the next cell in your Jupyter Notebook on your own computer,
    from a command terminal on your own computer, or in Google Colab. We’ll look at
    each of these options in turn.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，有三种方法可以调用TensorBoard。您可以在您自己的计算机上的Jupyter Notebook中的下一个单元格中运行它，也可以在您自己计算机上的命令终端中运行，或者在Google
    Colab中运行。我们将依次查看这些选项。
- en: Invoking TensorBoard by Local Jupyter Notebook
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过本地Jupyter Notebook调用TensorBoard
- en: 'If you choose to use your Jupyter Notebook, run the following command in the
    next cell:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择使用您的Jupyter Notebook，在下一个单元格中运行以下命令：
- en: '[PRE44]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Notice that in this case, when you specify the path to find the training logs,
    the argument is `logdir`, not `log_dir` as it was when you defined `myTensorBoard`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，当您指定路径以查找训练日志时，参数是`logdir`，而不是在定义`myTensorBoard`时的`log_dir`。
- en: 'Once you run the preceding command, you’ll see this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述命令后，您将看到以下内容：
- en: '[PRE45]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As you can see, TensorBoard is running at your current compute instance (localhost)
    on port number 6006.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，TensorBoard正在您当前的计算实例（localhost）上以端口号6006运行。
- en: Now open a browser and navigate to *http://localhost:6006*, and you will see
    TensorBoard running, as shown in [Figure 7-5](#tensorboard_visualization).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开浏览器，导航至*http://localhost:6006*，您将看到TensorBoard正在运行，如[图7-5](#tensorboard_visualization)所示。
- en: '![TensorBoard visualization](Images/t2pr_0705.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard可视化](Images/t2pr_0705.png)'
- en: Figure 7-5\. TensorBoard visualization
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. TensorBoard可视化
- en: As you can see, through each epoch, accuracy and loss are traced in graphs.
    Training data is shown in lighter gray and validation data is shown in a darker
    gray.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，通过每个时代，准确性和损失都在图表中追踪。训练数据显示为浅灰色，验证数据显示为深灰色。
- en: The main advantage of using a Jupyter Notebook cell is convenience. A drawback
    is that the cell that runs the `!tensorboard` command will remain active, and
    you won’t be able to use this notebook until you stop TensorBoard.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Jupyter Notebook单元格的主要优势是方便。缺点是运行`!tensorboard`命令的单元格将保持活动状态，直到您停止TensorBoard，您将无法使用此笔记本。
- en: Invoking TensorBoard by Local Command Terminal
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过本地命令终端调用TensorBoard
- en: 'Your second option is to launch TensorBoard from a command terminal in your
    local environment. As shown in [Figure 7-6](#tensorboard_invoked_from_the_command_ter),
    the equivalent command is:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第二个选项是在本地环境的命令终端中启动TensorBoard。如[图7-6](#tensorboard_invoked_from_the_command_ter)所示，等效命令是：
- en: '[PRE46]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![TensorBoard invoked from the command terminal](Images/t2pr_0706.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![从命令终端调用TensorBoard](Images/t2pr_0706.png)'
- en: Figure 7-6\. TensorBoard invoked from the command terminal
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. 从命令终端调用TensorBoard
- en: Remember that `logdir` is the directory path to the training logs created by
    the training callback API. The command in the preceding code uses relative path
    notation; you may use a full path if you prefer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`logdir`是由训练回调API创建的训练日志的目录路径。前面代码中的命令使用相对路径表示法；如果您愿意，可以使用完整路径。
- en: 'The output is exactly the same as seen in the Jupyter Notebook: the URL (*http://localhost:6006*).
    Open this URL with a browser to display TensorBoard.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与在Jupyter Notebook中看到的完全相同：URL（*http://localhost:6006*）。使用浏览器打开此URL以显示TensorBoard。
- en: Invoking TensorBoard by Colab Notebook
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Colab笔记本调用TensorBoard
- en: 'Now for our third option. If you are using a Google Colab notebook for this
    exercise, then invoking TensorBoard will be a little different from what you have
    seen so far. You won’t be able to open a browser on your local computer to point
    to the Colab notebook, because it will be running in Google’s cloud environment.
    You’ll thus need to install a TensorBoard notebook extension. This may be done
    in the first cell, when you import all the libraries. Just add this command and
    run it in the first Colab cell:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是我们的第三个选项。如果您在此练习中使用Google Colab笔记本，那么调用TensorBoard将与您迄今为止看到的有所不同。您将无法在本地计算机上打开浏览器指向Colab笔记本，因为它将在Google的云环境中运行。因此，您需要安装TensorBoard笔记本扩展。这可以在第一个单元格中完成，当您导入所有库时。只需添加此命令并在第一个Colab单元格中运行：
- en: '[PRE47]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Once this is done, whenever you are ready to invoke TensorBoard (such as after
    training is complete), use this command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，每当您准备调用TensorBoard（例如在训练完成后），请使用此命令：
- en: '[PRE48]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: You will see the output running inside your Colab notebook, looking as shown
    in [Figure 7-5](#tensorboard_visualization).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到输出在您的Colab笔记本中运行，看起来如[图7-5](#tensorboard_visualization)所示。
- en: Visualizing Model Overfitting Using TensorBoard
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化模型过拟合
- en: When you use TensorBoard as a callback for model training, you will get graphs
    of model accuracy and loss from the first epoch to the last.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将TensorBoard用作模型训练的回调时，您将获得从第一个时代到最后一个时代的模型准确性和损失的图表。
- en: For example, with our CIFAR-10 image classification model, you’ll see output
    like that shown in [Figure 7-5](#tensorboard_visualization). In that particular
    training run, while both training and validation accuracy are increasing and losses
    are decreasing, both trends start to flatten out, indicating that further training
    epochs are likely to deliver only marginal gains.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于我们的CIFAR-10图像分类模型，您将看到类似于[图7-5](#tensorboard_visualization)中所示的输出。在该特定训练运行中，尽管训练和验证准确性都在提高，损失在减少，但这两个趋势开始趋于平缓，表明进一步的训练时期可能只会带来较小的收益。
- en: Note also that in this run, validation accuracy is lower than training accuracy,
    while validation loss is higher than training loss. This makes sense, because
    the model performs better with training data than when tested with cross-validation
    data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在此运行中，验证准确性低于训练准确性，而验证损失高于训练损失。这是有道理的，因为模型在训练数据上的表现比在交叉验证数据上测试时更好。
- en: You may also get graphs in the Scalars tab of TensorBoard, like those shown
    in [Figure 7-7](#model_overfitting_shown_in_tensorboard).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在TensorBoard的Scalars选项卡中获得图表，就像[图7-7](#model_overfitting_shown_in_tensorboard)中所示的那样。
- en: In [Figure 7-7](#model_overfitting_shown_in_tensorboard), the darker lines indicate
    validation metrics, while lighter gray lines indicate training metrics. Model
    accuracy in the training data is much higher than in the cross-validation data,
    and the loss in the training data is much lower than in the cross-validation data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7-7](#model_overfitting_shown_in_tensorboard)中，较暗的线表示验证指标，而较浅灰色的线表示训练指标。训练数据中的模型准确性远高于交叉验证数据，而训练数据中的损失远低于交叉验证数据。
- en: You may also notice that cross-validation accuracy peaked at epoch 10, with
    a value slightly above 0.7\. After that, validation data accuracy started to decrease,
    while loss started to increase. This is a clear sign of model overfitting. What
    these graphs are telling you is that after epoch 10, your model started to memorize
    training data patterns. That doesn’t help when it encounters new, previously unseen
    data (like the cross-validation images). In fact, the model’s performance in cross
    validation (accuracy and loss) will start to worsen.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还注意到，交叉验证准确率在第10个时期达到峰值，略高于0.7。之后，验证数据准确率开始下降，而损失开始增加。这是模型过拟合的明显迹象。这些图表告诉您的是，在第10个时期之后，您的模型开始记忆训练数据的模式。当遇到新的、以前未见过的数据（如交叉验证图像）时，这并没有帮助。事实上，模型在交叉验证中的表现（准确率和损失）将开始变差。
- en: '![Model overfitting shown in TensorBoard](Images/t2pr_0707.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![在TensorBoard中显示的模型过拟合](Images/t2pr_0707.png)'
- en: Figure 7-7\. Model overfitting shown in TensorBoard
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7. 在TensorBoard中显示的模型过拟合
- en: Once you inspect these graphs, you’ll know which epoch delivered the best model
    of the training process. You’ll also be well informed about when the model starts
    to overfit and memorize its training data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您检查了这些图表，您将知道哪个时期提供了训练过程中最佳的模型。您还将了解模型何时开始过拟合并记忆其训练数据。
- en: If your model still has room for improvement, like the one in [Figure 7-5](#tensorboard_visualization),
    you may decide to increase your training epochs and keep looking for the best
    model before the overfitting pattern starts to appear (see [Figure 7-7](#model_overfitting_shown_in_tensorboard)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型仍有改进的空间，就像 [图7-5](#tensorboard_visualization) 中的那个，您可能决定增加训练时期，并在过拟合模式开始出现之前继续寻找最佳模型（参见
    [图7-7](#model_overfitting_shown_in_tensorboard)）。
- en: Visualizing the Learning Process Using TensorBoard
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化学习过程
- en: Another cool feature in TensorBoard is the histogram of weights and bias distribution.
    These are shown across each epoch as the result of training. By visualizing how
    these parameters are distributed and how their distribution changes over time,
    you gain insights into the impact of the training process.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard中的另一个很酷的功能是权重和偏差分布的直方图。这些显示为训练结果的每个时期。通过可视化这些参数是如何分布的，以及它们的分布随时间如何变化，您可以深入了解训练过程的影响。
- en: Let’s look at how to use TensorBoard to inspect model weights and bias distributions.
    This information will be in TensorBoard’s Histogram tab ([Figure 7-8](#weights_and_bias_histogram_in_tensorboar)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用TensorBoard来检查模型的权重和偏差分布。这些信息将在TensorBoard的直方图选项卡中（[图7-8](#weights_and_bias_histogram_in_tensorboar)）。
- en: '![Weights and bias histogram in TensorBoard](Images/t2pr_0708.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard中的权重和偏差直方图](Images/t2pr_0708.png)'
- en: Figure 7-8\. Weights and bias histogram in TensorBoard
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8. TensorBoard中的权重和偏差直方图
- en: On the left is the panel with all the models trained. Notice there are two models
    selected. On the right are their weights (denoted as `kernel_0`) and bias distributions
    across each epoch of training. Each row of figures represents a particular layer
    in the model. The first layer is named `conv_1`, which is what you named this
    layer back when you set up the model architecture.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是训练过的所有模型的面板。请注意有两个模型被选中。右侧是它们的权重（表示为 `kernel_0`）和偏差在每个训练时期的分布。每一行的图表示模型中的特定层。第一层被命名为
    `conv_1`，这是您在设置模型架构时给这一层取的名字。
- en: Let’s examine these figures a bit more closely. We’ll start with the conv_1
    layer shown in [Figure 7-9](#bias_distribution_in_conv_one_layer_thro).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地检查这些图表。我们将从 conv_1 层开始，如 [图7-9](#bias_distribution_in_conv_one_layer_thro)
    所示。
- en: '![Bias distribution in conv_1 layer through training](Images/t2pr_0709.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![conv_1层中的偏差分布经过训练](Images/t2pr_0709.png)'
- en: Figure 7-9\. Bias distribution in conv_1 layer through training
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9. conv_1层中的偏差分布经过训练
- en: In both models, the distribution of bias values in the `conv_1` layer definitely
    changed from the first epoch (background) to the last epoch (foreground). The
    boxes indicate that as training progresses, a certain distribution pattern of
    bias starts to emerge in all the nodes of this layer. The new values are away
    from zero, or the center of the overall distribution.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个模型中，`conv_1` 层中偏差值的分布从第一个时期（背景）到最后一个时期（前景）肯定发生了变化。方框表明随着训练的进行，这一层的所有节点中开始出现某种偏差分布模式。新值远离零，或整体分布的中心。
- en: 'Let’s also take a look at the distribution of weights. This time, let’s just
    focus on one model and one layer: conv_3\. This is shown in [Figure 7-10](#weight_distribution_in_conv_three_layer).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看一看权重的分布。这次，让我们只关注一个模型和一个层：conv_3。这在 [图7-10](#weight_distribution_in_conv_three_layer)
    中显示。
- en: '![Weight distribution in conv_3 layer through training](Images/t2pr_0710.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![conv_3层中的权重分布经过训练](Images/t2pr_0710.png)'
- en: Figure 7-10\. Weight distribution in conv_3 layer through training
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10. conv_3层中的权重分布经过训练
- en: What’s worth noting is that the distribution became broader and flatter as training
    progressed. This can be seen from the peak count of the histogram from the first
    to the last epochs, from 1.22e+4 to 7.0e+3\. This means that the histogram gradually
    becomes broader, and that there are more weights being updated with values away
    from zero (the center of the histogram).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，随着训练的进行，分布变得更广泛、更平坦。这可以从直方图从第一个到最后一个时期的峰值计数中看出，从1.22e+4到7.0e+3。这意味着直方图逐渐变得更广泛，有更多的权重被更新为远离零的值（直方图的中心）。
- en: Using TensorBoard, you can examine different layers and combinations of model
    training runs to see how they are impacted by the training process or by changes
    in model architecture. This is why TensorBoard is frequently used to visually
    inspect the model training process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorBoard，您可以检查不同层和模型训练运行的组合，看看它们如何受训练过程或模型架构的变化影响。这就是为什么TensorBoard经常用于直观检查模型训练过程。
- en: Wrapping Up
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you saw some of the most popular methods for tracking the
    model training process. This chapter presented the concept of model checkpointing
    and offered two important ways to help you manage how to save models during training:
    either save the model at every epoch, or only save the model when an incremental
    improvement in model metrics is validated. You also learned that model accuracy
    in cross validation determines when the model starts to overfit the training data.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了一些用于跟踪模型训练过程的最流行方法。本章介绍了模型检查点的概念，并提供了两种重要的方法来帮助您管理如何在训练过程中保存模型：在每个周期保存模型，或者仅在模型指标有增量改进时保存模型。您还了解到，交叉验证中的模型准确度决定了模型何时开始过拟合训练数据。
- en: Another important tool you learned about in this chapter is TensorBoard, which
    can be used to visualize the training process. TensorBoard presents visual images
    of basic metrics (accuracy and loss) trends through training epochs. It also lets
    you inspect the weight and bias distributions of each layer throughout the training.
    All these techniques are easy to implement in the training routine via callbacks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解到的另一个重要工具是TensorBoard，它可以用来可视化训练过程。TensorBoard通过训练周期展示基本指标（准确度和损失）的趋势的可视图像。它还允许您检查每个层的权重和偏差分布。所有这些技术都可以通过回调函数轻松实现在训练过程中。
- en: In the next chapter, you will see how to implement distributed training in TensorFlow,
    which takes advantage of performant compute units such as GPUs to provide shorter
    training times.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将看到如何在TensorFlow中实现分布式训练，利用诸如GPU之类的高性能计算单元，以提供更短的训练时间。
- en: ^([1](ch07.xhtml#ch06fn01-marker)) Two other common and useful functions not
    covered here are [LearningRateScheduler](https://oreil.ly/CyuGs) and [CSVLogger](https://oreil.ly/vmeaY).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#ch06fn01-marker)) 这里没有涵盖的另外两个常见且有用的功能是[LearningRateScheduler](https://oreil.ly/CyuGs)和[CSVLogger](https://oreil.ly/vmeaY)。
