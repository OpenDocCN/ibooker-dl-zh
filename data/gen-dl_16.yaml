- en: Chapter 12\. World Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。世界模型
- en: This chapter introduces one of the most interesting applications of generative
    models in recent years, namely their use within so-called world models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了近年来生成模型最有趣的应用之一，即它们在所谓的世界模型中的使用。
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In March 2018, David Ha and Jürgen Schmidhuber published their “World Models”
    paper.^([1](ch12.xhtml#idm45387001861120)) The paper showed how it is possible
    to train a model that can learn how to perform a particular task through experimentation
    within its own generated dream environment, rather than inside the real environment.
    It is an excellent example of how generative modeling can be used to solve practical
    problems, when applied alongside other machine learning techniques such as reinforcement
    learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年3月，David Ha和Jürgen Schmidhuber发表了他们的“World Models”论文。该论文展示了如何通过在自己生成的梦境环境中进行实验来训练一个模型，而不是在真实环境中。这是一个很好的例子，说明了当与强化学习等其他机器学习技术一起应用时，生成建模如何解决实际问题。
- en: A key component of the architecture is a generative model that can construct
    a probability distribution for the next possible state, given the current state
    and action. Having built up an understanding of the underlying physics of the
    environment through random movements, the model is then able to train itself from
    scratch on a new task, entirely within its own internal representation of the
    environment. This approach led to world-best scores for both of the tasks on which
    it was tested.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的一个关键组件是一个生成模型，它可以构建给定当前状态和动作的下一个可能状态的概率分布。通过随机移动建立对环境基础物理的理解后，模型能够在自己对环境的内部表示中完全自行训练新任务。这种方法导致了在测试时两个任务的世界最佳得分。
- en: In this chapter we will explore the model from the paper in detail, with particular
    focus on a task that requires the agent to learn how to drive a car around a virtual
    racetrack as fast as possible. While we will be using a 2D computer simulation
    as our environment, the same technique could also be applied to real-world scenarios
    where testing strategies in the live environment is expensive or infeasible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细探讨论文中的模型，特别关注一个需要代理学习如何尽可能快地在虚拟赛道上驾驶汽车的任务。虽然我们将使用2D计算机模拟作为我们的环境，但相同的技术也可以应用于在现实环境中测试策略昂贵或不可行的情况。
- en: Tip
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In this chapter we will reference the excellent TensorFlow implementation of
    the “World Models” paper available publicly on [GitHub](https://oreil.ly/_OlJX),
    which I encourage you to clone and run yourself!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将引用“World Models”论文的优秀TensorFlow实现，该实现公开在GitHub上，我鼓励您克隆并运行！
- en: Before we start exploring the model, we need to take a closer look at the concept
    of reinforcement learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索模型之前，我们需要更仔细地了解强化学习的概念。
- en: Reinforcement Learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Reinforcement learning can be defined as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以定义如下：
- en: Reinforcement learning (RL) is a field of machine learning that aims to train
    an agent to perform optimally within a given environment, with respect to a particular
    goal.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种机器学习领域，旨在训练一个代理在给定环境中以达到特定目标的最佳表现。
- en: While both discriminative modeling and generative modeling aim to minimize a
    loss function over a dataset of observations, reinforcement learning aims to maximize
    the long-term reward of an agent in a given environment. It is often described
    as one of the three major branches of machine learning, alongside *supervised
    learning* (predicting using labeled data) and *unsupervised learning* (learning
    structure from unlabeled data).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然判别建模和生成建模都旨在在观察数据集上最小化损失函数，但强化学习旨在最大化给定环境中代理的长期奖励。它通常被描述为机器学习的三大分支之一，与*监督学习*（使用标记数据进行预测）和*无监督学习*（从未标记数据中学习结构）并列。
- en: 'Let’s first introduce some key terminology related to reinforcement learning:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先介绍一些与强化学习相关的关键术语：
- en: Environment
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 环境
- en: The world in which the agent operates. It defines the set of rules that govern
    the game state update process and reward allocation, given the agent’s previous
    action and current game state. For example, if we were teaching a reinforcement
    learning algorithm to play chess, the environment would consist of the rules that
    govern how a given action (e.g., the pawn move `e2e4`) affects the next game state
    (the new positions of the pieces on the board) and would also specify how to assess
    if a given position is checkmate and allocate the winning player a reward of 1
    after the winning move.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 代理操作的世界。它定义了规则集，这些规则管理游戏状态更新过程和奖励分配，考虑到代理的先前动作和当前游戏状态。例如，如果我们正在教一个强化学习算法下棋，环境将包括规定给定动作（例如，兵的移动`e2e4`）如何影响下一个游戏状态（棋盘上棋子的新位置）的规则，并且还会指定如何评估给定位置是否为将军，并在获胜移动后为获胜玩家分配奖励1。
- en: Agent
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代理
- en: The entity that takes actions in the environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中采取行动的实体。
- en: Game state
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏状态
- en: The data that represents a particular situation that the agent may encounter
    (also just called a *state*). For example, a particular chessboard configuration
    with accompanying game information such as which player will make the next move.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可能会遇到的特定情况的数据（也称为*状态*）。例如，具有伴随游戏信息的特定棋盘配置，例如哪个玩家将进行下一步移动。
- en: Action
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 行动
- en: A feasible move that an agent can make.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以采取的可行移动。
- en: Reward
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励
- en: The value given back to the agent by the environment after an action has been
    taken. The agent aims to maximize the long-term sum of its rewards. For example,
    in a game of chess, checkmating the opponent’s king has a reward of 1 and every
    other move has a reward of 0\. Other games have rewards constantly awarded throughout
    the episode (e.g., points in a game of *Space Invaders*).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 环境在采取行动后向代理返回的值。代理的目标是最大化其奖励的长期总和。例如，在国际象棋游戏中，将对手的国王将军的奖励为1，而其他每一步的奖励为0。其他游戏在整个episode中不断授予奖励（例如，在*Space
    Invaders*游戏中的得分）。
- en: Episode
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Episode
- en: One run of an agent in the environment; this is also called a *rollout*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中代理的一次运行；这也被称为*rollout*。
- en: Timestep
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步
- en: For a discrete event environment, all states, actions, and rewards are subscripted
    to show their value at timestep <math alttext="t"><mi>t</mi></math> .
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散事件环境，所有状态、动作和奖励都被标注以显示它们在时间步<math alttext="t"><mi>t</mi></math>的值。
- en: The relationship between these concepts is shown in [Figure 12-1](#rl).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念之间的关系在[图12-1](#rl)中显示。
- en: '![](Images/gdl2_1201.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1201.png)'
- en: Figure 12-1\. Reinforcement learning diagram
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. 强化学习图表
- en: The environment is first initialized with a current game state, <math alttext="s
    0"><msub><mi>s</mi> <mn>0</mn></msub></math> . At timestep <math alttext="t"><mi>t</mi></math>
    , the agent receives the current game state <math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math> and uses this to decide on its next best action <math
    alttext="a Subscript t"><msub><mi>a</mi> <mi>t</mi></msub></math> , which it then
    performs. Given this action, the environment then calculates the next state <math
    alttext="s Subscript t plus 1"><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>
    and reward <math alttext="r Subscript t plus 1"><msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>
    and passes these back to the agent, for the cycle to begin again. The cycle continues
    until the end criterion of the episode is met (e.g., a given number of timesteps
    elapse or the agent wins/loses).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 环境首先使用当前游戏状态<math alttext="s 0"><msub><mi>s</mi> <mn>0</mn></msub></math>进行初始化。在时间步<math
    alttext="t"><mi>t</mi></math>，代理接收当前游戏状态<math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math>并使用它来决定下一个最佳动作<math alttext="a Subscript t"><msub><mi>a</mi>
    <mi>t</mi></msub></math>，然后执行。给定这个动作，环境然后计算下一个状态<math alttext="s Subscript t plus
    1"><msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>和奖励<math
    alttext="r Subscript t plus 1"><msub><mi>r</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>，并将它们传递回代理，以便循环再次开始。这个循环会持续直到episode的结束条件满足（例如，经过给定数量的时间步或代理赢得/输掉）。
- en: How can we design an agent to maximize the sum of rewards in a given environment?
    We could build an agent that contains a set of rules for how to respond to any
    given game state. However, this quickly becomes infeasible as the environment
    becomes more complex and doesn’t ever allow us to build an agent that has superhuman
    ability in a particular task, as we are hardcoding the rules. Reinforcement learning
    involves creating an agent that can learn optimal strategies by itself in complex
    environments through repeated play.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计一个代理来最大化在给定环境中的奖励总和？我们可以构建一个包含一组规则的代理，用于如何响应任何给定的游戏状态。然而，随着环境变得更加复杂，这很快变得不可行，并且永远不允许我们构建一个在特定任务中具有超人能力的代理，因为我们正在硬编码规则。强化学习涉及创建一个代理，通过反复游戏在复杂环境中学习最佳策略。
- en: Let’s now take a look at the `CarRacing` environment that simulates a car driving
    around a track.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下模拟汽车在赛道上行驶的`CarRacing`环境。
- en: The CarRacing Environment
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 赛车环境
- en: '`CarRacing` is an environment that is available through the [Gymnasium](https://gymnasium.farama.org)
    package. Gymnasium is a Python library for developing reinforcement learning algorithms
    that contains several classic reinforcement learning environments, such as `CartPole`
    and `Pong`, as well as environments that present more complex challenges, such
    as training an agent to walk on uneven terrain or win an Atari game.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`CarRacing`是通过[Gymnasium](https://gymnasium.farama.org)包提供的环境。Gymnasium是一个用于开发强化学习算法的Python库，其中包含几个经典的强化学习环境，如`CartPole`和`Pong`，以及提出更复杂挑战的环境，比如训练代理在不平坦地形上行走或赢得Atari游戏。'
- en: Gymnasium
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gymnasium
- en: Gymnasium is a maintained fork of OpenAI’s Gym library—since 2021, further development
    of Gym has shifted to Gymnasium. In this book, we therefore refer to Gymnasium
    environments as Gym environments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium是OpenAI的Gym库的维护分支——自2021年以来，Gym的进一步开发已转移到Gymnasium。因此，在本书中，我们将Gymnasium环境称为Gym环境。
- en: All of the environments provide a *step* method through which you can submit
    a given action; the environment will return the next state and the reward. By
    repeatedly calling the step method with the actions chosen by the agent, you can
    play out an episode in the environment. There is also a *reset* method for returning
    the environment to its initial state and a *render* method that allows you to
    watch your agent perform in a given environment. This is useful for debugging
    and finding areas where your agent could improve.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有环境都提供了一个*step*方法，通过该方法您可以提交一个给定的动作；环境将返回下一个状态和奖励。通过反复调用代理选择的动作来调用step方法，您可以在环境中玩出一个episode。还有一个*reset*方法，用于将环境恢复到初始状态，以及一个*render*方法，允许您观看您的代理在给定环境中执行。这对于调试和找到代理可以改进的地方非常有用。
- en: 'Let’s see how the game state, action, reward, and episode are defined for the
    `CarRacing` environment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`CarRacing`环境中游戏状态、动作、奖励和episode是如何定义的：
- en: Game state
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏状态
- en: A 64 × 64–pixel RGB image depicting an overhead view of the track and car.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个64×64像素的RGB图像，描绘了赛道和汽车的俯视图。
- en: Action
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 动作
- en: 'A set of three values: the steering direction (–1 to 1), acceleration (0 to
    1), and braking (0 to 1). The agent must set all three values at each timestep.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一组三个值：方向盘方向（-1到1）、加速度（0到1）和刹车（0到1）。代理必须在每个时间步设置这三个值。
- en: Reward
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励
- en: A negative penalty of –0.1 for each timestep taken and a positive reward of
    1,000/ <math alttext="upper N"><mi>N</mi></math> if a new track tile is visited,
    where <math alttext="upper N"><mi>N</mi></math> is the total number of tiles that
    make up the track.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步骤都会受到-0.1的负惩罚，如果访问了新的赛道瓷砖，则会获得1000/<math alttext="upper N"><mi>N</mi></math>的正奖励，其中<math
    alttext="upper N"><mi>N</mi></math>是构成赛道的瓷砖总数。
- en: Episode
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 剧集
- en: The episode ends when the car completes the track or drives off the edge of
    the environment, or when 3,000 timesteps have elapsed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当汽车完成赛道或驶出环境边缘，或者经过了3000个时间步骤时，剧集结束。
- en: These concepts are shown on a graphical representation of a game state in [Figure 12-2](#car_racing_example).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在[图12-2](#car_racing_example)中的游戏状态的图形表示中显示。
- en: '![](Images/gdl2_1202.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1202.png)'
- en: Figure 12-2\. A graphical representation of one game state in the `CarRacing`
    environment
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. `CarRacing`环境中一个游戏状态的图形表示
- en: Perspective
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视角
- en: We should imagine the agent floating above the track and controlling the car
    from a bird’s-eye view, rather than viewing the track from the driver’s perspective.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该想象代理人漂浮在赛道上方，从鸟瞰视角控制汽车，而不是从驾驶员的视角看赛道。
- en: World Model Overview
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 世界模型概述
- en: We’ll now cover a high-level overview of the entire world model architecture
    and training process, before diving into each component in more detail.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对整个世界模型架构和训练过程进行高层概述，然后深入研究每个组件。
- en: Architecture
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: 'The solution consists of three distinct parts, as shown in [Figure 12-3](#world_models_diagram),
    that are trained separately:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案由三个不同部分组成，如[图12-3](#world_models_diagram)所示，它们分别进行训练：
- en: V
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: V
- en: A variational autoencoder (VAE)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAE）
- en: M
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: M
- en: A recurrent neural network with a mixture density network (MDN-RNN)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 带有混合密度网络（MDN-RNN）的递归神经网络
- en: C
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: C
- en: A controller
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制器
- en: '![](Images/gdl2_1203.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1203.png)'
- en: Figure 12-3\. World model architecture diagram
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 世界模型架构图
- en: The VAE
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VAE
- en: When you make decisions while driving, you don’t actively analyze every single
    pixel in your view—instead, you condense the visual information into a smaller
    number of latent entities, such as the straightness of the road, upcoming bends,
    and your position relative to the road, to inform your next action.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当您驾驶时做出决策时，您并不会主动分析视野中的每个像素—而是将视觉信息压缩成较少数量的潜在实体，例如道路的直线程度、即将到来的弯道以及您相对于道路的位置，以指导您的下一个动作。
- en: We saw in [Chapter 3](ch03.xhtml#chapter_vae) how a VAE can take a high-dimensional
    input image and condense it into a latent random variable that approximately follows
    a standard Gaussian distribution, through minimization of the reconstruction error
    and KL divergence. This ensures that the latent space is continuous and that we
    are able to easily sample from it to generate meaningful new observations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](ch03.xhtml#chapter_vae)中看到，VAE可以将高维输入图像压缩成一个潜在随机变量，该变量近似遵循标准高斯分布，通过最小化重构误差和KL散度。这确保了潜在空间是连续的，我们能够轻松从中进行采样以生成有意义的新观察。
- en: In the car racing example, the VAE condenses the 64 × 64 × 3 (RGB) input image
    into a 32-dimensional normally distributed random variable, parameterized by two
    variables, `mu` and `logvar`. Here, `logvar` is the logarithm of the variance
    of the distribution. We can sample from this distribution to produce a latent
    vector `z` that represents the current state. This is passed on to the next part
    of the network, the MDN-RNN.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车赛道示例中，VAE将64×64×3（RGB）输入图像压缩成一个32维正态分布的随机变量，由两个变量`mu`和`logvar`参数化。这里，`logvar`是分布方差的对数。我们可以从该分布中采样以产生代表当前状态的潜在向量`z`。这将传递给网络的下一个部分，MDN-RNN。
- en: The MDN-RNN
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MDN-RNN
- en: As you drive, each subsequent observation isn’t a complete surprise to you.
    If the current observation suggests a left turn in the road ahead and you turn
    the wheel to the left, you expect the next observation to show that you are still
    in line with the road.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当您驾驶时，每个后续观察对您来说并不是完全意外的。如果当前观察表明前方道路左转，您向左转动方向盘，您期望下一个观察显示您仍然与道路保持一致。
- en: If you didn’t have this ability, your car would probably snake all over the
    road as you wouldn’t be able to see that a slight deviation from the center is
    going to be worse in the next timestep unless you do something about it now.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有这种能力，您的汽车可能会在道路上蛇行，因为您无法看到稍微偏离中心会在下一个时间步骤中变得更糟，除非您现在采取措施。
- en: This forward thinking is the job of the MDN-RNN, a network that tries to predict
    the distribution of the next latent state based on the previous latent state and
    the previous action.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种前瞻性是MDN-RNN的任务，它试图根据先前的潜在状态和先前的动作来预测下一个潜在状态的分布。
- en: Specifically, the MDN-RNN is an LSTM layer with 256 hidden units followed by
    a mixture density network (MDN) output layer that allows for the fact that the
    next latent state could actually be drawn from any one of several normal distributions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，MDN-RNN是一个具有256个隐藏单元的LSTM层，后面跟着一个混合密度网络（MDN）输出层，允许下一个潜在状态实际上可以从几个正态分布中的任何一个中抽取。
- en: The same technique was applied by one of the authors of the “World Models” paper,
    David Ha, to a [handwriting generation](https://oreil.ly/WmPGp) task, as shown
    in [Figure 12-4](#handwriting_MDN), to describe the fact that the next pen point
    could land in any one of the distinct red areas.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: “世界模型”论文的一位作者David Ha也将相同的技术应用于[手写生成](https://oreil.ly/WmPGp)任务，如[图12-4](#handwriting_MDN)所示，描述了下一个笔尖可能落在几个不同红色区域中的事实。
- en: '![](Images/gdl2_1204.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1204.png)'
- en: Figure 12-4\. MDN for handwriting generation
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 用于手写生成的MDN
- en: In the car racing example, we allow for each element of the next observed latent
    state to be drawn from any one of five normal distributions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车赛道示例中，我们允许下一个观察到的潜在状态的每个元素都可以从五个正态分布中的任何一个中抽取。
- en: The controller
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制器
- en: Until this point, we haven’t mentioned anything about choosing an action. That
    responsibility lies with the controller. The controller is a densely connected
    neural network, where the input is a concatenation of `z` (the current latent
    state sampled from the distribution encoded by the VAE) and the hidden state of
    the RNN. The three output neurons correspond to the three actions (turn, accelerate,
    brake) and are scaled to fall in the appropriate ranges.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有提到选择动作的事情。这个责任在于控制器。控制器是一个密集连接的神经网络，其中输入是`z`（从VAE编码的分布中采样的当前潜在状态）和RNN的隐藏状态的串联。三个输出神经元对应于三个动作（转向、加速、刹车），并且被缩放以落入适当的范围内。
- en: The controller is trained using reinforcement learning as there is no training
    dataset that will tell us that a certain action is *good* and another is *bad*.
    Instead, the agent discovers this for itself through repeated experimentation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器使用强化学习进行训练，因为没有训练数据集会告诉我们某个动作是*好*还是*坏*。相反，代理通过反复实验自己发现这一点。
- en: As we shall see later in the chapter, the crux of the “World Models” paper is
    that it demonstrates how this reinforcement learning can take place within the
    agent’s own generative model of the environment, rather than the Gym environment.
    In other words, it takes place in the agent’s *hallucinated* version of how the
    environment behaves, rather than the real thing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章后面看到的那样，“世界模型”论文的关键在于它展示了如何在代理的环境生成模型中进行强化学习，而不是在Gym环境中。换句话说，它发生在代理对环境行为的*幻想*版本中，而不是真实的环境中。
- en: 'To understand the different roles of the three components and how they work
    together, we can imagine a dialogue between them:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解三个组件的不同角色以及它们如何共同工作，我们可以想象它们之间的对话：
- en: '*VAE* (looking at latest 64 × 64 × 3 observation): This looks like a straight
    road, with a slight left bend approaching, with the car facing in the direction
    of the road (`z`).'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*VAE*（查看最新的64×64×3观察）：这看起来像一条笔直的道路，接近一个轻微的左弯，汽车面向道路的方向（`z`）。'
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*RNN*: Based on that description (`z`) and the fact that the controller chose
    to accelerate hard at the last timestep (`action`), I will update my hidden state
    (`h`) so that the next observation is predicted to still be a straight road, but
    with slightly more left turn in view.'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*RNN*：基于那个描述（`z`）和控制器选择在上一个时间步加速的事实（`action`），我将更新我的隐藏状态（`h`），以便下一个观察被预测为仍然是一条笔直的道路，但在视野中有稍微更多的左转。'
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Controller*: Based on the description from the VAE (`z`) and the current hidden
    state from the RNN (`h`), my neural network outputs `[0.34, 0.8, 0]` as the next
    action.'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*控制器*：基于来自VAE的描述（`z`）和来自RNN的当前隐藏状态（`h`），我的神经网络输出`[0.34, 0.8, 0]`作为下一个动作。'
- en: The action from the controller is then passed to the environment, which returns
    an updated observation, and the cycle begins again.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，控制器的动作传递给环境，环境返回更新后的观察结果，循环再次开始。
- en: Training
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training process consists of five steps, run in sequence, which are outlined
    here:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括五个步骤，按顺序运行，概述如下：
- en: Collect random rollout data. Here, the agent does not care about the given task,
    but instead simply explores the environment using random actions. Multiple episodes
    are simulated and the observed states, actions, and rewards at each timestep are
    stored. The idea is to build up a dataset of how the physics of the environment
    works, which the VAE can then learn from to capture the states efficiently as
    latent vectors. The MDN-RNN can then subsequently learn how the latent vectors
    evolve over time.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集随机回滚数据。在这里，代理不关心给定任务，而是简单地使用随机动作探索环境。多个剧集被模拟，每个时间步的观察状态、动作和奖励被存储。这个想法是建立一个关于环境物理工作方式的数据集，然后VAE可以从中学习以有效地捕捉状态作为潜在向量。MDN-RNN随后可以学习潜在向量随时间的演变方式。
- en: Train the VAE. Using the randomly collected data, we train a VAE on the observation
    images.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练VAE。使用随机收集的数据，我们在观察图像上训练VAE。
- en: Collect data to train the MDN-RNN. Once we have a trained VAE, we use it to
    encode each of the collected observations into `mu` and `logvar` vectors, which
    are saved alongside the current action and reward.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据以训练MDN-RNN。一旦我们有了训练好的VAE，我们使用它将每个收集到的观察编码为`mu`和`logvar`向量，并将其保存在当前动作和奖励旁边。
- en: Train the MDN-RNN. We take batches of episodes and load the corresponding `mu`,
    `logvar`, `action`, and `reward` variables at each timestep that were generated
    in step 3\. We then sample a `z` vector from the `mu` and `logvar` vectors. Given
    the current `z` vector, `action`, and `reward`, the MDN-RNN is then trained to
    predict the subsequent `z` vector and `reward`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练MDN-RNN。我们获取一批批次的剧集，并在每个时间步加载在步骤3生成的`mu`、`logvar`、`action`和`reward`变量。然后我们从`mu`和`logvar`向量中采样一个`z`向量。给定当前的`z`向量、`action`和`reward`，然后训练MDN-RNN来预测随后的`z`向量和`reward`。
- en: Train the controller. With a trained VAE and RNN, we can now train the controller
    to output an action given the current `z` and hidden state, `h`, of the RNN. The
    controller uses an evolutionary algorithm, CMA-ES, as its optimizer. The algorithm
    rewards matrix weightings that generate actions that lead to overall high scores
    on the task, so that future generations are also likely to inherit this desired
    behavior.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练控制器。通过训练好的VAE和RNN，我们现在可以训练控制器，以输出一个动作，给定当前的`z`和RNN的隐藏状态`h`。控制器使用进化算法CMA-ES作为其优化器。该算法奖励生成导致任务整体得分较高的动作的矩阵权重，以便未来的代际也可能继承这种期望的行为。
- en: Let’s now take look at each of these steps in more detail.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在更详细地看看每个步骤。
- en: Collecting Random Rollout Data
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集随机回滚数据
- en: The first step is to collect rollout data from the environment, using an agent
    taking random actions. This may seem strange, given we ultimately want our agent
    to learn how to take intelligent actions, but this step will provide the data
    that the agent will use to learn how the world operates and how its actions (albeit
    random at first) influence subsequent observations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从环境中收集rollout数据，使用一个代理器执行随机动作。这可能看起来很奇怪，因为我们最终希望我们的代理器学会如何采取智能动作，但这一步将提供代理器将用于学习世界运作方式以及其动作（尽管起初是随机的）如何影响随后观察的数据。
- en: We can capture multiple episodes in parallel by spinning up multiple Python
    processes, each running a separate instance of the environment. Each process will
    run on a separate core, so if your machine has lots of cores you can collect data
    much faster than if you only have a few cores.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过启动多个Python进程并行捕获多个episode，每个进程运行环境的单独实例。每个进程将在单独的核心上运行，因此如果您的计算机有很多核心，您可以比只有几个核心时更快地收集数据。
- en: 'The hyperparameters used by this step are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步使用的超参数如下：
- en: '`parallel_processes`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`parallel_processes`'
- en: The number of parallel processes to run (e.g., `8` if your machine has ≥8 cores)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行的并行进程数（例如，如果您的计算机有≥8个核心，则为`8`）
- en: '`max_trials`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_trials`'
- en: How many episodes each process should run in total (e.g., `125`, so 8 processes
    would create 1,000 episodes overall)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个进程应总共运行多少个episode（例如，`125`，因此8个进程将总共创建1,000个episode）
- en: '`max_frames`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_frames`'
- en: The maximum number of timesteps per episode (e.g., `300`)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个episode的最大时间步数（例如，`300`）
- en: '[Figure 12-5](#observations_excerpt) shows an excerpt from frames 40 to 59
    of one episode, as the car approaches a corner, alongside the randomly chosen
    action and reward. Note how the reward changes to 3.22 as the car rolls over new
    track tiles but is otherwise –0.1.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-5](#observations_excerpt)显示了一个episode的第40到59帧的摘录，汽车驶向一个拐角，同时显示了随机选择的动作和奖励。请注意，随着汽车经过新的赛道瓷砖，奖励变为3.22，但其他情况下为-0.1。'
- en: '![Frames 40 to 59 of one episode](Images/gdl2_1205.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![一个episode的第40到59帧](Images/gdl2_1205.png)'
- en: Figure 12-5\. Frames 40 to 59 of one episode
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5。一个episode的第40到59帧
- en: Training the VAE
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练VAE
- en: 'We now build a generative model (a VAE) on this collected data. Remember, the
    aim of the VAE is to allow us to collapse one 64 × 64 × 3 image into a normally
    distributed random variable `z`, whose distribution is parameterized by two vectors,
    `mu` and `logvar`. Each of these vectors is of length 32\. The hyperparameters
    of this step are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在收集的数据上构建一个生成模型（VAE）。请记住，VAE的目的是让我们将一个64×64×3的图像折叠成一个正态分布的随机变量`z`，其分布由两个向量`mu`和`logvar`参数化。这两个向量的长度均为32。这一步的超参数如下：
- en: '`vae_batch_size`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`vae_batch_size`'
- en: The batch size to use when training the VAE (how many observations per batch)
    (e.g., `100`)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练VAE时使用的批量大小（每批次观察数量）（例如，`100`）
- en: '`z_size`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`z_size`'
- en: The length of latent `z` vector (and therefore `mu` and `logvar` variables)
    (e.g., `32`)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在`z`向量的长度（因此`mu`和`logvar`变量）（例如，`32`）
- en: '`vae_num_epoch`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`vae_num_epoch`'
- en: The number of training epochs (e.g., `10`)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时的epoch数量（例如，`10`）
- en: The VAE Architecture
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAE架构
- en: 'As we have seen previously, Keras allows us to not only define the VAE model
    that will be trained end-to-end, but also additional submodels that define the
    encoder and decoder of the trained network separately. These will be useful when
    we want to encode a specific image or decode a given `z` vector, for example.
    We’ll define the VAE model and three submodels, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，Keras允许我们不仅定义将进行端到端训练的VAE模型，还可以定义额外的子模型，分别定义训练网络的编码器和解码器。例如，当我们想要对特定图像进行编码或解码给定的`z`向量时，这些将非常有用。我们将定义VAE模型和三个子模型，如下所示：
- en: '`vae`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`vae`'
- en: This is the end-to-end VAE that is trained. It accepts a 64 × 64 × 3 image as
    input and outputs a reconstructed 64 × 64 × 3 image.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经过训练的端到端VAE。它接受一个64×64×3的图像作为输入，并输出一个重建的64×64×3的图像。
- en: '`encode_mu_logvar`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_mu_logvar`'
- en: This accepts a 64 × 64 × 3 image as input and outputs the `mu` and `logvar`
    vectors corresponding to this input. Running the same input image through this
    model multiple times will produce the same `mu` and `logvar` vectors each time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这接受一个64×64×3的图像作为输入，并输出与该输入对应的`mu`和`logvar`向量。多次通过该模型运行相同的输入图像将每次产生相同的`mu`和`logvar`向量。
- en: '`encode`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode`'
- en: This accepts a 64 × 64 × 3 image as input and outputs a sampled `z` vector.
    Running the same input image through this model multiple times will produce a
    different `z` vector each time, using the calculated `mu` and `logvar` values
    to define the sampling distribution.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这接受一个64×64×3的图像作为输入，并输出一个采样的`z`向量。多次通过该模型运行相同的输入图像将每次产生不同的`z`向量，使用计算出的`mu`和`logvar`值来定义采样分布。
- en: '`decode`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`decode`'
- en: This accepts a `z` vector as input and returns the reconstructed 64 × 64 × 3
    image.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这接受一个`z`向量作为输入，并返回重建的64×64×3图像。
- en: A diagram of the model and submodels is shown in [Figure 12-6](#car_racing_vae).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和子模型的图表显示在[图12-6](#car_racing_vae)中。
- en: '![The VAE architecture for the World Models paper](Images/gdl2_1206.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![《World Models》论文中的VAE架构](Images/gdl2_1206.png)'
- en: Figure 12-6\. The VAE architecture from the “World Models” paper
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6。《World Models》论文中的VAE架构
- en: Exploring the VAE
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索VAE
- en: We’ll now take a look at the output from the VAE and each submodel and then
    see how the VAE can be used to generate completely new track observations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看VAE和每个子模型的输出，然后看看VAE如何用于生成全新的赛道观察。
- en: The VAE model
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VAE模型
- en: If we feed the VAE with an observation, it is able to accurately reconstruct
    the original image, as shown in [Figure 12-7](#vae_full_model). This is useful
    to visually check that the VAE is working correctly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一个观察输入到VAE中，它能够准确重建原始图像，如[图12-7](#vae_full_model)所示。这对于直观检查VAE是否正常工作非常有用。
- en: '![Input and output of the VAE model](Images/gdl2_1207.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![VAE模型的输入和输出](Images/gdl2_1207.png)'
- en: Figure 12-7\. The input and output from the VAE model
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7。VAE模型的输入和输出
- en: The encoder models
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器模型
- en: If we feed the `encode_mu_logvar` model with an observation, the output is the
    generated `mu` and `logvar` vectors describing a multivariate normal distribution.
    The `encode` model goes one step further by sampling a particular `z` vector from
    this distribution. The diagram showing the output from the two encoder models
    is shown in [Figure 12-8](#vae_encoder_output).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用一个观察来喂`encode_mu_logvar`模型，输出将是描述多元正态分布的生成`mu`和`logvar`向量。`encode`模型进一步采样特定的`z`向量。显示两个编码器模型输出的图表在[图12-8](#vae_encoder_output)中。
- en: '![The output from the encoder models](Images/gdl2_1208.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![来自编码器模型的输出](Images/gdl2_1208.png)'
- en: Figure 12-8\. The output from the encoder models
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-8。编码器模型的输出
- en: The latent variable `z` is sampled from the Gaussian defined by `mu` and `logvar`
    by sampling from a standard Gaussian and then scaling and shifting the sampled
    vector ([Example 12-1](#example-8_2)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 潜变量`z`是从由`mu`和`logvar`定义的高斯分布中采样的，通过从标准高斯中采样，然后缩放和移位采样的向量（[示例12-1](#example-8_2)）。
- en: Example 12-1\. Sampling `z` from the multivariate normal distribution defined
    by `mu` and `logvar`
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例12-1。从由`mu`和`logvar`定义的多元正态分布中采样`z`
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The decoder model
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器模型
- en: The `decode` model accepts a `z` vector as input and reconstructs the original
    image. In [Figure 12-9](#vae_decoder) we linearly interpolate two of the dimensions
    of `z` to show how each dimension appears to encode a particular aspect of the
    track—in this example `z[4]` controls the immediate left/right direction of the
    track nearest the car and `z[7]` controls the sharpness of the approaching left
    turn.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`decode`模型接受一个`z`向量作为输入，并重构原始图像。在[图12-9](#vae_decoder)中，我们线性插值`z`的两个维度，以展示每个维度似乎编码轨道的特定方面——在这个例子中，`z[4]`控制了最接近汽车的轨道的左右方向，`z[7]`控制了即将到来的左转的急剧程度。'
- en: This shows that the latent space that the VAE has learned is continuous and
    can be used to generate new track segments that have never before been observed
    by the agent.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明VAE学习到的潜在空间是连续的，可以用来生成代理以前从未观察过的新轨迹段。
- en: '![A linear interpolation of two dimensions of z](Images/gdl2_1209.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![z的两个维度的线性插值](Images/gdl2_1209.png)'
- en: Figure 12-9\. A linear interpolation of two dimensions of `z`
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9。`z`的两个维度的线性插值
- en: Collecting Data to Train the MDN-RNN
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据以训练MDN-RNN
- en: Now that we have a trained VAE, we can use this to generate training data for
    our MDN-RNN.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个经过训练的VAE，我们可以用它来为我们的MDN-RNN生成训练数据。
- en: In this step, we pass all of the random rollout observations through the `encode_mu_logvar`
    model and store the `mu` and `logvar` vectors corresponding to each observation.
    This encoded data, along with the already collected `action`, `reward`, and `done`
    variables, will be used to train the MDN-RNN. This process is shown in [Figure 12-10](#creating_rnn_training).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们通过`encode_mu_logvar`模型传递所有随机回滚观察，并存储与每个观察相对应的`mu`和`logvar`向量。这些编码数据，以及已经收集的`action`、`reward`和`done`变量，将用于训练MDN-RNN。这个过程在[图12-10](#creating_rnn_training)中显示。
- en: '![Creating the RNN training dataset](Images/gdl2_1210.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![创建RNN训练数据集](Images/gdl2_1210.png)'
- en: Figure 12-10\. Creating the MDN-RNN training dataset
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-10。创建MDN-RNN训练数据集
- en: Training the MDN-RNN
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练MDN-RNN
- en: We can now train the MDN-RNN to predict the distribution of the next `z` vector
    and reward one timestep ahead into the future, given the current `z` vector, current
    action, and previous reward. We can then use the internal hidden state of the
    RNN (which can be thought of as the model’s current understanding of the environment
    dynamics) as part of the input into the controller, which will ultimately decide
    on the best next action to take.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练MDN-RNN来预测下一个`z`向量的分布，并在未来一个时间步骤内奖励，给定当前的`z`向量、当前的动作和先前的奖励。然后我们可以使用RNN的内部隐藏状态（可以被视为模型对环境动态的当前理解）作为控制器的输入之一，控制器最终将决定最佳的下一步动作。
- en: 'The hyperparameters of this step of the process are as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的超参数如下：
- en: '`rnn_batch_size`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_batch_size`'
- en: The batch size to use when training the MDN-RNN (how many sequences per batch)
    (e.g., `100`)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 训练MDN-RNN时使用的批量大小（每批次多少个序列）（例如，`100`）
- en: '`rnn_num_steps`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_num_steps`'
- en: The total number of training iterations (e.g., `4000`)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的总迭代次数（例如，`4000`）
- en: The MDN-RNN Architecture
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MDN-RNN架构
- en: The architecture of the MDN-RNN is shown in [Figure 12-11](#rnn_model).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MDN-RNN的架构在[图12-11](#rnn_model)中显示。
- en: '![The RNN architecture](Images/gdl2_1211.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![RNN架构](Images/gdl2_1211.png)'
- en: Figure 12-11\. The MDN-RNN architecture
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-11。MDN-RNN架构
- en: The MDN-RNN consists of an LSTM layer (the RNN), followed by a densely connected
    layer (the MDN) that transforms the hidden state of the LSTM into the parameters
    of a mixture distribution. Let’s walk through the network step by step.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MDN-RNN由一个LSTM层（RNN）组成，后面是一个密集连接层（MDN），将LSTM的隐藏状态转换为混合分布的参数。让我们逐步走过网络。
- en: The input to the LSTM layer is a vector of length 36—a concatenation of the
    encoded `z` vector (length 32) from the VAE, the current action (length 3), and
    the previous reward (length 1).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层的输入是一个长度为36的向量，是从VAE的编码`z`向量（长度为32）、当前动作（长度为3）和先前奖励（长度为1）连接而成的。
- en: The output from the LSTM layer is a vector of length 256—one value for each
    LSTM cell in the layer. This is passed to the MDN, which is just a densely connected
    layer that transforms the vector of length 256 into a vector of length 481.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层的输出是一个长度为256的向量，每个LSTM单元在该层中有一个值。这被传递给MDN，MDN只是一个密集连接层，将长度为256的向量转换为长度为481的向量。
- en: Why 481? [Figure 12-12](#mixture_output) explains the composition of the output
    from the MDN-RNN. The aim of a mixture density network is to model the fact that
    our next `z` could be drawn from one of several possible distributions with a
    certain probability. In the car racing example, we choose five normal distributions.
    How many parameters do we need to define these distributions? For each of the
    5 mixtures, we need a `mu` and a `logvar` (to define the distribution) and a log-probability
    of this mixture being chosen (`logpi`), for each of the 32 dimensions of `z`.
    This makes 5 × 3 × 32 = 480 parameters. The one extra parameter is for the reward
    prediction.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是481？[图12-12](#mixture_output)解释了从MDN-RNN的输出组成。混合密度网络的目的是模拟我们的下一个`z`可能从几个可能的分布中以一定概率抽取的事实。在汽车赛车示例中，我们选择了五个正态分布。我们需要多少参数来定义这些分布？对于这5个混合物，我们需要一个`mu`和一个`logvar`（来定义分布）以及被选择的这个混合物的对数概率（`logpi`），对于`z`的每个32个维度。这使得5
    × 3 × 32 = 480个参数。额外的一个参数是用于奖励预测。
- en: '![The output from the mixture density network](Images/gdl2_1212.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: 从混合密度网络的输出
- en: Figure 12-12\. The output from the mixture density network
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-12。混合密度网络的输出
- en: Sampling from the MDN-RNN
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从MDN-RNN中抽样
- en: 'We can sample from the MDN output to generate a prediction for the next `z`
    and reward at the following timestep, through the following process:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从MDN输出中抽样，通过以下过程生成下一个`z`和下一个时间步的奖励的预测：
- en: Split the 481-dimensional output vector into the 3 variables (`logpi`, `mu`,
    `logvar`) and the reward value.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将481维输出向量分割为3个变量（`logpi`、`mu`、`logvar`）和奖励值。
- en: Exponentiate and scale `logpi` so that it can be interpreted as 32 probability
    distributions over the 5 mixture indices.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`logpi`进行指数化和缩放，以便将其解释为5个混合索引上的32个概率分布。
- en: For each of the 32 dimensions of `z`, sample from the distributions created
    from `logpi` (i.e., choose which of the 5 distributions should be used for each
    dimension of `z`).
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`z`的32个维度中的每一个，从由`logpi`创建的分布中抽样（即选择哪个分布应该用于`z`的每个维度）。
- en: Fetch the corresponding values of `mu` and `logvar` for this distribution.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取此分布的相应`mu`和`logvar`的值。
- en: Sample a value for each dimension of `z` from the normal distribution parameterized
    by the chosen parameters of `mu` and `logvar` for this dimension.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从由所选参数`mu`和`logvar`参数化的正态分布中为`z`的每个维度抽样一个值。
- en: The loss function for the MDN-RNN is the sum of the `z` vector reconstruction
    loss and the reward loss. The `z` vector reconstruction loss is the negative log-likelihood
    of the distribution predicted by the MDN-RNN, given the true value of `z`, and
    the reward loss is the mean squared error between the predicted reward and the
    true reward.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: MDN-RNN的损失函数是`z`向量重构损失和奖励损失的总和。`z`向量重构损失是MDN-RNN预测的分布的负对数似然，给定`z`的真实值，奖励损失是预测奖励和真实奖励之间的均方误差。
- en: Training the Controller
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练控制器
- en: The final step is to train the controller (the network that outputs the chosen
    action) using an evolutionary algorithm called the covariance matrix adaptation
    evolution strategy (CMA-ES).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用协方差矩阵适应进化策略（CMA-ES）来训练控制器（输出选择的动作的网络）。
- en: 'The hyperparameters of this step of the process are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 该步骤的超参数如下：
- en: '`controller_num_worker`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`controller_num_worker`'
- en: The number of workers that will test solutions in parallel
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将以并行方式测试解决方案的工作者数量
- en: '`controller_num_worker_trial`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`controller_num_worker_trial`'
- en: The number of solutions that each worker will be given to test at each generation
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作者在每一代将被给予测试的解决方案数量
- en: '`controller_num_episode`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`controller_num_episode`'
- en: The number of episodes that each solution will be tested against to calculate
    the average reward
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解决方案将被测试的情节数量，以计算平均奖励
- en: '`controller_eval_steps`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`controller_eval_steps`'
- en: The number of generations between evaluations of the current best parameter
    set
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 评估当前最佳参数集之间的代数数量
- en: The Controller Architecture
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制器架构
- en: The architecture of the controller is very simple. It is a densely connected
    neural network with no hidden layers. It connects the input vector directly to
    the action vector.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器的架构非常简单。它是一个没有隐藏层的密集连接神经网络。它将输入向量直接连接到动作向量。
- en: The input vector is a concatenation of the current `z` vector (length 32) and
    the current hidden state of the LSTM (length 256), giving a vector of length 288\.
    Since we are connecting each input unit directly to the 3 output action units,
    the total number of weights to tune is 288 × 3 = 864, plus 3 bias weights, giving
    867 in total.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量是当前`z`向量（长度32）和LSTM当前隐藏状态（长度256）的串联，得到长度为288的向量。由于我们将每个输入单元直接连接到3个输出动作单元，所以要调整的权重总数为288
    × 3 = 864，再加上3个偏置权重，总共为867。
- en: How should we train this network? Notice that this is not a supervised learning
    problem—we are not trying to *predict* the correct action. There is no training
    set of correct actions, as we do not know what the optimal action is for a given
    state of the environment. This is what distinguishes this as a reinforcement learning
    problem. We need the agent to discover the optimal values for the weights itself
    by experimenting within the environment and updating its weights based on received
    feedback.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何训练这个网络？请注意，这不是一个监督学习问题——我们不是在尝试*预测*正确的动作。没有正确动作的训练集，因为我们不知道对于环境的给定状态来说最佳动作是什么。这就是将这个问题区分为强化学习问题的原因。我们需要代理通过在环境中进行实验并根据接收到的反馈更新其权重来发现权重的最佳值。
- en: Evolutionary strategies are a popular choice for solving reinforcement learning
    problems, due to their simplicity, efficiency, and scalability. We shall use one
    particular strategy, known as CMA-ES.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 进化策略是解决强化学习问题的流行选择，因为它们简单、高效且可扩展。我们将使用一种特定的策略，称为CMA-ES。
- en: CMA-ES
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CMA-ES
- en: 'Evolutionary strategies generally adhere to the following process:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 进化策略通常遵循以下过程：
- en: Create a *population* of agents and randomly initialize the parameters to be
    optimized for each agent.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一组代理并随机初始化每个代理要优化的参数。
- en: 'Loop over the following:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环以下步骤：
- en: Evaluate each agent in the environment, returning the average reward over multiple
    episodes.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估环境中的每个代理，返回多个周期的平均奖励。
- en: Breed the agents with the best scores to create new members of the population.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 繁殖得分最高的代理，以创建种群的新成员。
- en: Add randomness to the parameters of the new members.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为新成员的参数添加随机性。
- en: Update the population pool by adding the newly created agents and removing poorly
    performing agents.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加新创建的代理和删除表现不佳的代理来更新种群池。
- en: This is similar to the process through which animals evolve in nature—hence
    the name *evolutionary* strategies. “Breeding” in this context simply means combining
    the existing best-scoring agents such that the next generation are more likely
    to produce high-quality results, similar to their parents. As with all reinforcement
    learning solutions, there is a balance to be found between greedily searching
    for locally optimal solutions and exploring unknown areas of the parameter space
    for potentially better solutions. This is why it is important to add randomness
    to the population, to ensure we are not too narrow in our search field.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于动物在自然界中进化的过程 - 因此称为*进化*策略。在这种情况下，“繁殖”简单地意味着结合现有的得分最高的代理，使得下一代更有可能产生高质量的结果，类似于它们的父母。与所有强化学习解决方案一样，需要在贪婪地寻找局部最优解和探索参数空间中未知区域以寻找潜在更好解决方案之间找到平衡。这就是为什么向种群中添加随机性很重要，以确保我们的搜索领域不会太狭窄。
- en: CMA-ES is just one form of evolutionary strategy. In short, it works by maintaining
    a normal distribution from which it can sample the parameters of new agents. At
    each generation, it updates the mean of the distribution to maximize the likelihood
    of sampling the high-scoring agents from the previous timestep. At the same time,
    it updates the covariance matrix of the distribution to maximize the likelihood
    of sampling the high-scoring agents, given the previous mean. It can be thought
    of as a form of naturally arising gradient descent, but with the added benefit
    that it is derivative-free, meaning that we do not need to calculate or estimate
    costly gradients.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES只是进化策略的一种形式。简而言之，它通过维护一个正态分布来采样新代理的参数。在每一代中，它更新分布的均值以最大化从上一个时间步采样高分代理的可能性。同时，它更新分布的协方差矩阵以最大化在给定先前均值的情况下采样高分代理的可能性。它可以被视为一种自然产生的梯度下降形式，但它的优势在于它是无导数的，这意味着我们不需要计算或估计昂贵的梯度。
- en: One generation of the algorithm demonstrated on a toy example is shown in [Figure 12-13](#cmaes_step).
    Here we are trying to find the minimum point of a highly nonlinear function in
    two dimensions—the value of the function in the red/black areas of the image is
    greater than the value of the function in the white/yellow parts of the image.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-13](#cmaes_step)中展示了算法在一个玩具示例上的一个代的演示。在这里，我们试图找到一个高度非线性函数在二维空间中的最小点 -
    图像中红/黑区域的函数值大于图像中白/黄区域的函数值。
- en: '![One generational update from the CMA-ES algorithm](Images/gdl2_1213.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![CMA-ES算法的一个代更新](Images/gdl2_1213.png)'
- en: 'Figure 12-13\. One update step from the CMA-ES algorithm (source: [Ha, 2017](http://bit.ly/2XufRwq))^([2](ch12.xhtml#idm45387001521648))'
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-13\. CMA-ES算法的一个更新步骤（来源：[Ha, 2017](http://bit.ly/2XufRwq))^([2](ch12.xhtml#idm45387001521648))
- en: 'The steps are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: We start with a randomly generated 2D normal distribution and sample a population
    of candidates, shown in blue in [Figure 12-13](#cmaes_step).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从随机生成的2D正态分布开始，并从中采样候选人种群，如[图12-13](#cmaes_step)中的蓝色所示。
- en: We then calculate the value of the function for each candidate and isolate the
    best 25%, shown in purple in [Figure 12-13](#cmaes_step)—we’ll call this set of
    points `P`.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算每个候选者的函数值，并将最佳25%孤立出来，如[图12-13](#cmaes_step)中的紫色所示 - 我们将这组点称为`P`。
- en: We set the mean of the new normal distribution to be the mean of the points
    in `P`. This can be thought of as the breeding stage, wherein we only use the
    best candidates to generate a new mean for the distribution. We also set the covariance
    matrix of the new normal distribution to be the covariance matrix of the points
    in `P`, but use the existing mean in the covariance calculation rather than the
    current mean of the points in `P`. The larger the difference between the existing
    mean and the mean of the points in `P`, the wider the variance of the next normal
    distribution. This has the effect of naturally creating *momentum* in the search
    for the optimal parameters.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将新正态分布的均值设置为`P`中点的均值。这可以被视为繁殖阶段，在这个阶段我们只使用最佳候选者来生成分布的新均值。我们还将新正态分布的协方差矩阵设置为`P`中点的协方差矩阵，但在协方差计算中使用现有的均值而不是`P`中点的当前均值。现有均值与`P`中点的均值之间的差异越大，下一个正态分布的方差就越大。这会自然地在寻找最佳参数的过程中产生*动量*效应。
- en: We can then sample a new population of candidates from our new normal distribution
    with an updated mean and covariance matrix.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以从具有更新均值和协方差矩阵的新正态分布中采样一个新的候选人种群。
- en: '[Figure 12-14](#cmaes) shows several generations of the process. See how the
    covariance widens as the mean moves in large steps toward the minimum, but narrows
    as the mean settles into the true minimum.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-14](#cmaes)展示了该过程的几代。请看均值向最小值大步移动时协方差如何扩大，但当均值稳定在真实最小值时，协方差如何变窄。'
- en: '![CMA-ES](Images/gdl2_1214.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![CMA-ES](Images/gdl2_1214.png)'
- en: 'Figure 12-14\. CMA-ES (source: [Wikipedia](https://oreil.ly/FObGZ))'
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-14\. CMA-ES（来源：[维基百科](https://oreil.ly/FObGZ))
- en: For the car racing task, we do not have a well-defined function to maximize,
    but instead an environment where the 867 parameters to be optimized determine
    how well the agent scores. Initially, some sets of parameters will, by random
    chance, generate scores that are higher than others and the algorithm will gradually
    move the normal distribution in the direction of those parameters that score highest
    in the environment.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于汽车赛车任务，我们没有一个明确定义的函数来最大化，而是一个环境，其中要优化的867个参数决定了代理的得分如何。最初，一些参数集将以随机方式生成比其他参数更高的得分，算法将逐渐将正态分布移向在环境中得分最高的那些参数的方向。
- en: Parallelizing CMA-ES
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化CMA-ES
- en: One of the great benefits of CMA-ES is that it can be easily parallelized. The
    most time-consuming part of the algorithm is calculating the score for a given
    set of parameters, since it needs to simulate an agent with these parameters in
    the environment. However, this process can be parallelized, since there are no
    dependencies between individual simulations. There is a orchestrator process that
    sends out parameter sets to be tested to many node processes in parallel. The
    nodes return the results to the orchestrator, which accumulates the results and
    then passes the overall result of the generation to the CMA-ES object. This object
    updates the mean and covariance matrix of the normal distribution as per [Figure 12-13](#cmaes_step)
    and provides the orchestrator with a new population to test. The loop then starts
    again. [Figure 12-15](#cmaes_loop) explains this in a diagram.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES的一个巨大优势是它可以很容易地并行化。算法中最耗时的部分是计算给定参数集的得分，因为它需要在环境中模拟具有这些参数的代理。然而，这个过程可以并行化，因为个别模拟之间没有依赖关系。有一个协调器进程，它将要测试的参数集并行发送给许多节点进程。节点将结果返回给协调器，协调器累积结果，然后将该代的整体结果传递给CMA-ES对象。该对象根据[图12-13](#cmaes_step)更新正态分布的均值和协方差矩阵，并为协调器提供一个新的人口进行测试。然后循环重新开始。[图12-15](#cmaes_loop)在图表中解释了这一点。
- en: '![Parallelising CMA-ES](Images/gdl2_1215.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![并行化CMA-ES](Images/gdl2_1215.png)'
- en: Figure 12-15\. Parallelizing CMA-ES—here there is a population size of eight
    and four nodes (so t = 2, the number of trials that each node is responsible for)
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-15\. 并行化CMA-ES——这里有一个人口规模为八个和四个节点（因此t = 2，每个节点负责的试验次数）
- en: '![1](Images/1.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![1](Images/1.png)'
- en: The orchestrator asks the CMA-ES object (`es`) for a set of parameters to trial.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器向CMA-ES对象(`es`)请求一组要试验的参数。
- en: '![2](Images/2.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![2](Images/2.png)'
- en: The orchestrator divides the parameters into the number of nodes available.
    Here, each of the four node processes gets two parameter sets to trial.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器将参数分成可用节点的数量。在这里，每个四个节点进程都会得到两组参数进行试验。
- en: '![3](Images/3.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![3](Images/3.png)'
- en: The nodes run a worker process that loops over each set of parameters and runs
    several episodes for each. Here we run three episodes for each set of parameters.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 节点运行一个工作进程，循环遍历每组参数，并为每组参数运行几集。在这里，我们为每组参数运行三集。
- en: '![4](Images/4.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![4](Images/4.png)'
- en: The rewards from each episode are averaged to give a single score for each set
    of parameters.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 每集剧集的奖励被平均以给出每组参数的单个得分。
- en: '![5](Images/5.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![5](Images/5.png)'
- en: Each node returns its list of scores to the orchestrator.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点将其得分列表返回给协调器。
- en: '![6](Images/6.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![6](Images/6.png)'
- en: The orchestrator groups all the scores together and sends this list to the `es`
    object.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器将所有得分组合在一起，并将此列表发送给`es`对象。
- en: '![7](Images/7.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![7](Images/7.png)'
- en: The `es` object uses this list of rewards to calculate the new normal distribution
    as per [Figure 12-13](#cmaes_step).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`es`对象使用这个奖励列表来计算新的正态分布，如[图12-13](#cmaes_step)所示。'
- en: After around 200 generations, the training process achieves an average reward
    score of around 840 for the car racing task, as shown in [Figure 12-16](#controller_training_outputs).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 大约经过200代，训练过程为汽车赛车任务实现了约840的平均奖励分数，如[图12-16](#controller_training_outputs)所示。
- en: '![Parallelising CMA-ES](Images/gdl2_1216.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![并行化CMA-ES](Images/gdl2_1216.png)'
- en: 'Figure 12-16\. Average episode reward of the controller training process, by
    generation (source: [Zac Wellmer, “World Models”](https://github.com/zacwellmer/WorldModels))'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-16\. 控制器训练过程的平均剧集奖励，按代数（来源：[Zac Wellmer，“World Models”](https://github.com/zacwellmer/WorldModels)）
- en: In-Dream Training
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梦中训练
- en: So far, the controller training has been conducted using the Gym `CarRacing`
    environment to implement the step method that moves the simulation from one state
    to the next. This function calculates the next state and reward, given the current
    state of the environment and chosen action.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，控制器训练是使用Gym的`CarRacing`环境来实现将模拟从一个状态移动到下一个状态的步骤方法。该函数根据环境的当前状态和选择的动作计算下一个状态和奖励。
- en: Notice how the step method performs a very similar function to the MDN-RNN in
    our model. Sampling from the MDN-RNN outputs a prediction for the next `z` and
    reward, given the current `z` and chosen action.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意步骤方法在我们模型中执行的功能与MDN-RNN非常相似。从MDN-RNN中采样输出了下一个`z`和奖励的预测，给出了当前`z`和选择的动作。
- en: In fact, the MDN-RNN can be thought of as an environment in its own right, but
    operating in `z`-space rather than in the original image space. Incredibly, this
    means that we can actually substitute the real environment with a copy of the
    MDN-RNN and train the controller entirely within an MDN-RNN-inspired *dream* of
    how the environment should behave.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，MDN-RNN可以被视为一个独立的环境，但是在`z`空间中运行，而不是在原始图像空间中。令人难以置信的是，这意味着我们实际上可以用MDN-RNN的副本替换真实环境，并在MDN-RNN启发的*梦境*中完全训练控制器，以模拟环境应该如何行为。
- en: In other words, the MDN-RNN has learned enough about the general physics of
    the real environment from the original random movement dataset that it can be
    used as a proxy for the real environment when training the controller. This is
    quite remarkable—it means that the agent can train itself to learn a new task
    by *thinking* about how it can maximize reward in its dream environment, without
    ever having to test out strategies in the real world. It can then perform well
    at the task the first time, having never attempted the task in reality.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，MDN-RNN已经从原始随机移动数据集中学到了关于真实环境的一般物理知识，因此可以在训练控制器时作为真实环境的代理使用。这是非常了不起的——这意味着代理可以通过*思考*如何在梦境环境中最大化奖励来训练自己学习新任务，而无需在真实世界中测试策略。然后，它可以在第一次尝试任务时表现良好，而实际上从未尝试过这项任务。
- en: 'A comparison of the architectures for training in the real environment and
    the dream environment follows: the real-world architecture is shown in [Figure 12-17](#real_world_training)
    and the in-dream training setup is illustrated in [Figure 12-18](#dream_training).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是在真实环境和梦境中进行训练的架构比较：真实世界架构显示在[图12-17](#real_world_training)中，梦境训练设置在[图12-18](#dream_training)中说明。
- en: '![](Images/gdl2_1217.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1217.png)'
- en: Figure 12-17\. Training the controller in the Gym environment
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-17。在Gym环境中训练控制器
- en: Notice how in the dream architecture, the training of the controller is performed
    entirely in `z`-space without the need to ever decode the `z` vectors back into
    recognizable track images. We can of course do so, in order to visually inspect
    the performance of the agent, but it is not required for training.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在梦境架构中，控制器的训练完全在`z`空间中进行，而无需将`z`向量解码回可识别的轨道图像。当然，我们可以这样做，以便视觉检查代理的性能，但这并不是训练所必需的。
- en: '![](Images/gdl2_1218.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1218.png)'
- en: Figure 12-18\. Training the controller in the MDN-RNN dream environment
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-18。在MDN-RNN梦境环境中训练控制器
- en: One of the challenges of training agents entirely within the MDN-RNN dream environment
    is overfitting. This occurs when the agent finds a strategy that is rewarding
    in the dream environment but does not generalize well to the real environment,
    due to the MDN-RNN not fully capturing how the true environment behaves under
    certain conditions.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDN-RNN梦境环境中完全训练代理的一个挑战是过拟合。当代理在梦境环境中找到一种有益的策略，但在真实环境中泛化能力不强时，就会发生这种情况，这是因为MDN-RNN没有完全捕捉到在某些条件下真实环境的行为方式。
- en: The authors of the original paper highlight this challenge and show how including
    a `temperature` parameter to control model uncertainty can help alleviate the
    problem. Increasing this parameter magnifies the variance when sampling `z` through
    the MDN-RNN, leading to more volatile rollouts when training in the dream environment.
    The controller receives higher rewards for safer strategies that encounter well-understood
    states and therefore tend to generalize better to the real environment. Increased
    temperature, however, needs to be balanced against not making the environment
    so volatile that the controller cannot learn any strategy, as there is not enough
    consistency in how the dream environment evolves over time.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文的作者强调了这一挑战，并展示了如何包含一个`温度`参数来控制模型的不确定性可以帮助缓解问题。增加这个参数会放大通过MDN-RNN对`z`进行采样时的方差，导致在梦境环境中训练时出现更多波动。控制器对于遇到已知状态的更安全策略会获得更高的奖励，因此往往更容易泛化到真实环境。然而，增加温度需要平衡，以免使环境变得太波动，以至于控制器无法学习任何策略，因为梦境环境在时间上的演变不够一致。
- en: 'In the original paper, the authors show this technique successfully applied
    to a different environment: `DoomTakeCover`, based around the computer game *Doom*.
    [Figure 12-19](#doom-temp) shows how changing the `temperature` parameter affects
    both the virtual (dream) score and the actual score in the real environment.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，作者展示了这种技术成功应用于不同的环境：`DoomTakeCover`，基于电脑游戏*Doom*。[图12-19](#doom-temp)显示了改变`温度`参数如何影响虚拟（梦境）得分和真实环境中的实际得分。
- en: '![](Images/gdl2_1219.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1219.png)'
- en: 'Figure 12-19\. Using temperature to control dream environment volatility (source:
    [Ha and Schmidhuber, 2018](https://arxiv.org/abs/1803.10122))'
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-19。使用温度控制梦境环境波动性（来源：[Ha and Schmidhuber, 2018](https://arxiv.org/abs/1803.10122)）
- en: The optimal temperature setting of 1.15 achieves a score of 1,092 in the real
    environment, surpassing the current Gym leader at the time of publication. This
    is an amazing achievement—remember, the controller has *never* attempted the task
    in the real environment. It has only ever taken random steps in the real environment
    (to train the VAE and MDN-RNN *dream* model) and then used the dream environment
    to train the controller.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实环境中，最佳温度设置为1.15，在发表时超过了当前Gym领导者的得分1,092。这是一个惊人的成就——请记住，控制器从未在真实环境中尝试过这项任务。它只是在真实环境中随机行动（用于训练VAE和MDN-RNN
    *梦*模型），然后使用梦境环境来训练控制器。
- en: A key benefit of using generative world models as an approach to reinforcement
    learning is that each generation of training in the dream environment is much
    faster than training in the real environment. This is because the `z` and reward
    prediction by the MDN-RNN is faster than the `z` and reward calculation by the
    Gym environment.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成世界模型作为强化学习方法的一个关键优势是，在梦境环境中的每一代训练比在真实环境中的训练要快得多。这是因为MDN-RNN对`z`和奖励预测比Gym环境中的`z`和奖励计算更快。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we have seen how a generative model (a VAE) can be utilized
    within a reinforcement learning setting to enable an agent to learn an effective
    strategy by testing policies within its own generated dreams, rather than within
    the real environment.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何在强化学习环境中利用生成模型（VAE）使代理能够通过在自己生成的梦境中测试策略来学习有效策略，而不是在真实环境中进行测试。
- en: The VAE is trained to learn a latent representation of the environment, which
    is then used as input to a recurrent neural network that forecasts future trajectories
    within the latent space. Amazingly, the agent can then use this generative model
    as a pseudo-environment to iteratively test policies, using an evolutionary methodology,
    that generalize well to the real environment.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: VAE被训练来学习环境的潜在表示，然后作为输入传递给一个递归神经网络，该网络在潜在空间内预测未来轨迹。令人惊讶的是，代理可以使用这个生成模型作为伪环境，通过演化方法迭代地测试策略，以便在真实环境中得到良好的泛化。
- en: For further information on the model, there is an excellent interactive explanation
    available [online](https://worldmodels.github.io), written by the authors of the
    original paper.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 有关该模型的更多信息，请参阅原始论文作者编写的出色互动解释，可在[在线](https://worldmodels.github.io)获取。
- en: ^([1](ch12.xhtml#idm45387001861120-marker)) David Ha and Jürgen Schmidhuber,
    “World Models,” March 27, 2018, [*https://arxiv.org/abs/1803.10122*](https://arxiv.org/abs/1803.10122).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.xhtml#idm45387001861120-marker)) 大卫·哈和尤尔根·施密德胡伯，“世界模型”，2018年3月27日，[*https://arxiv.org/abs/1803.10122*](https://arxiv.org/abs/1803.10122)。
- en: ^([2](ch12.xhtml#idm45387001521648-marker)) David Ha, “A Visual Guide to Evolution
    Strategies,” October 29, 2017, [*https://blog.otoro.net/2017/10/29/visual-evolution-strategies*](https://blog.otoro.net/2017/10/29/visual-evolution-strategies).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.xhtml#idm45387001521648-marker)) 大卫·哈，“演化策略的视觉指南”，2017年10月29日，[*https://blog.otoro.net/2017/10/29/visual-evolution-strategies*](https://blog.otoro.net/2017/10/29/visual-evolution-strategies)。
