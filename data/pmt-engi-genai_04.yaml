- en: Chapter 4\. Advanced Techniques for Text Generation with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 LangChain 文本生成高级技术
- en: 'Using simple prompt engineering techniques will often work for most tasks,
    but occasionally you’ll need to use a more powerful toolkit to solve complex generative
    AI problems. Such problems and tasks include:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的提示工程技巧通常适用于大多数任务，但偶尔您需要使用更强大的工具包来解决复杂的生成式AI问题。这些问题和任务包括：
- en: Context length
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文长度
- en: Summarizing an entire book into a digestible synopsis.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将整本书总结成可消化的摘要。
- en: Combining sequential LLM inputs/outputs
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 结合顺序LLM输入/输出
- en: Creating a story for a book including the characters, plot, and world building.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为一本书创造故事，包括角色、情节和世界构建。
- en: Performing complex reasoning tasks
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 执行复杂的推理任务
- en: LLMs acting as an agent. For example, you could create an LLM agent to help
    you achieve your personal fitness goals.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代理的LLMs。例如，您可以创建一个LLM代理来帮助您实现个人健身目标。
- en: To skillfully tackle such complex generative AI challenges, becoming acquainted
    with LangChain, an open source framework, is highly beneficial. This tool simplifies
    and enhances your LLM’s workflows substantially.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '为了巧妙地应对这样的复杂生成式AI挑战，熟悉开源框架LangChain非常有帮助。这个工具大大简化并增强了您LLM的工作流程。 '
- en: Introduction to LangChain
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain 简介
- en: 'LangChain is a versatile framework that enables the creation of applications
    utilizing LLMs and is available as both a [Python](https://oreil.ly/YPid-) and
    a [TypeScript](https://oreil.ly/5Vl0W) package. Its central tenet is that the
    most impactful and distinct applications won’t merely interface with a language
    model via an API, but will also:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是一个多功能的框架，它使创建利用LLMs的应用程序成为可能，并且作为[Python](https://oreil.ly/YPid-)和[TypeScript](https://oreil.ly/5Vl0W)包提供。其核心原则是，最有影响力和独特的应用程序不仅通过API与语言模型接口，而且还将：
- en: Enhance data awareness
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 增强数据意识
- en: The framework aims to establish a seamless connection between a language model
    and external data sources.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架旨在在语言模型和外部数据源之间建立无缝连接。
- en: Enhance agency
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 增强代理能力
- en: It strives to equip language models with the ability to engage with and influence
    their environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它致力于赋予语言模型与环境互动和影响的能力。
- en: The LangChain framework illustrated in [Figure 4-1](#figure-4-1) provides a
    range of modular abstractions that are essential for working with LLMs, along
    with a broad selection of implementations for these abstractions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图4-1](#figure-4-1)所示，LangChain框架提供了一系列模块化抽象，这对于与LLMs一起工作至关重要，以及这些抽象的广泛实现。
- en: '![pega 0401](assets/pega_0401.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0401](assets/pega_0401.png)'
- en: Figure 4-1\. The major modules of the LangChain LLM framework
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. LangChain LLM框架的主要模块
- en: 'Each module is designed to be user-friendly and can be efficiently utilized
    independently or together. There are currently six common modules within LangChain:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块都设计得易于使用，可以独立或一起高效地利用。目前LangChain中有六个常见的模块：
- en: Model I/O
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型I/O
- en: Handles input/output operations related to the model
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 处理与模型相关的输入/输出操作
- en: Retrieval
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 检索
- en: Focuses on retrieving relevant text for the LLM
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于检索对LLM相关的文本
- en: Chains
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 链
- en: Also known as *LangChain runnables*, chains enable the construction of sequences
    of LLM operations or function calls
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为*LangChain可运行程序*，链允许构建LLM操作或函数调用的序列
- en: Agents
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 代理
- en: Allows chains to make decisions on which tools to use based on high-level directives
    or instructions
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 允许链根据高级指令或指示决定使用哪些工具
- en: Memory
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内存
- en: Persists the state of an application between different runs of a chain
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在链的不同运行之间保持应用程序的状态
- en: Callbacks
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回调
- en: For running additional code on specific events, such as when every new token
    is generated
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在特定事件上运行附加代码，例如每当生成新令牌时
- en: Environment Setup
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'You can install LangChain on your terminal with either of these commands:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下任一命令在终端上安装LangChain：
- en: '`pip install langchain langchain-openai`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install langchain langchain-openai`'
- en: '`conda install -c conda-forge langchain langchain-openai`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conda install -c conda-forge langchain langchain-openai`'
- en: If you would prefer to install the package requirements for the entire book,
    you can use the [*requirements.txt*](https://oreil.ly/WKOma) file from the GitHub
    repository.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望安装整本书的包要求，可以使用GitHub仓库中的[*requirements.txt*](https://oreil.ly/WKOma)文件。
- en: 'It’s recommended to install the packages within a virtual environment:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在虚拟环境中安装这些包：
- en: Create a virtual environment
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创建虚拟环境
- en: '`python -m venv venv`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`python -m venv venv`'
- en: Activate the virtual environment
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 激活虚拟环境
- en: '`source venv/bin/activate`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`source venv/bin/activate`'
- en: Install the dependencies
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 安装依赖项
- en: '`pip install -r requirements.txt`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install -r requirements.txt`'
- en: LangChain requires integrations with one or more model providers. For example,
    to use OpenAI’s model APIs, you’ll need to install their Python package with `pip
    install openai`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain需要与一个或多个模型提供商进行集成。例如，要使用OpenAI的模型API，你需要使用`pip install openai`安装他们的Python包。
- en: 'As discussed in [Chapter 1](ch01.html#five_principles_01), it’s best practice
    to set an environment variable called `OPENAI_API_KEY` in your terminal or load
    it from an *.env* file using [`python-dotenv`](https://oreil.ly/wvuO7). However,
    for prototyping you can choose to skip this step by passing in your API key directly
    when loading a chat model in LangChain:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#five_principles_01)中所述，最佳实践是在你的终端中设置一个名为`OPENAI_API_KEY`的环境变量，或者使用`python-dotenv`从*.env*文件中加载它（[python-dotenv](https://oreil.ly/wvuO7)）。然而，对于原型设计，你可以选择跳过此步骤，通过在LangChain中加载聊天模型时直接传递你的API密钥：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Hardcoding API keys in scripts is not recommended due to security reasons. Instead,
    utilize environment variables or configuration files to manage your keys.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安全原因，不建议在脚本中硬编码API密钥。相反，利用环境变量或配置文件来管理你的密钥。
- en: In the constantly evolving landscape of LLMs, you can encounter the challenge
    of disparities across different model APIs. The lack of standardization in interfaces
    can induce extra layers of complexity in prompt engineering and obstruct the seamless
    integration of diverse models into your projects.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs不断演变的领域中，你可能会遇到不同模型API之间差异的挑战。接口缺乏标准化可能会在提示工程中引入额外的复杂性层，并阻碍不同模型无缝集成到你的项目中。
- en: This is where LangChain comes into play. As a comprehensive framework, LangChain
    allows you to easily consume the varying interfaces of different models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LangChain发挥作用的地方。作为一个综合框架，LangChain允许你轻松消费不同模型的各种接口。
- en: LangChain’s functionality ensures that you aren’t required to reinvent your
    prompts or code every time you switch between models. Its platform-agnostic approach
    promotes rapid experimentation with a broad range of models, such as [Anthropic](https://www.anthropic.com),
    [Vertex AI](https://cloud.google.com/vertex-ai), [OpenAI](https://openai.com),
    and [BedrockChat](https://oreil.ly/bedrock). This not only expedites the model
    evaluation process but also saves critical time and resources by simplifying complex
    model integrations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain的功能确保了你在切换模型时不需要重新发明你的提示或代码。其平台无关的方法促进了广泛模型（如[Anthropic](https://www.anthropic.com)、[Vertex
    AI](https://cloud.google.com/vertex-ai)、[OpenAI](https://openai.com)、[BedrockChat](https://oreil.ly/bedrock)）的快速实验。这不仅加快了模型评估过程，而且通过简化复杂模型集成，节省了关键的时间和资源。
- en: In the sections that follow, you’ll be using the OpenAI package and their API
    in LangChain.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将使用OpenAI包及其在LangChain中的API。
- en: Chat Models
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Chat Models
- en: Chat models such as GPT-4 have become the primary way to interface with OpenAI’s
    API. Instead of offering a straightforward “input text, output text” response,
    they propose an interaction method where *chat messages* are the input and output
    elements.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如GPT-4之类的聊天模型已成为与OpenAI API交互的主要方式。它们不是提供简单的“输入文本，输出文本”响应，而是提出一种交互方法，其中*聊天消息*是输入和输出元素。
- en: Generating LLM responses using chat models involves inputting one or more messages
    into the chat model. In the context of LangChain, the currently accepted message
    types are `AIMessage`, `HumanMessage`, and `SystemMessage`. The output from a
    chat model will always be an `AIMessage`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聊天模型生成LLM响应涉及将一个或多个消息输入到聊天模型中。在LangChain的上下文中，目前接受的消息类型是`AIMessage`、`HumanMessage`和`SystemMessage`。聊天模型的输出始终是`AIMessage`。
- en: SystemMessage
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SystemMessage
- en: Represents information that should be instructions to the AI system. These are
    used to guide the AI’s behavior or actions in some way.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代表应该是对AI系统的指令信息。这些用于以某种方式引导AI的行为或行动。
- en: HumanMessage
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: HumanMessage
- en: Represents information coming from a human interacting with the AI system. This
    could be a question, a command, or any other input from a human user that the
    AI needs to process and respond to.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 代表来自与AI系统交互的人类信息。这可能是一个问题、一个命令，或任何其他人类用户需要AI处理和响应的输入。
- en: AIMessage
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AIMessage
- en: Represents information coming from the AI system itself. This is typically the
    AI’s response to a `HumanMessage` or the result of a `SystemMessage` instruction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 代表来自AI系统本身的信息。这通常是AI对`HumanMessage`的响应或`SystemMessage`指令的结果。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to leverage the `SystemMessage` for delivering explicit directions.
    OpenAI has refined GPT-4 and upcoming LLM models to pay particular attention to
    the guidelines given within this type of message.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 确保利用`SystemMessage`来提供明确的指示。OpenAI已经改进了GPT-4和即将推出的LLM模型，特别注意这类消息中给出的指南。
- en: Let’s create a joke generator in LangChain.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在LangChain中创建一个笑话生成器。
- en: 'Input:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, you’ll import `ChatOpenAI`, `AIMessage`, `HumanMessage`, and `SystemMessage`.
    Then create an instance of the `ChatOpenAI` class with a temperature parameter
    of 0.5 (randomness).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入`ChatOpenAI`、`AIMessage`、`HumanMessage`和`SystemMessage`。然后创建一个具有0.5（随机性）温度参数的`ChatOpenAI`类实例。
- en: After creating a model, a list named `messages` is populated with a `SystemMessage`
    object, defining the role for the LLM, and a `HumanMessage` object, which asks
    for a software engineer—related joke.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后，一个名为`messages`的列表会被填充一个`SystemMessage`对象，定义了LLM的角色，以及一个`HumanMessage`对象，它要求一个与软件工程师相关的笑话。
- en: Calling the chat model with `.invoke(input=messages)` feeds the LLM with a list
    of messages, and then you retrieve the LLM’s response with `response.content`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`.invoke(input=messages)`调用聊天模型将消息列表喂给LLM，然后你通过`response.content`检索LLM的响应。
- en: 'There is a legacy method that allows you to directly call the `chat` object
    with `chat(messages=messages)`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个遗留方法允许你直接使用`chat`对象调用`chat(messages=messages)`：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Streaming Chat Models
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式聊天模型
- en: 'You might have observed while using ChatGPT how words are sequentially returned
    to you, one character at a time. This distinct pattern of response generation
    is referred to as *streaming*, and it plays a crucial role in enhancing the performance
    of chat-based applications:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用ChatGPT时，你可能已经注意到单词是逐个返回给你的，一个字符接一个字符。这种独特的响应生成模式被称为*流式传输*，它在增强基于聊天的应用程序性能中起着至关重要的作用：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When you call `chat.stream(messages)`, it yields chunks of the message one at
    a time. This means each segment of the chat message is individually returned.
    As each chunk arrives, it is then instantaneously printed to the terminal and
    flushed. This way, *streaming* allows for minimal latency from your LLM responses.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用`chat.stream(messages)`时，它会逐个产生消息块。这意味着聊天消息的每个部分都是单独返回的。随着每个块到达，它立即被打印到终端并刷新。这样，*流式传输*允许从LLM响应中减少最小延迟。
- en: Streaming holds several benefits from an end-user perspective. First, it dramatically
    reduces the waiting time for users. As soon as the text starts generating character
    by character, users can start interpreting the message. There’s no need for a
    full message to be constructed before it is seen. This, in turn, significantly
    enhances user interactivity and minimizes latency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从最终用户的角度来看，流式传输有几个好处。首先，它大大减少了用户的等待时间。一旦文本开始逐个字符生成，用户就可以开始解读消息。在看到消息之前，不需要构建完整的消息。这反过来又显著增强了用户交互并最小化了延迟。
- en: Nevertheless, this technique comes with its own set of challenges. One significant
    challenge is parsing the outputs while they are being streamed. Understanding
    and appropriately responding to the message as it is being formed can prove to
    be intricate, especially when the content is complex and detailed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种技术也带来了一系列挑战。一个显著的挑战是在流式传输过程中解析输出。在消息形成时理解和适当地回应消息可能很复杂，尤其是当内容复杂且详细时。
- en: Creating Multiple LLM Generations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建多个LLM生成
- en: There may be scenarios where you find it useful to generate multiple responses
    from LLMs. This is particularly true while creating dynamic content like social
    media posts. Rather than providing a list of messages, you provide a *list of
    message lists*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在一些场景，从LLM生成多个响应可能很有用。这在创建动态内容，如社交媒体帖子时尤其如此。你不需要提供消息列表，而是提供一个*消息列表的列表*。
- en: 'Input:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The benefit of using `.batch()` over `.invoke()` is that you can parallelize
    the number of API requests made to OpenAI.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`.batch()`而不是`.invoke()`的好处是你可以并行化对OpenAI发出的API请求的数量。
- en: 'For any runnable in LangChain, you can add a `RunnableConfig` argument to the
    `batch` function that contains many configurable parameters, including `max_``concurrency`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在LangChain中可运行的任何内容，你可以在`batch`函数中添加一个`RunnableConfig`参数，该参数包含许多可配置的参数，包括`max_concurrency`：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In computer science, *asynchronous (async) functions* are those that operate
    independently of other processes, thereby enabling several API requests to be
    run concurrently without waiting for each other. In LangChain, these async functions
    let you make many API requests all at once, not one after the other. This is especially
    helpful in more complex workflows and decreases the overall latency to your users.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，*异步（async）函数*是独立于其他进程运行的函数，因此可以在不等待彼此的情况下并发运行多个API请求。在LangChain中，这些异步函数允许你一次性发出多个API请求，而不是一个接一个。这在更复杂的流程中特别有帮助，并减少了用户整体延迟。
- en: Most of the asynchronous functions within LangChain are simply prefixed with
    the letter `a`, such as `.ainvoke()` and `.abatch()`. If you would like to use
    the async API for more efficient task performance, then utilize these functions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain中的大多数异步函数只是以字母`a`为前缀，例如`.ainvoke()`和`.abatch()`。如果你想使用异步API以更高效的任务性能，那么请使用这些函数。
- en: LangChain Prompt Templates
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain提示模板
- en: Up until this point, you’ve been hardcoding the strings in the `ChatOpenAI`
    objects. As your LLM applications grow in size, it becomes increasingly important
    to utilize *prompt templates*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你一直在硬编码`ChatOpenAI`对象中的字符串。随着你的LLM应用程序规模的扩大，利用*提示模板*变得越来越重要。
- en: '*Prompt templates* are good for generating reproducible prompts for AI language
    models. They consist of a *template*, a text string that can take in parameters,
    and construct a text prompt for a language model.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示模板*非常适合为AI语言模型生成可重复的提示。它由一个*模板*组成，这是一个可以接受参数的文本字符串，用于构建语言模型的文本提示。'
- en: 'Without prompt templates, you would likely use Python `f-string` formatting:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 没有提示模板，你可能会使用Python `f-string`格式化：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'But why not simply use an `f-string` for prompt templating? Using LangChain’s
    prompt templates instead allows you to easily:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么不简单地使用`f-string`进行提示模板化？使用LangChain的提示模板而不是这样做，可以让你轻松地：
- en: Validate your prompt inputs
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证你的提示输入
- en: Combine multiple prompts together with composition
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过组合将多个提示组合在一起
- en: Define custom selectors that will inject k-shot examples into your prompt
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义自定义选择器，将k-shot示例注入到你的提示中
- en: Save and load prompts from *.yml* and *.json* files
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从*.yml*和*.json*文件中保存和加载提示
- en: Create custom prompt templates that execute additional code or instructions
    when created
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建在创建时执行附加代码或指令的自定义提示模板
- en: LangChain Expression Language (LCEL)
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain表达式语言（LCEL）
- en: The `|` pipe operator is a key component of LangChain Expression Language (LCEL)
    that allows you to chain together different components or *runnables* in a data
    processing pipeline.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`|`管道运算符是LangChain表达式语言（LCEL）的关键组件，它允许你在数据处理管道中将不同的组件或*可运行*组件连接起来。'
- en: 'In LCEL, the `|` operator is similar to the Unix pipe operator. It takes the
    output of one component and feeds it as input to the next component in the chain.
    This allows you to easily connect and combine different components to create a
    complex chain of operations:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在LCEL中，`|`运算符类似于Unix管道运算符。它接受一个组件的输出，并将其作为输入传递给链中的下一个组件。这允许你轻松连接和组合不同的组件，以创建一个复杂的操作链：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `|` operator is used to chain together the prompt and model components.
    The output of the prompt component is passed as input to the model component.
    This chaining mechanism allows you to build complex chains from basic components
    and enables the seamless flow of data between different stages of the processing
    pipeline.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`|`运算符用于将提示和模型组件连接起来。提示组件的输出作为模型组件的输入。这种链式机制允许你从基本组件构建复杂的链，并使数据处理管道不同阶段之间的数据流无缝。'
- en: 'Additionally, *the order matters*, so you could technically create this chain:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*顺序很重要*，因此你可以技术上创建这个链：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: But it would produce an error after using the `invoke` function, because the
    values returned from `model` are not compatible with the expected inputs for the
    prompt.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但在使用`invoke`函数之后，它会产生错误，因为`model`返回的值与提示预期的输入不兼容。
- en: 'Let’s create a business name generator using prompt templates that will return
    five to seven relevant business names:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个使用提示模板的商业名称生成器，该生成器将返回五个到七个相关的商业名称：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First, you’ll import `ChatOpenAI`, `SystemMessagePromptTemplate`, and `ChatPromptTemplate`.
    Then, you’ll define a prompt template with specific guidelines under `template`,
    instructing the LLM to generate business names. `ChatOpenAI()` initializes the
    chat, while `SystemMessagePromptTemplate.from_template(template)` and `ChatPromptTemplate.from_messages([system_prompt])`
    create your prompt template.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将导入 `ChatOpenAI`、`SystemMessagePromptTemplate` 和 `ChatPromptTemplate`。然后，你将在
    `template` 下定义一个具有特定指南的提示模板，指示 LLM 生成商业名称。`ChatOpenAI()` 初始化聊天，而 `SystemMessagePromptTemplate.from_template(template)`
    和 `ChatPromptTemplate.from_messages([system_prompt])` 创建你的提示模板。
- en: You create an LCEL `chain` by piping together `chat_prompt` and the `model`,
    which is then *invoked*. This replaces the `{industries}`, `{context}`, and `{principles}`
    placeholders in the prompt with the dictionary values within the `invoke` function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过将 `chat_prompt` 和 `model` 连接在一起创建一个 LCEL `chain`，然后调用它。这将在提示中替换 `{industries}`、`{context}`
    和 `{principles}` 占位符，以字典 `invoke` 函数中的值。
- en: Finally, you extract the LLM’s response as a string accessing the `.content`
    property on the `result` variable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过访问 `result` 变量的 `.content` 属性，你可以提取 LLM 的响应作为字符串。
- en: Give Direction and Specify Format
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指明方向并指定格式
- en: Carefully crafted instructions might include things like “You are a creative
    consultant brainstorming names for businesses” and “Please generate a numerical
    list of five to seven catchy names for a start-up.” Cues like these guide your
    LLM to perform the exact task you require from it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 精心设计的指令可能包括“你是一位创意顾问，正在为公司起名”和“请生成五个到七个吸引人的初创公司名字的数字列表。”这样的提示将引导你的 LLM 执行你要求的精确任务。
- en: Using PromptTemplate with Chat Models
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Chat 模型与 PromptTemplate
- en: LangChain provides a more traditional template called `PromptTemplate`, which
    requires `input_variables` and `template` arguments.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一个更传统的模板 `PromptTemplate`，它需要 `input_variables` 和 `template` 参数。
- en: 'Input:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Output Parsers
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出解析器
- en: In [Chapter 3](ch03.html#standard_practices_03), you used regular expressions
    (regex) to extract structured data from text that contained numerical lists, but
    it’s possible to do this automatically in LangChain with *output parsers*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 3 章](ch03.html#standard_practices_03) 中，你使用了正则表达式 (regex) 从包含数字列表的文本中提取结构化数据，但在
    LangChain 中使用 *输出解析器* 可以自动完成此操作。
- en: '*Output parsers* are a higher-level abstraction provided by LangChain for parsing
    structured data from LLM string responses. Currently the available output parsers
    are:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出解析器* 是 LangChain 提供的用于从 LLM 字符串响应中解析结构化数据的高级抽象。目前可用的输出解析器包括：'
- en: List parser
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表解析器
- en: Returns a list of comma-separated items.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个以逗号分隔的项目列表。
- en: Datetime parser
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 日期时间解析器
- en: Parses an LLM output into datetime format.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 输出解析为日期时间格式。
- en: Enum parser
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举解析器
- en: Parses strings into enum values.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将字符串解析为枚举值。
- en: Auto-fixing parser
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自动修复解析器
- en: Wraps another output parser, and if that output parser fails, it will call another
    LLM to fix any errors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 包装另一个输出解析器，如果该输出解析器失败，它将调用另一个 LLM 来修复任何错误。
- en: Pydantic (JSON) parser
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Pydantic (JSON) 解析器
- en: Parses LLM responses into JSON output that conforms to a Pydantic schema.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 响应解析为符合 Pydantic 方案的 JSON 输出。
- en: Retry parser
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 重试解析器
- en: Provides retrying a failed parse from a previous output parser.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 提供从之前的输出解析器中重试失败解析的功能。
- en: Structured output parser
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化输出解析器
- en: Can be used when you want to return multiple fields.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想返回多个字段时可以使用。
- en: XML parser
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: XML 解析器
- en: Parses LLM responses into an XML-based format.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 响应解析为基于 XML 的格式。
- en: 'As you’ll discover, there are two important functions for LangChain output
    parsers:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，对于 LangChain 输出解析器有两个重要的函数：
- en: '`.get_format_instructions()`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`.get_format_instructions()`'
- en: This function provides the necessary instructions into your prompt to output
    a structured format that can be parsed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将必要的指令输入到提示中，以输出可解析的结构化格式。
- en: '`.parse(llm_output: str)`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`.parse(llm_output: str)`'
- en: This function is responsible for parsing your LLM responses into a predefined
    format.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数负责将你的 LLM 响应解析为预定义的格式。
- en: Generally, you’ll find that the Pydantic (JSON) parser with `ChatOpenAI()` provides
    the most flexibility.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会发现使用 `ChatOpenAI()` 的 Pydantic (JSON) 解析器提供了最大的灵活性。
- en: The Pydantic (JSON) parser takes advantage of the [Pydantic](https://oreil.ly/QIMih)
    library in Python. Pydantic is a data validation library that provides a way to
    validate incoming data using Python type annotations. This means that Pydantic
    allows you to create schemas for your data and automatically validates and parses
    input data according to those schemas.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Pydantic（JSON）解析器利用Python中的[Pydantic](https://oreil.ly/QIMih)库。Pydantic是一个数据验证库，它提供了一种使用Python类型注解验证传入数据的方法。这意味着Pydantic允许您为您的数据创建模式，并自动根据这些模式验证和解析输入数据。
- en: 'Input:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After you’ve loaded the necessary libraries, you’ll set up a ChatOpenAI model.
    Then create `SystemMessagePromptTemplate` from your template and form a `ChatPromptTemplate`
    with it. You’ll use the Pydantic models `BusinessName` and `BusinessNames` to
    structure your desired output, a list of unique business names. You’ll create
    a `Pydantic` parser for parsing these models and format the prompt using user-inputted
    variables by calling the `invoke` function. Feeding this customized prompt to
    your model, you’re enabling it to produce creative, unique business names by using
    the `parser`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载必要的库之后，您将设置一个ChatOpenAI模型。然后从您的模板创建`SystemMessagePromptTemplate`，并使用它形成一个`ChatPromptTemplate`。您将使用Pydantic模型`BusinessName`和`BusinessNames`来结构化您期望的输出，即独特的公司名称列表。您将为这些模型创建一个`Pydantic`解析器，并通过调用`invoke`函数使用用户输入的变量来格式化提示。将这个定制的提示输入到模型中，您就使它能够通过使用`parser`生成创造性的、独特的公司名称。
- en: 'It’s possible to use output parsers inside of LCEL by using this syntax:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在LCEL中使用以下语法使用输出解析器：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s add the output parser directly to the chain.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将输出解析器直接添加到链中。
- en: 'Input:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The chain is now responsible for prompt formatting, LLM calling, and parsing
    the LLM’s response into a `Pydantic` object.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在链负责提示格式化、LLM调用以及将LLM的响应解析为`Pydantic`对象。
- en: Specify Format
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定格式
- en: The preceding prompts use Pydantic models and output parsers, allowing you explicitly
    tell an LLM your desired response format.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的提示使用了Pydantic模型和输出解析器，允许您明确告诉LLM您期望的响应格式。
- en: It’s worth knowing that by asking an LLM to provide structured JSON output,
    you can create a flexible and generalizable API from the LLM’s response. There
    are limitations to this, such as the size of the JSON created and the reliability
    of your prompts, but it still is a promising area for LLM applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过要求LLM提供结构化的JSON输出，您可以从LLM的响应中创建一个灵活且可通用的API。尽管存在一些限制，例如创建的JSON大小和您提示的可靠性，但这仍然是一个LLM应用的有希望领域。
- en: Warning
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You should take care of edge cases as well as adding error handling statements,
    since LLM outputs might not always be in your desired format.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该注意边缘情况，并添加错误处理语句，因为LLM的输出可能不会总是符合您的期望格式。
- en: Output parsers save you from the complexity and intricacy of regular expressions,
    providing easy-to-use functionalities for a variety of use cases. Now that you’ve
    seen them in action, you can utilize output parsers to effortlessly structure
    and retrieve the data you need from an LLM’s output, harnessing the full potential
    of AI for your tasks.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出解析器让您免于正则表达式的复杂性和复杂性，为各种用例提供易于使用的功能。现在您已经看到了它们在实际中的应用，您可以使用输出解析器轻松地构建和检索LLM输出的数据，充分利用AI为您的工作任务提供全部潜力。
- en: Furthermore, using parsers to structure the data extracted from LLMs allows
    you to easily choose how to organize outputs for more efficient use. This can
    be useful if you’re dealing with extensive lists and need to sort them by certain
    criteria, like business names.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用解析器来结构化从LLM中提取的数据，您可以轻松选择如何组织输出，以便更有效地使用。如果您处理的是大量列表并且需要按某些标准（如公司名称）进行排序，这将非常有用。
- en: LangChain Evals
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain Evals
- en: As well as output parsers to check for formatting errors, most AI systems also
    make use of *evals*, or evaluation metrics, to measure the performance of each
    prompt response. LangChain has a number of off-the-shelf evaluators, which can
    be directly be logged in their [LangSmith](https://oreil.ly/0Fn94) platform for
    further debugging, monitoring, and testing. [Weights and Biases](https://wandb.ai/site)
    is alternative machine learning platform that offers similar functionality and
    tracing capabilities for LLMs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于检查格式错误的输出解析器外，大多数AI系统还使用*evals*，或评估指标，来衡量每个提示响应的性能。LangChain提供了一系列现成的评估器，可以直接在它们的[LangSmith](https://oreil.ly/0Fn94)平台上进行记录，以便进行进一步的调试、监控和测试。[Weights
    and Biases](https://wandb.ai/site)是另一个提供类似功能和对LLM进行跟踪的机器学习平台。
- en: Evaluation metrics are useful for more than just prompt testing, as they can
    be used to identify positive and negative examples for retrieval as well as to
    build datasets for fine-tuning custom models.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标不仅对提示测试有用，还可以用于识别用于检索的正负示例，以及构建用于微调自定义模型的训练数据集。
- en: Most eval metrics rely on a set of test cases, which are input and output pairings
    where you know the correct answer. Often these reference answers are created or
    curated manually by a human, but it’s also common practice to use a smarter model
    like GPT-4 to generate the ground truth answers, which has been done for the following
    example. Given a list of descriptions of financial transactions, we used GPT-4
    to classify each transaction with a `transaction_category` and `transaction_type`.
    The process can be found in the `langchain-evals.ipynb` Jupyter Notebook in the
    [GitHub repository](https://oreil.ly/a4Hut) for the book.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数评估指标都依赖于一组测试用例，这些测试用例是已知正确答案的输入和输出配对。通常，这些参考答案是由人工创建或整理的，但使用更智能的模型（如 GPT-4）生成真实答案的做法也很常见，以下示例就是这样做的。给定一系列金融交易的描述，我们使用
    GPT-4 对每个交易进行分类，并赋予 `transaction_category` 和 `transaction_type`。该过程可以在书籍的 [GitHub
    仓库](https://oreil.ly/a4Hut) 中的 `langchain-evals.ipynb` Jupyter Notebook 中找到。
- en: With the GPT-4 answer being taken as the correct answer, it’s now possible to
    rate the accuracy of smaller models like GPT-3.5-turbo and Mixtral 8x7b (called
    `mistral-small` in the API). If you can achieve good enough accuracy with a smaller
    model, you can save money or decrease latency. In addition, if that model is available
    open source like [Mistral’s model](https://oreil.ly/Ec578), you can migrate that
    task to run on your own servers, avoiding sending potentially sensitive data outside
    of your organization. We recommend testing with an external API first, before
    going to the trouble of self-hosting an OS model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以 GPT-4 的答案作为正确答案，现在可以评估 GPT-3.5-turbo 和 Mixtral 8x7b（在 API 中称为 `mistral-small`）等较小模型的准确性。如果您能够使用较小模型达到足够的准确性，您可以节省资金或减少延迟。此外，如果该模型是开源的，如
    [Mistral 的模型](https://oreil.ly/Ec578)，您可以将该任务迁移到您自己的服务器上运行，从而避免将可能敏感的数据发送到组织外部。我们建议在尝试自托管
    OS 模型之前，先使用外部 API 进行测试。
- en: '[Remember to sign up](https://mistral.ai) and subscribe to obtain an API key;
    then expose that as an environment variable by typing in your terminal:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[请记住注册](https://mistral.ai) 并订阅以获取 API 密钥；然后通过在终端中输入以下命令将其作为环境变量公开：'
- en: '`**export MISTRAL_API_KEY=api-key**`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**export MISTRAL_API_KEY=api-key**`'
- en: The following script is part of a [notebook](https://oreil.ly/DqDOf) that has
    previously defined a dataframe `df`. For brevity let’s investigate only the evaluation
    section of the script, assuming a dataframe is already defined.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本是笔记本的一部分，该笔记本之前已定义了一个名为 `df` 的数据框。为了简洁起见，让我们只调查脚本的评估部分，假设已经定义了数据框。
- en: 'Input:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The code does the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 代码执行以下操作：
- en: '`from langchain_mistralai.chat_models import ChatMistralAI`: We import LangChain’s
    Mistral implementation.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`from langchain_mistralai.chat_models import ChatMistralAI`：我们导入 LangChain
    的 Mistral 实现。'
- en: '`from langchain.output_parsers import PydanticOutputParser`: Imports the `PydanticOutputParser`
    class, which is used for parsing output using Pydantic models. We also import
    a string output parser to handle an interim step where we remove backslashes from
    the JSON key (a common problem with responses from Mistral).'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`from langchain.output_parsers import PydanticOutputParser`：导入 `PydanticOutputParser`
    类，用于使用 Pydantic 模型解析输出。我们还导入了一个字符串输出解析器，用于处理从 Mistral 获取的响应中移除反斜杠的中间步骤（这是 Mistral
    响应中常见的问题）。'
- en: '`mistral_api_key = os.environ["MISTRAL_API_KEY"]`: Retrieves the Mistral API
    key from the environment variables. This needs to be set prior to running the
    notebook.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mistral_api_key = os.environ["MISTRAL_API_KEY"]`: 从环境变量中检索 Mistral API 密钥。在运行笔记本之前需要设置此变量。'
- en: '`model = ChatMistralAI(model="mistral-small", mistral_api_key=mistral_api_key)`:
    Initializes an instance of `ChatMistralAI` with the specified model and API key.
    Mistral Small is what they call the Mixtral 8x7b model (also available open source)
    in their API.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model = ChatMistralAI(model="mistral-small", mistral_api_key=mistral_api_key)`:
    使用指定的模型和 API 密钥初始化 `ChatMistralAI` 实例。Mistral Small 是他们在 API 中对 Mixtral 8x7b 模型的称呼（也是开源的）。'
- en: '`system_prompt` and `user_prompt`: These lines define templates for the system
    and user prompts used in the chat to classify the transactions.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`system_prompt` 和 `user_prompt`：这些行定义了在聊天中用于分类交易的系统和用户提示模板。'
- en: '`class EnrichedTransactionInformation(BaseModel)`: Defines a Pydantic model
    `EnrichedTransactionInformation` with two fields: `transaction_type` and `transaction_category`,
    each with specific allowed values and the possibility of being `None`. This is
    what tells us if the output is in the correct format.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`class EnrichedTransactionInformation(BaseModel)`: 定义一个 Pydantic 模型 `EnrichedTransactionInformation`，包含两个字段：`transaction_type`
    和 `transaction_category`，每个字段都有特定的允许值，并且可能为 `None`。这正是告诉我们输出是否处于正确格式的依据。'
- en: '`def remove_back_slashes(string)`: Defines a function to remove backslashes
    from a string.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`def remove_back_slashes(string)`: 定义一个函数，用于从字符串中删除反斜杠。'
- en: '`chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser`:
    Updates the chain to include a string output parser and the `remove_back_slashes`
    function before the original output parser.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser`:
    更新链，包括一个字符串输出解析器和 `remove_back_slashes` 函数，在原始输出解析器之前。'
- en: '`transaction = df.iloc[0]["Transaction Description"]`: Extracts the first transaction
    description from a dataframe `df`. This dataframe is loaded earlier in the [Jupyter
    Notebook](https://oreil.ly/-koAO) (omitted for brevity).'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transaction = df.iloc[0]["Transaction Description"]`: 从数据框 `df` 中提取第一条交易描述。这个数据框在之前的
    [Jupyter Notebook](https://oreil.ly/-koAO) 中已加载（为了简洁起见省略）。'
- en: '`for i, row in tqdm(df.iterrows(), total=len(df))`: Iterates over each row
    in the dataframe `df`, with a progress bar.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`for i, row in tqdm(df.iterrows(), total=len(df))`: 遍历数据框 `df` 中的每一行，并显示进度条。'
- en: '`result = chain.invoke(...)`: Inside the loop, the chain is invoked for each
    transaction.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`result = chain.invoke(...)`: 在循环内部，为每个交易调用链。'
- en: '`except`: In case of an exception, a default `EnrichedTransactionInformation`
    object with `None` values is created. These will be treated as errors in evaluation
    but will not break the processing loop.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`except`: 如果发生异常，将创建一个默认的 `EnrichedTransactionInformation` 对象，其值为 `None`。这些将在评估中被视为错误，但不会中断处理循环。'
- en: '`df["mistral_transaction_type"] = transaction_types`, `df["mistral_transaction_category"]
    = transaction_categories`: Adds the transaction types and categories as new columns
    in the dataframe, which we then display with `df.head()`.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df["mistral_transaction_type"] = transaction_types`, `df["mistral_transaction_category"]
    = transaction_categories`: 在数据框中添加交易类型和类别作为新列，然后我们使用 `df.head()` 显示它们。'
- en: With the responses from Mistral saved in the dataframe, it’s possible to compare
    them to the transaction categories and types defined earlier to check the accuracy
    of Mistral. The most basic LangChain eval metric is to do an exact string match
    of a prediction against a reference answer, which returns a score of 1 if correct,
    and a 0 if incorrect. The notebook gives an example of how to [implement this](https://oreil.ly/vPUfI),
    which shows that Mistral’s accuracy is 77.5%. However, if all you are doing is
    comparing strings, you probably don’t need to implement it in LangChain.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Mistral 的响应保存在数据框中后，可以将其与之前定义的交易类别和类型进行比较，以检查 Mistral 的准确性。最基础的 LangChain
    评估指标是对预测结果与参考答案进行精确字符串匹配，如果匹配正确则返回分数 1，如果错误则返回 0。笔记本提供了一个如何[实现此功能](https://oreil.ly/vPUfI)的示例，这表明
    Mistral 的准确率为 77.5%。然而，如果你只是比较字符串，你可能不需要在 LangChain 中实现它。
- en: 'Where LangChain is valuable is in its standardized and tested approaches to
    implementing more advanced evaluators using LLMs. The evaluator `labeled_pairwise_string`
    compares two outputs and gives a reason for choosing between them, using GPT-4\.
    One common use case for this type of evaluator is to compare the outputs from
    two different prompts or models, particularly if the models being tested are less
    sophisticated than GPT-4\. This evaluator using GPT-4 does still work for evaluating
    GPT-4 responses, but you should manually review the reasoning and scores to ensure
    it is doing a good job: if GPT-4 is bad at a task, it may also be bad at evaluating
    that task. In [the notebook](https://oreil.ly/9O7Mb), the same transaction classification
    was run again with the model changed to `model = ChatOpenAI(model="gpt-3.5-turbo-1106",
    model_kwargs={"response_format": {"type": "json_object"}},)`. Now it’s possible
    to do pairwise comparison between the Mistral and GPT-3.5 responses, as shown
    in the following example. You can see in the output the reasoning that is given
    to justify the score.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'LangChain的价值在于其标准化和经过测试的方法，用于使用LLM实现更高级的评估器。`labeled_pairwise_string`评估器比较两个输出，并给出选择它们的原因，使用GPT-4。此类评估器的常见用例是比较来自两个不同提示或模型的输出，尤其是如果被测试的模型不如GPT-4复杂。使用GPT-4的此评估器仍然可以用于评估GPT-4的响应，但您应该手动审查推理和评分以确保其表现良好：如果GPT-4在某个任务上表现不佳，它也可能在评估该任务上表现不佳。在[notebook](https://oreil.ly/9O7Mb)中，相同的交易分类再次运行，但模型已更改为`model
    = ChatOpenAI(model="gpt-3.5-turbo-1106", model_kwargs={"response_format": {"type":
    "json_object"}})`。现在可以进行Mistral和GPT-3.5响应的成对比较，如下例所示。您可以在输出中看到用于证明评分的推理。'
- en: 'Input:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This code demonstrates the simple exact string matching evaluator from LangChain:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码演示了LangChain中的简单精确字符串匹配评估器：
- en: '`evaluator = load_evaluator("labeled_pairwise_string")`: This is a helper function
    that can be used to load any LangChain evaluator by name. In this case, it is
    the `labeled_pairwise_string` evaluator being used.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`evaluator = load_evaluator("labeled_pairwise_string")`: 这是一个辅助函数，可以用来通过名称加载任何LangChain评估器。在这种情况下，使用的是`labeled_pairwise_string`评估器。'
- en: '`row = df.iloc[0]`: This line and the seven lines that follow get the first
    row and extract the values for the different columns needed. It includes the transaction
    description, as well as the Mistral and GPT-3.5 transaction category and types.
    This is showcasing a single transaction, but this code can easily run in a loop
    through each transaction, replacing this line with an `iterrows` function `for
    i, row in tqdm(df.iterrows(), total=len(df)):`, as is done later in [the notebook](https://oreil.ly/dcCOO).'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`row = df.iloc[0]`: 这一行以及随后的七行代码获取第一行并提取所需的不同列的值。这包括交易描述，以及Mistral和GPT-3.5的交易类别和类型。这展示了单个交易，但这段代码可以很容易地通过循环遍历每个交易运行，将此行替换为`iterrows`函数`for
    i, row in tqdm(df.iterrows(), total=len(df)):`，就像在[notebook](https://oreil.ly/dcCOO)中做的那样。'
- en: '`gpt3pt5_data = f"""{{`: To use the pairwise comparison evaluator, we need
    to pass the results in a way that is formatted correctly for the prompt. This
    is done for Mistral and GPT-3.5, as well as the reference data.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gpt3pt5_data = f"""{{`: 要使用成对比较评估器，我们需要以正确格式化的方式传递结果，以便用于提示。这适用于Mistral和GPT-3.5，以及参考数据。'
- en: '`input_prompt = """You are an expert...`: The other formatting we have to get
    right is in the prompt. To get accurate evaluation scores, the evaluator needs
    to see the instructions that were given for the task.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`input_prompt = """You are an expert...`: 我们还需要正确格式化提示。为了获得准确的评估分数，评估器需要看到为任务提供的说明。'
- en: '`evaluator.evaluate_string_pairs(...`: All that remains is to run the evaluator
    by passing in the `prediction` and `prediction_b` (GPT-3.5 and Mistral, respectively),
    as well as the `input` prompt, and `reference` data, which serves as the ground
    truth.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`evaluator.evaluate_string_pairs(...`: 剩下的就是通过传递`prediction`和`prediction_b`（分别代表GPT-3.5和Mistral）以及`input`提示和`reference`数据（作为真实数据）来运行评估器。'
- en: Following this code [in the notebook](https://oreil.ly/hW8Wr), there is an example
    of looping through and running the evaluator on every row in the dataframe and
    then saving the results and reasoning back to the dataframe.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[notebook](https://oreil.ly/hW8Wr)中的此代码之后，有一个示例，展示了如何遍历dataframe中的每一行并运行评估器，然后将结果和推理保存回dataframe。
- en: This example demonstrates how to use a LangChain evaluator, but there are many
    different kinds of evaluator available. String distance ([Levenshtein](https://oreil.ly/Al5G3))
    or [embedding distance](https://oreil.ly/0p_nE) evaluators are often used in scenarios
    where answers are not an exact match for the reference answer, but only need to
    be close enough semantically. Levenshtein distance allows for fuzzy matches based
    on how many single-character edits would be needed to transform the predicted
    text into the reference text, and embedding distance makes use of vectors (covered
    in [Chapter 5](ch05.html#vector_databases_05)) to calculate similarity between
    the answer and reference.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子演示了如何使用 LangChain 评估器，但有许多不同类型的评估器可用。字符串距离 ([Levenshtein](https://oreil.ly/Al5G3))
    或 [嵌入距离](https://oreil.ly/0p_nE) 评估器通常用于答案不是与参考答案完全匹配，但只需要在语义上足够接近的场景。Levenshtein
    距离允许基于需要多少单个字符编辑来将预测文本转换为参考文本进行模糊匹配，而嵌入距离则使用向量（在第 5 章中介绍）来计算答案与参考之间的相似度。
- en: The other kind of evaluator we often use in our work is pairwise comparisons,
    which are useful for comparing two different prompts or models, using a smarter
    model like GPT-4\. This type of comparison is helpful because reasoning is provided
    for each comparison, which can be useful in debugging why one approach was favored
    over another. The [notebook for this section](https://oreil.ly/iahTJ) shows an
    example of using a pairwise comparison evaluator to check GPT-3.5-turbo’s accuracy
    versus Mixtral 8x7b.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在工作中经常使用的另一种评估器是成对比较，这对于比较两个不同的提示或模型非常有用，可以使用像 GPT-4 这样的智能模型。这种比较类型很有帮助，因为每个比较都提供了推理，这有助于调试为什么一个方法被优先考虑而不是另一个。本节[笔记本](https://oreil.ly/iahTJ)展示了使用成对比较评估器检查
    GPT-3.5-turbo 的准确度与 Mixtral 8x7b 的示例。
- en: Evaluate Quality
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估质量
- en: Without defining an appropriate set of eval metrics to define success, it can
    be difficult to tell if changes to the prompt or wider system are improving or
    harming the quality of responses. If you can automate eval metrics using smart
    models like GPT-4, you can iterate faster to improve results without costly or
    time-consuming manual human review.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有定义适当的评估指标集来定义成功，就很难判断提示或更广泛系统的更改是否在提高或损害响应的质量。如果你可以使用像 GPT-4 这样的智能模型自动化评估指标，你就可以更快地迭代以改进结果，而无需昂贵的或耗时的手动人工审查。
- en: OpenAI Function Calling
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI 函数调用
- en: '*Function calling* provides an alternative method to output parsers, leveraging
    fine-tuned OpenAI models. These models identify when a function should be executed
    and generate a JSON response with the *name and arguments* for a predefined function.
    Several use cases include:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*函数调用* 提供了一种输出解析器的替代方法，利用微调后的 OpenAI 模型。这些模型识别何时应该执行函数，并为预定义的函数生成包含 *名称和参数*
    的 JSON 响应。几个用例包括：'
- en: Designing sophisticated chat bots
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 设计复杂的聊天机器人
- en: 'Capable of organizing and managing schedules. For example, you can define a
    function to schedule a meeting: `schedule_meeting(date: str, time: str, attendees:
    List[str])`.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '能够组织和管理工作安排。例如，你可以定义一个安排会议的函数：`schedule_meeting(date: str, time: str, attendees:
    List[str])`。'
- en: Convert natural language into actionable API calls
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将自然语言转换为可操作的 API 调用
- en: 'A command like “Turn on the hallway lights” can be converted to `control_device(device:
    str, action: ''on'' | ''off'')` for interacting with your home automation API.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '像这样的命令“打开走廊的灯”可以转换为 `control_device(device: str, action: ''on'' | ''off'')`
    以与你的家庭自动化 API 进行交互。'
- en: Extracting structured data
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 提取结构化数据
- en: 'This could be done by defining a function such as `extract_contextual_data(context:
    str, data_points: List[str])` or `search_database(query: str)`.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '这可以通过定义一个函数如 `extract_contextual_data(context: str, data_points: List[str])`
    或 `search_database(query: str)` 来实现。'
- en: 'Each function that you use within function calling will require an appropriate
    *JSON schema*. Let’s explore an example with the `OpenAI` package:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数调用中使用的每个函数都将需要一个适当的 *JSON 架构*。让我们通过 `OpenAI` 包的例子来探索一下：
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After importing `OpenAI` and `json`, you’ll create a function named `schedule_meeting`.
    This function is a mock-up, simulating the process of scheduling a meeting, and
    returns details such as `event_id`, `date`, `time`, and `attendees`. Following
    that, make an `OPENAI_FUNCTIONS` dictionary to map the function name to the actual
    function for ease of reference.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入 `OpenAI` 和 `json` 之后，你会创建一个名为 `schedule_meeting` 的函数。这个函数是一个原型，模拟安排会议的过程，并返回诸如
    `event_id`、`date`、`time` 和 `attendees` 等详细信息。随后，创建一个 `OPENAI_FUNCTIONS` 字典，将函数名映射到实际函数，以便于参考。
- en: 'Next, define a `functions` list that provides the function’s JSON schema. This
    schema includes its name, a brief description, and the parameters it requires,
    guiding the LLM on how to interact with it:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个 `functions` 列表，提供函数的 JSON 架构。此架构包括其名称、简短描述以及它所需的参数，指导 LLM 如何与之交互：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Specify Format
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定格式
- en: When using function calling with your OpenAI models, always ensure to define
    a detailed JSON schema (including the name and description). This acts as a blueprint
    for the function, guiding the model to understand when and how to properly invoke
    it.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 OpenAI 模型进行函数调用时，始终确保定义一个详细的 JSON 架构（包括名称和描述）。这作为函数的蓝图，指导模型理解何时以及如何正确调用它。
- en: After defining the functions, let’s make an OpenAI API request. Set up a `messages`
    list with the user query. Then, using an OpenAI `client` object, you’ll send this
    message and the function schema to the model. The LLM analyzes the conversation,
    discerns a need to trigger a function, and provides the function name and arguments.
    The `function` and `function_args` are parsed from the LLM response. Then the
    function is executed, and its results are added back into the conversation. Then
    you call the model again for a user-friendly summary of the entire process.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数后，让我们发起一个 OpenAI API 请求。设置一个包含用户查询的 `messages` 列表。然后，使用 OpenAI `client`
    对象发送此消息和函数架构到模型。LLM 分析对话，识别出需要触发函数的需求，并提供函数名称和参数。`function` 和 `function_args`
    从 LLM 响应中解析出来。然后执行函数，并将结果添加回对话中。然后再次调用模型以获取整个过程的用户友好总结。
- en: 'Input:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE27]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Several important points to note while function calling:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 函数调用时需要注意的几个重要点：
- en: It’s possible to have many functions that the LLM can call.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 可以调用许多函数。
- en: OpenAI can hallucinate function parameters, so be more explicit within the `system`
    message to overcome this.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 可以推测函数参数，因此在 `system` 消息中要更加明确，以克服这一点。
- en: 'The `function_call` parameter can be set in various ways:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_call` 参数可以以多种方式设置：'
- en: 'To mandate a specific function call: `tool_choice: {"type: "function", "function":
    {"name": "my_function"}}}`.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '强制调用特定函数：`tool_choice: {"type: "function", "function": {"name": "my_function"}}}`。'
- en: 'For a user message without function invocation: `tool_choice: "none"`.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于没有函数调用的用户消息：`tool_choice: "none"`。'
- en: 'By default (`tool_choice: "auto"`), the model autonomously decides if and which
    function to call.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '默认情况下（`tool_choice: "auto"`），模型会自主决定是否以及调用哪个函数。'
- en: Parallel Function Calling
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行函数调用
- en: You can set your chat messages to include intents that request simultaneous
    calls to multiple tools. This strategy is known as *parallel function calling*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将你的聊天消息设置为包含请求同时调用多个工具的意图。这种策略被称为 *并行函数调用*。
- en: 'Modifying the previously used code, the `messages` list is updated to mandate
    the scheduling of two meetings:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 修改之前使用的代码，`messages` 列表更新为强制安排两个会议：
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Then, adjust the previous code section by incorporating a `for` loop.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过添加一个 `for` 循环来调整之前的代码部分。
- en: 'Input:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE29]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Output:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE30]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: From this example, it’s clear how you can effectively manage multiple function
    calls. You’ve seen how the `schedule_meeting` function was called twice in a row
    to arrange different meetings. This demonstrates how flexibly and effortlessly
    you can handle varied and complex requests using AI-powered tools.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，可以看出如何有效地管理多个函数调用。你看到了 `schedule_meeting` 函数连续两次被调用以安排不同的会议。这展示了如何灵活且轻松地使用
    AI 工具处理各种复杂请求。
- en: Function Calling in LangChain
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain 中的函数调用
- en: If you’d prefer to avoid writing JSON schema and simply want to extract structured
    data from an LLM response, then LangChain allows you to use function calling with
    Pydantic.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望避免编写 JSON 架构，而只想从 LLM 响应中提取结构化数据，那么 LangChain 允许你使用 Pydantic 进行函数调用。
- en: 'Input:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE32]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You’ll start by importing various modules, including `PydanticToolsParser` and
    `ChatPromptTemplate`, essential for parsing and templating your prompts. Then,
    you’ll define a Pydantic model, `Article`, to specify the structure of the information
    you want to extract from a given text. With the use of a custom prompt template
    and the ChatOpenAI model, you’ll instruct the AI to extract key points and contrarian
    views from an article. Finally, the extracted data is neatly converted into your
    predefined Pydantic model and printed out, allowing you to see the structured
    information pulled from the text.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先导入各种模块，包括`PydanticToolsParser`和`ChatPromptTemplate`，这些对于解析和模板化你的提示至关重要。然后，你将定义一个Pydantic模型`Article`，以指定从给定文本中提取信息所需的结构。通过使用自定义提示模板和ChatOpenAI模型，你将指示AI从文章中提取关键点和相反观点。最后，提取的数据将被整洁地转换为预定义的Pydantic模型并打印出来，让你看到从文本中提取的结构化信息。
- en: 'There are several key points, including:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个关键点，包括：
- en: Converting Pydantic schema to OpenAI tools
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 将Pydantic模式转换为OpenAI工具
- en: '`tools = [convert_to_openai_tool(p) for p in pydantic_schemas]`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`tools = [convert_to_openai_tool(p) for p in pydantic_schemas]`'
- en: Binding the tools directly to the LLM
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将工具直接绑定到LLM
- en: '`model = model.bind_tools(tools=tools)`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`model = model.bind_tools(tools=tools)`'
- en: Creating an LCEL chain that contains a tools parser
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 创建包含工具解析器的LCEL链
- en: '`chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)`'
- en: Extracting Data with LangChain
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain提取数据
- en: The `create_extraction_chain_pydantic` function provides a more concise version
    of the previous implementation. By simply inserting a Pydantic model and an LLM
    that supports function calling, you can easily achieve parallel function calling.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_extraction_chain_pydantic`函数提供了先前实现的一个更简洁版本。通过简单地插入一个Pydantic模型和一个支持函数调用的LLM，你可以轻松实现并行函数调用。'
- en: 'Input:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE33]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `Person` Pydantic model has two properties, `name` and `age`; by calling
    the `create_extraction_chain_pydantic` function with the input text, the LLM invokes
    the same function twice and creates two `People` objects.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`Person` Pydantic模型有两个属性，`name`和`age`；通过使用输入文本调用`create_extraction_chain_pydantic`函数，LLM将调用同一函数两次并创建两个`People`对象。'
- en: Query Planning
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询规划
- en: 'You may experience problems when user queries have multiple intents with intricate
    dependencies. *Query planning* is an effective way to parse a user’s query into
    a series of steps that can be executed as a query graph with relevant dependencies:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户查询具有复杂依赖关系的多个意图时，你可能会遇到问题。*查询规划*是一种有效的将用户查询解析成一系列步骤的方法，这些步骤可以作为具有相关依赖关系的查询图执行：
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Defining `QueryPlan` and `Query` allows you to first ask an LLM to parse a user’s
    query into multiple steps. Let’s investigate how to create the query plan.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`QueryPlan`和`Query`允许你首先让LLM将用户的查询解析成多个步骤。让我们来研究如何创建查询计划。
- en: 'Input:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE36]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Initiate a `ChatOpenAI` instance and create a `PydanticOutputParser` for the
    `QueryPlan` structure. Then the LLM response is called and parsed, producing a
    structured `query_graph` for your tasks with their unique dependencies.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个`ChatOpenAI`实例并为`QueryPlan`结构创建一个`PydanticOutputParser`。然后调用LLM响应并解析，为你的任务生成具有独特依赖关系的结构化`query_graph`。
- en: Creating Few-Shot Prompt Templates
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建少样本提示模板
- en: Working with the generative capabilities of LLMs often involves making a choice
    between *zero-shot* and *few-shot learning (k-shot)*. While zero-shot learning
    requires no explicit examples and adapts to tasks based solely on the prompt,
    its dependence on the pretraining phase means it may not always yield precise
    results.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用LLM的生成能力时，通常需要在*零样本学习*和*少样本学习（k样本学习）*之间做出选择。虽然零样本学习不需要显式的示例，并且仅根据提示适应任务，但其对预训练阶段的依赖意味着它可能不会总是产生精确的结果。
- en: On the other hand, with few-shot learning, which involves providing a few examples
    of the desired task performance in the prompt, you have the opportunity to optimize
    the model’s behavior, leading to more desirable outputs.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在少样本学习的情况下，即在提示中提供少量所需任务性能的示例，你有机会优化模型的行为，从而得到更令人满意的结果。
- en: Due to the token LLM context length, you will often finding yourself competing
    between adding lots of high-quality k-shot examples into your prompts while still
    aiming to generate an effective and deterministic LLM output.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM的token上下文长度限制，你将经常发现自己需要在添加大量高质量k样本示例的同时，仍然旨在生成有效且确定的LLM输出。
- en: Note
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Even as the token context window limit within LLMs continues to increase, providing
    a specific number of k-shot examples helps you minimize API costs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在LLM中标记上下文窗口限制持续增加，提供特定数量的k-shot示例也有助于您最小化API成本。
- en: 'Let’s explore two methods for adding k-shot examples into your prompts with
    *few-shot prompt templates*: using *fixed examples* and using an *example selector*.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨两种将k-shot示例添加到您的提示中的方法，使用*固定示例*和使用*示例选择器*的*少量提示模板*。
- en: Fixed-Length Few-Shot Examples
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固定长度少量示例
- en: 'First, let’s look at how to create a few-shot prompt template using a fixed
    number of examples. The foundation of this method lies in creating a robust set
    of few-shot examples:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用固定数量的示例创建少量提示模板。这种方法的基础是创建一组强大的少量示例：
- en: '[PRE38]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Each example is a dictionary containing a `question` and `answer` key that will
    be used to create pairs of `HumanMessage` and `AIMessage` messages.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 每个示例都是一个包含`question`和`answer`键的字典，这些键将用于创建`HumanMessage`和`AIMessage`消息对。
- en: Formatting the Examples
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式化示例
- en: Next, you’ll configure a `ChatPromptTemplate` for formatting the individual
    examples, which will then be inserted into a `FewShotChatMessagePromptTemplate`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将配置一个用于格式化单个示例的`ChatPromptTemplate`，然后将其插入到`FewShotChatMessagePromptTemplate`中。
- en: 'Input:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Output:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE40]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice how `example_prompt` will create `HumanMessage` and `AIMessage` pairs
    with the prompt inputs of `{question}` and `{answer}`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`example_prompt`如何创建具有提示输入`{question}`和`{answer}`的`HumanMessage`和`AIMessage`对。
- en: After running `few_shot_prompt.format()`, the few-shot examples are printed
    as a string. As you’d like to use these within a `ChatOpenAI()` LLM request, let’s
    create a new `ChatPromptTemplate`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`few_shot_prompt.format()`之后，少量示例作为字符串打印出来。由于您希望将其用于`ChatOpenAI()` LLM请求中，让我们创建一个新的`ChatPromptTemplate`。
- en: 'Input:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE41]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE42]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: After invoking the LCEL chain on `final_prompt`, your few-shot examples are
    added after the `SystemMessage`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在对`final_prompt`调用LCEL链后，少量示例被添加到`SystemMessage`之后。
- en: Notice that the LLM only returns `'Washington, D.C.'` This is because after
    the LLMs response is returned, *it is parsed* by `StrOutputParser()`, an output
    parser. Adding `StrOutputParser()` is a common way to ensure that LLM responses
    in chains *return string values*. You’ll explore this more in depth while learning
    sequential chains in LCEL.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LLM只返回`'Washington, D.C.'`。这是因为LLM的响应返回后，它会被`StrOutputParser()`解析，这是一个输出解析器。添加`StrOutputParser()`是确保LLM在链中返回*字符串值*的常见方法。您将在学习LCEL中的顺序链时深入了解这一点。
- en: Selecting Few-Shot Examples by Length
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据长度选择少量示例
- en: 'Before diving into the code, let’s outline your task. Imagine you’re building
    a storytelling application powered by GPT-4\. A user enters a list of character
    names with previously generated stories. However, each user’s list of characters
    might have a different length. Including too many characters might generate a
    story that surpasses the LLM’s context window limit. That’s where you can use
    `LengthBasedExampleSelector` to adapt the prompt according to the length of user
    input:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，让我们概述您的任务。想象一下，您正在构建一个由GPT-4驱动的讲故事应用。用户输入一组之前生成的角色名称列表。然而，每个用户的角色列表长度可能不同。包含过多的角色可能会生成超出LLM上下文窗口限制的故事。这就是您可以使用`LengthBasedExampleSelector`根据用户输入的长度调整提示的地方：
- en: '[PRE43]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: First, you set up a `PromptTemplate` that takes two input variables for each
    example. Then `LengthBasedExampleSelector` adjusts the number of examples according
    to the *length of the examples input*, ensuring your LLM doesn’t generate a story
    beyond its context window.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您设置一个`PromptTemplate`，它为每个示例接受两个输入变量。然后`LengthBasedExampleSelector`根据示例输入的*长度*调整示例数量，确保您的LLM不会生成超出其上下文窗口的故事。
- en: 'Also, you’ve customized the `get_text_length` function to use the `num_tokens_from_string`
    function that counts the total number of tokens using `tiktoken`. This means that
    `max_length=1000` represents the *number of tokens* rather than using the following
    default function:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您已自定义`get_text_length`函数，使用`num_tokens_from_string`函数通过`tiktoken`计算总令牌数。这意味着`max_length=1000`代表的是*令牌数*，而不是使用以下默认函数：
- en: '`get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))`'
- en: 'Now, to tie all these elements together:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些元素结合起来：
- en: '[PRE44]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Output:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Provide Examples and Specify Format
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供示例并指定格式
- en: When working with few-shot examples, the length of the content matters in determining
    how many examples the AI model can take into account. Tune the length of your
    input content and provide apt examples for efficient results to prevent the LLM
    from generating content that might surpass its context window limit.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用少样本示例时，内容的长度在确定 AI 模型可以考虑到多少示例时很重要。调整输入内容的长度，并提供适当的示例以获得高效的结果，以防止大型语言模型生成可能超过其上下文窗口限制的内容。
- en: After formatting the prompt, you create a chat model with `ChatOpenAI()` and
    load the formatted prompt into a `SystemMessage` that creates a small story about
    Frodo from *Lord of the Rings*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在格式化提示后，你使用 `ChatOpenAI()` 创建一个聊天模型，并将格式化后的提示加载到一个 `SystemMessage` 中，创建一个关于《指环王》中弗罗多的简短故事。
- en: 'Rather than creating and formatting a `ChatPromptTemplate`, it’s often much
    easier to simply invoke a `SystemMesage` with a formatted prompt:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是创建和格式化一个 `ChatPromptTemplate`，通常简单地调用一个带有格式化提示的 `SystemMessage` 要容易得多：
- en: '[PRE46]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Limitations with Few-Shot Examples
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少样本示例的局限性
- en: 'Few-shot learning has limitations. Although it can prove beneficial in certain
    scenarios, it might not always yield the expected high-quality results. This is
    primarily due to two reasons:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习有其局限性。尽管在某些场景中可能有益，但它并不总是能产生预期的高质量结果。这主要是由于两个原因：
- en: Pretrained models like GPT-4 can sometimes overfit to the few-shot examples,
    making them prioritize the examples over the actual prompt.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型如 GPT-4 有时会对少样本示例过度拟合，使它们优先考虑示例而不是实际的提示。
- en: LLMs have a token limit. As a result, there will always be a trade-off between
    the number of examples and the length of the response. Providing more examples
    might limit the response length and vice versa.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）有一个令牌限制。因此，在示例数量和响应长度之间总会有权衡。提供更多示例可能会限制响应长度，反之亦然。
- en: These limitations can be addressed in several ways. First, if few-shot prompting
    is not yielding the desired results, consider using differently framed phrases
    or experimenting with the language of the prompts themselves. Variations in how
    the prompt is phrased can result in different responses, highlighting the trial-and-error
    nature of prompt engineering.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这些局限性可以通过几种方式解决。首先，如果少样本提示没有产生期望的结果，考虑使用不同框架的短语，或者尝试调整提示本身的语言。提示语法的不同变化可能会导致不同的响应，突出了提示工程中的试错性质。
- en: Second, think about including explicit instructions to the model to ignore the
    examples after it understands the task or to use the examples just for formatting
    guidance. This might influence the model to not overfit to the examples.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，考虑向模型添加明确的指令，让它忽略理解任务后的示例，或者仅将示例用于格式化指导。这可能会影响模型不会过度拟合示例。
- en: If the tasks are complex and the performance of the model with few-shot learning
    is not satisfactory, you might need to consider [fine-tuning](https://oreil.ly/S40bZ)
    your model. Fine-tuning provides a more nuanced understanding of a specific task
    to the model, thus improving the performance significantly.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务复杂且模型在少样本学习中的表现不令人满意，你可能需要考虑[微调](https://oreil.ly/S40bZ)你的模型。微调使模型对特定任务有更细微的理解，从而显著提高性能。
- en: Saving and Loading LLM Prompts
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载 LLM 提示
- en: To effectively leverage generative AI models such as GPT-4, it is beneficial
    to store prompts as files instead of Python code. This approach enhances the shareability,
    storage, and versioning of your prompts.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地利用 GPT-4 等生成式 AI 模型，将提示存储为文件而不是 Python 代码是有益的。这种方法增强了提示的可分享性、存储和版本控制。
- en: LangChain supports both saving and loading prompts from JSON and YAML. Another
    key feature of LangChain is its support for detailed specification in one file
    or distributed across multiple files. This means you have the flexibility to store
    different components such as templates, examples, and others in distinct files
    and reference them as required.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 支持从 JSON 和 YAML 中保存和加载提示。LangChain 的另一个关键特性是支持在一个文件中详细指定或在多个文件中分布。这意味着你可以灵活地将不同的组件，如模板、示例等存储在不同的文件中，并按需引用它们。
- en: 'Let’s learn how to save and load prompts:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何保存和加载提示：
- en: '[PRE47]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: After importing `PromptTemplate` and `load_prompt` from the `langchain.prompts`
    module, you define a `PromptTemplate` for English-to-Spanish translation tasks
    and save it as *translation_prompt.json*. Finally, you load the saved prompt template
    using the `load_prompt` function, which returns an instance of `PromptTemplate`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在从 `langchain.prompts` 模块导入 `PromptTemplate` 和 `load_prompt` 之后，你为英语到西班牙语的翻译任务定义一个
    `PromptTemplate` 并将其保存为 *translation_prompt.json*。最后，使用 `load_prompt` 函数加载保存的提示模板，该函数返回一个
    `PromptTemplate` 实例。
- en: Warning
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Please be aware that LangChain’s prompt saving may not work with all types of
    prompt templates. To mitigate this, you can utilize the *pickle* library or *.txt*
    files to read and write any prompts that LangChain does not support.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LangChain 的提示保存可能不适用于所有类型的提示模板。为了减轻这种情况，你可以利用 *pickle* 库或 *.txt* 文件来读取和写入
    LangChain 不支持的任何提示。
- en: 'You’ve learned how to create few-shot prompt templates using LangChain with
    two techniques: a fixed number of examples and using an example selector.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何使用 LangChain 通过两种技术创建少量提示模板：固定数量的示例和使用示例选择器。
- en: The former creates a set of few-shot examples and uses a `ChatPromptTemplate`
    object to format these into chat messages. This forms the basis for creating a
    `FewShotChatMessagePromptTemplate` object.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 前者创建一组少量示例，并使用 `ChatPromptTemplate` 对象将这些格式化为聊天消息。这构成了创建 `FewShotChatMessagePromptTemplate`
    对象的基础。
- en: The latter approach, using an example selector, is handy when user input varies
    significantly in length. In such scenarios, a `LengthBasedExampleSelector` can
    be utilized to adjust the number of examples based on user input length. This
    ensures your LLM does not exceed its context window limit.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 后者方法，即使用示例选择器，当用户输入长度差异显著时很有用。在这种情况下，可以使用 `LengthBasedExampleSelector` 来根据用户输入长度调整示例数量。这确保你的
    LLM 不会超过其上下文窗口限制。
- en: Moreover, you’ve seen how easy it is to store/load prompts as files, enabling
    enhanced shareability, storage, and versioning.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你已经看到存储/加载提示作为文件是多么容易，这增强了可分享性、存储和版本控制。
- en: Data Connection
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据连接
- en: Harnessing an LLM application, coupled with your data, uncovers a plethora of
    opportunities to boost efficiency while refining your decision-making processes.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合 LLM 应用程序和你的数据，可以揭示大量提升效率并细化决策过程的机会。
- en: 'Your organization’s data may manifest in various forms:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 你的组织数据可能以各种形式呈现：
- en: Unstructured data
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据
- en: This could include Google Docs, threads from communication platforms such as
    Slack or Microsoft Teams, web pages, internal documentation, or code repositories
    on GitHub.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括 Google Docs、来自 Slack 或 Microsoft Teams 等通信平台的线程、网页、内部文档或 GitHub 上的代码仓库。
- en: Structured data
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据
- en: Data neatly housed within SQL, NoSQL, or Graph databases.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整齐地存储在 SQL、NoSQL 或图数据库中。
- en: To query your unstructured data, a process of loading, transforming, embedding,
    and subsequently storing it within a vector database is necessary. A *vector database*
    is a specialized type of database designed to efficiently store and query data
    in the form of vectors, which represent complex data like text or images in a
    format suitable for machine learning and similarity search.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询你的非结构化数据，需要加载、转换、嵌入，然后将其存储在向量数据库中的过程。*向量数据库* 是一种专门类型的数据库，旨在高效地存储和查询以向量形式表示的数据，这些向量以适合机器学习和相似性搜索的格式表示复杂数据，如文本或图像。
- en: As for structured data, given its already indexed and stored state, you can
    utilize a LangChain agent to conduct an intermediate query on your database. This
    allows for the extraction of specific features, which can then be used within
    your LLM prompts.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构化数据，鉴于其已索引和存储的状态，你可以利用 LangChain 代理在你的数据库上执行中间查询。这允许提取特定特征，然后可以在你的 LLM 提示中使用。
- en: There are multiple Python packages that can help with your data ingestion, including
    [Unstructured](https://oreil.ly/n0hDD), [LlamaIndex](https://www.llamaindex.ai),
    and [LangChain](https://oreil.ly/PjV9o).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个 Python 包可以帮助你进行数据摄取，包括 [Unstructured](https://oreil.ly/n0hDD)、[LlamaIndex](https://www.llamaindex.ai)
    和 [LangChain](https://oreil.ly/PjV9o)。
- en: '[Figure 4-2](#figure-4-2) illustrates a standardized approach to data ingestion.
    It begins with the data sources, which are then loaded into documents. These documents
    are then chunked and stored within a vector database for later retrieval.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-2](#figure-4-2) 展示了数据摄取的标准化方法。它从数据源开始，然后将这些数据加载到文档中。这些文档随后被分块并存储在向量数据库中以供后续检索。'
- en: '![Data Connection](assets/pega_0402.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![数据连接](assets/pega_0402.png)'
- en: Figure 4-2\. A data connection to retrieval pipeline
  id: totrans-352
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 数据连接到检索管道
- en: 'In particular LangChain equips you with essential components to load, modify,
    store, and retrieve your data:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是LangChain为你提供了加载、修改、存储和检索数据的必要组件：
- en: Document loaders
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 文档加载器
- en: These facilitate uploading informational resources, or *documents*, from a diverse
    range of sources such as Word documents, PDF files, text files, or even web pages.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具便于上传来自各种来源的信息资源或*文档*，例如Word文档、PDF文件、文本文件，甚至是网页。
- en: Document transformers
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 文档转换器
- en: These tools allow the segmentation of documents, conversion into a Q&A layout,
    elimination of superfluous documents, and much more.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具允许对文档进行分段、转换为问答布局、消除冗余文档等。
- en: Text embedding models
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入模型
- en: These can transform unstructured text into a sequence of floating-point numbers
    used for similarity search by vector stores.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具可以将非结构化文本转换为浮点数序列，这些序列用于向量存储库进行相似度搜索。
- en: Vector databases (vector stores)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库（向量存储库）
- en: These databases can save and execute searches over embedded data.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据库可以保存并执行对嵌入数据的搜索。
- en: Retrievers
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器
- en: These tools offer the capability to query and retrieve data.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具提供了查询和检索数据的能力。
- en: Also, it’s worth mentioning that other LLM frameworks such as [LlamaIndex](https://oreil.ly/9NcTB)
    work seamlessly with LangChain. [LlamaHub](https://llamahub.ai) is another open
    source library dedicated to document loaders and can create LangChain-specific
    `Document` objects.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，其他LLM框架如[LlamaIndex](https://oreil.ly/9NcTB)与LangChain无缝协作。[LlamaHub](https://llamahub.ai)是另一个致力于文档加载器的开源库，可以创建LangChain特定的`Document`对象。
- en: Document Loaders
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档加载器
- en: 'Let’s imagine you’ve been tasked with building an LLM data collection pipeline
    for NutriFusion Foods. The information that you need to gather for the LLM is
    contained within:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设你被分配了一个为NutriFusion Foods构建LLM数据收集管道的任务。你需要为LLM收集的信息包含在以下内容中：
- en: A PDF of a book called *Principles of Marketing*
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一本名为《市场营销原理》的PDF文件
- en: Two *.docx* marketing reports in a public Google Cloud Storage bucket
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个存储在公共Google Cloud Storage桶中的*.docx*市场营销报告
- en: Three *.csv* files showcasing the marketing performance data for 2021, 2022,
    and 2023
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示2021年、2022年和2023年市场营销表现数据的三个*.csv*文件
- en: Create a new Jupyter Notebook or Python file in *content/chapter_4* of the [shared
    repository](https://oreil.ly/cVTyI), and then run `pip install pdf2image docx2txt
    pypdf`, which will install three packages.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在[共享仓库](https://oreil.ly/cVTyI)的*content/chapter_4*中创建一个新的Jupyter Notebook或Python文件，然后运行`pip
    install pdf2image docx2txt pypdf`，这将安装三个包。
- en: All of the data apart from *.docx* files can be found in [*content/chapter_4/data*](https://oreil.ly/u9gMx).
    You can start by importing all of your various data loaders and creating an empty
    `all_documents` list to store all of the `Document` objects across your data sources.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*.docx*文件之外的所有数据都可以在[*content/chapter_4/data*](https://oreil.ly/u9gMx)中找到。你可以先导入所有各种数据加载器，并创建一个空的`all_documents`列表来存储来自你数据源的所有`Document`对象。
- en: 'Input:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE48]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Output:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE49]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Then using `PyPDFLoader`, you can import a *.pdf* file and split it into multiple
    pages using the `.load_and_split()` function.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`PyPDFLoader`，你可以导入一个*.pdf*文件，并使用`.load_and_split()`函数将其分割成多个页面。
- en: Additionally, it’s possible to add extra metadata to each page because the metadata
    is a Python dictionary on each `Document` object. Also, notice in the preceding
    output for `Document` objects the metadata `source` is attached to.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以为每一页添加额外的元数据，因为元数据是每个`Document`对象上的Python字典。注意，在先前的`Document`对象输出中，元数据`source`是附加的。
- en: Using the package `glob`, you can easily find all of the *.csv* files and individually
    load these into LangChain `Document` objects with a `CSVLoader`.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`glob`包，你可以轻松地找到所有*.csv*文件，并使用`CSVLoader`将它们单独加载到LangChain `Document`对象中。
- en: Finally, the two marketing reports are loaded from a public Google Cloud Storage
    bucket and are then split into 200 token-chunk sizes using a `text_splitter`.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，两个市场营销报告从公共Google Cloud Storage桶中加载，然后使用`text_splitter`分割成200个token块大小。
- en: This section equipped you with the necessary knowledge to create a comprehensive
    document-loading pipeline for NutriFusion Foods’ LLM. Starting with data extraction
    from a PDF, several CSV files and two .*docx* files, each document was enriched
    with relevant metadata for better context.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 本节为你提供了创建NutriFusion Foods LLM全面文档加载管道所需的知识。从从PDF中提取数据、几个CSV文件和两个.*docx*文件开始，每个文档都添加了相关的元数据以获得更好的上下文。
- en: You now have the ability to seamlessly integrate data from a variety of document
    sources into a cohesive data pipeline.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以无缝地将来自各种文档来源的数据集成到一个统一的数据管道中。
- en: Text Splitters
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分割器
- en: Balancing the length of each document is also a crucial factor. If a document
    is too lengthy, it may surpass the *context length* of the LLM (the maximum number
    of tokens that an LLM can process within a single request). But if the documents
    are excessively fragmented into smaller chunks, there’s a risk of losing significant
    contextual information, which is equally undesirable.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡每个文档的长度也是一个关键因素。如果文档太长，可能会超过 LLM 的 *上下文长度*（LLM 在单个请求中可以处理的令牌的最大数量）。但如果文档被过度分割成更小的块，可能会丢失重要的上下文信息，这也是不可取的。
- en: 'You might encounter specific challenges while text splitting, such as:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分割过程中，您可能会遇到一些特定的挑战，例如：
- en: Special characters such as hashtags, @ symbols, or links might not split as
    anticipated, affecting the overall structure of the split documents.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊字符，如哈希标签、@符号或链接可能不会按预期分割，从而影响分割文档的整体结构。
- en: If your document contains intricate formatting like tables, lists, or multilevel
    headings, the text splitter might find it difficult to retain the original formatting.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的文档包含复杂的格式，如表格、列表或多级标题，文本分割器可能难以保留原始格式。
- en: There are ways to overcome these challenges that we’ll explore later.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后探讨克服这些挑战的方法。
- en: This section introduces you to text splitters in LangChain, tools utilized to
    break down large chunks of text to better adapt to your model’s context window.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您介绍了 LangChain 中的文本分割器，这些工具用于将大块文本分解，以更好地适应您模型的上下文窗口。
- en: Note
  id: totrans-389
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There isn’t a perfect document size. Start by using good heuristics and then
    build a training/test set that you can use for LLM evaluation.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 没有完美的文档大小。首先使用良好的启发式方法，然后构建一个可以用于 LLM 评估的训练/测试集。
- en: 'LangChain provides a range of text splitters so that you can easily split by
    any of the following:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一系列文本分割器，以便您可以轻松地按以下方式分割：
- en: Token count
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令牌计数
- en: Recursively by multiple characters
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过多个字符递归
- en: Character count
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符计数
- en: Code
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码
- en: Markdown headers
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markdown 标题
- en: 'Let’s explore three popular splitters: `CharacterTextSplitter`, `TokenTextSplitter`,
    and `RecursiveCharacterTextSplitter`.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索三种流行的分割器：`CharacterTextSplitter`、`TokenTextSplitter` 和 `RecursiveCharacterTextSplitter`。
- en: Text Splitting by Length and Token Size
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按长度和令牌大小进行文本分割
- en: In [Chapter 3](ch03.html#standard_practices_03), you learned how to count the
    number of tokens within a GPT-4 call with [tiktoken](https://oreil.ly/uz05O).
    You can also use tiktoken to split strings into appropriately sized chunks and
    documents.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 3 章](ch03.html#standard_practices_03) 中，您学习了如何使用 [tiktoken](https://oreil.ly/uz05O)
    计算一个 GPT-4 调用中的令牌数量。您还可以使用 tiktoken 将字符串分割成适当大小的块和文档。
- en: Remember to install tiktoken and langchain-text-splitters with `pip install
    tiktoken langchain-text-splitters`.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住使用 `pip install tiktoken langchain-text-splitters` 安装 tiktoken 和 langchain-text-splitters。
- en: To split by token count in LangChain, you can use a `CharacterTextSplitter`
    with a `.from_tiktoken_encoder()` function.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 LangChain 中按令牌计数进行分割，您可以使用具有 `.from_tiktoken_encoder()` 函数的 `CharacterTextSplitter`。
- en: You’ll initially create a `CharacterTextSplitter` with a chunk size of 50 characters
    and no overlap. Using the `split_text` method, you’re chopping the text into pieces
    and then printing out the total number of chunks created.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 您最初将创建一个具有50个字符块大小且无重叠的 `CharacterTextSplitter`。使用 `split_text` 方法，您将文本分割成片段，然后打印出创建的总块数。
- en: 'Then you’ll do the same thing, but this time with a *chunk overlap* of 48 characters.
    This shows how the number of chunks changes based *on whether you allow overlap*,
    illustrating the impact of these settings on how your text gets divided:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将执行相同操作，但这次带有 48 个字符的 *块重叠*。这显示了是否允许重叠如何影响块的数量，说明了这些设置对文本分割方式的影响：
- en: '[PRE50]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Output:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE51]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the previous section, you used the following to load and split the *.pdf*
    into LangChain documents:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您使用了以下方法将 *.pdf* 加载并分割成 LangChain 文档：
- en: '`pages = loader.load_and_split()`'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pages = loader.load_and_split()`'
- en: 'It’s possible for you to have more granular control on the size of each document
    by creating a `TextSplitter` and attaching it to your `Document` loading pipelines:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建一个 `TextSplitter` 并将其附加到您的 `Document` 加载管道上来对每个文档的大小进行更细粒度的控制：
- en: '`def load_and_split(text_splitter: TextSplitter | None = None) -> List[Document]`'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def load_and_split(text_splitter: TextSplitter | None = None) -> List[Document]`'
- en: 'Simply create a `TokenTextSplitter` with a `chunk_size=500` and a `chunk_overlap`
    of 50:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地创建一个`TokenTextSplitter`，设置`chunk_size=500`和`chunk_overlap`为50：
- en: '[PRE52]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The *Principles of Marketing* book contains 497 pages, but after using a `TokenTextSplitter`
    with a `chunk_size` of 500 tokens, you’ve created 776 smaller LangChain `Document`
    objects.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 《市场营销原理》一书共有497页，但使用`chunk_size`为500个标记的`TokenTextSplitter`后，您已创建了776个较小的LangChain
    `Document`对象。
- en: Text Splitting with Recursive Character Splitting
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归字符分割进行文本分割
- en: Dealing with sizable blocks of text can present unique challenges in text analysis.
    A helpful strategy for such situations involves the use of *recursive character
    splitting*. This method facilitates the division of a large body of text into
    manageable segments, making further analysis more accessible.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量文本块在文本分析中可能带来独特的挑战。在这种情况下，一种有用的策略是使用*递归字符分割*。这种方法便于将大量文本分割成可管理的段落，使得进一步分析更加容易。
- en: This approach becomes incredibly effective when handling generic text. It leverages
    a list of characters as parameters and sequentially splits the text based on these
    characters. The resulting sections continue to be divided until they reach an
    acceptable size. By default, the character list comprises `"\n\n"`, `"\n"`, `"
    "`, and `""`. This arrangement aims to retain the integrity of paragraphs, sentences,
    and words, preserving the semantic context.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理通用文本时，这种方法变得非常有效。它利用字符列表作为参数，并按顺序根据这些字符分割文本。生成的部分将继续分割，直到达到可接受的大小。默认情况下，字符列表包括`"\n\n"`、`"\n"`、`"
    "`和`""`。这种安排旨在保留段落的完整性、句子和单词，保留语义上下文。
- en: The process hinges on the character list provided and sizes the resulting sections
    based on the character count.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程取决于提供的字符列表，并根据字符计数来调整生成的部分大小。
- en: Before diving into the code, it’s essential to understand what the `RecursiveCharacterTextSplitter`
    does. It takes a text and a list of delimiters (characters that define the boundaries
    for splitting the text). Starting from the first delimiter in the list, the splitter
    attempts to divide the text. If the resulting chunks are still too large, it proceeds
    to the next delimiter, and so on. This process continues *recursively* until the
    chunks are small enough or all delimiters are exhausted.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，理解`RecursiveCharacterTextSplitter`的作用至关重要。它接受一段文本和一个分隔符列表（定义文本分割边界的字符）。从列表中的第一个分隔符开始，分割器尝试分割文本。如果生成的块仍然太大，它将继续到下一个分隔符，依此类推。这个过程以递归的方式继续进行，直到块足够小或者所有分隔符都耗尽。
- en: Using the preceding `text` variable, start by importing `RecursiveCharacterText​Splitter`.
    This instance will be responsible for splitting the text. When initializing the
    splitter, parameters `chunk_size`, `chunk_overlap`, and `length_function` are
    set. Here, `chunk_size` is set to 100, and `chunk_overlap` to 20.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的`text`变量，首先导入`RecursiveCharacterTextSplitter`。此实例将负责分割文本。在初始化分割器时，设置参数`chunk_size`、`chunk_overlap`和`length_function`。在这里，`chunk_size`设置为100，`chunk_overlap`设置为20。
- en: 'The `length_function` is defined as `len` to determine the size of the chunks.
    It’s also possible to modify the `length_function` argument to use a tokenizer
    count instead of using the default `len` function, which will count characters:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`length_function`被定义为`len`，用于确定块的大小。还可以修改`length_function`参数，使用分词器计数而不是默认的`len`函数，这将计算字符：'
- en: '[PRE53]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Once the `text_splitter` instance is ready, you can use `.split_text` to split
    the `text` variable into smaller chunks. These chunks are stored in the `texts`
    Python list:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`text_splitter`实例准备就绪，您可以使用`.split_text`将`text`变量分割成更小的块。这些块存储在`texts` Python列表中：
- en: '[PRE54]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As well as simply splitting the text with overlap into a list of strings, you
    can easily create LangChain `Document` objects with the `.create_documents` function.
    Creating `Document` objects is useful because it allows you to:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单地将带有重叠的文本分割成字符串列表外，您还可以使用`.create_documents`函数轻松创建LangChain `Document`对象。创建`Document`对象很有用，因为它允许您：
- en: Store documents within a vector database for semantic search
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量数据库中存储文档以进行语义搜索
- en: Add metadata to specific pieces of text
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将元数据添加到特定的文本片段中
- en: Iterate over multiple documents to create a higher-level summary
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历多个文档以创建高级摘要
- en: 'To add metadata, provide a list of dictionaries to the `metadatas` argument:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加元数据，请将字典列表提供给`metadatas`参数：
- en: '[PRE55]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: But what if your existing `Document` objects are too long?
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果您的现有`Document`对象太长怎么办？
- en: 'You can easily handle that by using the `.split_documents` function with a
    `TextSplitter`. This will take in a list of `Document` objects and will return
    a new list of `Document` objects based on your `TextSplitter` class argument settings:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用带有`TextSplitter`的`.split_documents`函数轻松处理这个问题。这将接受一个`Document`对象列表，并根据你的`TextSplitter`类参数设置返回一个新的`Document`对象列表：
- en: '[PRE56]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: You’ve now gained the ability to craft an efficient data loading pipeline, leveraging
    sources such as PDFs, CSVs, and Google Cloud Storage links. Furthermore, you’ve
    learned how to enrich the collected documents with relevant metadata, providing
    meaningful context for analysis and prompt engineering.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经获得了构建高效数据加载管道的能力，利用PDF、CSV和Google Cloud Storage链接等资源。此外，你还学会了如何通过相关元数据丰富收集到的文档，为分析和提示工程提供有意义的背景。
- en: With the introduction of text splitters, you can now strategically manage document
    sizes, optimizing for both the LLM’s context window and the preservation of context-rich
    information. You’ve navigated handling larger texts by employing recursion and
    character splitting. This newfound knowledge empowers you to work seamlessly with
    various document sources and integrate them into a robust data pipeline.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 随着文本分割器的引入，你现在可以战略性地管理文档大小，优化LLM的上下文窗口和保留丰富上下文信息的保存。你通过递归和字符分割处理了更大的文本。这种新获得的知识使你能够无缝地与各种文档源合作，并将它们集成到强大的数据管道中。
- en: Task Decomposition
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务分解
- en: '*Task decomposition* is the strategic process of dissecting complex problems
    into a suite of manageable subproblems. This approach aligns seamlessly with the
    natural tendencies of software engineers, who often conceptualize tasks as interrelated
    subcomponents.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务分解*是将复杂问题分解成一系列可管理的子问题的战略过程。这种方法与软件工程师的自然倾向无缝对接，他们通常将任务视为相互关联的子组件。'
- en: In software engineering, by utilizing task decomposition you can reduce cognitive
    burden and harness the advantages of problem isolation and adherence to the single
    responsibility principle.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，通过利用任务分解可以减少认知负担，并利用问题隔离和遵循单一责任原则的优势。
- en: Interestingly, LLMs stand to gain considerably from the application of task
    decomposition across a range of use cases. This approach aids in maximizing the
    utility and effectiveness of LLMs in problem-solving scenarios by enabling them
    to handle intricate tasks that would be challenging to resolve as a single entity,
    as illustrated in [Figure 4-3](#figure-4-3).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，大型语言模型（LLMs）在多种用例中应用任务分解将获得相当大的收益。这种方法通过使LLMs能够处理作为单一实体难以解决的复杂任务，从而有助于最大化LLMs在问题解决场景中的效用和有效性，如图[4-3](#figure-4-3)所示。
- en: 'Here are several examples of LLMs using decomposition:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些LLM使用分解的例子：
- en: Complex problem solving
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂问题解决
- en: In instances where a problem is multifaceted and cannot be solved through a
    single prompt, task decomposition is extremely useful. For example, solving a
    complex legal case could be broken down into understanding the case’s context,
    identifying relevant laws, determining legal precedents, and crafting arguments.
    Each subtask can be solved independently by an LLM, providing a comprehensive
    solution when combined.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题多方面且不能通过单一提示解决的问题中，任务分解非常有用。例如，解决复杂的法律案件可以分解为理解案件背景、识别相关法律、确定法律先例和构建论点。每个子任务都可以由LLM独立解决，当结合在一起时，提供全面的解决方案。
- en: Content generation
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 内容生成
- en: For generating long-form content such as articles or blogs, the task can be
    decomposed into generating an outline, writing individual sections, and then compiling
    and refining the final draft. Each step can be individually managed by GPT-4 for
    better results.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成长篇内容，如文章或博客，任务可以分解为生成提纲、撰写各个部分，然后编译和精炼最终草稿。每个步骤都可以由GPT-4独立管理，以获得更好的结果。
- en: Large document summary
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 大型文档摘要
- en: Summarizing lengthy documents such as research papers or reports can be done
    more effectively by decomposing the task into several smaller tasks, like understanding
    individual sections, summarizing them independently, and then compiling a final
    summary.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将任务分解为几个更小的任务，如理解各个部分、独立总结它们，然后编译最终摘要，可以更有效地总结像研究论文或报告这样篇幅较长的文档。
- en: Interactive conversational agents
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式对话代理
- en: For creating advanced chatbots, task decomposition can help manage different
    aspects of conversation such as understanding user input, maintaining context,
    generating relevant responses, and managing dialogue flow.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 对于创建高级聊天机器人，任务分解可以帮助管理对话的不同方面，如理解用户输入、保持上下文、生成相关响应和管理对话流程。
- en: Learning and tutoring systems
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和辅导系统
- en: In digital tutoring systems, decomposing the task of teaching a concept into
    understanding the learner’s current knowledge, identifying gaps, suggesting learning
    materials, and evaluating progress can make the system more effective. Each subtask
    can leverage GPT-4’s generative abilities.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字辅导系统中，将教授一个概念的任务分解为理解学习者的当前知识、识别差距、建议学习材料和评估进度可以使系统更有效。每个子任务都可以利用GPT-4的生成能力。
- en: '![.Task decomposition with GPT-4.](assets/pega_0403.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![.使用GPT-4进行任务分解](assets/pega_0403.png)'
- en: Figure 4-3\. Task decomposition with LLMs
  id: totrans-451
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 使用LLMs进行任务分解
- en: Divide Labor
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分工
- en: Task decomposition is a crucial strategy for you to tap into the full potential
    of LLMs. By dissecting complex problems into simpler, manageable tasks, you can
    leverage the problem-solving abilities of these models more effectively and efficiently.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分解是你挖掘LLMs（大型语言模型）全部潜力的关键策略。通过将复杂问题分解成更简单、可管理的任务，你可以更有效地利用这些模型的问题解决能力。
- en: In the sections ahead, you’ll learn how to create and integrate multiple LLM
    chains to orchestrate more complicated workflows.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何创建和集成多个LLM链来编排更复杂的流程。
- en: Prompt Chaining
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示链
- en: Often you’ll find that attempting to do a single task within one prompt is impossible.
    You can utilize a mixture of *prompt chaining*, which involves combining multiple
    prompt inputs/outputs with specifically tailored LLM prompts to build up an idea.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你会发现，在一个提示中尝试执行单个任务是不可能的。你可以利用*提示链*的组合，这涉及到将多个提示输入/输出与专门定制的LLM提示结合在一起，以构建一个想法。
- en: 'Let’s imagine an example with a film company that would like to partially automate
    their film creation. This could be broken down into several key components, such
    as:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个例子，一个电影公司希望部分自动化他们的电影制作。这可以分解成几个关键组成部分，例如：
- en: Character creation
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色创建
- en: Plot generation
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情节生成
- en: Scenes/world building
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景/世界构建
- en: '[Figure 4-4](#figure-4-4) shows what the prompt workflow might look like.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-4](#figure-4-4)显示了提示工作流程可能的样子。'
- en: '![Sequential Story Creation Process](assets/pega_0404.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![顺序故事创建过程](assets/pega_0404.png)'
- en: Figure 4-4\. A sequential story creation process
  id: totrans-463
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 顺序故事创建过程
- en: Sequential Chain
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序链
- en: 'Let’s decompose the task into *multiple chains* and recompose them into a single
    chain:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将任务分解成*多个链*，然后将它们重新组合成一个链：
- en: '`character_generation_chain`'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '`character_generation_chain`'
- en: A chain responsible for creating multiple characters given a `'genre'`.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 一个链，根据`'genre'`创建多个角色。
- en: '`plot_generation_chain`'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_generation_chain`'
- en: A chain that will create the plot given the `'characters'` and `'genre'` keys.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一个链，根据`'characters'`和`'genre'`键创建情节。
- en: '`scene_generation_chain`'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '`scene_generation_chain`'
- en: This chain will generate any missing scenes that were not initially generated
    from the `plot_generation_chain`.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这个链将生成从`plot_generation_chain`最初未生成的任何缺失场景。
- en: 'Let’s start by creating three separate `ChatPromptTemplate` variables, one
    for each chain:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建三个独立的`ChatPromptTemplate`变量，每个链一个：
- en: '[PRE57]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Notice that as the prompt templates flow from character to plot and scene generation,
    you add more placeholder variables from the previous steps.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着提示模板从角色到情节和场景生成的流动，你从上一步添加了更多的占位符变量。
- en: The question remains, how can you guarantee that these extra strings are available
    for your downstream `ChatPromptTemplate` variables?
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，你如何保证这些额外的字符串对下游的`ChatPromptTemplate`变量可用？
- en: itemgetter and Dictionary Key Extraction
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`itemgetter`和字典键提取'
- en: 'Within LCEL you can use the `itemgetter` function from the `operator` package
    to extract keys from the previous step, as long as a dictionary was present within
    the previous step:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在LCEL中，你可以使用`operator`包中的`itemgetter`函数从上一步提取键，只要上一步中存在字典：
- en: '[PRE58]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `RunnablePassThrough` function simply passes any inputs directly to the
    next step. Then a new dictionary is created by using the same key within the `invoke`
    function; this key is extracted by using `itemgetter("genre")`.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '`RunnablePassThrough`函数简单地将任何输入直接传递到下一步。然后，通过在`invoke`函数中使用相同的键创建一个新的字典；这个键是通过使用`itemgetter("genre")`提取的。'
- en: It’s essential to use the `itemgetter` function throughout parts of your LCEL
    chains so that any subsequent `ChatPromptTemplate` placeholder variables will
    always have valid values.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的LCEL链的各个部分使用`itemgetter`函数是至关重要的，这样任何后续的`ChatPromptTemplate`占位符变量都将始终具有有效的值。
- en: 'Additionally, you can use `lambda` or `RunnableLambda` functions within an
    LCEL chain to manipulate previous dictionary values. A lambda is an anonynous
    function within Python:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以在LCEL链中使用`lambda`或`RunnableLambda`函数来操作之前的字典值。在Python中，lambda是一个匿名函数：
- en: '[PRE59]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now that you’re aware of how to use `RunnablePassThrough`, `itemgetter`, and
    `lambda` functions, let’s introduce one final piece of syntax: `RunnableParallel`:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何使用`RunnablePassThrough`、`itemgetter`和`lambda`函数，让我们介绍最后一种语法：`RunnableParallel`：
- en: '[PRE60]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: First, you import `RunnableParallel` and create two LCEL chains called `master_chain`
    and `master_chain_two`. These are then invoked with exactly the same arguments;
    the `RunnablePassthrough` then passes the dictionary into the second part of the
    chain.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入`RunnableParallel`并创建两个名为`master_chain`和`master_chain_two`的LCEL链。然后，它们将使用完全相同的参数被调用；`RunnablePassthrough`随后将字典传递到链的下一部分。
- en: The second part of `master_chain` and `master_chain_two` will return exactly
    the *same result.*
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '`master_chain`和`master_chain_two`的第二部分将返回**完全相同的结果**。'
- en: So rather than directly using a dictionary, you can choose to use a `RunnableParallel`
    function instead. These two chain outputs *are interchangeable*, so choose whichever
    syntax you find more comfortable.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，而不是直接使用字典，你可以选择使用`RunnableParallel`函数。这两个链输出**可以互换**，所以选择你更舒适的语法。
- en: 'Let’s create three LCEL chains using the prompt templates:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用提示模板创建三个LCEL链：
- en: '[PRE61]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: After creating all the chains, you can then attach them to a master LCEL chain.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建所有链之后，你可以将它们附加到一个主LCEL链上。
- en: 'Input:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE62]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The output is truncated when you see `...` to save space. However, in total
    there were five characters and nine scenes generated.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到`...`时，输出将被截断以节省空间。然而，总共生成了五个角色和九个场景。
- en: 'Output:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE63]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The scenes are split into separate items within a Python list. Then two new
    prompts are created to generate both a character script and a summarization prompt:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 场景被分割成Python列表中的单独项目。然后创建了两个新的提示，用于生成角色剧本和总结提示：
- en: '[PRE64]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Technically, you could generate all of the scenes asynchronously. However, it’s
    beneficial to know what each character has done in the *previous scene to avoid
    repeating points*.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你可以异步生成所有场景。然而，了解每个角色在**前一个场景中做了什么**以避免重复点是很有益的。
- en: 'Therefore, you can create two LCEL chains, one for generating the character
    scripts per scene and the other for summarizations of previous scenes:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以创建两个LCEL链，一个用于每个场景生成角色剧本，另一个用于前一个场景的总结：
- en: '[PRE65]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: First, you’ll establish a `character_script_generation_chain` in your script,
    utilizing various runnables like `RunnablePassthrough` for smooth data flow. Crucially,
    this chain integrates model = `ChatOpenAI(model='gpt-3.5-turbo-16k')`, a powerful
    model with a generous 16k context window, ideal for extensive content generation
    tasks. When invoked, this chain adeptly generates character scripts, drawing on
    inputs such as character profiles, genre, and scene specifics.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将在脚本中建立一个`character_script_generation_chain`，利用各种可运行对象如`RunnablePassthrough`来实现流畅的数据流。关键的是，这个链集成了模型`ChatOpenAI(model='gpt-3.5-turbo-16k')`，这是一个具有16k上下文窗口的强大模型，非常适合广泛的内容生成任务。当被调用时，这个链能够熟练地生成角色剧本，利用输入如角色档案、类型和场景细节。
- en: You dynamically enrich each scene by adding the summary of the previous scene,
    creating a simple yet effective buffer memory. This technique ensures continuity
    and context in the narrative, enhancing the LLM’s ability to generate coherent
    character scripts.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过添加前一个场景的总结来动态丰富每个场景，创建一个简单而有效的缓冲记忆。这种技术确保了叙事的连续性和上下文，增强了LLM生成连贯角色剧本的能力。
- en: Additionally, you’ll see how the `StrOutputParser` elegantly converts model
    outputs into structured strings, making the generated content easily usable.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将看到`StrOutputParser`如何优雅地将模型输出转换为结构化字符串，使得生成的内容易于使用。
- en: Divide Labor
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分工合作
- en: Remember, designing your tasks in a sequential chain greatly benefits from the
    Divide Labor principle. Breaking tasks down into smaller, manageable chains can
    increase the overall quality of your output. Each chain in the sequential chain
    contributes its individual effort toward achieving the overarching task goal.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在设计顺序链中的任务时，从劳动分工原则中获益良多。将任务分解成更小、更易于管理的链可以提高输出的整体质量。顺序链中的每个链都为其个别努力做出贡献，以实现总体任务目标。
- en: Using chains gives you the ability to use different models. For example, using
    a smart model for the ideation and a cheap model for the generation usually gives
    optimal results. This also means you can have fine-tuned models on each step.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链可以让你使用不同的模型。例如，使用智能模型进行构思，使用廉价模型进行生成通常会产生最佳结果。这也意味着你可以在每个步骤上拥有微调的模型。
- en: Structuring LCEL Chains
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化 LCEL 链
- en: 'In LCEL you must ensure that the first part of your LCEL chain is a *runnable*
    type. The following code will throw an error:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LCEL 中，你必须确保你的 LCEL 链的第一部分是可运行的类型。以下代码将引发错误：
- en: '[PRE66]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'A Python dictionary with a value of 18 will not create a runnable LCEL chain.
    However, all of the following implementations will work:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值为 18 的 Python 字典不会创建一个可运行的 LCEL 链。然而，以下所有实现都将工作：
- en: '[PRE67]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Sequential chains are great at incrementally building generated knowledge that
    is used by future chains, but they often yield slower response times due to their
    sequential nature. As such, `SequentialChain` data pipelines are best suited for
    server-side tasks, where immediate responses are not a priority and users aren’t
    awaiting real-time feedback.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序链非常适合逐步构建未来链将使用的生成知识，但由于其顺序性质，它们通常会产生较慢的响应时间。因此，`SequentialChain` 数据管道最适合服务器端任务，在这些任务中，即时响应不是优先事项，用户也不需要等待实时反馈。
- en: Document Chains
  id: totrans-513
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档链
- en: Let’s imagine that before accepting your generated story, the local publisher
    has requested that you provide a summary based on all of the character scripts.
    This is a good use case for *document chains* because you need to provide an LLM
    with a large amount of text that wouldn’t fit within a single LLM request due
    to the context length restrictions.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，在接受你生成的故事之前，本地出版商要求你根据所有角色脚本提供摘要。这是一个很好的使用文档链的场景，因为你需要向 LLM 提供大量文本，由于上下文长度限制，这些文本不会适合单个
    LLM 请求。
- en: Before delving into the code, let’s first get a sense of the broader picture.
    The script you are going to see performs a text summarization task on a collection
    of scenes.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，让我们先了解更广泛的背景。你将要看到的脚本将对一系列场景执行文本摘要任务。
- en: Remember to install Pandas with `pip install pandas`.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 记得使用 `pip install pandas` 安装 Pandas。
- en: 'Now, let’s start with the first set of code:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从第一组代码开始：
- en: '[PRE68]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: These lines are importing all the necessary tools you need. `CharacterTextSplitter`
    and `load_summarize_chain` are from the LangChain package and will help with text
    processing, while Pandas (imported as `pd`) will help manipulate your data.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行导入了你需要的所有必要工具。`CharacterTextSplitter` 和 `load_summarize_chain` 来自 LangChain
    包，将帮助进行文本处理，而 Pandas（导入为 `pd`）将帮助你操作数据。
- en: 'Next, you’ll be dealing with your data:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将处理你的数据：
- en: '[PRE69]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Here, you create a Pandas DataFrame from the `generated_scenes` variable, effectively
    converting your raw scenes into a tabular data format that Pandas can easily manipulate.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你从 `generated_scenes` 变量创建一个 Pandas DataFrame，有效地将你的原始场景转换为 Pandas 可以轻松操作的表格数据格式。
- en: 'Then you need to consolidate your text:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要整合你的文本：
- en: '[PRE70]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this line, you’re transforming the `character_script` column from your DataFrame
    into a single text string. Each entry in the column is converted into a list item,
    and all items are joined together with new lines in between, resulting in a single
    string that contains all character scripts.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行中，你将 DataFrame 中的 `character_script` 列转换成一个单独的文本字符串。列中的每个条目都被转换成一个列表项，所有项之间用换行符连接，从而形成一个包含所有角色脚本的单一字符串。
- en: 'Once you have your text ready, you prepare it for the summarization process:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的文本准备就绪，你将其准备用于摘要过程：
- en: '[PRE71]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Here, you create a `CharacterTextSplitter` instance using its class method `from_tiktoken_encoder`,
    with specific parameters for chunk size and overlap. You then use this text splitter
    to split your consolidated script text into chunks suitable for processing by
    your summarization tool.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你使用其类方法 `from_tiktoken_encoder` 创建一个 `CharacterTextSplitter` 实例，并使用特定的块大小和重叠参数。然后，你使用这个文本分割器将你的综合脚本文本分割成适合你的摘要工具处理的块。
- en: 'Next, you set up your summarization tool:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您设置您的摘要工具：
- en: '[PRE72]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: This line is about setting up your summarization process. You’re calling a function
    that loads a summarization chain with a chat model in a `map-reduce` style approach.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行是关于设置您的摘要过程。您正在调用一个函数，该函数以 `map-reduce` 风格加载一个包含聊天模型的摘要链。
- en: 'Then you run the summarization:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您运行摘要：
- en: '[PRE73]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This is where you actually perform the text summarization. The `invoke` method
    executes the summarization on the chunks of text you prepared earlier and stores
    the summary into a variable.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是在执行文本摘要。`invoke` 方法在您之前准备好的文本块上执行摘要，并将摘要存储到一个变量中。
- en: 'Finally, you print the result:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您打印结果：
- en: '[PRE74]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This is the culmination of all your hard work. The resulting summary text is
    printed to the console for you to see.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您所有辛勤工作的结晶。生成的摘要文本将打印到控制台供您查看。
- en: 'This script takes a collection of scenes, consolidates the text, chunks it
    up, summarizes it, and then prints the summary:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本接受一系列场景，合并文本，将其分块，摘要，然后打印摘要：
- en: '[PRE75]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Output:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE76]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: It’s worth noting that even though you’ve used a `map_reduce` chain, there are
    four core chains for working with `Document` objects within LangChain.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管您已经使用了 `map_reduce` 链，但在 LangChain 中处理 `Document` 对象时，仍然有四个核心链。
- en: Stuff
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stuff
- en: The document insertion chain, also referred to as the *stuff* chain (drawing
    from the concept of *stuffing* or *filling*), is the simplest approach among various
    document chaining strategies. [Figure 4-5](#figure-4-5) illustrates the process
    of integrating multiple documents into a single LLM request.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 文档插入链，也称为 *stuff* 链（借鉴了 *stuffing* 或 *filling* 的概念），在各种文档链策略中是最简单的方法。[图 4-5](#figure-4-5)
    阐述了将多个文档整合到单个 LLM 请求中的过程。
- en: '![Stuff Documents Chain](assets/pega_0405.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![Stuff 文档链](assets/pega_0405.png)'
- en: Figure 4-5\. Stuff documents chain
  id: totrans-546
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. Stuff 文档链
- en: Refine
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精炼
- en: The refine documents chain ([Figure 4-6](#figure-4-6)) creates an LLM response
    through a cyclical process that *iteratively updates its output*. During each
    loop, it combines the current output (derived from the LLM) with the current document.
    Another LLM request is made to *update the current output*. This process continues
    until all documents have been processed.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼文档链（[图 4-6](#figure-4-6)）通过一个循环过程创建 LLM 响应，该过程会 *迭代更新其输出*。在每次循环中，它会将当前输出（来自
    LLM）与当前文档结合。然后发出另一个 LLM 请求来 *更新当前输出*。这个过程会一直持续到所有文档都被处理。
- en: '![Refine Documents Chain](assets/pega_0406.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![精炼文档链](assets/pega_0406.png)'
- en: Figure 4-6\. Refine documents chain
  id: totrans-550
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 精炼文档链
- en: Map Reduce
  id: totrans-551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map Reduce
- en: The map reduce documents chain in [Figure 4-7](#figure-4-7) starts with an LLM
    chain to each separate document (a process known as the Map step), interpreting
    the resulting output as a newly generated document.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#figure-4-7) 中的 map reduce 文档链从对每个单独文档的 LLM 链开始（这个过程被称为 Map 步骤），将生成的输出解释为一个新的文档。'
- en: Subsequently, all these newly created documents are introduced to a distinct
    combine documents chain to formulate a singular output (a process referred to
    as the Reduce step). If necessary, to ensure the new documents seamlessly fit
    into the context length, an optional compression process is used on the mapped
    documents. If required, this compression happens recursively.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，所有这些新创建的文档都被引入到一个独特的组合文档链中，以形成一个单一输出（这个过程被称为 Reduce 步骤）。如果需要，为了确保新文档能够无缝地适应上下文长度，可以在映射文档上使用可选的压缩过程。如果需要，这个过程会递归地进行。
- en: '![Map Reduce Chain](assets/pega_0407.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![Map Reduce 链](assets/pega_0407.png)'
- en: Figure 4-7\. Map reduce documents chain
  id: totrans-555
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. Map reduce 文档链
- en: Map Re-rank
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map Re-rank
- en: There is also map re-rank, which operates by executing an initial prompt on
    each document. This not only strives to fulfill a given task but also assigns
    a confidence score reflecting the certainty of its answer. The response with the
    highest confidence score is then selected and returned.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有 map 重新排序，它通过在每个文档上执行初始提示来操作。这不仅努力完成给定的任务，还分配一个反映其答案确定性的置信度分数。然后选择置信度分数最高的响应并返回。
- en: '[Table 4-1](#table-4-1) demonstrates the advantages and disadvantages for choosing
    a specific document chain strategy.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-1](#table-4-1) 展示了选择特定文档链策略的优缺点。'
- en: Table 4-1\. Overview of document chain strategies
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 文档链策略概述
- en: '| Approach | Advantages | Disadvantages |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Stuff Documents Chain | Simple to implement. Ideal for scenarios with small
    documents and few inputs. | May not be suitable for handling large documents or
    multiple inputs due to prompt size limitation. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 文档链 | 简单易实现。适用于文档数量少、输入较少的场景。| 由于提示大小限制，可能不适合处理大量文档或多个输入。|'
- en: '| Refine Documents Chain | Allows iterative refining of the response. More
    control over each step of response generation. Good for progressive extraction
    tasks. | Might not be optimal for real-time applications due to the loop process.
    |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 精细化文档链 | 允许迭代优化响应。对响应生成的每一步都有更多控制。适用于渐进式提取任务。| 由于循环过程，可能不适合实时应用。|'
- en: '| Map Reduce Documents Chain | Enables independent processing of each document.
    Can handle large datasets by reducing them into manageable chunks. | Requires
    careful management of the process. Optional compression step can add complexity
    and loses document order. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 映射/归约文档链 | 可独立处理每个文档。可以通过将数据集缩减为可管理的块来处理大型数据集。| 需要仔细管理流程。可选的压缩步骤可能增加复杂性并丢失文档顺序。|'
- en: '| Map Re-rank Documents Chain | Provides a confidence score for each answer,
    allowing for better selection of responses. | The ranking algorithm can be complex
    to implement and manage. May not provide the best answer if the scoring mechanism
    is not reliable or well-tuned. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 映射/重新排序文档链 | 为每个答案提供置信度分数，允许更好地选择响应。| 排名算法可能难以实现和管理。如果评分机制不可靠或未调优，可能不会提供最佳答案。|'
- en: You can read more about how to implement different document chains in [LangChain’s
    comprehensive API](https://oreil.ly/FQUK_) and [here](https://oreil.ly/9xr_6).
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读更多关于如何在[LangChain的全面API](https://oreil.ly/FQUK_)和[这里](https://oreil.ly/9xr_6)实现不同的文档链的信息。
- en: 'Also, it’s possible to simply change the chain type within the `load_summarize_chain`
    function:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以在`load_summarize_chain`函数中简单地更改链类型：
- en: '[PRE77]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: There are newer, more customizable approaches to creating summarization chains
    using LCEL, but for most of your needs `load_summarize_chain` provides sufficient
    results.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LCEL创建总结链有更新、更可定制的方案，但对于大多数需求来说，`load_summarize_chain`提供的功能已经足够。
- en: Summary
  id: totrans-570
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you comprehensively reviewed the LangChain framework and its
    essential components. You learned about the importance of document loaders for
    gathering data and the role of text splitters in handling large text blocks.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你全面回顾了LangChain框架及其基本组件。你了解了文档加载器在收集数据中的重要性以及文本分割器在处理大型文本块中的作用。
- en: Moreover, you were introduced to the concepts of task decomposition and prompt
    chaining. By breaking down complex problems into smaller tasks, you saw the power
    of problem isolation. Furthermore, you now grasp how prompt chaining can combine
    multiple inputs/outputs for richer idea generation.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还介绍了任务分解和提示链的概念。通过将复杂问题分解为更小的任务，你看到了问题隔离的力量。此外，你现在也明白了提示链如何结合多个输入/输出以产生更丰富的想法。
- en: In the next chapter, you’ll learn about vector databases, including how to integrate
    these with documents from LangChain, and this ability will serve a pivotal role
    in enhancing the accuracy of knowledge extraction from your data.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解向量数据库，包括如何将它们与LangChain中的文档集成，这种能力将在增强从你的数据中提取知识准确度方面发挥关键作用。
