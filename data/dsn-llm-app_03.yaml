- en: Chapter 2\. Pre-Training Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章\. 预训练数据
- en: In [Chapter 1](ch01.html#chapter_llm-introduction), we introduced language models,
    noted their strengths and limitations, explored current and potential use cases,
    and presented the scaling laws that seemingly govern progress in this field. To
    set the stage for the rest of this book, in the next three chapters we will discuss
    in detail the recipe for pre-training LLMs and the ingredients that go into them.
    But wait, this book is about utilizing pre-trained LLMs to design and build user
    applications. Why do we need to discuss the nuances of pre-training these gargantuan
    models from scratch, something most machine learning practitioners are never going
    to do in their lives?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#chapter_llm-introduction)中，我们介绍了语言模型，指出了它们的优点和局限性，探讨了当前和潜在的应用场景，并提出了似乎支配该领域进展的扩展定律。为了为本书的其余部分奠定基础，在接下来的三章中，我们将详细讨论预训练LLM的配方以及构成它们的成分。但是等等，这本书是关于利用预训练的LLM来设计和构建用户应用的。为什么我们需要讨论从头开始预训练这些巨型模型的细微差别，这对于大多数机器学习从业者来说在他们的一生中都不太可能去做？
- en: Actually, this information is very important because many of the decisions made
    during the pre-training process heavily impact downstream performance. As we will
    notice in subsequent chapters, failure modes are more easily understandable when
    you comprehend the training process. Just like we appreciate having ingredients
    listed on packages at our grocery stores, we would like to know the ingredients
    that go into making a language model before we use it in serious applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个信息非常重要，因为预训练过程中做出的许多决策会严重影响下游性能。正如我们将在后续章节中注意到的，当你理解了训练过程时，失败模式更容易理解。就像我们欣赏在杂货店的包装上列出的成分一样，在我们将语言模型用于严重应用之前，我们希望了解构成语言模型的成分。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not much information is available in the public realm about some of the proprietary
    LLMs that are accessible only through an API. This book will provide as much information
    as has been made public. While the lack of information doesn’t mean that we should
    avoid using these models, model transparency is something that you might need
    to consider while making a final decision regarding what model to use.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 关于一些只能通过API访问的专有LLM，在公共领域可用的信息不多。本书将提供尽可能多的公开信息。虽然信息不足并不意味着我们应该避免使用这些模型，但在做出最终决定选择哪个模型时，模型透明度可能是你需要考虑的因素。
- en: Ingredients of an LLM
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的成分
- en: Let’s start with the ingredients that go into making an LLM.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从构成LLM的成分开始。
- en: 'Broadly speaking, we have:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，我们有：
- en: 'Pre-training data: What’s it trained on?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练数据：它们是在什么上训练的？
- en: The old computer science adage “garbage in, garbage out” is still accurate when
    it comes to language modeling. In this chapter we will explore popular pre-training
    datasets and dig into the various preprocessing steps taken to ensure *high-quality*
    data is fed to the model. We will also showcase some tools that allow us to probe
    these datasets and understand how pre-training data composition impacts downstream
    tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言建模方面，古老的计算机科学格言“垃圾输入，垃圾输出”仍然适用。在本章中，我们将探讨流行的预训练数据集，并深入了解为确保向模型提供高质量数据而采取的各种预处理步骤。我们还将展示一些工具，这些工具允许我们探测这些数据集，并了解预训练数据组成如何影响下游任务。
- en: 'Vocabulary and tokenizer: What’s it trained over?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇和分词器：它们是在什么上训练的？
- en: To build a model over a language, we first have to determine the vocabulary
    of the language we are modeling and rules to break down a stream of text into
    the right vocabulary units, referred to as tokenization. (We will dedicate [Chapter 3](ch03.html#chapter-LLM-tokenization)
    to discussing these concepts.) Linguistically, humans process language in terms
    of meaning-bearing words and sentences. Language models process language in terms
    of tokens. We will explore the downstream impact when there is a mismatch between
    the two.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要在一种语言上构建模型，我们首先必须确定我们正在建模的语言的词汇，以及将文本流分解成正确的词汇单元（称为分词）的规则。（我们将把[第3章](ch03.html#chapter-LLM-tokenization)专门用于讨论这些概念。）从语言学的角度来看，人类从意义承载的单词和句子处理语言。语言模型从标记处理语言。我们将探讨当两者之间存在不匹配时的下游影响。
- en: 'Learning objective: What is it being trained to do?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学习目标：它被训练去做什么？
- en: By pre-training a language model, we aim to imbue the language model with general
    skills in syntax, semantics, reasoning, and so on, that hopefully will enable
    it to reliably solve any task you throw at it, even if it was not specifically
    trained on the task. Therefore the training objectives should be sufficiently
    general to capture all these skills. In [Chapter 4](ch04.html#chapter_transformer-architecture),
    we will discuss the various tasks (learning objectives) that pre-trained models
    are trained on. You might wonder if LLMs are better suited to solving downstream
    tasks that are similar to the tasks the pre-trained model has been trained to
    solve. We will test this assumption and discuss the impact various learning objectives
    have on task performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过预训练语言模型，我们旨在赋予语言模型在语法、语义、推理等方面的通用技能，希望它能可靠地解决你抛给它的任何任务，即使它没有专门针对该任务进行训练。因此，训练目标应该足够通用，以捕捉所有这些技能。在第
    [4 章](ch04.html#chapter_transformer-architecture) 中，我们将讨论预训练模型所训练的各种任务（学习目标）。你可能想知道
    LLM 是否更适合解决与预训练模型训练任务相似的下游任务。我们将测试这个假设，并讨论各种学习目标对任务性能的影响。
- en: 'Architecture: What’s its internal structure?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 架构：它的内部结构是什么？
- en: The architecture of a model refers to the components of a model, how they connect
    and interact with each other, and how they process input. Each architecture has
    its own inductive bias, a set of assumptions made about the data and tasks it
    will be used for, biasing the model toward certain types of solutions. In [Chapter 4](ch04.html#chapter_transformer-architecture),
    we will conduct a deep dive into the Transformer architecture, which, as discussed
    in [Chapter 1](ch01.html#chapter_llm-introduction), is the predominantly used
    architecture currently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的架构指的是模型的组件，它们如何连接和相互作用，以及它们如何处理输入。每种架构都有自己的归纳偏差，这是一组关于将要用于数据和任务的假设，使模型偏向于某些类型的解决方案。在第
    [4 章](ch04.html#chapter_transformer-architecture) 中，我们将深入探讨 Transformer 架构，正如在第
    [1 章](ch01.html#chapter_llm-introduction) 中讨论的那样，这是目前主要使用的架构。
- en: Let’s look at how these ingredients fit together in [Figure 2-1](#ingredients-of-llm).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些成分如何在 [图 2-1](#ingredients-of-llm) 中组合在一起。
- en: '![LLM Ingredients](assets/dllm_0201.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![LLM 成分](assets/dllm_0201.png)'
- en: Figure 2-1\. How all the ingredients come together to make an LLM
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 所有成分如何结合在一起形成一个 LLM
- en: 'The language models trained using the process described in this chapter and
    the next are called *base models*. Lately, model providers have been augmenting
    the base model by fine-tuning it on much smaller datasets to steer them toward
    being more aligned with human needs and preferences. Some popular tuning modes
    are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章和下一章中描述的过程训练的语言模型被称为*基础模型*。最近，模型提供商通过在更小的数据集上微调基础模型来增强它，使其更符合人类的需求和偏好。一些流行的微调模式包括：
- en: Supervised instruction fine-tuning (SFT), so that the model is better at following
    human instructions
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督指令微调（SFT），使模型更好地遵循人类指令
- en: Reinforcement learning by human feedback (RLHF), so that the model is better
    aligned with human preferences
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过人类反馈进行强化学习（RLHF），使模型更好地与人类偏好对齐
- en: Domain-adaptive or task-adaptive continued pre-training, so that the model is
    better attuned to specific domains and tasks
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域自适应或任务自适应的持续预训练，使模型更好地适应特定领域和任务
- en: Based on the specific augmentation carried out, the resulting models are called
    *instruct models*, *chat models*, and so on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所进行的特定增强，得到的模型被称为*指令模型*、*聊天模型*等等。
- en: We will cover instruct and chat models in [Chapter 6](ch06.html#llm-fine-tuning),
    and domain-adaptive and task-adaptive pre-training in [Chapter 7](ch07.html#ch07).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 [6 章](ch06.html#llm-fine-tuning) 中介绍指令和聊天模型，在第 [7 章](ch07.html#ch07) 中介绍领域自适应和任务自适应的预训练。
- en: '![Derivative Models](assets/dllm_0202.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![衍生模型](assets/dllm_0202.png)'
- en: Figure 2-2\. The relationship between base models and their derivatives
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 基础模型及其衍生模型之间的关系
- en: Pre-Training Data Requirements
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练数据要求
- en: Although it has been shown that higher-capacity models are relatively more [sample
    efficient](https://oreil.ly/PbN6F), in general today’s language models are very
    sample inefficient, meaning they need tons of examples to learn a task. It is
    infeasible to create such a large supervised dataset with human annotations, hence
    the predominant means to pre-train language models is using *self-supervised*
    learning, where the target labels exist within your training inputs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经证明，高容量模型相对更 [样本高效](https://oreil.ly/PbN6F)，但总的来说，今天的语言模型非常样本低效，这意味着它们需要大量的例子来学习一个任务。创建如此大的带有人类标注的监督数据集是不切实际的，因此，预训练语言模型的主要手段是使用
    *自监督* 学习，其中目标标签存在于你的训练输入中。
- en: Using this setup, virtually any type of text is fair game to be included in
    a pre-training dataset, and theoretically any nontextual signal with some structure
    can be encoded in text and included as part of a pre-training dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种设置，几乎任何类型的文本都可以被包含在预训练数据集中，从理论上讲，任何具有一些结构的非文本信号都可以编码成文本，并作为预训练数据集的一部分。
- en: From our scaling laws discussion in [Chapter 1](ch01.html#chapter_llm-introduction),
    we know that model performance increases by just training them longer and on more
    data. Also, as discussed in [Chapter 1](ch01.html#chapter_llm-introduction), the
    *consolidation effect* at play in the field raises expectations on what a single
    language model is expected to do end-to-end. Today a single model is expected
    to answer factual questions about the world, employ arithmetic and logical reasoning,
    write code, and come up with creative ideas.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在第 1 章[“缩放定律讨论”](ch01.html#chapter_llm-introduction)中讨论的内容，我们知道模型性能通过仅仅训练更长的时间和更多的数据就能提高。此外，正如在第
    1 章[“语言模型介绍”](ch01.html#chapter_llm-introduction)中讨论的那样，该领域的 *巩固效应* 提高了人们对单个语言模型端到端期望的期望。今天，一个模型被期望能够回答关于世界的事实性问题，运用算术和逻辑推理，编写代码，并提出创新的想法。
- en: All this means that the data needs for language model pre-training are enormous.
    Now, the key question is whether textual data available in the world actually
    contains sufficient and relevant signals needed to learn all the skills we want
    LLMs to learn.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都意味着语言模型预训练所需的数据量是巨大的。现在，关键问题是世界上可用的文本数据是否实际上包含足够且相关的信号，这些信号是我们希望 LLMs 学习所有技能所需的。
- en: Note that language models that are trained solely on text only have access to
    the linguistic form, i.e., the sequence of characters making up a sentence like,
    “Walter White tossed the pizza onto the roof.” To understand its meaning, the
    linguistic form has to be mapped to the communicative intent of the writer/speaker.
    While a [section](https://oreil.ly/3iYA2) of the research community argues that
    one cannot learn meaning from form alone, recent language models are increasingly
    proving otherwise.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，仅基于文本训练的语言模型只能访问语言形式，即构成句子如“Walter White 把披萨扔到了屋顶上。”的字符序列。要理解其含义，语言形式必须映射到作者/说话者的沟通意图。虽然研究界的一个
    [部分](https://oreil.ly/3iYA2) 认为不能仅从形式中学习意义，但最近的语言模型越来越多地证明并非如此。
- en: 'To have access to the full picture, the linguistic form needs to be grounded
    to the real world. In the cognitive sciences, grounding is defined as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得完整的图景，语言形式需要与现实世界联系起来。在认知科学中，grounding 被定义为：
- en: The process of establishing what mutual information is required for successful
    communication between two interlocutors
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 建立两个对话者之间成功沟通所需的互信息量的过程
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chandu et al., [“Grounding ‘grounding’ in NLP”](https://oreil.ly/kPyXu)
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chandu 等人，[“在自然语言处理中‘grounding’的定位”](https://oreil.ly/kPyXu)
- en: Human text is generally very underspecified, with a lot of communicative intent
    existing outside the textual context, depending on the reader/listener to use
    their common sense, world knowledge, and ability to detect and understand emotional
    subtext to interpret it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 人类文本通常非常不具体，大量的沟通意图存在于文本之外，依赖于读者/听众使用他们的常识、世界知识和检测、理解情感隐含意义的能力来解释它。
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is estimated that only around [12% of information](https://oreil.ly/jg4tW)
    we understand from text is explicitly mentioned in text. There are several theories
    explaining why we communicate thus, including [Zipf’s principle of least effort](https://oreil.ly/UX7Nd),
    which states it is “human nature to want the greatest outcome at the least amount
    of work.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 据估计，我们从文本中理解的信息中只有大约 [12%](https://oreil.ly/jg4tW) 是在文本中明确提到的。有几种理论解释了为什么我们会这样沟通，包括
    [Zipf 的最小努力原则](https://oreil.ly/UX7Nd)，它表明“人类的天性是在最少的努力下获得最大的成果。”
- en: The field of NLP has seen [a lot of work](https://oreil.ly/PbIhT) in grounding
    language models to the real world. [Multimodal models](https://oreil.ly/ysAeM)
    that combine different modalities like image, video, speech, and text are a promising
    avenue of research, and they are likely to see more widespread usage in the coming
    years. Imagine a model seeing “pizza” in the training text, but also getting signals
    on how it looks, how it sounds, and how it tastes!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域已经看到了[大量工作](https://oreil.ly/PbIhT)将语言模型与现实世界联系起来。结合不同模态（如图像、视频、语音和文本）的多模态模型是一个有希望的研究方向，并且它们在未来几年可能会得到更广泛的应用。想象一下，一个模型在训练文本中看到了“披萨”，同时也获得了关于它的外观、声音和味道的信号！
- en: But do multimodal models really help with the grounding problem? Can we instead
    achieve the effect of grounding by just feeding the model with massive amounts
    of diverse text? These are unsolved questions, and there are good arguments in
    both directions as shown by this [debate](https://oreil.ly/oacht).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但多模态模型真的有助于解决基础问题吗？我们能否仅仅通过向模型提供大量多样化的文本来实现基础的效果？这些问题尚未解决，正如这个[辩论](https://oreil.ly/oacht)所示，双方都有很好的论据。
- en: Whether training on massive amounts of text alone can enable language models
    to learn skills like logical reasoning is another open question. Note that text
    on the internet contains a lot of text describing reasoning steps, like theorem
    proofs, explanations of jokes, step-by-step answers to puzzles, and so on. However,
    there is simply not enough of derivational text going around, which leads us to
    cover the shortfall by using prompting methods like CoT (described further in
    [Chapter 5](ch05.html#chapter_utilizing_llms)). There is [recent evidence](https://oreil.ly/Qlntp)
    that process supervision, where feedback is provided for each step of the problem-solving
    process, as opposed to outcome supervision, where feedback is provided only on
    the final solution, helps improve arithmetic reasoning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单独在大量文本上进行训练是否能够使语言模型学习到诸如逻辑推理等技能，这也是一个悬而未决的问题。请注意，互联网上的文本包含大量描述推理步骤的内容，如定理证明、笑话解释、拼图逐步解答等。然而，推导性文本的量显然不足，这导致我们通过使用如CoT（在第5章中进一步描述）等提示方法来弥补这一不足。有[最新证据](https://oreil.ly/Qlntp)表明，过程监督，即对问题解决过程中的每一步提供反馈，与结果监督（仅在最终解决方案上提供反馈）相比，有助于提高算术推理能力。
- en: A crucial skill that language models have to learn is dealing with the inherently
    ambiguous nature of language. Following up on the aforementioned Zipf’s principle
    of least effort, ambiguity enables speakers to manage the efficiency-clarity tradeoff
    in communication. We can leave a lot unsaid because we have established sufficient
    common ground with the people we are communicating with and trust that they are
    able to fill in the gaps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型必须学习的一项关键技能是处理语言固有的歧义性。继上述的Zipf最小努力原则之后，歧义使得说话者能够在沟通中的效率和清晰度之间进行权衡。我们可以留下很多未说，因为我们已经与沟通对象建立了足够的共同基础，并相信他们能够填补空白。
- en: 'Earlier language models struggled a lot with modeling ambiguity. I long used
    this sentence as a canonical example in my NLP talks to highlight ambiguity in
    language: “WWE’s John Cena surprises Make-A-Wish 7-year-old with cancer.”'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的语言模型在建模歧义方面遇到了很多困难。我长期以来一直将这句话作为NLP演讲中的典范例子，以突出语言的歧义性：“WWE的约翰·塞纳出人意料地让患有癌症的7岁Make-A-Wish儿童感到惊喜。”
- en: While state-of-the-art models are able to correctly interpret this particular
    sentence and not mistakenly identify John Cena as an evil disease-spreading wizard,
    [recent work](https://oreil.ly/BrSwb) shows that even the best models of today
    still struggle to deal with ambiguity in general. Whether just scaling up models
    and data is enough for LLMs to model ambiguity is an open question.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最先进的模型能够正确解读这个特定的句子，并且不会错误地将约翰·塞纳（John Cena）识别为邪恶的疾病传播巫师，但[最近的研究](https://oreil.ly/BrSwb)显示，即使是今天最好的模型在处理一般性的歧义时仍然存在困难。是否仅仅通过扩大模型和数据规模就足以让大型语言模型（LLMs）建模歧义，这是一个悬而未决的问题。
- en: If our only option to resolve all these shortcomings is to scale up dataset
    sizes, the next question is if we actually have enough data available in the world
    that is sufficient for LLMs to learn these skills. Are we at risk of running out
    of training data anytime soon? There is a misconception in certain quarters of
    our field that we already have. However, lack of raw data is not yet a bottleneck
    in training models. For instance, there are billions of publicly available documents
    accessible by scraping or via a free API that haven’t yet made it into most pre-training
    data sets, such as parliamentary proceedings, court judgments, and most SEC filings.
    [“How much LLM training data is there, in the limit?”](https://oreil.ly/XnmHL)
    by Educating Silicon estimates the amount of text present in the world. On the
    other hand, it is true that at a sufficiently large scale, there is simply not
    enough naturally occurring data to feed our models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们解决所有这些缺陷的唯一选择是扩大数据集的大小，那么接下来的问题是，我们实际上是否拥有足够的数据来让LLMs学习这些技能。我们是否很快就会面临训练数据不足的风险？在我们领域的某些方面存在一种误解，即我们已经拥有了足够的数据。然而，缺乏原始数据还不是训练模型的瓶颈。例如，有数十亿可以通过抓取或通过免费API访问的公开文档，但尚未纳入大多数预训练数据集，如议会程序、法院判决和大多数SEC文件。[“LLM训练数据在极限情况下有多少？”](https://oreil.ly/XnmHL)由Educating
    Silicon估计了世界上存在的文本量。另一方面，确实，在足够大的规模上，自然发生的数据根本不足以喂养我们的模型。
- en: Thus, there are efforts to use text generated by language models, termed *synthetic
    data*, to train models, albeit with the [risk](https://oreil.ly/RdzX0) that training
    on LLM-generated data can potentially be detrimental, as the model deviates from
    the true distribution of the data. Later in this chapter, we will learn the process
    behind creating synthetic data for pre-training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有人试图使用语言模型生成的文本，称为*合成数据*，来训练模型，尽管存在[风险](https://oreil.ly/RdzX0)，即基于LLM生成的数据进行训练可能会对模型产生潜在的负面影响，因为模型偏离了数据的真实分布。在本章后面，我们将了解创建用于预训练的合成数据的过程。
- en: Of course, not all data is created equal. We can achieve more sample efficiency
    with high-quality data, thus needing smaller dataset sizes. We can preprocess
    data in order to filter out low-quality data or make them higher quality. What
    exactly makes data high quality is a nuanced question, which we will explore later
    in the chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非所有数据都是平等的。我们可以通过高质量的数据实现更高的样本效率，从而需要更小的数据集大小。我们可以预处理数据，以过滤掉低质量数据或提高它们的质量。究竟是什么使数据成为高质量数据是一个复杂的问题，我们将在本章后面探讨。
- en: Popular Pre-Training Datasets
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行预训练数据集
- en: A lot of text is not freely available in public. This includes data exposed
    behind paywalled APIs and login screens, and paywalled books and documents, many
    of which may not even be digitized. Larger companies like Google and OpenAI can
    afford to purchase this data; for example, OpenAI has [struck deals](https://oreil.ly/ygIO2)
    worth hundreds of millions of dollars with the *Wall Street Journal*, *Financial
    Times*, and other news organizations for access to their data. Domain-specific
    text is often proprietary and available only to large incumbents (for example,
    Bloomberg trained [BloombergGPT](https://oreil.ly/87r4j) partly on its proprietary
    financial data). However, even for models trained by the largest companies, a
    significant proportion of training data comes from publicly available data sources.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 许多文本在公共领域是不可自由获取的。这包括隐藏在付费API和登录屏幕背后的数据，以及付费书籍和文档，其中许多甚至尚未数字化。像Google和OpenAI这样的大公司可以负担得起购买这些数据；例如，OpenAI已经与《华尔街日报》、《金融时报》和其他新闻机构达成了价值数亿美元的交易，以获取他们的数据。特定领域的文本通常是专有的，并且仅对大型现有企业（例如，Bloomberg在训练[BloombergGPT](https://oreil.ly/87r4j)时部分使用了其专有的金融数据）。然而，即使是最大公司训练的模型，其训练数据中也有相当一部分来自公开数据源。
- en: Next, we will cover some of the most popular general-purpose pre-training datasets
    that are being used to train LLMs. While this is not a comprehensive list, most
    LLMs, including closed-source ones, have at least a large subset of their training
    data drawn from these sources. We will defer discussion of domain-specific (catered
    to a particular field like social media, finance, biomedical, etc.) datasets to
    [Chapter 7](ch07.html#ch07).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一些最流行的通用预训练数据集，这些数据集被用来训练LLMs。虽然这不是一个详尽无遗的列表，但大多数LLMs，包括闭源LLMs，至少有它们训练数据的大多数子集来自这些来源。我们将把特定领域（针对特定领域，如社交媒体、金融、生物医学等）数据集的讨论推迟到[第7章](ch07.html#ch07)。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Most general-purpose LLMs are trained to be a jack-of-all-trades—to be able
    to solve tasks from a variety of domains. If the data domain for your use case
    is included in the pre-training dataset, models trained on those datasets may
    show relative performance improvements on downstream tasks compared to models
    that aren’t, even if the pre-training data is unlabeled. This means that if you
    intend to use LLMs for specific, well-defined use cases in a particular domain,
    domain-specific models could prove promising. You can also perform *continued
    domain-adaptive* or *task-adaptive pre-training* on your domain data to leverage
    this phenomenon. This will be discussed in detail in [Chapter 7](ch07.html#ch07).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数通用LLM都被训练成多面手——能够解决来自各个领域的任务。如果你的用例数据域包含在预训练数据集中，与未包含这些数据集的模型相比，在这些数据集上训练的模型在下游任务上的相对性能可能有所提高，即使预训练数据是无标签的。这意味着，如果你打算在特定领域使用LLM进行特定、明确的用例，领域特定模型可能会证明是有希望的。你还可以在你的领域数据上执行*持续领域自适应*或*任务自适应预训练*，以利用这一现象。这将在第7章（ch07.html#ch07）中详细讨论。
- en: 'Here are some examples of commonly used data sources for general-purpose language
    models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些通用语言模型常用的数据源示例：
- en: Common Crawl/C4
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl/C4
- en: The web is the largest source of openly available textual data, and hence forms
    a significant proportion of pre-training datasets. [Common Crawl](https://oreil.ly/dhBvu)
    is a nonprofit that creates and publishes snapshots of all web crawl data, updated
    every month. However, as one could imagine, this is an extremely coarse dataset
    and needs to be significantly cleaned before it is ready to use. Google prepared
    C4 (Colossal Clean Crawled Corpus), a 750GB English-language dataset, after applying
    a set of preprocessing and filtering steps to a Common Crawl snapshot from 2019
    and released the code for it. [Dodge et al.](https://oreil.ly/bxmVR) used this
    script to reproduce C4 and have made it publicly available. C4 has been used for
    training several well-known LLMs including all models from the T5 family.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网是公开可获取的文本数据最大的来源，因此构成了预训练数据集的一个重要比例。[Common Crawl](https://oreil.ly/dhBvu)
    是一个非营利组织，它创建并发布所有网络爬取数据的快照，每月更新一次。然而，正如人们可以想象的那样，这是一个极其粗略的数据集，在使用之前需要显著地进行清理。谷歌在将2019年Common
    Crawl快照应用一系列预处理和过滤步骤后，准备了C4（Colossal Clean Crawled Corpus），一个750GB的英语语言数据集，并发布了相应的代码。[Dodge等人](https://oreil.ly/bxmVR)
    使用这个脚本重新生成了C4，并将其公开。C4已被用于训练包括T5系列中所有模型在内的几个知名LLM。
- en: The Pile
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: The Pile
- en: '[The Pile](https://oreil.ly/7UAcY) is a 825GB dataset from Eleuther AI, which
    focused on publishing a dataset drawn from more diverse sources. Diversity of
    data is important since in-domain unlabeled data in pre-training is helpful for
    downstream performance on that domain, and diverse data sets also enable generalization
    to previously unseen tasks and domains. To this end, the data from The Pile comes
    not only from Common Crawl but also PubMed Central, arXiv, GitHub, the FreeLaw
    Project, Stack Exchange, the US Patent and Trademark Office, Ubuntu IRC, HackerNews,
    YouTube, PhilPapers, NIH ExPorter, Project Gutenberg, and Wikipedia, among others.
    The Pile and its subsets have been preferred as a data source for training several
    LLMs, including [Llama](https://oreil.ly/_8eOD).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[The Pile](https://oreil.ly/7UAcY) 是来自Eleuther AI的825GB数据集，它专注于从更多样化的来源发布数据集。数据的多样性很重要，因为预训练中的领域内无标签数据有助于该领域的下游性能，多样化的数据集也使得模型能够泛化到之前未见过的任务和领域。为此，The
    Pile的数据不仅来自Common Crawl，还包括PubMed Central、arXiv、GitHub、FreeLaw项目、Stack Exchange、美国专利商标局、Ubuntu
    IRC、HackerNews、YouTube、PhilPapers、NIH ExPorter、Project Gutenberg和维基百科等。The Pile及其子集已被选为训练几个LLM的数据源，包括[Llama](https://oreil.ly/_8eOD)。'
- en: WebText/OpenWebText/OpenWebText2
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: WebText/OpenWebText/OpenWebText2
- en: These refer to a subset of web text and are limited to web pages representing
    outbound links on Reddit that have at least three *karma*, the absolute difference
    between user upvotes and downvotes. The assumption is that the wisdom of the crowd
    will enable only high-quality links to surface, which contain information people
    actually find interesting. Models that have been trained on this data include
    GPT-2 and GPT-3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指的是网络文本的一个子集，仅限于Reddit上代表至少有三个*karma*（用户点赞和踩的绝对差值）的外链网页。假设是群众的智慧将只允许高质量链接浮现，这些链接包含人们实际上感兴趣的信息。在训练了这些数据的模型包括GPT-2和GPT-3。
- en: Wikipedia
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科
- en: Wikipedia assumes a major role in the training of just about every general-purpose
    LLM. A full dump of Wikipedia contains valuable encyclopedic text that provides
    factual knowledge to the model. Wikipedia’s editorial system ensures that the
    text follows a highly structured format. However, it is not diverse stylistically,
    as the text is written in a formal manner. Therefore, Wikipedia alone is not sufficient
    to train a rudimentary language model and needs to be combined with data sources
    comprising diverse writing styles.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科在几乎所有通用型LLM的训练中都扮演着重要角色。维基百科的完整存档包含了为模型提供事实知识的宝贵百科全书文本。维基百科的编辑系统确保文本遵循高度结构化的格式。然而，从风格上讲，文本是正式的，因此维基百科本身不足以训练一个基本的语言模型，需要与包含不同写作风格的数据源相结合。
- en: BooksCorpus/BooksCorpus2
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BooksCorpus/BooksCorpus2
- en: Probably the most historically influential of all pre-training datasets, this
    dataset was part of the training corpus for well-known models like BERT, RoBERTa,
    GPT-2/3, etc. The BooksCorpus contains over 7,000 free, mostly fiction books written
    by unpublished authors. Twenty-six percent of books in the original dataset belonged
    to the Romance genre. A replication of the BooksCorpus is present in The Pile
    as BooksCorpus2.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是所有预训练数据集中历史影响力最大的一个，这个数据集是BERT、RoBERTa、GPT-2/3等知名模型的训练语料库的一部分。BooksCorpus包含超过7,000本由未发表作者撰写的免费、主要是小说书籍。原始数据集中26%的书籍属于浪漫小说类别。BooksCorpus的副本在The
    Pile中以BooksCorpus2的形式存在。
- en: FineWeb
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: FineWeb
- en: As of the book’s writing, [FineWeb](https://oreil.ly/1GyZd) is the world’s largest
    publicly available pre-training dataset. Published by Hugging Face, FineWeb has
    15 trillion tokens and is drawn from 96 snapshots of Common Crawl, after a rigorous
    cleaning and filtering process. Hugging Face also released [FineWeb-Edu](https://oreil.ly/8XHH-),
    a subset of FineWeb composed of educational data, which is crucial in enabling
    LLMs to pass standardized tests and popular benchmarks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到本书写作时，[FineWeb](https://oreil.ly/1GyZd)是世界上最大的公开可用的预训练数据集。由Hugging Face发布，FineWeb包含1.5万亿个标记，并从经过严格清洗和过滤的96个Common
    Crawl快照中提取而来。Hugging Face还发布了[FineWeb-Edu](https://oreil.ly/8XHH-)，这是FineWeb的一个子集，由教育数据组成，这对于LLM通过标准化测试和流行基准至关重要。
- en: '[Table 2-1](#popular-pretraining-datasets) provides a list of some of the most
    commonly used datasets, their size, year of release, and the means to access them.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-1](#popular-pretraining-datasets)列出了一些最常用的数据集，包括它们的大小、发布年份和访问方式。'
- en: Table 2-1\. Popular pretraining datasets
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 常用预训练数据集
- en: '| Name | Data source(s) | Size | Year released | Public? | Models using this
    dataset |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Name | 数据来源 | 大小 | 发布年份 | 公开？ | 使用此数据集的模型 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| C4 | Common Crawl | 750GB | 2019 | Yes (reproduced version) | T5, FLAN-T5,
    UL2, Llama, etc. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| C4 | Common Crawl | 750GB | 2019 | 是（复制品） | T5, FLAN-T5, UL2, Llama, etc.
    |'
- en: '| The Pile | Common Crawl, PubMed Central, Wikipedia, arXiv, Project Gutenberg,
    Stack Exchange, USPTO, GitHub, etc. | 825GB | 2020 | Yes | GPT-NeoX, GPT-J, Cerebras-GPT,
    StableLM, Pythia, etc. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| The Pile | Common Crawl, PubMed Central, Wikipedia, arXiv, Project Gutenberg,
    Stack Exchange, USPTO, GitHub, etc. | 825GB | 2020 | 是 | GPT-NeoX, GPT-J, Cerebras-GPT,
    StableLM, Pythia, etc. |'
- en: '| RedPajama | Common Crawl, GitHub, Wikipedia, arXiv, Stack Exchange, etc.
    | 1.2T tokens | 2023 | Yes | Red Pajama-INCITE, MPT |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| RedPajama | Common Crawl, GitHub, Wikipedia, arXiv, Stack Exchange, etc.
    | 1.2T tokens | 2023 | 是 | Red Pajama-INCITE, MPT |'
- en: '| BooksCorpus | Sampled from smashwords.com | 74M sentences | 2015 | Original
    not available anymore | Most models including BERT, GPT, etc. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| BooksCorpus | 从smashwords.com采样 | 74M sentences | 2015 | 原始数据不再可用 | 包括BERT、GPT等在内的多数模型
    |'
- en: '| OpenWebText2 | Outbound Reddit links | 65GB | 2020 | Yes | GPT-2, GPT-3 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| OpenWebText2 | 来自Reddit的外链 | 65GB | 2020 | 是 | GPT-2, GPT-3 |'
- en: '| ROOTS | Big Science Catalogue, Common Crawl, GitHub | 1.6T tokens | 2022
    | No (but available on request) | BLOOM |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ROOTS | 大科学目录，Common Crawl，GitHub | 1.6T tokens | 2022 | 否（但可请求获得） | BLOOM
    |'
- en: '| RefinedWeb | Common Crawl | 5T tokens | 2023 | Yes (600B subset only) | Falcon
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| RefinedWeb | Common Crawl | 5T tokens | 2023 | 是（仅600B子集） | Falcon |'
- en: '| SlimPajama | Cleaned from RedPajama | 627B tokens | 2023 | Yes | N/A |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| SlimPajama | 从RedPajama清洗而来 | 627B tokens | 2023 | 是 | N/A |'
- en: The table highlights the fact that most models are trained on similar data sources.
    In this chapter, we are limiting our coverage to pre-training datasets for base
    models. We will cover datasets used to augment base models like instruction tuning
    datasets, RLHF datasets, etc. in [Chapter 6](ch06.html#llm-fine-tuning).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表格突出了这样一个事实：大多数模型都是在类似的数据源上训练的。在本章中，我们将限制我们的覆盖范围到基础模型的预训练数据集。我们将在[第6章](ch06.html#llm-fine-tuning)中介绍用于增强基础模型的数据集，如指令微调数据集、RLHF数据集等。
- en: 'Let’s explore the content of these pre-training datasets. Using a Google Colab
    notebook or a code editor of your choice, load the `realnewslike` subset of the
    C4 dataset, which consumes around 15 GB:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些预训练数据集的内容。使用Google Colab笔记本或您选择的代码编辑器，加载C4数据集的`realnewslike`子集，该子集大约消耗15
    GB：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this code, we can observe all the instances in which Iceland appears in
    this C4 subset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，我们可以观察到冰岛在这个C4子集中出现的所有实例。
- en: Synthetic Pre-Training Data
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合成预训练数据
- en: An emerging trend is the use of LLMs to generate synthetic data that can be
    used for pre-training LLMs. One of the first success stories in training LLMs
    on datasets with a significant proportion of synthetic data is Microsoft’s [phi
    series of models](https://oreil.ly/eFphR). For the phi-1.5 model, Microsoft created
    20 billion tokens of synthetic data, using 20,000 seed topics and samples from
    real-world web datasets in their prompts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新兴趋势是使用大型语言模型（LLM）生成可用于预训练LLM的合成数据。在数据集中包含大量合成数据的LLM训练中的第一个成功案例之一是微软的[phi系列模型](https://oreil.ly/eFphR)。对于phi-1.5模型，微软创建了200亿个合成数据标记，使用20,000个种子主题和来自现实世界网络数据集的样本作为提示。
- en: Hugging Face released [Cosmopedia](https://oreil.ly/Pdwnw), an open source synthetic
    dataset used to train the SmolLM series of models. Its seed data included curated
    sources like Stanford courses, Khan Academy, and WikiHow, as well as general web
    data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face发布了[Cosmopedia](https://oreil.ly/Pdwnw)，这是一个开源的合成数据集，用于训练SmolLM系列模型。其种子数据包括经过精选的资源，如斯坦福课程、可汗学院和WikiHow，以及通用网络数据。
- en: For curated sources, synthetic data was generated by extracting outlines of
    courses from Khan Academy and other sources and prompting the Mistral LLM to generate
    lengthy, detailed textbooks for individual sections. To generate diverse data
    at scale, Hugging Face issues several variants of the same prompt for each topic,
    like “create a textbook on this topic for young children” and “create a textbook
    on this topic for professionals.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于精选资源，通过从可汗学院和其他来源提取课程大纲并提示Mistral LLM为单个部分生成详细的长篇教科书来生成合成数据。为了大规模生成多样化的数据，Hugging
    Face为每个主题发布了几个相同的提示变体，例如“为儿童创建这个主题的教科书”和“为专业人士创建这个主题的教科书”。
- en: For general web data, Hugging Face clustered a subset of the RefinedWeb dataset
    into over a hundred topics. The LLM was then prompted with web page snippets and
    asked to generate an extensive blog post within the context of the topic the web
    page fell under. The cluster visualization can be explored in [Nomic Atlas](https://oreil.ly/t8R-6).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通用网络数据，Hugging Face将RefinedWeb数据集的一个子集聚类到超过一百个主题中。然后，LLM被提示使用网页片段，并要求在网页所属的主题背景下生成一篇广泛的博客文章。聚类可视化可以在[Nomic
    Atlas](https://oreil.ly/t8R-6)中探索。
- en: Training Data Preprocessing
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据预处理
- en: Once we have collected or procured data, we need to filter and clean the data
    by running it through a preprocessing pipeline. Data preprocessing is the most
    unglamorous and underappreciated part of the LLM training pipeline, yet perhaps
    the most important. Based on my experience, spending more effort and resources
    during this phase can lead to significant downstream performance gains. As we
    walk through the data processing pipeline, I hope you come to appreciate the complexity
    of language text and the difficulty in processing it. Note that since these datasets
    are enormous, any preprocessing step should also be very efficient (ideally linear
    time).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收集或获取了数据，我们需要通过运行预处理管道来过滤和清理数据。数据预处理是LLM训练流程中最不引人注目且最不被重视的部分，但也许是最重要的。根据我的经验，在这一阶段投入更多努力和资源可以带来显著的下游性能提升。随着我们走过数据处理管道，我希望你能体会到语言文本的复杂性和处理它的难度。请注意，由于这些数据集非常庞大，任何预处理步骤都应该非常高效（理想情况下为线性时间）。
- en: '[Figure 2-3](#data-collection) shows the typical preprocessing steps used to
    generate a pre-training dataset. The ordering of steps is not fixed, but there
    are dependencies between some of the steps.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-3](#data-collection)展示了用于生成预训练数据集的典型预处理步骤。步骤的顺序不是固定的，但某些步骤之间存在依赖关系。'
- en: '![Data preprocessing pipeline](assets/dllm_0203.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![数据预处理流程](assets/dllm_0203.png)'
- en: Figure 2-3\. Data collection and preprocessing pipeline
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 数据收集和预处理流程
- en: Let’s go through these steps in detail.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细地过一遍这些步骤。
- en: Data Filtering and Cleaning
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据过滤和清理
- en: 'A majority of text extracted from HTML files is gibberish, like menu text from
    websites, boilerplate text, and random web page artifacts. There is a significant
    amount of pornography and toxic/hateful language on the web as well. For example,
    here is a text sample from an uncleaned version of the C4 dataset:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从 HTML 文件中提取的大多数文本都是无意义的，比如来自网站的菜单文本、模板文本和随机的网页碎片。互联网上还有大量的色情和有害/仇恨性语言。例如，以下是从
    C4 数据集未清洗版本中提取的文本样本：
- en: Skip to Main Content Skip to Footer Skip to Email Signup Skip to Feedback Form
    MY REWARDS SIGN OUT SIGN IN & EARN REWARDS 0 Keyboard Controls Welcome to the
    main navigation. This menu has three levels of product categories. Use and keys
    to navigate between each category in the current level. Use the key to navigate
    down a level. Use the key to navigate up a level. Hit the key to be taken to the
    selected category page. Men What’s Hot New Arrivals Brand That Unites Performance
    Shop Online Exclusives Express Essentials Vacation Getaway Wedding Tuxedos Military
    Trend 9 Pieces / 33 Looks The Edit x Express NBA Collection Express + NBA Fashion
    NBA Game Changers Suiting & Blazers Find
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 跳转到主要内容 跳转到页脚 跳转到电子邮件注册 跳转到反馈表单 我的奖励 登出 登录并赚取奖励 0 键盘控制 欢迎来到主导航。此菜单有三个级别的产品类别。使用和键在当前级别的每个类别之间导航。使用键向下导航一个级别。使用键向上导航一个级别。按键可转到所选类别页面。菜单
    热门新品 新品上市 品牌联合性能在线独家快讯必备度假婚礼礼服军装趋势 9 件/33 款造型 The Edit x Express NBA Collection
    Express + NBA 时尚 NBA 改变游戏规则 西装夹克
- en: How useful do you think this text is for language and task learning?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为这篇文本对语言和任务学习有多大的帮助？
- en: Data from Common Crawl is made available via both raw HTML and web-extracted
    text (WET) format. While many dataset creators directly use the WET files, the
    open source organization Eleuther AI [noticed](https://oreil.ly/hciZS) that the
    quality of the WET files left much to be desired, with HTML boilerplate still
    prominent as seen above. To create The Pile, Eleuther AI thus used the [jusText
    library](https://oreil.ly/YRFzZ) to more reliably remove boilerplate text from
    HTML documents.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl 的数据以原始 HTML 和网络提取文本（WET）格式提供。虽然许多数据集创建者直接使用 WET 文件，但开源组织 Eleuther
    AI [注意到](https://oreil.ly/hciZS)，WET 文件的质量还有很多需要改进的地方，如上所述，HTML 模板文本仍然突出。因此，为了创建
    The Pile，Eleuther AI 使用了 [jusText 库](https://oreil.ly/YRFzZ) 来更可靠地从 HTML 文档中去除模板文本。
- en: 'Let’s explore the effect of using jusText with an example. In your Google Colab
    or Jupyter notebook, try this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来探索使用 jusText 的影响。在你的 Google Colab 或 Jupyter 笔记本中，尝试以下操作：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output displays all the boilerplate that is filtered out from a standard
    Wikipedia article:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了从标准维基百科文章中过滤出的所有模板文本：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: jusText just so happens to be more aggressive in removing content, but this
    is generally OK for cleaning pre-trained datasets since there is an abundance
    of text available. Some alternative libraries used for this task include [Dragnet](https://oreil.ly/URvsq),
    [html2text](https://oreil.ly/xk7Hc), [inscriptis](https://oreil.ly/6-2z1), [Newspaper](https://oreil.ly/LPXe1),
    and [Trafilatura](https://oreil.ly/zdZxj). According to the creators of [The Pile](https://oreil.ly/DZG7w),
    dividing the extraction pipeline across multiple libraries can reduce the risk
    of the resulting dataset being affected by any bias introduced by one of these
    libraries.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: jusText 正好更积极地去除内容，但这对清理预训练数据集来说通常是可行的，因为可用的文本量很大。用于此任务的替代库包括 [Dragnet](https://oreil.ly/URvsq)、[html2text](https://oreil.ly/xk7Hc)、[inscriptis](https://oreil.ly/6-2z1)、[Newspaper](https://oreil.ly/LPXe1)
    和 [Trafilatura](https://oreil.ly/zdZxj)。根据 [The Pile](https://oreil.ly/DZG7w)
    的创建者，将提取流程分散到多个库中可以降低结果数据集受到其中任何一个库引入的任何偏差的影响。
- en: Boilerplate removal in web pages is a challenging task. Web pages may also contain
    code blocks, tables, and math formulas, which need careful processing. [Meta](https://oreil.ly/bXELJ)
    noted that it built a custom HTML parser for preparing the dataset to train Llama
    3\. It also mentioned that Meta retains the *alt* attribute in images, which it
    found contains useful information like math content.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在网页中去除模板文本是一项具有挑战性的任务。网页还可能包含代码块、表格和数学公式，这些都需要仔细处理。[Meta](https://oreil.ly/bXELJ)
    指出，它为训练 Llama 3 构建了一个定制的 HTML 解析器来准备数据集。它还提到，Meta 保留了图像中的 *alt* 属性，它发现其中包含有用的信息，如数学内容。
- en: LLMs can also be utilized for accurate content extraction from web pages. However,
    as of this book’s writing, it is prohibitively expensive to do so, given the scale
    of the dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）也可以用于从网页中准确提取内容。然而，鉴于数据集的规模，截至本书编写时，这样做成本过高。
- en: 'Once text is extracted, the documents are passed through a series of data filtering
    steps. First, rudimentary filtering steps based on heuristics are applied. While
    the details differ across datasets, here are some of the steps typically performed:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被提取，文档将通过一系列数据过滤步骤。首先，应用基于启发式的基本过滤步骤。虽然不同数据集的细节不同，但以下是一些通常执行的步骤：
- en: Boilerplate removal
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模板文本移除
- en: Only lines that end with punctuation, like the period, exclamation point, and
    question mark are retained. This ensures that menu text from websites is removed.
    Only lines with greater than a particular threshold of words and documents with
    greater than a particular threshold of sentences are retained. The latter helps
    in modeling long sequences, which is an important capability for language models
    to have. Documents containing “lorem ipsum…” and other boilerplate text are filtered
    out.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 只有以标点符号（如句号、感叹号和问号）结尾的行会被保留。这确保了来自网站的菜单文本被移除。只有包含超过特定阈值单词的行和包含超过特定阈值句子的文档会被保留。后者有助于建模长序列，这对于语言模型来说是一个重要的能力。包含“lorem
    ipsum…”和其他模板文本的文档会被过滤掉。
- en: Non-English text removal
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 非英语文本移除
- en: Libraries like langdetect, langid, fasttext, and pycld2 are used to detect the
    language of the text. For example, C4 retains text that has > 0.99 probability
    of English as judged by langdetect. Note that these libraries can also be used
    to remove boilerplate and web page artifacts since they give a lower probability
    of English to those texts.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如langdetect、langid、fasttext和pycld2等库来检测文本的语言。例如，C4保留由langdetect判断出概率大于0.99的英语文本。请注意，这些库也可以用来移除模板文本和网页碎片，因为它们给这些文本的英语概率较低。
- en: Search engine optimization (SEO) text/spam removal
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎优化（SEO）文本/垃圾邮件移除
- en: Documents with a lot of repeated character sequences are removed. Documents
    with a low proportion of closed class words are removed. Closed class words in
    English are function words like “of,” “at,” “the,” and “is.” If a page is engaged
    in keyword stuffing and other SEO tricks, then they would have a lower closed
    class words ratio.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 包含大量重复字符序列的文档会被移除。包含低比例封闭类词汇的文档会被移除。英语中的封闭类词汇是功能词，如“of”、“at”、“the”和“is”。如果一个页面涉及关键词堆砌和其他SEO技巧，那么它们的封闭类词汇比例会较低。
- en: Pornographic/abusive text removal
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 恶俗/侮辱性文本移除
- en: Documents containing any words from keyword lists like the [“List of Dirty,
    Naughty, Obscene or Otherwise Bad Words”](https://oreil.ly/w3u_r) are removed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 包含来自如“脏话、下流、淫秽或其他不良词汇列表” [“List of Dirty, Naughty, Obscene or Otherwise Bad
    Words”](https://oreil.ly/w3u_r) 中任何单词的文档将被移除。
- en: Tools like langdetect and langid are helpful for speedy determination of the
    language in which the text is written at scale, but how do they deal with code-switched
    text (text in multiple languages, where English is often interspersed with a local
    language)?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如langdetect和langid等工具对于大规模快速确定文本所写语言很有帮助，但它们如何处理代码切换文本（包含多种语言，其中英语通常与本地语言交织）？
- en: 'You can try it! Here is an example for Taglish (Tagalog + English, which is
    a common mode of communication in the Philippines). In your notebook, run the
    following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以试试！以下是一个Taglish（菲律宾常见的交流模式，即菲律宾语+英语）的例子。在你的笔记本中运行以下代码：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The second paragraph would get included in the C4 dataset, as per its filtering
    criteria (probability of English should be greater than .99). Therefore, even
    datasets that claim to be English-only routinely contain text in other languages,
    leading to surprising multilingual behavior during inference. Ever wondered why
    some monolingual models seem to perform well at machine translation? This is a
    major reason.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其过滤标准（英语概率应大于.99），第二段将被包括在C4数据集中。因此，即使声称是纯英语的数据集也经常包含其他语言的文本，导致推理过程中出现令人惊讶的多语言行为。你是否曾想过为什么一些单语模型在机器翻译方面似乎表现良好？这是一个主要原因。
- en: 'The way langdetect is implemented makes it poor at identifying language when
    short sequences are provided. For example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: langdetect的实现方式使其在提供短序列时在识别语言方面表现不佳。例如：
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: returns
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: sk refers to Slovak here.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: sk在这里指的是斯洛伐克语。
- en: Selecting Quality Documents
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择高质量文档
- en: Not all data is created equal. Text from a high school physics textbook is considered
    higher quality compared to promotional text about a footwear brand. There are
    several ways we can operationalize the notion of quality and separate high-quality
    from low-quality data. In this section we will highlight a few such ways.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据都是平等的。高中物理教科书中的文本被认为比关于鞋类品牌的促销文本质量更高。我们可以通过几种方式来操作质量的概念，并将高质量数据与低质量数据分开。在本节中，我们将突出介绍几种这样的方法。
- en: Token-distribution K-L divergence
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记分布K-L散度
- en: In this method, documents with a token distribution that deviates too much from
    a reference token distribution are removed. In effect, this removes documents
    that have a lot of outlier tokens. This is calculated by using the [Kullback-Liebler
    (K-L) divergence](https://oreil.ly/gd5GH).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，那些与参考标记分布差异过大的文档会被移除。实际上，这移除了包含大量异常标记的文档。这是通过使用[库尔巴克-利布勒（K-L）散度](https://oreil.ly/gd5GH)来计算的。
- en: Classifier-based approaches
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于分类器的方法
- en: We can also build a classifier for identifying high-quality data. A simple way
    to build a quality-based classifier is to have examples for the positive class
    come from high-quality data sources like Wikipedia, and examples for the negative
    class to be drawn from random documents in the Common Crawl data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以构建一个用于识别高质量数据的分类器。构建基于质量分类器的一个简单方法是将正类的示例来自高质量数据源（如维基百科），而负类的示例则来自Common
    Crawl数据中的随机文档。
- en: Meta employed a variety of classifier models for high-quality data extraction
    for its [Llama 3 model](https://oreil.ly/O-CKF). One of them was a [fasttext classification
    model](https://oreil.ly/EWic6) trained to identify if a text is likely to be referenced
    by Wikipedia. Meta also trained a classifier whose training data was generated
    by Llama 2 by providing it with cleaned web documents and quality requirements
    and asking it to determine if the quality requirements are met. To extract code
    and text containing reasoning steps, Meta built classifiers that can identify
    them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Meta为其[Llama 3模型](https://oreil.ly/O-CKF)的高质量数据提取使用了多种分类器模型。其中之一是一个[fasttext分类模型](https://oreil.ly/EWic6)，该模型被训练以识别文本是否可能被维基百科引用。Meta还训练了一个分类器，其训练数据由Llama
    2生成，通过向其提供清洗过的网络文档和质量要求，并要求其判断是否满足质量要求。为了提取包含推理步骤的代码和文本，Meta构建了能够识别它们的分类器。
- en: '[Figure 2-4](#classifier-filtering) shows how a classifier can be built to
    discriminate between high-quality and low-quality data.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-4](#classifier-filtering)展示了如何构建一个分类器来区分高质量和低质量数据。'
- en: '![classifier-filtering](assets/dllm_0204.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![分类器过滤](assets/dllm_0204.png)'
- en: Figure 2-4\. Classifier-based quality filtering
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 基于分类器的质量过滤
- en: Perplexity for quality selection
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 质量选择的困惑度
- en: '[Perplexity](https://oreil.ly/OfycZ), an intrinsic evaluation measure for language
    models, has been used for document filtering in the context of preparing pre-training
    datasets, notably by the creators of [CCNet](https://oreil.ly/VF98y). Perplexity
    measures how well a model can predict a given text; the lower the perplexity,
    the better the model.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[困惑度](https://oreil.ly/OfycZ)，作为语言模型的一个内在评估指标，在准备预训练数据集的上下文中被用于文档过滤，特别是由[CCNet](https://oreil.ly/VF98y)的创建者所采用。困惑度衡量模型预测给定文本的能力；困惑度越低，模型越好。'
- en: Just like the classifier approach, we select documents from data sources that
    we deem high quality (like Wikipedia) as the positive class. We then train a 5-gram
    language model using [KenLM](https://oreil.ly/EU5r3) (a library facilitating training
    of n-gram language models) over it. Next, we take the dataset we want to filter
    and calculate the perplexity of each paragraph in it over the trained language
    model. The lower the perplexity, the more similar it is to the positive class.
    We can then discard documents with high perplexity.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类器方法一样，我们从我们认为高质量的数据源（如维基百科）中选择文档作为正类。然后，我们使用[KenLM](https://oreil.ly/EU5r3)（一个促进n-gram语言模型训练的库）在上面训练一个5-gram语言模型。接下来，我们取我们想要过滤的数据集，并计算其中每个段落相对于训练语言模型的困惑度。困惑度越低，它与正类越相似。然后我们可以丢弃困惑度高的文档。
- en: Low perplexity may not always be a good thing, however. Short, repetitive text
    can have low perplexity. Note that writing style gets factored into perplexity.
    If the reference language model is trained over Wikipedia, then documents written
    in an informal style may receive higher perplexity scores. Therefore, it would
    be beneficial to have a more involved filtering strategy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 低困惑度不一定总是好事。简短、重复的文本可以具有低困惑度。请注意，写作风格会被纳入困惑度计算。如果参考语言模型是在维基百科上训练的，那么非正式风格的文档可能会得到更高的困惑度分数。因此，拥有一个更复杂的过滤策略将会是有益的。
- en: To resolve this, the creators of [BERTIN](https://oreil.ly/uI9eV) introduced
    the concept of perplexity sampling. In perplexity sampling, instead of just filtering
    out low-perplexity text, it uses a sampling strategy that oversamples from the
    middle part of the perplexity probability distribution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，[BERTIN](https://oreil.ly/uI9eV) 的创造者引入了困惑度采样的概念。在困惑度采样中，它不仅仅过滤掉低困惑度文本，而是使用一种采样策略，从困惑度概率分布的中间部分进行过采样。
- en: '[Figure 2-5](#perplexity-sampling) shows how perplexity sampling is achieved
    in practice.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-5](#困惑度采样) 展示了困惑度采样在实际中的实现方式。'
- en: '![perplexity-sampling](assets/dllm_0205.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![困惑度采样](assets/dllm_0205.png)'
- en: Figure 2-5\. Perplexity sampling
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 困惑度采样
- en: 'Let’s explore the perplexity scores assigned by a model trained on Wikipedia
    text. Download this [file](https://oreil.ly/xwYjY). After placing the file in
    your home directory, run this code in a new file:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一个在维基百科文本上训练的模型分配的困惑度分数。下载这个[文件](https://oreil.ly/xwYjY)。将文件放置在您的家目录后，在一个新文件中运行以下代码：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`` `###### Note    According to an [analysis of C4](https://oreil.ly/Nzla7),
    the internet domain that contributed the largest proportion of text in the dataset
    was patents.google.com. Over 10% of the text from this domain is in fact machine
    translated, with patents from countries like Japan being translated from Japanese
    to English. So a significant amount of pre-training data is already not generated
    by humans!    Propelled by LLMs, the internet is slated to see widespread prevalence
    of AI-generated text. Recognizing whether text was written by a human or an LLM
    is a nontrivial task and certainly not feasible at scale. How this will affect
    future LLM performance is an open research question.    Despite all the data cleaning
    steps, the resulting dataset is still not going to be perfect at this level of
    scale. For example, Eleuther AI [reported](https://oreil.ly/WEBne) that the boilerplate
    sentence “select the forum that you want to visit from the selection below” occurs
    180K times in The Pile.` ``  [PRE10]` [PRE11] ## Deduplication    So far we have
    discussed data extraction and cleaning, language identification, and quality filtering.
    Let’s now explore the most contentious step in the pipeline: deduplication.    We
    know that web-crawled text is ridden with a lot of duplicates. Duplicates form
    a nontrivial portion of the training dataset, so any decision made about them
    will have a noticeable impact on the ensuing model.    How do we define a duplicate?
    We will make a distinction between three kinds:    Exact matches      Two sequences
    with the same text are exact-match duplicates. They are the easiest to handle.      Approximate
    matches      In many cases, there are near-duplicates, where sequences of text
    are identical except for a few characters. Sometimes these sequences are slightly
    different only due to HTML text extraction artifacts and other filtering processes.      Semantic
    duplicates      Duplicates that semantically convey the same content but using
    different wordings. This is usually treated as out of scope.      Duplicates can
    also be categorized based on the granularity at which they occur:    Document-level
    duplicates      Duplicate documents are removed during the preparation of most
    pre-training datasets. However, in some datasets like The Pile, certain subsets
    (like Wikipedia) are deliberately duplicated, so that they are seen more often
    by the model.      Sequence-level duplicates      These are lines or sentences
    in documents that are repeated across multiple documents. In some cases they can
    be massively duplicated, like terms of service text, copyright notices, website
    prefaces, etc.      ###### Note    Dededuplication is a very complex process,
    typically performed using the MinHash algorithm. This writeup by [Cheng Hao](https://oreil.ly/2RO9f)
    details the deduplication process followed in the Big Science and Big Code open
    source LLM projects.    Deduplicating data has several benefits:    *   A small
    subset of the pre-training dataset is usually set aside for validation/test. Deduplication
    can ensure the removal/reduction of overlap between the train and test sets, which
    is essential for an unbiased evaluation. Without sequence-level deduplication,
    there is a high likelihood of overlap of common text sequences in the train and
    test sets.           *   Removing duplicate sequences reduces the overall size
    of the training dataset. However, [Lee et al.](https://oreil.ly/k5OwJ) show that
    the perplexity of a model trained on the smaller dataset isn’t affected. Thus,
    the model can be trained for a shorter period yet with the same benefit.           *   Deduplication
    can also reduce the tendency of the model to memorize its training data. Memorization
    is closely linked to model overfitting and thwarts the model’s ability to generalize.
    While there are many ways to quantify memorization, we will focus on *memorization
    by generation*, where a model is said to have memorized a sequence if it is capable
    of generating it verbatim. [Lee et al.](https://oreil.ly/xpoz7) have shown that
    models trained on datasets that have been deduplicated at the sequence level generate
    ten times less verbatim training data.              ###### Tip    One advantage
    of using models trained on publicly available datasets is that you can search
    through the datasets to see if the text generated by the model exists verbatim
    in the dataset.    [Figure 2-6](#privacy-attacks-against-llms) demonstrates the
    flow of a rudimentary training-data extraction attack.  ![Privacy attacks](assets/dllm_0206.png)  ######
    Figure 2-6\. Privacy attacks against LLMs    ## Removing Personally Identifiable
    Information    While deduplication can reduce the likelihood of the model memorizing
    training data, it is by no means a panacea for the memorization problem. Even
    information that appears only once in the training set could potentially be memorized
    (and leaked). While a lot of content in the training data is innocuous (terms
    of service text) and perhaps even desirable to memorize (factual information,
    like the capital of Canada), memorization of personally identifiable information
    (PII) is a major concern.    Let us see what PII entails. The formal definition
    from [Cornell Law](https://oreil.ly/kN3J8) is as follows:    > Information that
    can be used to distinguish or trace an individual’s identity, either alone or
    when combined with other personal or identifying information that is linked or
    linkable to a specific individual.    Based on this definition, non-PII can become
    PII when another piece of information becomes public, which when combined with
    the non-PII can be used to uniquely identify an individual.    The legal definition
    of PII varies by jurisdiction. For example, the [General Data Protection Regulation
    (GDPR)](https://oreil.ly/F2dGL) in Europe says:    > Protection should be extended
    to anything used to directly or indirectly identify a person (or data subject).
    This may be extended to include characteristics that describe “physical, physiological,
    genetic, mental, commercial, cultural, or social identity of a person.”    Most
    open source models are trained on publicly available datasets. These datasets
    might contain PII, but one might be tempted to say, “Well it is already out in
    the open, so there is no need for privacy protection.” This argument overlooks
    the importance of consent and discoverability controls. For instance, I might
    have shared my PII on my blog, which resides in an obscure corner of the internet
    and is not easily discoverable through search engines, but if it ends up being
    added to a pre-training dataset, it suddenly brings this data into the spotlight,
    without my consent. This concept is called *contextual integrity*: data should
    only be shared in the original context in which it was shared.    So ideally,
    we would like to *detect* PII in the dataset, and then *remediate* it in some
    fashion, so that the PII is no longer present in the training data or at least
    not memorizable. The presence of *public-figure PII* adds a layer of complexity
    to this problem. We would like our model to be able to accurately answer factual
    questions about public figures, such as providing their birth date. The privacy
    expectations for public figures are lower, showcasing how the values of transparency
    and openness clash with privacy. Determining who is a public figure and what level
    of privacy they are entitled to is a complex socio-technical challenge.    Data
    considered private includes names, addresses, credit card data, government IDs,
    medical history and diagnosis data, email IDs, phone numbers, identity and affinity
    groups the person belongs to (religion, race, union membership), geolocation data,
    and so on.    Attacks can be either targeted or untargeted. In an untargeted attack,
    the attacker just generates a large body of text using the model and then runs
    a membership inference attack to determine text within it that is most likely
    to be memorized. In a targeted attack, the attacker attempts to recover personal
    information about a particular individual or a group of individuals. Targeted
    attacks are more difficult to execute, because while language models are good
    at memorization, they are bad at *association*, for instance, identifying that
    an email ID belongs to a specific person.    Most pre-training datasets have undergone
    little to no PII remediation. The Privacy working group (of which I was the co-lead)
    of the Big Science project that trained the BLOOM model developed a pipeline for
    PII detection and remediation, which we will discuss next.    ###### Note    Language
    models are also susceptible to training data poisoning attacks. Since a large
    portion of training data is sourced from web-crawled text, bad actors have an
    opportunity to influence the content of the training set. [Tramer er al.](https://oreil.ly/g_A-d)
    have shown that one can poison less than 0.1% of the training set with data whose
    effect is to make it easier for other data in the training set to leak more easily.    As
    LLMs increasingly get used as search engines, the demand for LLM SEO is cropping
    up. For example, a company could write content on their web sites in a manner
    that makes it more likely to be chosen in a pre-training dataset creation process
    that uses perplexity filtering.    [Figure 2-7](#PII-processing-pipeline) shows
    a typical PII processing pipeline.  ![PII Processing Pipeline](assets/dllm_0207.png)  ######
    Figure 2-7\. PII processing pipeline    ### PII detection    The task of PII detection
    is similar to the NLP task of NER, introduced in [Chapter 1](ch01.html#chapter_llm-introduction).
    However, not all named entities constitute PII. For our task we determined the
    PII tags to be PERSON, AGE, NORP (nationality, race, religion, political party
    affiliation, socio-economic class, and union membership), STREET_ADDRESS, CREDIT_CARD,
    GOVT_ID, EMAIL_ADDRESS, USER_ID, and PUBLIC_FIGURE.    We used the PUBLIC_FIGURE
    tag to identify information about public figures, since we didn’t want to filter
    them out. We also assigned fictional characters this tag.    Some of the structured
    tags in this list like emails and government IDs can be identified using regular
    expressions. For other tags, we annotated datasets that could then be used to
    train Transformer-based NER-like models. Interestingly, we observed a very high
    degree of inter-annotator disagreement (same example being annotated differently
    by different people) that underscored the cultural nuances of the definition of
    privacy and what constitutes personal information.    Here is the [regular expression](https://oreil.ly/8YwG9)
    to detect SSN (US Social Security numbers):    [PRE12]py   [PRE13] [PRE14] from
    faker import Faker fake = Faker(''en_IN'')   # Indian locale Faker.seed(0) for
    i in range(5):    print(fake.aadhaar_id) [PRE15] for i in range(5):    print(fake.address)
    [PRE16]` [PRE17][PRE18][PRE19] `` `# Effect of Pre-Training Data on Downstream
    Tasks    Given a pre-training dataset for an LLM, what assumptions can we make
    from it about downstream performance? It turns out that there is a correlation
    between the model’s performance on a given task or input and the pre-training
    dataset frequency of the task or the salient words in the input, respectively.
    First observed by [Razeghi et al.](https://oreil.ly/cPYej), this phenomenon has
    been studied in detail in McCoy et al.’s [“Embers of Autoregression” paper](https://oreil.ly/_O2NK).    McCoy
    et al. show that language models perform better at tasks that are more frequently
    represented in the training dataset than ones that are less frequently represented.
    For example, language models are better at base 10 addition than base 9 addition.
    They are also better at sorting by alphabetical order than they are at sorting
    by reverse alphabetical order.    Similarly, McCoy et al. also show that for a
    given task, models perform relatively better when the output is text with high
    frequency in the pre-training dataset as opposed to when the text is lower frequency.
    This phenomenon is also observed for inputs; models do relatively better with
    higher-frequency inputs compared to lower-frequency inputs.    As an example,
    consider the sentence: “record a be that miles, yes, hour, per fifty clocked he.”
    We ask the LLM to reverse the words in the sentence, which would lead to “He clocked
    fifty per hour, yes, miles, that be a record,” a rather low-probability sequence,
    due to its odd linguistic construction.    As of the book’s writing, GPT-4o returns
    the wrong answer: “He clocked fifty miles per hour that be a record,” but you
    can notice that it performs relatively better when the output sequence is higher
    probability.    # Bias and Fairness Issues in Pre-Training Datasets    A multitude
    of ethical questions arise during the productization of large language models.
    The existence of significant bias and fairness issues in these models often leads
    to a no-ship condition for a large number of use cases. In this section we will
    go through some bias and fairness issues specifically related to the collection
    and filtering of pre-training data.    The scale of data that LLMs are fed with
    means that they are not just constructing models of language but also of the world
    we inhabit. This gives rise to the question of whether we want to model the world
    the way it is or the way we would like it to be. The internet is filled with hate,
    violence, and abusive language and is often used as an outlet for humanity’s worst
    impulses. The text in it implicitly encodes long-existing biases against groups
    of people. For example, in The Pile, an [analysis](https://oreil.ly/hu3-b) of
    word co-occurrence statistics shows the word “radical” co-occurs with the word
    “Muslim” substantially more than it does for other religions.    The phenomenon
    of *bias amplification* makes these problems all the more critical. It has been
    shown that large language models [amplify the biases](https://oreil.ly/x-ba9)
    that are encoded in their pre-training data: they make biased predictions against
    groups of people at higher rates than what the training data statistics would
    suggest.    So, can we “fix” our training data such that we can model a world
    that encodes our values and principles that downstream applications will inherit?
    There is substantial debate in the research community about this. Opponents argue
    it is hard to identify and fix all societal biases encoded in the data since there
    are so many dimensions of bias that intersect in complex ways. Values are not
    universal, and model providers would like to be value-neutral to cater to all
    sections of society.    However, as Anna Rogers describes in her [paper](https://oreil.ly/hxU_-),
    this question is already moot. Data curation is already happening, whether we
    like it or not, and the values and interests of model providers are already being
    encoded into the models. For example, only a small proportion of available data
    is selected to be part of the pre-training set. This selection process is not
    value-neutral, even if one might not explicitly think in terms of it.    Wikipedia
    is one of the more popular datasets used in training LLMs. While it might be a
    no-brainer to include Wikipedia in a pre-training dataset, let’s explore the implications.
    Wikipedia is edited by volunteers, a very large proportion of them being men.
    Since the determination of whether a topic is reputable enough to deserve a Wikipedia
    page rests with the editors who are largely made up of men, we see disparities
    like obscure male football players from lower-level leagues getting their own
    pages while a disproportionate number of biography articles about women are slated
    for deletion.    Similarly, the highly influential WebText dataset is sourced
    from Reddit outbound links. Reddit is a predominantly male site, with [74% of
    users](https://oreil.ly/i2RkB) being men. Naturally, links posted on Reddit are
    more likely to be catered to male interests.    Bias can also be introduced during
    the data filtering stages. Earlier, we noted that keyword lists are often used
    to filter out pornographic material and abusive text. However, using a naive keyword
    list is a lazy approach that not only has problems with effectiveness (false negatives)
    but also inadvertently [results in](https://oreil.ly/XWBjV) filtering out positive
    text written by or about minority communities, as well as text written in dialects
    like African American English and Hispanic-aligned English. The fact that words
    in English have multiple senses has resulted in certain documents about breastfeeding
    being filtered out of the C4 dataset.    Overall, whether a word is hateful, abusive,
    or toxic depends on the social context, the intentions of the reader, and the
    intended audience. Keyword-based methods simply do not capture this nuance. The
    question of whether it is more effective to handle these issues at the pre-training
    stage or further downstream is an open area of research. We will explore techniques
    that can be employed downstream in [Chapter 10](ch10.html#ch10).    ###### Note    The
    authors of the [Pythia model](https://oreil.ly/r4oAT) experimented by replacing
    masculine pronouns with feminine ones for the last 7% of training tokens and noticed
    a de-biasing impact on downstream tasks.    # Summary    In this chapter, we outlined
    the key ingredients of a language model: the pre-training data, the vocabulary
    and tokenizer, the language objective, and the model architecture. We walked through
    the steps involved in creating a pre-training dataset in detail, including language
    identification, text extraction and cleaning, quality filtering, deduplication,
    PII removal, and test set decontamination. We also provided a list of commonly
    used pre-training datasets and the steps taken for preprocessing each of them.
    In the next chapter, we will explore the vocabulary and tokenizer of the language
    model: the language we intend the model to learn.    ^([1](ch02.html#id625-marker))
    From Dodge et al., [“A Case Study on the Colossal Clean Crawled Corpus”](https://oreil.ly/PwtVp),
    EMNLP 2021.` `` ```'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`` `###### 注意    根据 [C4 分析](https://oreil.ly/Nzla7)，在数据集中贡献最大比例文本的互联网域名是 patents.google.com。实际上，该域名超过
    10% 的文本是机器翻译的，例如来自日本的专利是从日语翻译成英语的。因此，大量的预训练数据实际上并非由人类生成！    在 LLM 的推动下，互联网预计将广泛普及
    AI 生成的文本。识别文本是由人类还是 LLM 编写的，是一项非平凡的任务，并且在大规模上肯定不可行。这将如何影响未来的 LLM 性能，是一个开放的研究问题。    尽管进行了所有数据清洗步骤，但在这个规模级别上，生成的数据集仍然不会完美。例如，Eleuther
    AI [报告](https://oreil.ly/WEBne)称，模板句子“从以下选择中选择您想要访问的论坛”在 The Pile 中出现了 180K 次。《PRE10》[PRE11]##
    去重    到目前为止，我们已经讨论了数据提取和清洗、语言识别和品质过滤。现在让我们探讨管道中最具争议的步骤：去重。    我们知道，网络爬取的文本充满了大量重复内容。重复内容构成了训练数据集的非小部分，因此对它们的任何决定都将对后续模型产生明显的影响。    我们如何定义重复内容？我们将区分三种类型：    完全匹配      两个具有相同文本的序列是完全匹配的重复内容。它们是最容易处理的。      近似匹配      在许多情况下，存在近似的重复内容，其中文本序列除了少数字符外完全相同。有时这些序列之所以略有不同，仅是因为
    HTML 文本提取伪影和其他过滤过程。      语义重复      语义上传达相同内容但使用不同措辞的重复内容。这通常被视为超出范围。      根据它们发生的粒度，重复内容也可以分类：    文档级重复      在大多数预训练数据集的准备过程中，会移除重复文档。然而，在某些数据集（如
    The Pile）中，某些子集（如维基百科）被故意重复，以便模型能更频繁地看到它们。      序列级重复      这些是跨多个文档重复的文档中的行或句子。在某些情况下，它们可以被大量重复，例如服务条款文本、版权声明、网站前言等。      ######
    注意    去重是一个非常复杂的过程，通常使用 MinHash 算法执行。Cheng Hao 的这篇文档详细介绍了 Big Science 和 Big Code
    开源 LLM 项目中遵循的去重过程。[Cheng Hao](https://oreil.ly/2RO9f)    去重数据有几个好处：    *   预训练数据集通常留出一小部分用于验证/测试。去重可以确保训练集和测试集之间重叠的减少/降低，这对于无偏评估至关重要。如果没有序列级去重，训练集和测试集中常见文本序列的重叠可能性很高。           *   移除重复序列可以减少训练数据集的整体大小。然而，[Lee
    等人](https://oreil.ly/k5OwJ)表明，在较小的数据集上训练的模型的困惑度并未受到影响。因此，模型可以在更短的时间内训练，同时获得相同的好处。           *   去重还可以减少模型记住其训练数据的倾向。记忆与模型过拟合密切相关，并阻碍了模型泛化的能力。虽然有许多方法可以量化记忆，但我们将重点关注
    *生成记忆*，即如果模型能够逐字生成一个序列，则认为它已经记住了该序列。[Lee 等人](https://oreil.ly/xpoz7)表明，在已去重序列级别的数据集上训练的模型生成的逐字训练数据减少了十倍。              ######
    提示    使用在公共数据集上训练的模型的一个优点是，您可以搜索数据集以查看模型生成的文本是否存在于数据集中。    [图 2-6](#privacy-attacks-against-llms)
    展示了基本的训练数据提取攻击流程。  ![隐私攻击](assets/dllm_0206.png)  ###### 图 2-6\. 对 LLM 的隐私攻击    ##
    移除个人身份信息    尽管去重可以减少模型记住训练数据的可能性，但它绝不是记忆问题的万能药。即使仅在训练集中出现一次的信息也可能被记住（并泄露）。虽然训练数据中的许多内容是无害的（服务条款文本）并且可能甚至希望记住（事实信息，如加拿大的首都），但个人身份信息（PII）的记忆是一个主要问题。    让我们看看
    PII 包括哪些内容。来自 [康奈尔法学院](https://oreil.ly/kN3J8) 的正式定义如下：    > 可以用来区分或追踪个人身份的信息，无论是单独使用还是与其他个人或识别信息结合使用，这些信息与特定个人相关联或可关联。    根据这个定义，当另一条信息变得公开时，非
    PII 可以成为 PII，因为当它与非 PII 结合使用时，可以用来唯一识别个人。    PII 的法律定义因司法管辖区而异。例如，欧洲的 [通用数据保护条例
    (GDPR)](https://oreil.ly/F2dGL) 表示：    > 应将保护扩展到任何用于直接或间接识别个人（或数据主体）的东西。这可能包括描述“个人的身体、生理、遗传、心理、商业、文化或社会身份”的特征。    大多数开源模型都是在公开可用的数据集上训练的。这些数据集可能包含
    PII，但有人可能会想，“好吧，它已经公开了，所以没有必要进行隐私保护。”这种论点忽视了同意和可发现性控制的重要性。例如，我可能在博客上分享了包含我的 PII
    的内容，该博客位于互联网的一个隐蔽角落，并且不容易通过搜索引擎发现，但如果它最终被添加到预训练数据集中，它突然将数据置于聚光灯下，而没有我的同意。这个概念被称为
    *情境完整性*：数据应仅在共享的原始上下文中共享。    因此，理想情况下，我们希望能够在数据集中 *检测* PII，并以某种方式 *修复* 它，以便 PII
    不再存在于训练数据中，或者至少不再是可记忆的。*公众人物 PII* 的存在给这个问题增加了复杂性。我们希望我们的模型能够准确地回答有关公众人物的事实性问题，例如提供他们的出生日期。公众人物的隐私期望较低，展示了透明度和开放性价值观与隐私之间的冲突。确定谁是公众人物以及他们应享有的隐私水平是一个复杂的社会技术挑战。    考虑为私密的包括姓名、地址、信用卡数据、政府
    ID、医疗历史和诊断数据、电子邮件 ID、电话号码、个人所属的认同和亲和群体（宗教、种族、工会会员资格）、地理位置数据等。    攻击可以是针对性的或非针对性的。在非针对性攻击中，攻击者仅使用模型生成大量文本，然后运行成员推理攻击以确定其中最有可能被记住的文本。在针对性攻击中，攻击者试图恢复有关特定个人或一组个人的个人信息。针对性攻击更难执行，因为虽然语言模型擅长记忆，但它们在
    *关联* 方面表现不佳，例如，确定电子邮件 ID 属于特定个人。    大多数预训练数据集都经历了很少或没有 PII 修复。训练 BLOOM 模型的 Big
    Science 项目（我是联合负责人）的隐私工作组开发了一个 PII 检测和修复管道，我们将在下面讨论。    ###### 注意    语言模型也容易受到训练数据中毒攻击。由于大量训练数据来自网络爬取的文本，不良行为者有机会影响训练集的内容。[Tramer
    等人](https://oreil.ly/g_A-d) 已经表明，可以用不到 0.1% 的训练集来中毒，其效果是使训练集中的其他数据更容易泄露。    随着
    LLM 越来越多地被用作搜索引擎，LLM SEO 的需求正在出现。例如，一家公司可以在其网站上以使其更有可能被选入使用困惑度过滤的预训练数据集创建过程的方式编写内容。    [图
    2-7](#PII-processing-pipeline) 展示了典型的 PII 处理管道。  ![PII 处理管道](assets/dllm_0207.png)  ######
    图 2-7\. PII 处理管道    ### PII 检测    PII 检测的任务类似于在 [第 1 章](ch01.html#chapter_llm-introduction)
    中介绍的 NLP 任务 NER。然而，并非所有命名实体都构成 PII。对于我们的任务，我们确定 PII 标签为 PERSON、AGE、NORP（国籍、种族、宗教、政党隶属、社会经济阶层和工会会员资格）、STREET_ADDRESS、CREDIT_CARD、GOVT_ID、EMAIL_ADDRESS、USER_ID
    和 PUBLIC_FIGURE。    我们使用 PUBLIC_FIGURE 标签来识别有关公众人物的信息，因为我们不希望过滤掉它们。我们还为虚构人物分配了这个标签。    列表中的一些结构化标签（如电子邮件和政府
    ID）可以使用正则表达式识别。对于其他标签，我们注释了数据集，然后可以使用这些数据集来训练基于 Transformer 的类似 NER 的模型。有趣的是，我们观察到注释者之间的高度不一致（不同的人对同一示例进行不同的注释），这突出了隐私定义的文化细微差别以及构成个人信息的内容。    这里是检测
    SSN（美国社会安全号码）的 [正则表达式](https://oreil.ly/8YwG9)：    [PRE12]py   [PRE13] [PRE14]
    from faker import Faker fake = Faker(''en_IN'')   # 印度地区 Faker.seed(0) for i in
    range(5):    print(fake.aadhaar_id) [PRE15] for i in range(5):    print(fake.address)
    [PRE16]` [PRE17][PRE18][PRE19] `` `# 预训练数据对下游任务的影响    给定一个 LLM 的预训练数据集，我们可以从它对下游性能的哪些假设？事实证明，模型在给定任务或输入上的性能与预训练数据集中任务或输入中显著词的频率之间存在相关性。这一现象首先由
    [Razeghi 等人](https://oreil.ly/cPYej) 发现，并在 McCoy 等人 [“自回归的余烬”论文](https://oreil.ly/_O2NK)
    中进行了详细研究。    McCoy 等人表明，语言模型在训练数据集中更频繁表示的任务上表现更好，而在表示较少的任务上表现较差。例如，语言模型在十进制加法方面比九进制加法表现更好。它们在按字母顺序排序方面也比按逆字母顺序排序表现更好。    类似地，McCoy
    等人也表明，对于给定任务，当输出是预训练数据集中频率较高的文本时，模型的表现相对较好，而不是当文本频率较低时。这种现象也适用于输入；与低频率输入相比，模型在高频率输入上表现相对较好。    以一个句子为例：“record
    a be that miles, yes, hour, per fifty clocked he。”我们要求 LLM 逆序排列句子中的单词，这将导致“He
    clocked fifty per hour, yes, miles, that be a record，”这是一个相当不可能的序列，因为它具有奇怪的语构。    到本书写作时，GPT-4o
    返回了错误的答案：“He clocked fifty miles per hour that be a record，”但你可以注意到，当输出序列的概率较高时，它的表现相对较好。    #
    预训练数据集中的偏差和公平性问题    在大型语言模型的产品化过程中，出现了许多伦理问题。这些模型中存在的重大偏差和公平性问题往往会导致大量用例无法发货。在本节中，我们将讨论一些与预训练数据的收集和过滤特别相关的偏差和公平性问题。    LLM
    被喂入的数据规模意味着它们不仅构建了语言模型，也构建了我们所在世界的模型。这引发了一个问题：我们是否想要以世界本来的样子来建模，还是以我们希望它成为的样子来建模。互联网充满了仇恨、暴力和侮辱性语言，经常被用作人类最恶劣冲动的一种出口。其中的文本隐式编码了对某些人群长期存在的偏见。例如，在
    The Pile 中，[分析](https://oreil.ly/hu3-b)单词共现统计表明，“radical”一词与“穆斯林”一词的共现频率比与其他宗教的共现频率要高得多。    *偏差放大*
    的现象使这些问题变得更加严重。已经证明，大型语言模型 [放大了](https://oreil.ly/x-ba9)其预训练数据中编码的偏差：它们以比训练数据统计所暗示的更高的比率对某些人群做出有偏见的预测。    那么，我们能否“修复”我们的训练数据，以便我们可以模拟一个编码了我们的价值观和原则的世界，下游应用将继承这些价值观和原则？在研究界中，对此存在大量争议。反对者认为，由于存在许多相互交织的偏差维度，因此很难识别和修复数据中编码的所有社会偏见。价值观并非普遍存在，模型提供商希望保持价值中立，以迎合社会的各个部分。    然而，正如
    Anna Rogers 在她的 [论文](https://oreil.ly/hxU_-) 中所描述的，这个问题已经没有意义了。数据整理已经在进行中，无论我们是否喜欢，模型提供商的价值观和利益已经编码到模型中。例如，只有一小部分可用的数据被选中作为预训练集的一部分。这个选择过程并非价值中立，即使一个人可能没有明确地从这个角度思考。    维基百科是用于训练
    LLM 的更受欢迎的数据集之一。虽然将其包含在预训练数据集中可能是一个不言而喻的选择，但让我们探讨其影响。维基百科是由志愿者编辑的，其中很大一部分是男性。由于确定一个主题是否足够有信誉以获得维基百科页面的是编辑，而这些编辑主要由男性组成，因此我们看到了像来自低级别联赛的男性足球运动员获得自己的页面这样的差异，而大量关于女性的传记文章则被安排删除。    类似地，具有高度影响力的
    WebText 数据集来自 Reddit 的外链。Reddit 是一个以男性为主的网站，74% 的用户是男性。[74% 的用户](https://oreil.ly/i2RkB)是男性。自然地，Reddit
    上发布的链接更有可能迎合男性的兴趣。    偏差也可以在数据过滤阶段引入。早些时候，我们提到关键字列表通常用于过滤掉色情材料和侮辱性文本。然而，使用天真关键字列表是一种懒惰的方法，它不仅存在有效性问题（假阴性），而且无意中
    [导致](https://oreil.ly/XWBjV)过滤掉来自少数族裔社区或使用像非洲裔美国英语和西班牙裔英语这样的方言撰写的文本的积极文本。由于英语单词具有多种含义，因此某些关于母乳喂养的文档被从
    C4 数据集中过滤掉。    总体而言，一个词是否具有仇恨性、侮辱性或毒性取决于社会环境、读者的意图以及预期的受众。基于关键字的方法根本无法捕捉这种细微差别。是否在预训练阶段或更下游处理这些问题更有效，是一个开放的研究领域。我们将在
    [第 10 章](ch10.html#ch10) 中探讨可以用于下游的技术。    ###### 注意    Pythia 模型的作者通过将最后 7% 的训练标记中的男性代词替换为女性代词进行了实验，并注意到这对其下游任务产生了去偏影响。    #
    总结    在本章中，我们概述了语言模型的关键组成部分：预训练数据、词汇和分词器、语言目标和模型架构。我们详细介绍了创建预训练数据集的步骤，包括语言识别、文本提取和清洗、品质过滤、去重、PII
    移除和测试集净化。我们还提供了一份常用预训练数据集列表以及预处理每个数据集的步骤。在下一章中，我们将探讨语言模型的词汇和分词器：我们希望模型学习的语言。    ^([1](ch02.html#id625-marker))
    来自 Dodge 等人，《“大型清洁爬取语料库案例研究”》，EMNLP 2021。` ``'
