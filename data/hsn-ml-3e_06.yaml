- en: Chapter 5\. Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 支持向量机
- en: A *support vector machine* (SVM) is a powerful and versatile machine learning
    model, capable of performing linear or nonlinear classification, regression, and
    even novelty detection. SVMs shine with small to medium-sized nonlinear datasets
    (i.e., hundreds to thousands of instances), especially for classification tasks.
    However, they don’t scale very well to very large datasets, as you will see.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机*（SVM）是一个强大且多功能的机器学习模型，能够执行线性或非线性分类、回归，甚至新颖性检测。SVM在小到中等大小的非线性数据集（即，数百到数千个实例）上表现出色，尤其适用于分类任务。然而，它们在处理非常大的数据集时并不很好，您将看到。'
- en: This chapter will explain the core concepts of SVMs, how to use them, and how
    they work. Let’s jump right in!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将解释SVM的核心概念，如何使用它们以及它们的工作原理。让我们开始吧！
- en: Linear SVM Classification
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性SVM分类
- en: The fundamental idea behind SVMs is best explained with some visuals. [Figure 5-1](#large_margin_classification_plot)
    shows part of the iris dataset that was introduced at the end of [Chapter 4](ch04.html#linear_models_chapter).
    The two classes can clearly be separated easily with a straight line (they are
    *linearly separable*). The left plot shows the decision boundaries of three possible
    linear classifiers. The model whose decision boundary is represented by the dashed
    line is so bad that it does not even separate the classes properly. The other
    two models work perfectly on this training set, but their decision boundaries
    come so close to the instances that these models will probably not perform as
    well on new instances. In contrast, the solid line in the plot on the right represents
    the decision boundary of an SVM classifier; this line not only separates the two
    classes but also stays as far away from the closest training instances as possible.
    You can think of an SVM classifier as fitting the widest possible street (represented
    by the parallel dashed lines) between the classes. This is called *large margin
    classification*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机背后的基本思想最好通过一些可视化来解释。[图5-1](#large_margin_classification_plot)展示了在[第4章](ch04.html#linear_models_chapter)末尾介绍的鸢尾花数据集的一部分。这两个类可以很容易地用一条直线分开（它们是*线性可分*的）。左图显示了三种可能线性分类器的决策边界。决策边界由虚线表示的模型非常糟糕，甚至不能正确地分开这两个类。其他两个模型在这个训练集上表现完美，但它们的决策边界与实例非常接近，因此这些模型在新实例上可能表现不佳。相比之下，右图中的实线代表SVM分类器的决策边界；这条线不仅分开了两个类，而且尽可能远离最接近的训练实例。您可以将SVM分类器视为在类之间拟合最宽可能的街道（由平行虚线表示）。这被称为*大边距分类*。
- en: '![mls3 0501](assets/mls3_0501.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0501](assets/mls3_0501.png)'
- en: Figure 5-1\. Large margin classification
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1. 大边距分类
- en: 'Notice that adding more training instances “off the street” will not affect
    the decision boundary at all: it is fully determined (or “supported”) by the instances
    located on the edge of the street. These instances are called the *support vectors*
    (they are circled in [Figure 5-1](#large_margin_classification_plot)).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，添加更多训练实例“离开街道”不会对决策边界产生任何影响：它完全由位于街道边缘的实例决定（或“支持”）。这些实例被称为*支持向量*（它们在[图5-1](#large_margin_classification_plot)中被圈出）。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: SVMs are sensitive to the feature scales, as you can see in [Figure 5-2](#sensitivity_to_feature_scales_plot).
    In the left plot, the vertical scale is much larger than the horizontal scale,
    so the widest possible street is close to horizontal. After feature scaling (e.g.,
    using Scikit-Learn’s `StandardScaler`), the decision boundary in the right plot
    looks much better.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机对特征的尺度敏感，如您可以在[图5-2](#sensitivity_to_feature_scales_plot)中看到。在左图中，垂直尺度远大于水平尺度，因此最宽可能的街道接近水平。经过特征缩放（例如，使用Scikit-Learn的`StandardScaler`），右图中的决策边界看起来好多了。
- en: '![mls3 0502](assets/mls3_0502.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0502](assets/mls3_0502.png)'
- en: Figure 5-2\. Sensitivity to feature scales
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2. 特征尺度的敏感性
- en: Soft Margin Classification
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软边距分类
- en: 'If we strictly impose that all instances must be off the street and on the
    correct side, this is called *hard margin classification*. There are two main
    issues with hard margin classification. First, it only works if the data is linearly
    separable. Second, it is sensitive to outliers. [Figure 5-3](#sensitivity_to_outliers_plot)
    shows the iris dataset with just one additional outlier: on the left, it is impossible
    to find a hard margin; on the right, the decision boundary ends up very different
    from the one we saw in [Figure 5-1](#large_margin_classification_plot) without
    the outlier, and the model will probably not generalize as well.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们严格要求所有实例必须远离街道并位于正确的一侧，这被称为*硬边距分类*。硬边距分类存在两个主要问题。首先，它仅在数据线性可分时有效。其次，它对异常值敏感。[图5-3](#sensitivity_to_outliers_plot)展示了鸢尾花数据集中仅有一个额外异常值的情况：在左侧，找到硬边距是不可能的；在右侧，决策边界与[图5-1](#large_margin_classification_plot)中看到的没有异常值的情决策边界非常不同，模型可能不会很好地泛化。
- en: '![mls3 0503](assets/mls3_0503.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0503](assets/mls3_0503.png)'
- en: Figure 5-3\. Hard margin sensitivity to outliers
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3. 硬边距对异常值的敏感性
- en: To avoid these issues, we need to use a more flexible model. The objective is
    to find a good balance between keeping the street as large as possible and limiting
    the *margin violations* (i.e., instances that end up in the middle of the street
    or even on the wrong side). This is called *soft margin classification*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这些问题，我们需要使用一个更灵活的模型。目标是在尽可能保持街道尽可能宽阔和限制*边距违规*（即，最终位于街道中间甚至错误一侧的实例）之间找到一个良好的平衡。这被称为*软边距分类*。
- en: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters,
    including the regularization hyperparameter `C`. If you set it to a low value,
    then you end up with the model on the left of [Figure 5-4](#regularization_plot).
    With a high value, you get the model on the right. As you can see, reducing `C`
    makes the street larger, but it also leads to more margin violations. In other
    words, reducing `C` results in more instances supporting the street, so there’s
    less risk of overfitting. But if you reduce it too much, then the model ends up
    underfitting, as seems to be the case here: the model with `C=100` looks like
    it will generalize better than the one with `C=1`.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Scikit-Learn创建SVM模型时，您可以指定几个超参数，包括正则化超参数`C`。如果将其设置为较低的值，则会得到左侧[图5-4](#regularization_plot)中的模型。如果设置为较高的值，则会得到右侧的模型。正如您所看到的，减少`C`会使街道变宽，但也会导致更多的间隔违规。换句话说，减少`C`会导致更多的实例支持街道，因此过拟合的风险较小。但是，如果减少得太多，那么模型最终会欠拟合，就像这里的情况一样：`C=100`的模型看起来比`C=1`的模型更容易泛化。
- en: '![mls3 0504](assets/mls3_0504.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0504](assets/mls3_0504.png)'
- en: Figure 5-4\. Large margin (left) versus fewer margin violations (right)
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. 大间隔（左）与较少间隔违规（右）
- en: Tip
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your SVM model is overfitting, you can try regularizing it by reducing `C`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的SVM模型过拟合，可以尝试通过减少`C`来对其进行正则化。
- en: 'The following Scikit-Learn code loads the iris dataset and trains a linear
    SVM classifier to detect *Iris virginica* flowers. The pipeline first scales the
    features, then uses a `LinearSVC` with `C=1`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Scikit-Learn代码加载了鸢尾花数据集，并训练了一个线性SVM分类器来检测*Iris virginica*花。该流水线首先对特征进行缩放，然后使用`LinearSVC`和`C=1`进行训练：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resulting model is represented on the left in [Figure 5-4](#regularization_plot).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型在[图5-4](#regularization_plot)的左侧表示。
- en: 'Then, as usual, you can use the model to make predictions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像往常一样，您可以使用模型进行预测：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first plant is classified as an *Iris virginica*, while the second is not.
    Let’s look at the scores that the SVM used to make these predictions. These measure
    the signed distance between each instance and the decision boundary:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个植物被分类为*Iris virginica*，而第二个没有。让我们看看SVM用于做出这些预测的分数。这些分数衡量了每个实例与决策边界之间的有符号距离：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unlike `LogisticRegression`, `LinearSVC` doesn’t have a `predict_proba()` method
    to estimate the class probabilities. That said, if you use the `SVC` class (discussed
    shortly) instead of `LinearSVC`, and if you set its `probability` hyperparameter
    to `True`, then the model will fit an extra model at the end of training to map
    the SVM decision function scores to estimated probabilities. Under the hood, this
    requires using 5-fold cross-validation to generate out-of-sample predictions for
    every instance in the training set, then training a `LogisticRegression` model,
    so it will slow down training considerably. After that, the `predict_proba()`
    and `predict_log_proba()` methods will be available.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与`LogisticRegression`不同，`LinearSVC`没有`predict_proba()`方法来估计类概率。也就是说，如果您使用`SVC`类（稍后讨论）而不是`LinearSVC`，并将其`probability`超参数设置为`True`，那么模型将在训练结束时拟合一个额外的模型，将SVM决策函数分数映射到估计概率。在幕后，这需要使用5倍交叉验证为训练集中的每个实例生成样本外预测，然后训练一个`LogisticRegression`模型，因此会显著减慢训练速度。之后，`predict_proba()`和`predict_log_proba()`方法将可用。
- en: Nonlinear SVM Classification
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性SVM分类
- en: 'Although linear SVM classifiers are efficient and often work surprisingly well,
    many datasets are not even close to being linearly separable. One approach to
    handling nonlinear datasets is to add more features, such as polynomial features
    (as we did in [Chapter 4](ch04.html#linear_models_chapter)); in some cases this
    can result in a linearly separable dataset. Consider the lefthand plot in [Figure 5-5](#higher_dimensions_plot):
    it represents a simple dataset with just one feature, *x*[1]. This dataset is
    not linearly separable, as you can see. But if you add a second feature *x*[2]
    = (*x*[1])², the resulting 2D dataset is perfectly linearly separable.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管线性SVM分类器高效且通常表现出色，但许多数据集甚至远非线性可分。处理非线性数据集的一种方法是添加更多特征，例如多项式特征（就像我们在[第4章](ch04.html#linear_models_chapter)中所做的那样）；在某些情况下，这可能会导致一个线性可分的数据集。考虑[图5-5](#higher_dimensions_plot)中的左侧图：它代表一个只有一个特征*x*[1]的简单数据集。正如您所看到的，这个数据集是线性不可分的。但是，如果添加第二个特征*x*[2]
    = (*x*[1])²，那么得到的2D数据集就是完全线性可分的。
- en: '![mls3 0505](assets/mls3_0505.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0505](assets/mls3_0505.png)'
- en: Figure 5-5\. Adding features to make a dataset linearly separable
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. 添加特征使数据集线性可分
- en: 'To implement this idea using Scikit-Learn, you can create a pipeline containing
    a `PolynomialFeatures` transformer (discussed in [“Polynomial Regression”](ch04.html#polynomial_regression)),
    followed by a `StandardScaler` and a `LinearSVC` classifier. Let’s test this on
    the moons dataset, a toy dataset for binary classification in which the data points
    are shaped as two interleaving crescent moons (see [Figure 5-6](#moons_polynomial_svc_plot)).
    You can generate this dataset using the `make_moons()` function:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Scikit-Learn实现这个想法，您可以创建一个包含`PolynomialFeatures`转换器（在[“多项式回归”](ch04.html#polynomial_regression)中讨论）、`StandardScaler`和`LinearSVC`分类器的流水线。让我们在moons数据集上测试这个流水线，这是一个用于二元分类的玩具数据集，其中数据点呈两个交错新月形状（参见[图5-6](#moons_polynomial_svc_plot)）。您可以使用`make_moons()`函数生成这个数据集：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![mls3 0506](assets/mls3_0506.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0506](assets/mls3_0506.png)'
- en: Figure 5-6\. Linear SVM classifier using polynomial features
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 使用多项式特征的线性SVM分类器
- en: Polynomial Kernel
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式核
- en: Adding polynomial features is simple to implement and can work great with all
    sorts of machine learning algorithms (not just SVMs). That said, at a low polynomial
    degree this method cannot deal with very complex datasets, and with a high polynomial
    degree it creates a huge number of features, making the model too slow.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 添加多项式特征很容易实现，并且可以与各种机器学习算法（不仅仅是SVM）很好地配合。也就是说，在低多项式度数下，这种方法无法处理非常复杂的数据集，而在高多项式度数下，它会创建大量特征，使模型变得过于缓慢。
- en: 'Fortunately, when using SVMs you can apply an almost miraculous mathematical
    technique called the *kernel trick* (which is explained later in this chapter).
    The kernel trick makes it possible to get the same result as if you had added
    many polynomial features, even with a very high degree, without actually having
    to add them. This means there’s no combinatorial explosion of the number of features.
    This trick is implemented by the `SVC` class. Let’s test it on the moons dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在使用SVM时，你可以应用一种几乎神奇的数学技术，称为*核技巧*（稍后在本章中解释）。核技巧使得可以获得与添加许多多项式特征相同的结果，即使是非常高次的，而无需实际添加它们。这意味着特征数量不会组合爆炸。这个技巧由`SVC`类实现。让我们在moons数据集上测试一下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code trains an SVM classifier using a third-degree polynomial kernel, represented
    on the left in [Figure 5-7](#moons_kernelized_polynomial_svc_plot). On the right
    is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if
    your model is overfitting, you might want to reduce the polynomial degree. Conversely,
    if it is underfitting, you can try increasing it. The hyperparameter `coef0` controls
    how much the model is influenced by high-degree terms versus low-degree terms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用三次多项式核训练了一个SVM分类器，左侧在[图5-7](#moons_kernelized_polynomial_svc_plot)中表示。右侧是另一个使用十次多项式核的SVM分类器。显然，如果你的模型出现过拟合，你可能需要降低多项式次数。相反，如果出现欠拟合，你可以尝试增加它。超参数`coef0`控制模型受高次项和低次项影响的程度。
- en: '![mls3 0507](assets/mls3_0507.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0507](assets/mls3_0507.png)'
- en: Figure 5-7\. SVM classifiers with a polynomial kernel
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 使用多项式核的SVM分类器
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Although hyperparameters will generally be tuned automatically (e.g., using
    randomized search), it’s good to have a sense of what each hyperparameter actually
    does and how it may interact with other hyperparameters: this way, you can narrow
    the search to a much smaller space.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然超参数通常会自动调整（例如使用随机搜索），但了解每个超参数实际上是做什么以及它如何与其他超参数交互是很有帮助的：这样，你可以将搜索范围缩小到一个更小的空间。
- en: Similarity Features
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似性特征
- en: Another technique to tackle nonlinear problems is to add features computed using
    a similarity function, which measures how much each instance resembles a particular
    *landmark*, as we did in [Chapter 2](ch02.html#project_chapter) when we added
    the geographic similarity features. For example, let’s take the 1D dataset from
    earlier and add two landmarks to it at *x*[1] = –2 and *x*[1] = 1 (see the left
    plot in [Figure 5-8](#kernel_method_plot)). Next, we’ll define the similarity
    function to be the Gaussian RBF with *γ* = 0.3\. This is a bell-shaped function
    varying from 0 (very far away from the landmark) to 1 (at the landmark).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解决非线性问题的另一种技术是添加使用相似性函数计算的特征，该函数衡量每个实例与特定“地标”的相似程度，就像我们在[第2章](ch02.html#project_chapter)中添加地理相似性特征时所做的那样。例如，让我们取之前的一维数据集，在*x*[1]
    = -2和*x*[1] = 1处添加两个地标（参见[图5-8](#kernel_method_plot)中的左图）。接下来，我们将定义相似性函数为带有*γ*
    = 0.3的高斯RBF。这是一个钟形函数，从0（远离地标）变化到1（在地标处）。
- en: 'Now we are ready to compute the new features. For example, let’s look at the
    instance *x*[1] = –1: it is located at a distance of 1 from the first landmark
    and 2 from the second landmark. Therefore, its new features are *x*[2] = exp(–0.3
    × 1²) ≈ 0.74 and *x*[3] = exp(–0.3 × 2²) ≈ 0.30\. The plot on the right in [Figure 5-8](#kernel_method_plot)
    shows the transformed dataset (dropping the original features). As you can see,
    it is now linearly separable.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备计算新特征。例如，让我们看一下实例*x*[1] = -1：它距离第一个地标1，距离第二个地标2。因此，它的新特征是*x*[2] = exp(–0.3
    × 1²) ≈ 0.74和*x*[3] = exp(–0.3 × 2²) ≈ 0.30。[图5-8](#kernel_method_plot)中右侧的图显示了转换后的数据集（放弃了原始特征）。如你所见，现在它是线性可分的。
- en: '![mls3 0508](assets/mls3_0508.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0508](assets/mls3_0508.png)'
- en: Figure 5-8\. Similarity features using the Gaussian RBF
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 使用高斯RBF的相似性特征
- en: You may wonder how to select the landmarks. The simplest approach is to create
    a landmark at the location of each and every instance in the dataset. Doing that
    creates many dimensions and thus increases the chances that the transformed training
    set will be linearly separable. The downside is that a training set with *m* instances
    and *n* features gets transformed into a training set with *m* instances and *m*
    features (assuming you drop the original features). If your training set is very
    large, you end up with an equally large number of features.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道如何选择地标。最简单的方法是在数据集中的每个实例位置创建一个地标。这样做会创建许多维度，从而增加转换后的训练集线性可分的机会。缺点是，一个包含*m*个实例和*n*个特征的训练集会转换为一个包含*m*个实例和*m*个特征的训练集（假设你放弃了原始特征）。如果你的训练集非常大，最终会得到同样数量的特征。
- en: Gaussian RBF Kernel
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯RBF核
- en: 'Just like the polynomial features method, the similarity features method can
    be useful with any machine learning algorithm, but it may be computationally expensive
    to compute all the additional features (especially on large training sets). Once
    again the kernel trick does its SVM magic, making it possible to obtain a similar
    result as if you had added many similarity features, but without actually doing
    so. Let’s try the `SVC` class with the Gaussian RBF kernel:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与多项式特征方法一样，相似性特征方法可以与任何机器学习算法一起使用，但计算所有额外特征可能会很昂贵（尤其是在大型训练集上）。再次，核技巧发挥了SVM的魔力，使得可以获得与添加许多相似性特征相同的结果，但实际上并没有这样做。让我们尝试使用高斯RBF核的`SVC`类：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This model is represented at the bottom left in [Figure 5-9](#moons_rbf_svc_plot).
    The other plots show models trained with different values of hyperparameters `gamma`
    (*γ*) and `C`. Increasing `gamma` makes the bell-shaped curve narrower (see the
    lefthand plots in [Figure 5-8](#kernel_method_plot)). As a result, each instance’s
    range of influence is smaller: the decision boundary ends up being more irregular,
    wiggling around individual instances. Conversely, a small `gamma` value makes
    the bell-shaped curve wider: instances have a larger range of influence, and the
    decision boundary ends up smoother. So *γ* acts like a regularization hyperparameter:
    if your model is overfitting, you should reduce *γ*; if it is underfitting, you
    should increase *γ* (similar to the `C` hyperparameter).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在[图5-9](#moons_rbf_svc_plot)的左下角表示。其他图显示了使用不同超参数`gamma`（*γ*）和`C`训练的模型。增加`gamma`会使钟形曲线变窄（参见[图5-8](#kernel_method_plot)中的左侧图）。因此，每个实例的影响范围更小：决策边界最终变得更加不规则，围绕个别实例摆动。相反，较小的`gamma`值会使钟形曲线变宽：实例的影响范围更大，决策边界变得更加平滑。因此，*γ*就像一个正则化超参数：如果你的模型过拟合，应该减小*γ*；如果欠拟合，应该增加*γ*（类似于`C`超参数）。
- en: '![mls3 0509](assets/mls3_0509.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0509](assets/mls3_0509.png)'
- en: Figure 5-9\. SVM classifiers using an RBF kernel
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9。使用RBF核的SVM分类器
- en: Other kernels exist but are used much more rarely. Some kernels are specialized
    for specific data structures. *String kernels* are sometimes used when classifying
    text documents or DNA sequences (e.g., using the string subsequence kernel or
    kernels based on the Levenshtein distance).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其他核存在，但使用得更少。一些核专门用于特定的数据结构。*字符串核*有时用于对文本文档或DNA序列进行分类（例如，使用字符串子序列核或基于Levenshtein距离的核）。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: With so many kernels to choose from, how can you decide which one to use? As
    a rule of thumb, you should always try the linear kernel first. The `LinearSVC`
    class is much faster than `SVC(kernel="linear")`, especially if the training set
    is very large. If it is not too large, you should also try kernelized SVMs, starting
    with the Gaussian RBF kernel; it often works really well. Then, if you have spare
    time and computing power, you can experiment with a few other kernels using hyperparameter
    search. If there are kernels specialized for your training set’s data structure,
    make sure to give them a try too.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有这么多核可供选择，你如何决定使用哪一个？作为一个经验法则，你应该始终首先尝试线性核。`LinearSVC`类比`SVC(kernel="linear")`快得多，特别是当训练集非常大时。如果不太大，你也应该尝试核化的SVM，首先使用高斯RBF核；它通常效果很好。然后，如果你有多余的时间和计算能力，你可以尝试使用一些其他核进行超参数搜索。如果有专门针对你的训练集数据结构的核，也要试一试。
- en: SVM Classes and Computational Complexity
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVM类和计算复杂度
- en: The `LinearSVC` class is based on the `liblinear` library, which implements
    an [optimized algorithm](https://homl.info/13) for linear SVMs.⁠^([1](ch05.html#idm45720213459744))
    It does not support the kernel trick, but it scales almost linearly with the number
    of training instances and the number of features. Its training time complexity
    is roughly *O*(*m* × *n*). The algorithm takes longer if you require very high
    precision. This is controlled by the tolerance hyperparameter *ϵ* (called `tol`
    in Scikit-Learn). In most classification tasks, the default tolerance is fine.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinearSVC`类基于`liblinear`库，该库实现了线性SVM的[优化算法](https://homl.info/13)。⁠^([1](ch05.html#idm45720213459744))
    它不支持核技巧，但随着训练实例数量和特征数量的增加，它的缩放几乎是线性的。其训练时间复杂度大约为*O*(*m* × *n*)。如果需要非常高的精度，算法会花费更长的时间。这由容差超参数*ϵ*（在Scikit-Learn中称为`tol`）控制。在大多数分类任务中，默认容差是可以接受的。'
- en: The `SVC` class is based on the `libsvm` library, which implements an [algorithm
    that supports the kernel trick](https://homl.info/14).⁠^([2](ch05.html#idm45720213447328))
    The training time complexity is usually between *O*(*m*² × *n*) and *O*(*m*³ ×
    *n*). Unfortunately, this means that it gets dreadfully slow when the number of
    training instances gets large (e.g., hundreds of thousands of instances), so this
    algorithm is best for small or medium-sized nonlinear training sets. It scales
    well with the number of features, especially with sparse features (i.e., when
    each instance has few nonzero features). In this case, the algorithm scales roughly
    with the average number of nonzero features per instance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`SVC`类基于`libsvm`库，该库实现了一个支持核技巧的[算法](https://homl.info/14)。⁠^([2](ch05.html#idm45720213447328))
    训练时间复杂度通常在*O*(*m*² × *n*)和*O*(*m*³ × *n*)之间。不幸的是，这意味着当训练实例数量变大时（例如，数十万个实例），算法会变得非常慢，因此这个算法最适合小型或中等大小的非线性训练集。它对特征数量的缩放效果很好，特别是对于稀疏特征（即每个实例具有很少的非零特征）。在这种情况下，算法的缩放大致与每个实例的平均非零特征数量成比例。'
- en: The `SGDClassifier` class also performs large margin classification by default,
    and its hyperparameters–especially the regularization hyperparameters (`alpha`
    and `penalty`) and the `learning_rate`–can be adjusted to produce similar results
    as the linear SVMs. For training it uses stochastic gradient descent (see [Chapter 4](ch04.html#linear_models_chapter)),
    which allows incremental learning and uses little memory, so you can use it to
    train a model on a large dataset that does not fit in RAM (i.e., for out-of-core
    learning). Moreover, it scales very well, as its computational complexity is *O*(*m*
    × *n*). [Table 5-1](#svm_classification_algorithm_comparison) compares Scikit-Learn’s
    SVM classification classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`SGDClassifier`类默认也执行大边距分类，其超参数，特别是正则化超参数（`alpha`和`penalty`）和`learning_rate`，可以调整以产生与线性SVM类似的结果。它使用随机梯度下降进行训练（参见[第4章](ch04.html#linear_models_chapter)），允许增量学习并且使用很少的内存，因此可以用于在RAM中无法容纳的大型数据集上训练模型（即用于外存学习）。此外，它的缩放非常好，因为其计算复杂度为*O*(*m*
    × *n*)。[表5-1](#svm_classification_algorithm_comparison)比较了Scikit-Learn的SVM分类类。'
- en: Table 5-1\. Comparison of Scikit-Learn classes for SVM classification
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1。Scikit-Learn用于SVM分类的类比较
- en: '| Class | Time complexity | Out-of-core support | Scaling required | Kernel
    trick |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 时间复杂度 | 外存支持 | 需要缩放 | 核技巧 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| `LinearSVC` | *O*(*m* × *n*) | No | Yes | No |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `LinearSVC` | *O*(*m* × *n*) | 否 | 是 | 否 |'
- en: '| `SVC` | *O*(*m*² × *n*) to *O*(*m*³ × *n*) | No | Yes | Yes |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `SVC` | *O*(*m*² × *n*) 到 *O*(*m*³ × *n*) | 否 | 是 | 是 |'
- en: '| `SGDClassifier` | *O*(*m* × *n*) | Yes | Yes | No |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `SGDClassifier` | *O*(*m* × *n*) | 是 | 是 | 否 |'
- en: Now let’s see how the SVM algorithms can also be used for linear and nonlinear
    regression.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看SVM算法如何用于线性和非线性回归。
- en: SVM Regression
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM回归
- en: 'To use SVMs for regression instead of classification, the trick is to tweak
    the objective: instead of trying to fit the largest possible street between two
    classes while limiting margin violations, SVM regression tries to fit as many
    instances as possible *on* the street while limiting margin violations (i.e.,
    instances *off* the street). The width of the street is controlled by a hyperparameter,
    *ϵ*. [Figure 5-10](#svm_regression_plot) shows two linear SVM regression models
    trained on some linear data, one with a small margin (*ϵ* = 0.5) and the other
    with a larger margin (*ϵ* = 1.2).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要将SVM用于回归而不是分类，关键是调整目标：不再试图在两个类之间拟合尽可能大的间隔同时限制间隔违规，SVM回归试图在尽可能多的实例*在*间隔上拟合，同时限制间隔违规（即实例*在*间隔之外）。间隔的宽度由超参数*ϵ*控制。[图5-10](#svm_regression_plot)显示了在一些线性数据上训练的两个线性SVM回归模型，一个具有较小的间隔（*ϵ*
    = 0.5），另一个具有较大的间隔（*ϵ* = 1.2）。
- en: '![mls3 0510](assets/mls3_0510.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0510](assets/mls3_0510.png)'
- en: Figure 5-10\. SVM regression
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10。SVM回归
- en: Reducing *ϵ* increases the number of support vectors, which regularizes the
    model. Moreover, if you add more training instances within the margin, it will
    not affect the model’s predictions; thus, the model is said to be *ϵ-insensitive*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 减小*ϵ*会增加支持向量的数量，从而对模型进行正则化。此外，如果在间隔内添加更多训练实例，不会影响模型的预测；因此，该模型被称为*ϵ-不敏感*。
- en: 'You can use Scikit-Learn’s `LinearSVR` class to perform linear SVM regression.
    The following code produces the model represented on the left in [Figure 5-10](#svm_regression_plot):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Scikit-Learn的`LinearSVR`类执行线性SVM回归。以下代码生成了左侧图中表示的模型[图5-10](#svm_regression_plot)：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To tackle nonlinear regression tasks, you can use a kernelized SVM model. [Figure 5-11](#svm_with_polynomial_kernel_plot)
    shows SVM regression on a random quadratic training set, using a second-degree
    polynomial kernel. There is some regularization in the left plot (i.e., a small
    `C` value), and much less in the right plot (i.e., a large `C` value).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理非线性回归任务，您可以使用核化的SVM模型。[图5-11](#svm_with_polynomial_kernel_plot)显示了在随机二次训练集上使用二次多项式核进行SVM回归。左图中有一些正则化（即较小的`C`值），右图中的正则化要少得多（即较大的`C`值）。
- en: '![mls3 0511](assets/mls3_0511.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0511](assets/mls3_0511.png)'
- en: Figure 5-11\. SVM regression using a second-degree polynomial kernel
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-11。使用二次多项式核的SVM回归
- en: 'The following code uses Scikit-Learn’s `SVR` class (which supports the kernel
    trick) to produce the model represented on the left in [Figure 5-11](#svm_with_polynomial_kernel_plot):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用Scikit-Learn的`SVR`类（支持核技巧）生成了左侧图中表示的模型[图5-11](#svm_with_polynomial_kernel_plot)：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `SVR` class is the regression equivalent of the `SVC` class, and the `LinearSVR`
    class is the regression equivalent of the `LinearSVC` class. The `LinearSVR` class
    scales linearly with the size of the training set (just like the `LinearSVC` class),
    while the `SVR` class gets much too slow when the training set grows very large
    (just like the `SVC` class).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`SVR`类是`SVC`类的回归等价物，`LinearSVR`类是`LinearSVC`类的回归等价物。`LinearSVR`类与训练集的大小呈线性比例（就像`LinearSVC`类一样），而`SVR`类在训练集增长非常大时变得非常慢（就像`SVC`类一样）。'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: SVMs can also be used for novelty detection, as you will see in [Chapter 9](ch09.html#unsupervised_learning_chapter).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机也可以用于新颖性检测，正如您将在[第9章](ch09.html#unsupervised_learning_chapter)中看到的那样。
- en: The rest of this chapter explains how SVMs make predictions and how their training
    algorithms work, starting with linear SVM classifiers. If you are just getting
    started with machine learning, you can safely skip this and go straight to the
    exercises at the end of this chapter, and come back later when you want to get
    a deeper understanding of SVMs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将解释SVM如何进行预测以及它们的训练算法是如何工作的，从线性SVM分类器开始。如果您刚开始学习机器学习，可以安全地跳过这部分，直接转到本章末尾的练习，并在以后想要更深入地了解SVM时再回来。
- en: Under the Hood of Linear SVM Classifiers
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性SVM分类器的内部工作原理
- en: A linear SVM classifier predicts the class of a new instance **x** by first
    computing the decision function **θ**^⊺ **x** = *θ*[0] *x*[0] + ⋯ + *θ*[*n*] *x*[*n*],
    where *x*[0] is the bias feature (always equal to 1). If the result is positive,
    then the predicted class *ŷ* is the positive class (1); otherwise it is the negative
    class (0). This is exactly like `LogisticRegression` (discussed in [Chapter 4](ch04.html#linear_models_chapter)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 线性SVM分类器通过首先计算决策函数**θ**^⊺ **x** = *θ*[0] *x*[0] + ⋯ + *θ*[*n*] *x*[*n*]来预测新实例**x**的类别，其中*x*[0]是偏置特征（始终等于1）。如果结果为正，则预测的类别*ŷ*为正类（1）；否则为负类（0）。这与`LogisticRegression`（在[第4章](ch04.html#linear_models_chapter)中讨论）完全相同。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Up to now, I have used the convention of putting all the model parameters in
    one vector **θ**, including the bias term **θ**[0] and the input feature weights
    **θ**[1] to **θ**[*n*]. This required adding a bias input *x*[0] = 1 to all instances.
    Another very common convention is to separate the bias term *b* (equal to **θ**[0])
    and the feature weights vector **w** (containing **θ**[1] to **θ**[*n*]). In this
    case, no bias feature needs to be added to the input feature vectors, and the
    linear SVM’s decision function is equal to **w**^⊺ **x** + *b* = *w*[1] *x*[1]
    + ⋯ + *w*[*n*] *x*[*n*] + *b*. I will use this convention throughout the rest
    of this book.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我一直使用将所有模型参数放在一个向量**θ**中的约定，包括偏置项**θ**[0]和输入特征权重**θ**[1]到**θ**[*n*]。这需要向所有实例添加一个偏置输入*x*[0]
    = 1。另一个非常常见的约定是将偏置项*b*（等于**θ**[0]）和特征权重向量**w**（包含**θ**[1]到**θ**[*n*]）分开。在这种情况下，不需要向输入特征向量添加偏置特征，线性SVM的决策函数等于**w**^⊺
    **x** + *b* = *w*[1] *x*[1] + ⋯ + *w*[*n*] *x*[*n*] + *b*。我将在本书的其余部分中使用这种约定。
- en: 'So, making predictions with a linear SVM classifier is quite straightforward.
    How about training? This requires finding the weights vector **w** and the bias
    term *b* that make the street, or margin, as wide as possible while limiting the
    number of margin violations. Let’s start with the width of the street: to make
    it larger, we need to make **w** smaller. This may be easier to visualize in 2D,
    as shown in [Figure 5-12](#small_w_large_margin_plot). Let’s define the borders
    of the street as the points where the decision function is equal to –1 or +1\.
    In the left plot the weight *w[1]* is 1, so the points at which *w*[1] *x*[1]
    = –1 or +1 are *x*[1] = –1 and +1: therefore the margin’s size is 2\. In the right
    plot the weight is 0.5, so the points at which *w*[1] *x*[1] = –1 or +1 are *x*[1]
    = –2 and +2: the margin’s size is 4\. So, we need to keep **w** as small as possible.
    Note that the bias term *b* has no influence on the size of the margin: tweaking
    it just shifts the margin around, without affecting its size.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用线性SVM分类器进行预测非常简单。那么训练呢？这需要找到使街道或边界尽可能宽阔的权重向量**w**和偏置项*b*，同时限制边界违规的数量。让我们从街道的宽度开始：为了使其更宽，我们需要使**w**更小。这在2D中可能更容易可视化，如[图5-12](#small_w_large_margin_plot)所示。让我们将街道的边界定义为决策函数等于-1或+1的点。在左图中，权重*w[1]*为1，因此*w*[1]
    *x*[1] = -1或+1的点是*x*[1] = -1和+1：因此边界的大小为2。在右图中，权重为0.5，因此*w*[1] *x*[1] = -1或+1的点是*x*[1]
    = -2和+2：边界的大小为4。因此，我们需要尽可能保持**w**较小。请注意，偏置项*b*对边界的大小没有影响：调整它只是移动边界，而不影响其大小。
- en: '![mls3 0512](assets/mls3_0512.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0512](assets/mls3_0512.png)'
- en: Figure 5-12\. A smaller weight vector results in a larger margin
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-12\. 较小的权重向量导致较大的边界
- en: We also want to avoid margin violations, so we need the decision function to
    be greater than 1 for all positive training instances and lower than –1 for negative
    training instances. If we define *t*^((*i*)) = –1 for negative instances (when
    *y*^((*i*)) = 0) and *t*^((*i*)) = 1 for positive instances (when *y*^((*i*))
    = 1), then we can write this constraint as *t*^((*i*))(**w**^⊺ **x**^((*i*)) +
    *b*) ≥ 1 for all instances.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望避免边界违规，因此我们需要决策函数对所有正训练实例大于1，对负训练实例小于-1。如果我们定义*t*^((*i*)) = -1为负实例（当*y*^((*i*))
    = 0时），*t*^((*i*)) = 1为正实例（当*y*^((*i*)) = 1时），那么我们可以将这个约束写为*t*^((*i*))(**w**^⊺
    **x**^((*i*)) + *b*) ≥ 1对所有实例成立。
- en: We can therefore express the hard margin linear SVM classifier objective as
    the constrained optimization problem in [Equation 5-1](#hard_margin_objective).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将硬间隔线性SVM分类器的目标表达为[方程5-1](#hard_margin_objective)中的约束优化问题。
- en: Equation 5-1\. Hard margin linear SVM classifier objective
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-1\. 硬间隔线性SVM分类器目标
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi></mrow></munder>
    <mrow><mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mi mathvariant="bold">w</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mtext>for</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi></mrow></munder>
    <mrow><mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mi mathvariant="bold">w</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mtext>for</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
- en: Note
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We are minimizing ½ **w**^⊺ **w**, which is equal to ½∥ **w** ∥², rather than
    minimizing ∥ **w** ∥ (the norm of **w**). Indeed, ½∥ **w** ∥² has a nice, simple
    derivative (it is just **w**), while ∥ **w** ∥ is not differentiable at **w**
    = 0\. Optimization algorithms often work much better on differentiable functions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最小化的是½ **w**^⊺ **w**，它等于½∥ **w** ∥²，而不是最小化∥ **w** ∥（**w**的范数）。实际上，½∥ **w**
    ∥²具有一个简单的导数（就是**w**），而∥ **w** ∥在**w** = 0处不可微。优化算法在可微函数上通常效果更好。
- en: 'To get the soft margin objective, we need to introduce a *slack variable* *ζ*^((*i*))
    ≥ 0 for each instance:⁠^([3](ch05.html#idm45720213138176)) *ζ*^((*i*)) measures
    how much the *i*^(th) instance is allowed to violate the margin. We now have two
    conflicting objectives: make the slack variables as small as possible to reduce
    the margin violations, and make ½ **w**^⊺ **w** as small as possible to increase
    the margin. This is where the `C` hyperparameter comes in: it allows us to define
    the trade-off between these two objectives. This gives us the constrained optimization
    problem in [Equation 5-2](#soft_margin_objective).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到软间隔目标，我们需要为每个实例引入一个*松弛变量* *ζ*^((*i*)) ≥ 0：⁠^([3](ch05.html#idm45720213138176))
    *ζ*^((*i*))衡量第*i*个实例允许违反边界的程度。现在我们有两个相互冲突的目标：尽量减小松弛变量以减少边界违规，同时尽量减小½ **w**^⊺ **w**以增加边界。这就是`C`超参数的作用：它允许我们定义这两个目标之间的权衡。这给我们带来了[方程5-2](#soft_margin_objective)中的约束优化问题。
- en: Equation 5-2\. Soft margin linear SVM classifier objective
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-2. 软间隔线性SVM分类器目标
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>ζ</mi></mrow></munder>
    <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mn>2</mn></mfrac></mstyle>
    <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi>
    <mo>+</mo> <mi>C</mi> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mo>-</mo> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mtext>and</mtext> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>≥</mo> <mn>0</mn> <mtext>for</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">最小化</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>ζ</mi></mrow></munder>
    <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mn>2</mn></mfrac></mstyle>
    <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi>
    <mo>+</mo> <mi>C</mi> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>受限于</mtext> <mtext>满足</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mo>-</mo> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mtext>且</mtext> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>≥</mo> <mn>0</mn> <mtext>对于</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
- en: The hard margin and soft margin problems are both convex quadratic optimization
    problems with linear constraints. Such problems are known as *quadratic programming*
    (QP) problems. Many off-the-shelf solvers are available to solve QP problems by
    using a variety of techniques that are outside the scope of this book.⁠^([4](ch05.html#idm45720213085920))
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 硬间隔和软间隔问题都是具有线性约束的凸二次优化问题。这些问题被称为*二次规划*（QP）问题。许多现成的求解器可用于通过使用本书范围之外的各种技术来解决QP问题。⁠^([4](ch05.html#idm45720213085920))
- en: 'Using a QP solver is one way to train an SVM. Another is to use gradient descent
    to minimize the *hinge loss* or the *squared hinge loss* (see [Figure 5-13](#hinge_plot)).
    Given an instance **x** of the positive class (i.e., with *t* = 1), the loss is
    0 if the output *s* of the decision function (*s* = **w**^⊺ **x** + *b*) is greater
    than or equal to 1\. This happens when the instance is off the street and on the
    positive side. Given an instance of the negative class (i.e., with *t* = –1),
    the loss is 0 if *s* ≤ –1\. This happens when the instance is off the street and
    on the negative side. The further away an instance is from the correct side of
    the margin, the higher the loss: it grows linearly for the hinge loss, and quadratically
    for the squared hinge loss. This makes the squared hinge loss more sensitive to
    outliers. However, if the dataset is clean, it tends to converge faster. By default,
    `LinearSVC` uses the squared hinge loss, while `SGDClassifier` uses the hinge
    loss. Both classes let you choose the loss by setting the `loss` hyperparameter
    to `"hinge"` or `"squared_hinge"`. The `SVC` class’s optimization algorithm finds
    a similar solution as minimizing the hinge loss.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用QP求解器是训练SVM的一种方法。另一种方法是使用梯度下降来最小化*铰链损失*或*平方铰链损失*（见[图5-13](#hinge_plot)）。给定正类别（即，*t*=1）的实例**x**，如果决策函数的输出*s*（*s*
    = **w**^⊺ **x** + *b*）大于或等于1，则损失为0。这发生在实例偏离街道并位于正侧时。给定负类别（即，*t*=-1）的实例，如果*s* ≤
    -1，则损失为0。这发生在实例偏离街道并位于负侧时。实例距离正确边界越远，损失越高：对于铰链损失，它线性增长，对于平方铰链损失，它二次增长。这使得平方铰链损失对异常值更敏感。但是，如果数据集干净，它往往会更快地收敛。默认情况下，`LinearSVC`使用平方铰链损失，而`SGDClassifier`使用铰链损失。这两个类允许您通过将`loss`超参数设置为`"hinge"`或`"squared_hinge"`来选择损失。`SVC`类的优化算法找到了与最小化铰链损失类似的解。
- en: '![mls3 0513](assets/mls3_0513.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0513](assets/mls3_0513.png)'
- en: Figure 5-13\. The hinge loss (left) and the squared hinge loss (right)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-13. 铰链损失（左）和平方铰链损失（右）
- en: 'Next, we’ll look at yet another way to train a linear SVM classifier: solving
    the dual problem.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看另一种训练线性SVM分类器的方法：解决对偶问题。
- en: The Dual Problem
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对偶问题
- en: Given a constrained optimization problem, known as the *primal problem*, it
    is possible to express a different but closely related problem, called its *dual
    problem*. The solution to the dual problem typically gives a lower bound to the
    solution of the primal problem, but under some conditions it can have the same
    solution as the primal problem. Luckily, the SVM problem happens to meet these
    conditions,⁠^([5](ch05.html#idm45720213055696)) so you can choose to solve the
    primal problem or the dual problem; both will have the same solution. [Equation
    5-3](#svm_dual_form) shows the dual form of the linear SVM objective. If you are
    interested in knowing how to derive the dual problem from the primal problem,
    see the extra material section in [this chapter’s notebook](https://homl.info/colab3).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个约束优化问题，称为*原始问题*，可以表达一个不同但密切相关的问题，称为*对偶问题*。对于对偶问题的解通常给出原始问题解的下界，但在某些条件下，它可以与原始问题具有相同的解。幸运的是，SVM问题恰好符合这些条件，⁠^([5](ch05.html#idm45720213055696))，因此您可以选择解决原始问题或对偶问题；两者都将有相同的解。[方程5-3](#svm_dual_form)显示了线性SVM目标的对偶形式。如果您想了解如何从原始问题推导出对偶问题，请参阅[本章笔记本](https://homl.info/colab3)中的额外材料部分。
- en: Equation 5-3\. Dual form of the linear SVM objective
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-3. 线性SVM目标的对偶形式
- en: <math display="block"><munder><mtext>minimize </mtext><mi mathvariant="bold">α</mi></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>α</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>⊺</mo></msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>-</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mtext>subject to </mtext><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>≥</mo><mn>0</mn><mtext> for all </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>m</mi><mtext> and </mtext><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mn>0</mn></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><munder><mtext>最小化 </mtext><mi mathvariant="bold">α</mi></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>α</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mrow></msup><msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>⊺</mo></msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>-</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mtext>受限于 </mtext><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>≥</mo><mn>0</mn><mtext> 对于所有 </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>m</mi><mtext> 和 </mtext><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mn>0</mn></math>
- en: Once you find the vector <math><mover accent="true"><mi mathvariant="bold">α</mi>
    <mo>^</mo></mover></math> that minimizes this equation (using a QP solver), use
    [Equation 5-4](#from_alpha_to_w_and_b) to compute the <math><mover accent="true"><mi
    mathvariant="bold">w</mi> <mo>^</mo></mover></math> and <math><mover accent="true"><mi>b</mi><mo>^</mo></mover></math>
    that minimize the primal problem. In this equation, *n*[*s*] represents the number
    of support vectors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到最小化这个方程的向量<math><mover accent="true"><mi mathvariant="bold">α</mi> <mo>^</mo></mover></math>（使用QP求解器），使用[方程5-4](#from_alpha_to_w_and_b)来计算最小化原始问题的<math><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>和<math><mover
    accent="true"><mi>b</mi><mo>^</mo></mover></math>。在这个方程中，*n*[*s*]代表支持向量的数量。
- en: Equation 5-4\. From the dual solution to the primal solution
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-4. 从对偶解到原始解
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <mrow><msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <mrow><msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mtd></mtr></mtable></math>
- en: The dual problem is faster to solve than the primal one when the number of training
    instances is smaller than the number of features. More importantly, the dual problem
    makes the kernel trick possible, while the primal problem does not. So what is
    this kernel trick, anyway?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练实例的数量小于特征数量时，对偶问题比原始问题更快解决。更重要的是，对偶问题使核技巧成为可能，而原始问题则不行。那么这个核技巧到底是什么呢？
- en: Kernelized SVMs
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核化支持向量机
- en: Suppose you want to apply a second-degree polynomial transformation to a two-dimensional
    training set (such as the moons training set), then train a linear SVM classifier
    on the transformed training set. [Equation 5-5](#example_second_degree_polynomial_mapping)
    shows the second-degree polynomial mapping function *ϕ* that you want to apply.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想对一个二维训练集（比如moons训练集）应用二次多项式转换，然后在转换后的训练集上训练一个线性SVM分类器。[方程5-5](#example_second_degree_polynomial_mapping)展示了你想应用的二次多项式映射函数*ϕ*。
- en: Equation 5-5\. Second-degree polynomial mapping
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-5. 二次多项式映射
- en: <math display="block"><mrow><mi>ϕ</mi> <mfenced open="(" close=")"><mi mathvariant="bold">x</mi></mfenced>
    <mo>=</mo> <mi>ϕ</mi> <mfenced open="(" close=")"><mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>x</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt>
    <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd></mtr>
    <mtr><mtd><msup><mrow><msub><mi>x</mi> <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>ϕ</mi> <mfenced open="(" close=")"><mi mathvariant="bold">x</mi></mfenced>
    <mo>=</mo> <mi>ϕ</mi> <mfenced open="(" close=")"><mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>x</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt>
    <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd></mtr>
    <mtr><mtd><msup><mrow><msub><mi>x</mi> <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced></mrow></math>
- en: Notice that the transformed vector is 3D instead of 2D. Now let’s look at what
    happens to a couple of 2D vectors, **a** and **b**, if we apply this second-degree
    polynomial mapping and then compute the dot product⁠^([6](ch05.html#idm45720212940064))
    of the transformed vectors (see [Equation 5-6](#kernel_trick_for_second_degree_polynomial_mapping)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，转换后的向量是3D而不是2D。现在让我们看看如果我们应用这个二次多项式映射，然后计算转换后向量的点积，2D向量**a**和**b**会发生什么（参见[方程5-6](#kernel_trick_for_second_degree_polynomial_mapping)）。
- en: Equation 5-6\. Kernel trick for a second-degree polynomial mapping
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-6. 二次多项式映射的核技巧
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ϕ</mi>
    <msup><mrow><mo>(</mo><mi mathvariant="bold">a</mi><mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt><msub><mi>a</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced> <mo>⊺</mo></msup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>b</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt> <msub><mi>b</mi>
    <mn>1</mn></msub> <msub><mi>b</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>b</mi>
    <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <msup><mrow><msub><mi>a</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <msup><mrow><msub><mi>b</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo> <mn>2</mn> <msub><mi>a</mi>
    <mn>1</mn></msub> <msub><mi>b</mi> <mn>1</mn></msub> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>b</mi> <mn>2</mn></msub> <mo>+</mo> <msup><mrow><msub><mi>a</mi> <mn>2</mn></msub></mrow>
    <mn>2</mn></msup> <msup><mrow><msub><mi>b</mi> <mn>2</mn></msub></mrow> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced separators="" open="("
    close=")"><msub><mi>a</mi> <mn>1</mn></msub> <msub><mi>b</mi> <mn>1</mn></msub>
    <mo>+</mo><msub><mi>a</mi> <mn>2</mn></msub> <msub><mi>b</mi> <mn>2</mn></msub></mfenced>
    <mn>2</mn></msup> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><msup><mfenced
    open="(" close=")"><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mo>⊺</mo></msup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>b</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mfenced>
    <mn>2</mn></msup> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">a</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ϕ</mi>
    <msup><mrow><mo>(</mo><mi mathvariant="bold">a</mi><mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></sup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt><msub><mi>a</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>2</mn></msub></mrow> <mn>2</mn></sup></mtd></mtr></mtable></mfenced> <mo>⊺</mo></msup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>b</mi> <mn>1</mn></sub></mrow>
    <mn>2</mn></sup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt> <msub><mi>b</mi>
    <mn>1</mn></sub> <msub><mi>b</mi> <mn>2</mn></sub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>b</mi>
    <mn>2</mn></sub></mrow> <mn>2</mn></sup></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <msup><mrow><msub><mi>a</mi> <mn>1</mn></sub></mrow> <mn>2</mn></sup> <msup><mrow><msub><mi>b</mi>
    <mn>1</mn></sub></mrow> <mn>2</mn></sup> <mo>+</mo> <mn>2</mn> <msub><mi>a</mi>
    <mn>1</mn></sub> <msub><mi>b</mi> <mn>1</mn></sub> <msub><mi>a</mi> <mn>2</mn></sub>
    <msub><mi>b</mi> <mn>2</mn></sub> <mo>+</mo> <msup><mrow><msub><mi>a</mi> <mn>2</mn></sub></mrow>
    <mn>2</mn></sup> <msup><mrow><msub><mi>b</mi> <mn>2</mn></sub></mrow> <mn>2</mn></sup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced separators="" open="("
    close=")"><msub><mi>a</mi> <mn>1</mn></sub> <msub><mi>b</mi> <mn>1</mn></sub>
    <mo>+</mo><msub><mi>a</mi> <mn>2</mn></sub> <msub><mi>b</mi> <mn>2</mn></sub></mfenced>
    <mn>2</mn></sup> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><msup><mfenced
    open="(" close=")"><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></sub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>2</mn></sub></mtd></mtr></mtable></mfenced> <mo>⊺</mo></sup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>b</mi> <mn>1</mn></sub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>2</mn></sub></mtd></mtr></mtable></mfenced></mfenced>
    <mn>2</mn></sup> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">a</mi>
    <mo>⊺</mo></sup> <mi mathvariant="bold">b</mi><mo>)</mo></mrow> <mn>2</mn></sup></mrow></mtd></mtr></mtable></math>
- en: 'How about that? The dot product of the transformed vectors is equal to the
    square of the dot product of the original vectors: *ϕ*(**a**)^⊺ *ϕ*(**b**) = (**a**^⊺
    **b**)².'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如何？转换后的向量的点积等于原始向量的点积的平方：*ϕ*(**a**)^⊺ *ϕ*(**b**) = (**a**^⊺ **b**)²。
- en: 'Here is the key insight: if you apply the transformation *ϕ* to all training
    instances, then the dual problem (see [Equation 5-3](#svm_dual_form)) will contain
    the dot product *ϕ*(**x**^((*i*)))^⊺ *ϕ*(**x**^((*j*))). But if *ϕ* is the second-degree
    polynomial transformation defined in [Equation 5-5](#example_second_degree_polynomial_mapping),
    then you can replace this dot product of transformed vectors simply by <math><msup><mrow><mo>(</mo><msup><mrow><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow>
    <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mn>2</mn></msup></math> . So, you don’t need to transform the
    training instances at all; just replace the dot product by its square in [Equation
    5-3](#svm_dual_form). The result will be strictly the same as if you had gone
    through the trouble of transforming the training set and then fitting a linear
    SVM algorithm, but this trick makes the whole process much more computationally
    efficient.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键见解是：如果将转换 *ϕ* 应用于所有训练实例，那么对偶问题（参见[方程5-3](#svm_dual_form)）将包含点积 *ϕ*(**x**^((*i*)))^⊺
    *ϕ*(**x**^((*j*)))。但如果 *ϕ* 是在[方程5-5](#example_second_degree_polynomial_mapping)中定义的二次多项式变换，那么你可以简单地用<math><msup><mrow><mo>(</mo><msup><mrow><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow>
    <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mrow></msup>
    <mo>)</mo></mrow> <mn>2</mn></msup></math>来替换这些转换后向量的点积。因此，你根本不需要转换训练实例；只需在[方程5-3](#svm_dual_form)中用其平方替换点积。结果将严格与你经历转换训练集然后拟合线性SVM算法的麻烦完全相同，但这个技巧使整个过程更加高效。
- en: The function *K*(**a**, **b**) = (**a**^⊺ **b**)² is a second-degree polynomial
    kernel. In machine learning, a *kernel* is a function capable of computing the
    dot product *ϕ*(**a**)^⊺ *ϕ*(**b**), based only on the original vectors **a**
    and **b**, without having to compute (or even to know about) the transformation
    *ϕ*. [Equation 5-7](#common_kernels) lists some of the most commonly used kernels.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*K*（**a**，**b**）=（**a**^⊺ **b**）²是一个二次多项式核。在机器学习中，*核*是一个能够基于原始向量**a**和**b**计算点积*ϕ*（**a**）^⊺
    *ϕ*（**b**）的函数，而无需计算（甚至了解）变换*ϕ*。[方程5-7](#common_kernels)列出了一些最常用的核。
- en: Equation 5-7\. Common kernels
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-7。常见核
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mtext>Linear:</mtext></mtd>
    <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo> <mi mathvariant="bold">a</mi>
    <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mtext>Polynomial:</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><mi>γ</mi><msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi><mo>+</mo><mi>r</mi></mfenced>
    <mi>d</mi></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Gaussian</mtext>
    <mtext>RBF:</mtext></mrow></mtd> <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mrow><mo>-</mo> <mi>γ</mi> <msup><mfenced separators="" open="∥"
    close="∥"><mi mathvariant="bold">a</mi><mo>-</mo><mi mathvariant="bold">b</mi></mfenced>
    <mn>2</mn></msup></mrow></mstyle> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mtext>Sigmoid:</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">tanh</mo> <mfenced separators=""
    open="(" close=")"><mi>γ</mi> <msup><mi mathvariant="bold">a</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">b</mi> <mo>+</mo> <mi>r</mi></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mtext>线性：</mtext></mtd>
    <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo> <mi mathvariant="bold">a</mi>
    <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mtext>多项式：</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><mi>γ</mi><msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi><mo>+</mo><mi>r</mi></mfenced>
    <mi>d</mi></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>高斯</mtext>
    <mtext>RBF：</mtext></mrow></mtd> <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mrow><mo>-</mo> <mi>γ</mi> <msup><mfenced separators="" open="∥"
    close="∥"><mi mathvariant="bold">a</mi><mo>-</mo><mi mathvariant="bold">b</mi></mfenced>
    <mn>2</mn></msup></mrow></mstyle> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mtext>双曲正切：</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">tanh</mo> <mfenced separators=""
    open="(" close=")"><mi>γ</mi> <msup><mi mathvariant="bold">a</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">b</mi> <mo>+</mo> <mi>r</mi></mfenced></mrow></mtd></mtr></mtable></math>
- en: There is still one loose end we must tie up. [Equation 5-4](#from_alpha_to_w_and_b)
    shows how to go from the dual solution to the primal solution in the case of a
    linear SVM classifier. But if you apply the kernel trick, you end up with equations
    that include *ϕ*(*x*^((*i*))). In fact, <math><mover accent="true"><mi mathvariant="bold">w</mi>
    <mo>^</mo></mover></math> must have the same number of dimensions as *ϕ*(*x*^((*i*))),
    which may be huge or even infinite, so you can’t compute it. But how can you make
    predictions without knowing <math><mover accent="true"><mi mathvariant="bold">w</mi>
    <mo>^</mo></mover></math> ? Well, the good news is that you can plug the formula
    for <math><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>
    from [Equation 5-4](#from_alpha_to_w_and_b) into the decision function for a new
    instance **x**^((*n*)), and you get an equation with only dot products between
    input vectors. This makes it possible to use the kernel trick ([Equation 5-8](#making_predictions_with_a_kernelized_svm)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个问题要解决。[方程5-4](#from_alpha_to_w_and_b)展示了如何在线性SVM分类器的情况下从对偶解到原始解的转换。但是如果应用核技巧，你会得到包含*ϕ*（*x*^((*i*)))的方程。事实上，<math><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>必须具有与*ϕ*（*x*^((*i*)))相同数量的维度，这可能非常庞大甚至无限，因此无法计算。但是，如何在不知道<math><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>的情况下进行预测呢？好消息是，你可以将[方程5-4](#from_alpha_to_w_and_b)中的<math><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>公式代入新实例**x**^((*n*))的决策函数中，得到一个只涉及输入向量点积的方程。这使得可以使用核技巧（[方程5-8](#making_predictions_with_a_kernelized_svm)）。
- en: Equation 5-8\. Making predictions with a kernelized SVM
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-8。使用核化SVM进行预测
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>h</mi>
    <mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover><mo>,</mo><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></msub> <mfenced separators=""
    open="(" close=")"><mi>ϕ</mi> <mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mi>ϕ</mi><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfenced>
    <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>+</mo>
    <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mfenced separators="" open="(" close=")"><mi>ϕ</mi> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfenced>
    <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr></mtable></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>h</mi>
    <mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover><mo>,</mo><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></msub> <mfenced separators=""
    open="(" close=")"><mi>ϕ</mi> <mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mi>ϕ</mi><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfenced>
    <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mrow></msup> <mo>)</mo></mrow> <mo>+</mo> <mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mfenced separators=""
    open="(" close=")"><mi>ϕ</mi> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mrow></msup>
    <mo>)</mo></mrow></mfenced> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr></mtable></math>
- en: Note that since *α*^((*i*)) ≠ 0 only for support vectors, making predictions
    involves computing the dot product of the new input vector **x**^((*n*)) with
    only the support vectors, not all the training instances. Of course, you need
    to use the same trick to compute the bias term <math><mover><mi>b</mi><mo>^</mo></mover></math>
    ([Equation 5-9](#bias_term_using_the_kernel_trick)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于*α*^((*i*)) ≠ 0 仅对支持向量有效，因此进行预测涉及计算新输入向量 **x**^((*n*)) 与仅支持向量的点积，而不是所有训练实例。当然，您需要使用相同的技巧来计算偏置项
    <math><mover><mi>b</mi><mo>^</mo></mover></math>（[方程5-9](#bias_term_using_the_kernel_trick)）。
- en: Equation 5-9\. Using the kernel trick to compute the bias term
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-9。使用核技巧计算偏置项
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn>
    <msub><mi>n</mi> <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mfenced
    separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>ϕ</mi><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow> <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></row></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn>
    <msub><mi>n</mi> <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></row></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mfenced
    separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>ϕ</mi><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow> <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></row></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></row></mstyle></mfrac>
    <mi>m</mi></munderover> <mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></mfenced></mrow></mtd></mtr></mtable></math>
- en: 'If you are starting to get a headache, that’s perfectly normal: it’s an unfortunate
    side effect of the kernel trick.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您开始头痛，那是完全正常的：这是核技巧的一个不幸的副作用。
- en: Note
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is also possible to implement online kernelized SVMs, capable of incremental
    learning, as described in the papers [“Incremental and Decremental Support Vector
    Machine Learning”](https://homl.info/17)⁠^([7](ch05.html#idm45720212562608)) and
    [“Fast Kernel Classifiers with Online and Active Learning”](https://homl.info/18).⁠^([8](ch05.html#idm45720212560496))
    These kernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinear
    problems, you may want to consider using random forests (see [Chapter 7](ch07.html#ensembles_chapter))
    or neural networks (see [Part II](part02.html#neural_nets_part)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以实现在线核化SVM，能够进行增量学习，如论文[“增量和减量支持向量机学习”](https://homl.info/17)⁠^([7](ch05.html#idm45720212562608))和[“具有在线和主动学习的快速核分类器”](https://homl.info/18)中所述。⁠^([8](ch05.html#idm45720212560496))这些核化SVM是用Matlab和C++实现的。但对于大规模非线性问题，您可能需要考虑使用随机森林（参见[第7章](ch07.html#ensembles_chapter)）或神经网络（参见[第II部分](part02.html#neural_nets_part)）。
- en: Exercises
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is the fundamental idea behind support vector machines?
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持向量机背后的基本思想是什么？
- en: What is a support vector?
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持向量是什么？
- en: Why is it important to scale the inputs when using SVMs?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用SVM时为什么重要对输入进行缩放？
- en: Can an SVM classifier output a confidence score when it classifies an instance?
    What about a probability?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVM分类器在对一个实例进行分类时能输出置信度分数吗？概率呢？
- en: How can you choose between `LinearSVC`, `SVC`, and `SGDClassifier`?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您如何在`LinearSVC`、`SVC`和`SGDClassifier`之间进行选择？
- en: Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit
    the training set. Should you increase or decrease *γ* (`gamma`)? What about `C`?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您使用RBF核训练了一个SVM分类器，但似乎对训练集欠拟合。您应该增加还是减少*γ*（`gamma`）？`C`呢？
- en: What does it mean for a model to be *ϵ-insensitive*?
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是*ϵ-insensitive*是什么意思？
- en: What is the point of using the kernel trick?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用核技巧的目的是什么？
- en: Train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and
    a `SGDClassifier` on the same dataset. See if you can get them to produce roughly
    the same model.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个线性可分数据集上训练一个`LinearSVC`。然后在相同数据集上训练一个`SVC`和一个`SGDClassifier`。看看是否可以让它们产生大致相同的模型。
- en: 'Train an SVM classifier on the wine dataset, which you can load using `sklearn.datasets.load_wine()`.
    This dataset contains the chemical analyses of 178 wine samples produced by 3
    different cultivators: the goal is to train a classification model capable of
    predicting the cultivator based on the wine’s chemical analysis. Since SVM classifiers
    are binary classifiers, you will need to use one-versus-all to classify all three
    classes. What accuracy can you reach?'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在葡萄酒数据集上训练一个SVM分类器，您可以使用`sklearn.datasets.load_wine()`加载该数据集。该数据集包含了由3个不同的种植者生产的178个葡萄酒样本的化学分析：目标是训练一个能够根据葡萄酒的化学分析预测种植者的分类模型。由于SVM分类器是二元分类器，您需要使用一对所有来对所有三个类进行分类。您能达到什么准确度？
- en: Train and fine-tune an SVM regressor on the California housing dataset. You
    can use the original dataset rather than the tweaked version we used in [Chapter 2](ch02.html#project_chapter),
    which you can load using `sklearn.datasets.fetch_california_housing()`. The targets
    represent hundreds of thousands of dollars. Since there are over 20,000 instances,
    SVMs can be slow, so for hyperparameter tuning you should use far fewer instances
    (e.g., 2,000) to test many more hyperparameter combinations. What is your best
    model’s RMSE?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加利福尼亚住房数据集上训练和微调一个SVM回归器。您可以使用原始数据集，而不是我们在[第2章](ch02.html#project_chapter)中使用的调整版本，您可以使用`sklearn.datasets.fetch_california_housing()`加载该数据集。目标代表数十万美元。由于有超过20,000个实例，SVM可能会很慢，因此在超参数调整中，您应该使用更少的实例（例如2,000）来测试更多的超参数组合。您最佳模型的RMSE是多少？
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch05.html#idm45720213459744-marker)) Chih-Jen Lin et al., “A Dual Coordinate
    Descent Method for Large-Scale Linear SVM”, *Proceedings of the 25th International
    Conference on Machine Learning* (2008): 408–415.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm45720213459744-marker)) Chih-Jen Lin等人，“用于大规模线性SVM的双坐标下降方法”，*第25届国际机器学习会议论文集*（2008年）：408–415。
- en: '^([2](ch05.html#idm45720213447328-marker)) John Platt, “Sequential Minimal
    Optimization: A Fast Algorithm for Training Support Vector Machines” (Microsoft
    Research technical report, April 21, 1998).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#idm45720213447328-marker)) John Platt，“顺序最小优化：用于训练支持向量机的快速算法”（微软研究技术报告，1998年4月21日）。
- en: ^([3](ch05.html#idm45720213138176-marker)) Zeta (*ζ*) is the sixth letter of
    the Greek alphabet.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#idm45720213138176-marker)) Zeta（*ζ*）是希腊字母表的第六个字母。
- en: ^([4](ch05.html#idm45720213085920-marker)) To learn more about quadratic programming,
    you can start by reading Stephen Boyd and Lieven Vandenberghe’s book [*Convex
    Optimization*](https://homl.info/15) (Cambridge University Press) or watching
    Richard Brown’s [series of video lectures](https://homl.info/16).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#idm45720213085920-marker)) 要了解更多关于二次规划的知识，您可以开始阅读Stephen Boyd和Lieven
    Vandenberghe的书籍[*凸优化*](https://homl.info/15)（剑桥大学出版社）或观看Richard Brown的[系列视频讲座](https://homl.info/16)。
- en: ^([5](ch05.html#idm45720213055696-marker)) The objective function is convex,
    and the inequality constraints are continuously differentiable and convex functions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.html#idm45720213055696-marker)) 目标函数是凸函数，不等式约束是连续可微的凸函数。
- en: ^([6](ch05.html#idm45720212940064-marker)) As explained in [Chapter 4](ch04.html#linear_models_chapter),
    the dot product of two vectors **a** and **b** is normally noted **a** · **b**.
    However, in machine learning, vectors are frequently represented as column vectors
    (i.e., single-column matrices), so the dot product is achieved by computing **a**^⊺**b**.
    To remain consistent with the rest of the book, we will use this notation here,
    ignoring the fact that this technically results in a single-cell matrix rather
    than a scalar value.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch05.html#idm45720212940064-marker)) 如第4章中所解释的，两个向量**a**和**b**的点积通常表示为**a**·**b**。然而，在机器学习中，向量经常被表示为列向量（即单列矩阵），因此点积通过计算**a**^⊺**b**来实现。为了与本书的其余部分保持一致，我们将在这里使用这种表示法，忽略了这实际上导致了一个单元格矩阵而不是标量值的事实。
- en: '^([7](ch05.html#idm45720212562608-marker)) Gert Cauwenberghs and Tomaso Poggio,
    “Incremental and Decremental Support Vector Machine Learning”, *Proceedings of
    the 13th International Conference on Neural Information Processing Systems* (2000):
    388–394.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch05.html#idm45720212562608-marker)) Gert Cauwenberghs和Tomaso Poggio，“增量和减量支持向量机学习”，《第13届国际神经信息处理系统会议论文集》（2000年）：388–394。
- en: '^([8](ch05.html#idm45720212560496-marker)) Antoine Bordes et al., “Fast Kernel
    Classifiers with Online and Active Learning”, *Journal of Machine Learning Research*
    6 (2005): 1579–1619.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch05.html#idm45720212560496-marker)) Antoine Bordes等人，“具有在线和主动学习的快速核分类器”，《机器学习研究杂志》6（2005年）：1579–1619。
