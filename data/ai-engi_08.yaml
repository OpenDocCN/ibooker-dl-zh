- en: Chapter 8\. Dataset Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 数据集工程
- en: The quality of a model depends on the quality of its training data. The best
    ML team in the world with infinite compute can’t help you finetune a good model
    if you don’t have data. The goal of dataset engineering is to create a dataset
    that allows you to train the best model, ideally within your allocated budget.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的质量取决于其训练数据的质量。即使拥有无限计算能力的世界上最好的机器学习团队也无法帮助你微调一个好的模型，如果你没有数据。数据集工程的目标是创建一个数据集，让你能够训练出最好的模型，理想情况下在分配的预算内。
- en: As fewer companies can afford to develop models from scratch, more are turning
    to data to differentiate their AI performance. As models demand more data, data
    handling becomes more challenging and demands more investments in talent and infrastructure.^([1](ch08.html#id1508))
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于越来越少的公司能够负担得起从头开始开发模型，越来越多的公司正在转向数据来区分其人工智能性能。随着模型对数据的需求增加，数据处理变得更加具有挑战性，并需要更多的人才和基础设施投资。[1](ch08.html#id1508)
- en: Data operations have evolved from side tasks that people handle when they have
    time to dedicated roles. Many AI companies now employ data labelers, dataset creators,
    and data quality engineers, either integrated into or working alongside their
    core engineering teams.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作已经从人们在有时间时处理的一些辅助任务发展成为专门的职位。许多人工智能公司现在雇佣数据标注员、数据集创建者和数据质量工程师，他们要么与核心工程团队集成，要么在其旁边工作。
- en: If the model landscape is confusing enough with numerous offerings, the data
    landscape is even more complex, with an ever-growing array of datasets and techniques
    being introduced. This chapter gives you an overview of the data landscape and
    considerations to take into account when building your own dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型领域已经足够复杂，有众多产品，那么数据领域则更加复杂，因为不断有越来越多的数据集和技术被引入。本章为您提供了数据领域的概述以及构建自己的数据集时应考虑的因素。
- en: It begins with data curation, addressing questions like What data do you need?
    How much? What does it mean for data to be of high quality? It then discusses
    techniques for data synthesis and processing. Data curation, generation, and processing
    don’t follow a linear path. You’ll likely have to go back and forth between different
    steps.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切始于数据整理，解决诸如你需要什么数据？需要多少？高质量数据意味着什么等问题。然后讨论数据合成和处理的技巧。数据整理、生成和处理并不遵循线性路径。你可能会在不同的步骤之间来回走动。
- en: For the same model, different training phases aim to teach the model different
    capabilities, and, therefore, require datasets with different attributes. For
    example, data quantity for pre-training is often measured in the number of tokens,
    whereas data quantity for supervised finetuning is often measured in the number
    of examples. However, at a high level, their curation processes follow the same
    principle. This chapter focuses on post-training data because that’s more relevant
    to application developers. However, I’ll also include lessons from pre-training
    data when these lessons are insightful for post-training.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同一个模型，不同的训练阶段旨在教会模型不同的能力，因此需要具有不同属性的数据集。例如，预训练的数据量通常以标记的数量来衡量，而监督微调的数据量通常以示例的数量来衡量。然而，从高层次来看，它们的整理过程遵循相同的原理。本章重点介绍训练后的数据，因为这与应用开发者更相关。然而，当这些经验教训对训练后具有洞察力时，我也会包括预训练数据的经验教训。
- en: There are best practices you can follow and tools that you can use to automate
    parts of the process. However, data will mostly just be toil, tears, and sweat.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以遵循最佳实践并使用工具来自动化过程的部分。然而，数据大部分只是辛勤的劳作、眼泪和汗水。
- en: Data Curation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理
- en: While not all issues with AI models can be solved with data, data is often a
    key part of the solution. The right data can make the model more capable, safer,
    and able to handle longer contexts. Conversely, poor data can cause the model
    to increase biases and hallucinations. Mistakes in data can harm the model and
    waste resources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非所有人工智能模型的问题都可以通过数据来解决，但数据通常是解决方案的关键部分。正确的数据可以使模型更强大、更安全，并能处理更长的上下文。相反，差的数据会导致模型增加偏差和幻觉。数据错误会损害模型并浪费资源。
- en: Data curation is a science that requires understanding how the model learns
    and what resources are available to help it learn. Dataset builders should work
    closely with application and model developers. In a small team, they might be
    the same person—the person responsible for training a model is also responsible
    for acquiring the data for it. However, organizations with high data demands often
    employ specialized roles.^([2](ch08.html#id1512))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理是一门需要理解模型如何学习和有哪些资源可以帮助它学习的科学。数据集构建者应与应用程序和模型开发者紧密合作。在一个小团队中，他们可能就是同一个人——负责训练模型的人也负责获取数据。然而，对数据需求高的组织通常雇佣专门的职位。[2](ch08.html#id1512)
- en: What data you need depends on your task and what you want to teach the model.
    For self-supervised finetuning, you need sequences of data. For instruction finetuning,
    you need data in the (instruction, response) format. For preference finetuning,
    you need data in the (instruction, winning response, losing response) format.
    To train a reward model, you can use the same data format as preference finetuning
    or use data with annotated scores for each of your examples in the ((instruction,
    response), score) format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的数据取决于你的任务以及你想要教给模型什么。对于自监督微调，你需要数据序列。对于指令微调，你需要（指令，响应）格式的数据。对于偏好微调，你需要（指令，获胜响应，失败响应）格式的数据。为了训练奖励模型，你可以使用与偏好微调相同的数据格式，或者使用带有每个示例的（指令，响应），分数）格式的数据。
- en: 'Training data should exhibit the behaviors you want your model to learn. Acquiring
    high-quality data annotations is always challenging, but it’s even more challenging
    if you want to teach models complex behaviors such as chain-of-thought (CoT) reasoning
    and tool use. Let’s go over these two examples to understand why:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据应表现出你希望模型学习的那些行为。获取高质量的数据标注始终具有挑战性，但如果你想要教模型复杂的行为，如思维链（CoT）推理和工具使用，那就更具挑战性。让我们回顾这两个例子，了解为什么：
- en: Chain-of-thought
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链
- en: As discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551),
    CoT prompting nudges the model to work through a problem step-by-step before producing
    the final answer. To teach a model to generate step-by-step responses, its training
    data should include CoT responses. “Scaling Instruction-Finetuned Language Models”
    ([Chun et al., 2024](https://oreil.ly/imdhy)) shows that incorporating step-by-step
    responses in the finetuning data greatly enhances the performance of models of
    various sizes on CoT tasks, with accuracy nearly doubling for certain tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如第5章[讨论的](ch05.html#ch05a_prompt_engineering_1730156991195551)，思维链提示会引导模型在给出最终答案之前逐步解决问题。为了教会模型生成逐步响应，其训练数据应包括思维链响应。“扩展指令微调语言模型”([Chun等人，2024](https://oreil.ly/imdhy))表明，在微调数据中包含逐步响应可以显著提高各种规模模型在思维链任务上的性能，某些任务的准确率几乎翻倍。
- en: 'Generating multi-step responses can be tedious and time-consuming—explaining
    how to solve a math problem step-by-step is much more challenging than simply
    giving the final answer. To illustrate this, here are two examples, one with only
    the final answer and one with CoT. Both are from Chun et al. (2024):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成多步响应可能会很繁琐且耗时——逐步解释如何解决数学问题比仅仅给出最终答案要更具挑战性。为了说明这一点，这里有两个例子，一个只有最终答案，另一个有CoT。这两个例子都来自Chun等人（2024年）：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As a result, CoT datasets are less common compared to other instruction datasets.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与其它指令数据集相比，思维链数据集较少见。
- en: Tool use
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用
- en: Given the vast amount of knowledge a model acquires during pre-training, many
    models might intuitively know how to use certain tools. However, a model’s tool
    use ability can be improved by showing it tool use examples. It’s common to use
    domain experts to create tool use data, where each prompt is a task that requires
    tool use, and its response is the actions needed to perform that task. For example,
    if you want data to finetune a model to act as a personal assistant, you might
    want to ask professional personal assistants what types of tasks they usually
    perform, how they perform them, and what tools they need. If you ask human experts
    to explain how they do things, they might miss certain steps, either because of
    faulty memory or because they might think these steps aren’t important. It’s often
    necessary to observe how humans perform these tasks to ensure accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到模型在预训练期间获取的大量知识，许多模型可能直观地知道如何使用某些工具。然而，通过展示工具使用示例，可以提高模型使用工具的能力。通常，使用领域专家来创建工具使用数据，其中每个提示都是一个需要使用工具的任务，其响应是执行该任务所需的动作。例如，如果您想获取微调模型以充当个人助理的数据，您可能会询问专业个人助理他们通常执行哪些任务，他们如何执行这些任务，以及他们需要哪些工具。如果您要求人类专家解释他们是如何做事的，他们可能会遗漏某些步骤，要么是因为记忆错误，要么是因为他们认为这些步骤不重要。通常，观察人类执行这些任务以确保准确性是必要的。
- en: However, what’s efficient for humans might not be efficient for AI, and vice
    versa. As a result, human annotations might not be ideal for AI agents. For example,
    a human might prefer a web interface, whereas it’s easier for a model to use an
    API. To search for something, a human might first open a browser, copy and paste
    that query into the search bar, and click on each result. Meanwhile, a model can
    just send a request to the search API with the query and process all the results
    at once. For this reason, many rely on simulations and other synthetic techniques
    to generate tool use data, as explored later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对人类有效的方法可能对AI无效，反之亦然。因此，人类标注可能不适合AI代理。例如，人类可能更喜欢Web界面，而使用API对模型来说更容易。为了搜索某物，人类可能会首先打开浏览器，将查询复制粘贴到搜索栏中，然后点击每个结果。与此同时，模型只需发送一个带有查询的请求到搜索API，并一次性处理所有结果。因此，许多人依赖于模拟和其他合成技术来生成工具使用数据，如本章后面所探讨的。
- en: Tool use data might also require special formats. In typical conversation data,
    the user and AI take turns, with each turn containing one message. However, for
    tool use, the AI might need to generate multiple messages each turn, with each
    message sent to a different location. For example, it might send one message to
    the code interpreter and one message to the user (such as to inform the user what
    it’s doing). To support this, Llama 3 authors ([Dubey et al., 2024](https://arxiv.org/abs/2407.21783))
    designed a multi-message chat format that consists of message headers that specify
    the source and destination of each message, and special termination tokens to
    specify where the human and AI turns start.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用数据可能还需要特殊的格式。在典型的对话数据中，用户和AI轮流发言，每个回合包含一条消息。然而，对于工具使用，AI可能需要在每个回合生成多条消息，每条消息发送到不同的位置。例如，它可能向代码解释器发送一条消息，向用户（例如，通知用户它在做什么）发送一条消息。为了支持这一点，Llama
    3的作者（[Dubey等人，2024](https://arxiv.org/abs/2407.21783)）设计了一种多消息聊天格式，该格式由指定每条消息来源和目的地的消息头以及用于指定人类和AI回合开始的特殊终止令牌组成。
- en: When curating data for applications with conversation interfaces, you need to
    consider whether you require single-turn data, multi-turn data, or both. Single-turn
    data helps train a model to respond to individual instructions. Multi-turn data,
    on the other hand, teaches the model how to solve tasks—many real-world tasks
    involve back-and-forth. For instance, when given a query, a model may need to
    first clarify the user’s intent before addressing the task. After the model’s
    response, the user might provide corrections or additional information for the
    next step.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当为具有对话界面的应用整理数据时，您需要考虑您是否需要单轮数据、多轮数据，或者两者都需要。单轮数据有助于训练模型响应单个指令。另一方面，多轮数据则教导模型如何解决问题——许多现实世界的问题涉及来回交流。例如，当接收到一个查询时，模型可能需要首先澄清用户的意图，然后再处理任务。在模型响应之后，用户可能会提供纠正或额外信息以供下一步使用。
- en: Single-turn data is simpler and, therefore, easier to obtain. Multi-turn data
    often requires purpose-built scenarios or more involved interactions to capture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮数据更简单，因此更容易获取。多轮数据通常需要专门构建的场景或更复杂的交互来捕捉。
- en: 'Data curation isn’t just about creating new data to help a model learn new
    behaviors but is also about removing existing data to help a model unlearn bad
    behaviors. Imagine you work on a chatbot like ChatGPT and you hear user complaints
    that the chatbot is a bit arrogant, annoying users and wasting their tokens. For
    example, when a user asks it to verify if a statement is factually correct, the
    chatbot responds with: “The statement is correct, but its style can be improved
    to be better.” It then continues to produce an unsolicited rewriting of the statement.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理不仅仅是创建新数据以帮助模型学习新行为，还涉及移除现有数据以帮助模型消除不良行为。想象一下，你正在开发一个类似于ChatGPT的聊天机器人，你听到用户投诉说聊天机器人有点傲慢，让用户感到烦恼，浪费了他们的代币。例如，当用户要求它验证一个陈述是否在事实上正确时，聊天机器人会这样回应：“这个陈述是正确的，但它的风格可以改进以更好。”然后它继续产生对陈述的非请求重写。
- en: You investigate and find that in the training data, there are several examples
    of annotations with unsolicited suggestions. You put in a request to remove these
    examples from the training data and another request to acquire new examples that
    demonstrate fact-checking without unsolicited rewriting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你调查后发现，在训练数据中，有几个带有非请求建议的注释示例。你提出请求，要求从训练数据中移除这些示例，并要求获取新的示例，以展示不带非请求重写的核实事实。
- en: 'Each application might require data of different characteristics. Different
    training phases also require different data mixes. At a high level, however, data
    curation follows the three criteria: data quality, data coverage, and data quantity.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用可能需要不同特征的数据。不同的训练阶段也要求不同的数据组合。然而，从高层次来看，数据整理遵循三个标准：数据质量、数据覆盖度和数据数量。
- en: To give an intuition about these terms, if you think of model training as cooking,
    the data fed into the model is the ingredients. Data quality is equivalent to
    the quality of the ingredients—you can’t have good food if your ingredients are
    spoiled. Data coverage is equivalent to having the right mix of ingredients (e.g.,
    you shouldn’t have too much or too little sugar). Data quantity is about how many
    ingredients you should have. Let’s explore these terms in detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这些术语有一个直观的理解，如果你将模型训练比作烹饪，那么输入到模型中的数据就是原料。数据质量等同于原料的质量——如果你的原料已经变质，你就无法做出好的食物。数据覆盖度等同于拥有合适的原料搭配（例如，你不应该放太多或太少糖）。数据数量关乎你应该有多少原料。让我们详细探讨这些术语。
- en: Data Quality
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: A small amount of high-quality data can outperform a large amount of noisy data,
    e.g., data that is irrelevant or inconsistent. The creators of the Yi model family
    found that 10K carefully crafted instructions are superior to hundreds of thousands
    of noisy instructions ([Young et al., 2024](https://arxiv.org/abs/2403.04652)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一小部分高质量数据可以超越大量噪声数据，例如无关或不一致的数据。Yi模型系列的创造者发现，10K精心设计的指令优于数十万噪声指令 ([Young et
    al., 2024](https://arxiv.org/abs/2403.04652))。
- en: 'Similarly, “LIMA: Less Is More for Alignment” ([Zhou et al., 2023](https://arxiv.org/abs/2305.11206))
    shows that a 65B-parameter Llama model, finetuned with 1,000 carefully curated
    prompts and responses, can produce answers that are either equivalent or strictly
    preferred to GPT-4 in 43% of cases, as judged by human annotators. However, the
    downside of having too few data examples is that LIMA is not as robust as product-grade
    models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '类似地，“LIMA: Less Is More for Alignment” ([Zhou et al., 2023](https://arxiv.org/abs/2305.11206))表明，一个65B参数的Llama模型，通过1,000精心挑选的提示和响应进行微调，在43%的情况下，根据人类注释者的判断，其生成的答案要么与GPT-4相当，要么更受青睐。然而，数据示例过少的缺点是LIMA不如产品级模型稳健。'
- en: The [Llama 3 team](https://arxiv.org/abs/2407.21783) also arrived at the same
    conclusion. Notably, they found that human-generated data is more prone to errors
    and inconsistencies, particularly for nuanced safety policies. This led them to
    develop AI-assisted annotation tools to ensure high data quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama 3团队](https://arxiv.org/abs/2407.21783)也得出了相同的结论。值得注意的是，他们发现人类生成数据更容易出现错误和不一致，尤其是在细微的安全政策方面。这促使他们开发AI辅助注释工具，以确保高质量的数据。'
- en: 'Most people understand the importance of data quality, but what does it mean
    for data to be high-quality? The short answer is that data is considered high-quality
    if it helps you do your job efficiently and reliably. The long answers, however,
    differ for different people.^([3](ch08.html#id1515)) In general, data can be considered
    high-quality if it has the following six characteristics: relevant, aligned with
    task requirements, consistent, correctly formatted, unique, and compliant. Some
    specific use cases might have other requirements:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人理解数据质量的重要性，但高质量数据意味着什么？简短的答案是，如果数据能帮助你高效可靠地完成工作，则被视为高质量数据。然而，不同的人对长答案的理解可能不同.^([3](ch08.html#id1515))
    通常，如果数据具有以下六个特征，则可以被认为是高质量的：相关、符合任务要求、一致、格式正确、独特和合规。某些特定用例可能还有其他要求：
- en: Relevant
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性
- en: The training examples should be relevant to the task you’re training the model
    to do. For example, if the task is to answer legal questions today, a legal dataset
    from the 19th century might not be relevant. However, if the task is about the
    legal system in the 19th century, this dataset is highly relevant.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例应与你要训练模型执行的任务相关。例如，如果任务是回答今天的法律问题，那么19世纪的法学数据集可能就不相关。然而，如果任务是关于19世纪的司法体系，这个数据集就非常相关。
- en: Aligned with task requirements
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 符合任务要求
- en: The annotations should align with the task’s requirements. For example, if the
    task requires factual consistency, the annotations should be factually correct.
    If the task requires creativity, the annotations should be creative. If the task
    demands not just a score but also a justification for that score, the annotations
    should include both scores and justifications. But if the task demands concise
    answers, the annotations should be concise.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标注应与任务要求相一致。例如，如果任务要求事实一致性，标注应事实正确。如果任务要求创造力，标注应具有创造性。如果任务不仅要求评分，还要求对评分进行解释，标注应包括评分和解释。但如果任务要求简洁的回答，标注应简洁。
- en: I used “aligned” instead of “accurate” or “correct” because, depending on the
    task, an accurate or correct response might not be what a user wants.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用“符合”而不是“准确”或“正确”，因为根据任务的不同，准确或正确的回答可能不是用户想要的。
- en: Consistent
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性
- en: Annotations should be consistent across examples and annotators. If you ask
    two annotators to annotate the same example, their annotations shouldn’t be too
    different. If the task is to score essays from 1 to 5, would two essays with the
    same score be of the same quality? Inconsistent annotations can confuse the model,
    making it harder for the model to learn.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标注应跨示例和标注者保持一致。如果你要求两位标注者对同一示例进行标注，他们的标注不应过于不同。如果任务是给论文评分从1到5，那么得分相同的两篇论文质量是否相同？不一致的标注可能会使模型困惑，使得模型更难学习。
- en: Having a good annotation guideline is essential for having annotations that
    are both aligned with task requirements and consistent.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个良好的标注指南对于确保标注既符合任务要求又保持一致至关重要。
- en: Correctly formatted
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 格式正确
- en: All examples should follow the format expected by the model. Redundant formatting
    tokens can interfere with the model’s learning, and, therefore, they should be
    removed. For example, if you scrape product reviews from a website, you should
    remove HTML tags. Beware of trailing white spaces, new lines, inconsistent casing,
    and numerical formats.^([4](ch08.html#id1516))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有示例都应遵循模型期望的格式。冗余的格式标记可能会干扰模型的学习，因此应将其删除。例如，如果你从网站上抓取产品评论，应删除HTML标签。注意尾随空格、换行、不一致的大小写和数字格式.^([4](ch08.html#id1516))
- en: Sufficiently unique
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 足够独特
- en: This refers to unique examples in your data.^([5](ch08.html#id1517)) In the
    context of model training, duplications can introduce biases and cause data contamination.
    I use “sufficiently unique” because specific use cases can tolerate different
    levels of duplications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是数据中的独特示例.^([5](ch08.html#id1517)) 在模型训练的背景下，重复可能会引入偏差并导致数据污染。我使用“足够独特”是因为特定用例可以容忍不同水平的重复。
- en: Compliant
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 合规
- en: Data should be compliant with all relevant internal and external policies (including
    laws and regulations). For example, if you’re not allowed to use PII data to train
    your models, your data shouldn’t contain any PII data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据应符合所有相关内部和外部政策（包括法律和法规）。例如，如果你不允许使用PII数据来训练你的模型，你的数据不应包含任何PII数据。
- en: Before setting out to create data, it’s important to think about what each of
    these characteristics means for you. The techniques discussed in this section
    aim to produce data with these characteristics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在着手创建数据之前，重要的是要考虑这些特征对你意味着什么。本节讨论的技术旨在产生具有这些特征的数据。
- en: Data Coverage
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据覆盖范围
- en: A model’s training data should cover the range of problems you expect it to
    solve. Real-world users often have a wide range of problems, and the way they
    express those problems can vary significantly. Having data that captures the diverse
    usage patterns of your application is key for the model to perform well. Coverage
    requires sufficient *data diversity*, which is why many refer to this attribute
    as data diversity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练数据应该涵盖你期望它解决的问题范围。现实世界的用户往往有广泛的问题，他们表达问题的方式可能差异很大。拥有捕捉你应用程序多样化使用模式的数据对于模型表现良好至关重要。覆盖范围需要足够的数据多样性，这就是为什么许多人将此属性称为数据多样性。
- en: For example, if some users construct detailed instructions with abundant references
    while some other users prefer short instructions, your finetuning data should
    include both detailed and short instructions. If user queries typically have typos,
    you should include examples with typos. If your application works with multiple
    programming languages, your training data should include the programming languages
    your users care about.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一些用户构建了包含丰富参考的详细指令，而另一些用户则更喜欢简短的指令，你的微调数据应包括详细和简短的指令。如果用户查询通常有拼写错误，你应该包括带有拼写错误的示例。如果你的应用程序与多种编程语言一起工作，你的训练数据应包括用户关心的编程语言。
- en: Different applications have different dimensions of diversity. For example,
    a French-to-English tool doesn’t need language diversity but might benefit from
    diversity in topics, lengths, and speaking styles. On the other hand, a chatbot
    that recommends products to global customers doesn’t necessarily need domain diversity,
    but linguistic and cultural diversity will be important.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用程序有不同的多样性维度。例如，一个从法语到英语的工具不需要语言多样性，但可能从主题、长度和说话风格的多样性中受益。另一方面，为全球客户提供产品推荐的聊天机器人可能不需要领域多样性，但语言和文化多样性将很重要。
- en: For general-purpose use cases like chatbots, the finetuning data should be diverse,
    representing a wide range of topics and speaking patterns. [Ding et al., (2023)](https://arxiv.org/abs/2305.14233)
    believe that the most straightforward way to further improve the performance of
    chat language models is to increase the quality and diversity of data employed
    in the training process. To develop Nemotron ([Adler et al., 2024](https://arxiv.org/abs/2406.11704)),
    NVIDIA researchers focused on creating a dataset with task diversity, topic diversity,
    and instruction diversity, which includes instructions for different output formats,
    instructions with different output lengths, and instructions for open-ended answers
    as well as yes-or-no answers. “The Data Addition Dilemma” ([Shen et al., 2024](https://www.arxiv.org/abs/2408.04154))
    demonstrated that in some cases, adding more heterogeneous data can lead to worse
    performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像聊天机器人这样的通用用途场景，微调数据应该是多样化的，代表广泛的主题和说话方式。[Ding 等人，(2023)](https://arxiv.org/abs/2305.14233)
    认为进一步提高聊天语言模型性能的最直接方法是在训练过程中增加数据的质量和多样性。为了开发 Nemotron ([Adler 等人，2024](https://arxiv.org/abs/2406.11704))，NVIDIA
    研究人员专注于创建一个具有任务多样性、主题多样性和指令多样性的数据集，这包括不同输出格式的指令、不同输出长度的指令以及开放性问题以及是或否问题的指令。“数据添加困境”
    ([Shen 等人，2024](https://www.arxiv.org/abs/2408.04154)) 证明了在某些情况下，添加更多异构数据可能会导致性能下降。
- en: 'Meta shared that [Llama 3](https://arxiv.org/abs/2407.21783) doesn’t deviate
    significantly from older Llama versions in terms of model architecture. Llama
    3’s performance gains are “primarily driven by improvements in data quality and
    diversity as well as by increased training scale.” The Llama 3 paper has rich
    details on data coverage through all three phases of training: pre-training, supervised
    finetuning, and preference finetuning. While this chapter focuses on post-training
    data, it’s useful to look at the *data mix* for the same model across all different
    training phases to compare and highlight the considerations for each phase.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Meta表示[Llama 3](https://arxiv.org/abs/2407.21783)在模型架构方面与较老的Llama版本没有显著差异。Llama
    3的性能提升“主要是由数据质量和多样性的改进以及训练规模的增加驱动的。”Llama 3论文详细介绍了数据覆盖范围，涵盖了训练的所有三个阶段：预训练、监督微调和偏好微调。虽然本章重点介绍训练后的数据，但查看同一模型在不同训练阶段的数据组合对于比较和突出每个阶段考虑因素是有用的。
- en: A diversity axis that is consistent in all three phases is domain diversity,
    though what exactly *diverse* means differs, as shown in [Table 8-1](#ch08_table_1_1730130931981135).
    This table shows only high-level domains and doesn’t include finer-grained topics,
    like “geometry”, which is a sub-category in math. Post-training data also has
    different diversity axes not shown in the table, such as the number of tokens
    (both for context and response) and the number of turns. Llama 3 uses synthetic
    data for post-training, so another dimension is the ratio of human-generated data
    to AI-generated data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三个阶段都保持一致的多样性轴是领域多样性，尽管“多样性”的确切含义有所不同，如[表8-1](#ch08_table_1_1730130931981135)所示。此表仅显示高级领域，不包括更细粒度的主题，如“几何”，这是数学的一个子类别。训练后的数据也有不同的多样性轴，这些轴在表中没有显示，例如，标记的数量（包括上下文和响应）以及轮数。Llama
    3使用合成数据用于训练后，因此另一个维度是人工生成数据与AI生成数据的比率。
- en: Table 8-1\. For Llama 3, different training phases have different optimal domain
    mixes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1。对于Llama 3，不同的训练阶段有不同的最佳领域组合。
- en: '|  | Pre-training | Supervised finetuning | Preference finetuning |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 预训练 | 监督微调 | 偏好微调 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| General knowledge (English) | 50% | 52.66% | 81.99% |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 通用知识（英语） | 50% | 52.66% | 81.99% |'
- en: '| Math and reasoning | 25% | 21.19% | 5.89% |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 数学与推理 | 25% | 21.19% | 5.89% |'
- en: '| Coding | 17% | 14.89% | 6.93% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | 17% | 14.89% | 6.93% |'
- en: '| Multilingual | 8% | 3.01% | 5.19% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 多语言 | 8% | 3.01% | 5.19% |'
- en: '| Exam-like | X | 8.14% | X |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 考试类型 | X | 8.14% | X |'
- en: '| Long context | X | 0.11% | X |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 长文本上下文 | X | 0.11% | X |'
- en: It’s interesting to note that during pre-training and supervised finetuning,
    the number of combined math, reasoning, and code tokens accounts for almost half
    of the training data. While I don’t know exactly what percentage of the internet
    data is math and code, I believe that it’s far below 50%. Llama 3 authors shared
    that *annealing* the model on small amounts of high-quality code and math data
    (training the model using an increasingly smaller learning rate with increasingly
    more code and math data) can boost the performance of their models on key benchmarks.
    This confirms a common belief that high-quality code and math data is more effective
    than natural language text in boosting the model’s reasoning capabilities.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在预训练和监督微调期间，数学、推理和代码标记的总数几乎占训练数据的一半。虽然我不知道互联网数据中数学和代码的确切百分比是多少，但我相信它远低于50%。Llama
    3的作者分享说，在少量高质量的代码和数学数据上“退火”模型（使用越来越小的学习率，同时使用越来越多的代码和数学数据进行训练）可以提高他们在关键基准上的模型性能。这证实了一个普遍的看法，即高质量的代码和数学数据在提升模型推理能力方面比自然语言文本更有效。
- en: The percentage of code and math data during preference finetuning is much smaller
    (12.82% combined), likely because the goal is to reflect the real distribution
    of user preferences.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏好微调期间，代码和数学数据的百分比要小得多（合计12.82%），这很可能是由于目标是反映用户偏好的真实分布。
- en: 'This brings up a question: How do we decide on the right data mix? A simple
    approach is to choose a data mix that accurately reflects the real-world application
    usage. You can also use experiments to find optimal data mixes. For example, Meta
    performed scaling law experiments similar to what is discussed in [“Scaling extrapolation”](ch02.html#ch02_scaling_extrapolation_1730147895572029).
    For each candidate data mix, they trained several small models on a data mix and
    used that to predict the performance of a large model on that mix. The final model
    mix is the best-guess mix derived from the experiment results.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题：我们如何决定合适的数据混合比例？一个简单的方法是选择一个能够准确反映现实世界应用使用情况的数据混合比例。你也可以通过实验来找到最佳的数据混合比例。例如，Meta进行了类似于在[“扩展外推”](ch02.html#ch02_scaling_extrapolation_1730147895572029)中讨论的扩展定律实验。对于每个候选数据混合比例，他们在数据混合上训练了几个小型模型，并使用这些模型来预测大型模型在该混合上的性能。最终的模型混合是从实验结果中得出的最佳猜测混合。
- en: To evaluate the impact of data diversity and quality, [Zhou et al. (2023)](https://arxiv.org/abs/2305.11206)
    carried out an interesting experiment where they trained a 7B-parameter language
    model on three datasets of the same size—2,000 examples—but with different characteristics.
    The first is high-quality but not diverse. The second is diverse but low-quality.
    The third is both diverse and high-quality. [Figure 8-1](#ch08_figure_1_1730130931958804)
    shows the generation quality of the three resulting models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据多样性和质量的影响，[周等（2023）](https://arxiv.org/abs/2305.11206)进行了一个有趣的实验，他们在三个大小相同但特征不同的数据集上训练了一个70亿参数的语言模型——每个数据集有2,000个示例。第一个是高质量但不多样化。第二个是多样化但低质量。第三个是既多样化又高质量。[图8-1](#ch08_figure_1_1730130931958804)显示了三个结果模型的生成质量。
- en: '![A graph of different sizes of blue and red bars'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![不同大小的蓝色和红色条形图'
- en: Description automatically generated with medium confidence](assets/aien_0801.png)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成中等置信度的描述](assets/aien_0801.png)
- en: Figure 8-1\. A 7B-parameter model, finetuned on a dataset that is both high-quality
    and diverse, outperforms that same model finetuned on a dataset that is either
    diverse or high-quality. Image from Zhou et al. (2023). The image is licensed
    under CC BY 4.0.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 在一个高质量且多样化的数据集上微调的70亿参数模型，其性能优于在多样化或高质量数据集上微调的同一模型。图片来自周等（2023年）。此图片根据CC
    BY 4.0许可。
- en: Data Quantity
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据量
- en: Asking how much data you need is like asking how much money you need. The answer
    varies widely from one situation to the next. At one extreme, [Jeremy Howard and
    Jonathan Whitaker](https://oreil.ly/mUEJO) did a fun experiment to show that LLMs
    can learn from a single example. At another extreme, some teams have finetuned
    models with millions of examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 询问你需要多少数据就像询问你需要多少钱一样。答案因情况而异。在一端，[杰里米·霍华德和乔纳森·惠特克](https://oreil.ly/mUEJO)进行了一个有趣的实验，以展示LLMs可以从单个示例中学习。在另一端，一些团队使用数百万个示例微调模型。
- en: While millions of examples sounds like a lot, it’s small compared to the data
    typically needed to train a foundation model from scratch. For reference, Llama
    2 and Llama 3 were trained using 2 trillion and 16 trillion tokens, respectively.
    If each example is 2,000 tokens, it’d be equivalent to 1 billion and 15 billion
    examples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数百万个示例听起来很多，但与从头开始训练基础模型所需的数据量相比，这仍然很小。以参考为例，Llama 2和Llama 3分别使用了2万亿和16万亿个标记进行训练。如果每个示例是2,000个标记，那么它相当于10亿和150亿个示例。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You might wonder: if I have millions of examples, shouldn’t I just train a
    model from scratch? You can and should evaluate whether training a model from
    scratch would improve your performance. While finetuning on top of a pre-trained
    model is typically more efficient than training from scratch, there are situations
    when finetuning can be worse, especially when you have a lot of training data.
    This is due to a phenomenon called *ossification*, where pre-training can *ossify*
    (i.e., freeze) the model weights so that they don’t adapt as well to the finetuning
    data ([Hernandez et al., 2021](https://arxiv.org/abs/2102.01293)). Smaller models
    are more susceptible to ossification than larger models.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果我有很多示例，难道我不应该从头开始训练一个模型吗？你可以也应该评估从头开始训练模型是否会提高你的性能。虽然基于预训练模型进行微调通常比从头开始训练更高效，但在某些情况下，微调可能会更差，尤其是在你有大量训练数据时。这是由于一种称为*僵化*的现象，其中预训练可能会*僵化*（即，冻结）模型权重，使得它们不能很好地适应微调数据（[Hernandez等，2021](https://arxiv.org/abs/2102.01293)）。较小的模型比较大的模型更容易受到僵化的影响。
- en: 'Other than data quality and data diversity, three other factors influence how
    much data you need:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据质量和数据多样性之外，还有三个其他因素会影响你需要多少数据：
- en: Finetuning techniques
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术
- en: Full finetuning promises to give the best performance, but it requires orders
    of magnitude more data than PEFT methods like LoRA. If you have tens of thousands
    to millions of (instruction, response) pairs, you might want to attempt full finetuning.
    If you have only a few hundred or a few thousand examples, PEFT might work best.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 全量微调承诺提供最佳性能，但它需要比LoRA等PEFT方法多几个数量级的数据。如果你有数千到数百万（指令，响应）对，你可能想尝试全量微调。如果你只有几百或几千个示例，PEFT可能效果最佳。
- en: Task complexity
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 任务复杂性
- en: A simple task, such as classifying whether a product review is positive or negative,
    will require much less data than a complex task, such as a question answering
    about financial filings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的任务，例如判断产品评论是正面还是负面，将需要比一个复杂任务（例如，关于财务报告的问答）更少的数据。
- en: Base model’s performance
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型性能
- en: The closer the base model is to the desirable performance, the fewer examples
    are needed to get there. Assuming that bigger base models are better, you might
    need fewer examples to finetune big models. This is the opposite of pre-training,
    where bigger models need more training data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型越接近期望的性能，所需的示例就越少。假设更大的基础模型更好，你可能需要更少的示例来微调大型模型。这与预训练相反，更大的模型需要更多的训练数据。
- en: '[OpenAI’s finetuning guide](https://oreil.ly/-R3Wd) shows that if you have
    fewer examples (100), more advanced models give you better finetuning performance.
    This is likely because the more advanced models already perform better out of
    the box. However, after finetuning on a lot of examples (550,000), all five models
    in the experiment performed similarly, as illustrated in [Figure 8-2](#ch08_figure_2_1730130931958852).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI的微调指南](https://oreil.ly/-R3Wd)显示，如果你有更少的示例（100个），更先进的模型会提供更好的微调性能。这很可能是由于更先进的模型在开箱即用的情况下已经表现更好。然而，在大量示例（550,000个）上进行微调后，实验中的所有五个模型的表现相似，如图[图8-2](#ch08_figure_2_1730130931958852)所示。'
- en: '![A graph of different colored bars'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![不同颜色条形图的图表'
- en: Description automatically generated](assets/aien_0802.png)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0802.png)
- en: Figure 8-2\. With 100 examples, more advanced models give much better performance
    after finetuning. With 550,000 examples, all models give similar performance after
    finetuning. Experiments done by Stanford Natural Language Inference (SNLI) Corpus.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2。使用100个示例，更先进的模型在微调后提供了更好的性能。使用550,000个示例，所有模型在微调后都提供了相似的性能。由斯坦福自然语言推理（SNLI）语料库进行的实验。
- en: In short, if you have a small amount of data, you might want to use PEFT methods
    on more advanced models. If you have a large amount of data, use full finetuning
    with smaller models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果你只有少量数据，你可能想在更先进的模型上使用PEFT方法。如果你有大量数据，使用较小模型的全量微调。
- en: Before investing in curating a large dataset, you might want to start with a
    small, well-crafted dataset (e.g., 50 examples) to see if finetuning can improve
    the model. If this small dataset is sufficient to achieve your desirable performance,
    that’s great. Clear improvements suggest that more data will improve the performance
    even more. If no improvement is observed with small data, a bigger dataset will
    rarely do the trick.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在投资创建大型数据集之前，你可能想从一个小型、精心制作的数据集（例如，50个示例）开始，看看微调是否可以提高模型性能。如果这个小型数据集足以实现你期望的性能，那就太好了。明显的改进表明，更多的数据将进一步提高性能。如果没有观察到小数据集的改进，更大的数据集很少能起到作用。
- en: However, be careful before concluding that finetuning with a small dataset doesn’t
    improve a model. Many things, other than data, can impact finetuning’s results,
    such as the choice of hyperparameters (e.g., the learning rate is too high or
    too low), data quality, poorly crafted prompts, etc. *In the vast majority of
    cases, you should see improvements after finetuning with 50–100 examples.*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在得出使用小数据集微调不会提高模型性能的结论之前要小心。除了数据之外，许多其他因素都可能影响微调的结果，例如超参数的选择（例如，学习率过高或过低）、数据质量、制作不佳的提示等。*在绝大多数情况下，使用50-100个示例进行微调后，你应该看到改进。*
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'It’s possible to reduce the amount of high-quality data needed by first finetuning
    your model using lower-quality or less-relevant data. Here are three examples
    of this approach:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先使用低质量或不太相关的数据进行微调，可以减少所需高质量数据量。以下是这种方法的三个示例：
- en: Self-supervised → supervised
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督 → 监督学习
- en: You want to finetune a model to answer legal questions. Your (question, answer)
    set is small, but you have many legal documents. You can first finetune your model
    on legal documents in a self-supervised manner, then further finetune the model
    on (question, answer) pairs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望微调一个模型以回答法律问题。你的（问题，答案）集很小，但你有很多法律文件。你可以首先以自监督的方式在法律文件上微调你的模型，然后进一步在（问题，答案）对上微调模型。
- en: Less-relevant data → relevant data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不太相关的数据 → 相关数据
- en: You want to finetune a model to classify sentiments for product reviews, but
    you have little product sentiment data and much more tweet sentiment data. You
    can first finetune your model to classify tweet sentiments, then further finetune
    it to classify product sentiments.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望微调一个模型以对产品评论进行情感分类，但你只有很少的产品情感数据，而有很多推文情感数据。你可以首先微调你的模型以分类推文情感，然后进一步微调它以分类产品情感。
- en: Synthetic data → real data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据 → 真实数据
- en: You want to finetune a model to predict medical conditions from medical reports.
    Due to the sensitive nature of this task, your data is limited. You can use AI
    models to synthesize a large amount of data to finetune your model first, then
    further finetune it on your real data. This approach is harder to get right, as
    you’ll have to do two distinct finetuning jobs while coordinating the transitioning
    between them. If you don’t know what you’re doing, you might end up using more
    compute just to produce a model worse than what you would’ve gotten by just finetuning
    with high-quality data.^([6](ch08.html#id1523))
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望微调一个模型以从医疗报告中预测医疗状况。由于这项任务的敏感性，你的数据有限。你可以使用AI模型合成大量数据以首先微调你的模型，然后进一步在你的真实数据上微调。这种方法更难正确实施，因为你将不得不同时进行两次不同的微调工作，并协调它们之间的过渡。如果你不知道自己在做什么，你可能会使用更多的计算资源，结果却得到一个比仅使用高质量数据进行微调更差的模型。[^6](ch08.html#id1523)
- en: Experimenting with a small dataset can help you estimate how much more data
    you’ll need. You can finetune a model on subsets of your current dataset—e.g.,
    25%, 50%, 100%—and plot how performance scales with dataset size. A steep performance
    gain slope with increasing dataset size means that you can expect significant
    performance improvement by doubling your data. A plateau slope means that doubling
    your data will give only a small improvement. [Figure 8-3](#ch08_figure_3_1730130931958912)
    shows an example of this plot.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在小数据集上进行实验可以帮助你估计你需要多少额外数据。你可以在当前数据集的子集上微调模型——例如，25%，50%，100%——并绘制性能随数据集大小变化的曲线。随着数据集大小的增加，陡峭的性能提升斜率意味着你可以预期通过加倍数据量实现显著的性能提升。平台斜率意味着加倍数据量只会带来小幅度的改进。[图8-3](#ch08_figure_3_1730130931958912)
    展示了这种曲线的一个例子。
- en: '![A graph with a line'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![一条线的图表'
- en: Description automatically generated](assets/aien_0803.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0803.png)
- en: Figure 8-3\. The performance gain curve with different dataset sizes can help
    you estimate the impact of additional training examples on your model’s performance.
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. 不同数据集大小下的性能提升曲线可以帮助你估计额外训练示例对模型性能的影响。
- en: 'The performance gain curve shown in [Figure 8-3](#ch08_figure_3_1730130931958912)
    is fairly typical. In most cases, additional training examples yield diminishing
    returns: the same number of examples typically gives a lower performance boost
    as the dataset grows. For example, the first 1,000 examples might improve a model’s
    accuracy by ten percentage points, but the next 1,000 examples might only improve
    it by five.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#ch08_figure_3_1730130931958912) 所示的性能提升曲线相当典型。在大多数情况下，额外的训练示例会产生递减的回报：随着数据集的增长，相同数量的示例通常只能带来较低的性能提升。例如，前1000个示例可能会将模型的准确率提高十个百分点，但接下来的1000个示例可能只能提高五个。'
- en: While a larger number of finetuning examples generally improves a model’s performance,
    the diversity of the examples matters, too. The paper “Scaling Instruction-Finetuned
    Language Models” ([Chung et al., 2022](https://arxiv.org/abs/2210.11416)) shows
    that model performance increased significantly when the number of finetuning tasks
    increased from 9 to 282\. Beyond 282 tasks, the performance gains started to plateau,
    though there were still positive but incremental improvements up to 1,836 tasks,
    as shown in [Figure 8-4](#ch08_figure_4_1730130931958939). This suggests that
    the model benefits greatly from exposure to a diverse set of tasks during finetuning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然更多的微调示例通常可以提高模型性能，但示例的多样性也同样重要。论文“扩展指令微调语言模型”（[Chung等，2022](https://arxiv.org/abs/2210.11416)）显示，当微调任务的数量从9增加到282时，模型性能显著提高。超过282个任务后，性能提升开始趋于平稳，尽管在1,836个任务时仍有正面的但渐进的改进，如图[图8-4](#ch08_figure_4_1730130931958939)所示。这表明模型在微调期间接触到多样化的任务时受益匪浅。
- en: The diversity of data can be reflected in task types (such as summarization
    and question answering), topic diversity (such as fashion, finance, and technology),
    and the expected output formats (such as JSON outputs or yes-or-no answers).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的多样性可以反映在任务类型（如摘要和问答）、主题多样性（如时尚、金融和技术）以及预期的输出格式（如JSON输出或是与否的答案）。
- en: '![A graph of numbers and a number of finetuning tasks'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个数字和微调任务数量的图表'
- en: Description automatically generated](assets/aien_0804.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0804.png)
- en: Figure 8-4\. Diversity in finetuning number, measured by the number of tasks,
    can impact model performance. Image from “Scaling Instruction-Finetuned Language
    Models” (Chung et al., 2022). The image is licensed under CC BY 4.0.
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 通过任务数量衡量的微调数量多样性可能会影响模型性能。图片来自“扩展指令微调语言模型”（Chung等，2022年）。此图片根据CC BY 4.0许可。
- en: How much data to use for finetuning is determined not just by what you need
    but also by what you can afford. If you budget $10,000 for data annotation and
    each example costs $2 to annotate, you can have at most 5,000 examples. You might
    also need to balance the budget for data and compute. Spending more money on data
    leaves you less money for compute, and vice versa.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调的数据量不仅取决于您需要什么，还取决于您能负担什么。如果您为数据标注预算了10,000美元，每个示例标注成本为2美元，您最多可以有5,000个示例。您可能还需要平衡数据和计算预算。在数据上花费更多资金会减少您用于计算的资金，反之亦然。
- en: Data Acquisition and Annotation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据获取和标注
- en: 'The goal of data acquisition is to produce a sufficiently large dataset with
    the quality and diversity you need, while ensuring that your data practices respect
    user privacy and comply with regulations. Data acquisition involves gathering
    data through methods such as sourcing public data, purchasing proprietary data,
    annotating data, and synthesizing data. There’s a niche but growing field of research
    in *data acquisition strategy*: how to best acquire a dataset that meets specific
    requirements given a budget.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据获取的目标是生产一个足够大的数据集，该数据集具有您所需的质量和多样性，同时确保您的数据实践尊重用户隐私并符合法规。数据获取涉及通过诸如获取公共数据、购买专有数据、标注数据和合成数据等方法收集数据。在*数据获取策略*领域有一个利基但正在增长的研究领域：如何在预算限制下最佳地获取满足特定要求的数据集。
- en: The most important source of data, however, is typically data from your own
    application. If you can figure out a way to create a *data flywheel* that leverages
    data generated by your users to continually improve your product, you will gain
    a significant advantage.^([7](ch08.html#id1526)) Application data is ideal because
    it’s perfectly relevant and aligned with your task. In other words, it matches
    the distribution of the data that you care about, which is incredibly hard to
    achieve with other data sources. User-generated data can be user content, system-generated
    data from user usage, or user feedback. How to design your user feedback system
    is discussed in [Chapter 10](ch10.html#ch10_ai_engineering_architecture_and_user_feedback_1730130985311851).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的数据来源通常是来自您自己的应用程序的数据。如果您能想出一种方法来创建一个利用用户生成数据不断改进您产品的*数据飞轮*，您将获得显著的优势。[7](ch08.html#id1526)
    应用数据是理想的，因为它与您的任务完美相关且一致。换句话说，它匹配了您关心的数据分布，而用其他数据源实现这一点极其困难。用户生成数据可以是用户内容、用户使用产生的系统数据或用户反馈。如何设计您的用户反馈系统将在[第10章](ch10.html#ch10_ai_engineering_architecture_and_user_feedback_1730130985311851)中讨论。
- en: 'Before investing in creating your own data, check available datasets first.
    Data marketplaces are vast and offer both open source and proprietary data. If
    you’re lucky, some of them might be exactly what you need. However, it’s often
    a mix-and-match approach. A dataset can be developed from multiple data sources
    via multiple acquisition channels. For example, the process of creating an (instruction,
    response) dataset might look as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在投资创建自己的数据之前，首先检查可用的数据集。数据市场非常庞大，提供开源和专有数据。如果你很幸运，其中一些可能正是你所需要的。然而，通常需要混合匹配。数据集可以通过多个获取渠道从多个数据源开发。例如，创建一个（指令，响应）数据集的过程可能如下所示：
- en: Find available datasets with the desirable characteristics. You might find one
    promising dataset with 10,000 examples.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找具有所需特性的可用数据集。你可能会找到一个包含10,000个示例的有前景的数据集。
- en: Remove low-quality instructions. Let’s say this leaves you with 9,000 examples.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除低质量的指令。比如说这让你剩下9,000个示例。
- en: Set aside the instructions with low-quality responses. Let’s say you find 3,000
    such examples. This leaves you with 6,000 examples of high-quality instructions
    and high-quality responses.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将低质量的指令留出。比如说你找到了3,000个这样的示例。这让你剩下6,000个高质量的指令和高质量的响应。
- en: Manually write responses for the 3,000 high-quality instructions. Now your dataset
    has a total of 9,000 high-quality examples.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为3,000个高质量的指令手动编写响应。现在你的数据集总共有9,000个高质量的示例。
- en: Realizing that there’s not enough data for topic X, manually create a set of
    100 instruction templates about X. Use an AI model to synthesize 2,000 instructions
    using these 10 templates.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 认识到对于主题X来说数据不足，手动创建一组关于X的100个指令模板。使用AI模型使用这10个模板合成2,000个指令。
- en: Manually annotate these 2,000 synthetic instructions. Now your dataset has a
    total of 11,000 examples.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动标注这2,000个合成指令。现在你的数据集总共有11,000个示例。
- en: This is, of course, an oversimplification of the actual dataset curation process,
    with the vast majority of steps hidden to conserve paper and save readers from
    tedium. For example, there might be several steps in which you realize that many
    of the annotations aren’t helpful, so you have to update the annotation guidelines
    and re-annotate your data. Worse, you might find that some of them are factually
    incorrect, so you have to hire another set of annotators to fact-check your original
    annotations. Or you might find that having 100 synthetic instructions per template
    hurts your data’s diversity, so you have to create more templates and generate
    fewer instructions per template. And so on.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然是对实际数据集整理过程的过度简化，因为大多数步骤都被隐藏起来以节省纸张并避免让读者感到厌烦。例如，可能会有几个步骤，你意识到许多标注没有帮助，因此你必须更新标注指南并重新标注你的数据。更糟糕的是，你可能会发现其中一些实际上是错误的，因此你必须雇佣另一组标注员来核实你的原始标注。或者你可能会发现每个模板有100个合成指令会损害你数据的多样性，因此你必须创建更多的模板并减少每个模板生成的指令数量。等等。
- en: Often, you might need to annotate your own data for finetuning. Annotation is
    challenging not just because of the annotation process but also due to the complexity
    of creating clear annotation guidelines. For example, you need to explicitly state
    what a good response looks like, and what makes it good. Can a response be correct
    but unhelpful? What’s the difference between responses that deserve a score of
    3 and 4? Annotation guidelines are needed for both manual and AI-powered annotations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可能需要为自己的数据标注以进行微调。标注不仅因为标注过程具有挑战性，还因为创建清晰的标注指南的复杂性。例如，你需要明确说明一个好的响应是什么样的，以及是什么让它变得好。一个响应可以是正确的但无用的吗？3分和4分的响应之间有什么区别？需要为手动和AI辅助标注提供标注指南。
- en: Some teams, including [LinkedIn](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US),
    have reported that annotation guidelines were among the most challenging parts
    of their AI engineering pipeline. It’s alarming how often people abandon careful
    annotation halfway due to the time and effort required, hoping instead that their
    models will figure out the right responses on their own. Many models are strong
    enough that they can occasionally succeed, but relying on models to figure that
    out might be too risky for many applications.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队，包括[LinkedIn](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US)，报告称标注指南是他们AI工程流程中最具挑战性的部分之一。人们因所需的时间和精力而经常在标注过程中半途而废，希望模型能自行找出正确的响应。许多模型足够强大，有时可以成功，但依赖模型自行找出这些可能对许多应用来说风险太大。
- en: The good news is that these guidelines are the same as those for evaluation
    data, as discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863).
    This is another argument for why you should invest more time in curating evaluation
    guidelines and data. If you’re lucky, your evaluation examples can be augmented
    or used as seed examples to synthesize new data. In the next section we’ll discuss
    how to do so.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，这些指南与第4章中讨论的评估数据指南相同。这是为什么你应该花更多时间精心制作评估指南和数据的原因之一。如果你幸运的话，你的评估示例可以被增强或用作种子示例来合成新数据。在下一节中，我们将讨论如何做到这一点。
- en: Data Augmentation and Synthesis
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强与合成
- en: Together with compute and talent, data is the hardest challenge of AI. It’s
    been a long-term goal of the whole industry to be able to generate data programmatically.
    Two processes commonly used are *data augmentation* and *data synthesis:*
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算和人才一样，数据是AI最艰巨的挑战。整个行业长期的目标是能够以编程方式生成数据。常用的两种过程是*数据增强*和*数据合成*：
- en: Data augmentation creates new data from existing data (which is real). For example,
    given a real image of a cat, you can flip it to create a new image of the same
    cat.^([8](ch08.html#id1535))
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强是从现有数据（真实的）中创建新数据。例如，给定一张真实的猫的图片，你可以翻转它来创建同一只猫的新图片.^([8](ch08.html#id1535))
- en: Data synthesis generates data to mimic the properties of real data. For example,
    you can simulate how a mouse moves through a web page to generate data for what
    bot movements would look like.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据合成生成数据以模拟真实数据的特性。例如，你可以模拟鼠标在网页中移动的方式，以生成模拟机器人运动的数据。
- en: In other words, augmented data is derived from real data, whereas synthetic
    data isn’t real. However, since the goal of both augmentation and synthesis is
    to automate data creation, sometimes the two terms are used interchangeably. In
    this chapter, I’ll often use data synthesis to refer to both.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，增强数据是从真实数据中派生出来的，而合成数据则不是真实的。然而，由于增强和合成的目标都是自动化数据创建，有时这两个术语会被互换使用。在本章中，我经常使用数据合成来指代两者。
- en: Artificially generated data has a long history in software engineering. It was
    originally used to generate fake data for testing purposes. For example, libraries
    like [*Faker*](https://github.com/joke2k/faker) and [*Chance*](https://chancejs.com)
    let you generate data in simple formats such as names, addresses, phone numbers,
    and email addresses for testing. Let’s say you’ve built a program to parse shipping
    addresses. You can use fake data generators to generate addresses in different
    countries and states with different formats to make sure your program can parse
    all of them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 人工生成数据在软件工程中有着悠久的历史。最初，它被用于生成用于测试目的的假数据。例如，像[*Faker*](https://github.com/joke2k/faker)和[*Chance*](https://chancejs.com)这样的库允许你以简单的格式（如姓名、地址、电话号码和电子邮件地址）生成数据以进行测试。假设你已经构建了一个解析运输地址的程序。你可以使用假数据生成器生成不同国家、不同格式的地址，以确保你的程序可以解析所有这些地址。
- en: With AI being capable of generating data indistinguishable from that generated
    by humans, it’s possible to synthesize much more sophisticated data, such as doctor’s
    notes, contracts, financial statements, product descriptions, images, video commercials,
    etc. This makes it easier to generate data and enables more synthetic data use
    cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AI能够生成与人类生成数据无法区分的数据，因此可以合成更复杂的数据，例如医生的笔记、合同、财务报表、产品描述、图像、视频广告等。这使得生成数据更加容易，并使得更多合成数据用例成为可能。
- en: While synthetic data promises to significantly reduce the pressure for human-generated
    data, synthetic data doesn’t completely replace human data. In many use cases,
    as discussed in [“Limitations to AI-generated data”](#ch08_limitations_to_ai_generated_data_1730130932021346),
    mixing human- and AI-generated data often produces the best value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然合成数据承诺可以显著减少对人类生成数据的压力，但它并不能完全取代人类数据。在许多用例中，如“AI生成数据的局限性”（#ch08_limitations_to_ai_generated_data_1730130932021346）中讨论的那样，混合人类和AI生成数据通常会产生最佳价值。
- en: Why Data Synthesis
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么需要数据合成
- en: 'Synthetic data is appealing for many reasons. You can synthesize data to improve
    the golden data trio: quantity, coverage, and quality. You can also synthesize
    data to mitigate privacy concerns and distill models:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据因其许多原因而具有吸引力。您可以通过合成数据来提升黄金数据三要素：数量、覆盖范围和质量。您还可以通过合成数据来缓解隐私问题并提炼模型：
- en: To increase data quantity
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加数据数量
- en: The biggest reason for data synthesis is that it allows you to produce data
    at scale, promising an abundant supply of data for training and testing AI models.
    More data, in theory, helps models generalize to a wider range of tasks. This
    is especially helpful where real-world data is scarce or difficult to obtain,
    such as data for rare weather conditions, data for deep sea exploration, or data
    involving accidents for self-driving cars.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合成的最大原因是它允许您大规模地生成数据，承诺为训练和测试AI模型提供充足的数据供应。理论上，更多的数据有助于模型泛化到更广泛的任务。这在现实世界数据稀缺或难以获取的情况下特别有帮助，例如罕见天气条件的数据、深海探索的数据或涉及自动驾驶汽车事故的数据。
- en: To increase data coverage
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加数据覆盖范围
- en: You can generate data with targeted characteristics to improve model performance
    or to get a model to express specific behaviors. For example, you can generate
    very short texts or very long texts. You can create conversations that contain
    toxic phrases for a toxic detection model. Vice versa, if real-world data is toxic,
    you can synthesize safe data. It’s especially common to use AI to synthesize adversarial
    examples. It’s also possible to generate data for the rare class to address the
    challenges of class imbalance. As described in “TrueTeacher”, [Gekhman et al.
    (2022)](https://arxiv.org/abs/2305.11171) used LLMs to generate factually inconsistent
    summaries that they then used to train models to detect factual inconsistency.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以生成具有针对性特征的数据来提高模型性能或使模型表达特定的行为。例如，您可以生成非常短或非常长的文本。您可以创建包含有害语句的对话，用于有害检测模型。反之，如果现实世界的数据是有害的，您可以合成安全数据。使用AI合成对抗性示例尤为常见。也有可能为罕见类别生成数据，以解决类别不平衡的挑战。正如“TrueTeacher”中描述的，[Gekhman等人（2022）](https://arxiv.org/abs/2305.11171)使用LLM生成事实不一致的摘要，然后使用这些摘要来训练检测事实不一致性的模型。
- en: In their paper, “Discovering Language Model Behaviors with Model-Written Evaluations”
    ([Perez et al., 2022](https://arxiv.org/abs/2212.09251)), Anthropic discussed
    various data synthesis techniques to generate specific datasets that can test
    154 different AI behaviors, including personality traits, political views, ethical
    stances, and social biases. They found that in head-to-head comparisons between
    LM (language model)-generated and human-generated datasets, “LM-written datasets
    approach the quality of human-written ones, sometimes even exceeding them.”
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文“使用模型编写的评估发现语言模型行为”（[Perez等人，2022](https://arxiv.org/abs/2212.09251)）中，Anthropic讨论了各种数据合成技术，以生成可以测试154种不同AI行为的特定数据集，包括性格特征、政治观点、伦理立场和社会偏见。他们发现，在与LM（语言模型）生成数据和人类生成数据集的面对面比较中，“LM编写的数据集接近人类编写的质量，有时甚至超过它们。”
- en: 'In other words, you can use synthetic data to increase data coverage: generate
    targeted data to cover the areas where existing data is insufficient.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，您可以使用合成数据来增加数据覆盖范围：生成针对性数据以覆盖现有数据不足的领域。
- en: To increase data quality
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高数据质量
- en: Even though the common perception is that synthetic data is often of lower quality
    than human-generated data, sometimes, the reverse can be true. *Sometimes, humans
    might have fundamental limitations that cause human-generated data to be of lower
    quality than AI-generated data.* One example is tool use data discussed earlier—humans
    and AI have fundamentally different modes of operations and tool preferences.
    Another example is in generating complex math problems—AI can generate questions
    that are far more complex than what an average human expert might conceive.^([9](ch08.html#id1536))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管普遍认为合成数据的质量通常低于人类生成数据，但有时情况可能相反。*有时，人类可能存在根本性的局限性，导致人类生成数据的质量低于AI生成数据。* 一个例子是之前讨论过的工具使用数据——人类和AI在操作模式和工具偏好上有根本性的不同。另一个例子是在生成复杂数学问题方面——AI可以生成比普通人类专家可能想象的要复杂得多的题目。[9](ch08.html#id1536)
- en: Some teams also prefer using AI to generate preference data. While each individual
    human can be somewhat consistent in their preference, performance across different
    people tends to vary significantly, influenced not only by each person’s preference
    but also by mood and motivations. AI-generated preference ratings, in contrast,
    can be far more consistent and reliable.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队也更倾向于使用AI来生成偏好数据。虽然每个人在偏好上可能有一定的连贯性，但不同人之间的表现往往差异很大，不仅受个人偏好的影响，还受情绪和动机的影响。相比之下，AI生成的偏好评分可以更加一致和可靠。
- en: To mitigate privacy concerns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解隐私担忧
- en: Synthetic data is often the only option for use cases where you can’t use human-generated
    data due to privacy concerns. For instance, in healthcare, where legislation makes
    it hard, if not impossible, to use real patient records to train a model, you
    can generate synthetic patient records that do not contain any sensitive information.
    In insurance, you can use synthetic claims instead of using real claims that include
    sensitive personal and financial information.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据通常是唯一的选择，在这些用例中，由于隐私问题无法使用人类生成数据。例如，在医疗保健领域，由于立法使得使用真实患者记录来训练模型变得困难，甚至不可能，你可以生成不包含任何敏感信息的合成患者记录。在保险领域，你可以使用合成索赔而不是使用包含敏感个人和财务信息的真实索赔。
- en: To distill models
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精炼模型
- en: Sometimes, you might want to train a model to imitate the behavior of another
    model. The goal is often to create a cheaper and/or faster model (the distilled
    model) with performance comparable to that of the original model. This is done
    by training the distilled model using data generated by the original model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可能想要训练一个模型来模仿另一个模型的行为。目标通常是创建一个更便宜且/或更快的模型（即精炼模型），其性能与原始模型相当。这是通过使用原始模型生成数据来训练精炼模型实现的。
- en: These are just five of the many reasons why people turn to data synthesis. Because
    of its undeniable appeal, more models are being trained with synthetic data and
    more techniques are being developed to synthesize data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是人们转向数据合成的许多原因中的五个。由于其不可否认的吸引力，越来越多的模型正在使用合成数据进行训练，并且正在开发更多数据合成技术。
- en: Traditional Data Synthesis Techniques
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统的数据合成技术
- en: Data synthesis isn’t unique to AI. It has a long history in software testing,
    gaming, and robotics. Using algorithms to generate data is also called *procedural
    generation*, as opposed to *manual generation*. Procedural generation is commonly
    used in gaming to generate content such as levels, maps, items, and characters
    on the fly.^([10](ch08.html#id1539)) Most data generation techniques used in these
    industries can be applied to AI.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合成并非仅限于AI。它在软件测试、游戏和机器人技术中有着悠久的历史。使用算法生成数据也被称为*程序生成*，与*手动生成*相对。程序生成在游戏中被广泛用于即时生成内容，如关卡、地图、物品和角色。[10](ch08.html#id1539)
    这些行业中使用的多数数据生成技术都可以应用于AI。
- en: Traditionally, two approaches for data synthesis and augmentation have been
    rule-based and simulation. A newer method made possible by advanced AI models
    is using AI itself to synthesize data. This section gives a quick overview of
    these two traditional techniques before moving on to AI-powered data synthesis
    in the next section.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据合成和增强的两种方法是基于规则和模拟。一种由先进AI模型使可能的新方法是使用AI本身来合成数据。本节简要概述了这两种传统技术，然后再介绍下一节中基于AI的数据合成。
- en: Rule-based data synthesis
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于规则的规则数据合成
- en: 'The simplest way to generate data is to use predefined rules and templates.
    For example, to create a credit card transaction, start with a transaction template
    and use a random generator like Faker to populate each field in this template:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据的最简单方法是使用预定义的规则和模板。例如，要创建一张信用卡交易，从一个交易模板开始，并使用像Faker这样的随机生成器来填充这个模板中的每个字段：
- en: '[PRE1]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Due to the sensitivity of transaction data, many fraud detection models are
    first trained on synthetic transaction data generated from templates like this
    to prove their feasibility before being given access to real data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交易数据的敏感性，许多欺诈检测模型首先在由这种模板生成的合成交易数据上训练，以证明其可行性，然后再获得访问真实数据的机会。
- en: It’s common to use templates to generate documents that follow a specific structure,
    such as invoices, resumes, tax forms, bank statements, event agendas, product
    catalogs, contracts, configuration files, etc. Templates can also be used to generate
    data that follows a certain grammar and syntax, such as regular expressions and
    math equations. You can use templates to generate math equations for AI models
    to solve. DeepMind trained an Olympiad-level geometry model, AlphaGeometry, using
    100 million synthetic examples ([Trinh et al., 2024](https://oreil.ly/skn8z)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模板生成遵循特定结构的文档是很常见的，例如发票、简历、税务表格、银行对账单、活动议程、产品目录、合同、配置文件等。模板也可以用来生成遵循特定语法和语法的数据，例如正则表达式和数学方程式。你可以使用模板为AI模型生成数学方程式。DeepMind使用1亿个合成示例训练了一个奥林匹克级别的几何模型，AlphaGeometry([Trinh等人，2024](https://oreil.ly/skn8z))。
- en: You can procedurally generate new data from existing data by applying simple
    transformations. For images, you can randomly rotate, crop, scale, or erase part
    of an image. A flipped image of a cat should still be a cat. A slightly cropped
    image of a soccer game should still be a soccer game. [Krizhevsky et al. (2012)](https://oreil.ly/ez6Iw)
    demonstrated in their legendary AlexNet paper the usefulness of this technique
    by using it to augment the ImageNet dataset ([Deng et al., 2009](https://oreil.ly/i7hpS)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过应用简单的转换从现有数据中程序化地生成新数据。对于图像，你可以随机旋转、裁剪、缩放或擦除图像的一部分。一只猫的翻转图像仍然是一只猫。一场足球比赛的轻微裁剪图像仍然是一场足球比赛。[Krizhevsky等人（2012）](https://oreil.ly/ez6Iw)在他们著名的AlexNet论文中通过使用它来增强ImageNet数据集([Deng等人，2009](https://oreil.ly/i7hpS))展示了这种技术的有用性。
- en: 'For texts, you can randomly replace a word with a similar word, assuming that
    this replacement wouldn’t change the meaning or the sentiment of the sentence.
    For example, the original sentence “She’s a *fantastic* nurse” can generate a
    new example: “She’s a *great* nurse”.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本，你可以随机用一个类似词替换一个词，假设这种替换不会改变句子的意义或情感。例如，原始句子“她是一位 *出色的* 护士”可以生成一个新的例子：“她是一位
    *优秀的* 护士”。
- en: This approach can be used to mitigate potential biases in your data. If you’re
    concerned that there’s a gender bias in your data, where, for example, the word
    “nurse” is associated with women while the word “doctor” is associated with men,
    you can replace typically gendered words with their opposites, such as “she” with
    “he”, as shown in [Table 8-2](#ch08_table_2_1730130931981173).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以用来减轻你数据中的潜在偏差。如果你担心你的数据中存在性别偏差，例如，“护士”这个词与女性相关联，而“医生”这个词与男性相关联，你可以用它们的对立词替换通常具有性别特征的词，例如用“她”替换“他”，正如[表8-2](#ch08_table_2_1730130931981173)中所示。
- en: Table 8-2\. Data augmentation can help mitigate certain biases in your data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2\. 数据增强可以帮助减轻你数据中的某些偏差。
- en: '| Original data | Augmented data |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 原始数据 | 增强数据 |'
- en: '| --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| She’s a fantastic nurse. | *He*’s a fantastic nurse. She’s a fantastic *doctor*.
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 她是一位出色的护士。| *他* 是一位出色的护士。她是一位出色的 *医生*。|'
- en: '| The CEO of the firm, Mr. Alex Wang, … | The CEO of the firm, *Ms. Alexa Wang*,
    … |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 公司的CEO，亚历克斯·王先生，… | 公司的CEO，*亚历克萨·王女士*，…|'
- en: '| Today, my mom made a casserole for dinner. | Today, my *dad* made a casserole
    for dinner. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 今天，我妈妈为晚餐做了一道烤菜。| 今天，我的 *爸爸* 为晚餐做了一道烤菜。|'
- en: '| Emily has always loved the violin. | *Mohammed* has always loved the violin.
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 埃米莉一直喜欢小提琴。| *穆罕默德* 一直喜欢小提琴。|'
- en: Similar words can be found either with a dictionary of synonymous words or by
    finding words whose embeddings are close to each other in a word embedding space.
    You can go beyond simple word replacement by asking AI to rephrase or translate
    an example, as we’ll discuss later.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过同义词词典或者找到在词嵌入空间中彼此接近的词语来找到类似的表达。你可以通过让AI重写或翻译一个例子来超越简单的词语替换，正如我们稍后将要讨论的。
- en: 'One interesting transformation is perturbation: adding noise to existing data
    to generate new data. Initially, researchers discovered that perturbing a data
    sample slightly can trick models into misclassifying it. For example, adding white
    noise to a picture of a ship can cause the model to misclassify it as a car. The
    paper “One Pixel Attack for Fooling Deep Neural Networks” ([Su et al., 2017](https://arxiv.org/abs/1710.08864))
    showed that 67.97% of the natural images in the Kaggle CIFAR-10 test dataset and
    16.04% of the ImageNet test images could be misclassified by changing just one
    pixel. This poses a serious risk if exploited. An attacker could trick an AI model
    into misidentifying them as an authorized employee or make a self-driving car
    mistake a divider for a lane, leading to accidents.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的转换是扰动：向现有数据添加噪声以生成新数据。最初，研究人员发现对数据样本进行轻微扰动可以欺骗模型将其错误分类。例如，向一艘船的图片添加白噪声可以使模型将其错误分类为汽车。论文“用于欺骗深度神经网络的单一像素攻击”（[Su等人，2017](https://arxiv.org/abs/1710.08864)）显示，Kaggle
    CIFAR-10测试数据集中的自然图像中有67.97%和ImageNet测试图像中的16.04%可以通过仅更改一个像素而被错误分类。如果被利用，这会带来严重风险。攻击者可能会欺骗AI模型将其误识别为授权员工，或者使自动驾驶汽车将隔离带误认为是车道，从而导致事故。
- en: You can train your model on perturbed data. Perturbation can both improve the
    model’s performance and make it more robust against attacks; see [Goodfellow et
    al., 2013](https://arxiv.org/abs/1302.4389) and [Moosavi-Dezfooli et al., 2015](https://arxiv.org/abs/1511.04599)).
    In 2019, Hendrycks and Dietterich created [ImageNet-C and ImageNet-P](https://arxiv.org/abs/1903.12261)
    by applying 15 common visual corruptions, such as changing brightness, adding
    snow, changing contrast, and adding noises to ImageNet images.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在扰动的数据上训练您的模型。扰动不仅可以提高模型的表现，还可以使其对攻击更加鲁棒；参见[Goodfellow等人，2013](https://arxiv.org/abs/1302.4389)和[Moosavi-Dezfooli等人，2015](https://arxiv.org/abs/1511.04599)）。2019年，Hendrycks和Dietterich通过应用15种常见的视觉破坏（如改变亮度、添加雪、改变对比度和向ImageNet图像添加噪声）创建了[ImageNet-C和ImageNet-P](https://arxiv.org/abs/1903.12261)。
- en: Perturbation can also be used for texts. For example, to train BERT, the authors
    replaced 1.5% of the tokens with random words ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)).
    They found this perturbation led to a small performance boost.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动也可以用于文本。例如，为了训练BERT，作者将1.5%的标记替换为随机单词（[Devlin等人，2018](https://arxiv.org/abs/1810.04805)）。他们发现这种扰动导致性能略有提升。
- en: Visual data can be augmented using more sophisticated algorithms. [Snap (2022)](https://oreil.ly/1YFbA)
    has a great case study on how they augment their assets to create unrepresented
    corner cases and mitigate implicit biases in their data. Given a character, they
    synthesize similar characters but with different skin colors, body types, hairstyles,
    clothes, and even facial expressions. These augmented assets are then used to
    train AI models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用更复杂的算法对视觉数据进行增强。[Snap (2022)](https://oreil.ly/1YFbA)有一个很好的案例研究，说明了他们如何增强他们的资产以创建未代表的边缘情况并减轻数据中的隐含偏见。给定一个角色，他们合成具有不同肤色、体型、发型、衣服甚至面部表情的相似角色。这些增强资产随后用于训练AI模型。
- en: Simulation
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模拟
- en: Instead of running experiments to collect data in the real world, where it can
    be expensive and dangerous, you can simulate these experiments virtually. For
    example, to test how a self-driving car reacts when encountering a horse on the
    highway, it’d be dangerous to release an actual horse on the highway. Instead,
    you simulate this situation in a virtual environment. Examples of self-driving
    simulation engines include CARLA ([Dosovitskiy et al., 2017](https://arxiv.org/abs/1711.03938)),
    [Waymo’s SimulationCity](https://oreil.ly/xbyXd), and [Tesla’s simulation of San
    Francisco](https://oreil.ly/YnbiK).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与在现实世界中运行实验以收集数据相比，这些实验可能既昂贵又危险，您可以在虚拟环境中模拟这些实验。例如，为了测试自动驾驶汽车在高速公路上遇到马时的反应，在高速公路上释放真正的马是危险的。相反，您可以在虚拟环境中模拟这种情况。自动驾驶模拟引擎的例子包括CARLA
    ([Dosovitskiy等人，2017](https://arxiv.org/abs/1711.03938))、[Waymo的SimulationCity](https://oreil.ly/xbyXd)和[Tesla的旧金山模拟](https://oreil.ly/YnbiK)。
- en: Similarly, it’s very common to simulate training data for robotics in a virtual
    environment. Let’s say you want to train a robot to pour coffee, but you don’t
    know exactly how each joint should move to make the action successful. You can
    simulate multiple scenarios with different joint movements and use only the scenarios
    where coffee is successfully poured to train the robot.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在虚拟环境中模拟机器人训练数据非常常见。比如说，你想训练一个机器人倒咖啡，但你不知道每个关节应该如何移动才能使动作成功。你可以模拟多种不同的关节运动场景，并仅使用成功倒出咖啡的场景来训练机器人。
- en: Simulations allow you to run multiple experiments with minimal costs while avoiding
    accidents and physical damage. A robot that works in simulations might not work
    in the real world, but if it fails in simulations, it’ll likely fail in the real
    world. No matter how sophisticated your simulations are, however, they are simplifications
    of the real world. Sim2Real is a subfield that focuses on adapting algorithms
    that have been trained in simulations to the real world.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟允许你在最小成本的情况下运行多个实验，同时避免事故和物理损坏。在模拟中工作的机器人可能在现实世界中不起作用，但如果它在模拟中失败，它很可能在现实世界中也会失败。然而，无论你的模拟多么复杂，它们都是对现实世界的简化。Sim2Real是一个子领域，专注于将已在模拟中训练的算法适应到现实世界。
- en: Simulations are common to generate data to teach models to use tools. As mentioned
    earlier, human-generated actions might not always be the most efficient for AI
    agents. Simulations might help uncover actions that humans overlook. Given a query,
    you can simulate different action sequences, execute these sequences, and validate
    their outcomes. The most efficient action sequence is then used as the annotated
    response for the query.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟常用于生成数据以教授模型使用工具。如前所述，人类生成的动作可能并不总是对AI代理最有效。模拟可能有助于揭示人类忽视的动作。给定一个查询，你可以模拟不同的动作序列，执行这些序列，并验证它们的成果。然后，最有效的动作序列被用作查询的标注响应。
- en: Simulations are particularly valuable for generating data for rare events. For
    example, in finance, researchers can simulate scenarios such as a company successfully
    going public or a significant bankruptcy to understand their market impacts. Manufacturers
    can simulate defects in materials or assemblies to generate data to train anomaly
    detection and quality control models. Similarly, by simulating the Earth’s systems,
    climate scientists can create variations in temperature changes, precipitation
    patterns, and extreme weather scenarios. This synthetic data is then fed into
    AI models, enabling them to learn from a broader spectrum of possible futures.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟对于生成罕见事件的数据尤其有价值。例如，在金融领域，研究人员可以模拟公司成功上市或重大破产等场景，以了解它们的市场影响。制造商可以模拟材料或组件的缺陷，以生成数据来训练异常检测和质量控制模型。同样，通过模拟地球系统，气候学家可以创建温度变化、降水模式和极端天气情景的变体。然后，这些合成数据被输入到AI模型中，使它们能够从更广泛的可能未来中学习。
- en: Both rule-based and simulation-based techniques have been useful for many use
    cases, but it wasn’t until AI become capable of generating realistic and high-quality
    data that data synthesis really took off. Let’s look into those methods next.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则和基于模拟的技术在许多用例中都很有用，但直到AI能够生成逼真且高质量的数据，数据合成才真正起飞。让我们接下来看看那些方法。
- en: AI-Powered Data Synthesis
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI驱动的数据合成
- en: Just as there are virtually infinite ways for humans to generate data, AI can
    also do so in many ways. The techniques discussed here are not comprehensive,
    but they should give you a good overview.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人类生成数据有几乎无限种方式一样，AI也可以以多种方式做到这一点。这里讨论的技术并不全面，但它们应该能给你一个良好的概述。
- en: '*Powerful AI models open many new possibilities for simulations*. AI can simulate
    the outcomes of arbitrary programs. For example, “StableToolBench” ([Guo et al.,
    2024](https://arxiv.org/abs/2403.07714)) demonstrates how to use AI to simulate
    APIs without having to evoke them. Imagine you want to train a model to interact
    with a set of APIs. Instead of making actual API calls—which might be costly or
    slow—you can use an AI model to simulate the expected outcomes of those calls.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*强大的AI模型为模拟开辟了许多新的可能性*。AI可以模拟任意程序的输出。例如，“StableToolBench”([Guo等人，2024](https://arxiv.org/abs/2403.07714))展示了如何使用AI模拟API而无需调用它们。想象一下，你想训练一个模型与一组API交互。与其进行实际的API调用——这可能是成本高昂或缓慢的——你还可以使用AI模型来模拟这些调用的预期结果。'
- en: AI can simulate humans. For example, imagine you want to train a bot to play
    chess. A game played by humans might take too long. Matches with AI players would
    be much faster. To train its Dota 2 bot, OpenAI used a simulator that enabled
    the bot to play approximately 180 years’ worth of games every day. The bot learned
    by playing against itself, an approach called *self-play*, which helped it develop
    and refine strategies over time ([OpenAI, 2019](https://oreil.ly/rX6oc)). Similarly,
    DeepMind used self-play to collect data from millions of Go games to train AlphaGo
    ([Silver et al., 2016](https://oreil.ly/prIw9)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: AI可以模拟人类。例如，想象一下您想要训练一个机器人下棋。由人类进行的游戏可能需要太长时间。与AI玩家的比赛会快得多。为了训练其Dota 2机器人，OpenAI使用了一个模拟器，使机器人每天可以玩大约180年的游戏。机器人通过自我博弈学习，这是一种称为*自我博弈*的方法，帮助它在一段时间内发展和完善策略([OpenAI,
    2019](https://oreil.ly/rX6oc))。同样，DeepMind使用自我博弈从数百万场围棋游戏中收集数据来训练AlphaGo([Silver等人，2016](https://oreil.ly/prIw9))。
- en: Self-play is useful not just for game bots but also for general agents. You
    can have AIs negotiate against each other using different strategies to see which
    one works better. You can have one version of the model play the role of a customer
    with issues and another play the customer support agent.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自我博弈不仅对游戏机器人有用，对通用智能体也有用。您可以让AI使用不同的策略相互博弈，以查看哪种策略更有效。您可以有一个模型版本扮演有问题的客户角色，另一个扮演客户支持代表。
- en: '*AI’s paraphrasing and translation abilities can be used to augment existing
    datasets.* For example, given the query “How to reset my password?”, AI can paraphrase
    it to create three new queries:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI的释义和翻译能力可以用来增强现有数据集。* 例如，给定查询“如何重置我的密码？”，AI可以将其释义为创建三个新的查询：'
- en: “I forgot my password.”
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我忘记了我的密码。”
- en: “How can I change my password?”
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我该如何更改我的密码？”
- en: “Steps to reset passwords.”
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “重置密码的步骤。”
- en: '[Yu et al. (2023)](https://arxiv.org/abs/2309.12284) rewrote the 15,000 examples
    in MATH and GSM-8K in different ways to create MetaMath, a new dataset of almost
    400,000 examples. They showed that their models, trained on this new dataset,
    outperformed larger models on related math benchmarks.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yu等人（2023）](https://arxiv.org/abs/2309.12284)以不同的方式重写了MATH和GSM-8K中的15,000个示例，创建了MetaMath，这是一个包含近40万个示例的新数据集。他们展示了在新的数据集上训练的模型在相关的数学基准测试中优于更大的模型。'
- en: It’s common to use AI to translate data in high-resource languages (more available
    online) into low-resource languages to help train models in low-resource languages.
    This is useful for training a small model specializing in a low-resource language
    like Quechua or Lao.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AI将资源丰富的语言（在线上更易获得）的数据翻译成资源匮乏的语言（以帮助训练资源匮乏语言中的模型）是很常见的。这对于训练一个专门针对资源匮乏语言（如克丘亚语或老挝语）的小型模型很有用。
- en: You can verify the quality of translations with *back-translation*. Let’s say
    the original English sentence is *X* and the translated Lao sentence is *Y*. You
    can use another model to translate the translation back into the original language,
    *X*ʹ, then compare *X*ʹ with the original sentence X. If they are very different,
    the translation *Y* is likely bad.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过*回译*来验证翻译的质量。比如说，原始的英语句子是*X*，翻译成老挝语的句子是*Y*。您可以使用另一个模型将翻译回译成原始语言，即*X*ʹ，然后比较*X*ʹ与原始句子X。如果它们非常不同，那么翻译*Y*很可能是错误的。
- en: AI can translate not just natural languages but also programming languages.
    You can use AI to translate code written in one language to another. The [Llama
    3 authors](https://arxiv.org/abs/2407.21783) used code translation of their SFT
    dataset with a wider range of programming languages. In fact, the training of
    Llama 3 depends heavily on synthetic data, and the authors used many creative
    techniques to generate useful data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: AI不仅可以翻译自然语言，还可以翻译编程语言。您可以使用AI将一种语言的代码翻译成另一种语言。[Llama 3的作者](https://arxiv.org/abs/2407.21783)使用更广泛编程语言的代码翻译他们的SFT数据集。实际上，Llama
    3的训练很大程度上依赖于合成数据，作者们使用了许多创造性的技术来生成有用的数据。
- en: For example, they used back-translation to generate code explanations and documentation.
    Starting with code snippets, they used AI to generate explanations and documentation.
    They then again used AI to generate code snippets from the explanations and documentation.
    Only if the generated code is considered faithful to the original will the explanation
    and documentation be used to finetune the model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，他们使用回译来生成代码解释和文档。从代码片段开始，他们使用AI生成解释和文档。然后他们再次使用AI从解释和文档中生成代码片段。只有当生成的代码被认为忠实于原始代码时，解释和文档才会被用来微调模型。
- en: AI can generate data for both pre-training and post-training, though synthetic
    data is intentionally included much more often in post-training than in pre-training.
    One possible explanation for this is that pre-training’s goal is to increase the
    model’s knowledge, and while AI can synthesize existing knowledge in different
    formats, it’s harder to synthesize new knowledge.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: AI可以为预训练和训练后生成数据，尽管在训练后比在预训练中更频繁地包含合成数据。一个可能的解释是，预训练的目标是增加模型的知识，虽然AI可以以不同格式合成现有知识，但合成新知识更困难。
- en: However, as the internet becomes flooded with AI-generated content, models that
    rely on internet data are likely already pre-trained on synthetic data. There
    are also synthetic datasets such as [Cosmopedia](https://oreil.ly/0ymnI) (Allal
    et al., 2024), a 25-billion-token collection of synthetic textbooks, blog posts,
    stories, posts, and WikiHow articles generated by [Mixtral-8x7B-Instruct-v0.1](https://oreil.ly/FyHwn)
    (Jiang et al., 2024).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着互联网上充斥着AI生成的内容，依赖于互联网数据的模型可能已经预先在合成数据上进行了训练。还有一些合成数据集，例如[Cosmopedia](https://oreil.ly/0ymnI)（Allal等，2024年），这是一个包含25亿个标记的合成教科书、博客文章、故事、帖子以及WikiHow文章的集合，由[Mixtral-8x7B-Instruct-v0.1](https://oreil.ly/FyHwn)（江等，2024年）生成。
- en: Data synthesis for post-training is also more common because post-training data,
    including both instruction data and preference data, generally demands the most
    effort to produce. Using AI to pick the better response among several responses
    is more straightforward—much of it was already covered in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067).
    The main challenge is to take into account the model’s biases, such as first-position
    bias, where the model is more likely to prefer the first option. To avoid this,
    NVIDIA researchers asked the AI judge twice, once with the response order swapped.
    They picked a valid (prompt, winning, losing) triplet only when the AI judge picked
    the same winner both times ([NVIDIA, 2024](https://oreil.ly/f8LPj)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 针对训练后数据的数据合成也更为常见，因为训练后数据，包括指令数据和偏好数据，通常需要最多的努力来生成。使用AI从多个响应中选择更好的响应更为直接——其中许多内容已在[第3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)中介绍。主要挑战是考虑到模型的偏差，例如首位置偏差，模型更有可能偏好第一个选项。为了避免这种情况，NVIDIA研究人员让AI评判员两次进行评判，一次是响应顺序颠倒。只有当AI评判员两次都选择了相同的获胜者时，他们才选择一个有效的（提示、获胜、失败）三元组([NVIDIA,
    2024](https://oreil.ly/f8LPj))。
- en: The next section will focus on how to use AI to synthesize instruction data
    for supervised finetuning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将重点介绍如何使用AI来合成用于监督微调的指令数据。
- en: Instruction data synthesis
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令数据合成
- en: 'During instruction finetuning, each example includes an instruction and a response.
    AI can be used to synthesize the instructions, the responses, or both. For example,
    you can use AI to generate instructions and humans to write responses. You can
    also use humans to write instructions and AI to generate responses:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在指令微调过程中，每个示例都包含一个指令和一个响应。可以使用AI来合成指令、响应或两者。例如，你可以使用AI生成指令，由人类编写响应。你也可以由人类编写指令，由AI生成响应：
- en: For instruction generation, to ensure that you generate sufficient instructions
    to cover your use case, you can start with a list of topics, keywords, and/or
    the instruction types you want in your dataset. Then, for each item on this list,
    generate a certain number of instructions. You can also begin with a set of templates
    and generate a certain number of examples per template. Note that both the topic
    list and templates can be generated by AI.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于指令生成，为了确保你生成的指令足够覆盖你的用例，你可以从一个包含你想要在数据集中包含的主题、关键词和/或指令类型的列表开始。然后，为列表中的每个项目生成一定数量的指令。你也可以从一个模板集合开始，并为每个模板生成一定数量的示例。请注意，主题列表和模板都可以由AI生成。
- en: For response generation, you can generate one or more responses per instruction.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于响应生成，你可以为每个指令生成一个或多个响应。
- en: For instance, to create UltraChat ([Ding et al., 2023](https://arxiv.org/abs/2305.14233)),
    a multi-turn dialogue dataset, the authors first asked ChatGPT to generate 30
    topics about various aspects of our daily lives, such as technology, food and
    drink, fashion, nature, education, finance, travel, etc. For each topic, they
    asked ChatGPT to generate 30 to 50 subtopics. The authors then used the same model
    to generate instructions and corresponding responses for these subtopics.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了创建UltraChat ([Ding et al., 2023](https://arxiv.org/abs/2305.14233))，一个多轮对话数据集，作者首先让ChatGPT生成关于我们日常生活各个方面（如技术、食品和饮料、时尚、自然、教育、金融、旅行等）的30个主题。对于每个主题，他们让ChatGPT生成30到50个子主题。然后，作者使用相同的模型为这些子主题生成指令和相应的响应。
- en: Similarly, to train Alpaca ([Taori et al., 2023](https://oreil.ly/u9ghd)), Stanford
    researchers began with 175 (instruction, response) examples from the Self-Instruct
    seed dataset ([Wang et al., 2022](https://arxiv.org/abs/2212.10560)). These examples
    were originally written to cover a diverse and interesting range of uses. Alpaca
    authors then used a GPT-3 model, *text-davinci-003*, to generate 52,000 (instruction,
    response) pairs that mirrored these seed examples, as shown in [Figure 8-5](#ch08_figure_6_1730130931958982).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为了训练Alpaca ([Taori et al., 2023](https://oreil.ly/u9ghd))，斯坦福研究人员从Self-Instruct种子数据集的175个（指令，响应）示例开始([Wang
    et al., 2022](https://arxiv.org/abs/2212.10560))。这些示例最初是为了涵盖各种多样和有趣的使用范围。Alpaca的作者随后使用了一个GPT-3模型，*text-davinci-003*，生成了52,000个（指令，响应）对，这些对与种子示例相匹配，如图8-5所示。
- en: '![A close-up of a sign'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个标志的特写'
- en: Description automatically generated](assets/aien_0805.png)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0805.png)
- en: Figure 8-5\. A seed task and a generated task used to train Alpaca.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 用于训练Alpaca的种子任务和生成任务。
- en: 'There are also many creative ways to synthesize instruction data with certain
    characteristics. For example, just like it’s harder for humans to write longer
    content than shorter content, it’s harder for AI to generate high-quality long
    responses than short instructions. The longer the response, the more chance AI
    has to hallucinate. What if we use human-generated responses with AI-generated
    instructions? Some researchers, such as [Köksal et al. (2023)](https://arxiv.org/abs/2304.08460),
    [Li et al. (2023)](https://arxiv.org/abs/2308.06259), and [Chen et al. (2023)](https://arxiv.org/abs/2309.05447),
    follow the *reverse instruction* approach: take existing long-form, high-quality
    content like stories, books, and Wikipedia articles and use AI to generate prompts
    that would elicit such content. This yields higher-quality instruction data, avoiding
    AI-generated hallucinations in the responses.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，有许多创造性的方法可以合成具有特定特征的指令数据。例如，正如人类写长内容比写短内容更难一样，AI生成高质量的长响应比短指令更难。响应越长，AI产生幻觉的机会就越多。如果我们使用人类生成的响应与AI生成的指令结合会怎样？一些研究人员，如[Köksal
    et al. (2023)](https://arxiv.org/abs/2304.08460)，[Li et al. (2023)](https://arxiv.org/abs/2308.06259)，和[Chen
    et al. (2023)](https://arxiv.org/abs/2309.05447)，遵循*反向指令*方法：取现有的长篇、高质量内容，如故事、书籍和维基百科文章，并使用AI生成能够引发此类内容的提示。这产生了更高质量的指令数据，避免了AI在响应中产生的幻觉。
- en: 'It’s possible to use reverse instruction to develop increasingly powerful models
    without adding manually annotated data.^([11](ch08.html#id1549)) Li et al. (2023)
    shows how this works:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用反向指令在不添加手动标注数据的情况下开发越来越强大的模型。[11](ch08.html#id1549) 李等（2023）展示了这是如何工作的：
- en: Start with a small number of seed examples to train a weak model.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从少量种子示例开始训练一个弱模型。
- en: Use this weak model to generate instructions for existing high-quality content
    to create high-quality instruction data.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个弱模型为现有高质量内容生成指令，以创建高质量指令数据。
- en: Finetune the weak model with this new high-quality instruction data.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个新的高质量指令数据微调弱模型。
- en: Repeat until desirable performance is reached.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复，直到达到期望的性能。
- en: 'A creative approach is to use synthetic data to finetune a model for understanding
    longer contexts. For example, if your current model processes a maximum of 8K
    tokens but you want it to handle 128K tokens, the long-context finetuning process
    might look like this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一种创造性的方法是使用合成数据微调一个模型以理解更长的上下文。例如，如果你的当前模型最多处理8K个标记，但你希望它处理128K个标记，长上下文微调过程可能看起来像这样：
- en: Split long documents into shorter chunks (e.g., under 8K tokens).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将长文档拆分成较短的片段（例如，低于8K个标记）。
- en: For each short chunk, generate several (question, answer) pairs.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个短片段，生成几个（问题，答案）对。
- en: For each (question, answer) pair, use the original long document, which may
    exceed 8K tokens but be shorter than your target length, as the context. This
    trains the model to use the extended context to answer questions.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个（问题，答案）对，使用原始的长文档作为上下文，该文档可能超过8K个标记，但短于你的目标长度，以训练模型使用扩展上下文来回答问题。
- en: 'The level of detail in the Llama 3 paper ([Dubey et al., 2024](https://arxiv.org/abs/2407.21783))
    makes it an excellent case study for instruction data synthesis. I’ve already
    mentioned two ways in which Llama 3 synthesized data: code translation and code
    back-translation. Both of these methods generate more data from existing code
    snippets. However, the authors also used AI to synthesize coding instruction data
    from scratch, using the following workflow:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3论文（[Dubey等人，2024](https://arxiv.org/abs/2407.21783)）中的细节水平使其成为指令数据合成的优秀案例研究。我已经提到了Llama
    3合成数据的方式：代码翻译和代码回译。这两种方法都能从现有的代码片段中生成更多数据。然而，作者们还使用了AI从头开始合成编码指令数据，采用以下工作流程：
- en: Use AI to generate a large collection of programming problem descriptions that
    span a diverse range of topics.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI生成大量涵盖广泛主题的编程问题描述。
- en: Given a problem description and a programming language, generate a solution.
    Dubey et al. found that including general rules of good programming and CoT reasoning
    helped improve response quality.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个问题描述和编程语言，生成一个解决方案。Dubey等人发现，包括良好的编程规则和CoT推理有助于提高响应质量。
- en: 'To ensure the quality of the generated data, they employed a rigorous correctness
    analysis and error correction pipeline:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保生成数据的质量，他们采用了严格的正确性分析和错误纠正流程：
- en: Run generated code through parsers and linters to catch syntactic errors such
    as missing imports and uninitialized variables.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行生成的代码通过解析器和linters来捕捉语法错误，如缺少导入和未初始化的变量。
- en: Use unit tests to catch runtime execution errors. Interestingly enough, they
    used AI to generate these unit tests.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用单元测试来捕捉运行时执行错误。有趣的是，他们使用了AI来生成这些单元测试。
- en: When a solution fails at any step, prompt the model to revise the code. The
    prompt included the original problem description, the faulty solution, and feedback
    from the parser, linter, and unit tests. Only examples that pass all checks are
    included in the final supervised finetuning dataset.^([12](ch08.html#id1551))
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当解决方案在任何步骤失败时，提示模型修改代码。提示包括原始问题描述、有缺陷的解决方案以及来自解析器、linters和单元测试的反馈。只有通过所有检查的示例才包含在最终的监督微调数据集中.^([12](ch08.html#id1551))
- en: 'Combining all three methods together—code translation, code back-translation,
    and code generation—Llama 3’s data synthesis workflow is quite impressive. To
    summarize, here’s how these three methods work together:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 将三种方法结合起来——代码翻译、代码回译和代码生成——Llama 3的数据合成工作流程相当令人印象深刻。为了总结，以下是这三个方法如何协同工作：
- en: Use AI to generate problem descriptions.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI生成问题描述。
- en: Use AI to generate solutions for each problem in different programming languages.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI为每个问题生成不同编程语言的解决方案。
- en: Use AI to generate unit tests to test the generated code.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI生成单元测试以测试生成的代码。
- en: Prompt AI to fix errors in the synthesized code.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示AI修复合成代码中的错误。
- en: Use AI to translate generated code to different programming languages. Filter
    out translated code that doesn’t pass tests.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI将生成的代码翻译成不同的编程语言。过滤掉未通过测试的翻译代码。
- en: Use AI to generate conversations about the code, including code explanation
    and adding documentation. Filter out generated explanations and documentation
    that doesn’t pass back-translation verification.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AI生成关于代码的对话，包括代码解释和添加文档。过滤掉未通过回译验证的生成解释和文档。
- en: Using this pipeline, Dubey et al. were able to generate over 2.7 million synthetic
    coding-related examples for the supervised finetuning of Llama 3.1.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个流程，Dubey等人能够为Llama 3.1的监督微调生成超过270万个合成编码相关示例。
- en: Data verification
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据验证
- en: Given the importance of data quality in the model’s performance, it’s crucial
    that we have a way to verify the quality of data. The quality of AI-generated
    data can be measured the same way you’d evaluate other AI outputs—by functional
    correctness and AI judges.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据质量对模型性能的重要性，我们必须有一种方法来验证数据质量。AI生成数据的质量可以通过与评估其他AI输出相同的方式进行衡量——通过功能正确性和AI评判。
- en: While this section focuses on synthetic data, most of the techniques can be
    used to evaluate the quality of training data in general.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节重点介绍合成数据，但大多数技术也可以用来评估训练数据的一般质量。
- en: Recall the concept of evaluation-driven development from [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863),
    where companies are more likely to create applications they can evaluate. Similarly,
    people tend to synthesize data they can verify. Coding is one of the most popular
    foundation model use cases because it can be functionally evaluated, and for the
    same reason, coding-related examples are among the most commonly synthesized data.
    Most of the synthetic data used to train Llama 3 is coding-related. All three
    methods the authors used to synthesize data result in data that can be programmatically
    verified, x, by code execution and back-translation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中提到的评估驱动开发的概念，其中公司更有可能创建他们可以评估的应用程序。同样，人们倾向于合成他们可以验证的数据。编码是最受欢迎的基础模型用例之一，因为它可以进行功能评估，同样地，与编码相关的示例也是最常见的合成数据之一。用于训练Llama
    3的大多数合成数据都与编码相关。作者用来合成数据的所有三种方法都产生了可以通过代码执行和回译进行程序验证的数据，即x。
- en: For synthetic data that can’t be verified by functional correctness, it’s common
    to use AI verifiers. An AI verifier can be a general-purpose AI judge or a specialized
    scorer. There are many ways to frame the verification problem. In the simplest
    form, the AI verifier can assign each generated example a score from 1 to 5 or
    classify each example as good or bad. You can also describe to a foundation model
    the quality requirements and instruct the model to determine if a data example
    meets these requirements.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无法通过功能正确性验证的合成数据，通常使用AI验证器。AI验证器可以是一般用途的AI裁判或专门的评分员。有许多方法可以构建验证问题。在 simplest
    form，AI验证器可以为每个生成的示例分配1到5的分数，或者将每个示例分类为好或坏。你也可以向基础模型描述质量要求，并指示模型确定数据示例是否满足这些要求。
- en: If you care about the factual consistency of data, you can use the factual inconsistency
    detection techniques discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)
    to filter out examples that are likely to contain hallucinations.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关心数据的真实性一致性，你可以使用在[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中讨论的真实性不一致检测技术来过滤掉可能包含幻觉的示例。
- en: Depending on the use case and the generated data, you can also get creative.
    For instance, if you want synthetic data to mimic real data, its quality can be
    measured by how difficult it is to distinguish between the two. You could train
    an AI content detector to identify AI-generated data—if it’s easy to differentiate
    between real and synthetic data, the synthetic data isn’t good. Or, if you want
    the synthetic data to resemble high-quality academic work, you could train a classifier
    to predict whether a generated paper would be accepted at a prestigious conference
    like NeurIPS (the Conference and Workshop on Neural Information Processing Systems)
    and discard any papers predicted to be clear rejects.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用例和生成数据，你也可以有创意。例如，如果你想合成的数据模仿真实数据，其质量可以通过区分这两者的难度来衡量。你可以训练一个AI内容检测器来识别AI生成的数据——如果区分真实和合成数据很容易，那么合成数据就不太好。或者，如果你想合成的数据类似于高质量的学术作品，你可以训练一个分类器来预测生成的论文是否会被NeurIPS（神经信息处理系统会议和研讨会）等著名会议接受，并丢弃任何被预测为明显拒绝的论文。
- en: You can have a model to detect the topic of each generated example and then
    remove examples whose topics are irrelevant to your task. If you expect all data
    to follow a similar pattern, you can also use anomaly detection to identify outliers—outlier
    examples might be of low quality.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有一个模型来检测每个生成的示例的主题，然后移除与你的任务无关的主题的示例。如果你期望所有数据都遵循类似的模式，你也可以使用异常检测来识别异常值——异常示例可能是低质量的。
- en: 'Just like real data, synthetic data can also be filtered using heuristics.
    In general, you might want to remove examples that are empty or too short for
    your application. If an example is too long, you might want to truncate or remove
    it. You can filter out data by keywords, by user/author, by creation date, by
    metadata, or by source. For example, the Self-Instruct authors ([Wang et al.,
    2022](https://arxiv.org/abs/2212.10560)) filtered out generated examples using
    the following heuristics:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如真实数据一样，合成数据也可以使用启发式方法进行过滤。一般来说，你可能想要移除对于你的应用来说为空或太短的例子。如果例子太长，你可能想要截断或移除它。你可以通过关键词、用户/作者、创建日期、元数据或来源来过滤数据。例如，Self-Instruct的作者（[王等，2022](https://arxiv.org/abs/2212.10560)）使用以下启发式方法过滤了生成的例子：
- en: Repetitive examples
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复的例子
- en: Instructions that are too long or too short
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过长或过短的指令
- en: Examples with the same instruction but different responses
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令相同但响应不同的例子
- en: Examples where the output is a repetition of the input
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是输入重复的例子
- en: Even though there are many techniques to evaluate synthetic data, evaluation
    remains challenging. As with other AI applications, the ultimate quality test
    for AI-generated data is its real-world performance—whether it can improve the
    model’s performance—and synthetic data has passed this test for many models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多技术可以评估合成数据，但评估仍然具有挑战性。与其他AI应用一样，AI生成数据的最终质量测试是其现实世界的表现——是否能够提高模型的表现——而合成数据已经通过了许多模型的这一测试。
- en: Limitations to AI-generated data
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI生成数据的局限性
- en: Given the increasing usefulness of synthetic data, it’s exciting to imagine
    the possibility of never having to worry about human-annotated data again. However,
    while the role of synthetic data will certainly continue to grow in importance
    over time, AI-generated data might never entirely replace human-generated data.
    There are many reasons why, but the four major ones are the difference in quality,
    the limitations of imitation, potential model collapse, and the way AI generation
    of data obscures its lineage.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到合成数据日益增加的有用性，想象不再需要担心人工标注数据是令人兴奋的。然而，尽管合成数据在未来的重要性肯定会继续增长，但AI生成的数据可能永远不会完全取代人工生成的数据。有许多原因可以解释这一点，但其中四个主要原因是质量差异、模仿的局限性、潜在模型崩溃以及AI生成数据模糊其血统的方式。
- en: Quality control
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 质量控制
- en: AI’s generated data can be of low quality, and, as people never tire of saying,
    “garbage in, garbage out.” As mentioned earlier, people will be hesitant to use
    synthetic data if they can’t verify its quality. Being able to develop reliable
    methods and metrics to evaluate data will be essential in making synthetic data
    more useful.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的数据可能质量低下，正如人们常说的，“垃圾输入，垃圾输出。”如前所述，如果人们无法验证其质量，他们可能会犹豫使用合成数据。能够开发可靠的方法和指标来评估数据，对于使合成数据更有用将是至关重要的。
- en: Superficial imitation
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表面的模仿
- en: As warned by “The False Promise of Imitating Proprietary LLMs” ([Gudibande et
    al., 2023](https://arxiv.org/abs/2305.15717)), the perceived performance achieved
    by mimicking might be superficial. This research shows that the imitation models
    are good at mimicking the style of the teacher models but might struggle with
    factual accuracy and generalization to tasks outside the training data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如“模仿专有LLMs的虚假承诺”一文（[Gudibande等，2023](https://arxiv.org/abs/2305.15717)）所警告，通过模仿获得的可感知性能可能只是表面的。这项研究显示，模仿模型擅长模仿教师模型的风貌，但可能在事实准确性和泛化到训练数据之外的任务上遇到困难。
- en: Worse, imitation can force the student model to hallucinate. Imagine if the
    teacher model is capable of answering complex math questions, so its responses
    to those questions are solutions. Training a student model on these solutions
    effectively teaches it to produce answers that look like solutions, even if the
    student model isn’t capable of solving these questions.^([13](ch08.html#id1561))
    Gudibande et al. (2023) suggest that for improvement in reasoning capabilities,
    we need to focus on improving the quality of the base models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，模仿可能会迫使学生模型产生幻觉。想象一下，如果教师模型能够回答复杂的数学问题，那么它的回答就是解决方案。在这些问题上训练学生模型实际上是在教它产生看起来像解决方案的答案，即使学生模型本身无法解决这些问题。[13](ch08.html#id1561)
    Gudibande等（2023）建议，为了提高推理能力，我们需要专注于提高基础模型的质量。
- en: Potential model collapse
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在模型崩溃
- en: 'It’s also unclear how much AI-generated data a model can train on. Some studies
    have shown that *recursively* using AI-generated data in training causes irreversible
    defects in the resulting models, degrading their performance over time. In “The
    Curse of Recursion: Training on Generated Data Makes Models Forget”, [Shumailov
    et al. (2023)](https://arxiv.org/abs/2305.17493) named this phenomenon *model
    collapse* and demonstrated its occurrences in models including Variational Autoencoders,
    Gaussian mixture models, and LLMs. Model collapse can happen during both pre-training
    and post-training.^([14](ch08.html#id1564))'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 也不清楚模型可以训练多少AI生成的数据。一些研究表明，*递归地*使用AI生成的数据进行训练会导致结果模型中不可逆的缺陷，随着时间的推移降低其性能。在“递归的诅咒：在生成数据上训练使模型忘记”
    ([Shumailov 等人, 2023](https://arxiv.org/abs/2305.17493)) 中，该现象被命名为*模型崩溃*，并在包括变分自编码器、高斯混合模型和LLMs在内的模型中展示了其发生。模型崩溃可以在预训练和后训练期间发生。[14](ch08.html#id1564)
- en: One possible explanation is that AI models are more likely to generate probable
    events (e.g., not having cancer) and less likely to generate improbable events
    (e.g., having cancer). Over multiple iterations, probable events become over-represented,
    whereas improbable events become under-represented in the generated data. This
    causes models to output more common events over time while forgetting rare events.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解释是，AI模型更有可能生成可能的事件（例如，没有癌症），而不太可能生成不可能的事件（例如，有癌症）。在多次迭代中，可能的事件变得过度代表，而不可能的事件在生成数据中变得代表性不足。这导致模型随着时间的推移输出更多常见的事件，而忘记罕见的事件。
- en: In “Is Model Collapse Inevitable?” [Gerstgrasser et al. (2024)](https://arxiv.org/abs/2404.01413)
    argue that while model collapse is inevitable if the entire training dataset is
    synthetic, it can be avoided by mixing synthetic data with real data. [Bertrand
    et al. (2023)](https://arxiv.org/abs/2310.00429) and [Dohmatob et al. (2024)](https://arxiv.org/abs/2402.07043)
    show similar results. However, none of these papers has a definitive recommendation
    for the proportion of synthetic data to real data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在“模型崩溃是否不可避免？” ([Gerstgrasser 等人, 2024](https://arxiv.org/abs/2404.01413)) 中，作者认为，如果整个训练数据集是合成的，模型崩溃是不可避免的，但可以通过混合合成数据和真实数据来避免。
    [Bertrand 等人, 2023](https://arxiv.org/abs/2310.00429) 和 [Dohmatob 等人, 2024](https://arxiv.org/abs/2402.07043)
    展示了类似的结果。然而，这些论文中没有任何一篇对合成数据与真实数据比例有明确的推荐。
- en: Some people have been able to improve model performance using a large amount
    of synthetic data. For example, “Common 7B Language Models Already Possess Strong
    Math Capabilities” ([Li et al., 2024](https://arxiv.org/abs/2403.04706)) demonstrates
    that synthetic data is nearly as effective as real data in finetuning Llama 2-7B
    models on math problems. In their experiments, synthetic data shows no clear saturation
    when scaled up to approximately one million samples. Similarly, [Nemotron-4 340B-Instruct](https://oreil.ly/IUA3j)
    (NVIDIA, 2024) used 98% synthetic data during its instruction finetuning and preference
    finetuning phase. However, these experiments were carried out for only one model
    iteration.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人能够通过使用大量合成数据来提高模型性能。例如，“常见的7B语言模型已经拥有强大的数学能力” ([Li 等人, 2024](https://arxiv.org/abs/2403.04706))
    证明了合成数据在微调Llama 2-7B模型解决数学问题时几乎与真实数据一样有效。在他们的实验中，当扩展到大约一百万个样本时，合成数据没有显示出明显的饱和。同样，[Nemotron-4
    340B-Instruct](https://oreil.ly/IUA3j) (NVIDIA, 2024) 在其指令微调和偏好微调阶段使用了98%的合成数据。然而，这些实验仅针对一个模型迭代进行了。
- en: 'AI-generated data might also perpetuate biases. “Data Feedback Loops: Model-driven
    Amplification of Dataset Biases” ([Taori and Hashimoto, 2023](https://oreil.ly/OZxiz))
    demonstrates that when models are trained on datasets that include previous model
    outputs, any existing biases in the model can be amplified. The authors find that
    the more faithful the model’s outputs to the characteristics of the original training
    distribution, the more stable the feedback loop, thus minimizing the risk of bias
    amplification.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的数据也可能 perpetuate biases. “数据反馈循环：数据集偏差的模型驱动放大” ([Taori 和 Hashimoto, 2023](https://oreil.ly/OZxiz))
    证明了当模型在包含先前模型输出的数据集上训练时，模型中存在的任何偏差都可能被放大。作者发现，模型的输出越忠实于原始训练分布的特征，反馈循环就越稳定，从而最小化偏差放大的风险。
- en: Obscure data lineage
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据血缘不明确
- en: This limitation of AI-generated data is more subtle. AI generation obscures
    data lineage. AI models are influenced by their training data and can sometimes
    regurgitate it without the user knowing. This creates risks. Let’s say you use
    model X to generate data to train your model. If model X was trained on data with
    copyright violations, your model might also violate copyrights.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能生成数据的这种限制更为微妙。人工智能生成会模糊数据来源。人工智能模型会受到其训练数据的影响，有时会在用户不知情的情况下重复它。这会带来风险。假设你使用模型
    X 生成数据来训练你的模型。如果模型 X 是在侵犯版权的数据上训练的，那么你的模型也可能侵犯版权。
- en: Or imagine you then use benchmark B to evaluate your model, which shows a strong
    performance. However, if model X was also trained on benchmark B, your result
    on B is contaminated. Without clear data lineage, it’s hard to assess a model’s
    commercial viability or trust its performance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 或者想象你随后使用基准 B 来评估你的模型，它显示出强大的性能。然而，如果模型 X 也接受了基准 B 的训练，那么你在 B 上的结果就会受到污染。没有明确的数据来源，很难评估模型的市场可行性或信任其性能。
- en: 'We’ve discussed how to use AI to generate data and how to evaluate the generated
    data, as well as its limitations. In the next section, let’s switch gears to discuss
    one special use case of data synthesis where AI-generated data isn’t just supplementary
    but is required: model distillation.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何使用人工智能生成数据以及如何评估生成数据及其局限性。在下一节中，让我们转换话题，讨论数据合成的特殊用例，其中人工智能生成的数据不仅仅是补充，而是必需的：模型蒸馏。
- en: Model Distillation
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型蒸馏
- en: '*Model distillation* (also called *knowledge distillation*) is a method in
    which a small model (student) is trained to mimic a larger model (teacher) ([Hinton
    et al., 2015](https://arxiv.org/abs/1503.02531)). The knowledge of the big model
    is distilled into the small model, hence the term distillation.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型蒸馏*（也称为*知识蒸馏*）是一种方法，其中一个小型模型（学生）被训练来模仿一个更大的模型（教师）([Hinton 等人，2015](https://arxiv.org/abs/1503.02531))。大模型的知识被蒸馏到小模型中，因此得名蒸馏。'
- en: Traditionally, the goal of model distillation is to produce smaller models for
    deployment. Deploying a big model can be resource-intensive. Distillation can
    produce a smaller, faster student model that retains performance comparable to
    the teacher. For example, DistilBERT, a model distilled from BERT, reduces the
    size of a BERT model by 40% while retaining 97% of its language comprehension
    capabilities and being 60% faster ([Sanh et al., 2019](https://arxiv.org/abs/1910.01108)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，模型蒸馏的目标是生成更小的模型以供部署。部署大型模型可能需要大量资源。蒸馏可以生成一个更小、更快的学生模型，同时保持与教师模型相当的性能。例如，从
    BERT 蒸馏出的 DistilBERT 模型将 BERT 模型的尺寸减少了 40%，同时保留了 97% 的语言理解能力，并且速度提高了 60% ([Sanh
    等人，2019](https://arxiv.org/abs/1910.01108))。
- en: The student model can be trained from scratch like DistilBERT or finetuned from
    a pre-trained model like [Alpaca](https://github.com/tatsu-lab/stanford_alpaca).
    In 2023, Taori et al. finetuned Llama-7B, the 7-billion-parameter version of Llama,
    on examples generated by *text-davinci-003*, a 175-billion-parameter model. The
    resulting model, Alpaca, behaves similarly to *text-davinci-003*, while being
    4% the size of the teacher model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型可以从头开始训练，如 DistilBERT，或者从预训练模型如 [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    进行微调。在 2023 年，Taori 等人对 Llama-7B 进行了微调，这是 Llama 的 70 亿参数版本，使用的是由 1750 亿参数模型 *text-davinci-003*
    生成的示例。生成的模型 Alpaca 的行为与 *text-davinci-003* 类似，但其大小仅为教师模型的 4%。
- en: Note
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not all models can be distilled. Many model licenses prohibit using their outputs
    to train other models, particularly to train competing models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有模型都可以进行蒸馏。许多模型许可证禁止使用其输出训练其他模型，尤其是训练竞争模型。
- en: Synthetic instruction data is commonly used together with adapter-based techniques,
    such as LoRA. For example, [BuzzFeed](https://oreil.ly/U7gfm) finetuned a Flan-T5
    model using LoRA and examples generated by OpenAI’s *text-davinci-003*. The resulting
    model reduced their inference cost by 80%, though it was unclear how well the
    model performed (2023).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 合成指令数据通常与基于适配器的技术（如 LoRA）一起使用。例如，[BuzzFeed](https://oreil.ly/U7gfm) 使用 LoRA
    和 OpenAI 的 *text-davinci-003* 生成的示例微调了一个 Flan-T5 模型。生成的模型将他们的推理成本降低了 80%，尽管不清楚该模型的表现如何（2023）。
- en: Note that not all training with synthetic data is model distillation. Model
    distillation implies that the teacher model’s performance is the student’s gold
    standard. However, it’s possible to use synthetic data to train a student model
    that is larger and more powerful than the teacher.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并非所有使用合成数据的训练都是模型蒸馏。模型蒸馏意味着教师模型的表现是学生模型的金标准。然而，使用合成数据训练一个比教师模型更大、更强大的学生模型是可能的。
- en: Model bootstrapping with reverse instruction ([Li et al., 2023](https://arxiv.org/abs/2308.06259)),
    discussed in the previous section, is one example. Another example is NVIDIA’s
    Nemotron-4\. A team of NVIDIA researchers first pre-trained a 340B parameter base
    model. This base model was then finetuned using instruction and preference data
    generated by [Mixtral-8x7B-Instruct-v0.1](https://oreil.ly/-Vd_q) (Jiang et al.,
    2024), a 56-billion-parameter mixture-of-experts model.^([15](ch08.html#id1573))
    The resulting student model, Nemotron-4-340B-Instruct, outperformed the teacher
    model on a variety of tasks ([NVIDIA, 2024](https://oreil.ly/iGToR)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中讨论的模型自举反向指令([Li 等人，2023](https://arxiv.org/abs/2308.06259))是一个例子。另一个例子是
    NVIDIA 的 Nemotron-4。NVIDIA 研究团队首先预训练了一个 340B 参数的基础模型。然后，该基础模型使用由 [Mixtral-8x7B-Instruct-v0.1](https://oreil.ly/-Vd_q)
    (Jiang 等人，2024) 生成的指令和偏好数据进行微调，这是一个 560 亿参数的专家混合模型.^([15](ch08.html#id1573)) 结果生成的学生模型，Nemotron-4-340B-Instruct，在各种任务上优于教师模型([NVIDIA，2024](https://oreil.ly/iGToR))。
- en: The Llama 3 paper notes that while training on data generated by a more competent
    model can significantly improve a model’s performance, training indiscriminately
    on self-generated data doesn’t improve the model’s performance and can even degrade
    it. However, by introducing mechanisms to verify the quality of synthetic data
    and using only verified synthetic data, they were able to continually improve
    a model using its generated data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 论文指出，虽然使用更胜任的模型生成数据训练可以显著提高模型性能，但无差别地使用自生成数据并不能提高模型性能，甚至可能降低性能。然而，通过引入验证合成数据质量的机制，并仅使用经过验证的合成数据，他们能够不断使用其生成数据来提高模型。
- en: Data Processing
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理
- en: Data needs to be processed according to the requirements of each use case. This
    section discusses some data processing steps for reference.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要根据每个用例的要求进行处理。本节讨论了一些数据处理的步骤以供参考。
- en: I find it helpful to read model papers that disclose their dataset details,
    as they often contain great tips on how the researchers curated, generated, and
    processed data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现阅读公开其数据集详细信息的模型论文很有帮助，因为它们通常包含关于研究人员如何整理、生成和处理数据的宝贵建议。
- en: Tip
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'With a large amount of data, each of these processing steps can take hours,
    if not days. Tips to help optimize efficiency during the process include:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量数据时，每个处理步骤可能需要数小时，甚至数天。以下是一些在处理过程中提高效率的建议：
- en: You can do these data processing steps in whichever order saves time and compute.
    For example, if it takes more time to clean each example than to deduplicate data,
    you might want to remove the duplicated examples first before cleaning them. But
    if deduplication takes more time than filtering out low-quality data, filter out
    low-quality data first.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以根据节省时间和计算资源的方式执行这些数据处理步骤。例如，如果清理每个示例所需的时间比去重数据所需的时间更长，您可能希望先删除重复的示例，然后再进行清理。但如果去重所需的时间比过滤低质量数据所需的时间更长，则应先过滤低质量数据。
- en: Always do trial runs to validate that your processing scripts work as expected
    before applying the scripts to all your data.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将脚本应用于所有数据之前，始终进行试运行以验证您的处理脚本是否按预期工作。
- en: 'Avoid changing data in place. Consider keeping a copy of the original data
    for two reasons:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免就地更改数据。考虑保留原始数据的副本，原因如下：
- en: You or another team might need to process the data in different ways for other
    applications.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您或另一个团队可能需要以不同的方式处理数据以适应其他应用。
- en: Bugs in your scripts can potentially corrupt your data.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您脚本中的错误可能会损坏您的数据。
- en: Inspect Data
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据
- en: Let’s say that after combing through public and internal data, you’ve gathered
    a raw dataset. The first thing to do is inspect the data to get a sense of its
    quality. Get the data’s information and statistics. Where does the data come from?
    How has it been processed? What else has it been used for?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在整理公共和内部数据后，已经收集到了原始数据集。首先要做的是检查数据，以了解其质量。获取数据的信息和统计数据。数据来自哪里？它是如何被处理的？它还被用于其他什么目的？
- en: Plot the distribution of tokens (to see what tokens are common), input lengths,
    response lengths, etc. Does the data use any special tokens? Can you get a distribution
    of the topics and languages in the data? How relevant are these topics and languages
    to your task?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制标记的分布（以查看哪些标记是常见的），输入长度，响应长度等。数据是否使用了任何特殊标记？您能否获取数据中主题和语言的分布？这些主题和语言与您的任务的相关性如何？
- en: You can be creative in the statistics to use to understand your data. For example,
    [a group of Microsoft researchers (2023)](https://arxiv.org/abs/2304.03277) used
    the distribution of (verb, direct object, noun) pairs and response length to compare
    the difference between GPT-3’s and GPT-4’s generations for the same set of instructions,
    as shown in [Figure 8-6](#ch08_figure_7_1730130931959005) and [Figure 8-7](#ch08_figure_8_1730130931959026).
    This type of analysis is helpful not only to evaluate data but also to evaluate
    models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在统计数据上发挥创意，以理解你的数据。例如，[一组微软研究人员（2023）](https://arxiv.org/abs/2304.03277)
    使用了（动词，直接宾语，名词）对和响应长度的分布来比较 GPT-3 和 GPT-4 在同一组指令下的生成差异，如图 8-6 和图 8-7 所示。这种分析不仅有助于评估数据，也有助于评估模型。
- en: '![A graph with numbers and a bar chart'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含数字和条形图的图表'
- en: Description automatically generated](assets/aien_0806.png)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0806.png)
- en: Figure 8-6\. One statistic you can use is the distribution of (verb, direct
    object noun) in your data. Image from “Instruction Tuning with GPT-4” (Peng et
    al., 2023).
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6。你可以使用的一个统计是数据中（动词，直接宾语，名词）的分布。图片来自“使用 GPT-4 进行指令调整”（Peng 等人，2023）。
- en: '![A graph showing a line of output sequence'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![展示输出序列线的图表'
- en: Description automatically generated with medium confidence](assets/aien_0807.png)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成中等置信度的描述](assets/aien_0807.png)
- en: Figure 8-7\. The distribution of response length for GPT-4 and GPT-3\. Image
    from “Instruction Tuning with GPT-4” (Peng et al., 2023).
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7。GPT-4 和 GPT-3 的响应长度分布。图片来自“使用 GPT-4 进行指令调整”（Peng 等人，2023）。
- en: GPT-4 seems to have a broader and more diverse range of verb-noun pairings and
    tends to generate longer responses.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 似乎有更广泛和更多样化的动词-名词搭配，并且倾向于生成更长的响应。
- en: Plot these distributions by data source, time, annotator, etc. Do you notice
    any question patterns that tend to get longer/shorter responses or higher/lower
    scores? Are there any outliers? What might be the cause of these outliers? What
    to do with them?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据源、时间、标注者等来绘制这些分布。你是否注意到某些问题模式倾向于得到更长/更短的响应或更高/更低的评分？是否有异常值？这些异常值可能的原因是什么？如何处理它们？
- en: If the scores are supposed to follow a normal distribution, do scores by all
    annotators follow a normal distribution? You might notice that some annotators
    tend to give much shorter responses or bias toward higher scores, and it’s up
    to you to decide what to do with their annotations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果评分应该遵循正态分布，所有标注者的评分是否都遵循正态分布？你可能会注意到一些标注者倾向于给出非常短的响应或偏向于更高的评分，而决定如何处理他们的标注取决于你。
- en: If each example has more than one annotation, compute the inter-annotator disagreement.
    Check the examples with conflicting annotations and resolve the conflicts.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个示例有多个标注，计算标注者之间的不一致性。检查有冲突标注的示例并解决冲突。
- en: 'There are many data exploration tools you should use, but they won’t be replacements
    for manual data inspection. In every project I’ve worked on, *staring at data*
    *for just* *15 minutes usually gives me some insight that could save me hours
    of headaches*. [Greg Brockman, an OpenAI co-founder](https://x.com/gdb/status/1622683988736479232),
    tweeted: “Manual inspection of data has probably the highest value-to-prestige
    ratio of any activity in machine learning.”'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多数据探索工具你应该使用，但它们不能替代手动数据检查。在我参与的每一个项目中，*仅仅盯着数据* *15 分钟通常就能让我获得一些可以节省我数小时头痛的见解*。[OpenAI
    联合创始人 Greg Brockman](https://x.com/gdb/status/1622683988736479232) 推特说：“数据的手动检查可能是机器学习中最有价值且声望最高的活动。”
- en: Look at your data to see if the examples make sense. If it’s annotated data,
    pick out a few queries and try to annotate them yourself to see if your annotations
    match the given annotations. This will give you a sense of how trustworthy the
    annotations are. Fact-check the responses. How unique are the examples? Are there
    any examples with the same query but with different responses? Are there any examples
    with the same responses but with different queries?
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 查看你的数据，看看示例是否合理。如果是标注数据，挑选几个查询并尝试自己标注它们，看看你的标注是否与给定的标注相符。这将让你对标注的可靠性有一个感觉。核实响应。示例的独特性如何？是否有相同查询但响应不同的示例？是否有相同响应但查询不同的示例？
- en: Deduplicate Data
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去重数据
- en: Duplicated data can skew the data distribution and introduce biases into your
    model. Imagine a dataset that looks like [Table 8-3](#ch08_table_3_1730130931981189).
    The duplicated entries might lead the model to the wrong conclusion that all red-colored
    items should be expensive. Duplications can cause test set contamination. When
    splitting duplicated data into train and test sets, one example might be in the
    train set and its duplicate in the test set.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 重复数据可能会扭曲数据分布，并将偏差引入你的模型。想象一个看起来像[表8-3](#ch08_table_3_1730130931981189)的数据集。重复的条目可能导致模型得出错误的结论，即所有红色物品都应该是昂贵的。重复可能导致测试集污染。当将重复数据分割成训练集和测试集时，一个示例可能在训练集中，而其重复项在测试集中。
- en: Table 8-3\. A toy dataset with duplicate examples in grey cells.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-3. 一个包含灰色单元格中重复示例的玩具数据集。
- en: '|  | Input (Product description) | Output (Price) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入（产品描述） | 输出（价格） |'
- en: '| --- | --- | --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | `{item: pencil, color: red}` | `$20` |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `{item: pencil, color: red}` | `$20` |'
- en: '| 2 | `{item: compass, color: green}` | `$2` |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `{item: compass, color: green}` | `$2` |'
- en: '| 3 | `{item: pencil, color: red}` | `$20` |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `{item: pencil, color: red}` | `$20` |'
- en: '| 4 | `{item: pencil, color: red}` | `$20` |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 4 | `{item: pencil, color: red}` | `$20` |'
- en: '| 5 | `{item: pencil, color: green}` | `$1` |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 5 | `{item: pencil, color: green}` | `$1` |'
- en: Multiple studies have shown the negative impact of training data duplications
    on model performance; see [Lee et al. (2021)](https://arxiv.org/abs/2107.06499)
    and [Tirumala et al. (2023)](https://arxiv.org/abs/2308.12284). An Anthropic study
    demonstrated that repeating 0.1% of the data 100 times can cause an 800M parameter
    model’s performance to degrade to that of a 400M parameter model despite the other
    90% of the training tokens remaining unique ([Hernandez et al., 2022](https://arxiv.org/abs/2205.10487)).
    Even when duplications don’t hurt your model’s performance, they can waste your
    time and compute.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究表明，训练数据重复对模型性能的负面影响；参见[Lee等人（2021）](https://arxiv.org/abs/2107.06499)和[Tirumala等人（2023）](https://arxiv.org/abs/2308.12284)。一项Anthropic研究显示，将0.1%的数据重复100次可以使一个800M参数模型的性能下降到400M参数模型的水平，尽管其他90%的训练标记仍然是唯一的([Hernandez等人，2022](https://arxiv.org/abs/2205.10487))。即使重复不会损害你的模型性能，它们也可能浪费你的时间和计算资源。
- en: 'Depending on the data, there are many forms of duplication, some of which are
    harder to detect. For example, here are a few types of duplications in a dataset
    of documents:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据的不同，重复的形式有很多种，其中一些比较难以检测。例如，以下是一个文档数据集中几种重复的类型：
- en: 'Whole document duplications: the same document appearing more than once.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个文档重复：相同的文档出现多次。
- en: 'Intra-document duplications: e.g., the same paragraph appears twice in one
    document.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档内重复：例如，一个文档中相同的段落出现两次。
- en: 'Cross-document duplications: e.g., the same popular quote appears in multiple
    documents.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨文档重复：例如，相同的流行引语出现在多个文档中。
- en: What can be considered duplications also depends on your definition. For example,
    do you want to deal with duplications at the document level, paragraph level,
    sentence level, or token level? Would two texts have to match exactly to be considered
    duplicates, or would an 80% overlap be sufficient? Are two lists considered duplicates
    if they have the same items but in different order?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 可以被认为是重复的内容也取决于你的定义。例如，你希望在文档级别、段落级别、句子级别还是标记级别处理重复？两个文本是否必须完全匹配才能被认为是重复的，或者80%的重叠就足够了？如果两个列表有相同的条目但顺序不同，它们是否被认为是重复的？
- en: 'The task of deduplication can leverage the same techniques used for similarity
    measurements (discussed in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067)).
    Data deduplication is also used for identity resolution, determining whether two
    identities (e.g., two social media profiles) are the same. Here are some concrete
    ways you can deduplicate data:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 去重任务可以利用用于相似度测量的相同技术（第3章中讨论）（ch03.html#ch03a_evaluation_methodology_1730150757064067）进行。数据去重也用于身份解析，确定两个身份（例如，两个社交媒体档案）是否相同。以下是一些具体的数据去重方法：
- en: Pairwise comparison
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 配对比较
- en: Compute the similarity score of each example to every other example in the dataset,
    using exact match, n-gram match, fuzzy match, or semantic similarity score, as
    discussed in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067).
    This approach can be expensive with large datasets, however.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 使用精确匹配、n-gram匹配、模糊匹配或语义相似度得分来计算数据集中每个示例与其他每个示例的相似度得分，如第3章中所述（ch03.html#ch03a_evaluation_methodology_1730150757064067）。然而，这种方法在大数据集中可能会很昂贵。
- en: Hashing
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希
- en: Hash examples into different buckets and check only among examples that fall
    into the same bucket. Hash-related deduplication methods include [MinHash](https://en.wikipedia.org/wiki/MinHash)
    and [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将示例哈希到不同的桶中，并只检查落入同一桶的示例。与哈希相关的去重方法包括[MinHash](https://en.wikipedia.org/wiki/MinHash)和[Bloom
    filter](https://en.wikipedia.org/wiki/Bloom_filter)。
- en: Dimensionality reduction
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低
- en: Use a dimensionality reduction technique to first reduce the dimensions of your
    data and then do a pairwise comparison. Many techniques used for vector search,
    as discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386), can
    be used for this.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 使用维度降低技术首先降低数据的维度，然后进行成对比较。正如在第6章中讨论的用于向量搜索的许多技术可以用于此。
- en: A quick search will return many libraries that help with deduplication. Some
    of them are [dupeGuru](https://github.com/arsenetar/dupeguru), [Dedupe](https://github.com/dedupeio/dedupe),
    [datasketch](https://github.com/ekzhu/datasketch), [TextDistance](https://github.com/life4/textdistance),
    [TheFuzz](https://github.com/seatgeek/thefuzz), and [deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets).^([16](ch08.html#id1584))
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 快速搜索将返回许多帮助进行去重（deduplication）的库。其中一些是[dupeGuru](https://github.com/arsenetar/dupeguru)、[Dedupe](https://github.com/dedupeio/dedupe)、[datasketch](https://github.com/ekzhu/datasketch)、[TextDistance](https://github.com/life4/textdistance)、[TheFuzz](https://github.com/seatgeek/thefuzz)和[duplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)^([16](ch08.html#id1584))。
- en: Clean and Filter Data
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理和过滤数据
- en: Data needs to be cleaned to make your model performant and safe.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要被清理以确保模型的表现力和安全性。
- en: First, you might want to remove extraneous formatting tokens. Since many public
    datasets are scraped from the internet, extraneous HTML tags are quite common.
    Unless you want to train your model on HMTL tags, remove them. [Databricks](https://oreil.ly/Gbu2T)
    found that removing extraneous Markdown and HTML tokens improved their model’s
    accuracy by 20% while reducing their input token lengths by 60%.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可能想要移除额外的格式化标记。由于许多公共数据集是从互联网上抓取的，额外的HTML标签相当常见。除非你想要在HTML标签上训练你的模型，否则请移除它们。[Databricks](https://oreil.ly/Gbu2T)发现，移除额外的Markdown和HTML标记将他们的模型准确率提高了20%，同时将输入标记长度减少了60%。
- en: You need to clean your data of anything that isn’t compliant with your policies,
    such as PII, sensitive data, copyrighted data, or data that is considered toxic.
    Techniques discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)
    can help. Remove all the fields that you’re not allowed to use, such as zip code,
    name, and gender.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要清理不符合你政策的数据，例如PII、敏感数据、受版权保护的数据或被认为是有毒的数据。第4章中讨论的技术可以帮助。移除所有不允许使用的字段，例如邮政编码、姓名和性别。
- en: You also might want to remove low-quality data, using techniques discussed in
    [“Data verification”](#ch08_data_verification_1730130932021284) to detect low-quality
    data.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可能想要移除低质量数据，使用在第8章“数据验证”中讨论的技术来检测低质量数据。
- en: Manual inspection of data is especially important in this step. Staring at data
    might help you notice patterns that you can use as heuristics to detect low-quality
    data. Heuristics to detect low-quality data might be non-obvious. For example,
    [Kern et al. (2024)](https://arxiv.org/html/2311.14212v2) found that annotations
    made in the second half of an annotation session are of lower quality, likely
    due to annotator boredom or fatigue.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，手动检查数据尤为重要。盯着数据可能有助于你注意到可以用作启发式方法来检测低质量数据的模式。检测低质量数据的启发式方法可能并不明显。例如，[Kern等人（2024）](https://arxiv.org/html/2311.14212v2)发现，在标注会话的后半段所做的标注质量较低，这可能是由于标注者无聊或疲劳造成的。
- en: If there is more data than you need or can afford to use (e.g., due to your
    compute budget), you can further filter your data. For example, you can use *active
    learning* techniques to select examples that are the most helpful for your model
    to learn from. You can also use [importance sampling](https://oreil.ly/Tb4-W)
    to find examples that are most important to your task. Their efficiencies depend
    on whether you have a good way to evaluate the importance of each training example.
    Meta researchers, in their paper on data pruning ([Sorscher et al., 2022](https://arxiv.org/abs/2206.14486)),
    concluded that the discovery of good data-pruning metrics can significantly reduce
    the resource costs of modern deep learning.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要或能够使用的数据量超过你的预算（例如，由于你的计算预算），你可以进一步过滤你的数据。例如，你可以使用*主动学习*技术来选择对模型学习最有帮助的示例。你也可以使用[重要性抽样](https://oreil.ly/Tb4-W)来找到对你任务最重要的示例。它们的效率取决于你是否有一个好的方法来评估每个训练示例的重要性。Meta研究人员在其关于数据剪枝的论文中([Sorscher等人，2022](https://arxiv.org/abs/2206.14486))得出结论，发现好的数据剪枝指标可以显著降低现代深度学习的资源成本。
- en: Format Data
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式数据
- en: Once you’ve deduplicated and cleaned your data, you need to get it into the
    right format expected by the model you’re finetuning. Each model uses a specific
    tokenizer and expects data in a specific chat template, as discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551).
    Getting data into the wrong chat template can cause strange bugs in your model.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你去除了重复并清理了你的数据，你需要将其转换为模型微调所期望的正确格式。每个模型都使用特定的分词器，并期望数据以特定的聊天模板格式呈现，如第5章中讨论的[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)。将数据放入错误的聊天模板可能导致模型中出现奇怪的错误。
- en: If you’re doing supervised finetuning, your data is most likely in the format
    (instruction, response). Instructions can be further decomposed into (system prompt,
    user prompt). If you’ve graduated to finetuning from prompt engineering, the instructions
    used for finetuning might be different from the instructions used during prompt
    engineering. During finetuning, instructions typically don’t need task descriptions
    or examples. If you have sufficient training examples, the model can learn the
    expected behavior of the task from the examples directly.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在进行监督式微调，你的数据最可能是以下格式（指令，响应）。指令可以进一步分解为（系统提示，用户提示）。如果你已经从提示工程过渡到微调，用于微调的指令可能与提示工程期间使用的指令不同。在微调过程中，通常不需要任务描述或示例。如果你有足够的训练示例，模型可以直接从示例中学习任务的预期行为。
- en: 'As an example, imagine that you’ve been using this three-shot instruction for
    your food classification task with a base model:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你一直在使用这个三步指令与基础模型进行食品分类任务：
- en: '[PRE2]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For finetuning, all the examples included in the 3-shot prompt can be converted
    into training examples. The training data for finetuning will look like [Table 8-4](#ch08_table_4_1730130931981212).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，3次提示中包含的所有示例都可以转换为训练示例。微调的训练数据将类似于[表8-4](#ch08_table_4_1730130931981212)。
- en: Table 8-4\. Example training data used for a food classification task.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-4\. 用于食品分类任务的示例训练数据。
- en: '| Example ID | Input | Output |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 示例 ID | 输入 | 输出 |'
- en: '| --- | --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | `burger -->` | `edible` |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `burger -->` | `edible` |'
- en: '| 2 | `car -->` | `inedible` |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `car -->` | `inedible` |'
- en: '| 3 | `mushroom -->` | `edible` |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `mushroom -->` | `edible` |'
- en: '| … | … | … |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: 'Once the model is finetuned, you can use a prompt as simple as:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完成微调，你可以使用一个像这样简单的提示：
- en: '[PRE3]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is much shorter than the prompt used with the base model. Therefore, if
    you’re worried about the input tokens of your instructions, finetuning can be
    one way to help manage the cost.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这比与基础模型一起使用的提示要短得多。因此，如果你担心指令的输入标记，微调可以是一种帮助管理成本的方法。
- en: Different finetuning data formats can impact your finetuned model’s performance.
    Experiments to determine the best format for you can be helpful.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的微调数据格式可能影响微调模型的性能。确定最适合你的最佳格式的实验可能是有帮助的。
- en: 'When you use the finetuned model, make sure that the prompts you use match
    the format of the finetuning data. For example, if the training data uses the
    prompt in the format “burger -->”, any of the following prompts can cause issues:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用微调后的模型时，请确保你使用的提示与微调数据的格式相匹配。例如，如果训练数据使用“burger -->”格式的提示，以下任何提示都可能导致问题：
- en: '“burger”: missing the end arrow'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “burger”：缺少结束箭头
- en: '“Item: burger -->”: extra prefix'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“Item: burger -->”：附加了额外前缀'
- en: '“burger --> ”: extra space appended'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “burger --> ”：附加了额外空格
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Even though the actual process of creating training data is incredibly intricate,
    the principles of creating a dataset are surprisingly straightforward. To build
    a dataset to train a model, you start by thinking through the behaviors you want
    your model to learn and then design a dataset to show these behaviors. Due to
    the importance of data, teams are introducing dedicated data roles responsible
    for acquiring appropriate datasets while ensuring privacy and compliance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管创建训练数据的实际过程极其复杂，但创建数据集的原则却出奇地简单。为了构建一个用于训练模型的数据库，你首先需要思考你希望模型学习的行为，然后设计一个数据集来展示这些行为。由于数据的重要性，团队正在引入专门的数据角色，负责获取适当的数据库，同时确保隐私和合规性。
- en: 'What data you need depends not only on your use case but also on the training
    phase. Pre-training requires different data from instruction finetuning and preferred
    finetuning. However, dataset design across training phases shares the same three
    core criteria: quality, coverage, and quantity.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的数据不仅取决于你的用例，还取决于训练阶段。预训练需要与指令微调和偏好微调不同的数据。然而，训练阶段之间的数据集设计共享相同的三个核心标准：质量、覆盖面和数量。
- en: While how much data a model is trained on grabs headlines, having high-quality
    data with sufficient coverage is just as important. A small amount of high-quality
    data can outperform a large amount of noisy data. Similarly, many teams have found
    that increasing the diversity of their datasets is key to improving their models’
    performance.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型训练所使用的数据量常常成为焦点，但拥有高质量且覆盖面足够的数据同样重要。少量高质量的数据可以超越大量噪声数据的表现。同样，许多团队发现，增加数据集的多样性是提高模型性能的关键。
- en: Due to the challenge of acquiring high-quality data, many teams have turned
    to synthetic data. While generating data programmatically has long been a goal,
    it wasn’t until AI could create realistic, complex data that synthetic data became
    a practical solution for many more use cases. This chapter discussed different
    techniques for data synthesis with a deep dive into synthesizing instruction data
    for finetuning.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 由于获取高质量数据的挑战，许多团队转向使用合成数据。虽然程序化生成数据长期以来一直是目标，但直到AI能够创建真实、复杂的数据，合成数据才成为更多用例的实用解决方案。本章讨论了数据合成的不同技术，并深入探讨了用于微调的指令数据合成。
- en: Just like real data, synthetic data must be evaluated to ensure its quality
    before being used to train models. Evaluating AI-generated data is just as tricky
    as evaluating other AI outputs, and people are more likely to use generated data
    that they can reliably evaluate.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 正如真实数据一样，合成数据在使用于训练模型之前必须经过评估以确保其质量。评估AI生成数据与评估其他AI输出一样棘手，人们更倾向于使用他们可以可靠评估的生成数据。
- en: Data is challenging because many steps in dataset creation aren’t easily automatable.
    It’s hard to annotate data, but it’s even harder to create annotation guidelines.
    It’s hard to automate data generation, but it’s even harder to automate verifying
    it. While data synthesis helps generate more data, you can’t automate thinking
    through what data you want. You can’t easily automate annotation guidelines. You
    can’t automate paying attention to details.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 数据具有挑战性，因为数据集创建中的许多步骤并不容易自动化。标注数据很困难，但创建标注指南更难。自动化数据生成很困难，但自动化验证它更难。虽然数据合成有助于生成更多数据，但你不能自动化思考你想要的数据。你不能轻易自动化标注指南。你不能自动化关注细节。
- en: However, challenging problems lead to creative solutions. One thing that stood
    out to me when doing research for this chapter is how much creativity is involved
    in dataset design. There are so many ways people construct and evaluate data.
    I hope that the range of data synthesis and verification techniques discussed
    in this chapter will give you inspiration for how to design your dataset.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，挑战性的问题会导致创造性的解决方案。在进行本章研究时，让我印象深刻的一点是数据集设计中的创造性。人们构建和评估数据的方式有很多。我希望本章讨论的数据合成和验证技术的范围能够给你在设计数据集时提供灵感。
- en: Let’s say that you’ve curated a wonderful dataset that allows you to train an
    amazing model. How should you serve this model? The next chapter will discuss
    how to optimize inference for latency and cost.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经整理了一个优秀的数据集，可以用来训练一个出色的模型。你应该如何部署这个模型？下一章将讨论如何优化推理以降低延迟和成本。
- en: ^([1](ch08.html#id1508-marker)) The increasing importance of data is reflected
    in how data effort changed from GPT-3 to GPT-4\. In the contribution list for
    GPT-3 ([OpenAI, 2020](https://oreil.ly/R4-VI)), only two people were credited
    with data collecting, filtering, and deduplicating, and conducting overlap analysis
    on the training data. This dramatically changed three years later. For GPT-4 ([OpenAI,
    2023](https://oreil.ly/F9Fyc)), eighty people were credited for being involved
    in different data processes. This list doesn’t yet include data annotators that
    OpenAI contracted through data providers. For something that sounds as simple
    as a ChatML format, eleven people were involved, and many of them are senior researchers.
    Back in their [2016 AMA (ask me anything) thread](https://oreil.ly/h-lAl), Wojciech
    Zaremba, one of OpenAI’s cofounders, said that they intended to conduct most of
    their research using publicly available datasets.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#id1508-marker)) 数据日益重要的趋势反映在从GPT-3到GPT-4的数据努力变化上。在GPT-3的贡献列表中([OpenAI,
    2020](https://oreil.ly/R4-VI))，只有两个人被归功于数据收集、过滤、去重以及在对训练数据进行重叠分析。三年后，这一情况发生了戏剧性的变化。对于GPT-4([OpenAI,
    2023](https://oreil.ly/F9Fyc))，有八十人被归功于参与不同的数据处理过程。这份名单尚未包括OpenAI通过数据提供商签订的数据标注员。对于听起来如此简单的ChatML格式，就有十一个人参与，其中许多人都是资深研究员。回到他们2016年的AMA(问我任何问题)帖子([OpenAI,
    2023](https://oreil.ly/h-lAl))，OpenAI的联合创始人之一Wojciech Zaremba表示，他们打算使用公开可用的数据集进行大部分研究。
- en: ^([2](ch08.html#id1512-marker)) If you use a lot of data, ensuring data compliance
    alone can be a full-time job.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.html#id1512-marker)) 如果你使用大量数据，确保数据合规性本身就可以是一项全职工作。
- en: '^([3](ch08.html#id1515-marker)) While I love writing, one of the things I absolutely
    do not enjoy is trying to condense everyone’s opinions into one single definition.
    [IBM](https://oreil.ly/3d_EG) defined data quality along seven dimensions: completeness,
    uniqueness, validity, timeliness, accuracy, consistency, and fitness for purpose.
    [Wikipedia](https://en.wikipedia.org/wiki/Data_quality) added accessibility, comparability,
    credibility, flexibility, and plausibility. Many of these definitions focus on
    data quality in a broad range of use cases. Here, I want to focus on data quality
    for finetuning.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.html#id1515-marker)) 虽然我喜欢写作，但我绝对不喜欢尝试将每个人的观点压缩成一个单一的定义。[IBM](https://oreil.ly/3d_EG)从七个维度定义了数据质量：完整性、唯一性、有效性、及时性、准确性、一致性和适用性。[维基百科](https://en.wikipedia.org/wiki/Data_quality)增加了可访问性、可比性、可信度、灵活性和合理性。许多这些定义都关注于广泛使用场景中的数据质量。在这里，我想专注于微调中的数据质量。
- en: ^([4](ch08.html#id1516-marker)) One painful bug I still remember is when a float
    column in my data was wrongly stored as integers, which round these values, leading
    to perplexing behaviors.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.html#id1516-marker)) 我仍然记得的一个痛苦的错误是，当我的数据中的一个浮点列错误地存储为整数时，这些值被四舍五入，导致令人困惑的行为。
- en: ^([5](ch08.html#id1517-marker)) While this doesn’t refer to the uniqueness of
    your data, having data that nobody else has can be extremely valuable.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch08.html#id1517-marker)) 虽然这并不涉及你数据的唯一性，但拥有别人没有的数据可以极其有价值。
- en: ^([6](ch08.html#id1523-marker)) In [*Designing Machine Learning Systems*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/),
    I also covered other techniques to reduce the demand for annotated data, including
    weak supervision, semi-supervision, and active learning.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch08.html#id1523-marker)) 在[*设计机器学习系统*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)中，我还介绍了其他减少对标注数据需求的技术，包括弱监督、半监督和主动学习。
- en: ^([7](ch08.html#id1526-marker)) I’ve heard so many companies talking about data
    flywheels in their pitches that I’m convinced it isn’t legal to start an AI startup
    without mentioning the data flywheel.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch08.html#id1526-marker)) 我听到许多公司在他们的提案中谈论数据飞轮，以至于我相信不提及数据飞轮就无法合法地启动一个AI初创公司。
- en: ^([8](ch08.html#id1535-marker)) My book, [*Designing Machine Learning Systems*](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch04.html#perturbation),
    discusses data augmentation in Chapter 4.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch08.html#id1535-marker)) 我的书[*设计机器学习系统*](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch04.html#perturbation)在第4章讨论了数据增强。
- en: ^([9](ch08.html#id1536-marker)) One obvious example that I didn’t include in
    the main text is when you want to train a model to detect AI-generated content.
    You need AI-generated content as training examples.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch08.html#id1536-marker)) 我在正文文本中没有包括的一个明显例子是，当你想要训练一个模型来检测AI生成的内容时。你需要AI生成的内容作为训练示例。
- en: ^([10](ch08.html#id1539-marker)) Many awesome games are possible only because
    of procedural generation. Games like *Minecraft* and *No Man’s Sky* use noise
    functions and fractal algorithms to create vast, immersive worlds. In *Dungeons
    & Dragons*, procedural generation can be used to create random dungeons, quests,
    and encounters, making the game more appealing by adding an element of unpredictability
    and endless possibilities.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch08.html#id1539-marker)) 许多令人惊叹的游戏之所以成为可能，仅仅是因为程序生成。像*Minecraft*和*No
    Man’s Sky*这样的游戏使用噪声函数和分形算法来创建广阔、沉浸式的世界。在*Dungeons & Dragons*中，程序生成可以用来创建随机的地下城、任务和遭遇，通过增加不可预测性和无限可能性的元素，使游戏更具吸引力。
- en: ^([11](ch08.html#id1549-marker)) The implication of this is that, in theory,
    it’s possible to train a model that can continually improve upon itself. However,
    whether this is possible in practice is another story.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch08.html#id1549-marker)) 这意味着，从理论上讲，训练一个能够不断自我改进的模型是可能的。然而，在实践中是否可行则是另一回事。
- en: ^([12](ch08.html#id1551-marker)) They “observed that about 20% of solutions
    were initially incorrect but self-corrected, indicating that the model learned
    from the execution feedback and improved its performance.”
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch08.html#id1551-marker)) 他们“观察到大约20%的解决方案最初是错误的，但自我纠正了，这表明模型从执行反馈中学习并提高了其性能。”
- en: ^([13](ch08.html#id1561-marker)) The same issue can happen with human annotations.
    If the human labeler uses the knowledge they have but the model doesn’t to answer
    a question, they are effectively teaching the model to hallucinate.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch08.html#id1561-marker)) 同样的问题也可能出现在人工标注中。如果人工标注者使用了他们所拥有的知识，但模型没有用来回答问题，那么他们实际上是在教模型产生幻觉。
- en: ^([14](ch08.html#id1564-marker)) The concept was also later explained by the
    same authors in [“AI Models Collapse When Trained on Recursively Generated Data”](https://oreil.ly/hJhTF)
    (*Nature*, July 2024).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch08.html#id1564-marker)) 这个概念后来也被同一作者在[“AI Models Collapse When Trained
    on Recursively Generated Data”](https://oreil.ly/hJhTF)（*自然*, 2024年7月）一文中解释过。
- en: ^([15](ch08.html#id1573-marker)) Comparing the parameter count of a mixture-of-experts
    model like Mixtral to that of a dense model like Nemotron-4 isn’t fair, but the
    point that the teacher model (Mixtral) is smaller than the student model (Nemotron-4)
    still holds.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch08.html#id1573-marker)) 将专家混合模型（如Mixtral）与密集模型（如Nemotron-4）的参数数量进行比较并不公平，但教师模型（Mixtral）比学生模型（Nemotron-4）更小的观点仍然成立。
- en: ^([16](ch08.html#id1584-marker)) One of my open source libraries, [lazyNLP](https://github.com/chiphuyen/lazynlp),
    also supports overlap estimation and deduplication using Bloom filter.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch08.html#id1584-marker)) 我的一个开源库[lazyNLP](https://github.com/chiphuyen/lazynlp)也支持使用Bloom过滤器进行重叠估计和去重。
