- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: In 2006, Geoffrey Hinton et al. published [a paper](https://homl.info/hinton2006)⁠⁠^([1](preface01.html#id742))
    showing how to train a deep neural network capable of recognizing handwritten
    digits with state-of-the-art precision (>98%). They branded this technique “deep
    learning”. A deep neural network is a (very) simplified model of our cerebral
    cortex, composed of a stack of layers of artificial neurons. Training a deep neural
    net was widely considered impossible at the time,⁠^([2](preface01.html#id744))
    and most researchers had abandoned the idea in the late 1990s. This paper revived
    the interest of the scientific community, and before long many new papers demonstrated
    that deep learning was not only possible, but capable of mind-blowing achievements
    that no other machine learning (ML) technique could hope to match (with the help
    of tremendous computing power and great amounts of data). This enthusiasm soon
    extended to many other areas of machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在2006年，Geoffrey Hinton等人发表了一篇[论文](https://homl.info/hinton2006)⁠⁠^([1](preface01.html#id742))，展示了如何训练一个能够以最先进的精度（>98%）识别手写数字的深度神经网络。他们将这项技术命名为“深度学习”。深度神经网络是我们大脑皮层（一个非常简化的模型）的堆叠层，由人工神经元层组成。当时，训练深度神经网络被认为是无法实现的，⁠^([2](preface01.html#id744))，大多数研究人员在20世纪90年代末放弃了这个想法。这篇论文重新激发了科学界的兴趣，不久之后，许多新的论文证明了深度学习不仅可行，而且能够实现令人惊叹的成就，这是其他机器学习（ML）技术所无法比拟的（借助巨大的计算能力和大量的数据）。这种热情很快扩展到了机器学习的许多其他领域。
- en: 'A decade later, machine learning had already conquered many industries, ranking
    web results, recommending videos to watch and products to buy, sorting items on
    production lines, sometimes even driving cars. Machine learning often made the
    headlines, for example when DeepMind’s AlphaFold machine learning system solved
    a long-standing protein-folding problem that had stomped researchers for decades.
    But most of the time, machine learning was just working discretely in the background.
    However, another decade later came the rise of AI assistants: from ChatGPT in
    2022, Gemini, Claude, and Grok in 2023, and many others since then. AI has now
    truly taken off and it is rapidly transforming every single industry: what used
    to be sci-fi is now very real.⁠^([3](preface01.html#id745))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 十年后，机器学习已经征服了许多行业，包括排名网页结果、推荐观看的视频和购买的产品、在生产线上排序物品，有时甚至驾驶汽车。机器学习经常成为头条新闻，例如DeepMind的AlphaFold机器学习系统解决了困扰研究人员数十年的蛋白质折叠问题。但大多数时候，机器学习只是在后台默默地工作。然而，十年后，AI助手开始兴起：从2022年的ChatGPT，到2023年的Gemini、Claude和Grok，以及此后出现的许多其他AI助手。AI现在真正起飞了，它正在迅速改变每一个行业：曾经是科幻的东西现在变得非常真实。⁠^([3](preface01.html#id745))
- en: Machine Learning in Your Projects
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的项目中应用机器学习
- en: So, naturally you are excited about machine learning and would love to join
    the party! Perhaps you would like to give your homemade robot a brain of its own?
    Make it recognize faces? Or learn to walk around?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你自然会对手动学习感到兴奋，并希望加入这个行列！也许你想要给你的自制机器人一个自己的大脑？让它能识别人脸？或者学会四处走动？
- en: 'Or maybe your company has tons of data (user logs, financial data, production
    data, machine sensor data, hotline stats, HR reports, etc.), and more than likely
    you could unearth some hidden gems if you just knew where to look. With machine
    learning, you could accomplish the following [and much more](https://homl.info/usecases):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也许你的公司拥有大量的数据（用户日志、财务数据、生产数据、机器传感器数据、热线统计数据、人力资源报告等），如果你知道该往哪里看，很可能你会挖掘到一些隐藏的宝藏。利用机器学习，你可以完成以下任务[以及更多](https://homl.info/usecases)：
- en: Segment customers and find the best marketing strategy for each group.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将客户细分并找到适合每个群体的最佳营销策略。
- en: Recommend products for each client based on what similar clients bought.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据类似客户购买的产品向每位客户推荐产品。
- en: Detect which transactions are likely to be fraudulent.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别哪些交易可能存在欺诈。
- en: Forecast next year’s revenue.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测明年的收入。
- en: Predict peak workloads and suggest optimal staffing levels.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测峰值工作负载并建议最佳人员配置水平。
- en: Build a chatbot to assist your customers.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立聊天机器人以协助你的客户。
- en: Whatever the reason, you have decided to learn machine learning and implement
    it in your projects. Great idea!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无论出于什么原因，你已经决定学习机器学习并将其应用于你的项目中。这是一个伟大的想法！
- en: Objective and Approach
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标和方法
- en: This book assumes that you know close to nothing about machine learning. Its
    goal is to give you the concepts, tools, and intuition you need to implement programs
    capable of *learning from data*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设你对机器学习几乎一无所知。其目标是为你提供实现能够从数据中学习的程序所需的概念、工具和直觉。
- en: 'We will cover a large number of techniques, from the simplest and most commonly
    used (such as linear regression) to some of the deep learning techniques that
    regularly win competitions. For this, we will be using Python—the leading language
    for data science and machine learning—as well as open source and production-ready
    Python frameworks:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖大量技术，从最简单和最常用的（如线性回归）到一些经常赢得比赛的深度学习技术。为此，我们将使用Python——数据科学和机器学习的领先语言，以及开源和现成的Python框架：
- en: '[Scikit-Learn](https://scikit-learn.org) is very easy to use, yet it implements
    many machine learning algorithms efficiently, so it makes for a great entry point
    to learning machine learning. It was created by David Cournapeau in 2007, then
    led by a team of researchers at the French Institute for Research in Computer
    Science and Automation (Inria), and recently Probabl.ai.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-Learn](https://scikit-learn.org) 非常易于使用，但它的效率高，实现了许多机器学习算法，因此是学习机器学习的绝佳起点。它由David
    Cournapeau于2007年创建，后来由法国计算机科学和自动化研究所（Inria）的研究团队领导，最近由Probabl.ai领导。'
- en: '[PyTorch](https://pytorch.org) is a powerful and flexible library for deep
    learning. It makes it possible to train and run all sorts of neural networks efficiently,
    and it can distribute the computations across multiple GPUs (graphics processing
    units). PyTorch (PT) was developed by Facebook’s AI Research lab (FAIR) and first
    released in 2016\. It evolved from Torch, an older framework coded in Lua. In
    2022, PyTorch was transitioned to the PyTorch Foundation, under the Linux Foundation,
    to promote community-driven development.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch](https://pytorch.org) 是一个功能强大且灵活的深度学习库。它使得训练和运行各种神经网络变得高效，并且可以将计算分布到多个GPU（图形处理单元）上。PyTorch（PT）由Facebook的人工智能研究实验室（FAIR）开发，并于2016年首次发布。它源自一个名为Torch的较老框架，该框架是用Lua编写的。到2022年，PyTorch已过渡到Linux基金会下的PyTorch基金会，以促进社区驱动的开发。'
- en: 'We will also use these open source machine learning libraries along the way:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用以下开源机器学习库：
- en: '[XGBoost](https://xgboost.readthedocs.io) in [Chapter 6](ch06.html#ensembles_chapter)
    to implement a powerful technique called *gradient boosting*.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost](https://xgboost.readthedocs.io) 在[第6章](ch06.html#ensembles_chapter)中实现一种称为*梯度提升*的强大技术。'
- en: '[Hugging Face](https://huggingface.co) libraries in Chapters [13](ch13.html#rnn_chapter)
    and [15](ch15.html#transformer_chapter) to download datasets and pretrained models,
    including transformer models. Transformers are incredibly powerful and versatile,
    and they are the main building block of virtually all AI assistants today.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face](https://huggingface.co) 库在第[13章](ch13.html#rnn_chapter)和[15章](ch15.html#transformer_chapter)中用于下载数据集和预训练模型，包括转换器模型。转换器非常强大且多功能，它们是今天几乎所有AI助手的主体构建块。'
- en: '[Gymnasium](https://gymnasium.farama.org) in [Chapter 19](ch19.html#rl_chapter)
    for reinforcement learning (i.e., training autonomous agents).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gymnasium](https://gymnasium.farama.org) 在[第19章](ch19.html#rl_chapter)中用于强化学习（即训练自主代理）。'
- en: The book favors a hands-on approach, growing an intuitive understanding of machine
    learning through concrete working examples and just a little bit of theory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书倾向于实践方法，通过具体的实际例子和一点点理论来培养对机器学习的直观理解。
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: While you can read this book without picking up your laptop, I highly recommend
    you experiment with the code examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以不拿起笔记本电脑阅读这本书，但我强烈建议你尝试代码示例。
- en: Code Examples
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码示例
- en: 'All the code examples in this book are open source and available online at
    [*https://github.com/ageron/handson-mlp*](https://github.com/ageron/handson-mlp),
    as Jupyter notebooks. These are interactive documents containing text, images,
    and executable code snippets (Python in our case). The easiest and quickest way
    to get started is to run these notebooks using Google Colab: this is a free service
    that allows you to run any Jupyter notebook directly online without having to
    install anything on your machine. All you need is a web browser and a Google account.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有代码示例都是开源的，可在[*https://github.com/ageron/handson-mlp*](https://github.com/ageron/handson-mlp)上在线获取，作为Jupyter笔记本。这些是包含文本、图像和可执行代码片段（在我们的情况下是Python）的交互式文档。开始的最简单和最快的方式是使用Google
    Colab运行这些笔记本：这是一个免费服务，允许你直接在线运行任何Jupyter笔记本，而无需在您的机器上安装任何东西。你只需要一个网络浏览器和一个Google账户。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this book, I will assume that you are using Google Colab, but I have also
    tested the notebooks on other online platforms such as Kaggle and Binder, so you
    can use those if you prefer. Alternatively, you can install the required libraries
    and tools (or the Docker image for this book) and run the notebooks directly on
    your own machine. See the instructions at [*https://homl.info/install-p*](https://homl.info/install-p).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我将假设你正在使用Google Colab，但我已经测试了其他在线平台上的笔记本，例如Kaggle和Binder，所以如果你更喜欢这些，你也可以使用它们。或者，你可以安装所需的库和工具（或本书的Docker镜像）并在自己的机器上直接运行笔记本。请参阅[*https://homl.info/install-p*](https://homl.info/install-p)中的说明。
- en: This book is here to help you get your job done. If you wish to use additional
    content beyond the code examples, and that use falls outside the scope of fair
    use guidelines, (such as selling or distributing content from O’Reilly books,
    or incorporating a significant amount of material from this book into your product’s
    documentation), please reach out to O’Reilly for permission, at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助你完成工作。如果你希望使用超出代码示例的额外内容，并且这种使用超出了合理使用指南的范围（例如，销售或分发O’Reilly书籍的内容，或者将本书的大量内容纳入你的产品文档中），请联系O’Reilly获取许可，邮箱为[*permissions@oreilly.com*](mailto:permissions@oreilly.com)。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Hands-On Machine
    Learning with Scikit-Learn and PyTorch* by Aurélien Geron. Copyright 2026 Aurélien
    Geron, 979-8-341-60798-9.”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但通常不需要署名。署名通常包括标题、作者、出版社和ISBN。例如：“*《使用Scikit-Learn和PyTorch动手学习机器学习》* 作者：Aurélien
    Geron。版权所有2026年Aurélien Geron，979-8-341-60798-9。”
- en: Prerequisites
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前置条件
- en: This book assumes that you have some Python programming experience. If you don’t
    know Python yet, [*https://learnpython.org*](https://learnpython.org) is a great
    place to start. The official tutorial on [Python.org](https://docs.python.org/3/tutorial)
    is also quite good.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设你有一些Python编程经验。如果你还不了解Python，[*https://learnpython.org*](https://learnpython.org)是一个很好的起点。Python.org上的官方教程[Python.org](https://docs.python.org/3/tutorial)也非常不错。
- en: This book also assumes that you are familiar with Python’s main scientific libraries—in
    particular, [NumPy](https://numpy.org), [pandas](https://pandas.pydata.org), and
    [Matplotlib](https://matplotlib.org). If you have never used these libraries,
    don’t worry; they’re easy to learn, and I’ve created a tutorial for each of them.
    You can access them online at [*https://homl.info/tutorials-p*](https://homl.info/tutorials-p).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还假设你熟悉Python的主要科学库——特别是[NumPy](https://numpy.org)、[pandas](https://pandas.pydata.org)和[Matplotlib](https://matplotlib.org)。如果你从未使用过这些库，不要担心；它们很容易学习，我为每个库都创建了一个教程。你可以在[*https://homl.info/tutorials-p*](https://homl.info/tutorials-p)在线访问它们。
- en: Moreover, if you want to fully understand how the machine learning algorithms
    work (not just how to use them), then you should have at least a basic understanding
    of a few math concepts, especially linear algebra. Specifically, you should know
    what vectors and matrices are, and how to perform some simple operations like
    adding vectors, or transposing and multiplying matrices. If you need a quick introduction
    to linear algebra (it’s really not rocket science!), I provide a tutorial at [*https://homl.info/tutorials-p*](https://homl.info/tutorials-p).
    You will also find a tutorial on differential calculus, which may be helpful to
    understand how neural networks are trained, but it’s not entirely essential to
    grasp the important concepts. This book also uses other mathematical concepts
    occasionally, such as exponentials and logarithms, a bit of probability theory,
    and some basic concepts from statistics, but nothing too advanced. If you need
    help on any of these, please check out [*https://khanacademy.org*](https://khanacademy.org),
    which offers many excellent and free math courses online.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想全面理解机器学习算法是如何工作的（而不仅仅是如何使用它们），那么你应该至少对几个数学概念有基本的了解，特别是线性代数。具体来说，你应该知道什么是向量矩阵，以及如何执行一些简单的操作，比如向量相加，或者矩阵转置和乘法。如果你需要快速了解线性代数（这真的不是火箭科学！），我在[*https://homl.info/tutorials-p*](https://homl.info/tutorials-p)提供了一个教程。你还会找到一个关于微分学的教程，这可能有助于理解神经网络是如何训练的，但这不是掌握重要概念所必需的。本书偶尔也会使用其他数学概念，例如指数和对数，一点概率理论，以及一些基本的统计学概念，但都不是太高级。如果你需要这方面的帮助，请查看[*https://khanacademy.org*](https://khanacademy.org)，它提供了许多优秀且免费的在线数学课程。
- en: Roadmap
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路线图
- en: 'This book is organized in two parts. [Part I, “The Fundamentals of Machine
    Learning”](part01.html#fundamentals_part), covers the following topics:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为两部分。[第一部分，“机器学习基础”](part01.html#fundamentals_part)，涵盖了以下主题：
- en: What machine learning is, what problems it tries to solve, and the main categories
    and fundamental concepts of its systems
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是什么，它试图解决什么问题，以及其系统的主要类别和基本概念
- en: The steps in a typical machine learning project
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型机器学习项目的步骤
- en: Learning by fitting a model to data
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过拟合模型到数据中学习
- en: Minimizing a cost function (i.e., a measure of prediction errors)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化损失函数（即预测误差的度量）
- en: Handling, cleaning, and preparing data
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理、清洗和准备数据
- en: Selecting and engineering features (i.e., data fields)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择和工程化特征（即数据字段）
- en: Selecting a model and tuning hyperparameters using cross-validation (e.g., training
    many model variants and choosing the one that performs best on data it didn’t
    see during training)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证选择模型和调整超参数（例如，训练许多模型变体，并选择在训练数据中表现最佳的模型）
- en: The challenges of machine learning, in particular underfitting and overfitting
    (the bias/variance trade-off)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的挑战，特别是欠拟合和过拟合（偏差/方差权衡）
- en: 'The most common learning algorithms: linear and polynomial regression, logistic
    regression, *k*-nearest neighbors, decision trees, random forests, and ensemble
    methods'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的学习算法：线性回归和多项式回归、逻辑回归、*k*最近邻、决策树、随机森林和集成方法
- en: Reducing the dimensionality of the training data to fight the “curse of dimensionality”
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练数据的维度降低以对抗“维度诅咒”
- en: Other unsupervised learning techniques, including clustering, density estimation,
    and anomaly detection
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他无监督学习技术，包括聚类、密度估计和异常检测
- en: '[Part II, “Neural Networks and Deep Learning”](part02.html#neural_nets_part),
    covers the following topics:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二部分，“神经网络与深度学习”](part02.html#neural_nets_part)，涵盖了以下主题：'
- en: What neural nets are and what they’re good for
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是什么，它们有什么优点
- en: Building and training deep neural nets using PyTorch
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch构建和训练深度神经网络
- en: 'The most important neural net architectures: feedforward neural nets for tabular
    data; convolutional nets for computer vision; recurrent nets and long short-term
    memory (LSTM) nets for sequence processing; encoder-decoders, transformers, state
    space models (SSMs), and hybrid architectures for natural language processing,
    vision, and more; autoencoders, generative adversarial networks (GANs), and diffusion
    models for generative learning'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的神经网络架构：用于表格数据的前馈神经网络；用于计算机视觉的卷积神经网络；用于序列处理的循环神经网络和长短期记忆（LSTM）网络；用于自然语言处理、视觉等领域的编码器-解码器、转换器、状态空间模型（SSM）和混合架构；用于生成学习的自编码器、生成对抗网络（GAN）和扩散模型
- en: How to build an agent (e.g., a bot in a game) that can learn good strategies
    through trial and error, using reinforcement learning
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个通过试错学习良好策略的智能体（例如，游戏中的人工智能机器人），使用强化学习
- en: Loading and preprocessing large amounts of data efficiently
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效地加载和预处理大量数据
- en: The first part is based mostly on Scikit-Learn; the second part uses mostly
    PyTorch.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分主要基于Scikit-Learn；第二部分主要使用PyTorch。
- en: Caution
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 谨慎
- en: 'Don’t jump into deep waters too hastily: deep learning is no doubt one of the
    most exciting areas in machine learning, but you should master the fundamentals
    first. Moreover, many problems can be solved quite well using simpler techniques
    such as random forests and ensemble methods (discussed in [Part I](part01.html#fundamentals_part)).
    Deep learning is best suited for complex problems such as image recognition, speech
    recognition, or natural language processing, and it often requires a lot of data,
    computing power, and patience (unless you can leverage a pretrained neural network,
    as you will see).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不要急于跳入深水区：深度学习无疑是机器学习中最激动人心的领域之一，但你应该首先掌握基础知识。此外，许多问题可以使用更简单的技术（如随机森林和集成方法）得到很好的解决（在[第一部分](part01.html#fundamentals_part)中讨论）。深度学习最适合解决复杂问题，如图像识别、语音识别或自然语言处理，并且通常需要大量数据、计算能力和耐心（除非你能利用预训练的神经网络，正如你将看到的）。
- en: If you are particularly interested in one topic and want to reach it as quickly
    as possible, [Figure P-1](#chapter_dependencies_diagram) will show you which chapters
    you must read first, and which ones you can safely skip.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你特别感兴趣于某个主题，并希望尽快达到，[图P-1](#chapter_dependencies_diagram)将显示你必须首先阅读的章节，以及你可以安全跳过的章节。
- en: '![Diagram illustrating the dependencies between chapters in a machine learning
    book, showing which chapters must be read first before others.](assets/hmls_0001.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![展示机器学习书籍中章节之间依赖关系的图，显示哪些章节必须在阅读其他章节之前先阅读。](assets/hmls_0001.png)'
- en: Figure P-1\. Chapter dependencies
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图P-1. 章节依赖关系
- en: Changes Between the TensorFlow and PyTorch Versions
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow和PyTorch版本之间的变化
- en: 'I wrote three TensorFlow (TF) editions of this book, published by O’Reilly
    in 2017, 2019, and 2022\. TF was the leading deep learning library for many years,
    used internally by Google and therefore optimized for production at scale. But
    PyTorch has gradually taken the lead, owing to its simplicity, flexibility and
    openness: it now dominates research papers and open source projects, which means
    that most new models are available in PyTorch first. As a result, the industry
    has also gradually shifted toward PyTorch.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这本书写了三个TensorFlow（TF）版本，分别由O’Reilly在2017年、2019年和2022年出版。TF多年来一直是领先的深度学习库，由谷歌内部使用，因此针对大规模生产进行了优化。但PyTorch逐渐取得了领先地位，这得益于其简单性、灵活性和开放性：它现在主导着研究论文和开源项目，这意味着大多数新模型首先在PyTorch中可用。因此，行业也逐步转向PyTorch。
- en: In recent years, Google has reduced its investments in TensorFlow, and focused
    more on JAX, another excellent deep learning library with a great mix of qualities
    for both research and production. However, its adoption is still low compared
    to PyTorch.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，谷歌减少了在TensorFlow上的投资，并更多地关注JAX，这是一个优秀的深度学习库，它结合了研究生产和质量。然而，与PyTorch相比，其采用率仍然较低。
- en: This is why I chose to use PyTorch this time around! O’Reilly and I decided
    to make it the first edition of a new PyTorch series rather than the fourth edition
    of the original series. This leaves the door open for a JAX series or perhaps
    a new edition for the TF series (time will tell if either are needed).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么这次我选择使用PyTorch的原因！O’Reilly和我决定将其作为新PyTorch系列的第一个版本，而不是原始系列的第四个版本。这为JAX系列或TF系列的全新版本留下了空间（时间将证明是否需要这些）。
- en: 'If you have already read the latest TensorFlow version of this book, here are
    the main changes you will find in this book (see [*https://homl.info/changes-p*](https://homl.info/changes-p)
    for more details):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你已经阅读了这本书的最新TensorFlow版本，以下是你在本书中会发现的主要变化（更多详细信息请参阅[*https://homl.info/changes-p*](https://homl.info/changes-p)）： '
- en: All the code in the book was updated to recent library versions.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书中的所有代码都已更新到最近的库版本。
- en: All the code in [Part II](part02.html#neural_nets_part) was migrated from TensorFlow
    and Keras to PyTorch. There were significant changes in all of these chapters.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第二部分](part02.html#neural_nets_part)中的所有代码都已从TensorFlow和Keras迁移到PyTorch。这些章节都发生了重大变化。'
- en: TensorFlow-specific content was removed, including former Chapters 12 and 13,
    and former Appendices C and D.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除了TensorFlow特定的内容，包括以前的第12章和第13章，以及以前的附录C和D。
- en: '[Chapter 10](ch10.html#pytorch_chapter) now introduces PyTorch.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第10章](ch10.html#pytorch_chapter)现在介绍了PyTorch。'
- en: 'I also added three new chapters on transformers:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我还增加了三个关于转换器的新章节：
- en: '[Chapter 15](ch15.html#transformer_chapter) covers transformers for natural
    language processing, including how to build a chatbot.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第15章](ch15.html#transformer_chapter)涵盖了自然语言处理中的转换器，包括如何构建聊天机器人。'
- en: '[Chapter 16](ch16.html#vit_chapter) presents vision transformers and multimodal
    transformers.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第16章](ch16.html#vit_chapter)介绍了视觉转换器和多模态转换器。'
- en: '[Chapter 17](ch17.html#speedup_chapter), available online at [*https://homl.info/*](https://homl.info/),
    discusses several advanced techniques to speed up and scale up transformers. This
    includes FlashAttention, mixture of experts (MoE), low-rank adaptation (LoRA),
    and many more.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第17章](ch17.html#speedup_chapter)，可在[*https://homl.info/*](https://homl.info/)在线查看，讨论了几个用于加速和扩展转换器的先进技术。这包括FlashAttention、专家混合（MoE）、低秩适应（LoRA）等。'
- en: 'There are also three new appendices: [Appendix B](app02.html#precision_appendix)
    explains how to shrink models so they can run faster and fit on smaller devices,
    “Relative Positional Encoding” discusses advanced positional encoding techniques
    for transformers, and “State-Space Models (SSMs)” presents state-space models.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，还增加了三个新的附录：[附录B](app02.html#precision_appendix)解释了如何缩小模型以便它们可以运行得更快并适应更小的设备，“相对位置编码”讨论了转换器的先进位置编码技术，“状态空间模型（SSMs）”介绍了状态空间模型。
- en: To make room for the newer content, the chapter on support vector machines (SVMs)
    was [moved online](https://homl.info) and renamed “Support Vector Machines”; the
    last two appendices are also online at the same URL, and the deployment chapter
    was partially merged into [Chapter 10](ch10.html#pytorch_chapter).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了为新内容腾出空间，关于支持向量机（SVMs）的章节已被[移至线上](https://homl.info)并更名为“支持向量机”；最后两个附录也位于同一网址，部署章节部分合并到了[第10章](ch10.html#pytorch_chapter)。
- en: The three editions of the TensorFlow/Keras version of this book are nicknamed
    homl1, homl2, and homl3\. This book, which is the first edition of the PyTorch
    version, is nicknamed homlp. Try saying that three times in a row.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的TensorFlow/Keras版本有三个版本，分别被称为homl1、homl2和homl3。这是PyTorch版本的第一个版本，被称为homlp。试着连续说三次。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Most of the changes compared to the latest TensorFlow edition are in the second
    part of the book. If you have read homl3, then don’t expect big changes in the
    first part of the book: the fundamental concepts of machine learning haven’t changed
    since 2022.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与最新版本的TensorFlow相比，大部分变化都在书的第二部分。如果你已经阅读了homl3，那么不要期待书中第一部分有大的变化：自2022年以来，机器学习的基本概念没有变化。
- en: Other Resources
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他资源
- en: Many excellent resources are available to learn about machine learning. For
    example, Andrew Ng’s [ML course on Coursera](https://homl.info/ngcourse) is amazing,
    although it requires a significant time investment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优秀的资源可以学习机器学习。例如，安德鲁·吴在Coursera上的[ML课程](https://homl.info/ngcourse)非常出色，尽管它需要大量的时间投入。
- en: There are also many interesting websites about machine learning, including Scikit-Learn’s
    exceptional [User Guide](https://homl.info/skdoc). You may also enjoy [Dataquest](https://dataquest.io),
    which provides very nice interactive tutorials, and countless ML blogs and YouTube
    channels.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多关于机器学习的有趣网站，包括Scikit-Learn的杰出[用户指南](https://homl.info/skdoc)。你也许也会喜欢[Dataquest](https://dataquest.io)，它提供了非常棒的交互式教程，以及无数的机器学习博客和YouTube频道。
- en: 'There are many other introductory books about machine learning. In particular:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多关于机器学习的入门书籍。特别是：
- en: Joel Grus’s [*Data Science from Scratch*](https://homl.info/grusbook), 2nd edition
    (O’Reilly), presents the fundamentals of machine learning and implements some
    of the main algorithms in pure Python (from scratch, as the name suggests).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乔尔·格鲁斯的[*《从零开始的数据科学》*](https://homl.info/grusbook)，第2版（O'Reilly），介绍了机器学习的基础知识，并使用纯Python（从头开始，正如书名所示）实现了一些主要算法。
- en: 'Stephen Marsland’s *Machine Learning: An Algorithmic Perspective*, 2nd edition
    (Chapman & Hall), is a great introduction to machine learning, covering a wide
    range of topics in depth with code examples in Python (also from scratch, but
    using NumPy).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 史蒂芬·马尔斯兰德的*《机器学习：算法视角》*，第2版（Chapman & Hall），是机器学习的优秀入门书籍，深入探讨了广泛的主题，并使用Python（也是从头开始，但使用NumPy）提供了代码示例。
- en: Sebastian Raschka’s *Machine Learning with PyTorch and Scikit-Learn*, 1st edition
    (Packt Publishing), is also a great introduction to machine learning using Scikit-Learn
    and PyTorch.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 塞巴斯蒂安·拉斯奇卡的*《使用PyTorch和Scikit-Learn进行机器学习》*，第1版（Packt Publishing），也是使用Scikit-Learn和PyTorch进行机器学习的优秀入门书籍。
- en: François Chollet’s *Deep Learning with Python*, 3rd edition (Manning), is a
    very practical book that covers a large range of topics in a clear and concise
    way, as you might expect from the author of the excellent Keras library. It favors
    code examples over mathematical theory.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弗朗索瓦·肖莱特的*《用Python进行深度学习》*，第3版（Manning），是一本非常实用的书籍，以清晰简洁的方式涵盖了广泛的主题，正如你从优秀的Keras库的作者那里所期望的那样。它更倾向于代码示例而不是数学理论。
- en: Andriy Burkov’s [*The Hundred-Page Machine Learning Book*](https://themlbook.com)
    (self-published) is very short but covers an impressive range of topics, introducing
    them in approachable terms without shying away from the math equations.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安德烈·布尔科夫的[*《百页机器学习书》*](https://themlbook.com)（自出版）非常简短，但涵盖了令人印象深刻的主题范围，以易于理解的方式介绍了这些主题，并没有回避数学方程式。
- en: Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s *Learning from
    Data* (AMLBook) is a more theoretical approach to ML that provides deep insights,
    in particular on the bias/variance trade-off (see [Chapter 4](ch04.html#linear_models_chapter)).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚瑟·S·阿布-穆斯塔法、马利克·马戈德-伊斯拉伊尔和许安-天林的*《从数据中学习》*（AMLBook）是ML的一种更理论的方法，提供了深刻的见解，特别是在偏差/方差权衡方面（见[第4章](ch04.html#linear_models_chapter)）。
- en: 'Stuart Russell and Peter Norvig’s *Artificial Intelligence: A Modern Approach*,
    4th edition (Pearson), is a great (and huge) book covering an incredible amount
    of topics, including machine learning. It helps put ML into perspective.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stuart Russell 和 Peter Norvig 的 *《人工智能：现代方法》* 第四版 (Pearson) 是一本涵盖大量主题（包括机器学习）的伟大（且庞大）的书籍。它有助于将机器学习置于正确的视角。
- en: Jeremy Howard and Sylvain Gugger’s [*Deep Learning for Coders with fastai and
    PyTorch*](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)
    (O’Reilly) provides a wonderfully clear and practical introduction to deep learning
    using the fastai and PyTorch libraries.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremy Howard 和 Sylvain Gugger 的 [*《使用 fastai 和 PyTorch 的编码者深度学习》*](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)
    (O’Reilly) 提供了一个清晰且实用的深度学习入门介绍，使用了 fastai 和 PyTorch 库。
- en: Andrew Ng’s *Machine Learning Yearning* is a free ebook that provides a thoughtful
    exploration of machine learning, focusing on the practical considerations of building
    and deploying models, including data quality and long-term maintenance.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng 的 *《机器学习求索》* 是一本免费的电子书，深入思考了机器学习，重点关注构建和部署模型的实际考虑因素，包括数据质量和长期维护。
- en: 'Lewis Tunstall, Leandro von Werra, and Thomas Wolf’s [*Natural Language Processing
    with Transformers: Building Language Applications with Hugging Face*](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
    (O’Reilly) is a great practical dive into transformers using popular libraries
    by Hugging Face.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis Tunstall、Leandro von Werra 和 Thomas Wolf 的 [*《使用 Hugging Face 的 Transformer
    自然语言处理：构建语言应用》*](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
    (O’Reilly) 是一个关于使用 Hugging Face 流行库的 Transformer 的实用深入探讨。
- en: Jay Alammar and Maarten Grootendorst’s *Hands-On Large Language Models* is a
    beautifully illustrated book on LLMs, covering everything you need to know to
    understand, train, fine-tune, and use LLMs across a wide variety of tasks.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jay Alammar 和 Maarten Grootendorst 的 *《动手学习大型语言模型》* 是一本关于 LLM 的精美插图书籍，涵盖了理解、训练、微调和使用
    LLM 在各种任务中的所有必要知识。
- en: Finally, joining ML competition websites such as [Kaggle.com](https://kaggle.com)
    will allow you to practice your skills on real-world problems, with help and insights
    from some of the best ML professionals out there.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，加入像 [Kaggle.com](https://kaggle.com) 这样的机器学习竞赛网站，将允许你在真实世界的问题上练习你的技能，并获得一些最好的机器学习专业人士的帮助和见解。
- en: Conventions Used in This Book
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用的排版约定：
- en: '*Italic*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`常宽`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**`常宽粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应逐字输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*`常宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应替换为用户提供的值或由上下文确定的值的文本。
- en: Punctuation
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 标点符号
- en: To avoid any confusion, punctuation appears outside of quotes throughout the
    book. My apologies to the purists.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免任何混淆，本书中所有引号外的标点符号都位于引号之外。对那些讲究的人士表示歉意。
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: This element signifies a general note.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般注释。
- en: Warning
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意事项。
- en: O’Reilly Online Learning
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly 在线学习
- en: Note
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 超过 40 年来，[*O’Reilly 媒体*](https://oreilly.com) 为公司提供技术培训和业务培训、知识和见解，以帮助公司成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专业知识。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和200多家其他出版商的大量文本和视频。更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 141 Stony Circle, Suite 195
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 141 Stony Circle, Suite 195
- en: Santa Rosa, CA 95401
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santa Rosa, CA 95401
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969（在美国或加拿大）
- en: 707-827-7019 (international or local)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/hands-on-machine-learning*](https://oreil.ly/hands-on-machine-learning).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，上面列出了勘误、示例和任何其他附加信息。您可以通过[*https://oreil.ly/hands-on-machine-learning*](https://oreil.ly/hands-on-machine-learning)访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Never in my wildest dreams did I imagine that the three TensorFlow editions
    of this book would reach such a large audience. I received so many messages from
    readers, many asking questions, some kindly pointing out errata, and most sending
    me encouraging words. I cannot express how grateful I am to all these readers
    for their tremendous support. Thank you all so very much! Please do not hesitate
    to [file issues on GitHub](https://homl.info/issues-p) if you find errors in the
    code examples (or just to ask questions), or to submit [errata](https://homl.info/errata-p)
    if you find errors in the text. Some readers also shared how this book helped
    them get their first job, or how it helped them solve a concrete problem they
    were working on. I find such feedback incredibly motivating. If you find this
    book helpful, I would love it if you could share your story with me, either privately
    (e.g., via [LinkedIn](https://linkedin.com/in/aurelien-geron)) or publicly (e.g.,
    tweet me at @aureliengeron or write an [Amazon review](https://homl.info/amazon-p)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我梦寐以求的时刻，我也没想到这本书的三个TensorFlow版本能吸引如此庞大的读者群。我收到了许多读者的消息，许多人提问，有些人友好地指出错误，大多数人给我送来了鼓励的话语。我无法表达我对所有这些读者巨大支持的感激之情。非常感谢大家！如果您在代码示例中找到错误（或只是想提问），请毫不犹豫地[在GitHub上提交问题](https://homl.info/issues-p)，或者如果您在文本中找到错误，请提交[勘误](https://homl.info/errata-p)。一些读者还分享了这本书如何帮助他们找到第一份工作，或者如何帮助他们解决他们正在工作的具体问题。我发现这样的反馈非常鼓舞人心。如果您觉得这本书有用，我非常希望您能与我分享您的经历，无论是私下（例如，通过[LinkedIn](https://linkedin.com/in/aurelien-geron)）还是公开（例如，在@aureliengeron上发推文或写[Amazon评论](https://homl.info/amazon-p))）。
- en: 'Huge thanks as well to all the generous people who offered their time and expertise
    to review this book, correcting errors and making countless suggestions. They
    made this book so much better: Jeremy Howard, Haesun Park, Omar Sanseviero, Lewis
    Tunstall, Leandro Von Werra, and Sam Witteveen reviewed the table of contents
    and helped me refine the scope of the book. Hesam Hassani, Ashu Jha, Meetu Malhotra,
    and Ammar Mohanna reviewed the first part, while Ulf Bissbort, Louis-Francois
    Bouchard, Luba Elliot, Thomas Lacombe, Tarun Narayanan, Marco Tabor, and my dear
    brother Sylvain reviewed the second part. Special thanks to Haesun Park, who reviewed
    every single chapter. You are all amazing!'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还要感谢所有慷慨的人们，他们抽出时间和专业知识来审阅这本书，纠正错误并提出了无数建议。他们使这本书变得如此出色：Jeremy Howard、Haesun
    Park、Omar Sanseviero、Lewis Tunstall、Leandro Von Werra和Sam Witteveen审阅了目录，并帮助我精炼了本书的范围。Hesam
    Hassani、Ashu Jha、Meetu Malhotra和Ammar Mohanna审阅了第一部分，而Ulf Bissbort、Louis-Francois
    Bouchard、Luba Elliot、Thomas Lacombe、Tarun Narayanan、Marco Tabor以及我的亲爱的兄弟Sylvain审阅了第二部分。特别感谢Haesun
    Park，她审阅了每一章。你们都是如此出色！
- en: Of course, this book would not exist without the fantastic staff at O’Reilly.
    I am especially indebted to Michele Cronin, who reviewed every chapter and supported
    me weekly for a whole year. I am also deeply grateful to Nicole Butterfield for
    leading this project and helping refine the book’s scope, and to the production
    team—particularly Beth Kelly and Kristen Brown—who did a remarkable job. I want
    to acknowledge Sonia Saruba for her countless careful copyedits, Kate Dullea for
    making my diagrams much prettier, and Susan Thompson for the beautiful orangutan
    on the cover.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有O’Reilly的出色员工，这本书就不会存在。我特别感激Michele Cronin，她审阅了每一章，并在整个一年中每周都支持我。我也非常感谢Nicole
    Butterfield领导了这个项目，并帮助精炼了本书的范围，以及制作团队——特别是Beth Kelly和Kristen Brown——他们完成了非凡的工作。我想感谢Sonia
    Saruba，她进行了无数细致的校对，Kate Dullea使我的图表变得更加美观，以及Susan Thompson为封面上的美丽猩猩所做的贡献。
- en: 'Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle,
    and to our three wonderful children—Alexandre, Rémi, and Gabrielle—for encouraging
    me to work so hard on this book. Their insatiable curiosity was priceless: explaining
    some of the most difficult concepts in this book to my wife and children helped
    me clarify my own thoughts and directly improved many parts of it. Plus, they
    kept bringing me cookies and coffee. Who could ask for more?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我对我的挚爱妻子，Emmanuelle，以及我们的三个可爱的孩子——Alexandre、Rémi和Gabrielle——表示无限的感激，因为他们鼓励我如此努力地完成这本书。他们永不满足的好奇心是无价的：向我的妻子和孩子解释这本书中的一些最困难的概念，帮助我澄清了自己的想法，并直接改进了其中的许多部分。此外，他们还不断地给我带来饼干和咖啡。还有什么比这更好的呢？
- en: '^([1](preface01.html#id742-marker)) Geoffrey E. Hinton et al., “A Fast Learning
    Algorithm for Deep Belief Nets”, *Neural Computation* 18 (2006): 1527–1554.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](preface01.html#id742-marker)) Geoffrey E. Hinton等人，“用于深度信念网的快速学习算法”，*神经计算*
    18 (2006): 1527–1554。'
- en: ^([2](preface01.html#id744-marker)) Despite the fact that Yann LeCun’s deep
    convolutional neural networks had worked well for image recognition since the
    1990s, although they were not as general-purpose.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](preface01.html#id744-marker)) 尽管Yann LeCun的深度卷积神经网络自1990年代以来在图像识别方面表现良好，尽管它们并不像通用性那么强。
- en: ^([3](preface01.html#id745-marker)) Geoffrey Hinton was awarded the 2018 Turing
    Award (with Yann LeCun and Yoshua Bengio) and the 2024 Nobel Prize in Physics
    (with John Hopfield) for early work on neural networks back in the 1980s. DeepMind’s
    founder and CEO Demis Hassabis and director John Jumper were awarded the 2024
    Nobel Prize in Chemistry for their work on AlphaFold. They shared this Nobel Prize
    with another protein researcher, David Baker.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](preface01.html#id745-marker)) 乔治·H·霍顿（Geoffrey Hinton）因在20世纪80年代早期对神经网络的研究工作而获得了2018年图灵奖（与Yann
    LeCun和Yoshua Bengio共同获得）和2024年诺贝尔物理学奖（与John Hopfield共同获得）。DeepMind的创始人兼首席执行官Demis
    Hassabis和导演John Jumper因他们在AlphaFold上的工作而获得了2024年诺贝尔化学奖。他们与另一位蛋白质研究员David Baker分享了这一诺贝尔奖。
