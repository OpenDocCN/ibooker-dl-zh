- en: Chapter 5\. Adapting LLMs to Your Use Case
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章\. 将 LLM 适配到您的用例
- en: In this chapter, we will continue with our journey through the LLM landscape,
    exploring the various LLMs available for commercial use and providing pointers
    on how to choose the right LLM for your task. We will also examine how to load
    LLMs of various sizes and run inference on them. We will then decipher various
    decoding strategies for text generation. We will also investigate how to interpret
    the outputs and intermediate results from language models, surveying interpretability
    tools like LIT-NLP.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续我们的 LLM 格局之旅，探索可用于商业用途的各种 LLM，并提供如何选择适合您任务的正确 LLM 的指南。我们还将检查如何加载各种大小的
    LLM 并在其上进行推理。然后我们将解码各种用于文本生成的解码策略。我们还将研究如何解释语言模型的输出和中间结果，并调查如 LIT-NLP 等可解释性工具。
- en: Navigating the LLM Landscape
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 LLM 格局
- en: Seemingly a new LLM is being released every few days, many claiming to be state
    of the art. Most of these LLMs are not very different from each other, so you
    need not spend too much time tracking new LLM releases. This book’s [GitHub repository](https://oreil.ly/llm-playbooks)
    attempts to keep track of the major releases, but I don’t promise it will be complete.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来每隔几天就会发布一个新的 LLM，许多都声称是业界领先。这些 LLM 之间并没有太大的区别，因此您不需要花费太多时间跟踪新的 LLM 发布。本书的
    [GitHub 仓库](https://oreil.ly/llm-playbooks) 尝试跟踪主要发布，但我不能保证它会完全完整。
- en: Nevertheless, it is a good idea to have a broad understanding of the different
    types of LLM providers out there, the kinds of LLMs being made available, and
    the copyright and licensing implications. Therefore, let’s now explore the LLM
    landscape through this lens and understand the choices at our disposal.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，了解现有的不同类型 LLM 提供商、可用的 LLM 类型以及版权和许可影响是一个好主意。因此，现在让我们通过这个视角来探索 LLM 的格局，并了解我们可用的选择。
- en: Who Are the LLM providers?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 提供商是谁？
- en: 'LLM providers can be broadly categorized into the following types:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 提供商可以大致分为以下几类：
- en: Companies providing proprietary LLMs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 提供专有 LLM 的公司
- en: These include companies like OpenAI [(GPT)](https://oreil.ly/r-lb1), Google
    [(Gemini)](https://oreil.ly/KF9Kh), Anthropic [(Claude)](https://oreil.ly/T5Wvo),
    [Cohere](https://oreil.ly/PiKxN), [AI21](https://oreil.ly/Y8T3q), etc. that train
    proprietary LLMs and make them available as an API endpoint (LLM-as-a-service).
    Many of these companies have also partnered with cloud providers that facilitate
    access to these models as a fully managed service. The relevant offerings from
    the major cloud providers are [Amazon Bedrock](https://oreil.ly/FVqRj) and [SageMaker
    JumpStart by Amazon](https://oreil.ly/e0a59), [Vertex AI by Google](https://oreil.ly/mURoC),
    and [Azure OpenAI by Microsoft](https://oreil.ly/Ag1r5).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括像 OpenAI [(GPT)](https://oreil.ly/r-lb1)、Google [(Gemini)](https://oreil.ly/KF9Kh)、Anthropic
    [(Claude)](https://oreil.ly/T5Wvo)、[Cohere](https://oreil.ly/PiKxN)、[AI21](https://oreil.ly/Y8T3q)
    等公司，它们训练专有 LLM 并将其作为 API 端点（LLM-as-a-service）提供。许多这些公司还与云服务提供商合作，以完全托管服务的形式提供对这些模型的访问。主要云服务提供商的相关产品包括
    [Amazon Bedrock](https://oreil.ly/FVqRj) 和 [SageMaker JumpStart by Amazon](https://oreil.ly/e0a59)、[Vertex
    AI by Google](https://oreil.ly/mURoC) 以及 [Azure OpenAI by Microsoft](https://oreil.ly/Ag1r5)。
- en: Companies providing open source LLMs
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 提供开源 LLM 的公司
- en: These include companies that make the LLM weights public and monetize through
    providing deployment services ([Together AI](https://oreil.ly/urcAf)), companies
    whose primary business would benefit from more LLM adoption ([Cerebras](https://oreil.ly/2cVYY)),
    and research labs that have been releasing LLMs since the early days of Transformers
    (Microsoft, Google, Meta, Salesforce, etc.). Note that companies like Google have
    released both proprietary and open source LLMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括那些使 LLM 权重公开并通过提供部署服务来盈利的公司 ([Together AI](https://oreil.ly/urcAf))，那些主要业务将从更多
    LLM 采用中受益的公司 ([Cerebras](https://oreil.ly/2cVYY))，以及从 Transformer 早期就开始发布 LLM
    的研究实验室（微软、谷歌、Meta、Salesforce 等）。请注意，像谷歌这样的公司已经发布了专有和开源的 LLM。
- en: Self-organizing open source collectives and community research organizations
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织开源集体和社区研究组织
- en: This includes the pioneering community research organization [Eleuther AI](https://oreil.ly/ZSlbG),
    and [Big Science](https://oreil.ly/_NlUD). These organizations rely on grants
    for compute infrastructure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括开创性的社区研究组织 [Eleuther AI](https://oreil.ly/ZSlbG) 和 [Big Science](https://oreil.ly/_NlUD)。这些组织依赖资助来获取计算基础设施。
- en: Academia and government
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界和政府
- en: Due to the high capital costs, not many LLMs have come out of academia so far.
    Examples of LLMs from government/academia include the Abu Dhabi government-funded
    [Technology Innovation Institute](https://oreil.ly/aMwO2), which released the
    [Falcon model](https://oreil.ly/vdhsL), and Tsinghua University, which released
    the [GLM model](https://oreil.ly/K0_zX).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高资本成本，到目前为止，很少有 LLM 从学术界出现。来自政府/学术界的 LLM 例子包括阿布扎比政府资助的[技术创新研究所](https://oreil.ly/aMwO2)，它发布了[
    Falcon 模型](https://oreil.ly/vdhsL)，以及清华大学，它发布了[ GLM 模型](https://oreil.ly/K0_zX)。
- en: '[Table 5-1](#llm-provider-categories) shows the players in the LLM space, the
    category of entity they belong to, and the pre-trained models they have published.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5-1](#llm-provider-categories) 展示了 LLM 领域的参与者、他们所属的实体类别以及他们发布的预训练模型。'
- en: Table 5-1\. LLM Providers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. LLM 提供商
- en: '| Name | Category | Pre-trained models released |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 类别 | 发布的预训练模型 |'
- en: '| --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Google | Company | BERT, MobileBERT, T5, FLAN-T5, ByT5, Canine, UL2, Flan-UL2,
    Pegasus PaLM, PaLMV2, ELECTRA, Tapas, Switch |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 谷歌 | 公司 | BERT, MobileBERT, T5, FLAN-T5, ByT5, Canine, UL2, Flan-UL2, Pegasus
    PaLM, PaLMV2, ELECTRA, Tapas, Switch |'
- en: '| Microsoft | Company | DeBERTa, DialoGPT, BioGPT, MPNet |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 微软 | 公司 | DeBERTa, DialoGPT, BioGPT, MPNet |'
- en: '| OpenAI | Company | GPT-2, GPT-3, GPT-3.5, GPT-4 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 公司 | GPT-2, GPT-3, GPT-3.5, GPT-4 |'
- en: '| Amazon | Company | Titan |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊 | 公司 | Titan |'
- en: '| Anthropic | Company | Claude, Claude-2 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Anthropic | 公司 | Claude, Claude-2 |'
- en: '| Cohere | Company | Cohere Command, Cohere Base |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Cohere | 公司 | Cohere Command, Cohere Base |'
- en: '| Meta | Company | RoBERTa, Llama, Llama 2, BART, OPT, Galactica |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Meta | 公司 | RoBERTa, Llama, Llama 2, BART, OPT, Galactica |'
- en: '| Salesforce | Company | CTRL, XGen, EinsteinGPT |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Salesforce | 公司 | CTRL, XGen, EinsteinGPT |'
- en: '| MosaicML | Company (Acquired by Databricks) | MPT |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| MosaicML | 公司（被 Databricks 收购）| MPT |'
- en: '| Cerebras | Company | Cerebras-GPT, BTLM |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Cerebras | 公司 | Cerebras-GPT, BTLM |'
- en: '| Databricks | Company | Dolly-V1, Dolly-V2 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Databricks | 公司 | Dolly-V1, Dolly-V2 |'
- en: '| Stability AI | Company | StableLM |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Stability AI | 公司 | StableLM |'
- en: '| Together AI | Company | RedPajama |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Together AI | 公司 | RedPajama |'
- en: '| Ontocord AI | Nonprofit | MDEL |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Ontocord AI | 非营利组织 | MDEL |'
- en: '| Eleuther AI | Nonprofit | Pythia, GPT Neo, GPT-NeoX, GPT-J |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Eleuther AI | 非营利组织 | Pythia, GPT Neo, GPT-NeoX, GPT-J |'
- en: '| Big Science | Nonprofit | BLOOM |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 大科学 | 非营利组织 | BLOOM |'
- en: '| Tsinghua University | Academic | GLM |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 清华大学 | 学术机构 | GLM |'
- en: '| Technology Innovation Institute | Academic | Falcon |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 技术创新研究所 | 学术机构 | Falcon |'
- en: '| UC Berkeley | Academic | OpenLLaMA |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 加州大学伯克利分校 | 学术机构 | OpenLLaMA |'
- en: '| Adept AI | Company | Persimmon |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Adept AI | 公司 | Persimmon |'
- en: '| Mistral AI | Company | Mistral |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Mistral AI | 公司 | Mistral |'
- en: '| AI21 Labs | Company | Jurassic |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| AI21 Labs | 公司 | Jurassic |'
- en: '| X.AI | Company | Grok |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| X.AI | 公司 | Grok |'
- en: Model Flavors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型风味
- en: Each model is usually released with multiple variants. It is customary to release
    different-sized variants of the same model. As an example, Llama 2 comes in 7B,
    13B, and 70B sizes, where these numbers refer to the number of parameters in the
    model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型通常都会发布多个变体。发布相同模型的不同大小的变体是惯例。例如，Llama 2 有 7B、13B 和 70B 的大小，这些数字指的是模型中的参数数量。
- en: These days, LLM providers augment their pre-trained models in various ways to
    make them more amenable to user tasks. The augmentation process typically involves
    fine-tuning the model in some way, often incorporating human supervision. Some
    of these fine-tuning exercises can cost millions of dollars in terms of human
    annotations. We will refer to pre-trained models that have not undergone any augmentation
    as base models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些日子里，LLM 提供商以各种方式增强他们的预训练模型，使其更易于满足用户任务。增强过程通常涉及以某种方式微调模型，通常包括人工监督。这些微调练习中的一些可能需要数百万美元的人工标注费用。我们将未经过任何增强的预训练模型称为基础模型。
- en: The following sections describe some of the popular augmentation types.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节描述了一些流行的增强类型。
- en: Instruct-models
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Instruct-models
- en: Instruct-models, or instruction-tuned models, are specialized in following instructions
    written in natural language. While base models possess powerful capabilities,
    they are akin to a rebellious teenager; effectively interacting with them is possible
    only after tediously engineering the right prompts through trial and error, which
    tend to be brittle. This is because the base models are trained on either denoising
    objectives or next-word prediction objectives, which are different from the tasks
    users typically want to solve. By instruction-tuning the base model, the resulting
    model is able to more effectively respond to human instructions and be helpful.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 指令模型，或称为指令微调模型，擅长遵循自然语言中编写的指令。尽管基础模型拥有强大的能力，但它们就像一个叛逆的青少年；只有通过反复试验和错误地精心设计正确的提示，才能有效地与之互动，这往往很脆弱。这是因为基础模型是在去噪目标或下一词预测目标上训练的，这些目标与用户通常想要解决的问题不同。通过指令微调基础模型，得到的模型能够更有效地响应人类指令并发挥作用。
- en: A typical instruction-tuning dataset consists of a diverse set of tasks expressed
    in natural language, along with input-output pairs. In [Chapter 6](ch06.html#llm-fine-tuning),
    we will explore various techniques to construct instruction-tuning datasets and
    demonstrate how to perform instruction-tuning on a model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的指令微调数据集包括用自然语言表达的各种任务，以及输入-输出对。在[第6章](ch06.html#llm-fine-tuning)中，我们将探讨构建指令微调数据集的各种技术，并演示如何在模型上执行指令微调。
- en: Here is an example from a popular instruction-tuning dataset called [FLAN](https://oreil.ly/YJ_Xr).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个来自流行的指令微调数据集[FLAN](https://oreil.ly/YJ_Xr)的例子。
- en: '*Prompt:* “What is the sentiment of the following review? The pizza was ok
    but the service was terrible. I stopped in for a quick lunch and got the slice
    special but it ended up taking an hour after waiting several minutes for someone
    at the front counter and then again for the slices. The place was empty other
    than myself, yet I couldn’t get any help/service. OPTIONS: - negative - positive”'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* “以下评论的情感是什么？披萨还可以，但服务太糟糕了。我停下来吃了个快速午餐，点了特制披萨，但等了十几分钟后，前面柜台的某人，然后又等披萨，结果花了一个小时。除了我自己，这个地方空无一人，但我得不到任何帮助/服务。选项：-
    负面 - 正面”'
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*FLAN:* “Negative”'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*FLAN:* “负面”'
- en: In this example, the input consists of an instruction, “What is the sentiment
    of the following review?” expressed in a way that humans would naturally express,
    along with the input and output. The input is the actual review and the output
    is the solution to the task, either generated by a model or annotated by a human.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，输入包括一个指令，“以下评论的情感是什么？”以人类自然表达的方式表达，以及输入和输出。输入是实际的评论，输出是任务的解决方案，可以是模型生成的或由人类标注的。
- en: '[Figure 5-1](#instruction-tuning1) demonstrates the instruction-tuning process.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-1](#instruction-tuning1)演示了指令微调过程。'
- en: '![Instruction tuning process](assets/dllm_0501.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![指令微调过程](assets/dllm_0501.png)'
- en: Figure 5-1\. Instruction-tuning process
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 指令微调过程
- en: Instruction-tuning is one of several techniques that come under the umbrella
    of supervised fine-tuning (SFT). In addition to improving the ability of a model
    to respond effectively to user tasks, SFT-based approaches can also be used to
    make it less harmful by training on safety datasets that help align model outputs
    with the values and preferences of the model creators.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调是监督微调（SFT）技术范畴下的几种技术之一。除了提高模型有效响应用户任务的能力外，基于SFT的方法还可以通过在安全数据集上训练来减少其潜在的危害，这些数据集有助于使模型输出与模型创建者的价值观和偏好保持一致。
- en: More advanced techniques to achieve this alignment include reinforcement learning-based
    methods like reinforcement learning from human feedback (RLHF) and reinforcement
    learning from AI feedback (RLAIF).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这种一致性的更高级技术包括基于强化学习的方法，如基于人类反馈的强化学习（RLHF）和基于AI反馈的强化学习（RLAIF）。
- en: In RLHF training, human annotators select or rank candidate outputs based on
    certain criteria, like helpfulness and harmlessness. These annotations are used
    to iteratively train a reward model, which ultimately leads to the LLM being more
    controllable, for example, by refusing to answer inappropriate requests from users.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF训练中，人类标注者根据某些标准（如有用性和无害性）选择或对候选输出进行排序。这些标注用于迭代训练奖励模型，最终导致LLM更具可控性，例如，拒绝回答用户的不适当请求。
- en: '[Figure 5-2](#rlhf-1) shows the RLHF training process.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-2](#rlhf-1)展示了RLHF训练过程。'
- en: '![RLHF](assets/dllm_0502.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![RLHF](assets/dllm_0502.png)'
- en: Figure 5-2\. Reinforcement learning from human feedback
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2. 基于人类反馈的强化学习
- en: We will cover RLHF and other alignment techniques in detail in [Chapter 8](ch08.html#ch8).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第8章](ch08.html#ch8)中详细介绍RLHF和其他对齐技术。
- en: Instead of relying on human feedback for alignment training, one can also leverage
    LLMs to choose between outputs based on their adherence to a set of principles
    (don’t be racist, don’t be rude, etc.). This technique was introduced by Anthropic
    and is called RLAIF. In this technique, humans only provide a desired set of principles
    and values (referred to as [Constitutional AI](https://oreil.ly/d8FeW)), and the
    LLM is tasked with determining whether its outputs adhere to these principles.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了依赖人类反馈进行对齐训练外，还可以利用LLM根据其遵循一组原则（不要种族歧视，不要无礼等）来选择输出。这种技术由Anthropic引入，称为RLAIF。在这种技术中，人类只提供一组期望的原则和价值观（称为[宪法AI](https://oreil.ly/d8FeW)），而LLM的任务是确定其输出是否遵循这些原则。
- en: Instruction-tuned models often take the suffix *instruct*, like RedPajama-Instruct.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调整模型通常以*instruct*结尾，例如RedPajama-Instruct。
- en: Chat-models
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聊天模型
- en: Chat-models are instruction-tuned models that are optimized for multi-turn dialog.
    Examples include ChatGPT, Llama 2-Chat, MPT-Chat, OpenAssistant, etc.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型是针对多轮对话优化的指令调整模型。例子包括ChatGPT、Llama 2-Chat、MPT-Chat、OpenAssistant等。
- en: Long-context models
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长内容模型
- en: As discussed in [Chapter 1](ch01.html#chapter_llm-introduction), Transformer-based
    LLMs have a limited context length. To recap, context length typically refers
    to the sum of the number of input and output tokens processed by the model per
    invocation. Typical context lengths of modern LLMs range from 8,000 to 128,000
    tokens, with some variants of Gemini supporting over a million tokens. Some models
    are released with a long-context variant; for example GPT 3.5 comes with a default
    4K context size but also has a 16K context size variant. [MPT](https://oreil.ly/wKqdL)
    also has a long-context variant that has been trained on 65k context length but
    can potentially be used for even longer contexts during inference.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)中所述，基于Transformer的LLM具有有限的内容长度。为了回顾，内容长度通常指每次调用模型处理的输入和输出标记的总数。现代LLM的典型内容长度从8,000到128,000个标记不等，一些Gemini的变体支持超过一百万个标记。一些模型以长内容变体发布；例如，GPT
    3.5默认的上下文大小为4K，但也有16K上下文大小的变体。[MPT](https://oreil.ly/wKqdL)也有一个长内容变体，该变体在65k上下文长度上进行了训练，但在推理过程中可以用于甚至更长的上下文。
- en: Domain-adapted or task-adapted models
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域自适应或任务自适应模型
- en: LLM providers also might perform fine-tuning on specific tasks like summarization
    or financial sentiment analysis. They may also produce distilled versions of the
    model, where a smaller model is fine-tuned on outputs from the larger model for
    a particular task. Examples of task-specific fine-tunes include [FinBERT](https://oreil.ly/uKUAp),
    which is fine-tuned on financial sentiment analysis datasets, and [UniversalNER](https://oreil.ly/8A0pn),
    which is distilled using named-entity-recognition data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM提供商也可能在特定任务（如摘要或金融情感分析）上进行微调。他们还可能产生模型的蒸馏版本，其中较小的模型在特定任务的较大模型输出上进行微调。特定任务微调的例子包括在金融情感分析数据集上微调的[FinBERT](https://oreil.ly/uKUAp)，以及使用命名实体识别数据蒸馏的[UniversalNER](https://oreil.ly/8A0pn)。
- en: Open Source LLMs
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源LLM
- en: 'Open source is often used as a catch-all phrase to refer to models with some
    aspect that is publicly available. We will define open source as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 开源通常用作一个万能短语，指代具有某些公开方面模型的模型。我们将开源定义为：
- en: Software artifacts that are released under a license that allows users to *study*,
    *use*, *modify*, and *redistribute* them to *anyone* and for any *purpose*.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在许可下发布的软件工件，允许用户*研究*、*使用*、*修改*并将它们*重新分发*给*任何人*和任何*目的*。
- en: For a more formal and comprehensive definition of open source software, refer
    to the Open Source Initiative’s [official definition](https://oreil.ly/7cezH).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开源软件的更正式和全面定义，请参阅开源促进会（Open Source Initiative）的[官方定义](https://oreil.ly/7cezH)。
- en: 'For an LLM to be considered fully open, all of the following needs to be published:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要使一个大型语言模型（LLM）被视为完全开源，以下所有内容都需要公开发布：
- en: Model weights
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型权重
- en: This includes all the parameters of the model and the model configuration. Having
    access to this enables us to add to or modify the model parameters in any way
    we deem fit. Model checkpoints at various stages of training are also encouraged
    to be released.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括模型的全部参数和模型配置。能够访问这些内容使我们能够以任何我们认为合适的方式添加或修改模型参数。鼓励发布训练过程中各个阶段的模型检查点。
- en: Model code
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型代码
- en: Releasing only the weights of the model is akin to providing a software binary
    without providing the source code. Model code not only includes model training
    code and hyperparameter settings but also code used for pre-processing training
    data. Releasing information about infrastructure setup and configuration also
    goes a long way toward enhancing model reproducibility. In most cases, even with
    model code fully available, models may not be easily reproducible due to resource
    limitations and the nondeterministic nature of training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 仅发布模型权重类似于提供没有源代码的软件二进制文件。模型代码不仅包括模型训练代码和超参数设置，还包括用于预处理训练数据的代码。发布有关基础设施设置和配置的信息也有助于提高模型的可重复性。在大多数情况下，即使模型代码完全可用，由于资源限制和训练的非确定性，模型可能也不容易重复。
- en: Training data
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: This includes the training data used for the model, and ideally information
    or code on how it was sourced. It is also encouraged to release data at different
    stages of transformation of the data preprocessing pipeline, as well as the order
    in which the data was fed to the model. Training data is the component that is
    least published by model providers. Thus, most open source models are not *fully
    open* because the dataset is not public.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括用于模型的训练数据，以及理想情况下有关其来源的信息或代码。还鼓励在数据预处理管道的不同阶段发布数据，以及数据被输入到模型中的顺序。训练数据是模型提供者最少公开的组件。因此，大多数开源模型并不是“完全开放”的，因为数据集不是公开的。
- en: Training data is often not released due to competitive reasons. As discussed
    in Chapters [3](ch03.html#chapter-LLM-tokenization) and [4](ch04.html#chapter_transformer-architecture),
    most LLMs today use variants of the same architecture and training code. The distinguishing
    factor can often be the data content and preprocessing. Parts of the training
    data might be acquired using a licensing agreement, which prohibits the model
    provider from releasing the data publicly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于竞争原因，训练数据通常不会公开。正如第[3](ch03.html#chapter-LLM-tokenization)章和第[4](ch04.html#chapter_transformer-architecture)章所讨论的，今天的大多数大型语言模型（LLM）都使用相同的架构和训练代码的变体。区分因素通常可以归结为数据内容和预处理。部分训练数据可能通过许可协议获得，这禁止模型提供者公开数据。
- en: Another reason for not releasing training data is that there are unresolved
    legal issues pertaining to training data, especially surrounding copyright. As
    an example, The Pile dataset created by Eleuther AI is no longer available at
    the official link because it contains text from copyrighted books (the Books3
    dataset). Note that The Pile is pre-processed so the books are not in human-readable
    form and are not easily reproducible, as they are split, shuffled, and mixed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不公开训练数据的另一个原因是与训练数据相关的未解决的法律问题，特别是围绕版权的问题。例如，由Eleuther AI创建的The Pile数据集不再在官方链接上可用，因为它包含受版权保护书籍（Books3数据集）中的文本。请注意，The
    Pile已经过预处理，书籍不是人类可读的形式，并且不容易重复，因为它们被分割、打乱和混合。
- en: Most training data is sourced from the open web and thus may potentially contain
    violent or sexual content that is illegal in certain jurisdictions. Despite the
    best intentions and rigorous filtering, some of these data might still be present
    in the final dataset. Thus many datasets that have been previously open are no
    longer open, LAION’s image datasets being one example.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数训练数据来源于公开网络，因此可能包含在某些司法管辖区非法的暴力或色情内容。尽管有最好的意图和严格的过滤，但这些数据中的一些可能仍然存在于最终数据集中。因此，许多之前公开的数据集现在不再公开，LAION的图像数据集就是一个例子。
- en: 'Ultimately, the license under which the model has been released determines
    the terms under which you can use, modify, or redistribute the original or modified
    LLM. Broadly speaking, open LLMs are distributed under three types of licenses:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型发布所依据的许可证决定了您可以使用、修改或重新分发原始或修改后的LLM的条款。广义而言，开放LLM的发布通常遵循三种类型的许可证：
- en: Noncommercial
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 非商业
- en: These licenses only allow research and personal use and prohibit the use of
    the model for commercial purposes. In many cases, the model artifacts are gated
    through an application form where a user would have to justify their need for
    access by providing a compelling research use case.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些许可证仅允许研究和个人使用，并禁止将模型用于商业目的。在许多情况下，模型工件通过申请表进行控制，用户必须通过提供有说服力的研究用例来证明他们访问的需求。
- en: Copy-left
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 知识共享
- en: This type of license permits commercial usage, but all source or derivative
    work needs to be released under the same license, thus making it harder to develop
    proprietary modifications. The degree to which this condition applies depends
    on the specific license being used.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此类许可证允许商业使用，但所有源代码或衍生作品都需要在相同的许可证下发布，这使得开发专有修改变得更加困难。此条件适用的程度取决于所使用的具体许可证。
- en: Permissive
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 宽松
- en: This type of license permits commercial usage, including modifying and redistributing
    it in proprietary applications, i.e., there is no obligation for the redistribution
    to be open source. Some licenses in this category also permit patents.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此类许可证允许商业使用，包括在专有应用程序中修改和重新分发，即没有义务将重新分发作为开源。此类别中的一些许可证还允许专利。
- en: New types of licenses are being devised that restrict usage of the model for
    particular use cases, often for safety reasons. An example of this is the [Open
    RAIL-M license](https://oreil.ly/2UVMe), which prohibits usage of the model in
    use cases like providing medical advice, law enforcement, immigration and asylum
    processes, etc. For a full list of restricted use cases, see Attachment A of the
    license.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正在制定新的许可证类型，这些许可证限制模型在特定用途案例中的使用，通常出于安全原因。一个例子是[Open RAIL-M许可证](https://oreil.ly/2UVMe)，它禁止在提供医疗建议、执法、移民和庇护程序等用途案例中使用该模型。有关受限用途案例的完整列表，请参阅许可证附件A。
- en: As a practitioner intending to use open LLMs in your organization for commercial
    reasons, it is best to use ones with permissive licenses. Popular examples of
    permissive licenses include the Apache 2.0 and the MIT license.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作为打算在组织中出于商业目的使用开源LLM的从业者，最好使用具有宽松许可证的LLM。宽松许可证的流行例子包括Apache 2.0和MIT许可证。
- en: '[Creative Commons (CC) licenses](https://oreil.ly/PQy6D) are a popular class
    of licenses used to distribute open LLMs.The licenses have names like CC-BY-NC-SA,
    etc. Here is an easy way to remember what these names mean:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[创意共享（CC）许可证](https://oreil.ly/PQy6D)是用于分发开源LLM的流行许可证类别。这些许可证的名称类似于CC-BY-NC-SA等。这里有一个记住这些名称含义的简单方法：'
- en: BY
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: BY
- en: If the license contains this term, it means attribution is needed. If it contains
    only CC-BY, it means the license is permissive.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果许可证包含此条款，则意味着需要署名。如果只包含CC-BY，则意味着许可证是宽松的。
- en: SA
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SA
- en: If the license contains this term, it means redistribution should occur under
    the same terms as this license. In other words, it is a copy-left license.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果许可证包含此条款，则意味着重新分发应在此许可证的相同条款下进行。换句话说，它是一个版权左许可证。
- en: NC
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: NC
- en: NC stands for noncommercial. Thus, if the license contains this term, the model
    can only be used for research or personal use cases.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: NC代表非商业。因此，如果许可证包含此条款，则模型只能用于研究或个人用途案例。
- en: ND
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ND
- en: ND stands for no derivatives. If the license contains this term, then distribution
    of modifications to the model is not allowed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ND代表无衍生。如果许可证包含此条款，则不允许对模型进行修改的分发。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: Today, models that have open weights and open code and are released under a
    license that allows redistribution to anyone and for any use case are considered
    open source models. Arguably, however, access to the training data is also crucial
    to inspect and study the model, which is part of the open source definition we
    introduced earlier.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，具有公开权重和公开代码，并且发布在允许任何人出于任何用途案例重新分发的许可证下的模型被认为是开源模型。然而，可以争论的是，访问训练数据对于检查和研究模型也至关重要，这是我们在之前引入的开源定义的一部分。
- en: '[Table 5-2](#llm-taxonomy) shows the various LLMs available, the licenses under
    which they are published, and their available sizes and flavors. Note that the
    LLM may be instruction-tuned or chat-tuned by a different entity than the one
    that pre-trained the LLM.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[表5-2](#llm-taxonomy)显示了可用的各种LLM、它们发布的许可证以及它们可用的尺寸和版本。请注意，LLM可能由不同于预训练LLM的实体进行指令微调或聊天微调。'
- en: Table 5-2\. List of available LLMs
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-2。可用的LLM列表
- en: '| Name | Availability | Sizes | Variants |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 可用性 | 尺寸 | 变体 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-4 | Proprietary | Unknown | GPT-4 32K context, GPT-4 8K context |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 专有 | 未知 | GPT-4 32K上下文，GPT-4 8K上下文 |'
- en: '| GPT-3.5 Turbo | Proprietary | Unknown | GPT-3.5 4K context, GPT-3.5 16K context
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | 专有 | 未知 | GPT-3.5 4K上下文，GPT-3.5 16K上下文 |'
- en: '| Claude Instant | Proprietary | Unknown | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Claude Instant | 专有 | 未知 | - |'
- en: '| Claude 2 | Proprietary | Unknown | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Claude 2 | 专有 | 未知 | - |'
- en: '| MPT | Apache 2.0 | 1B, 7B, 30B | MPT 65K storywriter |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MPT | Apache 2.0 | 1B、7B、30B | MPT 65K故事作家 |'
- en: '| CerebrasGPT | Apache 2.0 | 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, 13B | CerebrasGPT
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| CerebrasGPT | Apache 2.0 | 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, 13B | CerebrasGPT
    |'
- en: '| Stability LM | CC-BY-SA | 7B | - |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 稳定性 LM | CC-BY-SA | 7B | - |'
- en: '| RedPajama | Apache 2.0 | 3B, 7B | RedPajama-INCITE-Instruct, RedPajama-INCITE-Chat
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| RedPajama | Apache 2.0 | 3B, 7B | RedPajama-INCITE-Instruct, RedPajama-INCITE-Chat
    |'
- en: '| GPT-Neo X | Apache 2.0 | 20B | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo X | Apache 2.0 | 20B | - |'
- en: '| BLOOM | Open, restricted use | 176B | BLOOMZ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | 开放，限制性使用 | 176B | BLOOMZ |'
- en: '| Llama | Open, no commercial use | 7B, 13B, 33B, 65B | - |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Llama | 开放，无商业用途 | 7B, 13B, 33B, 65B | - |'
- en: '| Llama 2 | Open, commercial use | 7B, 13B, 70B | Llama 2-Chat |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 | 开放，商业用途 | 7B, 13B, 70B | Llama 2-Chat |'
- en: '| Zephyr | Apache 2.0 | 7B | - |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Zephyr | Apache 2.0 | 7B | - |'
- en: '| Gemma | Open, restricted use | 2B, 7B | Gemma-Instruction Tuned |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | 开放，限制性使用 | 2B, 7B | Gemma-Instruction Tuned |'
- en: How to Choose an LLM for Your Task
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何为您的任务选择LLM
- en: 'Given the plethora of options available, how do you ensure you choose the right
    LLM for your task? Depending on your situation, there are a multitude of criteria
    to consider, including:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多选项中，您如何确保选择适合您任务的正确LLM？根据您的具体情况，有许多标准需要考虑，包括：
- en: Cost
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: This includes inference or fine-tuning costs, and costs associated with building
    software scaffolding, monitoring and observability, deployment and maintenance
    (collectively referred to as LLMOps).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括推理或微调成本，以及构建软件框架、监控和可观察性、部署和维护（统称为LLMOps）的成本。
- en: '[Time per output token (TPOT)](https://oreil.ly/mEDRt)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[每输出令牌时间（TPOT）](https://oreil.ly/mEDRt)'
- en: This is a metric used to measure the speed of text generation as experienced
    by the end user.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于衡量终端用户体验到的文本生成速度的指标。
- en: Task performance
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 任务性能
- en: This refers to the performance requirements of the task and the relevant metrics
    like precision or accuracy. What level of performance is *good enough*?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是任务的性能要求以及相关的指标，如精确度或准确性。什么水平的性能是*足够好*的？
- en: Type of tasks
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 任务类型
- en: The nature of the tasks the LLM will be used for, like summarization, question
    answering, classification, etc.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LLM将要使用的任务性质，如摘要、问答、分类等。
- en: Capabilities required
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的能力
- en: Examples of capabilities include arithmetic reasoning, logical reasoning, planning,
    task decomposition, etc. A lot of these capabilities, to the extent that they
    actually exist or approximate, are *emergent properties* of an LLM as discussed
    in [Chapter 1](ch01.html#chapter_llm-introduction), and are not exhibited by smaller
    models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 能力的例子包括算术推理、逻辑推理、规划、任务分解等。许多这些能力，在它们实际存在或近似到一定程度时，是LLM的*涌现属性*，如[第1章](ch01.html#chapter_llm-introduction)中讨论的那样，并且不是由较小模型展示的。
- en: Licensing
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证
- en: You can use only those models that allow your mode of usage. Even models that
    explicitly allow commercial use can have restrictions on certain types of use
    cases. For example, as noted earlier, the Big Science OpenRAIL-M license restricts
    the usage of the LLM in use cases pertaining to law enforcement, immigration,
    or asylum processes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您只能使用允许您使用模式的模型。即使某些类型的用例明确允许商业使用，模型也可能有限制。例如，如前所述，Big Science OpenRAIL-M许可证限制了LLM在涉及执法、移民或庇护程序用例中的使用。
- en: In-house ML/MLOps talent
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 内部机器学习/ML/MLOps人才
- en: The strength of in-house talent determines the customizations you can afford.
    For example, do you have enough in-house talent for building inference optimization
    systems?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 内部人才的实力决定了您可以承担的定制化程度。例如，您是否有足够的内部人才来构建推理优化系统？
- en: Other nonfunctional criteria
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其他非功能标准
- en: This includes safety, security, privacy, etc. Cloud providers and startups are
    already implementing solutions that can address these issues.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括安全、安全、隐私等。云提供商和初创公司已经在实施可以解决这些问题的解决方案。
- en: You may have to choose between proprietary and open source LLMs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要在专有和开源LLM之间做出选择。
- en: Open Source Versus Proprietary LLMs
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源与专有LLM
- en: Debates about the merits of open source versus proprietary software have been
    commonplace in the tech industry for several decades now, and we are seeing it
    become increasingly relevant in the realm of LLMs as well. The biggest advantage
    of open source models are the transparency and flexibility they provide, not necessarily
    the cost. Self-hosting open source LLMs can incur a lot of engineering overhead
    and compute/memory costs, and using managed services might not always be able
    to match proprietary models in terms of latency, throughput, and inference cost.
    Moreover, many open source LLMs are not easily accessible through managed services
    and other third-party deployment options. This situation is bound to change dramatically
    as the field matures, but in the meanwhile, run through your calculations for
    your specific situation to determine the costs incurred for using each (type of)
    model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于开源软件与专有软件优缺点的争论在科技行业已经司空见惯了几十年，现在我们也在LLM（大型语言模型）领域看到这一现象变得越来越相关。开源模型最大的优势是它们提供的透明度和灵活性，而不仅仅是成本。自托管开源LLM可能会产生大量的工程开销和计算/内存成本，而使用托管服务可能无法始终在延迟、吞吐量和推理成本方面与专有模型相匹配。此外，许多开源LLM通过托管服务和第三方部署选项并不容易获得。随着该领域的成熟，这种状况必将发生巨大变化，但在此期间，请针对您的具体情况运行计算，以确定使用每种（类型）模型所产生的成本。
- en: The flexibility provided by open source models helps with your ability to debug,
    interpret, and augment the LLM with any kind of training/fine-tuning you choose,
    instead of the restricted avenues made available by the LLM provider. This allows
    you to more substantially align the LLM to your preferences and values instead
    of the ones decided by the LLM provider. Having full availability of all the token
    probabilities (logits) is a superpower, as we will see throughout the book.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型提供的灵活性有助于您调试、解释和通过您选择的任何类型的训练/微调来增强LLM，而不是LLM提供商提供的受限途径。这使您能够更实质性地将LLM与您的偏好和价值观对齐，而不是由LLM提供商决定的那些。在整个书中，我们将看到拥有所有标记概率（logits）的完全可用性是一种超级能力。
- en: The availability of open source LLMs has enabled teams to develop models and
    applications that might not be lucrative for larger companies with a profit motive,
    like fine-tuning models to support low-resource languages (languages that do not
    have a significant data footprint on the internet, like regional languages of
    India or Indigenous languages of Canada). An example is the [Kannada Llama model](https://oreil.ly/hoBQ1),
    built over Llama 2 by continually pre-training and fine-tuning on tokens from
    the Kannada language, a regional language of India.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 开源LLM的可用性使团队能够开发出对具有盈利动机的大型公司来说可能并不盈利的模型和应用，例如微调模型以支持低资源语言（在互联网上没有显著数据足迹的语言，如印度的地区语言或加拿大的土著语言）。一个例子是[卡纳达Llama模型](https://oreil.ly/hoBQ1)，它是在Llama
    2的基础上，通过不断在卡纳达语言（印度的地区语言）的标记上进行预训练和微调而构建的。
- en: Not all open source models are fully transparent. As mentioned earlier, most
    for-profit companies that release open source LLMs do not make the training datasets
    public. For instance, Meta hasn’t disclosed all the details of the training datasets
    used to train the Llama 2 model. Knowing which datasets are used to train the
    model can help you assess whether there is test set contamination and understand
    what kind of knowledge you can expect the LLM to possess.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有开源模型都是完全透明的。如前所述，大多数发布开源LLM的盈利性公司并不公开其训练数据集。例如，Meta并未披露用于训练Llama 2模型的训练数据集的所有细节。了解用于训练模型的哪些数据集可以帮助您评估是否存在测试集污染，并了解LLM可能拥有的知识类型。
- en: As of this book’s writing, open source models like Llama 3.2 and DeepSeek v3
    have more or less caught up to state-of-the-art proprietary models from OpenAI
    or Anthropic. However, there is a new gap developing between proprietary and open
    source models in the realm of reasoning models like OpenAI’s o3, that use inference-time
    compute techniques (discussed in [Chapter 8](ch08.html#ch8)). Throughout this
    book, we will showcase scenarios where open source models have an advantage.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就本书撰写时的情况来看，开源模型如Llama 3.2和DeepSeek v3在某种程度上已经赶上了OpenAI或Anthropic等公司最先进的专有模型。然而，在推理模型领域，如OpenAI的o3，它使用推理时计算技术（在第8章中讨论）的情况下，专有模型与开源模型之间正在出现一个新的差距。在整个书中，我们将展示开源模型具有优势的场景。
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Always check if the model provider has an active developer community on GitHub/Discord/Slack,
    and that the development team is actively engaged in those channels, responding
    to user comments and questions. I recommend preferring models with active developer
    communities, provided they satisfy your primary criteria.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 总是检查模型提供者是否在GitHub/Discord/Slack上有活跃的开发者社区，并且开发团队是否积极参与这些渠道，回应用户评论和问题。如果它们满足你的主要标准，我建议优先考虑具有活跃开发者社区的模型。
- en: LLM Evaluation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM评估
- en: 'We will start this section with a caveat: evaluating LLMs is probably the most
    challenging task in the LLM space at present. Current methods of benchmarking
    are broken, easily gamed, and hard to interpret. Nevertheless, benchmarks are
    still a useful starting point on your road to evaluation. We will start by looking
    at current public benchmarks and then discuss how you can build more holistic
    internal benchmarks.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个部分开始，先提出一个警告：在当前LLM（大型语言模型）领域，评估LLM可能是最具挑战性的任务。现有的基准测试方法存在缺陷，容易被操纵，且难以解释。尽管如此，基准测试仍然是你在评估之路上的一个有用起点。我们将从查看当前的公共基准测试开始，然后讨论你如何构建更全面的内部基准测试。
- en: To evaluate LLMs on their task performance, there are a lot of benchmark datasets
    that test a wide variety of skills. Not all skills are relevant to your use case,
    so you can choose to focus on specific benchmarks that test the skills you need
    the LLM to perform well on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估LLM在任务性能上的表现，有许多基准数据集可以测试广泛的技能。并非所有技能都与你的用例相关，因此你可以选择专注于特定的基准，这些基准测试的是LLM需要表现良好的技能。
- en: The leaderboard on these benchmark tests changes very often, especially if only
    open source models are being evaluated, but that does not mean you need to change
    the LLMs you use every time there is a new leader on the board. Usually, the differences
    between the top models are quite marginal. The fine-grained choice of LLM usually
    isn’t the most important criteria determining the success of your task, and you
    are better off spending that bandwidth working on cleaning and understanding your
    data, which is still the most important component of the project.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基准测试的排行榜变化非常频繁，尤其是在只评估开源模型的情况下，但这并不意味着每次榜单上出现新的领先者时，你都需要更换你使用的LLM。通常，顶级模型之间的差异相当微小。LLM的精细选择通常不是决定你任务成功与否的最重要标准，你最好将带宽用于清理和理解你的数据，这仍然是项目最重要的组成部分。
- en: Let’s look at a few popular ways in which the field is evaluating LLMs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看该领域评估LLM的几种流行方式。
- en: Eleuther AI LM Evaluation Harness
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Eleuther AI LM评估工具
- en: Through the [LM Evaluation Harness](https://oreil.ly/SiOXq), Eleuther AI supports
    benchmarking on over 400 different benchmark tasks, evaluating skills as varied
    as open-domain question answering, arithmetic and logical reasoning, linguistic
    tasks, machine translation, toxic language detection, etc. You can use this tool
    to evaluate any model on the [Hugging Face Hub](https://oreil.ly/IHd22), a platform
    containing thousands of pre-trained and fine-tuned models, on the benchmarks of
    your choice.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[LM评估工具](https://oreil.ly/SiOXq)，Eleuther AI支持在超过400个不同的基准任务上进行基准测试，评估的技能范围包括开放域问答、算术和逻辑推理、语言任务、机器翻译、有害语言检测等。你可以使用这个工具在[Hugging
    Face Hub](https://oreil.ly/IHd22)上评估任何模型，这是一个包含数千个预训练和微调模型的平台，并在你选择的基准上进行测试。
- en: 'Here is an example from `bigbench_formal_fallacies_syllogisms_negation`, one
    of the benchmark tasks:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个来自`bigbench_formal_fallacies_syllogisms_negation`基准任务的示例：
- en: '[PRE0]` `premises``,` `deductively` `valid` `or` `invalid``?``",` [PRE1] [PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]` `前提``,` `演绎` `有效` `或` `无效``?``",` [PRE1] [PRE2]'
- en: '[PRE3] [PRE4]`py  [PRE5]py  [PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3] [PRE4]`py  [PRE5]py  [PRE6]'
