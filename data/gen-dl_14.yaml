- en: Chapter 10\. Advanced GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 高级GANs
- en: '[Chapter 4](ch04.xhtml#chapter_gan) introduced generative adversarial networks
    (GANs), a class of generative model that has produced state-of-the-art results
    across a wide variety of image generation tasks. The flexibility in the model
    architecture and training process has led academics and deep learning practitioners
    to find new ways to design and train GANs, leading to many different advanced
    *flavors* of the architecture that we shall explore in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](ch04.xhtml#chapter_gan)介绍了生成对抗网络（GANs），这是一类生成模型，在各种图像生成任务中取得了最先进的结果。模型架构和训练过程的灵活性导致学术界和深度学习从业者找到了设计和训练GAN的新方法，从而产生了许多不同的高级架构，我们将在本章中探讨。'
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Explaining all GAN developments and their repercussions in detail could easily
    fill another book. The [GAN Zoo repository](https://oreil.ly/Oy6bR) on GitHub
    contains over 500 distinct examples of GANs with linked papers, ranging from ABC-GAN
    to ZipNet-GAN!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 详细解释所有GAN发展及其影响可能需要另一本书。GitHub上的[GAN Zoo代码库](https://oreil.ly/Oy6bR)包含了500多个不同的GAN示例，涵盖了从ABC-GAN到ZipNet-GAN的各种GAN，并附有相关论文链接！
- en: In this chapter we will cover the main GANs that have been influential in the
    field, including a detailed explanation of the model architecture and training
    process for each.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍对该领域产生影响的主要GANs，包括对每个模型的模型架构和训练过程的详细解释。
- en: 'We will first explore three important models from NVIDIA that have pushed the
    boundaries of image generation: ProGAN, StyleGAN, and StyleGAN2\. We will analyze
    each of these models in enough detail to understand the fundamental concepts that
    underpin the architectures and see how they have each built on ideas from earlier
    papers.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨NVIDIA推动图像生成边界的三个重要模型：ProGAN、StyleGAN和StyleGAN2。我们将对每个模型进行足够详细的分析，以理解支撑架构的基本概念，并看看它们如何各自建立在早期论文的想法基础上。
- en: 'We will also explore two other important GAN architectures that incorporate
    attention: the Self-Attention GAN (SAGAN) and BigGAN, which built on many of the
    ideas in the SAGAN paper. We have already seen the power of the attention mechanism
    in the context of Transformers in [Chapter 9](ch09.xhtml#chapter_transformer).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨另外两种重要的GAN架构，包括引入注意力机制的Self-Attention GAN（SAGAN）和BigGAN，后者在SAGAN论文中的许多想法基础上构建。我们已经在[第9章](ch09.xhtml#chapter_transformer)中看到了注意力机制在变换器中的威力。
- en: Lastly, we will cover VQ-GAN and ViT VQ-GAN, which incorporate a blend of ideas
    from variational autoencoders, Transformers, and GANs. VQ-GAN is a key component
    of Google’s state-of-the-art text-to-image generation model Muse.^([1](ch10.xhtml#idm45387005226448))
    We will explore so-called multimodal models in more detail in [Chapter 13](ch13.xhtml#chapter_multimodal).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍VQ-GAN和ViT VQ-GAN，它们融合了变分自动编码器、变换器和GAN的思想。VQ-GAN是谷歌最先进的文本到图像生成模型Muse的关键组成部分。我们将在[第13章](ch13.xhtml#chapter_multimodal)中更详细地探讨所谓的多模型。
- en: Training Your Own Models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的模型
- en: For conciseness I have chosen not to include code to directly build these models
    in the code repository for this book, but instead will point to publicly available
    implementations where possible, so that you can train your own versions if you
    wish.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我选择不在本书的代码库中直接构建这些模型的代码，而是将尽可能指向公开可用的实现，以便您可以根据需要训练自己的版本。
- en: ProGAN
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ProGAN
- en: ProGAN is a technique developed by NVIDIA Labs in 2017^([2](ch10.xhtml#idm45387005216528))
    to improve both the speed and stability of GAN training. Instead of immediately
    training a GAN on full-resolution images, the ProGAN paper suggests first training
    the generator and discriminator on low-resolution images of, say, 4 × 4 pixels
    and then incrementally adding layers throughout the training process to increase
    the resolution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ProGAN是NVIDIA实验室在2017年开发的一种技术，旨在提高GAN训练的速度和稳定性。ProGAN论文建议，不要立即在全分辨率图像上训练GAN，而是首先在低分辨率图像（例如4×4像素）上训练生成器和鉴别器，然后在训练过程中逐步添加层以增加分辨率。
- en: Let’s take a look at the concept of *progressive training* in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解*渐进式训练*的概念。
- en: Training Your Own ProGAN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的ProGAN
- en: There is an excellent tutorial by Bharath K on training your own ProGAN using
    Keras available on the [Paperspace blog](https://oreil.ly/b2CJm). Bear in mind
    that training a ProGAN to achieve the results from the paper requires a significant
    amount of computing power.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Bharath K在[Paperspace博客](https://oreil.ly/b2CJm)上提供了一个关于使用Keras训练自己的ProGAN的优秀教程。请记住，训练ProGAN以达到论文中的结果需要大量的计算能力。
- en: Progressive Training
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 渐进式训练
- en: As always with GANs, we build two independent networks, the generator and discriminator,
    with a fight for dominance taking place during the training process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与GANs一样，我们构建两个独立的网络，生成器和鉴别器，在训练过程中进行统治之争。
- en: In a normal GAN, the generator always outputs full-resolution images, even in
    the early stages of training. It is reasonable to think that this strategy might
    not be optimal—the generator might be slow to learn high-level structures in the
    early stages of training, because it is immediately operating over complex, high-resolution
    images. Wouldn’t it be better to first train a lightweight GAN to output accurate
    low-resolution images and then see if we can build on this to gradually increase
    the resolution?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通的GAN中，生成器总是输出全分辨率图像，即使在训练的早期阶段也是如此。可以合理地认为，这种策略可能不是最佳的——生成器可能在训练的早期阶段学习高级结构较慢，因为它立即在复杂的高分辨率图像上操作。首先训练一个轻量级的GAN以输出准确的低分辨率图像，然后逐渐增加分辨率，这样做会更好吗？
- en: This simple idea leads us to *progressive training*, one of the key contributions
    of the ProGAN paper. The ProGAN is trained in stages, starting with a training
    set that has been condensed down to 4 × 4–pixel images using interpolation, as
    shown in [Figure 10-1](Images/#condensed_images).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的想法引导我们进入*渐进式训练*，这是ProGAN论文的一个关键贡献。ProGAN分阶段训练，从一个已经通过插值压缩到4×4像素图像的训练集开始，如[图10-1](Images/#condensed_images)所示。
- en: '![](Images/gdl2_1001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1001.png)'
- en: Figure 10-1\. Images in the dataset can be compressed to lower resolution using
    interpolation
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。数据集中的图像可以使用插值压缩到较低分辨率
- en: We can then initially train the generator to transform a latent input noise
    vector <math alttext="z"><mi>z</mi></math> (say, of length 512) into an image
    of shape 4 × 4 × 3\. The matching discriminator will need to transform an input
    image of size 4 × 4 × 3 into a single scalar prediction. The network architectures
    for this first step are shown in [Figure 10-2](#progan_4).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以最初训练生成器，将潜在输入噪声向量<math alttext="z"><mi>z</mi></math>（比如长度为512）转换为形状为4×4×3的图像。匹配的鉴别器需要将大小为4×4×3的输入图像转换为单个标量预测。这第一步的网络架构如[图10-2](#progan_4)所示。
- en: The blue box in the generator represents the convolutional layer that converts
    the set of feature maps into an RGB image (`toRGB`), and the blue box in the discriminator
    represents the convolutional layer that converts the RGB images into a set of
    feature maps (`fromRGB`).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器中的蓝色框表示将特征图转换为RGB图像的卷积层（`toRGB`），鉴别器中的蓝色框表示将RGB图像转换为一组特征图的卷积层（`fromRGB`）。
- en: '![](Images/gdl2_1002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1002.png)'
- en: Figure 10-2\. The generator and discriminator architectures for the first stage
    of the ProGAN training process
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。ProGAN训练过程的第一阶段的生成器和鉴别器架构
- en: In the paper, the authors train this pair of networks until the discriminator
    has seen 800,000 real images. We now need to understand how the generator and
    discriminator are expanded to work with 8 × 8–pixel images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者训练这对网络，直到鉴别器看到了800,000张真实图像。现在我们需要了解如何扩展生成器和鉴别器以处理8×8像素图像。
- en: To expand the generator and discriminator, we need to blend in additional layers.
    This is managed in two phases, transition and stabilization, as shown in [Figure 10-3](#progan_training_gen).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展生成器和鉴别器，我们需要融入额外的层。这在两个阶段中进行，过渡和稳定，如[图10-3](#progan_training_gen)所示。
- en: '![](Images/gdl2_1003.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1003.png)'
- en: Figure 10-3\. The ProGAN generator training process, expanding the network from
    4 × 4 images to 8 × 8 (dotted lines represent the rest of the network, not shown)
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3。ProGAN生成器训练过程，将网络从4×4图像扩展到8×8（虚线代表网络的其余部分，未显示）
- en: Let’s first look at the generator. During the *transition phase*, new upsampling
    and convolutional layers are appended to the existing network, with a residual
    connection set up to maintain the output from the existing trained `toRGB` layer.
    Crucially, the new layers are initially masked using a parameter <math alttext="alpha"><mi>α</mi></math>
    that is gradually increased from 0 to 1 throughout the transition phase to allow
    more of the new `toRGB` output through and less of the existing `toRGB` layer.
    This is to avoid a *shock* to the network as the new layers take over.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下生成器。在*过渡阶段*中，新的上采样和卷积层被附加到现有网络中，建立了一个残差连接以保持现有训练过的`toRGB`层的输出。关键的是，新层最初使用一个参数<math
    alttext="alpha"><mi>α</mi></math>进行掩蔽，该参数在整个过渡阶段逐渐从0增加到1，以允许更多新的`toRGB`输出通过，减少现有的`toRGB`层。这是为了避免网络在新层接管时出现*冲击*。
- en: Eventually, there is no flow through the old `toRGB` layer and the network enters
    the *stabilization phase*—a further period of training where the network can fine-tune
    the output, without any flow through the old `toRGB` layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，旧的`toRGB`层不再有输出流，网络进入*稳定阶段*——进一步的训练期间，网络可以微调输出，而不经过旧的`toRGB`层。
- en: The discriminator uses a similar process, as shown in [Figure 10-4](#progan_training_dis).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器使用类似的过程，如[图10-4](#progan_training_dis)所示。
- en: '![](Images/gdl2_1004.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1004.png)'
- en: Figure 10-4\. The ProGAN discriminator training process, expanding the network
    from 4 × 4 images to 8 × 8 (dotted lines represent the rest of the network, not
    shown)
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4。ProGAN鉴别器训练过程，将网络从4×4图像扩展到8×8（虚线代表网络的其余部分，未显示）
- en: Here, we need to blend in additional downscaling and convolutional layers. Again,
    the layers are injected into the network—this time at the start of the network,
    just after the input image. The existing `fromRGB` layer is connected via a residual
    connection and gradually phased out as the new layers take over during the transition
    phase. The stabilization phase allows the discriminator to fine-tune using the
    new layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要融入额外的降采样和卷积层。同样，这些层被注入到网络中——这次是在网络的开始部分，就在输入图像之后。现有的`fromRGB`层通过残差连接连接，并在过渡阶段逐渐淡出，随着新层在过渡阶段接管时逐渐淡出。稳定阶段允许鉴别器使用新层进行微调。
- en: All transition and stabilization phases last until the discriminator has been
    shown 800,000 real images. Note that even through the network is trained progressively,
    no layers are *frozen*. Throughout the training process, all layers remain fully
    trainable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有过渡和稳定阶段持续到鉴别器已经看到了800,000张真实图像。请注意，即使网络是渐进训练的，也没有层被*冻结*。在整个训练过程中，所有层都保持完全可训练。
- en: This process continues, growing the GAN from 4 × 4 images to 8 × 8, then 16
    × 16, 32 × 32, and so on, until it reaches full resolution (1,024 × 1,024), as
    shown in [Figure 10-5](#progan).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程继续进行，将GAN从4×4图像扩展到8×8，然后16×16，32×32，依此类推，直到达到完整分辨率（1,024×1,024），如[图10-5](#progan)所示。
- en: '![](Images/gdl2_1005.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1005.png)'
- en: 'Figure 10-5\. The ProGAN training mechanism, and some example generated faces
    (source: [Karras et al., 2017](https://arxiv.org/abs/1710.10196))'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。ProGAN训练机制，以及一些示例生成的人脸（来源：[Karras等人，2017](https://arxiv.org/abs/1710.10196)）
- en: The overall structure of the generator and discriminator after the full progressive
    training process is complete is shown in [Figure 10-6](#progan_network_diagram).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 完整渐进训练过程完成后，生成器和鉴别器的整体结构如[图10-6](#progan_network_diagram)所示。
- en: '![](Images/gdl2_1006.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1006.png)'
- en: 'Figure 10-6\. The ProGAN generator and discriminator used to generate 1,024
    × 1,024–pixel CelebA faces (source: [Karras et al., 2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 用于生成1,024×1,024像素CelebA面孔的ProGAN生成器和鉴别器的结构（来源：[Karras等人，2018](https://arxiv.org/abs/1812.04948))
- en: The paper also makes several other important contributions, namely minibatch
    standard deviation, equalized learning rates, and pixelwise normalization, which
    are described briefly in the following sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文还提出了其他几个重要贡献，即小批量标准差、均衡学习率和像素级归一化，以下部分将简要描述。
- en: Minibatch standard deviation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小批量标准差
- en: The *minibatch standard deviation* layer is an extra layer in the discriminator
    that appends the standard deviation of the feature values, averaged across all
    pixels and across the minibatch as an additional (constant) feature. This helps
    to ensure the generator creates more variety in its output—if variety is low across
    the minibatch, then the standard deviation will be small, and the discriminator
    can use this feature to distinguish the fake batches from the real batches! Therefore,
    the generator is incentivized to ensure it generates a similar amount of variety
    as is present in the real training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*小批量标准差*层是鉴别器中的额外层，附加了特征值的标准差，跨所有像素和整个小批量平均作为额外（常数）特征。这有助于确保生成器在输出中创建更多的变化——如果整个小批量中的变化较小，则标准差将很小，鉴别器可以使用此特征来区分假批次和真实批次！因此，生成器被激励确保它生成与真实训练数据中存在的变化量相似的数量。'
- en: Equalized learning rates
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均衡学习率
- en: All dense and convolutional layers in ProGAN use *equalized learning rates*.
    Usually, weights in a neural network are initialized using a method such as *He
    initialization*—a Gaussian distribution where the standard deviation is scaled
    to be inversely proportional to the square root of the number of inputs to the
    layer. This way, layers with a greater number of inputs will be initialized with
    weights that have a smaller deviation from zero, which generally improves the
    stability of the training process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ProGAN中的所有全连接和卷积层都使用*均衡学习率*。通常，神经网络中的权重是使用诸如*He初始化*之类的方法进行初始化的——这是一个高斯分布，其标准差被缩放为与层的输入数量的平方根成反比。这样，具有更多输入的层将使用与零的偏差较小的权重进行初始化，通常会提高训练过程的稳定性。
- en: The authors of the ProGAN paper found that this was causing problems when used
    in combination with modern optimizers such as Adam or RMSProp. These methods normalize
    the gradient update for each weight, so that the size of the update is independent
    of the scale (magnitude) of the weight. However, this means that weights with
    a larger dynamic range (i.e., layers with fewer inputs) will take comparatively
    longer to adjust than weights with a smaller dynamic range (i.e., layers with
    more inputs). It was found that this causes an imbalance between the speed of
    training of the different layers of the generator and discriminator in ProGAN,
    so they used *equalized learning rates* to solve this problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ProGAN论文的作者发现，当与Adam或RMSProp等现代优化器结合使用时，这会导致问题。这些方法会对每个权重的梯度更新进行归一化，使得更新的大小与权重的规模（幅度）无关。然而，这意味着具有较大动态范围（即具有较少输入的层）的权重将比具有较小动态范围（即具有更多输入的层）的权重需要更长时间来调整。发现这导致了ProGAN中生成器和鉴别器不同层的训练速度之间的不平衡，因此他们使用*均衡学习率*来解决这个问题。
- en: In ProGAN, weights are initialized using a simple standard Gaussian, regardless
    of the number of inputs to the layer. The normalization is applied dynamically,
    as part of the call to the layer, rather than only at initialization. This way,
    the optimizer sees each weight as having approximately the same dynamic range,
    so it applies the same learning rate. It is only when the layer is called that
    the weight is scaled by the factor from the He initializer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在ProGAN中，权重使用简单的标准高斯进行初始化，而不管层的输入数量如何。归一化是动态应用的，作为对层的调用的一部分，而不仅仅是在初始化时。这样，优化器会将每个权重视为具有大致相同的动态范围，因此会应用相同的学习率。只有在调用层时，权重才会按照He初始化器的因子进行缩放。
- en: Pixelwise normalization
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 像素级归一化
- en: Lastly, in ProGAN *pixelwise normalization* is used in the generator, rather
    than batch normalization. This normalizes the feature vector in each pixel to
    a unit length and helps to prevent the signal from spiraling out of control as
    it propagates through the network. The pixelwise normalization layer has no trainable
    weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在ProGAN中，生成器中使用*像素级归一化*，而不是批归一化。这将每个像素中的特征向量归一化为单位长度，并有助于防止信号在网络中传播时失控。像素级归一化层没有可训练的权重。
- en: Outputs
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出
- en: In addition to the CelebA dataset, ProGAN was also applied to images from the
    Large-scale Scene Understanding (LSUN) dataset with excellent results, as shown
    in [Figure 10-7](#progan_examples). This demonstrated the power of ProGAN over
    earlier GAN architectures and paved the way for future iterations such as StyleGAN
    and StyleGAN2, which we shall explore in the next sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除CelebA数据集外，ProGAN还应用于大规模场景理解（LSUN）数据集的图像，并取得了出色的结果，如[图10-7](#progan_examples)所示。这展示了ProGAN相对于早期GAN架构的强大之处，并为未来的迭代（如StyleGAN和StyleGAN2）铺平了道路，我们将在接下来的部分中探讨。
- en: '![](Images/gdl2_1007.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1007.png)'
- en: 'Figure 10-7\. Generated examples from a ProGAN trained progressively on the
    LSUN dataset at 256 × 256 resolution (source: [Karras et al., 2017](https://arxiv.org/abs/1710.10196))'
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. 在LSUN数据集上渐进训练的ProGAN生成的示例，分辨率为256×256（来源：[Karras等人，2017](https://arxiv.org/abs/1710.10196)）
- en: StyleGAN
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StyleGAN
- en: StyleGAN^([3](ch10.xhtml#idm45387005140128)) is a GAN architecture from 2018
    that builds on the earlier ideas in the ProGAN paper. In fact, the discriminator
    is identical; only the generator is changed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN^([3](ch10.xhtml#idm45387005140128))是2018年的一个GAN架构，建立在ProGAN论文中的早期思想基础上。实际上，鉴别器是相同的；只有生成器被改变。
- en: Often when training GANs it is difficult to separate out vectors in the latent
    space corresponding to high-level attributes—they are frequently *entangled*,
    meaning that adjusting an image in the latent space to give a face more freckles,
    for example, might also inadvertently change the background color. While ProGAN
    generates fantastically realistic images, it is no exception to this general rule.
    We would ideally like to have full control of the style of the image, and this
    requires a disentangled separation of features in the latent space.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在训练GAN时，很难将潜在空间中对应于高级属性的向量分离出来——它们经常是*纠缠在一起*，这意味着调整潜在空间中的图像以使脸部更多雀斑，例如，可能也会无意中改变背景颜色。虽然ProGAN生成了极其逼真的图像，但它也不例外。我们理想情况下希望完全控制图像的风格，这需要在潜在空间中对特征进行分离。
- en: 'StyleGAN achieves this by explicitly injecting style vectors into the network
    at different points: some that control high-level features (e.g., face orientation)
    and some that control low-level details (e.g., the way the hair falls across the
    forehead).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN通过在网络的不同点显式注入风格向量来实现这一点：一些控制高级特征（例如，面部方向）的向量，一些控制低级细节（例如，头发如何落在额头上）的向量。
- en: The overall architecture of the StyleGAN generator is shown in [Figure 10-8](#stylegan_arch).
    Let’s walk through this architecture step by step, starting with the mapping network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN生成器的整体架构如[图10-8](#stylegan_arch)所示。让我们逐步走过这个架构，从映射网络开始。
- en: '![](Images/gdl2_1008.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1008.png)'
- en: 'Figure 10-8\. The StyleGAN generator architecture (source: [Karras et al.,
    2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8。StyleGAN生成器架构（来源：[Karras et al., 2018](https://arxiv.org/abs/1812.04948)）
- en: Training Your Own StyleGAN
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的StyleGAN
- en: There is an excellent tutorial by Soon-Yau Cheong on training your own StyleGAN
    using Keras available on the [Keras website](https://oreil.ly/MooSe). Bear in
    mind that training a StyleGAN to achieve the results from the paper requires a
    significant amount of computing power.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Soon-Yau Cheong在[Keras网站](https://oreil.ly/MooSe)上提供了一个关于使用Keras训练自己的StyleGAN的优秀教程。请记住，要实现论文中的结果，训练StyleGAN需要大量的计算资源。
- en: The Mapping Network
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射网络
- en: The *mapping network* <math alttext="f"><mi>f</mi></math> is a simple feed-forward
    network that converts the input noise <math alttext="bold z element-of script
    upper Z"><mrow><mi>𝐳</mi> <mo>∈</mo> <mi>𝒵</mi></mrow></math> into a different
    latent space <math alttext="bold w element-of script upper W"><mrow><mi>𝐰</mi>
    <mo>∈</mo> <mi>𝒲</mi></mrow></math> . This gives the generator the opportunity
    to disentangle the noisy input vector into distinct factors of variation, which
    can be easily picked up by the downstream style-generating layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*映射网络* <math alttext="f"><mi>f</mi></math> 是一个简单的前馈网络，将输入噪声 <math alttext="bold
    z element-of script upper Z"><mrow><mi>𝐳</mi> <mo>∈</mo> <mi>𝒵</mi></mrow></math>
    转换为不同的潜在空间 <math alttext="bold w element-of script upper W"><mrow><mi>𝐰</mi> <mo>∈</mo>
    <mi>𝒲</mi></mrow></math>。这使得生成器有机会将嘈杂的输入向量分解为不同的变化因素，这些因素可以被下游的风格生成层轻松捕捉到。'
- en: The point of doing this is to separate out the process of choosing a style for
    the image (the mapping network) from the generation of an image with a given style
    (the synthesis network).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是将图像的风格选择过程（映射网络）与生成具有给定风格的图像的过程（合成网络）分开。
- en: The Synthesis Network
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成网络
- en: 'The synthesis network is the generator of the actual image with a given style,
    as provided by the mapping network. As can be seen from [Figure 10-8](#stylegan_arch),
    the style vector <math alttext="bold w"><mi>𝐰</mi></math> is injected into the
    synthesis network at different points, each time via a differently densely connected
    layer <math alttext="upper A Subscript i"><msub><mi>A</mi> <mi>i</mi></msub></math>
    , which generates two vectors: a bias vector <math alttext="bold y Subscript b
    comma i"><msub><mi>𝐲</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    and a scaling vector <math alttext="bold y Subscript s comma i"><msub><mi>𝐲</mi>
    <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math> . These vectors define
    the specific style that should be injected at this point in the network—that is,
    they tell the synthesis network how to adjust the feature maps to move the generated
    image in the direction of the specified style.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 合成网络是生成具有给定风格的实际图像的生成器，由映射网络提供。如[图10-8](#stylegan_arch)所示，风格向量 <math alttext="bold
    w"><mi>𝐰</mi></math> 被注入到合成网络的不同点，每次通过不同的密集连接层 <math alttext="upper A Subscript
    i"><msub><mi>A</mi> <mi>i</mi></msub></math>，生成两个向量：一个偏置向量 <math alttext="bold
    y Subscript b comma i"><msub><mi>𝐲</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    和一个缩放向量 <math alttext="bold y Subscript s comma i"><msub><mi>𝐲</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math>。这些向量定义了应该在网络中的这一点注入的特定风格，也就是告诉合成网络如何调整特征图以使生成的图像朝着指定的风格方向移动。
- en: This adjustment is achieved through *adaptive instance normalization* (AdaIN)
    layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*自适应实例归一化*（AdaIN）层实现这种调整。
- en: Adaptive instance normalization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应实例归一化
- en: 'An AdaIN layer is a type of neural network layer that adjusts the mean and
    variance of each feature map <math alttext="bold x Subscript i"><msub><mi>𝐱</mi>
    <mi>i</mi></msub></math> with a reference style bias <math alttext="bold y Subscript
    b comma i"><msub><mi>𝐲</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    and scale <math alttext="bold y Subscript s comma i"><msub><mi>𝐲</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    , respectively.^([4](ch10.xhtml#idm45387005090240)) Both vectors are of length
    equal to the number of channels output from the preceding convolutional layer
    in the synthesis network. The equation for adaptive instance normalization is
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: AdaIN层是一种神经网络层，通过参考风格偏差<math alttext="bold y Subscript b comma i"><msub><mi>𝐲</mi>
    <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>和比例<math alttext="bold
    y Subscript s comma i"><msub><mi>𝐲</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math>调整每个特征图<math
    alttext="bold x Subscript i"><msub><mi>𝐱</mi> <mi>i</mi></msub></math>的均值和方差。这两个向量的长度等于合成网络中前一卷积层输出的通道数。自适应实例归一化的方程如下：
- en: <math alttext="StartLayout 1st Row  AdaIN left-parenthesis bold x Subscript
    i Baseline comma bold y right-parenthesis equals bold y Subscript s comma i Baseline
    StartFraction bold x Subscript i Baseline minus mu left-parenthesis bold x Subscript
    i Baseline right-parenthesis Over sigma left-parenthesis bold x Subscript i Baseline
    right-parenthesis EndFraction plus bold y Subscript b comma i Baseline EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>AdaIN</mtext>
    <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>i</mi></msub> <mo>,</mo> <mi>𝐲</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝐲</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mfrac><mrow><msub><mi>𝐱</mi> <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mrow><mo>(</mo><msub><mi>𝐱</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>σ</mi><mo>(</mo><msub><mi>𝐱</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac> <mo>+</mo> <msub><mi>𝐲</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  AdaIN left-parenthesis bold x Subscript
    i Baseline comma bold y right-parenthesis equals bold y Subscript s comma i Baseline
    StartFraction bold x Subscript i Baseline minus mu left-parenthesis bold x Subscript
    i Baseline right-parenthesis Over sigma left-parenthesis bold x Subscript i Baseline
    right-parenthesis EndFraction plus bold y Subscript b comma i Baseline EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>AdaIN</mtext>
    <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>i</mi></msub> <mo>,</mo> <mi>𝐲</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝐲</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mfrac><mrow><msub><mi>𝐱</mi> <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mrow><mo>(</mo><msub><mi>𝐱</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>σ</mi><mo>(</mo><msub><mi>𝐱</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac> <mo>+</mo> <msub><mi>𝐲</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
- en: The adaptive instance normalization layers ensure that the style vectors that
    are injected into each layer only affect features at that layer, by preventing
    any style information from leaking through between layers. The authors show that
    this results in the latent vectors <math alttext="bold w"><mi>𝐰</mi></math> being
    significantly more disentangled than the original <math alttext="bold z"><mi>𝐳</mi></math>
    vectors.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应实例归一化层确保注入到每一层的风格向量只影响该层的特征，防止任何风格信息在层之间泄漏。作者表明，这导致潜在向量<math alttext="bold
    w"><mi>𝐰</mi></math>比原始<math alttext="bold z"><mi>𝐳</mi></math>向量更具解耦性。
- en: Since the synthesis network is based on the ProGAN architecture, it is trained
    progressively. The style vectors at earlier layers in the synthesis network (when
    the resolution of the image is lowest—4 × 4, 8 × 8) will affect coarser features
    than those later in the network (64 × 64 to 1,024 × 1,024–pixel resolution). This
    means that not only do we have complete control over the generated image through
    the latent vector <math alttext="bold w"><mi>𝐰</mi></math> , but we can also switch
    the <math alttext="bold w"><mi>𝐰</mi></math> vector at different points in the
    synthesis network to change the style at a variety of levels of detail.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于合成网络基于ProGAN架构，因此是逐步训练的。在合成网络中较早层的风格向量（当图像分辨率最低时为4×4、8×8）将影响比网络后期（64×64到1,024×1,024像素分辨率）更粗糙的特征。这意味着我们不仅可以通过潜在向量<math
    alttext="bold w"><mi>𝐰</mi></math>完全控制生成的图像，还可以在合成网络的不同点切换<math alttext="bold
    w"><mi>𝐰</mi></math>向量以改变各种细节级别的风格。
- en: Style mixing
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风格混合
- en: The authors use a trick known as *style mixing* to ensure that the generator
    cannot utilize correlations between adjacent styles during training (i.e., the
    styles injected at each layer are as disentangled as possible). Instead of sampling
    only a single latent vector <math alttext="bold z"><mi>𝐳</mi></math> , two are
    sampled <math alttext="left-parenthesis bold z bold 1 comma bold z bold 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>𝐳</mi> <mn mathvariant="bold">1</mn></msub> <mo>,</mo> <msub><mi>𝐳</mi>
    <mn mathvariant="bold">2</mn></msub> <mo>)</mo></mrow></math> , corresponding
    to two style vectors <math alttext="left-parenthesis bold w bold 1 comma bold
    w bold 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>𝐰</mi> <mn mathvariant="bold">1</mn></msub>
    <mo>,</mo> <msub><mi>𝐰</mi> <mn mathvariant="bold">2</mn></msub> <mo>)</mo></mrow></math>
    . Then, at each layer, either <math alttext="left-parenthesis bold w bold 1"><mrow><mo>(</mo>
    <msub><mi>𝐰</mi> <mn mathvariant="bold">1</mn></msub></mrow></math> or <math alttext="bold
    w bold 2 right-parenthesis"><mrow><msub><mi>𝐰</mi> <mn mathvariant="bold">2</mn></msub>
    <mrow><mo>)</mo></mrow></mrow></math> is chosen at random, to break any possible
    correlation between the vectors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用一种称为*风格混合*的技巧，确保生成器在训练过程中不能利用相邻风格之间的相关性（即，每层注入的风格尽可能解耦）。不仅仅是采样单个潜在向量<math
    alttext="bold z"><mi>𝐳</mi></math>，而是采样两个<math alttext="left-parenthesis bold
    z bold 1 comma bold z bold 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>𝐳</mi>
    <mn mathvariant="bold">1</mn></msub> <mo>,</mo> <msub><mi>𝐳</mi> <mn mathvariant="bold">2</mn></msub>
    <mo>)</mo></mrow></math>，对应两个风格向量<math alttext="left-parenthesis bold w bold 1
    comma bold w bold 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>𝐰</mi> <mn mathvariant="bold">1</mn></msub>
    <mo>,</mo> <msub><mi>𝐰</mi> <mn mathvariant="bold">2</mn></msub> <mo>)</mo></mrow></math>。然后，在每一层，随机选择<math
    alttext="left-parenthesis bold w bold 1"><mrow><mo>(</mo> <msub><mi>𝐰</mi> <mn
    mathvariant="bold">1</mn></msub></mrow></math>或<math alttext="bold w bold 2 right-parenthesis"><mrow><msub><mi>𝐰</mi>
    <mn mathvariant="bold">2</mn></msub> <mrow><mo>)</mo></mrow></mrow></math>，以打破可能存在的向量之间的任何相关性。
- en: Stochastic variation
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机变化
- en: The synthesizer network adds noise (passed through a learned broadcasting layer
    <math alttext="upper B"><mi>B</mi></math> ) after each convolution to account
    for stochastic details such as the placement of individual hairs, or the background
    behind the face. Again, the depth at which the noise is injected affects the coarseness
    of the impact on the image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 合成器网络在每个卷积后添加噪音（通过一个学习的广播层<math alttext="upper B"><mi>B</mi></math>传递），以考虑诸如单个头发的放置或面部背后的背景等随机细节。再次强调，噪音注入的深度会影响对图像的影响粗糙程度。
- en: This also means that the initial input to the synthesis network can simply be
    a learned constant, rather than additional noise. There is enough stochasticity
    already present in the style inputs and the noise inputs to generate sufficient
    variation in the images.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着合成网络的初始输入可以简单地是一个学习到的常量，而不是额外的噪音。在风格输入和噪音输入中已经存在足够的随机性，以生成图像的足够变化。
- en: Outputs from StyleGAN
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGAN的输出
- en: '[Figure 10-9](#stylegan_w) shows StyleGAN in action.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-9](#stylegan_w)展示了StyleGAN的工作原理。'
- en: '![](Images/gdl2_1009.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1009.png)'
- en: 'Figure 10-9\. Merging styles between two generated images at different levels
    of detail (source: [Karras et al., 2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-9. 在不同细节级别上合并两个生成图像的风格（来源：[Karras等人，2018](https://arxiv.org/abs/1812.04948)）
- en: Here, two images, source A and source B, are generated from two different <math
    alttext="bold w"><mi>𝐰</mi></math> vectors. To generate a merged image, the source
    A <math alttext="bold w"><mi>𝐰</mi></math> vector is passed through the synthesis
    network but, at some point, switched for the source B <math alttext="bold w"><mi>𝐰</mi></math>
    vector. If this switch happens early on (4 × 4 or 8 × 8 resolution), coarse styles
    such as pose, face shape, and glasses from source B are carried across onto source
    A. However, if the switch happens later, only fine-grained detail is carried across
    from source B, such as colors and microstructure of the face, while the coarse
    features from source A are preserved.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，两个图像，源 A 和源 B，是从两个不同的 <math alttext="bold w"><mi>𝐰</mi></math> 向量生成的。为了生成一个合并的图像，源
    A 的 <math alttext="bold w"><mi>𝐰</mi></math> 向量通过合成网络，但在某个时刻，被切换为源 B 的 <math alttext="bold
    w"><mi>𝐰</mi></math> 向量。如果这个切换发生得很早（4 × 4 或 8 × 8 分辨率），则从源 B 传递到源 A 的是粗略的风格，如姿势、脸型和眼镜。然而，如果切换发生得更晚，只有来自源
    B 的细粒度细节被传递，比如脸部的颜色和微结构，而来自源 A 的粗略特征被保留。
- en: StyleGAN2
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StyleGAN2
- en: The final contribution in this chain of important GAN papers is StyleGAN2.^([5](ch10.xhtml#idm45387005019232))
    This builds further upon the StyleGAN architecture, with some key changes that
    improve the quality of the generated output. In particular, StyleGAN2 generations
    do not suffer as greatly from *artifacts*—water droplet–like areas of the image
    that were found to be caused by the adaptive instance normalization layers in
    StyleGAN, as shown in [Figure 10-10](#artifacts_stylegan).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列重要的 GAN 论文中的最终贡献是 StyleGAN2。这进一步构建在 StyleGAN 架构之上，通过一些关键改变提高了生成输出的质量。特别是，StyleGAN2
    生成不会像 *伪影* 那样受到严重影响——在 StyleGAN 中发现的图像中的水滴状区域，这些伪影是由于 StyleGAN 中的自适应实例归一化层引起的，如
    [图 10-10](#artifacts_stylegan) 所示。
- en: '![](Images/gdl2_1010.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1010.png)'
- en: 'Figure 10-10\. An artifact in a StyleGAN-generated image of a face (source:
    [Karras et al., 2019](https://arxiv.org/abs/1912.04958))'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 一个 StyleGAN 生成的人脸图像中的伪影（来源：[Karras et al., 2019](https://arxiv.org/abs/1912.04958)）
- en: Both the generator and the discriminator in StyleGAN2 are different from the
    StyleGAN. In the next sections we will explore the key differences between the
    architectures.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN2 中的生成器和鉴别器与 StyleGAN 不同。在接下来的章节中，我们将探讨这两种架构之间的关键区别。
- en: Training Your Own StyleGAN2
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的 StyleGAN2
- en: The official code for training your own StyleGAN using TensorFlow is available
    on [GitHub](https://oreil.ly/alB6w). Bear in mind that training a StyleGAN2 to
    achieve the results from the paper requires a significant amount of computing
    power.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 训练您自己的 StyleGAN 的官方代码可在 [GitHub](https://oreil.ly/alB6w) 上找到。请注意，为了实现论文中的结果，训练一个
    StyleGAN2 需要大量的计算资源。
- en: Weight Modulation and Demodulation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重调制和去调制
- en: The artifact problem is solved by removing the AdaIN layers in the generator
    and replacing them with weight modulation and demodulation steps, as shown in
    [Figure 10-11](#stylegan2_styleblock). <math alttext="bold w"><mi>𝐰</mi></math>
    represents the weights of the convolutional layer, which are directly updated
    by the modulation and demodulation steps in StyleGAN2 at runtime. In comparison,
    the AdaIN layers of StyleGAN operate on the image tensor as it flows through the
    network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除生成器中的 AdaIN 层并将其替换为权重调制和去调制步骤，解决了伪影问题，如 [图 10-11](#stylegan2_styleblock)
    所示。 <math alttext="bold w"><mi>𝐰</mi></math> 代表卷积层的权重，在 StyleGAN2 中通过调制和去调制步骤直接在运行时更新。相比之下，StyleGAN
    的 AdaIN 层在图像张量通过网络时操作。
- en: The AdaIN layer in StyleGAN is simply an instance normalization followed by
    style modulation (scaling and bias). The idea in StyleGAN2 is to apply style modulation
    and normalization (demodulation) directly to the weights of the convolutional
    layers at runtime, rather than the output from the convolutional layers, as shown
    in [Figure 10-11](#stylegan2_styleblock). The authors show how this removes the
    artifact issue while retaining control of the image style.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN 中的 AdaIN 层只是一个实例归一化，后面跟着样式调制（缩放和偏置）。StyleGAN2 中的想法是在运行时直接将样式调制和归一化（去调制）应用于卷积层的权重，而不是卷积层的输出，如
    [图 10-11](#stylegan2_styleblock) 所示。作者展示了这如何消除了伪影问题，同时保持对图像样式的控制。
- en: '![](Images/gdl2_1011.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1011.png)'
- en: Figure 10-11\. A comparison between the StyleGAN and StyleGAN2 style blocks
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. StyleGAN 和 StyleGAN2 样式块之间的比较
- en: 'In StyleGAN2, each dense layer <math alttext="upper A"><mi>A</mi></math> outputs
    a single style vector <math alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>
    , where <math alttext="i"><mi>i</mi></math> indexes the number of input channels
    in the corresponding convolutional layer. This style vector is then applied to
    the weights of the convolutional layer as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 StyleGAN2 中，每个密集层 <math alttext="upper A"><mi>A</mi></math> 输出一个单一的样式向量 <math
    alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>，其中 <math alttext="i"><mi>i</mi></math>
    索引了相应卷积层中的输入通道数。然后将这个样式向量应用于卷积层的权重，如下所示：
- en: <math alttext="w Subscript i comma j comma k Superscript prime Baseline equals
    s Subscript i Baseline dot w Subscript i comma j comma k" display="block"><mrow><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <mo>=</mo> <msub><mi>s</mi> <mi>i</mi></msub> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i comma j comma k Superscript prime Baseline equals
    s Subscript i Baseline dot w Subscript i comma j comma k" display="block"><mrow><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <mo>=</mo> <msub><mi>s</mi> <mi>i</mi></msub> <mo>·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></math>
- en: Here, <math alttext="j"><mi>j</mi></math> indexes the output channels of the
    layer and <math alttext="k"><mi>k</mi></math> indexes the spatial dimensions.
    This is the *modulation* step of the process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="j"><mi>j</mi></math> 索引了层的输出通道，<math alttext="k"><mi>k</mi></math>
    索引了空间维度。这是过程的 *调制* 步骤。
- en: 'Then, we need to normalize the weights so that they again have a unit standard
    deviation, to ensure stability in the training process. This is the *demodulation*
    step:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要归一化权重，使它们再次具有单位标准差，以确保训练过程的稳定性。这是 *去调制* 步骤：
- en: <math alttext="w Subscript i comma j comma k Superscript double-prime Baseline
    equals StartFraction w Subscript i comma j comma k Superscript prime Baseline
    Over StartRoot sigma-summation Underscript i comma k Endscripts w Subscript i
    comma j comma k Superscript prime Baseline squared plus epsilon EndRoot EndFraction"
    display="block"><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mrow><mo>'</mo><mo>'</mo></mrow></msup></msubsup> <mo>=</mo> <mfrac><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <msqrt><mrow><munder><mo>∑</mo> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></munder>
    <msup><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mo>'</mo></msup></msubsup></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mrow></math>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i comma j comma k Superscript double-prime Baseline
    equals StartFraction w Subscript i comma j comma k Superscript prime Baseline
    Over StartRoot sigma-summation Underscript i comma k Endscripts w Subscript i
    comma j comma k Superscript prime Baseline squared plus epsilon EndRoot EndFraction"
    display="block"><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mrow><mo>'</mo><mo>'</mo></mrow></msup></msubsup> <mo>=</mo> <mfrac><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <msqrt><mrow><munder><mo>∑</mo> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></munder>
    <msup><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mo>'</mo></msup></msubsup></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mrow></math>
- en: where <math alttext="epsilon"><mi>ϵ</mi></math> is a small constant value that
    prevents division by zero.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="epsilon"><mi>ϵ</mi></math> 是一个小的常数值，用于防止除以零。
- en: In the paper, the authors show how this simple change is enough to prevent water-droplet
    artifacts, while retaining control over the generated images via the style vectors
    and ensuring the quality of the output remains high.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者展示了这个简单的改变足以防止水滴状伪影，同时通过样式向量保持对生成图像的控制，并确保输出的质量保持高水平。
- en: Path Length Regularization
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路径长度正则化
- en: Another change made to the StyleGAN architecture is the inclusion of an additional
    penalty term in the loss function—*this is known as path length regularization*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN架构的另一个变化是在损失函数中包含了额外的惩罚项——*这被称为路径长度正则化*。
- en: We would like the latent space to be as smooth and uniform as possible, so that
    a fixed-size step in the latent space in any direction results in a fixed-magnitude
    change in the image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望潜在空间尽可能平滑和均匀，这样在任何方向上潜在空间中的固定大小步长会导致图像的固定幅度变化。
- en: 'To encourage this property, StyleGAN2 aims to minimize the following term,
    alongside the usual Wasserstein loss with gradient penalty:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励这一属性，StyleGAN2旨在最小化以下术语，以及通常的Wasserstein损失和梯度惩罚：
- en: <math alttext="double-struck upper E Subscript w comma y Baseline left-parenthesis
    parallel-to bold upper J Subscript w Superscript down-tack Baseline y parallel-to
    Subscript 2 Baseline minus a right-parenthesis squared" display="block"><mrow><msub><mi>𝔼</mi>
    <mrow><mi>𝑤</mi><mo>,</mo><mi>𝑦</mi></mrow></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mfenced separators="" open="∥" close="∥"><msubsup><mi>𝐉</mi>
    <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced> <mn>2</mn></msub> <mo>-</mo><mi>a</mi></mfenced>
    <mn>2</mn></msup></mrow></math>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E Subscript w comma y Baseline left-parenthesis
    parallel-to bold upper J Subscript w Superscript down-tack Baseline y parallel-to
    Subscript 2 Baseline minus a right-parenthesis squared" display="block"><mrow><msub><mi>𝔼</mi>
    <mrow><mi>𝑤</mi><mo>,</mo><mi>𝑦</mi></mrow></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mfenced separators="" open="∥" close="∥"><msubsup><mi>𝐉</mi>
    <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced> <mn>2</mn></msub> <mo>-</mo><mi>a</mi></mfenced>
    <mn>2</mn></msup></mrow></math>
- en: Here, <math alttext="w"><mi>𝑤</mi></math> is a set of style vectors created
    by the mapping network, <math alttext="y"><mi>𝑦</mi></math> is a set of noisy
    images drawn from <math alttext="script upper N left-parenthesis 0 comma bold
    upper I right-parenthesis"><mrow><mi>𝒩</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>𝐈</mi>
    <mo>)</mo></mrow></math> , and <math alttext="bold upper J Subscript w Baseline
    equals StartFraction normal partial-differential g Over normal partial-differential
    w EndFraction"><mrow><msub><mi>𝐉</mi> <mi>𝑤</mi></msub> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>g</mi></mrow>
    <mrow><mi>∂</mi><mi>𝑤</mi></mrow></mfrac></mrow></math> is the Jacobian of the
    generator network with respect to the style vectors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="w"><mi>𝑤</mi></math>是由映射网络创建的一组样式向量，<math alttext="y"><mi>𝑦</mi></math>是从<math
    alttext="script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>𝒩</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>𝐈</mi> <mo>)</mo></mrow></math>中绘制的一组嘈杂图像，<math
    alttext="bold upper J Subscript w Baseline equals StartFraction normal partial-differential
    g Over normal partial-differential w EndFraction"><mrow><msub><mi>𝐉</mi> <mi>𝑤</mi></msub>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>g</mi></mrow> <mrow><mi>∂</mi><mi>𝑤</mi></mrow></mfrac></mrow></math>是生成器网络相对于样式向量的雅可比矩阵。
- en: The term <math alttext="parallel-to bold upper J Subscript w Superscript down-tack
    Baseline y parallel-to Subscript 2"><msub><mfenced separators="" open="∥" close="∥"><msubsup><mi>𝐉</mi>
    <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced> <mn>2</mn></msub></math>
    measures the magnitude of the images <math alttext="y"><mi>𝑦</mi></math> after
    transformation by the gradients given in the Jacobian. We want this to be close
    to a constant <math alttext="a"><mi>a</mi></math> , which is calculated dynamically
    as the exponential moving average of <math alttext="parallel-to bold upper J Subscript
    w Superscript down-tack Baseline y parallel-to Subscript 2"><msub><mfenced separators=""
    open="∥" close="∥"><msubsup><mi>𝐉</mi> <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced>
    <mn>2</mn></msub></math> as the training progresses.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 术语<math alttext="parallel-to bold upper J Subscript w Superscript down-tack
    Baseline y parallel-to Subscript 2"><msub><mfenced separators="" open="∥" close="∥"><msubsup><mi>𝐉</mi>
    <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced> <mn>2</mn></msub></math>测量了经雅可比矩阵给出的梯度变换后图像<math
    alttext="y"><mi>𝑦</mi></math>的幅度。我们希望这个值接近一个常数<math alttext="a"><mi>a</mi></math>，这个常数是动态计算的，作为训练进行时<math
    alttext="parallel-to bold upper J Subscript w Superscript down-tack Baseline y
    parallel-to Subscript 2"><msub><mfenced separators="" open="∥" close="∥"><msubsup><mi>𝐉</mi>
    <mi>𝑤</mi> <mi>⊤</mi></msubsup> <mi>𝑦</mi></mfenced> <mn>2</mn></msub></math>的指数移动平均值。
- en: The authors find that this additional term makes exploring the latent space
    more reliable and consistent. Moreover, the regularization terms in the loss function
    are only applied once every 16 minibatches, for efficiency. This technique, called
    *lazy regularization*, does not cause a measurable drop in performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现，这个额外的术语使探索潜在空间更可靠和一致。此外，损失函数中的正则化项仅在每16个小批次中应用一次，以提高效率。这种技术称为*懒惰正则化*，不会导致性能的明显下降。
- en: No Progressive Growing
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有渐进增长
- en: Another major update is in how StyleGAN2 is trained. Rather than adopting the
    usual progressive training mechanism, StyleGAN2 utilizes skip connections in the
    generator and residual connections in the discriminator to train the entire network
    as one. It no longer requires different resolutions to be trained independently
    and blended as part of the training process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN2训练的另一个重大更新是在训练方式上。StyleGAN2不再采用通常的渐进式训练机制，而是利用生成器中的跳过连接和鉴别器中的残差连接来将整个网络作为一个整体进行训练。它不再需要独立训练不同分辨率，并将其作为训练过程的一部分混合。
- en: '[Figure 10-12](#stylegan2_gen_dis) shows the generator and discriminator blocks
    in StyleGAN2.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-12](#stylegan2_gen_dis)展示了StyleGAN2中的生成器和鉴别器块。'
- en: '![](Images/gdl2_1012.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1012.png)'
- en: Figure 10-12\. The generator and discriminator blocks in StyleGAN2
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12。StyleGAN2中的生成器和鉴别器块
- en: The crucial property that we would like to be able to preserve is that the StyleGAN2
    starts by learning low-resolution features and gradually refines the output as
    training progresses. The authors show that this property is indeed preserved using
    this architecture. Each network benefits from refining the convolutional weights
    in the lower-resolution layers in the earlier stages of training, with the skip
    and residual connections used to pass the output through the higher-resolution
    layers mostly unaffected. As training progresses, the higher-resolution layers
    begin to dominate, as the generator discovers more intricate ways to improve the
    realism of the images in order to fool the discriminator. This process is demonstrated
    in [Figure 10-13](#stylegan2_contrib).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够保留的关键属性是，StyleGAN2从学习低分辨率特征开始，并随着训练的进行逐渐完善输出。作者表明，使用这种架构确实保留了这一属性。在训练的早期阶段，每个网络都受益于在较低分辨率层中细化卷积权重，而通过跳过和残差连接将输出传递到较高分辨率层的方式基本上不受影响。随着训练的进行，较高分辨率层开始占主导地位，因为生成器发现了更复杂的方法来改善图像的逼真度，以欺骗鉴别器。这个过程在[图10-13](#stylegan2_contrib)中展示。
- en: '![](Images/gdl2_1013.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1013.png)'
- en: Figure 10-13\. The contribution of each resolution layer to the output of the
    generator, by training time (adapted from [Karras et al., 2019](https://arxiv.org/pdf/1912.04958.pdf))
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13。每个分辨率层对生成器输出的贡献，按训练时间（改编自[Karras等人，2019](https://arxiv.org/pdf/1912.04958.pdf)）
- en: Outputs from StyleGAN2
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGAN2的输出
- en: Some examples of StyleGAN2 output are shown in [Figure 10-14](#stylegan2_output).
    To date, the StyleGAN2 architecture (and scaled variations such as StyleGAN-XL^([6](ch10.xhtml#idm45387004898624)))
    remain state of the art for image generation on datasets such as Flickr-Faces-HQ
    (FFHQ) and CIFAR-10, according to the benchmarking website [Papers with Code](https://oreil.ly/VwH2r).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一些StyleGAN2输出的示例显示在[图10-14](#stylegan2_output)中。迄今为止，StyleGAN2架构（以及诸如StyleGAN-XL这样的扩展变体）仍然是Flickr-Faces-HQ（FFHQ）和CIFAR-10等数据集上图像生成的最先进技术，根据基准网站[Papers
    with Code](https://oreil.ly/VwH2r)。
- en: '![](Images/gdl2_1014.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1014.png)'
- en: 'Figure 10-14\. Uncurated StyleGAN2 output for the FFHQ face dataset and LSUN
    car dataset (source: [Karras et al., 2019](https://arxiv.org/pdf/1912.04958.pdf))'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-14。FFHQ人脸数据集和LSUN汽车数据集的未筛选StyleGAN2输出（来源：[Karras等人，2019](https://arxiv.org/pdf/1912.04958.pdf))
- en: Other Important GANs
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他重要的GAN
- en: In this section, we will explore two more architectures that have also contributed
    significantly to the development of GANs—SAGAN and BigGAN.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将探讨另外两种架构，它们也对GAN的发展做出了重大贡献——SAGAN和BigGAN。
- en: Self-Attention GAN (SAGAN)
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力生成对抗网络（SAGAN）
- en: The Self-Attention GAN (SAGAN)^([7](ch10.xhtml#idm45387004886752)) is a key
    development for GANs as it shows how the attention mechanism that powers sequential
    models such as the Transformer can also be incorporated into GAN-based models
    for image generation. [Figure 10-15](#sagan_attention) shows the self-attention
    mechanism from the paper introducing this architecture.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力生成对抗网络（SAGAN）是GAN的一个重要发展，因为它展示了如何将驱动序列模型（如Transformer）的注意机制也纳入到基于GAN的图像生成模型中。[图10-15](#sagan_attention)展示了介绍这种架构的论文中的自注意力机制。
- en: '![](Images/gdl2_1015.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1015.png)'
- en: 'Figure 10-15\. The self-attention mechanism within the SAGAN model (source:
    [Zhang et al., 2018](https://arxiv.org/abs/1805.08318))'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-15。SAGAN模型中的自注意机制（来源：[Zhang等人，2018](https://arxiv.org/abs/1805.08318)）
- en: The problem with GAN-based models that do not incorporate attention is that
    convolutional feature maps are only able to process information locally. Connecting
    pixel information from one side of an image to the other requires multiple convolutional
    layers that reduce the size of the image, while increasing the number of channels.
    Precise positional information is reduced throughout this process in favor of
    capturing higher-level features, making it computationally inefficient for the
    model to learn long-range dependencies between distantly connected pixels. SAGAN
    solves this problem by incorporating the attention mechanism that we explored
    earlier in this chapter into the GAN. The effect of this inclusion is shown in
    [Figure 10-16](Images/#sagan_images).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不包含注意力的基于GAN的模型的问题在于，卷积特征图只能在局部处理信息。连接图像一侧的像素信息到另一侧需要多个卷积层，这会减小图像的尺寸，同时增加通道数。在这个过程中，精确的位置信息会被减少，以捕捉更高级的特征，这使得模型学习远距离像素之间的长距离依赖性变得计算上低效。SAGAN通过将我们在本章前面探讨过的注意力机制纳入到GAN中来解决这个问题。这种包含的效果在[图10-16](Images/#sagan_images)中展示。
- en: '![](Images/gdl2_1016.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1016.png)'
- en: 'Figure 10-16\. A SAGAN-generated image of a bird (leftmost cell) and the attention
    maps of the final attention-based generator layer for the pixels covered by the
    three colored dots (rightmost cells) (source: [Zhang et al., 2018](https://arxiv.org/abs/1805.08318))'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。SAGAN生成的一幅鸟的图像（最左侧单元格）以及由最终基于注意力的生成器层生成的像素的注意力图（右侧单元格）（来源：[Zhang等人，2018](https://arxiv.org/abs/1805.08318))
- en: The red dot is a pixel that is part of the bird’s body, and so attention naturally
    falls on the surrounding body cells. The green dot is part of the background,
    and here the attention actually falls on the other side of the bird’s head, on
    other background pixels. The blue dot is part of the bird’s long tail and so attention
    falls on other tail pixels, some of which are distant from the blue dot. It would
    be difficult to maintain this long-range dependency for pixels without attention,
    especially for long, thin structures in the image (such as the tail in this case).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 红点是鸟身体的一部分，因此注意力自然地集中在周围的身体细胞上。绿点是背景的一部分，这里注意力实际上集中在鸟头的另一侧，即其他背景像素上。蓝点是鸟的长尾的一部分，因此注意力集中在其他尾部像素上，其中一些与蓝点相距较远。对于没有注意力的像素来说，尤其是对于图像中的长、细结构（例如这种情况下的尾巴），要维持这种长距离依赖性将会很困难。
- en: Training Your Own SAGAN
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的SAGAN
- en: The official code for training your own SAGAN using TensorFlow is available
    on [GitHub](https://oreil.ly/rvej0). Bear in mind that training a SAGAN to achieve
    the results from the paper requires a significant amount of computing power.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow训练自己的SAGAN的官方代码可在[GitHub](https://oreil.ly/rvej0)上找到。请注意，要实现论文中的结果，训练SAGAN需要大量的计算资源。
- en: BigGAN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BigGAN
- en: BigGAN,^([8](ch10.xhtml#idm45387004870736)) developed at DeepMind, extends the
    ideas from the SAGAN paper. [Figure 10-17](#biggan_examples) shows some of the
    images generated by BigGAN, trained on the ImageNet dataset at 128 × 128 resolution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: BigGAN，由DeepMind开发，扩展了SAGAN论文中的思想。[图10-17](#biggan_examples)展示了一些由BigGAN生成的图像，该模型在ImageNet数据集上进行了训练，分辨率为128×128。
- en: '![](Images/gdl2_1017.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1017.png)'
- en: 'Figure 10-17\. Examples of images generated by BigGAN (source: [Brock et al.,
    2018](https://arxiv.org/abs/1809.11096))'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17。由BigGAN生成的图像示例（来源：[Brock等人，2018](https://arxiv.org/abs/1809.11096))
- en: As well as some incremental changes to the base SAGAN model, there are also
    several innovations outlined in the paper that take the model to the next level
    of sophistication. One such innovation is the so-called *truncation trick*. This
    is where the latent distribution used for sampling is different from the <math
    alttext="z tilde script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>z</mi>
    <mo>∼</mo> <mi>𝒩</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>𝐈</mi> <mo>)</mo></mrow></math>
    distribution used during training. Specifically, the distribution used during
    sampling is a *truncated normal distribution* (resampling values of <math alttext="z"><mi>z</mi></math>
    that have magnitude greater than a certain threshold). The smaller the truncation
    threshold, the greater the believability of generated samples, at the expense
    of reduced variability. This concept is shown in [Figure 10-18](#truncation).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对基本 SAGAN 模型进行一些增量更改外，论文中还概述了将模型提升到更高层次的几项创新。其中一项创新是所谓的“截断技巧”。这是指用于采样的潜在分布与训练期间使用的
    <math alttext="z tilde script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>z</mi>
    <mo>∼</mo> <mi>𝒩</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>𝐈</mi> <mo>)</mo></mrow></math>
    分布不同。具体来说，采样期间使用的分布是“截断正态分布”（重新采样具有大于一定阈值的 <math alttext="z"><mi>z</mi></math>
    值）。截断阈值越小，生成样本的可信度越高，但变异性降低。这个概念在[图 10-18](#truncation)中展示。
- en: '![](Images/gdl2_1018.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1018.png)'
- en: 'Figure 10-18\. The truncation trick: from left to right, the threshold is set
    to 2, 1, 0.5, and 0.04 (source: [Brock et al., 2018](https://arxiv.org/abs/1809.11096))'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-18\. 截断技巧：从左到右，阈值设置为 2、1、0.5 和 0.04（来源：[Brock 等人，2018](https://arxiv.org/abs/1809.11096)）
- en: Also, as the name suggests, BigGAN is an improvement over SAGAN in part simply
    by being *bigger*. BigGAN uses a batch size of 2,048—8 times larger than the batch
    size of 256 used in SAGAN—and a channel size that is increased by 50% in each
    layer. However, BigGAN additionally shows that SAGAN can be improved structurally
    by the inclusion of a shared embedding, by orthogonal regularization, and by incorporating
    the latent vector <math alttext="z"><mi>z</mi></math> into each layer of the generator,
    rather than just the initial layer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，BigGAN 在某种程度上是对 SAGAN 的改进，仅仅是因为它更“大”。BigGAN 使用的批量大小为 2,048，比 SAGAN 中使用的
    256 的批量大小大 8 倍，并且每一层的通道大小增加了 50%。然而，BigGAN 还表明，通过包含共享嵌入、正交正则化以及将潜在向量 <math alttext="z"><mi>z</mi></math>
    包含到生成器的每一层中，而不仅仅是初始层，可以在结构上改进 SAGAN。
- en: For a full description of the innovations introduced by BigGAN, I recommend
    reading the original paper and [accompanying presentation material](https://oreil.ly/vPn8T).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面了解 BigGAN 引入的创新，我建议阅读原始论文和[相关演示材料](https://oreil.ly/vPn8T)。
- en: Using BigGAN
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BigGAN
- en: A tutorial for generating images using a pre-trained BigGAN is available on
    [the TensorFlow website](https://oreil.ly/YLbLb).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ TensorFlow 网站](https://oreil.ly/YLbLb)上提供了一个使用预训练的 BigGAN 生成图像的教程。
- en: VQ-GAN
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQ-GAN
- en: Another important type of GAN is the Vector Quantized GAN (VQ-GAN), introduced
    in 2020.^([9](ch10.xhtml#idm45387004838864)) This model architecture builds upon
    an idea introduced in the 2017 paper “Neural Discrete Representation Learning”^([10](ch10.xhtml#idm45387004834704))—namely,
    that the representations learned by a VAE can be discrete, rather than continuous.
    This new type of model, the Vector Quantized VAE (VQ-VAE), was shown to generate
    high-quality images while avoiding some of the issues often seen with traditional
    continuous latent space VAEs, such as *posterior collapse* (where the learned
    latent space becomes uninformative due to an overly powerful decoder).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种重要的 GAN 类型是 2020 年推出的 Vector Quantized GAN（VQ-GAN）。这种模型架构建立在 2017 年的论文“神经离散表示学习”中提出的一个想法之上，即
    VAE 学习到的表示可以是离散的，而不是连续的。这种新型模型，即 Vector Quantized VAE（VQ-VAE），被证明可以生成高质量的图像，同时避免了传统连续潜在空间
    VAE 经常出现的一些问题，比如“后验坍缩”（学习到的潜在空间由于过于强大的解码器而变得无信息）。
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The first version of DALL.E, a text-to-image model released by OpenAI in 2021
    (see [Chapter 13](ch13.xhtml#chapter_multimodal)), utilized a VAE with a discrete
    latent space, similar to VQ-VAE.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在 2021 年发布的文本到图像模型 DALL.E 的第一个版本（参见[第 13 章](ch13.xhtml#chapter_multimodal)）使用了具有离散潜在空间的
    VAE，类似于 VQ-VAE。
- en: By a *discrete latent space*, we mean a learned list of vectors (the *codebook*),
    each associated with a corresponding index. The job of the encoder in a VQ-VAE
    is to collapse the input image to a smaller grid of vectors that can then be compared
    to the codebook. The closest codebook vector to each grid square vector (by Euclidean
    distance) is then taken forward to be decoded by the decoder, as shown in [Figure 10-19](#vqvae).
    The codebook is a list of learned vectors of length <math alttext="d"><mi>d</mi></math>
    (the embedding size) that matches the number of channels in the output of the
    encoder and input to the decoder. For example, <math alttext="e 1"><msub><mi>e</mi>
    <mn>1</mn></msub></math> is a vector that can be interpreted as *background*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过“离散潜在空间”，我们指的是一个学习到的向量列表（“码书”），每个向量与相应的索引相关联。VQ-VAE 中编码器的工作是将输入图像折叠到一个较小的向量网格中，然后将其与码书进行比较。然后，将每个网格方格向量（通过欧氏距离）最接近的码书向量传递给解码器进行解码，如[图
    10-19](#vqvae)所示。码书是一个长度为 <math alttext="d"><mi>d</mi></math>（嵌入大小）的学习向量列表，与编码器输出和解码器输入中的通道数相匹配。例如，<math
    alttext="e 1"><msub><mi>e</mi> <mn>1</mn></msub></math> 是一个可以解释为“背景”的向量。
- en: '![](Images/gdl2_1019.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1019.png)'
- en: Figure 10-19\. A diagram of a VQ-VAE
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-19\. VQ-VAE 的示意图
- en: The codebook can be thought of as a set of learned discrete concepts that are
    shared by the encoder and decoder in order to describe the contents of a given
    image. The VQ-VAE must find a way to make this set of discrete concepts as informative
    as possible so that the encoder can accurately *label* each grid square with a
    particular code vector that is meaningful to the decoder. The loss function for
    a VQ-VAE is therefore the reconstruction loss added to two terms (alignment and
    commitment loss) that ensure that the output vectors from the encoder are as close
    as possible to vectors in the codebook. These terms replace the the KL divergence
    term between the encoded distribution and the standard Gaussian prior in a typical
    VAE.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码本可以被看作是一组学习到的离散概念，这些概念由编码器和解码器共享，以描述给定图像的内容。VQ-VAE必须找到一种方法，使这组离散概念尽可能具有信息量，以便编码器可以准确地用特定的代码向量*标记*每个网格方块，这对解码器是有意义的。因此，VQ-VAE的损失函数是重构损失加上两个项（对齐和承诺损失），以确保编码器的输出向量尽可能接近代码本中的向量。这些项取代了典型VAE中编码分布和标准高斯先验之间的KL散度项。
- en: However, this architecture poses a question—how do we sample novel code grids
    to pass to the decoder to generate new images? Clearly, using a uniform prior
    (picking each code with equal probability for each grid square) will not work.
    For example in the MNIST dataset, the top-left grid square is highly likely to
    be coded as *background*, whereas grid squares toward the center of the image
    are not as likely to be coded as such. To solve this problem, the authors used
    another model, an autoregressive PixelCNN (see [Chapter 5](ch05.xhtml#chapter_autoregressive)),
    to predict the next code vector in the grid, given previous code vectors. In other
    words, the prior is learned by the model, rather than static as in the case of
    the vanilla VAE.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种架构提出了一个问题——我们如何对新颖的代码网格进行采样，以传递给解码器生成新的图像？显然，使用均匀先验（为每个网格方块均等概率选择每个代码）是行不通的。例如，在MNIST数据集中，左上角的网格方块很可能被编码为*背景*，而靠近图像中心的网格方块不太可能被编码为这样。为了解决这个问题，作者使用了另一个模型，一个自回归的PixelCNN（参见[第5章](ch05.xhtml#chapter_autoregressive)），来预测网格中下一个代码向量，给定先前的代码向量。换句话说，先验是由模型学习的，而不是像普通VAE中的标准高斯先验那样静态的。
- en: Training Your Own VQ-VAE
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练您自己的VQ-VAE
- en: There is an excellent tutorial by Sayak Paul on training your own VQ-VAE using
    Keras available on the [Keras website](https://oreil.ly/dmcb4).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有一篇由Sayak Paul撰写的优秀教程，介绍如何使用Keras在[Keras网站](https://oreil.ly/dmcb4)上训练自己的VQ-VAE。
- en: The VQ-GAN paper details several key changes to the VQ-VAE architecture, as
    shown in [Figure 10-20](#vqgan).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-GAN论文详细介绍了VQ-VAE架构的几个关键变化，如[图10-20](#vqgan)所示。
- en: '![](Images/gdl2_1020.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1020.png)'
- en: 'Figure 10-20\. A diagram of a VQ-GAN: the GAN discriminator helps to encourage
    the VAE to generate less blurry images through an additional adversarial loss
    term'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-20。VQ-GAN的图表：GAN鉴别器通过额外的对抗损失项帮助VAE生成更清晰的图像
- en: Firstly, as the name suggests, the authors include a GAN discriminator that
    tries to distinguish between the output from the VAE decoder and real images,
    with an accompanying adversarial term in the loss function. GANs are known to
    produce sharper images than VAEs, so this addition improves the overall image
    quality. Notice that despite the name, the VAE is still present in a VQ-GAN model—the
    GAN discriminator is an additional component rather than a replacement of the
    VAE. The idea of combining a VAE with a GAN discriminator (VAE-GAN) was first
    introduced by Larsen et al. in their 2015 paper.^([11](ch10.xhtml#idm45387004808112))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如名称所示，作者包括一个GAN鉴别器，试图区分VAE解码器的输出和真实图像，损失函数中还有一个对抗项。众所周知，GAN生成的图像比VAE更清晰，因此这个添加改善了整体图像质量。请注意，尽管名称中有VAE，但VAE仍然存在于VQ-GAN模型中——GAN鉴别器是一个额外的组件，而不是VAE的替代品。将VAE与GAN鉴别器（VAE-GAN）结合的想法首次由Larsen等人在他们2015年的论文中提出。
- en: Secondly, the GAN discriminator predicts if small patches of the images are
    real or fake, rather than the entire image at once. This idea (*PatchGAN*) was
    applied in the successful *pix2pix* image-to-image model introduced in 2016 by
    Isola et al.^([12](ch10.xhtml#idm45387004801680)) and was also successfully applied
    as part of *CycleGAN*,^([13](ch10.xhtml#idm45387004798080)) another image-to-image
    style transfer model. The PatchGAN discriminator outputs a prediction vector (a
    prediction for each patch), rather than a single prediction for the overall image.
    The benefit of using a PatchGAN discriminator is that the loss function can then
    measure how good the discriminator is at distinguishing images based on their
    *style*, rather than their *content*. Since each individual element of the discriminator
    prediction is based on a small square of the image, it must use the style of the
    patch, rather than its content, to make its decision. This is useful as we know
    that VAEs produce images that are stylistically more blurry than real images,
    so the PatchGAN discriminator can encourage the VAE decoder to generate sharper
    images than it would naturally produce.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，GAN鉴别器预测图像的小块是否真实或伪造，而不是一次性预测整个图像。这个想法（*PatchGAN*）被应用在2016年由Isola等人介绍的成功的*pix2pix*图像到图像模型中，并且也成功地作为*CycleGAN*的一部分应用，另一个图像到图像的风格转移模型。PatchGAN鉴别器输出一个预测向量（每个块的预测），而不是整个图像的单个预测。使用PatchGAN鉴别器的好处在于，损失函数可以衡量鉴别器在基于*风格*而不是*内容*来区分图像方面的表现如何。由于鉴别器预测的每个单独元素基于图像的一个小方块，它必须使用块的风格而不是内容来做出决定。这是有用的，因为我们知道VAE生成的图像在风格上比真实图像更模糊，因此PatchGAN鉴别器可以鼓励VAE解码器生成比其自然产生的更清晰的图像。
- en: Thirdly, rather than use a single MSE reconstruction loss that compares the
    input image pixels with the output pixels from the VAE decoder, VQ-GAN uses a
    *perceptual loss* term that calculates the difference between feature maps at
    intermediate layers of the encoder and corresponding layers of the decoder. This
    idea is from the 2016 paper by Hou et al.,^([14](ch10.xhtml#idm45387004793216))
    where the authors show that this change to the loss function results in more realistic
    image generations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，与使用单个MSE重建损失不同，该损失将输入图像像素与VAE解码器输出像素进行比较，VQ-GAN使用*感知损失*项，计算编码器中间层的特征图与解码器相应层之间的差异。这个想法来自于侯等人2016年的论文，^([14](ch10.xhtml#idm45387004793216))作者在其中展示了这种对损失函数的改变导致更逼真的图像生成。
- en: Lastly, instead of PixelCNN, a Transformer is used as the autoregressive part
    of the model, trained to generate sequences of codes. The Transformer is trained
    in a separate phase, after the VQ-GAN has been fully trained. Rather than use
    all previous tokens in a fully autoregressive manner, the authors choose to only
    use tokens that fall within a sliding window around the token to be predicted.
    This ensures that the model scales to larger images, which require a larger latent
    grid size and therefore more tokens to be generated by the Transformer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型的自回归部分使用Transformer而不是PixelCNN，训练以生成代码序列。Transformer在VQ-GAN完全训练后的一个单独阶段中进行训练。作者选择仅使用在要预测的令牌周围的滑动窗口内的令牌，而不是完全自回归地使用所有先前的令牌。这确保了模型可以扩展到需要更大潜在网格大小和因此需要Transformer生成更多令牌的更大图像。
- en: ViT VQ-GAN
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViT VQ-GAN
- en: One final extension to the VQ-GAN was made by Yu et al. in their 2021 paper
    entitled “Vector-Quantized Image Modeling with Improved VQGAN.”^([15](ch10.xhtml#idm45387004783968))
    Here, the authors show how the convolutional encoder and decoder of the VQ-GAN
    can be replaced with Transformers as shown in [Figure 10-21](#vit_vqgan).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Yu等人在2021年的论文“Vector-Quantized Image Modeling with Improved VQGAN”中对VQ-GAN进行了最后一个扩展。^([15](ch10.xhtml#idm45387004783968))
    在这里，作者展示了如何将VQ-GAN的卷积编码器和解码器替换为Transformer，如[图10-21](#vit_vqgan)所示。
- en: For the encoder, the authors use a *Vision Transformer* (ViT).^([16](ch10.xhtml#idm45387004780000))
    A ViT is a neural network architecture that applies the Transformer model, originally
    designed for natural language processing, to image data. Instead of using convolutional
    layers to extract features from an image, a ViT divides the image into a sequence
    of patches, which are tokenized and then fed as input to an encoder Transformer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器，作者使用*Vision Transformer*（ViT）。^([16](ch10.xhtml#idm45387004780000)) ViT是一种神经网络架构，将最初设计用于自然语言处理的Transformer模型应用于图像数据。ViT不使用卷积层从图像中提取特征，而是将图像分成一系列补丁，对其进行标记化，然后将其作为输入馈送到编码器Transformer中。
- en: Specifically, in the ViT VQ-GAN, the nonoverlapping input patches (each of size
    8 × 8) are first flattened, then projected into a low-dimensional embedding space,
    where positional embeddings are added. This sequence is then fed to a standard
    encoder Transformer and the resulting embeddings are quantized according to a
    learned codebook. These integer codes are then processed by a decoder Transformer
    model, with the overall output being a sequence of patches that can be stitched
    back together to form the original image. The overall encoder-decoder model is
    trained end-to-end as an autoencoder.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在ViT VQ-GAN中，非重叠的输入补丁（每个大小为8×8）首先被展平，然后投影到低维嵌入空间中，位置嵌入被添加。然后，这个序列被馈送到标准编码器Transformer中，生成的嵌入根据学习的码书进行量化。这些整数代码然后由解码器Transformer模型处理，最终输出是一系列补丁，可以被拼接在一起形成原始图像。整体的编码器-解码器模型被作为自动编码器端到端训练。
- en: '![](Images/gdl2_1021.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1021.png)'
- en: 'Figure 10-21\. A diagram of a ViT VQ-GAN: the GAN discriminator helps to encourage
    the VAE to generate less blurry images through an additional adversarial loss
    term (source: [Yu and Koh, 2022](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html))^([17](ch10.xhtml#idm45387004774560))'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-21。ViT VQ-GAN的图表：GAN鉴别器通过额外的对抗损失项帮助VAE生成更清晰的图像（来源：[Yu and Koh, 2022](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html)）^([17](ch10.xhtml#idm45387004774560))
- en: As with the original VQ-GAN model, the second phase of training involves using
    an autoregressive decoder Transformer to generate sequences of codes. Therefore
    in total, there are three Transformers in a ViT VQ-GAN, in addition to the GAN
    discriminator and learned codebook. Examples of images generated by the ViT VQ-GAN
    from the paper are shown in [Figure 10-22](#vit_vqgan_ex).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始VQ-GAN模型一样，训练的第二阶段涉及使用自回归解码器Transformer生成代码序列。因此，在ViT VQ-GAN中总共有三个Transformer，另外还有GAN鉴别器和学习的码书。论文中ViT
    VQ-GAN生成的图像示例显示在[图10-22](#vit_vqgan_ex)中。
- en: '![](Images/gdl2_1022.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1022.png)'
- en: 'Figure 10-22\. Example images generated by a ViT VQ-GAN trained on ImageNet
    (source: [Yu et al., 2021](https://arxiv.org/pdf/2110.04627.pdf))'
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-22。ViT VQ-GAN在ImageNet上训练生成的示例图像（来源：[Yu et al., 2021](https://arxiv.org/pdf/2110.04627.pdf)）
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have taken a tour of some of the most important and influential
    GAN papers since 2017\. In particular, we have explored ProGAN, StyleGAN, StyleGAN2,
    SAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了自2017年以来一些最重要和有影响力的GAN论文。特别是，我们探讨了ProGAN、StyleGAN、StyleGAN2、SAGAN、BigGAN、VQ-GAN和ViT
    VQ-GAN。
- en: We started by exploring the concept of progressive training that was pioneered
    in the 2017 ProGAN paper. Several key changes were introduced in the 2018 StyleGAN
    paper that gave greater control over the image output, such as the mapping network
    for creating a specific style vector and synthesis network that allowed the style
    to be injected at different resolutions. Finally, StyleGAN2 replaced the adaptive
    instance normalization of StyleGAN with weight modulation and demodulation steps,
    alongside additional enhancements such as path regularization. The paper also
    showed how the desirable property of gradual resolution refinement could be retained
    without having to the train the network progressively.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从2017年ProGAN论文中首创的渐进训练概念开始探索。2018年StyleGAN论文引入了几个关键改变，使对图像输出有更大的控制，例如用于创建特定样式向量的映射网络和允许在不同分辨率注入样式的合成网络。最后，StyleGAN2用权重调制和解调制步骤替换了StyleGAN的自适应实例归一化，同时还进行了额外的增强，如路径正则化。该论文还展示了如何保留渐进分辨率细化的可取属性，而无需逐步训练网络。
- en: We also saw how the concept of attention could be built into a GAN, with the
    introduction of SAGAN in 2018\. This allows the network to capture long-range
    dependencies, such as similar background colors over opposite sides of an image,
    without relying on deep convolutional maps to spread the information over the
    spatial dimensions of the image. BigGAN was an extension of this idea that made
    several key changes and trained a larger network to improve the image quality
    further.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了如何将注意力的概念构建到GAN中，2018年引入了SAGAN。这使网络能够捕捉长距离依赖关系，例如图像相对两侧的相似背景颜色，而无需依赖深度卷积映射将信息传播到图像的空间维度。BigGAN是这个想法的延伸，进行了几个关键改变，并训练了一个更大的网络以进一步提高图像质量。
- en: In the VQ-GAN paper, the authors show how several different types of generative
    models can be combined to great effect. Building on the original VQ-VAE paper
    that introduced the concept of a VAE with a discrete latent space, VQ-GAN additionally
    includes a discriminator that encourages the VAE to generate less blurry images
    through an additional adversarial loss term. An autoregressive Transformer is
    used to construct a novel sequence of code tokens that can be decoded by the VAE
    decoder to produce novel images. The ViT VQ-GAN paper extends this idea even further,
    by replacing the convolutional encoder and decoder of VQ-GAN with Transformers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在VQ-GAN论文中，作者展示了如何将几种不同类型的生成模型结合起来产生很好的效果。在最初引入具有离散潜在空间的VAE概念的VQ-VAE论文的基础上，VQ-GAN还包括一个鼓励VAE通过额外的对抗损失项生成更清晰图像的鉴别器。自回归变压器用于构建一个新颖的代码令牌序列，可以由VAE解码器解码以生成新颖图像。ViT
    VQ-GAN论文进一步扩展了这个想法，通过用变压器替换VQ-GAN的卷积编码器和解码器。
- en: '^([1](ch10.xhtml#idm45387005226448-marker)) Huiwen Chang et al., “Muse: Text-to-Image
    Generation via Masked Generative Transformers,” January 2, 2023, [*https://arxiv.org/abs/2301.00704*](https://arxiv.org/abs/2301.00704).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch10.xhtml#idm45387005226448-marker)) Huiwen Chang等人，“Muse: 通过遮罩生成变压器进行文本到图像生成”，2023年1月2日，[*https://arxiv.org/abs/2301.00704*](https://arxiv.org/abs/2301.00704)。'
- en: ^([2](ch10.xhtml#idm45387005216528-marker)) Tero Karras et al., “Progressive
    Growing of GANs for Improved Quality, Stability, and Variation,” October 27, 2017,
    [*https://arxiv.org/abs/1710.10196*](https://arxiv.org/abs/1710.10196).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.xhtml#idm45387005216528-marker)) Tero Karras等人，“用于改善质量、稳定性和变化的GAN的渐进增长”，2017年10月27日，[*https://arxiv.org/abs/1710.10196*](https://arxiv.org/abs/1710.10196)。
- en: ^([3](ch10.xhtml#idm45387005140128-marker)) Tero Karras et al., “A Style-Based
    Generator Architecture for Generative Adversarial Networks,” December 12, 2018,
    [*https://arxiv.org/abs/1812.04948*](https://arxiv.org/abs/1812.04948).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.xhtml#idm45387005140128-marker)) Tero Karras等人，“用于生成对抗网络的基于样式的生成器架构”，2018年12月12日，[*https://arxiv.org/abs/1812.04948*](https://arxiv.org/abs/1812.04948)。
- en: ^([4](ch10.xhtml#idm45387005090240-marker)) Xun Huang and Serge Belongie, “Arbitrary
    Style Transfer in Real-Time with Adaptive Instance Normalization,” March 20, 2017,
    [*https://arxiv.org/abs/1703.06868*](https://arxiv.org/abs/1703.06868).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.xhtml#idm45387005090240-marker)) Xun Huang和Serge Belongie，“使用自适应实例归一化实时进行任意风格转移”，2017年3月20日，[*https://arxiv.org/abs/1703.06868*](https://arxiv.org/abs/1703.06868)。
- en: ^([5](ch10.xhtml#idm45387005019232-marker)) Tero Karras et al., “Analyzing and
    Improving the Image Quality of StyleGAN,” December 3, 2019, [*https://arxiv.org/abs/1912.04958*](https://arxiv.org/abs/1912.04958).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.xhtml#idm45387005019232-marker)) Tero Karras等人，“分析和改进StyleGAN的图像质量”，2019年12月3日，[*https://arxiv.org/abs/1912.04958*](https://arxiv.org/abs/1912.04958)。
- en: '^([6](ch10.xhtml#idm45387004898624-marker)) Axel Sauer et al., “StyleGAN-XL:
    Scaling StyleGAN to Large Diverse Datasets,” February 1, 2022, [*https://arxiv.org/abs/2202.00273v2*](https://arxiv.org/abs/2202.00273v2).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch10.xhtml#idm45387004898624-marker)) Axel Sauer等人，“StyleGAN-XL: 将StyleGAN扩展到大型多样数据集”，2022年2月1日，[*https://arxiv.org/abs/2202.00273v2*](https://arxiv.org/abs/2202.00273v2)。'
- en: ^([7](ch10.xhtml#idm45387004886752-marker)) Han Zhang et al., “Self-Attention
    Generative Adversarial Networks,” May 21, 2018, [*https://arxiv.org/abs/1805.08318*](https://arxiv.org/abs/1805.08318).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.xhtml#idm45387004886752-marker)) Han Zhang等人，“自注意力生成对抗网络”，2018年5月21日，[*https://arxiv.org/abs/1805.08318*](https://arxiv.org/abs/1805.08318)。
- en: ^([8](ch10.xhtml#idm45387004870736-marker)) Andrew Brock et al., “Large Scale
    GAN Training for High Fidelity Natural Image Synthesis,” September 28, 2018, [*https://arxiv.org/abs/1809.11096*](https://arxiv.org/abs/1809.11096).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch10.xhtml#idm45387004870736-marker)) Andrew Brock等人，“用于高保真自然图像合成的大规模GAN训练”，2018年9月28日，[*https://arxiv.org/abs/1809.11096*](https://arxiv.org/abs/1809.11096)。
- en: ^([9](ch10.xhtml#idm45387004838864-marker)) Patrick Esser et al., “Taming Transformers
    for High-Resolution Image Synthesis,” December 17, 2020, [*https://arxiv.org/abs/2012.09841*](https://arxiv.org/abs/2012.09841).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch10.xhtml#idm45387004838864-marker)) Patrick Esser等人，“驯服变压器以进行高分辨率图像合成”，2020年12月17日，[*https://arxiv.org/abs/2012.09841*](https://arxiv.org/abs/2012.09841)。
- en: ^([10](ch10.xhtml#idm45387004834704-marker)) Aaron van den Oord et al., “Neural
    Discrete Representation Learning,” November 2, 2017, [*https://arxiv.org/abs/1711.00937v2*](https://arxiv.org/abs/1711.00937v2).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch10.xhtml#idm45387004834704-marker)) Aaron van den Oord等人，“神经离散表示学习”，2017年11月2日，[*https://arxiv.org/abs/1711.00937v2*](https://arxiv.org/abs/1711.00937v2)。
- en: ^([11](ch10.xhtml#idm45387004808112-marker)) Anders Boesen Lindbo Larsen et
    al., “Autoencoding Beyond Pixels Using a Learned Similarity Metric,” December
    31, 2015, [*https://arxiv.org/abs/1512.09300*](https://arxiv.org/abs/1512.09300).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch10.xhtml#idm45387004808112-marker)) Anders Boesen Lindbo Larsen等人，“超越像素的自动编码：使用学习的相似度度量”，2015年12月31日，[*https://arxiv.org/abs/1512.09300*](https://arxiv.org/abs/1512.09300)。
- en: ^([12](ch10.xhtml#idm45387004801680-marker)) Phillip Isola et al., “Image-to-Image
    Translation with Conditional Adversarial Networks,” November 21, 2016, [*https://arxiv.org/abs/1611.07004v3*](https://arxiv.org/abs/1611.07004v3).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch10.xhtml#idm45387004801680-marker)) Phillip Isola等人，“带条件对抗网络的图像到图像翻译”，2016年11月21日，[*https://arxiv.org/abs/1611.07004v3*](https://arxiv.org/abs/1611.07004v3)。
- en: ^([13](ch10.xhtml#idm45387004798080-marker)) Jun-Yan Zhu et al., “Unpaired Image-to-Image
    Translation using Cycle-Consistent Adversarial Networks,” March 30, 2017, [*https://arxiv.org/abs/1703.10593*](https://arxiv.org/abs/1703.10593).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch10.xhtml#idm45387004798080-marker)) Jun-Yan Zhu等人，“使用循环一致性对抗网络进行无配对图像到图像翻译”，2017年3月30日，[*https://arxiv.org/abs/1703.10593*](https://arxiv.org/abs/1703.10593)。
- en: ^([14](ch10.xhtml#idm45387004793216-marker)) Xianxu Hou et al., “Deep Feature
    Consistent Variational Autoencoder,” October 2, 2016, [*https://arxiv.org/abs/1610.00291*](https://arxiv.org/abs/1610.00291).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch10.xhtml#idm45387004793216-marker)) Xianxu Hou等人，“深度特征一致变分自动编码器”，2016年10月2日，[*https://arxiv.org/abs/1610.00291*](https://arxiv.org/abs/1610.00291)。
- en: ^([15](ch10.xhtml#idm45387004783968-marker)) Jiahui Yu et al., “Vector-Quantized
    Image Modeling with Improved VQGAN,” October 9, 2021, [*https://arxiv.org/abs/2110.04627*](https://arxiv.org/abs/2110.04627).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch10.xhtml#idm45387004783968-marker)) Jiahui Yu等人，“改进的VQGAN进行矢量量化图像建模”，2021年10月9日，[*https://arxiv.org/abs/2110.04627*](https://arxiv.org/abs/2110.04627)。
- en: '^([16](ch10.xhtml#idm45387004780000-marker)) Alexey Dosovitskiy et al., “An
    Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale,” October
    22, 2020, [*https://arxiv.org/abs/2010.11929v2*](https://arxiv.org/abs/2010.11929v2).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch10.xhtml#idm45387004780000-marker)) Alexey Dosovitskiy等人，“一幅图像价值16x16个词：规模化图像识别的变压器”，2020年10月22日，[*https://arxiv.org/abs/2010.11929v2*](https://arxiv.org/abs/2010.11929v2)。
- en: ^([17](ch10.xhtml#idm45387004774560-marker)) Jiahui Yu and Jing Yu Koh, “Vector-Quantized
    Image Modeling with Improved VQGAN,” May 18, 2022, [*https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html*](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch10.xhtml#idm45387004774560-marker)) Jiahui Yu和Jing Yu Koh，“改进的VQGAN进行矢量量化图像建模”，2022年5月18日，[*https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html*](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html)。
