- en: 'Chapter 3\. Going Beyond the Basics: Detecting Features in Images'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 超越基础：检测图像中的特征
- en: In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    you learned how to get started with computer vision by creating a simple neural
    network that matched the input pixels of the Fashion MNIST dataset to 10 labels,
    each of which represented a type (or class) of clothing. And while you created
    a network that was pretty good at detecting clothing types, there was a clear
    drawback. Your neural network was trained on small monochrome images, each of
    which contained only a single item of clothing, and each item was centered within
    the image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中，你学习了如何通过创建一个简单的神经网络来开始计算机视觉之旅，该神经网络将Fashion
    MNIST数据集的输入像素与10个标签相匹配，每个标签代表一种（或一类）服装。虽然你创建了一个在检测服装类型方面相当不错的网络，但有一个明显的缺点。你的神经网络是在小单色图像上训练的，每个图像只包含一件服装，而且每件服装都在图像中居中。
- en: To take the model to the next level, you need it to be able to detect *features*
    in images. So, for example, instead of looking merely at the raw pixels in the
    image, what if we could filter the images down to constituent elements? Matching
    those elements, instead of raw pixels, would help the model detect the contents
    of images more effectively. For example, consider the Fashion MNIST dataset that
    we used in the last chapter. When detecting a shoe, the neural network may have
    been activated by lots of dark pixels clustered at the bottom of the image, which
    it would see as the sole of the shoe. But if the shoe were not centered and filling
    the frame, this logic wouldn’t hold.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将模型提升到下一个层次，你需要它能够检测图像中的*特征*。所以，例如，如果我们能够将图像过滤到构成元素，那会怎么样？匹配这些元素而不是原始像素，将有助于模型更有效地检测图像内容。例如，考虑我们在上一章中使用的Fashion
    MNIST数据集。在检测鞋子时，神经网络可能被图像底部聚集的大量暗色像素激活，它将看到这是鞋底。但如果鞋子没有居中且填满整个画面，这种逻辑就不成立了。
- en: One method of detecting features comes from photography and image processing
    methodologies that you may already be familiar with. If you’ve ever used a tool
    like Photoshop or GIMP to sharpen an image, you’ve used a mathematical filter
    that works on the pixels of the image. Another word for what these filters do
    is *convolution*, and by using such filters in a neural network, you’ll create
    a *convolutional neural network* (CNN).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 检测特征的一种方法来源于摄影和图像处理方法，这些方法你可能已经熟悉。如果你曾经使用过像Photoshop或GIMP这样的工具来锐化图像，你就已经使用了一个作用于图像像素的数学滤波器。这些滤波器所执行的操作另一个词叫*卷积*，通过在神经网络中使用这样的滤波器，你会创建一个*卷积神经网络*
    (CNN)。
- en: In this chapter, you’ll start by learning about how to use convolutions to detect
    features in an image. Then, you’ll dig deeper into classifying images based on
    the features within. We’ll also explore augmentation of images to get more features
    and transfer learning to take preexisting features that were learned by others,
    and then we’ll look briefly into optimizing your models by using dropouts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将首先学习如何使用卷积来检测图像中的特征。然后，你将深入了解基于图像内部特征进行图像分类。我们还将探讨图像增强以获取更多特征，以及迁移学习，即利用他人学习到的现有特征，然后我们将简要探讨通过使用dropout来优化你的模型。
- en: Convolutions
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: A *convolution* is simply a filter of weights that are used to multiply a pixel
    by its neighbors to get a new value for the pixel. For example, consider the ankle
    boot image from Fashion MNIST and the pixel values for it (see [Figure 3-1](#ch03_figure_1_1748570891059985)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*简单来说就是一组权重，这些权重用于将一个像素与其邻居相乘以得到该像素的新值。例如，考虑来自Fashion MNIST的踝靴图像及其像素值（见[图3-1](#ch03_figure_1_1748570891059985)）。'
- en: '![](assets/aiml_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0301.png)'
- en: Figure 3-1\. Ankle boot with convolution
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 带卷积的踝靴
- en: If we look at the pixel in the middle of the selection, we can see that it has
    the value 192\. (Recall that Fashion MNIST uses monochrome images with pixel values
    from 0 to 255.) The pixel above and to the left has the value 0, the one immediately
    above has the value 64, etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看选择区域中间的像素，我们可以看到它的值是192。（回想一下，Fashion MNIST使用的是单色图像，像素值从0到255。）上面的像素值是0，直接上面的像素值是64，等等。
- en: If we then define a filter in the same 3 × 3 grid, as shown below the original
    values, we can transform that pixel by calculating a new value for it. We do this
    by multiplying the current value of each pixel in the grid by the value in the
    same position in the filter grid and then summing up the total amount. This total
    will be the new value for the current pixel, and we then repeat this calculation
    for all pixels in the image.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们然后在下面的原始值下方定义一个3 × 3的网格中的滤波器，就像下面所示，我们可以通过计算其新值来转换该像素。我们通过将网格中每个像素的当前值乘以滤波器网格中相同位置的值，然后求和来实现这一点。这个总和将是当前像素的新值，然后我们重复这个过程来计算图像中所有像素的新值。
- en: 'So, in this case, while the current value of the pixel in the center of the
    selection is 192, we calculate the new value after applying the filter as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，虽然选择中心像素的当前值为192，但我们通过以下方式计算应用滤波器后的新值：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The result equals 577, which will be the new value for the pixel. Repeating
    this process for every pixel in the image will give us a filtered image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 结果等于577，这将是像素的新值。对图像中的每个像素重复此过程将给我们一个过滤后的图像。
- en: 'Now, let’s consider the impact of applying a filter on a more complicated image:
    specifically, the [ascent image](https://oreil.ly/wP8TE) that’s built into SciPy
    for easy testing. This is a 512 × 512 grayscale image that shows two people climbing
    a staircase.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑在一个更复杂的图像上应用滤波器的影响：具体来说，是SciPy内置的[上升图像](https://oreil.ly/wP8TE)，便于测试。这是一张512
    × 512的灰度图像，显示了两个人在爬楼梯。
- en: Using a filter with negative values on the left, positive values on the right,
    and zeros in the middle will end up removing most of the information from the
    image except for vertical lines (see [Figure 3-2](#ch03_figure_2_1748570891060023)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用左侧具有负值、右侧具有正值和中间为零的滤波器，最终会从图像中移除大部分信息，除了垂直线（参见[图3-2](#ch03_figure_2_1748570891060023)）。
- en: '![](assets/aiml_0302.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0302.png)'
- en: Figure 3-2\. Using a filter to derive vertical lines
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 使用滤波器提取垂直线
- en: Similarly, a small change to the filter can emphasize the horizontal lines (see
    [Figure 3-3](#ch03_figure_3_1748570891060045)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对滤波器进行微小的调整可以强调水平线（参见[图3-3](#ch03_figure_3_1748570891060045)）。
- en: '![](assets/aiml_0303.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0303.png)'
- en: Figure 3-3\. Using a filter to derive horizontal lines
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 使用滤波器提取水平线
- en: These examples also show that the amount of information in the image is reduced.
    Therefore, we can potentially *learn* a set of filters that reduce the image to
    features, and those features can be matched to labels as before. Previously, we
    learned parameters that were used in neurons to match inputs to outputs, and similarly,
    we can learn the best filters to match inputs to outputs over time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子也表明，图像中的信息量减少了。因此，我们可能可以*学习*一组将图像减少到特征的滤波器，而这些特征可以像以前一样与标签匹配。以前，我们学习了用于匹配输入到输出的神经元参数，同样，我们可以随着时间的推移学习最佳的滤波器来匹配输入到输出。
- en: When we combine convolution with pooling, we can reduce the amount of information
    in the image while maintaining the features. We’ll explore that next.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将卷积与池化结合使用时，我们可以在保持特征的同时减少图像中的信息量。我们将在下一节中探讨这一点。
- en: Pooling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: '*Pooling* is the process of eliminating pixels in your image while maintaining
    the semantics of the content within the image. It’s best explained visually. [Figure 3-4](#ch03_figure_4_1748570891060063)
    depicts the concept of *max pooling*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*池化*是在保持图像内容语义的同时消除图像中像素的过程。它最好通过视觉来解释。[图3-4](#ch03_figure_4_1748570891060063)描绘了*最大池化*的概念。'
- en: '![](assets/aiml_0304.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0304.png)'
- en: Figure 3-4\. An example of max pooling
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 最大池化的示例
- en: In this case, consider the box on the left to be the pixels in a monochrome
    image. We group them into 2 × 2 arrays, so in this case, the 16 pixels are grouped
    into four 2 × 2 arrays. These arrays are called *pools*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，将左侧的框视为单色图像中的像素。我们将它们分组为2 × 2的数组，因此在这种情况下，16个像素被分组为四个2 × 2的数组。这些数组被称为*池*。
- en: Then, we select the *maximum* value in each of the groups and reassemble them
    into a new image. Thus, the pixels on the left are reduced by 75% (from 16 to
    4), with the maximum value from each pool making up the new image. [Figure 3-5](#ch03_figure_5_1748570891060080)
    shows the version of ascent from [Figure 3-2](#ch03_figure_2_1748570891060023),
    with the vertical lines enhanced, after max pooling has been applied.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从每个组中选择*最大*值，并将它们重新组合成一张新图像。因此，左边的像素减少了 75%（从 16 减少到 4），每个池中的最大值构成了新图像。[图
    3-5](#ch03_figure_5_1748570891060080)显示了从[图 3-2](#ch03_figure_2_1748570891060023)的上升版本，在应用最大池化后，垂直线得到了增强。
- en: '![](assets/aiml_0305.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0305.png)'
- en: Figure 3-5\. Ascent after applying vertical filter and max pooling
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 应用垂直滤波和最大池化后的上升
- en: Note how the filtered features have not just been maintained but have been further
    enhanced. Also, the image size has changed from 512 × 512 to 256 × 256—making
    it a quarter of the original size.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意过滤后的特征不仅被保留，而且得到了进一步的增强。此外，图像大小已从 512 × 512 变为 256 × 256——变成了原始大小的四分之一。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are other approaches to pooling. These include *min pooling*, which takes
    the smallest pixel value from the pool, and *average pooling*, which takes the
    overall average value from the pool.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 存在其他池化的方法。这些包括*最小池化*，它从池中取最小的像素值，以及*平均池化*，它从池中取整体平均值。
- en: Implementing Convolutional Neural Networks
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现卷积神经网络
- en: 'In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    you created a neural network that recognized fashion images. For convenience,
    here’s the code to define the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中，你创建了一个能够识别时尚图像的神经网络。为了方便，这里提供了定义模型的代码：
- en: '[PRE1] `# Training process` `epochs` `=` `5` `for` `t` `in` `range``(``epochs``):`     `print``(``f``"Epoch`
    `{``t``+``1``}``\n``-------------------------------"``)`     `train``(``train_loader``,`
    `model``,` `loss_function``,` `optimizer``)` `print``(``"Done!"``)` [PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] `# 训练过程` `epochs` `=` `5` `for` `t` `in` `range``(``epochs``):`     `print``(``f``"Epoch`
    `{``t``+``1``}``\n``-------------------------------"``)`     `train``(``train_loader``,`
    `model``,` `loss_function``,` `optimizer``)` `print``(``"Done!"``)` [PRE2]'
- en: '[PRE3]`To convert this to a CNN, you simply use convolutional layers in our
    model definition on top of the current linear ones. You’ll also add pooling layers.    To
    implement a convolutional layer, you’ll use the `nn.Conv2D` type. It accepts as
    parameters the number of convolutions to use in the layer, the size of the convolutions,
    the activation function, etc.    For example, here’s a convolutional layer that
    uses this type:    [PRE4]    In this case, we want the layer to learn `64` convolutions.
    It will randomly initialize them, and over time, it will learn the filter values
    that work best to match the input values to their labels. The `kernel_size = 3`
    indicates the size of the filter. Earlier, we showed you 3 × 3 filters, and that’s
    what we’re specifying here. The 3 × 3 filter is the most common size of filter.
    You can change it as you see fit, but you’ll typically see an odd number of axes
    like 5 × 5 or 7 × 7 because of how filters remove pixels from the borders of the
    image, as you’ll see later.    Here’s how to use a pooling layer in the neural
    network. You’ll typically do this immediately after the convolutional layer:    [PRE5]    In
    the example back in [Figure 3-4](#ch03_figure_4_1748570891060063), we split the
    image into 2 × 2 pools and picked the maximum value in each. However, we could
    have used the parameters that you see here to define the pool size. The `kernel_size=2`
    parameter indicates that our pools are 2 × 2, and the `stride=2` parameter means
    that the filter will jump over two pixels to get the next pool.    Now, let’s
    explore the full code to define a model for Fashion MNIST with a CNN:    [PRE6]    Here,
    we see that the class has two functions, one for initialization and one that will
    be called during the forward pass in each epoch during training.    The `init`
    simply defines what each of the layers in our neural network will look like. The
    first layer (`self.layer1`) will take in the one-dimensional input, have `64`
    convolutions, a `kernel_size` of `3`, and `padding` of `1`. It will then ReLU
    the output before max pooling it.    The next layer (`self.layer2`) will take
    the 64 convolutions of output from the previous layer and then output `64` of
    its own before ReLUing them and max pooling them. Its output will now be `64 ×
    6 × 6` because the `MaxPool` halves the size of the image.    The data is then
    fed to the next layer (`self.fc1`, where `fc` stands for *fully connected*), with
    the input being the shape of the output of the previous layer. The output is 128,
    which is the same number of neurons we used in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    for the deep neural network (DNN).    Finally, these 128 are fed into the final
    layer (`self.fc1`) with 10 outputs—that represent the 10 classes.    ###### Note    In
    the DNN, we ran the input through a `Flatten` layer prior to feeding it into the
    first `Dense` layer. We’ve lost that in the input layer here—instead, we’ve just
    specified the 1-D input shape. Note that prior to the first `Linear` layer, after
    convolutions and pooling, the data will be flattened.    Then, we stack these
    layers in the `forward` function. We can see that we get the data `x` and pass
    it through `layer1` to get `out`, which is passed to `layer2` to get a new `out`.
    At this point, we have the convolutions that we’ve learned, but we need to flatten
    them before loading them into the `Linear` layers `fc1` and `fc2`. The `out =
    out.view(out.size(0), -1)` achieves this.    If we train this network on the same
    data for the same 50 epochs as we used when training the network shown in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    we will see that it works nicely. We can get to 91% accuracy on the test set quite
    easily:    [PRE7]    So, we can see that adding convolutions to the neural network
    definitely increases its ability to classify images. Next, let’s take a look at
    the journey an image takes through the network so we can get a little bit more
    of an understanding of why this process works.    ###### Note    If you are using
    the accompanying code from my GitHub, you’ll notice that I’m using model.to(device)
    a lot. In PyTorch, if an accelerator is available, you can request that the model
    and/or its data use the accelerator with this command.[PRE8]``  [PRE9]` [PRE10]
    # Exploring the Convolutional Network    With the torchsummary library, you can
    inspect your model. When you run it on the Fashion MNIST convolutional network
    we’ve been working on, you’ll see something like this:    [PRE11]py    Let’s first
    take a look at the `Output Shape` column to get an understanding of what’s going
    on here. Our first layer will have 28 × 28 images and apply 64 filters to them.
    But because our filter is 3 × 3, a one-pixel border around the image would typically
    be lost, reducing our overall information to 26 × 26 pixels. However, because
    we used the `padding=1` parameter, the image was artificially inflated to 30 ×
    30, meaning that its output would be the correct 28 × 28 and no information would
    be lost.    If you don’t pad the image, you’ll end up with a result like the one
    in [Figure 3-6](#ch03_figure_6_1748570891060097). If we take each of the boxes
    as a pixel in the image, the first possible filter we can use starts in the second
    row and the second column. The same would happen on the right side and at the
    bottom of the diagram.  ![](assets/aiml_0306.png)  ###### Figure 3-6\. Losing
    pixels when running a filter    Thus, an image that is *a* × *b* pixels in shape
    when run through a 3 × 3 filter will become (*a* – 2) × (*b* – 2) pixels in shape.
    Similarly, a 5 × 5 filter would make it (*a* – 4) × (*b* – 4), and so on. As we’re
    using a 28 × 28 image and a 3 × 3 filter, our output would now be 26 × 26\. But
    because we padded the image up to 30 × 30 (again, to prevent loss of information),
    the output is now 28 × 28.    After that, the pooling layer will be 2 × 2, so
    the size of the image will halve on each axis, and it will then become 14 × 14\.
    The next convolutional layer does *not* use padding, so it will reduce this further
    to 12 × 12, and the next pooling will output 6 x 6.    So, by the time the image
    has gone through two convolutional layers, the result will be many 6 × 6 images.
    How many? We can see that in the `Param #` (number of parameters) column.    Each
    convolution is a 3 × 3 filter, plus a bias. Remember earlier, with our dense layers,
    when each layer was *y* = *wx* + *b*, where *w* was our parameter (aka weight)
    and *b* was our bias? This case is very similar, except that because the filter
    is 3 × 3, there are 9 parameters to learn. Given that we have 64 convolutions
    defined, we’ll have 640 overall parameters. (Each convolution has 9 parameters
    plus a bias, for a total of 10, and there are 64 of them.)    The `ReLU and MaxPooling`
    layers don’t learn anything; they just reduce the image, so there are no learned
    parameters there—hence, 0 are reported.    The next convolutional layer has 64
    filters, but each of them is multiplied across the *previous* 64 filters, each
    of which has 9 parameters. We have a bias on each of the new 64 filters, so our
    number of parameters should be (64 × (64 × 9)) + 64, which gives us 36,928 parameters
    the network needs to learn.    If this is confusing, try changing the number of
    convolutions in the first layer to something else—for example, 10\. You’ll see
    that the number of parameters in the second layer becomes 5,824, which is (64
    × (10 × 9)) + 64).    By the time we get through the second convolution, our images
    are 6 × 6, and we have 64 of them. If we multiply this out, we’ll have 1,600 values,
    which we’ll feed into a dense layer of 128 neurons. Each neuron has a weight and
    a bias, and we’ll have 128 of them, so the number of parameters the network will
    learn is ((6 × 6 × 64) × 128) + 128, giving us 295,040 parameters.    Then, our
    final dense layer of 10 neurons will take in the output of the previous 128, so
    the number of parameters learned will be (128 × 10) + 10, which is 1,290.    The
    total number of parameters will be the sum of all of these: 333,898.    Training
    this network requires us to learn the best set of these 333,898 parameters to
    match the input images to their labels. It’s a slower process because there are
    more parameters, but as we can see from the results, it also builds a more accurate
    model!    Of course, with this dataset, we still have the limitation that the
    images are 28 × 28, monochrome, and centered. So next we’ll take a look at using
    convolutions to explore a more complex dataset comprising color pictures of horses
    and humans, and we’ll try to make the model determine whether an image contains
    one or the other. In this case, the subject won’t always be centered in the image
    like with Fashion MNIST, so we’ll have to rely on convolutions to spot distinguishing
    features.    # Building a CNN to Distinguish Between Horses and Humans    In this
    section, we’ll explore a more complex scenario than the Fashion MNIST classifier.
    We’ll extend what we’ve learned about convolutions and CNNs to try to classify
    the contents of images in which the location of a feature isn’t always in the
    same place. I’ve created the “Horses or Humans” dataset for this purpose.    ##
    The “Horses or Humans” Dataset    [The dataset for this section](https://oreil.ly/8VXwy)
    contains over a thousand 300 × 300–pixel images. Approximately half the images
    are of horses, and the other half are of humans—and all are rendered in different
    poses. You can see some examples in [Figure 3-7](#ch03_figure_7_1748570891060112).  ![](assets/aiml_0307.png)  ######
    Figure 3-7\. Horses and humans    As you can see, the subjects have different
    orientations and poses, and the image composition varies. Consider the two horses,
    for example—their heads are oriented differently, and one image is zoomed out
    (showing the complete animal), while the other is zoomed in (showing just the
    head and part of the body). Similarly, the humans are lit differently, have different
    skin tones, and are posed differently. The man has his hands on his hips, while
    the woman has hers outstretched. The images also contain backgrounds such as trees
    and beaches, so a classifier will have to determine which parts of the image are
    the important features that determine what makes a horse a horse and a human a
    human, without being affected by the background.    While the previous examples
    of predicting *y* = 2*x* – 1 or classifying small monochrome images of clothing
    *might* have been possible with traditional coding, it’s clear that this example
    is far more difficult and that you are crossing the line into where ML is essential
    to solve a problem.    An interesting side note is that these images are all computer
    generated. The theory is that features spotted in a CGI image of a horse should
    apply to a real image, and you’ll see how well this works later in this chapter.    ##
    Handling the Data    The Fashion MNIST dataset that you’ve been using up to this
    point comes with labels, and every image file has an associated file with the
    label details. Many image-based datasets do not have this, and “Horses or Humans”
    is no exception. Instead of labels, the images are sorted into subdirectories
    of each type, and with the DataLoader in PyTorch, you can use this structure to
    *automatically* assign labels to images.    First, you simply need to ensure that
    your directory structure has a set of named subdirectories, with each subdirectory
    being a label. For example, the “Horses or Humans” dataset is available as a set
    of ZIP files, one of which contains the training data (1,000+ images) and another
    of which contains the validation data (256 images). When you download and unpack
    them into a local directory for training and validation, you need to ensure that
    they are in a file structure like the one in [Figure 3-8](#ch03_figure_8_1748570891060128).    Here’s
    the code to get the training data and extract it into the appropriately named
    subdirectories, as shown in [Figure 3-8](#ch03_figure_8_1748570891060128):    [PRE12]py
    `file_name` `=` `"horse-or-human.zip"` `training_dir` `=` `''horse-or-human/training/''`
    `urllib``.``request``.``urlretrieve``(``url``,` `file_name``)`   `zip_ref` `=`
    `zipfile``.``ZipFile``(``file_name``,` `''r''``)` `zip_ref``.``extractall``(``training_dir``)`
    `zip_ref``.``close``()` [PRE13]py  [PRE14] [PRE15] class HorsesHumansCNN(nn.Module):     def
    __init__(self):         super(HorsesHumansCNN, self).__init__()         self.conv1
    = nn.Conv2d(3, 16, kernel_size=3, padding=1)         self.conv2 = nn.Conv2d(16,
    32, kernel_size=3, padding=1)         self.conv3 = nn.Conv2d(32, 64, kernel_size=3,
    padding=1)         self.pool = nn.MaxPool2d(2, 2)         self.fc1 = nn.Linear(64
    * 18 * 18, 512)         self.drop = nn.Dropout(0.25)         self.fc2 = nn.Linear(512,
    1)       def forward(self, x):         x = self.pool(F.relu(self.conv1(x)))         x
    = self.pool(F.relu(self.conv2(x)))         x = self.pool(F.relu(self.conv3(x)))         x
    = x.view(–1, 64 * 18 * 18)         x = F.relu(self.fc1(x))         x = self.drop(x)         x
    = self.fc2(x)         x = torch.sigmoid(x)  # Use sigmoid to output probabilities         return
    x [PRE16] ----------------------------------------------------------------         Layer
    (type)               Output Shape         Param # ================================================================             Conv2d-1         [–1,
    16, 150, 150]             448          MaxPool2d-2           [–1, 16, 75, 75]               0             Conv2d-3           [–1,
    32, 75, 75]           4,640          MaxPool2d-4           [–1, 32, 37, 37]               0             Conv2d-5           [–1,
    64, 37, 37]          18,496          MaxPool2d-6           [–1, 64, 18, 18]               0             Linear-7                  [–1,
    512]      10,617,344            Dropout-8                  [–1, 512]               0             Linear-9                    [–1,
    1]             513 ================================================================
    Total params: 10,641,441 Trainable params: 10,641,441 Non-trainable params: 0
    ---------------------------------------------------------------- Input size (MB):
    0.26 Forward/backward pass size (MB): 5.98 Params size (MB): 40.59 Estimated Total
    Size (MB): 46.83 ----------------------------------------------------------------
    [PRE17] criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001)
    [PRE18] def train_model(num_epochs):     for epoch in range(num_epochs):         model.train()         running_loss
    = 0.0         for images, labels in train_loader:             images, labels =
    images.to(device), labels.to(device).float()               optimizer.zero_grad()             outputs
    = model(images).view(–1)             loss = criterion(outputs, labels)             loss.backward()             optimizer.step()             running_loss
    += loss.item() [PRE19] def train_model(num_epochs):     for epoch in range(num_epochs):         model.train()         running_loss
    = 0.0         for images, labels in train_loader:             images, labels =
    images.to(device), labels.to(device).float()               optimizer.zero_grad()             outputs
    = model(images).view(–1)             loss = criterion(outputs, labels)             loss.backward()             optimizer.step()             running_loss
    += loss.item()           print(f''Epoch {epoch + 1}, Loss: {running_loss /                  len(train_loader)}'')       #
    Evaluate on training set         model.eval()         with torch.no_grad():             correct
    = 0             total = 0             for images, labels in train_loader:                 images,
    labels = images.to(device),                                   labels.to(device).float()                 outputs
    = model(images).view(–1)                 predicted = outputs > 0.5  # Threshold
    predictions                 total += labels.size(0)                 correct +=
    (predicted == labels).sum().item()             print(f''Test Set Accuracy: {100
    * correct / total}%'')           # Evaluate on validation set         model.eval()         with
    torch.no_grad():             correct = 0             total = 0             for
    images, labels in val_loader:                 images, labels = images.to(device),                                   labels.to(device).float()                 outputs
    = model(images).view(–1)                 predicted = outputs > 0.5  # Threshold
    predictions                 total += labels.size(0)                 correct +=
    (predicted == labels).sum().item()             print(f''Validation Set Accuracy:
    {100 * correct / total}%'')       train_model(50) [PRE20] Epoch 7, Loss: 0.0016404045829699512
    Training Set Accuracy: 100.0% Validation Set Accuracy: 88.28125% Epoch 8, Loss:
    0.0010613293736610378 Training Set Accuracy: 100.0% Validation Set Accuracy: 89.0625%
    Epoch 9, Loss: 0.0008372313717332979 Training Set Accuracy: 100.0% Validation
    Set Accuracy: 86.328125% Epoch 10, Loss: 0.0006578459407812646 Training Set Accuracy:
    100.0% Validation Set Accuracy: 87.5% [PRE21] def load_image(image_path, transform):     #
    Load image     image = Image.open(image_path).convert(''RGB'')  # Convert to RGB     #
    Apply transformations     image = transform(image)     # Add batch dimension,
    as the model expects batches     image = image.unsqueeze(0)     return image [PRE22]
    with torch.no_grad():     output = model(image) [PRE23] class_name = "Human" if
    prediction.item() == 1 else "Horse" [PRE24]` [PRE25]`` [PRE26] # Define transformations
    transform = transforms.Compose([     transforms.Resize((150, 150)),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5]) ]) [PRE27] # Transforms for the training data
    train_transforms = transforms.Compose([     transforms.RandomHorizontalFlip(),     transforms.RandomRotation(20),     transforms.RandomResizedCrop(150),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5])   ])   # Transforms for the validation data val_transforms
    = transforms.Compose([     transforms.Resize(150),     transforms.CenterCrop(150),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5]) ]) [PRE28] transforms.RandomAffine(     degrees=0,  #
    No rotation     translate=(0.2, 0.2),  # Translate up to 20% vert and horizontally     scale=(0.8,
    1.2),  # Zoom in or out by 20%     shear=20,  # Shear by up to 20 degrees ), [PRE29]
    import torch import torch.nn as nn from torchvision import models, transforms
    from torch.utils.data import DataLoader from torchvision.datasets import ImageFolder
    from torch.optim import RMSprop   # Load the pretrained Inception V3 model pre_trained_model
    = models.inception_v3(pretrained=True, aux_logits=True) [PRE30] def print_model_summary(model):     for
    name, module in model.named_modules():         print(f"{name} : {module.__class__.__name__}")   #
    Example of how to use the function with your pretrained model print_model_summary(pre_trained_model)
    [PRE31] # Freeze all layers up to and including the ''Mixed_7c'' for name, parameter
    in pre_trained_model.named_parameters():     parameter.requires_grad = False     if
    ''Mixed_7c'' in name:         break [PRE32] # Modify the existing fully connected
    layer num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc = nn.Sequential(     nn.Linear(num_ftrs,
    1024),  # New fully connected layer      nn.ReLU(),                # Activation
    layer     nn.Linear(1024, 2)         # Final layer for binary classification )
    [PRE33]      def load_image(image_path, transform):     # Load image     image
    = Image.open(image_path).convert(''RGB'')  # Convert to RGB      # Apply transformations     image
    = transform(image)     # Add batch dimension, as the model expects batches     image
    = image.unsqueeze(0)     return image      # Prediction function def predict(image_path,
    model, device, transform):     model.eval()     image = load_image(image_path,
    transform)     image = image.to(device)     with torch.no_grad():         output
    = model(image)         print(output)         prediction = torch.max(output, 1)         print(prediction)
    [PRE34] !wget --no-check-certificate \  https://storage.googleapis.com/learning-datasets/rps.zip
    \  -O /tmp/rps.zip local_zip = ''/tmp/rps.zip'' zip_ref = zipfile.ZipFile(local_zip,
    ''r'') zip_ref.extractall(''/tmp/'') zip_ref.close() training_dir = "/tmp/rps/"   train_dataset
    = ImageFolder(root=training_dir, transform=transform) [PRE35] train_loader = DataLoader(train_dataset,
    batch_size=32, shuffle=True) [PRE36] # Modify the existing fully connected layer
    num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc = nn.Sequential(     nn.Linear(num_ftrs,
    1024),  # New fully connected layer      nn.ReLU(),                  # Activation
    layer     nn.Linear(1024, 3)         # Final layer for binary classification )
    [PRE37] # Only optimize parameters that are set to be trainable optimizer = RMSprop(filter(lambda
    p: p.requires_grad,                      pre_trained_model.parameters()), lr=0.001)   criterion
    = nn.CrossEntropyLoss()   # Train the model train_model(pre_trained_model, criterion,
    optimizer, train_loader, num_epochs=3) [PRE38] def load_image(image_path, transform):     #
    Load image     image = Image.open(image_path).convert(''RGB'')  # Convert to RGB      #
    Apply transformations     image = transform(image)     # Add batch dimension,
    as the model expects batches     image = image.unsqueeze(0)     return image       #
    Prediction function def predict(image_path, model, device, transform):     model.eval()     image
    = load_image(image_path, transform)     image = image.to(device)     with torch.no_grad():         output
    = model(image)         print(output)         prediction = torch.max(output, 1)         print(prediction)
    [PRE39] nn.Dropout(0.5) [PRE40] num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc
    = nn.Sequential(     nn.Linear(num_ftrs, 1024),  # New fully connected layer      nn.ReLU(),                #
    Activation layer     nn.Linear(1024, 3)         # Final layer for RPS ) [PRE41]
    num_ftrs = model.fc.in_features model.fc = nn.Sequential(     nn.Dropout(0.5),  #
    Adding dropout before the final FC layer     nn.Linear(num_ftrs, 1024),  # Reduce
    dimensionality to 1024     nn.ReLU(),     nn.Dropout(0.5),  # Adding another dropout
    layer after ReLU activation     nn.Linear(1024, 3)  # Final layer for RPS ) [PRE42]`
    [PRE43][PRE44]````'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]`要将此转换为CNN，你只需在我们的模型定义中在当前的线性层之上使用卷积层。你还会添加池化层。要实现卷积层，你将使用`nn.Conv2D`类型。它接受作为参数的层中要使用的卷积数量、卷积大小、激活函数等。例如，这里有一个使用此类型的卷积层：[PRE4]。在这种情况下，我们希望层学习`64`个卷积。它将随机初始化它们，随着时间的推移，它将学习最佳的滤波器值，以匹配输入值和它们的标签。`kernel_size
    = 3`表示滤波器的大小。之前，我们向您展示了3 × 3滤波器，这就是我们在这里指定的。3 × 3滤波器是最常见的滤波器大小。你可以根据需要更改它，但通常你会看到像5
    × 5或7 × 7这样的奇数轴，因为滤波器会从图像的边缘移除像素，就像你稍后看到的那样。[PRE5]在[图3-4](#ch03_figure_4_1748570891060063)中的示例中，我们将图像分割成2
    × 2池，并选择每个池中的最大值。然而，我们可以使用你在这里看到的参数来定义池大小。`kernel_size=2`参数表示我们的池是2 × 2，而`stride=2`参数表示滤波器将跳过两个像素以获取下一个池。[PRE6]现在，让我们探索完整的代码，以定义一个用于Fashion
    MNIST的CNN模型：[PRE7]。在这里，我们看到类有两个函数，一个用于初始化，一个在训练过程中每个epoch的前向传递期间将被调用。[init]简单地定义了我们神经网络中的每一层将是什么样子。第一层(`self.layer1`)将接受一维输入，有`64`个卷积，`kernel_size`为`3`，`padding`为`1`。然后它将在最大池化之前ReLU输出。下一层(`self.layer2`)将接受来自前一层的`64`个卷积输出，然后输出`64`个自己的输出，在ReLU和最大池化之前。由于`MaxPool`将图像大小减半，其输出现在将是`64
    × 6 × 6`。数据随后被馈送到下一层(`self.fc1`，其中`fc`代表*全连接*)，输入是前一层的输出形状。输出是128，这与我们在[第2章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中用于深度神经网络(DNN)的神经元数量相同。最后，这128个被馈送到最终层(`self.fc1`)，有10个输出——代表10个类别。####
    注意在DNN中，我们在将输入馈送到第一个`Dense`层之前，通过`Flatten`层运行输入。我们在这里丢失了它——相反，我们只是指定了1-D输入形状。注意，在卷积和池化之后，在第一个`Linear`层之前，数据将被展平。然后，我们在`forward`函数中堆叠这些层。我们可以看到我们得到数据`x`，并通过`layer1`传递它以获得`out`，然后将其传递到`layer2`以获得新的`out`。在这个时候，我们有我们学到的卷积，但我们需要在将它们加载到`Linear`层`fc1`和`fc2`之前将它们展平。`out
    = out.view(out.size(0), -1)`实现了这一点。如果我们用与我们在[第2章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中训练网络时相同的50个epoch来训练这个网络，我们将看到它工作得很好。我们可以在测试集上相当容易地达到91%的准确率：[PRE8]。因此，我们可以看到向神经网络添加卷积确实增加了其分类图像的能力。接下来，让我们看看图像在网络中经历的旅程，这样我们就可以更深入地了解为什么这个过程有效。####
    注意如果你使用GitHub上的配套代码，你会注意到我大量使用`model.to(device)`。在PyTorch中，如果可用加速器，你可以使用此命令请求模型和/或其数据使用加速器。[PRE9]`[PRE10]#
    探索卷积网络使用torchsummary库，你可以检查你的模型。当你运行它在我们一直在工作的Fashion MNIST卷积网络上时，你会看到类似这样的东西：[PRE11]。让我们首先看看`Output
    Shape`列，以了解这里发生了什么。我们的第一层将具有28 × 28图像，并对其应用64个滤波器。但由于我们的滤波器是3 × 3，图像周围的一个像素边框通常会丢失，使我们的总信息减少到26
    × 26像素。然而，因为我们使用了`padding=1`参数，图像被人工膨胀到30 × 30，这意味着其输出将是正确的28 × 28，并且不会丢失信息。如果你不填充图像，你最终会得到[图3-6](#ch03_figure_6_1748570891060097)中的结果。如果我们把每个框当作图像中的一个像素，第一个可能的滤波器可以从第二行和第二列开始。同样，在图的右侧和底部也会发生这种情况。![图3-6\.
    运行滤波器时丢失像素](assets/aiml_0306.png)#### 图3-6\. 运行滤波器时丢失像素因此，一个形状为*a* × *b*像素的图像在通过3
    × 3滤波器时将变成(*a* – 2) × (*b* – 2)像素的形状。同样，5 × 5滤波器将使其变成(*a* – 4) × (*b* – 4)，依此类推。因为我们使用的是28
    × 28图像和3 × 3滤波器，所以我们的输出现在将是26 × 26。但由于我们填充了图像到30 × 30（再次，为了防止信息丢失），输出现在是28 × 28。在那之后，池化层将是2
    × 2，所以图像在每个轴上的大小将减半，然后变成14 × 14。下一个卷积层不使用填充，所以它将进一步将其减少到12 × 12，下一个池化将输出6 x 6。因此，当图像通过两个卷积层时，结果将是许多6
    × 6图像。有多少个？我们可以在`Param #`（参数数量）列中看到。每个卷积是一个3 × 3滤波器，加上一个偏差。记住，在我们之前的密集层中，每个层都是*y*
    = *wx* + *b*，其中*w*是我们的参数（也称为权重）和*b*是我们的偏差？这个情况非常相似，只是因为滤波器是3 × 3，所以有9个参数要学习。鉴于我们定义了64个卷积，我们将有640个总参数。（每个卷积有9个参数加上一个偏差，总共10个，有64个。）`ReLU和MaxPooling`层不学习任何东西；它们只是减少图像，所以那里没有学习的参数——因此，报告了0个。下一个卷积层有64个滤波器，但每个滤波器都乘以*前面的*64个滤波器，每个滤波器都有9个参数。我们在每个新的64个滤波器上都有一个偏差，所以我们的参数数量应该是(64
    × (64 × 9)) + 64，这给我们36,928个网络需要学习的参数。如果这很令人困惑，尝试将第一层的卷积数量更改为其他值——例如，10。你会看到第二层的参数数量变为5,824，这是(64
    × (10 × 9)) + 64)。当我们通过第二个卷积时，我们的图像是6 × 6，我们有64个这样的图像。如果我们乘以这个数字，我们将有1,600个值，我们将它们馈送到一个有128个神经元的密集层。每个神经元都有一个权重和一个偏差，我们将有128个这样的神经元，所以网络需要学习的参数数量是((6
    × 6 × 64) × 128) + 128，这给我们295,040个参数。然后，我们的最终密集层有10个神经元，它将接受来自前一层的128个输出，所以学习的参数数量将是(128
    × 10) + 10，即1,290。总参数数量将是所有这些的总和：333,898。训练这个网络需要我们学习最佳的333,898个参数集，以匹配输入图像和它们的标签。这是一个较慢的过程，因为参数更多，但正如我们从结果中可以看到的那样，它也构建了一个更准确的模型！当然，对于这个数据集，我们仍然有图像是28
    × 28、单色和居中的限制。因此，接下来我们将看看如何使用卷积来探索一个更复杂的数据集，该数据集包含马和人的彩色图片，我们将尝试使模型确定图像是否包含一个或另一个。在这种情况下，主题不会像Fashion
    MNIST那样总是在图像中居中，所以我们将不得不依赖于卷积来识别区分特征。# 使用CNN区分马和人在这部分，我们将探索比Fashion MNIST分类器更复杂的场景。我们将扩展我们对卷积和CNN的了解，以尝试对图像内容进行分类，其中特征的位置不总是在相同的位置。我为此目的创建了“Horses
    or Humans”数据集。## “Horses or Humans”数据集[本节的数据集](https://oreil.ly/8VXwy)包含超过一千张300
    × 300像素的图像。大约一半的图像是马，另一半是人——并且它们都以不同的姿势呈现。你可以在[图3-7](#ch03_figure_7_1748570891060112)中看到一些示例。![图3-7\.
    马和人](assets/aiml_0307.png)#### 图3-7\. 马和人，如你所见，主题有不同的方向和姿势，图像构图也各不相同。以两匹马为例，例如——它们的头部方向不同，一张图像被缩小了（显示整个动物），而另一'
