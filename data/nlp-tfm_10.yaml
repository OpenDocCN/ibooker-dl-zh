- en: Chapter 9\. Dealing with Few to No Labels
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。处理少量或没有标签
- en: 'There is one question so deeply ingrained into every data scientist’s mind
    that it’s usually the first thing they ask at the start of a new project: is there
    any labeled data? More often than not, the answer is “no” or “a little bit,” followed
    by an expectation from the client that your team’s fancy machine learning models
    should still perform well. Since training models on very small datasets does not
    typically yield good results, one obvious solution is to annotate more data. However,
    this takes time and can be very expensive, especially if each annotation requires
    domain expertise to validate.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个问题深深地植根在每个数据科学家的脑海中，通常是他们在新项目开始时首先问的事情：是否有任何标记的数据？往往情况是“没有”或“有一点”，然后客户期望你的团队的高级机器学习模型仍然能够表现良好。由于在非常小的数据集上训练模型通常不会产生良好的结果，一个明显的解决方案是标注更多的数据。然而，这需要时间，而且可能非常昂贵，特别是如果每个标注都需要领域专业知识来验证。
- en: Fortunately, there are several methods that are well suited for dealing with
    few to no labels! You may already be familiar with some of them, such as *zero-shot*
    or *few-shot learning*, as witnessed by GPT-3’s impressive ability to perform
    a diverse range of tasks with just a few dozen examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有几种方法非常适合处理少量或没有标签的情况！你可能已经熟悉其中一些，比如*零样本*或*少样本学习*，正如GPT-3仅凭几十个例子就能执行各种任务的能力所示。
- en: In general, the best-performing method will depend on the task, the amount of
    available data, and what fraction of that data is labeled. The decision tree shown
    in [Figure 9-1](#decision-tree) can help guide us through the process of picking
    the most appropriate method.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，最佳的方法取决于任务、可用数据量以及该数据中有多少是标记的。[图9-1](#decision-tree)中显示的决策树可以帮助我们在选择最合适的方法的过程中进行指导。
- en: '![decision-tree](Images/nlpt_0901.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![decision-tree](Images/nlpt_0901.png)'
- en: Figure 9-1\. Several techniques that can be used to improve model performance
    in the absence of large amounts of labeled data
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。在缺乏大量标记数据的情况下，可以用来提高模型性能的几种技术
- en: 'Let’s walk through this decision tree step-by-step:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这个决策树：
- en: '*1\. Do you have labeled data?*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 你有标记的数据吗？
- en: Even a handful of labeled samples can make a difference with regard to which
    method works best. If you have no labeled data at all, you can start with the
    zero-shot learning approach, which often sets a strong baseline to work from.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有少量标记的样本，也可以对哪种方法最有效产生影响。如果根本没有标记的数据，可以从零样本学习方法开始，这通常可以为后续工作奠定坚实的基础。
- en: '*2\. How many labels?*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 有多少标签？
- en: If labeled data is available, the deciding factor is how much. If you have a
    lot of training data available you can use the standard fine-tuning approach discussed
    in [Chapter 2](ch02.xhtml#chapter_classification).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有标记的数据可用，决定因素是有多少。如果你有大量的训练数据可用，你可以使用[第2章](ch02.xhtml#chapter_classification)中讨论的标准微调方法。
- en: '*3\. Do you have unlabeled data?*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 你有未标记的数据吗？
- en: If you only have a handful of labeled samples it can help immensely if you have
    access to large amounts of unlabeled data. If you have access to unlabeled data
    you can either use it to fine-tune the language model on the domain before training
    a classifier, or you can use more sophisticated methods such as unsupervised data
    augmentation (UDA) or uncertainty-aware self-training (UST).^([1](ch09.xhtml#idm46238704545312))
    If you don’t have any unlabeled data available, you don’t have the option of annotating
    more data. In this case you can use few-shot learning or use the embeddings from
    a pretrained language model to perform lookups with a nearest neighbor search.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有少量标记的样本，如果你可以访问大量未标记的数据，这将非常有帮助。如果你可以访问未标记的数据，你可以在训练分类器之前使用它来微调语言模型，或者你可以使用更复杂的方法，如无监督数据增强（UDA）或不确定性感知的自我训练（UST）。^([1](ch09.xhtml#idm46238704545312))
    如果你没有任何未标记的数据可用，你就没有标注更多数据的选择。在这种情况下，你可以使用少样本学习，或者使用预训练语言模型的嵌入来执行最近邻搜索。
- en: 'In this chapter we’ll work our way through this decision tree by tackling a
    common problem facing many support teams that use issue trackers like [Jira](https://oreil.ly/TVqZQ)
    or [GitHub](https://oreil.ly/e0Bd1) to assist their users: tagging issues with
    metadata based on the issue’s description. These tags might define the issue type,
    the product causing the problem, or which team is responsible for handling the
    reported issue. Automating this process can have a big impact on productivity
    and enables the support teams to focus on helping their users. As a running example,
    we’ll use the GitHub issues associated with a popular open source project: ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers! Let’s now take a look at what information is contained in these
    issues, how to frame the task, and how to get the data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过解决许多支持团队面临的常见问题来逐步走过这个决策树，这些团队使用像[Jira](https://oreil.ly/TVqZQ)或[GitHub](https://oreil.ly/e0Bd1)这样的问题跟踪器来帮助他们的用户：根据问题的描述为问题打标签。这些标签可能定义问题类型、导致问题的产品，或者负责处理报告问题的团队。自动化这个过程可以对生产力产生重大影响，并使支持团队能够专注于帮助他们的用户。作为一个运行的示例，我们将使用与一个流行的开源项目相关的GitHub问题：![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers！现在让我们来看看这些问题中包含了哪些信息，如何构建任务，以及如何获取数据。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The methods presented in this chapter work well for text classification, but
    other techniques such as data augmentation may be necessary for tackling more
    complex tasks like named entity recognition, question answering, or summarization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的方法对于文本分类非常有效，但对于更复杂的任务，如命名实体识别、问答或摘要，可能需要其他技术，如数据增强。
- en: Building a GitHub Issues Tagger
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建GitHub问题标记器
- en: 'If you navigate to the [Issues tab](https://oreil.ly/StdH3) of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers repository, you’ll find issues like the one shown in [Figure 9-2](#issue-example),
    which contains a title, a description, and a set of tags or labels that characterize
    the issue. This suggests a natural way to frame the supervised learning task:
    given a title and description of an issue, predict one or more labels. Since each
    issue can be assigned a variable number of labels, this means we are dealing with
    a *multilabel text classification* problem. This is usually more challenging than
    the multiclass problem that we encountered in [Chapter 2](ch02.xhtml#chapter_classification),
    where each tweet was assigned to only one emotion.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您导航到![nlpt_pin01](Images/nlpt_pin01.png) Transformers存储库的[Issues标签](https://oreil.ly/StdH3)，您会发现像[图9-2](#issue-example)中所示的问题，其中包含标题、描述和一组标签，这些标签或标签表征了问题。这表明了一个自然的方式来构建监督学习任务：给定一个问题的标题和描述，预测一个或多个标签。由于每个问题可以分配一个可变数量的标签，这意味着我们正在处理一个*多标签文本分类*问题。这通常比我们在[第2章](ch02.xhtml#chapter_classification)中遇到的多类问题更具挑战性，那里每个推文只分配给一个情感。
- en: '![issue-example](Images/nlpt_0902.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![issue-example](Images/nlpt_0902.png)'
- en: Figure 9-2\. A typical GitHub issue on the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers repository
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。![nlpt_pin01](Images/nlpt_pin01.png) Transformers存储库上的典型GitHub问题
- en: Now that we’ve seen what the GitHub issues look like, let’s see how we can download
    them to create our dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了GitHub问题的样子，让我们看看如何下载它们以创建我们的数据集。
- en: Getting the Data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: To grab all the repository’s issues, we’ll use the [GitHub REST API](https://oreil.ly/q605k)
    to poll the [`Issues` endpoint](https://oreil.ly/qXdWV). This endpoint returns
    a list of JSON objects, with each containing a large number of fields about the
    issue at hand, including its state (open or closed), who opened the issue, as
    well as the title, body, and labels we saw in [Figure 9-2](#issue-example).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取存储库的所有问题，我们将使用[GitHub REST API](https://oreil.ly/q605k)来轮询[`Issues`端点](https://oreil.ly/qXdWV)。这个端点返回一个JSON对象列表，每个对象包含有关问题的大量字段，包括其状态（打开或关闭），谁打开了问题，以及我们在[图9-2](#issue-example)中看到的标题、正文和标签。
- en: Since it takes a while to fetch all the issues, we’ve included a *github-issues-transformers.jsonl*
    file in this book’s [GitHub repository](https://oreil.ly/if2dm), along with a
    `fetch_issues()` function that you can use to download them yourself.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于获取所有问题需要一些时间，我们在本书的[GitHub存储库](https://oreil.ly/if2dm)中包含了一个*github-issues-transformers.jsonl*文件，以及一个`fetch_issues()`函数，您可以使用它来自行下载问题。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The GitHub REST API treats pull requests as issues, so our dataset contains
    a mix of both. To keep things simple, we’ll develop our classifier for both types
    of issue, although in practice you might consider building two separate classifiers
    to have more fine-grained control over the model’s performance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub REST API将拉取请求视为问题，因此我们的数据集包含两者的混合。为了保持简单，我们将为两种问题类型开发我们的分类器，尽管在实践中，您可能考虑构建两个单独的分类器，以更精细地控制模型的性能。
- en: Now that we know how to grab the data, let’s take a look at cleaning it up.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何获取数据，让我们来看看如何清理数据。
- en: Preparing the Data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Once we’ve downloaded all the issues, we can load them using Pandas:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们下载了所有问题，我们可以使用Pandas加载它们：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are almost 10,000 issues in our dataset, and by looking at a single row
    we can see that the information retrieved from the GitHub API contains many fields
    such as URLs, IDs, dates, users, title, body, as well as labels:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中有近10000个问题，通过查看单个行，我们可以看到从GitHub API检索到的信息包含许多字段，如URL、ID、日期、用户、标题、正文以及标签：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|  | 2 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | 2 |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| url | https://api.github.com/repos/huggingface/trans... |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| url | https://api.github.com/repos/huggingface/trans... |'
- en: '| id | 849529761 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| id | 849529761 |'
- en: '| title | [DeepSpeed] ZeRO stage 3 integration: getting ... |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| title | [DeepSpeed] ZeRO stage 3 integration: getting ... |'
- en: '| user | {''login’: ’stas00'', ‘id’: 10676103, ‘node_id’:... |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| user | {''login’: ’stas00'', ‘id’: 10676103, ‘node_id’:... |'
- en: '| labels | [{''id’: 2659267025, ‘node_id’: ‘MDU6TGFiZWwyNj... |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| labels | [{''id’: 2659267025, ‘node_id’: ‘MDU6TGFiZWwyNj... |'
- en: '| state | open |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| state | open |'
- en: '| created_at | 2021-04-02 23:40:42 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| created_at | 2021-04-02 23:40:42 |'
- en: '| body | **[This is not yet alive, preparing for the re... |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| body | **[This is not yet alive, preparing for the re... |'
- en: 'The `labels` column is the thing that we’re interested in, and each row contains
    a list of JSON objects with metadata about each label:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`列是我们感兴趣的东西，每一行都包含一个关于每个标签的元数据的JSON对象列表：'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For our purposes, we’re only interested in the `name` field of each label object,
    so let’s overwrite the `labels` column with just the label names:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们只对每个标签对象的`name`字段感兴趣，因此让我们用标签名称覆盖`labels`列：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | labels |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | labels |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | [] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 0 | [] |'
- en: '| 1 | [] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | [] |'
- en: '| 2 | [DeepSpeed] |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 2 | [DeepSpeed] |'
- en: '| 3 | [] |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 3 | [] |'
- en: '| 4 | [] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 4 | [] |'
- en: 'Now each row in the `labels` column is a list of GitHub labels, so we can compute
    the length of each row to find the number of labels per issue:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`labels`列中的每一行都是GitHub标签的列表，因此我们可以计算每一行的长度，以找到每个问题的标签数量：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| labels | 6440 | 3057 | 305 | 100 | 25 | 3 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| labels | 6440 | 3057 | 305 | 100 | 25 | 3 |'
- en: 'This shows that the majority of issues have zero or one label, and much fewer
    have more than one. Next let’s take a look at the top 10 most frequent labels
    in the dataset. In Pandas we can do this by “exploding” the `labels` column so
    that each label in the list becomes a row, and then simply counting the occurrences
    of each label:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明大多数问题没有标签或只有一个标签，而更少的问题有多个标签。接下来让我们来看看数据集中前10个最频繁的标签。在Pandas中，我们可以通过“爆炸”`labels`列来做到这一点，使列表中的每个标签成为一行，然后简单地计算每个标签的出现次数：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  | wontfix | model card | Core: Tokenization | New model | Core: Modeling
    | Help wanted | Good First Issue | Usage |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | wontfix | model card | Core: Tokenization | New model | Core: Modeling
    | Help wanted | Good First Issue | Usage |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| labels | 2284 | 649 | 106 | 98 | 64 | 52 | 50 | 46 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| labels | 2284 | 649 | 106 | 98 | 64 | 52 | 50 | 46 |'
- en: We can see that there are 65 unique labels in the dataset and that the classes
    are very imbalanced, with `wontfix` and `model card` being the most common labels.
    To make the classification problem more tractable, we’ll focus on building a tagger
    for a subset of the labels. For example, some labels, such as `Good First Issue`
    or `Help Wanted`, are potentially very difficult to predict from the issue’s description,
    while others, such as `model card`, could be classified with a simple rule that
    detects when a model card is added on the Hugging Face Hub.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据集中有65个唯一的标签，并且类别非常不平衡，`wontfix`和`model card`是最常见的标签。为了使分类问题更易处理，我们将专注于构建一部分标签的标签器。例如，一些标签，如`Good
    First Issue`或`Help Wanted`，可能非常难以从问题的描述中预测，而其他一些标签，如`model card`，可以通过简单的规则进行分类，以检测何时在Hugging
    Face Hub上添加了模型卡。
- en: 'The following code filters the dataset for the subset of labels that we’ll
    work with, along with a standardization of the names to make them easier to read:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将过滤数据集，以便我们将使用的标签子集，以及对名称的标准化，使其更易于阅读：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let’s look at the distribution of the new labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下新标签的分布：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|  | tokenization | new model | model training | usage | pipeline | tensorflow
    or tf | pytorch | documentation | examples |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | tokenization | new model | model training | usage | pipeline | tensorflow
    or tf | pytorch | documentation | examples |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| labels | 106 | 98 | 64 | 46 | 42 | 41 | 37 | 28 | 24 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 106 | 98 | 64 | 46 | 42 | 41 | 37 | 28 | 24 |'
- en: 'Later in this chapter we’ll find it useful to treat the unlabeled issues as
    a separate training split, so let’s create a new column that indicates whether
    the issue is unlabeled or not:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们会发现将未标记的问题视为单独的训练拆分是有用的，因此让我们创建一个新列，指示问题是否被标记：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|  | split |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 拆分 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| unlabeled | 9489 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 未标记 | 9489 |'
- en: '| labeled | 441 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 441 |'
- en: 'Let’s now take a look at an example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一个例子：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example a new model architecture is proposed, so the `new model` tag
    makes sense. We can also see that the `title` contains information that will be
    useful for our classifier, so let’s concatenate it with the issue’s description
    in the `body` field:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，提出了一个新的模型架构，因此`new model`标签是有意义的。我们还可以看到`title`包含了对我们的分类器有用的信息，因此让我们将其与`body`字段中的问题描述连接起来：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Before we look at the rest of the data, let’s check for any duplicates in the
    data and drop them with the `drop_duplicates()` method:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看数据的其余部分之前，让我们检查数据中是否有重复项，并使用`drop_duplicates()`方法删除它们：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can see that there were a few duplicate issues in our dataset, but they
    only represented a small percentage. As we’ve done in other chapters, it’s also
    a good idea to have a quick look at the number of words in our texts to see if
    we’ll lose much information when we truncate to each model’s context size:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的数据集中有一些重复的问题，但它们只占很小的比例。与其他章节一样，快速查看我们文本中的单词数量也是一个好主意，以查看当我们截断到每个模型的上下文大小时是否会丢失太多信息：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](Images/nlpt_09in01.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in01.png)'
- en: The distribution has the long tail characteristic of many text datasets. Most
    of the texts are fairly short, but there are also issues with more than 500 words.
    It is common to have some very long issues, especially when error messages and
    code snippets are posted along with them. Given that most transformer models have
    a context size of 512 tokens or larger, truncating a handful of long issues is
    not likely to affect the overall performance. Now that we’ve explored and cleaned
    up our dataset, the final thing to do is define our training and validation sets
    to benchmark our classifiers. Let’s take a look at how to do this.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分布具有许多文本数据集的长尾特征。大多数文本都相当短，但也有超过500个单词的问题。通常会有一些非常长的问题，特别是当错误消息和代码片段与它们一起发布时。鉴于大多数转换器模型的上下文大小为512个标记或更大，截断少数长问题不太可能影响整体性能。现在我们已经探索和清理了我们的数据集，最后要做的是定义我们的训练和验证集，以对我们的分类器进行基准测试。让我们看看如何做到这一点。
- en: Creating Training Sets
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练集
- en: 'Creating training and validation sets is a bit trickier for multlilabel problems
    because there is no guaranteed balance for all labels. However, it can be approximated,
    and we can use the [Scikit-multilearn library](http://scikit.ml), which is specifically
    set up for this purpose. The first thing we need to do is transform our set of
    labels, like `pytorch` and `tokenization`, into a format that the model can process.
    Here we can use Scikit-learn’s `Multi​La⁠bel​Binarizer` class, which takes a list
    of label names and creates a vector with zeros for absent labels and ones for
    present labels. We can test this by fitting `Multi​La⁠bel​Binarizer` on `all_labels`
    to learn the mapping from label name to ID as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多标签问题，创建训练和验证集会有些棘手，因为并不是所有标签都能保证平衡。然而，可以进行近似处理，我们可以使用专门为此目的设置的[Scikit-multilearn库](http://scikit.ml)。我们需要做的第一件事是将我们的标签集（如`pytorch`和`tokenization`）转换为模型可以处理的格式。在这里，我们可以使用Scikit-learn的`MultiLabelBinarizer`类，它接受一个标签名称列表，并创建一个向量，其中缺失的标签为零，存在的标签为一。我们可以通过将`MultiLabelBinarizer`拟合到`all_labels`上来测试这一点，以学习从标签名称到ID的映射，如下所示：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this simple example we can see the first row has two ones corresponding to
    the `tokenization` and `new model` labels, while the second row has just one hit
    with `pytorch`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们可以看到第一行有两个对应于`tokenization`和`new model`标签的1，而第二行只有一个对应于`pytorch`的命中。
- en: 'To create the splits we can use the `iterative_train_test_split()` function
    from Scikit-multilearn, which creates the train/test splits iteratively to achieve
    balanced labels. We wrap it in a function that we can apply to `DataFrame`s. Since
    the function expects a two-dimensional feature matrix, we need to add a dimension
    to the possible indices before making the split:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '为了创建拆分，我们可以使用Scikit-multilearn的`iterative_train_test_split()`函数，该函数会迭代创建平衡标签的训练/测试拆分。我们将其包装在一个可以应用于`DataFrame`的函数中。由于该函数期望一个二维特征矩阵，因此在进行拆分之前，我们需要为可能的索引添加一个维度： '
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Armed with the `balanced_split()` function, we can split the data into supervised
    and unsupervised datasets, and then create balanced training, validation, and
    test sets for the supervised part:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`balanced_split()`函数，我们可以将数据分成监督和非监督数据集，然后为监督部分创建平衡的训练、验证和测试集：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, let’s create a `DatasetDict` with all the splits so that we can easily
    tokenize the dataset and integrate with the `Trainer`. Here we’ll use the nifty
    `from_pandas()` method to load each split directly from the corresponding Pandas
    `DataFrame`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个`DatasetDict`，包含所有的拆分，这样我们就可以轻松地对数据集进行标记，并与`Trainer`集成。在这里，我们将使用巧妙的`from_pandas()`方法直接从相应的Pandas`DataFrame`中加载每个拆分：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This looks good, so the last thing to do is to create some training slices so
    that we can evaluate the performance of each classifier as a function of the training
    set size.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错，最后要做的事情就是创建一些训练切片，这样我们就可以评估每个分类器的性能，作为训练集大小的函数。
- en: Creating Training Slices
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练切片
- en: 'The dataset has the two characteristics that we’d like to investigate in this
    chapter: sparse labeled data and multilabel classification. The training set consists
    of only 220 examples to train with, which is certainly a challenge even with transfer
    learning. To drill down into how each method in this chapter performs with little
    labeled data, we’ll also create slices of the training data with even fewer samples.
    We can then plot the number of samples against the performance and investigate
    various regimes. We’ll start with only eight samples per label and build up until
    the slice covers the full training set using the `iterative_train_test_split()`
    function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集具有我们想要在本章中调查的两个特征：稀疏标记数据和多标签分类。训练集只包含220个示例进行训练，即使使用迁移学习也是一个挑战。为了深入研究本章中每种方法在少量标记数据下的表现，我们还将创建训练数据的切片，其中包含更少的样本。然后，我们可以绘制样本数量与性能，并调查各种情况。我们将从每个标签仅有八个样本开始，并逐渐增加，直到切片覆盖整个训练集，使用`iterative_train_test_split()`函数：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that this iterative approach only approximately splits the samples to
    the desired size, since it is not always possible to find a balanced split at
    a given split size:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种迭代方法只是大致将样本分割成所需的大小，因为在给定的拆分大小下，不总是可能找到一个平衡的拆分：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We’ll use the specified split sizes as the labels for the following plots. Great,
    we’ve finally prepared our dataset into training splits—let’s next take a look
    at training a strong baseline model!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用指定的拆分大小作为以下图表的标签。太好了，我们终于将我们的数据集准备成了训练拆分，接下来让我们看看如何训练一个强大的基线模型！
- en: Implementing a Naive Bayesline
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施一个朴素贝叶斯基线
- en: 'Whenever you start a new NLP project, it’s always a good idea to implement
    a set of strong baselines. There are two main reasons for this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你开始一个新的NLP项目时，实施一组强大的基线总是一个好主意。这样做有两个主要原因：
- en: A baseline based on regular expressions, handcrafted rules, or a very simple
    model might already work really well to solve the problem. In these cases, there
    is no reason to bring out big guns like transformers, which are generally more
    complex to deploy and maintain in production environments.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于正则表达式、手工制作的规则或非常简单的模型的基线可能已经非常有效地解决了问题。在这些情况下，没有理由使用transformers等大型工具，这些工具通常在生产环境中更复杂。
- en: The baselines provide quick checks as you explore more complex models. For example,
    suppose you train BERT-large and get an accuracy of 80% on your validation set.
    You might write it off as a hard dataset and call it a day. But what if you knew
    that a simple classifier like logistic regression gets 95% accuracy? That would
    raise your suspicions and prompt you to debug your model.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基线提供了快速检查，当你探索更复杂的模型时。例如，假设你训练BERT-large并在验证集上获得80%的准确率。你可能会认为这是一个难题，然后结束了。但是如果你知道一个简单的分类器如逻辑回归获得了95%的准确率呢？那就会引起你的怀疑，并促使你调试你的模型。
- en: 'So let’s start our analysis by training a baseline model. For text classification,
    a great baseline is a *Naive Bayes classifier* as it is very simple, quick to
    train, and fairly robust to perturbations in the inputs. The Scikit-learn implementation
    of Naive Bayes does not support multilabel classification out of the box, but
    fortunately we can again use the Scikit-multilearn library to cast the problem
    as a one-versus-rest classification task where we train *L* binary classifiers
    for *L* labels. First, let’s use a multilabel binarizer to create a new `label_ids`
    column in our training sets. We can use the `map()` function to take care of all
    the processing in one go:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从训练一个基线模型开始我们的分析。对于文本分类，一个很好的基线是*朴素贝叶斯分类器*，因为它非常简单、训练速度快，并且对输入的扰动相当稳健。Scikit-learn的朴素贝叶斯实现不直接支持多标签分类，但幸运的是，我们可以再次使用Scikit-multilearn库，将问题转化为一个一对多的分类任务，其中我们为*L*标签训练*L*个二元分类器。首先，让我们使用一个多标签二值化器在我们的训练集中创建一个新的`label_ids`列。我们可以使用`map()`函数一次性处理所有的处理：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To measure the performance of our classifiers, we’ll use the micro and macro
    *F*[1]-scores, where the former tracks performance on the frequent labels and
    the latter on all labels disregarding the frequency. Since we’ll be evaluating
    each model across different-sized training splits, let’s create a `defaultdict`
    with a list to store the scores per split:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量我们分类器的性能，我们将使用微观和宏观*F*[1]-scores，前者跟踪频繁标签的性能，后者忽略频率，跟踪所有标签的性能。由于我们将评估每个模型在不同大小的训练拆分上，让我们创建一个`defaultdict`，其中包含一个列表，用于存储每个拆分的分数：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we’re finally ready to train our baseline! Here’s the code to train the
    model and evaluate our classifier across increasing training set sizes:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于准备好训练我们的基线了！下面是训练模型和评估我们的分类器在不断增加的训练集大小上的代码：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There’s quite a lot going on in this block of code, so let’s unpack it. First,
    we get the training slice and encode the labels. Then we use a count vectorizer
    to encode the texts by simply creating a vector of the size of the vocabulary
    where each entry corresponds to the frequency with which a token appeared in the
    text. This is called a *bag-of-words* approach, since all information on the order
    of the words is lost. Then we train the classifier and use the predictions on
    the test set to get the micro and macro *F*[1]-scores via the classification report.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码块中有很多内容，让我们来解开它。首先，我们获取训练切片并对标签进行编码。然后我们使用计数向量化器对文本进行编码，简单地创建一个与词汇量大小相同的向量，其中每个条目对应于文本中标记出现的频率。这被称为*词袋*方法，因为所有关于单词顺序的信息都丢失了。然后我们训练分类器，并使用测试集上的预测来通过分类报告得到微观和宏观*F*[1]-分数。
- en: 'With the following helper function we can plot the results of this experiment:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下辅助函数，我们可以绘制这个实验的结果：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](Images/nlpt_09in02.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in02.png)'
- en: Note that we plot the number of samples on a logarithmic scale. From the figure
    we can see that the micro and macro *F*[1]-scores both improve as we increase
    the number of training samples. With so few samples to train on, the results are
    also slightly noisy since each slice can have a different class distribution.
    Nevertheless, what’s important here is the trend, so let’s now see how these results
    fare against transformer-based approaches!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在对数刻度上绘制样本数量。从图中我们可以看到，随着训练样本数量的增加，微观和宏观*F*[1]-分数都有所提高。由于每个切片可能具有不同的类分布，因此在训练样本很少的情况下，结果也稍微有些嘈杂。然而，这里重要的是趋势，所以现在让我们看看这些结果与基于变压器的方法相比如何！
- en: Working with No Labeled Data
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无标记数据
- en: The first technique that we’ll consider is *zero-shot classification*, which
    is suitable in settings where you have no labeled data at all. This is surprisingly
    common in industry, and might occur because there is no historic data with labels
    or because acquiring the labels for the data is difficult. We will cheat a bit
    in this section since we will still use the test data to measure the performance,
    but we will not use any data to train the model (otherwise the comparison to the
    following approaches would be difficult).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑的第一种技术是*零样本分类*，这在没有任何标记数据的情况下非常适用。这在行业中非常常见，可能是因为没有带标签的历史数据，或者因为获取数据的标签很困难。在本节中，我们会有点作弊，因为我们仍然会使用测试数据来衡量性能，但我们不会使用任何数据来训练模型（否则与后续方法的比较将会很困难）。
- en: 'The goal of zero-shot classification is to make use of a pretrained model without
    any additional fine-tuning on your task-specific corpus. To get a better idea
    of how this could work, recall that language models like BERT are pretrained to
    predict masked tokens in text on thousands of books and large Wikipedia dumps.
    To successfully predict a missing token, the model needs to be aware of the topic
    in the context. We can try to trick the model into classifying a document for
    us by providing a sentence like:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本分类的目标是利用预训练模型，在任务特定语料库上没有进行额外的微调。为了更好地了解这种工作原理，回想一下像BERT这样的语言模型是预训练的，用于在成千上万本书和大量维基百科转储中预测文本中的屏蔽标记。为了成功预测缺失的标记，模型需要了解上下文中的主题。我们可以尝试欺骗模型，通过提供一个句子来为我们对文档进行分类：
- en: “This section was about the topic [MASK].”
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这一部分是关于主题[MASK]的。”
- en: The model should then give a reasonable suggestion for the document’s topic,
    since this is a natural text to occur in the dataset.^([2](ch09.xhtml#idm46238696702096))
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是数据集中自然出现的文本，模型应该能够合理地对文档的主题提出建议。^([2](ch09.xhtml#idm46238696702096))
- en: 'Let’s illustrate this further with the following toy problem: suppose you have
    two children, and one of them likes movies with cars while the other enjoys movies
    with animals better. Unfortunately, they have already seen all the ones you know,
    so you want to build a function that tells you what topic a new movie is about.
    Naturally, you turn to transformers for this task. The first thing to try is to
    load BERT-base in the `fill-mask` pipeline, which uses the masked language model
    to predict the content of the masked tokens:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下玩具问题进一步说明这一点：假设你有两个孩子，一个喜欢有汽车的电影，而另一个更喜欢有动物的电影。不幸的是，他们已经看过你知道的所有电影，所以你想建立一个函数，告诉你一个新电影的主题是什么。自然地，你会转向变压器来完成这个任务。首先要尝试的是在`fill-mask`管道中加载BERT-base，该管道使用屏蔽语言模型来预测屏蔽标记的内容：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, let’s construct a little movie description and add a prompt to it with
    a masked word. The goal of the prompt is to guide the model to help us make a
    classification. The `fill-mask` pipeline returns the most likely tokens to fill
    in the masked spot:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一个小电影描述，并在其中添加一个带有屏蔽词的提示。提示的目标是引导模型帮助我们进行分类。`fill-mask`管道返回填充屏蔽位置的最有可能的标记：
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Clearly, the model predicts only tokens that are related to animals. We can
    also turn this around, and instead of getting the most likely tokens we can query
    the pipeline for the probability of a few given tokens. For this task we might
    choose `cars` and `animals`, so we can pass them to the pipeline as targets:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型只预测与动物相关的标记。我们也可以反过来，而不是获取最有可能的标记，我们可以查询管道获取几个给定标记的概率。对于这个任务，我们可以选择`cars`和`animals`，所以我们可以将它们作为目标传递给管道：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Unsurprisingly, the predicted probability for the token `cars` is much smaller
    than for `animals`. Let’s see if this also works for a description that is closer
    to cars:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，对于标记为“cars”的预测概率要远远小于“animals”。让我们看看这是否也适用于更接近汽车的描述：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'It does! This is only a simple example, and if we want to make sure it works
    well we should test it thoroughly, but it illustrates the key idea of many approaches
    discussed in this chapter: find a way to adapt a pretrained model for another
    task without training it. In this case we set up a prompt with a mask in such
    a way that we can use a masked language model directly for classification. Let’s
    see if we can do better by adapting a model that has been fine-tuned on a task
    that’s closer to text classification: *natural language inference* (NLI).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实可以！这只是一个简单的例子，如果我们想确保它运行良好，我们应该进行彻底的测试，但它说明了本章讨论的许多方法的关键思想：找到一种方法，使预训练模型适应另一个任务，而无需对其进行训练。在这种情况下，我们设置了一个提示，其中包含一个掩码，以便我们可以直接使用掩码语言模型进行分类。让我们看看是否可以通过调整一个在更接近文本分类的任务上进行了微调的模型来做得更好：*自然语言推理*（NLI）。
- en: Using the masked language model for classification is a nice trick, but we can
    do better still by using a model that has been trained on a task that is closer
    to classification. There is a neat proxy task called *text entailment* that fits
    the bill. In text entailment, the model needs to determine whether two text passages
    are likely to follow or contradict each other. Models are typically trained to
    detect entailments and contradictions with datasets such as Multi-Genre NLI Corpus
    (MNLI) or Cross-Lingual NLI Corpus (XNLI).^([3](ch09.xhtml#idm46238696454192))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用掩码语言模型进行分类是一个不错的技巧，但是我们可以通过使用一个在更接近分类的任务上训练过的模型来做得更好。有一个称为*文本蕴涵*的巧妙代理任务符合要求。在文本蕴涵中，模型需要确定两个文本段落是否可能相互跟随或相互矛盾。模型通常是使用诸如多种体裁NLI语料库（MNLI）或跨语言NLI语料库（XNLI）等数据集进行蕴涵和矛盾的检测。^([3](ch09.xhtml#idm46238696454192))
- en: 'Each sample in these datasets is composed of three parts: a premise, a hypothesis,
    and a label, which can be one of `entailment`, `neutral`, or `contradiction`.
    The `entailment` label is assigned when the hypothesis text is necessarily true
    under the premise. The `contradiction` label is used when the hypothesis is necessarily
    false or inappropriate under the premise. If neither of these cases applies, then
    the `neutral` label is assigned. See [Table 9-1](#mnli) for examples of each.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集中的每个样本由三部分组成：前提、假设和标签，标签可以是`蕴涵`、`中性`或`矛盾`中的一个。当假设文本在前提下必然为真时，分配`蕴涵`标签。当假设在前提下必然为假或不合适时，使用`矛盾`标签。如果这两种情况都不适用，则分配`中性`标签。参见[表9-1](#mnli)中的示例。
- en: Table 9-1\. The three classes in the MLNI dataset
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1。MLNI数据集中的三个类
- en: '| Premise | Hypothesis | Label |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 前提 | 假设 | 标签 |'
- en: '| --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| His favourite color is blue. | He is into heavy metal music. | `neutral`
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 他最喜欢的颜色是蓝色。 | 他喜欢重金属音乐。 | `中性` |'
- en: '| She finds the joke hilarious. | She thinks the joke is not funny at all.
    | `contradiction` |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 她觉得这个笑话很搞笑。 | 她认为这个笑话一点都不好笑。 | `矛盾` |'
- en: '| The house was recently built. | The house is new. | `entailment` |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 这所房子最近建造。 | 这所房子是新的。 | `蕴涵` |'
- en: 'Now, it turns out that we can hijack a model trained on the MNLI dataset to
    build a classifier without needing any labels at all! The key idea is to treat
    the text we wish to classify as the premise, and then formulate the hypothesis
    as:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事实证明我们可以劫持一个在MNLI数据集上训练的模型，构建一个分类器而无需任何标签！关键思想是将我们希望分类的文本视为前提，然后将假设制定为：
- en: “This example is about {label}.”
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这个例子是关于{label}的。”
- en: where we insert the class name for the label. The entailment score then tells
    us how likely that premise is to be about that topic, and we can run this for
    any number of classes sequentially. The downside of this approach is that we need
    to execute a forward pass for each class, which makes it less efficient than a
    standard classifier. Another slightly tricky aspect is that the choice of label
    names can have a large impact on the accuracy, and choosing labels with semantic
    meaning is generally the best approach. For example, if the label is simply `Class
    1`, the model has no hint what this might mean and whether this constitutes a
    contradiction or entailment.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们插入标签的类名。蕴涵分数告诉我们前提很可能是关于那个主题的，我们可以依次为任意数量的类运行这个。这种方法的缺点是我们需要为每个类执行一次前向传播，这使得它比标准分类器效率低。另一个稍微棘手的方面是标签名称的选择可能对准确性产生很大影响，通常最好选择具有语义含义的标签。例如，如果标签只是`Class
    1`，模型就不知道这可能意味着什么，以及这是否构成了矛盾或蕴涵。
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Transformers has an MNLI model for zero-shot
    classification built in. We can initialize it via a pipeline as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![nlpt_pin01](Images/nlpt_pin01.png) Transformers具有内置的零样本分类MNLI模型。我们可以通过管道初始化它如下：'
- en: '[PRE37]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The setting `device=0` makes sure that the model runs on the GPU instead of
    the default CPU to speed up inference. To classify a text, we simply need to pass
    it to the pipeline along with the label names. In addition, we can set `multi_label=True`
    to ensure that all the scores are returned and not only the maximum for single-label
    classification:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`device=0`确保模型在GPU上运行，而不是默认的CPU，以加快推理速度。要对文本进行分类，我们只需要将其传递给管道，并附上标签名称。此外，我们可以设置`multi_label=True`以确保返回所有分数，而不仅仅是单标签分类的最大值：
- en: '[PRE38]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since we are using a subword tokenizer, we can even pass code to the model!
    The tokenization might not be very efficient because only a small fraction of
    the pretraining dataset for the zero-shot pipeline consists of code snippets,
    but since code is also made up of a lot of natural words this is not a big issue.
    Also, the code block might contain important information, such as the framework
    (PyTorch or TensorFlow).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是子词分词器，我们甚至可以将代码传递给模型！分词可能不太高效，因为零样本管道的预训练数据集只包含了一小部分代码片段，但由于代码也由许多自然词组成，这并不是一个大问题。此外，代码块可能包含重要信息，例如框架（PyTorch或TensorFlow）。
- en: We can see that the model is very confident that this text is about a new model,
    but it also produces relatively high scores for the other labels. An important
    aspect for zero-shot classification is the domain we’re operating in. The texts
    we are dealing with here are very technical and mostly about coding, which makes
    them quite different from the original text distribution in the MNLI dataset.
    Thus, it is not surprising that this is a challenging task for the model; it might
    work much better for some domains than others, depending on how close they are
    to the training data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型非常确信这段文本是关于一个新模型的，但它也为其他标签产生了相对较高的分数。零射击分类的一个重要方面是我们所处的领域。我们在这里处理的文本非常技术化，大部分是关于编码的，这使它们与MNLI数据集中的原始文本分布相当不同。因此，这对于模型来说是一个具有挑战性的任务并不奇怪；它可能对某些领域的工作效果比其他领域要好得多，这取决于它们与训练数据的接近程度。
- en: 'Let’s write a function that feeds a single example through the zero-shot pipeline,
    and then scale it out to the whole validation set by running `map()`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数，通过零射击管道将单个示例传递，并通过运行`map()`将其扩展到整个验证集：
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now that we have our scores, the next step is to determine which set of labels
    should be assigned to each example. There are a few options we can experiment
    with:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了分数，下一步是确定应该为每个示例分配哪组标签。我们可以尝试一些选项：
- en: Define a threshold and select all labels above the threshold.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个阈值，并选择高于阈值的所有标签。
- en: Pick the top *k* labels with the *k* highest scores.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有最高分数的前*k*个标签。
- en: 'To help us determine which method is best, let’s write a `get_preds()` function
    that applies one of the approaches to retrieve the predictions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们确定哪种方法最好，让我们编写一个`get_preds()`函数，应用其中一种方法来获取预测：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, let’s write a second function, `get_clf_report()`, that returns the Scikit-learn
    classification report from a dataset with the predicted labels:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写第二个函数`get_clf_report()`，它从具有预测标签的数据集中返回Scikit-learn分类报告：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Armed with these two functions, let’s start with the top-*k* method by increasing
    *k* for several values and then plotting the micro and macro *F*[1]-scores across
    the validation set:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这两个函数，让我们从增加几个值的*k*开始，然后绘制验证集上的微观和宏观* F * [1]-分数：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](Images/nlpt_09in03.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in03.png)'
- en: 'From the plot we can see that the best results are obtained by selecting the
    label with the highest score per example (top 1). This is perhaps not so surprising,
    given that most of the examples in our datasets have only one label. Let’s now
    compare this against setting a threshold, so we can potentially predict more than
    one label per example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，通过选择每个示例的最高分数的标签（top 1）获得了最佳结果。这也许并不奇怪，因为我们数据集中的大多数示例只有一个标签。现在让我们将其与设置阈值进行比较，这样我们可能可以对每个示例进行多个标签的预测：
- en: '[PRE45]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](Images/nlpt_09in04.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in04.png)'
- en: '[PRE47]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This approach fares somewhat worse than the top-1 results, but we can see the
    precision/recall trade-off clearly in this graph. If we set the threshold too
    low, then there are too many predictions, which leads to a low precision. If we
    set the threshold too high, then we will make hardly any predictions, which produces
    a low recall. From the plot we can see that a threshold value of around 0.8 is
    the sweet spot between the two.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的表现略逊于top-1的结果，但我们可以在这张图中清楚地看到精确度/召回率的权衡。如果我们将阈值设置得太低，那么会有太多的预测，这会导致精确度低。如果我们将阈值设置得太高，那么我们几乎不会进行任何预测，这会产生低召回率。从图中我们可以看到，阈值约为0.8是两者之间的最佳平衡点。
- en: 'Since the top-1 method performs best, let’s use this to compare zero-shot classification
    against Naive Bayes on the test set:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于top-1方法表现最佳，让我们使用它来将零射击分类与朴素贝叶斯在测试集上进行比较：
- en: '[PRE49]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](Images/nlpt_09in05.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in05.png)'
- en: 'Comparing the zero-shot pipeline to the baseline, we observe two things:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将零射击管道与基线进行比较，我们观察到两件事：
- en: If we have less than 50 labeled samples, the zero-shot pipeline handily outperforms
    the baseline.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们少于50个有标签的样本，零射击管道将轻松胜过基线。
- en: Even above 50 samples, the performance of the zero-shot pipeline is superior
    when considering both the micro and macro *F*[1]-scores. The results for the micro
    *F*[1]-score tell us that the baseline performs well on the frequent classes,
    while the zero-shot pipeline excels at those since it does not require any examples
    to learn from.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使超过50个样本，零射击管道的性能在考虑微观和宏观* F * [1]-分数时仍然优越。微观* F * [1]-分数的结果告诉我们，基线在频繁类别上表现良好，而零射击管道在这些类别上表现出色，因为它不需要任何示例来学习。
- en: Note
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You might notice a slight paradox in this section: although we talk about dealing
    with no labels, we still use the validation and test sets. We use them to showcase
    different techniques and to make the results comparable between them. Even in
    a real use case, it makes sense to gather a handful of labeled examples to run
    some quick evaluations. The important point is that we did not adapt the parameters
    of the model with the data; instead, we just adapted some hyperparameters.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到本节中存在一个小悖论：尽管我们谈论处理没有标签的情况，但我们仍然使用验证集和测试集。我们使用它们来展示不同的技术，并使结果可以相互比较。即使在实际用例中，收集一些有标签的示例进行快速评估也是有意义的。重要的一点是，我们没有根据数据调整模型的参数；相反，我们只是调整了一些超参数。
- en: 'If you find it difficult to get good results on your own dataset, here are
    a few things you can do to improve the zero-shot pipeline:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现在自己的数据集上难以获得良好的结果，可以尝试以下几种方法来改进零射击管道：
- en: The way the pipeline works makes it very sensitive to the names of the labels.
    If the names don’t make much sense or are not easily connected to the texts, the
    pipeline will likely perform poorly. Either try using different names or use several
    names in parallel and aggregate them in an extra step.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道的工作方式使其对标签的名称非常敏感。如果名称不太合理或与文本不容易联系起来，那么管道可能表现不佳。可以尝试使用不同的名称或并行使用几个名称，并在额外的步骤中对它们进行聚合。
- en: Another thing you can improve is the form of the hypothesis. By default it is
    `hypothesis="This is example is about {}"`, but you can pass any other text to
    the pipeline. Depending on the use case, this might improve the performance.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一件你可以改进的事情是假设的形式。默认情况下是 `hypothesis="This is example is about {}"`，但你可以传递任何其他文本到管道中。根据使用情况，这可能会提高性能。
- en: Let’s now turn to the regime where we have a few labeled examples we can use
    to train a model.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向我们有少数标记示例可以用来训练模型的情况。
- en: Working with a Few Labels
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用少数标签
- en: In most NLP projects, you’ll have access to at least a few labeled examples.
    The labels might come directly from a client or cross-company team, or you might
    decide to just sit down and annotate a few examples yourself. Even for the previous
    approach, we needed a few labeled examples to evaluate how well the zero-shot
    approach worked. In this section, we’ll have a look at how we can best leverage
    the few, precious labeled examples that we have. Let’s start by looking at a technique
    known as data augmentation that can help us multiply the little labeled data that
    we have.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数NLP项目中，您至少会有一些标记的示例。标签可能直接来自客户或跨公司团队，或者您可能决定自己注释一些示例。即使对于以前的方法，我们也需要一些标记的示例来评估零-shot方法的效果。在本节中，我们将看看如何最好地利用我们拥有的少数宝贵的标记示例。让我们首先看看一种称为数据增强的技术，它可以帮助我们扩大我们拥有的少量标记数据。
- en: Data Augmentation
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'One simple but effective way to boost the performance of text classifiers on
    small datasets is to apply *data augmentation* techniques to generate new training
    examples from the existing ones. This is a common strategy in computer vision,
    where images are randomly perturbed without changing the meaning of the data (e.g.,
    a slightly rotated cat is still a cat). For text, data augmentation is somewhat
    trickier because perturbing the words or characters can completely change the
    meaning. For example, the two questions “Are elephants heavier than mice?” and
    “Are mice heavier than elephants?” differ by a single word swap, but have opposite
    answers. However, if the text consists of more than a few sentences (like our
    GitHub issues do), then the noise introduced by these types of transformations
    will generally not affect the label. In practice, there are two types of data
    augmentation techniques that are commonly used:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在小数据集上提高文本分类器性能的一种简单但有效的方法是应用*数据增强*技术，从现有数据中生成新的训练样本。这是计算机视觉中的常见策略，其中图像被随机扰动而不改变数据的含义（例如，稍微旋转的猫仍然是一只猫）。对于文本来说，数据增强有些棘手，因为扰动单词或字符可能会完全改变含义。例如，两个问题“大象比老鼠重吗？”和“老鼠比大象重吗？”只有一个词交换，但答案相反。然而，如果文本包含超过几句话（就像我们的GitHub问题一样），那么这些类型的转换引入的噪音通常不会影响标签。实际上，通常使用两种类型的数据增强技术：
- en: Back translation
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 回译
- en: Take a text in the source language, translate it into one or more target languages
    using machine translation, and then translate it back to the source language.
    Back translation tends to works best for high-resource languages or corpora that
    don’t contain too many domain-specific words.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 取源语言中的文本，使用机器翻译将其翻译成一个或多个目标语言，然后将其翻译回源语言。回译通常适用于高资源语言或不包含太多领域特定词汇的语料库。
- en: Token perturbations
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 标记扰动
- en: Given a text from the training set, randomly choose and perform simple transformations
    like random synonym replacement, word insertion, swap, or deletion.⁠^([4](ch09.xhtml#idm46238695402224))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练集中的文本，随机选择并执行简单的转换，如随机同义词替换、单词插入、交换或删除。⁠^([4](ch09.xhtml#idm46238695402224))
- en: Examples of these transformations are shown in [Table 9-2](#data-aug). For a
    detailed list of other data augmentation techniques for NLP, we recommend reading
    Amit Chaudhary’s blog post [“A Visual Survey of Data Augmentation in NLP”](https://oreil.ly/j6euX).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换的示例显示在[表9-2](#data-aug)中。有关NLP的其他数据增强技术的详细列表，我们建议阅读Amit Chaudhary的博文[“NLP中数据增强的视觉调查”](https://oreil.ly/j6euX)。
- en: Table 9-2\. Different types of data augmentation techniques for text
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2。文本的不同类型的数据增强技术
- en: '| Augmentation | Sentence |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | 句子 |'
- en: '| --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| None | Even if you defeat me Megatron, others will rise to defeat your tyranny
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 即使你打败我梅杰特龙，其他人也会起来打败你的暴政 |'
- en: '| Synonym replace | Even if you kill me Megatron, others will prove to defeat
    your tyranny |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 同义词替换 | 即使你杀了我梅杰特龙，其他人将证明打败你的暴政 |'
- en: '| Random insert | Even if you defeat me Megatron, others humanity will rise
    to defeat your tyranny |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 随机插入 | 即使你打败我梅杰特龙，其他人类也会起来打败你的暴政 |'
- en: '| Random swap | You even if defeat me Megatron, others will rise defeat to
    tyranny your |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: 随机交换 | 即使你打败我梅杰特龙，其他人也会起来打败你的暴政
- en: '| Random delete | Even if you me Megatron, others to defeat tyranny |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 随机删除 | 即使你我梅杰特龙，其他人也会起来打败暴政 |'
- en: '| Back translate (German) | Even if you defeat me, others will rise up to defeat
    your tyranny |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 回译（德语） | 即使你打败我，其他人也会起来打败你的暴政 |'
- en: You can implement back translation using machine translation models like [M2M100](https://oreil.ly/gfJCq),
    while libraries like [*NlpAug*](https://oreil.ly/UVRci) and [*TextAttack*](https://oreil.ly/NMtYi)
    provide various recipes for token perturbations. In this section, we’ll focus
    on using synonym replacement as it’s simple to implement and gets across the main
    idea behind data augmentation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用像[M2M100](https://oreil.ly/gfJCq)这样的机器翻译模型来实现回译，而像[*NlpAug*](https://oreil.ly/UVRci)和[*TextAttack*](https://oreil.ly/NMtYi)这样的库提供了各种用于标记扰动的方法。在本节中，我们将专注于使用同义词替换，因为它简单易行，并且能够传达数据增强背后的主要思想。
- en: 'We’ll use the `ContextualWordEmbsAug` augmenter from NlpAug to leverage the
    contextual word embeddings of DistilBERT for our synonym replacements. Let’s start
    with a simple example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用NlpAug中的`ContextualWordEmbsAug`增强器来利用DistilBERT的上下文词嵌入进行同义词替换。让我们从一个简单的例子开始：
- en: '[PRE51]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here we can see how the word “are” has been replaced with an apostrophe to
    generate a new synthetic training example. We can wrap this augmentation in a
    simple function as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到单词“are”已被替换为撇号，以生成一个新的合成训练示例。我们可以将这种增强包装在一个简单的函数中，如下所示：
- en: '[PRE53]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now when we pass this function to the `map()` method, we can generate any number
    of new examples with the `transformations_per_example` argument. We can use this
    function in our code to train the Naive Bayes classifier by simply adding one
    line after we select the slice:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们将这个函数传递给`map()`方法时，我们可以使用`transformations_per_example`参数生成任意数量的新示例。我们可以在我们的代码中使用这个函数来训练朴素贝叶斯分类器，只需在选择切片后添加一行：
- en: '[PRE54]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Including this and rerunning the analysis produces the plot shown here:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 包括这一点并重新运行分析会产生如图所示的图表：
- en: '[PRE55]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](Images/nlpt_09in06.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in06.png)'
- en: From the figure, we can see that a small amount of data augmentation improves
    the *F*[1]-score of the Naive Bayes classifier by around 5 points, and it overtakes
    the zero-shot pipeline for the macro scores once we have around 170 training samples.
    Let’s now take a look at a method based on using the embeddings of large language
    models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，少量数据增强可以将朴素贝叶斯分类器的*F*[1]-分数提高约5个点，并且一旦我们有大约170个训练样本，它就会超过零-shot管道的宏分数。现在让我们来看一个基于使用大型语言模型嵌入的方法。
- en: Using Embeddings as a Lookup Table
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将嵌入用作查找表
- en: Large language models such as GPT-3 have been shown to be excellent at solving
    tasks with limited data. The reason is that these models learn useful representations
    of text that encode information across many dimensions, such as sentiment, topic,
    text structure, and more. For this reason, the embeddings of large language models
    can be used to develop a semantic search engine, find similar documents or comments,
    or even classify text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，像GPT-3这样的大型语言模型在解决数据有限的任务方面表现出色。原因是这些模型学习了有用的文本表示，跨越许多维度编码信息，如情感、主题、文本结构等。因此，大型语言模型的嵌入可以用于开发语义搜索引擎，找到相似的文档或评论，甚至对文本进行分类。
- en: 'In this section we’ll create a text classifier that’s modeled after the [OpenAI
    API classification endpoint](https://oreil.ly/aMgIr). The idea follows a three-step
    process:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个文本分类器，它的模型是基于[OpenAI API分类端点](https://oreil.ly/aMgIr)。这个想法遵循一个三步过程：
- en: Use the language model to embed all labeled texts.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用语言模型嵌入所有标记文本。
- en: Perform a nearest neighbor search over the stored embeddings.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储的嵌入上执行最近邻搜索。
- en: Aggregate the labels of the nearest neighbors to get a prediction.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚合最近邻的标签以获得预测。
- en: The process is illustrated in [Figure 9-3](#nearest-neighbours), which shows
    how labeled data is embedded with a model and stored with the labels. When a new
    text needs to be classified it is embedded as well, and the label is given based
    on the labels of the nearest neighbors. It is important to calibrate the number
    of neighbors to be searched for, as too few might be noisy and too many might
    mix in neighboring groups.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在[图9-3](#nearest-neighbours)中有所说明，它展示了标记数据是如何嵌入模型并与标签一起存储的。当需要对新文本进行分类时，它也会被嵌入，并且基于最近邻的标签给出标签。重要的是要校准要搜索的邻居数量，因为太少可能会有噪音，太多可能会混入相邻的群体。
- en: '![nearest-neighbours](Images/nlpt_0903.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![nearest-neighbours](Images/nlpt_0903.png)'
- en: Figure 9-3\. An illustration of nearest neighbor embedding lookup
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 最近邻嵌入查找的示意图
- en: The beauty of this approach is that no model fine-tuning is necessary to leverage
    the few available labeled data points. Instead, the main decision to make this
    approach work is to select an appropriate model that is ideally pretrained on
    a similar domain to your dataset.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的美妙之处在于，不需要对模型进行微调就可以利用少量可用的标记数据点。相反，使这种方法起作用的主要决定是选择一个理想情况下在类似领域上预训练的适当模型。
- en: Since GPT-3 is only available through the OpenAI API, we’ll use GPT-2 to test
    the technique. Specifically, we’ll use a variant of GPT-2 that was trained on
    Python code, which will hopefully capture some of the context contained in our
    GitHub issues.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT-3只能通过OpenAI API获得，我们将使用GPT-2来测试这种技术。具体来说，我们将使用一个在Python代码上训练的GPT-2变体，这将有望捕捉到我们GitHub问题中包含的一些上下文。
- en: Let’s write a helper function that takes a list of texts and uses the model
    to create a single-vector representation for each text. One problem we have to
    deal with is that transformer models like GPT-2 will actually return one embedding
    vector per token. For example, given the sentence “I took my dog for a walk”,
    we can expect several embedding vectors, one for each token. But what we really
    want is a single embedding vector for the whole sentence (or GitHub issue in our
    application). To deal with this, we can use a technique called *pooling*. One
    of the simplest pooling methods is to average the token embeddings, which is called
    *mean pooling*. With mean pooling, the only thing we need to watch out for is
    that we don’t include padding tokens in the average, so we can use the attention
    mask to handle that.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个辅助函数，它接受一个文本列表，并使用模型为每个文本创建单一向量表示。我们需要处理的一个问题是，像GPT-2这样的转换器模型实际上会返回每个标记一个嵌入向量。例如，给定句子“I
    took my dog for a walk”，我们可以期望有几个嵌入向量，每个标记一个。但我们真正想要的是整个句子（或我们应用程序中的GitHub问题）的单一嵌入向量。为了处理这个问题，我们可以使用一种称为*池化*的技术。最简单的池化方法之一是对标记嵌入进行平均，这称为*均值池化*。使用均值池化，我们唯一需要注意的是不要在平均值中包括填充标记，所以我们可以使用注意力掩码来处理。
- en: 'To see how this works, let’s load a GPT-2 tokenizer and model, define the mean
    pooling operation, and wrap the whole process in a simple `embed_text()` function:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看这是如何工作的，让我们加载一个GPT-2分词器和模型，定义均值池化操作，并将整个过程包装在一个简单的`embed_text()`函数中：
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now we can get the embeddings for each split. Note that GPT-style models don’t
    have a padding token, and therefore we need to add one before we can get the embeddings
    in a batched fashion as implemented in the preceding code. We’ll just recycle
    the end-of-string token for this purpose:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为每个拆分获取嵌入。请注意，GPT风格的模型没有填充标记，因此我们需要在可以批量获取嵌入之前添加一个标记，就像在前面的代码中实现的那样。我们将只是为此目的重复使用字符串结束标记：
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now that we have all the embeddings, we need to set up a system to search them.
    We could write a function that calculates, say, the cosine similarity between
    a new text embedding that we’ll query and the existing embeddings in the training
    set. Alternatively, we can use a built-in structure of ![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets called a *FAISS index*.^([5](ch09.xhtml#idm46238694754800)) We already
    encountered FAISS in [Chapter 7](ch07.xhtml#chapter_qa). You can think of this
    as a search engine for embeddings, and we’ll have a closer look at how it works
    in a minute. We can use an existing field of the dataset to create a FAISS index
    with `add_faiss_index()`, or we can load new embeddings into the dataset with
    `add_faiss_index_from_external_arrays()`. Let’s use the former function to add
    our training embeddings to the dataset as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的嵌入，我们需要建立一个系统来搜索它们。我们可以编写一个函数，计算我们将查询的新文本嵌入与训练集中现有嵌入之间的余弦相似度。或者，我们可以使用![nlpt_pin01](Images/nlpt_pin01.png)数据集中的内置结构，称为*FAISS索引*。^([5](ch09.xhtml#idm46238694754800))我们在[第7章](ch07.xhtml#chapter_qa)中已经遇到了FAISS。您可以将其视为嵌入的搜索引擎，我们将在一分钟内更仔细地看看它是如何工作的。我们可以使用数据集的现有字段创建一个FAISS索引，使用`add_faiss_index()`，或者使用`add_faiss_index_from_external_arrays()`将新的嵌入加载到数据集中。让我们使用前一个函数将我们的训练嵌入添加到数据集中：
- en: '[PRE58]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This created a new FAISS index called `embedding`. We can now perform a nearest
    neighbor lookup by calling the function `get_nearest_examples()`. It returns the
    closest neighbors as well as the matching score for each neighbor. We need to
    specify the query embedding as well as the number of nearest neighbors to retrieve.
    Let’s give it a spin and have a look at the documents that are closest to an example:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个名为`embedding`的新FAISS索引。现在我们可以通过调用函数`get_nearest_examples()`执行最近邻查找。它返回最接近的邻居以及每个邻居的匹配分数。我们需要指定查询嵌入以及要检索的最近邻居的数量。让我们试一试，看看与示例最接近的文档：
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Nice! This is exactly what we hoped for: the three retrieved documents that
    we got via embedding lookup all have the same labels and we can already see from
    the titles that they are all very similar. The query as well as the retrieved
    documents revolve around adding new and efficient transformer models. The question
    remains, however, what is the best value for *k*? Similarly, how we should then
    aggregate the labels of the retrieved documents? Should we, for example, retrieve
    three documents and assign all labels that occurred at least twice? Or should
    we go for 20 and use all labels that appeared at least 5 times? Let’s investigate
    this systematically: we’ll try several values for *k* and then vary the threshold
    <math alttext="m less-than k"><mrow><mi>m</mi> <mo><</mo> <mi>k</mi></mrow></math>
    for label assignment with a helper function. We’ll record the macro and micro
    performance for each setting so we can decide later which run performed best.
    Instead of looping over each sample in the validation set we can make use of the
    function `get_nearest_examples_batch()`, which accepts a batch of queries:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！这正是我们所希望的：通过嵌入查找得到的三个文档都具有相同的标签，我们已经可以从标题中看出它们都非常相似。查询以及检索到的文档都围绕着添加新的高效变压器模型。然而，问题仍然存在，*k*的最佳值是多少？同样，我们应该如何聚合检索到的文档的标签？例如，我们应该检索三个文档，并分配至少出现两次的所有标签吗？还是应该选择20个，并使用至少出现5次的所有标签？让我们系统地调查一下：我们将尝试几个*k*的值，然后使用一个辅助函数改变标签分配的阈值<mrow><mi>m</mi>
    <mo><</mo> <mi>k</mi></mrow>。我们将记录每个设置的宏和微性能，以便稍后决定哪个运行效果最好。我们可以使用`get_nearest_examples_batch()`函数，而不是循环遍历验证集中的每个样本，它接受一个查询的批处理：
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let’s check what the best values would be with all the training samples and
    visualize the scores for all *k* and *m* configurations:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查在所有训练样本中最佳的值，并可视化所有*k*和*m*配置的分数：
- en: '[PRE62]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](Images/nlpt_09in07.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in07.png)'
- en: 'From the plots we can see that there is a pattern: choosing *m* too large or
    small for a given *k* yields suboptimal results. The best performance is achieved
    when choosing a ratio of approximately <math alttext="m slash k equals 1 slash
    3"><mrow><mi>m</mi> <mo>/</mo> <mi>k</mi> <mo>=</mo> <mn>1</mn> <mo>/</mo> <mn>3</mn></mrow></math>
    . Let’s see which *k* and *m* give the best result overall:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到有一个模式：对于给定的*k*选择太大或太小会产生次优结果。当选择大约为<mrow><mi>m</mi> <mo>/</mo> <mi>k</mi>
    <mo>=</mo> <mn>1</mn> <mo>/</mo> <mn>3</mn></mrow>的比率时，可以获得最佳性能。让我们看看哪个*k*和*m*能够在整体上获得最佳结果：
- en: '[PRE64]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The perfomance is best when we choose <math alttext="k equals 15"><mrow><mi>k</mi>
    <mo>=</mo> <mn>15</mn></mrow></math> and <math alttext="m equals 5"><mrow><mi>m</mi>
    <mo>=</mo> <mn>5</mn></mrow></math> , or in other words when we retrieve the 15
    nearest neighbors and then assign the labels that occurred at least 5 times. Now
    that we have a good method for finding the best values for the embedding lookup,
    we can play the same game as with the Naive Bayes classifier where we go through
    the slices of the training set and evaluate the performance. Before we can slice
    the dataset, we need to remove the index since we cannot slice a FAISS index like
    the dataset. The rest of the loops stay exactly the same, with the addition of
    using the validation set to get the best *k* and *m* values:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择 <math alttext="k equals 15"><mrow><mi>k</mi> <mo>=</mo> <mn>15</mn></mrow></math>
    和 <math alttext="m equals 5"><mrow><mi>m</mi> <mo>=</mo> <mn>5</mn></mrow></math>
    时，性能最佳，换句话说，当我们检索15个最近的邻居，然后分配至少出现5次的标签。现在我们有了一个找到嵌入查找最佳值的好方法，我们可以像使用朴素贝叶斯分类器一样玩同样的游戏，我们遍历训练集的切片并评估性能。在我们可以切片数据集之前，我们需要删除索引，因为我们不能像数据集那样切片FAISS索引。其余的循环保持完全相同，另外使用验证集来获取最佳的*k*和*m*值：
- en: '[PRE66]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](Images/nlpt_09in08.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in08.png)'
- en: The embedding lookup is competitive on the micro scores with the previous approaches
    while just having two “learnable” parameters, *k* and *m*, but performs slightly
    worse on the macro scores.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入查找在微分数上与先前的方法竞争，只有两个“可学习”参数*k*和*m*，但在宏分数上表现稍差。
- en: Take these results with a grain of salt; which method works best strongly depends
    on the domain. The zero-shot pipeline’s training data is quite different from
    the GitHub issues dataset we’re using it on, which contains a lot of code that
    the model likely has not encountered much before. For a more common task such
    as sentiment analysis of reviews, the pipeline might work much better. Similarly,
    the embeddings’ quality depends on the model and the data it was trained on. We
    tried half a dozen models, such as `sentence-transformers/stsb-roberta-large`,
    which was trained to give high-quality embeddings of sentences, and `microsoft/codebert-base`
    and `dbernsohn/roberta-python`, which were trained on code and documentation.
    For this specific use case, GPT-2 trained on Python code worked best.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请以一颗谷物的方式接受这些结果；哪种方法最有效强烈取决于领域。零-shot管道的训练数据与我们正在使用的GitHub问题数据集有很大不同，其中包含模型可能之前很少遇到的大量代码。对于更常见的任务，例如对评论的情感分析，该管道可能效果更好。同样，嵌入的质量取决于模型和它训练的数据。我们尝试了半打模型，例如`sentence-transformers/stsb-roberta-large`，它经过训练以提供句子的高质量嵌入，以及`microsoft/codebert-base`和`dbernsohn/roberta-python`，它们是在代码和文档上进行训练的。对于这个特定的用例，GPT-2在Python代码上的训练效果最好。
- en: Since you don’t actually need to change anything in your code besides replacing
    the model checkpoint name to test another model, you can quickly try out a few
    models once you have the evaluation pipeline set up.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您实际上不需要在代码中更改任何内容，只需将模型检查点名称替换为测试另一个模型，一旦设置了评估管道，您可以快速尝试几个模型。
- en: Let’s now compare this simple embedding trick against simply fine-tuning a transformer
    on the limited data we have.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这个简单的嵌入技巧与简单地微调我们拥有的有限数据的变压器进行比较。
- en: Fine-Tuning a Vanilla Transformer
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调一个普通的变压器
- en: 'If we have access to labeled data, we can also try to do the obvious thing:
    simply fine-tune a pretrained transformer model. In this section, we’ll use the
    standard BERT checkpoint as a starting point. Later, we’ll see the effect that
    fine-tuning the language model has on performance.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以访问标记数据，我们也可以尝试做一件显而易见的事情：简单地微调预训练的变压器模型。在本节中，我们将使用标准的BERT检查点作为起点。稍后，我们将看到微调语言模型对性能的影响。
- en: Tip
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For many applications, starting with a pretrained BERT-like model is a good
    idea. However, if the domain of your corpus differs significantly from the pretraining
    corpus (which is usually Wikipedia), you should explore the many models that are
    available on the Hugging Face Hub. Chances are someone has already pretrained
    a model on your domain!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序来说，从预训练的类似BERT的模型开始是一个好主意。但是，如果您的语料库领域与预训练语料库（通常是维基百科）有显著差异，您应该探索Hugging
    Face Hub上提供的许多模型。很可能已经有人在您的领域上预训练了一个模型！
- en: 'Let’s start by loading the pretrained tokenizer, tokenizing our dataset, and
    getting rid of the columns we don’t need for training and evaluation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载预训练的标记器开始，对我们的数据集进行标记化，并摆脱我们在训练和评估中不需要的列：
- en: '[PRE68]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The multilabel loss function expects the labels to be of type float, since
    it also allows for class probabilities instead of discrete labels. Therefore,
    we need to change the type of the column `label_ids`. Since changing the format
    of the column element-wise does not play well with Arrow’s typed format, we’ll
    do a little workaround. First, we create a new column with the labels. The format
    of that column is inferred from the first element. Then we delete the original
    column and rename the new one to take the place of the original one:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签损失函数期望标签的类型为浮点数，因为它还允许类概率而不是离散标签。因此，我们需要更改`label_ids`列的类型。由于逐元素更改列格式与Arrow的类型格式不兼容，我们将做一些变通。首先，我们创建一个带有标签的新列。该列的格式是从第一个元素推断出来的。然后，我们删除原始列，并将新列重命名为原始列的位置：
- en: '[PRE69]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Since we are likely to quickly overfit the training data due to its limited
    size, we set `load_best_model_at_end=True` and choose the best model based on
    the micro *F*[1]-⁠score:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于由于训练数据的有限大小，我们很可能会很快过度拟合训练数据，因此我们设置`load_best_model_at_end=True`并根据微观*F*[1]-⁠score选择最佳模型：
- en: '[PRE70]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We need the *F*[1]-score to choose the best model, so we need to make sure
    it is calculated during the evaluation. Because the model returns the logits,
    we first need to normalize the predictions with a sigmoid function and can then
    binarize them with a simple threshold. Then we return the scores we are interested
    in from the classification report:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要*F*[1]-score来选择最佳模型，因此我们需要确保在评估过程中计算它。因为模型返回logits，所以我们首先需要使用sigmoid函数对预测进行归一化，然后可以使用简单的阈值对其进行二值化。然后我们从分类报告中返回我们感兴趣的分数：
- en: '[PRE71]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now we are ready to rumble! For each training set slice we train a classifier
    from scratch, load the best model at the end of the training loop, and store the
    results on the test set:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好了！对于每个训练集切片，我们从头开始训练一个分类器，在训练循环结束时加载最佳模型，并将结果存储在测试集上：
- en: '[PRE72]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '![](Images/nlpt_09in10.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in10.png)'
- en: First of all we see that simply fine-tuning a vanilla BERT model on the dataset
    leads to competitive results when we have access to around 64 examples. We also
    see that before this the behavior is a bit erratic, which is again due to training
    a model on a small sample where some labels can be unfavorably unbalanced. Before
    we make use of the unlabeled part of our dataset, let’s take a quick look at another
    promising approach for using language models in the few-shot domain.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到简单地在数据集上微调一个普通的BERT模型会在我们拥有大约64个示例时导致竞争力的结果。我们还看到在此之前，行为有点不稳定，这又是由于在小样本上训练模型时，一些标签可能不平衡。在利用数据集的未标记部分之前，让我们快速看一下在少样本领域使用语言模型的另一种有前途的方法。
- en: In-Context and Few-Shot Learning with Prompts
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示进行上下文和少样本学习
- en: We saw earlier in this chapter that we can use a language model like BERT or
    GPT-2 and adapt it to a supervised task by using prompts and parsing the model’s
    token predictions. This is different from the classic approach of adding a task-specific
    head and tuning the model parameters for the task. On the plus side, this approach
    does not require any training data, but on the negative side it seems we can’t
    leverage labeled data if we have access to it. There is a middle ground that we
    can sometimes take advantage of called *in-context* or *few-shot learning*.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面看到，我们可以使用BERT或GPT-2等语言模型，并使用提示和解析模型的标记预测来使其适应监督任务。这与添加特定任务头部并调整模型参数的经典方法不同。优点是，这种方法不需要任何训练数据，但缺点是，如果我们可以访问标记数据，似乎我们无法利用它。有一个中间地带，我们有时可以利用，称为*in-context*或*少样本学习*。
- en: 'To illustrate the concept, consider an English to French translation task.
    In the zero-shot paradigm, we would construct a prompt that might look as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，考虑一个英语到法语的翻译任务。在零-shot范式中，我们会构建一个提示，可能如下所示：
- en: '[PRE75]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: This hopefully prompts the model to predict the tokens of the word “merci”.
    We already saw when using GPT-2 for summarization in [Chapter 6](ch06.xhtml#chapter_summarization)
    that adding “TL;DR” to a text prompted the model to generate a summary without
    explicitly being trained to do this. An interesting finding of the GPT-3 paper
    was the ability of large language models to effectively learn from examples presented
    in the prompt—so, the previous translation example could be augmented with several
    English to German examples, which would make the model perform much better on
    this task.^([6](ch09.xhtml#idm46238693103824))
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这有望促使模型预测单词“merci”的标记。我们已经在[第6章](ch06.xhtml#chapter_summarization)中使用GPT-2进行摘要时看到，向文本中添加“TL;DR”提示模型生成摘要，而无需明确训练。GPT-3论文的一个有趣发现是大型语言模型有效地从提示中学习示例的能力，因此，前面的翻译示例可以增加几个英语到德语的示例，这将使模型在这个任务上表现得更好。^([6](ch09.xhtml#idm46238693103824))
- en: Furthermore, the authors found that the larger the models are scaled, the better
    they are at using the in-context examples, leading to significant performance
    boosts. Although GPT-3-sized models are challenging to use in production, this
    is an exciting emerging research field and people have built cool applications,
    such as a natural language shell where commands are entered in natural language
    and parsed by GPT-3 to shell commands.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者发现，模型规模越大，它们越擅长使用上下文示例，从而显著提高性能。尽管GPT-3大小的模型在生产中具有挑战性，但这是一个令人兴奋的新兴研究领域，人们已经构建了一些很酷的应用，比如自然语言shell，其中命令以自然语言输入，并由GPT-3解析为shell命令。
- en: An alternative approach to using labeled data is to create examples of the prompts
    and desired predictions and continue training the language model on these examples.
    A novel method called ADAPET uses such an approach and beats GPT-3 on a wide variety
    of tasks,^([7](ch09.xhtml#idm46238693077040)) tuning the model with generated
    prompts. Recent work by Hugging Face researchers suggests that such an approach
    can be more data-efficient than fine-tuning a custom head.^([8](ch09.xhtml#idm46238693075456))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标记数据的另一种方法是创建提示和期望预测的示例，并继续在这些示例上训练语言模型。一种名为ADAPET的新方法使用了这种方法，并在各种任务上击败了GPT-3，通过生成提示来调整模型。最近Hugging
    Face研究人员的工作表明，这种方法比微调自定义头部更节约数据。^([7](ch09.xhtml#idm46238693077040))
- en: In this section we briefly looked at various ways to make good use of the few
    labeled examples that we have. Very often we also have access to a lot of unlabeled
    data in addition to the labeled examples; in the next section we’ll discuss how
    to make good use of that.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要地讨论了利用我们拥有的少量标记示例的各种方法。通常情况下，除了标记的示例，我们还可以访问大量未标记的数据；在下一节中，我们将讨论如何充分利用这些数据。
- en: Leveraging Unlabeled Data
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用未标记的数据
- en: 'Although having access to large volumes of high-quality labeled data is the
    best-case scenario to train a classifier, this does not mean that unlabeled data
    is worthless. Just think about the pretraining of most models we have used: even
    though they are trained on mostly unrelated data from the internet, we can leverage
    the pretrained weights for other tasks on a wide variety of texts. This is the
    core idea of transfer learning in NLP. Naturally, if the downstream task has similar
    textual structure as the pretraining texts the transfer works better, so if we
    can bring the pretraining task closer to the downstream objective we could potentially
    improve the transfer.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管拥有大量高质量标记数据是训练分类器的最佳情况，但这并不意味着未标记数据毫无价值。想想我们使用的大多数模型的预训练：即使它们是在互联网上大多数不相关的数据上训练的，我们也可以利用预训练的权重来处理各种各样的文本上的其他任务。这是自然语言处理中迁移学习的核心思想。当下游任务的文本结构与预训练文本相似时，迁移效果更好，因此如果我们可以使预训练任务更接近下游目标，我们可能会改善迁移效果。
- en: 'Let’s think about this in terms of our concrete use case: BERT is pretrained
    on the BookCorpus and English Wikipedia, and texts containing code and GitHub
    issues are definitely a small niche in these datasets. If we pretrained BERT from
    scratch we could do it on a crawl of all of the issues on GitHub, for example.
    However, this would be expensive, and a lot of aspects about language that BERT
    has learned are still valid for GitHub issues. So is there a middle ground between
    retraining from scratch and just using the model as is for classification? There
    is, and it is called domain adaptation (which we also saw for question answering
    in [Chapter 7](ch07.xhtml#chapter_qa)). Instead of retraining the language model
    from scratch, we can continue training it on data from our domain. In this step
    we use the classic language model objective of predicting masked words, which
    means we don’t need any labeled data. After that we can load the adapted model
    as a classifier and fine-tune it, thus leveraging the unlabeled data.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们具体的用例来考虑这个问题：BERT 在 BookCorpus 和英文维基百科上进行了预训练，而包含代码和 GitHub 问题的文本在这些数据集中显然是一个小众。如果我们从头开始对
    BERT 进行预训练，我们可以在 GitHub 上对所有问题进行爬取，例如。然而，这将是昂贵的，并且 BERT 学到的许多关于语言的方面仍然适用于 GitHub
    问题。那么在从头开始重新训练和仅仅使用模型进行分类之间是否有一个中间地带？有，它被称为域自适应（我们在第7章中也看到了用于问答的域自适应）。我们可以在不从头开始重新训练语言模型的情况下，继续在我们领域的数据上训练它。在这一步中，我们使用预测掩码词的经典语言模型目标，这意味着我们不需要任何标记数据。之后，我们可以将适应后的模型加载为分类器并进行微调，从而利用未标记的数据。
- en: The beauty of domain adaptation is that compared to labeled data, unlabeled
    data is often abundantly available. Furthermore, the adapted model can be reused
    for many use cases. Imagine you want to build an email classifier and apply domain
    adaptation on all your historic emails. You can later use the same model for named
    entity recognition or another classification task like sentiment analysis, since
    the approach is agnostic to the downstream task.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 域自适应的美妙之处在于，与标记数据相比，未标记数据通常是丰富可得的。此外，适应后的模型可以重复用于许多用例。想象一下，您想构建一个电子邮件分类器，并在所有历史电子邮件上应用域自适应。稍后您可以使用相同的模型进行命名实体识别或其他分类任务，比如情感分析，因为该方法对下游任务是不可知的。
- en: Let’s now see the steps we need to take to fine-tune a pretrained language model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们需要采取哪些步骤来微调预训练语言模型。
- en: Fine-Tuning a Language Model
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调语言模型
- en: 'In this section we’ll fine-tune the pretrained BERT model with masked language
    modeling on the unlabeled portion of our dataset. To do this we only need two
    new concepts: an extra step when tokenizing the data and a special data collator.
    Let’s start with the tokenization.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对我们数据集的未标记部分进行预训练 BERT 模型的掩码语言建模进行微调。为此，我们只需要两个新概念：在对数据进行分词时需要额外的步骤和一个特殊的数据整理器。让我们从分词开始。
- en: 'In addition to the ordinary tokens from the text the tokenizer also adds special
    tokens to the sequence, such as the `[CLS]` and `[SEP]` tokens that are used for
    classification and next sentence prediction. When we do masked language modeling,
    we want to make sure we don’t train the model to also predict these tokens. For
    this reason we mask them from the loss, and we can get a mask when tokenizing
    by setting `return_special_tokens_mask=True`. Let’s retokenize the text with that
    setting:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文本中的普通标记外，分词器还向序列添加特殊标记，例如 `[CLS]` 和 `[SEP]` 标记，用于分类和下一个句子预测。当我们进行掩码语言建模时，我们希望确保不训练模型来预测这些标记。出于这个原因，我们从损失中屏蔽它们，并且我们可以通过设置
    `return_special_tokens_mask=True` 来在分词时获得掩码。让我们使用该设置重新对文本进行分词：
- en: '[PRE76]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: What’s missing to start with masked language modeling is the mechanism to mask
    tokens in the input sequence and have the target tokens in the outputs. One way
    we could approach this is by setting up a function that masks random tokens and
    creates labels for these sequences. But this would double the size of the dataset,
    since we would also store the target sequence in the dataset, and it would mean
    we would use the same masking of a sequence every epoch.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 开始进行掩码语言建模所缺少的是在输入序列中屏蔽标记并在输出中有目标标记的机制。我们可以采用的一种方法是设置一个函数，对随机标记进行屏蔽并为这些序列创建标签。但这将使数据集的大小翻倍，因为我们还将在数据集中存储目标序列，并且这意味着我们将在每个时期使用相同的序列屏蔽。
- en: 'A much more elegant solution is to use a data collator. Remember that the data
    collator is the function that builds the bridge between the dataset and the model
    calls. A batch is sampled from the dataset, and the data collator prepares the
    elements in the batch to feed them to the model. In the simplest case we have
    encountered, it simply concatenates the tensors of each element into a single
    tensor. In our case we can use it to do the masking and label generation on the
    fly. That way we don’t need to store the labels and we get new masks every time
    we sample. The data collator for this task is called `DataCollatorForLanguageModeling`.
    We initialize it with the model’s tokenizer and the fraction of tokens we want
    to mask via the `mlm_probability` argument. We’ll use this collator to mask 15%
    of the tokens, which follows the procedure in the BERT paper:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更加优雅的解决方案是使用数据整理器。记住，数据整理器是构建数据集和模型调用之间的桥梁的函数。从数据集中抽取一个批次，并且数据整理器准备好批次中的元素以将它们馈送到模型中。在我们遇到的最简单的情况下，它只是将每个元素的张量连接成一个单一的张量。在我们的情况下，我们可以使用它来动态进行掩码和标签生成。这样我们就不需要存储标签，每次抽样时都会得到新的掩码。这个任务的数据整理器称为`DataCollatorForLanguageModeling`。我们使用模型的标记器和我们想要掩码的标记的分数来初始化它。我们将使用这个整理器来掩码15%的标记，这遵循了BERT论文中的程序：
- en: '[PRE77]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let’s have a quick look at the data collator in action to see what it actually
    does. To quickly show the results in a `DataFrame`, we switch the return formats
    of the tokenizer and the data collator to NumPy:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下数据整理器的操作，看看它实际上做了什么。为了快速在`DataFrame`中显示结果，我们将标记器和数据整理器的返回格式切换为NumPy：
- en: '[PRE78]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Original tokens | [CLS] | transformers | are | awesome | ! | [SEP] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 原始标记 | [CLS] | transformers | are | awesome | ! | [SEP] |'
- en: '| Masked tokens | [CLS] | transformers | are | awesome | [MASK] | [SEP] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 掩码标记 | [CLS] | transformers | are | awesome | [MASK] | [SEP] |'
- en: '| Original input_ids | 101 | 19081 | 2024 | 12476 | 999 | 102 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 原始input_ids | 101 | 19081 | 2024 | 12476 | 999 | 102 |'
- en: '| Masked input_ids | 101 | 19081 | 2024 | 12476 | 103 | 102 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 掩码input_ids | 101 | 19081 | 2024 | 12476 | 103 | 102 |'
- en: '| Labels | -100 | -100 | -100 | -100 | 999 | -100 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | -100 | -100 | -100 | -100 | 999 | -100 |'
- en: 'We see that the token corresponding to the exclamation mark has been replaced
    with a mask token. In addition, the data collator returned a label array, which
    is –100 for the original tokens and the token ID for the masked tokens. As we
    have seen previously, the entries containing –100 are ignored when calculating
    the loss. Let’s switch the format of the data collator back to PyTorch:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到与感叹号对应的标记已被替换为掩码标记。此外，数据整理器返回了一个标签数组，原始标记为-100，掩码标记的标记ID。正如我们之前看到的，包含-100的条目在计算损失时被忽略。让我们将数据整理器的格式切换回PyTorch：
- en: '[PRE79]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With the tokenizer and data collator in place, we are ready to fine-tune the
    masked language model. We set up the `TrainingArguments` and `Trainer` as usual:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '有了标记器和数据整理器，我们就可以开始微调掩码语言模型了。我们像往常一样设置`TrainingArguments`和`Trainer`： '
- en: '[PRE80]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We can access the trainer’s log history to look at the training and validation
    losses of the model. All logs are stored in `trainer.state.log_history` as a list
    of dictionaries that we can easily load into a Pandas `DataFrame`. Since the training
    and validation loss are recorded at different steps, there are missing values
    in the dataframe. For this reason we drop the missing values before plotting the
    metrics:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以访问训练器的日志历史记录，查看模型的训练和验证损失。所有日志都存储在`trainer.state.log_history`中，作为一个字典列表，我们可以轻松地加载到Pandas的`DataFrame`中。由于训练和验证损失记录在不同的步骤，数据框中存在缺失值。因此我们在绘制指标之前删除缺失值：
- en: '[PRE82]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_09in11.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in11.png)'
- en: It seems that both the training and validation loss went down considerably.
    So let’s check if we can also see an improvement when we fine-tune a classifier
    based on this model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎训练和验证损失都显著下降了。所以让我们看看当我们基于这个模型微调分类器时是否也能看到改进。
- en: Fine-Tuning a Classifier
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调分类器
- en: 'Now we’ll repeat the fine-tuning procedure, but with the slight difference
    that we load our own custom checkpoint:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重复微调过程，但有一个细微的不同，即加载我们自己的自定义检查点：
- en: '[PRE83]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Comparing the results to the fine-tuning based on vanilla BERT, we see that
    we get an advantage especially in the low-data domain. We also gain a few percentage
    points in the regime where more labeled data is available:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果与基于vanilla BERT的微调进行比较，我们发现在低数据领域特别是有优势。在更多标记数据可用的情况下，我们也获得了几个百分点的优势：
- en: '[PRE84]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '![](Images/nlpt_09in12.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_09in12.png)'
- en: This highlights that domain adaptation can provide a slight boost to the model’s
    performance with unlabeled data and little effort. Naturally, the more unlabeled
    data and the less labeled data you have, the more impact you will get with this
    method. Before we conclude this chapter, we’ll show you a few more tricks for
    taking advantage of unlabeled data.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了领域适应可以在没有标记数据和很少努力的情况下提供模型性能的轻微提升。自然地，未标记数据越多，标记数据越少，使用这种方法会产生更大的影响。在结束本章之前，我们将向您展示一些利用未标记数据的更多技巧。
- en: Advanced Methods
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级方法
- en: Fine-tuning the language model before tuning the classification head is a simple
    yet reliable method to boost performance. However, there are sophisticated methods
    than can leverage unlabeled data even further. We summarize a few of these methods
    here, which should provide a good starting point if you need more performance.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调分类头之前微调语言模型是一种简单而可靠的提升性能的方法。然而，还有更复杂的方法可以进一步利用未标记数据。我们在这里总结了一些这些方法，如果您需要更高性能，这些方法应该提供一个很好的起点。
- en: Unsupervised data augmentation
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督数据增强
- en: The key idea behind unsupervised data augmentation (UDA) is that a model’s predictions
    should be consistent for an unlabeled example and a slightly distorted one. Such
    distortions are introduced with standard data augmentation strategies such as
    token replacement and back translation. Consistency is then enforced by minimizing
    the KL divergence between the predictions of the original and distorted examples.
    This process is illustrated in [Figure 9-5](#uda), where the consistency requirement
    is incorporated by augmenting the cross-entropy loss with an additional term from
    the unlabeled examples. This means that one trains a model on the labeled data
    with the standard supervised approach, but constrains the model to make consistent
    predictions on the unlabeled data.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督数据增强（UDA）背后的关键思想是，模型对未标记的示例和略微扭曲的示例应该保持一致。这些扭曲是通过标准的数据增强策略引入的，例如标记替换和回译。然后通过最小化原始和扭曲示例的预测之间的KL散度来强制执行一致性。这个过程在[图9-5](#uda)中有所说明，其中一致性要求通过从未标记的示例中增加交叉熵损失的额外项来实现。这意味着使用标准监督方法在标记数据上训练模型，但约束模型对未标记数据进行一致的预测。
- en: '![uda](Images/nlpt_0905.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![uda](Images/nlpt_0905.png)'
- en: Figure 9-5\. Training a model M with UDA (courtesy of Qizhe Xie)
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. 使用UDA训练模型M（由Qizhe Xie提供）
- en: 'The performance of this approach is quite impressive: with a handful of labeled
    examples, BERT models trained with UDA get similar performance to models trained
    on thousands of examples. The downside is that you need a data augmentation pipeline,
    and training takes much longer since you need multiple forward passes to generate
    the predicted distributions on the unlabeled and augmented examples.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的性能相当令人印象深刻：使用UDA训练的BERT模型在有限数量的示例上获得了与数千个示例训练的模型相似的性能。缺点是您需要一个数据增强管道，并且训练时间更长，因为您需要多次前向传递来生成未标记和增强示例的预测分布。
- en: Uncertainty-aware self-training
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不确定性感知的自我训练
- en: Another promising method to leverage unlabeled data is uncertainty-aware self-training
    (UST). The idea here is to train a teacher model on the labeled data and then
    use that model to create pseudo-labels on the unlabeled data. Then a student is
    trained on the pseudo-labeled data, and after training it becomes the teacher
    for the next iteration.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 利用未标记数据的另一种有前途的方法是不确定性感知的自我训练（UST）。这里的想法是在标记数据上训练一个教师模型，然后使用该模型在未标记数据上创建伪标签。然后在伪标记数据上训练一个学生，训练后它成为下一次迭代的教师。
- en: 'One interesting aspect of this method is how the pseudo-labels are generated:
    to get an uncertainty measure of the model’s predictions the same input is fed
    several times through the model with dropout turned on. Then the variance in the
    predictions gives a proxy for the certainty of the model on a specific sample.
    With that uncertainty measure the pseudo-labels are then sampled using a method
    called Bayesian Active Learning by Disagreement (BALD). The full training pipeline
    is illustrated in [Figure 9-6](#ust).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个有趣之处在于伪标签的生成方式：为了获得模型预测的不确定性度量，相同的输入通过模型多次，同时打开辍学。然后预测的方差给出了模型对特定样本的确定性的代理。有了这种不确定性度量，然后使用一种称为贝叶斯主动学习的方法来采样伪标签。完整的训练管道在[图9-6](#ust)中有所说明。
- en: '![ust](Images/nlpt_0906.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![ust](Images/nlpt_0906.png)'
- en: Figure 9-6\. The UST method consists of a teacher that generates pseudo-labels
    and a student that is subsequently trained on those labels; after the student
    is trained it becomes the teacher and the step is repeated (courtesy of Subhabrata
    Mukherjee)^([9](ch09.xhtml#idm46238692237904))
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. UST方法包括一个生成伪标签的教师和随后在这些标签上进行训练的学生；学生训练后成为教师，然后重复这个步骤（由Subhabrata Mukherjee提供）^([9](ch09.xhtml#idm46238692237904))
- en: With this iteration scheme the teacher continuously gets better at creating
    pseudo-labels, and thus the model’s performance improves. In the end this approach
    gets within a few percent of models trained on the full training data with thousands
    of samples and even beats UDA on several datasets.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种迭代方案，教师不断改进创建伪标签的能力，因此模型的性能得到了提高。最终，这种方法在几个百分点内接近使用数千个样本进行全面训练的模型，并且在几个数据集上甚至超过了UDA。
- en: Now that we’ve seen a few advanced methods, let’s take a step back and summarize
    what we’ve learned in this chapter.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一些先进的方法，让我们退一步，总结一下我们在本章学到的内容。
- en: Conclusion
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter we’ve seen that even if we have only a few or even no labels,
    not all hope is lost. We can utilize models that have been pretrained on other
    tasks, such as the BERT language model or GPT-2 trained on Python code, to make
    predictions on the new task of GitHub issue classification. Furthermore, we can
    use domain adaptation to get an additional boost when training the model with
    a normal classification head.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到即使只有少量甚至没有标签，也不是所有的希望都已经失去。我们可以利用已经在其他任务上预训练的模型，比如BERT语言模型或在Python代码上训练的GPT-2，来对GitHub问题分类的新任务进行预测。此外，我们可以使用领域自适应在使用普通分类头训练模型时获得额外的提升。
- en: 'Which of the presented approaches will work best on a specific use case depends
    on a variety of aspects: how much labeled data you have, how noisy is it, how
    close the data is to the pretraining corpus, and so on. To find out what works
    best, it is a good idea to set up an evaluation pipeline and then iterate quickly.
    The flexible API of ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Transformers allows
    you to quickly load a handful of models and compare them without the need for
    any code changes. There are over 10,000 models on the Hugging Face Hub, and chances
    are somebody has worked on a similar problem in the past and you can build on
    top of this.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定用例上，所提出的方法中哪种方法最有效取决于各种方面：您拥有多少标记数据，它有多嘈杂，数据与预训练语料库有多接近等等。要找出最有效的方法，建立评估管道然后快速迭代是一个好主意。​![nlpt_pin01](Images/nlpt_pin01.png)⁠
    Transformers的灵活API允许您快速加载一些模型并进行比较，而无需进行任何代码更改。Hugging Face Hub上有超过10,000个模型，很可能有人过去曾经处理过类似的问题，您可以在此基础上构建。
- en: One aspect that is beyond the scope of this book is the trade-off between a
    more complex approach like UDA or UST and getting more data. To evaluate your
    approach, it makes sense to at least build a validation and test set early on.
    At every step of the way you can also gather more labeled data. Usually annotating
    a few hundred examples is a matter of a couple of hours’ or a few days’ work,
    and there are many tools that can assist you in doing so. Depending on what you
    are trying to achieve, it can make sense to invest some time in creating a small,
    high-quality dataset rather than engineering a very complex method to compensate
    for the lack thereof. With the methods we’ve presented in this chapter you can
    ensure that you get the most value out of your precious labeled data.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书超出了UDA或UST等更复杂方法与获取更多数据之间的权衡范围。为了评估你的方法，至少在早期建立验证和测试集是有意义的。在每一步，你也可以收集更多的标记数据。通常标注几百个例子只需要几个小时或几天的工作，而且有许多工具可以帮助你做到这一点。根据你想要实现的目标，投入一些时间创建一个小而高质量的数据集可能比设计一个非常复杂的方法来弥补其不足更有意义。通过本章中我们提出的方法，你可以确保充分利用你宝贵的标记数据。
- en: 'Here, we have ventured into the low-data regime and seen that transformer models
    are still powerful even with just a hundred examples. In the next chapter we’ll
    look at the complete opposite case: we’ll see what we can do when we have hundreds
    of gigabytes of data and a lot of compute. We’ll train a large transformer model
    from scratch to autocomplete code for us.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经涉足了低数据范畴，并且看到变压器模型即使只有一百个例子，仍然非常强大。在下一章中，我们将看到完全相反的情况：当我们有数百吉字节的数据和大量计算资源时，我们将会做些什么。我们将从头开始训练一个大型的变压器模型，让它为我们自动完成代码。
- en: ^([1](ch09.xhtml#idm46238704545312-marker)) Q. Xie et al., [“Unsupervised Data
    Augmentation for Consistency Training”](https://arxiv.org/abs/1904.12848), (2019);
    S. Mukherjee and A.H. Awadallah, [“Uncertainty-Aware Self-Training for Few-Shot
    Text Classification”](https://arxiv.org/abs/2006.15315), (2020).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm46238704545312-marker)) Q. Xie et al., [“Unsupervised Data
    Augmentation for Consistency Training”](https://arxiv.org/abs/1904.12848), (2019);
    S. Mukherjee and A.H. Awadallah, [“Uncertainty-Aware Self-Training for Few-Shot
    Text Classification”](https://arxiv.org/abs/2006.15315), (2020).
- en: ^([2](ch09.xhtml#idm46238696702096-marker)) We thank [Joe Davison](https://joeddav.github.io)
    for suggesting this approach to us.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm46238696702096-marker)) 我们感谢[Joe Davison](https://joeddav.github.io)向我们提出了这种方法。
- en: '^([3](ch09.xhtml#idm46238696454192-marker)) A. Williams, N. Nangia, and S.R.
    Bowman, [“A Broad-Coverage Challenge Corpus for Sentence Understanding Through
    Inference”](https://arxiv.org/abs/1704.05426), (2018); A. Conneau et al., [“XNLI:
    Evaluating Cross-Lingual Sentence Representations”](https://arxiv.org/abs/1809.05053),
    (2018).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch09.xhtml#idm46238696454192-marker)) A. Williams, N. Nangia, and S.R.
    Bowman, [“A Broad-Coverage Challenge Corpus for Sentence Understanding Through
    Inference”](https://arxiv.org/abs/1704.05426), (2018); A. Conneau et al., [“XNLI:
    Evaluating Cross-Lingual Sentence Representations”](https://arxiv.org/abs/1809.05053),
    (2018).'
- en: '^([4](ch09.xhtml#idm46238695402224-marker)) J. Wei and K. Zou, [“EDA: Easy
    Data Augmentation Techniques for Boosting Performance on Text Classification Tasks”](https://arxiv.org/abs/1901.11196),
    (2019).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch09.xhtml#idm46238695402224-marker)) J. Wei and K. Zou, [“EDA: Easy
    Data Augmentation Techniques for Boosting Performance on Text Classification Tasks”](https://arxiv.org/abs/1901.11196),
    (2019).'
- en: ^([5](ch09.xhtml#idm46238694754800-marker)) J. Johnson, M. Douze, and H. Jégou,
    [“Billion-Scale Similarity Search with GPUs”](https://arxiv.org/abs/1702.08734),
    (2017).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm46238694754800-marker)) J. Johnson, M. Douze, and H. Jégou,
    [“Billion-Scale Similarity Search with GPUs”](https://arxiv.org/abs/1702.08734),
    (2017).
- en: ^([6](ch09.xhtml#idm46238693103824-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.xhtml#idm46238693103824-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
- en: ^([7](ch09.xhtml#idm46238693077040-marker)) D. Tam et al., [“Improving and Simplifying
    Pattern Exploiting Training”](https://arxiv.org/abs/2103.11955), (2021).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.xhtml#idm46238693077040-marker)) D. Tam et al., [“Improving and Simplifying
    Pattern Exploiting Training”](https://arxiv.org/abs/2103.11955), (2021).
- en: ^([8](ch09.xhtml#idm46238693075456-marker)) T. Le Scao and A.M. Rush, [“How
    Many Data Points Is a Prompt Worth?”](https://arxiv.org/abs/2103.08493), (2021).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch09.xhtml#idm46238693075456-marker)) T. Le Scao and A.M. Rush, [“How
    Many Data Points Is a Prompt Worth?”](https://arxiv.org/abs/2103.08493), (2021).
- en: ^([9](ch09.xhtml#idm46238692237904-marker)) S. Mukherjee and A.H. Awadallah,
    [“Uncertainty-Aware Self-Training for Few-Shot Text Classification”](https://arxiv.org/abs/2006.15315),
    (2020).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch09.xhtml#idm46238692237904-marker)) S. Mukherjee and A.H. Awadallah,
    [“Uncertainty-Aware Self-Training for Few-Shot Text Classification”](https://arxiv.org/abs/2006.15315),
    (2020).
