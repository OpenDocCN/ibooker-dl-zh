- en: 1 Improving LLM accuracy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 提高LLM的准确性
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Large language models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: Limitations of large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型的局限性
- en: Shortcomings of continuously finetuning a model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续微调模型的不足
- en: Retrieval-augmented generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: Combining structured and unstructured data to support all types of questions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合结构化和非结构化数据以支持所有类型的问题
- en: Large language models (LLMs) have shown impressive abilities across a variety
    of domains, but they have significant limitations that affect their utility, particularly
    when tasked with generating accurate and up-to-date information. One widely adopted
    approach to addressing these limitations is retrieval-augmented generation (RAG),
    a workflow that combines an LLM with an external knowledge base to deliver accurate
    and current responses. By pulling data from trusted sources at run time, RAG can
    significantly reduce, though not completely eliminate, hallucinations, one of
    the most persistent challenges with LLMs. In addition, RAG allows systems to seamlessly
    bridge general knowledge with niche, domain-specific information that may not
    be well represented in the pretraining data of the model. Despite these advantages,
    RAG implementations have often focused solely on unstructured data, overlooking
    the potential of structured sources like knowledge graphs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在多个领域展现出了令人印象深刻的能力，但它们存在显著的局限性，影响了它们的实用性，尤其是在需要生成准确和最新信息的情况下。解决这些局限性的一个广泛采用的方法是检索增强生成（RAG），这是一种将LLM与外部知识库相结合的工作流程，以提供准确和最新的响应。通过在运行时从可信来源提取数据，RAG可以显著减少，尽管不能完全消除，幻觉，这是LLMs最持久的挑战之一。此外，RAG允许系统无缝地将通用知识与模型预训练数据中可能没有得到良好表示的特定领域信息相结合。尽管有这些优势，RAG的实现往往只关注非结构化数据，而忽略了如知识图谱这样的结构化来源的潜力。
- en: Knowledge graphs are structured representations of entities, their attributes,
    and their relationships, offering a semantic framework that bridges structured
    and unstructured data. For instance, a customer support transcript is unstructured
    text, while a product catalog or user database is structured. Bridging them means
    enabling a system to connect conversational mentions of “my recent laptop order”
    to the structured record of the exact model, purchase date, and warranty status.
    Knowledge graphs serve as a critical component to RAG by enabling accurate, context-rich,
    and interconnected information retrieval—such as linking a customer query about
    a drug interaction to structured medical guidelines, prior case studies, and the
    patient’s history in real time. Integrating knowledge graphs into RAG pipelines
    can overcome LLM limitations, enhance data retrieval, and facilitate a holistic
    approach to managing and using diverse data types across domains like healthcare,
    finance, and technical support.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是实体、它们的属性及其关系的结构化表示，提供了一个语义框架，将结构化和非结构化数据连接起来。例如，客户支持记录是不结构化的文本，而产品目录或用户数据库是结构化的。连接它们意味着使系统能够将关于“我最近的笔记本电脑订单”的对话提及与精确型号、购买日期和保修状态的记录相连接。知识图谱通过实现准确、丰富上下文和互联的信息检索——例如，将关于药物相互作用的客户查询实时链接到结构化的医疗指南、先前案例研究和患者的病史——成为RAG的一个关键组件。将知识图谱集成到RAG管道中可以克服LLM的局限性，增强数据检索，并促进在医疗保健、金融和技术支持等领域的跨领域数据类型管理和使用的整体方法。
- en: This book is for developers, researchers, and data practitioners who want to
    build more robust, explainable, and capable RAG systems. You’ll learn both how
    to augment existing RAG architectures with knowledge graphs and how to build new
    GraphRAG pipelines from scratch. Along the way, you’ll gain practical skills in
    data modeling, graph construction, retrieval workflows, and system evaluation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书面向希望构建更稳健、可解释和强大的RAG系统的开发者、研究人员和数据从业者。您将学习如何通过知识图谱增强现有的RAG架构，以及如何从头开始构建新的GraphRAG管道。在这个过程中，您将在数据建模、图构建、检索工作流程和系统评估方面获得实际技能。
- en: By the end of this book, you’ll have a clear understanding of how LLMs, RAG,
    and knowledge graphs intersect to create robust systems capable of addressing
    complex queries and delivering accurate, reliable, and explainable results.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完本书之后，您将清楚地了解LLMs、RAG和知识图谱如何交叉，以创建能够解决复杂查询并交付准确、可靠和可解释结果的稳健系统。
- en: 1.1 Introduction to LLMs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 LLMs 简介
- en: By now, you’ve likely encountered or heard about ChatGPT, one of the most prominent
    examples of conversational AI. ChatGPT is a conversational user interface developed
    by OpenAI and powered by LLMs, such as GPT-4 (OpenAI et al., 2024). LLMs are built
    on transformer architecture (Vaswani et al., 2017), which enables them to process
    and generate text efficiently. These models are trained on vast amounts of textual
    data, allowing them to learn patterns, grammar, context, and even some degree
    of reasoning. The training process involves feeding the model large datasets that
    include a diverse range of text with the primary objective of enabling the model
    to accurately predict the next word in a sequence. This extensive exposure enables
    the models to understand and generate human-like text based on the patterns they
    have learned from the data. For example, if you use “Never gonna” as input to
    an LLM, you might get a response similar to that shown in figure 1.1\.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经遇到或听说过ChatGPT，这是对话式AI中最突出的例子之一。ChatGPT是由OpenAI开发的一个对话式用户界面，由LLM（如GPT-4，OpenAI
    et al.，2024）提供支持。LLM建立在Transformer架构（Vaswani et al.，2017）之上，这使得它们能够高效地处理和生成文本。这些模型在大量文本数据上训练，使它们能够学习模式、语法、上下文，甚至一定程度上的推理。训练过程涉及向模型提供包含各种文本的大型数据集，主要目标是使模型能够准确预测序列中的下一个单词。这种广泛的接触使模型能够根据从数据中学到的模式理解和生成类似人类的文本。例如，如果你将“Never
    gonna”作为输入给LLM，你可能会得到类似于图1.1所示的响应。
- en: Figure 1.1 shows an LLM processing the input “Never gonna” and generating the
    output “give you up.” This highlights how an LLM relies on patterns and associations
    it learned during training, such as those derived from common cultural references,
    including popular music. The quality and relevance of these responses depend significantly
    on the diversity and depth of the training dataset, which determines the LLM’s
    ability to recognize and replicate such patterns.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1展示了一个LLM处理输入“Never gonna”并生成输出“give you up”的过程。这突显了LLM如何依赖于在训练期间学习的模式和相关联，例如从常见的文化参考中得出的，包括流行音乐。这些响应的质量和相关性在很大程度上取决于训练数据集的多样性和深度，这决定了LLM识别和复制此类模式的能力。
- en: '![figure](../Images/1-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-1.png)'
- en: Figure 1.1 LLMs are trained to predict the next word.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 LLM被训练来预测下一个单词。
- en: While LLMs excel at generating contextually appropriate text, they are far more
    than just autocomplete systems. Their remarkable ability to follow instructions
    and adapt to a wide range of tasks is impressive. For example, as shown in figure
    1.2, you can ask ChatGPT to generate a haiku about a specific topic in a particular
    style. This capability illustrates not just pattern recognition but an understanding
    of task-specific instructions, enabling creative and nuanced outputs well beyond
    simple text prediction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LLM在生成上下文相关的文本方面表现出色，但它们远不止是自动完成系统。它们遵循指令和适应广泛任务的能力令人印象深刻。例如，如图1.2所示，你可以要求ChatGPT以特定风格就特定主题生成俳句。这种能力不仅体现了模式识别，还体现了对特定任务指令的理解，能够产生超越简单文本预测的创造性和细微的输出。
- en: '![figure](../Images/1-2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-2.png)'
- en: Figure 1.2 Writing a haiku with ChatGPT
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 使用ChatGPT写俳句
- en: The ability of LLMs to follow instructions and generate diverse, complex outputs,
    whether crafting a haiku or providing structured responses, goes beyond simply
    predicting the next word in a sequence. This ability to understand and execute
    detailed instructions makes LLMs uniquely suited for a wide variety of tasks.
    In this book, you will use this instruction-following ability to design and refine
    RAG pipelines. By tapping into instruction-following capabilities, you can integrate
    retrieval components more effectively, tailor responses to specific contexts,
    and optimize your systems for accuracy and usability.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM遵循指令和生成多样化、复杂输出的能力，无论是创作俳句还是提供结构化响应，都超越了简单地预测序列中的下一个单词。这种理解和执行详细指令的能力使LLM非常适合广泛的任务。在这本书中，你将利用这种遵循指令的能力来设计和优化RAG管道。通过利用遵循指令的能力，你可以更有效地集成检索组件，针对特定上下文定制响应，并优化你的系统以提高准确性和可用性。
- en: ChatGPT’s breadth of general knowledge is equally remarkable. For example, figure
    1.3 illustrates ChatGPT’s response when prompted about the first manned moon landing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的通用知识范围同样引人注目。例如，图1.3展示了当被提示关于第一次载人登月时ChatGPT的响应。
- en: '![figure](../Images/1-3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-3.png)'
- en: Figure 1.3 Retrieving factual information from ChatGPT
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3 从ChatGPT检索事实信息
- en: If you verify this response with external information from NASA or Wikipedia,
    you can observe that the model produces an accurate response with no false information.
    Such a response might give you the impression that an LLM constructs a vast database
    of facts from which it can retrieve when prompted. However, the model doesn’t
    store specific facts, events, or information from its training dataset. Instead,
    it develops complex mathematical representations of the language it is trained
    on. Remember, the LLMs are based on the transformer, which is a deep learning
    architecture based on neural networks to predict the next word, as shown in figure
    1.4.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用来自NASA或维基百科的外部信息来验证这个回答，你可以观察到模型产生了一个准确无误的回答，没有错误信息。这样的回答可能会让你觉得LLM构建了一个庞大的事实数据库，可以从其中检索信息。然而，模型并不存储其训练数据集中的具体事实、事件或信息。相反，它发展了其训练语言复杂数学表示。记住，LLMs基于Transformer，这是一种基于神经网络的深度学习架构，用于预测下一个单词，如图1.4所示。
- en: 'Figure 1.4 illustrates a neural network predicting the next word in a sequence,
    similar to how LLMs function. The central part shows the network with multiple
    layers of neurons, connected by lines that represent the flow of information.
    Each connection has a weight, such as the example value 0.04, which influences
    the strength of the connection. During training, the model learns the values of
    these weights to improve its predictions. When asked about a specific historical
    event, an LLM doesn’t recall the event from its training data. Instead, it generates
    a response based on the learned weights in its neural network, similar to predicting
    the next word in a sequence. Therefore, while LLMs can provide seemingly knowledgeable
    answers, their responses are based on these learned weights rather than explicit
    memory. To quote Andrej Karpathy: “We kind of understand that they (LLMs) build
    and maintain some kind of a knowledge database, but even this knowledge base is
    very strange and imperfect and weird”([https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40](https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4展示了神经网络预测序列中的下一个单词，这与LLMs的工作方式相似。中心部分显示了具有多层神经元的网络，通过代表信息流的线条连接。每个连接都有一个权重，例如示例值0.04，它影响连接的强度。在训练过程中，模型学习这些权重的值以提高其预测能力。当被问及一个具体的历史事件时，LLM不会从其训练数据中回忆起该事件。相反，它根据其神经网络中学习的权重生成一个回答，类似于预测序列中的下一个单词。因此，虽然LLMs可以提供看似知识渊博的答案，但它们的回答是基于这些学习的权重，而不是明确的记忆。正如Andrej
    Karpathy所说：“我们多少理解了它们（LLMs）构建并维护某种知识数据库，但即使这个知识库也非常奇怪、不完美且奇怪”([https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40](https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40))。
- en: '![figure](../Images/1-4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-4.png)'
- en: Figure 1.4 Neural network trained to predict the next word based on the input
    sequence of words
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 基于输入序列单词预测下一个单词的神经网络
- en: 1.2 Limitations of LLMs
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 LLMs的限制
- en: LLMs represent a groundbreaking step in the evolution of AI, offering remarkable
    capabilities across a range of applications. Yet, as with any transformative technology,
    they are not without their challenges and constraints. In the following section,
    we will delve into some of these limitations and their implications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs代表了人工智能进化中的一个里程碑，在众多应用领域提供了令人瞩目的能力。然而，正如任何变革性技术一样，它们也面临着挑战和限制。在接下来的部分，我们将深入探讨一些这些限制及其影响。
- en: 1.2.1 Knowledge cutoff problem
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 知识截止问题
- en: The most obvious limitation is that LLMs are unaware of events or information
    not included in their training dataset. At this moment, ChatGPT is aware of information
    that occurred up to October 2023\. For example, if you asked ChatGPT about an
    event in 2024, you would get a response similar to that shown in figure 1.5\.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的限制是LLMs对其训练数据集中未包含的事件或信息一无所知。目前，ChatGPT了解的信息截止到2023年10月。例如，如果你问ChatGPT关于2024年发生的事件，你会得到类似于图1.5所示的回答。
- en: In the context of LLMs, the *knowledge cutoff date* refers to the most recent
    point at which the model’s training data includes information. The model has access
    to a broad spectrum of text data containing information about events up to this
    date from diverse sources, which it utilizes to generate responses and provide
    information. Anything that has occurred or been published after this cutoff date
    is unknown to the model as it was not included in the training dataset; therefore,
    it cannot provide information about events, developments, or research that occurred
    after the cutoff date.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型的背景下，*知识截止日期*指的是模型训练数据中包含信息的最新点。模型可以访问包含有关截止日期之前事件信息的广泛文本数据，这些数据来自不同的来源，它利用这些数据生成回复和提供信息。任何在截止日期之后发生或发表的事情，由于它没有被包含在训练数据集中，因此模型不知道，因此它不能提供关于截止日期之后发生的事件、发展或研究的信息。
- en: '![figure](../Images/1-5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-5.png)'
- en: Figure 1.5 Example of a knowledge cutoff date disclaimer
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.5 知识截止日期声明示例
- en: 1.2.2 Outdated information
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 过时信息
- en: A less obvious limitation is that LLMs can sometimes provide outdated responses.
    While they can deliver detailed and accurate information up until their knowledge
    cutoff, they may not reflect recent developments. For instance, as of late 2023,
    Mark Cuban sold his majority stake in the Dallas Mavericks franchise to the Adelson
    family and the Dumonts while retaining a minority share. This major update highlights
    how information that was correct in the past can become outdated. For example,
    in a query about the Dallas Mavericks, a response shown in figure 1.6 reflects
    Cuban as the sole owner, which is no longer accurate (Rader, 2023).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不那么明显的限制是，大型语言模型有时会提供过时的回复。尽管它们可以提供详细和准确的信息，直到它们的截止日期，但它们可能不会反映最近的发展。例如，截至
    2023 年底，马克·库班将其在达拉斯小牛队股份中的多数股权出售给阿德尔森家族和杜蒙特家族，同时保留少数股权。这一重大更新突显了过去正确的信息如何变得过时。例如，在关于达拉斯小牛队的查询中，图
    1.6 中显示的回复反映了库班是唯一的所有者，这已经不再准确（Rader，2023）。
- en: '![figure](../Images/1-6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-6.png)'
- en: Figure 1.6 Sometimes ChatGPT responds with outdated information.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.6 有时 ChatGPT 会回复过时的信息。
- en: This highlights the importance of regularly updating training data for models
    or enabling them to access real-time information. With continuously evolving events
    and facts, even small details like ownership structures can significantly impact
    how we perceive an organization or individual. This limitation underlines the
    importance of ensuring AI systems remain accurate and relevant in dynamic environments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了定期更新模型训练数据或使它们能够访问实时信息的重要性。随着事件和事实的不断演变，即使是所有权结构这样的小细节也可能对我们感知一个组织或个人产生重大影响。这种限制强调了确保人工智能系统在动态环境中保持准确和相关的必要性。
- en: 1.2.3 Pure hallucinations
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 纯幻觉
- en: Another well-known limitation of LLMs is their tendency to provide assertive,
    confident answers—even when those answers contain incorrect or fabricated information.
    One might assume that, despite their knowledge cutoff dates, these models provide
    accurate factual data up to that point. However, even information about events
    that occurred before the cutoff can be unreliable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个大型语言模型广为人知的限制是，它们倾向于提供自信、肯定的答案——即使这些答案包含错误或编造的信息。人们可能会假设，尽管它们的截止日期，这些模型提供的数据直到那个点都是准确的。然而，甚至关于截止日期之前发生的事件的信息也可能不可靠。
- en: A striking example of this occurred when lawyers in the United States submitted
    bogus, fictitious legal citations to a court, unaware that they had been generated
    by ChatGPT (Neumeister, 2023). These kinds of confident inaccuracies are commonly
    known as hallucinations, where the model outputs information that sounds plausible
    but is factually incorrect or entirely fabricated. External references such as
    URLs, academic citations, or identifiers like WikiData IDs are especially prone
    to this behavior.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一个显著例子发生在美国律师提交了由 ChatGPT 生成的虚假、虚构的法律引证到法庭，而他们并不知道这些引证是由 ChatGPT 生成的（Neumeister，2023）。这类自信的不准确通常被称为幻觉，即模型输出看似合理但实际上是错误或完全编造的信息。外部引用，如
    URL、学术引用或 WikiData IDs 这样的标识符，特别容易产生这种行为。
- en: Hallucinations occur because LLMs are not reasoning engines. They are probabilistic
    language models trained to predict what sounds like a good next token, based on
    patterns in their training data. They don’t know facts the way humans do. Rather,
    they generate text by guessing the most likely continuation, regardless of whether
    it’s true. This fundamental difference between statistical pattern matching and
    actual understanding is what separates LLMs from human cognition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉发生是因为LLMs（大型语言模型）不是推理引擎。它们是基于其训练数据中的模式训练出来的概率语言模型，预测出听起来像是一个好的下一个标记。它们不知道事实，就像人类那样。相反，它们通过猜测最可能的后续内容来生成文本，无论其是否真实。这种统计模式匹配与实际理解之间的基本差异是LLMs与人类认知的区别。
- en: To illustrate this, we can ask ChatGPT to provide the WikiData ID of the Dallas
    Mavericks NBA franchise. As shown in figure 1.7, the model confidently returns
    an identifier—but it’s incorrect.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们可以要求ChatGPT提供达拉斯小牛NBA球队的WikiData ID。如图1.7所示，模型自信地返回了一个标识符——但它是不正确的。
- en: '![figure](../Images/1-7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-7.png)'
- en: Figure 1.7 ChatGPT can produce responses with incorrect information.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7 ChatGPT可以生成包含错误信息的回复。
- en: The model assertively replied with an ID that follows the WikiData format. However,
    if you verify this information, you can observe that Q152232 is the WikiData ID
    of the movie titled *Womanlight* ([https://www.wikidata.org/wiki/Q152232](https://www.wikidata.org/wiki/Q152232)).
    Therefore, users must recognize that LLMs, while often informative, are not infallible
    and can produce erroneous information. It’s crucial to approach their responses
    critically and verify their accuracy through reliable external sources, especially
    in contexts where precision and factual correctness are central.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型断然回复了一个遵循WikiData格式的ID。然而，如果你验证这个信息，你会观察到Q152232是电影《Womanlight》的WikiData ID（[https://www.wikidata.org/wiki/Q152232](https://www.wikidata.org/wiki/Q152232)）。因此，用户必须认识到，尽管LLMs通常很有信息量，但它们并非完美无缺，可能会产生错误信息。批判性地对待它们的回复，并通过可靠的外部来源验证其准确性至关重要，特别是在精确性和事实正确性至关重要的环境中。
- en: 1.2.4 Lack of private information
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 缺乏私人信息
- en: If you were building a company chatbot using an LLM, you’d likely want it to
    answer questions involving internal or proprietary information that isn’t publicly
    available. In such cases, even if the information or events occurred before the
    LLM’s knowledge cutoff date, they wouldn’t have been part of its training data.
    As a result, the model cannot generate accurate responses for such queries, as
    illustrated in figure 1.8\.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用LLM构建一个公司聊天机器人，你可能会希望它能够回答涉及内部或专有信息的问题，这些信息不是公开可用的。在这种情况下，即使信息或事件发生在LLM的知识截止日期之前，它们也不会成为其训练数据的一部分。因此，该模型无法为这类查询生成准确的回复，如图1.8所示。
- en: '![figure](../Images/1-8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-8.png)'
- en: Figure 1.8 ChatGPT didn’t have access to some private or confidential information
    during training.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8 ChatGPT在训练期间没有访问某些私人或机密信息。
- en: One potential solution would be to make the company’s internal information publicly
    available in the hope that it gets included in the training dataset of an LLM.
    However, this approach is neither practical nor secure. Instead, we will explore
    and demonstrate more effective strategies to overcome these limitations while
    maintaining data privacy and control.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是将公司的内部信息公开，希望它被包含在LLM的训练数据集中。然而，这种方法既不实用也不安全。相反，我们将探索并展示更有效的策略来克服这些限制，同时保持数据隐私和控制。
- en: Note on other limitations of LLMs
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于LLMs其他局限性的说明
- en: While this book will focus on the limitations of LLMs in providing factually
    correct and up-to-date information in responses, it’s important to acknowledge
    that LLMs also have other restrictions. Some of these include
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书将重点关注LLMs在提供事实正确和最新信息方面的局限性，但承认LLMs也有其他限制是很重要的。其中一些包括
- en: '*Bias in responses*—LLMs can sometimes generate biased responses, reflecting
    biases present in the training data.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回复中的偏见*——LLMs有时会生成带有偏见的回复，反映了训练数据中存在的偏见。'
- en: '*Lack of understanding and context*—LLMs, despite their complexity, do not
    truly understand the text. They process language based on patterns learned from
    data, which means they can miss nuances and contextual subtleties.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏理解和上下文*——尽管LLMs非常复杂，但它们并不真正理解文本。它们根据从数据中学习到的模式处理语言，这意味着它们可能会错过细微差别和上下文细微之处。'
- en: '*Vulnerability to prompt injection*—LLMs are susceptible to prompt injection
    attacks, where malicious users craft inputs to manipulate the model into generating
    inappropriate, biased, or harmful responses. This vulnerability poses significant
    challenges for ensuring the security and integrity of LLM applications in real-world
    scenarios.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易受提示注入攻击的影响*—LLMs容易受到提示注入攻击，恶意用户精心设计输入以操纵模型生成不适当、有偏见或有害的响应。这种漏洞对确保LLM应用在现实场景中的安全和完整性构成了重大挑战。'
- en: '*Inconsistent responses*—LLMs can produce different answers to the same question
    across multiple interactions. This inconsistency arises from their probabilistic
    nature and lack of persistent memory, which can hinder their usefulness in applications
    that require stability and repeatability.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不一致的响应*—大型语言模型（LLMs）在多次交互中可能会对同一问题给出不同的答案。这种不一致性源于它们的概率性质和缺乏持久记忆，这可能会阻碍它们在需要稳定性和可重复性的应用中的有用性。'
- en: This book is dedicated to exploring and addressing the specific limitations
    of LLMs concerning the generation of factually accurate and up-to-date responses.
    Although we recognize other limitations of LLMs, our discussion will not cover
    them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本书致力于探索和解决LLMs在生成事实准确和最新响应方面的具体局限性。尽管我们认识到LLMs的其他局限性，但我们的讨论将不会涉及它们。
- en: 1.3 Overcoming the limitations of LLMs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 克服LLMs的局限性
- en: LLMs are powerful tools, but they often face limitations when handling domain-specific
    questions or accessing specialized, up-to-date knowledge. Implementing a ChatGPT-like
    application in a business environment requires outputs that are both precise and
    factually accurate. To overcome these challenges, we can inject domain-specific
    knowledge into LLMs using approaches like supervised finetuning and RAG. In this
    section, we’ll explore how these methods work and how they can be applied to inject
    domain-specific knowledge into LLMs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是强大的工具，但它们在处理特定领域的问题或访问专业、最新知识时往往面临局限性。在商业环境中实施类似ChatGPT的应用需要既精确又符合事实的输出。为了克服这些挑战，我们可以通过监督微调和RAG等方法将特定领域的知识注入LLMs。在本节中，我们将探讨这些方法是如何工作的，以及如何将它们应用于将特定领域知识注入LLMs。
- en: 1.3.1 Supervised finetuning
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 监督微调
- en: 'At first, many of us thought we would overcome the limitations of LLMs with
    additional training. For example, we could overcome the knowledge cutoff date
    limitation by continuously updating the model. However, to address this limitation
    effectively, we first need to better understand the training of an LLM. The training
    of an LLM like ChatGPT can be split into the following four stages, as described
    by Andrew Karpathy ([https://www.youtube.com/watch?v=bZQun8Y4L2A](https://www.youtube.com/watch?v=bZQun8Y4L2A)):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们中的许多人认为我们可以通过额外的训练来克服LLMs的局限性。例如，我们可以通过持续更新模型来克服知识截止日期的局限性。然而，为了有效地解决这个问题，我们首先需要更好地理解LLMs的训练。像ChatGPT这样的LLMs的训练可以分解为以下四个阶段，如Andrew
    Karpathy所述（[https://www.youtube.com/watch?v=bZQun8Y4L2A](https://www.youtube.com/watch?v=bZQun8Y4L2A)）：
- en: '*Pretraining* —The model reads a vast amount of text, often more than a trillion
    tokens, to learn basic language patterns. It practices predicting what word comes
    next in a sentence. This is the foundational step, like learning vocabulary and
    grammar before you can write. This is the most resource-intensive phase, which
    can require thousands of GPUs and can take months of continuous training.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*预训练*—模型阅读大量文本，通常超过万亿个标记，以学习基本语言模式。它练习预测句子中下一个单词。这是基础步骤，就像在学习写作之前先学习词汇和语法一样。这是最资源密集的阶段，可能需要数千个GPU，并且可能需要数月的持续训练。'
- en: '*Supervised finetuning* —The model is given specific examples of high-quality
    conversations to improve its ability to respond like a helpful assistant. It continues
    to practice language but now with a focus on generating useful and accurate responses.
    Think of it as moving from basic language learning to practicing conversation
    skills. This requires significantly fewer resources than pretraining and can nowadays
    even run on a single laptop for smaller LLMs.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监督微调*—模型被提供高质量对话的特定示例，以提高其像有帮助的助手一样响应的能力。它继续练习语言，但现在专注于生成有用和准确的响应。把它想象成从基本语言学习到练习对话技能的转变。这比预训练需要显著更少的资源，对于较小的LLMs，现在甚至可以在单个笔记本电脑上运行。'
- en: '*Reward modeling* —The model learns to distinguish between good and bad responses
    by comparing different answers to the same questions. It’s like having a coach
    who shows the model what a good performance looks like so it can aim to replicate
    that quality.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*奖励建模* — 模型通过比较对同一问题的不同答案来学习区分好与坏的回答。这就像有一个教练向模型展示良好表现的样子，以便它能努力复制这种质量。'
- en: '*Reinforcement learning* —The model interacts with users or simulated environments
    to further refine its responses based on feedback. It’s similar to learning a
    sport: practicing not just by drills but by playing actual games and learning
    from the experience.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*强化学习* — 模型通过与用户或模拟环境互动，根据反馈进一步细化其回答。这类似于学习一项运动：不仅通过练习，还要通过实际比赛并从经验中学习。'
- en: Since the pretraining phase is costly and time consuming and, therefore, not
    feasible for continuous updating, the idea was to use the supervised finetuning
    phase to overcome the limitations of LLMs. During the supervised finetuning phase,
    you supply the language model with specific examples of input prompts along with
    the corresponding desired outputs you aim for the model to produce. One such example
    is shown in figure 1.9\.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练阶段成本高昂且耗时，因此不适合持续更新，因此想法是利用监督微调阶段来克服 LLM 的局限性。在监督微调阶段，你向语言模型提供特定输入提示的示例，以及你希望模型产生的相应期望输出。图
    1.9 展示了这样一个示例。
- en: '![figure](../Images/1-9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-9.png)'
- en: Figure 1.9 Sample record of a supervised finetuning dataset
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.9 监督微调数据集的样本记录
- en: Figure 1.9 shows an example of a question–answer pair that could be used to
    finetune an LLM. In this example, the input prompt or the question is about which
    team won the 2023 NBA championship, and the corresponding answer is the Denver
    Nuggets. The theory was that, through this example, the LLM would include this
    fact in its mathematical representation of the language and be able to answer
    questions revolving around the 2023 NBA champions. Some research studies have
    shown that supervised finetuning can improve LLM factuality (Tian et al., 2023).
    However, other studies using different methods also show that LLMs struggle to
    learn new factual information through finetuning (Ovadia et al., 2023).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 展示了一个可用于微调 LLM 的问答对示例。在这个例子中，输入提示或问题是关于哪个队伍赢得了 2023 年 NBA 总冠军，相应的答案是丹佛掘金队。理论上是，通过这个例子，LLM
    会将其包含在其语言的数学表示中，并能够回答围绕 2023 年 NBA 冠军的问题。一些研究已经表明，监督微调可以提高 LLM 的真实性（Tian 等人，2023
    年）。然而，其他使用不同方法的研究也表明，LLM 在通过微调学习新事实信息方面存在困难（Ovadia 等人，2023 年）。
- en: While supervised finetuning can enhance the overall knowledge of a model, it
    remains a complex and evolving field of research. As such, deploying a reliable,
    finetuned language model in a production environment poses significant challenges
    at the current stage of technological development. Fortunately, a more efficient
    and simpler method to address the knowledge limitations of LLMs exists.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然监督微调可以增强模型的整体知识，但它仍然是一个复杂且不断发展的研究领域。因此，在当前技术发展阶段，在生产环境中部署一个可靠、经过微调的语言模型面临着重大挑战。幸运的是，存在一种更高效、更简单的方法来解决
    LLM 的知识局限性。
- en: 1.3.2 Retrieval-augmented generation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 检索增强生成
- en: The second strategy for improving LLM accuracy and overcoming its limitations
    is the RAG workflow, which combines an LLM with an external knowledge base to
    deliver accurate and up-to-date responses. Instead of depending on an LLM’s internal
    knowledge, relevant facts or information are provided directly in the input prompt
    (Lewis et al., 2020). This concept (RAG) uses the LLM’s strengths in understanding
    and generating natural language, while factual information is supplied in the
    prompt, reducing dependence on the LLM’s internal knowledge base and consequently
    hallucinations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 提高 LLM 准确率并克服其局限性的第二种策略是 RAG 工作流程，该流程结合了 LLM 和外部知识库，以提供准确和最新的回答。而不是依赖于 LLM 的内部知识，相关事实或信息直接在输入提示中提供（Lewis
    等人，2020 年）。这个概念（RAG）利用了 LLM 在理解和生成自然语言方面的优势，而事实信息则通过提示提供，从而减少了对外部知识库的依赖，并因此减少了幻觉。
- en: 'The RAG workflow operates in two main stages:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 工作流程分为两个主要阶段：
- en: Retrieval
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索
- en: Augmented generation
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强生成
- en: In the retrieval stage, relevant information is located from an external knowledge
    base or database. During the augmented generation stage, this retrieved information
    is combined with the user’s input to enhance the context provided to the LLM,
    enabling it to generate a response grounded in reliable, external facts. The RAG
    workflow is illustrated in figure 1.10.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索阶段，相关信息从外部知识库或数据库中定位。在增强生成阶段，检索到的信息与用户的输入相结合，以增强提供给LLM的上下文，使其能够生成基于可靠、外部事实的响应。RAG工作流程如图1.10所示。
- en: '![figure](../Images/1-10.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-10.png)'
- en: Figure 1.10 Providing relevant information to the LLM as part of the input
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10 将相关信息作为输入提供给LLM
- en: As mentioned, LLMs are great at understanding natural language and following
    instructions in the prompt. In the RAG workflow, the goal shifts to task-oriented
    response generation, where LLMs follow a set of instructions. The process involves
    utilizing a retrieval tool to fetch relevant documents from a specific knowledge
    base. The LLM then generates answers based on the provided documents, ensuring
    responses are accurate, contextually relevant, and aligned with specific guidelines.
    This systematic approach transforms the answer generation process into a targeted
    task of inspecting and using the retrieved information to produce the final answer.
    An example of providing factual information in the input prompt is shown in figure
    1.11.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM擅长理解自然语言并遵循提示中的指令。在RAG工作流程中，目标转向任务导向的响应生成，其中LLM遵循一系列指令。该过程涉及使用检索工具从特定知识库中检索相关文档。然后，LLM根据提供的文档生成答案，确保响应准确、上下文相关，并符合特定指南。这种系统方法将答案生成过程转化为一个目标任务，即检查和使用检索到的信息来生成最终答案。在输入提示中提供事实信息的示例如图1.11所示。
- en: Figure 1.11 illustrates an example of how an LLM processes follows the prompt
    instructions of a RAG workflow. The prompt highlights the importance of using
    retrieved context to ensure accurate and relevant responses and can be broken
    down into
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11展示了LLM如何遵循RAG工作流程的提示指令的示例。提示强调了使用检索到的上下文确保准确和相关性响应的重要性，并且可以分解为
- en: '*Provided context* —A factual statement that introduces relevant information—in
    this case, identifying the Denver Nuggets as the 2023 NBA champions with a 4:1
    victory over the Miami Heat. This acts as the knowledge base input for the LLM.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提供的内容* — 一个事实陈述，引入相关信息——在这种情况下，确定丹佛掘金队以4:1战胜迈阿密热火队成为2023年NBA冠军。这作为LLM的知识库输入。'
- en: '*User query* —A specific question, “Who won the 2023 NBA championship?” which
    directs the LLM to extract relevant information from the provided context.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户查询* — 一个具体的问题，“谁赢得了2023年NBA冠军？”这个问题指导LLM从提供的内容中提取相关信息。'
- en: '*Generated answer* —The LLM’s response is aligned with the retrieved context:
    “The Denver Nuggets won the 2023 NBA championship.”'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成的答案* — LLM的响应与检索到的上下文一致：“丹佛掘金队赢得了2023年NBA冠军。”'
- en: '![figure](../Images/1-11.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-11.png)'
- en: Figure 1.11 Providing relevant information to the answer as part of the prompt
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.11 将相关信息作为提示的一部分提供
- en: You might wonder what the advantage of the RAG process is if the user has to
    provide both the context and the questions. In practice, the retrieval system
    operates independently from the user. The user only needs to provide the question,
    while the retrieval process occurs behind the scenes, as illustrated in figure
    1.12.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如果用户必须提供内容和问题，RAG过程的优点是什么。在实践中，检索系统独立于用户操作。用户只需要提供问题，而检索过程在幕后进行，如图1.12所示。
- en: '![figure](../Images/1-12.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-12.png)'
- en: Figure 1.12 Populating the relevant data from the user and knowledge base into
    the prompt template and then passing it to an LLM to generate the final answer
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.12 将用户和知识库中的相关信息填充到提示模板中，然后传递给LLM生成最终答案
- en: In the RAG process, the user starts by asking a question. Behind the scenes,
    the system turns that question into a search query and retrieves relevant information
    from sources like company documents, knowledge articles, or databases. Advanced
    retrieval algorithms find the most suitable content, which is then combined with
    the original question to form an enriched prompt. This prompt is sent to an LLM,
    which generates a response based on both the question and the retrieved context.
    The entire retrieval process is automatic, and no extra input is required beyond
    the original question from the user. This makes RAG both seamless and effective,
    improving factual accuracy while reducing the chance of hallucinated answers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG过程中，用户首先提出一个问题。在幕后，系统将这个问题转换为一个搜索查询，并从公司文档、知识文章或数据库等来源检索相关信息。高级检索算法找到最合适的内容，然后将这些内容与原始问题结合，形成一个丰富的提示。这个提示被发送到一个LLM，LLM根据问题和检索到的上下文生成响应。整个检索过程是自动的，除了用户原始的问题外，不需要额外的输入。这使得RAG既无缝又有效，提高了事实准确性，同时减少了幻觉答案的可能性。
- en: The RAG approach has gained mainstream popularity due to its simplicity and
    efficiency. It is now also part of the ChatGPT interface, where the LLM can use
    Web Search to search for relevant information before generating the final answer.
    Users of the paid version of ChatGPT may be familiar with the RAG process as depicted
    in figure 1.13.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RAG方法因其简单性和高效性而获得了主流的认可。现在，它也成为了ChatGPT界面的一个部分，其中LLM可以在生成最终答案之前使用网络搜索来查找相关信息。ChatGPT付费版本的用户可能对图1.13中描述的RAG过程比较熟悉。
- en: '![figure](../Images/1-13.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-13.png)'
- en: Figure 1.13 ChatGPT uses Web Search to find relevant information to generate
    an up-to-date answer.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.13 ChatGPT使用网络搜索来查找相关信息以生成一个最新的答案。
- en: While the exact implementation of RAG in ChatGPT is not publicly disclosed,
    we can try to infer what it does under the hood. When the LLM decides, for whatever
    reason, that it needs to pull additional information, it can input a query into
    Web Search. We don’t know precisely how it navigates through search results, parses
    information from web pages, or decides that it has retrieved sufficient information.
    Nevertheless, we know that it used `2023` `NBA` `championship` `winner` keyword
    as input to Web Search and generated the final response based on the information
    available on the official NBA website ([https://www.nba.com/playoffs/2023/the-finals](https://www.nba.com/playoffs/2023/the-finals)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ChatGPT中RAG的确切实现并未公开披露，但我们仍可以尝试推断其内部的工作原理。当LLM出于任何原因决定需要获取额外信息时，它可以向网络搜索输入一个查询。我们不清楚它如何精确地导航搜索结果，从网页中解析信息，或者决定已经检索到足够的信息。然而，我们知道它使用了`2023`
    `NBA` `championship` `winner`关键词作为网络搜索的输入，并根据官方NBA网站（[https://www.nba.com/playoffs/2023/the-finals](https://www.nba.com/playoffs/2023/the-finals)）上的信息生成了最终响应。
- en: 1.4 Knowledge graphs as the data storage for RAG applications
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 知识图谱作为RAG应用的数据存储
- en: When planning to implement a RAG application, choosing the right storage solution
    is important. While there are many database options, we argue that knowledge graphs
    and graph databases are especially well suited for most RAG applications. A knowledge
    graph is a data structure that uses nodes to represent concepts and entities and
    relationships to connect these nodes. An example knowledge graph is shown in figure
    1.14\.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在计划实施RAG应用时，选择合适的存储解决方案非常重要。虽然有许多数据库选项，但我们认为知识图谱和图数据库特别适合大多数RAG应用。知识图谱是一种使用节点表示概念和实体，以及使用关系连接这些节点的数据结构。一个示例知识图谱如图1.14所示。
- en: '![figure](../Images/1-14.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-14.png)'
- en: '**Figure 1.14 A knowledge graph can store complex structured and unstructured
    data in a single database system.**'
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图1.14 知识图谱可以在单个数据库系统中存储复杂的有结构和无结构数据。**'
- en: Knowledge graphs are highly versatile, capable of storing both structured information
    (such as employee details, task statuses, and company hierarchies) and unstructured
    information (such as article contents). This dual capability, as illustrated in
    figure 1.14, makes them uniquely suited for complex RAG applications. Structured
    data allows for precise and efficient querying to answer questions such as, “How
    many tasks are assigned to a specific employee?” or “Which employees report to
    a particular manager?” For example, in figure 1.14, structured data such as “Sam
    Altman is the CEO of OpenAI” or “John Doe has been an employee of OpenAI since
    01-01-2023” can be directly queried to answer questions like “Who is the CEO of
    OpenAI?” or “How long has John Doe been with the company?” Similarly, structured
    relationships like “John Doe is assigned to a task with the status Completed”
    enable precise queries such as “Which tasks have been completed by employees?”
    or “Who is assigned to specific tasks at OpenAI?” This capability is critical
    for generating actionable insights from complex, interconnected data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱非常灵活，能够存储结构化信息（例如员工详情、任务状态和公司层级）和非结构化信息（例如文章内容）。如图1.14所示，这种双重能力使它们特别适合复杂的RAG应用。结构化数据允许进行精确和高效的查询以回答诸如“有多少任务分配给特定员工？”或“哪些员工向特定经理汇报？”等问题。例如，在图1.14中，如“Sam
    Altman是OpenAI的首席执行官”或“John Doe自2023年1月1日起一直是OpenAI的员工”这样的结构化数据可以直接查询以回答“OpenAI的首席执行官是谁？”或“John
    Doe在公司工作多久了？”等问题。同样，如“John Doe被分配给一个状态为完成的任务”这样的结构化关系可以启用精确查询，例如“哪些任务已被员工完成？”或“在OpenAI中谁被分配到特定任务？”这种能力对于从复杂、相互关联的数据中生成可操作的见解至关重要。
- en: On the other hand, unstructured data, such as article text, complements structured
    data by providing rich contextual information that adds depth and nuance. For
    instance, the unstructured article node in figure 1.14 provides details about
    a new LLM model and embeddings, but without a structured framework, it cannot
    answer specific queries like “How is this article related to OpenAI employees?”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，非结构化数据，如文章文本，通过提供丰富的上下文信息来补充结构化数据，增加了深度和细微差别。例如，图1.14中的非结构化文章节点提供了关于新LLM模型和嵌入的详细信息，但没有结构化框架，它无法回答诸如“这篇文章与OpenAI员工有何关联？”这样的特定查询。
- en: Importantly, unstructured data alone cannot answer all types of questions. While
    it can provide insights for open-ended or fuzzy queries, it lacks the structure
    needed for precise operations such as filtering, counting, or aggregating. For
    example, answering “How many tasks are completed within a company?” or “Which
    employees are assigned to tasks related to OpenAI?” requires structured relationships
    and attributes, as depicted in the right-hand side of figure 1.14\. Without structured
    data, these types of queries would require exhaustive text parsing and inference,
    which are computationally expensive and often imprecise. By integrating structured
    and unstructured information in the same framework, knowledge graphs enable the
    seamless blending of both worlds, making them a powerful tool for answering a
    broad range of questions efficiently and accurately in RAG applications. Moreover,
    explicit connections between unstructured and structured data unlock advanced
    retrieval strategies such as linking entities in text to graph nodes or contextualizing
    structured results with source passages that would be difficult or impossible
    to achieve using either type of data alone.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，仅凭非结构化数据无法回答所有类型的问题。虽然它可以提供开放式或模糊查询的见解，但它缺乏进行精确操作（如过滤、计数或聚合）所需的架构。例如，回答“公司内有多少任务已完成？”或“哪些员工被分配到与OpenAI相关的任务？”需要如图1.14右侧所示的结构化关系和属性。没有结构化数据，这类查询将需要详尽的文本解析和推理，这既计算成本高又往往不够精确。通过在相同框架中整合结构和非结构化信息，知识图谱能够无缝融合这两个世界，使它们成为在RAG应用中高效且准确地回答广泛问题的强大工具。此外，非结构化和结构化数据之间的显式连接解锁了高级检索策略，如将文本中的实体链接到图节点或用源段落上下文化结构化结果，这些策略单独使用任何一种数据类型都难以或无法实现。
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs, such as ChatGPT, are built on transformer architecture, enabling them
    to process and generate text efficiently by learning patterns from extensive textual
    data.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs，如ChatGPT，建立在Transformer架构之上，通过从大量文本数据中学习模式，使它们能够高效地处理和生成文本。
- en: While LLMs exhibit remarkable abilities in natural language understanding and
    generation, they have inherent limitations, such as a knowledge cutoff, the potential
    to generate outdated or hallucinated information, and an inability to access private
    or domain-specific knowledge.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然LLM在自然语言理解和生成方面表现出惊人的能力，但它们存在固有的局限性，例如知识截止点、可能生成过时或虚构信息，以及无法访问私人或特定领域知识。
- en: Continuous finetuning of LLMs to enhance their internal knowledge base is not
    practical due to resource constraints and the complexity of updating the models
    regularly.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于资源限制和定期更新模型的复杂性，对LLM（大型语言模型）进行持续微调以增强其内部知识库并不可行。
- en: RAG addresses LLM limitations by combining them with external knowledge bases,
    providing accurate, context-rich responses by injecting relevant facts directly
    into the input prompt.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG通过将外部知识库与LLM结合来解决其局限性，通过直接将相关事实注入输入提示中，提供准确、内容丰富的响应。
- en: RAG implementations have traditionally focused on unstructured data sources,
    limiting their scope and effectiveness for tasks requiring structured, precise,
    and interconnected information.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG（检索增强生成）的实现传统上侧重于非结构化数据源，限制了其在需要结构化、精确和相互关联信息任务中的范围和有效性。
- en: Knowledge graphs use nodes and relationships to represent and connect entities
    and concepts, integrating structured and unstructured data to provide a holistic
    data representation.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱通过节点和关系来表示和连接实体与概念，整合结构化和非结构化数据，以提供全面的数据表示。
- en: Integrating knowledge graphs into RAG workflows enhances their capability to
    retrieve and organize contextually relevant data, allowing LLMs to generate accurate,
    reliable, and explainable responses.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将知识图谱集成到RAG工作流程中增强了其检索和组织上下文相关数据的能力，使LLM能够生成准确、可靠和可解释的响应。
