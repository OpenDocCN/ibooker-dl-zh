- en: Appendix B. References and Further Reading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 参考文献和进一步阅读
- en: B.1 Chapter 1
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 第 1 章
- en: 'Custom-built LLMs are able to outperform general-purpose LLMs as a team at
    Bloomberg showed via a version of GPT pretrained on finance data from scratch.
    The custom LLM outperformed ChatGPT on financial tasks while maintaining good
    performance on general LLM benchmarks:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 定制构建的大型语言模型能够超越通用大型语言模型，正如 Bloomberg 的团队通过一种从头开始预训练的金融数据版本的 GPT 所展示的那样。该定制大型语言模型在金融任务上优于
    ChatGPT，同时在通用大型语言模型基准上表现良好：
- en: '*BloombergGPT: A Large Language Model for Finance* (2023) by Wu *et al.*, [https://arxiv.org/abs/2303.17564](abs.html)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BloombergGPT：一个大型金融语言模型*（2023）作者为 Wu *等*，[https://arxiv.org/abs/2303.17564](abs.html)'
- en: 'Existing LLMs can be adapted and finetuned to outperform general LLMs as well,
    which teams from Google Research and Google DeepMind showed in a medical context:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大型语言模型也可以适应并微调，以超越通用大型语言模型，这一点由 Google Research 和 Google DeepMind 的团队在医疗环境中展示：
- en: '*Towards Expert-Level Medical Question Answering with Large Language Models*
    (2023) by Singhal *et al.*, [https://arxiv.org/abs/2305.09617](abs.html)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Towards Expert-Level Medical Question Answering with Large Language Models*（2023）作者为
    Singhal *等*，[https://arxiv.org/abs/2305.09617](abs.html)'
- en: 'The paper that proposed the original transformer architecture:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了原始变换器架构的论文：
- en: '*Attention Is All You Need* (2017) by Vaswani *et al.*, [https://arxiv.org/abs/1706.03762](abs.html)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Attention Is All You Need*（2017）作者为 Vaswani *等*，[https://arxiv.org/abs/1706.03762](abs.html)'
- en: 'The original encoder-style transformer, called BERT:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 原始编码器风格的变换器，称为 BERT：
- en: '*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*
    (2018) by Devlin *et al.*, [https://arxiv.org/abs/1810.04805](abs.html)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BERT：用于语言理解的深度双向变换器预训练*（2018）作者为 Devlin *等*，[https://arxiv.org/abs/1810.04805](abs.html)'
- en: 'The paper describing the decoder-style GPT-3 model, which inspired modern LLMs
    and will be used as a template for implementing an LLM from scratch in this book:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 描述解码器风格的 GPT-3 模型的论文，该模型启发了现代大型语言模型，并将在本书中用作从头实现大型语言模型的模板：
- en: '*Language Models are Few-Shot Learners* (2020) by Brown *et al.*, [https://arxiv.org/abs/2005.14165](abs.html)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言模型是少样本学习者*（2020）作者为 Brown *等*，[https://arxiv.org/abs/2005.14165](abs.html)'
- en: 'The original vision transformer for classifying images, which illustrates that
    transformer architectures are not only restricted to text inputs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像分类的原始视觉变换器，说明变换器架构不仅限于文本输入：
- en: '*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*
    (2020) by Dosovitskiy *et al.*, [https://arxiv.org/abs/2010.11929](abs.html)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一幅图像价值 16x16 个单词：用于大规模图像识别的变换器*（2020）作者为 Dosovitskiy *等*，[https://arxiv.org/abs/2010.11929](abs.html)'
- en: 'Two experimental (but less popular) LLM architectures that serve as examples
    that not all LLMs need to be based on the transformer architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两个实验性（但不太流行）的大型语言模型架构作为示例，表明并非所有大型语言模型都需基于变换器架构：
- en: '*RWKV: Reinventing RNNs for the Transformer Era* (2023) by Peng *et al.*, [https://arxiv.org/abs/2305.13048](abs.html)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RWKV：为变换器时代重新定义 RNN*（2023）作者为 Peng *等*，[https://arxiv.org/abs/2305.13048](abs.html)'
- en: '*Hyena Hierarchy: Towards Larger Convolutional Language Models (2023)* by Poli
    *et al.,* [https://arxiv.org/abs/2302.10866](abs.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hyena Hierarchy: Towards Larger Convolutional Language Models (2023)*作者为 Poli
    *等*，[https://arxiv.org/abs/2302.10866](abs.html)'
- en: '*Mamba: Linear-Time Sequence Modeling with Selective State Spaces* (2023) by
    Gu and Dao, [https://arxiv.org/abs/2312.00752](abs.html)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mamba：具有选择性状态空间的线性时间序列建模*（2023）作者为 Gu 和 Dao，[https://arxiv.org/abs/2312.00752](abs.html)'
- en: 'Meta AI''s model is a popular implementation of a GPT-like model that is openly
    available in contrast to GPT-3 and ChatGPT:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI 的模型是一个受欢迎的 GPT 类模型的实现，与 GPT-3 和 ChatGPT 相比是公开可用的：
- en: '*Llama 2: Open Foundation and Fine-Tuned Chat Models* (2023) by Touvron *et
    al.*, [https://arxiv.org/abs/2307.09288](abs.html)[1](abs.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Llama 2：开放基础和微调聊天模型*（2023）作者为 Touvron *等*，[https://arxiv.org/abs/2307.09288](abs.html)[1](abs.html)'
- en: 'For readers interested in additional details about the dataset references in
    section 1.5, this paper describes the publicly available *The Pile* dataset curated
    by Eleuther AI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对第 1.5 节数据集参考的额外细节感兴趣的读者，本文描述了由 Eleuther AI 策划的公开可用数据集 *The Pile*：
- en: '*The Pile: An 800GB Dataset of Diverse Text for Language Modeling* (2020) by
    Gao *et al.*, [https://arxiv.org/abs/2101.00027](abs.html).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*The Pile：用于语言建模的 800GB 多样文本数据集*（2020）作者为 Gao *等*，[https://arxiv.org/abs/2101.00027](abs.html)。'
- en: 'The following paper provides the reference for InstructGPT for finetuning GPT-3,
    which was mentioned in section 1.6 and will be discussed in more detail in chapter
    7:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文提供了用于微调 GPT-3 的 InstructGPT 的参考，该内容在第 1.6 节中提到，并将在第 7 章中更详细讨论：
- en: '*Training Language Models to Follow Instructions with Human Feedback* (2022)
    by *Ouyang et al.*, [https://arxiv.org/abs/2203.02155](abs.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过人类反馈训练语言模型以遵循指令*（2022）由*Ouyang等*著，[https://arxiv.org/abs/2203.02155](abs.html)'
- en: B.2 Chapter 2
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 第二章
- en: 'Readers who are interested in discussion and comparison of embedding spaces
    with latent spaces and the general notion of vector representations can find more
    information in the first chapter of my book Machine Learning Q and AI:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣讨论和比较嵌入空间与潜在空间以及向量表示一般概念的读者，可以在我的书《机器学习 Q 和 AI》的第一章中找到更多信息：
- en: '*Machine Learning Q and AI* (2023) by Sebastian Raschka, [https://leanpub.com/machine-learning-q-and-ai](machine-learning-q-and-ai.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习 Q 和 AI*（2023）由Sebastian Raschka著，[https://leanpub.com/machine-learning-q-and-ai](machine-learning-q-and-ai.html)'
- en: 'The following paper provides more in-depth discussions of how how byte pair
    encoding is used as a tokenization method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文提供了更深入的讨论，说明字节对编码如何作为一种标记化方法使用：
- en: Neural Machine Translation of Rare Words with Subword Units (2015) by Sennrich
    at al., [https://arxiv.org/abs/1508.07909](abs.html)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀有词汇的神经机器翻译与子词单元（2015）由Sennrich等著，[https://arxiv.org/abs/1508.07909](abs.html)
- en: 'The code for the byte pair encoding tokenizer used to train GPT-2 was open-sourced
    by OpenAI:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练GPT-2的字节对编码标记器的代码已由OpenAI开源：
- en: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](src.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](src.html)'
- en: 'OpenAI provides an interactive web UI to illustrate how the byte pair tokenizer
    in GPT models works:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一个交互式网络UI，说明GPT模型中的字节对标记器是如何工作的：
- en: '[https://platform.openai.com/tokenizer](platform.openai.com.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/tokenizer](platform.openai.com.html)'
- en: 'For readers interested in coding and training a BPE tokenizer from the ground
    up, Andrej Karpathy''s GitHub repository `minbpe` offers a minimal and readable
    implementation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有兴趣从头开始编码和训练BPE标记器的读者，Andrej Karpathy的GitHub库`minbpe`提供了一个最小且易读的实现：
- en: A minimal implementation of a BPE tokenizer, [https://github.com/karpathy/minbpe](karpathy.html)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPE标记器的最小实现，[https://github.com/karpathy/minbpe](karpathy.html)
- en: 'Readers who are interested in studying alternative tokenization schemes that
    are used by some other popular LLMs can find more information in the SentencePiece
    and WordPiece papers:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣研究一些其他流行大型语言模型使用的替代标记化方案的读者，可以在SentencePiece和WordPiece论文中找到更多信息：
- en: 'SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer
    for Neural Text Processing (2018) by Kudo and Richardson, [https://aclanthology.org/D18-2012/](D18-2012.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SentencePiece：一种简单且与语言无关的子词标记器和去标记器，用于神经文本处理（2018）由Kudo和Richardson著，[https://aclanthology.org/D18-2012/](D18-2012.html)
- en: Fast WordPiece Tokenization (2020) by Song et al., [https://arxiv.org/abs/2012.15524](abs.html)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速WordPiece标记化（2020）由Song等著，[https://arxiv.org/abs/2012.15524](abs.html)
- en: B.3 Chapter 3
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 第三章
- en: 'Readers interested in learning more about Bahdanau attention for RNN and language
    translation can find detailed insights in the following paper:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣了解Bahdanau注意力在RNN和语言翻译中应用的读者，可以在以下论文中找到详细的见解：
- en: '*Neural Machine Translation by Jointly Learning to Align and Translate* (2014)
    by Bahdanau, Cho, and Bengio, [https://arxiv.org/abs/1409.0473](abs.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过联合学习对齐和翻译的神经机器翻译*（2014）由Bahdanau、Cho和Bengio著，[https://arxiv.org/abs/1409.0473](abs.html)'
- en: 'The concept of self-attention as scaled dot-product attention was introduced
    in the original transformer paper:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的概念作为缩放点积注意力在原始变换器论文中被介绍：
- en: '*Attention Is All You Need* (2017) by Vaswani et al., [https://arxiv.org/abs/1706.03762](abs.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力机制就是你所需要的一切*（2017）由Vaswani等著，[https://arxiv.org/abs/1706.03762](abs.html)'
- en: '*FlashAttentio*n is a highly efficient implementation of self-attention mechanism,
    which accelerates the computation process by optimizing memory access patterns.
    FlashAttention is mathematically the same as the standard self-attention mechanism
    but optimizes the computational process for efficiency:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*FlashAttention*是自注意力机制的高效实现，通过优化内存访问模式加速计算过程。FlashAttention在数学上与标准自注意力机制相同，但优化了计算过程以提高效率：'
- en: '*FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awarenes*s
    (2022) by Dao *et al.*, [https://arxiv.org/abs/2205.14135](abs.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FlashAttention: 快速且内存高效的确切注意力与IO感知*（2022）由Dao *等*著，[https://arxiv.org/abs/2205.14135](abs.html)'
- en: '*FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning*
    (2023) by Dao, [https://arxiv.org/abs/2307.08691](abs.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FlashAttention-2: 更快的注意力与更好的并行性和工作分配*（2023）由Dao著，[https://arxiv.org/abs/2307.08691](abs.html)'
- en: 'PyTorch implements a function for self-attention and causal attention that
    supports FlashAttention for efficiency. This function is beta and subject to change:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch实现了一个支持FlashAttention以提高效率的自注意力和因果注意力功能。该功能为测试版，可能会有所更改：
- en: '`scaled_dot_product_attention` documentation: [https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html](generated.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaled_dot_product_attention`文档：[https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html](generated.html)'
- en: 'PyTorch also implements an efficient `MultiHeadAttention` class based on the
    `scaled_dot_product` function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还实现了基于`scaled_dot_product`函数的高效`MultiHeadAttention`类：
- en: '`MultiHeadAttention` documentation: [https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html](generated.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiHeadAttention`文档：[https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html](generated.html)'
- en: 'Dropout is a regularization technique used in neural networks to prevent overfitting
    by randomly dropping units (along with their connections) from the neural network
    during training:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种正则化技术，用于在神经网络中防止过拟合，通过在训练期间随机丢弃单元（连同它们的连接）来实现：
- en: '*Dropout: A Simple Way to Prevent Neural Networks from Overfitting* (2014)
    by Srivastava *et al.*, [https://jmlr.org/papers/v15/srivastava14a.html](v15.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout: A Simple Way to Prevent Neural Networks from Overfitting*（2014）由Srivastava
    *等*，[https://jmlr.org/papers/v15/srivastava14a.html](v15.html)'
- en: 'While using the multi-head attention based on scaled-dot product attention
    remains the most common variant of self-attention in practice, authors found that
    it''s possible to also achieve good performance without the value weight matrix
    and projection layer:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于缩放点积注意力的多头注意力在实践中仍然是自注意力最常见的变体，作者发现也可以在没有值权重矩阵和投影层的情况下实现良好的性能：
- en: '*Simplifying Transformer Blocks* (2023) by He and Hofmann, [https://arxiv.org/abs/2311.01906](abs.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Simplifying Transformer Blocks*（2023）由He和Hofmann，[https://arxiv.org/abs/2311.01906](abs.html)'
- en: B.4 Chapter 4
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 第四章
- en: 'The layer normalization paper, titled "Layer Normalization," introduces a technique
    that stabilizes the hidden state dynamics neural networks by normalizing the summed
    inputs to the neurons within a hidden layer, significantly reducing training time
    compared to previously published methods:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 标题为“层归一化”的论文介绍了一种通过归一化隐藏层内神经元的输入总和来稳定神经网络的隐藏状态动态的技术，相比于之前发布的方法显著减少了训练时间：
- en: '*Layer Normalization* (2016) by Ba, Kiros, and Hinton, [https://arxiv.org/abs/1607.06450](abs.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Layer Normalization*（2016）由Ba、Kiros和Hinton，[https://arxiv.org/abs/1607.06450](abs.html)'
- en: 'Post-LayerNorm, used in the original Transformer model, applies layer normalization
    after the self-attention and feed forward networks. In contrast, Pre-LayerNorm,
    as adopted in models like GPT-2 and newer LLMs, applies layer normalization before
    these components, which can lead to more stable training dynamics and has been
    shown to improve performance in some cases, as discussed in the following papers:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始Transformer模型中使用的Post-LayerNorm在自注意力和前馈网络之后应用层归一化。相比之下，像GPT-2和更新的LLM中采用的Pre-LayerNorm在这些组件之前应用层归一化，这可能导致更稳定的训练动态，并且在某些情况下已被证明可以提高性能，如以下论文中所讨论：
- en: '*On Layer Normalization in the Transformer Architecture* (2020) by Xiong *et
    al.*, [https://arxiv.org/abs/2002.04745](abs.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*On Layer Normalization in the Transformer Architecture*（2020）由Xiong *等*，[https://arxiv.org/abs/2002.04745](abs.html)'
- en: '*ResiDual: Transformer with Dual Residual Connections* (2023) by Tie *et al.*,
    [https://arxiv.org/abs/2304.14802](abs.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ResiDual: Transformer with Dual Residual Connections*（2023）由Tie *等*， [https://arxiv.org/abs/2304.14802](abs.html)'
- en: 'A popular variant of LayerNorm used in modern LLMs is RMSNorm due to its improved
    computing efficiency. This variant simplifies the normalization process by normalizing
    the inputs using only the root mean square of the inputs, without subtracting
    the mean before squaring. This means it does not center the data before computing
    the scale. RMSNorm is described in more detail in the following paper:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLM中使用的流行层归一化变体是RMSNorm，因为它提高了计算效率。该变体通过仅使用输入的均方根来归一化输入，从而简化了归一化过程，而无需在平方前减去均值。这意味着在计算比例之前不对数据进行中心化。RMSNorm在以下论文中有更详细的描述：
- en: '*Root Mean Square Layer Normalization* (2019) by Zhang and Sennrich, [https://arxiv.org/abs/1910.07467](abs.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Root Mean Square Layer Normalization*（2019）由Zhang和Sennrich，[https://arxiv.org/abs/1910.07467](abs.html)'
- en: 'The GELU (Gaussian Error Linear Unit) activation function combines the properties
    of both the classic ReLU activation function and the normal distribution''s cumulative
    distribution function to model layer outputs, allowing for stochastic regularization
    and non-linearities in deep learning models, as introduced in the following paper:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GELU（高斯误差线性单元）激活函数结合了经典ReLU激活函数和正态分布累积分布函数的特性，以建模层输出，从而在深度学习模型中允许随机正则化和非线性，如以下论文所介绍：
- en: '*Gaussian Error Linear Units (GELUs)* (2016) by Hendricks and Gimpel, [https://arxiv.org/abs/1606.08415](abs.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高斯误差线性单元（GELUs）*（2016）由Hendricks和Gimpel撰写，[https://arxiv.org/abs/1606.08415](abs.html)'
- en: 'The GPT-2 paper introduced a series of transformer-based LLMs with varying
    sizes—124M, 355M, 774M, and 1.5B parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2论文介绍了一系列基于变压器的LLM，参数大小分别为124M、355M、774M和1.5B：
- en: '*Language Models are Unsupervised Multitask Learners* (2019) by Radford *et
    al.*, [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](better-language-models.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言模型是无监督的多任务学习者*（2019）由Radford *等人*撰写，[https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](better-language-models.html)'
- en: 'OpenAI''s GPT-3 uses fundamentally the same architecture as GPT-2, except that
    the largest version (175 billion) is 100x larger than the largest GPT-2 model
    and has been trained on much more data. Interested readers can refer to the official
    GPT-3 paper by OpenAI and the technical overview by Lambda Labs, which calculates
    that training GPT-3 on a single RTX 8000 consumer GPU would take 665 years:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3使用的架构基本上与GPT-2相同，但最大版本（1750亿参数）比最大的GPT-2模型大100倍，并且训练数据量也更多。感兴趣的读者可以参考OpenAI的官方GPT-3论文和Lambda
    Labs的技术概述，该概述计算在单个RTX 8000消费者GPU上训练GPT-3需要665年：
- en: Language Models are Few-Shot Learners (2023) by Brown et al., [https://arxiv.org/abs/2005.14165](abs.html)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型是少样本学习者（2023）由Brown等人撰写，[https://arxiv.org/abs/2005.14165](abs.html)
- en: 'OpenAI''s GPT-3 Language Model: A Technical Overview, [https://lambdalabs.com/blog/demystifying-gpt-3](blog.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3语言模型：技术概述，[https://lambdalabs.com/blog/demystifying-gpt-3](blog.html)
- en: 'NanoGPT is a code repository with a minimalist yet efficient implementation
    of a GPT-2 model, similar to the model implemented in this book. While the code
    in this book is different from nanoGPT, this repository inspired the reorganization
    of a large GPT Python parent class implementation into smaller submodules:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NanoGPT是一个代码库，提供了一个简约而高效的GPT-2模型实现，类似于本书中实现的模型。虽然本书中的代码与nanoGPT不同，但该代码库启发了将大型GPT
    Python父类实现重组为更小的子模块：
- en: NanoGPT, a repository for training medium-sized GPTs, [https://github.com/karpathy/nanoGPT](karpathy.html)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NanoGPT，一个用于训练中等大小GPT的代码库，[https://github.com/karpathy/nanoGPT](karpathy.html)
- en: 'An informative blog post showing that most of the computation in LLMs is spent
    in the feed forward layers rather than attention layers when the context size
    is smaller than 32,000 tokens:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇信息丰富的博客文章显示，当上下文大小小于32,000个标记时，LLM中的大部分计算花费在前馈层而非注意力层上：
- en: '"In the long (context) run" by Harm de Vries, [https://www.harmdevries.com/post/context-length/](context-length.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在长（上下文）运行中”由Harm de Vries撰写，[https://www.harmdevries.com/post/context-length/](context-length.html)
- en: B.5 Chapter 5
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 第五章
- en: 'A video lecture by the author detailing the loss function and applying a log
    transformation to make it easier to handle for mathematical optimization:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的一场视频讲座详细说明了损失函数，并应用对数变换以便于数学优化处理：
- en: L8.2 Logistic Regression Loss Function, [https://www.youtube.com/watch?v=GxJe0DZvydM](www.youtube.com.html)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L8.2 逻辑回归损失函数，[https://www.youtube.com/watch?v=GxJe0DZvydM](www.youtube.com.html)
- en: 'The following two papers detail the dataset, hyperparameter, and architecture
    details used for pretraining LLMs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两篇论文详细说明了用于预训练LLM的数据集、超参数和架构细节：
- en: 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling
    (2023) by Biderman *et al.*, [https://arxiv.org/abs/2304.01373](abs.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythia：分析大型语言模型的训练和扩展套件（2023）由Biderman *等人*撰写，[https://arxiv.org/abs/2304.01373](abs.html)
- en: 'OLMo: Accelerating the Science of Language Models (2024) by Groeneveld *et
    al.*, [https://arxiv.org/abs/2402.00838](abs.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OLMo：加速语言模型科学（2024）由Groeneveld *等人*撰写，[https://arxiv.org/abs/2402.00838](abs.html)
- en: 'The following supplementary code available for this book contains instructions
    for preparing 60,000 public domain books from Project Gutenberg for LLM training:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附带的以下补充代码包含为LLM训练准备来自古腾堡计划的60,000本公共领域书籍的说明：
- en: Pretraining GPT on the Project Gutenberg Dataset, [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg](ch05.html)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在古腾堡项目数据集上预训练GPT，[https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg](ch05.html)
- en: 'Chapter 5 discusses the pretraining of LLMs, and Appendix D covers more advanced
    training functions, such as linear warmup and cosine annealing. The following
    paper finds that similar techniques can be successfully applied to continue pretraining
    already pretrained LLMs, along with additional tips and insights:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第五章讨论了LLM的预训练，附录D涵盖了更高级的训练功能，如线性预热和余弦退火。以下论文发现，类似的技术可以成功应用于继续预训练已经预训练的LLM，并提供额外的建议和见解：
- en: Simple and Scalable Strategies to Continually Pre-train Large Language Models
    (2024) by Ibrahim *et al.*, [https://arxiv.org/abs/2403.08763](abs.html)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续预训练大型语言模型的简单且可扩展的策略（2024），作者*Ibrahim*等，[https://arxiv.org/abs/2403.08763](abs.html)
- en: 'BloombergGPT is an example of a domain-specific large language model (LLM)
    created by training on both general and domain-specific text corpora, specifically
    in the field of finance:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BloombergGPT是一个领域特定的大型语言模型（LLM）的例子，通过对一般和领域特定文本语料库的训练而创建，特别是在金融领域：
- en: 'BloombergGPT: A Large Language Model for Finance (2023) by Wu *et al.*, [https://arxiv.org/abs/2303.17564](abs.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BloombergGPT：用于金融的大型语言模型（2023），作者*Wu*等，[https://arxiv.org/abs/2303.17564](abs.html)
- en: GaLore is a recent research project that aims to make LLM pretraining more efficient.
    The required code change boils down to just replacing PyTorch's `AdamW` optimizer
    in the training function with the `GaLoreAdamW` optimizer provided by the `galore-torch`
    Python package.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GaLore是一个近期的研究项目，旨在提高LLM预训练的效率。所需的代码更改归结为仅将训练函数中PyTorch的`AdamW`优化器替换为由`galore-torch`
    Python包提供的`GaLoreAdamW`优化器。
- en: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (2024)
    by Zhao *et al.*, [https://arxiv.org/abs/2403.03507](abs.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GaLore：通过梯度低秩投影进行内存高效的LLM训练（2024），作者*Zhao*等，[https://arxiv.org/abs/2403.03507](abs.html)
- en: GaLore code repository, [https://github.com/jiaweizzhao/GaLore](jiaweizzhao.html)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GaLore代码库，[https://github.com/jiaweizzhao/GaLore](jiaweizzhao.html)
- en: 'The following papers and resources share openly available, large-scale pretraining
    datasets for LLMs that consist of hundreds of gigabytes to terabytes of text data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文和资源公开分享了用于LLM的大规模预训练数据集，这些数据集包含数百GB到TB级别的文本数据：
- en: 'Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research
    by Soldaini *et al.* 2024, [https://arxiv.org/abs/2402.00159](abs.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dolma：一个用于LLM预训练研究的三万亿标记开放语料库，作者*Soldaini*等（2024），[https://arxiv.org/abs/2402.00159](abs.html)
- en: 'The Pile: An 800GB Dataset of Diverse Text for Language Modeling by Gao et
    al. 2020, [https://arxiv.org/abs/2101.00027](abs.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: The Pile：一个包含800GB多样化文本的数据集，用于语言建模，作者Gao等（2020），[https://arxiv.org/abs/2101.00027](abs.html)
- en: 'The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web
    Data, and Web Data Only, by Penedo *et al.* (2023) [https://arxiv.org/abs/2306.01116](abs.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falcon LLM的RefinedWeb数据集：通过网络数据超越精心策划的语料库，并仅使用网络数据，作者*Penedo*等（2023），[https://arxiv.org/abs/2306.01116](abs.html)
- en: RedPajama by Together AI, [https://github.com/togethercomputer/RedPajama-Data](togethercomputer.html)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RedPajama，由Together AI提供，[https://github.com/togethercomputer/RedPajama-Data](togethercomputer.html)
- en: 'The paper that originally introduced top-k sampling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最早引入top-k采样的论文：
- en: Hierarchical Neural Story Generation by Fan *et al.* (2018), [https://arxiv.org/abs/1805.04833](abs.html)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层神经故事生成，作者*Fan*等（2018），[https://arxiv.org/abs/1805.04833](abs.html)
- en: 'Beam search (not cover in chapter 5) is an alternative decoding algorithm that
    generates output sequences by keeping only the top-scoring partial sequences at
    each step to balance efficiency and quality:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索（在第五章中未涵盖）是一种替代的解码算法，通过在每一步仅保留得分最高的部分序列来生成输出序列，以平衡效率和质量：
- en: 'Diverse Beam Sea*rch: Decoding Diverse Solutions from Neural Sequence Models
    by Vijayakumar* et al. (2016), [https://arxiv.org/abs/1610.02424](abs.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样化的束搜索：解码来自神经序列模型的多样化解决方案，作者*Vijayakumar*等（2016），[https://arxiv.org/abs/1610.02424](abs.html)
