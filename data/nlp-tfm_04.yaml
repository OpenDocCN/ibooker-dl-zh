- en: Chapter 3\. Transformer Anatomy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。变压器解剖学
- en: In [Chapter 2](ch02.xhtml#chapter_classification), we saw what it takes to fine-tune
    and evaluate a transformer. Now let’s take a look at how they work under the hood.
    In this chapter we’ll explore the main building blocks of transformer models and
    how to implement them using PyTorch. We’ll also provide guidance on how to do
    the same in TensorFlow. We’ll first focus on building the attention mechanism,
    and then add the bits and pieces necessary to make a transformer encoder work.
    We’ll also have a brief look at the architectural differences between the encoder
    and decoder modules. By the end of this chapter you will be able to implement
    a simple transformer model yourself!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#chapter_classification)中，我们看到了微调和评估变压器所需的内容。现在让我们来看看它们在内部是如何工作的。在本章中，我们将探索变压器模型的主要构建模块以及如何使用PyTorch实现它们。我们还将提供如何在TensorFlow中进行相同操作的指导。我们首先将专注于构建注意力机制，然后添加必要的部分来使变压器编码器工作。我们还将简要介绍编码器和解码器模块之间的架构差异。通过本章结束时，您将能够自己实现一个简单的变压器模型！
- en: While a deep technical understanding of the Transformer architecture is generally
    not necessary to use ![nlpt_pin01](Images/nlpt_pin01.png) Transformers and fine-tune
    models for your use case, it can be helpful for comprehending and navigating the
    limitations of transformers and using them in new domains.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不需要对变压器架构有深入的技术理解来使用![nlpt_pin01](Images/nlpt_pin01.png)变压器并为您的用例微调模型，但对于理解和应对变压器的局限性并在新领域中使用它们可能会有所帮助。
- en: This chapter also introduces a taxonomy of transformers to help you understand
    the zoo of models that have emerged in recent years. Before diving into the code,
    let’s start with an overview of the original architecture that kick-started the
    transformer revolution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还介绍了变压器的分类法，以帮助您了解近年来出现的各种模型。在深入代码之前，让我们先来看一下开启变压器革命的原始架构的概述。
- en: The Transformer Architecture
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器架构
- en: 'As we saw in [Chapter 1](ch01.xhtml#chapter_introduction), the original Transformer
    is based on the *encoder-decoder* architecture that is widely used for tasks like
    machine translation, where a sequence of words is translated from one language
    to another. This architecture consists of two components:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#chapter_introduction)中所看到的，原始Transformer基于“编码器-解码器”架构，这种架构被广泛用于诸如机器翻译之类的任务，其中一个词序列被翻译成另一种语言。这种架构由两个组件组成：
- en: Encoder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器
- en: Converts an input sequence of tokens into a sequence of embedding vectors, often
    called the *hidden state* or *context*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入的令牌序列转换为嵌入向量序列，通常称为“隐藏状态”或“上下文”
- en: Decoder
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器
- en: Uses the encoder’s hidden state to iteratively generate an output sequence of
    tokens, one token at a time
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用编码器的隐藏状态迭代地生成一个令牌序列的输出，每次生成一个令牌
- en: As illustrated in [Figure 3-1](#transformer-encoder-decoder), the encoder and
    decoder are themselves composed of several building blocks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-1](#transformer-encoder-decoder)所示，编码器和解码器本身由几个构建模块组成。
- en: '![transformer-encoder-decoder](Images/nlpt_0301.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![transformer-encoder-decoder](Images/nlpt_0301.png)'
- en: Figure 3-1\. Encoder-decoder architecture of the transformer, with the encoder
    shown in the upper half of the figure and the decoder in the lower half
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。变压器的编码器-解码器架构，编码器显示在图的上半部分，解码器显示在图的下半部分
- en: 'We’ll look at each of the components in detail shortly, but we can already
    see a few things in [Figure 3-1](#transformer-encoder-decoder) that characterize
    the Transformer architecture:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将详细查看每个组件，但我们已经可以在[图3-1](#transformer-encoder-decoder)中看到一些特征，这些特征表征了变压器架构：
- en: The input text is tokenized and converted to *token embeddings* using the techniques
    we encountered in [Chapter 2](ch02.xhtml#chapter_classification). Since the attention
    mechanism is not aware of the relative positions of the tokens, we need a way
    to inject some information about token positions into the input to model the sequential
    nature of text. The token embeddings are thus combined with *positional embeddings*
    that contain positional information for each token.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入文本被标记化，并使用我们在[第2章](ch02.xhtml#chapter_classification)中遇到的技术转换为“令牌嵌入”。由于注意力机制不知道令牌的相对位置，我们需要一种方法将一些关于令牌位置的信息注入到输入中，以建模文本的顺序性质。因此，令牌嵌入与包含每个令牌的位置信息的“位置嵌入”相结合。
- en: The encoder is composed of a stack of *encoder layers* or “blocks,” which is
    analogous to stacking convolutional layers in computer vision. The same is true
    of the decoder, which has its own stack of *decoder layers*.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器由一堆“编码器层”或“块”组成，类似于在计算机视觉中堆叠卷积层。解码器也是如此，它有自己的一堆“解码器层”。
- en: The encoder’s output is fed to each decoder layer, and the decoder then generates
    a prediction for the most probable next token in the sequence. The output of this
    step is then fed back into the decoder to generate the next token, and so on until
    a special end-of-sequence (EOS) token is reached. In the example from [Figure 3-1](#transformer-encoder-decoder),
    imagine the decoder has already predicted “Die” and “Zeit”. Now it gets these
    two as an input as well as all the encoder’s outputs to predict the next token,
    “fliegt”. In the next step the decoder gets “fliegt” as an additional input. We
    repeat the process until the decoder predicts the EOS token or we reached a maximum
    length.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的输出被馈送到每个解码器层，然后解码器生成一个最可能的下一个令牌序列的预测。这一步的输出然后被反馈到解码器中以生成下一个令牌，依此类推，直到达到特殊的序列结束（EOS）令牌。在[图3-1](#transformer-encoder-decoder)的示例中，想象一下解码器已经预测了“Die”和“Zeit”。现在它将这两个作为输入以及所有编码器的输出来预测下一个令牌，“fliegt”。在下一步中，解码器获得“fliegt”作为额外的输入。我们重复这个过程，直到解码器预测出EOS令牌或者达到最大长度。
- en: 'The Transformer architecture was originally designed for sequence-to-sequence
    tasks like machine translation, but both the encoder and decoder blocks were soon
    adapted as standalone models. Although there are hundreds of different transformer
    models, most of them belong to one of three types:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构最初是为机器翻译等序列到序列任务设计的，但编码器和解码器块很快被改编为独立的模型。虽然有数百种不同的transformer模型，但它们大多属于以下三种类型之一：
- en: Encoder-only
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 仅编码器
- en: These models convert an input sequence of text into a rich numerical representation
    that is well suited for tasks like text classification or named entity recognition.
    BERT and its variants, like RoBERTa and DistilBERT, belong to this class of architectures.
    The representation computed for a given token in this architecture depends both
    on the left (before the token) and the right (after the token) contexts. This
    is often called *bidirectional attention*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型将文本输入序列转换为丰富的数值表示，非常适合文本分类或命名实体识别等任务。BERT及其变种，如RoBERTa和DistilBERT，属于这类架构。在这种架构中，对于给定标记的表示取决于左侧（标记之前）和右侧（标记之后）的上下文。这经常被称为*双向注意力*。
- en: Decoder-only
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器
- en: Given a prompt of text like “Thanks for lunch, I had a…” these models will auto-complete
    the sequence by iteratively predicting the most probable next word. The family
    of GPT models belong to this class. The representation computed for a given token
    in this architecture depends only on the left context. This is often called *causal*
    or *autoregressive attention*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本提示，比如“谢谢午餐，我吃了一个...”，这些模型将通过迭代预测最有可能的下一个词来自动完成序列。GPT模型系列属于这一类。在这种架构中，对于给定标记的表示仅取决于左侧上下文。这经常被称为*因果*或*自回归注意力*。
- en: Encoder-decoder
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: These are used for modeling complex mappings from one sequence of text to another;
    they’re suitable for machine translation and summarization tasks. In addition
    to the Transformer architecture, which as we’ve seen combines an encoder and a
    decoder, the BART and T5 models belong to this class.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型用于对一个文本序列到另一个文本序列的复杂映射进行建模；它们适用于机器翻译和摘要任务。除了我们已经看到的Transformer架构，它结合了编码器和解码器，BART和T5模型也属于这一类。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In reality, the distinction between applications for decoder-only versus encoder-only
    architectures is a bit blurry. For example, decoder-only models like those in
    the GPT family can be primed for tasks like translation that are conventionally
    thought of as sequence-to-sequence tasks. Similarly, encoder-only models like
    BERT can be applied to summarization tasks that are usually associated with encoder-decoder
    or decoder-only models.^([1](ch03.xhtml#idm46238730148944))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，解码器-仅模型与仅编码器模型的应用区别有些模糊。例如，像GPT系列中的解码器-仅模型可以被用于传统上被认为是序列到序列任务的翻译等任务。同样，像BERT这样的仅编码器模型可以应用于通常与编码器-解码器或仅解码器模型相关的摘要任务。^([1](ch03.xhtml#idm46238730148944))
- en: Now that you have a high-level understanding of the Transformer architecture,
    let’s take a closer look at the inner workings of the encoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对Transformer架构有了高层次的理解，让我们更仔细地看看编码器的内部工作。
- en: The Encoder
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: 'As we saw earlier, the transformer’s encoder consists of many encoder layers
    stacked next to each other. As illustrated in [Figure 3-2](#encoder-zoom), each
    encoder layer receives a sequence of embeddings and feeds them through the following
    sublayers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，transformer的编码器由许多相邻堆叠的编码器层组成。如[图3-2](#encoder-zoom)所示，每个编码器层接收一系列嵌入，并通过以下子层进行处理：
- en: A multi-head self-attention layer
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力层
- en: A fully connected feed-forward layer that is applied to each input embedding
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用于每个输入嵌入的全连接前馈层
- en: The output embeddings of each encoder layer have the same size as the inputs,
    and we’ll soon see that the main role of the encoder stack is to “update” the
    input embeddings to produce representations that encode some contextual information
    in the sequence. For example, the word “apple” will be updated to be more “company-like”
    and less “fruit-like” if the words “keynote” or “phone” are close to it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器层的输出嵌入与输入的大小相同，我们很快就会看到编码器堆栈的主要作用是“更新”输入嵌入，以产生编码一些上下文信息的表示。例如，如果“keynote”或“phone”这样的词靠近“apple”，那么“apple”这个词将被更新为更“公司化”而不是更“水果化”。
- en: '![encoder-zoom](Images/nlpt_0302.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![encoder-zoom](Images/nlpt_0302.png)'
- en: Figure 3-2\. Zooming into the encoder layer
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 放大到编码器层
- en: 'Each of these sublayers also uses skip connections and layer normalization,
    which are standard tricks to train deep neural networks effectively. But to truly
    understand what makes a transformer work, we have to go deeper. Let’s start with
    the most important building block: the self-attention layer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子层也使用跳跃连接和层归一化，这是训练深度神经网络的标准技巧。但要真正理解transformer的工作原理，我们必须深入了解。让我们从最重要的构建模块开始：自注意力层。
- en: Self-Attention
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: As we discussed in [Chapter 1](ch01.xhtml#chapter_introduction), attention is
    a mechanism that allows neural networks to assign a different amount of weight
    or “attention” to each element in a sequence. For text sequences, the elements
    are *token embeddings* like the ones we encountered in [Chapter 2](ch02.xhtml#chapter_classification),
    where each token is mapped to a vector of some fixed dimension. For example, in
    BERT each token is represented as a 768-dimensional vector. The “self” part of
    self-attention refers to the fact that these weights are computed for all hidden
    states in the same set—for example, all the hidden states of the encoder. By contrast,
    the attention mechanism associated with recurrent models involves computing the
    relevance of each encoder hidden state to the decoder hidden state at a given
    decoding timestep.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#chapter_introduction)中讨论的那样，注意力是一种机制，它允许神经网络为序列中的每个元素分配不同数量的权重或“注意力”。对于文本序列，元素是*标记嵌入*，就像我们在[第2章](ch02.xhtml#chapter_classification)中遇到的那样，其中每个标记都被映射到某个固定维度的向量。例如，在BERT中，每个标记表示为一个768维向量。“自注意力”中的“自”指的是这些权重是针对同一集合中的所有隐藏状态计算的，例如，编码器的所有隐藏状态。相比之下，与循环模型相关的注意力机制涉及计算每个编码器隐藏状态对于给定解码时间步的解码器隐藏状态的相关性。
- en: 'The main idea behind self-attention is that instead of using a fixed embedding
    for each token, we can use the whole sequence to compute a *weighted average*
    of each embedding. Another way to formulate this is to say that given a sequence
    of token embeddings <math alttext="x 1 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    , self-attention produces a sequence of new embeddings <math alttext="x prime
    1 comma ellipsis comma x prime Subscript n"><mrow><msubsup><mi>x</mi> <mn>1</mn>
    <mo>''</mo></msubsup> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>x</mi> <mi>n</mi>
    <mo>''</mo></msubsup></mrow></math> where each <math alttext="x prime Subscript
    i"><msubsup><mi>x</mi> <mi>i</mi> <mo>''</mo></msubsup></math> is a linear combination
    of all the <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>
    :'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的主要思想是，我们可以使用整个序列来计算每个嵌入的*加权平均*，而不是为每个标记使用固定的嵌入。另一种表述方法是，给定标记嵌入序列<math alttext="x
    1 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>，自注意力会产生一个新嵌入序列<math
    alttext="x prime 1 comma ellipsis comma x prime Subscript n"><mrow><msubsup><mi>x</mi>
    <mn>1</mn> <mo>'</mo></msubsup> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>x</mi>
    <mi>n</mi> <mo>'</mo></msubsup></mrow></math>，其中每个<math alttext="x prime Subscript
    i"><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></math>都是所有<math alttext="x
    Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>的线性组合：
- en: <math alttext="x prime Subscript i Baseline equals sigma-summation Underscript
    j equals 1 Overscript n Endscripts w Subscript j i Baseline x Subscript j" display="block"><mrow><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub>
    <msub><mi>x</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x prime Subscript i Baseline equals sigma-summation Underscript
    j equals 1 Overscript n Endscripts w Subscript j i Baseline x Subscript j" display="block"><mrow><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub>
    <msub><mi>x</mi> <mi>j</mi></msub></mrow></math>
- en: The coefficients <math alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    are called *attention weights* and are normalized so that <math alttext="sigma-summation
    Underscript j Endscripts w Subscript j i Baseline equals 1"><mrow><msub><mo>∑</mo>
    <mi>j</mi></msub> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub> <mo>=</mo>
    <mn>1</mn></mrow></math> . To see why averaging the token embeddings might be
    a good idea, consider what comes to mind when you see the word “flies”. You might
    think of annoying insects, but if you were given more context, like “time flies
    like an arrow”, then you would realize that “flies” refers to the verb instead.
    Similarly, we can create a representation for “flies” that incorporates this context
    by combining all the token embeddings in different proportions, perhaps by assigning
    a larger weight <math alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    to the token embeddings for “time” and “arrow”. Embeddings that are generated
    in this way are called *contextualized embeddings* and predate the invention of
    transformers in language models like ELMo.^([2](ch03.xhtml#idm46238730081440))
    A diagram of the process is shown in [Figure 3-3](#contextualized-embeddings),
    where we illustrate how, depending on the context, two different representations
    for “flies” can be generated via self-attention.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 系数<math alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>被称为*注意力权重*，并且被归一化，使得<math
    alttext="sigma-summation Underscript j Endscripts w Subscript j i Baseline equals
    1"><mrow><msub><mo>∑</mo> <mi>j</mi></msub> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math>。要了解为什么对标记嵌入进行平均可能是一个好主意，请考虑当你看到“flies”这个词时会想到什么。你可能会想到讨厌的昆虫，但如果你得到更多的上下文，比如“time
    flies like an arrow”，那么你会意识到“flies”是指动词。同样，我们可以通过以不同的比例组合所有标记嵌入，也许通过给“time”和“arrow”的标记嵌入分配更大的权重<math
    alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>，来创建一个包含这个上下文的“flies”的表示。以这种方式生成的嵌入称为*上下文化嵌入*，并且早于像ELMo这样的语言模型中的transformers的发明。[2](ch03.xhtml#idm46238730081440)显示了该过程的图表，我们在其中说明了如何通过自注意力，根据上下文，可以生成“flies”的两种不同表示。
- en: '![Contextualized embeddings](Images/nlpt_0303.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![上下文化嵌入](Images/nlpt_0303.png)'
- en: Figure 3-3\. Diagram showing how self-attention updates raw token embeddings
    (upper) into contextualized embeddings (lower) to create representations that
    incorporate information from the whole sequence
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3。显示了自注意力如何将原始标记嵌入（上部）更新为上下文化嵌入（下部），以创建包含整个序列信息的表示。
- en: Let’s now take a look at how we can calculate the attention weights.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何计算注意力权重。
- en: Scaled dot-product attention
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放点积注意力
- en: 'There are several ways to implement a self-attention layer, but the most common
    one is *scaled dot-product attention*, from the paper introducing the Transformer
    architecture.^([3](ch03.xhtml#idm46238730065680)) There are four main steps required
    to implement this mechanism:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种实现自注意力层的方法，但最常见的是来自Transformer架构的“缩放点积注意力”。实现这种机制需要四个主要步骤：
- en: Project each token embedding into three vectors called *query*, *key*, and *value*.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个标记嵌入投影到称为查询、键和值的三个向量中。
- en: Compute attention scores. We determine how much the query and key vectors relate
    to each other using a *similarity function*. As the name suggests, the similarity
    function for scaled dot-product attention is the dot product, computed efficiently
    using matrix multiplication of the embeddings. Queries and keys that are similar
    will have a large dot product, while those that don’t share much in common will
    have little to no overlap. The outputs from this step are called the *attention
    scores*, and for a sequence with *n* input tokens there is a corresponding <math
    alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math> matrix
    of attention scores.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算注意力分数。我们使用相似性函数来确定查询和键向量之间的关系程度。正如其名称所示，缩放点积注意力的相似性函数是点积，通过嵌入的矩阵乘法进行高效计算。相似的查询和键将具有较大的点积，而那些没有共同之处的将几乎没有重叠。这一步的输出被称为注意力分数，对于具有n个输入标记的序列，有一个相应的n×n的注意力分数矩阵。
- en: Compute attention weights. Dot products can in general produce arbitrarily large
    numbers, which can destabilize the training process. To handle this, the attention
    scores are first multiplied by a scaling factor to normalize their variance and
    then normalized with a softmax to ensure all the column values sum to 1\. The
    resulting *n* × *n* matrix now contains all the attention weights, <math alttext="w
    Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    .
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算注意力权重。点积通常会产生任意大的数，这可能会使训练过程不稳定。为了处理这个问题，首先将注意力分数乘以一个缩放因子来归一化它们的方差，然后通过softmax进行归一化，以确保所有列的值总和为1。得到的n×n矩阵现在包含了所有的注意力权重，wji。
- en: Update the token embeddings. Once the attention weights are computed, we multiply
    them by the value vector <math alttext="v 1 comma ellipsis comma v Subscript n
    Baseline"><mrow><msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>v</mi> <mi>n</mi></msub></mrow></math> to obtain an updated representation
    for embedding <math alttext="x prime Subscript i Baseline equals sigma-summation
    Underscript j Endscripts w Subscript j i Baseline v Subscript j"><mrow><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup> <mo>=</mo> <msub><mo>∑</mo> <mi>j</mi></msub>
    <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub> <msub><mi>v</mi> <mi>j</mi></msub></mrow></math>
    .
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新标记嵌入。一旦计算出注意力权重，我们将它们乘以值向量v1，直到vn，以获得嵌入的更新表示xi'。
- en: 'We can visualize how the attention weights are calculated with a nifty library
    called [*BertViz* for Jupyter](https://oreil.ly/eQK3I). This library provides
    several functions that can be used to visualize different aspects of attention
    in transformer models. To visualize the attention weights, we can use the `neuron_view`
    module, which traces the computation of the weights to show how the query and
    key vectors are combined to produce the final weight. Since BertViz needs to tap
    into the attention layers of the model, we’ll instantiate our BERT checkpoint
    with the model class from BertViz and then use the `show()` function to generate
    the interactive visualization for a specific encoder layer and attention head.
    Note that you need to click the “+” on the left to activate the attention visualization:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个称为“BertViz for Jupyter”的巧妙库来可视化注意力权重的计算。这个库提供了几个函数，可以用来可视化变压器模型中注意力的不同方面。为了可视化注意力权重，我们可以使用`neuron_view`模块，它跟踪权重的计算过程，以展示查询和键向量是如何组合产生最终权重的。由于BertViz需要访问模型的注意力层，我们将使用BertViz的模型类来实例化我们的BERT检查点，然后使用`show()`函数来为特定的编码器层和注意力头生成交互式可视化。请注意，您需要点击左侧的“+”来激活注意力可视化：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/nlpt_03in01.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_03in01.png)'
- en: From the visualization, we can see the values of the query and key vectors are
    represented as vertical bands, where the intensity of each band corresponds to
    the magnitude. The connecting lines are weighted according to the attention between
    the tokens, and we can see that the query vector for “flies” has the strongest
    overlap with the key vector for “arrow”.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从可视化中，我们可以看到查询和键向量的值被表示为垂直条带，其中每个条带的强度对应于其大小。连接线的权重根据标记之间的注意力而加权，我们可以看到“flies”的查询向量与“arrow”的键向量有最强的重叠。
- en: Let’s take a look at this process in more detail by implementing the diagram
    of operations to compute scaled dot-product attention, as shown in [Figure 3-4](#attention-ops).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实现计算缩放点积注意力的操作图来更详细地了解这个过程，如[图3-4](#attention-ops)所示。
- en: '![Operations in scaled dot-product attention](Images/nlpt_0304.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![缩放点积注意力中的操作](Images/nlpt_0304.png)'
- en: Figure 3-4\. Operations in scaled dot-product attention
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4. 缩放点积注意力中的操作
- en: We will use PyTorch to implement the Transformer architecture in this chapter,
    but the steps in TensorFlow are analogous. We provide a mapping between the most
    important functions in the two frameworks in [Table 3-1](#tf-pt-table).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用PyTorch来实现Transformer架构，但TensorFlow中的步骤是类似的。我们提供了两个框架中最重要函数的映射，详见[表3-1](#tf-pt-table)。
- en: Table 3-1\. PyTorch and TensorFlow (Keras) classes and methods used in this
    chapter
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1。本章中使用的PyTorch和TensorFlow（Keras）类和方法
- en: '| PyTorch | TensorFlow (Keras) | Creates/implements |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | TensorFlow (Keras) | 创建/实现 |'
- en: '| --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `nn.Linear` | `keras.layers.Dense` | A dense neural network layer |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `nn.Linear` | `keras.layers.Dense` | 一个密集的神经网络层 |'
- en: '| `nn.Module` | `keras.layers.Layer` | The building blocks of models |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `nn.Module` | `keras.layers.Layer` | 模型的构建模块 |'
- en: '| `nn.Dropout` | `keras.layers.Dropout` | A dropout layer |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `nn.Dropout` | `keras.layers.Dropout` | 一个dropout层 |'
- en: '| `nn.LayerNorm` | `keras.layers.LayerNormalization` | Layer normalization
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `nn.LayerNorm` | `keras.layers.LayerNormalization` | 层归一化 |'
- en: '| `nn.Embedding` | `keras.layers.Embedding` | An embedding layer |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `nn.Embedding` | `keras.layers.Embedding` | 一个嵌入层 |'
- en: '| `nn.GELU` | `keras.activations.gelu` | The Gaussian Error Linear Unit activation
    function |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `nn.GELU` | `keras.activations.gelu` | 高斯误差线性单元激活函数 |'
- en: '| `nn.bmm` | `tf.matmul` | Batched matrix multiplication |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `nn.bmm` | `tf.matmul` | 批量矩阵乘法 |'
- en: '| `model.forward` | `model.call` | The model’s forward pass |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `model.forward` | `model.call` | 模型的前向传播 |'
- en: 'The first thing we need to do is tokenize the text, so let’s use our tokenizer
    to extract the input IDs:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是对文本进行标记化，因此让我们使用我们的标记器提取输入ID：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we saw in [Chapter 2](ch02.xhtml#chapter_classification), each token in
    the sentence has been mapped to a unique ID in the tokenizer’s vocabulary. To
    keep things simple, we’ve also excluded the `[CLS]` and `[SEP]` tokens by setting
    `add_special_tokens=False`. Next, we need to create some dense embeddings. *Dense*
    in this context means that each entry in the embeddings contains a nonzero value.
    In contrast, the one-hot encodings we saw in [Chapter 2](ch02.xhtml#chapter_classification)
    are *sparse*, since all entries except one are zero. In PyTorch, we can do this
    by using a `torch.nn.Embedding` layer that acts as a lookup table for each input
    ID:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2章](ch02.xhtml#chapter_classification)中看到的，句子中的每个标记都被映射到标记器词汇表中的唯一ID。为了保持简单，我们还通过设置`add_special_tokens=False`来排除了`[CLS]`和`[SEP]`标记。接下来，我们需要创建一些密集的嵌入。在这种情况下，“密集”意味着嵌入中的每个条目都包含非零值。相比之下，在[第2章](ch02.xhtml#chapter_classification)中看到的one-hot编码是“稀疏”的，因为除一个之外的所有条目都是零。在PyTorch中，我们可以通过使用`torch.nn.Embedding`层来实现这一点，该层充当每个输入ID的查找表：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we’ve used the `AutoConfig` class to load the *config.json* file associated
    with the `bert-base-uncased` checkpoint. In ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, every checkpoint is assigned a configuration file that specifies
    various hyperparameters like `vocab_size` and `hidden_size`, which in our example
    shows us that each input ID will be mapped to one of the 30,522 embedding vectors
    stored in `nn.Embedding`, each with a size of 768\. The `AutoConfig` class also
    stores additional metadata, such as the label names, which are used to format
    the model’s predictions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`AutoConfig`类来加载与`bert-base-uncased`检查点相关联的*config.json*文件。在![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers中，每个检查点都分配了一个配置文件，指定了诸如`vocab_size`和`hidden_size`之类的各种超参数，例如我们的示例中显示每个输入ID将映射到`nn.Embedding`中存储的30,522个嵌入向量之一，每个嵌入向量的大小为768。`AutoConfig`类还存储其他元数据，例如标签名称，用于格式化模型的预测。
- en: Note that the token embeddings at this point are independent of their context.
    This means that homonyms (words that have the same spelling but different meaning),
    like “flies” in the previous example, have the same representation. The role of
    the subsequent attention layers will be to mix these token embeddings to disambiguate
    and inform the representation of each token with the content of its context.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此时的标记嵌入与它们的上下文无关。这意味着在上一个示例中的“flies”等同义词（拼写相同但含义不同的单词）具有相同的表示。随后的注意力层的作用将是混合这些标记嵌入，以消除歧义并使用上下文的内容来形成每个标记的表示。
- en: 'Now that we have our lookup table, we can generate the embeddings by feeding
    in the input IDs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了查找表，我们可以通过输入ID生成嵌入：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This has given us a tensor of shape `[batch_size, seq_len, hidden_dim]`, just
    like we saw in [Chapter 2](ch02.xhtml#chapter_classification). We’ll postpone
    the positional encodings, so the next step is to create the query, key, and value
    vectors and calculate the attention scores using the dot product as the similarity
    function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个形状为`[batch_size, seq_len, hidden_dim]`的张量，就像我们在[第2章](ch02.xhtml#chapter_classification)中看到的一样。我们将推迟位置编码，因此下一步是创建查询、键和值向量，并使用点积作为相似性函数来计算注意力分数：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This has created a <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>
    matrix of attention scores per sample in the batch. We’ll see later that the query,
    key, and value vectors are generated by applying independent weight matrices <math
    alttext="upper W Subscript upper Q comma upper K comma upper V"><msub><mi>W</mi>
    <mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></msub></math>
    to the embeddings, but for now we’ve kept them equal for simplicity. In scaled
    dot-product attention, the dot products are scaled by the size of the embedding
    vectors so that we don’t get too many large numbers during training that can cause
    the softmax we will apply next to saturate.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个每个批次样本的<math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>注意力分数矩阵。我们将在后面看到，查询、键和值向量是通过将独立的权重矩阵<math
    alttext="upper W Subscript upper Q comma upper K comma upper V"><msub><mi>W</mi>
    <mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></msub></math>应用于嵌入来生成的，但现在为了简单起见，我们将它们保持相等。在缩放的点积注意力中，点积被嵌入向量的大小缩放，以便在训练过程中不会得到太多的大数，这可能会导致我们接下来将应用的softmax饱和。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The `torch.bmm()` function performs a *batch matrix-matrix product* that simplifies
    the computation of the attention scores where the query and key vectors have the
    shape `[batch_size, seq_len,` `hidden_dim``]`. If we ignored the batch dimension
    we could calculate the dot product between each query and key vector by simply
    transposing the key tensor to have the shape `[hidden_dim, seq_len]` and then
    using the matrix product to collect all the dot products in a `[seq_len, seq_len]`
    matrix. Since we want to do this for all sequences in the batch independently,
    we use `torch.bmm()`, which takes two batches of matrices and multiplies each
    matrix from the first batch with the corresponding matrix in the second batch.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.bmm()`函数执行*批量矩阵-矩阵乘积*，简化了注意力分数的计算，其中查询和键向量的形状为`[batch_size, seq_len,
    hidden_dim]`。如果忽略批处理维度，我们可以通过简单地转置键张量的形状为`[hidden_dim, seq_len]`，然后使用矩阵乘积来收集所有点积在`[seq_len,
    seq_len]`矩阵中。由于我们希望对批中的所有序列独立地执行此操作，我们使用`torch.bmm()`，它接受两个矩阵批次，并将第一个批次中的每个矩阵与第二个批次中的相应矩阵相乘。'
- en: 'Let’s apply the softmax now:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用softmax：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The final step is to multiply the attention weights by the values:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将注意力权重乘以值：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And that’s it—we’ve gone through all the steps to implement a simplified form
    of self-attention! Notice that the whole process is just two matrix multiplications
    and a softmax, so you can think of “self-attention” as just a fancy form of averaging.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——我们已经完成了实现简化形式的自注意力的所有步骤！请注意，整个过程只是两次矩阵乘法和一个softmax，因此你可以将“自注意力”看作是一种花哨的平均形式。
- en: 'Let’s wrap these steps into a function that we can use later:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些步骤封装成一个我们以后可以使用的函数：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our attention mechanism with equal query and key vectors will assign a very
    large score to identical words in the context, and in particular to the current
    word itself: the dot product of a query with itself is always 1\. But in practice,
    the meaning of a word will be better informed by complementary words in the context
    than by identical words—for example, the meaning of “flies” is better defined
    by incorporating information from “time” and “arrow” than by another mention of
    “flies”. How can we promote this behavior?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的注意机制使用相等的查询和键向量将为上下文中相同的单词分配非常大的分数，特别是对于当前单词本身：查询与自身的点积始终为1。但实际上，一个单词的含义更多地受到上下文中的补充单词的影响，而不是相同的单词，例如，“flies”的含义更好地通过“time”和“arrow”的信息来定义，而不是通过另一个“flies”的提及。我们如何促进这种行为呢？
- en: Let’s allow the model to create a different set of vectors for the query, key,
    and value of a token by using three different linear projections to project our
    initial token vector into three different spaces.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让模型通过使用三个不同的线性投影为一个标记的查询、键和值创建不同的向量集。
- en: Multi-headed attention
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头注意力
- en: In our simple example, we only used the embeddings “as is” to compute the attention
    scores and weights, but that’s far from the whole story. In practice, the self-attention
    layer applies three independent linear transformations to each embedding to generate
    the query, key, and value vectors. These transformations project the embeddings
    and each projection carries its own set of learnable parameters, which allows
    the self-attention layer to focus on different semantic aspects of the sequence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的例子中，我们只是使用嵌入“原样”来计算注意力分数和权重，但这远非全部。实际上，自注意力层对每个嵌入应用三个独立的线性变换，以生成查询、键和值向量。这些变换将嵌入投影到不同的空间，并且每个投影都携带其自己的可学习参数集，这使得自注意力层能够关注序列的不同语义方面。
- en: It also turns out to be beneficial to have *multiple* sets of linear projections,
    each one representing a so-called *attention head*. The resulting *multi-head
    attention layer* is illustrated in [Figure 3-5](#multihead-attention). But why
    do we need more than one attention head? The reason is that the softmax of one
    head tends to focus on mostly one aspect of similarity. Having several heads allows
    the model to focus on several aspects at once. For instance, one head can focus
    on subject-verb interaction, whereas another finds nearby adjectives. Obviously
    we don’t handcraft these relations into the model, and they are fully learned
    from the data. If you are familiar with computer vision models you might see the
    resemblance to filters in convolutional neural networks, where one filter can
    be responsible for detecting faces and another one finds wheels of cars in images.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 同时拥有*多个*线性投影集也被证明是有益的，每个代表一个所谓的*注意头*。结果的*多头注意力层*在[图3-5](#multihead-attention)中有所说明。但为什么我们需要多个注意头呢？原因是一个头的softmax倾向于主要关注相似性的某个方面。拥有多个头允许模型同时关注多个方面。例如，一个头可以关注主谓交互，而另一个可以找到附近的形容词。显然，我们不会手工将这些关系编码到模型中，它们完全是从数据中学习到的。如果你熟悉计算机视觉模型，你可能会看到它与卷积神经网络中的滤波器的相似之处，其中一个滤波器可以负责检测脸部，另一个可以在图像中找到车轮。
- en: '![Multi-head attention](Images/nlpt_0305.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![多头注意力](Images/nlpt_0305.png)'
- en: Figure 3-5\. Multi-head attention
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5。多头注意力
- en: 'Let’s implement this layer by first coding up a single attention head:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先编写一个单个注意力头来实现这一层：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here we’ve initialized three independent linear layers that apply matrix multiplication
    to the embedding vectors to produce tensors of shape `[batch_size, seq_len, head_dim]`,
    where `head_dim` is the number of dimensions we are projecting into. Although
    `head_dim` does not have to be smaller than the number of embedding dimensions
    of the tokens (`embed_dim`), in practice it is chosen to be a multiple of `embed_dim`
    so that the computation across each head is constant. For example, BERT has 12
    attention heads, so the dimension of each head is <math alttext="768 slash 12
    equals 64"><mrow><mn>768</mn> <mo>/</mo> <mn>12</mn> <mo>=</mo> <mn>64</mn></mrow></math>
    .
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化了三个独立的线性层，它们对嵌入向量进行矩阵乘法，以产生形状为`[batch_size, seq_len, head_dim]`的张量，其中`head_dim`是我们投影到的维度的数量。虽然`head_dim`不一定要小于标记的嵌入维度(`embed_dim`)，但实际上它被选择为`embed_dim`的倍数，以便每个头部的计算是恒定的。例如，BERT有12个注意力头，因此每个头的维度是<math
    alttext="768 slash 12 equals 64"><mrow><mn>768</mn> <mo>/</mo> <mn>12</mn> <mo>=</mo>
    <mn>64</mn></mrow></math>。
- en: 'Now that we have a single attention head, we can concatenate the outputs of
    each one to implement the full multi-head attention layer:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个单独的注意力头，我们可以将每个头的输出连接起来，以实现完整的多头注意力层：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Notice that the concatenated output from the attention heads is also fed through
    a final linear layer to produce an output tensor of shape `[batch_size, seq_len,`
    `hidden_dim``]` that is suitable for the feed-forward network downstream. To confirm,
    let’s see if the multi-head attention layer produces the expected shape of our
    inputs. We pass the configuration we loaded earlier from the pretrained BERT model
    when initializing the `MultiHeadAttention` module. This ensures that we use the
    same settings as BERT:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，注意力头的连接输出也通过最终的线性层，以产生适合下游前馈网络的形状为`[batch_size, seq_len, hidden_dim]`的输出张量。为了确认，让我们看看多头注意力层是否产生了我们输入的预期形状。我们在初始化`MultiHeadAttention`模块时传递了之前从预训练BERT模型加载的配置。这确保我们使用与BERT相同的设置：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It works! To wrap up this section on attention, let’s use BertViz again to
    visualize the attention for two different uses of the word “flies”. Here we can
    use the `head_view()` function from BertViz by computing the attentions of a pretrained
    checkpoint and indicating where the sentence boundary lies:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它起作用了！在注意力这一部分结束时，让我们再次使用BertViz来可视化单词“flies”两种不同用法的注意力。在这里，我们可以使用BertViz的`head_view()`函数，通过计算预训练检查点的注意力，并指示句子边界的位置：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](Images/nlpt_03in02.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_03in02.png)'
- en: This visualization shows the attention weights as lines connecting the token
    whose embedding is getting updated (left) with every word that is being attended
    to (right). The intensity of the lines indicates the strength of the attention
    weights, with dark lines representing values close to 1, and faint lines representing
    values close to 0.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可视化将注意力权重显示为连接正在更新嵌入的标记（左）与正在被关注的每个单词（右）的线条。线条的强度表示注意力权重的强度，深色线条表示接近1的值，淡色线条表示接近0的值。
- en: In this example, the input consists of two sentences and the `[CLS]` and `[SEP]`
    tokens are the special tokens in BERT’s tokenizer that we encountered in [Chapter 2](ch02.xhtml#chapter_classification).
    One thing we can see from the visualization is that the attention weights are
    strongest between words that belong to the same sentence, which suggests BERT
    can tell that it should attend to words in the same sentence. However, for the
    word “flies” we can see that BERT has identified “arrow” as important in the first
    sentence and “fruit” and “banana” in the second. These attention weights allow
    the model to distinguish the use of “flies” as a verb or noun, depending on the
    context in which it occurs!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，输入由两个句子组成，而`[CLS]`和`[SEP]`标记是BERT的分词器中的特殊标记，我们在[第2章](ch02.xhtml#chapter_classification)中遇到过。从可视化中我们可以看到的一件事是，注意力权重在属于同一句子的单词之间最强，这表明BERT可以知道它应该关注同一句子中的单词。然而，对于单词“flies”，我们可以看到BERT已经确定了第一个句子中“arrow”是重要的，第二个句子中是“fruit”和“banana”。这些注意力权重使模型能够区分“flies”作为动词或名词，取决于它出现的上下文！
- en: 'Now that we’ve covered attention, let’s take a look at implementing the missing
    piece of the encoder layer: position-wise feed-forward networks.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了注意力，让我们来看看如何实现编码器层中缺失的位置逐层前馈网络。
- en: The Feed-Forward Layer
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈层
- en: 'The feed-forward sublayer in the encoder and decoder is just a simple two-layer
    fully connected neural network, but with a twist: instead of processing the whole
    sequence of embeddings as a single vector, it processes each embedding *independently*.
    For this reason, this layer is often referred to as a *position-wise feed-forward
    layer*. You may also see it referred to as a one-dimensional convolution with
    a kernel size of one, typically by people with a computer vision background (e.g.,
    the OpenAI GPT codebase uses this nomenclature). A rule of thumb from the literature
    is for the hidden size of the first layer to be four times the size of the embeddings,
    and a GELU activation function is most commonly used. This is where most of the
    capacity and memorization is hypothesized to happen, and it’s the part that is
    most often scaled when scaling up the models. We can implement this as a simple
    `nn.Module` as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器中的前馈子层只是一个简单的两层全连接神经网络，但有一个变化：它不是将整个嵌入序列作为单个向量处理，而是*独立地*处理每个嵌入。因此，这一层通常被称为*位置逐层前馈层*。您可能还会看到它被称为具有大小为1的核的一维卷积，通常是由具有计算机视觉背景的人（例如，OpenAI
    GPT代码库使用这种命名方式）。文献中的一个经验法则是，第一层的隐藏大小应为嵌入大小的四倍，最常用的是GELU激活函数。这是假设大部分容量和记忆发生的地方，也是在扩大模型规模时最常扩展的部分。我们可以将其实现为一个简单的`nn.Module`，如下所示：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that a feed-forward layer such as `nn.Linear` is usually applied to a
    tensor of shape `(batch_size, input_dim)`, where it acts on each element of the
    batch dimension independently. This is actually true for any dimension except
    the last one, so when we pass a tensor of shape `(batch_size, seq_len, hidden_dim)`
    the layer is applied to all token embeddings of the batch and sequence independently,
    which is exactly what we want. Let’s test this by passing the attention outputs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，诸如`nn.Linear`之类的前馈层通常应用于形状为`(batch_size, input_dim)`的张量，其中它独立地作用于批处理维度的每个元素。这实际上对除了最后一个维度之外的任何维度都是真实的，因此当我们传递形状为`(batch_size,
    seq_len, hidden_dim)`的张量时，该层将独立地作用于批处理和序列中的所有令牌嵌入，这正是我们想要的。让我们通过传递注意力输出来测试一下：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We now have all the ingredients to create a fully fledged transformer encoder
    layer! The only decision left to make is where to place the skip connections and
    layer normalization. Let’s take a look at how this affects the model architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有了创建一个完整的transformer编码器层的所有要素！唯一剩下的决定是在哪里放置跳过连接和层归一化。让我们看看这如何影响模型架构。
- en: Adding Layer Normalization
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加层归一化
- en: 'As mentioned earlier, the Transformer architecture makes use of *layer normalization*
    and *skip connections*. The former normalizes each input in the batch to have
    zero mean and unity variance. Skip connections pass a tensor to the next layer
    of the model without processing and add it to the processed tensor. When it comes
    to placing the layer normalization in the encoder or decoder layers of a transformer,
    there are two main choices adopted in the literature:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，Transformer架构使用了*层归一化*和*跳过连接*。前者将批处理中的每个输入归一化为零均值和单位方差。跳过连接将一个张量传递到模型的下一层而不进行处理，并将其添加到处理过的张量中。在将层归一化放置在transformer的编码器或解码器层中时，文献中采用了两种主要选择：
- en: Post layer normalization
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 后层归一化
- en: This is the arrangement used in the Transformer paper; it places layer normalization
    in between the skip connections. This arrangement is tricky to train from scratch
    as the gradients can diverge. For this reason, you will often see a concept known
    as *learning rate warm-up*, where the learning rate is gradually increased from
    a small value to some maximum value during training.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Transformer论文中使用的安排；它将层归一化放置在跳过连接之间。这种安排很难从头开始训练，因为梯度可能会发散。因此，你经常会看到一个称为“学习率预热”的概念，在训练过程中学习率会逐渐从一个小值增加到某个最大值。
- en: Pre layer normalization
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 层前归一化
- en: This is the most common arrangement found in the literature; it places layer
    normalization within the span of the skip connections. This tends to be much more
    stable during training, and it does not usually require any learning rate warm-up.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是文献中最常见的安排；它将层归一化放置在跳过连接的范围内。这在训练过程中往往更加稳定，通常不需要任何学习率预热。
- en: The difference between the two arrangements is illustrated in [Figure 3-6](#layer-norm).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 两种安排的区别在[图3-6](#layer-norm)中有所说明。
- en: '![Transformer layer normalization](Images/nlpt_0306.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer层归一化](Images/nlpt_0306.png)'
- en: Figure 3-6\. Different arrangements of layer normalization in a transformer
    encoder layer
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6. Transformer编码器层中层归一化的不同安排
- en: 'We’ll use the second arrangement, so we can simply stick together our building
    blocks as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第二种安排，因此我们可以简单地将我们的构建模块连接在一起：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s now test this with our input embeddings:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用我们的输入嵌入来测试一下：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ve now implemented our very first transformer encoder layer from scratch!
    However, there is a caveat with the way we set up the encoder layers: they are
    totally invariant to the position of the tokens. Since the multi-head attention
    layer is effectively a fancy weighted sum, the information on token position is
    lost.^([4](ch03.xhtml#idm46238726765328))'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经从头开始实现了我们的第一个transformer编码器层！然而，我们设置编码器层的方式有一个问题：它们对令牌的位置是完全不变的。由于多头注意力层实际上是一种花哨的加权和，令牌位置的信息会丢失。^([4](ch03.xhtml#idm46238726765328))
- en: Luckily, there is an easy trick to incorporate positional information using
    positional embeddings. Let’s take a look.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个简单的技巧可以使用位置嵌入来纳入位置信息。让我们看看。
- en: Positional Embeddings
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: 'Positional embeddings are based on a simple, yet very effective idea: augment
    the token embeddings with a position-dependent pattern of values arranged in a
    vector. If the pattern is characteristic for each position, the attention heads
    and feed-forward layers in each stack can learn to incorporate positional information
    into their transformations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入基于一个简单但非常有效的思想：用一个与位置相关的值模式增强令牌嵌入，这些值排列在一个向量中。如果该模式对每个位置都是特征性的，那么每个堆栈中的注意力头和前馈层可以学习将位置信息纳入它们的转换中。
- en: There are several ways to achieve this, and one of the most popular approaches
    is to use a learnable pattern, especially when the pretraining dataset is sufficiently
    large. This works exactly the same way as the token embeddings, but using the
    position index instead of the token ID as input. With that approach, an efficient
    way of encoding the positions of tokens is learned during pretraining.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以实现这一点，其中最流行的方法之一是使用可学习的模式，特别是当预训练数据集足够大时。这与令牌嵌入的方式完全相同，但是使用位置索引而不是令牌ID作为输入。通过这种方式，在预训练期间学习到了一种有效的编码令牌位置的方式。
- en: 'Let’s create a custom `Embeddings` module that combines a token embedding layer
    that projects the `input_ids` to a dense hidden state together with the positional
    embedding that does the same for `position_ids`. The resulting embedding is simply
    the sum of both embeddings:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个自定义的`Embeddings`模块，它结合了一个令牌嵌入层，将`input_ids`投影到一个稠密的隐藏状态，以及一个位置嵌入，对`position_ids`做同样的事情。最终的嵌入就是这两种嵌入的和：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We see that the embedding layer now creates a single, dense embedding for each
    token.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到嵌入层现在为每个令牌创建了一个单一的稠密嵌入。
- en: 'While learnable position embeddings are easy to implement and widely used,
    there are some alternatives:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可学习的位置嵌入易于实现并被广泛使用，但也有一些替代方案：
- en: Absolute positional representations
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对位置表示
- en: Transformer models can use static patterns consisting of modulated sine and
    cosine signals to encode the positions of the tokens. This works especially well
    when there are not large volumes of data available.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型可以使用由调制正弦和余弦信号组成的静态模式来编码标记的位置。当没有大量数据可用时，这种方法特别有效。
- en: Relative positional representations
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相对位置表示
- en: Although absolute positions are important, one can argue that when computing
    an embedding, the surrounding tokens are most important. Relative positional representations
    follow that intuition and encode the relative positions between tokens. This cannot
    be set up by just introducing a new relative embedding layer at the beginning,
    since the relative embedding changes for each token depending on where from the
    sequence we are attending to it. Instead, the attention mechanism itself is modified
    with additional terms that take the relative position between tokens into account.
    Models such as DeBERTa use such representations.^([5](ch03.xhtml#idm46238726501696))
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管绝对位置很重要，但可以说在计算嵌入时，周围的标记最重要。相对位置表示遵循这种直觉，并对标记之间的相对位置进行编码。这不能仅通过在开始时引入一个新的相对嵌入层来设置，因为相对嵌入会根据我们从序列的哪个位置进行关注而为每个标记更改。相反，注意力机制本身被修改，增加了考虑标记之间相对位置的额外项。DeBERTa等模型使用这样的表示。^([5](ch03.xhtml#idm46238726501696))
- en: 'Let’s put all of this together now by building the full transformer encoder
    combining the embeddings with the encoder layers:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将所有这些组合起来，通过将嵌入与编码器层组合来构建完整的Transformer编码器：
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s check the output shapes of the encoder:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查编码器的输出形状：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see that we get a hidden state for each token in the batch. This output
    format makes the architecture very flexible, and we can easily adapt it for various
    applications such as predicting missing tokens in masked language modeling or
    predicting the start and end position of an answer in question answering. In the
    following section we’ll see how we can build a classifier like the one we used
    in [Chapter 2](ch02.xhtml#chapter_classification).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们为批处理中的每个标记获得一个隐藏状态。这种输出格式使得架构非常灵活，我们可以轻松地将其调整为各种应用，比如预测掩码语言建模中的缺失标记，或者在问答中预测答案的起始和结束位置。在接下来的部分中，我们将看到如何构建一个类似于我们在[第2章](ch02.xhtml#chapter_classification)中使用的分类器。
- en: Adding a Classification Head
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加分类头部
- en: 'Transformer models are usually divided into a task-independent body and a task-specific
    head. We’ll encounter this pattern again in [Chapter 4](ch04.xhtml#chapter_ner)
    when we look at the design pattern of ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
    What we have built so far is the body, so if we wish to build a text classifier,
    we will need to attach a classification head to that body. We have a hidden state
    for each token, but we only need to make one prediction. There are several options
    to approach this. Traditionally, the first token in such models is used for the
    prediction and we can attach a dropout and a linear layer to make a classification
    prediction. The following class extends the existing encoder for sequence classification:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型通常分为与任务无关的主体和与任务相关的头部。当我们在[第4章](ch04.xhtml#chapter_ner)中查看![nlpt_pin01](Images/nlpt_pin01.png)Transformer的设计模式时，我们将再次遇到这种模式。到目前为止，我们构建的是主体，因此如果我们希望构建文本分类器，我们将需要将分类头部附加到主体上。我们对每个标记都有一个隐藏状态，但我们只需要做一个预测。有几种方法可以解决这个问题。传统上，这种模型中的第一个标记用于预测，我们可以附加一个dropout和一个线性层来进行分类预测。以下类扩展了用于序列分类的现有编码器：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before initializing the model we need to define how many classes we would like
    to predict:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化模型之前，我们需要定义我们想要预测多少个类别：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That is exactly what we have been looking for. For each example in the batch
    we get the unnormalized logits for each class in the output. This corresponds
    to the BERT model that we used in [Chapter 2](ch02.xhtml#chapter_classification)
    to detect emotions in tweets.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们一直在寻找的。对于批处理中的每个示例，我们得到输出中每个类别的非归一化logits。这对应于我们在[第2章](ch02.xhtml#chapter_classification)中使用的BERT模型，用于检测推文中的情绪。
- en: This concludes our analysis of the encoder and how we can combine it with a
    task-specific head. Let’s now cast our attention (pun intended!) to the decoder.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对编码器的分析，以及我们如何将其与特定任务的头部结合起来。现在让我们把注意力（双关语！）转向解码器。
- en: The Decoder
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: 'As illustrated in [Figure 3-7](#decoder-zoom), the main difference between
    the decoder and encoder is that the decoder has *two* attention sublayers:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-7](#decoder-zoom)所示，解码器和编码器之间的主要区别在于解码器有*两个*注意力子层：
- en: Masked multi-head self-attention layer
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码多头自注意力层
- en: Ensures that the tokens we generate at each timestep are only based on the past
    outputs and the current token being predicted. Without this, the decoder could
    cheat during training by simply copying the target translations; masking the inputs
    ensures the task is not trivial.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 确保我们在每个时间步生成的标记仅基于过去的输出和当前正在预测的标记。如果没有这一点，解码器在训练过程中可以通过简单地复制目标翻译来作弊；屏蔽输入确保任务不是微不足道的。
- en: Encoder-decoder attention layer
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器注意力层
- en: Performs multi-head attention over the output key and value vectors of the encoder
    stack, with the intermediate representations of the decoder acting as the queries.^([6](ch03.xhtml#idm46238726113792))
    This way the encoder-decoder attention layer learns how to relate tokens from
    two different sequences, such as two different languages. The decoder has access
    to the encoder keys and values in each block.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对编码器堆栈的输出键和值向量执行多头注意力，其中解码器的中间表示充当查询。^([6](ch03.xhtml#idm46238726113792)) 这样，编码器-解码器注意力层学习如何关联来自两个不同序列的标记，比如两种不同的语言。解码器在每个块中都可以访问编码器的键和值。
- en: 'Let’s take a look at the modifications we need to make to include masking in
    our self-attention layer, and leave the implementation of the encoder-decoder
    attention layer as a homework problem. The trick with masked self-attention is
    to introduce a *mask matrix* with ones on the lower diagonal and zeros above:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们需要对自注意力层进行的修改，以包含掩码，并将编码器-解码器注意力层的实现作为一个作业问题留下。掩码自注意力的技巧是引入一个*掩码矩阵*，在下对角线上为1，在上方为0：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here we’ve used PyTorch’s `tril()` function to create the lower triangular
    matrix. Once we have this mask matrix, we can prevent each attention head from
    peeking at future tokens by using `Tensor.masked_fill()` to replace all the zeros
    with negative infinity:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了PyTorch的`tril()`函数来创建下三角矩阵。一旦有了这个掩码矩阵，我们可以使用`Tensor.masked_fill()`来将所有的零替换为负无穷大，从而防止每个注意力头窥视未来的标记：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Transformer decoder zoom](Images/nlpt_0307.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer解码器放大](Images/nlpt_0307.png)'
- en: Figure 3-7\. Zooming into the transformer decoder layer
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7。放大到transformer解码器层
- en: 'By setting the upper values to negative infinity, we guarantee that the attention
    weights are all zero once we take the softmax over the scores because <math alttext="e
    Superscript negative normal infinity Baseline equals 0"><mrow><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>∞</mi></mrow></msup> <mo>=</mo> <mn>0</mn></mrow></math> (recall
    that softmax calculates the normalized exponential). We can easily include this
    masking behavior with a small change to our scaled dot-product attention function
    that we implemented earlier in this chapter:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将上限值设置为负无穷大，我们保证了一旦我们对分数进行softmax计算，注意力权重都将为零，因为<math alttext="e Superscript
    negative normal infinity Baseline equals 0"><mrow><msup><mi>e</mi> <mrow><mo>-</mo><mi>∞</mi></mrow></msup>
    <mo>=</mo> <mn>0</mn></mrow></math>（回想一下，softmax计算的是归一化指数）。我们可以通过对我们在本章早些时候实现的缩放点积注意力函数进行小的修改，轻松地包含这种掩码行为：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: From here it is a simple matter to build up the decoder layer; we point the
    reader to the excellent implementation of [minGPT](https://oreil.ly/kwsOP) by
    Andrej Karpathy for details.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始构建解码器层就很简单了；我们指向[Andrej Karpathy的minGPT](https://oreil.ly/kwsOP)的出色实现以获取详细信息。
- en: We’ve given you a lot of technical information here, but now you should have
    a good understanding of how every piece of the Transformer architecture works.
    Before we move on to building models for tasks more advanced than text classification,
    let’s round out the chapter by stepping back a bit and looking at the landscape
    of different transformer models and how they relate to each other.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里给了你很多技术信息，但现在你应该对Transformer架构的每个部分是如何工作有了很好的理解。在我们继续构建比文本分类更高级的任务模型之前，让我们稍微回顾一下，看看不同transformer模型的景观以及它们之间的关系。
- en: Meet the Transformers
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遇见Transformer
- en: 'As you’ve seen in this chapter, there are three main architectures for transformer
    models: encoders, decoders, and encoder-decoders. The initial success of the early
    transformer models triggered a Cambrian explosion in model development as researchers
    built models on various datasets of different size and nature, used new pretraining
    objectives, and tweaked the architecture to further improve performance. Although
    the zoo of models is still growing fast, they can still be divided into these
    three categories.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本章中看到的，transformer模型有三种主要的架构：编码器、解码器和编码器-解码器。早期transformer模型的初步成功引发了模型开发的寒武纪爆发，研究人员在不同大小和性质的各种数据集上构建模型，使用新的预训练目标，并调整架构以进一步提高性能。尽管模型的种类仍在快速增长，但它们仍然可以分为这三类。
- en: In this section we’ll provide a brief overview of the most important transformer
    models in each class. Let’s start by taking a look at the transformer family tree.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍每个类别中最重要的transformer模型。让我们从看一下transformer家族谱开始。
- en: The Transformer Tree of Life
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer的生命之树
- en: Over time, each of the three main architectures has undergone an evolution of
    its own. This is illustrated in [Figure 3-8](#family-tree), which shows a few
    of the most prominent models and their descendants.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，每种主要架构都经历了自己的演变。这在[图3-8](#family-tree)中有所体现，该图显示了一些最重要的模型及其后代。
- en: '![Transformer family tree](Images/nlpt_0308.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer家族谱](Images/nlpt_0308.png)'
- en: Figure 3-8\. An overview of some of the most prominent transformer architectures
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。一些最重要的transformer架构的概述
- en: 'With over 50 different architectures included in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, this family tree by no means provides a complete overview of all
    the ones that exist: it simply highlights a few of the architectural milestones.
    We’ve covered the original Transformer architecture in depth in this chapter,
    so let’s take a closer look at some of the key descendants, starting with the
    encoder branch.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含50多种不同架构的![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，这个家族谱并不能提供所有存在的模型的完整概述：它只是突出了一些架构上的重要里程碑。我们在本章中深入讨论了原始的Transformer架构，所以让我们更仔细地看一下一些关键后代，从编码器分支开始。
- en: The Encoder Branch
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器分支
- en: 'The first encoder-only model based on the Transformer architecture was BERT.
    At the time it was published, it outperformed all the state-of-the-art models
    on the popular GLUE benchmark,^([7](ch03.xhtml#idm46238725854208)) which measures
    natural language understanding (NLU) across several tasks of varying difficulty.
    Subsequently, the pretraining objective and the architecture of BERT have been
    adapted to further improve performance. Encoder-only models still dominate research
    and industry on NLU tasks such as text classification, named entity recognition,
    and question answering. Let’s have a brief look at the BERT model and its variants:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer架构的第一个仅编码器模型是BERT。在发布时，它在流行的GLUE基准测试中表现优异，^([7](ch03.xhtml#idm46238725854208))该测试衡量了自然语言理解（NLU）在多个不同难度的任务中的表现。随后，BERT的预训练目标和架构已经被调整以进一步提高性能。仅编码器模型仍然在NLU任务（如文本分类、命名实体识别和问答）的研究和行业中占主导地位。让我们简要地看一下BERT模型及其变种：
- en: BERT
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: BERT
- en: BERT is pretrained with the two objectives of predicting masked tokens in texts
    and determining if one text passage is likely to follow another.^([8](ch03.xhtml#idm46238725844992))
    The former task is called *masked language modeling* (MLM) and the latter *next
    sentence prediction* (NSP).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在预训练时具有两个目标：预测文本中的屏蔽标记，以及确定一个文本段是否可能跟随另一个文本段。前者任务称为*屏蔽语言建模*（MLM），后者称为*下一个句子预测*（NSP）。
- en: DistilBERT
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT
- en: Although BERT delivers great results, it’s size can make it tricky to deploy
    in environments where low latencies are required. By using a technique known as
    knowledge distillation during pretraining, DistilBERT achieves 97% of BERT’s performance
    while using 40% less memory and being 60% faster.^([9](ch03.xhtml#idm46238725839424))
    You can find more details on knowledge distillation in [Chapter 8](ch08.xhtml#chapter_compression).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管BERT取得了很好的结果，但其规模使得在需要低延迟的环境中部署变得棘手。通过在预训练期间使用一种称为知识蒸馏的技术，DistilBERT在使用40%更少的内存和速度提高60%的情况下，实现了BERT性能的97%。您可以在第8章中找到有关知识蒸馏的更多细节。
- en: RoBERTa
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa
- en: A study following the release of BERT revealed that its performance can be further
    improved by modifying the pretraining scheme. RoBERTa is trained longer, on larger
    batches with more training data, and it drops the NSP task.^([10](ch03.xhtml#idm46238725833888))
    Together, these changes significantly improve its performance compared to the
    original BERT model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布BERT后的一项研究发现，通过修改预训练方案可以进一步提高其性能。RoBERTa在更大的批次上进行更长时间的训练，并且放弃了NSP任务。这些改变显著提高了其性能，与原始BERT模型相比。
- en: XLM
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: XLM
- en: Several pretraining objectives for building multilingual models were explored
    in the work on the cross-lingual language model (XLM),^([11](ch03.xhtml#idm46238725828528))
    including the autoregressive language modeling from GPT-like models and MLM from
    BERT. In addition, the authors of the paper on XLM pretraining introduced *translation
    language modeling* (TLM), which is an extension of MLM to multiple language inputs.
    Experimenting with these pretraining tasks, they achieved state-of-the-art results
    on several multilingual NLU benchmarks as well as on translation tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨语言语言模型（XLM）的工作中探索了构建多语言模型的几个预训练目标，包括来自GPT类模型的自回归语言建模和来自BERT的MLM。此外，XLM预训练论文的作者介绍了*翻译语言建模*（TLM），这是对多语言输入的MLM的扩展。通过对这些预训练任务进行实验，他们在几个多语言NLU基准测试以及翻译任务上取得了最先进的结果。
- en: XLM-RoBERTa
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-RoBERTa
- en: Following the work of XLM and RoBERTa, the XLM-RoBERTa or XLM-R model takes
    multilingual pretraining one step further by massively upscaling the training
    data.^([12](ch03.xhtml#idm46238725820864)) Using the [Common Crawl corpus](https://commoncrawl.org),
    its developers created a dataset with 2.5 terabytes of text; they then trained
    an encoder with MLM on this dataset. Since the dataset only contains data without
    parallel texts (i.e., translations), the TLM objective of XLM was dropped. This
    approach beats XLM and multilingual BERT variants by a large margin, especially
    on low-resource languages.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在XLM和RoBERTa的工作之后，XLM-RoBERTa或XLM-R模型通过大规模扩展训练数据，进一步推动了多语言预训练的发展。其开发者利用Common
    Crawl语料库创建了一个包含2.5TB文本的数据集，然后在该数据集上进行了MLM编码器的训练。由于数据集只包含没有平行文本（即翻译）的数据，因此XLM的TLM目标被取消。这种方法在低资源语言上明显优于XLM和多语言BERT变体。
- en: ALBERT
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT
- en: 'The ALBERT model introduced three changes to make the encoder architecture
    more efficient.^([13](ch03.xhtml#idm46238725814016)) First, it decouples the token
    embedding dimension from the hidden dimension, thus allowing the embedding dimension
    to be small and thereby saving parameters, especially when the vocabulary gets
    large. Second, all layers share the same parameters, which decreases the number
    of effective parameters even further. Finally, the NSP objective is replaced with
    a sentence-ordering prediction: the model needs to predict whether or not the
    order of two consecutive sentences was swapped rather than predicting if they
    belong together at all. These changes make it possible to train even larger models
    with fewer parameters and reach superior performance on NLU tasks.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT模型引入了三个改变，使得编码器架构更加高效。首先，它将标记嵌入维度与隐藏维度解耦，从而允许嵌入维度较小，从而节省参数，特别是当词汇量变大时。其次，所有层共享相同的参数，这进一步减少了有效参数的数量。最后，NSP目标被替换为句子排序预测：模型需要预测两个连续句子的顺序是否交换，而不是预测它们是否完全属于一起。这些改变使得可以使用更少的参数训练更大的模型，并在NLU任务上取得更优越的性能。
- en: ELECTRA
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ELECTRA
- en: One limitation of the standard MLM pretraining objective is that at each training
    step only the representations of the masked tokens are updated, while the other
    input tokens are not. To address this issue, ELECTRA uses a two-model approach:^([14](ch03.xhtml#idm46238725808096))
    the first model (which is typically small) works like a standard masked language
    model and predicts masked tokens. The second model, called the *discriminator*,
    is then tasked to predict which of the tokens in the first model’s output were
    originally masked. Therefore, the discriminator needs to make a binary classification
    for every token, which makes training 30 times more efficient. For downstream
    tasks the discriminator is fine-tuned like a standard BERT model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 标准MLM预训练目标的一个局限性是，在每个训练步骤中，只有被屏蔽的标记的表示会被更新，而其他输入标记不会被更新。为了解决这个问题，ELECTRA采用了两模型方法：第一个模型（通常较小）类似于标准的屏蔽语言模型，预测被屏蔽的标记。然后，称为*鉴别器*的第二个模型被要求预测第一个模型输出的标记中哪些是最初被屏蔽的。因此，鉴别器需要对每个标记进行二元分类，这使得训练效率提高了30倍。对于下游任务，鉴别器像标准的BERT模型一样进行微调。
- en: DeBERTa
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa
- en: 'The DeBERTa model introduces two architectural changes.^([15](ch03.xhtml#idm46238725800480))
    First, each token is represented as two vectors: one for the content, the other
    for relative position. By disentangling the tokens’ content from their relative
    positions, the self-attention layers can better model the dependency of nearby
    token pairs. On the other hand, the absolute position of a word is also important,
    especially for decoding. For this reason, an absolute position embedding is added
    just before the softmax layer of the token decoding head. DeBERTa is the first
    model (as an ensemble) to beat the human baseline on the SuperGLUE benchmark,^([16](ch03.xhtml#idm46238725798384))
    a more difficult version of GLUE consisting of several subtasks used to measure
    NLU performance.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa模型引入了两个架构变化。首先，每个标记表示为两个向量：一个用于内容，另一个用于相对位置。通过将标记的内容与它们的相对位置分离，自注意力层可以更好地建模附近标记对的依赖关系。另一方面，单词的绝对位置也很重要，特别是对于解码。因此，在标记解码头的softmax层之前添加了绝对位置嵌入。DeBERTa是第一个（作为集合）在SuperGLUE基准上击败人类基线的模型，这是GLUE的更难版本，由几个子任务组成，用于衡量NLU性能。
- en: Now that we’ve highlighted some of the major encoder-only architectures, let’s
    take a look at the decoder-only models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经强调了一些主要的仅编码器架构，让我们来看一下仅解码器模型。
- en: The Decoder Branch
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器分支
- en: 'The progress on transformer decoder models has been spearheaded to a large
    extent by OpenAI. These models are exceptionally good at predicting the next word
    in a sequence and are thus mostly used for text generation tasks (see [Chapter 5](ch05.xhtml#chapter_generation)
    for more details). Their progress has been fueled by using larger datasets and
    scaling the language models to larger and larger sizes. Let’s have a look at the
    evolution of these fascinating generation models:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器解码器模型的进展在很大程度上是由OpenAI带头的。这些模型在预测序列中的下一个单词方面表现出色，因此主要用于文本生成任务。它们的进展得益于使用更大的数据集，并将语言模型扩展到越来越大的规模。让我们来看看这些迷人生成模型的演变：
- en: GPT
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: GPT
- en: The introduction of GPT combined two key ideas in NLP:^([17](ch03.xhtml#idm46238725786352))
    the novel and efficient transformer decoder architecture, and transfer learning.
    In that setup, the model was pretrained by predicting the next word based on the
    previous ones. The model was trained on the BookCorpus and achieved great results
    on downstream tasks such as classification.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: GPT的引入将NLP中的两个关键思想结合在一起：新颖而高效的变压器解码器架构和迁移学习。在这种设置下，模型通过基于先前单词预测下一个单词来进行预训练。该模型在BookCorpus上进行了训练，并在分类等下游任务上取得了很好的结果。
- en: GPT-2
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2
- en: Inspired by the success of the simple and scalable pretraining approach, the
    original model and training set were upscaled to produce GPT-2.^([18](ch03.xhtml#idm46238725779984))
    This model is able to produce long sequences of coherent text. Due to concerns
    about possible misuse, the model was released in a staged fashion, with smaller
    models being published first and the full model later.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 受到简单且可扩展的预训练方法成功的启发，原始模型和训练集被扩大，产生了GPT-2。这个模型能够生成连贯文本的长序列。由于可能被滥用的担忧，该模型是分阶段发布的，先发布较小的模型，后来再发布完整的模型。
- en: CTRL
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: CTRL
- en: Models like GPT-2 can continue an input sequence (also called a *prompt*). However,
    the user has little control over the style of the generated sequence. The Conditional
    Transformer Language (CTRL) model addresses this issue by adding “control tokens”
    at the beginning of the sequence.^([19](ch03.xhtml#idm46238725774016)) These allow
    the style of the generated text to be controlled, which allows for diverse generation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-2这样的模型可以继续输入序列（也称为*提示*）。然而，用户对生成序列的风格几乎没有控制。条件变压器语言（CTRL）模型通过在序列开头添加“控制标记”来解决这个问题。这些标记允许控制生成文本的风格，从而实现多样化的生成。
- en: GPT-3
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3
- en: 'Following the success of scaling GPT up to GPT-2, a thorough analysis on the
    behavior of language models at different scales revealed that there are simple
    power laws that govern the relation between compute, dataset size, model size,
    and the performance of a language model.^([20](ch03.xhtml#idm46238725769248))
    Inspired by these insights, GPT-2 was upscaled by a factor of 100 to yield GPT-3,^([21](ch03.xhtml#idm46238725767728))
    with 175 billion parameters. Besides being able to generate impressively realistic
    text passages, the model also exhibits few-shot learning capabilities: with a
    few examples of a novel task such as translating text to code, the model is able
    to accomplish the task on new examples. OpenAI has not open-sourced this model,
    but provides an interface through the [OpenAI API](https://oreil.ly/SEGRW).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在将GPT扩展到GPT-2的成功之后，对不同规模的语言模型行为进行了彻底分析，发现了计算、数据集大小、模型大小和语言模型性能之间的简单幂律关系。受到这些见解的启发，GPT-2被放大了100倍，产生了拥有1750亿参数的GPT-3。除了能够生成令人印象深刻的逼真文本段落外，该模型还表现出了少样本学习的能力：通过几个示例，如将文本翻译成代码的新任务，模型能够在新示例上完成任务。OpenAI没有开源这个模型，但通过[OpenAI
    API](https://oreil.ly/SEGRW)提供了一个接口。
- en: GPT-Neo/GPT-J-6B
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-Neo/GPT-J-6B
- en: GPT-Neo and GPT-J-6B are GPT-like models that were trained by [EleutherAI](https://eleuther.ai),
    a collective of researchers who aim to re-create and release GPT-3 scale models.^([22](ch03.xhtml#idm46238725759200))
    The current models are smaller variants of the full 175-billion-parameter model,
    with 1.3, 2.7, and 6 billion parameters, and are competitive with the smaller
    GPT-3 models OpenAI offers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-Neo和GPT-J-6B是类似GPT的模型，由[EleutherAI](https://eleuther.ai)训练，这是一个旨在重新创建和发布GPT-3规模模型的研究人员集体。当前的模型是完整1750亿参数模型的较小变体，具有13亿、27亿和60亿参数，并且与OpenAI提供的较小GPT-3模型具有竞争力。
- en: The final branch in the transformers tree of life is the encoder-decoder models.
    Let’s take a look.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: transformers树的最终分支是编码器-解码器模型。让我们来看一下。
- en: The Encoder-Decoder Branch
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器分支
- en: 'Although it has become common to build models using a single encoder or decoder
    stack, there are several encoder-decoder variants of the Transformer architecture
    that have novel applications across both NLU and NLG domains:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用单个编码器或解码器堆栈构建模型已经很普遍，但Transformer架构的编码器-解码器变体有许多新颖的应用，涵盖了NLU和NLG领域：
- en: T5
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: T5
- en: The T5 model unifies all NLU and NLG tasks by converting them into text-to-text
    tasks.^([23](ch03.xhtml#idm46238725745824)) All tasks are framed as sequence-to-sequence
    tasks, where adopting an encoder-decoder architecture is natural. For text classification
    problems, for example, this means that the text is used as the encoder input and
    the decoder has to generate the label as normal text instead of a class. We will
    look at this in more detail in [Chapter 6](ch06.xhtml#chapter_summarization).
    The T5 architecture uses the original Transformer architecture. Using the large
    crawled C4 dataset, the model is pretrained with masked language modeling as well
    as the SuperGLUE tasks by translating all of them to text-to-text tasks. The largest
    model with 11 billion parameters yielded state-of-the-art results on several benchmarks.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: T5模型通过将所有NLU和NLG任务统一转换为文本到文本任务。^([23](ch03.xhtml#idm46238725745824)) 所有任务都被构建为序列到序列任务，采用编码器-解码器架构是自然的。例如，对于文本分类问题，这意味着文本被用作编码器输入，解码器必须生成标签作为普通文本，而不是类别。我们将在[第6章](ch06.xhtml#chapter_summarization)中更详细地讨论这个问题。T5架构使用了原始的Transformer架构。使用大型抓取的C4数据集，该模型通过将所有任务转换为文本到文本任务，进行了掩码语言建模以及SuperGLUE任务的预训练。具有110亿参数的最大模型在几个基准测试中取得了最先进的结果。
- en: BART
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: BART
- en: BART combines the pretraining procedures of BERT and GPT within the encoder-decoder
    architecture.^([24](ch03.xhtml#idm46238725739936)) The input sequences undergo
    one of several possible transformations, from simple masking to sentence permutation,
    token deletion, and document rotation. These modified inputs are passed through
    the encoder, and the decoder has to reconstruct the original texts. This makes
    the model more flexible as it is possible to use it for NLU as well as NLG tasks,
    and it achieves state-of-the-art-performance on both.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: BART将BERT和GPT的预训练程序结合到编码器-解码器架构中。^([24](ch03.xhtml#idm46238725739936)) 输入序列经历了几种可能的转换，从简单的屏蔽到句子排列、标记删除和文档旋转。这些修改后的输入通过编码器，解码器必须重建原始文本。这使得模型更加灵活，因为可以将其用于NLU和NLG任务，并且在两者上都实现了最先进的性能。
- en: M2M-100
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: M2M-100
- en: Conventionally a translation model is built for one language pair and translation
    direction. Naturally, this does not scale to many languages, and in addition there
    might be shared knowledge between language pairs that could be leveraged for translation
    between rare languages. M2M-100 is the first translation model that can translate
    between any of 100 languages.^([25](ch03.xhtml#idm46238725734576)) This allows
    for high-quality translations between rare and underrepresented languages. The
    model uses prefix tokens (similar to the special `[CLS]` token) to indicate the
    source and target language.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，翻译模型是为一种语言对和翻译方向构建的。自然地，这无法扩展到许多语言，而且可能存在语言对之间的共享知识，可以用于罕见语言之间的翻译。M2M-100是第一个可以在100种语言之间进行翻译的翻译模型。^([25](ch03.xhtml#idm46238725734576))
    这允许在罕见和代表性不足的语言之间进行高质量的翻译。该模型使用前缀标记（类似于特殊的`[CLS]`标记）来指示源语言和目标语言。
- en: BigBird
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: BigBird
- en: One main limitation of transformer models is the maximum context size, due to
    the quadratic memory requirements of the attention mechanism. BigBird addresses
    this issue by using a sparse form of attention that scales linearly.^([26](ch03.xhtml#idm46238725727968))
    This allows for the drastic scaling of contexts from 512 tokens in most BERT models
    to 4,096 in BigBird. This is especially useful in cases where long dependencies
    need to be conserved, such as in text summarization.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型的一个主要限制是最大上下文大小，这是由于注意机制的二次内存需求。BigBird通过使用一种稀疏形式的注意力来解决这个问题，从而实现了线性扩展。^([26](ch03.xhtml#idm46238725727968))
    这允许将大多数BERT模型中的512个标记的上下文急剧扩展到BigBird中的4,096个标记。这在需要保留长依赖性的情况下特别有用，比如在文本摘要中。
- en: Pretrained checkpoints of all models that we have seen in this section are available
    on the [Hugging Face Hub](https://oreil.ly/EIOrN) and can be fine-tuned to your
    use case with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, as described
    in the previous chapter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中看到的所有模型的预训练检查点都可以在[Hugging Face Hub](https://oreil.ly/EIOrN)上找到，并且可以根据前一章中描述的情况使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers进行微调。
- en: Conclusion
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter we started at the heart of the Transformer architecture with
    a deep dive into self-attention, and we subsequently added all the necessary parts
    to build a transformer encoder model. We added embedding layers for tokens and
    positional information, we built in a feed-forward layer to complement the attention
    heads, and finally we added a classification head to the model body to make predictions.
    We also had a look at the decoder side of the Transformer architecture, and concluded
    the chapter with an overview of the most important model architectures.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Transformer架构的核心开始，深入研究了自注意力，随后添加了构建Transformer编码器模型所需的所有必要部分。我们为标记和位置信息添加了嵌入层，为了补充注意力头，我们添加了一个前馈层，最后我们为模型主体添加了一个分类头来进行预测。我们还研究了Transformer架构的解码器部分，并总结了本章中最重要的模型架构。
- en: Now that you have a better understanding of the underlying principles, let’s
    go beyond simple classification and build a multilingual named entity recognition
    model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对基本原理有了更好的理解，让我们超越简单的分类，构建一个多语言命名实体识别模型。
- en: ^([1](ch03.xhtml#idm46238730148944-marker)) Y. Liu and M. Lapata, [“Text Summarization
    with Pretrained Encoder”](https://arxiv.org/abs/1908.08345), (2019).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm46238730148944-marker)) Y. Liu and M. Lapata, [“Text Summarization
    with Pretrained Encoder”](https://arxiv.org/abs/1908.08345), (2019)。
- en: ^([2](ch03.xhtml#idm46238730081440-marker)) M.E. Peters et al., [“Deep Contextualized
    Word Representations”](https://arxiv.org/abs/1802.05365), (2017).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 7. M.E. Peters等人，《深度上下文化的词表示》，（2017年）。
- en: ^([3](ch03.xhtml#idm46238730065680-marker)) A. Vaswani et al., [“Attention Is
    All You Need”](https://arxiv.org/abs/1706.03762), (2017).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 14. A. Vaswani等人，《Attention Is All You Need》，（2017年）。
- en: ^([4](ch03.xhtml#idm46238726765328-marker)) In fancier terminology, the self-attention
    and feed-forward layers are said to be *permutation equivariant*—if the input
    is permuted then the corresponding output of the layer is permuted in exactly
    the same way.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 更高级的术语是，自注意力和前馈层被称为*置换等变* - 如果输入被置换，那么层的相应输出将以完全相同的方式被置换。
- en: ^([5](ch03.xhtml#idm46238726501696-marker)) By combining the idea of absolute
    and relative positional representations, rotary position embeddings achieve excellent
    results on many tasks. GPT-Neo is an example of a model with rotary position embeddings.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 通过结合绝对和相对位置表示的思想，旋转位置嵌入在许多任务上取得了出色的结果。GPT-Neo是具有旋转位置嵌入的模型的一个例子。
- en: ^([6](ch03.xhtml#idm46238726113792-marker)) Note that unlike the self-attention
    layer, the key and query vectors in encoder-decoder attention can have different
    lengths. This is because the encoder and decoder inputs will generally involve
    sequences of differing length. As a result, the matrix of attention scores in
    this layer is rectangular, not square.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 请注意，与自注意力层不同，编码器-解码器注意力中的关键和查询向量可以具有不同的长度。这是因为编码器和解码器的输入通常涉及不同长度的序列。因此，该层中的注意力分数矩阵是矩形的，而不是正方形的。
- en: '^([7](ch03.xhtml#idm46238725854208-marker)) A. Wang et al., [“GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding”](https://arxiv.org/abs/1804.07461),
    (2018).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '17. A. Wang等人，《GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
    Language Understanding》，（2018年）。'
- en: '^([8](ch03.xhtml#idm46238725844992-marker)) J. Devlin et al., [“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805),
    (2018).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '9. J. Devlin等人，《BERT: Pre-Training of Deep Bidirectional Transformers for Language
    Understanding》，（2018年）。'
- en: '^([9](ch03.xhtml#idm46238725839424-marker)) V. Sanh et al., [“DistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”](https://arxiv.org/abs/1910.01108),
    (2019).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '19. V. Sanh等人，《DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper
    and Lighter》，（2019年）。'
- en: '^([10](ch03.xhtml#idm46238725833888-marker)) Y. Liu et al., [“RoBERTa: A Robustly
    Optimized BERT Pretraining Approach”](https://arxiv.org/abs/1907.11692), (2019).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '1. Y. Liu等人，《RoBERTa: A Robustly Optimized BERT Pretraining Approach》，（2019年）。'
- en: ^([11](ch03.xhtml#idm46238725828528-marker)) G. Lample, and A. Conneau, [“Cross-Lingual
    Language Model Pretraining”](https://arxiv.org/abs/1901.07291), (2019).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 12. G. Lample和A. Conneau，《跨语言语言模型预训练》，（2019年）。
- en: ^([12](ch03.xhtml#idm46238725820864-marker)) A. Conneau et al., [“Unsupervised
    Cross-Lingual Representation Learning at Scale”](https://arxiv.org/abs/1911.02116),
    (2019).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 18. A. Conneau等人，《规模化的无监督跨语言表示学习》，（2019年）。
- en: '^([13](ch03.xhtml#idm46238725814016-marker)) Z. Lan et al., [“ALBERT: A Lite
    BERT for Self-Supervised Learning of Language Representations”](https://arxiv.org/abs/1909.11942),
    (2019).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '10. Z. Lan等人，《ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations》，（2019年）。'
- en: '^([14](ch03.xhtml#idm46238725808096-marker)) K. Clark et al., [“ELECTRA: Pre-Training
    Text Encoders as Discriminators Rather Than Generators”](https://arxiv.org/abs/2003.10555),
    (2020).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '2. K. Clark等人，《ELECTRA: Pre-Training Text Encoders as Discriminators Rather
    Than Generators》，（2020年）。'
- en: '^([15](ch03.xhtml#idm46238725800480-marker)) P. He et al., [“DeBERTa: Decoding-Enhanced
    BERT with Disentangled Attention”](https://arxiv.org/abs/2006.03654), (2020).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '6. P. He等人，《DeBERTa: Decoding-Enhanced BERT with Disentangled Attention》，（2020年）。'
- en: '^([16](ch03.xhtml#idm46238725798384-marker)) A. Wang et al., [“SuperGLUE: A
    Stickier Benchmark for General-Purpose Language Understanding Systems”](https://arxiv.org/abs/1905.00537),
    (2019).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '13. A. Wang等人，《SuperGLUE: A Stickier Benchmark for General-Purpose Language
    Understanding Systems》，（2019年）。'
- en: ^([17](ch03.xhtml#idm46238725786352-marker)) A. Radford et al., [“Improving
    Language Understanding by Generative Pre-Training”](https://openai.com/blog/language-unsupervised),
    OpenAI (2018).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 5. A. Radford等人，《通过生成预训练改进语言理解》，OpenAI（2018年）。
- en: ^([18](ch03.xhtml#idm46238725779984-marker)) A. Radford et al., [“Language Models
    Are Unsupervised Multitask Learners”](https://openai.com/blog/better-language-models),
    OpenAI (2019).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 15. A. Radford等人，《语言模型是无监督多任务学习者》，OpenAI（2019年）。
- en: '^([19](ch03.xhtml#idm46238725774016-marker)) N.S. Keskar et al., [“CTRL: A
    Conditional Transformer Language Model for Controllable Generation”](https://arxiv.org/abs/1909.05858),
    (2019).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '11. N.S. Keskar等人，《CTRL: A Conditional Transformer Language Model for Controllable
    Generation》，（2019年）。'
- en: ^([20](ch03.xhtml#idm46238725769248-marker)) J. Kaplan et al., [“Scaling Laws
    for Neural Language Models”](https://arxiv.org/abs/2001.08361), (2020).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 20. J. Kaplan等人，《神经语言模型的缩放定律》，（2020年）。
- en: ^([21](ch03.xhtml#idm46238725767728-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 16. T. Brown等人，《语言模型是少样本学习者》，（2020年）。
- en: '^([22](ch03.xhtml#idm46238725759200-marker)) S. Black et al., [“GPT-Neo: Large
    Scale Autoregressive Language Modeling with Mesh-TensorFlow”](https://doi.org/10.5281/zenodo.5297715),
    (2021); B. Wang and A. Komatsuzaki, [“GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model”](https://github.com/kingoflolz/mesh-transformer-jax), (2021).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '^([22](ch03.xhtml#idm46238725759200-marker)) S. Black等，[“GPT-Neo: Large Scale
    Autoregressive Language Modeling with Mesh-TensorFlow”](https://doi.org/10.5281/zenodo.5297715)，(2021);
    B. Wang和A. Komatsuzaki，[“GPT-J-6B: A 6 Billion Parameter Autoregressive Language
    Model”](https://github.com/kingoflolz/mesh-transformer-jax)，(2021)。'
- en: ^([23](ch03.xhtml#idm46238725745824-marker)) C. Raffel et al., [“Exploring the
    Limits of Transfer Learning with a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683),
    (2019).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch03.xhtml#idm46238725745824-marker)) C. Raffel等，[“Exploring the Limits
    of Transfer Learning with a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683)，(2019)。
- en: '^([24](ch03.xhtml#idm46238725739936-marker)) M. Lewis et al., [“BART: Denoising
    Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation,
    and Comprehension”](https://arxiv.org/abs/1910.13461), (2019).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '^([24](ch03.xhtml#idm46238725739936-marker)) M. Lewis等，[“BART: Denoising Sequence-to-Sequence
    Pre-Training for Natural Language Generation, Translation, and Comprehension”](https://arxiv.org/abs/1910.13461)，(2019)。'
- en: ^([25](ch03.xhtml#idm46238725734576-marker)) A. Fan et al., [“Beyond English-Centric
    Multilingual Machine Translation”](https://arxiv.org/abs/2010.11125), (2020).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch03.xhtml#idm46238725734576-marker)) A. Fan等，[“Beyond English-Centric
    Multilingual Machine Translation”](https://arxiv.org/abs/2010.11125)，(2020)。
- en: '^([26](ch03.xhtml#idm46238725727968-marker)) M. Zaheer et al., [“Big Bird:
    Transformers for Longer Sequences”](https://arxiv.org/abs/2007.14062), (2020).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '^([26](ch03.xhtml#idm46238725727968-marker)) M. Zaheer等，[“Big Bird: Transformers
    for Longer Sequences”](https://arxiv.org/abs/2007.14062)，(2020)。'
