- en: Part 3\. Natural language processing and Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：自然语言处理和Transformer
- en: Part III focuses on text generation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分专注于文本生成。
- en: In chapter 8, you’ll learn to build and train a recurrent neural network to
    generate text. Along the way, you’ll learn how tokenization and word embedding
    work. You’ll also learn to generate text autoregressively and how to use temperature
    and top-K sampling to control the creativity of the generated text. In chapters
    9 and 10, you’ll build a Transformer from scratch, based on the paper “Attention
    Is All You Need,” to translate English to French. In chapter 11, you’ll learn
    to build GPT-2XL, the largest version of GPT-2, from scratch. After that, you’ll
    learn how to extract the pretrained weights from Hugging Face and load them to
    your own GPT-2 model. You’ll use your GPT-2 to generate text by feeding a prompt
    to the model. In chapter 12, you’ll build and train a GPT model to generate text
    in Hemingway style.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章，你将学习如何构建和训练一个循环神经网络来生成文本。在这个过程中，你将了解标记化和词嵌入是如何工作的。你还将学习如何自回归地生成文本，以及如何使用温度和top-K采样来控制生成文本的创造力。在第9章和第10章，你将从头开始构建一个Transformer，基于论文“Attention
    Is All You Need”，来翻译英语到法语。在第11章，你将学习如何从头开始构建GPT-2XL，这是GPT-2的最大版本。之后，你将学习如何从Hugging
    Face提取预训练的权重并将它们加载到自己的GPT-2模型中。你将通过向模型输入提示来使用你的GPT-2生成文本。在第12章，你将构建和训练一个GPT模型，以生成海明威风格的文本。
