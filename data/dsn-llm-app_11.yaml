- en: Chapter 9\. Inference Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章\. 推理优化
- en: In the past few chapters, we learned several techniques for adapting and utilizing
    LLMs to solve specific tasks. In this chapter, we will learn how to efficiently
    perform inference on them for real-world usage. LLMs’ large size make deployment
    and inference particularly challenging, as they exert significant pressure on
    compute, memory, and energy requirements. This proves to be especially challenging
    on edge devices like mobile phones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们学习了多种适应和利用LLM来解决特定任务的技术。在本章中，我们将学习如何高效地对它们进行推理，以用于实际应用。LLM的大尺寸使得部署和推理特别具有挑战性，因为它们对计算、内存和能源需求施加了巨大的压力。这在移动手机等边缘设备上尤其具有挑战性。
- en: For the rest of the chapter, we will focus on the field of inference optimization,
    discussing the factors influencing LLM inference time. We will then showcase a
    variety of optimization techniques including caching, knowledge distillation,
    early exiting, quantization, parallel and speculative decoding, and more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将专注于推理优化领域，讨论影响LLM推理时间的因素。然后，我们将展示包括缓存、知识蒸馏、早期退出、量化、并行和推测解码等多种优化技术。
- en: LLM Inference Challenges
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM推理挑战
- en: 'What are the bottlenecks affecting LLM inference? As we all know, their gargantuan
    sizes necessitate vast computing and memory resources. Apart from that, two additional
    factors exacerbate the situation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 影响LLM推理的瓶颈有哪些？众所周知，它们庞大的体积需要大量的计算和内存资源。除此之外，还有两个额外因素加剧了这种情况：
- en: As seen in [Chapter 4](ch04.html#chapter_transformer-architecture), contemporary
    LLMs are based largely on decoder-only models that operate autoregressively. This
    means that each token is generated one after the other, thus imposing a sequential
    limitation. Later in this chapter, we will discuss techniques for parallel and
    speculative decoding that aim to speed up the decoding process.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如[第4章](ch04.html#chapter_transformer-architecture)中所示，当代LLM主要基于仅解码器模型，这些模型以自回归方式运行。这意味着每个标记都是依次生成的，从而施加了顺序限制。在本章的后面部分，我们将讨论旨在加快解码过程的并行和推测解码技术。
- en: As the input sequence length increases, the amount of compute needed increases
    quadratically. Later this chapter, we will discuss techniques like K-V caching
    that aim to alleviate this bottleneck.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着输入序列长度的增加，所需的计算量呈二次增长。在本章的后面部分，我们将讨论旨在缓解这一瓶颈的K-V缓存等技术。
- en: Let’s dive into the techniques used to optimize inference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解用于优化推理的技术。
- en: Inference Optimization Techniques
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理优化技术
- en: Since this is a problem that severely impacts the deployment of LLMs in real-world
    use cases, considerable attention has been given to inference optimization research
    in major industry and academic labs. Dozens of optimization techniques have been
    developed in recent years, without which the present ubiquity of LLMs would not
    have been achieved. For a near-comprehensive survey of the various types of techniques
    used for optimizing inference, check out [Zhou et al.’s survey paper](https://oreil.ly/MtzNn).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个严重影响LLM在实际应用场景中部署的问题，因此在主要行业和学术实验室中，对推理优化研究给予了相当大的关注。近年来，已经开发出数十种优化技术，没有这些技术，LLM的当前普及程度就不会达到。要了解用于优化推理的各种类型技术的近全面调查，请参阅[周等人调查论文](https://oreil.ly/MtzNn)。
- en: We will now focus on some of the most promising and effective inference optimization
    techniques used in LLM deployments. While most of you may not be implementing
    these techniques by yourself but instead rely on third-party tools, understanding
    the optimization techniques and the tradeoffs involved provide valuable insights
    that can help you choose among various solutions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注一些在LLM部署中使用的一些最有希望和最有效的推理优化技术。虽然你们中的大多数人可能不会亲自实施这些技术，而是依赖第三方工具，但了解优化技术和涉及的权衡可以提供有价值的见解，这有助于你们在各种解决方案中进行选择。
- en: 'Techniques for efficient inference aim to achieve the following three goals:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高效推理的技术旨在实现以下三个目标：
- en: Reduce compute
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 减少计算
- en: Techniques like caching, knowledge distillation, and early exit, each of them
    employing distinct strategies to reduce computation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像缓存、知识蒸馏和早期退出这样的技术，每个都采用不同的策略来减少计算。
- en: Speed up decoding
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 加快解码
- en: 'Techniques for parallel and speculative decoding aim to improve the throughput
    of the model: the number of tokens generated per second.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 并行和推测解码的技术旨在提高模型的吞吐量：每秒生成的标记数。
- en: Reduce storage needs
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 减少存储需求
- en: Quantization techniques aim to reduce the amount of storage needed for weights
    and activations of the model, by reducing space required to store numbers from
    32 bits to 16, 8, or even 4 bits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术旨在通过减少存储模型权重和激活所需的存储空间，将存储数字的空间从32位减少到16位、8位甚至4位。
- en: Techniques for Reducing Compute
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少计算的技术
- en: 'We can reduce compute required during inference by:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式减少推理过程中所需的计算：
- en: Trading compute for extra storage, using methods like caching.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用缓存等方法以存储空间换取计算资源。
- en: Foregoing certain operations during inference, using methods like *early exit*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中省略某些操作，使用如*早期退出*等方法。
- en: Deriving a smaller model from a larger model while preserving as many characteristics
    and capabilities from the larger model as possible, using techniques like knowledge
    distillation.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用知识蒸馏等技术，从较大的模型中导出较小的模型，同时尽可能保留较大的模型的特征和能力。
- en: The next sections will explore each of these methods in detail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将详细介绍这些方法。
- en: K-V Caching
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-V 缓存
- en: As seen in [Chapter 1](ch01.html#chapter_llm-introduction), LLMs do not have
    session memory; at every turn in an LLM conversation, the previous conversation
    history is added to the input. This means that every request to an LLM could potentially
    contain a lot of repetitive content in the prompt. For the repetitive parts of
    the prompt, the same computation is performed during the inference step again
    and again. Moreover, in autoregressive decoding, each token is generated as a
    function of the entire input and the previously generated tokens. Thus, there
    is a lot of duplicative computation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)中所述，LLM没有会话记忆；在LLM对话的每个回合中，之前的对话历史都会被添加到输入中。这意味着对LLM的每个请求都可能包含大量重复的内容在提示中。对于提示中的重复部分，在推理步骤中会反复执行相同的计算。此外，在自回归解码中，每个标记都是作为整个输入和之前生成的标记的函数生成的。因此，存在大量的重复计算。
- en: One way to alleviate this duplicative computation is to cache the data and reuse
    them when required. More specifically, we cache the keys (K) and values (V) of
    the self-attention blocks of the Transformer architecture, referred to as the
    K-V cache. Recall our discussion in [Chapter 4](ch04.html#chapter_transformer-architecture)
    about keys and values in the self-attention block of the Transformer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这种重复计算的一种方法是在需要时缓存数据并重复使用它们。更具体地说，我们缓存Transformer架构中自注意力块的键（K）和值（V），称为K-V缓存。回想一下我们在[第4章](ch04.html#chapter_transformer-architecture)中关于Transformer自注意力块中的键和值的讨论。
- en: Let’s look at some examples. Consider the task of analyzing sentiment of movie
    reviews. You might have a lengthy prompt providing detailed instructions on the
    nuances involved in analyzing sentiment. These instructions are included in the
    prompt for every input review being fed to the LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些例子。考虑分析电影评论情感的任务。你可能有一个长长的提示，提供了关于分析情感所涉及的细微之处的详细说明。这些说明被包含在提供给LLM的每个输入评论的提示中。
- en: Instead of incurring unnecessary overhead by repetitively processing the instruction
    tokens, the cache is consulted to fetch the K-V values for these tokens.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通过重复处理指令标记来产生不必要的开销，缓存被用来检索这些标记的K-V值。
- en: Similarly, consider the example of a question-answering assistant that provides
    customer support by answering questions from a product manual. In this case, the
    K-V values representing the product manual tokens can be cached and then reused
    for any requests where the product manual needs to be part of the prompt.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，考虑一个问答助手提供客户支持的例子，该助手通过回答产品手册中的问题来提供支持。在这种情况下，代表产品手册标记的K-V值可以被缓存，然后用于任何需要产品手册作为提示部分的请求。
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Caching can also enable adding a lot of few-shot examples in the prompt. This
    can sometimes be a lightweight alternative to fine-tuning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存还可以使在提示中添加大量少样本示例成为可能。这有时可以成为微调的轻量级替代方案。
- en: Major LLM providers like Google’s Gemini and Anthropic’s Claude provide caching
    support for their models through their APIs, calling it context caching. This
    also vastly reduces the cost for end users, as cached tokens are billed only once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的LLM提供商，如Google的Gemini和Anthropic的Claude，通过它们的API为它们的模型提供缓存支持，称之为上下文缓存。这也大大降低了最终用户的成本，因为缓存的标记只计费一次。
- en: Warning
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that in the caching strategy, we are trading compute for additional storage.
    K-V caches can get unfeasibly large, especially at longer sequence lengths.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在缓存策略中，我们是以额外的存储空间换取计算资源。K-V缓存可能会变得非常大，尤其是在较长的序列长度下。
- en: To keep costs under control, LLM providers typically limit the age of the cache
    to a short period or price users by caching duration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制成本，LLM提供商通常将缓存的年龄限制在很短的期限内，或者通过缓存持续时间向用户收费。
- en: 'As an example, let’s look at a request to Anthropic’s Claude suite of models
    that utilizes context caching:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看一个请求Anthropic的Claude模型套件，该套件利用上下文缓存：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `cache_control` parameter is used to specify that the system prompt and
    the product manual is to be cached. As of the book’s writing, Claude’s cache is
    live by default for five minutes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`cache_control`参数用于指定系统提示和产品手册将被缓存。截至本书编写时，Claude的缓存默认为五分钟。'
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Organize your prompt to place the cacheable components at the beginning of the
    prompt, i.e., the prompt prefix.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 组织你的提示，将可缓存的组件放在提示的开头，即提示前缀。
- en: Ultimately, caching can be very valuable in reducing inference time, especially
    in settings where instructions are repeated for a large number of calls, or the
    context window contains data like API documentation or RAG output that needs to
    persist across multiple calls.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，缓存可以在减少推理时间方面非常有价值，尤其是在指令在大量调用中重复或在上下文窗口包含需要跨多个调用持久化的数据（如API文档或RAG输出）的情况下。
- en: Next, we’ll explore the early exit method for reducing inference-time compute.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨用于减少推理时间计算量的早期退出方法。
- en: Early Exit
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期退出
- en: As shown in [Chapter 4](ch04.html#chapter_transformer-architecture), the Transformer
    architecture is made up of repeating blocks called layers. The output of each
    layer is an intermediate representation that gets fed as input to the layer above
    it. One simple way of reducing compute during inference is to exit inference at
    an intermediate layer and interpret it as the final output. This technique is
    called early exit. [Figure 9-1](#early-exit) shows early exit in practice.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第4章](ch04.html#chapter_transformer-architecture)所示，Transformer 架构由重复的块组成，称为层。每一层的输出是一个中间表示，作为输入传递给其上层的层。在推理过程中减少计算的一个简单方法是在中间层退出推理并将其解释为最终输出。这种技术称为早期退出。[图9-1](#early-exit)展示了早期退出的实际应用。
- en: '![early-exit](assets/dllm_0901.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![early-exit](assets/dllm_0901.png)'
- en: Figure 9-1\. Early exit in practice
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 实际中的早期退出
- en: Early exit can happen both at the sequence level and at the token level.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 早期退出可以在序列级别和标记级别发生。
- en: Sequence-level early exit
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列级别早期退出
- en: In this scenario, the forward pass in the Transformer is stopped at a particular
    layer for the entire input sequence, and the intermediate representations from
    that layer are taken as the final output. The layer at which to exit can be determined
    in advance or can be dynamically decided based on the input sequence.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Transformer中的前向传递在整个输入序列上停止在特定的层，并将该层的中间表示作为最终输出。退出的层可以预先确定，也可以根据输入序列动态决定。
- en: To dynamically decide the layer to exit, you can train adapters on top of each
    layer, as shown in [Chapter 7](ch07.html#ch07). These modules can then be used
    to predict whether the exit can happen at the current layer. For example, [FastBERT](https://oreil.ly/GCfpt)
    implements modules at each layer that learn to solve a binary classification problem
    (to exit or not exit).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了动态决定退出的层，你可以在每个层上训练适配器，如[第7章](ch07.html#ch07)中所示。这些模块可以用来预测是否可以在当前层退出。例如，[FastBERT](https://oreil.ly/GCfpt)在每个层上实现了学习解决二元分类问题（退出或不退出）的模块。
- en: Not all methods depend on adding trainable modules. For example, the [hash-based
    early exiting approach (HashEE)](https://oreil.ly/_JqqH) by Sun et al. uses an
    annotated set of sequences along with their exit layers as the basis for determining
    the exit layers for new input sequences. This method is based on the hypothesis
    that similar sequences should exit at the same layers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有方法都依赖于添加可训练模块。例如，Sun等人提出的基于哈希的早期退出方法（HashEE）[https://oreil.ly/_JqqH](https://oreil.ly/_JqqH)使用一组带有其退出层的注释序列作为确定新输入序列退出层的基础。该方法基于假设相似序列应该在相同的层退出。
- en: The second early exit option is token-level early exit.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种早期退出选项是标记级别的早期退出。
- en: Token-level early exit
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记级别早期退出
- en: In this approach, different tokens from the same sequence can exit at different
    layers. This is more complex to implement than sequence-level early exit.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，同一序列的不同标记可以在不同的层退出。这比序列级别的早期退出更复杂。
- en: Similar to sequence-level early exit techniques, you can implement binary classifiers
    to decide whether to exit at a particular layer, but this happens at each token
    at each layer, instead of the entire sequence. For more details on token-level
    early exit, refer to [Schuster et al.](https://oreil.ly/hfdCd), who introduced
    the technique Confident Adaptive Language Modeling (CALM), that implements token-level
    early exit.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与序列级早期退出技术类似，你可以实现二元分类器来决定是否在特定层退出，但这是在每个层的每个标记处发生的，而不是整个序列。有关标记级早期退出的更多详细信息，请参阅[Schuster等人](https://oreil.ly/hfdCd)介绍的技术自信自适应语言建模（CALM），该技术实现了标记级早期退出。
- en: Recall that in the self-attention subblock of the Transformer, the representation
    of a token is calculated using the representations of all other tokens in the
    sequence in the same layer. But if we are using token-level early exit, it is
    possible that some tokens in a sequence might already have exited before that
    layer. The easiest way to resolve this issue is to copy the representations of
    the exited token to every layer above it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在Transformer的自注意力子块中，一个标记的表示是通过使用同一层中序列中所有其他标记的表示来计算的。但如果我们使用标记级早期退出，序列中的一些标记可能在那一层之前就已经退出了。解决这个问题的最简单方法是将退出标记的表示复制到它上面的每一层。
- en: While token-level early exit can be more fine-grained and effective than sequence-level
    early exit, it is slower than sequence-level early exit.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标记级早期退出可能比序列级早期退出更精细和有效，但它比序列级早期退出慢。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In early exit, the reduction in compute comes at the cost of performance. However,
    this can be minimized by learning to exit at the optimal layer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期退出中，计算量的减少是以性能为代价的。然而，这可以通过学习在最佳层退出来最小化。
- en: Dynamic early exit belongs to a class of techniques called *dynamic inference*,
    where the inference compute is determined dynamically, based on the characteristics
    of the input. One important example is the mixture of experts (MoE) class of models,
    introduced in [Chapter 4](ch04.html#chapter_transformer-architecture). In MoE
    models, a routing function chooses a small subset of expert modules to run inference
    on, thus reducing the amount of compute required.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 动态早期退出属于一类称为 *动态推理* 的技术，其中推理计算是动态确定的，基于输入的特性。一个重要例子是混合专家（MoE）模型类，在[第4章](ch04.html#chapter_transformer-architecture)中介绍。在MoE模型中，一个路由函数会选择一小部分专家模块来运行推理，从而减少所需的计算量。
- en: Next, let’s explore how we can reduce inference time by creating a smaller derivative
    model from a larger model while limiting performance degradation, using a technique
    called knowledge distillation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨如何通过创建一个较小的衍生模型来减少推理时间，同时限制性能下降，使用一种称为知识蒸馏的技术。
- en: Knowledge Distillation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: In [Chapter 5](ch05.html#chapter_utilizing_llms), we briefly introduced distilled
    versions of models, like [DistilBERT](https://oreil.ly/rgiHZ). These are smaller
    models that approximate the capabilities of the larger models they are distilled
    from, thus enabling speedier inference.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#chapter_utilizing_llms)中，我们简要介绍了模型的蒸馏版本，如[DistilBERT](https://oreil.ly/rgiHZ)。这些是较小的模型，它们近似于从它们蒸馏出来的较大模型的能力，从而实现更快的推理。
- en: Over the years, several techniques have been developed for knowledge distillation.
    For a survey of research advances in this field, refer to [Xu et al.’s](https://oreil.ly/JZQf3)
    survey paper.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已经开发出几种用于知识蒸馏的技术。关于该领域研究进展的综述，请参阅[Xu等人](https://oreil.ly/JZQf3)的调查论文。
- en: 'The process of knowledge distillation can be divided into two steps: distillation
    data preparation and training. The base model is referred to as the teacher model
    and the distilled model is referred to as the student model.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的过程可以分为两个步骤：蒸馏数据准备和训练。基本模型被称为教师模型，而蒸馏模型被称为学生模型。
- en: '[Figure 9-2](#knowledge-distillation) depicts the process of distilling a model.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-2](#knowledge-distillation)展示了模型蒸馏的过程。'
- en: '![knowledge-distillation](assets/dllm_0902.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![知识蒸馏](assets/dllm_0902.png)'
- en: Figure 9-2\. Knowledge distillation
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 知识蒸馏
- en: Here’s how the distillation data preparation and training steps work.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何进行蒸馏数据准备和训练步骤的说明。
- en: Distillation data preparation
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识蒸馏数据准备
- en: 'Data for distillation is typically prepared by appropriately querying the teacher
    model and using the teacher’s outputs as the *knowledge* to be distilled. Ways
    to elicit relevant outputs from the teacher include:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏数据通常是通过适当查询教师模型并使用教师的输出作为要蒸馏的*知识*来准备的。从教师那里引出相关输出的方法包括：
- en: Unsupervised generation
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督生成
- en: In this technique, the teacher is prompted with instructions and/or examples
    for solving a task. The teacher’s responses comprise the distillation dataset.
    This technique is commonly used to teach capabilities like CoT or instruction-following
    to smaller models. To accomplish that, teacher models are asked to respond to
    queries with the thought process leading up to the answer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项技术中，教师会收到解决任务的指令和/或示例。教师的回答构成了蒸馏数据集。这种技术通常用于向较小的模型教授像CoT或指令遵循这样的能力。为了实现这一点，教师模型被要求对查询做出回应，并展示出达到答案之前的思维过程。
- en: Data augmentation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强
- en: In this technique, the teacher is shown a set of seed input-output examples.
    Based on the seed examples, the teacher generates similar input-output examples,
    constituting the distillation dataset. Note that both the input and output are
    generated by the teacher model in this setting. The limitation of this technique
    is that the teacher is unable to generate sufficiently diverse examples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项技术中，教师被展示一组种子输入-输出示例。基于种子示例，教师生成类似的输入-输出示例，构成蒸馏数据集。请注意，在这种设置中，输入和输出都是由教师模型生成的。这项技术的局限性在于教师无法生成足够多样化的示例。
- en: Intermediate representations
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 中间表示
- en: This class of techniques is known as white-box distillation. Here the distillation
    dataset consists of intermediate representations from a model, which can include
    activations or output logits. This data can be used to align the student model
    with the teacher model. The alignment is learned using methods like KL-divergence,
    discussed in [Chapter 4](ch04.html#chapter_transformer-architecture).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这类技术被称为白盒蒸馏。在这里，蒸馏数据集由模型的中间表示组成，这可能包括激活或输出logits。这些数据可以用来使学生模型与教师模型对齐。对齐是通过如KL散度等方法学习的，这些方法在第4章（ch04.html#chapter_transformer-architecture）中讨论过。
- en: Teacher feedback
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 教师反馈
- en: In this class of techniques, the outputs from a student model are assessed by
    the teacher model to generate feedback. The teacher model can be used to generate
    preference data, i.e., the quality ranking of outputs from the student. Feedback
    can also be given in the form of detailed instructions on how to improve on a
    given task. A popular technique using teacher feedback is RLAIF, which we introduced
    in [Chapter 5](ch05.html#chapter_utilizing_llms).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这类技术中，学生模型的输出由教师模型评估以生成反馈。教师模型可以用来生成偏好数据，即学生输出质量排名。反馈也可以以如何改进给定任务的详细说明的形式给出。使用教师反馈的一种流行技术是RLAIF，我们在[第5章](ch05.html#chapter_utilizing_llms)中介绍了它。
- en: Self-teaching
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自我教学
- en: In this class of techniques, the teacher and student model are one and the same.
    The student model progressively refines its own outputs and uses them as the distillation
    set. One way of self-teaching is to generate multiple outputs for each task, along
    with reasoning steps, and choosing the best one to be part of the distillation
    set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这类技术中，教师和学生模型是同一个。学生模型逐步改进其输出，并使用它们作为蒸馏集。自我教学的一种方式是为每个任务生成多个输出，包括推理步骤，并选择最好的一个作为蒸馏集的一部分。
- en: How many distillation examples do you need? Perhaps surprisingly, not a whole
    lot. [Zhou et al.](https://oreil.ly/MuOOj) show that even one thousand very high-quality
    examples are enough to create a strong distillation set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要多少个蒸馏示例？也许令人惊讶的是，并不多。[周等人](https://oreil.ly/MuOOj)表明，即使是一千个非常高质量的示例也足以创建一个强大的蒸馏集。
- en: Warning
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Just like fine-tuning and continued pre-training, knowledge distillation is
    susceptible to the catastrophic forgetting problem (introduced in [Chapter 7](ch07.html#ch07)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就像微调和持续预训练一样，知识蒸馏容易受到灾难性遗忘问题（在第7章中介绍）的影响。
- en: Now that we have seen the various ways to create distillation datasets, let’s
    turn to the actual distillation process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了创建蒸馏数据集的各种方法，让我们转向实际的蒸馏过程。
- en: Distillation
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒸馏
- en: 'Here are some techniques used to perform the distillation task. For a more
    detailed survey of techniques, refer to [Xu et al.](https://oreil.ly/9mbiN):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列举了一些用于执行蒸馏任务的技术。对于技术的更详细调查，请参阅[Xu等人](https://oreil.ly/9mbiN)：
- en: Supervised fine-tuning
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调
- en: This is the simplest way to accomplish knowledge distillation. The student model
    is fine-tuned using the distillation set with the objective of aligning its predictions
    with that of the teacher model. This method is typically used in black-box knowledge
    distillation settings, where the distillation set does not comprise any internal
    representations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实现知识蒸馏的最简单方法。学生模型使用蒸馏集进行微调，目的是使其预测与教师模型的对齐。这种方法通常用于黑盒知识蒸馏设置，其中蒸馏集不包含任何内部表示。
- en: K-L divergence of output probabilities
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出概率的K-L散度
- en: In this method, our objective function is to minimize the K-L divergence between
    the output probability distribution of the teacher model and the student model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们的目标函数是最小化教师模型和学生模型的输出概率分布之间的K-L散度。
- en: Internal representation similarity
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 内部表示相似性
- en: Conversely, instead of minimizing divergence, you can maximize similarity between
    aspects of the teacher and student model. This can be leveraged to perform layerwise
    distillation, where the internal representations of the teacher and the student
    are aligned at each layer. Refer to [Liang et al.](https://oreil.ly/g-C4L) for
    an effective technique for layerwise distillation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你不仅可以最小化差异，还可以最大化教师模型和学生模型各部分的相似性。这可以用来执行分层蒸馏，其中教师模型和学生模型的内部表示在每个层上都对齐。有关分层蒸馏的有效技术，请参阅[Liang等人](https://oreil.ly/g-C4L)。
- en: Reinforcement learning
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: This involves training a reward model using the distillation data. The student
    model is then trained to maximize the reward as per the reward model. Recall our
    discussion on reinforcement learning in [Chapter 8](ch08.html#ch8).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及使用蒸馏数据训练奖励模型。然后，学生模型被训练以根据奖励模型最大化奖励。回想一下我们在[第8章](ch08.html#ch8)中关于强化学习的讨论。
- en: Ultimately, the technique you choose to distill your models depends on whether
    you have access to the teacher weights. If you do not have access to the teacher
    weights, then you can perform only supervised fine-tuning. White-box distillation,
    where you are trying to align intermediate representations and not just the output
    tokens, can be challenging to achieve. Note that all knowledge distillation techniques
    carry the risk of capability degradation or catastrophic forgetting, so you will
    have to evaluate the student model very carefully to quantify the difference in
    capabilities from the teacher model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你选择用于蒸馏模型的技术取决于你是否可以访问教师权重。如果你无法访问教师权重，那么你只能执行监督微调。白盒蒸馏，其中你试图对齐中间表示而不是仅仅输出标记，可能很难实现。请注意，所有知识蒸馏技术都存在能力退化或灾难性遗忘的风险，因此你必须非常仔细地评估学生模型，以量化与教师模型的能力差异。
- en: 'In this section, we discussed three distinct techniques for reducing compute
    during inference: caching, early exit, and knowledge distillation. Next, let’s
    discuss techniques that can accelerate the decoding process.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了三种在推理过程中减少计算的技术：缓存、提前退出和知识蒸馏。接下来，让我们讨论可以加速解码过程的技术。
- en: Techniques for Accelerating Decoding
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速解码的技术
- en: As we know, autoregressive models output one token at a time, where the next
    token being generated is a function of the input tokens and all the previously
    generated tokens. This imposes a sequential limitation as you have to wait for
    the current token to be generated before generating the next one. Can we bypass
    this limitation? Several recent techniques like *speculative decoding* and *parallel
    decoding* have been developed. Let’s examine them in detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，自回归模型一次输出一个标记，下一个生成的标记是输入标记和所有先前生成的标记的函数。这强加了顺序限制，你必须等待当前标记生成后才能生成下一个标记。我们能绕过这个限制吗？最近已经开发出几种技术，如*演示解码*和*并行解码*。让我们详细考察它们。
- en: Speculative Decoding
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示解码
- en: The concept behind speculative decoding is simple. A smaller model, called a
    draft model, is used to generate several subsequent candidate output tokens. Then,
    the main larger model is used to compute the conditional probabilities of the
    candidate output tokens at once, using them to decide which tokens to accept and
    which ones to reject. The more draft tokens accepted, the better the draft model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 演示解码背后的概念很简单。使用一个较小的模型，称为草稿模型，来生成几个后续的候选输出标记。然后，使用主要的大模型一次性计算候选输出标记的条件概率，并据此决定接受哪些标记以及拒绝哪些标记。接受的草稿标记越多，草稿模型越好。
- en: '[Figure 9-3](#speculative-decoding) depicts the speculative decoding process.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](#speculative-decoding)展示了推测解码过程。'
- en: '![speculative-decoding](assets/dllm_0903.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![speculative-decoding](assets/dllm_0903.png)'
- en: Figure 9-3\. Speculative decoding in action
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 实际中的推测解码
- en: 'Two important metrics in speculative decoding are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在推测解码中有两个重要指标：
- en: Token acceptance rate
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌接受率
- en: This is the percentage of tokens generated by the draft model that are accepted.
    Typically, this does not reach 1, because if it did, there is no need to use the
    main larger model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是草稿模型生成的令牌被接受的百分比。通常，这个比例不会达到1，因为如果达到了，就没有必要使用更大的主要模型。
- en: Decoding speedup
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 解码加速
- en: This refers to the reduction in latency between a model purely using autoregressive
    decoding versus one using speculative decoding.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是纯自回归解码模型与使用推测解码的模型之间的延迟减少。
- en: Parallel Decoding
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行解码
- en: Can we generate more than one token at the same time? This can be done either
    using the same model (multi-token decoding) or multiple instances of the same
    model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否同时生成多个令牌？这可以通过使用相同的模型（多令牌解码）或相同模型的多个实例来实现。
- en: For the latter, we can control parallel generation through the prompt. For example,
    say you are writing an article about a tourist site, containing sections like
    Food, Stay, Safety Tips, etc. You can prompt the LLM to list the sections, marked
    with special tokens. These sections can then be generated in parallel, assuming
    the sections are fully independent of each other.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后者，我们可以通过提示来控制并行生成。例如，假设你正在撰写一篇关于旅游景点的文章，包含如食物、住宿、安全提示等部分。你可以提示LLM列出这些部分，并使用特殊令牌标记。然后，这些部分可以并行生成，前提是这些部分之间完全独立。
- en: '[Figure 9-4](#parallel-generation) depicts the workflow of a system that generates
    parts of the output in a parallel fashion.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-4](#parallel-generation)展示了生成输出部分并行方式的系统工作流程。'
- en: Let’s now explore how the same model can generate multiple tokens at a time,
    called multi-token decoding. Several techniques have been proposed recently for
    multi-token decoding, one of the most promising being Medusa by [Cai et al.](https://oreil.ly/qT94i)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索同一个模型如何一次生成多个令牌，这被称为多令牌解码。最近已经提出了几种多令牌解码技术，其中最有前景的是由Cai等人提出的Medusa。[Cai
    et al.](https://oreil.ly/qT94i)
- en: In Medusa, additional decoding heads are added to the model. These decoding
    heads represent subsequent tokens to be generated. For example, the standard decoding
    head is predicting the next (n + 1st) token in the sequence, and the additional
    decoding heads are predicting the n + 2nd, n + 3rd, and so on, tokens, respectively.
    Refer to the Medusa paper for more details on how this is implemented.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在Medusa中，模型中增加了额外的解码头。这些解码头代表随后要生成的令牌。例如，标准解码头预测序列中的下一个（n + 1）个令牌，而额外的解码头分别预测n
    + 2、n + 3等令牌。有关如何实现的更多详细信息，请参阅Medusa论文。
- en: So far, we have learned techniques to accelerate the decoding process and to
    reduce compute. Next, let’s dive into quantization, a class of techniques to reduce
    the storage required by the model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了加速解码过程和减少计算的技术。接下来，让我们深入了解量化，这是一类减少模型所需存储的技术。
- en: '![Parallel Generation](assets/dllm_0904.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Parallel Generation](assets/dllm_0904.png)'
- en: Figure 9-4\. Parallel decoding workflow
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. 并行解码工作流程
- en: Techniques for Reducing Storage Needs
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少存储需求的技术
- en: In Chapters [5](ch05.html#chapter_utilizing_llms) and [6](ch06.html#llm-fine-tuning),
    we briefly touched upon quantization but promised to go into detail later. Let’s
    dive in!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](ch05.html#chapter_utilizing_llms)章和第[6](ch06.html#llm-fine-tuning)章中，我们简要介绍了量化，但承诺稍后会详细介绍。让我们深入探讨！
- en: The forward pass of a language model involves numbers representing inputs, weights,
    and activations. How are these numbers represented in memory?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的正向传播涉及表示输入、权重和激活的数字。这些数字在内存中是如何表示的？
- en: Several types of numerical representation formats are available, like integer,
    floating point, etc. Typically, numbers in language models are represented in
    floating-point32 (FP32), also called single-precision floating point, which refers
    to a floating point number composed of 32 bits, or 4 bytes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的数值表示格式有多种，例如整数、浮点数等。通常，语言模型中的数字以浮点32（FP32）表示，也称为单精度浮点数，它指的是由32位或4字节组成的浮点数。
- en: 'A number represented in FP32 is composed of three parts:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 用FP32表示的数字由三部分组成：
- en: A sign bit
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号位
- en: The exponent (8 bits)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数（8位）
- en: The mantissa/significand (23 bits)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尾数/有效数字（23位）
- en: For more details on how FP32 works, see [“Demystifying Floating Point Precision”](https://oreil.ly/uCYYl).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于FP32如何工作的更多细节，请参阅[“揭秘浮点精度”](https://oreil.ly/uCYYl)。
- en: 'The maximum and minimum value that can be represented using FP32 is 3.4028237
    × 10^(38) and 1.175494 × 10^(38), respectively. This is referred to as the range
    of values that can be represented by this format. Similarly, a number represented
    in float16 (FP16), also referred to as half-precision floating point, is composed
    of these three parts:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FP32可以表示的最大和最小值分别为3.4028237 × 10^(38)和1.175494 × 10^(38)。这被称为该格式可以表示的值域。同样，用float16（FP16）表示的数字，也称为半精度浮点数，由这三个部分组成：
- en: A sign bit
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个符号位
- en: The exponent (5 bits)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数（5位）
- en: The mantissa/significand (10 bits)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尾数/有效数字（10位）
- en: What happens when you take a number that is represented using FP32 and represent
    it in FP16? This amounts to a lossy conversion. In this case, both the range and
    the precision are impacted, because in FP16, 65,504 is the largest number you
    can represent, compared to 3.4 × 10^(38) for FP32\. The precision is impacted
    too, as the 32-bit version offers ~7 digits of precision, but the 16-bit version
    only offers ~3 digits of precision.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将使用FP32表示的数字表示为FP16时会发生什么？这相当于有损转换。在这种情况下，范围和精度都会受到影响，因为在FP16中，你可以表示的最大数字是65,504，而FP32为3.4
    × 10^(38)。精度也会受到影响，因为32位版本提供约7位精度，而16位版本只提供约3位精度。
- en: To prevent the massive loss in precision with FP16, bfloat16 (BF16), also called
    the brain floating point, was invented by Google Brain. In BF16, there are 8 digits
    for the exponent, and 7 bits for the mantissa. This keeps the range of numbers
    represented the same as that of float32 at the cost of reduced precision.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止FP16带来的精度大量损失，Google Brain发明了bfloat16（BF16），也称为脑浮点。在BF16中，指数有8位，尾数有7位。这保持了表示的数字范围与float32相同，但牺牲了精度。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Older GPUs like the NVIDIA T4 do not support BF16.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 较旧的GPU，如NVIDIA T4，不支持BF16。
- en: The process of converting representation of a number from a higher-precision
    format to a lower-precision format is called quantization. We can quantize 32-bit
    values to 8-bit integer formats as well. This reduces memory requirements by a
    factor of 4, at the cost of even more precision. With 8-bit quantization, we can
    represent numbers between –127 and 127, without any decimal point.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将数字的表示从高精度格式转换为低精度格式的过程称为量化。我们还可以将32位值量化为8位整数格式。这通过4倍减少了内存需求，但以牺牲更多精度为代价。在8位量化中，我们可以表示介于-127和127之间的数字，没有小数点。
- en: Integer quantization can be performed either symmetrically or asymmetrically.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 整数量化可以是对称的也可以是非对称的。
- en: Symmetric Quantization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对称量化
- en: In this setting, the *0* value in the original format is mapped to the *0* value
    in the integer representation. This means that when you quantize 0 represented
    in fp32 to int8, the value remains 0.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，原始格式中的*0*值映射到整数表示中的*0*值。这意味着当你将fp32中表示的0量化为int8时，值保持为0。
- en: The remaining values can be mapped using various techniques, the most common
    being absmax quantization. In this method, if we know or can estimate the range
    of numbers that need to be represented, we can take the absolute maximum of the
    range and map it to the largest number in int8 (127), while the negative of the
    absolute maximum is mapped to the smallest number in int8 (–127). The remaining
    numbers are mapped according to scale.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的值可以使用各种技术进行映射，最常见的是absmax量化。在此方法中，如果我们知道或可以估计需要表示的数字的范围，我们可以取该范围的绝对最大值并将其映射到int8中的最大数（127），而绝对最大值的负数则映射到int8中的最小数（-127）。其余的数字根据比例进行映射。
- en: '[Figure 9-5](#symmetric-quantization) depicts absmax quantization at work,
    quantizing a number represented in FP32 to int8.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-5](#symmetric-quantization)展示了absmax量化在起作用，将FP32表示的数字量化为int8。'
- en: '![Absmax quantization](assets/dllm_0905.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Absmax量化](assets/dllm_0905.png)'
- en: Figure 9-5\. Absmax quantization
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5. Absmax量化
- en: Asymmetric Quantization
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非对称量化
- en: In this setting, the *0* value in the original format is not guaranteed to be
    mapped to the *0* value in the integer representation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，原始格式中的*0*值不一定映射到整数表示中的*0*值。
- en: A common technique is to take the minimum and maximum value that we need represented
    and map it to the minimum (–127) and maximum (127) values that can be represented
    in int8, respectively. For example, if the range of numbers we want represented
    is –23 to 87, then –23 is mapped to –127 and 87 is mapped to 127.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的技术是取我们需要表示的最小值和最大值，并将其映射到 int8 可以表示的最小值（-127）和最大值（127）。例如，如果我们想要表示的数字范围是
    -23 到 87，那么 -23 就映射到 -127，87 就映射到 127。
- en: Tip
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the range of numbers you want represented include outliers, they can play
    spoilsport. You can take care of outliers by clipping them, so that all outliers
    will be represented by the same maximum/minimum value.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要表示的数字范围包括异常值，它们可能会成为破坏者。你可以通过截断来处理异常值，这样所有异常值都将表示为相同的最大/最小值。
- en: How is quantization used in practice? Typically, quantization is applied after
    training. Both the model’s weights and activations can be quantized.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中如何使用量化？通常，量化是在训练之后应用的。模型的权重和激活都可以进行量化。
- en: Quantizing weights is much easier than quantizing activations. Since we know
    the weights beforehand, we can calculate the range, outliers, scaling factors,
    etc. that are needed for the quantization algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 量化权重比量化激活要容易得多。因为我们事先知道权重，所以我们可以计算量化算法所需的范围、异常值、缩放因子等。
- en: For activations, depending on how much latency we can tolerate, we can either
    perform dynamic or static scaling. In dynamic scaling, statistics like range,
    outliers, etc. are calculated dynamically during inference at each layer. In static
    scaling, we take a reference calibration dataset to estimate the statistics. While
    this approach speeds up inference, it can result in more quantization errors.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活，根据我们能够容忍的延迟量，我们可以进行动态或静态缩放。在动态缩放中，范围、异常值等统计信息在每个层推理期间动态计算。在静态缩放中，我们使用参考校准数据集来估计统计信息。虽然这种方法可以加快推理速度，但它可能导致更多的量化错误。
- en: For more details on implementing quantization, see [“A Visual Guide to Quantization”](https://oreil.ly/bpi3b)
    by Maarten Grootendorst.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 关于实现量化的更多细节，请参阅 Maarten Grootendorst 撰写的[“量化视觉指南”](https://oreil.ly/bpi3b)。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the causes of bottlenecks in LLM inference. We
    discussed a wide variety of techniques to make LLM inference more efficient, including
    techniques to reduce compute requirements, reduce storage requirements, and accelerate
    the decoding process. We explored techniques like caching, early exit, knowledge
    distillation, speculative and parallel decoding techniques, and quantization.
    In the next and final part of the book, we will explore LLM application paradigms
    and discuss the nuances involved in building full-fledged applications.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 LLM 推理中瓶颈的原因。我们讨论了多种技术来提高 LLM 推理的效率，包括减少计算需求、减少存储需求以及加速解码过程的技术。我们探讨了缓存、早期退出、知识蒸馏、推测和并行解码技术以及量化等技术。在本书的下一部分和最后一部分，我们将探讨
    LLM 应用范式，并讨论构建完整应用程序涉及的细微差别。
