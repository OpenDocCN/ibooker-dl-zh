- en: 7 Evolving RAGOps stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 RAGOps 堆栈的演变
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The design of RAG systems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 系统的设计
- en: Available tools and technologies that enable a RAG system
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于实现 RAG 系统的工具和技术
- en: Production best practices for RAG systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 系统的生产最佳实践
- en: So far, we have discussed the indexing pipeline, generation pipeline, and evaluation
    of a retrieval-augmented generation (RAG) system. Chapter 6 also covered some
    advanced strategies and techniques that are useful when building production-grade
    RAG systems. These strategies help improve the accuracy of retrieval and generation
    and, in some cases, reduce the system latency. With all this information, you
    should be able to stitch together a RAG system for your use cases. Chapter 2 briefly
    laid out the design of a RAG system. This chapter elaborates on that design.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了检索增强生成 (RAG) 系统的索引管道、生成管道和评估。第 6 章还介绍了一些在构建生产级 RAG 系统时有用的高级策略和技术。这些策略有助于提高检索和生成的准确性，在某些情况下，还可以减少系统延迟。有了所有这些信息，你应该能够为你的用例构建一个
    RAG 系统。第 2 章简要概述了 RAG 系统的设计。本章将详细阐述该设计。
- en: A RAG system is composed of standard application layers, as well as layers specific
    to generative AI applications. Stacked together, these layers create a robust
    RAG system.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统由标准应用程序层以及特定于生成 AI 应用的层组成。这些层堆叠在一起，创建了一个健壮的 RAG 系统。
- en: These layers are supported by a technology infrastructure. We delve into these
    layers and the available technologies and tools offered by popular service providers
    that can be used in crafting a RAG system. Some providers have started offering
    managed end-to-end RAG solutions, which we touch upon in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层由技术基础设施支持。我们深入探讨这些层以及流行的服务提供商提供的可用于构建 RAG 系统的技术和工具。一些提供商已经开始提供端到端管理的 RAG
    解决方案，本章将简要介绍。
- en: We wrap up the chapter with some learnings and best practices for putting RAG
    systems in production. Chapter 7 also marks the end of part 3 of the book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一些关于将 RAG 系统投入生产的经验和最佳实践来结束本章。第 7 章也标志着本书第三部分的结束。
- en: By the end of this chapter, you should
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该
- en: Understand the details of the layers in a RAG (RAGOps) stack.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 RAG (RAGOps) 堆栈中各层的细节。
- en: Be familiar with a host of service providers and the tools and technologies
    they offer for RAG systems.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉众多服务提供商以及他们为 RAG 系统提供的工具和技术。
- en: Know some of the pitfalls and best practices of putting RAG systems in production.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解将 RAG 系统投入生产的一些陷阱和最佳实践。
- en: A RAG system includes a lot of additional components compared to traditional
    software applications. Vector stores and embeddings models are essential components
    of the indexing pipeline. Knowledge graphs are becoming increasingly popular indexing
    structures. The generation component can have different kinds of language models.
    In addition, prompt management is becoming increasingly complex. The production
    ecosystem for RAG and LLM (large language models) applications is still evolving,
    but early tooling and design patterns have emerged. RAGOps refers to the operational
    practices, tools, and processes involved in deploying, maintaining, and optimizing
    RAG systems in production environments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统软件应用相比，RAG 系统包含许多额外的组件。向量存储和嵌入模型是索引管道的基本组件。知识图谱正变得越来越流行的索引结构。生成组件可以有不同的语言模型。此外，提示管理变得越来越复杂。RAG
    和 LLM (大型语言模型) 应用程序的生产生态系统仍在演变，但早期的工具和设计模式已经出现。RAGOps 指的是在生产环境中部署、维护和优化 RAG 系统所涉及的操作实践、工具和流程。
- en: 7.1 The evolving RAGOps stack
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 RAGOps 堆栈的演变
- en: This section describes different components required to build a RAG system in
    layers. These layers come together to form the operations stack for RAG. We will
    also take this opportunity to revise the workflow of the RAG system discussed
    in this book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了构建 RAG 系统所需的不同组件。这些层组合在一起形成了 RAG 的操作堆栈。我们还将利用这个机会来修订本书中讨论的 RAG 系统的工作流程。
- en: 'It should be noted that RAG, like generative AI in general, is an evolving
    technology, and therefore, the operations stack continues to evolve. You may find
    varying definitions and structures. This chapter provides a holistic view and
    discusses the components from the perspective of their criticality to the RAG
    system. We look at the layers divided into the following three categories:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意的是，RAG，就像一般的生成式AI一样，是一项不断发展的技术，因此，操作堆栈也在不断演变。你可能会发现不同的定义和结构。本章提供了一个全面的视角，并从其对RAG系统的重要性角度讨论了组件。我们查看以下三个类别划分的层：
- en: Critical layers that are fundamental to the operation of a RAG system. A RAG
    system is likely to fail if any of these layers are missing or are incomplete.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对RAG系统操作至关重要的关键层。如果这些层中的任何一个缺失或不完整，RAG系统很可能会失败。
- en: Essential layers that are important for performance, reliability, and safety
    of the system. These essential components bring the system to a standard that
    provides value to the user.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的层，对于系统的性能、可靠性和安全性至关重要。这些基本组件将系统提升到一个为用户提供价值的标准。
- en: Enhancement layers that improve the efficiency, scalability, and usability of
    the system. These components are used to make the RAG system better and are selected
    based on the end requirements.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强层，它提高了系统的效率、可扩展性和可用性。这些组件用于使RAG系统变得更好，并基于最终需求进行选择。
- en: 7.1.1 Critical layers
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 关键层
- en: The indexing pipeline and the generation pipeline (discussed in detail in chapters
    3 and 4) form the core of a RAG system. Figure 7.1 illustrates the indexing pipeline
    that facilitates the creation of the knowledge base for RAG systems and the generation
    pipeline that uses the knowledge base to generate context-aware responses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 索引管道和生成管道（在第3章和第4章中详细讨论）构成了RAG系统的核心。图7.1展示了索引管道，它有助于为RAG系统创建知识库，以及生成管道，它使用知识库来生成上下文感知的响应。
- en: '![](../Images/CH07_F01_Kimothi.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F01_Kimothi.png)'
- en: Figure 7.1  Indexing and generation pipelines forming the core of a RAG system
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1  索引和生成管道构成RAG系统的核心
- en: Layers enabling these two pipelines form the critical layers of the RAGOps stack.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 形成这两个管道的层的层，是RAGOps堆栈的关键层。
- en: Data layer
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据层
- en: 'The data layer serves the critical role of creating and storing the knowledge
    base for RAG. It is responsible for collecting data from source systems, transforming
    it into a usable format, and storing it for efficient retrieval. Here are some
    components of the data layer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据层在为RAG创建和存储知识库方面发挥着关键作用。它负责从源系统中收集数据，将其转换为可用的格式，并存储以实现高效检索。以下是数据层的一些组件：
- en: '*Data ingestion componen**t*—It collects data from source systems such as databases,
    content management systems, file systems, APIs, devices, and even the internet.
    The data can be ingested in batches or as a stream, depending on the use case.
    For ingesting data, your choice of tool can depend on factors such as data volume,
    types of data source, ingestion frequency, cost, and ease of setup. Data ingestion
    is not specific to RAG but is a mainstream component in modern software applications.
    AWS Glue, Azure Data Factory, Google Cloud Dataflow, Fivetran, Apache NiFi, Apache
    Kafka, and Airbyte are among tools available for use. For rapid prototyping and
    proof of concepts (PoCs), frameworks such as Lang­Chain and LlamaIndex have inbuilt
    functions that can assist in connecting to some sources and extracting information.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据采集组件*—它从数据库、内容管理系统、文件系统、API、设备甚至互联网等源系统中收集数据。数据可以批量或作为流式数据采集，具体取决于用例。对于数据采集，你选择的工具可能取决于数据量、数据源类型、采集频率、成本和设置简便性等因素。数据采集不仅限于RAG，而是现代软件应用中的主流组件。AWS
    Glue、Azure Data Factory、Google Cloud Dataflow、Fivetran、Apache NiFi、Apache Kafka和Airbyte等都是可用的工具。对于快速原型设计和概念验证（PoC），LangChain和LlamaIndex等框架内置了可以帮助连接某些源并提取信息的函数。'
- en: '*Data transformation componen**t*—It converts the ingested data from a raw
    to a usable form. A core process in the indexing pipeline is the *chunking* of
    data. We know that *embeddings* is the preferred format of choice for RAG applications
    because it makes it easier to apply semantic search. *Graph structures* are becoming
    increasingly popular in advanced systems. Certain pre-processing steps such as
    cleaning, de-duplication, metadata enrichment, and masking of sensitive information
    are also a part of this phase. While the volume of data and the nature of transformation
    play an important role in any data-transformation step, they are especially critical
    in RAG systems. All the extract–transform–load (ETL) tools mentioned in the data
    ingestion step in conjunction with tools such as Apache Spark and dbt also allow
    transformations. However, if we focus just on RAG, Unstructured.io specializes
    in processing and transforming unstructured data for use in LLM applications.
    It offers open source libraries as well as managed services. Constructing knowledge
    graphs from unstructured data has evolved today from early semantic networks and
    ontologies into robust frameworks.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据转换组件*—它将摄取的数据从原始形式转换为可用的形式。在索引管道中的核心过程是数据的*分块*。我们知道*嵌入*是RAG应用的首选格式，因为它使得应用语义搜索变得更容易。*图结构*在先进系统中越来越受欢迎。某些预处理步骤，如清理、去重、元数据丰富和敏感信息屏蔽，也是这一阶段的一部分。虽然数据量和转换的性质在任何数据转换步骤中都起着重要作用，但在RAG系统中它们尤其关键。在数据摄取步骤中提到的所有提取-转换-加载（ETL）工具，以及Apache
    Spark和dbt等工具，也允许转换。然而，如果我们只关注RAG，Unstructured.io专注于处理和转换非结构化数据，以便在LLM应用中使用。它提供开源库以及托管服务。从非结构化数据构建知识图谱已经从早期的语义网络和本体论发展到今天稳健的框架。'
- en: Microsoft’s GraphRAG is a framework that has pioneered the use of LLMs to extract
    entities and relationships from text.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微软的GraphRAG是一个框架，它开创了使用LLM从文本中提取实体和关系的方法。
- en: '*Data storage componen**t*—It stores the transformed data in a way that allows
    for fast and efficient retrieval. We have discussed that to store embeddings,
    *vector databases* are widely used because they are efficient in similarity search.
    For graph structures, *graph databases* are used. Most traditional database providers
    are incorporating vector search capabilities into their systems. Cost, scale,
    and speed are the primary drivers in the choice of data storage. We have used
    a vector index such as FAISS in this book. Pinecone is a fully managed cloud-native
    service. Milvus, Qdrant, and Chroma are among the open source vector databases.
    Weviate is another database that also has a GraphQL-based interface for knowledge
    graphs. Neo4j is a leading graph database for storing and querying graph data.
    A comparison of popular vector databases is available at [https://www.superlinked.com/vector-db-comparison](https://www.superlinked.com/vector-db-comparison).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据存储组件*—它以允许快速高效检索的方式存储转换后的数据。我们讨论过，为了存储嵌入，*向量数据库*被广泛使用，因为它们在相似性搜索中效率很高。对于图结构，使用*图数据库*。大多数传统数据库提供商正在将向量搜索功能纳入其系统。成本、规模和速度是选择数据存储的主要驱动因素。我们在这本书中使用了FAISS这样的向量索引。Pinecone是一个完全托管的云原生服务。Milvus、Qdrant和Chroma是开源向量数据库之一。Weviate是另一个具有基于GraphQL接口的知识图的数据库。Neo4j是存储和查询图数据的领先图数据库。有关流行向量数据库的比较，请参阅[https://www.superlinked.com/vector-db-comparison](https://www.superlinked.com/vector-db-comparison)。'
- en: The flow from source systems to data storage via the ingestion and transformation
    components that lead to the creation of the knowledge base is shown in figure
    7.2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从源系统通过摄取和转换组件流向数据存储，这些组件导致知识库的创建，如图7.2所示。
- en: '![](../Images/CH07_F02_Kimothi.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F02_Kimothi.png)'
- en: 'Figure 7.2  Data layer: Creating the knowledge base by extracting, transforming,
    and loading (ETL) data from source systems'
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 数据层：通过从源系统提取、转换和加载（ETL）数据来创建知识库
- en: A strong data layer is the foundation of an efficient RAG system. The data layer
    also comes in handy when there is a need for fine-tuning of models. We discuss
    this feature briefly later in the chapter. Next, we look at the model layer, which
    includes the embeddings models used to transform text into vectors and the LLMs
    used in generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的数据层是高效RAG系统的基石。当需要微调模型时，数据层也很有用。我们将在本章稍后简要讨论这一功能。接下来，我们将探讨模型层，它包括用于将文本转换为向量的嵌入模型和用于生成的LLM。
- en: Model layer
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型层
- en: 'Predictive models enable generative AI applications. Some models are provided
    by third parties, and some need to be custom trained or fine-tuned. Generating
    quick and cost-effective model responses is also an important aspect of using
    predictive models. The model layer includes the following three components:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 预测模型使生成式人工智能应用成为可能。一些模型由第三方提供，而另一些则需要定制训练或微调。生成快速且成本效益高的模型响应也是使用预测模型的一个重要方面。模型层包括以下三个组件：
- en: '*Model librar**y*—It contains the list of models that have been chosen for
    the application. The most popular models are the LLMs that generate text and other
    generative models that can generate images, video, and audio. We saw that in the
    data layer, raw text is transformed into vector embeddings, and this is done using
    embeddings models. Apart from this, there are other models used in RAG systems:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型库*——它包含为应用选择的所有模型的列表。最受欢迎的模型是生成文本和其他能够生成图像、视频和音频的生成模型，即大型语言模型（LLMs）。我们在数据层中看到，原始文本被转换成向量嵌入，这是通过嵌入模型完成的。除此之外，还有其他在RAG系统中使用的模型：'
- en: Embeddings models are used to transform data into vector format. We have discussed
    embeddings models in detail in chapter 3\. Recall that the choice of embeddings
    model depends on the domain, use case, and cost considerations. Providers such
    as OpenAI, Gemini by Google, Voyage AI, and Cohere provide a variety of embeddings
    model choices, and a host of open source embeddings models can also be used via
    Hugging Face transformers. Multimodal embeddings map data of different modalities
    into a shared embeddings space.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型用于将数据转换成向量格式。我们在第3章中详细讨论了嵌入模型。回想一下，嵌入模型的选择取决于领域、用例和成本考虑。OpenAI、Google的Gemini、Voyage
    AI和Cohere等提供商提供了各种嵌入模型选择，通过Hugging Face transformers也可以使用大量开源嵌入模型。多模态嵌入将不同模态的数据映射到共享嵌入空间中。
- en: Foundation models or the pre-trained LLMs are used for the generation of outputs,
    as well as for evaluation and adaptive tasks where LLMs are used to judge. We
    have discussed LLMs as part of the generation pipeline in chapter 4\. Recall that
    the GPT series by OpenAI, Gemini Series by Google, Claude Series by Anthropic,
    and Command R series by Cohere are popular proprietary LLMs. The llama series
    by Meta and Mistral are open source models that have gained popularity. Most LLMs
    now include multimodal capabilities and are continuously evolving.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型或预训练的LLMs用于生成输出，以及用于评估和自适应任务，在这些任务中LLMs被用来判断。我们在第4章中讨论了LLMs作为生成管道的一部分。回想一下，OpenAI的GPT系列、Google的Gemini系列、Anthropic的Claude系列和Cohere的Command
    R系列是流行的专有LLMs。Meta的llama系列和Mistral是开源模型，它们已经获得了流行。现在，大多数LLMs都包括多模态功能，并且正在不断进化。
- en: Task-specific models are machine learning models that are not core to RAG but
    come in handy for various tasks. These models are used in advanced RAG pipelines.
    Query classification models for efficient routing and intent detection, NER models
    to detect entities for metadata, query-expansion models, hallucination-detection
    models, and bias- and toxicity-moderation models are some examples of task-specific
    models useful in RAG systems. While task-specific models are generally custom
    trained, providers such as OpenAI, Hugging Face, and Google also offer these services.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务特定模型是RAG非核心的机器学习模型，但在各种任务中非常有用。这些模型用于高级RAG管道。用于高效路由和意图检测的查询分类模型、用于检测元数据的实体识别（NER）模型、查询扩展模型、幻觉检测模型以及偏见和毒性调节模型是RAG系统中一些有用的任务特定模型示例。虽然任务特定模型通常需要定制训练，但OpenAI、Hugging
    Face和Google等提供商也提供这些服务。
- en: '*Model training and fine-tuning componen**t*—This component is responsible
    for building custom models and fine-tuning foundation models on custom data. In
    chapter 4, we discussed that fine-tuning of LLMs is sometimes required for domain
    adaptation. Fine-tuning can also be done for embeddings models. Additionally,
    the task-specific models can be trained on custom data. This component supports
    the algorithms used for training and fine-tuning the models. For training data,
    this component interacts with the data layer where the training data can be created
    and managed. A regular MLOps layer is also recommended for the development and
    maintenance of the models. This is enabled via ML platforms such as Hugging Face,
    AWS SageMaker, Azure ML, and similar.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型训练和微调组件*—此组件负责在自定义数据上构建自定义模型和微调基础模型。在第4章中，我们讨论了LLM的微调有时对于领域适应是必要的。微调也可以用于嵌入模型。此外，特定任务的模型可以在自定义数据上训练。此组件支持用于训练和微调模型的算法。对于训练数据，此组件与数据层交互，在数据层中可以创建和管理训练数据。还建议使用常规MLOps层来开发和维护模型。这可以通过Hugging
    Face、AWS SageMaker、Azure ML和类似平台实现。'
- en: Inference optimization component—This component is responsible for generating
    responses quickly and cost-effectively, which can be done by employing a variety
    of methods such as quantization, batching, KV(Key Value)-caching, and similar.
    ONNX and NVIDIA TensorRT-LLM are popular frameworks that optimize inferencing.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理优化组件—此组件负责快速且经济高效地生成响应，可以通过量化、批处理、KV（键值）缓存等方法实现。ONNX和NVIDIA TensorRT-LLM是流行的框架，用于优化推理。
- en: Figure 7.3 illustrates different components of the model layer. It shows how
    the model layer helps in deciding which models to use in the RAG system, facilitates
    training and fine-tuning of the model, and optimizes the models for efficient
    serving.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3展示了模型层的不同组件。它显示了模型层如何帮助决定在RAG系统中使用哪些模型，促进模型的训练和微调，并优化模型以实现高效的服务。
- en: '![](../Images/CH07_F03_Kimothi.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F03_Kimothi.png)'
- en: 'Figure 7.3  The model layer: The model library is the store for all models
    selected for the application, model training and fine-tuning interact with the
    data layer to source training data and train custom models, while the inference
    optimization component is responsible for efficient serving of the model.'
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 模型层：模型库是存储所有为应用、模型训练和微调所选模型的仓库，模型训练和微调与数据层交互以获取训练数据和训练自定义模型，而推理优化组件负责高效地提供模型服务。
- en: Model deployment
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'This layer is responsible for making the RAG system available to the application
    layer. It handles the infrastructure of the models. It also ensures that the models
    can be accessed reliably. There are four main methods by which the models can
    be deployed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此层负责使RAG系统对应用层可用。它处理模型的底层基础设施。它还确保模型可以可靠地访问。模型可以通过以下四种主要方法进行部署：
- en: '*Fully managed deploymen**t*—Itcan be provided by proprietary model providers
    such as OpenAI, Google, Anthropic, and Cohere, where all infrastructure for model
    deployment, serving, and scaling is managed and optimized by these providers.
    Services such as AWS SageMaker, Google Vertex AI, Azure Machine Learning, and
    Hugging Face offer platforms to deploy, serve, and monitor both open source and
    custom-developed models. Amazon Bedrock is another fully managed service that
    provides access to a variety of foundation models, both proprietary and open source,
    simplifying model access and deployment.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完全托管部署*—这可以由像OpenAI、Google、Anthropic和Cohere这样的专有模型提供商提供，其中所有模型部署、服务和扩展的基础设施都由这些提供商管理和优化。AWS
    SageMaker、Google Vertex AI、Azure Machine Learning和Hugging Face等服务提供平台，用于部署、服务和监控开源和自定义开发的模型。Amazon
    Bedrock是另一种完全托管的服务，提供对各种基础模型（专有和开源）的访问，简化了模型访问和部署。'
- en: '*Self-hosted deploymen**t*—This type of deployment is enabled by cloud VM providers
    such as AWS, GCP, Azure, and hardware providers such as Nvidia. In this scenario,
    models are deployed in private clouds or on-premises, and the infrastructure is
    managed by the application developer. Tools such as Kubernetes and Docker are
    widely used for containerization and orchestration of models, while Nvidia Triton
    Inference Server can optimize inference on Nvidia hardware.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自托管部署*—此类部署由云VM提供商（如AWS、GCP、Azure）和硬件提供商（如Nvidia）启用。在这种情况下，模型在私有云或本地部署，基础设施由应用开发者管理。Kubernetes和Docker等工具广泛用于模型的容器化和编排，而Nvidia
    Triton Inference Server可以优化Nvidia硬件上的推理。'
- en: '*Local/edge deploymen**t*—It involves running optimized versions of models
    on local hardware or edge devices, ensuring data privacy, reduced latency, and
    offline functionality. Local/edge deployment typically requires model compression
    techniques such as quantization and pruning, and smaller models tailored for resource-constrained
    environments. Tools such as ONNX, TensorFlow Lite, and PyTorch Mobile enable efficient
    deployment on mobile and embedded platforms, while GGML and NVIDIA TensorRT support
    CPU and GPU optimizations. GPT4All is a popular open source solution for running
    quantized LLMs locally on devices such as laptops, IoT devices, and edge servers
    without relying on cloud infrastructure. These frameworks facilitate low-latency,
    power-efficient execution, making AI accessible in decentralized environments.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地/边缘部署*—涉及在本地硬件或边缘设备上运行模型的优化版本，确保数据隐私、降低延迟和离线功能。本地/边缘部署通常需要模型压缩技术，如量化修剪，以及针对资源受限环境定制的较小模型。ONNX、TensorFlow
    Lite和PyTorch Mobile等工具使移动和嵌入式平台上的高效部署成为可能，而GGML和NVIDIA TensorRT支持CPU和GPU优化。GPT4All是一个流行的开源解决方案，可以在笔记本电脑、物联网设备和边缘服务器等设备上本地运行量化LLM，而不依赖于云基础设施。这些框架促进了低延迟、节能的执行，使AI在去中心化环境中变得可访问。'
- en: Model deployment is a relatively complex task that requires engineering skills
    when self-hosted and local/edge deployment is done. Figure 7.4 illustrates the
    three ways in which models are deployed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署是一项相对复杂的任务，当进行自托管和本地/边缘部署时需要工程技能。图7.4说明了模型部署的三种方式。
- en: '![](../Images/CH07_F04_Kimothi.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F04_Kimothi.png)'
- en: Figure 7.4  The model deployment layer manages the infrastructure for hosting
    and deployment for efficient serving of all the models in the RAG system.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4  模型部署层管理RAG系统中所有模型的托管和部署基础设施，以实现高效服务。
- en: With the data and the model layers, the most essential components of the RAG
    system are in place. Now we need a layer that manages the co-ordination between
    the data and the models. This is the responsibility of the application orchestration
    layer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据和模型层一起，RAG系统的最基本组件已经就绪。现在我们需要一个管理数据和模型之间协调的层。这是应用编排层的责任。
- en: Application orchestration layer
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用编排层
- en: When we hear the term *orchestration*, a musical conductor leading a group of
    musicians in an orchestra comes to mind. An application orchestration layer is
    somewhat similar. It is responsible for managing the interactions among the other
    layers in the system. It is a central coordinator that enables communication between
    data, retrieval systems, generation models, and other services. The major components
    of the orchestration layer are
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们听到“编排”这个词时，会想到一个音乐指挥在交响乐团中领导一群音乐家。应用编排层在某种程度上类似。它负责管理系统中其他层之间的交互。它是一个中央协调器，使数据、检索系统、生成模型和其他服务之间的通信成为可能。编排层的主要组件包括
- en: '*Query orchestration componen**t*—Responsible for receiving and orchestrating
    user queries. All pre-retrieval query optimization steps such as query classification,
    expansion, and rewriting are orchestrated by this component. The query orchestration
    layer may coordinate with the end application layer to receive the input, and
    the model layer to access the models required for the query optimization. This
    component will generally pass on the processed query to the retrieval coordination
    and the generation coordination components.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询编排组件*—负责接收和编排用户查询。所有预处理查询优化步骤，如查询分类、扩展和重写，都由该组件编排。查询编排层可能协调与终端应用层接收输入，以及模型层访问用于查询优化的模型。该组件通常将处理后的查询传递给检索协调和生成协调组件。'
- en: '*Retrieval coordination componen**t*—Hosts the various retrieval logics. Depending
    on the input from the query orchestration module, it selects the appropriate retrieval
    method (dense retrieval or hybrid retrieval) and interacts with the data layer.
    Depending on the retrieval strategy, it may also interact with the model layer
    if any recursive or adaptive retrieval method is invoked.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索协调组件*——托管各种检索逻辑。根据查询编排模块的输入，它选择适当的方法（密集检索或混合检索）并与数据层交互。根据检索策略，它还可能根据是否调用了任何递归或自适应检索方法与模型层交互。'
- en: '*Generation coordination componen**t*—Receives the query and the context from
    the previous components and coordinates all the post-retrieval steps. Its primary
    function is to interact with the model layer and prompt the LLM to generate the
    output. Apart from generation, all the post-retrieval steps such as re-ranking
    and contextual compression are coordinated by this component. Post-generation
    tasks such as reflection, fact-checking, and moderation can be coordinated by
    the generation component. This component can also be made responsible for passing
    the output to the application layer.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成协调组件*——接收来自前一个组件的查询和上下文，并协调所有检索后的步骤。其主要功能是与模型层交互并提示LLM生成输出。除了生成之外，所有检索后的步骤，如重新排序和上下文压缩，都由该组件协调。生成后的任务，如反思、事实核查和审查，也可以由生成组件协调。该组件还可以负责将输出传递到应用层。'
- en: 'These are the three primary components of the orchestration layer. There are
    two additional components to consider:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是编排层的三个主要组件。还有两个额外的组件需要考虑：
- en: '*Multi-agent orchestration componen**t*—Used for agentic RAG where multiple
    agents handle specific tasks. We will take a deeper look at agentic RAG in chapter
    8\. The orchestration layer is responsible for managing agent interactions and
    coordination.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多代理编排组件*——用于处理多个代理的RAG，其中多个代理处理特定任务。我们将在第8章中更深入地探讨代理RAG。编排层负责管理代理交互和协调。'
- en: '*Workflow automation componen**t*—Sometimes employed for managing the flow
    and the movement of data between different components. This component is not specific
    to RAG systems but is commonly employed in data products. Apache Airflow and Dagster
    are popular tools used for workflow automation.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流自动化组件*——有时用于管理不同组件之间数据流和移动。这个组件并非特定于RAG系统，但在数据产品中应用较为普遍。Apache Airflow和Dagster是用于工作流自动化的流行工具。'
- en: Figure 7.5 illustrates the orchestration layer components interacting with the
    application layer, which is supported by the model deployment and data layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5展示了编排层组件与应用层交互的情况，应用层由模型部署和数据层支持。
- en: '![](../Images/CH07_F05_Kimothi.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F05_Kimothi.png)'
- en: Figure 7.5  The app orchestration layer accepts the user query from the application
    layer and sends the response back to the application layer.
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5  应用编排层接受来自应用层用户查询，并将响应发送回应用层。
- en: LangChain and LlamaIndex are the most common orchestration frameworks used to
    develop RAG systems. They provide abstractions for different components. Microsoft’s
    AutoGen and CrewAI are upcoming frameworks for multi-agent orchestration.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain和LlamaIndex是最常用的编排框架，用于开发RAG系统。它们为不同的组件提供了抽象。微软的AutoGen和CrewAI是多代理编排的即将推出的框架。
- en: With these four layers (i.e., data, model, model deployment, and application
    orchestration), the critical RAG system is complete. This core system can interact
    with the end-software application layer, which acts as the interface between the
    RAG system and the user. While the application layer is generally custom built,
    platforms such as Streamlit, Vercel, and Heroku are popular for hosting the application.
    Figure 7.6 summarizes the critical layers of the RAGOps stack.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这四层（即数据、模型、模型部署和应用编排），关键的RAG系统就完整了。这个核心系统可以与终端软件应用层交互，该层作为RAG系统与用户之间的接口。虽然应用层通常是定制构建的，但Streamlit、Vercel和Heroku等平台因其托管应用而受到欢迎。图7.6总结了RAGOps堆栈的关键层。
- en: Now that you are familiar with the core layers of the stack, let’s look next
    at the essential layers that improve the performance and reliability of the system.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了堆栈的核心层，接下来让我们看看那些提高系统性能和可靠性的关键层。
- en: '![](../Images/CH07_F06_Kimothi.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F06_Kimothi.png)'
- en: Figure 7.6  Core RAGOps stack where data, model, model deployment, and app orchestration
    layers interact with source systems and managed service providers, and co-ordinate
    with the application layer to interface with the user
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 核心RAGOps堆栈，其中数据、模型、模型部署和应用编排层与源系统和托管服务提供商交互，并与应用层协调以与用户接口
- en: 7.1.2 Essential layers
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 必要层
- en: While the critical layers form the core of the stack, they do not evaluate or
    monitor the system. They do not test the prompting strategies or offer any protection
    against the vulnerabilities of LLMs. These layers are essential to the system.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关键层构成了堆栈的核心，但它们并不评估或监控系统。它们不测试提示策略，也不提供针对LLMs（大型语言模型）漏洞的任何保护。这些层对于系统至关重要。
- en: Prompt layer
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示层
- en: While the generation coordination component of the orchestration layer can simply
    put together the user query and the retrieved context, poor prompting can lead
    to hallucinations and subpar results. Proper engineering and evaluation of the
    prompts are vital to guiding the model toward generating relevant, grounded, and
    accurate responses. This process often involves experimentation. Developers create
    prompts, observe the results, and then iterate on the prompts to improve the effectiveness
    of the app. This also requires tracking and collaboration. Azure Prompt Flow,
    Lang­Chain Expression Language (LCEL), Weights & Biases prompts, and PromptLayer
    are among the several applications that can be used to create and manage prompts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然编排层的生成协调组件可以简单地将用户查询和检索到的上下文组合起来，但糟糕的提示可能导致幻觉和低劣的结果。正确工程化和评估提示对于引导模型生成相关、有根据和准确的响应至关重要。这个过程通常涉及实验。开发者创建提示，观察结果，然后迭代提示以提高应用的有效性。这也需要跟踪和协作。Azure
    Prompt Flow、LangChain表达式语言（LCEL）、Weights & Biases提示和PromptLayer是可用于创建和管理提示的几个应用程序之一。
- en: Evaluation layer
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估层
- en: Chapter 5 discussed RAG evaluations at length. Regular evaluation of retrieval
    accuracy, context relevance, faithfulness, and answer relevance of the system
    is necessary to ensure the quality of responses. TruLens by TruEra, Ragas, and
    Weights & Biases are commonly used platforms and frameworks for evaluation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章详细讨论了RAG评估。定期评估检索准确性、上下文相关性、忠实度和答案相关性对于确保响应质量是必要的。TruEra的TruLens、Ragas和Weights
    & Biases是常用的评估平台和框架。
- en: Monitoring layer
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控层
- en: Continuous monitoring ensures the long-term health of the RAG system. Observing
    the execution of the processing chain is essential for understanding system behavior
    and identifying points of failure. Assessing the relevance and adequacy of information
    provided to the language model is also critical. Apart from this, regular system
    metrics tracking such as resource utilization, latency, and error rates form the
    part of the monitoring layer. ARISE, RAGAS, and ARES are evaluation frameworks
    that are also used in monitoring. TraceLoop, TruLens, and Galileo are examples
    of providers that offer monitoring services.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 持续监控确保RAG系统的长期健康。观察处理链的执行对于理解系统行为和识别故障点至关重要。评估提供给语言模型的信息的相关性和充分性也是至关重要的。除此之外，定期跟踪系统指标，如资源利用率、延迟和错误率，也是监控层的一部分。ARISE、RAGAS和ARES是也用于监控的评估框架。TraceLoop、TruLens和Galileo是提供监控服务的提供商的例子。
- en: LLM security and privacy layer
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM安全和隐私层
- en: While security and privacy are features of any software system, in the context
    of RAG, there are additional aspects to this. RAG systems rely on large knowledge
    bases stored in vector databases, which can contain sensitive information. They
    need to follow all data privacy regulations. AI models are susceptible to manipulation
    and poisoning. Prompt injection is a malicious attack via prompts to retrieve
    sensitive information. Data protection strategies such as anonymization, encryption,
    and differential privacy should be employed. Query validation, sanitization, and
    output filtering assist in protection against attacks. Implementing guardrails,
    access controls, monitoring, and auditing are also components of the security
    and privacy layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然安全和隐私是任何软件系统的特性，但在RAG（检索增强生成）的背景下，还有其他方面需要考虑。RAG系统依赖于存储在向量数据库中的大型知识库，这些数据库可能包含敏感信息。它们需要遵守所有数据隐私法规。AI模型容易受到操纵和中毒的影响。提示注入是通过提示检索敏感信息的恶意攻击。应采用数据保护策略，如匿名化、加密和差分隐私。查询验证、清理和输出过滤有助于防止攻击。实施护栏、访问控制、监控和审计也是安全和隐私层的组成部分。
- en: Caching layer
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存层
- en: Caching has become a very important component of any LLM-based application.
    This is because of the high costs and inherent latency of generative AI models.
    With the addition of a retrieval layer, the costs and latency increase further
    in RAG systems. One way to control this increase is to cache responses to frequently
    asked queries. In principle, caching LLM responses is like caching in any other
    software application, but for generative AI apps, it becomes more important.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存已成为任何基于LLM的应用程序的一个重要组成部分。这是因为生成式AI模型的高成本和固有的延迟。在添加检索层后，RAG系统的成本和延迟进一步增加。控制这种增加的一种方法是对常见查询的响应进行缓存。原则上，缓存LLM响应就像在其他任何软件应用程序中缓存一样，但对于生成式AI应用程序来说，它变得更加重要。
- en: These essential layers stacked together with the critical layers create a robust,
    accurate, and high-performing RAG system. Figure 7.7 adds the essential layers
    and their components to the critical RAGOps stack.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本层与关键层叠加在一起，创建了一个强大、准确且高性能的RAG系统。图7.7添加了基本层及其组件到关键RAGOps堆栈中。
- en: '![](../Images/CH07_F07_Kimothi.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F07_Kimothi.png)'
- en: Figure 7.7  Adding essential layers to the critical RAGOps stack lays the path
    to a robust RAG system for user applications.
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 向关键RAGOps堆栈添加基本层为用户应用程序构建了一个强大的RAG系统之路。
- en: Table 7.1 is a recap of the critical and essential layers of the RAGOps stack.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1是RAGOps堆栈中关键和基本层的总结。
- en: Table 7.1 Critical and essential layers of the RAGOps stack
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 RAGOps堆栈的关键和基本层
- en: '| Layer | Category | Description | Example tools |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 类别 | 描述 | 示例工具 |'
- en: '| Data layer | Critical | Responsible for creating and storing the knowledge
    base via ingestion from various sources, transformation into embeddings or graph
    structures, and storing for retrieval | AWS Glue, Apache Kafka, FAISS, Pinecone,
    Neo4j, Weaviate, Milvus |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 数据层 | 关键层 | 负责通过从各种来源摄取、转换为嵌入或图结构以及存储以供检索来创建和存储知识库 | AWS Glue, Apache Kafka,
    FAISS, Pinecone, Neo4j, Weaviate, Milvus |'
- en: '| Model layer | Critical | Contains the models required for generation and
    retrieval in RAG; includes embeddings models for vector generation, LLMs for text
    generation, and models for query classification, hallucination detection, or re-ranking
    | OpenAI, Hugging Face Transformers, Google Gemini, Llama, Anthropic |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 模型层 | 关键层 | 包含RAG中生成和检索所需的模型；包括用于向量生成的嵌入模型、用于文本生成的LLM以及用于查询分类、幻觉检测或重新排序的模型
    | OpenAI, Hugging Face Transformers, Google Gemini, Llama, Anthropic |'
- en: '| Model deployment | Critical | Ensures the models are accessible, performant,
    and scalable; responsible for serving models and optimizing inference for fast
    response times | SageMaker, Vertex AI, NVIDIA Triton, Hugging Face |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 模型部署 | 关键层 | 确保模型可访问、性能良好且可扩展；负责提供模型并优化推理以实现快速响应时间 | SageMaker, Vertex AI,
    NVIDIA Triton, Hugging Face |'
- en: '| Application orchestration layer | Critical | Manages the interaction between
    layers and services, ensures that queries flow through retrieval and generation
    stages, and coordinates retrieval methods and generation tasks | LangChain, Haystack,
    Dagster, Apache Airflow, AutoGen, CrewAI |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 应用编排层 | 关键层 | 管理层与服务之间的交互，确保查询通过检索和生成阶段流动，并协调检索方法和生成任务 | LangChain, Haystack,
    Dagster, Apache Airflow, AutoGen, CrewAI |'
- en: '| Prompt layer | Essential | Designs and maintains the input queries to ensure
    the LLM generates relevant, high-quality outputs; ensures continuous prompt refinement
    to avoid hallucinations and improve accuracy | Weights & Biases Prompts, Azure
    Prompt Flow |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 提示层 | 基本层 | 设计和维护输入查询以确保LLM生成相关、高质量输出；确保持续提示优化以避免幻觉并提高准确性 | Weights & Biases
    Prompts, Azure Prompt Flow |'
- en: '| Evaluation layer | Essential | Evaluates the performance of the retrieval
    and generation stages, ensuring that the outputs are relevant, factual, and accurate.
    | TruLens by TruEra, Ragas, Weights & Biases |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 评估层 | 基本层 | 评估检索和生成阶段的性能，确保输出相关、事实准确。 | TruLens by TruEra, Ragas, Weights
    & Biases |'
- en: '| Monitoring layer | Essential | Continuously monitors the performance, health,
    and resource usage of the RAG system; tracks key metrics such as latency, resource
    consumption, and error rates to ensure system stability. | Prometheus, Grafana,
    TruLens, Galileo |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 监控层 | 基本层 | 持续监控RAG系统的性能、健康和资源使用情况；跟踪关键指标，如延迟、资源消耗和错误率，以确保系统稳定性。 | Prometheus,
    Grafana, TruLens, Galileo |'
- en: '| LLM security & privacy layer | Essential | Ensures that the RAG system adheres
    to data privacy regulations and protects against prompt injection or other forms
    of AI manipulation; implements security strategies such as encryption, access
    control, and guardrails | AWS KMS, Azure Key Vault, Prompt Injection Guards |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LLM安全与隐私层 | 必要 | 确保RAG系统遵守数据隐私法规，并防止提示注入或其他形式的AI操纵；实施加密、访问控制和安全措施等安全策略 |
    AWS KMS, Azure Key Vault, Prompt Injection Guards |'
- en: '| Model training/Fine-tuning layer | Essential | Handles the training and fine-tuning
    of models for specific domains or tasks; fine-tuning models such as embeddings
    or LLMs using domain-specific datasets ensure better performance for specialized
    use cases. | Hugging Face, AWS SageMaker, Google Vertex AI, Azure ML |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 模型训练/微调层 | 必要 | 处理特定领域或任务的模型训练和微调；使用特定领域的数据集对嵌入或LLM等模型进行微调，确保在特定用例中表现更佳 |
    Hugging Face, AWS SageMaker, Google Vertex AI, Azure ML |'
- en: '| Caching layer | Essential | Caching frequently used queries and responses
    to reduce the latency and cost associated with repeated retrieval and generation
    tasks; ensures faster response times for common queries and minimizes resource
    usage for repeated tasks. | Redis, Varnish, ElasticCache |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 缓存层 | 必要 | 缓存频繁使用的查询和响应，以减少重复检索和生成任务相关的延迟和成本；确保常见查询的响应时间更快，并最小化重复任务的资源使用。
    | Redis, Varnish, ElasticCache |'
- en: We will now briefly look at a few enhancement layers, which are not mandatory
    but may be employed to further improve the RAG systems. Note that there can be
    several enhancement layers and that they should be tailored to the use case requirements.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要地看看几个增强层，这些层不是强制性的，但可以进一步改进RAG系统。请注意，可能有多个增强层，并且它们应该根据用例需求定制。
- en: 7.1.3 Enhancement layers
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 增强层
- en: Enhancement layers are the parts of the RAGOps stack that are optional but can
    lead to significant gains, depending on the use case environment. They focus on
    the efficiency, usability, and scalability of the system. Some possible layers
    are described in the following.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 增强层是RAGOps堆栈的可选部分，但根据用例环境，它们可以带来显著收益。它们关注系统的效率、可用性和可扩展性。以下描述了一些可能的层。
- en: Human-in-the-loop layer
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工介入层
- en: This layer provides critical oversight where human judgment is necessary, especially
    for use cases requiring higher accuracy or ethical considerations. It helps reduce
    model hallucinations and bias.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层在需要人类判断的情况下提供关键监督，特别是对于需要更高精度或伦理考虑的用例。它有助于减少模型幻觉和偏差。
- en: Cost optimization layer
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本优化层
- en: RAG systems can become very costly, especially with multiple calls to the LLMs
    for advanced techniques, evaluations, guardrails, and monitoring. This layer helps
    manage resources efficiently, which is particularly important for large-scale
    systems. Optimizing infrastructure can save significant costs but is not critical
    to the system functioning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统可能会变得非常昂贵，特别是当进行多次LLM调用以实现高级技术、评估、安全措施和监控时。这一层有助于高效地管理资源，这对于大规模系统尤为重要。优化基础设施可以节省大量成本，但不是系统功能的关键。
- en: Explainability and interpretability layer
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可解释性和可理解性层
- en: This layer helps provide transparency for system decisions, especially important
    for domains requiring accountability (e.g., legal and healthcare). However, many
    applications can still function without this in nonregulated environments.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层有助于提供系统决策的透明度，对于需要问责制的领域（例如法律和医疗保健）尤为重要。然而，许多应用程序在没有监管的环境下仍然可以正常工作。
- en: Collaboration and experimentation layer
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 协作和实验层
- en: This layer is useful for teams working on development and experimentation but
    noncritical for system operation. This layer enhances productivity and iterative
    improvements. Weights & Biases is a popular platform that helps track experiments.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层对于在开发和实验中工作的团队很有用，但对于系统运行来说不是关键。这一层提高了生产力和迭代改进。Weights & Biases是一个流行的平台，它有助于跟踪实验。
- en: These enhancement layers should be chosen depending on the application requirements.
    There may be other layers that you may deem fit for your use case.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 应根据应用需求选择增强层。可能还有其他适合您用例的层。
- en: Managed RAG solutions
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 管理RAG解决方案
- en: Building a RAG system can be complex if you don’t have prior knowledge, budget,
    or time. To address these challenges, service providers offer managed RAG solutions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有先前的知识、预算或时间，构建RAG系统可能会很复杂。为了解决这些挑战，服务提供商提供管理RAG解决方案。
- en: OpenAI offers the File Search tool that automatically parses and chunks your
    documents, creates and stores the embeddings, and uses both vector and keyword
    search to retrieve relevant content to answer user queries. AWS offers Amazon
    Bedrock Knowledge Bases, which is fully managed support for end-to-end RAG workflow.
    Azure AI, such as OpenAI file search, provides indexing and querying. Anthropic
    offers Claude projects where users can upload documents and provide context to
    have focused chats.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供的文件搜索工具可以自动解析和分块您的文档，创建并存储嵌入，并使用向量和关键词搜索来检索相关内容以回答用户查询。AWS 提供的 Amazon
    Bedrock 知识库，为端到端 RAG 工作流程提供全面管理的支持。Azure AI，例如 OpenAI 文件搜索，提供索引和查询。Anthropic 提供的
    Claude 项目允许用户上传文档并提供上下文以进行专注的对话。
- en: Several other providers offer RAG as a service and can handle video and audio
    transcription, image content extraction, and document parsing. For quick and easy
    deployment of a RAG solution, managed service providers can be considered.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 几家其他提供商提供 RAG 作为服务，并能处理视频和音频转录、图像内容提取和文档解析。为了快速轻松地部署 RAG 解决方案，可以考虑使用托管服务提供商。
- en: We have also discussed several service providers, tools, and technologies that
    you can use in the development of RAG systems. The choice of these tools and technologies
    may depend on factors such as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了几种服务提供商、工具和技术，您可以在开发 RAG 系统时使用。这些工具和技术的选择可能取决于诸如
- en: '*Scalability and performance require**d*—RAG systems need to handle large volumes
    of data efficiently, while maintaining low latency. As data scales or traffic
    spikes, the system must remain performant to ensure fast response times. Choose
    cloud platforms that allow for auto-scaling and variable loads. For high-performance
    and scalable retrieval, choose the vector databases that can handle millions of
    embeddings with low-latency search capabilities. Use inference optimization tools
    to help reduce latency during the generation phase.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性和性能需求*—RAG 系统需要高效地处理大量数据，同时保持低延迟。随着数据规模的扩大或流量激增，系统必须保持高性能以确保快速响应时间。选择允许自动扩展和可变负载的云平台。对于高性能和可扩展的检索，选择能够处理数百万嵌入并具有低延迟搜索能力的向量数据库。使用推理优化工具可以帮助在生成阶段降低延迟。'
- en: '*Integration with existing stac**k*—Seamless integration with your current
    technology stack minimizes disruption and reduces complexity. If your system already
    operates on AWS, GCP, or Azure, using services that integrate well with these
    platforms can streamline development and maintenance. Choosing tools that natively
    integrate with your cloud provider, offer strong API support, and ensure that
    the chosen frameworks support these tools can be highly beneficial.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与现有堆栈的集成*—与您当前技术堆栈的无缝集成可以最小化中断并降低复杂性。如果您的系统已经在 AWS、GCP 或 Azure 上运行，使用与这些平台集成良好的服务可以简化开发和维护。选择与云提供商原生集成、提供强大
    API 支持、并确保所选框架支持这些工具的工具可以非常有益。'
- en: '*Cost efficienc**y*—LLMs require much more resources than traditional ML models.
    Costs, even with pay-as-you-go models, can escalate quickly with scale. Caching
    and inference optimization can help manage the costs.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本效益*—与传统的机器学习模型相比，大型语言模型（LLMs）需要更多的资源。即使使用按需付费的模型，成本也会随着规模的扩大而迅速增加。缓存和推理优化可以帮助管理成本。'
- en: '*Domain adaptatio**n*—RAG systems often need to be adapted to specific industries
    or domains (e.g., healthcare and legal). Pre-trained models might not be fully
    effective for specific use cases unless fine-tuned with domain-specific data.
    For domain adaptation, models that can be easily fine-tuned should be chosen.
    Existing domain-specific models can also be considered.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域适应性*—RAG 系统通常需要适应特定行业或领域（例如医疗保健和法律）。除非使用特定领域的数据进行微调，否则预训练模型可能对特定用例并不完全有效。对于领域适应性，应选择易于微调的模型。也可以考虑现有的特定领域模型。'
- en: '*Vendor lock-in constraint**s*—Since generative AI is an evolving field, using
    proprietary tools or services from a single vendor may lead to vendor lock-in,
    making it difficult to migrate to other platforms or adjust your stack as requirements
    change. Using open source or interoperable technologies where possible helps in
    maintaining flexibility. Choosing tools that are cloud-agnostic or support multi-cloud
    deployments to reduce dependency on a single vendor. A modular architecture is
    advised to swap components without a system redesign.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*供应商锁定约束**—由于生成式AI是一个不断发展的领域，使用单一供应商的专有工具或服务可能会导致供应商锁定，使得迁移到其他平台或根据需求变化调整你的技术栈变得困难。尽可能使用开源或互操作技术有助于保持灵活性。选择云无关或支持多云部署的工具，以减少对单一供应商的依赖。建议采用模块化架构，以便在不重新设计系统的情况下更换组件。'
- en: '*Community suppor**t*—Strong community support means access to resources, tutorials,
    troubleshooting, and regular updates, which can accelerate development and reduce
    debugging time. This is especially true for rapidly evolving fields such as LLMs
    and RAG. Tools with active communities such as Hugging Face, LangChain, and similar
    are more likely to offer frequent updates, plugins, and third-party integrations.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社区支持**—强大的社区支持意味着可以访问资源、教程、故障排除和定期更新，这可以加速开发并减少调试时间。这对于LLMs和RAG等快速发展的领域尤其如此。拥有活跃社区的工具有助于提供频繁的更新、插件和第三方集成，如Hugging
    Face、LangChain等。'
- en: With the knowledge of the critical, essential, and enhancement layers, you should
    be ready to put together a technology stack to build your RAG system. Let’s now
    look at some common pitfalls and best practices to consider when building and
    deploying production-grade RAG system.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解关键、基本和增强层知识的基础上，你应该准备好搭建一个技术栈来构建你的RAG系统。现在让我们看看在构建和部署生产级RAG系统时需要考虑的一些常见陷阱和最佳实践。
- en: 7.2 Production best practices
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 生产最佳实践
- en: 'Despite earnest efforts in designing and planning the RAG system, some problems
    will inevitably creep up during development and deployment. Although RAG is still
    in its nascent form, some early trends of common mishaps and best practices have
    emerged. There have been many experiments and learnings derived from them to make
    RAG systems work. This section discusses five such practices:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在设计规划RAG系统时付出了诚挚的努力，但在开发和部署过程中仍不可避免地会出现一些问题。尽管RAG仍处于起步阶段，但一些常见的失误和最佳实践的趋势已经显现。已有许多实验和经验从中得出，以使RAG系统有效运作。本节讨论了五种这样的实践：
- en: '*Latency of the system*—RAG systems can introduce latency due to the need for
    multiple steps: retrieval, reranking, and generation. High latency can significantly
    degrade user experience, especially in real-time applications like chatbots or
    interactive search engines, which happens because each component adds processing
    time. Effective classification and routing of the queries can help in optimizing
    latency. A filtering approach is useful in hybrid retrieval, which first filters
    the embeddings based on keywords or sparse retrieval techniques and then uses
    similarity search on the filtered results. This reduces the time taken to calculate
    similarity, especially in large knowledge bases.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统的延迟**—RAG系统可能由于需要多个步骤：检索、重新排序和生成而引入延迟。高延迟会显著降低用户体验，尤其是在聊天机器人或交互式搜索引擎等实时应用中，这是因为每个组件都会增加处理时间。有效的查询分类和路由可以帮助优化延迟。在混合检索中，一种有用的方法是首先根据关键词或稀疏检索技术过滤嵌入，然后对过滤后的结果进行相似度搜索。这减少了计算相似度所需的时间，尤其是在大型知识库中。'
- en: '*Continued hallucinatio**n*—Despite best efforts, LLMs may continue to generate
    responses that are factually incorrect or irrelevant to the retrieved content.
    This may happen if the retrieved data is ambiguous or incomplete. Post-processing
    validation steps may be required to address these. A common approach is to make
    RAG systems recommendation oriented rather than action oriented. This means that
    a human is looped into the system for verification and final action.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续的幻觉**—尽管付出了最大努力，LLMs仍可能继续生成事实错误或不相关于检索内容的响应。如果检索的数据模糊或不完整，这种情况可能会发生。可能需要后处理验证步骤来解决这些问题。一种常见的方法是将RAG系统调整为以推荐为导向，而不是以行动为导向。这意味着系统会循环一个人类进行验证和最终行动。'
- en: '*Insufficient scalability planning*—Early prototypes of RAG systems often work
    well on small datasets but can struggle as the volume of data or the number of
    concurrent users grows. Managed vector database services with autoscaling features
    can be an easier way to plan for growth in demand and computation requirements.
    Similarly, autoscaling can also be used for the overall application using cloud-native
    solutions such as AWS Lambda.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏可扩展性规划*—RAG系统的早期原型在小数据集上通常表现良好，但随着数据量或并发用户数量的增长可能会遇到困难。具有自动扩展功能的托管向量数据库服务可以更容易地规划需求增长和计算需求。同样，自动扩展也可以用于整体应用程序，使用云原生解决方案，如AWS
    Lambda。'
- en: '*Domain-adaptation challenge**s*—The embeddings and language models may not
    work well in niche or specialized domains. Also, the retrieval model and the language
    model may not always complement each other well, leading to disjointed or incoherent
    results. Retrieval models and LLMs are often developed and fine-tuned independently,
    which can cause a mismatch between the content retrieved and the way the LLM generates
    responses. It becomes important to fine-tune both the retrieval and generation
    models together for highly specialized domains.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域适应性挑战*—嵌入和语言模型可能在细分或特定领域工作不佳。此外，检索模型和语言模型可能并不总是很好地互补，导致结果不连贯或混乱。检索模型和大型语言模型通常独立开发和微调，这可能导致检索的内容与大型语言模型生成响应的方式不匹配。对于高度专业化的领域，同时微调检索和生成模型变得非常重要。'
- en: '*Inadequate handling of data privacy and PII*—Pre-trained models may generate
    content that includes sensitive information (e.g., personal data and confidential
    details) due to biases in training data. RAG systems may inadvertently leak sensitive
    information or personally identifiable information (PII) in their responses, leading
    to privacy breaches. Data exfiltration, also known as data theft, extrusion, or
    exportation, is a major threat in the digital world. The solution is to use PII
    masking and data redaction during both the pre- and post-processing stages. Ensure
    compliance with privacy regulations such as GDPR or HIPAA and deploy models with
    privacy filters.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私和PII处理不足*—由于训练数据中的偏差，预训练模型可能会生成包含敏感信息（例如，个人数据和机密细节）的内容。RAG系统可能在响应中无意中泄露敏感信息或个人身份信息（PII），导致隐私泄露。数据泄露，也称为数据盗窃、外泄或出口，是数字世界中的主要威胁。解决方案是在预处理和后处理阶段使用PII掩码和数据编辑。确保遵守GDPR或HIPAA等隐私法规，并部署带有隐私过滤器的模型。'
- en: The list of best practices continues to evolve. Latency and scalability are
    critical for managing user experience and access. The promise of hallucination-free
    generation and data safety needs to be maintained for the reliability of the system.
    Table 7.2 summarizes the challenges of and potential solutions to putting RAG
    systems into production.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践列表持续演变。延迟和可扩展性对于管理用户体验和访问至关重要。确保无幻觉生成和数据安全对于系统的可靠性至关重要。表7.2总结了将RAG系统投入生产的挑战和潜在解决方案。
- en: Table 7.2 Production challenges and potential solutions
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2 生产挑战和潜在解决方案
- en: '| Challenge | Description | Solution |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 描述 | 解决方案 |'
- en: '| Latency of the system | RAG systems add latency due to retrieval, re-ranking,
    and generation steps, affecting real-time performance. | Use query classification,
    hybrid retrieval filtering, and limit similarity searches |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 系统延迟 | RAG系统由于检索、重新排序和生成步骤而增加延迟，影响实时性能。 | 使用查询分类、混合检索过滤和限制相似度搜索 |'
- en: '| Continued hallucination | LLMs may generate incorrect or irrelevant responses
    due to ambiguous or incomplete data. | Add post-processing validation and make
    systems recommendation-based with human verification. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 持续的幻觉 | 由于数据模糊或不完整，大型语言模型可能会生成错误或不相关的响应。 | 添加后处理验证，并使系统基于人类验证的建议。 |'
- en: '| Insufficient scalability planning | Early RAG systems struggle with scalability
    as data and user load grow. | Use autoscaling vector databases and cloud solutions
    such as AWS Lambda. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 缺乏可扩展性规划 | 早期的RAG系统随着数据和用户负载的增长而难以扩展。 | 使用自动扩展的向量数据库和云解决方案，如AWS Lambda。 |'
- en: '| Domain-adaptation challenges | Embeddings and LLMs may perform poorly in
    specialized domains, leading to incoherent results. | Fine-tune both retrieval
    and generation models for niche use cases. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 领域适应性挑战 | 嵌入和大型语言模型在特定领域可能表现不佳，导致结果不连贯。 | 对特定用例微调检索和生成模型。 |'
- en: '| Inadequate handling of data privacy and PII | Models may expose sensitive
    data or PII, leading to privacy issues. | Apply PII masking, data redaction, and
    privacy filters, ensuring compliance with regulations. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私和PII处理不当 | 模型可能暴露敏感数据或PII，导致隐私问题。 | 应用PII掩码、数据编辑和隐私过滤器，确保符合法规。 |'
- en: In this chapter, we have looked at a holistic RAGOps stack that enables the
    building of production-grade RAG systems. You also learned about some commonly
    available tools and technologies, along with a few best practices. This brings
    us to a close in our discussion of the RAGOps stack. We have now completed part
    3 of the book, which means you should be ready to build RAG systems and put them
    into production. In the last part of this book, we discuss some emerging patterns
    in RAG-like multimodal capabilities, agentic RAG, and graphRAG, along with closing
    comments on future directions and continued learning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了整体RAGOps堆栈，该堆栈使构建生产级RAG系统成为可能。你还了解了一些常用的工具和技术，以及一些最佳实践。这标志着我们对RAGOps堆栈的讨论结束。我们现在完成了本书的第三部分，这意味着你应该准备好构建RAG系统并将其投入生产。本书的最后一部分，我们将讨论RAG-like多模态能力的一些新兴模式，包括代理RAG和graphRAG，以及对未来方向和持续学习的总结性评论。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: RAGOps stack is a layered approach to designing a RAG system.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGOps堆栈是设计RAG系统的一种分层方法。
- en: These layers are categorized into critical, essential, and enhancement layers.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些层被分为关键、基本和增强层。
- en: Critical layers are fundamental for operation; essential layers ensure performance
    and reliability; and enhancement layers improve efficiency, scalability, and usability.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键层对于操作至关重要；基本层确保性能和可靠性；增强层提高效率、可扩展性和可用性。
- en: Critical layers
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键层
- en: '*Data laye**r*—Responsible for collecting, transforming, and storing the knowledge
    base. Ingestion tools such as AWS Glue, Azure Data Factory, and Apache Kafka enable
    data collection. Data transformation includes chunking, metadata enrichment, and
    converting data into vector formats. Tools such as FAISS, Pinecone, and Neo4j
    are used for storing embeddings and graph data.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据层*—负责收集、转换和存储知识库。AWS Glue、Azure Data Factory和Apache Kafka等工具可进行数据收集。数据转换包括分块、元数据丰富和将数据转换为向量格式。FAISS、Pinecone和Neo4j等工具用于存储嵌入和图数据。'
- en: '*Model laye**r*—Includes embeddings models and LLMs for generation. Embeddings
    models transform the text into vectors, with options from OpenAI, Google, Cohere,
    and Hugging Face. Foundation models (LLMs) such as GPT, Claude, and Llama generate
    outputs and evaluate tasks. Task-specific models handle specialized tasks such
    as query classification and bias detection.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型层*—包括用于生成的嵌入模型和LLM。嵌入模型将文本转换为向量，选项包括OpenAI、Google、Cohere和Hugging Face。基础模型（LLM）如GPT、Claude和Llama生成输出并评估任务。特定任务的模型处理专门的任务，如查询分类和偏差检测。'
- en: '*Model deploymen**t*—Manages hosting and serving of LLMs and embeddings models.
    Popular platforms include AWS SageMaker, Google Vertex, and Hugging Face. Inference
    optimization reduces response time and costs with methods such as quantization
    and batching.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署*—管理LLM和嵌入模型的托管和提供。流行的平台包括AWS SageMaker、Google Vertex和Hugging Face。推理优化通过量化、批处理等方法减少响应时间和成本。'
- en: '*Application orchestration laye**r*—Coordinates data flow between different
    components:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用编排层*—协调不同组件之间的数据流：'
- en: Query orchestration handles query classification and optimization.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询编排处理查询分类和优化。
- en: Retrieval coordination manages retrieval methods like dense or hybrid search.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索协调管理密集或混合搜索等检索方法。
- en: Generation coordination handles prompt generation and post-retrieval tasks such
    as re-ranking.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成协调处理提示生成和检索后的任务，如重新排序。
- en: Essential layers
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本层
- en: '*Prompt layer*—Ensures prompts are well-engineered to guide LLMs for relevant,
    accurate responses. Tools such as LangChain and Azure Prompt Flow assist in prompt
    management.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示层*—确保提示被精心设计，以引导LLM产生相关、准确的响应。LangChain和Azure Prompt Flow等工具协助提示管理。'
- en: '*Evaluation laye**r*—Monitors system performance by evaluating retrieval accuracy,
    faithfulness, and context relevance. Tools such as TruLens and Ragas provide evaluation
    frameworks.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估层*—通过评估检索准确性、忠实度和上下文相关性来监控系统性能。TruLens和Ragas等工具提供评估框架。'
- en: '*Monitoring layer*—Tracks system health, resource usage, and latency. Platforms
    such as TraceLoop and Galileo provide monitoring services.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控层*—跟踪系统健康、资源使用和延迟。TraceLoop和Galileo等平台提供监控服务。'
- en: '*LLM security and privacy laye**r*—Protects against data breaches and prompt
    injection attacks. Tools such as encryption, anonymization, and differential privacy
    should be used to safeguard sensitive data.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM安全和隐私层*—保护免受数据泄露和提示注入攻击。应使用加密、匿名化和差分隐私等工具来保护敏感数据。'
- en: '*Caching layer*—Caches frequently generated responses to reduce costs and latency
    in RAG systems.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缓存层*—缓存频繁生成的响应，以减少RAG系统中的成本和延迟。'
- en: Enhancement layers
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强层
- en: '*Human-in-the-loop laye**r*—Adds human oversight to ensure higher accuracy
    and ethical decision-making.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工介入层*—增加人工监督以确保更高的准确性和道德决策。'
- en: '*Cost optimization laye**r*—Reduces infrastructure costs, especially in large-scale
    RAG systems.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本优化层*—降低基础设施成本，尤其是在大规模RAG系统中。'
- en: '*Explainability and interpretability laye**r*—Provides transparency into system
    decisions, critical for domains such as healthcare and legal.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性和可理解性层*—提供对系统决策的透明度，这对于医疗保健和法律等领域至关重要。'
- en: '*Collaboration and experimentation laye**r*—Useful for team-based development
    and continuous improvement.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*协作和实验层*—对基于团队的开发和持续改进很有用。'
- en: Production best practices
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产最佳实践
- en: '*Latenc**y*—RAG systems often introduce latency due to multiple steps. Using
    techniques such as filtering in hybrid retrieval can help reduce response times.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟*—RAG系统由于多个步骤常常引入延迟。使用如混合检索中的过滤等技巧可以帮助减少响应时间。'
- en: '*Hallucinatio**n*—LLMs may still generate incorrect responses. Post-processing
    validation and human-in-the-loop systems help mitigate this.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*幻觉*—LLMs可能仍然会生成不正确的响应。后处理验证和人工介入系统有助于减轻这一问题。'
- en: '*Scalabilit**y*—Early prototypes may struggle to scale. Managed vector database
    services with autoscaling can help plan for growth.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*—早期原型可能难以扩展。具有自动扩展功能的托管向量数据库服务可以帮助规划增长。'
- en: '*Domain adaptatio**n*—Embeddings and language models may not perform well in
    niche domains. Fine-tuning both retrieval and generation models is necessary.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域适应性*—嵌入和语言模型可能在利基领域表现不佳。对检索和生成模型进行微调是必要的。'
- en: '*Data privac**y*—Models may leak sensitive information. PII masking, encryption,
    and compliance with data regulations are essential for protecting user data.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私*—模型可能泄露敏感信息。PII掩码、加密和遵守数据法规对于保护用户数据至关重要。'
