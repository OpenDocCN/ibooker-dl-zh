- en: 2 Introduction to large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型简介
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An overview of LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs概述
- en: Key use cases powered by LLMs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由LLMs驱动的关键用例
- en: Foundational models and their effect on AI development
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型及其对AI发展的影响
- en: New architecture concepts for LLMs, such as prompts, prompt engineering, embeddings,
    tokens, model parameters, context window, and emergent behavior
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）的新架构概念，例如提示（prompts）、提示工程（prompt engineering）、嵌入（embeddings）、标记（tokens）、模型参数（model
    parameters）、上下文窗口（context window）和涌现行为（emergent behavior）
- en: An overview of small language models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型语言模型概述
- en: Comparison of open source and commercial LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源和商业LLMs的比较
- en: Large language models (LLMs) are generative AI models that can understand and
    generate human-like text based on a given input. LLMs are the foundation of many
    natural language processing (NLP) tasks, such as search, speech-to-text, sentiment
    analysis, text summarization, and more. In addition, they are general-purpose
    language models that are pretrained and can be fine-tuned for specific tasks and
    purposes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是基于给定输入能够理解和生成类似人类文本的生成式AI模型。LLMs是许多自然语言处理（NLP）任务的基础，如搜索、语音转文本、情感分析、文本摘要等。此外，它们是通用语言模型，经过预训练，可以针对特定任务和目的进行微调。
- en: This chapter explores the fascinating world of LLMs and their transformative
    effect on artificial intelligence (AI). As a significant advancement in AI, LLMs
    have demonstrated remarkable capabilities in understanding and generating human-like
    text, thus enabling numerous applications across various industries. Here, we
    dive into the critical use cases of LLMs, the different types of LLMs, and the
    concept of foundational models that has revolutionized AI development.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了LLMs的迷人世界及其对人工智能（AI）的变革性影响。作为AI领域的一项重大进步，LLMs在理解和生成类似人类文本方面表现出惊人的能力，从而使得LLMs在各个行业得到广泛应用。在这里，我们深入探讨LLMs的关键用例、不同类型的LLMs以及颠覆了AI发展的基础模型概念。
- en: The chapter discusses essential LLM concepts, such as prompts, prompt engineering,
    embeddings, tokens, model parameters, context windows, transformer architecture,
    and emergent behavior. Finally, we compare open source and commercial LLMs, highlighting
    their advantages and disadvantages. By the end of this chapter, you will have
    a comprehensive understanding of LLMs and their implications for AI applications
    and research. LLMs are built on foundational models; therefore, we will start
    by outlining what these models are before discussing LLMs in more depth.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了LLMs的基本概念，如提示、提示工程、嵌入、标记、模型参数、上下文窗口、Transformer架构和涌现行为。最后，我们比较了开源和商业LLMs，突出了它们的优缺点。在本章结束时，您将全面了解LLMs及其对AI应用和研究的影响。LLMs建立在基础模型之上；因此，在更深入地讨论LLMs之前，我们将首先概述这些模型是什么。
- en: 2.1 Overview of foundational models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 基础模型概述
- en: Introduced by Stanford researchers in 2021, foundational models have substantially
    transformed the construction of AI systems. They diverge from task-specific models,
    shifting to broader, more adaptable models trained on large data volumes. These
    models can excel in diverse natural language tasks, such as machine translation
    and question answering, as they learn general language representations from extensive
    text and code datasets. These representations can then be used to perform various
    tasks, even tasks they were not explicitly trained on, as shown in figure 2.1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由斯坦福研究人员于2021年提出的基础模型在很大程度上改变了AI系统的构建方式。它们不同于特定任务的模型，转向了更广泛、更适应性的模型，这些模型在大数据量上训练。这些模型在多种自然语言任务上表现出色，如机器翻译和问答，因为它们从大量的文本和代码数据集中学习通用的语言表示。这些表示可以用于执行各种任务，甚至包括它们没有明确训练过的任务，如图2.1所示。
- en: In more technical terms, foundational models utilize established machine learning
    techniques such as self-supervised learning and transfer learning, enabling them
    to apply acquired knowledge across various tasks. Developed by means of deep learning,
    these models employ multilayered artificial neural networks to comprehend complex
    data patterns; hence, their proficiency with unstructured data such as images,
    audio, and text. This also extends to 3D signals—data representing 3D attributes
    that capture spatial dimensions and depth, such as 3D point clouds from LiDAR
    sensors, 3D medical imaging such as CT scans, or 3D models used in computer graphics
    and simulations. These can be utilized to make predictions based on 3D data for
    tasks such as object recognition, scene understanding, and navigation in robotics
    and autonomous vehicles.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在更技术性的术语中，基础模型利用了已建立的机器学习技术，如自监督学习和迁移学习，使它们能够将获得的知识应用于各种任务。通过深度学习开发，这些模型采用多层人工神经网络来理解复杂的数据模式；因此，它们在处理非结构化数据方面的能力，如图像、音频和文本。这也扩展到
    3D 信号——代表 3D 属性的数据，如捕获空间维度和深度的激光雷达传感器的 3D 点云、3D 医学成像（如 CT 扫描）或用于计算机图形和模拟的 3D 模型。这些可以用于基于
    3D 数据进行预测，例如物体识别、场景理解和机器人以及自动驾驶车辆的导航。
- en: Note  Transfer learning is a machine learning technique in which a model developed
    for one task is reused as a starting point for a similar task. Instead of starting
    from scratch, we use the knowledge from the previous task to perform better on
    the new one. It’s like using knowledge from a previous job to excel at a new but
    related job.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：迁移学习是一种机器学习技术，其中为某一任务开发的模型被重新用作类似任务的起点。我们不是从头开始，而是使用先前任务的知识来在新任务上表现更好。这就像使用以前工作的知识来在新但相关的岗位上表现出色。
- en: Generative AI and foundational models are closely interlinked. As outlined,
    foundational models, trained on massive datasets, can be adapted to perform various
    tasks; this property makes them particularly suitable for generative AI and allows
    for creating new content. The broad knowledge base of these models allows for
    effective transfer learning, which can be used to generate new, contextually appropriate
    content across diverse domains. They represent a unified approach, where a single
    model can generate various outputs, offering state-of-the-art performance owing
    to their extensive training. Without foundational models as the backbone, there
    would be no generative AI models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 和基础模型紧密相连。如前所述，在大量数据集上训练的基础模型可以适应执行各种任务；这种特性使它们特别适合于生成式 AI，并允许创建新内容。这些模型广泛的知识库允许有效的迁移学习，这可以用于在多个领域生成新的、上下文适当的内容。它们代表了一种统一的方法，其中单个模型可以生成各种输出，由于广泛的训练，它们提供了最先进的性能。如果没有基础模型作为骨干，就不会有生成式
    AI 模型。
- en: '![figure](../Images/CH02_F01_Bahree.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F01_Bahree.png)'
- en: Figure 2.1 Foundational model overview
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.1 基础模型概述
- en: 'Here are some examples of the common foundation models:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些常见的基础模型示例：
- en: GPT (Generative Pre-trained Transformer) Family is an NLP family of models developed
    by OpenAI. It is a large language model trained on a massive dataset of text and
    code, which makes it capable of generating text, translating languages, writing
    creative content, and answering your questions informatively. GPT-4, the latest
    version at the time of this writing, is also a multimodal model—it can manage
    both language and images.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT（生成预训练转换器）家族是由 OpenAI 开发的 NLP 模型家族。它是一个在大量文本和代码数据集上训练的大型语言模型，这使得它能够生成文本、翻译语言、创作内容以及提供信息性的回答。GPT-4，在撰写本文时的最新版本，也是一个多模态模型——它既能处理语言也能处理图像。
- en: Codex is a large language model trained specifically on code that is used to
    help with code generation. It supports over a dozen programming languages, including
    some of the more commonly used, such as C#, Java, Python, JavaScript, SQL, Go,
    PHP, and Shell, among others.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codex 是一个专门针对代码进行训练的大型语言模型，用于帮助代码生成。它支持超过十种编程语言，包括一些常用的，如 C#、Java、Python、JavaScript、SQL、Go、PHP
    和 Shell 等。
- en: Claude is an LLM built by a startup called Anthropic. Like OpenAI’s ChatGPT,
    it predicts the next token in a sequence when given a certain prompt and can generate
    text, write code, summarize, and reason.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claude 是由一家名为 Anthropic 的初创公司构建的 LLM。与 OpenAI 的 ChatGPT 类似，当给出一定的提示时，它可以预测序列中的下一个标记，并可以生成文本、编写代码、总结和推理。
- en: BERT (Bidirectional Encoder Representations from Transformers) is an NLP model
    developed by Google. It is a bidirectional model, meaning it can process text
    in both directions, from left to right and right to left. This feature makes it
    better at understanding the context of words and phrases.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT（来自Transformer的双向编码器表示）是由谷歌开发的一种自然语言处理模型。它是一个双向模型，这意味着它可以从左到右和从右到左处理文本。这一特性使得它更擅长理解单词和短语的上下文。
- en: PaLM (Pathway Language Model) and its successor PaLM2 are large multimodal language
    models developed by Google. The multimodal model can process text, code, and images
    simultaneously, making it capable of performing a wider range of tasks across
    those modalities compared to traditional language models operating only in one
    modality.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM（路径语言模型）及其继任者PaLM2是由谷歌开发的大型多模态语言模型。多模态模型可以同时处理文本、代码和图像，使其能够在这些模态中执行比仅在一个模态中操作的传统语言模型更广泛的任务。
- en: Gemini is Google’s latest AI model, capable of understanding text, images, videos,
    and audio. It’s a multimodal model described as being able to complete complex
    tasks in math, physics, and other areas, as well as understanding and generating
    high-quality code in various programming languages. Gemini was built from the
    ground up to be multimodal, meaning it can generalize and seamlessly understand,
    operate across, and combine different types of information. It’s also the new
    umbrella name for all of Google’s AI tools, replacing Google Bard and Duet AI,
    and is considered a successor to the PaLM model.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini是谷歌最新的AI模型，能够理解文本、图像、视频和音频。它是一个多模态模型，描述为能够在数学、物理和其他领域完成复杂任务，以及理解和生成各种编程语言中的高质量代码。Gemini是从头开始构建的多模态模型，这意味着它可以泛化并无缝地理解、操作和结合不同类型的信息。它也是谷歌所有AI工具的新总称，取代了Google
    Bard和Duet AI，并被认为是对PaLM模型的继承。
- en: Once a foundational model is trained, it can be adapted to a wide range of downstream
    tasks by fine-tuning its parameters. Fine-tuning involves adjusting the model’s
    parameters to optimize the model for a specific task. It can be done using a small
    amount of labeled data. By fine-tuning these models for specific tasks or domains,
    we use their general understanding of language and supplement it with task-specific
    knowledge. The benefits of this approach include time and resource efficiency,
    coupled with remarkable versatility. We can also adapt a model via Prompt engineering,
    which we’ll discuss later in this chapter. Now that we know more about foundational
    models, let’s explore LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了基础模型，就可以通过微调其参数来将其适应广泛的下游任务。微调涉及调整模型的参数以优化模型以特定任务。这可以使用少量标记数据进行。通过为特定任务或领域微调这些模型，我们利用它们对语言的普遍理解，并补充以特定任务的知识。这种方法的好处包括时间和资源效率，以及显著的灵活性。我们还可以通过提示工程来调整模型，我们将在本章后面讨论。现在我们更了解基础模型，让我们来探索LLMs。
- en: 2.2 Overview of LLMs
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 LLMs概述
- en: LLMs represent a significant advancement in AI. They are trained on a vast amount
    of text data, such as books, articles, and websites, to learn patterns in human
    language. They are also hard to develop and maintain, as they require lots of
    data, computing, and engineering resources. OpenAI’s ChatGPT is an example of
    an LLM—it generates human-like text by predicting the probability of a word considering
    the words already used in the text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs代表了人工智能的重大进步。它们在大量文本数据上接受训练，如书籍、文章和网站，以学习人类语言的模式。它们也难以开发和维护，因为它们需要大量的数据、计算和工程资源。OpenAI的ChatGPT是LLM的一个例子——它通过预测文本中已使用的单词的概率来生成类似人类的文本。
- en: The model learns to generate coherent and contextually relevant sentences by
    adjusting its internal parameters to minimize the difference between its predictions
    and the actual outcomes in the training data. When generating text, the model
    chooses the word with the highest probability as its subsequent output and then
    repeats the process for the next word.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过调整其内部参数以最小化预测与训练数据中实际结果之间的差异，从而学会生成连贯且与上下文相关的句子。在生成文本时，模型会选择概率最高的单词作为其后续输出，然后对下一个单词重复此过程。
- en: LLMs are foundational models adapted for natural language processing and language
    generation tasks. These LLMs are general-purpose and can handle tasks without
    task-specific training data. As briefly described in the previous chapter, given
    the right prompt, they can answer questions, write essays, summarize texts, translate
    languages, and even generate code. LLMs can be applied to many applications across
    different industries, as outlined in chapter 1—from summarization to classification,
    Q&A chatbots, content generation, data analysis, entity extraction, and more.
    Before we get into more details of LLMs, let us look at the Transformer architecture,
    which makes these foundational models possible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是为自然语言处理和语言生成任务而改编的基础模型。这些LLMs是通用的，可以处理没有特定任务训练数据的任务。正如前一章简要描述的，给定正确的提示，它们可以回答问题、撰写文章、总结文本、翻译语言，甚至生成代码。LLMs可以应用于不同行业中的许多应用，如第1章概述的——从摘要到分类、问答聊天机器人、内容生成、数据分析、实体提取等。在我们深入了解LLMs之前，让我们看看Transformer架构，这是这些基础模型成为可能的原因。
- en: 2.3 Transformer architecture
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 Transformer架构
- en: Transformers are the bedrock of foundational models and are responsible for
    their remarkable language understanding capabilities. The Transformer model was
    first introduced in the paper “Attention Is All You Need” by Vaswani et al. in
    2017 [1]. Since then, Transformer-based models have become state-of-the-art for
    many tasks. GPT and BERT are examples of Transformer-based models, and the “T”
    in GPT stands for Transformers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是基础模型的基础，并负责其卓越的语言理解能力。Transformer模型首次在2017年由Vaswani等人撰写的论文“Attention Is
    All You Need”中提出[1]。从那时起，基于Transformer的模型在许多任务中已成为最先进的模型。GPT和BERT是基于Transformer的模型的例子，GPT中的“T”代表Transformer。
- en: At their core, Transformers use a mechanism known as attention (specifically
    self-attention), which allows the model to consider the entire context of a sentence,
    considering all words simultaneously rather than processing the sentence word
    by word. This approach is more efficient and can improve the results of many NLP
    tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Transformer使用一种称为注意力（特别是自注意力）的机制，这使得模型能够考虑句子的整个上下文，同时考虑所有单词，而不是逐词处理句子。这种方法更高效，可以提高许多NLP任务的结果。
- en: The strength of this approach is that it captures dependencies regardless of
    their position in the text, which is an essential factor in language understanding.
    This is key for tasks such as machine translation and text summarization, where
    the meaning of a sentence can depend on terms that are several words apart.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它捕捉了无论它们在文本中的位置如何的依赖关系，这是语言理解中的一个关键因素。这对于机器翻译和文本摘要等任务至关重要，在这些任务中，一个句子的意义可能取决于相隔几个单词的术语。
- en: Transformers can parallelize their computations, which makes them much faster
    to train than other types of neural networks. This mechanism enables the model
    to pay attention to the most relevant parts of the task input.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器可以并行化其计算，这使得它们比其他类型的神经网络训练得更快。这种机制使得模型能够关注任务输入中最相关的部分。
- en: In the context of generative AI, a transformer model would take an input (such
    as a prompt) and generate an output (such as the next word or the completion of
    the sentence) by weighing the importance of each part of the input in generating
    the output. For example, in the sentence “The cat sat on the...,” a Transformer
    model would likely give much weight to the word “cat” when determining that the
    likely next word might be “mat.” These models exhibit generative properties by
    predicting the next item in a sequence—the next word in a sentence or the next
    note in a melody. We explore this more in the next chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI的背景下，一个Transformer模型会通过权衡输入的每个部分在生成输出中的重要性，接收一个输入（例如一个提示）并生成一个输出（例如下一个单词或句子的完成）。例如，在句子“The
    cat sat on the...”中，一个Transformer模型在确定可能的下一个单词可能是“mat”时，可能会给单词“cat”赋予很大的权重。这些模型通过预测序列中的下一个项目——句子中的下一个单词或旋律中的下一个音符——来展示其生成特性。我们将在下一章中进一步探讨这一点。
- en: Transformer models are usually very large, requiring significant computational
    resources to train and use. Using a car analogy, think of Transformer models as
    supercharged engines that need much power to run but do amazing things. Think
    of them as the next step after models such as ResNET 50, which is used for recognizing
    images. While ResNET 50 is like a car with 50 gears, OpenAI’s GPT-3 is like a
    megatruck with 96 gears and extra features. Because of their advanced capabilities,
    these models are a top pick for creating intelligent AI outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型通常非常大，需要大量的计算资源来训练和使用。用汽车类比，可以将变压器模型视为超级引擎，需要大量能量来运行，但能做惊人的事情。可以将其视为在 ResNET
    50 等模型之后的下一步，ResNET 50 用于识别图像。ResNET 50 像一辆有 50 个档位的汽车，而 OpenAI 的 GPT-3 像一辆有 96
    个档位和额外功能的重型卡车。由于它们的先进能力，这些模型是创建智能 AI 输出的首选。
- en: LLMs use transformers, which are composed of an encoder and a decoder. The encoder
    processes the input text (i.e., the prompt) and generates a sequence of hidden
    states that represent the meaning of the input text. The decoder uses these hidden
    states to generate the output text. These encoders and decoders form one layer,
    similar to a mini-brain. Multiple layers can be stacked one upon another. As outlined
    earlier, GPT3 is a decoder-only model with 96 layers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 使用变压器，它由编码器和解码器组成。编码器处理输入文本（即提示），并生成一系列表示输入文本意义的隐藏状态。解码器使用这些隐藏状态来生成输出文本。这些编码器和解码器形成一层，类似于一个小型大脑。可以将多层堆叠在一起。如前所述，GPT3
    是一个仅包含解码器的模型，有 96 层。
- en: 2.4 Training cutoff
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 训练截止点
- en: In the context of foundational models, the training cutoff refers to the point
    at which the model’s training ends, that is, the time until the data used to train
    the model was collected. In the case of AI models developed by OpenAI, such as
    GPT-3 or GPT-4, the training cutoff is when the model was last trained on new
    data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型的情况下，训练截止点指的是模型训练结束的点，即收集用于训练模型的数据的时间。在 OpenAI 开发的 AI 模型（如 GPT-3 或 GPT-4）的情况下，训练截止点是模型最后一次在新数据上训练的时间。
- en: This cutoff is important because after this point, the model is not aware of
    any events, advancements, new concepts, or changes in language usage. For example,
    the training data cutoff for the GPT-3.5 Turbo was in September 2021, GPT-4 Turbo
    in April 2023, and GPT-4o in October 2023, meaning the model does not know about
    real-world events or advancements in various fields beyond that point.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个截止点很重要，因为在此之后，模型不会意识到任何事件、进步、新概念或语言使用的变化。例如，GPT-3.5 Turbo 的训练数据截止日期是 2021
    年 9 月，GPT-4 Turbo 是 2023 年 4 月，GPT-4o 是 2023 年 10 月，这意味着模型不知道在此点之后的现实世界事件或各个领域的进步。
- en: The key point is that while these models can generate text based on the data
    they were trained on, they do not learn or update their knowledge after the training
    cutoff. They cannot access or retrieve real-time information from the internet
    or any external database. Their responses are generated purely based on patterns
    they have learned during their training period.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于，尽管这些模型可以根据它们训练的数据生成文本，但在训练截止点之后，它们不会学习或更新它们的知识。它们无法访问或从互联网或任何外部数据库检索实时信息。它们的响应完全是基于它们在训练期间学到的模式生成的。
- en: Note  The recent announcement that the premium versions of ChatGPT will have
    access to the internet via the Bing plugin doesn’t mean that the model has more
    up-to-date information. This uses a pattern called RAG (retrieval-augmented generation),
    which will be covered later in chapter 7.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：最近宣布 ChatGPT 的付费版本将通过 Bing 插件访问互联网，并不意味着模型有更准确的信息。这使用了一种称为 RAG（检索增强生成）的模式，将在第
    7 章中介绍。
- en: 2.5 Types of LLMs
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 LLMs 类型
- en: As shown in table 2.1, there are three categories of LLMs. When we talk about
    LLMs, having the context is crucial, and it might not be evident in some cases.
    This is of great importance, as the paths we can go down when using the models
    aren’t interchangeable, and picking the right type depends on the use case one
    tries to solve. Furthermore, there is also a dependency on how effectively one
    can adapt the models to specific use cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 2.1 所示，存在三种 LLMs 类别。当我们谈论 LLMs 时，拥有上下文至关重要，在某些情况下可能并不明显。这非常重要，因为我们使用模型时可以走的路径不是可以互换的，选择正确的类型取决于试图解决的问题的使用案例。此外，还依赖于一个人如何有效地将模型适应特定的使用案例。
- en: Table 2.1 Types of LLMs
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.1 LLMs 类型
- en: '| LLM Type | Description |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LLM 类型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Base LLM  | These are the original models, pretrained on a massive corpus
    of text data, and they can generate text based on the patterns they learned during
    this training. Some also call these raw language models or even refer to them
    as foundational models; they can be used out of the box to generate text. They
    learn powerful and general representations but lack specific expertise in a particular
    task. GPT-3’s DaVinci model is an example of a base LLM.  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 基础大型语言模型 | 这些是原始模型，在大量文本数据语料库上预训练，可以根据在训练期间学习到的模式生成文本。有些人也把这些原始语言模型称为基础模型，甚至称其为基础模型；它们可以直接用于生成文本。它们学习到强大且通用的表示，但在特定任务上缺乏专业知识。GPT-3的DaVinci模型就是一个基础大型语言模型的例子。  |'
- en: '| Instruction-based LLM  | This involves using a base LLM and providing explicit
    instructions in prompt input. In many examples we saw in the previous chapter,
    we instructed the model to follow instructions, such as “Translate the following
    text to French:” or “Summarize this article:” Sometimes, these models are also
    called instruction-tuned LLMs.  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 基于指令的大型语言模型 | 这涉及使用基础大型语言模型，并在提示输入中提供明确的指令。在前一章中我们看到的许多示例中，我们指示模型遵循指令，例如“将以下文本翻译成法语：”或“总结这篇文章：”。有时，这些模型也被称为指令调整的大型语言模型。  |'
- en: '| Fine-tuned LLM  | Fine-tuning involves taking the base LLM and training it
    further on a task it might perform poorly at, often in a specific domain. An example
    would be training the model on medical literature if we want it to understand
    medical topics or training it on customer service interactions if we want it to
    respond to customer inquiries for a specific industry. Fine- tuning can help make
    the model more accurate or helpful to particular tasks or domains, but it requires
    additional data and training time.  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 微调的大型语言模型 | 微调涉及在基础大型语言模型上进一步训练，通常是在它在特定领域表现不佳的任务上。一个例子是，如果我们想让它理解医学主题，就在医学文献上训练模型；如果我们想让它对特定行业的客户咨询做出回应，就在客户服务互动上训练模型。微调可以帮助使模型更准确或对特定任务或领域更有帮助，但它需要额外的数据和训练时间。  |'
- en: 'It’s worth noting that all these methods have their pros and cons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，所有这些方法都有其优缺点：
- en: Base LLMs are versatile and can handle many tasks without additional training.
    However, they might not be as accurate or reliable as you’d like for specific
    tasks or domains, especially in an enterprise setting.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础大型语言模型具有多功能性，可以在不进行额外训练的情况下处理许多任务。然而，它们可能不如您期望的那样准确或可靠，尤其是在特定任务或领域，尤其是在企业环境中。
- en: Instruction-based usage can be very effective for some tasks, but it requires
    careful prompt crafting and doesn’t fundamentally change the model’s capabilities.
    This is where many of the prompt engineering techniques and best practices apply.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些任务，基于指令的使用可能非常有效，但它需要仔细的提示制作，并且并不从根本上改变模型的能力。这正是许多提示工程技术和最佳实践适用的地方。
- en: Fine-tuning can yield excellent results for specific tasks or domains. However,
    it requires additional resources and comes with the risk of overfitting the training
    data, which could limit the model’s ability to generalize to new examples.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调对于特定任务或领域可以产生出色的结果。然而，它需要额外的资源，并伴随着过度拟合训练数据的危险，这可能会限制模型泛化到新示例的能力。
- en: 'In addition, one can take approaches (zero-shot, few-shot, and transfer learning)
    to adapt the LLM further for a specific task to make it perform better and be
    more robust in those specific domains. To some extent, the type of LLM implemented
    also dictates which approach is better suited for the need. Each approach has
    its strengths and weaknesses, and the best one depends on the specific task, data
    available, and resources at hand:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以采取（零样本、少样本和迁移学习）等方法来进一步调整大型语言模型以适应特定任务，使其在这些特定领域表现更好且更稳健。在某种程度上，所实施的大型语言模型类型也决定了哪种方法更适合需求。每种方法都有其优点和缺点，最佳选择取决于具体任务、可用数据和现有资源：
- en: '*Zero-shot learning*—A model can generalize to a task without seeing examples
    of that task during training. For example, we could ask a model trained only in
    English to translate from English to German, even if it hasn’t seen any specific
    examples of the German language during training. It can do this zero-shot translation
    using semantic similarity, word embeddings, and machine learning. Using these
    techniques, we can measure how similar two words are even in different languages.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*零样本学习*——模型可以在训练过程中没有看到该任务的示例的情况下泛化到该任务。例如，我们可以要求一个仅在英语上训练的模型将英语翻译成德语，即使它在训练期间没有看到任何特定的德语语言示例。它可以通过语义相似性、词嵌入和机器学习来实现零样本翻译。使用这些技术，我们可以测量即使在不同的语言中，两个单词的相似程度。'
- en: '*Few-shot learning*—This involves showing the model examples of the task we
    want to perform and then asking the model to do the same task with a new example.
    Thus, if we want a model to identify the sentiment of a tweet, we might show it
    a few examples of tweets and associated sentiment and then ask it to predict the
    sentiment of a new tweet.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*少样本学习*——这涉及到向模型展示我们想要执行的任务的示例，然后要求模型用新的示例执行相同的任务。因此，如果我们想让一个模型识别推文的情感，我们可能会向它展示一些推文及其相关情感的示例，然后要求它预测新推文的情感。'
- en: '*Transfer learning*—This involves training a model on one task and then using
    what it learned for a different but related task. For example, although LLMs have
    been trained in language and not specific customer support tickets, they can be
    used to categorize customer support tickets into different categories, such as
    billing, technical issues, or general inquiries. This can help streamline the
    customer support process and ensure a speedy resolution, with higher customer
    satisfaction.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迁移学习*——这涉及到在一个任务上训练一个模型，然后使用它在不同但相关的任务上应用学到的知识。例如，尽管LLMs是在语言上而不是在特定的客户支持票据上训练的，但它们可以被用来将客户支持票据分类到不同的类别，如账单、技术问题或一般咨询。这有助于简化客户支持流程并确保快速解决，提高客户满意度。'
- en: 2.6 Small language models
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 小型语言模型
- en: Small language models (SLMs) are an emerging trend that has excited many enterprises.
    They are scaled-down versions of larger language models designed to provide many
    benefits of their larger counterparts while being more resource efficient and
    accessible. They differ from LLMs (e.g., GPT-4) in several ways, primarily in
    size and complexity, computational resources, training and operational costs,
    and performance quality. Techniques such as knowledge distillation and transfer
    learning empower smaller models to excel in analysis, translation, and summarization
    with faster training. In some cases, they can also match or outperform the larger
    LLMs, making them a game changer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 小型语言模型（SLMs）是一种新兴趋势，它激发了众多企业的兴趣。它们是较大语言模型的缩小版，旨在提供较大模型的许多好处，同时更加资源高效和易于访问。它们在多个方面与LLMs（例如，GPT-4）不同，主要在于规模和复杂性、计算资源、训练和运营成本以及性能质量。知识蒸馏和迁移学习等技术使小型模型能够在分析、翻译和摘要方面表现出色，且训练速度更快。在某些情况下，它们甚至可以匹配或超越较大的LLMs，从而成为游戏规则的改变者。
- en: 'Regarding size and complexity, SLMs are notably smaller and have fewer parameters
    than large models such as GPT-4\. This difference in scale is substantial: a small
    model may have millions to a few billion parameters, in contrast to the tens or
    hundreds of billions of parameters in large models. This reduction in size and
    complexity makes SLMs fundamentally different in how they process and generate
    language.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在规模和复杂性方面，SLMs比GPT-4等大型模型明显更小，参数也更少。这种规模差异是显著的：一个小型模型可能有数百万到数十亿个参数，而大型模型可能有数十亿或数百亿个参数。这种规模和复杂性的减少使得SLMs在处理和生成语言的方式上与大型模型有根本性的不同。
- en: From a computational resource perspective, SLMs’ smaller size requires less
    computational power. This reduced requirement makes them more suitable for applications
    with limited processing capabilities or situations where real-time responses are
    crucial. The lesser demand for computational resources also means that SLMs can
    be deployed in a wider range of environments, including on-edge devices or systems
    with lower processing capacities.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算资源的角度来看，SLMs较小的规模需要更少的计算能力。这种减少的需求使它们更适合于处理能力有限的应用或实时响应至关重要的场合。对计算资源的需求减少也意味着SLMs可以在更广泛的环境中部署，包括边缘设备或处理能力较低的系统中。
- en: Regarding training and operational costs, SLMs are generally less expensive
    to train and operate. This cost-effectiveness stems from their reduced complexity
    and the smaller amount of data needed to train them. Consequently, SLMs become
    more accessible to individuals and organizations with limited budgets, democratizing
    access to advanced language-processing technologies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于培训和运营成本，SLMs通常培训成本和运营成本较低。这种成本效益源于其简化了的结构和更少的数据量需求。因此，SLMs对预算有限的个人和组织来说更加可负担，从而实现了高级语言处理技术的民主化。
- en: However, when it comes to performance and quality, while capable of handling
    a wide range of language tasks effectively, SLMs generally do not match the performance
    level of larger models. This is particularly evident in more complex tasks that
    require a broad understanding of context or specialized knowledge. Large models,
    with their greater depth and breadth of data and understanding, are better equipped
    to handle such complexities. In contrast, SLMs might struggle with these challenges
    due to their inherent limitations in size and training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在性能和质量方面，尽管能够有效地处理广泛的语言任务，SLMs通常无法达到大型模型的性能水平。这在需要广泛理解上下文或专业知识的更复杂任务中尤为明显。大型模型凭借其更深入和广泛的数据和理解，更能应对这种复杂性。相比之下，SLMs可能由于自身在规模和训练方面的内在限制而难以应对这些挑战。
- en: Strategic data selection and a new, innovative training approach are the two
    key reasons why SLMs such as the Phi series have been so successful. Strategic
    data selection prioritizes high quality over quantity and involves using textbook-quality
    data, which includes synthetic datasets and meticulously curated web data [2].
    Data is selected to provide a robust foundation of common-sense reasoning and
    general knowledge. This strategic approach to data selection is crucial for the
    model’s exceptional performance across a spectrum of tasks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 战略性数据选择和新的创新培训方法是Phi系列等SLMs之所以如此成功的关键原因。战略性数据选择优先考虑质量而非数量，并涉及使用教科书质量的数据，包括合成数据集和精心挑选的网页数据
    [2]。数据的选择旨在提供坚实的常识推理和一般知识基础。这种数据选择战略对于模型在一系列任务中的卓越性能至关重要。
- en: In contrast, the innovative training approach includes scaling up from smaller
    models such as Phi-1.5 and embedding its knowledge in Phi-2 [3]. This method accelerates
    training convergence and enhances benchmark scores, challenging conventional scaling
    laws and demonstrating that remarkable capabilities can be achieved even with
    smaller-scale language models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，创新的培训方法包括从小型模型如Phi-1.5扩展到Phi-2 [3]，并将知识嵌入其中。这种方法加速了训练收敛并提高了基准分数，挑战了传统的扩展定律，并证明了即使是小型语言模型也能实现显著的性能。
- en: 'SLMs are still early in the lifecycle but have been increasingly considered
    for production use in enterprises. However, their readiness largely depends on
    the specific requirements and application context. Here are some factors to consider:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SLMs目前仍处于生命周期早期，但越来越多的企业开始考虑将其用于生产环境。然而，它们的准备情况很大程度上取决于具体需求和应用环境。以下是一些需要考虑的因素：
- en: '*Task complexity*—SLMs are suitable for simpler, more defined tasks. However,
    a larger model might be necessary for optimal performance if the enterprise application
    involves complex language understanding or generation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务复杂性*—SLMs适合更简单、更明确的任务。然而，如果企业应用涉及复杂语言理解或生成，可能需要更大的模型以实现最佳性能。'
- en: '*Resource constraints*—SLMs are an excellent choice for businesses with limited
    computational resources or those needing to deploy models on edge devices due
    to their lower resource requirements.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*资源限制*—SLMs是计算资源有限或需要将模型部署到边缘设备的企业的一个优秀选择，因为它们对资源的需求较低。'
- en: '*Cost efficiency*—Operating SLMs is generally more cost-effective than operating
    larger models in terms of computational resources and energy consumption. This
    can be a significant advantage for enterprises looking to minimize operational
    costs.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本效益*—运营SLMs在计算资源和能源消耗方面通常比运营大型模型更具成本效益。这对于希望最小化运营成本的企业来说可能是一个重大优势。'
- en: '*Speed and responsiveness*—SLMs can offer faster response times, which are
    beneficial for applications where real-time interaction is critical, such as in
    customer service chatbots.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*速度和响应性*—SLMs可以提供更快的响应时间，这对于需要实时交互的应用程序至关重要，例如客户服务聊天机器人。'
- en: '*On-premise*—For applications that cannot be deployed or connected to the cloud
    for regulatory or policy reasons, SLMs can be an option, as they can be more easily
    deployed on-premises or in a private cloud.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地化*——对于由于监管或政策原因无法部署或连接到云的应用，SLM可以是一个选择，因为它们可以更容易地在本地或私有云中部署。'
- en: A few examples of SLMs available today include
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可用的SLM的一些例子包括
- en: '*Phi-3*—A family of small language models, which is a Phi-2 successor, recently
    introduced by Microsoft. They are noted for outperforming other models of similar
    or even larger sizes across various benchmarks and come in three sizes: mini (3.8B),
    small (7B), and medium (14B).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Phi-3*——微软最近推出的Phi-2后继者，一个小型语言模型系列，在各种基准测试中优于其他类似甚至更大的模型，有三种尺寸：mini（3.8B）、small（7B）和medium（14B）。'
- en: '*Phi-2*—A 2.7 billion parameter model from Microsoft that demonstrates state-of-the-art
    performance on reasoning and language-understanding tasks, which can outperform
    models 25x its size.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Phi-2*——微软的一个27亿参数模型，在推理和语言理解任务上展示了最先进的性能，其性能可以超过其25倍大小的模型。'
- en: '*Orca 2*—A 7-billion- or 13-billion-parameter model from Microsoft that learns
    various reasoning techniques and solution strategies from a more capable teacher
    model 5\.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Orca 2*——微软的一个7亿或13亿参数模型，从更强大的教师模型中学习各种推理技术和解决方案策略5。'
- en: '*Gemini Nano*—A 122-million-parameter model from Google, part of the Gemini
    series, designed for efficient inference and deployment on edge devices.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gemini Nano*——谷歌的一个1.22亿参数模型，是Gemini系列的一部分，专为在边缘设备上进行高效推理和部署而设计。'
- en: '*DistilBERT*—A smaller version of BERT that retains 97% of its language understanding
    capabilities while being 40% smaller and 60% faster.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DistilBERT*——BERT的一个更小的版本，在保持97%的语言理解能力的同时，体积缩小了40%，速度提高了60%。'
- en: '*GPT-Neo*—A smaller version of the GPT architecture (125M and 1.3B), part of
    the GPT-Neo series created by EleutherAI.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GPT-Neo*——GPT架构的一个更小的版本（125M和1.3B），是EleutherAI创建的GPT-Neo系列的一部分。'
- en: These SLMs are particularly useful when deploying a large model is infeasible
    due to cost, speed, or computational requirements. They balance performance and
    efficiency, making advanced NLP capabilities more accessible.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当由于成本、速度或计算需求而无法部署大型模型时，这些SLM特别有用。它们平衡性能和效率，使高级NLP能力更加易于访问。
- en: While SLMs might not be suitable for every enterprise application, especially
    those requiring deep understanding or complex language generation, they are ready
    for production in many scenarios, particularly where efficiency, speed, and cost
    are key considerations. Enterprises should evaluate their specific needs and constraints
    to determine if an SLM is the right choice for their application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SLM可能不适合每个企业应用，尤其是那些需要深度理解或复杂语言生成的应用，但在许多场景中，它们已经准备好投入生产，尤其是在效率、速度和成本是关键考虑因素的情况下。企业应评估其具体需求和限制，以确定SLM是否适合其应用。
- en: 2.7 Open source vs. commercial LLMs
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 开源LLM与商业LLM
- en: Today’s commercial models provide top-notch performance in terms of AI quality
    and wide-ranging abilities. However, since the release of ChatGPT, there has been
    a significant shift toward open source models. Many of these open source initiatives
    focus on developing smaller foundational models, asserting they can achieve nearly
    the same quality levels without significant loss. Figure 2.2 [4] illustrates these
    lineages and how they have exploded quickly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的商业模型在AI质量和广泛能力方面提供了顶级性能。然而，自从ChatGPT发布以来，开源模型的方向发生了显著转变。许多这些开源项目专注于开发更小的基础模型，声称它们可以在不造成重大损失的情况下达到几乎相同的质量水平。图2.2
    [4]展示了这些谱系以及它们如何迅速爆炸。
- en: '![figure](../Images/CH02_F02_Bahree.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F02_Bahree.png)'
- en: 'Figure 2.2 Timeline of LLMs with 10+B parameters: A Survey of LLMs'
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2：拥有10+B参数的LLM时间线：LLM综述
- en: 2.7.1 Commercial LLMs
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.1 商业LLM
- en: 'Currently, there are several commercial LLMs, and they will print and have
    started making traction. Almost all of them follow the OpenAI paradigm and are
    exposed via an API we use. Although still startups, many have serious funding,
    and their founders have deep research backgrounds:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有几种商业LLM，它们已经开始印刷并开始产生影响。几乎所有的LLM都遵循OpenAI模式，并通过我们使用的API公开。尽管它们仍然是初创公司，但许多都有严重的资金，并且它们的创始人有深厚的科研背景：
- en: OpenAI is an AI research lab that develops and publishes cutting-edge AI models,
    such as the GPT series. It certainly has the most mindshare today. It has several
    foundational models, such as GPT-4, DALL.E, and ChatGPT, and is the most mature
    in this group, with serious backing and ownership by Microsoft.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 是一个 AI 研究实验室，开发和发布前沿 AI 模型，如 GPT 系列。它无疑是今天最具影响力的。它有几个基础模型，如 GPT-4、DALL.E
    和 ChatGPT，在这个群体中最为成熟，得到了微软的全力支持和拥有。
- en: Azure OpenAI and OpenAI offer access to powerful language models but differ
    in their nature and implementation. The primary distinction is that Azure OpenAI
    is a managed service, whereas OpenAI is not. Microsoft handles the underlying
    infrastructure and maintenance for Azure OpenAI, making it a valuable option for
    businesses lacking the resources to manage their OpenAI deployment. Furthermore,
    Azure OpenAI packages the raw OpenAI models into developer-friendly services that
    developers can seamlessly integrate into their applications. These services run
    on Azure, ensuring high scalability, reliability, and global availability.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 和 OpenAI 提供了对强大语言模型的访问，但它们的性质和实现方式不同。主要区别在于 Azure OpenAI 是一种托管服务，而
    OpenAI 则不是。微软负责 Azure OpenAI 的底层基础设施和维护，使其成为资源不足的企业管理 OpenAI 部署的有价值选择。此外，Azure
    OpenAI 将原始 OpenAI 模型打包成开发者友好的服务，开发者可以无缝地将这些服务集成到他们的应用程序中。这些服务运行在 Azure 上，确保了高可扩展性、可靠性和全球可用性。
- en: 'Anthropic is a startup founded by ex-OpenAI engineers that has released Claude,
    an LLM that can generate text and code. Their key differentiator is implementing
    the LLM using constitutional AI [5]. Constitutional AI uses reinforcement learning
    (RL) and traditional supervised learning and claims to produce less harmful outputs.
    As of this publication, Anthropic was backed by both Google and Amazon. Claude
    3, the latest family of models, has three versions: Haiku (small-sized), Sonnet
    (medium-sized), and Opus (large-sized) models.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic 是由前 OpenAI 工程师创立的初创公司，发布了 Claude，这是一个可以生成文本和代码的 LLM。他们的关键区别在于使用宪法
    AI [5] 实现了 LLM。宪法 AI 使用强化学习（RL）和传统的监督学习，并声称可以产生更少的危害性输出。截至本文发表时，Anthropic 获得了谷歌和亚马逊的支持。Claude
    3，最新一代的模型，有三个版本：Haiku（小型）、Sonnet（中型）和 Opus（大型）模型。
- en: Gemini is Google’s latest GenAI model, available as part of the Google Cloud
    offering in the recently launched Google AI Studio product. At the time of this
    writing, Google is opening up API access to the models in private preview.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini 是谷歌最新的 GenAI 模型，作为最近推出的 Google AI Studio 产品的一部分，可在 Google Cloud 提供的服务中使用。在撰写本文时，谷歌正在对模型进行私人预览，开放
    API 访问。
- en: Cohere AI, a startup originating from a Transformer paper (“Attention is all
    you need”), has an LLM and other products such as Neural Search and Embed.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohere AI，一家起源于 Transformer 论文（《注意力即一切所需》）的初创公司，拥有 LLM 以及其他产品，如 Neural Search
    和 Embed。
- en: 2.7.2 Open source LLMs
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.2 开源 LLMs
- en: A new crop of OSS LLMs is coming up, and some of these will compete with ChatGPT.
    As figure 2.1 shows, there are too many to mention, but a few noteworthy ones
    are listed in table 2.2.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 新一代的开源 LLM 正在涌现，其中一些将与 ChatGPT 竞争。如图 2.1 所示，数量太多，难以一一列举，但表中列出了几个值得注意的例子。
- en: Table 2.2 Open source LLMs
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.2 开源 LLMs
- en: '| Company | Open source LLM | Parameter size |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 公司 | 开源 LLM | 参数大小 |'
- en: '| --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Meta  | Llama LLM is one of the models that has inspired many other OSS models.
    It comes in many sizes (7B, 13B, 33B, and 65B), and while smaller than GPT-3,
    it can be matched across many tasks. Meta shared the models with researchers (and
    they were also leaked separately online), inspiring many others to use that as
    a jumping-off point.  | Various (7B–65B)  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Meta | Meta 的 Llama LLM 是许多其他 OSS 模型受到启发的模型之一。它有多种大小（7B、13B、33B 和 65B），虽然比
    GPT-3 小，但在许多任务上可以匹敌。Meta 与研究人员分享了这些模型（并且它们也被单独泄露到网络上），激励了许多其他人以此作为起点。 | 各种（7B–65B）
    |'
- en: '| Databricks  | Databricks recently released v2 of Dolly, which they label
    the “world’s first truly open instruction-tuned LLM.” It is released under the
    CCA 3.0 license, allowing anyone to use, extend, and modify it, including for
    commercial purposes.  | 12B  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Databricks | Databricks 最近发布了 Dolly 的 v2 版本，他们将其称为“世界上第一个真正开放的指令调整 LLM”。它是在
    CCA 3.0 许可证下发布的，允许任何人使用、扩展和修改它，包括商业用途。 | 12B |'
- en: '| Alpaca  | Standford University’s Alpaca, an instruction model based on Llama,
    claims to match the GPT-3.5 Turbo performance in some tasks.  | 7B  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 斯坦福大学的 Alpaca，基于 Llama 的指令模型，声称在某些任务上可以匹配 GPT-3.5 Turbo 的性能。 | 7B
    |'
- en: '| FreedomGPT  | This is an OSS conversational agent based on Alpaca. They claim
    to offer 100% uncensored and private conversations.  | Not disclosed  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| FreedomGPT  | 这是一个基于 Alpaca 的开源对话代理。他们声称提供 100% 无审查和私密的对话。  | 未公开  |'
- en: '| Vicuna  | Academic researchers from multiple institutions (UC Berkeley, CMU,
    Stanford, UC San Diego, and MBZUAI) released Vicuna, a fine-tuned version of Llama
    that matches the GPT4 performance across many tasks.  | 13B  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna  | 来自多个机构（加州大学伯克利分校、卡内基梅隆大学、斯坦福大学、加州大学圣地亚哥分校和 MBZUAI）的学术研究人员发布了 Vicuna，这是
    Llama 的一个微调版本，在许多任务上与 GPT4 的性能相匹配。  | 13B  |'
- en: '| Koala  | Berkley AI Research released Koala, a fine-tuned version of Llama
    using internet dialogues.  | 13B  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Koala  | 伯克利人工智能研究实验室发布了 Koala，这是使用互联网对话微调的 Llama 版本。  | 13B  |'
- en: '| ChatLLaMa  | Technically, it’s not a model but tooling for models. Nebuly
    AI released ChatLLaMa, a library that can create ChatGPT-like conversational assistance
    using your data.  | 7B  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ChatLLaMa  | 从技术上来说，它不是一个模型，而是为模型提供的工具。Nebuly AI 发布了 ChatLLaMa，这是一个库，可以使用您的数据创建类似
    ChatGPT 的对话助手。  | 7B  |'
- en: '| ColossalChat  | UC Berkeley’s ColossalAI project released ColossalChat, a
    ChatGPT-like model that includes complete RLHF pipelines based on Llama.  | 7B  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ColossalChat  | 加州大学伯克利分校的 ColossalAI 项目发布了 ColossalChat，这是一个类似于 ChatGPT
    的模型，包括基于 Llama 的完整强化学习与人类反馈（RLHF）管道。  | 7B  |'
- en: '| Falcon  | The Technology Innovation Institute (TII) in the United Arab Emirates
    released a family of LLMs called the Falcon LLM model. At the time, Falcon was
    the biggest OSS LLM ever released and was at the top of the OSS LLM Leaderboard.
    More recently, a more powerful 180B parameters model is again at the top of the
    leaderboard.  | Various (1B–180B)  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Falcon  | 阿联酋的技术创新研究所（TII）发布了一系列名为 Falcon LLM 模型的 LLM。当时，Falcon 是有史以来发布的最大开源
    LLM，并在开源 LLM 排行榜上位居榜首。最近，一个更强大的 180B 参数模型再次位居排行榜首位。  | 各种（1B–180B）  |'
- en: '| Mistral  | Mistral AI, a French startup, has developed a range of models.
    Some are open source models licensed under Apache 2.0, a permissive license allowing
    unrestricted use in any context. As mentioned in the previous chapter, they also
    have commercial models.  | Various (7B – 141B)  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Mistral  | 法国初创公司 Mistral AI 开发了一系列模型。其中一些是开源模型，根据 Apache 2.0 许可证发布，这是一个允许在任何环境下无限制使用的宽松许可。如前一章所述，他们也有商业模型。  |
    各种（7B – 141B）  |'
- en: OpenAI vs. Azure OpenAI
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OpenAI 与 Azure OpenAI
- en: Azure OpenAI and OpenAI are both services that provide access to OpenAI’s powerful
    language models, but they have some key differences. OpenAI caters more to smalland
    medium-business individual developers and startups. In contrast, Azure OpenAI
    is intended for enterprises that need additional security and availability in
    different parts of the world and that have regulatory needs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 和 OpenAI 都是提供访问 OpenAI 强大语言模型的服务，但它们有一些关键区别。OpenAI 更适合小型和中型企业的个人开发者和新创公司。相比之下，Azure
    OpenAI 旨在满足需要在不同地区提供额外安全和可用性的企业，以及有监管需求的企业。
- en: Azure OpenAI offers additional enterprise-ready features, such as data privacy,
    customer-managed keys, encryption at rest, private networking, regional availability,
    and responsible AI content filtering. These features can be important for businesses
    that need to comply with specific security or regulatory requirements.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 提供了额外的企业级功能，例如数据隐私、客户管理的密钥、静态加密、私有网络、区域可用性和负责任的 AI 内容过滤。这些功能对于需要遵守特定安全或监管要求的企业来说可能很重要。
- en: Consequently, the APIs between the two are similar but not the same. However,
    the underlying models are the same, and Azure OpenAI has a deployment that incorporates
    these additional features that most enterprises require.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这两个服务之间的 API 相似但不完全相同。然而，底层模型是相同的，Azure OpenAI 有一个包含这些大多数企业所需额外功能的部署。
- en: 2.8 Key concepts of LLMs
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 LLM 的关键概念
- en: This section describes the architecture of a typical LLM implementation. Figure
    2.3 shows the abstract structure of a common LLM implementation at a high level;
    it follows this process whenever we use an LLM such as OpenAI’s GPT.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了典型大型语言模型（LLM）实现的架构。图 2.3 展示了在高级别上常见 LLM 实现的抽象结构；每次我们使用 LLM，如 OpenAI 的 GPT，都会遵循此过程。
- en: '![figure](../Images/CH02_F03_Bahree.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F03_Bahree.png)'
- en: Figure 2.3 Conceptual architecture of an LLM
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.3 LLM 的概念架构
- en: The model starts with the input text—the prompt. It is first converted into
    a sequence of tokens using tokenization. Each token is then converted into a numerical
    vector via a process called embedding, which acts as the encoder input.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模型从输入文本——提示开始。首先，使用标记化将其转换为一系列标记。然后，每个标记通过称为嵌入的过程转换为数值向量，这作为编码器的输入。
- en: The encoder processes the input sequence and generates a sequence of hidden
    states. These hidden states are then fed into the decoder with a start token.
    The decoder generates the output sequence one token at a time by predicting the
    next token based on the previous tokens and hidden states.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器处理输入序列并生成一系列隐藏状态。然后，这些隐藏状态被输入到解码器中，并带有起始标记。解码器通过预测下一个标记（基于前一个标记和隐藏状态）一次生成一个输出序列。
- en: Once all the layers have processed the information, the model predicts the next
    token in the learned sequence. This outcome is converted back to the text, and
    we see the response. This process runs in an iterative loop and occurs for each
    new token generated, thus creating a coherent text output. The final text that
    the model generates is an emergent property of this layered, iterative process.
    The final output sequence is also called a completion.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有层都处理了信息，模型就会预测学习序列中的下一个标记。这个结果被转换回文本，我们看到响应。这个过程在一个迭代循环中运行，并且对每个新生成的标记都会发生，从而创建出一个连贯的文本输出。模型生成的最终文本是这个分层、迭代过程的涌现属性。最终的输出序列也被称为完成。
- en: Let’s examine each of these LLM aspects in more detail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地检查这些大型语言模型（LLM）的各个方面。
- en: 2.8.1 Prompts
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 提示
- en: A prompt is how we “talk” to these models. A prompt is just text describing
    the task we are trying to achieve using natural language. The output of these
    models is also text. The ability to express our intention in this manner (natural
    language) instead of conforming to the input restrictions of a machine makes prompts
    powerful. Crafting or designing the text in the prompt is akin to programming
    the model and creating a new paradigm called prompt engineering, which will be
    covered later in the book.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们与这些模型“交谈”的方式。提示只是用自然语言描述我们试图通过自然语言完成的任务的文本。这些模型的输出也是文本。以这种方式（自然语言）表达我们的意图（而不是遵守机器的输入限制）的能力使提示变得强大。在提示中构建或设计文本类似于编程模型，并创建一个称为提示工程的新范式，这将在本书的后面部分介绍。
- en: 'Prompts allow us to capture a broader set of intents and emotions using the
    same paradigm. In the following two examples (generated by AI), a prompt asks
    for a Haiku outlining how great LLMs are: one haiku is happy, and the other is
    sad. Getting this level of emotion and expressiveness on the same task was impossible.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 提示允许我们使用相同的范式捕捉更广泛的意图和情感。在以下两个由AI生成的示例中，一个提示要求写一首俳句来概述LLM是多么伟大：一首俳句是快乐的，另一首是悲伤的。在同一个任务上获得这种程度情感和表现力是不可能的。
- en: '**![image](../Images/Prompt.png)**Write a happy haiku on how great LLMs are.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt.png)**写一首快乐的俳句，赞美大型语言模型是多么伟大。'
- en: '**![image](../Images/Response.png)**AI-generated response'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**AI生成的响应'
- en: LLMs are great
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs很棒
- en: They can chat and generate
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以聊天并生成
- en: Creative and smart
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 创造性和智能
- en: '**![image](../Images/Prompt.png)**Write a sad haiku on how great LLMs are.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt.png)**写一首悲伤的俳句，赞美大型语言模型是多么伟大。'
- en: '**![image](../Images/Response.png)**AI-generated response'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**AI生成的响应'
- en: LLMs are great
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs很棒
- en: But they cannot feel or love
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们无法感受或爱
- en: Only simulate
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 只能模拟
- en: NOTE  LLMs do not understand text like humans do. They are statistical models
    that learn patterns in data and use those patterns to generate output. Despite
    their impressive output, these models don’t comprehend the content generated in
    the same sense as we do—including beliefs, desires, consciousness, and right and
    wrong. They are just extremely good at predicting the next word in a sequence
    of text based on patterns they’ve seen millions of times.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：LLM并不像人类那样理解文本。它们是统计模型，通过学习数据中的模式并使用这些模式来生成输出。尽管它们的输出令人印象深刻，但这些模型并不像我们那样理解生成的内容——包括信念、欲望、意识、对与错。它们只是极其擅长根据他们看到的数百万次模式预测文本序列中的下一个单词。
- en: 2.8.2 Tokens
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 标记
- en: Tokens are the basic units of text that an LLM uses to process both the request
    and the response, that is, to understand and generate text. Tokenization is the
    process of converting text into a sequence of smaller units called tokens. When
    using LLMs, we use tokens to converse with these models, which is one of the most
    fundamental elements of understanding LLMs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 标记是LLM用于处理请求和响应（即理解和生成文本）的基本文本单位。分词是将文本转换为称为标记的较小单元序列的过程。在使用LLM时，我们使用标记与这些模型进行对话，这是理解LLM的最基本要素之一。
- en: Tokens are the new currency when incorporating LLMs into your application or
    solutions. They directly correlate with the cost of running models, both in terms
    of money and of the experience with latency and throughput. The more tokens, the
    more processing the model must do. This means more computational resources are
    required for the model, which means lower performance and higher latency.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当将LLM集成到您的应用程序或解决方案中时，标记是新的货币。它们直接关联到运行模型的成本，无论是金钱还是延迟和吞吐量的体验。标记越多，模型必须进行的处理就越多。这意味着模型需要更多的计算资源，这意味着性能降低和延迟增加。
- en: LLMs convert the text into tokens before processing. Depending on the tokenization
    algorithm, they can be individual characters, words, sub-words, or even larger
    linguistic units. A rough rule of thumb is that one token is approximately four
    characters or 0.75 words for English text. For most LLMs today, the token size
    that they support includes both the input prompt and the response.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在处理文本之前将其转换为标记。根据分词算法的不同，它们可以是单个字符、单词、子词，甚至是更大的语言单位。一个粗略的规则是，一个标记大约是四个字符或0.75个单词的英文文本。对于今天的大多数LLM，它们支持的标记大小包括输入提示和响应。
- en: Let’s illustrate this through an example. Figure 2.4 shows how the sentence
    “I have a white dog named Champ” gets tokenized (using OpenAI’s tokenizer in this
    case). Each block represents a different token. In this example, we use eight
    tokens.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明这一点。图2.4展示了句子“我有一只名叫Champ的白色狗”是如何被分词的（在此例中使用的是OpenAI的分词器）。每个块代表一个不同的标记。在这个例子中，我们使用了八个标记。
- en: '![figure](../Images/CH02_F04_Bahree.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F04_Bahree.png)'
- en: Figure 2.4 Tokenizer example
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 分词器示例
- en: LLMs generate text by predicting the next word or symbol (token) most likely
    to follow a given sequence of words or symbols (tokens) they use as input, that
    is, the prompt. We show a visual representation of this in figure 2.5, where the
    list of tokens on the right shows the highest probability of tokens following
    the prompt “The dog sat on.” We can influence some of this probability of tokens
    using a few parameters we will see later in the chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LLM通过预测给定序列（即输入提示）中最有可能跟随的下一个单词或符号（标记）来生成文本。我们在图2.5中展示了这一过程的视觉表示，其中右侧的标记列表显示了跟随提示“The
    dog sat on.”的标记的最高概率。我们可以通过本章后面将要看到的几个参数来影响其中一些标记的概率。
- en: '![figure](../Images/CH02_F05_Bahree.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F05_Bahree.png)'
- en: Figure 2.5 LLM—next token predictor
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 LLM—下一个标记预测器
- en: Suppose we have a sequence of tokens with a length of n. Utilizing these n tokens
    as the context, we generate the subsequent token, n + 1\. This newly predicted
    token is then appended to the original sequence of tokens, thereby expanding the
    context. Consequently, the expanded context window for generating token n + 2
    becomes n + (n + 1). This process is repeated in a continuous loop until a predetermined
    stop condition, such as a specific sequence or a size limit for the tokens, is
    reached.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个长度为n的标记序列。利用这些n个标记作为上下文，我们生成随后的标记，即n + 1。这个新预测的标记随后被添加到原始的标记序列中，从而扩展了上下文。因此，生成标记n
    + 2的扩展上下文窗口变为n + (n + 1)。这个过程在达到预定的停止条件（如特定的序列或标记的大小限制）时重复进行。
- en: For example, if we have a sentence, “Hawaiian pizza is my favorite,” the probability
    distribution of the next word we see is shown in figure 2.6\. The most likely
    word is “type,” finishing the sentence “Hawaiian pizza is my favorite type.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个句子，“夏威夷披萨是我的最爱”，我们看到下一个单词的概率分布如图2.6所示。最可能的单词是“类型”，完成句子“夏威夷披萨是我的最爱类型”。
- en: '![figure](../Images/CH02_F06_Bahree.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F06_Bahree.png)'
- en: Figure 2.6 Next token probability distribution
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 下一个标记概率分布
- en: If you run this example again, you will get a probability different from the
    one shown here. This is because most AI is nondeterministic, specifically in the
    case of LLMs. Simultaneously, it might predict one token, and it is probably being
    looked at across all the possible tokens that the model has learned in the training
    phase.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次运行这个示例，你将得到一个与这里显示不同的概率。这是因为大多数AI是非确定性的，特别是在LLMs的情况下。同时，它可能预测一个标记，并且很可能在模型在训练阶段学习到的所有可能的标记中进行观察。
- en: We also use two examples that outline how one token changes the distribution
    dramatically (changing one word from “the” to “a”). Figure 2.7 shows that the
    most probable next token is “mat” at 41% probability. We also see a list of the
    other tokens and their probabilistic distributions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了两个示例，说明了单个标记如何极大地改变分布（将一个单词从“the”改为“a”）。图2.7显示，最可能的下一个标记是“mat”，概率为41%。我们还看到了其他标记及其概率分布的列表。
- en: '![figure](../Images/CH02_F07_Bahree.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F07_Bahree.png)'
- en: Figure 2.7 Example 1
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 示例1
- en: However, changing one token from “the” to “a” dramatically changes the next
    distribution set, with the mat jumping up 30 points to a probability of nearly
    75%, as shown in figure 2.8.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将一个标记从“the”改为“a”会极大地改变下一个分布集，使得“mat”的概率从30点上升到近75%，如图2.8所示。
- en: '![figure](../Images/CH02_F08_Bahree.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F08_Bahree.png)'
- en: Figure 2.8 Example 2
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 示例2
- en: Some settings related to LLMs are important and can change how the model behaves
    and generates text. These settings are the model configurations and can be changed
    via an API, GUI, or both. We cover model configurations in more detail later in
    the chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一些与LLMs相关的设置很重要，并且可以改变模型的行为和文本生成方式。这些设置是模型配置，可以通过API、GUI或两者来更改。我们将在本章后面更详细地介绍模型配置。
- en: 2.8.3 Counting tokens
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.3 计数标记
- en: 'Many developers will probably be new to tracking tokens when using LLM, especially
    in an enterprise setting. However, counting tokens is important for several reasons:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 许多开发者在使用LLM时可能对跟踪标记不太熟悉，尤其是在企业环境中。然而，计数标记有多个重要原因：
- en: '*Memory limitations*—LLMs can process a maximum number of tokens in a single
    pass. This is due to the memory limitations of their architecture, often defined
    by their context window (another concept we discuss later in this chapter). For
    example, OpenAI’s latest GPT-4o model has a content window of 128K, and Google’s
    latest Gemini 1.5 Pro has a context window of 1M tokens. GPT3.5-Turbo, another
    OpenAI model, has two models supporting 8K and 16K token lengths. There is research
    ongoing to see how to solve this, such as LongNet [6] from Microsoft Research,
    which shows how to scale to 1B context windows. It is important to point out that
    this is still an active research area and has not been productized yet.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内存限制*—LLMs在一次遍历中可以处理的最大标记数量。这是由于它们架构的内存限制，通常由它们的上下文窗口定义（我们将在本章后面讨论的另一个概念）。例如，OpenAI最新的GPT-4o模型有一个128K的内容窗口，而Google最新的Gemini
    1.5 Pro有一个1M标记的内容窗口。另一个OpenAI模型GPT3.5-Turbo有两个模型支持8K和16K标记长度。正在进行研究以解决这一问题，例如微软研究机构的LongNet
    [6]，它展示了如何扩展到1B上下文窗口。重要的是指出，这仍然是一个活跃的研究领域，尚未产品化。'
- en: '*Cost*—When thinking about cost, there are two dimensions: the computational
    costs in terms of latency, memory, and the overall experience, and the actual
    cost in terms of money. For each call, the computational resources required for
    processing tokens directly correlate to the tokens’ length. As the token length
    increases, it requires more processing time, leading to more computational requirements
    (specifically memory and GPUs) and higher latency. This also means increased costs
    for using the LLMs.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本*—在考虑成本时，有两个维度：从延迟、内存和整体体验的角度来看的计算成本，以及实际成本。对于每次调用，处理标记所需的计算资源与标记的长度直接相关。随着标记长度的增加，它需要更多的时间来处理，导致更高的计算需求（特别是内存和GPU）和更高的延迟。这也意味着使用LLMs的成本增加。'
- en: '*AI quality*—The quality of a model’s output depends on the number of tokens
    it is asked to generate or process. If the text is too short, the model might
    not have enough context to provide a good answer. Conversely, if the text is too
    long, the model might lose coherence in its response. We will touch on the notion
    of good versus poor as part of prompt engineering later in chapter 6\.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI质量*—模型输出的质量取决于它被要求生成或处理的标记数量。如果文本太短，模型可能没有足够的上下文来提供好的答案。相反，如果文本太长，模型可能在回答中失去连贯性。我们将在第6章的后续部分讨论提示工程中关于好与坏的概念。'
- en: For many enterprises, cost and performance are key factors in deciding whether
    to use tokens. Generally speaking, smaller models are more cost-effective and
    efficient than bigger ones.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多企业来说，成本和性能是决定是否使用标记的关键因素。一般来说，较小的模型比较大的模型更经济高效。
- en: Listing 2.1 shows a simple way to calculate the number of tokens. In this example,
    we use an open source library called `tiktoken`, released by OpenAI. This tokenizer
    library implements a byte-pair encoding (BPE) algorithm. These tokenizers are
    designed with their respective LLMs, ensuring efficient tokenization and optimal
    performance during pretraining and fine-tuning processes. If you use one of the
    OpenAI models, you must use this tokenizer; many other transformer models also
    use it. If needed, you can install the `tiktoken` library using `pip` `install`
    `tiktoken`
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1展示了计算标记数量的简单方法。在这个例子中，我们使用了一个由OpenAI发布的开源库`tiktoken`。这个标记化器库实现了一个字节对编码（BPE）算法。这些标记化器是为它们各自的LLM设计的，确保了在预训练和微调过程中的高效标记化和最佳性能。如果你使用OpenAI的任何模型，你必须使用这个标记化器；许多其他transformer模型也使用它。如果需要，你可以使用`pip
    install tiktoken`来安装`tiktoken`库。
- en: Listing 2.1 Counting tokens for GPT
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 GPT的标记计数
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 The encoding specifies how the text is converted into tokens.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 编码指定了文本如何转换为标记。'
- en: 'Running this code, as expected, gives us the following output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码，正如预期的那样，给出了以下输出：
- en: '[PRE1]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note  Byte-pair encoding (BPE) is a compression algorithm widely used in NLP
    tasks, such as text classification, text generation, and machine translation.
    One of the BPE advantages is that it is reversible and lossless, so we can get
    the original text. BPE works on any text that the tokenizer’s training data hasn’t
    seen, and it compresses the text, resulting in shorter token sequences than the
    original text. BPE also helps generalize repeating patterns in a language and
    provides a better understanding of grammar. For example, the gerund -ing form
    is quite common in English (swimming, running, debugging, etc.). BPE will split
    it into different tokens, so “swim” and “-ing” in swimming become two tokens and
    generalize better.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：字节对编码（BPE）是一种在NLP任务中广泛使用的压缩算法，如文本分类、文本生成和机器翻译。BPE的一个优点是它是可逆的且无损的，因此我们可以获取原始文本。BPE可以在标记化器训练数据未见过的任何文本上工作，并压缩文本，从而产生比原始文本更短的标记序列。BPE还有助于泛化语言中的重复模式，并提供对语法的更好理解。例如，现在分词（-ing形式）在英语中相当常见（swimming,
    running, debugging等）。BPE会将其分割成不同的标记，所以“swim”和“-ing”在swimming中变成了两个标记，并且泛化得更好。
- en: If we are not sure of the name of the encoding to use, instead of the function
    `get_ encoding()`, we can use the `encoding_for_model()`function. This takes the
    name of the model we want to use and utilizes the corresponding encoding, such
    as `encoding =` `tiktoken.encoding_for_model('gpt-4').` For OpenAI, table 2.3
    shows different supported encodings.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不确定要使用的编码名称，而不是使用`get_encoding()`函数，我们可以使用`encoding_for_model()`函数。这个函数接受我们想要使用的模型的名称，并利用相应的编码，例如`encoding
    = tiktoken.encoding_for_model('gpt-4').`对于OpenAI，表格2.3显示了不同的支持编码。
- en: Table 2.3 OpenAI encodings
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表格2.3 OpenAI编码
- en: '| Encoding | OpenAI model |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 编码 | OpenAI模型 |'
- en: '| --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `cl100k_base`  | gpt-4, gpt-3.5-turbo, gpt-35-turbo, text-embedding-ada-002  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `cl100k_base` | gpt-4, gpt-3.5-turbo, gpt-35-turbo, text-embedding-ada-002
    |'
- en: '| `p50k_base`  | Codex models, text-davinci-002, text-davinci-003  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `p50k_base` | Codex模型，text-davinci-002，text-davinci-003 |'
- en: '| `r50k_base`  | GPT-3 models (davinci, curie, babage, ada)  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `r50k_base` | GPT-3模型（davinci, curie, babage, ada）|'
- en: Listing 2.2 shows how to use different encodings and how to get the original
    text from the tokens. We should understand this as a basic construct for now,
    but it is useful for more advanced use cases such as caching and chunking text—aspects
    that we cover later in the book.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2展示了如何使用不同的编码以及如何从标记中获取原始文本。我们现在应该将其视为一个基本结构，但对于更高级的使用案例，如缓存和文本分块——这些内容我们将在本书的后续章节中介绍。
- en: Listing 2.2 Tokens
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 标记
- en: '[PRE2]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the `tiktoken` library we have been using in the examples, there
    are a few other popular tokenizers. Remember that each tokenizer is designed for
    the corresponding LLM and cannot be interchanged:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在示例中使用的`tiktoken`库之外，还有一些其他流行的标记化器。请记住，每个标记化器都是为相应的LLM设计的，不能互换：
- en: '*WordPiece*—Used by the BERT model from Google, it splits text into smaller
    units based on the most frequent word pieces, allowing for efficient representation
    of rare or out-of-vocabulary words.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WordPiece*——由谷歌的BERT模型使用，它根据最频繁的词片段将文本分割成更小的单元，允许高效地表示罕见或不在词汇表中的单词。'
- en: '*SentencePiece*—Meta’s RoBERTa model (Robustly Optimized BERT) uses the model.
    It combines WordPiece and BPE approaches into a single language-agnostic framework,
    allowing for more flexibility.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SentencePiece*——Meta的RoBERTa模型（鲁棒优化BERT）使用该模型。它将WordPiece和BPE方法结合到一个单一的语言无关框架中，从而提供了更多的灵活性。'
- en: '*T5 tokenizer*—Based on SentencePiece, it is used by Google’s T5 model (Text-to-Text
    Transfer Transformer).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T5分词器*——基于SentencePiece，它被Google的T5模型（文本到文本迁移转换器）所使用。'
- en: '*XLM tokenizer*—This is used in Meta’s XLM (Cross-lingual Language Model) and
    implements a BPE method with learned embeddings (BPEmb). It is designed to handle
    multilingual text and support cross-lingual transfer learning.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*XLM分词器*——它在Meta的XLM（跨语言语言模型）中使用，并实现了一个带有学习嵌入的BPE方法（BPEmb）。它旨在处理多语言文本并支持跨语言迁移学习。'
- en: 2.8.4 Embeddings
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.4 嵌入
- en: Embeddings are powerful machine-learning tools for large inputs representing
    words. They capture semantic similarities in a vector space (i.e., a collection
    of vectors, as shown in figure 2.9), allowing us to determine if two text chunks
    represent the same meaning. By providing a similarity score, embeddings can help
    us better understand the relationships between different pieces of text.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是强大的机器学习工具，用于表示单词的大输入。它们在向量空间中捕捉语义相似性（即如图2.9所示的一组向量），使我们能够确定两个文本片段是否代表相同的意义。通过提供相似度分数，嵌入可以帮助我们更好地理解不同文本片段之间的关系。
- en: The idea behind embeddings is that words with similar meanings should have similar
    vector representations, as measured by their distances. Vectors with smaller distances
    between them suggest they are highly related, and those with longer distances
    suggest low relatedness. There are a few ways to measure similarities; we will
    cover these later in chapter 7.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入背后的理念是，含义相似的单词应该有相似的向量表示，这是通过它们的距离来衡量的。它们之间距离较小的向量表明它们高度相关，而距离较长的向量表明相关性较低。有几种方法可以衡量相似性；我们将在第7章中介绍这些方法。
- en: These vectors are learned during training and are used to capture the meaning
    of words or phrases. AI algorithms can easily utilize these vectors of floating-point
    numbers.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量是在训练过程中学习的，用于捕捉单词或短语的含义。AI算法可以轻松地利用这些浮点数向量。
- en: '![figure](../Images/CH02_F09_Bahree.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F09_Bahree.png)'
- en: Figure 2.9 Embeddings
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 嵌入
- en: For example, the word “cat” might be represented by a vector as [0.2, 0.3, -0.1],
    while the word “dog” might be represented as [0.4, 0.1, 0.2]. These vectors can
    then be used as input to machine learning models for tasks such as text classification,
    sentiment analysis, and machine translation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，单词“猫”可能由一个向量表示为[0.2, 0.3, -0.1]，而单词“狗”可能表示为[0.4, 0.1, 0.2]。这些向量然后可以被用作机器学习模型的输入，用于诸如文本分类、情感分析和机器翻译等任务。
- en: Embeddings are learned when the model is trained on a large corpus of text data.
    The idea is to capture the meaning of words or phrases based on their context
    in the training data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在大量的文本数据语料库上训练时，会学习到嵌入。其理念是基于训练数据中单词或短语的上下文来捕捉它们的含义。
- en: 'Depending on the task, there are several algorithms for creating embeddings:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，有几种创建嵌入的算法：
- en: Similarity embeddings are good at capturing semantic similarity between two
    or more pieces of text.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度嵌入擅长捕捉两篇或多篇文本之间的语义相似性。
- en: Text search embeddings measure whether long documents are relevant to a short
    query.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本搜索嵌入衡量长文档是否与短查询相关。
- en: Code search embeddings are useful for embedding code snippets and natural language
    search queries.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码搜索嵌入对于嵌入代码片段和自然语言搜索查询是有用的。
- en: Note  Embeddings created by one method cannot be understood by another. In other
    words, if you create an embedding using OpenAI’s API, embeddings of another provider
    will not understand the vectors created, and vice versa.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由一种方法创建的嵌入不能被另一种方法理解。换句话说，如果你使用OpenAI的API创建嵌入，其他提供商的嵌入将无法理解你创建的向量，反之亦然。
- en: Listing 2.3 shows how to get an embedding (from OpenAI in this example). We
    define a function called `get_embedding()` that takes a string for which we need
    to create embeddings as a parameter. The function uses OpenAI’s API to generate
    an embedding for the input text using the `text-embedding-ada-002` model. The
    embedding is returned as a list of floating-point numbers.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3展示了如何获取嵌入（以本例中的OpenAI为例）。我们定义了一个名为`get_embedding()`的函数，它接受一个字符串作为参数，我们需要为该字符串创建嵌入。该函数使用OpenAI的API，通过`text-embedding-ada-002`模型为输入文本生成嵌入。嵌入以浮点数列表的形式返回。
- en: Listing 2.3 Getting an embedding in OpenAI
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 在OpenAI中获取嵌入
- en: '[PRE3]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The vector space resulting from the embedding isn’t a one-to-one mapping to
    the tokens but can be a lot more. The output of the previous examples is shown
    next. For brevity, we only show the first five items in the list:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从嵌入产生的向量空间不是一对一映射到标记，但可以更多。以下展示了前几个示例的输出。为了简洁，我们只显示了列表中的前五项：
- en: '[PRE4]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2.8.5 Model configuration
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.5 模型配置
- en: Most LLMs expose some configuration settings to the user, allowing one to tweak
    how the model operates and its behavior to some extent. While a few parameters
    would change depending on the model implementation, the three key configurations
    are temperature, top probability (`top_p`), and max response. Note that some implementations
    might have a different name but mean the same thing. The OpenAI implementation
    of GPT calls the maximum response as max tokens. Let us explore these in a little
    more detail.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型语言模型（LLM）向用户公开了一些配置设置，允许用户在一定程度上调整模型的操作和行为。虽然一些参数会根据模型实现而变化，但三个关键配置是温度、顶概率（`top_p`）和最大响应。请注意，某些实现可能具有不同的名称，但含义相同。OpenAI对GPT的实现将最大响应称为max
    tokens。让我们更详细地探讨这些设置。
- en: Max response
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大响应
- en: The parameter known as max response essentially defines the upper limit for
    the text length that the model generates. This means that once the model hits
    this predetermined length, it halts text generation, regardless of whether it
    is mid-word or mid-sentence. It’s crucial to grasp this configuration because
    there is a size limit to the tokens most models can process. Increasing this size
    corresponds to heightened computational demands, leading to increased latency
    and cost.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为最大响应的参数实际上定义了模型生成的文本长度的上限。这意味着一旦模型达到这个预定的长度，无论它是否在单词或句子中间，它都会停止文本生成。理解这个配置非常重要，因为大多数模型可以处理的标记大小有限。增加这个大小对应着更高的计算需求，导致延迟和成本的增加。
- en: Temperature
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 温度
- en: When generating text, as with any foundational model, inherent randomness yields
    a different output each time we call the model. Temperature is one of the most
    important settings for controlling the degree of the model’s randomness. Typically,
    this is a value from 0 to 1, with 0 representing a more accurate and predictable
    output. In contrast, setting a 1 makes the output more diverse and random, as
    shown in figure 2.10.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本时，与任何基础模型一样，固有的随机性导致每次调用模型时都会产生不同的输出。温度是控制模型随机程度最重要的设置之一。通常，这是一个从0到1的值，其中0代表更准确和可预测的输出。相反，将值设置为1会使输出更加多样化和随机，如图2.10所示。
- en: '![figure](../Images/CH02_F10_Bahree.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F10_Bahree.png)'
- en: Figure 2.10 Temperature settings and their effect on probability
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 温度设置及其对概率的影响
- en: Top probability (top_p)
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 顶概率（top_p）
- en: The top probability (`top_p`) parameter (also known as nucleus sampling) is
    a setting in language model APIs that steers the randomness of the text-generation
    process. This parameter allows one to fine-tune the balance between creativity
    and reliability in the text that the model generates. It defines a threshold probability;
    only words with probabilities above this threshold are considered when the model
    generates text. When a language model generates text, it predicts the probability
    of each word being the next in the sequence. The `top_p` parameter helps truncate
    this probability distribution to enhance the quality of the generated text.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 顶概率（`top_p`）参数（也称为核采样）是语言模型API中的一个设置，它引导文本生成过程的随机性。此参数允许用户微调模型生成的文本中创造性和可靠性的平衡。它定义了一个阈值概率；当模型生成文本时，只有概率高于此阈值的单词才会被考虑。当语言模型生成文本时，它会预测每个单词在序列中作为下一个单词的概率。`top_p`参数有助于截断这个概率分布，以增强生成文本的质量。
- en: For example, for output generation, setting `top_p` to a lower value (e.g.,
    0.3), the model will only consider the top 30% most probable words for the next
    word in the sequence, as shown in figure 2.11\. This makes the text more predictable
    and less varied. However, if we set `top_p` to a higher value (e.g., 0.9), the
    model will consider a much broader range of words, including those that are less
    likely. This can lead to a more diverse and potentially interesting generation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于输出生成，将`top_p`设置为较低值（例如，0.3），模型将只考虑序列中下一个单词概率最高的前30%的单词，如图2.11所示。这使得文本更加可预测和变化较少。然而，如果我们将`top_p`设置为较高值（例如，0.9），模型将考虑一个更广泛的单词范围，包括那些不太可能的单词。这可能导致更加多样化和可能更有趣的生成。
- en: '![figure](../Images/CH02_F11_Bahree.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F11_Bahree.png)'
- en: Figure 2.11 Example showing how top-p works
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 展示了top-p的工作原理示例
- en: An example
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个示例
- en: 'Let’s show how these settings can be programmatically used. The following code
    snippet shows an example of how to do these configurations with OpenAI. Most of
    these settings are used sparingly, are optional, and would fall to default values.
    Only some options, such as `max_tokens` and `temperature`, are used in almost
    every use case:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示如何程序化地使用这些设置。以下代码片段展示了如何使用OpenAI进行这些配置的示例。大多数这些设置都很少使用，是可选的，并且会默认值。只有一些选项，如`max_tokens`和`temperature`，在几乎每个用例中都会使用：
- en: '[PRE5]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Given that the API is stateless, these settings can differ between different
    instances and API calls of the same instance, depending on the business scenario
    one tries to achieve. Furthermore, there are no globally optimal values for these
    settings, which depend on the task. In general, if you want to have a balanced
    output and not have the model hallucinate much, a setting of 0.7 or 0.8 is good
    for temperature. Table 2.4 outlines configuration settings to control the behavior.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于API是无状态的，这些设置可以在不同实例和同一实例的API调用之间有所不同，具体取决于试图实现的业务场景。此外，这些设置的全球最优值并不存在，它们取决于任务。一般来说，如果您想要一个平衡的输出并且不希望模型产生过多的幻觉，温度设置为0.7或0.8是好的。表2.4概述了控制行为的配置设置。
- en: Table 2.4 LLM configuration settings
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.4 LLM配置设置
- en: '| Configuration | Description |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 描述 |'
- en: '| --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Max tokens  | This sets a limit on the number of tokens per model response.
    Depending on the model, a maximum limit is shared between the prompt (including
    system message, examples, message history, and user query) and the model’s response.  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 最大标记数 | 这为每个模型响应的标记数设置了一个限制。根据模型的不同，最大限制在提示（包括系统消息、示例、消息历史和用户查询）和模型的响应之间共享。
    |'
- en: '| Temperature  | This controls randomness. Lowering the temperature means the
    model produces more repetitive and deterministic responses. Increasing the temperature
    results in more unexpected or creative responses. Try adjusting the temperature
    or `top_p` but not both. Typically, as sequences get longer, the model naturally
    becomes more confident in its predictions, and one can use a much higher temperature
    for long prompts without going off-topic. Conversely, using a high-temperature
    setting on short prompts can lead to unstable outputs.  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 这控制随机性。降低温度意味着模型会产生更多重复和确定性的响应。提高温度会导致更多意外或创造性的响应。尝试调整温度或`top_p`，但不要同时调整。通常，随着序列变长，模型在预测上自然变得更加自信，因此可以在长提示中使用更高的温度而不会离题。相反，在短提示上使用高温设置可能会导致输出不稳定。
    |'
- en: '| Top probability ( `top_p`)  | This is a probability threshold that, like
    temperature, controls randomness but uses a different method. Lowering `top_p`
    narrows the model’s token selection to the most likely tokens and ignores the
    long tail of less likely tokens. Increasing `top_p` will allow the model to choose
    from high- and low-likelihood tokens. Try adjusting either temperature or `top_p,`
    but not both.  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Top概率（`top_p`） | 这是一个概率阈值，类似于温度，控制随机性，但使用不同的方法。降低`top_p`将缩小模型标记选择的范围，只选择最可能的标记，并忽略不太可能的标记的长尾。提高`top_p`将允许模型从高概率和低概率的标记中进行选择。尝试调整温度或`top_p`，但不要同时调整。
    |'
- en: '| Stop sequences  | This makes the model end its response at a desired point.
    The model response will end before the specified sequence so that it won’t contain
    the stop-sequence text.  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 停止序列 | 这使得模型在期望的点结束其响应。模型响应将在指定的序列之前结束，这样它就不会包含停止序列文本。 |'
- en: '| Frequency penalty  | This reduces the chance of repeating a token proportionally
    based on how often it has appeared in the text. This decreases the likelihood
    of repeating the same text in response.  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 频率惩罚 | 这根据标记在文本中出现的频率成比例地减少了重复标记的可能性。这降低了在回复中重复相同文本的可能性。 |'
- en: '| Presence penalty  | This reduces the chance of repeating any token that has
    appeared in the text so far. This increases the likelihood of introducing new
    topics in a response.  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 出现惩罚 | 这减少了重复之前文本中已出现任何标记的可能性。这增加了在回复中引入新主题的可能性。 |'
- en: 2.8.6 Context window
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.6 上下文窗口
- en: A context window is a relatively new and very important concept. It refers to
    the range of tokens or words surrounding a particular word or token that an LLM
    considers when making predictions. The context window helps the model understand
    the dependencies and relationships between the words, enabling it to generate
    more accurate and coherent predictions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文窗口是一个相对较新且非常重要的概念。它指的是LLM在做出预测时考虑的特定单词或标记周围的标记或词语的范围。上下文窗口帮助模型理解词语之间的依赖关系和关系，使其能够生成更准确和连贯的预测。
- en: For example, when predicting the next word in a sentence, the context window
    might include several words preceding the target word. The context window size
    may vary depending on the model and its architecture. In LLMs, context windows
    can be quite large, allowing the model to capture long-range dependencies and
    intricate semantic relationships among the words. These longer-context windows
    can help get better output for tasks such as text generation, translation, and
    summarization.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在预测句子中的下一个单词时，上下文窗口可能包括几个在目标单词之前出现的单词。上下文窗口的大小可能因模型及其架构而异。在LLM中，上下文窗口可以相当大，允许模型捕捉到词语之间的长距离依赖关系和复杂的语义关系。这些较长的上下文窗口可以帮助在文本生成、翻译和摘要等任务上获得更好的输出。
- en: The current LLM architecture limits the context window size to several thousand
    tokens. Although some of the newer models support up to a million tokens, the
    context window is still a critical focal point, mainly because the global nature
    of the attention mechanism imposes computational costs that are quadratic in context
    length. In other words, the bigger the context windows, the more the computation
    cost is proportional to the square of the input data size. While having a larger
    context window might seem good, it is important to understand that it has both
    positive and negative implications. With larger context windows, the performance
    of the model is much slower in terms of both understanding the request and the
    generation, with a higher latency. While we might feel like a longer context would
    be better, in general, use a smaller window if that would suffice for the task
    at hand—it would have a better performance than the larger one. Some of the pros
    are
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的LLM架构将上下文窗口大小限制在几千个标记。尽管一些较新的模型支持高达一百万个标记，但上下文窗口仍然是一个关键的关注点，主要是因为注意力机制的全球性质导致计算成本与上下文长度呈二次方关系。换句话说，上下文窗口越大，计算成本与输入数据大小的平方成正比。虽然拥有更大的上下文窗口可能看起来很好，但重要的是要理解它既有积极的一面，也有消极的一面。拥有更大的上下文窗口，模型的性能在理解和生成方面都会变得较慢，并且具有更高的延迟。虽然我们可能觉得更长的上下文会更好，但通常情况下，如果小窗口足以完成当前任务，则使用较小的窗口会更好——它的性能会比大窗口更好。其中一些优点是
- en: '*Improved comprehension of context*—A longer context window allows the model
    to capture long-range dependencies and intricate semantic relationships among
    words, resulting in better predictions and more coherent text generation. However,
    this comes at a considerable cost and should be used judiciously.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提高对上下文的理解*—更长的上下文窗口允许模型捕捉到长距离依赖关系和词语之间复杂的语义关系，从而实现更好的预测和更连贯的文本生成。然而，这需要付出相当大的代价，并且应该谨慎使用。'
- en: '*Better performance at complex tasks*—With a more extensive context window,
    language models can perform better at tasks that require a better comprehension
    of the broader context, such as machine translation, summarization, and sentiment
    analysis.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在复杂任务中的更好表现*—拥有更广泛的上下文窗口，语言模型在需要更好理解更广泛上下文的任务上表现更佳，例如机器翻译、摘要和情感分析。'
- en: 'Here are the cons:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些缺点：
- en: '*Increased computational requirements*—A longer context window requires more
    memory and computational power to process and store the additional information,
    which can result in longer training and inference times and require more powerful
    hardware or distributed computing solutions.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增加的计算需求*—更长的上下文窗口需要更多的内存和计算能力来处理和存储额外的信息，这可能导致更长的训练和推理时间，并需要更强大的硬件或分布式计算解决方案。'
- en: '*Potential for overfittin*g—As the context window increases, the model becomes
    more complex and prone to overfitting, especially if the training data is limited.
    Overfitting occurs when the model learns to perform exceptionally well on the
    training data but struggles to generalize new and unseen data.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过拟合的潜力*—随着上下文窗口的增加，模型变得更加复杂，更容易过拟合，尤其是在训练数据有限的情况下。过拟合发生在模型学会在训练数据上表现出色，但在处理新的和未见过的数据时遇到困难。'
- en: '*Difficulty handling very long sequences*—Although longer context windows can
    improve performance, they may also introduce challenges when processing extremely
    long sequences. Some models may struggle to maintain the necessary information
    over such long distances, leading to a degradation in performance.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理非常长序列的困难*—尽管较长的上下文窗口可以提高性能，但它们在处理极长序列时也可能带来挑战。一些模型可能难以在如此长的距离上保持必要的信息，从而导致性能下降。'
- en: '*Diminishing returns*—While increasing the context window size can improve
    performance, there may be a point of diminishing returns where further increases
    yield only marginal improvements. It’s essential to balance the context window
    size with the computational cost and model complexity to achieve optimal performance.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*收益递减*—虽然增加上下文窗口大小可以提高性能，但可能存在一个收益递减的点，进一步增加只会带来微小的改进。在达到最佳性能时，平衡上下文窗口大小、计算成本和模型复杂性是至关重要的。'
- en: 'The context window as a concept is quite important for several reasons:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文窗口作为一个概念，对于几个原因来说非常重要：
- en: '*Captures dependencies*—A context window allows the model to understand relationships
    between words, phrases, or sentences within a text. This helps the model grasp
    the overall meaning and context of the input.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*捕捉依赖关系*—上下文窗口允许模型理解文本中单词、短语或句子之间的关系。这有助于模型掌握输入的整体意义和上下文。'
- en: '*Improved predictions*—This is probably what most of us see when using LLMs,
    where the context window enables the model to generate more accurate and coherent
    suggestions based on the preceding text.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提高预测准确性*—这可能是我们使用LLMs时看到的大部分情况，其中上下文窗口使模型能够根据前面的文本生成更准确和连贯的建议。'
- en: '*Provides context for better understanding*—By considering the context window,
    LLMs can better understand the context at play and, as a result, the syntactical
    structure and semantic relationships in the text; this allows for more accurate
    language understanding and generation.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提供更好的理解背景*—通过考虑上下文窗口，大型语言模型（LLMs）可以更好地理解正在发挥作用的上下文，从而在文本中理解句法和语义关系；这有助于更准确的语言理解和生成。'
- en: 2.8.7 Prompt engineering
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.7 提示工程
- en: Prompt engineering is a relatively new field that involves curating or designing
    prompts to elicit desired responses or behaviors from a machine learning model,
    particularly LLMs. It is a powerful technique that can improve the performance
    of language models on various tasks. Prompt engineering is an emerging field that
    requires creativity and attention to detail.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是一个相对较新的领域，涉及整理或设计提示以从机器学习模型（尤其是LLMs）中引发期望的响应或行为。这是一种强大的技术，可以提高语言模型在各种任务上的性能。提示工程是一个新兴领域，需要创造力和对细节的关注。
- en: Prompt engineering can be seen as both an art and a science. It involves careful
    selection and phrasing of input commands to help guide the AI in producing the
    desired output. These input commands can be as simple as selecting the right words,
    phrases, and formats to guide the model in generating high-quality and relevant
    texts for a specific task.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程可以被视为一种艺术和科学。它涉及仔细选择和措辞输入命令，以帮助引导AI产生期望的输出。这些输入命令可以简单到选择正确的单词、短语和格式，以引导模型为特定任务生成高质量和相关的文本。
- en: For instance, to get a detailed answer, you might use a prompt such as “Explain
    in detail …,” or to get a quick summary, you might use “Summarize in a few bullets
    … ” Similarly, to make the AI write in the style of a particular author, you might
    use a prompt like “Write a passage as if you were P. G. Wodehouse.”
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了得到详细的答案，你可能使用这样的提示：“详细解释……”，或者为了得到快速总结，你可能使用“用几个要点总结……”。同样，为了使AI以特定作者的风格写作，你可能使用这样的提示：“以P.
    G. Wodehouse的笔触写一段文字。”
- en: Prompt engineering requires understanding the AI model’s capabilities, underlying
    training data, and how it responds to different kinds of input. Effective prompt
    engineering can significantly improve the usefulness of AI models in various tasks.
    Note that this section is just an introduction to prompt engineering as a fundamental
    concept; we will cover prompt engineering in more depth later in chapter 6.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程需要理解AI模型的能力、底层训练数据和它对不同类型输入的反应方式。有效的提示工程可以显著提高AI模型在各种任务中的实用性。请注意，本节只是对提示工程作为一个基本概念进行介绍；我们将在第6章中更深入地探讨提示工程。
- en: 2.8.8 Model adaptation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.8 模型适应性
- en: LLMs are pretrained and general-purpose, and sometimes they must be fine-tuned.
    They are trained on a large corpus of text data and can be used as a starting
    point for training on a smaller dataset for a specific task. Fine-tuning the base
    LLM on the smaller dataset can improve its performance for that specific task.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是预训练和通用的，有时它们必须进行微调。它们在大量文本数据集上训练，可以作为在较小数据集上训练特定任务的起点。在较小数据集上微调基础LLM可以提高其在该特定任务上的表现。
- en: Fine-tuning is taking a pretrained model and training it further on a new task
    or dataset. The pretrained model is used as a starting point, and the weights
    of the model are adjusted during training to improve its performance on the new
    task. Fine-tuning is often used in transfer learning, where a model trained on
    one task is adapted to another related task.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是将预训练模型进一步训练在新任务或数据集上。预训练模型被用作起点，并在训练过程中调整模型的权重以提高其在新任务上的表现。微调常用于迁移学习，其中在一个任务上训练的模型被适应到另一个相关任务。
- en: 'Some examples of fine-tuning LLMs include the following:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs的一些例子包括以下内容：
- en: '*Text classification*—Fine-tuning an LLM on a specific text classification
    task, such as sentiment analysis or spam detection'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本分类*——在特定的文本分类任务上微调LLM，例如情感分析或垃圾邮件检测'
- en: '*Question answering*—Fine-tuning an LLM on a specific question-answering task,
    such as answering questions about a specific topic'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答*——在特定的问答任务上微调LLM，例如回答关于特定主题的问题'
- en: '*Language generation*—Fine-tuning an LLM on a specific language generation
    task, such as generating summaries or translations'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言生成*——在特定的语言生成任务上微调LLM，例如生成摘要或翻译'
- en: Fine-tuning an LLM such as GPT-3.5 Turbo can be a powerful way to customize
    the model for specific tasks, but it can also be very expensive and should be
    one of the last options. In some cases, fine-tuning can also lead to catastrophic
    forgetting. This occurs when the model is fine-tuned on a new dataset, causing
    it to forget the knowledge it had learned from its original training data, resulting
    in the fine-tuned model losing its reasoning skills. We will cover model adaptation
    in more detail in chapter 9, including any pitfalls of fine-tuning.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM，如GPT-3.5 Turbo，可以是一种强大的方式来为特定任务定制模型，但它也可能非常昂贵，应该是最后的选项之一。在某些情况下，微调也可能导致灾难性遗忘。这发生在模型在新数据集上微调时，导致它忘记从原始训练数据中学到的知识，从而导致微调模型失去推理能力。我们将在第9章更详细地介绍模型适应，包括微调的任何陷阱。
- en: 2.8.9 Emergent behavior
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.9 演化行为
- en: The concept of emergent behavior defines the significance of foundation models
    and LLMs. Emergent behavior implies unexpected behaviors exhibited by LLMs when
    interacting with their environment, specifically when trained with large amounts
    of data. LLMs are not explicitly trained to have these abilities but learn them
    by observing natural language. The emergence of the behavior is implicitly induced
    rather than explicitly constructed; it is both the source of scientific excitement
    and anxiety about unanticipated consequences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 演化行为的概念定义了基础模型和大型语言模型（LLMs）的重要性。演化行为指的是LLMs在与环境交互时表现出的意外行为，特别是在用大量数据进行训练时。LLMs并非明确训练以具备这些能力，而是通过观察自然语言来学习它们。行为的出现是隐式诱导的，而不是显式构建的；它既是科学兴奋的源泉，也是对未预见到后果的焦虑。
- en: As figure 2.12 shows, a model’s performance on multiple natural language benchmarks
    (e.g., question answering) is no better than random chance until they reach a
    certain scale, measured in this example by training computation in FLOPs. The
    model’s performance sharply increases at this point, which is an example of emergent
    abilities. This helps us understand that emergent properties of LLMs are not present
    in smaller models. Furthermore, these abilities show up only at a scale when the
    model’s size reaches a certain threshold. Emergent properties cannot be predicted
    by extrapolating the performance of smaller models.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.12所示，一个模型在多个自然语言基准测试（例如，问答）上的表现并不比随机机会好，直到它们达到一定的规模，在本例中是通过训练计算量（以FLOPs衡量）。在这个点上，模型的表现急剧上升，这是演化能力的例子。这有助于我们理解LLMs的演化属性并不存在于较小的模型中。此外，这些能力只在模型规模达到一定阈值时才会出现。演化属性不能通过外推较小模型的表现来预测。
- en: '![figure](../Images/CH02_F12_Bahree.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F12_Bahree.png)'
- en: Figure 2.12 Model scaling as measured by training computation requirements in
    FLOP
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12 模型规模按训练计算需求（FLOP）衡量
- en: Let us see an example using code, as the exact logic and nature of what a piece
    of code is doing cannot be predetermined or programmed for an LLM. It needs to
    rely on the emergent behavior property of foundational models to understand, analyze,
    and comprehend the code.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过代码示例来看看，因为代码的确切逻辑和性质无法预先确定或为 LLM 编程。它需要依赖基础模型的涌现行为特性来理解、分析和理解代码。
- en: Say we have C code that reverses a string, as shown in the following listing.
    This uses some of C’s more advanced features, such as pointers, XOR operators,
    and macros. In our example, we use GPT-4 to convert this C function to Python.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有如下所示的反转字符串的 C 代码。这使用了 C 的一些更高级的特性，如指针、XOR 运算符和宏。在我们的例子中，我们使用 GPT-4 将这个
    C 函数转换为 Python。
- en: Listing 2.4 Function written in C to reverse a string
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.4 用于反转字符串的 C 语言函数
- en: '[PRE6]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first step would be to ask our LLM to convert this code to Python using
    a prompt similar to the next one. We are omitting the C function (listing 2.4)
    again for brevity. The code is part of the prompt.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将是要求我们的语言模型使用类似于下一个提示的提示将此代码转换为 Python。为了简洁起见，我们再次省略了 C 函数（列表 2.4）。代码是提示的一部分。
- en: '**![image](../Images/Prompt.png)**Convert this C function to Python:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt.png)**将此 C 函数转换为 Python：'
- en: '[PRE7]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Listing 2.5 shows the output from our model with the code converted to Python.
    It does a good job of understanding what the C function is doing and outlines
    Python’s different constraints and properties in the response. Note that we did
    not give it any hints as to what the function is doing. It deciphered the `while`
    loop itself, showing an example of emergent behavior.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 展示了将代码转换为 Python 后我们模型的输出。它很好地理解了 C 函数的执行过程，并在响应中概述了 Python 的不同约束和特性。请注意，我们没有给它任何关于函数执行方式的提示。它自己解析了
    `while` 循环，展示了涌现行为的例子。
- en: '**![image](../Images/Response.png)**This function will return the reversed
    string rather than modify the original string because Python strings are immutable.
    In other words, once a string is created in Python, it cannot be changed. Hence,
    the function `inplace_reverse` is a bit of a misnomer in this Python context.
    You could consider renaming it to something like `reverse_string`.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**这个函数将返回反转后的字符串而不是修改原始字符串，因为 Python 字符串是不可变的。换句话说，一旦在
    Python 中创建了一个字符串，它就不能被更改。因此，在这个 Python 上下文中，函数 `inplace_reverse` 有点名不副实。你可以考虑将其重命名为类似
    `reverse_string` 的名称。'
- en: Listing 2.5 Converted Python function for string reversal
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.5 转换为 Python 的字符串反转函数
- en: '[PRE8]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can simplify this further by asking the model the following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过询问模型以下内容来进一步简化它：
- en: '**![image](../Images/Prompt.png)**Can you further simplify and optimize the
    Python code?'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt.png)**你能进一步简化并优化 Python 代码吗？'
- en: '**![image](../Images/Response.png)**Sure! One of the simplest ways to reverse
    a string in Python uses slicing. Here’s a function that will return the reverse
    of a string:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**当然可以！在 Python 中反转字符串最简单的方法之一是使用切片。以下是一个返回字符串反转的函数：'
- en: '[PRE9]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `[::-1]` is a slice that starts at the end of the string and moves backward.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`[::-1]` 是一个从字符串末尾开始并向后移动的切片。'
- en: 'However, if you want to emulate the same logic as the C program, that is, reversing
    a string in place using two pointers, here is a more Pythonic way of achieving
    this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想模拟 C 程序中的相同逻辑，即使用两个指针就地反转字符串，这里有一个更 Pythonic 的实现方式：
- en: '[PRE10]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this version, we convert the string to a list, reverse the list in place
    using the built-in `reverse` function, and then join the list back into a string.
    The `reverse` function in Python uses a similar two-pointer approach, but it’s
    all under the hood, so the code looks much simpler.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本中，我们将字符串转换为列表，使用内置的 `reverse` 函数就地反转列表，然后将列表重新连接成字符串。Python 中的 `reverse`
    函数使用类似的两个指针方法，但这一切都在幕后进行，所以代码看起来要简单得多。
- en: The point the response calls out on a similar two-pointer approach demonstrates
    the emergent property’s power. We never explicitly called this out anywhere in
    the conversation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 响应中指出的类似两个指针方法展示了涌现特性的强大。我们从未在对话的任何地方明确指出这一点。
- en: In this chapter, we looked at the core concepts that make LLMs work and what
    we need to be mindful of when implementing these new generative AI techniques
    in the enterprise. We cover new concepts such as prompts, prompt engineering,
    embeddings, tokens, model parameters, context windows, transformer architecture,
    and emergent behavior. Now that we have a broad understanding of LLMs and their
    implications in the next chapter, let us put this into action and work through
    an API to generate text, both as completions where we provide instructions via
    a prompt and as chat where we can have a dialogue, and in that, generate text,
    in the context of the conversation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使LLMs工作核心概念，以及在我们将新的生成式AI技术在企业中实施时需要注意的事项。我们涵盖了新的概念，如提示、提示工程、嵌入、标记、模型参数、上下文窗口、Transformer架构和涌现行为。现在，我们已经对LLMs及其在下一章中的影响有了广泛的理解，让我们将其付诸实践，通过API生成文本，包括通过提示提供指令的补全和可以进行对话的聊天，并在对话的上下文中生成文本。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) represent a major advancement in AI. They are trained
    on vast amounts of text data to learn patterns in human language.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）代表了人工智能领域的一项重大进步。它们通过在大量文本数据上训练来学习人类语言的模式。
- en: LLMs are general-purpose and can handle tasks without task-specific training
    data, such as answering questions, writing essays, summarizing texts, translating
    languages, and generating code.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs是通用型的，可以处理不需要特定任务训练数据的任务，例如回答问题、撰写文章、总结文本、翻译语言和生成代码。
- en: Key LLM use cases include summarization, classification, Q&A/chatbots, content
    generation, data analysis, translation and localization, process automation, research
    and development, sentiment analysis, and entity extraction.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的LLM应用案例包括摘要、分类、问答/聊天机器人、内容生成、数据分析、翻译和本地化、流程自动化、研发、情感分析和实体提取。
- en: Types of LLMs include base, instruction-based, and fine-tuned LLM. Each has
    pros and cons and is powered by foundational models.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs的类型包括基础型、基于指令的和微调型LLM。每种类型都有其优缺点，并由基础模型驱动。
- en: Foundational models are large AI models trained on vast quantities of data at
    a massive scale, resulting in models that can be adapted to a wide range of downstream
    tasks.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型是在大规模数据上训练的大型AI模型，这使得模型能够适应广泛的下游任务。
- en: Some key LLM concepts include prompts, prompt engineering, embeddings, tokens,
    model parameters, context windows, transformer architecture, and emergent behavior.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些关键的LLM概念包括提示、提示工程、嵌入、标记、模型参数、上下文窗口、Transformer架构和涌现行为。
- en: Open source and commercial LLMs have advantages and disadvantages, with commercial
    models typically offering state-of-the-art performance and open source models
    providing more flexibility for customization and integration.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源和商业LLMs各有优缺点，商业模型通常提供最先进的性能，而开源模型则提供更多定制和集成的灵活性。
- en: Small language models (SLMs) are a new emerging trend of lightweight generative
    AI models that produce text, summarize documents, translate languages, and answer
    questions. In some cases, they offer capabilities similar to those of larger models.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型语言模型（SLMs）是新兴的轻量级生成式AI模型的新趋势，可以生成文本、总结文档、翻译语言和回答问题。在某些情况下，它们提供的能力与较大模型相似。
