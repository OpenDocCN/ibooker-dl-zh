- en: 6 Using a neural network to fit the data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 使用神经网络拟合数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Nonlinear activation functions as the key difference compared with linear models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与线性模型相比，非线性激活函数是关键区别
- en: Working with PyTorch’s `nn` module
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch的`nn`模块
- en: Solving a linear-fit problem with a neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络解决线性拟合问题
- en: So far, we’ve taken a close look at how a linear model can learn and how to
    make that happen in PyTorch. We’ve focused on a very simple regression problem
    that used a linear model with only one input and one output. Such a simple example
    allowed us to dissect the mechanics of a model that learns, without getting overly
    distracted by the implementation of the model itself. As we saw in the overview
    diagram in chapter 5, figure 5.2 (repeated here as figure 6.1), the exact details
    of a model are not needed to understand the high-level process that trains the
    model. Backpropagating errors to parameters and then updating those parameters
    by taking the gradient with respect to the loss is the same no matter what the
    underlying model is.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经仔细研究了线性模型如何学习以及如何在PyTorch中实现这一点。我们专注于一个非常简单的回归问题，使用了一个只有一个输入和一个输出的线性模型。这样一个简单的例子使我们能够剖析一个学习模型的机制，而不会过于分散注意力于模型本身的实现。正如我们在第5章概述图中看到的，图5.2（这里重复为图6.1），了解训练模型的高级过程并不需要模型的确切细节。通过将错误反向传播到参数，然后通过对损失的梯度更新这些参数，无论底层模型是什么，这个过程都是相同的。
- en: '![](../Images/CH06_F01_Stevens2_GS.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F01_Stevens2_GS.png)'
- en: Figure 6.1 Our mental model of the learning process, as implemented in chapter
    5
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 我们在第5章中实现的学习过程的心理模型
- en: 'In this chapter, we will make some changes to our model architecture: we’re
    going to implement a full artificial neural network to solve our temperature-conversion
    problem. We’ll continue using our training loop from the last chapter, along with
    our Fahrenheit-to-Celsius samples split into training and validation sets. We
    could start to use a quadratic model: rewriting `model` as a quadratic function
    of its input (for example, `y = a * x**2 + b * x + c`). Since such a model would
    be differentiable, PyTorch would take care of computing gradients, and the training
    loop would work as usual. That wouldn’t be too interesting for us, though, because
    we would still be fixing the shape of the function.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将对我们的模型架构进行一些更改：我们将实现一个完整的人工神经网络来解决我们的温度转换问题。我们将继续使用上一章的训练循环，以及我们将华氏度转换为摄氏度的样本分为训练集和验证集。我们可以开始使用一个二次模型：将`model`重写为其输入的二次函数（例如，`y
    = a * x**2 + b * x + c`）。由于这样的模型是可微的，PyTorch会负责计算梯度，训练循环将像往常一样工作。然而，对我们来说这并不是太有趣，因为我们仍然会固定函数的形状。
- en: This is the chapter where we begin to hook together the foundational work we’ve
    put in and the PyTorch features you’ll be using day in and day out as you work
    on your projects. You’ll gain an understanding of what’s going on underneath the
    porcelain of the PyTorch API, rather than it just being so much black magic. Before
    we get into the implementation of our new model, though, let’s cover what we mean
    by *artificial neural network*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们开始将我们的基础工作和您在项目中每天使用的PyTorch功能连接在一起的章节。您将了解PyTorch API背后的工作原理，而不仅仅是黑魔法。然而，在我们进入新模型的实现之前，让我们先了解一下*人工神经网络*的含义。
- en: 6.1 Artificial neurons
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 人工神经元
- en: 'At the core of deep learning are neural networks: mathematical entities capable
    of representing complicated functions through a composition of simpler functions.
    The term *neural network* is obviously suggestive of a link to the way our brain
    works. As a matter of fact, although the initial models were inspired by neuroscience,[¹](#pgfId-1011901)
    modern artificial neural networks bear only a slight resemblance to the mechanisms
    of neurons in the brain. It seems likely that both artificial and physiological
    neural networks use vaguely similar mathematical strategies for approximating
    complicated functions because that family of strategies works very effectively.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心是神经网络：能够通过简单函数的组合表示复杂函数的数学实体。术语*神经网络*显然暗示了与我们大脑工作方式的联系。事实上，尽管最初的模型受到神经科学的启发，现代人工神经网络与大脑中神经元的机制几乎没有相似之处。人工和生理神经网络似乎使用了略有相似的数学策略来逼近复杂函数，因为这类策略非常有效。
- en: '*Note* We are going to drop the *artificial* and refer to these constructs
    as just *neural networks* from here forward.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 从现在开始，我们将放弃*人工*这个词，将这些构造称为*神经网络*。'
- en: The basic building block of these complicated functions is the *neuron*, as
    illustrated in figure 6.2\. At its core, it is nothing but a linear transformation
    of the input (for example, multiplying the input by a number [the *weight*] and
    adding a constant [the *bias*]) followed by the application of a fixed nonlinear
    function (referred to as the *activation function*).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些复杂函数的基本构建块是*神经元*，如图6.2所示。在其核心，它只是输入的线性变换（例如，将输入乘以一个数字[权重]并加上一个常数[偏置]）后跟一个固定的非线性函数（称为*激活函数）。
- en: '![](../Images/CH06_F02_Stevens2_GS.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F02_Stevens2_GS.png)'
- en: 'Figure 6.2 An artificial neuron: a linear transformation enclosed in a nonlinear
    function'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 人工神经元：包含在非线性函数中的线性变换
- en: Mathematically, we can write this out as *o* = *f*(*w* * *x* + *b*), with *x*
    as our input, *w* our weight or scaling factor, and *b* as our bias or offset.
    *f* is our activation function, set to the hyperbolic tangent, or `tanh` function
    here. In general, *x* and, hence, *o* can be simple scalars, or vector-valued
    (meaning holding many scalar values); and similarly, *w* can be a single scalar
    or matrix, while *b* is a scalar or vector (the dimensionality of the inputs and
    weights must match, however). In the latter case, the previous expression is referred
    to as a *layer* of neurons, since it represents many neurons via the multidimensional
    weights and biases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以将其写为*o* = *f*(*w* * *x* + *b*)，其中*x*是我们的输入，*w*是我们的权重或缩放因子，*b*是我们的偏置或偏移。*f*是我们的激活函数，这里设置为双曲正切函数，或者`tan`函数。一般来说，*x*和因此*o*可以是简单的标量，或者是矢量值（表示许多标量值）；类似地，*w*可以是单个标量或矩阵，而*b*是标量或矢量（然而，输入和权重的维度必须匹配）。在后一种情况下，前面的表达式被称为一个神经元*层*，因为它通过多维权重和偏置表示许多神经元。
- en: 6.1.1 Composing a multilayer network
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 组合多层网络
- en: A multilayer neural network, as represented in figure 6.3, is made up of a composition
    of functions like those we just discussed
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如图6.3所示，一个多层神经网络由我们刚刚讨论的函数组合而成
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where the output of a layer of neurons is used as an input for the following
    layer. Remember that `w_0` here is a matrix, and `x` is a vector! Using a vector
    allows `w_0` to hold an entire *layer* of neurons, not just a single weight.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元层的输出被用作下一层的输入。请记住，这里的`w_0`是一个矩阵，而`x`是一个向量！使用向量允许`w_0`保存整个*层*的神经元，而不仅仅是一个单独的权重。
- en: '![](../Images/CH06_F03_Stevens2_GS.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_Stevens2_GS.png)'
- en: Figure 6.3 A neural network with three layers
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 一个具有三层的神经网络
- en: 6.1.2 Understanding the error function
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 理解误差函数
- en: An important difference between our earlier linear model and what we’ll actually
    be using for deep learning is the shape of the error function. Our linear model
    and error-squared loss function had a convex error curve with a singular, clearly
    defined minimum. If we were to use other methods, we could solve for the parameters
    minimizing the error function automatically and definitively. That means that
    our parameter updates were attempting to *estimate* that singular correct answer
    as best they could.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的线性模型和我们实际用于深度学习的模型之间的一个重要区别是误差函数的形状。我们的线性模型和误差平方损失函数具有凸误差曲线，具有一个明确定义的最小值。如果我们使用其他方法，我们可以自动和明确地解决最小化误差函数的参数。这意味着我们的参数更新试图*估计*那个明确的正确答案。
- en: Neural networks do not have that same property of a convex error surface, even
    when using the same error-squared loss function! There’s no single right answer
    for each parameter we’re attempting to approximate. Instead, we are trying to
    get all of the parameters, when acting *in concert*, to produce a useful output.
    Since that useful output is only going to *approximate* the truth, there will
    be some level of imperfection. Where and how imperfections manifest is somewhat
    arbitrary, and by implication the parameters that control the output (and, hence,
    the imperfections) are somewhat arbitrary as well. This results in neural network
    training looking very much like parameter estimation from a mechanical perspective,
    but we must remember that the theoretical underpinnings are quite different.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用相同的误差平方损失函数，神经网络也不具有凸误差曲面的属性！对于我们试图逼近的每个参数，没有一个单一的正确答案。相反，我们试图让所有参数在*协同作用*下产生一个有用的输出。由于这个有用的输出只会*近似*真相，所以会有一定程度的不完美。不完美会在何处和如何显现在某种程度上是任意的，因此控制输出（因此也是不完美）的参数也是任意的。这导致神经网络训练在机械角度上看起来非常像参数估计，但我们必须记住理论基础是完全不同的。
- en: A big part of the reason neural networks have non-convex error surfaces is due
    to the activation function. The ability of an ensemble of neurons to approximate
    a very wide range of useful functions depends on the combination of the linear
    and nonlinear behavior inherent to each neuron.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有非凸误差曲面的一个重要原因是激活函数。一组神经元能够逼近非常广泛的有用函数的能力取决于每个神经元固有的线性和非线性行为的组合。
- en: 6.1.3 All we need is activation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 我们只需要激活
- en: 'As we have seen, the simplest unit in (deep) neural networks is a linear operation
    (scaling + offset) followed by an activation function. We already had our linear
    operation in our latest model--the linear operation *was* the entire model. The
    activation function plays two important roles:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，(深度)神经网络中最简单的单元是线性操作（缩放 + 偏移）后跟一个激活函数。我们在我们最新的模型中已经有了我们的线性操作--线性操作*就是*整个模型。激活函数发挥着两个重要的作用：
- en: In the inner parts of the model, it allows the output function to have different
    slopes at different values--something a linear function by definition cannot do.
    By trickily composing these differently sloped parts for many outputs, neural
    networks can approximate arbitrary functions, as we will see in section 6.1.6.[²](#pgfId-1012479)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型的内部部分，它允许输出函数在不同值处具有不同的斜率--这是线性函数根据定义无法做到的。通过巧妙地组合这些具有不同斜率的部分来产生许多输出，神经网络可以逼近任意函数，正如我们将在第6.1.6节中看到的。
- en: At the last layer of the network, it has the role of concentrating the outputs
    of the preceding linear operation into a given range.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络的最后一层，它的作用是将前面的线性操作的输出集中到给定范围内。
- en: Let’s talk about what the second point means. Pretend that we’re assigning a
    “good doggo” score to images. Pictures of retrievers and spaniels should have
    a high score, while images of airplanes and garbage trucks should have a low score.
    Bear pictures should have a lowish score, too, although higher than garbage trucks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈第二点的含义。假设我们正在为图像分配“好狗狗”分数。金毛猎犬和西班牙猎犬的图片应该有一个高分，而飞机和垃圾车的图片应该有一个低分。熊的图片也应该有一个较低的分数，尽管比垃圾车高。
- en: 'The problem is, we have to define a “high score”: we’ve got the entire range
    of `float32` to work with, and that means we can go pretty high. Even if we say
    “it’s a 10-point scale,” there’s still the issue that sometimes our model is going
    to produce a score of 11 out of 10\. Remember that under the hood, it’s all sums
    of (`w*x+b`) matrix multiplications, and those won’t naturally limit themselves
    to a specific range of outputs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，我们必须定义一个“高分”：我们有整个`float32`范围可供使用，这意味着我们可以得到相当高的分数。即使我们说“这是一个10分制”，仍然存在一个问题，即有时我们的模型会产生11分中的11分。请记住，在底层，这都是(`w*x+b`)矩阵乘法的总和，它们不会自然地限制自己在特定范围的输出。
- en: Capping the output range
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制输出范围
- en: We want to firmly constrain the output of our linear operation to a specific
    range so that the consumer of this output doesn’t have to handle numerical inputs
    of puppies at 12/10, bears at -10, and garbage trucks at -1,000.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望牢固地约束我们线性操作的输出到特定范围，这样输出的消费者就不必处理小狗得分为12/10，熊得分为-10，垃圾车得分为-1,000的数值输入。
- en: 'One possibility is to just cap the output values: anything below 0 is set to
    0, and anything above 10 is set to 10\. That’s a simple activation function called
    `torch.nn.Hardtanh` ([https://pytorch.org/docs/stable/nn.html#hardtanh](https://pytorch.org/docs/stable/nn.html#hardtanh),
    but note that the default range is -1 to +1).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能性是简单地限制输出数值：低于0的设为0，高于10的设为10。这是一个简单的激活函数称为`torch.nn.Hardtanh`（[https://pytorch.org/docs/stable/nn.html#hardtanh](https://pytorch.org/docs/stable/nn.html#hardtanh)，但请注意默认范围是-1到+1）。
- en: Compressing the output range
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩输出范围
- en: Another family of functions that work well is `torch.nn.Sigmoid`, which includes
    `1 / (1 + e ** -x)`, `torch.tanh`, and others that we’ll see in a moment. These
    functions have a curve that asymptotically approaches 0 or -1 as *x* goes to negative
    infinity, approaches 1 as *x* increases, and have a mostly constant slope at *x*
    == *0*. Conceptually, functions shaped this way work well because there’s an area
    in the middle of our linear function’s output that our neuron (which, again, is
    just a linear function followed by an activation) will be sensitive to, while
    everything else gets lumped next to the boundary values. As we can see in figure
    6.4, our garbage truck gets a score of -0.97, while bears and foxes and wolves
    end up somewhere in the -0.3 to 0.3 range.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组效果良好的函数是`torch.nn.Sigmoid`，其中包括`1 / (1 + e ** -x)`，`torch.tanh`，以及我们马上会看到的其他函数。这些函数的曲线在*x*趋于负无穷时渐近地接近0或-1，在*x*增加时接近1，并且在*x*
    == *0*时具有大致恒定的斜率。从概念上讲，这种形状的函数效果很好，因为我们线性函数输出的中间区域是我们的神经元（再次强调，这只是一个线性函数后跟一个激活函数）会敏感的区域，而其他所有内容都被归类到边界值旁边。正如我们在图6.4中看到的，我们的垃圾车得分为-0.97，而熊、狐狸和狼的得分则在-0.3到0.3的范围内。
- en: 'This results in garbage trucks being flagged as “not dogs,” our good dog mapping
    to “clearly a dog,” and our bear ending up somewhere in the middle. In code, we
    can see the exact values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致垃圾车被标记为“不是狗”，我们的好狗被映射为“明显是狗”，而我们的熊则处于中间位置。在代码中，我们可以看到确切的数值：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Garbage truck
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 垃圾车
- en: ❷ Bear
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 熊
- en: ❸ Good doggo
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 好狗狗
- en: 'With the bear in the sensitive range, small changes to the bear will result
    in a noticeable change to the result. For example, we could switch from a grizzly
    to a polar bear (which has a vaguely more traditionally canine face) and see a
    jump up the *Y*-axis as we slide toward the “very much a dog” end of the graph.
    Conversely, a koala bear would register as less dog-like, and we would see a drop
    in the activated output. There isn’t much we could do to the garbage truck to
    make it register as dog-like, though: even with drastic changes, we might only
    see a shift from -0.97 to -0.8 or so.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当熊处于敏感范围时，对熊进行微小的更改将导致结果明显变化。例如，我们可以从灰熊切换到北极熊（其面部略带更传统的犬类面孔），随着我们滑向图表“非常像狗”的一端，我们会看到*Y*轴上的跳跃。相反，考拉熊会被认为不太像狗，我们会看到激活输出下降。然而，我们几乎无法让垃圾车被认为像狗：即使进行
    drastical 改变，我们可能只会看到从-0.97到-0.8左右的变化。
- en: '![](../Images/CH06_F04_Stevens2_GS.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Stevens2_GS.png)'
- en: Figure 6.4 Dogs, bears, and garbage trucks being mapped to how dog-like they
    are via the `tanh` activation function
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 显示了狗、熊和垃圾车通过`tanh`激活函数映射为它们的狗样程度
- en: 6.1.4 More activation functions
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 更多激活函数
- en: 'There are quite a few activation functions, some of which are shown in figure
    6.5\. In the first column, we see the smooth functions `Tanh` and `Softplus`,
    while the second column has “hard” versions of the activation functions to their
    left: `Hardtanh` and `ReLU`. `ReLU` (for *rectified linear unit*) deserves special
    note, as it is currently considered one of the best-performing general activation
    functions; many state-of-the-art results have used it. The `Sigmoid` activation
    function, also known as the *logistic function*, was widely used in early deep
    learning work but has since fallen out of common use except where we explicitly
    want to move to the 0...1 range: for example, when the output should be a probability.
    Finally, the `LeakyReLU` function modifies the standard `ReLU` to have a small
    positive slope, rather than being strictly zero for negative inputs (typically
    this slope is 0.01, but it’s shown here with slope 0.1 for clarity).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多激活函数，其中一些显示在图6.5中。在第一列中，���们看到平滑函数`Tanh`和`Softplus`，而第二列有激活函数的“硬”版本：`Hardtanh`和`ReLU`。`ReLU`（*修正线性单元*）值得特别注意，因为它目前被认为是表现最佳的通用激活函数之一；许多最新技术的结果都使用了它。`Sigmoid`激活函数，也称为*逻辑函数*，在早期深度学习工作中被广泛使用，但自那时以来已经不再常用，除非我们明确希望将其移动到0...1范围内：例如，当输出应该是概率时。最后，`LeakyReLU`函数修改了标准的`ReLU`，使其具有小的正斜率，而不是对负输入严格为零（通常这个斜率为0.01，但这里显示为0.1以便清楚显示）。
- en: 6.1.5 Choosing the best activation function
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 选择最佳激活函数
- en: Activation functions are curious, because with such a wide variety of proven
    successful ones (many more than shown in figure 6.5), it’s clear that there are
    few, if any, strict requirements. As such, we’re going to discuss some generalities
    about activation functions that can probably be trivially disproved in the specific.
    That said, by definition,[³](#pgfId-1026734) activation functions
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数很奇特，因为有许多被证明成功的种类（远远不止图6.5中显示的），很明显几乎没有严格的要求。因此，我们将讨论一些关于激活函数的一般性，这些一般性可能在具体情况下很容易被证伪。也就是说，根据定义，激活函数
- en: Are nonlinear. Repeated applications of (`w*x+b`) without an activation function
    results in a function of the same (affine linear) form. The nonlinearity allows
    the overall network to approximate more complex functions.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是非线性的。重复应用(`w*x+b`)而没有激活函数会导致具有相同（仿射线性）形式的函数。非线性允许整个网络逼近更复杂的函数。
- en: Are differentiable, so that gradients can be computed through them. Point discontinuities,
    as we can see in `Hardtanh` or `ReLU`, are fine.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是可微的，因此可以通过它们计算梯度。像`Hardtanh`或`ReLU`中看到的点间断是可以接受的。
- en: Without these characteristics, the network either falls back to being a linear
    model or becomes difficult to train.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这些特征，网络要么退回成为线性模型，要么变得难以训练。
- en: 'The following are true for the functions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下对这些函数是正确的：
- en: They have at least one sensitive range, where nontrivial changes to the input
    result in a corresponding nontrivial change to the output. This is needed for
    training.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们至少有一个敏感范围，在这个范围内对输入进行非平凡的更改会导致相应的输出发生非平凡的变化。这对于训练是必要的。
- en: Many of them have an insensitive (or saturated) range, where changes to the
    input result in little or no change to the output.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多激活函数具有不敏感（或饱和）范围，在这个范围内对输入进行更改几乎不会对输出产生任何或很少的变化。
- en: By way of example, the `Hardtanh` function could easily be used to make piecewise-linear
    approximations of a function by combining the sensitive range with different weights
    and biases on the input.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，`Hardtanh`函数可以通过在输入上组合敏感范围与不同的权重和偏置来轻松地用于制作函数的分段线性逼近。
- en: '![](../Images/CH06_F05_Stevens2_GS.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F05_Stevens2_GS.png)'
- en: Figure 6.5 A collection of common and not-so-common activation functions
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 常见和不那么常见的激活函数集合
- en: 'Often (but far from universally so), the activation function will have at least
    one of these:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常（但远非普遍如此），激活函数至少具有以下之一：
- en: A lower bound that is approached (or met) as the input goes to negative infinity
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个下界，当输入趋于负无穷时接近（或达到）
- en: A similar-but-inverse upper bound for positive infinity
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个类似但相反的正无穷的上界
- en: Thinking of what we know about how backpropagation works, we can figure out
    that the errors will propagate backward through the activation more effectively
    when the inputs are in the response range, while errors will not greatly affect
    neurons for which the input is saturated (since the gradient will be close to
    zero, due to the flat area around the output).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 想想我们对反向传播如何工作的了解，我们可以得出结论，当输入处于响应范围时，错误将通过激活向后传播得更有效，而当输入饱和时，错误不会对神经元产生很大影响（因为梯度将接近于零，由于输出周围的平坦区域）。
- en: 'Put together, all this results in a pretty powerful mechanism: we’re saying
    that in a network built out of linear + activation units, when different inputs
    are presented to the network, (a) different units will respond in different ranges
    for the same inputs, and (b) the errors associated with those inputs will primarily
    affect the neurons operating in the sensitive range, leaving other units more
    or less unaffected by the learning process. In addition, thanks to the fact that
    derivatives of the activation with respect to its inputs are often close to 1
    in the sensitive range, estimating the parameters of the linear transformation
    through gradient descent for the units that operate in that range will look a
    lot like the linear fit we have seen previously.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，这就产生了一个非常强大的机制：我们在说，当将不同的输入呈现给由线性 + 激活单元构建的网络时，（a）不同的单元将对相同的输入在不同的范围内做出响应，而（b）与这些输入相关的错误主要会影响在敏感范围内运行的神经元，使其他单元在学习过程中基本上不受影响。此外，由于激活函数相对于其输入的导数在敏感范围内通常接近于1，通过梯度下降估计在该范围内运行的单元的线性变换的参数将看起来很像我们之前看到的线性拟合。
- en: We are starting to get a deeper intuition for how joining many linear + activation
    units in parallel and stacking them one after the other leads us to a mathematical
    object that is capable of approximating complicated functions. Different combinations
    of units will respond to inputs in different ranges, and those parameters for
    those units are relatively easy to optimize through gradient descent, since learning
    will behave a lot like that of a linear function until the output saturates.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始对如何将许多线性 + 激活单元并行连接并依次堆叠起来形成一个能够逼近复杂函数的数学对象有了更深入的直觉。不同的单元组合将对输入在不同的范围内做出响应，并且这些单元的参数相对容易通过梯度下降进行优化，因为学习过程将表现得很像线性函数直到输出饱和。
- en: 6.1.6 What learning means for a neural network
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.6 神经网络的学习意味着什么
- en: Building models out of stacks of linear transformations followed by differentiable
    activations leads to models that can approximate highly nonlinear processes and
    whose parameters we can estimate surprisingly well through gradient descent. This
    remains true even when dealing with models with millions of parameters. What makes
    using deep neural networks so attractive is that it saves us from worrying too
    much about the exact function that represents our data--whether it is quadratic,
    piecewise polynomial, or something else. With a deep neural network model, we
    have a universal approximator and a method to estimate its parameters. This approximator
    can be customized to our needs, in terms of model capacity and its ability to
    model complicated input/output relationships, just by composing simple building
    blocks. We can see some examples of this in figure 6.6.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠线性变换和可微激活函数构建模型，可以得到能够近似高度非线性过程的模型，并且我们可以通过梯度下降方法出奇地好地估计其参数。即使处理具有数百万参数的模型时，这仍然成立。使用深度神经网络如此吸引人的原因在于，它使我们不必过多担心代表我们数据的确切函数--无论是二次的、分段多项式的，还是其他什么。通过深度神经网络模型，我们有一个通用的逼近器和一个估计其参数的方法。这个逼近器可以根据我们的需求进行定制，无论是模型容量还是模型复杂输入/输出关系的能力，只需组合简单的构建块。我们可以在图6.6中看到一些例子。
- en: '![](../Images/CH06_F06_Stevens2_GS.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F06_Stevens2_GS.png)'
- en: Figure 6.6 Composing multiple linear units and `tanh` activation functions to
    produce nonlinear outputs
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 组合多个线性单元和`tanh`激活函数以产生非线性输出
- en: The four upper-left graphs show four neurons--A, B, C, and D--each with its
    own (arbitrarily chosen) weight and bias. Each neuron uses the `Tanh` activation
    function with a min of -1 and a max of 1\. The varied weights and biases move
    the center point and change how drastically the transition from min to max happens,
    but they clearly all have the same general shape. The columns to the right of
    those show both pairs of neurons added together (A + B and then C + D). Here,
    we start to see some interesting properties that mimic a single layer of neurons.
    A + B shows a slight *S* curve, with the extremes approaching 0, but both a positive
    bump and a negative bump in the middle. Conversely, C + D has only a large positive
    bump, which peaks at a higher value than our single-neuron max of 1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 四个左上角的图显示了四个神经元--A、B、C和D--每个都有自己（任意选择的）权重和偏置。每个神经元使用`Tanh`激活函数，最小值为-1，最大值为1。不同���权重和偏置移动了中心点，并改变了从最小到最大的过渡有多么剧烈，但它们显然都有相同的一般形状。在这些右侧的列中，显示了两对神经元相加在一起（A
    + B，然后是C + D）。在这里，我们开始看到一些模仿单层神经元的有趣特性。A + B显示了一个轻微的*S*曲线，极端值接近0，但中间有一个正峰和一个负峰。相反，C
    + D只有一个大的正峰，峰值高于我们单个神经元的最大值1。
- en: 'In the third row, we begin to compose our neurons as they would be in a two-layer
    network. Both C(A + B) and D(A + B) have the same positive and negative bumps
    that A + B shows, but the positive peak is more subtle. The composition of C(A
    + B) + D(A + B) shows a new property: *two* clearly negative bumps, and possibly
    a very subtle second positive peak as well, to the left of the main area of interest.
    All this with only four neurons in two layers!'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三行，我们开始组合我们的神经元，就像它们在一个两层网络中的样子。C(A + B)和D(A + B)都有与A + B相同的正负峰，但正峰更加微妙。C(A
    + B) + D(A + B)的组合显示了一个新的特性：*两个*明显的负峰，可能还有一个非常微妙的第二个正峰，位于主要感兴趣区域的左侧。所有这些只用了两层中的四个神经元！
- en: Again, these neurons’ parameters were chosen only to have a visually interesting
    result. Training consists of finding acceptable values for these weights and biases
    so that the resulting network correctly carries out a task, such as predicting
    likely temperatures given geographic coordinates and time of the year. By *carrying
    out a task successfully*, we mean obtaining a correct output on unseen data produced
    by the same data-generating process used for training data. A successfully trained
    network, through the values of its weights and biases, will capture the inherent
    structure of the data in the form of meaningful numerical representations that
    work correctly for previously unseen data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些神经元的参数仅仅是为了得到一个视觉上有趣的结果而选择的。训练的过程是找到这些权重和偏置的可接受值，使得最终的网络能够正确执行任务，比如根据地理坐标和年份时间预测可能的温度。通过*成功执行任务*，我们指的是在由用于训练数据的相同数据生成过程产生的未见数据上获得正确的输出。一个成功训练的网络，通过其权重和偏置的值，将以有意义的数字表示形式捕捉数据的固有结构，这些数字表示对以前未见的数据能够正确工作。
- en: 'Let’s take another step in our realization of the mechanics of learning: deep
    neural networks give us the ability to approximate highly nonlinear phenomena
    without having an explicit model for them. Instead, starting from a generic, untrained
    model, we specialize it on a task by providing it with a set of inputs and outputs
    and a loss function from which to backpropagate. Specializing a generic model
    to a task using examples is what we refer to as *learning*, because the model
    wasn’t built with that specific task in mind--no rules describing how that task
    worked were encoded in the model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在了解学习机制方面再迈出一步：深度神经网络使我们能够近似高度非线性的现象，而无需为其建立明确的模型。相反，从一个通用的、未经训练的模型开始，我们通过提供一组输入和输出以及一个损失函数来专门针对一个任务进行特化，并通过反向传播来优化。通过示例将通用模型专门化到一个任务上，这就是我们所说的*学习*，因为该模型并不是为特定任务而构建的--没有规则描述该任务如何工作被编码在模型中。
- en: 'For our thermometer example, we assumed that both thermometers measured temperatures
    linearly. That assumption is where we implicitly encoded a rule for our task:
    we hardcoded the shape of our input/output function; we couldn’t have approximated
    anything other than data points sitting around a line. As the dimensionality of
    a problem grows (that is, many inputs to many outputs) and input/output relationships
    get complicated, assuming a shape for the input/output function is unlikely to
    work. The job of a physicist or an applied mathematician is often to come up with
    a functional description of a phenomenon from first principles, so that we can
    estimate the unknown parameters from measurements and get an accurate model of
    the world. Deep neural networks, on the other hand, are families of functions
    that have the ability to approximate a wide range of input/output relationships
    without necessarily requiring us to come up with an explanatory model of a phenomenon.
    In a way, we’re renouncing an explanation in exchange for the possibility of tackling
    increasingly complicated problems. In another way, we sometimes lack the ability,
    information, or computational resources to build an explicit model of what we’re
    presented with, so data-driven methods are our only way forward.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的温度计示例，我们假设两个温度计都是线性测量温度的。这个假设是我们隐式编码任务规则的地方：我们硬编码了输入/输出函数的形状；我们无法逼近除了围绕一条直线的数据点之外的任何东西。随着问题的维度增加（即，许多输入到许多输出）和输入/输出关系变得复杂，假设输入/输出函数的形状不太可能奏效。物理学家或应用数学家的工作通常是从第一原理提出现象的功能性描述，这样我们就可以从测量中估计未知参数，并获得对世界的准确模型。另一方面，深度神经网络是一类函数族，具有近似各种输入/输出关系的能力，而不一定需要我们提出现象的解释模型。在某种程度上，我们放弃了解释，以换取解决日益复杂问题的可能性。另一方面，我们有时缺乏建立我们所面对的事物的显式模型的能力、信息或计算资源，因此数据驱动的方法是我们前进的唯一途径。
- en: 6.2 The PyTorch nn module
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 PyTorch nn模块
- en: All this talking about neural networks is probably making you really curious
    about building one from scratch with PyTorch. Our first step will be to replace
    our linear model with a neural network unit. This will be a somewhat useless step
    backward from a correctness perspective, since we’ve already verified that our
    calibration only required a linear function, but it will still be instrumental
    for starting on a sufficiently simple problem and scaling up later.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些关于神经网络的讨论可能让您对使用PyTorch从头开始构建一个神经网络感到非常好奇。我们的第一步将是用一个神经网络单元替换我们的线性模型。从正确性的角度来看，这将是一个有点无用的后退，因为我们已经验证了我们的校准只需要一个线性函数，但从足够简单的问题开始并随后扩展仍然是非常重要的。
- en: PyTorch has a whole submodule dedicated to neural networks, called `torch.nn`.
    It contains the building blocks needed to create all sorts of neural network architectures.
    Those building blocks are called *modules* in PyTorch parlance (such building
    blocks are often referred to as *layers* in other frameworks). A PyTorch module
    is a Python class deriving from the `nn.Module` base class. A module can have
    one or more `Parameter` instances as attributes, which are tensors whose values
    are optimized during the training process (think `w` and `b` in our linear model).
    A module can also have one or more submodules (subclasses of `nn.Module`) as attributes,
    and it will be able to track their parameters as well.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch有一个专门用于神经网络的子模块，称为`torch.nn`。它包含创建各种神经网络架构所需的构建模块。在PyTorch的术语中，这些构建模块称为*模块*（在其他框架中，这些构建模块通常被称为*层*）。PyTorch模块是从`nn.Module`基类派生的Python类。一个模块可以有一个或多个`Parameter`实例作为属性，这些张量的值在训练过程中进行优化（想想我们线性模型中的`w`和`b`）。一个模块也可以有一个或多个子模块（`nn.Module`的子类）作为属性，并且它将能够跟踪它们的参数。
- en: '*Note* The submodules must be top-level *attributes*, not buried inside `list`
    or `dict` instances! Otherwise, the optimizer will not be able to locate the submodules
    (and, hence, their parameters). For situations where your model requires a list
    or dict of submodules, PyTorch provides `nn.ModuleList` and `nn.ModuleDict`.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 子模块必须是顶级*属性*，而不是嵌套在`list`或`dict`实例中！否则，优化器将无法定位子模块（因此也无法定位它们的参数）。对于您的模型需要子模块列表或字典的情况，PyTorch提供了`nn.ModuleList`和`nn.ModuleDict`。'
- en: Unsurprisingly, we can find a subclass of `nn.Module` called `nn.Linear`, which
    applies an affine transformation to its input (via the parameter attributes `weight`
    and `bias`) and is equivalent to what we implemented earlier in our thermometer
    experiments. We’ll now start precisely where we left off and convert our previous
    code to a form that uses `nn`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，我们可以找到一个名为`nn.Linear`的`nn.Module`子类，它对其输入应用一个仿射变换（通过参数属性`weight`和`bias`）并等同于我们在温度计实验中早期实现的内容。我们现在将从我们离开的地方精确开始，并将我们以前的代码转换为使用`nn`的形式。
- en: 6.2.1 Using __call__ rather than forward
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 使用__call__而不是forward
- en: 'All PyTorch-provided subclasses of `nn.Module` have their `__call__` method
    defined. This allows us to instantiate an `nn.Linear` and call it as if it was
    a function, like so (code/p1ch6/1_neural_networks.ipynb):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有PyTorch提供的`nn.Module`的子类都定义了它们的`__call__`方法。这使我们能够实例化一个`nn.Linear`并将其调用为一个函数，就像这样（代码/p1ch6/1_neural_networks.ipynb）：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ We’ll look into the constructor arguments in a moment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们马上会看构造函数参数。
- en: 'Calling an instance of `nn.Module` with a set of arguments ends up calling
    a method named `forward` with the same arguments. The `forward` method is what
    executes the forward computation, while `__call__` does other rather important
    chores before and after calling `forward`. So, it is technically possible to call
    `forward` directly, and it will produce the same output as `__call__`, but this
    should not be done from user code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一组参数调用`nn.Module`的实例最终会调用一个名为`forward`的方法，该方法使用相同的参数。`forward`方法执行前向计算，而`__call__`在调用`forward`之前和之后执行其他相当重要的任务。因此，从技术上讲，可以直接调用`forward`，它将产生与`__call__`相同的输出，但不应该从用户代���中这样做：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Correct!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 正确！
- en: ❷ Silent error. Don’t do it!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 沉默的错误。不要这样做！
- en: 'Here’s the implementation of `Module._call_` (we left out the bits related
    to the JIT and made some simplifications for clarity; torch/nn/modules/module.py,
    line 483, class: Module):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `Module._call_` 的���现（我们省略了与 JIT 相关的部分，并对清晰起见进行了一些简化；torch/nn/modules/module.py，第
    483 行，类：Module）：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, there are a lot of hooks that won’t get called properly if we
    just use `.forward(...)` directly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，如果我们直接使用 `.forward(...)`，将无法正确调用许多钩子。
- en: 6.2.2 Returning to the linear model
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 返回线性模型
- en: 'Back to our linear model. The constructor to `nn.Linear` accepts three arguments:
    the number of input features, the number of output features, and whether the linear
    model includes a bias or not (defaulting to `True`, here):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的线性模型。`nn.Linear` 的构造函数接受三个参数：输入特征的数量、输出特征的数量，以及线性模型是否包括偏置（默认为 `True`）：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The arguments are input size, output size, and bias defaulting to True.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 参数是输入大小、输出大小和默认为 True 的偏置。
- en: The number of features in our case just refers to the size of the input and
    the output tensor for the module, so 1 and 1\. If we used both temperature and
    barometric pressure as input, for instance, we would have two features in input
    and one feature in output. As we will see, for more complex models with several
    intermediate modules, the number of features will be associated with the capacity
    of the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况中，特征的数量只是指模块的输入和输出张量的大小，因此为 1 和 1。例如，如果我们将温度和气压作为输入，那么输入中将有两个特征，输出中将有一个特征。正如我们将看到的，对于具有多个中间模块的更复杂模型，特征的数量将与模型的容量相关联。
- en: 'We have an instance of `nn.Linear` with one input and one output feature. That
    only requires one weight and one bias:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个具有一个输入和一个输出特征的 `nn.Linear` 实例。这只需要一个权重和一个偏置：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can call the module with some input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一些输入调用该模块：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Although PyTorch lets us get away with it, we don’t actually provide an input
    with the right dimensionality. We have a model that takes one input and produces
    one output, but PyTorch `nn.Module` and its subclasses are designed to do so on
    multiple samples at the same time. To accommodate multiple samples, modules expect
    the zeroth dimension of the input to be the number of samples in the *batch*.
    We encountered this concept in chapter 4, when we learned how to arrange real-world
    data into tensors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PyTorch 让我们可以这样做，但实际上我们并没有提供正确维度的输入。我们有一个接受一个输入并产生一个输出的模型，但 PyTorch 的 `nn.Module`
    及其子类是设计用于同时处理多个样本的。为了容纳多个样本，模块期望输入的零维是*批次*中的样本数量。我们在第 4 章遇到过这个概念，当时我们学习如何将现实世界的数据排列成张量。
- en: Batching inputs
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批处理输入
- en: 'Any module in `nn` is written to produce outputs for a *batch* of multiple
    inputs at the same time. Thus, assuming we need to run `nn.Linear` on 10 samples,
    we can create an input tensor of size *B* × *Nin*, where *B* is the size of the
    batch and *Nin* is the number of input features, and run it once through the model.
    For example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn` 中的任何模块都是为了一次对*批量*中的多个输入产生输出而编写的。因此，假设我们需要在 10 个样本上运行 `nn.Linear`，我们可以创建一个大小为
    *B* × *Nin* 的输入张量，其中 *B* 是批次的大小，*Nin* 是输入特征的数量，并将其一次通过模型运行。例如：'
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s dig into what’s going on here, with figure 6.7 showing a similar situation
    with batched image data. Our input is *B* × *C* × *H* × *W* with a batch size
    of 3 (say, images of a dog, a bird, and then a car), three channel dimensions
    (red, green, and blue), and an unspecified number of pixels for height and width.
    As we can see, the output is a tensor of size *B* × *Nout*, where *Nout* is the
    number of output features: four, in this case.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入研究一下这里发生的情况，图 6.7 显示了批处理图像数据的类似情况。我们的输入是 *B* × *C* × *H* × *W*，批处理大小为 3（比如，一只狗、一只鸟和一辆车的图像），三个通道维度（红色、绿色和蓝色），以及高度和宽度的未指定像素数量。正如我们所看到的，输出是大小为
    *B* × *Nout* 的张量，其中 *Nout* 是输出特征的数量：在这种情况下是四个。
- en: Optimizing batches
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化批处理
- en: The reason we want to do this batching is multifaceted. One big motivation is
    to make sure the computation we’re asking for is big enough to saturate the computing
    resources we’re using to perform the computation. GPUs in particular are highly
    parallelized, so a single input on a small model will leave most of the computing
    units idle. By providing batches of inputs, the calculation can be spread across
    the otherwise-idle units, which means the batched results come back just as quickly
    as a single result would. Another benefit is that some advanced models use statistical
    information from the entire batch, and those statistics get better with larger
    batch sizes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望进行批处理的原因是多方面的。一个重要的动机是确保我们请求的计算量足够大，以充分利用我们用来执行计算的计算资源。特别是 GPU 是高度并行化的，因此在小型模型上单个输入会使大多数计算单元处于空闲状态。通过提供输入的批处理，计算可以分布在否则空闲的单元上，这意味着批处理结果会像单个结果一样快速返回。另一个好处是一些高级模型使用整个批次的统计信息，这些统计信息随着批次大小的增加而变得更好。
- en: '![](../Images/CH06_F07_Stevens2_GS.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F07_Stevens2_GS.png)'
- en: Figure 6.7 Three RGB images batched together and fed into a neural network.
    The output is a batch of three vectors of size 4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 三个 RGB 图像一起批处理并输入到神经网络中。输出是大小为 4 的三个向量的批处理结果。
- en: 'Back to our thermometer data, `t_u` and `t_c` were two 1D tensors of size `B`.
    Thanks to broadcasting, we could write our linear model as `w * x + b`, where
    `w` and `b` were two scalar parameters. This worked because we had a single input
    feature: if we had two, we would need to add an extra dimension to turn that 1D
    tensor into a matrix with samples in the rows and features in the columns.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的温度计数据，`t_u` 和 `t_c` 是大小为 `B` 的两个 1D 张量。借助广播，我们可以将我们的线性模型写成 `w * x + b`，其中
    `w` 和 `b` 是两个标量参数。这是因为我们只有一个输入特征：如果有两个，我们需要添加一个额外维度，将该 1D 张量转换为一个矩阵，其中行中有样本，列中有特征。
- en: 'That’s exactly what we need to do to switch to using `nn.Linear`. We reshape
    our *B* inputs to *B* × *Nin*, where *Nin* is 1\. That is easily done with `unsqueeze`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们需要做的，以切换到使用 `nn.Linear`。我们将我们的 *B* 输入重塑为 *B* × *Nin*，其中 *Nin* 为 1。这可以很容易地通过
    `unsqueeze` 完成：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Adds the extra dimension at axis 1
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在轴 1 处添加额外维度
- en: 'We’re done; let’s update our training code. First, we replace our handmade
    model with `nn.Linear(1,1)`, and then we need to pass the linear model parameters
    to the optimizer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了；让我们更新我们的训练代码。首先，我们用`nn.Linear(1,1)`替换我们手工制作的模型，然后我们需要将线性模型的参数传递给优化器：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ This is just a redefinition from earlier.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这只是之前的重新定义。
- en: ❷ This method call replaces [params].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这个方法调用替换了[params]。
- en: 'Earlier, it was our responsibility to create parameters and pass them as the
    first argument to `optim.SGD`. Now we can use the `parameters` method to ask any
    `nn.Module` for a list of parameters owned by it or any of its submodules:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们的责任是创建参数并将它们作为`optim.SGD`的第一个参数传递。现在我们可以使用`parameters`方法向任何`nn.Module`询问由它或其任何子模块拥有的参数列表：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This call recurses into submodules defined in the module’s `init` constructor
    and returns a flat list of all parameters encountered, so that we can conveniently
    pass it to the optimizer constructor as we did previously.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用递归地进入模块的`init`构造函数中定义的子模块，并返回遇到的所有参数的平面列表，这样我们就可以方便地将其传递给优化器构造函数，就像我们之前做的那样。
- en: We can already figure out what happens in the training loop. The optimizer is
    provided with a list of tensors that were defined with `requires_grad = True`--all
    `Parameter`s are defined this way by definition, since they need to be optimized
    by gradient descent. When `training_loss.backward()` is called, `grad` is accumulated
    on the leaf nodes of the graph, which are precisely the parameters that were passed
    to the optimizer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以弄清楚训练循环中发生了什么。优化器提供了一个张量列表，这些张量被定义为`requires_grad = True`--所有的`Parameter`都是这样定义的，因为它们需要通过梯度下降进行优化。当调用`training_loss.backward()`时，`grad`会在图的叶节点上累积，这些叶节点恰好是传递给优化器的参数。
- en: At this point, the SGD optimizer has everything it needs. When `optimizer.step()`
    is called, it will iterate through each `Parameter` and change it by an amount
    proportional to what is stored in its `grad` attribute. Pretty clean design.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，SGD优化器已经拥有了一切所需的东西。当调用`optimizer.step()`时，它将遍历每个`Parameter`，并按照其`grad`属性中存储的量进行更改。设计相当干净。
- en: 'Let’s take a look a the training loop now:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下训练循环：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The model is now passed in, instead of the individual params.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 现在传入的是模型，而不是单独的参数。
- en: ❷ The loss function is also passed in. We’ll use it in a moment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 损失函数也被传入。我们马上会用到它。
- en: It hasn’t changed practically at all, except that now we don’t pass `params`
    explicitly to `model` since the model itself holds its `Parameters` internally.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上几乎没有任何变化，只是现在我们不再显式地将`params`传递给`model`，因为模型本身在内部保存了它的`Parameters`。
- en: 'There’s one last bit that we can leverage from `torch.nn`: the loss. Indeed,
    `nn` comes with several common loss functions, among them `nn.MSELoss` (MSE stands
    for Mean Square Error), which is exactly what we defined earlier as our `loss_fn`.
    Loss functions in `nn` are still subclasses of `nn.Module`, so we will create
    an instance and call it as a function. In our case, we get rid of the handwritten
    `loss_fn` and replace it:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 还有最后一点，我们可以从`torch.nn`中利用的：损失。确实，`nn`带有几种常见的损失函数，其中包括`nn.MSELoss`（MSE代表均方误差），这正是我们之前定义的`loss_fn`。`nn`中的损失函数仍然是`nn.Module`的子类，因此我们将创建一个实例并将其作为函数调用。在我们的情况下，我们摆脱了手写的`loss_fn`并替换它：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ We are no longer using our hand-written loss function from earlier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们不再使用之前手写的损失函数。
- en: Everything else input into our training loop stays the same. Even our results
    remain the same as before. Of course, getting the same results is expected, as
    a difference would imply a bug in one of the two implementations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有输入到我们的训练循环中的其他内容保持不变。即使我们的结果仍然与以前相同。当然，得到相同的结果是预期的，因为任何差异都意味着两种实现中的一个存在错误。
- en: 6.3 Finally a neural network
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 最后是神经网络
- en: It’s been a long journey--there has been a lot to explore for these 20-something
    lines of code we require to define and train a model. Hopefully by now the magic
    involved in training has vanished and left room for the mechanics. What we learned
    so far will allow us to own the code we write instead of merely poking at a black
    box when things get more complicated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个漫长的旅程--这20多行代码中有很多可以探索的内容，我们需要定义和训练一个模型。希望到现在为止，训练中涉及的魔法已经消失，为机械留下了空间。到目前为止我们学到的东西将使我们能够拥有我们编写的代码，而不仅仅是在事情变得更加复杂时摸黑箱。
- en: 'There’s one last step left to take: replacing our linear model with a neural
    network as our approximating function. We said earlier that using a neural network
    will not result in a higher-quality model, since the process underlying our calibration
    problem was fundamentally linear. However, it’s good to make the leap from linear
    to neural network in a controlled environment so we won’t feel lost later.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 还有最后一步要走：用神经网络替换我们的线性模型作为我们的逼近函数。我们之前说过，使用神经网络不会导致更高质量的模型，因为我们校准问题的过程基本上是线性的。然而，在受控环境中从线性到神经网络的跃迁是有好处的，这样我们以后就不会感到迷失。
- en: 6.3.1 Replacing the linear model
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 替换线性模型
- en: 'We are going to keep everything else fixed, including the loss function, and
    only redefine `model`. Let’s build the simplest possible neural network: a linear
    module, followed by an activation function, feeding into another linear module.
    The first linear + activation layer is commonly referred to as a *hidden* layer
    for historical reasons, since its outputs are not observed directly but fed into
    the output layer. While the input and output of the model are both of size 1 (they
    have one input and one output feature), the size of the output of the first linear
    module is usually larger than 1\. Recalling our earlier explanation of the role
    of activations, this can lead different units to respond to different ranges of
    the input, which increases the capacity of our model. The last linear layer will
    take the output of activations and combine them linearly to produce the output
    value.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持其他所有内容不变，包括损失函数，并且只重新定义`model`。让我们构建可能的最简单的神经网络：一个线性模块，后跟一个激活函数，进入另一个线性模块。第一个线性
    + 激活层通常被称为*隐藏*层，出于历史原因，因为它的输出不是直接观察到的，而是馈送到输出层。虽然模型的输入和输出都是大小为1（它们具有一个输入和一个输出特征），但第一个线性模块的输出大小通常大于1。回顾我们之前对激活作用的解释，这可以导致不同的单元对输入的不同范围做出响应，从而增加我们模型的容量。最后一个线性层将获取激活的输出，并将它们线性组合以产生输出值。
- en: 'There is no standard way to depict neural networks. Figure 6.8 shows two ways
    that seem to be somewhat prototypical: the left side shows how our network might
    be depicted in basic introductions, whereas a style similar to that on the right
    is often used in the more advanced literature and research papers. It is common
    to make diagram blocks that roughly correspond to the neural network modules PyTorch
    offers (though sometimes things like the `Tanh` activation layer are not explicitly
    shown). Note that one somewhat subtle difference between the two is that the graph
    on the left has the inputs and (intermediate) results in the circles as the main
    elements. On the right, the computational steps are more prominent.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 没有标准的神经网络表示方法。图6.8显示了两种似乎有些典型的方式：左侧显示了我们的网络可能在基本介绍中如何描述，而右侧类似于更高级文献和研究论文中经常使用的风格。通常制作大致对应于PyTorch提供的神经网络模块的图块（尽管有时像`Tanh`激活层这样的东西并没有明确显示）。请注意，两者之间的一个略微微妙的区别是左侧的图中将��入和（中间）结果放在圆圈中作为主要元素。右侧，计算步骤更加突出。
- en: '![](../Images/CH06_F08_Stevens2_GS.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F08_Stevens2_GS.png)'
- en: 'Figure 6.8 Our simplest neural network in two views. Left: beginner’s version.
    Right: higher-level version.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 我们最简单的神经网络的两个视图。左：初学者版本。右：高级版本。
- en: '`nn` provides a simple way to concatenate modules through the `nn.Sequential`
    container:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn`通过`nn.Sequential`容器提供了一种简单的方法来连接模块：'
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ We chose 13 arbitrarily. We wanted a number that was a different size from
    the other tensor shapes we have floating around.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们随意选择了13。我们希望这个数字与我们周围漂浮的其他张量形状大小不同。
- en: ❷ This 13 must match the first size, however.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这个13必须与第一个大小匹配。
- en: The end result is a model that takes the inputs expected by the first module
    specified as an argument of `nn.Sequential`, passes intermediate outputs to subsequent
    modules, and produces the output returned by the last module. The model fans out
    from 1 input feature to 13 hidden features, passes them through a `tanh` activation,
    and linearly combines the resulting 13 numbers into 1 output feature.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个模型，它接受由`nn.Sequential`的第一个模块指定的输入，将中间输出传递给后续模块，并产生由最后一个模块返回的输出。该模型从1个输入特征扩展到13个隐藏特征，通过一个`tanh`激活，然后将产生的13个数字线性组合成1个输出特征。
- en: 6.3.2 Inspecting the parameters
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 检查参数
- en: 'Calling `model.parameters()` will collect `weight` and `bias` from both the
    first and second linear modules. It’s instructive to inspect the parameters in
    this case by printing their shapes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`model.parameters()`将收集第一个和第二个线性模块的`weight`和`bias`。在这种情况下通过打印它们的形状来检查参数是很有启发性的：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These are the tensors that the optimizer will get. Again, after we call `model.backward()`,
    all parameters are populated with their `grad`, and the optimizer then updates
    their values accordingly during the `optimizer.step()` call. Not that different
    from our previous linear model, eh? After all, they’re both differentiable models
    that can be trained using gradient descent.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是优化器将获得的张量。再次，在我们调用`model.backward()`之后，所有参数都将填充其`grad`，然后优化器在`optimizer.step()`调用期间相应地更新它们的值。和我们之前的线性模型没有太大不同，对吧？毕竟，它们都是可以使用梯度下降进行训练的可微分模型。
- en: 'A few notes on parameters of `nn.Modules`. When inspecting parameters of a
    model made up of several submodules, it is handy to be able to identify parameters
    by name. There’s a method for that, called `named_parameters`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`nn.Modules`参数的一些注意事项。当检查由几个子模块组成的模型的参数时，能够通过名称识别参数是很方便的。有一个方法可以做到这一点，称为`named_parameters`：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The name of each module in `Sequential` is just the ordinal with which the
    module appears in the arguments. Interestingly, `Sequential` also accepts an `OrderedDict`,[⁴](#pgfId-1030126)
    in which we can name each module passed to `Sequential`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequential`中每个模块的名称只是模块在参数中出现的顺序。有趣的是，`Sequential`还接受一个`OrderedDict`，在其中我们可以为传递给`Sequential`的每个模块命名：'
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This allows us to get more explanatory names for submodules:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们可以为子模块获得更具解释性的名称：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is more descriptive; but it does not give us more flexibility in the flow
    of data through the network, which remains a purely sequential pass-through--the
    `nn.Sequential` is very aptly named. We will see how to take full control of the
    processing of input data by subclassing `nn.Module` ourselves in chapter 8.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这更具描述性；但它并没有给我们更多控制数据流的灵活性，数据流仍然是纯粹的顺序传递--`nn.Sequential`的命名非常贴切。我们将在第8章中看到如何通过自己子类化`nn.Module`来完全控制输入数据的处理。
- en: 'We can also access a particular `Parameter` by using submodules as attributes:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用子模块作为属性来访问特定的`Parameter`：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is useful for inspecting parameters or their gradients: for instance,
    to monitor gradients during training, as we did at the beginning of this chapter.
    Say we want to print out the gradients of `weight` of the linear portion of the
    hidden layer. We can run the training loop for the new neural network model and
    then look at the resulting gradients after the last epoch:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于检查参数或它们的梯度非常有用：例如，要监视训练过程中的梯度，就像我们在本章开头所做的那样。假设我们想要打印出隐藏层线性部分的`weight`的梯度。我们可以运行新神经网络模型的训练循环，然后在最后一个时期查看结果梯度：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ We’ve dropped the learning rate a bit to help with stability.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们稍微降低了学习率以提高稳定性。
- en: 6.3.3 Comparing to the linear model
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 与线性模型比较
- en: 'We can also evaluate the model on all of the data and see how it differs from
    a line:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以评估模型在所有数据上的表现，并查看它与一条直线的差异：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The result is shown in figure 6.9\. We can appreciate that the neural network
    has a tendency to overfit, as we discussed in chapter 5, since it tries to chase
    the measurements, including the noisy ones. Even our tiny neural network has too
    many parameters to fit the few measurements we have. It doesn’t do a bad job,
    though, overall.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在图6.9中。我们可以看到神经网络有过拟合的倾向，正如我们在第5章讨论的那样，因为它试图追踪测量值，包括嘈杂的值。即使我们微小的神经网络有太多参数来拟合我们所拥有的少量测量值。总的来说，它做得还不错。
- en: '![](../Images/CH06_F09_Stevens2_GS.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F09_Stevens2_GS.png)'
- en: Figure 6.9 The plot of our neural network model, with input data (circles) and
    model output (Xs). The continuous line shows behavior between samples.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 我们的神经网络模型的绘图，包括输入数据（圆圈）和模型输出（X）。连续线显示样本之间的行为。
- en: 6.4 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 结论
- en: We’ve covered a lot in chapters 5 and 6, although we have been dealing with
    a very simple problem. We dissected building differentiable models and training
    them using gradient descent, first using raw autograd and then relying on `nn`.
    By now you should have confidence in your understanding of what’s going on behind
    the scenes. Hopefully this taste of PyTorch has given you an appetite for more!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们一直在处理一个非常简单的问题，但在第5章和第6章中我们已经涵盖了很多内容。我们分析了构建可微分模型并使用梯度下降进行训练，首先使用原始自动求导，然后依赖于`nn`。到目前为止，您应该对幕后发生的事情有信心。希望这一次PyTorch的体验让您对更多内容感到兴奋！
- en: 6.5 Exercises
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 练习
- en: Experiment with the number of hidden neurons in our simple neural network model,
    as well as the learning rate.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们简单的神经网络模型中尝试隐藏神经元的数量以及学习率。
- en: What changes result in more linear output from the model?
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么改变会导致模型输出更线性？
- en: Can you get the model to obviously overfit the data?
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能明显地使模型过拟合数据吗？
- en: The third-hardest problem in physics is finding a proper wine to celebrate discoveries.
    Load the wine data from chapter 4, and create a new model with the appropriate
    number of input parameters.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理学中第三难的问题是找到一种合适的葡萄酒来庆祝发现。从第4章加载葡萄酒数据，并创建一个具有适当数量输入参数的新模型。
- en: How long does it take to train compared to the temperature data we have been
    using?
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练所需时间与我们一直在使用的温度数据相比需要多长时间？
- en: Can you explain what factors contribute to the training times?
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释哪些因素导致训练时间？
- en: Can you get the loss to decrease while training on this dataset?
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在这个数据集上训练时使损失减少吗？
- en: How would you go about graphing this dataset?
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何绘制���个数据集的图表？
- en: 6.6 Summary
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 总结
- en: Neural networks can be automatically adapted to specialize themselves on the
    problem at hand.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以自动适应专门解决手头问题。
- en: Neural networks allow easy access to the analytical derivatives of the loss
    with respect to any parameter in the model, which makes evolving the parameters
    very efficient. Thanks to its automated differentiation engine, PyTorch provides
    such derivatives effortlessly.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络允许轻松访问模型中任何参数相对于损失的解析导数，这使得演化参数非常高效。由于其自动微分引擎，PyTorch轻松提供这些导数。
- en: Activation functions around linear transformations make neural networks capable
    of approximating highly nonlinear functions, at the same time keeping them simple
    enough to optimize.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环绕线性变换的激活函数使神经网络能够逼近高度非线性函数，同时保持足够简单以进行优化。
- en: The `nn` module together with the tensor standard library provide all the building
    blocks for creating neural networks.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn`模块与张量标准库一起提供了创建神经网络的所有构建模块。'
- en: To recognize overfitting, it’s essential to maintain the training set of data
    points separate from the validation set. There’s no one recipe to combat overfitting,
    but getting more data, or more variability in the data, and resorting to simpler
    models are good starts.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要识别过拟合，保持训练数据点与验证集分开是至关重要的。没有一种对抗过拟合的固定方法，但增加数据量，或增加数据的变化性，并转向更简单的模型是一个良好的开始。
- en: Anyone doing data science should be plotting data all the time.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 做数据科学的人应该一直在绘制数据。
- en: '* * *'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.)See F. Rosenblatt, “The Perceptron: A Probabilistic Model for Information
    Storage and Organization in the Brain,” *Psychological Review* 65(6), 386-408
    (1958), [https://pubmed.ncbi.nlm.nih.gov/13602029/](https://pubmed.ncbi.nlm.nih.gov/13602029/).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)参见F. Rosenblatt，“感知器：大脑中信息存储和组织的概率模型”，*心理评论* 65(6)，386-408（1958年），[https://pubmed.ncbi.nlm.nih.gov/13602029/](https://pubmed.ncbi.nlm.nih.gov/13602029/)。
- en: ^(2.)For an intuitive appreciation of this universal approximation property,
    you can pick a function from figure 6.5 and then build a building-block function
    that is almost zero in most parts and positive around *x* = 0 from scaled (including
    multiplied by negative numbers) and translated copies of the activation function.
    With scaled, translated, and dilated (squeezed along the *X*-axis) copies of this
    building-block function, you can then approximate any (continuous) function. In
    figure 6.6 the function in the middle row to the right could be such a building
    block. Michael Nielsen has an interactive demonstration in his online book *Neural
    Networks and Deep Learning* at [http://mng.bz/Mdon](http://mng.bz/Mdon).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)为了直观地理解这种通用逼近性质，你可以从图6.5中选择一个函数，然后构建一个几乎在大部分区域为零且在*x* = 0周围为正的基本函数，通过缩放（包括乘以负数）、平移激活函数的副本。通过这个基本函数的缩放、平移和扩展（沿*X*轴挤压）的副本，你可以逼近任何（连续）函数。在图6.6中，右侧中间行的函数可能是这样一个基本构件。Michael
    Nielsen在他的在线书籍*神经网络与深度学习*中有一个交互式演示，网址为[http://mng.bz/Mdon](http://mng.bz/Mdon)。
- en: ^(3.)Of course, even these statements aren’t *always* true; see Jakob Foerster,
    “Nonlinear Computation in Deep Linear Networks,” OpenAI, 2019, [http://mng.bz/gygE](http://mng.bz/gygE).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)当然，即使这些说法并不总是*正确*；参见Jakob Foerster的文章，“深度线性网络中的非线性计算”，OpenAI，2019，[http://mng.bz/gygE](http://mng.bz/gygE)。
- en: ^(4.)Not all versions of Python specify the iteration order for `dict`, so we’re
    using `OrderedDict` here to ensure the ordering of the layers and emphasize that
    the order of the layers matters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)并非所有版本的Python都指定了`dict`的迭代顺序，因此我们在这里使用`OrderedDict`来确保层的顺序，并强调层的顺序很重要。
