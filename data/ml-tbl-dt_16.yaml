- en: Appendix B. K-nearest neighbors and support vector machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. K-最近邻和支持向量机
- en: In this appendix, we examine classical machine learning algorithms with a more
    computational nature that we didn’t treat in the book because they are less frequently
    used nowadays and are considered outdated compared to decision tree ensembles
    in most applications. Overall, support vector machines (SVMs) are still a practical
    machine learning algorithm well suited for high-dimensional, noisy, or small-sized
    data applications. On the other end, k-nearest neighbors (k-NN) is well suited
    for running applications where the data has few features, there can be outliers,
    and it is not necessary to get a high degree of accuracy in predictions. For instance,
    SVMs can still be used to classify medical images, such as mammograms and X-rays;
    for vehicle detection and tracking in the automotive industry; or to detect email
    spam. Instead, k-NN is mainly applied in recommender systems, particularly collaborative
    filtering approaches, to recommend products or services based on users’ past behavior.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们考察了具有更多计算性质的经典机器学习算法，这些算法在书中没有涉及，因为它们现在使用较少，并且与大多数应用中的决策树集成相比被认为是过时的。总的来说，支持向量机（SVMs）仍然是一种适合于高维、噪声或小规模数据应用的实用机器学习算法。另一方面，k-最近邻（k-NN）非常适合在数据特征较少、可能存在异常值且预测不需要高度准确性的应用中运行。例如，SVMs仍然可以用于分类医学图像，如乳腺X光片和X射线；在汽车行业中用于车辆检测和跟踪；或用于检测电子邮件垃圾邮件。相反，k-NN主要应用于推荐系统，特别是基于用户过去行为的协同过滤方法，以推荐产品或服务。
- en: 'They are suited in most tabular data situations when your data is not too small
    or exceedingly big—as a rule of thumb, where there are fewer than 10,000 rows.
    We will start with k-NN, an algorithm that data scientists have used for decades
    in machine learning problems and that is easy to understand and implement. Then
    we will complete our overview with SVMs and a brief excursus on using GPUs to
    have these algorithms perform when using a moderately sized dataset. All the examples
    need the Airbnb NYC Dataset presented in chapter 4\. You can reprise it by executing
    the following code snippet:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在大多数表格数据情况下都适用，当您的数据不是太小或太大时——作为一个经验法则，当行数少于10,000行时。我们将从k-NN算法开始，这是数据科学家在机器学习问题中使用了几十年的算法，它易于理解和实现。然后我们将通过SVMs和关于使用GPU以在中等规模数据集上运行这些算法的简要说明来完成我们的概述。所有示例都需要第4章中介绍的Airbnb纽约市数据集。您可以通过执行以下代码片段来重新执行它：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① List of column names to be excluded from the analysis
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ① 要排除的分析列名列表
- en: ② List of names of columns that likely represent categorical variables in the
    dataset
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ② 可能代表数据集中分类变量的列名列表
- en: ③ List of names of columns that represent continuous numerical variables in
    the dataset
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 代表数据集中连续数值变量的列名列表
- en: ④ A binary balanced target
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 二元平衡目标
- en: The code will load your dataset and define what features to be excluded from
    the analysis or considered as continuous or categorical for processing purposes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将加载您的数据集并定义要排除的分析特征或考虑为连续或分类变量以进行处理的特征。
- en: B.1 k-NN
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 k-NN
- en: Applicable to regression and classification tasks, the k-NN algorithm is considered
    one of the simplest and most intuitive algorithms for making predictions. It finds
    the k (where k is an integer number) closest examples from the training set and
    uses their information to make a prediction. For example, if the task is a regression,
    it will take the average of the k closest examples. If the task is a classification,
    it will choose the most common class among the k closest examples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法适用于回归和分类任务，被认为是制作预测的最简单和最直观的算法之一。它从训练集中找到k个（k是一个整数）最近的例子，并使用它们的信息进行预测。例如，如果任务是回归，它将取k个最近例子平均值。如果任务是分类，它将在k个最近例子中选择最常见的类别。
- en: Technically, k-NN is commonly regarded as an instance-based learning algorithm
    because it memorizes the training examples as they are. It is also regarded as
    a “lazy algorithm” because, contrary to most machine learning algorithms, there
    is little processing at training time. During training, there is usually some
    processing of the distances by optimized algorithms and data structures that render
    it less computationally costly afterward to look for the neighboring points near
    a training example. Most of the computational work is done at testing time (see
    figure B.1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，k-NN通常被认为是一种基于实例的学习算法，因为它以原样记住训练示例。它也被认为是一种“懒惰算法”，因为与大多数机器学习算法相反，在训练时间几乎没有处理。在训练期间，通常会有一些通过优化算法和数据结构处理距离的过程，这使得在训练示例附近查找邻近点之后的计算成本较低。大部分的计算工作是在测试时间完成的（见图B.1）。
- en: '![](../Images/APPB_F01_Ryan2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPB_F01_Ryan2.png)'
- en: Figure B.1 Classifying new samples (the triangles) with a k-NN where k = 3
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.1 使用k = 3的k-NN对新样本（三角形）进行分类
- en: We apply a k-NN classifier to the Airbnb NYC data, as seen in chapter 4, in
    listing B.1\. Since k-NN works based on distances, to obtain a functioning solution,
    features must be on the same scale, thus assuring an equal weight to each dimension
    in the distance measurement process. If a feature is on a different or smaller
    scale, it would be overweighted in the process. The contrary would happen if a
    larger scale characterizes a feature. To give an idea of the problem, let’s consider
    what happens when we compare distances based on kilometers, meters, and centimeters.
    Even if distances are comparable, meters and centimeters will numerically exceed
    kilometers measurements. This problem is usually solved by scaling features—for
    instance, by subtracting their mean and dividing by their standard deviation (an
    operation known as z-score normalization
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将k-NN分类器应用于第4章中提到的Airbnb纽约市数据，如附录B.1所示。由于k-NN是基于距离工作的，为了获得一个有效的解决方案，特征必须在同一尺度上，从而确保在距离测量过程中每个维度都有相同的权重。如果一个特征在不同的或较小的尺度上，它会在过程中被过度加权。如果较大的尺度描述了一个特征，则相反的情况会发生。为了说明这个问题，让我们考虑当我们基于千米、米和厘米比较距离时会发生什么。即使距离是可比较的，米和厘米的数值将超过千米测量值。这个问题通常通过缩放特征来解决——例如，通过减去它们的平均值并除以它们的标准差（这种操作称为z分数标准化）。
- en: or standardization). Also, techniques such as dimensional reduction or feature
    selection are helpful with this algorithm because rearranging predictors or different
    sets of predictors may result in more or less predictive performances on the problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或标准化）。此外，诸如降维或特征选择等技术对于此算法也是有帮助的，因为重新排列预测因子或不同的预测因子集可能会导致在问题上的预测性能有所提高或降低。
- en: 'In our case, as it is often with tabular data, the situation is complicated
    by categorical features, which, once one-hot encoded, will turn into binaries
    ranging from 0 to 1, with a different scale from normalized features. The solution
    we propose is first to discretize the numeric features, thus effectively turning
    them into binary features, each representing if a feature’s numeric values will
    fall into a specific range. Binarization of continuous features is obtained thanks
    to the KBinsDiscretizer class ([https://mng.bz/N12N](https://mng.bz/N12N)) embedded
    into the `numeric_discretizing` pipeline, which will turn each numeric feature
    into five binary ones, each one covering a bin of values. At processing time,
    we also apply principal component analysis (PCA) to reduce the dimensionality
    and make all the features unrelated. However, we might attenuate nonlinearities
    inside the data since PCA is a technique based on linear combinations of the variables.
    Having uncorrelated resulting features is a characteristic of data processed by
    PCA, which suits k-NN: k-NN is based on distances, and distance measurement properly
    works if dimensions are unrelated. Therefore, any distance change is due to changes
    in a single dimension, not multiple ones. The following listing shows the code
    implementing the data transformation process and training the k-NN.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，正如表格数据通常所做的那样，情况因分类特征而复杂化，这些特征一旦进行独热编码，就会变成从0到1的二进制值，其比例与归一化特征不同。我们提出的解决方案是首先对数值特征进行离散化，从而有效地将它们转换为二进制特征，每个特征表示一个特征的数值是否将落在特定的范围内。连续特征的二值化是通过嵌入到`numeric_discretizing`管道中的KBinsDiscretizer类（[https://mng.bz/N12N](https://mng.bz/N12N)）实现的，它将每个数值特征转换为五个二进制特征，每个特征覆盖一个值范围。在处理时间，我们还应用主成分分析（PCA）来降低维度并使所有特征无关。然而，我们可能会减弱数据中的非线性，因为PCA是一种基于变量线性组合的技术。PCA处理的数据具有不相关结果特征的特点，这适合k-NN：k-NN基于距离，如果维度无关，距离测量才能正确工作。因此，任何距离变化都是由于单个维度的变化，而不是多个维度的变化。以下列表显示了实现数据转换过程和训练k-NN的代码。
- en: Listing B.1 k-NN classifier
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.1 k-NN分类器
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Creates a scoring function using the accuracy_score metric
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用accuracy_score度量创建一个评分函数
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有洗牌和固定随机状态的五折交叉验证迭代器
- en: ③ Creates an instance of the KNeighborsClassifier with specified hyperparameters
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个具有指定超参数的KNeighborsClassifier实例
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and discretization to numerical features
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义一个ColumnTransformer来预处理特征，对低基数分类特征应用独热编码，对数值特征应用离散化
- en: ⑤ Creates a pipeline that sequentially applies the column transformation, performs
    PCA dimensionality reduction, and then fits the k-nn model to the data
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个管道，按顺序应用列转换，执行PCA降维，然后将k-nn模型拟合到数据上
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用定义的管道对数据进行交叉验证，使用准确率评分
- en: ⑦ Prints the mean and standard deviation of test scores
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打印测试分数的均值和标准差
- en: 'After running the script, you will obtain a result that is close to the performance
    of the Naive Bayes solution:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，您将获得一个接近朴素贝叶斯解决方案性能的结果：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The performance is good, though the inference time is relatively high. Since
    this algorithm works by analogy (it will look for similar cases in your training
    to get an idea of the possible prediction), it performs better with large enough
    datasets where there is a higher likelihood of finding some instances resembling
    those to be predicted. Naturally, the right size for the dataset is dictated by
    the number of features used because the more features, the more cases you will
    need for the algorithm to generalize well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 性能良好，尽管推理时间相对较高。由于此算法通过类比工作（它将在训练中寻找类似案例以获得可能的预测想法），因此在大数据集上表现更好，在大数据集中找到与要预测的实例相似的实例的可能性更高。自然地，数据集的正确大小由使用的特征数量决定，因为特征越多，算法需要更多的案例来很好地泛化。
- en: 'Though often the emphasis is placed on setting the best value to the k parameter
    as the key to balancing the underfitting and overfitting of the algorithm to the
    training data, we instead raise attention to other aspects for an effective employ
    of this model. As the algorithm works by analogy and distances in complex spaces,
    we consider two important matters about this approach:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常人们将重点放在设置k参数的最佳值，将其视为平衡算法对训练数据欠拟合和过拟合的关键，但我们反而将注意力转向其他方面，以有效地使用此模型。由于算法通过类比和复杂空间中的距离来工作，我们考虑了关于此方法两个重要的问题：
- en: The dimensions to measure and the curse of dimensionality
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要测量的维度和维度诅咒
- en: The appropriate distance measure and how to process the features
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的距离度量以及如何处理特征
- en: In k-NN, the classification or the regression estimates depend on the most similar
    examples based on a distance metric computed on the features. However, in a dataset,
    not all features can be deemed important in judging an example similar to the
    other, and not all of the features can be compared in the same way. Prior knowledge
    of the problem does count a lot when using k-NN because you have to select only
    the features relevant to the task you want to solve. If you assemble too many
    features for the problem, you will rely on too much complex space to navigate.
    Figure B.1 shows how a k-NN algorithm works with just two features (represented
    on the x and y dimensions), and you can intuitively grasp that classifying new
    instances (the triangles in the figure) may be difficult if there are mixed classes
    in an area or if there are no train examples near to a new instance. You have
    to rely on farther ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-NN中，分类或回归估计取决于基于特征计算的距离度量的最相似示例。然而，在数据集中，并非所有特征都可以被认为在判断一个示例与其他示例相似时很重要，并且并非所有特征都可以以相同的方式进行比较。在应用k-NN时，对问题的先验知识非常重要，因为你必须只选择与你要解决的问题相关的特征。如果你为问题组装了过多的特征，你将依赖于过多的复杂空间来导航。图B.1展示了k-NN算法如何仅使用两个特征（在x和y维度上表示）工作，你可以直观地理解，如果某个区域有混合的类别或者没有训练示例靠近新实例，那么对新实例进行分类（图中的三角形）可能很困难。你必须依赖于更远的那些实例。
- en: Here comes into the game the curse of dimensionality, which says that as the
    number of features increases, the more examples you should have available to maintain
    a meaningful distance between data points. In addition, the curse implies that
    the number of necessary examples grows exponentially with respect to the number
    of features. For a k-NN algorithm, it means that if you provide too many features,
    it will work in an empty space if the number of examples is not enough. Looking
    for neighbors will become daunting. In addition, if you have just assembled relevant
    and irrelevant features, the risk is that the algorithm will mark as neighbors
    some examples that are very far from the case you have to predict, and the choice
    could be based on features that are not useful for the problem. Hence, if you
    are going to use k-NN, you should choose with great care the features to be used
    (if you don’t know which to use, you need to rely on feature selection) or be
    very familiar with the problem to determine what should go into the algorithm.
    Parsimony is essential for the proper working of a k-NN.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏中出现了维度诅咒，它指出随着特征数量的增加，你需要更多的示例来保持数据点之间有意义的距离。此外，维度诅咒还意味着必要示例的数量会随着特征数量的增加而指数级增长。对于k-NN算法来说，这意味着如果你提供了过多的特征，而示例数量不足，它将在空空间中工作。寻找邻居将变得令人畏惧。此外，如果你只是组装了相关和不相关的特征，风险是算法可能会将一些与你要预测的案例非常远的示例标记为邻居，并且选择可能基于对问题无用的特征。因此，如果你打算使用k-NN，你应该非常小心地选择要使用的特征（如果你不知道使用哪些，你需要依赖特征选择）或者非常熟悉问题，以确定应该将什么放入算法中。简约对于k-NN的正确工作至关重要。
- en: When you have decided about the features, regarding the distance metric you
    will be using, you will need to standardize, remove redundancies, and transform
    the features. This is because distance metrics are based on absolute measurements,
    and different scales can weigh in different ways. Consider using measurements
    in kilometers, meters, and centimeters together. The centimeters will likely predominate
    because they will easily have the largest numbers. Also, having features similar
    to each other (the problem of multicollinearity) can entail the distance measurement
    to overweight certain sets of features over others. Finally, a distance measurement
    implies having the same dimensions to measure. However, in a dataset, you may
    find different kinds of data— numeric, categorical, and time-based—which often
    need to fit together better in a distance calculation because they have different
    numeric characteristics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当你决定好特征后，关于你将使用的距离度量，你需要标准化、去除冗余和转换特征。这是因为距离度量基于绝对测量，不同的尺度可以以不同的方式权衡。考虑使用千米、米和厘米的测量值。厘米可能会占主导地位，因为它们很容易就有最大的数字。此外，具有相似特征（多重共线性问题）可能导致距离测量对某些特征集的权重超过其他特征集。最后，距离测量意味着具有相同的维度进行测量。然而，在数据集中，你可能会发现不同类型的数据——数值、分类和时间相关的数据，它们通常需要在距离计算中更好地结合在一起，因为它们具有不同的数值特征。
- en: For this reason, in addition to carefully selecting beforehand which features
    to use, when employing k-NN, we suggest using features that are all of the same
    kind (or all numeric or all categorical) to standardize them if necessary and
    also to reduce their informative redundancy by methods such as PCA ([https://mng.bz/8OrZ](https://mng.bz/8OrZ)),
    which will reformulate the dataset into a new one where features are not correlated
    between themselves.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了事先仔细选择要使用的特征外，当使用 k-NN 时，我们建议使用所有同种类的特征（或所有数值或所有分类）来标准化它们，如果需要的话，并通过如 PCA（[https://mng.bz/8OrZ](https://mng.bz/8OrZ)）等方法减少它们的信噪比，这将重新制定数据集成为一个新的数据集，其中特征之间不相关。
- en: B.2 SVMs
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 支持向量机（SVMs）
- en: Before the 2010s, SVMs had a reputation as the most promising algorithm for
    tabular problems. However, in the past 10 years, tree-based models have eclipsed
    SVMs as the go-to approach for tabular data. However, SVMs remain a family of
    techniques for handling binary, multiclass, regression, and anomaly/novelty detection.
    They are based on the idea that if your observations can be represented as points
    in a multidimensional space, there is a hyperplane (i.e., a separation plane cutting
    through multiple dimensions) that can separate them into classes or values that,
    by assuring the largest separation possible between them, also guarantees the
    most robust and reliable predictions. Figure B.2 shows a simple example of an
    SVM applied to a binary classification problem with two features, represented
    on the x- and y-axis, as predictors. The SVM model produces a separator line with
    the largest slack space between the two groups, as shown in the figure, where
    the dashed lines delimit the slack space. In doing so, it considers only a few
    points near the separator, called the support vectors. Instead, it ignores the
    points that are near but are confusing for the algorithm because, for instance,
    they are on the wrong side. It also ignores the points that are far away from
    the separator line. Outliers have little influence on this algorithm.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2010 年之前，SVMs 以表格问题中最有前途的算法而闻名。然而，在过去 10 年中，基于树的模型已经超越了 SVMs，成为表格数据的首选方法。然而，SVMs
    仍然是一系列处理二元、多类、回归和异常/新颖性检测的技术。它们基于这样的想法：如果你的观察结果可以表示为多维空间中的点，那么存在一个超平面（即穿过多个维度的分离平面）可以将它们分开成类别或值，通过确保它们之间最大的分离，也保证了最稳健和可靠的预测。图
    B.2 展示了一个简单的 SVM 应用到具有两个特征的二元分类问题示例，这些特征在 x 轴和 y 轴上表示，作为预测因子。SVM 模型产生了一条分隔线，在两组之间有最大的松弛空间，如图所示，其中虚线界定松弛空间。在这样做的时候，它只考虑靠近分隔器的几个点，称为支持向量。相反，它忽略了靠近但会混淆算法的点，例如，它们在错误的一侧。它还忽略了远离分隔线远离的点。异常值对这种算法的影响很小。
- en: '![](../Images/APPB_F02_Ryan2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPB_F02_Ryan2.png)'
- en: Figure B.2 A separating hyperplane from a SVM
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.2 SVM 中的一个分离超平面
- en: The strong points of SVMs are their robust handling of overfitting, noise in
    data, and outliers and how they can successfully handle datasets that include
    numerous multicollinear features. Applying different nonlinear approaches to data,
    SVMs don’t require the transformations (such as polynomial expansion) we have
    seen for logistic regression. However, they can use domain-based feature engineering
    like all other machine learning algorithms.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的强点在于它们对过拟合、数据中的噪声和异常值的鲁棒处理，以及它们如何成功处理包含众多多重共线性特征的集合。将不同的非线性方法应用于数据时，SVM不需要我们为逻辑回归所看到的变换（如多项式展开）。然而，它们可以使用基于领域的特征工程，就像所有其他机器学习算法一样。
- en: On the weak side, SVM optimization is complex and can be feasible only for a
    limited number of examples. Moreover, they are best fit for binary predictions
    and for just class prediction; they are not a probabilistic algorithm, and you
    need to wrap them with another algorithm for calibration (such as logistic regression)
    to extract probabilities from them. That renders SVMs valid only for a limited
    range of tasks in risk estimation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在弱点方面，SVM优化复杂，并且仅适用于有限数量的示例。此外，它们最适合二元预测和仅用于类别预测；它们不是概率算法，你需要将它们与另一个算法（如逻辑回归）结合使用以进行校准（以从它们中提取概率）。这使得SVM在风险估计的有限范围内有效。
- en: In our example, we reapply our problem with Airbnb NYC data using a binary classification
    SVM using a radial basis function kernel, an approach capable of automatically
    modeling complex nonlinear relationships between the provided features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用具有径向基函数核的二进制分类SVM和Airbnb纽约市数据重新应用我们的问题，这是一种能够自动建模提供特征之间复杂非线性关系的途径。
- en: Listing B.2 Support vector classifier
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.2 支持向量机分类器
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Creates a scoring function using the accuracy_score metric
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用accuracy_score度量创建一个评分函数
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有洗牌和固定随机状态的五折交叉验证迭代器
- en: ③ Creates an instance of the Support Vector Classifier with specified hyperparameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个具有指定超参数的支持向量机分类器实例
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and standardization to numerical
    features
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义一个ColumnTransformer以预处理特征，对低基数分类特征应用独热编码，对数值特征进行标准化
- en: ⑤ Creates a pipeline that sequentially applies the column transformation
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个管道，按顺序应用列转换
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用定义的管道在数据上执行交叉验证，使用准确率评分
- en: ⑦ Prints the mean and standard deviation of test scores
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打印测试分数的均值和标准差
- en: 'The results are pretty interesting, and they can probably even turn better
    by adjusting the hyperparameters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果相当有趣，通过调整超参数可能还会变得更好：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, the time needed for training a single fold is excessive compared to
    all the previous machine learning algorithms. In the next section of this appendix,
    we will discuss how GPU cards can speed up the process while still using the Scikit-learn
    API.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练单个折叠所需的时间与所有之前的机器学习算法相比过于冗长。在本附录的下一节中，我们将讨论如何使用GPU卡加速过程，同时仍然使用Scikit-learn
    API。
- en: B.3 Using GPUs for machine learning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 使用GPU进行机器学习
- en: Due to the rapid rise of deep learning in the data science field, GPUs are now
    widespread and accessible both for local and cloud computing. Earlier, you only
    heard about GPUs in 3D gaming, graphic processing rendering, and animation. Since
    they are cheap and apt at fast matrix multiplication tasks, academics and practitioners
    have rapidly picked up GPUs for neural network computations. RAPIDS, developed
    by NVIDIA (one of the top manufacturers of GPUs), is a set of packages for doing
    the full spectrum of data science, not just deep learning, on GPUs. The RAPIDS
    packages promise to help in all phases of a machine learning pipeline, end to
    end. That’s a game changer for many classical machine learning algorithms, especially
    for the SVMs, the most credible choice for more complex tasks involving noisy,
    outlying observations and vast datasets (having a large set of features, especially
    if multicollinear or sparse ones). In the RAPIDS packages (table B.1), all commands
    have adopted existing APIs for their commands. Such assures an immediate market
    adoption of the packages, and for the user, there is no need to relearn how the
    wheel works.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习在数据科学领域的迅速崛起，GPU现在在本地和云计算中都得到了广泛应用。以前，你只听说过GPU在3D游戏、图形处理渲染和动画中的应用。由于它们便宜且擅长快速矩阵乘法任务，学者和实践者迅速将GPU用于神经网络计算。RAPIDS是由NVIDIA（GPU顶级制造商之一）开发的一系列用于在GPU上执行数据科学全光谱的包，而不仅仅是深度学习。RAPIDS包承诺帮助机器学习管道的各个阶段，从端到端。这对许多经典机器学习算法来说是一个变革，特别是对于SVMs，它是处理涉及噪声、异常值和大型数据集（特别是如果特征多线性或稀疏）的复杂任务的最可靠选择。在RAPIDS包（表B.1）中，所有命令都采用了现有的API作为它们的命令。这确保了包的即时市场采用，对于用户来说，无需重新学习轮子的工作方式。
- en: Table B.1 Rapids packages
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表B.1 Rapids包
- en: '| Rapids package | Task | API mimicked |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Rapids包 | 任务 | API模拟 |'
- en: '| --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| cuPy | Array operations | NumPy |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| cuPy | 数组操作 | NumPy |'
- en: '| cuDF | Data processing | pandas |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| cuDF | 数据处理 | pandas |'
- en: '| cuML | Machine learning | Scikit-learn |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| cuML | 机器学习 | Scikit-learn |'
- en: This section will focus on how easy it is to replace your Scikit-learn algorithms
    with the RAPIDS cuML package. Currently, this package includes implementations
    for linear models, k-NN, and SVMs, as well as for clustering and dimensionality
    reduction. The following listing shows the code for testing the support vector
    classifier with the radial basis function kernel we just tried in the previous
    section in its RAPIDS implementation (using a P100 GPU).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍如何轻松地将你的Scikit-learn算法替换为RAPIDS cuML包。目前，此包包括线性模型、k-NN和SVMs的实现，以及聚类和降维。以下列表显示了测试支持向量分类器（使用我们在上一节中尝试的径向基函数核）的RAPIDS实现（使用P100
    GPU）的代码。
- en: Listing B.3 Support vector classifier from RAPIDS cuML
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.3 RAPIDS cuML支持向量分类器
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Creates a scoring function using the accuracy_score metric
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用accuracy_score度量创建评分函数
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有洗牌和固定随机状态的五折交叉验证迭代器
- en: ③ Creates an instance of a Support Vector Classifier from the GPU-accelerated
    cuML library with specified hyperparameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从GPU加速的cuML库中创建一个支持向量分类器实例，并指定超参数
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and standardization to numerical
    features
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义一个ColumnTransformer来预处理特征，对低基数分类特征应用独热编码，对数值特征应用标准化
- en: ⑤ Creates a pipeline that sequentially applies the column transformation and
    the model to the data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个管道，按顺序将列转换和模型应用于数据
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用定义的管道在数据上执行交叉验证，并使用准确率评分
- en: ⑦ Prints the mean and standard deviation of test scores
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打印测试分数的均值和标准差
- en: The results we obtained are
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得的结果是
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, we obtained the same results by reusing the same code but relying
    on cuML. However, the processing time has dropped from 102 seconds per folder
    to 4 seconds per folder. If you calculate the time savings, that’s a 25x speed
    increase. The exact performance benefit depends on the GPU model you use; the
    more powerful the GPU, the speedier the results because it depends on how fast
    the GPU card can transfer data from CPU memory and how fast it can process a matrix
    multiplication.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们通过重用相同的代码但依赖cuML获得了相同的结果。然而，处理时间已从每个文件夹102秒降低到每个文件夹4秒。如果您计算时间节省，那将是25倍的速度提升。确切的表现效益取决于您使用的GPU型号；GPU越强大，结果越快，因为这与GPU卡从CPU内存传输数据以及处理矩阵乘法的速度有关。
- en: Based on such performances on standard GPUs accessible to the general public,
    we recently saw applications fusing tabular data with large embeddings from deep
    learning models (such as text or images). SVMs work well with numerous features
    (but not more than the examples) and sparse values (many zero values). In such
    situations, SVMs can easily obtain a state-of-the-art result, outperforming other
    more popular tabular algorithms at this time—namely XGBoost and other gradient
    boosting implementations as well as end-to-end deep learning solutions, which
    are weaker when you don’t have enough cases to feed them with.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于标准GPU（公众可访问的通用GPU）上的此类性能，我们最近看到了将表格数据与深度学习模型（如文本或图像）的大嵌入融合的应用。SVMs（支持向量机）与众多特征（但不超过示例中的数量）和稀疏值（许多零值）配合良好。在这种情况下，SVMs可以轻松获得最先进的结果，超越当时其他更受欢迎的表格算法——即XGBoost和其他梯度提升实现，以及端到端深度学习解决方案，后者在没有足够案例提供时表现较弱。
- en: Having a GPU and adapting your code to use RAPIDS algorithms makes certain classic
    algorithms for tabular machine learning quite competitive again, as a general
    rule, based on the principle that there is no free lunch in machine learning (more
    details about no-free-lunch theorems are available at [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/)).
    Taking into account your project constraints (for instance, you may not have certain
    resources available in your project environment), never exclude apriori testing
    your problem against all available algorithms, if this is feasible.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有GPU并使您的代码适应使用RAPIDS算法，使得某些经典的表格机器学习算法再次具有竞争力，这通常基于机器学习中没有免费午餐的原则（关于没有免费午餐定理的更多细节可在[http://www.no-free-lunch.org/](http://www.no-free-lunch.org/)找到）。考虑到您的项目限制（例如，您可能无法在项目环境中获得某些资源），如果可行，永远不要排除在先验测试中将您的问题与所有可用算法进行比较。
