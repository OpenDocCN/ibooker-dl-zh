- en: Image classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter08_image-classification](https://deeplearningwithpython.io/chapters/chapter08_image-classification)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://deeplearningwithpython.io/chapters/chapter08_image-classification](https://deeplearningwithpython.io/chapters/chapter08_image-classification)'
- en: Computer vision was the first big success story of deep learning. It led to
    the initial rise of deep learning between 2011 and 2015. A type of deep learning
    called *convolutional neural networks* started getting remarkably good results
    on image classification competitions around that time, first with Dan Ciresan
    winning two niche competitions (the ICDAR 2011 Chinese character recognition competition
    and the IJCNN 2011 German traffic signs recognition competition) and then, more
    notably, in fall 2012, with Hinton’s group winning the high-profile ImageNet large-scale
    visual recognition challenge. Many more promising results quickly started bubbling
    up in other computer vision tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习第一个重大的成功故事。它导致了2011年至2015年间深度学习的初步兴起。一种名为**卷积神经网络**的深度学习类型在那个时期开始在图像分类竞赛中取得显著的好成绩，最初是丹·西雷桑赢得了两个小众竞赛（2011年ICDAR汉字识别竞赛和2011年IJCNN德国交通标志识别竞赛），然后，更为显著的是，在2012年秋季，辛顿团队赢得了备受瞩目的ImageNet大规模视觉识别挑战赛。在其他计算机视觉任务中，更多有希望的结果也迅速涌现。
- en: Interestingly, these early successes weren’t quite enough to make deep learning
    mainstream at the time — it took a few years. The computer vision research community
    had spent many years investing in methods other than neural networks, and it wasn’t
    quite ready to give up on them just because there was a new kid on the block.
    In 2013 and 2014, deep learning still faced intense skepticism from many senior
    computer vision researchers. It was only in 2016 that it finally became dominant.
    One author remembers exhorting an ex-professor, in February 2014, to pivot to
    deep learning. “It’s the next big thing!” he would say. “Well, maybe it’s just
    a fad,” the professor would reply. By 2016, his entire lab was doing deep learning.
    There’s no stopping an idea whose time has come.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这些早期的成功并不足以使深度学习在当时成为主流——这需要几年时间。计算机视觉研究界已经投入了多年时间研究神经网络以外的其他方法，并且并不因为新来者的出现就准备放弃它们。2013年和2014年，深度学习仍然面临着许多资深计算机视觉研究者的强烈怀疑。直到2016年，它才最终成为主流。一位作者记得在2014年2月敦促一位前教授转向深度学习。“这是下一个大趋势！”他会说。“嗯，也许这只是个潮流，”教授会回答。到2016年，他整个实验室都在做深度学习。一个时代到来了，任何阻止它的想法都是徒劳的。
- en: Today, you’re constantly interacting with deep learning–based vision models
    — via Google Photos, Google image search, the camera on your phone, YouTube, OCR
    software, and many more. These models are also at the heart of cutting-edge research
    in autonomous driving, robotics, AI-assisted medical diagnosis, autonomous retail
    checkout systems, and even autonomous farming.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你不断地与基于深度学习的视觉模型互动——通过Google Photos、Google图片搜索、你手机上的相机、YouTube、OCR软件等等。这些模型也是自动驾驶、机器人、AI辅助医疗诊断、自主零售结账系统，甚至自主农业等尖端研究的核心。
- en: This chapter introduces convolutional neural networks, also known as *ConvNets*
    or *CNNs*, the type of deep-learning model that is used by most computer vision
    applications. You’ll learn to apply ConvNets to image classification problems
    — in particular, those involving small training datasets, which are the most common
    use case if you aren’t a large tech company.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了卷积神经网络，也称为**ConvNets**或**CNNs**，这是一种大多数计算机视觉应用所使用的深度学习模型。你将学习如何将卷积神经网络应用于图像分类问题——特别是那些涉及小型训练数据集的问题，如果你不是大型科技公司，这将是最常见的用例。
- en: Introduction to ConvNets
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络简介
- en: We’re about to dive into the theory of what ConvNets are and why they have been
    so successful at computer vision tasks. But first, let’s take a practical look
    at a simple ConvNet example. It uses a ConvNet to classify MNIST digits, a task
    we performed in chapter 2 using a densely connected network (our test accuracy
    then was 97.8%). Even though the ConvNet will be basic, its accuracy will blow
    out of the water that of the densely connected model from chapter 2.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将深入探讨卷积神经网络（ConvNets）的理论，以及它们为何在计算机视觉任务中取得了如此巨大的成功。但首先，让我们先从实际的角度来看一个简单的卷积神经网络示例。它使用卷积神经网络来分类MNIST数字，这是我们第二章中使用密集连接网络（那时我们的测试准确率为97.8%）所执行的任务。尽管这个卷积神经网络将非常基础，但它的准确率将远远超过第二章中密集连接模型的水平。
- en: The following lines of code show you what a basic ConvNet looks like. It’s a
    stack of `Conv2D` and `MaxPooling2D` layers. You’ll see in a minute exactly what
    they do. We’ll build the model using the Functional API, which we introduced in
    the previous chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行展示了基本卷积神经网络（ConvNet）的外观。它是由一系列的`Conv2D`和`MaxPooling2D`层堆叠而成。你将在下一分钟内确切地看到它们的作用。我们将使用在上一章中介绍的功能API来构建模型。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 8.1](#listing-8-1): Instantiating a small ConvNet'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.1](#listing-8-1)：实例化一个小型卷积神经网络'
- en: Importantly, a ConvNet takes as input tensors of shape `(image_height, image_width,
    image_channels)` (not including the batch dimension). In this case, we’ll configure
    the ConvNet to process inputs of size `(28, 28, 1)`, which is the format of MNIST
    images.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，卷积神经网络（ConvNet）接受形状为`(image_height, image_width, image_channels)`的张量作为输入（不包括批处理维度）。在这种情况下，我们将配置卷积神经网络（ConvNet）以处理大小为`(28,
    28, 1)`的输入，这是MNIST图像的格式。
- en: Let’s display the architecture of our ConvNet.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示我们的卷积神经网络（ConvNet）的架构。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 8.2](#listing-8-2): Displaying the model’s summary'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.2](#listing-8-2)：显示模型的摘要'
- en: You can see that the output of every `Conv2D` and `MaxPooling2D` layer is a
    3D tensor of shape `(height, width, channels)`. The width and height dimensions
    tend to shrink as you go deeper in the model. The number of channels is controlled
    by the first argument passed to the `Conv2D` layers (64, 128, or 256).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个`Conv2D`和`MaxPooling2D`层的输出都是一个形状为`(height, width, channels)`的三维张量。随着你在模型中深入，宽度和高度维度往往会缩小。通道数由传递给`Conv2D`层的第一个参数控制（64、128或256）。
- en: 'After the last `Conv2D` layer, we end up with an output of shape `(3, 3, 256)`
    — a 3 × 3 feature map of 256 channels. The next step is to feed this output into
    a densely connected classifier like those you’re already familiar with: a stack
    of `Dense` layers. These classifiers process vectors, which are 1D, whereas the
    current output is a rank-3 tensor. To bridge the gap, we flatten the 3D outputs
    to 1D with a `GlobalAveragePooling2D` layer before adding the `Dense` layers.
    This layer will take the average of each 3 × 3 feature map in the tensor of shape
    `(3, 3, 256)`, resulting in an output vector of shape `(256,)`. Finally, we’ll
    do 10-way classification, so our last layer has 10 outputs and a softmax activation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个`Conv2D`层之后，我们得到一个形状为`(3, 3, 256)`的输出——一个256通道的3×3特征图。下一步是将这个输出输入到一个密集连接的分类器中，就像你已经熟悉的那些：一系列的`Dense`层。这些分类器处理向量，它们是一维的，而当前的输出是一个三阶张量。为了弥合这个差距，我们在添加`Dense`层之前，使用一个`GlobalAveragePooling2D`层将3D输出展平到一维。这个层将张量形状为`(3,
    3, 256)`中的每个3×3特征图的平均值取出来，结果得到一个形状为`(256,)`的输出向量。最后，我们将进行10种分类，所以我们的最后一层有10个输出和一个softmax激活函数。
- en: Now, let’s train the ConvNet on the MNIST digits. We’ll reuse a lot of the code
    from the MNIST example in chapter 2\. Because we’re doing 10-way classification
    with a softmax output, we’ll use the categorical crossentropy loss, and because
    our labels are integers, we’ll use the sparse version, `sparse_categorical_crossentropy`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在MNIST数字上训练卷积神经网络（ConvNet）。我们将重用第2章中MNIST示例中的大量代码。因为我们正在进行带有softmax输出的10种分类，所以我们将使用分类交叉熵损失，因为我们标签是整数，我们将使用稀疏版本，`sparse_categorical_crossentropy`。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 8.3](#listing-8-3): Training the ConvNet on MNIST images'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.3](#listing-8-3)：在MNIST图像上训练卷积神经网络'
- en: Let’s evaluate the model on the test data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试数据上评估模型。
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 8.4](#listing-8-4): Evaluating the ConvNet'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.4](#listing-8-4)：评估卷积神经网络'
- en: 'Whereas the densely connected model from chapter 2 had a test accuracy of 97.8%,
    the basic ConvNet has a test accuracy of 99.1%: we decreased the error rate by
    about 60% (relative). Not bad!'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与第2章中的密集连接模型相比，测试准确率为97.8%，而基本的卷积神经网络（ConvNet）的测试准确率为99.1%：我们降低了大约60%（相对）的错误率。还不错！
- en: But why does this simple ConvNet work so well, compared to a densely connected
    model? To answer this, let’s dive into what the `Conv2D` and `MaxPooling2D` layers
    do.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么与密集连接模型相比，这个简单的卷积神经网络（ConvNet）表现得如此出色？为了回答这个问题，让我们深入了解`Conv2D`和`MaxPooling2D`层的作用。
- en: The convolution operation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'The fundamental difference between a densely connected layer and a convolution
    layer is this: `Dense` layers learn global patterns in their input feature space
    (for example, for a MNIST digit, patterns involving all pixels), whereas convolution
    layers learn local patterns (see figure 8.1): in the case of images, patterns
    found in small 2D windows of the inputs. In the previous example, these windows
    were all 3 × 3.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接层和卷积层之间的基本区别是这样的：`Dense`层在其输入特征空间中学习全局模式（例如，对于一个MNIST数字，涉及所有像素的模式），而卷积层学习局部模式（见图8.1）：在图像的情况下，在输入的小2D窗口中找到的模式。在先前的例子中，这些窗口都是3
    × 3。
- en: '![](../Images/1d29271b09d5f37759936df42a9f5aca.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1d29271b09d5f37759936df42a9f5aca.png)'
- en: '[Figure 8.1](#figure-8-1): Images can be broken into local patterns such as
    edges, textures, and so on.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.1](#figure-8-1)：图像可以被分解成局部模式，如边缘、纹理等。'
- en: 'This key characteristic gives ConvNets two interesting properties:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关键特性给卷积神经网络带来了两个有趣的特性：
- en: '*The patterns they learn are translation invariant*. After learning a certain
    pattern in the lower-right corner of a picture, a ConvNet can recognize it anywhere:
    for example, in the upper-left corner. A densely connected model would have to
    learn the pattern anew if it appeared at a new location. This makes ConvNets data
    efficient when processing images — because *the visual world is fundamentally
    translation invariant*. They need fewer training samples to learn representations
    that have generalization power.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们学习的模式是平移不变的*。在图片的右下角学习到一定模式后，卷积神经网络可以在任何地方识别它：例如，在左上角。如果密集连接模型在新的位置出现，它必须重新学习该模式。这使得卷积神经网络在处理图像时数据效率高——因为*视觉世界在本质上具有平移不变性*。它们需要更少的训练样本来学习具有泛化能力的表示。'
- en: '*They can learn spatial hierarchies of patterns (see figure 8.2)*. A first
    convolution layer will learn small local patterns such as edges, a second convolution
    layer will learn larger patterns made of the features of the first layers, and
    so on. This allows ConvNets to efficiently learn increasingly complex and abstract
    visual concepts — because *the visual world is fundamentally spatially hierarchical*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们可以学习模式的空间层次结构（见图8.2）*。第一层卷积层将学习小的局部模式，如边缘，第二层卷积层将学习由第一层特征组成的大模式，依此类推。这使得卷积神经网络能够高效地学习越来越复杂和抽象的视觉概念——因为*视觉世界在本质上具有空间层次性*。'
- en: '![](../Images/fd7bec6c7a9246339ceec605eca1d67b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/fd7bec6c7a9246339ceec605eca1d67b.png)'
- en: '[Figure 8.2](#figure-8-2): The visual world forms a spatial hierarchy of visual
    modules: elementary lines or textures combine into simple objects such as eyes
    or ears, which combine into high-level concepts such as “cat.”'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.2](#figure-8-2)：视觉世界形成了一个视觉模块的空间层次结构：基本的线条或纹理组合成简单的物体，如眼睛或耳朵，这些物体再组合成高级概念，如“猫”。'
- en: 'Convolutions operate over rank-3 tensors, called *feature maps*, with two spatial
    axes (*height* and *width*) as well as a *depth* axis (also called the *channels*
    axis). For an RGB image, the dimension of the depth axis is 3, because the image
    has three color channels: red, green, and blue. For a black-and-white picture,
    like the MNIST digits, the depth is 1 (levels of gray). The convolution operation
    extracts patches from its input feature map and applies the same transformation
    to all of these patches, producing an *output feature map*. This output feature
    map is still a rank-3 tensor: it has a width and a height. Its depth can be arbitrary
    because the output depth is a parameter of the layer, and the different channels
    in that depth axis no longer stand for specific colors as in RGB input; rather,
    they stand for *filters*. Filters encode specific aspects of the input data: at
    a high level, a single filter could encode the concept “presence of a face in
    the input,” for instance.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作在称为*特征图*的三阶张量上操作，具有两个空间轴（*高度*和*宽度*）以及一个*深度*轴（也称为*通道*轴）。对于RGB图像，深度轴的维度是3，因为图像有三个颜色通道：红色、绿色和蓝色。对于像MNIST数字这样的黑白图片，深度是1（灰度级别）。卷积操作从其输入特征图中提取补丁，并将相同的变换应用于所有这些补丁，产生一个*输出特征图*。这个输出特征图仍然是一个三阶张量：它有宽度和高度。其深度可以是任意的，因为输出深度是层的参数，在该深度轴上的不同通道不再像RGB输入那样代表特定的颜色；相反，它们代表*过滤器*。过滤器编码输入数据的特定方面：在较高层次上，单个过滤器可以编码“输入中存在面部”的概念，例如。
- en: 'In the MNIST example, the first convolution layer takes a feature map of size
    `(28, 28, 1)` and outputs a feature map of size `(26, 26, 64)`: it computes 64
    filters over its input. Each of these 64 output channels contains a 26 × 26 grid
    of values, which is a *response map* of the filter over the input, indicating
    the response of that filter pattern at different locations in the input (see figure
    8.3). That is what the term *feature map* means: every dimension in the depth
    axis is a feature (or filter), and the rank-2 tensor `output[:, :, n]` is the
    2D spatial *map* of the response of this filter over the input.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在MNIST示例中，第一个卷积层接受大小为`(28, 28, 1)`的特征图，并输出大小为`(26, 26, 64)`的特征图：它在其输入上计算64个过滤器。这些64个输出通道中每个都包含一个26
    × 26的值网格，这是过滤器在输入上的*响应图*，指示该过滤器模式在输入的不同位置的反应（参见图8.3）。这就是术语*特征图*的含义：深度轴上的每个维度都是一个特征（或过滤器），而秩为2的张量`output[:,
    :, n]`是此过滤器在输入上的2D空间*图*。 '
- en: '![](../Images/d506560758d6cee6f0fc59966eb67536.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d506560758d6cee6f0fc59966eb67536.png)'
- en: '[Figure 8.3](#figure-8-3): The concept of a response map: a 2D map of the presence
    of a pattern at different locations in an input'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.3](#figure-8-3)：响应图的概念：一个2D图，表示在输入的不同位置上模式的呈现'
- en: 'Convolutions are defined by two key parameters:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积由两个关键参数定义：
- en: '*Size of the patches extracted from the inputs*  — These are typically 3 ×
    3 or 5 × 5\. In the example, they were 3 × 3, which is a common choice.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从输入中提取的补丁的大小* — 这些通常是3 × 3或5 × 5。在示例中，它们是3 × 3，这是一个常见的选择。'
- en: '*Depth of the output feature map*  — The number of filters computed by the
    convolution. The example started with a depth of 32 and ended with a depth of
    64.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出特征图的深度* — 通过卷积计算出的过滤器数量。示例从32个深度开始，以64个深度结束。'
- en: 'In Keras `Conv2D` layers, these parameters are the first arguments passed to
    the layer: `Conv2D(output_depth, (window_height, window_width))`.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras的`Conv2D`层中，这些参数是传递给层的第一个参数：`Conv2D(output_depth, (window_height, window_width))`。
- en: A convolution works by *sliding* these windows of size 3 × 3 or 5 × 5 over the
    3D input feature map, stopping at every possible location, and extracting the
    3D patch of surrounding features of shape `(window_height, window_width, input_depth)`.
    Each such 3D patch is then transformed into a 1D vector of shape `(output_depth,)`,
    which is done via a tensor product with a learned weight matrix, called the *convolution
    kernel* — the same kernel is reused across every patch. All of these vectors (one
    per patch) are then spatially reassembled into a 3D output map of shape `(height,
    width, output_depth)`. Every spatial location in the output feature map corresponds
    to the same location in the input feature map (for example, the lower-right corner
    of the output contains information about the lower-right corner of the input).
    For instance, with 3 × 3 windows, the vector `output[i, j, :]` comes from the
    3D patch `input[i-1:i+1, j-1:j+1, :]`. The full process is detailed in figure
    8.4.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积通过*滑动*大小为3 × 3或5 × 5的窗口在3D输入特征图上，在每个可能的位置停止，并提取形状为`(window_height, window_width,
    input_depth)`的周围特征3D补丁。然后，每个这样的3D补丁被转换成一个形状为`(output_depth,)`的1D向量，这是通过与一个称为*卷积核*的学习权重矩阵的张量积来完成的——相同的核在每一个补丁上被重复使用。然后，所有这些向量（每个补丁一个）在空间上重新组装成一个形状为`(height,
    width, output_depth)`的3D输出图。输出特征图中的每个空间位置对应于输入特征图中的相同位置（例如，输出图的右下角包含有关输入右下角的信息）。例如，使用3
    × 3窗口时，向量`output[i, j, :]`来自3D补丁`input[i-1:i+1, j-1:j+1, :]`。整个过程在图8.4中详细说明。
- en: '![](../Images/934c8e82aac0247164ceb48739a78d9d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/934c8e82aac0247164ceb48739a78d9d.png)'
- en: '[Figure 8.4](#figure-8-4): How convolution works'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.4](#figure-8-4)：卷积的工作原理'
- en: 'Note that the output width and height may differ from the input width and height.
    They may differ for two reasons:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出宽度和高度可能与输入宽度和高度不同。它们可能因为以下两个原因而不同：
- en: Border effects, which can be countered by padding the input feature map
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界效应，可以通过填充输入特征图来抵消
- en: The use of *strides*, which we’ll define in a second
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*步长*，我们将在下一节定义
- en: Let’s take a deeper look at these notions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨这些概念。
- en: Understanding border effects and padding
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解边界效应和填充
- en: 'Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around
    which you can center a 3 × 3 window, forming a 3 × 3 grid (see figure 8.5). Hence,
    the output feature map will be 3 × 3\. It shrinks a little: by exactly two tiles
    alongside each dimension, in this case. You can see this border effect in action
    in the earlier example: you start with 28 × 28 inputs, which become 26 × 26 after
    the first convolution layer.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个5 × 5特征图（总共25个瓦片）。你只能在9个瓦片周围放置一个3 × 3窗口，形成一个3 × 3网格（见图8.5）。因此，输出特征图将是3 ×
    3。它略微缩小：在每个维度上精确地缩小两个瓦片。你可以在前面的例子中看到这个边界效应的实际应用：你从28 × 28的输入开始，经过第一层卷积后变成26 ×
    26。
- en: '![](../Images/c33e84c0fad9d7d5fd595d8177a06c46.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/c33e84c0fad9d7d5fd595d8177a06c46.png)'
- en: '[Figure 8.5](#figure-8-5): Valid locations of 3 × 3 patches in a 5 × 5 input
    feature map'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.5](#figure-8-5)：在5 × 5输入特征图中3 × 3补丁的有效位置'
- en: If you want to get an output feature map with the same spatial dimensions as
    the input, you can use *padding*. Padding consists of adding an appropriate number
    of rows and columns on each side of the input feature map so as to make it possible
    to fit centered convolution windows around every input tile. For a 3 × 3 window,
    you add one column on the right, one column on the left, one row at the top, and
    one row at the bottom. For a 5 × 5 window, you add two rows (see figure 8.6).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想得到与输入具有相同空间维度的输出特征图，你可以使用*填充*。填充包括在输入特征图的每一边添加适当数量的行和列，以便在每个输入瓦片周围放置中心卷积窗口。对于3
    × 3窗口，你需要在右边添加一列，左边添加一列，顶部添加一行，底部添加一行。对于5 × 5窗口，你需要在顶部添加两行（见图8.6）。
- en: '![](../Images/c875426d9fe089ca3ef09266ff1bdd92.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/c875426d9fe089ca3ef09266ff1bdd92.png)'
- en: '[Figure 8.6](#figure-8-6): Padding a 5 × 5 input to be able to extract 25 3
    × 3 patches'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.6](#figure-8-6)：填充5 × 5输入以提取25个3 × 3补丁'
- en: 'In `Conv2D` layers, padding is configurable via the `padding` argument, which
    takes two values: `"valid"`, which means no padding (only valid window locations
    will be used); and `"same"`, which means “pad in such a way as to have an output
    with the same width and height as the input.” The `padding` argument defaults
    to `"valid"`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Conv2D`层中，填充可以通过`padding`参数进行配置，该参数接受两个值：`"valid"`，表示没有填充（只使用有效的窗口位置）；和`"same"`，表示“以这种方式填充，以便输出具有与输入相同的宽度和高度。”`padding`参数的默认值为`"valid"`。
- en: Understanding convolution strides
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解卷积步长
- en: 'The other factor that can influence output size is the notion of *strides*.
    The description of convolution so far has assumed that the center tiles of the
    convolution windows are all contiguous. But the distance between two successive
    windows is a parameter of the convolution, called its *stride*, which defaults
    to 1\. It’s possible to have *strided convolutions*: convolutions with a stride
    higher than 1\. In figure 8.7, you can see the patches extracted by a 3 × 3 convolution
    with stride 2 over a 5 × 5 input (without padding)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 影响输出大小的另一个因素是*步长*的概念。到目前为止的卷积描述假设卷积窗口的中心瓦片都是连续的。但是，两个连续窗口之间的距离是卷积的一个参数，称为其*步长*，默认值为1。可以有*步长卷积*：步长大于1的卷积。在图8.7中，你可以看到通过在5
    × 5输入上使用步长为2的3 × 3卷积提取的补丁。
- en: '![](../Images/fa3a3d514d7e28b55abc09e59d9e9dfd.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/fa3a3d514d7e28b55abc09e59d9e9dfd.png)'
- en: '[Figure 8.7](#figure-8-7): 3 × 3 convolution patches with 2 × 2 strides'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.7](#figure-8-7)：步长为2的3 × 3卷积补丁'
- en: Using stride 2 means the width and height of the feature map are downsampled
    by a factor of 2 (in addition to any changes induced by border effects). Strided
    convolutions are rarely used in classification models, but they come in handy
    for some types of models, as you will find out in the next chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步长2意味着特征图的宽度和高度以2的倍数下采样（除了任何由边界效应引起的改变）。步长卷积在分类模型中很少使用，但在某些类型的模型中很有用，你将在下一章中了解到。
- en: In classification models, instead of strides, we tend to use the *max-pooling*
    operation to downsample feature maps — which you saw in action in our first ConvNet
    example. Let’s look at it in more depth.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类模型中，我们倾向于使用*最大池化*操作来下采样特征图——你可以在我们的第一个卷积神经网络示例中看到它的实际应用。让我们更深入地了解一下。
- en: The max-pooling operation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大池化操作
- en: 'In the ConvNet example, you may have noticed that the size of the feature maps
    is halved after every `MaxPooling2D` layer. For instance, before the first `MaxPooling2D`
    layers, the feature map is 26 × 26, but the max-pooling operation halves it to
    13 × 13\. That’s the role of max pooling: to aggressively downsample feature maps,
    much like strided convolutions.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（ConvNet）的示例中，你可能已经注意到，在每一个`MaxPooling2D`层之后，特征图的大小都会减半。例如，在第一个`MaxPooling2D`层之前，特征图的大小是26
    × 26，但最大池化操作将其减半到13 × 13。这就是最大池化的作用：积极地对特征图进行下采样，就像步长卷积一样。
- en: Max pooling consists of extracting windows from the input feature maps and outputting
    the max value of each channel. It’s conceptually similar to convolution, except
    that instead of transforming local patches via a learned linear transformation
    (the convolution kernel), they’re transformed via a hardcoded `max` tensor operation.
    A big difference from convolution is that max pooling is usually done with 2 ×
    2 windows and stride 2 to downsample the feature maps by a factor of 2\. On the
    other hand, convolution is typically done with 3 × 3 windows and no stride (stride
    1).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化包括从输入特征图中提取窗口，并输出每个通道的最大值。从概念上讲，它与卷积相似，除了不是通过学习到的线性变换（卷积核）来转换局部块，而是通过硬编码的`max`张量操作来转换。与卷积的一个重大区别是，最大池化通常使用2
    × 2窗口和步长2来将特征图下采样2倍。另一方面，卷积通常使用3 × 3窗口而没有步长（步长1）。
- en: Why downsample feature maps this way? Why not remove the max-pooling layers
    and keep fairly large feature maps all the way up? Let’s look at this option.
    Our model would then look like this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要以这种方式下采样特征图？为什么不移除最大池化层，而保持相当大的特征图直到最后？让我们看看这个选项。那么，我们的模型将看起来像这样。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 8.5](#listing-8-5): An incorrectly structured ConvNet missing its
    max-pooling layers'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.5](#listing-8-5)：缺少最大池化层的错误结构化的ConvNet'
- en: 'Here’s a summary of the model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对模型的总结：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What’s wrong with this setup? Two things:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置有什么问题？两点：
- en: It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows
    in the third layer will only contain information coming from 7 × 7 windows in
    the initial input. The high-level patterns learned by the ConvNet will still be
    very small with regard to the initial input, which may not be enough to learn
    to classify digits (try recognizing a digit by only looking at it through windows
    that are 7 × 7 pixels!). We need the features from the last convolution layer
    to contain information about the totality of the input.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不利于学习特征的空间层次。第三层的3 × 3窗口将只包含来自初始输入中7 × 7窗口的信息。卷积神经网络学习的高级模式与初始输入相比仍然非常小，这可能不足以学习对数字进行分类（尝试通过只有7
    × 7像素的窗口来识别数字！）。我们需要最后卷积层的特征包含关于输入整体的信息。
- en: The final feature map has dimensions 22 × 22\. That’s huge — when you take the
    average of each 22 × 22 feature map, you are going to be destroying a lot of information
    compared to when your feature maps were only 3 × 3.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的特征图尺寸为22 × 22。这非常大——当你对每个22 × 22的特征图取平均值时，与你的特征图只有3 × 3时相比，你将丢失大量的信息。
- en: In short, the reason to use downsampling is to reduce the size of the feature
    maps, making the information they contain increasingly less spatially distributed
    and increasingly contained in the channels, while also inducing spatial-filter
    hierarchies by making successive convolution layers “look” at increasingly large
    windows (in terms of the fraction of the original input image they cover).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用下采样的原因是为了减小特征图的大小，使得它们包含的信息在空间上的分布越来越不均匀，并且越来越多地包含在通道中，同时通过使连续的卷积层“观察”到越来越大的窗口（从原始输入图像覆盖的分数来看）来诱导空间滤波层次。
- en: Note that max pooling isn’t the only way you can achieve such downsampling.
    As you already know, you can also use strides in the prior convolution layer.
    And you can use average pooling instead of max pooling, where each local input
    patch is transformed by taking the average value of each channel over the patch,
    rather than the max. But max pooling tends to work better than these alternative
    solutions. In a nutshell, the reason is that features tend to encode the spatial
    presence of some pattern or concept over the different tiles of the feature map
    (hence the term *feature map*), and it’s more informative to look at the *maximal
    presence* of different features than at their *average presence*. So the most
    reasonable subsampling strategy is to first produce dense maps of features (via
    unstrided convolutions) and then look at the maximal activation of the features
    over small patches, rather than looking at sparser windows of the inputs (via
    strided convolutions) or averaging input patches, which could cause you to miss
    or dilute feature-presence information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最大池化并不是实现这种下采样的唯一方法。正如你所知，你还可以在先前的卷积层中使用步长。你还可以使用平均池化而不是最大池化，其中每个局部输入块通过取块中每个通道的平均值来转换，而不是取最大值。但最大池化通常比这些替代方案更有效。简而言之，原因在于特征倾向于在特征图的不同块中编码某种模式或概念的时空存在（因此得名*特征图*），观察不同特征的*最大存在*比观察它们的*平均存在*更有信息量。因此，最合理的下采样策略是首先生成特征（通过无步长的卷积）的密集图，然后观察特征在小型块上的最大激活，而不是观察输入的稀疏窗口（通过步长卷积）或平均输入块，这可能会导致你错过或稀释特征存在信息。
- en: At this point, you should understand the basics of ConvNets — feature maps,
    convolution, and max pooling — and you know how to build a small ConvNet to solve
    a toy problem such as MNIST digits classification. Now let’s move on to more useful,
    practical applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经了解了卷积神经网络的基本知识——特征图、卷积和最大池化——并且你知道如何构建一个小型卷积神经网络来解决玩具问题，例如MNIST数字分类。现在让我们继续探讨更有用、更实际的应用。
- en: Training a ConvNet from scratch on a small dataset
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在小型数据集上从头开始训练卷积神经网络
- en: Having to train an image-classification model using very little data is a common
    situation, which you’ll likely encounter in practice if you ever do computer vision
    in a professional context. A “few” samples can mean anywhere from a few hundred
    to a few tens of thousands of images. As a practical example, we’ll focus on classifying
    images as dogs or cats. We’ll work with a dataset containing 5,000 pictures of
    cats and dogs (2,500 cats, 2,500 dogs), taken from the original Kaggle dataset.
    We’ll use 2,000 pictures for training, 1,000 for validation, and 2,000 for testing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 必须使用非常少的数据来训练图像分类模型是一种常见情况，如果你在专业环境中进行计算机视觉，你可能会在实践中遇到这种情况。所谓的“少量”样本可能意味着从几百到几万张图片。作为一个实际例子，我们将专注于将图像分类为狗或猫。我们将使用一个包含5,000张猫和狗图片的数据集（2,500只猫，2,500只狗），这些图片来自原始的Kaggle数据集。我们将使用2,000张图片进行训练，1,000张进行验证，2,000张进行测试。
- en: 'In this section, we’ll review one basic strategy to tackle this problem: training
    a new model from scratch using what little data we have. We’ll start by naively
    training a small ConvNet on the 2,000 training samples, without any regularization,
    to set a baseline for what can be achieved. This will get us to a classification
    accuracy of about 80%. At that point, the main issue will be overfitting. Then
    we’ll introduce *data augmentation*, a powerful technique for mitigating overfitting
    in computer vision. By using data augmentation, we’ll improve the model to reach
    a test accuracy of about 84%.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一种基本策略来解决这个问题：从头开始训练一个新模型，使用我们拥有的少量数据。我们将首先天真地在一个包含2,000个训练样本的小型卷积神经网络（ConvNet）上训练，没有任何正则化，以设定一个基准，看看能实现什么效果。这将使我们的分类准确率达到大约80%。在那个点上，主要问题将是过拟合。然后我们将介绍**数据增强**，这是一种在计算机视觉中减轻过拟合的强大技术。通过使用数据增强，我们将提高模型，使其测试准确率达到大约84%。
- en: 'In the next section, we’ll review two more essential techniques for applying
    deep learning to small datasets: *feature extraction with a pretrained model*
    and *fine-tuning a pretrained model* (which will get us to a final accuracy of
    98.5%). Together, these three strategies — training a small model from scratch,
    doing feature extraction using a pretrained model, and fine-tuning a pretrained
    model — will constitute your future toolbox for tackling the problem of performing
    image classification with small datasets.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾两个将深度学习应用于小数据集的必要技术：*使用预训练模型进行特征提取*和*微调预训练模型*（这将使我们的最终准确率达到98.5%）。这三个策略——从头开始训练小模型、使用预训练模型进行特征提取和微调预训练模型——将构成您未来解决使用小数据集进行图像分类问题的工具箱。
- en: The relevance of deep learning for small-data problems
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习对于小数据集问题的相关性
- en: What qualifies as “enough samples” to train a model is relative — relative to
    the size and depth of the model you’re trying to train, for starters. It isn’t
    possible to train a ConvNet to solve a complex problem with just a few tens of
    samples, but a few hundred can potentially suffice if the model is small and well
    regularized and the task is simple. Because ConvNets learn local, translation-invariant
    features, they’re highly data efficient on perceptual problems. Training a ConvNet
    from scratch on a very small image dataset will still yield reasonable results
    despite a relative lack of data, without the need for any custom feature engineering.
    You’ll see this in action in this section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: “足够的样本”来训练一个模型是相对的——首先，相对于您试图训练的模型的大小和深度。不可能只用几十个样本来训练一个卷积神经网络来解决复杂问题，但如果模型很小且很好地正则化，任务简单，那么几百个样本可能就足够了。由于卷积神经网络学习局部、平移不变的特征，它们在感知问题上的数据效率很高。在非常小的图像数据集上从头开始训练卷积神经网络，尽管数据相对较少，但仍然可以产生合理的结果，而无需任何定制的特征工程。您将在本节中看到这一点。
- en: 'What’s more, deep learning models are by nature highly repurposable: you can
    take, say, an image-classification or speech-to-text model trained on a large-scale
    dataset and reuse it on a significantly different problem with only minor changes.
    Specifically, in the case of computer vision, many pretrained classification models
    are publicly available for download and can be used to bootstrap powerful vision
    models out of very little data. This is one of the greatest strengths of deep
    learning: feature reuse. You’ll explore this in the next section.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型在本质上具有很强的可重用性：您可以从大型数据集上训练的图像分类或语音到文本模型中提取，并在一个显著不同的问题上仅进行少量修改后重用。具体来说，在计算机视觉领域，许多预训练的分类模型可供公开下载，并可用于从非常少的数据中启动强大的视觉模型。这是深度学习的一个最大优势：特征重用。您将在下一节中探索这一点。
- en: Let’s start by getting our hands on the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先着手获取数据。
- en: Downloading the data
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载数据
- en: The Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was
    made available by Kaggle as part of a computer-vision competition in late 2013,
    back when ConvNets weren’t mainstream. You can download the original dataset from
    `www.kaggle.com/c/dogs-vs-cats/data` (you’ll need to create a Kaggle account if
    you don’t already have one — don’t worry, the process is painless). You can also
    use the Kaggle API to download the dataset in Colab.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的Dogs vs. Cats数据集并不包含在Keras中。它是由Kaggle在2013年底作为计算机视觉竞赛的一部分提供的，那时卷积神经网络还不是主流。您可以从`www.kaggle.com/c/dogs-vs-cats/data`下载原始数据集（如果您还没有Kaggle账户，需要创建一个——别担心，这个过程很简单）。您还可以使用Kaggle
    API在Colab中下载数据集。
- en: The pictures in our dataset are medium-resolution color JPEGs. Figure 8.8 shows
    some examples.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的图片是中等分辨率的彩色JPEG。图8.8展示了几个示例。
- en: '![](../Images/f90c51c5a2e790835b3e0c4525ac4835.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/f90c51c5a2e790835b3e0c4525ac4835.png)'
- en: '[Figure 8.8](#figure-8-8): Samples from the Dogs vs. Cats dataset. Sizes weren’t
    modified: the samples come in different sizes, colors, backgrounds, etc.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.8](#figure-8-8)：Dogs vs. Cats数据集的样本。大小未修改：样本有不同的尺寸、颜色、背景等。'
- en: Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way
    back in 2013, was won by entrants who used ConvNets. The best entries achieved
    up to 95% accuracy. In this example, we will get fairly close to this accuracy
    (in the next section), even though we will train our models on less than 10% of
    the data that was available to the competitors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，原始的2013年狗与猫Kaggle竞赛，所有参赛者都使用了卷积神经网络。最佳参赛者实现了高达95%的准确率。在这个例子中，我们将接近这个准确率（在下一节中），尽管我们将模型训练在竞争对手可用的数据不到10%的情况下。
- en: 'This dataset contains 25,000 images of dogs and cats (12,500 from each class)
    and is 543 MB (compressed). After downloading and uncompressing the data, we’ll
    create a new dataset containing three subsets: a training set with 1,000 samples
    of each class, a validation set with 500 samples of each class, and a test set
    with 1,000 samples of each class. Why do this? Because many of the image datasets
    you’ll encounter in your career only contain a few thousand samples, not tens
    of thousands. Having more data available would make the problem easier — so it’s
    good practice to learn with a small dataset.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含25,000张狗和猫的图像（每个类别12,500张）和543 MB（压缩）。在下载并解压缩数据后，我们将创建一个新的数据集，包含三个子集：一个包含每个类别1,000个样本的训练集，一个包含每个类别500个样本的验证集，以及一个包含每个类别1,000个样本的测试集。为什么要这样做？因为你在职业生涯中遇到的大多数图像数据集只包含几千个样本，而不是成千上万。有更多的数据可用会使问题更容易——因此，使用小数据集进行学习是一个好的实践。
- en: 'The subsampled dataset we will work with will have the following directory
    structure:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要工作的子采样数据集将具有以下目录结构：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s make it happen in a coupl of calls to `shutil`, a Python library for running
    shell-like commands.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`shutil`库的几次调用中实现它，这是一个用于运行类似shell命令的Python库。
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 8.6](#listing-8-6): Copying images to training, validation, and test
    directories'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.6](#listing-8-6)：将图像复制到训练、验证和测试目录'
- en: 'We now have 2,000 training images, 1,000 validation images, and 2,000 test
    images. Each split contains the same number of samples from each class: this is
    a balanced binary classification problem, which means classification accuracy
    will be an appropriate measure of success.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有2,000个训练图像，1,000个验证图像和2,000个测试图像。每个分割包含每个类别的样本数量相同：这是一个平衡的二分类问题，这意味着分类准确率将是一个适当的成功衡量标准。
- en: Building your model
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建你的模型
- en: 'We will reuse the same general model structure you saw in the first example:
    the ConvNet will be a stack of alternated `Conv2D` (with `relu` activation) and
    `MaxPooling2D` layers.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用你在第一个示例中看到的相同的一般模型结构：卷积神经网络将是一系列交替的`Conv2D`（带有`relu`激活）和`MaxPooling2D`层。
- en: 'But because we’re dealing with bigger images and a more complex problem, we’ll
    make our model larger, accordingly: it will have two more `Conv2D` + `MaxPooling2D`
    stages. This serves both to augment the capacity of the model and to further reduce
    the size of the feature maps so they aren’t overly large when we reach the pooling
    layer. Here, because we start from inputs of size 180 × 180 pixels (a somewhat
    arbitrary choice), we end up with feature maps of size 7 × 7 just before the `GlobalAveragePooling2D`
    layer.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但因为我们处理的是更大的图像和更复杂的问题，我们将使我们的模型更大，相应地：它将有两个额外的`Conv2D` + `MaxPooling2D`阶段。这既增加了模型的容量，也进一步减小了特征图的大小，以便在达到池化层时它们不会过大。在这里，因为我们从180
    × 180像素的输入开始（这是一个有些任意的选择），我们最终在`GlobalAveragePooling2D`层之前得到7 × 7大小的特征图。
- en: Because we’re looking at a binary classification problem, we’ll end the model
    with a single unit (a `Dense` layer of size 1) and a `sigmoid` activation. This
    unit will encode the probability that the model is looking at one class or the
    other.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在查看一个二分类问题，我们将以一个单元（大小为1的`Dense`层）和一个`sigmoid`激活函数结束模型。这个单元将编码模型正在查看一个类别还是另一个类别的概率。
- en: 'One last small difference: we will start the model with a `Rescaling` layer,
    which will rescale image inputs (whose values are originally in the [0, 255] range)
    to the [0, 1] range.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个小差异：我们将从`Rescaling`层开始构建模型，该层将重缩放图像输入（其值最初在[0, 255]范围内）到[0, 1]范围。
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 8.7](#listing-8-7): Instantiating a small ConvNet for dogs vs. cats
    classification'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.7](#listing-8-7)：实例化一个小型卷积神经网络进行狗与猫分类'
- en: 'Let’s look at how the dimensions of the feature maps change with every successive
    layer:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看特征图维度是如何随着每一层连续变化的：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For the compilation step, you’ll go with the `adam` optimizer, as usual. Because
    you ended the model with a single sigmoid unit, you’ll use binary crossentropy
    as the loss (as a reminder, check out table 6.1 in chapter 6 for a cheat sheet
    on what loss function to use in various situations).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编译步骤，您将像往常一样使用 `adam` 优化器。因为您以单个 sigmoid 单元结束模型，所以您将使用二元交叉熵作为损失（作为提醒，请查看第
    6 章中的表 6.1，以获取各种情况下使用损失函数的速查表）。
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 8.8](#listing-8-8): Configuring the model for training'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.8](#listing-8-8)：配置模型以进行训练'
- en: Data preprocessing
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'As you know by now, data should be formatted into appropriately preprocessed
    floating-point tensors before being fed into the model. Currently, the data sits
    on a drive as JPEG files, so the steps for getting it into the model are roughly
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，在输入模型之前，数据应格式化为适当的预处理的浮点张量。目前，数据以 JPEG 文件的形式存储在驱动器上，因此将数据输入模型的大致步骤如下：
- en: Read the picture files.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取图片文件。
- en: Decode the JPEG content to RGB grids of pixels.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 JPEG 内容解码为像素的 RGB 网格。
- en: Convert these into floating-point tensors.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些转换为浮点张量。
- en: Resize them to a shared size (we’ll use 180 x 180).
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们调整到共享大小（我们将使用 180 x 180）。
- en: Pack them into batches (we’ll use batches of 32 images).
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们打包成批次（我们将使用 32 张图像的批次）。
- en: This may seem a bit daunting, but fortunately Keras has utilities to take care
    of these steps automatically. In particular, Keras features the utility function
    `image_dataset_from_directory`, which lets you quickly set up a data pipeline
    that can automatically turn image files on disk into batches of preprocessed tensors.
    This is what you’ll use here.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点令人畏惧，但幸运的是，Keras 提供了自动处理这些步骤的工具。特别是，Keras 特有的实用函数 `image_dataset_from_directory`
    允许您快速设置一个数据管道，该管道可以自动将磁盘上的图像文件转换为预处理的张量批次。这就是您在这里要使用的。
- en: Calling `image_dataset_from_directory(directory)` will first list the subdirectories
    of `directory` and assume each one contains images from one of your classes. It
    will then index the image files in each subdirectory. Finally, it will create
    and return a `tf.data.Dataset` object configured to read these files, shuffle
    them, decode them to tensors, resize them to a shared size, and pack them into
    batches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `image_dataset_from_directory(directory)` 将首先列出 `directory` 的子目录，并假设每个子目录包含您的一个类别的图像。然后，它将索引每个子目录中的图像文件。最后，它将创建并返回一个配置为读取这些文件的
    `tf.data.Dataset` 对象，对它们进行洗牌，将它们解码为张量，将它们调整到共享大小，并将它们打包成批次。
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 8.9](#listing-8-9): Using `image_dataset_from_directory` to read images
    from directories'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.9](#listing-8-9)：使用 `image_dataset_from_directory` 从目录中读取图像'
- en: Understanding TensorFlow Dataset objects
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 数据集对象
- en: TensorFlow makes available the `tf.data` API to create efficient input pipelines
    for machine learning models. Its core class is `tf.data.Dataset`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了 `tf.data` API 来创建机器学习模型的效率输入管道。其核心类是 `tf.data.Dataset`。
- en: The `Dataset` class can be used for data loading and preprocessing in any framework
    — not just TensorFlow. You can use it together with JAX or PyTorch. When you use
    it with a Keras model, it works the same, independently of the backend you’re
    currently using.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 类可用于任何框架中的数据加载和预处理，而不仅仅是 TensorFlow。您可以使用它与 JAX 或 PyTorch 一起使用。当您与
    Keras 模型一起使用时，它的工作方式相同，独立于您当前使用的后端。'
- en: 'A `Dataset` object is an iterator: you can use it in a `for` loop. It will
    typically return batches of input data and labels. You can pass a `Dataset` object
    directly to the `fit()` method of a Keras model.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 对象是一个迭代器：您可以在 `for` 循环中使用它。它通常会返回输入数据和标签的批次。您可以直接将 `Dataset` 对象传递给
    Keras 模型的 `fit()` 方法。'
- en: The `Dataset` class handles many key features that would otherwise be cumbersome
    to implement yourself, in particular parallelization of the preprocessing logic
    across multiple CPU cores, as well as asynchronous data prefetching (preprocessing
    the next batch of data while the previous one is being handled by the model, which
    keeps execution flowing without interruptions).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 类处理了许多关键特性，否则您可能难以自行实现，特别是跨多个 CPU 核心的预处理逻辑并行化，以及异步数据预取（在处理前一个批次的同时预处理下一个批次的数据，这可以保持执行流程不间断）。'
- en: 'The `Dataset` class also exposes a functional-style API for modifying datasets.
    Here’s a quick example: let’s create a `Dataset` instance from a NumPy array of
    random numbers. We’ll consider 1,000 samples, where each sample is a vector of
    size 16.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 类还公开了一个功能式 API，用于修改数据集。这里有一个快速示例：让我们从一个随机数的 NumPy 数组创建一个 `Dataset`
    实例。我们将考虑 1,000 个样本，其中每个样本是一个大小为 16 的向量。'
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 8.10](#listing-8-10): Instantiating a `Dataset` from a NumPy array'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.10](#listing-8-10)：从NumPy数组实例化`Dataset`'
- en: At first, our dataset just yields single samples.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们的数据集只产生单个样本。
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 8.11](#listing-8-11): Iterating on a dataset'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.11](#listing-8-11)：迭代数据集'
- en: You can use the `.batch()` method to batch the data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`.batch()`方法对数据进行分批。
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 8.12](#listing-8-12): Batching a dataset'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.12](#listing-8-12)：批处理数据集'
- en: 'More broadly, you have access to a range of useful dataset methods, such as
    these:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，您可以使用一系列有用的数据集方法，例如这些：
- en: '`.shuffle(buffer_size)` will shuffle elements within a buffer.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.shuffle(buffer_size)`将在缓冲区内部进行元素洗牌。'
- en: '`.prefetch(buffer_size)` will prefetch a buffer of elements in GPU memory to
    achieve better device utilization.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.prefetch(buffer_size)`将预取GPU内存中的元素缓冲区，以实现更好的设备利用率。'
- en: '`.map(callable)` will apply an arbitrary transformation to each element of
    the dataset (the function `callable`, expected to take as input a single element
    yielded by the dataset).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.map(callable)`将对数据集的每个元素应用任意转换（期望接受数据集产生的单个元素作为输入的函数`callable`）。'
- en: 'The method `.map(function, num_parallel_calls)` in particular is one that you
    will use often. Here’s an example: let’s use it to reshape the elements in our
    toy dataset from shape `(16,)` to shape `(4, 4)`.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`.map(function, num_parallel_calls)`方法尤其是一个您会经常使用的方法。这里有一个例子：让我们用它来将我们的玩具数据集中的元素从形状`(16,)`重塑为形状`(4,
    4)`。'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 8.13](#listing-8-13): Applying a transformation to `Dataset` elements
    using `map()`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.13](#listing-8-13)：使用`map()`对`Dataset`元素应用转换'
- en: You’re about to see more `map()` action over the next chapters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将看到更多的`map()`操作。
- en: Fitting the model
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Let’s look at the output of one of these `Dataset` objects: it yields batches
    of 180 × 180 RGB images (shape `(32, 180, 180, 3)`) and integer labels (shape
    `(32,)`). There are 32 samples in each batch (the batch size).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看这些`Dataset`对象之一的输出：它产生180 × 180 RGB图像的批次（形状`(32, 180, 180, 3)`）和整数标签（形状`(32,)`）。每个批次中有32个样本（批次大小）。
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 8.14](#listing-8-14): Displaying the shapes yielded by the `Dataset`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.14](#listing-8-14)：显示`Dataset`产生的形状'
- en: Let’s fit the model on our dataset. We use the `validation_data` argument in
    `fit()` to monitor validation metrics on a separate `Dataset` object.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的数据集上拟合模型。我们在`fit()`中使用`validation_data`参数来监控单独的`Dataset`对象上的验证指标。
- en: 'Note that we also use a `ModelCheckpoint` callback to save the model after
    each epoch. We configure it with the path where to save the file, as well as the
    arguments `save_best_only=True` and `monitor="val_loss"`: they tell the callback
    to only save a new file (overwriting any previous one) when the current value
    of the `val_loss` metric is lower than at any previous time during training. This
    guarantees that your saved file will always contain the state of the model corresponding
    to its best-performing training epoch, in terms of its performance on the validation
    data. As a result, we won’t have to retrain a new model for a lower number of
    epochs if we start overfitting: we can just reload our saved file.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还在每个epoch后使用一个`ModelCheckpoint`回调来保存模型。我们配置它以保存文件的路径，以及参数`save_best_only=True`和`monitor="val_loss"`：它们告诉回调只有在当前`val_loss`指标值低于训练过程中任何先前时间时才保存新文件（覆盖任何先前的文件）。这保证了您保存的文件将始终包含对应于其表现最佳训练epoch的模型状态，从其在验证数据上的性能来看。因此，如果我们开始过拟合，我们不需要重新训练一个具有较少epoch数的新模型：我们只需重新加载我们的保存文件即可。
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 8.15](#listing-8-15): Fitting the model using a `Dataset`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.15](#listing-8-15)：使用`Dataset`拟合模型'
- en: Let’s plot the loss and accuracy of the model over the training and validation
    data during training (see figure 8.9).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练过程中绘制模型在训练和验证数据上的损失和准确率（见图8.9）。
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Listing 8.16](#listing-8-16): Displaying curves of loss and accuracy during
    training'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.16](#listing-8-16)：显示训练期间的损失和准确率曲线'
- en: '![](../Images/e154cf5f9475f3c0d64a49453174de12.png) ![](../Images/19d507efdd59c5b2077f28641a60ec3a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e154cf5f9475f3c0d64a49453174de12.png) ![](../Images/19d507efdd59c5b2077f28641a60ec3a.png)'
- en: '[Figure 8.9](#figure-8-9): Training and validation metrics for a simple ConvNet'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.9](#figure-8-9)：简单卷积神经网络的训练和验证指标'
- en: These plots are characteristic of overfitting. The training accuracy increases
    linearly over time, until it reaches nearly 100%, whereas the validation accuracy
    peaks around 80%. The validation loss reaches its minimum after only 10 epochs
    and then stalls, whereas the training loss keeps decreasing linearly as training
    proceeds.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图是过拟合的特征。训练准确率随时间线性增加，直到接近 100%，而验证准确率在 80% 左右达到峰值。验证损失在第 10 个 epoch 后达到最低点，然后停滞，而训练损失随着训练的进行而线性下降。
- en: Let’s check the test accuracy. We’ll reload the model from its saved file to
    evaluate it as it was before it started overfitting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。我们将从其保存的文件中重新加载模型，以评估它在开始过拟合之前的状态。
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 8.17](#listing-8-17): Evaluating the model on the test set'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.17](#listing-8-17)：在测试集上评估模型'
- en: We get a test accuracy of 78.6% (due to the randomness of neural network initializations,
    you may get numbers within a few percentage points of that).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 78.6% 的测试准确率（由于神经网络初始化的随机性，你可能会得到相差几个百分点的数字）。
- en: 'Because you have relatively few training samples (2,000), overfitting will
    be your number-one concern. You already know about a number of techniques that
    can help mitigate overfitting, such as dropout and weight decay (L2 regularization).
    We’re now going to work with a new one, specific to computer vision and used almost
    universally when processing images with deep learning models: *data augmentation*.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的训练样本相对较少（2,000 个），过拟合将成为你的首要关注点。你已经知道一些可以帮助减轻过拟合的技术，例如 dropout 和权重衰减（L2
    正则化）。我们现在将使用一个新的技术，专门针对计算机视觉，并且在用深度学习模型处理图像时几乎被普遍使用：*数据增强*。
- en: Using data augmentation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用数据增强
- en: 'Overfitting is caused by having too few samples to learn from, rendering you
    unable to train a model that can generalize to new data. Given infinite data,
    your model would be exposed to every possible aspect of the data distribution
    at hand: you would never overfit. Data augmentation takes the approach of generating
    more training data from existing training samples, by *augmenting* the samples
    via a number of random transformations that yield believable-looking images. The
    goal is that at training time, your model will never see the exact same picture
    twice. This helps expose the model to more aspects of the data and generalize
    better.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是由于样本太少，无法学习，导致你无法训练一个可以泛化到新数据的模型。给定无限的数据，你的模型将接触到数据分布的每一个可能方面：你永远不会过拟合。数据增强通过通过一系列随机变换来增强样本，从而生成看起来可信的图像，从而从现有的训练样本中生成更多的训练数据。目标是，在训练时，你的模型永远不会看到完全相同的图片两次。这有助于使模型接触到数据的更多方面，并更好地泛化。
- en: 'In Keras, this can be done via *data augmentation layers*. Such layers could
    be added in one of two ways:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，这可以通过 *数据增强层* 来实现。这些层可以通过两种方式之一添加：
- en: '*At the start of the model* — *Inside* the model. In our case, the layers would
    come right before the `Rescaling` layer.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在模型的开始处* — *在模型内部*。在我们的例子中，层将直接位于 `Rescaling` 层之前。'
- en: '*Inside the data pipeline* — *Outside* the model. In our case, we’d apply them
    to our `Dataset` via a `map()` call.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在数据管道内部* — *在模型外部*。在我们的例子中，我们将通过 `map()` 调用将它们应用于我们的 `Dataset`。'
- en: The main difference between these two options is that data augmentation done
    inside the model would be running on the GPU, just like the rest of the model.
    Meanwhile, data augmentation done in the data pipeline would be running on the
    CPU, typically in a parallel way on multiple CPU cores. Sometimes, there can be
    performance benefits to doing the former, but the latter is usually the better
    option. So let’s go with that!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种选项之间的主要区别在于，在模型内部进行的数据增强将在 GPU 上运行，就像模型的其他部分一样。同时，在数据管道中进行的数据增强将在 CPU 上运行，通常在多个
    CPU 核心上并行运行。有时，进行前者可能会有性能上的好处，但后者通常是更好的选择。所以我们就这么做吧！
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 8.18](#listing-8-18): Defining a data augmentation stage'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.18](#listing-8-18)：定义数据增强阶段'
- en: 'These are just a few of the layers available (for more, see the Keras documentation).
    Let’s quickly go over this code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是可用的几层（更多内容请参阅 Keras 文档）。让我们快速浏览一下这段代码：
- en: '`RandomFlip("horizontal")` will apply horizontal flipping to a random 50% of
    the images that go through it.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomFlip("horizontal")` 将将水平翻转应用于通过它的随机 50% 的图像。'
- en: '`RandomRotation(0.1)` will rotate the input images by a random value in the
    range [–10%, +10%] (these are fractions of a full circle — in degrees the range
    would be [–36 degrees, +36 degrees]).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomRotation(0.1)`将随机旋转输入图像，旋转角度在[–10%，+10%]范围内（这些是完整圆周的分数——以度为单位，范围将是[–36度，+36度]）。'
- en: '`RandomZoom(0.2)` will zoom in or out of the image by a random factor in the
    range [–20%, +20%].'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomZoom(0.2)`将根据随机因子在[–20%，+20%]范围内放大或缩小图像。'
- en: Let’s look at the augmented images (see figure 8.10).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看增强后的图像（见图 8.10）。
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 8.19](#listing-8-19): Displaying some randomly augmented training
    images'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.19](#listing-8-19)：显示一些随机增强的训练图像'
- en: '![](../Images/c23ec10d204d38425efe734fd98e79ec.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c23ec10d204d38425efe734fd98e79ec.png)'
- en: '[Figure 8.10](#figure-8-10): Generating variations of a very good boy via random
    data augmentation'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.10](#figure-8-10)：通过随机数据增强生成一个非常好的男孩的变体'
- en: If you train a new model using this data augmentation configuration, the model
    will never see the same input twice. But the inputs it sees are still heavily
    intercorrelated, because they come from a small number of original images — you
    can’t produce new information; you can only remix existing information. As such,
    this may not be enough to completely get rid of overfitting. To further fight
    overfitting, you’ll also add a `Dropout` layer to your model, right before the
    densely connected classifier.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这个数据增强配置训练一个新的模型，模型将永远不会看到相同的输入两次。但是它看到的输入仍然高度相关，因为它们来自少量原始图像——你无法产生新信息；你只能重新混合现有信息。因此，这可能不足以完全消除过拟合。为了进一步对抗过拟合，你还将向你的模型中添加一个`Dropout`层，就在密集连接分类器之前。
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Listing 8.20](#listing-8-20): Defining a new ConvNet that includes dropout'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.20](#listing-8-20)：定义一个新的包含`dropout`的卷积神经网络'
- en: Let’s train the model using data augmentation and dropout. Because we expect
    overfitting to occur much later during training, we will train for twice as many
    epochs — 100\. Note that we evaluate on images that aren’t augmented — data augmentation
    is usually only performed at training time, as it is a regularization technique.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用数据增强和`dropout`来训练模型。因为我们预计过拟合将在训练后期发生，我们将训练两倍的周期数——100 个。请注意，我们将在未增强的图像上进行评估——数据增强通常仅在训练时执行，因为它是一种正则化技术。
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 8.21](#listing-8-21): Training the regularized ConvNet on augmented
    images'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.21](#listing-8-21)：在增强图像上训练正则化卷积神经网络'
- en: Let’s plot the results again; see figure 8.11\. Thanks to data augmentation
    and dropout, we start overfitting much later, around epochs 60–70 (compared to
    epoch 10 for the original model). The validation accuracy ends up peaking above
    85% — a big improvement over our first try.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果；见图 8.11。多亏了数据增强和`dropout`，我们开始过拟合的时间大大推迟，大约在 60-70 个周期（与原始模型的第 10
    个周期相比）。验证准确率最终达到 85% 以上——比我们第一次尝试有了很大的改进。
- en: '![](../Images/b2977f5c12049346adc8bb619d2010b7.png) ![](../Images/7377fd180ae66d4f52ab60a6e968fd80.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2977f5c12049346adc8bb619d2010b7.png) ![](../Images/7377fd180ae66d4f52ab60a6e968fd80.png)'
- en: '[Figure 8.11](#figure-8-11): Training and validation metrics with data augmentation'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.11](#figure-8-11)：数据增强后的训练和验证指标'
- en: Let’s check the test accuracy.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 8.22](#listing-8-22): Evaluating the model on the test set'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.22](#listing-8-22)：在测试集上评估模型'
- en: We get a test accuracy of 83.9%. It’s starting to look good! If you’re using
    Colab, make sure to download the saved file (`convnet_from_scratch_with_augmentation.keras`),
    as we will use it for some experiments in the next chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 83.9% 的测试准确率。它开始看起来不错了！如果你使用 Colab，请确保下载保存的文件（`convnet_from_scratch_with_augmentation.keras`），因为我们将用它进行下一章的一些实验。
- en: By further tuning the model’s configuration (such as the number of filters per
    convolution layer or the number of layers in the model), you may be able to get
    an even better accuracy, likely up to 90%. But it would prove difficult to go
    any higher just by training your own ConvNet from scratch because you have so
    little data to work with. As a next step to improve your accuracy on this problem,
    you’ll have to use a pretrained model, which is the focus of the next two sections.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进一步调整模型的配置（例如每个卷积层的滤波器数量或模型中的层数），你可能能够获得更高的准确率，可能高达 90%。但是，仅通过从头开始训练自己的卷积神经网络来提高这个问题的准确率将非常困难，因为你可用的数据非常少。为了提高这个问题的准确率，下一步你将不得不使用预训练模型，这是下一两个章节的重点。
- en: Using a pretrained model
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练模型
- en: A common and highly effective approach to deep learning on small image datasets
    is to use a pretrained model. A *pretrained model* is a model that was previously
    trained on a large dataset, typically on a large-scale image-classification task.
    If this original dataset is large enough and general enough, then the spatial
    hierarchy of features learned by the pretrained model can effectively act as a
    generic model of the visual world, and hence its features can prove useful for
    many different computer vision problems, even though these new problems may involve
    completely different classes than those of the original task. For instance, you
    might train a model on ImageNet (where classes are mostly animals and everyday
    objects) and then repurpose this trained model for something as remote as identifying
    furniture items in images. Such portability of learned features across different
    problems is a key advantage of deep learning compared to many older, shallow learning
    approaches, and it makes deep learning very effective for small-data problems.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在小型图像数据集上进行深度学习的一种常见且高度有效的方法是使用预训练模型。*预训练模型*是在大型数据集上预先训练的模型，通常是在大规模图像分类任务上。如果这个原始数据集足够大且足够通用，那么预训练模型学习到的特征的空间层次可以有效地作为视觉世界的通用模型，因此其特征可以证明对许多不同的计算机视觉问题是有用的，即使这些新问题可能涉及与原始任务完全不同的类别。例如，你可以在ImageNet（其中类别主要是动物和日常物品）上训练一个模型，然后将其重新用于识别图像中的家具项目等遥远的应用。这种在不同问题之间学习特征的便携性是深度学习相对于许多较老、较浅学习方法的显著优势，并且使得深度学习在小型数据问题上非常有效。
- en: In this case, let’s consider a large ConvNet trained on the ImageNet dataset
    (1.4 million labeled images and 1,000 different classes). ImageNet contains many
    animal classes, including different species of cats and dogs, and you can thus
    expect it to perform well on the dogs-versus-cats classification problem.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，让我们考虑一个在ImageNet数据集（1.4百万个标记图像和1,000个不同类别）上训练的大型卷积神经网络（ConvNet）。ImageNet包含许多动物类别，包括不同种类的猫和狗，因此你可以期待它在狗与猫的分类问题上表现良好。
- en: We’ll use the Xception architecture. This may be your first encounter with one
    of these cutesy model names — Xception, ResNet, EfficientNet, and so on; you’ll
    get used to them if you keep doing deep learning for computer vision because they
    will come up frequently. You’ll learn about the architectural details of Xception
    in the next chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Xception架构。这可能是你第一次遇到这些可爱的模型名称之一——Xception、ResNet、EfficientNet等等；如果你继续进行计算机视觉的深度学习，你会习惯它们的，因为它们会经常出现。你将在下一章中了解Xception的架构细节。
- en: 'There are two ways to use a pretrained model: *feature extraction* and *fine-tuning*.
    We’ll cover both of them. Let’s start with feature extraction.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型有两种方式：*特征提取*和*微调*。我们将介绍这两种方法。让我们从特征提取开始。
- en: Feature extraction with a pretrained model
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练模型进行特征提取
- en: Feature extraction consists of using the representations learned by a previously
    trained model to extract interesting features from new samples. These features
    are then run through a new classifier, which is trained from scratch.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取包括使用先前训练的模型学习到的表示来从新的样本中提取有趣的特征。然后，这些特征将通过一个新的分类器进行处理，该分类器是从零开始训练的。
- en: 'As you saw previously, ConvNets used for image classification comprise two
    parts: they start with a series of pooling and convolution layers, and they end
    with a densely connected classifier. The first part is called the *convolutional
    base* or *backbone* of the model. In the case of ConvNets, feature extraction
    consists of taking the convolutional base of a previously trained network, running
    the new data through it, and training a new classifier on top of the output (see
    figure 8.12).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前看到的，用于图像分类的卷积神经网络由两部分组成：它们从一系列池化和卷积层开始，并以一个密集连接的分类器结束。第一部分被称为模型的*卷积基*或*骨干*。在卷积神经网络的情况下，特征提取包括从先前训练的网络中提取卷积基，将新数据通过它运行，并在输出之上训练一个新的分类器（见图8.12）。
- en: '![](../Images/6ae9c33c2b3199361221a86298064605.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6ae9c33c2b3199361221a86298064605.png)'
- en: '[Figure 8.12](#figure-8-12): Swapping classifiers while keeping the same convolutional
    base'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.12](#figure-8-12)：在保持相同的卷积基的同时交换分类器'
- en: 'Why only reuse the convolutional base? Could you reuse the densely connected
    classifier as well? In general, doing so should be avoided. The reason is that
    the representations learned by the convolutional base are likely to be more generic
    and therefore more reusable: the feature maps of a ConvNet are presence maps of
    generic concepts over a picture, which is likely to be useful regardless of the
    computer vision problem at hand. But the representations learned by the classifier
    will necessarily be specific to the set of classes on which the model was trained
    — they will only contain information about the presence probability of this or
    that class in the entire picture. Additionally, representations found in densely
    connected layers no longer contain any information about where objects are located
    in the input image: these layers get rid of the notion of space, whereas the object
    location is still described by convolutional feature maps. For problems where
    object location matters, densely connected features are largely useless.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只重用卷积基？难道密集连接分类器也不能重用吗？通常，这样做应该避免。原因是卷积基学习到的表示可能更通用，因此更可重用：卷积神经网络的特征图是在图片上对通用概念的呈现图，这可能在处理任何计算机视觉问题时都很有用。但分类器学习到的表示必然是特定于模型训练的类别集的——它们只包含关于整个图片中某个或某个类别的存在概率的信息。此外，密集连接层中找到的表示不再包含任何关于输入图像中对象位置的信息：这些层消除了空间的概念，而对象位置仍然由卷积特征图描述。对于对象位置重要的问题，密集连接特征在很大程度上是无用的。
- en: Note that the level of generality (and therefore reusability) of the representations
    extracted by specific convolution layers depends on the depth of the layer in
    the model. Layers that come earlier in the model extract local, highly generic
    feature maps (such as visual edges, colors, and textures), whereas layers that
    are higher up extract more abstract concepts (such as “cat ear” or “dog eye”).
    So if your new dataset differs a lot from the dataset on which the original model
    was trained, you may be better off using only the first few layers of the model
    to do feature extraction, rather than using the entire convolutional base.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过特定卷积层提取的表示的通用性（以及因此的可重用性）取决于该层在模型中的深度。模型中较早的层提取的是局部、高度通用的特征图（例如视觉边缘、颜色和纹理），而较上面的层则提取更抽象的概念（例如“猫耳”或“狗眼”）。因此，如果你的新数据集与原始模型训练的数据集差异很大，你最好只使用模型的前几层来进行特征提取，而不是使用整个卷积基。
- en: In this case, because the ImageNet class set contains multiple dog and cat classes,
    it’s likely to be beneficial to reuse the information contained in the densely
    connected layers of the original model. But we’ll choose not to, so we can cover
    the more general case where the class set of the new problem doesn’t overlap the
    class set of the original model. Let’s put this in practice by using the convolutional
    base of our pretrained model to extract interesting features from cat and dog
    images and then train a dogs-versus-cats classifier on top of these features.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于ImageNet类别集包含多个狗和猫类别，重新使用原始模型密集连接层中的信息可能是有益的。但我们将选择不这样做，以便涵盖新问题的类别集与原始模型的类别集不重叠的更一般情况。让我们通过使用预训练模型的卷积基从猫和狗图像中提取有趣的特征，然后在这些特征之上训练一个猫狗分类器来将这一点付诸实践。
- en: We will use the *KerasHub* library to create all pretrained models used in this
    book. KerasHub contains Keras implementations of popular pretrained model architectures
    paired with pretrained weights that can be downloaded to your machine. It contains
    a number of ConvNets like Xception, ResNet, EfficientNet and MobileNet, as well
    as larger, generative models we will use in the later chapters of this book. Let’s
    try using it to instantiate the Xception model trained on the ImageNet dataset.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 *KerasHub* 库来创建本书中使用的所有预训练模型。KerasHub 包含了流行预训练模型架构的 Keras 实现，并配以可以下载到您机器上的预训练权重。它包含许多卷积神经网络，如
    Xception、ResNet、EfficientNet 和 MobileNet，以及我们将在本书后续章节中使用的更大型的生成模型。让我们尝试使用它来实例化在
    ImageNet 数据集上训练的 Xception 模型。
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 8.23](#listing-8-23): Instantiating the Xception convolutional base'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.23](#listing-8-23)：实例化 Xception 卷积基'
- en: You’ll note a couple of things. First, KerasHub uses the term *backbone* to
    refer to the underlying feature extractor network without the classification head
    (it’s a little easier to type than “convolutional base”). It also uses a special
    constructor called `from_preset()` that will download the configuration and weights
    for the Xception model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到几个问题。首先，KerasHub 使用术语 *backbone* 来指代没有分类头的底层特征提取网络（比“卷积基础”容易输入一些）。它还使用一个特殊的构造函数
    `from_preset()`，该函数将下载 Xception 模型的配置和权重。
- en: What’s that “41” in the name of the model we are using? Pretrained ConvNets
    are by convention often named by how “deep” they are. In this case, the 41 means
    that our Xception model has 41 trainable layers (conv and dense layers) stacked
    on top of each other. It’s the “deepest” model we’ve used so far in the book by
    a good margin.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的模型名称中的“41”是什么意思？按照惯例，预训练的卷积神经网络通常根据它们的“深度”来命名。在这种情况下，41 表示我们的 Xception
    模型有 41 个可训练层（卷积和密集层）堆叠在一起。这是我们迄今为止在书中使用的“最深层”模型，差距很大。
- en: There’s one more missing piece we need before we can use this model. Every pretrained
    ConvNet will do some rescaling and resizing of images before pretraining. It’s
    important to make sure our input images *match*; otherwise, our model will need
    to relearn how to extract features from images with a totally different input
    range. Rather than keep track of which pretrained models use a `[0, 1]` input
    range for pixel values and which use a `[-1, 1]` range, we can use a KerasHub
    layer called `ImageConverter` that will rescale our images to match our pretrained
    checkpoint. It has the same special `from_preset()` constructor as the backbone
    class.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用这个模型之前，还有一个缺失的部分需要补充。每个预训练的卷积神经网络（ConvNet）在预训练之前都会对图像进行一些缩放和调整大小。确保我们的输入图像*匹配*是很重要的；否则，我们的模型将需要重新学习如何从具有完全不同输入范围的图像中提取特征。与其跟踪哪些预训练模型使用
    `[0, 1]` 的像素值输入范围，哪些使用 `[-1, 1]` 的范围，我们不如使用一个名为 `ImageConverter` 的 KerasHub 层，它将把我们的图像缩放到与我们的预训练检查点匹配。它具有与骨干类相同的特殊
    `from_preset()` 构造函数。
- en: '[PRE26]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 8.24](#listing-8-24): Instantiating the preprocessing paired with
    the Xception model'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.24](#listing-8-24)：实例化与 Xception 模型配合使用的预处理'
- en: 'At this point, there are two ways you could proceed:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你可以选择两种方法继续：
- en: Running the convolutional base over your dataset, recording its output to a
    NumPy array on disk, and then using this data as input to a standalone, densely
    connected classifier similar to those you saw in chapters 4 and 5. This solution
    is fast and cheap to run, because it only requires running the convolutional base
    once for every input image, and the convolutional base is by far the most expensive
    part of the pipeline. But for the same reason, this technique won’t allow you
    to use data augmentation.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的数据集上运行卷积基础，将其输出记录到磁盘上的 NumPy 数组中，然后使用这些数据作为独立、密集连接的分类器的输入，类似于你在第 4 章和第 5
    章中看到的。这个解决方案运行速度快且成本低，因为它只需要为每个输入图像运行一次卷积基础，而卷积基础是整个流程中最昂贵的部分。但出于同样的原因，这种技术不允许你使用数据增强。
- en: Extending the model you have (`conv_base`) by adding `Dense` layers on top and
    running the whole thing end to end on the input data. This will allow you to use
    data augmentation because every input image goes through the convolutional base
    every time it’s seen by the model. But for the same reason, this technique is
    far more expensive than the first.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在输入数据上添加 `Dense` 层来扩展你现有的模型（`conv_base`），并从头到尾运行整个模型。这将允许你使用数据增强，因为每次模型看到输入图像时，每个输入图像都会通过卷积基础。但出于同样的原因，这种技术比第一种技术昂贵得多。
- en: 'We’ll cover both techniques. Let’s walk through the code required to set up
    the first one: recording the output of `conv_base` on your data and using these
    outputs as inputs to a new model.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍这两种技术。让我们通过设置第一个所需的代码来逐步进行：记录 `conv_base` 在你的数据上的输出，并使用这些输出作为新模型的输入。
- en: Fast feature extraction without data augmentation
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无数据增强的快速特征提取
- en: We’ll start by extracting features as NumPy arrays, by calling the `predict()`
    method of the `conv_base` model on our training, validation, and testing datasets.
    Let’s iterate over our datasets to extract the pretrained model’s features.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过调用 `conv_base` 模型的 `predict()` 方法来提取特征，作为我们的训练、验证和测试数据集。让我们遍历我们的数据集以提取预训练模型的特征。
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 8.25](#listing-8-25): Extracting the image features and corresponding
    labels'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.25](#listing-8-25)：提取图像特征和相应的标签'
- en: Importantly, `predict()` only expects images, not labels, but our current dataset
    yields batches that contain both images and their labels.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`predict()` 只期望图像，而不是标签，但我们的当前数据集生成的批次包含图像及其标签。
- en: 'The extracted features are currently of shape `(samples, 6, 6, 2048)`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '提取的特征当前形状为 `(samples, 6, 6, 2048)`:'
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: At this point, you can define your densely connected classifier (note the use
    of dropout for regularization) and train it on the data and labels that you just
    recorded.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可以定义你的密集连接分类器（注意正则化时使用 dropout），并在你刚刚记录的数据和标签上对其进行训练。
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Listing 8.26](#listing-8-26): Defining and training the densely connected
    classifier'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.26](#listing-8-26)：定义和训练密集连接分类器'
- en: Training is very fast because you only have to deal with two `Dense` layers
    — an epoch takes less than 1 second even on CPU.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常快，因为你只需要处理两个 `Dense` 层——即使是在 CPU 上，一个 epoch 也只需不到 1 秒。
- en: Let’s look at the loss and accuracy curves during training (see figure 8.13).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练期间的损失和准确率曲线（见图 8.13）。
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 8.27](#listing-8-27): Plotting the results'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.27](#listing-8-27)：绘制结果'
- en: '![](../Images/605acfe8452fb67a1808d09146951f69.png) ![](../Images/df8ababe79bc755a8d10cdfc5323b506.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/605acfe8452fb67a1808d09146951f69.png) ![](../Images/df8ababe79bc755a8d10cdfc5323b506.png)'
- en: '[Figure 8.13](#figure-8-13): Training and validation metrics for plain feature
    extraction'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.13](#figure-8-13)：平面特征提取的训练和验证指标'
- en: You reach a validation accuracy of slightly over 98% — much better than you
    achieved in the previous section with the small model trained from scratch. This
    is a bit of an unfair comparison, however, because ImageNet contains many dog
    and cat instances, which means that our pretrained model already has the exact
    knowledge required for the task at hand. This won’t always be the case when you
    use pretrained features.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你达到了略高于 98% 的验证准确率——比你在上一节中使用从头开始训练的小模型所达到的准确率要好得多。然而，这种比较有点不公平，因为 ImageNet
    包含许多狗和猫的实例，这意味着我们的预训练模型已经具备了完成任务所需的精确知识。当你使用预训练特征时，这种情况并不总是如此。
- en: However, the plots also indicate that you’re overfitting almost from the start
    — despite using dropout with a fairly large rate. That’s because this technique
    doesn’t use data augmentation, which is essential for preventing overfitting with
    small image datasets.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些图表也表明你几乎从一开始就过度拟合——尽管使用了相当大的 dropout 率。这是因为这项技术没有使用数据增强，这对于防止使用小型图像数据集时的过度拟合是至关重要的。
- en: 'Let’s check the test accuracy:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率：
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We get test accuracy of 98.1% — a very nice improvement over training a model
    from scratch!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 98.1% 的测试准确率——与从头开始训练模型相比，这是一个非常好的改进！
- en: Feature extraction together with data augmentation
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征提取与数据增强相结合
- en: 'Now, let’s review the second technique we mentioned for doing feature extraction,
    which is much slower and more expensive but allows you to use data augmentation
    during training: creating a model that chains the `conv_base` with a new dense
    classifier and training it end to end on the inputs.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾我们提到的第二个用于特征提取的技术，它速度较慢且成本更高，但允许你在训练期间使用数据增强：创建一个将 `conv_base` 与新的密集分类器链式连接的模型，并在输入上从头到尾进行训练。
- en: To do this, we will first freeze the convolutional base. *Freezing* a layer
    or set of layers means preventing their weights from being updated during training.
    Here, if you don’t do this, then the representations that were previously learned
    by the convolutional base will be modified during training. Because the `Dense`
    layers on top are randomly initialized, very large weight updates would be propagated
    through the network, effectively destroying the representations previously learned.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先冻结卷积基。*冻结*一个层或一组层意味着防止它们在训练期间更新权重。在这里，如果你不这样做，那么卷积基之前学习到的表示将在训练期间被修改。因为顶部的
    `Dense` 层是随机初始化的，非常大的权重更新将通过网络传播，实际上破坏了之前学习到的表示。
- en: In Keras, you freeze a layer or model by setting its `trainable` attribute to
    `False`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，你可以通过将层的 `trainable` 属性设置为 `False` 来冻结一个层或模型。
- en: '[PRE32]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[Listing 8.28](#listing-8-28): Creating the frozen convolutional base'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.28](#listing-8-28)：创建冻结的卷积基'
- en: Setting `trainable` to `False` empties the list of trainable weights of the
    layer or model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `trainable` 设置为 `False` 将清空层或模型的可训练权重列表。
- en: '[PRE33]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Listing 8.29](#listing-8-29): Printing the list of trainable weights before
    and after freezing'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 8.29](#listing-8-29)：冻结前后的可训练权重列表'
- en: 'Now, we can just create a new model that chains together our frozen convolutional
    base and a dense classifier, like this:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需创建一个新的模型，将冻结的卷积基础和密集分类器链接在一起，如下所示：
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With this setup, only the weights from the two `Dense` layers that you added
    will be trained. That’s a total of four weight tensors: two per layer (the main
    weight matrix and the bias vector). Note that for these changes to take effect,
    you must first compile the model. If you ever modify weight trainability after
    compilation, you should then recompile the model, or these changes will be ignored.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种设置，只有你添加的两个`Dense`层的权重将被训练。总共是四个权重张量：每个层两个（主权重矩阵和偏置向量）。请注意，为了使这些更改生效，你必须首先编译模型。如果你在编译后修改权重的可训练性，那么你应该重新编译模型，否则这些更改将被忽略。
- en: 'Let’s train our model. We’ll reuse our augmented dataset `augmented_train_dataset`.
    Thanks to data augmentation, it will take much longer for the model to start overfitting,
    so we can train for more epochs — let’s do 30:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练我们的模型。我们将重用我们的增强数据集`augmented_train_dataset`。多亏了数据增强，模型开始过度拟合需要更长的时间，因此我们可以训练更多的轮次——让我们做30轮：
- en: '[PRE35]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let’s plot the results again (see figure 8.14). This model reaches a validation
    accuracy of 98.2%.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果（见图8.14）。这个模型达到了98.2%的验证准确率。
- en: '![](../Images/8e3a8a5c36838e7499ef2d4ccb9849c0.png) ![](../Images/ae553ac8e7101ab0653eeff0e1edba20.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14](../Images/8e3a8a5c36838e7499ef2d4ccb9849c0.png) ![图8.15](../Images/ae553ac8e7101ab0653eeff0e1edba20.png)'
- en: '[Figure 8.14](#figure-8-14): Training and validation metrics for feature extraction
    with data augmentation'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.14](#figure-8-14)：使用数据增强进行特征提取的训练和验证指标'
- en: Let’s check the test accuracy.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。
- en: '[PRE36]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[Listing 8.30](#listing-8-30): Evaluating the model on the test set'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.30](#listing-8-30)：在测试集上评估模型'
- en: We get a test accuracy of 98.4%. This is not an improvement over the previous
    model, which is a bit disappointing. This could be a sign that our data augmentation
    configuration does not exactly match the distribution of the test data. Let’s
    see if we can do better with our latest attempt.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了98.4%的测试准确率。这并没有比之前的模型有改进，这有点令人失望。这可能表明我们的数据增强配置并不完全符合测试数据的分布。让我们看看我们能否在我们的最新尝试中做得更好。
- en: Fine-tuning a pretrained model
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调预训练模型
- en: Another widely used technique for model reuse, complementary to feature extraction,
    is *fine-tuning* (see figure 8.15). Fine-tuning consists of unfreezing the frozen
    model base used for feature extraction and jointly training both the newly added
    part of the model (in this case, the fully connected classifier) and the base
    model. This is called *fine-tuning* because it slightly adjusts the more abstract
    representations of the model being reused to make them more relevant for the problem
    at hand.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛使用的模型重用技术，与特征提取互补的是*微调*（见图8.15）。微调包括解冻用于特征提取的冻结模型基础，并联合训练模型的新添加部分（在这种情况下，全连接分类器）和基础模型。这被称为*微调*，因为它略微调整了被重用的模型的更抽象的表示，使其更相关于当前的问题。
- en: 'We stated earlier that it’s necessary to freeze the pretrained convolution
    base first to be able to train a randomly initialized classifier on top. For the
    same reason, it’s only possible to fine-tune the convolutional base once the classifier
    on top has already been trained. If the classifier isn’t already trained, then
    the error signal propagating through the network during training will be too large,
    and the representations previously learned by the layers being fine-tuned will
    be destroyed. Thus, the steps for fine-tuning a network are as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，为了能够在顶部训练一个随机初始化的分类器，首先需要冻结预训练的卷积基础。出于同样的原因，只有在顶部的分类器已经训练好的情况下，才能微调卷积基础。如果分类器尚未训练，那么在训练过程中通过网络传播的错误信号将太大，并且之前由正在微调的层学到的表示将被破坏。因此，微调网络的步骤如下：
- en: Add your custom network on top of an already trained base network.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在已经训练好的基础网络上添加你的自定义网络。
- en: Freeze the base network.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻基础网络。
- en: Train the part you added.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练你添加的部分。
- en: Unfreeze the base network.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻基础网络。
- en: Jointly train both these layers and the part you added.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 联合训练这两个层以及你添加的部分。
- en: Note that you should not unfreeze “batch normalization” layers (`BatchNormalization`).
    Batch normalization and its effect on fine-tuning is explained in the next chapter.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不应该解冻“批量归一化”层（`BatchNormalization`）。批量归一化及其对微调的影响将在下一章中解释。
- en: 'You already completed the first three steps when doing feature extraction.
    Let’s proceed with step 4: you’ll unfreeze your `conv_base`.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行特征提取时，你已经完成了前三个步骤。让我们继续进行第4步：你将解冻你的`conv_base`。
- en: Let’s start fine-tuning the model using a very low learning rate. The reason
    for using a low learning rate is that you want to limit the magnitude of the modifications
    you make to the representations of the layers you’re fine-tuning. Updates that
    are too large may harm these representations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个非常低的学习率开始微调模型。使用低学习率的原因是，你希望限制你对正在微调的层的表示所做的修改的幅度。过大的更新可能会损害这些表示。
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[Listing 8.31](#listing-8-31): Fine-tuning the model'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.31](#listing-8-31)：微调模型'
- en: 'You can now finally evaluate this model on the test data (see figure 8.15):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以最终在测试数据上评估这个模型了（见图8.15）：
- en: '[PRE38]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](../Images/a4eed2da3b1ba70dcee533725f83fcc8.png) ![](../Images/6549d0428cc9b3aa20c7b92abef1c8e1.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](../Images/a4eed2da3b1ba70dcee533725f83fcc8.png) ![图片2](../Images/6549d0428cc9b3aa20c7b92abef1c8e1.png)'
- en: '[Figure 8.15](#figure-8-15): Training and validation metrics for fine-tuning'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15](#figure-8-15)：微调的训练和验证指标'
- en: Here, you get a test accuracy of 98.6% (again, your own results may be within
    half a percentage point). In the original Kaggle competition around this dataset,
    this would have been one of the top results. It’s not quite a fair comparison,
    however, since you used pretrained features that already contained prior knowledge
    about cats and dogs, which competitors couldn’t use at the time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你得到了98.6%的测试准确率（再次强调，你的结果可能在半个百分点之内）。在围绕这个数据集的原始Kaggle竞赛中，这将是一份顶尖的结果。然而，这种比较并不完全公平，因为你使用了已经包含有关猫和狗先前知识的预训练特征，而当时的竞争对手无法使用这些特征。
- en: On the positive side, by using modern deep learning techniques, you managed
    to reach this result using only a small fraction of the training data that was
    available for the competition (about 10%). There is a huge difference between
    being able to train on 20,000 samples compared to 2,000 samples!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在积极的一面，通过使用现代深度学习技术，你只用到了竞赛中可用训练数据的一小部分（大约10%）就达到了这个结果。与训练2,000个样本相比，能够在20,000个样本上进行训练有着巨大的差异！
- en: Now you have a solid set of tools for dealing with image-classification problems
    — in particular, with small datasets.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了一套处理图像分类问题（特别是小型数据集）的强大工具。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: ConvNets excel at computer vision tasks. It’s possible to train one from scratch,
    even on a very small dataset, with decent results.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络在计算机视觉任务中表现出色。即使是在非常小的数据集上，从头开始训练一个卷积神经网络也是可能的，并且可以得到相当不错的结果。
- en: ConvNets work by learning a hierarchy of modular patterns and concepts to represent
    the visual world.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络通过学习一系列模块化的模式和概念来表示视觉世界，从而工作。
- en: On a small dataset, overfitting will be the main issue. Data augmentation is
    a powerful way to fight overfitting when you’re working with image data.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在小型数据集上，过拟合将是主要问题。数据增强是当你处理图像数据时对抗过拟合的一种强大方式。
- en: It’s easy to reuse an existing ConvNet on a new dataset via feature extraction.
    This is a valuable technique for working with small image datasets.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征提取，在新的数据集上重用现有的卷积神经网络（ConvNet）非常容易。这对于处理小型图像数据集来说是一种非常有价值的技巧。
- en: As a complement to feature extraction, you can use fine-tuning, which adapts
    to a new problem some of the representations previously learned by an existing
    model. This pushes performance a bit further.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为特征提取的补充，你可以使用微调，它适应于新问题，并调整了现有模型之前学习的一些表示。这进一步提升了性能。
