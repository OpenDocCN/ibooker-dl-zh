- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'Since their introduction in 2017, transformers have become the de facto standard
    for tackling a wide range of natural language processing (NLP) tasks in both academia
    and industry. Without noticing it, you probably interacted with a transformer
    today: Google now uses BERT to enhance its search engine by better understanding
    users’ search queries. Similarly, the GPT family of models from OpenAI have repeatedly
    made headlines in mainstream media for their ability to generate human-like text
    and images.^([1](preface01.xhtml#idm46238735099696)) These transformers now power
    applications like [GitHub’s Copilot](https://copilot.github.com), which, as shown
    in [Figure P-1](#copilot), can convert a comment into source code that automatically
    creates a neural network for you!'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年推出以来，变压器已成为应对各种自然语言处理（NLP）任务的事实标准，无论是在学术界还是工业界。您可能没有注意到，但今天您可能已经与变压器互动了：谷歌现在使用BERT来增强其搜索引擎，以更好地理解用户的搜索查询。同样，OpenAI的GPT系列模型因其生成类似人类的文本和图像的能力而多次成为主流媒体的头条新闻。^([1](preface01.xhtml#idm46238735099696))
    这些变压器现在驱动着应用程序，比如[GitHub的Copilot](https://copilot.github.com)，正如[图P-1](#copilot)所示，它可以将评论转换为自动生成的源代码，为您自动创建神经网络！
- en: So what is it about transformers that changed the field almost overnight? Like
    many great scientific breakthroughs, it was the synthesis of several ideas, like
    *attention*, *transfer learning*, and *scaling up neural networks*, that were
    percolating in the research community at the time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让变压器几乎一夜之间改变了这个领域？就像许多伟大的科学突破一样，它是几个想法的综合，比如*注意力*、*迁移学习*和*扩展神经网络*，它们当时正在研究界中酝酿。
- en: But however useful it is, to gain traction in industry any fancy new method
    needs tools to make it accessible. The ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    library](https://oreil.ly/Z79jF) and its surrounding ecosystem answered that call
    by making it easy for practitioners to use, train, and share models. This greatly
    accelerated the adoption of transformers, and the library is now used by over
    five thousand organizations. Throughout this book we’ll guide you on how to train
    and optimize these models for practical applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，无论它有多有用，要在行业中获得关注，任何新的花哨方法都需要工具来使其易于访问。![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers库](https://oreil.ly/Z79jF)及其周边生态系统通过使从业者能够轻松使用、训练和共享模型来回应了这一需求。这大大加速了变压器的采用，该库现在被超过五千个组织使用。在本书中，我们将指导您如何为实际应用程序训练和优化这些模型。
- en: '![copilot](Images/nlpt_p001.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![copilot](Images/nlpt_p001.png)'
- en: Figure P-1\. An example from GitHub Copilot where, given a brief description
    of the task, the application provides a suggestion for the entire class (everything
    following `class` is autogenerated)
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图P-1\. GitHub Copilot的一个示例，根据任务的简要描述，该应用程序提供了整个类的建议（`class`后面的所有内容都是自动生成的）
- en: Who Is This Book For?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书适合谁？
- en: This book is written for data scientists and machine learning engineers who
    may have heard about the recent breakthroughs involving transformers, but are
    lacking an in-depth guide to help them adapt these models to their own use cases.
    The book is not meant to be an introduction to machine learning, and we assume
    you are comfortable programming in Python and has a basic understanding of deep
    learning frameworks like [PyTorch](https://pytorch.org) and [TensorFlow](https://www.tensorflow.org).
    We also assume you have some practical experience with training models on GPUs.
    Although the book focuses on the PyTorch API of ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, [Chapter 2](ch02.xhtml#chapter_classification) shows you how to
    translate all the examples to TensorFlow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为数据科学家和机器学习工程师写的，他们可能听说过涉及变压器的最新突破，但缺乏深入指南来帮助他们将这些模型适应到自己的用例中。这本书不是为机器学习入门而写的，我们假设您能够熟练使用Python进行编程，并且对像[PyTorch](https://pytorch.org)和[TensorFlow](https://www.tensorflow.org)这样的深度学习框架有基本的了解。我们还假设您有一些在GPU上训练模型的实际经验。尽管这本书侧重于![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers的PyTorch API，[第2章](ch02.xhtml#chapter_classification)向您展示了如何将所有示例转换为TensorFlow。
- en: 'The following resources provide a good foundation for the topics covered in
    this book. We assume your technical knowledge is roughly at their level:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下资源为本书涵盖的主题提供了良好的基础。我们假设您的技术知识大致在它们的水平上：
- en: '*Hands-On Machine Learning with Scikit-Learn and TensorFlow*, by Aurélien Géron
    (O’Reilly)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Scikit-Learn和TensorFlow进行实践性机器学习*，作者Aurélien Géron（O''Reilly）'
- en: '*Deep Learning for Coders with fastai and PyTorch*, by Jeremy Howard and Sylvain
    Gugger (O’Reilly)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用fastai和PyTorch进行深度学习*，作者Jeremy Howard和Sylvain Gugger（O''Reilly）'
- en: '*Natural Language Processing with PyTorch*, by Delip Rao and Brian McMahan
    (O’Reilly)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用PyTorch进行自然语言处理*，作者Delip Rao和Brian McMahan（O''Reilly）'
- en: '[*The Hugging Face Course*](https://oreil.ly/n3MaR), by the open source team
    at Hugging Face'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Hugging Face课程*](https://oreil.ly/n3MaR)，由Hugging Face的开源团队'
- en: What You Will Learn
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么
- en: The goal of this book is to enable you to build your own language applications.
    To that end, it focuses on practical use cases, and delves into theory only where
    necessary. The style of the book is hands-on, and we highly recommend you experiment
    by running the code examples yourself.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是让您能够构建自己的语言应用程序。为此，它侧重于实际用例，并只在必要时深入理论。本书的风格是实践性的，我们强烈建议您通过运行代码示例来进行实验。
- en: 'The book covers all the major applications of transformers in NLP by having
    each chapter (with a few exceptions) dedicated to one task, combined with a realistic
    use case and dataset. Each chapter also introduces some additional concepts. Here’s
    a high-level overview of the tasks and topics we’ll cover:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖了NLP中变压器的所有主要应用，每一章（有少数例外）都专门致力于一个任务，结合一个现实的用例和数据集。每一章还介绍了一些额外的概念。以下是我们将要涵盖的任务和主题的高级概述：
- en: '[Chapter 1](ch01.xhtml#chapter_introduction), *Hello Transformers*, introduces
    transformers and puts them into context. It also provides an introduction to the
    Hugging Face ecosystem.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](ch01.xhtml#chapter_introduction)，*你好，变压器*，介绍了变压器并将其置于上下文中。它还介绍了Hugging
    Face生态系统。'
- en: '[Chapter 2](ch02.xhtml#chapter_classification), *Text Classification*, focuses
    on the task of sentiment analysis (a common text classification problem) and introduces
    the `Trainer` API.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2章，“文本分类”，侧重于情感分析的任务（一种常见的文本分类问题），并介绍了“Trainer”API。
- en: '[Chapter 3](ch03.xhtml#chapter_anatomy), *Transformer Anatomy*, dives into
    the Transformer architecture in more depth, to prepare you for the chapters that
    follow.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3章，“变压器解剖”，更深入地探讨了变压器架构，为接下来的章节做准备。
- en: '[Chapter 4](ch04.xhtml#chapter_ner), *Multilingual Named Entity Recognition*,
    focuses on the task of identifying entities in texts in multiple languages (a
    token classification problem).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4章，“多语言命名实体识别”，专注于在多种语言文本中识别实体的任务（一个标记分类问题）。
- en: '[Chapter 5](ch05.xhtml#chapter_generation), *Text Generation*, explores the
    ability of transformer models to generate text, and introduces decoding strategies
    and metrics.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5章，“文本生成”，探讨了变压器模型生成文本的能力，并介绍了解码策略和度量标准。
- en: '[Chapter 6](ch06.xhtml#chapter_summarization), *Summarization*, digs into the
    complex sequence-to-sequence task of text summarization and explores the metrics
    used for this task.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6章，“摘要”，深入探讨了文本摘要的复杂序列到序列任务，并探索了用于此任务的度量标准。
- en: '[Chapter 7](ch07.xhtml#chapter_qa), *Question Answering*, focuses on building
    a review-based question answering system and introduces retrieval with Haystack.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7章，“问答”，侧重于构建基于评论的问答系统，并介绍了Haystack的检索功能。
- en: '[Chapter 8](ch08.xhtml#chapter_compression), *Making Transformers Efficient
    in Production*, focuses on model performance. We’ll look at the task of intent
    detection (a type of sequence classification problem) and explore techniques such
    a knowledge distillation, quantization, and pruning.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第8章，“使变压器在生产中更高效”，侧重于模型性能。我们将研究意图检测的任务（一种序列分类问题），并探索知识蒸馏、量化和修剪等技术。
- en: '[Chapter 9](ch09.xhtml#chapter_fewlabels), *Dealing with Few to No Labels*,
    looks at ways to improve model performance in the absence of large amounts of
    labeled data. We’ll build a GitHub issues tagger and explore techniques such as
    zero-shot classification and data augmentation.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第9章，“处理少量或没有标签的数据”，探讨了在缺乏大量标记数据的情况下改善模型性能的方法。我们将构建一个GitHub问题标记器，并探索零样本分类和数据增强等技术。
- en: '[Chapter 10](ch10.xhtml#chapter_fromscratch), *Training Transformers from Scratch*,
    shows you how to build and train a model for autocompleting Python source code
    from scratch. We’ll look at dataset streaming and large-scale training, and build
    our own tokenizer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第10章，“从头开始训练变压器”，向您展示了如何从头开始构建和训练一个用于自动完成Python源代码的模型。我们将研究数据集流和大规模训练，并构建我们自己的分词器。
- en: '[Chapter 11](ch11.xhtml#chapter_future), *Future Directions*, explores the
    challenges transformers face and some of the exciting new directions that research
    in this area is going into.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第11章，“未来方向”，探讨了变压器面临的挑战，以及这一领域的研究正在进行的一些令人兴奋的新方向。
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Transformers offers several layers of
    abstraction for using and training transformer models. We’ll start with the easy-to-use
    pipelines that allow us to pass text examples through the models and investigate
    the predictions in just a few lines of code. Then we’ll move on to tokenizers,
    model classes, and the `Trainer` API, which allow us to train models for our own
    use cases. Later, we’ll show you how to replace the `Trainer` with the ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate library, which gives us full control over the training loop and allows
    us to train large-scale transformers entirely from scratch! Although each chapter
    is mostly self-contained, the difficulty of the tasks increases in the later chapters.
    For this reason, we recommend starting with Chapters [1](ch01.xhtml#chapter_introduction)
    and [2](ch02.xhtml#chapter_classification), before branching off into the topic
    of most interest.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![nlpt_pin01](Images/nlpt_pin01.png)变压器为使用和训练变压器模型提供了几层抽象。我们将从易于使用的管道开始，通过几行代码就可以将文本示例传递给模型，并调查预测结果。然后我们将转向分词器、模型类和“Trainer”API，这些允许我们为自己的用例训练模型。稍后，我们将向您展示如何用![nlpt_pin01](Images/nlpt_pin01.png)加速库替换“Trainer”，这使我们完全控制训练循环，并允许我们完全从头开始训练大规模变压器！尽管每一章基本上是独立的，但后面章节的任务难度会增加。因此，我们建议先从第1章和第2章开始，然后再进入最感兴趣的主题。'
- en: Besides ![nlpt_pin01](Images/nlpt_pin01.png) Transformers and ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate, we will also make extensive use of ![nlpt_pin01](Images/nlpt_pin01.png)⁠
    ⁠Datasets, which seamlessly integrates with other libraries. ![nlpt_pin01](Images/nlpt_pin01.png)⁠
    ⁠Datasets offers similar functionality for data processing as Pandas but is designed
    from the ground up for tackling large datasets and machine learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了![nlpt_pin01](Images/nlpt_pin01.png)变压器和![nlpt_pin01](Images/nlpt_pin01.png)加速器，我们还将广泛使用![nlpt_pin01](Images/nlpt_pin01.png)⁠
    ⁠数据集，它与其他库无缝集成。![nlpt_pin01](Images/nlpt_pin01.png)⁠ ⁠数据集提供了与Pandas类似的数据处理功能，但是从头开始设计，用于处理大型数据集和机器学习。
- en: With these tools, you have everything you need to tackle almost any NLP challenge!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些工具，您几乎可以解决任何自然语言处理挑战！
- en: Software and Hardware Requirements
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件和硬件要求
- en: 'Due to the hands-on approach of this book, we highly recommend that you run
    the code examples while you read each chapter. Since we’re dealing with transformers,
    you’ll need access to a computer with an NVIDIA GPU to train these models. Fortunately,
    there are several free online options that you can use, including:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的实践性方法，我们强烈建议您在阅读每一章时运行代码示例。由于我们涉及变压器，您需要访问一台配备NVIDIA GPU的计算机来训练这些模型。幸运的是，有几种免费的在线选项可供您使用，包括：
- en: '[Google Colaboratory](https://oreil.ly/jyXgA)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google Colaboratory](https://oreil.ly/jyXgA)'
- en: '[Kaggle Notebooks](https://oreil.ly/RnMP3)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle笔记本](https://oreil.ly/RnMP3)'
- en: '[Paperspace Gradient Notebooks](https://oreil.ly/mZEKy)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Paperspace Gradient笔记本](https://oreil.ly/mZEKy)'
- en: To run the examples, you’ll need to follow the installation guide that we provide
    in the book’s GitHub repository. You can find this guide and the code examples
    at [*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例，您需要按照我们在书的GitHub存储库中提供的安装指南。您可以在[*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks)找到这个指南和代码示例。
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We developed most of the chapters using NVIDIA Tesla P100 GPUs, which have 16GB
    of memory. Some of the free platforms provide GPUs with less memory, so you may
    need to reduce the batch size when training the models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大部分章节都是使用NVIDIA Tesla P100 GPU开发的，它们有16GB的内存。一些免费平台提供的GPU内存较少，因此在训练模型时可能需要减少批处理大小。
- en: Conventions Used in This Book
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`常量宽度`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及在段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**`常量宽度粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应该按照字面意思输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*`常量宽度斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应由用户提供的值或由上下文确定的值替换的文本。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元素表示提示或建议。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元素表示一般说明。
- en: Warning
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元素表示警告或注意。
- en: Using Code Examples
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks)下载。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有技术问题或在使用代码示例时遇到问题，请发送电子邮件至[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助您完成工作。一般来说，如果本书提供了示例代码，您可以在您的程序和文档中使用它。除非您复制了代码的大部分内容，否则您无需联系我们以获得许可。例如，编写一个使用本书中几个代码块的程序不需要许可。出售或分发O’Reilly图书中的示例需要许可。引用本书并引用示例代码来回答问题不需要许可。将本书中大量示例代码合并到产品文档中需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Natural Language
    Processing with Transformers* by Lewis Tunstall, Leandro von Werra, and Thomas
    Wolf (O’Reilly). Copyright 2022 Lewis Tunstall, Leandro von Werra, and Thomas
    Wolf, 978-1-098-10324-8.”'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感激，但通常不需要归属。归属通常包括标题、作者、出版商和ISBN。例如：“*使用Transformers进行自然语言处理*，作者Lewis Tunstall、Leandro
    von Werra和Thomas Wolf（O’Reilly）。版权2022年Lewis Tunstall、Leandro von Werra和Thomas
    Wolf，978-1-098-10324-8。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得您对代码示例的使用超出了合理使用范围或上述许可，请随时通过[*permissions@oreilly.com*](mailto:permissions@oreilly.com)与我们联系。
- en: O’Reilly Online Learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](http://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 40多年来，[*O’Reilly Media*](http://oreilly.com)提供技术和商业培训、知识和见解，帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入学习路径、交互式编码环境以及来自O’Reilly和其他200多家出版商的大量文本和视频。有关更多信息，请访问[*http://oreilly.com*](http://oreilly.com)。
- en: How to Contact Us
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题发送至出版商：
- en: O’Reilly Media, Inc.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/nlp-with-transformers*](https://oreil.ly/nlp-with-transformers).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书创建了一个网页，列出勘误、示例和任何其他信息。您可以访问[*https://oreil.ly/nlp-with-transformers*](https://oreil.ly/nlp-with-transformers)。
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 发送电子邮件[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)评论或提出有关本书的技术问题。
- en: For news and information about our books and courses, visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的图书和课程的新闻和信息，请访问[*http://oreilly.com*](http://oreilly.com)。
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我们的Twitter：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*http://youtube.com/oreillymedia*](http://youtube.com/oreillymedia)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上观看我们：[*http://youtube.com/oreillymedia*](http://youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Writing a book about one of the fastest-moving fields in machine learning would
    not have been possible without the help of many people. We thank the wonderful
    O’Reilly team, and especially Melissa Potter, Rebecca Novack, and Katherine Tozer
    for their support and advice. The book has also benefited from amazing reviewers
    who spent countless hours to provide us with invaluable feedback. We are especially
    grateful to Luca Perozzi, Hamel Husain, Shabie Iqbal, Umberto Lupo, Malte Pietsch,
    Timo Möller, and Aurélien Géron for their detailed reviews. We thank Branden Chan
    at [deepset](https://www.deepset.ai) for his help with extending the Haystack
    library to support the use case in [Chapter 7](ch07.xhtml#chapter_qa). The beautiful
    illustrations in this book are due to the amazing [Christa Lanz](https://christalanz.ch)—thank
    you for making this book extra special. We were also fortunate enough to have
    the support of the whole Hugging Face team. Many thanks to Quentin Lhoest for
    answering countless questions on ![nlpt_pin01](Images/nlpt_pin01.png) Datasets,
    to Lysandre Debut for help on everything related to the Hugging Face Hub, Sylvain
    Gugger for his help with ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate, and
    Joe Davison for his inspiration for [Chapter 9](ch09.xhtml#chapter_fewlabels)
    with regard to zero-shot learning. We also thank Sidd Karamcheti and the whole
    [Mistral team](https://oreil.ly/aOYLt) for adding stability tweaks for GPT-2 to
    make [Chapter 10](ch10.xhtml#chapter_fromscratch) possible. This book was written
    entirely in Jupyter Notebooks, and we thank Jeremy Howard and Sylvain Gugger for
    creating delightful tools like [fastdoc](https://oreil.ly/yVCfT) that made this
    possible.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要写一本关于机器学习中发展最快的领域之一的书，没有许多人的帮助是不可能的。我们感谢美妙的O'Reilly团队，特别是Melissa Potter，Rebecca
    Novack和Katherine Tozer，感谢他们的支持和建议。这本书还受益于花费无数时间为我们提供宝贵反馈的令人惊叹的审阅者。我们特别感谢Luca Perozzi，Hamel
    Husain，Shabie Iqbal，Umberto Lupo，Malte Pietsch，Timo Möller和Aurélien Géron的详细审阅。我们感谢[deepset](https://www.deepset.ai)的Branden
    Chan在扩展Haystack库以支持[第7章](ch07.xhtml#chapter_qa)中的用例方面的帮助。本书中美丽的插图要归功于了不起的[Christa
    Lanz](https://christalanz.ch)——感谢你让这本书变得格外特别。我们还很幸运地得到了整个Hugging Face团队的支持。非常感谢Quentin
    Lhoest在![nlpt_pin01](Images/nlpt_pin01.png)数据集方面回答了无数问题，Lysandre Debut在与Hugging
    Face Hub相关的一切方面的帮助，Sylvain Gugger在![nlpt_pin01](Images/nlpt_pin01.png)加速方面的帮助，以及Joe
    Davison在[第9章](ch09.xhtml#chapter_fewlabels)中对零样本学习的启发。我们还要感谢Sidd Karamcheti和整个[Mistral团队](https://oreil.ly/aOYLt)为GPT-2添加稳定性调整，使[第10章](ch10.xhtml#chapter_fromscratch)成为可能。这本书完全是用Jupyter笔记本写的，我们要感谢Jeremy
    Howard和Sylvain Gugger创建了像[fastdoc](https://oreil.ly/yVCfT)这样令人愉快的工具，使这成为可能。
- en: Lewis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lewis
- en: To Sofia, thank you for being a constant source of support and encouragement—without
    both, this book would not exist. After a long stretch of writing, we can finally
    enjoy our weekends again!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对Sofia，感谢你始终给予的支持和鼓励——没有这两者，这本书就不会存在。经过长时间的写作，我们终于可以再次享受周末了！
- en: Leandro
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leandro
- en: Thank you Janine, for your patience and encouraging support during this long
    year with many late nights and busy weekends.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Janine，在这漫长的一年里，有许多深夜和忙碌的周末，你的耐心和鼓励支持。
- en: Thomas
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Thomas
- en: I would like to thank first and foremost Lewis and Leandro for coming up with
    the idea of this book and pushing strongly to produce it in such a beautiful and
    accessible format. I would also like to thank all the Hugging Face team for believing
    in the mission of AI as a community effort, and the whole NLP/AI community for
    building and using the libraries and research we describe in this book together
    with us.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先要感谢Lewis和Leandro提出了这本书的想法，并强烈推动以如此美丽和易于访问的格式出版。我还要感谢所有Hugging Face团队，他们相信AI是一个社区努力的使命，以及整个NLP/AI社区，他们与我们一起构建和使用本书中描述的库和研究。
- en: More than what we build, the journey we take is what really matters, and we
    have the privilege to travel this path with thousands of community members and
    readers like you today. Thank you all from the bottom of our hearts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所建立的不仅仅是重要的，我们所走过的旅程才是真正重要的，我们有幸能够与成千上万的社区成员和像你们今天一样的读者一起走这条路。衷心感谢你们所有人。
- en: ^([1](preface01.xhtml#idm46238735099696-marker)) NLP researchers tend to name
    their creations after characters in *Sesame Street*. We’ll explain what all these
    acronyms mean in [Chapter 1](ch01.xhtml#chapter_introduction).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](preface01.xhtml#idm46238735099696-marker)) NLP研究人员倾向于以*Sesame Street*中的角色命名他们的创作。我们将在[第1章](ch01.xhtml#chapter_introduction)中解释所有这些首字母缩略词的含义。
