- en: Classification and regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类和回归
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter04_classification-and-regression](https://deeplearningwithpython.io/chapters/chapter04_classification-and-regression)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter04_classification-and-regression](https://deeplearningwithpython.io/chapters/chapter04_classification-and-regression)
- en: 'This chapter is designed to get you started with using neural networks to solve
    real problems. You’ll consolidate the knowledge you gained from chapters 2 and
    3, and you’ll apply what you’ve learned to three new tasks covering the three
    most common use cases of neural networks — binary classification, categorical
    classification, and scalar regression:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这章旨在帮助你开始使用神经网络解决实际问题。你将巩固从第 2 章和第 3 章中获得的知识，并将所学应用到三个新的任务中，涵盖神经网络最常见的三个用例——二元分类、分类分类和标量回归：
- en: Classifying movie reviews as positive or negative (binary classification)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电影评论分类为正面或负面（二元分类）
- en: Classifying news wires by topic (categorical classification)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据主题对新闻通讯进行分类（分类分类）
- en: Estimating the price of a house, given real estate data (scalar regression)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据房地产数据估算房屋价格（标量回归）
- en: 'These examples will be your first contact with end-to-end machine learning
    workflows: you’ll get introduced to data preprocessing, basic model architecture
    principles, and model evaluation.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例将是你第一次接触端到端机器学习工作流程：你将了解数据预处理、基本模型架构原则以及模型评估。
- en: By the end of this chapter, you’ll be able to use neural networks to handle
    simple classification and regression tasks over vector data. You’ll then be ready
    to start building a more principled, theory-driven understanding of machine learning
    in chapter 5.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用神经网络处理简单的基于向量数据的分类和回归任务。然后你将准备好开始在第 5 章中构建一个更原则性、理论驱动的机器学习理解。
- en: 'Classifying movie reviews: A binary classification example'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对电影评论进行分类：一个二分类示例
- en: Two-class classification, or binary classification, is one of the most common
    kinds of machine learning problem. In this example, you’ll learn to classify movie
    reviews as positive or negative, based on the text content of the reviews.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类，或称为二元分类，是机器学习中最常见的类型之一。在这个例子中，你将学习如何根据评论的文本内容将电影评论分类为正面或负面。
- en: The IMDb dataset
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IMDb 数据集
- en: 'You’ll work with the IMDb dataset: a set of 50,000 highly polarized reviews
    from the Internet Movie Database. They’re split into 25,000 reviews for training
    and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive
    reviews.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 IMDb 数据集：来自互联网电影数据库的 50,000 条高度两极化的评论。它们被分为 25,000 条用于训练和 25,000 条用于测试，每个集合包含
    50% 的负面评论和 50% 的正面评论。
- en: 'Just like the MNIST dataset, the IMDb dataset comes packaged with Keras. It
    has already been preprocessed: the reviews (sequences of words) have been turned
    into sequences of integers, where each integer stands for a specific word in a
    dictionary. This enables us to focus on model building, training, and evaluation.
    In chapter 14, you’ll learn how to process raw text input from scratch.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 MNIST 数据集一样，IMDb 数据集已经打包在 Keras 中。它已经被预处理过：评论（单词序列）已经被转换成整数序列，其中每个整数代表字典中的一个特定单词。这使得我们能够专注于模型构建、训练和评估。在第
    14 章中，你将学习如何从头开始处理原始文本输入。
- en: The following code will load the dataset (when you run it the first time, about
    80 MB of data will be downloaded to your machine).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将加载数据集（当你第一次运行时，大约 80 MB 的数据将被下载到你的机器上）。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 4.1](#listing-4-1): Loading the IMDb dataset'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.1](#listing-4-1)：加载 IMDb 数据集'
- en: The argument `num_words=10000` means you’ll only keep the top 10,000 most frequently
    occurring words in the training data. Rare words will be discarded. This allows
    you to work with vector data of manageable size. If we didn’t set this limit,
    we’d be working with 88,585 unique words in the training data, which is unnecessarily
    large. Many of these words only occur in a single sample, and thus can’t be meaningfully
    used for classification.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `num_words=10000` 的意思是，你将只保留训练数据中频率最高的前 10,000 个单词。不常见的单词将被丢弃。这允许你处理可管理的向量数据。如果我们没有设置这个限制，我们将在训练数据中处理
    88,585 个独特的单词，这是不必要的大的。其中许多单词只出现在单个样本中，因此不能有意义地用于分类。
- en: 'The variables `train_data` and `test_data` are NumPy arrays of reviews; each
    review is a list of word indices (encoding a sequence of words). `train_labels`
    and `test_labels` are NumPy arrays of 0s and 1s, where 0 stands for *negative*
    and 1 stands for *positive*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`train_data`和`test_data`是评论的NumPy数组；每个评论是一系列单词索引（编码单词序列）。`train_labels`和`test_labels`是0s和1s的NumPy数组，其中0代表*负面*，1代表*正面*：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Because you’re restricting yourself to the top 10,000 most frequent words,
    no word index will exceed 10,000:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你将自己限制在频率最高的前10,000个单词上，没有单词索引会超过10,000：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For kicks, let’s quickly decode one of these reviews back to English words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，让我们快速解码其中一个评论回英文单词。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 4.2](#listing-4-2): Decoding reviews back to text'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.2](#listing-4-2)：将评论解码回文本'
- en: 'Let’s take a look at what we got:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the leading `?` corresponds to a start token that has been prefixed
    to each review.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的`?`对应于被添加到每个评论前的起始标记。
- en: Preparing the data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'You can’t directly feed lists of integers into a neural network. They have
    all different lengths, while a neural network expects to process contiguous batches
    of data. You have to turn your lists into tensors. There are two ways to do that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能直接将整数列表输入到神经网络中。它们的长度各不相同，而神经网络期望处理连续的数据批次。你必须将你的列表转换为张量。有两种方法可以做到这一点：
- en: Pad your lists so that they all have the same length, then turn them into an
    integer tensor of shape `(samples, max_length)`, and start your model with a layer
    capable of handling such integer tensors (the `Embedding` layer, which we’ll cover
    in detail later in the book).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的列表填充到相同的长度，然后将其转换为形状为`(samples, max_length)`的整数张量，然后以一个能够处理此类整数张量的层（`Embedding`层，我们将在本书的后面详细讨论）开始你的模型。
- en: '*Multi-hot encode* your lists to turn them into vectors of 0s and 1s reflecting
    the presence or absence of all possible words. This would mean, for instance,
    turning the sequence `[8, 5]` into a 10,000-dimensional vector that would be all
    0s except for indices 5 and 8, which would be 1s.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多热编码*你的列表，将它们转换为反映所有可能单词存在或不存在情况的0s和1s的向量。这意味着，例如，将序列 `[8, 5]` 转换为一个10,000维度的向量，除了索引5和8之外的所有位置都是0s，而这两个索引位置是1s。'
- en: Let’s go with the latter solution to vectorize the data. When done manually,
    the process looks like the following.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择后者来矢量化数据。当手动完成时，过程如下所示。
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 4.3](#listing-4-3): Encoding the integer sequences via multi-hot encoding'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.3](#listing-4-3)：通过多热编码对整数序列进行编码'
- en: 'Here’s what the samples look like now:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在样本看起来是这样的：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In addition to vectorizing the input sequences, you should also vectorize their
    labels, which is straightforward. Our labels are already NumPy arrays, so just
    convert the type from ints to floats:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了矢量化输入序列外，你还应该矢量化它们的标签，这很简单。我们的标签已经是NumPy数组，所以只需将类型从int转换为float：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now the data is ready to be fed into a neural network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已经准备好输入到神经网络中。
- en: Building your model
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建你的模型
- en: 'The input data is vectors, and the labels are scalars (1s and 0s): this is
    one of the simplest problem setups you’ll ever encounter. A type of model that
    performs well on such a problem is a plain stack of densely connected (`Dense`)
    layers with `relu` activations.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是向量，标签是标量（1s和0s）：这是你将遇到的简单问题设置之一。在这样一个问题上表现良好的模型类型是带有`relu`激活的密集连接（`Dense`）层的简单堆叠。
- en: 'There are two key architecture decisions to be made about such a stack of `Dense`
    layers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个`Dense`层堆叠中，有两个关键架构决策需要做出：
- en: How many layers to use
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多少层
- en: How many units to choose for each layer
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层选择多少个单元
- en: 'In chapter 5, you’ll learn formal principles to guide you in making these choices.
    For the time being, you’ll have to trust us with the following architecture choice:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，你将学习正式原则来指导你做出这些选择。目前，你必须相信我们以下架构选择：
- en: Two intermediate layers with 16 units each
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个中间层，每个层有16个单元
- en: A third layer that will output the scalar prediction regarding the sentiment
    of the current review
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层将输出关于当前评论情感的标量预测
- en: Figure 4.1 shows what the model looks like. Here’s the Keras implementation,
    similar to the MNIST example you saw previously.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1显示了模型的外观。这是Keras实现，类似于你之前看到的MNIST示例。
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 4.4](#listing-4-4): Model definition'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.4](#listing-4-4)：模型定义'
- en: '![](../Images/68f662b777d64b6a3aabdcb729b94ca2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68f662b777d64b6a3aabdcb729b94ca2.png)'
- en: '[Figure 4.1](#figure-4-1): The three-layer model'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.1](#figure-4-1)：三层模型'
- en: 'The first argument being passed to each `Dense` layer is the number of *units*
    in the layer: the dimensionality of representation space of the layer. You remember
    from chapters 2 and 3 that each such `Dense` layer with a `relu` activation implements
    the following chain of tensor operations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给每个`Dense`层的第一个参数是该层的*单元*数量：该层的表示空间维度。你从第2章和第3章中记得，每个这样的`Dense`层使用`relu`激活函数实现以下链式张量操作：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Having 16 units means the weight matrix `W` will have shape `(input_dimension,
    16)`: the dot product with `W` will project the input data onto a 16-dimensional
    representation space (and then you’ll add the bias vector `b` and apply the `relu`
    operation). You can intuitively understand the dimensionality of your representation
    space as “how much freedom you’re allowing the model to have when learning internal
    representations.” Having more units (a higher-dimensional representation space)
    allows your model to learn more complex representations, but it makes the model
    more computationally expensive and may lead to learning unwanted patterns (patterns
    that will improve performance on the training data but not on the test data).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有16个单元意味着权重矩阵`W`的形状将是`(input_dimension, 16)`：输入数据与`W`的点积将把输入数据投影到16维表示空间（然后你会添加偏置向量`b`并应用`relu`操作）。你可以直观地理解你的表示空间的维度为“你在模型学习内部表示时允许多少自由度。”拥有更多的单元（更高维的表示空间）允许你的模型学习更复杂的表示，但它会使模型计算成本更高，并可能导致学习到不希望的模式（这些模式会在训练数据上提高性能，但在测试数据上不会）。
- en: The intermediate layers use `relu` as their activation function, and the final
    layer uses a sigmoid activation to output a probability (a score between 0 and
    1, indicating how likely the review is to be positive). A `relu` (rectified linear
    unit) is a function meant to zero-out negative values (see figure 4.2), whereas
    a sigmoid “squashes” arbitrary values into the `[0, 1]` interval (see figure 4.3),
    outputting something that can be interpreted as a probability.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层使用`relu`作为它们的激活函数，而最后一层使用sigmoid激活来输出一个概率（一个介于0和1之间的分数，表示评论可能是正面的可能性）。`relu`（修正线性单元）是一个旨在将负值置零的函数（见图4.2），而sigmoid函数“压缩”任意值到`[0,
    1]`区间（见图4.3），输出可以解释为概率的东西。
- en: '![](../Images/49a10336cc1638ad3a5a3adc18dfac80.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49a10336cc1638ad3a5a3adc18dfac80.png)'
- en: '[Figure 4.2](#figure-4-2): The rectified linear unit function'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.2](#figure-4-2)：修正线性单元函数'
- en: '![](../Images/94ee06bdd6e5edf30361b37018c9c9b6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94ee06bdd6e5edf30361b37018c9c9b6.png)'
- en: '[Figure 4.3](#figure-4-3): The sigmoid function'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.3](#figure-4-3)：sigmoid函数'
- en: 'Finally, you need to choose a loss function and an optimizer. Because you’re
    facing a binary classification problem and the output of your model is a probability
    (you end your model with a single-unit layer with a sigmoid activation), it’s
    best to use the `binary_crossentropy` loss. It isn’t the only viable choice: you
    could use, for instance, `mean_squared_error`. But crossentropy is usually the
    best choice when you’re dealing with models that output probabilities. *Crossentropy*
    is a quantity from the field of information theory that measures the distance
    between probability distributions or, in this case, between the ground-truth distribution
    and your predictions.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要选择一个损失函数和一个优化器。因为你面临的是一个二元分类问题，并且你的模型输出是一个概率（你的模型以一个具有sigmoid激活的单单元层结束），最好使用`binary_crossentropy`损失。这不是唯一可行的选择：例如，你可以使用`mean_squared_error`。但是，当处理输出概率的模型时，交叉熵通常是最佳选择。*交叉熵*是信息论领域的一个量，它衡量概率分布之间的距离，在这种情况下，是真实分布和你的预测之间的距离。
- en: As for the choice of the optimizer, we’ll go with `adam`, which is usually a
    good default choice for virtually any problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化器的选择，我们将采用`adam`，这对于几乎任何问题来说通常都是一个很好的默认选择。
- en: Here’s the step where you configure the model with the `adam` optimizer and
    the `binary_crossentropy` loss function. Note that you’ll also monitor accuracy
    during training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是配置模型使用`adam`优化器和`binary_crossentropy`损失函数的步骤。请注意，你将在训练过程中监控准确度。
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 4.5](#listing-4-5): Compiling the model'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.5](#listing-4-5)：编译模型'
- en: Validating your approach
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证你的方法
- en: As you learned in chapter 3, a deep learning model should never be evaluated
    on its training data — it’s standard practice to use a “validation set” to monitor
    the accuracy of the model during training. Here, you’ll create a validation set
    by setting apart 10,000 samples from the original training data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在第三章中学到的，深度学习模型永远不应该在其训练数据上评估——使用“验证集”来监控模型在训练期间的准确率是标准做法。在这里，您将通过从原始训练数据中分出
    10,000 个样本来创建一个验证集。
- en: You might ask, why not simply use the *test* data to evaluate the model? That
    seems like it would be easier. The reason is that you’re going to want to use
    the results you get on the validation set to inform your next choices to improve
    training — for instance, your choice of what model size to use or how many epochs
    to train for. When you start doing this, your validation scores stop being an
    accurate reflection of the performance of the model on brand-new data, since the
    model has been deliberately modified to perform better on the validation data.
    It’s good to keep around a set of never-before-seen samples that you can use to
    perform the final evaluation round in a completely unbiased way, and that’s exactly
    what the test set is. We’ll talk more about this in the next chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问，为什么不简单地使用 *测试* 数据来评估模型？这似乎会更容易。原因是您将想要使用在验证集上得到的结果来指导您下一步的选择以改进训练——例如，您选择使用什么模型大小或训练多少个周期。当您开始这样做时，您的验证分数就不再准确反映模型在全新数据上的性能，因为模型已经被故意修改以在验证数据上表现更好。保留一组从未见过的新样本以完全无偏见地执行最终评估回合是很好的，这正是测试集的作用。我们将在下一章中更多地讨论这一点。
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 4.6](#listing-4-6): Setting aside a validation set'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.6](#listing-4-6)：设置验证集'
- en: You’ll now train the model for 20 epochs (20 iterations over all samples in
    the training data), in mini-batches of 512 samples. At the same time, you’ll monitor
    loss and accuracy on the 10,000 samples that you set apart. You do so by passing
    the validation data as the `validation_data` argument to `model.fit()`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将使用 20 个周期（在训练数据中的所有样本上迭代 20 次）来训练模型，每次批处理 512 个样本。同时，您将监控您分出的 10,000 个样本的损失和准确率。您可以通过将验证数据作为
    `validation_data` 参数传递给 `model.fit()` 来做到这一点。
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 4.7](#listing-4-7): Training your model'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.7](#listing-4-7)：训练您的模型'
- en: On CPU, this will take less than 2 seconds per epoch — training is over in 20
    seconds. At the end of every epoch, there is a slight pause as the model computes
    its loss and accuracy on the 10,000 samples of the validation data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，这将在每个周期中花费不到 2 秒——训练在 20 秒内完成。在每个周期的末尾，当模型在验证数据的 10,000 个样本上计算其损失和准确率时，会有轻微的暂停。
- en: 'Note that the call to `model.fit()` returns a `History` object, as you’ve seen
    in chapter 3\. This object has a member `history`, which is a dictionary containing
    data about everything that happened during training. Let’s look at it:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`model.fit()` 的调用返回一个 `History` 对象，正如您在第三章中看到的。该对象有一个 `history` 成员，它是一个字典，包含有关训练期间发生的一切的数据。让我们来看看它：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The dictionary contains four entries: one per metric that was being monitored
    during training and during validation. In the following two listings, let’s use
    Matplotlib to plot the training and validation loss side by side (see figure 4.4),
    as well as the training and validation accuracy (see figure 4.5). Note that your
    own results may vary slightly due to a different random initialization of your
    model.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该字典包含四个条目：每个条目对应于训练期间和验证期间被监控的指标。在接下来的两个列表中，我们将使用 Matplotlib 并排绘制训练和验证损失（见图
    4.4），以及训练和验证准确率（见图 4.5）。请注意，由于您自己的模型随机初始化不同，您自己的结果可能会有所不同。
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 4.8](#listing-4-8): Plotting the training and validation loss'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.8](#listing-4-8)：绘制训练和验证损失'
- en: '![](../Images/4250e81cf571d19717642a9c2078b966.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4250e81cf571d19717642a9c2078b966.png)'
- en: '[Figure 4.4](#figure-4-4): Training and validation loss'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.4](#figure-4-4)：训练和验证损失'
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 4.9](#listing-4-9): Plotting the training and validation accuracy'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.9](#listing-4-9)：绘制训练和验证准确率'
- en: '![](../Images/360b66ab70d2777c93c1bd914592906d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/360b66ab70d2777c93c1bd914592906d.png)'
- en: '[Figure 4.5](#figure-4-5): Training and validation accuracy'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.5](#figure-4-5)：训练和验证准确率'
- en: 'As you can see, the training loss decreases with every epoch, and the training
    accuracy increases with every epoch. That’s what you would expect when running
    gradient-descent optimization — the quantity you’re trying to minimize should
    be less with every iteration. But that isn’t the case for the validation loss
    and accuracy: they seem to peak at the fourth epoch. This is an example of what
    we warned against earlier: a model that performs better on the training data isn’t
    necessarily a model that will do better on data it has never seen before. In precise
    terms, what you’re seeing is *overfitting*: after the fourth epoch, you’re overoptimizing
    on the training data, and you end up learning representations that are specific
    to the training data and don’t generalize to data outside of the training set.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，随着每个epoch的进行，训练损失减少，训练准确率提高。当你运行梯度下降优化时，你应该期望看到这种情况——你试图最小化的量应该随着每次迭代而减少。但是，验证损失和准确率并不是这样：它们似乎在第4个epoch达到峰值。这是我们之前警告过的一个例子：在训练数据上表现更好的模型并不一定会在之前未见过的数据上表现更好。精确地说，你所看到的是*过拟合*：在第4个epoch之后，你对训练数据进行了过度优化，最终学习到的表示只针对训练数据，不能推广到训练集之外的数据。
- en: In this case, to prevent overfitting, you could stop training after four epochs.
    In general, you can use a range of techniques to mitigate overfitting, which we’ll
    cover in chapter 5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了防止过拟合，你可以在四个epoch后停止训练。一般来说，你可以使用一系列技术来减轻过拟合，我们将在第5章中介绍这些技术。
- en: Let’s train a new model from scratch for four epochs and then evaluate it on
    the test data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从零开始训练一个新的模型四个epoch，然后对测试数据进行评估。
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 4.10](#listing-4-10): Training the model for four epochs'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.10](#listing-4-10)：对模型进行四个epoch的训练'
- en: 'The final results are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This fairly naive approach achieves an accuracy of 88%. With state-of-the-art
    approaches, you should be able to get close to 95%.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相当简单的方法达到了88%的准确率。使用最先进的方法，你应该能够接近95%。
- en: Using a trained model to generate predictions on new data
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用训练好的模型对新数据进行预测
- en: 'After having trained a model, you’ll want to use it in a practical setting.
    You can generate the likelihood of reviews being positive by using the `predict`
    method, as you’ve learned in chapter 3:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好一个模型后，你可能会想在实际环境中使用它。你可以通过使用第3章中学到的`predict`方法来生成评论为正面的可能性：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, the model is confident for some samples (0.99 or more, or 0.01
    or less) but less confident for others (0.6, 0.4).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于某些样本（0.99或更多，或0.01或更少），模型很有信心，但对于其他样本（0.6，0.4）则不太自信。
- en: Further experiments
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步实验
- en: 'The following experiments will help convince you that the architecture choices
    you’ve made are all fairly reasonable, although there’s still room for improvement:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验将帮助你相信你所做的架构选择都是相当合理的，尽管仍有改进的空间：
- en: You used two representation layers before the final classification layer. Try
    using one or three representation layers and see how doing so affects validation
    and test accuracy.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终分类层之前，你使用了两个表示层。尝试使用一个或三个表示层，看看这样做如何影响验证和测试准确率。
- en: 'Try using layers with more units or fewer units: 32 units, 64 units, and so
    on.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用具有更多或更少单元的层：32个单元，64个单元，依此类推。
- en: Try using the `mean_squared_error` loss function instead of `binary_crossentropy`.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`mean_squared_error`损失函数代替`binary_crossentropy`。
- en: Try using the `tanh` activation (an activation that was popular in the early
    days of neural networks) instead of `relu`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`tanh`激活（在神经网络早期很受欢迎的激活函数）代替`relu`。
- en: Wrapping up
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Here’s what you should take away from this example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，你应该吸取以下教训：
- en: You usually need to do quite a bit of preprocessing on your raw data to be able
    to feed it — as tensors — into a neural network. Sequences of words can be encoded
    as binary vectors, but there are other encoding options, too.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通常需要对原始数据进行相当多的预处理，以便将其作为张量输入到神经网络中。单词序列可以编码为二进制向量，但还有其他编码选项。
- en: Stacks of `Dense` layers with `relu` activations can solve a wide range of problems
    (including sentiment classification), and you’ll use them frequently.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有`relu`激活的`Dense`层堆叠可以解决广泛的问题（包括情感分类），你将经常使用它们。
- en: 'In a binary classification problem (two output classes), your model should
    end with a `Dense` layer with one unit and a `sigmoid` activation: the output
    of your model should be a scalar between 0 and 1, encoding a probability.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类问题（两个输出类别）中，你的模型应该以一个具有一个单元和`sigmoid`激活的`Dense`层结束：你的模型输出应该是一个介于0和1之间的标量，表示一个概率。
- en: With such a scalar sigmoid output on a binary classification problem, the loss
    function you should use is `binary_crossentropy`.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类问题上使用这样的标量sigmoid输出时，你应该使用的损失函数是`binary_crossentropy`。
- en: The `adam` optimizer is generally a good enough choice, whatever your problem.
    That’s one less thing for you to worry about.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam`优化器通常是一个足够好的选择，无论你的问题是什么。这样你就少了一件需要担心的事情。'
- en: As they get better on their training data, neural networks eventually start
    overfitting and end up obtaining increasingly worse results on data they’ve never
    seen before. Be sure to always monitor performance on data that is outside of
    the training set!
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着神经网络在训练数据上的表现越来越好，它们最终开始过拟合，并在之前从未见过的数据上获得越来越差的结果。务必始终监控训练集之外的数据的性能！
- en: 'Classifying newswires: A multiclass classification example'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新闻稿分类：多类分类示例
- en: In the previous section, you saw how to classify vector inputs into two mutually
    exclusive classes using a densely connected neural network. But what happens when
    you have more than two classes?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你看到了如何使用密集连接神经网络将向量输入分类到两个互斥的类别。但是，当你有超过两个类别时会发生什么呢？
- en: In this section, you’ll build a model to classify Reuters newswires into 46
    mutually exclusive topics. Because you have many classes, this problem is an instance
    of *multiclass classification*, and because each data point should be classified
    into only one category, the problem is more specifically an instance of *single-label*,
    *multiclass classification*. If each data point could belong to multiple categories
    (in this case, topics), you’d be facing a *multilabel*, *multiclass classification*
    problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将构建一个模型，将路透社新闻稿分类到46个互斥的主题中。因为你有很多类别，这个问题是一个多类分类的实例，而且由于每个数据点应该只被分类到单个类别，所以这个问题更具体地是一个单标签多类分类的实例。如果你每个数据点可以属于多个类别（在这种情况下，主题），你将面临一个多标签多类分类的问题。
- en: The Reuters dataset
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路透社数据集
- en: You’ll work with the Reuters dataset, a set of short newswires and their topics,
    published by Reuters in 1986\. It’s a simple, widely used toy dataset for text
    classification. There are 46 different topics; some topics are more represented
    than others, but each topic has at least 10 examples in the training set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用路透社数据集，这是一组由路透社在1986年发布的简短新闻稿及其主题。它是一个简单且广泛使用的文本分类玩具数据集。共有46个不同的主题；有些主题比其他主题更常见，但每个主题在训练集中至少有10个示例。
- en: Like IMDb and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s
    take a look.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就像IMDb和MNIST一样，路透社数据集是Keras的一部分。让我们看看。
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 4.11](#listing-4-11): Loading the Reuters dataset'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.11](#listing-4-11)：加载路透社数据集'
- en: As with the IMDb dataset, the argument `num_words=10000` restricts the data
    to the 10,000 most frequently occurring words found in the data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 就像IMDb数据集一样，`num_words=10000`参数限制了数据只包含在数据中出现频率最高的10,000个单词。
- en: 'You have 8,982 training examples and 2,246 test examples:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你有8,982个训练示例和2,246个测试示例：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As with the IMDb reviews, each example is a list of integers (word indices):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就像IMDb评论一样，每个示例都是一个整数列表（单词索引）：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here’s how you can decode it back to words, in case you’re curious.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何将其解码回单词的，以防你感兴趣。
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Listing 4.12](#listing-4-12): Decoding newswires back to text'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.12](#listing-4-12)：将新闻稿解码回文本'
- en: 'The label associated with an example is an integer between 0 and 45 — a topic
    index:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与示例相关的标签是一个介于0到45之间的整数——一个主题索引：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Preparing the data
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: You can vectorize the data with the exact same code as in the previous example.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用与上一个示例完全相同的代码来向量化数据。
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 4.13](#listing-4-13): Encoding the input data'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.13](#listing-4-13)：编码输入数据'
- en: 'To vectorize the labels, there are two possibilities: you can leave the labels
    untouched as integers, or you can use *one-hot encoding*. One-hot encoding is
    a widely used format for categorical data, also called *categorical encoding*.
    In this case, one-hot encoding of the labels consists of embedding each label
    as an all-zero vector with a 1 in the place of the label index. Here’s an example.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向量化标签，有两种可能性：你可以保持标签不变作为整数，或者你可以使用 *one-hot encoding*。One-hot encoding 是一种广泛使用的分类数据格式，也称为
    *分类编码*。在这种情况下，标签的 one-hot encoding 是将每个标签嵌入为一个全零向量，标签索引的位置为 1。以下是一个示例。
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 4.14](#listing-4-14): Encoding the labels'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.14](#listing-4-14)：编码标签'
- en: 'Note that there is a built-in way to do this in Keras:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 Keras 中有内置的方式来完成这个操作：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Building your model
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建你的模型
- en: 'This topic classification problem looks similar to the previous movie review
    classification problem: in both cases, you’re trying to classify short snippets
    of text. But there is a new constraint here: the number of output classes has
    gone from 2 to 46\. The dimensionality of the output space is much larger.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题分类问题看起来与先前的电影评论分类问题相似：在两种情况下，你都在尝试对短文本片段进行分类。但这里有一个新的约束：输出类别的数量从 2 增加到 46。输出空间的维度要大得多。
- en: 'In a stack of `Dense` layers like those you’ve been using, each layer can only
    access information present in the output of the previous layer. If one layer drops
    some information relevant to the classification problem, this information can
    never be recovered by later layers: each layer can potentially become an information
    bottleneck. In the previous example, you used 16-dimensional intermediate layers,
    but a 16-dimensional space may be too limited to learn to separate 46 different
    classes: such small layers may act as information bottlenecks, permanently dropping
    relevant information.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在像你一直在使用的 `Dense` 层堆叠中，每一层只能访问前一层输出的信息。如果一个层丢失了与分类问题相关的某些信息，这些信息将永远无法被后续层恢复：每一层都可能成为一个信息瓶颈。在先前的例子中，你使用了
    16 维的中间层，但 16 维的空间可能太小，无法学习区分 46 个不同的类别：这样的小层可能充当信息瓶颈，永久性地丢失相关信息。
- en: For this reason, you’ll use larger intermediate layers. Let’s go with 64 units.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你会使用更大的中间层。让我们使用 64 个单位。
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 4.15](#listing-4-15): Model definition'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.15](#listing-4-15)：模型定义'
- en: 'There are two other things you should note about this architecture:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个架构，还有两件事你应该注意：
- en: You end the model with a `Dense` layer of size 46\. This means for each input
    sample, the network will output a 46-dimensional vector. Each entry in this vector
    (each dimension) will encode a different output class.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你以一个大小为 46 的 `Dense` 层结束模型。这意味着对于每个输入样本，网络将输出一个 46 维的向量。这个向量中的每个条目（每个维度）将编码一个不同的输出类别。
- en: The last layer uses a `softmax` activation. You saw this pattern in the MNIST
    example. It means the model will output a *probability distribution* over the
    46 different output classes — for every input sample, the model will produce a
    46-dimensional output vector, where `output[i]` is the probability that the sample
    belongs to class `i`. The 46 scores will sum to 1.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层使用 `softmax` 激活函数。你在 MNIST 示例中见过这种模式。这意味着模型将在 46 个不同的输出类别上输出一个 *概率分布* ——
    对于每个输入样本，模型将产生一个 46 维的输出向量，其中 `output[i]` 是样本属于类别 `i` 的概率。这 46 个分数加起来等于 1。
- en: The best loss function to use in this case is `categorical_crossentropy`. It
    measures the distance between two probability distributions — here, between the
    probability distribution outputted by the model and the true distribution of the
    labels. By minimizing the distance between these two distributions, you train
    the model to output something as close as possible to the true labels.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，最佳损失函数是 `categorical_crossentropy`。它衡量两个概率分布之间的距离——在这里，是模型输出的概率分布和标签真实分布之间的距离。通过最小化这两个分布之间的距离，你训练模型输出尽可能接近真实标签的内容。
- en: 'Like last time, we’ll also monitor accuracy. However, accuracy is a bit of
    a crude metric in this case: if the model has the correct class as its second
    choice for a given sample, with an incorrect first choice, the model will still
    have an accuracy of zero on that sample — even though such a model would be much
    better than a random guess. A more nuanced metric in this case is top-k accuracy,
    such as top-3 or top-5 accuracy. It measures whether the correct class was among
    the top-k predictions of the model. Let’s add top-3 accuracy to our model.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 和上次一样，我们也会监控准确率。然而，在这个案例中，准确率是一个相当粗略的指标：如果模型对一个样本的第二选择是正确的类别，而第一个选择是错误的，那么模型在这个样本上的准确率仍然是零——即使这样的模型会比随机猜测好得多。在这种情况下，一个更细致的指标是
    top-k 准确率，比如 top-3 或 top-5 准确率。它衡量正确的类别是否在模型的 top-k 预测中。让我们将 top-3 准确率添加到我们的模型中。
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 4.16](#listing-4-16): Compiling the model'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.16](#listing-4-16)：编译模型'
- en: Validating your approach
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证你的方法
- en: Let’s set apart 1,000 samples in the training data to use as a validation set.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练数据中留出 1,000 个样本作为验证集。
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Listing 4.17](#listing-4-17): Setting aside a validation set'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.17](#listing-4-17)：留出验证集'
- en: Now, let’s train the model for 20 epochs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型 20 个周期。
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 4.18](#listing-4-18): Training the model'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.18](#listing-4-18)：训练模型'
- en: And finally, let’s display its loss and accuracy curves (see figures 4.6 and
    4.7).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们显示其损失和准确率曲线（见图 4.6 和 4.7）。
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Listing 4.19](#listing-4-19): Plotting the training and validation loss'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.19](#listing-4-19)：绘制训练和验证损失'
- en: '![](../Images/700f48941826f86c77529d558bb444c4.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/700f48941826f86c77529d558bb444c4.png)'
- en: '[Figure 4.6](#figure-4-6): Training and validation loss'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.6](#figure-4-6)：训练和验证损失'
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[Listing 4.20](#listing-4-20): Plotting the training and validation top-3 accuracy'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.20](#listing-4-20)：绘制训练和验证 top-3 准确率'
- en: '![](../Images/1fca6a813a775f4369c4651c43e4b471.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1fca6a813a775f4369c4651c43e4b471.png)'
- en: '[Figure 4.7](#figure-4-7): Training and validation accuracy'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.7](#figure-4-7)：训练和验证准确率'
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Listing 4.21](#listing-4-21): Plotting the training and validation top-3 accuracy'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.21](#listing-4-21)：绘制训练和验证 top-3 准确率'
- en: '![](../Images/a76873d2f26e20d65d983d713807a26a.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a76873d2f26e20d65d983d713807a26a.png)'
- en: '[Figure 4.8](#figure-4-8): Training and validation accuracy'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.8](#figure-4-8)：训练和验证准确率'
- en: The model begins to overfit after nine epochs. Let’s train a new model from
    scratch for nine epochs and then evaluate it on the test set.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在第九个周期后开始过拟合。让我们从头开始训练一个新的模型九个周期，然后在测试集上评估它。
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[Listing 4.22](#listing-4-22): Retraining a model from scratch'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.22](#listing-4-22)：从头开始重新训练模型'
- en: 'Here are the final results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终结果：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This approach reaches an accuracy of approximately 80%. With a balanced binary
    classification problem, the accuracy reached by a purely random classifier would
    be 50%. But in this case, we have 46 classes, and they may not be equally represented.
    What would be the accuracy of a random baseline? We could try quickly implementing
    one to check this empirically:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法达到了大约 80% 的准确率。对于一个平衡的二分类问题，纯随机分类器达到的准确率将是 50%。但在这个案例中，我们有 46 个类别，它们可能并不均衡地被表示。随机基线的准确率会是多少？我们可以快速实现一个来验证这一点：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, a random classifier would score around 19% classification accuracy,
    so the results of our model seem pretty good in that light.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一个随机分类器的大约分类准确率为 19%，因此从这个角度看，我们模型的成果看起来相当不错。
- en: Generating predictions on new data
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在新数据上生成预测
- en: 'Calling the model’s `predict` method on new samples returns a class probability
    distribution over all 46 topics for each sample. Let’s generate topic predictions
    for all of the test data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在新样本上调用模型的 `predict` 方法返回每个样本对所有 46 个主题的类别概率分布。让我们为所有测试数据生成主题预测：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Each entry in “predictions” is a vector of length 46:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: “预测”中的每个条目都是一个长度为 46 的向量：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The coefficients in this vector sum to 1, as they form a probability distribution:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量中的系数之和为 1，因为它们形成了一个概率分布：
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The largest entry is the predicted class — the class with the highest probability:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的条目是预测的类别——概率最高的类别：
- en: '[PRE40]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: A different way to handle the labels and the loss
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理标签和损失的不同方法
- en: 'We mentioned earlier that another way to encode the labels would be to leave
    them untouched as integer tensors, like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，另一种编码标签的方法是保持它们不变，作为整数张量，如下所示：
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The only thing this approach would change is the choice of the loss function.
    The loss function used in listing 4.22, `categorical_crossentropy`, expects the
    labels to follow a categorical encoding. With integer labels, you should use `sparse_categorical_crossentropy`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法唯一改变的是损失函数的选择。列表4.22中使用的损失函数`categorical_crossentropy`期望标签遵循分类编码。对于整数标签，您应使用`sparse_categorical_crossentropy`：
- en: '[PRE42]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This new loss function is still mathematically the same as `categorical_crossentropy`;
    it just has a different interface.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的损失函数在数学上仍然与`categorical_crossentropy`相同；它只是有一个不同的接口。
- en: The importance of having sufficiently large intermediate layers
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拥有足够大的中间层的重要性
- en: 'We mentioned earlier that because the final outputs are 46-dimensional, you
    should avoid intermediate layers with much fewer than 46 units. Now let’s see
    what happens when you introduce an information bottleneck by having intermediate
    layers that are significantly less than 46-dimensional: for example, 4-dimensional.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，由于最终输出是46维的，您应该避免具有少于46个单位的中间层。现在让我们看看当您通过具有显著小于46维的中间层引入信息瓶颈时会发生什么：例如，4维。
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[Listing 4.23](#listing-4-23): A model with an information bottleneck'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.23](#listing-4-23)：具有信息瓶颈的模型'
- en: The model now peaks at approximately 71% validation accuracy, an 8% absolute
    drop. This drop is mostly due to the fact that you’re trying to compress a lot
    of information (enough information to recover the separation hyperplanes of 46
    classes) into an intermediate space that is too low-dimensional. The model is
    able to cram *most* of the necessary information into these 4-dimensional representations,
    but not all of it.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在在约71%的验证准确率处达到峰值，下降了8%。这种下降主要是由于您试图将大量信息（足够的信息来恢复46个类的分离超平面）压缩到一个维度太低的中间空间。模型能够将这些必要信息中的*大部分*压缩到这些4维表示中，但并非全部。
- en: Further experiments
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步实验
- en: 'Like in the previous example, we encourage you to try out the following experiments
    to train your intuition about the kind of configuration decisions you have to
    make with such models:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例类似，我们鼓励您尝试以下实验，以训练您对这些模型必须做出的配置决策的直觉：
- en: 'Try using larger or smaller layers: 32 units, 128 units, and so on.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更大或更小的层：32个单位，128个单位，等等。
- en: You used two intermediate layers before the final softmax classification layer.
    Now try using a single intermediate layer, or three intermediate layers.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在最终的softmax分类层之前使用了两个中间层。现在尝试使用一个中间层，或者三个中间层。
- en: Wrapping up
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Here’s what you should take away from this example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该从这个示例中吸取以下教训：
- en: If you’re trying to classify data points among *N* classes, your model should
    end with a `Dense` layer of size *N*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您试图在*N*个类别中分类数据点，您的模型应以大小为*N*的`Dense`层结束。
- en: In a single-label, multiclass classification problem, your model should end
    with a `softmax` activation so that it will output a probability distribution
    over the *N* output classes.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单标签多类分类问题中，您的模型应以`softmax`激活结束，以便它将输出一个关于*N*个输出类别的概率分布。
- en: Categorical crossentropy is almost always the loss function you should use for
    such problems. It minimizes the distance between the probability distributions
    output by the model and the true distribution of the targets.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类交叉熵几乎总是您应该用于此类问题的损失函数。它最小化模型输出的概率分布与目标真实分布之间的距离。
- en: 'There are two ways to handle labels in multiclass classification:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多类分类中处理标签有两种方法：
- en: Encoding the labels via categorical encoding (also known as one-hot encoding)
    and using `categorical_crossentropy` as a loss function
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分类编码（也称为独热编码）对标签进行编码，并使用`categorical_crossentropy`作为损失函数
- en: Encoding the labels as integers and using the `sparse_categorical_crossentropy`
    loss function
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签编码为整数并使用`sparse_categorical_crossentropy`损失函数
- en: If you need to classify data into a large number of categories, you should avoid
    creating information bottlenecks in your model due to intermediate layers that
    are too small.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要将数据分类到大量类别中，您应该避免由于中间层太小而在模型中创建信息瓶颈。
- en: 'Predicting house prices: A regression example'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测房价：回归示例
- en: 'The two previous examples were considered classification problems, where the
    goal was to predict a single discrete label of an input data point. Another common
    type of machine learning problem is *regression*, which consists of predicting
    a continuous value instead of a discrete label: for instance, predicting the temperature
    tomorrow given meteorological data, or predicting the time that a software project
    will take to complete given its specifications.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个例子被认为是分类问题，目标是预测输入数据点的单个离散标签。另一种常见的机器学习问题是 *回归*，它由预测连续值而不是离散标签组成：例如，根据气象数据预测明天的温度，或者根据软件项目的规格预测完成项目所需的时间。
- en: The California Housing Price dataset
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加利福尼亚房价数据集
- en: You’ll attempt to predict the median price of homes in different areas of California,
    based on data from the 1990 census.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 您将尝试根据 1990 年人口普查的数据预测加利福尼亚不同地区的房屋中位数价格。
- en: Each data point in the dataset represents information about a “block group,”
    a group of homes located in the same area. You can think of it as a district.
    This dataset has two versions, the “small” version with just 600 districts, and
    the “large” version with 20,640 districts. Let’s use the small version, because
    real-world datasets can often be tiny, and you need to know how to handle such
    cases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个数据点代表一个“街区组”的信息，这是一个位于同一区域的住宅群组。您可以将其视为一个区域。这个数据集有两个版本，一个是只有 600 个区域的“小”版本，另一个是包含
    20,640 个区域的“大”版本。让我们使用小版本，因为现实世界的数据集通常可能非常小，您需要知道如何处理这种情况。
- en: For each district, we know
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个区域，我们知道
- en: The longitude and latitude of the approximate geographic center of the area.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该区域大约地理中心的经纬度。
- en: The median age of houses in the district.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该区域房屋的中位数年龄。
- en: 'The population of the district. The districts are pretty small: the average
    population is 1,425.5.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该区域的人口。这些区域相当小：平均人口为 1,425.5。
- en: The total number of households.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭总数。
- en: The median income of those households.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些家庭的中位数收入。
- en: The total number of rooms in the district, across all homes located there. This
    is typically in the low thousands.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该区域所有房屋的总房间数。这通常在几千以下。
- en: The total number of bedrooms in the district.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该区域卧室总数。
- en: That’s eight variables in total (longitude and latitude count as two variables).
    The goal is to use these variables to predict the median value of the houses in
    the district. Let’s get started by loading the data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有八个变量（经纬度算作两个变量）。目标是使用这些变量来预测该区域房屋的中位数价格。让我们通过加载数据开始吧。
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[Listing 4.24](#listing-4-24): Loading the California Housing Prices dataset'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.24](#listing-4-24)：加载加利福尼亚房价数据集'
- en: 'Let’s look at the data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据：
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As you can see, we have 480 training samples and 120 test samples, each with
    8 numerical features. The targets are the median values of homes in the district
    considered, in dollars:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有 480 个训练样本和 120 个测试样本，每个样本包含 8 个数值特征。目标是预测该区域考虑的房屋的中位数价格，单位为美元：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The prices are between $60,000 and $500,000\. If that sounds cheap, remember
    that this was in 1990, and these prices aren’t adjusted for inflation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 价格在 60,000 美元到 500,000 美元之间。如果听起来很便宜，请记住这是在 1990 年，这些价格没有考虑通货膨胀。
- en: Preparing the data
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'It would be problematic to feed into a neural network values that all take
    wildly different ranges. The model might be able to automatically adapt to such
    heterogeneous data, but it would definitely make learning more difficult. A widespread
    best practice to deal with such data is to do feature-wise normalization: for
    each feature in the input data (a column in the input data matrix), you subtract
    the mean of the feature and divide by the standard deviation, so that the feature
    is centered around 0 and has a unit standard deviation. This is easily done in
    NumPy.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有范围差异很大的值输入神经网络可能会出现问题。模型可能能够自动适应这种异构数据，但这肯定会使学习更加困难。处理此类数据的一个普遍最佳实践是进行特征归一化：对于输入数据中的每个特征（输入数据矩阵中的一列），您从特征的平均值中减去，然后除以标准差，这样特征就围绕
    0 对齐，并且具有单位标准差。这可以在 NumPy 中轻松完成。
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[Listing 4.25](#listing-4-25): Normalizing the data'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.25](#listing-4-25)：数据归一化'
- en: Note that the quantities used for normalizing the test data are computed using
    the training data. You should never use in your workflow any quantity computed
    on the test data, even for something as simple as data normalization.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用于归一化测试数据的量是通过训练数据计算的。你永远不应该在你的工作流程中使用在测试数据上计算的任何量，即使是像数据归一化这样简单的事情。
- en: In addition, we should also scale the targets. Our normalized inputs have their
    value in a small range close to 0, and our model’s weights are initialized with
    small random values. This means that our model’s prediction will also be small
    values when we start training. If the targets are in the range 60,000–500,000,
    the model is going to need very large weight values to output those. With a small
    learning rate, it would take a very long time to get there. The simplest fix is
    to divide all target values by 100,000, so that the smallest target becomes 0.6,
    and the largest becomes 5\. We can then convert the model’s predictions back to
    dollar values by multiplying them by 100,000 accordingly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还应该缩放目标值。我们的归一化输入值在一个接近 0 的小范围内，我们的模型权重是用小的随机值初始化的。这意味着当我们开始训练时，我们的模型预测也将是小的值。如果目标值在
    60,000–500,000 的范围内，模型将需要非常大的权重值来输出这些值。使用小的学习率，到达那里将需要非常长的时间。最简单的修复方法是除以所有目标值
    100,000，这样最小的目标值变为 0.6，最大的变为 5。然后我们可以通过相应地乘以 100,000 将模型的预测值转换回美元值。
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[Listing 4.26](#listing-4-26): Scaling the targets'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.26](#listing-4-26)：缩放目标值'
- en: Building your model
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建你的模型
- en: Because so few samples are available, you’ll use a very small model with two
    intermediate layers, each with 64 units. In general, the less training data you
    have, the worse overfitting will be, and using a small model is one way to mitigate
    overfitting.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用的样本非常少，你将使用一个非常小的模型，包含两个中间层，每个层有 64 个单元。一般来说，你拥有的训练数据越少，过拟合的风险就越大，使用小模型是减轻过拟合的一种方法。
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[Listing 4.27](#listing-4-27): Model definition'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.27](#listing-4-27)：模型定义'
- en: 'The model ends with a single unit and no activation: it will be a linear layer.
    This is a typical setup for scalar regression — a regression where you’re trying
    to predict a single continuous value. Applying an activation function would constrain
    the range the output can take; for instance, if you applied a `sigmoid` activation
    function to the last layer, the model could only learn to predict values between
    0 and 1\. Here, because the last layer is purely linear, the model is free to
    learn to predict values in any range.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以单个单元和没有激活函数结束：它将是一个线性层。这是标量回归的典型设置——你试图预测一个单一连续值。应用激活函数会限制输出可以取的范围；例如，如果你在最后一层应用
    `sigmoid` 激活函数，模型只能学习预测介于 0 和 1 之间的值。在这里，因为最后一层完全是线性的，模型可以自由地学习预测任何范围内的值。
- en: Note that you compile the model with the `mean_squared_error` loss function
    — *mean squared error*, the square of the difference between the predictions and
    the targets. This is a widely used loss function for regression problems.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你使用 `mean_squared_error` 损失函数来编译模型——均方误差，预测值与目标值之间差异的平方。这是回归问题中广泛使用的损失函数。
- en: 'You’re also monitoring a new metric during training: *mean absolute error*
    (MAE). It’s the absolute value of the difference between the predictions and the
    targets. For instance, an MAE of 0.5 on this problem would mean your predictions
    are off by $50,000 on average (remember the target scaling of factor 100,000).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你还正在监控训练过程中的一个新指标：*平均绝对误差*（MAE）。它是预测值与目标值之间差异的绝对值。例如，这个问题上的 MAE 为 0.5 意味着你的预测平均偏离目标值
    50,000（记住目标缩放因子为 100,000）。
- en: Validating your approach using K-fold validation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 K 折交叉验证来验证你的方法
- en: 'To evaluate your model while you keep adjusting its parameters (such as the
    number of epochs used for training), you could split the data into a training
    set and a validation set, as you did in the previous examples. But because you
    have so few data points, the validation set would end up being very small (for
    instance, about 100 examples). As a consequence, the validation scores might change
    a lot depending on which data points you chose to use for validation and which
    you chose for training: the validation scores might have a high *variance* with
    regard to the validation split. This would prevent you from reliably evaluating
    your model.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调整模型的参数（如训练使用的 epoch 数量）时，评估你的模型，你可以将数据分成训练集和验证集，就像之前的例子中做的那样。但由于数据点很少，验证集最终会非常小（例如，大约
    100 个示例）。结果，验证分数可能会根据你选择用于验证和训练的数据点而大幅变化：验证分数可能会与验证分割有很大的 *方差*。这会阻止你可靠地评估你的模型。
- en: The best practice in such situations is to use *K-fold* cross-validation (see
    figure 4.9). It consists of splitting the available data into *K* partitions (typically
    *K* = 4 or 5), instantiating *K* identical models, and training each one on *K*
    – 1 partitions while evaluating on the remaining partition. The validation score
    for the model used is then the average of the *K* validation scores obtained.
    In terms of code, this is straightforward.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最佳实践是使用 *K 折* 交叉验证（见图 4.9）。它包括将可用数据分成 *K* 个分区（通常 *K* = 4 或 5），实例化 *K*
    个相同的模型，并在 *K* - 1 个分区上训练每个模型，同时在剩余的分区上进行评估。然后，用于模型的验证分数是获得的 *K* 个验证分数的平均值。从代码的角度来看，这是直截了当的。
- en: '![](../Images/bb6a837d8d083d01576c5920aa7ca957.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb6a837d8d083d01576c5920aa7ca957.png)'
- en: '[Figure 4.9](#figure-4-9): Three-fold cross-validation'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.9](#figure-4-9)：三折交叉验证'
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[Listing 4.28](#listing-4-28): K-fold validation'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.28](#listing-4-28)：K 折验证'
- en: 'Running this with `num_epochs = 50` yields the following results:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `num_epochs = 50` 运行此代码得到以下结果：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The different runs do indeed show meaningfully different validation scores,
    from 0.232 to 0.349\. The average (0.296) is a much more reliable metric than
    any single score — that’s the entire point of K-fold cross-validation. In this
    case, you’re off by $29,600 on average, which is significant considering that
    the prices range from $60,000 to $500,000.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的运行确实显示了有意义的不同的验证分数，从 0.232 到 0.349。平均数（0.296）比任何单个分数都更可靠——这正是 K 折交叉验证的全部意义。在这种情况下，平均误差为
    $29,600，考虑到价格范围在 $60,000 到 $500,000 之间，这是一个显著的数值。
- en: 'Let’s try training the model a bit longer: 200 epochs. To keep a record of
    how well the model does at each epoch, you’ll modify the training loop to save
    the per-epoch validation score log.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练模型更长一些：200 个 epoch。为了记录模型在每个 epoch 的表现，你需要修改训练循环以保存每个 epoch 的验证分数日志。
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[Listing 4.29](#listing-4-29): Saving the validation logs at each fold'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.29](#listing-4-29)：在每个折叠中保存验证日志'
- en: You can then compute the average of the per-epoch mean absolute error (MAE)
    scores for all folds.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以计算所有折叠的每个 epoch 均值绝对误差（MAE）分数的平均值。
- en: '[PRE53]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[Listing 4.30](#listing-4-30): Building the history of successive mean K-fold
    validation scores'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.30](#listing-4-30)：构建连续的 K 折验证平均分数的历史记录'
- en: Let’s plot this; see figure 4.10.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个图表；见图 4.10。
- en: '[PRE54]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[Listing 4.31](#listing-4-31): Plotting validation scores'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.31](#listing-4-31)：绘制验证分数'
- en: '![](../Images/3a62bf8f25afa58de847c1fe58574fe5.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a62bf8f25afa58de847c1fe58574fe5.png)'
- en: '[Figure 4.10](#figure-4-10): Validation MAE by epoch'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.10](#figure-4-10)：按 epoch 验证的 MAE'
- en: 'It may be a little difficult to read the plot due to a scaling issue: the validation
    MAE for the first few epochs is dramatically higher than the values that follow.
    Let’s omit the first 10 data points, which are on a different scale than the rest
    of the curve.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缩放问题，可能难以阅读图表：前几个 epoch 的验证 MAE 比后续的值高得多。让我们省略前 10 个数据点，这些数据点与曲线的其余部分处于不同的尺度。
- en: '[PRE55]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[Listing 4.32](#listing-4-32): Plotting validation scores, excluding the first
    10 data points'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 4.32](#listing-4-32)：绘制验证分数，排除前 10 个数据点'
- en: '![](../Images/e5f84d5faf3a31c710e45f85a1a04052.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5f84d5faf3a31c710e45f85a1a04052.png)'
- en: '[Figure 4.11](#figure-4-11): Validation MAE by epoch, excluding the first 10
    data points'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.11](#figure-4-11)：按 epoch 验证的 MAE，排除前 10 个数据点'
- en: According to this plot (see figure 4.11), validation MAE stops improving significantly
    after 120–140 epochs (this number includes the 10 epochs we omitted). Past that
    point, you start overfitting.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个图（见图 4.11），验证 MAE 在 120-140 个周期后（这个数字包括我们省略的 10 个周期）不再显著提高。超过这个点，你开始过拟合。
- en: Once you’re finished tuning other parameters of the model (in addition to the
    number of epochs, you could also adjust the size of the intermediate layers),
    you can train a final production model on all of the training data, with the best
    parameters, and then look at its performance on the test data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了模型其他参数的调整（除了周期数，你还可以调整中间层的大小），你可以在所有训练数据上使用最佳参数训练一个最终的生产模型，然后查看其在测试数据上的性能。
- en: '[PRE56]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[Listing 4.33](#listing-4-33): Training the final model'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.33](#listing-4-33)：训练最终模型'
- en: 'Here’s the final result:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终结果：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We’re still off by about $31,000 on average.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们平均偏差约为 $31,000。
- en: Generating predictions on new data
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在新数据上生成预测
- en: 'When calling `predict()` on our binary classification model, we retrieved a
    scalar score between 0 and 1 for each input sample. With our multiclass classification
    model, we retrieved a probability distribution over all classes for each sample.
    Now, with this scalar regression model, `predict()` returns the model’s guess
    for the sample’s price in hundreds of thousands of dollars:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当在二分类模型上调用 `predict()` 时，我们为每个输入样本检索了一个介于 0 和 1 之间的标量分数。对于我们的多类分类模型，我们为每个样本检索了所有类别的概率分布。现在，使用这个标量回归模型，`predict()`
    返回模型对样本价格的猜测，以十万美元为单位：
- en: '[PRE58]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The first district in the test set is predicted to have an average home price
    of about $283,000.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集中的第一个区域预测的平均房价约为 $283,000。
- en: Wrapping up
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Here’s what you should take away from this scalar regression example:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你应该从这个标量回归示例中吸取的教训：
- en: Regression is done using a different loss function than what we used for classification.
    Mean squared error (MSE) is a loss function commonly used for regression.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归使用与分类不同的损失函数。均方误差（MSE）是回归中常用的损失函数。
- en: Similarly, evaluation metrics to be used for regression differ from those used
    for classification; naturally, the concept of accuracy doesn’t apply for regression.
    A common regression metric is MAE.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，用于回归的评估指标与用于分类的指标不同；自然地，准确性的概念不适用于回归。MAE 是一个常见的回归指标。
- en: When features in the input data have values in different ranges, each feature
    should be scaled independently as a preprocessing step.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入数据中的特征具有不同范围的价值时，每个特征应该作为预处理步骤独立缩放。
- en: When there is little data available, using K-fold validation is a great way
    to reliably evaluate a model.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可用的数据很少时，使用 K 折验证是可靠评估模型的好方法。
- en: When little training data is available, it’s preferable to use a small model
    with few intermediate layers (typically only one or two), in order to avoid severe
    overfitting.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可用的训练数据很少时，最好使用具有少量中间层（通常只有一个或两个）的小型模型，以避免严重的过拟合。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The three most common kinds of machine learning tasks on vector data are binary
    classification, multiclass classification, and scalar regression. Each task uses
    different loss functions:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量数据上最常见的三种机器学习任务是二分类、多类分类和标量回归。每个任务使用不同的损失函数：
- en: '`binary_crossentropy` for binary classification'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_crossentropy` 用于二分类'
- en: '`categorical_crossentropy` for multiclass classification'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_crossentropy` 用于多类分类'
- en: '`mean_squared_error` for scalar regression'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean_squared_error` 用于标量回归'
- en: You’ll usually need to preprocess raw data before feeding it into a neural network.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将原始数据输入神经网络之前，你通常需要预处理原始数据。
- en: When your data has features with different ranges, scale each feature independently
    as part of preprocessing.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的数据具有不同范围的特征时，作为预处理的一部分，独立地对每个特征进行缩放。
- en: As training progresses, neural networks eventually begin to overfit and obtain
    worse results on never-before-seen data.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，神经网络最终开始过拟合，并在从未见过的数据上获得更差的结果。
- en: If you don’t have much training data, use a small model with only one or two
    intermediate layers, to avoid severe overfitting.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有太多训练数据，使用只有一个或两个中间层的较小模型，以避免严重的过拟合。
- en: If your data is divided into many categories, you may cause information bottlenecks
    if you make the intermediate layers too small.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据分为许多类别，如果你使中间层太小，可能会导致信息瓶颈。
- en: When you’re working with little data, K-fold validation can help reliably evaluate
    your model.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你处理少量数据时，K折交叉验证可以帮助你可靠地评估你的模型。
