- en: 6 Advanced feature processing methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 高级特征处理方法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Processing features with more advanced methods
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更高级的方法处理特征
- en: Selecting useful features for lighter, more understandable models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择有用的特征以创建更轻便、更易于理解的模型
- en: Optimizing hyperparameters to make your models shine in performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化超参数以使您的模型在性能上更加出色
- en: Mastering the specific characteristics and options from gradient boosting decision
    trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握梯度提升决策树的具体特性和选项
- en: We’ve now discussed decision trees, their characteristics, their limitations,
    and all their ensemble models, both those based on random resamplings, such as
    random forests, and those based on boosting, such as gradient boosting. Since
    boosting solutions are considered the state of the art in tabular data modeling,
    we have explained how it works and optimized its predictions at length. In particular,
    we have presented a couple of solid gradient boosting implementations, XGBoost
    and LightGBM, that are proving the best solutions available to a data scientist
    dealing with tabular data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经讨论了决策树，它们的特性，它们的局限性，以及所有它们的集成模型，无论是基于随机重采样的，如随机森林，还是基于提升的，如梯度提升。由于提升解决方案被认为是表格数据建模的当前最佳状态，我们详细解释了它是如何工作的以及如何优化其预测。特别是，我们介绍了几种可靠的梯度提升实现，XGBoost和LightGBM，它们正在证明是处理表格数据的科学家所能获得的最佳解决方案。
- en: This chapter will deal with more general topics regarding classical machine
    learning. However, we will focus on gradient boosting decision trees (GBDTs),
    especially XGBoost. In the chapter, we discuss more advanced methods for feature
    processing, such as multivariate imputation of missing values, target encoding
    for reducing high categorical features to simple numeric ones, and a general way
    to figure out how to transform or elaborate your features based on how they relate
    to the target. We will propose a few ways to reduce the number of your features
    to the essential and optimize your hyperparameters depending on the computational
    resources available and the model you have chosen. The chapter will then close
    with a section on only advanced methods and options related to GBDTs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及有关经典机器学习的更一般主题。然而，我们将专注于梯度提升决策树（GBDTs），特别是XGBoost。在本章中，我们将讨论更高级的特征处理方法，例如多元缺失值插补、将高基数分类特征转换为简单数值的目标编码，以及根据它们与目标的关系来确定如何转换或细化特征的一般方法。我们将提出几种方法来减少特征数量到基本要素，并根据可用的计算资源和您选择的模型来优化超参数。然后，本章将以仅与GBDTs相关的先进方法和选项部分结束。
- en: 6.1 Processing features
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 特征处理
- en: There are any number of problems you may face when dealing with real-world tabular
    datasets, and all the methods we’ve discussed so far will produce substandard
    results if you aren’t adjusting your techniques to address the realities of your
    data. Here, we’ll consider a few such problems, such as dealing with missing values
    in the smartest way possible, transforming categorical features with a large number
    of unique values, and finding a way to reprocess your features after you have
    trained your model to squeeze out even more performance. This isn’t an exhaustive
    list, of course, but it should give you some practice in spotting problems and
    planning an appropriate approach.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理现实世界的表格数据集时，您可能会遇到各种问题，如果我们不调整技术以应对数据的现实情况，我们之前讨论的所有方法都将产生不理想的结果。在这里，我们将考虑一些这样的问题，例如以最智能的方式处理缺失值，转换具有大量唯一值的分类特征，以及找到在训练模型后重新处理特征以挤出更多性能的方法。这当然不是详尽的列表，但它应该能帮助您练习发现问题和规划适当的方案。
- en: As in the previous chapter, for our explanations and examples, we will rely
    again on the Airbnb NYC dataset to present practical examples to tackle the most
    challenging task in tabular data problems. The following listing reprises the
    data and some key functions and classes we will use again in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，为了解释和举例，我们再次将依赖于Airbnb纽约市数据集来展示处理表格数据问题中最具挑战性的任务的实用示例。以下列表回顾了我们将再次在本章中使用的数据和一些关键函数和类。
- en: Listing 6.1 Reprising the Airbnb NYC dataset
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 回顾Airbnb纽约市数据集
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① List of excluded columns for feature processing
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ① 特征处理中排除的列列表
- en: ② List of categorical columns with low cardinality to be one-hot encoded
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ② 需要一元编码的低基数分类列列表
- en: ③ List of categorical columns with high cardinality to be ordinally encoded
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 需要顺序编码的高基数分类列列表
- en: ④ List of continuous feature columns
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 列出连续特征列
- en: ⑤ Creates a binary target indicating whether the price is above the mean (unbalanced
    binary target)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个二元目标，指示价格是否高于平均值（不平衡的二元目标）
- en: ⑥ Creates a binary target indicating whether the price is above the median (balanced
    binary target)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 创建一个二元目标，指示价格是否高于中位数（平衡的二元目标）
- en: ⑦ Creates a multiclass target by quantile binning the price into five classes
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 通过将价格分箱为五个类别来创建多类目标
- en: ⑧ Sets the target for regression as the price column
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 将回归的目标设置为价格列
- en: ⑨ Creates a column transformer that applies different transformations to different
    groups of features
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 创建一个列转换器，对不同的特征组应用不同的转换
- en: ⑩ Creates a column transformer suitable for linear models
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 创建一个适合线性模型的列转换器
- en: We refer to the explanations presented in the previous chapter for all the details
    about what the code does. The only addition is a column transformer designed explicitly
    for linear models. This transformer handles just low cardinality categorical features
    by performing one-hot encoding, leaving high cardinality categorical ones apart.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们参考前一章中提供的代码解释的所有细节。唯一的补充是一个专门为线性模型设计的列转换器。这个转换器仅通过执行独热编码处理低基数分类特征，而将高基数分类特征排除在外。
- en: 6.1.1 Multivariate missing data imputation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 多变量缺失数据填充
- en: Having missing data in your tabular dataset is a blocking problem because, apart
    from GBDTs solutions such as XGBoost, LightGBM, and Scikit-learn’s HistGradientBoosting,
    classical machine learning algorithms do not have any native support for missing
    values. In addition, even if your GBDTs algorithm of choice can handle missingness,
    as explained in the next section, you may still find directly imputing the missing
    values more effective because you can check beforehand how each feature or specific
    case is handled.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的表格数据集中存在缺失数据是一个阻碍问题，因为除了GBDTs解决方案，如XGBoost、LightGBM和Scikit-learn的HistGradientBoosting之外，经典的机器学习算法没有对缺失值的原生支持。此外，即使你选择的GBDTs算法可以处理缺失值，正如下一节所解释的，你仍然可能发现直接填充缺失值更有效，因为你可以在事先检查每个特征或特定案例是如何处理的。
- en: In chapter 2, we discussed simple imputation methods, such as using the mean
    or the median, and the usefulness of building missing indicators, thus enabling
    algorithms to spot missing patterns that are present more easily. This section
    will provide more details about these techniques and multivariate imputation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们讨论了简单的填充方法，例如使用平均值或中位数，以及构建缺失指示器的有用性，从而使得算法更容易发现存在的缺失模式。本节将提供更多关于这些技术和多元填充的细节。
- en: First, unless missing cases depend on unobserved variables such as features
    you don’t have access to, missing data can be categorized as
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，除非缺失案例依赖于未观测变量，例如你无法访问的特征，否则缺失数据可以归类为
- en: '*Missing completely at random* (MCAR)—In this scenario, data missingness is
    unrelated to observed and unobserved variables. The missingness occurs randomly
    across the dataset.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完全随机缺失*（MCAR）——在这种情况下，数据缺失与观测到的和未观测到的变量无关。缺失发生在数据集的随机位置。'
- en: '*Missing at random* (MAR)—MAR assumes that observed variables, not the unobserved
    ones, can explain the missingness. In other words, the probability of missingness
    solely depends on the observed data.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机缺失*（MAR）——MAR假设观测变量，而不是未观测变量，可以解释缺失。换句话说，缺失的概率完全取决于观测数据。'
- en: When the missing cases depend on the unobserved values of the missing data itself,
    you fall into the case of missing not at random (MNAR), which requires quite a
    specialized treatment that is not a topic of this book. However, suppose you understand
    the mechanism behind some missing not at random missing data, such as when you
    don’t get information in the census about people who are too rich (because of
    privacy) or too poor (because of a general lack of access). In that case, you
    can try to gather new features that hint at their wealth to add to your dataset
    and fall back to the MAR case.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当缺失案例依赖于缺失数据的未观测值时，你陷入了“缺失非随机”（MNAR）的情况，这需要相当专业的处理，但这本书不是主题。然而，假设你理解某些缺失非随机缺失数据背后的机制，例如当你没有在人口普查中获得关于太富有人（由于隐私）或太穷的人（由于普遍缺乏访问）的信息时。在这种情况下，你可以尝试收集一些暗示他们财富的新特征，以添加到你的数据集中，并回到“随机缺失”（MAR）的情况。
- en: Generally, you often have cases where missing data is MCAR or MAR. In both cases,
    apart from simple imputation using an expected value that works perfectly with
    MCAR, you can better reconstruct your missing data through multivariate imputation.
    *Multivariate imputation* is a method that uses the correlations among predictors
    in a dataset to impute missing values. It involves building a series of models
    to estimate the missing values based on the relationships between variables. In
    this approach, each model treats a feature with missing values as the target variable
    (by modeling only its known values) and uses the remaining features as predictors.
    The resulting model is then used to determine what values to replace the missing
    values in the target. You may set how the algorithm cycles through the features
    to impute. You usually use the default setting, starting from the features with
    less missing data and progressing to those with more missing values, which is
    preferred and most effective.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你经常会遇到缺失数据是MCAR或MAR的情况。在这两种情况下，除了使用与MCAR完美配合的期望值进行简单填充外，你还可以通过多元填充更好地重建缺失数据。*多元填充*是一种使用数据集中预测变量之间的相关性来填充缺失值的方法。它涉及构建一系列模型，根据变量之间的关系来估计缺失值。在此方法中，每个模型将具有缺失值的特征视为目标变量（通过仅对其已知值进行建模），并使用剩余的特征作为预测变量。然后，使用得到的模型来确定用哪些值替换目标中的缺失值。你可以设置算法如何循环特征进行填充。你通常使用默认设置，从缺失数据较少的特征开始，逐步过渡到缺失值较多的特征，这是首选且最有效的方法。
- en: 'To handle missing values in the predictors, an initial imputation step is performed
    using a simple mean or another basic imputation method. Then, through multiple
    iterations, the imputation process refines the initial estimates by incorporating
    the results from the imputing models. This iterative process continues until the
    imputed values reach a state of stability, where further iterations do not lead
    to significant changes. Multivariate imputation is implemented in Scikit-learn
    by `IterativeImputer` ([https://mng.bz/MDZQ](https://mng.bz/MDZQ)). Inspired by
    the R MICE package (Multivariate Imputation by Chained Equations: [https://mng.bz/avEj](https://mng.bz/avEj)),
    it allows both for multivariate imputation and multiple imputations, a common
    approach in statistics and social sciences where, instead of a single imputed
    value, you get a distribution of plausible replacements. Multiple imputations
    are possible with `IterativeImputer` by running it multiple times with the `sample_posterior`
    parameter set to True, using a different random seed each time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理预测变量中的缺失值，首先使用简单均值或其他基本填充方法进行初步填充。然后，通过多次迭代，通过结合填充模型的成果来细化初始估计。这种迭代过程持续进行，直到填充值达到稳定状态，进一步的迭代不会导致显著变化。Scikit-learn通过`IterativeImputer`实现多元填充（[https://mng.bz/MDZQ](https://mng.bz/MDZQ)）。受R
    MICE包（通过链式方程进行多元填充：[https://mng.bz/avEj](https://mng.bz/avEj)）的启发，它允许进行多元填充和多次填充，这是统计学和社会科学中常见的方法，在这种方法中，你得到的不是单个填充值，而是一系列可能的替代值的分布。通过将`sample_posterior`参数设置为True并使用不同的随机种子多次运行`IterativeImputer`，可以实现多次填充。
- en: However, multivariate imputation is the favored choice in data science tabular
    data applications because it allows building models based on single but precise
    estimations. In our example, we take the Airbnb NYC dataset’s continuous features
    and randomly remove 5% of the data, thus mimicking an MCAR situation. Afterward,
    we run a `SimpleImputer`, replacing missing values with a mean and an `IterativeImputer`.
    Finally, we compare, using the mean absolute error (MAE), the features reconstructed
    by each method against the original values.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在数据科学中的表格数据应用中，多元填充是首选的选择，因为它允许基于单一但精确的估计来构建模型。在我们的例子中，我们取Airbnb NYC数据集的连续特征，并随机删除5%的数据，从而模拟MCAR情况。之后，我们运行`SimpleImputer`，用均值和`IterativeImputer`替换缺失值。最后，我们使用平均绝对误差（MAE）比较每种方法重建的特征与原始值。
- en: Listing 6.2 Multivariate imputation
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 多元填充
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Imports IterativeImputer, which is still experimental and under improvement
    in Scikit-learn
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入IterativeImputer，它在Scikit-learn中仍然是实验性的，并且正在改进中
- en: ② Creates a copy of continuous feature data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建连续特征数据的副本
- en: ③ Creates a mask to randomly mark missing values
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个掩码以随机标记缺失值
- en: ④ Uses a SimpleImputer instance with mean imputation strategy
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用具有均值填充策略的SimpleImputer实例
- en: ⑤ Instantiates a RandomForestRegressor for iterative imputation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 实例化RandomForestRegressor进行迭代填充
- en: ⑥ Creates an IterativeImputer instance with max_iter and tol are the stopping
    criteria
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 创建一个具有最大迭代次数 `max_iter` 和容忍度 `tol` 作为停止标准的 `IterativeImputer` 实例
- en: ⑦ Imputes missing data using iterative imputation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 使用迭代插补来插补缺失数据
- en: ⑧ Calculates MAE for imputed data and original data
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 计算插补数据和原始数据的平均绝对误差（MAE）
- en: 'The result provided by the command `print(mae)` is a table that compares the
    simple imputation with the multivariate imputation method:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 命令 `print(mae)` 提供的结果是一个表格，它比较了简单插补和多变量插补方法：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The comparison results demonstrate that the multivariate method, specifically
    the `IterativeImputer`, consistently yields lower MAE values than the simple imputation
    method, even after a single iteration. This indicates that `IterativeImputer`
    is more effective at replacing missing values with fewer errors. To obtain even
    better estimations, you can increase the `max_iter` to a higher number and leave
    the algorithm to decide if to stop earlier based on the tol values, a tolerance
    threshold used to check if the results are stable. Increasing the `max_iter` will
    lead to longer imputation times because, as an imputing model, we are using a
    random forests algorithm. Random forests are usually the most effective way to
    handle multivariate estimations (a method known in the R community as *MissForest*:
    [https://rpubs.com/lmorgan95/MissForest](https://rpubs.com/lmorgan95/MissForest)).
    However, you can choose faster methods based on linear models or k-nearest neighbors
    by simply replacing the `estimator` in the `IterativeImputer`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比较结果表明，多变量方法，特别是 `IterativeImputer`，在单次迭代后始终产生比简单插补方法更低的 MAE 值。这表明 `IterativeImputer`
    在用更少的错误替换缺失值方面更有效。为了获得更好的估计，你可以将 `max_iter` 增加到更高的数值，并让算法根据容忍度值（用于检查结果是否稳定的容忍度阈值）来决定是否提前停止。增加
    `max_iter` 将导致更长的插补时间，因为作为一个插补模型，我们正在使用随机森林算法。随机森林通常是处理多变量估计（在 R 社区中称为 *MissForest*
    的方法）的最有效方式：[https://rpubs.com/lmorgan95/MissForest](https://rpubs.com/lmorgan95/MissForest)。然而，你可以通过简单地替换
    `IterativeImputer` 中的 `estimator` 来选择基于线性模型或 k 近邻的更快方法：
- en: BayesianRidge—Regularized linear regression simply using `BayesianRidge()`
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BayesianRidge—简单地使用 `BayesianRidge()`
- en: RandomForestRegressor—For forests of randomized trees regression, you can set
    `n_estimators`, `max_depth`, and `max_features` to create shallower trees and
    thus accelerate the imputation process such as `RandomForestRegressor(n_estimators=30,`
    `max_depth=6,` `max_samples=0.5)`
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RandomForestRegressor—对于随机树回归的森林，你可以设置 `n_estimators`、`max_depth` 和 `max_features`
    来创建更浅的树，从而加速插补过程，例如 `RandomForestRegressor(n_estimators=30, max_depth=6, max_samples=0.5)`
- en: 'Nystroem + Ridge—A pipeline with the expansion of a degree 2 polynomial kernel
    and regularized linear regression by combining different Scikit-learn commands:
    `make_pipeline(Nystroem(kernel="polynomial",` `degree=2,` `random_state=0),` `Ridge(alpha=1e3))`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nystroem + Ridge—一个通过组合不同的 Scikit-learn 命令（`make_pipeline(Nystroem(kernel="polynomial",
    degree=2, random_state=0), Ridge(alpha=1e3))`）来扩展二次多项式核和正则化线性回归的管道
- en: KNeighborsRegressor—A k-nearest neighbors imputation approach where you decide
    the number of neighbors to consider, such as `KNeighbors-Regressor(n_neighbors=5)`
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNeighborsRegressor—一个 k 近邻插补方法，你可以决定要考虑的邻居数量，例如 `KNeighbors-Regressor(n_neighbors=5)`
- en: The estimator you use will affect the quality of the results you obtain and
    the computation time. As a start, `BayesianRidge` is the default choice and also
    the fastest. If you have more time, `RandomForestRegressor` will provide you with
    better estimates. By jointly inputting multiple variables, `IterativeImputer`
    captures the dependencies between variables more accurately at the price of more
    computations and written code. For a straightforward, out-of-the-box solution,
    some GBDT implementations provide native support for handling missing values,
    which we will discover in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的估计器将影响你获得的结果的质量和计算时间。作为起点，`BayesianRidge` 是默认选择，也是最快的。如果你有更多时间，`RandomForestRegressor`
    将为你提供更好的估计。通过联合输入多个变量，`IterativeImputer` 以更多的计算和编写代码为代价，更准确地捕捉变量之间的依赖关系。对于简单直接、即插即用的解决方案，一些
    GBDT 实现提供了处理缺失值的原生支持，我们将在下一节中了解到这一点。
- en: 6.1.2 Handling missing data with GBDTs
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 使用 GBDT 处理缺失数据
- en: Both XGBoost and LightGBM algorithms (and Scikit-learn’s HistGradientBoosting)
    handle missing values similarly by assigning them to the side that minimizes the
    loss function the most in each split. XGBoost introduced this technique with its
    sparsity-aware split finding algorithm, which provides a default direction to
    use when data is missing, either because it is missing or stored in a sparse matrix
    where only non-zero values are kept.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost和LightGBM算法（以及Scikit-learn的HistGradientBoosting）通过在每个分割点将缺失值分配给最大化损失函数最小值的那个分支来相似地处理缺失值。XGBoost通过其稀疏度感知分割查找算法引入了这项技术，该算法在数据缺失时提供了一个默认的方向，无论是由于缺失还是存储在只保留非零值的稀疏矩阵中。
- en: Consequently, don’t forget that XGBoost will treat the zeros in a sparse matrix
    as missing and apply its specific algorithm to handle missing data. Hence, on
    the one hand, you may find it convenient when analyzing one-hot encoded matrices
    of categorical variables with high cardinality to create them as sparse matrices
    because that will save you a lot of memory and computations. On the other hand,
    you may notice that XGBoost returns completely different models if you analyze
    some data represented as a dense matrix or as a sparse matrix.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，别忘了XGBoost会将稀疏矩阵中的零视为缺失值，并应用其特定的算法来处理缺失数据。因此，一方面，当你分析具有高基数分类变量的one-hot编码矩阵时，将其创建为稀疏矩阵可能会很方便，因为这将节省你大量的内存和计算。另一方面，你可能会注意到，如果你分析的数据以密集矩阵或稀疏矩阵的形式表示，XGBoost返回的模型可能完全不同。
- en: The difference is in what happens when XGBoost encounters a missing example.
    During training, the algorithm learns at each split point whether samples with
    missing values should go to the left or right branching based on the resulting
    gain. When making predictions, samples with missing values are assigned to the
    appropriate child accordingly. This allows the algorithm to split on the feature
    value’s missingness pattern if it is predictive. If there are no missing values
    for a given feature during training, then samples with missing values are assigned
    to whichever child has the most samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 差异在于当XGBoost遇到缺失示例时会发生什么。在训练过程中，算法在每个分割点学习，具有缺失值的样本应该根据结果增益分配到左分支或右分支。在做出预测时，具有缺失值的样本将相应地分配到适当的子节点。这允许算法根据特征值的缺失模式进行分割，如果它是预测性的。如果在训练过程中给定特征没有缺失值，则具有缺失值的样本将被分配到具有最多样本的子节点。
- en: You can use the missing parameter to specify what value XGBoost will have to
    consider as missing. This parameter is set to NaN by default, but you can decide
    on any value you want.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用缺失参数来指定XGBoost将考虑为缺失的值。此参数默认设置为NaN，但你可以选择任何你想要的值。
- en: Another critical thing to remember about XGBoost is that the `gblinear` booster,
    using linear models as base learners, treats missing values as zeros. Suppose
    you standardize your numeric features, as is often used with linear models. In
    that case, the `gblinear` booster will treat missing values as the average value
    for that feature because the average takes the zero value in a standardized variable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于XGBoost的另一个重要事项是，使用线性模型作为基学习者的`gblinear`增强器将缺失值视为零。假设你标准化了你的数值特征，正如在线性模型中常用那样。在这种情况下，`gblinear`增强器将缺失值视为该特征的平均值，因为平均数会取标准化变量中的零值。
- en: 'LightGBM employs a similar approach (see [https://github.com/microsoft/LightGBM/issues/2921](https://github.com/microsoft/LightGBM/issues/2921)),
    using specific parameters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM采用类似的方法（见[https://github.com/microsoft/LightGBM/issues/2921](https://github.com/microsoft/LightGBM/issues/2921)），使用特定的参数：
- en: LightGBM enables the missing value to be handled by default. Turn it off by
    setting `use_missing=false`.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM默认启用处理缺失值的功能。通过设置`use_missing=false`来关闭它。
- en: LightGBM uses NA (NaN) to represent missing values by default. Change it to
    use zero by setting `zero_as_missing=true`.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM默认使用NA（NaN）来表示缺失值。通过设置`zero_as_missing=true`将其更改为使用零。
- en: When `zero_as_missing=false` (default), the unrecorded values in sparse matrices
    (and LightSVM) are treated as zeros.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`zero_as_missing=false`（默认值）时，稀疏矩阵（和LightSVM）中的未记录值被视为零。
- en: When `zero_as_missing=true`, NA and zeros (including unrecorded values in sparse
    matrices [and LightSVM]) are treated as missing.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`zero_as_missing=true`时，NA和零（包括稀疏矩阵[和LightSVM]中的未记录值）被视为缺失。
- en: This strategy for handling missing data works well on average, especially if
    your data is MCAR. This means the pattern of missing instances is completely random
    and unrelated to any other feature or hidden underlying process. The situation
    is different with MAR when missingness is related to other features’ values but
    not to the values of the feature itself and NMAR, where there is a systematic
    pattern of missing values related to the feature itself and other features. In
    MAR and NMAR cases, the best solution is to try to impute these values by other
    means because the XGBoost and LightGBM strategy for missing data may reveal itself
    as underperforming.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '这种处理缺失数据的方法在平均情况下效果很好，特别是如果你的数据是MCAR（完全随机缺失）。这意味着缺失实例的模式是完全随机的，并且与任何其他特征或隐藏的潜在过程无关。当缺失与其他特征值相关，但与特征本身的值无关时，情况就不同了，这就是MAR（相关缺失）。在NMAR（系统缺失）的情况下，存在与特征本身和其他特征相关的缺失值的系统模式。在MAR和NMAR的情况下，最佳解决方案是尝试通过其他方式尝试填充这些值，因为XGBoost和LightGBM的缺失数据策略可能表现出性能不足。 '
- en: There are alternatives to imputing missing data, however. For instance, you
    can create missing data indicators, which are binary features valued in correspondence
    to missing instances in a variable. Missing data indicators can prove quite valuable
    if your data is not completely missing at random, and they can work with any classical
    machine learning algorithm. Another popular solution with decision trees is to
    assign missing values to an extreme value (usually a negative extreme value) not
    used by any variable in the dataset. If you use exact splits, not histogram-based
    ones, where values are reduced into bins, missing data replaced by extreme values
    can prove an efficient and easy solution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于缺失数据的填充，也有一些替代方案。例如，你可以创建缺失数据指示器，这些是二进制特征，其值对应于变量中缺失实例。如果数据不是完全随机缺失，缺失数据指示器可能非常有价值，并且它们可以与任何经典机器学习算法一起工作。另一个与决策树相关的流行解决方案是将缺失值分配给数据集中任何变量都没有使用的极端值（通常是负极端值）。如果你使用的是精确分割，而不是基于直方图的分割，其中值被归入桶中，那么用极端值替换缺失数据可以证明是一个高效且简单的方法。
- en: 6.1.3 Target encoding
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 目标编码
- en: Categorical features, usually represented in a dataset as strings, can be efficiently
    dealt with through different strategies. We already mentioned one-hot-encoding
    in chapter 2 and chapter 4\. As one-hot-encoding, all the other strategies for
    categorical features, whether presenting low or high cardinality, require *encoding*,
    a procedure to transform data numerically into a suitable format for machine learning
    algorithms. Although there are some similarities, encoding is not to be confused
    with *embeddings*, which is a procedure to reduce high-dimensional data, such
    as text or images, into a lower-dimensional space while preserving certain characteristics
    or relationships of the original data. Embeddings are typically learned through
    neural network-based models and are briefly touched on in our book.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征，通常在数据集中以字符串的形式表示，可以通过不同的策略有效地处理。我们已经在第2章和第4章中提到了独热编码。与独热编码一样，所有其他用于分类特征的策略，无论其基数是高还是低，都需要进行*编码*，这是一种将数据数值化并转换为适合机器学习算法的合适格式的程序。尽管有一些相似之处，但编码不应与*嵌入*混淆，嵌入是一种将高维数据（如文本或图像）降低到较低维空间的过程，同时保留原始数据的一些特征或关系。嵌入通常通过基于神经网络的模型学习，并在我们的书中简要介绍。
- en: 'The Scikit learn package offers a couple of encoding solutions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit learn 包提供了一些编码解决方案：
- en: '`OneHotEncoder`—For one-hot encoding (i.e., transforming each unique string
    value into a binary feature), which is the solution we have up so far used'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneHotEncoder`—用于独热编码（即将每个唯一的字符串值转换为二进制特征），这是我们迄今为止使用的解决方案'
- en: '`OrdinalEncoder`—For ordinal encoding (i.e., transforming the string values
    in a feature into ordered numeric ones; there is also `LabelEncoder`, but it works
    the same, and it is mainly for transforming categorical targets)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OrdinalEncoder`—用于顺序编码（即，将特征中的字符串值转换为有序的数值；还有一个`LabelEncoder`，它的工作方式相同，主要用于将分类目标转换为数值）'
- en: Generally, one-hot encoding works fine both for linear models and tree-based
    models, and ordinal encoding works fine for more complex tree-based models, such
    as random forests and GBDTs, because trees can recursively split on the categorical
    feature and finally find a set of partitions useful for predictions. However,
    problems arise with high cardinality categoricals when using one-hot or ordinal
    encoding. High cardinality is a weak point for both linear and tree-based models.
    When one-hot encoded, high cardinality categoricals produce sparse matrices that
    cannot easily be turned into dense ones because of memory limitations. In addition,
    decision trees with many branching levels may need help splitting ordinally encoded
    high cardinality categorical features into meaningful partitions for prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一热编码对于线性模型和基于树的模型都适用，有序编码对于更复杂的基于树的模型，如随机森林和GBDTs，也适用，因为树可以递归地根据类别特征进行分割，并最终找到一组对预测有用的分区。然而，当使用一热编码或有序编码时，高基数类别会引发问题。高基数是线性模型和基于树的模型的弱点。当进行一热编码时，高基数类别会产生稀疏矩阵，由于内存限制，这些矩阵不能轻易转换为密集矩阵。此外，具有许多分支级别的决策树可能需要帮助将有序编码的高基数类别特征分割成对预测有意义的分区。
- en: There is no commonly fixed standard to declare when a categorical is high cardinality
    because that also depends on how many rows your dataset has and how large one-hot
    encoded features your computer memory could handle. However, high cardinality
    categorical features generally include IDs, zip codes, and product or geographical
    names with many unique values. For instance, a reasonable threshold could be over
    512, but it may be lower depending on the dataset. Using the rule of thumb that
    the number of classes in a feature should not exceed 5%–10% of the total rows
    in the dataset, 512 may be too high for smaller datasets. In such circumstances,
    standard practice, especially from data science competitions such as Kaggle’s,
    suggests resorting to *target encoding* (also known as *mean encoding*).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一个普遍固定的标准来声明何时一个类别具有高基数，因为这还取决于你的数据集有多少行以及你的计算机内存可以处理多少个一热编码的特征。然而，高基数类别特征通常包括ID、邮编以及具有许多唯一值的产品或地理名称。例如，一个合理的阈值可能是超过512，但根据数据集可能更低。根据经验法则，一个特征中的类别数量不应超过数据集总行数的5%–10%，对于较小的数据集来说，512可能过高。在这种情况下，标准做法，尤其是来自像Kaggle这样的数据科学竞赛，建议求助于*目标编码*（也称为*均值编码*）。
- en: 'First presented in the paper by Micci-Barreca, “A Preprocessing Scheme for
    High-Cardinality Attributes in Classification and Prediction Problems” (ACM SIGKDD
    Explorations Newsletter 3.1, 2001), target encoding is simply a way to transform
    the values in a categorical feature into their corresponding expected target values.
    If your problem is a regression, target encoding will use the average target value
    corresponding to that value in the dataset, with a classification problem: conditional
    probability or odds ratio. Such a process, bringing about the risk of overfitting
    for the model when the category has few examples in the dataset, is mitigated
    by using a weighted average between the expected value for that category (posterior
    probability of the target) and the average expected value of all the dataset (the
    prior probability of the target over all the training data).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码首次在Micci-Barreca的论文中提出，“用于分类和预测问题中高基数属性的预处理方案”（ACM SIGKDD Explorations Newsletter
    3.1，2001），目标编码简单地将类别特征中的值转换为它们对应的预期目标值。如果你的问题是回归，目标编码将使用与数据集中该值相对应的平均目标值，对于分类问题：条件概率或优势比。这个过程，当数据集中该类别示例较少时，可能会给模型带来过拟合的风险，可以通过使用该类别预期值（目标的后验概率）与所有数据集的平均预期值（所有训练数据中目标的前验概率）之间的加权平均来减轻这种风险。
- en: Target encoding is available in the category-encoders package ([https://mng.bz/gave](https://mng.bz/gave)),
    a Scikit-learn compatible project as the target TargetEncoder class ([https://mng.bz/5glq](https://mng.bz/5glq)),
    and you can install it by a `pip` `install` `category_encoders` command in the
    shell. In the TargetEncoder class, you have to specify a smoothing parameter (to
    be fixed at a value above zero) to balance between the posterior probability of
    the target and the prior probability all over the training data. The best smoothing
    parameter for your data has to be found by experimentation, or you can rely on
    another similar encoder, the James Steiner encoder, which guesses the best way
    to smooth your expected target values based on the variance conditioned by the
    category you want to encode ([https://mng.bz/5glq](https://mng.bz/5glq)). The
    James Stenier encoder makes stronger assumptions about your data. You have to
    decide among different ways to estimate conditional variance by the model parameter
    (for regression problems, it is advisable to use “independent” and for classification
    ones, “binary”). Still, it lifts you from experimenting with different blending
    thresholds as if it were a hyperparameter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码在category-encoders包中可用（[https://mng.bz/gave](https://mng.bz/gave)），这是一个与Scikit-learn兼容的项目，作为目标TargetEncoder类（[https://mng.bz/5glq](https://mng.bz/5glq)），您可以通过在shell中运行`pip
    install category_encoders`命令来安装它。在TargetEncoder类中，您必须指定一个平滑参数（应固定在零以上的值）以在目标的后验概率和整个训练数据中的先验概率之间进行平衡。您数据中最佳的平滑参数必须通过实验来找到，或者您可以依赖另一个类似的编码器，James
    Steiner编码器，它根据您想要编码的类别的条件方差猜测平滑预期目标值的最优方式（[https://mng.bz/5glq](https://mng.bz/5glq)）。James
    Stenier编码器对您的数据做出了更强的假设。您必须决定通过模型参数（对于回归问题，建议使用“独立”，对于分类问题，使用“二元”）来估计条件方差的不同方法。尽管如此，它还是让您免于像超参数一样尝试不同的混合阈值。
- en: In our example, we use the `neighborhood` feature, which has over 200 unique
    values, and the latitude and longitude coordinates after mapping them into a 100
    x 100 grid space. The mapping returns a feature presenting over 2,000 distinct
    values, which makes it a high cardinality categorical feature without any doubt.
    In listing 6.3, we first bin latitude and longitude and then combine them by summing
    them in a way that results in a distinct code for every bin combination of latitude
    and longitude. Binning is obtained by dividing the range between the feature’s
    minimum and maximum value into equal parts. Also, the code snippet performs binning
    on two separate features, resulting in sets of integer values for each feature.
    The values of one feature are multiplied by a power of 10, which is larger than
    the maximum value of the other feature. This ensures that a unique value is always
    obtained when the two sets of values are summed, regardless of the specific values
    being summed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用`neighborhood`特征，它有超过200个唯一值，以及将纬度和经度坐标映射到100 x 100网格空间后的坐标。映射返回一个具有超过2,000个不同值的特征，毫无疑问，它是一个高基数分类特征。在列表6.3中，我们首先对纬度和经度进行分箱，然后通过将它们相加以产生每个纬度和经度分箱组合的唯一代码来组合它们。分箱是通过将特征的最小值和最大值之间的范围分成相等的部分来获得的。此外，代码片段对两个不同的特征执行分箱，为每个特征生成整数值集合。一个特征的值乘以一个大于另一个特征最大值的10的幂，这确保了当将两组值相加时，总是获得一个唯一值，无论相加的具体值是什么。
- en: Listing 6.3 Creating a high cardinality categorical feature
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 创建高基数分类特征
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Function to convert numerical data into categorical bins
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ① 函数将数值数据转换为分类箱
- en: ② Converts latitude and longitude to categorical coordinates
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将纬度和经度转换为分类坐标
- en: ③ Prints the number of unique values in the high-cardinality categorical features
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印高基数分类特征中的唯一值数量
- en: 'The code snippet closes by checking the number of unique values for each feature
    among the high cardinality ones:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段以检查高基数特征中每个特征的唯一值数量结束：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With two categorical features considered high cardinality, we can add to our
    preprocessing pipeline the `TargetEncoder` from category-encoders.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到有两个分类特征被认为是高基数，我们可以在我们的预处理管道中添加category-encoders的`TargetEncoder`。
- en: Listing 6.4 Using target encoding in the pipeline
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 在管道中使用目标编码
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Initializes TargetEncoder for high cardinality categorical features
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为高基数分类特征初始化TargetEncoder
- en: ② Smoothes value to blend prior and posterior probabilities
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ② 平滑值以融合先验和后验概率
- en: ③ Initializes XGBoost classifier with specific hyperparameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用特定超参数初始化XGBoost分类器
- en: ④ Defines ColumnTransformer for preprocessing features with TargetEncoder for
    high cardinality categorical features
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义ColumnTransformer以使用TargetEncoder对高基数分类特征进行预处理
- en: ⑤ Creates a pipeline that combines preprocessing and modeling
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个结合预处理和建模的管道
- en: ⑥ Performs five-fold cross-validation and obtaining evaluation metrics
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 执行五折交叉验证并获取评估指标
- en: ⑦ Prints mean accuracy, fit time, and prediction time from cross-validation
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 从交叉验证中打印出平均准确率、拟合时间和预测时间
- en: 'When executed, the code procedures the results for running XGBoost on the problem
    with the extra help of the high cardinality categorical features. The results
    point to a slight improvement in the accuracy. Later in this chapter, we will
    investigate the weight of the contribution by target encoding when examining the
    explainability:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，代码过程将运行XGBoost的结果，并额外帮助处理高基数分类特征的问题。结果指向准确率的一点点提升。在本章的后面部分，我们将调查在检查可解释性时目标编码贡献的权重：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although target encoding is a convenient procedure because it may quickly transform
    any categorical into a numeric feature, in doing so, you have to pay attention
    to keeping all important information from your data. Target encoding renders further
    modeling of any interaction between features impossible. Let’s say, for instance,
    that you are working on an advertising response dataset with click results for
    many websites and advertising formats. If you encode both features, having transformed
    two high cardinality categoricals with potentially thousands and thousands of
    values, you may easily create any kind of classical model. However, after encoding,
    your model, whether linear or tree-based, won’t be able to grasp any possible
    interaction between the encoded features. In this case, the solution is to create
    a new feature beforehand, combining the two high cardinality categorical features
    and then target encode their combination.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目标编码是一个方便的程序，因为它可以快速将任何分类特征转换为数值特征，但在这样做的时候，你必须注意保留数据中的所有重要信息。目标编码使得对特征之间任何交互的进一步建模成为不可能。比如说，如果你正在处理一个广告响应数据集，其中包含许多网站和广告格式的点击结果。如果你对两个特征进行编码，将两个可能具有数千个值的高的基数分类特征转换，你可能会轻松地创建任何类型的经典模型。然而，在编码之后，你的模型，无论是线性的还是基于树的，将无法理解编码特征之间任何可能的交互。在这种情况下，解决方案是在事先创建一个新特征，结合这两个高基数分类特征，然后对它们的组合进行目标编码。
- en: 'Hence, as for other tools, we should consider the pros and cons of this advanced
    encoding technicality. In our experience, depending on the situation, before resorting
    to target encoding, there are a few options for classic machine learning algorithms
    and for gradient boosting for dealing with high cardinality categorical features:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于其他工具，我们也应该考虑这种高级编码技术的利弊。根据我们的经验，在求助于目标编码之前，对于经典机器学习算法和梯度提升算法处理高基数分类特征，有几个选项：
- en: Just dropping problematic categorical features
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接删除有问题的分类特征
- en: Using a OneHotEncoder
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OneHotEncoder
- en: Using an OrdinalEncoder and treating categories as ordered equidistant quantities
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OrdinalEncoder并将类别视为有序等距的数量
- en: Using an OrdinalEncoder and relying on the native category support of gradient
    boosting histogram-based algorithms
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OrdinalEncoder并依赖梯度提升直方图算法的本地类别支持
- en: Using target encoding as a last resort
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为最后的手段使用目标编码
- en: Dropping features is only sometimes considered. However, we already mentioned
    in chapter 2 how you can evaluate how a nominal feature can contribute to predicting
    a target utilizing Cramer’s V measure of association.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 删除特征只被考虑在少数情况下。然而，我们在第二章中已经提到，你可以如何利用Cramer的V相关度度量来评估一个名义特征如何有助于预测目标。
- en: When confronted with a high cardinality categorical feature, opting for one-hot
    encoding is almost necessary for linear models. When dealing with other models,
    such as decision trees and their ensembles, there might be a more suitable approach.
    This is because one-hot encoding creates an additional feature for each category
    value of the categorical feature. This leads to an increased number of split points
    that the tree-based model must consider during fitting. Consequently, using one-hot
    encoded data requires more depth in a decision tree to achieve an equivalent split
    that could be achieved with a single split point using a different way of handling
    the categorical feature.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对高基数分类特征时，对于线性模型来说，选择one-hot编码几乎是必要的。当处理其他模型，如决策树及其集成时，可能存在更合适的方案。这是因为one-hot编码为分类特征的每个分类值创建了一个额外的特征。这导致树模型在拟合过程中必须考虑的分割点数量增加。因此，使用one-hot编码的数据需要在决策树中具有更多的深度，以实现等效的分割，这可以通过使用处理分类特征的不同方式通过单个分割点实现。
- en: For an ordinal encoder, the categories are encoded as 0, 1, 2, and so on, treating
    them as continuous features. While this approach can be misleading for linear
    models, it works effectively for decision trees. Decision trees can accurately
    split the data based on ordinal encoding, separating the categories according
    to their relationship with the target variable. This happens with XGBoost, which
    treats all the features as numerical, continuous features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有序编码器，分类被编码为0，1，2等等，将它们视为连续特征。虽然这种方法对于线性模型来说可能会误导，但它对于决策树来说非常有效。决策树可以根据目标变量的关系准确地对数据进行分割，根据分类分离。这在XGBoost中发生，它将所有特征视为数值，连续特征。
- en: If we decide to use the native support for categorical features, this option
    is available in LightGBM and in the version of XGBoost provided by the H2O.ai
    library ([https://mng.bz/6e75](https://mng.bz/6e75)). Native categorical support
    allows these models to handle categorical features more effectively, without converting
    them into numerical values. In that case, since native handling requires sorting
    categories, we expect the algorithm to be slightly slower when using native handling
    of categorical features with respect to treating categories as ordinal numbers.
    In the native category support, the sorting of the categories of a feature is
    based on the associated target variance for each category. Once the sorting has
    happened, the feature can be used as a continuous numerical attribute.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定使用对分类特征的本地支持，这个选项在LightGBM以及由H2O.ai库提供的XGBoost版本中都是可用的（[https://mng.bz/6e75](https://mng.bz/6e75)）。本地分类支持使得这些模型能够更有效地处理分类特征，而无需将它们转换为数值。在这种情况下，由于本地处理需要排序分类，我们预计在使用本地处理分类特征相对于将分类视为有序数时，算法会稍微慢一些。在本地分类支持中，一个特征的分类排序是基于每个分类的关联目标方差。一旦排序完成，该特征就可以用作连续的数值属性。
- en: 6.1.4 Transforming numerical data
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 转换数值数据
- en: Decision trees can automatically handle nonlinearities and interactions in data.
    This is because they can split a variable at any point into two parts and then
    further split them repeatedly. This property comes in particularly handy for handling
    subtle and deep interactions in the data, with a caveat, because decision trees
    are quite rough approximators. Under the aspect of precisely modeling complex
    relationships in data, neural networks with enough examples are better approximators.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以自动处理数据中的非线性性和交互作用。这是因为它们可以在任何点上将变量分割成两部分，然后反复进一步分割。这种特性在处理数据中的微妙和深层交互时特别有用，但有一个前提，因为决策树是相当粗糙的近似器。从精确建模数据中的复杂关系的角度来看，具有足够示例的神经网络是更好的近似器。
- en: Figure 6.1 shows how a bagged ensemble of decision trees can approximate a nonlinear
    function. The result is an approximation constructed by a set of if-then-else
    decision rules that recursively divide the space. However, noise in the data can
    result in inaccuracies in certain parts of the space. In contrast, a neural network
    with the same number of nodes as the trees used in the bagged decision trees can
    provide a smoother and more accurate curve estimation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1展示了如何通过决策树集成来近似非线性函数。结果是通过对空间进行递归分割的一系列if-then-else决策规则构建的近似。然而，数据中的噪声可能导致空间某些部分的不准确。相比之下，具有与袋装决策树中使用的树相同数量的节点的神经网络可以提供更平滑、更准确的曲线估计。
- en: '![](../Images/CH06_F01_Ryan2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F01_Ryan2.png)'
- en: Figure 6.1 Comparison of predictions between a neural network and a bagged trees
    ensemble for a random dataset with a noisy sine function
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 随机数据集上神经网络与袋装树集成预测的比较，数据集包含有噪声的正弦函数
- en: Since GBDT is also based on decision trees, it may similarly struggle in shaping
    nonlinear functions using binary splits. Consequently, when using GBDT, and you
    know specific nonlinearities or interactions, it benefits you to explicitly define
    them by using transformations toward linear forms, binning or discretization,
    and precomputed interactions between features. For nonlinearities, transformations
    help reduce the number of splits. In addition, computing specific interactions
    beforehand also reduces the number of splits, which occur at better split points.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GBDT（梯度提升决策树）也是基于决策树的，它可能在用二分分裂来塑造非线性函数时遇到类似的困难。因此，当使用GBDT，并且你知道特定的非线性或交互时，通过使用向线性形式转换、分箱或离散化以及特征之间的预计算交互来明确地定义它们，对你是有益的。对于非线性性，转换有助于减少分裂的数量。此外，事先计算特定的交互也可以减少在更好的分裂点发生的分裂数量。
- en: However, before applying such transformations, you need to understand your data.
    Linearities and nonlinearities, even if there is no relationship with the target,
    can be easily spotted after you complete fitting your training data by a partial
    dependence plot (PDP). This model-agnostic chart technique explains how features
    and targets are related through the model you have trained.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在应用这些转换之前，你需要了解你的数据。线性性和非线性性，即使与目标没有关系，也可以在通过部分依赖图（PDP）完成训练数据的拟合后轻松发现。这种模型无关的图表技术解释了特征和目标是如何通过训练的模型相互关联的。
- en: PDPs display how the target output changes based on specific input features
    while ignoring the effects of other input features. In other words, it shows us
    the average expected prediction if we set a certain value on all the data points
    of the specific input feature we are examining. The assumption underlying the
    analysis is that the input we represent by a PDP is independent of other features.
    Under such conditions, the PDP represents how the input feature directly affects
    the target. However, this assumption is often violated in practice, meaning that
    the input feature we are examining is usually not completely independent of the
    other features. As a result, the plot typically shows how the target value changes
    as we vary the value of the input feature, while also reflecting the average effects
    of the other features in the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs（部分依赖图）显示了目标输出如何根据特定的输入特征变化，同时忽略其他输入特征的影响。换句话说，它展示了如果我们为特定输入特征的每个数据点设置一个特定的值，我们将得到的平均预期预测。分析背后的假设是，我们通过PDP表示的输入与其他特征是独立的。在这种条件下，PDP表示输入特征如何直接影响目标。然而，在实践中，这个假设通常被违反，这意味着我们正在检查的输入特征通常并不完全独立于其他特征。因此，图表通常显示了目标值如何随着输入特征值的改变而变化，同时也反映了模型中其他特征的总体影响。
- en: In listing 6.5, we explore PDPs’ possible usage and limitations. Given an XGBoost
    model trained on our Airbnb NYC dataset, we demonstrate how our target changes
    regarding our numeric features, trying to spot any nonlinearities or other characteristics
    of the modeled data. The four resulting charts are plotted using matplotlib axes
    and are to be analyzed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.5中，我们探讨了PDPs的可能用途和局限性。针对我们在Airbnb NYC数据集上训练的XGBoost模型，我们展示了我们的目标如何根据我们的数值特征变化，试图发现任何非线性或其他模型数据的特征。四个生成的图表使用matplotlib轴绘制，并进行分析。
- en: Listing 6.5 Partial dependence plot
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 部分依赖图
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Creates a model pipeline combining data processing and XGBoost classifier
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个结合数据处理和XGBoost分类器的模型管道
- en: ② Creates a 2 × 2 subplot layout
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个2×2的子图布局
- en: ③ Creates a partial dependence plot of the average effect
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建平均效应的部分依赖图
- en: ④ A list specifying features for the plot
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 指定用于图表的特征列表
- en: ⑤ Adds a red dashed line at y=0.5 on each subplot, a reference line for interpretation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 在每个子图上添加一条y=0.5的红色虚线，作为解释的参考线
- en: Figure 6.2 shows the four charts. The dashed line marks the classification threshold
    for one (above or equal to 0.5) and zero (below 0.5). The solid line describes
    the relationship between the feature value on the x-axis and the target probability
    on the y-axis. The tick marks on the x-axes point out the distribution deciles
    of the feature, hinting at ranges denser (where the ticks are next to each other)
    with values and at those sparser (where the ticks are farther from each other).
    Ranges sparser with values are less reliable in their estimates. For instance,
    `minimum_nights` and `calculated_host_listings_count` display a nonlinear pattern,
    whereas `number_of_reviews` and `availability_365` oscillate stationary.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2显示了四个图表。虚线标记了一个（大于或等于0.5）和零（小于0.5）的分类阈值。实线描述了x轴上的特征值与y轴上的目标概率之间的关系。x轴上的刻度标记指出特征的分布十分位数，暗示着值密集（刻度彼此相邻）的区域和值稀疏（刻度彼此远离）的区域。值稀疏的区域在估计上不太可靠。例如，`minimum_nights`和`calculated_host_listings_count`显示非线性模式，而`number_of_reviews`和`availability_365`则表现出稳定的振荡。
- en: '![](../Images/CH06_F02_Ryan2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F02_Ryan2.png)'
- en: Figure 6.2 A panel of PDPs for numeric features
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 数字特征的PDP面板
- en: Given such results, you may evaluate to try to transform `minimum_nights` and
    `calculated_host_listings_count` using transformative functions by trial and error,
    such as
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在得到这样的结果后，你可以尝试通过试错法使用变换函数来评估`minimum_nights`和`calculated_host_listings_count`，例如
- en: Square or cubic transformations
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方或立方变换
- en: Square root or cube root
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方根或立方根
- en: Log or exponent transformations
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数或指数变换
- en: Tangent, sine, and cosine transformations
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切线、正弦和余弦变换
- en: Inverse, squared inverse, cubed inverse, square root inverse, cube root inverse
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆变换、平方逆变换、立方逆变换、平方根逆变换、立方根逆变换
- en: Log inverse, exponent inverse, tangent inverse, sine inverse, cosine inverse
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数逆变换、指数逆变换、切线逆变换、正弦逆变换、余弦逆变换
- en: However, before rushing to test transformation, it is important to verify if
    the obtained PDP average curve represents that feature’s behavior under all circumstances.
    You can verify that using individual conditional expectation (ICE) plots. ICE
    plots are the single components of the PDP curve. You can obtain ICE plots with
    a slight change in the previous code.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在匆忙进行变换测试之前，重要的是要验证获得的PDP平均曲线是否代表该特征在所有情况下的行为。你可以使用个体条件期望（ICE）图来验证这一点。ICE图是PDP曲线的单个组成部分。你可以通过稍微修改之前的代码来获得ICE图。
- en: Listing 6.6 ICE plots
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 ICE图
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Creates a partial dependence plot showing both individual and average effects
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个部分依赖图，显示个体和平均效应
- en: ② Uses a random subset of 30% of data for plotting efficiency
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用30%的数据随机子集进行绘图以提高效率
- en: After running the code, you may examine the results, as seen in figure 6.3\.
    You can see the same PDP average curve as before, represented with a dashed lighter
    line, and a sample of 30 curves taken randomly from the sample. Suppose you can
    verify that the sampled curves are being clustered together, approximately reproducing
    the shape of the average curve. In that case, you have a confirmation that the
    average PDP curve is representative of the behavior of the feature with respect
    to the target. Otherwise, as in our example, if the single curve appears different
    and dispersed, the other features somehow mediate the feature’s relationship because
    of collinearity or interactions, and you cannot benefit much from transforming
    the feature.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你可以检查结果，如图6.3所示。你可以看到与之前相同的PDP平均曲线，用虚线表示，以及从样本中随机抽取的30条曲线。假设你可以验证这些抽样曲线是聚集在一起的，大致复制了平均曲线的形状。在这种情况下，你可以确认平均PDP曲线代表了特征相对于目标的行为。否则，就像我们的例子一样，如果单条曲线看起来不同且分散，其他特征由于共线性或交互作用而在某种程度上调节了特征与特征的关系，你无法从变换特征中获得太多好处。
- en: '![](../Images/CH06_F03_Ryan2.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F03_Ryan2.png)'
- en: Figure 6.3 A panel of ICE plots for numeric features
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 数字特征的ICE图面板
- en: Up to now, we have just used PDP for numeric features. Still, you can also apply
    them to binary and categorical features after encoding them by one-hot encoding.
    In this case, you have first to compute the curve value by the stand-alone function
    `partial_dependence` and afterward represent the obtained values as bars (for
    PDP average curves) or boxplots (for PDP and ICE curves together). In the following
    listing, we extract the necessary values and create a box plot representation
    for the single levels of the `neighbourhood_group`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是使用了PDP（部分依赖图）来处理数值特征。然而，在通过独热编码进行编码后，你也可以将它们应用于二进制和分类特征。在这种情况下，你首先必须使用独立的函数`partial_dependence`计算曲线值，然后以条形图（用于PDP平均曲线）或箱线图（用于PDP和ICE曲线一起）的形式表示获得的价值。在以下列表中，我们提取了必要的值，并为`neighbourhood_group`的单个级别创建了一个箱线图表示。
- en: Listing 6.7 Partial dependence plot for binary features
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 二进制特征的PDP图
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Imports the partial_dependence function that computes the curve values
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入计算曲线值的部分依赖函数
- en: ② Creates a box plot of individual ICE curves
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建单个ICE曲线的箱线图
- en: ③ Adds a red dashed line at y = 0.5 on each subplot, a reference line for interpretation
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在每个子图中添加一条红色虚线，y = 0.5，作为解释的参考线
- en: Figure 6.4 shows the result, providing insights on how a flat’s location in
    Manhattan is usually associated with higher prices. The other locations are associated
    with lower prices, according to the model. However, Brooklyn shows the largest
    variability, with sometimes higher prices similar to Manhattan, clearly depending
    on other factors related to the exact position or characteristics of the flat.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4显示了结果，提供了关于曼哈顿公寓位置通常与更高价格相关联的见解。根据模型，其他位置与较低价格相关联。然而，布鲁克林显示出最大的变异性，有时价格较高，类似于曼哈顿，这显然取决于与公寓的确切位置或特征相关的其他因素。
- en: '![](../Images/CH06_F04_Ryan2.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Ryan2.png)'
- en: Figure 6.4 For each binary feature, the boxplot associated target values obtained
    by PDP
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 对于每个二进制特征，PDP获得的相关目标值的箱线图
- en: As with numeric features, PDP curves also provide useful insight on how to power
    up your model. For instance, they can be used to aggregate the levels of a categorical
    feature that behave the same—in our example, Bronx, Staten Island, and probably
    also Queens.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与数值特征一样，PDP曲线也提供了关于如何增强你的模型的有用见解。例如，它们可以用来聚合表现相同的分类特征的级别——在我们的例子中，布朗克斯、斯塔滕岛，也许还有皇后区。
- en: PDPs show us what we can expect from the target output based on the input features
    we’re interested in. They also help us understand the relationship between the
    target response and the input feature of interest, whether linear or nonlinear.
    By observing the shape of the curve drawn by the analysis, we can also figure
    out what transformation could linearize it. When providing the `features` parameter
    of the `PartialDependenceDisplay` function with tuples of features, the function
    will output a contour map showing the conjoint effects of two specific features.
    Discovering interactions this way is long and tedious, especially if you have
    many features to explore. A solution would be to automatically discover the potential
    interactions and then test them with the PDP conjoint chart. Detecting interactions
    automatically using XGBoost is straightforward by using a project such as XGBoost
    Feature Interactions Reshaped (XGBFIR; [https://github.com/limexp/xgbfir](https://github.com/limexp/xgbfir)).
    The following listing shows an example you can run after installing the package
    by `pip` `install` `xgbfir` on a command shell.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: PDP（部分依赖图）向我们展示了基于我们感兴趣的输入特征我们可以期待的目标输出。它们还帮助我们理解目标响应与感兴趣输入特征之间的关系，无论是线性的还是非线性的。通过观察分析绘制的曲线形状，我们还可以找出可以使其线性化的转换。当将特征元组作为`PartialDependenceDisplay`函数的`features`参数提供时，该函数将输出一个等高线图，显示两个特定特征的联合效应。以这种方式发现交互是漫长而繁琐的，尤其是如果你有很多特征要探索。一个解决方案是自动发现潜在的交互，然后使用PDP联合图表进行测试。通过使用XGBoost
    Feature Interactions Reshaped（XGBFIR；[https://github.com/limexp/xgbfir](https://github.com/limexp/xgbfir)）这样的项目，自动检测交互是直截了当的。以下列表显示了在命令行中通过`pip
    install xgbfir`安装包后可以运行的示例。
- en: Listing 6.8 Discovering interactions by XGBFIR
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 通过XGBFIR发现交互
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Generates a report with xgbfir and saves it to an Excel file
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用xgbfir生成报告并将其保存到Excel文件中
- en: ② Reads the Excel file created in the previous steps
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ② 读取之前步骤中创建的Excel文件
- en: ③ Extracts and sorts based on split gain the “Interaction” and “Gain” columns
    from the feature interaction report
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从特征交互报告中提取并按分裂增益排序“交互”和“增益”列
- en: ④ Generates a partial dependence plot for the features “minimum_nights” and
    “calculated_host_listings_count”
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为“minimum_nights”和“calculated_host_listings_count”这两个特征生成部分依赖图
- en: The code will print a series of interactions. If you work with a linear model,
    each interaction returned by XGBFIR should be tested because they could enhance
    your model’s performance. If you work with decision trees, you can ignore the
    ones that involve a binary feature and concentrate on only numeric ones. An example
    is the interaction between `minimum_nights` and `calculated_host_listings_count`.
    Figure 6.5 shows how combining them with specific values is strongly associated
    with a positive target response.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将打印一系列交互。如果你使用线性模型，应该测试XGBFIR返回的每个交互，因为它们可能会提高你的模型性能。如果你使用决策树，可以忽略涉及二进制特征的交互，只关注数值特征。一个例子是`minimum_nights`和`calculated_host_listings_count`之间的交互。图6.5显示了将它们与特定值结合如何与正目标响应强烈相关。
- en: '![](../Images/CH06_F05_Ryan2.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F05_Ryan2.png)'
- en: Figure 6.5 Conjoint PDP for two numeric features
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 两个数值特征的联合部分依赖图
- en: In such cases, combining the numeric features by multiplying them will optimize
    your GDBT model much faster and more effectively.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，通过乘法组合数值特征将优化你的GDBT模型的速度更快、更有效。
- en: 6.2 Selecting features
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 选择特征
- en: Feature selection is not always necessary. Still, when it is, it plays a vital
    role in identifying the most valuable features for training among the existing
    set of features, whether they derive directly from the data extraction or are
    the product of your feature engineering work. By employing effective feature selection
    techniques, you can pinpoint and retain the most relevant features that contribute
    significantly to the machine learning process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择并不总是必要的。尽管如此，当需要时，它在识别现有特征集中对训练最有价值的特征方面发挥着至关重要的作用，无论这些特征是否直接来自数据提取，还是你特征工程工作的产物。通过采用有效的特征选择技术，你可以精确地识别并保留对机器学习过程贡献显著的最重要的特征。
- en: In chapter 2, section 2.2.3, we discussed avoiding collecting irrelevant and
    redundant features for the tasks based on your knowledge of the problem and exploratory
    data analysis. In the subsequent chapters, we discussed machine learning algorithms
    that handle irrelevant and redundant features.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章的第2.2.3节中，我们讨论了根据你对问题的了解和探索性数据分析来避免收集无关和冗余特征。在随后的章节中，我们讨论了处理无关和冗余特征的机器学习算法。
- en: In classic machine learning, we have a large set of algorithms, including the
    family of linear models, that are particularly susceptible to irrelevant and redundant
    features, reducing performance and accuracy. Uninformative and noisy features,
    deemed irrelevant because they lack a meaningful association with the target of
    the learning task, can pose significant challenges for linear models. This is
    due to the possibility of random alignment between the feature values and the
    target, which can mislead the algorithm and assign undue importance to these features.
    Linear models utilize all the features provided, making them particularly vulnerable
    since the more noisy features there are, the more the results will be degraded.
    Ensembles based on decision trees are instead less affected by irrelevant and
    redundant features because they automatically select what features to use and
    ignore. This also happens with deep learning. However, deep learning may not be
    as robust as decision tree ensemble methods when dealing with noisy or irrelevant
    features on tabular data. For optimal performance under such conditions, large
    amounts of data are required, as well as careful choice of architecture, such
    as using dropout, regularization, or batch normalization layers, and tuning of
    learning rates.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典机器学习中，我们有一大批算法，包括线性模型系列，它们特别容易受到无关和冗余特征的影响，这会降低性能和准确性。被认为无关的信息和无用的噪声特征，因为它们与学习任务的靶子缺乏有意义的关联，可能会给线性模型带来重大挑战。这是由于特征值与靶子之间可能存在随机对齐的可能性，这可能会误导算法并赋予这些特征过多的重视。线性模型利用所有提供的特征，这使得它们特别容易受到噪声特征的影响，因为噪声特征越多，结果就会越差。基于决策树的集成方法则较少受到无关和冗余特征的影响，因为它们会自动选择使用哪些特征并忽略其他特征。这种情况也适用于深度学习。然而，当处理表格数据中的噪声或不相关特征时，深度学习可能不如决策树集成方法鲁棒。在这种情况下，为了获得最佳性能，需要大量数据，以及仔细选择架构，例如使用dropout、正则化或批量归一化层，以及调整学习率。
- en: Feature selection benefits classic machine learning algorithms such as linear
    models. However, it is also valuable in the case of decision tree-based ensembles
    and deep learning architectures, and it should not be ignored that it makes a
    machine learning process faster because of fewer columns to handle. By selecting
    the features before training, these complex algorithms can achieve improved clarity
    and ease of explanation by distilling the most relevant and informative features
    and enabling a clearer understanding of the underlying patterns and relationships
    captured by the models. This simplification enhances interpretability and facilitates
    the communication of the algorithm’s decision-making process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择对经典机器学习算法，如线性模型，有益。然而，在基于决策树集成和深度学习架构的情况下，它同样有价值，不应忽视的是，由于处理列数减少，它使得机器学习过程更快。通过在训练前选择特征，这些复杂算法可以通过提炼最相关和最有信息量的特征，并使对模型所捕获的潜在模式和关系的理解更加清晰，从而实现提高清晰度和易于解释。这种简化增强了可解释性，并促进了算法决策过程的沟通。
- en: In the following sections, we discuss and test a few solutions that can work
    well, stand-alone or sequentially, to select only essential features for solving
    your tabular data problem with the best results. We discuss algorithms for figuring
    out both relevant features (the all-relevant set), which may lead to redundant
    but useful sets of features, from algorithms to select minimal subsets of features
    (the nonredundant set) that produce models comparable to the set of relevant features
    but with the added advantage of increased interpretability due to a fewer number
    of features.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们讨论和测试了一些解决方案，这些解决方案可以独立或顺序使用，以选择仅对解决您的表格数据问题至关重要的特征，并取得最佳结果。我们讨论了确定相关特征（所有相关集）的算法，这些特征可能导致冗余但有用的特征集，以及选择特征最小子集（非冗余集）的算法，这些子集产生的模型与相关特征集相当，但增加了由于特征数量减少而提高的可解释性优势。
- en: 6.2.1 Stability selection for linear models
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 线性模型的稳定性选择
- en: '*Stability selection* is based on the idea that if you use a variable selection
    procedure, you won’t always get the same results if you subsample or bootstrap
    your data because of variability in the process itself. For instance, if you use
    L1 regularization for feature selection in a linear model, you may find that different
    samples may return different non-zero coefficients, especially for highly correlated
    features.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*稳定性选择*基于这样的观点：如果你使用变量选择程序，由于过程本身的变异性，你不会总是得到相同的结果，因为子采样或自助法可能会改变数据。例如，如果你在线性模型中使用L1正则化进行特征选择，你可能会发现不同的样本可能会返回不同的非零系数，尤其是对于高度相关的特征。'
- en: As we have discussed, the L1 regularization penalty results in sparsity in the
    coefficient estimates. It works by adding a penalty term to the loss function,
    the sum of the absolute values of the coefficients. Such a penalty term imposes
    a constraint on the sum of the absolute magnitudes of the coefficients, promoting
    some coefficients to become exactly zero. Consequently, the L1 regularization
    can effectively select features by shrinking some coefficients to zero and excluding
    the corresponding features from the model. In the presence of highly correlated
    features, the L1 regularization may face difficulty selecting a unique set of
    features due to their similarity in their contributions to the target variable.
    Here, chance plays a role in the fact that certain features get non-zero coefficients
    depending on what data you have in your sample. However, this can be used to our
    advantage.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，L1正则化惩罚导致系数估计的稀疏性。它是通过向损失函数添加一个惩罚项来实现的，即系数绝对值的总和。这样的惩罚项对系数绝对值的总和施加了约束，促使某些系数变为正好为零。因此，L1正则化可以通过将某些系数缩小到零并排除相应的特征来有效地选择特征。在高度相关的特征存在的情况下，由于它们对目标变量的贡献相似，L1正则化可能难以选择一组独特的特征。在这里，机会在某种程度上起着作用，即某些特征根据你在样本中的数据得到非零系数。然而，这可以成为我们的优势。
- en: By introducing randomness through data sampling, stability selection aims to
    identify features that consistently appear important across multiple subsets,
    indicating their robustness and reducing the likelihood of selecting features
    by chance or noise. Stability selection will provide a useful set of features,
    not a minimal one. By ruling out unimportant features, stability selection ensures
    that all the relevant features are identified, thus making it a perfect algorithm
    for the first step in reducing the number of your features.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据采样引入随机性，稳定性选择旨在识别在多个子集中始终出现的重要特征，这表明它们的鲁棒性，并减少随机或噪声选择特征的可能性。稳定性选择将提供一组有用的特征，而不是最小的一组。通过排除不重要的特征，稳定性选择确保识别出所有相关特征，因此它是一个完美的算法，用于减少特征数量的第一步。
- en: Presented in the paper by Meinshausen and Büehlmann ([https://arxiv.org/abs/0809.2932](https://arxiv.org/abs/0809.2932)),
    for some time, stability selection has been offered as part of Scikit-learn and
    then maintained among the Scikit-learn compatible projects. We can easily replicate
    its procedures using Scikit-learn’s `BaggingClassifier` with `LogisticRegression`
    with L1 regularization for a classification problem. You can also adopt the same
    code for regression problems, using `BaggingRegressor` with the L1 regression
    class, `Lasso`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如Meinshausen和Büehlmann在论文中提出（[https://arxiv.org/abs/0809.2932](https://arxiv.org/abs/0809.2932)），稳定性选择在一段时间内已被作为Scikit-learn的一部分提供，并在Scikit-learn兼容的项目中维护。我们可以使用Scikit-learn的`BaggingClassifier`和具有L1正则化的`LogisticRegression`来轻松复制其过程，以解决分类问题。您还可以为回归问题采用相同的代码，使用`BaggingRegressor`和L1回归类`Lasso`。
- en: In our implementation, we test a series of C values for the L1 logistic regression
    against bootstrap resamples. The procedure creates a series of logistic regression
    coefficients that we can sum, average, or count how many times they differ from
    zero. Given that we are using a mix of binary and continuous features, we find
    it more useful to count the times the coefficient associated with a variable has
    an absolute value above a threshold. Thus, we can finally deem all the features
    that, most of the time, tend to have a relevant coefficient as relevant, which
    can affect the resulting prediction.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们对L1逻辑回归进行了一系列C值的测试，以对抗bootstrap重采样。该过程创建了一系列逻辑回归系数，我们可以对它们求和、平均或计算它们与零不同的次数。鉴于我们使用的是二进制和连续特征的混合，我们发现计算与变量相关的系数的绝对值超过阈值的次数更有用。因此，我们可以最终认为，大多数情况下，倾向于具有相关系数的特征是相关的，这可能会影响最终的预测。
- en: Listing 6.9 Stability selection
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9稳定性选择
- en: '[PRE11]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Generates a grid of lambda values using a logarithmic scale for use in L1
    regularization
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用对数尺度生成一个lambda值的网格，用于L1正则化
- en: ② Creates a Logistic Regression estimator with L1 (Lasso) penalty
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有L1（Lasso）惩罚的逻辑回归估计器
- en: ③ Creates a BaggingClassifier that uses the Logistic Regression estimator as
    its base model
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个使用逻辑回归估计器作为其基础模型的BaggingClassifier
- en: ④ Standardizes after data processing renders all the coefficients comparable,
    no matter the scale
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 数据处理标准化后，所有系数均可比较，无论其规模如何
- en: ⑤ Sets a small value as epsilon for a threshold
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将一个小的值作为epsilon设置为一个阈值
- en: ⑥ Sets a threshold value for selecting significant coefficients
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 设置一个阈值值以选择显著的系数
- en: 'The output highlights both the distributions of relevant coefficients and the
    selected features:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出突出了相关系数的分布和选定的特征：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Stability selection offers several advantages. It can handle high-dimensional
    data, avoid overfitting by incorporating randomness, and provide a measure of
    feature importance that considers the selection process’s stability. It is commonly
    used in applications with large features, such as genomics, text mining, or image
    analysis. On the other hand, the selection algorithm is limited to classic machine
    learning algorithms that use L1 regularization and return a set of coefficients,
    which are among the ones we discussed before: logistic regression and lasso regression.
    You can extend the concept proposed by stability selection by using feature importance
    (many ensemble models estimate feature importance), such as done in Scikit-learn
    by the command `SelectFromModel` ([https://mng.bz/oKej](https://mng.bz/oKej)),
    but things will get trickier because you’ll have to figure how what makes an importance
    estimate relevant and what selection threshold to use. In the next section, we
    reprise how feature importance works, and we present Boruta. Using a solid automatic
    feature selection procedure, this algorithm can figure out the relevant features
    for a decision-tree ensemble, such as random forests or gradient boosting.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定性选择提供了一些优势。它可以处理高维数据，通过引入随机性来避免过拟合，并提供一个考虑选择过程稳定性的特征重要性度量。它通常用于具有大量特征的复杂应用中，如基因组学、文本挖掘或图像分析。另一方面，选择算法仅限于使用
    L1 正则化的经典机器学习算法，并返回一组系数，这些系数是我们之前讨论过的：逻辑回归和 Lasso 回归。您可以通过使用特征重要性（许多集成模型估计特征重要性）来扩展稳定性选择的概念，例如
    Scikit-learn 中的 `SelectFromModel` 命令 ([https://mng.bz/oKej](https://mng.bz/oKej))，但事情会变得复杂，因为您需要弄清楚什么使重要性估计相关，以及使用什么选择阈值。在下一节中，我们将回顾特征重要性是如何工作的，并介绍
    Boruta。使用一个可靠的自动特征选择过程，该算法可以确定决策树集成（如随机森林或梯度提升）的相关特征。
- en: 6.2.2 Shadow features and Boruta
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 影子特征与 Boruta
- en: 'Boruta is a smart procedure for determining whether a feature is relevant in
    a machine learning problem by relying on the internal parameters of the model,
    such as coefficients in linear models or importance values based on gain, such
    as in decision trees and their ensembles. It was first published in “Feature Selection
    with the Boruta Package” by Miron B. Kursa and Witold R. Rudnicki [*Journal of
    Statistical Software* 36 (2010): 1-13]; for a copy of the article, see [https://www.jstatsoft.org/article/view/v036i11](https://www.jstatsoft.org/article/view/v036i11).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'Boruta 是一种智能过程，通过依赖模型内部参数（如线性模型中的系数或基于增益的重要性值，例如在决策树及其集成中）来确定特征在机器学习问题中的相关性。它首次发表在
    Miron B. Kursa 和 Witold R. Rudnicki 的《使用 Boruta 包进行特征选择》[*《统计软件杂志》36 (2010): 1-13*]一文中；欲获取文章副本，请参阅
    [https://www.jstatsoft.org/article/view/v036i11](https://www.jstatsoft.org/article/view/v036i11)。'
- en: Boruta, although innovative, presents quite a few analogies with stability selection.
    It can be used only with decision tree-based ensembles. To measure the relevance
    of a feature, as in stability selection, we look for non-zero coefficients. In
    Boruta, we count the times when the importance of a feature exceeds the highest
    importance obtained by shadow features. We call them hits. Shadow features are
    random versions of the feature themselves (basically shuffled features), which,
    given that they are random, should attain any importance just by chance. If any
    feature cannot exceed the same importance of a shadow feature, it cannot be considered
    more predictive than any random sequence of values.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Boruta 具有创新性，但它与稳定性选择有许多相似之处。它只能与基于决策树的集成一起使用。为了测量特征的相关性，就像在稳定性选择中一样，我们寻找非零系数。在
    Boruta 中，我们计算特征重要性超过影子特征获得的最重要性的次数。我们称之为“击中”。影子特征是特征自身的随机版本（基本上是打乱顺序的特征），鉴于它们是随机的，应该仅通过偶然获得任何重要性。如果任何特征不能超过影子特征相同的
    Importance，则不能认为它比任何随机值序列更具预测性。
- en: A threshold for selection, usually a minimum number of occurrences of non-zero
    coefficients in stability selection, is given in Boruta by how the number of hits
    translates into a binomial distribution. A significance threshold for retaining
    or dropping a feature test according to the distribution if the number of hits
    can prove that the feature is consistently better than any random construct.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Boruta 中，通过将击中次数转换为二项分布来确定选择的阈值，通常是一个非零系数在稳定性选择中的最小出现次数。如果击中次数可以证明特征始终优于任何随机结构，则根据分布保留或删除特征测试的显著性阈值。
- en: Listing 6.10 shows an example using Boruta to select all the relevant features
    for an XGBoost classification on the Airbnb NYC dataset. Boruta in the BorutaPy
    implementation ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py))
    has some limitations because, apart from working only with tree-based models,
    such as random forests or gradient boosting (no matter what the implementation
    is), it cannot work with pipelines. Hence, we first had to transform the data
    and then run Boruta on the transformed features as we were training the final
    model. Boruta takes as key parameters the estimator—that is, the model you want
    to use, the number of decision trees in the ensemble, and the `n_estimators` hyperparameter,
    which can be left empty, set to an integer, or set to “auto” where the number
    of trees is decided upon the size of the dataset. Other important parameters in
    Boruta are `max_iter`, the number of rounds of testing, usually set to 100, and
    the alpha threshold for the binomial test, which can be increased from 0.05 to
    allow for more features to be retained or decreased for more features to be discarded.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.10 展示了一个使用 Boruta 在 Airbnb 纽约数据集上对 XGBoost 分类选择所有相关特征的示例。在 BorutaPy 实现（[https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)）中，Boruta
    有一些限制，因为除了只能与基于树的模型（如随机森林或梯度提升，无论实现方式如何）一起工作之外，它还不能与管道一起工作。因此，我们首先必须转换数据，然后在训练最终模型时对转换后的特征运行
    Boruta。Boruta 的关键参数包括估计器——即你想要使用的模型、集成中的决策树数量以及 `n_estimators` 超参数，该参数可以留空、设置为整数或设置为“auto”，此时树的数量将根据数据集的大小来决定。Boruta
    的其他重要参数包括 `max_iter`，测试轮数，通常设置为 100，以及二项式检验的 alpha 阈值，该阈值可以从 0.05 增加以允许保留更多特征，或减少以丢弃更多特征。
- en: Listing 6.10 Boruta selection
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.10 Boruta 选择
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Transforms the input data, performing any necessary preprocessing steps
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ① 转换输入数据，执行任何必要的预处理步骤
- en: ② Initializes a BorutaPy feature selection object using an XGBoost classifier
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 XGBoost 分类器初始化一个 BorutaPy 特征选择对象
- en: ③ Fits the Boruta feature selector
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 调整 Boruta 特征选择器
- en: ④ Retrieves a Boolean mask of selected features determined by the Boruta feature
    selector
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取由 Boruta 特征选择器确定的所选特征的布尔掩码
- en: 'After a few iterations, you should get the results of only a single feature
    discarded as not relevant to the problem:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几次迭代后，你应该只得到一个被丢弃为与问题不相关的特征的结论：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The same procedure can be executed using LightGBM as predictor, instead of
    XGBoost:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 LightGBM 作为预测器执行相同的程序，而不是 XGBoost：
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Initializes a BorutaPy feature selection object using the provided LightGBM
    classifier
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用提供的 LightGBM 分类器初始化一个 BorutaPy 特征选择对象
- en: 'The result is reached after only 9 iterations, and this time, we have an increased
    number of rejected features:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在仅 9 次迭代后达到，这次我们有一个增加的被拒绝特征数量：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: LightGBM not only converges more quickly, but the way it splits allows, in this
    problem, the creation of a performing model with many fewer features than XGBoost.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 不仅收敛速度更快，而且其分割方式允许在这个问题中创建一个性能良好的模型，其特征数量比 XGBoost 少得多。
- en: In our example, we have trained on all the available data. Still, you can use
    Boruta even in a cross-validation loop, where you can consolidate a result for
    the dataset by using all the selected features in all the folds or by only those
    selected at least a minimum number of times across the folds.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们在所有可用数据上进行了训练。尽管如此，你仍然可以使用 Boruta，即使在交叉验证循环中，你可以在所有折叠中使用所有选定的特征或仅使用至少在折叠中至少被选中一定次数的特征来巩固数据集的结果。
- en: 6.2.3 Forward and backward selection
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 前向和后向选择
- en: One limitation of Boruta is that it selects all the relevant features for your
    problem but not the essential ones. This means you may end up with a list with
    redundant and highly correlated features that can be cut to a shorter selection.
    After applying Boruta, we suggest resorting to a sequential feature selection
    procedure, as implemented in the Scikit-learn function `SequentialFeatureSelector`.
    This procedure adds by forward selection or removes by backward elimination features
    from your selection based on their performance on the prediction in a greedy fashion—that
    is, always picking up the best-performing choice, based on the cross-validation
    score, in terms of addition or discard. The technique relies on the learning algorithm
    and its objective function. Hence, its selection will always be among the best
    possible. Since it is a greedy procedure, there is always the risk of choosing
    a local optimum set.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Boruta的一个局限性是它选择了你问题的所有相关特征，但不是必要的特征。这意味着你最终可能得到一个包含冗余和高度相关特征的列表，这些特征可以被缩短选择。在应用Boruta之后，我们建议回到序列特征选择过程，如Scikit-learn函数`SequentialFeatureSelector`中实现的那样。此过程通过正向选择添加或通过反向消除根据它们在预测中的性能以贪婪的方式从你的选择中添加或删除特征——也就是说，总是根据交叉验证分数选择最佳性能的选择，无论是添加还是丢弃。该技术依赖于学习算法及其目标函数。因此，其选择将始终在最佳可能的选择之中。由于它是一个贪婪过程，总是存在选择局部最优集的风险。
- en: Sequential selection is a very effective procedure in reducing the number of
    features you have to deal with. Still, it is quite time-consuming because the
    algorithm has to evaluate all the candidates at each round. In the forward procedure,
    this will turn slower and slower as you proceed because, despite having fewer
    candidates to evaluate at each round, the increasing number of features used will
    slow down the training. However, in the backward procedure, you start slow and
    tend to accelerate after discarding a number of features. The backward procedure
    may be impractical if you start from many features to evaluate and the training
    is very slow.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 序列选择是一种非常有效的减少需要处理的特征数量的方法。然而，它相当耗时，因为算法必须在每一轮评估所有候选者。在正向过程中，随着你继续进行，这会变得越来越慢，因为尽管每一轮评估的候选者数量减少，但使用的特征数量增加会减慢训练速度。然而，在反向过程中，你开始较慢，并在丢弃一定数量的特征后倾向于加速。如果你从许多特征开始评估且训练非常缓慢，反向过程可能不切实际。
- en: 'As a stopping rule for the procedure, you may set a certain number of features,
    or you can leave the selection algorithm to find out the point at which adding
    or removing a feature bears no more improvements to the predictions. A tolerance
    threshold helps give the algorithm a certain freedom to proceed or not: the larger
    the tolerance, the more likely the algorithm will proceed in its operations, even
    if adding or removing a feature somehow deteriorates the performance.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该过程的停止规则，你可以设置一定数量的特征，或者你可以让选择算法找出添加或删除特征不再对预测带来改进的点。容忍度阈值有助于给算法提供一定的自由度来决定是否继续：容忍度越大，算法在其操作中继续的可能性就越大，即使添加或删除特征在某种程度上降低了性能。
- en: In listing 6.11, we apply a forward selection to an XGBoost model trained on
    the Airbnb NYC dataset. The selection algorithm is set free to determine the correct
    number of features to add, and the low tolerance (set to 0.0001 on accuracy measures
    ranging from 0 to 1) should stop it at the first signs of deterioration of the
    predictive performances.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.11中，我们对在Airbnb纽约数据集上训练的XGBoost模型应用了正向选择。选择算法被设置为自由确定要添加的正确特征数量，并且低容忍度（在0到1的准确度度量上设置为0.0001）应该在其预测性能开始下降的第一个迹象时停止。
- en: Listing 6.11 Forward selection
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 正向选择
- en: '[PRE17]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Initializes a KFold cross-validation splitter object with five folds
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用五个折点初始化一个KFold交叉验证分割对象
- en: ② Creates a scoring function for use in the feature selection process
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为特征选择过程创建一个评分函数
- en: ③ Sets a tolerance value used by the sequential feature selector to determine
    convergence during the search
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置序列特征选择器用于在搜索过程中确定收敛的容忍度值
- en: ④ Specifies the direction of feature selection (which is “forward” in this case)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 指定特征选择的方向（在本例中为“正向”）
- en: ⑤ Retrieves a boolean mask of selected features
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 获取所选特征的布尔掩码
- en: 'The obtained results point out to six features to be used: three binary ones,
    a high cardinality categorical, and two numeric ones:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的结果指出需要使用六个特征：三个二元特征，一个高基数分类特征，以及两个数值特征：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can replicate the experiment in a backward fashion by running the following
    commands:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令以反向方式复制实验：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Specifies the direction of feature selection (which is “backward” in this
    case)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ① 指定特征选择的方向（在这种情况下是“向后”）
- en: 'The resulting selection is made of nine features, many of those already seen
    in the set resulting from the forward selection:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结果选择由九个特征组成，其中许多已经在正向选择的结果集中出现过：
- en: '[PRE20]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In our own experience, choosing a forward or backward selection depends on the
    need you may have to risk leaving out some slightly important feature from your
    chosen set. With forward addition, you are sure to keep only the essential features
    but risk leaving something marginally relevant out. With the backward elimination,
    you are assured that all the key features are in the set, allowing for some redundancy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们自己的经验，选择正向或反向选择取决于你可能需要承担的风险，即从所选集中遗漏一些稍微重要的特征。使用正向添加，你可以确保只保留基本特征，但风险遗漏一些边际相关的特征。使用反向消除，你可以确保所有关键特征都在集合中，允许一些冗余。
- en: Besides choosing a forward or backward procedure, sequential selection will
    help you build models faster in training and prediction and will be much easier
    to interpret and maintain because of the limited number of features involved.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择正向或反向过程之外，顺序选择可以帮助你在训练和预测中更快地构建模型，并且由于涉及的特征数量有限，它将更容易解释和维护。
- en: 6.3 Optimizing hyperparameters
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 优化超参数
- en: Feature engineering can improve the results you obtain from your classical machine
    learning models. Creating new features can reveal the underlying patterns and
    relationships in data that the models cannot grasp because of their limitations.
    Feature selection can improve your models’ results by removing unuseful and redundant
    features for the problem, thus reducing the noise and spurious signals in data.
    Finally, by optimizing hyperparameters, you can gain another performance boost
    and have your classical machine learning model shine on the tabular data problem
    you are dealing with.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以提高你从经典机器学习模型中获得的结果。创建新的特征可以揭示数据中模型由于局限性而无法把握的潜在模式和关系。通过移除对于问题无用的和冗余的特征，特征选择可以提高你的模型结果，从而减少数据中的噪声和虚假信号。最后，通过优化超参数，你可以获得另一个性能提升，并让你的经典机器学习模型在处理表格数据问题时更加出色。
- en: As discussed in chapter 4, hyperparameters are those settings that work under
    the hood of all machine learning algorithms and determine how specifically they
    can work. From an abstract point of view, each machine learning algorithm potentially
    offers a limited, yet still wide, range of functional forms—that is, the mathematical
    ways you can relate your predictor variables to your outcome. Straight out of
    the box, a machine learning algorithm may less or more match the functional form
    required by your specific machine learning problem.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如第4章所述，超参数是所有机器学习算法幕后工作的设置，决定了它们可以如何具体工作。从抽象的角度来看，每个机器学习算法可能提供有限的、但仍然很宽的范围的功能形式——即你可以用数学方式将预测变量与结果相关联的方式。直接从盒子里出来，机器学习算法可能更少或更多符合你特定机器学习问题所需的功能形式。
- en: For instance, if you are using a gradient boosting algorithm to solve a classification
    problem, it may be that the default number of iterations or how its trees are
    grown do not match the requirements of the problem. You may need fewer or more
    iterations and tree growth than specified by the default values. By opportunely
    setting its hyperparameters, you can find the best settings that work better with
    your problem.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在使用梯度提升算法来解决分类问题，可能默认的迭代次数或其树的生长方式并不符合问题的要求。你可能需要比默认值更少或更多的迭代和树生长。通过恰当地设置其超参数，你可以找到与你的问题更好地配合的最佳设置。
- en: 'However, it is not just a matter of twiddling all the many knobs that an algorithm
    presents until it gets the results you expect. Sometimes the knobs are too many
    to be tested together, and even if you manage to test enough of them, if not done
    properly, it will result in overfitting your data and, on the contrary, obtaining
    worse results. You need a systematic approach after defining one or more evaluation
    metrics:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这不仅仅是调整算法提供的所有许多旋钮直到得到你期望的结果的问题。有时旋钮太多，无法一起测试，即使你设法测试了足够多的旋钮，如果操作不当，可能会导致数据过拟合，并且相反，得到更差的结果。在定义一个或多个评估指标之后，你需要一个系统性的方法：
- en: Defining a search space containing the hyperparameters whose effects you want
    to explore and the boundaries of the values to test
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个包含你想要探索的超参数及其要测试的值边界的搜索空间
- en: Building a proper cross-validation scheme to ensure that you are discovering
    a solution that generalizes beyond the data you have
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个适当的交叉验证方案，以确保你发现的是一个可以推广到你所拥有数据的解决方案
- en: Choosing a search algorithm that, by a proper strategy, will find out in less
    time and with less cost—for instance, in terms of computations—the solution you
    need
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个搜索算法，通过适当的策略，可以在更短的时间内以更低的成本（例如，从计算的角度来看）找到你需要的解决方案
- en: In the following subsections, under the light of different search strategies,
    we discuss the way you can accomplish tuning some of the classical machine learning
    algorithms we have seen so far.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，根据不同的搜索策略，我们讨论了如何调整我们迄今为止所看到的某些经典机器学习算法的方法。
- en: 6.3.1 Searching systematically
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 系统性搜索
- en: Grid search works through all the possible combinations of hyperparameters’
    values. For every hyperparameter you want to test, you pick a sequence of values
    and iterate through all their combinations exhaustively. In the end, you pick
    the combination that returns the best results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索通过所有超参数值的可能组合。对于你想要测试的每个超参数，你选择一个值序列，并彻底迭代它们的所有组合。最后，你选择返回最佳结果的组合。
- en: In listing 6.12, we apply it to a logistic regression model, helping to choose
    the kind of regularization and the settings of L1 and L2 regularization values.
    The most important part of the code is the search grid, which is a list containing
    one or multiple dictionaries. Instead, each dictionary is a search space, a sequence
    of hyperparameters (the keys of the dictionary) associated with a list of a generator
    of values, which are the possible values you want to test (the values of the dictionary).
    Structuring one or more search spaces is a common practice across all the optimization
    methods, whether they are from Scikit-learn or not. Just notice how the name of
    the hyperparameters are formulated in the form `model__name_of_the_hyperparameter`
    because we are optimizing a pipeline and addressing parameters that are first
    internal to the pipeline and then of the model. We will come back to this in the
    next subsection with more explanations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.12中，我们将其应用于逻辑回归模型，帮助选择正则化的类型以及L1和L2正则化值的设置。代码中最重要的一部分是搜索网格，它是一个包含一个或多个字典的列表。每个字典是一个搜索空间，一个与值生成器列表关联的超参数序列（字典的键），这些值是你想要测试的可能值（字典的值）。在所有优化方法中，结构一个或多个搜索空间是一种常见的做法，无论它们是否来自Scikit-learn。只需注意超参数的名称是如何以`model__name_of_the_hyperparameter`的形式制定的，因为我们正在优化一个管道，并解决管道内部和模型参数。我们将在下一小节中对此进行更多解释。
- en: Listing 6.12 Grid search
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 网格搜索
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① A list of dictionaries specifying a search grid of hyperparameters for the
    logistic regression model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个字典列表，指定逻辑回归模型的超参数搜索网格
- en: ② Initializes a GridSearchCV object using the defined search grid
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用定义的搜索网格初始化一个GridSearchCV对象
- en: ③ Prints the best hyperparameters found by the grid search
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印网格搜索找到的最佳超参数
- en: ④ Prints the best score achieved by the model using the best hyperparameters
    found during the grid search
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印模型在网格搜索过程中找到的最佳超参数所达到的最佳得分
- en: 'After testing all the combinations, the grid search procedure returns that
    the best set of hyperparameters is just not to use any penalty at all. It returns
    the best cross-validated score in support of its report:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 测试所有组合后，网格搜索过程返回最佳的超参数组合就是根本不使用任何惩罚。它返回支持其报告的最佳交叉验证得分：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Grid search is effective when your hyperparameters are few; they take discrete
    values, and you can parallelize in memory the testing operations because your
    dataset is not too large.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的超参数很少时，网格搜索是有效的；它们取离散值，并且你可以并行化内存中的测试操作，因为你的数据集不是太大。
- en: First, the more combinations, the more tests you have to take and the longer
    and more computations you’ll have to spend. It could be a serious problem if you
    need to test many hyperparameters and suspect some of them are irrelevant for
    properly tuning your algorithm. When you add a hyperparameter to the grid search,
    you must make all the other hyperparameters cycle through it, which can turn into
    a waste of energy if the hyperparameter you are testing is irrelevant.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，组合越多，你必须进行的测试就越多，你需要花费更长的时间和更多的计算。如果你需要测试许多超参数，并且怀疑其中一些对正确调整你的算法无关紧要，这可能会成为一个严重的问题。当你将一个超参数添加到网格搜索中时，你必须让所有其他超参数通过它循环，这可能会在测试的超参数无关紧要时变成浪费能量。
- en: In addition, if a parameter takes continuous values, you must decide how to
    turn its continuous search space into a discrete one. Usually, this is done by
    uniformly dividing the continuum of values into discrete values, but by doing
    so without any knowledge of the way the algorithm behaves with respect to that
    hyperparameter and its values may again turn into wasting multiple computations
    on testing values that cannot improve the algorithm performances.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果一个参数取连续值，你必须决定如何将其连续的搜索空间转换为离散的。通常，这是通过将值的连续体均匀地划分为离散值来完成的，但这样做而没有了解算法相对于该超参数及其值的行为，其值可能再次变成在测试值上浪费多次计算，而这些测试值无法提高算法性能。
- en: The last aspect to consider is using multiple cores and parallelizing their
    operations. Grid search is completely unaware of the results each test obtains.
    The results are only ranked at the end, and you are offered the best result. Hence,
    grid search is fine if your algorithm naturally works on a single core. However,
    suppose your algorithm uses multiple threads and cores, such as a random forest
    or an XGBoost. In that case, you have to trade off between having the algorithm
    running at full speed or having the optimization procedure go parallel and speedier.
    Usually, the best choice is to push the algorithm to run faster by using parallel
    running. Regardless of whether you decide to take advantage of the parallelization
    capabilities of the algorithm or those of the search procedure, grid search is
    not the best-performing option when working with a multicore algorithm.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的最后一个方面是使用多个核心并并行化它们的操作。网格搜索完全不知道每个测试的结果。结果只有在最后才会排名，你只能得到最佳结果。因此，如果你的算法自然地工作在单个核心上，网格搜索是可行的。然而，如果你的算法使用多个线程和核心，例如随机森林或XGBoost，那么你必须在算法以全速运行和优化过程并行化以加快速度之间进行权衡。通常，最佳选择是使用并行运行来推动算法更快地运行。无论你决定利用算法的并行化能力还是搜索过程的并行化能力，当与多核算法一起工作时，网格搜索都不是性能最佳的选择。
- en: Based on our experience and the limits of the grid search strategy, we deem
    it the best fit for testing linear models since they are easily parallelizable
    and have few limited parameters, often characterized by taking Boolean or discrete
    values.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验和网格搜索策略的局限性，我们认为它最适合测试线性模型，因为它们易于并行化，并且参数有限，通常以布尔值或离散值的形式出现。
- en: 6.3.2 Using random trials
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 使用随机试验
- en: Important limitations when using grid search are that
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网格搜索时的重要限制包括：
- en: You need to discretize continuous hyperparameters.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要将连续的超参数离散化。
- en: If a hyperparameter is irrelevant to the problem, you are going to have many
    trials wasted as they test the space of the irrelevant feature.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个超参数与问题无关，你将浪费很多试验，因为它们测试了无关特征的搜索空间。
- en: For these reasons, the idea of sampling the search space randomly is rooted
    in the machine learning community. As described in the paper “Random Search for
    Hyper-Parameter Optimization” by James Bergstra and Yoshua Bengio (*Journal of
    Machine Learning Research;* [https://mng.bz/nRg8](https://mng.bz/nRg8)), random
    search optimization becomes the standard optimization when you have many hyperparameters
    and you don’t know exactly how they affect the results or how they work together.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，随机采样搜索空间的想法在机器学习社区中根深蒂固。正如James Bergstra和Yoshua Bengio在论文“随机搜索用于超参数优化”中所述（*《机器学习研究杂志》；[https://mng.bz/nRg8](https://mng.bz/nRg8)），当有多个超参数且你不知道它们如何影响结果或如何协同工作时，随机搜索优化成为标准优化方法。
- en: In our example, we reprise our classification problem using an XGBoost classifier.
    XGBoost, as with other gradient boosting implementations, features several hyperparameters
    that can be deemed important, and you should try to test them to check if your
    model’s performance can be improved. In the example, we also make things a bit
    more sophisticated because we operate by the XGBoost model wrapped into a pipeline,
    thus requiring a specific way to address hyperparameters. Since each element in
    the pipeline has a name, you must address each parameter in a part of the pipeline
    by the name of the element in the pipeline, two underlines, and then the name
    of the hyperparameter. For instance, in our example, XGBoost is in a part of the
    pipeline named “xgb.” To address the hyperparameter `n_estimators` of XGBoost,
    just use the label `xgb__n_estimators` in your search space. The idea is to demonstrate
    how to optimize a model and its pipeline without testing all the possible choices
    influencing a model’s predictive performance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用XGBoost分类器重新审视我们的分类问题。XGBoost与其他梯度提升实现一样，具有几个可以被认为是重要的超参数，您应该尝试测试它们以检查您的模型性能是否可以改进。在示例中，我们还使事情变得更加复杂，因为我们通过将XGBoost模型包装到管道中来操作，因此需要特定的方式来处理超参数。由于管道中的每个元素都有一个名称，您必须通过管道中元素的名称、两个下划线和超参数的名称来指定管道中的每个参数。例如，在我们的例子中，XGBoost位于名为“xgb”的管道部分中。要指定XGBoost的`n_estimators`超参数，只需在搜索空间中使用标签`xgb__n_estimators`即可。这个想法是展示如何在不测试所有可能影响模型预测性能的选择的情况下优化模型及其管道。
- en: Listing 6.13 Random search
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.13 随机搜索
- en: '[PRE23]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Creates a pipeline that combines data processing and the XGBoost classifier
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个结合数据预处理和XGBoost分类器的管道
- en: ② A dictionary containing various hyperparameters with their search spaces for
    the RandomizedSearchCV
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ② 包含各种超参数及其搜索空间的字典，用于RandomizedSearchCV
- en: ③ Specifies the number of iterations for the random search process
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 指定随机搜索过程的迭代次数
- en: ④ Specifies the number of parallel jobs to run for the search
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 指定用于搜索的并行作业数量
- en: ⑤ Prints the best hyperparameters found by the RandomizedSearchCV
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印由RandomizedSearchCV找到的最佳超参数
- en: ⑥ Prints the best score achieved using the best hyperparameters found during
    the random search
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 打印在随机搜索过程中使用最佳超参数获得的最佳分数
- en: 'After a while (the code runs in about one hour in a Google Colab instance),
    we get the set of the best parameters and the cross-validated score obtained:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一段时间（在Google Colab实例中代码运行大约需要一小时），我们得到了最佳参数集和通过交叉验证得到的分数：
- en: '[PRE24]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Random search optimization, in spite of its simplicity and the fact it relies
    on randomness, really works, and it provides the best optimization in many situations.
    Many AutoML systems rely on this optimization strategy when there are many hyperparameters
    to tune (see, for instance, “Google Vizier: A Service for Black-Box Optimization,”
    by D. Golovinb et al., 2017 at [https://mng.bz/8OrZ](https://mng.bz/8OrZ)). Compared
    to grid search, which works well when you have a limited set of hyperparameters
    that you expect to be impactful and a limited set of values to test, random search
    works the best when you have too many values to tune, without prior knowledge
    of how they work.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机搜索优化简单且依赖于随机性，但它确实有效，并且在许多情况下提供了最佳的优化。许多AutoML系统在需要调整许多超参数时依赖于这种优化策略（例如，参见D.
    Golovinb等人于2017年发表的“Google Vizier：一个黑盒优化服务”，[https://mng.bz/8OrZ](https://mng.bz/8OrZ)）。与网格搜索相比，当您有一组有限的超参数预期会产生影响，以及一组有限的值要测试时，随机搜索在您有太多值要调整且没有先验知识了解它们如何工作时效果最佳。
- en: All you have to do is rely on enough random tests to have a good combination
    emerge, which may not take long. In our experience, 30 to 60 random draws usually
    suffice for good optimization. One strong point of random search optimization
    is that it works well for complex problems and is not affected by irrelevant hyperparameters.
    The number of relevant ones determines how fast you can find a good solution.
    The algorithm is also suited for parallel search on different computers or instances
    (you pick the best result among all). Still, this positive point comes with the
    limitation that since tests are independent, they do not inform each other about
    their results.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你所需要做的就是依靠足够的随机测试，以便出现良好的组合，这可能不会花费很长时间。根据我们的经验，30到60次随机抽取通常足以进行良好的优化。随机搜索优化的一个优点是它适用于复杂问题，并且不受无关超参数的影响。相关参数的数量决定了你可以多快找到一个好的解决方案。该算法也适用于在不同计算机或实例上的并行搜索（你从所有结果中选择最佳结果）。然而，这个积极点也有局限性，因为测试是独立的，它们不会相互告知结果。
- en: 6.3.3 Reducing the computational burden
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 减少计算负担
- en: Both grid search and random search do not utilize the outcomes from previous
    experiments. Grid search strictly adheres to a predefined procedure, while random
    search conducts a set of independent tests. In both cases, the prior results are
    not considered or used in any way during the search process. *Successive halving*,
    a wrapper of both strategies, can instead take advantage of knowing the prior
    results. The idea is like that of a tournament where you first hold many rounds
    and put forth few resources to test different hyperparameters’ values. Then, as
    you progress and drop the values that underperform, you invest more resources
    to test the remaining values thoroughly. Usually, the resource you initially dilute
    and then later concentrate on is the number of training examples. More examples
    imply certain results from a hyperparameter test, but it costs more computational
    power.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索和随机搜索都不利用先前实验的结果。网格搜索严格遵循预定义的程序，而随机搜索进行一系列独立的测试。在这两种情况下，先前结果在搜索过程中都没有被考虑或使用。*连续减半*，这两种策略的包装器，可以利用知道先前结果的优势。想法就像一个锦标赛，你首先进行多轮比赛，投入少量资源来测试不同的超参数值。然后，随着你前进并淘汰表现不佳的值，你将更多资源投入到剩余值的彻底测试中。通常，你最初稀释然后后来集中的资源是训练样本的数量。更多的例子意味着来自超参数测试的某些结果，但这需要更多的计算能力。
- en: Available as `HalvingGridSearchCV` and `HalvingRandomSearchCV` in Scikit-learn,
    in listing 6.14, we test the random search variant to verify if we can obtain
    similar optimization results at a fraction of the time. As stated, we use the
    number of samples as a scarce resource to optimize, using just the 30% available
    at the start. In addition, we instruct the algorithm to start from 20 initial
    candidates and to decrease the number of candidates by three times at each round
    (from 20 to 6 to 2).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中作为`HalvingGridSearchCV`和`HalvingRandomSearchCV`提供，在列表6.14中，我们测试了随机搜索变体，以验证我们是否可以在一小部分时间内获得类似的优化结果。如前所述，我们使用样本数量作为稀缺资源进行优化，仅使用初始的30%。此外，我们指示算法从20个初始候选者开始，并在每一轮中将候选者数量减少到原来的三分之一（从20减少到6，再减少到2）。
- en: Listing 6.14 Halving random search
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.14减半随机搜索
- en: '[PRE25]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Enables the experimental HalvingRandomSearchCV module
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ① 启用实验性的HalvingRandomSearchCV模块
- en: ② Specifies that the resource being used for halving is the number of samples
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ② 指定用于减半的资源是样本数量
- en: ③ Sets the number of candidates that will be sampled and evaluated at the first
    iteration
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置在第一次迭代中将采样和评估的候选者数量
- en: ④ Determines the factor by which the number of candidates will be reduced in
    each iteration
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 确定每次迭代中候选者数量减少的因子
- en: ⑤ Sets the minimum number of resources (samples) that will be used in the halving
    process
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置在减半过程中将使用的最小资源（样本）数量
- en: ⑥ Sets the maximum number of resources (samples) that will be used in the halving
    process
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 设置在减半过程中将使用的最大资源（样本）数量
- en: 'The following are the results you obtain, at a fraction of the time previously
    required (in Google Colab the procedure runs in about 10 minutes):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在所需时间的一小部分内获得的结果（在Google Colab中，该过程大约需要10分钟）：
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In our experience with this optimization strategy, the strategy is to set the
    initial round in a way that it can catch some good hyperparameters. Hence it is
    important to have the highest possible number of candidates running at a minimum
    of resources, although not so low to compromise the results of optimization. Setting
    as little as 1,000 starting samples should work sufficiently well if the factor
    parameter is reduced. This determines the proportion of candidates selected for
    each subsequent iteration to two instead of three, thus going to a longer number
    of rounds.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用这种优化策略的经验中，策略是设置初始轮次，使其能够捕捉到一些好的超参数。因此，拥有尽可能多的候选者以最低的资源运行是非常重要的，尽管这不会太低以至于影响优化的结果。如果减少因子参数，那么至少1,000个起始样本应该足够好。这决定了每个后续迭代选择的候选者比例从三个变为两个，从而进行更多的轮次。
- en: 6.3.4 Extending your search by Bayesian methods
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 使用贝叶斯方法扩展搜索
- en: Another optimization strategy that makes informed choices is Bayesian optimization.
    Introduced in the paper “Practical Bayesian Optimization of Machine Learning Algorithms”
    by Snoek, Larochelle, and Adams ([https://arxiv.org/abs/1206.2944](https://arxiv.org/abs/1206.2944)),
    the idea behind this optimization strategy is to understand how the hyperparameters
    of a model work, by building a model of themselves. The algorithm optimizes a
    proxy function, called the surrogate function, to increase the algorithm’s performance.
    Of course, the surrogate function is updated by the feedback from the objective
    function of the machine learning model under optimization. However, the Bayesian
    optimization algorithm’s decisions are solely based on the surrogate function.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种做出明智选择的优化策略是贝叶斯优化。这一策略由Snoek、Larochelle和Adams在论文“Practical Bayesian Optimization
    of Machine Learning Algorithms”（[https://arxiv.org/abs/1206.2944](https://arxiv.org/abs/1206.2944)）中提出，其背后的理念是通过构建模型来理解模型超参数的工作方式。该算法通过优化一个代理函数，即代理函数，来提高算法的性能。当然，代理函数会根据优化中的机器学习模型的目标函数的反馈进行更新。然而，贝叶斯优化算法的决策完全基于代理函数。
- en: 'In particular, it is another that alternates exploration with exploitation:
    the acquisition function. The acquisition function reports how much exploring
    a certain combination of parameters is promising and how much is uncertain, based
    on the surrogate function. Exploration implies trying combinations of the parameters
    that have never been tried before, and this happens when there is much uncertainty,
    and thus consequently hope, in certain areas of the search space that need at
    least to be tried to improve the surrogate function. On the contrary, exploitation
    happens when the acquisition function ensures that the algorithm can improve performance
    when trying a certain set of hyperparameters.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是另一种交替探索与利用的策略：获取函数。获取函数根据代理函数报告了探索特定参数组合的潜力有多大以及不确定性有多大。探索意味着尝试从未尝试过的参数组合，这发生在存在很多不确定性和因此希望至少尝试改进代理函数的搜索空间区域时。相反，当获取函数确保算法在尝试特定一组超参数时可以提高性能时，就会发生利用。
- en: As the “Bayesian” in the name implies, and from our brief description of how
    the Bayesian optimization works under the hood, the process is influenced by prior
    expectations and subsequently corrected by posterior observations in a fine-tuning
    cycle. The surrogate function in all of this is nothing more than a model of our
    model. Usually, Gaussian processes are chosen as a model for the surrogate function.
    Still, there are alternatives, such as using tree algorithms such as random forests
    or tree-structured Parzen estimators, which are a multivariate distribution capable
    of describing the behavior of the hyperparameters in our model. Packages such
    as Scikit-optimize ([https://scikit-optimize.github.io/stable/](https://scikit-optimize.github.io/stable/))
    or KerasTuner ([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))
    use Gaussian processes, with Scikit-optimize also capable of using tree ensembles
    and KerasTuner using multiarmed bandits, as well. Optuna, an optimization framework
    developed by Preferred Networks, a Japanese AI research and development company,
    instead uses tree-structured Parzen estimators. Initially released in May 2019
    as an open-source project, Optuna is particularly popular in the Python machine
    learning community due to its simplicity, versatility, and integration with popular
    machine learning libraries such as TensorFlow, PyTorch, and Scikit-learn.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字中的“贝叶斯”所暗示的，以及我们对贝叶斯优化内部工作原理的简要描述，这个过程受到先验期望的影响，并在微调周期中通过后验观察进行修正。在这个过程中，代理函数不过是我们模型的一个模型。通常，高斯过程被选为代理函数的模型。尽管如此，还有其他替代方案，例如使用随机森林或树结构Parzen估计器等树算法，这些是多变量分布，能够描述我们模型中超参数的行为。Scikit-optimize
    ([https://scikit-optimize.github.io/stable/](https://scikit-optimize.github.io/stable/))
    或 KerasTuner ([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))
    等包使用高斯过程，Scikit-optimize还能够在使用树集成的同时，KerasTuner则使用多臂老虎机。Optuna，由日本人工智能研发公司Preferred
    Networks开发的优化框架，则使用树结构Parzen估计器。Optuna最初于2019年5月作为一个开源项目发布，由于其简单性、多功能性和与TensorFlow、PyTorch和Scikit-learn等流行机器学习库的集成，在Python机器学习社区中特别受欢迎。
- en: In our example, we use Optuna to improve our XGBoost classifier. When using
    Optuna, you just set up a study and provide its running parameters, such as the
    number of trials, `n_trials`, and the direction parameter if you want to minimize
    or maximize your objective function. Behind the scenes, all the heavy lifting
    is done by the objective function, which you define and returns an evaluation.
    The objective function expects just an input parameter, trial, which Optuna provides.
    By the trial parameter, you define the values of the hyperparameters to test.
    Then, you just test them as you like because it is up to you inside the objective
    function to decide if to apply a cross-validation, a simple test on a sample,
    or anything else. This flexibility also allows you to run complex optimizations
    where certain hyperparameters are used or depend on others and their values. It
    is up to you to code the procedure you want.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用Optuna来改进我们的XGBoost分类器。当使用Optuna时，你只需设置一个研究并为其提供运行参数，例如试验次数`n_trials`以及如果你想要最小化或最大化目标函数的方向参数。在幕后，所有繁重的工作都是由目标函数完成的，你定义的目标函数返回一个评估结果。目标函数期望只有一个输入参数，即试验，这是Optuna提供的。通过试验参数，你定义要测试的超参数的值。然后，你只需按照你的喜好进行测试，因为是否应用交叉验证、对样本进行简单测试或其他任何事情，完全取决于你在目标函数内部的决定。这种灵活性还允许你运行复杂的优化，其中某些超参数被使用或依赖于其他超参数及其值。你需要编写你想要的程序代码。
- en: Listing 6.15 Bayesian search with Optuna
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.15 使用Optuna进行贝叶斯搜索
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① A dictionary defining the search space for hyperparameters for Optuna
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义Optuna超参数搜索空间的字典
- en: ② Creates an XGBoost classifier with hyperparameters suggested by Optuna
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个由Optuna建议超参数的XGBoost分类器
- en: ③ Performs cross-validation to evaluate the model’s performance using the hyperparameters
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用超参数执行交叉验证以评估模型的性能
- en: ④ A function acting as the objective value for optimization by returning the
    mean accuracy score from cross-validation
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个作为优化目标值的函数，通过返回交叉验证的平均准确率得分
- en: ⑤ Creates an Optuna study object with the goal of maximizing the objective function
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个Optuna研究对象，目标是最大化目标函数
- en: ⑥ Starts the optimization process using the defined objective function and a
    maximum of 60 trials
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用定义的目标函数和最多60次试验启动优化过程
- en: ⑦ Prints the best-achieved value of the objective function
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打印目标函数的最佳实现值
- en: ⑧ Prints the best hyperparameters found by Optuna
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 打印 Optuna 找到的最佳超参数
- en: 'On a Google Colab instance, the process can take up to two hours, but the results
    are by far the best in class you can obtain from hyperparameter optimization:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 实例上，这个过程可能需要长达两小时，但就超参数优化而言，结果无疑是同类中你能获得的最佳：
- en: '[PRE28]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As an extra feature offered by Optuna, with a few simple additions to the previous
    code, you can store your study in a project database and restart optimization
    at any time. Optuna can integrate its optimization procedures with SQLite if,
    at the time of the creation of the study, you declare the name of the study and
    a target database:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Optuna 提供的额外功能，通过在之前的代码中添加几个简单的修改，你可以将你的研究存储在项目数据库中，并在任何时间重新启动优化。如果在创建研究时声明了研究的名称和目标数据库，Optuna
    可以将其优化过程与 SQLite 集成：
- en: '[PRE29]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ① Defines the path to the SQLite database where Optuna will store study-related
    information
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义 Optuna 将存储研究相关信息的 SQLite 数据库的路径
- en: ② Provides a name for the Optuna study
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为 Optuna 研究提供一个名称
- en: ③ Creates an Optuna study object and connects it to the SQLite database
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个 Optuna 研究对象并将其连接到 SQLite 数据库
- en: Regarding the specification of the SQLite storage database, `sqlite://` is a
    Uniform Resource Identifier (URI) scheme used to specify the protocol or mechanism
    for connecting to an SQLite database. In the context of the URI scheme, `sqlite://`
    indicates that the database connection will be established using the SQLite database
    engine. When using this URI scheme, the `sqlite://+` portion is followed by the
    path to the SQLite database file. In your example, `sqlite:///sqlite.db` specifies
    that the SQLite database file is named `sqlite.db` and is located in the current
    directory. The three slashes (`///`) after `sqlite:` are optional and indicate
    that the path is relative to the current directory.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 SQLite 存储数据库的指定，`sqlite://` 是一个统一资源标识符（URI）方案，用于指定连接到 SQLite 数据库的协议或机制。在
    URI 方案中，`sqlite://` 表示数据库连接将通过 SQLite 数据库引擎建立。当使用此 URI 方案时，`sqlite://+` 部分后面跟着
    SQLite 数据库文件的路径。在你的例子中，`sqlite:///sqlite.db` 指定 SQLite 数据库文件名为 `sqlite.db`，并且位于当前目录。`sqlite:`
    后面的三个斜杠（`///`）是可选的，表示路径是相对于当前目录的。
- en: 'Once the study has been completed, you can also obtain useful visualization
    regarding the results of the iterations and gain insights useful on successive
    runs of the same search. For instance, you can explore the optimization history
    and check if you have reached a plateau in the optimization or if going on with
    more iterations is advisable:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦研究完成，你还可以获得有关迭代结果的实用可视化，并在后续运行相同的搜索中获得有价值的见解。例如，你可以探索优化历史记录，检查你是否已经达到了优化的平台期，或者继续进行更多迭代是否可取：
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Figure 6.6 shows how our optimization proceeded. After a few iterations, the
    optimization reached a good result, but then it struggled to progress through
    the rest of the available iterations. Under such conditions, further progress
    in optimization is improbable because any gains cannot be but tiny at this point.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 展示了我们的优化过程。经过几次迭代后，优化达到了一个良好的结果，但随后在剩余的可用迭代中进展缓慢。在这种情况下，进一步优化进展的可能性很小，因为此时的任何收益都将微乎其微。
- en: '![](../Images/CH06_F06_Ryan2.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F06_Ryan2.png)'
- en: Figure 6.6 History of optimization results across the trials
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 展示了优化结果的历史记录
- en: 'Another useful plot depicts how the hyperparameters have been determinant in
    the resulting optimum settings:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的图表描绘了超参数如何决定最终的优化设置：
- en: '[PRE31]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Figure 6.7 shows the estimated importance of our optimization of the XGBoost
    algorithm. The results appear dominated by the `max_depth` hyperparameter and
    somehow by the subsample values. Such an outcome suggests that the algorithm is
    susceptible to the depth of the trees and that increasing the depth significantly
    affects the optimization results. This could indicate that the data contains complex
    patterns that require deeper trees to capture, and the sweet point of seven found
    by the optimization marks a point after which the algorithm starts overfitting.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 展示了我们优化 XGBoost 算法估计的重要性。结果显示，`max_depth` 超参数占据主导地位，同时某种程度上也受到子样本值的影响。这样的结果表明，算法对树的深度很敏感，并且增加深度会显著影响优化结果。这可能表明数据中包含复杂的模式，需要更深的树来捕捉，而优化中找到的七这个最佳点标志着算法开始过拟合的点。
- en: '![](../Images/CH06_F07_Ryan2.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F07_Ryan2.png)'
- en: Figure 6.7 A plot chart of hyperparameter estimated importance during the Optuna
    optimization process
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 Optuna优化过程中超参数估计重要性的图表
- en: Understanding why your XGBoost (or LightGBM) behaves better under certain conditions
    differs from problem to problem. However, being able to understand why, explain
    the reasons to others (such as the stakeholders), and take steps to adjust your
    data or optimization settings is indeed an invaluable feature offered by Optuna
    in comparison to other optimization methods.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么你的XGBoost（或LightGBM）在特定条件下表现更好，这因问题而异。然而，能够理解原因，向他人（如利益相关者）解释原因，并采取措施调整你的数据或优化设置，确实是Optuna相较于其他优化方法提供的一项宝贵功能。
- en: After completing the panoramic illustration on optimization techniques, we are
    left dealing with the case when you don’t want to set up anything complicated
    to make your machine learning algorithms work but you need some direction on how
    to make fast adjustments by trial and error.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成优化技术的全景图示后，我们面临的情况是，你可能不想设置任何复杂的东西来让你的机器学习算法工作，但你又需要一些指导，了解如何通过试错快速调整。
- en: 6.3.5 Manually setting hyperparameters
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 手动设置超参数
- en: Despite the efficiency of the previously described optimization strategies,
    you may not be surprised to read that we know that many practitioners still tune
    the settings of their models by intuition and trial and error. Such a procedure
    seems particularly well-grounded during the experimentation phase when you try
    to make everything work reasonably as you look for ways to improve your solution
    in various iterations. A thorough optimization is, therefore, left after the processing
    and experimentation iterations have been completed.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前描述的优化策略效率很高，但你可能不会惊讶地读到我们知道许多从业者仍然通过直觉和试错来调整他们模型的设置。这种程序在实验阶段似乎特别有根据，当你试图让一切合理工作，寻找改进解决方案的各种迭代方法时。因此，彻底的优化是在处理和实验迭代完成后留下的。
- en: The book’s appendices provide a comprehensive guide to the key parameters of
    the machine learning algorithms covered thus far. We begin with linear models,
    such as linear or logistic regressions, which can be effectively tuned using a
    grid search due to their limited number of parameters and ease of discretization.
    A table covers random forests and extremely randomized trees, as they share similar
    hyperparameters, being based on the same bootstrapped ensemble approach.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附录提供了对迄今为止涵盖的机器学习算法关键参数的全面指南。我们首先从线性模型开始，如线性或逻辑回归，由于参数数量有限且离散化容易，它们可以通过网格搜索有效地调整。一个表格涵盖了随机森林和极端随机树，因为它们具有相似的超参数，都是基于相同的自助集成方法。
- en: Regarding GBDTs, we have different sets of hyperparameters depending on the
    specific implementation. For your convenience, we have selected the most essential
    ones. Feel free to use them along with the proposed ranges for manual or automatic
    optimization. The guide starts with HistGradientBoosting and then covers XGBoost
    and LightGBM. It’s important to note that XGBoost has a larger set of relevant
    hyperparameters (you can find the complete list at [https://mng.bz/6e7e](https://mng.bz/6e7e)).
    Lastly, we include the list of hyperparameters for LightGBM, which differs slightly
    from XGBoost (you can find the complete list at [https://mng.bz/vK8q](https://mng.bz/vK8q)).
    This comprehensive guide will aid you in effectively tuning the machine learning
    algorithms and optimizing their performance based on the specific hyperparameter
    settings.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GBDTs，根据具体实现，我们有不同的超参数集。为了您的方便，我们选择了最关键的几个。您可以根据建议的范围手动或自动优化它们。指南从HistGradientBoosting开始，然后涵盖XGBoost和LightGBM。重要的是要注意，XGBoost有一组更大的相关超参数（你可以在[https://mng.bz/6e7e](https://mng.bz/6e7e)找到完整列表）。最后，我们还包括了LightGBM的超参数列表，它与XGBoost略有不同（你可以在[https://mng.bz/vK8q](https://mng.bz/vK8q)找到完整列表）。这份全面的指南将帮助您有效地调整机器学习算法，并根据特定的超参数设置优化它们的性能。
- en: 'As for manually tuning GBDTs, the models tend to work worst out of the box,
    so you should be aware of a few tricks of the trade. Let’s begin with a 1999 paper
    titled “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome
    Friedman. In this paper, Friedman discusses the tradeoff between the number of
    trees and the learning rate. It was observed that lower learning rates tend to
    result in higher optimal numbers of trees. Furthermore, it is advisable to reduce
    the learning rate when increasing the maximum depth of the decision trees in your
    model. This precautionary measure is because deeper trees introduce more complexity,
    potentially leading to overfitting. Overfitting occurs when the model becomes
    excessively tailored to the training data and performs poorly on unseen data.
    By simultaneously reducing the learning rate, you can mitigate this risk. This
    is because a lower learning rate translates to smaller and more cautious updates
    to the model. This gradual learning process allows for finer adjustments, helping
    the model strike a better balance between capturing complex relationships and
    avoiding overfitting.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 至于手动调整GBDT，模型通常在默认设置下表现最差，因此你应该了解一些行业技巧。让我们从Jerome Friedman于1999年发表的一篇题为“贪婪函数逼近：梯度提升机”的论文开始。在这篇论文中，Friedman讨论了树的数量和学习率之间的权衡。观察到较低的学习率往往会导致更高的最优树的数量。此外，当增加模型中决策树的最大深度时，建议降低学习率。这种预防措施是因为更深的树引入了更多的复杂性，可能导致过拟合。过拟合发生在模型过度定制于训练数据，在未见过的数据上表现不佳时。通过同时降低学习率，可以减轻这种风险。这是因为较低的学习率意味着模型更新更小、更谨慎。这种渐进的学习过程允许进行更精细的调整，帮助模型在捕捉复杂关系和避免过拟合之间取得更好的平衡。
- en: 'Another great resource for hints on manually adjusting parameters in a GBDT
    is Owen Zhang’s talk to the NYC Data Science Academy in 2015 titled “Winning Data
    Science Competitions.” Owen, previously a top competitor on Kaggle, provided a
    few interesting tips:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关于在GBDT中手动调整参数的宝贵资源是Owen Zhang在2015年向纽约市数据科学学院发表的题为“赢得数据科学竞赛”的演讲。Owen之前是Kaggle的顶尖竞争者，提供了一些有趣的建议：
- en: Decide the number of trees to use based on the dataset size (usually in the
    range of 100 to 1,000) and keep it fixed during the optimization. Prefer fewer
    trees to more.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据数据集大小（通常在100到1,000之间）决定使用的树的数量，并在优化过程中保持固定。更倾向于使用较少的树而不是更多的树。
- en: Test the learning rate in the range from 2 to 10 divided by the number of trees.
    Hence, for 1,000 trees, test learning rates in the interval from 0.002 to 0.01.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2到10之间除以树的数量范围内测试学习率。因此，对于1,000棵树，测试学习率在0.002到0.01的区间内。
- en: Test row sampling on 0.5, 0.75, 1.0 values.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在0.5、0.75、1.0的值上测试行采样。
- en: Test column sampling on 0.4, 0.6, 0.8, 1.0 values.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在0.4、0.6、0.8、1.0的值上测试列采样。
- en: Test max tree depth on 4, 6, 8, 10 values.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在4、6、8、10的值上测试最大树深度。
- en: Tune the minimum leaf weight/count as an approximate ratio of 3 over the square
    root of the percentage of the rarest class you have to predict. Therefore, if
    the class you need to predict has a 10% coverage in the data, you should set the
    minimum leaf weight/count to about 9\. This figure is calculated by dividing 3
    by the square root of 0.1 (since 10% coverage is 0.1 as a decimal).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最小叶子权重/计数调整为相对于预测的最稀疏类百分比的平方根的3倍的大约比例。因此，如果需要预测的类在数据中的覆盖率为10%，则应将最小叶子权重/计数设置为约9。这个数字是通过将3除以0.1的平方根（因为10%的覆盖率为小数0.1）计算得出的。
- en: In the concluding section, we keep exploring some ideas and tricks to master
    even better GBDTs when solving tabular data problems.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结部分，我们继续探讨一些想法和技巧，以在解决表格数据问题时更好地掌握GBDT。
- en: 6.4 Mastering gradient boosting
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 精通梯度提升
- en: Having discussed how gradient boosting works and its implementations, we close
    this chapter with suggestions about how to use gradient boosting at its best,
    understand how it works under the hood, and speed it up to cut time at training
    and prediction.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了梯度提升的工作原理及其实现之后，我们以关于如何最佳使用梯度提升、理解其内部工作原理以及加快训练和预测速度的建议来结束本章。
- en: 6.4.1 Deciding between XGBoost and LightGBM
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 在XGBoost和LightGBM之间做出选择
- en: 'When considering using gradient boosting for your data problem, XGBoost and
    LightGBM (along with HistGradientBoosting) are among the most popular and high-performing
    implementations of histogram gradient boosted machines. Despite their being so
    powerful, in our experience, you never can a priori go for XGBoost or LightGBM
    or just generally favor GBDTs in regards to other classical or deep learning solutions
    because of the no free lunch theorem in machine learning: there is no universal
    learning algorithm that performs best for all possible problems. Hence, stating
    that “XGBoost is all you need” for tabular data problems is surely a catchy phrase,
    but it may not always fit your specific problem or situation with data. GDBTs
    often tend to overperform other solutions for tabular data. Thus, starting with
    them, but not limited to them, is a good choice. Returning to specific implementations,
    while it is always advisable to test any algorithm on your data and make your
    own decisions, there are also a few other criteria to consider when deciding whether
    to try one implementation first or the other. We have validated them based on
    our own experience. They are summarized in table 6.1.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑使用梯度提升来解决您的数据问题时，XGBoost和LightGBM（以及HistGradientBoosting）是直方图梯度提升机的最流行和性能最高的实现之一。尽管它们如此强大，但根据我们的经验，您永远不能事先选择XGBoost或LightGBM，或者一般地偏好GBDT相对于其他经典或深度学习解决方案，因为机器学习中的“没有免费午餐定理”：没有一种通用的学习算法对所有可能的问题都表现最佳。因此，声称“XGBoost是您需要的所有东西”对于表格数据问题来说确实是一个吸引人的说法，但它可能并不总是适合您特定的数据问题或情况。GBDT通常倾向于在表格数据问题上优于其他解决方案。因此，从它们开始，但不仅限于它们，是一个不错的选择。回到具体的实现，虽然始终建议在您的数据上测试任何算法并做出自己的决定，但在决定是否首先尝试一个实现而不是另一个时，还有一些其他标准要考虑。我们已经根据我们的经验验证了它们。它们总结在表6.1中。
- en: Table 6.1 Criteria to consider when using GBDTs
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 使用GBDT时考虑的标准
- en: '| Type | Description |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Amount of data | XGBoost works fine for all tabular problems; LightGBM, because
    of its leaf-wise splitting method that can create deeper trees, tends to overfit
    more often with smaller datasets. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 数据量 | XGBoost对所有表格问题都表现良好；由于LightGBM的叶状分割方法可以创建更深的树，因此在使用较小的数据集时，它更容易过拟合。
    |'
- en: '| Scalability | XGBoost is more scalable and GPU ready; LightGBM struggles
    more. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | XGBoost的可扩展性和GPU就绪性更强；LightGBM面临更多挑战。 |'
- en: '| Speed of experimentation | On CPUs, LightGBM is undoubtedly faster than XGBoost.
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 实验速度 | 在CPU上，LightGBM无疑比XGBoost快。 |'
- en: The availability of large amounts of data is the first criterion to consider.
    LightGBM uses leaf-wise (vertical) growth, which can result in overfitting. The
    tendency to overfit the data available explains well the algorithm’s success in
    Kaggle competitions. Hence, LightGBM works better when you have a lot of data
    available. In contrast, XGBoost builds more robust models than LightGBM on smaller
    data samples.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据的可用性是首先要考虑的标准。LightGBM使用叶状（垂直）增长，这可能导致过拟合。对可用数据进行过拟合的趋势很好地解释了该算法在Kaggle竞赛中的成功。因此，当您有大量可用数据时，LightGBM表现得更好。相比之下，XGBoost在较小的数据样本上构建的模型比LightGBM更稳健。
- en: Another criterion to consider is whether you have access to multiple GPUs and
    strong CPUs or more limited access to computational resources. If you have plenty
    of resources, XGBoost is more scalable, making it a better option for use in institutional
    or business settings. However, if you prefer to focus on experimentation and feature
    engineering and cannot access GPUs, LightGBM makes more sense because of its faster
    training time. You can use the saved training time to improve the robustness of
    your final model. If you have limited resources, such as a stand-alone computer,
    you should consider that the training time for XGBoost increases linearly with
    the sample size, while LightGBM requires a much smaller fraction of training time.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要考虑的标准是您是否有权访问多个GPU和强大的CPU，或者只能有限地访问计算资源。如果您有大量资源，XGBoost的可扩展性更强，使其成为在机构或商业环境中使用的更好选择。然而，如果您更倾向于专注于实验和特征工程，并且无法访问GPU，那么由于LightGBM的训练时间更快，它更有意义。您可以使用节省下来的训练时间来提高最终模型的稳健性。如果您资源有限，例如只有一台独立的计算机，您应该考虑XGBoost的训练时间会随着样本大小的增加而线性增加，而LightGBM所需的训练时间则小得多。
- en: 6.4.2 Exploring tree structures
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 探索树结构
- en: As previously discussed, GBDTs are complicated algorithms, not inexplicable
    or unreplicable ones. You just need to reproduce the various decision trees they
    are made of in a more performing way and combine them to obtain your fast predictions.
    Both XGboost and LightGBM allow for exploring and extracting their model’s structure.
    In listing 6.16, we take a few steps to demonstrate that. After dumping an XGBoost
    simple solution on a JSON file, we navigate inside its structure like a graph,
    using a depth-first search strategy. In depth-first search, the algorithm explores
    each branch as far as possible before backtracking.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GBDTs是复杂的算法，并非无法解释或无法复制的。你只需要以更高效的方式重现它们所组成的各种决策树，并将它们结合起来以获得快速的预测。XGboost和LightGBM都允许探索和提取它们的模型结构。在列表6.16中，我们采取了一些步骤来展示这一点。在将XGBoost简单解决方案导出到JSON文件后，我们使用深度优先搜索策略在其结构内部导航，就像在图中一样。在深度优先搜索中，算法在回溯之前尽可能深入地探索每个分支。
- en: Taking a closer look at the code in listing 6.16, you can notice that in the
    `traverse_xgb_tree` function, the code recursively explores the tree by first
    traversing the left subtree (`tree['children'][0]`) and then the right subtree
    (`tree['children'][1]`). This is evident from the recursive calls `traverse_xgb_tree(tree['children'][0])`
    and `traverse_xgb_tree(tree['children'][1])`.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看列表6.16中的代码，你可以在`traverse_xgb_tree`函数中注意到，代码通过首先遍历左子树（`tree['children'][0]`）然后遍历右子树（`tree['children'][1]`）来递归地探索树。这从递归调用`traverse_xgb_tree(tree['children'][0])`和`traverse_xgb_tree(tree['children'][1])`中可以明显看出。
- en: Listing 6.16 Extracting XGBoost tree structure
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.16 提取XGBoost树结构
- en: '[PRE32]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Creates an XGBoost classifier limited to 10 estimators and trees of three
    levels
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个限制为10个估计量和三个级别的树的XGBoost分类器
- en: ② Extracts the XGBoost model from the pipeline
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从管道中提取XGBoost模型
- en: ③ Dumps the XGBoost model’s information (the booster) into a JSON file
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将XGBoost模型的信息（增强器）导出到一个JSON文件中
- en: ④ Creates a plot of the first tree in the ensemble
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建集成中第一棵树的图表
- en: ⑤ Retrieves the JSON structure from disk with the model’s information
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从磁盘检索包含模型信息的JSON结构
- en: ⑥ Prints the number of trees in the model and extracts the structure of the
    first tree
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 打印模型中的树的数量并提取第一棵树的结构
- en: ⑦ Function extracting various information from a split node in the tree structure
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 函数从树结构中的分割节点提取各种信息
- en: ⑧ Function extracting information from a leaf node in the tree structure
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 函数从树结构中的叶节点提取信息
- en: ⑨ Function traversing the tree structure recursively to extract paths
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 函数递归遍历树结构以提取路径
- en: 'The code trains a XGBoost model, saves its tree structure, processes the structure
    into a readable way, and presents the results to the user:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码训练一个XGBoost模型，保存其树结构，将结构处理成可读的方式，并将结果呈现给用户：
- en: '[PRE33]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Figure 6.8 compares the obtained outputs with the graphical representation of
    the complete tree, as provided by the `plot_tree` from the XGBoost package itself.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8将获得的输出与XGBoost包本身提供的完整树的图形表示进行了比较。
- en: '![](../Images/CH06_F08_Ryan2.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F08_Ryan2.png)'
- en: Figure 6.8 XGBoost’s `plot_tree` output
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 XGBoost的`plot_tree`输出
- en: From the 10 trees built in the model, the code presents the first tree and,
    among the 8 different paths available from the sample to the prediction leaf,
    it represents the first path. Visually, this path is the leftmost one. The path
    is made up of different nodes in sequence. The code reports the name of the used
    feature, the split branch origin from the previous node (in XGBoost, minor always
    stands for the left branch, and major is equal to the right branch), the cut threshold,
    the gain with respect to the objective function, and the resulting reduction in
    the sample given the split of the dataset. All this information allows you to
    perfectly replicate the results of every tree of an XGBoost model.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中构建的10棵树中，代码展示了第一棵树，并在从样本到预测叶节点的8条不同路径中，表示了第一条路径。从视觉上看，这条路径是最左侧的。路径由一系列不同的节点组成。代码报告了使用的特征名称、从上一个节点（在XGBoost中，小分支总是代表左分支，而大分支等于右分支）的分割分支起源、分割阈值、相对于目标函数的增益以及根据数据集的分割导致的样本减少。所有这些信息都允许你完美地复制XGBoost模型中每棵树的结果。
- en: We can also extract the same tree structure from LightGBM, though the approach
    is a bit different because the LightGBM package follows a few slightly different
    conventions. For instance, XGBoost always splits the minus than the threshold
    on the left; LightGBM instead, for each node, defines a rule using minus or major-equal
    and threshold and splits on the left if the rule is true and on the right if it
    is false.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从LightGBM中提取相同的树结构，尽管方法略有不同，因为LightGBM包遵循一些略微不同的约定。例如，XGBoost总是先在左边的阈值上分割；而LightGBM则相反，对于每个节点，使用减号或大于等于和阈值定义一个规则，如果规则为真则在左边分割，如果为假则在右边分割。
- en: Listing 6.17 Extracting LightGBM tree structure
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.17 提取LightGBM树结构
- en: '[PRE34]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Extracts the tree information from the LightGBM model booster
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从LightGBM模型增强器中提取树信息
- en: ② Extracts the structure of the first tree from the tree information
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从树信息中提取第一个树的结构
- en: ③ Plots the first tree in the ensemble using the plot_tree function
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用plot_tree函数绘制集成中的第一个树
- en: ④ Function extracting various information from a split node in the LightGBM
    tree structure
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 从LightGBM树结构中的分割节点提取各种信息的函数
- en: ⑤ Function extracting information from a leaf node in the LightGBM tree structure
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从LightGBM树结构中的叶节点提取信息的函数
- en: ⑥ Function recursively traversing the LightGBM tree structure to extract paths
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 递归遍历LightGBM树结构以提取路径的函数
- en: 'The results from this exploration report a path from the structure of the first
    decision tree in the ensemble:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本探索报告的结果报告了从集成中第一个决策树的结构：
- en: '[PRE35]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Figure 6.9 shows the entire tree plotted by the `plot_tree` function, this time
    from the LightGBM package.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9显示了`plot_tree`函数绘制的整个树，这次是从LightGBM包中。
- en: '![](../Images/CH06_F09_Ryan2.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F09_Ryan2.png)'
- en: Figure 6.9 LightGBM’s `plot_tree` output
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 LightGBM的`plot_tree`输出
- en: The tree is plotted horizontally from left to right. We can check that the path
    returned by the code is the uppermost one, ending in leaf 0\.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 树从左到右水平绘制。我们可以检查代码返回的路径是最高路径，以叶节点0结束。
- en: 6.4.3 Speeding up by GBDTs and compiling
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 通过GBDT和编译加速
- en: When the number of cases or the available features are many, even the faster
    LightGBM may take a long time to train a model on such data. At training time,
    you can overcome long waits by reducing the cases and features handled by smaller
    values of the parameter `subsample` for limiting the cases involved in building
    each decision tree and the parameter `colsample_bytree` for limiting the number
    of features considered at tree splitting time. However, reducing cases or features
    may not be optimal for getting the best results from your model. An alternative
    is using GPUs, which are widespread because they utilize deep learning models.
    GPUs can speed up training operations, especially with XGBoost, and, in a lesser
    but significant way, with LightGBM models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 当案例数量或可用特征很多时，即使是较快的LightGBM也可能需要很长时间来训练此类数据上的模型。在训练时，你可以通过减小参数`subsample`的值来减少处理的案例和特征，以限制每个决策树中涉及的案例数量，以及参数`colsample_bytree`来限制在树分割时考虑的特征数量，从而克服长时间的等待。然而，减少案例或特征可能不是从你的模型中获得最佳结果的最佳选择。另一种选择是使用GPU，因为它们广泛用于深度学习模型。GPU可以加速训练操作，特别是与XGBoost一起，以及在LightGBM模型中，虽然程度较小但仍然显著。
- en: 'With XGBoost, from a modeling point of view, using your GPU is quite straightforward:
    you just need to specify `"gpu_hist"` as a value for the `tree_method` parameter.
    With the new 2.0.0 version, such a method is, however, deprecated, and users can
    now instead specify the used device by the parameter `device`. You can set it
    to `"cpu"` for XGBoost to execute on CPU or `device="cuda"` as well as `device="gpu"`
    to have it run on a CUDA-powered GPU, which is the only option at the moment,
    but in the future, more GPU types will be supported. If you have multiple GPUs,
    you can specify their ordinal to choose a particular one; for instance, `device="cuda:1"`
    will execute on your second GPU device.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用XGBoost，从建模的角度来看，使用你的GPU相当简单：你只需将`tree_method`参数的值指定为`"gpu_hist"`。然而，在新2.0.0版本中，这种方法已被弃用，用户现在可以通过`device`参数指定使用的设备。你可以将其设置为`"cpu"`以让XGBoost在CPU上执行，或者设置为`device="cuda"`以及`device="gpu"`以使其在CUDA支持的GPU上运行，目前这是唯一的选择，但将来将支持更多GPU类型。如果你有多个GPU，你可以指定它们的序号来选择特定的一个；例如，`device="cuda:1"`将在你的第二个GPU设备上执行。
- en: For XGBoost to perform, you need at least CUDA 11.00 installed and a GPU with
    a compute capability 5.0\. If you have more GPUs available, you can specify which
    to use by the `gpu_id` parameter, which represents the GPU device ordinal reported
    by CUDA runtime (usually set to zero if you have a single GPU). In this way, XGBoost
    moves the growth of decision trees to the GPU memory and processors, thus obtaining
    a relevant speed of operations, especially feature histograms, as described in
    the paper “Accelerating the XGBoost Algorithm Using GPU Computing” by Mitchell
    and Frank ([https://peerj.com/articles/cs-127/](https://peerj.com/articles/cs-127/)).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使XGBoost运行，您至少需要安装CUDA 11.00以及具有5.0计算能力的GPU。如果您有更多的GPU可用，您可以通过`gpu_id`参数指定使用哪一个，该参数代表CUDA运行时报告的GPU设备序号（如果您只有一个GPU，通常设置为0）。这样，XGBoost将决策树的生长移动到GPU内存和处理器中，从而获得相关的操作速度，特别是特征直方图，如Mitchell和Frank在论文“Using
    GPU Computing to Accelerate the XGBoost Algorithm”中所述（[https://peerj.com/articles/cs-127/](https://peerj.com/articles/cs-127/)）。
- en: 'Once GPU trains a model, it can be used for prediction on a machine with a
    GPU. All you have to set is the `predictor` parameter to `gpu_predictor` or to
    `cpu_predictor` if you want to use your CPU. Selecting the predictor parameter
    to GPU can also speed up things when you have to compute SHAP values and SHAP
    interaction values for model interpretability:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦GPU训练了一个模型，它就可以在具有GPU的机器上进行预测。您只需设置`predictor`参数为`gpu_predictor`或如果您想使用CPU，则设置为`cpu_predictor`。当您需要计算模型可解释性的SHAP值和SHAP交互值时，选择GPU作为预测器参数也可以加快速度：
- en: '[PRE36]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Although using a GPU with XGBoost is easy, it becomes a little bit trickier
    with LightGBM. LightGBM doesn’t have an option for GPU running but rather requires
    a special version of itself to be compiled for the purpose. Depending on your
    operating system (Windows, Linux/Ubuntu, MacOS), the compilation may be less or
    more challenging. Instructions are available at [https://mng.bz/nRg5](https://mng.bz/nRg5)
    for POSIX systems and at [https://mng.bz/vK8p](https://mng.bz/vK8p) for Windows
    systems. However, if you have all the prerequisites ready on your system as stated
    by the instructions at [https://mng.bz/4aJg](https://mng.bz/4aJg), you can just
    require to directly install it using the pip install instruction on your shell
    or command prompt:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用GPU与XGBoost一起使用很简单，但与LightGBM一起使用就变得有点复杂。LightGBM没有GPU运行的选项，而是需要为其编译一个特殊版本。根据您的操作系统（Windows、Linux/Ubuntu、MacOS），编译可能更具挑战性。对于POSIX系统，请参阅[https://mng.bz/nRg5](https://mng.bz/nRg5)的说明，对于Windows系统，请参阅[https://mng.bz/vK8p](https://mng.bz/vK8p)的说明。然而，如果您已按照[https://mng.bz/4aJg](https://mng.bz/4aJg)中的说明准备好所有先决条件，您可以直接在shell或命令提示符中使用pip
    install指令进行安装：
- en: '[PRE37]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Once everything has been installed, you need to set the parameter `device` to
    `gpu` Don’t expect astonishing performance improvements, however. As stated by
    LightGBM authors (see [https://mng.bz/vK8p](https://mng.bz/vK8p)), the best results
    are obtained on large-scale and dense datasets because of the inefficient data
    turnover that causes latencies when working on smaller datasets. In addition,
    setting a lower number of bins for the histogram algorithm will make the GPU work
    more efficiently with the LightGBM. The suggestion is to set `max_bin=15` and
    single precision, `gpu_use_dp=false`, for the best performances.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 一切安装完成后，您需要将参数`device`设置为`gpu`。不过，不要期待惊人的性能提升。正如LightGBM作者所述（见[https://mng.bz/vK8p](https://mng.bz/vK8p)），在大型和密集数据集上可以获得最佳结果，因为不高效的数据周转会导致在处理小型数据集时产生延迟。此外，为直方图算法设置更少的bins数量将使GPU与LightGBM更有效地工作。建议将`max_bin=15`和单精度，`gpu_use_dp=false`设置为最佳性能。
- en: GPUs are quite useful for speeding up training, but there are more options at
    prediction time. With tree structures so readily available, as we have seen in
    the previous section, it has been possible for specific projects to use such information
    for rebuilding prediction trees using more performing programming languages such
    as C, JAVA, or LLVM that can turn your model into pure assembly code. Such tree-compiling
    projects aim for fast prediction and easier deployment. Examples are Treelite
    ([https://github.com/dmlc/treelite](https://github.com/dmlc/treelite)), which
    can read models produced by XGBoost, LightGBM, and even Scikit-learn, and lleaves
    ([https://github.com/siboehm/lleaves](https://github.com/siboehm/lleaves)), which
    is a project for LightGBM only.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: GPU对于加速训练非常有用，但在预测时还有更多选项。正如我们在上一节中看到的，由于树结构如此容易获得，一些特定项目已经可以使用这种信息来重建预测树，使用性能更好的编程语言，如C、JAVA或LLVM，这些语言可以将你的模型转换为纯汇编代码。这样的树编译项目旨在实现快速预测和更容易部署。例如，Treelite
    ([https://github.com/dmlc/treelite](https://github.com/dmlc/treelite)) 可以读取由XGBoost、LightGBM甚至Scikit-learn生成的模型，还有lleaves
    ([https://github.com/siboehm/lleaves](https://github.com/siboehm/lleaves))，这是一个仅针对LightGBM的项目。
- en: 'Starting from Treelite, this project strives to be a universal model exchange
    and serialization format for decision tree forests. It compiles your GBDT into
    C or Java with the least possible dependencies, so you can easily deploy it into
    any system. To have it tested, you must install a few packages at the command
    line: `pip` `install` `tl2cgen treelite treelite_runtime`.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 从Treelite开始，这个项目致力于成为决策树森林的通用模型交换和序列化格式。它将你的GBDT编译成C或Java，依赖性尽可能少，因此你可以轻松地将它部署到任何系统。为了进行测试，你必须在命令行中安装几个包：`pip
    install tl2cgen treelite treelite_runtime`。
- en: Listing 6.18 XGBoost prediction speedup by Treelite
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.18 Treelite加速XGBoost预测
- en: '[PRE38]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Saves the XGBoost model to a JSON file
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将XGBoost模型保存到JSON文件
- en: ② Loads the XGBoost model in Treelite format from the JSON file
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从JSON文件加载Treelite格式的XGBoost模型
- en: ③ Generates C code from the Treelite model and exports it as a shared library
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从Treelite模型生成C代码并将其导出为共享库
- en: ④ Transforms the input data using the preprocessing steps defined in the pipeline
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用管道中定义的预处理步骤转换输入数据
- en: ⑤ Creates a Treelite DMatrix from the transformed data, compatible with the
    exported Treelite model
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从转换后的数据创建与导出Treelite模型兼容的Treelite DMatrix
- en: The result is a compiled model that, inside a Python script, can return predictions
    in a much faster fashion. Predictors must always be transformed beforehand since
    the pipeline is not part of the compiling. Only the model is. In addition, you
    also have to convert the data in DMatrix format, the native XGBoost data format,
    before it is sent to the compiled model.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个编译后的模型，在Python脚本中可以以更快的速度返回预测。预测器必须在转换之前进行转换，因为管道不是编译的一部分。只有模型是。此外，你还需要在将数据发送到编译模型之前将其转换为DMatrix格式，这是XGBoost的本地数据格式。
- en: Developed by Simon Boehm, lleaves promises x10 speed up by LLVM compiling into
    assembly based on the text tree structure that can be outputted from a LightGBM
    model. After having installed the package by a `pip` `install` `leaves` instruction
    on the command line, you can obtain a speed up by following these steps.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 由Simon Boehm开发，lleaves通过基于可以从LightGBM模型输出的文本树结构使用LLVM编译到汇编，承诺实现x10的速度提升。通过在命令行上使用`pip
    install leaves`指令安装包后，你可以按照以下步骤获得加速。
- en: Listing 6.19 LightGBM prediction speedup by `lleaves`
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.19 `lleaves`加速LightGBM预测
- en: '[PRE39]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① Saves the LightGBM model to a text file
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将LightGBM模型保存到文本文件
- en: ② Loads the LightGBM model using the lleaves library
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用lleaves库加载LightGBM模型
- en: ③ Compiles the loaded LightGBM model into LLVM representation
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将加载的LightGBM模型编译成LLVM表示
- en: ④ Transforms the input data using the preprocessing steps defined in the pipeline
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用管道中定义的预处理步骤转换输入数据
- en: Also, in this case, the model is compiled and can predict in a faster way inside
    a Python script. From a general point of view, `lleaves`, though limited only
    to LightGBM, is a compiling solution that requires many fewer settings and specifications
    from the user, resulting in a much simpler and straightforward usage.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型被编译，可以在Python脚本中以更快的速度进行预测。从一般的角度来看，尽管`lleaves`仅限于LightGBM，但它是一个需要用户设置和指定更少的编译解决方案，从而实现更简单、更直接的使用。
- en: Summary
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Among processing problems, missing data is one of the most problematic. If your
    data is MCR or is just MAR because missing patterns are related to the other features,
    multivariate imputation can use the correlations among predictors in a dataset
    to impute missing values.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理问题中，缺失数据是其中最棘手的问题之一。如果你的数据是 MCR 或只是 MAR（因为缺失模式与其他特征相关），多元插补可以使用数据集中预测变量的相关性来插补缺失值。
- en: Both XGBoost and LightGBM algorithms automatically handle missing data by assigning
    them to the side that minimizes the loss function the most in each split.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 和 LightGBM 算法自动处理缺失数据，通过将它们分配到每个分割中损失函数最小化的那一侧。
- en: When a categorical presents high cardinality because of its many labels, you
    can use target encoding, which gained popularity in Kaggle competitions. Target
    encoding is a way to transform the values in a categorical feature into their
    corresponding expected target values.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个分类特征由于许多标签而呈现高基数时，你可以使用目标编码，这在 Kaggle 竞赛中变得流行。目标编码是一种将分类特征中的值转换为它们对应的预期目标值的方法。
- en: PDP is a model-agnostic chart technique that explains how features and the target
    are related by means of the model you have trained. It is beneficial because it
    helps you better model the relationship between the predictive feature and the
    target if you notice it is nonlinear and complex.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDP 是一种模型无关的图表技术，它通过你所训练的模型解释特征和目标之间的关系。它是有益的，因为它可以帮助你更好地建模预测特征和目标之间的关系，如果你注意到它是非线性且复杂的。
- en: XGBoost, thanks to packages such as XGBFIR, can inform you about the most important
    interactions between predictive features.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost，得益于 XGBFIR 等包，可以告诉你预测特征之间最重要的交互。
- en: 'By employing effective feature selection techniques, you can pinpoint and retain
    the most relevant features that contribute significantly to the machine learning
    process. Standard techniques to handle feature selection are stability selection
    based on L1 regularization for linear models, iterative selection, and Boruta
    for tree ensembles:'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过采用有效的特征选择技术，你可以确定并保留对机器学习过程贡献显著的最重要的特征。处理特征选择的标准技术是基于 L1 正则化的稳定性选择（用于线性模型）、迭代选择和
    Boruta（用于树集成）：
- en: Based on L1 regularization, stability selection aims to identify features that
    consistently appear as important across multiple subsets, indicating their robustness
    and reducing the likelihood of selecting features by chance or noise.
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 L1 正则化，稳定性选择旨在识别在多个子集中始终出现为重要的特征，这表明它们的鲁棒性，并减少随机或噪声选择特征的可能性。
- en: Boruta is a procedure to determine if a feature is relevant in a machine learning
    problem by relying on the internal parameters of the model, such as coefficients
    in linear models or importance values based on gain, such as in decision trees
    and their ensembles.
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boruta 是一种通过依赖于模型内部参数（如线性模型中的系数或基于增益的重要性值，如决策树及其集成）来确定特征在机器学习问题中是否相关的程序。
- en: Iterative selection additions by forward selection, or removes by backward elimination,
    features from your selection based on their performance on the prediction in a
    greedy fashion, leaving only the essential features for your prediction.
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过前向选择或后向消除，迭代选择添加或删除特征，基于它们在预测中的性能，以贪婪的方式从你的选择中提取特征，只留下对预测至关重要的特征。
- en: 'By optimizing hyperparameters, you can gain another performance boost to your
    classical machine learning model. Apart from manually setting the hyperparameters,
    depending on the model you are working on, grid search, random search, successive
    halving, and Bayesian optimization are popular optimization methods within the
    data science community:'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过优化超参数，你可以给你的经典机器学习模型带来另一个性能提升。除了手动设置超参数外，根据你正在工作的模型，网格搜索、随机搜索、连续减半和贝叶斯优化是数据科学社区中流行的优化方法：
- en: Grid search simply works by searching through all the possible combinations
    of hyperparameters’ values. For every hyperparameter you want to test, you pick
    a sequence of values and iterate through all their combinations exhaustively.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索通过遍历所有可能的超参数值组合来简单工作。对于你想要测试的每个超参数，你选择一个值序列，并彻底迭代它们的所有组合。
- en: Random search optimization decides what values to test by randomly drawing them
    from the search space. The technique is particularly effective if you know little
    about your hyperparameters, if there are many of them, and if some are irrelevant
    but you don’t know which ones.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索优化通过从搜索空间中随机抽取值来决定要测试的值。如果你对超参数了解不多，如果有很多超参数，以及如果某些参数无关紧要但你不知道是哪些，这种技术特别有效。
- en: Successive halving is a wrapper of the previously discussed strategies. It works
    as a tournament between sets of hyperparameters, where first, they are tested
    using a few computational resources. Then, only a fraction of the best is further
    tested using more resources. In the end, there will be only one surviving set
    of hyperparameters.
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续减半是之前讨论策略的包装器。它作为一个超参数集之间的锦标赛工作，首先，它们使用少量计算资源进行测试。然后，只有最好的部分进一步使用更多资源进行测试。最后，将只剩下一组幸存的超参数。
- en: Bayesian optimization uses informed search to find the best set of hyperparameters.
    It builds a model of the hyperparameter’s behavior based on prior knowledge of
    how it works on the data problem. Then it sets a series of experiments to explore
    further and refine its own internal model, exploit the previous trials, and validate
    the actual performances of a solution.
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化通过有信息搜索来寻找最佳的超参数集。它基于对超参数在数据问题上的工作原理的先验知识，构建了超参数行为的模型。然后，它设置一系列实验来进一步探索并完善其内部模型，利用之前的试验，并验证解决方案的实际性能。
- en: Both XGBoost and LightGBM have specific settings and options that are not commonly
    found in other machine learning algorithms, such as the possibility of extracting
    and representing their internal structure and speeding up their execution by GPU
    use and compiling.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost和LightGBM都有特定的设置和选项，这在其他机器学习算法中并不常见，例如提取和表示它们的内部结构以及通过GPU使用和编译来加速它们的执行。
