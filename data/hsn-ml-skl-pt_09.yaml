- en: Chapter 8\. Unsupervised Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 无监督学习技术
- en: 'Yann LeCun, Turing Award winner and Meta’s Chief AI Scientist, famously said
    that “if intelligence was a cake, unsupervised learning would be the cake, supervised
    learning would be the icing on the cake, and reinforcement learning would be the
    cherry on the cake” (NeurIPS 2016). In other words, there is a huge potential
    in unsupervised learning that we have only barely started to sink our teeth into.
    Indeed, the vast majority of the available data is unlabeled: we have the input
    features **X**, but we do not have the labels **y**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun，图灵奖得主和Meta的首席人工智能科学家，曾著名地表示：“如果智能是一块蛋糕，无监督学习就是蛋糕本身，监督学习就是蛋糕上的糖霜，强化学习就是蛋糕上的樱桃”（NeurIPS
    2016）。换句话说，无监督学习具有巨大的潜力，我们才刚刚开始涉足其中。确实，大部分可用的数据都是未标记的：我们拥有输入特征**X**，但没有标签**y**。
- en: Say you want to create a system that will take a few pictures of each item on
    a manufacturing production line and detect which items are defective. You can
    fairly easily create a system that will take pictures automatically, and this
    might give you thousands of pictures every day. You can then build a reasonably
    large dataset in just a few weeks. But wait, there are no labels! If you want
    to train a regular binary classifier that will predict whether an item is defective
    or not, you will need to label every single picture as “defective” or “normal”.
    This will generally require human experts to sit down and manually go through
    all the pictures. This is a long, costly, and tedious task, so it will usually
    only be done on a small subset of the available pictures. As a result, the labeled
    dataset will be quite small, and the classifier’s performance will be disappointing.
    Moreover, every time the company makes any change to its products, the whole process
    will need to be started over from scratch. Wouldn’t it be great if the algorithm
    could just exploit the unlabeled data without needing humans to label every picture?
    Enter unsupervised learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想创建一个系统，该系统能够对制造生产线上的每个物品拍摄几张照片，并检测哪些物品是次品的。你可以相当容易地创建一个自动拍照的系统，这可能会每天给你成千上万的照片。然后你可以在几周内构建一个相当大的数据集。但是等等，没有标签！如果你想训练一个常规的二分类器，预测一个物品是否是次品，你需要将每张照片都标记为“次品”或“正常”。这通常需要专家坐下来手动检查所有照片。这是一项漫长、昂贵且繁琐的任务，因此通常只会对可用照片的一小部分进行操作。结果，标记的数据集将非常小，分类器的性能将令人失望。此外，每次公司对其产品进行任何更改时，整个流程都需要从头开始。如果算法能够仅利用未标记的数据，而不需要人为标记每张照片，那岂不是很好？这就是无监督学习的用武之地。
- en: 'In [Chapter 7](ch07.html#dimensionality_chapter) we looked at the most common
    unsupervised learning task: dimensionality reduction. In this chapter we will
    look at a few more unsupervised tasks:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.html#dimensionality_chapter)中，我们探讨了最常见的无监督学习任务：降维。在本章中，我们将探讨一些其他无监督学习任务：
- en: Clustering
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: The goal is to group similar instances together into *clusters*. Clustering
    is a great tool for data analysis, customer segmentation, recommender systems,
    search engines, image segmentation, semi-supervised learning, dimensionality reduction,
    and more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将相似的实例分组到*簇*中。聚类是数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等领域的强大工具。
- en: Anomaly detection (also called *outlier detection*)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测（也称为*离群值检测*）
- en: The objective is to learn what “normal” data looks like, and then use that to
    detect abnormal instances. These instances are called *anomalies*, or *outliers*,
    while the normal instances are called *inliers*. Anomaly detection is useful in
    a wide variety of applications, such as fraud detection, detecting defective products
    in manufacturing, identifying new trends in time series, or removing outliers
    from a dataset before training another model, which can significantly improve
    the performance of the resulting model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习“正常”数据看起来是什么样子，然后利用这一点来检测异常实例。这些实例被称为*异常*或*离群值*，而正常实例被称为*内群值*。异常检测在广泛的领域中都有用，例如欺诈检测、在制造中检测次品产品、识别时间序列中的新趋势，或在训练另一个模型之前从数据集中移除离群值，这可以显著提高最终模型的表现。
- en: Density estimation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 密度估计
- en: 'This is the task of estimating the *probability density function* (PDF) of
    the random process that generated the dataset.⁠^([1](ch08.html#id1959)) Density
    estimation is commonly used for anomaly detection: instances located in very low-density
    regions are likely to be anomalies. It is also useful for data analysis and visualization.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项估计生成数据集的随机过程的*概率密度函数*（PDF）的任务。⁠^([1](ch08.html#id1959)) 密度估计常用于异常检测：位于非常低密度区域的实例很可能是异常。它也适用于数据分析和可视化。
- en: Ready for some cake? We will start with two clustering algorithms, *k*-means
    and DBSCAN, then we’ll discuss Gaussian mixture models and see how they can be
    used for density estimation, clustering, and anomaly detection.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好一些蛋糕了吗？我们将从两种聚类算法*k*-means和DBSCAN开始，然后我们将讨论高斯混合模型，并看看它们如何用于密度估计、聚类和异常检测。
- en: 'Clustering Algorithms: k-means and DBSCAN'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法：k-means和DBSCAN
- en: 'As you enjoy a hike in the mountains, you stumble upon a plant you have never
    seen before. You look around and you notice a few more. They are not identical,
    yet they are sufficiently similar for you to know that they most likely belong
    to the same species (or at least the same genus). You may need a botanist to tell
    you what species that is, but you certainly don’t need an expert to identify groups
    of similar-looking objects. This is called *clustering*: it is the task of identifying
    similar instances and assigning them to *clusters*, or groups of similar instances.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在山中徒步旅行时，你偶然发现了一种你以前从未见过的植物。你环顾四周，注意到还有几株。它们并不完全相同，但足够相似，让你知道它们很可能属于同一物种（或者至少同一属）。你可能需要一个植物学家告诉你这是什么物种，但你当然不需要专家来识别外观相似的物体组。这被称为*聚类*：它是识别相似实例并将它们分配到*聚类*或相似实例组的任务。
- en: 'Just like in classification, each instance gets assigned to a group. However,
    unlike classification, clustering is an unsupervised task, there are no labels,
    so the algorithm needs to figure out on its own how to group instances. Consider
    [Figure 8-1](#classification_vs_clustering_plot): on the left is the iris dataset
    (introduced in [Chapter 4](ch04.html#linear_models_chapter)), where each instance’s
    species (i.e., its class) is represented with a different marker. It is a labeled
    dataset, for which classification algorithms such as logistic regression, SVMs,
    or random forest classifiers are well suited. On the right is the same dataset,
    but without the labels, so you cannot use a classification algorithm anymore.
    This is where clustering algorithms step in: many of them can easily detect the
    lower-left cluster. It is also quite easy to see with our own eyes, but it is
    not so obvious that the upper-right cluster is composed of two distinct subclusters.
    That said, the dataset has two additional features (sepal length and width) that
    are not represented here, and clustering algorithms can make good use of all features,
    so in fact they identify the three clusters fairly well (e.g., using a Gaussian
    mixture model, only 5 instances out of 150 are assigned to the wrong cluster).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在分类中一样，每个实例都被分配到一个组。然而，与分类不同，聚类是一个无监督的任务，没有标签，因此算法需要自己找出如何分组实例。考虑[图8-1](#classification_vs_clustering_plot)：左侧是鸢尾花数据集（在第4章[线性模型](ch04.html#linear_models_chapter)中介绍），其中每个实例的物种（即其类别）用不同的标记表示。这是一个有标签的数据集，对于逻辑回归、SVM或随机森林分类器等分类算法非常适合。右侧是相同的数据集，但没有标签，因此不能再使用分类算法。这就是聚类算法介入的地方：许多算法可以轻松地检测到左下角的聚类。用我们的眼睛看也很容易，但右上角的聚类由两个不同的子聚类组成这一点并不明显。尽管如此，数据集还有两个额外的特征（花瓣长度和宽度）在这里没有表示，聚类算法可以利用所有特征，因此实际上它们相当好地识别了三个聚类（例如，使用高斯混合模型，只有150个实例中的5个被分配到错误的聚类）。
- en: '![Diagram comparing classification (left) with labeled data and clustering
    (right) with unlabeled data on the iris dataset, highlighting how clustering identifies
    groups without prior labels.](assets/hmls_0801.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![比较分类（左侧）与有标签数据以及聚类（右侧）与无标签数据在鸢尾花数据集上的图表，突出聚类如何识别没有先前标签的组。](assets/hmls_0801.png)'
- en: 'Figure 8-1\. Classification (left) versus clustering (right): in clustering,
    the dataset is unlabeled so the algorithm must identify the clusters without guidance'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1. 分类（左侧）与聚类（右侧）：在聚类中，数据集是无标签的，因此算法必须在没有指导的情况下识别聚类
- en: 'Clustering is used in a wide variety of applications, including:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类被广泛应用于各种应用中，包括：
- en: Customer segmentation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 客户细分
- en: You can cluster your customers based on their purchases and their activity on
    your website. This is useful to understand who your customers are and what they
    need, so you can adapt your products and marketing campaigns to each segment.
    For example, customer segmentation can be useful in *recommender systems* to suggest
    content that other users in the same cluster enjoyed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据客户的购买和他们在您网站上的活动对客户进行聚类。这有助于了解您的客户是谁以及他们需要什么，因此您可以调整您的产品和营销活动以适应每个细分市场。例如，客户细分在*推荐系统*中非常有用，可以建议其他同一簇用户喜欢的相关内容。
- en: Data analysis
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析
- en: When you analyze a new dataset, it can be helpful to run a clustering algorithm,
    and then analyze each cluster separately.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当您分析一个新的数据集时，运行聚类算法然后分别分析每个簇可能会有所帮助。
- en: Dimensionality reduction
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简
- en: Once a dataset has been clustered, it is usually possible to measure each instance’s
    *affinity* with each cluster; affinity is any measure of how well an instance
    fits into a cluster. Each instance’s feature vector **x** can then be replaced
    with the vector of its cluster affinities. If there are *k* clusters, then this
    vector is *k*-dimensional. The new vector is typically much lower-dimensional
    than the original feature vector, but it can preserve enough information for further
    processing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被聚类，通常可以测量每个实例与每个簇的*亲和度*；亲和度是衡量实例如何适合簇的任何度量。然后每个实例的特征向量**x**可以被其簇亲和度的向量所替代。如果有*k*个簇，那么这个向量是*k*维的。新向量通常比原始特征向量低得多维，但它可以保留足够的信息以供进一步处理。
- en: Feature engineering
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: The cluster affinities can often be useful as extra features. For example, we
    used *k*-means in [Chapter 2](ch02.html#project_chapter) to add geographic cluster
    affinity features to the California housing dataset, and they helped us get better
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 簇亲和度通常可以作为额外的特征很有用。例如，我们在[第2章](ch02.html#project_chapter)中使用了*k*均值聚类来向加利福尼亚住房数据集添加地理簇亲和度特征，这有助于我们获得更好的性能。
- en: '*Anomaly detection* (also called *outlier detection*)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*异常检测*（也称为*离群值检测*）'
- en: Any instance that has a low affinity to all the clusters is likely to be an
    anomaly. For example, if you have clustered the users of your website based on
    their behavior, you can detect users with unusual behavior, such as an unusual
    number of requests per second.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 任何与所有簇亲和度低的实例很可能是一个异常。例如，如果您根据用户的行为对您的网站用户进行了聚类，您可以检测到具有不寻常行为（如每秒请求数量异常）的用户。
- en: Semi-supervised learning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习
- en: If you only have a few labels, you could perform clustering and propagate the
    labels to all the instances in the same cluster. This technique can greatly increase
    the number of labels available for a subsequent supervised learning algorithm,
    and thus improve its performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只有少量标签，您可以对聚类进行操作并将标签传播到同一簇的所有实例。这种技术可以大大增加后续监督学习算法可用的标签数量，从而提高其性能。
- en: Search engines
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎
- en: Some search engines let you search for images that are similar to a reference
    image. To build such a system, you would first apply a clustering algorithm to
    all the images in your database; similar images would end up in the same cluster.
    Then when a user provides a reference image, all you’d need to do is use the trained
    clustering model to find this image’s cluster, and you could then simply return
    all the images from this cluster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些搜索引擎允许您搜索与参考图像相似的图像。要构建这样的系统，您首先需要将聚类算法应用于数据库中的所有图像；相似的图像最终会落在同一个簇中。然后当用户提供一个参考图像时，您只需使用训练好的聚类模型来找到这个图像的簇，然后您可以简单地返回这个簇中的所有图像。
- en: Image segmentation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割
- en: By clustering pixels according to their color, then replacing each pixel’s color
    with the mean color of its cluster, it is possible to considerably reduce the
    number of different colors in an image. Image segmentation is used in many object
    detection and tracking systems, as it makes it easier to detect the contour of
    each object.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过根据颜色对像素进行聚类，然后用其簇的平均颜色替换每个像素的颜色，可以显著减少图像中的不同颜色数量。图像分割在许多目标检测和跟踪系统中使用，因为它使得检测每个对象的轮廓变得更容易。
- en: 'There is no universal definition of what a cluster is: it really depends on
    the context, and different algorithms will capture different kinds of clusters.
    Some algorithms look for instances centered around a particular point, called
    a *centroid*. Others look for continuous regions of densely packed instances:
    these clusters can take on any shape. Some algorithms are hierarchical, looking
    for clusters of clusters. And the list goes on.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于簇的定义没有普遍的定义：它实际上取决于上下文，不同的算法将捕获不同类型的簇。一些算法寻找围绕特定点的实例，称为**质心**。其他算法寻找密集实例的连续区域：这些簇可以具有任何形状。一些算法是层次化的，寻找簇的簇。等等。
- en: In this section, we will look at two popular clustering algorithms, *k*-means
    and DBSCAN, and explore some of their applications, such as nonlinear dimensionality
    reduction, semi-supervised learning, and anomaly detection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两种流行的聚类算法，*k*-means和DBSCAN，并探索它们的一些应用，例如非线性降维、半监督学习和异常检测。
- en: k-Means Clustering
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-Means 聚类
- en: 'Consider the unlabeled dataset represented in [Figure 8-2](#blobs_plot): you
    can clearly see five blobs of instances. The *k*-means algorithm is a simple algorithm
    capable of clustering this kind of dataset very quickly and efficiently, often
    in just a few iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957
    as a technique for pulse-code modulation, but it was only [published](https://homl.info/36)
    outside of the company in 1982.⁠^([2](ch08.html#id1973)) In 1965, Edward W. Forgy
    had published virtually the same algorithm, so *k*-means is sometimes referred
    to as the Lloyd–Forgy algorithm.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[图8-2](#blobs_plot)中表示的未标记数据集：您可以看到五个实例团块。k-means算法是一个简单的算法，能够非常快速和高效地对这类数据集进行聚类，通常只需几次迭代。它由贝尔实验室的Stuart
    Lloyd于1957年提出，作为一种脉冲编码调制技术，但直到1982年才在公司外部[发表](https://homl.info/36)。⁠^([2](ch08.html#id1973))
    1965年，Edward W. Forgy发表了几乎相同的算法，因此k-means有时被称为Lloyd-Forgy算法。
- en: '![Scatterplot displaying five distinct clusters of data points, illustrating
    an unlabeled dataset suitable for k-means clustering.](assets/hmls_0802.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示五个不同的数据点簇，说明一个适合k-means聚类的未标记数据集](assets/hmls_0802.png)'
- en: Figure 8-2\. An unlabeled dataset composed of five blobs of instances
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 由五个实例团块组成的未标记数据集
- en: 'Let’s train a *k*-means clusterer on this dataset. It will try to find each
    blob’s center and assign each instance to the closest blob:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这个数据集上训练一个*k*-means聚类器。它将尝试找到每个团块的中心并将每个实例分配到最近的团块：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that you have to specify the number of clusters *k* that the algorithm
    must find. In this example, it is pretty obvious from looking at the data that
    *k* should be set to 5, but in general it is not that easy. We will discuss this
    shortly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您必须指定算法必须找到的簇数*k*。在这个例子中，从数据中可以看出*k*应该设置为5，但通常并不那么容易。我们将在稍后讨论这个问题。
- en: 'Each instance will be assigned to one of the five clusters. In the context
    of clustering, an instance’s *label* is the index of the cluster to which the
    algorithm assigns this instance; this is not to be confused with the class labels
    in classification, which are used as targets (remember that clustering is an unsupervised
    learning task). The `KMeans` instance preserves the predicted labels of the instances
    it was trained on, available via the `labels_` instance variable:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实例将被分配到五个簇中的某一个。在聚类的上下文中，实例的**标签**是算法分配给该实例的簇的索引；这不要与分类中的类标签混淆，类标签用作目标（记住聚类是一个无监督学习任务）。`KMeans`实例保留了它在训练过程中预测的实例标签，这些标签通过`labels_`实例变量可用：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also take a look at the five centroids that the algorithm found:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看看算法找到的五个质心：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can easily assign new instances to the cluster whose centroid is closest:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地将新实例分配到最近的质心所属的簇：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you plot the cluster’s decision boundaries, you get a Voronoi tessellation:
    see [Figure 8-3](#voronoi_plot), where each centroid is represented with an ⓧ.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您绘制簇的决策边界，您将得到一个Voronoi镶嵌图：参见[图8-3](#voronoi_plot)，其中每个质心都用一个ⓧ表示。
- en: '![Diagram showing Voronoi tessellation with k-means centroids marked, highlighting
    decision boundaries between clusters.](assets/hmls_0803.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![显示带有k-means质心标记的Voronoi镶嵌图，突出显示簇之间的决策边界](assets/hmls_0803.png)'
- en: Figure 8-3\. k-means decision boundaries (Voronoi tessellation)
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. k-means决策边界（Voronoi镶嵌图）
- en: The vast majority of the instances were clearly assigned to the appropriate
    cluster, but a few instances were probably mislabeled, especially near the boundary
    between the top-left cluster and the central cluster. Indeed, the *k*-means algorithm
    does not behave very well when the blobs have very different diameters because
    all it cares about when assigning an instance to a cluster is the distance to
    the centroid.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数实例都被清楚地分配到了适当的簇中，但少数实例可能被错误标记，尤其是在左上角簇和中央簇之间的边界附近。事实上，当簇的直径非常不同时，k-means
    算法表现不佳，因为它在将实例分配到簇时只关心实例到质心的距离。
- en: 'Instead of assigning each instance to a single cluster, which is called *hard
    clustering*, it can be useful to give each instance a score per cluster, which
    is called *soft clustering*. The score can be the distance between the instance
    and the centroid or a similarity score (or affinity), such as the Gaussian radial
    basis function we used in [Chapter 2](ch02.html#project_chapter). In the `KMeans`
    class, the `transform()` method measures the distance from each instance to every
    centroid:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与将每个实例分配到单个簇中，即所谓的**硬聚类**不同，给每个实例分配每个簇的分数可能很有用，这被称为**软聚类**。这个分数可以是实例与质心的距离或相似度分数（或亲和度），例如我们在[第
    2 章](ch02.html#project_chapter)中使用的高斯径向基函数。在 `KMeans` 类中，`transform()` 方法测量每个实例到每个质心的距离：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example, the first instance in `X_new` is located at a distance of
    about 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the
    third centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid.
    If you have a high-dimensional dataset and you transform it this way, you end
    up with a *k*-dimensional dataset: this transformation can be a very efficient
    nonlinear dimensionality reduction technique. Alternatively, you can use these
    distances as extra features to train another model, as in [Chapter 2](ch02.html#project_chapter).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`X_new` 中的第一个实例距离第一个质心大约 2.81，距离第二个质心 0.33，距离第三个质心 2.90，距离第四个质心 1.49，距离第五个质心
    2.89。如果你有一个高维数据集并且以这种方式转换它，你最终会得到一个 *k*-维数据集：这种转换可以是一个非常有效的非线性降维技术。或者，你可以使用这些距离作为额外的特征来训练另一个模型，就像在[第
    2 章](ch02.html#project_chapter)中那样。
- en: The k-means algorithm
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means 算法
- en: So, how does the algorithm work? Well, suppose you were given the centroids.
    You could easily label all the instances in the dataset by assigning each of them
    to the cluster whose centroid is closest. Conversely, if you were given all the
    instance labels, you could easily locate each cluster’s centroid by computing
    the mean of the instances in that cluster. But you are given neither the labels
    nor the centroids, so how can you proceed? Start by placing the centroids randomly
    (e.g., by picking *k* instances at random from the dataset and using their locations
    as centroids). Then label the instances, update the centroids, label the instances,
    update the centroids, and so on until the centroids stop moving. The algorithm
    is guaranteed to converge in a finite number of steps (usually quite small). That’s
    because the mean squared distance between the instances and their closest centroids
    can only go down at each step, and since it cannot be negative, it’s guaranteed
    to converge.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，算法是如何工作的呢？好吧，假设你被给出了质心。你可以很容易地将数据集中的所有实例标记出来，通过将每个实例分配到其质心最近的簇。相反，如果你被给出了所有实例的标签，你可以很容易地通过计算该簇中实例的平均值来定位每个簇的质心。但是你没有给出标签也没有给出质心，那么你该如何进行呢？首先随机放置质心（例如，通过从数据集中随机选择
    *k* 个实例并使用它们的坐标作为质心）。然后标记实例，更新质心，标记实例，更新质心，以此类推，直到质心停止移动。该算法保证在有限步骤内收敛（通常相当小）。这是因为实例与其最近质心之间的均方距离在每一步只能下降，因为它不能是负数，所以它保证会收敛。
- en: 'You can see the algorithm in action in [Figure 8-4](#kmeans_algorithm_plot):
    the centroids are initialized randomly (top left), then the instances are labeled
    (top right), then the centroids are updated (center left), the instances are relabeled
    (center right), and so on. As you can see, in just three iterations the algorithm
    has reached a clustering that seems close to optimal.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 8-4](#kmeans_algorithm_plot)中看到算法的实际运行情况：质心是随机初始化的（左上角），然后实例被标记（右上角），然后质心被更新（中间左），实例被重新标记（中间右），以此类推。正如你所看到的，仅仅经过三次迭代，算法就达到了一个似乎接近最优的聚类。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The computational complexity of the algorithm is generally linear with regard
    to the number of instances *m*, the number of clusters *k*, and the number of
    dimensions *n*. However, this is only true when the data has a clustering structure.
    If it does not, then in the worst-case scenario the complexity can increase exponentially
    with the number of instances. In practice, this rarely happens, and *k*-means
    is generally one of the fastest clustering algorithms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的计算复杂度通常与实例数*m*、聚类数*k*和维度数*n*成线性关系。然而，这只在数据具有聚类结构时才成立。如果没有，则在最坏的情况下，复杂度可能会随着实例数的增加而指数级增长。在实践中，这种情况很少发生，*k*-均值通常是速度最快的聚类算法之一。
- en: '![Diagram illustrating the k-means algorithm''s process with random centroid
    initialization and subsequent instance labeling, showing potential clustering
    outcomes over iterations.](assets/hmls_0804.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图示k-means算法的过程，包括随机质心初始化和随后的实例标记，显示了迭代过程中的潜在聚类结果。](assets/hmls_0804.png)'
- en: Figure 8-4\. The *k*-means algorithm
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. *k*-均值算法
- en: 'Although the algorithm is guaranteed to converge, it may not converge to the
    right solution (i.e., it may converge to a local optimum): whether it does or
    not depends on the centroid initialization. [Figure 8-5](#kmeans_variability_plot)
    shows two suboptimal solutions that the algorithm can converge to if you are not
    lucky with the random initialization step.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然算法保证会收敛，但它可能不会收敛到正确的解（即，它可能收敛到局部最优解）：它是否收敛取决于质心初始化。![图8-5](#kmeans_variability_plot)显示了两种次优解，如果你在随机初始化步骤中运气不佳，算法可能会收敛到这些解。
- en: '![Diagram showing two suboptimal k-means clustering solutions caused by different
    random centroid initializations.](assets/hmls_0805.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图示两种由不同随机质心初始化引起的次优k-means聚类解。](assets/hmls_0805.png)'
- en: Figure 8-5\. Suboptimal solutions due to unlucky centroid initializations
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 由于不幸运的质心初始化导致的次优解
- en: Let’s take a look at a few ways you can mitigate this risk by improving the
    centroid initialization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些方法，通过改进质心初始化来降低这种风险。
- en: Centroid initialization methods
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 质心初始化方法
- en: 'If you happen to know approximately where the centroids should be (e.g., if
    you ran another clustering algorithm earlier), then you can set the `init` hyperparameter
    to a NumPy array containing the list of centroids:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你大致知道质心应该在哪里（例如，如果你之前运行了另一个聚类算法），则可以将`init`超参数设置为包含质心列表的NumPy数组：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Another solution is to run the algorithm multiple times with different random
    initializations and keep the best solution. The number of random initializations
    is controlled by the `n_init` hyperparameter: by default it is equal to `10` when
    using `init="random"`, which means that the whole algorithm described earlier
    runs 10 times when you call `fit()`, and Scikit-Learn keeps the best solution.
    But how exactly does it know which solution is the best? It uses a performance
    metric! That metric is called the model’s *inertia*, which is defined in [Equation
    8-1](#inertia_equation).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是多次运行算法，使用不同的随机初始化，并保留最佳解。随机初始化的次数由`n_init`超参数控制：当使用`init="random"`时，默认等于`10`，这意味着当你调用`fit()`时，上述整个算法将运行10次，Scikit-Learn将保留最佳解。但它是如何知道哪个解是最好的呢？它使用一个性能指标！这个指标被称为模型的*惯性*，它在[方程式8-1](#inertia_equation)中定义。
- en: Equation 8-1\. A model’s inertia is the sum of all squared distances between
    each instance **x**^((*i*)) and the closest centroid **c**^((*i*)) predicted by
    the model
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式8-1\. 模型的惯性是每个实例**x**^((*i*))与模型预测的最近质心**c**^((*i*))之间所有平方距离的总和
- en: <mrow><mtext>inertia</mtext> <mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder>
    <msup><mrow><mo>∥</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>-</mo><msup><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo rspace="0.1em">∥</mo></mrow> <mn>2</mn></msup></mrow>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mtext>惯性</mtext> <mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder> <msup><mrow><mo>∥</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi
    mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo
    rspace="0.1em">∥</mo></mrow> <mn>2</mn></msup></mrow>
- en: 'The inertia is roughly equal to 219.6 for the model on the left in [Figure 8-5](#kmeans_variability_plot),
    600.4 for the model on the right in [Figure 8-5](#kmeans_variability_plot), and
    only 211.6 for the model in [Figure 8-3](#voronoi_plot). The `KMeans` class runs
    the initialization algorithm `n_init` times and keeps the model with the lowest
    inertia. In this example, the model in [Figure 8-3](#voronoi_plot) will be selected
    (unless we are very unlucky with `n_init` consecutive random initializations).
    If you are curious, a model’s inertia is accessible via the `inertia_` instance
    variable:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 8-5](#kmeans_variability_plot) 左侧的模型中，惯性大约为 219.6，而在 [图 8-5](#kmeans_variability_plot)
    右侧的模型中为 600.4，而在 [图 8-3](#voronoi_plot) 中的模型仅为 211.6。`KMeans` 类运行初始化算法 `n_init`
    次并保留惯性最低的模型。在这个例子中，[图 8-3](#voronoi_plot) 中的模型将被选中（除非我们在 `n_init` 连续随机初始化中非常不幸）。如果你好奇，可以通过
    `inertia_` 实例变量访问模型的惯性：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `score()` method returns the negative inertia (it’s negative because a
    predictor’s `score()` method must always respect Scikit-Learn’s “greater is better”
    rule—if a predictor is better than another, its `score()` method should return
    a greater score):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`score()` 方法返回负惯性（它是负的，因为预测器的 `score()` 方法必须始终遵守 Scikit-Learn 的“越大越好”规则——如果一个预测器比另一个更好，它的
    `score()` 方法应该返回一个更高的分数）：'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An important improvement to the *k*-means algorithm, *k-means++*, was proposed
    in a [2006 paper](https://homl.info/37) by David Arthur and Sergei Vassilvitskii.⁠^([3](ch08.html#id1983))
    They introduced a smarter initialization step that tends to select centroids that
    are distant from one another. This change makes the *k*-means algorithm much more
    likely to locate all important clusters, and less likely to converge to a suboptimal
    solution (just like spreading out fishing boats increases the chance of locating
    more schools of fish). The paper showed that the additional computation required
    for the smarter initialization step is well worth it because it makes it possible
    to drastically reduce the number of times the algorithm needs to be run to find
    the optimal solution. The *k*-means++ initialization algorithm works like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *k*-means 算法的一个重要改进，*k-means++*，是在 David Arthur 和 Sergei Vassilvitskii 的 2006
    年论文中提出的。[2006 年论文](https://homl.info/37)⁠^([3](ch08.html#id1983)) 中，他们引入了一个更智能的初始化步骤，倾向于选择彼此距离较远的质心。这个变化使得
    *k*-means 算法更有可能定位所有重要的聚类，并且不太可能收敛到次优解（就像分散渔船可以增加找到更多鱼群的机会）。论文表明，为了更智能的初始化步骤所需的额外计算是值得的，因为它使得算法能够大幅减少找到最优解所需的运行次数。*k*-means++
    初始化算法的工作方式如下：
- en: Take one centroid **c**^((1)), chosen uniformly at random from the dataset.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中随机均匀选择一个质心 **c**^((1))。
- en: Take a new centroid **c**^((*i*)), choosing an instance **x**^((*i*)) with probability
    $upper D left-parenthesis bold x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis squared$ / $sigma-summation Underscript j equals 1
    Overscript m Endscripts upper D left-parenthesis bold x Superscript left-parenthesis
    j right-parenthesis Baseline right-parenthesis squared$ , where D(**x**^((*i*)))
    is the distance between the instance **x**^((*i*)) and the closest centroid that
    was already chosen. This probability distribution ensures that instances farther
    away from already chosen centroids are much more likely to be selected as centroids.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个新的质心 **c**^((*i*))，以概率 $upper D left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis squared$ / $sigma-summation Underscript
    j equals 1 Overscript m Endscripts upper D left-parenthesis bold x Superscript
    left-parenthesis j right-parenthesis Baseline right-parenthesis squared$ 选择一个实例
    **x**^((*i*)），其中 D(**x**^((*i*))) 是实例 **x**^((*i*))) 与已选择的最近质心之间的距离。这个概率分布确保了远离已选择质心的实例更有可能被选为质心。
- en: Repeat the previous step until all *k* centroids have been chosen.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，直到选择所有 *k* 个质心。
- en: 'When you set `init="k-means++"` (which is the default), the `KMeans` class
    actually uses a variant of *k*-means++ called *greedy k-means++*: instead of sampling
    a single centroid at each iteration, it samples multiple and picks the best one.
    When using this algorithm, `n_init` defaults to 1.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当你设置 `init="k-means++"`（这是默认值）时，`KMeans` 类实际上使用了一种称为 *greedy k-means++* 的 *k*-means++
    变体：在每个迭代中不是采样一个质心，而是采样多个并选择最好的一个。当使用此算法时，`n_init` 默认为 1。
- en: Accelerated k-means and mini-batch k-means
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加速 k-means 和 mini-batch k-means
- en: Another improvement to the *k*-means algorithm was proposed in a [2003 paper](https://homl.info/38)
    by Charles Elkan.⁠^([4](ch08.html#id1989)) On some large datasets with many clusters,
    the algorithm can be accelerated by avoiding many unnecessary distance calculations.
    Elkan achieved this by exploiting the triangle inequality (i.e., that a straight
    line is always the shortest distance between two points⁠^([5](ch08.html#id1990)))
    and by keeping track of lower and upper bounds for distances between instances
    and centroids. However, Elkan’s algorithm does not always accelerate training,
    and sometimes it can even slow down training significantly; it depends on the
    dataset. Still, if you want to give it a try, set `algorithm="elkan"`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Charles Elkan 在一篇 [2003 年的论文](https://homl.info/38) 中提出了对 *k*-均值算法的另一个改进。⁠^([4](ch08.html#id1989))
    在一些具有许多聚类的较大数据集上，可以通过避免许多不必要的距离计算来加速算法。Elkan 通过利用三角不等式（即，两点之间的直线总是最短距离⁠^([5](ch08.html#id1990)))
    并跟踪实例与质心之间距离的下限和上限来实现这一点。然而，Elkan 的算法并不总是加速训练，有时它甚至可以显著减慢训练速度；这取决于数据集。尽管如此，如果你想尝试一下，可以将
    `algorithm="elkan"` 设置。
- en: 'Yet another important variant of the *k*-means algorithm was proposed in a
    [2010 paper](https://homl.info/39) by David Sculley.⁠^([6](ch08.html#id1993))
    Instead of using the full dataset at each iteration, the algorithm is capable
    of using mini-batches, moving the centroids just slightly at each iteration. This
    speeds up the algorithm and makes it possible to cluster huge datasets that do
    not fit in memory. Scikit-Learn implements this algorithm in the `MiniBatchKMeans`
    class, which you can use just like the `KMeans` class:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: David Sculley 在一篇 [2010 年的论文](https://homl.info/39) 中提出了 *k*-均值算法的另一个重要变体。⁠^([6](ch08.html#id1993))
    该算法不是在每个迭代中使用完整的数据集，而是能够使用小批量数据，每次迭代只稍微移动质心。这加快了算法的速度，并使得对那些不适合内存的大数据集进行聚类成为可能。Scikit-Learn
    在 `MiniBatchKMeans` 类中实现了这个算法，你可以像使用 `KMeans` 类一样使用它：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the dataset does not fit in memory, the simplest option is to use the `memmap`
    class, as we did for incremental PCA in [Chapter 7](ch07.html#dimensionality_chapter).
    Alternatively, you can pass one mini-batch at a time to the `partial_fit()` method,
    but this will require much more work, since you will need to perform multiple
    initializations and select the best one yourself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集不适合内存，最简单的选项是使用 `memmap` 类，就像我们在第 7 章中为增量 PCA 所做的那样。或者，你可以一次传递一个小批量数据到
    `partial_fit()` 方法，但这将需要更多的工作，因为你需要执行多次初始化并自己选择最佳的一个。
- en: Finding the optimal number of clusters
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找最佳聚类数量
- en: So far, we’ve set the number of clusters *k* to 5 because it was obvious by
    looking at the data that this was the correct number of clusters. But in general,
    it won’t be so easy to know how to set *k*, and the result might be quite bad
    if you set it to the wrong value. As you can see in [Figure 8-6](#bad_n_clusters_plot),
    for this dataset setting *k* to 3 or 8 results in fairly bad models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已将聚类数量 *k* 设置为 5，因为通过观察数据很明显这是正确的聚类数量。但通常，知道如何设置 *k* 不会那么容易，如果你设置错误，结果可能会相当糟糕。如图
    [图 8-6](#bad_n_clusters_plot) 所示，对于这个数据集，将 *k* 设置为 3 或 8 会得到相当糟糕的模型。
- en: You might be thinking that you could just pick the model with the lowest inertia.
    Unfortunately, it is not that simple. The inertia for *k* = 3 is about 653.2,
    which is much higher than for *k* = 5 (211.7). But with *k* = 8, the inertia is
    just 127.1\. The inertia is not a good performance metric when trying to choose
    *k* because it keeps getting lower as we increase *k*. Indeed, the more clusters
    there are, the closer each instance will be to its closest centroid, and therefore
    the lower the inertia will be. Let’s plot the inertia as a function of *k*. When
    we do this, the curve often contains an inflexion point called the *elbow* (see
    [Figure 8-7](#inertia_vs_k_plot)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为你可以只选择惯性最低的模型。不幸的是，事情并不那么简单。当 *k* = 3 时，惯性约为 653.2，这比 *k* = 5 (211.7) 高得多。但是当
    *k* = 8 时，惯性仅为 127.1。当尝试选择 *k* 时，惯性不是一个好的性能指标，因为它会随着 *k* 的增加而不断降低。确实，聚类越多，每个实例与其最近质心的距离就越近，因此惯性就越低。让我们绘制惯性作为
    *k* 的函数的图表。当我们这样做时，曲线通常包含一个拐点，称为 *肘部*（见 [图 8-7](#inertia_vs_k_plot)）。
- en: '![Diagram showing clustering outcomes; with _k_ = 3, distinct clusters merge,
    and with _k_ = 8, some clusters split into multiple parts.](assets/hmls_0806.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![显示聚类结果的图示；当 _k_ = 3 时，不同的聚类合并，而当 _k_ = 8 时，一些聚类分裂成多个部分。](assets/hmls_0806.png)'
- en: 'Figure 8-6\. Bad choices for the number of clusters: when k is too small, separate
    clusters get merged (left), and when k is too large, some clusters get chopped
    into multiple pieces (right)'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6\. 对于簇数量的不良选择：当k值太小时，分离的簇会被合并（左），而当k值太大时，一些簇会被分割成多个部分（右）
- en: '![Line graph showing inertia decreasing sharply with increasing clusters \(k\),
    with an elbow at \(k = 4\).](assets/hmls_0807.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![展示随着簇数量增加（k）惯性急剧减少的线图，在k = 4处有一个拐点。](assets/hmls_0807.png)'
- en: Figure 8-7\. Plotting the inertia as a function of the number of clusters *k*
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7\. 将惯性作为簇数量k的函数进行绘图
- en: 'As you can see, the inertia drops very quickly as we increase *k* up to 4,
    but then it decreases much more slowly as we keep increasing *k*. This curve has
    roughly the shape of an arm, and there is an elbow at *k* = 4\. So, if we did
    not know better, we might think 4 was a good choice: any lower value would be
    dramatic, while any higher value would not help much, and we might just be splitting
    perfectly good clusters in half for no good reason.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当我们增加*k*直到4时，惯性会迅速下降，但当我们继续增加*k*时，它下降的速度会慢得多。这个曲线大致呈手臂形状，在*k* = 4处有一个拐点。所以，如果我们不知道更好的选择，我们可能会认为4是一个不错的选择：任何更低的值都会很显著，而任何更高的值都不会有很大帮助，我们可能只是没有理由地将完美的簇一分为二。
- en: This technique for choosing the best value for the number of clusters is rather
    coarse. A more precise (but also more computationally expensive) approach is to
    use the *silhouette score*, which is the mean *silhouette coefficient* over all
    the instances. An instance’s silhouette coefficient is equal to (*b* – *a*) /
    max(*a*, *b*), where *a* is the mean distance to the other instances in the same
    cluster (i.e., the mean intra-cluster distance) and *b* is the mean nearest-cluster
    distance (i.e., the mean distance to the instances of the next closest cluster,
    defined as the one that minimizes *b*, excluding the instance’s own cluster).
    The silhouette coefficient can vary between –1 and +1\. A coefficient close to
    +1 means that the instance is well inside its own cluster and far from other clusters,
    while a coefficient close to 0 means that it is close to a cluster boundary; finally,
    a coefficient close to –1 means that the instance may have been assigned to the
    wrong cluster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择簇数量最佳值的这种技术相当粗糙。一个更精确（但计算成本也更高）的方法是使用*轮廓得分*，它是所有实例的平均*轮廓系数*。一个实例的轮廓系数等于（*b*
    – *a*） / max(*a*, *b*），其中*a*是同一簇中其他实例的平均距离（即平均簇内距离），*b*是平均最近簇距离（即到下一个最近簇实例的平均距离，定义为最小化*b*的簇，排除实例所在的簇）。轮廓系数可以在-1和+1之间变化。接近+1的系数意味着实例很好地位于其自己的簇内，远离其他簇，而接近0的系数意味着它接近簇边界；最后，接近-1的系数意味着实例可能被分配到了错误的簇。
- en: 'To compute the silhouette score, you can use Scikit-Learn’s `silhouette_score()`
    function, giving it all the instances in the dataset and the labels they were
    assigned:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算轮廓得分，您可以使用Scikit-Learn的`silhouette_score()`函数，给它提供数据集中的所有实例及其分配的标签：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s compare the silhouette scores for different numbers of clusters (see [Figure 8-8](#silhouette_score_vs_k_plot)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较不同簇数量的轮廓得分（见[图8-8](#silhouette_score_vs_k_plot)）。
- en: '![Line plot showing silhouette scores for various cluster counts, with peaks
    at _k_ = 4 and 5, indicating optimal clustering choices.](assets/hmls_0808.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![展示各种簇数量轮廓得分的线图，在_k_ = 4和5处有峰值，指示最佳的聚类选择。](assets/hmls_0808.png)'
- en: Figure 8-8\. Selecting the number of clusters *k* using the silhouette score
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8\. 使用轮廓得分选择簇数量k
- en: 'As you can see, this visualization is much richer than the previous one: although
    it confirms that *k* = 4 is a very good choice, it also highlights the fact that
    *k* = 5 is quite good as well, and much better than *k* = 6 or 7\. This was not
    visible when comparing inertias.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这种可视化比之前的一个要丰富得多：虽然它确认*k* = 4是一个非常不错的选择，但它也突出了*k* = 5也是一个相当好的选择，并且比*k*
    = 6或7要好得多。在比较惯性时，这一点并没有显现出来。
- en: An even more informative visualization is obtained when we plot every instance’s
    silhouette coefficient, sorted by the clusters they are assigned to and by the
    value of the coefficient. This is called a *silhouette diagram* (see [Figure 8-9](#silhouette_analysis_plot)).
    Each diagram contains one knife shape per cluster. The shape’s height indicates
    the number of instances in the cluster, and its width represents the sorted silhouette
    coefficients of the instances in the cluster (wider is better).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按分配给每个实例的聚类和系数值对每个实例的轮廓系数进行排序并绘制时，我们获得了一个更富有信息量的可视化效果。这被称为 *轮廓图*（见 [图 8-9](#silhouette_analysis_plot)）。每个图包含每个聚类的单个刀片形状。形状的高度表示聚类中的实例数量，其宽度代表聚类中实例的排序轮廓系数（越宽越好）。
- en: 'The vertical dashed lines represent the mean silhouette score for each number
    of clusters. When most of the instances in a cluster have a lower coefficient
    than this score (i.e., if many of the instances stop short of the dashed line,
    ending to the left of it), then the cluster is rather bad since this means its
    instances are much too close to other clusters. Here we can see that when *k*
    = 3 or 6, we get bad clusters. But when *k* = 4 or 5, the clusters look pretty
    good: most instances extend beyond the dashed line, to the right and closer to
    1.0\. When *k* = 4, the cluster at index 0 (at the bottom) is rather big. When
    *k* = 5, all clusters have similar sizes. So, even though the overall silhouette
    score from *k* = 4 is slightly greater than for *k* = 5, it seems like a good
    idea to use *k* = 5 to get clusters of similar sizes.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直虚线代表每个聚类数量的平均轮廓得分。当一个聚类中的大多数实例的系数低于这个得分（即，如果许多实例停止在虚线之前，结束在它的左侧），那么这个聚类相当差，因为这意味着它的实例与其他聚类非常接近。在这里，我们可以看到当
    *k* = 3 或 6 时，我们得到的是较差的聚类。但是当 *k* = 4 或 5 时，聚类看起来相当不错：大多数实例延伸到虚线之外，向右并接近 1.0。当
    *k* = 4 时，索引为 0 的聚类（在底部）相当大。当 *k* = 5 时，所有聚类的大小相似。因此，尽管从 *k* = 4 得到的整体轮廓得分略高于
    *k* = 5，但使用 *k* = 5 来获得大小相似的聚类似乎是个好主意。
- en: '![Silhouette diagrams comparing cluster quality for different values of _k_,
    showing that clusters are more balanced when _k_ = 5, despite slightly higher
    overall scores for _k_ = 4.](assets/hmls_0809.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同 _k_ 值的聚类质量轮廓图，显示当 _k_ = 5 时，聚类更平衡，尽管 _k_ = 4 的整体得分略高。](assets/hmls_0809.png)'
- en: Figure 8-9\. Analyzing the silhouette diagrams for various values of *k*
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-9\. 分析不同 *k* 值的轮廓图
- en: Limits of k-Means
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-Means 的局限性
- en: Despite its many merits, most notably being fast and scalable, *k*-means is
    not perfect. As we saw, it is necessary to run the algorithm several times to
    avoid suboptimal solutions, plus you need to specify the number of clusters, which
    can be quite a hassle. Moreover, *k*-means does not behave very well when the
    clusters have varying sizes, different densities, or nonspherical shapes. For
    example, [Figure 8-10](#bad_kmeans_plot) shows how *k*-means clusters a dataset
    containing three ellipsoidal clusters of different dimensions, densities, and
    orientations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它有许多优点，尤其是速度快和可扩展，但 *k*-means 并非完美。正如我们所见，为了避免次优解，需要多次运行算法，而且你还需要指定聚类的数量，这可能会相当麻烦。此外，当聚类具有不同的大小、不同的密度或非球形形状时，*k*-means
    的表现并不好。例如，[图 8-10](#bad_kmeans_plot) 展示了 *k*-means 如何聚类包含三个不同维度、密度和方向的椭圆形聚类的数据集。
- en: As you can see, neither of these solutions is any good. The solution on the
    left is better, but it still chops off 25% of the middle cluster and assigns it
    to the cluster on the right. The solution on the right is just terrible, even
    though its inertia is lower. So, depending on the data, different clustering algorithms
    may perform better. On these types of elliptical clusters, Gaussian mixture models
    work great.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这两种解决方案都不好。左边的解决方案较好，但它仍然切掉了中间聚类 25% 的部分并将其分配给右边的聚类。右边的解决方案非常糟糕，尽管它的惯性较低。因此，根据数据的不同，不同的聚类算法可能表现更好。在这些类型的椭圆形聚类中，高斯混合模型工作得很好。
- en: '![Diagram comparing two k-means clustering attempts on ellipsoidal data, highlighting
    poor performance in clustering accuracy.](assets/hmls_0810.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![比较椭圆数据上两次 k-means 聚类尝试的图，突出聚类准确率差。](assets/hmls_0810.png)'
- en: Figure 8-10\. k-means fails to cluster these ellipsoidal blobs properly
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-10\. k-means 无法正确聚类这些椭圆形的块
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is important to scale the input features (see [Chapter 2](ch02.html#project_chapter))
    before you run *k*-means, or the clusters may be very stretched and *k*-means
    will perform poorly. Scaling the features does not guarantee that all the clusters
    will be nice and spherical, but it generally helps *k*-means.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 *k*-均值聚类之前，您需要缩放输入特征（参见[第2章](ch02.html#project_chapter)），否则聚类可能会非常拉伸，*k*-均值聚类的性能可能会不佳。缩放特征并不能保证所有聚类都会很完美地呈球形，但它通常有助于
    *k*-均值聚类。
- en: Now let’s look at a few ways we can benefit from clustering. We will use *k*-means,
    but feel free to experiment with other clustering algorithms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨一下我们可以从聚类中受益的几种方式。我们将使用 *k*-均值聚类，但请随意尝试其他聚类算法。
- en: Using Clustering for Image Segmentation
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用聚类进行图像分割
- en: '*Image segmentation* is the task of partitioning an image into multiple segments.
    There are several variants:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像分割* 是将图像分割成多个片段的任务。有几种变体：'
- en: In *color segmentation*, pixels with a similar color get assigned to the same
    segment. This is sufficient in many applications. For example, if you want to
    analyze satellite images to measure how much total forest area there is in a region,
    color segmentation may be just fine.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *颜色分割* 中，颜色相似的像素被分配到同一个片段。这在许多应用中是足够的。例如，如果您想分析卫星图像来测量一个区域中有多少总森林面积，颜色分割可能就足够了。
- en: In *semantic segmentation*, all pixels that are part of the same object type
    get assigned to the same segment. For example, in a self-driving car’s vision
    system, all pixels that are part of a pedestrian’s image might be assigned to
    the “pedestrian” segment (there would be one segment containing all the pedestrians).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *语义分割* 中，属于同一物体类型的所有像素都被分配到同一个片段。例如，在自动驾驶汽车的视觉系统中，属于行人图像的所有像素可能会被分配到“行人”片段（会有一个包含所有行人的片段）。
- en: In *instance segmentation*, all pixels that are part of the same individual
    object are assigned to the same segment. In this case there would be a different
    segment for each pedestrian.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *实例分割* 中，属于同一物体的所有像素都被分配到同一个片段。在这种情况下，每个行人都会有不同的片段。
- en: The state of the art in semantic or instance segmentation today is achieved
    using complex architectures based on convolutional neural networks (see [Chapter 12](ch12.html#cnn_chapter))
    or vision transformers (see [Chapter 16](ch16.html#vit_chapter)). In this chapter
    we are going to focus on the (much simpler) color segmentation task, using *k*-means.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在语义或实例分割方面达到的先进水平是使用基于卷积神经网络（参见[第12章](ch12.html#cnn_chapter)）或视觉转换器（参见[第16章](ch16.html#vit_chapter)）的复杂架构实现的。在本章中，我们将关注（相对简单得多的）颜色分割任务，使用
    *k*-均值聚类。
- en: 'We’ll start by importing the Pillow package (successor to the Python Imaging
    Library, PIL), which we’ll then use to load the *ladybug.png* image (see the upper-left
    image in [Figure 8-11](#image_segmentation_plot)), assuming it’s located at `filepath`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 Pillow 包（Python 图像库 PIL 的继任者），然后使用它来加载 *ladybug.png* 图像（参见[图8-11](#image_segmentation_plot)
    上方的图像），假设它位于 `filepath`：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The image is represented as a 3D array. The first dimension’s size is the height;
    the second is the width; and the third is the number of color channels, in this
    case red, green, and blue (RGB). In other words, for each pixel there is a 3D
    vector containing the intensities of red, green, and blue as unsigned 8-bit integers
    between 0 and 255\. Some images may have fewer channels (such as grayscale images,
    which only have one), and some images may have more channels (such as images with
    an additional *alpha channel* for transparency, or satellite images, which often
    contain channels for additional light frequencies, like infrared).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被表示为一个三维数组。第一个维度的尺寸是高度；第二个是宽度；第三个是颜色通道的数量，在这种情况下是红色、绿色和蓝色（RGB）。换句话说，对于每个像素，都有一个包含红色、绿色和蓝色强度（以0到255之间的无符号8位整数表示）的三维向量。有些图像可能有更少的通道（例如灰度图像，只有一个通道），有些图像可能有更多的通道（例如具有额外
    *alpha 通道* 的图像用于透明度，或卫星图像，通常包含用于额外光频率的通道，如红外线）。
- en: 'The following code reshapes the array to get a long list of RGB colors, then
    it clusters these colors using *k*-means with eight clusters. It creates a `segmented_img`
    array containing the nearest cluster center for each pixel (i.e., the mean color
    of each pixel’s cluster), and lastly it reshapes this array to the original image
    shape. The third line uses advanced NumPy indexing; for example, if the first
    10 labels in `kmeans_.labels_` are equal to 1, then the first 10 colors in `segmented_img`
    are equal to `kmeans.cluster_centers_[1]`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将数组重塑为包含 RGB 颜色长列表，然后使用八个簇的 *k*-means 对这些颜色进行聚类。它创建一个 `segmented_img` 数组，包含每个像素最近的簇中心（即每个像素簇的平均颜色），最后将此数组重塑为原始图像形状。第三行使用高级
    NumPy 索引；例如，如果 `kmeans_.labels_` 的前 10 个标签等于 1，那么 `segmented_img` 的前 10 个颜色等于
    `kmeans.cluster_centers_[1]`：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This outputs the image shown in the upper right of [Figure 8-11](#image_segmentation_plot).
    You can experiment with various numbers of clusters, as shown in the figure. When
    you use fewer than eight clusters, notice that the ladybug’s flashy red color
    fails to get a cluster of its own: it gets merged with colors from the environment.
    This is because *k*-means prefers clusters of similar sizes. The ladybug is small—much
    smaller than the rest of the image—so even though its color is flashy, *k*-means
    fails to dedicate a cluster to it.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出的是[图 8-11](#image_segmentation_plot)右上角所示的图像。你可以尝试使用图中的不同簇数量进行实验。当你使用少于八个簇时，请注意瓢虫的鲜艳红色无法获得自己的簇：它与环境中的颜色合并。这是因为
    *k*-means 倾向于相似大小的簇。瓢虫很小——比图像中的其他部分小得多——所以尽管它的颜色鲜艳，*k*-means 仍然无法为它分配一个簇。
- en: '![Image showing the effect of _k_-means clustering with 10, 8, 6, 4, and 2
    color clusters on an original image of a ladybug on a dandelion, illustrating
    how fewer clusters merge colors and lose detail.](assets/hmls_0811.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![展示在蒲公英上的瓢虫原始图像上使用 _k_-means 聚类（10、8、6、4 和 2 个颜色簇）的效果，说明簇数量减少时颜色合并并丢失细节。](assets/hmls_0811.png)'
- en: Figure 8-11\. Image segmentation using *k*-means with various numbers of color
    clusters
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-11\. 使用 *k*-means 和不同数量的颜色簇进行图像分割
- en: That wasn’t too hard, was it? Now let’s look at another application of clustering.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太难，对吧？现在让我们看看聚类的另一个应用。
- en: Using Clustering for Semi-Supervised Learning
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用聚类进行半监督学习
- en: 'Another use case for clustering is in semi-supervised learning, when we have
    plenty of unlabeled instances and very few labeled instances. For example, clustering
    can help choose which additional instances to label (e.g., near the cluster centroids).
    It can also be used to propagate the most common label in each cluster to the
    unlabeled instances in that cluster. Let’s try these ideas on the digits dataset,
    which is a simple MNIST-like dataset containing 1,797 grayscale 8 × 8 images representing
    the digits 0 to 9\. First, let’s load and split the dataset (it’s already shuffled):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的另一个用例是在半监督学习中，当我们有大量的未标记实例和非常少的标记实例时。例如，聚类可以帮助选择哪些额外的实例进行标记（例如，靠近簇质心）。它还可以用于将每个簇中最常见的标签传播到该簇中的未标记实例。让我们在数字数据集上尝试这些想法，这是一个简单的类似
    MNIST 的数据集，包含 1,797 个 8 × 8 灰度图像，代表数字 0 到 9。首先，让我们加载并拆分数据集（它已经打乱）：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will pretend we only have labels for 50 instances. To get a baseline performance,
    let’s train a logistic regression model on these 50 labeled instances:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假装我们只有 50 个实例的标签。为了获得基线性能，让我们在这些 50 个标记实例上训练逻辑回归模型：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then measure the accuracy of this model on the test set (note that the
    test set must be labeled):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以随后在测试集上测量此模型的准确性（注意测试集必须是标记过的）：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model’s accuracy is just 75.8%. That’s not great: indeed, if you try training
    the model on the full training set, you will find that it will reach about 90.9%
    accuracy. Let’s see how we can do better. First, let’s cluster the training set
    into 50 clusters. Then, for each cluster, we’ll find the image closest to the
    centroid. We’ll call these images the *representative images*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确性仅为 75.8%。这并不理想：实际上，如果你尝试在完整训练集上训练模型，你会发现它的准确性将达到约 90.9%。让我们看看我们如何做得更好。首先，让我们将训练集聚成
    50 个簇。然后，对于每个簇，我们将找到最接近质心的图像。我们将这些图像称为 *代表性图像*：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 8-12](#representative_images_plot) shows the 50 representative images.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-12](#representative_images_plot)显示了 50 个代表性图像。'
- en: '![Fifty handwritten digit images, each representing a distinct cluster for
    analysis.](assets/hmls_0812.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![五十个手写数字图像，每个图像代表一个用于分析的独立簇。](assets/hmls_0812.png)'
- en: Figure 8-12\. Fifty representative digit images (one per cluster)
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-12\. 五十个代表性的数字图像（每个簇一个）
- en: 'Let’s look at each image and manually label them:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看每一张图像并手动标记它们：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we have a dataset with just 50 labeled instances, but instead of being
    random instances, each of them is a representative image of its cluster. Let’s
    see if the performance is any better:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个只有 50 个标记实例的数据集，但与随机实例不同，每个实例都是其簇的代表性图像。让我们看看性能是否有所改善：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Wow! We jumped from 75.8% accuracy to 83.4%, although we are still only training
    the model on 50 instances. Since it is often costly and painful to label instances,
    especially when it has to be done manually by experts, it is a good idea to label
    representative instances rather than just random instances.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们的准确率从 75.8% 上升到 83.4%，尽管我们仍然只训练了 50 个实例。由于标记实例通常既昂贵又痛苦，尤其是当需要由专家手动完成时，标记代表性实例而不是随机实例是一个好主意。
- en: 'But perhaps we can go one step further: what if we propagated the labels to
    all the other instances in the same cluster? This is called *label propagation*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但也许我们可以更进一步：如果我们把标签传播到同一簇中的所有其他实例会怎样？这被称为 *标签传播*：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s train the model again and look at its performance:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次训练模型并查看其性能：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We got another significant accuracy boost! Let’s see if we can do even better
    by ignoring the 50% of instances that are farthest from their cluster center:
    this should eliminate some outliers. The following code first computes the distance
    from each instance to its closest cluster center, then for each cluster it sets
    the 50% largest distances to –1\. Lastly, it creates a set without these instances
    marked with a –1 distance:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们又获得了显著的准确率提升！让我们看看我们是否可以通过忽略距离其簇中心最远的 50% 的实例来做得更好：这应该会消除一些异常值。下面的代码首先计算每个实例到其最近簇中心的距离，然后对于每个簇，它将
    50% 最大的距离设置为 -1。最后，它创建了一个不标记这些实例的集合：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let’s train the model again on this partially propagated dataset and see
    what accuracy we get:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在这个部分传播的数据集上再次训练模型，看看我们能得到多少准确率：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Nice! With just 50 labeled instances (only 5 examples per class on average!)
    we got 88.4% accuracy, pretty close to the performance we got on the fully labeled
    digits dataset. This is partly thanks to the fact that we dropped some outliers,
    and partly because the propagated labels are actually pretty good—their accuracy
    is about 98.9%, as the following code shows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！仅用 50 个标记实例（每个类平均只有 5 个示例！）我们就达到了 88.4% 的准确率，几乎与我们在完全标记的数字数据集上获得的性能相当。这部分得益于我们删除了一些异常值，部分是因为传播的标签实际上相当不错——它们的准确率约为
    98.9%，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Tip
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Scikit-Learn also offers two classes that can propagate labels automatically:
    `LabelSpreading` and `LabelPropagation` in the `sklearn.​semi_supervised` package.
    Both classes construct a similarity matrix between all the instances, and iteratively
    propagate labels from labeled instances to similar unlabeled instances. There’s
    also a different class called `SelfTrainingClassifier` in the same package: you
    give it a base classifier (e.g., `RandomForestClassifier`) and it trains it on
    the labeled instances, then uses it to predict labels for the unlabeled samples.
    It then updates the training set with the labels it is most confident about, and
    repeats this process of training and labeling until it cannot add labels anymore.
    These techniques are not magic bullets, but they can occasionally give your model
    a little boost.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 还提供了两个可以自动传播标签的类：`LabelSpreading` 和 `LabelPropagation`，它们位于 `sklearn.semi_supervised`
    包中。这两个类会在所有实例之间构建一个相似度矩阵，并通过迭代将标签从标记实例传播到相似的未标记实例。在同一包中还有一个名为 `SelfTrainingClassifier`
    的不同类：你给它一个基础分类器（例如，`RandomForestClassifier`），然后它在标记实例上训练它，然后使用它来预测未标记样本的标签。然后，它使用最自信的标签更新训练集，并重复训练和标记的过程，直到不能再添加标签为止。这些技术不是万能的灵丹妙药，但它们有时可以给你的模型带来一点提升。
- en: Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another
    popular clustering algorithm that illustrates a very different approach based
    on local density estimation. This approach allows the algorithm to identify clusters
    of arbitrary shapes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续到高斯混合模型之前，让我们看看 DBSCAN，另一个流行的聚类算法，它展示了基于局部密度估计的非常不同的方法。这种方法允许算法识别任意形状的簇。
- en: DBSCAN
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN
- en: 'The *density-based spatial clustering of applications with noise* (DBSCAN)
    algorithm defines clusters as continuous regions of high density. Here is how
    it works:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 *密度聚类应用噪声*（DBSCAN）的算法将簇定义为高密度连续区域。以下是它是如何工作的：
- en: For each instance, the algorithm counts how many instances are located within
    a small distance ε (epsilon) from it. This region is called the instance’s *ε-neighborhood*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个实例，算法计算有多少实例位于距离它的小距离ε（epsilon）内。这个区域被称为实例的*ε-邻近区域*。
- en: If an instance has at least `min_samples` instances in its ε-neighborhood (including
    itself), then it is considered a *core instance*. In other words, core instances
    are those that are located in dense regions.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个实例在其ε邻近区域（包括自身）中至少有`min_samples`个实例，那么它被视为一个*核心实例*。换句话说，核心实例是位于密集区域中的实例。
- en: All instances in the neighborhood of a core instance belong to the same cluster.
    This neighborhood may include other core instances; therefore, a long sequence
    of neighboring core instances forms a single cluster.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心实例的邻近区域中的所有实例都属于同一个簇。这个邻近区域可能包括其他核心实例；因此，一系列相邻的核心实例形成一个单独的簇。
- en: Any instance that is not a core instance and does not have one in its neighborhood
    is considered an anomaly.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何不是核心实例且其邻近区域没有核心实例的实例被视为异常。
- en: 'This algorithm works well if all the clusters are well separated by low-density
    regions. The `DBSCAN` class in Scikit-Learn is as simple to use as you might expect.
    Let’s test it on the moons dataset, introduced in [Chapter 5](ch05.html#trees_chapter):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有簇都由低密度区域很好地分隔，则此算法效果良好。Scikit-Learn中的`DBSCAN`类使用起来就像你预期的那样简单。让我们在第五章中介绍的月亮数据集上测试它：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The labels of all the instances are now available in the `labels_` instance
    variable:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实例的标签现在都可在`labels_`实例变量中找到：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Notice that some instances have a cluster index equal to –1, which means that
    they are considered as anomalies by the algorithm. The indices of the core instances
    are available in the `core_sample_indices_` instance variable, and the core instances
    themselves are available in the `components_` instance variable:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一些实例的簇索引等于-1，这意味着它们被算法视为异常。核心实例的索引可在`core_sample_indices_`实例变量中找到，而核心实例本身可在`components_`实例变量中找到：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This clustering is represented in the lefthand plot of [Figure 8-13](#dbscan_plot).
    As you can see, it identified quite a lot of anomalies, plus seven different clusters.
    How disappointing! Fortunately, if we widen each instance’s neighborhood by increasing
    `eps` to 0.2, we get the clustering on the right, which looks perfect. Let’s continue
    with this model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类在[图8-13](#dbscan_plot)的左侧图中表示。如图所示，它识别出相当多的异常，以及七个不同的簇。多么令人失望！幸运的是，如果我们通过增加`eps`到0.2来扩大每个实例的邻近区域，我们就能得到右侧的聚类，看起来非常完美。让我们继续使用这个模型。
- en: '![DBSCAN clustering results with `eps` values of 0.05 and 0.20, showing improved
    clustering with fewer anomalies in the right plot.](assets/hmls_0813.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![使用`eps`值为0.05和0.20的DBSCAN聚类结果，显示右侧图中异常更少且聚类改进的情况。](assets/hmls_0813.png)'
- en: Figure 8-13\. DBSCAN clustering using two different neighborhood radiuses
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13。使用两个不同的邻近区域半径进行DBSCAN聚类
- en: 'Surprisingly, the `DBSCAN` class does not have a `predict()` method, although
    it has a `fit_predict()` method. In other words, it cannot predict which cluster
    a new instance belongs to. This decision was made because different classification
    algorithms can be better for different tasks, so the authors decided to let the
    user choose which one to use. Moreover, it’s not hard to implement. For example,
    let’s train a `KNeighborsClassifier`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 意外地，`DBSCAN`类没有`predict()`方法，尽管它有`fit_predict()`方法。换句话说，它不能预测一个新实例属于哪个簇。这个决定是因为不同的分类算法可能对不同任务更好，因此作者决定让用户选择使用哪一个。此外，这并不难实现。例如，让我们训练一个`KNeighborsClassifier`：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, given a few new instances, we can predict which clusters they most likely
    belong to and even estimate a probability for each cluster:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，给定几个新的实例，我们可以预测它们最有可能属于哪个簇，甚至可以估计每个簇的概率：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that we only trained the classifier on the core instances, but we could
    also have chosen to train it on all the instances, or all but the anomalies: this
    choice depends on the final task.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们只在核心实例上训练了分类器，但我们也可以选择在所有实例上训练，或者除了异常之外的所有实例：这个选择取决于最终任务。
- en: 'The decision boundary is represented in [Figure 8-14](#cluster_classification_plot)
    (the crosses represent the four instances in `X_new`). Notice that since there
    is no anomaly in the training set, the classifier always chooses a cluster, even
    when that cluster is far away. It is fairly straightforward to introduce a maximum
    distance, in which case the two instances that are far away from both clusters
    are classified as anomalies. To do this, use the `kneighbors()` method of the
    `KNeighborsClassifier`. Given a set of instances, it returns the distances and
    the indices of the *k*-nearest neighbors in the training set (two matrices, each
    with *k* columns):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界在[图8-14](#cluster_classification_plot)中表示（交叉点代表`X_new`中的四个实例）。请注意，由于训练集中没有异常值，分类器总是选择一个簇，即使那个簇离得很远。引入最大距离相当直接，在这种情况下，距离两个簇都远的两个实例被分类为异常。为此，使用`KNeighborsClassifier`的`kneighbors()`方法。给定一组实例，它返回训练集中*k*-近邻的距离和索引（两个矩阵，每个矩阵有*k*列）：
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Diagram showing a decision boundary between two clusters, illustrating how
    DBSCAN identifies clusters of varying shapes.](assets/hmls_0814.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![展示两个簇之间的决策边界，说明DBSCAN如何识别不同形状的簇的图](assets/hmls_0814.png)'
- en: Figure 8-14\. Decision boundary between two clusters
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-14\. 两个簇之间的决策边界
- en: In short, DBSCAN is a very simple yet powerful algorithm capable of identifying
    any number of clusters of any shape. It is robust to outliers, and it has just
    two hyperparameters (`eps` and `min_samples`). If the density varies significantly
    across the clusters, however, or if there’s no sufficiently low-density region
    around some clusters, DBSCAN can struggle to capture all the clusters properly.
    Moreover, its computational complexity is roughly *O*(*m*²*n*), so it does not
    scale well to large datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，DBSCAN是一个非常简单但功能强大的算法，能够识别任何数量和形状的簇。它对异常值具有鲁棒性，并且只有两个超参数（`eps`和`min_samples`）。然而，如果簇之间的密度差异很大，或者如果某些簇周围没有足够低密度的区域，DBSCAN可能难以正确捕获所有簇。此外，它的计算复杂度大约是*O*(*m*²*n*)，因此它不适合大型数据集。
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You may also want to try *hierarchical DBSCAN* (HDBSCAN), using `sklearn.cluster.HDBSCAN`:
    it is often better than DBSCAN at finding clusters of varying densities.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试使用`sklearn.cluster.HDBSCAN`的*层次DBSCAN*（HDBSCAN），它通常比DBSCAN在寻找不同密度的簇方面表现更好。
- en: Other Clustering Algorithms
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他聚类算法
- en: 'Scikit-Learn implements several more clustering algorithms that you should
    take a look at. I cannot cover them all in detail here, but here is a brief overview:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn实现了更多聚类算法，你应该看看。我无法在这里详细说明它们，但这里有一个简要概述：
- en: Agglomerative clustering
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类层次
- en: A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles
    floating on water and gradually attaching to each other until there’s one big
    group of bubbles. Similarly, at each iteration, agglomerative clustering connects
    the nearest pair of clusters (starting with individual instances). If you drew
    a tree with a branch for every pair of clusters that merged, you would get a binary
    tree of clusters, where the leaves are the individual instances. This approach
    can capture clusters of various shapes; it also produces a flexible and informative
    cluster tree instead of forcing you to choose a particular cluster scale, and
    it can be used with any pairwise distance. It can scale nicely to large numbers
    of instances if you provide a connectivity matrix, which is a sparse *m* × *m*
    matrix that indicates which pairs of instances are neighbors (e.g., returned by
    `sklearn.neighbors.kneighbors_graph()`). Without a connectivity matrix, the algorithm
    does not scale well to large datasets.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从底部向上构建簇的层次结构。想象许多小气泡在水面上漂浮，逐渐附着在一起，直到形成一个大的气泡群。同样，在每次迭代中，层次聚类连接最近的两个簇（从单个实例开始）。如果你为每次合并的簇对画一个分支，你会得到一个簇的二叉树，其中叶子是单个实例。这种方法可以捕获各种形状的簇；它还产生了一个灵活且信息丰富的簇树，而不是强迫你选择特定的簇尺度，并且可以使用任何成对距离。如果你提供一个连接矩阵，这种方法可以很好地扩展到大量实例，连接矩阵是一个稀疏的*m*
    × *m*矩阵，它指示哪些实例对是邻居（例如，由`sklearn.neighbors.kneighbors_graph()`返回）。如果没有连接矩阵，该算法不适合大型数据集。
- en: BIRCH
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: BIRCH
- en: 'The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm
    was designed specifically for very large datasets, and it can be faster than batch
    *k*-means, with similar results, as long as the number of features is not too
    large (<20). During training, it builds a tree structure containing just enough
    information to quickly assign each new instance to a cluster, without having to
    store all the instances in the tree: this approach allows it to use limited memory
    while handling huge datasets.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡迭代减少和层次聚类（BIRCH）算法是专门为非常大的数据集设计的，并且它可以比批处理*k*-means更快，只要特征数量不是太大（<20）。在训练过程中，它构建一个包含足够信息以快速将每个新实例分配到簇中的树结构，而无需在树中存储所有实例：这种方法允许它在处理大型数据集的同时使用有限的内存。
- en: Mean-shift
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移
- en: This algorithm starts by placing a circle centered on each instance; then for
    each circle it computes the mean of all the instances located within it, and it
    shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting
    step until all the circles stop moving (i.e., until each of them is centered on
    the mean of the instances it contains). Mean-shift shifts the circles in the direction
    of higher density, until each of them has found a local density maximum. Finally,
    all the instances whose circles have settled in the same place (or close enough)
    are assigned to the same cluster. Mean-shift has some of the same features as
    DBSCAN, like how it can find any number of clusters of any shape, it has very
    few hyperparameters (just one—the radius of the circles, called the *bandwidth*),
    and it relies on local density estimation. But unlike DBSCAN, mean-shift tends
    to chop clusters into pieces when they have internal density variations. Unfortunately,
    its computational complexity is *O*(*m*²*n*), so it is not suited for large datasets.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法首先在每个实例上放置一个圆圈；然后对每个圆圈，它计算位于其内的所有实例的平均值，并将圆圈移动到平均值中心。接下来，它迭代这个均值移动步骤，直到所有圆圈停止移动（即，直到每个圆圈都位于其包含的实例的平均值中心）。均值漂移将圆圈向更高密度的方向移动，直到每个圆圈都找到了一个局部密度最大值。最后，所有圆圈最终停留在相同位置（或足够接近）的实例被分配到同一个簇。均值漂移具有与DBSCAN一些相同的特性，例如它可以找到任何数量和形状的簇，它有非常少的超参数（只有一个——圆圈的半径，称为*带宽*），并且它依赖于局部密度估计。但与DBSCAN不同，当簇有内部密度变化时，均值漂移倾向于将簇分割成块。不幸的是，它的计算复杂度为*O*(*m*²*n*)，所以它不适合大型数据集。
- en: Affinity propagation
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和传播
- en: 'In this algorithm, instances repeatedly exchange messages between one another
    until every instance has elected another instance (or itself) to represent it.
    These elected instances are called *exemplars*. Each exemplar and all the instances
    that elected it form one cluster. In real-life politics, you typically want to
    vote for a candidate whose opinions are similar to yours, but you also want them
    to win the election, so you might choose a candidate you don’t fully agree with,
    but who is more popular. You typically evaluate popularity through polls. Affinity
    propagation works in a similar way, and it tends to choose exemplars located near
    the center of clusters, similar to *k*-means. But unlike with *k*-means, you don’t
    have to pick a number of clusters ahead of time: it is determined during training.
    Moreover, affinity propagation can deal nicely with clusters of different sizes.
    Sadly, this algorithm has a computational complexity of *O*(*m*²), so it is not
    suited for large datasets.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，实例之间会反复相互交换信息，直到每个实例都选举另一个实例（或它自己）来代表它。这些被选举的实例被称为*范例*。每个范例及其所有选举它的实例组成一个簇。在现实生活中的政治中，你通常希望投票给一个与你意见相似的候选人，但你同时也希望他们能赢得选举，所以你可能选择一个你并不完全同意，但更受欢迎的候选人。你通常通过民意调查来评估受欢迎程度。亲和传播以类似的方式工作，并且倾向于选择位于簇中心附近的范例，类似于*k*-means。但与*k*-means不同，你不必提前选择簇的数量：它是在训练过程中确定的。此外，亲和传播可以很好地处理不同大小的簇。遗憾的是，这个算法的计算复杂度为*O*(*m*²)，所以它不适合大型数据集。
- en: Spectral clustering
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类
- en: This algorithm takes a similarity matrix between the instances and creates a
    low-dimensional embedding from it (i.e., it reduces the matrix’s dimensionality),
    then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s
    implementation uses *k*-means). Spectral clustering can capture complex cluster
    structures, and it can also be used to cut graphs (e.g., to identify clusters
    of friends on a social network). It does not scale well to large numbers of instances,
    and it does not behave well when the clusters have very different sizes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法接受实例之间的相似性矩阵，并从中创建一个低维嵌入（即，它降低了矩阵的维度），然后它在这个低维空间中使用另一个聚类算法（Scikit-Learn 的实现使用
    *k*-means）。谱聚类可以捕捉复杂的聚类结构，并且也可以用于切割图（例如，用于识别社交网络上的朋友聚类）。它不适合大量实例，并且当聚类具有非常不同的尺寸时表现不佳。
- en: Now let’s dive into Gaussian mixture models, which can be used for density estimation,
    clustering, and anomaly detection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解高斯混合模型，它可以用于密度估计、聚类和异常检测。
- en: Gaussian Mixtures
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合
- en: A *Gaussian mixture model* (GMM) is a probabilistic model that assumes that
    the instances were generated from a mixture of several Gaussian distributions
    whose parameters are unknown. All the instances generated from a single Gaussian
    distribution form a cluster that typically looks like an ellipsoid. Each cluster
    can have a different ellipsoidal shape, size, density, and orientation, just like
    in [Figure 8-10](#bad_kmeans_plot).⁠^([7](ch08.html#id2046)) When you observe
    an instance, you know it was generated from one of the Gaussian distributions,
    but you are not told which one, and you do not know what the parameters of these
    distributions are.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯混合模型*（GMM）是一个概率模型，它假设实例是从几个未知参数的高斯分布的混合中生成的。来自单个高斯分布的所有实例形成一个通常看起来像椭球体的聚类。每个聚类可以具有不同的椭球形状、大小、密度和方向，就像在[图
    8-10](#bad_kmeans_plot) 中所示。当你观察一个实例时，你知道它是从某个高斯分布中生成的，但你没有被告知是哪一个，也不知道这些分布的参数是什么。'
- en: 'There are several GMM variants. In the simplest variant, implemented in the
    `GaussianMixture` class, you must know in advance the number *k* of Gaussian distributions.
    The dataset **X** is assumed to have been generated through the following probabilistic
    process:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种 GMM 变体。在最简单的变体中，由 `GaussianMixture` 类实现，你必须事先知道高斯分布的数量 *k*。数据集 **X** 假设是通过以下概率过程生成的：
- en: For each instance, a cluster is picked randomly from among *k* clusters. The
    probability of choosing the *j*^(th) cluster is the cluster’s weight *ϕ*^((*j*)).⁠^([8](ch08.html#id2050))
    The index of the cluster chosen for the *i*^(th) instance is denoted *z*^((*i*)).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个实例，从 *k* 个聚类中随机选择一个聚类。选择第 *j* 个聚类的概率是聚类的权重 *ϕ*^((*j*))。^([8](ch08.html#id2050))
    为第 *i* 个实例选择的聚类索引表示为 *z*^((*i*)).
- en: If the *i*^(th) instance was assigned to the *j*^(th) cluster (i.e., *z*^((*i*))
    = *j*), then the location **x**^((*i*)) of this instance is sampled randomly from
    the Gaussian distribution with mean **μ**^((*j*)) and covariance matrix **Σ**^((*j*)).
    This is denoted **x**^((*i*)) ~ 𝒩(μ*^((*j*)), **Σ**^((*j*))).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第 *i* 个实例被分配到第 *j* 个聚类（即，*z*^((*i*)) = *j*），那么这个实例的位置 **x**^((*i*)) 是从具有均值
    **μ**^((*j*)) 和协方差矩阵 **Σ**^((*j*)) 的高斯分布中随机采样的。这表示为 **x**^((*i*)) ~ 𝒩(μ*^((*j*)),
    **Σ**^((*j*))).
- en: 'So what can you do with such a model? Well, given the dataset **X**, you typically
    want to start by estimating the weights **ϕ** and all the distribution parameters
    **μ**^((1)) to **μ**^((*k*)) and **Σ**^((1)) to **Σ**^((*k*)). Scikit-Learn’s
    `GaussianMixture` class makes this super easy:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你可以用这样的模型做什么呢？嗯，给定数据集 **X**，你通常想首先估计权重 **ϕ** 以及所有分布参数 **μ**^((1)) 到 **μ**^((*k*))
    和 **Σ**^((1)) 到 **Σ**^((*k*)）。Scikit-Learn 的 `GaussianMixture` 类使这变得非常简单：
- en: '[PRE29]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s look at the parameters that the algorithm estimated:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看算法估计的参数：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Great, it worked fine! Indeed, two of the three clusters were generated with
    500 instances each, while the third cluster only contains 250 instances. So the
    true cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly what
    the algorithm found (in a different order). Similarly, the true means and covariance
    matrices are quite close to those found by the algorithm. But how? This class
    relies on the *expectation-maximization* (EM) algorithm, which has many similarities
    with the *k*-means algorithm: it also initializes the cluster parameters randomly,
    then it repeats two steps until convergence, first assigning instances to clusters
    (this is called the *expectation step*) and then updating the clusters (this is
    called the *maximization step*). Sounds familiar, right? In the context of clustering,
    you can think of EM as a generalization of *k*-means that not only finds the cluster
    centers (**μ**^((1)) to **μ**^((*k*))), but also their size, shape, and orientation
    (**Σ**^((1)) to **Σ**^((*k*))), as well as their relative weights (*ϕ*^((1)) to
    *ϕ*^((*k*))). Unlike *k*-means, though, EM uses soft cluster assignments, not
    hard assignments. For each instance, during the expectation step, the algorithm
    estimates the probability that it belongs to each cluster (based on the current
    cluster parameters). Then, during the maximization step, each cluster is updated
    using *all* the instances in the dataset, with each instance weighted by the estimated
    probability that it belongs to that cluster. These probabilities are called the
    *responsibilities* of the clusters for the instances. During the maximization
    step, each cluster’s update will mostly be impacted by the instances it is most
    responsible for.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，它运行得很好！确实，三个簇中的两个生成了包含 500 个实例的簇，而第三个簇只包含 250 个实例。所以真实的簇权重分别是 0.4、0.4 和
    0.2，这正是算法找到的（顺序不同）。同样，真实的均值和协方差矩阵与算法找到的相当接近。但这是如何实现的呢？这个课程依赖于 *期望最大化*（EM）算法，它与
    *k*-means 算法有许多相似之处：它也随机初始化簇参数，然后重复两个步骤直到收敛，首先将实例分配给簇（这被称为 *期望步骤*），然后更新簇（这被称为
    *最大化步骤*）。听起来很熟悉，对吧？在聚类的背景下，您可以将 EM 视为 *k*-means 的推广，它不仅找到簇中心（**μ**^((1)) 到 **μ**^((*k*)))，还找到它们的大小、形状和方向（**Σ**^((1))
    到 **Σ**^((*k*)))，以及它们的相对权重（*ϕ*^((1)) 到 *ϕ*^((*k*)))。然而，与 *k*-means 不同的是，EM 使用软簇分配，而不是硬分配。对于每个实例，在期望步骤中，算法根据当前的簇参数估计它属于每个簇的概率。然后，在最大化步骤中，每个簇使用数据集中的所有实例进行更新，每个实例根据它属于该簇的估计概率进行加权。这些概率被称为簇对实例的
    *责任*。在最大化步骤中，每个簇的更新将主要受其最负责的实例的影响。
- en: Warning
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Unfortunately, just like *k*-means, EM can end up converging to poor solutions,
    so it needs to be run several times, keeping only the best solution. This is why
    we set `n_init` to 10\. Be careful: by default `n_init` is set to 1.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，就像 *k*-means 算法一样，EM 算法可能会收敛到较差的解，因此需要多次运行，只保留最佳解。这就是为什么我们将 `n_init` 设置为
    10。请注意：默认情况下 `n_init` 被设置为 1。
- en: 'You can check whether the algorithm converged and how many iterations it took:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查算法是否收敛以及迭代了多少次：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that you have an estimate of the location, size, shape, orientation, and
    relative weight of each cluster, the model can easily assign each instance to
    the most likely cluster (hard clustering) or estimate the probability that it
    belongs to a particular cluster (soft clustering). Just use the `predict()` method
    for hard clustering, or the `predict_proba()` method for soft clustering:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经估计了每个簇的位置、大小、形状、方向和相对权重，模型可以轻松地将每个实例分配给最可能的簇（硬聚类）或估计它属于特定簇的概率（软聚类）。只需使用
    `predict()` 方法进行硬聚类，或使用 `predict_proba()` 方法进行软聚类：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'A Gaussian mixture model is a *generative model*, meaning you can sample new
    instances from it (note that they are ordered by cluster index):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型是一种 *生成模型*，这意味着您可以从它中抽取新的实例（请注意，它们按簇索引排序）：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is also possible to estimate the density of the model at any given location.
    This is achieved using the `score_samples()` method: for each instance it is given,
    this method estimates the log of the *probability density function* (PDF) at that
    location. The greater the score, the higher the density:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在任何给定位置估计模型的密度。这是通过使用 `score_samples()` 方法实现的：对于每个给定的实例，此方法估计该位置的 *概率密度函数*（PDF）的对数。分数越高，密度越高：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you compute the exponential of these scores, you get the value of the PDF
    at the location of the given instances. These are not probabilities, but probability
    *densities*: they can take on any positive value, not just a value between 0 and
    1\. To estimate the probability that an instance will fall within a particular
    region, you would have to integrate the PDF over that region (if you do so over
    the entire space of possible instance locations, the result will be 1).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算这些分数的指数，你将得到给定实例位置的PDF值。这些不是概率，而是概率*密度*：它们可以取任何正值，而不仅仅是介于0和1之间的值。为了估计实例落在特定区域内的概率，你必须在该区域上对PDF进行积分（如果你在整个可能的实例位置空间上这样做，结果将是1）。
- en: '[Figure 8-15](#gaussian_mixtures_plot) shows the cluster means, the decision
    boundaries (dashed lines), and the density contours of this model.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-15](#gaussian_mixtures_plot)显示了该模型的聚类均值、决策边界（虚线）和密度等高线。'
- en: '![Diagram of a Gaussian mixture model showing three cluster means, decision
    boundaries as dashed lines, and density contours indicating distribution.](assets/hmls_0815.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![显示具有三个聚类均值、作为虚线的决策边界和指示分布的密度等高线的高斯混合模型图](assets/hmls_0815.png)'
- en: Figure 8-15\. Cluster means, decision boundaries, and density contours of a
    trained Gaussian mixture model
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15。训练好的高斯混合模型的聚类均值、决策边界和密度等高线
- en: 'Nice! The algorithm clearly found an excellent solution. Of course, we made
    its task easy by generating the data using a set of 2D Gaussian distributions
    (unfortunately, real-life data is not always so Gaussian and low-dimensional).
    We also gave the algorithm the correct number of clusters. When there are many
    dimensions, or many clusters, or few instances, EM can struggle to converge to
    the optimal solution. You might need to reduce the difficulty of the task by limiting
    the number of parameters that the algorithm has to learn. One way to do this is
    to limit the range of shapes and orientations that the clusters can have. This
    can be achieved by imposing constraints on the covariance matrices. To do this,
    set the `covariance_type` hyperparameter to one of the following values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！算法明显找到了一个优秀的解决方案。当然，我们通过使用一组二维高斯分布生成数据来简化了其任务（不幸的是，现实生活中的数据并不总是如此高斯和低维）。我们还给出了算法正确的聚类数量。当维度很多、聚类很多或实例很少时，EM算法可能难以收敛到最优解。你可能需要通过限制算法必须学习的参数数量来降低任务的难度。一种方法是限制聚类可以具有的形状和方向的范围。这可以通过对协方差矩阵施加约束来实现。为此，将`covariance_type`超参数设置为以下值之一：
- en: '`"spherical"`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`"spherical"`'
- en: All clusters must be spherical, but they can have different diameters (i.e.,
    different variances).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聚类都必须是球形的，但它们可以有不同的直径（即不同的方差）。
- en: '`"diag"`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`"diag"`'
- en: Clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s
    axes must be parallel to the coordinate axes (i.e., the covariance matrices must
    be diagonal).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以具有任何大小和形状的椭圆形，但椭圆的轴必须与坐标轴平行（即协方差矩阵必须是对角线矩阵）。
- en: '`"tied"`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`"tied"`'
- en: All clusters must have the same ellipsoidal shape, size, and orientation (i.e.,
    all clusters share the same covariance matrix).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聚类必须具有相同的椭圆形形状、大小和方向（即所有聚类共享相同的协方差矩阵）。
- en: By default, `covariance_type` is equal to `"full"`, which means that each cluster
    can take on any shape, size, and orientation (it has its own unconstrained covariance
    matrix). [Figure 8-16](#covariance_type_plot) plots the solutions found by the
    EM algorithm when `covariance_type` is set to `"tied"` or `"spherical"`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`covariance_type`等于`"full"`，这意味着每个聚类可以具有任何形状、大小和方向（它有自己的无约束协方差矩阵）。[图8-16](#covariance_type_plot)绘制了当`covariance_type`设置为`"tied"`或`"spherical"`时EM算法找到的解决方案。
- en: '![Diagram showing Gaussian mixture models for tied covariance (left) with oval
    clusters and spherical covariance (right) with circular clusters.](assets/hmls_0816.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![显示具有椭圆形聚类的耦合协方差（左）和具有球形协方差（右）的圆形聚类的高斯混合模型图](assets/hmls_0816.png)'
- en: Figure 8-16\. Gaussian mixtures for tied clusters (left) and spherical clusters
    (right)
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16。耦合聚类（左）和球形聚类（右）的高斯混合模型
- en: Note
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The computational complexity of training a `GaussianMixture` model depends on
    the number of instances *m*, the number of dimensions *n*, the number of clusters
    *k*, and the constraints on the covariance matrices. If `covariance_type` is `"spherical"`
    or `"diag"`, it is *O*(*kmn*), assuming the data has a clustering structure. If
    `covariance_type` is `"tied"` or `"full"`, it is *O*(*kmn*² + *kn*³), so it will
    not scale to large numbers of features.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 训练`GaussianMixture`模型的计算复杂度取决于实例数量*m*、维度数量*n*、聚类数量*k*以及协方差矩阵的约束。如果`covariance_type`是`"spherical"`或`"diag"`，它是*O*(*kmn*)，假设数据具有聚类结构。如果`covariance_type`是`"tied"`或`"full"`，它是*O*(*kmn*²
    + *kn*³)，因此它不能扩展到大量特征。
- en: Gaussian mixture models can also be used for anomaly detection. We’ll see how
    in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型也可以用于异常检测。我们将在下一节中看到如何使用。
- en: Using Gaussian Mixtures for Anomaly Detection
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行异常检测
- en: 'Using a Gaussian mixture model for anomaly detection is quite simple: any instance
    located in a low-density region can be considered an anomaly. You must define
    what density threshold you want to use. For example, in a manufacturing company
    that tries to detect defective products, the ratio of defective products is usually
    well known. Say it is equal to 2%. You then set the density threshold to be the
    value that results in having 2% of the instances located in areas below that threshold
    density. If you notice that you get too many false positives (i.e., perfectly
    good products that are flagged as defective), you can lower the threshold. Conversely,
    if you have too many false negatives (i.e., defective products that the system
    does not flag as defective), you can increase the threshold. This is the usual
    precision/recall trade-off (see [Chapter 3](ch03.html#classification_chapter)).
    Here is how you would identify the outliers using the second percentile lowest
    density as the threshold (i.e., approximately 2% of the instances will be flagged
    as anomalies):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行异常检测非常简单：任何位于低密度区域的实例都可以被认为是异常。你必须定义你想要使用的密度阈值。例如，在一个试图检测次品产品的制造公司中，次品产品的比例通常是众所周知的。比如说它等于2%。然后你将密度阈值设置为在低于该阈值密度的区域中定位2%的实例的值。如果你注意到你得到了太多的假阳性（即被标记为次品的完美产品），你可以降低阈值。相反，如果你有太多的假阴性（即系统没有标记为次品的次品），你可以提高阈值。这是通常的精确度/召回率权衡（见[第3章](ch03.html#classification_chapter)）。以下是如何使用第二个百分位最低密度作为阈值（即，大约2%的实例将被标记为异常）来识别异常值：
- en: '[PRE35]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[Figure 8-17](#mixture_anomaly_detection_plot) represents these anomalies as
    stars.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-17](#mixture_anomaly_detection_plot)将这些异常值表示为星号。'
- en: '![Contour plot illustrating anomaly detection with a Gaussian mixture model,
    where anomalies are marked with red stars.](assets/hmls_0817.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用高斯混合模型进行异常检测的等高线图，其中异常值用红色星号标记](assets/hmls_0817.png)'
- en: Figure 8-17\. Anomaly detection using a Gaussian mixture model
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-17\. 使用高斯混合模型进行异常检测
- en: 'A closely related task is *novelty detection*: it differs from anomaly detection
    in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated
    by outliers, whereas anomaly detection does not make this assumption. Indeed,
    outlier detection is often used to clean up a dataset.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与之密切相关任务是*新颖性检测*：它与异常检测不同，因为算法假设是在一个“干净”的数据集上训练的，不受异常值污染，而异常检测不做出这种假设。实际上，异常值检测通常用于清理数据集。
- en: Tip
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Gaussian mixture models try to fit all the data, including the outliers; if
    you have too many of them this will bias the model’s view of “normality”, and
    some outliers may wrongly be considered as normal. If this happens, you can try
    to fit the model once, use it to detect and remove the most extreme outliers,
    then fit the model again on the cleaned-up dataset. Another approach is to use
    robust covariance estimation methods (see the `EllipticEnvelope` class).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型试图拟合所有数据，包括异常值；如果你有太多的异常值，这将偏颇模型对“正常性”的看法，并且一些异常值可能被错误地认为是正常的。如果发生这种情况，你可以尝试一次拟合模型，使用它来检测和删除最极端的异常值，然后再次在清理后的数据集上拟合模型。另一种方法是使用鲁棒的协方差估计方法（见`EllipticEnvelope`类）。
- en: Just like *k*-means, the `GaussianMixture` algorithm requires you to specify
    the number of clusters. So how can you find that number?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 就像*k*-means一样，`GaussianMixture`算法需要你指定聚类数量。那么你如何找到这个数量呢？
- en: Selecting the Number of Clusters
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择聚类数量
- en: With *k*-means, you can use the inertia or the silhouette score to select the
    appropriate number of clusters. But with Gaussian mixtures, it is not possible
    to use these metrics because they are not reliable when the clusters are not spherical
    or have different sizes. Instead, you can try to find the model that minimizes
    a *theoretical information criterion*, such as the *Bayesian information criterion*
    (BIC) or the *Akaike information criterion* (AIC), defined in [Equation 8-2](#information_criteria_equation).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*k*-means，你可以使用惯性或轮廓分数来选择合适的簇数。但是，对于高斯混合，由于簇不是球形或大小不同时这些指标不可靠，因此不能使用这些指标。相反，你可以尝试找到最小化*理论信息准则*的模型，例如定义在[方程8-2](#information_criteria_equation)中的*贝叶斯信息准则*（BIC）或*赤池信息准则*（AIC）。
- en: Equation 8-2\. Bayesian information criterion (BIC) and Akaike information criterion
    (AIC)
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程8-2\. 贝叶斯信息准则（BIC）和赤池信息准则（AIC）
- en: $StartLayout 1st Row 1st Column Blank 2nd Column upper B upper I upper C equals
    normal l normal o normal g left-parenthesis m right-parenthesis p minus 2 normal
    l normal o normal g left-parenthesis ModifyingAbove script upper L With caret
    right-parenthesis 2nd Row 1st Column Blank 2nd Column upper A upper I upper C
    equals 2 p minus 2 normal l normal o normal g left-parenthesis ModifyingAbove
    script upper L With caret right-parenthesis EndLayout$
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column 空白 2nd Column upper B upper I upper C 等于 正态
    l 正态 o 正态 g 左括号 m 右括号 p 减 2 正态 l 正态 o 正态 g 左括号 修改以上脚本 upper L With 右箭头 2nd Row
    1st Column 空白 2nd Column upper A upper I upper C 等于 2 p 减 2 正态 l 正态 o 正态 g 左括号
    修改以上脚本 upper L With 右箭头 EndLayout$
- en: 'In these equations:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程中：
- en: '*m* is the number of instances, as always.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* 是实例的数量，一如既往。'
- en: '*p* is the number of parameters learned by the model.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 是模型学习的参数数量。'
- en: $ModifyingAbove script upper L With caret$ is the maximized value of the *likelihood
    function* of the model.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $ModifyingAbove script upper L With caret$ 是模型*似然函数*的最大值。
- en: Both the BIC and the AIC penalize models that have more parameters to learn
    (e.g., more clusters) and reward models that fit the data well. They often end
    up selecting the same model. When they differ, the model selected by the BIC tends
    to be simpler (fewer parameters) than the one selected by the AIC, but tends to
    not fit the data quite as well (this is especially true for larger datasets).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: BIC和AIC都惩罚需要学习更多参数的模型（例如，更多簇）并奖励拟合数据良好的模型。它们通常会选择相同的模型。当它们不同时，BIC选择的模型通常比AIC选择的模型更简单（参数更少），但拟合数据并不完全一样（这对于大型数据集尤其如此）。
- en: 'To compute the BIC and AIC, call the `bic()` and `aic()` methods:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算BIC和AIC，请调用`bic()`和`aic()`方法：
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[Figure 8-19](#aic_bic_vs_k_plot) shows the BIC for different numbers of clusters
    *k*. As you can see, both the BIC and the AIC are lowest when *k* = 3, so it is
    most likely the best choice.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-19](#aic_bic_vs_k_plot)显示了不同簇数*k*的BIC。如图所示，当*k* = 3时，BIC和AIC都达到最低，因此这很可能是最佳选择。'
- en: '![Plot showing AIC and BIC values for different numbers of clusters _k_, with
    both criteria reaching their minimum at _k_ = 3.](assets/hmls_0819.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![展示不同簇数_k_的AIC和BIC值的图表，两个标准在_k_ = 3时达到最小值。](assets/hmls_0819.png)'
- en: Figure 8-19\. AIC and BIC for different numbers of clusters *k*
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-19\. 不同簇数*k*的AIC和BIC
- en: Bayesian Gaussian Mixture Models
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯高斯混合模型
- en: 'Rather than manually searching for the optimal number of clusters, you can
    use the `BayesianGaussianMixture` class, which is capable of giving weights equal
    (or close) to zero to unnecessary clusters. Set the number of clusters `n_components`
    to a value that you have good reason to believe is greater than the optimal number
    of clusters (this assumes some minimal knowledge about the problem at hand), and
    the algorithm will eliminate the unnecessary clusters automatically. For example,
    let’s set the number of clusters to 10 and see what happens:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是手动搜索最佳簇数，你可以使用`BayesianGaussianMixture`类，该类能够将权重设置为等于（或接近）零的不必要簇。将簇数`n_components`设置为大于最佳簇数的值（这假设对当前问题有一些基本了解），算法将自动消除不必要簇。例如，让我们将簇数设置为10，看看会发生什么：
- en: '[PRE37]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perfect: the algorithm automatically detected that only three clusters are
    needed, and the resulting clusters are almost identical to the ones in [Figure 8-15](#gaussian_mixtures_plot).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 完美：算法自动检测到只需要三个簇，并且生成的簇几乎与[图8-15](#gaussian_mixtures_plot)中的簇相同。
- en: 'A final note about Gaussian mixture models: although they work great on clusters
    with ellipsoidal shapes, they don’t do so well with clusters of very different
    shapes. For example, let’s see what happens if we use a Bayesian Gaussian mixture
    model to cluster the moons dataset (see [Figure 8-20](#moons_vs_bgm_plot)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 关于高斯混合模型的一个最后说明：尽管它们在椭圆形形状的簇上表现良好，但它们在形状非常不同的簇上表现不佳。例如，让我们看看如果我们使用贝叶斯高斯混合模型对月亮数据集进行聚类会发生什么（参见[图8-20](#moons_vs_bgm_plot))。
- en: Oops! The algorithm desperately searched for ellipsoids, so it found eight different
    clusters instead of two. The density estimation is not too bad, so this model
    could perhaps be used for anomaly detection, but it failed to identify the two
    moons. To conclude this chapter, let’s take a quick look at a few algorithms capable
    of dealing with arbitrarily shaped clusters.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！该算法拼命地寻找椭圆体，所以它找到了八个不同的簇而不是两个。密度估计还不错，所以这个模型或许可以用于异常检测，但它未能识别出两个月亮。为了总结本章，让我们快速看一下一些能够处理任意形状簇的算法。
- en: '![Diagram showing a Gaussian mixture model''s attempt to fit nonellipsoidal
    clusters, illustrating its limitation in detecting two moon-shaped clusters by
    identifying eight separate sections instead.](assets/hmls_0820.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![展示高斯混合模型尝试拟合非椭圆形簇的图表，说明其通过识别八个独立部分而不是两个月亮形状簇来检测簇的限制](assets/hmls_0820.png)'
- en: Figure 8-20\. Fitting a Gaussian mixture to nonellipsoidal clusters
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-20. 将高斯混合模型拟合到非椭圆形簇
- en: Other Algorithms for Anomaly and Novelty Detection
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他异常和新颖性检测算法
- en: 'Scikit-Learn implements other algorithms dedicated to anomaly detection or
    novelty detection:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn实现了其他针对异常检测或新颖性检测的算法：
- en: Fast-MCD (minimum covariance determinant)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 快速MCD（最小协方差行列式）
- en: Implemented by the `EllipticEnvelope` class, this algorithm is useful for outlier
    detection, in particular to clean up a dataset. It assumes that the normal instances
    (inliers) are generated from a single Gaussian distribution (not a mixture). It
    also assumes that the dataset is contaminated with outliers that were not generated
    from this Gaussian distribution. When the algorithm estimates the parameters of
    the Gaussian distribution (i.e., the shape of the elliptic envelope around the
    inliers), it is careful to ignore the instances that are most likely outliers.
    This technique gives a better estimation of the elliptic envelope and thus makes
    the algorithm better at identifying the outliers.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`EllipticEnvelope`类实现，此算法对离群值检测很有用，特别是用于清理数据集。它假设正常实例（内点）是从单个高斯分布（不是混合分布）生成的。它还假设数据集受到未从该高斯分布生成的离群值的污染。当算法估计高斯分布的参数（即内点周围的椭圆形包络的形状）时，它会小心地忽略最可能是离群值的实例。这种技术提供了更好的椭圆形包络估计，从而使算法在识别离群值方面表现得更好。
- en: Isolation forest
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离森林
- en: 'This is an efficient algorithm for outlier detection, especially in high-dimensional
    datasets. The algorithm builds a random forest in which each decision tree is
    grown randomly: at each node, it picks a feature randomly, then it picks a random
    threshold value (between the min and max values) to split the dataset in two.
    The dataset gradually gets chopped into pieces this way, until all instances end
    up isolated from the other instances. Anomalies are usually far from other instances,
    so on average (across all the decision trees) they tend to get isolated in fewer
    steps than normal instances.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个高效的离群值检测算法，尤其是在高维数据集上。该算法构建一个随机森林，其中每个决策树都是随机生长的：在每个节点，它随机选择一个特征，然后选择一个随机阈值值（在最小值和最大值之间）将数据集分成两部分。数据集就这样逐渐被切割成小块，直到所有实例都与其他实例隔离。异常值通常离其他实例很远，所以平均而言（在所有决策树中）它们倾向于比正常实例更快地被隔离。
- en: Local outlier factor (LOF)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 局部离群因子（LOF）
- en: This algorithm is also good for outlier detection. It compares the density of
    instances around a given instance to the density around its neighbors. An anomaly
    is often more isolated than its *k*-nearest neighbors.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法也适用于离群值检测。它比较给定实例周围的实例密度与其邻居周围的密度。异常值通常比其**k**个最近邻更孤立。
- en: One-class SVM
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 单类SVM
- en: 'This algorithm is better suited for novelty detection. Recall that a kernelized
    SVM classifier separates two classes by first (implicitly) mapping all the instances
    to a high-dimensional space, then separating the two classes using a linear SVM
    classifier within this high-dimensional space (see the online chapter on SVMs
    at [*https://homl.info*](https://homl.info)). Since we just have one class of
    instances, the one-class SVM algorithm instead tries to separate the instances
    in high-dimensional space from the origin. In the original space, this will correspond
    to finding a small region that encompasses all the instances. If a new instance
    does not fall within this region, it is an anomaly. There are a few hyperparameters
    to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter that
    corresponds to the probability of a new instance being mistakenly considered as
    novel when it is in fact normal. It works great, especially with high-dimensional
    datasets, but like all SVMs it does not scale to large datasets.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法更适合新颖性检测。回想一下，核化 SVM 分类器首先（隐式地）将所有实例映射到一个高维空间，然后在这个高维空间内使用线性 SVM 分类器来分离这两个类别（参见关于
    SVM 的在线章节 [*https://homl.info*](https://homl.info)）。由于我们只有一个类别的实例，单类 SVM 算法试图在高维空间中将实例与原点分离。在原始空间中，这相当于找到一个包含所有实例的小区域。如果一个新实例不落在这个区域内，它就是一个异常。有几个超参数需要调整：核化
    SVM 的常规超参数，以及一个对应于新实例被错误地视为新颖（实际上它是正常的）的概率的边缘超参数。它工作得很好，尤其是在高维数据集上，但像所有 SVM 一样，它不能扩展到大数据集。
- en: PCA and other dimensionality reduction techniques with an `inverse_transform()`
    method
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 具有逆变换方法的 PCA 和其他降维技术
- en: If you compare the reconstruction error of a normal instance with the reconstruction
    error of an anomaly, the latter will usually be much larger. This is a simple
    and often quite efficient anomaly detection approach (see this chapter’s exercises
    for an example).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你比较正常实例的重建误差与异常实例的重建误差，后者通常会大得多。这是一个简单且通常相当有效的异常检测方法（例如，参见本章的练习）。
- en: Exercises
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: How would you define clustering? Can you name a few clustering algorithms?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何定义聚类？你能列举一些聚类算法吗？
- en: What are some of the main applications of clustering algorithms?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类算法有哪些主要应用？
- en: Describe two techniques to select the right number of clusters when using *k*-means.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述两种在应用 *k*-means 算法时选择正确聚类数量的技术。
- en: What is label propagation? Why would you implement it, and how?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签传播是什么？为什么你会实现它，以及如何实现？
- en: Can you name two clustering algorithms that can scale to large datasets? And
    two that look for regions of high density?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能列举两种可以扩展到大数据集的聚类算法？以及两种寻找高密度区域的算法？
- en: Can you think of a use case where active learning would be useful? How would
    you implement it?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到一个主动学习有用的用例吗？你将如何实现它？
- en: What is the difference between anomaly detection and novelty detection?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 异常检测与新颖性检测有什么区别？
- en: What is a Gaussian mixture? What tasks can you use it for?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯混合是什么？你可以用它来完成哪些任务？
- en: Can you name two techniques to find the right number of clusters when using
    a Gaussian mixture model?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能列举两种在应用高斯混合模型时寻找正确聚类数量的技术吗？
- en: 'The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images
    of faces. Each image is flattened to a 1D vector of size 4,096\. Forty different
    people were photographed (10 times each), and the usual task is to train a model
    that can predict which person is represented in each picture. Load the dataset
    using the `sklearn.datasets.fetch_olivetti_faces()` function, then split it into
    a training set, a validation set, and a test set (note that the dataset is already
    scaled between 0 and 1). Since the dataset is quite small, you will probably want
    to use stratified sampling to ensure that there are the same number of images
    per person in each set. Next, cluster the images using *k*-means, and ensure that
    you have a good number of clusters (using one of the techniques discussed in this
    chapter). Visualize the clusters: do you see similar faces in each cluster?'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经典的Olivetti人脸数据集包含400张64 × 64像素的灰度图像。每张图像被展平成一个大小为4,096的1D向量。40个不同的人被拍摄（每人10次），通常的任务是训练一个模型，可以预测每张图片中代表的是哪个人。使用`sklearn.datasets.fetch_olivetti_faces()`函数加载数据集，然后将其分为训练集、验证集和测试集（注意数据集已经缩放到0到1之间）。由于数据集相当小，你可能想使用分层抽样以确保每个集合中每个人的图像数量相同。接下来，使用k-means聚类图像，并确保你有足够多的聚类（使用本章讨论的其中一种技术）。可视化聚类：你在每个聚类中看到类似的面孔吗？
- en: 'Continuing with the Olivetti faces dataset, train a classifier to predict which
    person is represented in each picture, and evaluate it on the validation set.
    Next, use *k*-means as a dimensionality reduction tool, and train a classifier
    on the reduced set. Search for the number of clusters that allows the classifier
    to get the best performance: what performance can you reach? What if you append
    the features from the reduced set to the original features (again, searching for
    the best number of clusters)?'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续使用Olivetti人脸数据集，训练一个分类器来预测每张图片中代表的是哪个人，并在验证集上评估它。接下来，将k-means作为降维工具，并在降维集上训练一个分类器。寻找允许分类器获得最佳性能的聚类数量：你可以达到什么样的性能？如果你将降维集的特征附加到原始特征上（再次寻找最佳聚类数量）会怎样？
- en: Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the
    algorithm, you should probably reduce the dataset’s dimensionality (e.g., use
    PCA, preserving 99% of the variance). Use the model to generate some new faces
    (using the `sample()` method), and visualize them (if you used PCA, you will need
    to use its `inverse_transform()` method). Try to modify some images (e.g., rotate,
    flip, darken) and see if the model can detect the anomalies (i.e., compare the
    output of the `score_samples()` method for normal images and for anomalies).
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Olivetti人脸数据集上训练一个高斯混合模型。为了加快算法速度，你可能需要降低数据集的维度（例如，使用PCA，保留99%的方差）。使用该模型生成一些新的面孔（使用`sample()`方法），并可视化它们（如果你使用了PCA，你需要使用它的`inverse_transform()`方法）。尝试修改一些图像（例如，旋转、翻转、变暗）并查看模型是否可以检测到异常（即比较正常图像和异常的`score_samples()`方法的输出）。
- en: 'Some dimensionality reduction techniques can also be used for anomaly detection.
    For example, take the Olivetti faces dataset and reduce it with PCA, preserving
    99% of the variance. Then compute the reconstruction error for each image. Next,
    take some of the modified images you built in the previous exercise and look at
    their reconstruction error: notice how much larger it is. If you plot a reconstructed
    image, you will see why: it tries to reconstruct a normal face.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些降维技术也可以用于异常检测。例如，以Olivetti人脸数据集为例，使用PCA对其进行降维，保留99%的方差。然后计算每个图像的重建误差。接下来，取之前练习中构建的一些修改后的图像，查看它们的重建误差：注意它有多大。如果你绘制一个重建图像，你会看到原因：它试图重建一个正常的人脸。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: ^([1](ch08.html#id1959-marker)) If you are not familiar with probability theory,
    I highly recommend the free online classes by Khan Academy.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#id1959-marker)) 如果你不太熟悉概率论，我强烈推荐Khan Academy提供的免费在线课程。
- en: '^([2](ch08.html#id1973-marker)) Stuart P. Lloyd, “Least Squares Quantization
    in PCM”, *IEEE Transactions on Information Theory* 28, no. 2 (1982): 129–137.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch08.html#id1973-marker)) Stuart P. Lloyd, “Least Squares Quantization
    in PCM”, *IEEE Transactions on Information Theory* 28, no. 2 (1982): 129–137。'
- en: '^([3](ch08.html#id1983-marker)) David Arthur and Sergei Vassilvitskii, “k-Means++:
    The Advantages of Careful Seeding”, *Proceedings of the 18th Annual ACM-SIAM Symposium
    on Discrete Algorithms* (2007): 1027–1035.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.html#id1983-marker)) 大卫·亚瑟和谢尔盖·瓦西里维茨基，“k-Means++：谨慎种子的优势”，*第18届ACM-SIAM离散算法年会论文集*（2007年）：1027–1035。
- en: '^([4](ch08.html#id1989-marker)) Charles Elkan, “Using the Triangle Inequality
    to Accelerate k-Means”, *Proceedings of the 20th International Conference on Machine
    Learning* (2003): 147–153.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.html#id1989-marker)) 查尔斯·埃尔坎，“利用三角不等式加速 k-Means”，*第20届国际机器学习会议论文集*（2003年）：147–153。
- en: ^([5](ch08.html#id1990-marker)) The triangle inequality is AC ≤ AB + BC, where
    A, B, and C are three points and AB, AC, and BC are the distances between these
    points.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch08.html#id1990-marker)) 三角不等式是 AC ≤ AB + BC，其中 A、B 和 C 是三个点，AB、AC 和
    BC 是这些点之间的距离。
- en: '^([6](ch08.html#id1993-marker)) David Sculley, “Web-Scale K-Means Clustering”,
    *Proceedings of the 19th International Conference on World Wide Web* (2010): 1177–1178.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch08.html#id1993-marker)) 大卫·斯库利，“Web-Scale K-Means Clustering”，*第19届国际万维网会议论文集*（2010年）：1177–1178。
- en: ^([7](ch08.html#id2046-marker)) In contrast, as we saw earlier, *k*-means implicitly
    assumes that clusters all have a similar size and density, and are all roughly
    round.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch08.html#id2046-marker)) 相反，正如我们之前看到的，*k*-means 隐含地假设所有簇都具有相似的大小和密度，并且都是大致圆形的。
- en: ^([8](ch08.html#id2050-marker)) Phi (*ϕ* or *φ*) is the 21st letter of the Greek
    alphabet.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch08.html#id2050-marker)) Phi (*ϕ* 或 *φ*) 是希腊字母的第21个字母。
