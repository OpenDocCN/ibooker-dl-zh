- en: 13 Music generation with MuseGAN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 使用 MuseGAN 进行音乐生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Music representation using musical instrument digital interface
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用乐器数字接口进行音乐表示
- en: Treating music generation as an object creation problem similar to image generation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音乐生成视为与图像生成类似的对象创建问题
- en: Building and training a generative adversarial network to generate music
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练生成对抗网络以生成音乐
- en: Generating music using the trained MuseGAN model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的 MuseGAN 模型生成音乐
- en: Up to now, we have successfully generated shapes, numbers, images, and text.
    In this chapter and the next, we will explore two different ways of generating
    lifelike music. This chapter will apply the techniques from image GANs, treating
    a piece of music as a multidimensional object akin to an image. The generator
    will produce a complete piece of music and submit it to the critic (serving as
    the discriminator because we use the Wasserstein distance with gradient penalty,
    as discussed in chapter 5) for evaluation. The generator will then modify the
    music based on the critic’s feedback until it closely resembles real music from
    the training dataset. In the next chapter, we will treat music as a sequence of
    musical events, employing natural language processing (NLP) techniques. We will
    use a GPT-style Transformer to predict the most probable musical event in a sequence
    based on previous events. This Transformer will generate a long sequence of musical
    events that can be converted into realistic-sounding music.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成功生成了形状、数字、图像和文本。在本章和下一章中，我们将探讨两种不同的生成逼真音乐的方法。本章将应用图像 GANs 的技术，将音乐视为类似于图像的多维对象。生成器将生成一首完整的音乐作品并将其提交给评论家（作为判别器，因为我们使用
    Wasserstein 距离和梯度惩罚，如第 5 章所述）进行评估。然后，生成器将根据评论家的反馈修改音乐，直到它与训练数据集中的真实音乐非常相似。在下一章中，我们将音乐视为一系列音乐事件，采用自然语言处理（NLP）技术。我们将使用
    GPT 风格的 Transformer 来根据先前的事件预测序列中最可能的音乐事件。这个 Transformer 将生成一系列音乐事件，这些事件可以转换为听起来逼真的音乐。
- en: The field of music generation using AI has gained significant attention; MuseGAN
    is a prominent model, which was introduced by Dong, Hsiao, Yang, and Yang in 2017.^([1](#footnote-000))
    MuseGAN is a deep neural network that utilizes generative adversarial networks
    (GANs) to create multitrack music, with the word Muse signifying the creative
    inspiration behind music. The model is adept at understanding the complex interactions
    between different tracks that represent different musical instruments or different
    voices (which is the case in our training data). As a result, MuseGAN can generate
    compositions that are harmonious and cohesive.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人工智能进行音乐生成的领域已经引起了广泛关注；MuseGAN 是一个突出的模型，由 Dong、Hsiao、Yang 和 Yang 在 2017 年提出。[1](#footnote-000)
    MuseGAN 是一个深度神经网络，利用生成对抗网络（GANs）来创建多轨音乐，其中的“Muse”一词象征着音乐背后的创造性灵感。该模型擅长理解代表不同乐器或不同声音的不同轨道之间的复杂交互（这在我们的训练数据中是这种情况）。因此，MuseGAN
    可以生成和谐且统一的乐曲。
- en: 'MuseGAN, similar to other GAN models, consists of two primary components: the
    generator and the critic (who provides a continuous measure of how real the sample
    is rather than classifying a sample into real or fake). The generator’s task is
    to generate music, whereas the critic assesses the music’s quality and offers
    feedback to the generator. This adversarial interaction enables the generator
    to gradually improve, leading to the creation of more realistic and appealing
    music.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 GAN 模型类似，MuseGAN 由两个主要组件组成：生成器和评论家（评论家提供对样本真实性的连续度量，而不是将样本分类为真实或虚假）。生成器的任务是生成音乐，而评论家评估音乐的质量并向生成器提供反馈。这种对抗性交互使得生成器能够逐步改进，从而创造出更真实、更具吸引力的音乐。
- en: Suppose you’re an avid fan of Johann Sebastian Bach and have listened to all
    his compositions. You might wonder if it’s possible to use MuseGAN to create synthetic
    music that mimics his style. The answer is yes, and you’ll learn how to do that
    in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一位热衷于约翰·塞巴斯蒂安·巴赫的粉丝，并且已经听过他所有的作品。你可能想知道是否可以使用 MuseGAN 创建模仿他风格的合成音乐。答案是肯定的，你将在本章中学习如何做到这一点。
- en: 'Specifically, you’ll first explore how to represent a piece of multitrack music
    as a multidimensional object. A track is essentially an individual line of music
    or sound, which can be a different instrument such as piano, bass, or drums or
    a different voice such as soprano, alto, tenor, or bass. When composing a track
    in electronic music, you typically organize it into bars (segments of time), subdivide
    each bar into steps for finer control over rhythm, and then assign a specific
    note to each step to craft your melodies and rhythms. As a result, each piece
    of music in our training set is structured with a (4, 2, 16, 84) shape: this means
    there are four music tracks, with each track consisting of 2 bars, each bar containing
    16 steps, and each step capable of playing one of the 84 different notes.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你将首先探索如何将一段多轨音乐表示为一个多维对象。一个轨道本质上是一行音乐或声音，可以是不同的乐器，如钢琴、贝斯或鼓，或者不同的声音，如女高音、女低音、男高音或男低音。在创作电子音乐中的轨道时，你通常将其组织成小节（时间段），然后将每个小节细分为步骤以更好地控制节奏，接着为每个步骤分配一个特定的音符来创作旋律和节奏。因此，我们训练集中的每首音乐都是以（4，2，16，84）的形状结构化的：这意味着有四个音乐轨道，每个轨道由2个小节组成，每个小节包含16个步骤，每个步骤可以播放84种不同音符中的任意一种。
- en: The style of the music generated by our MuseGAN will be influenced by the training
    data. Since you are interested in Bach’s work, you’ll be training MuseGAN with
    The JSB Chorales dataset, which is a collection of chorales composed by Bach,
    arranged for four tracks. These chorales have been converted into a piano roll
    representation, a method used for visualizing and encoding music, especially for
    digital processing purposes. You’ll learn how to transform a piece of music represented
    in the shape of (4, 2, 16, 84) into a musical instrument digital interface (MIDI)
    file, which can then be played on your computer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的MuseGAN音乐的风格将受到训练数据的影响。由于你对巴赫的作品感兴趣，你将使用《JSB圣歌集》数据集来训练MuseGAN，这是一个由巴赫创作的圣歌集合，为四个轨道编排。这些圣歌已被转换为钢琴卷表示，这是一种用于可视化和编码音乐的方法，特别是用于数字处理目的。你将学习如何将形状为（4，2，16，84）的音乐作品转换为音乐乐器数字接口（MIDI）文件，然后可以在你的电脑上播放。
- en: While the generator uses just one single noise vector from the latent space
    to generate different formats of content such as shapes, numbers, and images in
    earlier chapters, the generator in MuseGAN will use four noise vectors when producing
    a piece of music. The use of four separate noise vectors (chords, style, melody,
    and groove, which I’ll explain in detail later in this chapter) is a design choice
    that allows for greater control and diversity in the music generation process.
    Each of these noise vectors represents a different aspect of music, and by manipulating
    them individually, the model can generate more complex and nuanced compositions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前面章节中，生成器仅使用来自潜在空间的一个单一噪声向量来生成不同格式的内容，如形状、数字和图像，但MuseGAN中的生成器在生成音乐时会使用四个噪声向量。使用四个独立的噪声向量（和弦、风格、旋律和节奏，我将在本章后面详细解释）是设计选择，它允许在音乐生成过程中有更大的控制和多样性。这些噪声向量中的每一个都代表音乐的不同方面，通过单独操纵它们，模型可以生成更复杂和细腻的作品。
- en: Once the model is trained, we’ll discard the critic network, a common practice
    in GAN models. We’ll then utilize the trained generator to produce music pieces
    by inputting four noise vectors from the latent space. The music generated in
    this way closely mirrors the style of Bach.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，我们将丢弃批评网络，这是GAN模型中的常见做法。然后，我们将利用训练好的生成器，通过输入来自潜在空间的四个噪声向量来生成音乐作品。以这种方式生成的音乐与巴赫的风格非常相似。
- en: 13.1 Digital music representation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 数字音乐表示
- en: Our goal is to master the art of building and training a GAN model from scratch
    for music generation. To achieve this, we need to start with the fundamentals
    of music theory, including understanding musical notes, octaves, and pitch numbers.
    Following that, we’ll dive into the inner workings of digital music, specifically
    focusing on MIDI files.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是掌握从头开始构建和训练用于音乐生成的GAN模型的艺术。为了实现这一目标，我们需要从音乐理论的基础知识开始，包括理解音符、八度和音高数字。随后，我们将深入研究数字音乐的内部工作原理，特别是关注MIDI文件。
- en: 'Depending on the type of machine learning model we use for music generation,
    the representation of a piece of music in digital form will vary. For instance,
    in this chapter, we’ll represent music as a multidimensional object, while in
    the next chapter, we’ll use a different format: a sequence of indexes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们用于音乐生成的机器学习模型的类型，音乐作品的数字表示可能会有所不同。例如，在本章中，我们将音乐表示为一个多维对象，而在下一章中，我们将使用不同的格式：一系列索引。
- en: In this section, we’ll cover basic music theory and then move on to represent
    music digitally using piano rolls. You’ll learn to load and play an example MIDI
    file on your computer. We’ll also introduce the music21 Python library, which
    you’ll install and use to visualize the staff notes associated with the music
    piece. Finally, you’ll learn to represent a piece of music as a multidimensional
    object with the shape of (4, 2, 16, 84).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍基本的音乐理论，然后转向使用钢琴卷来数字化表示音乐。你将学习如何在电脑上加载和播放一个示例MIDI文件。我们还将介绍music21
    Python库，你将安装并使用它来可视化与音乐作品相关的乐谱音符。最后，你将学习将一首音乐作品表示为一个具有形状（4，2，16，84）的多维对象。
- en: 13.1.1 Musical notes, octave, and pitch
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 音乐音符、八度和音高
- en: In this chapter, we’ll be working with a training dataset that represents music
    pieces as 4D objects. To grasp the meaning of the music pieces in the training
    data, it’s essential to first familiarize ourselves with some fundamental concepts
    in music theory, such as musical notes, octaves, and pitch. These concepts are
    interrelated and crucial for understanding the dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个表示音乐作品为4D对象的训练数据集。要理解训练数据中的音乐作品的意义，首先熟悉音乐理论中的某些基本概念是至关重要的，例如音乐音符、八度和音高。这些概念相互关联，对于理解数据集至关重要。
- en: Figure 13.1 illustrates the relationships among these concepts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1说明了这些概念之间的关系。
- en: '![](../../OEBPS/Images/CH13_F01_Liu.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F01_Liu.png)'
- en: 'Figure 13.1 The relationship between musical notes, octaves, and pitches (also
    known as note numbers). The first column displays the 11 octaves (ranging from
    –1 to 9), representing different levels of musical sound. Each octave is subdivided
    into 12 semitones, which are listed in the top row: C, C#, D, D#, ..., B. Within
    each octave, each note is assigned a specific pitch number, ranging from 0 to
    127, as indicated in the figure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 音乐音符、八度和音高的关系（也称为音符编号）。第一列显示了11个八度（范围从-1到9），代表不同的音乐声音级别。每个八度被细分为12个半音，这些半音列在图的最上面一行：C、C#、D、D#、...、B。在每个八度内，每个音符都被分配一个特定的音高编号，范围从0到127，如图所示。
- en: 'A musical note is a symbol representing a specific sound in music. These notes
    are the foundational elements of music, used to craft melodies, chords, and rhythms.
    Each note is assigned a name (such as A, B, C, D, E, F, G) and corresponds to
    a specific frequency, which determines its pitch: whether the note sounds high
    or low. For instance, a middle C (C4) typically has a frequency of about 262 hertz,
    meaning its sound waves vibrate 262 times per second.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐音符是代表音乐中特定声音的符号。这些音符是音乐的基础元素，用于创作旋律、和弦和节奏。每个音符都被分配一个名称（如A、B、C、D、E、F、G）并对应一个特定的频率，这决定了它的音高：音符听起来是高还是低。例如，中C（C4）通常的频率约为262赫兹，这意味着其声波每秒振动262次。
- en: You might be wondering about the meaning of the term “middle C (C4).” The number
    4 in C4 refers to the octave, which is the distance between one level of musical
    pitch and the next. In figure 13.1, the far-left column displays 11 octave levels,
    ranging from –1 to 9\. The frequency of a sound doubles as you move from one octave
    level to the next. For example, note A4 is usually tuned to 440 hertz, while A5,
    one octave above A4, is tuned to 880 hertz..
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道“中C（C4）”这个术语的含义。C4中的数字4指的是八度，它是从一个音乐音高级别到下一个级别的距离。在图13.1中，最左边的列显示了11个八度级别，范围从-1到9。当你从一个八度级别移动到下一个时，声音的频率会翻倍。例如，A4音符通常调校为440赫兹，而比A4高一个八度的A5，则调校为880赫兹。
- en: 'In Western music, an octave is divided into 12 semitones, each corresponding
    to a specific note. The top row of figure 13.1 lists these 12 semitones: C, C#,
    D, D#, ..., B. Moving up or down by 12 semitones takes you to the same note name
    but in a higher or lower octave. As mentioned earlier, A5 is one octave above
    A4.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在西方音乐中，一个八度被分为12个半音，每个半音对应一个特定的音符。图13.1的顶部行列出了这12个半音：C、C#、D、D#、...、B。上下移动12个半音会到达相同的音符名称，但处于更高的或更低的八度。如前所述，A5比A4高一个八度。
- en: Each note within a specific octave is assigned a pitch number, ranging from
    0 to 127, as depicted in figure 13.1\. For example, the note C4 has a pitch number
    of 60, while F3 has a pitch number of 53\. The pitch number is a more efficient
    way to represent musical notes since it specifies both the octave level and the
    semitone. The training data you’ll be using in this chapter is encoded using pitch
    numbers for this very reason.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特定八度内的音符被分配一个音高数字，范围从0到127，如图13.1所示。例如，C4音符的音高数字为60，而F3的音高数字为53。音高数字是表示音乐音符的一种更有效的方式，因为它指定了八度水平和半音。你将在本章中使用的训练数据就是用音高数字编码的，正是出于这个原因。
- en: 13.1.2 An introduction to multitrack music
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 多轨音乐简介
- en: Let’s first talk about how multitrack music works and how it is represented
    digitally. In electronic music production, a “track” typically refers to an individual
    layer or component of the music, such as a drum track, a bass track, or a melody
    track. In classical music, tracks might represent different vocal parts, like
    soprano, alto, tenor, and bass. For instance, the training dataset we’re using
    in this chapter, the JSB Chorales dataset, consists of four tracks corresponding
    to four vocal parts. In music production, each track can be individually edited
    and processed within a digital audio workstation (DAW). These tracks are composed
    of various musical elements, including bars, steps, and notes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈多轨音乐是如何工作的以及它是如何以数字形式表示的。在电子音乐制作中，“轨”通常指的是音乐的一个单独的层次或组成部分，例如鼓轨、贝斯轨或旋律轨。在古典音乐中，轨可能代表不同的声乐部分，如女高音、女低音、男高音和男低音。例如，我们在这章中使用的训练数据集，JSB
    Chorales 数据集，包含四个轨，对应四个声乐部分。在音乐制作中，每个轨都可以在数字音频工作站（DAW）中单独编辑和处理。这些轨由各种音乐元素组成，包括小节、步骤和音符。
- en: A bar (or measure) is a segment of time defined by a specified number of beats,
    with each beat having a certain note duration. In many popular music genres, a
    bar typically contains four beats, although this can vary based on the time signature
    of the piece. The total number of bars in a track is determined by the track’s
    length and structure. For example, in our training dataset, each track comprises
    two bars.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 小节（或度量）是由特定数量的拍子定义的时间段，每个拍子有特定的音符持续时间。在许多流行的音乐流派中，小节通常包含四个拍子，尽管这可以根据作品的拍号而变化。一个轨中的小节数量由轨的长度和结构决定。例如，在我们的训练数据集中，每个轨由两个小节组成。
- en: In the context of step sequencing, a technique commonly used for programming
    rhythms and melodies in electronic music, a “step” represents a subdivision of
    a bar. In a standard 4/4 time signature (four beats in a bar and four steps in
    a beat), you might find 16 steps per bar, with each step corresponding to a sixteenth
    of a bar.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤序列的上下文中，这是一种在电子音乐中编程节奏和旋律的常用技术，一个“步骤”代表一小节的一个细分。在一个标准的4/4拍（一小节有四个拍子，每个拍子有四个步骤）中，你可能会发现每小节有16个步骤，每个步骤对应一小节的十六分之一。
- en: Lastly, each step contains a musical note. In our dataset, we limit the range
    to the 84 most frequently used notes (with pitch numbers from 0 to 83). Therefore,
    the musical note in a step is encoded as a one-hot vector with 84 values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个步骤包含一个音符。在我们的数据集中，我们限制范围为最常用的84个音符（音高数字从0到83）。因此，步骤中的音符被编码为一个包含84个值的one-hot向量。
- en: To illustrate these concepts with a practical example, download the file example.midi
    from the book’s GitHub repository at [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    and save it in the /files/ directory on your computer. A file with the .midi extension
    is a MIDI file. MIDI is a technical standard that outlines a protocol, digital
    interface, and connectors for enabling electronic musical instruments, computers,
    and other related devices to connect and communicate with each other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用实际例子说明这些概念，请从本书的GitHub仓库[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)下载文件example.midi，并将其保存在你电脑的/files/目录下。具有.midi扩展名的文件是MIDI文件。MIDI是一种技术标准，概述了协议、数字接口和连接器，以使电子乐器、计算机和其他相关设备能够相互连接和通信。
- en: 'MIDI files can be played on most music players on your computer. To get a sense
    of the type of music in our training data, open the file example.midi you just
    downloaded with a music player on your computer. It should sound like this music
    file I placed on my website: [https://mng.bz/lrJB](https://mng.bz/lrJB). The file
    example.midi is converted from one of the music pieces in the training dataset
    in this chapter. Later you’ll learn how to convert a piece of music in the training
    dataset with a shape of (4, 2, 16, 84) into a MIDI file that can be played on
    your computer.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MIDI文件可以在您计算机上的大多数音乐播放器上播放。为了了解我们训练数据中的音乐类型，请使用您计算机上的音乐播放器打开您刚刚下载的文件example.midi。它应该听起来像我在我的网站上放置的这个音乐文件：[https://mng.bz/lrJB](https://mng.bz/lrJB)。文件example.midi是从本章训练数据集中的某个音乐作品转换而来的。稍后您将学习如何将形状为(4,
    2, 16, 84)的音乐作品转换为可以在您计算机上播放的MIDI文件。
- en: 'We’ll use the music21 Python library, a powerful and comprehensive toolkit
    designed for music analysis, composition, and manipulation, to visualize how various
    music concepts work. Therefore, run the following line of code in a new cell in
    the Jupyter Notebook app on your computer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用music21 Python库，这是一个专为音乐分析、创作和操作设计的强大且全面的工具包，来可视化各种音乐概念是如何工作的。因此，请在您的计算机上的Jupyter
    Notebook应用程序的新单元中运行以下代码行：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The music21 library enables you to visualize music as staff notation to have
    a better understanding of tracks, bars, steps, and notes. To achieve this, you
    must first install the MuseScore application on your computer. Visit [https://musescore.org/en/download](https://musescore.org/en/download)
    and download the most recent version of the MuseScore app for your operating system.
    As of this writing, the latest version is MuseScore 4, which we’ll use as our
    example. Ensure you know the file path of the MuseScore app on your computer.
    For instance, in Windows, the path is C:\Program Files\MuseScore 4\bin\MuseScore4.exe.
    Run the code cell in the following listing to visualize the staff notation for
    the file example.midi.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: music21库允许您将音乐以乐谱形式可视化，以便更好地理解轨道、小节、音级和音符。为了实现这一点，您必须首先在您的计算机上安装MuseScore应用程序。访问[https://musescore.org/en/download](https://musescore.org/en/download)并下载适用于您操作系统的最新版本的MuseScore应用程序。截至本文撰写时，最新版本是MuseScore
    4，我们将以此为例。确保您知道MuseScore应用程序在您计算机上的文件路径。例如，在Windows上，路径是C:\Program Files\MuseScore
    4\bin\MuseScore4.exe。运行以下列表中的代码单元以可视化文件example.midi的乐谱。
- en: Listing 13.1 Visualizing the staff notation using the music21 library
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1 使用music21库可视化乐谱
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Shows the image in Jupyter notebook instead of in the original app
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在Jupyter笔记本中显示图像，而不是在原始应用程序中
- en: ② Opens the MIDI file
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ② 打开MIDI文件
- en: ③ Defines the path of the MuseScore app
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义MuseScore应用程序的路径
- en: ④ Shows the staff notation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 显示乐谱
- en: For users of the macOS operating system, change the path in the preceding code
    cell to /Applications/MuseScore 4.app/Contents/MacOS/mscore. For Linux users,
    modify the path to /home/[user name]/.local/bin/mscore4portable, substituting
    [user name] with your actual username. For instance, my username is `mark`, so
    the path is /home/mark/.local/bin/mscore4portable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于macOS操作系统的用户，将前面代码单元中的路径更改为/Applications/MuseScore 4.app/Contents/MacOS/mscore。对于Linux用户，将路径更改为/home/[用户名]/.local/bin/mscore4portable，将[用户名]替换为您实际的用户名。例如，我的用户名是`mark`，所以路径是/home/mark/.local/bin/mscore4portable。
- en: Executing the previous code cell will display a staff notation similar to what
    is illustrated in figure 13.2\. Please note that the annotations in the figure
    are added by me, so you will only see the staff notation without any annotations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码单元将显示类似于图13.2所示的乐谱。请注意，图中的注释是我添加的，所以您将只看到乐谱而没有任何注释。
- en: '![](../../OEBPS/Images/CH13_F02_Liu.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH13_F02_Liu.png)'
- en: 'Figure 13.2 Staff notation for a piece of music in JSB Chorales dataset. The
    music has four tracks, representing the four voices in a chorale: soprano, alto,
    tenor, and bass. The notation is structured into two bars for each track, with
    the left and right halves representing the first and second bars, respectively.
    Each bar consists of 16 steps, aligning with the 4/4 time signature where a bar
    contains four beats, each subdivided into four sixteenth notes. A total of 84
    different pitches are possible, and each note is represented as a one-hot vector
    with 84 values.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 JSB Chorales数据集中一首音乐作品的乐谱。音乐有四个轨道，代表合唱中的四个声部：女高音、女低音、男高音和男低音。乐谱结构为每个轨道两个小节，左右两侧分别代表第一和第二小节。每个小节由16个步骤组成，与4/4拍号相匹配，其中一小节包含四个节拍，每个节拍细分为四个十六分音符。总共可能有84个不同的音高，每个音符都表示为一个包含84个值的one-hot向量。
- en: 'The JSB Chorales dataset, which consists of chorale music pieces by Johann
    Sebastian Bach, is often used for training machine learning models in music generation
    tasks. The shape (4, 2, 16, 84) of each music piece in the dataset can be explained
    as follows. Four represents the four voices in a chorale: soprano, alto, tenor,
    and bass. Each voice is treated as a separate track in the dataset. Each piece
    is divided into two bars (also called measures). The dataset is formatted this
    way to standardize the length of the music pieces for training purposes. The number
    16 represents the number of steps (or subdivisions) in each bar. Finally, the
    note is one-hot encoded with 84 values, denoting the number of possible pitches
    (or notes) that can be played in each step.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: JSB Chorales数据集，由约翰·塞巴斯蒂安·巴赫的合唱作品组成，常用于音乐生成任务中训练机器学习模型。数据集中每首音乐作品的形状（4, 2, 16,
    84）可以这样解释。数字4代表合唱中的四个声部：女高音、女低音、男高音和男低音。每个声部在数据集中被视为一个单独的轨道。每首作品分为两个小节（也称为乐句）。数据集以这种方式格式化，以便标准化音乐作品的长度，用于训练目的。数字16代表每个小节中的步骤数（或细分）。最后，音符以84个值进行one-hot编码，表示每个步骤中可以演奏的可能音高（或音符）的数量。
- en: '13.1.3 Digitally represent music: Piano rolls'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.3 数字化音乐表示：钢琴卷轴
- en: A piano roll is a visual representation of music often used in MIDI sequencing
    software and DAWs. It is named after the traditional piano rolls used in player
    pianos, which contained a physical roll of paper with holes punched in it to represent
    musical notes. In a digital context, the piano roll serves a similar function
    but in a virtual format.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 钢琴卷轴是音乐的一种视觉表示，常用于MIDI编曲软件和DAWs中。它以传统钢琴卷轴命名，这种卷轴在自动演奏钢琴中使用，其中包含带有孔的物理卷纸，以表示音符。在数字环境中，钢琴卷轴发挥着类似的功能，但以虚拟格式存在。
- en: The piano roll is displayed as a grid, with time represented horizontally (from
    left to right) and pitch represented vertically (from bottom to top). Each row
    corresponds to a specific musical note, with higher notes at the top and lower
    notes at the bottom, similar to the layout of a piano keyboard.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 钢琴卷轴以网格形式显示，时间水平表示（从左到右），音高垂直表示（从下到上）。每一行对应一个特定的音符，高音在顶部，低音在底部，类似于钢琴键盘的布局。
- en: Notes are represented as bars or blocks on the grid. The position of a note
    block along the vertical axis indicates its pitch, while its position along the
    horizontal axis indicates its timing in the music. The length of the note block
    represents the duration of the note.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 音符以网格上的条形或块表示。音符块沿垂直轴的位置表示其音高，而沿水平轴的位置表示其在音乐中的时间。音符块的长度表示音符的持续时间。
- en: 'Let’s use the music21 library to illustrate what a piano roll looks like. Run
    this line of code in a new cell in your Jupyter Notebook app:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用music21库来展示钢琴卷轴的样子。在您的Jupyter Notebook应用的新单元格中运行以下代码行：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is shown in figure 13.3.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如图13.3所示。
- en: 'The music21 library also allows you to see the quantized notes corresponding
    to the preceding piano roll:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: music21库还允许您查看与前面钢琴卷轴对应的量化音符：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output is
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../../OEBPS/Images/CH13_F03_Liu.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F03_Liu.png)'
- en: Figure 13.3 The piano roll for a piece of music. The piano roll is a graphical
    representation of a musical piece, depicted as a grid with time progressing horizontally
    from left to right and pitch represented vertically from bottom to top. Each row
    on the grid corresponds to a distinct musical note, arranged in a manner akin
    to the keyboard of a piano, with higher notes positioned at the top and lower
    notes at the bottom. This specific piece of music comprises two bars, resulting
    in two distinct sections visible in the graph. The vertical placement of a note
    block signifies its pitch, while its horizontal location indicates when the note
    is played in the piece. Additionally, the length of the note block reflects the
    duration for which the note is sustained.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 一段音乐的钢琴卷。钢琴卷是音乐作品的图形表示，以网格形式呈现，时间从左到右水平进展，音高从下到上垂直表示。网格上的每一行对应一个独特的音乐音符，排列方式类似于钢琴键盘，高音符位于顶部，低音符位于底部。这首特定的音乐由两个小节组成，因此在图中可以看到两个不同的部分。音符块的垂直位置表示其音高，而其水平位置表示音符在作品中的演奏时间。此外，音符块的长度反映了音符持续的时长。
- en: I omitted most of the output. The first value in each line in the previous output
    represents time. It increases by 0.25 seconds after each line in most cases. If
    the time increase in the next line is more than 0.25 seconds, it means a note
    lasts more than 0.25 seconds. As you can see, the starting note is E4\. After
    0.25 seconds, the note changes to A4, and then G4, and so on. This explains the
    first three blocks (far left) in figure 13.3, which have values E, A, and G, respectively.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我省略了大部分输出。前一个输出中每行的第一个值代表时间。在大多数情况下，每行之后时间增加0.25秒。如果下一行的时间增加超过0.25秒，这意味着音符持续的时间超过0.25秒。正如你所见，起始音符是E4。0.25秒后，音符变为A4，然后是G4，以此类推。这解释了图13.3中（最左侧）的前三个块，它们分别具有E、A和G的值。
- en: 'You might be curious about how to convert the sequence of musical notes into
    an object with the shape (4, 2, 16, 84). To understand this, let’s examine the
    pitch number at each time step in the musical notes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能好奇如何将音乐音符序列转换成形状为（4, 2, 16, 84）的对象。为了理解这一点，让我们检查音乐音符中每个时间步的音高数字：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code block has converted the musical note in each time step into
    a pitch number, in the range of 0 to 83 based on the mapping used in figure 13.1\.
    Each of the pitch numbers is then converted to a one-hot variable with 84 values,
    with value –1 everywhere, except 1 in one position. We use –1 and 1 in one-hot
    encoding instead of 0 and 1 because placing values between –1 and 1 centers the
    data around 0, which can make training more stable and faster. Many activation
    functions and weight initialization methods assume input data is centered around
    0\. Figure 13.4 illustrates how a piece of MIDI music is encoded into an object
    in the shape of (4, 2, 16, 84).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块已将每个时间步的音乐音符转换为基于图13.1中使用的映射的音高数字，范围在0到83之间。然后，每个音高数字被转换为具有84个值的one-hot变量，其中所有值均为-1，只有一个位置为1。我们使用-1和1而不是0和1进行one-hot编码，因为将值放置在-1和1之间可以将数据围绕0中心化，这可以使训练更加稳定和快速。许多激活函数和权重初始化方法都假设输入数据围绕0中心化。图13.4说明了如何将一段MIDI音乐编码成形状为（4,
    2, 16, 84）的对象。
- en: '![](../../OEBPS/Images/CH13_F04_Liu.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F04_Liu.png)'
- en: Figure 13.4 How to represent a piece of music using a 4D object. In our training
    data, each piece of music is represented by a 4D object in the shape of (4, 2,
    16, 84). The first dimension represents the four music tracks, which are the four
    voices in the music (soprano, alto, tenor, and bass). Each music track is divided
    into two bars. There are four beats in each bar, and each beat has four notes;
    we therefore have 16 notes in a bar. Finally, each note is represented by a one-hot
    variable with 84 values, with –1 everywhere and 1 in one place.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 如何使用4D对象表示一段音乐。在我们的训练数据中，每段音乐都由形状为（4, 2, 16, 84）的4D对象表示。第一个维度代表四个音乐轨道，即音乐中的四个声部（女高音、女低音、男高音和男低音）。每个音乐轨道分为两个小节。每个小节有四个节拍，每个节拍有四个音符；因此，每个小节有16个音符。最后，每个音符由一个具有84个值的one-hot变量表示，其中所有值均为-1，只有一个位置为1。
- en: Figure 13.4 explains the dimensions of the music object shaped (4, 2, 16, 84).
    In essence, each musical piece comprises four tracks, with each track containing
    two bars. Each bar is subdivided into 16 notes. Given that the pitch numbers range
    from 0 to 83 in our training set, each note is represented by a one-hot vector
    with 84 values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4解释了音乐对象形状的维度（4, 2, 16, 84）。本质上，每首音乐作品包含四个轨道，每个轨道包含两个小节。每个小节被细分为16个音符。鉴于我们的训练集中音高数字从0到83，每个音符由一个包含84个值的one-hot向量表示。
- en: In subsequent discussions on preparing training data, we will explore how to
    transform an object with the shape (4, 2, 16, 84) back into a music piece in MIDI
    format, enabling playback on a computer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续关于准备训练数据的讨论中，我们将探讨如何将形状为（4, 2, 16, 84）的对象转换回MIDI格式的音乐作品，以便在计算机上播放。
- en: 13.2 A blueprint for music generation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 音乐生成的蓝图
- en: When creating music, we need to incorporate more detailed inputs for enhanced
    control and variety. Unlike the approach of utilizing a single noise vector from
    the latent space for generating shapes, numbers, and images, we will employ four
    distinct noise vectors in the music generation process. Since each music piece
    comprises four tracks and two bars, we’ll utilize four vectors to manage this
    structure. We’ll use one vector to govern all tracks and bars collectively, another
    vector to control each bar across all tracks, a third vector to oversee all tracks
    across bars, and a fourth one to manage each individual bar in each track. This
    section will introduce you to the concepts of chords, style, melody, and groove
    and explain how they influence various aspects of the music generation. After
    that, we’ll discuss the steps involved in building and training the MuseGAN model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在创作音乐时，我们需要融入更详细的输入以增强控制和多样性。与仅从潜在空间中利用单个噪声向量来生成形状、数字和图像的方法不同，我们在音乐生成过程中将使用四个不同的噪声向量。由于每首音乐作品包含四个轨道和两个小节，我们将使用四个向量来管理这种结构。我们将使用一个向量来控制所有轨道和小节，另一个向量来控制所有轨道中的每个小节，第三个向量来监督所有轨道跨越小节，第四个向量来管理每个轨道中的每个单独小节。本节将向您介绍和弦、风格、旋律和节奏的概念，并解释它们如何影响音乐生成的各个方面。之后，我们将讨论构建和训练MuseGAN模型所涉及的步骤。
- en: 13.2.1 Constructing music with chords, style, melody, and groove
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用和弦、风格、旋律和节奏构建音乐
- en: Later, in the music generation stage, we obtain four noise vectors (chords,
    style, melody, and groove) from the latent space and feed them to the generator
    to create a piece of music. You may be wondering the meaning of these four pieces
    of information. In music, chords, style, melody, and groove are key elements that
    contribute to a piece’s overall sound and feel. Next I provide a brief explanation
    of each element.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在音乐生成阶段稍后，我们从潜在空间中获取四个噪声向量（和弦、风格、旋律和节奏），并将它们输入到生成器中，以创建一首音乐作品。你可能想知道这四条信息的含义。在音乐中，和弦、风格、旋律和节奏是构成作品整体声音和感觉的关键元素。接下来，我将简要解释每个元素。
- en: Style refers to the characteristic way in which music is composed, performed,
    and experienced. It includes the genre (such as jazz, classical, rock, and so
    on), the era in which the music was created, and the unique approach of the composer
    or performer. Style is influenced by cultural, historical, and personal factors,
    and it helps to define the music’s identity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 风格指的是音乐创作、表演和体验的特征方式。它包括音乐类型（如爵士、古典、摇滚等）、音乐创作的时代以及作曲家或表演者的独特方法。风格受文化、历史和个人因素的影响，有助于定义音乐的个性。
- en: Groove is the rhythmic feel or swing in music, especially in styles like funk,
    jazz, and soul. It’s what makes you want to tap your foot or dance. A groove is
    created by the pattern of accents, the interplay between the rhythm section (drums,
    bass, etc.), and the tempo. It’s the element that gives music its sense of motion
    and flow.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 节奏感是音乐中的节奏感或摇摆，尤其是在放克、爵士和灵魂乐等风格中。这是让你想要脚打拍子或跳舞的原因。节奏感是通过强调模式、节奏部分（如鼓、贝斯等）之间的互动和速度来创造的。它是赋予音乐运动感和流畅感的关键元素。
- en: Chords are combinations of two or more notes played simultaneously. They provide
    the harmonic foundation for music. Chords are built on scales and are used to
    create progressions that give music its structure and emotional depth. Different
    chord types (major, minor, diminished, augmented, etc.) and their arrangements
    can evoke various moods and feelings in the listener.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 和弦是由两个或更多音符同时演奏的组合。它们为音乐提供了和声基础。和弦建立在音阶之上，用于创建音乐的结构和情感深度。不同的和弦类型（大调、小调、减调、增调等）及其编排可以唤起听众的各种情绪和感受。
- en: Finally, melody is the sequence of notes that is most easily recognizable in
    a piece of music. It’s the part that you might hum or sing along to. Melodies
    are often built from scales and are characterized by their pitch, rhythm, and
    contour (the pattern of rises and falls in pitch). A good melody is memorable
    and expressive, conveying the main musical and emotional themes of the piece.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，旋律是音乐作品中最容易识别的音符序列。它是你可能吹口哨或跟着唱的部分。旋律通常由音阶构成，以音高、节奏和轮廓（音高的上升和下降模式）为特征。一首好的旋律易于记忆且富有表现力，传达作品的主要音乐和情感主题。
- en: Together, these elements work in harmony to create the overall sound and experience
    of a musical piece. Each element has its role, but they all interact and influence
    each other to produce the final music piece. Specifically, a music piece consists
    of four tracks, each with two bars, resulting in eight bar/track combinations.
    We’ll use one noise vector for style, applied to all eight bars. We’ll use eight
    different noise vectors for melody, each used in a unique bar. There are four
    noise vectors for groove, each applied to a different track, remaining the same
    across both bars. Two noise vectors will be used for chords, one for each bar.
    Figure 13.5 provides a diagram of how these four elements contribute to the creation
    of a complete piece of music.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素共同协作，和谐地创造出音乐作品的整体声音和体验。每个元素都有其作用，但它们都相互影响，共同产生最终的音乐作品。具体来说，一首音乐作品由四个轨道组成，每个轨道有两个小节，从而产生八个小节/轨道组合。我们将使用一个噪声向量用于风格，应用于所有八个小节。我们将使用八个不同的噪声向量用于旋律，每个向量用于一个独特的小节。有四个噪声向量用于节奏，每个向量应用于不同的轨道，两个小节都保持相同。两个噪声向量将用于和弦，每个小节一个。图13.5展示了这四个元素如何共同贡献于完整音乐作品的创作。
- en: '![](../../OEBPS/Images/CH13_F05_Liu.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图13.5](../../OEBPS/Images/CH13_F05_Liu.png)'
- en: Figure 13.5 Music generation using chords, style, melody, and groove. Each music
    composition consists of four tracks and spans two bars. We will extract four noise
    vectors from the latent space for this purpose. The first vector, representing
    chords, has a dimension of (1, 32). This vector will be processed through a temporal
    network to expand the chords into two (1, 32) vectors, corresponding to the two
    bars, with identical values across all tracks. The second vector, denoting style,
    also has a dimension of (1, 32) and remains constant across all tracks and bars.
    The third vector, melody, is shaped as (4, 32). It will be stretched through a
    temporal network into two (4, 32) vectors, resulting in eight (1, 32) vectors,
    each representing a unique track and bar combination. Lastly, the fourth vector,
    groove, with a dimension of (4, 32), will be applied to the four tracks, maintaining
    the same values for both bars.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 使用和弦、风格、旋律和节奏生成音乐。每一首音乐作品由四个轨道组成，跨越两个小节。为此，我们将从潜在空间中提取四个噪声向量。第一个向量，代表和弦，其维度为（1，32）。这个向量将通过一个时间网络进行处理，将和弦扩展为两个（1，32）向量，对应两个小节，所有轨道上的值都相同。第二个向量，表示风格，其维度也为（1，32），在所有轨道和小节中保持不变。第三个向量，旋律，形状为（4，32）。它将通过一个时间网络拉伸成两个（4，32）向量，从而产生八个（1，32）向量，每个向量代表一个独特的轨道和小节组合。最后，第四个向量，节奏，其维度为（4，32），将应用于四个轨道，两个小节都保持相同的值。
- en: The generator creates a piece of music by generating one bar in one track at
    a time. For this, it requires four noise vectors, each with a shape of (1, 32),
    as input. These vectors represent chords, style, melody, and groove, and each
    controls a distinct aspect of the music, as previously explained. Since the music
    piece consists of four tracks, each with two bars, there are a total of eight
    bar/track combinations. Consequently, we need eight sets of chords, style, melody,
    and groove to generate all parts of the music piece.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器通过一次生成一个轨道的一小节来创建音乐作品。为此，它需要四个形状为（1, 32）的噪声向量作为输入。这些向量代表和弦、风格、旋律和节奏，每个都控制音乐的一个独特方面，如前所述。由于音乐作品由四个轨道组成，每个轨道有两个小节，因此总共有八个轨道/小节组合。因此，我们需要八组和弦、风格、旋律和节奏来生成音乐作品的全部部分。
- en: We’ll obtain four noise vectors from the latent space corresponding to chords,
    style, melody, and groove. We’ll also introduce a temporal network later, whose
    role is to expand the input along the bar dimension. With two bars, this means
    doubling the size of the input. Music is inherently temporal, with patterns and
    structures that unfold over time. The temporal network in MuseGAN is designed
    to capture these temporal dependencies, ensuring that the generated music has
    a coherent and logical progression.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从潜在空间中获取四个与和弦、风格、旋律和节奏对应的噪声向量。我们还将后来引入一个时间网络，其作用是沿着小节维度扩展输入。对于两个小节，这意味着输入大小的加倍。音乐本质上是时间的，具有随时间展开的模式和结构。MuseGAN中的时间网络旨在捕捉这些时间依赖关系，确保生成的音乐具有连贯和逻辑的进展。
- en: The noise vector for chords has a shape of (1, 32). After processing it through
    the temporal network, we obtain two (1, 32) sized vectors. The first vector is
    used across all four tracks in the first bar, while the second vector is used
    across all four tracks in the second bar.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 和弦的噪声向量形状为（1, 32）。在通过时间网络处理后，我们获得两个（1, 32）大小的向量。第一个向量用于第一小节中的所有四个轨道，而第二个向量用于第二小节中的所有四个轨道。
- en: The noise vector for style, also with a shape of (1, 32), is applied uniformly
    across all eight track/bar combinations. Note that we’ll not pass the style vector
    through the temporal network since the style vector is designed to be the same
    across bars.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 风格的噪声向量，同样形状为（1, 32），被均匀地应用于所有八个轨道/小节组合。请注意，我们不会将风格向量通过时间网络传递，因为风格向量被设计为在小节间保持相同。
- en: The noise vector for melody has a shape of (4, 32). When passed through the
    temporal network, it yields two (4, 32) sized vectors, which further break down
    into eight (1, 32) sized vectors. Each of these is used in a unique track/bar
    combination.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 旋律的噪声向量形状为（4, 32）。当通过时间网络传递时，它会产生两个（4, 32）大小的向量，这些向量进一步分解为八个（1, 32）大小的向量。每个向量都用于独特的轨道/小节组合。
- en: Lastly, the noise vector for groove, shaped as (4, 32), is used such that each
    (1, 32) sized vector is applied to a different track, remaining the same across
    both bars. We won’t pass the groove vector through the temporal network since
    the groove vector is designed to be the same across bars.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，节奏的噪声向量形状为（4, 32），其应用方式是每个（1, 32）大小的向量应用于不同的轨道，在小节间保持不变。我们不会将节奏向量通过时间网络传递，因为节奏向量被设计为在小节间保持相同。
- en: After generating a bar of music for each of the eight bar/track combinations,
    we’ll merge them to create a full piece of music, consisting of four distinct
    tracks, each comprising two unique bars.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在为八个轨道/小节组合中的每个组合生成一小节音乐后，我们将它们合并以创建一个完整的音乐作品，由四个不同的轨道组成，每个轨道包含两个独特的小节。
- en: 13.2.2 A blueprint to train a MuseGAN
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 训练MuseGAN的蓝图
- en: Chapter 1 provided an overview of the foundational concepts behind GANs. In
    chapters 3 to 5, you explored the creation and training of GANs for generating
    shapes, numbers, and images. This subsection will summarize the steps for building
    and training MuseGAN, highlighting the differences from the previous chapters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章提供了GANs背后基础概念的概述。在第3章到第5章中，你探索了用于生成形状、数字和图像的GANs的创建和训练。本小节将总结构建和训练MuseGAN的步骤，突出与之前章节的不同之处。
- en: The style of music generated by MuseGAN is influenced by the training data’s
    style. Therefore, you should first collect a dataset of Bach’s compositions in
    a format suitable for training. Next, you’ll create a MuseGAN model, which consists
    of a generator and a critic. The generator network takes four random noise vectors
    as input (chords, style, melody, and groove) and outputs a piece of music. The
    critic network evaluates a piece of music and assigns a rating, with higher scores
    for real music (from the training set) and lower scores for fake music (produced
    by the generator). Both the generator and critic networks utilize deep convolutional
    layers to capture the spatial features of the inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN生成的音乐风格受训练数据风格的影响。因此，你应该首先收集一个适合训练的巴赫作品数据集。接下来，你将创建一个MuseGAN模型，该模型由一个生成器和评论家组成。生成器网络接收四个随机噪声向量作为输入（和弦、风格、旋律和节奏），并输出一首音乐。评论家网络评估一首音乐并分配一个评分，对真实音乐（来自训练集）给予高评分，对由生成器产生的假音乐给予低评分。生成器和评论家网络都利用深度卷积层来捕捉输入的空间特征。
- en: '![](../../OEBPS/Images/CH13_F06_Liu.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F06_Liu.png)'
- en: Figure 13.6 A diagram of the steps involved in training MuseGAN to generate
    music. The generator produces a fake music piece by drawing four random noise
    vectors from the latent space (top left) and presents it to the critic (middle).
    The critic evaluates the piece and assigns a rating. A high rating suggests that
    the piece is likely from the training dataset, while a lower rating indicates
    that the piece is likely fake (generated by the generator). Additionally, an interpolated
    music piece created from a mix of real and fake samples (top left) is presented
    to the critic. The training process incorporates a gradient penalty based on the
    critic’s rating of this interpolated piece, which is added to the total loss.
    The ratings are then compared to the ground truth, allowing both the critic and
    the generator to learn from these evaluations. After numerous training iterations,
    the generator becomes proficient at producing music pieces that are virtually
    indistinguishable from real samples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6展示了训练MuseGAN生成音乐的步骤图。生成器通过从潜在空间中抽取四个随机噪声向量（图的上左角）来生成假音乐作品，并将其展示给评论家（中间）。评论家评估作品并分配一个评分。高评分表明该作品很可能是来自训练数据集的，而低评分则表明该作品很可能是假的（由生成器生成的）。此外，还向评论家展示了一个由真实和假样本混合而成的插值音乐作品（图的上左角）。训练过程结合了基于评论家对这一插值作品评分的梯度惩罚，并将其添加到总损失中。然后将评分与真实值进行比较，使评论家和生成器都能从这些评估中学习。经过多次训练迭代后，生成器变得擅长生成与真实样本几乎无法区分的音乐作品。
- en: Figure 13.6 illustrates the training process of MuseGAN. The generator (the
    bottom left of the figure) receives four random noise vectors (chords, style,
    melody, and groove) as input and produces fake music pieces (step 1 in figure
    13.6). These noise vectors are drawn from the latent space, which represents the
    range of potential outputs the GAN can generate, enabling the creation of diverse
    data samples. These fake music pieces, along with real ones from the training
    set (top right), are then evaluated by the critic (step 3). The critic (bottom
    center) assigns scores to all music pieces, aiming to give high scores to real
    music and low scores to fake music (step 4).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6展示了MuseGAN的训练过程。生成器（图的下左角）接收四个随机噪声向量（和弦、风格、旋律和节奏）作为输入，并生成假音乐作品（图13.6中的步骤1）。这些噪声向量来自潜在空间，它代表了GAN可以生成的潜在输出范围，从而能够创建多样化的数据样本。然后，这些假音乐作品（右上角来自训练集的真实音乐作品）被评论家（步骤3）评估。评论家（图的底部中央）对所有音乐作品进行评分，目的是给予真实音乐高评分，而给予假音乐低评分（步骤4）。
- en: To guide the adjustment of model parameters, appropriate loss functions must
    be chosen for both the generator and the critic. The generator’s loss function
    is designed to encourage the production of data points that closely resemble those
    from the training dataset. Specifically, the loss function for the generator is
    the negative of the critic’s rating. By minimizing this loss function, the generator
    strives to create music pieces that receive high ratings from the critic. On the
    other hand, the critic’s loss function is formulated to encourage accurate assessment
    of real and generated data points. Thus, the loss function for the critic is the
    rating itself if the music piece is from the training set and the negative of
    the rating if it is generated by the generator. In essence, the critic aims to
    assign high ratings to real music pieces and low ratings to fake ones.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导模型参数的调整，必须为生成器和批评家选择适当的损失函数。生成器的损失函数旨在鼓励生成与训练数据集中的数据点非常相似的数据点。具体来说，生成器的损失函数是批评家评分的负值。通过最小化这个损失函数，生成器努力创建能够从批评家那里获得高评分的音乐作品。另一方面，批评家的损失函数被制定为鼓励对真实和生成数据点进行准确评估。因此，如果音乐作品来自训练集，批评家的损失函数是评分本身；如果它是生成器生成的，则是评分的负值。本质上，批评家的目标是给真实音乐作品赋予高评分，给假音乐作品赋予低评分。
- en: Additionally, we incorporate the Wasserstein distance with gradient penalty
    into the loss function, as we did in chapter 5, to enhance the training stability
    and performance of GAN models. To achieve this, an interpolated music piece, blending
    real and fake music (top left in figure 13.6), is evaluated by the critic. The
    gradient penalty, based on the critic’s rating of this interpolated piece, is
    then added to the total loss during the training process.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们像第5章中那样，将Wasserstein距离和梯度惩罚纳入损失函数，以增强GAN模型的训练稳定性和性能。为此，一个混合了真实和假音乐的插值音乐作品（如图13.6左上角所示）由批评家进行评估。然后，基于批评家对这一插值作品的评分，梯度惩罚在训练过程中被添加到总损失中。
- en: Throughout the training loop, we alternate between training the critic and the
    generator. In each training iteration, we sample a batch of real music pieces
    from the training set and a batch of fake music pieces generated by the generator.
    We calculate the total loss by comparing the critic’s ratings (i.e., scores) with
    the ground truth (whether a music piece is real or fake). We then slightly adjust
    the weights in both the generator and critic networks so that, in subsequent iterations,
    the generator produces more realistic music pieces, and the critic assigns higher
    scores to real music and lower scores to fake music.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个训练循环中，我们在训练批评家和生成器之间交替。在每个训练迭代中，我们从训练集中采样一批真实音乐作品和一批由生成器生成的假音乐作品。我们通过比较批评家的评分（即分数）与真实值（音乐作品是真是假）来计算总损失。然后，我们稍微调整生成器和批评家网络中的权重，以便在后续迭代中，生成器产生更逼真的音乐作品，批评家对真实音乐赋予更高的分数，对假音乐赋予更低的分数。
- en: Once MuseGAN is fully trained, music can be created by inputting four random
    noise vectors into the trained generator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦MuseGAN完全训练完成，可以通过输入四个随机噪声向量到训练好的生成器来创建音乐。
- en: 13.3 Preparing the training data for MuseGAN
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 准备MuseGAN的训练数据
- en: We’ll use chorale compositions by Johann Sebastian Bach as our training dataset,
    expecting the generated music to resemble Bach’s style. If you prefer the style
    of a different musician, you can use their work as the training data instead.
    In this section, we’ll start by downloading the training data and organizing it
    into batches for later training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用约翰·塞巴斯蒂安·巴赫的合唱作品作为我们的训练数据集，期望生成的音乐类似于巴赫的风格。如果你更喜欢其他音乐家的风格，你可以使用他们的作品作为训练数据。在本节中，我们将首先下载训练数据，并将其组织成批次以供后续训练。
- en: Additionally, we’ve learned that the music pieces in the training set will be
    represented as 4D objects. In this section, you’ll also learn how to convert these
    multidimensional objects into playable music pieces on a computer. This conversion
    is essential because MuseGAN generates multidimensional objects similar to those
    in the training set. Later in the chapter, we’ll transform the multidimensional
    objects produced by MuseGAN into MIDI files, enabling you to listen to the generated
    music on your computer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还了解到训练集中的音乐作品将被表示为4D对象。在本节中，你还将学习如何将这些多维对象转换为计算机上可播放的音乐作品。这种转换是必不可少的，因为MuseGAN生成的多维对象与训练集中的类似。在本章的后面部分，我们将MuseGAN产生的多维对象转换为MIDI文件，使你能够在计算机上收听生成的音乐。
- en: 13.3.1 Downloading the training data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 下载训练数据
- en: We’ll use the JSB Chorales piano rolls dataset as our training set. Go to Cheng-Zhi
    Anna Huang’s GitHub repository ([https://github.com/czhuang/JSB-Chorales-dataset](https://github.com/czhuang/JSB-Chorales-dataset))
    and download the music file Jsb16thSeparated.npz. Save the file in the /files/
    directory on your computer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 JSB Chorales 钢琴卷数据集作为我们的训练集。访问 Cheng-Zhi Anna Huang 的 GitHub 仓库 ([https://github.com/czhuang/JSB-Chorales-dataset](https://github.com/czhuang/JSB-Chorales-dataset))
    并下载音乐文件 Jsb16thSeparated.npz。将文件保存在您电脑上的 /files/ 目录中。
- en: 'Then, download the two utility modules midi_util.py and MuseGAN_util.py from
    the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and save them in the /utils/ directory on your computer. The code in this chapter
    is adapted from the excellent GitHub repository by Azamat Kanametov ([https://github.com/akanametov/musegan](https://github.com/akanametov/musegan)).
    With these files in place, we can now load the music files and organize them into
    batches for processing:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从本书的 GitHub 仓库 ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    下载两个实用模块 midi_util.py 和 MuseGAN_util.py，并将它们保存在您电脑上的 /utils/ 目录中。本章中的代码改编自 Azamat
    Kanametov 的优秀 GitHub 仓库 ([https://github.com/akanametov/musegan](https://github.com/akanametov/musegan))。有了这些文件，我们现在可以加载音乐文件并将它们组织成批次以进行处理：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We load the dataset you just downloaded into Python, then extract the first
    song and name it `first_song`. Since songs are represented as multidimensional
    objects, we print out the shape of the first song. Finally, we place the training
    data in batches of 64, to be used later in the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将您刚刚下载的数据集加载到 Python 中，然后提取第一首歌曲并将其命名为 `first_song`。由于歌曲表示为多维对象，我们打印出第一首歌曲的形状。最后，我们将训练数据放入
    64 个批次的批次中，以便在章节的后续部分使用。
- en: The output from the preceding code block is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块的输出是
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each song in the dataset has a shape of (4, 2, 16, 84), as shown in the previous
    output. This indicates that each song consists of four tracks, each with two bars.
    Each bar contains 16 time steps, and at each time step, the musical note is represented
    by a one-hot vector with 84 values. In each one-hot vector, all values are set
    to –1, except for one position where the value is set to 1, indicating the presence
    of a note. You can verify the range of values in the dataset as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每首歌曲的形状为 (4, 2, 16, 84)，如前一个输出所示。这表明每首歌曲由四个曲目组成，每个曲目有两小节。每小节包含 16 个时间步，在每个时间步，音符由一个包含
    84 个值的独热向量表示。在每个独热向量中，所有值都设置为 -1，除了一个位置，其值设置为 1，表示音符的存在。您可以如下验证数据集中的值范围：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The previous output shows that the values in each music piece are either –1
    or 1.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个输出显示，每个音乐作品中的值要么是 -1，要么是 1。
- en: 13.3.2 Converting multidimensional objects to music pieces
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 将多维对象转换为音乐作品
- en: Currently, the songs are formatted as PyTorch tensors and are ready to be inputted
    into the MuseGAN model. However, before we proceed, it’s important to gain a better
    understanding of how to convert these multidimensional objects into playable music
    pieces on your computer. This will help us later to convert generated music pieces
    into playable files.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，歌曲格式为 PyTorch 张量，并准备好输入到 MuseGAN 模型中。然而，在我们继续之前，了解如何将这些多维对象转换为电脑上可播放的音乐作品非常重要。这将帮助我们稍后把生成的音乐作品转换为可播放的文件。
- en: 'To begin, we’ll convert all the 84-value one-hot variables into pitch numbers
    ranging from 0 to 83:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将所有 84 个值的独热变量转换为介于 0 到 83 之间的音高数：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Converts 84-value one-hot vectors to numbers between 0 and 83
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将 84 个值的独热向量转换为介于 0 到 83 之间的数字
- en: ② Reshapes the result to (32, 4)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将结果重塑为 (32, 4)
- en: The output is
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the output displayed here, each column represents a music track, with numbers
    ranging from 0 to 83\. These numbers correspond to pitch numbers, as you have
    seen earlier in figure 13.1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里显示的输出中，每一列代表一个音乐曲目，数值范围从 0 到 83。这些数字对应于音高数，正如您在图 13.1 中之前看到的。
- en: Now, we’ll proceed to convert the tensor `midi_note_score` in the previous code
    block into an actual MIDI file, allowing you to play it on your computer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把前一个代码块中的张量 `midi_note_score` 转换为实际的 MIDI 文件，让您可以在电脑上播放它。
- en: Listing 13.2 Converting pitch numbers to a MIDI file
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.2 将音高数转换为 MIDI 文件
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Iterates through four music tracks
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历四个音乐曲目
- en: ② Iterates through all notes in each track
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历每个曲目中的所有音符
- en: ③ Adds 0.25 seconds to each time step
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将 0.25 秒添加到每个时间步
- en: ④ Adds the note to the music stream
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将音符添加到音乐流中
- en: After running the preceding code cell, you’ll see a MIDI file, `first_song.midi`,
    on your computer. Play it with a music player on your computer to get a sense
    of what type of music we are using to train the MuseGAN.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码单元后，您将在电脑上看到一个MIDI文件，`first_song.midi`。使用您电脑上的音乐播放器播放它，以了解我们用来训练MuseGAN的音乐类型。
- en: Exercise 13.1
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 练习13.1
- en: Convert the second song in the training dataset into a MIDI file. Save it as
    `second_song.midi` and play it using a music player on your computer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据集中的第二首歌曲转换为MIDI文件。将其保存为`second_song.midi`，并使用您电脑上的音乐播放器播放。
- en: 13.4 Building a MuseGAN
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 构建MuseGAN
- en: In essence, we will treat a music piece as an object with multiple dimensions.
    Using techniques from chapters 4 to 6, we will tackle this task using deep convolutional
    neural networks for their ability to effectively extract spatial features from
    multidimensional objects. In MuseGAN, we’ll construct a generator and a critic,
    similar to how a generator in image creation refines an image based on a critic’s
    feedback. The generator will produce a music piece as a 4D object.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们将音乐作品视为一个具有多个维度的对象。利用第4章至第6章中的技术，我们将使用深度卷积神经网络来处理这项任务，因为它们能够有效地从多维对象中提取空间特征。在MuseGAN中，我们将构建一个生成器和评判器，类似于图像创建中的生成器根据评判器的反馈来细化图像。生成器将生成一个作为4D对象的音乐作品。
- en: Both real music from our training set and fake music from the generator will
    be presented to the critic. The critic will score each piece from negative infinity
    to positive infinity, with higher scores indicating a higher likelihood of the
    music being real. The critic aims to give high scores to real music and low scores
    to fake music. Conversely, the generator aims to produce music that is indistinguishable
    from real music, thereby receiving high scores from the critic.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将真实音乐（来自我们的训练集）和生成器生成的假音乐都展示给评判器。评判器将对每首作品从负无穷大到正无穷大进行评分，分数越高表示音乐更可能是真实的。评判器的目标是给真实音乐高分数，给假音乐低分数。相反，生成器的目标是生成与真实音乐难以区分的音乐，从而从评判器那里获得高分数。
- en: In this section, we will build a MuseGAN model, comprising a generator network
    and a critic network. The critic network employs deep convolutional layers to
    extract distinct features from multidimensional objects, enhancing its ability
    to evaluate music pieces. On the other hand, the generator network utilizes deep
    transposed convolutional layers to produce feature maps aimed at generating realistic
    music pieces. Later, we will train the MuseGAN model using music pieces from the
    training set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个MuseGAN模型，该模型包括一个生成网络和一个评判网络。评判网络使用深度卷积层从多维对象中提取独特特征，从而增强其评估音乐作品的能力。另一方面，生成网络利用深度转置卷积层生成旨在生成逼真音乐作品的特征图。稍后，我们将使用训练集中的音乐作品来训练MuseGAN模型。
- en: 13.4.1 A critic in MuseGAN
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.1 MuseGAN中的评判器
- en: As explained in chapter 5, incorporating the Wasserstein distance into the loss
    function can help stabilize training. Therefore, in MuseGAN, we adopt a similar
    approach and use a critic instead of a discriminator. The critic is not a binary
    classifier; rather, it evaluates the output of the generator (in this case, a
    music piece) and assigns a score ranging from –∞ to ∞. A higher score indicates
    a greater likelihood that the music is real (i.e., from the training set).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如第5章所述，将Wasserstein距离纳入损失函数可以帮助稳定训练。因此，在MuseGAN中，我们采用类似的方法，并使用评判器而不是判别器。评判器不是一个二元分类器；相反，它评估生成器的输出（在这种情况下，是一首音乐作品），并分配一个从-∞到∞的分数。更高的分数表示音乐更可能是真实的（即来自训练集）。
- en: We construct a music critic neural network as shown in the following listing,
    and its definition can be found in the file MuseGAN_util.py that you downloaded
    earlier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个如以下列表所示的音乐评判神经网络，其定义可以在您之前下载的文件MuseGAN_util.py中找到。
- en: Listing 13.3 The critic network in MuseGAN
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.3 MuseGAN中的评判器网络
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Passes the input through several Conv3d layers
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将输入通过几个Conv3d层
- en: ② Flattens the output
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将输出扁平化
- en: ③ Passes the output through two linear layers
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输出通过两个线性层
- en: The input to the critic network is a music piece with dimensions (4, 2, 16,
    84). The network primarily consists of several Conv3d layers. These layers treat
    each track of the music piece as a 3D object and apply filters to extract spatial
    features. The operation of the Conv3d layers is similar to the Conv2d layers used
    in image generation, as discussed in earlier chapters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 评论网络的输入是一个维度为（4，2，16，84）的音乐作品。该网络主要由几个Conv3d层组成。这些层将音乐作品中的每个轨道视为一个3D对象，并应用过滤器来提取空间特征。Conv3d层的操作与前面章节中讨论的用于图像生成的Conv2d层类似。
- en: It’s important to note that the final layer of the critic model is linear, and
    we do not apply any activation function to its output. As a result, the output
    from the critic model is a value ranging from –∞ to ∞, which can be interpreted
    as the critic’s rating of a music piece.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，评论模型的最后一层是线性的，我们没有对其输出应用任何激活函数。因此，评论模型的输出是一个从-∞到∞的值，这可以解释为评论家对音乐作品的评价。
- en: 13.4.2 A generator in MuseGAN
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.2 MuseGAN中的生成器
- en: As discussed earlier in this chapter, the generator will produce one bar of
    music at a time, and we will then combine these eight bars to form a complete
    piece of music.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，生成器将一次生成一段音乐，然后我们将这八段音乐组合成一首完整的乐曲。
- en: Instead of using just a single noise vector, the generator in MuseGAN takes
    four independent noise vectors as input to control various aspects of the music
    being generated. Two of these vectors will be processed through a temporal network
    to extend them along the bar dimension. While the style and groove vectors are
    designed to remain constant across both bars, the chords and melody vectors are
    designed to vary between bars. Therefore, we will first establish a temporal network
    to stretch the chords and melody vectors across the two bars, ensuring that the
    generated music has a coherent and logical progression over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN中的生成器不是只使用一个噪声向量，而是使用四个独立的噪声向量作为输入来控制生成的音乐的各个方面。其中两个向量将通过时间网络进行处理，以扩展它们在小节维度上的长度。而风格和节奏向量被设计为在小节间保持不变，和弦和旋律向量被设计为在小节间变化。因此，我们首先建立一个时间网络来扩展和弦和旋律向量跨越两个小节，确保生成的音乐在时间上有连贯和逻辑的进展。
- en: 'In the local module `MuseGAN_util` you downloaded earlier, we define the `TemporalNetwork()`
    class as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在您之前下载的本地模块`MuseGAN_util`中，我们定义了`TemporalNetwork()`类如下：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① The input dimension to the TemporalNetwork() class is (1, 32).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ① TemporalNetwork()类的输入维度是（1，32）。
- en: ② The output dimension is (2, 32).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输出维度是（2，32）。
- en: The `TemporalNetwork()` class described here employs two ConvTranspose2d layers
    to expand a single noise vector into two distinct noise vectors, each corresponding
    to one of the two bars. As we covered in chapter 4, transposed convolutional layers
    serve the purpose of upsampling and generating feature maps. In this context,
    they are utilized to extend noise vectors across different bars.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的`TemporalNetwork()`类使用两个ConvTranspose2d层将单个噪声向量扩展为两个不同的噪声向量，每个向量对应两个小节中的一个。正如我们在第4章中提到的，转置卷积层用于上采样和生成特征图。在这种情况下，它们被用来在不同的小节间扩展噪声向量。
- en: 'Instead of generating all bars in all tracks at once, we’ll generate the music
    one bar at a time. Doing so allows MuseGAN to balance computational efficiency,
    flexibility, and musical coherence, resulting in more structured and appealing
    musical compositions. Therefore, we proceed to construct a bar generator that
    is responsible for generating a segment of the music piece: one bar within a track.
    We introduce the `BarGenerator()` class within the local `MuseGAN_util` module:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一次性生成所有轨道的所有小节，而是逐个小节地生成音乐。这样做可以让MuseGAN在计算效率、灵活性和音乐连贯性之间取得平衡，从而产生更有结构和吸引力的音乐作品。因此，我们继续构建一个负责生成乐曲片段的生成器：轨道中的一小节。我们在本地的`MuseGAN_util`模块中引入了`BarGenerator()`类：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① We concatenate chords, style, melody, and groove into one vector, with a size
    of 4 * 32.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们将和弦、风格、旋律和节奏合并成一个向量，大小为4 * 32。
- en: ② The input is then reshaped into 2D, and we use several ConvTranspose2d layers
    for upsampling and music feature generation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ② 然后将输入重塑为二维，我们使用几个ConvTranspose2d层进行上采样和音乐特征生成。
- en: '③ The output has a shape of (1, 1, 16, 84): 1 track, 1 bar, and 16 notes, and
    each note is represented by a 84-value vector.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 输出的形状为（1，1，16，84）：1个轨道，1个小节，16个音符，每个音符由一个84值的向量表示。
- en: The `BarGenerator()` class accepts four noise vectors as input, each representing
    chords, style, melody, and groove for a specific bar in a different track, all
    with a shape of (1, 32). These vectors are concatenated into a single 128-value
    vector before being fed into the `BarGenerator()` class. The output from the `BarGenerator()`
    class is a bar of music, with dimensions (1, 1, 16, 84), indicating 1 track, 1
    bar, and 16 notes, with each note represented by an 84-value vector.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`BarGenerator()`类接受四个噪声向量作为输入，每个向量代表不同轨道中特定节拍的和弦、风格、旋律和节奏，所有这些向量的形状都是（1，32）。这些向量在输入到`BarGenerator()`类之前被连接成一个单一的128值向量。`BarGenerator()`类的输出是一个音乐节拍，维度为（1，1，16，84），表示1个轨道，1个节拍，16个音符，每个音符由一个84值的向量表示。'
- en: Finally, we will employ the `MuseGenerator()` class to generate a complete piece
    of music, consisting of four tracks with two bars per track. Each bar is constructed
    using the `BarGenerator()` class defined earlier. To achieve this, we define the
    `MuseGenerator()` class in the local MuseGAN_util module.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`MuseGenerator()`类生成一个完整的音乐作品，包含四个轨道，每个轨道有两个节拍。每个节拍都是使用之前定义的`BarGenerator()`类构建的。为了实现这一点，我们在本地`MuseGAN_util`模块中定义了`MuseGenerator()`类。
- en: Listing 13.4 The music generator in MuseGAN
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.4 MuseGAN中的音乐生成器
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Iterates through two bars
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历两个节拍
- en: ② Iterates through four tracks
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历四个轨道
- en: ③ Concatenates chords, style, melody, and groove into one input
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将和弦、风格、旋律和节奏合并为一个输入
- en: ④ Generates one bar using the bar generator
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用轨道生成器生成一个节拍
- en: ⑤ Concatenates eight bars into one complete piece of music
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将八个节拍合并成一个完整的音乐作品
- en: The generator takes four noise vectors as inputs. It iterates through four tracks
    and two bars. In each iteration, it utilizes the bar generator to create a single
    bar of music. Upon completing all iterations, the MuseGenerator() class merges
    the eight bars into one cohesive music piece, which has dimensions of (4, 2, 16,
    84).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受四个噪声向量作为输入。它遍历四个轨道和两个节拍。在每次迭代中，它使用轨道生成器创建一个音乐节拍。完成所有迭代后，`MuseGenerator()`类将八个节拍合并成一个连贯的音乐作品，其维度为（4，2，16，84）。
- en: 13.4.3 Optimizers and the loss function
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.3 优化器和损失函数
- en: 'We create a generator and a critic based on the `MuseGenerator()` and `MuseCritic()`
    classes in the local module:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于本地模块中的`MuseGenerator()`和`MuseCritic()`类创建一个生成器和批评家：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we discussed in chapter 5, the critic generates a rating instead of a classification,
    so the loss function is defined as the negative average of the product between
    the prediction and the target. As a result, we define the following `loss_fn()`
    function in the local module `MuseGAN_util`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第5章中讨论的，批评家生成一个评分而不是分类，因此损失函数定义为预测和目标之间的乘积的负平均值。因此，我们在本地模块`MuseGAN_util`中定义以下`loss_fn()`函数：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: During training, for the generator, we’ll assign a value of 1 to the target
    argument in the `loss_fn()` function. This setting aims to guide the generator
    in producing music that can achieve the highest possible rating (i.e., the variable
    pred in the `loss_fn()` function). For the critic, we’ll set the target to 1 for
    real music and –1 for fake music in the loss function. This setting guides the
    critic to assign a high rating to real music and a low rating to fake music.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，对于生成器，我们将`loss_fn()`函数中的目标参数赋值为1。这种设置旨在引导生成器产生能够获得最高评分（即`loss_fn()`函数中的变量pred）的音乐。对于批评家，我们在损失函数中将目标设置为1用于真实音乐，-1用于假音乐。这种设置引导批评家对真实音乐给予高评分，对假音乐给予低评分。
- en: 'Similar to the approach in chapter 5, we incorporate the Wasserstein distance
    with a gradient penalty into the critic’s loss function to ensure training stability.
    The gradient penalty is defined in the `MuseGAN_util.py` file as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与第5章中的方法类似，我们将Wasserstein距离和梯度惩罚结合到批评家的损失函数中，以确保训练稳定性。梯度惩罚在`MuseGAN_util.py`文件中定义如下：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `GradientPenalty()` class requires two inputs: interpolated music, which
    is a blend of real and fake music, and the ratings assigned by the critic network
    to this interpolated music. The class computes the gradient of the critic’s ratings
    concerning the interpolated music. The gradient penalty is then calculated as
    the squared difference between the norms of these gradients and the target value
    of 1, following a similar approach to what we did in chapter 5.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradientPenalty()` 类需要两个输入：插值音乐，这是真实和假音乐的结合，以及批评家网络分配给这种插值音乐的评分。该类计算批评家评分关于插值音乐的梯度。然后，梯度惩罚被计算为这些梯度的范数的平方差与目标值
    1 之间的平方差，这与我们在第 5 章中采取的方法类似。'
- en: 'As usual, we’ll use the Adam optimizer for both the critic and the generator:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们将使用 Adam 优化器来训练批评家和生成器：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With that, we have successfully constructed a MuseGAN, which is now ready to
    be trained using the data we prepared earlier in the chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经成功构建了一个 MuseGAN，现在可以使用本章前面准备的数据进行训练。
- en: 13.5 Training the MuseGAN to generate music
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5 训练 MuseGAN 以生成音乐
- en: Now that we have both the MuseGAN model and the training data, we’ll proceed
    to train the model in this section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 MuseGAN 模型和训练数据，我们将继续在本节中训练模型。
- en: Similar to our approach in chapters 3 and 4, when training GANs, we’ll alternate
    between training the critic and the generator. In each training iteration, we’ll
    sample a batch of real music from the training dataset and a batch of generated
    music from the generator and present them to the critic for evaluation. During
    critic training, we compare the critic’s ratings with the ground truth and adjust
    the critic network’s weights slightly so that, in the next iteration, the ratings
    will be as high as possible for real music and as low as possible for generated
    music. During generator training, we feed generated music to the critic model
    to obtain a rating and then slightly adjust the generator network’s weights so
    that, in the next iteration, the rating will be higher (as the generator aims
    to create music pieces that fool the critic into thinking they are real). We repeat
    this process for many iterations, gradually enabling the generator network to
    create more realistic music pieces.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与第 3 章和第 4 章中的方法类似，在训练 GAN 时，我们将交替训练批评家和生成器。在每个训练迭代中，我们将从训练数据集中采样一批真实音乐和从生成器中生成的一批音乐，并将它们呈现给批评家进行评估。在批评家训练期间，我们将批评家的评分与真实值进行比较，并稍微调整批评家网络的权重，以便在下一个迭代中，评分对真实音乐尽可能高，对生成音乐尽可能低。在生成器训练期间，我们将生成音乐输入到批评模型中，以获得评分，然后稍微调整生成器网络的权重，以便在下一个迭代中，评分更高（因为生成器旨在创建能够欺骗批评家的音乐作品）。我们重复这个过程多次迭代，逐渐使生成器网络能够创建更逼真的音乐作品。
- en: Once the model is trained, we’ll discard the critic network and use the trained
    generator to create music pieces by feeding it four noise vectors (chords, style,
    melody, and groove).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们将丢弃批评家网络，并使用训练好的生成器通过输入四个噪声向量（和弦、风格、旋律和节奏）来创建音乐作品。
- en: 13.5.1 Training the MuseGAN
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.1 训练 MuseGAN
- en: Before we embark on the training loops for the MuseGAN model, we first define
    a few hyperparameters and helper functions. The hyperparameter `repeat` controls
    how many times we train the critic in each iteration, `display_step` specifies
    how often we display output, and `epochs` is the number of epochs we train the
    model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练 MuseGAN 模型的训练循环之前，我们首先定义了一些超参数和辅助函数。超参数 `repeat` 控制我们在每次迭代中训练批评家的次数，`display_step`
    指定我们显示输出的频率，而 `epochs` 是我们训练模型的时代数。
- en: Listing 13.5 Hyperparameters and helper functions
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.5 超参数和辅助函数
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Defines a few hyperparameters
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义了一些超参数
- en: ② Defines alpha to create interpolated music
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义了 alpha 以创建插值音乐
- en: ③ Defines a gp() function to calculate gradient penalty
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义了一个 gp() 函数来计算梯度惩罚
- en: ④ Defines a noise() function to retrieve four random noise vectors
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义了一个 noise() 函数来检索四个随机噪声向量
- en: The batch size is set at 64, and this helps us determine how many sets of random
    noise vectors to retrieve to create a batch of fake music. We’ll train the critic
    for five iterations and the generator just once in each training loop because
    an effective critic is essential for training the generator. We’ll display training
    losses after every 10 epochs. We’ll train the model for 500 epochs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小设置为64，这有助于我们确定需要检索多少组随机噪声向量来创建一个假音乐批次。我们将在每个训练循环中训练批评家五次，生成器只训练一次，因为一个有效的批评家对于训练生成器至关重要。我们将在每个10个epoch后显示训练损失。我们将对模型进行500个epoch的训练。
- en: We instantiate the `GradientPenalty()` class in the local module to create a
    `gp()` function to calculate the gradient penalty. We also define a `noise()`
    function to generate four random noise vectors to feed to the generator.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本地模块中实例化`GradientPenalty()`类，以创建一个`gp()`函数来计算梯度惩罚。我们还定义了一个`noise()`函数来生成四个随机噪声向量，以输入到生成器中。
- en: Next, we define the following function, `train_epoch()`, to train the model
    for one epoch.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义以下函数，`train_epoch()`，用于训练模型一个epoch。
- en: Listing 13.6 Training the MuseGAN model for one epoch
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.6 在一个epoch中训练MuseGAN模型
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Iterates through all batches
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历所有批次
- en: ② Trains the critic five times in each iteration
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在每个迭代中训练批评家五次
- en: '③ The total loss for the critic has three components: loss from evaluating
    real music, loss from evaluating fake music, and the gradient penalty loss.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 批评家的总损失有三个组成部分：评估真实音乐的损失、评估假音乐的损失以及梯度惩罚损失。
- en: ④ Trains the generator
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 训练生成器
- en: The training process is very much like that we used in chapter 5 when we train
    the conditional GAN with gradient penalty.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程与我们第5章训练带有梯度惩罚的条件GAN时使用的非常相似。
- en: 'We now train the model for 500 epochs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对模型进行500个epoch的训练：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you use GPU training, it takes about an hour. Otherwise, it may take several
    hours. Once done, you can save the trained generator to the local folder as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用GPU进行训练，大约需要一个小时。否则，可能需要几个小时。一旦完成，你可以按照以下方式将训练好的生成器保存到本地文件夹：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Alternatively, you can download the trained generator from my website: [https://mng.bz/Bglr](https://mng.bz/Bglr).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以从我的网站上下载训练好的生成器：[https://mng.bz/Bglr](https://mng.bz/Bglr)。
- en: Next, we’ll discard the critic network and use the trained generator to create
    music that mimics the style of Bach.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将丢弃批评家网络，并使用训练好的生成器来创建模仿巴赫风格的音乐。
- en: 13.5.2 Generating music with the trained MuseGAN
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5.2 使用训练好的MuseGAN生成音乐
- en: To generate music with the trained generator, we’ll feed four noise vectors
    from the latent space to the generator. Note that we can generate multiple music
    objects at the same time and decode them together to form one continuous piece
    of music. You’ll learn how to do that in this subsection.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的生成器生成音乐，我们将从潜在空间中提取四个噪声向量输入到生成器中。请注意，我们可以在同一时间生成多个音乐对象并将它们一起解码，形成一个连续的音乐作品。你将在本小节中学习如何做到这一点。
- en: 'We first load the trained weights in the generator:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载生成器中的训练权重：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Rather than producing a single 4D music object, we can simultaneously generate
    multiple 4D music objects and convert them into one continuous piece of music
    later. For instance, if we aim to create five music objects, we begin by sampling
    five sets of noise vectors from the latent spaces. Each set consists of four vectors:
    chords, style, melody, and groove, like so:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅能够生成一个4D音乐对象，还可以同时生成多个4D音乐对象，并在之后将它们转换成一个连续的音乐作品。例如，如果我们旨在创建五个音乐对象，我们首先从潜在空间中采样五组噪声向量。每组包含四个向量：和弦、风格、旋律和节奏，如下所示：
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each generated music object can be transformed into a music piece that lasts
    approximately 8 seconds. In this case, we choose to generate five music objects
    and decode them into a single music piece later, resulting in a duration of about
    40 seconds. You can adjust the value of the variable `num_pieces` according to
    your preference, depending on the desired length of the music piece.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生成的音乐对象可以转换成一个大约持续8秒的音乐作品。在这种情况下，我们选择生成五个音乐对象，并在之后将它们解码成一个单一的音乐作品，结果持续大约40秒。你可以根据你的喜好调整变量`num_pieces`的值，以适应你希望的音乐作品长度。
- en: 'Next, we supply the generator with the five sets of latent variables to produce
    a set of music objects:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们向生成器提供五组潜在变量，以生成一组音乐对象：
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output, `preds`, consists of five music objects. Next, we decode these
    objects into a single piece of music, represented as a MIDI file:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，`preds`，包含五个音乐对象。接下来，我们将这些对象解码成一个单一的音乐作品，表示为一个MIDI文件：
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We import the `convert_to_midi()` function from the local module `midi_util`.
    Open the file `midi_util.py` that you downloaded earlier and review the definition
    of the `convert_to_midi()` function. This process is similar to what we have done
    earlier in this chapter when we converted the first music object in the training
    set into the file `first_song.midi`. Since MIDI files represent sequences of notes
    over time, we simply concatenate the five music pieces corresponding to the five
    music objects into one extended sequence of notes. This combined sequence is then
    saved as `MuseGAN_song.midi` on your computer.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本地模块 `midi_util` 中导入 `convert_to_midi()` 函数。打开你之前下载的文件 `midi_util.py` 并回顾
    `convert_to_midi()` 函数的定义。这个过程与我们本章前面将训练集中的第一个音乐对象转换为文件 `first_song.midi` 时所做的是类似的。由于
    MIDI 文件表示随时间变化的音符序列，我们只需将对应于五个音乐对象的五个音乐片段连接成一个扩展的音符序列。然后将这个组合序列保存为 `MuseGAN_song.midi`
    到你的电脑上。
- en: Find the generated music piece, `MuseGAN_song.midi`, on your computer. Open
    it with a music player of your choice and listen to see if it resembles the music
    pieces from the training set. For comparison, you can listen to a piece of music
    generated by the trained model on my website at [https://mng.bz/dZJv](https://mng.bz/dZJv).
    Note that since the input to the generator, the noise vectors, are randomly drawn
    from the latent space, the music pieces you generate will sound different.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的电脑上找到生成的音乐片段 `MuseGAN_song.midi`。用你选择的音乐播放器打开它并聆听，看它是否与训练集中的音乐片段相似。为了比较，你可以在我网站上听一听由训练模型生成的音乐片段，网址是
    [https://mng.bz/dZJv](https://mng.bz/dZJv)。请注意，由于生成器的输入，即噪声向量，是从潜在空间中随机抽取的，因此你生成的音乐片段听起来会有所不同。
- en: Exercise 13.2
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 13.2
- en: Obtain three sets of random noise vectors (each set should contain chords, style,
    melody, and groove) from the latent space. Feed them to the trained generator
    to obtain three music objects. Decode them into one single piece of music in the
    form of a MIDI file. Save it as `generated_song.midi` on your computer, and play
    it using a music player.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从潜在空间中获得三组随机的噪声向量（每组应包含和弦、风格、旋律和节奏）。将它们输入到训练好的生成器中，以获得三个音乐对象。将它们解码成一首单独的音乐，以
    MIDI 文件的形式。将其保存为 `generated_song.midi` 到你的电脑上，并使用音乐播放器播放。
- en: 'In this chapter, you’ve learned how to build and train a MuseGAN to generate
    music in the style of Bach. Specifically, you’ve approached a piece of music as
    a 4D object and applied the techniques from chapter 4 on deep convolutional layers
    to develop a GAN model. In the next chapter, you’ll explore a different way of
    generating music: treating a piece of music as a sequence of indexes and utilizing
    techniques from NLP to generate music pieces by predicting one index at a time.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何构建和训练 MuseGAN 以生成巴赫风格的音乐。具体来说，你将一首音乐视为一个 4D 对象，并应用了第 4 章中关于深度卷积层的技巧来开发
    GAN 模型。在下一章中，你将探索生成音乐的不同方式：将一首音乐视为一系列索引，并利用 NLP 技术通过逐个预测一个索引来生成音乐片段。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: MuseGAN treats a piece of music as a multidimensional object akin to an image.
    The generator produces a piece of music and submits it, along with real music
    pieces from the training set, to the critic for evaluation. The generator then
    modifies the music based on the critic’s feedback until it closely resembles real
    music from the training dataset.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuseGAN 将一首音乐视为一个类似于图像的多维对象。生成器生成一首音乐并将其提交，连同训练集中的真实音乐片段一起，供评论家评估。然后根据评论家的反馈修改音乐，直到它与训练数据集中的真实音乐非常相似。
- en: 'Musical notes, octaves, and pitch are fundamental concepts in music theory.
    Octaves represent different levels of musical sound. Each octave is subdivided
    into 12 semitones: C, C#, D, D#, E, F, F#, G, G#, A, A#, B. Within an octave,
    a note is assigned a specific pitch number.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐音符、八度和音高是音乐理论中的基本概念。八度代表不同的音乐声音水平。每个八度分为 12 个半音：C、C#、D、D#、E、F、F#、G、G#、A、A#、B。在八度内，一个音符被分配一个特定的音高数字。
- en: In electronic music production, a track typically refers to an individual layer
    or component of the music. Each track contains multiple bars (or measures). A
    bar is further divided into multiple steps.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在电子音乐制作中，一个轨道通常指的是音乐的一个单独的层或组件。每个轨道包含多个小节（或度量）。小节进一步分为多个步骤。
- en: 'To represent a piece of music as a multidimensional object, we structure it
    with a (4, 2, 16, 84) shape: 4 music tracks, with each track consisting of 2 bars,
    each bar containing 16 steps, and each step capable of playing 1 of the 84 different
    notes.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将一段音乐表示为一个多维对象，我们用(4, 2, 16, 84)的形状来结构化它：4个音乐轨道，每个轨道包含2个小节，每个小节包含16个步骤，每个步骤可以演奏84种不同音符中的任意一种。
- en: In music creation, incorporating more detailed inputs is essential for achieving
    greater control and variety. Instead of using a single noise vector from the latent
    space for generating shapes, numbers, and images as in previous chapters, we employ
    four distinct noise vectors in the music generation process. Given that each music
    piece consists of four tracks and two bars, we use these four vectors to effectively
    manage this structure. One vector controls all tracks and bars collectively, another
    controls each bar across all tracks, a third oversees all tracks across bars,
    and the fourth manages each individual bar in each track.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在音乐创作中，为了实现更大的控制和多样性，引入更详细的输入是至关重要的。与之前章节中用来自潜在空间的单个噪声向量生成形状、数字和图像不同，我们在音乐生成过程中使用了四个不同的噪声向量。鉴于每首音乐作品由四个轨道和两个小节组成，我们使用这四个向量来有效地管理这种结构。一个向量控制所有轨道和小节，另一个控制所有轨道中的每个小节，第三个监督所有轨道跨越小节的情况，第四个管理每个轨道中的每个小节。
- en: '* * *'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^([1](#footnote-000-backlink))  Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan
    Yang, 2017, “MuseGAN: Multi-track Sequential Generative Adversarial Networks for
    Symbolic Music Generation and Accompaniment.” [https://arxiv.org/abs/1709.06298](https://arxiv.org/abs/1709.06298).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink))  邓浩文，萧文仪，杨立嘉，杨宜璇，2017，“MuseGAN：用于符号音乐生成和伴奏的多轨道序列生成对抗网络。”
    [https://arxiv.org/abs/1709.06298](https://arxiv.org/abs/1709.06298).
