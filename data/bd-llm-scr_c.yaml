- en: Appendix C. Exercise Solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录C. 练习解答
- en: The complete code examples for the exercises answers can be found in the supplementary
    GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 练习答案的完整代码示例可以在补充的GitHub库中找到，网址为[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)。
- en: C.1 Chapter 2
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.1 第2章
- en: Exercise 2.1
  id: totrans-3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习2.1
- en: 'You can obtain the individual token IDs by prompting the encoder with one string
    at a time:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过逐次提示编码器输入一个字符串来获得各个标记的ID：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This prints:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then use the following code to assemble the original string:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下代码组装原始字符串：
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This returns:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exercise 2.2
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习2.2
- en: 'The code for the data loader with `max_length=2 and stride=2`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器的代码为`max_length=2 and stride=2`：
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It produces batches of the following format:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成以下格式的批次：
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code of the second data loader with `max_length=8 and stride=2`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据加载器的代码为`max_length=8 and stride=2`：
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'An example batch looks like as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例批次如下所示：
- en: '[PRE7]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: C.2 Chapter 3
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.2 第3章
- en: Exercise 3.1
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习3.1
- en: 'The correct weight assignment is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的权重分配如下：
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Exercise 3.2
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习3.2
- en: To achieve an output dimension of 2, similar to what we had in single-head attention,
    we need to change the projection dimension `d_out` to 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现与单头注意力相似的输出维度为2，我们需要将投影维度`d_out`更改为1。
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Exercise 3.3
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习3.3
- en: 'The initialization for the smallest GPT-2 model is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最小GPT-2模型的初始化如下：
- en: '[PRE10]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: C.3 Chapter 4
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.3 第4章
- en: Exercise 4.1
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习4.1
- en: 'We can calculate the number of parameters in the feed forward and attention
    modules as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式计算前馈和注意力模块中的参数数量：
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we can see, the feed forward module contains approximately twice as many
    parameters as the attention module:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，前馈模块的参数大约是注意力模块的两倍：
- en: '[PRE12]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Exercise 4.2
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习4.2
- en: 'To instantiate the other GPT model sizes, we can modify the configuration dictionary
    as follows (here shown for GPT-2 XL):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化其他GPT模型的尺寸，我们可以按如下方式修改配置字典（此处显示的是GPT-2 XL）：
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, reusing the code from Section 4.6 to calculate the number of parameters
    and RAM requirements, we find the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，重用第4.6节的代码来计算参数数量和RAM要求，我们发现如下：
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: C.4 Chapter 5
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.4 第5章
- en: Exercise 5.1
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习5.1
- en: We can print the number of times the token (or word) "pizza" is sampled using
    the `print_sampled_tokens` function we defined in this section. Let's start with
    the code we defined in section 5.3.1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在本节定义的`print_sampled_tokens`函数打印“pizza”标记（或单词）的采样次数。让我们从第5.3.1节中定义的代码开始。
- en: The "pizza" token is sampled 0x if the temperature is 0 or 0.1, and it is sampled
    32× if the temperature is scaled up to 5\. The estimated probability is 32/1000
    × 100% = 3.2%.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果温度为0或0.1，则“pizza”标记的采样次数为0x，如果温度提高到5，则采样32×。估计的概率为32/1000 × 100% = 3.2%。
- en: The actual probability is 4.3% and contained in the rescaled softmax probability
    tensor (`scaled_probas[2][6]`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实际概率为4.3%，包含在重新缩放的softmax概率张量中（`scaled_probas[2][6]`）。
- en: Exercise 5.2
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习5.2
- en: Top-k sampling and temperature scaling are settings that have to be adjusted
    based on the LLM and the desired degree of diversity and randomness in the output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k采样和温度缩放是需要根据LLM和输出中期望的多样性及随机性调整的设置。
- en: When using relatively small top-k values (e.g., smaller than 10) and the temperature
    is set below 1, the model's output becomes less random and more deterministic.
    This setting is useful when we need the generated text to be more predictable,
    coherent, and closer to the most likely outcomes based on the training data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用相对较小的top-k值（例如，小于10）并且温度设置低于1时，模型的输出变得不那么随机，更具确定性。这种设置在我们需要生成的文本更可预测、连贯，并且更接近基于训练数据的最可能结果时非常有用。
- en: Applications for such low k and temperature settings include generating formal
    documents or reports where clarity and accuracy are most important. Other examples
    of applications include technical analysis or code generation tasks, where precision
    is crucial. Also, question answering and educational content require accurate
    answers where a temperature below 1 is helpful.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种低k值和低温度设置的应用包括生成正式文档或报告，其中清晰度和准确性至关重要。其他应用示例包括技术分析或代码生成任务，在这些任务中，精确性至关重要。此外，问答和教育内容也需要准确的答案，在这种情况下，温度低于1是有帮助的。
- en: On the other hand, larger top-k values (e.g., values in the range of 20 to 40)
    and temperature values above 1 are useful when using LLMs for brainstorming or
    generating creative content, such as fiction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当使用 LLM 进行头脑风暴或生成创意内容（如小说）时，较大的 top-k 值（例如 20 到 40 的范围内的值）和温度值高于 1 是有用的。
- en: Exercise 5.3
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: 'There are multiple ways to force deterministic behavior with the `generate`
    function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以强制 `generate` 函数表现出确定性：
- en: Setting to `top_k=None` and applying no temperature scaling;
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置为 `top_k=None` 并不进行温度缩放；
- en: Setting `top_k=1`.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `top_k=1`。
- en: Exercise 5.4
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: 'In essence, we have to load the model and optimizer that we saved in the main
    chapter:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们需要加载在主要章节中保存的模型和优化器：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then, call the `train_simple_function` with `num_epochs=1` to train the model
    for another epoch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，调用 `train_simple_function`，设置 `num_epochs=1` 来训练模型一个新的周期。
- en: Exercise 5.5
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.5
- en: 'We can use the following code to calculate the training and validation set
    losses of the GPT model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码计算 GPT 模型的训练集和验证集损失：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The resulting losses for the 124M parameter are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 124M 参数的损失结果如下：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The main observation is that the training and validation set performances are
    in the same ballpark. This can have multiple explanations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主要观察是训练集和验证集的性能在同一水平上。这可能有多种解释。
- en: The Verdict was not part of the pretraining dataset when OpenAI trained GPT-2\.
    Hence, the model is not explicitly overfitting to the training set and performs
    similarly well on The Verdict's training and validation set portions. (The validation
    set loss is slightly lower than the training set loss, which is unusual in deep
    learning. However, it's likely due to random noise since the dataset is relatively
    small. In practice, if there is no overfitting, the training and validation set
    performances are expected to be roughly identical).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 OpenAI 训练 GPT-2 时，《判决》并不是预训练数据集的一部分。因此，模型并没有明显地过拟合训练集，并且在《判决》的训练和验证集部分表现相似。（验证集损失略低于训练集损失，这在深度学习中并不常见。然而，这可能是由于随机噪声，因为数据集相对较小。在实践中，如果没有过拟合，训练集和验证集的性能预计是大致相同的）。
- en: The Verdict was part of GPT -2's training dataset. In this case, we can't tell
    whether the model is overfitting the training data because the validation set
    would have been used for training as well. To evaluate the degree of overfitting,
    we'd need a new dataset generated after OpenAI finished training GPT-2 to make
    sure that it couldn't have been part of the pretraining.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 《判决》是 GPT-2 训练数据集的一部分。在这种情况下，我们无法判断模型是否在训练数据上过拟合，因为验证集也会被用于训练。要评估过拟合的程度，我们需要在
    OpenAI 完成 GPT-2 训练后生成的新数据集，以确保它不是预训练的一部分。
- en: Exercise 5.6
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.6
- en: 'In the main chapter, we experimented with the smallest GPT-2 model, which has
    only 124M parameters. The reason was to keep the resource requirements as low
    as possible. However, you can easily experiment with larger models with minimal
    code changes. For example, instead of loading the 1558M instead of 124M model
    in chapter 5, the only 2 lines of code that we have to change are the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要章节中，我们实验了最小的 GPT-2 模型，该模型仅有 124M 参数。这样做的原因是尽量降低资源需求。然而，你可以通过最小的代码更改轻松尝试更大的模型。例如，在第
    5 章中加载 1558M 而不是 124M 模型时，我们只需更改以下 2 行代码：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The updated code is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的代码如下：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
