- en: 2 Harnessing the power of large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 利用大型语言模型的力量
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the basics of LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLMs的基本知识
- en: Connecting to and consuming the OpenAI API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到并使用OpenAI API
- en: Exploring and using open source LLMs with LM Studio
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LM Studio探索和使用开源LLMs
- en: Prompting LLMs with prompt engineering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提示工程提示LLMs
- en: Choosing the optimal LLM for your specific needs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合您特定需求的最佳LLM
- en: The term *large language models* (LLMs) has now become a ubiquitous descriptor
    of a form of AI. These LLMs have been developed using generative pretrained transformers
    (GPTs). While other architectures also power LLMs, the GPT form is currently the
    most successful.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “大型语言模型”（LLMs）这一术语现在已成为一种AI形式的普遍描述。这些LLMs是使用生成预训练的transformers（GPTs）开发的。虽然其他架构也能驱动LLMs，但GPT形式目前是最成功的。
- en: LLMs and GPTs are *generative* models, which means they are trained to *generate*
    rather than predict or classify content. To illustrate this further, consider
    figure 2.1, which shows the difference between generative and predictive/classification
    models. Generative models create something from the input, whereas predictive
    and classifying models classify it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs和GPTs是**生成**模型，这意味着它们被训练来**生成**内容，而不是预测或分类。为了进一步说明这一点，请考虑图2.1，它展示了生成模型与预测/分类模型之间的区别。生成模型从输入中创建内容，而预测和分类模型则对其进行分类。
- en: '![figure](../Images/2-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 The difference between generative and predictive models
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 生成模型与预测模型的区别
- en: We can further define an LLM by its constituent parts, as shown in figure 2.2\.
    In this diagram, *data* represents the content used to train the model, and *architecture*
    is an attribute of the model itself, such as the number of parameters or size
    of the model. Models are further trained specifically to the desired use case,
    including chat, completions, or instruction. Finally, *fine-tuning* is a feature
    added to models that refines the input data and model training to better match
    a particular use case or domain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过其组成部分进一步定义LLM，如图2.2所示。在这个图中，“数据”代表用于训练模型的内容，“架构”是模型本身的属性，例如参数数量或模型大小。模型进一步被特定地训练以适应期望的应用场景，包括聊天、完成或指令。最后，“微调”是添加到模型中的功能，它通过优化输入数据和模型训练以更好地匹配特定用例或领域。
- en: '![figure](../Images/2-2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 The main elements that describe an LLM
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 描述LLM的主要元素
- en: The transformer architecture of GPTs, which is a specific architecture of LLMs,
    allows the models to be scaled to billions of parameters in size. This requires
    these large models to be trained on terabytes of documents to build a foundation.
    From there, these models will be successively trained using various methods for
    the desired use case of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GPTs的transformer架构，作为LLMs的一种特定架构，使得模型可以扩展到数十亿个参数的大小。这要求这些大型模型在数TB的文档上进行训练以建立基础。从那里，这些模型将依次使用各种方法进行训练，以适应模型期望的应用场景。
- en: ChatGPT, for example, is trained effectively on the public internet and then
    fine-tuned using several training strategies. The final fine-tuning training is
    completed using an advanced form called *reinforcement learning with human feedback*
    (RLHF). This produces a model use case called chat completions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以ChatGPT为例，它在公共互联网上进行了有效的训练，然后使用几种训练策略进行微调。最终的微调训练使用一种高级形式完成，称为**强化学习与人类反馈**（RLHF）。这产生了一个名为聊天完成的模型用例。
- en: '*Chat completions* LLMs are designed to improve through iteration and refinement—in
    other words, chatting. These models have also been benchmarked to be the best
    in task completion, reasoning, and planning, which makes them ideal for building
    agents and assistants. Completion models are trained/designed only to provide
    generated content on input text, so they don’t benefit from iteration.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天完成**型LLMs旨在通过迭代和改进来提高性能——换句话说，就是聊天。这些模型也已被基准测试为在任务完成、推理和规划方面表现最佳，这使得它们非常适合构建代理人和助手。完成模型仅被训练/设计来在输入文本上提供生成内容，因此它们不受益于迭代。'
- en: For our journey to build powerful agents in this book, we focus on the class
    of LLMs called chat completions models. That, of course, doesn’t preclude you
    from trying other model forms for your agents. However, you may have to significantly
    alter the code samples provided to support other model forms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们专注于构建强大代理人的旅程，我们关注的是被称为聊天完成模型的LLMs类别。当然，这并不妨碍你尝试为你的代理人使用其他模型形式。然而，你可能需要显著修改提供的代码示例以支持其他模型形式。
- en: We’ll uncover more details about LLMs and GPTs later in this chapter when we
    look at running an open source LLM locally. In the next section, we look at how
    to connect to an LLM using a growing standard from OpenAI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，当我们查看在本地运行开源 LLM 时，我们将揭示更多关于 LLM 和 GPT 的细节。在下一节中，我们将探讨如何使用 OpenAI 的一个日益增长的标准连接到
    LLM。
- en: 2.1 Mastering the OpenAI API
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 掌握 OpenAI API
- en: Numerous AI agents and assistant projects use the OpenAI API SDK to connect
    to an LLM. While not standard, the basic concepts describing a connection now
    follow the OpenAI pattern. Therefore, we must understand the core concepts of
    an LLM connection using the OpenAI SDK.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 AI 代理和助手项目使用 OpenAI API SDK 连接到 LLM。虽然不是标准，但现在描述连接的基本概念遵循 OpenAI 模式。因此，我们必须了解使用
    OpenAI SDK 进行 LLM 连接的核心概念。
- en: This chapter will look at connecting to an LLM model using the OpenAI Python
    SDK/package. We’ll discuss connecting to a GPT-4 model, the model response, counting
    tokens, and how to define consistent messages. Starting in the following subsection,
    we’ll examine how to use OpenAI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨使用 OpenAI Python SDK/包连接到 LLM 模型。我们将讨论连接到 GPT-4 模型、模型响应、计数字符数以及如何定义一致的消息。从以下子节开始，我们将检查如何使用
    OpenAI。
- en: 2.1.1 Connecting to the chat completions model
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 连接到聊天完成模型
- en: To complete the exercises in this section and subsequent ones, you must set
    up a Python developer environment and get access to an LLM. Appendix A walks you
    through setting up an OpenAI account and accessing GPT-4 or other models. Appendix
    B demonstrates setting up a Python development environment with Visual Studio
    Code (VS Code), including installing needed extensions. Review these sections
    if you want to follow along with the scenarios.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本节及后续章节的练习，你必须设置一个 Python 开发环境并获取访问 LLM 的权限。附录 A 会指导你如何设置 OpenAI 账户并访问 GPT-4
    或其他模型。附录 B 展示了如何使用 Visual Studio Code (VS Code) 设置 Python 开发环境，包括安装所需的扩展。如果你想跟随场景进行，请回顾这些部分。
- en: Start by opening the source code `chapter_2` folder in VS Code and creating
    a new Python virtual environment. Again, refer to appendix B if you need assistance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在 VS Code 中打开源代码 `chapter_2` 文件夹，并创建一个新的 Python 虚拟环境。再次，如果需要帮助，请参考附录 B。
- en: Then, install the OpenAI and Python dot environment packages using the command
    in the following listing. This will install the required packages into the virtual
    environment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下列表中的命令安装 OpenAI 和 Python dot 环境包。这将把所需的包安装到虚拟环境中。
- en: Listing 2.1 `pip` installs
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1 `pip` 安装
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, open the `connecting.py` file in VS Code, and inspect the code shown in
    listing 2.2\. Be sure to set the model’s name to an appropriate name—for example,
    gpt-4\. At the time of writing, the `gpt-4-1106-preview` was used to represent
    GPT-4 Turbo.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 VS Code 中打开 `connecting.py` 文件，检查列表 2.2 中显示的代码。确保将模型的名称设置为合适的名称——例如，gpt-4。在撰写本文时，使用
    `gpt-4-1106-preview` 来表示 GPT-4 Turbo。
- en: Listing 2.2 `connecting.py`
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.2 `connecting.py`
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Loads the secrets stored in the .env file'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载 .env 文件中存储的秘密'
- en: '#2 Checks to see whether the key is set'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检查是否设置了密钥'
- en: '#3 Creates a client with the key'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用密钥创建客户端'
- en: '#4 Uses the create function to generate a response'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 create 函数生成响应'
- en: '#5 Returns just the content of the response'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 仅返回响应的内容'
- en: '#6 Executes the request and returns the response'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 执行请求并返回响应'
- en: A lot is happening here, so let’s break it down by section, starting with the
    beginning and loading the environment variables. In the `chapter_2` folder is
    another file called `.env`, which holds environment variables. These variables
    are set automatically by calling the `load_dotenv` function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情，所以让我们按部分分解，从开始加载环境变量开始。在 `chapter_2` 文件夹中还有一个名为 `.env` 的文件，它包含环境变量。这些变量通过调用
    `load_dotenv` 函数自动设置。
- en: You must set your OpenAI API key in the `.env` file, as shown in the next listing.
    Again, refer to appendix A to find out how to get a key and find a model name.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须在 `.env` 文件中设置你的 OpenAI API 密钥，如下一个列表所示。再次，请参考附录 A 了解如何获取密钥和找到模型名称。
- en: Listing 2.3 `.env`
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.3 `.env`
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After setting the key, you can debug the file by pressing the F5 key or selecting
    Run > Start Debugging from the VS Code menu. This will run the code, and you should
    see something like “The capital of France is Paris.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设置密钥后，你可以通过按 F5 键或从 VS Code 菜单中选择“运行”>“开始调试”来调试文件。这将运行代码，你应该会看到类似“法国的首都是巴黎”的内容。
- en: Remember that the response from a generative model depends on the probability.
    The model will probably give us a correct and consistent answer in this case.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，生成模型的响应取决于概率。在这种情况下，模型很可能会给出正确且一致的答案。
- en: You can play with these probabilities by adjusting the temperature of the request.
    If you want a model to be more consistent, turn the temperature down to 0, but
    if you want the model to produce more variation, turn the temperature up. We’ll
    explore setting the temperature further in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调整请求的温度来玩这些概率。如果您想让模型更一致，请将温度降低到 0，但如果您想让模型产生更多变化，请提高温度。我们将在下一节进一步探讨设置温度。
- en: 2.1.2 Understanding the request and response
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 理解请求和响应
- en: Digging into the chat completions request and response features can be helpful.
    We’ll focus on the request first, as shown next. The request encapsulates the
    intended model, the messages, and the temperature.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究聊天完成请求和响应功能可能会有所帮助。我们将首先关注请求，如下所示。请求封装了预期的模型、消息和温度。
- en: Listing 2.4 The chat completions request
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 聊天完成请求
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 The model or deployment used to respond to the request'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于响应请求的模型或部署'
- en: '#2 The system role message'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 系统角色消息'
- en: '#3 The user role message'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 用户角色消息'
- en: '#4 The temperature or variability of the request'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 请求的温度或变异性'
- en: 'Within the request, the `messages` block describes a set of messages and roles
    used in a request. Messages for a chat completions model can be defined in three
    roles:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求中，`messages` 块描述了一组用于请求的消息和角色。聊天完成模型的消息可以定义在三个角色中：
- en: '*System role* —A message that describes the request’s rules and guidelines.
    It can often be used to describe the role of the LLM in making the request.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统角色* — 描述请求的规则和指南的消息。它通常用于描述 LLM 在请求中的角色。'
- en: '*User role* —Represents and contains the message from the user.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户角色* — 代表并包含用户的消息。'
- en: '*Assistant role* —Can be used to capture the message history of previous responses
    from the LLM. It can also inject a message history when perhaps none existed.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*助手角色* — 可以用来捕获 LLM 之前响应的消息历史。它还可以在可能没有消息历史的情况下注入消息历史。'
- en: The message sent in a single request can encapsulate an entire conversation,
    as shown in the JSON in the following listing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 单个请求中发送的消息可以封装整个对话，如下一个列表中的 JSON 所示。
- en: Listing 2.5 Messages with history
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5 带有历史记录的消息
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see how this can be applied by opening `message_history.py` in VS Code
    and debugging it by pressing F5\. After the file runs, be sure to check the output.
    Then, try to run the sample a few more times to see how the results change.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在 VS Code 中打开 `message_history.py` 并按 F5 调试来查看如何应用此功能。文件运行后，请务必检查输出。然后，尝试再次运行示例，以查看结果如何变化。
- en: The results will change from each run to the next due to the high temperature
    of `.7`. Go ahead and reduce the temperature to `.0`, and run the `message_history.py`
    sample a few more times. Keeping the temperature at `0` will show the same or
    similar results each time.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `.7` 的高温度，结果会从每次运行到下一次运行而变化。请继续将温度降低到 `.0`，并多次运行 `message_history.py` 示例。将温度保持在
    `0` 将每次都显示相同或类似的结果。
- en: Setting a request’s temperature will often depend on your particular use case.
    Sometimes, you may want to limit the responses’ stochastic nature (randomness).
    Reducing the temperature to `0` will give consistent results. Likewise, a value
    of `1.0` will give the most variability in the responses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 设置请求的温度通常会取决于您的特定用例。有时，您可能想限制响应的随机性（随机性）。将温度降低到 `0` 将给出一致的结果。同样，`1.0` 的值将给出响应中最多的变异性。
- en: Next, we also want to know what information is being returned for each request.
    The next listing shows the output format for the response. You can see this output
    by running the `message_history.py` file in VS Code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还想了解每次请求返回的信息。下一个列表显示了响应的输出格式。您可以通过在 VS Code 中运行 `message_history.py`
    文件来查看此输出。
- en: Listing 2.6 Chat completions response
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 聊天完成响应
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 A model may return more than one response.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 模型可能返回多个响应。'
- en: '#2 Responses returned in the assistant role'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在助手角色中返回的响应'
- en: '#3 Indicates the model used'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 指示使用的模型'
- en: '#4 Counts the number of input (prompt) and output (completion) tokens used'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算输入（提示）和输出（完成）标记的数量'
- en: It can be helpful to track the number of *input tokens* (those used in prompts)
    and the *output tokens* (the number returned through completions). Sometimes,
    minimizing and reducing the number of tokens can be essential. Having fewer tokens
    typically means LLM interactions will be cheaper, respond faster, and produce
    better and more consistent results.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪输入标记（用于提示中的标记）和输出标记（通过完成返回的标记数量）的数量可能会有所帮助。有时，最小化和减少标记数量可能是至关重要的。通常，标记数量较少意味着LLM交互将更便宜，响应更快，并产生更好、更一致的结果。
- en: That covers the basics of connecting to an LLM and returning responses. Throughout
    this book, we’ll review and expand on how to interact with LLMs. Until then, we’ll
    explore in the next section how to load and use open source LLMs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了连接到LLM并返回响应的基本知识。在整个书中，我们将回顾和扩展如何与LLM交互。在此之前，我们将在下一节中探讨如何加载和使用开源LLM。
- en: 2.2 Exploring open source LLMs with LM Studio
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用LM Studio探索开源LLM
- en: Commercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start
    to learn how to use modern AI and build agents. However, commercial agents are
    an external resource that comes at a cost, reduces data privacy and security,
    and introduces dependencies. Other external influences can further complicate
    these factors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 商业LLM，如OpenAI的GPT-4，是学习如何使用现代AI和构建代理的绝佳起点。然而，商业代理是一个外部资源，需要付费，会降低数据隐私和安全，并引入依赖。其他外部影响将进一步复杂化这些因素。
- en: It’s unsurprising that the race to build comparable open source LLMs is growing
    more competitive every day. As a result, there are now open source LLMs that may
    be adequate for numerous tasks and agent systems. There have even been so many
    advances in tooling in just a year that hosting LLMs locally is now very easy,
    as we’ll see in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 建立与开源大型语言模型（LLM）相媲美的竞争日益激烈，这并不令人惊讶。因此，现在已经有了一些开源LLM，它们可能足够用于众多任务和代理系统。仅在一年内，工具的发展就取得了许多进步，以至于现在在本地托管LLM变得非常容易，正如我们将在下一节中看到的。
- en: 2.2.1 Installing and running LM Studio
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 安装和运行LM Studio
- en: 'LM Studio is a free download that supports downloading and hosting LLMs and
    other models locally for Windows, Mac, and Linux. The software is easy to use
    and offers several helpful features to get you started quickly. Here is a quick
    summary of steps to download and set up LM Studio:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LM Studio是一个免费下载的软件，支持在Windows、Mac和Linux上本地下载和托管LLM和其他模型。该软件易于使用，并提供了一些有助于快速入门的有用功能。以下是下载和设置LM
    Studio的步骤快速总结：
- en: Download LM Studio from [https://lmstudio.ai/](https://lmstudio.ai/).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://lmstudio.ai/](https://lmstudio.ai/)下载LM Studio。
- en: After downloading, install the software per your operating system. Be aware
    that some versions of LM Studio may be in beta and require installation of additional
    tools or libraries.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载后，根据您的操作系统安装软件。请注意，LM Studio的一些版本可能处于测试版，需要安装额外的工具或库。
- en: Launch the software.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动软件。
- en: Figure 2.3 shows the LM Studio window running. From there, you can review the
    current list of hot models, search for others, and even download. The home page
    content can be handy for understanding the details and specifications of the top
    models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3显示了正在运行的LM Studio窗口。从那里，您可以查看当前的热门模型列表，搜索其他模型，甚至下载。主页内容对于了解顶级模型的详细信息和规格非常有用。
- en: '![figure](../Images/2-3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-3.png)'
- en: Figure 2.3 LM Studio software showing the main home page
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 LM Studio软件显示主主页
- en: An appealing feature of LM Studio is its ability to analyze your hardware and
    align it with the requirements of a given model. The software will let you know
    how well you can run a given model. This can be a great time saver in guiding
    what models you experiment with.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LM Studio的一个吸引人的特点是它能够分析您的硬件，并将其与给定模型的 requirements 对齐。软件将告诉您您能多好地运行给定模型。这可以在指导您尝试哪些模型时节省大量时间。
- en: Enter some text to search for a model, and click Go. You’ll be taken to the
    search page interface, as shown in figure 2.4\. From this page, you can see all
    the model variations and other specifications, such as context token size. After
    you click the Compatibility Guess button, the software will even tell you if the
    model will run on your system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输入一些文本以搜索模型，然后点击“Go”。您将被带到如图2.4所示的搜索页面界面。从该页面，您可以查看所有模型变体和其他规格，例如上下文标记大小。在您点击兼容性猜测按钮后，软件甚至会告诉您该模型是否能在您的系统上运行。
- en: '![figure](../Images/2-4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-4.png)'
- en: Figure 2.4 The LM Studio search page
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 LM Studio搜索页面
- en: Click to download any model that will run on your system. You may want to stick
    with models designed for chat completions, but if your system is limited, work
    with what you have. In addition, if you’re unsure of which model to use, go ahead
    and download to try them. LM Studio is a great way to explore and experiment with
    many models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下载将在你的系统上运行的任何模型。你可能想坚持使用为聊天完成设计的模型，但如果你的系统有限，就使用你拥有的。此外，如果你不确定使用哪个模型，可以下载来尝试。LM
    Studio是探索和实验许多模型的好方法。
- en: After the model is downloaded, you can then load and run the model on the chat
    page or as a server on the server page. Figure 2.5 shows loading and running a
    model on the chat page. It also shows the option for enabling and using a GPU
    if you have one.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型下载后，你可以在聊天页面或服务器页面作为服务器加载和运行模型。图2.5显示了在聊天页面加载和运行模型。它还显示了如果你有GPU，启用和使用GPU的选项。
- en: '![figure](../Images/2-5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-5.png)'
- en: Figure 2.5 The LM Studio chat page with a loaded, locally running LLM
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 加载并运行在本地运行的LLM的LM Studio聊天页面
- en: To load and run a model, open the drop-down menu at the top middle of the page,
    and select a downloaded model. A progress bar will appear showing the model loading,
    and when it’s ready, you can start typing into the UI.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载和运行一个模型，请打开页面顶部中间的下拉菜单，并选择一个已下载的模型。会出现一个进度条显示模型加载状态，当它准备好后，你可以在UI中开始输入。
- en: The software even allows you to use some or all of your GPU, if detected, for
    the model inference. A GPU will generally speed up the model response times in
    some capacities. You can see how adding a GPU can affect the model’s performance
    by looking at the performance status at the bottom of the page, as shown in figure
    2.5.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到GPU，该软件甚至允许你使用部分或全部GPU进行模型推理。GPU通常可以在某些方面加快模型响应时间。你可以通过查看页面底部的性能状态来了解添加GPU如何影响模型性能，如图2.5所示。
- en: Chatting with a model and using or playing with various prompts can help you
    determine how well a model will work for your given use case. A more systematic
    approach is using the prompt flow tool for evaluating prompts and LLMs. We’ll
    describe how to use prompt flow in chapter 9.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型聊天以及使用或玩转各种提示可以帮助你确定模型在你特定用例中的适用性。更系统的方法是使用提示流工具来评估提示和LLM。我们将在第9章中描述如何使用提示流。
- en: LM Studio also allows a model to be run on a server and made accessible using
    the OpenAI package. We’ll see how to use the server feature and serve a model
    in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LM Studio还允许在服务器上运行模型，并使用OpenAI包使其可访问。我们将在下一节中看到如何使用服务器功能并提供模型。
- en: 2.2.2 Serving an LLM locally with LM Studio
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 使用LM Studio在本地提供LLM服务
- en: Running an LLM locally as a server is easy with LM Studio. Just open the server
    page, load a model, and then click the Start Server button, as shown in figure
    2.6\. From there, you can copy and paste any of the examples to connect with your
    model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LM Studio在本地作为服务器运行LLM非常简单。只需打开服务器页面，加载一个模型，然后点击“启动服务器”按钮，如图2.6所示。从那里，你可以复制并粘贴任何示例来连接到你的模型。
- en: '![figure](../Images/2-6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-6.png)'
- en: Figure 2.6 The LM Studio server page and a server running an LLM
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 LM Studio服务器页面和运行LLM的服务器
- en: You can review an example of the Python code by opening `chapter_2/lmstudio_
    server.py` in VS Code. The code is also shown here in listing 2.7\. Then, run
    the code in the VS Code debugger (press F5).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在VS Code中打开`chapter_2/lmstudio_server.py`来查看Python代码的示例。代码也在这里列出为列表2.7。然后，在VS
    Code调试器中运行代码（按F5）。
- en: Listing 2.7 `lmstudio_server.py`
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7 `lmstudio_server.py`
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Currently not used; can be anything'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 目前未使用；可以是任何内容'
- en: '#2 Feel free to change the message as you like.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 随意更改消息。'
- en: '#3 Default code outputs the whole message.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 默认代码输出整个消息。'
- en: If you encounter problems connecting to the server or experience any other problems,
    be sure your configuration for the Server Model Settings matches the model type.
    For example, in figure 2.6, shown earlier, the loaded model differs from the server
    settings. The corrected settings are shown in figure 2.7.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到连接到服务器的问题或遇到其他任何问题，请确保你的服务器模型设置配置与模型类型匹配。例如，在前面显示的图2.6中，加载的模型与服务器设置不同。修正后的设置如图2.7所示。
- en: '![figure](../Images/2-7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-7.png)'
- en: Figure 2.7 Choosing the correct Server Model Settings for the loaded model
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 选择加载模型的正确服务器模型设置
- en: Now, you can use a locally hosted LLM or a commercial model to build, test,
    and potentially even run your agents. The following section will examine how to
    build prompts using prompt engineering more effectively.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用本地托管的LLM或商业模型来构建、测试，甚至可能运行你的代理。下一节将探讨如何更有效地使用提示工程构建提示。
- en: 2.3 Prompting LLMs with prompt engineering
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用提示工程提示语言模型
- en: A prompt defined for LLMs is the message content used in the request for better
    response output. *Prompt engineering* is a new and emerging field that attempts
    to structure a methodology for building prompts. Unfortunately, prompt building
    isn’t a well-established science, and there is a growing and diverse set of methods
    defined as prompt engineering.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为LLM定义的提示是用于请求以获得更好的响应输出的消息内容。“提示工程”是一个新兴的领域，试图为构建提示构建一种方法论。不幸的是，提示构建不是一个成熟的科学，而且定义了越来越多的方法，这些方法被称为提示工程。
- en: Fortunately, organizations such as OpenAI have begun documenting a universal
    set of strategies, as shown in figure 2.8\. These strategies cover various tactics,
    some requiring additional infrastructure and considerations. As such, the prompt
    engineering strategies relating to more advanced concepts will be covered in the
    indicated chapters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，像OpenAI这样的组织已经开始记录一套通用的策略，如图2.8所示。这些策略涵盖了各种战术，其中一些需要额外的基础设施和考虑。因此，与更高级概念相关的提示工程策略将在指定的章节中介绍。
- en: '![figure](../Images/2-8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-8.png)'
- en: Figure 2.8 OpenAI prompt engineering strategies reviewed in this book, by chapter
    location
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 本书中按章节位置审查的OpenAI提示工程策略
- en: Each strategy in figure 2.8 unfolds into tactics that can further refine the
    specific method of prompt engineering. This chapter will examine the fundamental
    Write Clear Instructions strategy. Figure 2.9 shows the tactics for this strategy
    in more detail, along with examples for each tactic. We’ll look at running these
    examples using a code demo in the following sections.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8中的每个策略都展开为可以进一步细化特定提示工程方法的战术。本章将检查基本的写清晰指令策略。图2.9更详细地展示了该策略的战术，以及每个战术的示例。我们将在下一节中查看如何使用代码演示运行这些示例。
- en: '![figure](../Images/2-9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-9.png)'
- en: Figure 2.9 The tactics for the Write Clear Instructions strategy
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 写清晰指令策略的战术
- en: The Write Clear Instructions strategy is about being careful and specific about
    what you ask for. Asking an LLM to perform a task is no different from asking
    a person to complete the same task. Generally, the more information and context
    relevant to a task you can specify in a request, the better the response.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 写清晰指令策略是关于在要求时保持谨慎和具体。要求一个语言模型（LLM）执行一项任务与要求一个人完成同样的任务并无不同。一般来说，你能在请求中指定与任务相关的更多信息和上下文，那么响应就会越好。
- en: This strategy has been broken down into specific tactics you can apply to prompts.
    To understand how to use those, a code demo (`prompt_engineering.py`) with various
    prompt examples is in the `chapter 2` source code folder.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略已经被分解为可以应用于提示的具体战术。为了理解如何使用这些战术，一个包含各种提示示例的代码演示（`prompt_engineering.py`）位于“第2章”源代码文件夹中。
- en: Open the `prompt_engineering.py` file in VS Code, as shown in listing 2.8\.
    This code starts by loading all the JSON Lines files in the `prompts` folder.
    Then, it displays the list of files as choices and allows the user to select a
    prompt option. After selecting the option, the prompts are submitted to an LLM,
    and the response is printed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在VS Code中打开`prompt_engineering.py`文件，如图2.8所示。此代码首先加载`prompts`文件夹中的所有JSON Lines文件。然后，它显示文件列表作为选择，并允许用户选择提示选项。选择选项后，提示被提交给LLM，并打印出响应。
- en: Listing 2.8 `prompt_engineering.py` `(main())`
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8 `prompt_engineering.py` `(main())`
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Collects all the files for the given folder'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 收集给定文件夹中的所有文件'
- en: '#2 Prints the list of files as choices'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 打印文件列表作为选择'
- en: '#3 Inputs the user’s choice'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输入用户的选择'
- en: '#4 Loads the prompt and parses it into messages'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 加载提示并将其解析为消息'
- en: '#5 Submits the prompt to an OpenAI LLM'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 将提示提交给OpenAI语言模型'
- en: A commented-out section from the listing demonstrates how to connect to a local
    LLM. This will allow you to explore the same prompt engineering tactics applied
    to open source LLMs running locally. By default, this example uses the OpenAI
    model we configured previously in section 2.1.1\. If you didn’t complete that
    earlier, please go back and do it before running this one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的注释部分演示了如何连接到本地 LLM。这将允许你探索应用于本地运行的开源 LLM 的相同提示工程策略。默认情况下，此示例使用我们在 2.1.1
    节中配置的 OpenAI 模型。如果你没有完成之前的操作，请返回并完成它，然后再运行此示例。
- en: Figure 2.10 shows the output of running the prompt engineering tactics tester,
    the `prompt_engineering.py` file in VS Code. When you run the tester, you can
    enter a value for the tactic you want to test and watch it run.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 显示了运行提示工程策略测试器的输出，即 VS Code 中的 `prompt_engineering.py` 文件。当你运行测试器时，你可以为要测试的策略输入一个值，并观察其运行。
- en: '![figure](../Images/2-10.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-10.png)'
- en: Figure 2.10 The output of the prompt engineering tactics tester
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.10 提示工程策略测试器的输出
- en: In the following sections, we’ll explore each prompt tactic in more detail.
    We’ll also examine the various examples.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将更详细地探讨每个提示策略。我们还将检查各种示例。
- en: 2.3.1 Creating detailed queries
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 创建详细查询
- en: The basic premise of this tactic is to provide as much detail as possible but
    also to be careful not to give irrelevant details. The following listing shows
    the JSON Lines file examples for exploring this tactic.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此策略的基本前提是尽可能提供详细的信息，但也要小心不要提供无关紧要的细节。以下列表显示了用于探索此策略的 JSON Lines 文件示例。
- en: Listing 2.9 `detailed_queries.jsonl`
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.9 `detailed_queries.jsonl`
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 The first example doesn’t use detailed queries.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个示例没有使用详细的查询。'
- en: '#2 First ask the LLM a very general question.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 首先向 LLM 提出一个非常一般的问题。'
- en: '#3 Ask a more specific question, and ask for examples.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 提出一个更具体的问题，并请求示例。'
- en: This example demonstrates the difference between using detailed queries and
    not. It also goes a step further by asking for examples. Remember, the more relevance
    and context you can provide in your prompt, the better the overall response. Asking
    for examples is another way of enforcing the relationship between the question
    and the expected output.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了使用详细查询和不使用查询之间的差异。它还进一步通过请求示例。记住，你能在提示中提供越多相关性和上下文，整体响应就越好。请求示例是加强问题与预期输出之间关系的一种方式。
- en: 2.3.2 Adopting personas
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 采用角色
- en: Adopting personas grants the ability to define an overarching context or set
    of rules to the LLM. The LLM can then use that context and/or rules to frame all
    later output responses. This is a compelling tactic and one that we’ll make heavy
    use of throughout this book.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 采用角色赋予 LLM 定义一个总体上下文或一组规则的能力。LLM 可以然后使用该上下文和/或规则来构建所有后续的输出响应。这是一个有吸引力的策略，我们将在整本书中大量使用它。
- en: Listing 2.10 shows an example of employing two personas to answer the same question.
    This can be an enjoyable technique for exploring a wide range of novel applications,
    from getting demographic feedback to specializing in a specific task or even rubber
    ducking.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 显示了使用两个角色回答相同问题的示例。这可以是一种探索广泛新颖应用的愉快技术，从获取人口统计反馈到专门从事特定任务，甚至橡皮鸭技术。
- en: GPT rubber ducking
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT 橡皮鸭
- en: '*Rubber ducking* is a problem-solving technique in which a person explains
    a problem to an inanimate object, like a rubber duck, to understand or find a
    solution. This method is prevalent in programming and debugging, as articulating
    the problem aloud often helps clarify the problem and can lead to new insights
    or solutions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*橡皮鸭* 是一种问题解决技术，其中一个人向一个无生命物体（如橡皮鸭）解释一个问题，以理解或找到解决方案。这种方法在编程和调试中很常见，因为大声阐述问题往往有助于澄清问题，并可能导致新的见解或解决方案。'
- en: GPT rubber ducking uses the same technique, but instead of an inanimate object,
    we use an LLM. This strategy can be expanded further by giving the LLM a persona
    specific to the desired solution domain.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 橡皮鸭技术使用相同的技巧，但使用的是 LLM 而不是无生命物体。通过给 LLM 赋予特定于所需解决方案领域的角色，这种策略可以进一步扩展。
- en: Listing 2.10 `adopting_personas.jsonl`
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.10 `adopting_personas.jsonl`
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 First persona'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个角色'
- en: '#2 Second persona'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第二个角色'
- en: A core element of agent profiles is the persona. We’ll employ various personas
    to assist agents in completing their tasks. When you run this tactic, pay particular
    attention to the way the LLM outputs the response.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 代理配置文件的核心元素是角色。我们将使用各种角色来帮助代理完成他们的任务。当你运行此策略时，请特别注意 LLM 输出响应的方式。
- en: 2.3.3 Using delimiters
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 使用分隔符
- en: Delimiters are a useful way of isolating and getting the LLM to focus on some
    part of a message. This tactic is often combined with other tactics but can work
    well independently. The following listing demonstrates two examples, but there
    are several other ways of describing delimiters, from XML tags to using markdown.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符是一种有用的方法，可以隔离并使LLM专注于消息的某些部分。这个策略通常与其他策略结合使用，但也可以独立工作。以下列表展示了两个示例，但还有其他几种描述分隔符的方法，从XML标签到使用Markdown。
- en: Listing 2.11 `using_delimiters.jsonl`
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.11 `using_delimiters.jsonl`
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The delimiter is defined by character type and repetition.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 分隔符由字符类型和重复定义。'
- en: '#2 The delimiter is defined by XML standards.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 分隔符由XML标准定义。'
- en: When you run this tactic, pay attention to the parts of the text the LLM focuses
    on when it outputs the response. This tactic can be beneficial for describing
    information in a hierarchy or other relationship patterns.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个策略时，请注意LLM在输出响应时关注的文本部分。这个策略对于描述层次结构或其他关系模式的信息非常有用。
- en: 2.3.4 Specifying steps
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 指定步骤
- en: Specifying steps is another powerful tactic that can have many uses, including
    in agents, as shown in listing 2.12\. It’s especially powerful when developing
    prompts or agent profiles for complex multistep tasks. You can specify steps to
    break down these complex prompts into a step-by-step process that the LLM can
    follow. In turn, these steps can guide the LLM through multiple interactions over
    a more extended conversation and many iterations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 指定步骤是另一种强大的策略，可以有很多用途，包括在代理中，如列表 2.12 所示。在开发复杂多步骤任务的提示或代理配置文件时，它尤其强大。你可以指定步骤将复杂的提示分解为LLM可以遵循的逐步过程。反过来，这些步骤可以引导LLM在更长时间的对话和多次迭代中进行多次交互。
- en: Listing 2.12 `specifying_steps.jsonl`
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.12 `specifying_steps.jsonl`
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Notice the tactic of using delimiters.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 注意使用分隔符的策略。'
- en: '#2 Steps can be completely different operations.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 步骤可以是完全不同的操作。'
- en: 2.3.5 Providing examples
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 提供示例
- en: Providing examples is an excellent way to guide the desired output of an LLM.
    There are numerous ways to demonstrate examples to an LLM. The system message/prompt
    can be a helpful way to emphasize general output. In the following listing, the
    example is added as the last LLM assistant reply, given the prompt “Teach me about
    Python.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 提供示例是引导LLM期望输出的绝佳方式。有无数种方法可以向LLM展示示例。系统消息/提示可以是一种强调一般输出的有用方式。在下面的列表中，示例被添加为最后一个LLM助手回复，提示为“教我关于Python。”
- en: Listing 2.13 `providing_examples.jsonl`
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.13 `providing_examples.jsonl`
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Injects the sample output as the “previous” assistant reply'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将样本输出作为“之前的”助手回复注入'
- en: '#2 Adds a limit output tactic to restrict the size of the output and match
    the example'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 添加限制输出策略以限制输出大小并匹配示例'
- en: Providing examples can also be used to request a particular output format from
    a complex series of tasks that derive the output. For example, asking an LLM to
    produce code that matches a sample output is an excellent use of examples. We’ll
    employ this tactic throughout the book, but other methods exist for guiding output.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提供示例也可以用来从一系列复杂的任务中请求特定的输出格式。例如，要求一个LLM生成与样本输出匹配的代码是一个很好的示例使用。我们将在整本书中采用这种策略，但还有其他方法可以用来指导输出。
- en: 2.3.6 Specifying output length
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.6 指定输出长度
- en: The tactic of specifying output length can be helpful in not just limiting tokens
    but also in guiding the output to a desired format. Listing 2.14 shows an example
    of using two different techniques for this tactic. The first limits the output
    to fewer than 10 words. This can have the added benefit of making the response
    more concise and directed, which can be desirable for some use cases. The second
    example demonstrates limiting output to a concise set of bullet points. This method
    can help narrow down the output and keep answers short. More concise answers generally
    mean the output is more focused and contains less filler.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 指定输出长度的策略不仅可以帮助限制标记数，还可以指导输出到期望的格式。列表 2.14 展示了使用两种不同技术实现此策略的示例。第一个示例将输出限制在10个单词以下。这可以带来额外的优势，使响应更加简洁和有针对性，这在某些用例中可能是所希望的。第二个示例演示了将输出限制为简短的要点集合。这种方法可以帮助缩小输出并保持答案简短。更简洁的答案通常意味着输出更加聚焦，包含的填充内容更少。
- en: Listing 2.14 `specifying_output_length.jsonl`
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.14 `specifying_output_length.jsonl`
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Restricting the output makes the answer more concise.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 限制输出使答案更加简洁。'
- en: '#2 Restricts the answer to a short set of bullets'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 限制答案为简短的要点集合'
- en: Keeping answers brief can have additional benefits when developing multi-agent
    systems. Any agent system that converses with other agents can benefit from more
    concise and focused replies. It tends to keep the LLM more focused and reduces
    noisy communication.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 保持答案简短，在开发多智能体系统时可以带来额外的益处。任何与其他智能体进行对话的智能体系统都可以从更加简洁和专注的回复中受益。这有助于使大型语言模型（LLM）更加专注，并减少噪声通信。
- en: Be sure to run through all the examples of the prompt tactics for this strategy.
    As mentioned, we’ll cover other prompt engineering strategies and tactics in future
    chapters. We’ll finish this chapter by looking at how to pick the best LLM for
    your use case.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要运行这个策略的所有提示技巧示例。正如之前提到的，我们将在未来的章节中介绍其他提示工程策略和技巧。我们将通过探讨如何为你的用例选择最佳的LLM来结束这一章。
- en: 2.4 Choosing the optimal LLM for your specific needs
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 为你的特定需求选择最佳LLM
- en: While being a successful crafter of AI agents doesn’t require an in-depth understanding
    of LLMs, it’s helpful to be able to evaluate the specifications. Like a computer
    user, you don’t need to know how to build a processor to understand the differences
    in processor models. This analogy holds well for LLMs, and while the criteria
    may be different, it still depends on some primary considerations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然成为一名成功的AI智能体制作者不需要深入了解LLM，但能够评估规格是有帮助的。就像计算机用户一样，你不需要知道如何构建处理器就能理解处理器型号之间的差异。这个类比对LLM同样适用，尽管标准可能不同，但仍然依赖于一些基本考虑因素。
- en: From our previous discussion and look at LM Studio, we can extract some fundamental
    criteria that will be important to us when considering LLMs. Figure 2.11 explains
    the essential criteria to define what makes an LLM worth considering for creating
    a GPT agent or any LLM task.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前的讨论和对LM Studio的观察中，我们可以提取出一些基本标准，这些标准在我们考虑LLM时将非常重要。图2.11解释了定义LLM为何值得考虑创建GPT智能体或任何LLM任务的基本标准。
- en: '![figure](../Images/2-11.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-11.png)'
- en: Figure 2.11 The important criteria to consider when consuming an LLM
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 消费LLM时需要考虑的重要标准
- en: 'For our purposes of building AI agents, we need to look at each of these criteria
    in terms related to the task. Model context size and speed could be considered
    the sixth and seventh criteria, but they are usually considered variations of
    a model deployment architecture and infrastructure. An eighth criterion to consider
    for an LLM is cost, but this depends on many other factors. Here is a summary
    of how these criteria relate to building AI agents:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们构建AI智能体的目的，我们需要从与任务相关的角度审视这些标准。模型上下文大小和速度可以被认为是第六和第七个标准，但它们通常被视为模型部署架构和基础设施的变体。对于LLM来说，第八个需要考虑的标准是成本，但这取决于许多其他因素。以下是这些标准与构建AI智能体相关性的总结：
- en: '*Model performance* —You’ll generally want to understand the LLM’s performance
    for a given set of tasks. For example, if you’re building an agent specific to
    coding, then an LLM that performs well on code will be essential.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型性能* — 你通常会想了解LLM在特定任务上的性能。例如，如果你正在构建一个针对编码的特定智能体，那么在代码上表现良好的LLM将是必不可少的。'
- en: '*Model parameters (size)* —The size of a model is often an excellent indication
    of inference performance and how well the model responds. However, the size of
    a model will also dictate your hardware requirements. If you plan to use your
    own locally hosted model, the model size will also primarily dictate the computer
    and GPU you need. Fortunately, we’re seeing small, very capable open source models
    being released regularly.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型参数（大小）* — 模型的大小通常是推理性能和模型响应能力的良好指标。然而，模型的大小也会决定你的硬件需求。如果你计划使用自己的本地托管模型，模型的大小也将主要决定你需要的计算机和GPU。幸运的是，我们经常看到小型、非常强大的开源模型被定期发布。'
- en: '*Use case (model type)* —The type of model has several variations. Chat completions
    models such as ChatGPT are effective for iterating and reasoning through a problem,
    whereas models such as completion, question/answer, and instruct are more related
    to specific tasks. A chat completions model is essential for agent applications,
    especially those that iterate.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用例（模型类型）* — 模型的类型有多种变体。像ChatGPT这样的聊天完成模型适用于迭代和推理问题，而像完成、问答和指令这样的模型则更多与特定任务相关。聊天完成模型对于智能体应用至关重要，尤其是那些需要迭代的智能体。'
- en: '*Training input* —Understanding the content used to train a model will often
    dictate the domain of a model. While general models can be effective across tasks,
    more specific or fine-tuned models can be more relevant to a domain. This may
    be a consideration for a domain-specific agent where a smaller, more fine-tuned
    model may perform as well as or better than a larger model such as GPT-4\.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练输入* — 了解用于训练模型的内容通常会决定模型的应用领域。虽然通用模型可以在多个任务中有效，但更具体或微调的模型可能对特定领域更相关。这可能是一个考虑特定领域代理的因素，其中较小、更微调的模型可能表现得与GPT-4等大型模型一样好，甚至更好。'
- en: '*Training method* —It’s perhaps less of a concern, but it can be helpful to
    understand what method was used to train a model. How a model is trained can affect
    its ability to generalize, reason, and plan. This can be essential for planning
    agents but perhaps less significant for agents than for a more task-specific assistant.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练方法* — 这可能不是一个大问题，但了解用于训练模型的方法可能会有所帮助。模型是如何训练的会影响其泛化、推理和规划的能力。这对于规划代理可能是至关重要的，但对于比特定任务助手更通用的代理可能不那么重要。'
- en: '*Context token size* —The context size of a model is more specific to the model
    architecture and type. It dictates the size of context or memory the model may
    hold. A smaller context window of less than 4,000 tokens is typically more than
    enough for simple tasks. However, a large context window can be essential when
    using multiple agents—all conversing over a task. The models will typically be
    deployed with variations on the context window size.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文标记大小* — 模型的上下文大小更具体地与模型架构和类型相关。它决定了模型可能持有的上下文或内存的大小。通常，小于4,000个标记的较小上下文窗口对于简单任务已经足够。然而，当使用多个代理（所有代理都在处理同一任务）时，较大的上下文窗口可能是必不可少的。模型通常将以上下文窗口大小的变体进行部署。'
- en: '*Model speed (model deployment)* —The speed of a model is dictated by its *inference
    speed* (or how fast a model replies to a request), which in turn is dictated by
    the infrastructure it runs on. If your agent isn’t directly interacting with users,
    raw real-time speed may not be necessary. On the other hand, an LLM agent interacting
    in real time needs to be as quick as possible. For commercial models, speed will
    be determined and supported by the provider. Your infrastructure will determine
    the speed for those wanting to run their LLMs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型速度（模型部署）* — 模型的速度由其*推理速度*（或模型响应请求的速度）决定，这反过来又由其运行的基础设施决定。如果你的代理没有直接与用户互动，原始的实时速度可能不是必需的。另一方面，实时交互的LLM代理需要尽可能快。对于商业模型，速度将由提供商确定并支持。你的基础设施将决定那些想要运行他们的LLM的人的速度。'
- en: '*Model cost (project budget)* —The cost is often dictated by the project. Whether
    learning to build an agent or implementing enterprise software, cost is always
    a consideration. A significant tradeoff exists between running your LLMs versus
    using a commercial API.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型成本（项目预算）* — 成本通常由项目决定。无论是学习构建代理还是实施企业软件，成本总是需要考虑的因素。在运行你的LLM与使用商业API之间存在着重大的权衡。'
- en: There is a lot to consider when choosing which model you want to build a production
    agent system on. However, picking and working with a single model is usually best
    for research and learning purposes. If you’re new to LLMs and agents, you’ll likely
    want to choose a commercial option such as GPT-4 Turbo. Unless otherwise stated,
    the work in this book will depend on GPT-4 Turbo.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择要在其上构建生产代理系统的模型时，有很多因素需要考虑。然而，出于研究和学习目的，通常最好选择单个模型。如果你是LLM和代理的新手，你可能会想选择一个商业选项，例如GPT-4
    Turbo。除非另有说明，本书中的工作将依赖于GPT-4 Turbo。
- en: Over time, models will undoubtedly be replaced by better models. So you may
    need to upgrade or swap out models. To do this, though, you must understand the
    performance metrics of your LLMs and agents. Fortunately, in chapter 9, we’ll
    explore evaluating LLMs, prompts, and agent profiles with prompt flow.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，模型无疑将被更好的模型所取代。因此，你可能需要升级或更换模型。但是，为了做到这一点，你必须了解你的LLM和代理的性能指标。幸运的是，在第9章中，我们将探讨使用提示流评估LLM、提示和代理配置文件。
- en: 2.5 Exercises
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 练习
- en: 'Use the following exercises to help you engage with the material in this chapter:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来帮助你参与本章的内容：
- en: '*Exercise 1*—Consuming Different LLMs'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习1* — 消费不同的LLM'
- en: '*Objective *—Use the `connecting.py` code example to consume a different LLM
    from OpenAI or another provider.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* — 使用`connecting.py`代码示例从OpenAI或其他提供商消费不同的LLM。'
- en: '*Tasks*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*：'
- en: Modify `connecting.py` to connect to a different LLM.
  id: totrans-199
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改`connecting.py`以连接到不同的LLM。
- en: Choose an LLM from OpenAI or another provider.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从OpenAI或其他提供商选择一个LLM。
- en: Update the API keys and endpoints in the code.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新代码中的API密钥和端点。
- en: Execute the modified code and validate the response.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行修改后的代码并验证响应。
- en: '*Exercise 2*—Exploring Prompt Engineering Tactics'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习2*—探索提示工程策略'
- en: '*Objective *—Explore various prompt engineering tactics, and create variations
    for each.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—探索各种提示工程策略，并为每个策略创建变体。'
- en: '*Tasks:*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Review the prompt engineering tactics covered in the chapter.
  id: totrans-206
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾章节中涵盖的提示工程策略。
- en: Write variations for each tactic, experimenting with different phrasing and
    structures.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个策略编写变体，尝试不同的措辞和结构。
- en: Test the variations with an LLM to observe different outcomes.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM测试变体以观察不同的结果。
- en: Document the results, and analyze the effectiveness of each variation.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录结果，并分析每个变体的有效性。
- en: '*Exercise 3*—Downloading and Running an LLM with LM Studio'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习3*—使用LM Studio下载和运行LLM'
- en: '*Objective *—Download an LLM using LM Studio, and connect it to prompt engineering
    tactics.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—使用LM Studio下载一个LLM，并将其连接到提示工程策略。'
- en: '*Tasks:*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Install LM Studio on your machine.
  id: totrans-213
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的机器上安装LM Studio。
- en: Download an LLM using LM Studio.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LM Studio下载一个LLM。
- en: Serve the model using LM Studio.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LM Studio提供模型。
- en: Write Python code to connect to the served model.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写Python代码以连接到已提供模型。
- en: Integrate the prompt engineering tactics example with the served model.
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提示工程策略示例与已提供模型集成。
- en: '*Exercise 4*—Comparing Commercial and Open source LLMs'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习4*—比较商业和开源LLM'
- en: '*Objective *—Compare the performance of a commercial LLM such as GPT-4 Turbo
    with an open source model using prompt engineering examples.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—通过提示工程示例比较商业LLM（如GPT-4 Turbo）与开源模型的性能。'
- en: '*Tasks:*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Implement the prompt engineering examples using GPT-4 Turbo.
  id: totrans-221
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-4 Turbo实现提示工程示例。
- en: Repeat the implementation using an open source LLM.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源LLM重复实现。
- en: Evaluate the models based on criteria such as response accuracy, coherence,
    and speed.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据响应准确性、连贯性和速度等标准评估模型。
- en: Document the evaluation process, and summarize the findings.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录评估过程，并总结发现。
- en: '*Exercise 5*—Hosting Alternatives for LLMs'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*练习5*—LLM的托管替代方案'
- en: '*Objective *—Contrast and compare alternatives for hosting an LLM versus using
    a commercial model.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标*—对比和比较托管LLM与使用商业模型的替代方案。'
- en: '*Tasks:*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务：*'
- en: Research different hosting options for LLMs (e.g., local servers, cloud services).
  id: totrans-228
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究LLM的不同托管选项（例如，本地服务器、云服务）。
- en: Evaluate the benefits and drawbacks of each hosting option.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估每种托管选项的优缺点。
- en: Compare these options to using a commercial model in terms of cost, performance,
    and ease of use.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在成本、性能和易用性方面将这些选项与使用商业模型进行比较。
- en: Write a report summarizing the comparison and recommending the best approach
    based on specific use cases.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一份报告，总结比较并基于特定用例推荐最佳方法。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs use a type of architecture called generative pretrained transformers (GPTs).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM使用一种称为生成预训练转换器（GPTs）的架构。
- en: Generative models (e.g., LLMs and GPTs) differ from predictive/classification
    models by learning how to represent data and not simply classify it.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型（例如，LLM和GPTs）与预测/分类模型的不同之处在于学习如何表示数据，而不仅仅是进行分类。
- en: LLMs are a collection of data, architecture, and training for specific use cases,
    called *fine-tuning.*
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM是一组针对特定用例的数据、架构和训练，称为*微调*。
- en: The OpenAI API SDK can be used to connect to an LLM from models, such as GPT-4,
    and also used to consume open source LLMs.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI API SDK可用于从模型（如GPT-4）连接到LLM，并用于消费开源LLM。
- en: You can quickly set up Python environments and install the necessary packages
    for LLM integration.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以快速设置Python环境并安装LLM集成所需的必要包。
- en: LLMs can handle various requests and generate unique responses that can be used
    to enhance programming skills related to LLM integration.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以处理各种请求并生成独特响应，可用于增强与LLM集成相关的编程技能。
- en: Open source LLMs are an alternative to commercial models and can be hosted locally
    using tools such as LM Studio.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源LLM是商业模型的替代品，可以使用LM Studio等工具在本地托管。
- en: Prompt engineering is a collection of techniques that help craft more effective
    prompts to improve LLM responses.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程是一系列技术，有助于制作更有效的提示，以改善LLM的响应。
- en: LLMs can be used to power agents and assistants, from simple chatbots to fully
    capable autonomous workers.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可用于驱动代理和助手，从简单的聊天机器人到完全能够自主工作的工人。
- en: Selecting the most suitable LLM for specific needs depends on the performance,
    parameters, use case, training input, and other criteria.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最适合特定需求的LLM（大型语言模型）取决于性能、参数、用例、训练输入和其他标准。
- en: Running LLMs locally requires a variety of skills, from setting up GPUs to understanding
    various configuration options.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行LLM需要各种技能，从设置GPU到理解各种配置选项。
