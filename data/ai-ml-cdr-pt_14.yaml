- en: Chapter 13\. Hosting PyTorch Models for Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章\. 为服务托管 PyTorch 模型
- en: In the earlier chapters of this book, we looked at many scenarios for training
    ML models, including those for computer vision, NLP, and sequence modeling. But
    that was just the first step—a model is of little use without a way for other
    people to use its power! In this chapter, we’ll take a brief tour of some of the
    more popular tools that allow you to give them a way to do that.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们探讨了训练 ML 模型的许多场景，包括计算机视觉、NLP 和序列建模。但那只是第一步——如果没有其他方式让人们使用其力量，模型几乎毫无用处！在本章中，我们将简要介绍一些更受欢迎的工具，这些工具允许您为它们提供这样的方式。
- en: You should note that taking a trained PyTorch model to a production-ready service
    will involve a lot more than just deploying it, and that the machine learning
    operations (MLOps) discipline is designed with that in mind. When you get into
    the world of serving these models, you’ll need to understand new challenges, such
    as handling real-time requests, managing computational resources, ensuring reliability,
    and maintaining performance under varying loads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该注意，将训练好的 PyTorch 模型部署到生产就绪的服务中，涉及的不仅仅是部署本身，而且机器学习操作（MLOps）学科正是为此而设计的。当您进入为这些模型提供服务的世界时，您需要了解新的挑战，例如处理实时请求、管理计算资源、确保可靠性和在变化负载下维持性能。
- en: Ultimately, MLOps is about bridging the gap between data science and software
    engineering. That’s beyond the scope of this chapter, but there are some great
    books about it out there from O’Reilly, including [*Implementing MLOps in the
    Enterprise*](https://learning.oreilly.com/library/view/implementing-mlops-in/9781098136574)by
    Yaron Haviv and Noah Gift and [*LLMOps*](https://learning.oreilly.com/library/view/llmops/9781098154196)
    by Abi Aryan.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，MLOps 是关于弥合数据科学与软件工程之间的差距。这超出了本章的范围，但市面上有一些关于它的优秀书籍，来自 O'Reilly，包括 Yaron
    Haviv 和 Noah Gift 的 [*Implementing MLOps in the Enterprise*](https://learning.oreilly.com/library/view/implementing-mlops-in/9781098136574)
    和 Abi Aryan 的 [*LLMOps*](https://learning.oreilly.com/library/view/llmops/9781098154196)。
- en: This chapter will introduce two popular approaches to serving PyTorch models
    in production environments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍两种在生产环境中服务 PyTorch 模型的流行方法。
- en: We’ll begin with TorchServe, the official serving solution from PyTorch, which
    provides a robust framework that’s designed specifically for serving deep-learning
    models at scale. TorchServe offers out-of-the-box solutions for standard serving
    requirements like model versioning, A/B testing, and metrics collection. It’s
    also an excellent choice for teams who are looking for a production-ready solution
    with minimal setup.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 TorchServe 开始，这是 PyTorch 的官方服务解决方案，它提供了一个专为大规模服务深度学习模型而设计的强大框架。TorchServe
    为标准服务需求提供了开箱即用的解决方案，如模型版本控制、A/B 测试和指标收集。它也是寻找具有最小设置的生产就绪解决方案的团队的绝佳选择。
- en: Then, we’ll explore how to build serving solutions using the popular Flask framework,
    which is for developers who need more flexibility or have more straightforward
    serving requirements. Flask’s simplicity and extensive ecosystem also make it
    an excellent choice for smaller-scale deployments and proof-of-concept services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨如何使用流行的 Flask 框架构建服务解决方案，这对于需要更多灵活性或具有更直接服务需求的开发者来说是一个不错的选择。Flask 的简单性和广泛的生态系统也使其成为小型部署和概念验证服务的绝佳选择。
- en: As you work through the chapter, you’ll take a hands-on approach in which you’ll
    take some of the models you created in earlier chapters, let us walk you through
    how to deploy them, and then call their hosting servers for inference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在您阅读本章的过程中，您将采取一种动手方法，其中您将使用在前几章中创建的一些模型，让我们引导您如何部署它们，然后调用它们的托管服务器进行推理。
- en: Introducing TorchServe
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 TorchServe
- en: '*TorchServe* is PyTorch’s default serving framework that’s designed for performance
    and flexibility. You can find it at [*pytorch.org/serve*](http://pytorch.org/serve).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*TorchServe* 是 PyTorch 的默认服务框架，旨在提供性能和灵活性。您可以在 [*pytorch.org/serve*](http://pytorch.org/serve)
    找到它。'
- en: TorchServe’s goal was originally to be a reference implementation on how to
    properly serve models with a modular extensible architecture, but it has grown
    beyond that into a fully performant professional-grade framework that is likely
    more than enough for any serving needs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 的最初目标是成为一个关于如何以模块化和可扩展的架构正确服务模型的参考实现，但它已经发展成为一个完全性能化的专业级框架，可能足以满足任何服务需求。
- en: 'It’s also built on a modular architecture with the aim to handle the complexity
    of serving models at scale. To this end, it’s built from the following key components:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它还基于模块化架构，旨在处理大规模服务模型的复杂性。为此，它由以下关键组件构建：
- en: The model server
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器
- en: The PyTorch model server is the central component that handles the lifecycle
    of models and handles all inference requests. It provides the endpoints for model
    management and inference, supporting REST and gRPC calls.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型服务器是处理模型生命周期并处理所有推理请求的核心组件。它提供模型管理和推理的端点，支持 REST 和 gRPC 调用。
- en: Model workers
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工作进程
- en: These are independent processes that load models and perform inference on them.
    Each one is isolated, so in a multimodel serving environment, they are designed
    to continue operating should issues in one model arise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是独立的过程，它们加载模型并在其上进行推理。每个过程都是隔离的，因此在多模型服务环境中，它们被设计为在某个模型出现问题时应继续运行。
- en: Frontend handlers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前端处理器
- en: These are custom Python classes that handle preprocessing, inference, and post-processing
    for specific model types. As you’ll see in a moment, when we get hands-on, frontend
    handlers are complementary to the training code for the model, and it’s good practice
    to create separate handler classes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是处理特定模型类型的前处理、推理和后处理的自定义 Python 类。正如你很快就会看到的，当我们动手实践时，前端处理器与模型的训练代码是互补的，创建单独的处理器类是一种良好的实践。
- en: The model store
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型存储
- en: Model serving in PyTorch uses “model archives,” or MAR files, for the servable
    object. Once you’ve trained your model, you’ll convert it into this format. The
    model store is where these are kept.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的模型服务使用“模型存档”或 MAR 文件作为可服务对象。一旦你训练了你的模型，你将将其转换为这种格式。模型存储是这些文件存放的地方。
- en: You can see the architecture diagram for a TorchServe system in [Figure 13-1](#ch13_figure_1_1748549772551219).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 13-1](#ch13_figure_1_1748549772551219)中查看 TorchServe 系统的架构图。
- en: '![](assets/aiml_1301.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1301.png)'
- en: Figure 13-1\. The TorchServe server infrastructure
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. TorchServe 服务器基础设施
- en: For inference, the client application will call the server infrastructure over
    REST (via default port 8080) or gRPC (via default port 7070). It also provides
    management endpoints at 8081 and 8082, respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，客户端应用程序将通过 REST（默认端口 8080）或 gRPC（默认端口 7070）调用服务器基础设施。它还分别在 8081 和 8082
    提供管理端点。
- en: The endpoint will then call the core model server, which in turn will spawn
    the appropriate number of model workers. The workers will then interface with
    the model handlers to do preprocessing, inference, and postprocessing. The model
    itself will be in the model store, and should it not be in memory, the model server
    will use the request queue to load it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 端点将调用核心模型服务器，然后服务器将启动适当数量的模型工作进程。工作进程将与模型处理器接口进行预处理、推理和后处理。模型本身将存储在模型存储中，如果不在内存中，模型服务器将使用请求队列来加载它。
- en: In the next section, we’ll explore setting up a TorchServe server and using
    it to provide inference for the first model you created in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨设置 TorchServe 服务器并使用它为你在[第 1 章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中创建的第一个模型提供推理。
- en: Setting Up TorchServe
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 TorchServe
- en: I find it’s easiest to learn something if I go through it step-by-step with
    a simple but representative scenario. So, for TorchServe, we’ll install the environment
    first and work from there.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，如果我能通过一个简单但具有代表性的场景一步一步地学习，那么学习某样东西会更容易。因此，对于 TorchServe，我们首先安装环境，然后从这里开始工作。
- en: Preparing Your Environment
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备你的环境
- en: I strongly recommend using a virtual environment, and I’ll be stepping through
    this chapter using venv, which is a freely available one from the Python community
    that you can find [in the Python documentation](https://oreil.ly/FNsPt).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐使用虚拟环境，并且在本章中，我将使用 venv 来演示，这是 Python 社区免费提供的，你可以在[Python 文档](https://oreil.ly/FNsPt)中找到它。
- en: Even if you’ve used virtual environments before, I’d recommend starting with
    a clean one to ensure you are going to install and use the right set of dependencies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你之前使用过虚拟环境，我也建议从一个新的环境开始，以确保你将安装并使用正确的依赖集。
- en: 'You can create a virtual environment like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建一个像这样的虚拟环境：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And once you’ve done that, you can start it with this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，你可以使用以下命令启动它：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, you’ll be ready to install PyTorch. To get started, I recommend installing
    TorchServe, the model archiver, and the workflow archiver like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将准备好安装PyTorch。为了开始，我建议安装TorchServe、模型归档器和工作流程归档器，如下所示：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You aren’t limited to these, and you’ll often need to install other dependencies.
    One of the difficulties of working with TorchServe is that errors may be buried
    in log files, so it’s hard to figure out which dependencies you’ll also need.
    At a minimum, I’ve found that starting from a clean system, I also needed to install
    a JDK after version 11 and PyYAML. Your mileage may vary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你不限于这些，你通常会需要安装其他依赖项。与TorchServe一起工作的一个困难是错误可能隐藏在日志文件中，因此很难确定你需要哪些依赖项。至少，我发现从干净的系统开始，在版本11之后，我也需要安装JDK和PyYAML。你的体验可能会有所不同。
- en: 'Once you’ve done this, you can change to the directory where you want to work,
    and within that, you can create a subdirectory called `model_store`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你可以切换到你想要工作的目录，并在其中创建一个名为 `model_store` 的子目录：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With that in place, the first thing you’ll need to do is set up the configuration
    file for your PyTorch server. It will be a file called *config.properties*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，你需要做的第一件事是为你的PyTorch服务器设置配置文件。它将是一个名为 *config.properties* 的文件。
- en: Setting Up Your config.properties File
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置您的 config.properties 文件
- en: 'There’s a lot going on in this file, and you can learn more about it on the
    [PyTorch site](https://oreil.ly/czQh5). But the important settings are the inference
    and management addresses, which are set to 0.0.0.0 and 8080/8081, respectively
    (similar to what’s shown in [Figure 13-1](#ch13_figure_1_1748549772551219)). Also,
    it’s important to set the directory of the model store to be the one you just
    created. I’ve also set it to log debug messages to help catch dependency issues:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件中有很多内容，你可以在[PyTorch网站](https://oreil.ly/czQh5)上了解更多信息。但重要的设置是推理和管理地址，分别设置为
    0.0.0.0 和 8080/8081（类似于[图13-1](#ch13_figure_1_1748549772551219)中所示）。此外，将模型存储的目录设置为刚刚创建的目录也很重要。我还将其设置为将调试信息记录到日志中，以帮助捕获依赖性问题：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure these settings are in a file with the name *config.properties*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些设置在一个名为 *config.properties* 的文件中。
- en: Defining Your Model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义您的模型
- en: 'In [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566), you
    created a model that learned the relationship between two sets of numbers, when
    the equation describing this relationship was *y* = 2*x* – 1\. When you’re getting
    the model ready for serving, it’s best practice to have a separate file for the
    model definition and the model training. The model definition file will then be
    used in another Python file called the *handler*, which you’ll see momentarily.
    So, to that end, you should create a model definition file, like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中，你创建了一个模型，该模型学习了两组数字之间的关系，当描述这种关系的方程是
    *y* = 2*x* – 1。当你准备将模型用于服务时，最佳实践是有一个单独的文件用于模型定义和模型训练。然后，模型定义文件将用于另一个名为 *handler*
    的Python文件中，你很快就会看到。因此，为此目的，你应该创建一个模型定义文件，如下所示：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, save this into a file called *linear.py* so that you can then load and
    train it with code like the following. Also note the `import` for `SimpleLinearModel`
    that is bolded:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将其保存到名为 *linear.py* 的文件中，这样你就可以使用以下代码加载和训练它。注意，`import` `SimpleLinearModel`
    被加粗了：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In particular, note the last line, where, after training the model, it’s saved
    as a *model.pth* file, along with this piece of code: `mode.state_dic()`, which
    saves out the *state dictionary* (aka the current state of the trained model).
    I find that this way of saving out a model works best with TorchServe.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 特别注意最后一行，在训练模型后，它被保存为 *model.pth* 文件，以及以下代码：`mode.state_dic()`，它保存了 *状态字典*（也称为训练模型的当前状态）。我发现这种保存模型的方式与TorchServe配合得最好。
- en: Then, you run this code to get the file. You’ll use this later to create a *.mar*
    file that goes in the model store. We’ll look at that shortly, but first, we’ll
    need a handler file. You’ll explore that next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行此代码以获取文件。你稍后会使用它来创建一个放入模型存储的 *.mar* 文件。我们很快就会看到这一点，但首先，我们需要一个处理器文件。你将在下一部分探索它。
- en: Creating the Handler File
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建处理器文件
- en: Once you’ve trained and saved the model, you’ll need to create the handler file.
    It’s the job of the handler to do the heavy lifting of serving inference—it loads
    your model, handles data preprocessing, does the inference, and handles any postprocessing
    to turn the inferred values back into data your users may want.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练并保存了模型，你需要创建处理文件。处理器的任务是执行推理的重负载工作——它加载你的模型，处理数据预处理，进行推理，并处理任何后处理，将推断值转换回用户可能需要的格式。
- en: 'This file should inherit from the `base_handler` torch class like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件应继承自 `base_handler` torch 类，如下所示：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you’ve done this, you’ll need to create a model handler class that overrides
    this base class and implements a number of methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你需要创建一个模型处理类，该类覆盖这个基类并实现多个方法。
- en: 'Let’s start with the class declaration and initialization. It’s pretty straightforward:
    just reporting on class initialization:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从类声明和初始化开始。这很简单：只是报告类的初始化：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that an `initialized` property is set to `False` in this code. That may
    seem odd, but the idea here is that this code will initialize the *class* but
    it won’t be ready to use until the `initialize` custom function is called. That
    function will then load the model and get it ready for inference, and at that
    point, we will set `self.initialized` to be `True`. Note that the model is initialized
    as a `SimpleLinearModel`, so you’ll need to import it in the same way as you did
    the training code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这段代码中，`initialized` 属性被设置为 `False`。这看起来可能有些奇怪，但这里的想法是，这段代码将初始化 *类*，但直到调用
    `initialize` 自定义函数之前，它将不会准备好使用。该函数将加载模型并使其准备好进行推理，然后我们将 `self.initialized` 设置为
    `True`。注意，模型被初始化为 `SimpleLinearModel`，所以你需要以与训练代码相同的方式导入它。
- en: 'Here’s the code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It begins by reading `ctx.manifest`—which is information in the *MAR* file (such
    as the model directory) that lets you use the model. We’ll see how to create that
    file shortly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先读取 `ctx.manifest`——这是在 *MAR* 文件（如模型目录）中的信息，它允许你使用模型。我们很快就会看到如何创建该文件。
- en: 'The rest of the code is pretty straightforward: we create an instance of the
    model (called `SimpleLinearModel`, as shown in the previous code) and load its
    weights from where they were saved in (*model.pth*). We can then push the model
    to the device, where we will do inference, such as the CPU or CUDA, if it’s available.
    Then, we’ll put the model into evaluation mode for inference. I have found that
    TorchServe works best if you save the model with its state dictionary, so be sure
    to load that back, as shown.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分相当简单：我们创建了一个模型实例（称为 `SimpleLinearModel`，如前述代码所示），并从保存的位置（*model.pth*）加载其权重。然后，我们可以将模型推送到设备，在那里我们将进行推理，例如
    CPU 或 CUDA（如果可用）。然后，我们将模型放入评估模式以进行推理。我发现，如果你使用状态字典保存模型，TorchServe 工作得最好，所以请确保以所示方式将其加载回来。
- en: At that point, we set `self.initialized` to `True`, and we’re good to go for
    inference!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将 `self.initialized` 设置为 `True`，然后我们就可以进行推理了！
- en: The next method we need to override is the *preprocess method*, which will take
    the input data and turn it into the format the model needs. There are many ways
    in which you could post the data to the backend server, and it’s in the preprocess
    function that you’d handle them. You could, for example, just take basic parameters,
    or you could allow your user to post a JSON file. It’s up to you, and this flexibility
    in the TorchServe architecture opens up those possibilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要覆盖的方法是 *preprocess 方法*，它将输入数据转换为模型所需格式。你可以以多种方式将数据发送到后端服务器，而在 preprocess
    函数中你会处理它们。例如，你可以只接受基本参数，或者允许用户发送 JSON 文件。这取决于你，而 TorchServe 架构中的这种灵活性为这些可能性打开了大门。
- en: 'For the sake of simplicity, I’m just going to take a basic parameter in this
    code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我将在代码中只使用一个基本参数：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the main purpose of this is not only to *get* the parameters
    but also to *reformat* them into the format the model needs. So, it gets the parameter
    from the request call and turns it into a single-dimension tensor, which it will
    then return. This tensor will be used in the inference method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个的主要目的不仅是要 *获取* 参数，还要将它们 *重新格式化* 成模型所需的格式。因此，它从请求调用中获取参数并将其转换为单维张量，然后将其返回。这个张量将在推理方法中使用。
- en: 'The next method to override is the inference method, and here’s where we will
    pass the data into the model and get its response back. Here’s the code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要覆盖的方法是推理方法，我们将在这里将数据传递到模型并获取其响应。以下是代码：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This receives the preprocessed data from the previous step, so we can just get
    the output by passing in this data. We run it with `torch.no_grad()` because we
    aren’t interested in backpropagation, just a straight inference.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这接收来自前一步的预处理数据，因此我们只需通过传递这些数据来获取输出。我们使用`torch.no_grad()`运行它，因为我们不感兴趣于反向传播，只进行直接推理。
- en: 'Finally, there’s postprocessing. The end user isn’t expecting a tensor back,
    but they are expecting more human-readable data, so we do the reverse of the preprocess
    step and cast the result back to a float by using NumPy:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是后处理。最终用户期望得到的是一个张量，但他们期望得到的是更易于阅读的数据，所以我们执行预处理步骤的反向操作，并使用NumPy将结果转换回浮点数：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These steps are, as you can see, very customized to this model, and I’ve deliberately
    made them bare-bones so you can see the flow—but the general architecture remains
    the same for other models, regardless of the complexity. The goal is to give you
    a standard way to approach the problem, and code that extends the `BaseHandler`
    like this makes it easy for you to take advantage of all the unseen aspects of
    the server infrastructure—not least, passing the data around!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤，如你所见，非常定制化，我故意使它们尽可能简单，以便你能看到流程——但其他模型，无论其复杂程度如何，其总体架构保持不变。目标是给你一个标准的方式来处理问题，并且像这样扩展`BaseHandler`的代码使你能够轻松利用服务器基础设施的所有未观察到的方面——最重要的是，传递数据！
- en: Creating the Model Archive
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型存档
- en: Starting with a trained model, you can create the archive file for it by calling
    `torch-model-archiver`, which is a command-line tool that’s provided as part of
    TorchServe.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个训练好的模型开始，你可以通过调用`torch-model-archiver`来创建它的存档文件，这是一个作为TorchServe一部分提供的命令行工具。
- en: 'There are a few things you need to note and be careful of here. Here’s the
    command, and I’ll discuss the parameters afterward:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你需要注意一些事情，并小心处理。以下是命令，之后我会讨论参数：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Before running it, make sure you have a *model-store* (or similar) directory
    that you will store the archived model in. Don’t mix it up with your source code!
    This can simply be a subdirectory, and you’ll specify that directory in the `export-path`
    parameter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，确保你有一个用于存储存档模型的`*model-store*`（或类似）目录。不要与你的源代码混淆！这可以简单地是一个子目录，你将在`export-path`参数中指定这个目录。
- en: The `model-name` parameter will specify the name of the model in the model store.
    You can call it whatever you want—it doesn’t have to be the class name. In this
    case, I called it `simple-linear`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`model-name`参数将指定模型存储中的模型名称。你可以称它为任何你想要的名字——它不必是类名。在这种情况下，我将其命名为`simple-linear`。'
- en: The `version` parameter can be whatever you want, and you’ll use it for tracking.
    As you train new versions of your model for new and different scenarios or bug
    fixes, you’ll likely want to keep track of which version does what. You can let
    the server know about that here.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`version`参数可以是任何你想要的内容，你将用它来跟踪。随着你为新的场景或错误修复训练模型的新版本，你可能会想要记录哪个版本做了什么。你可以在这里让服务器知道这一点。'
- en: When you trained the model, you saved the weights and the state dictionary out
    to a file. You specify this file with the `serialized-file` parameter. In this
    case, it’s `model.pth`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练模型时，你将权重和状态字典保存到文件中。你使用`serialized-file`参数指定这个文件。在这种情况下，它是`model.pth`。
- en: The handler file is specified with the `handler` parameter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器文件由`handler`参数指定。
- en: A common problem I have encountered occurs when the model training file contains
    the model definition *and* the handler also contains it. TorchServe gets confused
    as to where to get the model definition, and that’s why I separated it out in
    this case. But if you need to do that, it’s important to specify *where* the model
    definition is, and you can do that with the `model-file` parameter. Note that
    following this methodology, you should also point to the location of the model
    file by using the `extra-files` parameter. To make the model file importable to
    both the model training and model handler as a package, I put the file in a directory
    called *models* and put an empty file called *_ _init_ _.py* in there. To make
    sure that the model archiver uses these, *both* of these files are specified in
    the `extra-files` parameter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的一个常见问题是在模型训练文件中包含了模型定义 *以及* 处理器也包含它。TorchServe 对模型定义的位置感到困惑，这就是为什么我在这个例子中将它分离出来。但是，如果你需要这样做，重要的是要指定
    *模型定义的位置*，你可以通过 `model-file` 参数来实现。请注意，按照这种方法，你还应该使用 `extra-files` 参数指向模型文件的位置。为了使模型文件可以被模型训练和模型处理器作为一个包导入，我将文件放在一个名为
    *models* 的目录中，并在其中放置一个名为 *__init__.py* 的空文件。为了确保模型归档器使用这些文件，*这两个文件* 都在 `extra-files`
    参数中指定。
- en: The `force` parameter just overwrites any existing model archive in the *model-store*
    directory. When you’re learning, this parameter saves you from having to delete
    the model archive manually when trying different things. However, in a production
    system, you should use it carefully!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`force` 参数会覆盖 *model-store* 目录中任何现有的模型归档。在学习时，此参数可以让你在尝试不同的事情时不必手动删除模型归档。然而，在生产系统中，你应该谨慎使用它！'
- en: Once this line runs correctly, your *model-store* directory should contain a
    *simple_linear.mar* file.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这一行运行正确，你的 *model-store* 目录应该包含一个 *simple_linear.mar* 文件。
- en: Starting the Server
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动服务器
- en: 'Once you’ve created your archive, you can start TorchServe and have it load
    that model. Here’s an example command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了归档，你可以启动 TorchServe 并加载该模型。以下是一个示例命令：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `start` parameter instructs TorchServe to start. You can also use `--stop`
    in the same way to stop it from executing (and filling your terminal with a wall
    of text).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`start` 参数指示 TorchServe 启动。你还可以使用 `--stop` 以相同的方式停止它执行（并填充你的终端为文本墙）。'
- en: The `model-store` parameter should also point to the directory where you store
    the *.mar* file that you created earlier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`model-store` 参数也应该指向你存储之前创建的 *.mar* 文件的目录。'
- en: The `ts-config` parameter should point to the configuration properties file
    you created earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-config` 参数应该指向你之前创建的配置属性文件。'
- en: When you’re learning and testing, you can use `--disable-token-auth` so that
    commands you send to the server to test your models don’t need authentication.
    However, in proper production systems, you probably wouldn’t want to use it!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在学习和测试时，可以使用 `--disable-token-auth` 以便你发送到服务器的命令在测试你的模型时不需要进行身份验证。然而，在正式的生产系统中，你可能不会想使用它！
- en: The `models` parameter is a list of models that you want the server to make
    available to your users. In this case, there’ll just be one, and it’s the `simple_linear`
    model we defined. If you give the path to this model as the value, you’ll see
    that it’s the location of the *mar* file in the `model_store`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`models` 参数是你希望服务器向用户提供的模型列表。在这种情况下，只有一个，那就是我们定义的 `simple_linear` 模型。如果你将此模型的路径作为值提供，你会看到它就是
    `model_store` 中 *mar* 文件的位置。'
- en: If all goes well, you should then see TorchServe start in your terminal and
    give you a wall of status text, a little like in [Figure 13-2](#ch13_figure_2_1748549772551268).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会在终端看到 TorchServe 启动，并显示一堵状态文本墙，有点像 [图 13-2](#ch13_figure_2_1748549772551268)。
- en: Note that if this text is constantly scrolling, it’s likely that there was an
    error in starting up TorchServe. From experience, I would say that there are dependencies
    that TorchServe needs (like PyYAML) that haven’t been installed. If that’s the
    case, then the configuration file was set to debug, and you can inspect the *models_log.log*
    file in the *logs* directory to see what’s going on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果这段文本不断滚动，那么很可能是启动 TorchServe 时出现了错误。根据经验，我会说 TorchServe 需要一些依赖项（如 PyYAML）尚未安装。如果是这种情况，那么配置文件被设置为调试模式，你可以检查
    *logs* 目录中的 *models_log.log* 文件以查看发生了什么。
- en: You may also see errors like the one in [Figure 13-2](#ch13_figure_2_1748549772551268),
    where it can’t find the `nvgpu` module. This module is used by TorchServe to do
    GPU-based inference with an Nvidia GPU. Because I was running on a Mac in this
    case, you can safely ignore the error, and all inference will just run on the
    CPU, as per the code in the handler.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会看到像[图13-2](#ch13_figure_2_1748549772551268)中的错误，其中找不到`nvgpu`模块。这个模块被TorchServe用于在Nvidia
    GPU上进行基于GPU的推理。因为在这种情况下我正在Mac上运行，你可以安全地忽略这个错误，并且所有推理都将按照处理程序中的代码在CPU上运行。
- en: '![](assets/aiml_1302.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1302.png)'
- en: Figure 13-2\. Starting TorchServe
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. 启动TorchServe
- en: Testing Inference
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试推理
- en: Once the server is successfully up and running, you can test it by using `curl`
    from another terminal.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务器成功启动并运行，你可以通过另一个终端使用`curl`来测试它。
- en: 'So, to get an inference from the model, you can `curl` like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要从模型获取推理，你可以这样使用`curl`：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that it is an HTTP POST to the predictions endpoint. We specify the `simple_linear`
    model name as defined with the *.mar* file earlier, and we can then add the header
    (with the `-H` parameter) as plain text containing the data `5.0`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个对predictions端点的HTTP POST请求。我们指定了之前用*.mar*文件定义的`simple_linear`模型名称，然后我们可以添加一个头（使用`-H`参数），包含数据`5.0`的纯文本。
- en: As you may recall, the model learned the linear relationship *Y* = 2*x* – 1,
    so in this case, *x* will be 5 and the result will be a number close to 9.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆，模型学习到了线性关系 *Y* = 2*x* – 1，所以在这种情况下，*x*将是5，结果将是一个接近9的数字。
- en: 'The return should look like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 返回结果应如下所示：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Your value may vary, based on how your model was trained, but it should be a
    value very close to 9.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你的值可能会有所不同，这取决于你的模型是如何训练的，但它应该是一个非常接近9的值。
- en: 'You can also use the management endpoint to inspect the models that the server
    is hosting, like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用管理端点来检查服务器正在托管哪些模型，如下所示：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The response will contain the name and location of the *.mar* file for each
    model on the server:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 响应将包含服务器上每个模型的*.mar*文件的名称和位置：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that earlier, when you did inference on the model, you used the `predictions`
    endpoint followed by a model name, which in that case was `simple_linear`. This
    key should map to a model name in this models collection, or you would have gotten
    an error.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，之前当你对模型进行推理时，你使用了`predictions`端点后跟一个模型名称，在这种情况下是`simple_linear`。这个键应该映射到这个模型集合中的一个模型名称，否则你会得到一个错误。
- en: 'Finally, if you want to explore the details of a specific model, you can call
    the management URL (via port 8081, as earlier) with the model’s endpoint and the
    name of the model you want to know more about:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想探索特定模型的详细信息，你可以通过模型端点和想要了解更多信息的模型名称，调用管理URL（通过端口8081，如前所述）：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The server will return some detailed specs on the model, along with information
    you can use to help debug any issues. Here’s an example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将返回有关模型的详细规格，以及你可以用来帮助调试任何问题的信息。以下是一个示例：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice, for example, that the `deviceType` is expecting `gpu`. However, since
    I don’t have `nvgpu` on this system (see the earlier note about when you ran the
    server), it can’t load from the GPU, and the workers reported on that. It’s OK
    for that to be the case in my dev box, which doesn’t have the Nvidia GPU—but should
    you be running this on a server that *does* have the Nvidia GPU, that message
    is something you’d want to follow up on, and it’s likely an issue in your handler
    file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，例如，`deviceType`期望的是`gpu`。然而，由于我在这台系统上没有`nvgpu`（参见之前关于你运行服务器时的说明），它无法从GPU加载，并且工作者报告了这一点。在我的没有Nvidia
    GPU的开发箱中，这种情况是可以接受的——但是如果你在一个确实有Nvidia GPU的服务器上运行这个，那么这条消息是你需要跟进的，并且很可能是你的处理程序文件中的问题。
- en: Going Further
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步学习
- en: The foregoing was a bare-bones example to help you understand the nuts and bolts
    of how TorchServe works. As you use more sophisticated models that take in more
    complex data, the basic pattern you followed here should follow suit. In particular,
    the breakdown of preprocessing, inference, and postprocessing in the handler file
    is an enormous help! Additionally, the PyTorch ecosystem has add-ons for common
    scenarios to help you avoid having to write preprocessing code to begin with!
    For example, if you are interested in image classification and are worried about
    taking an image and turning it into tensors so that you can do inference on that
    image, the `ImageClassifier` class builds on the base handler to do this for you,
    and you can have image classification without needing to write a preprocessor.
    To see more of this in action, take a look at the open source examples at the
    PyTorch repository. In particular, you can go to [this GitHub page](https://oreil.ly/YA4v4)
    for an example of how to create a handler for MNIST images.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例是一个裸骨示例，旨在帮助你理解 TorchServe 的工作原理。随着你使用更复杂的模型，这些模型需要处理更复杂的数据，你应该遵循这里的基本模式。特别是，在处理文件中对预处理、推理和后处理的分解是一个巨大的帮助！此外，PyTorch
    生态系统为常见场景提供了附加组件，以帮助你避免一开始就必须编写预处理代码！例如，如果你对图像分类感兴趣，并且担心将图像转换为张量以便在该图像上进行推理，那么
    `ImageClassifier` 类基于基本处理程序为你完成这项工作，你可以在不编写预处理器的情况下进行图像分类。要了解更多关于这些功能的应用，请查看 PyTorch
    仓库中的开源示例。特别是，你可以访问 [这个 GitHub 页面](https://oreil.ly/YA4v4) 来查看如何为 MNIST 图像创建处理程序的示例。
- en: You’ll find many more useful examples in that repo, but I would still recommend
    going through the steps to get a bare-bones example like the one we showed here
    up and running first. There are a lot of steps and a lot of concepts, and it’s
    easy to get lost in the maze.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在那个仓库中找到更多有用的示例，但我仍然建议你先完成我们在这里展示的裸骨示例的设置。有很多步骤和概念，很容易迷失在迷宫中。
- en: Serving with Flask
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Flask 提供服务
- en: While TorchServe is extremely powerful, a great alternative that’s super easy
    to use is Flask. Flask is a lightweight and flexible web framework for Python
    that enables efficient development of web applications and APIs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TorchServe 非常强大，但 Flask 是一个超级容易使用的优秀替代品。Flask 是一个轻量级且灵活的 Python 网络框架，它使网络应用程序和
    API 的开发变得高效。
- en: You can use Flask to build everything from minimal single-endpoint services
    to complex web applications, starting with just a few lines of code. It perfectly
    complements PyTorch by giving you the ability to host models for inference, as
    we’ll explore in this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Flask 构建从最小单端点服务到复杂网络应用程序的一切，只需几行代码即可。它通过为你提供托管模型进行推理的能力，完美地补充了 PyTorch，正如我们将在本节中探讨的那样。
- en: As a microframework, Flask provides the essential components for web development—routing,
    request handling, and templating—while allowing you to select additional functionality
    as needed. It is highly extensible, with stuff like a backend database, authentication,
    etc., and there’s also a vibrant ecosystem of extensions available to use off-the-shelf.
    Because of all of this, Flask has become a standard tool in web development, powering
    applications across industries at all scales.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为微框架，Flask 提供了网络开发的基本组件——路由、请求处理和模板化——同时允许你根据需要选择额外的功能。它高度可扩展，包括后端数据库、身份验证等功能，并且还有一个充满活力的扩展生态系统，你可以直接使用。正因为如此，Flask
    已经成为网络开发的标准工具，为各个规模和行业的应用程序提供动力。
- en: In this chapter, we’ll just explore Flask from the hosting model’s perspective,
    but I’d encourage you to dig deeper into the framework if you’re interested in
    serving Python code—not just PyTorch!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将仅从托管模型的视角探索 Flask，但我鼓励你对框架进行更深入的研究，如果你对服务 Python 代码（而不仅仅是 PyTorch）感兴趣的话！
- en: Now, let’s take a look at the same example that we used for TorchServe. This
    will help you see how simple Flask makes serving applications!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们之前在 TorchServe 中使用的相同示例。这将帮助你看到 Flask 如何使服务应用程序变得简单！
- en: Creating an Environment for Flask
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Flask 创建环境
- en: 'First, to use Flask, you’ll need to install it and any dependencies. If you’ve
    been using the same environment as earlier in this chapter, you can simply update
    it with this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，要使用 Flask，你需要安装它及其依赖项。如果你一直在使用本章早期相同的环境，你可以简单地使用以下命令更新它：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then, just ensure that you have a model definition and training file—exactly
    the same as those from earlier in this chapter—and that you have trained a model
    and saved it with its state dictionary as a file called *model.pth*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，确保你有一个模型定义和训练文件——与本章前面完全相同——并且你已经训练了一个模型，并以名为*model.pth*的文件保存了其状态字典。
- en: With those in hand, all you’ll need is a single Python file that I’m going to
    call *app.py*, which will be the primary server application that Flask will use.
    We’ll explore the code in that file next.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些后，你只需要一个名为*app.py*的单个Python文件，这是Flask将使用的首要服务器应用。我们将在下一节中探讨该文件中的代码。
- en: Creating a Flask Server in Python
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中创建Flask服务器
- en: 'To create a server with Flask, you implement an app that creates a new Flask
    instance by using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Flask创建服务器，你需要实现一个应用，通过以下代码创建一个新的Flask实例：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, on the app object, you can specify routes, such as `predict` for doing
    prediction on models. To serve, you implement the code for the endpoints at these
    routes. Here’s an example of a full Flask server for our simple app:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在应用对象上，你可以指定路由，例如`predict`，用于在模型上进行预测。为了提供服务，你需要在这些路由上实现端点的代码。以下是我们简单应用的一个完整Flask服务器示例：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Flask documentation and samples tend to use port 5000\. If you’re using a Mac
    as your development box, you might have issues with this as it conflicts with
    the port used by Airplay. To that end, I’ve used 5001 in this sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Flask文档和示例通常使用端口5000。如果你使用Mac作为你的开发环境，你可能会遇到问题，因为这与Airplay使用的端口冲突。因此，我在这个示例中使用了5001。
- en: In this case, we declare the `SimpleLinearModel` and load it along with its
    state dictionary. Then, we put it into `eval()` mode to get it ready for inference.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们声明`SimpleLinearModel`并加载它及其状态字典。然后，我们将它放入`eval()`模式，以便为推理做好准备。
- en: Then, it becomes as simple as creating a route that we call `predict` and then
    implementing the inference within it. As you can see, we handle getting the `value`
    from the HTTP `POST`, converting it into a tensor, and getting the prediction
    back when we pass that tensor to the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它变得非常简单，只需创建一个名为`predict`的路由，并在其中实现推理。正如你所见，我们处理从HTTP `POST`获取`value`，将其转换为张量，并将该张量传递给模型以获取预测。
- en: To make the return a little friendlier, I used `jsonify` to turn it into a name-value
    pair. As you can see, that’s much simpler than using TorchServe, but for that
    simplicity, you give up power. There’s no set of base classes to handle preprocessing,
    post-processing, etc., and if you want to scale or implement multiple worker threads,
    you’ll have to do it yourself.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使返回值更友好，我使用了`jsonify`将其转换为键值对。正如你所见，这比使用TorchServe简单得多，但为了这种简单性，你放弃了功能。没有一组基类来处理预处理、后处理等，如果你想进行扩展或实现多个工作线程，你必须自己完成。
- en: I think this is a really useful and powerful server mechanism for smaller-scale
    environments, as well as for learning how to serve. For large-scale production
    environments, it’s a lot of extra work, but it can definitely handle the load.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这对于小规模环境以及学习如何提供服务来说是一个非常实用且强大的服务器机制。对于大规模生产环境，它需要做很多额外的工作，但确实可以处理负载。
- en: 'For inference, you can `curl` a POST to the model like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，你可以像这样向模型发送一个POST请求：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And the response will be the JSON payload:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 响应将是JSON有效载荷：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In addition to TorchServe and Flask, there are many other serving options, such
    as ONNX and FastAPI.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了TorchServe和Flask，还有许多其他的服务选项，例如ONNX和FastAPI。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored two popular approaches to serving PyTorch models
    in production environments. You started with TorchServe, which is PyTorch’s official
    serving solution that offers a robust framework with built-in support for model
    versioning, A/B testing, and metrics collection. It’s also designed to be highly
    scalable at runtime, with a worker-thread architecture that’s configurable based
    on the needs of your app. And while TorchServe requires more setup and understanding
    of its components like model workers and frontend handlers, I think it’s worth
    investing the time in rolling up your sleeves and understanding all the different
    components and how they work together. To that end, you explored step-by-step
    how to take the simple linear model example from [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566),
    save it, archive it, build a handler for it, and launch the server with the model
    details.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两种在生产环境中部署 PyTorch 模型的流行方法。你从 TorchServe 开始，这是 PyTorch 的官方服务解决方案，它提供了一个强大的框架，内置了对模型版本控制、A/B
    测试和指标收集的支持。它还设计为在运行时具有高度的可扩展性，具有可配置的工作线程架构，这取决于你应用程序的需求。虽然 TorchServe 需要更多的设置和对其组件（如模型工作者和前端处理器）的理解，但我认为投入时间去深入了解所有不同的组件以及它们是如何协同工作的是值得的。为此，你逐步探索了如何从[第
    1 章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中的简单线性模型示例开始，保存它、归档它，为它构建处理器，并使用模型详细信息启动服务器。
- en: Then, you explored how to use Flask as a lightweight alternative that’s extremely
    quick and simple to get up and running. You saw how its minimalist approach makes
    it ideal for smaller-scale deployments or proof-of-concept services. It’s not
    limited to those, but as you move to production scale, you’ll likely need to implement
    more code. Of course, that’s not necessarily a disadvantage, as it gives you more
    granular control over your serving environment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你探索了如何使用 Flask 作为一种轻量级的替代方案，它启动起来非常快且简单。你看到了它的简约方法使其非常适合小规模部署或概念验证服务。它不仅限于这些，但随着你转向生产规模，你可能会需要编写更多的代码。当然，这并不一定是缺点，因为它让你对你的服务环境有更细粒度的控制。
- en: Both approaches have their place in the ML serving ecosystem. TorchServe shines
    in enterprise environments requiring comprehensive model management, while Flask’s
    simplicity makes it perfect for smaller projects or learning environments with
    a smooth glide path toward scalable production. Of course, you aren’t limited
    to just these two, and new frameworks are coming online all the time—in particular,
    one called FastAPI, which is rapidly growing in popularity. Which one you should
    choose ultimately depends on your specific needs around scaling, monitoring, and
    deployment complexity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都在机器学习服务生态系统中有其位置。TorchServe 在需要全面模型管理的企业环境中表现出色，而 Flask 的简洁性使其非常适合小型项目或学习环境，这些环境可以平滑地过渡到可扩展的生产。当然，你并不局限于这两种方法，而且新的框架不断上线——特别是名为
    FastAPI 的框架，它正迅速增长其受欢迎程度。你应该选择哪一个最终取决于你关于扩展、监控和部署复杂性的具体需求。
- en: Next, in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    you’re going to look at third-party models that have been pretrained for you and
    various registries and hubs that you can load them from.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)中，你将了解为你预训练的第三方模型以及你可以从中加载它们的各种注册表和中心。
