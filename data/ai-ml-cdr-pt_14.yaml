- en: Chapter 13\. Hosting PyTorch Models for Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章\. 为服务托管PyTorch模型
- en: In the earlier chapters of this book, we looked at many scenarios for training
    ML models, including those for computer vision, NLP, and sequence modeling. But
    that was just the first step—a model is of little use without a way for other
    people to use its power! In this chapter, we’ll take a brief tour of some of the
    more popular tools that allow you to give them a way to do that.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们探讨了训练机器学习模型的各种场景，包括计算机视觉、自然语言处理和序列建模。但那只是第一步——如果没有其他方式让其他人使用其力量，模型几乎毫无用处！在本章中，我们将简要浏览一些更受欢迎的工具，这些工具可以帮助你为它们提供使用其功能的方法。
- en: You should note that taking a trained PyTorch model to a production-ready service
    will involve a lot more than just deploying it, and that the machine learning
    operations (MLOps) discipline is designed with that in mind. When you get into
    the world of serving these models, you’ll need to understand new challenges, such
    as handling real-time requests, managing computational resources, ensuring reliability,
    and maintaining performance under varying loads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意，将训练好的PyTorch模型部署到生产就绪的服务中，将涉及比仅仅部署它多得多的工作，并且机器学习操作（MLOps）学科正是为此而设计的。当你进入这些模型的服务领域时，你需要了解新的挑战，例如处理实时请求、管理计算资源、确保可靠性和在变化的工作负载下维持性能。
- en: Ultimately, MLOps is about bridging the gap between data science and software
    engineering. That’s beyond the scope of this chapter, but there are some great
    books about it out there from O’Reilly, including [*Implementing MLOps in the
    Enterprise*](https://learning.oreilly.com/library/view/implementing-mlops-in/9781098136574)by
    Yaron Haviv and Noah Gift and [*LLMOps*](https://learning.oreilly.com/library/view/llmops/9781098154196)
    by Abi Aryan.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，MLOps是关于弥合数据科学与软件工程之间的差距。这超出了本章的范围，但市面上有一些关于这个主题的出色书籍，来自O'Reilly出版社，包括由Yaron
    Haviv和Noah Gift所著的[*在企业中实施MLOps*](https://learning.oreilly.com/library/view/implementing-mlops-in/9781098136574)和由Abi
    Aryan所著的[*LLMOps*](https://learning.oreilly.com/library/view/llmops/9781098154196)。
- en: This chapter will introduce two popular approaches to serving PyTorch models
    in production environments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍两种在生产环境中服务PyTorch模型的热门方法。
- en: We’ll begin with TorchServe, the official serving solution from PyTorch, which
    provides a robust framework that’s designed specifically for serving deep-learning
    models at scale. TorchServe offers out-of-the-box solutions for standard serving
    requirements like model versioning, A/B testing, and metrics collection. It’s
    also an excellent choice for teams who are looking for a production-ready solution
    with minimal setup.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从TorchServe开始，这是PyTorch的官方服务解决方案，它提供了一个专为大规模服务深度学习模型而设计的强大框架。TorchServe为标准服务需求提供了现成的解决方案，如模型版本控制、A/B测试和指标收集。它也是寻找具有最小设置即可投入生产的解决方案的团队的绝佳选择。
- en: Then, we’ll explore how to build serving solutions using the popular Flask framework,
    which is for developers who need more flexibility or have more straightforward
    serving requirements. Flask’s simplicity and extensive ecosystem also make it
    an excellent choice for smaller-scale deployments and proof-of-concept services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨如何使用流行的Flask框架构建服务解决方案，这对于需要更多灵活性或具有更直接服务需求的开发者来说是一个很好的选择。Flask的简单性和广泛的生态系统也使其成为小型部署和概念验证服务的绝佳选择。
- en: As you work through the chapter, you’ll take a hands-on approach in which you’ll
    take some of the models you created in earlier chapters, let us walk you through
    how to deploy them, and then call their hosting servers for inference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成本章的过程中，你将采取一种动手实践的方法，其中你将使用在前几章中创建的一些模型，让我们带你了解如何部署它们，然后调用它们的托管服务器进行推理。
- en: Introducing TorchServe
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍TorchServe
- en: '*TorchServe* is PyTorch’s default serving framework that’s designed for performance
    and flexibility. You can find it at [*pytorch.org/serve*](http://pytorch.org/serve).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*TorchServe*是PyTorch的默认服务框架，旨在提供性能和灵活性。你可以在[*pytorch.org/serve*](http://pytorch.org/serve)找到它。'
- en: TorchServe’s goal was originally to be a reference implementation on how to
    properly serve models with a modular extensible architecture, but it has grown
    beyond that into a fully performant professional-grade framework that is likely
    more than enough for any serving needs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe的最初目标是成为一个关于如何以模块化和可扩展的架构正确服务模型的参考实现，但它已经发展成为一个功能齐全、专业级的框架，可能足以满足任何服务需求。
- en: 'It’s also built on a modular architecture with the aim to handle the complexity
    of serving models at scale. To this end, it’s built from the following key components:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它还基于模块化架构，旨在处理大规模服务模型的复杂性。为此，它由以下关键组件构建：
- en: The model server
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器
- en: The PyTorch model server is the central component that handles the lifecycle
    of models and handles all inference requests. It provides the endpoints for model
    management and inference, supporting REST and gRPC calls.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型服务器是处理模型生命周期并处理所有推理请求的中心组件。它提供模型管理和推理的端点，支持 REST 和 gRPC 调用。
- en: Model workers
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工作者
- en: These are independent processes that load models and perform inference on them.
    Each one is isolated, so in a multimodel serving environment, they are designed
    to continue operating should issues in one model arise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是独立的过程，它们加载模型并在其上进行推理。每个过程都是隔离的，因此在多模型服务环境中，它们被设计为在某个模型出现问题的情况下继续运行。
- en: Frontend handlers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前端处理器
- en: These are custom Python classes that handle preprocessing, inference, and post-processing
    for specific model types. As you’ll see in a moment, when we get hands-on, frontend
    handlers are complementary to the training code for the model, and it’s good practice
    to create separate handler classes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是处理特定模型类型的前处理、推理和后处理的自定义 Python 类。正如你将看到的，当我们实际操作时，前端处理器是对模型训练代码的补充，创建单独的处理器类是良好的实践。
- en: The model store
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型存储
- en: Model serving in PyTorch uses “model archives,” or MAR files, for the servable
    object. Once you’ve trained your model, you’ll convert it into this format. The
    model store is where these are kept.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的模型服务使用“模型存档”或 MAR 文件作为可服务对象。一旦你训练了你的模型，你将将其转换为这种格式。模型存储就是这些文件存放的地方。
- en: You can see the architecture diagram for a TorchServe system in [Figure 13-1](#ch13_figure_1_1748549772551219).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 13-1](#ch13_figure_1_1748549772551219)中看到 TorchServe 系统的架构图。
- en: '![](assets/aiml_1301.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1301.png)'
- en: Figure 13-1\. The TorchServe server infrastructure
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. TorchServe 服务器基础设施
- en: For inference, the client application will call the server infrastructure over
    REST (via default port 8080) or gRPC (via default port 7070). It also provides
    management endpoints at 8081 and 8082, respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，客户端应用程序将通过 REST（通过默认端口 8080）或 gRPC（通过默认端口 7070）调用服务器基础设施。它还分别在 8081 和 8082
    提供管理端点。
- en: The endpoint will then call the core model server, which in turn will spawn
    the appropriate number of model workers. The workers will then interface with
    the model handlers to do preprocessing, inference, and postprocessing. The model
    itself will be in the model store, and should it not be in memory, the model server
    will use the request queue to load it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 端点将调用核心模型服务器，然后服务器将启动适当数量的模型工作者。工作者将与模型处理器接口进行预处理、推理和后处理。模型本身将存储在模型存储中，如果它不在内存中，模型服务器将使用请求队列来加载它。
- en: In the next section, we’ll explore setting up a TorchServe server and using
    it to provide inference for the first model you created in [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何设置 TorchServe 服务器并使用它为你在[第 1 章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中创建的第一个模型提供推理。
- en: Setting Up TorchServe
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 TorchServe
- en: I find it’s easiest to learn something if I go through it step-by-step with
    a simple but representative scenario. So, for TorchServe, we’ll install the environment
    first and work from there.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，如果我能通过一个简单但具有代表性的场景一步一步地学习，那么学习某样东西会更容易。因此，对于 TorchServe，我们首先安装环境，然后从这里开始工作。
- en: Preparing Your Environment
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备你的环境
- en: I strongly recommend using a virtual environment, and I’ll be stepping through
    this chapter using venv, which is a freely available one from the Python community
    that you can find [in the Python documentation](https://oreil.ly/FNsPt).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议使用虚拟环境，我将在本章中使用 venv，这是 Python 社区免费提供的，你可以在[Python 文档](https://oreil.ly/FNsPt)中找到它。
- en: Even if you’ve used virtual environments before, I’d recommend starting with
    a clean one to ensure you are going to install and use the right set of dependencies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你之前使用过虚拟环境，我也建议从一个新的环境开始，以确保你将安装和使用正确的依赖项集合。
- en: 'You can create a virtual environment like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建一个虚拟环境，如下所示：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And once you’ve done that, you can start it with this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了这些，你可以用以下方式启动它：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, you’ll be ready to install PyTorch. To get started, I recommend installing
    TorchServe, the model archiver, and the workflow archiver like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您就准备好安装 PyTorch 了。为了开始，我建议安装 TorchServe、模型归档器和工作流程归档器，如下所示：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You aren’t limited to these, and you’ll often need to install other dependencies.
    One of the difficulties of working with TorchServe is that errors may be buried
    in log files, so it’s hard to figure out which dependencies you’ll also need.
    At a minimum, I’ve found that starting from a clean system, I also needed to install
    a JDK after version 11 and PyYAML. Your mileage may vary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您不仅限于这些，您通常还需要安装其他依赖项。使用 TorchServe 的工作中的一个困难是错误可能隐藏在日志文件中，因此很难确定您还需要哪些依赖项。至少，我发现从干净的系统开始，在版本
    11 之后，我还需要安装 JDK 和 PyYAML。您的体验可能会有所不同。
- en: 'Once you’ve done this, you can change to the directory where you want to work,
    and within that, you can create a subdirectory called `model_store`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些操作后，您可以切换到您想要工作的目录，并在其中创建一个名为 `model_store` 的子目录：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With that in place, the first thing you’ll need to do is set up the configuration
    file for your PyTorch server. It will be a file called *config.properties*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，您需要做的第一件事是为您的 PyTorch 服务器设置配置文件。它将是一个名为 *config.properties* 的文件。
- en: Setting Up Your config.properties File
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置您的 config.properties 文件
- en: 'There’s a lot going on in this file, and you can learn more about it on the
    [PyTorch site](https://oreil.ly/czQh5). But the important settings are the inference
    and management addresses, which are set to 0.0.0.0 and 8080/8081, respectively
    (similar to what’s shown in [Figure 13-1](#ch13_figure_1_1748549772551219)). Also,
    it’s important to set the directory of the model store to be the one you just
    created. I’ve also set it to log debug messages to help catch dependency issues:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件中有很多内容，您可以在 [PyTorch 网站上](https://oreil.ly/czQh5)了解更多信息。但重要的设置是推理和管理地址，分别设置为
    0.0.0.0 和 8080/8081（类似于 [图 13-1](#ch13_figure_1_1748549772551219) 中所示）。此外，将模型存储目录设置为刚刚创建的目录也很重要。我还将其设置为将调试信息记录到日志中，以帮助捕获依赖性问题：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure these settings are in a file with the name *config.properties*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些设置在一个名为 *config.properties* 的文件中。
- en: Defining Your Model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义您的模型
- en: 'In [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566), you
    created a model that learned the relationship between two sets of numbers, when
    the equation describing this relationship was *y* = 2*x* – 1\. When you’re getting
    the model ready for serving, it’s best practice to have a separate file for the
    model definition and the model training. The model definition file will then be
    used in another Python file called the *handler*, which you’ll see momentarily.
    So, to that end, you should create a model definition file, like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 1 章](ch01.html#ch01_introduction_to_pytorch_1748548870019566) 中，您创建了一个模型，该模型学习了两组数字之间的关系，当描述这种关系的方程为
    *y* = 2*x* – 1 时。当您为服务准备模型时，将模型定义和模型训练分开到不同的文件中是一种最佳实践。然后，模型定义文件将用于另一个名为 *handler*
    的 Python 文件中，您将很快看到。因此，为此目的，您应该创建一个模型定义文件，如下所示：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, save this into a file called *linear.py* so that you can then load and
    train it with code like the following. Also note the `import` for `SimpleLinearModel`
    that is bolded:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将此保存到名为 *linear.py* 的文件中，这样您就可以使用以下代码加载和训练它。请注意，`import` `SimpleLinearModel`
    被加粗了：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In particular, note the last line, where, after training the model, it’s saved
    as a *model.pth* file, along with this piece of code: `mode.state_dic()`, which
    saves out the *state dictionary* (aka the current state of the trained model).
    I find that this way of saving out a model works best with TorchServe.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 特别注意最后一行，在训练模型后，它被保存为 *model.pth* 文件，以及这段代码：`mode.state_dic()`，它保存了 *状态字典*（即训练模型的当前状态）。我发现这种保存模型的方式与
    TorchServe 一起使用效果最佳。
- en: Then, you run this code to get the file. You’ll use this later to create a *.mar*
    file that goes in the model store. We’ll look at that shortly, but first, we’ll
    need a handler file. You’ll explore that next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行此代码以获取文件。您将稍后使用此文件创建一个放入模型存储的 *.mar* 文件。我们稍后会看到这一点，但首先，我们需要一个处理文件。您将在下一节中探索它。
- en: Creating the Handler File
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建处理文件
- en: Once you’ve trained and saved the model, you’ll need to create the handler file.
    It’s the job of the handler to do the heavy lifting of serving inference—it loads
    your model, handles data preprocessing, does the inference, and handles any postprocessing
    to turn the inferred values back into data your users may want.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练并保存了模型，你需要创建处理文件。处理器的任务是执行推理服务中的繁重工作——它加载你的模型，处理数据预处理，执行推理，并处理任何后处理，将推断值转换回用户可能需要的格式。
- en: 'This file should inherit from the `base_handler` torch class like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件应继承自`base_handler` torch类，如下所示：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you’ve done this, you’ll need to create a model handler class that overrides
    this base class and implements a number of methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，你需要创建一个模型处理器类，该类覆盖此基类并实现多个方法。
- en: 'Let’s start with the class declaration and initialization. It’s pretty straightforward:
    just reporting on class initialization:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从类声明和初始化开始。这相当直接：只是报告类初始化：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that an `initialized` property is set to `False` in this code. That may
    seem odd, but the idea here is that this code will initialize the *class* but
    it won’t be ready to use until the `initialize` custom function is called. That
    function will then load the model and get it ready for inference, and at that
    point, we will set `self.initialized` to be `True`. Note that the model is initialized
    as a `SimpleLinearModel`, so you’ll need to import it in the same way as you did
    the training code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在此代码中，`initialized`属性被设置为`False`。这看起来可能有些奇怪，但这里的想法是，此代码将初始化*类*，但直到调用`initialize`自定义函数之前，它将不会准备好使用。该函数将加载模型并使其准备好进行推理，此时我们将`self.initialized`设置为`True`。请注意，模型被初始化为`SimpleLinearModel`，因此你需要以与训练代码相同的方式导入它。
- en: 'Here’s the code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It begins by reading `ctx.manifest`—which is information in the *MAR* file (such
    as the model directory) that lets you use the model. We’ll see how to create that
    file shortly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先读取`ctx.manifest`——这是*MAR*文件中的信息（如模型目录），它允许你使用模型。我们将在稍后看到如何创建该文件。
- en: 'The rest of the code is pretty straightforward: we create an instance of the
    model (called `SimpleLinearModel`, as shown in the previous code) and load its
    weights from where they were saved in (*model.pth*). We can then push the model
    to the device, where we will do inference, such as the CPU or CUDA, if it’s available.
    Then, we’ll put the model into evaluation mode for inference. I have found that
    TorchServe works best if you save the model with its state dictionary, so be sure
    to load that back, as shown.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分相当直接：我们创建了一个模型实例（称为`SimpleLinearModel`，如前述代码所示），并从保存的位置（*model.pth*）加载其权重。然后，我们可以将模型推送到设备，在那里我们将进行推理，例如CPU或CUDA（如果可用）。然后，我们将模型置于评估模式以进行推理。我发现，如果你使用状态字典保存模型，TorchServe将运行得最好，所以请确保按如下所示加载它。
- en: At that point, we set `self.initialized` to `True`, and we’re good to go for
    inference!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将`self.initialized`设置为`True`，我们就准备好进行推理了！
- en: The next method we need to override is the *preprocess method*, which will take
    the input data and turn it into the format the model needs. There are many ways
    in which you could post the data to the backend server, and it’s in the preprocess
    function that you’d handle them. You could, for example, just take basic parameters,
    or you could allow your user to post a JSON file. It’s up to you, and this flexibility
    in the TorchServe architecture opens up those possibilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要覆盖的下一个方法是*预处理方法*，它将接收输入数据并将其转换为模型所需格式。你可以通过多种方式将数据发送到后端服务器，而在预处理函数中你会处理这些数据。例如，你可以只接受基本参数，或者允许用户发送一个JSON文件。这取决于你，而TorchServe架构中的这种灵活性为这些可能性打开了大门。
- en: 'For the sake of simplicity, I’m just going to take a basic parameter in this
    code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我将在代码中只接受一个基本参数：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the main purpose of this is not only to *get* the parameters
    but also to *reformat* them into the format the model needs. So, it gets the parameter
    from the request call and turns it into a single-dimension tensor, which it will
    then return. This tensor will be used in the inference method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个的主要目的不仅是要*获取*参数，还要将它们*重新格式化*成模型需要的格式。因此，它从请求调用中获取参数并将其转换为单维张量，然后将其返回。这个张量将在推理方法中使用。
- en: 'The next method to override is the inference method, and here’s where we will
    pass the data into the model and get its response back. Here’s the code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要覆盖的方法是推理方法，在这里我们将数据传递给模型并获取其响应。以下是代码：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This receives the preprocessed data from the previous step, so we can just get
    the output by passing in this data. We run it with `torch.no_grad()` because we
    aren’t interested in backpropagation, just a straight inference.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步接收来自前一步的预处理数据，因此我们只需传入这些数据就可以得到输出。我们使用`torch.no_grad()`来运行它，因为我们不感兴趣于反向传播，只进行直接推理。
- en: 'Finally, there’s postprocessing. The end user isn’t expecting a tensor back,
    but they are expecting more human-readable data, so we do the reverse of the preprocess
    step and cast the result back to a float by using NumPy:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是后处理。最终用户不期望得到一个张量，但他们期望得到更多可读性强的数据，所以我们执行预处理步骤的反向操作，并使用NumPy将结果转换回浮点数：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These steps are, as you can see, very customized to this model, and I’ve deliberately
    made them bare-bones so you can see the flow—but the general architecture remains
    the same for other models, regardless of the complexity. The goal is to give you
    a standard way to approach the problem, and code that extends the `BaseHandler`
    like this makes it easy for you to take advantage of all the unseen aspects of
    the server infrastructure—not least, passing the data around!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤，正如你所见，非常定制化地针对这个模型，我故意使它们尽可能简单，以便你能看到流程——但其他模型的一般架构仍然保持不变，无论其复杂程度如何。目标是给你提供一个标准的方式来解决问题，并且像这样扩展`BaseHandler`的代码使你能够轻松利用服务器基础设施的所有未观察到的方面——尤其是，传递数据！
- en: Creating the Model Archive
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型存档
- en: Starting with a trained model, you can create the archive file for it by calling
    `torch-model-archiver`, which is a command-line tool that’s provided as part of
    TorchServe.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个训练好的模型开始，你可以通过调用`torch-model-archiver`来创建它的存档文件，这是一个作为TorchServe一部分提供的命令行工具。
- en: 'There are a few things you need to note and be careful of here. Here’s the
    command, and I’ll discuss the parameters afterward:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你需要注意几点并小心处理。以下是命令，之后我会讨论参数：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Before running it, make sure you have a *model-store* (or similar) directory
    that you will store the archived model in. Don’t mix it up with your source code!
    This can simply be a subdirectory, and you’ll specify that directory in the `export-path`
    parameter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，确保你有一个用于存储存档模型的`*model-store*`（或类似）目录。不要与你的源代码混淆！这可以简单地是一个子目录，你将在`export-path`参数中指定该目录。
- en: The `model-name` parameter will specify the name of the model in the model store.
    You can call it whatever you want—it doesn’t have to be the class name. In this
    case, I called it `simple-linear`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`model-name`参数将指定模型存储中的模型名称。你可以称它为任何你想要的名字——它不必是类名。在这个例子中，我称它为`simple-linear`。'
- en: The `version` parameter can be whatever you want, and you’ll use it for tracking.
    As you train new versions of your model for new and different scenarios or bug
    fixes, you’ll likely want to keep track of which version does what. You can let
    the server know about that here.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`version`参数可以是任何你想要的内容，你将用它来跟踪。当你为新的场景或错误修复训练模型的新版本时，你可能会想要记录哪个版本做了什么。你可以在这里让服务器知道这一点。'
- en: When you trained the model, you saved the weights and the state dictionary out
    to a file. You specify this file with the `serialized-file` parameter. In this
    case, it’s `model.pth`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练模型时，你将权重和状态字典保存到了一个文件中。你使用`serialized-file`参数指定这个文件。在这个例子中，它是`model.pth`。
- en: The handler file is specified with the `handler` parameter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`handler`参数指定处理文件。
- en: A common problem I have encountered occurs when the model training file contains
    the model definition *and* the handler also contains it. TorchServe gets confused
    as to where to get the model definition, and that’s why I separated it out in
    this case. But if you need to do that, it’s important to specify *where* the model
    definition is, and you can do that with the `model-file` parameter. Note that
    following this methodology, you should also point to the location of the model
    file by using the `extra-files` parameter. To make the model file importable to
    both the model training and model handler as a package, I put the file in a directory
    called *models* and put an empty file called *_ _init_ _.py* in there. To make
    sure that the model archiver uses these, *both* of these files are specified in
    the `extra-files` parameter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的一个常见问题是模型训练文件包含模型定义，并且处理器也包含它。TorchServe对此模型定义的位置感到困惑，这就是为什么我在这个例子中将它分离出来。但如果你需要这样做，重要的是要指定模型定义的位置，你可以通过`model-file`参数来实现。请注意，遵循此方法时，你还应该使用`extra-files`参数指向模型文件的位置。为了使模型文件可以被模型训练和模型处理器作为包导入，我将文件放在一个名为`models`的目录中，并在其中放置一个名为`__init__.py`的空文件。为了确保模型存档器使用这些文件，`extra-files`参数中指定了这两个文件。
- en: The `force` parameter just overwrites any existing model archive in the *model-store*
    directory. When you’re learning, this parameter saves you from having to delete
    the model archive manually when trying different things. However, in a production
    system, you should use it carefully!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`force`参数只是覆盖`model-store`目录中任何现有的模型存档。当你学习时，此参数可以让你在尝试不同的事情时不必手动删除模型存档。然而，在生产系统中，你应该小心使用它！'
- en: Once this line runs correctly, your *model-store* directory should contain a
    *simple_linear.mar* file.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这一行运行正确，你的`model-store`目录应该包含一个`simple_linear.mar`文件。
- en: Starting the Server
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动服务器
- en: 'Once you’ve created your archive, you can start TorchServe and have it load
    that model. Here’s an example command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了存档，你可以启动TorchServe并让它加载该模型。以下是一个示例命令：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `start` parameter instructs TorchServe to start. You can also use `--stop`
    in the same way to stop it from executing (and filling your terminal with a wall
    of text).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`start`参数指示TorchServe启动。你也可以用`--stop`以同样的方式停止它执行（并让你的终端充满文本）。'
- en: The `model-store` parameter should also point to the directory where you store
    the *.mar* file that you created earlier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`model-store`参数也应该指向你存储之前创建的*.mar*文件的目录。'
- en: The `ts-config` parameter should point to the configuration properties file
    you created earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-config`参数应该指向你之前创建的配置属性文件。'
- en: When you’re learning and testing, you can use `--disable-token-auth` so that
    commands you send to the server to test your models don’t need authentication.
    However, in proper production systems, you probably wouldn’t want to use it!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在学习和测试时，你可以使用`--disable-token-auth`，这样你发送给服务器的测试模型命令就不需要认证。然而，在正式的生产系统中，你可能不想使用它！
- en: The `models` parameter is a list of models that you want the server to make
    available to your users. In this case, there’ll just be one, and it’s the `simple_linear`
    model we defined. If you give the path to this model as the value, you’ll see
    that it’s the location of the *mar* file in the `model_store`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`models`参数是你希望服务器向用户提供的模型列表。在这种情况下，只有一个，就是我们定义的`simple_linear`模型。如果你将此模型的路径作为值提供，你会看到它在`model_store`中的`mar`文件位置。'
- en: If all goes well, you should then see TorchServe start in your terminal and
    give you a wall of status text, a little like in [Figure 13-2](#ch13_figure_2_1748549772551268).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会在你的终端中看到TorchServe启动，并给你一堵状态文本墙，有点像[图13-2](#ch13_figure_2_1748549772551268)。
- en: Note that if this text is constantly scrolling, it’s likely that there was an
    error in starting up TorchServe. From experience, I would say that there are dependencies
    that TorchServe needs (like PyYAML) that haven’t been installed. If that’s the
    case, then the configuration file was set to debug, and you can inspect the *models_log.log*
    file in the *logs* directory to see what’s going on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果这个文本不断滚动，那么很可能在启动TorchServe时出现了错误。根据经验，我会说TorchServe需要一些依赖（如PyYAML）尚未安装。如果是这种情况，那么配置文件被设置为调试模式，你可以检查`logs`目录中的`models_log.log`文件以了解发生了什么。
- en: You may also see errors like the one in [Figure 13-2](#ch13_figure_2_1748549772551268),
    where it can’t find the `nvgpu` module. This module is used by TorchServe to do
    GPU-based inference with an Nvidia GPU. Because I was running on a Mac in this
    case, you can safely ignore the error, and all inference will just run on the
    CPU, as per the code in the handler.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还会看到类似于 [图 13-2](#ch13_figure_2_1748549772551268) 中的错误，其中找不到 `nvgpu` 模块。此模块由
    TorchServe 用于使用 Nvidia GPU 进行基于 GPU 的推理。由于在这种情况下我在 Mac 上运行，您可以安全地忽略此错误，并且所有推理都将按照处理程序中的代码在
    CPU 上运行。
- en: '![](assets/aiml_1302.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1302.png)'
- en: Figure 13-2\. Starting TorchServe
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 启动 TorchServe
- en: Testing Inference
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试推理
- en: Once the server is successfully up and running, you can test it by using `curl`
    from another terminal.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务器成功启动并运行，您可以通过在另一个终端中使用 `curl` 来测试它。
- en: 'So, to get an inference from the model, you can `curl` like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要从模型获取推理，您可以使用以下 `curl` 命令：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that it is an HTTP POST to the predictions endpoint. We specify the `simple_linear`
    model name as defined with the *.mar* file earlier, and we can then add the header
    (with the `-H` parameter) as plain text containing the data `5.0`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个对预测端点的 HTTP POST 请求。我们指定了与之前定义的 *.mar* 文件相对应的 `simple_linear` 模型名称，然后我们可以添加带有
    `-H` 参数的标题（作为纯文本），包含数据 `5.0`。
- en: As you may recall, the model learned the linear relationship *Y* = 2*x* – 1,
    so in this case, *x* will be 5 and the result will be a number close to 9.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所回忆的，该模型学习到的线性关系是 *Y* = 2*x* – 1，因此在这种情况下，*x* 将是 5，结果将是一个接近 9 的数字。
- en: 'The return should look like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 返回结果应如下所示：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Your value may vary, based on how your model was trained, but it should be a
    value very close to 9.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您的值可能会有所不同，这取决于您的模型是如何训练的，但它应该是一个非常接近 9 的值。
- en: 'You can also use the management endpoint to inspect the models that the server
    is hosting, like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用管理端点来检查服务器托管的所有模型，如下所示：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The response will contain the name and location of the *.mar* file for each
    model on the server:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器上的每个模型将包含 *.mar* 文件的名称和位置：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that earlier, when you did inference on the model, you used the `predictions`
    endpoint followed by a model name, which in that case was `simple_linear`. This
    key should map to a model name in this models collection, or you would have gotten
    an error.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在您对模型进行推理时，您使用了 `predictions` 端点后跟一个模型名称，在那个例子中是 `simple_linear`。此键应映射到该模型集合中的模型名称，否则您将收到错误。
- en: 'Finally, if you want to explore the details of a specific model, you can call
    the management URL (via port 8081, as earlier) with the model’s endpoint and the
    name of the model you want to know more about:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想探索特定模型的详细信息，您可以通过管理 URL（通过端口 8081，如之前所述）调用模型的端点和您想了解更多信息的模型的名称：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The server will return some detailed specs on the model, along with information
    you can use to help debug any issues. Here’s an example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将返回有关该模型的详细规格，以及您可以使用这些信息来帮助调试任何问题的信息。以下是一个示例：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice, for example, that the `deviceType` is expecting `gpu`. However, since
    I don’t have `nvgpu` on this system (see the earlier note about when you ran the
    server), it can’t load from the GPU, and the workers reported on that. It’s OK
    for that to be the case in my dev box, which doesn’t have the Nvidia GPU—but should
    you be running this on a server that *does* have the Nvidia GPU, that message
    is something you’d want to follow up on, and it’s likely an issue in your handler
    file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，例如，`deviceType` 预期的是 `gpu`。然而，由于我在这台系统上没有 `nvgpu`（请参阅关于您运行服务器时的早期说明），它无法从
    GPU 加载，并且工作者报告了这一点。在我的开发箱中，这种情况是可以接受的，因为我没有 Nvidia GPU——但是如果您在确实有 Nvidia GPU 的服务器上运行此操作，那么这条消息是您需要跟进的，并且很可能是您处理文件中的问题。
- en: Going Further
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步学习
- en: The foregoing was a bare-bones example to help you understand the nuts and bolts
    of how TorchServe works. As you use more sophisticated models that take in more
    complex data, the basic pattern you followed here should follow suit. In particular,
    the breakdown of preprocessing, inference, and postprocessing in the handler file
    is an enormous help! Additionally, the PyTorch ecosystem has add-ons for common
    scenarios to help you avoid having to write preprocessing code to begin with!
    For example, if you are interested in image classification and are worried about
    taking an image and turning it into tensors so that you can do inference on that
    image, the `ImageClassifier` class builds on the base handler to do this for you,
    and you can have image classification without needing to write a preprocessor.
    To see more of this in action, take a look at the open source examples at the
    PyTorch repository. In particular, you can go to [this GitHub page](https://oreil.ly/YA4v4)
    for an example of how to create a handler for MNIST images.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是一个裸骨示例，旨在帮助你理解 TorchServe 的工作原理。随着你使用更复杂的模型并处理更复杂的数据，这里遵循的基本模式也应该如此。特别是，在处理文件中对预处理、推理和后处理的分解是一个巨大的帮助！此外，PyTorch
    生态系统为常见场景提供了附加组件，以帮助你避免一开始就必须编写预处理代码！例如，如果你对图像分类感兴趣，并且担心将图像转换为张量以便进行推理，那么 `ImageClassifier`
    类基于基本处理程序为你完成这项工作，你可以在不编写预处理器的情况下进行图像分类。要了解更多关于这一功能的实际应用，请查看 PyTorch 仓库中的开源示例。特别是，你可以访问
    [这个 GitHub 页面](https://oreil.ly/YA4v4) 来了解如何为 MNIST 图像创建处理程序。
- en: You’ll find many more useful examples in that repo, but I would still recommend
    going through the steps to get a bare-bones example like the one we showed here
    up and running first. There are a lot of steps and a lot of concepts, and it’s
    easy to get lost in the maze.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在那个仓库中找到更多有用的示例，但我仍然建议首先完成我们在这里展示的裸骨示例的设置。这里有众多步骤和概念，很容易迷失在迷宫中。
- en: Serving with Flask
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Flask 提供服务
- en: While TorchServe is extremely powerful, a great alternative that’s super easy
    to use is Flask. Flask is a lightweight and flexible web framework for Python
    that enables efficient development of web applications and APIs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TorchServe 非常强大，但还有一个超级容易使用的优秀替代品，那就是 Flask。Flask 是一个轻量级且灵活的 Python 网络框架，它能够高效地开发网络应用程序和
    API。
- en: You can use Flask to build everything from minimal single-endpoint services
    to complex web applications, starting with just a few lines of code. It perfectly
    complements PyTorch by giving you the ability to host models for inference, as
    we’ll explore in this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Flask 构建从最小单端点服务到复杂网络应用程序的一切，只需几行代码。它通过为你提供托管模型进行推理的能力，完美地补充了 PyTorch，我们将在本节中探讨这一点。
- en: As a microframework, Flask provides the essential components for web development—routing,
    request handling, and templating—while allowing you to select additional functionality
    as needed. It is highly extensible, with stuff like a backend database, authentication,
    etc., and there’s also a vibrant ecosystem of extensions available to use off-the-shelf.
    Because of all of this, Flask has become a standard tool in web development, powering
    applications across industries at all scales.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为微框架，Flask 提供了网络开发的基本组件——路由、请求处理和模板化——同时允许你根据需要选择额外的功能。它高度可扩展，包括后端数据库、身份验证等功能，还有现成的扩展生态系统可供使用。正因为如此，Flask
    已经成为网络开发的标准工具，为各个规模和行业的应用程序提供动力。
- en: In this chapter, we’ll just explore Flask from the hosting model’s perspective,
    but I’d encourage you to dig deeper into the framework if you’re interested in
    serving Python code—not just PyTorch!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将仅从托管模型的角度探索 Flask，但我鼓励你对框架进行更深入的研究，如果你对托管 Python 代码（而不仅仅是 PyTorch）感兴趣！
- en: Now, let’s take a look at the same example that we used for TorchServe. This
    will help you see how simple Flask makes serving applications!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们为 TorchServe 使用的相同示例。这将帮助你看到 Flask 如何使应用程序的提供变得简单！
- en: Creating an Environment for Flask
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Flask 创建环境
- en: 'First, to use Flask, you’ll need to install it and any dependencies. If you’ve
    been using the same environment as earlier in this chapter, you can simply update
    it with this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了使用 Flask，你需要安装它及其依赖项。如果你一直在使用本章早些时候相同的环境，你可以简单地使用以下命令更新它：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then, just ensure that you have a model definition and training file—exactly
    the same as those from earlier in this chapter—and that you have trained a model
    and saved it with its state dictionary as a file called *model.pth*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需确保你有一个模型定义和训练文件——与本章前面提到的完全相同——并且你已经训练了一个模型，并以名为 *model.pth* 的文件保存了其状态字典。
- en: With those in hand, all you’ll need is a single Python file that I’m going to
    call *app.py*, which will be the primary server application that Flask will use.
    We’ll explore the code in that file next.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些工具后，你只需要一个名为 *app.py* 的单个 Python 文件，我将用它作为 Flask 将使用的首要服务器应用程序。我们将在下一节中探索该文件中的代码。
- en: Creating a Flask Server in Python
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Python 中创建 Flask 服务器
- en: 'To create a server with Flask, you implement an app that creates a new Flask
    instance by using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Flask 创建服务器，你实现一个应用程序，通过以下代码创建一个新的 Flask 实例：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, on the app object, you can specify routes, such as `predict` for doing
    prediction on models. To serve, you implement the code for the endpoints at these
    routes. Here’s an example of a full Flask server for our simple app:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在应用程序对象上，你可以指定路由，例如 `predict` 用于对模型进行预测。为了提供服务，你实现这些路由的端点代码。以下是我们简单应用程序的完整
    Flask 服务器示例：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Flask documentation and samples tend to use port 5000\. If you’re using a Mac
    as your development box, you might have issues with this as it conflicts with
    the port used by Airplay. To that end, I’ve used 5001 in this sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 文档和示例通常使用端口 5000。如果你使用 Mac 作为开发环境，可能会遇到问题，因为它与 Airplay 使用的端口冲突。因此，我在这个示例中使用了
    5001。
- en: In this case, we declare the `SimpleLinearModel` and load it along with its
    state dictionary. Then, we put it into `eval()` mode to get it ready for inference.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们声明 `SimpleLinearModel` 并加载它及其状态字典。然后，我们将它放入 `eval()` 模式以准备好进行推理。
- en: Then, it becomes as simple as creating a route that we call `predict` and then
    implementing the inference within it. As you can see, we handle getting the `value`
    from the HTTP `POST`, converting it into a tensor, and getting the prediction
    back when we pass that tensor to the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它变得像创建一个名为 `predict` 的路由一样简单，并在其中实现推理。正如你所见，我们处理从 HTTP `POST` 获取 `value`，将其转换为张量，并将该张量传递给模型以获取预测结果。
- en: To make the return a little friendlier, I used `jsonify` to turn it into a name-value
    pair. As you can see, that’s much simpler than using TorchServe, but for that
    simplicity, you give up power. There’s no set of base classes to handle preprocessing,
    post-processing, etc., and if you want to scale or implement multiple worker threads,
    you’ll have to do it yourself.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使返回结果更友好，我使用了 `jsonify` 将其转换为键值对。正如你所见，这比使用 TorchServe 简单得多，但为了这种简单性，你将放弃一些功能。没有一组基类来处理预处理、后处理等，如果你想要扩展或实现多个工作线程，你必须自己完成。
- en: I think this is a really useful and powerful server mechanism for smaller-scale
    environments, as well as for learning how to serve. For large-scale production
    environments, it’s a lot of extra work, but it can definitely handle the load.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这对于小型环境以及学习如何提供服务来说是一个非常实用且强大的服务器机制。对于大规模生产环境，这需要很多额外的工作，但它确实可以处理负载。
- en: 'For inference, you can `curl` a POST to the model like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，你可以像这样向模型发送一个 POST 请求：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And the response will be the JSON payload:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 响应将是 JSON 负载：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In addition to TorchServe and Flask, there are many other serving options, such
    as ONNX and FastAPI.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 TorchServe 和 Flask，还有许多其他服务选项，例如 ONNX 和 FastAPI。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored two popular approaches to serving PyTorch models
    in production environments. You started with TorchServe, which is PyTorch’s official
    serving solution that offers a robust framework with built-in support for model
    versioning, A/B testing, and metrics collection. It’s also designed to be highly
    scalable at runtime, with a worker-thread architecture that’s configurable based
    on the needs of your app. And while TorchServe requires more setup and understanding
    of its components like model workers and frontend handlers, I think it’s worth
    investing the time in rolling up your sleeves and understanding all the different
    components and how they work together. To that end, you explored step-by-step
    how to take the simple linear model example from [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566),
    save it, archive it, build a handler for it, and launch the server with the model
    details.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在生产环境中提供 PyTorch 模型的两种流行方法。你从 TorchServe 开始，这是 PyTorch 的官方服务解决方案，它提供了一个强大的框架，内置了对模型版本控制、A/B
    测试和指标收集的支持。它还设计为在运行时具有高度可扩展性，具有可根据你的应用程序需求配置的工作线程架构。虽然 TorchServe 需要更多的设置和对其组件（如模型工作者和前端处理器）的理解，但我认为投入时间深入了解所有不同的组件以及它们如何协同工作是值得的。为此，你逐步探索了如何从[第
    1 章](ch01.html#ch01_introduction_to_pytorch_1748548870019566)中的简单线性模型示例开始，保存它、归档它、为它构建处理器，并使用模型详细信息启动服务器。
- en: Then, you explored how to use Flask as a lightweight alternative that’s extremely
    quick and simple to get up and running. You saw how its minimalist approach makes
    it ideal for smaller-scale deployments or proof-of-concept services. It’s not
    limited to those, but as you move to production scale, you’ll likely need to implement
    more code. Of course, that’s not necessarily a disadvantage, as it gives you more
    granular control over your serving environment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你探索了如何使用 Flask 作为一种轻量级的替代方案，它启动迅速且简单。你看到了它的极简主义方法使其非常适合小规模部署或概念验证服务。它不仅限于这些，但随着你转向生产规模，你可能会需要实现更多的代码。当然，这并不一定是缺点，因为它让你对你的服务环境有更细粒度的控制。
- en: Both approaches have their place in the ML serving ecosystem. TorchServe shines
    in enterprise environments requiring comprehensive model management, while Flask’s
    simplicity makes it perfect for smaller projects or learning environments with
    a smooth glide path toward scalable production. Of course, you aren’t limited
    to just these two, and new frameworks are coming online all the time—in particular,
    one called FastAPI, which is rapidly growing in popularity. Which one you should
    choose ultimately depends on your specific needs around scaling, monitoring, and
    deployment complexity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法在机器学习服务生态系统中都有其位置。TorchServe 在需要全面模型管理的企业环境中表现出色，而 Flask 的简单性使其非常适合小型项目或学习环境，这些环境可以平滑地过渡到可扩展的生产。当然，你并不局限于这两种选择，而且新的框架一直在上线——特别是名为
    FastAPI 的框架，它正迅速增长其受欢迎程度。你应该选择哪一个最终取决于你关于扩展、监控和部署复杂性的具体需求。
- en: Next, in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    you’re going to look at third-party models that have been pretrained for you and
    various registries and hubs that you can load them from.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[第 14 章](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)中，你将查看为你预训练的第三方模型以及你可以从中加载它们的各种注册表和中心。
