- en: Appendix A. Important Definitions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A. 重要的定义
- en: 'I’m afraid there’s no way around it: if we want to get the *full* benefits
    of AI, we’re going to have to swallow hard and absorb some serious concepts. Technology
    is complicated by design, and AI is a particularly complicated subset of technology.
    The good news is that we’re not trying to qualify as physicists and engineers,
    so a very basic grasp of these ideas and their history will work just fine for
    our purposes. But prepare for some turbulence all the same.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恐怕这是不可避免的：如果我们想要获得 AI 的*全部*好处，我们就必须咬紧牙关，吸收一些严肃的概念。技术是经过设计复杂的，而人工智能是技术的一个特别复杂的子集。好消息是，我们并不打算成为物理学家和工程师，所以对这些想法及其历史的基本了解就足够了。但是还是要做好一些动荡的准备。
- en: A.1 Some critical AI definitions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 一些关键的 AI 定义
- en: To get you started, here’s a helpful diagram illustrating the complex relationships
    between the many computational elements behind generative AI models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 要帮助你入门，这里有一张有用的图表，展示了生成式人工智能模型背后众多计算元素之间的复杂关系。
- en: Figure A.1 A left-to-right oriented mindmap of AI relationships
  id: totrans-4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.1 AI 关系的从左到右的思维导图
- en: '![gai app 1](images/gai-app-1.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![gai app 1](images/gai-app-1.png)'
- en: Having said that, even if you choose to skip this section altogether, you’ll
    still be able to successfully follow along with everything else in the book. But
    you might have trouble identifying some of the nuance (and weaknesses) in the
    AI responses you get. And some instructions and processes may feel a bit arbitrary.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，即使你选择完全跳过本节，你仍然可以成功地跟着本书的其他内容走。但你可能会有些难以确定所得到的 AI 响应中的一些微妙之处（和弱点）。而且一些指示和流程可能会感觉有些随意。
- en: I should note that the definitions for many of these concepts will reference
    other concepts. I’ll do my best to refer to only things that have been previously
    defined, but there are too many twisted (and recursive) relationships to make
    that happen every time. With that warning, here’s some fundamental knowledge that’ll
    make you more effective at working with generative AI.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该注意到，许多概念的定义将参考其他概念。我会尽力只参考先前定义过的事物，但是由于扭曲（和递归）关系太多，这不可能每次都做到。在提醒了这一点之后，下面是一些基本知识，它将使您更有效地处理生成式人工智能。
- en: '**Machine learning** is a branch of artificial intelligence that focuses on
    developing algorithms and models capable of automatically learning and improving
    from data without explicit programming. It involves training a system on a large
    dataset to recognize patterns, make predictions, or perform tasks. By iteratively
    adjusting model parameters, machine learning enables computers to learn from experience
    and adapt to new inputs, enabling them to make informed decisions and perform
    complex tasks with minimal human intervention.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是人工智能的一个分支，专注于开发能够自动从数据中学习和改进而无需明确编程的算法和模型。它涉及在大型数据集上训练系统以识别模式、进行预测或执行任务。通过迭代调整模型参数，机器学习使计算机能够从经验中学习并适应新的输入，使其能够做出明智的决策并在最小的人工干预下执行复杂的任务。'
- en: In the context of AI, a **model** refers to a mathematical representation or
    computational system that learns patterns, structures, or relationships from data.
    It’s a trained algorithm or network that can take input and generate meaningful
    output based on its learned knowledge or trained parameters. In generative AI,
    a model refers specifically to a system that can generate new data samples that
    resemble the training data, whether it’s generating images, text, music, or other
    forms of creative content. The model encapsulates the learned information and
    the ability to generate new instances based on that knowledge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 的背景下，**模型**指的是从数据中学习模式、结构或关系的数学表示或计算系统。它是一个经过训练的算法或网络，可以根据其学到的知识或训练参数接收输入并生成有意义的输出。在生成式人工智能中，模型特指能够生成类似训练数据的新数据样本的系统，无论是生成图像、文本、音乐还是其他形式的创造性内容。模型封装了学到的信息以及基于该知识生成新实例的能力。
- en: '**Labels** are categorizations or annotations assigned to data points. They
    provide explicit information about the characteristics or attributes associated
    with the input. Labels act as guiding signals to help the model learn and generate
    output that aligns with the desired attributes or properties. One place where
    labels are commonly used is for sentiment analysis. Sentiment analysis involves
    training a model to classify text as either positive, negative, or neutral based
    on its emotional tone. To perform this task, we need to label our training data
    with appropriate sentiments (e.g., "this review is positive," "this tweet is negative").'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签**是分配给数据点的分类或注释。它们提供了与输入相关的特征或属性的明确信息。标签作为指导信号，帮助模型学习并生成与所需属性或特性相符的输出。标签通常用于情感分析的一个地方。情感分析涉及训练模型根据文本的情感色调将其分类为正面、负面或中性。为执行此任务，我们需要使用适当的情感对我们的训练数据进行标记（例如，“这篇评论是积极的”，“这条推文是消极的”）。'
- en: '**Weighting** refers to the numerical values assigned to the connections between
    neurons or features in a model. These weights determine the strength or importance
    of each connection and play a crucial role in the model’s learning and decision-making
    process. During training, the weights are adjusted iteratively based on the observed
    errors or differences between predicted and actual outputs, enabling the model
    to learn from the data and improve its performance by assigning appropriate weights
    to different inputs and connections. Weighting is commonly used for named entity
    recognition (NER), which involves identifying and categorizing entities mentioned
    in text into predefined categories like person, organization, and location. A
    weighted NER model, for instance, be used for a chatbot application to extract
    and respond to user queries about specific topics or entities.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**加权**是指分配给模型中神经元或特征之间连接的数值。这些权重确定了每个连接的强度或重要性，并在模型的学习和决策过程中发挥关键作用。在训练过程中，权重根据观察到的错误或预测与实际输出之间的差异进行迭代调整，使模型能够从数据中学习，并通过为不同的输入和连接分配适当的权重来提高其性能。加权通常用于命名实体识别（NER），它涉及将文本中提及的实体识别和分类到预定义的类别，如人物、组织和地点。例如，加权的NER模型可用于聊天机器人应用程序，以提取和响应用户对特定主题或实体的查询。'
- en: A **parser** is a software component or algorithm that analyzes the structure
    of a given input, typically in the form of a sequence of symbols or text, and
    generates a structured representation based on a predefined grammar or set of
    rules. It is commonly used in natural language processing to parse sentences and
    extract syntactic or semantic information. Parsers break down the input into constituent
    parts, such as words or phrases, and establish relationships between them, enabling
    further analysis, understanding, or processing of the input data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**解析器**是分析给定输入的结构的软件组件或算法，通常以符号或文本序列的形式出现，并根据预定义的语法或一组规则生成结构化表示。它通常用于自然语言处理中解析句子并提取句法或语义信息。解析器将输入分解为组成部分，如单词或短语，并建立它们之间的关系，从而实现进一步分析、理解或处理输入数据。'
- en: By understanding the **dependencies** between words, sentences, or visual elements,
    generative AI models can generate meaningful sequences or images that maintain
    contextual consistency. Modeling dependencies allows the generated output to exhibit
    logical flow, semantic coherence, and adherence to patterns observed in the training
    data. Accurately capturing dependencies is essential for generating high-quality
    and coherent outputs in generative AI applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解单词、句子或视觉元素之间的**依赖关系**，生成式人工智能模型可以生成保持上下文一致性的有意义的序列或图像。建模依赖关系允许生成的输出展现逻辑流、语义一致性，并遵循训练数据中观察到的模式。准确捕捉依赖关系对于在生成式人工智能应用程序中生成高质量和连贯的输出至关重要。
- en: '**Regression** is a supervised machine learning technique used to predict or
    estimate a continuous output variable based on input features. It models the relationship
    between the input variables and the output variable by fitting a mathematical
    function to the training data. The goal is to find the best-fitting function that
    minimizes the difference between the predicted values and the actual values. Regression
    algorithms analyze the patterns and trends in the data to make predictions or
    infer relationships. Regression can be another tool for sentiment analysis. For
    customer service-related tasks, for instance, it’s important to be able to automatically
    classify customer complaints or praise to allow organizations to accurately route
    issues to the appropriate support agents.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**（Regression）是一种监督学习技术，用于根据输入特征预测或估计连续的输出变量。它通过将数学函数拟合到训练数据来建模输入变量和输出变量之间的关系。目标是找到最佳拟合函数，使预测值与实际值之间的差异最小化。回归算法分析数据中的模式和趋势，以进行预测或推断关系。对于情感分析，回归可以是另一个工具。例如，对于与客户服务相关的任务，能够自动对客户投诉或表扬进行分类是很重要的，这样组织就能够准确地将问题路由到适当的支持代理那里。'
- en: '**Classification** is a fundamental task in machine learning where the goal
    is to assign input data points to predefined categories or classes. It involves
    training a model on labeled data, where each data point is associated with a known
    class. The model learns patterns and relationships in the training data to make
    predictions on new, unseen data. The output of a classification model is a discrete
    class label that represents the predicted category to which the input belongs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**（Classification）是机器学习中的一项基本任务，其目标是将输入数据点分配给预定义的类别或类。它涉及对带有标签的数据进行模型训练，其中每个数据点都与已知类相关联。模型学习训练数据中的模式和关系，以便对新的、未见过的数据进行预测。分类模型的输出是一个离散的类标签，表示输入所属的预测类别。'
- en: '**Optimization algorithms** are mathematical procedures used to find the optimal
    solution for a given problem. In the context of machine learning and neural networks,
    these algorithms are employed to minimize an objective function, typically represented
    by a loss or cost function. The goal is to adjust the model’s parameters iteratively
    to reach the optimal set of values that minimize the objective function. In the
    world of optimizing models, there are some popular techniques like "stochastic
    gradient descent" and its variations. These methods help the model get better
    by adjusting its internal settings according to how much it’s improving or getting
    worse. By doing this, the model gets closer to finding the best possible solution
    and performs much better at its tasks.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化算法**（Optimization algorithms）是用于找到给定问题的最优解的数学过程。在机器学习和神经网络的背景下，这些算法用于最小化一个目标函数，通常由损失或成本函数表示。目标是通过迭代调整模型的参数，达到最小化目标函数的最佳值组合。在优化模型的世界中，有一些流行的技术，如“随机梯度下降”及其变体。这些方法通过根据模型的改善或恶化程度调整其内部设置，帮助模型变得更好。通过这样做，模型更接近于找到最佳解决方案，并在其任务上表现得更好。'
- en: '**Vectors** are mathematical entities used to represent both magnitude and
    direction in a multi-dimensional space. In the context of machine learning and
    data analysis, vectors are often used to represent features or data points. Each
    dimension of a vector corresponds to a specific attribute or variable, allowing
    for efficient storage and manipulation of data. Vectors can be operated upon using
    mathematical operations like addition, subtraction, and dot product, enabling
    calculations of similarity, distances, and transformations. Vectors play a fundamental
    role in various algorithms and models, such as clustering, classification, and
    dimensionality reduction.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**（Vectors）是用于表示多维空间中的大小和方向的数学实体。在机器学习和数据分析的背景下，向量通常用于表示特征或数据点。向量的每个维度对应于一个特定的属性或变量，允许数据的高效存储和操作。向量可以使用数学运算（如加法、减法和点积）进行操作，从而实现相似性、距离和转换的计算。向量在各种算法和模型中起着基本作用，如聚类、分类和降维。'
- en: '**Vector embeddings** help LLMs generalize knowledge across similar words and
    phrases, even if they were not encountered during training. This allows the model
    to handle out-of-vocabulary words effectively. Pre-trained embeddings can be used
    as starting points for various NLP tasks, enabling transfer learning and improving
    performance on downstream tasks with limited data. One practical application of
    all this would be medical imaging, where vector embeddings can be used to analyze
    and compare images of organs or tissues. A deep learning model can be trained
    to map brain scans onto a vector space, where similar scans are clustered together.
    This enables doctors to quickly identify patterns and abnormalities in patient
    scans, leading to earlier diagnosis and treatment of diseases such as cancer or
    neurological disorders.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量嵌入（Vector embeddings）**帮助LLMs在相似的单词和短语上推广知识，即使它们在训练期间没有遇到。这使得模型能够有效地处理未出现在词汇表中的单词。预训练的嵌入可以用作各种自然语言处理任务的起点，实现转移学习，并提高在有限数据的下游任务上的性能。所有这些的一个实际应用是在医学影像学中，向量嵌入可以用来分析和比较器官或组织的图像。可以训练深度学习模型将脑部扫描映射到向量空间中，在那里类似的扫描聚集在一起。这使医生能够快速识别病人扫描中的模式和异常，从而早期诊断和治疗癌症或神经系统疾病。'
- en: '**Word embeddings** are a way of representing words as vectors in a high-dimensional
    space, such that similar words are close together in that space. Word embeddings
    are typically represented as tensors, where each dimension represents a different
    aspect of the word’s meaning. For example, a word embedding tensor might have
    dimensions for the word’s synonyms, antonyms, and part of speech.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**词嵌入（Word embeddings）**是将单词表示为高维向量的一种方式，使得相似的单词在该空间中靠近。词嵌入通常表示为张量，其中每个维度表示单词含义的不同方面。例如，一个词嵌入张量可能具有单词的同义词、反义词和词性的维度。'
- en: '**Parallelization** refers to the technique of dividing a computational task
    into smaller subtasks that can be executed simultaneously on multiple computing
    resources. It leverages the power of parallel processing to speed up the overall
    computation and improve efficiency. In parallel computing, tasks are allocated
    to different processors, threads, or computing units, allowing them to work concurrently.
    This approach enables tasks to be completed faster by distributing the workload
    across multiple resources. Parallelization is widely used in various fields, including
    machine learning, scientific simulations, and data processing, to achieve significant
    performance gains and handle large-scale computations efficiently.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行计算（Parallelization）**指将计算任务分成较小的子任务，可以在多个计算资源上同时执行的技术。它利用并行处理的能力来加速整个计算和提高效率。在并行计算中，任务被分配给不同的处理器、线程或计算单元，允许它们同时运行。这种方法通过将工作负载分布到多个资源上来实现更快地完成任务。并行计算广泛应用于各个领域，包括机器学习、科学模拟和数据处理，以实现显著的性能提升和有效处理大规模计算。'
- en: '**Regularization techniques** are methods used to improve the generalization
    performance of models. These techniques add a penalty term to the loss function
    during training, discouraging the model from relying too heavily on complex or
    noisy patterns in the data. Regularization techniques help control model complexity,
    reduce overfitting, and improve the model’s ability to generalize to unseen data.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化技术（Regularization techniques）**是用来提高模型泛化性能的方法。这些技术在训练期间向损失函数添加惩罚项，防止模型过度依赖复杂或嘈杂的数据模式。正则化技术有助于控制模型复杂度，减少过拟合，并提高模型对未见过数据的泛化能力。'
- en: A common practical application of regularization techniques is in text classification,
    specifically when dealing with imbalanced datasets. Let’s say we have a dataset
    of movie reviews, where the majority class is positive reviews (e.g., "good movie")
    and the minority class is negative reviews (e.g., "bad movie"). Without regularization,
    the model might become biased towards the positive reviews and fail to accurately
    classify the negative reviews. To address this imbalance, we can add a regularization
    term to the loss function that penalizes the model for misclassifying negative
    reviews.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术的一个常见实际应用是在文本分类中，特别是处理不均衡数据集时。假设我们有一个电影评论数据集，其中大多数属于正面评价（比如“好电影”），少部分是负面评价（比如“烂电影”）。如果没有正则化，模型可能会对正面评价产生偏见，并无法准确分类负面评价。为了解决这种不平衡，我们可以在损失函数中添加正则化项，惩罚模型误分类负面评论。
- en: '**Convergence** refers to the process of training multiple models on the same
    dataset until they produce similar outputs. This is done to reduce the risk of
    overfitting and improve the generalization of the models. Convergence is typically
    evaluated using metrics such as validation loss or accuracy, and the training
    process is stopped once the models converge to a stable solution.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**收敛**是指在相同数据集上训练多个模型直到它们产生相似的输出的过程。这样做是为了减少过拟合的风险并提高模型的泛化能力。收敛通常使用诸如验证损失或准确率等指标进行评估，并且一旦模型收敛到稳定解决方案，训练过程就会停止。'
- en: 'All of which bring us to:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都带领我们来到：
- en: '**Natural Language Processing (NLP)** focuses on the interaction between computers
    and human language. It involves the development of algorithms and models to enable
    computers to understand, interpret, and generate human language in a meaningful
    way. NLP encompasses tasks such as text classification, sentiment analysis, machine
    translation, information extraction, and question answering. It utilizes techniques
    from various disciplines, including computational linguistics, machine learning,
    and deep learning, to process and analyze textual data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理（NLP）**专注于计算机与人类语言之间的交互。它涉及开发算法和模型，使计算机能够以有意义的方式理解、解释和生成人类语言。NLP包括诸如文本分类、情感分析、机器翻译、信息提取和问答等任务。它利用来自各种学科的技术，包括计算语言学、机器学习和深度学习，来处理和分析文本数据。'
- en: 'And, finally, to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，到最后：
- en: A **Large Language Model (LLM)**, is a tool in natural language processing (NLP)
    that leverages deep learning techniques to understand and generate human-like
    text. It analyzes patterns, contexts, and semantics within a given text corpus
    to learn the underlying structures of language. With its ability to comprehend
    and generate coherent and contextually relevant responses, an LLM can be used
    for various tasks, such as chatbots, language translation, text completion, and
    summarization. By capturing the intricacies of language, an LLM allows machines
    to communicate directly with humans.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型（LLM）**是自然语言处理（NLP）中的一种工具，利用深度学习技术来理解和生成类似人类的文本。它分析给定文本语料库中的模式、上下文和语义，以学习语言的潜在结构。通过其理解和生成连贯且上下文相关的响应的能力，LLM可用于各种任务，如聊天机器人、语言翻译、文本补全和摘要。通过捕捉语言的复杂性，LLM允许机器直接与人类进行交流。'
- en: Or, in other words, it enables generative AI.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者换句话说，它实现了生成式人工智能。
