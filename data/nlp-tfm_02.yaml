- en: Chapter 1\. Hello Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。你好，变压器
- en: In 2017, researchers at Google published a paper that proposed a novel neural
    network architecture for sequence modeling.^([1](ch01.xhtml#idm46238735114624))
    Dubbed the *Transformer*, this architecture outperformed recurrent neural networks
    (RNNs) on machine translation tasks, both in terms of translation quality and
    training cost.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，谷歌的研究人员发表了一篇关于序列建模的新型神经网络架构的论文。这种被称为*变压器*的架构在机器翻译任务中优于循环神经网络（RNN），无论是翻译质量还是训练成本。
- en: In parallel, an effective transfer learning method called ULMFiT showed that
    training long short-term memory (LSTM) networks on a very large and diverse corpus
    could produce state-of-the-art text classifiers with little labeled data.^([2](ch01.xhtml#idm46238735221744))
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，一种名为ULMFiT的有效迁移学习方法表明，对非常大和多样化的语料库进行长短期记忆（LSTM）网络训练可以产生具有很少标记数据的最先进文本分类器。
- en: 'These advances were the catalysts for two of today’s most well-known transformers:
    the Generative Pretrained Transformer (GPT)^([3](ch01.xhtml#idm46238735216480))
    and Bidirectional Encoder Representations from Transformers (BERT).^([4](ch01.xhtml#idm46238735215072))
    By combining the Transformer architecture with unsupervised learning, these models
    removed the need to train task-specific architectures from scratch and broke almost
    every benchmark in NLP by a significant margin. Since the release of GPT and BERT,
    a zoo of transformer models has emerged; a timeline of the most prominent entries
    is shown in [Figure 1-1](#transformer-timeline).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进步是当今两个最著名的变压器的催化剂：生成式预训练变压器（GPT）和来自变压器的双向编码器表示（BERT）。通过将变压器架构与无监督学习相结合，这些模型消除了需要从头开始训练特定任务的架构，并在NLP几乎每个基准测试中取得了显著的突破。自GPT和BERT发布以来，出现了一系列变压器模型；最突出的条目的时间表如[图1-1](#transformer-timeline)所示。
- en: '![transformer-timeline](Images/nlpt_0101.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![transformer-timeline](Images/nlpt_0101.png)'
- en: Figure 1-1\. The transformers timeline
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。变压器时间表
- en: 'But we’re getting ahead of ourselves. To understand what is novel about transformers,
    we first need to explain:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们正在走得太快了。要理解变压器的新颖之处，我们首先需要解释：
- en: The encoder-decoder framework
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器框架
- en: Attention mechanisms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意机制
- en: Transfer learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: In this chapter we’ll introduce the core concepts that underlie the pervasiveness
    of transformers, take a tour of some of the tasks that they excel at, and conclude
    with a look at the Hugging Face ecosystem of tools and libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍支持transformer广泛应用的核心概念，参观一些它们擅长的任务，并最后看一下Hugging Face工具和库的生态系统。
- en: Let’s start by exploring the encoder-decoder framework and the architectures
    that preceded the rise of transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先探讨编码器-解码器框架和在transformer崛起之前的架构。
- en: The Encoder-Decoder Framework
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器框架
- en: 'Prior to transformers, recurrent architectures such as LSTMs were the state
    of the art in NLP. These architectures contain a feedback loop in the network
    connections that allows information to propagate from one step to another, making
    them ideal for modeling sequential data like text. As illustrated on the left
    side of [Figure 1-2](#rnn), an RNN receives some input (which could be a word
    or character), feeds it through the network, and outputs a vector called the *hidden
    state*. At the same time, the model feeds some information back to itself through
    the feedback loop, which it can then use in the next step. This can be more clearly
    seen if we “unroll” the loop as shown on the right side of [Figure 1-2](#rnn):
    the RNN passes information about its state at each step to the next operation
    in the sequence. This allows an RNN to keep track of information from previous
    steps, and use it for its output predictions.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformer之前，像LSTM这样的循环架构是自然语言处理中的最新技术。这些架构在网络连接中包含反馈循环，允许信息从一个步骤传播到另一个步骤，使它们非常适合对文本等序列数据进行建模。如[图1-2](#rnn)左侧所示，RNN接收一些输入（可以是单词或字符），通过网络传递，并输出一个称为*隐藏状态*的向量。同时，模型通过反馈循环向自身反馈一些信息，然后在下一步中使用。如果我们像[图1-2](#rnn)右侧所示“展开”循环，就可以更清楚地看到这一点：RNN在每一步中传递其状态的信息给序列中的下一个操作。这使得RNN能够跟踪先前步骤的信息，并将其用于输出预测。
- en: '![rnn](Images/nlpt_0102.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![rnn](Images/nlpt_0102.png)'
- en: Figure 1-2\. Unrolling an RNN in time
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2。在时间上展开RNN
- en: These architectures were (and continue to be) widely used for NLP tasks, speech
    processing, and time series. You can find a wonderful exposition of their capabilities
    in Andrej Karpathy’s blog post, [“The Unreasonable Effectiveness of Recurrent
    Neural Networks”](https://oreil.ly/Q55o0).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构（并且继续）被广泛用于NLP任务、语音处理和时间序列。您可以在Andrej Karpathy的博客文章[“循环神经网络的不合理有效性”](https://oreil.ly/Q55o0)中找到它们能力的精彩阐述。
- en: One area where RNNs played an important role was in the development of machine
    translation systems, where the objective is to map a sequence of words in one
    language to another. This kind of task is usually tackled with an *encoder-decoder*
    or *sequence-to-sequence* architecture,^([5](ch01.xhtml#idm46238728352576)) which
    is well suited for situations where the input and output are both sequences of
    arbitrary length. The job of the encoder is to encode the information from the
    input sequence into a numerical representation that is often called the *last
    hidden state*. This state is then passed to the decoder, which generates the output
    sequence.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在机器翻译系统的发展中发挥了重要作用，其目标是将一种语言中的单词序列映射到另一种语言。这种任务通常使用*编码器-解码器*或*序列到序列*架构，适用于输入和输出都是任意长度序列的情况。编码器的工作是将输入序列的信息编码成通常称为*最后隐藏状态*的数值表示。然后将该状态传递给解码器，解码器生成输出序列。
- en: In general, the encoder and decoder components can be any kind of neural network
    architecture that can model sequences. This is illustrated for a pair of RNNs
    in [Figure 1-3](#enc-dec), where the English sentence “Transformers are great!”
    is encoded as a hidden state vector that is then decoded to produce the German
    translation “Transformer sind grossartig!” The input words are fed sequentially
    through the encoder and the output words are generated one at a time, from top
    to bottom.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，编码器和解码器组件可以是任何能够建模序列的神经网络架构。这在图1-3中对一对RNNs进行了说明，其中英语句子“Transformers are
    great!”被编码为一个隐藏状态向量，然后解码以生成德语翻译“Transformer sind grossartig!”输入单词依次通过编码器，输出单词从上到下逐个生成。
- en: '![enc-dec](Images/nlpt_0103.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![enc-dec](Images/nlpt_0103.png)'
- en: Figure 1-3\. An encoder-decoder architecture with a pair of RNNs (in general,
    there are many more recurrent layers than those shown here)
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3。具有一对RNNs的编码器-解码器架构（一般来说，这里显示的循环层比这里显示的要多得多）
- en: 'Although elegant in its simplicity, one weakness of this architecture is that
    the final hidden state of the encoder creates an *information bottleneck*: it
    has to represent the meaning of the whole input sequence because this is all the
    decoder has access to when generating the output. This is especially challenging
    for long sequences, where information at the start of the sequence might be lost
    in the process of compressing everything to a single, fixed representation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其简洁而优雅，这种架构的一个弱点是编码器的最终隐藏状态创建了一个*信息瓶颈*：它必须代表整个输入序列的含义，因为这是解码器在生成输出时所能访问的全部内容。这对于长序列尤其具有挑战性，因为序列开头的信息可能在压缩到单一固定表示的过程中丢失。
- en: Fortunately, there is a way out of this bottleneck by allowing the decoder to
    have access to all of the encoder’s hidden states. The general mechanism for this
    is called *attention*,^([6](ch01.xhtml#idm46238728612528)) and it is a key component
    in many modern neural network architectures. Understanding how attention was developed
    for RNNs will put us in good shape to understand one of the main building blocks
    of the Transformer architecture. Let’s take a deeper look.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过允许解码器访问编码器的所有隐藏状态，可以摆脱这一瓶颈。这一般机制称为*注意力*，^([6](ch01.xhtml#idm46238728612528))，它是许多现代神经网络架构的关键组成部分。了解注意力是如何为RNNs开发的将使我们能够更好地理解Transformer架构的主要构建模块之一。让我们深入了解一下。
- en: Attention Mechanisms
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意机制
- en: 'The main idea behind attention is that instead of producing a single hidden
    state for the input sequence, the encoder outputs a hidden state at each step
    that the decoder can access. However, using all the states at the same time would
    create a huge input for the decoder, so some mechanism is needed to prioritize
    which states to use. This is where attention comes in: it lets the decoder assign
    a different amount of weight, or “attention,” to each of the encoder states at
    every decoding timestep. This process is illustrated in [Figure 1-4](#enc-dec-attn),
    where the role of attention is shown for predicting the third token in the output
    sequence.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的主要思想是，编码器不是为输入序列产生单个隐藏状态，而是在每一步输出一个解码器可以访问的隐藏状态。然而，同时使用所有状态会为解码器创建一个巨大的输入，因此需要一些机制来优先使用哪些状态。这就是注意力的作用：它允许解码器在每个解码时间步为每个编码器状态分配不同数量的权重或“注意力”。这个过程在图1-4中进行了说明，显示了注意力在预测输出序列的第三个标记时的作用。
- en: '![enc-dec-attn](Images/nlpt_0104.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![enc-dec-attn](Images/nlpt_0104.png)'
- en: Figure 1-4\. An encoder-decoder architecture with an attention mechanism for
    a pair of RNNs
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。具有注意机制的编码器-解码器架构，用于一对RNNs
- en: By focusing on which input tokens are most relevant at each timestep, these
    attention-based models are able to learn nontrivial alignments between the words
    in a generated translation and those in a source sentence. For example, [Figure 1-5](#attention-alignment)
    visualizes the attention weights for an English to French translation model, where
    each pixel denotes a weight. The figure shows how the decoder is able to correctly
    align the words “zone” and “Area”, which are ordered differently in the two languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注每个时间步最相关的输入标记，这些基于注意力的模型能够学习生成翻译中的单词与源句中的单词之间的非平凡对齐。例如，图1-5可视化了英语到法语翻译模型的注意权重，其中每个像素表示一个权重。该图显示了解码器如何能够正确对齐两种语言中顺序不同的单词“zone”和“Area”。
- en: '![attention-alignment](Images/nlpt_0105.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![attention-alignment](Images/nlpt_0105.png)'
- en: Figure 1-5\. RNN encoder-decoder alignment of words in English and the generated
    translation in French (courtesy of Dzmitry Bahdanau)
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。英语和法语生成翻译中的RNN编码器-解码器单词对齐（由Dzmitry Bahdanau提供）
- en: 'Although attention enabled the production of much better translations, there
    was still a major shortcoming with using recurrent models for the encoder and
    decoder: the computations are inherently sequential and cannot be parallelized
    across the input sequence.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力使得翻译质量大大提高，但使用循环模型作为编码器和解码器仍然存在一个主要缺点：计算是固有的顺序性的，无法在输入序列上并行化。
- en: 'With the transformer, a new modeling paradigm was introduced: dispense with
    recurrence altogether, and instead rely entirely on a special form of attention
    called *self-attention*. We’ll cover self-attention in more detail in [Chapter 3](ch03.xhtml#chapter_anatomy),
    but the basic idea is to allow attention to operate on all the states in the *same
    layer* of the neural network. This is shown in [Figure 1-6](#transformer-self-attn),
    where both the encoder and the decoder have their own self-attention mechanisms,
    whose outputs are fed to feed-forward neural networks (FF NNs). This architecture
    can be trained much faster than recurrent models and paved the way for many of
    the recent breakthroughs in NLP.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用transformer引入了一种新的建模范式：完全放弃循环，而是完全依赖一种称为*自注意力*的特殊形式的注意力。我们将在[第3章](ch03.xhtml#chapter_anatomy)中更详细地介绍自注意力，但基本思想是允许注意力作用于神经网络*同一层*中的所有状态。这在[图1-6](#transformer-self-attn)中显示，编码器和解码器都有自己的自注意机制，其输出被馈送到前馈神经网络（FF
    NNs）。这种架构可以比循环模型快得多地训练，并为自然语言处理中的许多最新突破铺平了道路。
- en: '![transformer-self-attn](Images/nlpt_0106.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![transformer-self-attn](Images/nlpt_0106.png)'
- en: Figure 1-6\. Encoder-decoder architecture of the original Transformer
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6。原始Transformer的编码器-解码器架构
- en: 'In the original Transformer paper, the translation model was trained from scratch
    on a large corpus of sentence pairs in various languages. However, in many practical
    applications of NLP we do not have access to large amounts of labeled text data
    to train our models on. A final piece was missing to get the transformer revolution
    started: transfer learning.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始Transformer论文中，翻译模型是从头开始在各种语言的大量句对语料库上进行训练的。然而，在NLP的许多实际应用中，我们无法获得大量标记文本数据来训练我们的模型。要启动transformer革命，还缺少最后一块拼图：迁移学习。
- en: Transfer Learning in NLP
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的迁移学习
- en: It is nowadays common practice in computer vision to use transfer learning to
    train a convolutional neural network like ResNet on one task, and then adapt it
    to or *fine-tune* it on a new task. This allows the network to make use of the
    knowledge learned from the original task. Architecturally, this involves splitting
    the model into of a *body* and a *head*, where the head is a task-specific network.
    During training, the weights of the body learn broad features of the source domain,
    and these weights are used to initialize a new model for the new task.^([7](ch01.xhtml#idm46238728434256))
    Compared to traditional supervised learning, this approach typically produces
    high-quality models that can be trained much more efficiently on a variety of
    downstream tasks, and with much less labeled data. A comparison of the two approaches
    is shown in [Figure 1-7](#transfer-learning).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在计算机视觉中，通常使用迁移学习来训练卷积神经网络（如ResNet）进行一个任务的训练，然后在新任务上对其进行调整或*微调*。这允许网络利用从原始任务中学到的知识。在架构上，这涉及将模型分为*主体*和*头部*，其中头部是一个特定任务的网络。在训练过程中，主体的权重学习源域的广泛特征，并使用这些权重来初始化新任务的新模型。^([7](ch01.xhtml#idm46238728434256))与传统监督学习相比，这种方法通常能够在各种下游任务上更有效地训练高质量的模型，并且需要更少的标记数据。[图1-7](#transfer-learning)显示了这两种方法的比较。
- en: '![transfer-learning](Images/nlpt_0107.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![transfer-learning](Images/nlpt_0107.png)'
- en: Figure 1-7\. Comparison of traditional supervised learning (left) and transfer
    learning (right)
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7。传统监督学习（左）和迁移学习（右）的比较
- en: In computer vision, the models are first trained on large-scale datasets such
    as [ImageNet](https://image-net.org), which contain millions of images. This process
    is called *pretraining* and its main purpose is to teach the models the basic
    features of images, such as edges or colors. These pretrained models can then
    be fine-tuned on a downstream task such as classifying flower species with a relatively
    small number of labeled examples (usually a few hundred per class). Fine-tuned
    models typically achieve a higher accuracy than supervised models trained from
    scratch on the same amount of labeled data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，模型首先在包含数百万张图片的大规模数据集（如[ImageNet](https://image-net.org)）上进行训练。这个过程称为*预训练*，其主要目的是教会模型图片的基本特征，如边缘或颜色。然后，这些预训练模型可以在下游任务上进行微调，例如使用相对较少的标记示例（通常每类几百个）对花卉物种进行分类。微调模型通常比从头开始训练的监督模型在相同数量的标记数据上实现更高的准确性。
- en: Although transfer learning became the standard approach in computer vision,
    for many years it was not clear what the analogous pretraining process was for
    NLP. As a result, NLP applications typically required large amounts of labeled
    data to achieve high performance. And even then, that performance did not compare
    to what was achieved in the vision domain.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迁移学习已成为计算机视觉中的标准方法，但多年来，对于自然语言处理的类似预训练过程并不清楚。因此，NLP应用通常需要大量标记数据才能实现高性能。即使如此，其性能也无法与视觉领域所实现的性能相媲美。
- en: In 2017 and 2018, several research groups proposed new approaches that finally
    made transfer learning work for NLP. It started with an insight from researchers
    at OpenAI who obtained strong performance on a sentiment classification task by
    using features extracted from unsupervised pretraining.^([8](ch01.xhtml#idm46238727454704))
    This was followed by ULMFiT, which introduced a general framework to adapt pretrained
    LSTM models for various tasks.^([9](ch01.xhtml#idm46238727453120))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年和2018年，几个研究小组提出了新的方法，最终使得迁移学习在自然语言处理中起作用。这始于OpenAI研究人员的洞察，他们通过使用从无监督预训练中提取的特征，在情感分类任务上取得了强大的性能。^([8](ch01.xhtml#idm46238727454704))接着是ULMFiT，它引入了一个通用框架，用于调整预训练的LSTM模型以适应各种任务。^([9](ch01.xhtml#idm46238727453120))
- en: 'As illustrated in [Figure 1-8](#ulmfit), ULMFiT involves three main steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图1-8](#ulmfit)所示，ULMFiT包括三个主要步骤：
- en: Pretraining
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练
- en: 'The initial training objective is quite simple: predict the next word based
    on the previous words. This task is referred to as *language modeling*. The elegance
    of this approach lies in the fact that no labeled data is required, and one can
    make use of abundantly available text from sources such as Wikipedia.^([10](ch01.xhtml#idm46238727446112))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的训练目标非常简单：根据先前的单词预测下一个单词。这个任务被称为*语言建模*。这种方法的优雅之处在于不需要标记的数据，并且可以利用来自维基百科等来源的大量可用文本。^([10](ch01.xhtml#idm46238727446112))
- en: Domain adaptation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应
- en: Once the language model is pretrained on a large-scale corpus, the next step
    is to adapt it to the in-domain corpus (e.g., from Wikipedia to the IMDb corpus
    of movie reviews, as in [Figure 1-8](#ulmfit)). This stage still uses language
    modeling, but now the model has to predict the next word in the target corpus.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦语言模型在大规模语料库上预训练，下一步就是将其适应于领域语料库（例如，从维基百科到IMDb电影评论的语料库，如[图1-8](#ulmfit)所示）。这个阶段仍然使用语言建模，但现在模型必须预测目标语料库中的下一个单词。
- en: Fine-tuning
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 微调
- en: In this step, the language model is fine-tuned with a classification layer for
    the target task (e.g., classifying the sentiment of movie reviews in [Figure 1-8](#ulmfit)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，语言模型通过一个用于目标任务的分类层进行微调（例如，在[图1-8](#ulmfit)中对电影评论的情感进行分类）。
- en: '![ulmfit](Images/nlpt_0108.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![ulmfit](Images/nlpt_0108.png)'
- en: Figure 1-8\. The ULMFiT process (courtesy of Jeremy Howard)
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. ULMFiT过程（由Jeremy Howard提供）
- en: 'By introducing a viable framework for pretraining and transfer learning in
    NLP, ULMFiT provided the missing piece to make transformers take off. In 2018,
    two transformers were released that combined self-attention with transfer learning:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入NLP中的预训练和迁移学习的可行框架，ULMFiT提供了使变压器起飞的缺失环节。2018年，发布了两个将自注意力与迁移学习相结合的变压器：
- en: GPT
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GPT
- en: Uses only the decoder part of the Transformer architecture, and the same language
    modeling approach as ULMFiT. GPT was pretrained on the BookCorpus,^([11](ch01.xhtml#idm46238728418272))
    which consists of 7,000 unpublished books from a variety of genres including Adventure,
    Fantasy, and Romance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用变压器架构的解码器部分，以及ULMFiT相同的语言建模方法。GPT是在BookCorpus上预训练的，^([11](ch01.xhtml#idm46238728418272))其中包括来自各种流派的7000本未发表的书籍，包括冒险、奇幻和浪漫。
- en: BERT
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: BERT
- en: Uses the encoder part of the Transformer architecture, and a special form of
    language modeling called *masked language modeling*. The objective of masked language
    modeling is to predict randomly masked words in a text. For example, given a sentence
    like “I looked at my `[MASK]` and saw that `[MASK]` was late.” the model needs
    to predict the most likely candidates for the masked words that are denoted by
    `[MASK]`. BERT was pretrained on the BookCorpus and English Wikipedia.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Transformer架构的编码器部分，以及一种特殊形式的语言建模称为*掩码语言建模*。掩码语言建模的目标是预测文本中随机掩码的单词。例如，给定一个句子“我看着我的`[MASK]`，看到`[MASK]`迟到了。”模型需要预测由`[MASK]`表示的掩码单词的最可能的候选项。BERT是在BookCorpus和英文维基百科上预训练的。
- en: GPT and BERT set a new state of the art across a variety of NLP benchmarks and
    ushered in the age of transformers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPT和BERT在各种NLP基准测试中树立了新的技术水平，并开启了变压器时代。
- en: However, with different research labs releasing their models in incompatible
    frameworks (PyTorch or TensorFlow), it wasn’t always easy for NLP practitioners
    to port these models to their own applications. With the release of ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers](https://oreil.ly/Z79jF), a unified API across more than 50 architectures
    was progressively built. This library catalyzed the explosion of research into
    transformers and quickly trickled down to NLP practitioners, making it easy to
    integrate these models into many real-life applications today. Let’s have a look!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于不同的研究实验室在不兼容的框架（PyTorch或TensorFlow）中发布其模型，对于NLP从业者来说，将这些模型移植到自己的应用程序并不总是容易的。随着![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers](https://oreil.ly/Z79jF)的发布，逐渐构建了超过50种架构的统一API。这个库催生了对变压器的研究爆炸，并迅速渗透到NLP从业者中，使得将这些模型整合到今天的许多实际应用程序中变得容易。让我们来看看！
- en: 'Hugging Face Transformers: Bridging the Gap'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face Transformers：弥合差距
- en: 'Applying a novel machine learning architecture to a new task can be a complex
    undertaking, and usually involves the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将新的机器学习架构应用于新任务可能是一个复杂的过程，通常涉及以下步骤：
- en: Implement the model architecture in code, typically based on PyTorch or TensorFlow.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码中实现模型架构，通常基于PyTorch或TensorFlow。
- en: Load the pretrained weights (if available) from a server.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从服务器加载预训练的权重（如果可用）。
- en: Preprocess the inputs, pass them through the model, and apply some task-specific
    postprocessing.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理输入，将其通过模型，并应用一些特定于任务的后处理。
- en: Implement dataloaders and define loss functions and optimizers to train the
    model.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现数据加载器，并定义损失函数和优化器来训练模型。
- en: Each of these steps requires custom logic for each model and task. Traditionally
    (but not always!), when research groups publish a new article, they will also
    release the code along with the model weights. However, this code is rarely standardized
    and often requires days of engineering to adapt to new use cases.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都需要为每个模型和任务编写自定义逻辑。传统上（但并非总是如此！），当研究小组发布新文章时，他们通常会连同模型权重一起发布代码。然而，这些代码很少是标准化的，通常需要数天的工程来适应新的用例。
- en: This is where ![nlpt_pin01](Images/nlpt_pin01.png) Transformers comes to the
    NLP practitioner’s rescue! It provides a standardized interface to a wide range
    of transformer models as well as code and tools to adapt these models to new use
    cases. The library currently supports three major deep learning frameworks (PyTorch,
    TensorFlow, and JAX) and allows you to easily switch between them. In addition,
    it provides task-specific heads so you can easily fine-tune transformers on downstream
    tasks such as text classification, named entity recognition, and question answering.
    This reduces the time it takes a practitioner to train and test a handful of models
    from a week to a single afternoon!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是![nlpt_pin01](Images/nlpt_pin01.png) Transformers对NLP从业者的救援之处！它为各种变压器模型提供了标准化接口，以及用于适应这些模型到新用例的代码和工具。该库目前支持三个主要的深度学习框架（PyTorch、TensorFlow和JAX），并允许您轻松地在它们之间切换。此外，它提供了任务特定的头部，因此您可以轻松地在下游任务上微调变压器，如文本分类、命名实体识别和问题回答。这减少了从实践者训练和测试一些模型所需的时间，从一周减少到一个下午！
- en: You’ll see this for yourself in the next section, where we show that with just
    a few lines of code, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers can be
    applied to tackle some of the most common NLP applications that you’re likely
    to encounter in the wild.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在下一节中亲自看到这一点，在那里我们将展示，只需几行代码，![nlpt_pin01](Images/nlpt_pin01.png) Transformers就可以应用于解决您在实际中可能遇到的一些最常见的NLP应用。
- en: A Tour of Transformer Applications
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器应用之旅
- en: 'Every NLP task starts with a piece of text, like the following made-up customer
    feedback about a certain online order:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个NLP任务都始于一段文本，比如以下关于某个在线订单的虚构客户反馈：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Depending on your application, the text you’re working with could be a legal
    contract, a product description, or something else entirely. In the case of customer
    feedback, you would probably like to know whether the feedback is positive or
    negative. This task is called *sentiment analysis* and is part of the broader
    topic of *text classification* that we’ll explore in [Chapter 2](ch02.xhtml#chapter_classification).
    For now, let’s have a look at what it takes to extract the sentiment from our
    piece of text using ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的应用，您正在处理的文本可能是法律合同、产品描述或其他内容。在客户反馈的情况下，您可能想知道反馈是积极的还是消极的。这项任务被称为*情感分析*，是*文本分类*的更广泛主题的一部分，我们将在[第2章](ch02.xhtml#chapter_classification)中探讨。现在，让我们看看如何从我们的文本中提取情感，使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers。
- en: Text Classification
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类
- en: As we’ll see in later chapters, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    has a layered API that allows you to interact with the library at various levels
    of abstraction. In this chapter we’ll start with *pipelines*, which abstract away
    all the steps needed to convert raw text into a set of predictions from a fine-tuned
    model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在后面的章节中看到的，![nlpt_pin01](Images/nlpt_pin01.png) Transformers具有分层API，允许您以不同的抽象级别与库进行交互。在本章中，我们将从*管道*开始，它将所有将原始文本转换为经过精细调整的模型一系列预测所需的步骤抽象出来。
- en: 'In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we instantiate a pipeline
    by calling the `pipeline()` function and providing the name of the task we are
    interested in:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，我们通过调用`pipeline()`函数并提供我们感兴趣的任务的名称来实例化一个管道：
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first time you run this code you’ll see a few progress bars appear because
    the pipeline automatically downloads the model weights from the [Hugging Face
    Hub](https://oreil.ly/zLK11). The second time you instantiate the pipeline, the
    library will notice that you’ve already downloaded the weights and will use the
    cached version instead. By default, the `text-classification` pipeline uses a
    model that’s designed for sentiment analysis, but it also supports multiclass
    and multilabel classification.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行此代码时，您将看到一些进度条出现，因为管道会自动从[Hugging Face Hub](https://oreil.ly/zLK11)下载模型权重。第二次实例化管道时，库将注意到您已经下载了权重，并将使用缓存版本。默认情况下，`text-classification`管道使用的是专为情感分析设计的模型，但它也支持多类和多标签分类。
- en: 'Now that we have our pipeline, let’s generate some predictions! Each pipeline
    takes a string of text (or a list of strings) as input and returns a list of predictions.
    Each prediction is a Python dictionary, so we can use Pandas to display them nicely
    as a `Data⁠Frame`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的管道，让我们生成一些预测！每个管道都将一个文本字符串（或字符串列表）作为输入，并返回一系列预测。每个预测都是一个Python字典，所以我们可以使用Pandas将它们漂亮地显示为`Data⁠Frame`：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|  | label | score |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 标签 | 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | NEGATIVE | 0.901546 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 0 | NEGATIVE | 0.901546 |'
- en: In this case the model is very confident that the text has a negative sentiment,
    which makes sense given that we’re dealing with a complaint from an angry customer!
    Note that for sentiment analysis tasks the pipeline only returns one of the `POSITIVE`
    or `NEGATIVE` labels, since the other can be inferred by computing `1-score`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型非常确信文本具有消极情绪，这是有道理的，因为我们正在处理一个愤怒客户的投诉！请注意，对于情感分析任务，管道只返回`POSITIVE`或`NEGATIVE`标签中的一个，因为另一个可以通过计算`1-score`来推断。
- en: Let’s now take a look at another common task, identifying named entities in
    text.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看另一个常见的任务，识别文本中的命名实体。
- en: Named Entity Recognition
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Predicting the sentiment of customer feedback is a good first step, but you
    often want to know if the feedback was about a particular item or service. In
    NLP, real-world objects like products, places, and people are called *named entities*,
    and extracting them from text is called *named entity recognition* (NER). We can
    apply NER by loading the corresponding pipeline and feeding our customer review
    to it:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 预测客户反馈的情绪是一个很好的第一步，但通常您想知道反馈是否是关于特定的项目或服务。在自然语言处理中，像产品、地点和人这样的现实世界对象被称为*命名实体*，从文本中提取它们被称为*命名实体识别*（NER）。我们可以通过加载相应的管道并将我们的客户评论提供给它来应用NER：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  | entity_group | score | word | start | end |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 实体组 | 分数 | 单词 | 开始 | 结束 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | ORG | 0.879010 | Amazon | 5 | 11 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 0 | ORG | 0.879010 | 亚马逊 | 5 | 11 |'
- en: '| 1 | MISC | 0.990859 | Optimus Prime | 36 | 49 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1 | MISC | 0.990859 | 奥普蒂默斯·普莱姆 | 36 | 49 |'
- en: '| 2 | LOC | 0.999755 | Germany | 90 | 97 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2 | LOC | 0.999755 | 德国 | 90 | 97 |'
- en: '| 3 | MISC | 0.556569 | Mega | 208 | 212 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 3 | MISC | 0.556569 | Mega | 208 | 212 |'
- en: '| 4 | PER | 0.590256 | ##tron | 212 | 216 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 4 | PER | 0.590256 | ##tron | 212 | 216 |'
- en: '| 5 | ORG | 0.669692 | Decept | 253 | 259 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ORG | 0.669692 | 欺诈者 | 253 | 259 |'
- en: '| 6 | MISC | 0.498350 | ##icons | 259 | 264 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 6 | MISC | 0.498350 | ##icons | 259 | 264 |'
- en: '| 7 | MISC | 0.775361 | Megatron | 350 | 358 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 7 | MISC | 0.775361 | Megatron | 350 | 358 |'
- en: '| 8 | MISC | 0.987854 | Optimus Prime | 367 | 380 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 8 | MISC | 0.987854 | 奥普蒂默斯·普莱姆 | 367 | 380 |'
- en: '| 9 | PER | 0.812096 | Bumblebee | 502 | 511 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 9 | PER | 0.812096 | 大黄蜂 | 502 | 511 |'
- en: 'You can see that the pipeline detected all the entities and also assigned a
    category such as `ORG` (organization), `LOC` (location), or `PER` (person) to
    each of them. Here we used the `aggregation_strategy` argument to group the words
    according to the model’s predictions. For example, the entity “Optimus Prime”
    is composed of two words, but is assigned a single category: `MISC` (miscellaneous).
    The scores tell us how confident the model was about the entities it identified.
    We can see that it was least confident about “Decepticons” and the first occurrence
    of “Megatron”, both of which it failed to group as a single entity.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到管道检测到了所有实体，并为每个实体分配了类别，例如`ORG`（组织）、`LOC`（位置）或`PER`（人）。在这里，我们使用了`aggregation_strategy`参数根据模型的预测对单词进行分组。例如，实体“奥普蒂默斯·普莱姆”由两个单词组成，但被分配了一个单一的类别：`MISC`（杂项）。分数告诉我们模型对其识别的实体有多自信。我们可以看到它对“欺诈者”和“Megatron”的第一次出现的识别最不自信，它们都未能被分组为单个实体。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: See those weird hash symbols (`#`) in the `word` column in the previous table?
    These are produced by the model’s *tokenizer*, which splits words into atomic
    units called *tokens*. You’ll learn all about tokenization in [Chapter 2](ch02.xhtml#chapter_classification).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 看到上一个表格中`word`列中的奇怪的井号符号(`#`)了吗？这些是模型的*分词器*产生的，它将单词分割成称为*标记*的原子单位。您将在[第2章](ch02.xhtml#chapter_classification)中学习有关标记化的所有内容。
- en: Extracting all the named entities in a text is nice, but sometimes we would
    like to ask more targeted questions. This is where we can use *question answering*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提取文本中的所有命名实体是不错的，但有时我们想提出更有针对性的问题。这就是我们可以使用*问答*的地方。
- en: Question Answering
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问答
- en: 'In question answering, we provide the model with a passage of text called the
    *context*, along with a question whose answer we’d like to extract. The model
    then returns the span of text corresponding to the answer. Let’s see what we get
    when we ask a specific question about our customer feedback:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在问答中，我们向模型提供了一段文本，称为*上下文*，以及一个我们想要提取答案的问题。然后模型返回对应于答案的文本范围。让我们看看当我们针对客户反馈提出具体问题时会得到什么：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | score | start | end | answer |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 分数 | 开始 | 结束 | 答案 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.631291 | 335 | 358 | an exchange of Megatron |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.631291 | 335 | 358 | 与Megatron的交换 |'
- en: We can see that along with the answer, the pipeline also returned `start` and
    `end` integers that correspond to the character indices where the answer span
    was found (just like with NER tagging). There are several flavors of question
    answering that we will investigate in [Chapter 7](ch07.xhtml#chapter_qa), but
    this particular kind is called *extractive question answering* because the answer
    is extracted directly from the text.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，除了答案之外，管道还返回了`start`和`end`整数，这些整数对应于找到答案跨度的字符索引（就像NER标记一样）。我们将在[第7章](ch07.xhtml#chapter_qa)中调查几种问答的变体，但这种特定的称为*抽取式问答*，因为答案直接从文本中提取。
- en: With this approach you can read and extract relevant information quickly from
    a customer’s feedback. But what if you get a mountain of long-winded complaints
    and you don’t have the time to read them all? Let’s see if a summarization model
    can help!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，您可以快速阅读并从客户的反馈中提取相关信息。但是，如果您收到一大堆冗长的投诉，而您没有时间全部阅读，该怎么办呢？让我们看看摘要模型是否能帮上忙！
- en: Summarization
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The goal of text summarization is to take a long text as input and generate
    a short version with all the relevant facts. This is a much more complicated task
    than the previous ones since it requires the model to *generate* coherent text.
    In what should be a familiar pattern by now, we can instantiate a summarization
    pipeline as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要的目标是将长文本作为输入，并生成一个包含所有相关事实的简短版本。这比以前的任务要复杂得多，因为它要求模型*生成*连贯的文本。现在应该是一个熟悉的模式，我们可以像下面这样实例化一个摘要管道：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This summary isn’t too bad! Although parts of the original text have been copied,
    the model was able to capture the essence of the problem and correctly identify
    that “Bumblebee” (which appeared at the end) was the author of the complaint.
    In this example you can also see that we passed some keyword arguments like `max_length`
    and `clean_up_tokenization_spaces` to the pipeline; these allow us to tweak the
    outputs at runtime.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个摘要还不错！虽然原始文本的部分被复制，但模型能够捕捉到问题的本质，并正确识别“大黄蜂”（出现在末尾）是投诉的作者。在这个例子中，您还可以看到我们传递了一些关键字参数，如`max_length`和`clean_up_tokenization_spaces`给管道；这些允许我们在运行时调整输出。
- en: But what happens when you get feedback that is in a language you don’t understand?
    You could use Google Translate, or you can use your very own transformer to translate
    it for you!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当您收到一种您不懂的语言的反馈时会发生什么？您可以使用谷歌翻译，或者您可以使用自己的转换器为您翻译！
- en: Translation
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译
- en: 'Like summarization, translation is a task where the output consists of generated
    text. Let’s use a translation pipeline to translate an English text to German:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与摘要类似，翻译是一个输出生成文本的任务。让我们使用翻译管道将英文文本翻译成德文：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Again, the model produced a very good translation that correctly uses German’s
    formal pronouns, like “Ihrem” and “Sie.” Here we’ve also shown how you can override
    the default model in the pipeline to pick the best one for your application—and
    you can find models for thousands of language pairs on the Hugging Face Hub. Before
    we take a step back and look at the whole Hugging Face ecosystem, let’s examine
    one last application.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，该模型产生了一个非常好的翻译，正确使用了德语的正式代词，如“Ihrem”和“Sie”。在这里，我们还展示了如何覆盖管道中的默认模型，以选择最适合您应用的模型——您可以在Hugging
    Face Hub上找到成千上万种语言对的模型。在我们退后一步，看看整个Hugging Face生态系统之前，让我们再看一个应用。
- en: Text Generation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本生成
- en: 'Let’s say you would like to be able to provide faster replies to customer feedback
    by having access to an autocomplete function. With a text generation model you
    can do this as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望能够通过访问自动完成功能更快地回复客户反馈。使用文本生成模型，您可以这样做：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OK, maybe we wouldn’t want to use this completion to calm Bumblebee down, but
    you get the general idea.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，也许我们不想使用这个完成来安抚大黄蜂，但您大致明白了。
- en: Now that you’ve seen a few cool applications of transformer models, you might
    be wondering where the training happens. All of the models that we’ve used in
    this chapter are publicly available and already fine-tuned for the task at hand.
    In general, however, you’ll want to fine-tune models on your own data, and in
    the following chapters you will learn how to do just that.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了一些转换器模型的很酷的应用，您可能想知道训练是在哪里进行的。本章中使用的所有模型都是公开可用的，并且已经针对手头的任务进行了微调。然而，一般来说，您可能希望在自己的数据上微调模型，在接下来的章节中，您将学习如何做到这一点。
- en: But training a model is just a small piece of any NLP project—being able to
    efficiently process data, share results with colleagues, and make your work reproducible
    are key components too. Fortunately, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    is surrounded by a big ecosystem of useful tools that support much of the modern
    machine learning workflow. Let’s take a look.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，训练模型只是NLP项目的一小部分——能够高效处理数据、与同事分享结果以及使您的工作可复制也是关键组成部分。幸运的是，![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers周围有一个大型生态系统，支持现代机器学习工作流的许多有用工具。让我们来看看。
- en: The Hugging Face Ecosystem
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face生态系统
- en: 'What started with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers has quickly
    grown into a whole ecosystem consisting of many libraries and tools to accelerate
    your NLP and machine learning projects. The Hugging Face ecosystem consists of
    mainly two parts: a family of libraries and the Hub, as shown in [Figure 1-9](#ecosystem).
    The libraries provide the code while the Hub provides the pretrained model weights,
    datasets, scripts for the evaluation metrics, and more. In this section we’ll
    have a brief look at the various components. We’ll skip ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, as we’ve already discussed it and we will see a lot more of it throughout
    the course of the book.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从![nlpt_pin01](Images/nlpt_pin01.png) Transformers开始，迅速发展成为一个由许多库和工具组成的整个生态系统，以加速您的NLP和机器学习项目。
    Hugging Face生态系统主要由两部分组成：一系列库和Hub，如[图1-9](#ecosystem)所示。库提供代码，而Hub提供预训练模型权重、数据集、用于评估指标的脚本等。在本节中，我们将简要介绍各个组件。我们将跳过![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers，因为我们已经讨论过它，并且在本书的过程中还会看到更多。
- en: '![ecosystem](Images/nlpt_0109.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![ecosystem](Images/nlpt_0109.png)'
- en: Figure 1-9\. An overview of the Hugging Face ecosystem
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. Hugging Face生态系统概述
- en: The Hugging Face Hub
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face Hub
- en: As outlined earlier, transfer learning is one of the key factors driving the
    success of transformers because it makes it possible to reuse pretrained models
    for new tasks. Consequently, it is crucial to be able to load pretrained models
    quickly and run experiments with them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，迁移学习是推动转换器成功的关键因素之一，因为它使得可以重用预训练模型来处理新任务。因此，能够快速加载预训练模型并进行实验至关重要。
- en: The Hugging Face Hub hosts over 20,000 freely available models. As shown in
    [Figure 1-10](#hub-overview), there are filters for tasks, frameworks, datasets,
    and more that are designed to help you navigate the Hub and quickly find promising
    candidates. As we’ve seen with the pipelines, loading a promising model in your
    code is then literally just one line of code away. This makes experimenting with
    a wide range of models simple, and allows you to focus on the domain-specific
    parts of your project.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub托管了超过20,000个免费可用的模型。如[图1-10](#hub-overview)所示，有任务、框架、数据集等过滤器，旨在帮助您浏览Hub并快速找到有前途的候选模型。正如我们在管道中看到的那样，在您的代码中加载一个有前途的模型实际上只是一行代码的距离。这使得尝试各种模型变得简单，并且让您可以专注于项目的领域特定部分。
- en: '![hub-overview](Images/nlpt_0110.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![hub-overview](Images/nlpt_0110.png)'
- en: Figure 1-10\. The Models page of the Hugging Face Hub, showing filters on the
    left and a list of models on the right
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. Hugging Face Hub的模型页面，左侧显示了过滤器，右侧显示了模型列表
- en: In addition to model weights, the Hub also hosts datasets and scripts for computing
    metrics, which let you reproduce published results or leverage additional data
    for your application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型权重，Hub还托管数据集和用于计算指标的脚本，这些脚本可以让您重现已发布的结果或利用额外的数据进行应用。
- en: The Hub also provides *model* and *dataset* *cards* to document the contents
    of models and datasets and help you make an informed decision about whether they’re
    the right ones for you. One of the coolest features of the Hub is that you can
    try out any model directly through the various task-specific interactive widgets
    as shown in [Figure 1-11](#hub-model-card).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Hub还提供*模型*和*数据集* *卡片*，以记录模型和数据集的内容，并帮助您对是否适合您做出明智的决定。Hub最酷的功能之一是，您可以通过各种特定任务的交互式小部件直接尝试任何模型，如[图1-11](#hub-model-card)所示。
- en: '![hub-model-card](Images/nlpt_0111.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![hub-model-card](Images/nlpt_0111.png)'
- en: 'Figure 1-11\. An example model card from the Hugging Face Hub: the inference
    widget, which allows you to interact with the model, is shown on the right'
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11\. Hugging Face Hub的示例模型卡片：右侧显示了允许您与模型交互的推理小部件
- en: Let’s continue our tour with ![nlpt_pin01](Images/nlpt_pin01.png) Tokenizers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的旅程与Tokenizers。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[PyTorch](https://oreil.ly/AyTYC) and [TensorFlow](https://oreil.ly/JOKgq)
    also offer hubs of their own and are worth checking out if a particular model
    or dataset is not available on the Hugging Face Hub.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch和TensorFlow也提供了自己的中心，并且值得检查，如果Hugging Face Hub上没有特定的模型或数据集。
- en: Hugging Face Tokenizers
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face标记化器
- en: Behind each of the pipeline examples that we’ve seen in this chapter is a tokenization
    step that splits the raw text into smaller pieces called tokens. We’ll see how
    this works in detail in [Chapter 2](ch02.xhtml#chapter_classification), but for
    now it’s enough to understand that tokens may be words, parts of words, or just
    characters like punctuation. Transformer models are trained on numerical representations
    of these tokens, so getting this step right is pretty important for the whole
    NLP project!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们看到的每个管道示例背后都有一个标记化步骤，将原始文本分割成称为标记的较小部分。我们将在第2章中详细介绍这是如何工作的，但现在理解标记可能是单词、单词的部分，或者只是标点符号等字符就足够了。变压器模型是在这些标记的数值表示上进行训练的，因此正确进行这一步对整个NLP项目非常重要！
- en: '![nlpt_pin01](Images/nlpt_pin01.png) [Tokenizers](https://oreil.ly/Z79jF) provides
    many tokenization strategies and is extremely fast at tokenizing text thanks to
    its Rust backend.^([12](ch01.xhtml#idm46238728732576)) It also takes care of all
    the pre- and postprocessing steps, such as normalizing the inputs and transforming
    the model outputs to the required format. With ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers, we can load a tokenizer in the same way we can load pretrained model
    weights with ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Transformers.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenizers提供许多标记化策略，并且由于其Rust后端，它在标记化文本方面非常快速。它还负责所有的预处理和后处理步骤，例如规范化输入和将模型输出转换为所需的格式。使用Tokenizers，我们可以像加载预训练模型权重一样加载标记器。
- en: We need a dataset and metrics to train and evaluate models, so let’s take a
    look at ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Datasets, which is in charge of
    that aspect.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个数据集和指标来训练和评估模型，所以让我们来看看负责这方面的数据集。
- en: Hugging Face Datasets
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face数据集
- en: Loading, processing, and storing datasets can be a cumbersome process, especially
    when the datasets get too large to fit in your laptop’s RAM. In addition, you
    usually need to implement various scripts to download the data and transform it
    into a standard format.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 加载、处理和存储数据集可能是一个繁琐的过程，特别是当数据集变得太大而无法容纳在您的笔记本电脑的RAM中时。此外，通常需要实现各种脚本来下载数据并将其转换为标准格式。
- en: '![nlpt_pin01](Images/nlpt_pin01.png) [Datasets](https://oreil.ly/959YT) simplifies
    this process by providing a standard interface for thousands of datasets that
    can be found on the [Hub](https://oreil.ly/Rdhcu). It also provides smart caching
    (so you don’t have to redo your preprocessing each time you run your code) and
    avoids RAM limitations by leveraging a special mechanism called *memory mapping*
    that stores the contents of a file in virtual memory and enables multiple processes
    to modify a file more efficiently. The library is also interoperable with popular
    frameworks like Pandas and NumPy, so you don’t have to leave the comfort of your
    favorite data wrangling tools.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Datasets通过为数千个数据集提供标准接口来简化这个过程。它还提供智能缓存（这样您就不必每次运行代码时都重新进行预处理），并通过利用一种称为*内存映射*的特殊机制来避免RAM限制，该机制将文件的内容存储在虚拟内存中，并使多个进程更有效地修改文件。该库还与流行的框架如Pandas和NumPy兼容，因此您无需离开您喜爱的数据整理工具的舒适区。
- en: Having a good dataset and powerful model is worthless, however, if you can’t
    reliably measure the performance. Unfortunately, classic NLP metrics come with
    many different implementations that can vary slightly and lead to deceptive results.
    By providing the scripts for many metrics, ![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets helps make experiments more reproducible and the results more trustworthy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您无法可靠地衡量性能，拥有一个好的数据集和强大的模型是毫无价值的。不幸的是，经典的NLP指标有许多不同的实现，可能会略有不同，并导致误导性的结果。通过提供许多指标的脚本，Datasets有助于使实验更具再现性，结果更值得信赖。
- en: 'With the ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers, and ![nlpt_pin01](Images/nlpt_pin01.png) Datasets libraries we have
    everything we need to train our very own transformer models! However, as we’ll
    see in [Chapter 10](ch10.xhtml#chapter_fromscratch) there are situations where
    we need fine-grained control over the training loop. That’s where the last library
    of the ecosystem comes into play: ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有了Transformers、Tokenizers和Datasets库，我们有了训练自己的变压器模型所需的一切！然而，正如我们将在第10章中看到的那样，有些情况下我们需要对训练循环进行细粒度控制。这就是生态系统中最后一个库发挥作用的地方：Accelerate。
- en: Hugging Face Accelerate
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face加速
- en: If you’ve ever had to write your own training script in PyTorch, chances are
    that you’ve had some headaches when trying to port the code that runs on your
    laptop to the code that runs on your organization’s cluster. ![nlpt_pin01](Images/nlpt_pin01.png)
    [Accelerate](https://oreil.ly/iRfDe) adds a layer of abstraction to your normal
    training loops that takes care of all the custom logic necessary for the training
    infrastructure. This literally accelerates your workflow by simplifying the change
    of infrastructure when necessary.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您曾经不得不在PyTorch中编写自己的训练脚本，那么当尝试将在笔记本电脑上运行的代码移植到组织的集群上运行时，可能会遇到一些头痛。Accelerate为您的正常训练循环添加了一层抽象，负责处理训练基础设施所需的所有自定义逻辑。这实际上通过简化必要时的基础设施更改来加速您的工作流程。
- en: This sums up the core components of Hugging Face’s open source ecosystem. But
    before wrapping up this chapter, let’s take a look at a few of the common challenges
    that come with trying to deploy transformers in the real world.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了Hugging Face开源生态系统的核心组件。但在结束本章之前，让我们看一看在尝试在现实世界中部署变压器时会遇到的一些常见挑战。
- en: Main Challenges with Transformers
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器的主要挑战
- en: 'In this chapter we’ve gotten a glimpse of the wide range of NLP tasks that
    can be tackled with transformer models. Reading the media headlines, it can sometimes
    sound like their capabilities are limitless. However, despite their usefulness,
    transformers are far from being a silver bullet. Here are a few challenges associated
    with them that we will explore throughout the book:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经对可以使用变压器模型解决的各种自然语言处理任务有了一瞥。阅读媒体头条时，有时会觉得它们的能力是无限的。然而，尽管它们很有用，变压器远非灵丹妙药。以下是与它们相关的一些挑战，我们将在整本书中探讨：
- en: '*Language*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*语言*'
- en: NLP research is dominated by the English language. There are several models
    for other languages, but it is harder to find pretrained models for rare or low-resource
    languages. In [Chapter 4](ch04.xhtml#chapter_ner), we’ll explore multilingual
    transformers and their ability to perform zero-shot cross-lingual transfer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理研究主要以英语为主导。还有一些其他语言的模型，但很难找到稀有或低资源语言的预训练模型。在[第4章](ch04.xhtml#chapter_ner)中，我们将探讨多语言变压器及其进行零-shot跨语言转移的能力。
- en: '*Data availability*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据可用性*'
- en: Although we can use transfer learning to dramatically reduce the amount of labeled
    training data our models need, it is still a lot compared to how much a human
    needs to perform the task. Tackling scenarios where you have little to no labeled
    data is the subject of [Chapter 9](ch09.xhtml#chapter_fewlabels).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以使用迁移学习大大减少模型需要的标记训练数据量，但与人类执行任务所需的量相比，仍然是很多。解决标记数据很少或没有的情况是[第9章](ch09.xhtml#chapter_fewlabels)的主题。
- en: '*Working with long documents*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*处理长文档*'
- en: Self-attention works extremely well on paragraph-long texts, but it becomes
    very expensive when we move to longer texts like whole documents. Approaches to
    mitigate this are discussed in [Chapter 11](ch11.xhtml#chapter_future).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力在段落长的文本上效果非常好，但当我们转向整个文档等更长的文本时，成本就会变得非常高。我们将在[第11章](ch11.xhtml#chapter_future)中讨论缓解这一问题的方法。
- en: '*Opacity*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*不透明性*'
- en: As with other deep learning models, transformers are to a large extent opaque.
    It is hard or impossible to unravel “why” a model made a certain prediction. This
    is an especially hard challenge when these models are deployed to make critical
    decisions. We’ll explore some ways to probe the errors of transformer models in
    Chapters [2](ch02.xhtml#chapter_classification) and [4](ch04.xhtml#chapter_ner).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他深度学习模型一样，变压器在很大程度上是不透明的。很难或不可能解开模型为何做出某种预测的“原因”。当这些模型被部署用于做出关键决策时，这是一个特别困难的挑战。我们将在[第2章](ch02.xhtml#chapter_classification)和[第4章](ch04.xhtml#chapter_ner)中探讨一些探究变压器模型错误的方法。
- en: '*Bias*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*偏见*'
- en: Transformer models are predominantly pretrained on text data from the internet.
    This imprints all the biases that are present in the data into the models. Making
    sure that these are neither racist, sexist, or worse is a challenging task. We
    discuss some of these issues in more detail in [Chapter 10](ch10.xhtml#chapter_fromscratch).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型主要是在互联网文本数据上进行预训练的。这会将数据中存在的所有偏见都印刻到模型中。确保这些偏见既不是种族主义的、性别歧视的，也不是更糟糕的，是一项具有挑战性的任务。我们将在[第10章](ch10.xhtml#chapter_fromscratch)中更详细地讨论其中一些问题。
- en: Although daunting, many of these challenges can be overcome. As well as in the
    specific chapters mentioned, we will touch on these topics in almost every chapter
    ahead.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管令人生畏，许多这些挑战是可以克服的。除了特定提到的章节外，我们将在接下来的几乎每一章中涉及这些主题。
- en: Conclusion
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully, by now you are excited to learn how to start training and integrating
    these versatile models into your own applications! You’ve seen in this chapter
    that with just a few lines of code you can use state-of-the-art models for classification,
    named entity recognition, question answering, translation, and summarization,
    but this is really just the “tip of the iceberg.”
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到目前为止，您已经对如何开始训练和将这些多才多艺的模型集成到您自己的应用程序中感到兴奋！您在本章中已经看到，只需几行代码，您就可以使用最先进的模型进行分类、命名实体识别、问答、翻译和摘要，但这实际上只是“冰山一角”。
- en: In the following chapters you will learn how to adapt transformers to a wide
    range of use cases, such as building a text classifier, or a lightweight model
    for production, or even training a language model from scratch. We’ll be taking
    a hands-on approach, which means that for every concept covered there will be
    accompanying code that you can run on Google Colab or your own GPU machine.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将学习如何将变压器适应于各种用例，比如构建文本分类器，或者用于生产的轻量级模型，甚至从头开始训练语言模型。我们将采取实践方法，这意味着对于每个涵盖的概念，都会有相应的代码，您可以在Google
    Colab或您自己的GPU机器上运行。
- en: 'Now that we’re armed with the basic concepts behind transformers, it’s time
    to get our hands dirty with our first application: text classification. That’s
    the topic of the next chapter!'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了变压器背后的基本概念，是时候开始动手进行我们的第一个应用了：文本分类。这是下一章的主题！
- en: ^([1](ch01.xhtml#idm46238735114624-marker)) A. Vaswani et al., [“Attention Is
    All You Need”](https://arxiv.org/abs/1706.03762), (2017). This title was so catchy
    that no less than [50 follow-up papers](https://oreil.ly/wT8Ih) have included
    “all you need” in their titles!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#idm46238735114624-marker)) A. Vaswani et al., [“Attention Is
    All You Need”](https://arxiv.org/abs/1706.03762), (2017). 这个标题非常吸引人，以至于不少于[50篇后续论文](https://oreil.ly/wT8Ih)的标题中都包含了“all
    you need”！
- en: ^([2](ch01.xhtml#idm46238735221744-marker)) J. Howard and S. Ruder, [“Universal
    Language Model Fine-Tuning for Text Classification”](https://arxiv.org/abs/1801.06146),
    (2018).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#idm46238735221744-marker)) J. Howard and S. Ruder, [“Universal
    Language Model Fine-Tuning for Text Classification”](https://arxiv.org/abs/1801.06146),
    (2018).
- en: ^([3](ch01.xhtml#idm46238735216480-marker)) A. Radford et al., [“Improving Language
    Understanding by Generative Pre-Training”](https://openai.com/blog/language-unsupervised),
    (2018).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 10. A. Radford et al., [“Improving Language Understanding by Generative Pre-Training”](https://openai.com/blog/language-unsupervised),
    (2018).
- en: '^([4](ch01.xhtml#idm46238735215072-marker)) J. Devlin et al., [“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805),
    (2018).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '6. J. Devlin et al., [“BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding”](https://arxiv.org/abs/1810.04805), (2018).'
- en: ^([5](ch01.xhtml#idm46238728352576-marker)) I. Sutskever, O. Vinyals, and Q.V.
    Le, [“Sequence to Sequence Learning with Neural Networks”](https://arxiv.org/abs/1409.3215),
    (2014).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 1. I. Sutskever, O. Vinyals, and Q.V. Le, [“Sequence to Sequence Learning with
    Neural Networks”](https://arxiv.org/abs/1409.3215), (2014).
- en: ^([6](ch01.xhtml#idm46238728612528-marker)) D. Bahdanau, K. Cho, and Y. Bengio,
    [“Neural Machine Translation by Jointly Learning to Align and Translate”](https://arxiv.org/abs/1409.0473),
    (2014).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 3. D. Bahdanau, K. Cho, and Y. Bengio, [“Neural Machine Translation by Jointly
    Learning to Align and Translate”](https://arxiv.org/abs/1409.0473), (2014).
- en: ^([7](ch01.xhtml#idm46238728434256-marker)) Weights are the learnable parameters
    of a neural network.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 权重是神经网络的可学习参数。
- en: ^([8](ch01.xhtml#idm46238727454704-marker)) A. Radford, R. Jozefowicz, and I.
    Sutskever, [“Learning to Generate Reviews and Discovering Sentiment”](https://arxiv.org/abs/1704.01444),
    (2017).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 9. A. Radford, R. Jozefowicz, and I. Sutskever, [“Learning to Generate Reviews
    and Discovering Sentiment”](https://arxiv.org/abs/1704.01444), (2017).
- en: ^([9](ch01.xhtml#idm46238727453120-marker)) A related work at this time was
    ELMo (Embeddings from Language Models), which showed how pretraining LSTMs could
    produce high-quality word embeddings for downstream tasks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 在这个时候的相关工作是ELMo（来自语言模型的嵌入），它展示了如何通过预训练LSTMs可以为下游任务生成高质量的词嵌入。
- en: ^([10](ch01.xhtml#idm46238727446112-marker)) This is more true for English than
    for most of the world’s languages, where obtaining a large corpus of digitized
    text can be difficult. Finding ways to bridge this gap is an active area of NLP
    research and activism.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 这对于英语而言更为真实，而对于世界上大多数语言来说，获取大规模的数字化文本语料库可能会很困难。找到弥合这一差距的方法是自然语言处理研究和活动的一个活跃领域。
- en: '^([11](ch01.xhtml#idm46238728418272-marker)) Y. Zhu et al., [“Aligning Books
    and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading
    Books”](https://arxiv.org/abs/1506.06724), (2015).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '5. Y. Zhu et al., [“Aligning Books and Movies: Towards Story-Like Visual Explanations
    by Watching Movies and Reading Books”](https://arxiv.org/abs/1506.06724), (2015).'
- en: ^([12](ch01.xhtml#idm46238728732576-marker)) [Rust](https://rust-lang.org) is
    a high-performance programming language.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [Rust](https://rust-lang.org)是一种高性能的编程语言。
