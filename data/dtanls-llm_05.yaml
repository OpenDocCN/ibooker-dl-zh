- en: 4 Analyzing text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 分析文本数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Classifying text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Extracting information
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息提取
- en: Clustering documents
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档聚类
- en: Text data is ubiquitous and contains valuable information. For instance, think
    of newspaper articles, emails, reviews, or perhaps this book you are reading!
    However, analyzing text via computational means was difficult until only a few
    years ago. After all, unlike formal languages such as Python, natural language
    was not designed to be easy for computers to parse. The latest generation of language
    models enables text analysis at almost human levels for many popular tasks. In
    some cases, the performance of language models for text analysis and generation
    has even been shown, on average, to surpass the capabilities of humans [1].
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据无处不在，并包含有价值的信息。例如，想想报纸文章、电子邮件、评论，或者可能是您正在阅读的这本书！然而，直到几年前，通过计算手段分析文本仍然很困难。毕竟，与Python这样的形式化语言不同，自然语言并非旨在让计算机易于解析。最新一代的语言模型使许多流行任务的文本分析和生成几乎达到人类水平。在某些情况下，语言模型在文本分析和生成方面的性能甚至平均上超过了人类的能力
    [1]。
- en: 'In this chapter, we will see how to use large language models to analyze text.
    In certain ways, analyzing text data is a very “natural” application of language
    models. They have been trained on large amounts of text and can be applied directly
    for text analysis (i.e., without referring to external tools for the actual data
    analysis). This chapter covers several popular flavors of text analysis: classifying
    text documents, extracting tabular data from text, and clustering text documents
    into groups of semantically similar documents. For each of these use cases, we
    will see example code and discuss variants and extensions.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何使用大型语言模型来分析文本。在某种程度上，分析文本数据是语言模型的一个非常“自然”的应用。它们已经在大量文本上进行了训练，可以直接应用于文本分析（即，无需参考外部工具进行实际数据分析）。本章涵盖了文本分析的几种流行类型：对文本文档进行分类、从文本中提取表格数据以及将文本文档聚类成语义相似的文档组。对于这些用例中的每一个，我们将看到示例代码并讨论变体和扩展。
- en: Classification, information extraction, and clustering are three important types
    of text analysis but by no means the only ones you may need in practice. However,
    working through the examples in this chapter will enable you to create custom
    data-processing pipelines for text data based on language models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分类、信息提取和聚类是三种重要的文本分析类型，但绝不是您在实践中所需要的唯一类型。然而，通过本章的示例，您将能够根据语言模型创建自定义的文本数据处理管道。
- en: 4.1 Preliminaries
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 前言
- en: Let’s make sure your system is set up properly for the example projects. The
    following examples use OpenAI’s GPT model series, accessed via OpenAI’s Python
    library. This library was discussed in detail in chapter 3\. Make sure to follow
    the instructions in chapter 3 to be able to execute the example code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的系统已正确设置以用于示例项目。以下示例使用OpenAI的GPT模型系列，通过OpenAI的Python库访问。该库在第3章中已详细讨论。请确保遵循第3章中的说明，以便能够执行示例代码。
- en: Warning OpenAI’s Python library is changing quickly. The code in this chapter
    has been tested with version 1.29 of the OpenAI Python library but may not work
    with different versions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 OpenAI的Python库正在快速变化。本章中的代码已使用OpenAI Python库的1.29版本进行测试，但可能不适用于不同版本。
- en: 'Besides the OpenAI library, we will use the popular `pandas` library. `pandas`
    is a popular library for handling tabular data (which we will use as input and
    output format). We will only use basic functionality from that library and explain
    the corresponding commands as they occur in the code. Make sure `pandas` is installed
    (e.g., try `import pandas` in the Python interpreter); if it isn’t, install it
    by entering the following command in the terminal:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了OpenAI库之外，我们还将使用流行的`pandas`库。`pandas`是一个用于处理表格数据（我们将用作输入和输出格式）的流行库。我们只将从该库中使用基本功能，并在代码中相应地解释命令。请确保已安装`pandas`（例如，在Python解释器中尝试`import
    pandas`）；如果没有安装，请在终端中输入以下命令进行安装：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Finally, for the last section in this chapter, you will need the clustering
    algorithms from the `scikit-learn` library. Run the following command in the terminal
    to install the appropriate version:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本章的最后部分，您将需要`scikit-learn`库中的聚类算法。在终端中运行以下命令以安装适当的版本：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The following sections contain code for three mini-projects that use language
    models for text analysis. No need to type in the code—you can find all the code
    on the book’s companion website in the resource section for this chapter. Although
    you can execute the code on your own data, this book comes with a couple of sample
    data sets we use in the examples (also on the companion website). And now it’s
    time to use language models for text classification!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节包含使用语言模型进行文本分析的三个迷你项目的代码。无需输入代码——你可以在本书的配套网站上找到所有代码，在本书的配套网站上找到本章的资源部分。虽然你可以在自己的数据上执行代码，但本书附带了一些我们在示例中使用的样本数据集（也在配套网站上）。现在，是时候使用语言模型进行文本分类了！
- en: 4.2 Classification
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 分类
- en: So here you are, planning your Saturday evening and deliberating whether to
    go and see the newest installation of your favorite movie franchise. But is it
    worth it? Your social media feeds keep filling up with comments from your friends
    (and your friend’s friends), expanding on their movie experiences. You could browse
    through them manually, reading each one to get a better sense of whether the majority
    opinion about the movie is positive or negative. But who has time to do that?
    Can’t language models help us to automate this task?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你现在正在计划你的周六晚上，犹豫是否去看你最喜欢的电影系列的最新作品。但是，这值得吗？你的社交媒体动态不断被你的朋友（以及你朋友的朋友）的评论填满，他们在分享他们的观影体验。你可以手动浏览这些评论，逐个阅读以更好地了解大家对电影的普遍看法是正面还是负面。但是，谁有那么多时间来做这件事呢？语言模型不能帮助我们自动化这项任务吗？
- en: 'Indeed they can. What we have here is an instance of one of the most classic
    text-processing problems: we have a text and want to classify it, mapping it to
    one of a fixed set of categories. In this case, the text to classify is a movie
    review. We want to classify it as positive (i.e., the writer thinks it was a great
    movie, and you should go see it!) or negative (save your money!). That means we
    have two categories. Table [4.1](#tab__moviereviews) shows extracts from a few
    example reviews with the associated class labels. A review praising a movie as
    “well realized” is clearly positive, whereas one describing the movie as “obviously
    weak, cheap” is negative. You can find these and a few other reviews in a corresponding
    file on the book’s companion website.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以。这里是我们最经典的文本处理问题之一的一个实例：我们有一个文本，并希望将其分类，映射到一组固定的类别中。在这种情况下，要分类的文本是电影评论。我们希望将其分类为正面（即，作者认为这是一部伟大的电影，你应该去看！）或负面（省下你的钱！）。这意味着我们有两个类别。表
    [4.1](#tab__moviereviews) 展示了一些示例评论及其相关的类别标签。赞扬电影“再现得很好”的评论显然是正面的，而描述电影为“显然薄弱、廉价”的评论是负面的。你可以在本书的配套网站上找到这些评论和一些其他评论。
- en: Table 4.1 Extracts from movie reviews and associated class labels
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.1 电影评论及其相关的类别标签
- en: '| **Review** | **Class** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **评论** | **类别** |'
- en: '| First of all this movie is a piece of reality very well realized artistically.
    ... | Positive |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 首先，这部电影在艺术上非常真实地再现了现实。 ... | 正面 |'
- en: '| Re-titled “Gangs, Inc.”, this is an obviously weak, cheap mobster melodrama.
    ... | Negative |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 重命名为“Gangs, Inc.”，这是一部显然薄弱、廉价的黑帮情节剧。 ... | 负面 |'
- en: Classifying movie reviews is only one of many use cases for text classification.
    As another example, imagine trying to sort through your email inbox. Wouldn’t
    it be nice to automatically classify emails based on their content (e.g., using
    custom categories such as Work, Hobby, Childcare, etc.)? That’s yet another instance
    of text classification, this time with more than two categories. As a final example,
    imagine that you’re creating a website that enables users to leave free-text comments.
    Of course, you don’t want to show potentially offensive comments and would like
    to filter them out automatically. Again, that means you’re classifying text comments
    into one of two categories (Offensive and Inoffensive). We will now see how language
    models can easily be used for each scenario.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将电影评论进行分类只是文本分类的众多用例之一。例如，想象一下尝试整理你的电子邮件收件箱。自动根据内容（例如，使用自定义类别，如工作、爱好、育儿等）对电子邮件进行分类，这不是很好吗？这又是文本分类的一个例子，这次有超过两个类别。作为最后一个例子，想象一下你正在创建一个允许用户留下自由文本评论的网站。当然，你不想展示可能冒犯性的评论，并希望自动过滤它们。这意味着你正在将文本评论分类为两个类别之一（冒犯性和非冒犯性）。现在，我们将看到语言模型如何轻松地应用于每个场景。
- en: 4.2.1 Overview
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 概述
- en: We’ll focus on classifying movie reviews (or, really, any type of review) into
    Positive (great movie!) and Negative (stay home!) reviews. For that, we’ll use
    OpenAI’s language models. We’ll assume that we have collected reviews to classify
    in a file on disk. The code we develop will iterate over all reviews, classify
    each using the language model, and return the classification result for each review.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于将电影评论（或实际上，任何类型的评论）分类为正面评论（好电影！）和负面评论（待在家里！）。为此，我们将使用OpenAI的语言模型。我们假设我们已经收集了要分类的评论，并将它们存储在磁盘上的一个文件中。我们开发的代码将遍历所有评论，使用语言模型对每个评论进行分类，并返回每个评论的分类结果。
- en: 'But how can we classify reviews? We will use OpenAI’s Python library, presented
    in chapter 3\. For each review to classify, we will first generate a prompt. The
    prompt describes a task to a language model. In our case, that task assigns a
    review to one of our two categories (Positive or Negative). For instance, consider
    the following prompt as an example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何对评论进行分类呢？我们将使用第 3 章中介绍的 OpenAI 的 Python 库。对于要分类的每个评论，我们首先生成一个提示。该提示描述了一个任务给语言模型。在我们的情况下，该任务将评论分配到我们的两个类别之一（正面或负面）。例如，考虑以下提示作为示例：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Review'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 评论'
- en: '#2 Question'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 问题'
- en: '#3 Output format'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出格式'
- en: This prompt contains the review to classify (**1**), a question describing the
    classification task (**2**), and a final statement describing the desired output
    format (**3**). We will construct prompts of this type for each review, send the
    prompt to the language model, and (hopefully) get back one of the two possible
    answers (Positive or Negative). Figure [4.1](#fig__classificationOverview) illustrates
    the high-level classification process for each review.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此提示包含要分类的评论（**1**），一个描述分类任务的提问（**2**），以及一个描述所需输出格式的最终陈述（**3**）。我们将为每个评论构建此类提示，将其发送到语言模型，并（希望）得到两个可能答案之一（正面或负面）。图
    [4.1](#fig__classificationOverview) 展示了每个评论的高级分类过程。
- en: '![figure](../Images/CH04_F01_Trummer.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F01_Trummer.png)'
- en: Figure 4.1 For each review, we generate a prompt that contains the review, together
    with instructions describing the classification task. Given the prompt as input,
    the language model outputs a class label for the review.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1 对于每个评论，我们生成一个包含评论以及描述分类任务的指令的提示。给定提示作为输入，语言模型输出评论的类别标签。
- en: 4.2.2 Creating prompts
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 创建提示
- en: Given a review, we generate a prompt instructing the language model to classify
    it. All the prompts we generate for classification follow the same prompt template.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个评论，我们生成一个提示，指示语言模型对其进行分类。我们为分类生成的所有提示都遵循相同的提示模板。
- en: 'Reminder: What is a prompt template?'
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提醒：什么是提示模板？
- en: We briefly mentioned prompt templates in chapter 1\. A prompt template is a
    text that contains placeholders. By substituting actual text for these placeholders,
    we obtain a prompt that we can send to the language model. We also say that a
    prompt *instantiates* a prompt template if the prompt can be obtained by substituting
    the template’s placeholders.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一章中简要提到了提示模板。提示模板是一种包含占位符的文本。通过用实际文本替换这些占位符，我们可以获得可以发送到语言模型的提示。我们还称，如果提示可以通过替换模板的占位符来获得，则提示“实例化”了提示模板。
- en: 'The example prompt from the previous section instantiates the following prompt
    template:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中的示例提示实例化了以下提示模板：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Review (placeholder)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 评论（占位符）'
- en: '#2 Question'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 问题'
- en: '#3 Output format'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出格式'
- en: 'Our template contains only a single placeholder: the text of the review to
    classify (**1**). For each review, we will replace this placeholder with the actual
    review text. We also instruct the language model on what to do with the review
    text (**2**) (check whether the underlying sentiment is positive or negative)
    and define the output format (**3**). The latter step is important because there
    may be many ways to express the underlying sentiment: for example, “P” for positive
    and “N” for negative, or a longer answer such as “The review is positive.” If
    we don’t explicitly tell the language model to use a specific output format, it
    may choose any of these possibilities! In our scenario, we ultimately want to
    aggregate the classification results to learn the majority opinion (do most people
    like the movie or not?), and aggregating the results from each review becomes
    much simpler if all the classifications follow the same output format.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模板只包含一个占位符：要分类的评论文本（**1**）。对于每条评论，我们将用实际的评论文本替换此占位符。我们还指导语言模型如何处理评论文本（**2**）（检查其潜在的情感是正面还是负面）并定义输出格式（**3**）。后一步很重要，因为可能有多种方式来表达潜在的情感：例如，“P”代表正面，“N”代表负面，或者更长的回答，如“评论是正面的”。如果我们没有明确告诉语言模型使用特定的输出格式，它可能会选择这些可能性中的任何一个！在我们的场景中，我们最终想要汇总分类结果以了解大多数人的观点（大多数人喜欢这部电影吗？），如果所有分类都遵循相同的输出格式，汇总每条评论的结果会变得简单得多。
- en: 'The following function follows the template to generate a prompt for a given
    review (specified as the input parameter `text`):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数遵循模板为给定的评论（指定为输入参数 `text`）生成提示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result of the function is the prompt, instantiating the template for the
    input review.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的结果是提示，为输入评论实例化模板。
- en: 4.2.3 Calling the model
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 调用模型
- en: 'Next, we send generated prompts to a language model to obtain a solution. More
    precisely, we are using OpenAI’s GPT-4o model, OpenAI’s latest model at the time
    of writing. As this is one of OpenAI’s chat models, optimized for multistep interactions
    with users, we use the chat completions endpoint to communicate with the model.
    As discussed in more detail in chapter 3, this endpoint expects as input a history
    of prior messages (in addition to the specific model name). Here, we have only
    one prior “message”: the prompt. We classify it as a `user` message, encouraging
    the model to solve whatever task is described in the message. For instance, we
    can send prompts to the language model and collect the answers using the following
    piece of code (assuming that `prompt` contains the previously generated prompt
    text):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成的提示发送给语言模型以获得解决方案。更确切地说，我们正在使用 OpenAI 的 GPT-4o 模型，这是撰写本文时的 OpenAI 最新模型。由于这是
    OpenAI 的聊天模型之一，针对与用户的多次交互进行了优化，我们使用聊天完成端点与模型进行通信。如第 3 章所述，此端点期望输入先前的消息历史（除特定模型名称外）。在这里，我们只有一个先前的“消息”：提示。我们将其分类为
    `user` 消息，鼓励模型解决消息中描述的任何任务。例如，我们可以使用以下代码片段向语言模型发送提示并收集答案（假设 `prompt` 包含之前生成的提示文本）：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, using this code directly is problematic. OpenAI’s GPT models are hosted
    online and accessed remotely. This creates opportunities for failed attempts to
    reach the corresponding endpoint: for example, due to a temporary connection loss.
    Because of that, it is good practice to allow for a couple of retries when calling
    the model. In particular, when processing large data sets requiring many consecutive
    calls to OpenAI’s models, the chances of at least one unsuccessful call increase.
    Instead of interrupting computation with an exception, it is better to wait a
    few seconds before starting another try. Here is a completed version of the previous
    code—a function that calls the language model with automated retries:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接使用此代码存在问题。OpenAI 的 GPT 模型托管在网络上，并通过远程访问。这为尝试连接到相应端点失败创造了机会：例如，由于暂时性的连接丢失。因此，在调用模型时允许进行几次重试是一种良好的实践。特别是，当处理需要多次连续调用
    OpenAI 模型的大数据集时，至少有一次调用失败的可能性增加。与其通过异常中断计算，不如在开始另一次尝试之前等待几秒钟。以下是之前代码的完整版本——一个带有自动重试功能的调用语言模型的函数：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `call_lm` function allows up to three retries with an increasing delay between
    them. This delay is realized by a call to the `time.sleep` function (using Python’s
    `time` library) whenever an exception (indicating, for instance, a temporary connection
    loss) is encountered. After three retries, the function fails with an exception
    (assuming, pessimistically, that whatever problem prevents us from contacting
    OpenAI will not be resolved any time soon). Whenever the call succeeds, the function
    returns the corresponding result.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`call_lm` 函数允许最多三次重试，它们之间的延迟逐渐增加。这种延迟通过在遇到异常（例如，表示暂时性连接丢失）时调用 `time.sleep`
    函数（使用 Python 的 `time` 库）来实现。三次重试后，函数因异常失败（悲观地假设阻止我们联系 OpenAI 的任何问题都不会很快解决）。每次调用成功时，函数返回相应的结果。'
- en: 4.2.4 End-to-end classification code
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 端到端分类代码
- en: It’s time to put it all together! The next listing shows the code that matches
    the classification process we’ve discussed. It also contains the function for
    generating prompts (**2**) and the one for calling the language model (**3**).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将所有内容整合在一起了！下面的列表显示了与我们所讨论的分类过程相匹配的代码。它还包含了生成提示（**2**）和调用语言模型（**3**）的函数。
- en: Listing 4.1 Classifying input text by sentiment (positive, negative)
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1 通过情感（正面、负面）对输入文本进行分类
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Imports libraries'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入库'
- en: '#2 Generates classification prompts'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 生成分类提示'
- en: '#3 Calls the large language model'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 调用大型语言模型'
- en: '#4 Classifies one text document'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对一个文本文档进行分类'
- en: '#5 Reads text, classifies, and writes result'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 读取文本，分类，并写入结果'
- en: '#6 Defines command-line arguments'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 定义命令行参数'
- en: '#7 Reads input'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 读取输入'
- en: '#8 Classifies text'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 对文本进行分类'
- en: '#9 Generates output'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 生成输出'
- en: First, let’s discuss the libraries used in listing [4.1](#code__classification)
    (**1**). We will reuse those libraries for the following projects, so it makes
    sense to have a closer look at them (and why we need them here). We want to start
    our code from the command line, specifying relevant parameters (e.g., the path
    of the input data) as arguments. The `argparse` library features useful functions
    to specify and read out such command-line arguments. Next, we need the `openai`
    library, discussed in chapter 3, to call OpenAI’s language model from Python.
    The `pandas` library supports standard operations on tabular data. Of course,
    tabular data is not our focus in this chapter. However, we will store text documents
    and related metadata as rows in tables, so the `pandas` library comes in handy.
    Finally, as discussed previously, we use the `time` library to implement delayed
    retries when calling the language model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论列表 [4.1](#code__classification) 中使用的库（**1**）。我们将为以下项目重用这些库，因此有必要更仔细地查看它们（以及为什么在这里需要它们）。我们希望从命令行开始我们的代码，指定相关参数（例如，输入数据的路径）作为参数。`argparse`
    库提供了用于指定和读取此类命令行参数的有用函数。接下来，我们需要在第三章中讨论的 `openai` 库，从 Python 调用 OpenAI 的语言模型。`pandas`
    库支持表格数据的标准操作。当然，表格数据不是本章的重点。然而，我们将以表格中的行存储文本文档和相关元数据，因此 `pandas` 库很有用。最后，如前所述，我们使用
    `time` 库在调用语言模型时实现延迟重试。
- en: 4.2.5 Classifying documents
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.5 文档分类
- en: The classification of a single text document (**4**) combines the two functions
    discussed previously. Given an input text to classify, the code first creates
    a corresponding prompt (call to `create_prompt`) and then generates a suitable
    reply via a call to the language model (call to `call_llm`). The result is assumed
    to be the class label and is returned to the user.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 单个文本文档的分类（**4**）结合了之前讨论的两个函数。给定一个要分类的输入文本，代码首先创建相应的提示（调用`create_prompt`），然后通过调用语言模型（调用`call_llm`）生成合适的回复。结果假定是类别标签，并将其返回给用户。
- en: Now we put it together (**5**). This part of the code is executed when invoking
    the Python module from the command line and uses the functions we’ve introduced.
    The initial `if` condition (**5**) ensures that the following code is only executed
    when invoking the module directly (instead of importing it from a different module).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将其整合（**5**）。这部分代码在从命令行调用 Python 模块时执行，并使用我们已介绍的功能。初始的 `if` 条件（**5**）确保只有直接调用模块（而不是从不同的模块导入）时才执行以下代码。
- en: 'First (**6**), we define command-line arguments. We need only one argument
    here: a path to a .csv file containing the data to classify. We assume that each
    row contains one text document and that the text to classify is contained in the
    `text` column. We parse command-line arguments and make their values available
    in the `args` variable.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先（**6**），我们定义命令行参数。这里我们只需要一个参数：包含要分类的数据的 .csv 文件的路径。我们假设每一行包含一个文本文档，要分类的文本包含在
    `text` 列中。我们解析命令行参数并将它们的值存储在 `args` 变量中。
- en: 'Next, we load our input data from disk (**7**). We assume that data is stored
    as a .csv file (comma-separated value): that is, a header line with column names,
    followed by lines containing data (fields are separated by commas, as the name
    suggests). Here, the `pandas` library comes in handy and enables us to load such
    data with a single command. The `df` variable then contains a `pandas` DataFrame
    containing data from the input file. We retrieve the DataFrame `text` column (**8**)
    and apply the previously defined `classify` function to each row (using `pandas`’
    `apply` method). Finally (**9**), we generate and print out aggregate statistics
    (the number of occurrences for each answer generated by the model) and write the
    resulting classifications into a file (result.csv).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从磁盘加载我们的输入数据（**7**）。我们假设数据存储为 .csv 文件（逗号分隔值）：即包含列名的标题行，后面是包含数据的行（字段由逗号分隔，正如其名称所暗示）。在这里，`pandas`
    库派上了用场，使我们能够使用单个命令加载此类数据。然后 `df` 变量包含一个包含输入文件数据的 `pandas` DataFrame。我们检索 DataFrame
    的 `text` 列（**8**）并应用先前定义的 `classify` 函数到每一行（使用 `pandas` 的 `apply` 方法）。最后（**9**），我们生成并打印出汇总统计信息（模型生成的每个答案的出现次数）并将结果分类写入文件（result.csv）。
- en: 4.2.6 Running the code
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.6 运行代码
- en: 'On the book’s companion website, download the file reviews.csv. This file contains
    a small number of movie reviews that we can use for classification. The file contains
    two columns: the review text and the associated sentiment (`neg` for negative
    sentiment and `pos` for positive sentiment). Of course, our goal is to detect
    such sentiments automatically. However, having the ground truth also enables us
    to assess the quality of the classifications.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的配套网站上，下载文件 reviews.csv。此文件包含少量可用于分类的电影评论。文件包含两列：评论文本和相关的情感（`neg` 表示负面情感，`pos`
    表示正面情感）。当然，我们的目标是自动检测这样的情感。然而，拥有真实标签也使我们能够评估分类的质量。
- en: 'You can test the code for classification as described next (the following commands
    have been tested on a Linux operating system). Using a terminal, change to the
    directory containing a Python module (listing1.py) with the code in listing [4.1](#code__classification).
    Then, run the following command (replacing `python` with the name of your Python
    interpreter, such as `python3`, if needed):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下描述测试分类代码（以下命令已在 Linux 操作系统上测试过）。使用终端，切换到包含代码（listing1.py）的 Python 模块所在的目录（[4.1](#code__classification)）。然后，运行以下命令（如果需要，将
    `python` 替换为您的 Python 解释器名称，例如 `python3`）：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we assume that the input file (reviews.csv) is stored in the same repository
    as the code (otherwise, you have to substitute the corresponding path for the
    filename). Typically, the code should not take more than a few seconds to execute
    (slightly more if your connection is unstable, requiring retries). If execution
    succeeds, the only output you will see summarizes the number of labels assigned
    for each of the two possible classes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设输入文件（reviews.csv）存储在与代码相同的存储库中（否则，您必须用相应的路径替换文件名）。通常，代码的执行不应超过几秒钟（如果您的连接不稳定，可能需要重试，时间会稍长）。如果执行成功，您将看到的唯一输出将总结为每个可能的两个类别分配的标签数量。
- en: After executing the code, you will find a result.csv file in the same repository.
    In addition to the columns of the input file, the result file contains a new `class`
    column. This column contains the classification results (positive and negative).
    Compare the label assigned by our classifier to the ground-truth sentiment. You
    will find that the classification is consistent in a majority of cases. Not bad
    for a few lines of Python code, right?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，您将在同一存储库中找到一个 result.csv 文件。除了输入文件的列之外，结果文件还包含一个新的 `class` 列。此列包含分类结果（正面和负面）。将我们的分类器分配的标签与真实情感进行比较。您会发现大多数情况下分类是一致的。对于几行
    Python 代码来说，这还不错，对吧？
- en: 4.2.7 Trying out variants
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.7 尝试变体
- en: At this point, it is a good idea to play a bit more with the code and the data
    to get a better sense of how it works. For instance, try writing a few movie reviews
    yourself! For which reviews is the classification reliable, and where is it challenging?
    Also try a few variants of the prompt. Which instructions lead to better accuracy,
    and which degrade performance? To take just one example variation, try removing
    the part of the prompt that defines the output format precisely (the line `Answer
    ("Positive"/"Negative")`). Now try running the program with the changed prompt.
    What happens? In all likelihood, you will see more than two labels in your classification
    result (in the output of the program), including, for instance, abbreviations
    (e.g., “P” and “N”) as well as overly detailed answers (e.g., during testing,
    GPT-4o generated replies such as “The sentiment of this review is positive.”).
    In chapter 9, we evaluate the effect of different prompts on the model’s output
    quality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，尝试更多地对代码和数据进行操作，以更好地了解其工作原理是个不错的主意。例如，尝试自己写几篇电影评论！哪些评论的分类是可靠的，哪些又具有挑战性？还要尝试几种提示的变体。哪些指令能提高准确性，哪些又会降低性能？仅举一个例子，尝试移除提示中定义输出格式的部分（即“答案（正面/负面）”这一行）。现在尝试使用更改后的提示运行程序。会发生什么？很可能会在你的分类结果（程序输出）中看到超过两个标签，包括例如缩写（例如，“P”和“N”）以及过于详细的答案（例如，在测试期间，GPT-4o生成了“这篇评论的情感是积极的。”这样的回复）。在第9章中，我们评估了不同提示对模型输出质量的影响。
- en: You may also want to vary the model used for extraction. How about using one
    of the smaller model versions, such as GPT-3.5 (which is significantly cheaper
    per token processed)? And how about the model configuration? Listing [4.1](#code__classification)
    only uses two parameters (the model name and the message history), both of which
    are required. However, in chapter 3, we saw various configuration parameters that
    can be applied here. For instance, try changing the `temperature` parameter (e.g.,
    setting `temperature` to 0 will give you more deterministic results), or limit
    the length of the desired output! In rare cases, GPT models may generate output
    text that is longer than the desired classification result (which consists of
    a single token). You can avoid that by limiting the output length using the `max_tokens`
    parameter. At the same time, instead of restricting the output format only via
    instructions in the prompt, you may increase the likelihood of the two possible
    results (positive and negative) using the `logit_bias` parameter. We discuss model
    tuning further in chapter 9.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想改变用于提取的模型。使用较小版本的小型模型，如GPT-3.5（每处理一个标记的成本显著更低）怎么样？还有模型配置呢？列表[4.1](#code__classification)只使用了两个参数（模型名称和消息历史），这两个都是必需的。然而，在第3章中，我们看到了可以应用在这里的各种配置参数。例如，尝试更改`temperature`参数（例如，将`temperature`设置为0将给出更确定的结果），或者限制所需输出的长度！在罕见的情况下，GPT模型可能会生成比所需分类结果（由单个标记组成）更长的输出文本。你可以通过使用`max_tokens`参数限制输出长度来避免这种情况。同时，除了通过提示中的指令仅限制输出格式外，你还可以使用`logit_bias`参数增加两种可能结果（正面和负面）的可能性。我们将在第9章中进一步讨论模型调整。
- en: As yet another variant, try changing the classification task! For instance,
    it is relatively easy to classify using a different set of categories. All it
    takes is changing the instructions in the prompt (outlining all answer options
    as before). By changing a few lines of code, you can even obtain a versatile classification
    tool that enables users to specify the classification task and corresponding classes
    as additional command-line arguments. For example, beyond movie reviews, you can
    use this tool to categorize newspaper articles into one of several topic categories
    or to classify emails as either Urgent or Nonurgent. By now, you are hopefully
    convinced that language models enable text classification with relatively high
    quality and moderate implementation overheads. Time to broaden our scope to different
    tasks!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一种变体，尝试改变分类任务！例如，使用不同的一组类别进行分类相对容易。只需更改提示中的指令（如前所述，概述所有答案选项）。通过更改几行代码，你甚至可以获得一个通用的分类工具，允许用户通过额外的命令行参数指定分类任务和相应的类别。例如，除了电影评论之外，你可以使用这个工具将报纸文章分类到几个主题类别之一，或者将电子邮件分类为紧急或非紧急。到目前为止，你或许已经相信语言模型能够以相对较高的质量和适度的实现开销进行文本分类。是时候扩大我们的范围，探索不同的任务了！
- en: 4.3 Text extraction
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 文本提取
- en: Imagine that, given your expertise in data analysis with language models, you
    recently landed a highly sought-after job at Banana (a popular company producing
    various consumer electronics). The moment you sit down at the desk of your new
    office, emails from enthusiastic students inquiring about summer internships start
    rolling in. Having a summer intern would be nice, but how do you choose the best
    match? Ideally, you would like to compile a table comparing all applicants in
    terms of their GPA, their degree, the name of the company at which they did their
    most recent internship (if any), and so on. But combing through emails to compile
    that table manually seems tedious. Can’t you automate that?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，鉴于你在使用语言模型进行数据分析方面的专业知识，你最近在Banana（一家生产各种消费电子产品的流行公司）获得了一份备受追捧的工作。当你坐在新办公室的办公桌前，关于暑期实习的邮件开始源源不断地涌入。拥有一个暑期实习生当然很好，但你该如何选择最佳人选呢？理想情况下，你希望编制一个表格，比较所有申请者的GPA、学位、他们最近实习的公司名称（如果有）等等。但是手动浏览邮件来编制这个表格似乎很繁琐。难道不能自动化吗？
- en: 'Of course you can. Let’s use language models to analyze emails to extract all
    the relevant factors to choose our lucky summer intern. What we have here is,
    again, a standard problem in text analysis: information extraction! In information
    extraction, we generally extract structured information (e.g., a data table) from
    text. Here, we consider emails (from applicants) as text documents. For each email,
    we want to extract a range of attributes: for example, name, GPA, and (current
    or most recent) degree. For instance, consider the following extract from an email
    from one of the hopeful applicants:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以。让我们使用语言模型来分析邮件，提取所有相关因素以选择我们的幸运暑期实习生。这里我们又遇到了一个标准的文本分析问题：信息提取！在信息提取中，我们通常从文本中提取结构化信息（例如，数据表）。在这里，我们将申请者的邮件视为文本文档。对于每封邮件，我们希望提取一系列属性：例如，姓名、GPA和（当前或最近的）学位。例如，考虑以下来自一位有希望的申请者的邮件摘录：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Considering the three previously mentioned attributes, we can extract the name
    of the applicant (“Martin”), his GPA (“4.0”), and his degree (“Bachelor of Computer
    Science”). If analyzing emails from multiple applicants, we can represent the
    result as a data table, as shown in table [4.2](#tab__summerinterns). In the next
    section, we discuss how we can accomplish information extraction using language
    models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面提到的三个属性，我们可以提取申请者的姓名（“Martin”）、他的GPA（“4.0”）以及他的学位（“计算机科学学士”）。如果分析来自多个申请者的邮件，我们可以将结果表示为数据表，如表[4.2](#tab__summerinterns)所示。在下一节中，我们将讨论如何使用语言模型实现信息提取。
- en: Table 4.2 Extracted information about applicants for summer internships
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.2 暑期实习生申请者提取信息
- en: '| **Name** | **GPA** | **Degree** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **姓名** | **GPA** | **学位** |'
- en: '| Martin | 4.0 | Bachelor of Computer Science |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Martin | 4.0 | 计算机科学学士 |'
- en: '| Alice | 4.0 | Master of Software Engineering |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Alice | 4.0 | 软件工程硕士 |'
- en: '| Bob | 3.7 | Bachelor of Design |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Bob | 3.7 | 设计学士 |'
- en: 4.3.1 Overview
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 概述
- en: 'Again, we’ll assume that our emails are stored on disk (in a tabular data file
    where each row contains one email). We’ll iterate over emails and use the language
    model to extract all relevant attributes. Instead of hard-coding relevant attributes,
    we will allow users to specify those attributes on the command line (that way,
    you can easily reuse the code if your criteria for summer internships should change).
    As we use language models for text analysis (which are good at interpreting natural
    language), there is no need to specify attributes in any kind of formal language.
    Simply specify the attribute names (or, optionally, a short description in natural
    language), and the language model should be able to figure out what to extract.
    The output of our code will be a tabular data file (in .csv format) that contains
    content similar to table [4.2](#tab__summerinterns): the output table has one
    column for each extracted attribute and one row for each analyzed email.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们假设我们的邮件存储在磁盘上（在一个表格数据文件中，其中每一行包含一封邮件）。我们将遍历邮件并使用语言模型提取所有相关属性。我们不会硬编码相关属性，而是允许用户在命令行上指定这些属性（这样，如果您的暑期实习标准发生变化，您可以轻松地重用代码）。由于我们使用语言模型进行文本分析（它们擅长解释自然语言），因此无需用任何形式的正式语言指定属性。只需指定属性名称（或，可选地，用自然语言提供一个简短的描述），语言模型就应该能够确定要提取的内容。我们代码的输出将是一个表格数据文件（.csv格式），其内容类似于表[4.2](#tab__summerinterns)：输出表为每个提取的属性有一个列，为每封分析的邮件有一个行。
- en: 'So how can we extract attributes from a given email? Again, we want to generate
    a prompt that describes the extraction task to the language model. For instance,
    the following prompt should help us extract all relevant attributes from the previous
    email:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何从给定的电子邮件中提取属性呢？再次，我们希望生成一个描述提取任务的提示，以便向语言模型说明。例如，以下提示可以帮助我们从之前的电子邮件中提取所有相关属性：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Task description'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 任务描述'
- en: '#2 Text to analyze'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 要分析的文字'
- en: '#3 Output format'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出格式'
- en: 'The prompt consists of three parts: a task description, including a specification
    of the attributes to extract (**1**); the source text for extraction (**2**);
    and the desired output format, including values to use if the source text does
    not contain any information on specific attributes (**3**). Sending this prompt
    to the language model should yield text that contains the desired extraction results.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提示由三部分组成：任务描述，包括要提取的属性的指定（**1**）；提取的源文本（**2**）；以及期望的输出格式，包括如果源文本不包含关于特定属性的任何信息时使用的值（**3**）。将此提示发送到语言模型应产生包含所需提取结果的文本。
- en: The output from the language model is, first, a text string. Ultimately, we
    want to output a structured data table. That means we still need some postprocessing
    to extract values for all relevant attributes (name, GPA, and degree) from the
    output text. Figure [4.2](#fig__textExtractionOverview) illustrates the steps
    of the extraction process (for a single text document).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的输出首先是一个文本字符串。最终，我们希望输出一个结构化数据表。这意味着我们仍然需要进行一些后处理，以从输出文本中提取所有相关属性（姓名、GPA和学位）的值。图
    [4.2](#fig__textExtractionOverview) 展示了提取过程步骤（针对单个文本文档）。
- en: '![figure](../Images/CH04_F02_Trummer.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F02_Trummer.png)'
- en: Figure 4.2 For each email, we generate a prompt that contains the email and
    a description of the extraction task. This description references the attributes
    to extract specified by the user. Given the prompt as input, the language model
    generates an answer text containing extracted attribute values. Via postprocessing,
    we extract those values from the raw answer text.
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2 对于每封电子邮件，我们生成一个包含电子邮件和提取任务描述的提示。此描述引用了用户指定的要提取的属性。给定提示作为输入，语言模型生成包含提取属性值的答案文本。通过后处理，我们从原始答案文本中提取这些值。
- en: 4.3.2 Generating prompts
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 生成提示
- en: 'We want to generate prompts that instantiate the following prompt template:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望生成以下提示模板的实例：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Task description'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 任务描述'
- en: '#2 Text to analyze'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 要分析的文字'
- en: '#3 Output format'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出格式'
- en: 'The prompt template contains a task description (**1**), the source text for
    extraction (**2**), and a specification of the output format (**3**). Note that
    this prompt now contains two placeholders (the template we used in the previous
    section contained only a single placeholder): the list of attributes to extract
    and the source text for extraction.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板包含任务描述（**1**）、提取的源文本（**2**）以及输出格式的指定（**3**）。请注意，此提示现在包含两个占位符（我们之前章节中使用的模板只有一个占位符）：要提取的属性列表和提取的源文本。
- en: 'We will generate prompts using the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码生成提示：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Generates a task description'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 生成任务描述'
- en: '#2 Adds source text'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 添加源文本'
- en: '#3 Adds a description of the output format'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 添加输出格式的描述'
- en: This function takes as input the text to analyze (which we certainly want to
    include in the prompt) along with a list of attributes we want to extract. After
    generating the task description (**1**), including the list of attributes to extract,
    the function adds the source text (**2**), as well as a specification of the desired
    output format (**3**). The prompt concatenates these parts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受要分析的文字（我们当然希望将其包含在提示中）以及我们想要提取的属性列表作为输入。在生成任务描述（**1**），包括要提取的属性列表后，该函数添加源文本（**2**），以及期望的输出格式的指定（**3**）。提示将这些部分连接起来。
- en: 4.3.3 Postprocessing
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 后处理
- en: Compared to the previous project (text classification), our prompt has changed
    to adapt to the new task (text extraction). Even with a different prompt, we can
    still reuse the same function as in the last section to obtain an answer from
    the language model. On the other hand, we need to do a little more work than before
    to process the raw answer using the language model. For classification, we directly
    used the reply from the language model as the final result. In our current scenario
    (text extraction), we generally will want to extract values for multiple attributes
    for a single input text. As the output text from the language model contains values
    for all extracted attributes, we need to extract values for specific attributes
    from the raw answer text.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的项目（文本分类）相比，我们的提示词已经改变以适应新的任务（文本提取）。即使提示词不同，我们仍然可以像上一节那样重用相同的函数来从语言模型获取答案。另一方面，我们需要比以前做更多的工作来处理原始答案。对于分类，我们直接使用语言模型的回复作为最终结果。在我们当前的场景（文本提取）中，我们通常希望从单个输入文本中提取多个属性值。由于语言模型的输出文本包含所有提取的属性值，我们需要从原始答案文本中提取特定属性值。
- en: 'For instance, we might receive the following raw answer text from the language
    model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会从语言模型接收到以下原始答案文本：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To extract values for each attribute, we can split the raw text using pipe symbols
    as field delimiters (while removing the first and last pipe symbols in the answer).
    Ideally, we want to expand our scope beyond the specific use case we are currently
    considering (extracting information on applicants from emails). In some scenarios,
    we may extract multiple rows from the same text (imagine a scenario where multiple
    applicants together submit a group email—but that’s admittedly a less likely case).
    To support such use cases, we may also have to split the raw answer into text
    associated with different rows. To do that, we can use the newline symbol as row
    delimiters (as rows are split by newline symbols).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取每个属性值，我们可以使用管道符号作为字段分隔符来分割原始文本（同时移除答案中的第一个和最后一个管道符号）。理想情况下，我们希望将我们的范围扩展到我们目前考虑的具体用例之外（例如从电子邮件中提取申请人的信息）。在某些情况下，我们可能需要从同一文本中提取多行（想象一下多个申请人一起提交一个群发电子邮件的场景——但这是一个不太可能的情况）。为了支持这样的用例，我们可能还需要将原始答案分割成与不同行相关的文本。为此，我们可以使用换行符作为行分隔符（因为行是通过换行符分割的）。
- en: 'We can do all these things with the following function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下函数来完成所有这些操作：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Extracts table data'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提取表格数据'
- en: '#2 Splits by row'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 按行分割'
- en: '#3 Splits by field'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 按字段分割'
- en: 'The input to this function is raw text produced by the language model for a
    single text document. The output is a list of rows (where each result row is,
    again, represented as a list). To get from input to output, we first need to extract
    the part of the raw answer that contains the actual table data (**1**). Answers
    generated by GPT-4o may contain a preamble or additional explanations beyond the
    extracted table (e.g., “Sure, here is the table you wanted: ...”). We need to
    separate the data we are interested in. Fortunately, that’s easy as long as GPT-4o
    is following our instructions (which, typically, it does): the data we’re interested
    in should be contained between two markers (`<BeginTable>` and `<EndTable>`).
    Hence, the regular expression `''<BeginTable>(.*)<EndTable>''` exactly matches
    the part of the output we’re interested in. We retrieve it using Python’s `re.findall`
    function, which, given a string and regular expression as input, returns a list
    of matching substrings. We use the `re.DOTALL` flag to ensure that the dot within
    the regular expression matches all characters and newlines (because the table
    may contain multiple lines). From the resulting matches, we take the first one.
    Note that this implicitly assumes at least one table in GPT’s output. Although
    that is typically the case, think about how to make the function more robust toward
    answers from the language model that do not comply with our instructions in the
    prompt.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的输入是语言模型为单个文本文档生成的原始文本。输出是一个行列表（其中每个结果行再次表示为一个列表）。要从输入到输出，我们首先需要提取包含实际表格数据的原始答案部分（**1**）。GPT-4o
    生成的答案可能包含引言或超出提取表格的解释（例如，“当然，这是您想要的表格：...”）。我们需要分离我们感兴趣的数据。幸运的是，只要 GPT-4o 遵循我们的指示（通常情况下它会这样做），我们感兴趣的数据应该包含在两个标记（`<BeginTable>`
    和 `<EndTable>`）之间。因此，正则表达式 `'<BeginTable>(.*)<EndTable>'` 完全匹配我们感兴趣的输出部分。我们使用
    Python 的 `re.findall` 函数检索它，该函数接受字符串和正则表达式作为输入，并返回匹配的子字符串列表。我们使用 `re.DOTALL` 标志确保正则表达式中的点匹配所有字符和新行（因为表格可能包含多行）。从结果匹配中，我们取第一个。请注意，这隐含地假设
    GPT 的输出中至少有一个表格。尽管通常情况下是这样的，但请考虑如何使该函数对不符合提示中我们指示的语言模型答案更加健壮。
- en: Having extracted the table data in a text representation, we first split it
    into data associated with specific rows (**2**) and data associated with specific
    cells (**3**). After some cleanup (the Python function `strip` removes whitespace),
    we add the resulting cell values to our result list. This list of rows (where
    each row is, again, represented as a list) is returned.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在将表格数据以文本形式提取出来后，我们首先将其分为与特定行相关的数据（**2**）和与特定单元格相关的数据（**3**）。经过一些清理（Python 函数
    `strip` 移除空白字符），我们将得到的单元格值添加到我们的结果列表中。这个由行组成的列表（其中每一行再次表示为一个列表）被返回。
- en: 4.3.4 End-to-end extraction code
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 端到端提取代码
- en: Listing [4.2](#code__extraction) shows the completed Python code. The code structure
    is similar to listing [4.1](#code__classification), and some of the functions
    are shared among the two listings (rather than omitting repeated functions, this
    book aims to provide you with self-contained code so you don’t have to piece together
    code from multiple pages). In particular, the code uses the same libraries as
    before (**1**) and invokes the language model via the same function (**3**). You
    will recognize the function for creating prompts (**2**) and the one for postprocessing
    raw output from the language model (**4**), introduced earlier.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [4.2](#code__extraction) 展示了完成的 Python 代码。代码结构类似于列表 [4.1](#code__classification)，并且两个列表中的一些函数是共享的（而不是省略重复的函数，本书旨在提供自包含的代码，以便您不需要从多个页面中拼凑代码）。特别是，代码使用了之前相同的库（**1**）并通过相同的函数调用语言模型（**3**）。您将认出用于创建提示的函数（**2**）和用于后处理语言模型原始输出的函数（**4**），这些函数之前已经介绍过。
- en: Listing 4.2 Extracting user-defined attributes from text
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2 从文本中提取用户定义的属性
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Imports relevant libraries'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入相关库'
- en: '#2 Generates prompts'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 生成提示'
- en: '#3 Invokes the language model'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 调用语言模型'
- en: '#4 Postprocesses model output'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 后处理模型输出'
- en: '#5 Extracts data tables from text'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从文本中提取数据表'
- en: '#6 Extracts information and writes the result'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 提取信息并写入结果'
- en: '#7 Iterates over text'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 遍历文本'
- en: 'The main function (**6**) reads two input parameters from the command line:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数（**6**）从命令行读取两个输入参数：
- en: A path to a .csv file containing the text to analyze
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要分析的文本的 .csv 文件路径
- en: A list of attributes to extract, separated by pipe symbols
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要提取的属性列表，由管道符号分隔
- en: After opening the input file (using the `pandas` library), we iterate over all
    input text documents (**7**). Note that we expect the input text in the `text`
    column in the input file. To perform the actual extraction, we use the `extract_rows`
    function (**5**). Given input text and a list of attributes to extract, this function
    generates a suitable prompt, obtains a raw answer from the language model, and
    postprocesses the raw answer to get structured output (which it returns). After
    iterating over the input text (**7**), we store the final result in a file named
    result.csv (this file will be overwritten if it already exists).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`pandas`库打开输入文件后，我们遍历所有输入文本文档（**7**）。请注意，我们期望输入文件中的`text`列包含输入文本。为了执行实际提取，我们使用`extract_rows`函数（**5**）。给定输入文本和要提取的属性列表，此函数生成一个合适的提示，从语言模型获取原始答案，并对原始答案进行后处理以获得结构化输出（并将其返回）。遍历输入文本（**7**）后，我们将最终结果存储在名为result.csv的文件中（如果该文件已存在，则会被覆盖）。
- en: 4.3.5 Trying it out
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 尝试一下
- en: You can find the code from listing [4.2](#code__extraction) as listing2.py on
    the companion website. You can also download the file biographies.csv there, giving
    you a small data set to experiment on with your extractor (this is a bit different
    from our motivating scenario, but publicly available data on email applications
    is sparse). This file contains biographies of five famous people, as well as the
    associated names, with one person per row. Change into the directory containing
    listing2.py (as well as the data), and run
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在配套网站上找到列表[4.2](#code__extraction)中的代码，作为listing2.py。你还可以在那里下载biographies.csv文件，这为你提供了一个小数据集来测试你的提取器（这与我们的动机场景略有不同，但关于电子邮件应用的公开数据很稀缺）。此文件包含五位著名人物的传记，以及相关的姓名，每行一个人。切换到包含listing2.py（以及数据）的目录，并运行
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first parameter is the data set (if it is not in the same directory, adapt
    the path accordingly). The second parameter is the list of attributes to extract.
    We use the pipe symbol again to separate attributes. Note that we only identify
    attributes via their names; no need to refer to predefined categories. The language
    model can understand attribute semantics based on the name alone.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是数据集（如果它不在同一目录中，请相应地调整路径）。第二个参数是要提取的属性列表。我们再次使用管道符号来分隔属性。请注意，我们仅通过名称识别属性；无需引用预定义类别。语言模型可以根据名称理解属性语义。
- en: 'After executing the code (which should not take more than a minute), you will
    find the results stored in a file named result.csv. For example, executing the
    code on the sample data could yield the following table:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码（这不应超过一分钟）后，你将在名为result.csv的文件中找到结果。例如，在样本数据上执行代码可能会得到以下表格：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Even if you execute the same code on the same data, you may see slight variations
    (due to randomization when generating model output). Each row in that file (besides
    the header row) represents an extraction. We are extracting name, birth city,
    and birth date. Hence, we expect one extracted row per biography (and that is
    what happens in our sample run). Note that there are missing values: for Ann E.
    Wojcicki, the biography snippet does not contain the city of birth. The language
    model reacts appropriately and inserts a corresponding placeholder (“<N/A>”),
    instead of a concrete value.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你在相同的数据上执行相同的代码，你也可能看到一些细微的差异（这是由于生成模型输出时的随机化造成的）。该文件中的每一行（除了标题行）代表一个提取。我们正在提取姓名、出生城市和出生日期。因此，我们期望每篇传记都有一个提取行（这正是我们在样本运行中看到的情况）。请注意，存在缺失值：对于Ann
    E. Wojcicki，传记片段中不包含出生城市。语言模型会相应地做出反应，并插入相应的占位符（“<N/A>”），而不是具体值。
- en: 4.4 Clustering
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 聚类
- en: 'You’re a few weeks in at your new job at Banana. The job is great, but there
    is one problem: your inbox keeps overflowing with emails! It’s not only applications
    from hopeful summer interns (we took care of that in the last section). Those
    emails cover a variety of different topics, and making sure to read all relevant
    emails takes a lot of your time. Looking closer, you notice that many of the emails
    are redundant. For example, you observe that many emails try to draw attention
    to the same company events. For a moment, you ponder using your code for text
    classification (discussed in section 4.2) to categorize emails into several categories
    (e.g., associated with specific company events). After that, you can read only
    a few emails from each category to have a full overview of what’s happening at
    Banana. Alas, there is one problem: it is hard to come up with and maintain an
    exhaustive list of topics because those topics will keep changing over the course
    of your employment. Instead, it would be nice to automatically group different
    emails that are somewhat similar because, for instance, they discuss the same
    event. That way, you wouldn’t have to come up with a list of topics in advance.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你在Banana公司的新工作已经过去几周了。这份工作很棒，但有一个问题：你的收件箱里总是堆满了邮件！这些邮件不仅包括渴望成为暑期实习生的申请（我们已经在上一节处理了这些），而且涉及各种不同的主题。确保阅读所有相关的邮件花费了你很多时间。仔细观察后，你发现许多邮件是重复的。例如，你注意到许多邮件试图引起人们对同一公司活动的关注。一时间，你考虑使用你的代码进行文本分类（在第4.2节中讨论过）来将邮件分类到几个类别中（例如，与特定的公司活动相关）。之后，你只需阅读每个类别中的几封邮件，就能全面了解Banana公司正在发生的事情。然而，有一个问题：很难制定并维护一个详尽的主题列表，因为这些主题在你工作的过程中会不断变化。相反，如果能自动地将一些相似的不同邮件分组，那就太好了，因为例如，它们讨论的是同一事件。这样，你就不必提前制定一个主题列表。
- en: 'What we want is to group similar emails into clusters. That’s yet another classical
    text-processing problem: text clustering. If you want to bring related text documents
    together without knowing the set of categories beforehand, clustering methods
    are probably the way to go! In this section, we will see how to use language models
    for text clustering.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的是将相似的邮件分组到簇中。这又是一个经典的文本处理问题：文本聚类。如果你想在不知道事先的类别集合的情况下将相关的文本文档聚集在一起，那么聚类方法可能是最佳选择！在本节中，我们将看到如何使用语言模型进行文本聚类。
- en: 4.4.1 Overview
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 概述
- en: 'Clustering is a classical approach in computer science. Clustering methods
    predate language models and advanced text analysis by quite a bit. However, traditionally,
    clustering focuses on elements that are expressed as vectors. We want to bring
    together (in the same cluster) vectors that have a small distance from each other
    (and, of course, there are various distance metrics that we can apply for vectors).
    However, that’s not really the case here: in our scenario, we want to assign similar
    emails (or, in general, similar text documents) to the same cluster. So how do
    we get from documents to vectors?'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是计算机科学中的一个经典方法。聚类方法在语言模型和高级文本分析之前就已经存在了。然而，传统上，聚类主要关注以向量形式表达的因素。我们希望将彼此距离较近的向量（当然，我们可以为向量应用各种距离度量）聚集到同一个簇中。然而，这里的情况并非如此：在我们的场景中，我们希望将相似的邮件（或者更一般地说，相似的文本文档）分配到同一个簇中。那么我们如何从文档转换到向量呢？
- en: The answer is *embeddings*. An embedding represents a text document as a (typically
    high-dimensional) vector. That’s exactly what we’re looking for! Of course, this
    approach only makes sense if we map text documents to vectors that have something
    meaningful to say about the content of the documents. Ideally, we want documents
    with similar vectors (i.e., vectors with a small distance according to our preferred
    distance metric) to also have similar content. This means we cannot use naive
    methods to map text documents to vectors. Instead, we need an approach that considers
    the semantics of the text and takes them into account when generating a vector
    representation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是**嵌入**。嵌入将文本文档表示为一个（通常是高维的）向量。这正是我们所需要的！当然，这种方法只有在我们将文本文档映射到具有关于文档内容意义的向量时才有意义。理想情况下，我们希望具有相似向量的文档（即根据我们首选的距离度量具有较小距离的向量）也具有相似的内容。这意味着我们不能使用简单的方法将文本文档映射到向量。相反，我们需要一种考虑文本语义并在生成向量表示时考虑这些语义的方法。
- en: Fortunately, language models can help! Providers like OpenAI offer language
    models that take text as input and produce embedding vectors as output. So, having
    a collection of text documents to cluster, we can calculate embedding vectors
    for all of them and apply any classical clustering algorithm to the resulting
    vectors. Figure [4.3](#fig__clusteringOverview) illustrates this process. Next,
    we discuss how to implement it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，语言模型可以帮助！像OpenAI这样的提供商提供将文本作为输入并产生嵌入向量的语言模型。因此，如果我们有一组要聚类的文本文档，我们可以为它们计算嵌入向量，并将任何经典聚类算法应用于这些向量。图[4.3](#fig__clusteringOverview)说明了这个过程。接下来，我们将讨论如何实现它。
- en: '![figure](../Images/CH04_F03_Trummer.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F03_Trummer.png)'
- en: Figure 4.3 Clustering emails. We first calculate embedding vectors for all emails.
    Then we cluster those vectors to assign emails with similar content to the same
    cluster.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3 聚类电子邮件。我们首先为所有电子邮件计算嵌入向量。然后，我们将这些向量聚类，将内容相似的电子邮件分配到同一簇。
- en: 4.4.2 Calculating embeddings
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 计算嵌入
- en: For the examples discussed so far, we have used OpenAI’s chat completions endpoint.
    For clustering, we will use OpenAI’s embedding endpoint instead. The goal of embedding
    is to create a vector that compresses the semantics of a text. Different models
    can be used to calculate embeddings. The dimension of the vector depends on the
    model used. For the following code, we will use the `text-embedding-ada-002` model.
    You can try substituting other models for this one (you can find a list of OpenAI
    models for calculating embeddings at [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings))
    to compare the output quality.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于到目前为止讨论的示例，我们使用了OpenAI的聊天补全端点。对于聚类，我们将使用OpenAI的嵌入端点。嵌入的目标是创建一个压缩文本语义的向量。可以使用不同的模型来计算嵌入。向量的维度取决于所使用的模型。对于以下代码，我们将使用`text-embedding-ada-002`模型。您可以尝试用其他模型替换此模型（您可以在[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)找到OpenAI用于计算嵌入的模型列表）以比较输出质量。
- en: 'For instance, we can generate embeddings for text documents as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以按如下方式为文本文档生成嵌入：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here you see an extract from the corresponding response:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到相应响应的摘录：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Embedding vector'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 嵌入向量'
- en: '#2 Model that generated embeddings'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 生成嵌入的模型'
- en: '#3 Usage statistics'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用统计信息'
- en: The extract only shows values for the first few vector dimensions (**1**) (whereas
    the full vector has over 1,000 dimensions). Besides the embedding vector, the
    response contains the model name (**2**) and usage statistics (**3**). Unlike
    earlier, usage statistics only refer to the number of tokens in the prompt (which
    is also the total number of tokens processed). Unlike text completion, the language
    model only reads tokens but does not generate them.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该摘录仅显示了前几个向量维度（**1**）（而完整的向量有超过1,000个维度）。除了嵌入向量外，响应还包含模型名称（**2**）和使用统计信息（**3**）。与之前不同，使用统计信息仅指代提示中的标记数量（这也是处理的总标记数量）。与文本补全不同，语言模型只读取标记，但不生成它们。
- en: 'The most relevant part for us is, of course, the embedding vector itself. You
    can access that embedding vector via the following command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对我们来说最相关的是嵌入向量本身。您可以通过以下命令访问该嵌入向量：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Most of the time, invoking the language model once should provide you with
    the embedding you are searching for. Of course, when calculating embedding vectors
    for a large number of emails, we may run into problems (i.e., failed connection
    attempts) every once in a while. This is why the final version of our embedding
    function again contains a retry mechanism:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，调用一次语言模型应该能为您提供所需的嵌入。当然，当为大量电子邮件计算嵌入向量时，我们可能会偶尔遇到问题（即失败的连接尝试）。这就是为什么我们嵌入函数的最终版本再次包含重试机制：
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Given a text as input, we try up to three times to get a corresponding embedding
    vector (increasing the delay between retries after each failed attempt). This
    is the function we will use.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本作为输入，我们尝试最多三次来获取相应的嵌入向量（在每次失败尝试后增加重试之间的延迟）。这是我们将会使用的函数。
- en: 4.4.3 Clustering vectors
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 聚类向量
- en: To cluster vectors (representing documents), we will use the k-means clustering
    algorithm. K-means is a very popular clustering algorithm that works by iteratively
    refining the mapping from vectors to clusters. Unlike other clustering algorithms,
    the algorithm requires you to specify the number of clusters in advance. In our
    example scenario, that means choosing how fine-grained the partitioning of emails
    by their content should be.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了聚类表示文档的向量，我们将使用 k-means 聚类算法。K-means 是一个非常流行的聚类算法，它通过迭代地改进向量到聚类的映射来工作。与其他聚类算法不同，该算法要求你提前指定聚类数量。在我们的示例场景中，这意味着选择按内容对电子邮件进行分区应该有多精细。
- en: How does the k-means algorithm work?
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: k-means 算法是如何工作的？
- en: The k-means algorithm takes as input a set of elements to cluster and a target
    number of clusters. It works by iteratively refining the mapping from elements
    to clusters until a termination criterion (e.g., a maximum number of iterations
    or minimal changes in cluster assignments between consecutive iterations) is met.
    The k-means algorithm associates each cluster with a vector (representing the
    center of that cluster). In each iteration, it assigns each vector to the cluster
    with the nearest center. Then, it recalculates the vectors associated with clusters
    (by averaging over the vectors of all elements currently assigned to the cluster).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法将一组要聚类的元素和一个目标聚类数量作为输入。它通过迭代地改进元素到聚类的映射，直到满足终止条件（例如，最大迭代次数或连续迭代之间聚类分配的最小变化）为止。k-means
    算法将每个聚类与一个向量（表示该聚类的中心）关联。在每次迭代中，它将每个向量分配给最近的中心所在的聚类。然后，它重新计算与聚类关联的向量（通过平均分配给该聚类的所有元素的向量）。
- en: 'We will be using the k-means implementation in the `scikit-learn` library.
    Follow the instructions in the first section of this chapter to ensure that this
    library is installed (import clustering methods via `from sklearn.cluster import
    KMeans`). After importing the library, we can invoke the k-means implementation
    with the following (concise) piece of code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `scikit-learn` 库中的 k-means 实现。按照本章第一部分的说明确保该库已安装（通过 `from sklearn.cluster
    import KMeans` 导入聚类方法）。在导入库后，我们可以使用以下（简洁的）代码片段调用 k-means 实现：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The function takes a list of embedding vectors and the number of target clusters
    as input and then clusters those vectors using the k-means implementation. The
    result of clustering is labels associated with each embedding vector. Those labels
    indicate the ID of the associated cluster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受嵌入向量的列表和目标聚类数量作为输入，然后使用 k-means 实现对这些向量进行聚类。聚类的结果是与每个嵌入向量关联的标签。这些标签指示关联的聚类
    ID。
- en: 4.4.4 End-to-end code for text clustering
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 文本聚类的端到端代码
- en: The following listing shows the complete code for clustering text documents
    via embedding vectors. You will recognize the functions for calculating embedding
    vectors (**1**) and clustering them (**2**).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了通过嵌入向量聚类文本文档的完整代码。你会认出计算嵌入向量（**1**）和聚类它们（**2**）的函数。
- en: Listing 4.3 Clustering text documents using language models
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3 使用语言模型聚类文本文档
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Calculates embedding vectors'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算嵌入向量'
- en: '#2 Clusters embeddings'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 聚类嵌入'
- en: '#3 Reads text and writes out clusters'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 读取文本并输出聚类'
- en: The main function of listing [4.3](#code__clustering) (**3**) reads data from
    a file on disk. Again, we assume that data is contained in a .csv file and focus
    on the `text` column. First, we iterate over text documents and generate corresponding
    embeddings (by invoking the `get_embedding` function, discussed previously). Then,
    we cluster embedding vectors via the `get_kmeans` function. The cluster IDs become
    an additional column in the result table written to disk.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [4.3](#code__clustering) （**3**） 的主要功能是从磁盘上的文件读取数据。同样，我们假设数据包含在一个 .csv 文件中，并关注
    `text` 列。首先，我们遍历文本文档并生成相应的嵌入（通过调用之前讨论过的 `get_embedding` 函数）。然后，我们通过 `get_kmeans`
    函数对嵌入向量进行聚类。聚类 ID 成为写入磁盘的结果表中的一列。
- en: 4.4.5 Trying it out
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.5 尝试一下
- en: 'Time to try clustering via embedding vectors! You can find the code from listing
    [4.3](#code__clustering) on the book’s companion website (listing3.py), as well
    as a suitable data set (textmix.csv). This data set contains a mix of text snippets
    from two sources: a collection of poems and a repository of emails. We’ll try
    to separate the two via clustering: we expect emails and poems to be assigned
    to different clusters.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候尝试通过嵌入向量进行聚类了！你可以在书的配套网站上找到代码（[4.3](#code__clustering) 列表），以及一个合适的数据集（textmix.csv）。这个数据集包含来自两个来源的文本片段的混合：一首诗的集合和一个电子邮件存储库。我们将尝试通过聚类来分离这两个来源：我们预计电子邮件和诗歌将被分配到不同的簇中。
- en: 'Change into the directory containing the code and data, and run the following
    command in the terminal:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到包含代码和数据的目录，并在终端中运行以下命令：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, textmix.csv is the name of the input file, and 2 is the number of target
    clusters (in this specific case, two seems like a reasonable choice, whereas determining
    the right number of clusters can be more difficult in other scenarios). The result
    will be stored in the file result.csv. It contains all the columns from the input
    file, as well as an additional column with the cluster ID (because we only use
    two clusters, this ID is either 0 or 1). Running the command, you will likely
    see a result that places emails in one cluster while putting poems in the other.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，textmix.csv 是输入文件的名称，而 2 是目标簇的数量（在这个特定情况下，两个簇似乎是一个合理的选择，而在其他场景中确定正确的簇数量可能更困难）。结果将存储在
    result.csv 文件中。它包含来自输入文件的所有列，以及一个额外的列，包含簇 ID（因为我们只使用两个簇，这个 ID 要么是 0，要么是 1）。运行命令后，你可能会看到一个结果，将电子邮件放在一个簇中，而将诗歌放在另一个簇中。
- en: You may want to try different models to see differences in run time and result
    quality. You can also try different input text and vary the number of clusters.
    Besides that, you may want to implement some of the other use cases for embedding
    vectors, which are mentioned at the beginning of this section. For instance, how
    about implementing a retrieval interface that maps a natural language statement
    to the most closely related document (by comparing the embedding vectors of questions
    and documents)?
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想尝试不同的模型来观察运行时间和结果质量的不同。你也可以尝试不同的输入文本，并改变簇的数量。除此之外，你可能还想实现本节开头提到的其他嵌入向量的用例。例如，如何实现一个检索界面，将自然语言语句映射到最相关的文档（通过比较问题和文档的嵌入向量）？
- en: 4.4.6 Other use cases for embedding vectors
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.6 嵌入向量的其他用例
- en: So far, we have used vectors to identify similar documents via clustering. But
    this is not the only use case for embedding vectors! To name just a few examples,
    embedding vectors are often used to facilitate the retrieval of text documents
    related to a natural language question. Here, we compare an embedding vector associated
    with the question to embedding vectors associated with documents. Documents with
    similar vectors are more likely to be useful in answering the question.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用向量通过聚类来识别相似文档。但这并不是嵌入向量的唯一用例！仅举几个例子，嵌入向量通常用于促进与自然语言问题相关的文本文档的检索。在这里，我们比较与问题相关联的嵌入向量与与文档相关联的嵌入向量。具有相似向量的文档更有可能在回答问题时有用。
- en: For instance, we hope that the embedding vectors for the question “What is a
    Transformer model?” and the text “The Transformer is a neural network architecture,
    often used for language models” are similar due to related topics. If so, we can
    identify the document most relevant to the question by comparing embedding vectors.
    More precisely, we calculate embedding vectors once for each document that may
    be useful to answer questions. Then, whenever a new question is received, we calculate
    the associated embedding vector and retrieve documents with similar embedding
    vectors. We can then generate an answer based on those documents.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们希望“什么是 Transformer 模型？”这个问题的嵌入向量与“Transformer 是一种神经网络架构，常用于语言模型”这个文本的嵌入向量相似，因为它们有相关的话题。如果是这样，我们可以通过比较嵌入向量来识别与问题最相关的文档。更精确地说，我们为可能有助于回答问题的每个文档计算一次嵌入向量。然后，每当收到一个新的问题时，我们计算相关的嵌入向量并检索具有相似嵌入向量的文档。然后，我们可以根据这些文档生成答案。
- en: Another use case for embedding vectors is outlier detection. To identify text
    documents from a set that are strikingly different from other documents in the
    same set, we can compare their embedding vectors. Again, we only need to calculate
    embedding vectors once for each document. In doing so, we avoid having to use
    language models to compare documents. Instead, we simply compare embedding vectors
    (which is very fast).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量的另一个用例是异常检测。为了识别一组中与其他文档显著不同的文本文档，我们可以比较它们的嵌入向量。同样，我们只需为每个文档计算一次嵌入向量。这样做可以避免使用语言模型来比较文档。相反，我们只需比较嵌入向量（这非常快）。
- en: In summary, although we have focused on clustering, there are many use cases
    for embedding vectors. This makes it worthwhile to learn how to generate and use
    them!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管我们专注于聚类，但嵌入向量有许多用例。这使得学习如何生成和使用它们变得非常有价值！
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can apply language models directly to analyze text data.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以直接应用语言模型来分析文本数据。
- en: Prompts typically contain text to analyze, along with instructions. Instructions
    describe the task to solve as well as the output format.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示通常包含要分析的文字以及指令。指令描述了要解决的问题以及输出格式。
- en: You can use chat completion for classification, extraction, and question answering.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用聊天完成功能进行分类、提取和问答。
- en: Raw model output may need postprocessing to change the format.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始模型输出可能需要后处理以更改格式。
- en: Language models can transform a text into embedding vectors. You can create
    embedding vectors via the embedding endpoint. Comparing embedding vectors is relatively
    efficient.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型可以将文本转换为嵌入向量。您可以通过嵌入端点创建嵌入向量。比较嵌入向量相对高效。
- en: You can use embeddings for clustering, retrieval, and outlier detection.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用嵌入进行聚类、检索和异常检测。
- en: 4.6 References
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 参考文献
- en: 'Katz, D. M., Bommarito, M. J., Gao, S., et al. (2024). GPT-4 Passes the Bar
    Exam. *Philosophical Transactions of the Royal Society A: Mathematical, Physical
    and Engineering Sciences 382*(2270), 1–17.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Katz, D. M., Bommarito, M. J., Gao, S., 等人. (2024). GPT-4 通过了律师资格考试. *皇家学会哲学学报A：数学、物理和工程科学
    382*(2270), 1–17.
