- en: 3 The OpenAI Python library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 OpenAI Python库
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Installing the OpenAI library
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装OpenAI库
- en: Invoking GPT models using Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python调用GPT模型
- en: Configuration parameters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置参数
- en: In the last chapter, we used GPT models via the OpenAI web interface. This works
    well as long as we’re just trying to have a conversation or classify and summarize
    single reviews. However, imagine trying to classify hundreds of reviews. In that
    case, using the web interface manually for each review becomes very tedious (to
    say the least). Also, perhaps we want to use a language model in combination with
    other tools. For instance, we might want to use GPT models to translate questions
    to formal queries and then seamlessly execute those queries in the corresponding
    tool (without having to manually copy queries back and forth between different
    interfaces). In all these scenarios, we need a different interface.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过OpenAI的Web界面使用了GPT模型。如果我们只是尝试进行对话或对单个评论进行分类和总结，这效果很好。然而，想象一下尝试对数百条评论进行分类。在这种情况下，手动为每条评论使用Web界面会变得非常繁琐（至少可以说）。此外，也许我们还想将语言模型与其他工具结合使用。例如，我们可能想使用GPT模型将问题翻译成正式查询，然后在相应的工具中无缝执行这些查询（无需在不同界面之间手动复制查询）。在这些所有场景中，我们需要一个不同的接口。
- en: In this chapter, we’ll discuss a Python library from OpenAI that lets you call
    OpenAI’s language models directly from Python. This enables you to integrate calls
    to language models as a subfunction in your code. We will be using this library
    in most chapters of the book. Therefore, it makes sense to at least skim this
    chapter before proceeding to the following chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一个来自OpenAI的Python库，它允许您直接从Python调用OpenAI的语言模型。这使得您可以将对语言模型的调用作为代码中的子函数集成。我们将在这本书的大部分章节中使用这个库。因此，在继续阅读下一章之前至少浏览本章是有意义的。
- en: Although the current chapter focuses on OpenAI’s Python library, the libraries
    offered by other providers of language models (including Anthropic, Cohere, and
    Google) are similar.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然当前章节侧重于OpenAI的Python库，但其他语言模型提供者（包括Anthropic、Cohere和Google）提供的库是相似的。
- en: 3.1 Prerequisites
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 前提条件
- en: 'First, let’s make sure we have the right environment for OpenAI’s Python library.
    We will use the Python programming language, so make sure Python is installed.
    To do so, open a terminal, and enter the following command (this command should
    work for Linux, macOS, and Windows terminals):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确保我们有适合OpenAI Python库的正确环境。我们将使用Python编程语言，所以请确保Python已安装。要做到这一点，打开一个终端，并输入以下命令（此命令应该适用于Linux、macOS和Windows终端）：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If this command returns an error message, try replacing `python` with `python3`
    in the command and running it again. If Python is installed on your system, you
    should see a version number in reply (e.g., “Python 3.10.13”). If not, you will
    get an error message. For the following examples, you will need at least Python
    3.9 (or a later version). If Python is not installed on your system, or if your
    version is below the required one, visit [www.python.org](http://www.python.org),
    click Downloads, and follow the instructions to install Python. You may also want
    to install an integrated development environment (IDE). PyDev ([www.pydev.org](http://www.pydev.org))
    and PyCharm ([www.jetbrains.com/pycharm](http://www.jetbrains.com/pycharm)) are
    two of the many IDEs available for Python.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个命令返回错误信息，尝试在命令中将`python`替换为`python3`，然后再次运行。如果您的系统上安装了Python，您应该会看到一个版本号作为回复（例如，“Python
    3.10.13”）。如果没有，您将得到一个错误信息。对于以下示例，您至少需要Python 3.9（或更高版本）。如果您的系统上没有安装Python，或者版本低于所需版本，请访问[www.python.org](http://www.python.org)，点击“下载”，并按照说明安装Python。您可能还想安装一个集成开发环境（IDE）。PyDev([www.pydev.org](http://www.pydev.org))和PyCharm([www.jetbrains.com/pycharm](http://www.jetbrains.com/pycharm))是许多适用于Python的IDE中的两个。
- en: 'Along with Python, you will need pip, a package-management system used to install
    Python packages (the OpenAI library comes in the form of such a package). For
    recent Python versions (which you will need in any case), this program is already
    installed by default. Nevertheless, it can’t hurt to make sure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Python，您还需要pip，这是一个用于安装Python包的包管理系统（OpenAI库以这种包的形式提供）。对于最近的Python版本（您无论如何都需要），该程序默认已经安装。尽管如此，确保这一点也无妨：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Again, you should see a version number if everything is installed properly.
    Let’s make sure pip is up to date. The following command should work on Linux,
    macOS, and Windows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，如果一切安装正确，您应该看到一个版本号。让我们确保pip是最新的。以下命令应该在Linux、macOS和Windows上都能工作：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s it! Your system is ready to install the OpenAI Python client.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你的系统已准备好安装OpenAI Python客户端。
- en: What if it doesn’t work?
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如果它不起作用怎么办？
- en: Don’t panic! If any of the previously mentioned steps fail, you may not be able
    to execute the following code on your local machine. However, as long as you have
    web access, you can use a cloud platform instead. For instance, the Google Colab
    platform, accessible at [https://colab.research.google.com](https://colab.research.google.com),
    enables you to create notebooks that can execute all of the following code samples.
    Figure [3.1](#fig__colab) shows the interface after creating a cell installing
    the OpenAI library (upper cell) and the start of a corresponding Python program
    (lower cell). We will discuss library installation and usage in the following
    sections.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 别慌！如果之前提到的任何步骤失败，你可能无法在你的本地机器上执行以下代码。然而，只要你有网络访问，你就可以使用云平台。例如，可访问[https://colab.research.google.com](https://colab.research.google.com)的Google
    Colab平台，允许你创建可以执行所有以下代码样本的笔记本。图[3.1](#fig__colab)显示了创建安装OpenAI库的单元格（上单元格）和相应的Python程序开始（下单元格）后的界面。我们将在以下章节中讨论库的安装和使用。
- en: '![figure](../Images/CH03_F01_Trummer.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Trummer.png)'
- en: Figure 3.1 The Google Colab platform can be used to run the following examples.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 可以使用Google Colab平台运行以下示例。
- en: 3.2 Installing OpenAI’s Python library
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 安装OpenAI的Python库
- en: 'Time to start using GPT like a pro! Although the ChatGPT web interface, discussed
    in chapter 2, is useful for conversations and trying out new prompts, it is unsuitable
    for implementing complex data-processing pipelines. For that, OpenAI’s Python
    library is a much better choice, enabling you to invoke language models directly
    from Python. First, let’s install the corresponding library. Enter the following
    command into a terminal:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候像专业人士一样使用GPT了！尽管第2章讨论的ChatGPT网页界面对于对话和尝试新提示很有用，但它不适合实现复杂的数据处理管道。为此，OpenAI的Python库是一个更好的选择，它允许你直接从Python调用语言模型。首先，让我们安装相应的库。在终端中输入以下命令：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Can I use a different library version?
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我可以使用不同的库版本吗？
- en: You might have noticed the reference to a specific version (version 1.29) of
    the OpenAI library. The code presented in this and the following chapters has
    been tested with this version. As the syntax differs slightly across different
    library versions (unless you are willing to adapt the code), install this precise
    version.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了对OpenAI库特定版本（版本1.29）的引用。本书和后续章节中展示的代码已经与这个版本进行了测试。由于不同版本的库语法略有差异（除非你愿意修改代码），请安装这个精确版本。
- en: Every time we use the OpenAI library, we need to provide a key giving us access
    to the OpenAI models (this is required for billing purposes). If you have not
    yet created an OpenAI account, go to [https://platform.openai.com](https://platform.openai.com),
    click Sign Up, and follow the instructions. If you have an account but are not
    currently logged in, provide your account credentials instead. Make sure to add
    a payment method in the Billing section, and charge it with a couple of dollars.
    After that, if you haven’t done so yet, it is time to generate your secret key.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们使用OpenAI库时，都需要提供一个密钥，以便访问OpenAI模型（这是计费所必需的）。如果你还没有创建OpenAI账户，请访问[https://platform.openai.com](https://platform.openai.com)，点击“注册”，并按照指示操作。如果你已经有了账户但尚未登录，请提供你的账户凭证。确保在“计费”部分添加支付方式，并用几美元进行充值。之后，如果你还没有这样做，现在是时候生成你的密钥了。
- en: Go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    You should see the website shown in figure [3.2](#fig__openaikeys).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)。你应该能看到图[3.2](#fig__openaikeys)中显示的网站。
- en: '![figure](../Images/CH03_F02_Trummer.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F02_Trummer.png)'
- en: Figure 3.2 Managing secret keys for accessing the OpenAI API
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 管理访问OpenAI API的密钥
- en: Click the Create New Secret Key button. The interface will show a text string
    representing the key. Be sure to copy and store that key! You will not be able
    to retrieve the full key again after closing the corresponding window.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建新密钥”按钮。界面将显示代表密钥的文本字符串。务必复制并存储该密钥！关闭相应的窗口后，你将无法再次检索完整的密钥。
- en: 'Whenever we use the Python library, we need to provide our secret key to link
    our requests to the appropriate account. The easiest way to do that is to store
    the secret key in an environment variable named `OPENAI_API_KEY`. OpenAI will
    automatically extract the key from that variable if it exists. The precise command
    used to set environment variables depends on the operating system. For example,
    the following command works for Linux and macOS (replace the three dots with your
    key):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们使用 Python 库时，都需要提供我们的密钥来将我们的请求链接到适当的账户。最简单的方法是将密钥存储在名为 `OPENAI_API_KEY`
    的环境变量中。如果该变量存在，OpenAI 将自动从中提取密钥。设置环境变量的确切命令取决于操作系统。例如，以下命令适用于 Linux 和 macOS（用三个点代替您的密钥）：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Alternatively, you can set the key on a per-invocation basis by prefixing your
    calls to Python with the corresponding assignments. For example, use the following
    command to call the code listing presented in the next section while setting the
    key at the same time (again, substitute your key for the three dots):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过在每个 Python 调用前加上相应的赋值来按调用设置密钥。例如，使用以下命令调用下一节中展示的代码列表，同时设置密钥（同样，用三个点代替您的密钥）：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, if none of the other options work, you can specify your access key
    directly in your Python code. More precisely, right after importing OpenAI’s Python
    library, we can pass the API access key as a parameter when creating the `client`
    object (which we discuss in more detail later):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果其他选项都不起作用，您可以直接在 Python 代码中指定您的访问密钥。更确切地说，在导入 OpenAI 的 Python 库之后，我们可以在创建
    `client` 对象时将 API 访问密钥作为参数传递（我们将在后面更详细地讨论这一点）：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As before, replace the three dots with your OpenAI access key. The following
    code samples assume that the access key is specified in an environment variable
    and will therefore omit this parameter. If environment variables don’t work for
    you, change the code listings by passing your access key as a parameter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，将三个点替换为您的 OpenAI 访问密钥。以下代码示例假设访问密钥已指定在环境变量中，因此将省略此参数。如果环境变量对您不起作用，请通过传递访问密钥作为参数来更改代码列表。
- en: Warning Never share your code if it contains your OpenAI access key. Among other
    things, having your key would enable others to invoke OpenAI’s models while making
    you pay for it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果您的代码中包含您的 OpenAI 访问密钥，切勿共享代码。除此之外，拥有您的密钥将使其他人能够在您付费的情况下调用 OpenAI 的模型。
- en: Assuming you have specified your access key in one way or another, we are now
    ready to start calling GPT models using OpenAI’s Python library.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您以某种方式指定了您的访问密钥，我们现在就可以开始使用 OpenAI 的 Python 库调用 GPT 模型了。
- en: 3.3 Listing available models
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 列出可用模型
- en: We will use the Python library to retrieve a list of available OpenAI models.
    Listing [3.1](#code__listmodels) shows the corresponding Python code. (You can
    download this and all of the following code listings from the book’s companion
    website.) First, we import the OpenAI library (**1**). Then we create a client
    object, enabling us to access library functions (**2**). Next, we query for all
    available OpenAI models (**3**) and print out the result (**4**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 库检索可用 OpenAI 模型的列表。列表 [3.1](#code__listmodels) 展示了相应的 Python 代码。（您可以从本书的配套网站上下载此代码和所有后续的代码列表。）首先，我们导入
    OpenAI 库（**1**）。然后我们创建一个客户端对象，使我们能够访问库函数（**2**）。接下来，我们查询所有可用的 OpenAI 模型（**3**）并打印出结果（**4**）。
- en: Listing 3.1 Listing available OpenAI models
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.1 可用的 OpenAI 模型
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Imports the OpenAI Python library'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入 OpenAI Python 库'
- en: '#2 Creates an OpenAI client'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建 OpenAI 客户端'
- en: '#3 Gets available OpenAI models'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取可用的 OpenAI 模型'
- en: '#4 Prints out the retrieved models'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印出检索到的模型'
- en: 'You should see a result similar to the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的结果：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 GPT-4o'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 GPT-4o'
- en: '#2 Fine-tuned model version'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 微调模型版本'
- en: Each model is described by an ID (e.g., `GPT-4o` (**1**)). We will use this
    ID to tell OpenAI which model we want to use to process our requests. Besides
    the ID, each model comes with a creation timestamp and information about model
    ownership (`owned_by` field). In most cases, models are owned by OpenAI (marked,
    for example, `system` or `openai-internal`). In some cases, however, models are
    owned by `trummerlab` (**2**), the name of the account used by this book’s author.
    Those models are not publicly accessible but private to the owning account. You
    will not see those models when executing the code using your account. They are
    created by a process called *fine-tuning* from the publicly available base models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都由一个ID（例如，`GPT-4o`（**1**））来描述。我们将使用这个ID来告诉OpenAI我们想要使用哪个模型来处理我们的请求。除了ID之外，每个模型还包含一个创建时间戳和关于模型所有权的详细信息（`owned_by`字段）。在大多数情况下，模型归OpenAI所有（例如，标记为`system`或`openai-internal`）。然而，在某些情况下，模型归`trummerlab`（**2**）所有，这是本书作者使用的账户名称。这些模型不是公开可访问的，而是属于拥有账户的私有。当你使用自己的账户执行代码时，你将看不到这些模型。它们是通过称为*微调*的过程从公开可用的基础模型创建的。
- en: What is fine-tuning?
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是微调？
- en: By default, language models such as GPT-4o are trained to be versatile, meaning
    they can, in principle, perform any task. But sometimes we don’t want a model
    that is versatile but rather a model that does very well on one specific task.
    Fine-tuning enables us to specialize a model for a task we care about. We discuss
    fine-tuning in more detail in chapter 9.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，像GPT-4o这样的语言模型被训练成具有多功能性，这意味着原则上它们可以执行任何任务。但有时我们并不需要一个多功能性的模型，而是一个在特定任务上表现非常出色的模型。微调使我们能够为关心的任务专门化一个模型。我们将在第9章中更详细地讨论微调。
- en: 3.4 Chat completion
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 聊天完成
- en: 'Almost all the code in this book uses the same functionality of the OpenAI
    Python library: *chat completion*. With chat completion, your model generates
    a completion for a chat, provided as input. The input can contain various types
    of data, such as text and images. We will exploit those features in the following
    chapters but restrict ourselves to text for the moment. Chat completion is also
    used in the background of OpenAI’s ChatGPT web interface. Given the chat history
    as input (which includes the latest message as well as prior messages, possibly
    containing relevant context), the model generates the most suitable reply.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的几乎所有代码都使用了OpenAI Python库的相同功能：*聊天完成*。使用聊天完成，你的模型会根据提供的输入生成一个聊天完成。输入可以包含各种类型的数据，如文本和图像。我们将在接下来的章节中利用这些功能，但暂时只限于文本。聊天完成也用于OpenAI的ChatGPT网页界面的后台。给定聊天历史作为输入（包括最新的消息以及可能包含相关上下文的先前消息），模型会生成最合适的回复。
- en: 'To use chat completion from Python, we first need a format to describe the
    chat history. This is part of the input we’re providing for chat completion. In
    OpenAI’s Python library, chats are represented as a list of messages. Each message
    in turn is represented as a Python dictionary. This Python dictionary specifies
    values for several important properties of the message. At the very least, we
    need to specify two important attributes for each message:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Python中的聊天完成功能，我们首先需要一个格式来描述聊天历史。这是我们为聊天完成提供的输入的一部分。在OpenAI的Python库中，聊天被表示为一个消息列表。每个消息反过来又表示为一个Python字典。这个Python字典指定了消息的几个重要属性值。至少，我们需要为每个消息指定两个重要属性：
- en: The `role` attribute, which specifies the source of a message
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`role`属性，它指定了消息的来源'
- en: The `content` attribute, which specifies the content of a message
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content`属性，它指定了消息的内容'
- en: Let’s start by discussing the `role` attribute. As you know from the last chapter,
    a chat with GPT models is a back-and-forth series of messages, alternating between
    messages written by the user and messages written by the model. Accordingly, we
    can specify the value `user` for the `role` attribute to identify a message as
    written by the user. Alternatively, we can specify the value `assistant` to mark
    a message as generated by the language model. A third possible value for the `role`
    attribute is `system`. Such messages are typically used at the beginning of a
    chat history. They are meant to convey generic guidelines to the model, independent
    of the specific tasks submitted by users. For instance, a typical system message
    could have the content “You are a helpful assistant,” but more specialized versions
    (e.g., “You are an assistant that translates questions about data sets into SQL
    queries”) are also possible. We will not use `system` messages in this book, but
    feel free to experiment and try adding your own system messages to see if they
    influence the model output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论一下`role`属性。正如你从上一章所知，与GPT模型的聊天是一系列来回的消息，交替使用用户和模型编写的信息。因此，我们可以为`role`属性指定`user`值来标识由用户编写的消息。或者，我们可以指定`assistant`值来标记由语言模型生成的消息。`role`属性的第三个可能值是`system`。这类消息通常用于聊天历史的开始部分。它们的目的是向模型传达通用的指南，而不依赖于用户提交的具体任务。例如，一个典型的系统消息可能是“你是一个有用的助手”，但更专业的版本（例如，“你是一个将数据集相关问题翻译成SQL查询的助手”）也是可能的。在本书中，我们不会使用`system`消息，但请随意实验并尝试添加你自己的系统消息，看看它们是否会影响模型输出。
- en: 'The `content` attribute specifies the content of a message. In this chapter,
    we will restrict ourselves to text content. In later chapters, we will see how
    language models can be used to process more diverse types of content. In the following
    code samples, we will only need to specify a single message in our chat history.
    This message contains instructions describing a task that the language model should
    solve, as well as relevant context information. For instance, the following chat
    history encourages the model to generate a story for us:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`content`属性指定了消息的内容。在本章中，我们将限制自己只使用文本内容。在后面的章节中，我们将看到语言模型如何被用来处理更多样化的内容类型。在下面的代码示例中，我们只需要在我们的聊天历史中指定一条消息。这条消息包含描述语言模型应解决的问题的指令，以及相关的上下文信息。例如，以下聊天历史鼓励模型为我们生成故事：'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Message from user'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用户消息'
- en: '#2 Task specification'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 任务规范'
- en: The list of messages contains only a single message. This message is marked
    as originating from the user (**1**) and describes the previously mentioned task
    in its content (**2**). As a reply, we would expect the model to generate a story
    following the input instructions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 消息列表中只包含一条消息。这条消息被标记为来自用户（**1**），并在其内容中描述了之前提到的任务（**2**）。作为回复，我们期望模型根据输入指令生成故事。
- en: 'How can we invoke a model for chat completion? This can be realized with just
    a few lines of Python code. First, we need to import the OpenAI Python library
    (**1**) and create a `client` object (**2**):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何调用模型以完成聊天？这可以通过几行Python代码实现。首先，我们需要导入OpenAI Python库（**1**）并创建一个`client`对象（**2**）：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Imports the OpenAI Python library'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入OpenAI Python库'
- en: '#2 Creates the OpenAI client'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建OpenAI客户端'
- en: 'We will use the `client` object for all of the following invocations of the
    language model. The previous code appears in almost all of our code samples. Remember
    that you may need to pass the OpenAI access key manually as a parameter when creating
    the client (unless you specify your access key in an environment variable, which
    is the recommended approach). After creating the `client`, we can issue chat completion
    requests as shown here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`client`对象进行所有后续的语言模型调用。之前的代码几乎出现在我们所有的代码示例中。请记住，在创建客户端时，你可能需要手动传递OpenAI访问密钥作为参数（除非你在环境变量中指定了访问密钥，这是推荐的方法）。创建`client`之后，我们可以像下面这样发出聊天完成请求：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Selects a model'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 选择模型'
- en: '#2 Specifies input messages'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指定输入消息'
- en: We use the `client.chat.completions.create` function to create a new request.
    The `model` parameter (**1**) specifies the name of the model we want to use for
    completion. In this case, we’re selecting OpenAI’s GPT-4o model, which can process
    multimodal data. We will use this model for most of the code samples in this book.
    Next, we specify the chat history as input via the `messages` parameter (**2**).
    This is the chat history discussed before, instructing the model to generate a
    story.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`client.chat.completions.create`函数创建一个新的请求。`model`参数（**1**）指定了我们想要用于完成的模型名称。在这种情况下，我们选择了OpenAI的GPT-4o模型，该模型可以处理多模态数据。我们将在这个书的大部分代码示例中使用此模型。接下来，我们通过`messages`参数指定聊天历史作为输入（**2**）。这是之前讨论的聊天历史，指示模型生成一个故事。
- en: Let’s put it all together. The following listing (available as listing 2 in
    the chapter 3 section on the book’s companion website) uses GPT-4o to generate
    a story.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有内容整合起来。以下列表（可在本书配套网站上找到，作为第3章部分的列表2）使用GPT-4o生成一个故事。
- en: Listing 3.2 Using GPT-4o for chat completion
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 使用GPT-4o进行聊天完成
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Imports the OpenAI Python library'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入OpenAI Python库'
- en: '#2 Creates an OpenAI client'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建OpenAI客户端'
- en: '#3 Invokes chat completion'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 调用聊天完成'
- en: '#4 Selects a model/'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 选择模型/'
- en: '#5 Specifies input messages/'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 指定输入消息/'
- en: 'Running the code should produce a result such as the following (your precise
    story may differ due to randomization):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码应产生如下结果（由于随机化，您的故事可能有所不同）：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 List of completions'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 完成列表'
- en: '#2 Termination condition'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 终止条件'
- en: '#3 Completion message'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 完成消息'
- en: '#4 Token usage'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 令牌使用'
- en: Let’s discuss the different components of that result. First, we have a list
    of completion alternatives (**1**) (objects of type `Choice`). In our case, that
    list contains only a single entry. This is the default behavior, although we can
    ask for multiple alternative completions by setting the right configuration parameters
    (discussed in the next section). The `finish_reason` flag (**2**) indicates for
    each completion the reason to stop generating. For instance, this can be due to
    reaching a length limit on generated text. The `stop` value indicates that the
    language model was able to generate complete output (as opposed to reaching a
    length limit). The actual message (**3**) content is abbreviated, and in all likelihood,
    you will see different stories if you invoke the code repeatedly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论该结果的不同组成部分。首先，我们有一个完成选项列表（**1**）（`Choice`类型的对象）。在我们的情况下，该列表只包含一个条目。这是默认行为，尽管我们可以通过设置正确的配置参数来请求多个替代完成（下一节将讨论）。`finish_reason`标志（**2**）表示每个完成停止生成的理由。例如，这可能是由于生成的文本达到长度限制。`stop`值表示语言模型能够生成完整的输出（而不是达到长度限制）。实际消息（**3**）的内容被缩写，很可能会在多次调用代码时看到不同的故事。
- en: 'Besides the completions themselves, the result contains metadata and usage
    statistics (**4**). More precisely, we find values for the following properties:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了完成本身之外，结果还包含元数据和用法统计信息（**4**）。更确切地说，我们发现以下属性有值：
- en: '`completion_tokens`—The number of generated tokens'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`completion_tokens`—生成的令牌数量'
- en: '`prompt_tokens`—The number of tokens in the input'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_tokens`—输入中的令牌数量'
- en: '`total_tokens`—The number of tokens read and generated'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_tokens`—读取和生成的令牌总数'
- en: Why would we care about the number of tokens? Because pricing for most OpenAI
    models is proportional to the number of tokens read and generated. For instance,
    at the time of writing, using GPT-4o costs $5 per million tokens read and $15
    per million tokens generated. Note the difference in pricing between tokens read
    and generated. Typically, as in this case, generating tokens is more expensive
    than reading tokens. The pricing depends not only on the number of tokens but
    also on the model used. For example, replacing GPT-4o with the GPT-3.5 Turbo model
    (a slightly less powerful GPT version) cuts costs by a factor of 10\. Before analyzing
    large amounts of data with language models, choose the appropriate model size
    for your task and wallet.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要关心令牌的数量？因为大多数OpenAI模型的定价与读取和生成的令牌数量成正比。例如，在撰写本文时，使用GPT-4o的成本是每百万个读取的令牌5美元，每百万个生成的令牌15美元。注意读取和生成令牌之间的定价差异。通常，如本例所示，生成令牌比读取令牌更昂贵。定价不仅取决于令牌数量，还取决于使用的模型。例如，用GPT-3.5
    Turbo模型（一个稍微弱一点的GPT版本）替换GPT-4o可以将成本降低10倍。在用语言模型分析大量数据之前，请选择适合您任务和钱包的适当模型大小。
- en: 3.5 Customizing model behavior
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 自定义模型行为
- en: You can use various parameters to influence how the model replies to your input.
    These parameters can be specified in addition to the `model` and `messages` parameters
    when invoking the `chat.completions.create` function. In this section, we discuss
    different categories of parameters, classifying parameters by the aspect of model
    behavior they influence.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用各种参数来影响模型对您输入的回复方式。在调用 `chat.completions.create` 函数时，除了 `model` 和 `messages`
    参数外，还可以指定这些参数。在本节中，我们将讨论不同类别的参数，按它们影响的模型行为方面进行分类。
- en: 3.5.1 Configuring termination conditions
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 配置终止条件
- en: When we invoke a model for chat completion, it generates output until a stopping
    condition is met. The two parameters discussed next enable us to configure when
    text generation stops.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用聊天完成模型时，它会生成输出，直到满足停止条件。接下来讨论的两个参数使我们能够配置何时停止文本生成。
- en: The `max_tokens` parameter specifies the maximum number of tokens (i.e., the
    atomic unit at which language models represent text) generated during completion.
    A token corresponds to approximately four characters, and a typical paragraph
    contains around 100 tokens. The maximum admissible value for this parameter is
    determined by the model used. For instance, ada, one of the smallest GPT versions,
    allows up to 2,049 tokens, whereas GPT-4o supports up to 128,000 tokens. Keep
    in mind that the maximum number of tokens supported by the model includes tokens
    read and tokens generated. As `max_tokens` refers only to the number of tokens
    generated, you should not set it higher than the maximum number of tokens supported
    by the model used *minus the number of tokens in the prompt*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_tokens` 参数指定在完成过程中生成的最大标记数（即语言模型表示文本的原子单位）。一个标记大约对应四个字符，一个典型的段落大约包含 100
    个标记。此参数的最大可接受值由所使用的模型决定。例如，ada 是最小的 GPT 版本之一，允许最多 2,049 个标记，而 GPT-4o 支持多达 128,000
    个标记。请记住，模型支持的标记数包括读取的标记和生成的标记。由于 `max_tokens` 仅指生成的标记数，因此您不应将其设置得高于所使用模型支持的标记数减去提示中的标记数。'
- en: As a general rule, setting a reasonable value for `max_tokens` is almost always
    a good idea. After all, we’re paying for each generated token, and setting a bound
    on the number of tokens enables you to bound monetary fees per model invocation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般规则，为 `max_tokens` 设置一个合理的值几乎总是好主意。毕竟，我们为每个生成的标记付费，对标记数设置限制可以使您对每次模型调用的货币费用进行限制。
- en: In some scenarios, specific text patterns indicate the end of the desired output.
    For instance, when generating code, it can be a string specific to the corresponding
    programming language indicating the end of the program. On the other hand, when
    generating a fairy tale, it can be the string “and they lived happily ever after!”
    In those scenarios, we might want to use the `stop` parameter to configure the
    OpenAI library to stop generating output whenever a specific token sequence appears.
    In some cases, there is only one token sequence indicating termination. In those
    scenarios, we can directly assign the `stop` parameter to the corresponding string
    value. In other scenarios, there are multiple candidate sequences that indicate
    termination. In those cases, we can assign the `stop` parameter to a list of up
    to four sequences. Text generation terminates whenever any of those sequences
    is generated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，特定的文本模式表示所需输出的结束。例如，在生成代码时，可以是特定于相应编程语言的字符串，表示程序的结束。另一方面，在生成童话时，可以是字符串“从此他们幸福地生活在一起！”在这些场景中，我们可能希望使用
    `stop` 参数来配置 OpenAI 库，以便在出现特定标记序列时停止生成输出。在某些情况下，只有一个标记序列表示终止。在这些场景中，我们可以直接将 `stop`
    参数赋值为相应的字符串值。在其他场景中，存在多个候选序列表示终止。在这种情况下，我们可以将 `stop` 参数赋值为最多四个序列的列表。只要生成这些序列中的任何一个，文本生成就会终止。
- en: Note that you can use both of the previously mentioned parameters together.
    In those cases, output generation stops whenever the length limit is reached or
    one of the stop sequences appears (whichever happens first).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以使用之前提到的两个参数一起使用。在这种情况下，输出生成会在达到长度限制或出现任何一个停止序列时停止（哪个先发生就先停止）。
- en: 3.5.2 Configuring output generation
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 配置输出生成
- en: The parameters we just discussed enable you to choose when the output terminates.
    But how can you influence the output generated until that point? Here, OpenAI
    offers a few parameters that enable you to bias the way in which GPT models select
    output text.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才讨论的参数使您能够选择何时终止输出。但您如何影响直到那个点生成的输出呢？在这里，OpenAI提供了一些参数，使您能够影响GPT模型选择输出文本的方式。
- en: Several parameters enable you to influence how “repetitive” the generated output
    should be. More precisely, those parameters allow you to influence whether generating
    the same tokens repeatedly is desirable or not.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 几个参数使您能够影响生成的输出应该有多“重复”。更精确地说，这些参数允许您影响是否重复生成相同的标记是否可取。
- en: The `presence_penalty` parameter enables you to penalize chat completions that
    use the same tokens repeatedly. The presence penalty is a value between –2 and
    +2 (with a default value of 0). A positive penalty encourages the model to avoid
    reusing the same tokens. A negative penalty, on the other hand, encourages the
    model to use the same tokens repeatedly. The higher the absolute value, the stronger
    the corresponding effect.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`presence_penalty`参数使您能够惩罚重复使用相同标记的聊天完成。存在惩罚是一个介于-2和+2之间的值（默认值为0）。正惩罚鼓励模型避免重复使用相同的标记。另一方面，负惩罚鼓励模型重复使用相同的标记。绝对值越高，相应的影响越强。'
- en: The `frequency_penalty` relates to the prior parameter but enables a more fine-grained
    penalization scheme. The `presence_penalty` parameter is based on the mere *presence*
    of a token. For example, we do not differentiate between a token that appears
    twice and one that appears hundreds of times. The frequency penalty is used as
    a factor, multiplying the number of prior appearances of a token when aggregating
    its score (which is used to determine whether the token should appear next). Hence,
    the more often a token was used before, the less likely it is to appear again.
    Similar to the presence penalty, the `frequency_penalty` parameter takes values
    between –2 and +2 with a default setting of 0\. A positive penalty factor encourages
    GPT models to avoid repeating the same token, whereas a negative value encourages
    repetitions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`frequency_penalty`与先前的参数相关，但允许更精细的惩罚方案。`presence_penalty`参数基于标记的“存在”。例如，我们不区分出现两次的标记和出现数百次的标记。频率惩罚用作一个因子，在聚合标记的分数（用于确定标记是否应该出现）时乘以前出现次数。因此，一个标记被使用得越频繁，它再次出现的可能性就越小。与存在惩罚类似，`frequency_penalty`参数的值介于-2和+2之间，默认设置为0。正惩罚因子鼓励GPT模型避免重复相同的标记，而负值鼓励重复。'
- en: Sometimes we are only interested in one of a limited set of eligible tokens.
    For instance, when classifying text, the set of classes is typically determined
    a priori. If so, let’s tell the model about it! The `logit_bias` parameter allows
    mapping token IDs to a bias factor. A high bias factor encourages the model to
    consider the corresponding token as output. A sufficiently low bias score essentially
    prevents the model from using the token. A sufficiently high score almost guarantees
    that the corresponding token will appear in the output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们只对有限集合中的一个标记感兴趣。例如，在文本分类中，类别的集合通常是事先确定的。如果是这样，让我们告诉模型吧！`logit_bias`参数允许将标记ID映射到偏置因子。高偏置因子鼓励模型将相应的标记视为输出。足够低的偏置分数实际上阻止模型使用该标记。足够高的分数几乎可以保证相应的标记将出现在输出中。
- en: Using the `logit_bias` parameter avoids generating useless output in situations
    where we can narrow the set of reasonable tokens. The value for `logit_bias` is
    a Python dictionary that maps token IDs to values between –100 and +100\. Values
    between –1 and +1 are more typical and still give the model room to consider tokens
    with a low value (or to avoid using tokens that are associated with higher values).
    But how do we find the token IDs associated with relevant words? For that, we
    can use the GPT tokenizer tool, available at [https://platform.openai.com/tokenizer?view=bpe](https://platform.openai.com/tokenizer?view=bpe).
    Simply enter the words you want to encourage (or ban), and the associated token
    IDs will be displayed. Note that multiple tokenizer variants are available, associated
    with different models. Select the right tokenizer for your model (because otherwise,
    the token IDs may be incorrect).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`logit_bias`参数可以避免在我们可以缩小合理标记集合的情况下生成无用的输出。`logit_bias`的值是一个Python字典，将标记ID映射到-100到+100之间的值。介于-1和+1之间的值更为典型，同时仍然给模型留出考虑低值标记（或避免使用与高值相关的标记）的空间。但如何找到与相关单词相关的标记ID呢？为此，我们可以使用GPT标记化工具，该工具可在[https://platform.openai.com/tokenizer?view=bpe](https://platform.openai.com/tokenizer?view=bpe)找到。只需输入你想要鼓励（或禁止）的单词，相关的标记ID就会显示出来。请注意，有多种标记化变体可用，与不同的模型相关联。选择适合你模型的标记化工具（因为否则，标记ID可能是不正确的）。
- en: 3.5.3 Configuring randomization
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 配置随机化
- en: How do GPT models select the next output token? At a high level of abstraction,
    we calculate scores for all possible output tokens and then select a token based
    on those scores. Although tokens with higher scores tend to have better chances
    of being selected, we might not always want to select the token with the maximum
    score. For instance, think back to chapter 2, where we were able to regenerate
    replies for the same input, potentially leading to different results. This can
    be useful if the first output does not quite satisfy our requirements. If always
    selecting the tokens with the highest scores, regenerating an answer would be
    unlikely to change the output. Hence, to enable users to get diverse replies,
    we need to introduce a certain degree of randomization when mapping scores to
    output tokens.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型是如何选择下一个输出标记的？在高度抽象的层面上，我们计算所有可能的输出标记的分数，然后根据这些分数选择一个标记。尽管得分较高的标记通常有更好的被选中的机会，但我们可能并不总是想选择得分最高的标记。例如，回想一下第2章，我们能够为相同的输入生成回复，可能得到不同的结果。这可能是有用的，如果第一个输出并不完全满足我们的要求。如果我们总是选择得分最高的标记，那么重新生成答案不太可能改变输出。因此，为了使用户能够获得多样化的回复，我们需要在将分数映射到输出标记时引入一定程度的随机化。
- en: Of course, decoupling the output too much from token scores—that is, using too
    much randomization—may lead to useless output (at the extreme, the output no longer
    connects to the input and does not follow our instructions). On the other hand,
    using too little randomization can lead to outputs that are less diverse than
    desired. Choosing the right degree of randomization for a specific scenario can
    take some experimentation. In each case, OpenAI offers multiple parameters that
    enable you to fine-tune how token scores translate to output tokens. We will discuss
    those parameters next.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，过度解耦输出与标记分数——即使用过多的随机化——可能会导致无用的输出（在极端情况下，输出不再与输入相关联，也不遵循我们的指令）。另一方面，使用过少的随机化可能会导致输出的多样性低于预期。为特定场景选择合适的随机化程度可能需要一些实验。在每种情况下，OpenAI都提供了多个参数，允许你微调标记分数如何转换为输出标记。我们将在下一节讨论这些参数。
- en: One of the parameters most commonly used to tune randomization is the `temperature`
    parameter. A higher temperature means more randomization, whereas a lower temperature
    corresponds to less randomization. A low degree of randomization means the token
    with the highest score is very likely to be selected. A very high degree of randomization
    means tokens are (almost) selected with equal probability, independently of the
    scores assigned by the model. The `temperature` parameter enables you to thread
    the needle between those two extremes. Values for this parameter are chosen between
    0 and 2 with a default of 1.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 调整随机化最常用的参数之一是`温度`参数。温度越高，随机化程度越高；而温度越低，随机化程度则越低。低随机化程度意味着得分最高的标记很可能被选中。非常高的随机化程度意味着标记几乎以相等的概率被选中，与模型分配的分数无关。`温度`参数允许你在这两种极端之间找到平衡。此参数的值在0到2之间选择，默认值为1。
- en: 'Temperature is one possibility when choosing the degree of randomization. The
    `top_p` parameter is an alternative approach. (It is not recommended that you
    alter both `temperature` and `top_p` in the same invocation of the language model.)
    Based on their scores, we can associate a probability of being “correct” with
    each possible output token. Now imagine that we are sorting those tokens in decreasing
    order of probability. We can reduce the degree of randomization by focusing only
    on the first few tokens: we neglect tokens with lower probability. How many tokens
    should we consider? Instead of fixing the number of eligible tokens directly,
    the `top_p` parameter fixes the *probability mass* of those tokens. In other words,
    we add tokens to the set of eligible tokens in decreasing order of probability.
    Whenever the sum of probability values of all selected tokens (the probability
    mass) exceeds the value of `top_p`, we stop adding tokens. Finally, we pick the
    next output token among those eligible tokens.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是选择随机化程度的一个可能性。`top_p` 参数是另一种方法。（不建议你在语言模型的同一调用中同时更改 `temperature` 和 `top_p`。）根据它们的分数，我们可以将每个可能的输出标记与“正确”的概率关联起来。现在想象一下，我们正在按概率递减的顺序对这些标记进行排序。我们可以通过只关注前几个标记来减少随机化的程度：我们忽略概率较低的标记。我们应该考虑多少个标记？而不是直接固定合格标记的数量，`top_p`
    参数固定了这些标记的 *概率质量*。换句话说，我们按概率递减的顺序将标记添加到合格标记的集合中。每当所有选中标记的概率值之和（概率质量）超过 `top_p`
    的值时，我们就停止添加标记。最后，我们从合格标记中挑选下一个输出标记。
- en: As the `top_p` parameter describes a probability, its values are taken from
    the interval between 0 and 1\. Similar to temperature, choosing a higher value
    leads to more randomization (because even tokens with lower probability become
    eligible).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `top_p` 参数表示一个概率，其值取自 0 和 1 之间的区间。与温度类似，选择更高的值会导致更多的随机化（因为即使概率较低的标记也变得合格）。
- en: As soon as we are using a certain degree of randomization, it becomes useful
    to generate multiple answers for the same input prompt. After that, we can choose
    the preferred answer via postprocessing. For instance, assume that we are generating
    multiple SQL queries for the same input prompt. To select the preferred answer,
    we can try executing them on a target database and discard the queries that result
    in a syntax error message. Of course, we can simply call the language model repeatedly
    with the same prompt. However, it is more efficient to call the language model
    once and configure the number of generated replies. The parameter `n` determines
    the number of generated replies. By default, this parameter is set to 1 (i.e.,
    only a single answer is generated). You may choose a higher value to obtain more
    replies. Note that using a higher value for this parameter also increases per-invocation
    costs (because you pay for each token generated, counting tokens across different
    replies).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用了一定程度的随机化，生成相同输入提示的多个答案就变得有用。之后，我们可以通过后处理选择首选答案。例如，假设我们正在为相同的输入提示生成多个
    SQL 查询。为了选择首选答案，我们可以在目标数据库上尝试执行它们，并丢弃导致语法错误消息的查询。当然，我们可以简单地重复使用相同的提示调用语言模型。然而，一次调用语言模型并配置生成的回复数量更有效率。参数
    `n` 确定了生成的回复数量。默认情况下，此参数设置为 1（即只生成一个答案）。你可以选择更高的值以获得更多回复。请注意，使用此参数的更高值也会增加每次调用的成本（因为你为每个生成的标记付费，包括不同回复中的标记）。
- en: 3.5.4 Customization example
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 自定义示例
- en: Let’s try some of the parameters in our code. The following listing prompts
    GPT-4o to write a story, this time using some of the parameters we’ve discussed
    to customize chat completion.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中尝试一些参数。以下列表提示 GPT-4o 编写一个故事，这次使用我们讨论的一些参数来自定义聊天完成。
- en: Listing 3.3 Using GPT-4o for chat completion with custom parameter settings
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3 使用 GPT-4o 进行带有自定义参数设置的聊天完成
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Limits the output length'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 限制输出长度'
- en: '#2 Sets a stopping condition'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 设置停止条件'
- en: '#3 Sets temperature'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置温度'
- en: '#4 Penalizes repetitions'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对重复进行惩罚'
- en: '#5 Adds bias'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 添加偏差'
- en: 'First, to avoid getting a lengthy story, we set the maximum number of tokens
    to 512 (**1**). This should suffice for a few paragraphs of text. To avoid generating
    more content than necessary, we define a stop sequence as well: “happily ever
    after” (**2**). We do so, hoping that any reasonable story will, of course, end
    with the popular expression “and they lived happily ever after.” This assumption
    is certainly simplifying.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了避免得到冗长的故事，我们将最大标记数设置为512（**1**）。这应该足以容纳几段文字。为了避免生成不必要的多余内容，我们还定义了一个停止序列：“幸福地生活下去”（**2**）。我们这样做，希望任何合理的故事当然都会以流行的表达“他们幸福地生活下去”结束。这个假设无疑是简化的。
- en: Next, we set the temperature to 1.5 (**3**), allowing a certain degree of randomization.
    Having an overly repetitive story is boring. Therefore, we set a positive presence
    penalty (`presence_penalty`=0.5) (**4**), discouraging GPT-4o from reusing the
    same tokens. Finally, we don’t want a scary story (perhaps we want to read it
    to our kids), so we penalize the word *dragon* (we obtain the corresponding token
    ID as described in section [3.5.2](#sub__ConfigureGeneration)) (**5**).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将温度设置为1.5（**3**），允许一定程度的随机化。一个过于重复的故事是无聊的。因此，我们设置了一个正存在惩罚（`presence_penalty`=0.5）（**4**），以阻止GPT-4o重复使用相同的标记。最后，我们不想得到一个恐怖的故事（也许我们想给孩子读），所以我们惩罚了单词*龙*（我们获得相应的标记ID，如第[3.5.2](#sub__ConfigureGeneration)节所述）（**5**）。
- en: 'Let’s see what we get! For instance, in a sample invocation, GPT-4o generates
    the following story:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么！例如，在一个样本调用中，GPT-4o生成了以下故事：
- en: '**![chatGpt](../Images/chatGpt.png)***GPT* ****'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**![chatGpt](../Images/chatGpt.png)***GPT* ****'
- en: '*Once upon a time, in a quaint village nestled between rolling hills and dense
    forests, there lived a girl named Elara. She was known for her curiosity and adventurous
    spirit, traits that often led her to explore the uncharted parts of her village.
    One crisp autumn morning, Elara decided to venture into the Whispering Woods,
    a place shrouded in mystery and legend. The villagers often spoke of the woods
    as a place where magic lingered, but also one that nobody dared to enter beyond
    the edge. Equipped with her trusty satchel and a heart full of excitement, Elara
    set off with the sunrise. As she walked deeper into the woods, the whispers of
    rustling leaves seemed to greet her, and she felt an odd sense of familiarity.
    She wandered through thickets and climbed over fallen logs until she stumbled
    upon a hidden grove, bathed in shimmering light. In the center of the grove stood
    an ancient tree with silvery leaves, its trunk twisted like spiraling tendrils
    of smoke. Elara was mesmerized; it looked unlike any tree she had ever seen. She
    approached it cautiously and noticed a small, ornate chest nestled among its roots.
    Her heart racing, Elara opened the chest and found inside a delicate, golden locket.
    As soon as she touched it, the forest seemed to come alive. The whispers grew
    louder, forming words that she could understand. “Brave heart, you have found
    the Locket of Eternia,” the voice murmured. “It grants the bearer one wish.” Elara’s
    mind raced with possibilities, but she knew what her heart desired most: to bring
    joy and prosperity to her beloved village. Clutching the locket, she whispered
    her wish. In an instant, a warm glow enveloped her, and she felt a surge of energy.
    The grove shimmered momentarily and then returned to its tranquil stillness. Elara
    knew something extraordinary had happened. As she returned home, she noticed subtle
    changes—the crops seemed more vibrant, the animals healthier and the village air
    filled with a sense of contentment. The villagers welcomed her back, their faces
    glowing with happiness. Elara never revealed the secret of the Whispering Woods
    or the Locket of Eternia to anyone. She simply smiled whenever anyone commented
    on the newfound prosperity of the village. She knew that sometimes, the greatest
    magic lies not in what is seen, but in the courage to follow one’s heart. And
    so, Elara’s village thrived, becoming a beacon of joy and harmony. All because
    one brave girl dared to listen to the whispers of the woods. And she lived*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从前，在一个坐落在起伏的山丘和茂密森林之间的古老村庄里，住着一个名叫艾拉的女孩。她以好奇心和冒险精神著称，这些特质常常使她探索村庄未知的角落。在一个清爽的秋晨，艾拉决定冒险进入
    Whispering Woods，一个充满神秘和传说的地方。村民们经常谈论这片森林，说那里有魔法徘徊，但也没有人敢越过边缘进入。带着她信任的背包和一颗充满激情的心，艾拉在日出时分出发了。当她深入森林时，树叶沙沙作响的窃窃私语似乎在欢迎她，她感到一种奇怪的熟悉感。她穿过灌木丛，爬过倒下的树木，直到她偶然发现一个隐藏的树林，沐浴在闪烁的光芒中。树林中央有一棵古老的树，树叶银白，树干扭曲如螺旋状的烟雾。艾拉被迷住了；它看起来是她所见过的任何树都不一样。她小心翼翼地走近，注意到树根中有一个小巧精致的箱子。她的心跳加速，艾拉打开了箱子，里面发现了一个精致的金色护身符。一触碰到它，森林似乎就活了过来。窃窃私语变得更响亮，形成了她能理解的话语。“勇敢的心，你找到了永恒之锁，”声音低语。“它赋予持有者一个愿望。”艾拉的心中充满了可能性，但她知道她心中最渴望的是什么：为她深爱的村庄带来欢乐和繁荣。紧握着护身符，她低声许下了她的愿望。瞬间，一股温暖的亮光包围了她，她感到一股能量的涌动。树林闪烁了一下，然后又恢复了宁静。艾拉知道发生了非凡的事情。当她回家时，她注意到一些微妙的变化——庄稼看起来更加生机勃勃，动物看起来更健康，村庄的空气中充满了满足感。村民们欢迎她回来，他们的脸上洋溢着幸福。艾拉从未向任何人透露
    Whispering Woods 或永恒之锁的秘密。每当有人评论村庄的新兴繁荣时，她只是微笑。她知道，有时候，最大的魔法不在于所见，而在于跟随自己内心的勇气。因此，艾拉的村庄繁荣起来，成为欢乐和和谐的灯塔。这一切都因为一个勇敢的女孩敢于倾听森林的低语。她就这样生活着
- en: Happily ever after! It turns out that our stop sequence, the expression “happily
    ever after,” was indeed used at the end of the story (and is therefore omitted
    in the output returned by GPT-4o). Try a few more parameter settings, and see
    how the result changes as a function of the configuration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 幸福地生活下去！结果证明，我们的停止序列，“幸福地生活下去”的表达确实用于故事的结尾（因此，GPT-4o 返回的输出中省略了它）。尝试更多的参数设置，看看结果如何随着配置的变化而变化。
- en: 3.5.5 Further parameters
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.5 进一步的参数
- en: We have discussed the most important parameters for data-analysis purposes.
    You can use each of them when requesting a completion from OpenAI’s GPT models.
    Note that there are more parameters beyond the ones mentioned in this chapter.
    OpenAI’s API reference documentation ([https://platform.openai.com/docs/api](https://platform.openai.com/docs/api-reference/completions)[-reference/completions](https://platform.openai.com/docs/api-reference/completions))
    describes all parameters in detail.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了用于数据分析目的最重要的参数。您可以在请求OpenAI的GPT模型完成内容时使用它们。请注意，除了本章中提到的参数之外，还有更多参数。OpenAI的API参考文档([https://platform.openai.com/docs/api](https://platform.openai.com/docs/api-reference/completions)[-reference/completions](https://platform.openai.com/docs/api-reference/completions))详细描述了所有参数。
- en: Summary
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can use OpenAI’s language models via a Python API. Other providers offer
    similar libraries for accessing their models.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过Python API使用OpenAI的语言模型。其他提供商提供类似的库来访问他们的模型。
- en: To use OpenAI’s library, create a client object.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用OpenAI的库，创建一个客户端对象。
- en: You can use OpenAI’s models to complete chats. Chats to complete are specified
    as a list of messages.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用OpenAI的模型来完成聊天。要完成的聊天指定为消息列表。
- en: Each chat message is characterized by content and a role. Roles can be one of
    `user`, `assistant`, or `system`.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条聊天消息都由内容和角色特征化。角色可以是`user`、`assistant`或`system`之一。
- en: Obtain chat completions via the `chat.completions.create` function.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`chat.completions.create`函数获取聊天完成内容。
- en: 'You can configure models using various parameters:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用各种参数配置模型：
- en: The `max_tokens` parameter limits the number of tokens generated.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`max_tokens`参数限制了生成的标记数量。'
- en: '`stop` lets you define phrases that stop text generation.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`stop`允许您定义停止文本生成的短语。'
- en: You can penalize or encourage specific tokens via `logit_bias`.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以通过`logit_bias`对特定的标记进行惩罚或鼓励。
- en: '`presence_penalty` penalizes repetitive output.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`presence_penalty`对重复的输出进行惩罚。'
- en: '`frequency_penalty` penalizes repetitive output.'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`frequency_penalty`对重复的输出进行惩罚。'
- en: '`temperature` chooses the degree of randomization.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`temperature`参数选择随机化的程度。'
- en: '`top_p` determines the number of output tokens considered.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`top_p`确定考虑的输出标记数量。'
- en: '`n` chooses the number of generated completions.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`n`参数选择生成的完成内容的数量。'
