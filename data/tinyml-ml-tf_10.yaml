- en: 'Chapter 10\. Person Detection: Training a Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。人物检测：训练模型
- en: In [Chapter 9](ch09.xhtml#chapter_person_detection_example), we showed how you
    can deploy a pretrained model for recognizing people in images, but we didn’t
    explain where that model came from. If your product has different requirements,
    you’ll want to be able to train your own version, and this chapter explains how
    to do that.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#chapter_person_detection_example)中，我们展示了如何部署一个用于识别图像中人物的预训练模型，但我们没有解释该模型来自何处。如果您的产品有不同的要求，您将希望能够训练自己的版本，本章将解释如何做到这一点。
- en: Picking a Machine
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一台机器
- en: Training this image model takes a lot more compute power than our previous examples,
    so if you want your training to complete in a reasonable amount of time, you’ll
    need to use a machine with a high-end graphics processing unit (GPU). Unless you
    expect to be running a lot of training jobs, we recommend starting off by renting
    a cloud instance rather than buying a special machine. Unfortunately the free
    Colaboratory service from Google that we’ve used in previous chapters for smaller
    models won’t work, and you will need to pay for access to a machine. There are
    many great providers available, but our instructions will assume you’re using
    Google Cloud Platform because that’s the service we’re most familiar with. If
    you are already using Amazon Web Services (AWS) or Microsoft Azure, they also
    have TensorFlow support and the training instructions should be the same, but
    you’ll need to follow their tutorials for setting up a machine.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个图像模型需要比我们之前的示例更多的计算资源，因此如果您希望训练在合理的时间内完成，您需要使用一台配备高端图形处理单元（GPU）的机器。除非您期望运行大量训练作业，我们建议您首先租用云实例而不是购买特殊的机器。不幸的是，我们在之前章节中用于较小模型的免费Colaboratory服务将不起作用，您需要支付访问机器的费用。有许多优秀的提供商可供选择，但我们的说明将假定您正在使用谷歌云平台，因为这是我们最熟悉的服务。如果您已经在使用亚马逊网络服务（AWS）或微软Azure，它们也支持TensorFlow，并且训练说明应该是相同的，但您需要按照它们的教程设置机器。
- en: Setting Up a Google Cloud Platform Instance
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置谷歌云平台实例
- en: 'You can rent a virtual machine with TensorFlow and NVIDIA drivers preinstalled
    from Google Cloud Platform, and with support for a Jupyter Notebook web interface,
    which can be very convenient. The route to setting this up can be a bit involved,
    though. As of September 2019, here are the steps you need to take to create a
    machine:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从谷歌云平台租用一个预安装了TensorFlow和NVIDIA驱动程序的虚拟机，并支持Jupyter Notebook网络界面，这可能非常方便。不过，设置这一点可能有点复杂。截至2019年9月，以下是您需要执行的步骤来创建一台机器：
- en: Sign in to [*console.cloud.google.com*](https://oreil.ly/Of6oo). You’ll need
    to create a Google account if you don’t already have one, and you’ll have to set
    up billing to pay for the instance you create. If you don’t already have a project,
    you’ll need to create one.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录[*console.cloud.google.com*](https://oreil.ly/Of6oo)。如果您还没有谷歌账号，您需要创建一个，您还需要设置计费来支付您创建的实例。如果您还没有项目，您需要创建一个。
- en: In the upper-left corner of the screen, open the hamburger menu (the main menu
    with three horizontal lines as an icon, as illustrated in [Figure 10-1](#ai_platform_menu))
    and scroll down until you find the Artificial Intelligence section.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕的左上角，打开汉堡菜单（带有三条水平线的主菜单图标，如[#ai_platform_menu](#ai_platform_menu)所示），向下滚动直到找到人工智能部分。
- en: In this section, select AI Platform→Notebooks, as shown in [Figure 10-1](#ai_platform_menu).
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此部分中，选择AI平台→笔记本，如[#ai_platform_menu](#ai_platform_menu)所示。
- en: '![The AI Platform menu](Images/timl_1001.png)'
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![AI平台菜单](Images/timl_1001.png)'
- en: Figure 10-1\. The AI Platform menu
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。AI平台菜单
- en: You might see a prompt asking you to enable the Compute Engine API to proceed,
    as depicted in [Figure 10-2](#api_enable); go ahead and approve it. This can take
    several minutes to go through.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可能会看到一个提示，要求您启用计算引擎API以继续，如[#api_enable](#api_enable)所示；请继续批准。这可能需要几分钟的时间。
- en: '![The Compute Engine API](Images/timl_1002.png)'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算引擎API](Images/timl_1002.png)'
- en: Figure 10-2\. The Compute Engine API screen
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。计算引擎API界面
- en: A “Notebook instances” screen will open. In the menu bar at the top, select
    NEW INSTANCE. On the submenu that opens, choose “Customize instance,” as shown
    in [Figure 10-3](#new_instance).
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将打开一个“笔记本实例”屏幕。在顶部的菜单栏中，选择NEW INSTANCE。在打开的子菜单中，选择“自定义实例”，如[#new_instance](#new_instance)所示。
- en: '![The instance creation menu](Images/timl_1003.png)'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![实例创建菜单](Images/timl_1003.png)'
- en: Figure 10-3\. The instance creation menu
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3。实例创建菜单
- en: On the “New notebook instance” page, in the “instance name” box, give your machine
    a name, as illustrated in [Figure 10-4](#instance_naming), and then scroll down
    to set up the environment.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“新笔记本实例”页面上，在“实例名称”框中，为您的机器命名，如[#instance_naming](#instance_naming)所示，然后向下滚动设置环境。
- en: '![The instance naming interface](Images/timl_1004.png)'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![实例命名界面](Images/timl_1004.png)'
- en: Figure 10-4\. The naming interface
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4。命名界面
- en: As of September 2019, the correct TensorFlow version to choose is TensorFlow
    1.14\. The recommended version will likely have increased to 2.0 or beyond by
    the time you’re reading this, but there might be some incompatibilities, so if
    it’s still possible start by selecting 1.14 or another version in the 1.x branch.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截至2019年9月，选择的正确TensorFlow版本是TensorFlow 1.14。推荐的版本可能在您阅读本文时已经增加到2.0或更高，但可能存在一些不兼容性，因此如果可能的话，请从选择1.14或1.x分支的其他版本开始。
- en: In the “Machine configuration” section, choose at least 4 CPUs and 15 GB of
    RAM, as shown in [Figure 10-5](#system_setup).
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“机器配置”部分，选择至少4个CPU和15GB的RAM，如[#system_setup](#system_setup)所示。
- en: '![The CPU and version interface](Images/timl_1005.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CPU和版本界面](Images/timl_1005.png)'
- en: Figure 10-5\. The CPU and version interface
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。CPU和版本界面
- en: Picking the right GPU will make the biggest difference in your training speed.
    It can be tricky because not all zones offer the same kind of hardware. In our
    case, we’re using “us-west1 (Oregon)” as the region and “us-west-1b” as the zone
    because we know that they currently offer high-end GPUs. You can get the detailed
    pricing information using [Google Cloud Platform’s pricing calculator](https://oreil.ly/t2XO0),
    but for this example we’re choosing one NVIDIA Tesla V100 GPU, as illustrated
    in [Figure 10-6](#gpu_selection). This costs $1,300 a month to run but allows
    us to train the person-detector model in around a day, so the model training cost
    works out to about $45.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择正确的GPU将在训练速度上产生最大的差异。这可能有些棘手，因为并非所有区域都提供相同类型的硬件。在我们的情况下，我们使用“us-west1（俄勒冈州）”作为地区，“us-west-1b”作为区域，因为我们知道它们目前提供高端GPU。您可以使用[Google
    Cloud Platform的定价计算器](https://oreil.ly/t2XO0)获取详细的定价信息，但在本例中，我们选择了一块NVIDIA Tesla
    V100 GPU，如[图10-6](#gpu_selection)所示。这每月花费1300美元，但可以让我们在大约一天内训练人员检测器模型，因此模型训练成本约为45美元。
- en: '![The GPU selection interface](Images/timl_1006.png)'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![GPU选择界面](Images/timl_1006.png)'
- en: Figure 10-6\. The GPU selection interface
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6。GPU选择界面
- en: Tip
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: These high-end machines are expensive to run, so make sure you stop your instance
    when you’re not actively using it for training. Otherwise, you’ll be paying for
    an idle machine.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些高端机器的运行成本很高，因此请确保在不使用训练时停止实例。否则，您将为一个空闲的机器付费。
- en: It makes life easier to have the GPU drivers installed automatically, so make
    sure you select that option, as demonstrated in [Figure 10-7](#gpu_driver).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动安装GPU驱动程序会让生活变得更轻松，因此请确保选择该选项，如[图10-7](#gpu_driver)所示。
- en: '![The GPU driver interface](Images/timl_1007.png)'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![GPU驱动程序界面](Images/timl_1007.png)'
- en: Figure 10-7\. The GPU driver interface
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7。GPU驱动程序界面
- en: Because you’ll be downloading a dataset to this machine, we recommend making
    the boot disk a bit larger than the default 100 GB; maybe as big as 500 GB, as
    shown in [Figure 10-8](#disk_interface).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为您将在这台机器上下载数据集，我们建议将引导磁盘大小调整为比默认的100GB大一些；也许大到500GB，如[图10-8](#disk_interface)所示。
- en: '![The boot disk size](Images/timl_1008.png)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![引导磁盘大小](Images/timl_1008.png)'
- en: Figure 10-8\. Increasing the boot disk size
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8。增加引导磁盘大小
- en: When you’ve set all those options, at the bottom of the page, click the CREATE
    button, which should return you to the “Notebook instances” screen. There should
    be a new instance in the list with the name you gave to your machine. There will
    be spinners next to it for a few minutes while the instance is being set up. When
    that’s complete click the OPEN JUPYTERLAB link, as depicted in [Figure 10-9](#instances_screen).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您设置好所有这些选项后，在页面底部点击CREATE按钮，这将返回到“笔记本实例”屏幕。列表中应该有一个新的实例，名称与您给机器的名称相同。在实例设置完成时，列表旁边会有旋转器几分钟。设置完成后，点击打开JUPYTERLAB链接，如[图10-9](#instances_screen)所示。
- en: '![The instances screen](Images/timl_1009.png)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![实例屏幕](Images/timl_1009.png)'
- en: Figure 10-9\. The instances screen
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-9。实例屏幕
- en: In the screen that opens, choose to create a Python 3 notebook (see [Figure 10-10](#notebook_selection)).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开的屏幕中，选择创建一个Python 3笔记本（参见[图10-10](#notebook_selection)）。
- en: '![The notebook selection screen](Images/timl_1010.png)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![笔记本选择屏幕](Images/timl_1010.png)'
- en: Figure 10-10\. The notebook selection screen
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-10。笔记本选择屏幕
- en: This gives you a Jupyter notebook connected to your instance. If you’re not
    familiar with Jupyter, it gives you a nice web interface to a Python interpreter
    running on a machine, and stores the commands and results in a notebook you can
    share. To start using it, in the panel on the right, type `**print("Hello World!")**`
    and then press Shift+Return. You should see “Hello World!"” printed just below,
    as shown in [Figure 10-11](#hello_world). If so, you’ve successfully set up your
    machine instance. We use this notebook as the place in which we enter commands
    for the rest of this tutorial.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这为您提供了一个连接到实例的Jupyter笔记本。如果您不熟悉Jupyter，它为您提供了一个漂亮的Web界面，用于运行在计算机上的Python解释器，并将命令和结果存储在一个可以共享的笔记本中。要开始使用它，在右侧面板中键入`**print("Hello
    World!")**`，然后按Shift+Return。您应该会看到“Hello World！”打印在下方，如[图10-11](#hello_world)所示。如果是这样，您已成功设置了机器实例。我们将使用这个笔记本作为本教程其余部分输入命令的地方。
- en: '![The hello world example](Images/timl_1011.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![hello world示例](Images/timl_1011.png)'
- en: Figure 10-11\. The “hello world” example
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-11。"hello world"示例
- en: Many of the commands that follow assume that you’re running from a Jupyter notebook,
    so they begin with a `!`, which indicates they should be run as shell commands
    rather than Python statements. If you’re running directly from a terminal (for
    example, after opening a Secure Shell connection to commmunicate with an instance)
    you can remove the initial `!`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的许多命令假定您是从Jupyter笔记本中运行的，因此它们以`!`开头，表示它们应该作为shell命令而不是Python语句运行。如果您直接从终端运行（例如，在打开安全外壳连接后与实例通信），可以删除初始的`!`。
- en: Training Framework Choice
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练框架选择
- en: Keras is the recommended interface for building models in TensorFlow, but when
    the person detection model was being created it didn’t yet support all the features
    we needed. For that reason, we show you how to train a model using *tf.slim*,
    an older interface. It is still widely used but deprecated, so future versions
    of TensorFlow might not support this approach. We hope to publish Keras instructions
    online in the future; check [tinymlbook.com/persondetector](https://oreil.ly/sxP6q)
    for updates.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是在TensorFlow中构建模型的推荐接口，但在创建人员检测模型时，它还不支持我们需要的所有功能。因此，我们向您展示如何使用*tf.slim*训练模型，这是一个较旧的接口。它仍然被广泛使用，但已被弃用，因此未来版本的TensorFlow可能不支持这种方法。我们希望将来在线发布Keras说明；请查看[tinymlbook.com/persondetector](https://oreil.ly/sxP6q)获取更新。
- en: 'The model definitions for Slim are part of the [TensorFlow models repository](https://oreil.ly/iamdB),
    so to get started, you’ll need to download it from GitHub:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Slim的模型定义是[TensorFlow模型存储库](https://oreil.ly/iamdB)的一部分，因此要开始，您需要从GitHub下载它：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following guide assumes that you’ve done this from your home directory,
    so the model repository code is at *~/models*, and that all commands are run from
    the home directory unless otherwise noted. You can place the repository somewhere
    else, but you’ll need to update all references to it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指南假定您已经从您的主目录中完成了这些操作，因此模型存储库代码位于*~/models*，并且除非另有说明，否则所有命令都是从主目录运行的。您可以将存储库放在其他位置，但您需要更新所有对它的引用。
- en: 'To use Slim, you need to make sure that Python can find its modules and install
    one dependency. Here’s how to do this in an iPython notebook:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Slim，您需要确保Python可以找到其模块并安装一个依赖项。以下是如何在iPython笔记本中执行此操作：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Updating `PYTHONPATH` through an `EXPORT` statement like this works only for
    the current Jupyter session, so if you’re using bash directly you should add it
    to a persistent startup script, running something like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过像这样的`EXPORT`语句更新`PYTHONPATH`仅适用于当前的Jupyter会话，因此如果您直接使用bash，您应该将其添加到持久性启动脚本中，运行类似于这样的内容：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you see import errors running the Slim scripts, make sure the `PYTHONPATH`
    is set up correctly and that `contextlib2` has been installed. You can find more
    general information on *tf.slim* in the [repository’s README](https://oreil.ly/azuvk).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行Slim脚本时看到导入错误，请确保`PYTHONPATH`已正确设置并且已安装`contextlib2`。您可以在[存储库的README](https://oreil.ly/azuvk)中找到有关*tf.slim*的更多一般信息。
- en: Building the Dataset
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据集
- en: To train our person detection model, we need a large collection of images that
    are labeled depending on whether they have people in them. The ImageNet 1,000-class
    dataset that’s widely used for training image classifiers doesn’t include labels
    for people, but luckily the [COCO dataset](http://cocodataset.org/#home) does.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的人员检测模型，我们需要一个大量的图像集，这些图像根据它们是否有人员进行了标记。用于训练图像分类器的ImageNet 1,000类数据集不包括人员的标签，但幸运的是[COCO数据集](http://cocodataset.org/#home)包括这些标签。
- en: 'The dataset is designed to be used for training models for localization, so
    the images aren’t labeled with the “person,” “not person” categories for which
    we want to train. Instead, each image comes with a list of bounding boxes for
    all of the objects it contains. “Person” is one of these object categories, so
    to get to the classification labels we want, we need to look for images with bounding
    boxes for people. To make sure that they aren’t too tiny to be recognizable we
    also need to exclude very small bounding boxes. Slim contains a convenient script
    to both download the data and convert bounding boxes into labels:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集设计用于训练本地化模型，因此图像没有用“人”、“非人”类别标记，我们希望对其进行训练。相反，每个图像都附带一个包含其包含的所有对象的边界框列表。“人”是这些对象类别之一，因此为了获得我们想要的分类标签，我们需要寻找带有人边界框的图像。为了确保它们不会太小而无法识别，我们还需要排除非常小的边界框。Slim包含一个方便的脚本，可以同时下载数据并将边界框转换为标签：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a large download, about 40 GB, so it will take a while and you’ll need
    to make sure you have at least 100 GB free on your drive to allow space for unpacking
    and further processing. Don’t be surprised if the process takes around 20 minutes
    to complete. When it’s done, you’ll have a set of TFRecords in *data/visualwakewords*
    holding the labeled image information. This dataset was created by Aakanksha Chowdhery
    and is known as the [Visual Wake Words dataset](https://oreil.ly/EC6nd). It’s
    designed to be useful for benchmarking and testing embedded computer vision because
    it represents a very common task that we need to accomplish with tight resource
    constraints. We’re hoping to see it drive even better models for this and similar
    tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大型下载，大约40 GB，所以需要一段时间，您需要确保您的驱动器上至少有100 GB的空闲空间以供解压和进一步处理。如果整个过程需要大约20分钟才能完成，请不要感到惊讶。完成后，您将在*data/visualwakewords*中拥有一组TFRecords，其中包含带有标记的图像信息。此数据集由Aakanksha
    Chowdhery创建，被称为[Visual Wake Words数据集](https://oreil.ly/EC6nd)。它旨在用于基准测试和测试嵌入式计算机视觉，因为它代表了我们需要在严格的资源约束下完成的非常常见的任务。我们希望看到它推动更好的模型用于这个和类似的任务。
- en: Training the Model
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'One of the nice things about using *tf.slim* to handle the training is that
    the parameters we commonly need to modify are available as command-line arguments,
    so we can just call the standard *train_image_classifier.py* script to train our
    model. You can use this command to build the model we use in the example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*tf.slim*处理训练的好处之一是我们通常需要修改的参数可用作命令行参数，因此我们只需调用标准的*train_image_classifier.py*脚本来训练我们的模型。您可以使用此命令构建我们在示例中使用的模型：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It will take a couple of days on a single-GPU V100 instance to complete all
    one million steps, but you should be able to get a fairly accurate model after
    a few hours if you want to experiment early. Following are some additional considerations:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在单GPU V100实例上完成所有一百万步骤需要几天的时间，但如果您想要尽早进行实验，几个小时后您应该能够获得一个相当准确的模型。以下是一些额外的考虑事项：
- en: The checkpoints and summaries will be saved in the folder given in the `--train_dir`
    argument. This is where you’ll need to look for the results.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点和摘要将保存在`--train_dir`参数中给定的文件夹中。这是您需要查看结果的地方。
- en: The `--dataset_dir` parameter should match the one where you saved the TFRecords
    from the Visual Wake Words build script.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--dataset_dir`参数应该与您从Visual Wake Words构建脚本中保存TFRecords的目录匹配。'
- en: The architecture we use is defined by the `--model_name` argument. The `mobilenet_v1`
    prefix instructs the script to use the first version of MobileNet. We did experiment
    with later versions, but these used more RAM for their intermediate activation
    buffers, so for now we’re sticking with the original. The `025` is the depth multiplier
    to use, which mostly affects the number of weight parameters; this low setting
    ensures the model fits within 250 KB of flash memory.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用的架构由`--model_name`参数定义。`mobilenet_v1`前缀指示脚本使用MobileNet的第一个版本。我们尝试过后续版本，但这些版本对于其中间激活缓冲区使用了更多的RAM，因此目前我们仍然坚持使用原始版本。`025`是要使用的深度乘数，这主要影响权重参数的数量；这个低设置确保模型适合在250
    KB的闪存内。
- en: '`--preprocessing_name` controls how input images are modified before they’re
    fed into the model. The `mobilenet_v1` version shrinks the width and height of
    the images to the size given in `--train_image_size` (in our case 96 pixels because
    we want to reduce the compute requirements). It also scales the pixel values from
    integers in the range 0 to 255 to floating-point numbers in the range −1.0 to
    +1.0 floating-point numbers (though we’ll be quantizing those after pass:[training).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--preprocessing_name` 控制输入图像在被馈送到模型之前如何修改。`mobilenet_v1` 版本将图像的宽度和高度缩小到 `--train_image_size`
    中给定的大小（在我们的例子中为 96 像素，因为我们想要减少计算需求）。它还将像素值从 0 到 255 的整数缩放为范围为 -1.0 到 +1.0 的浮点数（尽管我们将在训练后对其进行量化）。'
- en: The [HM01B0 camera](https://oreil.ly/RGciN) we’re using on the SparkFun Edge
    board is monochrome, so to get the best results, we need to train our model on
    black-and-white images. We pass in the `--use_grayscale` flag to enable that preprocessing.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在 SparkFun Edge 板上使用的 [HM01B0 相机](https://oreil.ly/RGciN) 是单色的，因此为了获得最佳结果，我们需要对黑白图像进行模型训练。我们传入
    `--use_grayscale` 标志以启用该预处理。
- en: The `--learning_rate`, `--label_smoothing`, `--learning_rate_decay_factor`,
    `--num_epochs_per_decay`, `--moving_average_decay`, and `--batch_size` parameters
    all control how weights are updated during the the training process. Training
    deep networks is still a bit of a dark art, so these exact values we found through
    experimentation for this particular model. You can try tweaking them to speed
    up training or gain a small boost in accuracy, but we can’t give much guidance
    for how to make those changes, and it’s easy to get combinations where the training
    accuracy never converges.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--learning_rate`、`--label_smoothing`、`--learning_rate_decay_factor`、`--num_epochs_per_decay`、`--moving_average_decay`
    和 `--batch_size` 参数都控制训练过程中权重如何更新。训练深度网络仍然是一种黑暗的艺术，所以这些确切的值是通过对这个特定模型进行实验找到的。您可以尝试调整它们以加快训练速度或在准确性上获得一点提升，但我们无法为如何进行这些更改提供太多指导，而且很容易出现训练准确性永远不收敛的组合。'
- en: '`--max_number_of_steps` defines how long the training should continue. There’s
    no good way to establish this threshold in advance; you need to experiment to
    determine when the accuracy of the model is no longer improving to know when to
    cut it off. In our case, we default to a million steps because with this particular
    model we know that’s a good point to stop.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--max_number_of_steps` 定义了训练应该持续多久。没有好的方法来提前确定这个阈值；您需要进行实验来确定模型的准确性何时不再提高，以知道何时停止。在我们的情况下，我们默认为一百万步，因为对于这个特定模型，我们知道这是一个停止的好时机。'
- en: 'After you start the script, you should see output that looks something like
    this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 启动脚本后，您应该看到类似以下的输出：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Don’t worry about the line duplication: this is just a side effect of the way
    TensorFlow log printing interacts with Python. Each line has two key bits of information
    about the training process. The global step is a count of how far through the
    training we are. Because we’ve set the limit as a million steps, in this case
    we’re nearly 5% complete. Together with the steps-per-second estimate, this is
    useful because you can use it to estimate a rough duration for the entire training
    process. In this case, we’re completing about 4 steps per second, so a million
    steps will take about 70 hours, or 3 days. The other crucial piece of information
    is the loss. This is a measure of how close the partially trained model’s predictions
    are to the correct values, and lower values are better. This will show a lot of
    variation but should on average decrease during training if the model is learning.
    Because it’s so noisy the amounts will bounce around a lot over short time periods,
    but if things are working well you should see a noticeable drop if you wait an
    hour or so and check back. This kind of variation is a lot easier to see in a
    graph, which is one of the main reasons to try TensorBoard.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心行重复：这只是 TensorFlow 日志打印与 Python 交互的副作用。每行都包含关于训练过程的两个关键信息。全局步骤是我们进行训练的进度计数。因为我们将限制设置为一百万步，所以在这种情况下我们已经完成了近
    5%。连同每秒步数的估计，这是有用的，因为您可以用它来估计整个训练过程的大致持续时间。在这种情况下，我们每秒完成大约 4 步，所以一百万步将需要大约 70
    小时，或者 3 天。另一个关键信息是损失。这是部分训练模型的预测与正确值之间的接近程度的度量，较低的值更好。这将显示很多变化，但如果模型正在学习，平均情况下应该在训练过程中减少。因为它非常嘈杂，这些数量会在短时间内反弹很多次，但如果事情进展顺利，您应该在等待一个小时左右并返回检查时看到明显的下降。这种变化在图表中更容易看到，这也是尝试
    TensorBoard 的主要原因之一。
- en: TensorBoard
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard
- en: TensorBoard is a web application that lets you view data visualizations from
    TensorFlow training sessions, and it’s included by default in most cloud instances.
    If you’re using Google Cloud AI Platform, you can start up a new TensorBoard session
    by opening the command palette from the left tabs in the notebook interface and
    then scrolling down to select “Create a new tensorboard.” You’re then prompted
    for the location of the summary logs. Enter the path you used for `--train_dir`
    in the training script—in the previous example, the folder name is *vww_96_grayscale*.
    One common error to watch out for is adding a slash to the end of the path, which
    will cause TensorBoard to fail to find the directory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一个 Web 应用程序，允许您查看来自 TensorFlow 训练会话的数据可视化，它默认包含在大多数云实例中。如果您正在使用
    Google Cloud AI Platform，您可以通过在笔记本界面的左侧选项卡中打开命令面板，然后向下滚动选择“创建一个新的 TensorBoard”来启动一个新的
    TensorBoard 会话。然后会提示您输入摘要日志的位置。输入您在训练脚本中用于 `--train_dir` 的路径——在上一个示例中，文件夹名称是 *vww_96_grayscale*。要注意的一个常见错误是在路径末尾添加斜杠，这将导致
    TensorBoard 无法找到目录。
- en: If you’re starting TensorBoard from the command line in a different environment
    you’ll need to pass in this path as the `--logdir` argument to the TensorBoard
    command-line tool, and point your browser to [*http://localhost:6006*](http://localhost:6006)
    (or the address of the machine you’re running it on).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在不同环境中从命令行启动 TensorBoard，您需要将此路径作为 `--logdir` 参数传递给 TensorBoard 命令行工具，并将浏览器指向
    [*http://localhost:6006*](http://localhost:6006)（或者您正在运行它的机器的地址）。
- en: After navigating to the TensorBoard address or opening the session through Google
    Cloud, you should see a page that looks something like [Figure 10-12](#tensorboard_graphs_10).
    It might take a little while for the graphs to have anything useful in them given
    that the script only saves summaries every five minutes. [Figure 10-12](#tensorboard_graphs_10)
    shows the results after training for more than a day. The most important graph
    is called “clone_loss”; it shows the progression of the same loss value that’s
    displayed in the logging output. As you can see in this example it fluctuates
    a lot, but the overall trend is downward over time. If you don’t see this sort
    of progression after a few hours of training, it’s a good sign that your model
    isn’t converging to a good solution, and you might need to debug what’s going
    wrong either with your dataset or the training parameters.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在导航到TensorBoard地址或通过Google Cloud打开会话后，您应该看到一个类似于[图10-12](#tensorboard_graphs_10)的页面。鉴于脚本仅每五分钟保存一次摘要，可能需要一段时间才能在图表中找到有用的内容。[图10-12](#tensorboard_graphs_10)显示了训练一天以上后的结果。最重要的图表称为“clone_loss”；它显示了与日志输出中显示的相同损失值的进展。正如您在此示例中所看到的，它波动很大，但随着时间的推移总体趋势是向下的。如果在训练几个小时后没有看到这种进展，这表明您的模型没有收敛到一个好的解决方案，您可能需要调试数据集或训练参数出现的问题。
- en: TensorBoard defaults to the SCALARS tab when it opens, but the other section
    that can be useful during training is IMAGES ([Figure 10-13](Images/#tensorboard_images)).
    This shows a random selection of the pictures the model is currently being trained
    on, including any distortions and other preprocessing. In the figure, you can
    see that the image has been flipped and that it’s been converted to grayscale
    before being fed to the model. This information isn’t as essential as the loss
    graphs, but it can be useful to ensure that the dataset is what you expect, and
    it is interesting to see the examples updating as training progresses.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard在打开时默认显示SCALARS选项卡，但在训练期间可能有用的另一个部分是IMAGES([图10-13](Images/#tensorboard_images))。这显示了模型当前正在训练的图片的随机选择，包括任何扭曲和其他预处理。在图中，您可以看到图像已被翻转，并且在馈送到模型之前已被转换为灰度。这些信息并不像损失图表那样重要，但可以用来确保数据集符合您的期望，并且在训练进行时看到示例更新是有趣的。
- en: '![Training graphs in Tensorboard](Images/timl_1012.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Tensorboard中的训练图表](Images/timl_1012.png)'
- en: Figure 10-12\. Graphs in TensorBoard
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12. TensorBoard中的图表
- en: '![Training images in Tensorboard](Images/timl_1013.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![Tensorboard中的训练图片](Images/timl_1013.png)'
- en: Figure 10-13\. Images in TensorBoard
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13. TensorBoard中的图片
- en: Evaluating the Model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'The loss function correlates with how well your model is training, but it isn’t
    a direct, understandable metric. What we really care about is how many people
    our model detects correctly, but to get it to calculate this we need to run a
    separate script. You don’t need to wait until the model is fully trained, you
    can check the accuracy of any checkpoints in the `--train_dir` folder. To do this,
    run the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与模型训练的好坏相关，但它不是一个直接可理解的度量标准。我们真正关心的是模型正确检测到多少人，但要让它计算这一点，我们需要运行一个单独的脚本。您不需要等到模型完全训练，可以检查`--train_dir`文件夹中任何检查点的准确性。要执行此操作，请运行以下命令：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You’ll need to make sure that `--checkpoint_path` is pointing to a valid set
    of checkpoint data. Checkpoints are stored in three separate files, so the value
    should be their common prefix. For example, if you have a checkpoint file called
    *model.ckpt-5179.data-00000-of-00001*, the prefix would be *model.ckpt-5179*.
    The script should produce output that looks something like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要确保`--checkpoint_path`指向有效的检查点数据集。检查点存储在三个单独的文件中，因此值应为它们的公共前缀。例如，如果您有一个名为*model.ckpt-5179.data-00000-of-00001*的检查点文件，则前缀将是*model.ckpt-5179*。脚本应该生成类似于以下内容的输出：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The important number here is the accuracy. It shows the proportion of the images
    that were classified correctly, which is 72% in this case, after converting to
    a percentage. If you follow the example script, you should expect a fully trained
    model to achieve an accuracy of around 84% after one million steps and show a
    loss of around 0.4.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要数字是准确率。它显示了被正确分类的图片的比例，在这种情况下是72%，转换为百分比后。如果按照示例脚本进行，您应该期望完全训练的模型在一百万步后达到约84%的准确率，并显示约0.4的损失。
- en: Exporting the Model to TensorFlow Lite
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型导出到TensorFlow Lite
- en: When the model has trained to an accuracy you’re happy with, you’ll need to
    convert the results from the TensorFlow training environment into a form you can
    run on an embedded device. As we’ve seen in previous chapters, this can be a complex
    process, and *tf.slim* adds a few of its own wrinkles, too.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型训练到您满意的准确度时，您需要将结果从TensorFlow训练环境转换为可以在嵌入式设备上运行的形式。正如我们在之前的章节中看到的，这可能是一个复杂的过程，而*tf.slim*也添加了一些自己的特点。
- en: Exporting to a GraphDef Protobuf File
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出到GraphDef Protobuf文件
- en: 'Slim generates the architecture from the `model_name` every time one of its
    scripts is run, so for a model to be used outside of Slim, it needs to be saved
    in a common format. We’re going to use the GraphDef protobuf serialization format
    because that’s understood by both Slim and the rest of TensorFlow:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Slim每次运行其脚本时都会从`model_name`生成架构，因此要在Slim之外使用模型，需要将其保存为通用格式。我们将使用GraphDef protobuf序列化格式，因为Slim和TensorFlow的其余部分都能理解它：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If this succeeds, you should have a new *vww_96_grayscale_graph.pb* file in
    your home directory. This contains the layout of the operations in the model,
    but it doesn’t yet have any of the weight data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，您应该在主目录中有一个新的*vww_96_grayscale_graph.pb*文件。这包含了模型中操作的布局，但尚未包含任何权重数据。
- en: Freezing the Weights
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冻结权重
- en: 'The process of storing the trained weights together with the operation graph
    is known as *freezing*. This converts all of the variables in the graph to constants,
    after loading their values from a checkpoint file. The command that follows uses
    a checkpoint from the millionth training step, but you can supply any valid checkpoint
    path. The graph-freezing script is stored in the main TensorFlow repository, so
    you’ll need to download this from GitHub before running this command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练好的权重与操作图一起存储的过程称为*冻结*。这将所有图中的变量转换为常量，加载它们的值后从检查点文件中。接下来的命令使用了百万次训练步骤的检查点，但您可以提供任何有效的检查点路径。图冻结脚本存储在主TensorFlow存储库中，因此在运行此命令之前，您需要从GitHub下载这个脚本：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After this, you should see a file called *vww_96_grayscale_frozen.pb*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您应该看到一个名为*vww_96_grayscale_frozen.pb*的文件。
- en: Quantizing and Converting to TensorFlow Lite
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化和转换为TensorFlow Lite
- en: 'Quantization is a tricky and involved process, and it’s still very much an
    active area of research, so taking the float graph that we’ve trained so far and
    converting it down to an 8-bit entity takes quite a bit of code. You can find
    more of an explanation of what quantization is and how it works in [Chapter 15](ch15.xhtml#optimizing_latency),
    but here we’ll show you how to use it with the model we’ve trained. The majority
    of the code is preparing example images to feed into the trained network so that
    the ranges of the activation layers in typical use can be measured. We rely on
    the `TFLiteConverter` class to handle the quantization and conversion into the
    TensorFlow Lite FlatBuffer file that we need for the inference engine:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一个棘手而复杂的过程，仍然是一个活跃的研究领域，因此将我们迄今为止训练的浮点图转换为8位实体需要相当多的代码。您可以在[第15章](ch15.xhtml#optimizing_latency)中找到更多关于量化是什么以及它是如何工作的解释，但在这里我们将向您展示如何在我们训练的模型中使用它。大部分代码是准备示例图像以馈送到训练网络中，以便测量典型使用中激活层的范围。我们依赖`TFLiteConverter`类来处理量化并将其转换为我们需要用于推理引擎的TensorFlow
    Lite FlatBuffer文件：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Converting to a C Source File
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换为C源文件
- en: 'The converter writes out a file, but most embedded devices don’t have a filesystem.
    To access the serialized data from our program, we must compile it into the executable
    and store it in flash. The easiest way to do that is to convert the file to a
    C data array, as we’ve done in previous chapters:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器会写出一个文件，但大多数嵌入式设备没有文件系统。为了从我们的程序中访问序列化数据，我们必须将其编译到可执行文件中并存储在闪存中。最简单的方法是将文件转换为C数据数组，就像我们在之前的章节中所做的那样：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can now replace the existing *person_detect_model_data.cc* file with the
    version you’ve trained and will be able to run your own model on embedded devices.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以用您训练过的版本替换现有的*person_detect_model_data.cc*文件，并能够在嵌入式设备上运行您自己的模型。
- en: Training for Other Categories
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为其他类别训练
- en: 'There are more than 60 different object types in the COCO dataset, so an easy
    way to customize your model would be to choose one of those instead of `person`
    when you build the training dataset. Here’s an example that looks for cars:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: COCO数据集中有60多种不同的对象类型，因此自定义模型的一种简单方法是在构建训练数据集时选择其中一种而不是`person`。以下是一个查找汽车的示例：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You should be able to follow the same steps as you did for the person detector,
    substituting in the new `coco/processed_cars` path wherever `data/visualwakewords`
    used to be.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够按照与人员检测器相同的步骤进行操作，只需在以前的`data/visualwakewords`路径处用新的`coco/processed_cars`路径替换。
- en: If the kind of object you’re interested in isn’t present in COCO, you might
    be able to use transfer learning to help you train on a custom dataset you’ve
    gathered, even if it’s much smaller. Although we don’t have an example of this
    to share yet, you can check [*tinymlbook.com*](http://tinymlbook.com) for updates
    on this approach.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣的对象类型在COCO中不存在，您可能可以使用迁移学习来帮助您在您收集的自定义数据集上进行训练，即使它很小。虽然我们还没有分享这方面的示例，但您可以查看[*tinymlbook.com*](http://tinymlbook.com)以获取有关这种方法的更新。
- en: Understanding the Architecture
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解架构
- en: '[MobileNets](https://oreil.ly/tK57G) are a family of architectures designed
    to provide good accuracy for as few weight parameters and arithmetic operations
    as possible. There are now multiple versions, but in our case we’re using the
    original v1 because it requires the smallest amount of RAM at runtime. The core
    concept behind the architecture is *depthwise separable convolution*. This is
    a variant of classic 2D convolutions that works in a much more efficient way,
    without sacrificing very much accuracy. Regular convolution calculates an output
    value based on applying a filter of a particular size across all channels of the
    input. This means that the number of calculations involved in each output is the
    width of the filter multiplied by the height, multiplied by the number of input
    channels. Depthwise convolution breaks this large calculation into separate parts.
    First, each input channel is filtered by one or more rectangular filters to produce
    intermediate values. These values are then combined using pointwise convolutions.
    This dramatically reduces the number of calculations needed, and in practice produces
    similar results to regular convolution.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNets](https://oreil.ly/tK57G)是一系列旨在提供尽可能少的权重参数和算术运算的良好准确性的架构。现在有多个版本，但在我们的情况下，我们使用原始的v1，因为它在运行时需要的RAM最少。该架构背后的核心概念是*深度可分离卷积*。这是经典2D卷积的一种变体，以更高效的方式工作，而几乎不损失准确性。常规卷积根据在输入的所有通道上应用特定大小的滤波器来计算输出值。这意味着每个输出中涉及的计算数量是滤波器的宽度乘以高度，再乘以输入通道的数量。深度卷积将这个大计算分解为不同的部分。首先，每个输入通道通过一个或多个矩形滤波器进行滤波，以产生中间值。然后使用逐点卷积来组合这些值。这显著减少了所需的计算量，并且在实践中产生了与常规卷积类似的结果。'
- en: MobileNet v1 is a stack of 14 of these depthwise separable convolution layers
    with an average pool and then a fully connected layer followed by a softmax at
    the end. We have specified a *width multiplier* of 0.25, which has the effect
    of reducing the number of computations down to around 60 million per inference,
    by shrinking the number of channels in each activation layer by 75% compared to
    the standard model. In essence it’s very similar to a normal convolutional neural
    network in operation, with each layer learning patterns in the input. Earlier
    layers act more like edge recognition filters, spotting low-level structure in
    the image, and later layers synthesize that information into more abstract patterns
    that help with the final object classification.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet v1是由14个这些深度可分离卷积层堆叠而成，其中包括一个平均池化层，然后是一个全连接层，最后是一个softmax层。我们指定了一个*宽度乘数*为0.25，这样可以将每次推断的计算量减少到约6000万次，通过将每个激活层中的通道数量缩减75%与标准模型相比。本质上，它在操作上与普通的卷积神经网络非常相似，每一层都在学习输入中的模式。较早的层更像是边缘识别滤波器，识别图像中的低级结构，而较后的层将这些信息合成为更抽象的模式，有助于最终的对象分类。
- en: Wrapping Up
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Image recognition using machine learning requires large amounts of data and
    a lot of processing power. In this chapter you learned how to train a model from
    scratch, given nothing but a dataset, and how to convert that model into a form
    that is optimized for embedded devices.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习进行图像识别需要大量的数据和大量的处理能力。在本章中，您学习了如何从头开始训练模型，只提供数据集，并将该模型转换为适用于嵌入式设备的形式。
- en: This experience should give you a good foundation for tackling the machine vision
    problems that you need to solve for your product. There’s still something a bit
    magical about computers being able to see and understand the world around them,
    so we can’t wait to see what you come up with!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种经验应该为您解决产品所需解决的机器视觉问题奠定了良好的基础。计算机能够看到并理解周围世界仍然有些神奇，所以我们迫不及待地想看看您会有什么创意！
