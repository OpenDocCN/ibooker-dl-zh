- en: Chapter 2\. Model Development on Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章\. Kubernetes 上的模型开发
- en: In this chapter, we will provide an overview of prevailing technologies and
    techniques for developing machine learning models using Kubernetes as a compute
    platform. While we will focus on specific techniques relevant to large language
    models (LLMs) and generative AI, many of the techniques we discuss will apply
    to traditional predictive models and other architectures as well.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概述使用 Kubernetes 作为计算平台开发机器学习模型的现有技术和方法。虽然我们将关注与大型语言模型（LLMs）和生成式人工智能相关的特定技术，但我们讨论的许多技术也适用于传统的预测模型和其他架构。
- en: Historically, models have required extensive data preparation to curate high-quality,
    labeled datasets that sufficiently capture the problem domain. Creating these
    datasets was very labor-intensive and expensive. More recently, advances in computational
    power, improved algorithms for distributing training across compute resources,
    and widespread open access to training data have all paved the way for extremely
    powerful general-purpose models to be built without heavy data curation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，模型需要大量数据准备来创建高质量、充分捕获问题域的标记数据集。创建这些数据集非常劳动密集且成本高昂。最近，计算能力的进步、改进的算法用于在计算资源之间分配训练，以及训练数据的广泛开放访问，所有这些都为构建无需大量数据整理的极其强大的通用模型铺平了道路。
- en: Generally, foundation LLMs are created via self-supervised learning, a type
    of unsupervised learning, on extremely large, unlabeled datasets. For LLMs, this
    results in a model that understands patterns in human language and can predict
    the most likely output that should follow a given input. These foundational models
    exhibit usefulness across a wide breadth of tasks, but practitioners often need
    to adapt these pretrained base models to some specific use case.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基础 LLMs 是通过在极其庞大、未标记的数据集上进行的自监督学习，一种无监督学习类型来创建的。对于 LLMs 来说，这导致了一个能够理解人类语言中的模式并预测给定输入后最可能输出的模型。这些基础模型在广泛的任务中表现出实用性，但从业者通常需要将这些预训练的基础模型适应到某些特定用例中。
- en: There are several prevailing techniques for adapting these foundational models,
    which differ from each other in their intended use cases, ease of implementation,
    and costs of implementation. Collectively, we will refer to these approaches as
    *model customization techniques*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种流行的技术用于适应这些基础模型，这些技术在预期的用例、实施难度和实施成本方面各不相同。总的来说，我们将把这些方法称为*模型定制技术*。
- en: Overview of LLM Customization Techniques
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 定制技术概述
- en: 'The LLM customization space, like much of generative AI, is evolving rapidly
    with new techniques being invented regularly. In general, customization is achieved
    through one or more of the following fundamental techniques:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成式人工智能的许多领域一样，LLM 定制空间正在迅速发展，新技术被定期发明。一般来说，定制是通过以下一个或多个基本技术之一实现的：
- en: Customizing an existing model’s output by leaving the model unchanged but carefully
    constructing the input to get a desired result. Examples of this include *prompt
    engineering* and *retrieval-augmented generation* (RAG).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不改变模型但仔细构建输入以获得期望的结果来定制现有模型的输出。这种方法包括*提示工程*和*检索增强生成*（RAG）。
- en: Combining individual models to achieve an output that is more desirable than
    that from a single model. One example of this is the [*mixture-of-agents* approach](https://oreil.ly/vw3kM).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单个模型组合起来以实现比单个模型更理想的结果。这种方法的例子之一是[*混合代理*方法](https://oreil.ly/vw3kM)。
- en: Retraining an existing model using curated data specific to a given task. This
    is *fine-tuning*.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用针对特定任务定制的精选数据重新训练现有模型。这被称为*微调*。
- en: Novel model customization techniques are likely to be achieved through new algorithms
    for implementing these fundamental techniques more efficiently or through creatively
    combining these techniques, as in the case of [*retrieval-augmented fine-tuning*
    (RAFT)](https://oreil.ly/feMXE).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖的模型定制技术可能通过更有效地实现这些基本技术的新算法或通过创造性地结合这些技术来实现，例如在[*检索增强微调*（RAFT）](https://oreil.ly/feMXE)的情况下。
- en: 'In the rest of this section, we will provide a primer on two of the most prominent
    approaches (as of this writing) to model customization: RAG and fine-tuning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将介绍两种最突出的模型定制方法（截至本文撰写时）：RAG 和微调。
- en: Retrieval-Augmented Generation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Retrieval-Augmented Generation
- en: A fundamental limitation of pretrained foundation models is that they possess
    “knowledge” only of the data that they were trained on. If you ask a model about
    a piece of data that it was not trained on, it will fail to give the desired answer.
    RAG is a technique that extends an existing model’s knowledge by passing relevant
    contextual data as input to the model at query time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练基础模型的一个基本限制是，它们只拥有它们训练过的数据的“知识”。如果你询问一个模型关于它没有训练过的数据，它将无法给出所需的答案。RAG是一种技术，通过在查询时将相关上下文数据作为输入传递给模型，从而扩展了现有模型的知识。
- en: So how does RAG work? Generally, when a user queries a model, a database (typically
    a [vector database](https://oreil.ly/2L8-k)) is queried for information relevant
    to the input query. The RAG system parses the results and uses an algorithm like
    [cosine similarity](https://oreil.ly/7uOIh) to choose the results most relevant
    to the query. Once those are chosen, they are added to the original query as contextual
    information and sent on to the model in a format along the lines of “using information
    found only in this input document, answer this question for me.” [Figure 2-1](#ch02_figure_1_1738498450534664)
    illustrates a hypothetical RAG system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，RAG是如何工作的呢？通常，当用户查询模型时，会查询数据库（通常是[向量数据库](https://oreil.ly/2L8-k)）以获取与输入查询相关的信息。RAG系统解析结果，并使用类似[余弦相似度](https://oreil.ly/7uOIh)的算法来选择与查询最相关的结果。一旦选择了这些结果，它们就被添加到原始查询中作为上下文信息，并以“仅使用在此输入文档中找到的信息，为我回答这个问题”的格式发送给模型。[图2-1](#ch02_figure_1_1738498450534664)展示了假设的RAG系统。
- en: '![](assets/skia_0201.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/skia_0201.png)'
- en: Figure 2-1\. An illustration of a generalized RAG system showing the interactions
    between the user, retrieval system, vector database, and model
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 一个通用RAG系统的示意图，展示了用户、检索系统、向量数据库和模型之间的交互
- en: The chosen retrieval and ranking algorithm is critically important to the performance
    of the RAG system. If no relevant contextual data is retrieved by the system,
    the model will lack the knowledge needed to give the desired answer to the user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所选的检索和排名算法对RAG系统的性能至关重要。如果系统没有检索到相关的上下文数据，模型将缺乏向用户提供所需答案所需的知识。
- en: Even though RAG requires a retrieval system and additional data storage between
    the user and the model, it has a number of benefits. Because RAG supplements the
    model’s knowledge at runtime, it requires less knowledge to be baked into the
    model and opens up the possibility of using a smaller model that is cheaper to
    serve to users while simultaneously allowing users to incorporate rapidly changing
    data like stock prices on the fly. RAG can also reduce the time to achieve value
    with an LLM, because it doesn’t require a lengthy retraining process to work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RAG需要在用户和模型之间使用检索系统和额外的数据存储，但它具有许多优点。因为RAG在运行时补充了模型的知识，所以它需要将更少的知识烘焙到模型中，并开启了使用更小、更便宜的服务于用户的模型的可能性，同时允许用户即时整合快速变化的数据，如股票价格。RAG还可以通过不需要长时间的重训练过程来减少使用LLM实现价值所需的时间。
- en: On the other hand, the knowledge given to a model via RAG is transient, and
    only exists for a single query. You also have to carefully craft the input prompt
    to get the kind of output you’re interested in. However, this sort of customization
    also has its limits. If you want to make knowledge changes persistent or fully
    customize the format of the model’s output, retraining the foundation model is
    required.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过RAG提供给模型的知识是瞬时的，并且仅存在于单个查询中。你还得仔细设计输入提示，以获得你感兴趣的那种输出。然而，这种定制也有其局限性。如果你想使知识变化持久或完全自定义模型输出的格式，就需要重新训练基础模型。
- en: Model Fine-Tuning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型微调
- en: Training a foundation model is notoriously expensive and time-consuming, which
    isn’t an option for even the largest enterprises. Instead, we can make use of
    a technique called fine-tuning. With fine-tuning, you create a high-quality, labeled
    dataset that is specific to your domain-specific task, knowledge, or desired output
    format. You can then use that dataset to adjust a pretrained model in a fraction
    of the time and with a fraction of the data that would be required for training
    from scratch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基础模型众所周知既昂贵又耗时，即使是最大的企业也无法承担。相反，我们可以利用一种称为微调的技术。通过微调，你可以创建一个针对特定领域任务、知识或所需输出格式的优质、标记的数据集。然后你可以使用该数据集在极短的时间内以及极少的训练数据下调整预训练模型。
- en: The fine-tuned model will then have your desired knowledge and behavior baked
    in, allowing your production architecture to avoid the complexities required by
    techniques like RAG. However, fine-tuning requires knowing how to train a model,
    the time to curate a training dataset large enough to influence a model, and the
    sometimes high compute cost to perform the training itself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的模型将包含你希望的知识和行为，从而使你的生产架构避免 RAG 等技术所需的复杂性。然而，微调需要知道如何训练模型，以及收集足够大的训练数据集以影响模型所需的时间，以及进行训练本身可能的高计算成本。
- en: A number of techniques exist to optimize the compute cost of fine-tuning, such
    as parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA). Both
    of these techniques work by training only a subset of the pretrained model’s weights
    and biases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多技术可以优化微调的计算成本，例如参数高效的微调（PEFT）和低秩适应（LoRA）。这两种技术都是通过仅训练预训练模型权重和偏差的子集来工作的。
- en: While this is complex, there are many tools and entire platforms available to
    help with training and fine-tuning models, such as [InstructLab](https://instructlab.ai)
    and Hugging Face’s [sft_trainer](https://oreil.ly/9f4mP), with many of them available
    within the open source Kubernetes ecosystem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这很复杂，但有许多工具和整个平台可以帮助训练和微调模型，例如 [InstructLab](https://instructlab.ai) 和 Hugging
    Face 的 [sft_trainer](https://oreil.ly/9f4mP)，其中许多都在开源 Kubernetes 生态系统中可用。
- en: Kubernetes-Native Model Training Tools
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原生 Kubernetes 模型训练工具
- en: 'While many training tools and platforms are available, at a fundamental level
    they all provide easy access to the compute power necessary to train and fine-tune
    models. When evaluating a training tool or platform, the following requirements
    should be considered:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多训练工具和平台可用，但从基本层面来看，它们都提供了训练和微调模型所需的计算能力。在评估训练工具或平台时，应考虑以下要求：
- en: Integration with the training framework(s) (distributed or otherwise) that data
    scientists or data science teams use and are comfortable with (e.g., PyTorch,
    TensorFlow, etc.).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据科学家或数据科学团队使用和熟悉的训练框架（分布式或其他）集成。
- en: Support for training/fine-tuning algorithms that your team wants to use.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持您团队希望使用的训练/微调算法。
- en: Access to hardware optimizers, such as accelerators (e.g., GPUs), specialized
    network devices, and specialized storage providers with multi-write-capable storage.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问硬件优化器，例如加速器（例如，GPU）、专用网络设备和具有多写能力的存储的专用存储提供商。
- en: Integrations with the development environments data scientists or data science
    teams are already using. A tool that effectively abstracts away Kubernetes so
    that the data scientist or team doesn’t need to manage it is ideal.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据科学家或数据科学团队已经使用的开发环境数据集成。一个能够有效抽象 Kubernetes 的工具是理想的，这样数据科学家或团队就不需要管理它。
- en: In the following subsections, we will explore open source tools that meet these
    requirements and have strong community adoption.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将探讨满足这些要求并且具有强大社区采用的开源工具。
- en: Ray
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray
- en: '[Ray](https://oreil.ly/eu-Ll) is a framework that enables users to scale their
    training and fine-tuning processes up from single machines to clusters of machines,
    and can run natively on Kubernetes via the [KubeRay](https://oreil.ly/H7Xgz) operator.
    It seamlessly integrates with PyTorch and other frameworks via [Ray Train](https://oreil.ly/dLTwc)
    and has extensive support for [accelerators](https://oreil.ly/TrRP-). It also
    comes with a dashboard that provides key monitoring information to end users.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray](https://oreil.ly/eu-Ll) 是一个框架，它使用户能够将他们的训练和微调过程从单机扩展到机器集群，并且可以通过 [KubeRay](https://oreil.ly/H7Xgz)
    操作员在 Kubernetes 上原生运行。它通过 [Ray Train](https://oreil.ly/dLTwc) 与 PyTorch 和其他框架无缝集成，并对
    [加速器](https://oreil.ly/TrRP-) 提供广泛的支持。它还附带一个仪表板，为最终用户提供关键监控信息。'
- en: Note
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An [operator](https://oreil.ly/-lPOg) is an extension to Kubernetes that helps
    to manage Kubernetes applications by using custom resources to automate the application’s
    lifecycle.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[操作员](https://oreil.ly/-lPOg) 是 Kubernetes 的扩展，它通过使用自定义资源来自动化应用程序的生命周期来帮助管理
    Kubernetes 应用程序。'
- en: Ray’s biggest strength is its ease of adoption by data scientists who don’t
    know Kubernetes well, but it comes with the downside of increased overhead through
    the management of Ray clusters when compared to options that have a more “raw”
    interface to Kubernetes. It also doesn’t always scale well to extremely large-scale
    training jobs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 的最大优势是数据科学家易于采用，即使他们对 Kubernetes 不太了解，但它与具有更“原始” Kubernetes 接口的选项相比，在管理
    Ray 集群时会产生更高的开销。它也不总是适合极端大规模的训练作业。
- en: Kubeflow Training Operator
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow 训练操作符
- en: '[Kubeflow](https://www.kubeflow.org) is a community-managed open source ecosystem
    of Kubernetes components that support the full AI lifecycle. A part of that ecosystem,
    the Kubeflow Training Operator (KFTO) is a Kubernetes-native operator that allows
    users to use Kubernetes for distributed training and fine-tuning of large models.
    Its software development kit (SDK) allows for easy integration into existing environments
    and code and has extensive support for common frameworks like PyTorch.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kubeflow](https://www.kubeflow.org) 是一个由社区管理的开源生态系统，包含支持完整 AI 生命周期的 Kubernetes
    组件。该生态系统的一部分，Kubeflow 训练操作符（KFTO）是一个 Kubernetes 原生操作符，允许用户使用 Kubernetes 进行大规模模型的分布式训练和微调。它的软件开发工具包（SDK）允许轻松集成到现有环境和代码中，并广泛支持
    PyTorch 等常见框架。'
- en: KFTO accelerator support is tied to the chosen training framework, so it supports
    anything that the training framework and Kubernetes support and can scale to any
    level that the framework and Kubernetes are capable of scaling to. Unlike Ray,
    KFTO is a thin layer on top of the underlying Kubernetes objects, which introduces
    very little compute overhead. The flipside to that, though, is that more of the
    Kubernetes details are exposed to the user, which may be confusing for data scientists
    and developers who do not need to know these details.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: KFTO 加速器支持与所选训练框架相关联，因此它支持训练框架和 Kubernetes 支持的任何内容，并且可以扩展到框架和 Kubernetes 能够扩展到的任何级别。与
    Ray 不同，KFTO 是在底层 Kubernetes 对象之上的一个薄层，这引入了非常小的计算开销。然而，这也意味着更多的 Kubernetes 细节暴露给用户，这可能会让不需要了解这些细节的数据科学家和开发者感到困惑。
- en: Native Training Framework Integration with Kubernetes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与 Kubernetes 的原生训练框架集成
- en: Most training frameworks have framework-specific tooling for integrating with
    Kubernetes to provide computational resources. PyTorch, for example, has a universal
    job launcher called TorchX that includes Kubernetes support via its scheduler.
    While this kind of solution is the most lightweight and is the easiest for data
    scientists to adopt, it is less declarative and thus doesn’t lend itself as well
    to administration by MLOps teams.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数训练框架都有针对 Kubernetes 的特定工具，用于提供计算资源。例如，PyTorch 有一个通用的作业启动器称为 TorchX，它通过其调度器提供
    Kubernetes 支持。虽然这种解决方案是最轻量级的，也是数据科学家最容易采用的，但它不太声明性，因此不太适合 MLOps 团队的管理。
- en: Another potential downside is that these tools are framework-specific, so usage
    won’t necessarily scale in large organizations with several data science teams
    using different frameworks. These native integrations are best suited for small
    teams of data scientists during experimentation phases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的缺点是，这些工具是框架特定的，因此在大组织中使用不同框架的多个数据科学团队中，使用可能不会按比例扩展。这些原生集成最适合在实验阶段的小型数据科学团队。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Typically, once a model is trained or fine-tuned, you will want to evaluate
    its performance. Many existing model evaluation tools that data scientists use
    outside of Kubernetes can also be used when Kubernetes is used as a training platform.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一旦模型经过训练或微调，你将想要评估其性能。当 Kubernetes 被用作训练平台时，数据科学家在 Kubernetes 外部使用的许多现有模型评估工具也可以使用。
- en: Managing Compute Resources for Training
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理训练的计算资源
- en: 'While the tools described in the previous section allow you to train and fine-tune
    across many computational resources, this often requires extensive and costly
    hardware resources. Enterprises must pay particular attention to managing the
    cost incurred during training or fine-tuning. A robust management system should
    be able to do the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上一节中描述的工具允许你在许多计算资源上进行训练和微调，但这通常需要大量的和昂贵的硬件资源。企业必须特别注意在训练或微调过程中产生的成本。一个强大的管理系统应该能够做到以下几点：
- en: Facilitate the creation of job queues so that requests for compute hardware
    get serviced as soon as the hardware becomes available.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进作业队列的创建，以便在硬件可用时立即处理计算硬件的请求。
- en: Assign resource quotas to groups of users in order to constrain how many resources
    a given group can consume.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将资源配额分配给用户组，以限制特定组可以消耗的资源数量。
- en: Share resource quotas between groups when individual groups need to burst and
    there are free resources.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当个别组需要爆发并存在空闲资源时，在组之间共享资源配额。
- en: Manage request priorities for resources and priority-based job preemption.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理资源请求的优先级和基于优先级的作业抢占。
- en: Provide auditability and reporting on resource management at the model, job,
    and team levels.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型、作业和团队级别上提供资源管理的可审计性和报告。
- en: Allow all of these functions to be centrally managed by IT teams while maintaining
    transparency for users.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许IT团队集中管理所有这些功能，同时保持对用户透明。
- en: 'There are currently two major open source projects in this space: [Kueue](https://oreil.ly/JZQ_0)
    and [Volcano](https://oreil.ly/cCQQJ). Both projects are Kubernetes native and
    have strong community adoption. They also have support for managing resources
    of various types, like Ray clusters, KFTO jobs, and PyTorch training jobs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在这个领域有两个主要的开源项目：[Kueue](https://oreil.ly/JZQ_0) 和 [Volcano](https://oreil.ly/cCQQJ)。这两个项目都是Kubernetes原生，并且拥有强大的社区支持。它们还支持管理各种类型的资源，如Ray集群、KFTO作业和PyTorch训练作业。
- en: While these projects offer similar functionality, they do have some key differences.
    Kueue is an official Kubernetes special interest group project and is thus “blessed”
    by the wider Kubernetes community. It is based on the design principle of delegating
    functionality to existing Kubernetes components when applicable, and because of
    this, Kueue is fairly lightweight.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些项目提供类似的功能，但它们确实有一些关键的区别。Kueue是官方Kubernetes特别兴趣小组项目，因此得到了更广泛的Kubernetes社区的认可。它基于将功能委托给现有Kubernetes组件的设计原则，因此Kueue相对较轻量。
- en: Volcano, on the other hand, replicates some existing Kubernetes functionality,
    giving it more overhead but allowing it to be a more holistic and better-integrated
    solution. It is also more mature than Kueue and as of this writing offers more
    capabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Volcano复制了一些现有的Kubernetes功能，这给它带来更多的开销，但允许它成为一个更全面、更好的集成解决方案。它也比Kueue更成熟，截至本文撰写时提供了更多的功能。
- en: Once a data science team has a model and training procedure it is ready to send
    to production, it will be necessary to periodically retrain the model while keeping
    track of the datasets that went into each new version of the model. In [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759),
    we will discuss why periodic retraining, model versioning, and dataset versioning
    are necessary along with tools to help with these production workflows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据科学团队有了模型和训练流程，准备将其发送到生产环境，就需要定期重新训练模型，同时跟踪进入每个新版本模型的数据集。在[第3章](ch03.html#ch03_making_training_repeatable_1738498450655759)中，我们将讨论定期重新训练、模型版本化和数据集版本化的必要性，以及帮助这些生产流程的工具。
