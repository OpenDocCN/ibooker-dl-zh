- en: '11 Scaling up: Best practices for production deployment'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 扩展：生产部署的最佳实践
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Challenges and deployment options to consider for an application ready for production
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为准备生产的应用程序考虑的挑战和部署选项
- en: Production best practices covering scalability, latency, caching, and managed
    identities
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖可扩展性、延迟、缓存和管理身份的生产最佳实践
- en: Observability of LLM applications, with some practical examples
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM应用的可观察性，以及一些实际示例
- en: LLMOps and how it compliments MLOps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMOps及其如何补充MLOps
- en: When organizations are ready to take their generative AI models from the realm
    of proof of concept (PoC) to the real world of production, they embark on a journey
    that requires careful consideration of key aspects. This chapter will discuss
    deployment and scaling options, sharing best practices for making generative AI
    solutions operational, reliable, performant, and secure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当组织准备将他们的生成式AI模型从概念验证（PoC）领域带入现实世界的生产时，他们开始了一段需要仔细考虑关键方面的旅程。本章将讨论部署和扩展选项，分享使生成式AI解决方案可操作、可靠、高效和安全的最佳实践。
- en: Deploying and scaling generative AI models in a production setting is a complex
    task that requires meticulous consideration of various factors. While building
    a PoC can be a thrilling way to test an idea’s feasibility, taking it to production
    introduces a whole new realm of operational, technical, and business considerations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中部署和扩展生成式AI模型是一项复杂的任务，需要仔细考虑各种因素。虽然构建原型可以是一种令人兴奋的测试想法可行性的方式，但将其推向生产则引入了一个全新的操作、技术和商业考虑的领域。
- en: This chapter will focus on the key aspects developers must consider when deploying
    and scaling generative AI models in a production environment. We will discuss
    the operational criteria critical to monitoring the systems’ health, deployment
    options, and best practices for ensuring reliability, performance, and security.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论开发者在生产环境中部署和扩展生成式AI模型时必须考虑的关键方面。我们将讨论对监控系统健康至关重要的操作标准、部署选项，以及确保可靠性、性能和安全的最佳实践。
- en: We will also delve into the concepts of large language model operations (LLMOps)
    and machine learning operations (MLOps), which are essential and empowering for
    managing the lifecycle of generative AI models in production. Additionally, the
    chapter will underscore the importance of cost management and budgeting for models
    deployed in production and provide some enlightening case studies of successful
    deployment and scaling of generative AI models in a production environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将深入探讨大型语言模型操作（LLMOps）和机器学习操作（MLOps）的概念，这对于管理生产中生成式AI模型的生命周期至关重要。此外，本章还将强调成本管理和预算对于在生产中部署的模型的重要性，并提供一些关于在生产环境中成功部署和扩展生成式AI模型的启发性案例研究。
- en: By the end of this chapter, you will experience a transformative journey of
    understanding the key considerations and best practices for deploying generative
    AI models to production. Let’s dive into this exciting world of knowledge by exploring
    some of the challenges most enterprises face when deploying a GenAI application
    to production.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将经历一次变革性的旅程，了解将生成式AI模型部署到生产的关键考虑因素和最佳实践。让我们通过探讨企业在将GenAI应用程序部署到生产时面临的一些挑战，来深入这个激动人心的知识世界。
- en: 11.1 Challenges for production deployments
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 生产部署的挑战
- en: Generative AI apps in an enterprise production environment face specific challenges
    that differ from those in conventional machine learning (ML). However, some of
    the challenges remain the same. For example, developers must deal with the complicated
    relationship of computational resource requirements, data quality standards, performance
    goals, the possibility of output variability, and the changing security situation
    around these powerful models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 企业生产环境中的生成式AI应用程序面临的具体挑战与传统的机器学习（ML）不同。然而，一些挑战仍然存在。例如，开发者必须处理计算资源需求、数据质量标准、性能目标、输出可变性的可能性以及这些强大模型周围不断变化的安全状况的复杂关系。
- en: One of the primary challenges in deploying generative AI models is their complexity.
    These models can be computationally intensive and require significant resources
    to train and deploy, even when factoring in today’s cloud-scale infrastructure
    and computing. Consequently, scaling the models to handle large volumes of requests
    or deploying them in resource-constrained environments can be difficult. Developers
    must carefully consider the hardware and software requirements of the models,
    as well as the infrastructure required to support them to ensure that they can
    be deployed and scaled effectively.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署生成式AI模型时，其主要挑战之一是它们的复杂性。即使考虑到今天的云规模基础设施和计算能力，这些模型在计算上可能非常密集，需要大量的资源来训练和部署。因此，将模型扩展以处理大量请求或在资源受限的环境中部署可能很困难。开发者必须仔细考虑模型的硬件和软件要求，以及支持它们的所需基础设施，以确保它们可以有效地部署和扩展。
- en: Another challenge in deploying generative AI models is ensuring the quality
    and availability of data. A key aspect of the quality of data is also knowing
    the source of the data and whether it is an authoritative or authentic source,
    which is important. These models rely heavily on data quality and availability,
    and any problems with the data can significantly affect the models’ performance
    and accuracy. Developers must implement robust data validation and quality control
    processes and monitor the data sources and pipelines used to train and deploy
    the models to ensure the data is accurate, relevant, and current. This can be
    done by measuring accuracy with predictive performance metrics, relevance through
    task-specific evaluations, and currency by tracking data freshness. Enterprises
    should implement robust monitoring systems and document data lineage to maintain
    high data integrity standards. Chapter 12 covers evaluations and benchmarks in
    more detail.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署生成式AI模型时，另一个挑战是确保数据的质量和可用性。数据质量的关键方面还包括了解数据的来源以及它是否是一个权威或真实的来源，这一点非常重要。这些模型高度依赖于数据质量和可用性，任何数据问题都可能显著影响模型的性能和准确性。开发者必须实施稳健的数据验证和质量控制流程，并监控用于训练和部署模型的来源和管道，以确保数据准确、相关且最新。这可以通过使用预测性能指标来衡量准确性，通过特定任务的评估来衡量相关性，以及通过跟踪数据新鲜度来衡量时效性。企业应实施稳健的监控系统并记录数据血缘，以维持高数据完整性标准。第12章更详细地介绍了评估和基准测试。
- en: Model performance and accuracy are also critical considerations when deploying
    generative AI models. Developers must carefully monitor the models’ performance
    and accuracy and implement regular testing and validation processes to ensure
    the models function as expected. In an ideal world, this requires a deep understanding
    of the models’ underlying algorithms and architectures and the ability to diagnose
    and resolve any problems that may arise. However, in a practical sense, most enterprises
    will have a cross-functional team of developers, data scientists, and business
    experts who will collectively help understand, guide, and model architecture and
    deployment considerations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署生成式AI模型时，模型性能和准确性也是关键考虑因素。开发者必须仔细监控模型的性能和准确性，并实施定期的测试和验证流程，以确保模型按预期运行。在理想的世界里，这需要深入了解模型的底层算法和架构，以及诊断和解决可能出现的任何问题的能力。然而，在现实世界中，大多数企业将有一个跨职能的开发者、数据科学家和业务专家团队，他们将共同帮助理解、指导和考虑模型架构和部署问题。
- en: Reliability and availability are also key considerations in deploying generative
    AI models. These models must be reliable and available to meet the business’ needs,
    which requires careful consideration of factors such as redundancy, failover,
    and disaster recovery. Developers must implement robust monitoring and maintenance
    processes to ensure that the models function as expected and be prepared to respond
    quickly to any problems. Of course, most enterprises rely on the hyper-scaler
    they are using to provide much of this service. These services’ underlying reliability
    and availability are closely linked to those providers. With small language models
    (SLMs) also in the mix and being used with large language models (LLMs), the reliability
    and scale considerations are different, especially when considering edge deployments
    for SLMs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署生成式AI模型时，可靠性和可用性也是关键考虑因素。这些模型必须可靠且可用以满足业务需求，这需要仔细考虑冗余、故障转移和灾难恢复等因素。开发者必须实施强大的监控和维护流程，以确保模型按预期运行，并准备好迅速应对任何问题。当然，大多数企业依赖他们使用的超大规模云服务提供商来提供大部分此类服务。这些服务的底层可靠性和可用性与这些提供商紧密相关。随着小型语言模型（SLMs）也加入其中并与大型语言模型（LLMs）一起使用，可靠性和规模考虑因素不同，尤其是在考虑SLMs的边缘部署时。
- en: Security and compliance are also critical considerations. These models can process
    sensitive data, which must be protected from unauthorized access, theft, or misuse.
    Enterprises must ensure that the models comply with relevant regulations and standards,
    such as GDPR, HIPAA, or PCI-DSS, and implement robust security controls to protect
    the data and the models themselves.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和合规性也是关键考虑因素。这些模型可以处理敏感数据，这些数据必须得到保护，防止未经授权的访问、盗窃或滥用。企业必须确保模型符合相关法规和标准，如GDPR、HIPAA或PCI-DSS，并实施强大的安全控制措施来保护数据和模型本身。
- en: Companies must first know each regulation’s requirements to comply with these
    data protection regulations. This involves managing consent, securing sensitive
    information, and handling data breaches. They should track and control personal
    data used by LLMs, apply strong security measures, and include privacy in the
    system design from the beginning. Frequent compliance audits, employee training,
    and vendor management are important for maintaining standards. A good incident
    response plan for data breaches and careful record-keeping will help with compliance.
    Furthermore, using built-in compliance features of cloud services can assist in
    meeting these requirements. By keeping up with compliance standards and taking
    these steps, enterprises can use LLMs to match legal and regulatory obligations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 公司必须首先了解每项法规的要求，以便遵守这些数据保护法规。这包括管理同意、保护敏感信息和处理数据泄露。他们应跟踪和控制LLM使用的个人数据，采取强有力的安全措施，并在系统设计之初就考虑隐私问题。定期的合规审计、员工培训和供应商管理对于维持标准至关重要。良好的数据泄露事件响应计划和细致的记录保存将有助于合规。此外，利用云服务的内置合规功能可以帮助满足这些要求。通过跟上合规标准并采取这些步骤，企业可以使用LLM来匹配法律和监管义务。
- en: Cost management is another important consideration. Models can be expensive
    to deploy and maintain, particularly when it comes to computing, storage, and
    networking resources. Developers must carefully manage the costs associated with
    deploying and scaling the models and be prepared to make tradeoffs between cost
    and performance as needed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 成本管理也是另一个重要考虑因素。模型的部署和维护可能很昂贵，尤其是在计算、存储和网络资源方面。开发者必须仔细管理与部署和扩展模型相关的成本，并准备好在必要时在成本和性能之间做出权衡。
- en: Integrating existing systems and workflows is also critical in deploying generative
    AI models. These models often must be integrated with existing systems and workflows,
    which can be complex and time-consuming. Developers must ensure that the models
    are compatible with existing systems and can be easily integrated into existing
    workflows. They must also be prepared to work closely with other teams and stakeholders
    to ensure a smooth deployment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署生成式AI模型时，集成现有系统和流程也是关键。这些模型通常必须与现有系统和工作流程集成，这可能很复杂且耗时。开发者必须确保模型与现有系统兼容，并且可以轻松集成到现有工作流程中。他们还必须准备好与其他团队和利益相关者紧密合作，以确保平稳部署。
- en: Human-in-the-loop considerations are another important factor. These models
    often require human intervention or oversight, particularly when they are used
    to make critical decisions or generate content that requires human review. Developers
    must ensure the models are designed with human-in-the-loop considerations and
    implement robust processes for managing and monitoring human intervention.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在回路考量是另一个重要因素。这些模型通常需要人类干预或监督，尤其是在它们被用于做出关键决策或生成需要人类审查的内容时。开发者必须确保模型的设计考虑了人类在回路，并实施稳健的过程来管理和监控人类干预。
- en: Ethical considerations are the final important factor in deploying generative
    AI models. These models can have significant ethical implications, particularly
    regarding bias, fairness, and transparency. Thus, developers must ensure that
    the models are designed and deployed ethically and must be prepared to address
    ethical concerns. Chapter 13 covers this topic in depth.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 道德考量是部署生成式AI模型的最终重要因素。这些模型可能具有重大的道德影响，尤其是关于偏见、公平性和透明度。因此，开发者必须确保模型的设计和部署符合道德标准，并准备好解决道德问题。第13章深入探讨了这一主题。
- en: 'By understanding these challenges and considerations, developers can design
    and deploy generative AI models that are scalable, reliable, and secure and meet
    the business’ needs in a production environment. Several challenges and considerations
    must be addressed when deploying generative AI models in a production environment
    to ensure successful implementation. The following key points highlight these
    critical aspects:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些挑战和考量，开发者可以设计和部署可扩展、可靠且安全的生成式AI模型，以满足生产环境中的业务需求。在部署生成式AI模型到生产环境中时，必须解决几个挑战和考量以确保成功实施。以下关键点突出了这些关键方面：
- en: '*Complexity of generative AI models*—High computational requirements and significant
    resources are required for training and deployment. Consider hardware, software,
    and infrastructure for effective scaling.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成式AI模型的复杂性*—训练和部署需要高计算需求和大量资源。考虑硬件、软件和基础设施以实现有效扩展。'
- en: '*Data quality and availability*—These are essential for model performance and
    accuracy. Implement robust data validation and quality control processes, and
    monitor data sources.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据质量和可用性*—这对于模型性能和准确性至关重要。实施稳健的数据验证和质量控制流程，并监控数据源。'
- en: '*Model performance and accuracy*—Regular testing and validation are required.
    Cross-functional teams can aid in understanding and resolving problems.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型性能和准确性*—需要定期测试和验证。跨职能团队能够帮助理解和解决这些问题。'
- en: '*Reliability and availability*—Implement redundancy, failover, and disaster
    recovery. Use robust monitoring and maintenance processes. There is dependence
    on hyper-scalers for service reliability.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可靠性及可用性*—实施冗余、故障转移和灾难恢复。使用稳健的监控和维护流程。对超规模扩展器的服务可靠性有依赖。'
- en: '*Security and compliance*—Protect sensitive data from unauthorized access.
    Ensure compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Implement
    security controls, and manage data protection effectively.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全和合规性*—保护敏感数据免受未经授权的访问。确保符合GDPR、HIPAA和PCI-DSS等法规。实施安全控制，并有效管理数据保护。'
- en: '*Cost management*—This involves careful management of computing, storage, and
    networking costs, balancing cost and performance.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本管理*—这涉及对计算、存储和网络成本的谨慎管理，平衡成本和性能。'
- en: '*Integration with existing systems*—Ensure compatibility and smooth integration
    with current systems and workflows. Collaborate with other teams and stakeholders.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与现有系统集成*—确保与当前系统和工作流程的兼容性和平滑集成。与其他团队和利益相关者合作。'
- en: '*Human-in-the-loop considerations*—Design models with human oversight for critical
    decisions. Implement processes for managing human intervention.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人类在回路考量*—为关键决策设计具有人类监督的模型。实施管理人类干预的流程。'
- en: '*Ethical considerations*—Address bias, fairness, and transparency problems.
    Ensure ethical design and deployment of models.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*道德考量*—解决偏见、公平性和透明度问题。确保模型的设计和部署符合道德标准。'
- en: 11.2 Deployment options
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 部署选项
- en: Several options are available when deploying generative AI apps, with the best
    choice depending on factors such as model size and complexity, desired scalability
    and availability, and available infrastructure and resources.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署生成式AI应用时，有多种选择，最佳选择取决于模型大小和复杂性、所需的可扩展性和可用性、以及可用的基础设施和资源等因素。
- en: Cloud deployment offers advantages such as scalability, diverse compute options,
    and managed services for easier deployment. However, consider potential ongoing
    costs, vendor lock-in, and data privacy concerns. On-premise deployment provides
    greater control, performance optimization, and data security, but it requires
    significant upfront investment and in-house expertise and may involve slower scaling.
    A hybrid approach combines both strengths, allowing sensitive data to remain on-premise,
    while using cloud scalability and introducing management complexity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 云部署提供了诸如可扩展性、多样化的计算选项和便于部署的托管服务等优势。然而，请考虑潜在的持续成本、供应商锁定和数据隐私问题。本地部署提供更大的控制权、性能优化和数据安全性，但需要大量的前期投资和内部专业知识，可能涉及较慢的扩展。混合方法结合了两种优势，允许敏感数据保留在本地，同时使用云的可扩展性和引入管理复杂性。
- en: Regardless of the chosen deployment path, several core technologies facilitate
    the process. Containerization ensures consistent model execution across environments,
    while serverless functions are ideal for dynamic workloads. API gateways provide
    structured access for other applications to utilize models, and specialized GenAI
    platforms can streamline the deployment and management of LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种部署路径，一些核心技术都促进了这个过程。容器化确保模型在不同环境中的执行一致性，而无服务器函数适用于动态工作负载。API网关为其他应用程序提供结构化访问以利用模型，而专门的GenAI平台可以简化LLM的部署和管理。
- en: Cloud deployment is popular due to its scalability and flexibility, particularly
    with providers such as Microsoft Azure, Amazon Web Services (AWS), and Google
    Cloud Platform (GCP). Depending on their needs, developers can choose from virtual
    machines, containers, or serverless functions. However, it’s crucial to carefully
    assess the required infrastructure and resources, including GPUs, memory, storage,
    and network bandwidth. Implementing load balancing and redundancy strategies ensures
    scalability and availability, while robust monitoring and automated testing are
    essential for maintaining performance and health.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其可扩展性和灵活性，云部署很受欢迎，尤其是在微软Azure、亚马逊网络服务（AWS）和谷歌云平台（GCP）等提供商中。根据他们的需求，开发者可以选择虚拟机、容器或无服务器函数。然而，仔细评估所需的底层基础设施和资源至关重要，包括GPU、内存、存储和网络带宽。实施负载均衡和冗余策略确保可扩展性和可用性，而强大的监控和自动化测试对于维护性能和健康至关重要。
- en: By carefully considering these factors, developers can ensure reliable, scalable,
    and cost-effective deployment of generative AI Apps, regardless of the chosen
    environment.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细考虑这些因素，开发者可以确保无论选择哪种环境，都能可靠、可扩展且成本效益地部署生成式AI应用。
- en: 11.3 Managed LLMs via API
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 通过API的托管LLM
- en: In addition to the deployment options previously discussed, it’s important to
    note that some LLMs are only available via an API hosted online in a managed manner.
    This is often the case with cutting-edge models developed by AI research organizations
    or large tech companies. As we know, GenAI models require significant computational
    resources, making them difficult to run on-premise or in a hybrid manner. Table
    11.1 outlines some of the advantages of managed LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的部署选项之外，重要的是要注意，一些LLM仅通过托管在在线API中提供。这通常是AI研究组织或大型科技公司开发的尖端模型的情况。正如我们所知，GenAI模型需要大量的计算资源，这使得它们难以在本地或混合模式下运行。表11.1概述了托管LLM的一些优势。
- en: Table 11.1 Advantages of managed LLMs
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.1 托管LLM的优势
- en: '| Advantages | Description |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 描述 |'
- en: '| --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Ease of use  | Managed LLMs via API are typically easy to use. Developers
    can send requests to the API and receive responses without worrying about the
    underlying infrastructure or model complexities.  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 易用性 | 通过API的托管LLM通常易于使用。开发者可以向API发送请求并接收响应，无需担心底层基础设施或模型复杂性。 |'
- en: '| Continuous updates  | The providers of these managed LLMs often continuously
    update and improve their models. An API allows you to take advantage of these
    improvements without manually updating your models.  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 持续更新 | 这些托管LLM的提供商通常会持续更新和改进他们的模型。API允许您利用这些改进，而无需手动更新您的模型。 |'
- en: '| Scalability  | Managed LLMs via API can handle high volumes of requests and
    scale automatically based on demand, similar to other cloud-based services.  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 通过API的托管LLM可以处理大量请求，并根据需求自动扩展，类似于其他基于云的服务。 |'
- en: '| Model complexity  | LLMs are enormously complex ML models and can present
    several challenges for enterprises, particularly those without extensive experience
    in AI and ML. Managed services offload this complexity to the provider, exposing
    the inference via an API.  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 模型复杂性 | LLM 是极其复杂的机器学习模型，对企业来说可能带来一些挑战，尤其是那些在 AI 和 ML 方面没有丰富经验的企业。托管服务将这种复杂性转嫁给提供商，通过
    API 提供推理。 |'
- en: There are also some constraints and challenges to consider when using managed
    LLMs via an API, as outlined in table 11.2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用通过 API 的托管 LLM 时，也需要考虑一些约束和挑战，如表 11.2 所述。
- en: Table 11.2 Considerations with managed LLMs
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.2 使用托管 LLM 的考虑因素
- en: '| Considerations | Description |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 考虑因素 | 描述 |'
- en: '| --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Cost  | The cost of using a managed LLM via API can vary significantly based
    on usage. While some providers offer free tiers, more extensive use can incur
    significant costs.  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 通过 API 使用托管 LLM 的成本可能会因使用情况而显著变化。虽然一些提供商提供免费层，但更广泛的使用可能会产生重大成本。 |'
- en: '| Dependency  | Using a managed LLM via API, you depend on the provider for
    the model and the infrastructure. If the provider experiences downtime or discontinues
    the service, this could affect your application.  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 依赖性 | 通过 API 使用托管 LLM，您依赖于提供商提供的模型和基础设施。如果提供商出现停机或停止服务，这可能会影响您的应用程序。 |'
- en: '| Data privacy  | Data is sent to the provider’s servers for processing using
    a managed LLM via API, which can raise privacy concerns, especially regarding
    sensitive data.  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私 | 使用通过 API 的托管 LLM 将数据发送到提供商的服务器进行处理，这可能会引起隐私问题，尤其是关于敏感数据。 |'
- en: '| Limited customization  | While managed LLMs via API offers ease of use, they
    typically offer limited customization options. You’re limited to the capabilities
    and configurations provided by the API and can’t modify the underlying model.  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 有限的定制化 | 虽然通过 API 的托管 LLM 提供了易用性，但它们通常提供有限的定制选项。您受限于 API 提供的功能和配置，无法修改底层模型。
    |'
- en: In summary, while managed LLMs via API offer several benefits, they also come
    with certain considerations. Whether they are the right option for your GenAI
    application depends on your needs and constraints. If you require a high level
    of customization, have strict data privacy requirements, or need to run your model
    offline, then an on-premise or hybrid deployment might be more suitable. However,
    a managed LLM via API could be a good choice if you value ease of use, continuous
    updates, and automatic scaling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然通过 API 的托管 LLM 提供了多项好处，但也伴随着某些考虑因素。它们是否是您 GenAI 应用程序的正确选择取决于您的需求和限制。如果您需要高度定制化、有严格的数据隐私要求或需要离线运行模型，那么本地或混合部署可能更合适。然而，如果您重视易用性、持续更新和自动扩展，那么通过
    API 的托管 LLM 可能是一个不错的选择。
- en: 11.4 Best practices for production deployment
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 生产部署的最佳实践
- en: To use GenAI applications, a comprehensive approach is required that involves
    careful planning and execution to ensure scalability, reliability, and security.
    When using LLMs in your application, you need to think about aspects such as LLMOps,
    observability, and tooling to handle the lifecycle of your application effectively.
    In addition, you need to consider other aspects such as model serving and management,
    reliability and performance considerations, and security and compliance considerations.
    These areas are important to ensuring that the application does what it should
    and follows high reliability, security, and compliance standards.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 GenAI 应用程序，需要一种全面的方法，涉及周密的规划和执行，以确保可扩展性、可靠性和安全性。当在您的应用程序中使用 LLM 时，您需要考虑
    LLMOps、可观察性和工具等方面，以有效地处理应用程序的生命周期。此外，您还需要考虑其他方面，如模型托管和管理、可靠性和性能考虑、以及安全和合规性考虑。这些领域对于确保应用程序按预期工作并遵循高可靠性、安全和合规性标准至关重要。
- en: In this section, you will learn about many of these aspects, such as metrics
    for LLM inference, how to measure and understand latency for LLMs, scalability,
    inference options for LLMs, quotas and rate limits, and observability. It will
    provide you with a complete guide to help you scale the GenAI application in production.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解许多这些方面，例如 LLM 推理的度量标准、如何衡量和理解 LLM 的延迟、可扩展性、LLM 的推理选项、配额和速率限制，以及可观察性。它将为您提供一份完整的指南，帮助您在生产中扩展
    GenAI 应用程序。
- en: 11.4.1 Metrics for LLM inference
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 LLM 推理的度量标准
- en: 'One of the most important metrics from the production deployment perspective
    is related to LLM inference. This is the main area that we all work on and deal
    with when developing GenAI applications. As we have seen, LLMs produce text in
    two steps: the prompt, where the input tokens are processed at once, and decoding,
    where text is created one token at a time sequentially. Each created token is
    added to the input and used again by the model to create the next token. Generation
    ends when the LLM produces a special stop token or when a user-defined condition
    is satisfied (e.g., a maximum number of tokens has been produced).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从生产部署的角度来看，最重要的指标之一与LLM推理相关。这是我们所有人都在开发GenAI应用时共同努力和解决的问题。正如我们所见，LLM通过两个步骤生成文本：提示，其中一次性处理输入标记，解码，其中逐个顺序地创建文本。每个创建的标记都会添加到输入中，并由模型再次使用以创建下一个标记。生成在LLM生成一个特殊的停止标记或满足用户定义的条件（例如，已生成最大数量的标记）时结束。
- en: 'Understanding and managing key operational metrics related to LLM inference
    becomes critical. Many of these metrics are new and still too early for most users
    to be comfortable with, but the following four metrics are particularly important:
    time to the first token, time per output token, latency, and throughput. Table
    11.3 outlines the definition and importance of these operational criteria. Later
    in the chapter, you will see how to measure this on our LLM deployment.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和管理与LLM推理相关的关键操作指标变得至关重要。许多这些指标都是新的，对于大多数用户来说还太早，但以下四个指标尤其重要：第一个标记的时间、每个输出标记的时间、延迟和吞吐量。表11.3概述了这些操作标准的定义和重要性。在本章的后面部分，您将看到如何在我们LLM部署上测量这些指标。
- en: Table 11.3 LLM inference metrics
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.3 LLM推理指标
- en: '| Metric | Definition |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 定义 |'
- en: '| --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Time to first token (TTFT)  | Measures the time it takes for the model to
    generate the first token after a user query. Lower TTFT means a more responsive
    user experience. TTFT is influenced by the time required to process the prompt
    and generate the first output token.  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Time to first token (TTFT)  | 衡量模型在用户查询后生成第一个标记所需的时间。较低的TTFT意味着更快的用户体验。TTFT受处理提示和生成第一个输出标记所需的时间的影响。  |'
- en: '| Time-per-output token (TPOT)  | Calculates the time required for the model
    to generate one token for a specific query. Lower TPOT means faster text generation.
    The model size, the hardware configuration, and the decoding algorithm influence
    TPOT.  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Time-per-output token (TPOT)  | 计算模型为特定查询生成一个标记所需的时间。较低的TPOT意味着更快的文本生成。模型大小、硬件配置和解码算法影响TPOT。  |'
- en: '| Latency  | This metric measures the time it takes for data to move from its
    starting point to its destination. In the case of LLMs, it is the time for the
    model to generate a response to the user. The model and the tokens generated influence
    LLMs’ latency. Generally, most of the time is spent generating complete tokens,
    which are generated one at a time. The longer the generation, the higher the latency.  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Latency  | 此指标衡量数据从起点到目的地的移动时间。在LLM的情况下，这是模型生成对用户响应的时间。模型和生成的标记影响LLM的延迟。通常，大部分时间都花在生成完整的标记上，这些标记是一次性生成的。生成时间越长，延迟越高。  |'
- en: '| Throughput  | Measures the amount of data that can be transferred in a unit
    of time. In this case, the number of output tokens per second on a deployment
    unit can be served across all requests.  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Throughput  | 测量单位时间内可以传输的数据量。在这种情况下，部署单元每秒可以处理的输出标记数量可以服务于所有请求。  |'
- en: '| Request per second (RPS)  | RPS measures the throughput of LLMs in production
    and indicates the number of requests an LLM can handle every second. This metric
    is crucial for understanding the scalability and efficiency of LLMs when deployed
    in real-world applications.  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Request per second (RPS)  | RPS衡量LLM在生产中的吞吐量，并指示LLM每秒可以处理的请求数量。这个指标对于理解LLM在现实世界应用中部署的可扩展性和效率至关重要。  |'
- en: Note  RPS and throughput are often used interchangeably in the context of performance
    metrics, but they can have nuanced differences. In essence, while RPS is about
    the incoming load, throughput is about the server’s output or the successful handling
    of that load. A high throughput with a high RPS indicates a well-performing server,
    while a low throughput with a high RPS might suggest that the server is struggling
    to keep up with the demand.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在性能指标方面，RPS 和吞吐量经常被互换使用，但它们可能有细微的差别。本质上，RPS 是关于传入负载，而吞吐量是关于服务器的输出或成功处理该负载。高吞吐量和高
    RPS 指示服务器性能良好，而低吞吐量和高 RPS 可能表明服务器难以跟上需求。
- en: 11.4.2 Latency
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 延迟
- en: Latency is a common metric used by almost everyone, but it is unclear and needs
    to be reexamined generative AI. The usual definition of latency does not fit well,
    as those APIs only gave back one result instead of multiple streaming responses.
    Because output generation depends greatly on input, GenAI has different latency
    points to consider. For instance, one latency is the first token latency; another
    is the full end-to-end latency after all the generation is done.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是几乎每个人都使用的常见指标，但在生成式 AI 中它并不明确，需要重新审视。通常的延迟定义并不适用，因为这些 API 只返回了一个结果，而不是多个流式响应。因为输出生成很大程度上取决于输入，所以
    GenAI 有不同的延迟点需要考虑。例如，一个延迟是第一个令牌延迟；另一个是在所有生成完成后完整的端到端延迟。
- en: 'We can’t rely on the second end-to-end latency alone, as we now know prompt
    size and output token count are the key influencing factors. The generation varies
    with the query (i.e., the prompt)—it is not a useful metric unless we compare
    similar tokens. For example, the following two require different amounts of computation
    and time, even when the input tokens are roughly the same:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅仅依赖于端到端延迟的第二次测量，因为我们现在知道提示大小和输出令牌数量是关键影响因素。生成内容会随着查询（即提示）的变化而变化——除非我们比较相似的令牌，否则这不是一个有用的指标。例如，以下两个例子即使输入令牌数量大致相同，所需的计算量和时间也不同：
- en: '*Example 1*—Generate a three-verse poem on why dogs are amazing.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例 1*—生成一首关于为什么狗很棒的三节诗。'
- en: '*Example 2*—Generate a three-page poem on why dogs are amazing.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例 2*—生成一篇关于为什么狗很棒的三页诗。'
- en: The first example has 11 tokens, and the second one has 10 tokens when using
    the `cl100kbase` tokenizer (used by the newer GPT models). However, the generated
    tokens are very different. Also, as previously described, the time-per-output
    token (TPOT) does not consider the input prompt. The input prompt is also large
    for many tasks such as summarization because retrieval-augmented generation (RAG)
    is used for in-context information. Thus, using TPOT as a way of measuring latency
    is not precise.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子有 11 个令牌，第二个例子在使用 `cl100kbase` 令牌化器（由较新的 GPT 模型使用）时有 10 个令牌。然而，生成的令牌非常不同。此外，如前所述，每输出令牌时间（TPOT）没有考虑输入提示。对于许多任务（如摘要）来说，输入提示也很大，因为这些任务使用了检索增强生成（RAG）来获取上下文信息。因此，使用
    TPOT 作为测量延迟的方法并不精确。
- en: The model size also affects resource usage; a smaller model is usually more
    efficient and uses fewer resources, while a larger model is more capable and powerful
    but takes much more time. Let’s use an example to show how to measure this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小也会影响资源使用；较小的模型通常更高效，使用的资源更少，而较大的模型功能更强大，但需要更多的时间。让我们用一个例子来展示如何测量这一点。
- en: The following listing shows a simple method for measuring the latency of the
    Azure OpenAI Chat API. Unlike the previous examples, which use software development
    kits (SDK), this one uses the REST APIs, and hence, we have to construct the payload
    and call the POST methods. We choose the number of requests to simulate and have
    a main function that employs a `ThreadPoolExecutor` to send several API requests
    simultaneously. It passes the `call_api_and_measure_latency()` function to the
    executor for each simulated request, gathers the latencies of all the requests,
    computes the average latency, and displays it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了一种测量 Azure OpenAI 聊天 API 延迟的简单方法。与之前使用软件开发工具包（SDK）的例子不同，这个例子使用的是 REST
    API，因此我们必须构建有效负载并调用 POST 方法。我们选择要模拟的请求数量，并有一个主函数，它使用 `ThreadPoolExecutor` 同时发送多个
    API 请求。它将 `call_api_and_measure_latency()` 函数传递给执行器，为每个模拟请求收集延迟，计算平均延迟，并显示它。
- en: Listing 11.1 Measuring latency
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.1 测量延迟
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Setting Azure OpenAI Chat API endpoint and API key'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置 Azure OpenAI 聊天 API 端点和 API 密钥'
- en: '#2 Defines the payload, including the model details to use'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义有效负载，包括要使用的模型细节'
- en: '#3 We stream the response so we can start getting the response faster.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们流式传输响应，以便我们可以更快地开始获取响应。'
- en: '#4 Function to call the Azure OpenAI Chat API and measure latency'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 调用 Azure OpenAI 聊天 API 并测量延迟的函数'
- en: '#5 Start time used to calculate latency'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 用于计算延迟的起始时间'
- en: '#6 End time used to calculate latency'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 用于计算延迟的结束时间'
- en: '#7 Number of requests to simulate'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 模拟请求的数量'
- en: '#8 Simulates concurrent API calls'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 模拟并发 API 调用'
- en: '#9 Calculates and print latency metrics'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 计算并打印延迟度量'
- en: Figure 11.1 shows an example of the output executed with 50 iterations and an
    average latency of 11.35 seconds on a pay-as-you-go (PAYGO) instance. This is
    the round-trip call from the client to the service, not the latency of the service
    itself. This isn’t great, and for most production workloads, we need to look at
    the reserved capacity, which we will cover in the next section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 展示了使用 50 次迭代和平均延迟 11.35 秒在按量付费（PAYGO）实例上执行的输出示例。这是客户端到服务的往返调用，而不是服务本身的延迟。这并不理想，对于大多数生产工作负载，我们需要查看预留容量，我们将在下一节中介绍。
- en: '![figure](../Images/CH11_F01_Bahree.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F01_Bahree.png)'
- en: Figure 11.1 Azure OpenAI latency example
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1 Azure OpenAI 延迟示例
- en: As shown in figure 11.2, in this example, we can use Azure’s out-of-the-box
    features to get service metrics such as latency. Using the default metric options,
    we see an average latency on this PAYGO instance of 95.37 milliseconds.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 11.2 所示，在这个示例中，我们可以使用 Azure 的内置功能来获取服务度量，如延迟。使用默认的度量选项，我们看到这个 PAYGO 实例的平均延迟为
    95.37 毫秒。
- en: '![figure](../Images/CH11_F02_Bahree.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F02_Bahree.png)'
- en: Figure 11.2 Azure requests and latency average
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.2 Azure 请求和延迟平均值
- en: Note  The code we saw before is a basic example showing us how to measure latency
    and a view from a production perspective; it is not a good implementation for
    load testing latency, especially if one is not using PAYGO. A better approach
    is to use a script with OSS tools such as Apache JMeter ([https://jmeter.apache.org](https://jmeter.apache.org))
    or Locust ([https://locust.io](https://locust.io)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们之前看到的代码是一个基本示例，展示了如何测量延迟以及从生产角度的视图；它不是负载测试延迟的良好实现，尤其是如果您不使用 PAYGO。更好的方法是使用带有
    OSS 工具（如 Apache JMeter [https://jmeter.apache.org](https://jmeter.apache.org)）或
    Locust [https://locust.io](https://locust.io) 的脚本。
- en: 11.4.3 Scalability
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.3 可扩展性
- en: One of the main scaling options an enterprise should consider when deploying
    a production application that uses an LLM, such as Azure OpenAI, is provisioned
    throughput units (PTUs). PTUs for Azure OpenAI are units of model processing capacity
    that you can reserve and deploy for processing prompts and generating completions.
    They embody a normalized way of representing the throughput for your deployment,
    with each model–version pair requiring different amounts for deployment and throughput
    per PTU. The throughput per PTU can differ based on the model type and version,
    and it’s important to know this to scale your application well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 企业在部署使用 LLM（如 Azure OpenAI）的生产应用程序时应考虑的主要扩展选项之一是预置吞吐量单元（PTU）。Azure OpenAI 的
    PTU 是模型处理容量的单位，您可以预留并部署用于处理提示和生成完成。它们体现了一种标准化的方式来表示部署的吞吐量，每个模型-版本对在部署和每个 PTU 的吞吐量方面都需要不同的数量。每个
    PTU 的吞吐量可能因模型类型和版本而异，了解这一点对于良好地扩展您的应用程序很重要。
- en: A PTU is essentially the same as a reserved instance that other Azure services
    have, but it is only a feature of Azure’s OpenAI service. When an application
    needs to scale and uses multiple AI services, the reserved instance capacity must
    be considered across all of those services, as there isn’t a universal service
    that reserves capacity for a specific application or subscription.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PTU（预置单元）本质上与 Azure 其他服务中的预留实例相同，但它仅是 Azure OpenAI 服务的功能。当应用程序需要扩展并使用多个 AI 服务时，必须考虑所有这些服务的预留实例容量，因为没有一种通用服务为特定应用程序或订阅预留容量。
- en: To deploy a model in Azure OpenAI using PTUs, we must select the “provisioned-managed”
    deployment type and indicate how many PTUs are required for the workload, as shown
    in figure 11.3\. We also need to calculate the size of our specific workload shapes,
    which you can do with the Azure OpenAI Capacity calculator. This calculation helps
    determine the right number of PTUs for your deployment.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Azure OpenAI 中使用 PTU 部署模型，我们必须选择“预置管理”部署类型，并指明工作负载所需的 PTU 数量，如图 11.3 所示。我们还需要计算我们特定工作负载形状的大小，您可以使用
    Azure OpenAI 容量计算器来完成此操作。此计算有助于确定部署所需的正确 PTU 数量。
- en: '![figure](../Images/CH11_F03_Bahree.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F03_Bahree.png)'
- en: Figure 11.3 PTU deployment options on Azure OpenAI
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3 Azure OpenAI上的PTU部署选项
- en: In addition to PTUs, enterprises can utilize a PAYGO model, which uses tokens
    per minute (TPM) consumed on demand. This model can be combined with PTUs to optimize
    utilization and cost. Furthermore, API Management (APIM) can be used with Azure
    OpenAI to manage and implement policies for queuing, rate throttling, error handling,
    and usage quotas.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了PTUs之外，企业可以利用按需使用的PAYGO模型，该模型使用每分钟消耗的令牌（TPM）。此模型可以与PTUs结合使用，以优化利用率和成本。此外，API管理（APIM）可以与Azure
    OpenAI一起使用，以管理和实施排队、速率限制、错误处理和使用配额的政策。
- en: 'By running the same latency tests performed for PAYGO on the PTU instance with
    slight modifications, we get the following results across both when using GPT-4
    and the same model version. We randomly pick a prompt from a list to call and
    loop through 100 iterations in each case. An average of 2.9 seconds of end-to-end
    latency on PTUs is pretty decent compared to 6.3 seconds on PAYGO, which is not
    bad but not great:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在PTU实例上对PAYGO进行的相同延迟测试进行轻微修改，我们得到了以下结果，无论是使用GPT-4还是相同模型版本。我们在每种情况下随机从列表中选择一个提示进行调用，并循环100次。在PTUs上平均2.9秒的端到端延迟相当不错，与PAYGO上的6.3秒相比，这不算太坏，但也不算太好：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code in listing 11.2 shows the difference. This function iterates over the
    two OpenAI clients and their corresponding models. A `ThreadPoolExecutor` with
    20 workers is created for each client–model pair, and tasks are submitted. Each
    task is a call to the `call_completion_api()` function (a wrapper around the Azure
    OpenAI completion API) with a randomly chosen input from the test inputs. It collects
    the latencies of all the tasks, calculates the median, average, minimum, and maximum
    latency, and prints these metrics.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.2中的代码显示了差异。此函数遍历两个OpenAI客户端及其相应的模型。为每个客户端-模型对创建一个具有20个工作者的`ThreadPoolExecutor`，并提交任务。每个任务是对`call_completion_api()`函数（Azure
    OpenAI完成API的包装）的调用，该调用使用从测试输入中随机选择的输入。它收集所有任务的延迟，计算中位数、平均值、最小值和最大延迟，并打印这些指标。
- en: Listing 11.2 Measuring latency between PAYGO and PTU
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2 测量PAYGO和PTU之间的延迟
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 11.4.4 PAYGO
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.4 PAYGO
- en: The PAYGO model with TPM is a flexible payment method that lets you pay only
    for the resources you use. The method is especially helpful for applications that
    have changing usage patterns and do not need constant processing capacity. It
    is the standard for most customers and applications across most providers. TPM
    is the measure of the model’s processing power. When you send a request to the
    model, it uses a certain number of tokens based on the prompt and the response’s
    complexity and length. We are billed for each token consumed, so as the usage
    increases, you pay more, and if it decreases, you pay less.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 带有TPM的PAYGO模型是一种灵活的支付方式，让您只为使用的资源付费。对于具有变化的使用模式且不需要恒定处理能力的应用程序，此方法特别有用。它是大多数提供商和大多数客户的标准。TPM是模型处理能力的衡量标准。当您向模型发送请求时，它根据提示和响应的复杂性和长度使用一定数量的令牌。我们按每个消耗的令牌计费，因此随着使用量的增加，您支付的更多，如果减少，则支付的更少。
- en: Most cloud-based LLMs have a quota management feature that lets you assign rate
    limits to your deployments up to a global limit. Similarly, deployment and rate
    limits are associated with a model deployment. We can also assign a certain TPM
    to a specific deployment; when we do that, the available quota for that model
    will be reduced by that amount.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于云的LLM都具有配额管理功能，允许您将速率限制分配到您的部署，最高可达全局限制。同样，部署和速率限制与模型部署相关联。我们还可以将特定的TPM分配给特定的部署；当我们这样做时，该模型的可用配额将减少相应数量。
- en: The PAYGO model is advantageous for scaling because it allows you to distribute
    TPM globally within a subscription and region, providing the flexibility to manage
    the allocation of rate limits across the deployments within your subscription.
    This model is ideal for applications with peak times of high usage followed by
    periods of low or no usage, as it ensures you only pay for what you use.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PAYGO模型在扩展方面具有优势，因为它允许您在订阅和区域内全局分配TPM，提供在您的订阅内管理部署之间速率限制分配的灵活性。此模型非常适合具有高峰时段高使用量随后是低或无使用量的应用程序，因为它确保您只为使用付费。
- en: 11.4.5 Quotas and rate limits
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.5 配额和速率限制
- en: Quotas and rate limits are two mechanisms used in cloud services to manage and
    control resource usage. Quotas are the total amount of a resource that a user
    or service can consume over a specified period, such as a day or a month. They
    act as a cap on usage to prevent overconsumption of resources and ensure fair
    distribution among users.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 配额和速率限制是云服务中用于管理和控制资源使用的两种机制。配额是指用户或服务在指定期间（如一天或一个月）可以消耗的资源总量。它们作为使用上限，防止资源过度消耗并确保用户之间公平分配。
- en: In contrast, rate limits control the frequency of requests to a service. They
    are typically defined as the number of requests that can be made per second or
    minute. By limiting the rate at which users can make requests, rate limits help
    manage load and avoid overloading systems.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，速率限制控制对服务的请求频率。它们通常定义为每秒或每分钟可以发出的请求数量。通过限制用户可以发出请求的速率，速率限制有助于管理负载并避免系统过载。
- en: In essence, quotas refer to the quantity of resources you can use, while rate
    limits refer to the frequency of access to those resources. Understanding both
    is crucial for efficient API management and avoiding service disruptions for enterprises.
    By adhering to rate limits, enterprises can ensure their applications do not send
    more requests than a service can handle at a given time, which helps maintain
    performance and stability. Meanwhile, by staying within quotas, they can control
    their costs and prevent unexpected overages.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，配额指的是您可以使用的资源数量，而速率限制指的是访问这些资源的频率。理解这两者对于有效的API管理和避免企业服务中断至关重要。通过遵守速率限制，企业可以确保其应用程序不会发送超过服务在特定时间内可以处理的请求数量，这有助于保持性能和稳定性。同时，通过保持在配额内，它们可以控制成本并防止意外超支。
- en: Quotas for the OpenAI service, particularly for Azure OpenAI, are defined as
    limits on the resources or computational capacity a user or organization can consume.
    These quotas are typically measured in TPM and assigned on a per-region, per-model
    basis. The quotas ensure that the service can maintain consistent and predictable
    performance for all users.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI服务的配额，尤其是Azure OpenAI的配额，定义为用户或组织可以消耗的资源或计算能力的限制。这些配额通常以TPM（每秒事务数）衡量，并按区域和模型分配。配额确保服务可以为所有用户提供一致和可预测的性能。
- en: Enterprises should think about these quotas as a way to manage their usage and
    costs effectively. They must monitor their consumption to avoid exceeding these
    limits, which could lead to additional charges or service interruptions. It’s
    also important for enterprises to understand the rate limits associated with their
    deployments and plan accordingly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 企业应将这些配额视为一种有效管理使用和成本的方式。他们必须监控他们的消费，以避免超过这些限制，这可能导致额外费用或服务中断。对于企业来说，了解与其部署相关的速率限制并据此规划也同样重要。
- en: For example, if an enterprise has a quota of 240,000 TPM for a specific model
    in a region, it could create one deployment of 240K TPM, two of 120K TPM each,
    or multiple deployments adding up to less than 240K TPM in that region. For example,
    figure 11.4 outlines the quota setting for a specific Azure OpenAI endpoint and
    the various models deployed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一家企业在某个区域的一个特定模型上有240,000 TPM的配额，它可以创建一个240K TPM的部署，两个各120K TPM的部署，或者在该区域创建多个总计少于240K
    TPM的部署。例如，图11.4概述了特定Azure OpenAI端点和部署的各种模型的配额设置。
- en: '![figure](../Images/CH11_F04_Bahree.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F04_Bahree.png)'
- en: Figure 11.4 Azure OpenAI model quota setting
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4 Azure OpenAI模型配额设置
- en: OpenAI has its system of quotas, but they are structured differently. OpenAI’s
    quotas are typically related to usage limits that are set based on the billing
    information provided by the user. Once billing information is entered, users have
    an approved usage limit of a set amount per month (the default is $100), which
    can automatically increase as usage on the platform grows. Users move from one
    usage tier to another, as shown in figure 11.5\. Users can review their current
    usage limit in the account settings under the limits page.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI有其配额系统，但它们的结构不同。OpenAI的配额通常与基于用户提供的计费信息设定的使用限制相关。一旦输入计费信息，用户每月将有一个批准的使用限额（默认为100美元），随着平台使用量的增长，这个限额可以自动增加。用户会从一个使用层级移动到另一个层级，如图11.5所示。用户可以在“限制”页面下的账户设置中查看他们当前的限额。
- en: '![figure](../Images/CH11_F05_Bahree.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F05_Bahree.png)'
- en: Figure 11.5 OpenAI quota tiers
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5 OpenAI配额层级
- en: These quotas are designed to help manage and predict costs and prevent resource
    overuse. Enterprises should monitor their usage closely to ensure they stay within
    these limits and understand how these limits can scale with increased usage.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配额旨在帮助管理和预测成本以及防止资源过度使用。企业应密切监控其使用情况，以确保它们保持在这些限制内，并了解这些限制如何随着使用量的增加而扩展。
- en: 11.4.6 Managing quota
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.6 管理配额
- en: 'Managing quotas effectively is crucial for maintaining consistent and predictable
    application performance. Here are some best practices to consider:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地管理配额对于保持一致和可预测的应用程序性能至关重要。以下是一些值得考虑的最佳实践：
- en: '*Understand your limits*. Familiarize yourself with the default quotas and
    limits that apply to the models, as each model and region can have different default
    quota limits.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*了解您的限制*。熟悉适用于模型的基本配额和限制，因为每个模型和区域都可以有不同的默认配额限制。'
- en: '*Monitor your usage*. Implement monitoring strategies to keep track of your
    usage against the assigned quotas. This will help you avoid unexpected throttling
    and ensure a good customer experience.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控您的使用情况*。实施监控策略以跟踪您的使用情况与分配配额之间的关系。这将帮助您避免意外的限制并确保良好的客户体验。'
- en: '*Implement retry logic*. In your application, include retry logic to handle
    rate limit errors. This will allow your application to wait and retry the request
    after a brief pause rather than failing outright. A simple way to do this is to
    use the `Tenacity` library (an OSS library):'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实现重试逻辑*。在您的应用程序中包含重试逻辑以处理速率限制错误。这将允许您的应用程序在短暂的暂停后等待并重试请求，而不是直接失败。这样做的一种简单方法是使用`Tenacity`库（一个开源库）：'
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Avoid sharp workload changes*. Gradually increase your workload to prevent
    sudden spikes that could lead to throttling. Test different load increase patterns
    to find the most efficient approach for your application. Note that throttling
    intentionally slows down or limits the requests an app or service can handle over
    a certain period. The server or service provider usually enforces this to prevent
    system overloads, ensure fair usage, and maintain quality of service. As we know,
    throttling is a common practice in API management and cloud-based services to
    manage resources efficiently and protect the system from potential abuse or denial
    of service (DoS) attacks. It’s also used to prevent a single user or service from
    consuming all available resources and affecting the performance of other users
    or services.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*避免工作负载的急剧变化*。逐渐增加您的负载以防止突然的峰值，这可能导致限制。测试不同的负载增加模式以找到最适合您应用程序的最有效方法。请注意，限制会故意减慢或限制应用程序或服务在一定时期内可以处理的请求。服务器或服务提供商通常执行此操作以防止系统过载，确保公平使用并维护服务质量。正如我们所知，限制是API管理和基于云的服务中管理资源效率并保护系统免受潜在滥用或拒绝服务（DoS）攻击的常见做法。它还用于防止单个用户或服务消耗所有可用资源并影响其他用户或服务的性能。'
- en: '*Manage TPM allocation*. Use the quota management feature to increase TPM on
    deployments with high traffic and reduce TPM on deployments with limited needs.
    This helps balance the load and optimize resource utilization.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理TPM分配*。使用配额管理功能在流量高的部署中增加TPM，在需求有限的部署中减少TPM。这有助于平衡负载并优化资源利用率。'
- en: '*Request quota increases*. If you consistently exceed your quota limits, consider
    requesting an increase through the Azure portal or by contacting Microsoft support
    or your cloud provider for those not on Azure.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*请求配额增加*。如果您持续超出配额限制，请考虑通过Azure门户或联系Microsoft支持或非Azure云服务提供商来请求增加配额。'
- en: '*Distribute requests evenly*. To avoid hitting the requests-per-minute (RPM)
    rate limit, distribute your requests evenly over time. Many cloud providers, including
    Azure OpenAI, evaluate incoming requests’ rates over a short period and may throttle
    if the RPM limit is surpassed.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*均匀分配请求*。为了避免达到每分钟请求次数（RPM）的限制，请将您的请求均匀地分配到一段时间内。包括Azure OpenAI在内的许多云服务提供商都会在短时间内评估传入请求的速率，如果超过RPM限制，可能会进行限制。'
- en: Note  With Azure OpenAI, you can combine PAYGO and PTUs to meet your workloads.
    This hybrid approach lets you use the flexibility of PAYGO for variable workloads,
    while having the reliability and consistency of PTUs for steady workloads. When
    you do this, PTUs are good for workloads with stable performance needs as they
    give you a fixed amount of throughput capacity that you reserve ahead of time,
    ensuring low latency variation. Furthermore, PAYGO is great for handling uncertain
    workloads where the usage can change. You’re charged based on the tokens used
    per minute, which means you pay more when your usage is high and less when it’s
    low.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用Azure OpenAI，您可以结合PAYGO和PTUs来满足您的负载。这种混合方法让您可以使用PAYGO的灵活性来处理可变负载，同时拥有PTUs的可靠性和一致性，适用于稳定负载。当您这样做时，PTUs对于具有稳定性能需求的负载很有用，因为它们在提前预留固定数量的吞吐量容量，从而确保低延迟变化。此外，PAYGO非常适合处理不确定的负载，其中使用量可能会变化。您根据每分钟的令牌使用量付费，这意味着当您的使用量高时，您需要支付更多，而当使用量低时，您需要支付更少。
- en: By actively managing their quotas and rate limits, enterprises can ensure they
    have the necessary capacity for their applications, while controlling costs and
    maintaining service availability.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过积极管理配额和速率限制，企业可以确保它们有足够的容量来满足其应用程序的需求，同时控制成本并保持服务可用性。
- en: 11.4.7 Observability
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.7 可观测性
- en: 'Observability for LLM applications refers to monitoring, logging, and tracing
    to ensure the application works as intended and fixes problems when they occur.
    Let’s examine each one in a little more detail:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用的可观测性指的是监控、日志记录和跟踪，以确保应用程序按预期工作，并在出现问题时进行修复。让我们更详细地考察每一个方面：
- en: '*Monitoring*—Measure key performance indicators (KPIs) such as response times,
    throughput, error rates, and resource utilization. This data is essential for
    knowing the state of your application and making smart choices about scaling and
    optimization.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控*—衡量关键性能指标（KPIs），如响应时间、吞吐量、错误率和资源利用率。这些数据对于了解应用程序的状态和做出关于扩展和优化的明智选择至关重要。'
- en: '*Logging*—Detailed logs should record requests and responses, including the
    input prompts and the model’s outputs. This information is priceless for debugging,
    understanding model behavior, and enhancing the user experience.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日志记录*—详细的日志应记录请求和响应，包括输入提示和模型的输出。这些信息对于调试、理解模型行为和提升用户体验是无价的。'
- en: '*Tracing*—Use tracing to track the route of requests through your application.
    This is especially important for applications with complex architectures or multiple
    models and services. Tracing helps locate bottlenecks and areas for optimization.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跟踪*—使用跟踪来追踪请求通过应用程序的路径。这对于具有复杂架构或多个模型和服务的应用程序尤为重要。跟踪有助于定位瓶颈和优化区域。'
- en: In the following sections, we use MLflow, Traceloop, and Prompt flow to show
    you how to implement this. Let’s start with MLflow.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用MLflow、Traceloop和Prompt flow向您展示如何实现这一点。让我们从MLflow开始。
- en: MLFlow
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLFlow
- en: MLflow is an open source platform that aims to manage the ML lifecycle, including
    experimentation, reproducibility, and deployment. It helps practitioners simplify
    their MLflow works with tools for tracking experiments, packaging code, and managing
    models. MLflow’s main components include tracking, model registry, and a server
    for deploying models, facilitating teamwork and innovation in ML projects.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow是一个开源平台，旨在管理机器学习（ML）的生命周期，包括实验、可重复性和部署。它通过提供跟踪实验、打包代码和管理模型的工具，帮助从业者简化他们的MLflow工作。MLflow的主要组件包括跟踪、模型注册和用于部署模型的服务器，这有助于促进机器学习项目中的团队合作和创新。
- en: MLflow enhances the observability of LLMs by providing tools that streamline
    the deployment and monitoring process. It offers a unified interface for interacting
    with different LLM providers, simplifying the integration and management of models.
    MLflow’s platform-agnostic nature also facilitates seamless integrations and deployments
    across various cloud platforms, further aiding in the observability and management
    of LLMs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow通过提供简化部署和监控过程的工具，增强了LLM的可观测性。它提供了一个统一的界面，用于与不同的LLM提供商交互，简化了模型集成和管理。MLflow平台无关的特性也促进了跨各种云平台的无缝集成和部署，进一步有助于LLM的可观测性和管理。
- en: As shown in listing 11.3, we use MLflow to achieve this. This basic console
    chat application uses Azure OpenAI and randomly uses a few prompts in the list
    `text_inputs`. We can set how many times to repeat this using multiple threads.
    When we call the chat completion API, we log various features to demonstrate how
    MLflow can be applied.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表 11.3 所示，我们使用 MLflow 实现这一点。这个基本的控制台聊天应用程序使用 Azure OpenAI，并随机使用列表 `text_inputs`
    中的几个提示。我们可以通过多个线程设置重复次数。当我们调用聊天完成 API 时，我们记录各种特征以展示 MLflow 的应用方式。
- en: We require that MLflow and Prometheus ([https://prometheus.io](https://prometheus.io))
    be installed and running at an endpoint to run this. In our case, we run this
    locally in a Docker container exposed at port 5000\. The docker-compose file is
    shown in the following listing. The book’s GitHub repository ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    also has all the code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求 MLflow 和 Prometheus ([https://prometheus.io](https://prometheus.io)) 在一个端点安装并运行才能执行此操作。在我们的例子中，我们在端口
    5000 的 Docker 容器中本地运行。下面的列表显示了 docker-compose 文件。本书的 GitHub 仓库 ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    也包含了所有代码。
- en: Listing 11.3 docker-cmpose file for MLflow
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.3 MLflow 的 docker-cmpose 文件
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We start by running the Docker container using the docker compose command,
    as shown: `docker compose up -d`. The `-d` parameter runs this as detached, which
    can be helpful and run in the background. As outlined in listing 11.4, we begin
    by specifying MLflow’s tracking URI (`http://localhost:5000`); this is the location
    where MLflow will store the data that we log and also assign a name for the experiment
    (`GenAI_ book`) so we can distinguish it from others. Of course, we are the sole
    users of this example since it runs locally. In addition, we need to install the
    following two dependencies for this to work: `– mlflow` and `colorama`. With conda,
    this can be installed using `conda install -c conda-forge mlflow colorama`, or
    with pip using `pip install mlflow colorama.`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 docker compose 命令运行 Docker 容器，如下所示：`docker compose up -d`。`-d` 参数以分离模式运行，这很有帮助，可以在后台运行。如列表
    11.4 所述，我们首先指定 MLflow 的跟踪 URI (`http://localhost:5000`)；这是 MLflow 将存储我们记录的数据的位置，同时也为实验分配一个名称（`GenAI_book`）以便我们可以将其与其他内容区分开来。当然，我们是这个示例的唯一用户，因为它是在本地运行的。此外，我们还需要安装以下两个依赖项才能使其工作：`–
    mlflow` 和 `colorama`。使用 conda，可以使用 `conda install -c conda-forge mlflow colorama`
    安装，或者使用 pip 使用 `pip install mlflow colorama`。
- en: We measure features such as token count, prompts, conversation, and so forth.
    We also compute the time needed to receive a response and store it. We use the
    `mlflow.log_metrics()` function to store all these metrics. We also store the
    parameters used in the API request using the `mlflow.log_params()` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测量诸如令牌计数、提示、对话等特征。我们还计算接收响应所需的时间并将其存储。我们使用 `mlflow.log_metrics()` 函数存储所有这些指标。我们还使用
    `mlflow.log_params()` 函数存储 API 请求中使用的参数。
- en: Listing 11.4 MLflow observability example
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.4 MLflow 可观测性示例
- en: '[PRE5]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Logging this data allows you to compare different runs, examine your model’s
    performance, and see how parameter changes affect the output using the MLflow
    UI. Figure 11.6 shows an example of the information when we run multiple experiments
    and can contrast them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 记录这些数据允许您比较不同的运行，检查您模型的性能，并使用 MLflow UI 看到参数变化如何影响输出。图 11.6 展示了当我们运行多个实验并可以对比它们时的信息。
- en: '![figure](../Images/CH11_F06_Bahree.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F06_Bahree.png)'
- en: Figure 11.6 MLFlow experiments dashboard
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.6 MLFlow 实验仪表板
- en: 'Figure 11.7 shows some metrics we have been monitoring: the `completion_tokens`
    and how they relate to the request latency when the `request_latency` is plotted.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 展示了一些我们一直在监控的指标：`completion_tokens` 以及当 `request_latency` 绘图时它们与请求延迟的关系。
- en: '![figure](../Images/CH11_F07_Bahree.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F07_Bahree.png)'
- en: Figure 11.7 MLflow model metrics examples
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7 MLflow 模型指标示例
- en: Figure 11.8 illustrates how we can also log some of the prompt details and the
    generated response, which is very useful for observability. Of course, this should
    be done carefully, depending on the privacy and legal implications of who can
    access this telemetry.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 阐述了我们可以如何记录一些提示细节和生成的响应，这对于可观测性非常有用。当然，这应该谨慎进行，取决于谁可以访问此遥测的隐私和法律影响。
- en: '![figure](../Images/CH11_F08_Bahree.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F08_Bahree.png)'
- en: Figure 11.8 MLflow prompt and response details
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.8 MLflow 提示和响应细节
- en: Traceloop and OpenLLMetry
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Traceloop 和 OpenLLMetry
- en: Traceloop ([https://www.traceloop.com/](https://www.traceloop.com/)) is an observability
    tool for monitoring LLM applications. It offers features such as real-time alerts
    and execution tracing to ensure quality deployment. OpenLLMetry, built on OpenTelemetry,
    is an open source extension maintained by Traceloop that enhances LLM observability.
    It integrates with Traceloop’s tools and adds LLM-specific monitoring capabilities,
    facilitating developers’ work with LLM observability, while aligning with OpenTelemetry
    standards.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Traceloop ([https://www.traceloop.com/](https://www.traceloop.com/)) 是一个用于监控
    LLM 应用的可观测性工具。它提供实时警报和执行跟踪等功能，以确保质量部署。基于 OpenTelemetry 构建的 OpenLLMetry 是一个由 Traceloop
    维护的开源扩展，它增强了 LLM 的可观测性。它集成了 Traceloop 的工具，并增加了针对 LLM 的特定监控能力，便于开发者进行 LLM 可观测性工作，同时与
    OpenTelemetry 标准保持一致。
- en: OpenLLMetry extends OpenTelemetry’s functionality to cover generic operations
    such as database and API interactions and custom extensions for LLM-specific operations.
    This includes calls to LLM providers such as OpenAI or Anthropic and interactions
    with vector databases such as Chroma or Pinecone. In other words, OpenLLMetry
    offers a specialized toolkit for LLM applications, making it easier for developers
    to begin with observability in this domain, while still generating standard OpenTelemetry
    data that can be compatible with existing observability stacks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: OpenLLMetry 扩展了 OpenTelemetry 的功能，以涵盖通用操作，如数据库和 API 交互，以及针对 LLM 特定操作的定制扩展。这包括对
    OpenAI 或 Anthropic 等LLM 提供商的调用，以及与 Chroma 或 Pinecone 等向量数据库的交互。换句话说，OpenLLMetry
    为 LLM 应用程序提供了一套专业工具包，使开发者更容易在这个领域开始可观测性，同时仍然生成与现有可观测性堆栈兼容的标准 OpenTelemetry 数据。
- en: Integrating this with the existing application is quite simple. We need to install
    the Traceloop SDK (`pip` `install` `traceloop-sdk`). Next, we create a login and
    get an API key at [https://app.traceloop.com/](https://app.traceloop.com/). Initializing
    this is simple using `Traceloop.init()`, which instruments it automatically.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与现有应用程序集成相当简单。我们需要安装 Traceloop SDK (`pip install traceloop-sdk`)。接下来，我们在 [https://app.traceloop.com/](https://app.traceloop.com/)
    创建登录并获取 API 密钥。使用 `Traceloop.init()` 初始化它非常简单，它会自动进行仪表化。
- en: Listing 11.5 Using Traceloop
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.5 使用 Traceloop
- en: '[PRE6]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Traceloop also has multiple integration points into other systems and various
    LLM APIs. See [https://mng.bz/gAJx](https://mng.bz/gAJx) for more details. For
    our purposes, we’ll use the default dashboard for our example, as shown in figure
    11.9.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Traceloop 还与其他系统和各种 LLM API 有多个集成点。更多详情请见 [https://mng.bz/gAJx](https://mng.bz/gAJx)。为了我们的示例，我们将使用默认仪表板，如图
    11.9 所示。
- en: '![figure](../Images/CH11_F09_Bahree.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F09_Bahree.png)'
- en: Figure 11.9 Traceloop observability
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.9 Traceloop 可观测性
- en: Given that we can dig into various traces from an observability perspective,
    we get many details of the API calls (figure 11.10). In this example, we can see
    the system prompts, the user prompt, the completion, and other instrumentations,
    such as token usage. This can be a very powerful feature for many enterprise applications.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以从可观测性的角度深入挖掘各种跟踪，我们得到了许多 API 调用的详细信息（图 11.10）。在这个例子中，我们可以看到系统提示、用户提示、完成情况以及其他仪表，例如令牌使用情况。这可以是一个非常强大的功能，适用于许多企业应用程序。
- en: Prompt flow
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示流
- en: Prompt flow is an open source set of tools and features from Microsoft. It improves
    the creation process of AI applications, especially those that use LLMs. It helps
    with the design, evaluation, and implementation stages of AI applications, providing
    a simple interface for developers to work with LLMs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 提示流是来自微软的开源工具和功能集。它改善了 AI 应用程序的创建过程，特别是那些使用 LLM 的应用程序。它帮助设计、评估和实施 AI 应用程序，为开发者提供了一个简单的界面来与
    LLM 一起工作。
- en: '![figure](../Images/CH11_F10_Bahree.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F10_Bahree.png)'
- en: Figure 11.10 Traceloop observability example
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.10 Traceloop 可观测性示例
- en: Prompt flow is a key feature for developers who want to use LLMs in enterprise
    applications, as it helps with both observability and LLMOps aspects. It lets
    developers build executable workflows that combine LLMs, prompts, and Python tools.
    This allows developers to find and fix errors and improve flows more easily, with
    the extra advantage of team collaboration features. Developers can create different
    prompt options, evaluate their effectiveness, and adjust the LLM’s performance
    as needed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于想要在企业应用程序中使用 LLM 的开发者来说，提示流是一个关键特性，因为它帮助处理可观测性和 LLMOps 方面。它允许开发者构建结合 LLM、提示和
    Python 工具的可执行工作流程。这使得开发者能够更容易地查找和修复错误，并改进流程，同时额外提供团队协作功能的优势。开发者可以创建不同的提示选项，评估其有效性，并根据需要调整
    LLM 的性能。
- en: Prompt flow consists of four stages, as illustrated in figure 11.11\. The first
    stage, initialization, involves selecting a business use case, gathering a smaller
    dataset, and building a basic prompt and flow. Next, the experimentation stage
    requires testing and modifying the initial prompt until it reaches a good outcome.
    The third stage, evaluation and refinement, involves measuring the prompt’s quality
    and the flow’s performance on a larger dataset, with more adjustments and improvements
    made to achieve the desired output. Finally, the production stage involves launching
    the flow for production use, tracking usage, feedback, and any problems that may
    occur in a production setting.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 提示流由四个阶段组成，如图11.11所示。第一阶段，初始化，涉及选择一个业务用例，收集较小的数据集，并构建基本的提示和流程。接下来，实验阶段需要测试和修改初始提示，直到达到良好的结果。第三阶段，评估和改进，涉及在更大的数据集上测量提示的质量和流程的性能，进行更多调整和改进以达到预期的输出。最后，生产阶段涉及将流程投入生产使用，跟踪使用情况、反馈以及可能出现在生产环境中的任何问题。
- en: '![figure](../Images/CH11_F11_Bahree.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F11_Bahree.png)'
- en: Figure 11.11 Prompt flow lifecycle
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.11 提示流生命周期
- en: Prompt flow offers many benefits when an application moves from development
    to production. It helps the application work well with existing CI/CD pipelines
    and gives powerful version control and collaborative tools for scaling LLM applications.
    This complete environment allows developers to deploy LLM-powered applications
    with more confidence, supported by the ability to track and understand the model’s
    behavior in a live setting. Therefore, the prompt flow is a key part of the deployment
    strategy, ensuring that applications using LLMs are strong, dependable, and prepared
    for the production needs of the enterprise level. More details, including easy-start
    samples, can be found in Prompt flow’s GitHub repository at [https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序从开发阶段过渡到生产阶段时，提示流提供了许多好处。它有助于应用程序与现有的CI/CD管道良好地工作，并为扩展LLM应用程序提供强大的版本控制和协作工具。这个完整的环境使开发者能够更有信心地部署由LLM驱动的应用程序，并支持在实时环境中跟踪和理解模型的行为。因此，提示流是部署策略的关键部分，确保使用LLM的应用程序强大、可靠，并准备好满足企业级的生产需求。更多详细信息，包括易于开始的示例，可以在提示流的GitHub仓库[https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow)中找到。
- en: NOTE  Model serving involves deploying trained models to make predictions with
    new data. It’s a critical component for applications’ responsiveness and scalability.
    However, it demands significant investment in skills, computing resources across
    data centers, operational costs, and specialized hardware such as GPUs with InfiniBand
    connectivity. An open source software library such as vLLM could benefit organizations
    considering model serving. The efficient model hinges on scalable infrastructure,
    which can adjust resources for demand and ensure availability and cost-efficiency.
    Caching strategies and load balancing are key to reducing latency and evenly distributing
    requests. A solid update strategy, employing blue–green deployments, ensures smooth
    model transitions with minimal downtime. For more details on vLLM, see [https://www.vllm.ai/](https://www.vllm.ai/).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：模型服务涉及将训练好的模型部署到使用新数据进行预测。它是应用程序响应性和可扩展性的关键组件。然而，它需要在大数据中心、运营成本和如具有InfiniBand连接的GPU等专用硬件方面的重大投资。对于考虑模型服务的组织，开源软件库如vLLM可能有益。高效的模型取决于可扩展的基础设施，它可以调整资源以满足需求，并确保可用性和成本效益。缓存策略和负载均衡是减少延迟和均匀分配请求的关键。采用蓝绿部署的稳健更新策略确保模型平稳过渡，最小化停机时间。有关vLLM的更多详细信息，请参阅[https://www.vllm.ai/](https://www.vllm.ai/)。
- en: 11.4.8 Security and compliance considerations
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.8 安全性和合规性考虑
- en: Security and compliance are critical, especially when dealing with user data
    and potentially sensitive information. Adhering to best practices helps protect
    your users and ensures your application complies with relevant laws and regulations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和合规性至关重要，尤其是在处理用户数据和可能敏感信息时。遵循最佳实践有助于保护您的用户，并确保您的应用程序符合相关法律和法规。
- en: '*Data encryption*—Encrypt sensitive data at rest and in transit to protect
    against unauthorized access. Use secure protocols such as TLS for data in transit
    and utilize encryption features offered by your cloud provider for data at rest.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据加密*—对静态和传输中的敏感数据进行加密，以防止未经授权的访问。对于传输中的数据，使用如TLS等安全协议；对于静态数据，利用云提供商提供的加密功能。'
- en: '*Access control*—Implement strict access controls to ensure only authorized
    personnel can access production data and infrastructure. Use role-based access
    control (RBAC) and the principle of least privilege (PoLP) to minimize the risk
    of data breaches.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*访问控制*—实施严格的访问控制，以确保只有授权人员才能访问生产数据和基础设施。使用基于角色的访问控制（RBAC）和最小权限原则（PoLP）来最小化数据泄露的风险。'
- en: '*Compliance audits*—Regularly audit your application and its infrastructure
    for compliance with relevant regulations and standards, such as GDPR, HIPAA, or
    CCPA, depending on your application’s domain and geographical scope. This may
    involve conducting security assessments, vulnerability scanning, and compliance
    checks.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合规性审计*—定期审计您的应用程序及其基础设施，以确保符合相关法规和标准，例如根据您的应用程序领域和地理范围，GDPR、HIPAA 或 CCPA。这可能涉及进行安全评估、漏洞扫描和合规性检查。'
- en: '*Anomaly detection*—Deploy anomaly detection systems to monitor for unusual
    activity that could indicate a security breach or system misuse. This includes
    monitoring for abnormal usage patterns or unauthorized access attempts, allowing
    for rapid response to potential threats.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测*—部署异常检测系统以监控可能表明安全漏洞或系统滥用的异常活动。这包括监控异常使用模式或未经授权的访问尝试，以便对潜在威胁做出快速响应。'
- en: Azure OpenAI Service offers many of these features as standard to meet enterprise
    readiness and compliance needs. As most enterprises demand, other cloud providers
    such as AWS and GCP have some versions of these controls.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务将许多这些功能作为标准提供，以满足企业就绪和合规性需求。正如大多数企业所要求的那样，其他云服务提供商如 AWS 和 GCP
    也提供了一些这些控制版本的版本。
- en: 11.5 GenAI operational considerations
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 通用人工智能操作注意事项
- en: Operational aspects of GenAI applications, particularly those utilizing LLMs
    such as GPT-4, are critical for ensuring smooth and efficient functioning of these
    systems. Understanding and managing key operational metrics such as tokens, latency,
    requests per second (RPS), and time to first byte (TTFB) are vital for optimizing
    performance, user experience, and cost. Let’s examine the definition, get a better
    understanding of the importance of these operational criteria, and explore how
    to measure and manage them effectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能应用程序的操作方面，尤其是那些使用 GPT-4 等大型语言模型的应用程序，对于确保这些系统的平稳和高效运行至关重要。理解和管理关键操作指标，如令牌、延迟、每秒请求数（RPS）和首次字节到达时间（TTFB），对于优化性能、用户体验和成本至关重要。让我们来探讨这些操作标准的定义，更好地理解它们的重要性，并探讨如何有效地测量和管理它们。
- en: 11.5.1 Reliability and performance considerations
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 可靠性和性能注意事项
- en: 'Any production system, including the GenAI application, must be reliable and
    performant to meet the needs and expectations of your users. This means your system
    should be able to cope with different failures and scenarios. An API management
    or proxy system can assist you with many of these aspects, which we will discuss
    next:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 任何生产系统，包括通用人工智能应用程序，都必须可靠且性能良好，以满足用户的需求和期望。这意味着您的系统应该能够应对不同的故障和场景。API 管理或代理系统可以帮助您处理许多这些方面，我们将在下一部分讨论：
- en: '*Monitoring tools*—Utilize monitoring tools and services to measure these operational
    metrics continuously. Tools such as Prometheus ([https://prometheus.io](https://prometheus.io))
    for metric collection and Grafana ([https://grafana.com](https://grafana.com))
    for visualization can provide real-time insights into your application’s performance.
    Cloud providers also offer native monitoring solutions that can be employed.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控工具*—利用监控工具和服务持续测量这些操作指标。例如，Prometheus ([https://prometheus.io](https://prometheus.io))
    用于指标收集，Grafana ([https://grafana.com](https://grafana.com)) 用于可视化，可以提供对应用程序性能的实时洞察。云服务提供商还提供原生监控解决方案，可以加以利用。'
- en: '*Performance testing*—Regularly conduct performance testing to simulate various
    load conditions and measure how your application responds. Tools such as Apache
    JMeter ([https://jmeter.apache.org](https://jmeter.apache.org)) or Locust ([https://locust.io](https://locust.io))
    can simulate multiple users interacting with your application to assess its throughput
    and latency under stress.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能测试*—定期进行性能测试以模拟各种负载条件并测量应用程序的响应。例如，Apache JMeter ([https://jmeter.apache.org](https://jmeter.apache.org))
    或 Locust ([https://locust.io](https://locust.io)) 可以模拟多个用户与您的应用程序交互，以评估其在压力下的吞吐量和延迟。'
- en: '*Optimization techniques*—Implementing effective optimization techniques is
    crucial for overall application performance, resource utilization, and user experience:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化技术*—实施有效的优化技术对于整体应用程序性能、资源利用率和用户体验至关重要：'
- en: '*Token management*—Optimize the use of tokens by refining input prompts and
    responses. This can involve trimming unnecessary text, using more efficient encoding
    techniques, or customizing the model to produce shorter, more concise outputs
    without compromising quality.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*令牌管理*——通过优化输入提示和响应来优化令牌的使用。这可能包括修剪不必要的文本、使用更有效的编码技术或定制模型以生成更短、更简洁的输出，同时不牺牲质量。'
- en: '*Caching*—Implement caching strategies for frequently requested information
    to reduce latency and lower the computational load on your system. This is especially
    effective for static or rarely changing data.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缓存*——为频繁请求的信息实现缓存策略以减少延迟并降低系统计算负载。这对于静态或很少变化的数据特别有效。'
- en: '*Load balancing and auto-scaling*—Use load balancers to distribute traffic
    evenly across your infrastructure, and implement auto-scaling to adjust resources
    dynamically based on demand. This helps maintain low latency and high RPS by ensuring
    your system can handle spikes in traffic without manual intervention.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负载均衡和自动扩展*——使用负载均衡器在您的基础设施中均匀分配流量，并实现自动扩展以根据需求动态调整资源。这有助于通过确保系统可以处理流量峰值而无需手动干预，从而保持低延迟和高每秒请求数（RPS）。'
- en: '*Cost management*—Monitor and manage costs related to operational metrics,
    especially token usage, as this directly affects the cost of using LLM APIs. Implement
    quota systems or rate limiting if necessary to prevent unexpected spikes in usage.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本管理*——监控和管理与操作指标相关的成本，特别是令牌使用，因为这直接影响使用 LLM API 的成本。如有必要，实施配额系统或速率限制以防止使用量意外激增。'
- en: By focusing on these operational aspects and continuously monitoring and optimizing
    based on real-world data, developers can ensure that their GenAI applications
    are functional but also efficient, scalable, and cost-effective. This holistic
    approach to operational management is crucial for the success of any application
    using the power of LLMs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注这些操作方面，并基于实际数据持续监控和优化，开发者可以确保他们的生成式 AI 应用程序不仅功能齐全，而且高效、可扩展且成本效益高。这种对操作管理的整体方法对于任何利用
    LLM 力量的应用程序的成功至关重要。
- en: 11.5.2 Managed identities
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 管理标识
- en: Azure OpenAI has a key advantage over OpenAI or other LLM providers in terms
    of using managed identities for authentication. This method follows the best practices
    for enterprise production deployments, improving security and making credential
    management easier. Managed identities avoid the need to handle keys directly,
    lowering the chance of key exposure and simplifying the process of changing credentials.
    They also offer an automated way to authenticate services running on Azure with
    other Azure resources, using Azure Active Directory (AAD) for identity management
    (also known as Entra ID).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 在使用托管标识进行认证方面比 OpenAI 或其他 LLM 提供商具有关键优势。这种方法遵循企业生产部署的最佳实践，提高了安全性并简化了凭证管理。托管标识避免了直接处理密钥的需要，降低了密钥泄露的风险，并简化了更改凭证的过程。它们还提供了一种自动方式，使用
    Azure Active Directory（AAD）进行身份管理（也称为 Entra ID）来认证在 Azure 上运行的服务与其他 Azure 资源。
- en: When using managed identities with Azure OpenAI, enterprises have a couple of
    authentication methods available—RBAC and Entra ID. The former allows for more
    complex security scenarios and involves assigning roles (e.g., user or contributor)
    to enable API calls without key-based authentication. Conversely, the latter is
    used to authenticate our OpenAI resource using a bearer token obtained through
    the Azure CLI. It requires a custom subdomain name and is suitable for applications
    running on Azure services such as VMs, function apps, and VM scale sets.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure OpenAI 的托管标识时，企业有几种认证方法可供选择——基于角色的访问控制（RBAC）和 Entra ID。前者允许更复杂的网络安全场景，涉及分配角色（例如，用户或贡献者）以启用基于
    API 的调用而不需要基于密钥的认证。相反，后者用于使用通过 Azure CLI 获得的令牌来认证我们的 OpenAI 资源。它需要一个自定义子域名，适用于在
    Azure 服务（如 VM、函数应用和 VM 扩展集）上运行的应用程序。
- en: Managed identities offer several benefits over traditional key-based authentication
    methods, especially regarding security and management. Some of the key advantages
    are
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的基于密钥的认证方法相比，托管标识提供了几个优势，尤其是在安全和管理的方面。一些关键优势包括
- en: '*No need to manage credentials*—Managed identities eliminate the need for developers
    to manage the secrets, credentials, certificates, and keys used to secure communication
    between services.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无需管理凭证*——托管标识消除了开发者管理用于在服务之间安全通信的秘密、凭证、证书和密钥的需要。'
- en: '*Automatic credential rotation*—System-assigned managed identities are tied
    to the lifecycle of the Azure resource, and Azure automatically handles the lifecycle
    of the credentials, including their rotation.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动凭据轮换*——系统分配的托管标识与Azure资源的生命周期相关联，Azure自动处理凭据的生命周期，包括其轮换。'
- en: '*Enhanced security*—Since credentials are not stored in the code, there’s a
    reduced risk of credential leaks. Managed identities also use AAD for authentication,
    which is more secure than storing and managing keys within your application.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增强安全性*——由于凭据没有存储在代码中，因此降低了凭据泄露的风险。托管标识还使用AAD进行身份验证，这比在您的应用程序中存储和管理密钥更安全。'
- en: '*Simplified access management*—Managed identities can be granted access to
    other Azure resources supporting Azure AD authentication, simplifying access management.
    Furthermore, user-assigned managed identities can be used by multiple resources,
    which can be particularly useful for complex environments and applications that
    need to scale.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简化访问管理*——托管标识可以授予访问支持Azure AD身份验证的其他Azure资源的权限，从而简化了访问管理。此外，用户分配的托管标识可以被多个资源使用，这对于需要扩展的复杂环境和应用程序特别有用。'
- en: These benefits contribute to a more secure and efficient environment for managing
    access to Azure resources, making managed identities a preferred choice for many
    enterprise scenarios. The following listing shows a simple example of implementing
    a managed identity using Azure OpenAI. Note that this might require installing
    the Azure Identity package, which can be done via pip`:` `pip` `install azure-identity`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处有助于创建一个更安全和高效的环境来管理对Azure资源的访问，使托管标识成为许多企业场景的首选选择。以下列表展示了使用Azure OpenAI实现托管标识的简单示例。请注意，这可能需要安装Azure
    Identity包，可以通过pip`:` `pip` `install azure-identity`来完成。
- en: Listing 11.6 Using managed identities with Azure OpenAI
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.6 使用Azure OpenAI的托管标识
- en: '[PRE7]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 11.5.3 Caching
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.3 缓存
- en: Implementing caching when using OpenAI’s LLM in a production app is a strategic
    move to enhance performance and cost efficiency. Caching stores frequently requested
    data in a faster-access storage system, allowing for reduced latency, as repeated
    queries can be served swiftly. This improves user experience and minimizes operational
    costs by reducing the number of necessary API calls, often associated with fees.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产应用程序中使用OpenAI的LLM实现缓存是一种战略性的举措，旨在提高性能和成本效率。缓存将频繁请求的数据存储在快速访问的存储系统中，从而减少了延迟，因为重复查询可以迅速提供。这改善了用户体验并减少了运营成本，通过减少必要的API调用次数，这些调用通常与费用相关。
- en: Moreover, services typically impose rate limits to prevent excessive use, and
    caching helps us adhere to these limits while maintaining a responsive service.
    Regarding the best practices for caching with Redis, it’s crucial to design cache
    keys uniquely representing each request and its context. An effective invalidation
    strategy, such as setting a time-to-live (TTL) for keys, ensures the cache doesn’t
    serve outdated information.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，服务通常设置速率限制以防止过度使用，而缓存有助于我们遵守这些限制，同时保持服务的响应性。关于使用Redis进行缓存的最佳实践，设计唯一表示每个请求及其上下文的缓存键至关重要。一种有效的失效策略，例如为键设置生存时间（TTL），确保缓存不会提供过时信息。
- en: The cache-aside pattern is a recommended approach where the application checks
    the cache first and, upon a miss, retrieves data from the source, updates the
    cache, and then returns the response. Monitoring your cache’s hit rates and performance
    metrics is essential to gauge its effectiveness and make necessary optimizations.
    It’s important to handle cache misses gracefully and ensure the application can
    operate correctly even when temporarily unavailable.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存旁路模式是一种推荐的方法，其中应用程序首先检查缓存，如果发生未命中，则从源检索数据，更新缓存，然后返回响应。监控您缓存的命中率性能指标对于评估其有效性和进行必要的优化至关重要。优雅地处理缓存未命中并确保应用程序即使在暂时不可用的情况下也能正确运行是很重要的。
- en: We can illustrate how caching an LLM generation can benefit the application
    greatly in terms of cost and experience. However, we should not cache anything
    without a clear reason, hoping it will improve things, but consider it in the
    context of the use case and the related types of generations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说明缓存LLM生成如何从成本和体验方面极大地提高应用程序的效益。然而，我们不应在没有明确理由的情况下缓存任何内容，希望它能改善事情，而应将其放在用例和相关生成类型的背景下考虑。
- en: 'For our caching example, we will use Redis and build on that from our RAG implementation
    earlier in chapter 8\. Using the same Docker container, we will use the RedisVL
    library, a Python library designed for tasks like semantic search and real-time
    RAG pipelines. It provides an easy-to-use interface for vector-based searches
    and index management. RedisVL is built on the `redis-py` client and helps integrate
    Redis’ capabilities into AI-driven applications. We start by installing via pip:
    `pip` `install` `redisvl`.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的缓存示例中，我们将使用Redis，并在第8章早期我们的RAG实现的基础上构建。使用相同的Docker容器，我们将使用RedisVL库，这是一个为语义搜索和实时RAG管道等任务设计的Python库。它提供了一个易于使用的接口，用于基于向量的搜索和索引管理。RedisVL建立在`redis-py`客户端之上，并帮助将Redis的能力集成到AI驱动的应用中。我们首先通过pip安装：`pip
    install redisvl`。
- en: We continue by listing all the indexes in the Redis database, which only has
    one index, `posts`, from our RAG implementation earlier in chapter 8\.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续列出Redis数据库中的所有索引，其中只有一个索引`posts`，这是从第8章早期我们的RAG实现中得到的。
- en: 'The `rvl` `index` `listall` command to see all the indexes is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有索引的`rvl index listall`命令如下：
- en: '[PRE8]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we initialize the cache, which is created if the cache does not exist.
    The cache initialization requires some parameters—the name (case sensitive), the
    prefix for the hash entries, the connection string (local in our case, as we are
    running it in Docker locally), and the distance threshold. The distance threshold
    can vary depending on the embedding code and the use case and can be changed on
    the fly.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化缓存，如果缓存不存在，则会创建它。缓存初始化需要一些参数——名称（区分大小写）、散列条目的前缀、连接字符串（在我们的案例中是本地的，因为我们是在本地Docker中运行的），以及距离阈值。距离阈值可能取决于嵌入代码和用例，并且可以实时更改。
- en: Our function, `answer_question()`, takes a question and uses the `check()` method
    on the `llmcache` instance to search the question in the cache. If the cache has
    results, it gives back the response. If the cache is empty, it calls the `generate_response`
    function to get a response from the OpenAI client, which is then stored in the
    cache. Note that some of the code is skipped for simplicity. The following listing
    shows the whole thing.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能`answer_question()`接受一个问题，并在`llmcache`实例上使用`check()`方法在缓存中搜索该问题。如果缓存有结果，它将返回响应。如果缓存为空，它将调用`generate_response`函数从OpenAI客户端获取响应，然后将响应存储在缓存中。请注意，为了简化，一些代码被省略了。以下列表显示了整个流程。
- en: Listing 11.7 Using Redis cache for OpenAI response
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.7使用Redis缓存存储OpenAI响应
- en: '[PRE9]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Index name'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 索引名称'
- en: '#2 Redis key prefix for hash entries'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Redis散列条目键前缀'
- en: '#3 Redis connection url string'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Redis连接URL字符串'
- en: '#4 Semantic cache distance threshold'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 语义缓存距离阈值'
- en: When we run this, an example output is
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个时，一个示例输出是
- en: '[PRE10]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The TTL mechanism determines how long a piece of data should be stored in a
    cache before it’s considered stale and can be deleted. With Redis, once the TTL
    expires, the
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: TTL机制决定了数据在缓存中存储多长时间后被认为是过时的，可以被删除。在Redis中，一旦TTL过期，
- en: 'cached data is automatically removed, ensuring that outdated information isn’t
    served to users. This helps maintain the freshness of the data being accessed
    by the application. This can be set as follows: `llmcache.set_ttl(5)` `# 5` `seconds`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存数据会自动删除，确保不会向用户提供过时信息。这有助于保持应用程序访问的数据的新鲜度。这可以设置如下：`llmcache.set_ttl(5)` `#
    5` `seconds`。
- en: '![figure](../Images/CH11_F12_Bahree.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH11_F12_Bahree.png)'
- en: Figure 11.12 Redis cache statistics for `GenAIBookCache`
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12 `GenAIBookCache`的Redis缓存统计信息
- en: 'We can use the `rvl stats` command with the cache name as an argument to view
    the cache details. Figure 11.12 shows the output of this command: `rvl stats —i
    GenAIBookCache`.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`rvl stats`命令，将缓存名称作为参数来查看缓存详情。图11.12显示了此命令的输出：`rvl stats —i GenAIBookCache`。
- en: 'We have seen the components we must consider when making a GenAI application
    scalable and operational. There is one more topic to cover: LLMOps and MLOps.
    These are not just for getting AI applications to work; they’re for doing so in
    a maintainable, ethical, and scalable way. This is why they are regarded as vital
    for any enterprise that wants to use AI technology well. Let’s explore them more
    closely.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了在构建可扩展和可操作的GenAI应用时必须考虑的组件。还有一个主题需要讨论：LLMOps和MLOps。这些不仅仅是让AI应用工作；它们是为了以可维护、道德和可扩展的方式进行。这就是为什么它们被视为任何希望有效使用AI技术的企业的关键。让我们更深入地探讨它们。
- en: 11.6 LLMOps and MLOps
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 LLMOps和MLOps
- en: Machine learning operations (MLOps) apply DevOps principles and best practices
    to develop, deploy, and manage ML models and applications. MLOps aims to streamline
    the ML lifecycle, from data preparation and experimentation to model training
    and serving, while ensuring quality, reliability, and scalability.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习操作（MLOps）将DevOps原则和最佳实践应用于开发、部署和管理ML模型和应用。MLOps的目标是简化ML生命周期，从数据准备和实验到模型训练和服务的整个过程，同时确保质量、可靠性和可扩展性。
- en: LLMOps is a specialized domain within MLOps that focuses on the operational
    aspects of LLMs. LLMs are deep learning models that can generate natural language
    text and perform various natural language processing (NLP) tasks based on the
    input provided. Examples of LLMs include GPT-4, BERT, and similar advanced AI
    systems.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps是MLOps中的一个专业领域，专注于LLMs的运营方面。LLMs是深度学习模型，可以根据输入生成自然语言文本并执行各种自然语言处理（NLP）任务。LLMs的例子包括GPT-4、BERT以及类似的先进AI系统。
- en: LLMOps introduces tools and best practices that help manage the lifecycle of
    LLMs and LLM-powered applications, such as prompt engineering, fine-tuning, deployment,
    monitoring, and governance. LLMOps also addresses the unique challenges and risks
    associated with LLMs, such as bias, hallucination, prompt injection, and ethical
    concerns.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps引入了工具和最佳实践，帮助管理LLMs和LLM驱动的应用程序的生命周期，例如提示工程、微调、部署、监控和治理。LLMOps还解决了与LLMs相关的独特挑战和风险，例如偏差、幻觉、提示注入和伦理问题。
- en: Both LLMOps and MLOps share some common goals and challenges, such as automating
    and orchestrating the ML pipeline; ensuring reproducibility, traceability, and
    versioning of data, code, models, and experiments; monitoring and optimizing the
    performance, availability, and resource utilization of models and applications
    in production; implementing security, privacy, and compliance measures to protect
    data and models from unauthorized access and misuse; and incorporating feedback
    loops and continuous improvement cycles to update and refine models and applications
    based on changing requirements and user behavior.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps和MLOps有一些共同的目标和挑战，例如自动化和编排机器学习（ML）管道；确保数据、代码、模型和实验的可重复性、可追溯性和版本控制；监控和优化生产中模型和应用的性能、可用性和资源利用率；实施安全、隐私和合规措施以保护数据和模型免受未经授权的访问和滥用；以及通过反馈循环和持续改进周期根据不断变化的需求和用户行为更新和改进模型和应用。
- en: 'However, LLMOps and MLOps also have some distinct differences, and switching
    from MLOps to LLMOps is a paradigm shift—specifically in data, model complexity
    (including size), and model output in the context of generation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMOps和MLOps也有一些明显的区别，从MLOps切换到LLMOps是一种范式转变——特别是在生成上下文中的数据、模型复杂度（包括大小）和模型输出方面：
- en: '*Data*—LLMs are pretrained on massive text datasets, such as the Common Crawl
    corpus, and can be adapted for specific use cases using prompt engineering and
    fine-tuning techniques. This reduces the need for extensive data collection and
    labeling and introduces the risk of data leakage and contamination from the pretraining
    data.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据*——LLMs在大量文本数据集上进行预训练，例如Common Crawl语料库，可以使用提示工程和微调技术针对特定用例进行适配。这减少了大量数据收集和标注的需求，但也引入了从预训练数据中数据泄露和污染的风险。'
- en: '*Computational resources*—GenAI models, such as LLMs, are very large and complex,
    often consisting of billions of parameters and requiring specialized hardware
    and infrastructure to train and run, such as high-end GPUs, memory, and so forth.
    This poses significant challenges for model storage, distribution, inference,
    cost, and energy efficiency. This challenge is further amplified when we want
    to scale up to many users to handle incoming requests without compromising performance.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算资源*——生成AI模型，如LLMs，非常大且复杂，通常包含数十亿个参数，需要专门的硬件和基础设施进行训练和运行，例如高端GPU、内存等。这对模型存储、分发、推理、成本和能效提出了重大挑战。当我们想要扩展到许多用户以处理传入请求而不影响性能时，这一挑战进一步加剧。'
- en: '*Model generation*—LLMs are designed to generate coherent and contextually
    appropriate text rather than adhering to factual accuracy. This leads to various
    risks, such as bias amplification, hallucination, prompt injection, and ethical
    concerns. These risks require careful evaluation and mitigation strategies, such
    as responsible AI frameworks, human oversight, and explainability tools.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型生成*—LLMs被设计为生成连贯且上下文适当的文本，而不是遵循事实准确性。这导致各种风险，如偏差放大、幻觉、提示注入和道德关注。这些风险需要仔细评估和缓解策略，例如负责任的AI框架、人工监督和可解释性工具。'
- en: Table 11.4 outlines key differences in the shift to LLMOps from MLOps.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.4概述了从MLOps转向LLMOps的关键差异。
- en: Table 11.4 Differences between MLOps and LLMOps
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.4 MLOps和LLMOps之间的差异
- en: '| Area | Traditional MLOps | LLMOps |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 传统MLOps | LLMOps |'
- en: '| --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Target audience  | ML engineers, data scientists  | Application developers,
    ML engineering, and data scientists  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 目标受众 | ML工程师、数据科学家 | 应用开发者、ML工程和数据科学家 |'
- en: '| Components  | Model, data, inference environments, features  | LLMs, prompts,
    tokens, generations, APIs, embeddings, vector databases  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 模型、数据、推理环境、特征 | LLMs、提示、标记、生成、APIs、嵌入、向量数据库 |'
- en: '| Metrics  | Accuracy (F1 score, precision, recall, etc.)  | Quality (similarity),
    groundedness (accuracy), cost (tokens), latency, evaluations (Perplexity, BLEU,
    ROUGE, etc.)  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 准确度（F1分数、精确率、召回率等） | 质量（相似度）、扎根性（准确性）、成本（标记）、延迟、评估（困惑度、BLEU、ROUGE等）
    |'
- en: '| Models  | Typically built from scratch  | Typically, prebuilt with inference
    via an API and multiple versions in production simultaneously  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 通常从头开始构建 | 通常预先构建，通过API进行推理，同时在生产中同时拥有多个版本 |'
- en: '| Ethical concerns  | Bias in training data  | Misuse and generation of harmful,
    fake, and biased output  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 道德关注点 | 训练数据中的偏差 | 滥用和生成有害、虚假和有偏见的输出 |'
- en: Why LLMOps and MLOps?
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么需要LLMOps和MLOps？
- en: LLMOps and MLOps are key to the responsible and efficient deployment of LLMs
    and ML models, ensuring ethical and performance standards. They address problems
    such as slow development, inconsistent model quality, and high costs, while providing
    advantages such as speed, consistency, and risk management. LLMOps covers tools
    and practices for managing LLMs, including prompt engineering, fine-tuning, and
    governance, resulting in faster development, better quality, cost reduction, and
    risk control.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps和MLOps是负责和高效部署LLMs和ML模型的关键，确保道德和性能标准。它们解决诸如开发缓慢、模型质量不一致和高成本等问题，同时提供诸如速度、一致性和风险管理等优势。LLMOps涵盖了管理LLMs的工具和实践，包括提示工程、微调和治理，从而实现更快的发展、更好的质量、成本降低和风险控制。
- en: Given their complexity, effective management is critical for generative AI models’
    performance and cost efficiency. Important factors in LLMOps include model selection,
    deployment strategies, and version control. The right model size and configuration
    are essential, possibly customized to specific data. Options between cloud services
    and private infrastructure balance convenience and data security. Versioning and
    automated pipelines support smooth updates and rollbacks, enabling continuous
    integration and deployment. Adopting LLMOps ensures the successful, ethical use
    of generative AI, maximizing benefits and minimizing risks.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其复杂性，有效的管理对于生成AI模型的表现和成本效率至关重要。LLMOps中的重要因素包括模型选择、部署策略和版本控制。正确的模型大小和配置至关重要，可能需要针对特定数据进行定制。在云服务和私有基础设施之间进行选择，以平衡便利性和数据安全。版本控制和自动化管道支持平滑的更新和回滚，实现持续集成和部署。采用LLMOps确保生成AI的成功、道德使用，最大化利益并最小化风险。
- en: LLMOps and MLOps are crucial for the production deployment of AI applications.
    They provide the necessary infrastructure to ensure that AI applications are operational,
    sustainable, responsible, and capable of scaling according to user demand. For
    developers and technical professionals, these frameworks offer a way to maintain
    quality assurance, follow compliance and ethical standards, and cost-effectively
    manage AI applications. In an enterprise environment where reliability and scalability
    are vital, LLMOps and MLOps are essential for successfully integrating AI technology.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps和MLOps对于AI应用的量产部署至关重要。它们提供了必要的基础设施，以确保AI应用是可操作的、可持续的、负责任的，并且能够根据用户需求进行扩展。对于开发人员和技术人员来说，这些框架提供了一种保持质量保证、遵循合规和道德标准以及以成本效益管理AI应用的方法。在可靠性和可扩展性至关重要的企业环境中，LLMOps和MLOps对于成功整合AI技术至关重要。
- en: Monitoring and telemetry systems
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控和遥测系统
- en: While capable of delivering high-value business outcomes, powerful LLMs require
    careful monitoring and management to ensure optimal performance, accuracy, security,
    and user experience. Monitoring is an important part of LLMOps and MLOps, as it
    shows how well models and applications work in production. Continuous monitoring
    is vital for LLMOps, as for many production systems. It helps LLMOps teams solve
    problems quickly, ensuring the system is speedy and dependable. Monitoring covers
    performance metrics, such as response time, throughput, and resource utilization,
    enabling quick intervention if there are delays or performance declines. Telemetry
    tracking is crucial in this process, providing valuable insights into the model’s
    behavior and enabling continuous improvement.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管能够产生高价值的业务成果，但强大的 LLMs 需要仔细的监控和管理，以确保最佳性能、准确性、安全性和用户体验。监控是 LLMOps 和 MLOps
    的重要组成部分，因为它显示了模型和应用程序在生产中的工作情况。对于许多生产系统来说，持续监控至关重要，因为它有助于 LLMOps 团队快速解决问题，确保系统快速可靠。监控包括性能指标，如响应时间、吞吐量和资源利用率，如果出现延迟或性能下降，可以快速干预。遥测跟踪在这个过程中至关重要，它提供了关于模型行为的宝贵见解，并使持续改进成为可能。
- en: Moreover, ethical AI deployment must check for bias or harmful outputs. Using
    fairness-aware monitoring methods, LLMOps teams ensure that LLMs work ethically,
    minimizing unwanted biases and increasing user trust. Frequent model updates and
    maintenance, supported by automated pipelines, ensure that the LLM stays current
    with the latest developments and data trends, ensuring continued effectiveness
    and adaptability.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，伦理人工智能部署必须检查偏见或有害输出。使用公平性感知监控方法，LLMOps 团队确保 LLMs 以伦理方式工作，最小化不希望的偏见并增加用户信任。频繁的模型更新和维护，由自动化管道支持，确保
    LLM 与最新的发展和数据趋势保持同步，确保持续的有效性和适应性。
- en: 11.7 Checklist for production deployment
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 生产部署清单
- en: 'We covered many topics in this chapter. Before we end it, let’s summarize some
    of the advice into a simple checklist that can be handy as a reference guide when
    deploying applications to production. The following categories are the same as
    those described earlier in the chapter. Of course, as with most of this advice,
    this is incomplete and should be used as part of the wider set of responsibilities:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中讨论了许多主题。在我们结束之前，让我们将一些建议总结成一个简单的清单，这可以作为参考指南，在将应用程序部署到生产中时很有用。以下类别与本章前面描述的相同。当然，与大多数建议一样，这并不完整，应作为更广泛责任集的一部分使用：
- en: Scaling and deployment
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展和部署
- en: '*Assess computational resources*—Determine your generative AI models’ hardware
    and software requirements and ensure the infrastructure can support them effectively.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估计算资源*—确定您的生成式 AI 模型的硬件和软件需求，并确保基础设施能够有效地支持它们。'
- en: '*Quality and availability of data*—Implement robust data validation, quality
    control processes, and continuous monitoring to ensure data accuracy and relevance.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据和可用性质量*—实施稳健的数据验证、质量控制流程和持续监控，以确保数据准确性和相关性。'
- en: '*Model performance and reliability*—Set up regular testing and validation processes
    to monitor models’ performance. Plan for redundancy, failover, and disaster recovery
    to ensure high availability.'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型性能和可靠性*—建立定期的测试和验证流程来监控模型的性能。计划冗余、故障转移和灾难恢复以确保高可用性。'
- en: '*Security and compliance*—Apply encryption, access controls, and regular compliance
    audits. Ensure that your models adhere to regulations such as GDPR or HIPAA.'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全和合规性*—应用加密、访问控制和定期的合规性审计。确保您的模型遵守 GDPR 或 HIPAA 等规定。'
- en: '*Cost management*—Closely monitor and manage the costs of deploying and maintaining
    your models. Be prepared to make tradeoffs between cost and performance.'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本管理*—密切监控和管理部署和维护模型的成本。准备好在成本和性能之间做出权衡。'
- en: '*System integration*—Ensure that the generative AI models can be easily integrated
    into existing systems and workflows.'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统集成*—确保生成式 AI 模型可以轻松集成到现有系统和工作流程中。'
- en: '*Human in the loop*—Design the models to include human oversight and intervention
    where necessary.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工介入*—设计模型以包括必要时的人工监督和干预。'
- en: '*Ethical considerations*—When deploying your models, address ethical implications,
    such as bias and fairness.'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*伦理考量*—在部署模型时，考虑伦理影响，例如偏见和公平性。'
- en: Best practices for production deployment
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产部署的最佳实践
- en: '*Metrics for LLM inference*—Focus on key metrics such as time to first token
    (TTFT), time per output token (TPOT), latency, and throughput. Use tools such
    as MLflow to track these metrics.'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM推理的指标*—关注关键指标，如首次标记时间（TTFT）、每输出标记时间（TPOT）、延迟和吞吐量。使用MLflow等工具来跟踪这些指标。'
- en: '*Manage latency*—Understand different latency points, and measure them accurately.
    Consider the influence of prompt size and model size on latency.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理延迟*—了解不同的延迟点，并准确测量它们。考虑提示大小和模型大小对延迟的影响。'
- en: '*Scalability*—Utilize PTUs and PAYGO models to scale your application effectively.
    Use API management for queuing, rate throttling, and managing usage quotas.'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*—利用PTUs和PAYGO模型有效地扩展您的应用程序。使用API管理进行排队、速率限制和管理使用配额。'
- en: '*Quotas and rate limits*—Implement strategies to manage quotas and rate limits
    effectively, including understanding your limits, monitoring usage, and implementing
    retry logic.'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*配额和速率限制*—实施有效管理配额和速率限制的策略，包括了解您的限制、监控使用情况并实施重试逻辑。'
- en: '*Observability*—Use tools such as MLflow, Traceloop, and Prompt flow to monitor,
    log, and trace your application for improved performance and user experience.'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可观察性*—使用MLflow、Traceloop和Prompt flow等工具来监控、记录和跟踪您的应用程序，以改善性能和用户体验。'
- en: '*Security and compliance*—Encrypt data, control access, conduct compliance
    audits, and deploy anomaly detection systems.'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全和合规性*—加密数据、控制访问、进行合规性审计并部署异常检测系统。'
- en: LLMOps and MLOps
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMOps和MLOps
- en: '*Adopt LLMOps and MLOps frameworks*—Ensure that your application follows best
    practices in LLMOps and MLOps for maintainable, ethical, and scalable AI solutions.'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*采用LLMOps和MLOps框架*—确保您的应用程序遵循LLMOps和MLOps的最佳实践，以实现可维护的、道德的且可扩展的AI解决方案。'
- en: '*Monitoring and telemetry systems*—Use fairness-aware monitoring methods and
    telemetry tracking to ensure ethical AI deployment and continuous improvement
    of your models.'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控和遥测系统*—使用公平感知的监控方法和遥测跟踪来确保道德AI部署和模型持续改进。'
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative AI models are complex and resource intensive, requiring careful consideration
    of data quality, performance, security, cost, and ethical implications.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI模型复杂且资源密集，需要仔细考虑数据质量、性能、安全性、成本和道德影响。
- en: 'For any production deployments, we must follow several best practices: monitor
    key metrics, optimize latency, ensure scalability, implement observability tools,
    prioritize security and compliance, and employ managed identities and caching.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何生产部署，我们必须遵循以下最佳实践：监控关键指标、优化延迟、确保可扩展性、实施可观察性工具、优先考虑安全和合规性，以及使用托管身份和缓存。
- en: For observability, we implement monitoring, logging, and tracing tools such
    as MLflow, Traceloop, and Prompt flow to understand model behavior, diagnose problems,
    and improve user experience.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了可观察性，我们实施了监控、记录和跟踪工具，如MLflow、Traceloop和Prompt flow，以了解模型行为、诊断问题并改善用户体验。
- en: LLMOps is a specialized domain within MLOps that focuses on managing the unique
    challenges and risks of LLMs. Both share common goals such as automation, reproducibility,
    monitoring, and security but differ in data requirements, model complexity, and
    output characteristics. LLMOps addresses unique challenges such as bias, hallucination,
    and ethical concerns associated with LLMs.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMOps是MLOps中的一个专业领域，专注于管理LLMs的独特挑战和风险。两者都共享共同的目标，如自动化、可重复性、监控和安全，但在数据需求、模型复杂性和输出特征上有所不同。LLMOps解决与LLMs相关的独特挑战，如偏差、幻觉和道德问题。
