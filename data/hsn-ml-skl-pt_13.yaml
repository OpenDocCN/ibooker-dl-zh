- en: Chapter 11\. Training Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章\. 深度神经网络的训练
- en: 'In [Chapter 10](ch10.html#pytorch_chapter) you built, trained, and fine-tuned
    several artificial neural networks using PyTorch. But they were shallow nets with
    just a few hidden layers. What if you need to tackle a complex problem, such as
    detecting hundreds of types of objects in high-resolution images? You may need
    to train a much deeper ANN, perhaps with dozens or even hundreds of layers, each
    containing hundreds of neurons, linked by hundreds of thousands of connections.
    Training a deep neural network isn’t a walk in the park. Here are some of the
    problems you could run into:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#pytorch_chapter)中，你使用PyTorch构建、训练和微调了几个人工神经网络。但它们只是具有几个隐藏层的浅层网络。如果你需要解决一个复杂的问题，比如在高分辨率图像中检测数百种类型的对象，你可能需要训练一个深度更深的人工神经网络，可能包含数十甚至数百层，每层包含数百个神经元，通过数十万个连接相互连接。训练深度神经网络并非易事。以下是一些你可能遇到的问题：
- en: You may be faced with the problem of gradients growing ever smaller or larger
    when flowing backward through the DNN during training. Both of these problems
    make lower layers very hard to train.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能会遇到在训练过程中，当反向传播通过深度神经网络时梯度不断变小或变大的问题。这两个问题都使得训练较低层变得非常困难。
- en: You might not have enough training data for such a large network, or it might
    be too costly to label.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能没有足够的训练数据来训练如此大的网络，或者标注成本可能过高。
- en: Training may be extremely slow.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能非常缓慢。
- en: A model with millions of parameters risks severely overfitting the training
    set, especially if there are not enough training instances or if they are too
    noisy.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有数百万个参数的模型可能会严重过拟合训练集，尤其是如果没有足够的训练实例或者它们太嘈杂的情况下。
- en: 'In this chapter we will go through each of these problems and present various
    techniques to solve them. We will start by exploring the vanishing and exploding
    gradients problems and some of their most popular solutions, including smart weight
    initialization, better activation functions, batch-norm, layer-norm, and gradient
    clipping. Next, we will look at transfer learning and unsupervised pretraining,
    which can help you tackle complex tasks even when you have little labeled data.
    Then we will discuss a variety of optimizers that can speed up training large
    models tremendously. We will also discuss how you can tweak the learning rate
    during training to speed up training and produce better models. Finally, we will
    cover a few popular regularization techniques for large neural networks: ℓ[1]
    and ℓ[2] regularization, dropout, Monte Carlo dropout, and max-norm regularization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐一探讨这些问题，并介绍各种解决方法。我们将首先探讨梯度消失和爆炸问题及其最流行的解决方案，包括智能权重初始化、更好的激活函数、批归一化、层归一化和梯度裁剪。接下来，我们将探讨迁移学习和无监督预训练，这可以帮助你在只有少量标记数据的情况下解决复杂任务。然后，我们将讨论各种优化器，它们可以极大地加速大型模型的训练。我们还将讨论如何在训练过程中调整学习率以加快训练速度并产生更好的模型。最后，我们将介绍一些适用于大型神经网络的流行正则化技术：ℓ[1]和ℓ[2]正则化、dropout、蒙特卡洛dropout和最大范数正则化。
- en: With these tools, you will be able to train all sorts of deep nets. Welcome
    to *deep* learning!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工具，你将能够训练各种深度网络。欢迎来到*深度学习*！
- en: The Vanishing/Exploding Gradients Problems
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失/爆炸问题
- en: As discussed in [Chapter 9](ch09.html#ann_chapter), the backpropagation algorithm’s
    second phase works by going from the output layer to the input layer, propagating
    the error gradient along the way. Once the algorithm has computed the gradient
    of the cost function with regard to each parameter in the network, it uses these
    gradients to update each parameter with a gradient descent step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在[第9章](ch09.html#ann_chapter)中讨论的那样，反向传播算法的第二阶段是通过从输出层到输入层，沿着路径传播误差梯度来工作的。一旦算法计算了网络中每个参数相对于成本函数梯度的值，它就会使用这些梯度通过梯度下降步骤来更新每个参数。
- en: 'Unfortunately, gradients often get smaller and smaller as the algorithm progresses
    down to the lower layers. As a result, the gradient descent update leaves the
    lower layers’ connection weights virtually unchanged, and training never converges
    to a good solution. This is called the *vanishing gradients* problem. In some
    cases, the opposite can happen: the gradients can grow bigger and bigger until
    layers get insanely large weight updates and the algorithm diverges. This is the
    *exploding gradients* problem, which surfaces most often in recurrent neural networks
    (see [Chapter 13](ch13.html#rnn_chapter)). More generally, deep neural networks
    suffer from unstable gradients; different layers may learn at widely different
    speeds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，随着算法向底层推进，梯度往往会越来越小。结果，梯度下降更新几乎不会改变底层连接权重，训练永远不会收敛到一个好的解。这被称为*梯度消失*问题。在某些情况下，情况可能相反：梯度会越来越大，直到层得到极端巨大的权重更新，算法发散。这被称为*梯度爆炸*问题，在循环神经网络（见第13章）中最常见。更普遍地说，深度神经网络受到不稳定梯度的困扰；不同的层可能以非常不同的速度学习。
- en: This unfortunate behavior was empirically observed long ago, and it was one
    of the reasons deep neural networks were mostly abandoned in the early 2000s.
    It wasn’t clear what caused the gradients to be so unstable when training a DNN,
    but some light was shed in a [2010 paper](https://homl.info/47) by Xavier Glorot
    and Yoshua Bengio.⁠^([1](ch11.html#id2449)) The authors found a few suspects,
    including the combination of the popular sigmoid (logistic) activation function
    and the weight initialization technique that was most popular at the time (i.e.,
    a normal distribution with a mean of 0 and a standard deviation of 1). In short,
    they showed that with this activation function and this initialization scheme,
    the variance of the outputs of each layer is much greater than the variance of
    its inputs. Going forward in the network, the variance keeps increasing after
    each layer until the activation function saturates at the top layers. This saturation
    is actually made worse by the fact that the sigmoid function has a mean of 0.5,
    not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better
    than the sigmoid function in deep networks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不幸的行为很久以前就被观察到了，这也是深度神经网络在2000年代初大部分被放弃的原因之一。当训练深度神经网络时，不清楚是什么导致了梯度的如此不稳定，但在2010年的一篇论文（https://homl.info/47）中，Xavier
    Glorot和Yoshua Bengio提供了一些线索。⁠^([1](ch11.html#id2449)) 作者发现了一些嫌疑人，包括当时最受欢迎的sigmoid（逻辑）激活函数和最流行的权重初始化技术（即均值为0，标准差为1的正态分布）。简而言之，他们表明，使用这种激活函数和这种初始化方案，每一层的输出方差远大于其输入的方差。在网络中向前推进时，方差在每一层之后都会增加，直到激活函数在顶层饱和。这种饱和实际上因为sigmoid函数的均值为0.5而不是0（双曲正切函数的均值为0，在深度网络中表现略好于sigmoid函数）而变得更糟。
- en: Looking at the sigmoid activation function (see [Figure 11-1](#sigmoid_saturation_plot)),
    you can see that when inputs become large (negative or positive), the function
    saturates at 0 or 1, with a derivative extremely close to 0 (i.e., the curve is
    flat at both extremes). Thus, when backpropagation kicks in it has virtually no
    gradient to propagate back through the network, and what little gradient exists
    keeps getting diluted as backpropagation progresses down through the top layers,
    so there is really nothing left for the lower layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 观察sigmoid激活函数（见图11-1），你可以看到当输入变得很大（负数或正数）时，函数在0或1处饱和，导数非常接近0（即曲线在两端都是平的）。因此，当反向传播开始时，它几乎没有任何梯度可以传播回网络，而存在的少量梯度在反向传播通过顶层的过程中不断被稀释，所以底层实际上什么也没有剩下。
- en: '![Diagram illustrating the sigmoid activation function, showing how it saturates
    at 0 for large negative inputs and 1 for large positive inputs, with a linear
    region in the middle.](assets/hmls_1101.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![说明sigmoid激活函数的图表，显示它在大负输入时饱和于0，在大正输入时饱和于1，中间有一个线性区域。](assets/hmls_1101.png)'
- en: Figure 11-1\. Sigmoid activation function saturation
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. Sigmoid激活函数饱和
- en: Glorot Initialization and He Initialization
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glorot初始化和He初始化
- en: 'In their paper, Glorot and Bengio proposed a way to significantly alleviate
    the unstable gradients problem. They pointed out that we need the signal to flow
    properly in both directions: in the forward direction when making predictions,
    and in the reverse direction when backpropagating gradients. We don’t want the
    signal to die out, nor do we want it to explode and saturate. For the signal to
    flow properly, the authors argued that we need the variance of the outputs of
    each layer to be equal to the variance of its inputs,⁠^([2](ch11.html#id2451))
    and we need the gradients to have equal variance before and after flowing through
    a layer in the reverse direction (please check out the paper if you are interested
    in the mathematical details). It is actually not possible to guarantee both unless
    the layer has an equal number of inputs and outputs (these numbers are called
    the *fan-in* and *fan-out* of the layer), but Glorot and Bengio proposed a good
    compromise that has proven to work very well in practice: the connection weights
    of each layer must be initialized randomly, as described in [Equation 11-1](#xavier_initialization_equation),
    where *fan*[avg] = (*fan*[in] + *fan*[out]) / 2\. This initialization strategy
    is called *Xavier initialization* or *Glorot initialization*, after the paper’s
    first author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，Glorot 和 Bengio 提出了一种显著缓解不稳定梯度问题的方法。他们指出，我们需要信号在两个方向上正确流动：在预测时的前向方向，以及在反向传播梯度时的反向方向。我们不希望信号衰减，也不希望它爆炸并饱和。为了使信号正确流动，作者们认为我们需要每一层的输出方差等于其输入方差，⁠^([2](ch11.html#id2451))
    并且我们需要在反向方向通过一层之前和之后，梯度具有相同的方差（如果您对数学细节感兴趣，请查看论文）。实际上，除非层有相等数量的输入和输出（这些数字被称为层的
    *fan-in* 和 *fan-out*），否则无法保证两者都成立。但 Glorot 和 Bengio 提出了一个在实践中证明非常有效的良好折衷方案：每一层的连接权重必须随机初始化，如
    [方程式 11-1](#xavier_initialization_equation) 所述，其中 *fan*[avg] = (*fan*[in] + *fan*[out])
    / 2。这种初始化策略被称为 *Xavier 初始化* 或 *Glorot 初始化*，以论文的第一作者命名。
- en: Equation 11-1\. Glorot initialization (when using the sigmoid activation function)
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 11-1\. Glorot 初始化（当使用 sigmoid 激活函数时）
- en: <mtable columnalign="left"><mtr><mtd><mtext>Normal distribution with mean 0
    and variance </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or
    a uniform distribution between </mtext><mo>-</mo><mi>r</mi><mtext> and </mtext><mo>+</mo><mi>r</mi><mtext>,
    with </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable columnalign="left"><mtr><mtd><mtext>均值为 0 且方差为 </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>或介于 </mtext><mo>-</mo><mi>r</mi><mtext> 和 </mtext><mo>+</mo><mi>r</mi><mtext> 之间的均匀分布，其中 </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable>
- en: 'If you replace *fan*[avg] with *fan*[in] in [Equation 11-1](#xavier_initialization_equation),
    you get an initialization strategy that Yann LeCun proposed in the 1990s. He called
    it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended
    it in their 1998 book *Neural Networks: Tricks of the Trade* (Springer). LeCun
    initialization is equivalent to Glorot initialization when *fan*[in] = *fan*[out].
    It took over a decade for researchers to realize how important this trick is.
    Using Glorot initialization can speed up training considerably, and it is one
    of the tricks that led to the success of deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您将 [方程式 11-1](#xavier_initialization_equation) 中的 *fan*[avg] 替换为 *fan*[in]，您将得到
    Yann LeCun 在 1990 年代提出的一种初始化策略。他称之为 *LeCun 初始化*。Genevieve Orr 和 Klaus-Robert Müller
    在他们的 1998 年书籍 *Neural Networks: Tricks of the Trade* (Springer) 中甚至推荐了它。当 *fan*[in]
    = *fan*[out] 时，LeCun 初始化与 Glorot 初始化等价。研究人员花了十多年时间才意识到这个技巧的重要性。使用 Glorot 初始化可以显著加快训练速度，这是导致深度学习成功的关键技巧之一。'
- en: Some papers have provided similar strategies for different activation functions,
    most notably a [2015 paper by Kaiming He et al](https://homl.info/48).⁠^([3](ch11.html#id2456))
    These strategies differ only by the scale of the variance and whether they use
    *fan*[avg] or *fan*[in], as shown in [Table 11-1](#initialization_table) (for
    the uniform distribution, just use <mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt>).
    The initialization strategy proposed for the ReLU activation function and its
    variants is called *He initialization* or *Kaiming initialization*, after the
    paper’s first author. For SELU, use Yann LeCun’s initialization method, preferably
    with a normal distribution. We will cover all these activation functions shortly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文为不同的激活函数提供了类似的策略，最著名的是 Kaiming He 等人于 2015 年发表的一篇论文[2015 paper by Kaiming
    He et al](https://homl.info/48)。⁠^([3](ch11.html#id2456)) 这些策略仅在方差缩放的比例以及是否使用
    *fan*[avg] 或 *fan*[in] 方面有所不同，如 [表 11-1](#initialization_table) 所示（对于均匀分布，只需使用
    <mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt>）。为
    ReLU 激活函数及其变体提出的初始化策略被称为 *He 初始化* 或 *Kaiming 初始化*，以论文的第一作者命名。对于 SELU，使用 Yann LeCun
    的初始化方法，最好使用正态分布。我们将在稍后介绍所有这些激活函数。
- en: Table 11-1\. Initialization parameters for each type of activation function
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11-1\. 每种激活函数类型的初始化参数
- en: '| Initialization | Activation functions | *σ*² (Normal) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 初始化 | 激活函数 | *σ*² (正态) |'
- en: '| --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Xavier Glorot | None, tanh, sigmoid, softmax | 1 / *fan*[avg] |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Xavier Glorot | 无，tanh，sigmoid，softmax | 1 / *fan*[avg] |'
- en: '| Kaiming He | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish, SwiGLU, ReLU² | 2
    / *fan*[in] |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Kaiming He | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish, SwiGLU, ReLU² | 2
    / *fan*[in] |'
- en: '| Yann LeCun | SELU | 1 / *fan*[in] |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Yann LeCun | SELU | 1 / *fan*[in] |'
- en: 'For historical reasons, PyTorch’s `nn.Linear` module initializes its weights
    using Kaiming uniform initialization, except the weights are scaled down by a
    factor of $StartRoot 6 EndRoot$ (and the bias terms are also initialized randomly).
    Sadly, this is not the optimal scale for any common activation function.⁠^([4](ch11.html#id2457))
    One solution is to simply multiply the weights by $StartRoot 6 EndRoot$ (i.e.,
    6^(0.5)) just after creating the `nn.Linear` layer to get proper Kaiming initialization.
    To do this, we can update the parameter’s `data` attribute. We will also zero
    out the biases, as there’s no benefit in randomly initializing them:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于历史原因，PyTorch 的 `nn.Linear` 模块使用 Kaiming 均匀初始化来初始化其权重，但权重会按 $StartRoot 6 EndRoot$（即
    6 的 0.5 次方）的比例缩小（并且偏置项也会随机初始化）。遗憾的是，这并不是任何常见激活函数的最佳缩放比例。一种解决方案是在创建 `nn.Linear`
    层后立即将权重乘以 $StartRoot 6 EndRoot$（即 6 的 0.5 次方），以获得适当的 Kaiming 初始化。为此，我们可以更新参数的
    `data` 属性。我们还将清零偏置，因为没有随机初始化它们的任何好处：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This works, but it’s clearer and less error-prone to use one of the initialization
    functions available in the `torch.nn.init` module:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以工作，但使用 `torch.nn.init` 模块中可用的初始化函数之一会更清晰且更不容易出错：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to apply the same initialization method to the weights of every
    `nn.Linear` layer in a model, you can do so in the model’s constructor, after
    creating each `nn.Linear` layer. Alternatively, you can write a subclass of the
    `nn.Linear` class and tweak its constructor to initialize the weights as you wish.
    But arguably the simplest option is to write a little function that takes a module,
    checks whether it’s an instance of the `nn.Linear` class, and if so, applies the
    desired initialization function to its weights. You can then apply this function
    to the model and all of its submodules by passing it to the model’s `apply()`
    method. For example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要将相同的初始化方法应用到模型中每个 `nn.Linear` 层的权重上，你可以在创建每个 `nn.Linear` 层之后在模型的构造函数中这样做。或者，你可以编写
    `nn.Linear` 类的子类，并调整其构造函数以按你的意愿初始化权重。但可能最简单的方法是编写一个小的函数，该函数接受一个模块，检查它是否是 `nn.Linear`
    类的实例，如果是，则对其权重应用所需的初始化函数。然后，你可以通过将其传递给模型的 `apply()` 方法来将此函数应用到模型及其所有子模块。例如：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `torch.nn.init` module also contains an `orthogonal_()` function which
    initializes the weights using a random orthogonal matrix, as proposed in a [2014
    paper](https://homl.info/ortho-init) by Andrew Saxe et al.⁠^([5](ch11.html#id2461))
    Orthogonal matrices have a number of useful mathematical properties, including
    the fact that they preserve norms: given an orthogonal matrix **W** and an input
    vector **x**, the norm of **Wx** is equal to the norm of **x**, and therefore
    the magnitude of the inputs is preserved in the outputs. When the inputs are standardized,
    this results in a stable variance through the layer, which prevents the activations
    and gradients from vanishing or exploding in a deep network (at least at the beginning
    of training). This initialization technique is much less common than the initialization
    techniques discussed earlier, but it can work well in recurrent neural nets ([Chapter 13](ch13.html#rnn_chapter))
    or generative adversarial networks ([Chapter 18](ch18.html#autoencoders_chapter)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.init`模块还包含一个`orthogonal_()`函数，它使用随机正交矩阵初始化权重，正如Andrew Saxe等人在2014年的一篇论文[2014
    paper](https://homl.info/ortho-init)中提出的。正交矩阵具有许多有用的数学性质，包括它们保持范数的事实：给定一个正交矩阵**W**和一个输入向量**x**，**Wx**的范数等于**x**的范数，因此输入的大小在输出中得到了保留。当输入被标准化时，这会在层中产生稳定的方差，从而防止激活和梯度在深度网络中消失或爆炸（至少在训练开始时）。这种初始化技术比前面讨论的初始化技术要少见得多，但它可以在循环神经网络（[第13章](ch13.html#rnn_chapter)）或生成对抗网络（[第18章](ch18.html#autoencoders_chapter)）中工作得很好。'
- en: And that’s it! Scaling the weights properly will give a deep neural net a much
    better starting point for training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！适当地缩放权重将为深度神经网络提供一个更好的训练起点。
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In a classifier, it’s generally a good idea to scale down the weights of the
    output layer during initialization (e.g., by a factor of 10). Indeed, this will
    result in smaller logits at the beginning of training, which means they will be
    closer together, and hence the estimated probabilities will also be closer together.
    In other words, it encourages the model to be less confident about its predictions
    when training starts: this will avoid extreme losses and huge gradients that can
    often make the model’s weights bounce around randomly at the start of training,
    losing time and potentially preventing the model from learning anything.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器中，初始化时降低输出层的权重通常是一个好主意（例如，乘以10）。事实上，这将导致训练开始时的logits更小，这意味着它们将更接近，因此估计的概率也将更接近。换句话说，这鼓励模型在训练开始时对其预测不太自信：这将避免极端损失和巨大的梯度，这些梯度常常会导致模型在训练开始时权重随机跳动，浪费时间和可能阻止模型学习任何东西。
- en: Better Activation Functions
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的激活函数
- en: One of the insights in the 2010 paper by Glorot and Bengio was that the problems
    with unstable gradients were in part due to a poor choice of activation function.
    Until then most people had assumed that if Mother Nature had chosen to use something
    pretty close to sigmoid activation functions in biological neurons, they must
    be an excellent choice. But it turns out that other activation functions behave
    much better in deep neural networks—in particular, the ReLU activation function,
    mostly because it does not saturate for positive values, and also because it is
    very fast to compute.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Glorot和Bengio在2010年的论文中提出的一个见解是，梯度不稳定的问题部分原因是激活函数的选择不当。在此之前，大多数人认为如果大自然选择在生物神经元中使用接近sigmoid激活函数的东西，那么它们肯定是一个很好的选择。但事实表明，其他激活函数在深度神经网络中表现更好——特别是ReLU激活函数，主要是因为它对于正值不会饱和，而且计算速度非常快。
- en: 'Unfortunately, the ReLU activation function is not perfect. It suffers from
    a problem known as the *dying ReLUs*: during training, some neurons effectively
    “die”, meaning they stop outputting anything other than 0\. In some cases, you
    may find that half of your network’s neurons are dead, especially if you used
    a large learning rate. A neuron dies when its weights get tweaked in such a way
    that the input of the ReLU function (i.e., the weighted sum of the neuron’s inputs
    plus its bias term) is negative for all instances in the training set. When this
    happens, it just keeps outputting zeros, and gradient descent does not affect
    it anymore because the gradient of the ReLU function is zero when its input is
    negative.⁠^([6](ch11.html#id2473))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，ReLU激活函数并不完美。它存在一个被称为*渐逝ReLU*的问题：在训练过程中，一些神经元实际上“死亡”，意味着它们停止输出除了0以外的任何内容。在某些情况下，你可能发现你网络中一半的神经元都“死亡”了，尤其是如果你使用了较大的学习率。当一个神经元的权重被调整到使得ReLU函数的输入（即神经元输入的加权和加上其偏置项）对于训练集中的所有实例都是负值时，神经元就会“死亡”。当这种情况发生时，它只会持续输出0，因为当ReLU函数的输入为负值时，其梯度为0，所以梯度下降不再影响它。⁠^([6](ch11.html#id2473))
- en: To solve this problem, you may want to use a variant of the ReLU function, such
    as the *leaky ReLU*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可能想要使用ReLU函数的变体，例如*渗漏ReLU*。
- en: Leaky ReLU
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渗漏ReLU
- en: 'The leaky ReLU activation function is defined as LeakyReLU[*α*](*z*) = max(*αz*,
    *z*) (see [Figure 11-2](#leaky_relu_plot)). The hyperparameter *α* defines how
    much the function “leaks”: it is the slope of the function for *z* < 0\. Having
    a slope for *z* < 0 ensures that leaky ReLUs never actually die; they can go into
    a long coma, but they have a chance to eventually wake up. A [2015 paper](https://homl.info/49)
    by Bing Xu et al.⁠^([7](ch11.html#id2477)) compared several variants of the ReLU
    activation function, and one of its conclusions was that the leaky variants always
    outperformed the strict ReLU activation function. In fact, setting *α* = 0.2 (a
    huge leak) seemed to result in better performance than *α* = 0.01 (a small leak).
    The paper also evaluated the *randomized leaky ReLU* (RReLU), where *α* is picked
    randomly in a given range during training and is fixed to an average value during
    testing. RReLU also performed fairly well and seemed to act as a regularizer,
    reducing the risk of overfitting. Finally, the paper evaluated the *parametric
    leaky ReLU* (PReLU), where *α* is authorized to be learned during training: instead
    of being a hyperparameter, it becomes a parameter that can be modified by backpropagation
    like any other parameter. PReLU was reported to strongly outperform ReLU on large
    image datasets, but on smaller datasets it runs the risk of overfitting the training
    set.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 渗漏ReLU激活函数定义为LeakyReLU[*α*](*z*) = max(*αz*, *z*)（见[图11-2](#leaky_relu_plot)）。超参数*α*定义了函数“渗漏”的程度：它是函数在*z*
    < 0时的斜率。对于*z* < 0有斜率确保了渗漏ReLU永远不会真正“死亡”；它们可以进入长时间的昏迷，但最终有苏醒的机会。Bing Xu等人于2015年发表的一篇[2015论文](https://homl.info/49)⁠^([7](ch11.html#id2477))比较了ReLU激活函数的几个变体，其结论之一是渗漏变体总是优于严格的ReLU激活函数。实际上，将*α*设置为0.2（巨大的渗漏）似乎比*α*
    = 0.01（小的渗漏）有更好的性能。该论文还评估了*随机渗漏ReLU*（RReLU），其中*α*在训练期间随机选择一个范围内的值，并在测试期间固定为平均值。RReLU也表现相当不错，似乎起到了正则化的作用，减少了过拟合的风险。最后，论文评估了*参数化渗漏ReLU*（PReLU），其中*α*被允许在训练期间学习：它不再是超参数，而成为了一个可以通过反向传播修改的参数。PReLU据报道在大型图像数据集上显著优于ReLU，但在较小的数据集上存在过拟合训练集的风险。
- en: '![Diagram of the leaky ReLU activation function illustrating its slope for
    negative values, demonstrating the "leak."](assets/hmls_1102.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![渗漏ReLU激活函数的示意图，说明了其负值的斜率，展示了“渗漏”现象。](assets/hmls_1102.png)'
- en: 'Figure 11-2\. Leaky ReLU: like ReLU, but with a small slope for negative values'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. 渗漏ReLU：与ReLU类似，但负值具有小的斜率
- en: 'As you might expect, PyTorch includes modules for each of these activation
    functions: `nn.LeakyReLU`, `nn.RReLU`, and `nn.PReLU`. Just like for other ReLU
    variants, you should use these along with Kaiming initialization, but the variance
    should be slightly smaller due to the negative slope: it should be scaled down
    by a factor of 1 + *α*². PyTorch supports this: you can pass the *α* hyperparameter
    to the `kaiming_uniform_()` and `kaiming_normal_()` functions, along with `nonlinearity="leaky_relu"`
    to get the appropriately adjusted Kaiming initialization:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，PyTorch 包含每个这些激活函数的模块：`nn.LeakyReLU`、`nn.RReLU` 和 `nn.PReLU`。就像其他 ReLU
    变体一样，你应该与 Kaiming 初始化一起使用这些模块，但由于负斜率，方差应该略小：它应该通过一个因子 1 + *α*² 缩放。PyTorch 支持：你可以将
    *α* 超参数传递给 `kaiming_uniform_()` 和 `kaiming_normal_()` 函数，以及 `nonlinearity="leaky_relu"`
    以获得适当的调整 Kaiming 初始化：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth
    functions: their slopes abruptly change at *z* = 0\. As we saw in [Chapter 4](ch04.html#linear_models_chapter)
    when we discussed lasso, this sort of discontinuity in the derivatives can make
    gradient descent bounce around the optimum and slow down convergence. So now we
    will look at some smooth variants of the ReLU activation function, starting with
    ELU and SELU.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU、leaky ReLU 和 PReLU 都存在它们不是平滑函数的事实：它们的斜率在 *z* = 0 处突然改变。正如我们在 [第 4 章](ch04.html#linear_models_chapter)
    中讨论 lasso 时所看到的，这种导数中的不连续性可以使梯度下降在最优解周围弹跳，并减慢收敛速度。因此，现在我们将查看 ReLU 激活函数的一些平滑变体，从
    ELU 和 SELU 开始。
- en: ELU and SELU
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELU 和 SELU
- en: 'In 2015, a [paper](https://homl.info/50) by Djork-Arné Clevert et al.⁠^([8](ch11.html#id2492))
    proposed a new activation function, called the *exponential linear unit* (ELU),
    that outperformed all the ReLU variants in the authors’ experiments: training
    time was reduced, and the neural network performed better on the test set. [Equation
    11-2](#elu_activation_equation) shows this activation function’s definition.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年，Djork-Arné Clevert 等人发表了一篇 [论文](https://homl.info/50)⁠^([8](ch11.html#id2492))，提出了一种新的激活函数，称为
    *指数线性单元* (ELU)，在作者的实验中优于所有 ReLU 变体：训练时间减少，神经网络在测试集上的表现更好。[方程 11-2](#elu_activation_equation)
    展示了这个激活函数的定义。
- en: Equation 11-2\. ELU activation function
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11-2\. ELU 激活函数
- en: <mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mi>α</mi> <mo>(</mo> <mtext>exp</mtext> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mi>α</mi> <mo>(</mo> <mtext>exp</mtext> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
- en: 'The ELU activation function looks a lot like the ReLU function (see [Figure 11-3](#elu_and_selu_activation_plot)),
    with a few major differences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 激活函数看起来与 ReLU 函数非常相似（见 [图 11-3](#elu_and_selu_activation_plot)），但有几个主要区别：
- en: It takes on negative values when *z* < 0, which allows the unit to have an average
    output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter
    *α* defines the opposite of the value that the ELU function approaches when *z*
    is a large negative number. It is usually set to 1, but you can tweak it like
    any other hyperparameter.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *z* < 0 时，它取负值，这使得单元的平均输出更接近 0，有助于缓解梯度消失问题。超参数 *α* 定义了当 *z* 是一个很大的负数时，ELU
    函数趋近的值的相反数。它通常设置为 1，但你可以像调整任何其他超参数一样对其进行调整。
- en: It has a nonzero gradient for *z* < 0, which avoids the dead neurons problem.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在 *z* < 0 时具有非零梯度，这避免了神经元死亡问题。
- en: If *α* is equal to 1, then the function is smooth everywhere, including around
    *z* = 0, which helps speed up gradient descent since it does not bounce as much
    to the left and right of *z* = 0.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *α* 等于 1，那么函数在所有地方都是平滑的，包括在 *z* = 0 附近，这有助于加快梯度下降，因为它不会像在 *z* = 0 的左右两侧那样弹跳得那么厉害。
- en: Using ELU with PyTorch is as easy as using the `nn.ELU` module, along with Kaiming
    initialization. The main drawback of the ELU activation function is that it is
    slower to compute than the ReLU function and its variants (due to the use of the
    exponential function). Its faster convergence rate during training may compensate
    for that slow computation, but still, at test time an ELU network will be a bit
    slower than a ReLU network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 中的 ELU 与使用 `nn.ELU` 模块以及 Kaiming 初始化一样简单。ELU 激活函数的主要缺点是它比 ReLU 函数及其变体计算速度慢（由于使用了指数函数）。它在训练期间更快的收敛速度可能可以弥补这种慢速计算，但仍然，在测试时间，ELU
    网络将比 ReLU 网络慢一些。
- en: '![Graph comparing ELU and SELU activation functions, showing SELU is scaled
    higher than ELU.](assets/hmls_1103.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![比较 ELU 和 SELU 激活函数的图表，显示 SELU 的缩放高于 ELU。](assets/hmls_1103.png)'
- en: Figure 11-3\. ELU and SELU activation functions
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. ELU 和 SELU 激活函数
- en: 'Not long after, a [2017 paper](https://homl.info/selu) by Günter Klambauer
    et al.⁠^([9](ch11.html#id2500)) introduced the *scaled ELU* (SELU) activation
    function: as its name suggests, it is a scaled variant of the ELU activation function
    (about 1.05 times ELU, using *α* ≈ 1.67). The authors showed that if you build
    a neural network composed exclusively of a stack of dense layers (i.e., an MLP),
    and if all hidden layers use the SELU activation function, then the network will
    *self-normalize*: the output of each layer will tend to preserve a mean of 0 and
    a standard deviation of 1 during training, which solves the vanishing/exploding
    gradients problem. As a result, the SELU activation function may outperform other
    activation functions for MLPs, especially deep ones. To use it with PyTorch, just
    use `nn.SELU`. There are, however, a few conditions for self-normalization to
    happen (see the paper for the mathematical justification):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，Günter Klambauer 等人⁠^([9](ch11.html#id2500)) 在一篇 [2017 年的论文](https://homl.info/selu)
    中介绍了 *缩放 ELU* (SELU) 激活函数：正如其名所示，它是 ELU 激活函数的一个缩放变体（大约是 ELU 的 1.05 倍，使用 *α* ≈
    1.67）。作者们表明，如果你构建一个仅由堆叠的密集层（即 MLP）组成的神经网络，并且所有隐藏层都使用 SELU 激活函数，那么网络将 *自我归一化*：每个层的输出在训练过程中将倾向于保持均值为
    0 和标准差为 1，这解决了梯度消失/爆炸问题。因此，SELU 激活函数可能在 MLP 中优于其他激活函数，尤其是在深度 MLP 中。要在 PyTorch
    中使用它，只需使用 `nn.SELU`。然而，自我归一化发生有几个条件（参见论文中的数学证明）：
- en: 'The input features must be standardized: mean 0 and standard deviation 1.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征必须标准化：均值为 0，标准差为 1。
- en: Every hidden layer’s weights must be initialized using LeCun normal initialization.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的权重必须使用 LeCun 正态初始化。
- en: The self-normalizing property is only guaranteed with plain MLPs. If you try
    to use SELU in other architectures, like recurrent networks (see [Chapter 13](ch13.html#rnn_chapter))
    or networks with *skip connections* (i.e., connections that skip layers, such
    as in Wide & Deep neural networks), it will probably not outperform ELU.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我归一化属性仅在纯 MLP 中得到保证。如果你尝试在其他架构中使用 SELU，如循环网络（参见第 13 章）或具有 *跳过连接*（即跳过层的连接，例如在
    Wide & Deep 神经网络中）的网络，它可能不会优于 ELU。
- en: You cannot use regularization techniques like ℓ[1] or ℓ[2] regularization, batch-norm,
    layer-norm, max-norm, or regular dropout (these are discussed later in this chapter).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能使用正则化技术，如 ℓ[1] 或 ℓ[2] 正则化、批归一化、层归一化、最大归一化或常规 dropout（这些将在本章后面讨论）。
- en: These are significant constraints, so despite its promises, SELU did not gain
    a lot of traction. Moreover, other activation functions seem to outperform it
    quite consistently on most tasks. Let’s look at some of the most popular ones.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是重要的限制条件，因此尽管 SELU 有其承诺，但它并没有获得太多的关注。此外，其他激活函数似乎在大多数任务上的一致性上优于它。让我们看看其中一些最受欢迎的。
- en: GELU, Swish, SwiGLU, Mish, and RELU²
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GELU、Swish、SwiGLU、Mish 和 RELU²
- en: 'The *Gaussian Error Linear Unit* (*GELU*) was introduced in a [2016 paper](https://homl.info/gelu)
    by Dan Hendrycks and Kevin Gimpel.^([10](ch11.html#id2505)) Once again, you can
    think of it as a smooth variant of the ReLU activation function. Its definition
    is given in [Equation 11-3](#gelu_activation_equation), where Φ is the standard
    Gaussian cumulative distribution function (CDF): Φ(*z*) corresponds to the probability
    that a value sampled randomly from a normal distribution of mean 0 and variance
    1 is lower than *z*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯误差线性单元* (*GELU*) 在 Dan Hendrycks 和 Kevin Gimpel 的 [2016 年论文](https://homl.info/gelu)
    中被引入。再次，你可以将其视为 ReLU 激活函数的平滑变体。其定义在 [方程 11-3](#gelu_activation_equation) 中给出，其中
    Φ 是标准高斯累积分布函数（CDF）：Φ(*z*) 对应于从均值为 0 和方差为 1 的正态分布中随机抽取的值小于 *z* 的概率。'
- en: Equation 11-3\. GELU activation function
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11-3\. GELU 激活函数
- en: <mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo>
- en: 'As you can see in [Figure 11-4](#gelu_swish_mish_plot), GELU resembles ReLU:
    it approaches 0 when its input *z* is very negative, and it approaches *z* when
    *z* is very positive. However, whereas all the activation functions we’ve discussed
    so far were both convex and monotonic,^([11](ch11.html#id2506)) the GELU activation
    function is neither: from left to right, it starts by going straight, then it
    wiggles down, reaches a low point around –0.17 (near z ≈ –0.75), and finally bounces
    up and ends up going straight toward the top right. This fairly complex shape
    and the fact that it has a curvature at every point may explain why it works so
    well, especially for complex tasks: gradient descent may find it easier to fit
    complex patterns. In practice, it often outperforms every other activation function
    discussed so far. However, it is a bit more computationally intensive, and the
    performance boost it provides is not always sufficient to justify the extra cost.
    That said, it is possible to show that it is approximately equal to *z*σ(1.702
    *z*), where σ is the sigmoid function: using this approximation also works very
    well, and it has the advantage of being much faster to compute.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 [图 11-4](#gelu_swish_mish_plot) 中可以看到的，GELU 与 ReLU 相似：当其输入 *z* 非常负时，它接近
    0；当 *z* 非常正时，它接近 *z*。然而，与我们之前讨论的所有激活函数都是凸性和单调性不同，GELU 激活函数既不是凸性的也不是单调的：从左到右，它开始是直线，然后波动下降，在约
    –0.17（z ≈ –0.75）附近达到低点，最后弹起并最终直线向上到达右上角。这种相当复杂的形状以及它在每个点都有曲率的事实可能解释了为什么它工作得如此之好，尤其是在复杂任务中：梯度下降可能更容易拟合复杂模式。在实践中，它通常优于我们之前讨论的任何其他激活函数。然而，它计算上稍微复杂一些，它提供的性能提升并不总是足以证明额外成本是合理的。尽管如此，可以证明它大约等于
    *z*σ(1.702 *z*)，其中 σ 是 sigmoid 函数：使用这个近似也工作得很好，并且它具有计算速度更快的优势。
- en: '![Plot comparing the GELU, Swish, parametrized Swish, Mish, and ReLU² activation
    functions, illustrating their differences in behavior across input values.](assets/hmls_1104.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![比较 GELU、Swish、参数化 Swish、Mish 和 ReLU² 激活函数的图表，展示了它们在输入值上的行为差异。](assets/hmls_1104.png)'
- en: Figure 11-4\. GELU, Swish, parametrized Swish, Mish, and ReLU² activation functions
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. GELU、Swish、参数化 Swish、Mish 和 ReLU² 激活函数
- en: 'The GELU paper also introduced the *sigmoid linear unit* (SiLU) activation
    function, which is equal to *z*σ(*z*), but it was outperformed by GELU in the
    authors’ tests. Interestingly, a [2017 paper](https://homl.info/swish) by Prajit
    Ramachandran et al.^([12](ch11.html#id2509)) rediscovered the SiLU function by
    automatically searching for good activation functions. The authors named it *Swish*,
    and the name caught on. In their paper, Swish outperformed every other function,
    including GELU. Ramachandran et al. later generalized Swish by adding an extra
    scalar hyperparameter *β* to scale the sigmoid function’s input. The generalized
    Swish function is Swish[*β*](*z*) = *z*σ(*βz*), so GELU is approximately equal
    to the generalized Swish function using *β* = 1.702\. You can tune *β* like any
    other hyperparameter. Alternatively, it’s also possible to make *β* trainable
    and let gradient descent optimize it (a bit like PReLU): there is typically a
    single trainable *β* parameter for the whole model, or just one per layer, to
    keep the model efficient and avoid overfitting.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GELU 论文还介绍了 *sigmoid 线性单元*（SiLU）激活函数，它等于 *z*σ(*z*)，但在作者们的测试中，其性能不如 GELU。有趣的是，Prajit
    Ramachandran 等人于 2017 年发表的一篇 [论文](https://homl.info/swish) 通过自动搜索好的激活函数重新发现了 SiLU
    函数。作者将其命名为 *Swish*，这个名字很快流行起来。在他们的论文中，Swish 在性能上优于其他所有函数，包括 GELU。Ramachandran
    等人后来通过添加一个额外的标量超参数 *β* 来缩放 sigmoid 函数的输入，从而推广了 Swish。推广后的 Swish 函数是 Swish[*β*](*z*)
    = *z*σ(*βz*)，因此当 *β* = 1.702 时，GELU 大约等于推广后的 Swish 函数。你可以像调整任何其他超参数一样调整 *β*。或者，也可以使
    *β* 可训练，并让梯度下降优化它（有点像 PReLU）：通常整个模型只有一个可训练的 *β* 参数，或者每层只有一个，以保持模型高效并避免过拟合。
- en: 'A popular Swish variant is [*SwiGLU*](https://homl.info/swiglu):⁠^([13](ch11.html#id2519))
    the inputs go through the Swish activation function, and in parallel through a
    linear layer, then both outputs are multiplied itemwise. That’s SwiGLU(**z**)
    = Swish[*β*](**z**) ⊗ Linear(**z**). This is often implemented by doubling the
    output dimensions of the previous linear layer, then splitting the outputs in
    two along the feature dimension to get **z**[1] and **z**[2], and finally applying:
    SwiGLU[*β*](**z**) = Swish[*β*](**z**[1]) ⊗ **z**[2]. This is a variant of the
    [*gated linear unit* (GLU)](https://homl.info/glu)⁠^([14](ch11.html#id2520)) introduced
    by Facebook researchers in 2016\. The itemwise multiplication gives the model
    more expressive power, allowing it to learn when to turn off (i.e., multiply by
    0) or amplify specific features: this is called a *gating mechanism*. SwiGLU is
    very common in modern transformers (see [Chapter 15](ch15.html#transformer_chapter)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的 Swish 变体是 [*SwiGLU*](https://homl.info/swiglu):⁠^([13](ch11.html#id2519))
    输入通过 Swish 激活函数，同时并行通过一个线性层，然后逐项相乘输出。这就是 SwiGLU(**z**) = Swish[*β*](**z**) ⊗ Linear(**z**)。这通常通过将前一个线性层的输出维度加倍，然后沿着特征维度将输出分成两部分以获得
    **z**[1] 和 **z**[2]，最后应用：SwiGLU[*β*](**z**) = Swish[*β*](**z**[1]) ⊗ **z**[2]。这是
    Facebook 研究人员在 2016 年引入的 [*门控线性单元* (GLU)](https://homl.info/glu)⁠^([14](ch11.html#id2520))
    的一个变体。逐项乘法给模型提供了更多的表达能力，允许它学习何时关闭（即乘以 0）或放大特定特征：这被称为 *门控机制*。SwiGLU 在现代变压器中非常常见（参见
    [第 15 章](ch15.html#transformer_chapter)）。
- en: Another GELU-like activation function is *Mish*, which was introduced in a [2019
    paper](https://homl.info/mish) by Diganta Misra.^([15](ch11.html#id2524)) It is
    defined as mish(*z*) = *z*tanh(softplus(*z*)), where softplus(*z*) = log(1 + exp(*z*)).
    Just like GELU and Swish, it is a smooth, nonconvex, and nonmonotonic variant
    of ReLU, and once again the author ran many experiments and found that Mish generally
    outperformed other activation functions—even Swish and GELU, by a tiny margin.
    [Figure 11-4](#gelu_swish_mish_plot) shows GELU, Swish (both with the default
    *β* = 1 and with *β* = 0.6), and lastly Mish. As you can see, Mish overlaps almost
    perfectly with Swish when *z* is negative, and almost perfectly with GELU when
    *z* is positive.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类似于 GELU 的激活函数是 *Mish*，它由 Diganta Misra 在 [2019 年的一篇论文](https://homl.info/mish)中提出.^([15](ch11.html#id2524))
    它被定义为 mish(*z*) = *z*tanh(softplus(*z*))，其中 softplus(*z*) = log(1 + exp(*z*))。就像
    GELU 和 Swish 一样，它是一个平滑、非凸、非单调的 ReLU 变体，而且作者再次进行了许多实验，发现 Mish 通常优于其他激活函数——甚至比 Swish
    和 GELU 略胜一筹。![图 11-4](#gelu_swish_mish_plot) 展示了 GELU、Swish（默认 *β* = 1 和 *β* =
    0.6）以及最后的 Mish。如图所示，当 *z* 为负值时，Mish 几乎完美地与 Swish 重叠，而当 *z* 为正值时，几乎完美地与 GELU 重叠。
- en: 'Lastly, in 2021, Google researchers ran an automated architecture search to
    improve large transformers, and the search found a very simple yet effective activation
    function: [ReLU²](https://homl.info/relu2).⁠^([16](ch11.html#id2532)) As its name
    suggests, it’s simply ReLU squared: ReLU²(*z*) = (max(0, *z*))^2\. It has all
    the qualities of ReLU (simplicity, computational efficiency, sparse output, no
    saturation on the positive side) but it also has smooth gradients at *z* = 0,
    and it often outperforms other activation functions, especially for sparse models.
    However, training can be less stable, in part because of its increased sensitivity
    to outliers and dying ReLUs.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 2021 年，Google 研究人员运行了一个自动化的架构搜索来改进大型变压器，搜索发现了一个非常简单但有效的激活函数：[ReLU²](https://homl.info/relu2)。⁠^([16](ch11.html#id2532))
    如其名所示，它只是 ReLU 的平方：ReLU²(*z*) = (max(0, *z*))^2。它具有 ReLU 的所有特性（简单性、计算效率、稀疏输出、正侧无饱和）但它也有在
    *z* = 0 处的平滑梯度，并且通常优于其他激活函数，特别是对于稀疏模型。然而，训练可能不太稳定，部分原因是因为它对异常值和死亡 ReLUs 的敏感性增加。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'So, which activation function should you use for the hidden layers of your
    deep neural networks? ReLU remains a good default for most tasks: it’s often just
    as good as the more sophisticated activation functions, plus it’s very fast to
    compute, and many libraries and hardware accelerators provide ReLU-specific optimizations.
    However, Swish is probably a better default for complex tasks, and you can even
    try parametrized Swish with a learnable *β* parameter for the most complex tasks.
    Mish and SwiGLU may give you slightly better results, but they require a bit more
    compute. If you care a lot about runtime latency, then you may prefer leaky ReLU,
    or parametrized leaky ReLU for complex tasks, or even ReLU², especially for sparse
    models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该为你的深度神经网络隐藏层使用哪种激活函数呢？ReLU对于大多数任务来说仍然是一个好的默认选择：它通常与更复杂的激活函数一样好，而且计算速度非常快，许多库和硬件加速器提供了ReLU特定的优化。然而，对于复杂任务，Swish可能是一个更好的默认选择，你甚至可以尝试带有可学习**β**参数的参数化Swish，对于最复杂的任务。Mish和SwiGLU可能会给你带来略微更好的结果，但它们需要更多的计算。如果你非常关心运行时延迟，那么你可能更喜欢Leaky
    ReLU，或者对于复杂任务，选择参数化Leaky ReLU，甚至ReLU²，特别是对于稀疏模型。
- en: PyTorch supports GELU, Mish, and Swish out of the box (using `nn.GELU`, `nn.Mish`,
    and `nn.SiLU`, respectively). To implement SwiGLU, double the previous linear
    layer’s output dimension, then use `z1, z2 = z.chunk(2, dim=-1)` to split its
    output in two, and compute `F.silu(beta * z1) * z2` (where `F` is `torch.nn.functional`).
    For ReLU², simply compute `F.relu(z).square()`. PyTorch also includes simplified
    and approximated versions of several activation functions, which are much faster
    to compute and often more stable during training. These simplified versions have
    names starting with “Hard”, such as `nn.Hardsigmoid`, `nn.Hardtanh`, and `nn.Hardswish`,
    and they are often used on mobile devices.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch原生支持GELU、Mish和Swish（分别使用`nn.GELU`、`nn.Mish`和`nn.SiLU`）。要实现SwiGLU，将前一个线性层的输出维度加倍，然后使用`z1,
    z2 = z.chunk(2, dim=-1)`将其输出分成两部分，并计算`F.silu(beta * z1) * z2`（其中`F`是`torch.nn.functional`）。对于ReLU²，只需计算`F.relu(z).square()`。PyTorch还包括几个激活函数的简化和近似版本，这些版本计算速度更快，在训练过程中通常更稳定。这些简化版本的名字以“Hard”开头，例如`nn.Hardsigmoid`、`nn.Hardtanh`和`nn.Hardswish`，它们通常用于移动设备。
- en: 'That’s all for activation functions! Now, let’s look at a completely different
    way to solve the unstable gradients problem: batch normalization.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数就讲到这里！现在，让我们看看解决不稳定梯度问题的另一种完全不同的方法：批归一化。
- en: Batch Normalization
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批归一化
- en: Although using Kaiming initialization along with ReLU (or any of its variants)
    can significantly reduce the danger of the vanishing/exploding gradients problems
    at the beginning of training, it doesn’t guarantee that they won’t come back during
    training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在训练初期使用Kaiming初始化与ReLU（或其任何变体）可以显著减少梯度消失/爆炸问题的风险，但这并不能保证它们在训练过程中不会再次出现。
- en: 'In a [2015 paper](https://homl.info/51),⁠^([17](ch11.html#id2548)) Sergey Ioffe
    and Christian Szegedy proposed a technique called *batch normalization* (BN) that
    addresses these problems. The technique consists of adding an operation in the
    model just before or after the activation function of each hidden layer. This
    operation simply zero-centers and normalizes each input, then scales and shifts
    the result using two new parameter vectors per layer: one for scaling, the other
    for shifting. In other words, the operation lets the model learn the optimal scale
    and mean of each of the layer’s inputs. In many cases, if you add a BN layer as
    the very first layer of your neural network, you do not need to standardize your
    training set (no need for `StandardScaler`); the BN layer will do it for you (well,
    approximately, since it only looks at one batch at a time, and it can also rescale
    and shift each input feature).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年的一篇论文[2015 paper](https://homl.info/51)⁠^([17](ch11.html#id2548))中，谢尔盖·约夫伊和克里斯蒂安·塞格迪提出了一种称为**批归一化**（BN）的技术，用以解决这些问题。这项技术包括在每个隐藏层的激活函数之前或之后在模型中添加一个操作。这个操作简单地将每个输入归零并标准化，然后使用每个层的两个新参数向量进行缩放和偏移：一个用于缩放，另一个用于偏移。换句话说，这个操作让模型学习每个层输入的最佳缩放和均值。在许多情况下，如果你将BN层作为你的神经网络的第一层，你就不需要标准化你的训练集（不需要`StandardScaler`）；BN层会为你完成这项工作（好吧，大约是这样，因为它一次只查看一个批次，并且它还可以重新缩放和偏移每个输入特征）。
- en: In order to zero-center and normalize the inputs, the algorithm needs to estimate
    each input’s mean and standard deviation. It does so by evaluating the mean and
    standard deviation of the input over the current mini-batch (hence the name “batch
    normalization”). The whole operation is summarized step by step in [Equation 11-4](#batch_normalization_algorithm).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使输入零中心并归一化，算法需要估计每个输入的均值和标准差。它是通过评估当前迷你批次上的输入均值和标准差来做到这一点的（因此得名“批标准化”）。整个操作步骤总结在[方程
    11-4](#batch_normalization_algorithm)中。
- en: Equation 11-4\. Batch normalization algorithm
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11-4\. 批标准化算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi
    mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi
    mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable>
- en: 'In this algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此算法中：
- en: '**μ**[*B*] is the vector of input means, evaluated over the whole mini-batch
    *B* (it contains one mean per input).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**μ**[*B*] 是输入均值向量，在整个迷你批次 *B* 上评估（它包含每个输入的一个均值）。'
- en: '*m*[*B*] is the number of instances in the mini-batch.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*[*B*] 是迷你批次的实例数量。'
- en: '**x**^((*i*)) is the input vector of the batch-norm layer for instance *i*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**^((*i*))是批归一化层实例*i*的输入向量。'
- en: '**σ**[*B*] is the vector of input standard deviations, also evaluated over
    the whole mini-batch (it contains one standard deviation per input).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**σ**[*B*]是输入标准差的向量，在整个小批量上评估（它包含每个输入的一个标准差）。'
- en: <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> ^((*i*))
    is the vector of zero-centered and normalized inputs for instance *i*.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> ^((*i*))是实例*i*的零均值和归一化输入向量。
- en: '*ε* is a tiny number that avoids division by zero and ensures the gradients
    don’t grow too large (typically 10^(–5)). This is called a *smoothing term*.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ε*是一个很小的数，它避免了除以零并确保梯度不会变得太大（通常是10^(–5)）。这被称为*平滑项*。'
- en: '**γ** is the output scale parameter vector for the layer (it contains one scale
    parameter per input).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**γ**是层的输出缩放参数向量（它包含每个输入的一个缩放参数）。'
- en: ⊗ represents element-wise multiplication (each input is multiplied by its corresponding
    output scale parameter).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⊗表示逐元素乘法（每个输入都乘以其相应的输出缩放参数）。
- en: '**β** is the output shift (offset) parameter vector for the layer (it contains
    one shift parameter per input). Each input is offset by its corresponding shift
    parameter.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**β**是层的输出偏移（偏移量）参数向量（它包含每个输入的一个偏移参数）。每个输入都通过其相应的偏移参数进行偏移。'
- en: '**z**^((*i*)) is the output of the BN operation. It is a rescaled and shifted
    version of the inputs.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z**^((*i*))是BN操作的输出。它是输入的缩放和偏移版本。'
- en: 'So during training, BN standardizes its inputs, then rescales and offsets them.
    Good! What about at test time? Well, it’s not that simple. Indeed, we may need
    to make predictions for individual instances rather than for batches of instances:
    in this case, we will have no way to compute each input’s standard deviation.
    Moreover, even if we do have a batch of instances, it may be too small, or the
    instances may not be independent and identically distributed, so computing statistics
    over the batch instances would be unreliable. One solution is to wait until the
    end of training, then run the whole training set through the neural network and
    compute the mean and standard deviation of each input of the BN layer. These “final”
    input means and standard deviations can then be used instead of the batch input
    means and standard deviations when making predictions.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练期间，BN标准化其输入，然后缩放和偏移它们。很好！那么测试时间呢？嗯，并不那么简单。实际上，我们可能需要为单个实例而不是实例批量进行预测：在这种情况下，我们将无法计算每个输入的标准差。此外，即使我们有实例批量，它可能太小，或者实例可能不是独立同分布的，因此在小批量实例上计算统计量将不可靠。一种解决方案是在训练结束时等待，然后运行整个训练集通过神经网络，并计算BN层每个输入的均值和标准差。然后可以使用这些“最终”输入均值和标准差来代替批输入均值和标准差进行预测。
- en: 'However, most implementations of batch norm estimate these final statistics
    during training by using a moving average of the layer’s batch input means and
    variances. This is what PyTorch does automatically when you use its batch-norm
    layers, such as `nn.BatchNorm1d` (which we will discuss in the next section).
    To sum up, four parameter vectors are learned in each batch-norm layer: **γ**
    (the output scale vector) and **β** (the output offset vector) are learned through
    regular backpropagation, and **μ** (the final input mean vector) and **σ**² (the
    final input variance vector) are estimated using an exponential moving average.
    Note that **μ** and **σ**² are estimated during training, but they are used only
    after training, once you switch the model to evaluation mode using `model.eval()`:
    **μ** and **σ**² then replace **μ**[*B*] and **σ**[*B*]² in [Equation 11-4](#batch_normalization_algorithm).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数批归一化的实现都是通过使用层的批输入均值和方差的移动平均来估计这些最终统计量的。这就是当你使用PyTorch的批归一化层，如`nn.BatchNorm1d`（我们将在下一节中讨论）时，PyTorch自动执行的操作。总的来说，每个批归一化层中学习到四个参数向量：**γ**（输出缩放向量）和**β**（输出偏移向量）通过常规的反向传播学习，而**μ**（最终输入均值向量）和**σ**²（最终输入方差向量）使用指数移动平均来估计。请注意，**μ**和**σ**²是在训练期间估计的，但它们仅在训练完成后使用，一旦你使用`model.eval()`将模型切换到评估模式：**μ**和**σ**²将替换[方程11-4](#batch_normalization_algorithm)中的**μ**[*B*]和**σ**[*B*]²。
- en: 'Ioffe and Szegedy demonstrated that batch norm considerably improved all the
    deep neural networks they experimented with, leading to a huge improvement in
    the ImageNet classification task (ImageNet is a large database of images classified
    into many classes, commonly used to evaluate computer vision systems). The vanishing
    gradients problem was strongly reduced, to the point that they could use saturating
    activation functions such as tanh and even sigmoid. The networks were also much
    less sensitive to the weight initialization. The authors were able to use much
    larger learning rates, significantly speeding up the learning process. Specifically,
    they note that:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Ioffe和Szegedy证明了批量归一化显著提高了他们实验中所有深度神经网络的表现，这在ImageNet分类任务（ImageNet是一个包含许多类别的图像的大型数据库，常用于评估计算机视觉系统）中带来了巨大的改进（ImageNet是包含许多类别的图像的大型数据库，常用于评估计算机视觉系统）。梯度消失问题得到了显著减少，以至于他们可以使用饱和激活函数，如tanh和sigmoid。网络对权重初始化的敏感性也大大降低。作者能够使用更大的学习率，显著加快学习过程。具体来说，他们指出：
- en: 'Applied to a state-of-the-art image classification model, batch norm achieves
    the same accuracy with 14 times fewer training steps, and beats the original model
    by a significant margin. […​] Using an ensemble of batch-normalized networks,
    we improve upon the best published result on ImageNet classification: reaching
    4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human
    raters.'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将批量归一化应用于最先进的图像分类模型，批量归一化在14倍更少的训练步骤下达到了相同的准确率，并且显著优于原始模型。[……]使用批量归一化的网络集成，我们在ImageNet分类上取得了最佳已发表结果：达到4.9%的top-5验证错误（和4.8%的测试错误），超过了人类评分员的准确率。
- en: Finally, like a gift that keeps on giving, batch norm acts like a regularizer,
    reducing the need for other regularization techniques (such as dropout, described
    later in this chapter).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像源源不断的礼物一样，批量归一化还起到了正则化的作用，减少了其他正则化技术（如本章后面将要描述的dropout）的需求。
- en: 'Batch normalization does, however, add some complexity to the model (although
    it can remove the need for normalizing the input data, as discussed earlier).
    Moreover, there is a runtime penalty: the neural network makes slower predictions
    due to the extra computations required at each layer. Fortunately, it’s often
    possible to fuse the BN layer with the previous layer after training, thereby
    avoiding the runtime penalty. This is done by updating the previous layer’s weights
    and biases so that it directly produces outputs of the appropriate scale and offset.
    For example, if the previous layer computes **XW** + **b**, then the BN layer
    will compute **γ** ⊗ (**XW** + **b** – **μ**) / **σ** + **β** (ignoring the smoothing
    term *ε* in the denominator). If we define **W**′ = **γ**⊗**W** / **σ** and **b**′
    = **γ** ⊗ (**b** – **μ**) / **σ** + **β**, the equation simplifies to **XW**′
    + **b**′. So, if we replace the previous layer’s weights and biases (**W** and
    **b**) with the updated weights and biases (**W**′ and **b**′), we can get rid
    of the BN layer. This is one of the optimizations performed by `optimize_for_inference()`
    (see [Chapter 10](ch10.html#pytorch_chapter)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，批量归一化确实给模型增加了一些复杂性（尽管它可以消除前面讨论中提到的对输入数据进行归一化的需求）。此外，还存在运行时惩罚：由于在每个层中需要额外的计算，神经网络做出预测的速度会变慢。幸运的是，通常可以在训练后融合BN层和前一层的权重，从而避免运行时惩罚。这是通过更新前一层的权重和偏置，使其直接产生适当规模和偏移的输出来实现的。例如，如果前一层的计算是**XW**
    + **b**，那么BN层将计算**γ** ⊗ (**XW** + **b** – **μ**) / **σ** + **β**（忽略分母中的平滑项*ε*）。如果我们定义**W**′
    = **γ**⊗**W** / **σ**和**b**′ = **γ** ⊗ (**b** – **μ**) / **σ** + **β**，则方程可以简化为**XW**′
    + **b**′。因此，如果我们用更新的权重和偏置（**W**′和**b**′）替换前一层的权重和偏置（**W**和**b**），我们就可以去掉BN层。这是`optimize_for_inference()`（见第10章）执行的一种优化。 '
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may find that training is rather slow, because each epoch takes much more
    time when you use batch norm. This is usually counterbalanced by the fact that
    convergence is much faster with BN, so it will take fewer epochs to reach the
    same performance. All in all, *wall time* will usually be shorter (this is the
    time measured by the clock on your wall).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现训练相当慢，因为当你使用批量归一化时，每个epoch所需的时间会大大增加。这通常会被BN收敛速度更快的事实所抵消，因此达到相同性能所需的epoch会更少。总的来说，*wall
    time*（这是由你墙上的时钟测量的时间）通常会缩短。
- en: Implementing batch norm with PyTorch
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch实现批量归一化
- en: 'As with most things with PyTorch, implementing batch norm is straightforward
    and intuitive. Just add an `nn.BatchNorm1d` layer before or after each hidden
    layer’s activation function, and specify the number of inputs of each BN layer.
    You may also add a BN layer as the first layer in your model, which removes the
    need to standardize the inputs manually. For example, let’s create a Fashion MNIST
    image classifier (similar to the one we built in [Chapter 10](ch10.html#pytorch_chapter))
    using BN as the first layer in the model (after flattening the input images),
    then again after each hidden layer:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就像PyTorch中的大多数事情一样，实现批量归一化简单直观。只需在每个隐藏层的激活函数之前或之后添加一个`nn.BatchNorm1d`层，并指定每个BN层的输入数量。你还可以将BN层作为模型的第一层，这样就不需要手动标准化输入。例如，让我们创建一个Fashion
    MNIST图像分类器（类似于我们在[第10章](ch10.html#pytorch_chapter)中构建的），将BN作为模型的第一层（在展平输入图像之后），然后在每个隐藏层之后再次使用：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can now train the model normally (as you learned in [Chapter 10](ch10.html#pytorch_chapter)),
    and that’s it! In this tiny example with just two hidden layers, batch norm is
    unlikely to have a large impact, but for deeper networks it can make a tremendous
    difference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以正常训练模型（就像你在[第10章](ch10.html#pytorch_chapter)中学到的那样），就这样！在这个只有两层隐藏层的微小示例中，批量归一化不太可能产生重大影响，但对于更深的网络来说，它可能带来巨大的差异。
- en: Warning
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Since batch norm behaves differently during training and during evaluation,
    it’s critical to switch to training mode during training (using `model.train()`),
    and switch to evaluation mode during evaluation (using `model.eval()`). Forgetting
    to do so is one of the most common mistakes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批量归一化在训练和评估期间的行为不同，因此在训练期间切换到训练模式（使用`model.train()`）以及在评估期间切换到评估模式（使用`model.eval()》）至关重要。忘记这样做是最常见的错误之一。
- en: 'If you look at the parameters of the first BN layer, you will find two: `weight`
    and `bias`, which correspond to **γ** and **β** in [Equation 11-4](#batch_normalization_algorithm):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看第一个BN层的参数，你会找到两个：`weight`和`bias`，它们对应于[方程11-4](#batch_normalization_algorithm)中的**γ**和**β**：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And if you look at the buffers of this same BN layer, you will find three:
    `run⁠ning_​mean`, `running_var`, and `num_batches_tracked`. The first two correspond
    to the running means **μ** and **σ**² discussed earlier, and `num_batches_tracked`
    simply counts the number of batches seen during training:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看这个相同的BN层的缓冲区，你会找到三个：`running_mean`、`running_var`和`num_batches_tracked`。前两个对应于之前讨论的运行均值**μ**和**σ**²，而`num_batches_tracked`简单地统计了训练过程中看到的批次数量：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The authors of the BN paper argued in favor of adding the BN layers before
    the activation functions, rather than after (as we just did). There is some debate
    about this, and it seems to depend on the task, so you can experiment with this
    to see which option works best on your dataset. If you move the BN layers before
    the activation functions, you can also remove the bias term from the previous
    `nn.Linear` layers by setting their `bias` hyperparameter to `False`. Indeed,
    a batch-norm layer already includes one bias term per input. You can also drop
    the first BN layer to avoid sandwiching the first hidden layer between two BN
    layers, but this means you should normalize the training set before training.
    The updated code looks like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BN论文的作者们赞成在激活函数之前添加BN层，而不是之后（正如我们刚才所做的那样）。对此有一些争议，这似乎取决于任务，所以你可以尝试不同的选项，看看哪个在你的数据集上效果最好。如果你将BN层移动到激活函数之前，你也可以通过将它们的`bias`超参数设置为`False`来从之前的`nn.Linear`层中移除偏差项。确实，批量归一化层已经为每个输入包含了一个偏差项。你还可以删除第一个BN层，以避免在两个BN层之间夹着第一个隐藏层，但这意味着你应该在训练之前对训练集进行归一化。更新的代码如下：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `nn.BatchNorm1d` class has a few hyperparameters you can tweak. The defaults
    will usually be fine, but you may occasionally need to tweak the `momentum`. This
    hyperparameter is used by the `BatchNorm1d` layer when it updates the exponential
    moving averages; given a new value **v** (i.e., a new vector of input means or
    variances computed over the current batch), the layer updates the running average
    <mover><mi mathvariant="bold">v</mi><mo>^</mo></mover> using the following equation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.BatchNorm1d`类有几个你可以调整的超参数。默认值通常很好，但你可能偶尔需要调整`momentum`。这个超参数在`BatchNorm1d`层更新指数移动平均时使用；给定一个新值**v**（即，一个新向量，它是通过当前批次计算得到的输入均值或方差），该层使用以下方程更新运行平均<mover><mi
    mathvariant="bold">v</mi><mo>^</mo></mover>：'
- en: <mrow><mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>←</mo> <mi mathvariant="bold">v</mi> <mo>×</mo> <mtext>momentum</mtext> <mo>+</mo>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover> <mo>×</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>←</mo> <mi mathvariant="bold">v</mi> <mo>×</mo> <mtext>momentum</mtext> <mo>+</mo>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover> <mo>×</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow>
- en: A good momentum value is typically close to 0; for example, 0.01 or 0.001\.
    You want more 0s for smaller mini-batches, and fewer for larger mini-batches.
    The default is 0.1, which is good for large batch sizes, but not great for small
    batch sizes such as 32 or 64.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的动量值通常接近于0；例如，0.01或0.001。对于较小的mini-batch，你希望有更多的0，而对于较大的mini-batch，则希望有较少的0。默认值为0.1，这对于大batch
    size来说很好，但对于32或64这样的小batch size来说则不是很好。
- en: Warning
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When people talk about “momentum” in the context of a running mean, they usually
    refer to the weight of the current running mean in the update equation. Sadly,
    for historical reasons, PyTorch uses the opposite meaning in the BN layers. However,
    other parts of PyTorch use the conventional meaning (e.g., in optimizers), so
    don’t get confused.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们谈论运行均值的“动量”时，他们通常指的是更新方程中当前运行均值的权重。遗憾的是，由于历史原因，PyTorch在BN层中使用相反的含义。然而，PyTorch的其他部分使用传统的含义（例如，在优化器中），所以不要混淆。
- en: Batch norm 1D, 2D, and 3D
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批归一化1D、2D和3D
- en: In the previous examples, we flattened the input images before sending them
    through the first `nn.BatchNorm1d` layer. This is because an `nn.BatchNorm1d`
    layer works on batches of shape `[batch_size, num_features]` (just like the `nn.Linear`
    layer does), so you would get an error if you moved it before the `nn.Flatten`
    layer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们在将输入图像发送到第一个`nn.BatchNorm1d`层之前将其展平。这是因为`nn.BatchNorm1d`层作用于形状为`[batch_size,
    num_features]`的batch（就像`nn.Linear`层一样），所以如果你在`nn.Flatten`层之前移动它，你会得到一个错误。
- en: 'However, you could use an `nn.BatchNorm2d` layer before the `nn.Flatten` layer:
    indeed, it expects its inputs to be image batches of shape `[batch_size, channels,
    height, width]`, and it computes the batch mean and variance across both the batch
    dimension (dimension 0) and the spatial dimensions (dimensions 2 and 3). This
    means that all pixels in the same batch and channel get normalized using the same
    mean and variance: the `nn.BatchNorm2d` layer only has one weight per channel
    and one bias per channel (e.g., three weights and three bias terms for color images
    with three channels for red, green, and blue). This generally works better when
    dealing with image datasets.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以在`nn.Flatten`层之前使用`nn.BatchNorm2d`层：确实，它期望其输入为形状为`[batch_size, channels,
    height, width]`的图像batch，并且它计算批量的均值和方差，包括批维度（维度0）和空间维度（维度2和3）。这意味着同一batch和通道中的所有像素都使用相同的均值和方差进行归一化：`nn.BatchNorm2d`层每个通道只有一个权重和一个偏置（例如，对于具有三个通道的红、绿、蓝的颜色图像，有三个权重和三个偏置项）。这通常在处理图像数据集时效果更好。
- en: 'There’s also an `nn.BatchNorm3d` layer which expects batches of shape `[batch_size,
    channels, depth, height, width]`: this is useful for datasets of 3D images, such
    as CT scans.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个`nn.BatchNorm3d`层，它期望batch的形状为`[batch_size, channels, depth, height,
    width]`：这对于3D图像数据集，如CT扫描，非常有用。
- en: The `nn.BatchNorm1d` layer can also work on batches of sequences. The convention
    in PyTorch is to represent batches of sequences as 3D tensors of shape `[batch_size,
    sequence_length, num_features]`. For example, suppose you work on particle physics
    and you have a dataset of particle trajectories, where each trajectory is composed
    of a sequence of 100 points in 3D space, then a batch of 32 trajectories will
    have a shape of `[32, 100, 3]`. However, the `nn.BatchNorm1d` layer expects the
    shape to be `[batch_size, num_features, sequence_length]`, and it computes the
    batch mean and variance across the first and last dimensions to get one mean and
    variance per feature. So you must permute the last two dimensions of the data
    using `X.permute(0, 2, 1)` before letting it go through the `nn.BatchNorm1d` layer.
    We will discuss sequences further in [Chapter 13](ch13.html#rnn_chapter).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.BatchNorm1d`层也可以处理序列的批次。在PyTorch中，惯例是将序列的批次表示为形状为`[batch_size, sequence_length,
    num_features]`的3D张量。例如，假设您从事粒子物理学研究，并且您有一个粒子轨迹的数据集，其中每个轨迹由3D空间中的100个点组成的序列，那么32个轨迹的批次将具有形状`[32,
    100, 3]`。然而，`nn.BatchNorm1d`层期望形状为`[batch_size, num_features, sequence_length]`，并且它通过计算第一维和最后一维的批次均值和方差来为每个特征得到一个均值和方差。因此，您必须在通过`nn.BatchNorm1d`层之前使用`X.permute(0,
    2, 1)`对数据的最后两个维度进行置换。我们将在第13章中进一步讨论序列。'
- en: 'Batch normalization has become one of the most-used layers in deep neural networks,
    especially deep convolutional neural networks discussed in [Chapter 12](ch12.html#cnn_chapter),
    to the point that it is often omitted in the architecture diagrams: it is assumed
    that BN is added after every layer. That said, it is not perfect. In particular,
    the computed statistics for an instance are biased by the other samples in a batch,
    which may reduce performance (especially for small batch sizes). Moreover, BN
    struggles with some architectures, such as recurrent nets, as we will see in [Chapter 13](ch13.html#rnn_chapter).
    For these reasons, batch-norm is more and more often replaced by layer-norm.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化已成为深度神经网络中最常用的层之一，尤其是在第12章中讨论的深度卷积神经网络，以至于在架构图中经常被省略：假设在每一层之后都添加了BN。尽管如此，它并不完美。特别是，一个实例的统计数据会受到批量中其他样本的偏差影响，这可能会降低性能（尤其是对于小批量大小）。此外，BN在处理某些架构时存在困难，例如循环网络，正如我们将在第13章中看到的那样。因此，批归一化越来越多地被层归一化所取代。
- en: Layer Normalization
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层归一化
- en: Layer normalization (LN) is very similar to batch norm, but instead of normalizing
    across the batch dimension, LN normalizes across the feature dimensions. This
    simple idea was introduced by Jimmy Lei Ba et al. in a [2016 paper](https://homl.info/layernorm),⁠^([18](ch11.html#id2562))
    and initially applied mostly to recurrent nets. However, in recent years it has
    been successfully applied to many other architectures, such as convolutional nets,
    transformers, diffusion nets, and more.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化（LN）与批归一化（batch norm）非常相似，但不同之处在于LN不是在批量维度上进行归一化，而是在特征维度上进行归一化。这个简单想法是由Jimmy
    Lei Ba等人于2016年在一篇[论文](https://homl.info/layernorm)中提出的，⁠^([18](ch11.html#id2562))，最初主要应用于循环网络。然而，近年来它已成功应用于许多其他架构，例如卷积网络、Transformer、扩散网络等。
- en: One advantage is that LN can compute the required statistics on the fly, at
    each time step, independently for each instance. This also means that it behaves
    the same way during training and testing (as opposed to BN), and it does not need
    to use exponential moving averages to estimate the feature statistics across all
    instances in the training set, like BN does. Lastly, LN learns a scale and an
    offset parameter for each input feature, just like BN does.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优点是LN可以在每个时间步独立地为每个实例即时计算所需的统计数据。这也意味着它在训练和测试期间的行为方式相同（与BN相反），并且它不需要使用指数移动平均来估计训练集中所有实例的特征统计数据，就像BN所做的那样。最后，LN为每个输入特征学习一个缩放参数和一个偏移参数，就像BN所做的那样。
- en: 'PyTorch includes an `nn.LayerNorm` module. To create an instance, you must
    simply indicate the size of the dimensions that you want to normalize over. These
    must be the last dimension(s) of the inputs. For example, if the inputs are batches
    of 100 × 200 RGB images of shape `[3, 100, 200]`, and you want to normalize each
    image over each of the three color channels separately, you would use the following
    `nn.LayerNorm` module:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch包括一个`nn.LayerNorm`模块。要创建一个实例，您只需指定您想要归一化的维度大小。这些必须是输入的最后一个维度。例如，如果输入是100
    × 200 RGB图像的批次，形状为`[3, 100, 200]`，并且您想要分别对每个图像的三个颜色通道进行归一化，您将使用以下`nn.LayerNorm`模块：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code produces the same result:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码产生相同的结果：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'However, most computer vision architectures that use LN normalize over all
    channels at once. For this, you must include the size of the channel dimension
    when creating the `nn.LayerNorm` module:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数使用LN（层归一化）的计算机视觉架构一次对所有通道进行归一化。为此，在创建`nn.LayerNorm`模块时，你必须包含通道维度的尺寸：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And that’s all there is to it! Now let’s look at one last technique to stabilize
    gradients during training: gradient clipping.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容！现在让我们看看最后一个在训练期间稳定梯度的技术：梯度剪裁。
- en: Gradient Clipping
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度剪裁
- en: Another technique to mitigate the exploding gradients problem is to clip the
    gradients during backpropagation so that they never exceed some threshold. This
    is called [*gradient clipping*](https://homl.info/52).⁠^([19](ch11.html#id2572))
    This technique is generally used in recurrent neural networks, where using batch
    norm is tricky (as you will see in [Chapter 13](ch13.html#rnn_chapter)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减轻梯度爆炸问题的技术是在反向传播期间剪裁梯度，以确保它们永远不会超过某个阈值。这被称为[*梯度剪裁*](https://homl.info/52)。⁠^([19](ch11.html#id2572))
    这种技术通常用于循环神经网络，在那里使用批归一化是棘手的（正如你将在[第13章](ch13.html#rnn_chapter)中看到的那样）。
- en: 'In PyTorch, gradient clipping is generally implemented by calling either `torch.​nn.utils.clip_grad_norm_()`
    or `torch.nn.utils.clip_grad_value_()` at each iteration during training, right
    after the gradients are computed (i.e., after `loss.backward()`). Both functions
    take as a first argument the list of model parameters whose gradients must be
    clipped—typically all of them (`model.parameters()`). The `clip_grad_norm_()`
    function clips each gradient vector’s norm if it exceeds the given `max_norm`
    argument. This is a hyperparameter you can tune (a typical default value is 1.0).
    The `clip_grad_value_()` function independently clips the individual components
    of the gradient vector between `-clip_value` and `+clip_value`, where `clip_value`
    is a hyperparameter you can tune. For example, this training loop clips the norm
    of each gradient vector to 1.0:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，梯度剪裁通常通过在每次迭代的训练中调用`torch.nn.utils.clip_grad_norm_()`或`torch.nn.utils.clip_grad_value_()`来实现，在计算梯度之后（即`loss.backward()`之后）。这两个函数都将作为第一个参数接受必须剪裁其梯度的模型参数列表——通常是所有参数（`model.parameters()`）。`clip_grad_norm_()`函数如果梯度向量的范数超过给定的`max_norm`参数，则会剪裁每个梯度向量的范数。这是一个你可以调整的超参数（一个典型的默认值是1.0）。`clip_grad_value_()`函数独立地剪裁梯度向量的各个分量，介于`-clip_value`和`+clip_value`之间，其中`clip_value`是一个你可以调整的超参数。例如，这个训练循环将每个梯度向量的范数剪裁到1.0：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that `clip_grad_value_()` will change the orientation of the gradient
    vector when its components are clipped. For instance, if the original gradient
    vector is `[0.9, 100.0]`, it points mostly in the direction of the second dimension;
    but once you clip it by value, you get `[0.9, 1.0]`, which points roughly at the
    diagonal between the two axes. Despite this reorientation, this approach actually
    works quite well in practice. If you clipped the same vector by norm, the result
    would be `[0.00899964, 0.9999595]`: this would preserve the vector’s orientation,
    but almost eliminate the first component. The best clipping function to use depends
    on the dataset.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当`clip_grad_value_()`函数的分量被剪裁时，它将改变梯度向量的方向。例如，如果原始的梯度向量是`[0.9, 100.0]`，它主要指向第二个维度；但一旦你通过值来剪裁它，你得到`[0.9,
    1.0]`，这大致指向两个轴之间的对角线。尽管这种重新定向，这种方法在实践中实际上工作得相当好。如果你通过范数剪裁相同的向量，结果将是`[0.00899964,
    0.9999595]`：这将保留向量的方向，但几乎消除了第一个分量。最佳剪裁函数的选择取决于数据集。
- en: Reusing Pretrained Layers
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新使用预训练层
- en: It is generally not a good idea to train a very large DNN from scratch without
    first trying to find an existing neural network that accomplishes a similar task
    to the one you are trying to tackle (I will discuss how to find them in [Chapter 12](ch12.html#cnn_chapter)).
    If you find such a neural network, then you can generally reuse most of its layers,
    except for the top ones. This technique is called *transfer learning*. It will
    not only speed up training considerably, but also require significantly less training
    data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，在没有首先尝试找到现有神经网络来执行你试图解决的问题之前，从头开始训练一个非常大的深度神经网络不是一个好主意（我将在[第12章](ch12.html#cnn_chapter)中讨论如何找到它们）。如果你找到了这样的神经网络，那么你通常可以重用其大部分层，除了最顶层。这种技术被称为*迁移学习*。它不仅会显著加快训练速度，而且还需要显著更少的训练数据。
- en: Suppose you have access to a DNN that was trained to classify pictures into
    one hundred different categories, including animals, plants, vehicles, and everyday
    objects, and you now want to train a DNN to classify specific types of vehicles.
    These tasks are very similar, even partly overlapping, so you should try to reuse
    parts of the first network (see [Figure 11-5](#reuse_pretrained_diagram)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个经过训练以将图片分类为一百个不同类别（包括动物、植物、车辆和日常物品）的深度神经网络，现在你想要训练一个深度神经网络来分类特定类型的车辆。这些任务非常相似，甚至部分重叠，因此你应该尝试重用第一个网络的部分（见[图
    11-5](#reuse_pretrained_diagram)）。
- en: '![Diagram illustrating the reuse of pretrained layers from an existing deep
    neural network to train a new model for a similar task, with some weights fixed
    and others trainable.](assets/hmls_1105.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![说明从现有的深度神经网络重用预训练层以训练新模型，用于相似任务，部分权重固定，部分可训练的示意图。](assets/hmls_1105.png)'
- en: Figure 11-5\. Reusing pretrained layers
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. 重用预训练层
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the input pictures for your new task don’t have the same size as the ones
    used in the original task, you will usually have to add a preprocessing step to
    resize them to the size expected by the original model. More generally, transfer
    learning will work best when the inputs have similar low-level features. For example,
    a neural net trained on regular pictures taken from mobile phones will help with
    many other tasks on mobile phone pictures, but it will likely not help at all
    on satellite images or medical images.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你新任务中的输入图片与原始任务中使用的图片大小不同，你通常需要添加一个预处理步骤来将它们调整到原始模型期望的大小。更普遍地说，当输入具有相似的低级特征时，迁移学习将工作得最好。例如，在从手机拍摄的普通图片上训练的神经网络将有助于许多其他手机图片任务，但它可能对卫星图像或医学图像完全无帮助。
- en: The output layer of the original model should usually be replaced because it
    is most likely not useful at all for the new task, and it may not even have the
    right number of outputs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 原始模型的输出层通常需要替换，因为它很可能对新任务完全无用，甚至可能没有正确的输出数量。
- en: Similarly, the upper hidden layers of the original model are less likely to
    be as useful as the lower layers, since the high-level features that are most
    useful for the new task may differ significantly from the ones that were most
    useful for the original task. You want to find the right number of layers to reuse.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，原始模型的顶层隐藏层可能不如底层有用，因为对新任务最有用的中级特征可能与原始任务中最有用的特征有很大差异。你需要找到合适的层来重用。
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The more similar the tasks are, the more layers you will want to reuse (starting
    with the lower layers). For very similar tasks, try to keep all the hidden layers
    and just replace the output layer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 任务越相似，你希望重用的层就越多（从底层开始）。对于非常相似的任务，尽量保留所有隐藏层，只需替换输出层。
- en: 'Try freezing all the reused layers first (i.e., make their parameters nontrainable
    by setting `requires_grad` to `False` so that gradient descent won’t modify them
    and they will remain fixed), then train your model and see how it performs. Then
    try unfreezing one or two of the top hidden layers to let backpropagation tweak
    them and see if performance improves. The more training data you have, the more
    layers you can unfreeze. It is also useful to reduce the learning rate when you
    unfreeze reused layers: this will avoid wrecking their fine-tuned weights.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先尝试冻结所有重用的层（即，通过将 `requires_grad` 设置为 `False` 使其参数不可训练，这样梯度下降就不会修改它们，它们将保持固定），然后训练你的模型并观察其表现。然后尝试解冻一两个顶层隐藏层，让反向传播调整它们，看看性能是否有所改善。你拥有的训练数据越多，你可以解冻的层就越多。在解冻重用层时降低学习率也很有用：这将避免破坏它们的微调权重。
- en: If you still cannot get good performance, and you have little training data,
    try dropping the top hidden layer(s) and freezing all the remaining hidden layers
    again. You can iterate until you find the right number of layers to reuse. If
    you have plenty of training data, you may try replacing the top hidden layers
    instead of dropping them, and even adding more hidden layers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然无法获得良好的性能，并且你的训练数据很少，尝试删除顶层隐藏层，并再次冻结所有剩余的隐藏层。你可以迭代直到找到合适的层来重用。如果你有大量的训练数据，你可能尝试替换顶层隐藏层而不是删除它们，甚至添加更多隐藏层。
- en: Transfer Learning with PyTorch
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 进行迁移学习
- en: 'Let’s look at an example. Suppose the Fashion MNIST dataset only contained
    eight classes—for example, all the classes except for Pullover and T-shirt/top.
    Someone built and trained a PyTorch model on that set and got reasonably good
    performance (~92% accuracy). Let’s call this model A. You now want to tackle a
    different task: you have images of T-shirts and pullovers, and you want to train
    a binary classifier: positive for T-shirt/top, negative for Pullover. Your dataset
    is tiny; you only have 20 labeled images! When you train a new model for this
    task (let’s call it model B) with the same architecture as model A, you get 71.6%
    test accuracy. While drinking your morning coffee, you realize that your task
    is quite similar to task A, so perhaps transfer learning can help? Let’s find
    out!'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。假设Fashion MNIST数据集只包含八个类别——例如，除了Pullover和T-shirt/top之外的所有类别。有人在这个集合上构建并训练了一个PyTorch模型，并取得了相当好的性能（约92%的准确率）。让我们称这个模型为A。你现在想处理一个不同的任务：你有T恤和Pullover的图像，你想要训练一个二分类器：T-shirt/top为正，Pullover为负。你的数据集很小；你只有20个标记的图像！当你使用与模型A相同的架构为这个任务（让我们称它为模型B）训练一个新模型时，你得到71.6%的测试准确率。在喝早咖啡的时候，你意识到你的任务与任务A非常相似，所以也许迁移学习可以有所帮助？让我们来看看！
- en: 'First, let’s look at model A:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看模型A：
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now reuse the layers we want, for example, all layers except for the
    output layer:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以重用我们想要的层，例如，除了输出层之外的所有层：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this code, we use Python’s `copy.deepcopy()` function to copy all the modules
    in the `nn.Sequential` module (along with all their data and submodules), except
    for the last layer. Since we’re making a deep copy, all the submodules are copied
    as well. Then we create `model_B_on_A`, which is an `nn.Sequential` model based
    on the reused layers of model A, plus a new output layer for task B: it has a
    single output since task B is binary classification.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用Python的`copy.deepcopy()`函数来复制`nn.Sequential`模块中的所有模块（包括它们的所有数据和子模块），除了最后一层。由于我们进行的是深度复制，所以所有子模块也被复制。然后我们创建`model_B_on_A`，这是一个基于模型A重用层的`nn.Sequential`模型，并添加了一个新的输出层用于任务B：由于任务B是二分类，它只有一个输出。
- en: 'You could start training `model_B_on_A` for task B now, but since the new output
    layer was initialized randomly, it will make large errors (at least during the
    first few epochs), so there will be large error gradients that may wreck the reused
    weights. To avoid this, one approach is to freeze the reused layers during the
    first few epochs, giving the new layer some time to learn reasonable weights:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以开始训练`model_B_on_A`以执行任务B，但由于新的输出层是随机初始化的，它将导致大的错误（至少在最初的几轮中），因此会有大的误差梯度，可能会破坏重用的权重。为了避免这种情况，一种方法是在最初的几轮中将重用层冻结，给新层一些时间来学习合理的权重：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now you can train `model_B_on_A`. But don’t forget that task B is binary classification,
    so you must switch the loss to `nn.BCEWithLogitsLoss` (or to `nn.BCELoss` if you
    prefer to add an `nn.Sigmoid` activation function on the output layer), as we
    discussed in [Chapter 10](ch10.html#pytorch_chapter). Also, if you are using `torchmetrics`,
    make sure to set `task="binary"` when creating the `Accuracy` metric:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以训练`model_B_on_A`了。但别忘了任务B是二分类，所以你必须将损失函数切换到`nn.BCEWithLogitsLoss`（或者如果你更喜欢在输出层添加`nn.Sigmoid`激活函数，可以使用`nn.BCELoss`），正如我们在[第10章](ch10.html#pytorch_chapter)中讨论的那样。另外，如果你使用`torchmetrics`，确保在创建`Accuracy`度量时设置`task="binary"`：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After you have trained the model for a few epochs, you can unfreeze the reused
    layers (setting `param.requires_grad = True` for all parameters), reduce the learning
    rate, and continue training to fine-tune the reused layers for task B.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在你对模型训练了几轮之后，你可以解冻重用的层（将所有参数的`requires_grad`设置为`True`），降低学习率，并继续训练以微调任务B的重用层。
- en: So, what’s the final verdict? Well, this model’s test accuracy is 92.5%, which
    is much better than the 71.6% accuracy we reached without pretraining!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，最终的结论是什么？嗯，这个模型的测试准确率是92.5%，这比我们没有预训练时的71.6%准确率要好得多！
- en: Are you convinced? Well, you shouldn’t be; I cheated! I tried many configurations
    until I found one that demonstrated a strong improvement. If you try to change
    the classes or the random seed, you will see that the improvement generally drops,
    or even vanishes or reverses. What I did is called “torturing the data until it
    confesses”. When a paper looks too positive, you should be suspicious. Perhaps
    the flashy new technique does not actually help much (in fact, it may even degrade
    performance), but the authors tried many variants and reported only the best results—which
    may be due to sheer luck—without mentioning how many failures they encountered
    along the way. That’s called *p-hacking*. Most of the time, this is not malicious,
    but it is part of the reason why so many results in science can never be reproduced.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你信服了吗？好吧，你不应该信服；我作弊了！我尝试了许多配置，直到找到一个能展示显著改进的配置。如果你尝试更改类别或随机种子，你会看到改进通常下降，甚至消失或逆转。我所做的是“折磨数据直到它招供”。当一篇论文看起来过于乐观时，你应该怀疑。也许这个闪亮的新技术实际上并没有太大帮助（事实上，它甚至可能降低性能），但作者尝试了许多变体，只报告了最好的结果——这可能是纯粹运气的结果——而没有提及他们在过程中遇到了多少失败。这被称为*p-hacking*。大多数情况下，这并非恶意，但它也是科学中许多结果无法复制的部分原因。
- en: But why did I cheat? It turns out that transfer learning does not work very
    well with small dense networks, presumably because small networks learn few patterns,
    and dense networks learn very specific patterns, which are unlikely to be useful
    for other tasks. Transfer learning works best with deep convolutional neural networks
    and with Transformer architectures. We will revisit transfer learning in Chapters
    [12](ch12.html#cnn_chapter) and [15](ch15.html#transformer_chapter), using the
    techniques we just discussed (and this time it will work fine without cheating,
    I promise!).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么我会作弊呢？结果发现，迁移学习在小而密集的网络中效果并不好，可能是因为小网络学习的模式很少，而密集网络学习的模式非常具体，这些模式可能对其他任务没有太大帮助。迁移学习在深度卷积神经网络和Transformer架构中效果最佳。我们将在第[12](ch12.html#cnn_chapter)章和第[15](ch15.html#transformer_chapter)章重新探讨迁移学习，使用我们刚刚讨论的技术（这次我保证不会作弊！）。
- en: Unsupervised Pretraining
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督预训练
- en: Suppose you want to tackle a complex task for which you don’t have much labeled
    training data, but unfortunately you cannot find a model trained on a similar
    task. Don’t lose hope! First, you should try to gather more labeled training data,
    but if you can’t, you may still be able to perform *unsupervised pretraining*
    (see [Figure 11-6](#unsupervised_pretraining_diagram)). Indeed, it is often cheap
    to gather unlabeled training examples, but expensive to label them. If you can
    gather plenty of unlabeled training data, you can try to use it to train an unsupervised
    model, such as an autoencoder (see [Chapter 18](ch18.html#autoencoders_chapter)).
    Then you can reuse the lower layers of the autoencoder, add the output layer for
    your task on top, and fine-tune the final network using supervised learning (i.e.,
    with the labeled training examples).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要处理一个复杂任务，但你没有太多标记的训练数据，而且不幸的是，你找不到在类似任务上训练过的模型。不要失去希望！首先，你应该尝试收集更多的标记训练数据，但如果你做不到，你可能仍然能够执行*无监督预训练*（参见[图11-6](#unsupervised_pretraining_diagram)）。实际上，收集未标记的训练示例通常很便宜，但标记它们却很昂贵。如果你能收集到大量的未标记训练数据，你可以尝试用它来训练一个无监督模型，例如自动编码器（参见[第18章](ch18.html#autoencoders_chapter)）。然后你可以重用自动编码器的底层，在你的任务上方添加输出层，并使用监督学习（即使用标记的训练示例）微调最终的神经网络。
- en: '![Diagram illustrating greedy layer-wise pretraining in early deep learning,
    showing the sequential training of layers with unlabeled and labeled data.](assets/hmls_1106.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图11-6说明早期深度学习中贪婪层预训练的示意图，展示了带有未标记和标记数据的层级顺序训练。](assets/hmls_1106.png)'
- en: Figure 11-6\. Greedy layer-wise pretraining used in the early days of deep learning;
    nowadays the unsupervised part is typically done in one shot on all the data rather
    than one layer at a time
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-6\. 深度学习早期使用的贪婪层预训练；如今，通常在所有数据上一次性完成无监督部分，而不是逐层进行
- en: It is this technique that Geoffrey Hinton and his team used in 2006, and which
    led to the revival of neural networks and the success of deep learning. Until
    2010, unsupervised pretraining—typically with restricted Boltzmann machines (RBMs;
    see the notebook at [*https://homl.info/extra-anns*](https://homl.info/extra-anns))—was
    the norm for deep nets, and only after the vanishing gradients problem was alleviated
    did it become much more common to train DNNs purely using supervised learning.
    Unsupervised pretraining (today typically using autoencoders or diffusion models
    rather than RBMs) is still a good option when you have a complex task to solve,
    no similar model you can reuse, and little labeled training data, but plenty of
    unlabeled training data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这种技术在2006年由杰弗里·辛顿及其团队使用，并导致了神经网络的重生和深度学习的成功。直到2010年，无监督预训练（通常使用受限玻尔兹曼机RBM；参见[*https://homl.info/extra-anns*](https://homl.info/extra-anns)中的笔记本）是深度网络的常态，只有在梯度消失问题得到缓解之后，才更多地使用监督学习来纯训练DNN。当你要解决一个复杂任务，没有可重用的类似模型，以及少量标记的训练数据，但大量未标记的训练数据时，无监督预训练（如今通常使用自动编码器或扩散模型而不是RBM）仍然是一个好选择。
- en: 'Note that in the early days of deep learning it was difficult to train deep
    models, so people would use a technique called *greedy layer-wise pretraining*
    (depicted in [Figure 11-6](#unsupervised_pretraining_diagram)). They would first
    train an unsupervised model with a single layer, typically an RBM, then they would
    freeze that layer and add another one on top of it, then train the model again
    (effectively just training the new layer), then freeze the new layer and add another
    layer on top of it, train the model again, and so on. Nowadays, things are much
    simpler: people generally train the full unsupervised model in one shot and use
    models such as autoencoders or diffusion models rather than RBMs.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在深度学习的早期，训练深度模型很困难，因此人们会使用一种称为**贪婪层预训练**（如图11-6所示）的技术。他们首先使用单个层训练一个无监督模型，通常是RBM，然后冻结该层，在其上方添加另一层，然后再次训练模型（实际上只是训练新层），然后冻结新层，在其上方添加另一层，再次训练模型，依此类推。如今，事情要简单得多：人们通常一次性训练完整的无监督模型，并使用自动编码器或扩散模型等模型，而不是RBM。
- en: Pretraining on an Auxiliary Task
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在辅助任务上进行预训练
- en: If you do not have much labeled training data, one last option is to train a
    first neural network on an auxiliary task for which you can easily obtain or generate
    labeled training data, then reuse the lower layers of that network for your actual
    task. The first neural network’s lower layers will learn feature detectors that
    will likely be reusable by the second neural network.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有多少标记的训练数据，最后一个选项是在一个辅助任务上训练第一个神经网络，对于这个任务，你可以轻松地获得或生成标记的训练数据，然后重用该网络的底层来处理你的实际任务。第一个神经网络的底层将学习特征检测器，这些检测器很可能会被第二个神经网络重用。
- en: For example, if you want to build a system to recognize faces, you may only
    have a few pictures of each individual—clearly not enough to train a good classifier.
    Gathering hundreds of pictures of each person would not be practical. You could,
    however, use a public dataset containing millions of pictures of people (such
    as VGGFace2) and train a first neural network to detect whether two different
    pictures feature the same person. Such a network would learn good feature detectors
    for faces, so reusing its lower layers would allow you to train a good face classifier
    that uses little training data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想构建一个识别面部系统的系统，你可能只有每个个体的几张照片——显然不足以训练一个好的分类器。收集每个人的数百张照片也不切实际。然而，你可以使用包含数百万张人像图片的公共数据集（如VGGFace2），并训练第一个神经网络来检测两张不同的图片是否特征相同的人。这样的网络将学习到好的面部特征检测器，因此重用其底层将允许你训练一个使用少量训练数据的好面部分类器。
- en: Warning
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You could also just scrape pictures of random people from the web, but this
    would probably be illegal. Firstly, photos are usually copyrighted by their creators,
    and websites like Instagram or Facebook enforce these copyright protections through
    their terms of service, which prohibit scraping and unauthorized use. Secondly,
    over 40 countries require explicit consent for collecting and processing personal
    data, including facial images.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从网上抓取随机人的图片，但这可能是不合法的。首先，照片通常由其创作者拥有版权，像Instagram或Facebook这样的网站通过其服务条款执行这些版权保护，禁止抓取和未经授权的使用。其次，超过40个国家要求收集和处理个人数据（包括面部图像）时必须获得明确同意。
- en: For natural language processing (NLP) applications, you can download a corpus
    of millions of text documents and automatically generate labeled data from it.
    For example, you could randomly mask out some words and train a model to predict
    what the missing words are (e.g., it should predict that the missing word in the
    sentence “What ___ you saying?” is probably “are” or “were”). If you can train
    a model to reach good performance on this task, then it will already know quite
    a lot about language, and you can certainly reuse it for your actual task and
    fine-tune it on your labeled data (this is basically how large language models
    are trained and fine-tuned, as we will see in [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理（NLP）应用，您可以下载包含数百万文本文档的语料库，并从中自动生成标记数据。例如，您可以随机掩盖一些单词，并训练一个模型来预测缺失的单词（例如，它应该预测句子“你在说什么？”中缺失的单词可能是“are”或“were”）。如果您能够训练一个模型在这个任务上达到良好的性能，那么它就已经对语言有了相当多的了解，您当然可以将其用于实际任务并对其标记数据进行微调（这基本上是大型语言模型被训练和微调的方式，我们将在第15章中看到）。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Self-supervised learning* is when you automatically generate the labels from
    the data itself, as in the text-masking example, then you train a model on the
    resulting “labeled” dataset using supervised learning techniques.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**自监督学习**是指您从数据本身自动生成标签，例如在文本掩码示例中，然后使用监督学习技术训练一个模型来处理结果“标记”的数据集。'
- en: Faster Optimizers
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更快的优化器
- en: 'Training a very large deep neural network can be painfully slow. So far we
    have seen four ways to speed up training (and reach a better solution): applying
    a good initialization strategy for the connection weights, using a good activation
    function, using batch-norm or layer-norm, and reusing parts of a pretrained network
    (possibly built for an auxiliary task or using unsupervised learning). Another
    huge speed boost comes from using a faster optimizer than the regular gradient
    descent optimizer. In this section we will present the most popular optimization
    algorithms: momentum, Nesterov accelerated gradient, AdaGrad, RMSProp, and finally,
    Adam and its variants.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个非常大的深度神经网络可能会非常缓慢。到目前为止，我们已经看到了四种加速训练（并达到更好的解决方案）的方法：为连接权重应用一个好的初始化策略，使用一个好的激活函数，使用批归一化或层归一化，以及重用预训练网络的部分（可能是为辅助任务构建或使用无监督学习）。使用比常规梯度下降优化器更快的优化器还可以带来巨大的速度提升。在本节中，我们将介绍最流行的优化算法：动量、Nesterov加速梯度、AdaGrad、RMSProp，最后是Adam及其变体。
- en: Momentum
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量
- en: 'Imagine a bowling ball rolling down a gentle slope on a smooth surface: it
    will start out slowly, but it will quickly pick up momentum until it eventually
    reaches terminal velocity (if there is some friction or air resistance). This
    is the core idea behind *momentum optimization*, [proposed by Boris Polyak in
    1964](https://homl.info/54).⁠^([20](ch11.html#id2599)) In contrast, regular gradient
    descent will take small steps when the slope is gentle and big steps when the
    slope is steep, but it will never pick up speed. As a result, regular gradient
    descent is generally much slower to reach the minimum than momentum optimization.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个保龄球在光滑表面上沿着一个温和的斜坡滚动：它一开始会慢慢滚动，但很快就会积累动量，直到最终达到终端速度（如果有摩擦或空气阻力）。这是**动量优化**的核心思想，由Boris
    Polyak于1964年提出[proposed by Boris Polyak in 1964](https://homl.info/54)。⁠^([20](ch11.html#id2599))
    相比之下，常规梯度下降在斜坡平缓时会采取小步，在斜坡陡峭时会采取大步，但它永远不会加速。因此，常规梯度下降通常比动量优化慢得多，才能达到最小值。
- en: As we saw in [Chapter 4](ch04.html#linear_models_chapter), gradient descent
    updates the weights **θ** by directly subtracting the gradient of the cost function
    *J*(**θ**) with regard to the weights (∇[**θ**]*J*(**θ**)) multiplied by the learning
    rate *η*. The equation is **θ** ← **θ** – *η*∇[**θ**]*J*(**θ**). It does not care
    about what the earlier gradients were. If the local gradient is tiny, it goes
    very slowly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.html#linear_models_chapter)中看到的，梯度下降通过直接从权重中减去成本函数*J*(**θ**)相对于权重的梯度（∇[**θ**]*J*(**θ**))乘以学习率*η*来更新权重**θ**。方程是**θ**
    ← **θ** – *η*∇[**θ**]*J*(**θ**)。它不关心之前的梯度是什么。如果局部梯度很小，它就会非常慢。
- en: 'Momentum optimization cares a great deal about what previous gradients were:
    at each iteration, it subtracts the local gradient from the *momentum vector*
    **m** (multiplied by the learning rate *η*), and it updates the weights by adding
    this momentum vector (see [Equation 11-5](#momentum_equation)). In other words,
    the gradient is used as a force learning to an acceleration, not as a speed. To
    simulate some sort of friction mechanism and prevent the momentum from growing
    too large, the algorithm introduces a new hyperparameter *β*, called the *momentum
    coefficient*, which must be set between 0 (high friction) and 1 (no friction).
    A typical momentum value is 0.9.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化非常关注之前的梯度：在每次迭代中，它从动量向量**m**（乘以学习率**η**）中减去局部梯度，并通过添加这个动量向量来更新权重（见[方程11-5](#momentum_equation)）。换句话说，梯度被用作一个力，学习加速度，而不是速度。为了模拟某种摩擦机制并防止动量增长过大，算法引入了一个新的超参数**β**，称为**动量系数**，其值必须在0（高摩擦）和1（无摩擦）之间。一个典型的动量值是0.9。
- en: Equation 11-5\. Momentum algorithm
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-5\. 动量算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>
- en: You can verify that if the gradient remains constant, the terminal velocity
    (i.e., the maximum size of the weight updates) is equal to that gradient multiplied
    by the learning rate *η* multiplied by 1 / (1 – *β*) (ignoring the sign). For
    example, if *β* = 0.9, then the terminal velocity is equal to 10 times the gradient
    times the learning rate, so momentum optimization ends up going 10 times faster
    than gradient descent! In practice, the gradients are not constant, so the speedup
    is not always as dramatic, but momentum optimization can escape from plateaus
    much faster than regular gradient descent. We saw in [Chapter 4](ch04.html#linear_models_chapter)
    that when the inputs have very different scales, the cost function will look like
    an elongated bowl (see [Figure 4-7](ch04.html#elongated_bowl_diagram)). Gradient
    descent goes down the steep slope quite fast, but then it takes a very long time
    to go down the valley. In contrast, momentum optimization will roll down the valley
    faster and faster until it reaches the bottom (the optimum). In deep neural networks
    that don’t use batch-norm or layer-norm, the upper layers will often end up having
    inputs with very different scales, so using momentum optimization helps a lot.
    It can also help roll past local optima.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以验证，如果梯度保持不变，终端速度（即权重更新的最大大小）等于该梯度乘以学习率**η**乘以1 / (1 – **β**)（忽略符号）。例如，如果**β**
    = 0.9，那么终端速度等于梯度乘以学习率的10倍，因此动量优化最终比梯度下降快10倍！在实践中，梯度并不总是恒定的，所以加速效果并不总是那么显著，但动量优化比常规梯度下降更快地逃离平台期。我们在[第4章](ch04.html#linear_models_chapter)中看到，当输入具有非常不同的尺度时，损失函数将看起来像一个拉长的碗（见[图4-7](ch04.html#elongated_bowl_diagram)）。梯度下降可以相当快地沿着陡峭的斜坡下降，但随后需要很长时间才能下降到山谷。相比之下，动量优化会越来越快地滚动下山谷，直到达到底部（最优）。在未使用批归一化或层归一化的深度神经网络中，上层通常会具有具有非常不同尺度的输入，因此使用动量优化非常有帮助。它还可以帮助越过局部最优。
- en: Note
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot
    again, and oscillate like this many times before stabilizing at the minimum. This
    is one of the reasons why it’s good to have a bit of friction in the system: it
    reduces these oscillations and thus speeds up convergence.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于动量的存在，优化器可能会略微超出最小值，然后返回，再次超出，并像这样多次振荡，最终稳定在最小值。这是在系统中保持一点摩擦的好处之一：它减少了这些振荡，从而加快了收敛速度。
- en: 'Implementing momentum optimization in PyTorch is a no-brainer: just use the
    `SGD` optimizer and set its `momentum` hyperparameter, then sit back and profit!'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现动量优化是一件轻而易举的事情：只需使用`SGD`优化器并设置其`momentum`超参数，然后坐下来享受利润吧！
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The one drawback of momentum optimization is that it adds yet another hyperparameter
    to tune. However, the momentum value of 0.9 usually works well in practice and
    almost always goes faster than regular gradient descent.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化的一个缺点是它又增加了一个需要调整的超参数。然而，在实践中的动量值0.9通常效果很好，并且几乎总是比常规的梯度下降更快。
- en: Nesterov Accelerated Gradient
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nesterov加速梯度
- en: One small variant to momentum optimization, proposed by [Yurii Nesterov in 1983](https://homl.info/55),⁠^([21](ch11.html#id2609))
    is almost always faster than regular momentum optimization. The *Nesterov accelerated
    gradient* (NAG) method, also known as *Nesterov momentum optimization*, measures
    the gradient of the cost function not at the local position **θ** but slightly
    ahead in the direction of the momentum, at **θ** + *β***m** (see [Equation 11-6](#nesterov_momentum_equation)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化的一种小变体，由Yurii Nesterov于1983年提出（[Yurii Nesterov in 1983](https://homl.info/55)，⁠^([21](ch11.html#id2609)）几乎总是比常规动量优化更快。*Nesterov加速梯度*（NAG）方法，也称为*Nesterov动量优化*，不是在局部位置**θ**处测量成本函数的梯度，而是在动量方向上稍微领先的位置，即**θ**
    + *β***m**（见[方程11-6](#nesterov_momentum_equation)）。
- en: Equation 11-6\. Nesterov accelerated gradient algorithm
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-6\. Nesterov加速梯度算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>
- en: This small tweak works because in general the momentum vector will be pointing
    in the right direction (i.e., toward the optimum), so it will be slightly more
    accurate to use the gradient measured a bit farther in that direction rather than
    the gradient at the original position, as you can see in [Figure 11-7](#nesterov_momentum_diagram)
    (where ∇[1] represents the gradient of the cost function measured at the starting
    point **θ**, and ∇[2] represents the gradient at the point located at **θ** +
    *β***m**).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这种小的调整之所以有效，是因为一般来说，动量向量将指向正确的方向（即，指向最优解），因此使用在该方向上稍微远一点的梯度来测量，而不是在原始位置处的梯度，将更加准确，正如你在[图11-7](#nesterov_momentum_diagram)（其中∇[1]代表在起点**θ**处测量的成本函数的梯度，而∇[2]代表在**θ**
    + *β***m**处测量的梯度）中可以看到。
- en: '![Diagram illustrating regular versus Nesterov momentum optimization, showing
    that Nesterov updates by applying gradients after a momentum step, leading to
    improved convergence towards the optimum.](assets/hmls_1107.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![说明常规与Nesterov动量优化的图，显示Nesterov通过在动量步骤之后应用梯度来更新，从而提高了向最优解收敛的速度。](assets/hmls_1107.png)'
- en: 'Figure 11-7\. Regular versus Nesterov momentum optimization: the former applies
    the gradients computed before the momentum step, while the latter applies the
    gradients computed after'
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7\. 常规与Nesterov动量优化：前者应用动量步骤之前的梯度计算，而后者应用动量步骤之后的梯度计算
- en: As you can see, the Nesterov update ends up closer to the optimum. After a while,
    these small improvements add up and NAG ends up being significantly faster than
    regular momentum optimization. Moreover, note that when the momentum pushes the
    weights across a valley, ∇[1] continues to push farther across the valley, while
    ∇[2] pushes back toward the bottom of the valley. This helps reduce oscillations
    and thus NAG converges faster.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Nesterov更新最终会接近最优解。经过一段时间，这些小的改进累积起来，NAG最终会比常规动量优化快得多。此外，请注意，当动量将权重推过山谷时，∇[1]会继续推动更远，而∇[2]则会推动回山谷底部。这有助于减少振荡，因此NAG收敛得更快。
- en: 'To use NAG, simply set `nesterov=True` when creating the `SGD` optimizer:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用NAG，只需在创建`SGD`优化器时设置`nesterov=True`：
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: AdaGrad
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaGrad
- en: 'Consider the elongated bowl problem again: gradient descent starts by quickly
    going down the steepest slope, which does not point straight toward the global
    optimum, then it very slowly goes down to the bottom of the valley. It would be
    nice if the algorithm could correct its direction earlier to point a bit more
    toward the global optimum. The [*AdaGrad* algorithm](https://homl.info/56)⁠^([22](ch11.html#id2615))
    achieves this correction by scaling down the gradient vector along the steepest
    dimensions (see [Equation 11-7](#adagrad_algorithm)).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑拉长的碗问题：梯度下降首先快速沿着最陡的斜坡下降，但这并不直接指向全局最优解，然后它非常缓慢地下降到山谷底部。如果算法能够更早地纠正其方向，稍微指向全局最优解，那就太好了。[*AdaGrad*算法](https://homl.info/56)⁠^([22](ch11.html#id2615))通过沿最陡维度缩放梯度向量来实现这种纠正（参见[方程11-7](#adagrad_algorithm))。
- en: Equation 11-7\. AdaGrad algorithm
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-7\. AdaGrad算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">s</mi> <mo>←</mo> <mi mathvariant="bold">s</mi> <mo>+</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">s</mi> <mo>←</mo> <mi mathvariant="bold">s</mi> <mo>+</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
- en: The first step accumulates the square of the gradients into the vector **s**
    (recall that the ⊗ symbol represents the element-wise multiplication). This vectorized
    form is equivalent to computing *s*[*i*] ← *s*[*i*] + (∂*J*(**θ**) / ∂*θ*[*i*])²
    for each element *s*[*i*] of the vector **s**; in other words, each *s*[*i*] accumulates
    the squares of the partial derivative of the cost function with regard to parameter
    *θ*[*i*]. If the cost function is steep along the *i*^(th) dimension, then *s*[*i*]
    will get larger and larger at each iteration.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将梯度的平方累加到向量**s**中（回想一下，⊗符号代表逐元素乘法）。这种向量形式等同于计算 *s*[*i*] ← *s*[*i*] + (∂*J*(**θ**)
    / ∂*θ*[*i*])² 对于向量**s**中的每个元素 *s*[*i*]；换句话说，每个 *s*[*i*] 累积了关于参数 *θ*[*i*] 的成本函数偏导数的平方。如果成本函数沿着
    *i*^(th) 维度很陡峭，那么 *s*[*i*] 将在每次迭代中越来越大。
- en: 'The second step is almost identical to gradient descent, but with one big difference:
    the gradient vector is scaled down by a factor of <msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt>
    (the ⊘ symbol represents the element-wise division, the square root is also computed
    element-wise, and *ε* is a smoothing term to avoid division by zero, typically
    set to 10^(–10)). This vectorized form is equivalent to simultaneously computing
    <msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo>∂</mo><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo
    lspace="0.25em" rspace="0.25em">/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt>
    for all parameters *θ*[*i*].'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步几乎与梯度下降相同，但有一个重大区别：梯度向量被一个因子<msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt>（⊘符号表示逐元素除法，平方根也是逐元素计算的，*ε*是一个平滑项，用于避免除以零，通常设置为10^(–10)）缩小。这种向量形式相当于同时计算所有参数*θ*[*i*]的<msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo>∂</mo><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo
    lspace="0.25em" rspace="0.25em">/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt>。
- en: In short, this algorithm decays the learning rate, but it does so faster for
    steep dimensions than for dimensions with gentler slopes. This is called an *adaptive
    learning rate*. It helps point the resulting updates more directly toward the
    global optimum (see [Figure 11-8](#adagrad_diagram)). One additional benefit is
    that it requires much less tuning of the learning rate hyperparameter *η*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个算法会降低学习率，但对于斜率较陡的维度，它的降低速度比斜率较缓的维度更快。这被称为**自适应学习率**。它有助于使结果更新更直接地指向全局最优解（见[图11-8](#adagrad_diagram)）。一个额外的优点是它需要调整的学习率超参数*η*的次数要少得多。
- en: 'AdaGrad frequently performs well for simple quadratic problems, but it often
    stops too early when training neural networks: the learning rate gets scaled down
    so much that the algorithm ends up stopping entirely before reaching the global
    optimum.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad在简单的二次问题中通常表现良好，但在训练神经网络时往往过早停止：学习率被缩小得太多，以至于算法在达到全局最优解之前就完全停止了。
- en: '![Diagram comparing AdaGrad and gradient descent, showing AdaGrad adjusting
    direction earlier towards the optimum in steep dimensions.](assets/hmls_1108.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![比较AdaGrad和梯度下降的图表，显示AdaGrad在斜率较陡的维度上更早调整方向。](assets/hmls_1108.png)'
- en: 'Figure 11-8\. AdaGrad versus gradient descent: the former can correct its direction
    earlier to point to the optimum'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. AdaGrad与梯度下降：前者可以更早地纠正方向，指向最优解
- en: So even though PyTorch has an `Adagrad` optimizer, you should not use it to
    train deep neural networks (it may be efficient for simpler tasks such as linear
    regression, though). Still, understanding AdaGrad is helpful to comprehend the
    other adaptive learning rate optimizers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管PyTorch有一个`Adagrad`优化器，但你不应将其用于训练深度神经网络（尽管它可能对线性回归等简单任务效率较高）。然而，了解AdaGrad对于理解其他自适应学习率优化器是有帮助的。
- en: RMSProp
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSProp
- en: As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never
    converging to the global optimum. The *RMSProp* algorithm⁠^([23](ch11.html#id2619))
    fixes this by accumulating only the gradients from the most recent iterations,
    as opposed to all the gradients since the beginning of training. It does so by
    using exponential decay in the first step (see [Equation 11-8](#rmsprop_algorithm)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AdaGrad存在风险，可能会过快地减慢速度，并且永远不会收敛到全局最优解。*RMSProp*算法⁠^([23](ch11.html#id2619))通过仅累积最近迭代的梯度来解决这个问题，而不是从训练开始以来的所有梯度。它是通过在第一步中使用指数衰减来实现的（见[公式11-8](#rmsprop_algorithm)）。
- en: Equation 11-8\. RMSProp algorithm
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式11-8\. RMSProp算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">s</mi> <mo>←</mo> <mi>α</mi> <mi mathvariant="bold">s</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">s</mi> <mo>←</mo> <mi>α</mi> <mi mathvariant="bold">s</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
- en: The decay rate *α* is typically set to 0.9\. Yes, it is once again a new hyperparameter,
    but this default value often works well, so you may not need to tune it at all.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减率 *α* 通常设置为 0.9。是的，这又是一个新的超参数，但这个默认值通常效果很好，所以你可能根本不需要调整它。
- en: 'As you might expect, PyTorch has an `RMSprop` optimizer:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，PyTorch有一个 `RMSprop` 优化器：
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Except on very simple problems, this optimizer almost always performs much better
    than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers
    until Adam optimization came around.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在非常简单的问题上，这个优化器几乎总是比AdaGrad表现得更好。事实上，在Adam优化出现之前，它是许多研究人员的首选优化算法。
- en: Adam
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam
- en: '[*Adam*](https://homl.info/59),⁠^([24](ch11.html#id2626)) which stands for
    *adaptive moment estimation*, combines the ideas of momentum optimization and
    RMSProp: just like momentum optimization, it keeps track of an exponentially decaying
    average of past gradients; and just like RMSProp, it keeps track of an exponentially
    decaying average of past squared gradients (see [Equation 11-9](#adam_algorithm)).
    These are estimations of the mean and (uncentered) variance of the gradients.
    The mean is often called the *first moment*, while the variance is often called
    the *second moment*, hence the name of the algorithm.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Adam*](https://homl.info/59),⁠^([24](ch11.html#id2626)) 代表 *自适应动量估计*，它结合了动量优化和RMSProp的思想：就像动量优化一样，它跟踪过去梯度的指数衰减平均值；就像RMSProp一样，它跟踪过去平方梯度的指数衰减平均值（见[方程11-9](#adam_algorithm)）。这些都是梯度的均值和（未中心化）方差的估计。均值通常被称为
    *一阶矩*，而方差通常被称为 *二阶矩*，因此算法得名。'
- en: Equation 11-9\. Adam algorithm
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-9\. Adam算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi> <mo>←</mo> <msub><mi>β</mi>
    <mn>2</mn></msub> <mi mathvariant="bold">s</mi> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>←</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">m</mi>
    <mrow><mn>1</mn> <mo>-</mo> <msup><mrow><msub><mi>β</mi> <mn>1</mn></msub></mrow>
    <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>4</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover> <mo>←</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">s</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi>
    <mn>2</mn></msub></mrow> <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>5</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi>η</mi> <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>⊘</mo> <msqrt><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi> <mo>←</mo> <msub><mi>β</mi>
    <mn>2</mn></msub> <mi mathvariant="bold">s</mi> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover> <mo>←</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">m</mi> <mrow><mn>1</mn>
    <mo>-</mo> <msup><mrow><msub><mi>β</mi> <mn>1</mn></msub></mrow> <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
    <mo>←</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">s</mi>
    <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>5</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi>η</mi> <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>⊘</mo> <msqrt><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>
- en: In this equation, *t* represents the iteration number (starting at 1).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*t* 代表迭代次数（从 1 开始）。
- en: 'If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity
    to both momentum optimization and RMSProp: *β*[1] corresponds to *β* in momentum
    optimization, and *β*[2] corresponds to *α* in RMSProp. The only difference is
    that step 1 computes an exponentially decaying average rather than an exponentially
    decaying sum, but these are actually equivalent except for a constant factor (the
    decaying average is just 1 – *β*[1] times the decaying sum). Steps 3 and 4 are
    somewhat of a technical detail: since **m** and **s** are initialized at 0, they
    will be biased toward 0 at the beginning of training, so these two steps will
    help boost **m** and **s** at the beginning of training.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只看步骤 1、2 和 5，你会注意到 Adam 与动量优化和 RMSProp 非常相似：*β*[1] 对应于动量优化中的 *β*，*β*[2] 对应于
    RMSProp 中的 *α*。唯一的区别是步骤 1 计算的是指数衰减的平均值而不是指数衰减的和，但实际上除了一个常数因子外，这些是等价的（衰减平均只是 1
    – *β*[1] 倍的衰减和）。步骤 3 和 4 是一种技术细节：由于 **m** 和 **s** 初始化为 0，它们在训练开始时会偏向于 0，因此这两个步骤将有助于在训练开始时提升
    **m** 和 **s**。
- en: 'The momentum decay hyperparameter *β*[1] is typically initialized to 0.9, while
    the scaling decay hyperparameter *β*[2] is often initialized to 0.999\. As earlier,
    the smoothing term *ε* is usually initialized to a tiny number such as 10^(–8).
    These are the default values for the `Adam` class. Here is how to create an Adam
    optimizer using PyTorch:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 动量衰减超参数 *β*[1] 通常初始化为 0.9，而缩放衰减超参数 *β*[2] 通常初始化为 0.999。同样，平滑项 *ε* 通常初始化为一个很小的数字，例如
    10^(–8)。这些是 `Adam` 类的默认值。以下是使用 PyTorch 创建 Adam 优化器的示例：
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Since Adam is an adaptive learning rate algorithm, like AdaGrad and RMSProp,
    it requires less tuning of the learning rate hyperparameter *η*. You can often
    use the default value *η* = 0.001, making Adam even easier to use than gradient
    descent.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Adam 是一种自适应学习率算法，类似于 AdaGrad 和 RMSProp，它需要较少的学习率超参数 *η* 的调整。你通常可以使用默认值 *η*
    = 0.001，这使得 Adam 比梯度下降更容易使用。
- en: Tip
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you are starting to feel overwhelmed by all these different techniques and
    are wondering how to choose the right ones for your task, don’t worry: some practical
    guidelines are provided at the end of this chapter.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始觉得所有这些不同的技术让你感到不知所措，并想知道如何为你的任务选择正确的技术，请不要担心：本章末尾提供了一些实用的指南。
- en: 'Finally, three variants of Adam are worth mentioning: AdaMax, NAdam, and AdamW.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得提一下 Adam 的三种变体：AdaMax、NAdam 和 AdamW。
- en: AdaMax
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaMax
- en: The Adam paper also introduced AdaMax. Notice that in step 2 of [Equation 11-9](#adam_algorithm),
    Adam accumulates the squares of the gradients in **s** (with a greater weight
    for more recent gradients). In step 5, if we ignore *ε* and steps 3 and 4 (which
    are technical details anyway), Adam scales down the parameter updates by the square
    root of **s**. In short, Adam scales down the parameter updates by the ℓ[2] norm
    of the time-decayed gradients (recall that the ℓ[2] norm is the square root of
    the sum of squares).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 论文还介绍了 AdaMax。注意，在 [方程 11-9](#adam_algorithm) 的第二步中，Adam 累积梯度的平方到 **s**
    中（对较近的梯度赋予更大的权重）。在第五步中，如果我们忽略 *ε* 以及第三步和第四步（这些毕竟是技术细节），Adam 会通过 **s** 的平方根来缩小参数更新的规模。简而言之，Adam
    通过时间衰减梯度的 ℓ[2] 范数来缩小参数更新（回想一下，ℓ[2] 范数是平方和的平方根）。
- en: AdaMax replaces the ℓ[2] norm with the ℓ[∞] norm (a fancy way of saying the
    max). Specifically, it replaces step 2 in [Equation 11-9](#adam_algorithm) with
    <mi mathvariant="bold">s</mi><mo>←</mo><mpadded lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="bold">s</mi><mo lspace="0%" rspace="0%">,</mo> <mtext>abs(</mtext><msub><mo
    mathvariant="bold">∇</mo><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded>, it drops step
    4, and in step 5 it scales down the gradient updates by a factor of **s**, which
    is the max of the absolute value of the time-decayed gradients.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: AdaMax 将 ℓ[2] 范数替换为 ℓ[∞] 范数（一种更复杂的说法，即最大值）。具体来说，它将 [方程 11-9](#adam_algorithm)
    中的第二步替换为 <mi mathvariant="bold">s</mi><mo>←</mo><mpadded lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="bold">s</mi><mo lspace="0%" rspace="0%">,</mo> <mtext>abs(</mtext><msub><mo
    mathvariant="bold">∇</mo><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded>，它省略了第四步，并在第五步中将梯度更新缩小到
    **s** 的倍数，其中 **s** 是时间衰减梯度的绝对值中的最大值。
- en: In practice, this can make AdaMax more stable than Adam, but it really depends
    on the dataset, and in general Adam performs better. So, this is just one more
    optimizer you can try if you experience problems with Adam on some task.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这可以使AdaMax比Adam更稳定，但实际上这取决于数据集，通常Adam的表现更好。因此，如果你在某些任务上遇到Adam的问题，这只是一个你可以尝试的更多优化器之一。
- en: NAdam
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NAdam
- en: NAdam optimization is Adam optimization plus the Nesterov trick, so it will
    often converge slightly faster than Adam. In his report introducing this technique,⁠^([25](ch11.html#id2644))
    the researcher Timothy Dozat compares many different optimizers on various tasks
    and finds that NAdam generally outperforms Adam but is sometimes outperformed
    by RMSProp.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: NAdam优化是Adam优化加上Nesterov技巧，因此它通常会比Adam收敛得稍快。在介绍这项技术的报告中⁠^([25](ch11.html#id2644))，研究人员Timothy
    Dozat比较了各种任务上的许多不同优化器，并发现NAdam通常优于Adam，但有时会被RMSProp超越。
- en: AdamW
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdamW
- en: '[AdamW](https://homl.info/adamw)⁠^([26](ch11.html#id2645)) is a variant of
    Adam that integrates a regularization technique called *weight decay*. Weight
    decay reduces the size of the model’s weights at each training iteration by multiplying
    them by a decay factor such as 0.99\. This may remind you of ℓ[2] regularization
    (introduced in [Chapter 4](ch04.html#linear_models_chapter)), which also aims
    to keep the weights small, and indeed it can be shown mathematically that ℓ[2]
    regularization is equivalent to weight decay when using SGD. However, when using
    Adam or its variants, ℓ[2] regularization and weight decay are *not* equivalent:
    in practice, combining Adam with ℓ[2] regularization results in models that often
    don’t generalize as well as those produced by SGD. AdamW fixes this issue by properly
    combining Adam with weight decay.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdamW](https://homl.info/adamw)⁠^([26](ch11.html#id2645))是Adam的一个变体，它集成了称为**权重衰减**的正则化技术。权重衰减通过乘以衰减因子（如0.99）来减少每个训练迭代中模型权重的尺寸。这可能会让你想起ℓ[2]正则化（在第4章中介绍），它也旨在保持权重较小，实际上可以数学上证明ℓ[2]正则化在使用SGD时等同于权重衰减。然而，当使用Adam或其变体时，ℓ[2]正则化和权重衰减**并不**等价：在实践中，将Adam与ℓ[2]正则化结合使用会导致模型通常不如SGD生成的模型泛化得好。AdamW通过正确地将Adam与权重衰减结合来解决这个问题。'
- en: Warning
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Adaptive optimization methods (including RMSProp, Adam, AdaMax, NAdam, and
    AdamW optimization) are often great, converging fast to a good solution. However,
    a [2017 paper](https://homl.info/60)⁠^([27](ch11.html#id2650)) by Ashia C. Wilson
    et al. showed that they can lead to solutions that generalize poorly on some datasets.
    So when you are disappointed by your model’s performance, try using NAG instead:
    your dataset may just be allergic to adaptive gradients.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应优化方法（包括RMSProp、Adam、AdaMax、NAdam和AdamW优化）通常非常好，快速收敛到良好的解。然而，Ashia C. Wilson等人2017年的一篇论文⁠^([27](ch11.html#id2650))表明，它们可能导致在某些数据集上泛化不良的解。因此，当你对模型的表现感到失望时，尝试使用NAG：你的数据集可能只是对自适应梯度有过敏反应。
- en: To use NAdam, AdaMax, or AdamW in PyTorch, replace `torch.optim.Adam` with `torch.optim.NAdam`,
    `torch.optim.Adamax`, or `torch.optim.AdamW`. For AdamW, you probably want to
    tune the `weight_decay` hyperparameter.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中使用NAdam、AdaMax或AdamW，将`torch.optim.Adam`替换为`torch.optim.NAdam`、`torch.optim.Adamax`或`torch.optim.AdamW`。对于AdamW，你可能想要调整`weight_decay`超参数。
- en: All the optimization techniques discussed so far only rely on the *first-order
    partial derivatives* (*Jacobians*, which measure the slope of the loss function
    along each axis). The optimization literature also contains amazing algorithms
    based on the *second-order partial derivatives* (the *Hessians*, which are the
    partial derivatives of the Jacobians, measuring how each Jacobian changes along
    each axis; in other words, measuring the loss function’s curvature).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的所有优化技术都只依赖于**一阶偏导数**（雅可比矩阵，它测量损失函数沿每个轴的斜率）。优化文献中还包含基于**二阶偏导数**（海森矩阵，它是雅可比矩阵的偏导数，衡量每个雅可比矩阵沿每个轴的变化；换句话说，衡量损失函数的曲率）的惊人算法。
- en: Unfortunately, these Hessian-based algorithms are hard to apply directly to
    deep neural networks because there are *n*² second-order derivatives per output
    (where *n* is the number of parameters), as opposed to just *n* first-order derivatives
    per output. Since DNNs typically have hundreds of thousands of parameters or more,
    the second-order optimization algorithms often don’t even fit in memory, and even
    when they do, computing the *Hessian matrix* is just too slow.⁠^([28](ch11.html#id2656))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，这些基于Hessian的算法难以直接应用于深度神经网络，因为每个输出有 *n*² 个二阶导数（其中 *n* 是参数的数量），而每个输出只有 *n*
    个一阶导数。由于DNN通常有数万个参数或更多，二阶优化算法通常甚至无法适应内存，即使它们可以，计算 *Hessian 矩阵* 也太慢了。⁠^([28](ch11.html#id2656))
- en: Luckily, it is possible to use stochastic methods that can efficiently approximate
    second-order information. One such algorithm is Shampoo,⁠^([29](ch11.html#id2658))
    which uses accumulated gradient information to approximate the second-order terms,
    similar to how Adam accumulates first-order statistics. It is not included in
    the PyTorch library, but you can get it in the PyTorch-Optimizer library (`pip
    install torch_optimizer`).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，可以使用可以有效地近似二阶信息的随机方法。其中一种算法是洗发水算法，⁠^([29](ch11.html#id2658)) 它使用累积梯度信息来近似二阶项，类似于Adam累积一阶统计信息。它不包括在PyTorch库中，但您可以在PyTorch-Optimizer库中获取它（`pip
    install torch_optimizer`）。
- en: '[Table 11-2](#optimizer_summary_table) compares all the optimizers we’ve discussed
    so far (![](assets/star_2b50.png) is bad, ![](assets/star_2b50.png)![](assets/star_2b50.png)
    is average, and ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    is good).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11-2](#optimizer_summary_table)比较了我们迄今为止讨论的所有优化器（![星星](assets/star_2b50.png)表示不好，![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)表示一般，![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)表示好）。'
- en: Table 11-2\. Optimizer comparison
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-2\. 优化器比较
- en: '| Class | Convergence speed | Convergence quality |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 收敛速度 | 收敛质量 |'
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `SGD` | ![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `SGD` | ![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `SGD(momentum=...)` | ![](assets/star_2b50.png)![](assets/star_2b50.png)
    | ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `SGD(momentum=...)` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `SGD(momentum=..., nesterov=True)` | ![](assets/star_2b50.png)![](assets/star_2b50.png)
    | ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `SGD(momentum=..., nesterov=True)` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `Adagrad` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png) (stops
    too early) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `Adagrad` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png) (过早停止)
    |'
- en: '| `RMSprop` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `RMSprop` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) 
    或  ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `Adam` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| `Adam` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) 
    或  ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `AdaMax` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| `AdaMax` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) 
    或  ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `NAdam` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| `NAdam` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) 
    或  ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: '| `AdamW` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| `AdamW` | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) | ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png) 
    或  ![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)![星星](assets/star_2b50.png)
    |'
- en: Learning Rate Scheduling
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率调度
- en: 'Finding a good learning rate is very important. If you set it too high, training
    will diverge (as discussed in [“Gradient Descent”](ch04.html#gradientDescent4)).
    If you set it too low, then training will be painfully slow, and it may also get
    stuck in a local optimum and produce a suboptimal model. If you set the learning
    rate fairly high (but not high enough to diverge), then training will often make
    rapid progress at first, but it will end up dancing around the optimum toward
    the end of training and thereby produce a suboptimal model. If you find a really
    good learning rate, you can end up with an excellent model, but training will
    generally be a bit too slow. Luckily, you can do better than a constant learning
    rate. In particular, it’s a good idea to start with a fairly high learning rate
    and then reduce it toward the end of training (or whenever progress stops): this
    ensures that training starts fast, while also allowing backprop to settle down
    toward the end to really fine-tune the model parameters (see [Figure 11-9](#learning_schedule_diagram)).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个好的学习率非常重要。如果你设置得太高，训练会发散（如[“梯度下降”](ch04.html#gradientDescent4)中讨论的）。如果你设置得太低，那么训练会非常缓慢，它也可能陷入局部最优并产生次优模型。如果你设置的学习率相当高（但不足以发散），那么训练最初通常会取得快速进展，但最终会在训练结束时围绕最优解摇摆，从而产生次优模型。如果你找到一个真正好的学习率，你可能会得到一个非常好的模型，但训练通常会比较慢。幸运的是，你可以做得比恒定学习率更好。特别是，从相当高的学习率开始，然后在训练结束时（或进展停止时）降低它是一个好主意：这确保了训练开始得快，同时也允许反向传播在训练结束时稳定下来，以真正微调模型参数（参见[图
    11-9](#learning_schedule_diagram)）。
- en: There are various other strategies to tweak the learning rate during training.
    These are called *learning schedules* (I briefly introduced this concept in [Chapter 4](ch04.html#linear_models_chapter)).
    The `torch.optim.lr_scheduler` module provides several implementations of common
    learning schedules. Let’s look at the most important ones, starting with exponential
    scheduling.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间调整学习率的策略有很多。这些被称为**学习计划**（我在[第 4 章](ch04.html#linear_models_chapter)中简要介绍了这个概念）。`torch.optim.lr_scheduler`
    模块提供了几种常见学习计划的实现。让我们看看最重要的几个，从指数调度开始。
- en: '![Diagram illustrating the effects of different learning rates on loss over
    epochs, showing that starting with a high rate and reducing it achieves optimal
    training results.](assets/hmls_1109.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![说明不同学习率对损失随 epoch 变化的影响，显示从高率开始并降低它可以达到最佳训练结果。](assets/hmls_1109.png)'
- en: Figure 11-9\. Learning curves for various learning rates η
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-9\. 各种学习率 η 的学习曲线
- en: Exponential Scheduling
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指数调度
- en: The `ExponentialLR` class implements *exponential scheduling*, whereby the learning
    rate is multiplied by a constant factor `gamma` at some regular interval, typically
    at every epoch. As a result, after the *n*^(th) epoch, the learning rate will
    be equal to the initial learning rate times `gamma` to the power of *n*. This
    factor `gamma` is yet another hyperparameter you can tune. In general, you will
    want to set `gamma` to a value lower than 1, but fairly close to 1 to avoid decreasing
    the learning rate too fast. For example, if `gamma` is set to 0.9, then after
    10 epochs the learning rate will be about 35% of the initial learning rate, and
    after 20 epochs it will be about 12%.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExponentialLR` 类实现了**指数调度**，即学习率在某个固定间隔内乘以一个常数因子 `gamma`，通常是在每个 epoch 末进行。因此，在第
    *n* 个 epoch 之后，学习率将等于初始学习率乘以 `gamma` 的 *n* 次幂。这个因子 `gamma` 是另一个可以调整的超参数。一般来说，你会希望将
    `gamma` 设置为一个小于 1 但接近 1 的值，以避免学习率下降过快。例如，如果 `gamma` 设置为 0.9，那么在 10 个 epoch 后，学习率将大约是初始学习率的
    35%，而在 20 个 epoch 后将大约是 12%。'
- en: 'The `ExponentialLR` constructor expects at least two arguments—the optimizer
    whose learning rate will be tweaked during training, and the factor `gamma`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExponentialLR` 构造函数至少需要两个参数——将在训练期间调整其学习率的优化器，以及因子 `gamma`：'
- en: '[PRE20]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, you must update the training loop to call `scheduler.step()` at the end
    of each epoch to tweak the optimizer’s learning rate:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你必须更新训练循环，在每个 epoch 末调用 `scheduler.step()` 来调整优化器的学习率：
- en: '[PRE21]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you interrupt training and you later want to resume it where you left off,
    you should set the `last_epoch` argument of the scheduler’s constructor to the
    last epoch you ran (zero-indexed). The default is –1, which makes the scheduler
    start training from scratch.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你中断了训练，并且后来想从你停止的地方继续，你应该将调度器构造函数的 `last_epoch` 参数设置为最后一个运行的 epoch（从零开始计数）。默认值是
    –1，这使得调度器从头开始训练。
- en: Cosine Annealing
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 余弦退火
- en: Instead of decreasing the learning rate exponentially, you can use the cosine
    function to go from the maximum learning rate *η*[max] at the start of training,
    down to the minimum learning rate *η*[min] at the end. This is called *cosine
    annealing*. Compared to exponential scheduling, cosine annealing ensures that
    the learning rate remains fairly high during most of training, while getting closer
    to the minimum near the end (see [Figure 11-10](#cosine_annealing_diagram)). All
    in all, cosine annealing generally performs better. The learning rate at epoch
    *t* (zero-indexed) is given by [Equation 11-10](#cosine_annealing_equation), where
    *T*[max] is the maximum number of epochs.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与指数下降学习率不同，你可以使用余弦函数从训练开始的最高学习率 *η*[max] 下降到训练结束的最小学习率 *η*[min]。这被称为 *余弦退火*。与指数调度相比，余弦退火确保学习率在训练的大部分时间内保持相对较高，而在接近结束时逐渐接近最小值（见[图11-10](#cosine_annealing_diagram)）。总的来说，余弦退火通常表现更好。在时间步
    *t*（零索引）的学习率由[方程11-10](#cosine_annealing_equation)给出，其中 *T*[max] 是最大时间步数。
- en: '![A diagram comparing cosine and exponential learning rate schedules, showing
    the cosine schedule maintaining a higher rate over more epochs before decreasing.](assets/hmls_1110.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![比较余弦和指数学习率调度的图表，显示余弦调度在更多时间步内保持较高速率，然后降低。](assets/hmls_1110.png)'
- en: Figure 11-10\. Cosine annealing learning schedule
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-10。余弦退火学习调度
- en: Equation 11-10\. Cosine annealing equation
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-10。余弦退火方程
- en: <mrow><msub><mi>η</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>η</mi> <mtext>min</mtext></msub>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>(</mo> <msub><mi>η</mi>
    <mtext>max</mtext></msub> <mo>-</mo> <msub><mi>η</mi> <mtext>min</mtext></msub>
    <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>+</mo>
    <mo form="prefix">cos</mo> <mfenced separators="" open="(" close=")"><mfrac><mi>t</mi>
    <msub><mi>T</mi> <mtext>max</mtext></msub></mfrac> <mi>π</mi></mfenced></mfenced></mrow>
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><msub><mi>η</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>η</mi> <mtext>min</mtext></msub>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>(</mo> <msub><mi>η</mi>
    <mtext>max</mtext></msub> <mo>-</mo> <msub><mi>η</mi> <mtext>min</mtext></msub>
    <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>+</mo>
    <mo form="prefix">cos</mo> <mfenced separators="" open="(" close=")"><mfrac><mi>t</mi>
    <msub><mi>T</mi> <mtext>max</mtext></msub></mfrac> <mi>π</mi></mfenced></mfenced></mrow>`'
- en: 'PyTorch includes the `CosineAnnealingLR` scheduler, which you can create as
    follows (`T_max` is *T*[max] and `eta_min` is *η*[min]). You can then use it just
    like the `ExponentialLR` scheduler:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 包含 `CosineAnnealingLR` 调度器，你可以按照以下方式创建它（`T_max` 是 *T*[max]，`eta_min`
    是 *η*[min]）。然后你可以像使用 `ExponentialLR` 调度器一样使用它：
- en: '[PRE22]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: One problem with cosine annealing is that you have to set two new hyperparameters,
    *T*[max] and *η*[min], and it’s not easy to know in advance how many epochs to
    train and when to stop decreasing the learning rate. This is why I generally prefer
    to use the performance scheduling technique.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦退火的一个问题是，你必须设置两个新的超参数，*T*[max] 和 *η*[min]，并且很难事先知道训练需要多少时间步数以及何时停止降低学习率。这就是为什么我通常更喜欢使用性能调度技术。
- en: Performance Scheduling
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能调度
- en: '*Performance scheduling*, also called *adaptive scheduling*, is implemented
    by PyTorch’s `ReduceLROnPlateau` scheduler: it keeps track of a given metric during
    training—typically the validation loss—and if this metric stops improving for
    some time, it multiplies the learning rate by some factor. This scheduler has
    quite a few hyperparameters, but the default values work well for most of them.
    You may occasionally need to tweak the following (see the documentation for information
    on the other hyperparameters):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能调度*，也称为 *自适应调度*，由 PyTorch 的 `ReduceLROnPlateau` 调度器实现：它在训练过程中跟踪一个给定的指标——通常是验证损失——如果这个指标在一段时间内停止改进，它将学习率乘以某个因子。这个调度器有几个超参数，但默认值对大多数情况都很好。你偶尔可能需要调整以下内容（有关其他超参数的信息，请参阅文档）：'
- en: '`mode`'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`mode`'
- en: If the tracked metric must be maximized (such as the validation accuracy), then
    you must set the `mode` to `'max'`. The default is `'min'`, which is fine if the
    tracked metric must be minimized (such as the validation loss).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果跟踪的指标必须最大化（例如验证准确率），则必须将 `mode` 设置为 `'max'`。默认为 `'min'`，如果跟踪的指标必须最小化（例如验证损失），则这是可以接受的。
- en: '`patience`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`patience`'
- en: The number of consecutive steps (typically epochs) to wait for improvement in
    the monitored metric before reducing the learning rate. It defaults to 10, which
    is generally fine. If each epoch is very long, then you may want to reduce this
    value.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在降低学习率之前，等待监控指标改进的连续步数（通常是时间步数）。默认为10，这通常是可以接受的。如果每个时间步非常长，那么你可能想降低这个值。
- en: '`factor`'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`factor`'
- en: The factor by which the learning rate will be multiplied whenever the monitored
    metric fails to improve for too long. It defaults to 0.1, again a reasonable default,
    but perhaps a bit small in some cases.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当监控的指标长时间没有改善时，学习率将被乘以的系数。它默认为0.1，再次是一个合理的默认值，但在某些情况下可能有点小。
- en: 'For example, let’s implement performance scheduling based on the validation
    accuracy (i.e., which we want to maximize):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们实现基于验证准确度的性能调度（即，我们希望最大化的指标）：
- en: '[PRE23]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The training loop needs to be tweaked again because we must evaluate the desired
    metric at each epoch (in this example, we are using the `evaluate_tm()` function
    that we defined in [Chapter 10](ch10.html#pytorch_chapter)), and we must then
    pass the result to the scheduler’s `step()` method:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环需要再次调整，因为我们必须在每个epoch评估所需的指标（在这个例子中，我们使用的是我们在[第10章](ch10.html#pytorch_chapter)中定义的`evaluate_tm()`函数），然后我们必须将结果传递给调度器的`step()`方法：
- en: '[PRE24]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Warming Up the Learning Rate
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预热学习率
- en: So far, we have always started training with the maximum learning rate. However,
    this can sometimes cause gradient descent to bounce around randomly at the beginning
    of training, neither exploding nor making any significant progress. This typically
    happens with sensitive models, such as recurrent neural networks ([Chapter 13](ch13.html#rnn_chapter)),
    or when using a very large batch size. In such cases, one solution is to “warm
    up” the learning rate, starting close to zero and gradually increasing the learning
    rate over a few epochs, up to the maximum learning rate. During this warm-up phase,
    gradient descent has time to stabilize into a better region of the loss landscape,
    where it can then make quick progress using a high learning rate.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们总是以最大学习率开始训练。然而，这有时会导致梯度下降在训练开始时随机弹跳，既不会爆炸也不会取得任何显著的进展。这种情况通常发生在敏感的模型上，例如循环神经网络([第13章](ch13.html#rnn_chapter))，或者当使用非常大的批量大小时。在这种情况下，一种解决方案是“预热”学习率，从接近零开始，并在几个epoch内逐渐增加学习率，直到最大学习率。在这个预热阶段，梯度下降有足够的时间稳定到一个更好的损失地形区域，然后它可以使用高学习率快速进步。
- en: 'Why does this work? Well, the loss landscape sometimes resembles the Himalayas:
    it’s very high up and full of gigantic spikes. If you start with a high learning
    rate, you might jump from one mountain peak to the next for a very long time.
    If instead you start with a small learning rate, you will just walk down the mountain
    and valleys and escape the spiky mountain range altogether until you reach flatter
    lands. From then on, you can use a large learning rate for the rest of your journey,
    slowing down only toward the end.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这会起作用？好吧，损失地形有时类似于喜马拉雅山脉：它非常高，充满了巨大的尖峰。如果您以高学习率开始，您可能会长时间从一个山峰跳到另一个山峰。如果您相反地以低学习率开始，您将只是沿着山和山谷走下去，并完全避开尖峰山脉，直到您到达更平坦的土地。从那时起，您可以在剩余的旅途中使用高学习率，只在结束时减慢速度。
- en: 'A common way to implement learning rate warm up using PyTorch is to use a `LinearLR`
    scheduler to increase the learning rate linearly over a few epochs. For example,
    the following scheduler will increase the learning rate from 10% to 100% of the
    optimizer’s original learning rate over 3 epochs (i.e., 10% during the first epoch,
    40% during the second epoch, 70% during the third epoch, and 100% after that):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch实现学习率预热的一种常见方法是使用`LinearLR`调度器在几个epoch内线性增加学习率。例如，以下调度器将在3个epoch内将学习率从优化器的原始学习率的10%增加到100%（即第一个epoch为10%，第二个epoch为40%，第三个epoch为70%，之后为100%）：
- en: '[PRE25]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you would like more flexibility, you can write your own custom function
    and wrap it in a `LambdaLR` scheduler. For example, the following scheduler is
    equivalent to the `LinearLR` scheduler we just defined:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要更多的灵活性，您可以编写自己的自定义函数，并将其包装在`LambdaLR`调度器中。例如，以下调度器与我们刚刚定义的`LinearLR`调度器等效：
- en: '[PRE26]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You must then insert `warmup_scheduler.step()` at the beginning of each epoch,
    and make sure you deactivate the scheduler(s) you are using for the rest of training
    during the warm-up phase. And that’s all!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须在每个epoch的开始处插入`warmup_scheduler.step()`，并确保在预热阶段关闭您用于其余训练的调度器。就这样！
- en: '[PRE27]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In short, you pretty much always want to cool down the learning rate at the
    end of training, and you may also want to warm it up at the beginning if gradient
    descent needs a bit of help getting started. But are there any cases where you
    may want to tweak the learning rate in the middle of training? Well yes, there
    are; for example, if gradient descent gets stuck in a local optimum or a high
    plateau. Gradient descent could remain stuck here for a long time, or even forever.
    Luckily, there’s a way to escape this trap: just increase the learning rate for
    a little while.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你几乎总是希望在训练结束时降低学习率，如果你希望梯度下降在开始时需要一点帮助，你也许还希望在开始时预热学习率。但是，在训练过程中，你有没有可能想要调整学习率？嗯，是的，有；例如，如果梯度下降陷入局部最优或高平台期。梯度下降可能会在这里卡住很长时间，甚至永远。幸运的是，有一种方法可以摆脱这个陷阱：只需暂时增加学习率。
- en: You could spend your time staring at the learning curves during training, and
    manually interrupting it to tweak the learning rate when needed, but you probably
    have better things to do. Alternatively, you could implement a custom scheduler
    that monitors the validation metric—much like the `ReduceLROnPlateau` scheduler—and
    increases the learning rate for a while if the validation metric is stuck in a
    bad plateau. For this, you could subclass the `LRScheduler` base class. This is
    beyond the scope of this book, but you can take inspiration from the `ReduceLROnPlateau`
    scheduler’s source code (and get a little bit of help from your favorite AI assistant).
    But a much simpler option is to use the cosine annealing with warm restarts learning
    schedule. Let’s look at it now.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在训练过程中花费时间盯着学习曲线，并在需要时手动中断它来调整学习率，但你可能还有更好的事情要做。或者，你可以实现一个自定义的调度器，监控验证指标——就像
    `ReduceLROnPlateau` 调度器一样——如果验证指标陷入一个糟糕的平台期，就暂时增加学习率。为此，你可以从 `LRScheduler` 基类中派生。这超出了本书的范围，但你可以从
    `ReduceLROnPlateau` 调度器的源代码中获取灵感（并从你最喜欢的 AI 助手中获得一些帮助）。但一个更简单的选项是使用带有预热重启的余弦退火学习计划。现在让我们来看看它。
- en: Cosine Annealing with Warm Restarts
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有预热重启的余弦退火
- en: '*Cosine annealing with warm restarts* was introduced in a [2016 paper](https://homl.info/coslr)
    by Ilya Loshchilov and Frank Hutter.⁠^([30](ch11.html#id2674)) This schedule just
    repeats the cosine annealing schedule over and over again. Since the learning
    rate regularly shoots back up, this schedule allows gradient descent to escape
    local optima and plateaus automatically. The authors recommend starting with a
    fairly short round of cosine annealing, but then doubling *T*[max] after each
    round (see [Figure 11-11](#cosine_annealing_warm_restarts_diagram)). This allows
    gradient descent to do a lot of quick explorations at the start of training, while
    also taking the time to properly optimize the model later during training, possibly
    escaping a plateau or two along the way.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*带有预热重启的余弦退火* 是由 Ilya Loshchilov 和 Frank Hutter 在 2016 年的一篇论文中提出的。[2016 paper](https://homl.info/coslr)⁠^([30](ch11.html#id2674))
    这个调度计划只是反复执行余弦退火调度计划。由于学习率通常会定期反弹，这个调度计划允许梯度下降自动逃离局部最优和平台期。作者建议从一段相当短的余弦退火回合开始，但在每个回合后加倍
    *T*[max]（参见 [图 11-11](#cosine_annealing_warm_restarts_diagram)）。这允许梯度下降在训练开始时进行大量的快速探索，同时也留出时间在训练后期适当优化模型，可能在这个过程中逃离一个或两个平台期。'
- en: '![Diagram illustrating the cosine annealing with warm restarts schedule, showing
    fluctuations in learning rate with repeated cycles increasing in length.](assets/hmls_1111.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![说明带有预热重启的余弦退火调度计划的图，显示学习率在重复周期中长度增加时的波动。](assets/hmls_1111.png)'
- en: Figure 11-11\. Cosine annealing with warm restarts
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-11\. 带有预热重启的余弦退火
- en: 'Conveniently, PyTorch includes a `CosineAnnealingWarmRestarts` scheduler. You
    must set `T_0`, which is the value of *T*[max] for the first round of cosine annealing.
    You may also set `T_mult` to 2 if you want to double *T*[max] at each round (the
    default is 1, meaning *T*[max] stays constant and all rounds have the same length).
    Finally, you can set `eta_min` (it defaults to 0):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，PyTorch 包含一个 `CosineAnnealingWarmRestarts` 调度器。你必须设置 `T_0`，这是第一次余弦退火中 *T*[max]
    的值。如果你想在每个回合将 *T*[max] 加倍，也可以设置 `T_mult` 为 2（默认值为 1，意味着 *T*[max] 保持不变，所有回合长度相同）。最后，你可以设置
    `eta_min`（默认值为 0）：
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 1cycle Scheduling
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1cycle 调度
- en: Yet another popular learning schedule is *1cycle*, introduced in a [2018 paper](https://homl.info/1cycle)
    by Leslie Smith.⁠^([31](ch11.html#id2677)) It starts by warming up the learning
    rate, starting at *η*[0] and growing linearly up to *η*[1] halfway through training.
    Then it decreases the learning rate linearly down to *η*[0] again during the second
    half of training, finishing the last few epochs by dropping the rate down by several
    orders of magnitude (still linearly). The maximum learning rate *η*[1] is chosen
    using the same approach we used to find the optimal learning rate, and the initial
    learning rate *η*[0] is usually 10 times lower. When using a momentum, we start
    with a high momentum first (e.g., 0.95), then drop it down to a lower momentum
    during the first half of training (e.g., down to 0.85, linearly), and then bring
    it back up to the maximum value (e.g., 0.95) during the second half of training,
    finishing the last few epochs with that maximum value. Smith did many experiments
    showing that this approach was often able to speed up training considerably and
    reach better performance. For example, on the popular CIFAR10 image dataset, this
    approach reached 91.9% validation accuracy in just 100 epochs, compared to 90.3%
    accuracy in 800 epochs through a standard approach (using the same neural network
    architecture). This feat was dubbed *super-convergence*. PyTorch implements this
    schedule in the `OneCycleLR` scheduler.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的学习计划是 *1cycle*，由 Leslie Smith 在 2018 年的一篇 [论文](https://homl.info/1cycle)
    中提出。⁠^([31](ch11.html#id2677)) 它首先通过预热学习率开始，从 *η*[0] 开始，线性增长到训练中期的一半 *η*[1]。然后，在训练的第二半期间，线性降低学习率回到
    *η*[0]，通过将速率降低几个数量级（仍然是线性）来完成最后的几个周期。最大学习率 *η*[1] 是使用与我们用来找到最佳学习率相同的方法选择的，初始学习率
    *η*[0] 通常比它低 10 倍。当使用动量时，我们首先使用高动量（例如，0.95），然后在训练的前半部分将其降低到较低的动量（例如，线性降低到 0.85），然后在训练的第二半部分将其恢复到最大值（例如，0.95），在最后的几个周期中使用这个最大值。Smith
    进行了许多实验，表明这种方法通常能够显著加快训练速度并达到更好的性能。例如，在流行的 CIFAR10 图像数据集上，这种方法在仅仅 100 个周期内达到了
    91.9% 的验证准确率，而标准方法（使用相同的神经网络架构）在 800 个周期内达到了 90.3% 的准确率。这一成就被称为 *超级收敛*。PyTorch
    在 `OneCycleLR` 调度器中实现了这个计划。
- en: Tip
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you are not sure which learning schedule to use, 1cycle can be a good default,
    but I tend to have more luck with performance scheduling. If you run into instabilities
    at the start of training, try adding learning rate warm-up. And if training gets
    stuck on plateaus, try cosine annealing with warm restarts.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定使用哪种学习计划，1cycle 可以作为一个好的默认选项，但我倾向于使用性能调度获得更好的效果。如果在训练开始时遇到不稳定性，尝试添加学习率预热。如果训练陷入平台期，尝试使用带有预热重启的余弦退火。
- en: We have now covered the most popular learning schedules, but PyTorch offers
    a few extra schedulers (e.g., a polynomial scheduler, a cyclic scheduler, a scheduler
    that makes it easy to chain other schedulers, and a few more), so make sure to
    check out the documentation.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了最流行的学习计划，但 PyTorch 还提供了一些额外的调度器（例如，多项式调度器、循环调度器、易于链式其他调度器的调度器，以及一些其他调度器），所以请确保查看文档。
- en: 'Now let’s move on to one final topic before we complete this chapter on deep
    learning training techniques: regularization. Deep learning is highly prone to
    overfitting, so regularization is key!'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论最后一个主题，在完成关于深度学习训练技术的这一章之前：正则化。深度学习非常容易过拟合，所以正则化是关键！
- en: Avoiding Overfitting Through Regularization
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过正则化避免过拟合
- en: With four parameters I can fit an elephant and with five I can make him wiggle
    his trunk.
  id: totrans-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用四个参数我可以拟合一头大象，用五个参数我甚至可以让它扭动它的鼻子。
- en: ''
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John von Neumann, cited by Enrico Fermi in *Nature* 427
  id: totrans-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·冯·诺伊曼，被恩里科·费米在《自然》杂志 427 中引用
- en: With thousands of parameters, you can fit the whole zoo. Deep neural networks
    typically have tens of thousands of parameters, sometimes even millions or billions.
    This gives them an incredible amount of freedom and means they can fit a huge
    variety of complex datasets. But this great flexibility also makes the network
    prone to overfitting the training set. Regularization is often needed to prevent
    this.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 用数千个参数，你可以拟合整个动物园。深度神经网络通常有数万个参数，有时甚至达到数百万或数十亿。这给了它们巨大的自由度，意味着它们可以拟合大量复杂的数据集。但这种巨大的灵活性也使得网络容易过拟合训练集。通常需要正则化来防止这种情况。
- en: 'We already implemented a common regularization technique in [Chapter 4](ch04.html#linear_models_chapter):
    early stopping. Moreover, even though batch-norm and layer-norm were designed
    to solve the unstable gradients problems, they also act like pretty good regularizers.
    In this section we will examine other popular regularization techniques for neural
    networks: ℓ[1] and ℓ[2] regularization, dropout, MC dropout, and max-norm regularization.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第4章](ch04.html#linear_models_chapter)中实现了一种常见的正则化技术：提前停止。此外，尽管批归一化和层归一化是为了解决不稳定梯度问题而设计的，但它们也像很好的正则化器一样发挥作用。在本节中，我们将检查神经网络的其他流行正则化技术：ℓ[1]和ℓ[2]正则化、dropout、MC
    dropout和max-norm正则化。
- en: ℓ[1] and ℓ[2] Regularization
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ℓ[1]和ℓ[2]正则化
- en: 'Just like you did in [Chapter 4](ch04.html#linear_models_chapter) for simple
    linear models, you can use ℓ[2] regularization to constrain a neural network’s
    connection weights, and/or ℓ[1] regularization if you want a sparse model (with
    many weights equal to 0). As we saw earlier (when discussing the AdamW optimizer),
    ℓ[2] regularization is mathematically equivalent to weight decay when using an
    `SGD` optimizer (with or without momentum), so if that’s the case you can implement
    ℓ[2] regularization by simply setting the optimizer’s `weight_decay` argument.
    For example, here is how to apply ℓ[2] regularization to the connection weights
    of a PyTorch model trained using `SGD`, with a regularization factor of 10^(–4):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你在[第4章](ch04.html#linear_models_chapter)中为简单线性模型所做的那样，你可以使用ℓ[2]正则化来约束神经网络的连接权重，如果你想要一个稀疏模型（许多权重等于0），则可以使用ℓ[1]正则化。正如我们之前所看到的（在讨论AdamW优化器时），当使用`SGD`优化器（带或不带动量）时，ℓ[2]正则化在数学上等同于权重衰减，所以如果那样的话，你可以通过简单地设置优化器的`weight_decay`参数来实现ℓ[2]正则化。例如，以下是使用`SGD`训练的PyTorch模型的连接权重应用ℓ[2]正则化的方法，正则化因子为10^(–4)：
- en: '[PRE29]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If instead you are using an Adam optimizer, you should switch to AdamW and set
    the `weight_decay` argument. This is not exactly equivalent to ℓ[2] regularization,
    but as we saw earlier it’s pretty close and it works better.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Adam优化器，你应该切换到AdamW并设置`weight_decay`参数。这并不完全等同于ℓ[2]正则化，但正如我们之前所看到的，它非常接近并且效果更好。
- en: 'Note that weight decay is applied to every model parameter, including bias
    terms, and even parameters of batch-norm and layer-norm layers. Generally that’s
    not a big deal, but penalizing these parameters does not contribute much to regularization
    and it may sometimes negatively impact training performance. So how can we apply
    weight decay to some model parameters and not others? One approach is to implement
    ℓ[2] regularization manually, without relying on the optimizer’s weight decay
    feature. For this, you must tweak the training loop to manually compute the ℓ[2]
    loss based only on the parameters you want, and add this ℓ[2] loss to the main
    loss:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重衰减应用于每个模型参数，包括偏置项，甚至包括批归一化和层归一化层的参数。通常这并不是什么大问题，但惩罚这些参数对正则化的贡献不大，有时甚至可能对训练性能产生负面影响。那么我们如何只对某些模型参数应用权重衰减，而不对其他参数应用呢？一种方法是通过手动实现ℓ[2]正则化，而不依赖于优化器的权重衰减功能。为此，你必须调整训练循环，手动计算仅基于你想要的参数的ℓ[2]损失，并将这个ℓ[2]损失添加到主要损失中：
- en: '[PRE30]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Another approach is to use PyTorch’s *parameter groups* feature, which lets
    the optimizer apply different hyperparameters to different groups of model parameters.
    So far, we have always created optimizers by passing them the full list of model
    parameters: PyTorch automatically put them all in a single parameter group, sharing
    the same hyperparameters. Instead, we can pass a list of dictionaries to the optimizer,
    each with a `"params"` entry containing a list of parameters, and (optionally)
    some hyperparameter key/value pairs specific to this group of parameters. The
    group-specific hyperparameters take precedence over the optimizer’s global hyperparameters.
    For example, let’s create an optimizer with two parameter groups: the first group
    will contain all the parameters we want to regularize and it will use weight decay,
    while the second group will contain all the bias terms and BN parameters, and
    it will not use weight decay at all.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用PyTorch的**参数组**功能，它允许优化器将不同的超参数应用于模型参数的不同组。到目前为止，我们总是通过传递模型参数的完整列表来创建优化器：PyTorch自动将它们全部放入一个参数组，共享相同的超参数。相反，我们可以向优化器传递一个字典列表，每个字典都有一个包含参数列表的`"params"`条目，以及（可选地）一些特定于该参数组的超参数键/值对。组特定的超参数优先于优化器的全局超参数。例如，让我们创建一个具有两个参数组的优化器：第一个组将包含我们想要正则化的所有参数，它将使用权重衰减，而第二个组将包含所有偏差项和BN参数，并且它将完全不使用权重衰减。
- en: '[PRE31]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Tip
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Parameter groups also allow you to apply different learning rates to different
    parts of your model. This is most common for transfer learning, when you want
    new layers to be updated faster than reused ones.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 参数组还允许您将不同的学习率应用于模型的不同部分。这在迁移学习中最为常见，当您希望新层比复用层更新得更快时。
- en: 'Now how about ℓ[1] regularization? Well unfortunately PyTorch does not provide
    any helper for this, so you need to implement it manually, much like we did for
    ℓ[2] regularization. This means tweaking the training loop to compute the ℓ[1]
    loss and adding it to the main loss:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在关于ℓ[1]正则化怎么样？不幸的是，PyTorch没有提供任何辅助工具，所以您需要手动实现它，就像我们为ℓ[2]正则化所做的那样。这意味着调整训练循环以计算ℓ[1]损失并将其添加到主损失中：
- en: '[PRE32]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That’s all there is to it! Now let’s move on to Dropout, which is one of the
    most popular regularization techniques for deep neural networks.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！现在让我们继续讨论Dropout，这是深度神经网络中最受欢迎的正则化技术之一。
- en: Dropout
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: '*Dropout* was [proposed in a paper](https://homl.info/64)⁠^([32](ch11.html#id2696))
    by Geoffrey Hinton et al. in 2012 and further detailed in a [2014 paper](https://homl.info/65)⁠^([33](ch11.html#id2697))
    by Nitish Srivastava et al., and it has proven to be highly successful: many state-of-the-art
    neural networks use dropout, as it gives them a 1%–2% accuracy boost. This may
    not sound like a lot, but when a model already has 95% accuracy, getting a 2%
    accuracy boost means dropping the error rate by almost 40% (going from 5% error
    to roughly 3%).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout**是由Geoffrey Hinton等人于2012年在一篇论文中[提出](https://homl.info/64)⁠^([32](ch11.html#id2696))，并在Nitish
    Srivastava等人于2014年的另一篇论文中[进一步详细说明](https://homl.info/65)⁠^([33](ch11.html#id2697))，它已被证明是非常成功的：许多最先进的神经网络都使用dropout，因为它可以为它们提供1%–2%的准确率提升。这可能听起来不多，但当模型已经达到95%的准确率时，获得2%的准确率提升意味着将错误率降低近40%（从5%错误率降低到大约3%）。'
- en: 'It is a fairly simple algorithm: at every training step, every neuron (including
    the input neurons, but always excluding the output neurons) has a probability
    *p* of being temporarily “dropped out”, meaning it will be entirely ignored during
    this training step, but it may be active during the next step (see [Figure 11-12](#dropout_diagram)).
    The hyperparameter *p* is called the *dropout rate*, and it is typically set between
    10% and 50%: closer to 20%–30% in recurrent neural nets (see [Chapter 13](ch13.html#rnn_chapter)),
    and closer to 40%–50% in convolutional neural networks (see [Chapter 12](ch12.html#cnn_chapter)).
    After training, neurons don’t get dropped anymore. And that’s all (except for
    a technical detail we will discuss shortly).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的算法：在每一步训练中，每个神经元（包括输入神经元，但始终不包括输出神经元）有概率 *p* 被临时“丢弃”，这意味着在这次训练步骤中它将被完全忽略，但在下一个步骤中它可能活跃（见[图11-12](#dropout_diagram)）。超参数
    *p* 被称为**丢弃率**，通常设置在10%到50%之间：在循环神经网络中接近20%–30%（见[第13章](ch13.html#rnn_chapter)），在卷积神经网络中接近40%–50%（见[第12章](ch12.html#cnn_chapter)）。训练完成后，神经元不再被丢弃。这就是全部（除了我们很快将要讨论的技术细节）。
- en: It’s surprising at first that this destructive technique works at all. Would
    a company perform better if its employees were told to toss a coin every morning
    to decide whether to go to work? Well, who knows; perhaps it would! The company
    would be forced to adapt its organization; it could not rely on any single person
    to work the coffee machine or perform any other critical tasks, so this expertise
    would have to be spread across several people. Employees would have to learn to
    cooperate with many of their coworkers, not just a handful of them. The company
    would become much more resilient. If one person quit, it wouldn’t make much of
    a difference. It’s unclear whether this idea would actually work for companies,
    but it certainly does for neural networks. Neurons trained with dropout cannot
    co-adapt with their neighboring neurons; they have to be as useful as possible
    on their own. They also cannot rely excessively on just a few input neurons; they
    must pay attention to all of their input neurons. They end up being less sensitive
    to slight changes in the inputs. In the end, you get a more robust network that
    generalizes better.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，这种破坏性技术竟然能起作用确实令人惊讶。如果一家公司告诉其员工每天早上掷硬币决定是否去上班，这家公司会表现更好吗？好吧，谁知道呢；也许会！公司将被迫适应其组织；它不能依赖任何一个人来操作咖啡机或执行任何其他关键任务，因此这种专业知识必须分散到几个人身上。员工必须学会与许多同事合作，而不仅仅是少数人。公司会变得更加有弹性。如果一个人辞职，这不会造成太大的影响。不清楚这个想法是否真的适用于公司，但它确实适用于神经网络。使用dropout训练的神经元不能与其相邻的神经元共同适应；它们必须尽可能有用。它们也不能过度依赖少数输入神经元；它们必须注意所有输入神经元。最终，它们对输入的微小变化变得不那么敏感。最终，你得到的是一个更健壮的网络，泛化能力更强。
- en: '![Diagram illustrating dropout regularization in a neural network where random
    neurons are deactivated at each step, represented by dashed arrows and red crosses.](assets/hmls_1112.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图示神经网络中dropout正则化的示意图，其中每个步骤随机关闭一些神经元，用虚线和红色交叉表示。](assets/hmls_1112.png)'
- en: Figure 11-12\. With dropout regularization, at each training iteration a random
    subset of all neurons in one or more layers—except the output layer—are “dropped
    out”; these neurons output 0 at this iteration (represented by the dashed arrows)
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-12。在dropout正则化中，在每个训练迭代中，除了输出层之外，一个或多个层中的所有神经元中的随机子集被“丢弃”；这些神经元在这个迭代中输出0（用虚线箭头表示）
- en: Another way to understand the power of dropout is to realize that a unique neural
    network is generated at each training step. Since each neuron can be either present
    or absent, there are a total of 2^(*N*) possible networks (where *N* is the total
    number of droppable neurons). This is such a huge number that it is virtually
    impossible for the same neural network to be sampled twice. Once you have run
    10,000 training steps, you have essentially trained 10,000 different neural networks,
    each with just one training instance. These neural networks are obviously not
    independent because they share many of their weights, but they are nevertheless
    all different. The resulting neural network can be seen as an averaging ensemble
    of all these smaller neural networks.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 理解dropout强大功能的另一方法是意识到在每个训练步骤中都会生成一个独特的神经网络。由于每个神经元可以是存在或不存在，因此总共有2^(*N*)个可能的网络（其中*N*是可丢弃神经元的总数）。这是一个如此巨大的数字，以至于几乎不可能采样到相同的神经网络两次。一旦你运行了10,000个训练步骤，你就实际上训练了10,000个不同的神经网络，每个神经网络只有一个训练实例。这些神经网络显然不是独立的，因为它们共享许多权重，但它们确实是不同的。生成的神经网络可以看作是所有这些较小神经网络的平均集合。
- en: Tip
  id: totrans-350
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Higher layers, which learn more complex feature combinations, benefit more
    from dropout because they are more prone to overfitting. So you can usually apply
    dropout only to the neurons of the top hidden layers (e.g., one to three hidden
    layers). However, you should avoid dropping the output neurons, as this would
    be like changing the task during training: it wouldn’t help.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 高层，它们学习更复杂的特征组合，由于更容易过拟合，因此从dropout中受益更多。所以你通常只需要将dropout应用于顶层隐藏层（例如，一到三层隐藏层）的神经元。然而，你应该避免丢弃输出神经元，因为这就像在训练过程中改变任务一样：这不会有所帮助。
- en: 'There is one small but important technical detail. Suppose *p* = 75%: on average
    only 25% of all neurons are active at each step during training. This means that
    after training, each neuron receives four times more inputs than during training,
    on average. This discrepancy is so large that the model is unlikely to work well.
    To avoid this issue, a simple solution is to multiply the inputs by 4 during training,
    which is the same as dividing them by 25%. More generally, we need to divide the
    inputs by the *keep probability* (1 – *p*) during training.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个微小但重要的技术细节。假设 *p* = 75%：在训练过程中，平均只有 25% 的神经元在每一步都是活跃的。这意味着在训练结束后，每个神经元接收到的输入比训练期间平均多四倍。这种差异如此之大，以至于模型可能无法很好地工作。为了避免这个问题，一个简单的解决方案是在训练期间将输入乘以
    4，这相当于将它们除以 25%。更普遍地说，我们需要在训练期间将输入除以 *保持概率*（1 – *p*）。
- en: 'To implement dropout using PyTorch, you can use the `nn.Dropout` layer. It’s
    important to switch to training mode during training, and to evaluation mode during
    evaluation (just like for batch norm). In training mode, the layer randomly drops
    some inputs (setting them to 0) and divides the remaining inputs by the keep probability.
    In evaluation mode, it does nothing at all; it just passes the inputs to the next
    layer. The following code applies dropout regularization before every `nn.Linear`
    layer, using a dropout rate of 0.2:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 PyTorch 实现 dropout，你可以使用 `nn.Dropout` 层。在训练期间切换到训练模式，在评估期间切换到评估模式（就像批归一化一样）。在训练模式下，该层随机丢弃一些输入（将它们设置为
    0），并将剩余的输入除以保持概率。在评估模式下，它根本不做任何事情；它只是将输入传递到下一层。以下代码在每一个 `nn.Linear` 层之前应用 dropout
    正则化，使用丢弃率为 0.2：
- en: '[PRE33]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Warning
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Since dropout is only active during training, comparing the training loss and
    the validation loss can be misleading. In particular, a model may be overfitting
    the training set and yet have similar training and validation losses. So make
    sure to evaluate the training loss without dropout (e.g., after training).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 dropout 只在训练期间激活，比较训练损失和验证损失可能会产生误导。特别是，一个模型可能过拟合了训练集，但训练损失和验证损失却相似。所以请确保在没有
    dropout 的情况下评估训练损失（例如，在训练后）。
- en: If you observe that the model is overfitting, you can increase the dropout rate.
    Conversely, you should try decreasing the dropout rate if the model underfits
    the training set. It can also help to increase the dropout rate for large layers,
    and reduce it for small ones. Moreover, many state-of-the-art architectures only
    apply dropout to the last few hidden layers, so you may want to try this if full
    dropout is too strong.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察到模型过拟合，你可以增加丢弃率。相反，如果你发现模型欠拟合训练集，你应该尝试降低丢弃率。对于大型层，增加丢弃率也可能有所帮助，而对于小型层，则应减少丢弃率。此外，许多最先进的架构只对最后几个隐藏层应用丢弃率，所以如果你觉得全丢弃率太强，你可能想尝试这种方法。
- en: Dropout does tend to significantly slow down convergence, but it often results
    in a better model when tuned properly. So it is generally well worth the extra
    time and effort, especially for large models.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 确实会显著减慢收敛速度，但只要调整得当，通常会产生更好的模型。所以，它通常非常值得额外的时间和精力，尤其是对于大型模型。
- en: Tip
  id: totrans-359
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you want to regularize a self-normalizing network based on the SELU activation
    function (as discussed earlier), you should use *alpha dropout*: this is a variant
    of dropout that preserves the mean and standard deviation of its inputs. It was
    introduced in the same paper as SELU, as regular dropout would break self-normalization.
    PyTorch implements it in the `nn.AlphaDropout` layer.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要基于 SELU 激活函数（如前所述）的正则化自归一化网络，你应该使用 *alpha dropout*：这是一种保留其输入均值和标准差的 dropout
    变体。它是在与 SELU 同一篇论文中引入的，因为常规 dropout 会破坏自归一化。PyTorch 在 `nn.AlphaDropout` 层中实现了它。
- en: Monte Carlo Dropout
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛丢弃
- en: 'In 2016, a [paper](https://homl.info/mcdropout)⁠^([34](ch11.html#id2709)) by
    Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2016 年，Yarin Gal 和 Zoubin Ghahramani 的 [论文](https://homl.info/mcdropout)⁠^([34](ch11.html#id2709))
    为使用 dropout 添加了几个额外的良好理由：
- en: First, the paper established a profound connection between dropout networks
    (i.e., neural networks containing `Dropout` layers) and approximate Bayesian inference,⁠^([35](ch11.html#id2710))
    giving dropout a solid mathematical justification.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，该论文在丢弃网络（即包含 `Dropout` 层的神经网络）和近似贝叶斯推理之间建立了深刻的联系，⁠^([35](ch11.html#id2710))
    为丢弃提供了坚实的数学依据。
- en: Second, the authors introduced a powerful technique called *Monte Carlo (MC)
    dropout*, which can boost the performance of any trained dropout model without
    having to retrain it or even modify it at all. It also provides a much better
    measure of the model’s uncertainty, and it can be implemented in just a few lines
    of code.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，作者介绍了一种强大的技术，称为**蒙特卡洛（MC）dropout**，它可以在不重新训练或甚至修改模型的情况下，提升任何已训练 dropout 模型的性能。它还提供了对模型不确定性的更好度量，并且只需几行代码即可实现。
- en: 'This description of MC dropout sounds like some “one weird trick” clickbait,
    so let me explain: it is just like regular dropout, except it is active not only
    during training, but also during evaluation. This means that the predictions are
    always a bit random (hence the name Monte Carlo). But instead of making a single
    prediction, you make many predictions and average them out. It turns out that
    this produces better predictions than the original model.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对 MC dropout 的描述听起来像是一些“一个奇怪的小技巧”点击诱饵，让我来解释一下：它就像常规 dropout 一样，只不过它不仅在训练期间活跃，在评估期间也活跃。这意味着预测总是有点随机（因此得名蒙特卡洛）。但不是只做一个预测，而是做很多预测然后取平均值。结果发现，这种方法产生的预测比原始模型更好。
- en: 'Following is a full implementation of MC dropout, using the model we trained
    in the previous section to make predictions for a batch of images:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 MC dropout 的完整实现，使用我们在上一节中训练的模型对一个图像批次进行预测：
- en: '[PRE34]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s go through this code:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: First, we switch the model to evaluation mode as we always do before making
    predictions, but this time we immediately switch all the dropout layers back to
    training mode, so they will behave just like during training (i.e., randomly dropping
    out some of their inputs). In other words, we convert the dropout layers to MC
    dropout layers.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将模型切换到评估模式，就像我们预测之前总是做的那样，但这次我们立即将所有 dropout 层切换回训练模式，这样它们就会表现得就像在训练期间一样（即随机丢弃一些输入）。换句话说，我们将
    dropout 层转换为 MC dropout 层。
- en: Next we load a new batch of images `X_new`, and we move it to the GPU. In this
    example, let’s assume `X_new` contains three images.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们加载一个新的图像批次 `X_new`，并将其移动到 GPU 上。在这个例子中，让我们假设 `X_new` 包含三张图像。
- en: We then use the `repeat_interleave()` method to create a batch containing 100
    copies of each image in `X_new`. The images are repeated along the first dimension
    (`dim=0`) so `X_new_repeated` has a shape of `[300, 1, 28, 28]`.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们随后使用 `repeat_interleave()` 方法创建一个包含 `X_new` 中每个图像 100 份副本的批次。图像在第一个维度（`dim=0`）上重复，因此
    `X_new_repeated` 的形状为 `[300, 1, 28, 28]`。
- en: Next, we pass this big batch to the model, which predicts 10 logits per image,
    as usual. This tensor’s shape is `[300, 10]`, but we reshape it to `[3, 100, 10]`
    to group the predictions for each image. Remember that the dropout layers are
    active, which means that there’s some variability across the predictions, even
    for copies of the same image.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接着，我们将这个大的批次传递给模型，模型对每张图像预测 10 个 logits，就像往常一样。这个张量的形状是 `[300, 10]`，但我们将其重塑为
    `[3, 100, 10]` 以将每个图像的预测分组。记住，dropout 层是活跃的，这意味着预测之间会有一些变化，即使是相同图像的副本。
- en: Then we convert these logits to estimated probabilities using the softmax function.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 softmax 函数将这些 logits 转换为估计的概率。
- en: 'Lastly, we compute the mean over the second dimension (`dim=1`) to get the
    average estimated probability for each class and each image, across all 100 predictions.
    The result is a tensor of shape `[3, 10]`. These are our final predictions:'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在第二个维度（`dim=1`）上计算平均值，以获得每个类别和每个图像在所有 100 个预测中的平均估计概率。结果是形状为 `[3, 10]`
    的张量。这些就是我们的最终预测：
- en: '[PRE35]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Warning
  id: totrans-376
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Rather than converting the logits to probabilities and then computing the mean
    probabilities, you may be tempted to do the reverse: first average over the logits
    and *then* convert the mean logits to probabilities. This is faster but it does
    not properly reflect the model’s uncertainty, so it tends to produce overconfident
    models.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 与将 logits 转换为概率并计算平均概率不同，你可能想反过来操作：首先对 logits 进行平均，然后**再**将平均 logits 转换为概率。这样做更快，但它并不能正确反映模型的不确定性，因此往往会产生过度自信的模型。
- en: 'MC dropout tends to improve the reliability of the model’s probability estimates.
    This means that it’s less likely to be confidently wrong, making it safer (you
    don’t want a self-driving car confidently ignoring a stop sign). It’s also useful
    when you’re interested in the top *k* classes, not just the most likely. Additionally,
    you can take a look at the [standard deviation of each class probability](https://xkcd.com/2110):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: MC dropout 通常会提高模型概率估计的可靠性。这意味着它不太可能自信地犯错，使其更安全（你不想自动驾驶汽车自信地忽略停车标志）。当你对前 *k*
    个类别感兴趣，而不仅仅是可能性最大的类别时，这也很有用。此外，你可以查看每个类概率的[标准差](https://xkcd.com/2110)：
- en: '[PRE36]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'There’s a standard deviation of 0.02 for the probability estimate of class
    9 (ankle boot) for the first image. This adds a grain of salt to the estimated
    probability of 99% for this class: in fact, the model is really saying “mmh, I’m
    guessing over 95%”. If you were building a risk-sensitive system (e.g., a medical
    or financial system), you may want to consider only the predictions with both
    a high estimated probability *and* a low standard deviation.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图片中类别 9（踝靴）的概率估计的标准差为 0.02。这给该类别的 99% 估计概率增添了一丝怀疑：实际上，模型真正说的是“嗯，我猜超过 95%”。如果你正在构建一个对风险敏感的系统（例如，医疗或金融系统），你可能只想考虑那些既有高估计概率又有低标准差的预测。
- en: Note
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The number of Monte Carlo samples you use (100 in this example) is a hyperparameter
    you can tweak. The higher it is, the more accurate the predictions and their uncertainty
    estimates are, but also the slower the predictions are. Moreover, above a certain
    number of samples, you will notice little improvement. Your job is to find the
    right trade-off among latency, throughput, and accuracy, depending on your application.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的蒙特卡洛样本数量（本例中为 100）是一个可以调整的超参数。它的值越高，预测及其不确定性估计就越准确，但预测速度也会变慢。此外，超过一定数量的样本后，你将注意到改进很小。你的任务是找到延迟、吞吐量和准确性之间的最佳平衡，这取决于你的应用。
- en: 'If you want to train an MC dropout model from scratch rather than reuse an
    existing dropout model, you should probably use a custom `McDropout` module rather
    than using `nn.Dropout` and hacking around with `train()` and `eval()`, as this
    is a bit brittle (e.g., it won’t play nicely with the evaluation function). Here
    is a three-line implementation:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从头开始训练一个 MC dropout 模型，而不是重用现有的 dropout 模型，你可能应该使用自定义的 `McDropout` 模块，而不是使用
    `nn.Dropout` 并通过 `train()` 和 `eval()` 进行修改，因为这样做有点脆弱（例如，它不会很好地与评估函数配合）。以下是一个三行实现的示例：
- en: '[PRE37]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In short, MC dropout is a great technique that boosts dropout models and provides
    better uncertainty estimates. And of course, since it is just regular dropout
    during training, it also acts like a regularizer.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，MC dropout 是一种提升 dropout 模型并提供更好不确定性估计的出色技术。当然，由于它在训练期间只是普通的 dropout，它也充当了正则化器。
- en: Max-Norm Regularization
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大范数正则化
- en: 'Another fairly popular regularization technique for neural networks is called
    *max-norm regularization*: for each neuron, it constrains the weights **w** of
    the incoming connections such that ∥ **w** ∥[2] ≤ *r*, where *r* is the max-norm
    hyperparameter and ∥ · ∥[2] is the ℓ[2] norm.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种相当流行的神经网络正则化技术被称为 *最大范数正则化*：对于每个神经元，它约束输入连接的权重 **w**，使得 ∥ **w** ∥[2] ≤ *r*，其中
    *r* 是最大范数超参数，∥ · ∥[2] 是 ℓ[2] 范数。
- en: Reducing *r* increases the amount of regularization and helps reduce overfitting.
    Max-norm regularization can also help alleviate the unstable gradients problems
    (if you are not using batch-norm or layer-norm).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 减少 *r* 增加了正则化的量，有助于减少过拟合。最大范数正则化也可以帮助缓解不稳定梯度的问题（如果你没有使用批归一化或层归一化）。
- en: 'Rather than adding a regularization loss term to the overall loss function,
    max-norm regularization is typically implemented by computing ∥ **w** ∥[2] after
    each training step and rescaling **w** if needed (**w** ← **w** *r* / ∥ **w** ∥[2]).
    Here’s a common way to implement this in PyTorch:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 与将正则化损失项添加到整体损失函数不同，最大范数正则化通常通过在每次训练步骤后计算 ∥ **w** ∥[2] 并在需要时重新缩放 **w**（**w**
    ← **w** *r* / ∥ **w** ∥[2]）来实现。以下是在 PyTorch 中实现此方法的常见方式：
- en: '[PRE38]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This function iterates through all of the model’s weight matrices (i.e., all
    parameters except for the bias terms), and for each one of them it uses the `norm()`
    method to compute the ℓ[2] norm of each row (`dim=1`). A `nn.Linear` layer has
    weights of shape [*number of neurons*, *number of inputs*], so using `dim=1` means
    that we will get one norm per neuron, as desired. Then the function uses `torch.clamp()`
    to compute the target norm for each neuron’s weights: this creates a copy of the
    `actual_norm` tensor, except that all values greater than `max_norm` are replaced
    by `max_norm` (this corresponds to *r* in the previous equation). Lastly, we rescale
    the weight matrix so that each column ends up with the target norm. Note that
    the smoothing term `epsilon` is used to avoid division by zero in case some columns
    have a norm equal to zero.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数遍历模型的所有权重矩阵（即所有除了偏置项之外的所有参数），并对每个权重矩阵使用`norm()`方法来计算每行的ℓ[2]范数（`dim=1`）。`nn.Linear`层的权重形状为[*神经元数量*,
    *输入数量*]，因此使用`dim=1`意味着我们将为每个神经元得到一个范数，正如所期望的那样。然后，该函数使用`torch.clamp()`来计算每个神经元权重的目标范数：这会创建一个`actual_norm`张量的副本，除了所有大于`max_norm`的值都被替换为`max_norm`（这对应于前面方程中的*r*）。最后，我们重新缩放权重矩阵，使得每一列最终都达到目标范数。请注意，平滑项`epsilon`用于避免某些列范数为零时的除以零错误。
- en: Next, all you need to do is call `apply_max_norm(model)` in the training loop,
    right after calling the optimizer’s `step()` method. And of course you probably
    want to fine-tune the `max_norm` hyperparameter.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您只需在训练循环中调用`apply_max_norm(model)`，紧随调用优化器的`step()`方法之后。当然，您可能还希望微调`max_norm`超参数。
- en: Tip
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When using max-norm with layers other than `nn.Linear`, you may need to tweak
    the `dim` argument. For example, when using convolutional layers (see [Chapter 12](ch12.html#cnn_chapter)),
    you generally want to set `dim=[1, 2, 3]` to limit the norm of each convolutional
    kernel.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用最大范数与`nn.Linear`以外的层时，您可能需要调整`dim`参数。例如，当使用卷积层（见[第12章](ch12.html#cnn_chapter)）时，通常希望设置`dim=[1,
    2, 3]`以限制每个卷积核的范数。
- en: Practical Guidelines
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用指南
- en: In this chapter we have covered a wide range of techniques, and you may be wondering
    which ones you should use. This depends on the task, and there is no clear consensus
    yet, but I have found the configuration in [Table 11-3](#default_deep_neural_network_config)
    to work fine in most cases, without requiring much hyperparameter tuning. That
    said, please do not consider these defaults as hard rules!
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一系列技术，您可能会想知道应该使用哪些。这取决于任务，目前还没有明确的共识，但我发现[表11-3](#default_deep_neural_network_config)中的配置在大多数情况下都工作得很好，无需进行太多的超参数调整。但请记住，不要将这些默认值视为硬性规则！
- en: Table 11-3\. Default DNN configuration
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-3\. 默认深度神经网络配置
- en: '| Hyperparameter | Default value |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 默认值 |'
- en: '| --- | --- |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kernel initializer | He initialization |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 核初始化器 | He初始化 |'
- en: '| Activation function | ReLU if shallow; Swish if deep |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 浅层使用ReLU；深层使用Swish |'
- en: '| Normalization | None if shallow; batch-norm or layer-norm if deep |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 归一化 | 浅层使用无；深层使用批归一化或层归一化 |'
- en: '| Regularization | Early stopping; weight decay if needed |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 | 提前停止；如有需要则使用权重衰减 |'
- en: '| Optimizer | Nesterov accelerated gradients or AdamW |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Nesterov加速梯度或AdamW |'
- en: '| Learning rate schedule | Performance scheduling or 1cycle |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 性能调度或1cycle |'
- en: You should also try to reuse parts of a pretrained neural network if you can
    find one that solves a similar problem, or use unsupervised pretraining if you
    have a lot of unlabeled data, or use pretraining on an auxiliary task if you have
    a lot of labeled data for a similar task.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能找到一个解决类似问题的预训练神经网络，您应该尝试重用它的一部分；如果您有很多未标记的数据，则使用无监督预训练；如果您有大量类似任务的标记数据，则使用辅助任务的预训练。
- en: 'While the previous guidelines should cover most cases, there are some exceptions:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的指南应该涵盖大多数情况，但仍有一些例外：
- en: If you need a sparse model, use ℓ[1] regularization. You can also try zeroing
    out the smallest weights after training (for example, using the `torch.​nn.utils.prune.l1_unstructured()`
    function). This will break self-normalization, so you should use the default configuration
    in this case.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要稀疏模型，请使用ℓ[1]正则化。您还可以尝试在训练后清零最小的权重（例如，使用`torch.nn.utils.prune.l1_unstructured()`函数）。这将破坏自归一化，因此在这种情况下您应使用默认配置。
- en: If you need a low-latency model (one that performs lightning-fast predictions),
    you may need to use fewer layers; use a fast activation function such as `nn.ReLU`,
    `nn.LeakyReLU`, or `nn.Hardswish`; and fold the batch-norm and layer-norm layers
    into the previous layers after training. Having a sparse model will also help.
    Finally, you may want to reduce the float precision from 32 bits to 16 or even
    8 bits. [Appendix B](app02.html#precision_appendix) covers several techniques
    to make models smaller and faster, including reduced precision models, mixed precision
    models, and quantization.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要一个低延迟的模型（即执行闪电般快速预测的模型），你可能需要使用更少的层；使用快速激活函数，如`nn.ReLU`、`nn.LeakyReLU`或`nn.Hardswish`；并在训练后将批归一化和层归一化层折叠到之前的层中。拥有稀疏模型也将有所帮助。最后，你可能希望将浮点精度从32位降低到16位甚至8位。[附录B](app02.html#precision_appendix)涵盖了多种使模型更小、更快的技巧，包括降低精度模型、混合精度模型和量化。
- en: If you are building a risk-sensitive application, or inference latency is not
    very important in your application, you can use MC dropout to boost performance
    and get more reliable probability estimates, along with uncertainty estimates.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在构建一个对风险敏感的应用程序，或者推理延迟在你的应用程序中不是非常重要，你可以使用MC dropout来提升性能并获得更可靠的概率估计，同时获得不确定性估计。
- en: 'Over the last three chapters, we have learned what artificial neural nets are,
    how to build and train them using Scikit-Learn and PyTorch, and a variety of techniques
    that make it possible to train deep and complex nets. In the next chapter, all
    of this will come together as we dive into one of the most important applications
    of deep learning: computer vision.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的三章中，我们学习了什么是人工神经网络，如何使用Scikit-Learn和PyTorch构建和训练它们，以及各种使训练深度和复杂网络成为可能的技术。在下一章中，我们将深入探讨深度学习最重要的应用之一：计算机视觉。
- en: Exercises
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is the problem that Glorot initialization and He initialization aim to
    fix?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Glorot初始化和He初始化旨在解决什么问题？
- en: Is it OK to initialize all the weights to the same value as long as that value
    is selected randomly using He initialization?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只要使用He初始化随机选择该值，将所有权重初始化为相同的值是否可以？
- en: Is it OK to initialize the bias terms to 0?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将偏置项初始化为0是否可以？
- en: In which cases would you want to use each of the activation functions we discussed
    in this chapter?
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪些情况下，你会想使用本章讨论的每个激活函数？
- en: What may happen if you set the `momentum` hyperparameter too close to 1 (e.g.,
    0.99999) when using an `SGD` optimizer?
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用`SGD`优化器时，如果你将`momentum`超参数设置得太接近1（例如，0.99999），可能会发生什么？
- en: Name three ways you can produce a sparse model.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出三种你可以产生稀疏模型的方法。
- en: Does dropout slow down training? Does it slow down inference (i.e., making predictions
    on new instances)? What about MC dropout?
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dropout会减慢训练吗？它会减慢推理（即在新的实例上做出预测）吗？MC dropout呢？
- en: 'Practice training a deep neural network on the CIFAR10 image dataset:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CIFAR10图像数据集上练习训练深度神经网络：
- en: Load CIFAR10 just like you loaded the FashionMNIST dataset in [Chapter 10](ch10.html#pytorch_chapter),
    but using `torchvision.datasets.CIFAR10` instead of `FashionMNIST`. The dataset
    is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000
    for testing) with 10 classes.
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像在[第10章](ch10.html#pytorch_chapter)中加载FashionMNIST数据集一样加载CIFAR10，但使用`torchvision.datasets.CIFAR10`而不是`FashionMNIST`。该数据集由60,000个32
    × 32像素彩色图像组成（50,000个用于训练，10,000个用于测试），分为10个类别。
- en: Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but
    it’s the point of this exercise). Use He initialization and the Swish activation
    function (using `nn.SiLU`). Since this is a classification task, you will need
    an output layer with one neuron per class.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个具有20个隐藏层，每个层有100个神经元的DNN（这太多了，但这是练习的目的）。使用He初始化和Swish激活函数（使用`nn.SiLU`）。由于这是一个分类任务，你需要一个输出层，每个类别有一个神经元。
- en: Using NAdam optimization and early stopping, train the network on the CIFAR10
    dataset. Remember to search for the right learning rate each time you change the
    model’s architecture or hyperparameters.
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NAdam优化和早停策略，在CIFAR10数据集上训练网络。记住，每次更改模型架构或超参数时，都要寻找合适的学习率。
- en: 'Now try adding batch-norm and compare the learning curves: is it converging
    faster than before? Does it produce a better model? How does it affect training
    speed?'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在尝试添加批归一化，并比较学习曲线：它是否比以前更快地收敛？它是否产生了更好的模型？这对训练速度有何影响？
- en: Try replacing batch-norm with SELU, and make the necessary adjustments to ensure
    the network self-normalizes (i.e., standardize the input features, use LeCun normal
    initialization, make sure the DNN contains only a sequence of dense layers, etc.).
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将批归一化替换为 SELU，并做出必要的调整以确保网络自我归一化（即标准化输入特征，使用 LeCun 正态初始化，确保深度神经网络只包含一系列密集层等）。
- en: Try regularizing the model with alpha dropout. Then, without retraining your
    model, see if you can achieve better accuracy using MC dropout.
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用 alpha dropout 规范化模型。然后，在不重新训练模型的情况下，看看是否可以使用 MC dropout 实现更好的准确率。
- en: Retrain your model using 1cycle scheduling and see if it improves training speed
    and model accuracy.
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 1cycle 调度重新训练您的模型，看看是否可以提高训练速度和模型准确率。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，在 [*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch11.html#id2449-marker)) Xavier Glorot and Yoshua Bengio, “Understanding
    the Difficulty of Training Deep Feedforward Neural Networks”, *Proceedings of
    the 13th International Conference on Artificial Intelligence and Statistics* (2010):
    249–256.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch11.html#id2449-marker)) Xavier Glorot 和 Yoshua Bengio，"理解深度前馈神经网络的训练难度"，*《第13届国际人工智能与统计会议论文集》*
    (2010): 249–256。'
- en: '^([2](ch11.html#id2451-marker)) Here’s an analogy: if you set a microphone
    amplifier’s volume knob too close to zero, people won’t hear your voice, but if
    you set it too close to the max, your voice will be saturated and people won’t
    understand what you are saying. Now imagine a chain of such amplifiers: they all
    need to be set properly in order for your voice to come out loud and clear at
    the end of the chain. Your voice has to come out of each amplifier at the same
    amplitude as it came in.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.html#id2451-marker)) 这里有一个类比：如果你将麦克风放大器的音量旋钮设置得太接近零，人们就听不到你的声音，但如果你设置得太接近最大值，你的声音就会饱和，人们就无法理解你在说什么。现在想象一个这样的放大器链：它们都需要被正确设置，以便你的声音在链的末端清晰响亮。你的声音必须以与进入时相同的振幅从每个放大器中出来。
- en: '^([3](ch11.html#id2456-marker)) Kaiming He et al., “Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification”, *Proceedings of
    the 2015 IEEE International Conference on Computer Vision* (2015): 1026–1034.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch11.html#id2456-marker)) Kaiming He 等人，"深入探究修正器：在 ImageNet 分类上超越人类水平的表现"，*《2015年
    IEEE 国际计算机视觉会议论文集》* (2015): 1026–1034。'
- en: ^([4](ch11.html#id2457-marker)) A PyTorch issue (#18182) has been open since
    2019 to update the weight initialization to use the current best practices.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.html#id2457-marker)) 自 2019 年以来，PyTorch 的问题 (#18182) 已被打开，以更新权重初始化以使用当前最佳实践。
- en: ^([5](ch11.html#id2461-marker)) Andrew Saxe et al., “Exact solutions to the
    nonlinear dynamics of learning in deep linear neural networks”, ICLR (2014).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch11.html#id2461-marker)) Andrew Saxe 等人，"深度线性神经网络学习非线动态的精确解"，ICLR (2014)。
- en: ^([6](ch11.html#id2473-marker)) A dead neuron may come back to life if its inputs
    evolve over time and eventually return within a range where the ReLU activation
    function gets a positive input again. For example, this may happen if gradient
    descent tweaks the neurons in the layers below the dead neuron.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个神经元死亡，随着时间的推移其输入发生变化并最终返回到 ReLU 激活函数再次获得正输入的范围内，它可能会复活。例如，这可能发生在梯度下降调整死亡神经元下方的层中的神经元时。
- en: ^([7](ch11.html#id2477-marker)) Bing Xu et al., “Empirical Evaluation of Rectified
    Activations in Convolutional Network”, arXiv preprint arXiv:1505.00853 (2015).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch11.html#id2477-marker)) Bing Xu 等人，"卷积网络中修正激活函数的经验评估"，arXiv 预印本 arXiv:1505.00853
    (2015)。
- en: ^([8](ch11.html#id2492-marker)) Djork-Arné Clevert et al., “Fast and Accurate
    Deep Network Learning by Exponential Linear Units (ELUs)”, *Proceedings of the
    International Conference on Learning Representations*, arXiv preprint (2015).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch11.html#id2492-marker)) Djork-Arné Clevert 等人，"通过指数线性单元 (ELUs) 实现快速且精确的深度网络学习"，*《国际学习表示会议论文集》*，arXiv
    预印本 (2015)。
- en: '^([9](ch11.html#id2500-marker)) Günter Klambauer et al., “Self-Normalizing
    Neural Networks”, *Proceedings of the 31st International Conference on Neural
    Information Processing Systems* (2017): 972–981.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch11.html#id2500-marker)) Günter Klambauer 等人，"自我归一化神经网络"，*《第31届国际神经网络信息处理系统会议论文集》*
    (2017): 972–981。'
- en: ^([10](ch11.html#id2505-marker)) Dan Hendrycks and Kevin Gimpel, “Gaussian Error
    Linear Units (GELUs)”, arXiv preprint arXiv:1606.08415 (2016).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch11.html#id2505-marker)) Dan Hendrycks 和 Kevin Gimpel，"高斯误差线性单元 (GELUs)"，arXiv
    预印本 arXiv:1606.08415 (2016)。
- en: ^([11](ch11.html#id2506-marker)) A function is convex if the line segment between
    any two points on the curve never lies below the curve. A monotonic function only
    increases, or only decreases.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch11.html#id2506-marker)) 如果曲线上的任意两点之间的线段始终不低于曲线，则函数是凸函数。单调函数只增加或只减少。
- en: ^([12](ch11.html#id2509-marker)) Prajit Ramachandran et al., “Searching for
    Activation Functions”, arXiv preprint arXiv:1710.05941 (2017).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch11.html#id2509-marker)) Prajit Ramachandran等人，“寻找激活函数”，arXiv预印本arXiv:1710.05941（2017）。
- en: ^([13](ch11.html#id2519-marker)) Noam Shazeer, “GLU Variants Improve Transformer”,
    arXiv preprint arXiv:2002.05202 (2020).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch11.html#id2519-marker)) Noam Shazeer，“GLU变体改进Transformer”，arXiv预印本arXiv:2002.05202（2020）。
- en: ^([14](ch11.html#id2520-marker)) Yann Dauphin et al., “Language Modeling with
    Gated Convolutional Networks”, arXiv preprint arXiv:1612.08083 (2016).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch11.html#id2520-marker)) Yann Dauphin等人，“使用门控卷积网络进行语言建模”，arXiv预印本arXiv:1612.08083（2016）。
- en: '^([15](ch11.html#id2524-marker)) Diganta Misra, “Mish: A Self Regularized Non-Monotonic
    Activation Function”, arXiv preprint arXiv:1908.08681 (2019).'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch11.html#id2524-marker)) Diganta Misra，“Mish：一种自正则化的非单调激活函数”，arXiv预印本arXiv:1908.08681（2019）。
- en: '^([16](ch11.html#id2532-marker)) So et al., “Primer: Searching for Efficient
    Transformers for Language Modeling”, arXiv preprint arXiv:2109.08668 (2021).'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch11.html#id2532-marker)) So等人，“语言建模中高效Transformer的搜索”，arXiv预印本arXiv:2109.08668（2021）。
- en: '^([17](ch11.html#id2548-marker)) Sergey Ioffe and Christian Szegedy, “Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift”, *Proceedings of the 32nd International Conference on Machine Learning*
    (2015): 448–456.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch11.html#id2548-marker)) Sergey Ioffe和Christian Szegedy，“批量归一化：通过减少内部协变量偏移来加速深度网络训练”，*第32届国际机器学习会议论文集*（2015）：448–456。
- en: ^([18](ch11.html#id2562-marker)) Jimmy Lei Ba et al., “Layer Normalization”,
    arXiv preprint arXiv:1607.06450 (2016).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch11.html#id2562-marker)) Jimmy Lei Ba等人，“层归一化”，arXiv预印本arXiv:1607.06450（2016）。
- en: '^([19](ch11.html#id2572-marker)) Razvan Pascanu et al., “On the Difficulty
    of Training Recurrent Neural Networks”, *Proceedings of the 30th International
    Conference on Machine Learning* (2013): 1310–1318.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch11.html#id2572-marker)) Razvan Pascanu等人，“关于训练循环神经网络难度的研究”，*第30届国际机器学习会议论文集*（2013）：1310–1318。
- en: '^([20](ch11.html#id2599-marker)) Boris T. Polyak, “Some Methods of Speeding
    Up the Convergence of Iteration Methods”, *USSR Computational Mathematics and
    Mathematical Physics* 4, no. 5 (1964): 1–17.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch11.html#id2599-marker)) Boris T. Polyak，“一些加速迭代方法收敛速度的方法”，*苏联计算数学与数学物理*
    4，第5期（1964）：1–17。
- en: '^([21](ch11.html#id2609-marker)) Yurii Nesterov, “A Method for Unconstrained
    Convex Minimization Problem with the Rate of Convergence *O*(1/*k*²)”, *Doklady
    AN USSR* 269 (1983): 543–547.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch11.html#id2609-marker)) Yurii Nesterov，“一种具有收敛率*O*(1/*k*²)的无约束凸最小化问题的方法”，*苏联科学院院刊*
    269（1983）：543–547。
- en: '^([22](ch11.html#id2615-marker)) John Duchi et al., “Adaptive Subgradient Methods
    for Online Learning and Stochastic Optimization”, *Journal of Machine Learning
    Research* 12 (2011): 2121–2159.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch11.html#id2615-marker)) John Duchi等人，“自适应梯度下降法用于在线学习和随机优化”，*机器学习研究杂志*
    12（2011）：2121–2159。
- en: '^([23](ch11.html#id2619-marker)) This algorithm was created by Geoffrey Hinton
    and Tijmen Tieleman in 2012 and presented by Geoffrey Hinton in his Coursera class
    on neural networks (slides: [*https://homl.info/57*](https://homl.info/57), video:
    [*https://homl.info/58*](https://homl.info/58)). Amusingly, since the authors
    did not write a paper to describe the algorithm, researchers often cite “slide
    29 in lecture 6e” in their papers.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch11.html#id2619-marker)) 该算法由Geoffrey Hinton和Tijmen Tieleman于2012年创建，并由Geoffrey
    Hinton在他的Coursera神经网络课程中介绍（幻灯片：[*https://homl.info/57*](https://homl.info/57)，视频：[*https://homl.info/58*](https://homl.info/58)）。有趣的是，由于作者没有撰写论文来描述该算法，研究人员通常在他们的论文中引用“第6e讲幻灯片29”。
- en: '^([24](ch11.html#id2626-marker)) Diederik P. Kingma and Jimmy Ba, “Adam: A
    Method for Stochastic Optimization”, arXiv preprint arXiv:1412.6980 (2014).'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch11.html#id2626-marker)) Diederik P. Kingma和Jimmy Ba，“Adam：一种随机优化方法”，arXiv预印本arXiv:1412.6980（2014）。
- en: ^([25](ch11.html#id2644-marker)) Timothy Dozat, [“Incorporating Nesterov Momentum
    into Adam”](https://homl.info/nadam), (2016).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch11.html#id2644-marker)) Timothy Dozat，[“将Nesterov动量引入Adam”](https://homl.info/nadam)，（2016）。
- en: ^([26](ch11.html#id2645-marker)) Ilya Loshchilov, and Frank Hutter, “Decoupled
    Weight Decay Regularization”, arXiv preprint arXiv:1711.05101 (2017).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch11.html#id2645-marker)) Ilya Loshchilov和Frank Hutter，“解耦权重衰减正则化”，arXiv预印本arXiv:1711.05101（2017）。
- en: '^([27](ch11.html#id2650-marker)) Ashia C. Wilson et al., “The Marginal Value
    of Adaptive Gradient Methods in Machine Learning”, *Advances in Neural Information
    Processing Systems* 30 (2017): 4148–4158.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '^([27](ch11.html#id2650-marker)) Ashia C. Wilson 等人，“机器学习中自适应梯度方法边际价值的评估”，*神经网络信息处理系统进展*
    30 (2017): 4148–4158。'
- en: '^([28](ch11.html#id2656-marker)) The *Jacobian matrix* contains all the first-order
    partial derivatives of a function with multiple parameters and multiple outputs:
    one column per parameter, and one row per output. When training a neural net with
    gradient descent, there’s a single output—the loss—so the matrix contains a single
    row, and there’s one column per model parameter, so it’s a 1×*n* matrix. The *Hessian
    matrix* contains all the second-order derivatives of a single-output function
    with multiple parameters: for each model parameter it contains one row and one
    column, so it’s an *n*×*n* matrix. The informal names *Jacobians* and *Hessians*
    refer to the elements of these matrices.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch11.html#id2656-marker)) **雅可比矩阵** 包含了一个具有多个参数和多个输出的函数的所有一阶偏导数：每一列对应一个参数，每一行对应一个输出。当使用梯度下降训练神经网络时，只有一个输出——损失，因此矩阵包含一个行，并且每一列对应一个模型参数，所以它是一个
    1×*n* 矩阵。**海森矩阵** 包含了一个具有多个参数的单输出函数的所有二阶导数：对于每个模型参数，它包含一行和一列，所以它是一个 *n*×*n* 矩阵。非正式名称“雅可比”和“海森”指的是这些矩阵的元素。
- en: '^([29](ch11.html#id2658-marker)) V. Gupta et al., [“Shampoo: Preconditioned
    Stochastic Tensor Optimization”](https://homl.info/shampoo), arXiv preprint arXiv:1802.09568
    (2018).'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch11.html#id2658-marker)) V. Gupta 等人，“Shampoo：预条件随机张量优化”，[“洗发水：预条件随机张量优化”](https://homl.info/shampoo)，arXiv
    预印本 arXiv:1802.09568 (2018)。
- en: '^([30](ch11.html#id2674-marker)) Ilya Loshchilov and Frank Hutter, “SGDR: Stochastic
    Gradient Descent With Warm Restarts”, arXiv preprint arXiv:1608.03983 (2016).'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch11.html#id2674-marker)) Ilya Loshchilov 和 Frank Hutter，“SGDR：带有预热重启的随机梯度下降”，arXiv
    预印本 arXiv:1608.03983 (2016)。
- en: '^([31](ch11.html#id2677-marker)) Leslie N. Smith, “A Disciplined Approach to
    Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and
    Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch11.html#id2677-marker)) Leslie N. Smith，“对神经网络超参数的纪律性方法：第 1 部分——学习率、批量大小、动量和权重衰减”，arXiv
    预印本 arXiv:1803.09820 (2018)。
- en: ^([32](ch11.html#id2696-marker)) Geoffrey E. Hinton et al., “Improving Neural
    Networks by Preventing Co-Adaptation of Feature Detectors”, arXiv preprint arXiv:1207.0580
    (2012).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch11.html#id2696-marker)) Geoffrey E. Hinton 等人，“通过防止特征检测器的共适应来改进神经网络”，arXiv
    预印本 arXiv:1207.0580 (2012)。
- en: '^([33](ch11.html#id2697-marker)) Nitish Srivastava et al., “Dropout: A Simple
    Way to Prevent Neural Networks from Overfitting”, *Journal of Machine Learning
    Research* 15 (2014): 1929–1958.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '^([33](ch11.html#id2697-marker)) Nitish Srivastava 等人，“Dropout：防止神经网络过拟合的简单方法”，*机器学习研究杂志*
    15 (2014): 1929–1958。'
- en: '^([34](ch11.html#id2709-marker)) Yarin Gal and Zoubin Ghahramani, “Dropout
    as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”,
    *Proceedings of the 33rd International Conference on Machine Learning* (2016):
    1050–1059.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '^([34](ch11.html#id2709-marker)) Yarin Gal 和 Zoubin Ghahramani，“Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性”，*第
    33 届国际机器学习会议论文集* (2016): 1050–1059。'
- en: ^([35](ch11.html#id2710-marker)) Specifically, they show that training a dropout
    network is mathematically equivalent to approximate Bayesian inference in a specific
    type of probabilistic model called a *deep Gaussian process*.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch11.html#id2710-marker)) 特别是，他们表明训练 dropout 网络在数学上等同于在一种称为 **深度高斯过程**
    的特定类型概率模型中的近似贝叶斯推理。
