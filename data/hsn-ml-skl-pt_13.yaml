- en: Chapter 11\. Training Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章\. 深度神经网络的训练
- en: 'In [Chapter 10](ch10.html#pytorch_chapter) you built, trained, and fine-tuned
    several artificial neural networks using PyTorch. But they were shallow nets with
    just a few hidden layers. What if you need to tackle a complex problem, such as
    detecting hundreds of types of objects in high-resolution images? You may need
    to train a much deeper ANN, perhaps with dozens or even hundreds of layers, each
    containing hundreds of neurons, linked by hundreds of thousands of connections.
    Training a deep neural network isn’t a walk in the park. Here are some of the
    problems you could run into:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#pytorch_chapter)中，您使用PyTorch构建、训练和微调了几个人工神经网络。但它们只是具有几个隐藏层的浅层网络。如果您需要解决一个复杂的问题，比如在高分辨率图像中检测数百种类型的对象，您可能需要训练一个深度更大的ANN，可能包含数十甚至数百层，每层包含数百个神经元，通过数十万个连接相互连接。训练深度神经网络并非易事。以下是一些您可能会遇到的问题：
- en: You may be faced with the problem of gradients growing ever smaller or larger
    when flowing backward through the DNN during training. Both of these problems
    make lower layers very hard to train.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，您可能会遇到在反向传播通过深度神经网络（DNN）时梯度不断变小或变大的问题。这两个问题都使得训练低层变得非常困难。
- en: You might not have enough training data for such a large network, or it might
    be too costly to label.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能没有足够的训练数据来支持这样一个大的网络，或者标注成本可能过高。
- en: Training may be extremely slow.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能非常缓慢。
- en: A model with millions of parameters risks severely overfitting the training
    set, especially if there are not enough training instances or if they are too
    noisy.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有数百万个参数的模型可能会严重过拟合训练集，尤其是如果没有足够的训练实例或者它们太嘈杂的情况下。
- en: 'In this chapter we will go through each of these problems and present various
    techniques to solve them. We will start by exploring the vanishing and exploding
    gradients problems and some of their most popular solutions, including smart weight
    initialization, better activation functions, batch-norm, layer-norm, and gradient
    clipping. Next, we will look at transfer learning and unsupervised pretraining,
    which can help you tackle complex tasks even when you have little labeled data.
    Then we will discuss a variety of optimizers that can speed up training large
    models tremendously. We will also discuss how you can tweak the learning rate
    during training to speed up training and produce better models. Finally, we will
    cover a few popular regularization techniques for large neural networks: ℓ[1]
    and ℓ[2] regularization, dropout, Monte Carlo dropout, and max-norm regularization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐一探讨这些问题，并介绍各种解决方法。我们将首先探讨梯度消失和爆炸问题及其最流行的解决方案，包括智能权重初始化、更好的激活函数、批归一化、层归一化和梯度裁剪。接下来，我们将探讨迁移学习和无监督预训练，这可以帮助您在少量标记数据的情况下处理复杂任务。然后，我们将讨论各种优化器，它们可以极大地加速大型模型的训练。我们还将讨论如何在训练过程中调整学习率以加快训练速度并产生更好的模型。最后，我们将介绍适用于大型神经网络的几种流行正则化技术：ℓ[1]和ℓ[2]正则化、dropout、蒙特卡洛dropout和最大范数正则化。
- en: With these tools, you will be able to train all sorts of deep nets. Welcome
    to *deep* learning!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工具，您将能够训练各种深度网络。欢迎来到*深度*学习！
- en: The Vanishing/Exploding Gradients Problems
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失/爆炸问题
- en: As discussed in [Chapter 9](ch09.html#ann_chapter), the backpropagation algorithm’s
    second phase works by going from the output layer to the input layer, propagating
    the error gradient along the way. Once the algorithm has computed the gradient
    of the cost function with regard to each parameter in the network, it uses these
    gradients to update each parameter with a gradient descent step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第9章](ch09.html#ann_chapter)所述，反向传播算法的第二阶段是通过从输出层到输入层，沿途传播误差梯度来工作的。一旦算法计算了网络中每个参数相对于成本函数梯度的值，它就会使用这些梯度通过梯度下降步骤来更新每个参数。
- en: 'Unfortunately, gradients often get smaller and smaller as the algorithm progresses
    down to the lower layers. As a result, the gradient descent update leaves the
    lower layers’ connection weights virtually unchanged, and training never converges
    to a good solution. This is called the *vanishing gradients* problem. In some
    cases, the opposite can happen: the gradients can grow bigger and bigger until
    layers get insanely large weight updates and the algorithm diverges. This is the
    *exploding gradients* problem, which surfaces most often in recurrent neural networks
    (see [Chapter 13](ch13.html#rnn_chapter)). More generally, deep neural networks
    suffer from unstable gradients; different layers may learn at widely different
    speeds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，随着算法向底层推进，梯度往往会越来越小。因此，梯度下降更新几乎不会改变底层连接权重，训练永远不会收敛到一个好的解。这被称为*梯度消失*问题。在某些情况下，情况可能相反：梯度会越来越大，直到层得到极端巨大的权重更新，算法发散。这被称为*梯度爆炸*问题，在循环神经网络（见第13章）中最为常见。更普遍地说，深度神经网络受到不稳定梯度的困扰；不同的层可能以非常不同的速度学习。
- en: This unfortunate behavior was empirically observed long ago, and it was one
    of the reasons deep neural networks were mostly abandoned in the early 2000s.
    It wasn’t clear what caused the gradients to be so unstable when training a DNN,
    but some light was shed in a [2010 paper](https://homl.info/47) by Xavier Glorot
    and Yoshua Bengio.⁠^([1](ch11.html#id2449)) The authors found a few suspects,
    including the combination of the popular sigmoid (logistic) activation function
    and the weight initialization technique that was most popular at the time (i.e.,
    a normal distribution with a mean of 0 and a standard deviation of 1). In short,
    they showed that with this activation function and this initialization scheme,
    the variance of the outputs of each layer is much greater than the variance of
    its inputs. Going forward in the network, the variance keeps increasing after
    each layer until the activation function saturates at the top layers. This saturation
    is actually made worse by the fact that the sigmoid function has a mean of 0.5,
    not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better
    than the sigmoid function in deep networks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不幸的行为很久以前就被观察到了，这也是深度神经网络在2000年代初大部分被放弃的原因之一。当训练深度神经网络时，不清楚是什么导致了梯度如此不稳定，但在2010年的一篇论文（https://homl.info/47）中，Xavier
    Glorot和Yoshua Bengio提供了一些线索。⁠^([1](ch11.html#id2449)) 作者发现了一些嫌疑人，包括当时最受欢迎的sigmoid（逻辑）激活函数和最流行的权重初始化技术（即均值为0，标准差为1的正态分布）。简而言之，他们表明，使用这种激活函数和这种初始化方案，每一层的输出方差远大于其输入的方差。在网络中向前推进时，方差在每一层之后都会增加，直到激活函数在顶层饱和。这种饱和实际上因为sigmoid函数的均值为0.5而不是0（双曲正切函数的均值为0，在深度网络中表现略好于sigmoid函数）而变得更糟。
- en: Looking at the sigmoid activation function (see [Figure 11-1](#sigmoid_saturation_plot)),
    you can see that when inputs become large (negative or positive), the function
    saturates at 0 or 1, with a derivative extremely close to 0 (i.e., the curve is
    flat at both extremes). Thus, when backpropagation kicks in it has virtually no
    gradient to propagate back through the network, and what little gradient exists
    keeps getting diluted as backpropagation progresses down through the top layers,
    so there is really nothing left for the lower layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 观察sigmoid激活函数（见图11-1），你可以看到当输入变得很大（负数或正数）时，函数在0或1处饱和，导数非常接近0（即曲线在两端都是平的）。因此，当反向传播开始时，它几乎没有任何梯度可以传播回网络，而存在的少量梯度在反向传播通过顶层的过程中不断被稀释，所以实际上留给底层的东西很少。
- en: '![Diagram illustrating the sigmoid activation function, showing how it saturates
    at 0 for large negative inputs and 1 for large positive inputs, with a linear
    region in the middle.](assets/hmls_1101.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![说明sigmoid激活函数的图，展示了它在大负数输入时饱和于0，在大正数输入时饱和于1，中间有一个线性区域。](assets/hmls_1101.png)'
- en: Figure 11-1\. Sigmoid activation function saturation
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. Sigmoid激活函数饱和
- en: Glorot Initialization and He Initialization
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glorot初始化和He初始化
- en: 'In their paper, Glorot and Bengio proposed a way to significantly alleviate
    the unstable gradients problem. They pointed out that we need the signal to flow
    properly in both directions: in the forward direction when making predictions,
    and in the reverse direction when backpropagating gradients. We don’t want the
    signal to die out, nor do we want it to explode and saturate. For the signal to
    flow properly, the authors argued that we need the variance of the outputs of
    each layer to be equal to the variance of its inputs,⁠^([2](ch11.html#id2451))
    and we need the gradients to have equal variance before and after flowing through
    a layer in the reverse direction (please check out the paper if you are interested
    in the mathematical details). It is actually not possible to guarantee both unless
    the layer has an equal number of inputs and outputs (these numbers are called
    the *fan-in* and *fan-out* of the layer), but Glorot and Bengio proposed a good
    compromise that has proven to work very well in practice: the connection weights
    of each layer must be initialized randomly, as described in [Equation 11-1](#xavier_initialization_equation),
    where *fan*[avg] = (*fan*[in] + *fan*[out]) / 2\. This initialization strategy
    is called *Xavier initialization* or *Glorot initialization*, after the paper’s
    first author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，Glorot和Bengio提出了一种显著缓解不稳定梯度问题的方法。他们指出，我们需要信号在两个方向上正确流动：在预测时的前向方向，以及在反向传播梯度时的反向方向。我们不希望信号消失，也不希望它爆炸并饱和。为了使信号正确流动，作者们认为我们需要每一层的输出方差等于其输入方差，⁠^([2](ch11.html#id2451))
    并且在反向方向通过一层之前和之后，梯度需要具有相同的方差（如果你对数学细节感兴趣，请查看论文）。实际上，除非层有相等数量的输入和输出（这些数字被称为层的 *fan-in*
    和 *fan-out*），否则不可能保证两者都成立。但Glorot和Bengio提出了一种在实践中证明非常有效的良好折衷方案：每一层的连接权重必须随机初始化，如[Equation
    11-1](#xavier_initialization_equation)中所述，其中 *fan*[avg] = (*fan*[in] + *fan*[out])
    / 2。这种初始化策略被称为 *Xavier initialization* 或 *Glorot initialization*，以论文的第一作者命名。
- en: Equation 11-1\. Glorot initialization (when using the sigmoid activation function)
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 11-1\. Glorot initialization（使用 sigmoid 激活函数时）
- en: <mtable columnalign="left"><mtr><mtd><mtext>Normal distribution with mean 0
    and variance </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or
    a uniform distribution between </mtext><mo>-</mo><mi>r</mi><mtext> and </mtext><mo>+</mo><mi>r</mi><mtext>,
    with </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable columnalign="left"><mtr><mtd><mtext>均值为0且方差为 </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>或者在一个均匀分布之间 </mtext><mo>-</mo><mi>r</mi><mtext> 和 </mtext><mo>+</mo><mi>r</mi><mtext>，其中 </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi
    mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable>
- en: 'If you replace *fan*[avg] with *fan*[in] in [Equation 11-1](#xavier_initialization_equation),
    you get an initialization strategy that Yann LeCun proposed in the 1990s. He called
    it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended
    it in their 1998 book *Neural Networks: Tricks of the Trade* (Springer). LeCun
    initialization is equivalent to Glorot initialization when *fan*[in] = *fan*[out].
    It took over a decade for researchers to realize how important this trick is.
    Using Glorot initialization can speed up training considerably, and it is one
    of the tricks that led to the success of deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你将[Equation 11-1](#xavier_initialization_equation)中的 *fan*[avg] 替换为 *fan*[in]，你将得到一种Yann
    LeCun在20世纪90年代提出的初始化策略。他称之为 *LeCun initialization*。Genevieve Orr和Klaus-Robert
    Müller甚至在他们的1998年书籍 *Neural Networks: Tricks of the Trade* (Springer) 中推荐了它。当
    *fan*[in] = *fan*[out] 时，LeCun initialization 等同于 Glorot initialization。研究人员花了十多年时间才意识到这个技巧的重要性。使用
    Glorot initialization 可以显著加快训练速度，这是导致深度学习成功的关键技巧之一。'
- en: Some papers have provided similar strategies for different activation functions,
    most notably a [2015 paper by Kaiming He et al](https://homl.info/48).⁠^([3](ch11.html#id2456))
    These strategies differ only by the scale of the variance and whether they use
    *fan*[avg] or *fan*[in], as shown in [Table 11-1](#initialization_table) (for
    the uniform distribution, just use <mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt>).
    The initialization strategy proposed for the ReLU activation function and its
    variants is called *He initialization* or *Kaiming initialization*, after the
    paper’s first author. For SELU, use Yann LeCun’s initialization method, preferably
    with a normal distribution. We will cover all these activation functions shortly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文为不同的激活函数提供了类似的策略，最著名的是Kaiming He等人于2015年发表的一篇论文[2015 paper by Kaiming He
    et al](https://homl.info/48)。⁠^([3](ch11.html#id2456)) 这些策略仅在方差缩放的比例以及是否使用*fan*[avg]或*fan*[in]上有所不同，如[表11-1](#initialization_table)所示（对于均匀分布，只需使用<mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt>）。为ReLU激活函数及其变体提出的初始化策略被称为*He初始化*或*Kaiming初始化*，以论文的第一作者命名。对于SELU，使用Yann
    LeCun的初始化方法，最好使用正态分布。我们将在稍后介绍所有这些激活函数。
- en: Table 11-1\. Initialization parameters for each type of activation function
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1\. 每种激活函数的初始化参数
- en: '| Initialization | Activation functions | *σ*² (Normal) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 初始化 | 激活函数 | *σ*² (正态分布) |'
- en: '| --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Xavier Glorot | None, tanh, sigmoid, softmax | 1 / *fan*[avg] |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Xavier Glorot | None, tanh, sigmoid, softmax | 1 / *fan*[avg] |'
- en: '| Kaiming He | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish, SwiGLU, ReLU² | 2
    / *fan*[in] |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Kaiming He | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish, SwiGLU, ReLU² | 2
    / *fan*[in] |'
- en: '| Yann LeCun | SELU | 1 / *fan*[in] |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Yann LeCun | SELU | 1 / *fan*[in] |'
- en: 'For historical reasons, PyTorch’s `nn.Linear` module initializes its weights
    using Kaiming uniform initialization, except the weights are scaled down by a
    factor of $StartRoot 6 EndRoot$ (and the bias terms are also initialized randomly).
    Sadly, this is not the optimal scale for any common activation function.⁠^([4](ch11.html#id2457))
    One solution is to simply multiply the weights by $StartRoot 6 EndRoot$ (i.e.,
    6^(0.5)) just after creating the `nn.Linear` layer to get proper Kaiming initialization.
    To do this, we can update the parameter’s `data` attribute. We will also zero
    out the biases, as there’s no benefit in randomly initializing them:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于历史原因，PyTorch的`nn.Linear`模块使用Kaiming均匀初始化来初始化其权重，但权重会乘以一个因子$StartRoot 6 EndRoot$（并且偏置项也会随机初始化）。遗憾的是，这并不是任何常见激活函数的最佳缩放比例。一种解决方案是在创建`nn.Linear`层后立即将权重乘以$StartRoot
    6 EndRoot$（即6的0.5次方），以获得适当的Kaiming初始化。为此，我们可以更新参数的`data`属性。我们还将清零偏置，因为没有随机初始化它们的任何好处：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This works, but it’s clearer and less error-prone to use one of the initialization
    functions available in the `torch.nn.init` module:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这方法可行，但使用`torch.nn.init`模块中可用的初始化函数会更清晰且错误更少：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to apply the same initialization method to the weights of every
    `nn.Linear` layer in a model, you can do so in the model’s constructor, after
    creating each `nn.Linear` layer. Alternatively, you can write a subclass of the
    `nn.Linear` class and tweak its constructor to initialize the weights as you wish.
    But arguably the simplest option is to write a little function that takes a module,
    checks whether it’s an instance of the `nn.Linear` class, and if so, applies the
    desired initialization function to its weights. You can then apply this function
    to the model and all of its submodules by passing it to the model’s `apply()`
    method. For example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将相同的初始化方法应用到模型中每个`nn.Linear`层的权重上，你可以在创建每个`nn.Linear`层之后在模型的构造函数中这样做。或者，你可以编写`nn.Linear`类的子类，并调整其构造函数以按需初始化权重。但可能最简单的方法是编写一个函数，该函数接受一个模块，检查它是否是`nn.Linear`类的实例，如果是，则对其权重应用所需的初始化函数。然后，你可以通过将其传递给模型的`apply()`方法来将此函数应用到模型及其所有子模块。例如：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `torch.nn.init` module also contains an `orthogonal_()` function which
    initializes the weights using a random orthogonal matrix, as proposed in a [2014
    paper](https://homl.info/ortho-init) by Andrew Saxe et al.⁠^([5](ch11.html#id2461))
    Orthogonal matrices have a number of useful mathematical properties, including
    the fact that they preserve norms: given an orthogonal matrix **W** and an input
    vector **x**, the norm of **Wx** is equal to the norm of **x**, and therefore
    the magnitude of the inputs is preserved in the outputs. When the inputs are standardized,
    this results in a stable variance through the layer, which prevents the activations
    and gradients from vanishing or exploding in a deep network (at least at the beginning
    of training). This initialization technique is much less common than the initialization
    techniques discussed earlier, but it can work well in recurrent neural nets ([Chapter 13](ch13.html#rnn_chapter))
    or generative adversarial networks ([Chapter 18](ch18.html#autoencoders_chapter)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.init`模块还包含一个`orthogonal_()`函数，该函数使用随机正交矩阵初始化权重，正如Andrew Saxe等人在2014年发表的[论文](https://homl.info/ortho-init)中提出的。正交矩阵具有许多有用的数学性质，包括它们保持范数的事实：给定一个正交矩阵**W**和一个输入向量**x**，**Wx**的范数等于**x**的范数，因此输入的幅度在输出中得到了保留。当输入被标准化时，这会在层中产生稳定的方差，从而防止激活和梯度在深度网络中消失或爆炸（至少在训练的初期）。这种初始化技术比前面讨论的初始化技术要少见得多，但它可以在循环神经网络（[第13章](ch13.html#rnn_chapter)）或生成对抗网络（[第18章](ch18.html#autoencoders_chapter)）中工作得很好。'
- en: And that’s it! Scaling the weights properly will give a deep neural net a much
    better starting point for training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！适当地缩放权重将为深度神经网络提供一个更好的训练起点。
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In a classifier, it’s generally a good idea to scale down the weights of the
    output layer during initialization (e.g., by a factor of 10). Indeed, this will
    result in smaller logits at the beginning of training, which means they will be
    closer together, and hence the estimated probabilities will also be closer together.
    In other words, it encourages the model to be less confident about its predictions
    when training starts: this will avoid extreme losses and huge gradients that can
    often make the model’s weights bounce around randomly at the start of training,
    losing time and potentially preventing the model from learning anything.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器中，在初始化过程中降低输出层的权重通常是一个好主意（例如，乘以10）。事实上，这将导致训练开始时的logits更小，这意味着它们将更接近，因此估计的概率也将更接近。换句话说，这鼓励模型在训练开始时对其预测不太自信：这将避免极端损失和巨大的梯度，这通常会导致模型在训练开始时权重随机跳动，浪费时间和可能阻止模型学习任何东西。
- en: Better Activation Functions
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的激活函数
- en: One of the insights in the 2010 paper by Glorot and Bengio was that the problems
    with unstable gradients were in part due to a poor choice of activation function.
    Until then most people had assumed that if Mother Nature had chosen to use something
    pretty close to sigmoid activation functions in biological neurons, they must
    be an excellent choice. But it turns out that other activation functions behave
    much better in deep neural networks—in particular, the ReLU activation function,
    mostly because it does not saturate for positive values, and also because it is
    very fast to compute.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Glorot和Bengio在2010年的论文中的一个洞见是，不稳定梯度的部分问题源于激活函数的选择不当。在此之前，大多数人假设如果大自然选择在生物神经元中使用接近sigmoid激活函数的东西，那么它们肯定是一个很好的选择。但事实表明，其他激活函数在深度神经网络中表现得更好——特别是ReLU激活函数，主要是因为它对于正值不会饱和，而且它计算速度非常快。
- en: 'Unfortunately, the ReLU activation function is not perfect. It suffers from
    a problem known as the *dying ReLUs*: during training, some neurons effectively
    “die”, meaning they stop outputting anything other than 0\. In some cases, you
    may find that half of your network’s neurons are dead, especially if you used
    a large learning rate. A neuron dies when its weights get tweaked in such a way
    that the input of the ReLU function (i.e., the weighted sum of the neuron’s inputs
    plus its bias term) is negative for all instances in the training set. When this
    happens, it just keeps outputting zeros, and gradient descent does not affect
    it anymore because the gradient of the ReLU function is zero when its input is
    negative.⁠^([6](ch11.html#id2473))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，ReLU激活函数并不完美。它存在一个被称为*渐死ReLU*的问题：在训练过程中，一些神经元实际上“死亡”，意味着它们停止输出除了0以外的任何内容。在某些情况下，你可能发现你网络中一半的神经元都“死亡”了，尤其是如果你使用了较大的学习率。当一个神经元的权重被调整到使得ReLU函数的输入（即神经元输入的加权和加上其偏置项）对于训练集中的所有实例都是负值时，神经元就会“死亡”。当这种情况发生时，它只会持续输出0，因为当ReLU函数的输入为负值时，其梯度为0，所以梯度下降不再影响它。⁠^([6](ch11.html#id2473))
- en: To solve this problem, you may want to use a variant of the ReLU function, such
    as the *leaky ReLU*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可能想使用ReLU函数的一个变体，比如*Leaky ReLU*。
- en: Leaky ReLU
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'The leaky ReLU activation function is defined as LeakyReLU[*α*](*z*) = max(*αz*,
    *z*) (see [Figure 11-2](#leaky_relu_plot)). The hyperparameter *α* defines how
    much the function “leaks”: it is the slope of the function for *z* < 0\. Having
    a slope for *z* < 0 ensures that leaky ReLUs never actually die; they can go into
    a long coma, but they have a chance to eventually wake up. A [2015 paper](https://homl.info/49)
    by Bing Xu et al.⁠^([7](ch11.html#id2477)) compared several variants of the ReLU
    activation function, and one of its conclusions was that the leaky variants always
    outperformed the strict ReLU activation function. In fact, setting *α* = 0.2 (a
    huge leak) seemed to result in better performance than *α* = 0.01 (a small leak).
    The paper also evaluated the *randomized leaky ReLU* (RReLU), where *α* is picked
    randomly in a given range during training and is fixed to an average value during
    testing. RReLU also performed fairly well and seemed to act as a regularizer,
    reducing the risk of overfitting. Finally, the paper evaluated the *parametric
    leaky ReLU* (PReLU), where *α* is authorized to be learned during training: instead
    of being a hyperparameter, it becomes a parameter that can be modified by backpropagation
    like any other parameter. PReLU was reported to strongly outperform ReLU on large
    image datasets, but on smaller datasets it runs the risk of overfitting the training
    set.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU激活函数定义为LeakyReLU[*α*](*z*) = max(*αz*, *z*)（见[图11-2](#leaky_relu_plot)）。超参数*α*定义了函数“泄漏”的程度：它是函数在*z*
    < 0时的斜率。对于*z* < 0有斜率确保Leaky ReLU永远不会真正“死亡”；它们可以进入长时间的昏迷，但最终有苏醒的机会。Bing Xu等人于2015年发表的一篇[论文](https://homl.info/49)⁠^([7](ch11.html#id2477))比较了几种ReLU激活函数的变体，其结论之一是Leaky
    ReLU变体总是优于严格的ReLU激活函数。实际上，将*α*设置为0.2（巨大的泄漏）似乎比*α* = 0.01（小的泄漏）有更好的性能。该论文还评估了*随机Leaky
    ReLU*（RReLU），其中*α*在训练期间随机选择一个范围内的值，并在测试期间固定为平均值。RReLU也表现相当不错，似乎起到了正则化的作用，减少了过拟合的风险。最后，该论文评估了*参数Leaky
    ReLU*（PReLU），其中*α*在训练期间被允许学习：它不再是超参数，而成为一个可以像其他任何参数一样通过反向传播进行修改的参数。PReLU据报道在大型图像数据集上显著优于ReLU，但在较小的数据集上存在过拟合训练集的风险。
- en: '![Diagram of the leaky ReLU activation function illustrating its slope for
    negative values, demonstrating the "leak."](assets/hmls_1102.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Leaky ReLU激活函数的示意图，说明了其负值的斜率，展示了“泄漏”现象。](assets/hmls_1102.png)'
- en: 'Figure 11-2\. Leaky ReLU: like ReLU, but with a small slope for negative values'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. Leaky ReLU：与ReLU类似，但负值具有较小的斜率
- en: 'As you might expect, PyTorch includes modules for each of these activation
    functions: `nn.LeakyReLU`, `nn.RReLU`, and `nn.PReLU`. Just like for other ReLU
    variants, you should use these along with Kaiming initialization, but the variance
    should be slightly smaller due to the negative slope: it should be scaled down
    by a factor of 1 + *α*². PyTorch supports this: you can pass the *α* hyperparameter
    to the `kaiming_uniform_()` and `kaiming_normal_()` functions, along with `nonlinearity="leaky_relu"`
    to get the appropriately adjusted Kaiming initialization:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期，PyTorch 包含了这些激活函数各自的模块：`nn.LeakyReLU`、`nn.RReLU` 和 `nn.PReLU`。就像其他 ReLU
    变体一样，您应该与 Kaiming 初始化一起使用这些模块，但由于负斜率，方差应略小一些：它应该通过一个因子 1 + *α*² 缩放。PyTorch 支持：您可以将
    *α* 超参数传递给 `kaiming_uniform_()` 和 `kaiming_normal_()` 函数，以及 `nonlinearity="leaky_relu"`
    以获得适当的调整后的 Kaiming 初始化：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth
    functions: their slopes abruptly change at *z* = 0\. As we saw in [Chapter 4](ch04.html#linear_models_chapter)
    when we discussed lasso, this sort of discontinuity in the derivatives can make
    gradient descent bounce around the optimum and slow down convergence. So now we
    will look at some smooth variants of the ReLU activation function, starting with
    ELU and SELU.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU、leaky ReLU 和 PReLU 都存在它们不是平滑函数的事实：它们的斜率在 *z* = 0 处突然改变。正如我们在 [第 4 章](ch04.html#linear_models_chapter)
    中讨论 lasso 时所看到的，这种导数中的不连续性可以使梯度下降在最优解周围弹跳并减慢收敛速度。因此，现在我们将查看 ReLU 激活函数的一些平滑变体，从
    ELU 和 SELU 开始。
- en: ELU and SELU
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELU 和 SELU
- en: 'In 2015, a [paper](https://homl.info/50) by Djork-Arné Clevert et al.⁠^([8](ch11.html#id2492))
    proposed a new activation function, called the *exponential linear unit* (ELU),
    that outperformed all the ReLU variants in the authors’ experiments: training
    time was reduced, and the neural network performed better on the test set. [Equation
    11-2](#elu_activation_equation) shows this activation function’s definition.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年，Djork-Arné Clevert 等人发表的一篇 [论文](https://homl.info/50)⁠^([8](ch11.html#id2492))
    提出了一种新的激活函数，称为 *指数线性单元*（ELU），在作者们的实验中优于所有 ReLU 变体：训练时间减少，神经网络在测试集上的表现更好。[方程 11-2](#elu_activation_equation)
    展示了此激活函数的定义。
- en: Equation 11-2\. ELU activation function
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 11-2\. ELU 激活函数
- en: <mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mi>α</mi> <mo>(</mo> <mtext>exp</mtext> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mo form="prefix">ELU</mo> <mi>α</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mi>α</mi> <mo>(</mo> <mtext>exp</mtext> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>z</mi></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow>
- en: 'The ELU activation function looks a lot like the ReLU function (see [Figure 11-3](#elu_and_selu_activation_plot)),
    with a few major differences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 激活函数在许多方面与 ReLU 函数相似（参见 [图 11-3](#elu_and_selu_activation_plot)），但有几个主要区别：
- en: It takes on negative values when *z* < 0, which allows the unit to have an average
    output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter
    *α* defines the opposite of the value that the ELU function approaches when *z*
    is a large negative number. It is usually set to 1, but you can tweak it like
    any other hyperparameter.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *z* < 0 时，函数取负值，这使得单元的平均输出更接近 0，有助于缓解梯度消失问题。超参数 *α* 定义了当 *z* 是一个很大的负数时，ELU
    函数趋近的值的相反数。它通常设置为 1，但您可以像调整任何其他超参数一样调整它。
- en: It has a nonzero gradient for *z* < 0, which avoids the dead neurons problem.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *z* < 0 时，它具有非零梯度，这避免了死亡神经元问题。
- en: If *α* is equal to 1, then the function is smooth everywhere, including around
    *z* = 0, which helps speed up gradient descent since it does not bounce as much
    to the left and right of *z* = 0.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *α* 等于 1 时，函数在所有地方都是平滑的，包括在 *z* = 0 附近，这有助于加快梯度下降，因为它不会在 *z* = 0 的左右弹跳得那么厉害。
- en: Using ELU with PyTorch is as easy as using the `nn.ELU` module, along with Kaiming
    initialization. The main drawback of the ELU activation function is that it is
    slower to compute than the ReLU function and its variants (due to the use of the
    exponential function). Its faster convergence rate during training may compensate
    for that slow computation, but still, at test time an ELU network will be a bit
    slower than a ReLU network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ELU 与 PyTorch 一样简单，只需使用 `nn.ELU` 模块，以及 Kaiming 初始化。ELU 激活函数的主要缺点是它比 ReLU
    函数及其变体计算速度慢（由于使用了指数函数）。它在训练期间更快的收敛速度可能可以弥补这种慢速计算，但仍然，在测试时间一个 ELU 网络将比一个 ReLU 网络慢一点。
- en: '![Graph comparing ELU and SELU activation functions, showing SELU is scaled
    higher than ELU.](assets/hmls_1103.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![比较 ELU 和 SELU 激活函数的图表，显示 SELU 的缩放值高于 ELU。](assets/hmls_1103.png)'
- en: Figure 11-3\. ELU and SELU activation functions
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. ELU 和 SELU 激活函数
- en: 'Not long after, a [2017 paper](https://homl.info/selu) by Günter Klambauer
    et al.⁠^([9](ch11.html#id2500)) introduced the *scaled ELU* (SELU) activation
    function: as its name suggests, it is a scaled variant of the ELU activation function
    (about 1.05 times ELU, using *α* ≈ 1.67). The authors showed that if you build
    a neural network composed exclusively of a stack of dense layers (i.e., an MLP),
    and if all hidden layers use the SELU activation function, then the network will
    *self-normalize*: the output of each layer will tend to preserve a mean of 0 and
    a standard deviation of 1 during training, which solves the vanishing/exploding
    gradients problem. As a result, the SELU activation function may outperform other
    activation functions for MLPs, especially deep ones. To use it with PyTorch, just
    use `nn.SELU`. There are, however, a few conditions for self-normalization to
    happen (see the paper for the mathematical justification):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，Günter Klambauer 等人在 2017 年发表的一篇[论文](https://homl.info/selu)⁠^([9](ch11.html#id2500))中介绍了
    *缩放 ELU* (SELU) 激活函数：正如其名称所暗示的，它是 ELU 激活函数的一个缩放变体（大约是 ELU 的 1.05 倍，使用 *α* ≈ 1.67）。作者表明，如果你构建一个由堆叠的密集层（即
    MLP）组成的神经网络，并且如果所有隐藏层都使用 SELU 激活函数，那么网络将 *自我归一化*：每个层的输出在训练过程中将倾向于保持均值为 0 和标准差为
    1，这解决了梯度消失/爆炸问题。因此，SELU 激活函数可能比其他激活函数更适合 MLP，尤其是深度 MLP。要在 PyTorch 中使用它，只需使用 `nn.SELU`。然而，自我归一化发生有几个条件（参见论文中的数学证明）：
- en: 'The input features must be standardized: mean 0 and standard deviation 1.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征必须标准化：均值为 0，标准差为 1。
- en: Every hidden layer’s weights must be initialized using LeCun normal initialization.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的权重都必须使用 LeCun 正态初始化。
- en: The self-normalizing property is only guaranteed with plain MLPs. If you try
    to use SELU in other architectures, like recurrent networks (see [Chapter 13](ch13.html#rnn_chapter))
    or networks with *skip connections* (i.e., connections that skip layers, such
    as in Wide & Deep neural networks), it will probably not outperform ELU.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我归一化属性仅在简单的 MLP 中得到保证。如果你尝试在其他架构中使用 SELU，如循环网络（参见[第 13 章](ch13.html#rnn_chapter)）或具有
    *跳跃连接*（即跳过层的连接，例如在 Wide & Deep 神经网络中），它可能不会优于 ELU。
- en: You cannot use regularization techniques like ℓ[1] or ℓ[2] regularization, batch-norm,
    layer-norm, max-norm, or regular dropout (these are discussed later in this chapter).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能使用正则化技术，如 ℓ[1] 或 ℓ[2] 正则化、批归一化、层归一化、最大归一化或常规 dropout（这些将在本章后面讨论）。
- en: These are significant constraints, so despite its promises, SELU did not gain
    a lot of traction. Moreover, other activation functions seem to outperform it
    quite consistently on most tasks. Let’s look at some of the most popular ones.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是重要的约束条件，因此尽管 SELU 有其承诺，但它并没有获得太多的关注。此外，其他激活函数似乎在大多数任务上都能相当一致地优于它。让我们看看其中一些最受欢迎的。
- en: GELU, Swish, SwiGLU, Mish, and RELU²
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GELU, Swish, SwiGLU, Mish, 和 RELU²
- en: 'The *Gaussian Error Linear Unit* (*GELU*) was introduced in a [2016 paper](https://homl.info/gelu)
    by Dan Hendrycks and Kevin Gimpel.^([10](ch11.html#id2505)) Once again, you can
    think of it as a smooth variant of the ReLU activation function. Its definition
    is given in [Equation 11-3](#gelu_activation_equation), where Φ is the standard
    Gaussian cumulative distribution function (CDF): Φ(*z*) corresponds to the probability
    that a value sampled randomly from a normal distribution of mean 0 and variance
    1 is lower than *z*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯误差线性单元* (*GELU*) 是由 Dan Hendrycks 和 Kevin Gimpel 在一篇[2016 年的论文](https://homl.info/gelu)中引入的.^([10](ch11.html#id2505))
    再次，你可以将其视为 ReLU 激活函数的一个平滑变体。其定义在 [方程 11-3](#gelu_activation_equation) 中给出，其中 Φ
    是标准高斯累积分布函数 (CDF)：Φ(*z*) 对应于从均值为 0 和方差 1 的正态分布中随机抽取的值小于 *z* 的概率。'
- en: Equation 11-3\. GELU activation function
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11-3\. GELU激活函数
- en: <mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mi
    mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo>
- en: 'As you can see in [Figure 11-4](#gelu_swish_mish_plot), GELU resembles ReLU:
    it approaches 0 when its input *z* is very negative, and it approaches *z* when
    *z* is very positive. However, whereas all the activation functions we’ve discussed
    so far were both convex and monotonic,^([11](ch11.html#id2506)) the GELU activation
    function is neither: from left to right, it starts by going straight, then it
    wiggles down, reaches a low point around –0.17 (near z ≈ –0.75), and finally bounces
    up and ends up going straight toward the top right. This fairly complex shape
    and the fact that it has a curvature at every point may explain why it works so
    well, especially for complex tasks: gradient descent may find it easier to fit
    complex patterns. In practice, it often outperforms every other activation function
    discussed so far. However, it is a bit more computationally intensive, and the
    performance boost it provides is not always sufficient to justify the extra cost.
    That said, it is possible to show that it is approximately equal to *z*σ(1.702
    *z*), where σ is the sigmoid function: using this approximation also works very
    well, and it has the advantage of being much faster to compute.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图11-4](#gelu_swish_mish_plot)中看到的，GELU与ReLU相似：当其输入*z*非常负时，它接近0；当*z*非常正时，它接近*z*。然而，与我们之前讨论的所有激活函数都是凸性和单调的相反，GELU激活函数既不是：从左到右，它开始是直线，然后波动下降，在约-0.17（z
    ≈ -0.75）处达到低点，最后弹起并最终直线向上。这种相当复杂的形状以及它在每个点都有曲率的事实可能解释了为什么它工作得如此之好，尤其是在复杂任务中：梯度下降可能更容易拟合复杂模式。在实践中，它通常优于之前讨论的任何其他激活函数。然而，它计算上稍微复杂一些，它提供的性能提升并不总是足以证明额外成本是合理的。尽管如此，可以证明它大约等于*z*σ(1.702*z*)，其中σ是sigmoid函数：使用这个近似也工作得很好，并且它具有计算速度更快的优势。
- en: '![Plot comparing the GELU, Swish, parametrized Swish, Mish, and ReLU² activation
    functions, illustrating their differences in behavior across input values.](assets/hmls_1104.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![比较GELU、Swish、参数化Swish、Mish和ReLU²激活函数的图表，展示了它们在输入值上的行为差异。](assets/hmls_1104.png)'
- en: Figure 11-4\. GELU, Swish, parametrized Swish, Mish, and ReLU² activation functions
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4\. GELU、Swish、参数化Swish、Mish和ReLU²激活函数
- en: 'The GELU paper also introduced the *sigmoid linear unit* (SiLU) activation
    function, which is equal to *z*σ(*z*), but it was outperformed by GELU in the
    authors’ tests. Interestingly, a [2017 paper](https://homl.info/swish) by Prajit
    Ramachandran et al.^([12](ch11.html#id2509)) rediscovered the SiLU function by
    automatically searching for good activation functions. The authors named it *Swish*,
    and the name caught on. In their paper, Swish outperformed every other function,
    including GELU. Ramachandran et al. later generalized Swish by adding an extra
    scalar hyperparameter *β* to scale the sigmoid function’s input. The generalized
    Swish function is Swish[*β*](*z*) = *z*σ(*βz*), so GELU is approximately equal
    to the generalized Swish function using *β* = 1.702\. You can tune *β* like any
    other hyperparameter. Alternatively, it’s also possible to make *β* trainable
    and let gradient descent optimize it (a bit like PReLU): there is typically a
    single trainable *β* parameter for the whole model, or just one per layer, to
    keep the model efficient and avoid overfitting.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GELU论文还介绍了*sigmoid线性单元*（SiLU）激活函数，它等于*z*σ(*z*)，但在作者们的测试中输给了GELU。有趣的是，Prajit
    Ramachandran等人于2017年发表的一篇[论文](https://homl.info/swish)通过自动搜索好的激活函数重新发现了SiLU函数。作者们将其命名为*Swish*，这个名字流行起来。在他们的论文中，Swish优于所有其他函数，包括GELU。Ramachandran等人后来通过添加一个额外的标量超参数*β*来缩放sigmoid函数的输入，推广了Swish。广义Swish函数是Swish[*β*](*z*)
    = *z*σ(*βz*)，因此GELU大约等于使用*β* = 1.702的广义Swish函数。您可以像调整任何其他超参数一样调整*β*。或者，也可以使*β*可训练，并让梯度下降优化它（有点像PReLU）：通常整个模型只有一个可训练的*β*参数，或者每层只有一个，以保持模型高效并避免过拟合。
- en: 'A popular Swish variant is [*SwiGLU*](https://homl.info/swiglu):⁠^([13](ch11.html#id2519))
    the inputs go through the Swish activation function, and in parallel through a
    linear layer, then both outputs are multiplied itemwise. That’s SwiGLU(**z**)
    = Swish[*β*](**z**) ⊗ Linear(**z**). This is often implemented by doubling the
    output dimensions of the previous linear layer, then splitting the outputs in
    two along the feature dimension to get **z**[1] and **z**[2], and finally applying:
    SwiGLU[*β*](**z**) = Swish[*β*](**z**[1]) ⊗ **z**[2]. This is a variant of the
    [*gated linear unit* (GLU)](https://homl.info/glu)⁠^([14](ch11.html#id2520)) introduced
    by Facebook researchers in 2016\. The itemwise multiplication gives the model
    more expressive power, allowing it to learn when to turn off (i.e., multiply by
    0) or amplify specific features: this is called a *gating mechanism*. SwiGLU is
    very common in modern transformers (see [Chapter 15](ch15.html#transformer_chapter)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的 Swish 变体是 [*SwiGLU*](https://homl.info/swiglu):⁠^([13](ch11.html#id2519))
    输入通过 Swish 激活函数，同时并行通过一个线性层，然后逐项相乘输出。这就是 SwiGLU(**z**) = Swish[*β*](**z**) ⊗ Linear(**z**)。这通常通过将前一个线性层的输出维度加倍，然后沿着特征维度将输出分成两部分以获得
    **z**[1] 和 **z**[2]，最后应用：SwiGLU[*β*](**z**) = Swish[*β*](**z**[1]) ⊗ **z**[2]。这是
    Facebook 研究人员在 2016 年引入的 [*门控线性单元* (GLU)](https://homl.info/glu)⁠^([14](ch11.html#id2520))
    的一个变体。逐项乘法给模型提供了更多的表达能力，允许它学习何时关闭（即乘以 0）或放大特定特征：这被称为 *门控机制*。SwiGLU 在现代变压器中非常常见（参见
    [第 15 章](ch15.html#transformer_chapter)）。
- en: Another GELU-like activation function is *Mish*, which was introduced in a [2019
    paper](https://homl.info/mish) by Diganta Misra.^([15](ch11.html#id2524)) It is
    defined as mish(*z*) = *z*tanh(softplus(*z*)), where softplus(*z*) = log(1 + exp(*z*)).
    Just like GELU and Swish, it is a smooth, nonconvex, and nonmonotonic variant
    of ReLU, and once again the author ran many experiments and found that Mish generally
    outperformed other activation functions—even Swish and GELU, by a tiny margin.
    [Figure 11-4](#gelu_swish_mish_plot) shows GELU, Swish (both with the default
    *β* = 1 and with *β* = 0.6), and lastly Mish. As you can see, Mish overlaps almost
    perfectly with Swish when *z* is negative, and almost perfectly with GELU when
    *z* is positive.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类似于 GELU 的激活函数是 *Mish*，它由 Diganta Misra 在 [2019 年的一篇论文](https://homl.info/mish)中提出.^([15](ch11.html#id2524))
    它被定义为 mish(*z*) = *z*tanh(softplus(*z*))，其中 softplus(*z*) = log(1 + exp(*z*))。就像
    GELU 和 Swish 一样，它是一个平滑、非凸、非单调的 ReLU 变体，而且作者再次进行了许多实验，发现 Mish 通常优于其他激活函数——甚至比 Swish
    和 GELU 略胜一筹。![图 11-4](#gelu_swish_mish_plot) 展示了 GELU、Swish（默认 *β* = 1 和 *β* =
    0.6）以及最后的 Mish。如图所示，当 *z* 为负值时，Mish 几乎完美地与 Swish 重叠，而当 *z* 为正值时，几乎完美地与 GELU 重叠。
- en: 'Lastly, in 2021, Google researchers ran an automated architecture search to
    improve large transformers, and the search found a very simple yet effective activation
    function: [ReLU²](https://homl.info/relu2).⁠^([16](ch11.html#id2532)) As its name
    suggests, it’s simply ReLU squared: ReLU²(*z*) = (max(0, *z*))^2\. It has all
    the qualities of ReLU (simplicity, computational efficiency, sparse output, no
    saturation on the positive side) but it also has smooth gradients at *z* = 0,
    and it often outperforms other activation functions, especially for sparse models.
    However, training can be less stable, in part because of its increased sensitivity
    to outliers and dying ReLUs.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 2021 年，Google 研究人员运行了一个自动化的架构搜索来改进大型变压器，搜索发现了一个非常简单但有效的激活函数：[ReLU²](https://homl.info/relu2)。⁠^([16](ch11.html#id2532))
    如其名所示，它只是 ReLU 的平方：ReLU²(*z*) = (max(0, *z*))^2。它具有 ReLU 的所有特性（简单性、计算效率、稀疏输出、正侧无饱和）但它也有在
    *z* = 0 处的平滑梯度，并且通常优于其他激活函数，特别是对于稀疏模型。然而，训练可能不太稳定，部分原因是因为它对异常值和死亡 ReLUs 的敏感性增加。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'So, which activation function should you use for the hidden layers of your
    deep neural networks? ReLU remains a good default for most tasks: it’s often just
    as good as the more sophisticated activation functions, plus it’s very fast to
    compute, and many libraries and hardware accelerators provide ReLU-specific optimizations.
    However, Swish is probably a better default for complex tasks, and you can even
    try parametrized Swish with a learnable *β* parameter for the most complex tasks.
    Mish and SwiGLU may give you slightly better results, but they require a bit more
    compute. If you care a lot about runtime latency, then you may prefer leaky ReLU,
    or parametrized leaky ReLU for complex tasks, or even ReLU², especially for sparse
    models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该为你的深度神经网络隐藏层使用哪种激活函数呢？ReLU对于大多数任务来说仍然是一个好的默认选择：它通常和更复杂的激活函数一样好，而且计算速度非常快，许多库和硬件加速器都提供了ReLU特定的优化。然而，对于复杂任务，Swish可能是一个更好的默认选择，你甚至可以尝试带有可学习*β*参数的参数化Swish，用于最复杂的任务。Mish和SwiGLU可能会给你带来略微更好的结果，但它们需要更多的计算。如果你非常关心运行时延迟，那么你可能更喜欢漏斗ReLU，或者对于复杂任务，选择参数化漏斗ReLU，甚至ReLU²，特别是对于稀疏模型。
- en: PyTorch supports GELU, Mish, and Swish out of the box (using `nn.GELU`, `nn.Mish`,
    and `nn.SiLU`, respectively). To implement SwiGLU, double the previous linear
    layer’s output dimension, then use `z1, z2 = z.chunk(2, dim=-1)` to split its
    output in two, and compute `F.silu(beta * z1) * z2` (where `F` is `torch.nn.functional`).
    For ReLU², simply compute `F.relu(z).square()`. PyTorch also includes simplified
    and approximated versions of several activation functions, which are much faster
    to compute and often more stable during training. These simplified versions have
    names starting with “Hard”, such as `nn.Hardsigmoid`, `nn.Hardtanh`, and `nn.Hardswish`,
    and they are often used on mobile devices.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch原生支持GELU、Mish和Swish（分别使用`nn.GELU`、`nn.Mish`和`nn.SiLU`）。要实现SwiGLU，将前一个线性层的输出维度加倍，然后使用`z1,
    z2 = z.chunk(2, dim=-1)`将其输出分成两部分，并计算`F.silu(beta * z1) * z2`（其中`F`是`torch.nn.functional`）。对于ReLU²，只需计算`F.relu(z).square()`。PyTorch还包括几个激活函数的简化和近似版本，这些版本计算速度更快，在训练过程中通常更稳定。这些简化版本的名字以“Hard”开头，例如`nn.Hardsigmoid`、`nn.Hardtanh`和`nn.Hardswish`，它们通常用于移动设备。
- en: 'That’s all for activation functions! Now, let’s look at a completely different
    way to solve the unstable gradients problem: batch normalization.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关于激活函数就到这里！现在，让我们看看解决梯度不稳定问题的另一种完全不同的方法：批标准化。
- en: Batch Normalization
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批标准化
- en: Although using Kaiming initialization along with ReLU (or any of its variants)
    can significantly reduce the danger of the vanishing/exploding gradients problems
    at the beginning of training, it doesn’t guarantee that they won’t come back during
    training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用Kaiming初始化与ReLU（或其任何变体）结合可以在训练初期显著减少梯度消失/爆炸问题的风险，但这并不能保证它们不会在训练过程中再次出现。
- en: 'In a [2015 paper](https://homl.info/51),⁠^([17](ch11.html#id2548)) Sergey Ioffe
    and Christian Szegedy proposed a technique called *batch normalization* (BN) that
    addresses these problems. The technique consists of adding an operation in the
    model just before or after the activation function of each hidden layer. This
    operation simply zero-centers and normalizes each input, then scales and shifts
    the result using two new parameter vectors per layer: one for scaling, the other
    for shifting. In other words, the operation lets the model learn the optimal scale
    and mean of each of the layer’s inputs. In many cases, if you add a BN layer as
    the very first layer of your neural network, you do not need to standardize your
    training set (no need for `StandardScaler`); the BN layer will do it for you (well,
    approximately, since it only looks at one batch at a time, and it can also rescale
    and shift each input feature).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年的一篇论文[2015 paper](https://homl.info/51)，⁠^([17](ch11.html#id2548))中，Sergey
    Ioffe和Christian Szegedy提出了一种称为*批标准化*（BN）的技术，用于解决这些问题。这项技术包括在每个隐藏层的激活函数之前或之后添加一个操作。这个操作简单地将每个输入归零并标准化，然后使用每个层的两个新参数向量来缩放和移动结果：一个用于缩放，另一个用于移动。换句话说，这个操作让模型学习每个层输入的最佳缩放和均值。在许多情况下，如果你将BN层作为你的神经网络的第一层，你就不需要标准化你的训练集（不需要`StandardScaler`）；BN层会为你完成这个任务（好吧，大约是这样，因为它一次只查看一个批次，它还可以重新缩放和移动每个输入特征）。
- en: In order to zero-center and normalize the inputs, the algorithm needs to estimate
    each input’s mean and standard deviation. It does so by evaluating the mean and
    standard deviation of the input over the current mini-batch (hence the name “batch
    normalization”). The whole operation is summarized step by step in [Equation 11-4](#batch_normalization_algorithm).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将输入归零并标准化，算法需要估计每个输入的均值和标准差。它是通过评估当前小批次的输入均值和标准差来做到这一点的（因此得名“批标准化”）。整个操作步骤在[公式11-4](#batch_normalization_algorithm)中逐步总结。
- en: Equation 11-4\. Batch normalization algorithm
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式11-4\. 批标准化算法
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi
    mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo
    lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><msub><mi
    mathvariant="bold">μ</mi> <mi>B</mi></msub> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi></msub></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi>
    <mi>B</mi></msub></munderover> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mover accent="true"><mi mathvariant="bold">x</mi>
    <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msub><mi mathvariant="bold">μ</mi>
    <mi>B</mi></msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi>
    <mi>B</mi></msub></mrow> <mn>2</mn></msup> <mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>4</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <mi mathvariant="bold">γ</mi> <mo>⊗</mo> <msup><mover accent="true"><mi
    mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>+</mo> <mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable>
- en: 'In this algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此算法中：
- en: '**μ**[*B*] is the vector of input means, evaluated over the whole mini-batch
    *B* (it contains one mean per input).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**μ**[*B*]是整个小批次*B*中输入均值的向量（它包含每个输入的一个均值）。'
- en: '*m*[*B*] is the number of instances in the mini-batch.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*[*B*]是小批次中实例的数量。'
- en: '**x**^((*i*)) is the input vector of the batch-norm layer for instance *i*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**^((*i*)) 是批归一化层实例*i*的输入向量。'
- en: '**σ**[*B*] is the vector of input standard deviations, also evaluated over
    the whole mini-batch (it contains one standard deviation per input).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**σ**[*B*]是输入标准差的向量，在整个小批量上评估（它包含每个输入的一个标准差）。'
- en: <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> ^((*i*))
    is the vector of zero-centered and normalized inputs for instance *i*.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> ^((*i*))
    是实例*i*的零均值和归一化输入向量。
- en: '*ε* is a tiny number that avoids division by zero and ensures the gradients
    don’t grow too large (typically 10^(–5)). This is called a *smoothing term*.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ε*是一个很小的数，它避免了除以零并确保梯度不会变得太大（通常是10^(–5)）。这被称为*平滑项*。'
- en: '**γ** is the output scale parameter vector for the layer (it contains one scale
    parameter per input).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**γ**是层的输出缩放参数向量（它包含每个输入的一个缩放参数）。'
- en: ⊗ represents element-wise multiplication (each input is multiplied by its corresponding
    output scale parameter).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⊗ 表示逐元素乘法（每个输入乘以其相应的输出缩放参数）。
- en: '**β** is the output shift (offset) parameter vector for the layer (it contains
    one shift parameter per input). Each input is offset by its corresponding shift
    parameter.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**β**是层的输出偏移（偏移）参数向量（它包含每个输入的一个偏移参数）。每个输入都通过其相应的偏移参数进行偏移。'
- en: '**z**^((*i*)) is the output of the BN operation. It is a rescaled and shifted
    version of the inputs.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z**^((*i*)) 是BN操作的输出。它是输入的缩放和偏移版本。'
- en: 'So during training, BN standardizes its inputs, then rescales and offsets them.
    Good! What about at test time? Well, it’s not that simple. Indeed, we may need
    to make predictions for individual instances rather than for batches of instances:
    in this case, we will have no way to compute each input’s standard deviation.
    Moreover, even if we do have a batch of instances, it may be too small, or the
    instances may not be independent and identically distributed, so computing statistics
    over the batch instances would be unreliable. One solution is to wait until the
    end of training, then run the whole training set through the neural network and
    compute the mean and standard deviation of each input of the BN layer. These “final”
    input means and standard deviations can then be used instead of the batch input
    means and standard deviations when making predictions.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练期间，BN标准化其输入，然后缩放和偏移它们。很好！测试时间呢？嗯，并不简单。实际上，我们可能需要为单个实例而不是实例批次进行预测：在这种情况下，我们将无法计算每个输入的标准差。此外，即使我们有实例批次，它可能太小，或者实例可能不是独立同分布的，因此计算批次实例的统计量将不可靠。一种解决方案是在训练结束时等待，然后将整个训练集通过神经网络运行，并计算BN层每个输入的均值和标准差。然后可以使用这些“最终”输入均值和标准差来代替预测时使用的批输入均值和标准差。
- en: 'However, most implementations of batch norm estimate these final statistics
    during training by using a moving average of the layer’s batch input means and
    variances. This is what PyTorch does automatically when you use its batch-norm
    layers, such as `nn.BatchNorm1d` (which we will discuss in the next section).
    To sum up, four parameter vectors are learned in each batch-norm layer: **γ**
    (the output scale vector) and **β** (the output offset vector) are learned through
    regular backpropagation, and **μ** (the final input mean vector) and **σ**² (the
    final input variance vector) are estimated using an exponential moving average.
    Note that **μ** and **σ**² are estimated during training, but they are used only
    after training, once you switch the model to evaluation mode using `model.eval()`:
    **μ** and **σ**² then replace **μ**[*B*] and **σ**[*B*]² in [Equation 11-4](#batch_normalization_algorithm).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数批归一化的实现都是通过使用层的批输入均值和方差的移动平均来估计这些最终统计量的。这就是当你使用PyTorch的批归一化层，例如`nn.BatchNorm1d`（我们将在下一节中讨论）时，PyTorch自动执行的操作。总的来说，每个批归一化层中学习到四个参数向量：**γ**（输出缩放向量）和**β**（输出偏移向量）通过常规的反向传播学习，而**μ**（最终输入均值向量）和**σ**²（最终输入方差向量）使用指数移动平均来估计。请注意，**μ**和**σ**²是在训练期间估计的，但它们仅在训练完成后使用，一旦你使用`model.eval()`将模型切换到评估模式：**μ**和**σ**²将替换[方程11-4](#batch_normalization_algorithm)中的**μ**[*B*]和**σ**[*B*]²。
- en: 'Ioffe and Szegedy demonstrated that batch norm considerably improved all the
    deep neural networks they experimented with, leading to a huge improvement in
    the ImageNet classification task (ImageNet is a large database of images classified
    into many classes, commonly used to evaluate computer vision systems). The vanishing
    gradients problem was strongly reduced, to the point that they could use saturating
    activation functions such as tanh and even sigmoid. The networks were also much
    less sensitive to the weight initialization. The authors were able to use much
    larger learning rates, significantly speeding up the learning process. Specifically,
    they note that:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Ioffe和Szegedy证明了批量归一化显著提高了他们实验中所有深度神经网络的表现，这在ImageNet分类任务上带来了巨大的改进（ImageNet是一个包含许多类别的图像的大型数据库，常用于评估计算机视觉系统）。梯度消失问题得到了显著减少，以至于他们可以使用饱和激活函数如tanh和sigmoid。网络对权重初始化的敏感性也大大降低。作者能够使用更大的学习率，显著加快学习过程。具体来说，他们指出：
- en: 'Applied to a state-of-the-art image classification model, batch norm achieves
    the same accuracy with 14 times fewer training steps, and beats the original model
    by a significant margin. […​] Using an ensemble of batch-normalized networks,
    we improve upon the best published result on ImageNet classification: reaching
    4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human
    raters.'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将批量归一化应用于最先进的图像分类模型，批量归一化在14倍更少的训练步骤下达到了相同的准确率，并且显著优于原始模型。[……] 使用批量归一化的网络集成，我们在ImageNet分类任务上超越了已发表的最好结果：达到4.9%的top-5验证错误率（和4.8%的测试错误率），超过了人类评分员的准确率。
- en: Finally, like a gift that keeps on giving, batch norm acts like a regularizer,
    reducing the need for other regularization techniques (such as dropout, described
    later in this chapter).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像源源不断的礼物一样，批量归一化（batch norm）充当正则化器的作用，减少了其他正则化技术（如后续章节中描述的dropout）的需求。
- en: 'Batch normalization does, however, add some complexity to the model (although
    it can remove the need for normalizing the input data, as discussed earlier).
    Moreover, there is a runtime penalty: the neural network makes slower predictions
    due to the extra computations required at each layer. Fortunately, it’s often
    possible to fuse the BN layer with the previous layer after training, thereby
    avoiding the runtime penalty. This is done by updating the previous layer’s weights
    and biases so that it directly produces outputs of the appropriate scale and offset.
    For example, if the previous layer computes **XW** + **b**, then the BN layer
    will compute **γ** ⊗ (**XW** + **b** – **μ**) / **σ** + **β** (ignoring the smoothing
    term *ε* in the denominator). If we define **W**′ = **γ**⊗**W** / **σ** and **b**′
    = **γ** ⊗ (**b** – **μ**) / **σ** + **β**, the equation simplifies to **XW**′
    + **b**′. So, if we replace the previous layer’s weights and biases (**W** and
    **b**) with the updated weights and biases (**W**′ and **b**′), we can get rid
    of the BN layer. This is one of the optimizations performed by `optimize_for_inference()`
    (see [Chapter 10](ch10.html#pytorch_chapter)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，批量归一化确实给模型增加了一些复杂性（尽管它可以消除前面讨论中提到的对输入数据进行归一化的需求）。此外，还存在运行时惩罚：由于在每个层中需要额外的计算，神经网络预测速度变慢。幸运的是，通常在训练后可以将BN层与前一层的权重和偏差融合，从而避免运行时惩罚。这是通过更新前一层的权重和偏差，使其直接产生适当规模和偏移的输出来实现的。例如，如果前一层的计算是**XW**
    + **b**，那么BN层将计算**γ** ⊗ (**XW** + **b** – **μ**) / **σ** + **β**（忽略分母中的平滑项*ε*）。如果我们定义**W**′
    = **γ**⊗**W** / **σ**和**b**′ = **γ** ⊗ (**b** – **μ**) / **σ** + **β**，则方程简化为**XW**′
    + **b**′。因此，如果我们用更新的权重和偏差（**W**′和**b**′）替换前一层的权重和偏差（**W**和**b**），我们就可以去掉BN层。这是`optimize_for_inference()`（见第10章）执行的一种优化。 '
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may find that training is rather slow, because each epoch takes much more
    time when you use batch norm. This is usually counterbalanced by the fact that
    convergence is much faster with BN, so it will take fewer epochs to reach the
    same performance. All in all, *wall time* will usually be shorter (this is the
    time measured by the clock on your wall).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现训练相当慢，因为使用批量归一化时，每个epoch所需的时间要多得多。然而，这通常会被BN带来的更快收敛所抵消，因此达到相同性能所需的epoch会更少。总的来说，*wall
    time*（墙上的时钟测量时间）通常会更短。
- en: Implementing batch norm with PyTorch
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch实现批量归一化
- en: 'As with most things with PyTorch, implementing batch norm is straightforward
    and intuitive. Just add an `nn.BatchNorm1d` layer before or after each hidden
    layer’s activation function, and specify the number of inputs of each BN layer.
    You may also add a BN layer as the first layer in your model, which removes the
    need to standardize the inputs manually. For example, let’s create a Fashion MNIST
    image classifier (similar to the one we built in [Chapter 10](ch10.html#pytorch_chapter))
    using BN as the first layer in the model (after flattening the input images),
    then again after each hidden layer:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与PyTorch中的大多数事情一样，实现批量归一化（batch norm）既简单又直观。只需在每个隐藏层的激活函数之前或之后添加一个`nn.BatchNorm1d`层，并指定每个BN层的输入数量。你还可以将BN层作为模型的第一层，这样可以省去手动标准化输入的需求。例如，让我们创建一个Fashion
    MNIST图像分类器（类似于我们在[第10章](ch10.html#pytorch_chapter)中构建的），将BN作为模型的第一层（在展平输入图像之后），然后在每个隐藏层之后再次使用：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can now train the model normally (as you learned in [Chapter 10](ch10.html#pytorch_chapter)),
    and that’s it! In this tiny example with just two hidden layers, batch norm is
    unlikely to have a large impact, but for deeper networks it can make a tremendous
    difference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以像在[第10章](ch10.html#pytorch_chapter)中学到的那样正常训练模型，这就完成了！在这个只有两个隐藏层的微小示例中，批量归一化可能不会产生很大的影响，但对于更深层的网络，它可能带来巨大的差异。
- en: Warning
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Since batch norm behaves differently during training and during evaluation,
    it’s critical to switch to training mode during training (using `model.train()`),
    and switch to evaluation mode during evaluation (using `model.eval()`). Forgetting
    to do so is one of the most common mistakes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批量归一化在训练和评估期间表现不同，因此在训练期间切换到训练模式（使用`model.train()`）以及在评估期间切换到评估模式（使用`model.eval()》）至关重要。忘记这样做是犯得最常见错误之一。
- en: 'If you look at the parameters of the first BN layer, you will find two: `weight`
    and `bias`, which correspond to **γ** and **β** in [Equation 11-4](#batch_normalization_algorithm):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看第一个BN层的参数，你会发现两个：`weight`和`bias`，它们对应于[方程11-4](#batch_normalization_algorithm)中的**γ**和**β**：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6] >>> dict(model[1].named_buffers()).keys() `dict_keys([''running_mean'',
    ''running_var'', ''num_batches_tracked''])` [PRE7]`The authors of the BN paper
    argued in favor of adding the BN layers before the activation functions, rather
    than after (as we just did). There is some debate about this, and it seems to
    depend on the task, so you can experiment with this to see which option works
    best on your dataset. If you move the BN layers before the activation functions,
    you can also remove the bias term from the previous `nn.Linear` layers by setting
    their `bias` hyperparameter to `False`. Indeed, a batch-norm layer already includes
    one bias term per input. You can also drop the first BN layer to avoid sandwiching
    the first hidden layer between two BN layers, but this means you should normalize
    the training set before training. The updated code looks like this:    [PRE8]    The
    `nn.BatchNorm1d` class has a few hyperparameters you can tweak. The defaults will
    usually be fine, but you may occasionally need to tweak the `momentum`. This hyperparameter
    is used by the `BatchNorm1d` layer when it updates the exponential moving averages;
    given a new value **v** (i.e., a new vector of input means or variances computed
    over the current batch), the layer updates the running average <mover><mi mathvariant="bold">v</mi><mo>^</mo></mover>
    using the following equation:  <mrow><mover accent="true"><mi mathvariant="bold">v</mi>
    <mo>^</mo></mover> <mo>←</mo> <mi mathvariant="bold">v</mi> <mo>×</mo> <mtext>momentum</mtext>
    <mo>+</mo> <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow>  A
    good momentum value is typically close to 0; for example, 0.01 or 0.001\. You
    want more 0s for smaller mini-batches, and fewer for larger mini-batches. The
    default is 0.1, which is good for large batch sizes, but not great for small batch
    sizes such as 32 or 64.    ###### Warning    When people talk about “momentum”
    in the context of a running mean, they usually refer to the weight of the current
    running mean in the update equation. Sadly, for historical reasons, PyTorch uses
    the opposite meaning in the BN layers. However, other parts of PyTorch use the
    conventional meaning (e.g., in optimizers), so don’t get confused.[PRE9]``  [PRE10]  [PRE11]`##
    Layer Normalization    Layer normalization (LN) is very similar to batch norm,
    but instead of normalizing across the batch dimension, LN normalizes across the
    feature dimensions. This simple idea was introduced by Jimmy Lei Ba et al. in
    a [2016 paper](https://homl.info/layernorm),⁠^([18](ch11.html#id2562)) and initially
    applied mostly to recurrent nets. However, in recent years it has been successfully
    applied to many other architectures, such as convolutional nets, transformers,
    diffusion nets, and more.    One advantage is that LN can compute the required
    statistics on the fly, at each time step, independently for each instance. This
    also means that it behaves the same way during training and testing (as opposed
    to BN), and it does not need to use exponential moving averages to estimate the
    feature statistics across all instances in the training set, like BN does. Lastly,
    LN learns a scale and an offset parameter for each input feature, just like BN
    does.    PyTorch includes an `nn.LayerNorm` module. To create an instance, you
    must simply indicate the size of the dimensions that you want to normalize over.
    These must be the last dimension(s) of the inputs. For example, if the inputs
    are batches of 100 × 200 RGB images of shape `[3, 100, 200]`, and you want to
    normalize each image over each of the three color channels separately, you would
    use the following `nn.LayerNorm` module:    [PRE12]    The following code produces
    the same result:    [PRE13]    However, most computer vision architectures that
    use LN normalize over all channels at once. For this, you must include the size
    of the channel dimension when creating the `nn.LayerNorm` module:    [PRE14]    And
    that’s all there is to it! Now let’s look at one last technique to stabilize gradients
    during training: gradient clipping.    ## Gradient Clipping    Another technique
    to mitigate the exploding gradients problem is to clip the gradients during backpropagation
    so that they never exceed some threshold. This is called [*gradient clipping*](https://homl.info/52).⁠^([19](ch11.html#id2572))
    This technique is generally used in recurrent neural networks, where using batch
    norm is tricky (as you will see in [Chapter 13](ch13.html#rnn_chapter)).    In
    PyTorch, gradient clipping is generally implemented by calling either `torch.​nn.utils.clip_grad_norm_()`
    or `torch.nn.utils.clip_grad_value_()` at each iteration during training, right
    after the gradients are computed (i.e., after `loss.backward()`). Both functions
    take as a first argument the list of model parameters whose gradients must be
    clipped—typically all of them (`model.parameters()`). The `clip_grad_norm_()`
    function clips each gradient vector’s norm if it exceeds the given `max_norm`
    argument. This is a hyperparameter you can tune (a typical default value is 1.0).
    The `clip_grad_value_()` function independently clips the individual components
    of the gradient vector between `-clip_value` and `+clip_value`, where `clip_value`
    is a hyperparameter you can tune. For example, this training loop clips the norm
    of each gradient vector to 1.0:    [PRE15]    Note that `clip_grad_value_()` will
    change the orientation of the gradient vector when its components are clipped.
    For instance, if the original gradient vector is `[0.9, 100.0]`, it points mostly
    in the direction of the second dimension; but once you clip it by value, you get
    `[0.9, 1.0]`, which points roughly at the diagonal between the two axes. Despite
    this reorientation, this approach actually works quite well in practice. If you
    clipped the same vector by norm, the result would be `[0.00899964, 0.9999595]`:
    this would preserve the vector’s orientation, but almost eliminate the first component.
    The best clipping function to use depends on the dataset.[PRE16]``  [PRE17]``
    [PRE18]` # Reusing Pretrained Layers    It is generally not a good idea to train
    a very large DNN from scratch without first trying to find an existing neural
    network that accomplishes a similar task to the one you are trying to tackle (I
    will discuss how to find them in [Chapter 12](ch12.html#cnn_chapter)). If you
    find such a neural network, then you can generally reuse most of its layers, except
    for the top ones. This technique is called *transfer learning*. It will not only
    speed up training considerably, but also require significantly less training data.    Suppose
    you have access to a DNN that was trained to classify pictures into one hundred
    different categories, including animals, plants, vehicles, and everyday objects,
    and you now want to train a DNN to classify specific types of vehicles. These
    tasks are very similar, even partly overlapping, so you should try to reuse parts
    of the first network (see [Figure 11-5](#reuse_pretrained_diagram)).  ![Diagram
    illustrating the reuse of pretrained layers from an existing deep neural network
    to train a new model for a similar task, with some weights fixed and others trainable.](assets/hmls_1105.png)  ######
    Figure 11-5\. Reusing pretrained layers    ###### Note    If the input pictures
    for your new task don’t have the same size as the ones used in the original task,
    you will usually have to add a preprocessing step to resize them to the size expected
    by the original model. More generally, transfer learning will work best when the
    inputs have similar low-level features. For example, a neural net trained on regular
    pictures taken from mobile phones will help with many other tasks on mobile phone
    pictures, but it will likely not help at all on satellite images or medical images.    The
    output layer of the original model should usually be replaced because it is most
    likely not useful at all for the new task, and it may not even have the right
    number of outputs.    Similarly, the upper hidden layers of the original model
    are less likely to be as useful as the lower layers, since the high-level features
    that are most useful for the new task may differ significantly from the ones that
    were most useful for the original task. You want to find the right number of layers
    to reuse.    ###### Tip    The more similar the tasks are, the more layers you
    will want to reuse (starting with the lower layers). For very similar tasks, try
    to keep all the hidden layers and just replace the output layer.    Try freezing
    all the reused layers first (i.e., make their parameters nontrainable by setting
    `requires_grad` to `False` so that gradient descent won’t modify them and they
    will remain fixed), then train your model and see how it performs. Then try unfreezing
    one or two of the top hidden layers to let backpropagation tweak them and see
    if performance improves. The more training data you have, the more layers you
    can unfreeze. It is also useful to reduce the learning rate when you unfreeze
    reused layers: this will avoid wrecking their fine-tuned weights.    If you still
    cannot get good performance, and you have little training data, try dropping the
    top hidden layer(s) and freezing all the remaining hidden layers again. You can
    iterate until you find the right number of layers to reuse. If you have plenty
    of training data, you may try replacing the top hidden layers instead of dropping
    them, and even adding more hidden layers.    ## Transfer Learning with PyTorch    Let’s
    look at an example. Suppose the Fashion MNIST dataset only contained eight classes—for
    example, all the classes except for Pullover and T-shirt/top. Someone built and
    trained a PyTorch model on that set and got reasonably good performance (~92%
    accuracy). Let’s call this model A. You now want to tackle a different task: you
    have images of T-shirts and pullovers, and you want to train a binary classifier:
    positive for T-shirt/top, negative for Pullover. Your dataset is tiny; you only
    have 20 labeled images! When you train a new model for this task (let’s call it
    model B) with the same architecture as model A, you get 71.6% test accuracy. While
    drinking your morning coffee, you realize that your task is quite similar to task
    A, so perhaps transfer learning can help? Let’s find out!    First, let’s look
    at model A:    [PRE19]    We can now reuse the layers we want, for example, all
    layers except for the output layer:    [PRE20]    In this code, we use Python’s
    `copy.deepcopy()` function to copy all the modules in the `nn.Sequential` module
    (along with all their data and submodules), except for the last layer. Since we’re
    making a deep copy, all the submodules are copied as well. Then we create `model_B_on_A`,
    which is an `nn.Sequential` model based on the reused layers of model A, plus
    a new output layer for task B: it has a single output since task B is binary classification.    You
    could start training `model_B_on_A` for task B now, but since the new output layer
    was initialized randomly, it will make large errors (at least during the first
    few epochs), so there will be large error gradients that may wreck the reused
    weights. To avoid this, one approach is to freeze the reused layers during the
    first few epochs, giving the new layer some time to learn reasonable weights:    [PRE21]    Now
    you can train `model_B_on_A`. But don’t forget that task B is binary classification,
    so you must switch the loss to `nn.BCEWithLogitsLoss` (or to `nn.BCELoss` if you
    prefer to add an `nn.Sigmoid` activation function on the output layer), as we
    discussed in [Chapter 10](ch10.html#pytorch_chapter). Also, if you are using `torchmetrics`,
    make sure to set `task="binary"` when creating the `Accuracy` metric:    [PRE22]    After
    you have trained the model for a few epochs, you can unfreeze the reused layers
    (setting `param.requires_grad = True` for all parameters), reduce the learning
    rate, and continue training to fine-tune the reused layers for task B.    So,
    what’s the final verdict? Well, this model’s test accuracy is 92.5%, which is
    much better than the 71.6% accuracy we reached without pretraining!    Are you
    convinced? Well, you shouldn’t be; I cheated! I tried many configurations until
    I found one that demonstrated a strong improvement. If you try to change the classes
    or the random seed, you will see that the improvement generally drops, or even
    vanishes or reverses. What I did is called “torturing the data until it confesses”.
    When a paper looks too positive, you should be suspicious. Perhaps the flashy
    new technique does not actually help much (in fact, it may even degrade performance),
    but the authors tried many variants and reported only the best results—which may
    be due to sheer luck—without mentioning how many failures they encountered along
    the way. That’s called *p-hacking*. Most of the time, this is not malicious, but
    it is part of the reason why so many results in science can never be reproduced.    But
    why did I cheat? It turns out that transfer learning does not work very well with
    small dense networks, presumably because small networks learn few patterns, and
    dense networks learn very specific patterns, which are unlikely to be useful for
    other tasks. Transfer learning works best with deep convolutional neural networks
    and with Transformer architectures. We will revisit transfer learning in Chapters
    [12](ch12.html#cnn_chapter) and [15](ch15.html#transformer_chapter), using the
    techniques we just discussed (and this time it will work fine without cheating,
    I promise!).    ## Unsupervised Pretraining    Suppose you want to tackle a complex
    task for which you don’t have much labeled training data, but unfortunately you
    cannot find a model trained on a similar task. Don’t lose hope! First, you should
    try to gather more labeled training data, but if you can’t, you may still be able
    to perform *unsupervised pretraining* (see [Figure 11-6](#unsupervised_pretraining_diagram)).
    Indeed, it is often cheap to gather unlabeled training examples, but expensive
    to label them. If you can gather plenty of unlabeled training data, you can try
    to use it to train an unsupervised model, such as an autoencoder (see [Chapter 18](ch18.html#autoencoders_chapter)).
    Then you can reuse the lower layers of the autoencoder, add the output layer for
    your task on top, and fine-tune the final network using supervised learning (i.e.,
    with the labeled training examples).  ![Diagram illustrating greedy layer-wise
    pretraining in early deep learning, showing the sequential training of layers
    with unlabeled and labeled data.](assets/hmls_1106.png)  ###### Figure 11-6\.
    Greedy layer-wise pretraining used in the early days of deep learning; nowadays
    the unsupervised part is typically done in one shot on all the data rather than
    one layer at a time    It is this technique that Geoffrey Hinton and his team
    used in 2006, and which led to the revival of neural networks and the success
    of deep learning. Until 2010, unsupervised pretraining—typically with restricted
    Boltzmann machines (RBMs; see the notebook at [*https://homl.info/extra-anns*](https://homl.info/extra-anns))—was
    the norm for deep nets, and only after the vanishing gradients problem was alleviated
    did it become much more common to train DNNs purely using supervised learning.
    Unsupervised pretraining (today typically using autoencoders or diffusion models
    rather than RBMs) is still a good option when you have a complex task to solve,
    no similar model you can reuse, and little labeled training data, but plenty of
    unlabeled training data.    Note that in the early days of deep learning it was
    difficult to train deep models, so people would use a technique called *greedy
    layer-wise pretraining* (depicted in [Figure 11-6](#unsupervised_pretraining_diagram)).
    They would first train an unsupervised model with a single layer, typically an
    RBM, then they would freeze that layer and add another one on top of it, then
    train the model again (effectively just training the new layer), then freeze the
    new layer and add another layer on top of it, train the model again, and so on.
    Nowadays, things are much simpler: people generally train the full unsupervised
    model in one shot and use models such as autoencoders or diffusion models rather
    than RBMs.    ## Pretraining on an Auxiliary Task    If you do not have much labeled
    training data, one last option is to train a first neural network on an auxiliary
    task for which you can easily obtain or generate labeled training data, then reuse
    the lower layers of that network for your actual task. The first neural network’s
    lower layers will learn feature detectors that will likely be reusable by the
    second neural network.    For example, if you want to build a system to recognize
    faces, you may only have a few pictures of each individual—clearly not enough
    to train a good classifier. Gathering hundreds of pictures of each person would
    not be practical. You could, however, use a public dataset containing millions
    of pictures of people (such as VGGFace2) and train a first neural network to detect
    whether two different pictures feature the same person. Such a network would learn
    good feature detectors for faces, so reusing its lower layers would allow you
    to train a good face classifier that uses little training data.    ###### Warning    You
    could also just scrape pictures of random people from the web, but this would
    probably be illegal. Firstly, photos are usually copyrighted by their creators,
    and websites like Instagram or Facebook enforce these copyright protections through
    their terms of service, which prohibit scraping and unauthorized use. Secondly,
    over 40 countries require explicit consent for collecting and processing personal
    data, including facial images.    For natural language processing (NLP) applications,
    you can download a corpus of millions of text documents and automatically generate
    labeled data from it. For example, you could randomly mask out some words and
    train a model to predict what the missing words are (e.g., it should predict that
    the missing word in the sentence “What ___ you saying?” is probably “are” or “were”).
    If you can train a model to reach good performance on this task, then it will
    already know quite a lot about language, and you can certainly reuse it for your
    actual task and fine-tune it on your labeled data (this is basically how large
    language models are trained and fine-tuned, as we will see in [Chapter 15](ch15.html#transformer_chapter)).    ######
    Note    *Self-supervised learning* is when you automatically generate the labels
    from the data itself, as in the text-masking example, then you train a model on
    the resulting “labeled” dataset using supervised learning techniques.    # Faster
    Optimizers    Training a very large deep neural network can be painfully slow.
    So far we have seen four ways to speed up training (and reach a better solution):
    applying a good initialization strategy for the connection weights, using a good
    activation function, using batch-norm or layer-norm, and reusing parts of a pretrained
    network (possibly built for an auxiliary task or using unsupervised learning).
    Another huge speed boost comes from using a faster optimizer than the regular
    gradient descent optimizer. In this section we will present the most popular optimization
    algorithms: momentum, Nesterov accelerated gradient, AdaGrad, RMSProp, and finally,
    Adam and its variants.    ## Momentum    Imagine a bowling ball rolling down a
    gentle slope on a smooth surface: it will start out slowly, but it will quickly
    pick up momentum until it eventually reaches terminal velocity (if there is some
    friction or air resistance). This is the core idea behind *momentum optimization*,
    [proposed by Boris Polyak in 1964](https://homl.info/54).⁠^([20](ch11.html#id2599))
    In contrast, regular gradient descent will take small steps when the slope is
    gentle and big steps when the slope is steep, but it will never pick up speed.
    As a result, regular gradient descent is generally much slower to reach the minimum
    than momentum optimization.    As we saw in [Chapter 4](ch04.html#linear_models_chapter),
    gradient descent updates the weights **θ** by directly subtracting the gradient
    of the cost function *J*(**θ**) with regard to the weights (∇[**θ**]*J*(**θ**))
    multiplied by the learning rate *η*. The equation is **θ** ← **θ** – *η*∇[**θ**]*J*(**θ**).
    It does not care about what the earlier gradients were. If the local gradient
    is tiny, it goes very slowly.    Momentum optimization cares a great deal about
    what previous gradients were: at each iteration, it subtracts the local gradient
    from the *momentum vector* **m** (multiplied by the learning rate *η*), and it
    updates the weights by adding this momentum vector (see [Equation 11-5](#momentum_equation)).
    In other words, the gradient is used as a force learning to an acceleration, not
    as a speed. To simulate some sort of friction mechanism and prevent the momentum
    from growing too large, the algorithm introduces a new hyperparameter *β*, called
    the *momentum coefficient*, which must be set between 0 (high friction) and 1
    (no friction). A typical momentum value is 0.9.    ##### Equation 11-5\. Momentum
    algorithm  <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>  You can
    verify that if the gradient remains constant, the terminal velocity (i.e., the
    maximum size of the weight updates) is equal to that gradient multiplied by the
    learning rate *η* multiplied by 1 / (1 – *β*) (ignoring the sign). For example,
    if *β* = 0.9, then the terminal velocity is equal to 10 times the gradient times
    the learning rate, so momentum optimization ends up going 10 times faster than
    gradient descent! In practice, the gradients are not constant, so the speedup
    is not always as dramatic, but momentum optimization can escape from plateaus
    much faster than regular gradient descent. We saw in [Chapter 4](ch04.html#linear_models_chapter)
    that when the inputs have very different scales, the cost function will look like
    an elongated bowl (see [Figure 4-7](ch04.html#elongated_bowl_diagram)). Gradient
    descent goes down the steep slope quite fast, but then it takes a very long time
    to go down the valley. In contrast, momentum optimization will roll down the valley
    faster and faster until it reaches the bottom (the optimum). In deep neural networks
    that don’t use batch-norm or layer-norm, the upper layers will often end up having
    inputs with very different scales, so using momentum optimization helps a lot.
    It can also help roll past local optima.    ###### Note    Due to the momentum,
    the optimizer may overshoot a bit, then come back, overshoot again, and oscillate
    like this many times before stabilizing at the minimum. This is one of the reasons
    why it’s good to have a bit of friction in the system: it reduces these oscillations
    and thus speeds up convergence.    Implementing momentum optimization in PyTorch
    is a no-brainer: just use the `SGD` optimizer and set its `momentum` hyperparameter,
    then sit back and profit!    [PRE23]    The one drawback of momentum optimization
    is that it adds yet another hyperparameter to tune. However, the momentum value
    of 0.9 usually works well in practice and almost always goes faster than regular
    gradient descent.    ## Nesterov Accelerated Gradient    One small variant to
    momentum optimization, proposed by [Yurii Nesterov in 1983](https://homl.info/55),⁠^([21](ch11.html#id2609))
    is almost always faster than regular momentum optimization. The *Nesterov accelerated
    gradient* (NAG) method, also known as *Nesterov momentum optimization*, measures
    the gradient of the cost function not at the local position **θ** but slightly
    ahead in the direction of the momentum, at **θ** + *β***m** (see [Equation 11-6](#nesterov_momentum_equation)).    #####
    Equation 11-6\. Nesterov accelerated gradient algorithm  <mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mn>1</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi> <mo>←</mo> <mi>β</mi>
    <mi mathvariant="bold">m</mi> <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo> <mi>β</mi>
    <mi mathvariant="bold">m</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>+</mo>
    <mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable>  This small tweak works
    because in general the momentum vector will be pointing in the right direction
    (i.e., toward the optimum), so it will be slightly more accurate to use the gradient
    measured a bit farther in that direction rather than the gradient at the original
    position, as you can see in [Figure 11-7](#nesterov_momentum_diagram) (where ∇[1]
    represents the gradient of the cost function measured at the starting point **θ**,
    and ∇[2] represents the gradient at the point located at **θ** + *β***m**).  ![Diagram
    illustrating regular versus Nesterov momentum optimization, showing that Nesterov
    updates by applying gradients after a momentum step, leading to improved convergence
    towards the optimum.](assets/hmls_1107.png)  ###### Figure 11-7\. Regular versus
    Nesterov momentum optimization: the former applies the gradients computed before
    the momentum step, while the latter applies the gradients computed after    As
    you can see, the Nesterov update ends up closer to the optimum. After a while,
    these small improvements add up and NAG ends up being significantly faster than
    regular momentum optimization. Moreover, note that when the momentum pushes the
    weights across a valley, ∇[1] continues to push farther across the valley, while
    ∇[2] pushes back toward the bottom of the valley. This helps reduce oscillations
    and thus NAG converges faster.    To use NAG, simply set `nesterov=True` when
    creating the `SGD` optimizer:    [PRE24]    ## AdaGrad    Consider the elongated
    bowl problem again: gradient descent starts by quickly going down the steepest
    slope, which does not point straight toward the global optimum, then it very slowly
    goes down to the bottom of the valley. It would be nice if the algorithm could
    correct its direction earlier to point a bit more toward the global optimum. The
    [*AdaGrad* algorithm](https://homl.info/56)⁠^([22](ch11.html#id2615)) achieves
    this correction by scaling down the gradient vector along the steepest dimensions
    (see [Equation 11-7](#adagrad_algorithm)).    ##### Equation 11-7\. AdaGrad algorithm  <mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn> <mo lspace="0%"
    rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi>
    <mo>←</mo> <mi mathvariant="bold">s</mi> <mo>+</mo> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>  The first step
    accumulates the square of the gradients into the vector **s** (recall that the
    ⊗ symbol represents the element-wise multiplication). This vectorized form is
    equivalent to computing *s*[*i*] ← *s*[*i*] + (∂*J*(**θ**) / ∂*θ*[*i*])² for each
    element *s*[*i*] of the vector **s**; in other words, each *s*[*i*] accumulates
    the squares of the partial derivative of the cost function with regard to parameter
    *θ*[*i*]. If the cost function is steep along the *i*^(th) dimension, then *s*[*i*]
    will get larger and larger at each iteration.    The second step is almost identical
    to gradient descent, but with one big difference: the gradient vector is scaled
    down by a factor of <msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt>
    (the ⊘ symbol represents the element-wise division, the square root is also computed
    element-wise, and *ε* is a smoothing term to avoid division by zero, typically
    set to 10^(–10)). This vectorized form is equivalent to simultaneously computing
    <msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo>∂</mo><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo
    lspace="0.25em" rspace="0.25em">/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt>
    for all parameters *θ*[*i*].    In short, this algorithm decays the learning rate,
    but it does so faster for steep dimensions than for dimensions with gentler slopes.
    This is called an *adaptive learning rate*. It helps point the resulting updates
    more directly toward the global optimum (see [Figure 11-8](#adagrad_diagram)).
    One additional benefit is that it requires much less tuning of the learning rate
    hyperparameter *η*.    AdaGrad frequently performs well for simple quadratic problems,
    but it often stops too early when training neural networks: the learning rate
    gets scaled down so much that the algorithm ends up stopping entirely before reaching
    the global optimum.  ![Diagram comparing AdaGrad and gradient descent, showing
    AdaGrad adjusting direction earlier towards the optimum in steep dimensions.](assets/hmls_1108.png)  ######
    Figure 11-8\. AdaGrad versus gradient descent: the former can correct its direction
    earlier to point to the optimum    So even though PyTorch has an `Adagrad` optimizer,
    you should not use it to train deep neural networks (it may be efficient for simpler
    tasks such as linear regression, though). Still, understanding AdaGrad is helpful
    to comprehend the other adaptive learning rate optimizers.    ## RMSProp    As
    we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging
    to the global optimum. The *RMSProp* algorithm⁠^([23](ch11.html#id2619)) fixes
    this by accumulating only the gradients from the most recent iterations, as opposed
    to all the gradients since the beginning of training. It does so by using exponential
    decay in the first step (see [Equation 11-8](#rmsprop_algorithm)).    ##### Equation
    11-8\. RMSProp algorithm  <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">s</mi> <mo>←</mo> <mi>α</mi> <mi mathvariant="bold">s</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow>
    <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi
    mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>2</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi> <mo>-</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>⊘</mo> <msqrt><mrow><mi mathvariant="bold">s</mi>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>  The decay rate
    *α* is typically set to 0.9\. Yes, it is once again a new hyperparameter, but
    this default value often works well, so you may not need to tune it at all.    As
    you might expect, PyTorch has an `RMSprop` optimizer:    [PRE25]    Except on
    very simple problems, this optimizer almost always performs much better than AdaGrad.
    In fact, it was the preferred optimization algorithm of many researchers until
    Adam optimization came around.    ## Adam    [*Adam*](https://homl.info/59),⁠^([24](ch11.html#id2626))
    which stands for *adaptive moment estimation*, combines the ideas of momentum
    optimization and RMSProp: just like momentum optimization, it keeps track of an
    exponentially decaying average of past gradients; and just like RMSProp, it keeps
    track of an exponentially decaying average of past squared gradients (see [Equation
    11-9](#adam_algorithm)). These are estimations of the mean and (uncentered) variance
    of the gradients. The mean is often called the *first moment*, while the variance
    is often called the *second moment*, hence the name of the algorithm.    #####
    Equation 11-9\. Adam algorithm  <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">m</mi> <mo>←</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi mathvariant="bold">m</mi>
    <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>2</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi> <mo>←</mo> <msub><mi>β</mi>
    <mn>2</mn></msub> <mi mathvariant="bold">s</mi> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>∇</mi>
    <mi mathvariant="bold">θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow> <mo>⊗</mo> <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mi>J</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>3</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>←</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">m</mi>
    <mrow><mn>1</mn> <mo>-</mo> <msup><mrow><msub><mi>β</mi> <mn>1</mn></msub></mrow>
    <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mn>4</mn>
    <mo lspace="0%" rspace="0%">.</mo></mrow></mtd> <mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover> <mo>←</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">s</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi>
    <mn>2</mn></msub></mrow> <mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mn>5</mn> <mo lspace="0%" rspace="0%">.</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi> <mo>←</mo> <mi mathvariant="bold">θ</mi>
    <mo>+</mo> <mi>η</mi> <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
    <mo>⊘</mo> <msqrt><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
    <mo>+</mo> <mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable>  In this equation,
    *t* represents the iteration number (starting at 1).    If you just look at steps
    1, 2, and 5, you will notice Adam’s close similarity to both momentum optimization
    and RMSProp: *β*[1] corresponds to *β* in momentum optimization, and *β*[2] corresponds
    to *α* in RMSProp. The only difference is that step 1 computes an exponentially
    decaying average rather than an exponentially decaying sum, but these are actually
    equivalent except for a constant factor (the decaying average is just 1 – *β*[1]
    times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since
    **m** and **s** are initialized at 0, they will be biased toward 0 at the beginning
    of training, so these two steps will help boost **m** and **s** at the beginning
    of training.    The momentum decay hyperparameter *β*[1] is typically initialized
    to 0.9, while the scaling decay hyperparameter *β*[2] is often initialized to
    0.999\. As earlier, the smoothing term *ε* is usually initialized to a tiny number
    such as 10^(–8). These are the default values for the `Adam` class. Here is how
    to create an Adam optimizer using PyTorch:    [PRE26]    Since Adam is an adaptive
    learning rate algorithm, like AdaGrad and RMSProp, it requires less tuning of
    the learning rate hyperparameter *η*. You can often use the default value *η*
    = 0.001, making Adam even easier to use than gradient descent.    ###### Tip    If
    you are starting to feel overwhelmed by all these different techniques and are
    wondering how to choose the right ones for your task, don’t worry: some practical
    guidelines are provided at the end of this chapter.    Finally, three variants
    of Adam are worth mentioning: AdaMax, NAdam, and AdamW.    ## AdaMax    The Adam
    paper also introduced AdaMax. Notice that in step 2 of [Equation 11-9](#adam_algorithm),
    Adam accumulates the squares of the gradients in **s** (with a greater weight
    for more recent gradients). In step 5, if we ignore *ε* and steps 3 and 4 (which
    are technical details anyway), Adam scales down the parameter updates by the square
    root of **s**. In short, Adam scales down the parameter updates by the ℓ[2] norm
    of the time-decayed gradients (recall that the ℓ[2] norm is the square root of
    the sum of squares).    AdaMax replaces the ℓ[2] norm with the ℓ[∞] norm (a fancy
    way of saying the max). Specifically, it replaces step 2 in [Equation 11-9](#adam_algorithm)
    with <mi mathvariant="bold">s</mi><mo>←</mo><mpadded lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="bold">s</mi><mo lspace="0%" rspace="0%">,</mo> <mtext>abs(</mtext><msub><mo
    mathvariant="bold">∇</mo><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded>, it drops step
    4, and in step 5 it scales down the gradient updates by a factor of **s**, which
    is the max of the absolute value of the time-decayed gradients.    In practice,
    this can make AdaMax more stable than Adam, but it really depends on the dataset,
    and in general Adam performs better. So, this is just one more optimizer you can
    try if you experience problems with Adam on some task.    ## NAdam    NAdam optimization
    is Adam optimization plus the Nesterov trick, so it will often converge slightly
    faster than Adam. In his report introducing this technique,⁠^([25](ch11.html#id2644))
    the researcher Timothy Dozat compares many different optimizers on various tasks
    and finds that NAdam generally outperforms Adam but is sometimes outperformed
    by RMSProp.    ## AdamW    [AdamW](https://homl.info/adamw)⁠^([26](ch11.html#id2645))
    is a variant of Adam that integrates a regularization technique called *weight
    decay*. Weight decay reduces the size of the model’s weights at each training
    iteration by multiplying them by a decay factor such as 0.99\. This may remind
    you of ℓ[2] regularization (introduced in [Chapter 4](ch04.html#linear_models_chapter)),
    which also aims to keep the weights small, and indeed it can be shown mathematically
    that ℓ[2] regularization is equivalent to weight decay when using SGD. However,
    when using Adam or its variants, ℓ[2] regularization and weight decay are *not*
    equivalent: in practice, combining Adam with ℓ[2] regularization results in models
    that often don’t generalize as well as those produced by SGD. AdamW fixes this
    issue by properly combining Adam with weight decay.    ###### Warning    Adaptive
    optimization methods (including RMSProp, Adam, AdaMax, NAdam, and AdamW optimization)
    are often great, converging fast to a good solution. However, a [2017 paper](https://homl.info/60)⁠^([27](ch11.html#id2650))
    by Ashia C. Wilson et al. showed that they can lead to solutions that generalize
    poorly on some datasets. So when you are disappointed by your model’s performance,
    try using NAG instead: your dataset may just be allergic to adaptive gradients.    To
    use NAdam, AdaMax, or AdamW in PyTorch, replace `torch.optim.Adam` with `torch.optim.NAdam`,
    `torch.optim.Adamax`, or `torch.optim.AdamW`. For AdamW, you probably want to
    tune the `weight_decay` hyperparameter.    All the optimization techniques discussed
    so far only rely on the *first-order partial derivatives* (*Jacobians*, which
    measure the slope of the loss function along each axis). The optimization literature
    also contains amazing algorithms based on the *second-order partial derivatives*
    (the *Hessians*, which are the partial derivatives of the Jacobians, measuring
    how each Jacobian changes along each axis; in other words, measuring the loss
    function’s curvature).    Unfortunately, these Hessian-based algorithms are hard
    to apply directly to deep neural networks because there are *n*² second-order
    derivatives per output (where *n* is the number of parameters), as opposed to
    just *n* first-order derivatives per output. Since DNNs typically have hundreds
    of thousands of parameters or more, the second-order optimization algorithms often
    don’t even fit in memory, and even when they do, computing the *Hessian matrix*
    is just too slow.⁠^([28](ch11.html#id2656))    Luckily, it is possible to use
    stochastic methods that can efficiently approximate second-order information.
    One such algorithm is Shampoo,⁠^([29](ch11.html#id2658)) which uses accumulated
    gradient information to approximate the second-order terms, similar to how Adam
    accumulates first-order statistics. It is not included in the PyTorch library,
    but you can get it in the PyTorch-Optimizer library (`pip install torch_optimizer`).    [Table 11-2](#optimizer_summary_table)
    compares all the optimizers we’ve discussed so far (![](assets/star_2b50.png)
    is bad, ![](assets/star_2b50.png)![](assets/star_2b50.png) is average, and ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    is good).      Table 11-2\. Optimizer comparison   | Class | Convergence speed
    | Convergence quality | | --- | --- | --- | | `SGD` | ![](assets/star_2b50.png)
    | ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `SGD(momentum=...)` | ![](assets/star_2b50.png)![](assets/star_2b50.png) |
    ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png) |
    | `SGD(momentum=..., nesterov=True)` | ![](assets/star_2b50.png)![](assets/star_2b50.png)
    | ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `Adagrad` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png) (stops
    too early) | | `RMSprop` | ![](assets/star_2b50.png)![](assets/star_2b50.png)
    | ![](assets/star_2b50.png)![](assets/star_2b50.png)  or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `Adam` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `AdaMax` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `NAdam` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    | | `AdamW` | ![](assets/star_2b50.png)![](assets/star_2b50.png) | ![](assets/star_2b50.png)![](assets/star_2b50.png) 
    or  ![](assets/star_2b50.png)![](assets/star_2b50.png)![](assets/star_2b50.png)
    |    # Learning Rate Scheduling    Finding a good learning rate is very important.
    If you set it too high, training will diverge (as discussed in [“Gradient Descent”](ch04.html#gradientDescent4)).
    If you set it too low, then training will be painfully slow, and it may also get
    stuck in a local optimum and produce a suboptimal model. If you set the learning
    rate fairly high (but not high enough to diverge), then training will often make
    rapid progress at first, but it will end up dancing around the optimum toward
    the end of training and thereby produce a suboptimal model. If you find a really
    good learning rate, you can end up with an excellent model, but training will
    generally be a bit too slow. Luckily, you can do better than a constant learning
    rate. In particular, it’s a good idea to start with a fairly high learning rate
    and then reduce it toward the end of training (or whenever progress stops): this
    ensures that training starts fast, while also allowing backprop to settle down
    toward the end to really fine-tune the model parameters (see [Figure 11-9](#learning_schedule_diagram)).    There
    are various other strategies to tweak the learning rate during training. These
    are called *learning schedules* (I briefly introduced this concept in [Chapter 4](ch04.html#linear_models_chapter)).
    The `torch.optim.lr_scheduler` module provides several implementations of common
    learning schedules. Let’s look at the most important ones, starting with exponential
    scheduling.  ![Diagram illustrating the effects of different learning rates on
    loss over epochs, showing that starting with a high rate and reducing it achieves
    optimal training results.](assets/hmls_1109.png)  ###### Figure 11-9\. Learning
    curves for various learning rates η    ## Exponential Scheduling    The `ExponentialLR`
    class implements *exponential scheduling*, whereby the learning rate is multiplied
    by a constant factor `gamma` at some regular interval, typically at every epoch.
    As a result, after the *n*^(th) epoch, the learning rate will be equal to the
    initial learning rate times `gamma` to the power of *n*. This factor `gamma` is
    yet another hyperparameter you can tune. In general, you will want to set `gamma`
    to a value lower than 1, but fairly close to 1 to avoid decreasing the learning
    rate too fast. For example, if `gamma` is set to 0.9, then after 10 epochs the
    learning rate will be about 35% of the initial learning rate, and after 20 epochs
    it will be about 12%.    The `ExponentialLR` constructor expects at least two
    arguments—the optimizer whose learning rate will be tweaked during training, and
    the factor `gamma`:    [PRE27]    Next, you must update the training loop to call
    `scheduler.step()` at the end of each epoch to tweak the optimizer’s learning
    rate:    [PRE28]    ###### Tip    If you interrupt training and you later want
    to resume it where you left off, you should set the `last_epoch` argument of the
    scheduler’s constructor to the last epoch you ran (zero-indexed). The default
    is –1, which makes the scheduler start training from scratch.    ## Cosine Annealing    Instead
    of decreasing the learning rate exponentially, you can use the cosine function
    to go from the maximum learning rate *η*[max] at the start of training, down to
    the minimum learning rate *η*[min] at the end. This is called *cosine annealing*.
    Compared to exponential scheduling, cosine annealing ensures that the learning
    rate remains fairly high during most of training, while getting closer to the
    minimum near the end (see [Figure 11-10](#cosine_annealing_diagram)). All in all,
    cosine annealing generally performs better. The learning rate at epoch *t* (zero-indexed)
    is given by [Equation 11-10](#cosine_annealing_equation), where *T*[max] is the
    maximum number of epochs.  ![A diagram comparing cosine and exponential learning
    rate schedules, showing the cosine schedule maintaining a higher rate over more
    epochs before decreasing.](assets/hmls_1110.png)  ###### Figure 11-10\. Cosine
    annealing learning schedule    ##### Equation 11-10\. Cosine annealing equation  <mrow><msub><mi>η</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>η</mi> <mtext>min</mtext></msub> <mo>+</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>(</mo> <msub><mi>η</mi> <mtext>max</mtext></msub>
    <mo>-</mo> <msub><mi>η</mi> <mtext>min</mtext></msub> <mo>)</mo></mrow> <mfenced
    separators="" open="(" close=")"><mn>1</mn> <mo>+</mo> <mo form="prefix">cos</mo>
    <mfenced separators="" open="(" close=")"><mfrac><mi>t</mi> <msub><mi>T</mi> <mtext>max</mtext></msub></mfrac>
    <mi>π</mi></mfenced></mfenced></mrow>  PyTorch includes the `CosineAnnealingLR`
    scheduler, which you can create as follows (`T_max` is *T*[max] and `eta_min`
    is *η*[min]). You can then use it just like the `ExponentialLR` scheduler:    [PRE29]    One
    problem with cosine annealing is that you have to set two new hyperparameters,
    *T*[max] and *η*[min], and it’s not easy to know in advance how many epochs to
    train and when to stop decreasing the learning rate. This is why I generally prefer
    to use the performance scheduling technique.    ## Performance Scheduling    *Performance
    scheduling*, also called *adaptive scheduling*, is implemented by PyTorch’s `ReduceLROnPlateau`
    scheduler: it keeps track of a given metric during training—typically the validation
    loss—and if this metric stops improving for some time, it multiplies the learning
    rate by some factor. This scheduler has quite a few hyperparameters, but the default
    values work well for most of them. You may occasionally need to tweak the following
    (see the documentation for information on the other hyperparameters):    `mode`      If
    the tracked metric must be maximized (such as the validation accuracy), then you
    must set the `mode` to `''max''`. The default is `''min''`, which is fine if the
    tracked metric must be minimized (such as the validation loss).      `patience`      The
    number of consecutive steps (typically epochs) to wait for improvement in the
    monitored metric before reducing the learning rate. It defaults to 10, which is
    generally fine. If each epoch is very long, then you may want to reduce this value.      `factor`      The
    factor by which the learning rate will be multiplied whenever the monitored metric
    fails to improve for too long. It defaults to 0.1, again a reasonable default,
    but perhaps a bit small in some cases.      For example, let’s implement performance
    scheduling based on the validation accuracy (i.e., which we want to maximize):    [PRE30]    The
    training loop needs to be tweaked again because we must evaluate the desired metric
    at each epoch (in this example, we are using the `evaluate_tm()` function that
    we defined in [Chapter 10](ch10.html#pytorch_chapter)), and we must then pass
    the result to the scheduler’s `step()` method:    [PRE31]    ## Warming Up the
    Learning Rate    So far, we have always started training with the maximum learning
    rate. However, this can sometimes cause gradient descent to bounce around randomly
    at the beginning of training, neither exploding nor making any significant progress.
    This typically happens with sensitive models, such as recurrent neural networks
    ([Chapter 13](ch13.html#rnn_chapter)), or when using a very large batch size.
    In such cases, one solution is to “warm up” the learning rate, starting close
    to zero and gradually increasing the learning rate over a few epochs, up to the
    maximum learning rate. During this warm-up phase, gradient descent has time to
    stabilize into a better region of the loss landscape, where it can then make quick
    progress using a high learning rate.    Why does this work? Well, the loss landscape
    sometimes resembles the Himalayas: it’s very high up and full of gigantic spikes.
    If you start with a high learning rate, you might jump from one mountain peak
    to the next for a very long time. If instead you start with a small learning rate,
    you will just walk down the mountain and valleys and escape the spiky mountain
    range altogether until you reach flatter lands. From then on, you can use a large
    learning rate for the rest of your journey, slowing down only toward the end.    A
    common way to implement learning rate warm up using PyTorch is to use a `LinearLR`
    scheduler to increase the learning rate linearly over a few epochs. For example,
    the following scheduler will increase the learning rate from 10% to 100% of the
    optimizer’s original learning rate over 3 epochs (i.e., 10% during the first epoch,
    40% during the second epoch, 70% during the third epoch, and 100% after that):    [PRE32]    If
    you would like more flexibility, you can write your own custom function and wrap
    it in a `LambdaLR` scheduler. For example, the following scheduler is equivalent
    to the `LinearLR` scheduler we just defined:    [PRE33]    You must then insert
    `warmup_scheduler.step()` at the beginning of each epoch, and make sure you deactivate
    the scheduler(s) you are using for the rest of training during the warm-up phase.
    And that’s all!    [PRE34]    In short, you pretty much always want to cool down
    the learning rate at the end of training, and you may also want to warm it up
    at the beginning if gradient descent needs a bit of help getting started. But
    are there any cases where you may want to tweak the learning rate in the middle
    of training? Well yes, there are; for example, if gradient descent gets stuck
    in a local optimum or a high plateau. Gradient descent could remain stuck here
    for a long time, or even forever. Luckily, there’s a way to escape this trap:
    just increase the learning rate for a little while.    You could spend your time
    staring at the learning curves during training, and manually interrupting it to
    tweak the learning rate when needed, but you probably have better things to do.
    Alternatively, you could implement a custom scheduler that monitors the validation
    metric—much like the `ReduceLROnPlateau` scheduler—and increases the learning
    rate for a while if the validation metric is stuck in a bad plateau. For this,
    you could subclass the `LRScheduler` base class. This is beyond the scope of this
    book, but you can take inspiration from the `ReduceLROnPlateau` scheduler’s source
    code (and get a little bit of help from your favorite AI assistant). But a much
    simpler option is to use the cosine annealing with warm restarts learning schedule.
    Let’s look at it now.    ## Cosine Annealing with Warm Restarts    *Cosine annealing
    with warm restarts* was introduced in a [2016 paper](https://homl.info/coslr)
    by Ilya Loshchilov and Frank Hutter.⁠^([30](ch11.html#id2674)) This schedule just
    repeats the cosine annealing schedule over and over again. Since the learning
    rate regularly shoots back up, this schedule allows gradient descent to escape
    local optima and plateaus automatically. The authors recommend starting with a
    fairly short round of cosine annealing, but then doubling *T*[max] after each
    round (see [Figure 11-11](#cosine_annealing_warm_restarts_diagram)). This allows
    gradient descent to do a lot of quick explorations at the start of training, while
    also taking the time to properly optimize the model later during training, possibly
    escaping a plateau or two along the way.  ![Diagram illustrating the cosine annealing
    with warm restarts schedule, showing fluctuations in learning rate with repeated
    cycles increasing in length.](assets/hmls_1111.png)  ###### Figure 11-11\. Cosine
    annealing with warm restarts    Conveniently, PyTorch includes a `CosineAnnealingWarmRestarts`
    scheduler. You must set `T_0`, which is the value of *T*[max] for the first round
    of cosine annealing. You may also set `T_mult` to 2 if you want to double *T*[max]
    at each round (the default is 1, meaning *T*[max] stays constant and all rounds
    have the same length). Finally, you can set `eta_min` (it defaults to 0):    [PRE35]    ##
    1cycle Scheduling    Yet another popular learning schedule is *1cycle*, introduced
    in a [2018 paper](https://homl.info/1cycle) by Leslie Smith.⁠^([31](ch11.html#id2677))
    It starts by warming up the learning rate, starting at *η*[0] and growing linearly
    up to *η*[1] halfway through training. Then it decreases the learning rate linearly
    down to *η*[0] again during the second half of training, finishing the last few
    epochs by dropping the rate down by several orders of magnitude (still linearly).
    The maximum learning rate *η*[1] is chosen using the same approach we used to
    find the optimal learning rate, and the initial learning rate *η*[0] is usually
    10 times lower. When using a momentum, we start with a high momentum first (e.g.,
    0.95), then drop it down to a lower momentum during the first half of training
    (e.g., down to 0.85, linearly), and then bring it back up to the maximum value
    (e.g., 0.95) during the second half of training, finishing the last few epochs
    with that maximum value. Smith did many experiments showing that this approach
    was often able to speed up training considerably and reach better performance.
    For example, on the popular CIFAR10 image dataset, this approach reached 91.9%
    validation accuracy in just 100 epochs, compared to 90.3% accuracy in 800 epochs
    through a standard approach (using the same neural network architecture). This
    feat was dubbed *super-convergence*. PyTorch implements this schedule in the `OneCycleLR`
    scheduler.    ###### Tip    If you are not sure which learning schedule to use,
    1cycle can be a good default, but I tend to have more luck with performance scheduling.
    If you run into instabilities at the start of training, try adding learning rate
    warm-up. And if training gets stuck on plateaus, try cosine annealing with warm
    restarts.    We have now covered the most popular learning schedules, but PyTorch
    offers a few extra schedulers (e.g., a polynomial scheduler, a cyclic scheduler,
    a scheduler that makes it easy to chain other schedulers, and a few more), so
    make sure to check out the documentation.    Now let’s move on to one final topic
    before we complete this chapter on deep learning training techniques: regularization.
    Deep learning is highly prone to overfitting, so regularization is key!    # Avoiding
    Overfitting Through Regularization    > With four parameters I can fit an elephant
    and with five I can make him wiggle his trunk. >  > John von Neumann, cited by
    Enrico Fermi in *Nature* 427    With thousands of parameters, you can fit the
    whole zoo. Deep neural networks typically have tens of thousands of parameters,
    sometimes even millions or billions. This gives them an incredible amount of freedom
    and means they can fit a huge variety of complex datasets. But this great flexibility
    also makes the network prone to overfitting the training set. Regularization is
    often needed to prevent this.    We already implemented a common regularization
    technique in [Chapter 4](ch04.html#linear_models_chapter): early stopping. Moreover,
    even though batch-norm and layer-norm were designed to solve the unstable gradients
    problems, they also act like pretty good regularizers. In this section we will
    examine other popular regularization techniques for neural networks: ℓ[1] and
    ℓ[2] regularization, dropout, MC dropout, and max-norm regularization.    ## ℓ[1]
    and ℓ[2] Regularization    Just like you did in [Chapter 4](ch04.html#linear_models_chapter)
    for simple linear models, you can use ℓ[2] regularization to constrain a neural
    network’s connection weights, and/or ℓ[1] regularization if you want a sparse
    model (with many weights equal to 0). As we saw earlier (when discussing the AdamW
    optimizer), ℓ[2] regularization is mathematically equivalent to weight decay when
    using an `SGD` optimizer (with or without momentum), so if that’s the case you
    can implement ℓ[2] regularization by simply setting the optimizer’s `weight_decay`
    argument. For example, here is how to apply ℓ[2] regularization to the connection
    weights of a PyTorch model trained using `SGD`, with a regularization factor of
    10^(–4):    [PRE36]    If instead you are using an Adam optimizer, you should
    switch to AdamW and set the `weight_decay` argument. This is not exactly equivalent
    to ℓ[2] regularization, but as we saw earlier it’s pretty close and it works better.    Note
    that weight decay is applied to every model parameter, including bias terms, and
    even parameters of batch-norm and layer-norm layers. Generally that’s not a big
    deal, but penalizing these parameters does not contribute much to regularization
    and it may sometimes negatively impact training performance. So how can we apply
    weight decay to some model parameters and not others? One approach is to implement
    ℓ[2] regularization manually, without relying on the optimizer’s weight decay
    feature. For this, you must tweak the training loop to manually compute the ℓ[2]
    loss based only on the parameters you want, and add this ℓ[2] loss to the main
    loss:    [PRE37]    Another approach is to use PyTorch’s *parameter groups* feature,
    which lets the optimizer apply different hyperparameters to different groups of
    model parameters. So far, we have always created optimizers by passing them the
    full list of model parameters: PyTorch automatically put them all in a single
    parameter group, sharing the same hyperparameters. Instead, we can pass a list
    of dictionaries to the optimizer, each with a `"params"` entry containing a list
    of parameters, and (optionally) some hyperparameter key/value pairs specific to
    this group of parameters. The group-specific hyperparameters take precedence over
    the optimizer’s global hyperparameters. For example, let’s create an optimizer
    with two parameter groups: the first group will contain all the parameters we
    want to regularize and it will use weight decay, while the second group will contain
    all the bias terms and BN parameters, and it will not use weight decay at all.    [PRE38]    ######
    Tip    Parameter groups also allow you to apply different learning rates to different
    parts of your model. This is most common for transfer learning, when you want
    new layers to be updated faster than reused ones.    Now how about ℓ[1] regularization?
    Well unfortunately PyTorch does not provide any helper for this, so you need to
    implement it manually, much like we did for ℓ[2] regularization. This means tweaking
    the training loop to compute the ℓ[1] loss and adding it to the main loss:    [PRE39]    That’s
    all there is to it! Now let’s move on to Dropout, which is one of the most popular
    regularization techniques for deep neural networks.    ## Dropout    *Dropout*
    was [proposed in a paper](https://homl.info/64)⁠^([32](ch11.html#id2696)) by Geoffrey
    Hinton et al. in 2012 and further detailed in a [2014 paper](https://homl.info/65)⁠^([33](ch11.html#id2697))
    by Nitish Srivastava et al., and it has proven to be highly successful: many state-of-the-art
    neural networks use dropout, as it gives them a 1%–2% accuracy boost. This may
    not sound like a lot, but when a model already has 95% accuracy, getting a 2%
    accuracy boost means dropping the error rate by almost 40% (going from 5% error
    to roughly 3%).    It is a fairly simple algorithm: at every training step, every
    neuron (including the input neurons, but always excluding the output neurons)
    has a probability *p* of being temporarily “dropped out”, meaning it will be entirely
    ignored during this training step, but it may be active during the next step (see
    [Figure 11-12](#dropout_diagram)). The hyperparameter *p* is called the *dropout
    rate*, and it is typically set between 10% and 50%: closer to 20%–30% in recurrent
    neural nets (see [Chapter 13](ch13.html#rnn_chapter)), and closer to 40%–50% in
    convolutional neural networks (see [Chapter 12](ch12.html#cnn_chapter)). After
    training, neurons don’t get dropped anymore. And that’s all (except for a technical
    detail we will discuss shortly).    It’s surprising at first that this destructive
    technique works at all. Would a company perform better if its employees were told
    to toss a coin every morning to decide whether to go to work? Well, who knows;
    perhaps it would! The company would be forced to adapt its organization; it could
    not rely on any single person to work the coffee machine or perform any other
    critical tasks, so this expertise would have to be spread across several people.
    Employees would have to learn to cooperate with many of their coworkers, not just
    a handful of them. The company would become much more resilient. If one person
    quit, it wouldn’t make much of a difference. It’s unclear whether this idea would
    actually work for companies, but it certainly does for neural networks. Neurons
    trained with dropout cannot co-adapt with their neighboring neurons; they have
    to be as useful as possible on their own. They also cannot rely excessively on
    just a few input neurons; they must pay attention to all of their input neurons.
    They end up being less sensitive to slight changes in the inputs. In the end,
    you get a more robust network that generalizes better.  ![Diagram illustrating
    dropout regularization in a neural network where random neurons are deactivated
    at each step, represented by dashed arrows and red crosses.](assets/hmls_1112.png)  ######
    Figure 11-12\. With dropout regularization, at each training iteration a random
    subset of all neurons in one or more layers—except the output layer—are “dropped
    out”; these neurons output 0 at this iteration (represented by the dashed arrows)    Another
    way to understand the power of dropout is to realize that a unique neural network
    is generated at each training step. Since each neuron can be either present or
    absent, there are a total of 2^(*N*) possible networks (where *N* is the total
    number of droppable neurons). This is such a huge number that it is virtually
    impossible for the same neural network to be sampled twice. Once you have run
    10,000 training steps, you have essentially trained 10,000 different neural networks,
    each with just one training instance. These neural networks are obviously not
    independent because they share many of their weights, but they are nevertheless
    all different. The resulting neural network can be seen as an averaging ensemble
    of all these smaller neural networks.    ###### Tip    Higher layers, which learn
    more complex feature combinations, benefit more from dropout because they are
    more prone to overfitting. So you can usually apply dropout only to the neurons
    of the top hidden layers (e.g., one to three hidden layers). However, you should
    avoid dropping the output neurons, as this would be like changing the task during
    training: it wouldn’t help.    There is one small but important technical detail.
    Suppose *p* = 75%: on average only 25% of all neurons are active at each step
    during training. This means that after training, each neuron receives four times
    more inputs than during training, on average. This discrepancy is so large that
    the model is unlikely to work well. To avoid this issue, a simple solution is
    to multiply the inputs by 4 during training, which is the same as dividing them
    by 25%. More generally, we need to divide the inputs by the *keep probability*
    (1 – *p*) during training.    To implement dropout using PyTorch, you can use
    the `nn.Dropout` layer. It’s important to switch to training mode during training,
    and to evaluation mode during evaluation (just like for batch norm). In training
    mode, the layer randomly drops some inputs (setting them to 0) and divides the
    remaining inputs by the keep probability. In evaluation mode, it does nothing
    at all; it just passes the inputs to the next layer. The following code applies
    dropout regularization before every `nn.Linear` layer, using a dropout rate of
    0.2:    [PRE40]    ###### Warning    Since dropout is only active during training,
    comparing the training loss and the validation loss can be misleading. In particular,
    a model may be overfitting the training set and yet have similar training and
    validation losses. So make sure to evaluate the training loss without dropout
    (e.g., after training).    If you observe that the model is overfitting, you can
    increase the dropout rate. Conversely, you should try decreasing the dropout rate
    if the model underfits the training set. It can also help to increase the dropout
    rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art
    architectures only apply dropout to the last few hidden layers, so you may want
    to try this if full dropout is too strong.    Dropout does tend to significantly
    slow down convergence, but it often results in a better model when tuned properly.
    So it is generally well worth the extra time and effort, especially for large
    models.    ###### Tip    If you want to regularize a self-normalizing network
    based on the SELU activation function (as discussed earlier), you should use *alpha
    dropout*: this is a variant of dropout that preserves the mean and standard deviation
    of its inputs. It was introduced in the same paper as SELU, as regular dropout
    would break self-normalization. PyTorch implements it in the `nn.AlphaDropout`
    layer.    ## Monte Carlo Dropout    In 2016, a [paper](https://homl.info/mcdropout)⁠^([34](ch11.html#id2709))
    by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:    *   First,
    the paper established a profound connection between dropout networks (i.e., neural
    networks containing `Dropout` layers) and approximate Bayesian inference,⁠^([35](ch11.html#id2710))
    giving dropout a solid mathematical justification.           *   Second, the authors
    introduced a powerful technique called *Monte Carlo (MC) dropout*, which can boost
    the performance of any trained dropout model without having to retrain it or even
    modify it at all. It also provides a much better measure of the model’s uncertainty,
    and it can be implemented in just a few lines of code.              This description
    of MC dropout sounds like some “one weird trick” clickbait, so let me explain:
    it is just like regular dropout, except it is active not only during training,
    but also during evaluation. This means that the predictions are always a bit random
    (hence the name Monte Carlo). But instead of making a single prediction, you make
    many predictions and average them out. It turns out that this produces better
    predictions than the original model.    Following is a full implementation of
    MC dropout, using the model we trained in the previous section to make predictions
    for a batch of images:    [PRE41]    Let’s go through this code:    *   First,
    we switch the model to evaluation mode as we always do before making predictions,
    but this time we immediately switch all the dropout layers back to training mode,
    so they will behave just like during training (i.e., randomly dropping out some
    of their inputs). In other words, we convert the dropout layers to MC dropout
    layers.           *   Next we load a new batch of images `X_new`, and we move
    it to the GPU. In this example, let’s assume `X_new` contains three images.           *   We
    then use the `repeat_interleave()` method to create a batch containing 100 copies
    of each image in `X_new`. The images are repeated along the first dimension (`dim=0`)
    so `X_new_repeated` has a shape of `[300, 1, 28, 28]`.           *   Next, we
    pass this big batch to the model, which predicts 10 logits per image, as usual.
    This tensor’s shape is `[300, 10]`, but we reshape it to `[3, 100, 10]` to group
    the predictions for each image. Remember that the dropout layers are active, which
    means that there’s some variability across the predictions, even for copies of
    the same image.           *   Then we convert these logits to estimated probabilities
    using the softmax function.           *   Lastly, we compute the mean over the
    second dimension (`dim=1`) to get the average estimated probability for each class
    and each image, across all 100 predictions. The result is a tensor of shape `[3,
    10]`. These are our final predictions:              [PRE42]   [PRE43] ###### Warning    Rather
    than converting the logits to probabilities and then computing the mean probabilities,
    you may be tempted to do the reverse: first average over the logits and *then*
    convert the mean logits to probabilities. This is faster but it does not properly
    reflect the model’s uncertainty, so it tends to produce overconfident models.    MC
    dropout tends to improve the reliability of the model’s probability estimates.
    This means that it’s less likely to be confidently wrong, making it safer (you
    don’t want a self-driving car confidently ignoring a stop sign). It’s also useful
    when you’re interested in the top *k* classes, not just the most likely. Additionally,
    you can take a look at the [standard deviation of each class probability](https://xkcd.com/2110):    [PRE44]py   [PRE45]`py  [PRE46]py  [PRE47]``py`
    [PRE48]`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6] >>> dict(model[1].named_buffers()).keys() `dict_keys([''running_mean'',
    ''running_var'', ''num_batches_tracked''])` [PRE7]`The authors of the BN paper
    argued in favor of adding the BN layers before the activation functions, rather
    than after (as we just did). There is some debate about this, and it seems to
    depend on the task, so you can experiment with this to see which option works
    best on your dataset. If you move the BN layers before the activation functions,
    you can also remove the bias term from the previous `nn.Linear` layers by setting
    their `bias` hyperparameter to `False`. Indeed, a batch-norm layer already includes
    one bias term per input. You can also drop the first BN layer to avoid sandwiching
    the first hidden layer between two BN layers, but this means you should normalize
    the training set before training. The updated code looks like this:    [PRE8]    The
    `nn.BatchNorm1d` class has a few hyperparameters you can tweak. The defaults will
    usually be fine, but you may occasionally need to tweak the `momentum`. This hyperparameter
    is used by the `BatchNorm1d` layer when it updates the exponential moving averages;
    given a new value **v** (i.e., a new vector of input means or variances computed
    over the current batch), the layer updates the running average <mover accent="true"><mi
    mathvariant="bold">v</mi><mo>^</mo></mover> using the following equation:  <mrow><mover
    accent="true"><mi mathvariant="bold">v</mi><mo>^</mo></mover> <mo>←</mo> <mi mathvariant="bold">v</mi>
    <mo>×</mo> <mtext>momentum</mtext> <mo>+</mo> <mover accent="true"><mi mathvariant="bold">v</mi><mo>^</mo></mover>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mtext>momentum</mtext> <mo>)</mo></mrow></mrow>  A
    good momentum value is typically close to 0; for example, 0.01 or 0.001\. You
    want more 0s for smaller mini-batches, and fewer for larger mini-batches. The
    default is 0.1, which is good for large batch sizes, but not great for small batch
    sizes such as 32 or 64.    ###### Warning    When people talk about “momentum”
    in the context of a running mean, they usually refer to the weight of the current
    running mean in the update equation. Sadly, for historical reasons, PyTorch uses
    the opposite meaning in the BN layers. However, other parts of PyTorch use the
    conventional meaning (e.g., in optimizers), so don’t get confused.[PRE9]``  [PRE10]  [PRE11]`##
    Layer Normalization    Layer normalization (LN) is very similar to batch norm,
    but instead of normalizing across the batch dimension, LN normalizes across the
    feature dimensions. This simple idea was introduced by Jimmy Lei Ba et al. in
    a [2016 paper](https://homl.info/layernorm),⁠^([18](ch11.html#id2562)) and initially
    applied mostly to recurrent nets. However, in recent years it has been successfully
    applied to many other architectures, such as convolutional nets, transformers,
    diffusion nets, and more.    One advantage is that LN can compute the required
    statistics on the fly, at each time step, independently for each instance. This
    also means that it behaves the same way during training and testing (as opposed
    to BN), and it does not need to use exponential moving averages to estimate the
    feature statistics across all instances in the training set, like BN does. Lastly,
    LN learns a scale and an offset parameter for each input feature, just like BN
    does.    PyTorch includes an `nn.LayerNorm` module. To create an instance, you
    must simply indicate the size of the dimensions that you want to normalize over.
    These must be the last dimension(s) of the inputs. For example, if the inputs
    are batches of 100 × 200 RGB images of shape `[3, 100, 200]`, and you want to
    normalize each image over each of the three color channels separately, you would
    use the following `nn.LayerNorm` module:    [PRE12]    The following code produces
    the same result:    [PRE13]    However, most computer vision architectures that
    use LN normalize over all channels at once. For this, you must include the size
    of the channel dimension when creating the `nn.LayerNorm` module:    [PRE14]    And
    that’s all there is to it! Now let’s look'
