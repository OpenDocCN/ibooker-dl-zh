- en: Chapter 2\. Go with the Flow: Up and Running with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。随波逐流：TensorFlow快速入门
- en: In this chapter we start our journey with two working TensorFlow examples. The
    first (the traditional “hello world” program), while short and simple, includes
    many of the important elements we discuss in depth in later chapters. With the
    second, a first end-to-end machine learning model, you will embark on your journey
    toward state-of-the-art machine learning with TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从两个可工作的TensorFlow示例开始我们的旅程。第一个（传统的“hello world”程序），虽然简短简单，但包含了我们在后续章节中深入讨论的许多重要元素。通过第二个，一个首个端到端的机器学习模型，您将开始您的TensorFlow最新机器学习之旅。
- en: Before getting started, we briefly walk through the installation of TensorFlow.
    In order to facilitate a quick and painless start, we install the CPU version
    only, and defer the GPU installation to later.^([1](ch02.html#idm139707904506064)) (If
    you don’t know what this means, that’s OK for the time being!) If you already
    have TensorFlow installed, skip to the second section.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们简要介绍TensorFlow的安装。为了方便快速启动，我们仅安装CPU版本，并将GPU安装推迟到以后。如果你不知道这意味着什么，那没关系！如果你已经安装了TensorFlow，请跳到第二部分。
- en: Installing TensorFlow
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装TensorFlow
- en: 'If you are using a clean Python installation (probably set up for the purpose
    of learning TensorFlow), you can get started with the simple `pip` installation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是干净的Python安装（可能是为学习TensorFlow而设置的），您可以从简单的`pip`安装开始：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This approach does, however, have the drawback that TensorFlow will override
    existing packages and install specific versions to satisfy dependencies. If you
    are using this Python installation for other purposes as well, this will not do. One
    common way around this is to install TensorFlow in a virtual environment, managed
    by a utility called *virtualenv*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的缺点是TensorFlow将覆盖现有软件包并安装特定版本以满足依赖关系。如果您还将此Python安装用于其他目的，这样做将不起作用。一个常见的解决方法是在一个由*virtualenv*管理的虚拟环境中安装TensorFlow。
- en: 'Depending on your setup, you may or may not need to install *virtualenv* on
    your machine. To install *virtualenv*, type:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的设置，您可能需要或不需要在计算机上安装*virtualenv*。要安装*virtualenv*，请键入：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: See [*http://virtualenv.pypa.io*](http://virtualenv.pypa.io) for further instructions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[*http://virtualenv.pypa.io*](http://virtualenv.pypa.io)获取更多说明。
- en: 'In order to install TensorFlow in a virtual environment, you must first create
    the virtual environment—in this book we choose to place these in the *~/envs*
    folder, but feel free to put them anywhere you prefer:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在虚拟环境中安装TensorFlow，您必须首先创建虚拟环境——在本书中，我们选择将其放在*~/envs*文件夹中，但请随意将其放在您喜欢的任何位置：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will create a virtual environment named *tensorflow* in *~/envs* (which
    will manifest as the folder *~/envs/tensorflow*). To activate the environment,
    use:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在*~/envs*中创建一个名为*tensorflow*的虚拟环境（将显示为*~/envs/tensorflow*文件夹）。要激活环境，请使用：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The prompt should now change to indicate the activated environment:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 提示现在应该改变以指示已激活的环境：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point the `pip install` command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此时`pip install`命令：
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: will install TensorFlow into the virtual environment, without impacting other
    packages installed on your machine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将在虚拟环境中安装TensorFlow，而不会影响您计算机上安装的其他软件包。
- en: 'Finally, in order to exit the virtual environment, you type:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了退出虚拟环境，您需要键入：
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'at which point you should get back the regular prompt:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该会得到常规提示符：
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Adding an alias to ~/.bashrc
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在~/.bashrc中添加别名
- en: 'The process described for entering and exiting your virtual environment might
    be too cumbersome if you intend to use it often. In this case, you can simply
    append the following command to your *~/.bashrc* file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 描述进入和退出虚拟环境的过程可能会太繁琐，如果您打算经常使用它。在这种情况下，您可以简单地将以下命令附加到您的*~/.bashrc*文件中：
- en: '[PRE8]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: and use the command `tensorflow` to activate the virtual environment. To quit
    the environment, you will still use `deactivate`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用命令`tensorflow`来激活虚拟环境。要退出环境，您仍然会使用`deactivate`。
- en: Now that we have a basic installation of TensorFlow, we can proceed to our first
    working examples. We will follow the well-established tradition and start with
    a “hello world” program.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经基本安装了TensorFlow，我们可以继续进行我们的第一个工作示例。我们将遵循已经建立的传统，并从一个“hello world”程序开始。
- en: Hello World
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hello World
- en: Our first example is a simple program that combines the words “Hello” and “
    World!” and displays the output—the phrase “Hello World!” While simple and straightforward,
    this example introduces many of the core elements of TensorFlow and the ways in
    which it is different from a regular Python program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个示例是一个简单的程序，将单词“Hello”和“ World!”组合在一起并显示输出——短语“Hello World!”。虽然简单直接，但这个示例介绍了TensorFlow的许多核心元素以及它与常规Python程序的不同之处。
- en: We suggest you run this example on your machine, play around with it a bit,
    and see what works. Next, we will go over the lines of code and discuss each element
    separately.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您在计算机上运行此示例，稍微玩弄一下，并查看哪些有效。接下来，我们将逐行查看代码并分别讨论每个元素。
- en: 'First, we run a simple install and version check (if you used the virtualenv
    installation option, make sure to activate it before running TensorFlow code):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们运行一个简单的安装和版本检查（如果您使用了virtualenv安装选项，请确保在运行TensorFlow代码之前激活它）：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If correct, the output will be the version of TensorFlow you have installed
    on your system. Version mismatches are the most probable cause of issues down
    the line.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果正确，输出将是您在系统上安装的TensorFlow版本。版本不匹配是后续问题的最有可能原因。
- en: '[Example 2-1](#ex0201) shows the complete “hello world” example.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例2-1](#ex0201)显示了完整的“hello world”示例。'
- en: Example 2-1\. “Hello world” with TensorFlow
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-1。“Hello world”与TensorFlow
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We assume you are familiar with Python and imports, in which case the first
    line:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您熟悉Python和导入，那么第一行：
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: requires no explanation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要解释。
- en: IDE configuration
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IDE配置
- en: 'If you are running TensorFlow code from an IDE, then make sure to redirect
    to the virtualenv where the package is installed. Otherwise, you will get the
    following import error:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从IDE运行TensorFlow代码，请确保重定向到安装包的虚拟环境。否则，您将收到以下导入错误：
- en: '`ImportError: No module named tensorflow`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImportError: No module named tensorflow`'
- en: In the PyCharm IDE this is done by selecting Run→Edit Configurations, then changing
    Python Interpreter to point to *~/envs/tensorflow/bin/python*, assuming you used
    *~/envs/tensorflow* as the virtualenv directory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyCharm IDE中，通过选择Run→Edit Configurations，然后将Python Interpreter更改为指向*~/envs/tensorflow/bin/python*，假设您使用*~/envs/tensorflow*作为虚拟环境目录。
- en: 'Next, we define the constants `"Hello"` and `" World!"`, and combine them:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义常量`"Hello"`和`" World!"`，并将它们组合起来：
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, you might wonder how (if at all) this is different from the
    simple Python code for doing this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能会想知道这与用于执行此操作的简单Python代码有何不同（如果有的话）：
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The key point here is what the variable `hw` contains in each case. We can
    check this using the `print` command. In the pure Python case we get this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键点是每种情况下变量`hw`包含的内容。我们可以使用`print`命令来检查这一点。在纯Python情况下，我们得到这个：
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the TensorFlow case, however, the output is completely different:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在TensorFlow情况下，输出完全不同：
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Probably not what you expected!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不是您期望的！
- en: 'In the next chapter we explain the computation graph model of TensorFlow in
    detail, at which point this output will become completely clear. The key idea
    behind computation graphs in TensorFlow is that we first define what computations
    should take place, and then trigger the computation in an external mechanism.
    Thus, the TensorFlow line of code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细解释TensorFlow的计算图模型，到那时这个输出将变得完全清晰。TensorFlow中计算图的关键思想是，我们首先定义应该发生的计算，然后在外部机制中触发计算。因此，TensorFlow代码行：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: does *not* compute the sum of `h` and `w`, but rather adds the summation operation
    to a graph of computations to be done later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*不*计算`h`和`w`的总和，而是将求和操作添加到稍后要执行的计算图中。'
- en: 'Next, the `Session` object acts as an interface to the external TensorFlow
    computation mechanism, and allows us to run parts of the computation graph we
    have already defined. The line:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`Session`对象充当外部TensorFlow计算机制的接口，并允许我们运行已经定义的计算图的部分。代码行：
- en: '[PRE17]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: actually computes `hw` (as the sum of `h` and `w`, the way it was defined previously),
    following which the printing of `ans` displays the expected “Hello World!” message.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上计算`hw`（作为先前定义的`h`和`w`的总和），随后打印`ans`显示预期的“Hello World!”消息。
- en: This completes the first TensorFlow example. Next, we dive right in with a simple
    machine learning example, which already shows a great deal of the promise of the
    TensorFlow framework.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了第一个TensorFlow示例。接下来，我们将立即进行一个简单的机器学习示例，这个示例已经展示了TensorFlow框架的许多潜力。
- en: MNIST
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST
- en: The *MNIST* (Mixed National Institute of Standards and Technology) handwritten
    digits dataset is one of the most researched datasets in image processing and
    machine learning, and has played an important role in the development of artificial
    neural networks (now generally referred to as *deep learning*).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*MNIST*（混合国家标准技术研究所）手写数字数据集是图像处理和机器学习中最研究的数据集之一，并在人工神经网络（现在通常称为*深度学习*）的发展中发挥了重要作用。'
- en: As such, it is fitting that our first machine learning example should be dedicated
    to the classification of handwritten digits ([Figure 2-1](#fig0201) shows a random
    sample from the dataset). At this point, in the interest of keeping it simple,
    we will apply a very simple classifier. This simple model will suffice to classify
    approximately 92% of the test set correctly—the best models currently available
    reach over 99.75% correct classification, but we have a few more chapters to go
    until we get there! Later in the book, we will revisit this data and use more
    sophisticated methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第一个机器学习示例应该致力于手写数字的分类（[图2-1](#fig0201)显示了数据集的随机样本）。在这一点上，为了保持简单，我们将应用一个非常简单的分类器。这个简单模型足以正确分类测试集的大约92%——目前可用的最佳模型可以达到99.75%以上的正确分类，但在我们达到那里之前还有几章要学习！在本书的后面，我们将重新访问这些数据并使用更复杂的方法。
- en: '![MNIST100](assets/letf_0201.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST100](assets/letf_0201.png)'
- en: Figure 2-1\. 100 random MNIST images
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1. 100个随机MNIST图像
- en: Softmax Regression
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax回归
- en: In this example we will use a simple classifier called *softmax regression*.
    We will not go into the mathematical formulation of the model in too much detail
    (there are plenty of good resources where you can find this information, and we
    strongly suggest that you do so, if you have never seen this before). Rather,
    we will try to provide some intuition into the way the model is able to solve
    the digit recognition problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用一个称为*softmax回归*的简单分类器。我们不会详细介绍模型的数学公式（有很多好的资源可以找到这些信息，如果您以前从未见过这些信息，我们强烈建议您这样做）。相反，我们将尝试提供一些关于模型如何解决数字识别问题的直觉。
- en: Put simply, the softmax regression model will figure out, for each pixel in
    the image, which digits tend to have high (or low) values in that location. For
    instance, the center of the image will tend to be white for zeros, but black for
    sixes. Thus, a black pixel in the center of an image will be evidence against
    the image containing a zero, and in favor of it containing a six.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，softmax回归模型将找出图像中每个像素的数字在该位置具有高（或低）值的趋势。例如，图像中心对于零来说往往是白色的，但对于六来说是黑色的。因此，图像中心的黑色像素将是反对图像包含零的证据，并支持它包含六的证据。
- en: Learning in this model consists of finding weights that tell us how to accumulate
    evidence for the existence of each of the digits. With softmax regression, we
    will not use the spatial information in the pixel layout in the image. Later on,
    when we discuss convolutional neural networks, we will see that utilizing spatial
    information is one of the key elements in making great image-processing and object-recognition
    models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中的学习包括找到告诉我们如何累积每个数字的存在证据的权重。使用softmax回归，我们将不使用图像中像素布局中的空间信息。稍后，当我们讨论卷积神经网络时，我们将看到利用空间信息是制作出色的图像处理和对象识别模型的关键元素之一。
- en: Since we are not going to use the spatial information at this point, we will
    unroll our image pixels as a single long vector denoted *x* ([Figure 2-2](#fig0202)).
    Then
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这一点上不打算使用空间信息，我们将我们的图像像素展开为一个长向量表示为*x*（[Figure 2-2](#fig0202)）。然后
- en: '*xw*⁰ = ∑*x*[*i*] <math alttext="w Subscript i Superscript 0"><msubsup><mi>w</mi>
    <mi>i</mi> <mn>0</mn></msubsup></math>'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*xw*⁰ = ∑*x*[*i*] '
- en: will be the evidence for the image containing the digit 0 (and in the same way
    we will have <math alttext="w Superscript d"><msup><mi>w</mi> <mi>d</mi></msup></math>
     weight vectors for each one of the other digits, <math><mrow><mi>d</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mn>9</mn></mrow></math>
    ).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将是包含数字0的图像的证据（同样地，我们将为其他每个数字有<math alttext="w Superscript d"><msup><mi>w</mi>
    <mi>d</mi></msup></math> 个权重向量，<math><mrow><mi>d</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mn>9</mn></mrow></math>）。
- en: '![MNIST100](assets/letf_0202.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST100](assets/letf_0202.png)'
- en: Figure 2-2\. MNIST image pixels unrolled to vectors and stacked as columns (sorted
    by digit from left to right). While the loss of spatial information doesn’t allow
    us to recognize the digits, the block structure evident in this figure is what
    allows the softmax model to classify images. Essentially, all zeros (leftmost
    block) share a similar pixel structure, as do all ones (second block from the
    left), etc.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-2。MNIST图像像素展开为向量并按列堆叠（从左到右按数字排序）。虽然空间信息的丢失使我们无法识别数字，但在这个图中明显的块结构是softmax模型能够对图像进行分类的原因。基本上，所有的零（最左边的块）共享相似的像素结构，所有的一（从左边第二个块）也是如此，等等。
- en: All this means is that we sum up the pixel values, each multiplied by a weight,
    which we think of as the importance of this pixel in the overall evidence for
    the digit zero being in the image.^([2](ch02.html#idm139707905335440))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将像素值相加，每个值乘以一个权重，我们认为这个像素在图像中数字零的整体证据中的重要性。
- en: For instance, *w*⁰[38] will be a large positive number if the 38th pixel having
    a high intensity points strongly to the digit being a zero, a strong negative
    number if high-intensity values in this position occur mostly in other digits,
    and zero if the intensity value of the 38th pixel tells us nothing about whether
    or not this digit is a zero.^([3](ch02.html#idm139707905332608))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*w*⁰[38]如果第38个像素具有高强度，则将是一个较大的正数，指向该数字为零，如果在这个位置的高强度值主要出现在其他数字中，则将是一个较大的负数，如果第38个像素的强度值告诉我们这个数字是否为零，则为零。^([3](ch02.html#idm139707905332608))
- en: Performing this calculation at once for all digits (computing the evidence for
    each of the digits appearing in the image) can be represented by a single matrix
    operation. If we place the weights for each of the digits in the columns of a
    matrix *W*, then the length-10 vector with the evidence for each of the digits
    is
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一次为所有数字执行此计算（计算出现在图像中的每个数字的证据）可以通过单个矩阵操作表示。如果我们将每个数字的权重放在矩阵*W*的列中，那么每个数字的证据的长度为10的向量是
- en: '[*xw*⁰, ···, *xw*⁹] = *xW*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[*xw*⁰, ···, *xw*⁹] = *xW*'
- en: 'The purpose of learning a classifier is almost always to evaluate new examples.
    In this case, this means that we would like to be able to tell what digit is written
    in a new image we have not seen in our training data. In order to do this, we
    start by summing up the evidence for each of the 10 possible digits (i.e., computing
    *xW*). The final assignment will be the digit that “wins” by accumulating the
    most evidence:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的学习目的几乎总是为了评估新的示例。在这种情况下，这意味着我们希望能够判断我们在训练数据中没有见过的新图像中写的是什么数字。为了做到这一点，我们首先对10个可能数字中的每一个的证据进行求和（即计算*xW*）。最终的分配将是“赢得”最多证据的数字：
- en: '*digit* = *argmax*(*xW*)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*数字* = *argmax*(*xW*)'
- en: We start by presenting the code for this example in its entirety ([Example 2-2](#ex0202)),
    then walk through it line by line and go over the details. You may find that there
    are many novel elements or that some pieces of the puzzle are missing at this
    stage, but our advice is that you go with it for now. Everything will become clear
    in due course.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先完整地呈现这个示例的代码（[Example 2-2](#ex0202)），然后逐行走过它并详细讨论。您可能会发现在这个阶段有许多新颖的元素，或者一些拼图的部分缺失，但我们的建议是暂时接受它。一切将在适当的时候变得清晰。
- en: Example 2-2\. Classifying MNIST handwritten digits with softmax regression
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-2。使用softmax回归对MNIST手写数字进行分类
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you run the code on your machine, you should get output like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在您的机器上运行代码，您应该会得到如下输出：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s all it takes! If you have put similar models together before using other
    platforms, you might appreciate the simplicity and readability. However, these
    are just side bonuses, with the efficiency and flexibility gained from the computation
    graph model of TensorFlow being what we are really interested in.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！如果您之前在其他平台上组合过类似的模型，您可能会欣赏到其简单性和可读性。然而，这些只是附加的好处，我们真正感兴趣的是从TensorFlow的计算图模型中获得的效率和灵活性。
- en: The exact accuracy value you get will be just under 92%. If you run the program
    once more, you will get another value. This sort of stochasticity is very common
    in machine learning code, and you have probably seen similar results before. In
    this case, the source is the changing order in which the handwritten digits are
    presented to the model during learning. As a result, the learned parameters following
    training are slightly different from run to run.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的准确度值将略低于92%。如果再运行程序一次，你会得到另一个值。这种随机性在机器学习代码中非常常见，你可能以前也见过类似的结果。在这种情况下，源是手写数字在学习过程中呈现给模型的顺序发生了变化。因此，训练后学到的参数在每次运行时略有不同。
- en: 'Running the same program five times might therefore produce this result:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，运行相同的程序五次可能会产生这样的结果：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now briefly go over the code for this example and see what is new from
    the previous “hello world” example. We’ll break it down line by line:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要地查看这个示例的代码，并看看与之前的“hello world”示例有什么新的地方。我们将逐行分解它：
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first new element in this example is that we use external data! Rather
    than downloading the MNIST dataset (freely available at [*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/))
    and loading it into our program, we use a built-in utility for retrieving the
    dataset on the fly. Such utilities exist for most popular datasets, and when dealing
    with small ones (in this case only a few MB), it makes a lot of sense to do it
    this way. The second import loads the utility we will later use both to automatically
    download the data for us, and to manage and partition it as needed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中的第一个新元素是我们使用外部数据！我们不再下载MNIST数据集（在[*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/)免费提供），然后加载到我们的程序中，而是使用内置工具来动态检索数据集。对于大多数流行的数据集，存在这样的工具，当处理小数据集时（在这种情况下只有几MB），这种方式非常合理。第二个导入加载了我们稍后将使用的工具，用于自动下载数据，并根据需要管理和分区数据：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here we define some constants that we use in our program—these will each be
    explained in the context in which they are first used:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一些在程序中使用的常量，它们将在首次使用时的上下文中进行解释：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `read_data_sets()` method of the MNIST reading utility downloads the dataset
    and saves it locally, setting the stage for further use later in the program.
    The first argument, `DATA_DIR`, is the location we wish the data to be saved to
    locally. We set this to `'/tmp/data'`, but any other location would be just as
    good. The second argument tells the utility how we want the data to be labeled;
    we will not go into this right now.^([4](ch02.html#idm139707904192528))
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST阅读工具的`read_data_sets()`方法会下载数据集并将其保存在本地，为程序后续使用做好准备。第一个参数`DATA_DIR`是我们希望数据保存在本地的位置。我们将其设置为`'/tmp/data'`，但任何其他位置也同样适用。第二个参数告诉工具我们希望数据如何标记；我们现在不会深入讨论这个问题。
- en: 'Note that this is what prints the first four lines of the output, indicating
    the data was obtained correctly. Now we are finally ready to set up our model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这就是输出的前四行，表明数据已经正确获取。现在我们终于准备好设置我们的模型了：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the previous example we saw the TensorFlow constant element—this is now
    complemented by the `placeholder` and `Variable` elements. For now, it is enough
    to know that a variable is an element manipulated by the computation, while a
    placeholder has to be supplied when triggering it. The image itself (`x`) is a
    placeholder, because it will be supplied by us when running the computation graph.
    The size [`None, 784`] means that each image is of size 784 (28×28 pixels unrolled
    into a single vector), and `None` is an indicator that we are not currently specifying
    how many of these images we will use at once:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们看到了TensorFlow的常量元素，现在它被`placeholder`和`Variable`元素补充。现在，知道变量是计算中操作的元素，而占位符在触发时必须提供。图像本身（`x`）是一个占位符，因为当运行计算图时，我们将提供它。大小[`None,
    784`]表示每个图像的大小为784（28×28像素展开成一个单独的向量），`None`表示我们目前没有指定一次使用多少个这样的图像：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the next chapter these concepts will be dealt with in much more depth.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，这些概念将会更深入地讨论。
- en: 'A key concept in a large class of machine learning tasks is that we would like
    to learn a function from data examples (in our case, digit images) to their known
    labels (the identity of the digit in the image). This setting is called *supervised
    learning*. In most supervised learning models, we attempt to learn a model such
    that the true labels and the predicted labels are close in some sense. Here, `y_true`
    and `y_pred` are the elements representing the true and predicted labels, respectively:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在大类机器学习任务中的一个关键概念是，我们希望从数据示例（在我们的情况下是数字图像）到它们已知标签（图像中数字的身份）的函数。这种设置被称为*监督学习*。在大多数监督学习模型中，我们尝试学习一个模型，使得真实标签和预测标签在某种意义上接近。在这里，`y_true`和`y_pred`分别表示真实标签和预测标签的元素：
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The measure of similarity we choose for this model is what is known as *cross
    entropy*—a natural choice when the model outputs class probabilities. This element
    is often referred to as the *loss function*:^([5](ch02.html#idm139707904042960))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择用于此模型的相似度度量是所谓的*交叉熵*，这是当模型输出类别概率时的自然选择。这个元素通常被称为*损失函数*：
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The final piece of the model is how we are going to train it (i.e., how we are
    going to minimize the loss function). A very common approach is to use gradient
    descent optimization. Here, `0.5` is the learning rate, controlling how fast our
    gradient descent optimizer shifts model weights to reduce overall loss.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后一部分是我们将如何训练它（即我们将如何最小化损失函数）。一个非常常见的方法是使用梯度下降优化。这里，`0.5`是学习率，控制我们的梯度下降优化器如何快速地调整模型权重以减少总体损失。
- en: We will discuss optimizers and how they fit into the computation graph later
    on in the book.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书后面讨论优化器以及它们如何适应计算图。
- en: Once we have defined our model, we want to define the evaluation procedure we
    will use in order to test the accuracy of the model. In this case, we are interested
    in the fraction of test examples that are correctly classified:^([6](ch02.html#idm139707903782672))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了我们的模型，我们希望定义我们将使用的评估过程，以便测试模型的准确性。在这种情况下，我们对正确分类的测试示例的比例感兴趣：^([6](ch02.html#idm139707903782672))
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As with the “hello world” example, in order to make use of the computation
    graph we defined, we must create a session. The rest happens within the session:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与“hello world”示例一样，为了利用我们定义的计算图，我们必须创建一个会话。其余操作都在会话中进行：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, we must initialize all variables:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须初始化所有变量：
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This carries some specific implications in the realm of machine learning and
    optimization, which we will discuss further when we use models for which initialization
    is an important issue
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这在机器学习和优化领域具有一些特定的含义，当我们使用初始化是一个重要问题的模型时，我们将进一步讨论这些含义
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The actual training of the model, in the gradient descent approach, consists
    of taking many steps in “the right direction.” The number of steps we will make,
    `NUM_STEPS`, was set to 1,000 in this case. There are more sophisticated ways
    of deciding when to stop, but more about that later! In each step we ask our data
    manager for a bunch of examples with their labels and present them to the learner.
    The `MINIBATCH_SIZE` constant controls the number of examples to use for each
    step.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降方法中，模型的实际训练包括在“正确的方向”上进行多次步骤。在这种情况下，我们将进行的步数`NUM_STEPS`设置为1,000步。有更复杂的方法来决定何时停止，但稍后再讨论！在每一步中，我们向数据管理器请求一组带有标签的示例，并将它们呈现给学习者。`MINIBATCH_SIZE`常数控制每一步使用的示例数量。
- en: Finally, we use the `feed_dict` argument of `sess.run` for the first time. Recall
    that we defined placeholder elements when constructing the model. Now, each time
    we want to run a computation that will include these elements, we must supply
    a value for them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们第一次使用`sess.run`的`feed_dict`参数。回想一下，在构建模型时我们定义了占位符元素。现在，每当我们想要运行包含这些元素的计算时，我们必须为它们提供一个值。
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In order to evaluate the model we have just finished learning, we run the accuracy
    computing operation defined earlier (recall the accuracy was defined as the fraction
    of images that are correctly labeled). In this procedure, we feed a separate group
    of test images, which were never seen by the model during training:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们刚刚学习的模型，我们运行之前定义的准确性计算操作（回想一下，准确性被定义为正确标记的图像的比例）。在这个过程中，我们提供一组从未被模型在训练过程中看到的测试图像：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Lastly, we print out the results as percent values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果打印为百分比值。
- en: '[Figure 2-3](#fig0203) shows a graph representation of our model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-3](#fig0203)显示了我们模型的图形表示。'
- en: '![](assets/letf_0203.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0203.png)'
- en: Figure 2-3\. A graph representation of the model. Rectangular elements are Variables,
    and circles are placeholders. The top-left frame represents the label prediction
    part, and the bottom-right frame the evaluation. Here, *b* is a bias term that
    could be added to the mode.
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。模型的图形表示。矩形元素是变量，圆圈是占位符。左上角框表示标签预测部分，右下角框表示评估。这里，*b*是一个偏差项，可以添加到模型中。
- en: Model evaluation and memory errors
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估和内存错误
- en: 'When using TensorFlow, like any other system, it is important to be aware of
    the resources being used, and make sure not to exceed the capacity of the system.
    One possible pitfall is in the evaluation of models—testing their performance
    on a test set. In this example we evaluate the accuracy of the models by feeding
    all the test examples in one go:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用TensorFlow时，与任何其他系统一样，重要的是要注意正在使用的资源，并确保不超出系统的容量。在评估模型时可能会出现的一个潜在问题是在测试集上测试模型的性能。在这个示例中，我们通过一次性提供所有测试示例来评估模型的准确性：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If all the test examples (here, `data.test.images`) are not able to fit into
    the memory in the system you are using, you will get a memory error at this point.
    This is likely to be the case, for instance, if you are running this example on
    a typical low-end GPU.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有的测试示例（这里是`data.test.images`）无法在您使用的系统内存中容纳，那么在这一点上会出现内存错误。例如，如果您在典型的低端GPU上运行此示例，很可能会出现这种情况。
- en: The easy way around this (getting a machine with more memory is a temporary
    fix, since there will always be larger datasets) is to split the test procedure
    into batches, much as we did during training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的简单方法（获得更多内存的机器是一个临时解决方案，因为总会有更大的数据集）是将测试过程分成批处理，就像我们在训练过程中所做的那样。
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations! By now you have installed TensorFlow and taken it for a spin
    with two basic examples. You have seen some of the fundamental building blocks
    that will be used throughout the book, and have hopefully begun to get a feel
    for TensorFlow.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！到目前为止，您已经安装了TensorFlow并使用了两个基本示例。您已经看到了本书中将使用的一些基本构建模块，并且希望已经开始对TensorFlow有所了解。
- en: Next, we take a look under the hood and explore the computation graph model
    used by TensorFlow.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入了解TensorFlow使用的计算图模型。
- en: ^([1](ch02.html#idm139707904506064-marker)) We refer the reader to the official
    [TensorFlow install guide](https://www.tensorflow.org/install/) for further details,
    and especially the ever-changing details of GPU installations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#idm139707904506064-marker)) 我们建议读者参考官方[TensorFlow安装指南](https://www.tensorflow.org/install/)以获取更多详细信息，特别是有关GPU安装的不断变化的细节。
- en: ^([2](ch02.html#idm139707905335440-marker)) It is common to add a “bias term,”
    which is equivalent to stating which digits we believe an image to be before seeing
    the pixel values. If you have seen this before, then try adding it to the model
    and check how it affects the results.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#idm139707905335440-marker)) 添加“偏差项”是常见的，这相当于在看到像素值之前我们相信图像是哪些数字。如果您之前见过这个，那么尝试将其添加到模型中并查看它如何影响结果。
- en: ^([3](ch02.html#idm139707905332608-marker)) If you are familiar with softmax
    regression, you probably realize this is a simplification of the way it works,
    especially when pixel values are as correlated as with digit images.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#idm139707905332608-marker)) 如果您熟悉softmax回归，您可能意识到这是它工作方式的简化，特别是当像素值与数字图像一样相关时。
- en: ^([4](ch02.html#idm139707904192528-marker)) Here and throughout, before running
    the example code, make sure `DATA_DIR` fits the operating system you are using.
    On Windows, for instance, you would probably use something like *c:\tmp\data*
    instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#idm139707904192528-marker)) 在整个过程中，在运行示例代码之前，请确保`DATA_DIR`适合您正在使用的操作系统。例如，在Windows上，您可能会使用类似*c:\tmp\data*的路径。
- en: ^([5](ch02.html#idm139707904042960-marker)) As of TensorFlow 1.0 this is also
    contained in `tf.losses.softmax_cross_entropy`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#idm139707904042960-marker)) 从TensorFlow 1.0开始，这也包含在`tf.losses.softmax_cross_entropy`中。
- en: ^([6](ch02.html#idm139707903782672-marker)) As of TensorFlow 1.0 this is also
    contained in `tf.metrics.accuracy`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#idm139707903782672-marker)) 从TensorFlow 1.0开始，这也包含在`tf.metrics.accuracy`中。
