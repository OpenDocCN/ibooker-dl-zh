- en: 5 Managing data with GitHub Copilot and Copilot Chat
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 使用GitHub Copilot和Copilot Chat管理数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Persisting data into a relational database
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据持久化到关系型数据库
- en: Streaming data using Apache Kafka
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Kafka进行数据流
- en: Incorporating event-driven principles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成事件驱动原则
- en: Analyzing data to monitor the location using Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark分析数据以监控位置
- en: 'The last chapter laid the foundation for our information technology asset management
    (ITAM) system. However, this application will not fulfill our requirements without
    data. Data is the lifeblood of every application. That is what this chapter is
    all about: the various ways we can use generative AI to create data, stream data,
    transform data, react to data, and learn from data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章为我们信息技术资产管理（ITAM）系统奠定了基础。然而，这个应用程序没有数据将无法满足我们的需求。数据是每个应用程序的生命线。这正是本章的主题：我们可以使用生成式AI以各种方式创建数据、流数据、转换数据、对数据做出反应，并从数据中学习。
- en: Perceptive individuals may have noticed in the last chapter that our data access
    pattern would not have worked as it was incomplete. The opening section of this
    chapter will address this. After that, we will set up our database, fix the classes
    that access this data, and load some sample data to use in the rest of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，敏锐的人可能会注意到我们的数据访问模式因为不完整而无法工作。本章的开头部分将解决这个问题。之后，我们将设置我们的数据库，修复访问这些数据的类，并加载一些样本数据以供本章其余部分使用。
- en: 5.1 Amassing our dataset
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 收集我们的数据集
- en: 'Our first task will be to construct a substantial corpus of data to assist
    our experimentation in the remainder of the chapter. First we will use GitHub
    Copilot to generate 1,000 rows of asset information. We will soon find, however,
    that this may not be the tool best suited to this task. A key driver behind using
    these tools is the idea of discovery: testing their boundaries, pushing against
    them, and occasionally pushing back. But the journey is often where the joy is
    found. Once we have found this edge, we will be introduced to a new, previously
    unseen tool: GitHub Copilot Chat. Finally, when we have created our list of assets,
    we will add location information for those assets, again using GitHub Copilot
    Chat.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一项任务将是构建一个大量的数据集，以帮助我们在本章剩余部分的实验。首先，我们将使用GitHub Copilot生成1,000行资产信息。然而，我们很快就会发现问题可能不是最适合这个任务的工具。使用这些工具的一个关键驱动因素是发现的想法：测试它们的边界，对抗它们，偶尔甚至反击。但旅程往往就是乐趣所在。一旦我们找到了这个边缘，我们将被介绍到一个新的、以前未见过的工具：GitHub
    Copilot Chat。最后，当我们创建我们的资产列表后，我们将为这些资产添加位置信息，再次使用GitHub Copilot Chat。
- en: 'We need to get our database running before building our initial dataset. Docker
    makes this task trivial, allowing us to quickly spin up an empty PostgreSQL (or
    other RDBMS/NoSQL server) with minimal effort. Have you forgotten the command
    to do this? No worries—we can ask Copilot. Open a new file called data/initial_data_load.sql
    and enter the following prompt at the top of your newly minted SQL file:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在构建初始数据集之前让我们的数据库运行起来。Docker使得这项任务变得简单，只需最小努力，我们就能快速启动一个空的PostgreSQL（或其他RDBMS/NoSQL服务器）。你忘记了这个命令吗？不用担心——我们可以询问Copilot。打开一个名为data/initial_data_load.sql的新文件，并在你新创建的SQL文件顶部输入以下提示：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Copilot will slowly reveal the Docker command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot将逐步揭示Docker命令：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you run this command at your terminal or command line, we can build out
    our dataset. You should be able to connect to the locally running database. Notice
    that a database called `itam_db` is running in it. However, this database has
    no schema, tables, or data. Let’s first set up a new schema.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在终端或命令行运行此命令，我们就可以构建我们的数据集。你应该能够连接到本地运行的数据库。注意，一个名为`itam_db`的数据库正在其中运行。然而，这个数据库没有任何模式、表或数据。让我们首先设置一个新的模式。
- en: 'In our initial_data_load.sql, we will add a prompt to have Copilot draft the
    schema creation command. The following prompt (and response from Copilot) will
    allow you to create a new schema called `itam` if executed from within your database
    client application (e.g., DataGrip, SQuirreL, pdAdmin, or even using the Docker
    `exec` command `docker exec -i itam_db psql -U postgres -c "create schema itam"`):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的initial_data_load.sql中，我们将添加一个提示让Copilot起草创建模式的命令。以下提示（以及Copilot的响应）将允许你创建一个名为`itam`的新模式，如果从你的数据库客户端应用程序（例如DataGrip、SQuirreL、pdAdmin或甚至使用Docker的`exec`命令`docker
    exec -i itam_db psql -U postgres -c "create schema itam"`）执行：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we should add a user for use in our application. This user will be able
    to perform CRUD (create, read, update, delete) operations on our data but will
    not be able to affect the structure of the database tables or procedures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该添加一个用户用于我们的应用程序。此用户将能够对我们的数据进行CRUD（创建、读取、更新、删除）操作，但无法影响数据库表或过程的结构。
- en: Note The lines that start with double dashes (`--`) are comments in SQL. Commenting
    out these lines is optional from Copilot’s perspective, as it will generate solutions
    without the comments; it makes it easier to copy and paste the code directly into
    our database tool of choice.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：以双横线（`--`）开头的行是SQL中的注释。从Copilot的角度来看，取消这些行的注释是可选的，因为它将生成不带注释的解决方案；这使它更容易直接复制和粘贴到我们选择的数据库工具中。
- en: While we are at it, we will also add an administrative account to perform the
    operations that our read-write users cannot, such as creating or dropping tables.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们还将添加一个管理账户来执行我们的读写用户无法执行的操作，例如创建或删除表。
- en: Listing 5.1 Prompt to create new users
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 创建新用户的提示
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next we will grant ownership of this schema to the `itam_admin` account. Transferring
    this ownership will ensure that only this account can change the table structure
    (the data definition):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将此模式的所有权授予`itam_admin`账户。转让此所有权将确保只有此账户可以更改表结构（数据定义）。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With the setup, account creation, and ownership of the system out of the way,
    we can start to focus on the data. We will begin by adding the reference data,
    which supports the assets: the depreciation strategies. This data is more static
    in nature; it changes less frequently, if at all. Let’s define and store these
    strategies.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置、账户创建和系统所有权问题解决后，我们可以开始关注数据。我们将从添加支持资产的数据参考开始，即折旧策略。这种数据在性质上更静态；如果有的话，变化频率较低。让我们定义并存储这些策略。
- en: Listing 5.2 Prompt to create the `depreciation_strategy` table
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 创建`depreciation_strategy`表的提示
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will use a sequence as this table’s primary key. Although this is not strictly
    necessary for a table that will not be very large and that has known values we
    can and will enter manually, adding this sequence will allow us to work with Copilot
    more and have it make some suggestions. Moreover, it is amusing to ask Copilot
    questions and have Copilot answer in a text file.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个序列作为此表的主键。尽管对于不会很大且具有已知值可以手动输入的表来说，这并非绝对必要，但添加这个序列将使我们能够更多地与Copilot合作，并让它提出一些建议。此外，向Copilot提问并让它以文本文件的形式回答问题很有趣。
- en: Listing 5.3 Prompt to create a sequence to use as primary key
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 创建用作主键的序列的提示
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Naturally, with the sequence in our proverbial hand, we need to know how to
    associate the sequence with the primary key column of the `depreciation_strategy`
    table. Luckily, Copilot has the answer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，在我们传说中的手中有了序列，我们需要知道如何将序列与`depreciation_strategy`表的主键列关联起来。幸运的是，Copilot有答案。
- en: Listing 5.4 Asking Copilot how to associate the sequence with the primary key
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 询问Copilot如何将序列与主键关联
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we complete this table by inserting the following static entries into
    it. We will only use two depreciation strategies for now: straight-line and double
    declining balance.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将以下静态条目插入表中来完成此表。目前我们只使用两种折旧策略：直线法和双倍余额递减法。
- en: Listing 5.5 Adding the static entries to the `depreciation_strategy` table
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 将静态条目添加到`depreciation_strategy`表
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next we will move on to the `funding_details` table. This information tells
    us how we financed our equipment, the resale value, and instructions for what
    should be done with an asset once its useful life is over. The sequence of steps
    will be identical to what we did for the depreciation strategies, with the exception
    that we will not add static entries, as this data is directly related to an individual
    asset. We will define the table, create the sequence, and apply that sequence
    to the table, for which it functions as the primary key.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续处理`funding_details`表。这些信息告诉我们如何为我们的设备融资，其再销售价值，以及资产使用寿命结束后应如何处理资产的说明。步骤序列将与我们对折旧策略所做的工作相同，唯一的区别是我们不会添加静态条目，因为此数据直接与单个资产相关。我们将定义表，创建序列，并将该序列应用于表，其中它作为主键。
- en: Listing 5.6 Complete code listing for the `funding_details` table
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 `funding_details`表的完整代码列表
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The final information that we will define and generate is the assets themselves.
    This listing, too, is redundant but included for completeness. Finally, we create
    the table, make the sequence, and use it as the primary key.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义和生成的最终信息是资产本身。这个列表也是多余的，但为了完整性而包含在内。最后，我们创建表，创建序列，并将其用作主键。
- en: Listing 5.7 Complete code listing for the `assets` table
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 `assets` 表的完整代码列表
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the tables defined and created, we will now focus on creating the data.
    In our text file, we instruct Copilot with parameters for the dataset we are looking
    for. Copilot will likely attempt to assist you in outlining the attributes surrounding
    your new dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和创建表之后，我们现在将专注于创建数据。在我们的文本文件中，我们向 Copilot 指定我们正在寻找的数据集的参数。Copilot 可能会尝试协助您概述围绕您的新数据集的属性。
- en: Listing 5.8 Creating a dataset for the assets table
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.8 为资产表创建数据集
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The solution that Copilot provides is novel. It builds a large series using
    a Postgres built-in function, meaning this solution would not be portable. However,
    given that this is the database we will use, it is an appropriate enough solution.
    The resulting dataset is refined. We would have gotten better results if we had
    used Python and asked for Copilot’s assistance in coding a script to generate
    a file to load into Postgres. However, given that this dataset is only for playing
    with the application, we do not need to be overly concerned with the data quality
    for now—although in the real world, data quality is everything.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 提供的解决方案是新颖的。它使用 Postgres 内置函数构建一个大型序列，这意味着这个解决方案是不可移植的。然而，鉴于这是我们将会使用的数据库，这是一个足够合适的解决方案。生成的数据集已经经过优化。如果我们使用
    Python 并请求 Copilot 帮助编写一个脚本来生成一个可以加载到 Postgres 中的文件，我们可能会得到更好的结果。然而，鉴于这个数据集只是为了与应用程序一起使用，我们目前不需要过分关注数据质量——尽管在现实世界中，数据质量是至关重要的。
- en: 'Listing 5.9 Copilot’s response: an `insert` statement built off of a series'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.9 Copilot 的响应：基于序列构建的 `insert` 语句
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If we switch back to ChatGPT for a minute, we can get a second opinion about
    how to create such a dataset. ChatGPT suggests the Python library `faker`. The
    `faker` package is used to generate fake data, such as common English first names.
    `numpy` is used to generate the random float values for cost, useful life, and
    salvage value. `pandas` is used to manage the data in a `DataFrame` (the table).
    Additionally, we can save the `DataFrame` to a CSV file using the method `df.to_csv('assets.csv',
    index=False)`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们切换回 ChatGPT 一分钟，我们可以得到关于如何创建此类数据集的第二意见。ChatGPT 建议使用 Python 库 `faker`。`faker`
    包用于生成假数据，例如常见的英语名字。`numpy` 用于生成成本、使用寿命和残值等随机浮点值。`pandas` 用于在 `DataFrame`（表格）中管理数据。此外，我们可以使用
    `df.to_csv('assets.csv', index=False)` 方法将 `DataFrame` 保存到 CSV 文件中。
- en: Listing 5.10 ChatGPT suggests `Faker` to generate the fake dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 ChatGPT 建议使用 `Faker` 生成假数据集
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For each of these assets, we will need funding details as well: how they were
    financed (purchased, in this case) and the depreciation details. Unsurprisingly,
    we get a similar solution from Copilot: generate a series of entries using a prompt
    similar to the one we used for the assets. We will need to ensure that for each
    of the asset identifiers (1–1000), we have a corresponding funding details entry.
    Otherwise we risk getting null pointers when running this code.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些资产中的每一个，我们还需要资金细节：它们是如何融资的（在这个例子中是购买）以及折旧详情。不出所料，我们从 Copilot 得到了一个类似的解决方案：使用与用于资产的类似提示生成一系列条目。我们需要确保对于每个资产标识符（1–1000），我们都有一个相应的资金详情条目。否则，在运行此代码时我们可能会遇到空指针。
- en: Listing 5.11 Creating a dataset for the `funding_details` table
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 为 `funding_details` 表创建数据集
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With the dataset generated and stored in the database, we should be able to
    wire up the remainder of our application to store and display assets using the
    REST APIs. However, because we previously stripped out all the metadata for SQLAlchemy
    during our build phase (see chapter 4), we need a way to wire this metadata with
    our adapters differently.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集生成并存储在数据库中后，我们应该能够将我们应用程序的其余部分连接起来，使用 REST API 存储和显示资产。然而，由于我们在构建阶段移除了 SQLAlchemy
    的所有元数据（参见第 4 章），我们需要一种方法以不同的方式将此元数据与我们的适配器连接起来。
- en: 'At this point we have reached the edge of Copilot’s capabilities. We are perplexed
    by what comes next and how we can solve our most recent dilemma. Tempting as it
    is, we cannot give up and go home. Therefore, it is time to introduce the most
    recent addition to the Copilot product suite: Copilot Chat. Copilot Chat is a
    GPT-4 model embedded in an IDE (currently supported only by Visual Studio Code).
    We will open the chat dialog and ask how to keep our business model clean while
    still using SQLAlchemy’s object-relational model (ORM) features. Figure 5.1 shows
    the response from ChatGPT.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经触及了Copilot能力的边缘。我们对接下来会发生什么以及如何解决我们最新的困境感到困惑。尽管很诱人，但我们不能放弃并回家。因此，是时候介绍Copilot产品套件中最新的补充：Copilot
    Chat。Copilot Chat是一个嵌入在IDE中的GPT-4模型（目前仅由Visual Studio Code支持）。我们将打开聊天对话框，询问如何在仍然使用SQLAlchemy的对象关系模型（ORM）功能的同时保持我们的商业模式清洁。图5.1显示了ChatGPT的响应。
- en: Copilot Chat suggests that we create a separate data access layer. This approach
    maps nicely onto the ports and adapters approach we have used thus far. In addition,
    Copilot Chat recommends modeling these classes similarly to the domain classes
    but including the metadata required for ORM functionality to work correctly. The
    resulting code is shown in listing 5.12.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Crocker2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 GitHub Copilot Chat’s solution for how to solve our most recent quandary
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 ORM support outside of the domain classes
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.12 领域类之外的 ORM 支持
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that the external model classes have been created, we must map these ORM
    instances to our domain model before returning them to the system’s core. This
    may seem like over-engineered code for such a simple application, but it gives
    us great flexibility in how our domain model can operate. For example, our model
    can perform complex operations beyond just CRUD. We would be limited to these
    operations if we kept our domain model identity to the model used in the data
    access layer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经创建了外部模型类，在将它们返回到系统核心之前，我们必须将这些 ORM 实例映射到我们的领域模型。这可能看起来像是针对这样一个简单应用过度设计的代码，但它给了我们在领域模型如何操作方面极大的灵活性。例如，我们的模型可以执行超出
    CRUD 的复杂操作。如果我们保持领域模型身份与数据访问层中使用的模型一致，我们将仅限于这些操作。
- en: 'Next we will use Copilot and Copilot Chat to explore incorporating event-driven
    ideas into our application. Event-driven concepts will allow us to track our IT
    assets in real time: their location, status, and market value, for example.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Monitoring our assets in real time with Kafka
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 使用 Kafka 实时监控我们的资产
- en: We will monitor our assets in real time to motivate our exploration of using
    generative AI in conjunction with event-driven architecture. We will take it as
    a given that some system external to the ISAM system fires events as our assets
    move from one location to another.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实时监控我们的资产，以激励我们探索将生成式人工智能与事件驱动架构相结合的使用。我们将认为，当我们的资产从一个位置移动到另一个位置时，某些外部ISAM系统的系统会触发事件，这一点是理所当然的。
- en: To delve into ITAM events, we will need to configure a few additional services.
    In this case, we will use Apache Kafka. Apache Kafka is a distributed streaming
    platform that is used to build real-time data pipelines and streaming apps. It’s
    designed to handle data streams from multiple sources and deliver them to multiple
    consumers, effectively acting as a middleman for our real-time data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: To start, we will ask Copilot Chat how to run Kafka locally using Docker. Apache
    Kafka has an undeserved reputation for being difficult to install and configure,
    and running in Docker will allow us to side-step this controversy. Using Copilot
    Chat, we can produce a Docker Compose file. However, as is often the case, the
    versions are very old, to the point of not supporting some hardware. Listing 5.13
    is an updated listing from Confluent’s (the company that offers commercial support
    for Kafka) official GitHub repository. Notice that the Docker Compose file’s contents
    include both Kafka and Zookeeper. Zookeeper is a distributed coordination service
    that Kafka uses to manage and coordinate the brokers in the cluster, at least
    for now. Future versions aim to remove dependency on Zookeeper.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将询问 Copilot Chat 如何使用 Docker 在本地运行 Kafka。Apache Kafka 有一个不应得的声誉，即难以安装和配置，而在
    Docker 中运行将使我们能够绕过这一争议。使用 Copilot Chat，我们可以生成一个 Docker Compose 文件。然而，正如通常情况一样，版本非常旧，以至于不支持某些硬件。列表
    5.13 是来自 Confluent（提供 Kafka 商业支持的公司的）官方 GitHub 仓库的更新列表。请注意，Docker Compose 文件的内容包括
    Kafka 和 Zookeeper。Zookeeper 是 Kafka 用于管理和协调集群中代理的分布式协调服务，至少目前是这样。未来的版本旨在去除对 Zookeeper
    的依赖。
- en: Listing 5.13 Docker Compose file to launch Kafka with Zookeeper
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.13 启动 Kafka 与 Zookeeper 的 Docker Compose 文件
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With our new Kafka instance running, we now need a consumer that will pull
    the updated locations off the topic `asset_location` (which we will create shortly)
    and update the inventory in the database. Again, we can ask Copilot Chat to provide
    us with a suggestion for how to use Python to subscribe to the topic:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的新 Kafka 实例运行，我们现在需要一个消费者，它会从主题 `asset_location`（我们很快就会创建）中拉取更新的位置，并更新数据库中的库存。再次，我们可以要求
    Copilot Chat 提供一个如何使用 Python 订阅主题的建议：
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| In our ITAM project, we would like a Python consumer for the asset_location
    Kafka topic. Once it receives a new updated location, it should update the domain
    model to reflect the new location. How should we do this? |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 在我们的 ITAM 项目中，我们希望有一个用于资产位置 Kafka 主题的 Python 消费者。一旦它接收到新的更新位置，它应该更新领域模型以反映新的位置。我们应该怎么做呢？
    |'
- en: The code that Copilot Chat generates creates a consumer, listens to the topic,
    and uses a reference to the `AssetManager` class to update the location of the
    affected asset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 生成的代码创建了一个消费者，监听主题，并使用 `AssetManager` 类的引用来更新受影响资产的地点。
- en: Listing 5.14 Copilot Chat code to monitor assets’ locations
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.14 Copilot Chat 监控资产位置的代码
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'However, should the adapter be aware of `AssetManager`? The `AssetManager`
    does not directly sit inside the domain model; it is a critical intermediary.
    This coupling level between `AssetManager` and the adapter could arguably violate
    the hexagon in the hexagonal architecture model. Let’s have Copilot Chat weigh
    in. We can ask it this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，适配器是否应该知道 `AssetManager`？`AssetManager` 并不直接位于领域模型内部；它是一个关键的中介。`AssetManager`
    和适配器之间的这种耦合程度可能会被认为违反了六边形架构模型中的六边形。让我们让 Copilot Chat 评价一下。我们可以这样问它：
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| Given that this project uses hexagonal architecture, is it a good idea to
    have an adapter aware of the AssetManager? |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 由于本项目使用六边形架构，让适配器知道 AssetManager 是否是一个好主意？ |'
- en: 'Looking at figure 5.2, it appears that Copilot Chat agrees that this would
    indeed be considered a breach of the contractual responsibility of an adapter.
    We could add a new port to our `AssetManager` class that would use the Kafka port.
    However, let’s see if Copilot Chat has any other suggestions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图 5.2，Copilot Chat 同意这确实会被视为违反适配器的合同责任。我们可以在 `AssetManager` 类中添加一个新的端口，该端口将使用
    Kafka 端口。然而，让我们看看 Copilot Chat 是否有其他建议：
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| We do not want the Kafka consumer to interact directly with the AssetManager.
    Are there any ways to accomplish this? |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 我们不希望 Kafka 消费者直接与 AssetManager 交互。有没有什么方法可以完成这个任务？ |'
- en: '![](../Images/CH05_F02_Crocker2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F02_Crocker2.png)'
- en: Figure 5.2 GitHub Copilot Chat’s acceptance that it is, in fact, imperfect
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 GitHub Copilot Chat 对其不完美的接受
- en: Copilot Chat suggests that we apply the `Mediator` pattern, which resembles
    a port and adapter combination.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 建议我们应用 `Mediator` 模式，这类似于端口和适配器的组合。
- en: Mediator Pattern
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 中介模式
- en: The Mediator design pattern is a behavioral pattern that promotes loose coupling
    between objects by encapsulating their interactions in a mediator object. The
    mediator object acts as a central hub that facilitates communication between objects
    without the objects having to know about each other.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 中介者设计模式是一种行为模式，通过封装中介对象中的交互来促进对象之间的松散耦合。中介对象充当中央枢纽，在对象之间促进通信，而无需对象相互了解。
- en: Let’s first take a look at the `Mediator` class that Copilot Chat generates.
    The class sits between `AssetManager` and the Kafka consumer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看 Copilot Chat 生成的 `Mediator` 类。该类位于 `AssetManager` 和 Kafka 消费者之间。
- en: Listing 5.15 `Mediator` class between `AssetManager` and Kafka consumer
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.15 `Mediator` 类位于 `AssetManager` 和 Kafka 消费者之间
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This implementation closely resembles a message queue or message publisher,
    which is precisely the point: decoupling.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现非常类似于消息队列或消息发布者，这正是重点：解耦。
- en: Note Eagle-eyed readers will likely notice that we are playing a little fast
    and loose with the distinction between an `Asset` and an asset of type `Hardware`.
    In the original domain model, only `Hardware` had a location. Generally, we do
    not think of software as having a location. Of course, you could say that software
    is installed in its location, but it is arguable how convincing this argument
    is. As this project continues, we flatten the domain model for simplicity’s sake,
    as polymorphic structures in a persistence layer are a distractingly complex topic.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：细心的读者可能会注意到我们在 `Asset` 和类型为 `Hardware` 的资产之间的区别上有点草率。在原始领域模型中，只有 `Hardware`
    有位置。通常，我们不认为软件有位置。当然，你可以说软件安装在其位置上，但这种论点的说服力是有争议的。随着这个项目的继续，为了简化起见，我们将领域模型扁平化，因为持久化层中的多态结构是一个令人分心的复杂话题。
- en: Now that we have a strategy to decouple the Kafka consumer from `AssetManager`,
    we should update the Kafka consumer to take advantage of it. We need to pass the
    mediator into the class in its constructor. This way, `AssetManager` and the consumer
    will have access to the same instance, and messages can freely flow back and forth—
    or rather, in this case, the flow will be unidirectional. You should note that
    we intend to read and write JSON on this topic, so our value deserializer needs
    to understand this.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了将 Kafka 消费者从 `AssetManager` 解耦的策略，我们应该更新 Kafka 消费者以利用它。我们需要在构造函数中将中介者传递给类。这样，`AssetManager`
    和消费者将能够访问相同的实例，消息可以自由地来回流动——或者更确切地说，在这种情况下，流动将是单向的。你应该注意，我们打算在这个主题上读写 JSON，因此我们的值反序列化器需要理解这一点。
- en: Listing 5.16 Incorporating the mediator into the Kafka consumer class
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.16 将中介者集成到 Kafka 消费者类中
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next we will examine the changes that the `AssetManager` class requires to incorporate
    the ability to track these locations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查 `AssetManager` 类需要进行的更改，以集成跟踪这些位置的能力。
- en: Note To run this project in its entirety, you would need to modify the `AssetManager`,
    `SQLAlchemyAssetRepository`, and `Asset` classes and also create a new table in
    your database called `itam.asset_locations`. The complete and updated source code
    is available on the book’s website ([www.manning.com/books/ai-powered-developer](https://www.manning.com/books/ai-powered-developer))
    and in the book’s GitHub repository ([https://github.com/nathanbcrocker/ai_assisted_dev_public](https://github.com/nathanbcrocker/ai_assisted_dev_public)).
    For now, we will focus on the changes needed to get the events flowing through
    our system and use the repository for reference if the reader so chooses.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要完整运行此项目，您需要修改 `AssetManager`、`SQLAlchemyAssetRepository` 和 `Asset` 类，并在您的数据库中创建一个名为
    `itam.asset_locations` 的新表。完整的更新源代码可在本书的网站上找到（[www.manning.com/books/ai-powered-developer](https://www.manning.com/books/ai-powered-developer)）以及本书的
    GitHub 仓库中（[https://github.com/nathanbcrocker/ai_assisted_dev_public](https://github.com/nathanbcrocker/ai_assisted_dev_public)）。目前，我们将专注于使事件通过我们的系统流动所需的更改，并在读者选择的情况下使用存储库作为参考。
- en: Figure 5.3 shows the changes required to the `AssetManager` class to begin to
    track the location of our assets in real time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 展示了对 `AssetManager` 类所做的更改，以开始实时跟踪我们的资产位置。
- en: '![](../Images/CH05_F03_Crocker2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Crocker2.png)'
- en: Figure 5.3 `AssetManager` requires the addition of another constructor parameter
    and a method to handle the updates to its location objects.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 `AssetManager` 需要添加另一个构造函数参数和一个处理其位置对象更新的方法。
- en: There are two required changes for the `AssetManager` class. First, we need
    to add the `AssetLocationMediator` to the constructor, registering it to handle
    the `AssetLocationUpdated` event. And second, we need to add a method that will
    handle this event. In this case, we call the method `update_asset_location`. The
    abridged code is shown next.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `AssetManager` 类有两个必要的更改。首先，我们需要在构造函数中添加 `AssetLocationMediator`，以注册处理 `AssetLocationUpdated`
    事件。其次，我们需要添加一个处理此事件的方法。在这种情况下，我们调用方法 `update_asset_location`。简化的代码如下。
- en: Listing 5.17 Updated constructor and an event handler for `AssetManager`
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.17 更新的 `AssetManager` 构造函数和事件处理器
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `add_location` method of the `Asset` class merely appends the new `Location`
    to the end of a list of `Location`s. More sophisticated domain models may include
    a `current_location` attribute, relegating the rest to a list of historical locations;
    however, given that we are trying to get our events flowing through the system,
    it behooves us to keep things simple.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`Asset` 类的 `add_location` 方法仅将新的 `Location` 追加到 `Location` 列表的末尾。更复杂的领域模型可能包括一个
    `current_location` 属性，将其他内容降级为历史位置列表；然而，鉴于我们试图让事件通过系统流动，我们最好保持简单。'
- en: 'There is only one final item on our to-do list: create the topic. How do we
    do this? That is a good question. Fortunately, all the tools we need are available
    in our running Docker container. So, let’s log in to our Kafka Docker instance.
    We use the following command (assuming that your Docker instance is named `kafka`):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们待办事项列表中只剩下一项：创建主题。我们如何做？这是一个好问题。幸运的是，我们需要的所有工具都可在我们的运行 Docker 容器中找到。因此，让我们登录到我们的
    Kafka Docker 实例。我们使用以下命令（假设您的 Docker 实例名为 `kafka`）：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first thing to check is whether any topics are already created. We can
    do that with the following command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要检查的是是否已经创建了任何主题。我们可以使用以下命令来完成：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This command lists all the existing topics running on this Kafka cluster. As
    you can see, there aren’t any.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令列出了在此 Kafka 集群上运行的所有现有主题。如您所见，没有任何主题。
- en: 'Given the need for a topic, let’s create it. Use the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要创建一个主题，让我们创建它。使用以下命令：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you run the `kafka-topics --list` command again, you will see the new topic.
    The partitions and replication-factor instructions we included in the create-topic
    command inform Kafka that we want one partition and a replication factor of 1\.
    If we were setting this up for production or any purpose other than testing, we
    would likely want them to be greater than that to ensure the availability of data.
    Table 5.1 provides you with some of the commonly used Kafka commands that you
    will need for this and other projects.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次运行 `kafka-topics --list` 命令，您将看到新主题。我们在创建主题命令中包含的分区和复制因子指令通知 Kafka 我们想要一个分区和一个复制因子为
    1。如果我们正在为生产或任何其他非测试目的设置此配置，我们可能希望它们大于这个值，以确保数据的可用性。表 5.1 提供了您将需要用于此和其他项目的常用 Kafka
    命令摘要。
- en: Table 5.1 Summary of Kafka console commands
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 Kafka 控制台命令摘要
- en: '| Action | Command |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 命令 |'
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Create | `kafka-topics --create --bootstrap-server localhost:9092 --replication-factor
    1 --partitions 1 --topic asset_location` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 创建 | `kafka-topics --create --bootstrap-server localhost:9092 --replication-factor
    1 --partitions 1 --topic asset_location` |'
- en: '| Read | `kafka-console-consumer --broker-list localhost:9092 --topic asset_location
    –from-beginning` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 读取 | `kafka-console-consumer --broker-list localhost:9092 --topic asset_location
    –from-beginning` |'
- en: '| Write | `kafka-console-producer --broker-list localhost:9092 --topic asset_location`
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 写入 | `kafka-console-producer --broker-list localhost:9092 --topic asset_location`
    |'
- en: '| Delete | `kafka-topics --delete --topic asset_location --bootstrap-server
    localhost:9092` |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | `kafka-topics --delete --topic asset_location --bootstrap-server localhost:9092`
    |'
- en: '| List | `kafka-topics --list --bootstrap-server localhost:9092` |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 列表 | `kafka-topics --list --bootstrap-server localhost:9092` |'
- en: 'Now comes the fun part: observing the application in action. Kafka comes with
    a console producer that will allow us to publish messages to Kafka from standard
    input. To do this, launch the console producer with the following command:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是趣味部分：观察应用程序的实际运行。Kafka 附带了一个控制台生产者，它将允许我们从标准输入向 Kafka 发布消息。为此，使用以下命令启动控制台生产者：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will enter an interactive session allowing you to publish a message with
    every line. Let’s publish a few messages simulating our asset moving around or
    near Chicago.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您将进入一个交互式会话，允许您每行发布一条消息。让我们发布几条消息，模拟我们的资产在芝加哥周围移动。
- en: Listing 5.18 Entries for the Kafka console producer
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.18 Kafka 控制台生产者的条目
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you enter these messages, you should see the output from your application
    indicating that the location has been updated.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当您输入这些消息时，您应该看到应用程序的输出，指示位置已更新。
- en: Deleting a topic
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 删除主题
- en: 'For the sake of completeness, there is one more command you should be aware
    of. You might make a mistake when entering these messages, and an invalid message
    could potentially break your consumer. One possible solution is to delete the
    topic. Deleting a topic may sound dramatic, but it will solve the problem. So
    here is that command:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，您应该了解一个额外的命令。您在输入这些消息时可能会犯错，无效的消息可能会破坏您的消费者。一个可能的解决方案是删除该主题。删除主题听起来可能有些戏剧性，但它会解决问题。因此，以下是该命令：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this section, we have added the ability to see changes in the location of
    our `Asset`s in real-time tracking using Apache Kafka. In the final section of
    this chapter, we will work with Copilot Chat to extend the capacity by monitoring
    our assets in real time and attempting to determine if they are where they should
    be. We will explore using Spark and Kafka together to accomplish this analysis.
    Once completed, we will win the thanks of our Information Security team, who fear
    that too much of our core business and intellectual property exists on and in
    these `Asset`s.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们添加了使用Apache Kafka实时跟踪我们的`Asset`位置变化的能力。在本章的最后部分，我们将与Copilot Chat合作，通过实时监控我们的资产并尝试确定它们是否在正确的位置来扩展其能力。我们将探索如何结合使用Spark和Kafka来完成这项分析。一旦完成，我们将赢得信息安全团队的支持，他们担心我们的核心业务和知识产权过多地存在于这些`Asset`上。
- en: 5.3 Analyzing, learning, and tracking with Apache Spark
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 使用Apache Spark进行数据分析、学习和跟踪
- en: Real-time tracking of assets is a business-critical function. Your IT assets
    contain sensitive business data, client lists, sales figures, profit-and-loss
    (PnL) projections, and sales strategies, among many other items. A lost asset
    can be an existential event for a company. Therefore, careful management and monitoring
    are priority one for many InfoSec professionals. In this section, we aim to make
    their jobs substantially easier. Modern data platforms make it trivial to track
    your assets in real time and send notifications if questionable conditions arise.
    Let’s get into it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 实时跟踪资产是一项至关重要的业务功能。您的IT资产包含敏感的商业数据、客户名单、销售数据、损益（PnL）预测和销售策略等众多内容。丢失资产可能对公司构成生存威胁。因此，对许多信息安全专业人士来说，谨慎管理和监控是首要任务。在本节中，我们的目标是使他们的工作大大简化。现代数据平台使得实时跟踪您的资产变得轻而易举，并在出现可疑条件时发送通知。让我们深入了解。
- en: Apache Spark is a powerful open source data-processing engine built around speed,
    ease of use, and sophisticated analytics. It was developed to provide an improved
    alternative to MapReduce for processing big datasets and can handle batch and
    real-time analytics. Spark provides APIs for Scala, Java, Python, and R and a
    built-in module for SQL queries. Its core data structure, the resilient distributed
    dataset (RDD), enables fault-tolerant operation and allows data to be processed
    in parallel across a cluster of computers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个围绕速度、易用性和复杂分析构建的强大开源数据处理引擎。它被开发出来，旨在为处理大数据集提供MapReduce的改进替代方案，并能够处理批处理和实时分析。Spark为Scala、Java、Python和R提供了API，并内置了用于SQL查询的模块。其核心数据结构，弹性分布式数据集（RDD），实现了容错操作，并允许数据在计算机集群中并行处理。
- en: Spark also includes several libraries to broaden its capabilities, including
    MLlib for machine learning, Spark Streaming for processing live data streams,
    and Spark SQL and DataFrames for processing structured data. These tools make
    it well-suited for tasks ranging from machine learning to real-time data streaming
    and batch processing. Its in-memory processing capabilities make Spark significantly
    faster than its predecessor, so it is a popular choice for big data processing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还包括几个库来扩展其功能，包括用于机器学习的MLlib、用于处理实时数据流的Spark Streaming以及用于处理结构化数据的Spark
    SQL和DataFrames。这些工具使其非常适合从机器学习到实时数据流和批处理的各项任务。其内存处理能力使Spark比其前身快得多，因此它成为大数据处理的热门选择。
- en: 'First we will ask Copilot Chat to recommend a strategy for using Apache Spark
    to track our assets:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将要求Copilot Chat推荐一个使用Apache Spark跟踪我们的资产的战略：
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-NC.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-NC.png)'
- en: '| Let''s imagine that our company is in Chicago. We want to create a class
    called AssetLocationSparkAdapter that will continuously stream AssetLocation json
    messages from a Kafka topic called asset_location. An AssetLocation has the following
    attributes asset_id: int, latitude: float, longitude: float, and timestamp: datetime.
    The AssetLocations are stored in a Postgres database in a table called itam.asset_locations.
    AssetLocationSparkAdapter should calculate if the new AssetLocation is more than
    25 miles from Chicago. If it is, then it should write a message to the console.
    This should be using the latest version of Spark. How would we do this? |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 让我们假设我们的公司位于芝加哥。我们想要创建一个名为 `AssetLocationSparkAdapter` 的类，该类将从名为 `asset_location`
    的 Kafka 主题中持续流式传输 `AssetLocation` JSON 消息。`AssetLocation` 具有以下属性：`asset_id`: int，`latitude`:
    float，`longitude`: float，和 `timestamp`: datetime。`AssetLocations` 存储在名为 `itam.asset_locations`
    的 Postgres 数据库表中。`AssetLocationSparkAdapter` 应该计算新的 `AssetLocation` 是否比芝加哥远 25
    英里以上。如果是，则应在控制台写入一条消息。这应该使用 Spark 的最新版本。我们该如何做呢？|'
- en: Copilot Chat generates a class that you should be able to put into a file called
    asset_location_spark_adapter.py in the infrastructure package. Helpfully, it also
    includes comments for each line, so you should find the generated code easy to
    follow. The `import` statements include the Spark libraries as well as `geopy`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 生成了一个类，你应该能够将其放入名为 `asset_location_spark_adapter.py` 的文件中，位于基础设施包中。它还贴心地为每一行代码提供了注释，因此你应该会发现生成的代码很容易理解。`import`
    语句包括 Spark 库以及 `geopy`。
- en: Listing 5.19 The `import`s required to run Spark
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.19 运行 Spark 所需的 `import`s
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The class begins with an overstuffed constructor that defines the schema Spark
    will use when it translates the JSON to a DataFrame.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 类以一个过载的构造函数开始，该构造函数定义了 Spark 在将 JSON 转换为 DataFrame 时使用的模式。
- en: Note The `AssetLocationSparkAdapter`, as defined, is a blocking process. Therefore,
    your FastAPI application will not “fully” boot until the Spark process has been
    killed. You want this to be a standalone process, or you need to introduce an
    asynchronous framework to have these two processes run concomitantly.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：根据定义，`AssetLocationSparkAdapter` 是一个阻塞过程。因此，你的 FastAPI 应用程序将不会“完全”启动，直到 Spark
    进程被终止。你希望这是一个独立的过程，或者你需要引入一个异步框架以使这两个过程同时运行。
- en: Next it starts up a local Spark instance/session that will allow Spark to connect
    to the Kafka topic and continuously stream in the records.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它启动一个本地 Spark 实例/会话，这将允许 Spark 连接到 Kafka 主题并持续流式传输记录。
- en: Listing 5.20 `AssessLocationSparkAdapter`, which processes the Kafka topic
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.20 `AssessLocationSparkAdapter`，处理 Kafka 主题
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The final section of the `AssetLocationSparkAdapter` class calculates the distance
    from the asset’s current location to Chicago. If the difference is greater than
    25 miles, it sends the result set to the console. Additionally, it provides a
    method to start and stop the adapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`AssetLocationSparkAdapter` 类的最后一部分计算资产当前位置到芝加哥的距离。如果差异大于 25 英里，它将结果集发送到控制台。此外，它提供了一个启动和停止适配器的方法。'
- en: Listing 5.21 Calculating the distance from the `Asset` location to Chicago
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.21 计算 `Asset` 位置到芝加哥的距离
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `calculate_distance` method takes the longitude and latitude of the asset’s
    location and determines the distance from Chicago using the `geopy.distance` function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_distance` 方法接收资产位置的经纬度，并使用 `geopy.distance` 函数确定从芝加哥的距离。'
- en: Listing 5.22 Function to calculate the distance between Chi-town and `Asset`
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.22 计算Chi-town和 `Asset` 之间距离的函数
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In this instance, the code that Copilot Chat produced had some problems preventing
    it from running locally. After running it locally, encountering these problems,
    and trolling Stack Overflow, you would find a solution to the two main problems
    with the code: a missing environmental variable for running locally and failing
    to register your UDF (User Defined Function). Fortunately, you do not need to
    do the testing and research—a solution is provided in the following listing.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，Copilot Chat 生成的代码存在一些问题，阻止它在本地运行。在本地运行它、遇到这些问题并在 Stack Overflow 上搜索后，你会找到解决代码中两个主要问题的解决方案：缺少本地运行的环境变量以及未能注册你的
    UDF（用户定义函数）。幸运的是，你不需要进行测试和研究——以下列表提供了一个解决方案。
- en: Listing 5.23 Edits required to run the application locally
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.23 运行应用程序本地所需的编辑
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, to run your Spark application, update main.py with the following code
    in the `main` function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了运行你的 Spark 应用程序，在 `main` 函数中更新 main.py，如下所示。
- en: Listing 5.24 Updates to the `main` function
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.24 更新 `main` 函数
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you enter locations for your asset into the Kafka console producer that are
    further than 25 miles from downtown Chicago, you will notice that entries are
    written to the console. It would be trivial to update the class to output these
    results to Twilio’s SMS API or an email service such as SendGrid.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将您的资产位置输入到距离芝加哥市中心超过25英里的Kafka控制台生产者时，您会注意到条目被写入控制台。将类更新为输出这些结果到Twilio的短信API或SendGrid等电子邮件服务将是一件微不足道的事情。
- en: Listing 5.25 The streaming output from your asset location
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.25 从您的资产位置流出的输出
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Congratulations! You are tracking your assets in real time and sending real-time
    alerts in case your corporate resources grow legs and walk away.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您正在实时跟踪您的资产，并在企业资源“长腿”离开时发送实时警报。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GitHub Copilot Chat is an innovative tool that brings together the comprehensive
    language understanding of ChatGPT and the handy features of Copilot. It’s a noteworthy
    development in the realm of programming assistance, particularly for providing
    detailed and contextually relevant suggestions in real time, fostering a more
    efficient coding experience.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub Copilot Chat是一个创新工具，它结合了ChatGPT的全面语言理解和Copilot的便捷功能。这是编程辅助领域的一个值得注意的发展，特别是在提供实时、上下文相关的建议方面，它促进了更高效的编码体验。
- en: The Mediator design pattern is a distinct behavioral pattern that facilitates
    a high level of decoupling between objects, thus enhancing the modularity of your
    code. By encompassing the interactions between objects in a mediator object, objects
    can communicate indirectly, which reduces dependencies and promotes code reusability
    and ease of modification.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中介设计模式是一种独特的行为模式，它促进了对象之间的高度解耦，从而增强了代码的模块化。通过在中介对象中包含对象之间的交互，对象可以间接通信，这减少了依赖性并促进了代码的可重用性和易于修改。
- en: Apache Kafka is a robust, distributed streaming platform engineered for creating
    real-time data pipelines and streaming applications. It can effectively handle
    data streams from a multitude of sources and transmit them to various consumers,
    making it an ideal solution for use cases that require handling substantial volumes
    of real-time or near-real-time data. It’s important to remember that Kafka is
    optimized for append-only, immutable data and not for use cases that need record
    updates or deletions, or complex querying.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka是一个健壮的、分布式的流平台，旨在创建实时数据管道和流应用程序。它能够有效地处理来自多个来源的数据流并将它们传输到各种消费者，使其成为处理大量实时或近实时数据的理想解决方案。重要的是要记住，Kafka针对的是仅追加、不可变的数据，而不是需要记录更新或删除或复杂查询的使用场景。
- en: Apache Spark is a high-performance, distributed data processing engine renowned
    for its speed, user-friendliness, and advanced analytics capabilities. It’s highly
    suitable for scenarios necessitating real-time data processing or for operations
    on enormous datasets. However, for simpler tasks such as basic analytics and straightforward
    aggregations, a traditional relational database may be a more appropriate choice.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark是一个高性能、分布式的数据处理引擎，以其速度、用户友好性和高级分析能力而闻名。它非常适合需要实时数据处理或对大型数据集进行操作的场景。然而，对于诸如基本分析和简单的聚合等简单任务，传统的数据库可能是一个更合适的选择。
- en: Generative AI, despite its rapid evolution, is not infallible. It’s crucial
    to meticulously review all generated output to ensure that it aligns with your
    specific requirements and quality standards. Generative AI is not a substitute
    for deep domain knowledge or coding expertise, but it significantly enhances productivity
    by providing valuable insights and reducing the time spent on routine tasks.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管生成式AI发展迅速，但它并非完美无缺。仔细审查所有生成的输出以确保其符合您的具体要求和质量标准至关重要。生成式AI不是深度领域知识或编码专长的替代品，但它通过提供有价值的见解并减少常规任务所花费的时间，显著提高了生产力。
