- en: '4 *Data engineering for large language models: Setting up for success*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 *大型语言模型的数据工程：为成功做好准备*
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Common foundation models used in the industry
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业中常用的基础模型
- en: How to evaluate and compare large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估和比较大型语言模型
- en: Different data sources and how to prepare your own
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的数据源以及如何准备你自己的
- en: Creating your own custom tokenizers and embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建你自己的自定义分词器和嵌入
- en: Preparing a Slack dataset to be used in future chapters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于未来章节的Slack数据集
- en: Data is like garbage. You’d better know what you are going to do with it before
    you collect it.—Mark Twain
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据就像垃圾。在收集它之前，你最好知道你打算如何处理它。——马克·吐温
- en: Creating our own LLM is no different from any ML project in that we will start
    by preparing our assets—and there isn’t a more valuable asset than your data.
    All successful AI and ML initiatives are built on a good data engineering foundation.
    It’s important then that we acquire, clean, prepare, and curate our data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们自己的LLM与任何ML项目并无不同，我们首先需要准备我们的资产——没有比你的数据更宝贵的资产了。所有成功的AI和ML项目都是建立在良好的数据工程基础之上的。因此，我们获取、清理、准备和整理我们的数据是非常重要的。
- en: Unlike other ML models, you generally won’t be starting from scratch when creating
    an LLM customized for your specific task. Of course, if you do start from scratch,
    you’ll likely only do it once. Then it’s best to tweak and polish that model to
    further refine it for your specific needs. Selecting the right base model can
    make or break your project. Figure 4.1 gives a high-level overview of the different
    pieces and assets you’ll need to prepare before training or finetuning a new model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他ML模型不同，在创建针对特定任务定制的LLM时，你通常不会从零开始。当然，如果你确实是从零开始，你可能只会这样做一次。然后，最好调整和润色该模型，以进一步满足你的特定需求。选择正确的基模型可能会使你的项目成功或失败。图4.1给出了在训练或微调新模型之前需要准备的不同组件和资产的高级概述。
- en: '![figure](../Images/4-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-1.png)'
- en: Figure 4.1 The different elements of training an LLM. Combining earth, fire,
    water—wait, no, not those elements. To get started, you’ll need to collect several
    assets, including a foundation model, training data, text encoders (e.g., tokenizer),
    and evaluation data.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1 训练LLM的不同元素。结合地球、火、水——等等，不，不是那些元素。要开始，你需要收集几个资产，包括基础模型、训练数据、文本编码器（例如，分词器）和评估数据。
- en: 'As was so well defined in the book *Fundamentals of Data Engineering*[¹](#footnote-151):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如书中《数据工程基础》[¹](#footnote-151)所如此明确地定义的：
- en: Data engineering is the development, implementation, and maintenance of systems
    and processes that take in raw data and produce high-quality, consistent information
    that supports downstream use cases, such as analysis and machine learning.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据工程是开发、实施和维护系统及流程的过程，这些系统及流程接收原始数据并产生高质量、一致的信息，以支持下游用例，如分析和机器学习。
- en: In this chapter, we will discuss the steps you’ll need to take before you can
    start creating your LLM, which largely involves preparing the data assets necessary
    to train a model. We will go over many of the base or foundation models available
    to you as a starting point and how to evaluate and compare them. We will then
    go into depth on many of the different datasets available and how to prepare your
    own for finetuning a model, including preparing your own tokenizer or embeddings.
    Lastly, we will craft a dataset that we will use to finetune a model in the next
    chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在你开始创建LLM之前需要采取的步骤，这主要涉及准备训练模型所需的数据资产。我们将讨论许多可用的基或基础模型作为起点，以及如何评估和比较它们。然后，我们将深入探讨许多不同的数据集以及如何为微调模型准备你自己的数据集，包括准备你自己的分词器或嵌入。最后，我们将构建一个数据集，我们将在下一章中使用它来微调模型。
- en: 4.1 Models are the foundation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 模型是基础
- en: We will first discuss the most important dataset you will need to collect when
    training, which is the model weights of a pretrained model. A big reason why LLMs
    are so successful as a technology is that we can take a model already trained
    on language as a whole and tweak it to do well on a specific task. Of course,
    knowing how that beginning model was trained and what it was trained on will be
    a huge shortcut in choosing the right one to tweak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论你在训练时需要收集的最重要数据集，即预训练模型的模型权重。LLM作为一项技术之所以如此成功，一个重要原因是我们可以将已经在整个语言上训练好的模型进行调整，以在特定任务上表现良好。当然，了解那个初始模型是如何训练的以及它是基于什么进行训练的，将大大缩短选择正确模型进行调整的时间。
- en: Choosing the right one has become obnoxiously difficult since LLMs have been
    a hot research topic, resulting in a new one that sports benchmark-breaking records
    popping up almost every week. Because we know (or at least assume) you are eager
    to learn about them, we will first discuss the many different models currently
    out there. These models have already been trained (for better or worse) by professionals
    working to make your life easier and put powerful language models into the public
    arena. There are thousands upon thousands of open source models available on GitHub,
    Hugging Face Hub, and elsewhere, so to simplify, we’ll highlight our favorites,
    giving you details about each of the models to make it easier to compare and to
    give you an idea about whether you should use that particular model or opt for
    one of its lesser-known open source variants. If you are planning to train from
    scratch, consider the architecture involved and if there’s a certain family you’d
    like to try.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLM）已成为热门的研究课题，选择正确的一个变得异常困难，几乎每周都会出现一个新的模型，打破了基准记录。因为我们知道（或者至少假设）你急于了解它们，所以我们首先讨论目前市场上存在的许多不同模型。这些模型已经被专业人士训练（无论好坏），旨在使你的生活更轻松，并将强大的语言模型推向公众领域。GitHub、Hugging
    Face Hub和其他地方有数以千计的开源模型可供使用，为了简化，我们将突出我们最喜欢的模型，并详细介绍每个模型，以便更容易进行比较，并给你一个关于你是否应该使用该特定模型或选择其不太知名的开放源代码变体的想法。如果你计划从头开始训练，考虑涉及的架构，以及你是否想尝试某个特定的系列。
- en: 4.1.1 GPT
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 GPT
- en: There’s probably no better place to start than with GPT (Generative Pre-trained
    Transformer) models. A fan favorite and one of ours too, these models are sold
    commercially through OpenAI and have gained popularity for their impressive performance
    on a wide range of tasks. GPT models are so well known that laypersons often use
    “GPT” to replace “LLM,” just as one might say Kleenex or Band-Aid instead of tissue
    or bandage.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 没有比从GPT（生成式预训练Transformer）模型开始更好的地方了。这些模型是粉丝和我们的最爱之一，通过OpenAI进行商业销售，因其广泛的任务上的出色表现而受到欢迎。GPT模型如此知名，以至于外行人经常用“GPT”来代替“LLM”，就像人们可能会用Kleenex或Band-Aid来代替纸巾或绷带一样。
- en: The first GPT model was introduced in 2018, shortly after transformers were
    introduced, and only had 120M parameters. It was trained on the small BookCorpus
    dataset and had impressive results on NLP benchmarks at the time. The GPT-2 model
    came out the next year, increasing its size by 10-fold to 1.5B parameters; it
    was trained on the much larger WebText dataset. The next year, in 2020, GPT-3
    came out 100 times larger with 175B parameters and trained on the massive Common
    Crawl dataset. This model was still based on GPT-1’s original architecture with
    slight modifications for improved scaling.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个GPT模型于2018年推出，紧随transformers的推出之后，只有1.2亿个参数。它在BookCorpus小型数据集上进行了训练，在当时的NLP基准测试中取得了令人印象深刻的结果。GPT-2模型在第二年推出，其规模扩大了10倍，达到15亿个参数；它在更大的WebText数据集上进行了训练。到了2020年，GPT-3推出，其规模扩大了100倍，达到1750亿个参数，并在庞大的Common
    Crawl数据集上进行了训练。这个模型仍然基于GPT-1的原始架构，并进行了轻微的修改以改进扩展性。
- en: OpenAI has chosen to keep further iterations like GPT-4 under greater secrecy,
    not revealing training data or specific architectures, since it has started to
    productionize and sell them as a product. ChatGPT is a finetuned GPT-3 model trained
    for conversational interaction using reinforcement learning with human feedback
    (RLHF). Not to get into the weeds, but there is a whole host of GPT-3 models you
    can find under API names such as ada, babbage, curie, and davinci, as well as
    other finetuned models such as webGPT and InstructGPT. We leave it to the reader
    to investigate further if they are interested.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI选择对GPT-4等后续迭代版本保持更高的保密性，不透露训练数据或具体架构，因为它们已经开始量产并作为产品出售。ChatGPT是一个经过微调的GPT-3模型，通过使用带有人类反馈的强化学习（RLHF）进行对话交互训练。不过不深入探讨，但你可以找到一系列GPT-3模型，这些模型在API名称下如ada、babbage、curie和davinci，以及其他微调模型如webGPT和InstructGPT。如果读者感兴趣，我们可以留待进一步调查。
- en: Other open source variations like GPT-J were created by the open source community
    utilizing the knowledge gained from the whitepapers OpenAI published. Several
    GPT models have no relation to OpenAI, as Generative Pre-trained Transformer is
    a very generic name that fits most LLMs. Of course, OpenAI has started to see
    it as a brand and is trying to trademark the acronym.[²](#footnote-152)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他开源变体，如 GPT-J，是由开源社区利用 OpenAI 发布的论文中获得的知识创建的。一些 GPT 模型与 OpenAI 没有关系，因为生成预训练转换器是一个非常通用的名称，适用于大多数
    LLM。当然，OpenAI 已经开始将其视为一个品牌，并试图将缩写词注册商标。[²](#footnote-152)
- en: GPT-X models, although closed source, can be accessed via the OpenAI API, which
    also includes features for their finetuning. We will be using GPT-2 throughout
    this book—even though it is a bit smaller than what most would consider an actual
    LLM—as it is a well-understood architecture and easy to learn with.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-X模型是闭源的，但可以通过 OpenAI API 访问，该 API 还包括其微调的功能。我们将在这本书中使用 GPT-2——尽管它比大多数人认为的实际
    LLM 要小一些——因为它是一个易于理解的架构，并且易于学习。
- en: 4.1.2 BLOOM
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 BLOOM
- en: BLOOM is one of the most iconic LLMs because of the learning that has come from
    creating it. The model came out in 2022 and is the first public LLM to rival GPT-3’s
    size with 176B parameters; it was trained with complete transparency. It was put
    together by Hugging Face’s BigScience team, with help from Microsoft’s DeepSpeed
    team and NVIDIA’s Megatron-LM team, and was sponsored by French government grants.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM 是最具标志性的 LLM 之一，因为它在创建过程中所获得的学习成果。该模型于 2022 年发布，是第一个公开的 LLM，其参数量达到 176B，与
    GPT-3 的规模相媲美；它以完全透明的方式进行训练。该模型由 Hugging Face 的 BigScience 团队制作，得到了微软的 DeepSpeed
    团队和 NVIDIA 的 Megatron-LM 团队的协助，并由法国政府拨款赞助。
- en: BLOOM was trained on the BigScienceCorpus dataset, a conglomerate of many smaller
    datasets amounting to 1.6TB of pre-processed text. It is licensed under RAIL,
    which means it isn’t technically open source, since there are restrictions on
    how you can use it, but it can be commercialized.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM 在 BigScienceCorpus 数据集上进行训练，这是一个由许多较小的数据集组成的综合体，总容量达到 1.6TB 的预处理文本。它受
    RAIL 许可证约束，这意味着它技术上不是开源的，因为对其使用有约束，但它可以进行商业化。
- en: 'TIP  You can learn more about the RAIL license here: [https://mng.bz/mR20](https://mng.bz/mR20).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TIP  在这里可以了解更多关于 RAIL 许可证的信息：[https://mng.bz/mR20](https://mng.bz/mR20)。
- en: 'BLOOM was trained to be industry size and industry grade for all tasks. Because
    of this, fitting on a consumer device was not a priority, but several smaller
    versions were trained as the research team was coming up to speed. There are 560M-,
    3B-, and 7B-parameter versions. There is also BLOOMZ, a multitask, finetuned version
    of the full 176B parameter model. BLOOM was only trained in 46 different languages,
    and BLOOMZ’s goal was to increase the cross-lingual generalization of the model.[³](#footnote-153)
    You can find all of these models on Hugging Face’s hub: [https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM 被训练为适用于所有任务的行业规模和行业级别。因此，适应消费设备不是优先事项，但随着研究团队的速度加快，已经训练了几个较小的版本。有 560M-、3B-和
    7B 参数版本。还有一个 BLOOMZ，它是完整 176B 参数模型的多元任务微调版本。BLOOM 只在 46 种不同的语言上进行训练，而 BLOOMZ 的目标是提高模型的跨语言泛化能力。[³](#footnote-153)
    你可以在 Hugging Face 的 hub 上找到所有这些模型：[https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom)。
- en: The big downside to BLOOM is that it often gives poor responses and doesn’t
    compete very well in benchmarks—most likely due to limited funds and tight deadlines
    of the project, leading to a feeling that it was undertrained. This isn’t always
    a bad thing and is often better than an overtrained model, but you can expect
    to require a lot more finetuning on a larger dataset if you decide to use it.
    The benefit of using it, though, is that it is well understood and trained in
    the open, and you can check its training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM 的一个重大缺点是它经常给出较差的响应，在基准测试中竞争不佳——这很可能是由于项目资金有限和截止日期紧迫，导致给人一种它训练不足的感觉。这并不总是坏事，通常比过训练的模型要好，但如果你决定使用它，你可能会需要在大数据集上进行更多的微调。然而，使用它的好处是它被广泛理解和公开训练，你可以检查其训练数据。
- en: In general, the authors wouldn’t recommend using it as a foundation model anymore;
    there are better alternatives, but it’s one you should be familiar with because
    of its contributions. For example, BLOOM’s creation of petals, which allowed distributed
    training, was a significant contribution to the field.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，作者不会推荐再将其作为基础模型使用；有更好的替代品，但你应该熟悉它，因为它对领域做出了贡献。例如，BLOOM 创建花瓣，允许分布式训练，这对领域是一个重大贡献。
- en: 4.1.3 LLaMA
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 LLaMA
- en: LLaMA is the result of Meta’s foray into LLMs. The first version was released
    in February 2023 and was released to the research community with a noncommercial
    license. A week later, the weights were leaked on 4chan. In an unlikely turn of
    events, this leak has likely been very beneficial to Meta, as this model has become
    the standard for experimentation and development. Several more models we will
    discuss are based on it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA是Meta进入LLM领域的成果。第一个版本于2023年2月发布，并以非商业许可发布给研究社区。一周后，权重在4chan上泄露。在一系列不太可能的事件中，这次泄露可能对Meta非常有益，因为这个模型已经成为实验和开发的行业标准。我们将讨论的几个更多模型都是基于它的。
- en: 'Later, in July 2023, Meta released Llama 2, which has both a research and a
    commercial license. Llama 2 is a big deal since it’s the first commercially available
    model that really packs a punch, and you’ll see many other models based on its
    architecture. There are three different model sizes available: 7B, 13B, and 70B
    parameters. You can download them here: [https://ai.meta.com/llama/](https://ai.meta.com/llama/).
    You’ll need to request access and accept the terms and conditions if you plan
    to use it.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，在2023年7月，Meta发布了Llama 2，它同时拥有研究和商业许可。Llama 2是一个大事件，因为它是第一个真正具有强大功能的商业可用模型，你将看到基于其架构的许多其他模型。有三种不同的模型大小可供选择：7B、13B和70B参数。你可以从这里下载它们：[https://ai.meta.com/llama/](https://ai.meta.com/llama/)。如果你打算使用它，你需要申请访问权限并接受条款和条件。
- en: Llama 2 was trained on 2 trillion tokens from a curated dataset taken from the
    internet where they removed websites known to contain personal information and
    upsampled what they considered factual sources. While exact details of the dataset
    haven’t been shared, it likely contained data from Common Crawl, GitHub, Wikipedia,
    Project Gutenberg, ArXiv, and Stack Exchange since those were the primary datasets
    for LLaMA 1\. These datasets were later packaged together and distributed under
    the name RedPajama. Llama 2 was then further finetuned using RLHF, with one model
    finetuned for chat and another for code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2是在从互联网上收集的经过精心挑选的数据集上训练的，其中他们移除了已知包含个人信息的网站，并增加了他们认为的事实来源。尽管数据集的详细情况尚未公开，但它很可能包含了来自Common
    Crawl、GitHub、Wikipedia、Project Gutenberg、ArXiv和Stack Exchange的数据，因为这些都是LLaMA 1的主要数据集。这些数据集后来被包装在一起，并以RedPajama的名义分发。然后，Llama
    2使用RLHF进一步微调，一个模型用于聊天，另一个用于代码。
- en: 4.1.4 Wizard
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 巫师
- en: 'The Wizard family of language models comes from the 2023 paper “WizardLM: Empowering
    Large Language Models to Follow Complex Instructions.”[⁴](#footnote-154) These
    models follow the idea that LLMs function better when trained on dense training
    data filled with high-complexity tasks. Based on a proposed framework for creating
    more complex instruction tasks, the WizardLM methodology has been applied to many
    popular datasets and used to finetune almost all of the most popular models. The
    methodology is so popular that, amazingly, it only took the community two days
    after LlamaCoder34B came out to finetune the WizardCoder34B model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 巫师系列语言模型来自2023年的论文“WizardLM：赋予大型语言模型遵循复杂指令的能力”。[⁴](#footnote-154) 这些模型遵循的想法是，当在充满高复杂度任务的密集训练数据上训练时，LLM的表现更好。基于创建更复杂指令任务的框架，巫师LM方法已应用于许多流行的数据集，并用于微调几乎所有最流行的模型。这种方法如此受欢迎，以至于令人惊讶的是，在LlamaCoder34B发布后的两天内，社区就微调了WizardCoder34B模型。
- en: These models have been consistently praised for their human-like prose and their
    ability to correctly sort through complex problems that rivals many paid services.
    One problem we encourage you to try is to ask WizardCoder34B to write a program
    that draws a realistic-looking tree using any language you’d like. Because the
    Wizard models don’t revolve as much around a specific dataset as they do around
    the methodology of changing an existing dataset to fit the Wizard style, the applications
    are incredibly broad and diverse. If you hit a wall where you aren’t sure how
    to improve when using another model or architecture, try taking the dataset you’ve
    already used and applying the Wizard methodology. You’re welcome.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型因其类似人类的散文风格和正确处理复杂问题的能力而一直受到好评，这些能力甚至可以与许多付费服务相媲美。我们鼓励你尝试的一个问题是要求WizardCoder34B用你喜欢的任何语言编写一个绘制看起来逼真的树的程序。由于巫师模型不像它们围绕特定数据集那样围绕将现有数据集转换为巫师风格的方法，因此应用范围极其广泛且多样化。如果你在使用其他模型或架构时遇到瓶颈，不确定如何改进，请尝试使用你已经使用过的数据集并应用巫师方法。欢迎你这样做。
- en: As a side note, WizardCoder models tend to get a lot of attention, but the WizardMath
    models are also impressive in their own right. We note that a lot of readers likely
    deal more with data problems than code problems, and the WizardMath models might
    be a great place to start when working with talk-to-your-data applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附带说明，WizardCoder模型通常受到很多关注，但WizardMath模型在自身领域也非常令人印象深刻。我们注意到，许多读者可能更多地处理数据问题而不是代码问题，因此当处理与数据对话的应用程序时，WizardMath模型可能是一个很好的起点。
- en: 4.1.5 Falcon
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 鹰隼
- en: 'Falcon models are a model family from the Technology Innovation Institute in
    Abu Dhabi. They are the first state-of-the-art models to be released under a truly
    open source license, Apache 2.0\. You can get the model from the institute’s website:
    [https://falconllm.tii.ae/falcon-models.xhtml](https://falconllm.tii.ae/falcon-models.xhtml).
    Its easy access and the open license make this a dream for hackers, practitioners,
    and the industry.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鹰隼模型是阿布扎比科技创新研究所的一个模型系列。它们是第一个在真正开源许可Apache 2.0下发布的尖端模型。您可以从研究所的网站获取模型：[https://falconllm.tii.ae/falcon-models.xhtml](https://falconllm.tii.ae/falcon-models.xhtml)。其易于访问和开源许可使得这对于黑客、实践者和行业来说都是一个梦想。
- en: Falcon models first introduced in June 2023 only introduced 7B and 40B parameter
    models, but in September 2023, Falcon released a 180B parameter model that can
    truly compete with GPT-3–sized models. What’s also exciting and probably more
    important to many readers is that Falcon has often led LLM leaderboards in many
    benchmarking tasks. The models were primarily trained on the RefinedWeb dataset,
    which is a smaller but much higher-quality dataset that was carefully and meticulously
    curated and extracted from the Common Crawl dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 鹰隼模型首次于2023年6月推出，当时只推出了7B和40B参数模型，但到了2023年9月，鹰隼发布了一个180B参数模型，可以真正与GPT-3大小的模型竞争。对许多读者来说，更令人兴奋且可能更重要的是，鹰隼在许多基准测试任务中经常领先于LLM排行榜。这些模型主要在RefinedWeb数据集上训练，这是一个较小但质量更高的数据集，它是经过精心和细致的策划和从Common
    Crawl数据集中提取出来的。
- en: 4.1.6 Vicuna
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.6 维库纳
- en: Vicuna was trained on a dataset of user-shared conversations from ShareGPT.
    The logic is that a model trained off of the best outputs of ChatGPT will be able
    to emulate the performance of ChatGPT, piggy-backing off of the Llama–Alpaca trend.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 维库纳是在ShareGPT用户共享对话数据集上训练的。其逻辑是，在ChatGPT最佳输出基础上训练的模型将能够模仿ChatGPT的性能，借助于Llama-Alpaca的趋势。
- en: NOTE  We won’t talk about Alpaca here, but we introduced it in chapter 3 when
    discussing knowledge distillation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这里不会讨论Alpaca，但在第三章讨论知识蒸馏时我们已经介绍过它。
- en: Vicuna has been praised for both its performance and its relatively low training
    costs. Vicuna is an amazing example of why data coverage and quality matter so
    much while simultaneously demonstrating the dangers of model collapse from training
    on the output of another model. Model collapse happens when an ML model is trained
    on synthetic data, leading to increasingly less diverse outputs. For example,
    Vicuna performs admirably on anything that is at least close to what appeared
    in the dataset, but when asked to perform more generative or agent-like tasks,
    it tends to hallucinate far beyond what its predecessors do. Vicuna is not licensed
    for commercial use, but it is amazing for personal projects.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 维库纳因其性能和相对较低的训练成本而受到赞誉。维库纳是数据覆盖率和质量为何如此重要的绝佳例子，同时它也展示了从另一个模型的输出中训练模型导致模型崩溃的危险。当机器学习模型在合成数据上训练时，会导致输出越来越不多样化，这种现象称为模型崩溃。例如，维库纳在处理至少接近数据集中出现的内容时表现优秀，但当被要求执行更生成性或代理类任务时，它往往会产生远超其前辈的幻觉。维库纳未经许可不得用于商业用途，但对于个人项目来说非常出色。
- en: 4.1.7 Dolly
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.7 多莉
- en: Created by Databricks as more of a thought experiment than a competitive model,
    Dolly and its V2 do not perform well compared to other models of the same size.
    However, Dolly boasts one of the best underlying understandings of English and
    is a fantastic starting point for finetuning or creating low-ranking adaptations
    (LoRAs; which we will discuss in chapter 5) to influence other models. Dolly 1.0
    was trained on the Stanford Alpaca Dataset, while Dolly 2.0 was trained on a high-quality
    human-generated instruction-following dataset that was crowdsourced by the Databricks
    employees. Dolly 2.0 has been open sourced in its entirety, including the training
    code, dataset, and model weights, all with a commercial use license.[⁵](#footnote-155)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Dolly 是由 Databricks 创建的，与其说是竞争性模型，不如说是更多的一次思想实验，与相同大小的其他模型相比，Dolly 及其 V2 版本的表现并不出色。然而，Dolly
    在对英语的底层理解方面堪称最佳，是微调或创建低排名适应（LoRAs；我们将在第 5 章中讨论）以影响其他模型的绝佳起点。Dolly 1.0 是在斯坦福 Alpaca
    数据集上训练的，而 Dolly 2.0 是在由 Databricks 员工众包的高质量人类生成指令遵循数据集上训练的。Dolly 2.0 已经完全开源，包括训练代码、数据集和模型权重，所有这些均带有商业用途许可证。[⁵](#footnote-155)
- en: 4.1.8 OpenChat
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.8 OpenChat
- en: OpenChat is similar to Vicuna in that OpenChat used 80K ShareGPT conversations
    for training, but dissimilar in that their conditioning and weighted loss strategies
    end up creating a model that is undeniably great in its ability to generate human-like
    and, more importantly, human-preferred responses.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: OpenChat 与 Vicuna 类似，因为 OpenChat 使用了 80K ShareGPT 对话进行训练，但在条件化和加权损失策略方面有所不同，最终创建了一个在生成类似人类和，更重要的是，人类更喜欢的响应方面无可否认的出色的模型。
- en: OpenChat models—not to be confused with the open source chatbot console—are
    a collection of various finetunings for different tasks, with some meant for coding,
    others for agents, and others for chatting. Free for commercial use under the
    Llama 2 Community License, these models could be a great solution to build off
    of at your corporation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OpenChat 模型——不要与开源聊天机器人控制台混淆——是为不同任务而收集的各种微调集合，其中一些用于编码，其他用于代理，还有一些用于聊天。在 Llama
    2 社区许可证下免费用于商业用途，这些模型可以成为您公司构建的绝佳解决方案。
- en: We’ve discussed a lot of models already, and while we could go on like this
    for the rest of the chapter, it’s in everyone’s best interest that we don’t. Table
    4.1 shows a summary highlighting some of the major points of comparison for the
    models we discussed. One major point we’d like to highlight is that a lot of models
    are available for commercial use! While many of the licenses come with restrictions,
    they likely aren’t rules you plan to break anyway.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了很多模型，虽然我们可以继续这样讨论整个章节，但这对大家来说都不是最好的选择。表 4.1 展示了一个总结，突出了我们讨论的模型的一些主要比较点。我们想强调的一个主要点是，许多模型都可用于商业用途！虽然许多许可证都有限制，但它们可能不是你打算违反的规则。
- en: Table 4.1 Comparison of LLM model families
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.1 LLM 模型家族比较
- en: '| Model family | Dataset | Largest model size | Commercial license | Organization
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 模型家族 | 数据集 | 最大模型大小 | 商业许可证 | 组织 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT  | Common Crawl/RLHF  | 1.76T  | No  | OpenAI  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| GPT | Common Crawl/RLHF | 1.76T | 否 | OpenAI |'
- en: '| BLOOM  | BigScienceCorpus  | 176B  | Yes  | BigSciense  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | BigScienceCorpus | 176B | 是 | BigScience |'
- en: '| Llama  | RedPajama  | 70B  | Yes  | Meta  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Llama | RedPajama | 70B | 是 | Meta |'
- en: '| Wizard  | Evol-Instruct  | 70B  | No  | Microsoft  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Wizard | Evol-Instruct | 70B | 否 | Microsoft |'
- en: '| Falcon  | RefinedWeb  | 180B  | Yes  | TII  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | RefinedWeb | 180B | 是 | TII |'
- en: Now that you have an understanding of some of the more popular model families,
    you might have an idea of which model to pick to start for your project. But how
    can you be sure? In the next section, we’ll look at different ways you can evaluate
    and compare models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些更受欢迎的模型家族，你可能已经对为你的项目选择哪个模型有了一些想法。但你怎么能确定呢？在下一节中，我们将探讨不同的评估和比较模型的方法。
- en: 4.2 Evaluating LLMs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 评估大型语言模型
- en: While we have just discussed some of our favorite model families, there are
    so many more and varying models available out there, with many more coming out
    every month, all claiming to be the best. It is impossible to keep them all straight.
    So how do you pick the best one to use? Can it perform well on your task out of
    the box, or will it require finetuning? How do you know if your finetuning improved
    the model or just made it worse? How do you know if you picked the right size?
    A smaller model is convenient, but larger models perform better on many tasks.
    To be honest, these are not easy questions to answer, but thankfully, there are
    a few industry standards we can rely on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们刚刚讨论了一些我们最喜欢的模型家族，但还有许多其他不同的模型可供选择，每个月都有更多的新模型出现，所有这些模型都声称自己是最好的。不可能将它们全部理清楚。那么，你如何选择最好的一个来使用？它是否能够直接在你的任务上表现良好，或者是否需要微调？你如何知道你的微调是否提高了模型，或者只是让它变得更糟？你如何知道你选择了正确的大小？较小的模型很方便，但较大的模型在许多任务上的表现更好。说实话，这些问题并不容易回答，但幸运的是，有一些行业标准我们可以依赖。
- en: 'When evaluating a model, you will need two things: a metric and a dataset.
    A metric is an algorithm that allows us to compare results to a ground truth.
    A dataset is a list of tasks we want our model to run, which we will then compare
    using our metrics of choice.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估一个模型时，你需要两样东西：一个指标和一个数据集。指标是一个算法，它允许我们将结果与真实情况进行比较。数据集是我们希望模型执行的任务列表，然后我们将使用我们选择的指标进行比较。
- en: In this section, we will discuss many different methodologies employed to evaluate
    LLMs so we can evaluate and compare them objectively. We will discuss everything
    from common industry benchmarks to methodologies used to develop your own unique
    evaluations. Let’s get started.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论许多用于评估大型语言模型的不同方法，以便我们可以客观地评估和比较它们。我们将从常见的行业基准到开发你自己的独特评估方法的一切内容进行讨论。让我们开始吧。
- en: 4.2.1 Metrics for evaluating text
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 评估文本的指标
- en: Evaluating text is often difficult because it’s easy to say the exact same thing
    in two different ways. Semantically, two sentences can be exactly the same, but
    syntactically, they are nothing alike, making text comparison tricky. See what
    I did there?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估文本通常很困难，因为用两种不同的方式说出相同的话很容易。从语义上讲，两个句子可能是完全相同的，但从句法上讲，它们却毫无相似之处，这使得文本比较变得复杂。看看我做了什么？
- en: To evaluate our models, we will need better metrics than just an exact match
    or check for equality, which we can get away with for most other ML problems.
    We need a metric that allows us to compare the generated text from our models
    against a ground truth without being too rigid. Let’s look at some of the most
    common metrics used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们需要比仅仅匹配或检查相等性更好的指标，这对于大多数其他机器学习问题来说是可以做到的。我们需要一个指标，它允许我们比较模型生成的文本与真实情况，而不会过于严格。让我们看看一些最常用的指标。
- en: ROUGE
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ROUGE
- en: 'ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is one
    of the oldest metrics used for evaluating machine translation tasks, but still
    one of the most reliable. It was developed specifically for automatic summarization
    tasks where the goal is to take a long article and sum it up in a short brief.
    Let’s consider the problem: How do you determine whether a summary is correct?
    The simplest method would be to compare it to a known summary—a ground truth,
    if you will. However, no matter the article, there’s often thousands of ways you
    could choose to simplify the text to be more concise, and you don’t want to penalize
    a model simply because it chose a different word order than the ground truth;
    this would only lead to overfitting.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE，即“面向检索的摘要评估的辅助研究”，是用于评估机器翻译任务的最古老的指标之一，但仍然是最可靠的。它是专门为自动摘要任务开发的，其目标是把一长篇文章总结成简短的摘要。让我们考虑一下这个问题：你如何确定一个摘要是否正确？最简单的方法就是将其与一个已知的摘要——即“真实情况”——进行比较。然而，无论文章如何，通常都有成千上万种方法可以用来简化文本，使其更加简洁，你不想仅仅因为模型选择了与真实情况不同的词序就惩罚它；这只会导致过度拟合。
- en: Rouge doesn’t compare the generated summary to the ground truth summary expecting
    an exact match; instead, it looks for overlaps between the two summaries using
    N-grams—the greater the overlap, the higher the score. This is similar to how
    a full-text search engine works. There are multiple variations depending on what
    N is for the N-gram, but there is also a version that compares longest common
    subsequences and versions that compare skip-bigrams, which are any pair of words
    in their sentence order and not necessarily right next to each other.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Rouge不将生成的摘要与真实摘要进行比较，期望得到完全匹配；相反，它通过N-gram在两个摘要之间寻找重叠——重叠越大，分数越高。这与全文搜索引擎的工作方式类似。根据N-gram中的N值，有多种变体，但还有比较最长公共子序列的版本，以及比较跳过双词（skip-bigrams）的版本，跳过双词是指句子中任何两个按顺序排列的词，而不一定是紧挨着的。
- en: The original implementation of ROUGE was written in Perl, and we remember having
    to use it even a couple of years ago. Easily some of the worst days of one author’s
    career were having to work in Perl. Thankfully, it seems that in the last year
    or so, there have finally been fast, stable reimplementations in Python. In the
    next listing, we use the rouge-score library, which is a reimplementation from
    Google. We’ll compare two explanations of *The Legend of Zelda* and see how well
    they compare.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE的原始实现是用Perl编写的，我们记得甚至在几年前还不得不使用它。对于一位作者来说，一些最糟糕的日子之一就是不得不在Perl中工作。幸运的是，在去年或更早的时候，终于有了快速、稳定的Python重实现。在下一个列表中，我们使用rouge-score库，这是Google的一个重实现。我们将比较两个关于《塞尔达传说》的解释，看看它们如何比较。
- en: Listing 4.1 Using ROUGE
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1 使用ROUGE
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Example N-gram where N=1 and also using the longest common subsequence'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 示例 N-gram，其中 N=1，并使用最长公共子序列'
- en: As you can see from the example, even though these two texts are quite different
    syntactically, they are both accurate descriptions. Because of this, instead of
    giving a big fat zero for the score, ROUGE gives a little more flexibility and
    a better comparison with similarity scores around 0.25\. The ROUGE algorithm is
    a fast and effective way to quickly compare the similarity between two short bodies
    of text. ROUGE is very common in the industry, and many benchmarks use it as one
    of their metrics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从示例中可以看到，尽管这两段文本在语法上相当不同，但它们都是准确的描述。正因为如此，ROUGE（Recall-Oriented Understudy
    for Gisting Evaluation）算法不会给出一个大的零分，而是给予一定的灵活性，并提供相似度分数约为0.25的更好比较。ROUGE算法是一种快速有效的比较两段短文本相似度的方法。在业界，ROUGE非常常见，许多基准测试都将它作为其指标之一。
- en: BLEU
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BLEU
- en: BLEU, which stands for BiLingual Evaluation Understudy, is the oldest evaluation
    metric we will talk about in this book. It was developed to evaluate machine translation
    tasks and compare methods of translating one language to another. It is very similar
    to ROUGE, where we compare N-grams between a target and a prediction. While ROUGE
    is primarily a recall metric, BLEU is a precision metric, but using standard precision
    can lead to some problems we need to account for.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（BiLingual Evaluation Understudy），代表双语评估助手，是我们将在本书中讨论的最古老的评估指标。它被开发出来用于评估机器翻译任务，并比较将一种语言翻译成另一种语言的方法。它与ROUGE非常相似，我们在目标和预测之间比较N-gram。虽然ROUGE主要是一个召回率指标，但BLEU是一个精确度指标，但使用标准精确度可能会带来一些我们需要考虑的问题。
- en: To understand the problem, we can calculate standard precision with the code
    from listing 4.1\. Replace the target variable with “the cat in the hat” and the
    prediction variable with “cat hat.” Rerun the listing, and you’ll notice the recall
    is 0.4—we got two out of five words correct—but the precision is 1.0, a perfect
    score despite not being very good! This result is because both words “cat” and
    “hat” show up in the target.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个问题，我们可以使用列表4.1中的代码来计算标准精确度。将目标变量替换为“the cat in the hat”，将预测变量替换为“cat hat”。重新运行列表，您会注意到召回率为0.4——我们只正确地识别了五个词中的两个——但精确度为1.0，这是一个完美的分数，尽管表现并不好！这个结果是因为目标中的两个词“cat”和“hat”都出现了。
- en: 'BLEU fixes this by adding two adjustments. The first is straightforward: add
    a brevity penalty. If the prediction is shorter than the target, we’ll penalize
    it. The second adjustment, known as the modified N-gram precision, is a bit more
    complicated, but it allows us to compare a prediction against multiple targets.
    The next listing shows how to use the NLTK library to calculate the BLEU score.
    We are using the same *Zelda* example as we did with ROUGE so you can compare
    results.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU通过添加两个调整来解决这一点。第一个是直接的：添加简洁性惩罚。如果预测比目标短，我们将对其进行惩罚。第二个调整，称为修改后的N-gram精确度，稍微复杂一些，但它允许我们比较预测与多个目标。下面的列表展示了如何使用NLTK库来计算BLEU分数。我们使用与ROUGE相同的*Zelda*示例，这样你可以比较结果。
- en: Listing 4.2 Using BLEU
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2 使用BLEU
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: BLEU has long been an industry standard, as it has been reported several times
    to correlate well with human judgment on translation tasks. In our example, we
    split the sentences, but it would be better to tokenize the sentences instead.
    Of course, you can’t compare BLEU scores that use different tokenizers. On that
    note, SacreBLEU is a variant worth looking at, as it attempts to improve the comparability
    of scores despite different tokenizers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU长期以来一直是行业标准，因为它多次报道与人类在翻译任务上的判断有很好的相关性。在我们的例子中，我们拆分了句子，但最好是将句子进行分词。当然，你不能比较使用不同分词器的BLEU分数。在这方面，SacreBLEU是一个值得关注的变体，因为它试图提高不同分词器分数的可比性。
- en: BPC
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BPC
- en: The bits per character (BPC) evaluation is an example of an entropy-based evaluation
    for language models. These are metrics we try to minimize. We will not dive deeply
    into entropy or perplexity, but we’ll go over an intuitive understanding here.
    Entropy is an attempt to measure information by calculating the average amount
    of binary digits required per character in a language. Entropy is the average
    number of BPC.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每字符比特（BPC）评估是语言模型基于熵的评估的一个例子。这是我们试图最小化的指标。我们不会深入探讨熵或混淆度，但我们将在这里介绍一个直观的理解。熵是尝试通过计算语言中每个字符所需的平均二进制位数来衡量信息。熵是平均每字符比特数。
- en: Perplexity can be broken down into attempting to measure how often a language
    model draws particular sequences from its corpus or vocabulary. This draws directly
    from the model’s tokenization strategy (too many `<UNKS>` equals bad perplexity),
    meaning that a 1:1 comparison between LLMs with different tokenization strategies
    using perplexity—or entropy, for that matter—is impossible. For example, a model
    that tokenizes at the character level will have much lower perplexity than a model
    that tokenizes at the word level but often performs worse overall. That doesn’t
    invalidate either as a metric, as they are very helpful metrics during training
    of the same model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度可以分解为尝试测量语言模型从其语料库或词汇表中抽取特定序列的频率。这直接来源于模型的分词策略（过多的`<UNKS>`等于糟糕的混淆度），这意味着使用混淆度——或者熵，无论如何——在具有不同分词策略的LLM之间进行1:1比较是不可能的。例如，在字符级别进行分词的模型将比在词级别进行分词的模型具有更低的混淆度，但整体表现往往更差。这并不使它们作为指标无效，因为它们在相同模型的训练过程中非常有帮助。
- en: NOTE  Entropy-related metrics are highly related to information theory, which
    we don’t cover. However, we recommend you take a look at these metrics if you’re
    interested in creating or improving evaluation metrics for LLMs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与熵相关的指标高度相关于信息理论，这部分我们未涉及。然而，如果你对创建或改进LLM的评估指标感兴趣，我们建议你查看这些指标。
- en: To drive the point further with a hands-on example, comparing two models that
    use different tokenization strategies is like comparing how good one third-grader
    is at addition with another third-grader’s multiplication ability. Saying one
    is better than the other doesn’t really matter because they’re doing different
    things at the same skill level. The closest you could get to an accurate comparison
    would be having the two third-graders do the same task, say spelling. Then you
    could at least compare apples to apples, as much as possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地通过实际例子来说明这一点，比较使用不同分词策略的两个模型就像比较一个三年级学生在加法上的能力与另一个三年级学生在乘法上的能力一样。说一个比另一个好并没有真正意义，因为他们处于同一技能水平上做不同的事情。最接近准确比较的方法是让两个三年级学生做同样的任务，比如拼写。这样你至少可以尽可能多地比较苹果与苹果。
- en: Now that we have some metrics under our belt, let’s look into benchmark datasets
    that we will run our evaluations on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了一些指标，让我们来看看我们将运行评估的基准数据集。
- en: 4.2.2 Industry benchmarks
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 行业基准
- en: Evaluating language models’ performance is a notoriously difficult problem,
    and many benchmarks have been created to tackle it. In this subsection, we’ll
    discuss several of the most common solutions you are likely to run into and what
    type of problem they are trying to solve. Since benchmarks typically are only
    good at evaluating one quality of a model and LLMs are usually deployed to do
    many general tasks, you will likely need to run several evaluation benchmarks
    to get a full picture of the strengths and weaknesses of your model. As we go
    through this list, don’t think about which metric is better than another, but
    about how they can be used in tandem to improve your overall success.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 评估语言模型性能是一个众所周知的问题，已经创建了多个基准来解决它。在本节中，我们将讨论你可能会遇到的一些最常见解决方案以及它们试图解决的问题类型。由于基准通常只能评估模型的一个质量，而大型语言模型通常被部署来完成许多通用任务，因此你可能需要运行多个评估基准，以全面了解你模型的优点和缺点。在浏览这个列表时，不要考虑哪个指标比另一个指标更好，而要考虑它们如何协同使用来提高你的整体成功率。
- en: GLUE
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GLUE
- en: 'The General Language Understanding Evaluation (GLUE) is essentially a standardized
    test (think ACT, SAT, GRE, etc.) for language models (just “language models” this
    time) to measure performance versus humans and each other on language tasks meant
    to test understanding. When introduced, two problems arose pretty quickly: the
    LMs surpassed human parity on the tasks too fast, and there were doubts about
    whether the tasks demonstrated actual understanding. Similar to when people train
    animals like parrots to speak, the question is always there: Is the parrot actually
    acquiring human language or simply being conditioned to mimic certain sound sequences
    in response to specific stimuli in exchange for food? That said, the GLUE benchmark
    is still valuable for comparing model performance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通用语言理解评估（GLUE）本质上是一个针对语言模型的标准化测试（想想ACT、SAT、GRE等），用于衡量语言模型在语言任务上的表现，这些任务旨在测试对语言的理解。当GLUE被引入时，很快出现了两个问题：语言模型在任务上超越人类平等的速度太快，而且人们对这些任务是否真正展示了实际理解存在怀疑。类似于人们训练像鹦鹉这样的动物说话的情况，问题总是存在：鹦鹉实际上是在学习人类语言，还是仅仅被训练去模仿某些声音序列，以食物作为交换对特定刺激做出反应？尽管如此，GLUE基准对于比较模型性能仍然是有价值的。
- en: GLUE is no longer an industry standard, but it can still give you a fairly quick
    idea of how well your model is performing, especially if you are training on an
    instruction-based dataset and using GLUE to measure few or zero-shot performance
    on new tasks. You can view the leaderboard at [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE不再是行业标准，但它仍然可以给你一个相当快速的了解，了解你的模型表现如何，尤其是如果你正在基于指令数据集进行训练，并使用GLUE来衡量新任务上的少量或零样本性能。你可以查看排行榜，[https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)。
- en: SuperGLUE
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SuperGLUE
- en: 'As stated in the previous section, one problem that came up quickly was human
    parity on the GLUE tasks. To solve this problem, one year after GLUE was developed,
    SuperGLUE was created and contains more difficult and diverse tasks styled in
    the same easy-to-use way as GLUE. Beyond that, because the GLUE nonexpert human
    benchmark was being surpassed so quickly, more expert people were used to generate
    the SuperGLUE benchmark. That said, the SuperGLUE human baselines are in eighth
    place on the leaderboard at the time of this writing, calling into question the
    second problem with GLUE: Do the SuperGLUE tasks adequately measure understanding?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GLUE任务中迅速出现的一个问题是人类平等。为了解决这个问题，在GLUE开发一年后，SuperGLUE被创建出来，它包含更多困难和多样化的任务，其风格与GLUE一样易于使用。除此之外，由于GLUE的非专家人类基准被迅速超越，因此使用了更多专家人士来生成SuperGLUE基准。尽管如此，截至本文写作时，SuperGLUE的人类基准在排行榜上排名第八，这引发了GLUE的第二个问题：SuperGLUE任务是否充分衡量了理解？
- en: 'Considering that models like PaLM 540B, which are beating the human baseline,
    struggle to generate output generally considered acceptable to people, another
    question arises: How much of the training data and evaluation metrics are idealized
    and nonreflective of how we actually use language? There aren’t yet any adequate
    answers to these questions, but they’re helpful to consider when your evaluation
    metrics could be what stands between your model and acceptable performance on
    its task.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到像PaLM 540B这样的模型，它们在击败人类基准时，难以生成人们普遍认为可接受的输出，另一个问题随之而来：训练数据和评估指标中有多少是理想化的，并不反映我们实际使用语言的方式？对于这些问题，目前还没有适当的答案，但当你评估指标可能成为你的模型与任务可接受性能之间的障碍时，考虑这些问题是有帮助的。
- en: In listing 4.3, we show how to run a model against the MultiRC SuperGLUE test.
    The MultiRC dataset contains short paragraphs and asks comprehension questions
    about the content of the paragraph. Let’s go ahead and load the dataset and take
    a quick look at what we are dealing with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.3中，我们展示了如何运行模型对抗MultiRC SuperGLUE测试。MultiRC数据集包含简短的段落，并就段落内容提出理解问题。让我们继续加载数据集，快速看一下我们正在处理的内容。
- en: Listing 4.3 Example SuperGLUE Benchmark
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3 示例SuperGLUE基准
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 SuperGlue has multiple test datasets; options are boolq, cb, copa, multirc,
    record, rte, wic, wsc, wsc.fixed, axb, and axg.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 SuperGlue有多个测试数据集；选项有boolq、cb、copa、multirc、record、rte、wic、wsc、wsc.fixed、axb和axg。'
- en: 'Here we see a paragraph discussing some basic physics around forces along with
    a simple yes-or-no question and its answer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一段讨论关于力的基本物理的段落，以及一个简单的是非问题及其答案：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s go ahead and pull down a small model and run it against the dataset.
    For this example, we’ll print out the model’s generated answer to the correct
    answer to compare qualitatively:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下载一个小型模型并运行它来对抗数据集。在这个例子中，我们将打印出模型生成的答案与正确答案进行定性比较：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Replace this with the correct input for your benchmark.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将这个替换为你的基准的正确输入。'
- en: '#2 We use this to trim out the input.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们用这个来裁剪输入。'
- en: 'From this, you might get results similar to the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，你可能会得到以下类似的结果：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see our model isn’t doing all that great, but we aren’t too concerned;
    we just want to show a SuperGLUE test in action. You may be wondering why we aren’t
    using a metric like ROUGE or BLEU. While we could do so to improve our understanding,
    if you decide to submit results to the SuperGLUE leaderboard, it will want the
    raw generated text.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们的模型表现并不出色，但我们并不太担心；我们只想展示SuperGLUE测试的实际操作。你可能想知道为什么我们不使用像ROUGE或BLEU这样的指标。虽然我们可以这样做来提高我们的理解，但如果你决定将结果提交到SuperGLUE排行榜，它将需要原始生成的文本。
- en: 'NOTE  For more information on how to use SuperGLUE, check out SuperGLUE FAQs:
    [https://super.gluebenchmark.com/faq](https://super.gluebenchmark.com/faq).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：有关如何使用SuperGLUE的更多信息，请参阅SuperGLUE常见问题解答：[https://super.gluebenchmark.com/faq](https://super.gluebenchmark.com/faq)。
- en: 'SuperGLUE does exactly what it sets out to do: be GLUE but super. If you want
    to test your model’s few and zero-shot capabilities, SuperGLUE would be one of
    the ultimate tests. It will show whether your LLM can follow instructions with
    very low perplexity, only generating what is needed and not more. You can look
    at the current SuperGLUE leaderboard at [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE正是它所设定的目标：超越GLUE。如果你想测试你模型的少量和零样本能力，SuperGLUE将是终极测试之一。它将显示你的LLM是否能够以非常低的困惑度遵循指令，只生成所需的内容，而不多余。你可以查看当前的SuperGLUE排行榜：[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)。
- en: MMLU
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MMLU
- en: The Massive Multitask Language Understanding (MMLU) test was developed primarily
    by UC Berkeley in cooperation with several other universities to test deeper knowledge
    than the GLUE tasks. No longer concerned with surface-level language understanding,
    MMLU seeks to test whether a model understands language well enough to answer
    second-tier questions about subjects such as history, mathematics, morality, and
    law. For example, instead of asking, “What did Newton write about gravity?”, ask,
    “What arguments would Newton have gotten into with Einstein?”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 大型多任务语言理解（MMLU）测试主要是由加州大学伯克利分校与几所其他大学合作开发的，旨在测试比GLUE任务更深层次的知识。MMLU不再关注表面语言理解，而是试图测试一个模型是否足够理解语言，能够回答关于历史、数学、道德和法律等主题的第二级问题。例如，与其问“牛顿写了关于重力的什么？”不如问“牛顿会与爱因斯坦就什么问题产生争论？”
- en: MMLU’s questions range in difficulty from an elementary level to an advanced
    professional level, and they test both world knowledge and problem-solving ability.
    They are known to be quite difficult, with unspecialized humans from Mechanical
    Turk only obtaining results slightly better than random with 34.5% accuracy.[⁶](#footnote-156)
    Experts in their field performed much better, but generally only for the portion
    of the test that was their specialty. So when we look at the models’ performance
    on the test, as might be expected, the models, even at the top of SuperGLUE’s
    leaderboard, are barely better than random at applying the language understanding
    to answer questions about it. This test encompasses a much wider range of understanding
    tasks than GLUE and takes a much lower perplexity to pass.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU 的问题难度从基础水平到高级专业水平不等，它们既测试世界知识也测试解决问题的能力。众所周知，这些问题相当困难，来自 Mechanical Turk
    的非专业人员在 34.5% 的准确率下仅能获得略好于随机的成绩。[⁶](#footnote-156) 他们领域的专家表现要好得多，但通常只限于他们专长的部分。因此，当我们观察模型在测试中的表现时，正如预期的那样，即使是
    SuperGLUE 排行榜顶端的模型，在将语言理解应用于回答问题时，也几乎与随机水平相当。这个测试涵盖了比 GLUE 更广泛的理解任务范围，并且需要更低的困惑度才能通过。
- en: Listing 4.4 shows how to run this test. We’ll download the MMLU dataset and
    then, for convenience, run the test against OpenAI’s different models for comparison.
    The code also allows for different levels of few-shot prompting. We haven’t discussed
    this, but we wanted to show an example early. Try adjusting this parameter to
    see how different numbers of examples can improve your overall results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 展示了如何运行这个测试。我们将下载 MMLU 数据集，然后为了方便起见，将测试与 OpenAI 的不同模型进行对比。代码还允许不同级别的少样本提示。我们尚未讨论这一点，但我们想尽早展示一个例子。尝试调整这个参数，看看不同数量的示例如何改善你的整体结果。
- en: Listing 4.4 Example MMLU evaluation
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4 示例 MMLU 评估
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Sets up the model'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置模型'
- en: '#2 Δefines benchmark with specific tasks and shots'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义具有特定任务和射击的基准'
- en: '#3 Runs benchmark'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 运行基准'
- en: MMLU gets at a deeper understanding than any of the previous benchmarks, which
    is promising, and a correlation can be drawn between this test and chat models
    that generally produce human-preferred responses. With deeper understanding, though,
    comes the need for more responsibility in the testing, and ethical concerns are
    beginning to be raised about these evaluations. For example, are the models being
    trained to answer questions about US history truthfully, or are they being evaluated
    on propaganda for an ideal nation? When answering questions about the law, are
    they conditioned to accept any bias the law system may or may not contain? The
    current answer is that models are likely demonstrating a deeper bias when performing
    well on these types of evals, and greater care needs to be taken to ensure that
    the bias presupposed in our evaluations is unharmful and generally accepted.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU 比之前的任何基准测试都更深入地理解了问题，这是很有希望的，并且可以在这项测试和通常产生人类偏好的响应的聊天模型之间建立关联。然而，随着理解的加深，测试中需要承担更多的责任，关于这些评估的伦理问题也开始被提出。例如，这些模型是否被训练来诚实地回答关于美国历史的问题，或者它们是否被评估为理想国家的宣传？在回答关于法律的问题时，它们是否被设定为接受法律体系中可能或可能不包含的任何偏见？目前的答案是，当这些类型的评估表现良好时，模型可能表现出更深层次的偏见，因此需要更加小心，以确保我们评估中预设的偏见是无害且普遍接受的。
- en: 4.2.3 Responsible AI benchmarks
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 负责任的 AI 基准测试
- en: 'Pretty much all industry benchmarks that evaluate LLMs have focused strictly
    on the model’s capability: “Is it able to do the job?” Unfortunately, that’s where
    most industry benchmarks end. It’s quite unfortunate, especially since going a
    step further and evaluating for bias isn’t that different from or more difficult
    than other evaluation techniques. It’s your responsibility, and it makes good
    business sense. Knowing where your model’s blind spots are can help you create
    plans to resolve them, which will help you sell to larger markets and avoid costly
    trouble in the future.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有评估大型语言模型（LLM）的行业基准都严格关注模型的性能：“它能否完成这项工作？”不幸的是，这就是大多数行业基准的终点。这非常遗憾，特别是考虑到进一步评估偏见并不比其他评估技术更不同或更困难。这是你的责任，并且从商业角度来看也是明智的。了解你模型的盲点可以帮助你制定解决这些问题的计划，这将有助于你开拓更大的市场，并避免未来的高昂成本。
- en: The most common approach to evaluating bias is to take what we already know
    and segment the data across diverse groups. Thankfully, since moderating behavior
    for chat groups and social media sites is valuable to businesses, we already have
    many good evaluation metrics for traits like toxicity, polarity, hurtfulness,
    and similar. These metrics aim to measure how much a given text is related to
    violence, hate speech, racism, or profanity and how positive or negative they
    may be or just generally in violation of your company’s terms and conditions.
    Listing 4.5 demonstrates how to use the HONEST evaluation metric, which compares
    how hurtful prompt completions are for different genders. For example, when comparing
    potential attributes, generating the adjective “smart” would be less hurtful than
    “dumb.”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 评估偏差最常见的方法是将我们已知的内容按不同的群体进行数据分段。幸运的是，由于管理聊天组和社交媒体网站的行为对商业有价值，我们已经有许多好的评估指标，如毒性、极性、伤害性和类似指标。这些指标旨在衡量给定文本与暴力、仇恨言论、种族主义或亵渎的关联程度，以及它们可能是多么积极或消极，或者只是普遍违反你公司的条款和条件。列表4.5展示了如何使用HONEST评估指标，该指标比较了不同性别提示完成的伤害程度。例如，当比较潜在属性时，生成形容词“smart”（聪明的）会比“dumb”（愚蠢的）更不具伤害性。
- en: Listing 4.5 The HONEST metric
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5 HONEST指标
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you pick one or more evaluators, you’ll need a dataset of prompts. You
    can find several in Hugging Face’s dataset. WinoBias focuses on gender bias. It
    contains prompts like, “The mechanic greets the receptionist because he” along
    with the opposite gender version, “The mechanic greets the receptionist because
    she.” We would run these prompts through our model and compare the groups. Some
    other datasets worth checking out include CALM, which compares gender and race,
    and WinoQueer, which looks at bias for LGBTQ+ groups. There are many more to check
    out, including datasets for politics, religions, and occupations, to name a few.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了一个或多个评估者，你需要一个提示数据集。你可以在Hugging Face的数据集中找到几个。WinoBias专注于性别偏差。它包含像“修理工问候接待员因为他”这样的提示，以及相反性别的版本，“修理工问候接待员因为她”。我们会将这些提示通过我们的模型，并比较这些群体。还有一些其他的数据集值得检查，包括比较性别和种族的CALM，以及关注LGBTQ+群体偏差的WinoQueer。还有更多其他的数据集，包括政治、宗教和职业等领域的，仅举几例。
- en: 'NOTE  You can learn more about CALM at [https://arxiv.org/abs/2308.12539v1](https://arxiv.org/abs/2308.12539v1),
    and WinoQueer here: [https://arxiv.org/abs/2306.15087](https://arxiv.org/abs/2306.15087).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：您可以在[https://arxiv.org/abs/2308.12539v1](https://arxiv.org/abs/2308.12539v1)了解更多关于CALM的信息，以及WinoQueer在这里：[https://arxiv.org/abs/2306.15087](https://arxiv.org/abs/2306.15087)。
- en: To put this all together, in listing 4.6, we’ll create an evaluation pipeline
    utilizing the Regard metric. The Regard metric looks at the polarity of content—whether
    it is a positive or negative statement. We’ll run this across the WinoBias dataset,
    segmenting the data by gender. Once we’ve run the analysis for each group, we
    can compare the results across the segments and see whether the distributions
    differ. Before reading on, take a guess. Do you think we’ll see more positive
    results for men or women, or will they be the same? What about negative results?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些内容综合起来，在列表4.6中，我们将创建一个使用Regard指标的评价管道。Regard指标关注内容的极性——是正面还是负面陈述。我们将在这个WinoBias数据集上运行这个指标，按性别对数据进行分段。一旦我们对每个群体进行了分析，我们就可以比较各个分段的结果，看看分布是否有所不同。在继续阅读之前，先猜一猜。你认为我们会看到男性或女性的更多正面结果，还是它们会相同？负面结果又会如何？
- en: Listing 4.6 Running an evaluation pipeline on Regard
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.6 在Regard上运行评价管道
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Pulls model, data, and metrics'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 拉取模型、数据和指标'
- en: '#2 Prepares dataset'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 准备数据集'
- en: '#3 Runs through the evaluation pipeline'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 运行评价管道'
- en: '#4 Analyzes results'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 分析结果'
- en: '#5 Prints the mean polarity scores'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 打印平均极性分数'
- en: Surprisingly to many, this example shows that gender polarity is rather comparable
    in our model. A good sign for this model! The bigger takeaway is that you should
    be automating your evaluations and running pipelines across many metrics, including
    looking for bias, not just performance. Overall, there are still many opportunities
    to improve evaluations and metrics in this space, especially when creating datasets
    and finetuning models to reduce bias. We expect to see lots of growth and innovation
    in this area of research.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让很多人感到惊讶的是，这个例子表明在我们的模型中，性别极性相当相似。这对这个模型是个好兆头！更大的启示是，你应该自动化你的评估，并在多个指标上运行管道，包括寻找偏差，而不仅仅是性能。总的来说，在这个领域，还有很多机会改进评估和指标，尤其是在创建数据集和微调模型以减少偏差时。我们预计在这个研究领域将看到大量的增长和创新。
- en: 4.2.4 Developing your own benchmark
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 开发自己的基准
- en: 'Overall, developing good benchmark datasets is still an unsolved problem. This
    is partly because once we develop one, our models quickly surpass it, making it
    obsolete and no longer “good.” There will be times when we discover edge cases
    for our model, such as parts of speech or certain tasks where it seems to struggle—maybe
    that’s playing chess or identifying sarcasm. Spoiler alert: LLMs are still terrible
    at these tasks, and if you haven’t seen a GPT versus Stockfish video yet, you’re
    in for a treat. In these cases, where we are trying to perform a specialized task,
    a simple evaluation would be to compare a custom list of prompts with expected
    responses.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，开发良好的基准数据集仍然是一个未解决的问题。这部分的理由是因为一旦我们开发出一个，我们的模型很快就会超越它，使其变得过时，不再“良好”。有时我们会发现我们模型的边缘情况，比如词性或某些似乎有困难的任务——可能是下棋或识别讽刺。剧透一下：大型语言模型在这些任务上仍然很糟糕，如果你还没有看过GPT与Stockfish的视频，那你将大饱眼福。在这些我们试图执行专门任务的情况下，一个简单的评估方法就是比较自定义的提示列表与预期的响应。
- en: We recommend first checking out OpenAI’s Evals library ([https://github.com/openai/evals](https://github.com/openai/evals)),
    where OpenAI has open sourced its evaluations. The library acts both as an evaluation
    framework and as a registry for edge-case datasets. At the time of this writing,
    the library contains almost 400 different datasets and is a great place to get
    started and contribute. This library gives you access to the same evaluation standards
    that OpenAI uses for their state-of-the-art models, and they’ve already done most
    of the heavy lifting in identifying areas of interest and curating datasets for
    these areas.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议首先查看OpenAI的Evals库（[https://github.com/openai/evals](https://github.com/openai/evals)），在那里OpenAI开源了它的评估。这个库既是一个评估框架，也是一个边缘情况数据集的注册库。在撰写本文时，该库包含近400个不同的数据集，是一个开始和贡献的好地方。这个库为你提供了与OpenAI用于其最先进模型相同的评估标准，并且他们已经完成了大部分繁重的工作，确定了感兴趣的区域并为这些区域整理了数据集。
- en: As with most libraries built for a specific company but subsequently open sourced,
    it can be a bit of a pain to generalize. Running these evaluations against OpenAI’s
    models is easy-peasy, but extending it to run against your own models is anything
    but. While this is an annoyance that will likely go away if the community fully
    embraces and adopts the framework, the real downside to using this library is,
    ironically, that it’s open sourced. Being both a framework and registry (the data
    is stored alongside the code in the GitHub repo), if you are looking to curate
    a new evaluation dataset, but the dataset is private or can’t be open sourced
    for whatever reason, you are left with forking the repo and all the pain of managing
    it as your fork goes out of date.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就像大多数为特定公司构建但随后开源的库一样，它可能有点难以泛化。运行这些评估针对OpenAI的模型是轻而易举的，但扩展到运行你自己的模型却并非易事。虽然这可能会随着社区完全接受和采用这个框架而消失，但使用这个库的真正缺点是，讽刺的是，它是开源的。作为一个框架和注册库（数据存储在GitHub仓库中的代码旁边），如果你想要整理一个新的评估数据集，但数据集是私有的或由于某种原因不能开源，那么你只能通过分叉仓库并管理它，因为你的分叉版本会过时。
- en: Another library to pay attention to is Hugging Face’s Evaluate. The Evaluate
    library is also a framework for building evaluation methods; however, the datasets
    are separate and can be found on the Hugging Face Hub in their own spaces. Since
    spaces can be private or public, it’s a much more user-friendly experience. Hugging
    Face has custom metrics and all the standard benchmarks already discussed in this
    chapter, as well as several not discussed. In listing 4.7, we show how to use
    the Evaluate library to get SQuAD metrics. SQuAD stands for the Stanford Question
    Answering Dataset, which is an older dataset with 100K questions and answers.
    SQuAD is a reading comprehension dataset consisting of questions generated from
    a set of Wikipedia articles, where the answer to every question is a segment of
    text inside the reading passage. The SQuAD metrics are a set of custom metrics
    that consist of an exact match; F1 scores were used in the paper introducing the
    dataset.[⁷](#footnote-157)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要关注的库是Hugging Face的Evaluate。Evaluate库也是一个构建评估方法的框架；然而，数据集是分开的，可以在Hugging
    Face Hub的独立空间中找到。由于空间可以是私有的或公共的，这提供了一个更加用户友好的体验。Hugging Face有自定义的度量标准和本章中已经讨论的所有标准基准，以及一些未讨论的。在列表4.7中，我们展示了如何使用Evaluate库获取SQuAD度量标准。SQuAD代表斯坦福问答数据集，这是一个包含10万个问题和答案的旧数据集。SQuAD是一个阅读理解数据集，由一组维基百科文章生成的问题组成，每个问题的答案都是阅读段落中的文本片段。SQuAD度量标准是一组自定义度量标准，包括精确匹配；在介绍数据集的论文中使用了F1分数。[⁷](#footnote-157)
- en: Listing 4.7 Using the Evaluate library to run SQuAD
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7 使用Evaluate库运行SQuAD
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Δownloads a metric from Hugging Face''s Hub'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从Hugging Face的Hub下载度量标准'
- en: '#2 Example from the SQuAΔ dataset'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 来自SQuAΔ数据集的示例'
- en: If you are creating your own benchmark, with the Evaluate library, you can easily
    create your own metric in a metric space and the dataset to use with the metric.
    This process isn’t too difficult. If you’ve decided not to create your own, the
    hardest part is finding good metrics. Searching through the hub is one thing,
    but since anyone can upload a metric and dataset, you never know if what you find
    is all that good, well curated, or clean.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在创建自己的基准，使用Evaluate库，你可以轻松地在度量空间中创建自己的度量标准以及与度量标准一起使用的数据集。这个过程并不太难。如果你已经决定不创建自己的，那么最困难的部分是找到好的度量标准。在中心搜索是一回事，但由于任何人都可以上传度量标准和数据集，你永远不知道你找到的是否都很好，是否经过精心整理，或者是否干净。
- en: We haven’t dug too deeply into actually generating a dataset or metric, as that
    will be very specific to your use case, but what we have discussed are two great
    libraries you can use to do it. Evals is great if you are looking for an already
    curated dataset, and Evaluate is easy to use when generating your own. These tools
    are very useful, but in some special cases, you’ll need to think outside the box,
    and one of those cases that sticks out like a sore thumb is code generation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有深入挖掘实际生成数据集或度量标准的过程，因为这将非常具体于你的用例，但我们讨论的两个非常好的库，你可以用来完成这项工作。如果你正在寻找一个已经整理好的数据集，Evals是个不错的选择，而当你自己生成时，Evaluate则易于使用。这些工具非常有用，但在某些特殊情况下，你需要跳出思维定势，其中最引人注目的一个案例就是代码生成。
- en: 4.2.5 Evaluating code generators
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.5 评估代码生成器
- en: One of the most valuable and sought-after use cases for LLMs is to have them
    help us write code. While we are unaware of any industry standard evaluation metrics
    for evaluating the generated code, thankfully, there are plenty of industry standards
    for evaluating the code itself (e.g., tests, profiles, security scanners, etc.).
    Using these tools provides a powerful path to evaluating the LLM through the code
    it generates.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs来说，最有价值和最受欢迎的使用案例之一就是让他们帮助我们编写代码。虽然我们不知道有任何行业标准的评估指标来评估生成的代码，但幸运的是，有大量的行业标准用于评估代码本身（例如，测试、配置文件、安全扫描器等）。使用这些工具为评估LLM通过它生成的代码提供了一个强大的途径。
- en: 'The basic setup looks like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基本设置看起来是这样的：
- en: Have your model generate code based on docstrings.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让你的模型根据文档字符串生成代码。
- en: Run the generated code in a safe environment on prebuilt tests to ensure they
    work and that no errors are thrown.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预构建的测试中在一个安全的环境中运行生成的代码，以确保它们可以正常工作且不会抛出错误。
- en: Run the generated code through a profiler and record the time it takes to complete.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分析器运行生成的代码，并记录完成所需的时间。
- en: Run the generated code through a security scanner and count the number of vulnerabilities.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过安全扫描器运行生成的代码，并计算漏洞数量。
- en: Run the code against architectural fitness functions to determine artifacts,
    like how much coupling, integrations, and internal dependencies there are.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码与架构适应性函数运行，以确定如耦合、集成和内部依赖等工件。
- en: Run steps 1 to 5 on another LLM.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个 LLM 上运行步骤 1 到 5。
- en: Compare results.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较结果。
- en: Listing 4.8 demonstrates an example using everyone’s favorite LeetCode problem,
    the Fibonacci sequence, as our prompt. This example shows using a separate fibonacci.py
    file as a prompt for our LLM to generate code. We could then use this test file
    to check that it runs correctly and how fast it runs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.8 展示了一个使用大家最喜欢的 LeetCode 问题——斐波那契数列——作为提示的示例。这个示例展示了如何使用单独的 fibonacci.py
    文件作为我们的 LLM 生成代码的提示。然后我们可以使用这个测试文件来检查它是否正确运行以及运行速度如何。
- en: Listing 4.8 An example test for evaluating code generators
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.8 评估代码生成器的示例测试
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Runs tests using pytest and times it'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用 pytest 运行测试并计时'
- en: There is lots of flexibility to this system, but the major downside is that
    it requires you to either create docstrings of coding challenges and write tests
    for them ahead of time or scrape LeetCode. Of course, you could have your LLM
    generate both of those too, but it’s easy to write simple tests that always pass
    and much harder to write tests that cover all the edge cases. So at some point,
    you’ll want a human in the loop.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统有很多灵活性，但主要的缺点是它要求你提前创建文档字符串和编写测试，或者抓取 LeetCode。当然，你也可以让你的 LLM 生成这两者，但编写总是通过简单测试的简单测试很容易，而编写覆盖所有边缘情况的测试则要困难得多。所以，在某个时候，你将需要一个人类参与其中。
- en: 4.2.6 Evaluating model parameters
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.6 评估模型参数
- en: So far, all the evaluation methods we’ve looked at involve running the model
    and checking the results, but there is a lot we can learn by simply looking at
    the model. Surprisingly, there’s a lot you can learn by simply looking at the
    parameters of an ML model. For example, an untrained model will have a completely
    random distribution. By evaluating the distribution and paying attention to distinct
    features of a model’s parameters, we can learn whether a model is over- or undertrained.
    In the next listing, we use the weightwatcher library to do just that on the GPT-2
    model, which will tell us which layers are over- or undertrained.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们查看的所有评估方法都涉及运行模型并检查结果，但通过简单地查看模型，我们可以学到很多东西。令人惊讶的是，通过简单地查看机器学习模型的参数，你可以学到很多东西。例如，一个未训练的模型将有一个完全随机的分布。通过评估分布并关注模型参数的显著特征，我们可以了解模型是过拟合还是欠拟合。在下一个列表中，我们使用
    weightwatcher 库在 GPT-2 模型上执行此操作，这将告诉我们哪些层是过拟合或欠拟合的。
- en: Listing 4.9 Using the weightwatcher library to evaluate GPT-2
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.9 使用 weightwatcher 库评估 GPT-2
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This code prints out the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将打印出以下内容：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Along with summary statistics, weightwatcher provides spectral analysis plots,
    as shown in figure 4.2\. To create these plots, change line 8 in listing 4.9 to
    `plot=True`. The spectral analysis plots evaluate the frequencies of eigenvalues
    for each layer of a model. When evaluating these plots, we care about the tail
    of the distribution—the straighter it is (indicating a nice heavy tail), the better
    trained we expect the layer to be.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 除了总结统计信息外，weightwatcher 还提供光谱分析图，如图 4.2 所示。要创建这些图，请将列表 4.9 中的第 8 行更改为 `plot=True`。光谱分析图评估模型每一层的特征值频率。在评估这些图时，我们关注分布的尾部——它越直（表示有一个很好的重尾），我们预计该层训练得越好。
- en: '![figure](../Images/4-2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4-2.png)'
- en: Figure 4.2 weightwatcher Empirical Spectral Density (ESD) plots generated for
    GPT2’s second layer, which is predicted to be overtrained
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2 为 GPT2 的第二层生成的 weightwatcher 经验光谱密度 (ESD) 图，预测该层过拟合
- en: 'NOTE  These plots are created to mimic Spectral Density plots you might see
    in a physics lab. We will not discuss them in this book, but if interested, we
    recommend you check out the WeightWatchers documentation: [https://github.com/CalculatedContent/WeightWatcher](https://github.com/CalculatedContent/WeightWatcher).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这些图是为了模仿你可能在物理实验室中看到的谱密度图而创建的。本书中不会讨论这些图，但如果感兴趣，我们建议你查看 WeightWatchers 文档：[https://github.com/CalculatedContent/WeightWatcher](https://github.com/CalculatedContent/WeightWatcher)。
- en: weightwatcher is rather powerful, as it allows us to compare different models,
    helping us better understand which model is better trained without running them
    at all, making it relatively inexpensive. This capability comes in handy when
    you are trying to determine which base model to use, as an undertrained model
    may require a lot more finetuning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: weightwatcher相当强大，因为它允许我们比较不同的模型，帮助我们更好地理解哪个模型经过更好的训练，而无需实际运行它们，这使得它相对便宜。当你试图确定要使用哪个基础模型时，这种能力非常有用，因为一个训练不足的模型可能需要更多的微调。
- en: Since we are comparing models based on their parameters alone, this method provides
    a nice agnostic view of the current state of a model. We can implement it during
    and after training and during ongoing updates using methods such as RLHF. It is
    both an easy and powerful evaluation method. However, the downside is that it
    doesn’t provide any insight into the training data, so it can’t tell us which
    model is that effective at which task and is best paired with other evaluation
    methods already discussed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅根据参数来比较模型，这种方法提供了一个很好的无偏见视角，了解当前模型的状态。我们可以在训练期间和之后以及在进行中的更新中使用RLHF等方法来实现它。这是一个既简单又强大的评估方法。然而，缺点是它不提供关于训练数据的任何见解，因此它不能告诉我们哪个模型在哪个任务上最有效，以及它最好与已经讨论过的其他评估方法相结合。
- en: 'We’ve already spent quite a bit of time talking about data most data engineers
    likely don’t think about often: model weights and evaluation data. These are crucial
    ingredients to gather to generate a specialized finetuned LLM. Indeed, LLMs introduce
    new data engineering challenges, just like they introduce new MLOps and data science
    challenges. Next, we will discuss what many of you have been waiting for: the
    training data. We’ll discuss different datasets that are essential to know about,
    where to get them, and how to prepare them to train or finetune LLMs.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了很多时间讨论数据，这些数据大多数数据工程师可能不会经常考虑：模型权重和评估数据。这些是生成专用微调LLM的关键成分。确实，LLMs引入了新的数据工程挑战，就像它们引入了新的MLOps和数据科学挑战一样。接下来，我们将讨论你们很多人一直在等待的内容：训练数据。我们将讨论必须了解的不同数据集，在哪里可以获得它们，以及如何准备它们以训练或微调LLMs。
- en: 4.3 Data for LLMs
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 LLMs的数据
- en: It has been shown that data is the most important part of training an LLM. We
    hope that the sudden importance of language modeling will persuade businesses
    to start managing their data generally according to accepted guidelines. As is
    shown by experiments like LLaMA, Alpaca, Goat, Vicuna, and later, LIMA[⁸](#footnote-158)
    and SpQR,[⁹](#footnote-159) high-quality training data and clever modeling are
    much more important than the number of parameters or size of training data. Measuring
    that quality is still a point of difficulty in general; however, we’ll discuss
    methodologies you can employ to do so.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，数据是训练LLM（大型语言模型）最重要的部分。我们希望语言模型突然的重要性能够说服企业开始根据公认的指南一般性地管理他们的数据。正如LLaMA、Alpaca、Goat、Vicuna等实验所显示的，以及后来的LIMA[⁸](#footnote-158)和SpQR[⁹](#footnote-159)，高质量的训练数据和巧妙的建模比参数数量或训练数据的大小更为重要。然而，衡量这种质量仍然是一个难点；然而，我们将讨论你可以采用的方法来做到这一点。
- en: We’ll first discuss common datasets you should know about, what’s in them, why
    you would want them, and where you can get them. Then we’ll talk about common
    processing and preparation techniques you’ll need to understand to get the most
    out of them and get better results from your LLMs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将讨论你应该了解的常见数据集，它们包含什么，为什么你需要它们，以及你可以在哪里获得它们。然后我们将讨论你需要了解的常见处理和准备技术，以便充分利用它们并获得更好的LLMs（大型语言模型）结果。
- en: 4.3.1 Datasets you should know
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 你应该了解的数据集
- en: If you didn’t notice, in section 4.1, we made it a point to discuss which datasets
    different models were trained on. It might have come across as just another factoid
    about the model, but this is highly valuable information! Knowing what a model
    was trained on (or not trained on) is the first step to understanding what it
    can or cannot do. For example, knowing an LLM coding model was trained heavily
    on the C programming language but didn’t see a lick of C++ will be more than enough
    to realize why it seems to work syntactically but produces so many errors and
    bugs when writing C++ code.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有注意到，在第4.1节中，我们特别指出讨论了不同模型训练所使用的数据集。这可能会被看作是关于模型的另一个事实，但这是非常有价值的信息！了解一个模型是基于什么（或没有基于什么）训练的，是理解它能做什么或不能做什么的第一步。例如，如果一个LLM编码模型在C编程语言上进行了大量训练，但没有接触过C++，那么这足以解释为什么它在编写C++代码时似乎工作得很好，但产生了许多错误和漏洞。
- en: Wikitext
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wikitext
- en: One of the most familiar datasets, Wikitext is, as the name implies, essentially
    Wikipedia. It was crafted by the Salesforce team back in 2016\. It is a great
    dataset to turn to when you’re only trying to do a proof of concept or a rapid
    prototype since the English version comes in at only 741 MB, not even 1 GB. Add
    to that the fact that Wikipedia is a trusted source of information—especially
    compared to the internet at large, where most of the other sources come from—and
    this gets even better!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最熟悉的数据集之一，Wikitext，正如其名所示，本质上就是维基百科。它是由 Salesforce 团队在 2016 年制作的。当你只想进行概念验证或快速原型设计时，这是一个很好的数据集，因为其英语版本仅为
    741 MB，甚至不到 1 GB。再加上维基百科是一个可靠的信息来源——尤其是与互联网整体相比，其中大部分其他来源都来自互联网——这使得它更加出色！
- en: 'Some downsides: it is purely an English dataset, which greatly reduces the
    diversity of tokens the model will see; Wikipedia contains an idealized version
    of language—one that we subjectively value as clear—even though it doesn’t contain
    any instances of how language is actually used, only meta-explanations on usage.
    Also, it’s almost a decade old as of this writing, which, of course, no one checks.
    We’ve seen many teams use it to quickly prototype and create Q&A bots due to its
    ease of use and access. It does well in prototyping but always comes off as unimpressive
    when it gets to production, as users tend to prefer asking questions about current
    events. Always check the freshness of your data! Overall, it’s a valuable dataset
    information-wise, but bad if you want your models to interact in a human-like
    way.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些缺点：它纯粹是一个英语数据集，这大大减少了模型将看到的标记多样性；维基百科包含了一种理想化的语言版本——我们主观上认为它很清晰——即使它不包含任何关于语言实际使用的实例，只有关于使用的元解释。此外，截至本文撰写时，它几乎有十年历史，当然，没有人会检查。我们见过许多团队因为其易用性和可访问性而快速原型设计和创建问答机器人。它在原型设计方面做得很好，但在进入生产阶段时总是显得不够令人印象深刻，因为用户倾向于询问关于当前事件的问题。总是检查你数据的时效性！总的来说，它是一个信息价值很高的数据集，但如果你希望你的模型以类似人类的方式交互，那就不好了。
- en: Wiki-40B
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wiki-40B
- en: 'A good alternative is Wiki-40B from 2020, a cleaned-up version of Wikitext
    with 40 different language variations. It comes in at a little over 10 GB. So
    it’s still quite small for prototyping. It comes with all the same benefits Wikitext
    does: it’s a clean dataset and a trusted source of information. Plus, it’s newer
    and has more languages. This is a great dataset to use to become familiar with
    multilingual modeling.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的替代方案是 2020 年的 Wiki-40B，它是 Wikitext 的一个经过清洗的版本，包含 40 种不同的语言变体。它的体积略超过 10
    GB。所以它对于原型设计来说仍然相当小。它具有与 Wikitext 相同的所有好处：它是一个干净的数据集，是一个可靠的信息来源。此外，它更新，包含更多语言。这是一个了解多语言建模的绝佳数据集。
- en: Europarl
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Europarl
- en: One of the best toy datasets for multilingual problems, Europarl contains the
    European Parliament proceedings from 1996 to 2011\. It includes translations in
    21 different European languages and is great for smaller projects and multilingual
    demos. Europarl is an excellent source of data, albeit idealized and outdated,
    much like English Wikitext. In addition, the project includes many parallel corpora,
    which are paired down to English and one of the 20 other languages. The total
    dataset is just 1.5 GB and can be found at [https://www.statmt.org/europarl/](https://www.statmt.org/europarl/).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言问题中最好的玩具数据集之一，Europarl 包含了 1996 年至 2011 年的欧洲议会会议记录。它包含 21 种不同的欧洲语言的翻译，非常适合小型项目和多语言演示。尽管理想化且过时，但
    Europarl 是一个优秀的数据来源，就像英语维基文本一样。此外，该项目还包括许多平行语料库，这些语料库被缩减为英语和 20 种其他语言之一。整个数据集仅为
    1.5 GB，可在 [https://www.statmt.org/europarl/](https://www.statmt.org/europarl/)
    找到。
- en: Common Crawl
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Common Crawl
- en: The Common Crawl dataset is essentially the entire internet, web scraped and
    open sourced. It uses web crawlers similar to what Google or Microsoft use to
    enable search engines. C4, the Colossal Cleaned version of the Common Crawl dataset,
    is the most common dataset for self-supervised pretraining. Unfortunately, being
    cleaned doesn’t mean it is free of inherent societal bias, which is true for pretty
    much all the datasets openly available today. Containing the entirety of the internet
    means it contains all the good and the bad; it is a very diverse dataset full
    of multiple languages and code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl 数据集本质上就是整个互联网，通过网络爬虫抓取并开源。它使用与谷歌或微软类似的网络爬虫来启用搜索引擎。C4，即 Common Crawl
    数据集的巨大清洗版本，是最常见的自监督预训练数据集。不幸的是，经过清洗并不意味着它没有固有的社会偏见，这在今天公开可用的几乎所有数据集中都是真实的。包含整个互联网意味着它包含了所有的好与坏；它是一个包含多种语言和代码的非常多样化的数据集。
- en: 'The Common Crawl dataset is named after the nonprofit organization of the same
    name that is dedicated to providing a copy of the internet to anyone for the purpose
    of research and analysis. You can access the dataset at [https://commoncrawl.org/](https://commoncrawl.org/),
    where you will find many versions because Common Crawl periodically crawls the
    web and updates the dataset. The community has been archiving the internet since
    2008\. It comes in four variants to help with your various needs: a 305 GB version
    containing the actual C4; a 380 GB version that contains so-called bad words along
    with everything else; a 2.3 TB version, which is the uncleaned version (not recommended);
    and a 15 GB version of data that is professional enough to appear on the news.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl 数据集以同名非营利组织命名，该组织致力于向任何人提供互联网的副本，用于研究和分析。您可以在 [https://commoncrawl.org/](https://commoncrawl.org/)
    访问该数据集，在那里您会发现许多版本，因为 Common Crawl 定期抓取网络并更新数据集。自 2008 年以来，该社区一直在存档互联网。它有四种变体，以满足您的各种需求：包含实际
    C4 的 305 GB 版本；包含所谓的不良词汇以及所有其他内容的 380 GB 版本；2.3 TB 的未清理版本（不推荐使用）；以及足够专业以出现在新闻中的
    15 GB 数据版本。
- en: OpenWebText
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenWebText
- en: Another dataset we’d recommend for pretraining is OpenWebText, which only takes
    up 55 GB on disk. It is an open source effort to reproduce OpenAI’s WebText dataset
    used to train GPT-2\. Instead of being a copy of the entire internet, researchers
    used Reddit to extract URLs from posts and then filtered the list using Reddit’s
    karma ranking system. They then scraped the URLs to create the dataset. Since
    the content mainly comes from Reddit, it calls into question its real-world accuracy
    due to the selection bias of only including people with a Reddit account. It is
    made up mostly of news articles, blog posts, and other content often shared on
    forums. You can think of it as a highly curated and much smaller version of the
    Common Crawl dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还推荐另一个用于预训练的数据集是 OpenWebText，它在磁盘上仅占用 55 GB。这是一个开源项目，旨在重现 OpenAI 的 WebText
    数据集，该数据集用于训练 GPT-2。研究人员没有复制整个互联网，而是使用 Reddit 从帖子中提取 URL，然后使用 Reddit 的 karma 排名系统过滤列表。然后他们抓取
    URL 来创建数据集。由于内容主要来自 Reddit，它因仅包括拥有 Reddit 账户的人的选择偏差而对其现实世界的准确性提出了质疑。它主要由新闻文章、博客文章和其他在论坛上经常分享的内容组成。您可以将其视为一个高度精选且规模较小的
    Common Crawl 数据集版本。
- en: Like Wikitext, it’s a bit older; the most commonly used version was created
    in 2019, and a new version hasn’t been updated in four years at the time of writing.
    Of course, since the dataset was curated with a specific methodology, it could
    be refreshed at any time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Wikitext 类似，它稍微有些旧；最常用的版本是在 2019 年创建的，截至写作时已有四年未更新。当然，由于数据集是根据特定方法精心挑选的，它可以在任何时候进行更新。
- en: The Pile
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: The Pile
- en: One dataset that has garnered a lot of attention and should be on your radar
    is The Pile, which was created by EleutherAI in 2020 and published on December
    31 of the same year.[^(10)](#footnote-160) It is useful for self-supervised pretraining
    tasks. The Pile is one of the largest datasets we’ll discuss at 825 GB and consists
    of 22 smaller high-quality datasets combined to make a diverse and dense training
    set. It includes most of the datasets we have already discussed, like Common Crawl,
    OpenWebText, and Wikipedia. It also contains book datasets, like Books3 and Gutenberg;
    code datasets, like GitHub and Stack Exchange; and specialist datasets, like PubMed
    and FreeLaw. It also includes datasets like the Enron Emails, which we can’t help
    but think was a mistake.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一个备受关注且应引起您注意的数据集是 The Pile，它由 EleutherAI 于 2020 年创建，并于同年 12 月 31 日发布。[^(10)](#footnote-160)
    它适用于自监督预训练任务。The Pile 是我们将讨论的最大的数据集之一，达到 825 GB，由 22 个较小的、高质量的数据集组合而成，以形成一个多样化和密集的训练集。它包括我们之前讨论过的许多数据集，如
    Common Crawl、OpenWebText 和维基百科。它还包含书籍数据集，如 Books3 和 Gutenberg；代码数据集，如 GitHub 和
    Stack Exchange；以及专业数据集，如 PubMed 和 FreeLaw。它还包括像 Enron Emails 这样的数据集，我们不禁怀疑这是一个错误。
- en: 'Because it’s so massive and includes multiple languages and code samples, it
    has proven useful in training many LLMs. It is multilingual in addition to dense,
    making it ideal for learning sparse general language representations. Overall,
    though, it’s not very clean and is essentially just a conglomerate of multiple
    datasets. Unless you are training LLMs from scratch, you likely won’t use this
    dataset, but it’s important to become familiar with it, as many of the largest
    models have been trained on it. You can find the dataset at EleutherAI’s website:
    [https://pile.eleuther.ai/](https://pile.eleuther.ai/).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它规模庞大，包含多种语言和代码示例，所以在训练许多大型语言模型（LLMs）方面已被证明非常有用。它不仅内容密集，而且多语言，这使得它非常适合学习稀疏的通用语言表示。然而，总的来说，它并不十分整洁，本质上只是多个数据集的集合。除非你从头开始训练LLMs，否则你很可能不会使用这个数据集，但了解它很重要，因为许多最大的模型都是基于它训练的。你可以在EleutherAI的网站上找到这个数据集：[https://pile.eleuther.ai/](https://pile.eleuther.ai/).
- en: RedPajama
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RedPajama
- en: RedPajama is a dataset created by a collaboration of Together.ai, Ontocord.ai,
    ETH DS3Lab, Stanford CRFM, and Hazy Research. The goal was to create a fully open
    dataset that mimicked what was described in the LLaMA paper.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: RedPajama是由Together.ai、Ontocord.ai、ETH DS3Lab、斯坦福CRFM和Hazy Research合作创建的数据集。目标是创建一个完全开源的数据集，模仿LLaMA论文中描述的内容。
- en: 'NOTE  You can read the blog post introducing RedPajama here: [https://together.ai/blog/redpajama](https://together.ai/blog/redpajama).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：你可以在以下链接中阅读介绍RedPajama的博客文章：[https://together.ai/blog/redpajama](https://together.ai/blog/redpajama).
- en: 'The dataset is similar to The Pile but much larger at 5 TB and newer, published
    in April 2023\. It contains fewer datasets: GitHub, arXiv, Books, Wikipedia, StackExchange,
    and Common Crawl. It is so large because it contains five different dumps of the
    Common Crawl dataset with varying filters and the standard C4 dataset. It is made
    available through the Hugging Face Hub and can be found at [https://mng.bz/4ppD](https://mng.bz/4ppD).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集与The Pile相似，但更大，达到5 TB，且更新更近，于2023年4月发布。它包含的数据集较少：GitHub、arXiv、书籍、维基百科、StackExchange和Common
    Crawl。它之所以如此之大，是因为它包含了五个不同过滤器和标准C4数据集的Common Crawl数据集的转储。它通过Hugging Face Hub提供，可以在[https://mng.bz/4ppD](https://mng.bz/4ppD)找到。
- en: OSCAR
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OSCAR
- en: The best dataset by far to train on for multilingual models is OSCAR, which
    is larger than any other dataset discussed, coming in at 9.4TB, over 11 times
    as big as The Pile! It is an open source project started in 2019 and has been
    funded by a multitude of institutes and governments. You can learn more about
    the project and dataset at [https://oscar-project.org/](https://oscar-project.org/).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练多语言模型来说，迄今为止最好的数据集是OSCAR，它比任何其他讨论过的数据集都要大，达到9.4TB，是The Pile的11倍以上！这是一个始于2019年的开源项目，并由多个机构和政府资助。你可以在[https://oscar-project.org/](https://oscar-project.org/)了解更多关于这个项目和数据集的信息。
- en: This project is actively being worked on, and new releases come out annually
    with regular updates. It currently supports 166 languages at the time of this
    writing, much more than any other dataset. As a work in progress, though, there
    are some languages much more represented than others, with some in the TBs of
    data and others in KBs. This is one of our favorite datasets because it is actively
    being worked on, and the team is passionate about representation in LLMs and AI,
    as well as producing highly clean, high-quality data. We encourage all interested
    readers to contribute to this dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目正在积极开发中，并且每年都会发布新的版本，进行定期更新。截至本文写作时，它支持166种语言，比任何其他数据集都要多。然而，作为一个正在进行的项目，某些语言的数据量比其他语言多得多，有的数据量达到TB级别，有的只有KB级别。这是我们最喜欢的数据集之一，因为它正在积极开发中，团队对LLMs和AI中的代表性以及生产高度清洁、高质量数据充满热情。我们鼓励所有感兴趣的读者为此数据集做出贡献。
- en: Summary of datasets
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集概览
- en: In table 4.2, you can see a summary of the datasets we’ve discussed so far.
    These datasets are all commonly used in industry and worth familiarizing yourself
    with. We encourage you to investigate them further and take a closer look at the
    data within.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在表4.2中，你可以看到我们迄今为止讨论过的数据集的概览。这些数据集在工业界都普遍使用，值得你熟悉。我们鼓励你进一步调查它们，并更仔细地查看其中的数据。
- en: Table 4.2 Summary of datasets
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.2 数据集概览
- en: '| Dataset | Contents | Size | Last update |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 内容 | 大小 | 最后更新 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Wikitext  | English Wikipedia  | <1 GB  | 2016  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Wikitext | 英文维基百科 | <1 GB | 2016 |'
- en: '| Wiki-40B  | Multi-lingual Wikipedia  | 10 GB  | 2020  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Wiki-40B | 多语言维基百科 | 10 GB | 2020 |'
- en: '| Europarl  | European Parliament proceedings  | 1.5 GB  | 2011  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Europarl | 欧洲议会会议记录 | 1.5 GB | 2011 |'
- en: '| Common Crawl  | The internet  | ~300 GB  | Ongoing  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Common Crawl | 互联网 | ~300 GB | 持续更新中 |'
- en: '| OpenWebText  | Curated internet using Reddit  | 55 GB  | 2019  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| OpenWebText | 使用 Reddit 精选的互联网内容 | 55 GB | 2019 |'
- en: '| The Pile  | Everything above plus specialty datasets (books, law, med)  |
    825 GB  | 2020  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| The Pile | 包括所有上述内容以及专业数据集（书籍、法律、医学） | 825 GB | 2020 |'
- en: '| RedPajama  | GitHub, arXiv, Books, Wikipedia, StackExchange, and multiple
    version of Common Crawl  | 5 TB  | 2023  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| RedPajama | GitHub、arXiv、书籍、维基百科、StackExchange 和多个版本的 Common Crawl | 5 TB
    | 2023 |'
- en: '| OSCAR  | Highly curated multilingual dataset with 166 languages  | 9.4 TB  |
    Ongoing  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| OSCAR | 高度精选的多语言数据集，包含 166 种语言 | 9.4 TB | 持续更新中 |'
- en: Corpora
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语料库
- en: As you probably picked up on, most of the datasets out there are essentially
    just text dumps of the internet. If you’re looking for something with a little
    more finesse, something that contains more meta info to help your model disambiguate
    for more complex tasks, consider downloading a corpus. A corpus is just like a
    dataset, except it is more easily searchable, visualized, and explained. Corpora
    are often paid datasets that can be well worth your money. Corpora, like the Corpus
    Of Historical American English (COHA) and the Corpus of Contemporary American
    English (COCA), are excellent downloads. They contain not just text data but also
    frequency analysis (bag of words) and collocates (N-grams), all ready to go. Whether
    or not you are interested in the applications of allowing models to analyze metadata
    as part of training, using corpora can help with model explainability and quality
    of data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所可能注意到的，大多数数据集本质上只是互联网上的文本堆栈。如果你在寻找更精细的东西，包含更多元信息以帮助你的模型在更复杂的任务中进行区分的东西，考虑下载一个语料库。语料库就像一个数据集，但它更容易搜索、可视化和解释。语料库通常是付费数据集，可能非常值得你的投资。语料库，如历史美国英语语料库（COHA）和当代美国英语语料库（COCA），是优秀的下载资源。它们不仅包含文本数据，还包括频率分析（词袋）和搭配（N-gram），一切准备就绪。无论你是否对允许模型在训练过程中分析元数据感兴趣，使用语料库都可以帮助提高模型的可解释性和数据质量。
- en: You can think of a corpus as a vector database that has already been highly
    cleaned and curated and is ready to go. While it hasn’t yet been done, a corpus
    that combines both the linguistic explainability and time-series bucketing with
    precalculated embeddings put into a real-time vector database would likely be
    invaluable and highly profitable in this field for the foreseeable future, especially
    if both textual and audio data are captured. If your company has its own language
    data it wants to train on, your best course of action is to create a corpus where
    your biggest job is saying where data came from when and what the overall goal
    of the data going into the model is. Almost every NLP library has strategies for
    creating corpora, from NLTK to spaCy and even LangChain. Be mindful about which
    strategies and tools you pick because at the end of the day, your dataset or corpus
    contains everything your model will see.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将语料库想象成一个已经高度清洗和精选的向量数据库，可以直接使用。虽然尚未实现，但一个结合了语言可解释性、时间序列分桶和预计算的嵌入并将其放入实时向量数据库的语料库，在可预见的未来，在这个领域可能会非常有价值且利润丰厚，尤其是如果同时捕获了文本和音频数据。如果你的公司有自己的语言数据想要进行训练，你最好的做法是创建一个语料库，其中你的主要任务是说明数据来自何时何地，以及数据进入模型的整体目标。几乎每个
    NLP 库都有创建语料库的策略，从 NLTK 到 spaCy，甚至 LangChain。请注意你选择的策略和工具，因为最终，你的数据集或语料库包含了模型将看到的一切。
- en: 4.3.2 Data cleaning and preparation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 数据清洗和准备
- en: If you pulled any of the previously mentioned datasets, you might be surprised
    to realize most of them are just giant text dumps—a large parquet or text file.
    There are no labels or annotations, and feature engineering hasn’t been done at
    all. LLMs are trained via self-supervised methods to predict the next word or
    a masked word, so a lot of traditional data cleaning and preparation processes
    are unneeded. This fact leads many to believe that data cleaning as a whole is
    unnecessary, but this couldn’t be further from the truth. Datasets are the lifeblood
    of all ML, and they are so much more than a pile of data. Yet that’s what most
    businesses have—a pile of data. Data cleaning and curation are difficult, time-consuming,
    and ultimately subjective tasks that are difficult to tie to key performance indicators
    (KPIs). Still, taking the time and resources to clean your data will create a
    more consistent and unparalleled user experience.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了之前提到的任何数据集，你可能会惊讶地发现其中大部分只是巨大的文本堆——一个大型的parquet或文本文件。没有任何标签或注释，而且特征工程根本就没有进行。LLMs通过自监督方法训练，以预测下一个单词或一个被遮蔽的单词，因此许多传统的数据清理和准备过程都是不必要的。这一事实导致许多人认为数据清理整体上是不必要的，但这与事实相去甚远。数据集是所有机器学习的生命线，它们远不止是一堆数据。然而，这正是大多数企业所拥有的——一堆数据。数据清理和整理是困难、耗时且最终具有主观性的任务，难以与关键绩效指标（KPIs）联系起来。尽管如此，花时间和资源清理你的数据将创造一个更一致且无与伦比的用户体验。
- en: Since the 1990s, people have tested whether Big Data can produce better results
    than high-quality data; we believe the answer is no. Big Data is nowhere close
    to devoid of value. The Law of Big Numbers has been applied, and it has shown
    that models can generate convincing syntax at the same level as people. However,
    as we’ve said before, models have also soundly demonstrated that syntax is in
    no way connected to semantics or pragmatics.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年代以来，人们一直在测试大数据是否能产生比高质量数据更好的结果；我们相信答案是肯定的。大数据远非毫无价值。大数定律已经应用，并且已经证明模型可以在与人类相同的水平上生成令人信服的语法。然而，正如我们之前所说，模型也已经明确地证明了语法与语义或语用学没有任何联系。
- en: 'In this section, we hope to share with you the right frame of mind when preparing
    your dataset. We will focus on the high-level linguistic considerations you should
    be thinking about when preparing a dataset, and we won’t be going too deep into
    how to create the actual data pipelines. That said, the main logic is simple and
    follows these basic steps:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们希望与你分享准备数据集时的正确心态。我们将重点关注在准备数据集时应考虑的高级语言考虑因素，并且不会深入探讨如何创建实际的数据管道。话虽如此，主要逻辑很简单，遵循以下基本步骤：
- en: Take your pile of data, and determine a schema for the features.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的数据堆整理好，并为特征确定一个模式。
- en: Make sure all the features conform to a distribution that makes sense for the
    outcome you’re trying to get through normalization or scaling.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保所有特征都符合通过归一化或缩放得到的合理分布。
- en: Check the data for bias/anomalies (most businesses skip this step by using automated
    checking instead of informed verification).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据是否存在偏差/异常（大多数企业通过使用自动检查而不是有意识的验证来跳过这一步）。
- en: Convert the data into a format for the model to ingest (for LLMs, it’s through
    tokenization and embedding)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为模型可以摄入的格式（对于LLMs，是通过分词和嵌入）
- en: Train, check, and retrain.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练、检查和重新训练。
- en: 'NOTE For more information on creating data pipelines, check out Fundamentals
    of Data Engineering,[^(11)](#footnote-161) WizardLM,[^(12)](#footnote-162) and
    “LIMA: Less Is More for Alignment.”[^(13)](#footnote-163) These resources can
    help you create effective data pipelines to get as much data into a trainable
    state as possible.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：有关创建数据管道的更多信息，请参阅《数据工程基础》[^(11)](#footnote-161)、WizardLM[^(12)](#footnote-162)和“LIMA：对齐的‘少即是多’”。[^(13)](#footnote-163)
    这些资源可以帮助你创建有效的数据管道，尽可能地将数据转换为可训练的状态。
- en: None of these steps are necessarily easy, but we hope to share a few tips and
    tricks. Evaluating whether your distribution is correct can be as simple as looking
    at the data and asking yourself whether it truly represents the problem or as
    difficult as creating a whole human-in-the-loop workflow to validate your model’s
    output. Next, we’ll go over the first three steps, and in the next section, we’ll
    go over the fourth. The last step is covered in depth in the next chapter.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤并不一定容易，但我们希望分享一些技巧和窍门。评估你的分布是否正确可能就像查看数据并问自己它是否真正代表问题一样简单，也可能像创建一个完整的人机交互工作流程来验证你的模型输出一样困难。接下来，我们将介绍前三个步骤，在下一节中，我们将介绍第四个步骤。最后一个步骤将在下一章中详细介绍。
- en: Instruct schema
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令架构
- en: 'One of the best and most common data schemas you should consider when preparing
    your data, especially for finetuning, is the instruct schema. Instruction tuning
    is based on the intuitive logic that if we show a model how to perform a task
    with instructions, the model will perform better than if we just show it tasks
    and “answers.” Instruction tuning involves demonstrating for the model what you
    would like to happen, and as such, the datasets are more intensive to create than
    your run-of-the-mill crawl data. You need to prepare your data to match a format
    that will look something like this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备你的数据时，你应该考虑的最佳和最常见的数据架构之一，特别是对于微调，就是指令架构。指令调整基于直观的逻辑，即如果我们用指令向模型展示如何执行一个任务，那么模型的表现将比我们只展示任务和“答案”要好。指令调整涉及向模型展示你希望发生的事情，因此，这些数据集的创建比常规的爬取数据更为密集。你需要准备你的数据以匹配一个类似以下格式的格式：
- en: '**CB ###Instruction**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**CB ###指令**'
- en: '{user input}'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '{用户输入}'
- en: '**CB ###Input**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**CB ###输入**'
- en: '{meta info about the instruction}'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '{关于指令的元信息}'
- en: '**![chatGpt](../Images/chatGpt.png) ###Response**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**![chatGpt](../Images/chatGpt.png) ###响应**'
- en: '{model output}'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '{模型输出}'
- en: Instruction datasets are powerful because they allow the model to consider both
    instructions and relevant input. For example, if the instruction was “Translate
    this sentence to Japanese,” the input would be the sentence you’d want translated,
    and the response would be the Japanese translation. Thus, they prepare your model
    for many prompting techniques and prompt tuning, making them more effective later.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据集之所以强大，是因为它们允许模型考虑指令和相关的输入。例如，如果指令是“将这个句子翻译成日语”，输入将是你想翻译的句子，而响应将是日语翻译。因此，它们为你的模型准备了许多提示技术和提示调整，使它们在以后更加有效。
- en: Despite their name, instruction tuning datasets are not restricted to test-based
    modalities; they can also use vision instruction tuning (image–instruction–answer)
    and red teaming instruction (RLHF) datasets. The “instruction” offers a semblance
    of pragmatics within the model and prompt, providing important guardrails for
    the LLM as it generates responses. It grounds the prompt with syntax that repeats
    and is predictable, along with syntax that is unpredictable for the model to guess
    at. These syntactic landmarks `(###Instruction`, `User:`, `Chat` `History:`, etc.)
    also help lower the chance of an EOS (end-of-sequence) token being predicted early
    due to the variable length of what can come between each of them, like chat history.
    Chat history can be one message or thousands of tokens, but the pattern, given
    there’s another landmark coming afterward, helps the model succeed in long-term
    memory. When you are deciding what to train your model on, keep those landmarks
    in mind, as they can make an instruct-tuned model even better at a specific task
    if you only need it to do one thing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们的名称如此，指令调整数据集并不仅限于基于测试的模态；它们也可以使用视觉指令调整（图像-指令-答案）和红队指令（RLHF）数据集。这里的“指令”在模型和提示中提供了一种实用性的表象，为LLM在生成响应时提供了重要的安全线。它通过重复和可预测的语法以及模型猜测不可预测的语法将提示固定下来。这些句法地标（`###Instruction`，`User:`，`Chat
    History`等）还有助于降低由于它们之间可能出现的可变长度（如聊天历史）而提前预测EOS（序列结束）标记的可能性。聊天历史可能是一条消息或数千个标记，但考虑到之后还有另一个地标，这种模式有助于模型在长期记忆中成功。当你决定在什么上训练你的模型时，请记住这些地标，因为如果你只需要它做一件事，它们可以使指令调整模型在特定任务上表现得更好。
- en: This isn’t the only format; some competitors in the space include the evol-instruct
    format used by WizardLM and the self-instruct format used by Alpaca, both of which
    use scripts to create instruction-based prompts. The best format is still an open-ended
    question, and we’d like to extend a challenge to the reader to explore creating
    their own. GitHub ([https://mng.bz/5OmD](https://mng.bz/5OmD)) and Hugging Face
    datasets are both great places to look for vetted datasets at the moment, but
    keep in mind that if the dataset doesn’t contain many examples of the tasks you’d
    like your model to perform or it doesn’t contain enough examples of semantic ambiguity
    being resolved when completing the task, performance will be unstable—which takes
    us to step 2 in our cleaning process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是唯一的格式；该领域的某些竞争对手包括WizardLM使用的evol-instruct格式和Alpaca使用的self-instruct格式，它们都使用脚本创建基于指令的提示。最好的格式仍然是一个开放性问题，我们希望向读者发起挑战，探索创建他们自己的。GitHub
    ([https://mng.bz/5OmD](https://mng.bz/5OmD)) 和 Hugging Face数据集都是目前寻找经过验证的数据集的好地方，但请记住，如果数据集不包含你希望模型执行的任务的许多示例，或者不包含在完成任务时解决语义歧义的足够示例，性能将不稳定——这把我们带到了我们清理过程的第二步。
- en: Ensuring proficiency with speech acts
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确保对言语行为的熟练掌握
- en: In preparing the dataset, the most important consideration is what you want
    the model to do. If you want a model to predict housing prices in Boston, you
    probably shouldn’t train it on survivors of the Titanic. This is obvious when
    stated, but it raises the question, “Is my dataset correct for the problem, and
    how would I know?” When it comes to language data, the answer isn’t as obvious
    as we might hope. Let’s look at an example to figure out why.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备数据集时，最重要的考虑因素是你希望模型做什么。如果你想训练一个模型来预测波士顿的房价，你可能不应该用泰坦尼克号的幸存者来训练它。当这样表述时，这是显而易见的，但它提出了问题：“我的数据集是否适合这个问题，我该如何知道？”当涉及到语言数据时，答案并不像我们希望的那样明显。让我们通过一个例子来看看原因。
- en: 'Let’s say you want your model to take orders at a fast-food restaurant. This
    scenario may seem boring and mundane, where all we expect to see are queries like,
    “I’ll order the #3 combo,” which you will. But if you ask a cashier about how
    people actually talk to them, really, anything can happen! I had a friend who
    worked at Burger King tell me that because of Burger King’s slogan “Have It Your
    Way,” he received many crazy requests, like asking for a burger with two top buns.
    That blew my mind, but it was also a tame example. Not to mention, you never know
    when the next LARPing convention will bring more creative and colorful interactions
    to otherwise mundane scenarios. A generic dataset containing customer orders and
    cashier responses won’t be enough here. When you aren’t intentional about what
    kind of data goes into your model, the performance of the model suffers.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你希望你的模型能在快餐店接受指令。这种场景可能看起来无聊且平凡，我们期望看到的只是像“我要点第3号套餐”这样的查询，而你确实会这么做。但如果你询问收银员人们实际上是如何与他们交谈的，真的，任何情况都可能发生！我有一个朋友在汉堡王工作，他告诉我，由于汉堡王的口号“随你心意”，他收到了很多疯狂的要求，比如要求一个有两个顶部的汉堡。这让我震惊，但也是一个温和的例子。更不用说，你永远不知道下一个LARPing大会会带来更多创意和多彩的互动，让原本平凡的情景变得有趣。一个包含顾客订单和收银员回应的通用数据集在这里是不够的。当你没有故意考虑要放入模型中的数据类型时，模型的性能就会受到影响。
- en: DEFINITION LARP stands for live-action role-playing, and you can imagine the
    tomfoolery of a customer pretending to be an elf, orc, or pirate and thus breaking
    all rules and expectations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 定义LARP代表现场角色扮演，你可以想象顾客假装成精灵、兽人或海盗，从而打破所有规则和期望的胡闹场景。
- en: To ensure your data is right for the task, first, you should think about what
    speech acts generally go together to perform the task at hand. Speech acts refer
    to the various functions language can perform in communication beyond conveying
    information. They are a way of categorizing utterances based on their intended
    effect or purpose in a conversation. Speech acts are important, as they shed light
    on how communication goes beyond the literal meaning of words and involves the
    speaker’s intentions and the listener’s interpretation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你的数据适合任务，首先，你应该考虑哪些言语行为通常一起执行手头的任务。言语行为指的是语言在沟通中可以执行的各种功能，它超越了传达信息。它们是根据话语的预期效果或目的在对话中进行分类的一种方式。言语行为很重要，因为它们揭示了沟通如何超越文字的字面意义，并涉及说话者的意图和听者的解释。
- en: Speech acts defined
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义言语行为
- en: 'The following list includes common speech acts and their definitions:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包括常见的言语行为及其定义：
- en: '*Expressives*—Greetings, apologies, congratulations, condolences, thanksgivings
    (e.g., “You’re the best!”)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*表达句*—问候、道歉、祝贺、哀悼、感谢（例如，“你是最棒的！”）'
- en: '*Commissives*—Promises, oaths, pledges, threats, vows (e.g., “I swear by the
    realm, the princess will come to no harm.”)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*承诺句*—承诺、誓言、誓言、威胁、誓言（例如，“我以王国起誓，公主不会受到伤害。”）'
- en: '*Directives*—Commands, requests, challenges, invitations, orders, summons,
    entreaties, dares (e.g., “Get it done in the next three days.”)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指令句*—命令、请求、挑战、邀请、命令、召唤、恳求、打赌（例如，“在接下来的三天内完成。”）'
- en: '*Declarations*—Blessings, firings, baptisms, arrests, marrying, juridical speech
    acts such as sentencings, declaring a mistrial, declaring out of order (e.g.,
    “You’re hired!”)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*声明句*—祝福、解雇、洗礼、逮捕、结婚、司法言语行为，如宣判、宣布重审、宣布无效（例如，“你被录用了！”）'
- en: '*Verdictives*—Rankings, assessments, appraising, condoning (combinations such
    as representational declarations; e.g., “You’re out!”)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*陈述句*—排名、评估、评价、宽恕（例如，表示性声明；例如，“你出局了！”）'
- en: '*Questions*—Usually starting with interrogative words like *what*, *where*,
    *when*, *why*, *who*, or indicated with rising intonation at the end in English
    (e.g., “Which model is best for my task?”)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问题*—通常以疑问词如*什么*、*哪里*、*何时*、*为什么*、*谁*开头，或者在英语中通过结尾的升调来表示（例如，“哪种型号最适合我的任务？”）'
- en: '*Representatives*—Assertions, statements, claims, hypotheses, descriptions,
    suggestions, answers to questions (e.g., “This model is best for your task.”'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代表句*—断言、陈述、主张、假设、描述、建议、问题的答案（例如，“这个模型最适合你的任务。”）'
- en: The current way we measure the robustness of datasets for LLMs is the vanilla
    number of tokens. Instruct datasets are relatively new, but they rely on you being
    intentional with how instruction for the model happens. What will your model do
    when given a directive it shouldn’t respond to when it’s only been trained on
    helpful responses to directives? If you aren’t sure, now’s the time to consider.
    For example, imagine a user declaring with glee to your bot, “Promise you’ll help
    me take over the world!” If it was only trained to be helpful, it would likely
    respond by promising to do just that because similar scenarios are in the training
    set. And now we have an evil AI overlord taking over the world. Thanks. In actuality,
    this is a fairly innocuous example, but the unpredictability of the seemingly
    infinite number of possible responses from the model should make you think, especially
    if this agent has access to tools like Google or your internal HR documents. Being
    cognizant of speech acts can simplify your work so that you don’t have to focus
    as much on individual tokens for the vocabulary as on the overall structure of
    what your model will come in contact with during training.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们衡量LLM数据集鲁棒性的方法是纯文本标记的数量。指令数据集相对较新，但它们依赖于你对模型指令的意图性。当你的模型只接受过对指令的有益响应训练时，如果收到不应响应的指令，你的模型会做什么？如果你不确定，现在是考虑的时候了。例如，想象一个用户高兴地对你的机器人说，“保证你帮我统治世界！”如果它只接受过有益的培训，它可能会通过承诺这样做来回应，因为类似的场景在训练集中。现在我们有一个邪恶的AI霸主正在统治世界。谢谢。实际上，这是一个相当无害的例子，但模型可能出现的看似无限的可能响应的不确定性应该让你思考，尤其是如果这个代理可以访问像Google或你内部的人力资源文件这样的工具。意识到言语行为可以简化你的工作，这样你就不必过于关注词汇的个别标记，而是关注模型在训练过程中将接触到的整体结构。
- en: Going back, when you think about a customer-facing role like a cashier, how
    many of these speech acts are likely to occur in your average order? Take a minute
    to think it through. We can tell you that declarations and verdictives are out,
    and commissives are uncommon. But what if you get them regardless? You then need
    to consider how you might want to steer such highly expressive customers toward
    the speech acts you can work with, likely questions, directives, and representatives.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，当你考虑一个面向客户的角色，比如收银员时，在你的平均订单中可能有多少这样的言语行为会发生？花一分钟思考一下。我们可以告诉你，声明句和陈述句是不存在的，承诺句是罕见的。但如果你无论如何都得到了它们，那么你需要考虑如何引导这样高度表达性的客户向你可以处理的言语行为靠拢，可能是问题、指令和代表。
- en: To make matters more complicated, the form of a speech act doesn’t always have
    to match its function. For example, you could say “You’re fired” to your friend
    who doesn’t work for you, where, even though its form is declarative, its function
    is more likely expressive. Once you have a dataset or a trained LLM and are looking
    to improve its ability to take instruction, this is something to seriously consider
    to increase your data’s quality and your LLM’s performance. Does your model weirdly
    fail when users frame utterances as questions when they’re actually directives?
    Does your model start hallucinating when coming in contact with the representative-only
    HR documents you’ve been asked to analyze? As a note, you don’t have to completely
    finetune a model all over again to improve performance. We’ll go over this in
    more detail later, but giving specific examples within the prompt can patch a
    lot of these edge cases quickly and inexpensively.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要使事情更加复杂，言语行为的形式并不总是必须与其功能相匹配。例如，你可以对你的朋友说“你被解雇了”，即使它的形式是陈述性的，其功能更有可能是表达性的。一旦你拥有一个数据集或一个训练好的大型语言模型（LLM），并希望提高其接受指令的能力，这是你应该认真考虑的事情，以提高数据质量和LLM的性能。当用户将语句作为问题提出时，你的模型是否奇怪地失败了？当接触到你被要求分析的仅代表人力资源文档时，你的模型是否开始产生幻觉？作为备注，你不必完全重新微调模型以提高性能。我们将在稍后详细介绍这一点，但给出提示中的具体例子可以快速且低成本地修复许多这些边缘情况。
- en: Now that you have an understanding of the different features you should be looking
    for in your dataset, let’s consider the best ways to annotate your dataset so
    you can make sure it conforms to expectations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了在你的数据集中应该寻找的不同特征，让我们考虑最佳的标注方法，以确保你的数据集符合预期。
- en: Annotating the data
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据标注
- en: Annotation is labeling your data, usually in a positionally aware way. For speech
    recognition tasks, annotations would identify the different words as *noun*, *verb*,
    *adjective*, or *adverb*. Annotations were used as labels in supervised learning
    tasks as the main way to train a model. Now annotations essentially give us metadata
    that makes it easier to reason about and analyze our datasets. Instead of worrying
    about micro information like speech recognition or named-entity recognition, you’ll
    get more value by focusing on macro metadata, like the speech acts just discussed
    or what language the data is in.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 标注是对数据进行标记，通常是以位置感知的方式进行。对于语音识别任务，标注将识别不同的单词为 *名词*、*动词*、*形容词* 或 *副词*。标注在监督学习任务中用作标签，是训练模型的主要方式。现在，标注基本上为我们提供了元数据，这使得我们更容易推理和分析我们的数据集。与其担心像语音识别或命名实体识别这样的微观信息，不如关注像刚才讨论的言语行为或数据所在的语言这样的宏观元数据。
- en: 'Of course, this is the real trick, isn’t it? If this were easy, every company
    on the face of the earth would have its own models already in production. The
    fact is data wrangling is too large to be done by hand but too varying to be done
    automatically, and you need to find the middle ground as quickly as possible.
    You don’t want to ignore your data and just download a dataset someone recommended
    (even us) and then proceed to harm a real-world population because it contained
    harmful data. But you also don’t want to have to hand-validate millions of rows
    of utterances. Thankfully, there are tools to help with every part of this, but
    we’d like to specifically mention these first:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这确实是真正的技巧，不是吗？如果这很容易，那么地球上的每家公司都已经拥有自己的模型并投入生产了。事实是，数据整理既太大以至于无法手工完成，又太变化多端以至于无法自动完成，你需要尽快找到平衡点。你不想忽视你的数据，只是下载某人推荐（甚至是我们）的数据集，然后继续对现实世界的人群造成伤害，因为其中包含了有害数据。但你也不要被迫手动验证数百万行的语句。幸运的是，有工具可以帮助完成这一过程的每个部分，但我们想特别提及这些：
- en: '*Prodi.gy* ([https://prodi.gy/](https://prodi.gy/))—Prodigy takes a one-time
    payment for a quick and powerful multimodal annotation tool.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Prodi.gy* ([https://prodi.gy/](https://prodi.gy/))—Prodigy 是一款一次性付费的快速且强大的多模态标注工具。'
- en: '*doccano: Open source annotation tool for machine learning practitioners*([https://github.com/doccano/doccano](https://github.com/doccano/doccano))—A
    truly open-source and, at the time of writing, updated web-based platform for
    data annotation.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*doccano: 适用于机器学习从业者的开源标注工具*([https://github.com/doccano/doccano](https://github.com/doccano/doccano))—一个真正开源的，在撰写本文时，更新了基于网络的标注平台。'
- en: '*d5555/TagEditor: Annotation tool for spaCy* ([https://github.com/d5555/TagEditor](https://github.com/d5555/TagEditor))—Works
    in conjunction with [https://spacy.io](https://spacy.io). Both create an ecosystem
    on top of spaCy, a popular NLP framework that makes rapid prototyping well within
    the reach of your average ML team.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d5555/TagEditor: spaCy的标注工具* ([https://github.com/d5555/TagEditor](https://github.com/d5555/TagEditor))—与[https://spacy.io](https://spacy.io)协同工作。两者都在流行的
    NLP 框架 spaCy 上创建了一个生态系统，使您的平均 ML 团队能够轻松地进行快速原型设计。'
- en: '*Praat: Doing phonetics by computer* ([https://github.com/praat/praat](https://github.com/praat/praat))—The
    only audio annotation tool on this list, Praat is fundamentally a tool for phonetics
    with annotations thrown in. Given how much we predict the LLM space to shift toward
    phonetics, we couldn’t omit this one from the list.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Praat: 通过计算机进行语音学* ([https://github.com/praat/praat](https://github.com/praat/praat))—这是列表中唯一的音频标注工具，Praat
    本质上是一个语音学工具，附带标注功能。鉴于我们预测 LLM 领域将向语音学方向转变，我们无法将这个工具从列表中排除。'
- en: '*Galileo* ([https://www.rungalileo.io/llm-studio](https://www.rungalileo.io/llm-studio))—At
    the time of this writing, Galileo’s LLM studio has yet to come out, but it makes
    some big promises for prompt creation and evaluation, which would immensely speed
    up annotation and creation of instruction datasets.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*伽利略* ([https://www.rungalileo.io/llm-studio](https://www.rungalileo.io/llm-studio))—在撰写本文时，伽利略的
    LLM 工作室尚未推出，但它对提示创建和评估做出了一些重大承诺，这将极大地加快标注和指令数据集的创建速度。'
- en: 'Which tool is best for your project depends entirely on the goal of your annotation.
    Going into annotating without a specified goal leads nowhere, as you’ll find discrepancies
    on the other end of data processing. Of course, we recommend adding speech act
    annotations; you’ll also want to consider additional annotations looking for bias
    and anomalies. We can show that by measuring the number of pieces of outside context
    present in the text (things like insinuations or entailments), you can gain a
    confidence score about how high quality a particular data is. The reason for this
    is intuitive: the more ambiguity a set of examples can solve for the model, the
    more the model learns from that set. The hard part is that no one can pin any
    of these contextual information nuggets on repeating parts of orthography, such
    as individual characters or a particular word or subword.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 适合您项目的最佳工具完全取决于您标注的目标。在没有指定目标的情况下进行标注毫无意义，因为您会在数据处理的其他端发现差异。当然，我们建议添加言语行为标注；您还希望考虑额外的标注来寻找偏见和异常。我们可以通过测量文本中存在的背景信息片段的数量（如暗示或蕴涵）来展示这一点，从而获得关于特定数据质量的高置信度分数。原因很简单：一组示例能够为模型解决多少歧义，模型就能从那组示例中学到多少。困难之处在于，没有人可以将这些上下文信息碎片固定在正文的重复部分，如单个字符或特定的单词或子词。
- en: 'Annotating can be a lot of work, but the reason for all of this consideration
    at the front is fairly simple: your model can only learn what you teach it. Thankfully,
    to make matters much easier, the goal isn’t to annotate every bit of text in your
    dataset. We are simply annotating a large-enough sample to ensure our dataset
    is representative of the task. Remember, LLMs are generally trained in two steps:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 标注可能是一项繁重的工作，但所有这些前期考虑的原因相当简单：您的模型只能学习您教给它的内容。幸运的是，为了使事情变得容易得多，目标并不是标注数据集中每一块文本。我们只是标注足够大的样本，以确保我们的数据集能够代表任务。记住，LLMs
    通常分为两个步骤进行训练：
- en: '*Self-supervised pretraining *—Analyzing many different speech acts in varying
    forms and functions to learn general representations'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*自监督预训练*—分析不同形式和功能的多种言语行为，以学习通用表示'
- en: '*Finetuning and RLHF *—Teaching the model how/when to use the representations
    learned in step 1'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*微调和RLHF*—教会模型如何/何时使用第一步中学习到的表示'
- en: This training significantly lightens the burden on you as a trainer of attempting
    to parse every possible locution (what a person literally says) and illocution
    (what they actually mean in context) within the given task. Even for something
    viewed as simple work, like being a cashier, having to come up with a dataset
    vast enough to cover all edge cases would be quite a headache. For most cases,
    all you need to do is prepare a finetuning dataset, which often doesn’t need to
    be large at all—sometimes a dozen examples is more than enough to start getting
    good results.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这项训练显著减轻了您作为训练者尝试解析给定任务中每个可能的言外之意（一个人实际说的话）和言内之意（他们在特定语境中的实际含义）的负担。即使是像收银员这样的简单工作，需要想出一个足够大的数据集来覆盖所有边缘情况也会相当头疼。在大多数情况下，您只需要准备一个微调数据集，这通常根本不需要很大——有时十几个例子就足够开始获得良好的结果。
- en: 4.4 Text processors
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 文本处理器
- en: Now that you have a dataset for training or finetuning, we need to transform
    it into something that can be consumed by the LLM. Simply put, we need to turn
    the text into numbers. We’ve already briefly gone over the process of doing that
    conversion quickly and effectively, so let’s dive into different examples and
    methodologies.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了用于训练或微调的数据集，我们需要将其转换成LLM可以消费的形式。简单来说，我们需要将文本转换为数字。我们已经简要地讨论了如何快速有效地进行这种转换的过程，所以让我们深入探讨不同的示例和方法。
- en: In this section, we’ll show you how to train your own tokenizers, both byte-pair
    encoding (BPE) and SentencePiece tokenizers, and how to grab embeddings from (almost)
    any model for storage or manipulation later. This step is often ignored when working
    with an LLM through an API, but much of modern performance in data applications
    depends on doing this process correctly and specifically for your goal. There
    are many mathematically sound and correct ways to tokenize text, so you can’t
    rely on something someone else did when you have a specific use case. You need
    to prepare it for that use case. Training your own tokens will allow you to minimize
    unknown tokens, `<UKN>`, while also maximizing encoded semantics. Having control
    of this process is one of the simplest and easiest hacks to give your models a
    major boost in performance. Let’s start first with tokenization.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何训练自己的分词器，包括字节对编码（BPE）和SentencePiece分词器，以及如何从（几乎）任何模型中提取嵌入以供后续存储或操作。当通过API与LLM一起工作时，这一步通常会被忽略，但数据应用中的许多现代性能都取决于正确且具体地完成这一过程。有许多数学上合理且正确的方法来分词文本，所以当您有特定的用例时，不能依赖于别人的做法。您需要为该用例准备它。训练自己的词元将允许您最小化未知词元`<UKN>`，同时最大化编码语义。控制这个过程是给您的模型带来重大性能提升的最简单和最直接的方法之一。让我们首先从分词开始。
- en: 4.4.1 Tokenization
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 分词
- en: 'Tokenization is a bit more involved than simple vectorization but leads to
    the same overall result: text input, vector output, and the ability to encode
    and decode. We mentioned in chapter 2 the multilingual factor and in chapter 3
    the token tax of foreign languages, which are both motivations to be at least
    aware of your own tokenization strategies. However, it goes beyond those. Your
    tokenization strategy isn’t just important; it is vitally important for every
    subsequent step.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 分词比简单的向量化要复杂一些，但最终结果相同：文本输入，向量输出，以及编码和解码的能力。我们在第二章中提到了多语言因素，在第三章中提到了外语的词元税，这两者都是至少要了解自己的分词策略的动机。然而，它不仅仅如此。您的分词策略不仅很重要；它对于后续的每个步骤都至关重要。
- en: A good example is comparing GOAT 7B and GPT-4 in math and arithmetic. Consider
    table 4.3\. The left column is a simple arithmetic prompt. Then we see the two
    models’ answers and, for reference, the actual answer so you don’t have to pull
    out your calculator.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是比较GOAT 7B和GPT-4在数学和算术方面的表现。考虑表4.3。左列是一个简单的算术提示。然后我们看到两个模型的答案，以及作为参考的实际答案，这样您就不需要拿出计算器了。
- en: Table 4.3 Tokenization allows GOAT 7B to outperform GPT-4 in math
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.3 分词使GOAT 7B在数学上超越GPT-4
- en: '| Prompt | GOAT 7B | GPT-4 1.7T | Correct |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | GOAT 7B | GPT-4 1.7T | 正确 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 3978640188 + 42886272 =  | 4021526460  | 4,021,526,460  | 4,021,526,460  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 3978640188 加上 42886272 等于 | 4021526460 | 4,021,526,460 | 4,021,526,460 |'
- en: '| 4523646 minus 67453156  | –62929510  | –63,930,510  | –62,929,510  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 4523646 减去 67453156 | –62929510 | –63,930,510 | –62,929,510 |'
- en: '| Calculate 397 × 4429  | 1758313  | 1,757,413  | 1,758,313  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 计算 397 乘以 4429 | 1758313 | 1,757,413 | 1,758,313 |'
- en: '| What is 8914/64?  | 139 R 18  | 139.15625  | 139.28125 Or 139 R 18  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 8914除以64等于多少？ | 139 R 18 | 139.15625 | 139.28125 或 139 R 18 |'
- en: GOAT 7B consistently outperforms GPT-4, which leaves the question, “Why does
    GOAT perform better despite being 200 times smaller? Aren’t larger models more
    likely to show emergent behavior?” You probably already guessed the answer based
    on the subsection’s heading, but if you didn’t, it’s because of the tokenization
    algorithm used!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: GOAT 7B在性能上始终优于GPT-4，这引发了这样的问题：“尽管GOAT的体积小200倍，为什么它的表现更好？更大的模型不是更有可能出现涌现行为吗？”你可能已经根据小节标题猜到了答案，但如果你没有，那是因为使用了标记化算法！
- en: 'The GPT family of models tokenizes all subwords and digits in groups based
    purely on frequency only, meaning that if that exact group of numbers or words
    hadn’t shown up before, they could be grouped together during the embedding and
    inference processes later! GOAT is a finetuned Llama model, meaning that while
    it was finetuned on math to be good at it, the underlying secret to success lies
    in its tokenization strategy, which is the same as Llama’s. GPT-X tokenizes like
    this:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: GPT系列模型根据频率将所有子词和数字分组，这意味着如果这个确切的数字或词组之前没有出现过，它们可以在嵌入和推理过程中被分组在一起！GOAT是一个微调的Llama模型，这意味着虽然它在数学上进行了微调以擅长它，但成功背后的秘密在于其标记化策略，这与Llama的相同。GPT-X的标记化方式如下：
- en: '[PRE13]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Did you notice how the first group of numbers is seven digits long, but the
    entire output is eight tokens? This is the exact grouping methodology we’re talking
    about. Compare that to Llama’s tokenization strategy in figure 4.3\. Notice that
    each digit is highlighted individually, meaning that the model will eventually
    see all the digits. As this example demonstrates, your tokenization strategy will
    ultimately determine what your model will see and won’t see, as they’ll become
    `<UNK>` tokens—and that’s why it’s vitally important to get it right for your
    use case.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到第一个数字组是七位数长，但整个输出是八个标记吗？这正是我们正在讨论的分组方法。将其与图4.3中Llama的标记化策略进行比较。注意每个数字都是单独高亮的，这意味着模型最终会看到所有数字。正如这个例子所证明的，你的标记化策略最终将决定你的模型会看到什么，不会看到什么，因为它们将成为`<UNK>`标记——这就是为什么对于你的用例来说，正确地完成它至关重要。
- en: '![figure](../Images/4-3.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-3.png)'
- en: Figure 4.3 Llama’s tokenization of the first arithmetic problem in the comparison
    table. Notice that each digit is highlighted individually, meaning that the model
    will eventually see all the digits.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3展示了Llama在比较表中对第一个算术问题的标记化。注意每个数字都是单独高亮的，这意味着模型最终会看到所有数字。
- en: 'What started out as creating a simple set of bag-of-words conversion dictionaries
    has evolved immensely, and we couldn’t be happier about it. Tokenization essentially
    consists of two major steps: a step to split up the text and a step to turn it
    into numbers. The most obvious form of tokenization is splitting a string on whitespace
    and then converting it to a number based on a word-to-integer dictionary.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 从最初创建一组简单的词袋转换字典开始，我们已经取得了巨大的进步，对此我们感到非常高兴。标记化本质上包括两个主要步骤：一个步骤是将文本分割开，另一个步骤是将它转换成数字。最明显的标记化形式是将字符串根据空白字符分割，然后根据词到整数的字典将其转换为数字。
- en: 'This makes sense to most Indo-European language speakers, but we can’t recommend
    this because of the two assumptions presupposed: alphabets and whitespace. What
    will you do when you come across a language that doesn’t use an alphabet, like
    Chinese? And what will you do when you come across a language that doesn’t use
    whitespace in the same way as English, like Hungarian or Turkish? Or code, for
    that matter—whitespace is critical to Python’s syntax and is more than just a
    separator; it has semantic meanings. This is one reason why multilingual models
    end up outperforming monolinguals on the same tasks in almost every case: they’re
    forced to learn deeper representations for meaning without the bowling bumpers
    of easy tokenization. So let’s look at some deeper methodologies that work for
    UTF-8 encoded languages.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这对大多数印欧语系语言使用者来说是有意义的，但我们不能推荐这种方法，因为它基于两个预设的假设：字母表和空白字符。当你遇到不使用字母表的语言，比如中文时，你会怎么做？当你遇到像匈牙利语或土耳其语这样的语言，它们不使用与英语相同的空白字符时，你会怎么做？或者代码——空白字符对Python的语法至关重要，它不仅仅是分隔符；它还具有语义意义。这就是为什么多语言模型几乎在所有情况下都能在相同任务上优于单语言模型：它们被迫在没有简单标记化障碍的情况下学习更深层次的意义表示。因此，让我们看看一些适用于UTF-8编码语言的更深入的方法。
- en: 'Here are examples of all the current popular options for basing your tokenization:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是当前所有流行的基础标记化选项的示例：
- en: '*Word-based*—“Johannes Gutenberg” becomes `[''Johannes'',` `''Gutenberg'']`.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于词*——“Johannes Gutenberg”变为`[''Johannes'',` `''Gutenberg'']`。'
- en: '*Character-based* —“Shakespeare” becomes `[''S'',''h'',''a'',''k'',''e'',''s'',''p'',''e'',
    ''a'',''r'',''e'']`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于字符* — “莎士比亚”变为`[''S'',''h'',''a'',''k'',''e'',''s'',''p'',''e'', ''a'',''r'',''e'']`。'
- en: '*Subword-based*—“The quick red Delphox jumped over the lazy brown Emolga” becomes
    `[''the'',''quick'',''red'',''delph'',''ox'',''jump'',''ed'',''over'',''the'',
    ''laz'',''y'',''brown'',''emol'',''ga'']`'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于子词* — “The quick red Delphox jumped over the lazy brown Emolga”变为`[''the'',''quick'',''red'',''delph'',''ox'',''jump'',''ed'',''over'',''the'',
    ''laz'',''y'',''brown'',''emol'',''ga'']`'
- en: Let’s take a look at each of them in turn.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看看它们。
- en: Word-based
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于单词
- en: Word-based tokenizers most commonly split on whitespace, but there are other
    methods like using regular expressions, dictionaries, or punctuation. For example,
    a punctuation-based approach would split “It’s the truth!” into `['It',` `'` `‘'`
    `,` `'` `s',` `'` `the',` `'` `truth',` `'` `!']`, which gives us slightly better
    context than splitting on whitespace alone. The `TreebankWordTokenizer` from NLTK
    is an example of a regular expression tokenizer. Word-based tokenizers are relatively
    easy to implement but require us to keep an unmanageably large dictionary mapping
    to encode every single possible word. That’s unreasonable, so generally, you’ll
    implement a dictionary cutoff and return unknown tokens when the model runs into
    unrecognized words to make it work. This makes the tokenizer poor at many tasks
    like code, name, and entity recognition, as well as generalizing across domains.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单词的标记器通常在空白处分割，但还有其他方法，比如使用正则表达式、字典或标点符号。例如，基于标点的处理方法会将“ It’s the truth!”分割为`['It',`
    `'` `‘'` `,` `'` `s',` `'` `the',` `'` `truth',` `'` `!']`，这比仅基于空白分割提供了稍微更好的上下文。NLTK中的`TreebankWordTokenizer`是一个正则表达式标记器的例子。基于单词的标记器相对容易实现，但需要我们保持一个难以管理的巨大字典，以映射编码每一个可能的单词。这是不合理的，所以通常你会在模型遇到未识别的单词时实现字典截断并返回未知标记，使其工作。这使得标记器在许多任务上表现不佳，如代码、名称和实体识别，以及跨领域的泛化。
- en: Character-based
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于字符
- en: Character-based encoding methods are the most straightforward and easiest to
    implement since we split on the UTF-8 character encodings. With this method, we
    only need the tiniest of dictionaries to map characters to numbers, which means
    we can prevent the need for unknown tokens and related concerns. However, it comes
    with a major loss of information and fails to keep relevant syntax, semantics,
    or morphology of the text.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的编码方法是最直接且最容易实现的，因为我们是在UTF-8字符编码上进行分割。使用这种方法，我们只需要极小的字典来将字符映射到数字，这意味着我们可以避免未知标记和相关问题的需要。然而，它伴随着主要的信息损失，并且无法保留文本的相关语法、语义或形态。
- en: Subword-based
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于子词
- en: Just like Goldilocks and the Three Bears, while character-based tokenizers are
    too hard and word-based tokenizers are too soft, subword-based tokenizers are
    just right. Subword-based tokenizers have proven to be the best option, being
    a mixture of the previous two. We are able to use a smaller dictionary like a
    character-based tokenizer but lose less semantics like a word-based tokenizer.
    It even has the added bonus of including some morphological information. However,
    it’s an unsolved problem for where and how words should be split, and there are
    many different methods and approaches. The best method to choose will be, like
    all other things with LLMs, dependent on the task. If you don’t have a specific
    goal in mind for what you are trying to do, there will be consequences later.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 就像灰姑娘和三只熊的故事一样，基于字符的标记器太硬，基于单词的标记器太软，而基于子词的标记器则恰到好处。基于子词的标记器已被证明是最佳选择，它是前两种方法的混合体。我们能够使用像基于字符的标记器一样的小型字典，但丢失的语义像基于单词的标记器那样少。它甚至还有额外的优势，包括一些形态信息。然而，关于单词应该如何分割以及在哪里分割的问题尚未解决，并且存在许多不同的方法和途径。最佳方法的选择将像所有其他与LLMs相关的事物一样，取决于任务。如果你没有明确的目标来指导你想要做什么，那么将来可能会有后果。
- en: 'Three main algorithms are used to create the subword dictionaries: BPE, WordPiece,
    and Unigram. In addition, SentencePiece, a combination of the three that explicitly
    handles whitespaces, is also very common. It’s outside the scope of this book
    to discuss how they work, but as a book focused on production, you should know
    that the most popular subword tokenization methodologies are BPE (GPT-x) and SentencePiece
    (LlamaX).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 创建子词字典主要使用三种算法：BPE、WordPiece和Unigram。此外，SentencePiece，它是三种方法的结合，可以显式处理空白，也非常常见。讨论它们的工作原理超出了本书的范围，但作为一个专注于生产的书籍，你应该知道最流行的子词标记化方法是BPE（GPT-x）和SentencePiece（LlamaX）。
- en: 'In listing 4.10, we’ll go over how to train a custom version for both BPE and
    SentencePiece on your data so that you’re equipped to face (almost) any dataset
    head-on. While reading the code, pay attention to where we train the tokenizers.
    In particular, you’ll want to tune three key parameters: `vocab_size`, `min_frequency`,
    and `special_tokens`. A larger vocabulary size means your tokenizer will be more
    robust and will likely be better at handling more languages, but it will add computational
    complexity. Minimum frequency determines how often a particular subword token
    has to be seen in the dataset before it is added to the dictionary. Larger values
    prevent rare and likely unimportant tokens from filling our dictionary and prevent
    us from learning rare tokens that are important. Lastly, special tokens are relatively
    straightforward and include syntactical tokens we care about specifically for
    model training.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.10中，我们将介绍如何训练BPE和SentencePiece的定制版本，以便你能够直面（几乎）任何数据集。在阅读代码时，请注意我们在哪里训练标记器。特别是，你将想要调整三个关键参数：`vocab_size`、`min_frequency`和`special_tokens`。更大的词汇量意味着你的标记器将更加健壮，并且可能更擅长处理更多语言，但它会增加计算复杂性。最小频率决定了特定子词标记在数据集中出现多少次之前才会被添加到字典中。较大的值可以防止罕见且可能不重要的标记填充我们的字典，并防止我们学习到重要的罕见标记。最后，特殊标记相对简单，包括我们特别关注于模型训练的句法标记。
- en: Listing 4.10 Training your own subword tokenizers
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.10：训练自己的子词标记器
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Initializes the texts to train from'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化用于训练的文本'
- en: '#2 Trains a byte-pair encoding tokenizer'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 训练一个字节对编码标记器'
- en: '#3 Trains a SentencePiece tokenizer'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练一个SentencePiece标记器'
- en: '#4 Converts'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 转换'
- en: '#5 And saves for later!'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 并保存以备后用！'
- en: Out of the two, BPE and SentencePiece, we find ourselves using both about equally.
    It mostly depends on which model we’re finetuning or using as a base for a particular
    project. Algorithmically, we’re partial to SentencePiece because it tends to boost
    evaluation scores on pretty much any test for models trained on it, and it’s also
    closer to how we interact with morphology as people.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在BPE和SentencePiece这两种方法中，我们发现我们几乎同样地使用它们。这主要取决于我们正在微调或作为特定项目基础的模型。在算法上，我们更倾向于SentencePiece，因为它往往能提高在它之上训练的模型在几乎所有测试中的评估分数，而且它也更接近我们作为人类与形态学互动的方式。
- en: All in all, tokenization loses information, just as converting from speech to
    text does—namely, word order (syntax) and meaning (semantics). All of the information
    about what a number is and how it would differ from a letter is completely gone
    after tokenization. To circumvent potential semantic and syntactic problems, we
    need to create an approximation for each of these features and figure out how
    to mathematically represent them in abstraction to insert that meaning back into
    the tokenized vector. For this, we have embeddings.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，标记化会丢失信息，就像将语音转换为文本一样——即，词序（句法）和意义（语义）。关于一个数字是什么以及它如何与字母不同的所有信息在标记化后都完全消失了。为了规避潜在的语义和句法问题，我们需要为这些特征中的每一个创建一个近似值，并找出如何用数学方法在抽象中表示它们，以便将那种意义重新插入到标记化的向量中。为此，我们使用了嵌入。
- en: 4.4.2 Embeddings
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 嵌入
- en: Embeddings provide meaning to the vectors generated during tokenization. Tokenized
    text is just numbers assigned almost arbitrarily (occurrence-based) to a dictionary,
    but it’s at least in a format that the model can ingest. Embeddings are the next
    step, where positional and semantic encodings are created and looked up to give
    the model additional context for making decisions about how to (probably) complete
    the task it’s given.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入为标记化过程中生成的向量提供了意义。标记化文本只是将几乎任意分配（基于出现频率）的数字分配给字典，但至少它是一个模型可以摄入的格式。嵌入是下一步，其中创建并查找位置和语义编码，为模型提供额外的上下文，以便在如何（可能）完成所给任务方面做出决策。
- en: 'Embeddings are imperfect for several reasons, but perhaps the most relevant
    is this theoretical question: Can you represent a set using only a subset of that
    same set? In this case, the first set is language, one or more, and the second
    set is numbers, floats, and digits. Math is a subset of language used to describe
    things axiomatically that we accept as true. Take the English alphabet, for example:
    Can you represent the entire alphabet by only using some fraction of the 26 letters?
    Obviously not, but what if both the original set and the subset are infinite?
    Can you represent all digits using only the decimals between 0 and 1? Given that
    the first is a numerable infinite set and the second is a nonenumerable infinite
    set, the answer is yes, which should be enheartening for the field of language
    modeling.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入存在不完美之处，原因有很多，但最相关的问题可能是这个理论问题：能否仅使用该集合的子集来表示该集合？在这种情况下，第一个集合是语言，一个或多个，第二个集合是数字、浮点数和数字。数学是用于描述我们接受为真的公理性事物的语言的一个子集。以英语字母表为例：能否仅使用26个字母中的某些部分来表示整个字母表？显然不能，但如果原始集合和子集都是无限的，会怎样呢？能否仅使用0到1之间的十进制数来表示所有数字？鉴于第一个是可数的无限集合，第二个是不可数的无限集合，答案是肯定的，这对语言建模领域应该是一个鼓舞人心的消息。
- en: Now that we’ve talked about why embeddings shouldn’t be completely and blindly
    relied on, embeddings are what most businesses are looking for with LLMs. You
    don’t need a 1.7T-parameter model to handle customers asking questions about your
    pricing or performing a search through your documents. As we discussed in chapter
    2, embeddings have the innate advantage of being comparable by distance, provided
    both embeddings you’re comparing were created by the same model in the same dimensional
    space. That opens up the door for all sorts of speedy computation and retrieval
    where you never have to figure out how to host a gigantic model somewhere because
    you can run a smaller embedding model on a CPU, and it takes milliseconds for
    hundreds of tokens.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了为什么不应该完全盲目地依赖嵌入，嵌入就是大多数企业在LLMs中寻求的东西。您不需要一个1.7T参数的模型来处理客户询问您的定价或通过您的文档进行搜索。正如我们在第2章中讨论的，如果比较的两个嵌入都是由同一模型在相同维度空间中创建的，嵌入具有通过距离进行比较的天生优势。这为各种快速计算和检索打开了大门，您永远不必担心如何托管一个巨大的模型，因为您可以在CPU上运行一个较小的嵌入模型，处理数百个标记只需毫秒。
- en: One of the most popular and coolest applications of embeddings at the moment
    is retrieval-augmented generation (RAG), where you store data that is pertinent
    to the overall task of the model and give portions of that data as needed to a
    larger model at prompting time to improve results. Suppose we apply RAG to the
    Boston Housing dataset and attempt to predict the value of a new house. In that
    case, we can compare that house’s embedded data to the closest comparable houses
    in the area and generate an informed appraisal without ever needing an appraiser
    to verify, as long as the embeddings you’re retrieving from are up-to-date.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，嵌入应用中最受欢迎且最酷的应用之一是检索增强生成（RAG），在这种应用中，你存储与模型整体任务相关的数据，并在提示时将数据的一部分提供给更大的模型，以改善结果。假设我们将RAG应用于波士顿住房数据集，并尝试预测新房屋的价值。在这种情况下，我们可以将那所房子的嵌入数据与该地区最相似的房屋进行比较，并在无需评估师验证的情况下生成有根据的评估，只要您检索到的嵌入是最新更新的。
- en: Embeddings can be used for dozens of different tasks and are the result of taking
    final hidden state representations from your model. Every layer of your model
    is a potential option, but the general consensus is to take representations after
    the final layer before any decoding or final linear layers or softmaxes. Listing
    4.11 gives a practical example of how to extract the embeddings from both PyTorch
    and Hugging Face models. Best practice dictates that you should extract the embeddings
    from documents using whatever embedding model you are planning to use for inference,
    especially if those embeddings will end up being stored in a VectorDB later on.
    After creating our embeddings, we show how to do a simple similarity search on
    the results, which is the basis of RAG systems.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可用于数十种不同的任务，并且是取自模型最终隐藏状态表示的结果。您模型的每一层都是一个潜在的选择，但普遍共识是在任何解码或最终线性层或softmax之前，从最终层提取表示。列表4.11提供了一个从PyTorch和Hugging
    Face模型中提取嵌入的实际示例。最佳实践规定，您应该使用计划用于推理的任何嵌入模型从文档中提取嵌入，尤其是如果这些嵌入最终将存储在VectorDB中。在创建我们的嵌入后，我们展示了如何对结果进行简单的相似度搜索，这是RAG系统的基础。
- en: Listing 4.11 Example embeddings
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.11 示例嵌入
- en: '[PRE15]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Δownloads embedding model and dataset'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 下载嵌入模型和数据集'
- en: '#2 Creates embeddings'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建嵌入'
- en: '#3 Adds Faiss index that allows similarity search'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 添加Faiss索引以允许相似性搜索'
- en: '#4 Runs query'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 运行查询'
- en: '#5 Prints results'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 打印结果'
- en: Extracting embeddings, like the listing shows, is pretty simple and differs
    very little from simply running inference or training on a dataset. Remember,
    if you aren’t using sentence transformers, set your model to `eval` mode, run
    with `torch.no_grad()`, and if you’re running on torch 2.0+, run `torch.compile(model)`.
    Things should speed up and become more computationally efficient immediately.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 提取嵌入，就像列表中所示，相当简单，与在数据集上简单地运行推理或训练差别很小。记住，如果你没有使用sentence transformers，将你的模型设置为`eval`模式，使用`torch.no_grad()`运行，如果你在torch
    2.0+上运行，请运行`torch.compile(model)`。这样应该会立即加快速度并提高计算效率。
- en: Another as-of-yet unsolved problem is how to compare embedding spaces. Mathematically
    sound comparisons have popped up time and again over the years, but as has been
    demonstrated, mathematical soundness isn’t the first problem to be solved; the
    modality is. In addition, pairwise comparison functions have mathematical limits
    on how fast it is possible to run them. If you’re comparing language embeddings,
    a mathematically sound conversion of a linguistically sound comparison method
    is the solution, and a linguistically sound comparison is dependent upon the goal
    of the comparison. It’s too much to go into here, but we dive more deeply into
    this topic in appendix C, where we discuss diffusion and multimodal LLMs.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个尚未解决的难题是如何比较嵌入空间。多年来，数学上合理的比较方法层出不穷，但正如所证明的那样，数学上的合理性并不是首要解决的问题；模式才是。此外，成对比较函数在运行速度上有数学上的限制。如果你正在比较语言嵌入，一个数学上合理的语言上合理的比较方法的转换是解决方案，而语言上合理的比较取决于比较的目标。这里就不多说了，但在附录C中，我们将更深入地探讨这个话题，其中我们讨论了扩散和多模态LLM。
- en: 4.5 Preparing a Slack dataset
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 准备Slack数据集
- en: Now that we have learned the ins and outs of preparing the necessary assets
    to train our own LLM, we wanted to end this chapter by preparing a dataset that
    we can use later. For this exercise, we will tackle a very common problem in the
    industry. I’m sure most readers have experienced or witnessed an HR help channel
    constantly inundated with the same questions over and over. It doesn’t matter
    how many FAQ pages are created; users don’t want to waste their time searching
    for documentation when they could ask an expert. So let’s build a chatbot to answer
    these questions!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了准备训练我们自己的LLM所需必要资产的方方面面，我们想在结束本章时准备一个我们可以稍后使用的数据集。对于这个练习，我们将解决行业中的一个非常普遍的问题。我相信大多数读者都经历过或见证过人力资源帮助渠道不断被相同的问题淹没。不管创建了多少个FAQ页面，用户都不愿意浪费时间搜索文档，而他们可以询问专家。所以让我们构建一个聊天机器人来回答这些问题！
- en: We will show you how to pull your company’s Slack data and prepare it for training
    an LLM-based chatbot. In listing 4.12, we pull Slack data, filter it to keep just
    the user’s data, and save it to a parquet file. This way, you can create a bot
    that will talk like you, but feel free to edit it. For example, you might enjoy
    creating a bot that talks like your boss, but I’d recommend not telling them in
    case they feel threatened knowing you are automating them out of a job.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向你展示如何拉取你公司的Slack数据，并为其训练基于LLM的聊天机器人做准备。在列表4.12中，我们拉取Slack数据，过滤掉除了用户数据之外的所有内容，并将其保存到parquet文件中。这样，你可以创建一个像你一样说话的机器人，但请随意编辑它。例如，你可能喜欢创建一个像你老板一样说话的机器人，但我建议不要告诉他们，以防他们感到威胁，知道你正在自动化他们从工作中退出。
- en: Listing 4.12 Example of pulling Slack data
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.12 拉取Slack数据的示例
- en: '[PRE16]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, there’s not much to it! We have an example dataset we pulled
    using this script in the GitHub repo accompanying this book. We will use this
    dataset in the coming chapters.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这并不复杂！我们有一个示例数据集，我们使用这本书附带的GitHub仓库中的脚本拉取了这个数据集。我们将在接下来的章节中使用这个数据集。
- en: We’ve gone over a lot in this chapter, but you should now be prepared and know
    how to select and evaluate a foundation model, prepare and clean a dataset, and
    optimize your own text processors. We will use this information in the next chapter
    to train and finetune our own LLM model.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经讨论了很多内容，但现在你应该已经准备好，并知道如何选择和评估基础模型，准备和清理数据集，以及优化你自己的文本处理器。我们将在下一章中使用这些信息来训练和微调我们自己的LLM模型。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data engineers have unique datasets to acquire and manage LLMs, like model weights,
    evaluation datasets, and embeddings.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师拥有独特的数据集来获取和管理LLMs，例如模型权重、评估数据集和嵌入。
- en: No matter your task, there is a wide array of open source models to choose from
    to finetune your own model.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论你的任务是什么，都有大量的开源模型可供选择，以微调你自己的模型。
- en: Text-based tasks are harder to evaluate than simple equality metrics you’d find
    in traditional ML tasks, but there are many industry benchmarks to help you get
    started.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文本的任务比在传统机器学习任务中找到的简单相等性度量更难评估，但有许多行业基准可以帮助你开始。
- en: Evaluating LLMs for more than just performance, like bias and potential harm,
    is your responsibility.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LLMs不仅限于性能，如偏见和潜在危害，这也是你的责任。
- en: You can use the Evaluate library to build your own evaluation metrics.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用Evaluate库来构建自己的评估指标。
- en: There are many large open source datasets, but most come from scraping the web
    and require cleaning.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多大型开源数据集，但大多数来自网络抓取，需要清理。
- en: Instruct schemas and annotating your data can be effective ways to clean and
    analyze your data.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令模式和标注你的数据是清理和分析数据的有效方法。
- en: Finetuning a model on a dataset with an appropriate distribution of speech acts
    for the task you want your model to perform will help it generate context-appropriate
    content.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在具有适当分布的言语行为数据集上微调模型，将有助于其生成与上下文相符的内容。
- en: Building your own subword tokenizer to match your data can greatly improve your
    model’s performance.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建与你的数据相匹配的子词分词器可以大大提高你的模型性能。
- en: Many problems teams are trying to use LLMs for can be solved by using embeddings
    from your model instead.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多团队试图使用LLMs解决的问题可以通过使用模型中的嵌入来解决。
- en: '[[1]](#footnote-source-1) Joe Reis and Matt Housley, Fundamentals of Data Engineering,
    O’Reilly, 2022.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) Joe Reis 和 Matt Housley，数据工程基础，O’Reilly，2022年。'
- en: '[[2]](#footnote-source-2) C. Loizos, “‘GPT’ may be trademarked soon if OpenAI
    has its way,” TechCrunch, April 25, 2023, [https://mng.bz/5Omq](https://mng.bz/5Omq).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#footnote-source-2) C. Loizos，“如果OpenAI这样做，‘GPT’可能很快就会被商标化，” TechCrunch，2023年4月25日，[https://mng.bz/5Omq](https://mng.bz/5Omq)。'
- en: '[[3]](#footnote-source-3) N. Muennighoff et al., “Cross lingual generalization
    through multitask finetuning,” November 3, 2022, [https://arxiv.org/abs/2211.01786](https://arxiv.org/abs/2211.01786).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#footnote-source-3) N. Muennighoff等人，“通过多任务微调实现跨语言泛化，” 2022年11月3日，[https://arxiv.org/abs/2211.01786](https://arxiv.org/abs/2211.01786)。'
- en: '[[4]](#footnote-source-4) C. Xu et al., “WizardLM: Empowering large language
    models to follow complex instructions,” Jun. 10, 2023, [https://arxiv.org/abs/2304.12244](https://arxiv.org/abs/2304.12244).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#footnote-source-4) C. Xu等人，“WizardLM：赋予大型语言模型遵循复杂指令的能力，” 2023年6月10日，[https://arxiv.org/abs/2304.12244](https://arxiv.org/abs/2304.12244)。'
- en: '[[5]](#footnote-source-5) Mike Conover et al., “Free Dolly: Introducing the
    world’s first truly open instruction-tuned LLM,” Databricks, April 12, 2023, [https://mng.bz/n0e8](https://mng.bz/n0e8).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#footnote-source-5) Mike Conover等人，“免费Dolly：介绍世界上第一个真正开放的指令微调LLM，” Databricks，2023年4月12日，[https://mng.bz/n0e8](https://mng.bz/n0e8)。'
- en: '[[6]](#footnote-source-6) D. Hendrycks et al., “Measuring massive multitask
    language understanding,” arXiv (Cornell University), September 2020, [https://doi.org/10.48550/arxiv.2009.03300](https://doi.org/10.48550/arxiv.2009.03300).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#footnote-source-6) D. Hendrycks等人，“衡量大规模多任务语言理解，” arXiv（康奈尔大学），2020年9月，[https://doi.org/10.48550/arxiv.2009.03300](https://doi.org/10.48550/arxiv.2009.03300)。'
- en: '[[7]](#footnote-source-7) P. Rajpurkar, R. Jia, and P. Liang, “Know what you
    don’t know: Unanswerable questions for SQuAD,” June 2018, [https://arxiv.org/abs/1806.03822](https://arxiv.org/abs/1806.03822).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#footnote-source-7) P. Rajpurkar, R. Jia, 和 P. Liang, “知道你所不知道的：SQuAD中的不可回答问题，”
    2018年6月，[https://arxiv.org/abs/1806.03822](https://arxiv.org/abs/1806.03822)。'
- en: '[[8]](#footnote-source-8) C. Zhou et al., “LIMA: Less is more for alignment,”
    arXiv.org, May 18, 2023, [https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#footnote-source-8) C. Zhou等人，“LIMA：对齐的‘少即是多’，” arXiv.org，2023年5月18日，[https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206)。'
- en: '[[9]](#footnote-source-9) T. Dettmers et al., “SpQR: A sparse-quantized representation
    for near-lossless LLM weight compression,” arXiv.org, June 5, 2023, [https://arxiv.org/abs/2306.03078](https://arxiv.org/abs/2306.03078).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#footnote-source-9) T. Dettmers等人，“SpQR：一种用于近似无损LLM权重压缩的稀疏量化表示，” arXiv.org，2023年6月5日，[https://arxiv.org/abs/2306.03078](https://arxiv.org/abs/2306.03078)。'
- en: '[[10]](#footnote-source-10) L. Gao et al., “The Pile: An 800GB Dataset of Diverse
    Text for Language Modeling,” Dec. 2020, [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#footnote-source-10) 高磊等，“The Pile：用于语言模型的多样化文本数据集，800GB”，2020年12月，[https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)。'
- en: '[[11]](#footnote-source-11) Reis and Housley, Fundamentals of Data Engineering,
    2022.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#footnote-source-11) Reis 和 Housley, 数据工程基础，2022年。'
- en: '[[12]](#footnote-source-12) Xu et al., “WizardLM,” 2023.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#footnote-source-12) Xu 等人，“WizardLM”，2023年。'
- en: '[[13]](#footnote-source-13) Zou et al., “LIMA,” 2023.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#footnote-source-13) Zou 等人，“LIMA”，2023年。'
