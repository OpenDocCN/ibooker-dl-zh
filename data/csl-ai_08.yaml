- en: 6 Structural causal models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 结构因果模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Converting a general causal graphical model to a structural causal model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一般因果图形模型转换为结构因果模型
- en: Mastering the key elements of SCMs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握SCMs的关键要素
- en: Implementing SCMs for rule-based systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为基于规则的系统实现SCMs
- en: Building an SCM from scratch using additive models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用加性模型从头开始构建SCM
- en: Combining SCMs with deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将SCMs与深度学习相结合
- en: In this chapter, I’ll introduce a fundamental causal modeling approach called
    the structural causal model (SCM). An SCM is a special case of a causal generative
    model that can encode causal assumptions beyond those we can capture with a DAG.
    If a DAG tells us *what* causes what, an SCM tells us both *what* causes what
    and *how* the causes affect the effects. We can use that extra “how” information
    to make better causal inferences.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍一种基本的因果建模方法，称为结构因果模型（SCM）。SCM是因果生成模型的一个特例，它可以编码超出我们用DAG所能捕捉的因果假设。如果一个DAG告诉我们“什么”导致“什么”，那么SCM就会告诉我们“什么”导致“什么”以及“原因”如何影响“效果”。我们可以利用这额外的“如何”信息来做出更好的因果推断。
- en: In this chapter, we’ll focus on defining and building an intuition for SCMs
    using examples in code. In later chapters, we’ll see examples of causal inferences
    that we can’t make with a DAG alone but we can make with an SCM.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过代码示例来关注定义和构建SCMs（结构因果模型）的直觉。在后续章节中，我们将看到一些因果推断的例子，这些推断我们仅使用DAG（有向无环图）是无法做出的，但我们可以通过SCM来完成。
- en: 6.1 From a general causal graphical model to an SCM
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 从一般因果图形模型到SCM
- en: In the causal generative models we’ve built so far, we defined, for each node,
    a conditional probability distribution given the node’s direct parents, which
    we called a *causal Markov kernel*. We then *fit* these kernels using data. Specifically,
    we made a practical choice to use some parametric function class to fit these
    kernels. For example, we fit the parameters of a probability table using pgmpy’s
    `TabularCPD` because it let us work with pgmpy’s convenient d-separation and inference
    utilities. And we used a neural decoder in a VAE architecture because it solved
    the problem of modeling a high-dimensional variable like an image. These practical
    reasons have nothing to do with causality; our causal assumptions stopped at the
    causal DAG.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止构建的因果生成模型中，我们对每个节点定义了一个条件概率分布，该分布是在给定节点的直接父节点的情况下给出的，我们称之为“因果马尔可夫核”。然后我们使用数据来“拟合”这些核。具体来说，我们做出了一个实际的选择，即使用某些参数函数类来拟合这些核。例如，我们使用pgmpy的`TabularCPD`来拟合概率表的参数，因为它让我们能够使用pgmpy方便的d分离和推理工具。我们还使用VAE架构中的神经解码器，因为它解决了对高维变量（如图像）建模的问题。这些实际原因与因果性无关；我们的因果假设仅停留在因果DAG上。
- en: Now, with SCMs, we’ll use the parametric function class to capture additional
    causal assumptions beyond the causal DAG. As I said, the SCM lets us represent
    additional assumptions of *how* causes affect their effects; for example, that
    a change in the cause always leads to a proportional change in the effect. Indeed,
    a probability table or a neural network can be *too flexible* to capture assumptions
    about the “how” of causality; with enough data they can fit anything and thus
    don’t imply strong assumptions. More causal assumptions enable more causal inferences,
    at the cost of additional risk of modeling error.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了SCMs，我们将使用参数函数类来捕捉超出因果DAG的额外因果假设。正如我所说，SCM让我们能够表示关于“如何”原因影响其效果的额外假设；例如，原因的变化总是导致效果成比例的变化。确实，概率表或神经网络可能过于灵活，无法捕捉关于因果“如何”的假设；有了足够的数据，它们可以拟合任何东西，因此不暗示强烈的假设。更多的因果假设允许更多的因果推断，但代价是额外的建模错误风险。
- en: SCMs are a special case of causal graphical models (CGMs)—one with more constraints
    than the CGMs we’ve built so far. For clarity, I’ll use CGM to refer to the broader
    set of causal graphical models that are not SCMs. To make the distinction clear,
    let’s start by looking at how we might modify a CGM so it satisfies the constraints
    of an SCM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: SCM是因果图形模型（CGM）的一个特例——比我们迄今为止构建的CGM具有更多的约束。为了清晰起见，我将使用CGM来指代更广泛的因果图形模型集合，这些模型不是SCM。为了使区别清晰，让我们首先看看我们如何修改一个CGM，使其满足SCM的约束。
- en: 6.1.1 Forensics case study
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 法医案例研究
- en: Imagine you are a forensic scientist working for the police. The police discover
    decomposed human remains consisting of a skull, pelvic bone, several ribs, and
    a femur. An apparent blunt force trauma injury to the skull leads the police to
    open a murder investigation. First, they need you to help identify the victim.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你是一名为警察工作的法医科学家。警察发现了一具由颅骨、骨盆、几根肋骨和股骨组成的腐烂的人类遗骸。颅骨上的明显钝力创伤导致警察展开谋杀调查。首先，他们需要你帮助他们识别受害者。
- en: When the remains arrive in your lab, you measure and catalog the bones. From
    the shape of the pelvis, you can quickly tell that the remains most likely belong
    to an adult male. You note that the femur is 45 centimeters long. As you might
    suspect, there is a strong predictive relationship between femur length and an
    individual’s overall height. Moreover, that relationship is causal. Femur length
    is a cause of height. Simply put, having a long femur makes you taller, and having
    a short femur makes you shorter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当遗骸到达你的实验室时，你会对其进行测量和编目。从骨盆的形状，你可以迅速判断遗骸很可能是成年男性的。你注意到股骨长45厘米。正如你可能怀疑的那样，股骨长度和一个人的整体身高之间存在强烈的预测关系。此外，这种关系是因果的。股骨长度是身高的原因。简单来说，拥有长股骨会使你更高，而拥有短股骨会使你更矮。
- en: 'Indeed, when you consult your forensic text, it says that height is a *linear
    function* of femur length. It provides the following probabilistic model of height,
    given femur length (in males):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当你查阅你的法医文本时，它说身高是股骨长度的*线性函数*。它提供了以下基于股骨长度（男性）的概率模型：
- en: '*n*[*y*] ~ *N*(0, 3.3)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, 3.3)'
- en: '*y* = 25 + 3*x* + *n*[*y*]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = 25 + 3*x* + *n*[*y*]'
- en: Here, *x* is femur length in centimeters, and *y* is height in centimeters.
    Of course, exact height will vary with other causal factors, and *n*[*y*] represents
    variations in height from those factors. *N*[*y*] has a normal distribution with
    mean 0 and scale parameter 3.3 cm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*是厘米单位的股骨长度，*y*是厘米单位的身高。当然，精确的身高会因其他因果因素而变化，而*n*[*y*]代表这些因素引起的身高变化。*N*[*y*]具有均值为0和尺度参数3.3厘米的正态分布。
- en: This is an example of an SCM. We’ll expand this example as we go, but the key
    element to focus on here is that our model is assuming the causal mechanism underpinning
    height (*Y*) is linear. Height (*Y*) is a linear function of its causes, femur
    length (*X*) and *N*[*y*], which represents other causal determinants of height.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个SCM的例子。我们将随着讨论的深入扩展这个例子，但这里要关注的关键要素是我们的模型假设支撑身高的因果机制（*Y*）是线性的。身高（*Y*）是其原因的线性函数，即股骨长度（*X*）和*n*[*y*]，它代表其他影响身高的因果决定因素。
- en: Linear modeling is an attractive choice because it is simple, stands on centuries
    of theory, and is supported by countless statistical and linear algebra software
    libraries. But from a causal perspective, that’s beside the point. Our SCM is
    not using this linear function because it is convenient. Rather, we are intentionally
    asserting that the relationship between the cause and the effect is linear—that
    for a change in femur length, there is a proportional change in height.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 线性建模是一个吸引人的选择，因为它简单，建立在几个世纪的理论之上，并且得到了无数统计和线性代数软件库的支持。但从因果的角度来看，这并不是重点。我们的SCM不是使用这个线性函数是因为它方便。相反，我们有意断言因果关系是线性的——即对于股骨长度的变化，身高也会成比例地变化。
- en: Let’s drill down on this example to highlight the differences between a CGM
    and an SCM.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这个例子，以突出CGM和SCM之间的差异。
- en: 6.1.2 Converting to an SCM via reparameterization
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 通过重新参数化转换为SCM
- en: In this section, we will start by converting the type of CGM we’ve become familiar
    with into an SCM. Our conversion exercise will highlight those properties and
    make clear the technical structure of the SCM and how it differs relative to the
    CGMs we’ve seen so far. Note, however, that this “conversion” is intended to build
    intuition; in general, you should build your SCM from scratch rather than try
    to shoehorn non-SCMs into SCMs, for reasons we’ll see in section 6.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先将我们熟悉的CGM类型转换为SCM。我们的转换练习将突出这些属性，并清楚地说明SCM的技术结构和它与迄今为止我们所见的CGM之间的差异。然而，请注意，这种“转换”旨在建立直觉；一般来说，你应该从头开始构建你的SCM，而不是试图将非SCM强行纳入SCM，原因我们将在第6.2节中看到。
- en: Let’s suppose our forensic SCM were a CGM. We might implement it as in figure
    6.1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的法医供应链管理（SCM）是一个连续广义模型（CGM）。我们可能像图6.1中那样实现它。
- en: '![figure](../Images/CH06_F01_Ness.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F01_Ness.png)'
- en: Figure 6.1 A simple two-node CGM. Femur length (*X*) is a cause of height (*Y*).
    *X* has a normal distribution with a mean of 47 centimeters and a standard deviation
    of 2.3 centimeters. *Y* has a distribution with a mean of 25 + 3*x* centimeters
    and a standard deviation of 3.3 centimeters.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.1 一个简单的双节点 CGM。股骨长度 (*X*) 是身高 (*Y*) 的原因。*X* 具有均值为 47 厘米和标准差为 2.3 厘米的正态分布。*Y*
    具有均值为 25 + 3*x* 厘米和标准差为 3.3 厘米的分布。
- en: Recall from chapter 2 that *x* ~ *P*(*X*) and *y* ~ *P*(*Y*|*X*=*x*) means we
    generate from the probability distribution of *X* and conditional probability
    distribution of *Y* given *X*. In this case, *P*(*X*), the distribution of femur
    length, represented as a normal distribution with a mean of 47 centimeters and
    a standard deviation of 2.3 centimeters. *P*(*Y*|*X*=*x*) is the distribution
    on height given the femur length, given as a normal distribution with a mean of
    25 + 3*x* centimeters and a standard deviation of 3.3 centimeters. We would implement
    this model in Pyro as follows in listing 6.1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 2 章回忆起，*x* ~ *P*(*X*) 和 *y* ~ *P*(*Y*|*X*=*x*) 表示我们从 *X* 的概率分布和给定 *X* 的条件概率分布
    *Y* 中生成。在这种情况下，*P*(*X*)，股骨长度的分布，表示为均值为 47 厘米和标准差为 2.3 厘米的正态分布。*P*(*Y*|*X*=*x*)
    是给定股骨长度的身高分布，表示为均值为 25 + 3*x* 厘米和标准差为 3.3 厘米的正态分布。我们将在列表 6.1 中按如下方式在 Pyro 中实现此模型。
- en: Setting up your environment
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: The code in this chapter was written using Python version 3.10, Pyro version
    1.9.0, pgmpy version 0.1.25, and torch 2.3.0\. See [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to the notebooks that run the code. We are also using MATLAB for some
    plotting; this code was tested with version 3.7.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码使用 Python 版本 3.10，Pyro 版本 1.9.0，pgmpy 版本 0.1.25 和 torch 2.3.0 编写。有关运行代码的笔记本链接，请参阅
    [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。我们还使用
    MATLAB 进行一些绘图；此代码已在 3.7 版本上进行了测试。
- en: Listing 6.1 Pyro pseudocode of the CGM in figure 6.1
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1 图 6.1 中 CGM 的 Pyro 伪代码
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 x and y are sampled from their causal Markov kernels, in this case normal
    distributions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 x 和 y 从它们的因果马尔可夫核中采样，在这种情况下是正态分布。'
- en: '#2 Repeatedly calling cgm_model will return samples from P(X, Y).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 重复调用 cgm_model 将返回 P(X, Y) 的样本。'
- en: 'We are going to convert this model to an SCM using the following algorithm:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下算法将此模型转换为 SCM：
- en: Introduce a new latent causal parent for *X* called *N*[*x*] and a new latent
    causal parent for *Y* called *N*[*y*] with distributions *P*(*N*[*x*]) and *P*(*N*[*y*]).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 *X* 引入一个新的潜在因果父节点，称为 *N*[*x*]，为 *Y* 引入一个新的潜在因果父节点，称为 *N*[*y*]，它们的分布为 *P*(*N*[*x*])
    和 *P*(*N*[*y*])。
- en: Make *X* and *Y* deterministic functions of *N*[*x*] and *N*[*y*] such that
    *P*(*X*, *Y*) in this new model is the same as in the old model.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *X* 和 *Y* 作为 *N*[*x*] 和 *N*[*y*] 的确定性函数，使得在新模型中 *P*(*X*, *Y*) 与旧模型相同。
- en: Following these instructions and adding in two new variables, we get figure
    6.2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些说明并添加两个新变量，我们得到图 6.2。
- en: '![figure](../Images/CH06_F02_Ness.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F02_Ness.png)'
- en: Figure 6.2 To convert the CGM to an SCM, we introduce latent “exogenous” parents,
    *N**[x]* for *X* and *N**[y]* for *Y*, and probability distributions *P*(*N**[x]*)
    and *P*(*N**[y]*) for these latents. We then set *X* and *Y* deterministically,
    given their parents, via functions *f**[x]* and *f**[y]*.
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2 为了将 CGM 转换为 SCM，我们引入了潜在的“外生”父节点 *N**[x]* 和 *N**[y]*，以及这些潜在的概率分布 *P*(*N**[x]*)
    和 *P*(*N**[y]*)。然后，通过函数 *f**[x]* 和 *f**[y]*，根据它们的父节点确定性地设置 *X* 和 *Y*。
- en: We have two new latent variables *N*[*x*] and *N*[*y*] with distributions *P*(*N*[*x*])
    and *P*(*N*[*y*]). *X* and *Y* each have their own functions *f*[*x*] and *f*[*y*]
    that that deterministically set *X* and *Y*, given their parents in the graph.
    This difference is key; *X* and *Y* are generated in the model described in figure
    6.1 but set deterministically in this new model. To emphasize this, I use the
    assignment operator “:=” instead of the equal sign “=” to emphasize that *f*[*x*]
    and *f*[*y*] assign the values of *X* and *Y*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个新的潜在变量 *N*[*x*] 和 *N*[*y*]，它们的分布为 *P*(*N*[*x*]) 和 *P*(*N*[*y*])。*X* 和 *Y*
    各自有它们自己的函数 *f*[*x*] 和 *f*[*y*]，这些函数根据图中父母的值确定性设置 *X* 和 *Y*。这种差异是关键的；*X* 和 *Y*
    在图 6.1 所描述的模型中生成，但在新模型中是确定性设置的。为了强调这一点，我使用赋值运算符“:=”而不是等号“=”来强调 *f*[*x*] 和 *f*[*y*]
    赋予 *X* 和 *Y* 的值。
- en: To meet our goal of converting our CGM to an SCM, we want *P*(*X*) and *P*(*Y*|*X*=*x*)
    to be the same across both models. To achieve this, we have to choose *P*(*N*[*x*]),
    *P*(*N*[*y*]), *f*[*x*], and *f*[*y*] such that *P*(*X*) is still Normal(47, 2.3)
    and *P*(*Y*|*X*=*x*) is still Normal(25 + 3.3*x*, 3.3). One option is to do a
    simple reparameterization. Linear functions of normally distributed random variables
    are also normally distributed. We can implement the model in figure 6.3.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到将我们的CGM转换为SCM的目标，我们希望*P*(*X*)和*P*(*Y*|*X*=*x*)在两个模型中保持相同。为了实现这一点，我们必须选择*P*(*N*[*x*])、*P*(*N*[*y*])、*f*[*x*]和*f*[*y*]，使得*P*(*X*)仍然是Normal(47,
    2.3)且*P*(*Y*|*X*=*x*)仍然是Normal(25 + 3.3*x*, 3.3)。一个选择是进行简单的重新参数化。正态分布随机变量的线性函数也是正态分布的。我们可以在图6.3中实现该模型。
- en: '![figure](../Images/CH06_F03_Ness.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Ness.png)'
- en: Figure 6.3 A simple reparameterization of the original CGM produces a new SCM
    model with the same *P*(*X*) and *P*(*Y*|*X*) as the original.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 对原始CGM进行简单重新参数化后，产生了一个新的SCM模型，其*P*(*X*)和*P*(*Y*|*X*)与原始模型相同。
- en: In code, we rewrite this as follows.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们将其重写如下。
- en: Listing 6.2 CGM rewritten as an SCM
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2 将CGM重写为SCM
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 We sample these new latent variables from a standard normal distribution.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们从标准正态分布中采样这些新的潜在变量。'
- en: '#2 X and Y are calculated deterministically as linear transformations of n_x
    and n_y.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 X和Y作为n_x和n_y的线性变换确定性地计算。'
- en: '#3 The returned samples of P(X, Y) match the first model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 返回的P(X, Y)样本与第一个模型匹配。'
- en: With this introduction of new exogenous variables *N*[*x*] and *N*[*y*], some
    linear functions *f*[*x*] and *f*[*y*], and a reparameterization, we converted
    the CGM to an SCM that encodes the same distribution *P*(*X*, *Y*). Next, let’s
    look more closely at the elements we introduced.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入新的外生变量*N*[*x*]和*N*[*y*]、一些线性函数*f*[*x*]和*f*[*y*]以及重新参数化，我们将CGM转换为编码相同分布*P*(*X*,
    *Y*)的SCM。接下来，让我们更仔细地看看我们引入的元素。
- en: 6.1.3 Formalizing the new model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 形式化新模型
- en: 'To build an SCM, we’re going to assume we’ve already built a causal DAG, as
    in figure 6.1\. In figures 6.2 and 6.3, we see two kinds of variables: exogenous
    and endogenous. The *endogenous variables* are the original variables *X* and
    *Y*—we’ll define them as the variables we are modeling explicitly. These are the
    variables we included in our causal DAG.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个SCM，我们假设我们已经构建了一个因果DAG，如图6.1所示。在图6.2和6.3中，我们看到两种类型的变量：外生变量和内生变量。*内生变量*是原始变量*X*和*Y*——我们将它们定义为我们要明确建模的变量。这些是我们包含在因果DAG中的变量。
- en: The *exogenous variables* (also called *noise* variables) are our new nodes
    *N*[*x*] and *N*[*y*]. These variables represent all unmodeled causes of our endogenous
    variables. In our formulation, we pair each of the endogenous variable with its
    own exogenous variable parent; *X* gets new exogenous causal parent *N*[*x*],
    and *Y* gets exogenous parent *N*[*y*]. We add these to our DAG for completeness
    as in figures 6.2 and 6.3.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*外生变量*（也称为*噪声变量*）是我们新的节点*N*[*x*]和*N*[*y*]。这些变量代表我们内生变量的所有未建模原因。在我们的公式中，我们将每个内生变量与其自己的外生变量父节点配对；*X*获得新的外生因果父节点*N*[*x*]，而*Y*获得外生父节点*N*[*y*]。我们将这些添加到我们的DAG中，如图6.2和6.3所示，以保持完整性。'
- en: In our formulation, we’ll assume exogenous variables have no parents and have
    no edges between one another. In other words, they are root nodes in the graph,
    and they are independent relative to other exogenous variables. Further, we’ll
    treat the exogenous variables as latent variables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的公式中，我们将假设外生变量没有父节点，并且它们之间没有边。换句话说，它们是图中的根节点，并且相对于其他外生变量是独立的。此外，我们将外生变量视为潜在变量。
- en: Each endogenous variable also gets its own *assignment function* (also called
    a *structural assignment*) *f*[*x*], and *f*[*y*]. The assignment function *deterministically*
    sets the value of the endogenous variables *X* and *Y* given values of their parents
    in the causal DAG.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个内生变量也获得其自己的*赋值函数*（也称为*结构赋值*）*f*[*x*]和*f*[*y*]。赋值函数*确定性地*根据因果DAG中父节点的值设置内生变量*X*和*Y*的值。
- en: Assignment functions are how we capture assumptions about the “how” of causality.
    For instance, to say that the causal relationship between height (*Y*) and femur
    length (*X*) is linear, we specify that *f*[*x*] is a linear function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 赋值函数是我们捕捉关于因果关系“如何”的假设的方式。例如，要说明身高(*Y*)和股骨长度(*X*)之间的因果关系是线性的，我们指定*f*[*x*]是一个线性函数。
- en: While the endogenous variables are set deterministically, the SCM generates
    the values of the exogenous variables from probability distributions. In our femur
    example, we generate values *n*[*x*] and *n*[*y*] of exogenous variables *N*[*x*]
    and *N*[*y*] from distributions *P*(*N*[*x*]) and *P*(*N*[*y*]), which are *N*(0,
    2.3) and *N*(0, 3.3), as seen in figure 6.3.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然内生变量是确定性设置的，但 SCM 从概率分布中生成外生变量的值。在我们的股骨例子中，我们从分布 *P*(*N*[*x*]) 和 *P*(*N*[*y*])
    中生成外生变量 *N*[*x*] 和 *N*[*y*] 的值 *n*[*x*] 和 *n*[*y*]，这些分布是 *N*(0, 2.3) 和 *N*(0,
    3.3)，如图 6.3 所示。
- en: Elements of the generative SCM
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成式供应链管理模型（SCM）的元素
- en: '*A set of endogenous variables (e.g., X, Y)*—These are the variables we want
    to model explicitly. They are the models we build into our causal DAG.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组内生变量（例如，X，Y）*——这是我们想要显式建模的变量。它们是我们构建到我们的因果有向图（DAG）中的模型。'
- en: '*A set of exogenous variables (e.g., N**[x]* *and N**[y]**)*—These variables
    stand in for unmodeled causes of the endogenous variables. In our formulation,
    each endogenous variable has one corresponding latent exogenous variable.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组外生变量（例如，N**[x]* 和 N**[y]**）*——这些变量代表内生变量的未建模原因。在我们的公式中，每个内生变量都有一个相应的潜在外生变量。'
- en: '*A set of assignment functions (e.g., f**[x]* *and f**[y]**)*—Each endogenous
    variable has an assignment function that sets its value deterministically given
    its parents (its corresponding exogenous variable and other endogenous variables).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组分配函数（例如，f**[x]* 和 f**[y]**）*——每个内生变量都有一个分配函数，它根据其父节点（其对应的外生变量和其他内生变量）确定性地设置其值。'
- en: '*A set of exogenous variable probability distributions (e.g., P(N**[x]**) and
    P(N**[y]**))*—The SCM becomes a generative model with a set of distributions on
    the exogenous variables. Given values generated from these distributions, the
    endogenous variables are set deterministically.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组外生变量概率分布（例如，P(N**[x]**) 和 P(N**[y]**))——SCM 成为一个具有外生变量上的一组分布的生成模型。给定从这些分布生成的值，内生变量被确定性设置。'
- en: Let’s look at another example of an SCM, this time using discrete variables.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个供应链管理模型（SCM）的例子，这次使用离散变量。
- en: 6.1.4 A discrete, imperative example of an SCM
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 供应链管理模型（SCM）的一个离散、命令式示例
- en: Our femur example dealt with continuous variables like height and length. Let’s
    now return to our rock-throwing example from chapter 2 and consider a discrete
    case of an SCM. In this example, either Jenny or Brian or both throw a rock at
    window if they are inclined to do so. The window breaks depending on whether either
    or both Jenny and Brian throw and the strength of the windowpane.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于股骨的例子处理了连续变量，如身高和长度。现在让我们回到第 2 章中的投掷石头示例，并考虑一个离散的供应链管理模型（SCM）的例子。在这个例子中，如果珍妮或布莱恩或两者都愿意，他们会在窗户上投掷石头。窗户是否破碎取决于珍妮和布莱恩是否投掷以及窗户玻璃的强度。
- en: How might we convert this model to an SCM? In fact, this model is *already*
    an SCM. We captured this with the following code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个模型转换为供应链管理模型（SCM）？实际上，这个模型已经是供应链管理模型了。我们用以下代码捕捉了这一点。
- en: Listing 6.3 The rock-throwing example from chapter 2 is an SCM
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3 第 2 章中的投掷石头示例是一个供应链管理模型（SCM）。
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 The input values are instances of exogenous variables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 输入值是外生变量的实例。'
- en: '#2 Jenny and Brian throw the rock if so inclined. jenny_throws_rock and brian_throws_rock
    are endogenous variables.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果珍妮和布莱恩愿意的话，他们会投掷石头。`jenny_throws_rock` 和 `brian_throws_rock` 是内生变量。'
- en: '#3 strength_of_impact is an endogenous variable. This entire if-then expression
    is the assignment function for strength of impact.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 `strength_of_impact` 是一个内生变量。这个整个 if-then 表达式是影响强度的分配函数。'
- en: '#4 window_breaks is an endogenous variable. The assignment function is lambda
    strength_of_impact, window_strength: strength_of_impact &gt; window_strength.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 `window_breaks` 是一个内生变量。分配函数是 `lambda strength_of_impact, window_strength:
    strength_of_impact > window_strength`。'
- en: '#5 Each exogenous variable has a Uniform(0, 1) distribution.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 每个外生变量具有均匀分布（Uniform(0, 1)）。'
- en: You’ll see that it satisfies the requirements of an SCM. The arguments to the
    `true_dgp` function (namely `jenny_inclination,` `brian_inclination,` `window_strength`)
    are the exogenous variables. The named variables inside the function are the endogenous
    variables, which are set deterministically by the exogenous variables.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它满足供应链管理模型的要求。`true_dgp` 函数的参数（即 `jenny_inclination`、`brian_inclination`、`window_strength`）是外生变量。函数内部命名的变量是内生变量，它们由外生变量确定性设置。
- en: Most SCMs you’ll encounter in papers and textbooks are written down as math.
    However, this rock-throwing example shows us the power of reasoning causally with
    an imperative scripting language like Python. Some causal processes are easier
    to write in code than in math. It is only recently that tools such as Pyro have
    allowed us to make sophisticated code-based SCMs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文和教科书中遇到的大多数SCM（软件配置管理）都是以数学形式写下的。然而，这个投掷石头的例子向我们展示了使用像Python这样的命令式脚本语言进行因果推理的力量。一些因果过程用代码写比用数学写更容易。直到最近，像Pyro这样的工具才使我们能够制作复杂的基于代码的SCM。
- en: 6.1.5 Why use SCMs?
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 为什么使用SCM？
- en: More causal assumptions mean more ability to make causal inferences. The question
    of whether to use an SCM instead of a regular CGM is equivalent to asking whether
    the additional causal assumptions encoded in the functional assignments will serve
    your causal inference goal.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的因果假设意味着更强的因果推断能力。是否使用SCM而不是常规CGM的问题等同于询问编码在功能赋值中的额外因果假设是否能够服务于你的因果推断目标。
- en: In our femur example, our DAG says femur length causes height. Our SCM goes
    further and says that for every unit increase in femur length, there is a proportional
    increase in height. The question is whether that additional information helps
    us answer a causal question. One example where such a linear assumption helps
    make a causal inference is the use of *instrumental variable estimation* of causal
    effects, which I’ll discuss in chapter 11\. This approach relies on linearity
    assumptions to infer causal effects in cases where the assumptions in the DAG
    alone are not sufficient to make the inference. Another example is where an SCM
    can enable us to answer *counterfactual queries* using an algorithm discussed
    in chapter 9.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的股骨例子中，我们的DAG（有向无环图）表明股骨长度导致身高。我们的SCM更进一步，表明股骨长度每增加一个单位，身高也会相应增加。问题是这些额外信息是否有助于我们回答一个因果问题。一个这样的线性假设有助于做出因果推断的例子是使用*工具变量估计*来估计因果效应，这将在第11章中讨论。这种方法依赖于线性假设来推断在DAG的假设本身不足以做出推断的情况下因果效应。另一个例子是SCM可以让我们使用第9章中讨论的算法来回答*反事实查询*。
- en: Of course, if your causal inference is relying on an assumption, and that assumption
    is incorrect, your inference will probably be incorrect. The “what” assumptions
    in a DAG are simpler than the additional “how” assumptions in an SCM. An edge
    in a DAG is a true or false statement that *X* causes *Y.* An assignment function
    in an SCM model is a statement about *how* *X* causes *Y*. The latter assumption
    is more nuanced and quite hard to validate, so it’s easier to get incorrect. Consider
    the fact that there are longstanding drugs on the market that we know work, but
    we don’t fully understand their mechanism of action—*how* they work.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你的因果推断依赖于一个假设，而这个假设是错误的，那么你的推断可能也是错误的。DAG中的“什么”假设比SCM中的额外“如何”假设简单。DAG中的一条边是一个关于*X*导致*Y*的真是或假命题。SCM模型中的赋值函数是关于*X*如何导致*Y*的陈述。后者的假设更加微妙，而且很难验证，因此更容易出错。考虑这样一个事实：市场上有一些长期存在的药物我们知道它们有效，但我们并不完全了解它们的药理作用机制——*它们是如何工作的*。
- en: 6.1.6 Differences from related approaches
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.6 与相关方法的差异
- en: SCMs have a rich history across different fields. You may have seen formulations
    that are similar to but nonetheless different from what we’ve laid out here. Here,
    we’ll highlight the differentiating elements of this formulation and why they
    matter to us.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SCM在不同领域有着丰富的历史。你可能见过一些与我们所阐述的类似但又不尽相同的公式。在这里，我们将突出这个公式的区分性元素以及为什么它们对我们很重要。
- en: Generative SCMs with latent exogenous variables
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带有潜在外生变量的生成SCM
- en: We want to use our SCMs as generative models. To that end, we treat exogenous
    variables (variables we don’t want to model explicitly) as latent proxies for
    unmodeled causes of the endogenous variables. We just need to specify probability
    distributions of the exogenous variables and we get a generative latent variable
    model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将我们的SCM用作生成模型。为此，我们将外生变量（我们不希望明确建模的变量）视为内生变量未建模原因的潜在代理。我们只需要指定外生变量的概率分布，我们就能得到一个生成潜在变量模型。
- en: Flexible selection of assignment functions
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 灵活选择赋值函数
- en: You’ll find that the most common applications of SCMs use linear functions as
    assignment functions, like we did in the femur example. However, in a generative
    AI setting, we certainly don’t want to constrain ourselves to linear models. We
    want to work with rich function classes we can write as code, optimize with automatic
    differentiation, and apply to high-dimensional nonlinear problems, like images.
    These function classes can do just as well in representing the “how” of causality.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现SCM最常见的应用使用线性函数作为分配函数，就像我们在股骨示例中所做的那样。然而，在生成式AI环境中，我们当然不想将自己限制在线性模型上。我们希望与丰富的函数类一起工作，我们可以将其编写为代码，使用自动微分进行优化，并将其应用于高维非线性问题，如图像。这些函数类在表示因果的“如何”方面可以做得同样好。
- en: Connection to the DAG
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与有向无环图（DAG）的联系
- en: We contextualize the SCM within the DAG-based view of causality. First, we build
    a causal DAG as in chapters 3 and 4\. Each variable in the DAG becomes an endogenous
    variable (a variable we want to model explicitly) in the SCM. For each endogenous
    variable, we add a single latent exogenous parent node to the DAG. Next, we define
    “assignment function” as a function that assigns a given endogenous variable a
    value, given the values of its parents in the DAG. All of our DAG-based theory
    still applies, such as the causal Markov property and independence of mechanism.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在基于DAG的因果观中定位SCM。首先，我们像第3章和第4章中那样构建一个因果DAG。DAG中的每个变量在SCM中成为内生变量（我们想要明确建模的变量）。对于每个内生变量，我们在DAG中添加一个单一的潜在外生父节点。接下来，我们将“分配函数”定义为这样一个函数，它根据DAG中父节点的值给定的内生变量分配一个值。我们基于DAG的所有理论仍然适用，例如因果马尔可夫性质和机制独立性。
- en: Note that not all formulations of the SCM adhere so closely to the DAG. Some
    practitioners who don’t adopt a graphical view of causality still use SCM-like
    models (e.g., structural equation modeling in econometrics). And some variations
    of graphical SCMs allow us to relax acyclicity and work with cycles and feedback
    loops.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并非所有SCM的公式都如此紧密地遵循DAG。一些不采用因果图形视图的从业者仍然使用类似SCM的模型（例如，计量经济学中的结构方程模型）。一些图形SCM的变体允许我们放宽无环性，并处理循环和反馈回路。
- en: Independent exogenous variables
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 独立的外生变量
- en: Introducing one exogenous variable for every endogenous variable can be a nuisance;
    sometimes it is easier to treat a node with no parents in the original DAG as
    exogenous, or have the same exogenous parent for two endogenous nodes. But this
    approach lets us add exogenous variables in a way that maintains the d-separations
    entailed by the original DAG. It also allows us to make a distinction between
    *endogenous* variables we care to model explicitly, and all the *exogenous* causes
    we don’t want to model explicitly. This comes in handy when, for example, you’re
    building a causal image model like in chapter 5, and you don’t want to explicitly
    represent *all* the many causes of the appearance of an image.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个内生变量引入一个外生变量可能是一个麻烦；有时将原始DAG中没有父节点的节点视为外生变量，或者为两个内生节点有相同的外生父节点可能更容易。但这种方法使我们能够以保持原始DAG所蕴含的d分离的方式添加外生变量。它还允许我们区分*内生*变量，我们关心并希望明确建模，以及所有我们不希望明确建模的*外生*原因。这在例如，当你正在构建因果图像模型，如第5章中所述，并且你不想明确表示图像出现的所有许多原因时，非常有用。
- en: 6.1.7 Causal determinism and implications to how we model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.7 因果决定论及其对建模的影响
- en: The defining element of the SCM is that endogenous variables are set deterministically
    by assignment functions instead of probabilistically by drawing randomly from
    a distribution conditioned on causal parents. This deterministic assignment reflects
    the philosophical view of *causal determinism,* which argues that if you knew
    all the causal factors of an outcome, you would know the outcome with complete
    certainty.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 供应链管理（SCM）的定义要素是，内生变量是由分配函数确定性地设置，而不是通过从基于因果父代的分布中随机抽取来概率性地设置。这种确定性分配反映了*因果决定论*的哲学观点，该观点认为，如果你知道一个结果的所有因果因素，你就会以完全的确定性知道这个结果。
- en: The SCM stands on this philosophical foundation. Consider again our femur-height
    example, shown in figure 6.4\.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SCM建立在这样的哲学基础上。再次考虑我们的股骨高度示例，如图6.4所示。
- en: '![figure](../Images/CH06_F04_Ness.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F04_Ness.png)'
- en: Figure 6.4 The original CGM samples endogenous variables from causal Markov
    kernels. The new model sets the endogenous variables deterministically.
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 原始的CGM从因果马尔可夫核中采样内生变量。新模型确定性地设置内生变量。
- en: In the original CGM on the left of figure 6.4, we generate values of *X* and
    *Y* from models of their causal Markov kernels. In the corresponding SCM on the
    right, the endogenous variables are set deterministically, no longer drawn from
    distributions. The SCM is saying that given femur length and all the other unmodeled
    causes of height represented by *N*[*y*], height is a certainty.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.4左侧的原始CGM中，我们从其因果马尔可夫核的模型中生成*X*和*Y*的值。在相应的SCM右侧，内生变量是确定性设定的，不再从分布中抽取。SCM表示，给定股骨长度以及由*N*[*y*]表示的所有其他未建模的高度原因，身高是确定的。
- en: Note that despite this deterministic view, the SCM is still a probabilistic
    model of the joint probability distribution of the endogenous variables *P*(*X*,
    *Y*). But in comparison to the CGM on the left of figure 6.4, the SCM on the right
    shunts all the randomness of the model to the exogenous variable distributions.
    *X* and *Y* are still random variables in the SCM, because they are functions
    of *N*[*x*] and *N*[*y*], and a function of a random variable is a random variable.
    But conditional on the exogenous variables, the endogenous variables are fully
    determined (*degenerate*).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管这种决定性观点，SCM仍然是对内生变量联合概率分布的随机模型*P*(*X*, *Y*)。但与图6.4左侧的CGM相比，右侧的SCM将模型的全部随机性转移到外生变量分布上。*X*和*Y*在SCM中仍然是随机变量，因为它们是*N*[*x*]和*N*[*y*]的函数，而随机变量的函数仍然是随机变量。但在给定外生变量的条件下，内生变量是完全确定的（*退化的*）。
- en: The causal determinism leads to eye-opening conclusions for us as causal modelers.
    First, when we apply a DAG-based view of causality to a given problem*,* we implicitly
    assume *the ground-truth data generating process* (DGP) is an SCM. We already
    assumed that the ground-truth DGP had an underlying ground-truth DAG. Going a
    step further and assuming that each variable in that DAG is set deterministically,
    given all its causes (both those in and outside the DAG), is equivalent to assuming
    the ground-truth DGP is an SCM. The SCM might be a black box, or we might not
    be able to easily write it down in math or code, but it is an SCM nonetheless.
    That means, whether we’re using a traditional CGM or an SCM, we are *modeling
    a ground-truth SCM*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因果决定论为我们这些因果模型师带来了令人耳目一新的结论。首先，当我们将基于DAG的因果观应用于给定问题时*，*我们隐含地假设*真实数据生成过程*（DGP）是一个结构因果模型（SCM）。我们已经假设真实DGP有一个潜在的、真实的DAG。更进一步，假设该DAG中的每个变量都是基于其所有原因（包括DAG内外的原因）确定性设定的，这相当于假设真实DGP是一个SCM。SCM可能是一个黑盒，或者我们可能无法轻易用数学或代码将其写下来，但它仍然是一个SCM。这意味着，无论我们使用传统的CGM还是SCM，我们都是在*模拟一个真实SCM*。
- en: Second, it suggests that if we were to generate from the ground-truth SCM, all
    the random variation in those samples would be *entirely due to exogenous causes*.
    It would *not be due to an irreducible source of stochasticity* like, for example,
    Heisenberg’s uncertainty principle or butterfly effects. If such concepts drive
    the outcomes in your modeling domain, CGMs might not be the best choice.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它暗示了如果我们从真实SCM生成样本，这些样本中的所有随机变化都将*完全归因于外生原因*。它*不会是由于不可还原的随机性来源*，例如，海森堡的不确定性原理或蝴蝶效应。如果这些概念驱动你的建模领域的输出，CGM可能不是最佳选择。
- en: Now that we know we want to model a ground-truth SCM, let’s explore why we can’t
    simply learn it from data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们想要模拟一个真实SCM，让我们探索为什么我们不能简单地从数据中学习它。
- en: 6.2 Equivalence between SCMs
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 SCMs之间的等价性
- en: A key thing to understand about SCMs is that we can’t fully learn them from
    data. To see why, let’s revisit the case where we turned a CGM into an SCM. Let’s
    see why, in general, this can’t give us the ground-truth SCM.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SCM的一个关键理解是，我们不能完全从数据中学习它们。为了了解原因，让我们回顾一下我们将CGM转换为SCM的情况。让我们看看为什么在一般情况下，这不能给我们提供真实SCM。
- en: 6.2.1 Reparameterization is not enough
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 重参数化是不够的
- en: When we converted the generic CGM to the SCM, we used the fact that a linear
    transformation of a normally distributed random variable produces a normally distributed
    random variable. This ensured that the joint probability distribution of the endogenous
    variables was unchanged.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将通用的CGM转换为SCM时，我们使用了这样一个事实：正态分布随机变量的线性变换会产生一个正态分布随机变量。这确保了内生变量联合概率分布保持不变。
- en: We could use this “reparameterization trick” (as this technique is called in
    generative AI) with other distributions. When we apply the reparameterization
    trick, we are shunting all the uncertainty in those conditional probability distributions
    to the distributions of the newly introduced exogenous variables. The problem
    is that different “reparameterization tricks” can lead to different SCMs with
    different causal assumptions, leading to different causal inferences.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这种“重新参数化技巧”（在生成式AI中被称为此技术）来处理其他分布。当我们应用重新参数化技巧时，我们将所有条件概率分布中的不确定性转移到新引入的外生变量分布上。问题是不同的“重新参数化技巧”可能导致具有不同因果假设的不同SCM（结构因果模型），从而导致不同的因果推断。
- en: Reparameterization trick for a Bernoulli distribution
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伯努利分布的重新参数化技巧
- en: As an example, let *X* represent the choice of a weighted coin and *Y* represent
    the outcome of a flip of the chosen coin. *Y* is 1 if we flip heads and 0 if we
    flip tails. *X* takes two values, “coin A” or “coin B”. Coin A has a .8 chance
    of flipping heads, and coin B has a .4 chance of flipping heads, as shown in figure
    6.5\.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让*X*代表选择一个加权硬币，*Y*代表选择硬币抛掷的结果。*Y*为1表示抛出正面，为0表示抛出反面。*X*有两个值，“硬币A”或“硬币B”。硬币A抛出正面的概率为.8，硬币B抛出正面的概率为.4，如图6.5所示。
- en: '![figure](../Images/CH06_F05_Ness.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F05_Ness.png)'
- en: Figure 6.5 A simple CGM. *X* is a choice of one of two coins with different
    weights on heads and tails. *Y* is the outcome of the coin flip (heads or tails).
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 一个简单的CGM（因果图模型）。*X*是选择两个不同重量正反面概率的硬币中的一个。*Y*是抛硬币的结果（正面或反面）。
- en: We can simulate an outcome of the flip with a variable *Y* sampled from a Bernoulli
    distribution with parameter *p*[*x*], where *p*[*x*] is .8 or .4, depending on
    the value of *x*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用变量*Y*从参数为*p*[*x*]的伯努利分布中采样来模拟抛硬币的结果，其中*p*[*x*]是.8或.4，这取决于*x*的值。
- en: '*y* ~ Bernoulli(*p*[*x*])'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* ~ 伯努利(*p*[*x*])'
- en: How could we apply the reparameterization trick here to make the outcome *Y*
    be the result of a deterministic process?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在这里应用重新参数化技巧，使得结果 *Y* 成为确定性过程的产物？
- en: Imagine that we have a stick that’s one meter long (figure 6.6).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一根一米的棍子（图6.6）。
- en: '![figure](../Images/CH06_F06_Ness.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F06_Ness.png)'
- en: Figure 6.6 To turn the coin flip model into an SCM, first imagine a one meter
    long stick.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 要将抛硬币模型转换为SCM（结构因果模型），首先想象一根一米长的棍子。
- en: 'Imagine using a pocket knife to carve a mark that partitions the stick into
    two regions: one corresponding to “tails” and one for “heads.” We cut the mark
    at a point that makes the length of each region proportional to the probability
    of the corresponding outcome; the length of the heads region is *p*[*x*] meters,
    and the length of the tails region is 1 – *p*[*x*] meters. For coin *A*, this
    would be .8 meters (80 centimeters) for the heads region and .2 meters for the
    tails region (figure 6.7).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 想象使用一把小刀在棍子上刻一个标记，将棍子分成两个区域：一个对应“反面”，一个对应“正面”。我们在这个点切标记，使得每个区域的长度与对应结果的概率成比例；正面的区域长度为*p*[*x*]米，反面的区域长度为1
    – *p*[*x*]米。对于硬币A，正面的区域将是0.8米（80厘米），反面的区域将是0.2米（图6.7）。
- en: '![figure](../Images/CH06_F07_Ness.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F07_Ness.png)'
- en: Figure 6.7 Divide the stick into two regions corresponding to each outcome.
    The length of the region is proportional to the probability of the outcome.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 将棍子分成两个区域，分别对应每个结果。区域的长度与结果的概率成比例。
- en: After marking the partition, we will now randomly select a point on the stick’s
    length where we will break the stick. The probability that the break will occur
    in a given region is equal to the probability of that region’s associated outcome
    (figure 6.8). The equality comes from having the length of the region correspond
    to the probability of the outcome. If the break point is to the left of the partition
    we cut with our pocket knife, *y* is assigned 0 (“heads”), and if the break point
    is to the right, *y* is assigned 1 (“tails”).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记分区后，我们现在将在棍子长度上随机选择一个点来折断棍子。折断发生在给定区域的概率等于该区域相关结果的概率（图6.8）。这种等式来自于区域的长度与结果的概率相对应。如果折断点在我们用口袋刀切分的分区左侧，则*y*被分配为0（“正面”），如果折断点在右侧，则*y*被分配为1（“反面”）。
- en: To randomly select a point to break the stick, we can generate from a uniform
    distribution. Suppose we sample .15 from a uniform(0, 1) and thus break the stick
    at a point .15 meters along its length, as shown in figure 6.8\. The .15 falls
    into the “heads” region, so we return heads. If we repeat this stick-breaking
    procedure many times, we’ll get samples from our target Bernoulli distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要随机选择一个点来折断木棍，我们可以从均匀分布中生成。假设我们从均匀分布（0, 1）中采样得到0.15，因此将木棍折断在长度为0.15米的位置，如图6.8所示。0.15落在“正面”区域，因此我们返回正面。如果我们重复进行多次折断木棍的程序，我们将从我们的目标伯努利分布中获得样本。
- en: 'In math, we can write this new model as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，我们可以将这个新模型写成以下形式：
- en: '*n*[*y*] ~ Uniform(0, 1)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ Uniform(0, 1)'
- en: '*y* := *I*(*n*[*y*] ≤ *p*[*x*])'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* := *I*(*n*[*y*] ≤ *p*[*x*])'
- en: where *p*[*x*] is .8 if *X* is coin *A*, or .4 if *X* is coin *B*. Here, *I*(.)
    is the indicator function that returns 1 if *n*[*y*] < *p*[*x*] and 0 otherwise.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[*x*] 如果 *X* 是硬币 *A*，则为0.8，如果 *X* 是硬币 *B*，则为0.4。在这里，*I*(.) 是指示函数，当 *n*[*y*]
    < *p*[*x*] 时返回1，否则返回0。
- en: '![figure](../Images/CH06_F08_Ness.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F08_Ness.png)'
- en: Figure 6.8 Generate from a uniform distribution on 0 to 1 meters, break the
    stick at that point, and return the outcome associated with the region where the
    break occurred. Repeated generation of uniform variates will cause breaks in the
    “heads” region 80% of the time, because its length is 80% of the full stick length.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8 从0到1米的均匀分布中生成，在该点折断木棍，并返回与折断发生区域相关的结果。重复生成均匀变量将导致80%的时间在“正面”区域折断，因为其长度是整个木棍长度的80%。
- en: This new model is technically an SCM, because instead of *Y* being generated
    from a Bernoulli distribution, it is set deterministically by an indicator “assignment”
    function. We did a reparameterization that shunted all the randomness to an exogenous
    variable with a uniform distribution, and that variable is passed to the assignment
    function.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新模型在技术上是一个SCM，因为不是 *Y* 从伯努利分布生成，而是由一个指示“分配”函数确定。我们进行了重新参数化，将所有随机性转移到具有均匀分布的外生变量上，并将该变量传递给分配函数。
- en: Different “reparameterization tricks” lead to different SCMs
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同的“重新参数化技巧”会导致不同的SCM。
- en: The main reason to use SCM modeling is to have the functional assignments represent
    causal assumptions beyond those captured by the causal DAG. The problem with the
    reparameterization trick is that different reparameterization tricks applied to
    the same CGM will create SCMs with different assignment functions, implying different
    causal assumptions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SCM建模的主要原因是要让功能分配表示超出因果DAG所捕获的因果假设。重新参数化技巧的问题是，应用于相同CGM的不同重新参数化技巧将创建具有不同分配函数的SCM，这意味着不同的因果假设。
- en: To illustrate, suppose that instead of a coin flip, *Y* was a three-sided die,
    like we saw in chapter 2 (figure 6.9). *X* determines which die we’ll throw; die
    A or die B (figure 6.10). Each die is weighted differently, so they have different
    probabilities of rolling a 1, 2, or 3.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设不是抛硬币，而是 *Y* 是一个三面骰子，就像我们在第2章中看到的（图6.9）。*X* 决定了我们将掷哪个骰子；骰子A或骰子B（图6.10）。每个骰子的重量不同，因此它们掷出1、2或3的概率也不同。
- en: '![figure](../Images/CH06_F09_Ness.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F09_Ness.png)'
- en: Figure 6.9 Three-sided dice
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 三面骰子
- en: '![figure](../Images/CH06_F10_Ness.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F10_Ness.png)'
- en: Figure 6.10 Suppose we switch the model from choosing a coin (two outcomes)
    to choosing a three-sided die (three outcomes).
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10 假设我们将模型从选择硬币（两个结果）切换到选择三面骰子（三个结果）。
- en: 'We can extend the original model from a Bernoulli distribution (which is the
    same as a categorical distribution with two outcomes) to a categorical distribution
    with three outcomes:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将原始模型从伯努利分布（等同于具有两个结果的分类分布）扩展到具有三个结果的分类分布：
- en: '*y* ~ Categorical([*p*[*x*][1], *p*[*x*][2], *p*[*x*][3]])'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* ~ Categorical([*p*[*x*][1], *p*[*x*][2], *p*[*x*][3]])'
- en: where *p*[*x*][1], *p*[*x*][2], and *p*[*x*][3] are the probabilities of rolling
    a 1, 2, and 3 respectively (note that one of these is redundant, since *p*[*x*][1]
    = 1 – *p*[*x*][2] – *p*[*x*][3]).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[*x*][1]、*p*[*x*][2] 和 *p*[*x*][3] 分别是掷出1、2和3的概率（注意其中一个是冗余的，因为 *p*[*x*][1]
    = 1 – *p*[*x*][2] – *p*[*x*][3]）。
- en: We can use the stick-based reparameterization trick here as well; we just need
    to extend the stick to have one more region. Suppose for die *A*, the probability
    of rolling a 1 is *p*[*x*][1]=.1, rolling a 2 is *p*[*x*][2]=.3, and rolling a
    3 is *p*[*x*][3]=.6\. We’ll mark our stick as in figure 6.11.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在这里使用基于棍子的重新参数化技巧；我们只需要将棍子扩展到一个额外的区域。假设对于骰子*A*，掷出1的概率是*p*[*x*][1]=.1，掷出2的概率是*p*[*x*][2]=.3，掷出3的概率是*p*[*x*][3]=.6。我们将按照图6.11所示标记我们的棍子。
- en: '![figure](../Images/CH06_F11_Ness.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F11_Ness.png)'
- en: Figure 6.11 Divide the stick into three regions corresponding to outcomes of
    the three-sided die.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11 将棍子分为三个区域，对应于三面骰子的结果。
- en: We’ll then use the same selection of a remote region using a generated uniform
    variate as before (figure 6.12).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用之前使用生成的均匀变量选择远程区域的方法（图6.12）。
- en: '![figure](../Images/CH06_F12_Ness.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F12_Ness.png)'
- en: Figure 6.12 The conversion to the stick-breaking SCM when *Y* has three outcomes
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12 当*Y*有三个结果时，转换为棍子打破SCM
- en: 'In math we’ll write this as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，我们将这样写：
- en: '![figure](../Images/ness-ch6-eqs-4x.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-4x.png)'
- en: But what if we mark the stick differently, such that we change the ordering
    of the regions on the stick? In the second stick, the region order is 3, 1, and
    then 2 (figure 6.13).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们用不同的方式标记这根棍子，从而改变棍子上区域排列的顺序呢？在第二根棍子上，区域的顺序是3，1，然后是2（图6.13）。
- en: '![figure](../Images/CH06_F13_Ness.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F13_Ness.png)'
- en: Figure 6.13 Two different ways of reparameterizing a causal generative model
    yield two different SCMs. They encode the same joint probability distribution
    but different endogenous values given the same exogenous value.
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13 重新参数化因果生成模型的不同方式产生两个不同的SCM。它们编码相同的联合概率分布，但给定相同的外生值，具有不同的内生值。
- en: In terms of the probability of each outcome (1, 2, or 3), the two sticks are
    equivalent—the size of the stick regions assigned to each die-roll outcome are
    the same on both sticks. But our causal mechanism has changed! These two sticks
    can return *different* outcomes for a given value of *n*[*y*]. If we randomly
    draw .15 and thereby break the sticks at the .15 meter point, the first stick
    will break in region 2, returning a 2, and the second stick will break in region
    3, returning a 3.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个结果（1、2或3）的概率来看，两根棍子是等价的——分配给每个骰子掷出结果的棍子区域大小在两根棍子上是相同的。但我们的因果机制已经改变！这两根棍子对于给定的*n*[*y*]值可以返回*不同*的结果。如果我们随机抽取.15并因此将棍子折断在.15米处，第一根棍子将在区域2折断，返回2，而第二根棍子将在区域3折断，返回3。
- en: 'In math, the second stick-breaking SCM has this form:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，第二个棍子打破的SCM具有以下形式：
- en: '![figure](../Images/ness-ch6-eqs-5x.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-5x.png)'
- en: Metaphorically speaking, imagine that in your modeling domain, the sticks are
    always marked a certain way, with the regions ordered in a certain way. Then there
    is no guarantee that a simple reparameterization trick will give you the ground-truth
    marking.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 比喻地说，想象一下在你的建模领域中，棍子总是以某种方式标记，区域以某种方式排列。那么，简单的重新参数化技巧并不能保证给出正确的标记。
- en: To drive the point home, let’s look back at the reparameterization trick we
    performed to convert our femur-height model to an SCM (figure 6.14).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个观点更加明确，让我们回顾一下我们将股骨高度模型转换为SCM所执行的重新参数化技巧（图6.14）。
- en: '![figure](../Images/CH06_F14_Ness.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F14_Ness.png)'
- en: Figure 6.14 Revisiting the femur-height SCM
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.14 回顾股骨高度SCM
- en: 'Suppose we create a new SCM that is the same, except that the assignment function
    for *y* now looks like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们创建一个新的SCM，它与原来的相同，只是*y*的分配函数现在看起来像这样：
- en: '*y* := 25 + 3*x* – *n*[*y*]'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* := 25 + 3*x* – *n*[*y*]'
- en: Now we have a second SCM that subtracts *n*[*y*] instead of adding *n*[*y*].
    A normal distribution is symmetric around its mean, so since *n*[*y*] has a normal
    distribution with mean 0, the probability values of *n*[*y*] and –*n*[*y*] are
    the same, so the probability distribution of *Y* is the same in both models. But
    for the same values of *n*[*y*] and *x*, the actual assigned values of *y* will
    be different. Next, we’ll examine this idea in formal detail.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个第二个SCM，它减去*n*[*y*]而不是加上*n*[*y*]。正态分布是对称的，所以*n*[*y*]具有均值为0的正态分布，因此*n*[*y*]和-
    *n*[*y*]的概率值相同，所以*Y*的概率分布在两个模型中是相同的。但对于相同的*n*[*y*]和*x*值，*y*的实际分配值将是不同的。接下来，我们将对这个想法进行正式的详细探讨。
- en: 6.2.2 Uniqueness and equivalence of SCMs
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 SCM的唯一性和等价性
- en: Given a causal DAG and a joint probability distribution on endogenous variables,
    there can generally be multiple SCMs consistent with that DAG and joint probability
    distribution. This means that we can’t rely on data alone to learn the ground-truth
    SCM. We’ll explore this problem of *causal identifiability* in depth in chapter
    10\. For now, let’s break this idea down using concepts we’ve seen so far.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个因果有向无环图（DAG）和内生变量的联合概率分布，通常可以存在多个与该DAG和联合概率分布一致的统计因果模型（SCM）。这意味着我们不能仅仅依靠数据来学习真实的SCM。我们将在第10章深入探讨这个*因果可识别性*问题。现在，让我们使用我们迄今为止看到的概念来分解这个想法。
- en: Many SCMs are consistent with a DAG and corresponding distributions
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 许多SCM与一个DAG和相应的分布一致
- en: Recall the many-to-one relationships we outlined in figure 2.24, shown again
    here in figure 6.15.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在图2.24中概述的多对一关系，这里再次在图6.15中展示。
- en: '![figure](../Images/CH06_F15_Ness.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F15_Ness.png)'
- en: Figure 6.15 We have many-to-one relationships as we move from the DGP to observed
    data.
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.15 当我们从数据生成过程（DGP）移动到观测数据时，存在多对一的关系。
- en: If we can represent the underlying DGP as a ground-truth SCM, figure 6.15 becomes
    as shown in figure 6.16.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将潜在的DGP表示为一个真实的SCM，图6.15就会变成图6.16所示的样子。
- en: '![figure](../Images/CH06_F16_Ness.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F16_Ness.png)'
- en: Figure 6.16 Different SCMs can entail the same DAG structure and distributions.
    The SCMs can differ in assignment functions (and/or exogenous distributions).
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.16 不同的SCM可以蕴含相同的DAG结构和分布。SCM可以在分配函数（和/或外生分布）上有所不同。
- en: In other words, given a joint distribution on a set of variables, there can
    be multiple causal DAGs consistent with that distribution—in chapter 4 we called
    these DAGs a *Markov equivalence class*. Further, we can have *equivalence classes
    of SCMs*—given a causal DAG and a joint distribution, there can be multiple SCMs
    consistent with that DAG and distribution. We saw this with how the two variants
    of the stick-breaking die-roll SCM are both consistent with the DAG *X* (die choice)
    → *Y* (die roll) and with the distributions *P*(*X*) (probability distribution
    on die selection) and *P*(*Y*|*X*) (probability of die roll).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，给定一组变量的联合分布，可以存在多个与该分布一致的因果DAG——在第4章中我们称这些DAG为*马尔可夫等价类*。进一步，我们可以有*SCM的等价类*——给定一个因果DAG和联合分布，可以存在多个与该DAG和分布一致的SCM。我们通过stick-breaking骰子滚动SCM的两个变体都是与DAG
    *X*（骰子选择）→ *Y*（骰子滚动）以及分布*P*(*X*)（骰子选择的概率分布）和*P*(*Y*|*X*)（骰子滚动的概率）一致的例子看到了这一点。
- en: The ground-truth SCM can’t be learned from data (without causal assumptions)
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 真实的SCM不能从数据中学习（没有因果假设）
- en: When we were working to build a causal DAG in previous chapters, our implied
    objective was to reproduce the ground-truth causal DAG. Now we seek to reproduce
    the ground-truth SCM, as in figure 6.16.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前几章中努力构建因果DAG时，我们的隐含目标是重现真实的因果DAG。现在我们寻求重现图6.16中的真实SCM。
- en: 'In chapter 4, we saw that data cannot distinguish between causal DAGs in an
    equivalence class of DAGs. Similarly, data alone is not sufficient to recover
    the ground-truth SCM. Again, consider the stick-breaking SCMs we derived. We derived
    two marked sticks, with two different orderings of regions. Of course, there are
    3 × 2 × 1 = 6 ways of ordering the three outcomes: ({1, 2, 3}, {1, 3, 2}, {2,
    1, 3}, {2, 3, 2}, {3, 1, 2}, {3, 2, 1}). That’s six ways of marking the stick
    and thus six different possible SCMs consistent with the distributions *P*(*X*)
    and *P*(*Y*|*X*) (probability of die roll).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们看到了数据无法区分等价类中的因果DAG。同样，仅凭数据本身不足以恢复真实的SCM。再次考虑我们推导出的stick-breaking SCM。我们推导出两个标记的棍子，区域排序不同。当然，有三个结果排序的方式：({1,
    2, 3}, {1, 3, 2}, {2, 1, 3}, {2, 3, 2}, {3, 1, 2}, {3, 2, 1})。这就是六种标记棍子的方式，因此有六个与分布*P*(*X*)和*P*(*Y*|*X*)（骰子滚动的概率）一致的可能的SCM。
- en: Suppose one of these marked sticks was the ground-truth SCM, and it was hidden
    from us in a black box, as in figure 6.17\. Suppose we repeatedly ran the SCM
    to generate some die rolls. Based on those die rolls, could we figure out how
    the ground-truth stick was marked? In other words, which of the six orderings
    was the black box ordering?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这些标记的棍子中的一个就是真实的SCM，并且它被隐藏在一个黑盒子里，如图6.17所示。假设我们反复运行SCM来生成一些骰子滚动结果。基于这些骰子滚动结果，我们能否找出真实棍子的标记？换句话说，六个排序中的哪一个就是黑盒子的排序？
- en: '![figure](../Images/CH06_F17_Ness.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F17_Ness.png)'
- en: Figure 6.17 Suppose we didn’t know which “marked stick” was generating the observed
    die rolls. There would be no way of inferring the correct marked stick from the
    die rolls alone. More generally, SCMs cannot be learned from statistical information
    in the data alone.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.17 假设我们不知道哪个“标记棒”产生了观察到的骰子滚动。仅从骰子滚动中无法推断出正确的标记棒。更普遍地说，SCM不能仅从数据中的统计信息中学习。
- en: The answer is no. More generally, because of the many-to-one relationship between
    SCMs and data, you cannot learn the ground-truth SCM from statistical information
    in the data alone.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是否定的。更普遍地说，由于SCM和数据之间存在多对一的关系，你无法仅从数据中的统计信息中学习到真实的SCM。
- en: Let that sink in for a second. I’m telling you that even with infinite data,
    the most cutting-edge deep learning architecture, and a bottomless compute budget,
    you cannot figure out the true SCM even in this trivial three-outcome stick-breaking
    example. In terms of statistical likelihood, each SCM is equally likely, given
    the data. To prefer one SCM to another in the equivalence class, you would need
    additional assumptions, such as that {1, 2, 3} is the most likely marking because
    the person marking the stick would probably mark the regions in order. That’s
    a fine assumption to make, as long as you are *aware* you are making it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让这个想法在脑海中停留一会儿。我在告诉你，即使拥有无限的数据，最前沿的深度学习架构，以及无限的计算预算，你甚至在这个简单的三个结果的棒子断裂例子中都无法找出真正的SCM。从统计概率的角度来看，每个SCM在给定数据的情况下都是等可能的。要在等价类中偏好一个SCM而不是另一个，你需要额外的假设，例如，{1,
    2, 3} 是最可能的标记，因为标记棒子的人可能会按顺序标记区域。这是一个很好的假设，只要你意识到你正在做出这个假设。
- en: In the practice of machine learning, we are often unaware that we are making
    such assumptions. To illustrate, suppose you ran the following experiment. You
    created a bunch of stick-breaking SCMs and then simulated data from those SCMs.
    Then you vectorized the SCMs and used them as labels, and the simulated data as
    features, in a deep supervised learning training procedure focused on predicting
    the “true” SCM from simulated data, as illustrated in figure 6.18.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的实践中，我们常常没有意识到我们正在做出这样的假设。为了说明这一点，假设你进行了以下实验。你创建了一组棒子断裂SCM，并从这些SCM中模拟数据。然后你将SCM向量化，并将它们用作标签，将模拟数据作为特征，在图6.18中所示的深度监督学习训练过程中，专注于从模拟数据中预测“真实”的SCM。
- en: '![figure](../Images/CH06_F18_Ness.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F18_Ness.png)'
- en: Figure 6.18 You create many SCMs and simulate data from each of them. You could
    then do supervised learning of a deep net that predicted the ground-truth SCM
    from the simulated data. Given two SCMs of the same equivalence class, this approach
    would favor the SCM with attributes that appeared more often in the training data.
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.18 你创建了多个SCM，并从每个SCM中模拟数据。然后你可以对从模拟数据中预测真实SCM的深度神经网络进行监督学习。对于同一等价类中的两个SCM，这种方法将倾向于在训练数据中出现频率更高的属性的那个SCM。
- en: Suppose then you fed the trained model data actual samples of three-sided die
    rolls, with the goal of predicting the ground-truth SCM. That predictive model’s
    prediction might favor a stick with the {1, 2, 3} ordering over the equivalent
    {2, 3, 1} ordering. But it would only do so if the {1, 2, 3} ordering was more
    common in the training data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 假设然后你将训练好的模型数据实际的三面骰子滚动的样本输入，目标是预测真实的SCM。那个预测模型的预测可能会倾向于具有{1, 2, 3}排序的棒子，而不是等效的{2,
    3, 1}排序。但只有当{1, 2, 3}排序在训练数据中更常见时，它才会这样做。
- en: Analogy to program induction
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类比于程序归纳
- en: The problem of learning an SCM from data is related to the challenge of program
    induction in computer science. Suppose a program took “foo” and “bar” as inputs
    and returned “foobar” as the output. What is the program? You might think that
    the program simply concatenates the inputs. But it could be anything, including
    one that concatenates the inputs along with the word “aardvark”, then deletes
    the “aardvark” characters, and returns the result. The “data” (many examples of
    inputs to and outputs of the program) are not enough distinguish which program
    of all the possible programs is the correct one. For that you need additional
    assumptions or constraints, such as an Occam’s razor type of inductive bias that
    prefers the simplest program (e.g., the program with the *minimum description
    length*).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习SCM的问题与计算机科学中程序归纳的挑战相关。假设一个程序以“foo”和“bar”作为输入，并返回“foobar”作为输出。这个程序是什么？你可能认为程序只是简单地连接输入。但它可以是任何东西，包括一个将输入与单词“aardvark”连接起来，然后删除“aardvark”字符，并返回结果的程序。数据（许多程序的输入和输出示例）不足以区分所有可能的程序中哪一个是正确的。为此，你需要额外的假设或约束，例如奥卡姆剃刀类型的归纳偏见，它更喜欢最简单的程序（例如，具有*最小描述长度*的程序）。
- en: Trying to learn an SCM from data is a special case of this problem. The program’s
    inputs are the exogenous variable values, and the outputs are the endogenous variable
    values. Suppose you have the causal DAG, just not the assignment functions. The
    problem is that an infinite number of assignment functions could produce those
    outputs, given the inputs. Learning an SCM from data requires additional assumptions
    to constrain the assignment functions, such as constraining the function class
    and using Occam’s razor (e.g., model selection criterion).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试从数据中学习SCM是此类问题的特殊情况。程序的输入是外生变量值，输出是内生变量值。假设你有了因果DAG，但没有分配函数。问题是，给定输入，有无限多的分配函数可以产生那些输出。从数据中学习SCM需要额外的假设来约束分配函数，例如约束函数类和使用奥卡姆剃刀（例如，模型选择标准）。
- en: Next, we’ll dive into implementing an SCM in a discrete rule-based setting.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨在离散的基于规则的设置中实现SCM。
- en: 6.3 Implementing SCMs for rule-based systems
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 实现基于规则的系统的SCM
- en: A particularly useful application for SCMs is modeling rule-based systems. By
    “rule-based,” I mean that known rules, often set by humans, determine the “how”
    of causality. Games are a good example.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: SCM的一个特别有用的应用是建模基于规则的系统。通过“基于规则”，我指的是已知规则，通常由人类设定，决定了因果关系的“如何”。游戏是一个很好的例子。
- en: To illustrate, consider the *Monty Hall problem*—a probability-based brain teaser
    named after the host of a 1960’s game show with a similar setup.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，考虑一下*蒙提霍尔问题*——一个以1960年代一个类似设置的电视游戏节目主持人命名的基于概率的智力题。
- en: '6.3.1 Case study: The Monty Hall problem'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 案例研究：蒙提霍尔问题
- en: A contestant on a game show is asked to choose between three closed doors. Behind
    one door is a car; behind the others, goats. The player picks the first door.
    Then the host, who knows what’s behind the doors, opens another door, for example
    the third door, which has a goat. The host then asks the contestant, “Do you want
    to switch to the second door, or do you want to stay with your original choice?”
    The question is which is the better strategy, switching doors or staying?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个游戏节目中，参赛者被要求在三个关闭的门中选择。其中一扇门后面有一辆车；其他门后面是山羊。玩家选择了第一个门。然后，知道门后面是什么的主持人打开另一扇门，例如第三扇门，那扇门后面有山羊。主持人然后问参赛者，“你想换到第二扇门，还是想坚持你的原始选择？”问题是哪种策略更好，换门还是坚持？
- en: The correct answer is to switch doors. This question appeared in a column in
    *Parade* magazine in 1990, with the correct answer. Thousands of readers mailed
    in, including many with graduate-level mathematical training, to refute the answer
    and say that there is no advantage to switching, that staying or switching have
    the same probability of winning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案是换门。这个问题于1990年出现在《Parade》杂志的一篇文章中，并给出了正确答案。成千上万的读者邮寄了回复，包括许多受过研究生水平数学训练的人，他们试图反驳这个答案，并说换门没有优势，坚持或换门赢得比赛的概率相同。
- en: Figure 6.19 illustrates the intuition behind why switching is better. Switching
    doors is the correct answer because under the standard assumptions, the “switch”
    strategy has a probability of two-thirds of winning the car, while the “stay”
    strategy has only a one-third probability. It seems counterintuitive because each
    door has an equal chance of having the car when the game starts. It seems as if,
    once the host eliminates one door, each remaining door should have a 50-50 chance.
    This logic is false, because the host doesn’t eliminate a door at random. He only
    eliminates a door that isn’t the player’s initial selection *and* that doesn’t
    have the car. A third of the times, those are the same door, and two-thirds of
    the time they are different doors; that one-third to two-thirds asymmetry is why
    the remaining doors don’t each have a 50-50 chance of having the car.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19说明了为什么换门是更好的直觉。换门是正确答案，因为在标准假设下，“换门”策略赢得汽车的概率是三分之二，而“留下”策略只有三分之一的概率。这看起来似乎不合逻辑，因为当游戏开始时，每个门都有相等的机会藏有汽车。似乎一旦主持人排除了一个门，剩下的每个门都应该有50-50的机会。这种逻辑是错误的，因为主持人不是随机排除一个门。他只排除一个既不是玩家的初始选择，又没有汽车的门。三分之一的时候，这些是同一个门，三分之二的时候是不同的门；这种三分之一到三分之二的非对称性就是为什么剩下的门不各自有50-50的机会藏有汽车。
- en: '![figure](../Images/CH06_F19_Ness.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F19_Ness.png)'
- en: Figure 6.19 The Monty Hall problem. Each door has an equal probability of concealing
    a prize. The player chooses a door initially, the host reveals a losing door,
    and the player has the option to switch their initial choice. Contrary to intuition,
    the player should switch; if they switch, they will win two out of three times.
    This illustration assumes door 1 is chosen, but the results are the same regardless
    of the initial choice of door.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.19 蒙提霍尔问题。每个门都有相等的机会藏有奖品。玩家最初选择一个门，主持人揭示一个输掉的门，玩家有选择是否换门的选项。与直觉相反，玩家应该换门；如果他们换门，他们将赢得三分之二的时间。这个插图假设选择了门1，但无论初始选择哪个门，结果都是相同的。
- en: 6.3.2 A causal DAG for the Monty Hall problem
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 蒙提霍尔问题的因果DAG
- en: Causal modeling makes the Monty Hall problem much more intuitive. We can represent
    this game with the causal DAG in figure 6.20.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因果建模使蒙提霍尔问题变得更加直观。我们可以用图6.20中的因果DAG来表示这个游戏。
- en: '![figure](../Images/CH06_F20_Ness.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F20_Ness.png)'
- en: Figure 6.20 A causal DAG for the Monty Hall problem
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.20 蒙提霍尔问题的因果DAG
- en: 'The possible outcomes for each variable are as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量的可能结果如下：
- en: '*Door with Car*—Indicates the door that has the car behind it. 1^(st) for the
    first door, 2^(nd) for the second door, or 3^(rd) for the third door.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*藏有汽车的门*—表示后面有汽车的门。第1个门用1^(st)表示，第2个门用2^(nd)表示，或者第3个门用3^(rd)表示。'
- en: '*Player First Choice*—Indicates which door the player chooses first. 1^(st)
    for the first door, 2^(nd) for the second door, or 3^(rd) for the third door.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*玩家第一次选择*—表示玩家最初选择的门。第1个门用1^(st)表示，第2个门用2^(nd)表示，或者第3个门用3^(rd)表示。'
- en: '*Host Inclination*—Suppose the host is facing the doors, such that from left
    to right they are ordered 1^(st), 2^(nd), and 3^(rd). This *Host Inclination*
    variable has two outcomes, Left and Right. When the outcome is Left, the host
    is inclined to choose the left-most available door; otherwise the host will be
    inclined to choose the right-most available door.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主持人倾向*—假设主持人面对着门，从左到右它们是按1^(st)、2^(nd)、3^(rd)的顺序排列的。这个*主持人倾向*变量有两个结果，左和右。当结果是左时，主持人倾向于选择最左边的可用门；否则，主持人将倾向于选择最右边的可用门。'
- en: '*Host Door Selection*—The outcomes are again 1^(st), 2^(nd), and 3^(rd).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主持人门选择*—结果仍然是第1个、第2个和第3个。'
- en: '*Strategy*—The outcomes are Switch if the strategy is to switch doors from
    the first choice, or Stay if the strategy is to stay with the first choice.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略*—结果是换门，如果策略是从第一个选择换门，或者留下，如果策略是保留第一个选择。'
- en: '*Player Second Choice*—Indicates which door the player chooses after being
    asked by the host whether they want to switch or not. The outcomes again are 1^(st),
    2^(nd), and 3^(rd).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*玩家第二次选择*—表示在主持人询问玩家是否想要换门后，玩家选择的门。结果仍然是第1个、第2个和第3个。'
- en: '*Win or Lose*—Indicates whether the player wins; the outcomes are Win or Lose.
    Winning occurs when *Player Second Choice* == *Door with Car*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*赢或输*—表示玩家是否赢；结果是赢或输。当*玩家第二次选择*等于*藏有汽车的门*时，即为赢。'
- en: Next, we’ll see how to implement this as an SCM in pgmpy.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何将其作为SCM在pgmpy中实现。
- en: 6.3.3 Implementing Monty Hall as an SCM with pgmpy
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 使用pgmpy实现蒙提霍尔问题作为SCM
- en: The rules of the game give us clear logic for the assignment functions. For
    example, we can represent the assignment function for *Host Door Selection* with
    table 6.1.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏规则为我们提供了明确的分配函数逻辑。例如，我们可以用表6.1表示*主持人门选择*的分配函数。
- en: Table 6.1 A lookup table for *Host Door Selection*, given *Player First Choice,
    Door with Car**,* and *Host Inclination*. It shows which door the host selects,
    given the player’s first choice, which door has the car, and the *Host Inclination*,
    which refers to whether the host will choose the left-most or right-most door
    in cases when the host has two doors to choose from.
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1：给定*玩家第一个选择*、*带车的门*和*主持人倾向*时，*主持人门选择*的查找表。它显示了主持人根据玩家的第一个选择、有车的门以及主持人倾向（指主持人是否会在有两个选择时选择最左边的或最右边的门）选择的门。
- en: '| Host Inclination | Left | Right |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **主持人倾向** | 左侧 | 右侧 |'
- en: '| --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Door with Car**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **带车的门**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  |'
- en: '| **Player First Choice**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  |
    2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第一个选择**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |
    1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
- en: '| **Host Door Selection**  | 2^(nd)  | 3^(rd)  | 2^(nd)  | 3^(rd)  | 1^(st)  |
    1^(st)  | 2^(nd)  | 1^(st)  | 1^(st)  | 3^(rd)  | 3^(rd)  | 2^(nd)  | 3^(rd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 1^(st)  | 2^(nd)  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **主持人门选择**  | 2^(nd)  | 3^(rd)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 1^(st)  |
    2^(nd)  | 1^(st)  | 1^(st)  | 3^(rd)  | 3^(rd)  | 2^(nd)  | 3^(rd)  | 3^(rd)  |
    1^(st)  | 2^(nd)  | 1^(st)  | 2^(nd)  |'
- en: When the door with the car and the player’s first choice are different doors,
    the host can only choose the remaining door. But if the door with the car and
    the player’s first choice are the same door, the host has two doors to choose
    from. He will choose the left-most door if *Host Inclination* is Left. For example,
    if *Door with Car* and *Player First Choice* are both 1^(st), the host must choose
    between the 2^(nd) and 3^(rd) doors. He will choose the 2^(nd) door if *Host Inclination*
    == Left and the 3^(rd) if *Host Inclination* == Right.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当带车的门和玩家的第一个选择不同时，主持人只能选择剩下的门。但如果带车的门和玩家的第一个选择是同一扇门，主持人有两个门可以选择。如果*主持人倾向*是左侧，他将选择最左边的门。例如，如果*带车的门*和*玩家第一个选择*都是1^(st)，主持人必须在2^(nd)和3^(rd)门之间选择。如果*主持人倾向*
    == 左侧，他将选择2^(nd)门；如果*主持人倾向* == 右侧，他将选择3^(rd)门。
- en: This logic would be straightforward to write using if-then logic with a library
    like Pyro. But since the rules are simple, we can use the far more constrained
    pgmpy library to write this function as a conditional probability table (table
    6.2).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像Pyro这样的库编写if-then逻辑来表示这种逻辑非常直接。但由于规则很简单，我们可以使用更加受限的pgmpy库来将此函数编写为一个条件概率表（表6.2）。
- en: Table 6.2 We can convert the *Host Door Selection* lookup table (table 6.1)
    to a conditional probability table that we can implement as a `TabularCPD` object
    in pgmpy, where the probability of a given outcome is 0 or 1, and thus, deterministic.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.2：我们可以将*主持人门选择*的查找表（表6.1）转换为条件概率表，我们可以在pgmpy中将它实现为一个`TabularCPD`对象，其中给定结果的概率为0或1，因此是确定性的。
- en: '| Host Inclination | Left | Right |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **主持人倾向** | 左侧 | 右侧 |'
- en: '| --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Door with Car**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **带车的门**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  |'
- en: '| **Player First Choice**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第一个选择**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  |'
- en: '| **Host Door Selection**  | 1^(st)  | 0  | 0  | 0  | 0  | 1  | 1  | 0  | 1  |
    1  | 0  | 0  | 0  | 0  | 0  | 1  | 0  | 1  | 0  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **主持人门选择**  | 1^(st)  | 0  | 0  | 0  | 0  | 1  | 1  | 0  | 1  | 1  | 0  |
    0  | 0  | 0  | 0  | 1  | 0  | 1  | 0  |'
- en: '| 2^(nd)  | 1  | 0  | 1  | 0  | 0  | 0  | 1  | 0  | 0  | 0  | 0  | 1  | 0  |
    0  | 0  | 1  | 0  | 1  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 2^(nd)  | 1  | 0  | 1  | 0  | 0  | 0  | 1  | 0  | 0  | 0  | 0  | 1  | 0  |
    0  | 0  | 1  | 0  | 1  |'
- en: '| 3^(rd)  | 0  | 1  | 0  | 1  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 0  | 1  |
    1  | 0  | 0  | 0  | 0  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 3^(rd)  | 0  | 1  | 0  | 1  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 0  | 1  |
    1  | 0  | 0  | 0  | 0  |'
- en: The entries in the table correspond to the probability of the *Host Door Selection*
    outcome given the values of the causes. Each probability outcome is either 0 or
    1, given the causal parents, so the outcome is completely deterministic given
    the parents. Therefore, we can use this as our assignment function, and since
    it is a conditional probability table, we can implement it using the `TabularCPD`
    class in pgmpy.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的条目对应于在原因值给定的情况下主持人门选择结果发生的概率。每个概率结果要么是0要么是1，给定因果父节点，因此给定父节点，结果完全确定。因此，我们可以将其用作我们的分配函数，并且由于它是一个条件概率表，我们可以使用pgmpy中的`TabularCPD`类来实现它。
- en: Listing 6.4 Implementation of *Host Door Selection* assignment function in pgmpy
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4 在pgmpy中实现主持人门选择分配函数
- en: '[PRE3]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 The name of the variable'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 变量的名称'
- en: '#2 The cardinality (number of outcomes)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 卡迪纳尔（结果数量）'
- en: '#3 The probability table. The values match the value in table 6.2, as long
    as the ordering of the causal variables in the evidence argument matches the top-down
    ordering of causal variable names in the table.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 概率表。值与表6.2中的值匹配，只要证据参数中因果变量的顺序与表中因果变量名称的从上到下顺序一致。'
- en: '#4 The conditioning (causal) variables'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 条件（因果）变量'
- en: '#5 The cardinality (number of outcomes) for each conditioning (causal) variable'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 每个条件（因果）变量的卡迪纳尔（结果数量）'
- en: '#6 The state names of each the variables'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 每个变量的状态名称'
- en: This code produces `f_host_door_selection`, a `TabularCPD` object we can add
    to a model of the class `BayesianNetwork`. We can then use this in a CGM as we
    would a more typical `TabularCPD` object.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成`f_host_door_selection`，一个我们可以添加到`BayesianNetwork`类模型的`TabularCPD`对象。然后我们可以像使用更典型的`TabularCPD`对象一样使用它。
- en: Similarly, we can create a look-up table for *Player Second Choice*, as shown
    in table 6.3.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以创建一个玩家第二选择的查找表，如表6.3所示。
- en: Table 6.3 A lookup table for *Player Second Choice*, conditional on *Player
    First Choice*, *Host Door Selection*, and *Strategy. Player Second Choice* cells
    are empty in the impossible cases where *Player First Choice* and *Host Door Selection*
    are the same.
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.3 根据玩家第一选择、主持人门选择和策略条件下的玩家第二选择查找表。“玩家第二选择”单元格在玩家第一选择和主持人门选择相同的不可能情况下为空。
- en: '| Strategy | Stay | Switch |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 保持 | 切换 |'
- en: '| --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Host Door Selection**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| **主持人门选择**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  |'
- en: '| **Player First Choice**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  |
    2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第一选择**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |
    1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
- en: '| **Player Second Choice**  |  | 2^(nd)  | 3^(rd)  | 1^(st)  |  | 3^(rd)  |
    1^(st)  | 2^(nd)  |  |  | 3^(rd)  | 2^(nd)  | 3^(rd)  |  | 1^(st)  | 2^(nd)  |
    1^(st)  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第二选择**  |  | 2^(nd)  | 3^(rd)  | 1^(st)  |  | 3^(rd)  | 1^(st)  | 2^(nd)  |  |  |
    3^(rd)  | 2^(nd)  | 3^(rd)  |  | 1^(st)  | 2^(nd)  | 1^(st)  |  |'
- en: The host will never choose the same door as the player’s first choice, so *Host
    Door Selection* and *Player First Choice* can never have the same value. The entries
    of *Player Second Choice* are not defined in these cases.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 主持人永远不会选择与玩家第一选择相同的门，因此主持人门选择和玩家第一选择永远不会具有相同的值。在这些情况下，玩家第二选择的条目未定义。
- en: Expanding this to a conditional probability table gives us table 6.4\. Again,
    the cells with impossible outcomes are left blank.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 将此扩展为条件概率表，我们得到表6.4。同样，不可能结果单元格留空。
- en: Table 6.4 The result of converting the lookup table for *Player Second Choice*
    (table 6.3) to a conditional probability table that we can implement as a `TabularCPD`
    object
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.4 将玩家第二选择的查找表（表6.3）转换为我们可以实现为`TabularCPD`对象的条件概率表的结果
- en: '| Strategy | Stay | Switch |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 保持 | 切换 |'
- en: '| --- | --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Host Door Selection**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  |
    **2^(nd)**  | **3^(rd)**  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| **主持人门选择**  | **1^(st)**  | **2^(nd)**  | **3^(rd)**  | **1^(st)**  | **2^(nd)**  |
    **3^(rd)**  |'
- en: '| **Player First Choice**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  |
    2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第一选择**  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |
    1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  | 1^(st)  | 2^(nd)  |
    3^(rd)  | 1^(st)  | 2^(nd)  | 3^(rd)  |'
- en: '| **Player Second Choice**  | **1^(st)**  |  | 0  | 0  | 1  |  | 0  | 1  |
    0  |  |  | 0  | 0  | 0  |  | 1  | 0  | 1  |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **玩家第二次选择**  | **1^(st)**  |  | 0  | 0  | 1  |  | 0  | 1  | 0  |  |  | 0  |
    0  | 0  |  | 1  | 0  | 1  |  |'
- en: '| **2^(nd)**  |  | 1  | 0  | 0  |  | 0  | 0  | 1  |  |  | 0  | 1  | 0  |  |
    0  | 1  | 0  |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| **2^(nd)**  |  | 1  | 0  | 0  |  | 0  | 0  | 1  |  |  | 0  | 1  | 0  |  |
    0  | 1  | 0  |  |'
- en: '| **3^(rd)**  |  | 0  | 1  | 0  |  | 1  | 0  | 0  |  |  | 1  | 0  | 1  |  |
    0  | 0  | 0  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **3^(rd)**  |  | 0  | 1  | 0  |  | 1  | 0  | 0  |  |  | 1  | 0  | 1  |  |
    0  | 0  | 0  |  |'
- en: Unfortunately, we can’t leave the impossible values blank when we specify a
    `Tabular-CPD`, so in the following code, we’ll need to assign arbitrary values
    to these elements.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当我们指定`Tabular-CPD`时，我们不能将不可能的值留空，因此，在下面的代码中，我们需要为这些元素分配任意值。
- en: Listing 6.5 Implementation of *Player Second Choice* assignment function in
    pgmpy
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5 在pgmpy中实现*玩家第二次选择*分配函数
- en: '[PRE4]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 The probability values are 0 or 1, so the assignment function is deterministic.
    In cases where the parent combinations are impossible, we still have to assign
    a value.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 概率值是0或1，所以分配函数是确定性的。在父组合不可能的情况下，我们仍然需要分配一个值。'
- en: That gives us a second `TabularCPD` object. We’ll create one for each node.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们第二个`TabularCPD`对象。我们将为每个节点创建一个。
- en: First, let’s set up the causal DAG.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置因果DAG。
- en: Listing 6.6 Implementing the full Monty Hall SCM
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6 实现完整的蒙提霍尔SCM
- en: '[PRE5]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Build the causal DAG.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 构建因果DAG。'
- en: '`monty_hall_model` is now a causal DAG. It will become an SCM after we add
    the exogenous variable distributions and assignment functions.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`monty_hall_model`现在是一个因果DAG。在添加外生变量分布和分配函数后，它将成为SCM。'
- en: The following listing adds the exogenous variable distribution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表添加了外生变量分布。
- en: Listing 6.7 Create the exogenous variable distributions
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7 创建外生变量分布
- en: '[PRE6]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 A CPD for the Host Inclination variable. In cases when the player chooses
    the door with the car, the host has a choice between the two other doors. This
    variable is “left” when the host is inclined to choose the left-most door, and
    “right” if the host is inclined to choose the right-most door.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 主观倾向变量的CPD。当玩家选择有车的门时，主持人有两个其他门可以选择。当主持人倾向于选择最左边的门时，这个变量是“left”，如果主持人倾向于选择最右边的门，则变量是“right”。'
- en: '#2 A CPD for the variable representing which door has the prize car. Assume
    each door has an equal probability of having the car.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 表示哪个门有奖品车的变量的CPD。假设每个门有相等的机会有车。'
- en: '#3 A CPD for the variable representing the player’s first door choice. Each
    door has an equal probability of being chosen.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 表示玩家第一次选择门的变量的CPD。每个门被选择的概率相等。'
- en: '#4 A CPD for the variable representing the player''s strategy. “Stay” is the
    strategy of staying with the first choice, and “switch” is the strategy of switching
    doors.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为表示玩家策略的变量创建一个CPD。“停留”策略是指保持最初的选择，而“切换”策略是指改变门的选择。'
- en: Having created the exogenous distributions, we’ll now create the assignment
    functions. We’ve already created `f_host_door_selection` and `f_second_choice`,
    so we’ll add `f_win_or_lose`—the assignment function determining whether the player
    wins or loses.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了外生分布后，我们现在将创建分配函数。我们已经创建了`f_host_door_selection`和`f_second_choice`，所以我们将添加`f_win_or_lose`——确定玩家是赢还是输的分配函数。
- en: Listing 6.8 Create the assignment functions
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.8 创建分配函数
- en: '[PRE7]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, we’ll add the exogenous distribution and the assignment functions to
    `monty_hall_model` and create the SCM.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加外生分布和分配函数到`monty_hall_model`并创建SCM。
- en: Listing 6.9 Create the SCM for the Monty Hall problem
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.9 为蒙提霍尔问题创建SCM
- en: '[PRE8]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can run the variable elimination inference algorithm to verify the results
    of the algorithm. Let’s query the probability of winning, given that the player
    takes the “stay” strategy.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行变量消除推理算法来验证算法的结果。让我们查询在玩家采取“停留”策略的情况下获胜的概率。
- en: Listing 6.10 Inferring the winning strategy
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.10 推断获胜策略
- en: '[PRE9]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 We’ll use the inference algorithm called “variable elimination.”'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们将使用名为“变量消除”的推理算法。'
- en: '#2 Print the probabilities of winning and losing when the player uses the “stay”
    strategy.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 打印当玩家使用“停留”策略时赢得和输掉的概率。'
- en: '#3 Print the probabilities of winning and losing when the player uses the “switch”
    strategy.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印当玩家使用“切换”策略时赢得和输掉的概率。'
- en: '#4 Print the probabilities that the player used a stay strategy versus a switch
    strategy, given that the player won.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印当玩家获胜时，玩家使用“停留”策略与“切换”策略的概率。'
- en: 'This inference produces the following output:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个推理产生了以下输出：
- en: '[PRE10]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The probability of winning and losing under the “stay” strategy is 1/3 and
    2/3, respectively. In contrast, here’s the output for the “switch” strategy:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在“留在原门”策略下，获胜和失败的概率分别是1/3和2/3。相比之下，以下是“换门”策略的输出：
- en: '[PRE11]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The probability of winning and losing under the “switch” strategy is 2/3 and
    1/3, respectively. We can also condition on a winning outcome and infer the probability
    that each strategy leads to that outcome.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在“换门”策略下，获胜和失败的概率分别是2/3和1/3。我们还可以根据获胜结果进行条件化，并推断出每种策略导致该结果的概率。
- en: '[PRE12]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are plain vanilla non-causal probabilistic inferences—we were just validating
    that our SCM is capable of produce these inferences. In chapter 9, we’ll demonstrate
    how this SCM enables causal *counterfactual* inferences that simpler models can’t
    answer, such as “What would have happened had the losing player used a different
    strategy?”
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是普通的非因果概率推理——我们只是验证我们的SCM能够产生这些推理。在第9章中，我们将展示这个SCM如何使因果*反事实*推理成为可能，而简单的模型无法回答，例如“如果输掉游戏的玩家使用了不同的策略，会发生什么？”
- en: 6.3.4 Exogenous variables in the rule-based system
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 规则系统中的外生变量
- en: In this Monty Hall SCM, the root nodes (nodes with no incoming edges) in the
    causal DAG function as the exogenous variables. This is slightly different from
    our formal definition of an SCM, which states that exogenous variables represent
    causal factors outside the system. *Host Inclination* meets that definition, as
    this was not part of the original description. *Door with Car*, *Player First
    Choice*, and *Strategy* are another matter. To remedy this, we could introduce
    exogenous parents to these variables, and set these variables deterministically,
    given these parents, as we do elsewhere in this chapter. But while modeling this
    in pgmpy, that’s a bit redundant.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个蒙提霍尔SCM中，因果DAG中的根节点（没有入边节点的节点）充当外生变量。这与我们关于SCM的正式定义略有不同，该定义指出外生变量代表系统之外的因素。*主持人倾向*符合这个定义，因为这不是原始描述的一部分。*有车的门*、*玩家第一次选择*和*策略*则是另一回事。为了解决这个问题，我们可以向这些变量引入外生父节点，并给定这些父节点，将这些变量确定性地设置，就像我们在本章的其他地方所做的那样。但在使用pgmpy建模时，这有点多余。
- en: 6.3.5 Applications of SCM-modeling of rule-based systems
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 基于规则的系统SCM建模的应用
- en: While the Monty Hall game is simple, do not underestimate the expressive power
    of incorporating rules into assignment functions. Some of the biggest achievements
    in AI in previous decades have been at beating expert humans in board games with
    simple rules. Simulation software, often based on simple rules for how a system
    transitions from one state to another, can model highly complex behavior. Often,
    we want to apply causal analysis to rule-based systems engineered by humans (who
    know and can rewrite those rules), such as an automated manufacturing system.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然蒙提霍尔游戏很简单，但不要低估将规则纳入赋值函数的表达能力。在过去的几十年中，人工智能的一些最大成就就是通过简单的规则在棋类游戏中击败专家人类。基于如何从一个状态转换到另一个状态的简单规则，模拟软件可以模拟高度复杂的行为。我们通常希望对人类（他们知道并可以重写这些规则）设计的基于规则的系统进行因果分析，例如自动化制造系统。
- en: 6.4 Training an SCM on data
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 在数据上训练SCM
- en: Given a DAG, we make a choice of whether to use a CGM or an SCM. Let’s suppose
    we want to go with the SCM, and we want to “fit” or “train” this SCM on data.
    To do this, we choose some *parameterized* *function class* (e.g., linear functions,
    logistic functions, etc.) for each assignment function. That function class becomes
    a specific function once we’ve fit its parameters on data. Similarly, for each
    exogenous variable, we want to specify a canonical probability distribution, possibly
    with parameters we can fit on data.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个有向无环图（DAG），我们选择使用CGM还是SCM。假设我们想选择SCM，并希望“拟合”或“训练”这个SCM来处理数据。为此，我们为每个赋值函数选择一些*参数化*的*函数类*（例如，线性函数、逻辑函数等）。一旦我们在数据上拟合了其参数，这个函数类就变成了一个特定的函数。同样，对于每个外生变量，我们希望指定一个标准概率分布，可能包括我们可以拟合到数据中的参数。
- en: 'In our femur-height example, all the assignment functions were linear functions
    and the exogenous variables were normal distributions. But with tools like Pyro,
    you can specify each assignment function and exogenous distribution one by one.
    Then you can train the parameters just as you would with a CGM. For example, instead
    of taking this femur-height model from the forensic textbook:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关于股骨高度示例中，所有的赋值函数都是线性函数，外生变量是正态分布。但使用像 Pyro 这样的工具，你可以逐个指定每个赋值函数和外生分布。然后你可以像使用
    CGM 一样训练参数。例如，你不必从法医学教科书中获取这个股骨高度模型：
- en: '*n*[*y*] ~ *N*(0, 3.3)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, 3.3)'
- en: '*y* = 25 + 3*x* + *n*[*y*]'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = 25 + 3*x* + *n*[*y*]'
- en: 'you can just fit the parameters *α*, *β*, and *δ* of a linear model on actual
    forensic data:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接在真实的法医学数据上拟合线性模型的参数 *α*、*β* 和 *δ*：
- en: '*n*[*y*] ~ *N*(0, *δ*)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, *δ*)'
- en: '*y* = *α* + *β**x* + *n*[*y*]'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *α* + *β**x* + *n*[*y*]'
- en: In this forensics example, we use a linear assignment function because height
    is proportional to femur length. Let’s consider other ways to capture how causes
    influence their effects.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个法医学示例中，我们使用线性赋值函数，因为身高与股骨长度成正比。让我们考虑其他捕捉原因如何影响其效果的方法。
- en: 6.4.1 What assignment functions should I choose?
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 我应该选择什么样的赋值函数？
- en: The most important choice in an SCM model is your choice of *function classes*
    for the assignment functions, because these choices represent your assumptions
    about the “how” of causality. You can use function classes common in math, such
    as linear models. You can also use code (complete with if-then statements, loops,
    recursion, etc.) like we did with the rock-throwing example.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构因果模型（SCM）模型中，最重要的选择是赋值函数的 *函数类*，因为这些选择代表了你对因果“如何”的假设。你可以使用数学中常见的函数类，如线性模型。你还可以使用代码（包括
    if-then 语句、循环、递归等），就像我们在抛掷岩石的例子中所做的那样。
- en: Remember, you are modeling a ground-truth SCM. You are probably going to specify
    your assignment functions differently from those in the ground-truth SCM, but
    that’s fine. You don’t need your SCM to match the ground truth exactly; you just
    need your model to be right about the “how” assumptions it is relying on for your
    causal inferences.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你正在建模一个真实的结构因果模型（SCM）。你可能会指定与真实 SCM 中的不同的赋值函数，但这没关系。你不需要你的 SCM 与真实情况完全匹配；你只需要你的模型关于它所依赖的“如何”假设是正确的，以便进行因果推断。
- en: SCMs without “how” assumptions are just CGMs
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 没有关于“如何”假设的结构因果模型（SCM）只是因果图模型（CGM）
- en: Suppose you built an SCM where every assignment function is a linear function.
    You are using a linear Gaussian assumption because your library of choice requires
    it (e.g., `LinearGaussianCPD` is pretty much your only choice for modeling continuous
    variables in pgmpy). However, you are not planning on relying on that linear assumption
    for your causal inference. In this case, while your model checks the boxes of
    an SCM, it is effectively a CGM with linear models of the causal Markov kernels.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你构建了一个每个赋值函数都是线性函数的结构因果模型（SCM）。你使用线性高斯假设是因为你选择的库需要它（例如，在 pgmpy 中，`LinearGaussianCPD`
    几乎是你建模连续变量的唯一选择）。然而，你并不打算依赖这个线性假设来进行因果推断。在这种情况下，尽管你的模型符合 SCM 的要求，但实际上它是一个具有线性因果马尔可夫核模型的因果图模型（CGM）。
- en: Suppose, for example, that instead of a linear relationship between *X* and
    *Y*, *X* and *Y* followed a nonlinear S-curve, and your causal inference was sensitive
    to this S-curve. Imagine that the ground-truth SCM captured this with an assignment
    function in the form of the Hill equation (a function that arises in biochemistry
    and that can capture S-curves). But your SCM instead uses a logistic function
    fit on data. Your model, though wrong, will be sufficient to make a good causal
    inference if your logistic assignment function captured everything it needed to
    about the S-curve for your inference to work.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，*X* 和 *Y* 之间不是线性关系，而是遵循非线性 S 曲线，并且你的因果推断对这条 S 曲线很敏感。想象一下，真实的结构因果模型（SCM）通过一个形如希尔方程的赋值函数（在生物化学中出现的函数，可以捕捉
    S 曲线）来捕捉这一点。但你的 SCM 使用的是基于数据的逻辑函数拟合。尽管你的模型是错误的，但如果你的逻辑赋值函数捕捉到了关于 S 曲线所需的所有信息，以便你的推断能够工作，那么你的模型将足以做出良好的因果推断。
- en: 6.4.2 How should I model the exogenous variable distributions?
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 我应该如何建模外生变量分布？
- en: In section 6.1.3, we formulated our generative SCM in a particular way, where
    every node gets its own exogenous variable representing its unmodeled causes.
    Under that formulation, the role of the exogenous variable distribution is simply
    to provide sufficient variation for the SCM to model the joint distribution. This
    means that, assuming you have selected your assignment function classes, you can
    choose canonical distributions for the exogenous variables based on how well they
    would fit the data after parameter estimation. Some canonical distributions may
    fit better than others. You can contrast different choices using standard techniques
    for model comparison and cross-validation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在6.1.3节中，我们以特定方式表述了我们的生成式SCM，其中每个节点都得到一个代表其未建模原因的外部变量。根据这种表述，外部变量分布的作用仅仅是提供足够的变异，以便SCM可以模拟联合分布。这意味着，假设你已经选择了你的赋值函数类，你可以根据它们在参数估计后如何拟合数据来选择外部变量的规范分布。某些规范分布可能比其他分布拟合得更好。你可以使用标准的技术来对比不同的选择，这些技术用于模型比较和交叉验证。
- en: These canonical distributions can be parameterized, such as *N*(0, *δ*) in
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规范分布可以参数化，例如在
- en: '*n*[*y*] ~ *N*(0, *δ*)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, *δ*)'
- en: '*y* = *α* + *β**x* + *n*[*y*]'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *α* + *β**x* + *n*[*y*]'
- en: 'A more common approach in generative AI is to use constants in the canonical
    distribution and only train the parameters of the assignment function:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI中，更常见的方法是使用规范分布中的常数，并且只训练赋值函数的参数：
- en: '*n*[*y*] ~ *N*(0, 1)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, 1)'
- en: '*y* = *α* + *β**x* + *δ* *n*[*y*]'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *α* + *β**x* + *δ* *n*[*y*]'
- en: Either is fine, as long as your choice captures your “how” assumptions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 两种都行，只要你的选择捕捉到了你的“如何”假设。
- en: '6.4.3 Additive models: A popular choice for SCM modeling'
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 加性模型：SCM建模的流行选择
- en: 'Additive models are SCM templates that use popular trainable function classes
    for assignment functions. They can be a great place to start in SCM modeling.
    We’ll look at three common types of additive models: linear Gaussian additive
    model (LiGAM), linear non-Gaussian additive model (LiNGAM), and the nonlinear
    additive noise model (ANM). These models each encapsulate a pair of constraints:
    one on the structure of the assignment functions, and one on the distribution
    of the additive exogenous variables.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 加性模型是SCM模板，它们使用流行的可训练函数类作为赋值函数。它们在SCM建模中是一个很好的起点。我们将探讨三种常见的加性模型：线性高斯加性模型（LiGAM）、线性非高斯加性模型（LiNGAM）和非线性加性噪声模型（ANM）。这些模型每个都封装了一对约束：一个关于赋值函数的结构，另一个关于加性外部变量的分布。
- en: Additivity makes this approach easier because there are typically unique solutions
    to algorithms that learn the parameters of these additive models from data. In
    some cases, those parameters have a direct causal interpretation. There are also
    myriad software libraries for training additive models on data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 加法性使得这种方法更容易，因为通常这些加性模型从数据中学习参数的算法有唯一解。在某些情况下，这些参数有直接的因果解释。还有许多软件库用于在数据上训练加性模型。
- en: Let’s demonstrate the usefulness of additive models with an example. Suppose
    you were a biochemist studying the synthesis of a certain protein in a biological
    sample. The sample has some amount of an enzyme that reacts with some precursors
    in the sample and synthesizes the protein you are interested in. You measure the
    quantity of the protein you’re interested in. Let *X* be the amount of enzyme,
    and let *Y* be the measured amount of the protein of interest. We’ll model this
    system with an SCM, which has the DAG in figure 6.21.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来展示加性模型的有用性。假设你是一位生物化学家，正在研究生物样本中某种蛋白质的合成。样本中含有一部分酶，它与样本中的某些前体反应并合成你感兴趣的蛋白质。你测量了你感兴趣的蛋白质的量。让
    *X* 表示酶的量，让 *Y* 表示感兴趣蛋白质的测量量。我们将使用SCM来模拟这个系统，其DAG如图6.21所示。
- en: '![figure](../Images/CH06_F21_Ness.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F21_Ness.png)'
- en: Figure 6.21 The amount of enzyme (*X*) is a cause the measured quantity of protein
    (*Y*).
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.21 酶（*X*）的量是蛋白质（*Y*）测量量的原因。
- en: We have qualitative knowledge ofhow causes affect effects, but we have to turn
    that knowledge into explicit choices of function classes for assignment functions
    and exogenous variable distributions. Additive models are a good place to start.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对因果关系如何影响效果有定性知识，但我们必须将这种知识转化为对赋值函数和外部变量分布的函数类进行明确选择。加性模型是一个很好的起点。
- en: 'To illustrate, we’ll focus on the assignment function and exogenous variable
    distribution for *Y*, the amount of the target protein in our example. Generating
    from the exogenous variable, and setting *Y* via the assignment function, has
    the following notation:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将关注 *Y* 的分配函数和外生变量分布，*Y* 是我们示例中目标蛋白的量。从外生变量生成，并通过分配函数设置 *Y*，具有以下表示法：
- en: '*n*[*y*] ~ *P*(*N*[*y*])'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *P*(*N*[*y*])'
- en: '*y* := *f*[*y*](*x*, *n*[*y*])'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* := *f*[*y*](*x*, *n*[*y*])'
- en: '*f*[*y*](.) denotes the assignment function for *y*, which takes a value of
    the endogenous parent *X* and exogenous parent *N*[*y*] as inputs.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*[*y*](.) 表示 *y* 的分配函数，它以内源父代 *X* 和外生父代 *N*[*y*] 的值作为输入。'
- en: 'In an *additive* assignment function, the exogenous variable is always added
    to some function of endogenous parents. In our example, this means that the assignment
    function for *Y* has the following form:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 *additive* 分配函数中，外生变量始终添加到内源父代的某个函数中。在我们的例子中，这意味着 *Y* 的分配函数具有以下形式：
- en: '*y* := *f*[y](*x*, *n*[*y*]) = *g*(*x*) + *n*[*y*]'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* := *f*[y](*x*, *n*[*y*]) = *g*(*x*) + *n*[*y*]'
- en: Here, *g*(.) is some trainable function of the endogenous parent(s), and *n*[*y*]
    is added to the results of that function.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*g*(.) 是内源父代（s）的一些可训练函数，并且 *n*[*y*] 被添加到该函数的结果中。
- en: For our protein *Y*, these models say that the measured amount of protein *Y*
    is equal to some function of the enzyme amount *g*(*X*) plus some exogenous factors,
    such as noise in the measurement device. This assumption is attractive, because
    it lets us think of unmodeled exogenous causes as additive “noise.” In terms of
    statistical signal processing, it is relatively easy to disentangle some core
    signal (e.g., *g*(*x*)) from additive noise.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的蛋白 *Y*，这些模型表明测量的蛋白量 *Y* 等于酶量 *g*(*X*) 的某个函数加上一些外生因素，例如测量设备中的噪声。这种假设很有吸引力，因为它让我们将未建模的外生原因视为加性的“噪声”。从统计信号处理的角度来看，从加性噪声中分离出一些核心信号（例如，*g*(*x*））相对容易。
- en: In general, let *V* represent an endogenous variable in the model, *V*[*PA*]
    represent the endogenous parents of *V*, and *N*[*v*] represent an exogenous variable.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，让 *V* 代表模型中的内源变量，*V*[*PA*] 代表 *V* 的内源父代，*N*[*v*] 代表外生变量。
- en: '*v* := *f*[*v*](*V*[*PA*], *n*[*v*]) = *g*(*V*[*PA*]) + *n*[*v*]'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '*v* := *f*[*v*](*V*[*PA*], *n*[*v*]) = *g*(*V*[*PA*]) + *n*[*v*]'
- en: Additive SCMs have several benefits, but here we’ll focus on their benefit as
    a template for building SCMs. We’ll start with the simplest additive model, the
    linear Gaussian additive model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 加性结构因果模型（SCM）具有几个优点，但在这里我们将关注它们作为构建SCM模板的益处。我们将从最简单的加性模型，即线性高斯加性模型开始。
- en: 6.4.4 Linear Gaussian additive model
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 线性高斯加性模型
- en: In a linear Gaussian additive model, the assignment functions are linear functions
    of the parents, and the exogenous variables have a normal distribution.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个线性高斯加性模型中，分配函数是父代的线性函数，外生变量具有正态分布。
- en: 'In our enzyme example, *N*[*y*] and *Y* are given as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的酶示例中，*N*[*y*] 和 *Y* 给定如下：
- en: '*n*[*y*] ~ *N*(0, *σ*[*y*])'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[*y*] ~ *N*(0, *σ*[*y*])'
- en: '*y* := *β*[0] + *β*[*x*]*x* + *n*[*y*]'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* := *β*[0] + *β*[*x*]*x* + *n*[*y*]'
- en: Here, *β*[0] is an intercept term, and *β*[*x*] is a coefficient for *X*. We
    are assuming that for every unit increase in the amount of enzyme *X*, there is
    a *β*[*x*] increase in the expected amount of the measured protein. *N*[*y*] accounts
    for variation around that expected amount due to exogenous causal factors, and
    we assume it has a normal distribution with a mean of 0 and scale parameter *σ*[*y*].
    For example, we might assume that *N*[*y*] is composed mostly of technical noise
    from the measurement device, such as dust particles that interfere with the sensors.
    We might know from experience with this device that this noise has a normal distribution.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*β*[0] 是截距项，而 *β*[*x*] 是 *X* 的系数。我们假设对于酶量 *X* 的每单位增加，预期的测量蛋白量会增加 *β*[*x*]。*N*[*y*]
    考虑了由于外生因果因素导致的预期量周围的变异，并且我们假设它具有均值为0和尺度参数 *σ*[*y*] 的正态分布。例如，我们可能假设 *N*[*y*] 主要由测量设备的技术噪声组成，例如干扰传感器的灰尘颗粒。我们可能从使用该设备的经验中知道这种噪声具有正态分布。
- en: 'In general, for variable *V* with a set of *K* parents, *V*[*PA*] = {*V*[*pa*][,1],
    …, *V*[*pa*][,][*K*]}:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于具有 *K* 个父代的变量 *V*，*V*[*PA*] = {*V*[*pa*][,1], …, *V*[*pa*][,][*K*]}：
- en: '![figure](../Images/ness-ch6-eqs-13x.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-13x.png)'
- en: 'This model defines parameters: *β*[0] is an intercept term, *β*[*j*] is the
    coefficient attached to the *j*^(th) parent, and *σ*[*v*] is the scale parameter
    of *N*[*v*]’s normal distribution.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型定义了参数：*β*[0] 是截距项，*β*[*j*] 是附加到 *j*^(th) 父亲的系数，而 *σ*[*v*] 是 *N*[*v*] 的正态分布的尺度参数。
- en: Let’s see an example of a LiNGAM model in Pyro.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Pyro 中的 LiNGAM 模型示例。
- en: Listing 6.11 Pyro example of a linear Gaussian model
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.11 Pyro 的线性高斯模型示例
- en: '[PRE13]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 The distributions of the exogenous variables are normal (Gaussian).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 外生变量的分布是正态（高斯）的。'
- en: '#2 The functional assignments are linear.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 功能赋值是线性的。'
- en: Linear Gaussian SCMs are especially popular in econometric methods used in the
    social sciences because the model assumptions have many attractive statistical
    properties. Further, in linear models, we can interpret a parent causal regressor
    variable’s coefficient as the causal effect (average treatment effect) of that
    parent on the effect (response) variable.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 线性高斯 SCMs 在社会科学中使用的计量经济学方法中特别受欢迎，因为模型假设具有许多吸引人的统计特性。此外，在线性模型中，我们可以将父因果回归变量系数解释为该父变量对效应（响应）变量的因果效应（平均处理效应）。
- en: 6.4.5 Linear non-Gaussian additive models
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.5 线性非高斯加性模型
- en: Linear non-Gaussian additive models (LiNGAM) are useful when the Gaussian assumption
    on exogenous variables is not appropriate. In our example, the amount of protein
    *Y* cannot be negative, but that can easily occur in a linear model if *β*[0],
    *x*, or *n*[*x*] have low values. LiNGAM models remedy this by allowing the exogenous
    variable to have a non-normal distribution.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 线性非高斯加性模型（LiNGAM）在对于外生变量高斯假设不适用时很有用。在我们的例子中，蛋白质 *Y* 的数量不能为负，但在线性模型中，如果 *β*[0]、*x*
    或 *n*[*x*] 的值较低，这种情况很容易发生。LiNGAM 模型通过允许外生变量具有非正态分布来解决这个问题。
- en: Listing 6.12 Pyro example of a LiNGAM model
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.12 Pyro 的 LiNGAM 模型示例
- en: '[PRE14]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Instead of a normal (Gaussian) distribution, the exogenous variables have
    a gamma distribution with the same mean and variance.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与正态（高斯）分布不同，外生变量具有相同均值和方差的伽马分布。'
- en: '#2 These are the same assignment functions as in the linear Gaussian model.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这些是线性高斯模型中的相同赋值函数。'
- en: In the preceding model, we use a gamma distribution. The lowest possible value
    in a gamma distribution is 0, so *y* cannot be negative.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的模型中，我们使用伽马分布。伽马分布中的最低可能值是 0，因此 *y* 不能为负。
- en: 6.4.6 Nonlinear additive noise models
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.6 非线性加性噪声模型
- en: As I’ve mentioned, the power of the SCM is the ability to choose functional
    assignments that reflect *how* causes affect their direct effects. In our hypothetical
    example, you are a biochemist. Could you import knowledge from biochemistry to
    design the assignment function? Here is what that reasoning might look like. (You
    don’t need to understand the biology or the math, in this example, just the logic).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，SCM 的力量在于选择能够反映**如何**原因影响其直接效应的功能赋值。在我们的假设例子中，你是一位生物化学家。你能从生物化学中导入知识来设计赋值函数吗？以下就是这个推理可能的样子。（在这个例子中，你不需要理解生物学或数学，只需逻辑即可）。
- en: 'There is a common mathematical assumption in enzyme modeling called *mass action
    kinetics*. In this model, *T* is the maximum possible amount of the target protein.
    The biochemical reactions happen in real time, and during that time, the amount
    of the target protein fluctuates before stabilizing at some equilibrium value
    *Y*. Let *Y*(*t*) and *X*(*t*) be the amount of the target protein and enzyme
    at a given time point. Mass action kinetics give us the following ordinary differential
    equation:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在酶建模中有一个常见的数学假设，称为**质量作用动力学**。在这个模型中，*T* 是目标蛋白质的最大可能量。生化反应在实时发生，在这段时间内，目标蛋白质的数量在稳定在某个平衡值
    *Y* 之前会波动。设 *Y*(*t*) 和 *X*(*t*) 为给定时间点的目标蛋白质和酶的量。质量作用动力学给我们以下常微分方程：
- en: '![figure](../Images/ness-ch6-eqs-14x.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-14x.png)'
- en: Here, *v* and *α* are *rate* *parameters* that characterize the rates at which
    different biochemical reactions occur in time. This differential equation has
    the following equilibrium solution,
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*v* 和 *α* 是**速率**参数，它们表征了不同生化反应在时间上发生的速率。这个微分方程有以下的平衡解，
- en: '![figure](../Images/ness-ch6-eqs-15x.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-15x.png)'
- en: where *Y* and *X* are equilibrium values of *Y*(*t*) and *X*(*t*), and *β* =
    *v*/*α*.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Y* 和 *X* 是 *Y*(*t*) 和 *X*(*t*) 的平衡值，且 *β* = *v*/*α*。
- en: 'As an enzyme biologist, you know that this equation captures something of the
    actual mechanism underpinning the biochemistry of this system, like physics equations
    such as Ohm’s law and SIR models in epidemiology. You elect to use this as your
    assignment function for *Y*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名酶生物学家，你知道这个方程捕捉了该系统生物化学实际机制的某些方面，就像物理学方程如欧姆定律和流行病学中的SIR模型。你选择使用这个方程作为*Y*的分配函数：
- en: '![figure](../Images/ness-ch6-eqs-16x.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-16x.png)'
- en: 'This is a nonlinear additive noise model (ANM). In general, ANMs have the following
    structure:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非线性加性噪声模型（ANM）。一般来说，ANMs具有以下结构：
- en: '*V* = *g*(*V*[*pa*]) + *N*[*v*]'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '*V* = *g*(*V*[*pa*]) + *N*[*v*]'
- en: In our example *g*(*X*) = *T* × *β* *X* / (1 + *β* *X*). *N*[*y*] can be normal
    (Gaussian) or non-Gaussian.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中 *g*(*X*) = *T* × *β* *X* / (1 + *β* *X*)。*N*[*y*]可以是正态（高斯）或非高斯。
- en: Connecting dynamic modeling and simulation to SCMs
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将动态建模和仿真与SCMs（供应链管理）连接
- en: Dynamic models describe how a system’s behavior evolves in time. The use of
    dynamic modeling, as you saw in the enzyme modeling example, is one approach to
    addressing this knowledge elicitation problem for SCMs.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 动态模型描述了系统行为随时间演变的方式。正如你在酶建模示例中看到的，动态建模的使用是解决SCMs知识获取问题的方法之一。
- en: In this section, I illustrated how an enzyme biologist could use a domain-specific
    dynamic model, specifically an ODE, to construct an SCM. An ODE is just one type
    of dynamic model. Another example is computer simulator models, such as the simulators
    used in climate modeling, power-grid modeling, and manufacturing. Simulators can
    also model complex social processes, such as financial markets and epidemics.
    Simulator software is a growing multibillion dollar market.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我说明了酶生物学家如何使用特定领域的动态模型，特别是常微分方程（ODE），来构建SCM。常微分方程只是动态模型的一种类型。另一个例子是计算机仿真模型，如用于气候建模、电网建模和制造业的仿真器。仿真器还可以模拟复杂的社会过程，如金融市场和流行病。仿真软件是一个不断增长的数十亿美元的市场。
- en: In simulators and other dynamic models, specifying the “how” of causality can
    be easier than in SCMs. SCMs require assignment functions to explicitly capture
    the global behavior of the system. Dynamic models only require you to specify
    the rules for how things change from instant to instant. You can then see global
    behavior by running the simulation. The trade-off is that dynamic models can be
    computationally expensive to run, and it is generally difficult to train parameters
    of dynamic models on data or perform inferences given data as evidence. This has
    motivated interesting research in combining the knowledge elicitation convenience
    of dynamic models with the statistical and computational conveniences of SCMs.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在仿真器和其他动态模型中，指定因果关系的“如何”可能比在SCMs中更容易。SCMs需要分配函数来明确捕捉系统的全局行为。动态模型只需要你指定事物从瞬间到瞬间如何变化的规则。然后，通过运行仿真，你可以看到全局行为。权衡的是，动态模型可能运行起来计算成本高昂，并且通常难以在数据上训练动态模型的参数或根据数据作为证据进行推理。这促使了将动态模型的启发式便利性与SCMs的统计和计算便利性相结合的有趣研究。
- en: Next, we’ll examine using regression tools to train these additive models.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用回归工具来训练这些加性模型。
- en: 6.4.7 Training additive model SCMs with regression tools
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.7 使用回归工具训练加性模型SCMs
- en: In statistics, regression modeling finds parameter values that minimize the
    difference between a parameterized function of a set of predictors and a response
    variable. Regression modeling libraries are ubiquitous, and one advantage of additive
    SCM models is that they can use those libraries to fit an SCM’s parameters on
    data. For example, parameters of additive models can be fit with standard linear
    and nonlinear regression parameter fitting techniques (e.g., generalized least
    squares). We can also leverage these tools’ regression goodness-of-fit statistics
    to evaluate how well the model explains the data.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，回归建模找到参数值，以最小化一组预测变量参数化函数与响应变量之间的差异。回归建模库无处不在，加性SCM模型的一个优点是它们可以使用这些库来拟合数据上的SCM参数。例如，加性模型的参数可以使用标准的线性和非线性回归参数拟合技术（例如，广义最小二乘法）进行拟合。我们还可以利用这些工具的回归拟合优度统计来评估模型解释数据的好坏。
- en: Note that the predictors in a general regression model can be anything you like.
    Most regression modeling pedagogy encourages you to keep adding predictors that
    increase goodness-of-fit (e.g., adjusted R-squared) or reduce predictive error.
    But in an SCM, your predictors are limited to direct endogenous causes.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在一般回归模型中的预测者可以是任何你喜欢的。大多数回归建模教学法鼓励你继续添加预测者，以增加拟合优度（例如，调整R平方）或减少预测误差。但在SCM中，你的预测者仅限于直接的内因原因。
- en: Can I use generalized linear models as SCMs?
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我可以使用广义线性模型作为SCM吗？
- en: In statistical modeling, a generalized linear model (GLM) is a flexible generalization
    of linear regression. In a GLM, the response variable is related to a linear function
    of the predictors with a *link function*. Further, variance of the response variable
    can be a function of the predictors. Examples include logistic regression, Poisson
    regression, and gamma regression. GLMs are a fundamental statistical toolset for
    data scientists.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计建模中，广义线性模型（GLM）是线性回归的灵活推广。在GLM中，响应变量与预测者的线性函数相关联，并有一个 *链接函数*。此外，响应变量的方差可以是预测者的函数。例如，包括逻辑回归、泊松回归和伽马回归。GLMs是数据科学家的基本统计工具集。
- en: In a CGM (non-SCM), GLMs are good choices as models of causal Markov kernels.
    But a common question is whether GLMs can be used as assignment functions in an
    SCM.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在CGM（非SCM）中，GLM（广义线性模型）是因果马尔可夫核的不错选择。但一个常见的问题是GLM是否可以用作SCM中的分配函数。
- en: Several GLMs align with the structure of additive SCMs, but it’s generally best
    not to think of GLMs as templates for SCMs. The functional form of assignment
    functions in an SCM is meant to reflect the nature of the causal relationship
    between a variable and its causal parents. The functional form of a GLM applies
    a (in some cases nonlinear) link function to a linear function of the predictors.
    The link function is designed to map that linear function of the predictors to
    the mean of a canonical distribution (e.g., normal, Poisson, gamma). It is not
    designed to reflect causal assumptions.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 几个GLM与加性SCM的结构相匹配，但通常最好不将GLM视为SCM的模板。SCM中分配函数的功能形式旨在反映变量与其因果父变量之间的因果关系本质。GLM的功能形式将一个（在某些情况下非线性的）链接函数应用于预测者的线性函数。链接函数的设计是为了将预测者的线性函数映射到标准分布（例如，正态分布、泊松分布、伽马分布）的均值。它并不是为了反映因果假设。
- en: 6.4.8 Beyond the additive model
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.8 超越加性模型
- en: If the “how” of an assignment function requires more nuance than you can capture
    with an additive model, don’t constrain yourself to an additive model. Using biochemistry
    as an example, it is not hard to come up with scenarios where interactions between
    endogenous and exogenous causes would motivate a multiplicative model.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分配函数的“如何”需要比加性模型能捕捉到的更细微的差别，不要将自己限制在加性模型上。以生物化学为例，不难想出内因和外因之间的相互作用会促使采用乘性模型的场景。
- en: For these more complex scenarios, it starts making sense to move toward using
    probabilistic deep learning tools to implement an SCM.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些更复杂的场景，开始使用概率深度学习工具来实现SCM（结构化因果模型）是有意义的。
- en: 6.5 Combining SCMs with deep learning
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 将SCM与深度学习结合
- en: Let’s revisit the enzyme kinetic model, where the amount of an enzyme *X* is
    a cause of the amount of a target protein *Y*, as in figure 6.22.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下酶动力学模型，其中酶 *X* 的量是目标蛋白 *Y* 量的原因，如图6.22所示。
- en: '![figure](../Images/CH06_F22_Ness.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F22_Ness.png)'
- en: Figure 6.22 The amount of enzyme (*X*) is a cause of the measured quantity of
    protein (*Y*).
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.22 酶 (*X*) 的量是蛋白质 (*Y*) 测量量的原因。
- en: I said previously that, based on a dynamic mathematical model popular in the
    study of enzyme biology, a good candidate for an additive assignment function
    for *Y* is
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到，基于在酶生物学研究中流行的动态数学模型，*Y* 的加性分配函数的一个良好候选者是
- en: '![figure](../Images/ness-ch6-eqs-16x.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch6-eqs-16x.png)'
- en: Further, suppose that we knew from experiments that *T* was 100 and *β* was
    .08.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设我们从实验中得知 *T* 为100，*β* 为.08。
- en: Ideally, we would want to be able to reproduce these parameter values from data.
    Better yet, we should like to leverage the automatic differentiation-based frameworks
    that power modern deep learning.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望能够从数据中重现这些参数值。更好的是，我们希望利用现代深度学习背后的基于自动微分框架。
- en: 6.5.1 Implementing and training an SCM with basic PyTorch
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 使用基本PyTorch实现和训练SCM
- en: First, let’s create a PyTorch version of the enzyme model.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个PyTorch版本的酶模型。
- en: Listing 6.13 Implement the PyTorch enzyme model
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.13实现PyTorch酶模型
- en: '[PRE15]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Create the enzyme model.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建酶模型。'
- en: '#2 Initialize the parameter *D�*.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始化参数 *D�*。'
- en: '#3 Calculate the product of enzyme amount X and *D�*.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算酶量X和 *D�* 的乘积。'
- en: '#4 Implement the function u / (u + 1) as sigmoid(log(u)), since the sigmoid
    and log functions are native PyTorch transforms.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 实现函数 u / (u + 1) 作为 sigmoid(log(u))，因为sigmoid和log函数是PyTorch的本地转换。'
- en: '#5 Multiply by T = 100.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 乘以T = 100。'
- en: Suppose we observed the data from this system, visualized in figure 6.23.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们观察到了来自该系统的数据，如图6.23所示。
- en: '![figure](../Images/CH06_F23_Ness.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F23_Ness.png)'
- en: Figure 6.23 Exampled enzyme data. *X* is the amount of enzyme, and *Y* is the
    amount of target protein.
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.23示例酶数据。*X* 是酶的量，*Y* 是目标蛋白的量。
- en: Let’s try to learn *β* from this data using a basic PyTorch workflow.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用基本的PyTorch工作流程从这个数据中学习 *β*。
- en: Listing 6.14 Fitting enzyme data with PyTorch
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.14使用PyTorch拟合酶数据
- en: '[PRE16]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Load the enzyme data from GitHub. *#2 Convert the data to tensors.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从GitHub加载酶数据。*#2 将数据转换为张量。'
- en: '#3 Create the training algorithm.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建训练算法。'
- en: '#4 Print out losses during training.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 打印训练过程中的损失。'
- en: '#5 Set a random seed for reproducibility.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 设置随机种子以实现可重复性。'
- en: '#6 Initialize an instance of the Adam optimizer. Use a low value for the learning
    rate because loss is very sensitive to small changes in *D�*.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 初始化Adam优化器的一个实例。由于损失对 *D�* 的小变化非常敏感，因此使用低学习率。'
- en: '#7 Using mean squared loss error is equivalent to assuming Ny is additive and
    symmetric.*  *When I run this code with the given random seed, it produces a value
    of 0.1079 (you can access the value by printing `enzyme_model.`*β*`.data`), which
    only differs slightly from the ground-truth value of .08\. This implementation
    did not represent the exogenous variable *N*[y] explicitly, but statistics theory
    tells us that using the mean squared error loss function is equivalent to assuming
    *N*[y] was additive and had a normal distribution. However, it also assumes that
    the normal distribution had constant variance, while the funnel shape in the scatterplot
    indicates the variance of *N*[y] might increase with the value of *X*.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 使用均方损失误差相当于假设Ny是可加的和对称的。*当我用给定的随机种子运行此代码时，它产生了一个0.1079的值（你可以通过打印 `enzyme_model.`*β*`.data`
    来访问这个值），这个值仅略低于真实值0.08。此实现没有明确表示外生变量 *N*[y]，但统计学理论告诉我们，使用均方误差损失函数相当于假设 *N*[y]
    是可加的并且具有正态分布。然而，它还假设正态分布具有恒定的方差，而散点图中的漏斗形状表明 *N*[y] 的方差可能随着 *X* 的值而增加。'
- en: 6.5.2 Training an SCM with probabilistic PyTorch
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 使用概率PyTorch训练SCM
- en: The problem with this basic parameter optimization approach is that the SCM
    should encode a distribution *P*(*X*, *Y*). So we can turn to a probabilistic
    modeling approach to fit this model.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本参数优化方法的问题在于SCM应该编码一个分布 *P*(*X*, *Y*)。因此，我们可以转向概率建模方法来拟合此模型。
- en: Listing 6.15 Bayesian estimation *β* in a probabilistic enzyme model
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.15在概率酶模型中进行贝叶斯估计 *β*
- en: '[PRE17]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 The simple transform used in the assignment function for Y (amount of target
    protein)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于Y（目标蛋白的量）的分配函数中使用的简单转换'
- en: '#2 The probabilistic model'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 概率模型'
- en: '#3 A prior on the parameter *β* that we mean to fit with this model'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们打算用此模型拟合的参数 *β* 的先验。'
- en: '#4 A "plate" for the N=100 identical and independently distributed values of
    X and Y'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 为X和Y的N=100个相同且独立分布的值提供一个“板”。'
- en: '#5 The marginal probability of the enzyme P(X) is a uniform distribution between
    0 and 101.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 酶的边缘概率 P(X) 是0到101之间的均匀分布。'
- en: '#6 P(Y|X) is the conditional distribution of Y (protein concentration) given
    X (and *β*). I model P(Y|X) with a normal distribution with both a mean and variance
    that depends on Y.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 P(Y|X) 是给定X（和 *β*）的Y（蛋白质浓度）的条件分布。我用一个具有均值和方差的正态分布来模拟P(Y|X)，这两个值都取决于Y。'
- en: '#7 Condition the model on the observed evidence.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 在观察到的证据上对模型进行条件化。'
- en: '#8 Get the number of examples in the data (100).'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 获取数据中的示例数量（100）。'
- en: '#9 Set a random seed for reproducibility.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 设置随机种子以实现可重复性。'
- en: '#10 To learn *β*, I use a gradient-based MCMC algorithm called a No-U-Turn
    Sampler (NUTS). This algorithm is one of many probabilistic approaches for parameter
    learning, and this choice is independent of the causal elements of your model.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 为了学习 *β*，我使用了一个基于梯度的MCMC算法，称为No-U-Turn Sampler（NUTS）。这是许多概率参数学习方法之一，这个选择与你的模型因果元素无关。'
- en: 'The problem with this approach is that it doesn’t have an explicit representation
    of the exogenous variables. If we want to use a probabilistic machine learning
    framework to build an SCM, we need to make exogenous variables explicit. That
    is challenging with the preceding approach for one very nuanced reason: When I
    write the following statement in Pyro code, `y` `=` `pyro.sample("Y",` `Normal(…,
    …))`, Pyro knows to use that normal distribution to calculate the probability
    value (in more precise terms, the *likelihood*) of each value of *Y* in the training
    data. Those values are used in probabilistic inference algorithms like MCMC. But
    if I write a statement that represents an assignment function, like `y` `=` `f(x,`
    `ny)`, Pyro doesn’t automatically know how to calculate probability values for
    *Y*, especially since as far as Pyro is concerned, *f*(.) can be anything.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是它没有显式地表示外生变量。如果我们想使用概率机器学习框架来构建SCM，我们需要使外生变量显式化。这在前面的方法中具有挑战性，原因之一非常微妙：当我用Pyro代码写下以下语句时，`y`
    `=` `pyro.sample("Y",` `Normal(…, …))`，Pyro知道使用那个正态分布来计算训练数据中每个 *Y* 值的概率值（更精确地说，是
    *似然值*）。这些值用于概率推理算法，如MCMC。但如果我们写一个表示分配函数的语句，如 `y` `=` `f(x,` `ny)`，Pyro不会自动知道如何计算
    *Y* 的概率值，特别是由于Pyro认为 *f*(.)可以是任何东西。
- en: But there is another problem that is more important than this issue with inference.
    So far, we’ve been assuming that we conveniently know a domain-based mathematical
    functional form for *Y*’s assignment function. It would be nice to use deep learning
    to fit the assignment functions, but this is problematic.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一个比这个问题更重要的问题。到目前为止，我们一直假设我们方便地知道一个基于域的数学函数形式，用于 *Y* 的分配函数。使用深度学习来拟合分配函数会很理想，但这存在问题。
- en: 6.5.3 Neural SCMs and normalizing flows
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 神经网络SCM和正态化流
- en: Suppose we used a neural network to model `y` `=` `f(x,` `ny)`. Indeed for a
    given SCM, we could use a multilayer neural network to model each variable, given
    its parents—call this a “neural SCM.” The problem is that we want the trainable
    function class we use for our assignment functions to represent our assumptions
    about the “how” of causality. Neural networks, as universal function approximators,
    are, by definition, as assumption-free as curve-fitting functions get. Therefore,
    to use a neural SCM, we need ways to constrain the neural assignment function
    to remain faithful to our “how” assumptions. This could be done with constraints
    on the training feature, loss function, and elements of the neural network architecture.
    Normalizing flows are an example of the latter.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用神经网络来对 `y` `=` `f(x,` `ny)` 进行建模。实际上，对于给定的SCM，我们可以使用多层神经网络来对每个变量进行建模，前提是已知其父变量——我们可以称之为“神经网络SCM”。问题是，我们希望用于我们的分配函数的可训练函数类能够代表我们对因果“如何”的假设。神经网络作为通用的函数逼近器，按照定义，与曲线拟合函数一样，几乎没有假设。因此，为了使用神经网络SCM，我们需要方法来约束神经网络分配函数，使其忠实于我们的“如何”假设。这可以通过对训练特征、损失函数和神经网络架构元素施加约束来实现。正态化流是后者的一个例子。
- en: 'Returning to the enzyme modeling example, let’s start by enumerating some basic
    biological assumptions about the relationships between enzymes and the proteins
    they help synthesize:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 回到酶建模的例子，让我们首先列举一些关于酶和它们帮助合成的蛋白质之间关系的基本生物学假设：
- en: The process by which the protein leaves the system is independent of the amount
    of enzyme. So we expect the amount of target protein to *monotonically increase*,
    given the amount of enzyme.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蛋白质离开系统的过程与酶的量无关。因此，我们预计在酶的量一定的情况下，目标蛋白的量会 *单调递增*。
- en: However, systems tend to saturate, such that there are diminishing returns in
    adding more enzyme.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，系统往往会饱和，这意味着添加更多酶的回报递减。
- en: We need a neural network approach that *only* allows for monotonic functions
    with diminishing returns. For this, we’ll use a deep generative modeling approach
    called *normalizing flows*.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个只允许单调函数且有递减回报的神经网络方法。为此，我们将使用一种称为 *正态化流* 的深度生成建模方法。
- en: Normalizing flows model a complex probability density as an invertible transformation
    of a simple base density. I’m going to use flows to model the distribution of
    endogenous variables as invertible transformations of exogenous variable distributions.
    There are many different transformations, but I’m going to use *neural splines.[¹](#footnote-305)*
    Splines are a decades-old approach to curve-fitting using piece-wise polynomials;
    a neural spline is the neural network version of a spline.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 正态化流将复杂概率密度建模为简单基密度可逆变换。我将使用流来将内生变量的分布建模为外生变量分布的可逆变换。有许多不同的变换，但我将使用 *神经样条。[¹](#footnote-305)*
    样条是使用分段多项式进行曲线拟合的几十年老方法；神经样条是样条的网络版本。
- en: Listing 6.16 Initializing splines for assignment functions
  id: totrans-437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.16 为分配函数初始化样条
- en: '[PRE18]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 A neural spline transform is a type of invertible PyTorch neural network
    module.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 神经样条变换是一种可逆的 PyTorch 神经网络模块。'
- en: 'We get a three-layer neural network with ReLU activation functions:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个具有 ReLU 激活函数的三层神经网络：
- en: '[PRE19]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Normalizing flows solve our problem of not having a likelihood value for `y`
    `=` `f(x,` `ny)`. Like other probabilistic machine learning models, they allow
    us to connect an input random variable (like an exogenous variable) to an output
    variable (like an endogenous variable) using layers of transformations. The key
    difference is that normalizing flow models automatically calculate the probability
    values of instances of the output variable in the data (using the *change-of-variable
    formula* from probability theory). That automatic calculation relies on monotonicity;
    our causal “how” assumption is that the relationship between enzyme concentration
    and protein abundance is monotonic, and normalizing flows give us monotonicity.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 正态化流解决了我们没有 `y` `=` `f(x,` `ny)` 的似然值的问题。与其他概率机器学习模型一样，它们允许我们使用变换层将输入随机变量（如外生变量）连接到输出变量（如内生变量）。关键区别在于，正态化流模型自动计算数据中输出变量的实例的概率值（使用概率论中的
    *变量变换公式*）。这种自动计算依赖于单调性；我们的因果“如何”假设是酶浓度与蛋白质丰度之间的关系是单调的，而正态化流为我们提供了单调性。
- en: For example, in the following code, `NxDist` is the distribution of exogenous
    variable *N*[*x*]. We set the distribution as a Uniform(0, 1). `f_x` is the assignment
    function for *X*, implemented as an `AffineTransformation` that maps this distribution
    to Uniform(1, 101).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下代码中，`NxDist` 是外生变量 *N*[*x*] 的分布。我们将分布设置为均匀分布（0, 1）。`f_x` 是 *X* 的分配函数，实现为一个将此分布映射到均匀分布（1,
    101）的 `AffineTransformation`。
- en: Listing 6.17 Transforming a distribution of *N*x to a distribution of *X*
  id: totrans-444
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.17 将 *N*x 的分布转换为 *X* 的分布
- en: '[PRE20]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 The exogenous distribution of X is Uniform(0, 1).'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 X 的外生分布是均匀分布（0, 1）。'
- en: '#2 The assignment function for f_x. The AffineTransform multiplies Nx by 100
    and adds 1.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 f_x 的分配函数。AffineTransform 将 Nx 乘以 100 并加 1。'
- en: '#3 XDist is an explicit representation of P(X). Multiplying by 100 and adding
    1 gives you a Uniform(1, 101).'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 XDist 是 P(X) 的显式表示。乘以 100 并加 1 得到均匀分布（1, 101）。'
- en: So `XDist` allows us to calculate the probability value of *X* even when its
    value is set deterministically by an assignment function. You can calculate the
    log-probability value of 50 with `XDist.log_prob(torch.tensor([50.0]))`, which
    under the Uniform(1, 101) distribution will be log(1/100).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`XDist` 允许我们在其值由分配函数确定的情况下计算 *X* 的概率值。您可以使用 `XDist.log_prob(torch.tensor([50.0]))`
    计算值为 50 的对数概率值，在均匀分布（1, 101）下将是 log(1/100)。
- en: Let’s first specify the model.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们明确模型。
- en: Listing 6.18 Specify the flow-based SCM
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.18 指定基于流的 SCM
- en: '[PRE21]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 The exogenous distribution of X is Uniform(0, 1).'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 X 的外生分布是均匀分布（0, 1）。'
- en: '#2 The assignment function for f_x. The AffineTransform multiplies Nx by 100
    and adds 1.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 f_x 的分配函数。AffineTransform 将 Nx 乘以 100 并加 1。'
- en: '#3 XDist is an explicit representation of P(X). Multiplying by 100 and adding
    1 gives you a Uniform(1, 101).'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 XDist 是 P(X) 的显式表示。乘以 100 并加 1 得到均匀分布（1, 101）。'
- en: '#4 The exogenous distribution of Y is Normal(0, 1).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 Y 的外生分布是正态分布（0, 1）。'
- en: '#5 We implement the assignment function for f_y with a neural spline. Optimization
    will optimize the parameters of this spline.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们使用神经样条实现 f_y 的分配函数。优化将优化此样条的参数。'
- en: '#6 YDist is an explicit representation of P(Y|X).'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 YDist 是 P(Y|X) 的显式表示。'
- en: Now we run the training.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始训练。
- en: Listing 6.19 Train the SCM
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.19 训练 SCM
- en: '[PRE22]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Register the neural spline functional assignment function for Y.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为 Y 注册神经样条功能分配函数。'
- en: '#2 Initialize the optimizer.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始化优化器。'
- en: '#3 Normalize Y, since the assignment function is working with neural networks.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 对Y进行归一化，因为分配函数正在与神经网络一起工作。'
- en: '#4 Set all gradients to 0.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将所有梯度设置为0。'
- en: '#5 Use P(X) to calculate a log likelihood value for each value of X.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用P(X)计算每个X值的对数似然值。'
- en: '#6 Use P(Y|X) to calculate a log likelihood value for each value of Y, given
    X.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 使用P(Y|X)计算给定X的每个Y值的对数似然值。'
- en: '#7 Fit the parameters of the neural network modules using maximum likelihood
    as an objective.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 使用最大似然作为目标，拟合神经网络模块的参数。'
- en: '#8 Visualize losses during training.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 在训练期间可视化损失。'
- en: Figure 6.24 shows the training loss over training.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24显示了训练过程中的损失。
- en: '![figure](../Images/CH06_F24_Ness.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F24_Ness.png)'
- en: Figure 6.24 Training loss of the flow-based SCM-training procedure
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.24基于流的SCM训练过程的训练损失
- en: Now we can generate samples from the model and compare them to the training
    data.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从模型中生成样本，并将它们与训练数据进行比较。
- en: Listing 6.20 Generate from the trained model
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.20从训练模型生成
- en: '[PRE23]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Generate synthetic examples from the trained model.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从训练模型生成合成示例。'
- en: '#2 Visualize the synthetic examples over the examples in the training data
    to validate model fit.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将合成的示例可视化在训练数据中的示例上，以验证模型拟合。'
- en: Figure 6.25 overlays generated samples with the actual examples in the training
    data.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25将生成的样本叠加在训练数据中的实际示例上。
- en: '![figure](../Images/CH06_F25_Ness.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F25_Ness.png)'
- en: Figure 6.25 Generated examples from the trained model overlaid upon actual examples
    in the training data
  id: totrans-480
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.25从训练模型生成的示例叠加在训练数据中的实际示例上
- en: The ability to have multilayered flows as in other neural network frameworks
    makes this an extremely flexible modeling class. But this is not a mere curve-fitting
    exercise. With the variational autoencoder example in chapter 5, you saw that
    you can use neural networks to map causal parents to their child effects in the
    general class of CGMs. But that is not sufficient for SCMs, even if you set endogenous
    variables deterministically. Again, SCMs reflect causal assumptions about the
    “how” of causality in the form of assignment functions. In this enzyme example,
    we are asserting that the monotonic relationship between the enzyme and protein
    abundance is important in the causal inferences we want to make, and so we’re
    constraining the neural nets (and other transforms) in my assignment functions
    to those that preserve monotonicity.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 能够像其他神经网络框架一样具有多层流动，这使得它成为一个极其灵活的建模类别。但这不仅仅是一个曲线拟合练习。在第五章的变分自动编码器示例中，您看到可以使用神经网络将因果父节点映射到CGM泛类中的子效果。但对于SCM来说，即使您确定性设置内生变量，这也不够。再次强调，SCM以分配函数的形式反映了关于因果“如何”的因果假设。在这个酶示例中，我们断言酶和蛋白质丰度之间的单调关系对我们想要做的因果推断很重要，因此我们在分配函数中限制了神经网络（和其他转换）以保持单调性。
- en: Summary
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Structural causal models (SCMs) are a type of causal graphical model (CGM) that
    encode causal assumptions beyond the assumptions encoded in the causal DAG. The
    causal DAG assumptions capture *what* causes *what*. The SCM additionally captures
    *how* the causes affect the effects.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构因果模型（SCMs）是一种因果图模型（CGM），它编码的因果假设超越了因果有向无环图（DAG）中编码的假设。因果DAG假设捕捉了*什么*导致*什么*。SCM还捕捉了*如何*原因影响效果。
- en: SCMs are composed of exogenous variables, probability distributions on those
    exogenous variables, endogenous variables, and functional assignments.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCM由外生变量、这些外生变量的概率分布、内生变量和功能分配组成。
- en: Exogenous variables represent unmodeled causes.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外生变量代表未建模的原因。
- en: Endogenous variables are the variables explicitly included in the model, corresponding
    to the nodes we’ve seen in previous causal DAGs.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内生变量是显式包含在模型中的变量，对应于我们在之前的因果DAG中看到的节点。
- en: The functional assignments set each endogenous variable deterministically, given
    its causal parents.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能分配在给定其因果父变量的情况下，确定性地为每个内生变量设置。
- en: The SCM’s additional assumptions represent the “how” of causality in the form
    of functional assignments.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCM的附加假设以功能分配的形式代表了因果的“如何”。
- en: SCMs represent a deterministic view of causality, where an outcome is known
    for certain if all the causes are known.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCM代表因果的确定性观点，如果所有原因都已知，则结果可以确定。
- en: You can derive an SCM from a more general (non-SCM) CGM. But given a general
    CGM, there are potentially multiple SCMs that entail the same DAG and joint probability
    distribution as that CGM.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从一个更一般的（非SCM）CGM中推导出SCM。但给定一个一般的CGM，可能存在多个SCM，它们包含与该CGM相同的DAG和联合概率分布。
- en: You can’t learn the functional assignments of an SCM from statistical information
    in the data alone.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能仅从数据中的统计信息中学习SCM的功能分配。
- en: SCMs are an ideal choice for representing well-defined systems with simple,
    deterministic rules, such as games.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCM是表示具有简单、确定性规则（如游戏）的明确系统的理想选择。
- en: Additive noise models provide a useful template for building SCMs from scratch.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加性噪声模型为从头开始构建SCMs提供了一个有用的模板。
- en: Normalizing flows are a useful probabilistic machine learning framework for
    modeling SCMs when your causal “how” assumption is monotonicity.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态化流是当你的因果“如何”假设是单调性时，用于建模SCMs的有用概率机器学习框架。
- en: '[[1]](#footnote-source-1) For more information on neural splines, see C. Durkan,
    A. Bekasov, I. Murray, and G. Papamakarios, “Neural spline flows,” in *Advances
    in neural information processing systems*, *32 (NeurIPS 2019)*.*'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) 关于神经网络样条的更多信息，请参阅C. Durkan, A. Bekasov, I. Murray,
    和 G. Papamakarios在*Advances in neural information processing systems*, *32 (NeurIPS
    2019)*中的论文“Neural spline flows”。'
