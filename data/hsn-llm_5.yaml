- en: Chapter 6\. Tokens & Token Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬å…­ç« \. ä»¤ç‰Œä¸ä»¤ç‰ŒåµŒå…¥
- en: Embeddings are a central concept to using large language models (LLMs), as youâ€™ve
    seen over and over in part one of the book. They also are central to understanding
    how LLMs work, how theyâ€™re built, and where theyâ€™ll go in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ ¸å¿ƒæ¦‚å¿µï¼Œæ­£å¦‚ä½ åœ¨æœ¬ä¹¦ç¬¬ä¸€éƒ¨åˆ†ä¸­åå¤çœ‹åˆ°çš„é‚£æ ·ã€‚å®ƒä»¬å¯¹ç†è§£LLMsçš„å·¥ä½œåŸç†ã€æ„å»ºæ–¹å¼ä»¥åŠæœªæ¥å‘å±•æ–¹å‘è‡³å…³é‡è¦ã€‚
- en: The majority of the embeddings weâ€™ve looked at so far are *text embeddings*,
    vectors that represent an entire sentence, passage, or document. [FigureÂ 6-1](#fig_1__the_difference_between_text_embeddings_one_vect)
    shows this distinction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€æŸ¥çœ‹çš„å¤§å¤šæ•°åµŒå…¥éƒ½æ˜¯*æ–‡æœ¬åµŒå…¥*ï¼Œè¿™äº›å‘é‡è¡¨ç¤ºæ•´ä¸ªå¥å­ã€æ®µè½æˆ–æ–‡æ¡£ã€‚[å›¾6-1](#fig_1__the_difference_between_text_embeddings_one_vect)å±•ç¤ºäº†è¿™ä¸€åŒºåˆ«ã€‚
- en: '![  The difference between text embeddings  one vector for a sentence or paragraph  and
    token embeddings  one vector per word or token .](assets/tokens_token_embeddings_963889_01.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![æ–‡æœ¬åµŒå…¥ï¼ˆä¸€ä¸ªå‘é‡è¡¨ç¤ºä¸€ä¸ªå¥å­æˆ–æ®µè½ï¼‰å’Œä»¤ç‰ŒåµŒå…¥ï¼ˆæ¯ä¸ªå•è¯æˆ–ä»¤ç‰Œä¸€ä¸ªå‘é‡ï¼‰ä¹‹é—´çš„åŒºåˆ«ã€‚](assets/tokens_token_embeddings_963889_01.png)'
- en: Figure 6-1\. The difference between text embeddings (one vector for a sentence
    or paragraph) and token embeddings (one vector per word or token).
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-1\. æ–‡æœ¬åµŒå…¥ï¼ˆä¸€ä¸ªå‘é‡è¡¨ç¤ºä¸€ä¸ªå¥å­æˆ–æ®µè½ï¼‰å’Œä»¤ç‰ŒåµŒå…¥ï¼ˆæ¯ä¸ªå•è¯æˆ–ä»¤ç‰Œä¸€ä¸ªå‘é‡ï¼‰ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: In this chapter, we begin to discuss token embeddings in more detail. Chapter
    2 discussed tasks of token classification like Named Entity Recognition. In this
    chapter, we look more closely at what tokens are and the tokenization methods
    used to power LLMs. We will then go beyond the world of text and see how these
    concepts of token embeddings empower LLMs that can understand images and data
    modes (other than text, for example video, audio...etc). LLMs that can process
    modes of data in addition to text are called *multi-modal* models. We will then
    delve into the famous word2vec embedding method that preceded modern-day LLMs
    and see how itâ€™s extending the concept of token embeddings to build commercial
    recommendation systems that power a lot of the apps you use.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹æ›´è¯¦ç»†åœ°è®¨è®ºä»¤ç‰ŒåµŒå…¥ã€‚ç¬¬äºŒç« è®¨è®ºäº†å¦‚å‘½åå®ä½“è¯†åˆ«çš„ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ›´ä»”ç»†åœ°ç ”ç©¶ä»€ä¹ˆæ˜¯ä»¤ç‰Œä»¥åŠç”¨äºæ”¯æŒLLMsçš„ä»¤ç‰ŒåŒ–æ–¹æ³•ã€‚éšåæˆ‘ä»¬å°†è¶…è¶Šæ–‡æœ¬çš„ä¸–ç•Œï¼Œçœ‹çœ‹è¿™äº›ä»¤ç‰ŒåµŒå…¥çš„æ¦‚å¿µå¦‚ä½•ä½¿LLMsèƒ½å¤Ÿç†è§£å›¾åƒå’Œå…¶ä»–æ•°æ®æ¨¡å¼ï¼ˆä¾‹å¦‚è§†é¢‘ã€éŸ³é¢‘ç­‰ï¼‰ã€‚èƒ½å¤Ÿå¤„ç†é™¤æ–‡æœ¬ä¹‹å¤–çš„æ•°æ®æ¨¡å¼çš„LLMsè¢«ç§°ä¸º*å¤šæ¨¡æ€*æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è‘—åçš„word2vecåµŒå…¥æ–¹æ³•ï¼Œå®ƒæ˜¯ç°ä»£LLMsçš„å‰èº«ï¼Œå¹¶çœ‹çœ‹å®ƒå¦‚ä½•æ‰©å±•ä»¤ç‰ŒåµŒå…¥çš„æ¦‚å¿µï¼Œä»¥æ„å»ºæ¨åŠ¨è®¸å¤šåº”ç”¨çš„å•†ä¸šæ¨èç³»ç»Ÿã€‚
- en: LLM Tokenization
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMä»¤ç‰ŒåŒ–
- en: How tokenizers prepare the inputs to the language model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»¤ç‰ŒåŒ–å™¨å¦‚ä½•å‡†å¤‡è¯­è¨€æ¨¡å‹çš„è¾“å…¥
- en: Viewed from the outside, generative LLMs take an input prompt and generate a
    response, as we can see in [FigureÂ 6-2](#fig_2__high_level_view_of_a_language_model_and_its_inpu).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¤–éƒ¨çœ‹ï¼Œç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å—è¾“å…¥æç¤ºå¹¶ç”Ÿæˆå“åº”ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨[å›¾6-2](#fig_2__high_level_view_of_a_language_model_and_its_inpu)ä¸­æ‰€è§ã€‚
- en: '![  High level view of a language model and its input prompt.](assets/tokens_token_embeddings_963889_02.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![è¯­è¨€æ¨¡å‹åŠå…¶è¾“å…¥æç¤ºçš„é«˜çº§è§†å›¾ã€‚](assets/tokens_token_embeddings_963889_02.png)'
- en: Figure 6-2\. High-level view of a language model and its input prompt.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-2\. è¯­è¨€æ¨¡å‹åŠå…¶è¾“å…¥æç¤ºçš„é«˜çº§è§†å›¾ã€‚
- en: 'As weâ€™ve seen in Chapter 5, instruction-tuned LLMs produce better responses
    to prompts formulated as instructions or questions. At the most basic level of
    the code, letâ€™s assume we have a generate method that hits a language model and
    generates text:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äº”ç« ä¸­çœ‹åˆ°çš„ï¼Œç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„LLMså¯¹ä½œä¸ºæŒ‡ä»¤æˆ–é—®é¢˜è¡¨è¿°çš„æç¤ºäº§ç”Ÿæ›´å¥½çš„å“åº”ã€‚åœ¨ä»£ç çš„æœ€åŸºæœ¬å±‚é¢ä¸Šï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè°ƒç”¨è¯­è¨€æ¨¡å‹å¹¶ç”Ÿæˆæ–‡æœ¬çš„generateæ–¹æ³•ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Generation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆï¼š
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us look closer into that generation process to examine more of the steps
    involved in text generation. Letâ€™s start by loading our model and its tokenizer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°å®¡è§†è¿™ä¸€ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æ£€æŸ¥æ–‡æœ¬ç”Ÿæˆä¸­æ¶‰åŠçš„æ›´å¤šæ­¥éª¤ã€‚è®©æˆ‘ä»¬ä»åŠ è½½æ¨¡å‹åŠå…¶ä»¤ç‰ŒåŒ–å™¨å¼€å§‹ã€‚
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then proceed to the actual generation. Notice that the generation code
    always includes a tokenization step prior to the generation step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è¿›å…¥å®é™…ç”Ÿæˆã€‚è¯·æ³¨æ„ï¼Œç”Ÿæˆä»£ç æ€»æ˜¯åŒ…æ‹¬ä¸€ä¸ªä»¤ç‰ŒåŒ–æ­¥éª¤ï¼Œä½äºç”Ÿæˆæ­¥éª¤ä¹‹å‰ã€‚
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Looking at this code, we can see that the model does not in fact receive the
    text prompt. Instead, the tokenizers processed the input prompt, and returned
    the information the model needed in the variable input_ids, which the model used
    as its input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™æ®µä»£ç æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹å®é™…ä¸Šå¹¶æ²¡æœ‰æ¥æ”¶åˆ°æ–‡æœ¬æç¤ºã€‚ç›¸åï¼Œä»¤ç‰ŒåŒ–å™¨å¤„ç†äº†è¾“å…¥æç¤ºï¼Œå¹¶è¿”å›æ¨¡å‹æ‰€éœ€çš„ä¿¡æ¯ï¼Œå­˜å‚¨åœ¨å˜é‡input_idsä¸­ï¼Œæ¨¡å‹å°†å…¶ç”¨ä½œè¾“å…¥ã€‚
- en: 'Letâ€™s print input_ids to see what it holds inside:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°input_idsä»¥æŸ¥çœ‹å…¶å†…éƒ¨å†…å®¹ï¼š
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This reveals the inputs that LLMs respond to. A series of integers as shown
    in [FigureÂ 6-3](#fig_3__a_tokenizer_processes_the_input_prompt_and_prepa). Each
    one is the unique ID for a specific token (character, word or part of word). These
    IDs reference a table inside the tokenizer containing all the tokens it knows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­ç¤ºäº† LLMs å“åº”çš„è¾“å…¥ã€‚æ­£å¦‚[å›¾ 6-3](#fig_3__a_tokenizer_processes_the_input_prompt_and_prepa)ä¸­æ‰€ç¤ºï¼Œä¸€ç³»åˆ—æ•´æ•°ã€‚æ¯ä¸ªéƒ½æ˜¯ç‰¹å®šä»¤ç‰Œï¼ˆå­—ç¬¦ã€å•è¯æˆ–å•è¯çš„ä¸€éƒ¨åˆ†ï¼‰çš„å”¯ä¸€
    IDã€‚è¿™äº› ID å‚è€ƒåˆ†è¯å™¨å†…éƒ¨çš„ä¸€ä¸ªè¡¨ï¼ŒåŒ…å«å®ƒæ‰€çŸ¥é“çš„æ‰€æœ‰ä»¤ç‰Œã€‚
- en: '![  A tokenizer processes the input prompt and prepares the actual input into
    the language model  a list of token ids.](assets/tokens_token_embeddings_963889_03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ†è¯å™¨å¤„ç†è¾“å…¥æç¤ºå¹¶å‡†å¤‡å®é™…è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„ä»¤ç‰Œ ID åˆ—è¡¨ã€‚](assets/tokens_token_embeddings_963889_03.png)'
- en: 'Figure 6-3\. A tokenizer processes the input prompt and prepares the actual
    input into the language model: a list of token ids.'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 6-3\. åˆ†è¯å™¨å¤„ç†è¾“å…¥æç¤ºå¹¶å‡†å¤‡å®é™…è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼šä¸€ä¸ªä»¤ç‰Œ ID åˆ—è¡¨ã€‚
- en: 'If we want to inspect those IDs, we can use the tokenizerâ€™s decode method to
    translate the IDs back into text that we can read:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³æ£€æŸ¥è¿™äº› IDï¼Œå¯ä»¥ä½¿ç”¨åˆ†è¯å™¨çš„è§£ç æ–¹æ³•å°† ID è½¬æ¢å›å¯è¯»æ–‡æœ¬ï¼š
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Which prints:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ‰“å°ï¼š
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is how the tokenizer broke down our input prompt. Notice the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯åˆ†è¯å™¨å¦‚ä½•æ‹†è§£æˆ‘ä»¬çš„è¾“å…¥æç¤ºã€‚æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: 'The first token is the token with ID #1, which is <s>, a special token indicating
    the beginning of the text'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€ä¸ªä»¤ç‰Œæ˜¯ ID ä¸º #1 çš„ä»¤ç‰Œï¼Œå®ƒæ˜¯ <s>ï¼Œä¸€ä¸ªç‰¹æ®Šä»¤ç‰Œï¼Œè¡¨ç¤ºæ–‡æœ¬çš„å¼€å§‹ã€‚'
- en: Some tokens are complete words (e.g., *Write*, *an*, *email*)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€äº›ä»¤ç‰Œæ˜¯å®Œæ•´çš„å•è¯ï¼ˆä¾‹å¦‚ï¼Œ*å†™*ï¼Œ*ä¸€ä¸ª*ï¼Œ*ç”µå­é‚®ä»¶*ï¼‰ã€‚
- en: Some tokens are parts of words (e.g., *apolog*, *izing*, *trag*, *ic*)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€äº›ä»¤ç‰Œæ˜¯å•è¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼Œ*é“æ­‰*ï¼Œ*izing*ï¼Œ*æ‚²æƒ¨*ï¼Œ*ic*ï¼‰ã€‚
- en: Punctuation characters are their own token
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡ç‚¹ç¬¦å·å­—ç¬¦æ˜¯å®ƒä»¬è‡ªå·±çš„ä»¤ç‰Œã€‚
- en: Notice how the space character does not have its own token. Instead, partial
    tokens (like â€˜izingâ€™ and â€˜ic') have a special hidden character at their beginning
    that indicate that theyâ€™re connected with the token that precedes them in the
    text.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ç©ºæ ¼å­—ç¬¦æ²¡æœ‰è‡ªå·±çš„ä»¤ç‰Œã€‚ç›¸åï¼Œéƒ¨åˆ†ä»¤ç‰Œï¼ˆå¦‚ â€˜izingâ€™ å’Œ â€˜icâ€™ï¼‰åœ¨å¼€å¤´æœ‰ä¸€ä¸ªç‰¹æ®Šçš„éšè—å­—ç¬¦ï¼ŒæŒ‡ç¤ºå®ƒä»¬ä¸æ–‡æœ¬ä¸­å‰é¢çš„ä»¤ç‰Œç›¸è¿ã€‚
- en: There are three major factors that dictate how a tokenizer breaks down an input
    prompt. First, at model design time, the creator of the model chooses a tokenization
    method. Popular methods include Byte-Pair Encoding (BPE for short, widely used
    by GPT models), WordPiece (used by BERT), and SentencePiece (used by LLAMA). These
    methods are similar in that they aim to optimize an efficient set of tokens to
    represent a text dataset, but they arrive at it in different ways.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸‰ä¸ªä¸»è¦å› ç´ å†³å®šäº†åˆ†è¯å™¨å¦‚ä½•æ‹†è§£è¾“å…¥æç¤ºã€‚é¦–å…ˆï¼Œåœ¨æ¨¡å‹è®¾è®¡æ—¶ï¼Œæ¨¡å‹çš„åˆ›å»ºè€…é€‰æ‹©äº†ä¸€ç§åˆ†è¯æ–¹æ³•ã€‚å¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼Œå¹¿æ³›åº”ç”¨äº GPT æ¨¡å‹ï¼‰ã€WordPieceï¼ˆç”¨äº
    BERTï¼‰å’Œ SentencePieceï¼ˆç”¨äº LLAMAï¼‰ã€‚è¿™äº›æ–¹æ³•çš„å…±åŒç‚¹åœ¨äºï¼Œå®ƒä»¬æ—¨åœ¨ä¼˜åŒ–ä¸€ç»„æœ‰æ•ˆçš„ä»¤ç‰Œæ¥è¡¨ç¤ºæ–‡æœ¬æ•°æ®é›†ï¼Œä½†å®ƒä»¬çš„å®ç°æ–¹å¼å„ä¸ç›¸åŒã€‚
- en: Second, after choosing the method, we need to make a number of tokenizer design
    choices like vocabulary size, and what special tokens to use. More on this in
    the â€œComparing Trained LLM Tokenizersâ€ section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œé€‰æ‹©æ–¹æ³•åï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€ç³»åˆ—åˆ†è¯å™¨è®¾è®¡é€‰æ‹©ï¼Œå¦‚è¯æ±‡å¤§å°å’Œä½¿ç”¨å“ªäº›ç‰¹æ®Šä»¤ç‰Œã€‚å…³äºè¿™ä¸€ç‚¹å°†åœ¨â€œæ¯”è¾ƒè®­ç»ƒåçš„ LLM åˆ†è¯å™¨â€éƒ¨åˆ†ä¸­è¯¦ç»†ä»‹ç»ã€‚
- en: Thirdly, the tokenizer needs to be trained on a specific dataset to establish
    the best vocabulary it can use to represent that dataset. Even if we set the same
    methods and parameters, a tokenizer trained on an English text dataset will be
    different from another trained on a code dataset or a multilingual text dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ï¼Œåˆ†è¯å™¨éœ€è¦åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å»ºç«‹æœ€ä½³è¯æ±‡è¡¨æ¥è¡¨ç¤ºè¯¥æ•°æ®é›†ã€‚å³ä½¿æˆ‘ä»¬è®¾ç½®ç›¸åŒçš„æ–¹æ³•å’Œå‚æ•°ï¼Œåœ¨è‹±è¯­æ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†è¯å™¨ä¸åœ¨ä»£ç æ•°æ®é›†æˆ–å¤šè¯­è¨€æ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†è¯å™¨ä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚
- en: In addition to being used to process the input text into a language model, tokenizers
    are used on the output of the language model to turn the resulting token ID into
    the output word or token associated with it as [FigureÂ 6-4](#fig_4__tokenizers_are_also_used_to_process_the_output_o)
    shows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç”¨äºå°†è¾“å…¥æ–‡æœ¬å¤„ç†æˆè¯­è¨€æ¨¡å‹å¤–ï¼Œåˆ†è¯å™¨è¿˜ç”¨äºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œå°†ç”Ÿæˆçš„ä»¤ç‰Œ ID è½¬æ¢ä¸ºä¸ä¹‹å…³è”çš„è¾“å‡ºå•è¯æˆ–ä»¤ç‰Œï¼Œå¦‚[å›¾ 6-4](#fig_4__tokenizers_are_also_used_to_process_the_output_o)æ‰€ç¤ºã€‚
- en: '![  Tokenizers are also used to process the output of the model by converting
    the output token ID into the word or token associated with that ID.](assets/tokens_token_embeddings_963889_04.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ†è¯å™¨ä¹Ÿç”¨äºå¤„ç†æ¨¡å‹çš„è¾“å‡ºï¼Œé€šè¿‡å°†è¾“å‡ºçš„ä»¤ç‰Œ ID è½¬æ¢ä¸ºä¸è¯¥ ID å…³è”çš„å•è¯æˆ–ä»¤ç‰Œã€‚](assets/tokens_token_embeddings_963889_04.png)'
- en: Figure 6-4\. Tokenizers are also used to process the output of the model by
    converting the output token ID into the word or token associated with that ID.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 6-4\. åˆ†è¯å™¨ä¹Ÿç”¨äºå¤„ç†æ¨¡å‹çš„è¾“å‡ºï¼Œé€šè¿‡å°†è¾“å‡ºçš„ä»¤ç‰Œ ID è½¬æ¢ä¸ºä¸è¯¥ ID å…³è”çš„å•è¯æˆ–ä»¤ç‰Œã€‚
- en: Word vs. Subword vs. Character vs. Byte Tokens
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å•è¯ä¸å­è¯ã€å­—ç¬¦å’Œå­—èŠ‚ä»¤ç‰Œ
- en: 'The tokenization scheme weâ€™ve seen above is called subword tokenization. Itâ€™s
    the most commonly used tokenization scheme but not the only one. The four notable
    ways to tokenize are shown in [FigureÂ 6-5](#fig_5__there_are_multiple_methods_of_tokenization_that).
    Letâ€™s go over them:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°çš„åˆ†è¯æ–¹æ¡ˆç§°ä¸ºå­è¯åˆ†è¯ã€‚å®ƒæ˜¯æœ€å¸¸ç”¨çš„åˆ†è¯æ–¹æ¡ˆï¼Œä½†ä¸æ˜¯å”¯ä¸€çš„ã€‚å››ç§æ˜¾è‘—çš„åˆ†è¯æ–¹å¼å¦‚[å›¾6-5](#fig_5__there_are_multiple_methods_of_tokenization_that)æ‰€ç¤ºã€‚è®©æˆ‘ä»¬é€ä¸€äº†è§£å®ƒä»¬ï¼š
- en: Word tokens
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯ä»¤ç‰Œ
- en: This approach was common with earlier methods like Word2Vec but is being used
    less and less in NLP. Its usefulness, however, led it to be used outside of NLP
    for use cases such as recommendation systems, as weâ€™ll see later in the chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•åœ¨æ—©æœŸçš„Word2Vecç­‰æ–¹æ³•ä¸­å¾ˆå¸¸è§ï¼Œä½†åœ¨NLPä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šå°‘ã€‚ç„¶è€Œï¼Œå®ƒçš„æœ‰æ•ˆæ€§ä½¿å…¶è¢«ç”¨äºNLPä»¥å¤–çš„ç”¨ä¾‹ï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢çœ‹åˆ°çš„ã€‚
- en: '![  There are multiple methods of tokenization that break down the text to
    different sizes of components  words  subwords  characters  and bytes .](assets/tokens_token_embeddings_963889_05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![æœ‰å¤šç§åˆ†è¯æ–¹æ³•å°†æ–‡æœ¬æ‹†åˆ†ä¸ºä¸åŒå¤§å°çš„ç»„ä»¶ï¼šå•è¯ã€å­è¯ã€å­—ç¬¦å’Œå­—èŠ‚ã€‚](assets/tokens_token_embeddings_963889_05.png)'
- en: Figure 6-5\. There are multiple methods of tokenization that break down the
    text to different sizes of components (words, subwords, characters, and bytes).
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-5. æœ‰å¤šç§åˆ†è¯æ–¹æ³•å°†æ–‡æœ¬æ‹†åˆ†ä¸ºä¸åŒå¤§å°çš„ç»„ä»¶ï¼ˆå•è¯ã€å­è¯ã€å­—ç¬¦å’Œå­—èŠ‚ï¼‰ã€‚
- en: One challenge with word tokenization is that the tokenizer becomes unable to
    deal with new words that enter the dataset after the tokenizer was trained. It
    also results in a vocabulary that has a lot of tokens with minimal differences
    between them (e.g., apology, apologize, apologetic, apologist). This latter challenge
    is resolved by subword tokenization as weâ€™ve seen as it has a token for '*apolog**',*
    and then suffix tokens (e.g., *'-y*', '*-**ize*', '*-etic*', '-*ist*') that are
    common with many other tokens, resulting in a more expressive vocabulary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯åˆ†è¯çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œåˆ†è¯å™¨æ— æ³•å¤„ç†è®­ç»ƒåè¿›å…¥æ•°æ®é›†çš„æ–°å•è¯ã€‚è¿™ä¹Ÿå¯¼è‡´äº†ä¸€ä¸ªè¯æ±‡ä¸­å­˜åœ¨è®¸å¤šå·®å¼‚å¾®å°çš„ä»¤ç‰Œï¼ˆä¾‹å¦‚ï¼Œapology, apologize,
    apologetic, apologistï¼‰ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œè¿™ä¸€åç»­æŒ‘æˆ˜é€šè¿‡å­è¯åˆ†è¯å¾—åˆ°è§£å†³ï¼Œå› ä¸ºå®ƒæœ‰ä¸€ä¸ªä»¤ç‰Œä¸º'*apolog**'*ï¼Œç„¶åæ˜¯è®¸å¤šå…¶ä»–ä»¤ç‰Œé€šç”¨çš„åç¼€ä»¤ç‰Œï¼ˆä¾‹å¦‚ï¼Œ*'-y*',
    '*-**ize*', '*-etic*', '-*ist*'ï¼‰ï¼Œä»è€Œå½¢æˆæ›´å…·è¡¨ç°åŠ›çš„è¯æ±‡ã€‚
- en: Subword Tokens
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å­è¯ä»¤ç‰Œ
- en: This method contains full and partial words. In addition to the vocabulary expressivity
    mentioned earlier, another benefit of the approach is its ability to represent
    new words by breaking the new token down into smaller characters, which tend to
    be a part of the vocabulary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•åŒ…å«å®Œæ•´å’Œéƒ¨åˆ†å•è¯ã€‚é™¤äº†ä¹‹å‰æåˆ°çš„è¯æ±‡è¡¨ç°åŠ›å¤–ï¼Œè¯¥æ–¹æ³•çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯èƒ½å¤Ÿé€šè¿‡å°†æ–°ä»¤ç‰Œåˆ†è§£ä¸ºè¾ƒå°çš„å­—ç¬¦æ¥è¡¨ç¤ºæ–°å•è¯ï¼Œè¿™äº›å­—ç¬¦å¾€å¾€æ˜¯è¯æ±‡çš„ä¸€éƒ¨åˆ†ã€‚
- en: When compared to character tokens, this method benefits from the ability to
    fit more text within the limited context length of a Transformer model. So with
    a model with a context length of 1024, you may be able to fit three times as much
    text using subword tokenization than using character tokens (sub word tokens often
    average three characters per token).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å­—ç¬¦ä»¤ç‰Œç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„å¥½å¤„åœ¨äºèƒ½å¤Ÿåœ¨Transformeræ¨¡å‹çš„æœ‰é™ä¸Šä¸‹æ–‡é•¿åº¦å†…å®¹çº³æ›´å¤šæ–‡æœ¬ã€‚å› æ­¤ï¼Œåœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡é•¿åº¦ä¸º1024çš„æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å­è¯åˆ†è¯èƒ½å¤Ÿæ¯”ä½¿ç”¨å­—ç¬¦ä»¤ç‰Œå®¹çº³ä¸‰å€çš„æ–‡æœ¬ï¼ˆå­è¯ä»¤ç‰Œå¹³å‡æ¯ä¸ªä»¤ç‰Œæœ‰ä¸‰ä¸ªå­—ç¬¦ï¼‰ã€‚
- en: Character Tokens
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å­—ç¬¦ä»¤ç‰Œ
- en: This is another method that is able to deal successfully with new words because
    it has the raw letters to fall-back on. While that makes the representation easier
    to tokenize, it makes the modeling more difficult. Where a model with subword
    tokenization can represent â€œplayâ€ as one token, a model using character-level
    tokens needs to model the information to spell out â€œp-l-a-yâ€ in addition to modeling
    the rest of the sequence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§èƒ½å¤ŸæˆåŠŸå¤„ç†æ–°å•è¯çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒæœ‰åŸå§‹å­—æ¯ä½œä¸ºå¤‡ç”¨ã€‚è™½ç„¶è¿™ä½¿å¾—è¡¨ç¤ºæ›´å®¹æ˜“åˆ†è¯ï¼Œä½†ä¹Ÿä½¿å»ºæ¨¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä½¿ç”¨å­è¯åˆ†è¯çš„æ¨¡å‹å¯ä»¥å°†â€œplayâ€è¡¨ç¤ºä¸ºä¸€ä¸ªä»¤ç‰Œï¼Œè€Œä½¿ç”¨å­—ç¬¦çº§ä»¤ç‰Œçš„æ¨¡å‹éœ€è¦å»ºæ¨¡ä¿¡æ¯ä»¥æ‹¼å†™å‡ºâ€œp-l-a-yâ€ï¼Œå¹¶å»ºæ¨¡å…¶ä½™åºåˆ—ã€‚
- en: Byte Tokens
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å­—èŠ‚ä»¤ç‰Œ
- en: 'One additional tokenization method breaks down tokens into the individual bytes
    that are used to represent unicode characters. Papers like [CANINE: Pre-training
    an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)
    outline methods like this which are also called â€œtokenization free encodingâ€.
    Other works like [ByT5: Towards a token-free future with pre-trained byte-to-byte
    models](https://arxiv.org/abs/2105.13626) show that this can be a competitive
    method.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦ä¸€ç§åˆ†è¯æ–¹æ³•å°†åˆ†è¯åˆ†è§£ä¸ºç”¨äºè¡¨ç¤º Unicode å­—ç¬¦çš„å•ä¸ªå­—èŠ‚ã€‚åƒ [CANINE: Pre-training an Efficient Tokenization-Free
    Encoder for Language Representation](https://arxiv.org/abs/2103.06874) è¿™æ ·çš„è®ºæ–‡æ¦‚è¿°äº†è¿™ç§æ–¹æ³•ï¼Œä¹Ÿè¢«ç§°ä¸ºâ€œæ— åˆ†è¯ç¼–ç â€ã€‚å…¶ä»–ä½œå“å¦‚
    [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
    æ˜¾ç¤ºè¿™å¯ä»¥æ˜¯ä¸€ç§å…·æœ‰ç«äº‰åŠ›çš„æ–¹æ³•ã€‚'
- en: 'One distinction to highlight here: some subword tokenizers also include bytes
    as tokens in their vocabulary to be the final building block to fall back to when
    they encounter characters they canâ€™t otherwise represent. The GPT2 and RoBERTa
    tokenizers do this, for example. This doesnâ€™t make them tokenization-free byte-level
    tokenizers, because they donâ€™t use these bytes to represent everything, only a
    subset as weâ€™ll see in the next section.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¦å¼ºè°ƒçš„ä¸€ä¸ªåŒºåˆ«æ˜¯ï¼šä¸€äº›å­è¯åˆ†è¯å™¨è¿˜å°†å­—èŠ‚ä½œä¸ºè¯æ±‡ä¸­çš„åˆ†è¯ï¼Œä»¥ä¾¿åœ¨é‡åˆ°æ— æ³•ä»¥å…¶ä»–æ–¹å¼è¡¨ç¤ºçš„å­—ç¬¦æ—¶å›é€€åˆ°æœ€ç»ˆæ„å»ºå—ã€‚æ¯”å¦‚ï¼ŒGPT2 å’Œ RoBERTa
    åˆ†è¯å™¨å°±æ˜¯è¿™æ ·åšçš„ã€‚è¿™å¹¶ä¸æ„å‘³ç€å®ƒä»¬æ˜¯æ— åˆ†è¯çš„å­—èŠ‚çº§åˆ†è¯å™¨ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸ä½¿ç”¨è¿™äº›å­—èŠ‚æ¥è¡¨ç¤ºæ‰€æœ‰å†…å®¹ï¼Œè€Œåªä½¿ç”¨å­é›†ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Tokenizers are discussed in more detail in [Suhasâ€™ book]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨åœ¨ [Suhas çš„ä¹¦ä¸­] æœ‰æ›´è¯¦ç»†çš„è®¨è®º
- en: Comparing Trained LLM Tokenizers
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒç»è¿‡è®­ç»ƒçš„ LLM åˆ†è¯å™¨
- en: 'Weâ€™ve pointed out earlier three major factors that dictate the tokens that
    appear within a tokenizer: the tokenization method, the parameters and special
    tokens we use to initialize the tokenizer, and the dataset the tokenizer is trained
    on. Letâ€™s compare and contrast a number of actual, trained tokenizers to see how
    these choices change their behavior.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ä¸‰ä¸ªä¸»è¦å› ç´ å†³å®šäº†åˆ†è¯å™¨ä¸­å‡ºç°çš„åˆ†è¯ï¼šåˆ†è¯æ–¹æ³•ã€åˆå§‹åŒ–åˆ†è¯å™¨æ‰€ç”¨çš„å‚æ•°å’Œç‰¹æ®Šåˆ†è¯ï¼Œä»¥åŠåˆ†è¯å™¨è®­ç»ƒæ‰€ç”¨çš„æ•°æ®é›†ã€‚è®©æˆ‘ä»¬æ¯”è¾ƒå’Œå¯¹æ¯”å¤šä¸ªå®é™…çš„ã€ç»è¿‡è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä»¥è§‚å¯Ÿè¿™äº›é€‰æ‹©å¦‚ä½•æ”¹å˜å®ƒä»¬çš„è¡Œä¸ºã€‚
- en: 'Weâ€™ll use a number of tokenizers to encode the following text:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨å¤šç§åˆ†è¯å™¨å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼š
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will allow us to see how each tokenizer deals with a number of different
    kinds of tokens:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ°æ¯ä¸ªåˆ†è¯å™¨å¦‚ä½•å¤„ç†å¤šç§ä¸åŒç±»å‹çš„åˆ†è¯ï¼š
- en: Capitalization
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§å†™åŒ–
- en: Languages other than English
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™¤è‹±è¯­å¤–çš„å…¶ä»–è¯­è¨€
- en: Emojis
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·
- en: Programming code with its keywords and whitespaces often used for indentation
    (in languages like python for example)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼–ç¨‹ä»£ç åŠå…¶å…³é”®å­—å’Œç©ºç™½é€šå¸¸ç”¨äºç¼©è¿›ï¼ˆä¾‹å¦‚åœ¨ Python è¯­è¨€ä¸­ï¼‰
- en: Numbers and digits
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°å­—å’Œæ•°å­—ç¬¦å·
- en: Letâ€™s go from older to newer tokenizers and see how they tokenize this text
    and what that might say about the language model. Weâ€™ll tokenize the text, and
    then print each token with a gray background color.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è¾ƒæ—§çš„åˆ°è¾ƒæ–°çš„åˆ†è¯å™¨ï¼Œçœ‹çœ‹å®ƒä»¬å¦‚ä½•å¯¹è¿™æ®µæ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œä»¥åŠè¿™å¯èƒ½å¯¹è¯­è¨€æ¨¡å‹æ„å‘³ç€ä»€ä¹ˆã€‚æˆ‘ä»¬å°†å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œç„¶åæ‰“å°æ¯ä¸ªåˆ†è¯ï¼ŒèƒŒæ™¯é¢œè‰²ä¸ºç°è‰²ã€‚
- en: bert-base-uncased
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bert-base-uncased
- en: 'Tokenization method: WordPiece, introduced in [Japanese and Korean voice search](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–¹æ³•ï¼šWordPieceï¼Œä»‹ç»äº [æ—¥æœ¬å’ŒéŸ©å›½è¯­éŸ³æœç´¢](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
- en: 'Vocabulary size: 30522'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡å¤§å°ï¼š30522
- en: 'Special tokens: â€˜unk_tokenâ€™: ''[UNK]'''
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç‰¹æ®Šåˆ†è¯ï¼šâ€˜unk_tokenâ€™: ''[UNK]'''
- en: 'â€™sep_tokenâ€™: ''[SEP]'''
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'â€™sep_tokenâ€™: ''[SEP]'''
- en: 'â€˜pad_tokenâ€™: ''[PAD]'''
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'â€˜pad_tokenâ€™: ''[PAD]'''
- en: 'â€˜cls_tokenâ€™: ''[CLS]'''
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'â€˜cls_tokenâ€™: ''[CLS]'''
- en: 'â€˜mask_tokenâ€™: ''[MASK]'''
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'â€˜mask_tokenâ€™: ''[MASK]'''
- en: 'Tokenized text:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–‡æœ¬ï¼š
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the uncased (and more popular) version of the BERT tokenizer, we notice
    the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ— å¤§å°å†™ï¼ˆæ›´æµè¡Œï¼‰çš„ BERT åˆ†è¯å™¨ç‰ˆæœ¬ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: The newline breaks are gone, which makes the model blind to information encoded
    in newlines (e.g., a chat log when each turn is in a new line)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢è¡Œç¬¦è¢«å»æ‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹æ— æ³•è¯†åˆ«ç¼–ç åœ¨æ¢è¡Œä¸­çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ¯æ¬¡è½®æ¢åœ¨æ–°è¡Œä¸­çš„èŠå¤©è®°å½•ï¼‰
- en: All the text is in lower case
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ–‡æœ¬å‡ä¸ºå°å†™å­—æ¯
- en: 'The word â€œcapitalizationâ€ is encoded as two subtokens capital ##ization . The
    ## characters are used to indicate this token is a partial token connected to
    the token the precedes it. This is also a method to indicate where the spaces
    are, it is assumed tokens without ## before them have a space before them.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œcapitalizationâ€è¿™ä¸ªè¯è¢«ç¼–ç ä¸ºä¸¤ä¸ªå­åˆ†è¯ capital ##izationã€‚## å­—ç¬¦ç”¨äºæŒ‡ç¤ºè¯¥åˆ†è¯æ˜¯ä¸å‰ä¸€ä¸ªåˆ†è¯ç›¸è¿çš„éƒ¨åˆ†åˆ†è¯ã€‚è¿™ä¹Ÿæ˜¯æŒ‡ç¤ºç©ºæ ¼ä½ç½®çš„ä¸€ç§æ–¹æ³•ï¼Œå‡å®šæ²¡æœ‰
    ## çš„åˆ†è¯å‰æœ‰ä¸€ä¸ªç©ºæ ¼ã€‚'
- en: The emoji and Chinese characters are gone and replaced with the [UNK] special
    token indicating an â€œunknown tokenâ€.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·å’Œä¸­æ–‡å­—ç¬¦è¢«æ›¿æ¢ä¸º [UNK] ç‰¹æ®Šåˆ†è¯ï¼Œè¡¨ç¤ºâ€œæœªçŸ¥åˆ†è¯â€ã€‚
- en: bert-base-cased
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bert-base-cased
- en: 'Tokenization method: WordPiece'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–¹æ³•ï¼šWordPiece
- en: 'Vocabulary size: 28,996'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼š28,996
- en: 'Special tokens: Same as the uncased version'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼šä¸æ— å¤§å°å†™ç‰ˆæœ¬ç›¸åŒ
- en: 'Tokenized text:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–‡æœ¬ï¼š
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The cased version of the BERT tokenizer differs mainly in including upper-case
    tokens.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BERTæ ‡è®°å™¨çš„å¤§å°å†™ç‰ˆæœ¬ä¸»è¦ä¸åŒäºåŒ…æ‹¬å¤§å†™æ ‡è®°ã€‚
- en: 'Notice how â€œCAPITALIZATIONâ€ is now represented as eight tokens: CA ##PI ##TA
    ##L ##I ##Z ##AT ##ION'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æ³¨æ„â€œCAPITALIZATIONâ€ç°åœ¨è¡¨ç¤ºä¸ºå…«ä¸ªæ ‡è®°ï¼šCA ##PI ##TA ##L ##I ##Z ##AT ##ION'
- en: Both BERT tokenizers wrap the input within a starting [CLS] token and a closing
    [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and
    they serve their own purposes. [CLS] stands for Classification as itâ€™s a token
    used at times for sentence classification. [SEP] stands for Separator, as itâ€™s
    used to separate sentences in some applications that require passing two sentences
    to a model (For example, in the rerankers in chapter 3, we would use a [SEP] token
    to separate the text of the query and a candidate result).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªBERTæ ‡è®°å™¨åœ¨è¾“å…¥å‘¨å›´åŒ…è£¹ä¸€ä¸ªèµ·å§‹çš„[CLS]æ ‡è®°å’Œä¸€ä¸ªç»“æŸçš„[SEP]æ ‡è®°ã€‚[CLS]ä»£è¡¨åˆ†ç±»ï¼Œå› ä¸ºå®ƒæ˜¯æœ‰æ—¶ç”¨äºå¥å­åˆ†ç±»çš„æ ‡è®°ã€‚[SEP]ä»£è¡¨åˆ†éš”ç¬¦ï¼Œå› ä¸ºå®ƒç”¨äºåœ¨ä¸€äº›éœ€è¦å°†ä¸¤ä¸ªå¥å­ä¼ é€’ç»™æ¨¡å‹çš„åº”ç”¨ä¸­åˆ†éš”å¥å­ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç¬¬ä¸‰ç« çš„é‡æ–°æ’åºå™¨ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨[SEP]æ ‡è®°æ¥åˆ†éš”æŸ¥è¯¢æ–‡æœ¬å’Œå€™é€‰ç»“æœï¼‰ã€‚
- en: gpt2
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gpt2
- en: 'Tokenization method: BPE, introduced in [Neural Machine Translation of Rare
    Words with Subword Units](https://arxiv.org/abs/1508.07909)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–¹æ³•ï¼šBPEï¼Œä»‹ç»äº[ç¥ç»æœºå™¨ç¿»è¯‘ä¸­çš„ç¨€æœ‰è¯çš„å­è¯å•å…ƒ](https://arxiv.org/abs/1508.07909)
- en: 'Vocabulary size: 50,257'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼š50,257
- en: 'Special tokens: <|endoftext|>'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š<|endoftext|>
- en: 'Tokenized text:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–‡æœ¬ï¼š
- en: English and CAP ITAL IZ ATION
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è‹±è¯­å’ŒCAP ITAL IZ ATION
- en: ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½
- en: 'show _ t ok ens False None el if == >= else :'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'show _ t ok ens False None el if == >= else :'
- en: 'Four spaces : " " Two tabs : " "'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å››ä¸ªç©ºæ ¼ï¼š " " ä¸¤ä¸ªåˆ¶è¡¨ç¬¦ï¼š " "
- en: 12 . 0 * 50 = 600
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 12 . 0 * 50 = 600
- en: 'With the GPT-2 tokenizer, we notice the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨GPT-2æ ‡è®°å™¨ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: The newline breaks are represented in the tokenizer
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢è¡Œç¬¦åœ¨æ ‡è®°å™¨ä¸­è¡¨ç¤º
- en: Capitalization is preserved, and the word â€œCAPITALIZATIONâ€ is represented in
    four tokens
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å†™å­—æ¯ä¿æŒä¸å˜ï¼Œå•è¯â€œCAPITALIZATIONâ€ç”±å››ä¸ªæ ‡è®°è¡¨ç¤ºã€‚
- en: 'The ğŸµ èŸ characters are now represented into multiple tokens each. While we see
    these tokens printed as the ï¿½ character, they actually stand for different tokens.
    For example, the ğŸµ emoji is broken down into the tokens with token ids: 8582,
    236, and 113\. The tokenizer is successful in reconstructing the original character
    from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]),
    which prints out ğŸµ'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸµ èŸ å­—ç¬¦ç°åœ¨è¢«è¡¨ç¤ºä¸ºå¤šä¸ªæ ‡è®°ã€‚å°½ç®¡æˆ‘ä»¬çœ‹åˆ°è¿™äº›æ ‡è®°ä»¥ï¿½å­—ç¬¦æ‰“å°ï¼Œä½†å®ƒä»¬å®é™…ä¸Šä»£è¡¨ä¸åŒçš„æ ‡è®°ã€‚ä¾‹å¦‚ï¼ŒğŸµ emojiè¢«åˆ†è§£ä¸ºæ ‡è®°ï¼Œæ ‡è®°IDä¸ºï¼š8582ï¼Œ236å’Œ113ã€‚æ ‡è®°å™¨æˆåŠŸåœ°ä»è¿™äº›æ ‡è®°é‡æ„äº†åŸå§‹å­—ç¬¦ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰“å°tokenizer.decode([8582,
    236, 113])æ¥çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œå®ƒæ‰“å°å‡ºğŸµ
- en: The two tabs are represented as two tokens (token number 197 in that vocabulary)
    and the four spaces are represented as three tokens (number 220) with the final
    space being a part of the token for the closing quote character.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªåˆ¶è¡¨ç¬¦è¢«è¡¨ç¤ºä¸ºä¸¤ä¸ªæ ‡è®°ï¼ˆè¯¥è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ç¼–å·197ï¼‰ï¼Œå››ä¸ªç©ºæ ¼è¢«è¡¨ç¤ºä¸ºä¸‰ä¸ªæ ‡è®°ï¼ˆç¼–å·220ï¼‰ï¼Œæœ€åä¸€ä¸ªç©ºæ ¼æ˜¯å…³é—­å¼•å·å­—ç¬¦çš„æ ‡è®°çš„ä¸€éƒ¨åˆ†ã€‚
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: What is the significance of white space characters? These are important for
    models that understand or generate code. A model that uses a single token to represent
    four consecutive white space characters can be said to be more tuned to a python
    code dataset. While a model can live with representing it as four different tokens,
    it does make the modeling more difficult as the model needs to keep track of the
    indentation level. This is an example of where tokenization choices can help the
    model improve on a certain task.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç©ºç™½å­—ç¬¦çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿè¿™äº›å¯¹äºç†è§£æˆ–ç”Ÿæˆä»£ç çš„æ¨¡å‹å¾ˆé‡è¦ã€‚ä½¿ç”¨å•ä¸ªæ ‡è®°è¡¨ç¤ºå››ä¸ªè¿ç»­ç©ºç™½å­—ç¬¦çš„æ¨¡å‹ï¼Œå¯ä»¥è¯´æ›´é€‚åˆPythonä»£ç æ•°æ®é›†ã€‚å°½ç®¡æ¨¡å‹å¯ä»¥ç”¨å››ä¸ªä¸åŒçš„æ ‡è®°è¡¨ç¤ºï¼Œä½†è¿™ç¡®å®ä½¿å»ºæ¨¡æ›´åŠ å›°éš¾ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦è·Ÿè¸ªç¼©è¿›çº§åˆ«ã€‚è¿™æ˜¯æ ‡è®°åŒ–é€‰æ‹©å¯ä»¥å¸®åŠ©æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæ”¹è¿›çš„ä¸€ä¸ªä¾‹å­ã€‚
- en: google/flan-t5-xxl
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: google/flan-t5-xxl
- en: 'Tokenization method: SentencePiece, introduced in [SentencePiece: A simple
    and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–¹æ³•ï¼šSentencePieceï¼Œä»‹ç»äº[SentencePieceï¼šä¸€ç§ç®€å•çš„ä¸è¯­è¨€æ— å…³çš„å­è¯æ ‡è®°å™¨å’Œè§£ç å™¨ï¼Œç”¨äºç¥ç»æ–‡æœ¬å¤„ç†](https://arxiv.org/pdf/1808.06226.pdf)
- en: 'Vocabulary size: 32,100'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼š32,100
- en: 'Special tokens:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š
- en: '- â€˜unk_tokenâ€™: ''<unk>'''
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- â€˜unk_tokenâ€™: ''<unk>'''
- en: '- â€˜pad_tokenâ€™: ''<pad>'''
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '- â€˜pad_tokenâ€™: ''<pad>'''
- en: 'Tokenized text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–‡æœ¬ï¼š
- en: 'English and CA PI TAL IZ ATION <unk> <unk> show _ to ken s Fal s e None e l
    if = = > = else : Four spaces : " " Two tab s : " " 12\. 0 * 50 = 600 </s>'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'è‹±è¯­å’Œ CA PI TAL IZ ATION <unk> <unk> æ˜¾ç¤º _ to ken s Fal s e None e l if = = >
    = else : å››ä¸ªç©ºæ ¼ : " " ä¸¤ä¸ªåˆ¶è¡¨ç¬¦ : " " 12\. 0 * 50 = 600 </s>'
- en: 'The FLAN-T5 family of models use the sentencepiece method. We notice the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5æ¨¡å‹å®¶æ—ä½¿ç”¨sentencepieceæ–¹æ³•ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: No newline or whitespace tokens, this would make it challenging for the model
    to work with code.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰æ¢è¡Œæˆ–ç©ºç™½æ ‡è®°ï¼Œè¿™ä¼šä½¿æ¨¡å‹å¤„ç†ä»£ç å˜å¾—å›°éš¾ã€‚
- en: The emoji and Chinese characters are both replaced by the <unk> token. Making
    the model completely blind to them.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·å’Œæ±‰å­—éƒ½è¢«æ›¿æ¢ä¸º<unk>æ ‡è®°ï¼Œä½¿æ¨¡å‹å¯¹æ­¤å®Œå…¨æ— æ„Ÿã€‚
- en: GPT-4
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4
- en: 'Tokenization method: BPE'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–¹æ³•ï¼šBPE
- en: 'Vocabulary size: a little over 100,000'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼šç•¥è¶…è¿‡100,000
- en: 'Special tokens:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š
- en: <|endoftext|>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <|endoftext|>
- en: 'Fill in the middle tokens. These three tokens enable the GPT-4 capability of
    generating a completion given not only the text before it but also considering
    the text after it. This method is explained in more detail in the paper [Efficient
    Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255).
    These special tokens are:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¡«å……ä¸­é—´æ ‡è®°ã€‚è¿™ä¸‰ä¸ªæ ‡è®°ä½¿å¾—GPT-4èƒ½å¤Ÿç”Ÿæˆè¡¥å…¨ï¼Œä¸ä»…è€ƒè™‘ä¹‹å‰çš„æ–‡æœ¬ï¼Œè¿˜è€ƒè™‘ä¹‹åçš„æ–‡æœ¬ã€‚è¯¥æ–¹æ³•åœ¨è®ºæ–‡[é«˜æ•ˆè®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥å¡«å……ä¸­é—´éƒ¨åˆ†](https://arxiv.org/abs/2207.14255)ä¸­æœ‰æ›´è¯¦ç»†çš„è§£é‡Šã€‚è¿™äº›ç‰¹æ®Šæ ‡è®°ä¸ºï¼š
- en: <|fim_prefix|>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_prefix|>
- en: <|fim_middle|>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_middle|>
- en: <|fim_suffix|>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_suffix|>
- en: 'Tokenized text:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–‡æœ¬ï¼š
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The GPT-4 tokenizer behaves similarly with its ancestor, the GPT-2 tokenizer.
    Some differences are:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4çš„åˆ†è¯å™¨è¡Œä¸ºä¸å…¶å‰èº«GPT-2çš„åˆ†è¯å™¨ç›¸ä¼¼ã€‚æœ‰äº›å·®å¼‚æ˜¯ï¼š
- en: The GPT-4 tokenizer represents the four spaces as a single token. In fact, it
    has a specific token to every sequence of white spaces up until a list of 83 white
    spaces.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4çš„åˆ†è¯å™¨å°†å››ä¸ªç©ºæ ¼è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‡è®°ã€‚å®é™…ä¸Šï¼Œå®ƒå¯¹æ¯ä¸ªç©ºç™½åºåˆ—éƒ½æœ‰ç‰¹å®šçš„æ ‡è®°ï¼Œç›´åˆ°ä¸€åˆ—83ä¸ªç©ºæ ¼ã€‚
- en: The python keyword elif has its own token in GPT-4\. Both this and the previous
    point stem from the modelâ€™s focus on code in addition to natural language.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pythonå…³é”®å­—elifåœ¨GPT-4ä¸­æœ‰è‡ªå·±çš„æ ‡è®°ã€‚ä¸Šè¿°è¿™ä¸€ç‚¹å’Œå‰ä¸€ç‚¹å‡æºäºæ¨¡å‹å¯¹ä»£ç çš„å…³æ³¨ï¼Œé™¤äº†è‡ªç„¶è¯­è¨€ã€‚
- en: The GPT-4 tokenizer uses fewer tokens to represent most words. Example here
    include â€˜CAPITALIZATIONâ€™ (two tokens, vs. four) and â€˜tokensâ€™ (one token vs. three).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4çš„åˆ†è¯å™¨ä½¿ç”¨æ›´å°‘çš„æ ‡è®°æ¥è¡¨ç¤ºå¤§å¤šæ•°å•è¯ã€‚è¿™é‡Œçš„ä¾‹å­åŒ…æ‹¬â€˜CAPITALIZATIONâ€™ï¼ˆä¸¤ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯å››ä¸ªï¼‰å’Œâ€˜tokensâ€™ï¼ˆä¸€ä¸ªæ ‡è®°è€Œä¸æ˜¯ä¸‰ä¸ªï¼‰ã€‚
- en: bigcode/starcoder
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bigcode/starcoder
- en: 'Tokenization method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–¹æ³•ï¼š
- en: 'Vocabulary size: about 50,000'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼šçº¦50,000
- en: 'Special tokens:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š
- en: '''<|endoftext|>'''
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '''<|endoftext|>'''
- en: 'FIll in the middle tokens:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¡«å……ä¸­é—´æ ‡è®°ï¼š
- en: '''<fim_prefix>'''
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_prefix>'''
- en: '''<fim_middle>'''
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_middle>'''
- en: '''<fim_suffix>'''
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_suffix>'''
- en: '''<fim_pad>'''
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_pad>'''
- en: 'When representing code, managing the context is important. One file might make
    a function call to a function that is defined in a different file. So the model
    needs some way of being able to identify code that is in different files in the
    same code repository, while making a distinction between code in different repos.
    Thatâ€™s why starcoder uses special tokens for the name of the repository and the
    filename:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡¨ç¤ºä»£ç æ—¶ï¼Œç®¡ç†ä¸Šä¸‹æ–‡å¾ˆé‡è¦ã€‚ä¸€ä¸ªæ–‡ä»¶å¯èƒ½ä¼šè°ƒç”¨åœ¨å¦ä¸€ä¸ªæ–‡ä»¶ä¸­å®šä¹‰çš„å‡½æ•°ã€‚å› æ­¤ï¼Œæ¨¡å‹éœ€è¦æŸç§æ–¹å¼æ¥è¯†åˆ«åœ¨åŒä¸€ä»£ç åº“ä¸­ä¸åŒæ–‡ä»¶ä¸­çš„ä»£ç ï¼ŒåŒæ—¶åŒºåˆ†ä¸åŒä»£ç åº“ä¸­çš„ä»£ç ã€‚è¿™å°±æ˜¯starcoderä¸ºåº“åç§°å’Œæ–‡ä»¶åä½¿ç”¨ç‰¹æ®Šæ ‡è®°çš„åŸå› ï¼š
- en: '''<filename>'''
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '''<filename>'''
- en: '''<reponame>'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '''<reponame>'''
- en: '''<gh_stars>'''
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '''<gh_stars>'''
- en: 'The tokenizer also includes a bunch of the special tokens to perform better
    on code. These include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥åˆ†è¯å™¨è¿˜åŒ…å«ä¸€ç³»åˆ—ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¾¿åœ¨ä»£ç ä¸Šè¡¨ç°å¾—æ›´å¥½ã€‚è¿™äº›åŒ…æ‹¬ï¼š
- en: '''<issue_start>'''
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '''<issue_start>'''
- en: '''<jupyter_start>'''
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '''<jupyter_start>'''
- en: '''<jupyter_text>'''
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '''<jupyter_text>'''
- en: 'Paper: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ï¼š[StarCoderï¼šæ„¿æºä»£ç ä¸ä½ åŒåœ¨ï¼](https://arxiv.org/abs/2305.06161)
- en: 'Tokenized text:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–‡æœ¬ï¼š
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is an encoder that focuses on code generation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºä»£ç ç”Ÿæˆçš„ç¼–ç å™¨ã€‚
- en: Similarly to GPT-4, it encodes the list of white spaces as a single token
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸GPT-4ç±»ä¼¼ï¼Œå®ƒå°†ç©ºæ ¼åˆ—è¡¨ç¼–ç ä¸ºä¸€ä¸ªå•ä¸€æ ‡è®°ã€‚
- en: A major difference here to everyone weâ€™ve seen so far is that each digit is
    assigned its own token (so 600 becomes 6 0 0). The hypothesis here is that this
    would lead to better representation of numbers and mathematics. In GPT-2, for
    example, the number 870 is represented as a single token. But 871 is represented
    as two tokens (8 and 71). You can intuitively see how that might be confusing
    to the model and how it represents numbers.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é‡Œä¸€ä¸ªä¸»è¦çš„ä¸åŒä¹‹å¤„æ˜¯æ¯ä¸ªæ•°å­—éƒ½è¢«åˆ†é…äº†è‡ªå·±çš„æ ‡è®°ï¼ˆå› æ­¤600å˜ä¸º6 0 0ï¼‰ã€‚å‡è®¾è¿™æ˜¯ä¸ºäº†æ›´å¥½åœ°è¡¨ç¤ºæ•°å­—å’Œæ•°å­¦ã€‚åœ¨GPT-2ä¸­ï¼Œä¾‹å¦‚ï¼Œæ•°å­—870è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‡è®°ã€‚ä½†871è¡¨ç¤ºä¸ºä¸¤ä¸ªæ ‡è®°ï¼ˆ8å’Œ71ï¼‰ã€‚ä½ å¯ä»¥ç›´è§‚åœ°ç†è§£ï¼Œè¿™å¯èƒ½ä¼šè®©æ¨¡å‹åœ¨è¡¨ç¤ºæ•°å­—æ—¶æ„Ÿåˆ°å›°æƒ‘ã€‚
- en: facebook/galactica-1.3b
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: facebook/galactica-1.3b
- en: 'The galactica model described in [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
    is focused on scientific knowledge and is trained on many scientific papers, reference
    materials, and knowledge bases. It pays extra attention to tokenization that makes
    it more sensitive to the nuances of the dataset itâ€™s representing. For example,
    it includes special tokens for citations, reasoning, mathematics, Amino Acid sequences,
    and DNA sequences.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨[Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)ä¸­æè¿°çš„
    Galactica æ¨¡å‹ä¸“æ³¨äºç§‘å­¦çŸ¥è¯†ï¼Œå¹¶åœ¨è®¸å¤šç§‘å­¦è®ºæ–‡ã€å‚è€ƒææ–™å’ŒçŸ¥è¯†åº“ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚å®ƒç‰¹åˆ«å…³æ³¨åˆ†è¯ï¼Œä½¿å…¶å¯¹æ‰€ä»£è¡¨çš„æ•°æ®é›†çš„ç»†å¾®å·®åˆ«æ›´åŠ æ•æ„Ÿã€‚ä¾‹å¦‚ï¼Œå®ƒåŒ…å«ç”¨äºå¼•ç”¨ã€æ¨ç†ã€æ•°å­¦ã€æ°¨åŸºé…¸åºåˆ—å’Œ
    DNA åºåˆ—çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: 'Tokenization method:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–¹æ³•ï¼š
- en: 'Vocabulary size: 50,000'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°ï¼š50,000
- en: 'Special tokens:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ï¼š
- en: <s>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <s>
- en: <pad>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <pad>
- en: </s>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: </s>
- en: <unk>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <unk>
- en: 'References: Citations are wrapped within the two special tokens:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼šå¼•ç”¨è¢«åŒ…è£¹åœ¨ä¸¤ä¸ªç‰¹æ®Šæ ‡è®°å†…ï¼š
- en: '[START_REF]'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¼€å§‹å¼•ç”¨]'
- en: '[END_REF]'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç»“æŸå¼•ç”¨]'
- en: 'One example of usage from the paper is:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­çš„ä¸€ä¸ªä½¿ç”¨ç¤ºä¾‹æ˜¯ï¼š
- en: Recurrent neural networks, long short-term memory [START_REF]Long Short-Term
    Memory, Hochreiter[END_REF]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œï¼Œé•¿çŸ­æœŸè®°å¿†[å¼€å§‹å¼•ç”¨]é•¿çŸ­æœŸè®°å¿†ï¼Œéœå¸Œç‰¹[ç»“æŸå¼•ç”¨]
- en: Step-by-Step Reasoning -
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥éª¤æ¨ç† -
- en: <work> is an interesting token that the model uses for chain-of-thought reasoning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <work> æ˜¯æ¨¡å‹ç”¨äºè¿é”æ¨ç†çš„ä¸€ä¸ªæœ‰è¶£æ ‡è®°ã€‚
- en: 'Tokenized text:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–‡æœ¬ï¼š
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Galactica tokenizer behaves similar to star coder in that it has code in
    mind. It also encodes white spaces in the same way - assigning a single token
    to sequences of whitespace of different lengths. It differs in that it also does
    that for tabs, though. So from all the tokenizers weâ€™ve seen so far, itâ€™s the
    only one thatâ€™s assigned a single token to the string made up of two tabs ('\t\t')
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Galactica æ ‡è®°å™¨çš„è¡Œä¸ºç±»ä¼¼äº Star Coderï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†ä»£ç ã€‚å®ƒä¹Ÿä»¥ç›¸åŒçš„æ–¹å¼ç¼–ç ç©ºç™½ - å°†ä¸åŒé•¿åº¦çš„ç©ºç™½åºåˆ—åˆ†é…ç»™ä¸€ä¸ªå•ä¸€çš„æ ‡è®°ã€‚å®ƒçš„ä¸åŒä¹‹å¤„åœ¨äºå®ƒä¹Ÿä¼šå¯¹åˆ¶è¡¨ç¬¦è¿™æ ·å¤„ç†ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢çœ‹åˆ°çš„æ‰€æœ‰æ ‡è®°å™¨ä¸­ï¼Œå®ƒæ˜¯å”¯ä¸€ä¸€ä¸ªå°†ç”±ä¸¤ä¸ªåˆ¶è¡¨ç¬¦ï¼ˆ'\t\t'ï¼‰ç»„æˆçš„å­—ç¬¦ä¸²åˆ†é…ç»™å•ä¸€æ ‡è®°çš„æ ‡è®°å™¨ã€‚
- en: 'We can now recap our tour by looking at all these examples side by side:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡å¹¶æ’æŸ¥çœ‹æ‰€æœ‰è¿™äº›ç¤ºä¾‹æ¥å›é¡¾æˆ‘ä»¬çš„å¯¼è§ˆï¼š
- en: '| bert-base-uncased |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| bert-base-uncased |'
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| bert-base-cased |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| bert-base-cased |'
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| gpt2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| gpt2 |'
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| google/flan-t5-xxl |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| google/flan-t5-xxl |'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GPT-4 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 |'
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| bigcode/starcoder |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| bigcode/starcoder |'
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| facebook/galactica-1.3b |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| facebook/galactica-1.3b |'
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| meta-llama/Llama-2-70b-chat-hf |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| meta-llama/Llama-2-70b-chat-hf |'
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Notice how thereâ€™s a new tokenizer added in the bottom. By now, you should be
    able to understand many of its properties by just glancing at this output. This
    is the tokenizer for LLaMA2, the most recent of these models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåº•éƒ¨æ·»åŠ äº†ä¸€ä¸ªæ–°çš„æ ‡è®°å™¨ã€‚åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œä½ åº”è¯¥èƒ½å¤Ÿé€šè¿‡å¿«é€Ÿæµè§ˆè¿™ä¸ªè¾“å‡ºç†è§£å®ƒçš„è®¸å¤šå±æ€§ã€‚è¿™æ˜¯ LLaMA2 çš„æ ‡è®°å™¨ï¼Œè¿™äº›æ¨¡å‹ä¸­æœ€æ–°çš„ä¸€ä¸ªã€‚
- en: Tokenizer Properties
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ‡è®°å™¨å±æ€§
- en: 'The preceding guided tour of trained tokenizers showed a number of ways in
    which actual tokenizers differ from each other. But what determines their tokenization
    behavior? There are three major groups of design choices that determine how the
    tokenizer will break down text: The tokenization method, the initialization parameters,
    and the dataset we train the tokenizer (but not the model) on.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„è®­ç»ƒæ ‡è®°å™¨å¯¼è§ˆå±•ç¤ºäº†å®é™…æ ‡è®°å™¨ä¹‹é—´çš„å¤šç§å·®å¼‚ã€‚ä½†æ˜¯ä»€ä¹ˆå†³å®šäº†å®ƒä»¬çš„åˆ†è¯è¡Œä¸ºå‘¢ï¼Ÿæœ‰ä¸‰ä¸ªä¸»è¦çš„è®¾è®¡é€‰æ‹©å†³å®šäº†æ ‡è®°å™¨å¦‚ä½•æ‹†åˆ†æ–‡æœ¬ï¼šåˆ†è¯æ–¹æ³•ã€åˆå§‹åŒ–å‚æ•°ï¼Œä»¥åŠæˆ‘ä»¬è®­ç»ƒæ ‡è®°å™¨ï¼ˆä½†ä¸æ˜¯æ¨¡å‹ï¼‰æ‰€ç”¨çš„æ•°æ®é›†ã€‚
- en: Tokenization methods
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†è¯æ–¹æ³•
- en: As weâ€™ve seen, there are a number of tokenization methods with Byte-Pair Encoding
    (BPE), WordPiece, and SentencePiece being some of the more popular ones. Each
    of these methods outlines an algorithm for how to choose an appropriate set of
    tokens to represent a dataset. A great overview of all these methods can be found
    in the Hugging Face [Summary of the tokenizers page](https://huggingface.co/docs/transformers/tokenizer_summary).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæœ‰å¤šç§åˆ†è¯æ–¹æ³•ï¼Œå…¶ä¸­ Byte-Pair Encoding (BPE)ã€WordPiece å’Œ SentencePiece æ˜¯è¾ƒä¸ºæµè¡Œçš„å‡ ç§ã€‚æ¯ç§æ–¹æ³•éƒ½æ¦‚è¿°äº†ä¸€ç§ç®—æ³•ï¼Œç”¨äºé€‰æ‹©é€‚å½“çš„æ ‡è®°é›†æ¥è¡¨ç¤ºæ•°æ®é›†ã€‚æœ‰å…³æ‰€æœ‰è¿™äº›æ–¹æ³•çš„ä¼˜ç§€æ¦‚è¿°å¯ä»¥åœ¨
    Hugging Face çš„[åˆ†è¯å™¨é¡µé¢æ‘˜è¦](https://huggingface.co/docs/transformers/tokenizer_summary)ä¸­æ‰¾åˆ°ã€‚
- en: Tokenizer Parameters
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡è®°å™¨å‚æ•°
- en: 'After choosing a tokenization method, an LLM designer needs to make some decisions
    about the parameters of the tokenizer. These include:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆ†è¯æ–¹æ³•åï¼ŒLLM è®¾è®¡å¸ˆéœ€è¦å¯¹æ ‡è®°å™¨çš„å‚æ•°åšå‡ºä¸€äº›å†³ç­–ã€‚è¿™äº›åŒ…æ‹¬ï¼š
- en: Vocabulary size
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡è¡¨å¤§å°
- en: How many tokens to keep in the tokenizerâ€™s vocabulary? (30K, 50K are often used
    vocabulary size values, but more and more weâ€™re seeing larger sizes like 100K)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†è¯å™¨çš„è¯æ±‡ä¸­åº”ä¿ç•™å¤šå°‘ä¸ªä»¤ç‰Œï¼Ÿï¼ˆå¸¸ç”¨çš„è¯æ±‡å¤§å°å€¼ä¸º30Kã€50Kï¼Œä½†è¶Šæ¥è¶Šå¤šçš„æƒ…å†µæ˜¯çœ‹åˆ°åƒ100Kè¿™æ ·æ›´å¤§çš„å¤§å°ï¼‰
- en: Special tokens
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šä»¤ç‰Œ
- en: 'What special tokens do we want the model to keep track of. We can add as many
    of these as we want, especially if we want to build LLM for special use cases.
    Common choices include:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ¨¡å‹è·Ÿè¸ªå“ªäº›ç‰¹æ®Šä»¤ç‰Œã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ä»»æ„æ•°é‡ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæˆ‘ä»¬æƒ³ä¸ºç‰¹å®šç”¨ä¾‹æ„å»ºLLMã€‚å¸¸è§çš„é€‰æ‹©åŒ…æ‹¬ï¼š
- en: Beginning of text token (e.g., <s>)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬å¼€å§‹ä»¤ç‰Œï¼ˆä¾‹å¦‚ï¼Œ<s>ï¼‰
- en: End of text token
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç»“æŸä»¤ç‰Œ
- en: Padding token
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡«å……ä»¤ç‰Œ
- en: Unknown token
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªçŸ¥ä»¤ç‰Œ
- en: CLS token
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLSä»¤ç‰Œ
- en: Masking token
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ©ç ä»¤ç‰Œ
- en: Aside from these, the LLM designer can add tokens that help better model the
    domain of the problem theyâ€™re trying to focus on, as weâ€™ve seen with Galacticaâ€™s
    <work> and [START_REF] tokens.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤æ­¤ä¹‹å¤–ï¼ŒLLMè®¾è®¡è€…å¯ä»¥æ·»åŠ æœ‰åŠ©äºæ›´å¥½åœ°å»ºæ¨¡ä»–ä»¬è¯•å›¾å…³æ³¨çš„é—®é¢˜é¢†åŸŸçš„ä»¤ç‰Œï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨Galacticaçš„<work>å’Œ[START_REF]ä»¤ç‰Œä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Capitalization
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å†™
- en: In languages such as English, how do we want to deal with capitalization? Should
    we convert everything to lower-case? (Name capitalization often carries useful
    information, but do we want to waste token vocabulary space on all caps versions
    of words?). This is why some models are released in both cased and uncased versions
    (like [Bert-base cased](https://huggingface.co/bert-base-cased) and the more popular
    [Bert-base uncased](https://huggingface.co/bert-base-uncased)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åƒè‹±è¯­è¿™æ ·çš„è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å¦‚ä½•å¤„ç†å¤§å†™ï¼Ÿæˆ‘ä»¬æ˜¯å¦åº”è¯¥å°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™ï¼Ÿï¼ˆåç§°çš„å¤§å†™é€šå¸¸æºå¸¦æœ‰ç”¨çš„ä¿¡æ¯ï¼Œä½†æˆ‘ä»¬æ˜¯å¦æƒ³åœ¨æ‰€æœ‰å¤§å†™ç‰ˆæœ¬çš„å•è¯ä¸Šæµªè´¹ä»¤ç‰Œè¯æ±‡ç©ºé—´ï¼Ÿï¼‰è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€äº›æ¨¡å‹ä»¥å¤§å†™å’Œå°å†™ç‰ˆæœ¬å‘å¸ƒï¼ˆå¦‚[Bert-base
    cased](https://huggingface.co/bert-base-cased)å’Œæ›´æµè¡Œçš„[Bert-base uncased](https://huggingface.co/bert-base-uncased)ï¼‰ã€‚
- en: The Tokenizer Training Dataset
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è®­ç»ƒæ•°æ®é›†
- en: Even if we select the same method and parameters, tokenizer behavior will be
    different based on the dataset it was trained on (before we even start model training).
    The tokenization methods mentioned previously work by optimizing the vocabulary
    to represent a specific dataset. From our guided tour weâ€™ve seen how that has
    an impact on datasets like code, and multilingual text.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æˆ‘ä»¬é€‰æ‹©ç›¸åŒçš„æ–¹æ³•å’Œå‚æ•°ï¼Œåˆ†è¯å™¨çš„è¡Œä¸ºä¹Ÿä¼šå› å…¶è®­ç»ƒçš„æ•°æ®é›†è€Œå¼‚ï¼ˆåœ¨æˆ‘ä»¬ç”šè‡³å¼€å§‹æ¨¡å‹è®­ç»ƒä¹‹å‰ï¼‰ã€‚å‰é¢æåˆ°çš„åˆ†è¯æ–¹æ³•é€šè¿‡ä¼˜åŒ–è¯æ±‡æ¥ä»£è¡¨ç‰¹å®šçš„æ•°æ®é›†ã€‚ä»æˆ‘ä»¬çš„å¯¼è§ˆä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™å¯¹ä»£ç å’Œå¤šè¯­è¨€æ–‡æœ¬ç­‰æ•°æ®é›†äº§ç”Ÿäº†å½±å“ã€‚
- en: 'For code, for example, weâ€™ve seen that a text-focused tokenizer may tokenize
    the indentation spaces like this (Weâ€™ll highlight some tokens in yellow and green):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºä»£ç ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„åˆ†è¯å™¨å¯èƒ½ä¼šåƒè¿™æ ·åˆ†è¯ç¼©è¿›ç©ºæ ¼ï¼ˆæˆ‘ä»¬å°†ä¸€äº›ä»¤ç‰Œç”¨é»„è‰²å’Œç»¿è‰²çªå‡ºæ˜¾ç¤ºï¼‰ï¼š
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which may be suboptimal for a code-focused model. Code-focused models instead
    tend to make different tokenization choices:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»¥ä»£ç ä¸ºä¸­å¿ƒçš„æ¨¡å‹ï¼Œè¿™å¯èƒ½å¹¶ä¸ç†æƒ³ã€‚ä»¥ä»£ç ä¸ºä¸­å¿ƒçš„æ¨¡å‹å¾€å¾€ä¼šåšå‡ºä¸åŒçš„åˆ†è¯é€‰æ‹©ï¼š
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These tokenization choices make the modelâ€™s job easier and thus its performance
    has a higher probability of improving.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åˆ†è¯é€‰æ‹©ä½¿æ¨¡å‹çš„å·¥ä½œå˜å¾—æ›´è½»æ¾ï¼Œå› æ­¤å…¶æ€§èƒ½æ›´æœ‰å¯èƒ½æé«˜ã€‚
- en: A more detailed tutorial on training tokenizers can be found in the [Tokenizers
    section of the Hugging Face course](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt).
    and in [Natural Language Processing with Transformers, Revised Edition](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®­ç»ƒåˆ†è¯å™¨çš„æ›´è¯¦ç»†æ•™ç¨‹å¯ä»¥åœ¨[Hugging Faceè¯¾ç¨‹çš„åˆ†è¯å™¨éƒ¨åˆ†](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt)å’Œ[ã€Šä½¿ç”¨å˜æ¢å™¨çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä¿®è®¢ç‰ˆã€‹](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)ä¸­æ‰¾åˆ°ã€‚
- en: A Language Model Holds Embeddings for the Vocabulary of its Tokenizer
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹ä¸ºå…¶åˆ†è¯å™¨çš„è¯æ±‡æŒæœ‰åµŒå…¥
- en: After a tokenizer is initialized, it is then used in the training process of
    its associated language model. This is why a pre-trained language model is linked
    with its tokenizer and canâ€™t use a different tokenizer without training.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆå§‹åŒ–ä¸€ä¸ªåˆ†è¯å™¨åï¼Œå®ƒå°†åœ¨å…¶ç›¸å…³è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸å…¶åˆ†è¯å™¨ç›¸è¿ï¼Œå¹¶ä¸”åœ¨æœªç»è¿‡è®­ç»ƒçš„æƒ…å†µä¸‹ä¸èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨ã€‚
- en: The language model holds an embedding vector for each token in the tokenizerâ€™s
    vocabulary as we can see in [FigureÂ 6-6](#fig_6__a_language_model_holds_an_embedding_vector_assoc).
    In the beginning, these vectors are randomly initialized like the rest of the
    modelâ€™s weights, but the training process assigns them the values that enable
    the useful behavior theyâ€™re trained to perform.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹ä¸ºåˆ†è¯å™¨è¯æ±‡ä¸­çš„æ¯ä¸ªä»¤ç‰ŒæŒæœ‰ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨[å›¾ 6-6](#fig_6__a_language_model_holds_an_embedding_vector_assoc)ä¸­çœ‹åˆ°çš„ã€‚ä¸€å¼€å§‹ï¼Œè¿™äº›å‘é‡åƒæ¨¡å‹çš„å…¶ä»–æƒé‡ä¸€æ ·éšæœºåˆå§‹åŒ–ï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¼šèµ‹äºˆå®ƒä»¬èƒ½å¤Ÿæ‰§è¡Œæœ‰ç”¨è¡Œä¸ºçš„å€¼ã€‚
- en: '![  A language model holds an embedding vector associated with each token in
    its tokenizer.](assets/tokens_token_embeddings_963889_06.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸€ä¸ªè¯­è¨€æ¨¡å‹æŒæœ‰ä¸å…¶åˆ†è¯å™¨ä¸­æ¯ä¸ªæ ‡è®°ç›¸å…³çš„åµŒå…¥å‘é‡ã€‚](assets/tokens_token_embeddings_963889_06.png)'
- en: Figure 6-6\. A language model holds an embedding vector associated with each
    token in its tokenizer.
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-6\. è¯­è¨€æ¨¡å‹æŒæœ‰ä¸å…¶åˆ†è¯å™¨ä¸­æ¯ä¸ªæ ‡è®°ç›¸å…³çš„åµŒå…¥å‘é‡ã€‚
- en: Creating Contextualized Word Embeddings with Language Models
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯­è¨€æ¨¡å‹åˆ›å»ºä¸Šä¸‹æ–‡åŒ–å•è¯åµŒå…¥ã€‚
- en: Now that weâ€™ve covered token embeddings as the input to a language model, letâ€™s
    look at how language models can *create* better token embeddings. This is one
    of the main ways of using language models for text representation that empowers
    applications like named-entity recognition or extractive text summarization (which
    summarizes a long text by highlighting to most important parts of it, instead
    of generating new text as a summary).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è¦†ç›–äº†ä½œä¸ºè¯­è¨€æ¨¡å‹è¾“å…¥çš„æ ‡è®°åµŒå…¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¯­è¨€æ¨¡å‹å¦‚ä½•*åˆ›å»º*æ›´å¥½çš„æ ‡è®°åµŒå…¥ã€‚è¿™æ˜¯ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬è¡¨ç¤ºçš„ä¸»è¦æ–¹å¼ä¹‹ä¸€ï¼Œèµ‹èƒ½åº”ç”¨ç¨‹åºå¦‚å‘½åå®ä½“è¯†åˆ«æˆ–æå–å¼æ–‡æœ¬æ‘˜è¦ï¼ˆé€šè¿‡çªå‡ºæœ€é‡è¦çš„éƒ¨åˆ†æ¥æ€»ç»“é•¿æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ä½œä¸ºæ‘˜è¦ï¼‰ã€‚
- en: '![  Language models produce contextualized token embeddings that improve on
    raw  static token embeddings](assets/tokens_token_embeddings_963889_07.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸Šä¸‹æ–‡åŒ–æ ‡åµŒå…¥æ¯”åŸå§‹é™æ€æ ‡åµŒå…¥æ›´ä¸ºä¼˜è¶Šã€‚](assets/tokens_token_embeddings_963889_07.png)'
- en: Figure 6-7\. Language models produce contextualized token embeddings that improve
    on raw, static token embeddings
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-7\. è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸Šä¸‹æ–‡åŒ–æ ‡åµŒå…¥æ¯”åŸå§‹é™æ€æ ‡åµŒå…¥æ›´ä¸ºä¼˜è¶Šã€‚
- en: Instead of representing each token or word with a static vector, language models
    create contextualized word embeddings (shown in [FigureÂ 6-7](#fig_7__language_models_produce_contextualized_token_emb))
    that represent a word with a different token based on its context. These vectors
    can then be used by other systems for a variety of tasks. In addition to the text
    applications we mentioned in the previous paragraph, these contextualized vectors,
    for example, are what powers AI image generation systems like Dall-E, Midjourney,
    and Stable Diffusion, for example.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹å¹¶ä¸ä½¿ç”¨é™æ€å‘é‡æ¥è¡¨ç¤ºæ¯ä¸ªæ ‡è®°æˆ–å•è¯ï¼Œè€Œæ˜¯ç”Ÿæˆä¸Šä¸‹æ–‡åŒ–å•è¯åµŒå…¥ï¼ˆå¦‚[å›¾6-7](#fig_7__language_models_produce_contextualized_token_emb)æ‰€ç¤ºï¼‰ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡ä»¥ä¸åŒçš„æ ‡è®°è¡¨ç¤ºå•è¯ã€‚è¿™äº›å‘é‡å¯ä»¥è¢«å…¶ä»–ç³»ç»Ÿç”¨äºå„ç§ä»»åŠ¡ã€‚é™¤äº†æˆ‘ä»¬åœ¨å‰ä¸€æ®µä¸­æåˆ°çš„æ–‡æœ¬åº”ç”¨å¤–ï¼Œè¿™äº›ä¸Šä¸‹æ–‡åŒ–å‘é‡ï¼Œä¾‹å¦‚ï¼Œæ­£æ˜¯é©±åŠ¨AIå›¾åƒç”Ÿæˆç³»ç»Ÿå¦‚Dall-Eã€Midjourneyå’ŒStable
    Diffusionçš„åŠ›é‡ã€‚
- en: 'Code Example: Contextualized Word Embeddings From a Language Model (Like BERT)'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»£ç ç¤ºä¾‹ï¼šæ¥è‡ªè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä¸Šä¸‹æ–‡åŒ–å•è¯åµŒå…¥ã€‚
- en: 'Letâ€™s look at how we can generate contextualized word embeddings, the majority
    of this code should be familiar to you by now:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ç”Ÿæˆä¸Šä¸‹æ–‡åŒ–å•è¯åµŒå…¥ï¼Œè¿™æ®µä»£ç çš„å¤§éƒ¨åˆ†ç°åœ¨åº”è¯¥å¯¹ä½ å¾ˆç†Ÿæ‚‰ï¼š
- en: '[PRE23]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code downloads a pre-trained tokenizer and model, then uses them to process
    the string â€œHello worldâ€. The output of the model is then saved in the output
    variable. Letâ€™s inspect that variable by first printing its dimensions (we expect
    it to be a multi-dimensional array).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹ï¼Œç„¶åä½¿ç”¨å®ƒä»¬å¤„ç†å­—ç¬¦ä¸²â€œHello worldâ€ã€‚æ¨¡å‹çš„è¾“å‡ºç»“æœéšåä¿å­˜åœ¨è¾“å‡ºå˜é‡ä¸­ã€‚è®©æˆ‘ä»¬é€šè¿‡é¦–å…ˆæ‰“å°å…¶ç»´åº¦æ¥æ£€æŸ¥è¯¥å˜é‡ï¼ˆæˆ‘ä»¬é¢„è®¡å®ƒæ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼‰ã€‚
- en: 'The model weâ€™re using here is called DeBERTA v3, which at the time of writing,
    is one of the best-performing language models for token embeddings while being
    small and highly efficient. It is described in the paper [DeBERTaV3: Improving
    DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding
    Sharing](https://openreview.net/forum?id=sE7-XhLxHA).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨çš„æ¨¡å‹ç§°ä¸ºDeBERTA v3ï¼Œåœ¨æ’°å†™æ—¶ï¼Œå®ƒæ˜¯è¡¨ç°æœ€ä½³çš„æ ‡è®°åµŒå…¥è¯­è¨€æ¨¡å‹ä¹‹ä¸€ï¼ŒåŒæ—¶ä½“ç§¯å°ä¸”é«˜æ•ˆã€‚å…¶è¯¦ç»†æè¿°è§è®ºæ–‡[DeBERTaV3:
    Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled
    Embedding Sharing](https://openreview.net/forum?id=sE7-XhLxHA)ã€‚'
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This prints out:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºï¼š
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can ignore the first dimension and read this as four tokens, each one embedded
    in 384 values.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¿½ç•¥ç¬¬ä¸€ç»´ï¼Œå°†å…¶è§†ä¸ºå››ä¸ªæ ‡è®°ï¼Œæ¯ä¸ªæ ‡è®°åµŒå…¥384ä¸ªå€¼ã€‚
- en: 'But what are these four vectors? Did the tokenizer break the two words into
    four tokens, or is something else happening here? We can use what weâ€™ve learned
    about tokenizers to inspect them:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å››ä¸ªå‘é‡æ˜¯ä»€ä¹ˆï¼Ÿåˆ†è¯å™¨æ˜¯å¦å°†ä¸¤ä¸ªå•è¯åˆ†è§£æˆå››ä¸ªæ ‡è®°ï¼Œè¿˜æ˜¯å‘ç”Ÿäº†å…¶ä»–æƒ…å†µï¼Ÿæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ‰€å­¦çš„åˆ†è¯å™¨çŸ¥è¯†æ¥æ£€æŸ¥å®ƒä»¬ï¼š
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Which prints out:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºï¼š
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Which shows that this particular tokenizer and model operate by adding the [CLS]
    and [SEP] tokens to the beginning and end of a string.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¡¨æ˜è¿™ä¸ªç‰¹å®šçš„åˆ†è¯å™¨å’Œæ¨¡å‹é€šè¿‡åœ¨å­—ç¬¦ä¸²çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ [CLS]å’Œ[SEP]æ ‡è®°æ¥æ“ä½œã€‚
- en: 'Our language model has now processed the text input. The result of its output
    is the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ç°åœ¨å·²ç»å¤„ç†äº†æ–‡æœ¬è¾“å…¥ã€‚å…¶è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is the raw output of a language model. The applications of large language
    models build on top of outputs like this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¯­è¨€æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨åŸºäºè¿™æ ·çš„è¾“å‡ºã€‚
- en: We can recap the input tokenization and resulting outputs of a language model
    in [FigureÂ 6-8](#fig_8__a_language_model_operates_on_raw_static_embeddi). Technically,
    the switch from token IDs into raw embeddings is the first step that happens inside
    a language model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨[å›¾6-8](#fig_8__a_language_model_operates_on_raw_static_embeddi)ä¸­å›é¡¾è¾“å…¥æ ‡è®°åŒ–å’Œè¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚æŠ€æœ¯ä¸Šï¼Œä»è¯å…ƒIDè½¬æ¢ä¸ºåŸå§‹åµŒå…¥æ˜¯è¯­è¨€æ¨¡å‹å†…éƒ¨å‘ç”Ÿçš„ç¬¬ä¸€æ­¥ã€‚
- en: '![  A language model operates on raw  static embeddings as its input and produces
    contextual text embeddings.](assets/tokens_token_embeddings_963889_08.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸€ä¸ªè¯­è¨€æ¨¡å‹åœ¨åŸå§‹é™æ€åµŒå…¥ä¸Šæ“ä½œï¼Œå¹¶ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬åµŒå…¥ã€‚](assets/tokens_token_embeddings_963889_08.png)'
- en: Figure 6-8\. A language model operates on raw, static embeddings as its input
    and produces contextual text embeddings.
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-8\. ä¸€ä¸ªè¯­è¨€æ¨¡å‹åœ¨åŸå§‹é™æ€åµŒå…¥ä¸Šæ“ä½œï¼Œå¹¶ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬åµŒå…¥ã€‚
- en: A visual like this is essential for the next chapter when we start to look at
    how Transformer-based LLMs work under the hood.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„å¯è§†åŒ–å¯¹äºä¸‹ä¸€ç« æˆ‘ä»¬å¼€å§‹ç ”ç©¶åŸºäºTransformerçš„LLMså¦‚ä½•è¿ä½œè‡³å…³é‡è¦ã€‚
- en: Word Embeddings
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥
- en: Token embeddings are useful even outside of large language models. Embeddings
    generated by pre-LLM methods like Word2Vec, Glove, and Fasttext still have uses
    in NLP and beyond NLP. In this section, weâ€™ll look at how to use pre-trained Word2Vec
    embeddings and touch on how the method creates word embeddings. Seeing how Word2Vec
    is trained will prime you for the chapter on contrastive training. Then in the
    following section, weâ€™ll see how those embeddings can be used for recommendation
    systems.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¯å…ƒåµŒå…¥å³ä½¿åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹å¤–ä¹Ÿå¾ˆæœ‰ç”¨ã€‚é€šè¿‡å¦‚Word2Vecã€Gloveå’ŒFasttextç­‰é¢„-LLMæ–¹æ³•ç”Ÿæˆçš„åµŒå…¥åœ¨NLPåŠå…¶ä¹‹å¤–ä»ç„¶æœ‰ç”¨ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„Word2VecåµŒå…¥ï¼Œå¹¶ç®€è¦ä»‹ç»è¯¥æ–¹æ³•å¦‚ä½•åˆ›å»ºè¯åµŒå…¥ã€‚äº†è§£Word2Vecçš„è®­ç»ƒè¿‡ç¨‹å°†ä¸ºåç»­å…³äºå¯¹æ¯”è®­ç»ƒçš„ç« èŠ‚åšå¥½å‡†å¤‡ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†çœ‹åˆ°è¿™äº›åµŒå…¥å¦‚ä½•ç”¨äºæ¨èç³»ç»Ÿã€‚
- en: Using Pre-trained Word Embeddings
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥
- en: Letâ€™s look at how we can download pre-trained word embeddings using the [Gensim](https://radimrehurek.com/gensim/)
    library
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨[Gensim](https://radimrehurek.com/gensim/)åº“ä¸‹è½½é¢„è®­ç»ƒçš„è¯åµŒå…¥ã€‚
- en: '[PRE29]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, weâ€™ve downloaded the embeddings of a large number of words trained on
    wikipedia. We can then explore the embedding space by seeing the nearest neighbors
    of a specific word, â€˜kingâ€™ for example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸‹è½½äº†åœ¨ç»´åŸºç™¾ç§‘ä¸Šè®­ç»ƒçš„å¤§é‡å•è¯åµŒå…¥ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ç‰¹å®šå•è¯çš„æœ€è¿‘é‚»ï¼Œæ¥æ¢ç´¢åµŒå…¥ç©ºé—´ï¼Œä¾‹å¦‚â€˜kingâ€™ï¼š
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Which outputs:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶è¾“å‡ºä¸ºï¼š
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The Word2vec Algorithm and Contrastive Training
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2vecç®—æ³•ä¸å¯¹æ¯”è®­ç»ƒ
- en: The word2vec algorithm described in the paper [Efficient Estimation of Word
    Representations in Vector Space](https://arxiv.org/abs/1301.3781) is described
    in detail in [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/).
    The central ideas are condensed here as we build on them when discussing one method
    for creating embeddings for recommendation engines in the following section.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡[é«˜æ•ˆä¼°è®¡å‘é‡ç©ºé—´ä¸­çš„è¯è¡¨ç¤º](https://arxiv.org/abs/1301.3781)ä¸­æè¿°çš„word2vecç®—æ³•åœ¨[å›¾è§£Word2vec](https://jalammar.github.io/illustrated-word2vec/)ä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚è¿™é‡Œçš„ä¸­å¿ƒæ€æƒ³åœ¨äºæˆ‘ä»¬åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºä¸ºæ¨èå¼•æ“åˆ›å»ºåµŒå…¥çš„ä¸€ç§æ–¹æ³•æ—¶è¿›è¡Œæ‰©å±•ã€‚
- en: Just like LLMs, word2vec is trained on examples generated from text. Letâ€™s say
    for example, we have the text "*Thou shalt not make a machine in the likeness
    of a human mind*" from the *Dune* novels by Frank Herbert. The algorithm uses
    a sliding window to generate training examples. We can for example have a window
    size two, meaning that we consider two neighbors on each side of a central word.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒLLMsä¸€æ ·ï¼Œword2vecæ˜¯åŸºäºä»æ–‡æœ¬ç”Ÿæˆçš„ç¤ºä¾‹è¿›è¡Œè®­ç»ƒçš„ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬æœ‰æ¥è‡ªå¼—å…°å…‹Â·èµ«ä¼¯ç‰¹çš„*æ²™ä¸˜*å°è¯´ä¸­çš„æ–‡æœ¬â€œ*ä½ ä¸å¯åˆ¶é€ ä¸äººç±»å¿ƒæ™ºç›¸ä¼¼çš„æœºå™¨*â€ã€‚è¯¥ç®—æ³•ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆè®­ç»ƒç¤ºä¾‹ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¾å®šçª—å£å¤§å°ä¸º2ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬è€ƒè™‘ä¸­å¿ƒå•è¯ä¸¤ä¾§çš„ä¸¤ä¸ªé‚»å±…ã€‚
- en: The embeddings are generated from a classification task. This task is used to
    train a neural network to predict if words appear in the same context or not.
    We can think of this as a neural network that takes two words and outputs 1 if
    they tend to appear in the same context, and 0 if they do not.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥æ˜¯ä»åˆ†ç±»ä»»åŠ¡ä¸­ç”Ÿæˆçš„ã€‚è¯¥ä»»åŠ¡ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œä»¥é¢„æµ‹å•è¯æ˜¯å¦å‡ºç°åœ¨åŒä¸€ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒæ¥å—ä¸¤ä¸ªå•è¯ï¼Œå¹¶åœ¨å®ƒä»¬å€¾å‘äºå‡ºç°åœ¨åŒä¸€ä¸Šä¸‹æ–‡æ—¶è¾“å‡º1ï¼Œè€Œåœ¨å®ƒä»¬ä¸å‡ºç°åœ¨åŒä¸€ä¸Šä¸‹æ–‡æ—¶è¾“å‡º0ã€‚
- en: In the first position for the sliding window, we can generate four training
    examples as we can see in [FigureÂ 6-9](#fig_9__a_sliding_window_is_used_to_generate_training_ex).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ»‘åŠ¨çª—å£çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå››ä¸ªè®­ç»ƒç¤ºä¾‹ï¼Œå¦‚[å›¾6-9](#fig_9__a_sliding_window_is_used_to_generate_training_ex)æ‰€ç¤ºã€‚
- en: '![  A sliding window is used to generate training examples for the word2vec
    algorithm to later predict if two words are neighbors or not.](assets/tokens_token_embeddings_963889_09.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![  ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œä»¥ä¾¿word2vecç®—æ³•åç»­é¢„æµ‹ä¸¤ä¸ªå•è¯æ˜¯å¦ä¸ºé‚»å±…ã€‚](assets/tokens_token_embeddings_963889_09.png)'
- en: Figure 6-9\. A sliding window is used to generate training examples for the
    word2vec algorithm to later predict if two words are neighbors or not.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-9\. ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œä»¥ä¾¿word2vecç®—æ³•åç»­é¢„æµ‹ä¸¤ä¸ªå•è¯æ˜¯å¦ä¸ºé‚»å±…ã€‚
- en: In each of the produced training examples, the word in the center is used as
    one input, and each of its neighbors is a distinct second input in each training
    example. We expect the final trained model to be able to classify this neighbor
    relationship and output 1 if the two input words it receives are indeed neighbors.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬ä¸­ï¼Œä¸­å¿ƒçš„å•è¯ä½œä¸ºä¸€ä¸ªè¾“å…¥ï¼Œæ¯ä¸ªé‚»å±…åœ¨æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸­éƒ½æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„ç¬¬äºŒè¾“å…¥ã€‚æˆ‘ä»¬æœŸæœ›æœ€ç»ˆè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿåˆ†ç±»è¿™ç§é‚»å±…å…³ç³»ï¼Œå¹¶åœ¨æ”¶åˆ°ç¡®å®æ˜¯é‚»å±…çš„ä¸¤ä¸ªè¾“å…¥å•è¯æ—¶è¾“å‡º1ã€‚
- en: These training examples are visualized in [FigureÂ 6-10](#fig_10__each_generated_training_example_shows_a_pair_of).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®­ç»ƒæ ·æœ¬åœ¨[å›¾6-10](#fig_10__each_generated_training_example_shows_a_pair_of)ä¸­è¿›è¡Œäº†å¯è§†åŒ–ã€‚
- en: '![  Each generated training example shows a pair of neighboring words.](assets/tokens_token_embeddings_963889_10.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![  æ¯ä¸ªç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬å±•ç¤ºäº†ä¸€å¯¹é‚»å±…å•è¯ã€‚](assets/tokens_token_embeddings_963889_10.png)'
- en: Figure 6-10\. Each generated training example shows a pair of neighboring words.
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-10\. æ¯ä¸ªç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬å±•ç¤ºäº†ä¸€å¯¹é‚»å±…å•è¯ã€‚
- en: If, however, we have a dataset of only a target value of 1, then a model can
    ace it by output 1 all the time. To get around this, we need to enrich our training
    dataset with examples of words that are not typically neighbors. These are called
    negative examples and are shown in [FigureÂ 6-11](#fig_11__we_need_to_present_our_models_with_negative_exam).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬åªæœ‰ç›®æ ‡å€¼ä¸º1çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæ¨¡å‹å¯ä»¥é€šè¿‡å§‹ç»ˆè¾“å‡º1æ¥è½»æ¾åº”å¯¹ã€‚ä¸ºäº†ç»•è¿‡è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ç”¨é€šå¸¸ä¸æ˜¯é‚»å±…çš„å•è¯ä¾‹å­ä¸°å¯Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†ã€‚è¿™äº›è¢«ç§°ä¸ºè´Ÿä¾‹ï¼Œå¦‚[å›¾6-11](#fig_11__we_need_to_present_our_models_with_negative_exam)æ‰€ç¤ºã€‚
- en: '![  We need to present our models with negative examples  words that are not
    usually neighbors. A better model is able to better distinguish between the positive
    and negative examples.](assets/tokens_token_embeddings_963889_11.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![  æˆ‘ä»¬éœ€è¦å‘æ¨¡å‹å‘ˆç°è´Ÿä¾‹ï¼šé€šå¸¸ä¸æ˜¯é‚»å±…çš„å•è¯ã€‚æ›´å¥½çš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†æ­£ä¾‹å’Œè´Ÿä¾‹ã€‚](assets/tokens_token_embeddings_963889_11.png)'
- en: 'Figure 6-11\. We need to present our models with negative examples: words that
    are not usually neighbors. A better model is able to better distinguish between
    the positive and negative examples.'
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-11\. æˆ‘ä»¬éœ€è¦å‘æ¨¡å‹å‘ˆç°è´Ÿä¾‹ï¼šé€šå¸¸ä¸æ˜¯é‚»å±…çš„å•è¯ã€‚æ›´å¥½çš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†æ­£ä¾‹å’Œè´Ÿä¾‹ã€‚
- en: 'It turns out that we donâ€™t have to be too scientific in how we choose the negative
    examples. A lot of useful models are result from simple ability to detect positive
    examples from randomly generated examples (inspired by an important idea called
    Noise Contrastive Estimation and described in [Noise-contrastive estimation: A
    new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)).
    So in this case, we get random words and add them to the dataset and indicate
    that they are not neighbors (and thus the model should output 0 when it sees them.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬åœ¨é€‰æ‹©è´Ÿä¾‹æ—¶ä¸å¿…è¿‡äºç§‘å­¦ã€‚è®¸å¤šæœ‰ç”¨çš„æ¨¡å‹æ˜¯é€šè¿‡ç®€å•åœ°ä»éšæœºç”Ÿæˆçš„ä¾‹å­ä¸­æ£€æµ‹æ­£ä¾‹çš„èƒ½åŠ›è€Œå¾—æ¥çš„ï¼ˆçµæ„Ÿæ¥è‡ªä¸€ä¸ªé‡è¦çš„æ€æƒ³ï¼Œç§°ä¸ºå™ªå£°å¯¹æ¯”ä¼°è®¡ï¼Œå¹¶åœ¨[Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)ä¸­æè¿°ï¼‰ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è·å–éšæœºå•è¯å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œå¹¶è¡¨æ˜å®ƒä»¬ä¸æ˜¯é‚»å±…ï¼ˆå› æ­¤æ¨¡å‹åœ¨çœ‹åˆ°è¿™äº›æ—¶åº”è¾“å‡º0ï¼‰ã€‚'
- en: 'With this, weâ€™ve seen two of the main concepts of word2vec ([FigureÂ 6-12](#fig_12__skipgram_and_negative_sampling_are_two_of_the_ma)):
    Skipgram - the method of selecting neighboring words and negative sampling - adding
    negative examples by random sampling from the dataset.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†word2vecçš„ä¸¤ä¸ªä¸»è¦æ¦‚å¿µï¼ˆ[å›¾6-12](#fig_12__skipgram_and_negative_sampling_are_two_of_the_ma)ï¼‰ï¼šSkipgram
    - é€‰æ‹©é‚»å±…å•è¯çš„æ–¹æ³•ï¼Œä»¥åŠè´Ÿé‡‡æ · - é€šè¿‡ä»æ•°æ®é›†ä¸­éšæœºæŠ½æ ·æ·»åŠ è´Ÿä¾‹ã€‚
- en: '![  Skipgram and Negative Sampling are two of the main ideas behind the word2vec
    algorithm and are useful in many other problems that can be formulated as token
    sequence problems.](assets/tokens_token_embeddings_963889_12.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![  Skipgramå’Œè´Ÿé‡‡æ ·æ˜¯word2vecç®—æ³•èƒŒåçš„ä¸¤ä¸ªä¸»è¦æ€æƒ³ï¼Œåœ¨è®¸å¤šå…¶ä»–å¯ä»¥è¡¨è¿°ä¸ºä»¤ç‰Œåºåˆ—é—®é¢˜çš„é—®é¢˜ä¸­ä¹Ÿéå¸¸æœ‰ç”¨ã€‚](assets/tokens_token_embeddings_963889_12.png)'
- en: Figure 6-12\. Skipgram and Negative Sampling are two of the main ideas behind
    the word2vec algorithm and are useful in many other problems that can be formulated
    as token sequence problems.
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-12\. Skipgramå’Œè´Ÿé‡‡æ ·æ˜¯word2vecç®—æ³•èƒŒåçš„ä¸¤ä¸ªä¸»è¦æ€æƒ³ï¼Œå¹¶ä¸”åœ¨è®¸å¤šå¯ä»¥è¡¨è¿°ä¸ºæ ‡è®°åºåˆ—é—®é¢˜çš„å…¶ä»–é—®é¢˜ä¸­ä¹Ÿå¾ˆæœ‰ç”¨ã€‚
- en: We can generate millions and even billions of training examples like this from
    running text. Before proceeding to train a neural network on this dataset, we
    need to make a couple of tokenization decisions, which, just like weâ€™ve seen with
    LLM tokenizers, include how to deal with capitalization and punctuation and how
    many tokens we want in our vocabulary.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œæ–‡æœ¬ç”Ÿæˆæ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿ä¸ªè®­ç»ƒç¤ºä¾‹ã€‚åœ¨å¯¹è¯¥æ•°æ®é›†è¿›è¡Œç¥ç»ç½‘ç»œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åšå‡ºä¸€äº›æ ‡è®°å†³ç­–ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨LLMæ ‡è®°å™¨ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼ŒåŒ…æ‹¬å¦‚ä½•å¤„ç†å¤§å†™å’Œæ ‡ç‚¹ï¼Œä»¥åŠæˆ‘ä»¬æƒ³è¦åœ¨è¯æ±‡è¡¨ä¸­æœ‰å¤šå°‘ä¸ªæ ‡è®°ã€‚
- en: We then create an embedding vector for each token, and randomly initialize them,
    as can be seen in [FigureÂ 6-13](#fig_13__a_vocabulary_of_words_and_their_starting_random).
    In practice, this is a matrix of dimensions vocab_size x embedding_dimensions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä¸ºæ¯ä¸ªæ ‡è®°åˆ›å»ºä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œå¹¶éšæœºåˆå§‹åŒ–å®ƒä»¬ï¼Œå¦‚[å›¾6-13](#fig_13__a_vocabulary_of_words_and_their_starting_random)æ‰€ç¤ºã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªç»´åº¦ä¸ºvocab_size
    x embedding_dimensionsçš„çŸ©é˜µã€‚
- en: '![  A vocabulary of words and their starting  random  uninitialized embedding
    vectors.](assets/tokens_token_embeddings_963889_13.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![  è¯æ±‡åŠå…¶èµ·å§‹éšæœºæœªåˆå§‹åŒ–çš„åµŒå…¥å‘é‡ã€‚](assets/tokens_token_embeddings_963889_13.png)'
- en: Figure 6-13\. A vocabulary of words and their starting, random, uninitialized
    embedding vectors.
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-13\. è¯æ±‡åŠå…¶èµ·å§‹çš„éšæœºæœªåˆå§‹åŒ–çš„åµŒå…¥å‘é‡ã€‚
- en: 'A model is then trained on each example to take in two embedding vectors and
    predict if theyâ€™re related or not. We can see what this looks like in [FigureÂ 6-14](#fig_14__a_neural_network_is_trained_to_predict_if_two_wo):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¸€ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¥æ”¶ä¸¤ä¸ªåµŒå…¥å‘é‡å¹¶é¢„æµ‹å®ƒä»¬æ˜¯å¦ç›¸å…³ã€‚æˆ‘ä»¬å¯ä»¥åœ¨[å›¾6-14](#fig_14__a_neural_network_is_trained_to_predict_if_two_wo)ä¸­çœ‹åˆ°è¿™ä¸ªè¿‡ç¨‹çš„æ ·å­ï¼š
- en: '![  A neural network is trained to predict if two words are neighbors. It updates
    the embeddings in the training process to produce the final  trained embeddings.](assets/tokens_token_embeddings_963889_14.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![  ä¸€ä¸ªç¥ç»ç½‘ç»œè¢«è®­ç»ƒæ¥é¢„æµ‹ä¸¤ä¸ªè¯æ˜¯å¦ç›¸é‚»ã€‚å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°åµŒå…¥ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒåµŒå…¥ã€‚](assets/tokens_token_embeddings_963889_14.png)'
- en: Figure 6-14\. A neural network is trained to predict if two words are neighbors.
    It updates the embeddings in the training process to produce the final, trained
    embeddings.
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-14\. ä¸€ä¸ªç¥ç»ç½‘ç»œè¢«è®­ç»ƒæ¥é¢„æµ‹ä¸¤ä¸ªè¯æ˜¯å¦ç›¸é‚»ã€‚å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°åµŒå…¥ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒåµŒå…¥ã€‚
- en: Based on whether its prediction was correct or not, the typical machine learning
    training step updates the embeddings so that the next the model is presented with
    those two vectors, it has a better chance of being more correct. And by the end
    of the training process, we have better embeddings for all the tokens in our vocabulary.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é¢„æµ‹æ˜¯å¦æ­£ç¡®ï¼Œå…¸å‹çš„æœºå™¨å­¦ä¹ è®­ç»ƒæ­¥éª¤ä¼šæ›´æ–°åµŒå…¥ï¼Œä½¿å¾—ä¸‹æ¬¡æ¨¡å‹å±•ç¤ºè¿™ä¸¤ä¸ªå‘é‡æ—¶ï¼Œæœ‰æ›´å¤§çš„æœºä¼šå˜å¾—æ›´å‡†ç¡®ã€‚åˆ°è®­ç»ƒè¿‡ç¨‹ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä¸ºè¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰æ ‡è®°æ‹¥æœ‰æ›´å¥½çš„åµŒå…¥ã€‚
- en: This idea of a model that takes two vectors and predicts if they have a certain
    relation is one of the most powerful ideas in machine learning, and time after
    time has proven to work very well with language models. This is why weâ€™re dedicating
    chapter XXX to go over this concept and how it optimizes language models for specific
    tasks (like sentence embeddings and retrieval).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¨¡å‹çš„æƒ³æ³•æ˜¯å°†ä¸¤ä¸ªå‘é‡ç»“åˆå¹¶é¢„æµ‹å®ƒä»¬æ˜¯å¦å…·æœ‰æŸç§å…³ç³»ï¼Œè¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¼ºå¤§çš„æƒ³æ³•ä¹‹ä¸€ï¼Œå¹¶ä¸”ä¸€æ¬¡åˆä¸€æ¬¡åœ°è¯æ˜åœ¨è¯­è¨€æ¨¡å‹ä¸­éå¸¸æœ‰æ•ˆã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸“é—¨ç”¨ç« èŠ‚XXXæ¥è¯¦ç»†è®¨è®ºè¿™ä¸€æ¦‚å¿µï¼Œä»¥åŠå®ƒå¦‚ä½•ä¼˜åŒ–è¯­è¨€æ¨¡å‹ä»¥åº”å¯¹ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚å¥å­åµŒå…¥å’Œæ£€ç´¢ï¼‰ã€‚
- en: The same idea is also central to bridging modalities like text and images which
    is key to AI Image generation models. In that formulation, a model is presented
    with an image and a caption, and it should predict whether that caption describes
    this image or not.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·çš„æƒ³æ³•ä¹Ÿåœ¨å¼¥åˆæ–‡æœ¬å’Œå›¾åƒç­‰æ¨¡æ€ä¹‹é—´è‡³å…³é‡è¦ï¼Œè¿™å¯¹AIå›¾åƒç”Ÿæˆæ¨¡å‹æ¥è¯´æ˜¯å…³é”®ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œç»™å®šä¸€å¼ å›¾åƒå’Œä¸€ä¸ªæ ‡é¢˜ï¼Œæ¨¡å‹åº”è¯¥é¢„æµ‹è¿™ä¸ªæ ‡é¢˜æ˜¯å¦æè¿°äº†è¿™å¼ å›¾åƒã€‚
- en: Embeddings for Recommendation Systems
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨èç³»ç»Ÿçš„åµŒå…¥
- en: The concept of token embeddings is useful in so many other domains. In industry,
    itâ€™s widely used for recommendation systems, for example.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åµŒå…¥çš„æ¦‚å¿µåœ¨è®¸å¤šå…¶ä»–é¢†åŸŸä¹Ÿéå¸¸æœ‰ç”¨ã€‚åœ¨è¡Œä¸šä¸­ï¼Œä¾‹å¦‚ï¼Œå®ƒå¹¿æ³›ç”¨äºæ¨èç³»ç»Ÿã€‚
- en: Recommending songs by embeddings
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡åµŒå…¥æ¨èæ­Œæ›²
- en: In this section weâ€™ll use the Word2vec algorithm to embed songs using human-made
    music playlists. Imagine if we treated each song as we would a word or token,
    and we treated each playlist like a sentence. These embeddings can then be used
    to recommend similar songs which often appear together in playlists.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Word2vecç®—æ³•ï¼Œé€šè¿‡äººé€ éŸ³ä¹æ’­æ”¾åˆ—è¡¨æ¥åµŒå…¥æ­Œæ›²ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬å°†æ¯é¦–æ­Œè§†ä¸ºä¸€ä¸ªå•è¯æˆ–æ ‡è®°ï¼Œè€Œå°†æ¯ä¸ªæ’­æ”¾åˆ—è¡¨è§†ä¸ºä¸€ä¸ªå¥å­ã€‚è¿™äº›åµŒå…¥å¯ä»¥ç”¨æ¥æ¨èé‚£äº›ç»å¸¸ä¸€èµ·å‡ºç°åœ¨æ’­æ”¾åˆ—è¡¨ä¸­çš„ç›¸ä¼¼æ­Œæ›²ã€‚
- en: The [dataset](https://www.cs.cornell.edu/~shuochen/lme/data_page.html) weâ€™ll
    use was collected by Shuo Chen from Cornell University. The dataset contains playlists
    from hundreds of radio stations around the US. [FigureÂ 6-15](#fig_15__for_song_embeddings_that_capture_song_similarity)
    demonstrates this dataset.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨çš„[æ•°æ®é›†](https://www.cs.cornell.edu/~shuochen/lme/data_page.html)æ˜¯ç”±åº·å¥ˆå°”å¤§å­¦çš„Shuo
    Chenæ”¶é›†çš„ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç¾å›½æ•°ç™¾ä¸ªå¹¿æ’­ç”µå°çš„æ’­æ”¾åˆ—è¡¨ã€‚[å›¾6-15](#fig_15__for_song_embeddings_that_capture_song_similarity)å±•ç¤ºäº†è¯¥æ•°æ®é›†ã€‚
- en: '![  For song embeddings that capture song similarity we ll use a dataset made
    up of a collection of playlists  each containing a list of songs.](assets/tokens_token_embeddings_963889_15.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸ºäº†æ•æ‰æ­Œæ›²ç›¸ä¼¼æ€§çš„æ­Œæ›²åµŒå…¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç”±åŒ…å«æ­Œæ›²åˆ—è¡¨çš„æ’­æ”¾åˆ—è¡¨é›†åˆç»„æˆçš„æ•°æ®é›†ã€‚](assets/tokens_token_embeddings_963889_15.png)'
- en: Figure 6-15\. For song embeddings that capture song similarity weâ€™ll use a dataset
    made up of a collection of playlists, each containing a list of songs.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-15ã€‚ä¸ºäº†æ•æ‰æ­Œæ›²ç›¸ä¼¼æ€§çš„æ­Œæ›²åµŒå…¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç”±åŒ…å«æ­Œæ›²åˆ—è¡¨çš„æ’­æ”¾åˆ—è¡¨é›†åˆç»„æˆçš„æ•°æ®é›†ã€‚
- en: Letâ€™s demonstrate the end product before we look at how itâ€™s built. So letâ€™s
    give it a few songs and see what it recommends in response.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨æŸ¥çœ‹æ„å»ºæ–¹å¼ä¹‹å‰æ¼”ç¤ºæœ€ç»ˆäº§å“ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ç»™å‡ºå‡ é¦–æ­Œæ›²ï¼Œçœ‹çœ‹å®ƒæ¨èä»€ä¹ˆã€‚
- en: 'Letâ€™s start by giving it Michael Jacksonâ€™s *Billie Jean*, the song with ID
    #3822.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ï¼Œé€‰æ‹©è¿ˆå…‹å°”Â·æ°å…‹é€Šçš„*æ¯”åˆ©Â·ç*ï¼Œæ­Œæ›²IDä¸º#3822ã€‚
- en: '[PRE32]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '| id | title | artist |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| id | æ ‡é¢˜ | è‰ºæœ¯å®¶ |'
- en: '| --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 4181 | Kiss | Prince & The Revolution |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 4181 | å» | ç‹å­ä¸é©å‘½ |'
- en: '| 12749 | Wanna Be Startinâ€™ Somethinâ€™ | Michael Jackson |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 12749 | æƒ³è¦å¼€å§‹ä¸€äº›äº‹æƒ… | è¿ˆå…‹å°”Â·æ°å…‹é€Š |'
- en: '| 1506 | The Way You Make Me Feel | Michael Jackson |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 1506 | ä½ è®©æˆ‘æ„Ÿè§‰çš„æ–¹å¼ | è¿ˆå…‹å°”Â·æ°å…‹é€Š |'
- en: '| 3396 | Holiday | Madonna |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 3396 | å‡æœŸ | éº¦å½“å¨œ |'
- en: '| 500 | Donâ€™t Stop â€˜Til You Get Enough | Michael Jackson |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 500 | ä¸åœç›´åˆ°ä½ æ»¡è¶³ | è¿ˆå…‹å°”Â·æ°å…‹é€Š |'
- en: That looks reasonable. Madonna, Prince, and other Michael Jackson songs are
    the nearest neighbors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥åˆç†ã€‚éº¦å½“å¨œã€ç‹å­å’Œå…¶ä»–è¿ˆå…‹å°”Â·æ°å…‹é€Šçš„æ­Œæ›²æ˜¯æœ€è¿‘çš„é‚»å±…ã€‚
- en: 'Letâ€™s step away from Pop and into Rap, and see the neighbors of 2Pacâ€™s California
    Love:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æµè¡ŒéŸ³ä¹è½¬å‘è¯´å”±ï¼Œçœ‹çœ‹2Pacçš„ã€ŠåŠ å·çˆ±æƒ…ã€‹çš„é‚»å±…ï¼š
- en: '[PRE33]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '| id | title | artist |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| id | æ ‡é¢˜ | è‰ºæœ¯å®¶ |'
- en: '| --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 413 | If I Ruled The World (Imagine That) (w\/ Lauryn Hill) | Nas |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 413 | å¦‚æœæˆ‘ç»Ÿæ²»ä¸–ç•Œï¼ˆæƒ³è±¡ä¸€ä¸‹ï¼‰ï¼ˆä¸åŠ³ä¼¦Â·å¸Œå°”ï¼‰ | çº³æ–¯ |'
- en: '| 196 | Iâ€™ll Be Missing You | Puff Daddy & The Family |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 196 | æˆ‘ä¼šæƒ³å¿µä½  | Puff Daddy & The Family |'
- en: '| 330 | Hate It Or Love It (w\/ 50 Cent) | The Game |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 330 | çˆ±æˆ–æ¨ï¼ˆä¸50 Centï¼‰ | The Game |'
- en: '| 211 | Hypnotize | The Notorious B.I.G. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 211 | å‚¬çœ  | çŸ¥åB.I.G. |'
- en: '| 5788 | Drop It Like Itâ€™s Hot (w\/ Pharrell) | Snoop Dogg |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 5788 | åƒçƒ­ä¸€æ ·æŠ›æ‰ï¼ˆä¸æ³•ç‘å°”ï¼‰ | snoop dogg |'
- en: Another quite reasonable list!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªç›¸å½“åˆç†çš„åˆ—è¡¨ï¼
- en: '[PRE34]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That takes a minute or two to train and results in embeddings being calculated
    for each song that we have. Now we can use those embeddings to find similar songs
    exactly as we did earlier with words.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒéœ€è¦ä¸€ä¸¤åˆ†é’Ÿï¼Œç»“æœæ˜¯ä¸ºæˆ‘ä»¬æ‹¥æœ‰çš„æ¯é¦–æ­Œè®¡ç®—åµŒå…¥ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åƒä¹‹å‰å¤„ç†å•è¯ä¸€æ ·ä½¿ç”¨è¿™äº›åµŒå…¥æ¥æ‰¾åˆ°ç›¸ä¼¼æ­Œæ›²ã€‚
- en: '[PRE35]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Which outputs:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶è¾“å‡ºä¸ºï¼š
- en: '[PRE36]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And that is the list of the songs whose embeddings are most similar to song
    2172\. See the jupyter notebook for the code that links song ids to their names
    and artist names.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸æ­Œæ›²2172ç›¸ä¼¼çš„æ­Œæ›²åµŒå…¥åˆ—è¡¨ã€‚è¯·æŸ¥çœ‹jupyterç¬”è®°æœ¬ä¸­çš„ä»£ç ï¼Œè¯¥ä»£ç å°†æ­Œæ›²IDä¸å…¶åç§°å’Œè‰ºæœ¯å®¶åç§°é“¾æ¥èµ·æ¥ã€‚
- en: 'In this case, the song is:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­Œæ›²æ˜¯ï¼š
- en: '[PRE37]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Resulting in recommendations that are all in the same heavy metal and hard
    rock genre:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ¨èéƒ½æ˜¯åœ¨åŒä¸€é‡é‡‘å±å’Œç¡¬æ‘‡æ»šç±»å‹ä¸­ï¼š
- en: '| id | title | artist |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| id | æ ‡é¢˜ | è‰ºæœ¯å®¶ |'
- en: '| --- | --- | --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 11473 | Little Guitars | Van Halen |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 11473 | å°å‰ä»– | ç“¦æ©Â·æµ·ä¼¦ |'
- en: '| 3167 | Unchained | Van Halen |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 3167 | ä¸ç¾ | ç“¦æ©Â·æµ·ä¼¦ |'
- en: '| 5586 | The Last In Line | Dio |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 5586 | æœ€åçš„è¡Œåˆ— | è¿ªå¥¥ |'
- en: '| 5634 | Mr. Brownstone | Guns Nâ€™ Roses |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 5634 | å¸ƒæœ—çŸ³å…ˆç”Ÿ | æªä¸ç«ç‘° |'
- en: '| 3094 | Breaking The Law | Judas Priest |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 3094 | è¿åæ³•å¾‹ | çŒ¶é”æ–¯Â·æ™®é‡Œæ–¯ç‰¹ |'
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we have covered LLM tokens, tokenizers, and useful approaches
    to use token embeddings beyond language models.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ¶µç›–äº†LLMä»¤ç‰Œã€åˆ†è¯å™¨ä»¥åŠä½¿ç”¨ä»¤ç‰ŒåµŒå…¥è¶…è¶Šè¯­è¨€æ¨¡å‹çš„æœ‰ç”¨æ–¹æ³•ã€‚
- en: Tokenizers are the first step in processing the input to a LLM -- turning text
    into a list of token IDs.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨æ˜¯å¤„ç†è¾“å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¬¬ä¸€æ­¥â€”â€”å°†æ–‡æœ¬è½¬æ¢ä¸ºä»¤ç‰ŒIDåˆ—è¡¨ã€‚
- en: Some of the common tokenization schemes include breaking text down into words,
    subword tokens, characters, or bytes
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€äº›å¸¸è§çš„åˆ†è¯æ–¹æ¡ˆåŒ…æ‹¬å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ã€å­è¯æ ‡è®°ã€å­—ç¬¦æˆ–å­—èŠ‚ã€‚
- en: A tour of real-world pre-trained tokenizers (from BERT to GPT2, GPT4, and other
    models) showed us areas where some tokenizers are better (e.g., preserving information
    like capitalization, new lines, or tokens in other languages) and other areas
    where tokenizers are just different from each other (e.g., how they break down
    certain words).
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°å®ä¸–ç•Œé¢„è®­ç»ƒåˆ†è¯å™¨çš„å·¡è§ˆï¼ˆä»BERTåˆ°GPT2ã€GPT4åŠå…¶ä»–æ¨¡å‹ï¼‰å‘æˆ‘ä»¬å±•ç¤ºäº†æŸäº›åˆ†è¯å™¨åœ¨ä¿æŒä¿¡æ¯ï¼ˆå¦‚å¤§å†™ã€æ–°è¡Œæˆ–å…¶ä»–è¯­è¨€çš„æ ‡è®°ï¼‰æ–¹é¢è¡¨ç°æ›´ä½³çš„é¢†åŸŸï¼Œä»¥åŠåœ¨æŸäº›æ–¹é¢åˆ†è¯å™¨ä¹‹é—´çš„ä¸åŒä¹‹å¤„ï¼ˆä¾‹å¦‚ï¼Œå®ƒä»¬å¦‚ä½•æ‹†åˆ†æŸäº›è¯ï¼‰ã€‚
- en: Three of the major tokenizer design decisions are the tokenizer algorithm (e.g.,
    BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary
    size, special tokens, capitalization, treatment of capitalization and different
    languages), and the dataset the tokenizer is trained on.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªä¸»è¦çš„åˆ†è¯å™¨è®¾è®¡å†³ç­–æ˜¯åˆ†è¯å™¨ç®—æ³•ï¼ˆå¦‚BPEã€WordPieceã€SentencePieceï¼‰ã€åˆ†è¯å‚æ•°ï¼ˆåŒ…æ‹¬è¯æ±‡è¡¨å¤§å°ã€ç‰¹æ®Šæ ‡è®°ã€å¤§å†™å¤„ç†ã€å¯¹å¤§å†™å’Œä¸åŒè¯­è¨€çš„å¤„ç†ï¼‰ï¼Œä»¥åŠåˆ†è¯å™¨è®­ç»ƒæ‰€ç”¨çš„æ•°æ®é›†ã€‚
- en: Language models are also creators of high-quality contextualized token embeddings
    that improve on raw static embeddings. Those contextualized token embeddings are
    whatâ€™s used for tasks including NER, extractive text summarization, and span classification.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯é«˜è´¨é‡ä¸Šä¸‹æ–‡åŒ–è¯åµŒå…¥çš„åˆ›é€ è€…ï¼Œè¿™äº›è¯åµŒå…¥åœ¨é™æ€åµŒå…¥çš„åŸºç¡€ä¸Šå¾—åˆ°äº†æ”¹å–„ã€‚è¿™äº›ä¸Šä¸‹æ–‡åŒ–è¯åµŒå…¥ç”¨äºåŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€æŠ½å–å¼æ–‡æœ¬æ‘˜è¦å’Œè·¨åº¦åˆ†ç±»ç­‰ä»»åŠ¡ã€‚
- en: Before LLMs, word embedding methods like word2vec, Glove and Fasttext were popular.
    They still have some use cases within and outside of language processing.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡ºç°ä¹‹å‰ï¼Œè¯åµŒå…¥æ–¹æ³•å¦‚word2vecã€Gloveå’ŒFasttextæ›¾å¾ˆæµè¡Œã€‚å®ƒä»¬åœ¨è¯­è¨€å¤„ç†å†…å¤–ä»ç„¶æœ‰ä¸€äº›åº”ç”¨åœºæ™¯ã€‚
- en: 'The Word2Vec algorithm relies on two main ideas: Skipgram and Negative Sampling.
    It also uses contrastive training similar to the one weâ€™ll see in the contrastive
    training chapter.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vecç®—æ³•ä¾èµ–äºä¸¤ä¸ªä¸»è¦æ€æƒ³ï¼šSkipgramå’Œè´Ÿé‡‡æ ·ã€‚å®ƒè¿˜ä½¿ç”¨äº†å¯¹æ¯”è®­ç»ƒï¼Œç±»ä¼¼äºæˆ‘ä»¬å°†åœ¨å¯¹æ¯”è®­ç»ƒç« èŠ‚ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚
- en: Token embeddings are useful for creating and improving recommender systems as
    weâ€™ve seen in the music recommender weâ€™ve built from curated song playlists.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥å¯¹åˆ›å»ºå’Œæ”¹è¿›æ¨èç³»ç»Ÿéå¸¸æœ‰ç”¨ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æ„å»ºçš„åŸºäºç²¾é€‰æ­Œæ›²æ’­æ”¾åˆ—è¡¨çš„éŸ³ä¹æ¨èç³»ç»Ÿä¸­æ‰€çœ‹åˆ°çš„é‚£æ ·ã€‚
