- en: Chapter 5\. Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 决策树
- en: '*Decision trees* are versatile machine learning algorithms that can perform
    both classification and regression tasks, and even multioutput tasks. They are
    powerful algorithms, capable of fitting complex datasets. For example, in [Chapter 2](ch02.html#project_chapter)
    you trained a `DecisionTreeRegressor` model on the California housing dataset,
    fitting it perfectly (actually, overfitting it).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是通用的机器学习算法，可以执行分类和回归任务，甚至多输出任务。它们是强大的算法，能够拟合复杂的数据集。例如，在第2章中，您在加利福尼亚住房数据集上训练了一个`DecisionTreeRegressor`模型，并完美地（实际上，过度拟合）地拟合了它。'
- en: Decision trees are also the fundamental components of random forests (see [Chapter 6](ch06.html#ensembles_chapter)),
    which are among the most powerful machine learning algorithms available today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也是随机森林（参见第6章）的基本组成部分，随机森林是目前最强大的机器学习算法之一。
- en: In this chapter we will start by discussing how to train, visualize, and make
    predictions with decision trees. Then we will go through the CART training algorithm
    used by Scikit-Learn, and we will explore how to regularize trees and use them
    for regression tasks. Finally, we will discuss some of the limitations of decision
    trees.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论如何训练、可视化和使用决策树进行预测。然后我们将介绍Scikit-Learn使用的CART训练算法，并探讨如何正则化树以及如何将它们用于回归任务。最后，我们将讨论决策树的一些局限性。
- en: Training and Visualizing a Decision Tree
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化决策树
- en: 'To understand decision trees, let’s build one and take a look at how it makes
    predictions. The following code trains a `DecisionTreeClassifier` on the iris
    dataset (see [Chapter 4](ch04.html#linear_models_chapter)):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解决策树，让我们先构建一个，并看看它是如何进行预测的。以下代码在鸢尾花数据集上训练了一个`DecisionTreeClassifier`（参见第4章）：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can visualize the trained decision tree by first using the `export_graphviz()`
    function to output a graph definition file called *iris_tree.dot*:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过首先使用`export_graphviz()`函数输出一个名为*iris_tree.dot*的图形定义文件来可视化训练好的决策树：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then you can use `graphviz.Source.from_file()` to load and display the file
    in a Jupyter notebook:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`graphviz.Source.from_file()`在Jupyter笔记本中加载并显示该文件：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Graphviz](https://graphviz.org) is an open source graph visualization software
    package. It also includes a `dot` command-line tool to convert *.dot* files to
    a variety of formats, such as PDF or PNG.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Graphviz](https://graphviz.org)是一个开源的图形可视化软件包。它还包括一个`dot`命令行工具，可以将*.dot*文件转换为多种格式，如PDF或PNG。'
- en: Your first decision tree looks like [Figure 5-1](#iris_tree).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个决策树看起来像[图5-1](#iris_tree)。
- en: '![A diagram of a decision tree for classifying iris species based on petal
    length and width, showing split nodes and leaf nodes with classification results
    for setosa, versicolor, and virginica.](assets/hmls_0501.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![基于花瓣长度和宽度的鸢尾花物种分类决策树图，显示分割节点和叶节点以及分类结果（setosa、versicolor和virginica）](assets/hmls_0501.png)'
- en: Figure 5-1\. Iris decision tree
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 鸢尾花决策树
- en: Making Predictions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'Let’s see how the tree represented in [Figure 5-1](#iris_tree) makes predictions.
    Suppose you find an iris flower and you want to classify it based on its petals.
    You start at the *root node* (depth 0, at the top): this node asks whether the
    flower’s petal length is smaller than 2.45 cm. If it is, then you move down to
    the root’s left child node (depth 1, left). In this case, it is a *leaf node*
    (i.e., it does not have any child nodes), so it does not ask any questions: simply
    look at the predicted class for that node, and the decision tree predicts that
    your flower is an *Iris setosa* (`class=setosa`).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[图5-1](#iris_tree)中表示的树是如何进行预测的。假设您发现了一朵鸢尾花，并想根据其花瓣对其进行分类。您从*根节点*（深度0，顶部）开始：此节点询问花朵的花瓣长度是否小于2.45厘米。如果是，那么您将移动到根的左子节点（深度1，左）。在这种情况下，它是一个*叶节点*（即它没有子节点），所以它不提出任何问题：只需查看该节点的预测类别，决策树预测您的花朵是*Iris
    setosa*（类别=setosa）。
- en: 'Now suppose you find another flower, and this time the petal length is greater
    than 2.45 cm. You again start at the root but now move down to its right child
    node (depth 1, right). This is not a leaf node, it’s a *split node*, so it asks
    another question: is the petal width smaller than 1.75 cm? If it is, then your
    flower is most likely an *Iris versicolor* (depth 2, left). If not, it is likely
    an *Iris virginica* (depth 2, right). It’s really that simple.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你找到了另一朵花，这次花瓣长度大于 2.45 厘米。你再次从根节点开始，但这次移动到它的右子节点（深度 1，右）。这不是一个叶子节点，它是一个
    *分裂节点*，因此它会问另一个问题：花瓣宽度是否小于 1.75 厘米？如果是，那么你的花很可能是 *Iris versicolor*（深度 2，左）。如果不是，它很可能是
    *Iris virginica*（深度 2，右）。这真的很简单。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One of the many qualities of decision trees is that they require very little
    data preparation. In fact, they don’t require feature scaling or centering at
    all.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树众多优点之一是它们对数据准备的要求非常低。实际上，它们根本不需要特征缩放或中心化。
- en: 'A node’s `samples` attribute counts how many training instances it applies
    to. For example, 100 training instances have a petal length greater than 2.45
    cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75
    cm (depth 2, left). A node’s `value` attribute tells you how many training instances
    of each class this node applies to: for example, the bottom-right node applies
    to 0 *Iris setosa*, 1 *Iris versicolor*, and 45 *Iris virginica*. Finally, a node’s
    `gini` attribute measures its *Gini impurity*: a node is “pure” (`gini=0`) if
    all training instances it applies to belong to the same class. For example, since
    the depth-1 left node applies only to *Iris setosa* training instances, its Gini
    impurity is 0\. Conversely, the other nodes all apply to instances of multiple
    classes, so they are “impure”. [Equation 5-1](#gini_impurity) shows how the training
    algorithm computes the Gini impurity *G*[*i*] of the *i*^(th) node. The more classes
    and the more mixed they are, the larger the impurity. For example, the depth-2
    left node has a Gini impurity equal to 1 – (0/54)² – (49/54)² – (5/54)² ≈ 0.168.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的 `samples` 属性统计它应用了多少个训练实例。例如，100 个训练实例的花瓣长度大于 2.45 厘米（深度 1，右），其中 100 个中有
    54 个花瓣宽度小于 1.75 厘米（深度 2，左）。节点的 `value` 属性告诉你这个节点应用了多少个每个类别的训练实例：例如，右下角的节点应用了 0
    个 *Iris setosa*，1 个 *Iris versicolor* 和 45 个 *Iris virginica*。最后，节点的 `gini` 属性衡量其
    *吉尼不纯度*：如果一个节点应用的所有训练实例都属于同一类别，则该节点是“纯净的”（`gini=0`）。例如，由于深度 1 的左节点只应用了 *Iris setosa*
    的训练实例，其吉尼不纯度为 0。相反，其他节点都应用了多个类别的实例，因此它们是“不纯净的”。[方程式 5-1](#gini_impurity) 展示了训练算法如何计算第
    *i* 个节点的吉尼不纯度 *G*[*i*]。类别越多，混合程度越高，不纯度就越大。例如，深度 2 的左节点的吉尼不纯度等于 1 – (0/54)² – (49/54)²
    – (5/54)² ≈ 0.168。
- en: Equation 5-1\. Gini impurity
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5-1\. 吉尼不纯度
- en: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals
    1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2$
- en: 'In this equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*G*[*i*] is the Gini impurity of the *i*^(th) node.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*G*[*i*] 是第 *i* 个节点的吉尼不纯度。'
- en: '*p*[*i*,*k*] is the ratio of class *k* instances among the training instances
    in the *i*^(th) node.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*[*i*,*k*] 是第 *i* 个节点中训练实例中类别 *k* 的比例。'
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Scikit-Learn uses the CART algorithm (discussed shortly), which produces only
    *binary trees*, meaning trees where split nodes always have exactly two children
    (i.e., questions only have yes/no answers). However, other algorithms, such as
    ID3, can produce decision trees with nodes that have more than two children.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 使用 CART 算法（稍后讨论），它只产生 *二叉树*，这意味着分裂节点总是有两个子节点（即问题只有是/否的答案）。然而，其他算法，如
    ID3，可以产生具有超过两个子节点的决策树。
- en: '[Figure 5-2](#decision_tree_decision_boundaries_plot) shows this decision tree’s
    decision boundaries. The thick vertical line represents the decision boundary
    of the root node (depth 0): petal length = 2.45 cm. Since the lefthand area is
    pure (only *Iris setosa*), it cannot be split any further. However, the righthand
    area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented
    by the dashed line). Since `max_depth` was set to 2, the decision tree stops right
    there. If you set `max_depth` to 3, then the two depth-2 nodes would each add
    another decision boundary (represented by the two vertical dotted lines).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-2](#decision_tree_decision_boundaries_plot)展示了这个决策树的决策边界。粗垂直线代表根节点的决策边界（深度0）：花瓣长度=2.45厘米。由于左侧区域是纯的（只有*Iris
    setosa*），因此不能再进一步分割。然而，右侧区域是不纯的，因此深度1的右节点在花瓣宽度=1.75厘米处将其分割（由虚线表示）。由于`max_depth`被设置为2，决策树就在这里停止。如果你将`max_depth`设置为3，那么两个深度2的节点将各自添加另一个决策边界（由两条垂直虚线表示）。'
- en: '![Diagram illustrating decision tree decision boundaries with depth levels
    and data points for different Iris flower species.](assets/hmls_0502.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图解决策树决策边界，包括深度级别和不同Iris花种的数据点。](assets/hmls_0502.png)'
- en: Figure 5-2\. Decision tree decision boundaries
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 决策树决策边界
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The tree structure, including all the information shown in [Figure 5-1](#iris_tree),
    is available via the classifier’s `tree_` attribute. Type **`help(tree_clf.tree_)`**
    for details, and see [this chapter’s notebook](https://homl.info/colab-p) for
    an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树结构，包括[图5-1](#iris_tree)中显示的所有信息，都可通过分类器的`tree_`属性获取。输入**`help(tree_clf.tree_)`**获取详细信息，并查看[本章的笔记本](https://homl.info/colab-p)以获取示例。
- en: Estimating Class Probabilities
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计类概率
- en: 'A decision tree can also estimate the probability that an instance belongs
    to a particular class *k*. First it traverses the tree to find the leaf node for
    this instance, and then it returns the proportion of instances of class *k* among
    the training instances that would also reach this leaf node. For example, suppose
    you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding
    leaf node is the depth-2 left node, so the decision tree outputs the following
    probabilities: 0% for *Iris setosa* (0/54), 90.7% for *Iris versicolor* (49/54),
    and 9.3% for *Iris virginica* (5/54). And if you ask it to predict the class,
    it outputs *Iris versicolor* (class 1) because it has the highest probability.
    Let’s check this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树还可以估计一个实例属于特定类*k*的概率。首先它遍历树以找到该实例的叶节点，然后返回在训练实例中达到该叶节点的类*k*实例的比例。例如，假设你发现一朵花瓣长度为5厘米、宽度为1.5厘米的花。相应的叶节点是深度2的左节点，因此决策树输出以下概率：*Iris
    setosa*为0%（0/54），*Iris versicolor*为90.7%（49/54），*Iris virginica*为9.3%（5/54）。如果你要求它预测类别，它输出*Iris
    versicolor*（类别1），因为它具有最高的概率。让我们检查一下：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Perfect! Notice that the estimated probabilities would be identical anywhere
    else in the bottom-right rectangle of [Figure 5-2](#decision_tree_decision_boundaries_plot)—for
    example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious
    that it would most likely be an *Iris virginica* in this case).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！注意，估计的概率在任何其他[图5-2](#decision_tree_decision_boundaries_plot)右下角的矩形中都是相同的——例如，如果花瓣长度为6厘米、宽度为1.5厘米（尽管在这种情况下它最可能是*Iris
    virginica*）。
- en: The CART Training Algorithm
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CART训练算法
- en: Scikit-Learn uses the *Classification and Regression Tree* (CART) algorithm
    to train decision trees (also called “growing” trees). The algorithm works by
    first splitting the training set into two subsets using a single feature *k* and
    a threshold *t*[*k*] (e.g., “petal length ≤ 2.45 cm”). How does it choose *k*
    and *t*[*k*]? It searches for the pair (*k*, *t*[*k*]) that produces the purest
    subsets, weighted by their size. [Equation 5-2](#classification_cart_cost_function)
    gives the cost function that the algorithm tries to minimize.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn使用**分类和回归树**（CART）算法来训练决策树（也称为“生长”树）。该算法通过首先使用单个特征*k*和阈值*t*[*k*]（例如，“花瓣长度≤2.45厘米”）将训练集分割成两个子集来工作。它是如何选择*k*和*t*[*k*]的呢？它搜索产生最纯子集的(*k*,
    *t*[*k*])对，并按其大小进行加权。[方程5-2](#classification_cart_cost_function)给出了算法试图最小化的成本函数。
- en: Equation 5-2\. CART cost function for classification
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程5-2\. 分类CART成本函数
- en: $StartLayout 1st Row 1st Column upper J left-parenthesis k comma t Subscript
    k Baseline right-parenthesis 2nd Column equals StartFraction m Subscript left
    Baseline Over m EndFraction upper G Subscript left Baseline plus StartFraction
    m Subscript right Baseline Over m EndFraction upper G Subscript right Baseline
    2nd Row 1st Column where 2nd Column StartLayout Enlarged left-brace 1st Row  upper
    G Subscript left slash right Baseline measures the impurity of the left slash
    right subset 2nd Row  m Subscript left slash right Baseline is the number of instances
    in the left slash right subset 3rd Row  m equals m Subscript left Baseline plus
    m Subscript right EndLayout EndLayout$
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column upper J left-parenthesis k comma t Subscript
    k Baseline right-parenthesis 2nd Column equals StartFraction m Subscript left
    Baseline Over m EndFraction upper G Subscript left Baseline plus StartFraction
    m Subscript right Baseline Over m EndFraction upper G Subscript right Baseline
    2nd Row 1st Column where 2nd Column StartLayout Enlarged left-brace 1st Row  upper
    G Subscript left slash right Baseline measures the impurity of the left slash
    right subset 2nd Row  m Subscript left slash right Baseline is the number of instances
    in the left slash right subset 3rd Row  m equals m Subscript left Baseline plus
    m Subscript right EndLayout EndLayout$
- en: 'Once the CART algorithm has successfully split the training set in two, it
    splits the subsets using the same logic, then the sub-subsets, and so on, recursively.
    It stops recursing once it reaches the maximum depth (defined by the `max_depth`
    hyperparameter), or if it cannot find a split that will reduce impurity. A few
    other hyperparameters (described in a moment) control additional stopping conditions:
    `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, and more.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 CART 算法成功地将训练集分成两部分，它就会使用相同的逻辑分割子集，然后是子子集，依此类推，递归地进行。一旦达到最大深度（由 `max_depth`
    超参数定义），或者找不到可以减少纯度的分割，它就会停止递归。一些其他超参数（稍后描述）控制额外的停止条件：`min_samples_split`、`min_samples_leaf`、`max_leaf_nodes`
    以及更多。
- en: Warning
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'As you can see, the CART algorithm is a *greedy algorithm*: it greedily searches
    for an optimum split at the top level, then repeats the process at each subsequent
    level. It does not check whether the split will lead to the lowest possible impurity
    several levels down. A greedy algorithm often produces a solution that’s reasonably
    good but not guaranteed to be optimal.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，CART 算法是一种 *贪婪算法*：它在顶层贪婪地搜索最优分割，然后在每个后续级别重复此过程。它不会检查分割是否会在几层以下导致最低可能的纯度。贪婪算法通常会产生一个相当好的解决方案，但并不保证是最优的。
- en: Unfortunately, finding the optimal tree is known to be an *NP-complete* problem.⁠^([1](ch05.html#id1673))
    It requires *O*(exp(*m*)) time,⁠^([2](ch05.html#id1674)) making the problem intractable
    even for small training sets. This is why we must settle for a “reasonably good”
    solution when training decision trees.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，找到最优树是一个已知的 *NP 完全问题*。⁠^([1](ch05.html#id1673)) 它需要 *O*(exp(*m*)) 的时间，⁠^([2](ch05.html#id1674))
    即使对于小型训练集，这个问题也是不可行的。这就是为什么我们在训练决策树时必须满足“相当好”的解决方案。
- en: Computational Complexity
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算复杂性
- en: Making predictions requires traversing the decision tree from the root to a
    leaf. Decision trees are generally approximately balanced, so traversing the decision
    tree requires going through roughly *O*(log[2](*m*)) nodes, where *m* is the number
    of training instances, and log[2](*m*) is the *binary logarithm* of *m*, equal
    to log(*m*) / log(2). Since each node only requires checking the value of one
    feature, the overall prediction complexity is *O*(log[2](*m*)), independent of
    the number of features. So predictions are very fast, even when dealing with large
    training sets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 进行预测需要从根节点遍历决策树到叶子节点。决策树通常是近似平衡的，因此遍历决策树需要通过大约 *O*(log[2](*m*)) 个节点，其中 *m* 是训练实例的数量，log[2](*m*)
    是 *m* 的 *二进制对数*，等于 log(*m*) / log(2)。由于每个节点只需要检查一个特征值，因此整体预测复杂度为 *O*(log[2](*m*))，与特征数量无关。因此，即使处理大型训练集，预测也非常快。
- en: By default, the training algorithm compares all features on all samples at each
    node, which results in a training complexity of *O*(*n* × *m* log[2](*m*)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练算法在每个节点上比较所有样本的所有特征，这导致训练复杂度为 *O*(*n* × *m* log[2](*m*))。
- en: It’s possible to set a maximum tree depth using the `max_depth` hyperparameter,
    and/or set a maximum number of features to consider at each node (the features
    are then chosen randomly). Doing so will help speed up training considerably,
    and it can also reduce the risk of overfitting (but as always, going too far would
    result in underfitting).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `max_depth` 超参数设置最大树深度，并且/或者设置在每个节点上要考虑的最大特征数（然后特征会被随机选择）。这样做将大大加快训练速度，并且还可以减少过拟合的风险（但就像往常一样，做得太过分会导致欠拟合）。
- en: Gini Impurity or Entropy?
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gini 纯度或熵？
- en: 'By default, the `DecisionTreeClassifier` class uses the Gini impurity measure,
    but you can select the *entropy* impurity measure instead by setting the `criterion`
    hyperparameter to `"entropy"`. The concept of entropy originated in thermodynamics
    as a measure of molecular disorder: entropy approaches zero when molecules are
    still and well ordered. Entropy later spread to a wide variety of domains, including
    in Shannon’s information theory, where it measures the average information content
    of a message, as we saw in [Chapter 4](ch04.html#linear_models_chapter). Entropy
    is zero when all messages are identical. In machine learning, entropy is frequently
    used as an impurity measure: a set’s entropy is zero when it contains instances
    of only one class. [Equation 5-3](#entropy_function) shows the definition of the
    entropy of the *i*^(th) node. For example, the depth-2 left node in [Figure 5-1](#iris_tree)
    has an entropy equal to –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`DecisionTreeClassifier` 类使用基尼不纯度度量，但你可以通过将 `criterion` 超参数设置为 `"entropy"`
    来选择 *熵* 不纯度度量。熵的概念起源于热力学，作为分子无序度的度量：当分子静止且有序时，熵趋近于零。熵后来扩展到广泛的领域，包括在香农的信息理论中，它衡量信息的平均内容，正如我们在[第
    4 章](ch04.html#linear_models_chapter)中看到的。当所有消息都相同的时候，熵为零。在机器学习中，熵经常被用作不纯度度量：当集合只包含一个类的实例时，其熵为零。[方程式
    5-3](#entropy_function) 展示了第 *i* 个节点的熵的定义。例如，[图 5-1](#iris_tree) 中的深度 2 左节点具有的熵等于
    –(49/54) log[2] (49/54) – (5/54) log[2] (5/54) ≈ 0.445。
- en: Equation 5-3\. Entropy
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5-3\. 熵
- en: <mrow><msub><mi>H</mi> <mi>i</mi></msub> <mo>=</mo> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>≠</mo><mn>0</mn></mrow></mfrac>
    <mi>n</mi></munderover> <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mrow>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>H</mi> <mi>i</mi></msub> <mo>=</mo> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mrow><msub><mi>p</mi>
    <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>≠</mo><mn>0</mn></mrow></mfrac>
    <mi>n</mi></munderover> <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mrow>
- en: 'So, should you use Gini impurity or entropy? The truth is, most of the time
    it does not make a big difference: they lead to similar trees. Gini impurity is
    slightly faster to compute, so it is a good default. However, when they differ,
    Gini impurity tends to isolate the most frequent class in its own branch of the
    tree, while entropy tends to produce slightly more balanced trees.⁠^([3](ch05.html#id1691))'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该使用基尼不纯度还是熵？事实是，大多数时候这并没有太大的区别：它们会导致类似的树。基尼不纯度计算得稍微快一些，所以它是一个好的默认选项。然而，当它们不同时，基尼不纯度倾向于将最频繁的类隔离在树的分支中，而熵倾向于产生稍微更平衡的树。[3](ch05.html#id1691)
- en: Regularization Hyperparameters
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化超参数
- en: Decision trees make very few assumptions about the training data (as opposed
    to linear models, which assume that the data is linear, for example). If left
    unconstrained, the tree structure will adapt itself to the training data, fitting
    it very closely—indeed, most likely overfitting it. Such a model is often called
    a *nonparametric model*, not because it does not have any parameters (it often
    has a lot) but because the number of parameters is not determined prior to training,
    so the model structure is free to stick closely to the data. In contrast, a *parametric
    model*, such as a linear model, has a predetermined number of parameters, so its
    degree of freedom is limited, reducing the risk of overfitting (but increasing
    the risk of underfitting).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对训练数据几乎没有假设（与假设数据是线性的线性模型相反）。如果不受约束，树结构将适应训练数据，非常紧密地拟合它——实际上，很可能会过度拟合。这样的模型通常被称为
    *非参数模型*，并不是因为它没有任何参数（它通常有很多）而是因为参数的数量在训练之前没有确定，因此模型结构可以自由地紧密跟随数据。相比之下，*参数模型*，如线性模型，具有预定的参数数量，因此其自由度有限，降低了过度拟合的风险（但增加了欠拟合的风险）。
- en: To avoid overfitting the training data, you need to restrict the decision tree’s
    freedom during training. As you know by now, this is called regularization. The
    regularization hyperparameters depend on the algorithm used, but generally you
    can at least restrict the maximum depth of the decision tree. In Scikit-Learn,
    this is controlled by the `max_depth` hyperparameter. The default value is `None`,
    which means unlimited. Reducing `max_depth` will regularize the model and thus
    reduce the risk of overfitting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合训练数据，您需要在训练过程中限制决策树的自由度。如您现在所知，这被称为正则化。正则化超参数取决于所使用的算法，但通常您至少可以限制决策树的最大深度。在
    Scikit-Learn 中，这由 `max_depth` 超参数控制。默认值是 `None`，表示无限制。减少 `max_depth` 将正则化模型并因此降低过拟合的风险。
- en: 'The `DecisionTreeClassifier` class has a few other parameters that similarly
    restrict the shape of the decision tree:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 类有几个其他参数，这些参数以类似的方式限制决策树的结构：'
- en: '`max_features`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: Maximum number of features that are evaluated for splitting at each node
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点上评估用于分裂的最大特征数
- en: '`max_leaf_nodes`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`'
- en: Maximum number of leaf nodes
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最大叶子节点数
- en: '`min_samples_split`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_split`'
- en: Minimum number of samples a node must have before it can be split
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 节点在可以分裂之前必须拥有的最小样本数
- en: '`min_samples_leaf`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`'
- en: Minimum number of samples a leaf node must have to be created
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 叶节点必须拥有的最小样本数以创建节点
- en: '`min_weight_fraction_leaf`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_weight_fraction_leaf`'
- en: Same as `min_samples_leaf` but expressed as a fraction of the total number of
    weighted instances
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `min_samples_leaf` 相同，但以总加权实例数的分数表示
- en: '`min_impurity_decrease`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_impurity_decrease`'
- en: Only split a node if this split results in at least this reduction in impurity
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当这个分裂导致至少这种不纯度减少时才分裂节点
- en: '`ccp_alpha`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`ccp_alpha`'
- en: Controls *minimal cost-complexity pruning* (MCCP), i.e., pruning subtrees that
    don’t reduce impurity enough compared to their number of leaves; a larger `ccp_alpha`
    value leads to more pruning, resulting in a smaller tree (the default is 0—no
    pruning)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 控制最小成本复杂度剪枝（MCCP），即剪枝那些与叶子节点数量相比不足以减少不纯度的子树；更大的 `ccp_alpha` 值会导致更多的剪枝，从而得到更小的树（默认值为
    0—无剪枝）
- en: 'To limit the model’s complexity and thereby regularize the model, you can increase
    `min_*` hyperparameters or `ccp_alpha`, or decrease `max_*` hyperparameters. Tuning
    `max_depth` is usually a good default: it provides effective regularization, and
    it keeps the tree small and easy to interpret. Setting `min_samples_leaf` is also
    a good idea, especially for small datasets. And `max_features` is great when working
    with high-dimensional datasets.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制模型的复杂度并因此正则化模型，您可以增加 `min_*` 超参数或 `ccp_alpha`，或减少 `max_*` 超参数。调整 `max_depth`
    通常是一个好的默认值：它提供了有效的正则化，并保持树小且易于解释。设置 `min_samples_leaf` 也是一个好主意，特别是对于小数据集。当处理高维数据集时，`max_features`
    也是很好的选择。
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Other algorithms work by first training the decision tree without restrictions,
    then *pruning* (deleting) unnecessary nodes. A node whose children are all leaf
    nodes is considered unnecessary if the purity improvement it provides is not statistically
    significant. Standard statistical tests, such as the *χ*² *test* (chi-squared
    test), are used to estimate the probability that the improvement is purely the
    result of chance (which is called the *null hypothesis*). If this probability,
    called the *p-value*, is higher than a given threshold (typically 5%, controlled
    by a hyperparameter), then the node is considered unnecessary and its children
    are deleted. The pruning continues until all unnecessary nodes have been pruned.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法首先在不加限制的情况下训练决策树，然后进行 *剪枝*（删除）不必要的节点。如果一个节点的所有子节点都是叶子节点，并且它提供的纯度改进在统计上不显著，则该节点被认为是多余的。标准统计测试，如
    *χ*² *测试*（卡方测试），用于估计改进纯粹是偶然结果的可能性（这被称为 *零假设*）。如果这个概率，称为 *p-value*，高于给定的阈值（通常为
    5%，由超参数控制），则该节点被认为是多余的，并且其子节点被删除。剪枝会继续进行，直到所有不必要的节点都被剪枝。
- en: 'Let’s test regularization on the moons dataset: this is a toy dataset for binary
    classification in which the data points are shaped as two interleaving crescent
    moons (see [Figure 5-3](#min_samples_leaf_plot)). You can generate this dataset
    using the `make_moons()` function.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在月亮数据集上测试正则化：这是一个用于二元分类的玩具数据集，其中数据点形状为两个交错的新月（见[图 5-3](#min_samples_leaf_plot)）。您可以使用
    `make_moons()` 函数生成此数据集。
- en: 'We’ll train one decision tree without regularization, and another with `min_samples_leaf=5`.
    Here’s the code; [Figure 5-3](#min_samples_leaf_plot) shows the decision boundaries
    of each tree:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个不带正则化的决策树，另一个带有 `min_samples_leaf=5`。以下是代码；[图 5-3](#min_samples_leaf_plot)
    显示了每个树的决策边界：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Comparison of decision boundaries: the left diagram shows an unregularized
    decision tree''s complex boundaries, while the right depicts a regularized tree
    with smoother boundaries indicating potentially better generalization.](assets/hmls_0503.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![决策边界的比较：左侧图显示了未正则化的决策树的复杂边界，而右侧图描述了一个具有更平滑边界的正则化树，这表明可能具有更好的泛化能力。](assets/hmls_0503.png)'
- en: Figure 5-3\. Decision boundaries of an unregularized tree (left) and a regularized
    tree (right)
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 未正则化树（左侧）和正则化树（右侧）的决策边界
- en: 'The unregularized model on the left is clearly overfitting, and the regularized
    model on the right will probably generalize better. We can verify this by evaluating
    both trees on a test set generated using a different random seed:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的未正则化模型显然是过拟合的，而右侧的正则化模型可能会泛化得更好。我们可以通过使用不同的随机种子生成的测试集来评估这两棵树来验证这一点：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Indeed, the second tree has a better accuracy on the test set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，第二个树在测试集上的准确度更好。
- en: Regression
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: 'Decision trees are also capable of performing regression tasks. While linear
    regression only works well with linear data, decision trees can fit all sorts
    of complex datasets. Let’s build a regression tree using Scikit-Learn’s `DecisionTreeRegressor`
    class, training it on a noisy quadratic dataset with `max_depth=2`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也擅长执行回归任务。虽然线性回归只适用于线性数据，但决策树可以拟合各种复杂的数据集。让我们使用 Scikit-Learn 的 `DecisionTreeRegressor`
    类构建一个回归树，并在具有 `max_depth=2` 的嘈杂二次数据集上对其进行训练：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The resulting tree is represented in [Figure 5-4](#regression_tree).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果树在[图 5-4](#regression_tree)中表示。
- en: '![A decision tree diagram for regression, showing how input variables are split
    at different nodes with squared error, sample size, and predicted value at each
    node.](assets/hmls_0504.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![回归决策树图，展示了输入变量如何在不同的节点处根据平方误差、样本大小和每个节点的预测值进行分割。](assets/hmls_0504.png)'
- en: Figure 5-4\. A decision tree for regression
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 回归决策树
- en: This tree looks very similar to the classification tree you built earlier. The
    main difference is that instead of predicting a class in each node, it predicts
    a value. For example, suppose you want to make a prediction for a new instance
    with *x*[1] = 0.2\. The root node asks whether *x*[1] ≤ 0.343\. Since it is, the
    algorithm goes to the left child node, which asks whether *x*[1] ≤ –0.302\. Since
    it is not, the algorithm goes to the right child node. This is a leaf node, and
    it predicts `value=0.038`. This prediction is the average target value of the
    133 training instances associated with this leaf node, and it results in a mean
    squared error equal to 0.002 over these 133 instances.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树看起来与您之前构建的分类树非常相似。主要区别在于，在每个节点中，它预测的是一个值，而不是一个类别。例如，假设您想对一个新实例进行预测，其中 *x*[1]
    = 0.2。根节点询问 *x*[1] ≤ 0.343。由于它是，算法转到左侧子节点，该节点询问 *x*[1] ≤ –0.302。由于它不是，算法转到右侧子节点。这是一个叶节点，它预测
    `value=0.038`。这个预测是该叶节点关联的 133 个训练实例的平均目标值，并且在这些 133 个实例上产生均方误差等于 0.002。
- en: This model’s predictions are represented on the left in [Figure 5-5](#tree_regression_plot).
    If you set `max_depth=3`, you get the predictions represented on the right. Notice
    how the predicted value for each region is always the average target value of
    the instances in that region. The algorithm splits each region in a way that makes
    most training instances as close as possible to that predicted value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的预测结果在[图 5-5](#tree_regression_plot)的左侧表示。如果你设置`max_depth=3`，你将得到右侧表示的预测结果。注意，每个区域的预测值总是该区域实例的平均目标值。算法以使大多数训练实例尽可能接近该预测值的方式分割每个区域。
- en: '![Two plots show decision tree regression predictions: the left with max depth
    2 having simpler splits, and the right with max depth 3 showing more detailed
    splits.](assets/hmls_0505.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![两个图表显示了决策树回归预测：左侧具有最大深度 2，具有更简单的分割，右侧具有最大深度 3，显示了更详细的分割。](assets/hmls_0505.png)'
- en: Figure 5-5\. Predictions of two decision tree regression models
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 两个决策树回归模型的预测结果
- en: The CART algorithm works as described earlier, except that instead of trying
    to split the training set in a way that minimizes impurity, it now tries to split
    the training set in a way that minimizes the MSE. [Equation 5-4](#regression_cart_cost_function)
    shows the cost function that the algorithm tries to minimize.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CART算法的工作原理如前所述，只是现在它不是试图以最小化不纯度的方式分割训练集，而是试图以最小化均方误差（MSE）的方式分割训练集。[公式5-4](#regression_cart_cost_function)显示了算法试图最小化的成本函数。
- en: Equation 5-4\. CART cost function for regression
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式5-4\. 回归的CART成本函数
- en: $upper J left-parenthesis k comma t Subscript k Baseline right-parenthesis equals
    StartFraction m Subscript left Baseline Over m EndFraction MSE Subscript left
    Baseline plus StartFraction m Subscript right Baseline Over m EndFraction MSE
    Subscript right Baseline where StartLayout Enlarged left-brace 1st Row  MSE Subscript
    node Baseline equals StartFraction sigma-summation Underscript i element-of node
    Endscripts left-parenthesis ModifyingAbove y With caret Subscript node Baseline
    minus y Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    squared Over m Subscript node Baseline EndFraction 2nd Row  ModifyingAbove y With
    caret Subscript node Baseline equals StartFraction sigma-summation Underscript
    i element-of node Endscripts y Superscript left-parenthesis i right-parenthesis
    Baseline Over m Subscript node Baseline EndFraction EndLayout$
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $upper J left-parenthesis k comma t Subscript k Baseline right-parenthesis equals
    StartFraction m Subscript left Baseline Over m EndFraction MSE Subscript left
    Baseline plus StartFraction m Subscript right Baseline Over m EndFraction MSE
    Subscript right Baseline where StartLayout Enlarged left-brace 1st Row  MSE Subscript
    node Baseline equals StartFraction sigma-summation Underscript i element-of node
    Endscripts left-parenthesis ModifyingAbove y With caret Subscript node Baseline
    minus y Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    squared Over m Subscript node Baseline EndFraction 2nd Row  ModifyingAbove y With
    caret Subscript node Baseline equals StartFraction sigma-summation Underscript
    i element-of node Endscripts y Superscript left-parenthesis i right-parenthesis
    Baseline Over m Subscript node Baseline EndFraction EndLayout$
- en: Just like for classification tasks, decision trees are prone to overfitting
    when dealing with regression tasks. Without any regularization (i.e., using the
    default hyperparameters), you get the predictions on the left in [Figure 5-6](#tree_regression_regularization_plot).
    These predictions are obviously overfitting the training set very badly. Just
    setting `min_samples_leaf=10` results in a much more reasonable model, represented
    on the right in [Figure 5-6](#tree_regression_regularization_plot).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如对于分类任务一样，决策树在处理回归任务时容易过拟合。在没有任何正则化（即使用默认的超参数）的情况下，你会在[图5-6](#tree_regression_regularization_plot)的左侧得到预测。这些预测显然非常糟糕地过拟合了训练集。只需设置`min_samples_leaf=10`就会得到一个更加合理的模型，如[图5-6](#tree_regression_regularization_plot)右侧所示。
- en: '![Comparison of regression tree predictions showing overfitting with no restrictions
    and improved fit with `min_samples_leaf=10`.](assets/hmls_0506.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![比较回归树预测，显示无限制时的过拟合和`min_samples_leaf=10`时的改进拟合。](assets/hmls_0506.png)'
- en: Figure 5-6\. Predictions of an unregularized regression tree (left) and a regularized
    tree (right)
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 未正则化的回归树（左）和正则化树（右）的预测
- en: Sensitivity to Axis Orientation
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对轴方向的敏感性
- en: 'Hopefully by now you are convinced that decision trees have a lot going for
    them: they are relatively easy to understand and interpret, simple to use, versatile,
    and powerful. However, they do have a few limitations. First, as you may have
    noticed, decision trees love orthogonal decision boundaries (all splits are perpendicular
    to an axis), which makes them sensitive to the data’s orientation. For example,
    [Figure 5-7](#sensitivity_to_rotation_plot) shows a simple linearly separable
    dataset: on the left, a decision tree can split it easily, while on the right,
    after the dataset is rotated by 45°, the decision boundary looks unnecessarily
    convoluted. Although both decision trees fit the training set perfectly, it is
    very likely that the model on the right will not generalize well.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在你已经相信决策树有很多优点：它们相对容易理解和解释，使用简单，功能多样，且强大。然而，它们确实有一些局限性。首先，正如你可能已经注意到的，决策树喜欢正交的决策边界（所有分割都垂直于一个轴），这使得它们对数据的方向敏感。例如，[图5-7](#sensitivity_to_rotation_plot)显示了一个简单的线性可分数据集：在左侧，决策树可以轻松地分割它，而在右侧，数据集旋转45°后，决策边界看起来不必要地复杂。尽管两个决策树都完美地拟合了训练集，但右边的模型很可能无法很好地泛化。
- en: '![Diagram showing decision trees'' sensitivity to data orientation, with a
    clear boundary before rotation and a complex boundary after rotation by 45 degrees.](assets/hmls_0507.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![展示决策树对数据方向的敏感性的图，旋转前有清晰的边界，旋转45度后边界变得复杂。](assets/hmls_0507.png)'
- en: Figure 5-7\. Sensitivity to training set rotation
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. 对训练集旋转的敏感性
- en: One way to limit this problem is to scale the data, then apply a principal component
    analysis (PCA) transformation. We will look at PCA in detail in [Chapter 7](ch07.html#dimensionality_chapter),
    but for now you only need to know that it rotates the data in a way that reduces
    the correlation between the features, which often (not always) makes things easier
    for trees.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 限制这种问题的一种方法是对数据进行缩放，然后应用主成分分析 (PCA) 转换。我们将在[第 7 章](ch07.html#dimensionality_chapter)中详细讨论
    PCA，但就目前而言，您只需要知道它会以减少特征之间相关性的方式旋转数据，这通常（但不总是）使事情更容易。
- en: 'Let’s create a small pipeline that scales the data and rotates it using PCA,
    then train a `DecisionTreeClassifier` on that data. [Figure 5-8](#pca_preprocessing_plot)
    shows the decision boundaries of that tree: as you can see, the rotation makes
    it possible to fit the dataset pretty well using only one feature, *z*[1], which
    is a linear function of the original petal length and width. Here’s the code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小的管道，使用 PCA 缩放数据并旋转它，然后在数据上训练一个 `DecisionTreeClassifier`。[图 5-8](#pca_preprocessing_plot)
    显示了该树的决策边界：如您所见，旋转使得仅使用一个特征*z*[1]，即原始花瓣长度和宽度的线性函数，就可以很好地拟合数据集。以下是代码：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `DecisionTreeClassifier` and `DecisionTreeRegressor` classes both support
    missing values natively, no need for an imputer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 和 `DecisionTreeRegressor` 类都原生支持缺失值，无需使用填充器。'
- en: '![Diagram illustrating decision boundaries of a decision tree on the scaled
    and PCA-rotated iris dataset, with separate regions for Iris setosa, versicolor,
    and virginica.](assets/hmls_0508.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图解决策树在缩放和 PCA-旋转的鸢尾花数据集上的决策边界，分别表示 Iris setosa、versicolor 和 virginica 的区域](assets/hmls_0508.png)'
- en: Figure 5-8\. A tree’s decision boundaries on the scaled and PCA-rotated iris
    dataset
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. 树在缩放和 PCA-旋转的鸢尾花数据集上的决策边界
- en: Decision Trees Have a High Variance
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树具有高方差
- en: 'More generally, the main issue with decision trees is that they have quite
    a high variance: small changes to the hyperparameters or to the data may produce
    very different models. In fact, since the training algorithm used by Scikit-Learn
    is stochastic—it randomly selects the set of features to evaluate at each node—even
    retraining the same decision tree on the exact same data may produce a very different
    model, such as the one represented in [Figure 5-9](#decision_tree_high_variance_plot)
    (unless you set the `random_state` hyperparameter). As you can see, it looks very
    different from the previous decision tree ([Figure 5-2](#decision_tree_decision_boundaries_plot)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，决策树的主要问题是它们具有相当高的方差：超参数或数据的微小变化可能会产生非常不同的模型。实际上，由于 Scikit-Learn 使用的训练算法是随机的——它在每个节点随机选择要评估的特征集——即使在完全相同的数据上重新训练相同的决策树，也可能产生一个非常不同的模型，如[图
    5-9](#decision_tree_high_variance_plot) 中所示（除非您设置了 `random_state` 超参数）。如您所见，它与之前的决策树([图
    5-2](#decision_tree_decision_boundaries_plot))非常不同。
- en: '![Diagram showing decision boundaries for petal width and length in a decision
    tree, illustrating high variance due to changes in depth and classification regions.](assets/hmls_0509.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图解决策树中花瓣宽度和长度的决策边界，说明由于深度和分类区域的变化导致的高方差](assets/hmls_0509.png)'
- en: Figure 5-9\. Retraining the same model on the same data may produce a very different
    model
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 在相同的数据上重新训练相同的模型可能产生一个非常不同的模型
- en: Luckily, by averaging predictions over many trees, it’s possible to reduce variance
    significantly. Such an *ensemble* of trees is called a *random forest*, and it’s
    one of the most powerful types of models available today, as you will see in the
    next chapter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过平均许多树的预测，可以显著降低方差。这种树的*集成*被称为*随机森林*，它是今天最强大的模型类型之一，您将在下一章中看到。
- en: Exercises
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is the approximate depth of a decision tree trained (without restrictions)
    on a training set with one million instances?
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个包含一百万个实例的训练集上训练的决策树的近似深度是多少？
- en: Is a node’s Gini impurity generally lower or higher than its parent’s? Is it
    *generally* lower/higher, or *always* lower/higher?
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点的 Gini 不纯度通常比其父节点低还是高？是*通常*低/高，还是*总是*低/高？
- en: If a decision tree is overfitting the training set, is it a good idea to try
    decreasing `max_depth`?
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果决策树过度拟合训练集，尝试减少 `max_depth` 是否是一个好主意？
- en: If a decision tree is underfitting the training set, is it a good idea to try
    scaling the input features?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果决策树未能很好地拟合训练集，尝试缩放输入特征是否是一个好主意？
- en: 'If it takes one hour to train a decision tree on a training set containing
    one million instances, roughly how much time will it take to train another decision
    tree on a training set containing ten million instances? Hint: consider the CART
    algorithm’s computational complexity.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在一个包含一百万个实例的训练集上训练一个决策树需要一个小时，那么在一个包含一千万个实例的训练集上训练另一个决策树大约需要多少时间？提示：考虑 CART
    算法的计算复杂度。
- en: If it takes one hour to train a decision tree on a given training set, roughly
    how much time will it take if you double the number of features?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在一个给定的训练集上训练一个决策树需要一个小时，那么如果你将特征数量加倍，大约需要多少时间？
- en: 'Train and fine-tune a decision tree for the moons dataset by following these
    steps:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤训练并微调 moons 数据集的决策树：
- en: Use `make_moons(n_samples=10000, noise=0.4)` to generate a moons dataset.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `make_moons(n_samples=10000, noise=0.4)` 生成 moons 数据集。
- en: Use `train_test_split()` to split the dataset into a training set and a test
    set.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_test_split()` 将数据集拆分为训练集和测试集。
- en: 'Use grid search with cross-validation (with the help of the `GridSearchCV`
    class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint:
    try various values for `max_leaf_nodes`.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索和交叉验证（借助 `GridSearchCV` 类）来找到 `DecisionTreeClassifier` 的良好超参数值。提示：尝试 `max_leaf_nodes`
    的各种值。
- en: Train it on the full training set using these hyperparameters, and measure your
    model’s performance on the test set. You should get roughly 85% to 87% accuracy.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些超参数在完整训练集上训练它，并测量你的模型在测试集上的性能。你应该得到大约85%到87%的准确率。
- en: 'Grow a forest by following these steps:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤生长一个森林：
- en: 'Continuing the previous exercise, generate 1,000 subsets of the training set,
    each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s
    `ShuffleSplit` class for this.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续上一个练习，生成1,000个训练集的子集，每个子集包含随机选择的100个实例。提示：你可以使用Scikit-Learn的`ShuffleSplit`类来做这个。
- en: Train one decision tree on each subset, using the best hyperparameter values
    found in the previous exercise. Evaluate these 1,000 decision trees on the test
    set. Since they were trained on smaller sets, these decision trees will likely
    perform worse than the first decision tree, achieving only about 80% accuracy.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个子集上训练一个决策树，使用在上一个练习中找到的最佳超参数值。在测试集上评估这1,000个决策树。由于它们是在较小的集合上训练的，这些决策树可能表现不如第一个决策树，只能达到大约80%的准确率。
- en: Now comes the magic. For each test set instance, generate the predictions of
    the 1,000 decision trees, and keep only the most frequent prediction (you can
    use SciPy’s `mode()` function for this). This approach gives you *majority-vote
    predictions* over the test set.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是魔法时刻。对于每个测试集实例，生成1,000个决策树的预测，并只保留最频繁的预测（你可以使用SciPy的`mode()`函数来做这个）。这种方法在测试集上给出了*多数投票预测*。
- en: 'Evaluate these predictions on the test set: you should obtain a slightly higher
    accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you
    have trained a random forest classifier!'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估这些预测：你应该获得比你的第一个模型略高的准确率（大约高0.5到1.5%）。恭喜你，你已经训练了一个随机森林分类器！
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: ^([1](ch05.html#id1673-marker)) P is the set of problems that can be solved
    in *polynomial time* (i.e., a polynomial of the dataset size). NP is the set of
    problems whose solutions can be verified in polynomial time. An NP-hard problem
    is a problem that can be reduced to a known NP-hard problem in polynomial time.
    An NP-complete problem is both NP and NP-hard. A major open mathematical question
    is whether P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm
    will ever be found for any NP-complete problem (except perhaps one day on a quantum
    computer).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#id1673-marker)) P 是可以在*多项式时间内*解决的问题的集合（即，数据集大小的多项式）。NP 是那些解决方案可以在多项式时间内验证的问题的集合。一个
    NP-hard 问题是可以被在多项式时间内减少到一个已知的 NP-hard 问题的问题。一个 NP-complete 问题既是 NP 也是 NP-hard。一个主要的开放数学问题是
    P = NP。如果 P ≠ NP（这似乎很可能是这样），那么将永远不会找到任何 NP-complete 问题的多项式算法（也许有一天在量子计算机上除外）。
- en: ^([2](ch05.html#id1674-marker)) This *big O notation* means that as *m* (i.e.,
    the number of training instances) gets larger, the computation time becomes proportional
    to the exponential of *m* (it’s actually an upper bound, but we make it as small
    as we can). This tells us how “fast” the computation grows with *m*, and *O*(exp(*m*))
    is very fast.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#id1674-marker)) 这个**大O符号**表示，随着*m*（即训练实例的数量）的增加，计算时间与*m*的指数成正比（实际上是一个上界，但我们尽量使其尽可能小）。这告诉我们计算如何随着*m*的增长而“增长”，而*O*(exp(*m*))增长非常快。
- en: ^([3](ch05.html#id1691-marker)) See Sebastian Raschka’s [interesting analysis](https://homl.info/19)
    for more details.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#id1691-marker)) 更多细节请参阅Sebastian Raschka的[有趣分析](https://homl.info/19)。
