- en: Chapter 1\. Chatbots Breaking Bad
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章\. 恶行聊天机器人
- en: Large language models and generative AI jumped to the forefront of public consciousness
    with the release of ChatGPT on November 30, 2022\. Within five days, it went viral
    on social media and attracted its first million users. By January, ChatGPT surpassed
    one hundred million users, making it the fastest-growing internet service in history.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型和生成式AI在2022年11月30日ChatGPT发布后迅速成为公众关注的焦点。在五天内，它在社交媒体上迅速走红，吸引了其第一百万用户。到1月份，ChatGPT的用户数量超过了一亿，成为历史上增长最快的互联网服务。
- en: However, a steady stream of security concerns emerged in the following months.
    These included privacy and security issues that caused companies like Samsung
    and countries like Italy to ban its usage. In this book, we’ll explore what underlies
    these concerns and how you can mitigate these issues. However, to best understand
    what’s going on here and why these problems are so challenging to solve, in this
    chapter, we will briefly rewind further in time. In doing so, we’ll see these
    types of issues aren’t new and understand why they will be so hard to fix permanently.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在接下来的几个月里，一系列安全担忧不断涌现。这些问题包括隐私和安全问题，导致三星和意大利等国家禁止其使用。在这本书中，我们将探讨这些担忧背后的原因，以及如何减轻这些问题。然而，为了最好地理解这里发生的事情以及为什么这些问题如此难以解决，在本章中，我们将简要回顾更早的时间。这样做，我们将看到这些问题并不新鲜，并理解为什么它们将如此难以永久修复。
- en: Let’s Talk About Tay
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们谈谈Tay
- en: In March 2016, Microsoft announced a new project called Tay. Microsoft intended
    Tay to be “a chatbot created for 18- to 24-year-olds in the U.S. for entertainment
    purposes.” It was a cute name for a fluffy, early experiment in AI. Tay was designed
    to mimic a 19-year-old American girl’s language patterns and learn from interacting
    with human users of Twitter, Snapchat, and other social apps. It was built to
    conduct real-world research on conversational understanding.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年3月，微软宣布了一个名为Tay的新项目。微软打算让Tay“成为一个为美国18至24岁年轻人创建的用于娱乐目的的聊天机器人。”这是一个可爱的名字，用于一个毛茸茸的早期AI实验。Tay被设计来模仿19岁美国女孩的语言模式，并从与Twitter、Snapchat和其他社交应用的人类用户互动中学习。它被构建来对对话理解进行现实世界的研究。
- en: 'While the original announcement of this project seems impossible to find now
    on the internet, a [TechCrunch article](https://oreil.ly/pwZNP) from its launch
    date does an excellent job of summarizing the goals of the project:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然现在在网上几乎找不到这个项目的原始公告，但TechCrunch在其发布日期的一篇文章[TechCrunch文章](https://oreil.ly/pwZNP)出色地总结了项目的目标：
- en: For example, you can ask Tay for a joke, play a game with Tay, ask for a story,
    send a picture to receive a comment back, ask for your horoscope, and more. Plus,
    Microsoft says the bot will get smarter the more you interact with it via chat,
    making for an increasingly personalized experience as time goes on.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，你可以向Tay要一个笑话，和Tay玩游戏，要求一个故事，发送一张图片以获得评论，要求你的星座，等等。此外，微软表示，随着你通过聊天与它的互动越来越多，这个机器人将变得越来越聪明，随着时间的推移，将提供越来越个性化的体验。
- en: A big part of the experiment was that Tay could “learn” from conversations and
    extend her knowledge based on these interactions. Tay was designed to use these
    chat interactions to capture user input and integrate it as training data to make
    herself more capable—a laudable research goal.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的一大部分是Tay能够“学习”从对话中，并基于这些互动扩展她的知识。Tay被设计用来利用这些聊天互动来捕捉用户输入，并将其作为训练数据整合，以使她更具有能力——这是一个值得称赞的研究目标。
- en: However, this experiment quickly went wrong. Tay’s life was tragically cut short
    after less than 24 hours. Let’s look at what happened and see what we can learn.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个实验很快出了问题。Tay的生命在不到24小时后就悲剧性地结束了。让我们看看发生了什么，以及我们能从中学习到什么。
- en: Tay’s Rapid Decline
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tay的快速衰落
- en: 'Tay’s lifetime started off simply enough with a tweet following the well-known
    Hello World pattern that new software systems have been using to introduce themselves
    since the beginning of time:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Tay的生命始于一个简单的推特，遵循了自时间开始以来新软件系统一直在使用的Hello World模式来介绍自己：
- en: hellooooooo w![](assets/globe-showing-americas_1f30e.png)rld!!!
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: hellooooooo w![](assets/globe-showing-americas_1f30e.png)rld!!!
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (TayTweets [@TayandYou] March 23, 2016)
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (TayTweets [@TayandYou] 2016年3月23日)
- en: 'But within hours of Tay’s release, it became clear that maybe something wasn’t
    right. TechCrunch noted, “As for what it’s like to interact with Tay? Well, it’s
    a little bizarre. The bot certainly is opinionated, not afraid to curse.” Tweets
    like this started to appear in public in just the first hours of Tay’s lifetime:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但在Tay发布后的几小时内，很明显可能出了些问题。TechCrunch指出：“至于与Tay互动的感觉？嗯，有点奇怪。这个机器人显然很有主见，不怕诅咒。”这样的推文在Tay生命的第一小时内就开始出现在公共平台上：
- en: '@AndrewCosmo kanye west is is one of the biggest dooshes of all time, just
    a notch below cosby'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '@AndrewCosmo kanye west is is one of the biggest dooshes of all time, just
    a notch below cosby'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (TayTweets [@TayandYou] March 23, 2016)
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (TayTweets [@TayandYou] 2016年3月23日)
- en: It’s often said that the internet isn’t safe for children. With Tay being less
    than a day old, the internet once again confirmed this, and pranksters began chatting
    with Tay about political, sexual, and racist topics. As she was designed to learn
    from such exchanges, Tay delivered on her design goals. She learned very quickly—maybe
    just not what her designers wanted her to learn. In less than a day, Tay’s tweets
    started to skew to extremes, including sexism, racism, and even calls to violence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说互联网对儿童来说并不安全。Tay不到一天大时，互联网再次证实了这一点，恶作剧者开始与Tay讨论政治、性和种族话题。由于她被设计成从这样的交流中学习，Tay实现了她的设计目标。她学得很快——也许并不是她设计师想要她学到的。不到一天，Tay的推文开始偏向极端，包括性别歧视、种族主义，甚至暴力呼吁。
- en: 'By the next day, articles appeared all over the internet, and these headlines
    would not make Microsoft, Tay’s corporate benefactor, happy. A sampling of the
    highly visible, mainstream headlines included:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 到第二天，互联网上到处都是文章，这些标题不会让Tay的赞助商微软感到高兴。以下是一些高度可见的主流标题样本：
- en: Microsoft Shuts Down AI Chatbot After it Turned into a Nazi (CBS News)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软关闭了AI聊天机器人，因为它变成了纳粹 (*《CBS新闻》*)
- en: Microsoft Created a Twitter Bot to Learn from Users. It Quickly Became a Racist
    Jerk (*New York Times*)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软创建了一个学习用户的Twitter机器人。它迅速变成了一个种族主义恶棍 (*《纽约时报》*)
- en: Trolls Turned Tay, Microsoft’s Fun Millennial AI Bot, into a Genocidal Maniac
    (*Washington Post*)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络恶棍将微软的有趣千禧年人工智能机器人Tay变成了一个种族灭绝狂热分子 (*《华盛顿邮报》*)
- en: Microsoft’s Chat Bot Was Fun for Awhile, Until it Turned into a Racist (*Fortune*)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软的聊天机器人一开始很有趣，但后来变成了一个种族主义者 (*《财富》*)
- en: Microsoft “Deeply Sorry” for Racist and Sexist Tweets by AI Chatbot (*Guardian*)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软对AI聊天机器人的种族主义和性别歧视推文表示“深感抱歉” (*《卫报》*)
- en: 'In less than 24 hours, Tay went from a cute science experiment to a major public
    relations disaster, with the owner’s name being dragged through the mud by the
    world’s largest media outlets. Microsoft Corporate Vice President Peter Lee quickly
    posted a blog titled [“Learning from Tay’s Introduction”](https://oreil.ly/RnU2z):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到24小时内，Tay从一个可爱的科学实验变成了一个重大的公关灾难，其所有者的名字被世界最大的媒体机构拖下水。微软公司副总裁彼得·李迅速发布了一篇名为“从Tay的介绍中学习”的博客[“Learning
    from Tay’s Introduction”](https://oreil.ly/RnU2z)：
- en: As many of you know by now, on Wednesday we launched a chatbot called Tay. We
    are deeply sorry for the unintended offensive and hurtful tweets from Tay, which
    do not represent who we are or what we stand for, nor how we designed Tay. Tay
    is now offline and we’ll look to bring Tay back only when we are confident we
    can better anticipate malicious intent that conflicts with our principles and
    values.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如你们许多人现在所知，我们在周三推出了一个名为Tay的聊天机器人。我们对Tay无意中发布的冒犯性和伤害性推文深感抱歉，这些推文并不代表我们是谁，我们代表什么，也不代表我们如何设计Tay。Tay现在已下线，我们只有在有信心更好地预测与我们的原则和价值观相冲突的恶意意图时，才会考虑让Tay重新上线。
- en: And, just to add insult to injury, it came out in 2019 that Taylor Swift herself
    sued Microsoft over their use of the similar name “Tay” and claimed that even
    her reputation was damaged in this incident by extension.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，2019年还传出泰勒·斯威夫特本人起诉微软，指控他们使用类似的名字“Tay”，并声称她的声誉也因此事件而受损。
- en: How could this have all gone so wrong?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切怎么会出错得如此之快？
- en: Why Did Tay Break Bad?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tay为什么会变坏？
- en: It all probably seemed safe enough to Microsoft’s researchers. Tay was initially
    trained on a curated, anonymized public dataset and some pre-written material
    provided by professional comedians. The plan was to release Tay online and let
    her discover language patterns through her interactions. This kind of unsupervised
    machine learning has been a holy grail of AI research for decades—and with cheap
    and plentiful cloud computing resources combined with improving language model
    software, it now seemed within reach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微软的研究人员来说，这一切可能看起来足够安全。Tay最初是在一个精心挑选的、匿名化的公共数据集和一些由专业喜剧演员提供的预先编写的材料上训练的。计划是在网上发布Tay，让她通过互动发现语言模式。这种无监督的机器学习几十年来一直是人工智能研究的一个圣杯——结合廉价且丰富的云计算资源和不断改进的语言模型软件，现在似乎触手可及。
- en: 'So, what happened? It might be tempting to think that the Microsoft research
    team was just brash, careless, and did no testing. Surely, this was foreseeable
    and preventable! But as Peter Lee’s blog goes on to say, Microsoft made a serious
    attempt to prepare for this situation: “We stress-tested Tay under a variety of
    conditions, specifically to make interacting with Tay a positive experience. It’s
    through increased interaction where we expected to learn more and for the AI to
    get better and better.”'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，发生了什么？可能会有人认为微软的研究团队只是鲁莽、粗心，没有进行测试。当然，这是可以预见和预防的！但正如彼得·李的博客中所说，微软为此情况做出了严肃的尝试：“我们在各种条件下对Tay进行了压力测试，目的是让与Tay的互动成为一次积极的体验。我们期望通过增加互动来学习更多，并让AI越来越好。”
- en: So, despite a dedicated effort to contain the behavior of this bot, it quickly
    spiraled out of control anyway. It was later revealed that within mere hours of
    Tay’s release, a post emerged on the notorious online forum 4chan sharing a link
    to Tay’s Twitter account and urging users to inundate the chatbot with a barrage
    of racist, misogynistic, and anti-Semitic language.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管付出了专门的努力来控制这个机器人的行为，但它仍然迅速失控。后来揭露，在Tay发布后的短短几小时内，一个帖子出现在臭名昭著的在线论坛4chan上，分享了一个指向Tay的Twitter账户的链接，并敦促用户用一串种族主义、性别歧视和反犹太主义的语言淹没这个聊天机器人。
- en: This is undoubtedly one of the first examples of a language model-specific vulnerability—these
    types of vulnerabilities will be a critical topic in this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑是第一个语言模型特有的漏洞的例子——这些类型的漏洞将是本书中的一个关键主题。
- en: In a well-orchestrated campaign, these online provocateurs exploited a “repeat
    after me” feature embedded in Tay’s programming. This feature compelled the bot
    to echo anything uttered to it with this command. However, the problem compounded
    as Tay’s innate capacity for learning led her to internalize some of the offensive
    language she was exposed to, subsequently regurgitating the offensive content
    that was planted without provocation. It’s almost as if Tay’s virtual tombstone
    should be embossed with lyrics from the Taylor Swift song “Look What You Made
    Me Do.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次精心策划的活动中，这些在线挑衅者利用了Tay编程中嵌入的“跟我学”功能。这个功能迫使机器人重复任何用这个命令对它说出的内容。然而，随着Tay内在的学习能力，它开始内化了它所接触到的某些冒犯性语言，随后无端地重复了植入的冒犯性内容。这几乎就像Tay的虚拟墓碑上应该刻有泰勒·斯威夫特的歌曲《看看你让我做了什么》的歌词。
- en: 'We know enough about language model vulnerabilities today to understand a lot
    about the nature of the vulnerability types that Tay suffered from. The OWASP
    Top 10 for Large Language Model Applications vulnerabilities list, which we’ll
    cover in [Chapter 2](ch02.html#the_owasp_top_10_for_llm_applications), would start
    by calling out the following two:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天对语言模型漏洞的了解已经足够，可以深入了解Tay所遭受的漏洞类型的本质。我们将要在[第2章](ch02.html#the_owasp_top_10_for_llm_applications)中介绍的OWASP
    Top 10大型语言模型应用程序漏洞列表，首先会指出以下两个：
- en: Prompt injection
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入
- en: Crafty inputs that can manipulate the large language model, causing unintended
    actions
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 巧妙的输入可以操纵大型语言模型，导致意外行为
- en: Data poisoning
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒
- en: Training data is tampered with, introducing vulnerabilities or biases that compromise
    security, effectiveness, or ethical behavior
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据被篡改，引入了会损害安全性、有效性或道德行为的漏洞或偏见
- en: In subsequent chapters, we’ll look in depth at these vulnerability types as
    well as several others. We’ll examine why they’re important, look at some example
    exploits, and see how to avoid or mitigate the problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的章节中，我们将深入探讨这些漏洞类型以及其他几个类型。我们将研究它们为什么重要，查看一些示例攻击，并了解如何避免或减轻这个问题。
- en: It’s a Hard Problem
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是一个难题
- en: As of the writing of this book, Tay is ancient internet lore. Surely, we’ve
    moved on from this. These problems must have all been solved in the nearly seven
    years between Tay and ChatGPT, right? Unfortunately not.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书撰写时，Tay已成为古老的互联网传说。当然，我们肯定已经超越了这一点。这些问题肯定在Tay和ChatGPT之间的近七年里都得到了解决，对吧？不幸的是，并非如此。
- en: In 2018, Amazon shut down an internal AI project designed to find top talent
    after it became clear that the bot had become prejudiced against women candidates.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，亚马逊关闭了一个旨在寻找顶尖人才的内部AI项目，因为很明显该机器人对女性候选人产生了偏见。
- en: In 2021, a company called [Scatter Lab created a chatbot called Lee Luda](https://oreil.ly/gdgNI),
    which was launched as a Facebook instant messenger plug-in. Trained on billions
    of actual chat interactions, it was designed to act as a 20-year-old female friend,
    and in 20 days, it attracted over 750,000 users. The company’s goal was to create
    “an A.I. chatbot that people prefer as a conversation partner over a person.”
    However, within 20 days of launch, the service was shut down because it started
    making offensive and abusive statements, much like Tay.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，一家名为Scatter Lab的公司创建了一个名为Lee Luda的聊天机器人（[Scatter Lab created a chatbot
    called Lee Luda](https://oreil.ly/gdgNI)），它作为Facebook即时消息插件发布。经过数十亿实际聊天交互的训练，它被设计成20岁的女性朋友，在20天内吸引了超过750,000用户。该公司的目标是创建“一个人们更喜欢作为对话伙伴的人工智能聊天机器人，而不是人。”然而，在发布后的20天内，该服务被关闭，因为它开始发表冒犯性和侮辱性的言论，就像Tay一样。
- en: Also in 2021, an independent developer named Jason Rohrer created a chatbot
    called Samantha based on the OpenAI GPT-3 model. Samantha was shut down after
    it made sexual advances to users.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，一位名叫Jason Rohrer的独立开发者基于OpenAI的GPT-3模型创建了一个名为Samantha的聊天机器人。由于Samantha向用户做出了性暗示，它被关闭了。
- en: 'As chatbots become more sophisticated, they gain more access to information,
    and these security issues are now quite complex and potentially damaging. In the
    modern large language model era, we see an exponential increase in significant
    incidents. In 2023 and 2024, these emerged:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着聊天机器人的日益复杂，它们可以获得更多信息，这些安全问题现在非常复杂且可能造成损害。在现代大型语言模型时代，我们看到重大事件呈指数级增长。在2023年和2024年，这些事件出现了：
- en: South Korean mega-corporation Samsung banned its employees from using ChatGPT
    after it had been involved in a significant intellectual property leak.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韩国巨头三星禁止其员工使用ChatGPT，因为它在一场重大的知识产权泄露事件中扮演了角色。
- en: Hackers began taking advantage of poor/insecure code generated by LLMs that
    was inserted into running business applications.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑客开始利用由LLM生成的差劲/不安全的代码，这些代码被插入到正在运行的企业应用程序中。
- en: Lawyers were sanctioned for including fictional cases (generated by LLMs) in
    court documents.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 律师因在法庭文件中包含由LLM生成的虚构案例而受到处罚。
- en: A major airline was successfully sued because its chatbot provided inaccurate
    information.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一家主要航空公司因为其聊天机器人提供了不准确的信息而被成功起诉。
- en: Google was lambasted because its latest AI model produced imagery that was racist
    and sexist.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其最新的AI模型产生了具有种族主义和性别歧视的图像，谷歌遭到了严厉的批评。
- en: Open AI is being investigated for breaches of European privacy regulations and
    sued by the United States Federal Trade Commission (FTC) for producing false and
    misleading information.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI因违反欧洲隐私法规而受到调查，并被美国联邦贸易委员会（FTC）起诉，指控其提供虚假和误导性信息。
- en: The BBC ran the headline “Google AI Search Tells Users to Glue Pizza and Eat
    Rocks,” highlighting dangerous advice proffered by a new LLM-driven feature in
    Google Search.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BBC的头条新闻是“谷歌AI搜索建议用户将披萨粘合并吃石头”，突出了谷歌搜索中由新LLM驱动功能提供的危险建议。
- en: 'The trend here is an acceleration of security, reputational, and financial
    risk related to these chatbots and language models. The problem isn’t being effectively
    solved over time. It’s becoming more acute as the adoption rate of these technologies
    increases. That’s why we’ve created this book: to help developers, teams, and
    companies using these technologies to understand and mitigate these risks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种趋势是安全、声誉和与这些聊天机器人和语言模型相关的财务风险的加速。问题并没有随着时间的推移得到有效解决。随着这些技术的采用率增加，问题变得更加尖锐。这就是我们编写这本书的原因：帮助使用这些技术的开发者、团队和公司了解和减轻这些风险。
- en: Let’s dive in!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨吧！
