- en: Chapter 6\. Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章。微调
- en: In the previous chapter, we discussed the various factors that need to be taken
    into account while choosing the right LLM for your specific needs, including pointers
    on how to evaluate LLMs to be able to make an informed choice. Next, let us utilize
    these LLMs to solve our tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了在选择适合你特定需求的正确大型语言模型时需要考虑的各种因素，包括如何评估大型语言模型以做出明智选择的提示。接下来，让我们利用这些大型语言模型来解决我们的任务。
- en: In this chapter, we will explore the process of adapting an LLM to solve your
    task of interest, using fine-tuning. We will go through a full example of fine-tuning,
    covering all the important decisions one needs to make. We will also discuss the
    art and science of creating fine-tuning datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨通过微调使大型语言模型适应解决你感兴趣的任务的过程。我们将通过一个完整的微调示例，涵盖一个人需要做出的所有重要决策。我们还将讨论创建微调数据集的艺术和科学。
- en: The Need for Fine-Tuning
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调的必要性
- en: 'Why do we need to fine-tune LLMs? Why doesn’t a pre-trained LLM with few-shot
    prompts suffice for our needs? Let us look at a couple of examples to drive the
    point home:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要微调大型语言模型？为什么一个带有少量提示的预训练大型语言模型不足以满足我们的需求？让我们通过几个例子来说明这一点：
- en: Use Case 1
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 用例 1
- en: Consider you are working on the rather whimsical task of detecting all sentences
    written in the past tense within a body of text and transforming them to future
    tense. To solve this task, you might provide a few examples of past tense sentences
    and input-output pairs representing past tense and their corresponding future
    tense sentences. However, the LLM doesn’t seem to be able to tackle this task
    to your satisfaction, making mistakes in both the identification and transformation
    steps. In response, you elaborate on your instructions, adding grammar rules and
    exceptions in the English language into your prompt. You notice an increase in
    performance. But with each new rule added, your prompt balloons, slowly turning
    into a grammar mini-book.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个任务：你正在处理一个相当古怪的任务，即检测文本中所有用过去时态写的句子，并将它们转换为将来时态。为了解决这个问题，你可能需要提供一些过去时态句子的例子以及表示过去时态和它们对应将来时态句子的输入-输出对。然而，大型语言模型似乎无法满足你的要求，在识别和转换步骤中都出现了错误。作为回应，你详细阐述了你的指令，将英语语言的语法规则和例外情况添加到提示中。你注意到性能有所提升。但随着每一条新规则的添加，你的提示逐渐膨胀，慢慢地变成了一本语法迷你书。
- en: As we saw in [Chapter 5](ch05.html#chapter_utilizing_llms), the LLM can adhere
    to only a finite set of instructions in the prompt, and its effective context
    window is much smaller than the advertised context window. We have hit an impasse.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 5 章](ch05.html#chapter_utilizing_llms)中看到的，大型语言模型只能遵循提示中的有限指令集，其有效上下文窗口远小于宣传的上下文窗口。我们已经陷入了僵局。
- en: Use Case 2
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 用例 2
- en: Consider a task that deals with answering questions from content in financial
    text. LLMs are not financial experts and have difficulty dealing with financial
    jargon. To address this, you add the definitions of key financial terms in the
    prompt. While you notice a small improvement in performance, it is not long before
    you realize you need to stuff the entire curriculum of the CPA exam into your
    measly context window to achieve the desired gains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个处理从金融文本中回答问题的任务。大型语言模型不是金融专家，难以处理金融术语。为了解决这个问题，你在提示中添加了关键金融术语的定义。虽然你注意到性能有轻微的改善，但不久你就意识到，你需要把整个注册会计师考试的课程全部塞进你那微小的上下文窗口中，才能达到预期的效果。
- en: This is where fine-tuning comes in. By providing a dataset of input-output pairs,
    such that the model learns the input-output mapping by updating its weights, you
    can accomplish tasks that cannot be performed by in-context learning alone. For
    both the tasks mentioned above, fine-tuning the model massively improves performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是微调发挥作用的地方。通过提供一个输入-输出对的数据集，模型通过更新其权重来学习输入-输出映射，你可以完成仅凭上下文学习无法完成的任务。对于上述提到的两个任务，微调模型大大提高了性能。
- en: When should fine-tuning not be used? If your primary goal is to impart new or
    updated facts or knowledge to the language model, this is better served with techniques
    like RAG, which we will explore in Chapters [10](ch10.html#ch10) and [12](ch12.html#ch12).
    Fine-tuning is best suited for situations where you need the model to learn a
    particular input-output mapping, be familiarized to a new textual domain, or exhibit
    more complex capabilities and behavior.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在什么情况下不应该使用微调？如果你的主要目标是向语言模型传授新的或更新的事实或知识，那么使用RAG等技巧会更好，这些技巧我们将在第[10](ch10.html#ch10)章和第[12](ch12.html#ch12)章中进行探讨。微调最适合需要模型学习特定的输入输出映射、熟悉新的文本领域或展示更复杂的能力和行为的情况。
- en: Warning
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Recall from [Chapter 5](ch05.html#chapter_utilizing_llms) that updating a language
    model’s parameters can cause the base model capabilities to regress! Fine-tuning
    a model on one task can inadvertently cause the base model to perform worse on
    other tasks. Handle with care.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第5章](ch05.html#chapter_utilizing_llms)，更新语言模型的参数可能会导致基础模型的能力下降！在一个任务上微调模型可能会无意中导致基础模型在其他任务上的表现变差。请谨慎处理。
- en: 'Fine-Tuning: A Full Example'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调：一个完整示例
- en: Let’s walk through a practical fine-tuning example from start to finish. We
    would like to train a *political promises detector*, which can be used to identify
    promises made by representatives of the ruling party in campaign speeches or parliamentary
    proceedings. We define a political promise as something that is tangible, specific,
    and an action that the government has the agency to make.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头到尾走一遍一个实际的微调示例。我们希望训练一个*政治承诺检测器*，它可以用来识别执政党代表在竞选演讲或议会程序中做出的承诺。我们定义政治承诺为具体、明确，并且政府有权力采取的行动。
- en: 'An example of such a sentence is: “We will build 10,000 kilometres of subway
    lines in the next ten years.”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的句子例子是：“我们将在未来十年内建设10,000公里的地铁线路。”
- en: 'However, not all future tense or forward-looking statements are promises. The
    following sentences are not promises, per our definition:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有未来时态或前瞻性陈述都是承诺。以下句子根据我们的定义不是承诺：
- en: “We expect the Japanese to increase tariffs next year.” (expectation, and not
    something the government can control)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们预计日本明年将提高关税。”（预期，并且不是政府可以控制的事情）
- en: “We will work toward making Canada a better place.” (no specifics provided)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们将努力使加拿大成为一个更好的地方。”（未提供具体细节）
- en: “AI will cause the loss of a million jobs next year.” (prediction, not promise)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “AI明年将导致一百万个工作岗位的丧失。”（预测，而非承诺）
- en: Our base LLM, Llama2-7B, finds it difficult to accurately identify such promises
    in an in-context learning setup. Therefore, we will fine-tune it for this specific
    task. We can then use the resulting model to detect political promises, and then
    match those promises against structured datasets or budgetary text to track whether
    these promises have been fulfilled over a period of time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础LLM，Llama2-7B，在上下文学习设置中难以准确识别此类承诺。因此，我们将针对这个特定任务对其进行微调。然后我们可以使用生成的模型来检测政治承诺，并将这些承诺与结构化数据集或预算文本进行匹配，以跟踪这些承诺在一段时间内是否得到履行。
- en: To this end, I have constructed a synthetic fine-tuning dataset containing examples
    of both promises and mere statements. Later in this chapter, we will go through
    the process of creating such a dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我构建了一个包含承诺和陈述例子的合成微调数据集。在本章的后面部分，我们将介绍创建此类数据集的过程。
- en: Fortunately, fine-tuning today is easier due to the existence of several libraries
    that streamline the fine-tuning process. The most important of these libraries
    are [Transformers](https://oreil.ly/BTi76), [Accelerate](https://oreil.ly/W8oLi),
    [PEFT](https://oreil.ly/QbQoq), [TRL](https://oreil.ly/Ya9Xj), and [bitsandbytes](https://oreil.ly/ruVEX).
    The first four are from Hugging Face. You have encountered many of these libraries
    in prior chapters already. Being familiar with the inner workings of these libraries
    is a very useful skill.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于存在几个简化微调过程的库，今天的微调变得更加容易。其中最重要的库是[Transformers](https://oreil.ly/BTi76)、[Accelerate](https://oreil.ly/W8oLi)、[PEFT](https://oreil.ly/QbQoq)、[TRL](https://oreil.ly/Ya9Xj)和[bitsandbytes](https://oreil.ly/ruVEX)。前四个来自Hugging
    Face。你已经在之前的章节中遇到了许多这些库。熟悉这些库的内部工作原理是一项非常有用的技能。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Given that these libraries are relatively new and are part of a fast-moving
    field, they frequently undergo substantial updates. I recommend keeping in touch
    with major updates of these libraries, as they continue to introduce enhancements
    that will simplify your workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些库相对较新，并且属于快速发展的领域，它们经常经历重大更新。我建议您关注这些库的主要更新，因为它们持续引入的增强功能将简化您的流程。
- en: 'Let’s begin by loading the dataset. The custom dataset can be downloaded from
    this book’s [GitHub repo](https://oreil.ly/llm-playbooks):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始。自定义数据集可以从本书的[GitHub仓库](https://oreil.ly/llm-playbooks)下载：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: I highly recommend using the [*datasets* library](https://oreil.ly/3LX5X) for
    loading your training and fine-tuning datasets, as it is an excellent abstraction
    for efficiently loading large datasets, abstracting away memory management details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐使用[*datasets*库](https://oreil.ly/3LX5X)来加载您的训练和微调数据集，因为它是一个高效加载大型数据集的优秀抽象，可以抽象出内存管理细节。
- en: 'Next, let us set some relevant hyperparameters in the `Transformers` library
    through the `TrainingArguments` class:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过`TrainingArguments`类在`Transformers`库中设置一些相关的超参数：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are more than a hundred arguments available; we will go through the important
    ones. The arguments relate to the learning algorithms used, memory and space optimizations,
    quantization, regularization, and distributed training. Let’s explore these in
    detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的参数超过一百个；我们将介绍其中重要的参数。这些参数与使用的学习算法、内存和空间优化、量化、正则化和分布式训练相关。让我们详细探讨这些内容。
- en: Learning Algorithms Parameters
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习算法参数
- en: Let’s explore optimization algorithms used for training the network and learn
    how to choose the right one for our purposes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨用于训练网络的优化算法，并学习如何为我们的目的选择正确的优化器。
- en: Optimizers
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: AdamW and Adafactor are currently the most used optimizers. Other popular optimization
    algorithms include stochastic gradient descent (SGD), RMSProp, Adagrad, Lion,
    and their variants. For more background on optimization algorithms, refer to Florian
    June’s [blog post](https://oreil.ly/VTiDa).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW和Adafactor是目前最常用的优化器。其他流行的优化算法包括随机梯度下降（SGD）、RMSProp、Adagrad、Lion及其变体。有关优化算法的更多背景信息，请参阅Florian
    June的[博客文章](https://oreil.ly/VTiDa)。
- en: Adafactor and SGD use four bytes of memory per parameter, while AdamW uses eight
    bytes per parameter. This means that a 7B model undergoing full fine-tuning with
    the AdamW optimizer requires 7 * 8 = ~56GB of memory to store the optimizer states
    alone. Even more memory is needed to store the parameters, gradients, and the
    forward activations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Adafactor和SGD每个参数使用4个字节的内存，而AdamW每个参数使用8个字节的内存。这意味着使用AdamW优化器进行完全微调的7B模型，仅存储优化器状态就需要7
    * 8 = ~56GB的内存。存储参数、梯度和前向激活还需要更多的内存。
- en: More recently, [8-bit optimizers](https://oreil.ly/4Z14D) have been introduced
    that perform quantization of the optimizer state. A 7B model undergoing full fine-tuning
    with the AdamW 8-bit version requires only ~14GB of memory for the optimizer state.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，引入了[8位优化器](https://oreil.ly/4Z14D)，它们可以对优化器状态进行量化。使用AdamW 8位版本的7B模型进行完全微调，只需要大约14GB的内存来存储优化器状态。
- en: 'These 8-bit optimizers are available through the bitsnbytes library and are
    also supported by Hugging Face. For using the 8-bit AdamW version, you can set
    in the `TrainingArguments`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些8位优化器通过bitsnbytes库提供，并且也得到Hugging Face的支持。要使用8位AdamW版本，您可以在`TrainingArguments`中设置：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For all the optimizer options directly available through Hugging Face, refer
    to the [OptimizerNames class](https://oreil.ly/7kdSO).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有通过Hugging Face直接提供的优化器选项，请参阅[OptimizerNames类](https://oreil.ly/7kdSO)。
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In his benchmarking experiments, Stas Bekman [shows](https://oreil.ly/0_0lt)
    that surprisingly, the 8-bit AdamW optimizer is actually faster than the standard
    AdamW optimizer. His experiments also show that Adafactor is slightly slower than
    AdamW overall.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的基准测试实验中，Stas Bekman [显示](https://oreil.ly/0_0lt)令人惊讶的是，8位AdamW优化器实际上比标准AdamW优化器更快。他的实验还显示，Adafactor整体上略慢于AdamW。
- en: The default optimizer provided in the Hugging Face `TrainingArguments` class
    is AdamW. For most cases, the default optimizer works just fine. However, if it
    doesn’t, you can try Adafactor and Lion. For reinforcement learning, SGD seems
    to work well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face `TrainingArguments`类中提供的默认优化器是AdamW。对于大多数情况，默认优化器就足够好了。但是，如果它不起作用，您可以尝试Adafactor和Lion。对于强化学习，SGD似乎效果很好。
- en: If you are especially memory constrained, 8-bit AdamW is a compelling choice.
    If available, the paged version of these optimizers will further mitigate your
    memory requirements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你特别内存受限，8 位 AdamW 是一个不错的选择。如果可用，这些优化器的分页版本将进一步减轻你的内存需求。
- en: Learning rates
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: For each optimizer, certain learning rates have been shown to be very effective.
    A recommended learning rate for AdamW is 1e-4 with a weight decay of 0.01\. Weight
    decay is a regularization technique that helps reduce overfitting. Similarly,
    the default values for minor optimizer parameters like *adam_beta1*, *adam_beta2*,
    and *adam_epsilon* are good enough and need not be changed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个优化器，某些学习率已被证明非常有效。AdamW 的推荐学习率是 1e-4，权重衰减为 0.01。权重衰减是一种正则化技术，有助于减少过拟合。同样，像
    *adam_beta1*、*adam_beta2* 和 *adam_epsilon* 这样的次要优化器参数的默认值已经足够好，不需要更改。
- en: Learning schedules
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习计划
- en: Toward the end of the training process, it is a good idea to lower the learning
    rate because you do not want to overshoot when you are so close to convergence.
    In a similar vein, you would like to prevent your model from learning too much
    from the first few batches of examples. In either case, we would like to be able
    to automatically adjust the learning rate as training progresses. To facilitate
    this, we can use a learning schedule.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的后期，降低学习率是一个好主意，因为你不想在接近收敛时 overshoot。类似地，你希望防止你的模型从第一批次的例子中学习太多。在任何情况下，我们都希望能够在训练过程中自动调整学习率。为了实现这一点，我们可以使用学习计划。
- en: 'Hugging Face supports several different types of learning schedulers. Here
    are a few important ones:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 支持多种不同的学习调度器。以下是一些重要的：
- en: Constant
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 常数
- en: This is the vanilla training schedule where the learning rate remains constant
    throughout the course of the training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标准的训练计划，其中学习率在整个训练过程中保持不变。
- en: Constant with warmup
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 带预热常数
- en: In this setting, the learning rate starts from zero and is increased linearly
    toward the specified learning rate during a warmup phase. After the warmup phase
    is completed, the learning rate remains constant.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，学习率从零开始，在预热阶段线性增加到指定的学习率。预热阶段完成后，学习率保持不变。
- en: '[Figure 6-1](#constant-lr) shows how the learning rate changes over time while
    using the constant with warmup scheduler.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](#constant-lr) 展示了在使用带预热调度器的常数时学习率随时间的变化情况。'
- en: '![constant-lr](assets/dllm_0601.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![常数-lr](assets/dllm_0601.png)'
- en: Figure 6-1\. Learning rate with a constant schedule with warmup
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 带预热常数的 learning rate
- en: Cosine
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦
- en: In this setting, called *cosine annealing*, the learning rate has a warmup phase
    after which it slowly declines to zero, as per the cosine function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，称为 *余弦退火*，学习率在预热阶段之后会缓慢降低到零，遵循余弦函数。
- en: '[Figure 6-2](#cosine-warmup) shows how the learning rate changes over time
    while using the cosine scheduler.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-2](#cosine-warmup) 展示了在使用余弦调度器时学习率随时间的变化情况。'
- en: '![cosine-warmup](assets/dllm_0602.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![余弦预热](assets/dllm_0602.png)'
- en: Figure 6-2\. Learning rate with a cosine schedule
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 带余弦计划的 learning rate
- en: Cosine with restarts
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 带重启的余弦
- en: In this setting, called *cosine annealing with warm restart*, after a warmup
    phase, the learning rate decreases to zero following the cosine function, but
    undergoes several hard restarts, where the learning rate shoots back to the specified
    learning rate after it reaches zero. For more details on why this is effective,
    check out Loshcilov and Hutter’s [paper](https://oreil.ly/Q4c3o) that introduced
    this concept.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，称为 *余弦退火带预热重启*，在预热阶段之后，学习率按照余弦函数降低到零，但会经历几次硬重启，学习率在达到零后迅速回到指定的学习率。关于为什么这种方法有效的更多细节，请查看介绍了这一概念的
    Loshcilov 和 Hutter 的 [论文](https://oreil.ly/Q4c3o)。
- en: '[Figure 6-3](#cosine-restart) shows how the learning rate changes across time
    while using the cosine with restarts scheduler.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](#cosine-restart) 展示了在使用带重启调度器时学习率随时间的变化情况。'
- en: '![cosine-restart](assets/dllm_0603.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![余弦重启](assets/dllm_0603.png)'
- en: Figure 6-3\. Learning rate with a cosine with restarts schedule
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 带重启余弦的 learning rate
- en: Linear
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 线性
- en: This is very similar to the cosine setting, except that the learning rate decreases
    to zero linearly instead of following the cosine function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这与余弦设置非常相似，不同之处在于学习率线性降低到零，而不是遵循余弦函数。
- en: '[Figure 6-4](#linear-lr) shows how the learning rate changes over time while
    using the linear scheduler.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-4](#linear-lr) 展示了在使用线性调度器时学习率随时间的变化情况。'
- en: '![linear-lr](assets/dllm_0604.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![linear-lr](assets/dllm_0604.png)'
- en: Figure 6-4\. Learning rate with a linear scheduler
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 带线性调度器的学习率
- en: If you are using AdamW, schedulers with a warmup phase are even more important
    to prevent getting trapped in a bad minima. Empirically, it has been found that
    cosine annealing outperforms linear decay.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用AdamW，具有预热阶段的调度器甚至更加重要，以防止陷入局部最小值。经验上发现，余弦退火优于线性衰减。
- en: 'For our political promises detector fine-tuning, let’s use the paged variant
    of AdamW, a learning rate of 3e-4, a weight decay of 0.01, and the cosine learning
    schedule:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的政治承诺检测器微调，让我们使用AdamW的分页变体，学习率为3e-4，权重衰减为0.01，以及余弦学习调度：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Memory Optimization Parameters
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化参数
- en: After we have set the parameters related to the optimizers, let’s explore memory
    and compute optimization parameters. Two prevalent techniques in this area include
    gradient checkpointing and gradient accumulation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置了与优化器相关的参数之后，让我们探索内存和计算优化参数。这个领域中的两种流行技术包括梯度检查点和梯度累积。
- en: Gradient checkpointing
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: Gradient checkpointing helps save memory at the cost of more compute. During
    the forward pass of the backpropagation algorithm, activations are computed and
    saved in memory so that they can be used in the backward pass. What if we did
    not save all of the activations? The missing activations could be recalculated
    on the fly during the backward pass. This does cost us more compute, but we could
    save a lot of memory. We could even train models where a batch size of only one
    does not fit in our GPU memory. For more technical details on gradient checkpointing,
    check out Yaroslav Bulatov’s [blog](https://oreil.ly/i-R4I).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点技术以增加计算成本为代价来帮助节省内存。在反向传播算法的前向传递过程中，激活值被计算并保存在内存中，以便在反向传递中使用。如果我们没有保存所有的激活值会怎样？缺失的激活值可以在反向传递过程中即时重新计算。这确实会增加我们的计算成本，但我们可以节省大量的内存。我们甚至可以训练那些批次大小仅为一个且无法适应我们的GPU内存的模型。有关梯度检查点的更多技术细节，请参阅Yaroslav
    Bulatov的[博客](https://oreil.ly/i-R4I)。
- en: Gradient accumulation
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度累积
- en: Let’s say we have a desired batch size but we do not have the required memory
    to support that batch size. We can simulate the desired batch size using a technique
    called gradient accumulation. In this technique, the gradient updates are not
    done at every batch, but are accumulated over several batches and then summed
    or averaged.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个期望的批次大小，但我们没有足够的内存来支持该批次大小。我们可以使用一种称为梯度累积的技术来模拟期望的批次大小。在这种技术中，梯度更新不是在每个批次都进行，而是在几个批次中累积，然后求和或平均。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Gradient accumulation can make training slower, since there are fewer updates
    being made. Gradient accumulation does not reduce the computation required.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积可能会使训练变慢，因为更新的次数减少了。梯度累积不会减少所需的计算量。
- en: Quantization
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化
- en: A very effective form of saving memory is through quantization, as introduced
    in [Chapter 5](ch05.html#chapter_utilizing_llms). We will go through quantization
    techniques in more detail in [Chapter 9](ch09.html#ch09). For our use case, we
    will use bf16 as it represents a sound tradeoff between memory savings and performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量化来节省内存是一种非常有效的方法，如第5章（ch05.html#chapter_utilizing_llms）中所述。我们将在第9章（ch09.html#ch09）中更详细地介绍量化技术。对于我们的用例，我们将使用bf16，因为它在内存节省和性能之间提供了一个合理的权衡。
- en: 'For our political promises detector fine-tuning, we’ll set the following parameters
    for memory optimization, given that we are trying to train it on a relatively
    memory constrained 16 GB RAM GPU:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的政治承诺检测器微调，鉴于我们试图在一个相对内存受限的16 GB RAM GPU上训练它，我们将设置以下参数以进行内存优化：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Regularization Parameters
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化参数
- en: Next, let’s look at various techniques available for tackling model overfitting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看可用于解决模型过拟合的各种技术。
- en: Label smoothing
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签平滑
- en: Label smoothing is a technique that not only helps with combatting overfitting
    but also aids in model calibration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑是一种技术，它不仅有助于对抗过拟合，还有助于模型校准。
- en: Calibration is an underappreciated topic in deep learning. A model is said to
    be well-calibrated if there is a correlation between its output probability values
    and task accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 校准是深度学习中一个被低估的话题。如果一个模型其输出概率值与任务准确性之间存在相关性，则称该模型校准良好。
- en: For example, consider a task that classifies a sentence as being abusive or
    not. If the model is well-calibrated, then among all examples for which the model
    produces an output probability of 0.9, 90% of them would be expected to be correctly
    classified. Similarly, for an output probability of 0.6, there should be a lower
    (~60%) likelihood of the classification being correct. Simply put, the output
    probability should accurately reflect the confidence in the classification decision.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个将句子分类为辱骂性或不辱骂性的任务。如果模型校准良好，那么在模型输出概率为0.9的所有示例中，预期有90%会被正确分类。同样，对于输出概率为0.6的情况，分类正确的可能性应该更低（~60%）。简单来说，输出概率应该准确地反映对分类决策的信心。
- en: A model being well-calibrated implies that it is not overconfident. This helps
    us in nuanced handling of examples that have low output probabilities (using a
    bigger model to handle those examples, for instance).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个校准良好的模型意味着它不是过度自信的。这有助于我们细致地处理输出概率较低的示例（例如，使用更大的模型来处理这些示例）。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Larger models are less calibrated compared to models like BERT, according to
    a study by [Li et al.](https://oreil.ly/ij7mS) Larger models tend to be more confident
    in general about their predictions. The inability to calculate reasonably accurate
    uncertainty estimates for large language models could be an argument to use smaller
    ones instead!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Li等人的一项研究，与BERT等模型相比，更大的模型校准度较低。更大的模型通常对其预测更有信心。无法计算大型语言模型合理的不确定性估计可能是一个使用较小模型的论据！
- en: One of the techniques for calibrating models is label smoothing. The usual training
    process involves training against hard target labels (0 or 1 for a binary classification
    task). When using cross-entropy as the loss function, this amounts to pushing
    the model logits closer to 0 or 1, thus making the model highly confident. Label
    smoothing involves using a regularization term that is subtracted or divided from
    the hard target label.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 校准模型的技术之一是标签平滑。通常的训练过程涉及针对硬目标标签（二分类任务中的0或1）进行训练。当使用交叉熵作为损失函数时，这相当于将模型的对数几率推向0或1，从而使模型高度自信。标签平滑涉及使用一个正则化项，该项从硬目标标签中减去或除以。
- en: Label smoothing is especially useful when the input dataset is noisy, i.e.,
    contains some inaccurate labels. Regularization prevents the model from learning
    too much from incorrect examples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑在输入数据集有噪声时特别有用，即包含一些不准确标签。正则化防止模型从错误示例中学习过多。
- en: For the political promises detector, we will use label smoothing, given that
    some examples could be subjective or open to interpretation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于政治承诺检测器，我们将使用标签平滑，考虑到一些示例可能是主观的或可以解释的。
- en: Noise Embeddings
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 噪声嵌入
- en: The datasets we use for fine-tuning typically consist of a small number of examples
    (< 50,000). We would like our model to not overfit to the stylistic characteristics
    of the dataset, like the formatting, wording, and length of the text. One way
    to address this is by adding noise to the input embeddings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于微调的数据集通常由少量示例组成（< 50,000）。我们希望我们的模型不要过度拟合数据集的风格特征，如格式、措辞和文本长度。解决这一问题的方法之一是在输入嵌入中添加噪声。
- en: '[Jain et al.](https://oreil.ly/ouESL) observe that adding noise embeddings
    reduces the tendency of the model to overfit to wording and formatting of the
    fine-tuning datasets. An interesting side effect of noise embeddings is that the
    models generate longer, verbose texts. By measuring token diversity of the outputs,
    they confirmed that the longer texts actually include more information and are
    not just repetitive.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jain等人](https://oreil.ly/ouESL)观察到，添加噪声嵌入可以减少模型过度拟合微调数据集的措辞和格式的倾向。噪声嵌入的一个有趣副作用是模型生成的文本更长，更冗长。通过测量输出标记的多样性，他们证实了较长的文本实际上包含更多信息，而不仅仅是重复。'
- en: Hugging Face supports [Noisy Embedding Instruction Fine-Tuning (NEFTune)](https://oreil.ly/dSaem),
    a noise addition technique. In NEFTune, a noise vector is added to each embedding
    vector. The elements in the noise vector are generated by sampling independent
    and identically distributed (iid) from [-1,1]. The resulting vector is scaled
    using a scaling factor before being added to the embedding vector.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face支持[噪声嵌入指令微调（NEFTune）](https://oreil.ly/dSaem)，这是一种噪声添加技术。在NEFTune中，每个嵌入向量都添加一个噪声向量。噪声向量中的元素是通过从[-1,1]中独立同分布（iid）采样生成的。得到的向量在添加到嵌入向量之前使用缩放因子进行缩放。
- en: Noise embeddings have been empirically found to be very effective in reducing
    overfitting. Therefore, we will use it for our political promises detector fine-tuning.
    Note that the noise embeddings are added only during training and not during inference.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 经验表明，噪声嵌入在减少过拟合方面非常有效。因此，我们将它用于我们的政治承诺检测器微调。请注意，噪声嵌入仅在训练期间添加，而不是在推理期间。
- en: Warning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The impact of noise embeddings is not yet well understood. Improvements in the
    fine-tuning task could come at the cost of other model capabilities. Make sure
    you test the model for regressions!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声嵌入的影响尚未得到充分理解。在微调任务中的改进可能会以牺牲其他模型能力为代价。请确保测试模型以防止回归！
- en: 'For our political promises detector fine-tuning task, let’s activate both label
    smoothing and noise embeddings:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的政治承诺检测器微调任务，让我们激活标签平滑和噪声嵌入：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Batch Size
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小
- en: Along with the learning rate, the batch size is one of the most important hyperparameters
    we need to set. A larger batch size means training will proceed faster. However,
    larger batch sizes also require more memory. Larger batch sizes can also lead
    the model to land in a sharp local minima, which can be a sign of overfitting.
    Therefore, there are trade offs involving memory, compute, and performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习率之外，批量大小是我们需要设置的最重要超参数之一。更大的批量大小意味着训练将更快进行。然而，更大的批量大小也需要更多的内存。更大的批量大小也可能导致模型陷入尖锐的局部最小值，这可能是过拟合的迹象。因此，涉及内存、计算和性能的权衡。
- en: For the political promises detector, we will use a batch size of 8, given our
    memory limitations. Of course during inference, the maximum possible batch size
    is the ideal one. Note that it is recommended that the batch size be always a
    number that is a power of two, to reduce GPU I/O overhead.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于政治承诺检测器，鉴于我们的内存限制，我们将使用 8 的批量大小。当然，在推理期间，最大的可能批量大小是理想的。请注意，建议批量大小始终是 2 的幂，以减少
    GPU I/O 开销。
- en: 'The `TrainingArguments` class by Hugging Face supports *auto_find_batch_size*,
    which when set, selects the maximum possible batch size supported by the memory.
    To use this feature, you need to install the `accelerate` library:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 `TrainingArguments` 类支持 *auto_find_batch_size*，当设置时，选择由内存支持的最大的可能批量大小。要使用此功能，您需要安装
    `accelerate` 库：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can reduce your maximum sequence length to support a larger batch size.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将最大序列长度降低以支持更大的批量大小。
- en: 'Finally, let’s set some miscellaneous parameters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置一些杂项参数：
- en: '`max_grad_norm`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_grad_norm`'
- en: This is used for gradient clipping, which is a solution for the exploding gradients
    issue that is sometimes encountered during training. The `max_grad_norm` value
    is the threshold for gradient clipping. If the L2 gradient norm is above the threshold,
    then it will be rescaled to `max_grad_norm`. For more details on gradient clipping,
    see [“Understanding Gradient Clipping (and How It Can Fix Exploding Gradients
    Problem)”](https://oreil.ly/gH7L7).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于梯度裁剪，这是解决训练过程中有时遇到的梯度爆炸问题的解决方案。`max_grad_norm` 值是梯度裁剪的阈值。如果 L2 梯度范数高于阈值，则将其缩放到
    `max_grad_norm`。有关梯度裁剪的更多详细信息，请参阅[“理解梯度裁剪（以及它是如何修复梯度爆炸问题的）”](https://oreil.ly/gH7L7)。
- en: '`group_by_length`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`group_by_length`'
- en: This is used to group examples that have similar lengths in the same batch,
    so that the padding tokens can be optimized.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于将具有相似长度的示例分组到同一个批量中，以便优化填充标记。
- en: '`max_train_epochs`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_train_epochs`'
- en: 'Number of passes over the training dataset. This is usually set to less than
    five to prevent overfitting:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集上遍历的次数。这通常设置为少于五次，以防止过拟合：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameter-Efficient Fine-Tuning
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数高效微调
- en: After filling in the `TrainingArguments`, let’s next fill in parameters of the
    PEFT library.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在填写完 `TrainingArguments` 之后，接下来填写 PEFT 库的参数。
- en: The PEFT library by Hugging Face is an impressive facilitator of parameter-efficient
    fine-tuning. This refers to a set of fine-tuning techniques that update only a
    small proportion of parameters in the model while keeping the performance closer
    to what it would have been if all the parameters were updated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 PEFT 库是一个令人印象深刻的参数高效微调促进者。这指的是一组微调技术，这些技术只更新模型中一小部分参数，同时保持性能接近如果所有参数都被更新时的水平。
- en: 'In this example, we will use low-rank adaptation (LoRA) as the fine-tuning
    technique. Here are some hyperparameters to consider:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用低秩适应（LoRA）作为微调技术。以下是一些需要考虑的超参数：
- en: '`r`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`r`'
- en: The attention dimension of LoRA.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的注意力维度。
- en: '`lora_alpha`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`lora_alpha`'
- en: The alpha parameter in the LoRA technique.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA技术中的alpha参数。
- en: '`lora_dropout`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`lora_dropout`'
- en: The dropout probability used in the layers being tuned. This helps reduce overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在正在调整的层中使用的丢弃概率。这有助于减少过拟合。
- en: '`layers_to_transform`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`layers_to_transform`'
- en: This specifies the layers for which the LoRA transformation is to be applied.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这指定了要应用LoRA变换的层。
- en: 'Here are some recommended default values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For more background on LoRA, refer to Ogban Ugot’s [blog post](https://oreil.ly/_l91y).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LoRA的更多背景信息，请参阅Ogban Ugot的[博客文章](https://oreil.ly/_l91y)。
- en: Working with Reduced Precision
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用降低精度
- en: The bitsandbytes library, built by Tim Dettmers, facilitates working with reduced
    precision formats, which we introduced in [Chapter 5](ch05.html#chapter_utilizing_llms).
    In this example, we will work with the FP4 format. Note that you need the bitsandbytes
    version to be >= 0.39.0.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由Tim Dettmers构建的bitsandbytes库，简化了使用降低精度格式的操作，我们在[第5章](ch05.html#chapter_utilizing_llms)中介绍了这种格式。在这个例子中，我们将使用FP4格式。请注意，你需要bitsandbytes版本
    >= 0.39.0。
- en: 'Hugging Face has integrated bitsandbytes support into its ecosystem. The `BitsAndBytesConfig`
    class allows us to set the parameters. Here are some relevant ones:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face已经将其生态系统中的bitsandbytes支持集成。`BitsAndBytesConfig`类允许我们设置参数。以下是一些相关的参数：
- en: '`load_in_8bit/load_in_4bit`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_in_8bit/load_in_4bit`'
- en: This is used to specify if we want to load the model in 4-bit mode or 8-bit
    mode.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于指定我们是否要以4位模式或8位模式加载模型。
- en: '`llm_int8_threshold`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_threshold`'
- en: We need to specify a threshold of values beyond which fp16 will be used. This
    is because int8 quantization works well only for values lesser than 5–6.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定一个阈值，超过该阈值将使用fp16。这是因为int8量化对于小于5-6的值效果较好。
- en: '`llm_int8_skip_modules`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_skip_modules`'
- en: This is used to specify the exceptions for which we do not want int8 quantization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于指定我们不想使用int8量化的异常。
- en: '`llm_int8_enable_fp32_cpu_offload`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm_int8_enable_fp32_cpu_offload`'
- en: If we want parts of the model to be run in int8 on GPU and the rest in FP32
    on CPU, this parameter facilitates it. This is used in cases where the model is
    too large to fit on our GPU.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在GPU上以int8运行模型的一部分，而在CPU上以FP32运行其余部分，这个参数可以简化这个过程。这在模型太大而无法适应我们的GPU时使用。
- en: '`bnb_4bit_compute_dtype`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`bnb_4bit_compute_dtype`'
- en: This sets the computational type, regardless of the input type.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这设置计算类型，无论输入类型如何。
- en: '`bnb_4bit_quant_type`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`bnb_4bit_quant_type`'
- en: The options here are FP4 or NF4\. This is used to set the quantization type
    in the 4-bit layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的选项是FP4或NF4。这用于设置4位层的量化类型。
- en: 'Here are some recommended default values:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, we use the Transformer Reinforcement Learning (TRL) library that, in
    addition to reinforcement learning, provides support for supervised fine-tuning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用了Transformer Reinforcement Learning (TRL)库，除了强化学习之外，它还提供了对监督微调的支持。
- en: 'Here are some recommended default values:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些推荐的默认值：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Putting It All Together
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合所有
- en: 'Now that we have set up all the requisite parameters, here is the full code
    for the fine-tuning process:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了所有必要的参数，以下是微调过程的完整代码：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The relationship between the hyperparameters is very complex, and you might
    find surprising results. It will take several iterations before you hit the sweet
    spot. However, do not spend too much time squeezing out the last bit of performance
    from your fine-tuning, as that time is better spent developing better training
    data. In the next section, we will learn how to create effective training datasets.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数之间的关系非常复杂，你可能会发现令人惊讶的结果。在找到最佳点之前可能需要多次迭代。然而，不要花太多时间从微调中挤出最后一点性能，因为这段时间最好用于开发更好的训练数据。在下一节中，我们将学习如何创建有效的训练数据集。
- en: 'The exact memory you need to fine-tune an LLM depends on several factors: the
    optimizer used, whether gradient accumulation and gradient checkpointing are activated,
    the type of quantization used, etc.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM所需的精确内存取决于多个因素：使用的优化器、是否激活梯度累积和梯度检查点、使用的量化类型等。
- en: Fine-Tuning Datasets
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调数据集
- en: In our fine-tuning example, we directly loaded a preconstructed dataset, focusing
    primarily on the fine-tuning process. Now, let’s shift our attention to the dataset,
    to understand the various techniques for creating datasets.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的微调示例中，我们直接加载了一个预构建的数据集，主要关注微调过程。现在，让我们将注意力转向数据集，了解创建数据集的各种技术。
- en: 'First, let’s look into the dataset we used in our fine-tuning example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们在微调示例中使用的数据集：
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we can see, this is not a traditional dataset with just (input, output)
    pairs but one that also contains the task description in natural language. A typical
    example in this type of fine-tuning dataset consists of :'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这不仅仅是一个只有（输入，输出）对的常规数据集，而是一个还包含自然语言中任务描述的数据集。这类微调数据集的一个典型例子包括：
- en: The instruction, which describes the task and specifies the desired output format.
    Optionally, the instruction contains positive and/or negative examples of the
    task. It can also contain constraints and exceptions to be followed.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令，描述任务并指定所需的输出格式。可选地，指令包含任务的正面和/或负面示例。它还可以包含需要遵循的约束和例外情况。
- en: An optional input, which in our example is the sentence or paragraph for the
    model to evaluate.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可选的输入，在我们的例子中是模型要评估的句子或段落。
- en: The output, which is the correct answer to the task in the format specified
    in the instruction.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出，即任务在指令中指定的格式下的正确答案。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Fine-tuning datasets can be either multi-task or single-task. Multi-task datasets
    are used for instruction-tuning. In general, instruction-tuning can be treated
    as an intermediate step before single-task fine-tuning. For example, you can take
    a T5 language model, instruction-tune it with FLAN to create FLAN-T5, and then
    further fine-tune it with your task-specific dataset. This approach is [shown](https://oreil.ly/e-MVh)
    to yield better results than directly fine-tuning on T5 alone.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 微调数据集可以是多任务或单任务的。多任务数据集用于指令微调。通常，指令微调可以被视为单任务微调之前的一个中间步骤。例如，你可以使用T5语言模型，用FLAN进行指令微调以创建FLAN-T5，然后进一步使用你的特定任务数据集进行微调。这种方法[显示](https://oreil.ly/e-MVh)比仅在T5上直接微调有更好的结果。
- en: Later in this chapter, we will learn how to create task-specific datasets. First,
    let’s look at how we can create instruction-tuning datasets.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将学习如何创建特定任务的数据集。首先，让我们看看我们如何创建指令微调数据集。
- en: There are plenty of instruction-tuned LLMs available, both open source and proprietary.
    Why do we want to instruction-tune the LLM ourselves? Public datasets are too
    general, lack diversity, and are primarily geared to general usage. Leveraging
    your domain expertise and knowledge of intended use cases to construct the dataset
    can be highly effective. In fact, at my company, which specializes in the financial
    domain, this technique delivered the single largest boost in performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的指令微调大型语言模型（LLM）很多，既有开源的也有专有的。为什么我们想要自己指令微调LLM呢？公共数据集过于通用，缺乏多样性，并且主要针对通用用途。利用你的领域专业知识和对预期用例的了解来构建数据集可以非常有效。实际上，在我公司，我们专注于金融领域，这种技术带来了性能的最大提升。
- en: 'Approaches to creating instruction-tuning datasets include:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 创建指令微调数据集的方法包括：
- en: Utilizing publicly available instruction-tuning datasets
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用公开可用的指令微调数据集
- en: Transforming traditional fine-tuning datasets into instruction-tuning datasets
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将传统微调数据集转换为指令微调数据集
- en: Starting with manually crafted seed examples, followed by optionally augmenting
    the dataset by utilizing an LLM to generate similar examples
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从手动制作的种子示例开始，然后可选地通过利用LLM生成类似示例来扩充数据集
- en: Next, let’s examine these methods more closely.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地研究这些方法。
- en: Utilizing Publicly Available Instruction-Tuning Datasets
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用公开可用的指令微调数据集
- en: If your use case is sufficiently general or popular, you may be able to use
    publicly available datasets for instruction-tuning. The following table lists
    some popular instruction-tuning datasets, along with information on their creators,
    sizes, and creation process.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的用例足够通用或流行，你可能能够使用公开可用的数据集进行指令微调。以下表格列出了一些流行的指令微调数据集，以及它们创建者、大小和创建过程的信息。
- en: Table 6-1\. Popular instruction-tuning datasets
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1\. 流行的指令微调数据集
- en: '| Name | Size | Created by | Created using |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 大小 | 创建者 | 创建方式 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| OIG | 43M | [Ontocord](https://www.ontocord.ai/) | Rule-based  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| OIG | 43M | [Ontocord](https://www.ontocord.ai/) | 基于规则 |'
- en: '| FLAN | 4.4M | Google | Templates |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| FLAN | 4.4M | Google | 模板 |'
- en: '| P3 (Public Pool of Prompts) | 12M | Big Science | Templates |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| P3（公共提示池） | 12M | 大科学 | 模板 |'
- en: '| Natural Instruction | 193K | Allen AI | Templates |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 自然指令 | 193K | Allen AI | 模板 |'
- en: '| Unnatural Instructions | 240K | [Honovich et al.](https://github.com/orhonovich/unnatural-instructions),
    Meta | LLMs |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 非自然指令 | 240K | [Honovich 等人](https://github.com/orhonovich/unnatural-instructions),
    Meta | 大型语言模型 |'
- en: '| LIMA (Less Is More for Alignment) | 1K | [Zhou et al.](https://arxiv.org/abs/2305.11206),
    Meta | Templates |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LIMA (Less Is More for Alignment) | 1K | [周等人](https://arxiv.org/abs/2305.11206),
    Meta | 模板 |'
- en: '| Self-Instruct | 52K | [Wang et al.](https://github.com/yizhongw/self-instruct)
    | LLMs |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Self-Instruct | 52K | [王等人](https://github.com/yizhongw/self-instruct) |
    大型语言模型 |'
- en: '| Evol-Instruct | 52K | [Xu et al.](https://arxiv.org/abs/2304.12244) | LLMs
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Evol-Instruct | 52K | [徐等人](https://arxiv.org/abs/2304.12244) | 大型语言模型 |'
- en: '| InstructWild v2 | 110K | [Ni et al.](https://github.com/XueFuzhao/InstructionWild)
    | LLMs |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| InstructWild v2 | 110K | [倪等人](https://github.com/XueFuzhao/InstructionWild)
    | 大型语言模型 |'
- en: '| Alpaca | 52K | Stanford | LLMs |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 阿尔帕卡 | 52K | 斯坦福 | 大型语言模型 |'
- en: '| Guanaco | 534K | [Dettmers et al.](https://arxiv.org/abs/2305.14314) | LLMs
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 534K | [德特梅尔等人](https://arxiv.org/abs/2305.14314) | 大型语言模型 |'
- en: '| Vicuna | 70K | LMSYS | Human conversations |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 70K | LMSYS | 人际对话 |'
- en: '| OpenAssistant | 161K | Open Assistant | Human conversations |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| OpenAssistant | 161K | 开放助手 | 人际对话 |'
- en: Let’s go through fine-tuned language net (FLAN), one of the most popular instruction-tuning
    datasets in detail. Understanding how it was constructed will provide you with
    roadmaps to create your own instruction-tuning datasets. Most publicly available
    instruction-tuning datasets are meant to augment an LLM that will be used for
    open-ended tasks, as opposed to domain-specific use cases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下微调语言网络（FLAN），这是最受欢迎的指令微调数据集之一。了解其构建方式将为您提供创建自己的指令微调数据集的路线图。大多数公开可用的指令微调数据集旨在增强用于开放性任务的
    LLM，而不是特定领域的用例。
- en: 'FLAN is actually a collection of several datasets. The [FLAN collection](https://oreil.ly/SrXV-),
    published in 2022, is composed of five components:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN 实际上是一组数据集的集合。2022 年发布的 [FLAN 集合](https://oreil.ly/SrXV-) 由五个部分组成：
- en: FLAN 2021
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FLAN 2021
- en: T0
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T0
- en: Super-natural Instructions
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超自然指令
- en: Chain-of-Thought
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链
- en: Dialog
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话
- en: The original FLAN 2021 datasets were one of the pioneering instruction-tuning
    datasets, which were used to train FLAN-T5\. The FLAN 2021 datasets were constructed
    by taking existing academic NLP datasets and converting them to the instruction
    format using instruction templates. The templates were manually constructed, with
    ten templates created for each task. The templates are available [here](https://oreil.ly/DNKCv).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 FLAN 2021 数据集是首批指令微调数据集之一，用于训练 FLAN-T5。FLAN 2021 数据集是通过将现有的学术 NLP 数据集转换为使用指令模板的指令格式构建的。这些模板是手动构建的，每个任务创建了十个模板。模板可在[此处](https://oreil.ly/DNKCv)找到。
- en: 'Here is how a template list for a task looks, as drawn from the [templates.py](https://oreil.ly/DNKCv)
    file in the FLAN GitHub repo. Our example task is text summarization on the CNN/DailyMail
    news dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个任务模板列表的示例，它来自 FLAN GitHub 仓库中的 [templates.py](https://oreil.ly/DNKCv) 文件。我们的示例任务是
    CNN/DailyMail 新闻数据集上的文本摘要：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the last three instructions represent an inverted version of the task,
    where given a summary, the model is encouraged to write the entire article. This
    has been done to increase the diversity of the instructions at scale.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后三个指令代表了任务的倒置版本，即给定一个摘要，模型被鼓励编写整个文章。这样做是为了增加大规模指令的多样性。
- en: 'Rather than painstakingly constructing these templates by hand, can we automate
    their generation using LLMs? Yes, this is possible. We can leverage LLMs to generate
    more diverse templates. When I asked my favorite LLM to generate similar instructions
    to a news summarization task template provided in the prompt, it came up with:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过使用大型语言模型来自动化这些模板的构建过程？是的，这是可能的。我们可以利用大型语言模型生成更多样化的模板。当我要求我最喜欢的 LLM 生成与提示中提供的新闻摘要任务模板类似的指令时，它提出了以下内容：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the generated templates reflect various ways of expressing the
    summarization task.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，生成的模板反映了表达摘要任务的各种方式。
- en: 'For classification tasks, it is recommended to append the instruction with
    an *Options* clause. This introduces the LLM to the output space and can thus
    concentrate the probability mass over the defined label space. Without this guidance,
    the LLM would distribute its probability across several different tokens that
    express the same concept, for example there are several different ways of expressing
    the *True* label in a binary classification task. An example prompt is: “Identify
    the tone of this text. OPTIONS: happy, sad, neutral.”'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '对于分类任务，建议在指令后附加一个 *Options* 子句。这会将 LLM 引导到输出空间，从而可以将概率质量集中在定义的标签空间上。没有这种指导，LLM
    将会将概率分布到表示相同概念的几个不同标记上，例如在二元分类任务中表达 *True* 标签有几种不同的方式。一个示例提示是：“识别这段文本的语气。OPTIONS:
    happy, sad, neutral。”'
- en: 'Constructing these prompts manually can be a tedious exercise. The [*promptsource*
    tool](https://oreil.ly/WIyOq) enables you to create, access, and apply prompts
    through a graphical user interface tool or through the promptsource Python library.
    Here is an example from the Public Pool of Prompts (P3) collection for the paraphrasing
    task, constructed by Big Science, which is available through the promptsource
    tool. P3 prompts consist of an Input template, a Target template, and an Answer
    Choices template:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建这些提示可能是一项繁琐的工作。[*promptsource* 工具](https://oreil.ly/WIyOq) 允许您通过图形用户界面工具或通过
    promptsource Python 库创建、访问和应用提示。以下是从公共提示池（P3）收集的用于改写任务的示例，由 Big Science 构建，可通过
    promptsource 工具获取。P3 提示由一个输入模板、一个目标模板和一个答案选项模板组成：
- en: '[PRE16]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Another key component of the FLAN collection is the [Super-NaturalInstructions
    dataset](https://oreil.ly/D_rv_). This dataset contains very rich descriptions
    of instructions that contain not just task definitions, but also positive and
    negative examples, constraints, and things to watch out for. The answers are enriched
    with explanations on why the answer was chosen. The effectiveness of adding explanations
    to the answer is not yet determined.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN 收集的关键组成部分之一是 [Super-NaturalInstructions 数据集](https://oreil.ly/D_rv_)。这个数据集包含了非常丰富的指令描述，不仅包括任务定义，还包括正面和负面示例、约束条件以及需要注意的事项。答案被丰富了为什么选择该答案的解释。添加解释到答案的有效性尚未确定。
- en: 'Here is an example of such a task from the Super-NaturalInstructions dataset:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是从 Super-NaturalInstructions 数据集中这样一个任务的示例：
- en: '[PRE17]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`` `Let’s now look at datasets that are constructed with the help of LLMs.`
    ``  `` `## LLM-Generated Instruction-Tuning Datasets    As seen earlier, hand-constructing
    these datasets can be painstaking, and paraphrasing/synthetic data generation
    is where LLMs shine. Therefore, we can leverage LLMs to generate our instruction-tuning
    datasets. The [Self-Instruct](https://oreil.ly/HVBfK) and [Unnatural Instructions
    papers](https://oreil.ly/1wV_G) are the first attempts in this regard. Both start
    from a seed set of high-quality hand-generated examples, and then in a few-shot
    setting, ask the LLM to generate similar examples with more diverse linguistic
    expressions.    Given an instruction, a combination of input-first and output-first
    is shown to be beneficial for generating input-output pairs. Typically, you would
    generate input-output pairs using an input-first approach, where the LLM is asked
    to generate an input instance for the given instruction and subsequently asked
    to generate the output label for that input. However, this approach might lead
    to label imbalance as shown in [Wang et al.](https://oreil.ly/hYFYH), with certain
    labels being overrepresented. Therefore, it is a good approach to mix output-first
    generation, where you ask the LLM to generate the output label first and then
    ask it to generate an input text that satisfies the label.    ###### Warning    It
    is against OpenAI’s policies to use its outputs to generate data that can be used
    to train a competing model. While there are several public instruction-tuning
    datasets that have been synthetically generated using GPT-4, they are technically
    violating OpenAI’s terms of service. I recommend using open source LLMs for synthetic
    data generation instead.    Simply asking an LLM to generate similar examples
    to your seed set may not give you the desired results. You want a diverse but
    relevant set of examples, and it is easy for your LLM to drift into territory
    that ends up generating spurious examples outside of your desired distribution.    ######
    Note    How large should your instruction-tuning dataset be? The [“LIMA: Less
    Is More for Alignment” paper](https://oreil.ly/z0BWh) shows that you need only
    a few thousand high-quality examples to effectively fine-tune a model.    Xu et
    al. propose [Evol-Instruct](https://oreil.ly/9nw3G), a structured way to generate
    these synthetic instructions by making controlled edits to the seed examples.
    The process consists of three steps:    1.  Instruction evolution: The seed examples
    are evolved using in-depth and in-breadth strategies. In-depth evolution increases
    the complexity and difficulty of the original instruction through five types of
    prompts:               *   Adding constraints                       *   Increasing
    reasoning steps                       *   Asking deeper questions                       *   Asking
    more specific questions                       *   Increasing the complexity of
    the input                                    In-breadth evolution increases topic
    coverage by generating a completely new instruction from the same domain as the
    original instruction.                   2.  Response generation: The response
    for the evolved instruction is generated, either using humans or LLMs.           3.  Candidate
    filtering: Candidate instances that do not meet quality criteria are filtered
    out. You could use either heuristics or LLMs for candidate filtering.              ######
    Note    Why not pre-train on instruction-tuning datasets? If instruction-tuning
    is a necessary step after pre-training a model, why don’t we just pre-train the
    model using an instruction-tuning dataset? It is indeed possible, but these datasets
    are hard to construct at scale without incurring a significant drop in quality.    We
    need not wait until someone releases a massive dataset to reap the benefits of
    instruction-tuning during the pre-training phase. It has been [shown](https://oreil.ly/tfO4a)
    that mixing instruction-tuning data during pre-training is beneficial.` ``  ``
    `# Summary    In this chapter, we underscored the inevitability of needing to
    fine-tune models to solve more complex tasks. We performed a deep dive of the
    fine-tuning process and highlighted the tradeoffs involved in selecting hyperparameters.
    We also showed the uncanny effectiveness of instruction-tuning along with pointers
    on how to create your own instruction-tuning datasets.    In the next chapter,
    we will discuss more advanced techniques for updating an LLM’s parameters, including
    continual pre-training, parameter efficient fine-tuning, and model merging.` ``'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`` `让我们现在来看看使用LLM（大型语言模型）帮助构建的数据集。` ``  `` `## LLM生成的指令调整数据集    如前所述，手动构建这些数据集可能非常耗时，而改写/合成数据生成正是LLM擅长的领域。因此，我们可以利用LLM来生成我们的指令调整数据集。[Self-Instruct](https://oreil.ly/HVBfK)
    和 [Unnatural Instructions](https://oreil.ly/1wV_G) 论文是这方面的首次尝试。两者都从一个高质量的手动生成示例种子集开始，然后在几样本设置中，要求LLM生成具有更多样化语言表达的类似示例。    给定一个指令，输入优先和输出优先的组合已被证明对生成输入-输出对有益。通常，您会使用输入优先的方法来生成输入-输出对，即要求LLM为给定指令生成一个输入实例，然后要求它为该输入生成输出标签。然而，这种方法可能导致标签不平衡，如[Wang等人](https://oreil.ly/hYFYH)所示，某些标签被过度表示。因此，混合输出优先的生成方法是一个好方法，其中您要求LLM首先生成输出标签，然后要求它生成满足该标签的输入文本。    ######
    警告    使用OpenAI的输出生成可用于训练竞争模型的训练数据违反了OpenAI的政策。虽然已经有一些公开的指令调整数据集是使用GPT-4合成的，但它们在技术上违反了OpenAI的服务条款。我建议使用开源LLM进行合成数据生成。    简单地要求LLM生成与种子集相似的示例可能不会得到您期望的结果。您需要一个多样化但相关的示例集，而您的LLM很容易偏离您期望的分布，生成虚假的示例。    ######
    注意    您的指令调整数据集应该有多大？[“LIMA: Less Is More for Alignment”](https://oreil.ly/z0BWh)
    论文表明，您只需要几千个高质量示例就可以有效地微调模型。    Xu等人提出了[Evol-Instruct](https://oreil.ly/9nw3G)，这是一种通过对手动种子示例进行控制编辑来生成这些合成指令的结构化方法。该过程包括三个步骤：    1.
    指令进化：使用深度和广度策略进化种子示例。深度进化通过五种类型的提示增加原始指令的复杂性和难度：               *   添加约束                       *   增加推理步骤                       *   提出更深入的问题                       *   提出更具体的问题                       *   增加输入的复杂性                                    广度进化通过从与原始指令相同领域生成完全新的指令来增加主题覆盖范围。                   2.
    响应生成：为进化后的指令生成响应，可以使用人类或LLM。           3. 候选实例过滤：过滤掉不符合质量标准的候选实例。您可以使用启发式方法或LLM进行候选实例过滤。              ######
    注意    为什么不在指令调整数据集上预训练？如果指令调整是预训练模型后的一个必要步骤，为什么我们不直接使用指令调整数据集来预训练模型？这确实可能，但这些数据集在规模上很难构建，而不会导致质量显著下降。    我们不需要等到有人发布一个大规模数据集，就可以在预训练阶段获得指令调整的好处。已经[证明](https://oreil.ly/tfO4a)，在预训练期间混合指令调整数据是有益的。`
    ``  `` `# 摘要    在本章中，我们强调了需要微调模型以解决更复杂任务的必然性。我们对微调过程进行了深入研究，并强调了在选择超参数时涉及的权衡。我们还展示了指令调整的非凡有效性，并指出了如何创建您自己的指令调整数据集。    在下一章中，我们将讨论更新LLM参数的更高级技术，包括持续预训练、参数高效的微调和模型合并。`
    ``'
