- en: Chapter 16\. Simulating a Self-Driving Car Using End-to-End Deep Learning with
    Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章。使用Keras进行端到端深度学习模拟自动驾驶汽车
- en: 'Contributed by guest authors: Aditya Sharma and Mitchell Spryn'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 由客座作者Aditya Sharma和Mitchell Spryn贡献
- en: 'From the Batmobile to Knightrider, from robotaxis to autonomous pizza delivery,
    self-driving cars have captured the imagination of both modern pop culture and
    the mainstream media. And why wouldn’t they? How often does it happen that a science-fiction
    concept is brought to life? More important, autonomous cars promise to address
    some key challenges urban societies are facing today: road accidents, pollution
    levels, cities becoming increasingly more congested, hours of productivity lost
    in traffic, and the list goes on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从蝙蝠车到骑士骑手，从机器人出租车到自动送披萨，自动驾驶汽车已经吸引了现代流行文化和主流媒体的想象力。为什么不呢？科幻概念何时会变为现实？更重要的是，自动驾驶汽车承诺解决当今城市社会面临的一些关键挑战：道路事故、污染水平、城市越来越拥挤、在交通中浪费的生产力小时数等等。
- en: 'It should come as no surprise that a full self-driving system is quite complex
    to build, and cannot be tackled in one book chapter. Much like an onion, any complex
    problem contains layers that can be peeled. In our case, we intend to tackle one
    fundamental problem here: how to steer. Even better, we don’t need a real car
    to do this. We will be training a neural network to drive our car autonomously
    within a simulated environment, using realistic physics and visuals.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，一个完整的自动驾驶系统构建起来相当复杂，不可能在一章中解决。就像一个洋葱一样，任何复杂的问题都包含可以剥离的层。在我们的情况下，我们打算解决一个基本问题：如何转向。更好的是，我们不需要一辆真正的汽车来做到这一点。我们将训练一个神经网络在模拟环境中自主驾驶我们的汽车，使用逼真的物理和视觉效果。
- en: But first, a brief bit of history.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，简短的历史。
- en: '![The SAE Levels of Driving Automation (image source)](../images/00220.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![SAE驾驶自动化级别（图片来源）](../images/00220.jpeg)'
- en: Figure 16-1\. The SAE Levels of Driving Automation ([image source](https://oreil.ly/RHKUt))
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1。SAE驾驶自动化级别（图片来源）
- en: A Brief History of Autonomous Driving
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶的简史
- en: Even though it might seem like autonomous driving is a very recent technological
    development, research in this field originated more than 30 years ago. Scientists
    at Carnegie Mellon University first tried their hands at this seemingly impossible
    feat back in 1984 when they started working on what would be known as the Navlab
    1\. “Navlab” was a shorthand for Carnegie Mellon’s Navigation Laboratory, a not-so-fancy
    name for what would turn out to be the first step into a very impressive future
    for humanity. The Navlab 1 was a Chevrolet panel van ([Figure 16-2](part0019.html#the_navlab_1_in_all_its_glory_left_paren))
    with racks of hardware onboard that included, among other things, workstations
    for the research team and a Sun Microsystems supercomputer. It was considered
    cutting edge at the time, a self-contained computing system on wheels. Long-distance
    wireless communication was pretty much nonexistent back then, and cloud computing
    was not even a concept. Putting the entire research team in the back of a highly
    experimental self-driving van might seem like a dangerous idea, but the Navlab
    had a maximum speed of 20 MPH and navigated on only empty roads. This ensured
    the safety of the scientists working onboard. What is very impressive to note
    though, is that the technology used in this car laid the foundation for technology
    used in the self-driving cars of today. For example, Navlab 1 used video cameras
    and a rangefinder (an early version of lidar) to observe its surroundings. For
    navigation, they used a single-layer neural network to predict steering angles
    given sensor data. Pretty neat isn’t it?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动驾驶似乎是一个非常近期的技术发展，但这个领域的研究起源于30多年前。卡内基梅隆大学的科学家们在1984年首次尝试了这个看似不可能的壮举，当时他们开始研究后来被称为Navlab
    1的项目。 “Navlab”是卡内基梅隆大学导航实验室的简称，这个名字并不花哨，但它将成为人类未来非常令人印象深刻的第一步。Navlab 1是一辆雪佛兰面包车，车上装有硬件机架，其中包括研究团队的工作站和Sun
    Microsystems超级计算机等设备。当时它被认为是尖端技术，是一个自包含的移动计算系统。当时远程无线通信几乎不存在，云计算甚至还不是一个概念。把整个研究团队放在一辆高度实验性的自动驾驶面包车后面可能看起来像是一个危险的想法，但Navlab的最高时速只有20英里，只在空旷的道路上行驶。这确保了工作在车上的科学家们的安全。但非常令人印象深刻的是，这辆车使用的技术为今天的自动驾驶汽车所用的技术奠定了基础。例如，Navlab
    1使用视频摄像头和测距仪（激光雷达的早期版本）来观察周围环境。为了导航，他们使用了单层神经网络来根据传感器数据预测转向角度。相当不错，不是吗？
- en: '![The NavLab 1 in all its glory (image source)](../images/00006.jpeg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![NavLab 1的全貌（图片来源）](../images/00006.jpeg)'
- en: Figure 16-2\. The NavLab 1 in all its glory ([image source](https://oreil.ly/b2Bnn))
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2。NavLab 1的全貌（图片来源）
- en: Deep Learning, Autonomous Driving, and the Data Problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习、自动驾驶和数据问题
- en: Even 35 years ago scientists knew that neural networks were going to play a
    key role in making self-driving cars a reality. Back then, of course, we did not
    have the technology needed (GPUs, cloud computing, FPGAs, etc.) to train and deploy
    deep neural networks at scale for autonomous driving to become a reality. Today’s
    autonomous cars use deep learning to perform all kinds of tasks. [Table 16-1](part0019.html#examples_of_deep_learning_applications_i)
    lists some examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 即使35年前科学家们就知道神经网络将在实现自动驾驶汽车方面发挥关键作用。当时，当然我们没有所需的技术（GPU、云计算、FPGA等）来训练和部署大规模的深度神经网络，以使自动驾驶成为现实。今天的自动驾驶汽车使用深度学习来执行各种任务。表16-1列出了一些示例。
- en: Table 16-1\. Examples of deep learning applications in self-driving cars
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表16-1。深度学习在自动驾驶汽车中的应用示例
- en: '| **Task** | **Examples** |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **示例** |'
- en: '| --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Perception** | Detect drivable area, lane markings, intersections, crosswalks,
    etc. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **感知** | 检测可驾驶区域、车道标线、交叉口、人行横道等 |'
- en: '| Detect other vehicles, pedestrians, animals, objects on the road, etc. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 检测其他车辆、行人、动物、道路上的物体等 |'
- en: '| Detect traffic signs and signals |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 检测交通标志和信号 |'
- en: '| **Navigation** | Predict and output steering angle, throttle, etc., based
    on sensor input |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **导航** | 根据传感器输入预测和输出转向角、油门等 |'
- en: '| Perform overtaking, lane changes, U-turns, etc. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 进行超车、变道、掉头等操作 |'
- en: '| Perform merges, exits, navigate roundabouts, etc. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 进行并线、出口、穿越环岛等操作 |'
- en: '| Drive in tunnels, over bridges, etc. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 在隧道、桥梁等地方驾驶 |'
- en: '| Respond to traffic rule breakers, wrong-way drivers, unexpected environment
    changes, etc. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 响应交通违规者、逆行驾驶者、意外环境变化等 |'
- en: '| Navigate stop-and-go traffic |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 穿梭于交通拥堵中 |'
- en: We know that deep learning requires data. In fact, a general rule of thumb is
    that more data ensures better model performance. In previous chapters, we saw
    that training image classifiers can require millions of images. Imagine how much
    data it would take to train a car to drive itself. Things are also very different
    with self-driving cars when it comes to the data needed for validation and testing.
    Not only must a car be able to drive, it must do it safely. Testing and validating
    model performance is therefore fundamentally critical and requires a lot more
    data than what is needed for training, unlike traditional deep learning problems.
    However, it is difficult to accurately predict the exact amount of data needed,
    and estimates vary. [A 2016 study](https://oreil.ly/mOUeJ) shows that it would
    take 11 billion miles of driving for a car to become as good as a human driver.
    For a fleet of 100 self-driving cars collecting data 24 hours per day, 365 days
    per year at an average speed of 25 miles per hour, this would take more than 500
    years!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道深度学习需要数据。事实上，一个经验法则是更多的数据可以确保更好的模型性能。在之前的章节中，我们看到训练图像分类器可能需要数百万张图像。想象一下训练一辆汽车自动驾驶需要多少数据。当涉及到验证和测试所需的数据时，自动驾驶汽车的情况也非常不同。汽车不仅必须能够行驶，还必须安全行驶。因此，测试和验证模型性能是基本关键的，需要比传统深度学习问题所需的数据多得多。然而，准确预测所需数据的确切数量是困难的，估计各不相同。[一项2016年的研究](https://oreil.ly/mOUeJ)显示，一辆汽车要想达到与人类驾驶员一样的水平，需要行驶110亿英里。对于一队100辆自动驾驶汽车，每天24小时收集数据，以25英里每小时的平均速度行驶，这将需要超过500年！
- en: 'Now, of course, it is highly impractical and costly to collect 11 billion miles
    of data by having fleets of cars drive around in the real world. This is why almost
    everyone working in this space—be it a large automaker or a startup—uses simulators
    to collect data and validate trained models. [Waymo](https://waymo.com/tech) (Google’s
    self-driving car team), for example, had driven nearly 10 million miles on the
    road as of late 2018\. Even though this is more than any other company on the
    planet it represents less than one percent of our 11-billion-mile figure. On the
    other hand, Waymo has driven seven billion miles in simulation. Although everyone
    uses simulation for validation, some companies build their own simulation tools,
    whereas others license them from companies like Cognata, Applied Intuition, and
    AVSimulation. There are some great open source simulation tools available as well:
    [AirSim](https://github.com/Microsoft/AirSim) from Microsoft, [Carla](http://carla.org/)
    from Intel, and [Apollo simulation](https://oreil.ly/rag7v) from Baidu. Thanks
    to these tools, we don’t need to connect the CAN bus of our car to learn the science
    behind making it drive itself.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，通过让车队在现实世界中行驶110亿英里来收集数据是非常不切实际和昂贵的。这就是为什么几乎每个在这个领域工作的人——无论是大型汽车制造商还是初创公司——都使用模拟器来收集数据和验证训练模型。例如，[Waymo](https://waymo.com/tech)（谷歌的自动驾驶汽车团队）截至2018年底在道路上行驶了近1000万英里。尽管这比地球上任何其他公司都多，但它仅占我们110亿英里的不到1%。另一方面，Waymo在模拟中行驶了70亿英里。尽管每个人都使用模拟进行验证，但有些公司构建自己的模拟工具，而其他公司则从Cognata、Applied
    Intuition和AVSimulation等公司许可。还有一些很棒的开源模拟工具可用：来自微软的[AirSim](https://github.com/Microsoft/AirSim)，来自英特尔的[Carla](http://carla.org/)，以及来自百度的[Apollo模拟](https://oreil.ly/rag7v)。由于有了这些工具，我们不需要连接我们汽车的CAN总线来学习让它自动驾驶的科学。
- en: In this chapter, we use a custom-built version of AirSim, built specifically
    for Microsoft’s [Autonomous Driving Cookbook](https://oreil.ly/uzOGl).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个专为微软的[自动驾驶食谱](https://oreil.ly/uzOGl)定制的AirSim版本。
- en: 'The “Hello, World!” of Autonomous Driving: Steering Through a Simulated Environment'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶的“Hello, World！”：在模拟环境中操纵方向盘
- en: 'In this section, we implement the “Hello, World!” problem of autonomous driving.
    Self-driving cars are complex, with dozens of sensors streaming gigabytes of data
    and multiple decisions being made while driving down a road. Much like programming,
    for the “Hello, World!” of self-driving cars, we strip the requirements down to
    basic fundamentals:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们实现了自动驾驶的“Hello, World！”问题。自动驾驶汽车是复杂的，有数十个传感器传输着大量数据，同时在行驶中做出多个决策。就像编程一样，对于自动驾驶汽车的“Hello,
    World！”，我们将要求简化为基本原理：
- en: The car always stays on the road.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车始终保持在道路上。
- en: For external sensor input, the car uses a single image frame from a single camera
    mounted on the front of the hood. No other sensors (lidar, radar, etc.) are used.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于外部传感器输入，汽车使用安装在前引擎盖上的单个摄像头的单个图像帧。不使用其他传感器（激光雷达、雷达等）。
- en: Based on this single-image input, going at a constant speed, the car predicts
    its output steering angle. No other possible outputs (brake, throttle, gear changes,
    etc.) are predicted.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于这个单图像输入，以恒定速度行驶，汽车预测其输出的转向角。不会预测其他可能的输出（刹车、油门、换挡等）。
- en: There are no other vehicles, pedestrians, animals, or anything else on the road
    or relevant surroundings.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路上没有其他车辆、行人、动物或其他任何东西，也没有相关的环境。
- en: The road being driven on is single lane, with no markings or traffic signs and
    signals and no rules for staying on the left or right side of the road.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所行驶的道路是单车道，没有标线、交通标志和信号，也没有保持在道路左侧或右侧的规则。
- en: The road mostly changes in terms of turns (which will require changing steering
    angle to navigate) and not in terms of elevation (which would require a change
    in throttle, applying brakes, etc.).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路主要在转弯方面变化（这将需要改变转向角来导航），而不是在高程方面变化（这将需要改变油门，刹车等）。
- en: Setup and Requirements
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和要求
- en: We will be implementing the “Hello, World!” problem in AirSim’s Landscape map
    ([Figure 16-4](part0019.html#the_landscape_map_in_airsim)). AirSim is an open
    source photo-realistic simulation platform and is a popular research tool for
    training data collection and validation of deep learning–based autonomous system
    model development.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在AirSim的景观地图中实现“Hello, World!”问题（[图16-4](part0019.html#the_landscape_map_in_airsim)）。AirSim是一个开源的逼真模拟平台，是一个用于训练数据收集和验证基于深度学习的自主系统模型开发的研究工具。
- en: '![The Landscape map in AirSim](../images/00252.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![AirSim中的景观地图](../images/00252.jpeg)'
- en: Figure 16-4\. The Landscape map in AirSim
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4。AirSim中的景观地图
- en: 'You can find the code for this chapter in the [end-to-end deep learning tutorial](https://oreil.ly/_IJl9)
    from the [*Autonomous Driving Cookbook*](https://oreil.ly/uF5Bz) on GitHub in
    the form of Jupyter Notebooks. In the cookbook repository, go to [*AirSimE2EDeepLearning/*](https://oreil.ly/YVKPp).
    We can do so by running the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的[*Autonomous Driving Cookbook*](https://oreil.ly/uF5Bz)中找到本章的代码，形式为Jupyter笔记本的[端到端深度学习教程](https://oreil.ly/_IJl9)。在食谱仓库中，转到[*AirSimE2EDeepLearning/*](https://oreil.ly/YVKPp)。我们可以通过运行以下命令来实现：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use Keras, a library with which we are already familiar, to implement
    our neural network. It’s not necessary for us to learn any new deep learning concepts
    to work on this problem other than what has already been introduced in prior chapters
    of this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras这个我们已经熟悉的库来实现我们的神经网络。在这个问题上，除了本书之前章节介绍的内容，我们不需要学习任何新的深度学习概念。
- en: For this chapter, we have created a dataset by driving around in AirSim. The
    uncompressed size of the dataset is 3.25 GB. This is a little larger than datasets
    we’ve been using, but remember, we are implementing the “Hello, World!” of a highly
    complex problem. For comparison, it is fairly normal practice for automakers to
    collect multiple petabytes of data on the road every day. You can download the
    dataset by going to [*aka.ms/AirSimTutorialDataset*](https://aka.ms/AirSimTutorialDataset).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们通过在AirSim中驾驶来创建了一个数据集。数据集的未压缩大小为3.25 GB。这比我们之前使用的数据集稍大一些，但请记住，我们正在实现一个非常复杂问题的“Hello,
    World!”。相比之下，汽车制造商每天在道路上收集多个PB的数据是相当正常的做法。您可以通过访问[*aka.ms/AirSimTutorialDataset*](https://aka.ms/AirSimTutorialDataset)来下载数据集。
- en: We can run the provided code and train our model on a Windows or Linux machine.
    We created a standalone build of AirSim for this chapter, which you can download
    from [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage).
    After you’ve downloaded it, extract the simulator package to the location of your
    choice. Take note of the path because you will need it later. Please note that
    this build runs only on Windows but is needed only to see our final trained model
    in action. If you’re using Linux, you can take the model files and run them on
    a Windows machine with the provided simulator package.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Windows或Linux机器上运行提供的代码并训练我们的模型。我们为本章创建了AirSim的独立构建，您可以从[*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)下载。下载后，请将模拟器包提取到您选择的位置。请注意路径，因为您以后会需要它。请注意，此构建仅在Windows上运行，但仅需要查看我们最终训练好的模型。如果您使用Linux，可以将模型文件拷贝到提供的模拟器包的Windows机器上运行。
- en: AirSim is a highly photo-realistic environment, which means it is capable of
    generating lifelike pictures of the environment for the model to train on. You
    may have encountered similar graphics while playing high-definition video games.
    Given the size of the data and the graphic quality of the provided simulator package,
    a GPU is certainly preferable for running the code as well as the simulator.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: AirSim是一个高度逼真的环境，这意味着它能够生成逼真的环境图片供模型训练。在玩高清视频游戏时，你可能遇到过类似的图形。考虑到提供的模拟器包的数据量和图形质量，GPU肯定是运行代码和模拟器的首选。
- en: Finally, there are some additional tools and Python dependencies needed to run
    the code provided. You can find the details for these at [Environment Setup in
    the README file](https://oreil.ly/RzZe7) in the code repository. At a high level,
    we need Anaconda with Python 3.5 (or higher), TensorFlow (to run as a backend
    to Keras), and h5py (for storing and reading data and model files). After we have
    our Anaconda environment set up, we can install the needed Python dependencies
    by running the *InstallPackages.py* file as root or administrator.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行提供的代码还需要一些额外的工具和Python依赖项。您可以在代码仓库的[README文件中的环境设置](https://oreil.ly/RzZe7)中找到这些详细信息。在高层次上，我们需要Anaconda与Python
    3.5（或更高版本）、TensorFlow（作为Keras的后端运行）和h5py（用于存储和读取数据和模型文件）。设置好Anaconda环境后，我们可以通过以root或管理员身份运行*InstallPackages.py*文件来安装所需的Python依赖项。
- en: '[Table 16-2](part0019.html#setup_and_requirements_summary) provides a summary
    of all the requirements that we just defined.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[表16-2](part0019.html#setup_and_requirements_summary)提供了我们刚刚定义的所有要求的摘要。'
- en: Table 16-2\. Setup and requirements summary
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表16-2。设置和要求摘要
- en: '| **Item** | **Requirements/Link** | **Notes/Comments** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **要求/链接** | **备注/评论** |'
- en: '| --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Code repository | [*https://oreil.ly/_IJl9*](https://oreil.ly/_IJl9) | Can
    be run on a Windows or Linux machine |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 代码仓库 | [*https://oreil.ly/_IJl9*](https://oreil.ly/_IJl9) | 可在Windows或Linux机器上运行
    |'
- en: '| Jupyter Notebooks used | [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6yvCq)[*TrainModel.ipynb*](https://oreil.ly/rcR47)[*TestModel.ipynb*](https://oreil.ly/FE-EP)
    |   |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 使用的Jupyter笔记本 | [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6yvCq)[*TrainModel.ipynb*](https://oreil.ly/rcR47)[*TestModel.ipynb*](https://oreil.ly/FE-EP)
    |   |'
- en: '| Dataset download | [*aka.ms/AirSimTutorialDataset*](http://aka.ms/AirSimTutorialDataset)
    | 3.25 GB in size; needed to train model |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 数据集下载 | [*aka.ms/AirSimTutorialDataset*](http://aka.ms/AirSimTutorialDataset)
    | 大小为3.25 GB；用于训练模型 |'
- en: '| Simulator download | [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)
    | Not needed to train model, only to deploy and run it; runs only on Windows |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 模拟器下载 | [*aka.ms/ADCookbookAirSimPackage*](http://aka.ms/ADCookbookAirSimPackage)
    | 不需要用于训练模型，只用于部署和运行；仅在Windows上运行 |'
- en: '| GPU | Recommended for training, required for running simulator |   |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 推荐用于训练，运行模拟器所需 |   |'
- en: '| Other tools + Python dependencies | [Environment Setup section in README
    file in the repository](https://oreil.ly/RzZe7) |   |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 其他工具+Python依赖项 | [存储库中README文件中的环境设置部分](https://oreil.ly/RzZe7) |   |'
- en: With all that taken care of, let’s get started!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些准备工作，让我们开始吧！
- en: Data Exploration and Preparation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索和准备
- en: As we journey deeper into the world of neural networks and deep learning, you
    will notice that much of the magic of machine learning doesn’t come from how a
    neural network is built and trained, but from the data scientist’s understanding
    of the problem, the data, and the domain. Autonomous driving is no different.
    As you will see shortly, having a deep understanding of the data and the problem
    we are trying to solve is key when teaching our car how to drive itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入探索神经网络和深度学习的世界时，您会注意到，机器学习的许多魔力并不是来自神经网络是如何构建和训练的，而是来自数据科学家对问题、数据和领域的理解。自动驾驶也不例外。正如您很快将看到的，深入了解数据和我们试图解决的问题对于教会我们的汽车如何自动驾驶至关重要。
- en: 'All the steps here are also detailed in the Jupyter Notebook [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6aE7z).
    We start by importing all the necessary modules for this part of the exercise:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的所有步骤也都在Jupyter Notebook [*DataExplorationAndPreparation.ipynb*](https://oreil.ly/6aE7z)中详细说明。我们首先导入这部分练习所需的所有必要模块：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we make sure our program can find the location of our unzipped dataset.
    We also provide it a location to store our processed (or “cooked”) data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们确保我们的程序可以找到我们解压缩的数据集的位置。我们还为其提供一个位置来存储我们处理（或“烹饪”）的数据：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Looking at the provided dataset we will see that it contains nine folders:
    *normal_1* through *normal_6* and *swerve_1* through *swerve_3*. We come back
    to the naming of these folders later. Within each of these folders are images
    as well as *.tsv* (or *.txt*) files. Let’s check out one of these images:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 查看提供的数据集，我们会看到它包含九个文件夹：*normal_1*到*normal_6*和*swerve_1*到*swerve_3*。稍后我们会回到这些文件夹的命名。在每个文件夹中都有图像以及*.tsv*（或*.txt*）文件。让我们看看其中一个图像：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We should see the following output. We can see that this image was captured
    by a camera placed centrally on the hood of the car (which is slightly visible
    at the bottom in [Figure 16-5](part0019.html#plot_showing_the_contents_of_the_file_no)).
    We can also see the Landscape environment used for this problem from the simulator.
    Notice that no other vehicles or objects are on the road, keeping in line with
    our requirements. Although the road appears to be snowy, for our experiment, the
    snow is only visual and not physical; that is, it will have no effect on the physics
    and movement of our car. Of course, in the real world, it is critical to accurately
    replicate the physics of snow, rain, etc., so that our simulators can produce
    highly accurate versions of reality. This is a very complicated task to undertake
    and many companies dedicate a lot of resources to it. Thankfully for our purpose,
    we can safely ignore all that and treat the snow as just white pixels in our image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出。我们可以看到这张图像是由放置在汽车引擎盖中央的摄像头拍摄的（在[图16-5](part0019.html#plot_showing_the_contents_of_the_file_no)中底部略有可见）。我们还可以从模拟器中看到用于这个问题的景观环境。请注意，道路上没有其他车辆或物体，符合我们的要求。尽管道路看起来是多雪的，但对于我们的实验来说，雪只是视觉上的，不是实际的；也就是说，它不会对我们汽车的物理和运动产生影响。当然，在现实世界中，准确复制雪、雨等的物理特性非常重要，以便我们的模拟器可以生成高度准确的现实版本。这是一个非常复杂的任务，许多公司都投入了大量资源。幸运的是，对于我们的目的，我们可以安全地忽略所有这些，并将雪视为图像中的白色像素。
- en: '![Plot showing the contents of the file normal_1/images/img_0.png](../images/00211.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![显示文件normal_1/images/img_0.png的内容的图](../images/00211.jpeg)'
- en: Figure 16-5\. Plot showing the contents of the file normal_1/images/img_0.png
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-5\. 显示文件normal_1/images/img_0.png的内容的图
- en: 'Let’s also get a preview of what the rest of the data looks like by displaying
    the contents of one of the *.tsv/.txt* files:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过显示一个*.tsv/.txt*文件的内容来预览其余数据是什么样的：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We should see the following output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '![Image](../images/00177.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/00177.jpeg)'
- en: 'There is a lot going on here, so let’s break it down. First, we can see that
    each row in the table corresponds to an image frame (we are displaying only the
    first five frames here). Because this is at the beginning of our data collection
    drive, we can see that the car hasn’t started moving yet: the speed and throttle
    are 0 and the gear is neutral. Because we are trying to predict only steering
    angles for our problem, we won’t be predicting the speed, throttle, brake, or
    gear values. These values will be used, however, as part of the input data (more
    on this later).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容，让我们来分解一下。首先，我们可以看到表中的每一行对应一个图像帧（这里我们只显示了前五帧）。因为这是我们数据收集驱动的开始，我们可以看到汽车还没有开始移动：速度和油门都是0，档位是空档。因为我们只是尝试预测问题中的转向角，我们不会预测速度、油门、刹车或档位的值。然而，这些值将作为输入数据的一部分（稍后会详细介绍）。
- en: Identifying the Region of Interest
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别感兴趣区域
- en: Let’s get back to our sample image now. All the observations we made so far
    were from the eyes of a human being. For a moment, let’s take a look at the image
    again, only this time we step into the shoes (or tires) of a car that is just
    beginning to learn how to drive itself and is trying to understand what it is
    seeing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在回到我们的示例图像。到目前为止，我们所做的所有观察都是从人类的眼睛看到的。让我们再次看一下这幅图像，只是这一次我们换成了一辆刚开始学习自动驾驶并试图理解所看到的东西的汽车的角度（或轮胎）。
- en: If I’m the car, looking at this image that my camera presented me, I can divide
    it into three distinct parts ([Figure 16-6](part0019.html#the_three_parts_of_the_image_as_seen_by)).
    First, there is the lower third, which more or less looks consistent. It is made
    up of mostly straight lines (the road, lane divider, road fences, etc.) and has
    a uniform coloring of white, black, gray, and brown. There is also a weird black
    arc at the very bottom (this is the hood of the car). The upper third of the image,
    like the lower third, is also consistent. It is mostly gray and white (the sky
    and the clouds). Lastly, there is the middle third and it has a lot going on.
    There are big brown and grey shapes (mountains), which are not uniform at all.
    There are also four tall green figures (trees), their shape and color are different
    from anything else I see, so they must be super important. As I am presented more
    images, the upper third and the lower third don’t change all that much but the
    middle third undergoes a lot of change. Hence, I conclude that this is the part
    that I should be focusing on the most.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我是汽车，看着我的摄像头呈现给我的这张图像，我可以将其分为三个不同的部分（[图16-6](part0019.html#the_three_parts_of_the_image_as_seen_by)）。首先是下面的第三部分，看起来更或多少一致。它主要由直线（道路、车道分隔线、道路围栏等）组成，颜色一致，白色、黑色、灰色和棕色。底部还有一个奇怪的黑色弧线（这是汽车的引擎盖）。图像的上面第三部分，与下面的第三部分一样，也是一致的。主要是灰色和白色（天空和云）。最后，中间的第三部分有很多事情发生。有大的棕色和灰色形状（山），它们完全不一致。还有四个高大的绿色图形（树），它们的形状和颜色与我看到的其他任何东西都不同，所以它们一定非常重要。随着我看到更多的图像，上面的第三部分和下面的第三部分并没有太大变化，但中间的第三部分发生了很多变化。因此，我得出结论，这是我应该最关注的部分。
- en: '![The three parts of the image as seen by the self-driving car](../images/00091.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![自动驾驶汽车所看到的图像的三个部分](../images/00091.jpeg)'
- en: Figure 16-6\. The three parts of the image as seen by the self-driving car
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-6. 自动驾驶汽车所看到的图像的三个部分
- en: Do you see the challenge in front of our car? It has no way of knowing what
    part of the image being presented to it is important. It might try to associate
    the changing steering angles with the changing scenery instead of focusing on
    turns in the road, which are very subtle compared to everything else that is going
    on. Not only are the changes in the scenery confusing, but they are also not relevant
    to the problem we are trying to solve here at all. The sky above, although mostly
    unchanging, also does not provide us with any information relevant to the steering
    of our car. Finally, the part of the hood being captured in every picture is also
    nonrelevant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到我们汽车面临的挑战了吗？它无法知道呈现给它的图像的哪一部分是重要的。它可能会尝试将转向角度的变化与景观的变化联系起来，而不是专注于道路的转弯，这与其他事物相比非常微妙。景观的变化不仅令人困惑，而且与我们正在解决的问题毫不相关。尽管天空大部分时间都没有变化，但它也不提供与我们汽车转向相关的任何信息。最后，每张照片中捕捉到的引擎盖部分也是不相关的。
- en: 'Let us fix this by focusing only on the relevant portion of the image. We do
    this by creating a region of interest (ROI) in our image. As we can imagine, there
    is no hard-and-fast rule to dictate what our ROI should look like. It really depends
    on our use case. For us, because the camera is in a fixed position and there are
    no elevation changes on the road, the ROI is a simple rectangle focusing on the
    road and road boundaries. We can see the ROI by running the following code snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过仅关注图像的相关部分来解决这个问题。我们通过在图像中创建一个感兴趣区域（ROI）来实现这一点。正如我们可以想象的那样，没有一个硬性规则来规定我们的ROI应该是什么样子的。这真的取决于我们的用例。对于我们来说，因为摄像头处于固定位置，道路上没有高度变化，ROI是一个简单的矩形，专注于道路和道路边界。我们可以通过运行以下代码片段来查看ROI：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We should see the output shown in [Figure 16-7](part0019.html#the_roi_for_our_car_to_focus_on_during_t).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到[图16-7](part0019.html#the_roi_for_our_car_to_focus_on_during_t)中显示的输出。
- en: '![The ROI for our car to focus on during training](../images/00109.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![我们的汽车在训练期间应关注的ROI](../images/00109.jpeg)'
- en: Figure 16-7\. The ROI for our car to focus on during training
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-7. 我们的汽车在训练期间应关注的ROI
- en: Our model will focus only on the road now and not be confused by anything else
    going on in the environment. This also reduces the size of the image by half,
    which will make training our neural network easier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在将只关注道路，不会被环境中其他任何事情所困扰。这也将图像的大小减少了一半，这将使我们的神经网络训练更容易。
- en: Data Augmentation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: As mentioned earlier, it is always better to have as much data as we can get
    our hands on while training deep learning models. We have seen in previous chapters
    the importance of data augmentation and how it helps to not only get more training
    data but to also avoid overfitting. Most data augmentation techniques discussed
    previously for image classification, however, will not be useful for our current
    autonomous driving problem. Let’s take the example of rotation. Rotating images
    randomly by 20 degrees is very helpful when training a classifier for a smartphone
    camera, but the camera on the hood of our car is fixed in place and will never
    see a rotated image (unless our car is doing flips, at which point we have larger
    concerns). The same goes for random shifts; we will never encounter those while
    driving. There is, however, something else we can do to augment the data for our
    current problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，在训练深度学习模型时，尽可能获得尽可能多的数据总是更好的。我们已经在之前的章节中看到了数据增强的重要性，以及它如何帮助我们不仅获得更多的训练数据，还可以避免过拟合。然而，先前讨论的大多数用于图像分类的数据增强技术对于我们当前的自动驾驶问题并不适用。让我们以旋转为例。随机将图像旋转20度在训练智能手机相机分类器时非常有帮助，但是我们汽车引擎盖上的摄像头固定在一个位置，永远不会看到旋转的图像（除非我们的汽车在翻转，那时我们有更大的问题）。随机移位也是如此；我们在驾驶时永远不会遇到这些。然而，对于我们当前的问题，我们可以做一些其他事情来增强数据。
- en: Looking closely at our image, we can observe that flipping it on the y-axis
    produces an image that could have easily come from a different test run ([Figure 16-8](part0019.html#flipping_image_on_the_y-axis)).
    We can effectively double the size of our dataset if we used the flipped versions
    of images along with their regular ones. Flipping an image this way will require
    us to modify the steering angle associated with it as well. Because our new image
    is a reflection of our original, we can just change the sign of the corresponding
    steering angle (e.g., from 0.212 to –0.212).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察我们的图像，我们可以看到在y轴上翻转它会产生一个看起来很容易来自不同测试运行的图像（[图16-8](part0019.html#flipping_image_on_the_y-axis)）。如果我们使用图像的翻转版本以及它们的常规版本，我们可以有效地将数据集的大小加倍。以这种方式翻转图像将要求我们同时修改与之相关的转向角。因为我们的新图像是原始图像的反射，所以我们只需改变相应转向角的符号（例如，从0.212变为-0.212）。
- en: '![Flipping image on the y-axis](../images/00048.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![在y轴上翻转图像](../images/00048.jpeg)'
- en: Figure 16-8\. Flipping image on the y-axis
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-8。在y轴上翻转图像
- en: There is another thing we can do to augment our data. Autonomous cars need to
    be ready for not only changes on the roads, but also for changes in external conditions
    like weather, time of day, and available light. Most simulators available today
    allow us to create these conditions synthetically. All of our data was collected
    in bright lighting conditions. Instead of going back to the simulator to collect
    more data under a variety of lighting conditions, we could just introduce random
    lighting changes by adjusting image brightness while training on the data we have.
    [Figure 16-9](part0019.html#reducing_image_brightness_by_40percent) shows an example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做另一件事来增加我们的数据。自动驾驶汽车不仅需要准备好应对道路上的变化，还需要准备好应对外部条件的变化，如天气、时间和光照条件。今天大多数可用的模拟器都允许我们合成这些条件。我们所有的数据都是在明亮的光照条件下收集的。我们可以在训练过程中通过调整图像亮度引入随机光照变化，而不是返回模拟器收集更多不同光照条件下的数据。[图16-9](part0019.html#reducing_image_brightness_by_40percent)展示了一个例子。
- en: '![Reducing image brightness by 40%](../images/00075.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![将图像亮度降低40%](../images/00075.jpeg)'
- en: Figure 16-9\. Reducing image brightness by 40%
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-9。将图像亮度降低40%
- en: Dataset Imbalance and Driving Strategies
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集不平衡和驾驶策略
- en: Deep learning models are only as good as the data on which they are trained.
    Unlike humans, the AI of today cannot think and reason for itself. Hence, when
    presented with a completely new situation it will fall back on what it has seen
    before and make predictions based on the dataset it was trained on. Additionally,
    the statistical nature of deep learning makes it ignore instances that happen
    infrequently as aberrations and outliers, even if they are of significance. This
    is called *dataset imbalance*, and it is a common nuisance for data scientists.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的好坏取决于它们所训练的数据。与人类不同，今天的人工智能无法自行思考和推理。因此，当面对完全新的情况时，它会回归到以前看到的内容，并基于其训练的数据集进行预测。此外，深度学习的统计特性使其忽略那些不经常发生的实例，将其视为异常值和离群值，即使它们具有重要意义。这被称为*数据集不平衡*，对数据科学家来说是一个常见的困扰。
- en: 'Imagine that we are trying to train a classifier that looks at dermatoscopic
    images to detect whether a lesion is benign or malignant. Suppose that our dataset
    has one million images, 10,000 of which contain lesions that are malignant. It
    is very likely that the classifier we train on this data always predicts images
    to be benign and never malignant. This happens because deep learning algorithms
    are designed to minimize error (or maximize accuracy) while training. By classifying
    all images as benign, our model attains 99% accuracy and calls it a day, while
    severely failing at the job it was trained to perform: detecting cancerous lesions.
    This problem is usually solved by training predictors on a more balanced dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们正在尝试训练一个分类器，该分类器查看皮肤镜图像以检测病变是良性还是恶性。假设我们的数据集有一百万张图像，其中有1万张包含恶性病变。很可能我们在这些数据上训练的分类器总是预测图像是良性的，从不预测是恶性的。这是因为深度学习算法旨在在训练过程中最小化错误（或最大化准确性）。通过将所有图像分类为良性，我们的模型达到了99%的准确性，然后结束训练，严重失败于其训练的任务：检测癌症病变。这个问题通常通过在更平衡的数据集上训练预测器来解决。
- en: Autonomous driving is not immune to the problem of dataset imbalance. Let’s
    take our steering-angle prediction model. What do we know about steering angles
    from our day-to-day driving experience? While driving down a road we mostly drive
    straight. If our model was trained on only normal driving data, it would never
    learn how to navigate turns due to their relative scarcity in the training dataset.
    To solve this, it is important that our dataset contains data not only for a normal
    driving strategy but also for a “swerved” driving strategy in a statistically
    significant amount. To illustrate what we mean by this, let’s go back to our *.tsv*/*.txt*
    files. Earlier in the chapter, we pointed out the naming of our data folders.
    It should now be clear that our dataset has data from six collection runs using
    a normal driving strategy and from three collection runs using a swerve driving
    strategy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶也无法免于数据集不平衡的问题。让我们以我们的转向角预测模型为例。从我们日常驾驶经验中，我们对转向角了解多少？在驾驶过程中，我们大多数时间都是直线行驶。如果我们的模型只是在正常驾驶数据上进行训练，它将永远不会学会如何转弯，因为在训练数据集中相对稀缺。为了解决这个问题，我们的数据集不仅需要包含正常驾驶策略的数据，还需要包含“转弯”驾驶策略的数据，数量上具有统计学意义。为了说明我们的意思，让我们回到我们的*.tsv*/*.txt*文件。在本章的前面，我们指出了数据文件夹的命名。现在应该清楚了，我们的数据集包含了使用正常驾驶策略进行的六次收集运行和使用转弯驾驶策略进行的三次收集运行的数据。
- en: 'Let us aggregate the data from all *.tsv*/*.txt* files into a single DataFrame
    to make the analysis easier:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有*.tsv*/*.txt*文件中的数据聚合到一个DataFrame中，以便更容易进行分析：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s plot some steering angles from both driving strategies on a scatter plot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在散点图上绘制来自两种驾驶策略的转向角：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw) shows
    the results.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-10](part0019.html#plot_showing_steering_angles_from_the_tw)显示了结果。'
- en: '![Plot showing steering angles from the two driving strategies](../images/00299.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![显示两种驾驶策略的转向角的图](../images/00299.jpeg)'
- en: Figure 16-10\. Plot showing steering angles from the two driving strategies
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-10。显示两种驾驶策略的转向角的图
- en: 'Let’s also plot the number of data points collected with each strategy:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也绘制一下使用每种策略收集的数据点数量：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate) shows
    those results.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-11](part0019.html#dataset_split_for_the_two_driving_strate)显示了这些结果。'
- en: '![Dataset split for the two driving strategies](../images/00114.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![两种驾驶策略的数据集拆分](../images/00114.jpeg)'
- en: Figure 16-11\. Dataset split for the two driving strategies
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-11。两种驾驶策略的数据集拆分
- en: Looking at [Figure 16-10](part0019.html#plot_showing_steering_angles_from_the_tw),
    as we had expected, we see that the normal driving strategy produces steering
    angles that we would observe during everyday driving, mostly straight on the road
    with the occasional turn. By contrast, the swerve driving strategy mostly focuses
    on sharp turns and hence has higher values for steering angles. As shown in [Figure 16-11](part0019.html#dataset_split_for_the_two_driving_strate),
    a combination of these two strategies gives us a nice, albeit unrealistic in real
    life, distribution to train on with a 75/25 split. This also further solidifies
    the importance of simulation for autonomous driving because it is unlikely we’d
    be collecting data using the swerve strategy in real life with an actual car.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[图16-10](part0019.html#plot_showing_steering_angles_from_the_tw)，正如我们所预期的，我们看到正常驾驶策略产生的转向角大多数是我们在日常驾驶中观察到的，大部分是直行，偶尔转弯。相比之下，突然转向驾驶策略主要集中在急转弯，因此转向角的值较高。如[图16-11](part0019.html#dataset_split_for_the_two_driving_strate)所示，这两种策略的结合给我们一个不错的、虽然在现实生活中不太现实的分布，以75/25的比例进行训练。这也进一步巩固了模拟对自动驾驶的重要性，因为在现实生活中我们不太可能使用实际汽车进行突然转向策略来收集数据。
- en: 'Before closing our discussion on data preprocessing and dataset imbalance,
    let’s take a final look at the distribution of our steering angles for the two
    driving strategies by plotting them on a histogram ([Figure 16-12](part0019.html#steering_angle_distribution_for_the_two)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在结束关于数据预处理和数据集不平衡的讨论之前，让我们最后再看一下我们两种驾驶策略的转向角分布，通过在直方图上绘制它们（[图16-12](part0019.html#steering_angle_distribution_for_the_two)）： '
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Steering angle distribution for the two driving strategies](../images/00055.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![两种驾驶策略的转向角分布](../images/00055.jpeg)'
- en: Figure 16-12\. Steering angle distribution for the two driving strategies
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-12。两种驾驶策略的转向角分布
- en: As noted earlier, the swerve driving strategy gives us a much broader range
    of angles compared to the normal driving strategy, as shown in [Figure 16-12](part0019.html#steering_angle_distribution_for_the_two).
    These angles will help our neural network react appropriately in case it ever
    finds the car going off the road. The problem of imbalance in our dataset is somewhat
    solved, but not entirely. We still have a lot of zeros, across both driving strategies.
    To balance our dataset further, we could just ignore a portion of these during
    training. Although this gives us a very balanced dataset, it greatly reduces the
    overall number of data points available to us. We need to keep this in mind when
    we build and train our neural network.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，与正常驾驶策略相比，突然转向驾驶策略给我们提供了更广泛的角度范围，如[图16-12](part0019.html#steering_angle_distribution_for_the_two)所示。这些角度将帮助我们的神经网络在汽车偏离道路时做出适当的反应。我们数据集中的不平衡问题在一定程度上得到解决，但并非完全解决。我们仍然有很多零，跨越两种驾驶策略。为了进一步平衡我们的数据集，我们可以在训练过程中忽略其中的一部分。尽管这给我们提供了一个非常平衡的数据集，但大大减少了我们可用的数据点总数。在构建和训练神经网络时，我们需要记住这一点。
- en: 'Before moving on, let’s make sure our data is suitable for training. Let’s
    take the raw data from all folders, split it into train, test, and validation
    datasets, and compress them into HDF5 files. The HDF5 format allows us to access
    data in chunks without reading the entire dataset into memory at once. This makes
    it ideal for deep learning problems. It also works seamlessly with Keras. The
    following code will take some time to run. When it has finished running, we will
    have three dataset files: *train.h5*, *eval.h5*, and *test.h5*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们确保我们的数据适合训练。让我们从所有文件夹中获取原始数据，将其分割为训练、测试和验证数据集，并将它们压缩成HDF5文件。HDF5格式允许我们以块的方式访问数据，而不需要一次性将整个数据集读入内存。这使其非常适合深度学习问题。它还可以与Keras无缝配合。以下代码将需要一些时间来运行。当运行完成后，我们将得到三个数据集文件：*train.h5*、*eval.h5*和*test.h5*。
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Each dataset file has four parts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集文件有四个部分：
- en: Image
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图像
- en: A NumPy array containing the image data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 包含图像数据的NumPy数组。
- en: Previous_state
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个状态
- en: A NumPy array containing the last known state of the car. This is a (steering,
    throttle, brake, speed) tuple.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 包含汽车最后已知状态的NumPy数组。这是一个（转向、油门、刹车、速度）元组。
- en: Label
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: A NumPy array containing the steering angles that we wish to predict (normalized
    on the range -1..1).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 包含我们希望预测的转向角的NumPy数组（在范围-1..1上进行了标准化）。
- en: Metadata
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据
- en: A NumPy array containing metadata about the files (which folder they came from,
    etc.).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 包含有关文件的元数据的NumPy数组（它们来自哪个文件夹等）。
- en: We are now ready to take the observations and learnings from our dataset and
    start training our model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好从数据集中获取观察和经验，并开始训练我们的模型。
- en: Training Our Autonomous Driving Model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的自动驾驶模型
- en: 'All the steps in this section are also detailed in the Jupyter Notebook [*TrainModel.ipynb*](https://oreil.ly/ESHVS).
    As before, we begin by importing some libraries and defining paths:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有步骤也在Jupyter Notebook [*TrainModel.ipynb*](https://oreil.ly/ESHVS)中详细说明。与之前一样，我们首先导入一些库并定义路径：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s also set up our datasets:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也设置好我们的数据集：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Drive Data Generator
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 驾驶数据生成器
- en: We were introduced to the concept of Keras data generators in previous chapters.
    A data generator iterates through the dataset and reads data in chunks from the
    disk. This allows us to keep both our CPU and GPU busy, increasing throughput.
    To implement ideas discussed in the previous section we created our own Keras
    generator called `DriveDataGenerator`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中介绍了Keras数据生成器的概念。数据生成器会遍历数据集，并从磁盘中以块的形式读取数据。这使我们能够让CPU和GPU保持繁忙，提高吞吐量。为了实现前一节讨论的想法，我们创建了自己的Keras生成器，称为`DriveDataGenerator`。
- en: 'Let’s recap some of our observations from the previous section:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前一节中的一些观察结果：
- en: Our model should focus only on the ROI within each image.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的模型应该只关注每个图像中的ROI。
- en: We can augment our dataset by flipping images horizontally and reversing the
    sign of the steering angle.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过水平翻转图像并反转转向角的符号来增强我们的数据集。
- en: We can further augment the data by introducing random brightness changes in
    the images. This will simulate different lighting conditions and make our model
    more robust.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过在图像中引入随机亮度变化来进一步增强数据。这将模拟不同的光照条件，并使我们的模型更加健壮。
- en: We can randomly drop a percentage of data points where the steering angle is
    zero so that the model sees a balanced dataset when training.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以随机删除一定百分比的转向角为零的数据点，以便在训练时模型看到一个平衡的数据集。
- en: Our overall number of data points available will be reduced significantly post-balancing.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们平衡后可用的数据点总数将显著减少。
- en: Let’s see how the `DriveDataGenerator` takes care of the first four of these
    items. We will return to last item when we begin designing our neural network.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`DriveDataGenerator`如何处理这些前四项。当我们开始设计神经网络时，我们将回到最后一项。
- en: The ROI is a simple image crop. [76,135,0,255] (shown in the code block that
    follows) is the [x1,x2,y1,y2] value of the rectangle representing the ROI. The
    generator extracts this rectangle from each image. We can use the parameter `roi`
    to modify the ROI.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ROI是一个简单的图像裁剪。[76,135,0,255]（在接下来的代码块中显示）是表示ROI的矩形的[x1,x2,y1,y2]值。生成器从每个图像中提取此矩形。我们可以使用参数`roi`来修改ROI。
- en: The horizontal flip is fairly straightforward. When generating batches, random
    images are flipped along the y-axis and their steering angle values are reversed
    if the parameter `horizontal_flip` is set to `True`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 水平翻转相对简单。在生成批次时，随机图像沿y轴翻转，并且如果参数`horizontal_flip`设置为`True`，它们的转向角值将被反转。
- en: For random brightness changes, we introduce the parameter `brighten_range`.
    Setting this parameter to `0.4` modifies the brightness of images in any given
    batch randomly up to 40%. We do not recommend increasing this beyond `0.4`. To
    compute brightness, we transform the image from RGB to HSV space, scale the “V”
    coordinate up or down, and transform back to RGB.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机亮度变化，我们引入参数`brighten_range`。将此参数设置为`0.4`会随机地使任何给定批次中的图像亮度增加或减少最多40%。我们不建议将此值增加到`0.4`之上。为了计算亮度，我们将图像从RGB转换为HSV空间，将“V”坐标缩放上下，然后转换回RGB。
- en: For dataset balancing through dropping zeros, we introduce the parameter `zero_drop_percentage`.
    Setting this to `0.9` will randomly drop 90% of the 0-label data points in any
    given batch.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除零来平衡数据集，我们引入了参数`zero_drop_percentage`。将此设置为`0.9`将在任何给定批次中随机删除90%的0标签数据点。
- en: 'Let’s initialize our generator with these parameters:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些参数初始化我们的生成器：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s visualize some sample data points by drawing labels (steering angle)
    over their corresponding images:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在相应图像上绘制标签（转向角）来可视化一些样本数据点：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We should see an output similar to [Figure 16-13](part0019.html#drawing_steering_angles_on_images).
    Observe that we are now looking at only the ROI when training the model, thereby
    ignoring all the nonrelevant information present in the original images. The line
    indicates the ground truth steering angle. This is the angle the car was driving
    at when that image was taken by the camera during data collection.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似于[图16-13](part0019.html#drawing_steering_angles_on_images)的输出。请注意，我们现在在训练模型时只关注ROI，从而忽略原始图像中存在的所有非相关信息。线表示地面真实转向角。这是汽车在摄像机拍摄图像时行驶的角度。
- en: '![Drawing steering angles on images](../images/00182.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![在图像上绘制转向角](../images/00182.jpeg)'
- en: Figure 16-13\. Drawing steering angles on images
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-13。在图像上绘制转向角
- en: Model Definition
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定义
- en: We are now ready to define the architecture of our neural network. It is here
    that we must take into account the problem of our dataset being very limited in
    size after removing zeros. Because of this limitation, we cannot build a network
    that is too deep. Because we are dealing with images, we will need a few convolutional/max-pooling
    pairs to extract features.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义神经网络的架构。在这里，我们必须考虑到在删除零后，我们的数据集非常有限的问题。因此，我们不能构建太深的网络。由于我们处理的是图像，我们将需要一些卷积/最大池化对来提取特征。
- en: Images alone, however, might not be enough to lead our model to convergence.
    Using only images to train also does not align with how driving decisions are
    made in the real world. When driving down the road, we are not only perceiving
    our surrounding environment, we are also aware of how fast we are going, how much
    we are turning and, the state of our gas and brake pedals. Input from sensors
    like cameras, lidars, radars, and so on being fed into a neural network corresponds
    to only one portion of all information a driver has at hand while making a driving
    decision in the real world. An image presented to our neural network could have
    been taken from a stationary car or a car driving at 60 mph; the network would
    have no way of knowing which. Turning the steering wheel by two degrees to the
    right while driving at 5 mph will yield very different results compared to doing
    the same at 50 mph. In short, a model trying to predict steering angles should
    not rely on sensory input alone. It also needs information about the current state
    of the car. Thankfully for us, we do have this information available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅使用图像可能不足以使我们的模型收敛。仅仅使用图像进行训练也不符合现实世界中做驾驶决策的方式。在驾驶时，我们不仅感知周围环境，还意识到我们的速度、转向程度，以及油门和刹车踏板的状态。相机、激光雷达、雷达等传感器输入到神经网络中只对应驾驶员在做驾驶决策时手头上所有信息的一部分。呈现给神经网络的图像可能是从一辆静止的汽车或以60英里/小时行驶的汽车中拍摄的；网络无法知道是哪种情况。在以5英里/小时行驶时将方向盘向右转两度与以50英里/小时行驶时做同样动作将产生非常不同的结果。简而言之，试图预测转向角度的模型不应仅依赖感官输入。它还需要关于汽车当前状态的信息。幸运的是，我们有这些信息可用。
- en: At the end of the previous section, we noted that our datasets have four parts.
    For each image, in addition to the steering angle label and metadata, we also
    recorded the last known state of the car corresponding to the image. This was
    stored in the form of a (steering, throttle, brake, speed) tuple, and we will
    use this information, along with our images, as input to the neural network. Note
    that this does not violate our “Hello, World!” requirements, because we are still
    using the single camera as our only external sensor.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的结尾，我们指出我们的数据集有四个部分。对于每个图像，除了转向角标签和元数据外，我们还记录了与图像对应的汽车上次已知状态。这以 (转向、油门、刹车、速度)
    元组的形式存储，我们将使用这些信息以及我们的图像作为神经网络的输入。请注意，这不违反我们的“Hello, World!”要求，因为我们仍然将单个摄像头作为唯一的外部传感器。
- en: Putting all we discussed together, you can see the neural network that we will
    be using for this problem in [Figure 16-14](part0019.html#network_architecture).
    We are using three convolutional layers with 16, 32, 32 filters, respectively,
    and a (3,3) convolutional window. We are merging the image features (output from
    convolutional layers) with an input layer supplying the previous state of the
    car. The combined feature set is then passed through two fully connected layers
    with 64 and 10 hidden neurons, respectively. The activation function used in our
    network is ReLU. Notice that unlike the classification problems we have been working
    on in previous chapters, the last layer of our network is a single neuron with
    no activation. This is because the problem we are trying to solve is a regression
    problem. The output of our network is the steering angle, a floating-point number,
    unlike the discrete classes that we were predicting earlier.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们讨论的所有内容放在一起，您可以在 [图16-14](part0019.html#network_architecture) 中看到我们将用于此问题的神经网络。我们使用了三个卷积层，分别具有16、32、32个滤波器，并且使用了
    (3,3) 的卷积窗口。我们将图像特征（从卷积层输出）与提供汽车先前状态的输入层进行合并。然后将组合特征集传递到两个分别具有64和10个隐藏神经元的全连接层中。我们网络中使用的激活函数是
    ReLU。请注意，与我们在前几章中处理的分类问题不同，我们网络的最后一层是一个没有激活的单个神经元。这是因为我们要解决的问题是一个回归问题。我们网络的输出是转向角度，一个浮点数，而不是我们之前预测的离散类别。
- en: '![Network architecture](../images/00141.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![网络架构](../images/00141.jpeg)'
- en: Figure 16-14\. Network architecture
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-14\. 网络架构
- en: 'Let’s now implement our network. We can use `model.summary()`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现我们的网络。我们可以使用 `model.summary()`：
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Callbacks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回调
- en: 'One of the nice features of Keras is the ability to declare *callbacks*. Callbacks
    are functions that are executed after each epoch of training and help us to gain
    an insight into the training process as well as control hyperparameters to an
    extent. They also let us define conditions to perform certain actions while training
    is underway; for example, stopping the training early if the loss stops decreasing.
    We will use a few callbacks for our experiment:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一个很好的特性是能够声明 *回调函数*。回调函数在每个训练周期结束后执行，帮助我们深入了解训练过程并在一定程度上控制超参数。它们还让我们定义在训练进行时执行某些操作的条件；例如，如果损失停止减少，则提前停止训练。我们将为我们的实验使用一些回调函数：
- en: '`ReduceLROnPlateau`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReduceLROnPlateau`'
- en: If the model is near a minimum and the learning rate is too high, the model
    will circle around that minimum without ever reaching it. This callback will allow
    the model to reduce its learning rate when the validation loss hits a plateau
    and stops improving, allowing us to reach the optimal point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型接近最小值且学习率过高，模型将在该最小值周围循环而无法达到它。当验证损失达到平稳期并停止改善时，此回调将允许模型减少学习率，使我们能够达到最佳点。
- en: '`CSVLogger`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`CSVLogger`'
- en: This lets us log the output of the model after each epoch into a CSV file, which
    will allow us to track the progress without needing to use the console.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够将每个周期结束后模型的输出记录到一个 CSV 文件中，这样我们就可以跟踪进展而无需使用控制台。
- en: '`ModelCheckpoint`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelCheckpoint`'
- en: Generally, we will want to use the model that has the lowest loss on the validation
    set. This callback will save the model each time the validation loss improves.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望使用在验证集上损失最低的模型。此回调将在每次验证损失改善时保存模型。
- en: '`EarlyStopping`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStopping`'
- en: We will want to stop training when the validation loss stops improving. Otherwise,
    we risk overfitting. This option will detect when the validation loss stops improving
    and will stop the training process when that occurs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当验证损失不再改善时，我们将停止训练。否则，我们会面临过拟合的风险。此选项将检测验证损失停止改善的时候，并在发生这种情况时停止训练过程。
- en: 'Let’s now go ahead and implement these callbacks:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续实现这些回调：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’re now all set to kick off training. The model will take a while to train,
    so this will make for a nice Netflix break. The training process should terminate
    with a validation loss of approximately .0003:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好开始训练了。模型需要一段时间来训练，所以这将是一个不错的Netflix休息时间。训练过程应该以约0.0003的验证损失终止：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our model is now trained and ready to go. Before we see it in action, let’s
    do a quick sanity check and plot some predictions against images:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在已经训练好并准备就绪。在看到它的表现之前，让我们进行一个快速的合理性检查，并将一些预测绘制在图像上：
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We should see an output similar to [Figure 16-15](part0019.html#drawing_actual_and_predicted_steering_an).
    In this figure, the thick line is the predicted output, and the thin line is the
    label output. Looks like our predictions are fairly accurate (we can also see
    the actual and predicted values above the images). Time to deploy our model and
    see it in action.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似于[图16-15](part0019.html#drawing_actual_and_predicted_steering_an)的输出。在这个图中，粗线是预测输出，细线是标签输出。看起来我们的预测相当准确（我们还可以在图像上方看到实际和预测值）。是时候部署我们的模型并看看它的表现了。
- en: '![Drawing actual and predicted steering angles on images](../images/00095.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![在图像上绘制实际和预测的转向角](../images/00095.jpeg)'
- en: Figure 16-15\. Drawing actual and predicted steering angles on images
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-15。在图像上绘制实际和预测的转向角
- en: Deploying Our Autonomous Driving Model
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署我们的自动驾驶模型
- en: All of the steps in this section are also detailed in the Jupyter Notebook [*TestModel.ipynb*](https://oreil.ly/5saDl).
    Now that we have our model trained, it is time to spin up our simulator and use
    our model to drive our car around.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有步骤也在Jupyter Notebook [*TestModel.ipynb*](https://oreil.ly/5saDl)中有详细说明。现在我们的模型已经训练好了，是时候启动模拟器并使用我们的模型驾驶汽车了。
- en: 'As before, begin by importing some libraries and defining paths:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，首先导入一些库并定义路径：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, load the model and connect to AirSim in the Landscape environment. To
    start the simulator, on a Windows machine, open a PowerShell command window at
    the location where we unzipped the simulator package and run the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载模型并在景观环境中连接到AirSim。要启动模拟器，在Windows机器上，打开一个PowerShell命令窗口，位于我们解压缩模拟器包的位置，并运行以下命令：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Back in the Jupyter Notebook, run the following to connect the model to the
    AirSim client. Ensure that the simulator is running *before* kicking this off:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，运行以下命令将模型连接到AirSim客户端。确保模拟器在启动此过程之前已经运行：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With the connection established, let’s now set the initial state of the car
    as well as some buffers used to store the output from the model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 连接建立后，让我们现在设置汽车的初始状态以及用于存储模型输出的一些缓冲区：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to set up the model to expect an RGB image from the simulator
    as the input. We need to define a helper function for this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置模型期望从模拟器接收RGB图像作为输入。我们需要为此定义一个辅助函数：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, let’s set up an infinite loop for our model to read an image from
    the simulator along with the current state of the car, predict the steering angle,
    and send it back to the simulator. Because our model predicts only steering angles,
    we will need to supply a control signal to maintain speed ourselves. Let’s set
    it up so that the car will attempt to run at a constant 5 m/s:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们为我们的模型设置一个无限循环，从模拟器中读取图像以及汽车的当前状态，预测转向角，并将其发送回模拟器。因为我们的模型只预测转向角，所以我们需要提供一个控制信号来自行维持速度。让我们设置这样一个控制信号，使汽车尝试以恒定的5m/s速度行驶：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We should see output similar to the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似于以下的输出：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We did it! The car is driving around nicely on the road, keeping to the right
    side, for the most part, carefully navigating all the sharp turns and instances
    where it could potentially go off the road. Kudos on training our first-ever autonomous
    driving model!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功了！汽车在道路上很好地行驶，大部分时间保持在右侧，小心地穿过所有急转弯和潜在的偏离道路的地方。恭喜我们训练出了我们的第一个自动驾驶模型！
- en: '![Trained model driving the car](../images/00232.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型驾驶汽车](../images/00232.jpeg)'
- en: Figure 16-16\. Trained model driving the car
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-16。训练模型驾驶汽车
- en: Before we wrap up, there are a couple of things worth noting. First, notice
    that the motion of the car is not perfectly smooth. This is because we are working
    with a regression problem and making a steering angle prediction for every image
    frame seen by the car. One way to fix this would be to average out predictions
    over a buffer of consecutive images. Another idea could be to turn this into a
    classification problem. More specifically, we could define buckets for the steering
    angles (..., –0.1, –0.05, 0, 0.05, 0.1, ...), bucketize the labels, and predict
    the correct bucket for each image.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，有几件值得注意的事情。首先，请注意汽车的运动不是完全平滑的。这是因为我们正在处理一个回归问题，并为汽车看到的每个图像帧做出一个转向角预测。解决这个问题的一种方法是在一系列连续的图像上平均预测。另一个想法可能是将其转化为一个分类问题。更具体地，我们可以为转向角定义桶（...，-0.1，-0.05，0，0.05，0.1，...），将标签分桶化，并为每个图像预测正确的桶。
- en: If we let the model run for a while (a little more than five minutes), we observe
    that the car eventually veers off the road randomly and crashes. This happens
    on a portion of the track with a steep incline. Remember our last requirement
    during the problem setup? Elevation changes require manipulating throttle and
    brakes. Because our model can control only the steering angle, it doesn’t do too
    well on steep roads.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果让模型运行一段时间（略长于五分钟），我们会观察到汽车最终会随机偏离道路并撞车。这发生在一个有陡峭上坡的赛道部分。还记得我们在问题设置中的最后一个要求吗？高度变化需要操作油门和刹车。因为我们的模型只能控制转向角，所以在陡峭的道路上表现不佳。
- en: Further Exploration
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Having been trained on the “Hello, World!” scenario, our model is obviously
    not a perfect driver but don’t be disheartened. Keep in mind that we have barely
    scratched the surface of the possibilities at the intersection of deep learning
    and self-driving cars. The fact that we were able to have our car learn to drive
    around almost perfectly using a very small dataset is something to be proud of!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Hello, World!”场景中训练过后，我们的模型显然不是一个完美的驾驶员，但不要灰心。请记住，我们只是刚刚触及深度学习和自动驾驶汽车交汇点的可能性表面。我们能够让我们的汽车几乎完美地学会驾驶，只使用了一个非常小的数据集，这是值得骄傲的事情！
- en: Here are some new ideas for us to build on top of what we learned in this chapter.
    You can implement all of these ideas using the setup we already have in place
    for this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些新的想法，可以在本章学到的基础上进行扩展。您可以使用本章已经准备好的设置来实现所有这些想法。
- en: Expanding Our Dataset
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展我们的数据集
- en: As a general rule, using more data helps improve model performance. Now that
    we have the simulator up and running, it will be a useful exercise to expand our
    dataset by doing more data collection runs. We can even try combining data from
    various different environments available in AirSim and see how our model trained
    on this data performs in different environments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一般规则是，使用更多的数据有助于提高模型性能。现在我们已经启动并运行了模拟器，通过进行更多的数据收集运行来扩展我们的数据集将是一个有用的练习。我们甚至可以尝试将来自AirSim中各种不同环境的数据结合起来，看看我们在这些数据上训练的模型在不同环境中的表现如何。
- en: We used only RGB data from a single camera in this chapter. AirSim allows us
    to do a lot more. For example, we can collect images in depth view, segmentation
    view, surface normal view, and more for each of the cameras available. So, we
    can potentially have 20 different images (for five cameras operating in all four
    modes) for each instance. Can using all this extra data help us improve the model
    we just trained?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只使用了单个摄像头的RGB数据。AirSim允许我们做更多的事情。例如，我们可以收集深度视图、分割视图、表面法线视图等每个可用摄像头的图像。因此，对于每个实例，我们可能有20个不同的图像（对于四种模式中的所有五个摄像头）。使用所有这些额外数据能帮助我们改进我们刚刚训练的模型吗？
- en: Training on Sequential Data
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在序列数据上训练
- en: 'Our model currently uses a single image and a single vehicle state for each
    prediction. This is not really how we drive in real life, though. Our actions
    always take into account the recent series of events leading up to that given
    moment. In our dataset, we have timestamp information available for all our images,
    which we can use to create sequences. We can modify our model to make predictions
    using the previous *N* images and states. For example, given the past 10 images
    and past 10 states, predict the next steering angle. (Hint: This might require
    the use of RNNs.)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型目前对每个预测使用单个图像和单个车辆状态。然而，这并不是我们在现实生活中驾驶的方式。我们的行动总是考虑到导致给定时刻的最近一系列事件。在我们的数据集中，我们有所有图像的时间戳信息可用，我们可以使用这些信息创建序列。我们可以修改我们的模型，使用前
    *N* 个图像和状态进行预测。例如，给定过去的10个图像和过去的10个状态，预测下一个转向角度。（提示：这可能需要使用RNNs。）
- en: Reinforcement Learning
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: After we learn about reinforcement learning in the next chapter, we can come
    back and try the [Distributed Deep Reinforcement Learning for Autonomous Driving](https://oreil.ly/u1hoC)
    tutorial from the [*Autonomous Driving Cookbook*](https://oreil.ly/bTphH). Using
    the Neighborhood environment in AirSim, which is included in the package we downloaded
    for this chapter, we will learn how to scale a deep reinforcement learning training
    job and reduce training time from under a week to less than an hour using transfer
    learning and the cloud.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章学习强化学习之后，我们可以回来尝试[*自动驾驶食谱*](https://oreil.ly/bTphH)中的[Distributed Deep Reinforcement
    Learning for Autonomous Driving](https://oreil.ly/u1hoC)教程。使用AirSim中包含的Neighborhood环境，该环境已经包含在我们为本章下载的软件包中，我们将学习如何通过迁移学习和云来扩展深度强化学习训练作业，并将训练时间从不到一周缩短到不到一个小时。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter gave us a sneak peek into how deep learning enables the autonomous
    driving industry. Taking advantage of the skills acquired in the previous chapters,
    we implemented the “Hello, World!” problem of autonomous driving using Keras.
    Exploring the raw data at hand, we learned how to preprocess it to make it suitable
    for training a high-performing model. And we were able to accomplish this with
    a very small dataset. We were also able to take our trained model and deploy it
    to drive a car in the simulated world. Wouldn’t you agree that there’s just something
    magical about that?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本章让我们一窥深度学习如何推动自动驾驶行业。利用前几章学到的技能，我们使用Keras实现了自动驾驶的“Hello, World!”问题。通过探索手头的原始数据，我们学会了如何预处理数据，使其适合训练高性能模型。而且我们能够用一个非常小的数据集完成这个任务。我们还能够拿出我们训练好的模型，并将其部署到模拟世界中驾驶汽车。您不认为这其中有一些魔力吗？
