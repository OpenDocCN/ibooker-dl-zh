- en: 3 Dimensionality reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 降维
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The curse of dimensionality and its disadvantages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒及其不利之处
- en: Various methods of reducing dimensions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少维度的各种方法
- en: Principal component analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Singular value decomposition
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Python solutions for both principal component analysis and singular value decomposition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 解决主成分分析和奇异值分解的方法
- en: A case study on dimension reduction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于降维的案例研究
- en: Knowledge is a process of piling up facts; wisdom lies in their simplification.—Martin
    H. Fischer
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 知识是堆积事实的过程；智慧在于它们的简化。——马丁·H·费舍尔
- en: We face complex situations in life. Life throws multiple options at us, and
    we choose a few viable ones from them. This decision of shortlisting is based
    on the significance, feasibility, utility, and perceived profit from each of the
    options. The ones that fit the bill are then chosen. A perfect example can be
    selecting your vacation destination. Based on the weather, travel time, safety,
    food, budget, and several other options, we choose a few where we would like to
    spend our next vacation. In this chapter, we study precisely the same—how to reduce
    the number of options—albeit in the data science and machine learning world.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在生活中，我们面临复杂的情况。生活向我们抛出多种选择，我们从中选择几个可行的。这个筛选决策基于每个选项的重要性、可行性、效用和预期的利润。符合条件的选择被选中。一个完美的例子就是选择你的度假目的地。基于天气、旅行时间、安全性、食物、预算和其他几个选项，我们选择几个我们愿意在那里度过下一个假期的地方。在本章中，我们研究的是如何减少选项的数量——尽管是在数据科学和机器学习的世界中。
- en: In the last chapter, we covered major clustering algorithms. We also went over
    a case study. The datasets we generate and use in such real-world examples have
    a lot of variables. Sometimes, there can be more than 100 variables or *dimensions*
    in the data. But not all of them are important. Having a lot of dimensions in
    the dataset is referred to as the curse of dimensionality. To perform any further
    analysis, we choose a few from the list of all of the dimensions or variables.
    In this chapter, we study the need for dimension reductions, various dimensionality
    techniques, and the respective pros and cons. We will dive deeper into the concepts
    of principal component analysis (PCA) and singular value decomposition (SVD) and
    their mathematical foundations and complement these with Python implementation.
    Also, continuing our structure from the last chapter, we will examine a real-world
    case study in the telecommunication sector. There are other advanced dimensionality
    reduction techniques like t-distributed stochastic neighbor embedding (t-SNE)
    and linear discriminant analysis (LDA), which we will explore in later chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了主要的聚类算法，并进行了案例研究。我们在这些现实世界例子中生成和使用的数据集有很多变量。有时，数据中可能有超过100个变量或*维度*。但并非所有这些变量都是重要的。数据集中有很多维度被称为维度诅咒。为了进行任何进一步的分析，我们从所有维度或变量的列表中选择一些。在本章中，我们研究降维的需求、各种降维技术以及相应的优缺点。我们将更深入地探讨主成分分析（PCA）和奇异值分解（SVD）的概念及其数学基础，并用
    Python 实现来补充。此外，继续我们上一章的结构，我们将研究电信行业的实际案例研究。还有其他高级的降维技术，如t分布随机邻域嵌入（t-SNE）和线性判别分析（LDA），我们将在后面的章节中探讨。
- en: Clustering and dimensionality reductions are the major categories of unsupervised
    learning. We studied major clustering methods in the last chapter, and we discuss
    dimensionality reduction in this chapter. With these two solutions, we cover a
    lot of ground in the unsupervised learning domain. But there are many more advanced
    topics to be covered, which are part of the latter chapters of the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维是无监督学习的主要类别。我们在上一章研究了主要的聚类方法，在本章中我们将讨论降维。有了这两种解决方案，我们在无监督学习领域覆盖了大量的内容。但还有更多高级主题需要探讨，这些主题是本书后几章的内容。
- en: Let’s first understand what we mean by the “curse of dimensionality.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解“维度诅咒”的含义。
- en: 3.1 Technical toolkit
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 技术工具包
- en: We are using the same version of Python as in the last chapters. Jupyter Notebook
    will be used in this chapter too.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中使用的 Python 版本与上一章相同。本章也将使用 Jupyter Notebook。
- en: 'All the datasets and code files are available at the GitHub repository at ([https://mng.bz/ZlBR](https://mng.bz/ZlBR)).
    You need to install the following Python libraries to execute the code: `numpy`,
    `pandas`, `matplotlib`, `scipy`, and `sklearn`. Since you have used the same packages
    in the last chapter, you don’t need to install them again. CPU is good enough
    for execution, but if you face some computing problems, switch to GPU or Google
    Colab. Refer to the appendix if you face any problems with the installation of
    any of these packages.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据集和代码文件都可以在GitHub仓库中找到（[https://mng.bz/ZlBR](https://mng.bz/ZlBR)）。您需要安装以下Python库来执行代码：`numpy`、`pandas`、`matplotlib`、`scipy`和`sklearn`。由于您在上一个章节中已经使用了相同的包，因此您不需要再次安装它们。CPU足以执行，但如果您遇到一些计算问题，请切换到GPU或Google
    Colab。如果您在安装这些包的任何包时遇到问题，请参阅附录。
- en: 3.2 The curse of dimensionality
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 维度诅咒
- en: 'Let us continue with the vacation destination example we introduced earlier.
    The choice of destination is dependent on several parameters: safety, availability,
    food, nightlife, weather, budget, health, and so on. Having too many parameters
    is confusing. Let us understand by a real-life example.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续之前介绍的度假目的地示例。目的地的选择取决于几个参数：安全性、可用性、食物、夜生活、天气、预算、健康等等。参数太多会令人困惑。让我们通过一个现实生活中的例子来理解。
- en: 'Consider this: a retailer wishes to launch a new range of shoes in the market,
    and for that, a target group of customers should be chosen. This target group
    will be reached through email, SMS, newsletter, etc. The business objective is
    to entice these customers to buy the newly launched shoes. From the entire customer
    base, the target group of customers can be chosen based on variables like customer
    age, gender, budget, preferred category, average spend, frequency of shopping,
    and so on. These many variables or *dimensions* make it hard to shortlist the
    customers based on a sound data analysis technique. We would be analyzing too
    many parameters simultaneously, examining the effect of each on the shopping probability
    of the customer, and hence it becomes too tedious and confusing of a task. It
    is the curse of dimensionality problem we face in real-world data science projects.
    We can face the curse of dimensionality in one more situation wherein the number
    of observations is fewer than the number of variables. Consider a dataset where
    the number of observations is *X*, while the number of variables is more than
    *X*—in such a case, we face the curse of dimensionality.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这种情况：一个零售商希望在市场上推出一系列新鞋，为此，需要选择一个目标客户群。这个目标客户群将通过电子邮件、短信、新闻通讯等方式接触到。业务目标是吸引这些客户购买新推出的鞋子。从整个客户群中，可以根据客户年龄、性别、预算、偏好类别、平均消费、购物频率等变量选择目标客户群。这些许多变量或*维度*使得基于可靠的数据分析技术筛选客户变得困难。我们将同时分析太多参数，检查每个参数对客户购物概率的影响，因此这项任务变得既繁琐又令人困惑。这就是我们在现实世界的数据科学项目中面临的维度诅咒问题。我们还会在另一种情况下遇到维度诅咒，即观察数少于变量数。考虑一个观察数为*X*而变量数多于*X*的数据集——在这种情况下，我们面临维度诅咒。
- en: An easy method to understand any dataset is through visualization. Let’s visualize
    a dataset in a vector-space diagram. If we have only one attribute or feature
    in the dataset, we can represent it in one dimension (see the left diagram in
    figure 3.1). For example, we might wish to capture only the height of an object
    using a single dimension. If we have two attributes, we need two dimensions, as
    shown in the middle diagram in figure 3.1, wherein to get the area of an object,
    we will require both length and width. If we have three attributes, for example,
    to calculate the volume, which requires length, width, and height, we require
    a 3D space, as shown in the diagram at right in figure 3.1\. This requirement
    will continue to grow based on the number of attributes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理解任何数据集的一个简单方法是可视化。让我们在向量空间图中可视化一个数据集。如果我们只有一个属性或特征在数据集中，我们可以在一维中表示它（见图3.1左图）。例如，我们可能只想用一个维度来捕捉一个物体的高度。如果我们有两个属性，我们需要两个维度，如图3.1中间的图所示，要得到一个物体的面积，我们需要长度和宽度。如果我们有三个属性，例如，为了计算体积，这需要长度、宽度和高度，我们需要一个三维空间，如图3.1右边的图所示。这个需求将根据属性的数量继续增长。
- en: '![figure](../Images/CH03_F01_Verdhan.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Verdhan.png)'
- en: Figure 3.1 Only one dimension is required to represent the data points—for example,
    to represent the height of an object (left). We need two dimensions to represent
    a data point. Each data point can correspond to the length and width of an object,
    which can be used to calculate the area (middle). Three dimensions are required
    to show a point (right). Here, it can be length, width, and height, which are
    required to get the volume of an object. This process continues based on the number
    of dimensions present in the data.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 只需要一个维度来表示数据点——例如，表示一个物体的高度（左）。我们需要两个维度来表示一个数据点。每个数据点可以对应于物体的长度和宽度，这可以用来计算面积（中间）。需要三个维度来表示一个点（右）。在这里，可以是长度、宽度和高度，这些是计算物体体积所需的。这个过程基于数据中存在的维度数量继续进行。
- en: Consider a dataset where you have an attribute for a data point—for example,
    gender. Then we add age and then education, address, and so on. To represent these
    attributes, the number of dimensions will keep on increasing. Hence, it is quite
    easy for us to conclude that with an increase in the number of dimensions, the
    amount of space required to represent increases by leaps and bounds. This is referred
    to as the c*urse of dimensionality*. The term was introduced by Richard E. Bellman
    and is used to refer to the problem of having too many variables in a dataset—some
    of which are significant while many others may be less important.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个数据集，其中你有一个数据点的属性——例如，性别。然后我们添加年龄，然后是教育、地址等。为了表示这些属性，维度的数量将持续增加。因此，我们可以很容易地得出结论，随着维度数量的增加，表示所需的空间量会大幅增加。这被称为*维度诅咒*。这个术语是由理查德·E·贝尔曼提出的，用来指代数据集中变量过多的问题——其中一些变量很重要，而许多其他变量可能不太重要。
- en: There is another well-known theory named the *Hughes phenomenon,* shown in figure
    3.2\. Generally, in data science and machine learning, we wish to have as many
    variables as possible to train our model. The performance of the supervised learning
    classifier algorithm will increase to a certain limit and will peak with the most
    optimal number of variables. But, using the same amount of training data and with
    an increased number of dimensions, there is a decrease in the performance of a
    supervised classification algorithm. In other words, it is not advisable to have
    the variables in a dataset if they are not contributing to the accuracy of the
    solution. We should remove such variables from the dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一个著名的理论称为*哈格里斯现象*，如图3.2所示。通常，在数据科学和机器学习中，我们希望有尽可能多的变量来训练我们的模型。监督学习分类算法的性能将增加到一定极限，并随着最优变量数量的增加而达到峰值。但是，使用相同数量的训练数据和增加的维度，监督分类算法的性能会下降。换句话说，如果变量没有对解决方案的准确性做出贡献，那么在数据集中拥有这些变量是不明智的。我们应该从数据集中移除这些变量。
- en: '![figure](../Images/CH03_F02_Verdhan.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F02_Verdhan.png)'
- en: Figure 3.2 The Hughes phenomenon shows that the performance of a machine learning
    model will improve initially with an increase in the number of dimensions. But
    a further increase leads to a decrease in the model’s performance.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 哈格里斯现象表明，随着维度数量的增加，机器学习模型的性能最初会提高。但进一步的增加会导致模型性能下降。
- en: 'An increase in the number of dimensions has the following effects on the machine
    learning model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 维度数量的增加对机器学习模型有以下影响：
- en: As the model deals with an increased number of variables, the mathematical complexity
    increases. For example, in the case of the k-means clustering method we discussed
    in the last chapter, when we have a greater number of variables, the distance
    calculation between respective points will become complex. Hence the overall model
    becomes more complex.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型处理越来越多的变量时，数学复杂性会增加。例如，在上一章中我们讨论的k-means聚类方法中，当我们有更多的变量时，各点之间的距离计算将变得复杂。因此，整体模型变得更加复杂。
- en: The dataset generated in a larger dimensional space can be much sparser as compared
    to a smaller number of variables. The dataset will be sparser as some of the variables
    will have missing values, NULLs, etc. Therefore, space is much emptier, the dataset
    is less dense, and a smaller number of variables have values associated with them.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更大维度空间中生成的数据集可能比变量数量较少的数据集稀疏得多。数据集将更加稀疏，因为一些变量将具有缺失值、NULL等。因此，空间更加空旷，数据集密度更低，与它们关联的变量数量更少。
- en: With increased complexity in the model, the processing time required increases.
    The system feels the pressure to deal with so many dimensions.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着模型复杂性的增加，所需的处理时间也会增加。系统感受到处理这么多维度的压力。
- en: The overall solution becomes more complex to comprehend and execute. Recall
    chapter 1, where we discussed supervised learning algorithms. Due to the high
    number of dimensions, we might face the problem of overfitting in supervised learning
    models.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整体解决方案在理解和执行上变得更加复杂。回想一下第一章，我们讨论了监督学习算法。由于维度数量高，我们可能会在监督学习模型中遇到过拟合的问题。
- en: DEFINITION  When a supervised learning model has good accuracy on training data
    but lesser accuracy on unseen data, it is referred to as *overfitting*. Overfitting
    is a nuisance as the very aim of machine learning models is to work well on unseen
    datasets, and overfitting defeats this purpose.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：当监督学习模型在训练数据上具有良好的准确率但在未见数据上准确率较低时，这被称为**过拟合**。过拟合是一个麻烦，因为机器学习模型的根本目的是在未见数据集上表现良好，而过拟合则违背了这一目的。
- en: Let us relate things to a real-world example. Consider an insurance company
    offering different types of insurance policies like life insurance, vehicle insurance,
    health insurance, home insurance, etc. The company wishes to use data science
    and execute clustering use cases to enhance the customer base and the total number
    of policies sold. They have customer details like age, gender, profession, policy
    amount, historical transactions, number of policies held, annual income, type
    of policy, number of historical defaults, etc. At the same time, let us assume
    that variables like whether the customer is left-handed or right-handed, whether
    they wear black or brown shoes, what shampoo brand they use, the color of their
    hair, and their favorite restaurant are also captured. If we include all the variables
    in the dataset, the total number of variables in the resultant dataset will be
    quite high. The distance calculation will be more complex for a k-means clustering
    algorithm, the processing time will increase, and the overall solution will be
    quite complex.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个现实世界的例子来关联这些内容。考虑一家提供不同类型保险政策的保险公司，如人寿保险、车辆保险、健康保险、房屋保险等。该公司希望使用数据科学和执行聚类用例来扩大客户群和销售的总政策数。他们有客户详细信息，如年龄、性别、职业、保额、历史交易、持有的政策数量、年收入、政策类型、历史违约次数等。同时，让我们假设客户是否为左撇子或右撇子、他们是否穿黑色或棕色鞋子、他们使用的洗发水品牌、头发的颜色以及他们最喜欢的餐厅等信息也被记录。如果我们把所有变量都包含在数据集中，最终数据集中的变量总数将会相当高。对于k-means聚类算法，距离计算将更加复杂，处理时间将增加，整体解决方案也将相当复杂。
- en: It is also imperative to note that *not* all the dimensions or variables are
    significant. Hence, it is vital to filter out the important ones from all the
    variables we have. Remember, nature always prefers simpler solutions! In the case
    discussed previously, it is highly likely that variables like hair color and favorite
    restaurant, etc., will not affect the outputs. So it is in our best interest to
    reduce the number of dimensions to ease the complexity and reduce the computation
    time. At the same time, it is also vital to note that dimensionality reduction
    is not always desired. It depends on the type of dataset and the business problem
    we wish to resolve. We will explore this more when we work on the case study in
    subsequent sections of the chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还必须注意的是，**并非**所有维度或变量都是重要的。因此，从所有变量中筛选出重要的变量至关重要。记住，自然界总是偏好更简单的解决方案！在之前讨论的案例中，变量如发色和最喜欢的餐厅等，很可能不会影响输出。因此，减少维度以简化复杂性和减少计算时间是我们的最佳利益。同时，还必须注意的是，维度降低并不总是期望的。它取决于我们希望解决的类型数据集和业务问题。我们将在本章后续部分的案例研究中进一步探讨这一点。
- en: Exercise 3.1
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习3.1
- en: 'Answer these questions to check your understanding:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以检查你的理解：
- en: The curse of dimensionality refers to having a lot of data. True or False?
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维度诅咒指的是拥有大量数据。对还是错？
- en: Having a high number of variables will always increase the accuracy of a solution.
    True or False?
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量数量高总是会增加解决方案的准确率。对还是错？
- en: How does a large number of variables in a dataset affect the model?
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中大量变量是如何影响模型的？
- en: We have established that having a lot of dimensions is a challenge for us. We
    next examine the various methods to reduce the number of dimensions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定拥有很多维度对我们来说是一个挑战。接下来，我们将探讨各种降低维度数量的方法。
- en: 3.3 Dimension reduction methods
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 维度降低方法
- en: We studied the disadvantages of having really high-dimensional data in the last
    section. A fewer number of dimensions might result in a simpler structure for
    our data, which will be computationally efficient. At the same time, we should
    be careful when reducing the number of variables. The output of the dimension
    reduction method should be complete enough to represent the original data and
    should not lead to any information loss. In other words, if originally we had,
    for example, 500 variables and we reduced it to 120 significant ones, still these
    120 *should* be robust enough to capture *almost* all the information. Let us
    understand using a simple example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们研究了具有高维数据的缺点。更少的维度可能会导致我们的数据结构更简单，这将具有计算效率。同时，我们在减少变量数量时应该小心。降维方法的输出应该足够完整，以表示原始数据，并且不应导致任何信息丢失。换句话说，如果我们最初有，例如，500个变量，我们将它们减少到120个重要的变量，那么这120个变量*应该*足够稳健，以捕捉*几乎*所有的信息。让我们用一个简单的例子来理解。
- en: 'Consider this: we wish to predict the amount of rainfall a city will receive
    in the next month. The rainfall prediction for that city might be dependent on
    temperature over a period, wind speed measurements, pressure, distance from the
    sea, elevation above sea level, etc. These variables make sense if we wish to
    predict rainfall. At the same time, variables like the number of cinema halls
    in the city, whether the city is the capital of the country, or the number of
    red cars in the city will not affect the prediction of rainfall. In such a case,
    if we do not use the number of cinema halls in the city to predict the amount
    of rainfall, it will not reduce the capability of the system. The solution, in
    all probability, will still be able to perform quite well. Hence, in such a case,
    no information will be lost by dropping such a variable, and surely we can drop
    it from the dataset. On the other hand, removing variables such as temperature
    or distance from the ocean will very likely negatively affect the prediction accuracy.
    This is a very simple example highlighting the need to reduce the number of variables.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这种情况：我们希望预测一个城市下个月将接收到的降雨量。该城市的降雨预测可能依赖于一段时间内的温度、风速测量、气压、距离海洋的距离、海拔高度等因素。如果我们希望预测降雨量，这些变量是有意义的。同时，像城市中电影院的数量、该城市是否是国家的首都，或者城市中红色汽车的数量等因素不会影响降雨量的预测。在这种情况下，如果我们不使用城市中电影院的数量来预测降雨量，这不会降低系统的能力。很可能会，解决方案仍然能够表现得相当好。因此，在这种情况下，删除这样的变量不会丢失任何信息，我们当然可以从数据集中删除它。另一方面，删除诸如温度或距离海洋等因素可能会非常有可能地负面影响预测精度。这是一个简单的例子，强调了减少变量数量的必要性。
- en: The dimensions or the number of variables can be reduced by a combination of
    manual and algorithm-based methods. But before studying them in detail, there
    are a few mathematical terms and components we should be aware of, which we will
    discuss next.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 维度或变量的数量可以通过手动和基于算法的方法的组合来减少。但在详细研究它们之前，有一些数学术语和组成部分我们应该了解，我们将在下一节中讨论。
- en: 3.3.1 Mathematical foundation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 数学基础
- en: There are quite a few mathematical terms that one must know to develop a thorough
    understanding of dimensionality reduction methods. We are trying to reduce the
    number of dimensions of a dataset. A dataset is nothing but a matrix of values—thus,
    a lot of the concepts are related to matrix manipulation methods, their geometrical
    representation, and performing transformations on such matrices. The mathematical
    concepts are discussed in the appendix. You also need an understanding of eigenvalues
    and eigenvectors. These concepts will be reused throughout the book; they are
    been put in the appendix for quick reference. You are advised to go through them
    before proceeding.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要对降维方法有一个彻底的理解，必须知道很多数学术语。我们试图减少数据集的维度。数据集不过是一个值矩阵——因此，许多概念都与矩阵操作方法、它们的几何表示以及在这些矩阵上执行变换有关。数学概念在附录中讨论。您还需要了解特征值和特征向量。这些概念将在整本书中重复使用；它们被放在附录中以便快速参考。建议您在继续之前阅读它们。
- en: 3.4 Manual methods of dimensionality reduction
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 手动降维方法
- en: To tackle the curse of dimensionality, we wish to reduce the number of variables
    in a dataset. The reduction can be done by removing the variables from the dataset.
    Or a very simple solution for dimensionality reduction can be combining the variables
    that can be grouped logically or can be represented using a common mathematical
    operation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决维度灾难，我们希望减少数据集中的变量数量。这种减少可以通过从数据集中移除变量来实现。或者，一个非常简单的降维解决方案是将可以逻辑分组或可以使用共同数学运算表示的变量组合起来。
- en: For example, as shown in figure 3.3, the data can be from a retail store where
    different customers have generated different transactions. We will get the sales,
    the number of invoices, and the number of items bought by each customer over a
    period. In the table, customer 1 has generated two invoices, bought five items
    in total, and generated a total sale of 100\.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如图3.3所示，数据可以来自一家零售店，不同客户产生了不同的交易。我们将得到在一定时期内每个客户的销售额、发票数量和购买的项目数量。在表中，客户1产生了两个发票，总共购买了五个项目，并产生了100的总销售额。
- en: '![figure](../Images/CH03_F03_Verdhan.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F03_Verdhan.png)'
- en: Figure 3.3 In the first table, we have the sales, invoices, and number of items
    as the variables. In the second table, they have been combined to create new variables.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 在第一张表中，我们有销售、发票和项目数量作为变量。在第二张表中，它们已经被组合成新的变量。
- en: If we wish to reduce the number of variables, we might combine three variables
    into two variables. Here we have introduced variables average transaction value
    (ATV) and average basket size (ABS) wherein ATV = Sales/Invoices and ABS = Number
    Of Items/Invoices.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望减少变量的数量，我们可能会将三个变量合并成两个变量。在这里，我们引入了平均交易价值（ATV）和平均购物篮大小（ABS）这两个变量，其中ATV
    = 销售额/发票数，ABS = 项目数量/发票数。
- en: So, in the second table for customer 1, we have ATV as 50 and ABS as 2.5\. Hence,
    the number of variables has been reduced from three to two. The process here is
    only an example of how we can combine various variables. It does not mean that
    we should replace sales with ATV as a variable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在客户1的第二张表中，我们有ATV为50和ABS为2.5。因此，变量的数量已从三个减少到两个。这里的过程只是说明我们可以如何组合各种变量的一个例子。这并不意味着我们应该用ATV代替销售作为变量。
- en: 'This process can continue to reduce the number of variables. Similarly, for
    a telecom subscriber, say we have the minutes of mobile calls made during 30 days
    in a month. We can add them to create a single variable—minutes used in a month.
    These examples are very basic ones to start with. Using the manual process, we
    can employ two other commonly used methods: manual selection and using correlation
    coefficient.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以继续减少变量的数量。同样，对于一个电信用户，比如说我们有一个月内30天内手机通话的分钟数。我们可以将它们加起来创建一个单一变量——一个月内使用的分钟数。这些例子是非常基础的，作为开始。使用手动过程，我们可以采用两种其他常用的方法：手动选择和使用相关系数。
- en: 3.4.1 Manual feature selection
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 手动特征选择
- en: Continuing from the rainfall prediction example we discussed in the last section,
    a data scientist might be able to drop a few variables. This will be based on
    a deep understanding of the business problem at hand and the corresponding dataset
    being used. However, it is an underlying assumption that the dataset is quite
    comprehensible for the data scientist and that they understand the business domain
    well. Most of the time, the business stakeholders will be able to guide on such
    methods. The variables must also be unique, and not much dependency should exist.
    As shown in figure 3.4, we can remove a few of the variables that might not be
    useful for predicting rainfall.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论上一节中提到的降雨预测示例，数据科学家可能能够删除一些变量。这将是基于对当前业务问题的深入理解和所使用的相应数据集。然而，一个基本的假设是数据集对数据科学家来说是相当可理解的，并且他们很好地理解了业务领域。大多数时候，业务利益相关者将能够指导这些方法。变量也必须是唯一的，并且不应存在太多的依赖性。如图3.4所示，我们可以移除一些可能对预测降雨不有用的变量。
- en: '![figure](../Images/CH03_F04_Verdhan.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F04_Verdhan.png)'
- en: Figure 3.4 In the first table, we have all the variables present in the dataset.
    Using business logic, some of the variables that might not be of much use have
    been discarded in the second table. But this is to be done with due caution; the
    best way is to get guidance from the business stakeholders.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 在第一张表中，我们有数据集中所有的变量。使用业务逻辑，第二张表中已经丢弃了一些可能不太有用的变量。但这是需要谨慎进行的；最好的方式是获得业务利益相关者的指导。
- en: Sometimes, feature selection methods are also referred to as *wrapper methods*.
    Here, a machine learning model is wrapped or fitted with a subset of variables.
    In each iteration, we will get a different set of results. The set that generates
    the best results is selected for the final model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，特征选择方法也被称为*包装方法*。在这里，机器学习模型被包裹或拟合到变量子集。在每次迭代中，我们将得到一组不同的结果。产生最佳结果的那组被选为最终模型。
- en: 3.4.2 Correlation coefficient
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 相关系数
- en: Correlation between two variables simply means that they have a mutual relationship
    with each other. The change in the value of one variable will affect the value
    of another, which means that data points with similar values in one variable have
    similar values for the other variable. The variables that are highly correlated
    with each other supply similar information, so one of them can be dropped.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 两个变量之间的相关性简单地说就是它们彼此之间有相互关系。一个变量的值的变化将影响另一个变量的值，这意味着在一个变量中具有相似值的点在另一个变量中也有相似的值。彼此高度相关的变量提供相似的信息，因此可以删除其中一个。
- en: NOTE  Correlation is described in detail in the appendix.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：相关性在附录中有详细描述。
- en: For example, for a retail store, the number of invoices generated in a day will
    be highly correlated with the amount of sales generated, so one of them can be
    dropped. Another example is students who study for a higher number of hours will
    have better grades than the ones who study less (mostly!).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一家零售店，一天内生成的发票数量将与产生的销售额高度相关，因此可以删除其中一个。另一个例子是，学习时间更长的大学生将比学习时间较短的学生（大多数情况下）获得更好的成绩。
- en: But we should be careful in dropping the variables and not trust correlation
    alone. The business context of a variable should be thoroughly understood before
    making any decision.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们在删除变量时应该小心，不要仅仅依赖相关性。在做出任何决定之前，应该彻底了解变量的业务背景。
- en: NOTE  It is a good idea to discuss this with the business stakeholders before
    dropping any variables from the study.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在删除任何变量之前与业务利益相关者讨论是个好主意。
- en: Correlation-based methods are sometimes called *filter methods*. Using correlation
    coefficients, we can filter and choose the variables that are most significant.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相关性的方法有时被称为*过滤方法*。使用相关系数，我们可以过滤和选择最显著的变量。
- en: Exercise 3.2
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 3.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以检查你的理解：
- en: We can drop a variable simply if we feel it is not required. True or False?
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们认为某个变量不是必需的，我们可以简单地删除它。对还是错？
- en: If two variables are correlated, always drop one of them. True or False?
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果两个变量相关，总是删除其中一个。对还是错？
- en: Manual methods are easier solutions and can be executed quite efficiently. The
    dataset size is reduced, and we can proceed with the analysis. But manual methods
    are sometimes subjective and depend a lot on the business problem at hand. Many
    times, it is also not possible to employ manual methods for dimension reduction.
    In such situations, we have algorithm-based methods, which we study in the next
    section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 手动方法是更简单的解决方案，可以相当高效地执行。数据集的大小减少了，我们可以继续分析。但是，手动方法有时是主观的，并且很大程度上取决于手头的业务问题。很多时候，也不可能使用手动方法进行降维。在这种情况下，我们有基于算法的方法，我们将在下一节中研究。
- en: 3.4.3 Algorithm-based methods for reducing dimensions
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 降低维度的基于算法的方法
- en: We examined manual methods in the last section. Continuing from there, we examine
    algorithm-based methods in this section. The algorithm-based techniques are based
    on a more mathematical base and hence prove to be more scientific methods. In
    real-world business problems, we use a combination of both manual and algorithm-based
    techniques. Manual methods are straightforward to execute as compared to algorithm-based
    techniques. Also, we cannot comment on the comparison of both techniques, as they
    are based on different foundations. But at the same time, it is imperative that
    you put due diligence into the implementation of algorithm-based techniques.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了手动方法。在此基础上，本节我们将探讨基于算法的方法。基于算法的技术建立在更数学的基础上，因此证明是更科学的方法。在现实世界的商业问题中，我们使用手动和基于算法技术的组合。与基于算法的技术相比，手动方法执行起来更为直接。此外，由于它们基于不同的基础，我们无法对这两种技术的比较做出评论。但是，同时，在实施基于算法的技术时，进行充分的尽职调查是至关重要的。
- en: 'The major techniques used in dimensionality reductions are listed as follows.
    We explore some of them in this book:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 降维中使用的的主要技术如下。我们将在本书中探讨其中的一些：
- en: PCA
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: SVD
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD
- en: LDA
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA
- en: Generalized discriminant analysis
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义判别分析
- en: Non-negative matrix factorization
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非负矩阵分解
- en: Multidimension scaling
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维尺度
- en: Locally linear embeddings
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部线性嵌入
- en: IsoMaps
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IsoMaps
- en: Autoencoders
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器
- en: t-SNE
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE
- en: 'These techniques are utilized for the common end goal: transform the data from
    a high-dimensional space to a low-dimensional one. Some of the data transformations
    are linear in nature, while some are nonlinear.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术被用于一个共同的目标：将数据从高维空间转换到低维空间。其中一些数据转换是线性的，而另一些是非线性的。
- en: We discuss PCA and SVD in detail in this chapter. In the later chapters of the
    book, other major techniques will be explored. PCA is perhaps the most quoted
    dimensionality reduction method, which is explored in the next section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中详细讨论PCA和SVD。在本书的后续章节中，将探索其他主要技术。PCA可能是被引用最多的降维方法，将在下一节中进行探讨。
- en: 3.5 Principal component analysis
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 主成分分析
- en: 'Consider this: you are working on a dataset that has 250 variables. It is almost
    impossible to visualize such a high-dimensional space. Some of the 250 variables
    might be correlated with each other and some of them might not be, and there is
    a need to reduce the number of variables without losing much information. PCA
    allows us to mathematically select the most important features and leave the rest.
    PCA does reduce the number of dimensions but also preserves the most important
    relationships between the variables and the important structures in the dataset.
    Hence, the number of variables is reduced, but the important information in the
    dataset is kept safe.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这种情况：你正在处理一个包含250个变量的数据集。几乎不可能可视化这样一个高维空间。其中一些变量可能相互关联，而另一些可能不相关，并且需要在不丢失太多信息的情况下减少变量的数量。PCA使我们能够从数学上选择最重要的特征，并保留其余部分。PCA确实减少了维度数量，但同时也保留了变量之间以及数据集中重要结构之间最重要的关系。因此，变量的数量减少了，但数据集中的重要信息得到了安全保留。
- en: PCA is a projection of high-dimensional data in lower dimensions. In simpler
    terms, we are reducing an *n*-dimensional space into an *m*-dimensional one where
    *n* > *m* while maintaining the nature and the essence of the original dataset.
    In the process, the old variables are reduced to newer ones while maintaining
    the crux of the original dataset. The new variables thus created are called *principal
    components*. The principal components are a linear combination of the raw variables.
    As a result of this transformation, the first principal component captures the
    maximum randomness or the highest variance in the dataset. The second principal
    component created is orthogonal to the first component.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是将高维数据投影到低维空间的过程。用更简单的话说，我们正在将一个*n*-维空间减少到一个*m*-维空间，其中*n* > *m*，同时保持原始数据集的性质和本质。在这个过程中，旧变量被减少到新的变量，同时保持原始数据集的核心。因此创建的新变量被称为*主成分*。主成分是原始变量的线性组合。这种变换的结果是，第一个主成分捕捉到数据集中的最大随机性或最高方差。创建的第二个主成分与第一个成分正交。
- en: NOTE  If two straight lines are orthogonal to each other, it means they are
    at an angle of 90˚ to each other.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果两条直线相互垂直，这意味着它们之间的角度是90˚。
- en: The process continues to the third component and so on. Orthogonality allows
    us to maintain that there is no correlation between subsequent principal components.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程继续到第三成分，依此类推。正交性使我们能够保持后续主成分之间没有相关性。
- en: NOTE  PCA utilizes linear transformation of the dataset, and such methods are
    sometimes referred to as feature projections. The resultant dataset or the projection
    is used for further analysis.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：PCA利用数据集的线性变换，这类方法有时被称为特征投影。得到的或投影的数据集用于进一步分析。
- en: Let us understand this better using an example. In figure 3.5, we have represented
    the total perceived value of a home using some variables. The variables are area
    (sq m), number of bedrooms, number of balconies, distance from the airport, distance
    from the train station, and so on; we have 100+ variables.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子更好地理解这一点。在图3.5中，我们使用一些变量表示了房屋的总感知价值。这些变量包括面积（平方米）、卧室数量、阳台数量、机场距离、火车站距离等；我们有100多个变量。
- en: '![figure](../Images/CH03_F05_Verdhan.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F05_Verdhan.png)'
- en: Figure 3.5 The variables on which the price of a house can be estimated
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5 估计房价的变量
- en: We can combine some of the variables mathematically and logically. PCA will
    create a new variable that is a linear combination of some of the variables, as
    shown in the following example. It will get the best *linear* combination of original
    variables so that the new variable is able to capture the maximum variance of
    the dataset. Equation 3.1 is only an example shown for illustration purposes wherein
    we are showing a new variable created by a combination of other variables.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数学和逻辑上组合一些变量。主成分分析（PCA）将创建一个新变量，它是某些变量的线性组合，如下面的示例所示。它将得到原始变量的最佳*线性*组合，以便新变量能够捕捉到数据集的最大方差。方程式3.1仅是一个示例，用于说明目的，其中我们展示了由其他变量的组合创建的新变量。
- en: (3.1)
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.1)
- en: new_variable = *a**area – *b**bedrooms + *c**distance – *d**schools
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: new_variable = *a**面积 – *b**卧室数 + *c**距离 – *d**学校数
- en: Now let’s understand the concept visually. In a vector-space diagram, we can
    represent the dataset, as shown in figure 3.6\. The left figure represents the
    raw data where we can visualize the variables in an x-y diagram. As discussed
    earlier, we wish to create a linear combination of variables. In other words,
    we wish to create a mathematical equation that will be able to explain the relationship
    between x and y.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从视觉上理解这个概念。在向量空间图中，我们可以表示数据集，如图3.6所示。左图表示原始数据，其中我们可以在x-y图中可视化变量。如前所述，我们希望创建变量的线性组合。换句话说，我们希望创建一个数学方程，该方程能够解释x和y之间的关系。
- en: '![figure](../Images/CH03_F06_Verdhan.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F06_Verdhan.png)'
- en: Figure 3.6 The dataset can be represented in a vector-space diagram (left).
    The straight line can be called the line of best fit having the projections of
    all the data points on it (middle). The differences between the actual value and
    the projections are the error terms (right).
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 数据集可以用向量空间图表示（左）。直线可以称为最佳拟合线，因为它包含了所有数据点的投影（中间）。实际值与投影之间的差异是误差项（右）。
- en: The output of such a process will be a straight line as shown in the middle
    diagram in figure 3.6\. This straight line is sometimes referred to as the *line
    of best fit.* Using this line of best fit, we can predict a value of y for a given
    value of x. These predictions are nothing but the projections of data points on
    a straight line.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的结果将是一条直线，如图3.6中间图所示。这条直线有时被称为*最佳拟合线*。使用这条最佳拟合线，我们可以预测给定x值的y值。这些预测不过是数据点在直线上的投影。
- en: The difference between the actual value and the projections is the error, as
    shown in the right diagram in figure 3.6\. The total sum of these errors is called
    the total projection error.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值与投影之间的差异是误差，如图3.6右图所示。这些误差的总和称为总投影误差。
- en: There can be multiple options for this straight line, as shown in figure 3.7\.
    These different straight lines will have different errors and different values
    of variances captured.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这条直线，可能有多种选择，如图3.7所示。这些不同的直线将具有不同的误差和不同的方差值。
- en: '![figure](../Images/CH03_F07_Verdhan.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F07_Verdhan.png)'
- en: Figure 3.7 The dataset can be captured by several lines, but not all the straight
    lines will be able to capture the maximum variance. The equation that gives the
    minimum error will be the one chosen.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 数据集可以通过几条直线来捕捉，但并非所有直线都能捕捉到最大方差。给出最小误差的方程将被选中。
- en: The straight line that can capture the maximum variance will be the chosen one.
    In other words, it gives the minimum error. It will be the *first principal component,*
    and the direction of maximum spread will be the *principal axis*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 能够捕捉最大方差的直线将被选中。换句话说，它给出了最小的误差。它将是*第一个主成分*，最大扩散的方向将是*主轴*。
- en: The second principal component will be derived in a similar fashion. Since we
    know the first principal axis, we can subtract the variance along this principal
    axis from the total variance to get the residual variance. In other words, using
    the first principal component, we would capture some variance in the dataset.
    But there will be a portion of the total variance in the dataset that is still
    unexplained by the first principal component. The portion of the total variance
    unexplained is the residual variance. Using the second principal component, we
    wish to capture as much variance as we can.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主成分将以类似的方式推导出来。由于我们知道第一个主成分轴，我们可以从这个主成分轴的方差中减去总方差，以得到剩余方差。换句话说，使用第一个主成分，我们会捕捉到数据集中的一些方差。但数据集中的总方差中仍有一部分未被第一个主成分解释。未被解释的总方差部分是剩余方差。使用第二个主成分，我们希望尽可能多地捕捉到方差。
- en: Using the same process to capture the direction of maximum variance, we will
    get the second principal component. The second principal component can be at several
    angles with respect to the first one, as shown in figure 3.8\. It is mathematically
    proven that if the second principal component is orthogonal (i.e., 90˚)to the
    first principal component, this allows us to capture the maximum variance using
    the two principal components. In figure 3.8, we can observe that the two principal
    components are at an angle of 90˚ to each other.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的过程来捕捉最大方差的方向，我们将得到第二个主成分。第二个主成分可以相对于第一个主成分以几个不同的角度存在，如图3.8所示。数学上已经证明，如果第二个主成分与第一个主成分正交（即90˚），这将使我们能够通过两个主成分捕捉到最大方差。在图3.8中，我们可以观察到两个主成分之间的角度是90˚。
- en: '![figure](../Images/CH03_F08_Verdhan.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F08_Verdhan.png)'
- en: Figure 3.8 The first figure on the left is the first principal component. The
    second principal component can be at different angles with respect to the first
    principal component (middle). We should find the second principle that allows
    us to capture the maximum variance. To capture the maximum variance, the second
    principal component should be orthogonal to the first one, and thus the combined
    variance captured is maximized (right).
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 左侧的第一个图是第一个主成分。第二个主成分可以相对于第一个主成分以不同的角度存在（中间）。我们应该找到第二个主成分，使我们能够捕捉到最大方差。为了捕捉最大方差，第二个主成分应该与第一个主成分正交，从而使得捕捉到的总方差最大化（右侧）。
- en: The process continues for the third and fourth principal components and so on.
    With more principal components, the representation in a vector space becomes difficult
    to visualize. You can think of a vector space diagram with more than three axes.
    Once all the principal components are derived, the dataset is projected onto these
    axes. The columns in this transformed dataset are the *principal components*.
    The principal components created will be fewer than the number of original variables
    and will capture the maximum information present in the dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程继续应用于第三和第四主成分，依此类推。随着主成分数量的增加，向量空间中的表示变得难以可视化。你可以想象一个具有三个以上轴的向量空间图。一旦所有主成分都被推导出来，数据集就被投影到这些轴上。这个转换后的数据集的列是*主成分*。创建的主成分数量将少于原始变量的数量，并将捕捉到数据集中存在的最大信息。
- en: 'Before we examine the process of PCA in-depth, let’s study its important characteristics:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨PCA的过程之前，让我们研究其重要特性：
- en: PCA aims to reduce the number of dimensions in the resultant dataset.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA旨在减少结果数据集的维度数。
- en: PCA produces principal components that aim to reduce the noise in the dataset
    by maximizing the feature variance.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA产生主成分，旨在通过最大化特征方差来减少数据集中的噪声。
- en: At the same time, the principal components reduce the redundancy in the dataset.
    This is achieved by minimizing the covariance between the pairs of features.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，主成分减少了数据集中的冗余。这是通过最小化特征对之间的协方差来实现的。
- en: The original variables no longer exist in the newly created dataset. Instead,
    new variables are created using these variables.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始变量不再存在于新创建的数据集中。相反，使用这些变量创建了新变量。
- en: It is not necessary that the principal components map one-to-one with all the
    variables present in the dataset. They are a new combination of the existing variables.
    Hence, they can be a combination of several different variables in one principal
    component (as shown in equation 3.1).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分不一定与数据集中所有变量一一对应。它们是现有变量的新组合。因此，它们可以是一个主成分中几个不同变量的组合（如方程3.1所示）。
- en: The new features created from the dataset do not share the same column names.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中创建的新特征不共享相同的列名。
- en: The original variables might be correlated with each other, but the newly created
    variables are unrelated to each other.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始变量可能彼此相关，但新创建的变量彼此无关。
- en: The number of newly created variables is fewer than the original number of variables.
    The process to select the number of principal components has been described in
    section 3.5.2\. After all, that is the whole purpose of dimensionality reduction.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新创建的变量数量少于原始变量的数量。选择主成分数量的过程已在3.5.2节中描述。毕竟，这就是降维的全部目的。
- en: If PCA has been used for reducing the number of variables in a training dataset,
    the testing/validation datasets should be reduced by using PCA.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果PCA已用于减少训练数据集中的变量数量，则应使用PCA减少测试/验证数据集。
- en: PCA is not synonymous with dimensionality reduction only. It can be put into
    use for a number of other usages beyond dimensionality reduction like feature
    extraction, data visualization, multicollinearity detection, preprocessing, etc.
    Using a PCA only for dimensionality reduction will be a misnomer for sure.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）不仅仅等同于降维。它还可以用于降维以外的多种用途，如特征提取、数据可视化、多重共线性检测、预处理等。仅将PCA用于降维肯定是一个误称。
- en: 'We will now examine the approach used while implementing PCA, and then we will
    develop a Python solution using PCA. We need not apply all the steps while we
    develop the codes, as the heavy lifting has already been done by the packages
    and libraries. The steps given here are taken care of by the packages, but still,
    it is imperative that you understand these steps to properly appreciate how PCA
    works:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将检查实现PCA时使用的方法，然后我们将开发一个使用PCA的Python解决方案。在我们编写代码时，不需要应用所有步骤，因为重头戏已经被包和库完成了。这里给出的步骤由包处理，但仍然，理解这些步骤对于正确欣赏PCA的工作原理至关重要：
- en: In PCA, we start with *normalizing our dataset* as a first step. It ensures
    that all our variables have a common representation and become comparable. We
    have methods to perform the normalization in Python, which we will study when
    we develop the code. To explore more about normalizing the dataset, see the appendix.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PCA中，我们首先将数据集*标准化*作为第一步。这确保了所有变量都有共同的表现形式并变得可比较。Python中有执行标准化的方法，我们将在编写代码时学习。要了解更多关于数据集标准化的信息，请参阅附录。
- en: Get the covariance in the normalized dataset. It allows us to study the relationship
    between the variables. We generally create a covariance matrix, as shown in the
    Python example in the next section.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标准化后的数据集中获取协方差。这使我们能够研究变量之间的关系。我们通常创建一个协方差矩阵，如下一节中的Python示例所示。
- en: We can then calculate the eigenvectors and eigenvalues of the covariance matrix.
    The mathematical concept of eigenvectors is given in the appendix.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以计算协方差矩阵的特征向量和特征值。特征向量的数学概念在附录中有介绍。
- en: We then sort the eigenvalues in decreasing order of eigenvalues. We choose the
    eigenvectors corresponding to the maximum value of eigenvalues. The components
    chosen will be able to capture the maximum variance in the dataset. There are
    other methods to shortlist the principal components, which we will explore while
    we develop the Python code.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后按特征值降序排列特征值。我们选择与特征值最大值对应的特征向量。所选的成分将能够捕获数据集中的最大方差。还有其他方法来筛选主成分，我们将在开发Python代码时探讨。
- en: Exercise 3.3
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 3.3
- en: 'Answer these questions to check your understanding:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: PCA will result in the same number of variables in the dataset. True or False?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA将导致数据集中的变量数量相同。对或错？
- en: PCA will be able to capture 100% of the information in the dataset. True or
    False?
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA将能够捕获数据集中的100%信息。对或错？
- en: What is the logic of selecting principal components in PCA?
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA中主成分选择的逻辑是什么？
- en: So, in essence, principal components are the linear combinations of the original
    variables. The weight in this linear combination is the eigenvector satisfying
    the error criteria of the least square method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本质上，主成分是原始变量的线性组合。在这个线性组合中的权重是满足最小二乘法误差标准的特征向量。
- en: 3.5.1 Eigenvalue decomposition
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 特征值分解
- en: In the context of PCA, the eigenvector will represent the direction of the vector
    and the eigenvalue will be the variance that is captured along that eigenvector.
    See figure 3.9, where we break the original *n* x *n* matrix into components.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA的上下文中，特征向量将代表向量的方向，特征值将是沿着该特征向量捕获的方差。参见图3.9，其中我们将原始 *n* x *n* 矩阵分解成组件。
- en: '![figure](../Images/CH03_F09_Verdhan.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F09_Verdhan.png)'
- en: Figure 3.9 Using eigenvalue decomposition, the original matrix can be broken
    into an eigenvector matrix, an eigenvalue matrix, and an inverse of an eigenvector
    matrix. We implement PCA using this methodology.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9 使用特征值分解，原始矩阵可以被分解为一个特征向量矩阵、一个特征值矩阵和一个特征向量的逆矩阵。我们使用这种方法实现PCA。
- en: Mathematically, we can show the relation with equation 3.2
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上，我们可以用方程3.2展示其关系
- en: (3.2)
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （3.2）
- en: '*A***v* = *λ***v*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*A***v* = *λ***v*'
- en: where *A* is a square matrix, *v* is the eigenvector, and *λ* is the eigenvalue.
    Here, it is important to note that the eigenvector matrix is the orthonormal matrix,
    and its columns are eigenvectors. The eigenvalue matrix is the diagonal matrix,
    and its eigenvalues are the diagonal elements. The last component is the inverse
    of the eigenvector matrix. Once we have the eigenvalues and the eigenvectors,
    we can choose the significant eigenvectors for getting the principal components.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 是一个方阵，*v* 是特征向量，*λ* 是特征值。在这里，重要的是要注意特征向量矩阵是正交矩阵，其列是特征向量。特征值矩阵是对角矩阵，其对角元素是特征值。最后一个组件是特征向量矩阵的逆。一旦我们有了特征值和特征向量，我们可以选择重要的特征向量来获取主成分。
- en: We present PCA and SVD as two separate methods in this book. Both methods are
    used to reduce high-dimensional data into lower-dimensional ones and, in the process,
    retain the maximum information in the dataset. The difference between the two
    is SVD exists for any sort of matrix (rectangular or square), whereas eigen decomposition
    is possible only for square matrices. You will understand it better once we have
    covered SVD later in this chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将PCA和SVD作为两种独立的方法进行介绍。这两种方法都用于将高维数据降低到低维数据，并在过程中保留数据集中的最大信息。两者的区别在于SVD适用于任何类型的矩阵（矩形或方形），而特征值分解仅适用于方形矩阵。在我们后面章节中介绍了SVD之后，你会更好地理解这一点。
- en: 3.5.2 Python solution using PCA
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用PCA的Python解决方案
- en: We have studied the concepts of PCA and the process using eigenvalue decomposition.
    It is time for us to dive into Python and develop a PCA solution on a dataset.
    I will show you how to create eigenvectors and eigenvalues on the dataset. To
    implement the PCA algorithms, we will use the `sklearn` library. Libraries and
    packages provide a faster solution for implementing algorithms.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了PCA的概念和特征值分解的过程。现在是时候深入Python，在数据集上开发PCA解决方案了。我将向你展示如何在数据集上创建特征向量和特征值。为了实现PCA算法，我们将使用`sklearn`库。库和包提供了实现算法的更快解决方案。
- en: 'We use the Iris dataset for this problem. It is one of the most popular datasets
    used for machine learning problems. The dataset contains data of three iris species
    with 50 samples each and having properties of each flower, like petal length,
    sepal length, etc. The objective of the problem is to predict the species using
    the properties of the flower. The independent variables, hence, are the flower
    properties, whereas the variable “species” is the target variable. The dataset
    and the code are checked in at the GitHub repository. Here we are using the inbuilt
    PCA functions, which reduce the effort required to implement PCA. The steps are
    as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Iris数据集来解决这个问题。这是机器学习问题中最常用的数据集之一。该数据集包含三种鸢尾花物种的数据，每种物种有50个样本，并具有每个花朵的属性，如花瓣长度、萼片长度等。问题的目标是使用花朵的属性来预测物种。因此，独立变量是花朵属性，而“物种”变量是目标变量。数据集和代码已存入GitHub仓库。在这里，我们使用内置的PCA函数，这减少了实现PCA所需的工作量。步骤如下：
- en: Load all the necessary libraries. We are going to use `numpy`, `pandas`, `seaborn`,
    `matplotlib`, and  `sklearn`. Note that we have imported PCA from `sklearn`.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所有必要的库。我们将使用`numpy`、`pandas`、`seaborn`、`matplotlib`和`sklearn`。请注意，我们已经从`sklearn`中导入了PCA。
- en: 'NOTE  The following are the standard libraries. You will find that almost all
    the machine learning solutions would import these libraries in the solution notebook:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：以下是一些标准库。你会发现几乎所有的机器学习解决方案都会在解决方案笔记本中导入这些库：
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Load the dataset now. It is a .csv file:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 现在加载数据集。它是一个.csv文件：
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3\. We will now perform a basic check on the dataset, looking at the first five
    rows, the shape of the data, the spread of the variables, etc. We are not performing
    an extensive exploratory data analysis here as the steps are covered in chapter
    2\. The dataset has 150 rows and 6 columns (see figure 3.10).
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 我们现在将对数据集进行基本检查，查看前五行，数据的形状，变量的分布等。我们在这里不进行广泛的数据探索分析，因为这些步骤已在第2章中介绍。该数据集有150行和6列（见图3.10）。
- en: '[PRE2]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![figure](../Images/CH03_UN01_Verdhan.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_UN01_Verdhan.png)'
- en: '[PRE3]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![figure](../Images/CH03_F10_Verdhan.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F10_Verdhan.png)'
- en: Figure 3.10 Code output
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10 代码输出
- en: '4\. Here, we should break the dataset into independent variables and a target
    variable. `X_variables` here represent the independent variables, which are in
    columns 2–5 of the dataset while `y_variable` is the target variable, which is
    “species” in this case and is the final column in the dataset. Recall we wish
    to predict the species of a flower using the other properties. Hence, we have
    separated the target variable “species” and other independent variables:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 在这里，我们应该将数据集分为独立变量和目标变量。`X_variables`在这里代表独立变量，它们位于数据集的2-5列，而`y_variable`是目标变量，在这种情况下是“物种”，它是数据集的最后一列。回想一下，我们希望使用其他属性来预测花的物种。因此，我们已经将目标变量“物种”和其他独立变量分开：
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 5\. Normalize the dataset. The built-in method of `StandardScalar()` does the
    job for us quite easily.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 规范化数据集。`StandardScalar()`的内置方法非常容易地完成了这项工作。
- en: NOTE  The `StandardScalar()` method normalizes the dataset for us. It subtracts
    the mean from the variable and divides it by the standard deviation. For more
    details on normalization, refer to the appendix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：`StandardScalar()`方法为我们规范化了数据集。它从变量中减去平均值，然后除以标准差。有关规范化的更多详细信息，请参阅附录。
- en: 'We invoke the method and then use it on our dataset to get the transformed
    dataset. Since we are working on independent variables, we are using `X_variables`
    here. First, we invoke the `StandardScalar()` method. Then we use the `fit_transform`
    method. The `fit_transform` method first fits the transformers to *X* and *Y*
    and then returns a transformed version of *X*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用该方法，然后将其应用于我们的数据集以获取转换后的数据集。由于我们正在处理独立变量，所以我们在这里使用`X_variables`。首先，我们调用`StandardScalar()`方法。然后我们使用`fit_transform`方法。`fit_transform`方法首先将转换器拟合到*X*和*Y*，然后返回*X*的转换版本：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '6\. Calculate the covariance matrix and print it. The output is shown in figure
    3.11\. Getting the covariance matrix is straightforward using `numpy`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 计算协方差矩阵并打印出来。输出显示在图3.11中。使用`numpy`获取协方差矩阵非常直接：
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![figure](../Images/CH03_F11_Verdhan.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F11_Verdhan.png)'
- en: Figure 3.11 The covariance matrix
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11 协方差矩阵
- en: 7\. Calculate the eigenvalues. Inside the `numpy` library, we have the built-in
    functionality to calculate the eigenvalues. We will then sort the eigenvalues
    in descending order. To shortlist the principal components, we can choose eigenvalues
    greater than 1\. This criterion is called *Kaiser criteria.* We are exploring
    other methods too.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 计算特征值。在`numpy`库中，我们有内置的功能来计算特征值。然后我们将特征值按降序排序。为了筛选主成分，我们可以选择大于1的特征值。这个标准被称为*凯撒标准*。我们也在探索其他方法。
- en: NOTE  The eigenvalue represents how good a component is as a summary of the
    data. If the eigenvalue is 1, it means that the component contains the same amount
    of information as a single variable; hence, we choose the eigenvalue that is greater
    than 1\.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：特征值表示一个成分作为数据摘要的好坏。如果特征值为1，这意味着该成分包含与单个变量相同数量的信息；因此，我们选择大于1的特征值。
- en: 'In this code, first we get the `eigen_values` and `eigen_vectors`, and then
    we arrange them in descending order (see figure 3.12):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，首先我们获取`eigen_values`和`eigen_vectors`，然后按降序排列（见图3.12）：
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![figure](../Images/CH03_F12_Verdhan.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F12_Verdhan.png)'
- en: Figure 3.12 Eigenvalues arranged in descending order
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12 按降序排列的特征值
- en: '8\. Invoke the PCA method from the `sklearn` library. The method is used to
    fit the data here. Note we have not yet determined the number of principal components
    we wish to use in this problem:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 从`sklearn`库中调用PCA方法。该方法用于此处拟合数据。注意，我们尚未确定在这个问题中希望使用的主成分数量：
- en: '[PRE8]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '9\. The principal components are now set. Let’s have a look at the variance
    explained by them. We can observe that the first component captures 72.77% variation,
    the second captures 23.03% variation, and so on (figure 3.13):'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 9. 现在主成分已经设置好了。让我们看看它们所解释的方差。我们可以观察到第一个成分捕捉到了72.77%的变异，第二个成分捕捉到了23.03%的变异，等等（见图3.13）：
- en: '[PRE9]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![figure](../Images/CH03_F13_Verdhan.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F13_Verdhan.png)'
- en: Figure 3.13 The degree of variance of the principal components
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13主成分的方差程度
- en: '10\. We now plot the components in a bar plot for better visualization (see
    figure 3.14):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10. 我们现在将分量绘制成条形图以获得更好的可视化（见图3.14）：
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![figure](../Images/CH03_F14_Verdhan.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F14_Verdhan.png)'
- en: Figure 3.14 Bar plot of the principal components
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14主成分条形图
- en: '11\. Here we draw a scree plot to visualize the cumulative variance being explained
    by the principal components (see figure 3.15):'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 11. 在这里，我们绘制一个斯克里普图来可视化主成分所解释的累积方差（见图3.15）：
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![figure](../Images/CH03_F15_Verdhan.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F15_Verdhan.png)'
- en: Figure 3.15 Scree plot of cumulative variance
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15累积方差斯克里普图
- en: '12\. In this case study, we choose the top two principal components as the
    final solutions, as these two capture 95.08% of the total variance in the dataset:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 12. 在这个案例研究中，我们选择前两个主成分作为最终解决方案，因为这两个成分捕捉到了数据集中95.08%的总方差：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '13\. We will now plot the dataset with respect to two principal components.
    For that, species must be tied back to the actual values of the species variable,
    which are `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`. Here, `0` is
    mapped to `Iris-setosa`, `1` is `Iris-versicolor`, and `2` is `Iris-virginica`.
    In the following code, first the species variable gets its values replaced by
    using the mapping discussed earlier:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 13. 我们现在将数据集绘制在两个主成分上。为此，物种必须回溯到物种变量的实际值，即`Iris-setosa`、`Iris-versicolor`和`Iris-virginica`。在这里，`0`映射到`Iris-setosa`，`1`是`Iris-versicolor`，`2`是`Iris-virginica`。在下面的代码中，首先使用前面讨论的映射替换物种变量的值：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '14\. We will now plot the results with respect to two principal components.
    The plot shows the dataset reduced to two principal components we have just created.
    These principal components can capture 95.08% variance of the dataset. The first
    principal component represents the x-axis in the plot while the second principal
    component represents the y-axis in the plot (see figure 3.16). The color represents
    the various classes of Species. The print version of the book will not show the
    different colors, but the output of the Python code will. The same output is also
    available at the GitHub repository:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 14. 我们现在将结果绘制在两个主成分上。该图显示了将数据集减少到我们刚刚创建的两个主成分。这些主成分可以捕捉到数据集95.08%的方差。第一个主成分代表图中的x轴，而第二个主成分代表图中的y轴（见图3.16）。颜色代表物种的不同类别。书籍的印刷版不会显示不同的颜色，但Python代码的输出会显示。相同的输出也可以在GitHub仓库中找到：
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![figure](../Images/CH03_F16_Verdhan.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F16_Verdhan.png)'
- en: Figure 3.16 The results for two principal components
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16两个主成分的结果
- en: This solution has reduced the number of components from four to two and still
    is able to retain most of the information. Here, we have examined three approaches
    to select the principal components based on the Kaiser criteria, the variance
    captured, and the scree plot.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此解决方案已将成分数量从四个减少到两个，同时仍能保留大部分信息。在这里，我们检查了基于凯撒标准、捕获的方差和斯克里普图的三种选择主成分的方法。
- en: Let us quickly analyze what we have achieved using PCA. Figure 3.17 shows two
    representations of the same dataset. The one on the left is the original dataset
    of X_variables. It has four variables and 150 rows. The right is the output of
    PCA. It has 150 rows but only two variables. Recall we have reduced the number
    of dimensions from four to two. So, the number of observations has remained 150,
    while the number of variables has reduced from four to two.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速分析一下我们使用PCA所取得的成果。图3.17显示了同一数据集的两个表示。左边是X_variables的原始数据集。它有四个变量和150行。右边是PCA的输出。它有150行，但只有两个变量。回想一下，我们已经将维度数量从四个减少到两个。因此，观测数保持150个，而变量数量从四个减少到两个。
- en: '![figure](../Images/CH03_F17_Verdhan.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F17_Verdhan.png)'
- en: Figure 3.17 The figure on the left shows the original dataset, which has 150
    rows and four variables. After the implementation of PCA at right, the number
    of variables has been reduced to two. The number of rows remains the same as 150,
    which is shown by the length of pca_2d.
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.17 左侧的图显示了原始数据集，它有 150 行和四个变量。在右侧实施 PCA 之后，变量的数量已减少到两个。行数保持不变，为 150，这由 pca_2d
    的长度表示。
- en: Once we have reduced the number of components, we can continue to implement
    a supervised learning or an unsupervised learning solution. We can implement the
    preceding solution for any of the other real-world problems where we aim to reduce
    the number of dimensions. We explore this more in section 3.8\.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们减少了成分的数量，我们就可以继续实施监督学习或无监督学习解决方案。我们可以为任何其他旨在减少维度数的目标现实世界问题实施前面的解决方案。我们将在第
    3.8 节中进一步探讨这一点。
- en: With this, we have covered PCA. The GitHub repository contains a very interesting
    PCA decomposition with variables and a corresponding plot.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经涵盖了主成分分析（PCA）。GitHub 仓库包含一个非常有意思的带有变量和相应图表的 PCA 分解。
- en: 3.6 Singular value decomposition
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 奇异值分解
- en: PCA transforms the data linearly and generates principal components that are
    not correlated with each other. But the process followed in eigenvalue decomposition
    can only be applied to *square matrices*, whereas SVD can be implemented to any
    *m* × *n* matrix.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）线性变换数据并生成彼此不相关的主成分。但特征值分解过程中遵循的步骤只能应用于 *方阵*，而奇异值分解（SVD）可以应用于任何 *m*
    × *n* 矩阵。
- en: Say we have matrix *A*. The shape of *A* is *m* × *n*, or it contains *m* rows
    and *n* columns. The transpose of *A* can be represented as *A*^(*T*).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个矩阵 *A*。*A* 的形状是 *m* × *n*，或者它包含 *m* 行和 *n* 列。*A* 的转置可以表示为 *A*^(*T*)。
- en: 'We can create two other matrices using *A* and *A*^(*T*) as *A A*^(*T*)and
    *A*^(*T*)*A*. These resultant matrices *A A*^(*T*)and *A*^(*T*)*A* have some special
    properties, which are as follows (the mathematical proof of the properties is
    beyond the scope of the book):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 *A* 和 *A*^(*T*) 创建另外两个矩阵，即 *A A*^(*T*) 和 *A*^(*T*)*A*。这些结果矩阵 *A A*^(*T*)
    和 *A*^(*T*)*A* 具有一些特殊性质，如下（这些性质的数学证明超出了本书的范围）：
- en: They are symmetric and square matrices.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是对称的方阵。
- en: Their eigenvalues are either positive or zero.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的特征值要么是正的，要么是零。
- en: Both *A A*^(*T*)and *A*^(*T*)*A* have the same eigenvalue.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A A*^(*T*) 和 *A*^(*T*)*A* 与原始矩阵 A 有相同的特征值。'
- en: Both *A A*^(*T*)and *A*^(*T*)*A* have the same rank as the original matrix A.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A A*^(*T*) 和 *A*^(*T*)*A* 与原始矩阵 A 有相同的秩。'
- en: The eigenvectors of *A A*^(*T*)and *A*^(*T*)*A* are referred to as singular
    vectors of A. The square root of their eigenvalues is called singular values.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A A*^(*T*) 和 *A*^(*T*)*A* 的特征向量被称为 A 的奇异向量。它们的特征值的平方根被称为奇异值。
- en: Since both matrices (*A A*^(*T*)and *A*^(*T*)*A*) are symmetrical, their eigenvectors
    are orthonormal to each other. In other words, because they are symmetrical, the
    eigenvectors are perpendicular to each other and can be of unit length.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个矩阵（*A A*^(*T*) 和 *A*^(*T*)*A*）都是对称的，它们的特征向量彼此正交。换句话说，因为它们是对称的，所以特征向量相互垂直，并且可以是单位长度。
- en: 'Now, with this mathematical understanding, we can define SVD. As per the SVD
    method, it is possible to factorize any matrix A, as shown in equation 3.3:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了这种数学理解，我们可以定义奇异值分解（SVD）。根据奇异值分解方法，可以分解任何矩阵 A，如方程 3.3 所示：
- en: (3.3)
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （3.3）
- en: '*A* = *U* * *S* * *V*^(*T*)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *U* * *S* * *V*^(*T*)'
- en: Here, *A* is the original matrix, *U* and *V* are the orthogonal matrices with
    orthonormal eigenvectors taken from *A A*^(*T*)and *A*^(*T*)*A*, respectively,
    and *S* is the diagonal matrix with *r* elements equal to the singular values.
    In simple terms, SVD can be seen as an enhancement of the PCA methodology using
    eigenvalue decomposition.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*A* 是原始矩阵，*U* 和 *V* 分别是从 *A A*^(*T*) 和 *A*^(*T*)*A* 中取出的正交矩阵，具有正交特征向量，*S*
    是具有 *r* 个等于奇异值的对角矩阵。简单来说，奇异值分解可以看作是使用特征值分解增强的 PCA 方法。
- en: NOTE  Singular values are better and numerically more robust than eigenvalues
    decomposition.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：奇异值比特征值分解更好，并且在数值上更稳健。
- en: PCA was defined as the linear transformation of input variables using principal
    components. All those concepts of linear transformation, such as choosing the
    best components, etc., remain the same. The major process steps also remain similar,
    except in SVD we use a slightly different approach wherein the eigenvalue decomposition
    is replaced by singular vectors and singular values. It is often advisable to
    use SVD when we have a sparse dataset; in the case of a denser dataset, PCA can
    be utilized.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: PCA被定义为使用主成分对输入变量进行线性变换。所有这些线性变换的概念，如选择最佳组件等，都保持不变。主要过程步骤也保持相似，只是在SVD中，我们使用了一种稍微不同的方法，其中特征值分解被奇异向量和奇异值所取代。在稀疏数据集的情况下，通常建议使用SVD；在密集数据集的情况下，可以使用PCA。
- en: Exercise 3.4
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习3.4
- en: 'Answer these questions to check your understanding:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: SVD works on the eigenvalue decomposition technique. True or False?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVD基于特征值分解技术。对或错？
- en: PCA is a much more robust methodology than SVD. True or False?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA比SVD是一个更稳健的方法。对或错？
- en: What are singular values and singular vectors in SVD?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVD中的奇异值和奇异向量是什么？
- en: 3.6.1 Python solution using SVD
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 使用SVD的Python解决方案
- en: 'In this case study, we are using the *mushrooms* dataset. This dataset contains
    descriptions of 23 species of grilled mushrooms. There are two classes: either
    the mushroom is *e*, which means it is edible, or the mushroom is *p*, meaning
    it is poisonous. The steps are as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们使用的是*mushrooms*数据集。这个数据集包含了23种烤蘑菇的描述。有两种类别：要么蘑菇是*e*，这意味着它是可食用的，要么蘑菇是*p*，这意味着它是有毒的。步骤如下：
- en: 'Import the libraries:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '2\. Import the dataset and check for shape, head, etc. (see figure 3.18):'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 导入数据集并检查形状、头部等（见图3.18）：
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![figure](../Images/CH03_F18_Verdhan.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F18_Verdhan.png)'
- en: Figure 3.18 Code output
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.18 代码输出
- en: 3\. As we can observe, the values are categorical in nature in the dataset.
    They should be first encoded into numeric values. This is not the only approach
    for dealing with categorical variables. There are other techniques too, which
    we will explore throughout the book.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 如我们所观察到的，数据集中的值在本质上属于分类类型。它们首先应该被编码成数值。这不是处理分类变量的唯一方法。还有其他技术，我们将在整本书中探讨。
- en: 'First, invoke the `LabelEncoder` and then apply it to all the columns in the
    dataset. The `LabelEncoder` converts the categorical variables into numeric ones
    using the one-hot encoding method:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，调用`LabelEncoder`并将其应用于数据集中的所有列。`LabelEncoder`使用独热编码方法将分类变量转换为数值：
- en: '[PRE17]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '4\. Have another look at the dataset. All the categorical values have been
    converted to numeric ones (see figure 3.19):'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 再次查看数据集。所有的分类值都已转换为数值（见图3.19）：
- en: '[PRE18]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![figure](../Images/CH03_F19_Verdhan.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F19_Verdhan.png)'
- en: Figure 3.19 Code output
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19 代码输出
- en: '5\. The next two steps are the same as the last case study, wherein we break
    the dataset into `X_variables` and `y_label`. Then the dataset is normalized:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 下两个步骤与上一个案例研究相同，其中我们将数据集分为`X_variables`和`y_label`。然后对数据集进行归一化：
- en: '[PRE19]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '6\. Implement the SVD. There is a method in `numpy` that implements SVD. The
    output is `u`, `s`, and `v`, where `u` and `v` are the singular vectors and `s`
    is the singular value. If you wish, you can analyze their respective shapes and
    dimensions:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 实现SVD。`numpy`中有一个实现SVD的方法。输出是`u`、`s`和`v`，其中`u`和`v`是奇异向量，`s`是奇异值。如果您愿意，可以分析它们的形状和维度：
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '7\. We know that singular values allow us to compute variance explained by
    each of the singular vectors. We will now analyze the percentage variance explained
    by each singular vector and plot it (see figure 3.20). The results are shown to
    three decimal places. Then we plot the results as a histogram plot. On the x-axis,
    we have the singular vectors while on the y-axis we have the percent of variance
    explained:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 我们知道奇异值使我们能够计算每个奇异向量解释的方差。现在我们将分析每个奇异向量解释的百分比方差，并将其绘制出来（见图3.20）。结果保留到小数点后三位。然后我们将结果绘制成直方图。在x轴上，我们有奇异向量，而在y轴上，我们有解释的百分比方差：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![figure](../Images/CH03_F20_Verdhan.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F20_Verdhan.png)'
- en: Figure 3.20 Code output
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.20 代码输出
- en: '8\. Create a dataframe (see figure 3.21). This new dataframe svd_df contains
    the first two singular vectors and the metadata. We then print the first five
    rows using the head command:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 创建一个数据框（见图3.21）。这个新的数据框svd_df包含前两个奇异向量和元数据。然后我们使用head命令打印前五行：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![figure](../Images/CH03_F21_Verdhan.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F21_Verdhan.png)'
- en: Figure 3.21 Dataframe containing the first two singular vectors and the metadata
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.21 包含前两个奇异向量和元数据的DataFrame
- en: '9\. Like the last case study, we replace numeric values with actual class labels;
    `1` is edible while `0` is poisonous:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 9. 与上一个案例研究类似，我们将数值替换为实际的类别标签；`1` 表示可食用，而 `0` 表示有毒：
- en: '[PRE23]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '10\. We now plot the variance explained by the two components (see figure 3.22).
    Here, we have chosen only the first two components. You are advised to take the
    optimum number of components using the methods described in the last section and
    plot the respective scatter plots. Here, on the x-axis, we have shown the first
    singular vector SV1, and on the y-axis we have shown the second singular vector
    SV2\. The print version of the book does not show the different colors, but the
    output of the Python code does. The same output is available at the GitHub repository
    too:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10. 我们现在绘制了两个成分解释的方差（见图3.22）。在这里，我们只选择了前两个成分。建议您使用上一节中描述的方法选择最佳成分数量，并绘制相应的散点图。在这里，x轴上显示了第一个奇异向量SV1，y轴上显示了第二个奇异向量SV2。印刷版的书没有显示不同的颜色，但Python代码的输出显示了。相同的输出也可以在GitHub仓库中找到：
- en: '[PRE24]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![figure](../Images/CH03_F22_Verdhan.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F22_Verdhan.png)'
- en: Figure 3.22 Plot of the variance explained by two components
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.22 两个成分解释的方差图
- en: We can observe the distribution of the two classes with respect to the two components.
    The two classes—`Edible` and `Poison`—are color-coded as black and red, respectively.
    As we have noted previously, we have chosen only two components to show the effect
    using a visualization plot. You should choose the optimum number of components
    using the methods described in the last case study and then visualize the results
    using different singular vectors. This solution can be used to reduce dimensions
    in a real-world dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到两个类别相对于两个成分的分布。这两个类别——`可食用`和`有毒`——分别用黑色和红色编码。正如我们之前所提到的，我们只选择了两个成分来使用可视化图来展示效果。您应该使用上一案例研究中描述的方法选择最佳成分数量，然后使用不同的奇异向量可视化结果。这种解决方案可以用于在现实世界数据集中降低维度。
- en: 3.7 Pros and cons of dimensionality reduction
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 维度降低的优缺点
- en: In the initial sections of the chapter, we discussed the drawbacks of the curse
    of dimensionality. In the last few sections, we discovered PCA and SVD and implemented
    them using Python. Now we will examine the advantages and challenges of these
    techniques. The major advantages of implementing PCA or SVD are
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的初始部分，我们讨论了维度灾难的缺点。在最后几节中，我们发现了PCA和SVD，并使用Python实现了它们。现在我们将检查这些技术的优点和挑战。实现PCA或SVD的主要优点是
- en: A reduced number of dimensions leads to less complexity in the dataset. The
    correlated features are removed and transformed. Treating correlated variables
    manually is a tough task, which is quite manual and frustrating. Techniques like
    PCA and SVD do that job for us quite easily. The number of correlated features
    is minimized, and overall dimensions are reduced.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少的维度数量导致数据集的复杂性降低。相关的特征被移除并转换。手动处理相关变量是一项艰巨的任务，相当繁琐且令人沮丧。PCA和SVD等技术为我们轻松完成这项工作。相关特征的数量最小化，总体维度减少。
- en: Visualization of the datasetis better if the number of dimensions is fewer.
    It is very difficult to visualize and depict a very high-dimensional dataset.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果维度较少，数据集的可视化更好。非常难以可视化和描述一个高维数据集。
- en: The accuracyof the machine learning model is improved if the correlated variables
    are removed. These variables do not add anything to the performance of the model.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果移除了相关变量，机器学习模型的准确性会提高。这些变量不会对模型的性能增加任何东西。
- en: The training time is reduced as the dataset is less complex. Hence, less computation
    power and time are required.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集较为简单，训练时间减少。因此，所需的计算能力和时间也较少。
- en: Overfittingis a nuisance in supervised machine learning models. It is a condition
    where the model behaves very well on the training dataset but not so well on the
    testing/validation dataset. It means that the model may not be able to perform
    well on real-world unseen datasets. And it defeats the entire purpose of building
    the machine learning model. PCA/SVD helps tackle overfitting by reducing the number
    of variables.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度拟合是监督机器学习模型中的一个麻烦。这是一种条件，即模型在训练数据集上表现良好，但在测试/验证数据集上表现不佳。这意味着模型可能无法在现实世界中的未见数据集上表现良好。这违背了构建机器学习模型的整个目的。PCA/SVD通过减少变量的数量来帮助解决过度拟合问题。
- en: 'At the same time, there are a few challenges we face with dimensionality reduction
    techniques, which are as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们在降维技术中面临一些挑战，如下所述：
- en: The new components created by PCA/SVD are often less interpretable. They are
    a combination of the independent variables in the dataset and do not actually
    relate to the real world; hence it can be difficult to relate them to real-world
    scenarios.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA/SVD创建的新成分通常难以解释。它们是数据集中独立变量的组合，并不真正与现实世界相关；因此，将它们与现实世界场景联系起来可能很困难。
- en: Numeric variables are required for PCA/SVD. Hence all the categorical variables
    should be represented in numeric form.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA/SVD需要数值变量。因此，所有分类变量都应该以数值形式表示。
- en: Normalization/standardization of the dataset is required before the solution
    can be implemented.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实施解决方案之前，需要对数据集进行归一化/标准化。
- en: There might be information losswhen we use PCA or SVD. The principal components
    *cannot* replace the original dataset, and hence there might be some loss of information
    when we implement these methods.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们使用PCA或SVD时，可能会出现信息丢失。主成分**不能**替代原始数据集，因此在我们实施这些方法时可能会丢失一些信息。
- en: However, despite a few challenges, PCA and SVD are used for reducing dimensions
    in a dataset. They are two of the most popular methods and are quite heavily used.
    Note that these are linear methods; we cover nonlinear methods of dimensionality
    reduction in a later part of the book.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管有一些挑战，PCA和SVD仍然被用于数据集的降维。它们是最受欢迎的方法之一，并且被广泛使用。请注意，这些是线性方法；我们在本书的后面部分介绍了降维的非线性方法。
- en: We have now covered the two most important techniques used in dimensionality
    reduction. We will examine more advanced techniques in the later chapters. It
    is time to move on to the case study.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经介绍了在降维中使用的两种最重要的技术。在后面的章节中，我们将探讨更高级的技术。现在是时候转向案例研究了。
- en: 3.8 Case study for dimension reduction
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 降维案例研究
- en: 'Let’s explore a real-world case to relate the use of PCA and SVD in real-world
    business scenarios. Consider this: you are working for a telecommunication service
    provider. You have a subscriber base, and you wish to cluster the consumers over
    several parameters. The challenge is the huge number of dimensions available to
    be analyzed.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一个现实世界的案例，以了解PCA和SVD在现实商业场景中的应用。考虑以下情况：你为一家电信服务提供商工作。你有一个订阅者基础，你希望根据多个参数对消费者进行聚类。挑战在于可供分析的大量维度。
- en: 'The objective will be to reduce the number of attributes using dimension reduction
    algorithms. The consumer dataset might include the following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 目标将是使用降维算法减少属性数量。消费者数据集可能包括以下内容：
- en: Demographic details of the subscriber, which will consist of age, gender, occupation,
    household size, marital status, etc. (see figure 3.23).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅者的人口统计详情，包括年龄、性别、职业、家庭规模、婚姻状况等（见图3.23）。
- en: '![figure](../Images/CH03_F23_Verdhan.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F23_Verdhan.png)'
- en: Figure 3.23 Demographic details of a subscriber like age, gender, marital status,
    household size, city, etc.
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.23 订阅者的人口统计详情，如年龄、性别、婚姻状况、家庭规模、城市等。
- en: Subscription details of the consumer, which might look like figure 3.24.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者的订阅详情，可能看起来像图3.24。
- en: '![figure](../Images/CH03_F24_Verdhan.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F24_Verdhan.png)'
- en: Figure 3.24 Subscription details of a subscriber like tenure, postpaid/prepaid
    connection, etc.
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.24 订阅详情，如服务期限、后付费/预付费连接等。
- en: Consumer usage, such as the minutes, call rates, data usages, services, etc.
    (see figure 3.25).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者使用情况，如通话分钟数、通话费率、数据使用、服务等（见图3.25）。
- en: '![figure](../Images/CH03_F25_Verdhan.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F25_Verdhan.png)'
- en: Figure 3.25 Usage of a subscriber specifies the number of minutes used, SMS
    sent, data used, days spent in a network, national or international usage, etc.
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.25 订阅者使用情况指定了使用的分钟数、发送的短信、使用的数据、在网络中花费的天数、国内或国际使用等。
- en: Payment and transaction details of the subscribers, which could be the various
    transactions made, the mode of payment, frequency of payments, days since last
    payment made, etc. (see figure 3.26).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅者的支付和交易详情，可能包括各种交易、支付方式、支付频率、上次支付以来的天数等（见图3.26）。
- en: '![figure](../Images/CH03_F26_Verdhan.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F26_Verdhan.png)'
- en: Figure 3.26 Transaction details of a subscriber showing all the details of amount,
    mode, etc.
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.26 展示了订阅者交易详情，包括金额、方式等所有详细信息。
- en: Many more attributes. So far, we have established that the number of variables
    involved are indeed high. Once we join all these data points, the number of dimensions
    in the final data can be huge (see figure 3.27).
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多更多的属性。到目前为止，我们已经确定涉及的变量数量确实很高。一旦我们将所有这些数据点合并，最终数据中的维度数量可以非常大（见图3.27）。
- en: '![figure](../Images/CH03_F27_Verdhan.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F27_Verdhan.png)'
- en: Figure 3.27 The final dataset is a combination of all the aforementioned datasets.
    It will be a big, really high-dimensional dataset to be analyzed.
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.27 最终数据集是上述所有数据集的组合。它将是一个大型的、真正高维的数据集，需要进行分析。
- en: We should reduce the number of attributes before we proceed to any supervised
    or unsupervised solution. In this chapter, we focus on dimensionality reduction
    techniques, and hence the steps cover that aspect of the process. In later chapters,
    we will examine exploratory analysis in more detail.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行任何监督或无监督解决方案之前，我们应该减少属性的数量。在本章中，我们专注于降维技术，因此步骤涵盖了该过程的这一方面。在后面的章节中，我们将更详细地检查探索性分析。
- en: As a first step, we will perform a sanity check of the dataset and do the data
    cleaning. We will examine the number of data points, number of missing values,
    duplicates, junk values present, etc. This will allow us to delete any variables
    that might be very sparse or contain not much information. For example, if the
    gender is available for only 0.01% of the customer base, it might be a good idea
    to drop the variable. Or if all the customers state their gender is male, the
    variable is not adding any new information to us, and hence it can be discarded.
    Sometimes, using business logic, a variable might be dropped from the dataset.
    An example has been discussed in section 3.4\. In this step, we might combine
    a few variables. For example, we might create a new variable as average transaction
    value by dividing the total amount spent by the total number of transactions.
    In this way, we will be able to reduce a few dimensions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将对数据集进行合理性检查并进行数据清洗。我们将检查数据点的数量、缺失值的数量、重复项、垃圾值等。这将使我们能够删除任何可能非常稀疏或包含信息不多的变量。例如，如果性别只对客户基础的0.01%可用，那么删除该变量可能是个好主意。或者如果所有客户都声称他们的性别是男性，该变量没有为我们提供任何新信息，因此可以被丢弃。有时，使用业务逻辑，可能会从数据集中删除变量。一个例子在第3.4节中已讨论。在这一步中，我们可能会合并几个变量。例如，我们可能会创建一个新的变量，作为平均交易价值，通过将总支出除以总交易次数来计算。这样，我们将能够减少一些维度。
- en: NOTE  A Python Jupyter notebook is available in the GitHub repository, where
    we have given a very detailed solution for the data cleaning step.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：GitHub仓库中有一个Python Jupyter笔记本，其中我们提供了数据清洗步骤的非常详细的解决方案。
- en: Once we are done with the basic cleaning of the data, we start with the exploratory
    data analysis. As a part of exploratory analysis, we examine the spread of the
    variable, its distribution, mean/median/mode of numeric variables, and so on.
    This is sometimes referred to as *univariate analysis.* This step allows us to
    measure the spread of the variables, understand the central tendencies, examine
    the distribution of different classes for categorical variables, and look for
    any anomalies in the values. For example, using the dataset mentioned earlier,
    we will be interested in analyzing the maximum/minimum/average data usage or the
    percentage distribution of gender or age. We would want to know the most popular
    method to make a transaction, and we would also be interested to know the maximum/minimum/average
    amount of the transactions. The list goes on.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成数据的基本清洗，我们就开始进行探索性数据分析。作为探索性分析的一部分，我们检查变量的分布、其分布情况、数值变量的均值/中位数/众数等。这有时被称为*单变量分析*。这一步骤使我们能够测量变量的分布范围，了解中心趋势，检查分类变量的不同类别的分布，并寻找任何值中的异常。例如，使用前面提到的数据集，我们将对分析最大/最小/平均数据使用量或性别或年龄的百分比分布感兴趣。我们想知道进行交易的最受欢迎的方法，我们也想了解交易的最大/最小/平均金额。列表还在继续。
- en: Then we explore the relationships between variables, which is referred to as
    *bivariate analysis*. Crosstabs, or distribution of data, is a part of bivariate
    analysis. A correlation matrix is created during this step. Variables that are
    highly correlated are examined thoroughly. And based on business logic, one of
    them might be dropped. This step is useful to visualize and understand the behavior
    of one variable in the presence of other variables. We can examine their mutual
    relationships and the respective strength of the relationships. In this case study,
    we would answer questions such as, “Do subscribers who use more data spend more
    time on the network as compared to subscribers who send more SMS?”, “Do the subscribers
    who make a transaction using the online mode generate more revenue than the ones
    using cash?”, or “Is there a relationship between gender/age and the data usage?”
    Many such questions are answered during this phase of the project.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探索变量之间的关系，这被称为*双变量分析*。交叉表，或数据的分布，是双变量分析的一部分。在这一步中会创建一个相关矩阵。高度相关的变量将被彻底检查。根据业务逻辑，其中一个变量可能会被删除。这一步有助于可视化并理解一个变量在存在其他变量时的行为。我们可以检查它们之间的相互关系以及关系的强度。在本案例研究中，我们将回答诸如，“与发送更多短信的用户相比，使用更多数据的用户在网络上花费的时间更多吗？”，“使用在线模式进行交易的用户比使用现金的用户产生更多的收入吗？”，或者“性别/年龄与数据使用之间是否存在关系？”等问题。在项目这一阶段，我们将回答许多此类问题。
- en: NOTE  A Python Jupyter notebook is available in the GitHub repository, which
    provides detailed steps and code for the univariate and bivariate phases. Check
    it out!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：GitHub仓库中有一个Python Jupyter笔记本，其中提供了单变量和双变量阶段的详细步骤和代码。查看它！
- en: At this stage, we have a dataset that has a huge number of dimensions, and we
    want to reduce the number of dimensions. Now is a good time to implement PCA or
    SVD. The techniques will reduce the number of dimensions and will make the dataset
    ready for the next steps in the process, as shown in figure 3.28\. The figure
    is only representative in nature to depict the effect of dimensionality reduction
    methods. Notice how the large number of black lines in the left figure is reduced
    to a smaller number of red lines in the right figure.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们有一个具有大量维度的数据集，我们希望减少维度的数量。现在是实施PCA或SVD的好时机。这些技术将减少维度的数量，并将数据集准备好用于流程的下一步，如图3.28所示。该图仅具有代表性，以描述降维方法的效果。注意，左图中的大量黑色线条在右图中减少到了更少的红色线条。
- en: '![figure](../Images/CH03_F28_Verdhan.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F28_Verdhan.png)'
- en: Figure 3.28 A very high-dimensional dataset will be reduced to a low-dimensional
    one by using principal components that capture the maximum variance in the dataset.
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.28 通过使用捕捉数据集中最大方差的主成分，一个非常高维的数据集将被降低到低维。
- en: The output of dimensionality reduction methods will be a dataset with a lower
    number of variables. The dataset can be then used for supervised or unsupervised
    learning. We have already looked at the examples using Python in the earlier sections
    of the chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低方法的结果将是一个变量数量较少的数据集。该数据集可以用于监督学习或无监督学习。我们已经在章节的前几节中查看了一些使用Python的示例。
- en: This concludes our case study on telecom subscribers. The case can be extended
    to any other domain like retail; banking, financial services, and insurance; aviation;
    healthcare; manufacturing; and others.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对电信用户的案例研究的结束。该案例可以扩展到任何其他领域，如零售；银行、金融服务和保险；航空；医疗保健；制造业；以及其他领域。
- en: 3.9 Concluding thoughts
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.9 结论性思考
- en: Data is everywhere in various forms, levels, and dimensions and with varying
    levels of complexity. It is often mentioned that “the more data, the better.”
    It is indeed true to a certain extent. But with a really high number of dimensions,
    it becomes quite a herculean task to make sense of it. The analysis can become
    biased and very complex to deal with. We explored this curse of dimensionality
    in this chapter. We found PCA and SVD can be helpful to reduce this complexity.
    They make the dataset ready for the next steps.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在，以各种形式、水平和维度存在，并且具有不同的复杂程度。经常提到“数据越多，越好。”这在一定程度上确实是正确的。但是，当维度数量非常高时，理解它就变得非常困难。分析可能会变得有偏见，并且处理起来非常复杂。我们在本章中探讨了维度的诅咒。我们发现主成分分析（PCA）和奇异值分解（SVD）可以帮助减少这种复杂性。它们使数据集为下一步做好准备。
- en: Dimensionality reduction is not as straightforward as it looks. It is not an
    easy task, but it is certainly a very rewarding one. And it requires a combination
    of business acumen, logic, and common sense. The resultant dataset might still
    require some additional work. But it is a very good point for building a machine
    learning model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 降维并不像看起来那么简单。这不是一项容易的任务，但绝对是一项非常有价值的任务。它需要商业洞察力、逻辑和常识的结合。结果数据集可能还需要一些额外的工作。但它是构建机器学习模型的一个非常好的起点。
- en: This marks the end of the third chapter. It also ends the part 1 of the book.
    In this part, we have covered a few core algorithms. We started with the first
    chapter of the book, where we explored the fundamentals and basics of machine
    learning. In the second chapter, we examined three algorithms for clustering.
    In this third chapter, we explored PCA and SVD.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着第三章节的结束，也标志着本书第一部分的结束。在本部分，我们介绍了一些核心算法。我们从本书的第一章开始，探讨了机器学习的根本和基础。在第二章中，我们检查了三种聚类算法。在这一章中，我们探讨了PCA和SVD。
- en: In the second part of the book, we change gears and study more advanced topics.
    We start with association rules in the next chapter. Then we go into advanced
    clustering methods of time-series clustering, fuzzy clustering, Gaussian mixture
    mode clustering, etc. That is followed by a chapter on advanced dimensionality
    reduction algorithms like t-SNE and LDA. To conclude the second part, we examine
    unsupervised learning on text datasets. The third part of the book is even more
    advanced, so still a long way to go. Stay tuned!
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分，我们转换方向，研究更高级的主题。下一章我们将从关联规则开始。然后我们将探讨时间序列聚类、模糊聚类、高斯混合模型聚类等高级聚类方法。接下来是关于t-SNE和LDA等高级降维算法的章节。为了结束第二部分，我们检查文本数据集上的无监督学习。本书的第三部分更加高级，所以还有很长的路要走。请保持关注！
- en: 3.10 Practical next steps and suggested readings
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.10 实际下一步行动和建议阅读材料
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供下一步行动的建议，并提供一些有用的阅读材料：
- en: Use the vehicles dataset used in the last chapter for clustering and implement
    PCA and SVD on it. Compare the performance on clustering before and after implementing
    PCA and SVD.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上一章中使用的车辆数据集进行聚类，并在其上实现PCA和SVD。比较实现PCA和SVD前后聚类的性能。
- en: Get the datasets from [https://mng.bz/2y9g](https://mng.bz/2y9g). You can find
    many datasets. Compare the performance of PCA and SVD on these datasets.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://mng.bz/2y9g](https://mng.bz/2y9g)获取数据集。你可以找到许多数据集。比较这些数据集上PCA和SVD的性能。
- en: 'Go through the following papers on PCA:'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读以下关于PCA的论文：
- en: '[https://mng.bz/1XKX](https://mng.bz/1XKX)'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/1XKX](https://mng.bz/1XKX)'
- en: '[https://mng.bz/Pd0w](https://mng.bz/Pd0w)'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/Pd0w](https://mng.bz/Pd0w)'
- en: '[https://mng.bz/JYeo](https://mng.bz/JYeo)'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/JYeo](https://mng.bz/JYeo)'
- en: '[https://mng.bz/wJqO](https://mng.bz/wJqO)'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/wJqO](https://mng.bz/wJqO)'
- en: 'Go through the following research papers on SVD:'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读以下关于SVD的研究论文：
- en: '[https://mng.bz/qxqA](https://mng.bz/qxqA)'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/qxqA](https://mng.bz/qxqA)'
- en: '[https://mng.bz/7pNm](https://mng.bz/7pNm)'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mng.bz/7pNm](https://mng.bz/7pNm)'
- en: '[https://arxiv.org/pdf/1211.7102.pdf](https://arxiv.org/pdf/1211.7102.pdf)'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1211.7102.pdf](https://arxiv.org/pdf/1211.7102.pdf)'
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The “curse of dimensionality” refers to problems arising from high-dimensional
    datasets with too many variables, complicating the analysis and model performance.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “维度诅咒”指的是由具有太多变量的高维数据集引起的问题，这些问题使分析和模型性能复杂化。
- en: High dimensions can lead to a sparse dataset, increased mathematical complexity,
    longer processing times, and potential overfitting in machine learning models.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维可能导致稀疏数据集、增加数学复杂性、更长的处理时间，以及机器学习模型中潜在的过拟合。
- en: Hughes phenomenon shows that increasing variables only improves model performance
    up to a point, after which it declines.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍奇斯现象表明，增加变量只能提高模型性能到一定程度，之后性能会下降。
- en: Not all dimensions are significant; some may not contribute meaningfully to
    a model’s accuracy and should be removed to reduce complexity.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有维度都是重要的；一些可能不会对模型精度做出有意义的贡献，应该被删除以减少复杂性。
- en: Data visualization can help explain datasets by reducing them to fewer dimensions
    that still capture significant information.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化可以通过将数据集减少到仍能捕捉到重要信息的较少维度来帮助解释数据集。
- en: Manual dimension reduction includes dropping insignificant variables or combining
    them logically to reduce dataset dimensions.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动降维包括删除不显著的变量或将它们逻辑组合以减少数据集维度。
- en: Algorithm-based methods for dimension reduction include PCA, SVD, LDA, and t-SNE,
    among others, which transform high-dimensional data into low-dimensional spaces.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于算法的降维方法包括PCA、SVD、LDA和t-SNE等，它们将高维数据转换到低维空间。
- en: PCA reduces dimensions by creating principal components that capture maximum
    variance while minimizing redundancy and noise.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA通过创建能够捕捉最大方差的同时最小化冗余和噪声的主成分来降低维度。
- en: SVD enhances PCA, handling any matrix shape and decomposing them into singular
    values and vectors to maintain dataset information.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD增强了PCA，可以处理任何矩阵形状，并将它们分解为奇异值和向量，以保持数据集信息。
- en: Each reduction technique requires the normalization of data and converting categorical
    variables to numeric forms.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种降维技术都需要对数据进行归一化处理，并将分类变量转换为数值形式。
- en: Dimensionality reduction simplifies datasets, enhancing visualization and model
    accuracy, reducing computation time, and mitigating overfitting risks.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维简化了数据集，提高了可视化效果和模型精度，减少了计算时间，并减轻了过拟合风险。
- en: Challenges with dimensionality reduction include the loss of interpretability,
    information loss, and the requirement for numerical data.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维的挑战包括可解释性的丧失、信息损失以及对数值数据的要求。
- en: Both PCA and SVD are widely used to effectively reduce dimensions, and each
    is suitable for different dataset densities.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和奇异值分解（SVD）被广泛用于有效地降低维度，并且每种方法都适用于不同数据集的密度。
- en: The techniques can be applied in various industries like retail; banking, financial
    services, and insurance; and healthcare to simplify high-dimensional datasets
    for analysis.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些技术可以应用于各种行业，如零售业；银行业、金融服务和保险业；以及医疗保健业，以简化用于分析的高维数据集。
- en: The reduction process involves preliminary data cleaning and exploratory data
    analysis and then applying dimension-reduction techniques.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维过程包括初步数据清洗和探索性数据分析，然后应用降维技术。
