- en: 4 Implementing a GPT model from scratch to generate text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 ä»é›¶å¼€å§‹å®ç°GPTæ¨¡å‹ä»¥ç”Ÿæˆæ–‡æœ¬
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–äº†
- en: Coding a GPT-like large language model (LLM) that can be trained to generate
    human-like text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼–ç ä¸€ä¸ªå¯ä»¥è®­ç»ƒç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„GPT-likeå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
- en: Normalizing layer activations to stabilize neural network training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å±‚æ¿€æ´»å€¼å½’ä¸€åŒ–ä»¥ç¨³å®šç¥ç»ç½‘ç»œè®­ç»ƒ
- en: Adding shortcut connections in deep neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ·»åŠ å¿«æ·è¿æ¥
- en: Implementing transformer blocks to create GPT models of various sizes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°transformerå—ä»¥åˆ›å»ºå„ç§å¤§å°çš„GPTæ¨¡å‹
- en: Computing the number of parameters and storage requirements of GPT models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—GPTæ¨¡å‹çš„å‚æ•°æ•°é‡å’Œå­˜å‚¨éœ€æ±‚
- en: Youâ€™ve already learned and coded the *multi-head attention* mechanism, one of
    the core components of LLMs. Now, we will code the other building blocks of an
    LLM and assemble them into a GPT-like model that we will train in the next chapter
    to generate human-like text.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»å­¦ä¹ å’Œç¼–ç äº†LLMçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€**å¤šå¤´æ³¨æ„åŠ›**æœºåˆ¶ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ç¼–ç LLMçš„å…¶ä»–æ„å»ºå—ï¼Œå¹¶å°†å®ƒä»¬ç»„è£…æˆä¸€ä¸ªGPT-likeæ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­è®­ç»ƒå®ƒä»¥ç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚
- en: '![figure](../Images/4-1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-1.png)'
- en: 'Figure 4.1 The three main stages of coding an LLM. This chapter focuses on
    step 3 of stage 1: implementing the LLM architecture.'
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.1 ç¼–ç LLMçš„ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æœ¬ç« é‡ç‚¹ä»‹ç»ç¬¬ä¸€é˜¶æ®µæ­¥éª¤3ï¼šå®ç°LLMæ¶æ„ã€‚
- en: The LLM architecture referenced in figure 4.1, consists of several building
    blocks. We will begin with a top-down view of the model architecture before covering
    the individual components in more detail.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.1ä¸­å¼•ç”¨çš„LLMæ¶æ„ç”±å‡ ä¸ªæ„å»ºå—ç»„æˆã€‚æˆ‘ä»¬å°†ä»æ¨¡å‹æ¶æ„çš„é¡¶å‘ä¸‹è§†å›¾å¼€å§‹ï¼Œç„¶åå†æ›´è¯¦ç»†åœ°ä»‹ç»å„ä¸ªç»„ä»¶ã€‚
- en: 4.1 Coding an LLM architecture
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 ç¼–ç LLMæ¶æ„
- en: LLMs, such as GPT (which stands for *generative pretrained transformer*), are
    large deep neural network architectures designed to generate new text one word
    (or token) at a time. However, despite their size, the model architecture is less
    complicated than you might think, since many of its components are repeated, as
    we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with
    its main components highlighted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsï¼Œå¦‚GPTï¼ˆä»£è¡¨**ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨**ï¼‰ï¼Œæ˜¯è®¾è®¡ç”¨æ¥ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå•è¯ï¼ˆæˆ–æ ‡è®°ï¼‰çš„æ–°æ–‡æœ¬çš„å¤§å‹æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬çš„è§„æ¨¡å¾ˆå¤§ï¼Œä½†æ¨¡å‹æ¶æ„å¹¶ä¸åƒä½ å¯èƒ½æƒ³è±¡çš„é‚£æ ·å¤æ‚ï¼Œå› ä¸ºå…¶ä¸­è®¸å¤šç»„ä»¶æ˜¯é‡å¤çš„ï¼Œæ­£å¦‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°çš„ã€‚å›¾4.2æä¾›äº†ä¸€ä¸ªç±»ä¼¼GPTçš„LLMçš„é¡¶å‘ä¸‹è§†å›¾ï¼Œå…¶ä¸»è¦ç»„ä»¶è¢«çªå‡ºæ˜¾ç¤ºã€‚
- en: '![figure](../Images/4-2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-2.png)'
- en: Figure 4.2 A GPT model. In addition to the embedding layers, it consists of
    one or more transformer blocks containing the masked multi-head attention module
    we previously implemented.
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.2 GPTæ¨¡å‹ã€‚é™¤äº†åµŒå…¥å±‚å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªæˆ–å¤šä¸ªåŒ…å«æˆ‘ä»¬ä¹‹å‰å®ç°çš„æ©ç å¤šå¤´æ³¨æ„åŠ›æ¨¡å—çš„transformerå—ã€‚
- en: We have already covered several aspects of the LLM architecture, such as input
    tokenization and embedding and the masked multi-head attention module. Now, we
    will implement the core structure of the GPT model, including its *transformer
    blocks*, which we will later train to generate human-like text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ¶µç›–äº†LLMæ¶æ„çš„å‡ ä¸ªæ–¹é¢ï¼Œä¾‹å¦‚è¾“å…¥åˆ†è¯å’ŒåµŒå…¥ä»¥åŠä¹‹å‰å®ç°çš„æ©ç å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†å®ç°GPTæ¨¡å‹çš„æ ¸å¿ƒç»“æ„ï¼ŒåŒ…æ‹¬å…¶**transformerå—**ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥åè®­ç»ƒå®ƒä»¬ä»¥ç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚
- en: Previously, we used smaller embedding dimensions for simplicity, ensuring that
    the concepts and examples could comfortably fit on a single page. Now, we are
    scaling up to the size of a small GPT-2 model, specifically the smallest version
    with 124 million parameters, as described in â€œLanguage Models Are Unsupervised
    Multitask Learners,â€ by Radford et al. ([https://mng.bz/yoBq](https://mng.bz/yoBq)).
    Note that while the original report mentions 117 million parameters, this was
    later corrected. In chapter 6, we will focus on loading pretrained weights into
    our implementation and adapting it for larger GPT-2 models with 345, 762, and
    1,542 million parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è¾ƒå°çš„åµŒå…¥ç»´åº¦ï¼Œç¡®ä¿æ¦‚å¿µå’Œç¤ºä¾‹å¯ä»¥èˆ’é€‚åœ°æ”¾åœ¨ä¸€é¡µä¸Šã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æ­£åœ¨å°†å…¶æ‰©å±•åˆ°å°å‹GPT-2æ¨¡å‹çš„å¤§å°ï¼Œå…·ä½“æ˜¯æœ€å°çš„124ç™¾ä¸‡å‚æ•°ç‰ˆæœ¬ï¼Œå¦‚Radfordç­‰äººæ‰€æè¿°çš„â€œè¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ è€…â€ï¼ˆ[https://mng.bz/yoBq](https://mng.bz/yoBq)ï¼‰ã€‚è¯·æ³¨æ„ï¼Œå°½ç®¡åŸå§‹æŠ¥å‘Šæåˆ°117ç™¾ä¸‡å‚æ•°ï¼Œä½†è¿™åæ¥è¢«æ›´æ­£äº†ã€‚åœ¨ç¬¬6ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå°†é¢„è®­ç»ƒçš„æƒé‡åŠ è½½åˆ°æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œå¹¶é€‚åº”å…·æœ‰345äº¿ã€762äº¿å’Œ15.42äº¿å‚æ•°çš„æ›´å¤§GPT-2æ¨¡å‹ã€‚
- en: In the context of deep learning and LLMs like GPT, the term â€œparametersâ€ refers
    to the trainable weights of the model. These weights are essentially the internal
    variables of the model that are adjusted and optimized during the training process
    to minimize a specific loss function. This optimization allows the model to learn
    from the training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ å’Œ GPT ç­‰LLMçš„èƒŒæ™¯ä¸‹ï¼Œâ€œå‚æ•°â€ä¸€è¯æŒ‡çš„æ˜¯æ¨¡å‹çš„å¯è®­ç»ƒæƒé‡ã€‚è¿™äº›æƒé‡å®é™…ä¸Šæ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å’Œä¼˜åŒ–çš„å†…éƒ¨å˜é‡ï¼Œä»¥æœ€å°åŒ–ç‰¹å®šçš„æŸå¤±å‡½æ•°ã€‚è¿™ç§ä¼˜åŒ–ä½¿æ¨¡å‹èƒ½å¤Ÿä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ã€‚
- en: For example, in a neural network layer that is represented by a 2,048 Ã— 2,048â€“dimensional
    matrix (or tensor) of weights, each element of this matrix is a parameter. Since
    there are 2,048 rows and 2,048 columns, the total number of parameters in this
    layer is 2,048 multiplied by 2,048, which equals 4,194,304 parameters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç”± 2,048 Ã— 2,048 ç»´åº¦çš„æƒé‡çŸ©é˜µï¼ˆæˆ–å¼ é‡ï¼‰è¡¨ç¤ºçš„ç¥ç»ç½‘ç»œå±‚ä¸­ï¼Œè¿™ä¸ªçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå‚æ•°ã€‚ç”±äºæœ‰ 2,048 è¡Œå’Œ 2,048
    åˆ—ï¼Œè¿™ä¸ªå±‚ä¸­çš„å‚æ•°æ€»æ•°æ˜¯ 2,048 ä¹˜ä»¥ 2,048ï¼Œç­‰äº 4,194,304 ä¸ªå‚æ•°ã€‚
- en: GPT-2 vs. GPT-3
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-2 ä¸ GPT-3
- en: Note that we are focusing on GPT-2 because OpenAI has made the weights of the
    pretrained model publicly available, which we will load into our implementation
    in chapter 6\. GPT-3 is fundamentally the same in terms of model architecture,
    except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion
    parameters in GPT-3, and it is trained on more data. As of this writing, the weights
    for GPT-3 are not publicly available. GPT-2 is also a better choice for learning
    how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3
    requires a GPU cluster for training and inference. According to Lambda Labs ([https://lambdalabs.com/](https://lambdalabs.com/)),
    it would take 355 years to train GPT-3 on a single V100 datacenter GPU and 665
    years on a consumer RTX 8000 GPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬ä¸“æ³¨äº GPT-2ï¼Œå› ä¸º OpenAI å·²ç»å°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡å…¬å¼€ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬6ç« å°†å…¶åŠ è½½åˆ°æˆ‘ä»¬çš„å®ç°ä¸­ã€‚GPT-3 åœ¨æ¨¡å‹æ¶æ„æ–¹é¢åŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„ï¼Œä½†å®ƒä»
    GPT-2 çš„ 15 äº¿å‚æ•°æ‰©å±•åˆ° GPT-3 çš„ 1750 äº¿å‚æ•°ï¼Œå¹¶ä¸”è®­ç»ƒåœ¨æ›´å¤šæ•°æ®ä¸Šã€‚æˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼ŒGPT-3 çš„æƒé‡å°šæœªå…¬å¼€ã€‚GPT-2 ä¹Ÿæ˜¯å­¦ä¹ å¦‚ä½•å®ç°LLMçš„æ›´å¥½é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨å•ä¸ªç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œï¼Œè€Œ
    GPT-3 éœ€è¦GPUé›†ç¾¤è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚æ ¹æ® Lambda Labs ([https://lambdalabs.com/](https://lambdalabs.com/))
    çš„æ•°æ®ï¼Œåœ¨å•ä¸ª V100 æ•°æ®ä¸­å¿ƒGPUä¸Šè®­ç»ƒ GPT-3 éœ€è¦ 355 å¹´ï¼Œåœ¨æ¶ˆè´¹è€… RTX 8000 GPU ä¸Šéœ€è¦ 665 å¹´ã€‚
- en: 'We specify the configuration of the small GPT-2 model via the following Python
    dictionary, which we will use in the code examples later:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹ Python å­—å…¸æŒ‡å®šå°å‹ GPT-2 æ¨¡å‹çš„é…ç½®ï¼Œæˆ‘ä»¬å°†åœ¨åé¢çš„ä»£ç ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒï¼š
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the `GPT_CONFIG_124M` dictionary, we use concise variable names for clarity
    and to prevent long lines of code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ `GPT_CONFIG_124M` å­—å…¸ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€æ´çš„å˜é‡åä»¥æé«˜æ¸…æ™°åº¦å¹¶é˜²æ­¢ä»£ç è¡Œè¿‡é•¿ï¼š
- en: '`vocab_size` refers to a vocabulary of 50,257 words, as used by the BPE tokenizer
    (see chapter 2).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` æŒ‡çš„æ˜¯ BPE åˆ†è¯å™¨ä½¿ç”¨çš„ 50,257 ä¸ªå•è¯çš„è¯æ±‡é‡ï¼ˆè§ç¬¬2ç« ï¼‰ã€‚'
- en: '`context_length` denotes the maximum number of input tokens the model can handle
    via the positional embeddings (see chapter 2).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length` è¡¨ç¤ºæ¨¡å‹é€šè¿‡ä½ç½®åµŒå…¥å¯ä»¥å¤„ç†çš„æœ€å¤§è¾“å…¥æ ‡è®°æ•°ï¼ˆè§ç¬¬2ç« ï¼‰ã€‚'
- en: '`emb_dim` represents the embedding size, transforming each token into a 768-dimensional
    vector.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_dim` è¡¨ç¤ºåµŒå…¥å¤§å°ï¼Œå°†æ¯ä¸ªæ ‡è®°è½¬æ¢ä¸º 768 ç»´çš„å‘é‡ã€‚'
- en: '`n_heads` indicates the count of attention heads in the multi-head attention
    mechanism (see chapter 3).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_heads` è¡¨ç¤ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ³¨æ„åŠ›å¤´æ•°é‡ï¼ˆè§ç¬¬3ç« ï¼‰ã€‚'
- en: '`n_layers` specifies the number of transformer blocks in the model, which we
    will cover in the upcoming discussion.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layers` æŒ‡å®šäº†æ¨¡å‹ä¸­çš„ transformer å—çš„æ•°é‡ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­ä»‹ç»ã€‚'
- en: '`drop_rate` indicates the intensity of the dropout mechanism (0.1 implies a
    10% random drop out of hidden units) to prevent overfitting (see chapter 3).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_rate` è¡¨ç¤º dropout æœºåˆ¶çš„å¼ºåº¦ï¼ˆ0.1 è¡¨ç¤ºéšè—å•å…ƒéšæœºä¸¢å¼ƒ 10%ï¼‰ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆè§ç¬¬3ç« ï¼‰ã€‚'
- en: '`qkv_bias` determines whether to include a bias vector in the `Linear` layers
    of the multi-head attention for query, key, and value computations. We will initially
    disable this, following the norms of modern LLMs, but we will revisit it in chapter
    6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter
    6).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` å†³å®šäº†æ˜¯å¦åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ `Linear` å±‚ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼è®¡ç®—åŒ…å«ä¸€ä¸ªåç½®å‘é‡ã€‚æˆ‘ä»¬æœ€åˆå°†ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œéµå¾ªç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„èŒƒï¼Œä½†åœ¨ç¬¬6ç« ä¸­ï¼Œå½“æˆ‘ä»¬ä»
    OpenAI åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 æƒé‡åˆ°æˆ‘ä»¬çš„æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†é‡æ–°å®¡è§†å®ƒï¼ˆè§ç¬¬6ç« ï¼‰ã€‚'
- en: Using this configuration, we will implement a GPT placeholder architecture (`DummyGPTModel`),
    as shown in figure 4.3\. This will provide us with a big-picture view of how everything
    fits together and what other components we need to code to assemble the full GPT
    model architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤é…ç½®ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ª GPT å ä½ç¬¦æ¶æ„ (`DummyGPTModel`)ï¼Œå¦‚å›¾ 4.3 æ‰€ç¤ºã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›ä¸€ä¸ªæ•´ä½“è§†å›¾ï¼Œäº†è§£æ‰€æœ‰ç»„ä»¶å¦‚ä½•ç»„åˆåœ¨ä¸€èµ·ï¼Œä»¥åŠæˆ‘ä»¬éœ€è¦ç¼–å†™å“ªäº›å…¶ä»–ç»„ä»¶æ¥ç»„è£…å®Œæ•´çš„
    GPT æ¨¡å‹æ¶æ„ã€‚
- en: '![figure](../Images/4-3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-3.png)'
- en: Figure 4.3 The order in which we code the GPT architecture. We start with the
    GPT backbone, a placeholder architecture, before getting to the individual core
    pieces and eventually assembling them in a transformer block for the final GPT
    architecture.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾ 4.3 æˆ‘ä»¬ç¼–ç  GPT æ¶æ„çš„é¡ºåºã€‚æˆ‘ä»¬é¦–å…ˆä» GPT èƒŒéª¨ï¼Œä¸€ä¸ªå ä½ç¬¦æ¶æ„å¼€å§‹ï¼Œç„¶ååˆ°è¾¾å„ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œæœ€ç»ˆå°†å®ƒä»¬ç»„è£…æˆä¸€ä¸ª TransformerBlockï¼Œä»¥å½¢æˆæœ€ç»ˆçš„
    GPT æ¶æ„ã€‚
- en: The numbered boxes in figure 4.3 illustrate the order in which we tackle the
    individual concepts required to code the final GPT architecture. We will start
    with step 1, a placeholder GPT backbone we will call `DummyGPTModel`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.3 ä¸­çš„ç¼–å·æ¡†è¯´æ˜äº†æˆ‘ä»¬è§£å†³ç¼–ç æœ€ç»ˆ GPT æ¶æ„æ‰€éœ€çš„å„ä¸ªæ¦‚å¿µçš„é¡ºåºã€‚æˆ‘ä»¬å°†ä»æ­¥éª¤ 1 å¼€å§‹ï¼Œä¸€ä¸ªæˆ‘ä»¬å°†ç§°ä¹‹ä¸º `DummyGPTModel`
    çš„å ä½ç¬¦ GPT èƒŒéª¨ã€‚
- en: Listing 4.1 A placeholder GPT model architecture class
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 4.1 ä¸€ä¸ªå ä½ç¬¦ GPT æ¨¡å‹æ¶æ„ç±»
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Uses a placeholder for TransformerBlock'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 ä½¿ç”¨å ä½ç¬¦ä»£æ›¿ TransformerBlock'
- en: '#2 Uses a placeholder for LayerNorm'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 ä½¿ç”¨å ä½ç¬¦ä»£æ›¿ LayerNorm'
- en: '#3 A simple placeholder class that will be replaced by a real TransformerBlock
    later'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 ä¸€ä¸ªç®€å•çš„å ä½ç¬¦ç±»ï¼Œç¨åå°†ç”±å®é™…çš„ TransformerBlock æ›¿æ¢'
- en: '#4 This block does nothing and just returns its input.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 æ­¤å—ä¸åšä»»ä½•æ“ä½œï¼Œåªæ˜¯è¿”å›å…¶è¾“å…¥ã€‚'
- en: '#5 A simple placeholder class that will be replaced by a real LayerNorm later'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 ä¸€ä¸ªç®€å•çš„å ä½ç¬¦ç±»ï¼Œç¨åå°†ç”±å®é™…çš„ LayerNorm æ›¿æ¢'
- en: '#6 The parameters here are just to mimic the LayerNorm interface.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 è¿™é‡Œçš„å‚æ•°åªæ˜¯ä¸ºäº†æ¨¡ä»¿ LayerNorm æ¥å£ã€‚'
- en: The `DummyGPTModel` class in this code defines a simplified version of a GPT-like
    model using PyTorchâ€™s neural network module (`nn.Module`). The model architecture
    in the `DummyGPTModel` class consists of token and positional embeddings, dropout,
    a series of transformer blocks (`DummyTransformerBlock`), a final layer normalization
    (`DummyLayerNorm`), and a linear output layer (`out_head`). The configuration
    is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary
    we created earlier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ä¸­çš„ `DummyGPTModel` ç±»å®šä¹‰äº†ä¸€ä¸ªä½¿ç”¨ PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å— (`nn.Module`) çš„ç®€åŒ–ç‰ˆæœ¬çš„ç±»ä¼¼ GPT æ¨¡å‹ã€‚`DummyGPTModel`
    ç±»ä¸­çš„æ¨¡å‹æ¶æ„åŒ…æ‹¬æ ‡è®°å’Œä½ç½®åµŒå…¥ã€dropoutã€ä¸€ç³»åˆ—çš„ TransformerBlock (`DummyTransformerBlock`)ã€æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–
    (`DummyLayerNorm`) å’Œçº¿æ€§è¾“å‡ºå±‚ (`out_head`)ã€‚é…ç½®é€šè¿‡ Python å­—å…¸ä¼ å…¥ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ `GPT_CONFIG_124M`
    å­—å…¸ã€‚
- en: 'The `forward` method describes the data flow through the model: it computes
    token and positional embeddings for the input indices, applies dropout, processes
    the data through the transformer blocks, applies normalization, and finally produces
    logits with the linear output layer.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` æ–¹æ³•æè¿°äº†æ•°æ®åœ¨æ¨¡å‹ä¸­çš„æµåŠ¨ï¼šå®ƒä¸ºè¾“å…¥ç´¢å¼•è®¡ç®—æ ‡è®°å’Œä½ç½®åµŒå…¥ï¼Œåº”ç”¨ dropoutï¼Œé€šè¿‡ TransformerBlock å¤„ç†æ•°æ®ï¼Œåº”ç”¨å½’ä¸€åŒ–ï¼Œå¹¶æœ€ç»ˆé€šè¿‡çº¿æ€§è¾“å‡ºå±‚äº§ç”Ÿ
    logitsã€‚'
- en: The code in listing 4.1 is already functional. However, for now, note that we
    use placeholders (`DummyLayerNorm` and `DummyTransformerBlock`) for the transformer
    block and layer normalization, which we will develop later.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 4.1 ä¸­çš„ä»£ç å·²ç»å¯ä»¥å·¥ä½œã€‚ç„¶è€Œï¼Œç›®å‰è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å ä½ç¬¦ï¼ˆ`DummyLayerNorm` å’Œ `DummyTransformerBlock`ï¼‰æ¥ä»£æ›¿
    TransformerBlock å’Œå±‚å½’ä¸€åŒ–ï¼Œè¿™äº›æˆ‘ä»¬å°†åœ¨ä»¥åå¼€å‘ã€‚
- en: Next, we will prepare the input data and initialize a new GPT model to illustrate
    its usage. Building on our coding of the tokenizer (see chapter 2), letâ€™s now
    consider a high-level overview of how data flows in and out of a GPT model, as
    shown in figure 4.4.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‡†å¤‡è¾“å…¥æ•°æ®å¹¶åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ GPT æ¨¡å‹ä»¥å±•ç¤ºå…¶ç”¨æ³•ã€‚åŸºäºæˆ‘ä»¬å¯¹åˆ†è¯å™¨ï¼ˆè§ç¬¬ 2 ç« ï¼‰çš„ç¼–ç ï¼Œç°åœ¨è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ª GPT æ¨¡å‹ä¸­æ•°æ®æµå…¥å’Œæµå‡ºçš„é«˜çº§æ¦‚è¿°ï¼Œå¦‚å›¾
    4.4 æ‰€ç¤ºã€‚
- en: '![figure](../Images/4-4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-4.png)'
- en: Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded,
    and fed to the GPT model. Note that in our `DummyGPTClass` coded earlier, the
    token embedding is handled inside the GPT model. In LLMs, the embedded input token
    dimension typically matches the output dimension. The output embeddings here represent
    the context vectors (see chapter 3).
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾ 4.4 ä»ä¸€ä¸ªæ•´ä½“æ¦‚è¿°ä¸­å±•ç¤ºäº†è¾“å…¥æ•°æ®æ˜¯å¦‚ä½•è¢«æ ‡è®°åŒ–ã€åµŒå…¥å¹¶è¾“å…¥åˆ° GPT æ¨¡å‹ä¸­çš„ã€‚æ³¨æ„ï¼Œåœ¨æˆ‘ä»¬ä¹‹å‰ç¼–å†™çš„ `DummyGPTClass` ä¸­ï¼Œæ ‡è®°åµŒå…¥æ˜¯åœ¨
    GPT æ¨¡å‹å†…éƒ¨å¤„ç†çš„ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­ï¼ŒåµŒå…¥çš„è¾“å…¥æ ‡è®°ç»´åº¦é€šå¸¸ä¸è¾“å‡ºç»´åº¦ç›¸åŒ¹é…ã€‚è¿™é‡Œçš„è¾“å‡ºåµŒå…¥ä»£è¡¨ä¸Šä¸‹æ–‡å‘é‡ï¼ˆè§ç¬¬ 3 ç« ï¼‰ã€‚
- en: 'To implement these steps, we tokenize a batch consisting of two text inputs
    for the GPT model using the tiktoken tokenizer from chapter 2:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬2ç« ä¸­çš„tiktokenåˆ†è¯å™¨å¯¹GPTæ¨¡å‹çš„ä¸¤ä¸ªæ–‡æœ¬è¾“å…¥è¿›è¡Œåˆ†è¯ï¼š
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting token IDs for the two texts are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªæ–‡æœ¬çš„ç»“æœæ ‡è®°IDå¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 The first row corresponds to the first text, and the second row corresponds
    to the second text.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 ç¬¬ä¸€è¡Œå¯¹åº”äºç¬¬ä¸€æ®µæ–‡æœ¬ï¼Œç¬¬äºŒè¡Œå¯¹åº”äºç¬¬äºŒæ®µæ–‡æœ¬ã€‚'
- en: 'Next, we initialize a new 124-million-parameter `DummyGPTModel` instance and
    feed it the tokenized `batch`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„124ç™¾ä¸‡å‚æ•°çš„`DummyGPTModel`å®ä¾‹ï¼Œå¹¶ç»™å®ƒæä¾›åˆ†è¯åçš„`batch`ï¼š
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The model outputs, which are commonly referred to as logits, are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¾“å‡ºï¼Œé€šå¸¸ç§°ä¸ºlogitsï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output tensor has two rows corresponding to the two text samples. Each text
    sample consists of four tokens; each token is a 50,257-dimensional vector, which
    matches the size of the tokenizerâ€™s vocabulary.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå¼ é‡æœ‰ä¸¤è¡Œï¼Œå¯¹åº”äºä¸¤ä¸ªæ–‡æœ¬æ ·æœ¬ã€‚æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±å››ä¸ªæ ‡è®°ç»„æˆï¼›æ¯ä¸ªæ ‡è®°æ˜¯ä¸€ä¸ª50,257ç»´å‘é‡ï¼Œè¿™ä¸åˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°ç›¸åŒ¹é…ã€‚
- en: The embedding has 50,257 dimensions because each of these dimensions refers
    to a unique token in the vocabulary. When we implement the postprocessing code,
    we will convert these 50,257-dimensional vectors back into token IDs, which we
    can then decode into words.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æœ‰50,257ç»´ï¼Œå› ä¸ºè¿™äº›ç»´åº¦çš„æ¯ä¸€ä¸ªéƒ½æŒ‡ä»£è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªç‹¬ç‰¹æ ‡è®°ã€‚å½“æˆ‘ä»¬å®ç°åå¤„ç†ä»£ç æ—¶ï¼Œæˆ‘ä»¬å°†è¿™äº›50,257ç»´å‘é‡è½¬æ¢å›æ ‡è®°IDï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è§£ç æˆå•è¯ã€‚
- en: Now that we have taken a top-down look at the GPT architecture and its inputs
    and outputs, we will code the individual placeholders, starting with the real
    layer normalization class that will replace the `DummyLayerNorm` in the previous
    code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä»ä¸Šåˆ°ä¸‹å®¡è§†äº†GPTæ¶æ„åŠå…¶è¾“å…¥å’Œè¾“å‡ºï¼Œæˆ‘ä»¬å°†ç¼–å†™å•ä¸ªå ä½ç¬¦çš„ä»£ç ï¼Œä»æ›¿æ¢å…ˆå‰ä»£ç ä¸­çš„`DummyLayerNorm`çš„çœŸå®å±‚å½’ä¸€åŒ–ç±»å¼€å§‹ã€‚
- en: 4.2 Normalizing activations with layer normalization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 ä½¿ç”¨å±‚å½’ä¸€åŒ–å½’ä¸€åŒ–æ¿€æ´»
- en: Training deep neural networks with many layers can sometimes prove challenging
    due to problems like vanishing or exploding gradients. These problems lead to
    unstable training dynamics and make it difficult for the network to effectively
    adjust its weights, which means the learning process struggles to find a set of
    parameters (weights) for the neural network that minimizes the loss function.
    In other words, the network has difficulty learning the underlying patterns in
    the data to a degree that would allow it to make accurate predictions or decisions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šå±‚è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæœ‰æ—¶å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¯¼è‡´è®­ç»ƒåŠ¨æ€ä¸ç¨³å®šï¼Œä½¿å¾—ç½‘ç»œéš¾ä»¥æœ‰æ•ˆåœ°è°ƒæ•´å…¶æƒé‡ï¼Œè¿™æ„å‘³ç€å­¦ä¹ è¿‡ç¨‹éš¾ä»¥æ‰¾åˆ°ä¸€ç»„å‚æ•°ï¼ˆæƒé‡ï¼‰ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚æ¢å¥è¯è¯´ï¼Œç½‘ç»œéš¾ä»¥ä»¥å…è®¸å…¶åšå‡ºå‡†ç¡®é¢„æµ‹æˆ–å†³ç­–çš„ç¨‹åº¦å­¦ä¹ æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ã€‚
- en: Note â€ƒIf you are new to neural network training and the concepts of gradients,
    a brief introduction to these concepts can be found in section A.4 in appendix
    A. However, a deep mathematical understanding of gradients is not required to
    follow the contents of this book.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœä½ å¯¹ç¥ç»ç½‘ç»œè®­ç»ƒå’Œæ¢¯åº¦æ¦‚å¿µä¸ç†Ÿæ‚‰ï¼Œå¯ä»¥åœ¨é™„å½•Açš„A.4èŠ‚ä¸­æ‰¾åˆ°è¿™äº›æ¦‚å¿µçš„ç®€è¦ä»‹ç»ã€‚ç„¶è€Œï¼Œä¸ºäº†ç†è§£æœ¬ä¹¦çš„å†…å®¹ï¼Œä¸éœ€è¦å¯¹æ¢¯åº¦æœ‰æ·±å…¥æ•°å­¦ç†è§£ã€‚
- en: Letâ€™s now implement *layer normalization* to improve the stability and efficiency
    of neural network training. The main idea behind layer normalization is to adjust
    the activations (outputs) of a neural network layer to have a mean of 0 and a
    variance of 1, also known as unit variance. This adjustment speeds up the convergence
    to effective weights and ensures consistent, reliable training. In GPT-2 and modern
    transformer architectures, layer normalization is typically applied before and
    after the multi-head attention module, and, as we have seen with the `DummyLayerNorm`
    placeholder, before the final output layer. Figure 4.5 provides a visual overview
    of how layer normalization functions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥å®ç°*å±‚å½’ä¸€åŒ–*æ¥æé«˜ç¥ç»ç½‘ç»œè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚å±‚å½’ä¸€åŒ–çš„ä¸»è¦æ€æƒ³æ˜¯å°†ç¥ç»ç½‘ç»œå±‚çš„æ¿€æ´»ï¼ˆè¾“å‡ºï¼‰è°ƒæ•´ä¸ºå‡å€¼ä¸º0å’Œæ–¹å·®ä¸º1ï¼Œä¹Ÿç§°ä¸ºå•ä½æ–¹å·®ã€‚è¿™ç§è°ƒæ•´åŠ å¿«äº†æ”¶æ•›åˆ°æœ‰æ•ˆæƒé‡ï¼Œå¹¶ç¡®ä¿äº†ä¸€è‡´ã€å¯é çš„è®­ç»ƒã€‚åœ¨GPT-2å’Œç°ä»£å˜æ¢å™¨æ¶æ„ä¸­ï¼Œå±‚å½’ä¸€åŒ–é€šå¸¸åœ¨å¤šå¤´æ³¨æ„åŠ›æ¨¡å—å‰ååº”ç”¨ï¼Œæ­£å¦‚æˆ‘ä»¬é€šè¿‡`DummyLayerNorm`å ä½ç¬¦æ‰€çœ‹åˆ°çš„ï¼Œåœ¨æœ€ç»ˆè¾“å‡ºå±‚ä¹‹å‰ã€‚å›¾4.5æä¾›äº†å±‚å½’ä¸€åŒ–åŠŸèƒ½çš„è§†è§‰æ¦‚è¿°ã€‚
- en: '![figure](../Images/4-5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-5.png)'
- en: Figure 4.5 An illustration of layer normalization where the six outputs of the
    layer, also called activations, are normalized such that they have a 0 mean and
    a variance of 1.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.5 å±‚å½’ä¸€åŒ–çš„æ’å›¾ï¼Œå…¶ä¸­å±‚çš„å…­ä¸ªè¾“å‡ºï¼ˆä¹Ÿç§°ä¸ºæ¿€æ´»ï¼‰è¢«å½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬å…·æœ‰0å‡å€¼å’Œ1æ–¹å·®ã€‚
- en: 'We can recreate the example shown in figure 4.5 via the following code, where
    we implement a neural network layer with five inputs and six outputs that we apply
    to two input examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç é‡ç°å›¾4.5æ‰€ç¤ºçš„ç¤ºä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå…·æœ‰äº”ä¸ªè¾“å…¥å’Œå…­ä¸ªè¾“å‡ºçš„ç¥ç»ç½‘ç»œå±‚ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸¤ä¸ªè¾“å…¥ç¤ºä¾‹ï¼š
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Creates two training examples with five dimensions (features) each'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 åˆ›å»ºä¸¤ä¸ªå…·æœ‰äº”ä¸ªç»´åº¦ï¼ˆç‰¹å¾ï¼‰çš„è®­ç»ƒç¤ºä¾‹'
- en: 'This prints the following tensor, where the first row lists the layer outputs
    for the first input and the second row lists the layer outputs for the second
    row:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ‰“å°ä»¥ä¸‹å¼ é‡ï¼Œå…¶ä¸­ç¬¬ä¸€è¡Œåˆ—å‡ºç¬¬ä¸€ä¸ªè¾“å…¥çš„å±‚è¾“å‡ºï¼Œç¬¬äºŒè¡Œåˆ—å‡ºç¬¬äºŒä¸ªè¾“å…¥çš„å±‚è¾“å‡ºï¼š
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The neural network layer we have coded consists of a `Linear` layer followed
    by a nonlinear activation function, `ReLU` (short for rectified linear unit),
    which is a standard activation function in neural networks. If you are unfamiliar
    with `ReLU`, it simply thresholds negative inputs to 0, ensuring that a layer
    outputs only positive values, which explains why the resulting layer output does
    not contain any negative values. Later, we will use another, more sophisticated
    activation function in GPT.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¼–å†™çš„ç¥ç»ç½‘ç»œå±‚ç”±ä¸€ä¸ª`Linear`å±‚åè·Ÿä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°`ReLU`ï¼ˆå³ä¿®æ­£çº¿æ€§å•å…ƒï¼‰ç»„æˆï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œä¸­çš„æ ‡å‡†æ¿€æ´»å‡½æ•°ã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰`ReLU`ï¼Œå®ƒåªæ˜¯å°†è´Ÿè¾“å…¥é˜ˆå€¼è®¾ç½®ä¸º0ï¼Œç¡®ä¿å±‚åªè¾“å‡ºæ­£å€¼ï¼Œè¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆç»“æœå±‚è¾“å‡ºä¸åŒ…å«ä»»ä½•è´Ÿå€¼ã€‚ç¨åï¼Œæˆ‘ä»¬å°†åœ¨GPTä¸­ä½¿ç”¨å¦ä¸€ä¸ªæ›´å¤æ‚çš„æ¿€æ´»å‡½æ•°ã€‚
- en: 'Before we apply layer normalization to these outputs, letâ€™s examine the mean
    and variance:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å°†å±‚å½’ä¸€åŒ–åº”ç”¨äºè¿™äº›è¾“å‡ºä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å‡å€¼å’Œæ–¹å·®ï¼š
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output is
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first row in the mean tensor here contains the mean value for the first
    input row, and the second output row contains the mean for the second input row.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼å¼ é‡çš„ç¬¬ä¸€è¡ŒåŒ…å«ç¬¬ä¸€ä¸ªè¾“å…¥è¡Œçš„å‡å€¼å€¼ï¼Œç¬¬äºŒè¾“å‡ºè¡ŒåŒ…å«ç¬¬äºŒä¸ªè¾“å…¥è¡Œçš„å‡å€¼ã€‚
- en: Using `keepdim=True` in operations like mean or variance calculation ensures
    that the output tensor retains the same number of dimensions as the input tensor,
    even though the operation reduces the tensor along the dimension specified via
    `dim`. For instance, without `keepdim=True`, the returned mean tensor would be
    a two-dimensional vector `[0.1324,` `0.2170]` instead of a 2 Ã— 1â€“dimensional matrix
    `[[0.1324],` `[0.2170]]`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡å€¼æˆ–æ–¹å·®è®¡ç®—ç­‰æ“ä½œä¸­ä½¿ç”¨`keepdim=True`ç¡®ä¿è¾“å‡ºå¼ é‡ä¿ç•™ä¸è¾“å…¥å¼ é‡ç›¸åŒçš„ç»´åº¦æ•°ï¼Œå°½ç®¡æ“ä½œå‡å°‘äº†é€šè¿‡`dim`æŒ‡å®šçš„ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸ä½¿ç”¨`keepdim=True`ï¼Œè¿”å›çš„å‡å€¼å¼ é‡å°†æ˜¯ä¸€ä¸ªäºŒç»´å‘é‡`[0.1324,`
    `0.2170]`ï¼Œè€Œä¸æ˜¯ä¸€ä¸ª2 Ã— 1ç»´åº¦çš„çŸ©é˜µ`[[0.1324],` `[0.2170]]`ã€‚
- en: The `dim` parameter specifies the dimension along which the calculation of the
    statistic (here, mean or variance) should be performed in a tensor. As figure
    4.6 explains, for a two-dimensional tensor (like a matrix), using `dim=-1` for
    operations such as mean or variance calculation is the same as using `dim=1`.
    This is because `-1` refers to the tensorâ€™s last dimension, which corresponds
    to the columns in a two-dimensional tensor. Later, when adding layer normalization
    to the GPT model, which produces three-dimensional tensors with the shape `[batch_size,`
    `num_tokens,` `embedding_size]`, we can still use `dim=-1` for normalization across
    the last dimension, avoiding a change from `dim=1` to `dim=2`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim`å‚æ•°æŒ‡å®šäº†åœ¨å¼ é‡ä¸­è®¡ç®—ç»Ÿè®¡é‡ï¼ˆæ­¤å¤„ä¸ºå‡å€¼æˆ–æ–¹å·®ï¼‰åº”è¿›è¡Œçš„ç»´åº¦ã€‚å¦‚å›¾4.6æ‰€ç¤ºï¼Œå¯¹äºäºŒç»´å¼ é‡ï¼ˆå¦‚çŸ©é˜µï¼‰ï¼Œåœ¨å‡å€¼æˆ–æ–¹å·®è®¡ç®—ç­‰æ“ä½œä¸­ä½¿ç”¨`dim=-1`ä¸ä½¿ç”¨`dim=1`ç›¸åŒã€‚è¿™æ˜¯å› ä¸º`-1`æŒ‡çš„æ˜¯å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦ï¼Œåœ¨äºŒç»´å¼ é‡ä¸­å¯¹åº”äºåˆ—ã€‚åæ¥ï¼Œå½“æˆ‘ä»¬å°†å±‚å½’ä¸€åŒ–æ·»åŠ åˆ°GPTæ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹äº§ç”Ÿå½¢çŠ¶ä¸º`[batch_size,`
    `num_tokens,` `embedding_size]`çš„ä¸‰ç»´å¼ é‡æ—¶ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨`dim=-1`å¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…ä»`dim=1`å˜ä¸º`dim=2`ã€‚'
- en: '![figure](../Images/4-6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-6.png)'
- en: Figure 4.6 An illustration of the dim parameter when calculating the mean of
    a tensor. For instance, if we have a two-dimensional tensor (matrix) with dimensions
    `[rows,` `columns]`, using `dim=0` will perform the operation across rows (vertically,
    as shown at the bottom), resulting in an output that aggregates the data for each
    column. Using `dim=1` or `dim=-1` will perform the operation across columns (horizontally,
    as shown at the top), resulting in an output aggregating the data for each row.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.6å±•ç¤ºäº†è®¡ç®—å¼ é‡å‡å€¼æ—¶çš„`dim`å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªç»´åº¦ä¸º`[rows,` `columns]`çš„ä¸¤ç»´å¼ é‡ï¼ˆçŸ©é˜µï¼‰ï¼Œä½¿ç”¨`dim=0`å°†åœ¨è¡Œï¼ˆå‚ç›´ï¼Œå¦‚å›¾åº•éƒ¨æ‰€ç¤ºï¼‰ä¸Šæ‰§è¡Œæ“ä½œï¼Œç»“æœå°†èšåˆæ¯åˆ—çš„æ•°æ®ã€‚ä½¿ç”¨`dim=1`æˆ–`dim=-1`å°†åœ¨åˆ—ï¼ˆæ°´å¹³ï¼Œå¦‚å›¾é¡¶éƒ¨æ‰€ç¤ºï¼‰ä¸Šæ‰§è¡Œæ“ä½œï¼Œç»“æœå°†èšåˆæ¯è¡Œçš„æ•°æ®ã€‚
- en: 'Next, letâ€™s apply layer normalization to the layer outputs we obtained earlier.
    The operation consists of subtracting the mean and dividing by the square root
    of the variance (also known as the standard deviation):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å°†å±‚å½’ä¸€åŒ–åº”ç”¨äºæˆ‘ä»¬ä¹‹å‰è·å¾—çš„å±‚è¾“å‡ºã€‚è¿™ä¸ªæ“ä½œåŒ…æ‹¬å‡å»å¹³å‡å€¼å¹¶é™¤ä»¥æ–¹å·®çš„å¹³æ–¹æ ¹ï¼ˆä¹Ÿç§°ä¸ºæ ‡å‡†å·®ï¼‰ï¼š
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see based on the results, the normalized layer outputs, which now
    also contain negative values, have 0 mean and a variance of 1:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½’ä¸€åŒ–çš„å±‚è¾“å‡ºï¼Œç°åœ¨ä¹ŸåŒ…å«è´Ÿå€¼ï¼Œå…·æœ‰0å‡å€¼å’Œ1çš„æ–¹å·®ï¼š
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the value â€“5.9605e-08 in the output tensor is the scientific notation
    for â€“5.9605 Ã— 10^(-8), which is â€“0.000000059605 in decimal form. This value is
    very close to 0, but it is not exactly 0 due to small numerical errors that can
    accumulate because of the finite precision with which computers represent numbers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åˆ°è¾“å‡ºå¼ é‡ä¸­çš„å€¼-5.9605e-08æ˜¯ç§‘å­¦è®°æ•°æ³•è¡¨ç¤ºçš„-5.9605 Ã— 10^(-8)ï¼Œä»¥åè¿›åˆ¶å½¢å¼è¡¨ç¤ºä¸º-0.000000059605ã€‚è¿™ä¸ªå€¼éå¸¸æ¥è¿‘0ï¼Œä½†ç”±äºè®¡ç®—æœºè¡¨ç¤ºæ•°å­—çš„æœ‰é™ç²¾åº¦å¯èƒ½ç§¯ç´¯çš„å°æ•°å€¼è¯¯å·®ï¼Œå®ƒå¹¶ä¸å®Œå…¨ç­‰äº0ã€‚
- en: 'To improve readability, we can also turn off the scientific notation when printing
    tensor values by setting `sci_mode` to `False`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜å¯è¯»æ€§ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å°†`sci_mode`è®¾ç½®ä¸º`False`æ¥å…³é—­æ‰“å°å¼ é‡å€¼æ—¶çš„ç§‘å­¦è®°æ•°æ³•ï¼š
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So far, we have coded and applied layer normalization in a step-by-step process.
    Letâ€™s now encapsulate this process in a PyTorch module that we can use in the
    GPT model later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»é€æ­¥ç¼–å†™å¹¶åº”ç”¨äº†å±‚å½’ä¸€åŒ–ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿™ä¸ªè¿‡ç¨‹å°è£…åœ¨ä¸€ä¸ªPyTorchæ¨¡å—ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åé¢çš„GPTæ¨¡å‹ä¸­ä½¿ç”¨å®ƒã€‚
- en: Listing 4.2 A layer normalization class
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.2 å±‚å½’ä¸€åŒ–ç±»
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This specific implementation of layer normalization operates on the last dimension
    of the input tensor x, which represents the embedding dimension (`emb_dim`). The
    variable `eps` is a small constant (epsilon) added to the variance to prevent
    division by zero during normalization. The `scale` and `shift` are two trainable
    parameters (of the same dimension as the input) that the LLM automatically adjusts
    during training if it is determined that doing so would improve the modelâ€™s performance
    on its training task. This allows the model to learn appropriate scaling and shifting
    that best suit the data it is processing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å±‚å½’ä¸€åŒ–çš„å…·ä½“å®ç°æ“ä½œåœ¨è¾“å…¥å¼ é‡xçš„æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼Œå®ƒä»£è¡¨åµŒå…¥ç»´åº¦ï¼ˆ`emb_dim`ï¼‰ã€‚å˜é‡`eps`æ˜¯ä¸€ä¸ªå°çš„å¸¸æ•°ï¼ˆepsilonï¼‰ï¼Œåœ¨å½’ä¸€åŒ–è¿‡ç¨‹ä¸­æ·»åŠ åˆ°æ–¹å·®ä¸­ï¼Œä»¥é˜²æ­¢é™¤ä»¥é›¶ã€‚`scale`å’Œ`shift`æ˜¯ä¸¤ä¸ªå¯è®­ç»ƒçš„å‚æ•°ï¼ˆä¸è¾“å…¥å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœç¡®å®šè¿™æ ·åšä¼šæé«˜æ¨¡å‹åœ¨è®­ç»ƒä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒLLMä¼šè‡ªåŠ¨è°ƒæ•´è¿™äº›å‚æ•°ã€‚è¿™å…è®¸æ¨¡å‹å­¦ä¹ é€‚å½“çš„ç¼©æ”¾å’Œåç§»ï¼Œä»¥æœ€å¥½åœ°é€‚åº”å®ƒæ­£åœ¨å¤„ç†çš„æ•°æ®ã€‚
- en: Biased variance
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åå·®æ–¹å·®
- en: In our variance calculation method, we use an implementation detail by setting
    `unbiased=False`. For those curious about what this means, in the variance calculation,
    we divide by the number of inputs *n* in the variance formula. This approach does
    not apply Besselâ€™s correction, which typically uses *n* â€“ *1* instead of *n* in
    the denominator to adjust for bias in sample variance estimation. This decision
    results in a so-called biased estimate of the variance. For LLMs, where the embedding
    dimension *n* is significantly large, the difference between using *n* and *n*
    â€“ *1* is practically negligible. I chose this approach to ensure compatibility
    with the GPT-2 modelâ€™s normalization layers and because it reflects TensorFlowâ€™s
    default behavior, which was used to implement the original GPT-2 model. Using
    a similar setting ensures our method is compatible with the pretrained weights
    we will load in chapter 6\.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ–¹å·®è®¡ç®—æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è®¾ç½®`unbiased=False`ä½¿ç”¨äº†ä¸€ä¸ªå®ç°ç»†èŠ‚ã€‚å¯¹äºé‚£äº›å¯¹æ­¤æ„Ÿå…´è¶£çš„äººæ¥è¯´ï¼Œåœ¨æ–¹å·®è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®æ–¹å·®å…¬å¼ä¸­çš„è¾“å…¥æ•°é‡*n*è¿›è¡Œé™¤æ³•ã€‚è¿™ç§æ–¹æ³•ä¸åº”ç”¨è´å¡å°”æ ¡æ­£ï¼Œé€šå¸¸åœ¨åˆ†æ¯ä¸­ä½¿ç”¨*n*
    â€“ *1*è€Œä¸æ˜¯*n*æ¥è°ƒæ•´æ ·æœ¬æ–¹å·®ä¼°è®¡ä¸­çš„åå·®ã€‚è¿™ä¸ªå†³å®šå¯¼è‡´äº†ä¸€ä¸ªæ‰€è°“çš„åå·®æ–¹å·®ä¼°è®¡ã€‚å¯¹äºLLMsï¼Œå…¶ä¸­åµŒå…¥ç»´åº¦*n*éå¸¸å¤§ï¼Œä½¿ç”¨*n*å’Œ*n* â€“ *1*ä¹‹é—´çš„å·®å¼‚å®é™…ä¸Šå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æˆ‘é€‰æ‹©è¿™ç§æ–¹æ³•æ˜¯ä¸ºäº†ç¡®ä¿ä¸GPT-2æ¨¡å‹çš„å½’ä¸€åŒ–å±‚å…¼å®¹ï¼Œå¹¶ä¸”å› ä¸ºå®ƒåæ˜ äº†TensorFlowçš„é»˜è®¤è¡Œä¸ºï¼Œè¿™æ˜¯åŸå§‹GPT-2æ¨¡å‹æ‰€ä½¿ç”¨çš„ã€‚ä½¿ç”¨ç±»ä¼¼çš„è®¾ç½®ç¡®ä¿æˆ‘ä»¬çš„æ–¹æ³•ä¸æˆ‘ä»¬åœ¨ç¬¬6ç« ä¸­å°†è¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡å…¼å®¹ã€‚
- en: 'Letâ€™s now try the `LayerNorm` module in practice and apply it to the batch
    input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥å®é™…å°è¯•ä½¿ç”¨`LayerNorm`æ¨¡å—ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ‰¹è¾“å…¥ï¼š
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results show that the layer normalization code works as expected and normalizes
    the values of each of the two inputs such that they have a mean of 0 and a variance
    of 1:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œå±‚å½’ä¸€åŒ–ä»£ç æŒ‰é¢„æœŸå·¥ä½œï¼Œå¹¶å°†ä¸¤ä¸ªè¾“å…¥çš„å€¼å½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼š
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We have now covered two of the building blocks we will need to implement the
    GPT architecture, as shown in figure 4.7\. Next, we will look at the GELU activation
    function, which is one of the activation functions used in LLMs, instead of the
    traditional ReLU function we used previously.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»æ¶µç›–äº†æ„å»ºGPTæ¶æ„æ‰€éœ€çš„ä¸¤ä¸ªæ„å»ºå—ï¼Œå¦‚å›¾4.7æ‰€ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹GELUæ¿€æ´»å‡½æ•°ï¼Œè¿™æ˜¯LLMä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„ä¼ ç»ŸReLUå‡½æ•°ã€‚
- en: '![figure](../Images/4-7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-7.png)'
- en: Figure 4.7 The building blocks necessary to build the GPT architecture. So far,
    we have completed the GPT backbone and layer normalization. Next, we will focus
    on GELU activation and the feed forward network.
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.7 æ„å»ºGPTæ¶æ„æ‰€éœ€çš„æ„å»ºå—ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†GPTä¸»å¹²å’Œå±‚å½’ä¸€åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºGELUæ¿€æ´»å’Œå‰é¦ˆç½‘ç»œã€‚
- en: Layer normalization vs. batch normalization
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–ä¸æ‰¹å½’ä¸€åŒ–
- en: If you are familiar with batch normalization, a common and traditional normalization
    method for neural networks, you may wonder how it compares to layer normalization.
    Unlike batch normalization, which normalizes across the batch dimension, layer
    normalization normalizes across the feature dimension. LLMs often require significant
    computational resources, and the available hardware or the specific use case can
    dictate the batch size during training or inference. Since layer normalization
    normalizes each input independently of the batch size, it offers more flexibility
    and stability in these scenarios. This is particularly beneficial for distributed
    training or when deploying models in environments where resources are constrained.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ç†Ÿæ‚‰æ‰¹å½’ä¸€åŒ–ï¼Œè¿™æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç½‘ç»œä¼ ç»Ÿå½’ä¸€åŒ–æ–¹æ³•ï¼Œä½ å¯èƒ½æƒ³çŸ¥é“å®ƒä¸å±‚å½’ä¸€åŒ–ç›¸æ¯”å¦‚ä½•ã€‚ä¸æ‰¹å½’ä¸€åŒ–ä¸åŒï¼Œæ‰¹å½’ä¸€åŒ–æ˜¯åœ¨æ‰¹ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œå±‚å½’ä¸€åŒ–æ˜¯åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚LLMé€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè€Œå¯ç”¨çš„ç¡¬ä»¶æˆ–ç‰¹å®šçš„ç”¨ä¾‹å¯èƒ½ä¼šåœ¨è®­ç»ƒæˆ–æ¨ç†æœŸé—´å†³å®šæ‰¹å¤§å°ã€‚ç”±äºå±‚å½’ä¸€åŒ–ç‹¬ç«‹äºæ‰¹å¤§å°å¯¹æ¯ä¸ªè¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› æ­¤åœ¨è¿™äº›æƒ…å†µä¸‹å®ƒæä¾›äº†æ›´å¤šçš„çµæ´»æ€§å’Œç¨³å®šæ€§ã€‚è¿™åœ¨åˆ†å¸ƒå¼è®­ç»ƒæˆ–å°†æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤å…¶æœ‰ç›Šã€‚
- en: 4.3 Implementing a feed forward network with GELU activations
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°å®ç°å‰é¦ˆç½‘ç»œ
- en: Next, we will implement a small neural network submodule used as part of the
    transformer block in LLMs. We begin by implementing the *GELU* activation function,
    which plays a crucial role in this neural network submodule.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œå­æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½œä¸ºLLMä¸­transformerå—çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆå®ç°*GELU*æ¿€æ´»å‡½æ•°ï¼Œå®ƒåœ¨è¯¥ç¥ç»ç½‘ç»œå­æ¨¡å—ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚
- en: Note For additional information on implementing neural networks in PyTorch,
    see section A.5 in appendix A.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæœ‰å…³åœ¨PyTorchä¸­å®ç°ç¥ç»ç½‘ç»œçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…é™„å½•Aä¸­çš„A.5èŠ‚ã€‚
- en: Historically, the ReLU activation function has been commonly used in deep learning
    due to its simplicity and effectiveness across various neural network architectures.
    However, in LLMs, several other activation functions are employed beyond the traditional
    ReLU. Two notable examples are GELU (*Gaussian error linear unit*) and SwiGLU
    (*Swish-gated linear unit*).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å†å²ä¸Šçœ‹ï¼ŒReLUæ¿€æ´»å‡½æ•°ç”±äºå…¶ç®€å•æ€§å’Œåœ¨å„ç§ç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨LLMä¸­ï¼Œé™¤äº†ä¼ ç»Ÿçš„ReLUä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨äº†å…¶ä»–å‡ ç§æ¿€æ´»å‡½æ•°ã€‚ä¸¤ä¸ªå€¼å¾—æ³¨æ„çš„ä¾‹å­æ˜¯GELUï¼ˆé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼‰å’ŒSwiGLUï¼ˆSwishé—¨æ§çº¿æ€§å•å…ƒï¼‰ã€‚
- en: GELU and SwiGLU are more complex and smooth activation functions incorporating
    Gaussian and sigmoid-gated linear units, respectively. They offer improved performance
    for deep learning models, unlike the simpler ReLU.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: GELUå’ŒSwiGLUæ˜¯æ›´å¤æ‚ä¸”å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ï¼Œåˆ†åˆ«åŒ…å«é«˜æ–¯å’Œsigmoidé—¨æ§çº¿æ€§å•å…ƒã€‚ä¸ç®€å•çš„ReLUç›¸æ¯”ï¼Œå®ƒä»¬ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ”¹è¿›çš„æ€§èƒ½ã€‚
- en: 'The GELU activation function can be implemented in several ways; the exact
    version is defined as GELU(x) = xâ‹…ğ›·(x), where ğ›·(x) is the cumulative distribution
    function of the standard Gaussian distribution. In practice, however, itâ€™s common
    to implement a computationally cheaper approximation (the original GPT-2 model
    was also trained with this approximation, which was found via curve fitting):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GELUæ¿€æ´»å‡½æ•°å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼å®ç°ï¼›ç¡®åˆ‡ç‰ˆæœ¬å®šä¹‰ä¸ºGELU(x) = xâ‹…ğ›·(x)ï¼Œå…¶ä¸­ğ›·(x)æ˜¯æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œé€šå¸¸å®ç°ä¸€ä¸ªè®¡ç®—ä¸Šæ›´ä¾¿å®œçš„è¿‘ä¼¼ï¼ˆåŸå§‹GPT-2æ¨¡å‹ä¹Ÿæ˜¯ç”¨è¿™ä¸ªè¿‘ä¼¼è¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™ä¸ªè¿‘ä¼¼æ˜¯é€šè¿‡æ›²çº¿æ‹Ÿåˆå¾—åˆ°çš„ï¼‰ï¼š
- en: '![figure](../Images/Equation-eqs-4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-eqs-4.png)'
- en: In code, we can implement this function as a PyTorch module.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤å‡½æ•°å®ç°ä¸ºä¸€ä¸ªPyTorchæ¨¡å—ã€‚
- en: Listing 4.3 An implementation of the GELU activation function
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.3 GELUæ¿€æ´»å‡½æ•°çš„å®ç°
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, to get an idea of what this GELU function looks like and how it compares
    to the ReLU function, letâ€™s plot these functions side by side:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä¸ºäº†äº†è§£è¿™ä¸ªGELUå‡½æ•°çœ‹èµ·æ¥åƒä»€ä¹ˆä»¥åŠå®ƒä¸ReLUå‡½æ•°ç›¸æ¯”å¦‚ä½•ï¼Œè®©æˆ‘ä»¬å°†è¿™äº›å‡½æ•°å¹¶æ’ç»˜åˆ¶å‡ºæ¥ï¼š
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Creates 100 sample data points in the range â€“3 to 3'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 åœ¨-3åˆ°3çš„èŒƒå›´å†…åˆ›å»º100ä¸ªæ ·æœ¬æ•°æ®ç‚¹'
- en: As we can see in the resulting plot in figure 4.8, ReLU (right) is a piecewise
    linear function that outputs the input directly if it is positive; otherwise,
    it outputs zero. GELU (left) is a smooth, nonlinear function that approximates
    ReLU but with a non-zero gradient for almost all negative values (except at approximately
    *x* = â€“0.75).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬åœ¨å›¾4.8çš„ç»“æœå›¾ä¸­æ‰€è§ï¼ŒReLUï¼ˆå³ä¾§ï¼‰æ˜¯ä¸€ä¸ªåˆ†æ®µçº¿æ€§å‡½æ•°ï¼Œå¦‚æœè¾“å…¥ä¸ºæ­£ï¼Œåˆ™ç›´æ¥è¾“å‡ºè¾“å…¥ï¼›å¦åˆ™ï¼Œè¾“å‡ºé›¶ã€‚GELUï¼ˆå·¦ä¾§ï¼‰æ˜¯ä¸€ä¸ªå¹³æ»‘çš„éçº¿æ€§å‡½æ•°ï¼Œå®ƒè¿‘ä¼¼ReLUï¼Œä½†å‡ ä¹å¯¹æ‰€æœ‰è´Ÿå€¼ï¼ˆé™¤äº†å¤§çº¦*x*
    = â€“0.75ï¼‰éƒ½æœ‰ä¸€ä¸ªéé›¶æ¢¯åº¦ã€‚
- en: '![figure](../Images/4-8.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-8.png)'
- en: Figure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis
    shows the function inputs and the y-axis shows the function outputs.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.8 ä½¿ç”¨matplotlibç»˜åˆ¶çš„GELUå’ŒReLUå›¾ã€‚xè½´æ˜¾ç¤ºå‡½æ•°è¾“å…¥ï¼Œyè½´æ˜¾ç¤ºå‡½æ•°è¾“å‡ºã€‚
- en: The smoothness of GELU can lead to better optimization properties during training,
    as it allows for more nuanced adjustments to the modelâ€™s parameters. In contrast,
    ReLU has a sharp corner at zero (figure 4.18, right), which can sometimes make
    optimization harder, especially in networks that are very deep or have complex
    architectures. Moreover, unlike ReLU, which outputs zero for any negative input,
    GELU allows for a small, non-zero output for negative values. This characteristic
    means that during the training process, neurons that receive negative input can
    still contribute to the learning process, albeit to a lesser extent than positive
    inputs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GELUçš„å¹³æ»‘æ€§å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥æ›´å¥½çš„ä¼˜åŒ–ç‰¹æ€§ï¼Œå› ä¸ºå®ƒå…è®¸å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´ç»†è‡´çš„è°ƒæ•´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒReLUåœ¨é›¶ç‚¹æœ‰ä¸€ä¸ªå°–é”çš„æ‹è§’ï¼ˆå›¾4.18ï¼Œå³ä¾§ï¼‰ï¼Œè¿™æœ‰æ—¶ä¼šä½¿ä¼˜åŒ–æ›´åŠ å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨éå¸¸æ·±æˆ–å…·æœ‰å¤æ‚æ¶æ„çš„ç½‘ç»œä¸­ã€‚æ­¤å¤–ï¼Œä¸ReLUä¸åŒï¼ŒReLUå¯¹ä»»ä½•è´Ÿè¾“å…¥éƒ½è¾“å‡ºé›¶ï¼Œè€ŒGELUå…è®¸è´Ÿå€¼æœ‰ä¸€ä¸ªå°çš„ã€éé›¶çš„è¾“å‡ºã€‚è¿™ä¸€ç‰¹æ€§æ„å‘³ç€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¥æ”¶è´Ÿè¾“å…¥çš„ç¥ç»å…ƒä»ç„¶å¯ä»¥å‚ä¸åˆ°å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå°½ç®¡å…¶è´¡çŒ®ä¸å¦‚æ­£è¾“å…¥é‚£ä¹ˆå¤§ã€‚
- en: Next, letâ€™s use the GELU function to implement the small neural network module,
    `FeedForward`, that we will be using in the LLMâ€™s transformer block later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨GELUå‡½æ•°æ¥å®ç°æˆ‘ä»¬å°†è¦åœ¨LLMçš„transformerå—ä¸­ä½¿ç”¨çš„è¾ƒå°ç¥ç»ç½‘ç»œæ¨¡å—`FeedForward`ã€‚
- en: Listing 4.4 A feed forward neural network module
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.4 å‰é¦ˆç¥ç»ç½‘ç»œæ¨¡å—
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we can see, the `FeedForward` module is a small neural network consisting
    of two `Linear` layers and a `GELU` activation function. In the 124-million-parameter
    GPT model, it receives the input batches with tokens that have an embedding size
    of 768 each via the `GPT_CONFIG_124M` dictionary where `GPT_CONFIG_` `124M["emb_dim"]`
    `=` `768`. Figure 4.9 shows how the embedding size is manipulated inside this
    small feed forward neural network when we pass it some inputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œ`FeedForward`æ¨¡å—æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ª`Linear`å±‚å’Œä¸€ä¸ª`GELU`æ¿€æ´»å‡½æ•°ç»„æˆçš„å°å‹ç¥ç»ç½‘ç»œã€‚åœ¨124ç™¾ä¸‡å‚æ•°çš„GPTæ¨¡å‹ä¸­ï¼Œå®ƒé€šè¿‡`GPT_CONFIG_124M`å­—å…¸æ¥æ”¶è¾“å…¥æ‰¹æ¬¡ï¼Œå…¶ä¸­æ¯ä¸ªæ ‡è®°çš„åµŒå…¥å¤§å°ä¸º768ã€‚`GPT_CONFIG_`
    `124M["emb_dim"]` `=` `768`ã€‚å›¾4.9å±•ç¤ºäº†å½“æˆ‘ä»¬å‘è¿™ä¸ªå°å‰é¦ˆç¥ç»ç½‘ç»œä¼ é€’ä¸€äº›è¾“å…¥æ—¶ï¼Œå¦‚ä½•æ“ä½œåµŒå…¥å¤§å°ã€‚
- en: '![figure](../Images/4-9.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-9.png)'
- en: Figure 4.9 An overview of the connections between the layers of the feed forward
    neural network. This neural network can accommodate variable batch sizes and numbers
    of tokens in the input. However, the embedding size for each token is determined
    and fixed when initializing the weights.
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.9 å‰é¦ˆç¥ç»ç½‘ç»œå±‚ä¹‹é—´è¿æ¥çš„æ¦‚è¿°ã€‚è¿™ä¸ªç¥ç»ç½‘ç»œå¯ä»¥é€‚åº”å¯å˜çš„æ‰¹æ¬¡å¤§å°å’Œè¾“å…¥ä¸­çš„æ ‡è®°æ•°é‡ã€‚ç„¶è€Œï¼Œæ¯ä¸ªæ ‡è®°çš„åµŒå…¥å¤§å°åœ¨åˆå§‹åŒ–æƒé‡æ—¶æ˜¯ç¡®å®šå’Œå›ºå®šçš„ã€‚
- en: 'Following the example in figure 4.9, letâ€™s initialize a new `FeedForward` module
    with a token embedding size of 768 and feed it a batch input with two samples
    and three tokens each:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§å›¾4.9ä¸­çš„ç¤ºä¾‹ï¼Œè®©æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„`FeedForward`æ¨¡å—ï¼Œå…¶æ ‡è®°åµŒå…¥å¤§å°ä¸º768ï¼Œå¹¶ç»™å®ƒä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ ·æœ¬å’Œæ¯ä¸ªæ ·æœ¬ä¸‰ä¸ªæ ‡è®°çš„æ‰¹æ¬¡è¾“å…¥ï¼š
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Creates sample input with batch dimension 2'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 åˆ›å»ºå…·æœ‰æ‰¹æ¬¡ç»´åº¦2çš„æ ·æœ¬è¾“å…¥'
- en: 'As we can see, the shape of the output tensor is the same as that of the input
    tensor:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ä¸è¾“å…¥å¼ é‡çš„å½¢çŠ¶ç›¸åŒï¼š
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `FeedForward` module plays a crucial role in enhancing the modelâ€™s ability
    to learn from and generalize the data. Although the input and output dimensions
    of this module are the same, it internally expands the embedding dimension into
    a higher-dimensional space through the first linear layer, as illustrated in figure
    4.10\. This expansion is followed by a nonlinear GELU activation and then a contraction
    back to the original dimension with the second linear transformation. Such a design
    allows for the exploration of a richer representation space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeedForward`æ¨¡å—åœ¨å¢å¼ºæ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ å’Œæ³›åŒ–çš„èƒ½åŠ›æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å°½ç®¡è¯¥æ¨¡å—çš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦ç›¸åŒï¼Œä½†å®ƒé€šè¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚å°†åµŒå…¥ç»´åº¦æ‰©å±•åˆ°æ›´é«˜ç»´çš„ç©ºé—´ï¼Œå¦‚å›¾4.10æ‰€ç¤ºã€‚è¿™ç§æ‰©å±•éšåé€šè¿‡éçº¿æ€§GELUæ¿€æ´»ï¼Œç„¶åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å˜æ¢æ”¶ç¼©å›åŸå§‹ç»´åº¦ã€‚è¿™ç§è®¾è®¡å…è®¸æ¢ç´¢æ›´ä¸°å¯Œçš„è¡¨ç¤ºç©ºé—´ã€‚'
- en: '![figure](../Images/4-10.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-10.png)'
- en: Figure 4.10 An illustration of the expansion and contraction of the layer outputs
    in the feed forward neural network. First, the inputs expand by a factor of 4
    from 768 to 3,072 values. Then, the second layer compresses the 3,072 values back
    into a 768-dimensional representation.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.10 å±•ç¤ºäº†å‰é¦ˆç¥ç»ç½‘ç»œä¸­å±‚è¾“å‡ºçš„æ‰©å±•å’Œæ”¶ç¼©ã€‚é¦–å…ˆï¼Œè¾“å…¥é€šè¿‡4å€ä»768æ‰©å±•åˆ°3,072ä¸ªå€¼ã€‚ç„¶åï¼Œç¬¬äºŒå±‚å°†3,072ä¸ªå€¼å‹ç¼©å›768ç»´åº¦çš„è¡¨ç¤ºã€‚
- en: Moreover, the uniformity in input and output dimensions simplifies the architecture
    by enabling the stacking of multiple layers, as we will do later, without the
    need to adjust dimensions between them, thus making the model more scalable.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¾“å…¥å’Œè¾“å‡ºç»´åº¦çš„å‡åŒ€æ€§é€šè¿‡å…è®¸å †å å¤šä¸ªå±‚ï¼ˆæ­£å¦‚æˆ‘ä»¬ç¨åå°†è¦åšçš„ï¼‰æ¥ç®€åŒ–æ¶æ„ï¼Œè€Œæ— éœ€è°ƒæ•´å®ƒä»¬ä¹‹é—´çš„ç»´åº¦ï¼Œä»è€Œä½¿æ¨¡å‹æ›´å…·å¯æ‰©å±•æ€§ã€‚
- en: As figure 4.11 shows, we have now implemented most of the LLMâ€™s building blocks.
    Next, we will go over the concept of shortcut connections that we insert between
    different layers of a neural network, which are important for improving the training
    performance in deep neural network architectures.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾4.11æ‰€ç¤ºï¼Œæˆ‘ä»¬å·²å®ç°äº†LLMçš„å¤§éƒ¨åˆ†æ„å»ºæ¨¡å—ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œçš„ä¸åŒå±‚ä¹‹é—´æ’å…¥çš„æ·å¾„è¿æ¥çš„æ¦‚å¿µï¼Œè¿™å¯¹äºæé«˜æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„çš„è®­ç»ƒæ€§èƒ½è‡³å…³é‡è¦ã€‚
- en: '![figure](../Images/4-11.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-11.png)'
- en: Figure 4.11 The building blocks necessary to build the GPT architecture. The
    black checkmarks indicating those we have already covered.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.11 æ˜¾ç¤ºäº†æ„å»ºGPTæ¶æ„æ‰€éœ€çš„æ„å»ºæ¨¡å—ã€‚é»‘è‰²å‹¾å·è¡¨ç¤ºæˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„å†…å®¹ã€‚
- en: 4.4 Adding shortcut connections
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 æ·»åŠ æ·å¾„è¿æ¥
- en: Letâ€™s discuss the concept behind *shortcut connections*, also known as skip
    or residual connections. Originally, shortcut connections were proposed for deep
    networks in computer vision (specifically, in residual networks) to mitigate the
    challenge of vanishing gradients. The vanishing gradient problem refers to the
    issue where gradients (which guide weight updates during training) become progressively
    smaller as they propagate backward through the layers, making it difficult to
    effectively train earlier layers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹*æ·å¾„è¿æ¥*çš„æ¦‚å¿µï¼Œä¹Ÿç§°ä¸ºè·³è¿‡æˆ–æ®‹å·®è¿æ¥ã€‚æœ€åˆï¼Œæ·å¾„è¿æ¥æ˜¯ä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦ç½‘ç»œï¼ˆç‰¹åˆ«æ˜¯æ®‹å·®ç½‘ç»œï¼‰æå‡ºçš„ï¼Œä»¥å‡è½»æ¢¯åº¦æ¶ˆå¤±çš„æŒ‘æˆ˜ã€‚æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æŒ‡çš„æ˜¯æ¢¯åº¦ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŒ‡å¯¼æƒé‡æ›´æ–°çš„ï¼‰åœ¨åå‘ä¼ æ’­é€šè¿‡å±‚æ—¶é€æ¸å˜å°çš„ç°è±¡ï¼Œè¿™ä½¿å¾—æœ‰æ•ˆåœ°è®­ç»ƒæ—©æœŸå±‚å˜å¾—å›°éš¾ã€‚
- en: '![figure](../Images/4-12.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-12.png)'
- en: Figure 4.12 A comparison between a deep neural network consisting of five layers
    without (left) and with shortcut connections (right). Shortcut connections involve
    adding the inputs of a layer to its outputs, effectively creating an alternate
    path that bypasses certain layers. The gradients denote the mean absolute gradient
    at each layer, which we compute in listing 4.5.
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.12 æ¯”è¾ƒäº†ç”±äº”å±‚ç»„æˆçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­å·¦ä¾§ä¸ºæ— æ·å¾„è¿æ¥ï¼ˆå·¦ï¼‰ï¼Œå³ä¾§ä¸ºæœ‰æ·å¾„è¿æ¥ï¼ˆå³ï¼‰ã€‚æ·å¾„è¿æ¥æ¶‰åŠå°†æŸä¸€å±‚çš„è¾“å…¥æ·»åŠ åˆ°å…¶è¾“å‡ºä¸­ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºä¸€æ¡ç»•è¿‡æŸäº›å±‚çš„æ›¿ä»£è·¯å¾„ã€‚æ¢¯åº¦è¡¨ç¤ºæ¯ä¸€å±‚çš„å¹³å‡ç»å¯¹æ¢¯åº¦ï¼Œæˆ‘ä»¬åœ¨åˆ—è¡¨4.5ä¸­è®¡ç®—äº†è¿™äº›æ¢¯åº¦ã€‚
- en: Figure 4.12 shows that a shortcut connection creates an alternative, shorter
    path for the gradient to flow through the network by skipping one or more layers,
    which is achieved by adding the output of one layer to the output of a later layer.
    This is why these connections are also known as skip connections. They play a
    crucial role in preserving the flow of gradients during the backward pass in training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.12è¡¨æ˜ï¼Œé€šè¿‡è·³è¿‡ä¸€å±‚æˆ–å¤šå±‚ï¼Œæ·å¾„è¿æ¥ä¸ºæ¢¯åº¦æµåŠ¨åˆ°ç½‘ç»œä¸­åˆ›å»ºäº†ä¸€æ¡æ›¿ä»£çš„ã€æ›´çŸ­çš„è·¯å¾„ï¼Œè¿™æ˜¯é€šè¿‡å°†æŸä¸€å±‚çš„è¾“å‡ºæ·»åŠ åˆ°åç»­å±‚çš„è¾“å‡ºä¸­å®ç°çš„ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™äº›è¿æ¥ä¹Ÿè¢«ç§°ä¸ºè·³è¿‡è¿æ¥ã€‚å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå‘ä¼ é€’æ—¶ä¿æŒæ¢¯åº¦æµåŠ¨èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚
- en: In the following list, we implement the neural network in figure 4.12 to see
    how we can add shortcut connections in the `forward` method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹åˆ—è¡¨ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å›¾4.12ä¸­çš„ç¥ç»ç½‘ç»œï¼Œä»¥æŸ¥çœ‹æˆ‘ä»¬å¦‚ä½•åœ¨`forward`æ–¹æ³•ä¸­æ·»åŠ å¿«æ·è¿æ¥ã€‚
- en: Listing 4.5 A neural network to illustrate shortcut connections
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.5ï¼šç”¨äºè¯´æ˜å¿«æ·è¿æ¥çš„ç¥ç»ç½‘ç»œ
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Implements five layers'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 å®ç°äº”ä¸ªå±‚'
- en: '#2 Compute the output of the current layer'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 è®¡ç®—å½“å‰å±‚çš„è¾“å‡º'
- en: '#3 Check if shortcut can be applied'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 æ£€æŸ¥æ˜¯å¦å¯ä»¥åº”ç”¨å¿«æ·è¿æ¥'
- en: The code implements a deep neural network with five layers, each consisting
    of a `Linear` layer and a `GELU` activation function. In the forward pass, we
    iteratively pass the input through the layers and optionally add the shortcut
    connections if the `self.use_ shortcut` attribute is set to `True`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç å®ç°äº†ä¸€ä¸ªåŒ…å«äº”ä¸ªå±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªå±‚ç”±ä¸€ä¸ª`Linear`å±‚å’Œä¸€ä¸ª`GELU`æ¿€æ´»å‡½æ•°ç»„æˆã€‚åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿­ä»£åœ°å°†è¾“å…¥é€šè¿‡å±‚ä¼ é€’ï¼Œå¦‚æœå°†`self.use_shortcut`å±æ€§è®¾ç½®ä¸º`True`ï¼Œåˆ™å¯é€‰åœ°æ·»åŠ å¿«æ·è¿æ¥ã€‚
- en: 'Letâ€™s use this code to initialize a neural network without shortcut connections.
    Each layer will be initialized such that it accepts an example with three input
    values and returns three output values. The last layer returns a single output
    value:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç åˆå§‹åŒ–ä¸€ä¸ªæ²¡æœ‰å¿«æ·è¿æ¥çš„ç¥ç»ç½‘ç»œã€‚æ¯ä¸ªå±‚å°†è¢«åˆå§‹åŒ–ï¼Œä»¥ä¾¿å®ƒæ¥å—ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªè¾“å…¥å€¼çš„ç¤ºä¾‹ï¼Œå¹¶è¿”å›ä¸‰ä¸ªè¾“å‡ºå€¼ã€‚æœ€åä¸€å±‚è¿”å›ä¸€ä¸ªå•ä¸€çš„è¾“å‡ºå€¼ï¼š
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Specifies random seed for the initial weights for reproducibility'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 æŒ‡å®šåˆå§‹æƒé‡çš„éšæœºç§å­ä»¥å®ç°å¯é‡å¤æ€§'
- en: 'Next, we implement a function that computes the gradients in the modelâ€™s backward
    pass:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨æ¨¡å‹çš„åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—æ¢¯åº¦ï¼š
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Forward pass'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 æ­£å‘ä¼ æ’­'
- en: '#2 Calculates loss based on how close the target and output are'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 æ ¹æ®ç›®æ ‡å’Œè¾“å‡ºä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦è®¡ç®—æŸå¤±'
- en: '#3 Backward pass to calculate the gradients'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 åå‘ä¼ æ’­ä»¥è®¡ç®—æ¢¯åº¦'
- en: This code specifies a loss function that computes how close the model output
    and a user-specified target (here, for simplicity, the value 0) are. Then, when
    calling `loss.backward()`, PyTorch computes the loss gradient for each layer in
    the model. We can iterate through the weight parameters via `model.named_parameters()`.
    Suppose we have a 3 Ã— 3 weight parameter matrix for a given layer. In that case,
    this layer will have 3 Ã— 3 gradient values, and we print the mean absolute gradient
    of these 3 Ã— 3 gradient values to obtain a single gradient value per layer to
    compare the gradients between layers more easily.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ä»£ç æŒ‡å®šäº†ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°è®¡ç®—æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡ï¼ˆè¿™é‡Œï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæ˜¯å€¼0ï¼‰çš„æ¥è¿‘ç¨‹åº¦ã€‚ç„¶åï¼Œåœ¨è°ƒç”¨`loss.backward()`æ—¶ï¼ŒPyTorchè®¡ç®—æ¨¡å‹ä¸­æ¯ä¸ªå±‚çš„æŸå¤±æ¢¯åº¦ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡`model.named_parameters()`éå†æƒé‡å‚æ•°ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª3
    Ã— 3çš„æƒé‡å‚æ•°çŸ©é˜µï¼Œå¯¹äºç»™å®šçš„å±‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥å±‚å°†å…·æœ‰3 Ã— 3çš„æ¢¯åº¦å€¼ï¼Œæˆ‘ä»¬æ‰“å°è¿™äº›3 Ã— 3æ¢¯åº¦å€¼çš„å¹³å‡å€¼ç»å¯¹æ¢¯åº¦ï¼Œä»¥è·å¾—æ¯ä¸ªå±‚çš„å•ä¸ªæ¢¯åº¦å€¼ï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ°æ¯”è¾ƒå±‚ä¹‹é—´çš„æ¢¯åº¦ã€‚
- en: In short, the `.backward()` method is a convenient method in PyTorch that computes
    loss gradients, which are required during model training, without implementing
    the math for the gradient calculation ourselves, thereby making working with deep
    neural networks much more accessible.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œ`.backward()`æ–¹æ³•æ˜¯PyTorchä¸­çš„ä¸€ä¸ªæ–¹ä¾¿çš„æ–¹æ³•ï¼Œå®ƒè®¡ç®—æŸå¤±æ¢¯åº¦ï¼Œè¿™åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æ˜¯å¿…éœ€çš„ï¼Œè€Œä¸éœ€è¦æˆ‘ä»¬è‡ªå·±å®ç°æ¢¯åº¦è®¡ç®—çš„æ•°å­¦ï¼Œä»è€Œä½¿å¾—ä¸æ·±åº¦ç¥ç»ç½‘ç»œçš„å·¥ä½œå˜å¾—æ›´åŠ å®¹æ˜“ã€‚
- en: Note â€ƒIf you are unfamiliar with the concept of gradients and neural network
    training, I recommend reading sections A.4 and A.7 in appendix A.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœæ‚¨ä¸ç†Ÿæ‚‰æ¢¯åº¦ä»¥åŠç¥ç»ç½‘ç»œè®­ç»ƒçš„æ¦‚å¿µï¼Œæˆ‘å»ºè®®é˜…è¯»é™„å½•Aä¸­çš„A.4å’ŒA.7èŠ‚ã€‚
- en: 'Letâ€™s now use the `print_gradients` function and apply it to the model without
    skip connections:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç°åœ¨ä½¿ç”¨`print_gradients`å‡½æ•°å¹¶å°†å…¶åº”ç”¨äºæ²¡æœ‰è·³è¿‡è¿æ¥çš„æ¨¡å‹ï¼š
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output of the `print_gradients` function shows, the gradients become smaller
    as we progress from the last layer (`layers.4`) to the first layer (`layers.0`),
    which is a phenomenon called the *vanishing gradient problem*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_gradients`å‡½æ•°çš„è¾“å‡ºæ˜¾ç¤ºï¼Œéšç€æˆ‘ä»¬ä»æœ€åä¸€å±‚`(layers.4)`å‘ç¬¬ä¸€å±‚`(layers.0)`å‰è¿›ï¼Œæ¢¯åº¦å€¼ä¼šå˜å°ï¼Œè¿™æ˜¯ä¸€ç§ç§°ä¸º*æ¢¯åº¦æ¶ˆå¤±é—®é¢˜*çš„ç°è±¡ã€‚'
- en: 'Letâ€™s now instantiate a model with skip connections and see how it compares:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç°åœ¨å®ä¾‹åŒ–ä¸€ä¸ªå…·æœ‰è·³è¿‡è¿æ¥çš„æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒä¸æ²¡æœ‰å¿«æ·è¿æ¥çš„æ¨¡å‹ç›¸æ¯”å¦‚ä½•ï¼š
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output is
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The last layer `(layers.4`) still has a larger gradient than the other layers.
    However, the gradient value stabilizes as we progress toward the first layer (`layers.0`)
    and doesnâ€™t shrink to a vanishingly small value.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªå±‚`(layers.4`)çš„æ¢¯åº¦å€¼ä»ç„¶æ¯”å…¶ä»–å±‚è¦å¤§ã€‚ç„¶è€Œï¼Œéšç€æˆ‘ä»¬å‘ç¬¬ä¸€å±‚`(layers.0`)å‰è¿›ï¼Œæ¢¯åº¦å€¼è¶‹äºç¨³å®šï¼Œå¹¶ä¸ä¼šç¼©å°åˆ°ä¸€ä¸ªæå°çš„å€¼ã€‚
- en: In conclusion, shortcut connections are important for overcoming the limitations
    posed by the vanishing gradient problem in deep neural networks. Shortcut connections
    are a core building block of very large models such as LLMs, and they will help
    facilitate more effective training by ensuring consistent gradient flow across
    layers when we train the GPT model in the next chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œå¿«æ·è¿æ¥å¯¹äºå…‹æœæ·±åº¦ç¥ç»ç½‘ç»œä¸­æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„é™åˆ¶è‡³å…³é‡è¦ã€‚å¿«æ·è¿æ¥æ˜¯åƒLLMè¿™æ ·çš„å¤§å‹æ¨¡å‹çš„æ ¸å¿ƒæ„å»ºå—ï¼Œå®ƒä»¬å°†æœ‰åŠ©äºé€šè¿‡ç¡®ä¿åœ¨ä¸‹ä¸€ç« ä¸­è®­ç»ƒGPTæ¨¡å‹æ—¶å±‚é—´æ¢¯åº¦æµçš„è¿ç»­æ€§æ¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„è®­ç»ƒã€‚
- en: Next, weâ€™ll connect all of the previously covered concepts (layer normalization,
    GELU activations, feed forward module, and shortcut connections) in a transformer
    block, which is the final building block we need to code the GPT architecture.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠŠä¹‹å‰æ¶µç›–çš„æ‰€æœ‰æ¦‚å¿µï¼ˆå±‚å½’ä¸€åŒ–ã€GELUæ¿€æ´»ã€å‰é¦ˆæ¨¡å—å’Œå¿«æ·è¿æ¥ï¼‰è¿æ¥åˆ°transformerå—ä¸­ï¼Œè¿™æ˜¯æˆ‘ä»¬éœ€è¦ç¼–ç GPTæ¶æ„çš„æœ€åä¸€ä¸ªæ„å»ºå—ã€‚
- en: 4.5 Connecting attention and linear layers in a transformer block
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 åœ¨transformerå—ä¸­è¿æ¥æ³¨æ„åŠ›å’Œçº¿æ€§å±‚
- en: 'Now, letâ€™s implement the *transformer block*, a fundamental building block
    of GPT and other LLM architectures. This block, which is repeated a dozen times
    in the 124-million-parameter GPT-2 architecture, combines several concepts we
    have previously covered: multi-head attention, layer normalization, dropout, feed
    forward layers, and GELU activations. Later, we will connect this transformer
    block to the remaining parts of the GPT architecture.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®ç°*transformerå—*ï¼Œè¿™æ˜¯GPTå’Œå…¶ä»–LLMæ¶æ„çš„åŸºæœ¬æ„å»ºå—ã€‚è¿™ä¸ªå—åœ¨1.24äº¿å‚æ•°çš„GPT-2æ¶æ„ä¸­é‡å¤äº†12æ¬¡ï¼Œç»“åˆäº†æˆ‘ä»¬ä¹‹å‰æ¶µç›–çš„å‡ ä¸ªæ¦‚å¿µï¼šå¤šå¤´æ³¨æ„åŠ›ã€å±‚å½’ä¸€åŒ–ã€dropoutã€å‰é¦ˆå±‚å’ŒGELUæ¿€æ´»ã€‚ç¨åï¼Œæˆ‘ä»¬å°†æŠŠè¿™ä¸ªtransformerå—è¿æ¥åˆ°GPTæ¶æ„çš„å…¶ä½™éƒ¨åˆ†ã€‚
- en: '![figure](../Images/4-13.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-13.png)'
- en: Figure 4.13 An illustration of a transformer block. Input tokens have been embedded
    into 768-dimensional vectors. Each row corresponds to one tokenâ€™s vector representation.
    The outputs of the transformer block are vectors of the same dimension as the
    input, which can then be fed into subsequent layers in an LLM.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.13 transformerå—çš„ç¤ºæ„å›¾ã€‚è¾“å…¥æ ‡è®°å·²åµŒå…¥åˆ°768ç»´å‘é‡ä¸­ã€‚æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªæ ‡è®°çš„å‘é‡è¡¨ç¤ºã€‚transformerå—è¾“å‡ºçš„å‘é‡ä¸è¾“å…¥å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼Œç„¶åå¯ä»¥è¢«è¾“å…¥åˆ°LLMçš„åç»­å±‚ä¸­ã€‚
- en: Figure 4.13 shows a transformer block that combines several components, including
    the masked multi-head attention module (see chapter 3) and the `FeedForward` module
    we previously implemented (see section 4.3). When a transformer block processes
    an input sequence, each element in the sequence (for example, a word or subword
    token) is represented by a fixed-size vector (in this case, 768 dimensions). The
    operations within the transformer block, including multi-head attention and feed
    forward layers, are designed to transform these vectors in a way that preserves
    their dimensionality.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.13å±•ç¤ºäº†ä¸€ä¸ªç»“åˆäº†å¤šä¸ªç»„ä»¶çš„transformerå—ï¼ŒåŒ…æ‹¬ç¬¬3ç« ä¸­æåˆ°çš„æ©ç å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼ˆè§ç« èŠ‚3ï¼‰ä»¥åŠæˆ‘ä»¬ä¹‹å‰å®ç°è¿‡çš„`FeedForward`æ¨¡å—ï¼ˆè§ç¬¬4.3èŠ‚ï¼‰ã€‚å½“transformerå—å¤„ç†ä¸€ä¸ªè¾“å…¥åºåˆ—æ—¶ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå•è¯æˆ–å­è¯æ ‡è®°ï¼‰ç”±ä¸€ä¸ªå›ºå®šå¤§å°çš„å‘é‡è¡¨ç¤ºï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ768ç»´ï¼‰ã€‚transformerå—å†…çš„æ“ä½œï¼ŒåŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚ï¼Œè¢«è®¾è®¡æˆä»¥ä¿ç•™å…¶ç»´åº¦çš„è¿™ç§æ–¹å¼è½¬æ¢è¿™äº›å‘é‡ã€‚
- en: The idea is that the self-attention mechanism in the multi-head attention block
    identifies and analyzes relationships between elements in the input sequence.
    In contrast, the feed forward network modifies the data individually at each position.
    This combination not only enables a more nuanced understanding and processing
    of the input but also enhances the modelâ€™s overall capacity for handling complex
    data patterns.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç†å¿µåœ¨äºå¤šå¤´æ³¨æ„åŠ›å—ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«å¹¶åˆ†æè¾“å…¥åºåˆ—ä¸­å…ƒç´ ä¹‹é—´çš„å…³ç³»ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå‰é¦ˆç½‘ç»œåœ¨æ¯ä¸€ä¸ªä½ç½®å•ç‹¬ä¿®æ”¹æ•°æ®ã€‚è¿™ç§ç»„åˆä¸ä»…ä½¿å¯¹è¾“å…¥çš„ç†è§£å’Œå¤„ç†æ›´åŠ ç»†è…»ï¼Œè€Œä¸”å¢å¼ºäº†æ¨¡å‹å¤„ç†å¤æ‚æ•°æ®æ¨¡å¼çš„æ•´ä½“èƒ½åŠ›ã€‚
- en: We can create the `TransformerBlock` in code.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ä»£ç ä¸­åˆ›å»º`TransformerBlock`ã€‚
- en: Listing 4.6 The transformer block component of GPT
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.6 GPTçš„transformerå—ç»„ä»¶
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Shortcut connection for attention block'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 æ³¨æ„åŠ›å—çš„å¿«æ·è¿æ¥'
- en: '#2 Add the original input back'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 å°†åŸå§‹è¾“å…¥æ·»åŠ å›'
- en: '#3 Shortcut connection for feed forward block'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 å‰é¦ˆå—çš„å¿«æ·è¿æ¥'
- en: '#4 Adds the original input back'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 æ·»åŠ åŸå§‹è¾“å…¥å›'
- en: The given code defines a `TransformerBlock` class in PyTorch that includes a
    multi-head attention mechanism (`MultiHeadAttention`) and a feed forward network
    (`FeedForward`), both configured based on a provided configuration dictionary
    (`cfg`), such as `GPT_CONFIG_124M`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šçš„ä»£ç åœ¨ PyTorch ä¸­å®šä¹‰äº†ä¸€ä¸ª `TransformerBlock` ç±»ï¼Œè¯¥ç±»åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (`MultiHeadAttention`)
    å’Œä¸€ä¸ªå‰é¦ˆç½‘ç»œ (`FeedForward`)ï¼Œè¿™ä¸¤ä¸ªç»„ä»¶éƒ½æ˜¯åŸºäºæä¾›çš„é…ç½®å­—å…¸ (`cfg`) é…ç½®çš„ï¼Œä¾‹å¦‚ `GPT_CONFIG_124M`ã€‚
- en: Layer normalization (`LayerNorm`) is applied before each of these two components,
    and dropout is applied after them to regularize the model and prevent overfitting.
    This is also known as *Pre-LayerNorm*. Older architectures, such as the original
    transformer model, applied layer normalization after the self-attention and feed
    forward networks instead, known as *Post-LayerNorm*, which often leads to worse
    training dynamics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ä¸ªç»„ä»¶ä¹‹å‰åº”ç”¨å±‚å½’ä¸€åŒ– (`LayerNorm`)ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹ååº”ç”¨ dropout ä»¥æ­£åˆ™åŒ–æ¨¡å‹å¹¶é˜²æ­¢è¿‡æ‹Ÿåˆã€‚è¿™è¢«ç§°ä¸º *Pre-LayerNorm*ã€‚è¾ƒè€çš„æ¶æ„ï¼Œå¦‚åŸå§‹çš„
    Transformer æ¨¡å‹ï¼Œåœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œä¹‹ååº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œç§°ä¸º *Post-LayerNorm*ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ›´å·®çš„è®­ç»ƒåŠ¨æ€ã€‚
- en: The class also implements the forward pass, where each component is followed
    by a shortcut connection that adds the input of the block to its output. This
    critical feature helps gradients flow through the network during training and
    improves the learning of deep models (see section 4.4).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç±»è¿˜å®ç°äº†å‰å‘ä¼ é€’ï¼Œå…¶ä¸­æ¯ä¸ªç»„ä»¶ä¹‹åéƒ½è·Ÿç€ä¸€ä¸ªå¿«æ·è¿æ¥ï¼Œè¯¥è¿æ¥å°†å—çš„è¾“å…¥æ·»åŠ åˆ°å…¶è¾“å‡ºä¸­ã€‚è¿™ä¸ªå…³é”®ç‰¹æ€§æœ‰åŠ©äºåœ¨è®­ç»ƒæœŸé—´é€šè¿‡ç½‘ç»œæµåŠ¨æ¢¯åº¦ï¼Œå¹¶æé«˜æ·±åº¦æ¨¡å‹çš„å­¦ä¹ ï¼ˆå‚è§ç¬¬
    4.4 èŠ‚ï¼‰ã€‚
- en: 'Using the `GPT_CONFIG_124M` dictionary we defined earlier, letâ€™s instantiate
    a transformer block and feed it some sample data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„ `GPT_CONFIG_124M` å­—å…¸ï¼Œè®©æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ª Transformer æ¨¡å—å¹¶ç»™å®ƒä¸€äº›æ ·æœ¬æ•°æ®ï¼š
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Creates sample input of shape [batch_size, num_tokens, emb_dim]'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 åˆ›å»ºå½¢çŠ¶ä¸º [batch_size, num_tokens, emb_dim] çš„æ ·æœ¬è¾“å…¥'
- en: The output is
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç»“æœä¸º
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see, the transformer block maintains the input dimensions in its output,
    indicating that the transformer architecture processes sequences of data without
    altering their shape throughout the network.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼ŒTransformer æ¨¡å—åœ¨å…¶è¾“å‡ºä¸­ä¿æŒäº†è¾“å…¥ç»´åº¦ï¼Œè¿™è¡¨æ˜ Transformer æ¶æ„åœ¨å¤„ç†æ•°æ®åºåˆ—æ—¶ä¸ä¼šæ”¹å˜å®ƒä»¬çš„å½¢çŠ¶ã€‚
- en: The preservation of shape throughout the transformer block architecture is not
    incidental but a crucial aspect of its design. This design enables its effective
    application across a wide range of sequence-to-sequence tasks, where each output
    vector directly corresponds to an input vector, maintaining a one-to-one relationship.
    However, the output is a context vector that encapsulates information from the
    entire input sequence (see chapter 3). This means that while the physical dimensions
    of the sequence (length and feature size) remain unchanged as it passes through
    the transformer block, the content of each output vector is re-encoded to integrate
    contextual information from across the entire input sequence.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Transformer æ¨¡å—æ¶æ„ä¸­ä¿æŒå½¢çŠ¶ä¸æ˜¯å¶ç„¶çš„ï¼Œè€Œæ˜¯å…¶è®¾è®¡çš„ä¸€ä¸ªå…³é”®æ–¹é¢ã€‚è¿™ç§è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åº”ç”¨äºå¹¿æ³›çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å‡ºå‘é‡ç›´æ¥å¯¹åº”äºä¸€ä¸ªè¾“å…¥å‘é‡ï¼Œä¿æŒä¸€å¯¹ä¸€çš„å…³ç³»ã€‚ç„¶è€Œï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ï¼Œå®ƒå°è£…äº†æ•´ä¸ªè¾“å…¥åºåˆ—çš„ä¿¡æ¯ï¼ˆå‚è§ç¬¬
    3 ç« ï¼‰ã€‚è¿™æ„å‘³ç€å°½ç®¡åºåˆ—çš„ç‰©ç†ç»´åº¦ï¼ˆé•¿åº¦å’Œç‰¹å¾å¤§å°ï¼‰åœ¨é€šè¿‡ Transformer æ¨¡å—æ—¶ä¿æŒä¸å˜ï¼Œä½†æ¯ä¸ªè¾“å‡ºå‘é‡çš„å†…å®¹è¢«é‡æ–°ç¼–ç ä»¥æ•´åˆæ•´ä¸ªè¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- en: With the transformer block implemented, we now have all the building blocks
    needed to implement the GPT architecture. As illustrated in figure 4.14, the transformer
    block combines layer normalization, the feed forward network, GELU activations,
    and shortcut connections. As we will eventually see, this transformer block will
    make up the main component of the GPT architecture.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®ç°äº† Transformer æ¨¡å—ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†å®ç° GPT æ¶æ„æ‰€éœ€çš„æ‰€æœ‰æ„å»ºå—ã€‚å¦‚å›¾ 4.14 æ‰€ç¤ºï¼ŒTransformer æ¨¡å—ç»“åˆäº†å±‚å½’ä¸€åŒ–ã€å‰é¦ˆç½‘ç»œã€GELU
    æ¿€æ´»å’Œå¿«æ·è¿æ¥ã€‚æ­£å¦‚æˆ‘ä»¬æœ€ç»ˆå°†çœ‹åˆ°çš„ï¼Œè¿™ä¸ª Transformer æ¨¡å—å°†æˆä¸º GPT æ¶æ„çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚
- en: '![figure](../Images/4-14.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-14.png)'
- en: Figure 4.14 The building blocks necessary to build the GPT architecture. The
    black checks indicate the blocks we have completed.
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾ 4.14 æ„å»º GPT æ¶æ„æ‰€éœ€çš„æ„å»ºå—ã€‚é»‘è‰²å‹¾é€‰è¡¨ç¤ºæˆ‘ä»¬å·²å®Œæˆçš„å—ã€‚
- en: 4.6 Coding the GPT model
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 ç¼–å†™ GPT æ¨¡å‹
- en: We started this chapter with a big-picture overview of a GPT architecture that
    we called `DummyGPTModel`. In this `DummyGPTModel` code implementation, we showed
    the input and outputs to the GPT model, but its building blocks remained a black
    box using a `DummyTransformerBlock` and `DummyLayerNorm` class as placeholders.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥å¯¹ç§°ä¸º`DummyGPTModel`çš„GPTæ¶æ„çš„å¤§å›¾æ¦‚è¿°å¼€å§‹æœ¬ç« ã€‚åœ¨è¿™ä¸ª`DummyGPTModel`ä»£ç å®ç°ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†GPTæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä½†å…¶æ„å»ºå—ä»ç„¶æ˜¯ä¸€ä¸ªé»‘ç›’ï¼Œä½¿ç”¨`DummyTransformerBlock`å’Œ`DummyLayerNorm`ç±»ä½œä¸ºå ä½ç¬¦ã€‚
- en: Letâ€™s now replace the `DummyTransformerBlock` and `DummyLayerNorm` placeholders
    with the real `TransformerBlock` and `LayerNorm` classes we coded previously to
    assemble a fully working version of the original 124-million-parameter version
    of GPT-2\. In chapter 5, we will pretrain a GPT-2 model, and in chapter 6, we
    will load in the pretrained weights from OpenAI.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†`DummyTransformerBlock`å’Œ`DummyLayerNorm`å ä½ç¬¦æ›¿æ¢ä¸ºæˆ‘ä»¬ä¹‹å‰ç¼–å†™çš„çœŸå®`TransformerBlock`å’Œ`LayerNorm`ç±»ï¼Œä»¥ç»„è£…GPT-2åŸå§‹1.24äº¿å‚æ•°ç‰ˆæœ¬çš„å®Œæ•´å·¥ä½œç‰ˆæœ¬ã€‚åœ¨ç¬¬5ç« ä¸­ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒGPT-2æ¨¡å‹ï¼Œåœ¨ç¬¬6ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»OpenAIåŠ è½½é¢„è®­ç»ƒçš„æƒé‡ã€‚
- en: Before we assemble the GPT-2 model in code, letâ€™s look at its overall structure,
    as shown in figure 4.15, which includes all the concepts we have covered so far.
    As we can see, the transformer block is repeated many times throughout a GPT model
    architecture. In the case of the 124-million-parameter GPT-2 model, itâ€™s repeated
    12 times, which we specify via the `n_layers` entry in the `GPT_CONFIG_124M` dictionary.
    This transform block is repeated 48 times in the largest GPT-2 model with 1,542
    million parameters.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç”¨ä»£ç ç»„è£…GPT-2æ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„æ•´ä½“ç»“æ„ï¼Œå¦‚å›¾4.15æ‰€ç¤ºï¼Œå…¶ä¸­åŒ…æ‹¬æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æ‰€æ¶µç›–çš„æ‰€æœ‰æ¦‚å¿µã€‚æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå˜æ¢å—åœ¨æ•´ä¸ªGPTæ¨¡å‹æ¶æ„ä¸­è¢«é‡å¤å¤šæ¬¡ã€‚åœ¨1.24äº¿å‚æ•°çš„GPT-2æ¨¡å‹ä¸­ï¼Œå®ƒè¢«é‡å¤12æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡`GPT_CONFIG_124M`å­—å…¸ä¸­çš„`n_layers`æ¡ç›®æ¥æŒ‡å®šã€‚åœ¨å…·æœ‰15.42äº¿å‚æ•°çš„æœ€å¤§GPT-2æ¨¡å‹ä¸­ï¼Œå˜æ¢å—è¢«é‡å¤48æ¬¡ã€‚
- en: '![figure](../Images/4-15.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-15.png)'
- en: Figure 4.15 An overview of the GPT model architecture showing the flow of data
    through the GPT model. Starting from the bottom, tokenized text is first converted
    into token embeddings, which are then augmented with positional embeddings. This
    combined information forms a tensor that is passed through a series of transformer
    blocks shown in the center (each containing multi-head attention and feed forward
    neural network layers with dropout and layer normalization), which are stacked
    on top of each other and repeated 12 times.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.15å±•ç¤ºäº†GPTæ¨¡å‹æ¶æ„çš„æ¦‚è¿°ï¼Œæ˜¾ç¤ºäº†æ•°æ®é€šè¿‡GPTæ¨¡å‹æµåŠ¨çš„è¿‡ç¨‹ã€‚ä»åº•éƒ¨å¼€å§‹ï¼Œæ ‡è®°åŒ–æ–‡æœ¬é¦–å…ˆè¢«è½¬æ¢ä¸ºæ ‡è®°åµŒå…¥ï¼Œç„¶åæ·»åŠ ä½ç½®åµŒå…¥ã€‚è¿™äº›ç»„åˆä¿¡æ¯å½¢æˆä¸€ä¸ªå¼ é‡ï¼Œé€šè¿‡ä¸­å¿ƒçš„ä¸€ç³»åˆ—å˜æ¢å—ï¼ˆæ¯ä¸ªåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå…·æœ‰dropoutå’Œå±‚å½’ä¸€åŒ–çš„å‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼‰ä¼ é€’ï¼Œè¿™äº›å˜æ¢å—å †å åœ¨ä¸€èµ·å¹¶é‡å¤12æ¬¡ã€‚
- en: The output from the final transformer block then goes through a final layer
    normalization step before reaching the linear output layer. This layer maps the
    transformerâ€™s output to a high-dimensional space (in this case, 50,257 dimensions,
    corresponding to the modelâ€™s vocabulary size) to predict the next token in the
    sequence.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå˜æ¢å—è¾“å‡ºçš„æ•°æ®åœ¨åˆ°è¾¾çº¿æ€§è¾“å‡ºå±‚ä¹‹å‰ä¼šç»è¿‡æœ€åçš„å±‚å½’ä¸€åŒ–æ­¥éª¤ã€‚è¿™ä¸€å±‚å°†å˜æ¢å™¨çš„è¾“å‡ºæ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´ç©ºé—´ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ50,257ç»´ï¼Œå¯¹åº”äºæ¨¡å‹çš„è¯æ±‡é‡å¤§å°ï¼‰ï¼Œä»¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚
- en: Letâ€™s now code the architecture in figure 4.15.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥ç¼–å†™å›¾4.15ä¸­çš„æ¶æ„ã€‚
- en: Listing 4.7 The GPT model architecture implementation
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.7 GPTæ¨¡å‹æ¶æ„å®ç°
- en: '[PRE32]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 The device setting will allow us to train the model on a CPU or GPU, depending
    on which device the input data sits on.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 è®¾å¤‡è®¾ç½®å°†å…è®¸æˆ‘ä»¬åœ¨CPUæˆ–GPUä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå…·ä½“å–å†³äºè¾“å…¥æ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚'
- en: Thanks to the `TransformerBlock` class, the `GPTModel` class is relatively small
    and compact.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šäºäº†`TransformerBlock`ç±»ï¼Œ`GPTModel`ç±»ç›¸å¯¹è¾ƒå°ä¸”ç´§å‡‘ã€‚
- en: The `__init__` constructor of this `GPTModel` class initializes the token and
    positional embedding layers using the configurations passed in via a Python dictionary,
    `cfg`. These embedding layers are responsible for converting input token indices
    into dense vectors and adding positional information (see chapter 2).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª`GPTModel`ç±»çš„`__init__`æ„é€ å‡½æ•°ä½¿ç”¨é€šè¿‡Pythonå­—å…¸`cfg`ä¼ å…¥çš„é…ç½®åˆå§‹åŒ–æ ‡è®°å’Œä½ç½®åµŒå…¥å±‚ã€‚è¿™äº›åµŒå…¥å±‚è´Ÿè´£å°†è¾“å…¥æ ‡è®°ç´¢å¼•è½¬æ¢ä¸ºå¯†é›†å‘é‡å¹¶æ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆè§ç¬¬2ç« ï¼‰ã€‚
- en: Next, the `__init__` method creates a sequential stack of `TransformerBlock`
    modules equal to the number of layers specified in `cfg`. Following the transformer
    blocks, a `LayerNorm` layer is applied, standardizing the outputs from the transformer
    blocks to stabilize the learning process. Finally, a linear output head without
    bias is defined, which projects the transformerâ€™s output into the vocabulary space
    of the tokenizer to generate logits for each token in the vocabulary.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œ`__init__`æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªç­‰äº`cfg`ä¸­æŒ‡å®šå±‚æ•°çš„`TransformerBlock`æ¨¡å—çš„é¡ºåºå †å ã€‚åœ¨å˜æ¢å™¨å—ä¹‹åï¼Œåº”ç”¨äº†ä¸€ä¸ª`LayerNorm`å±‚ï¼Œæ ‡å‡†åŒ–å˜æ¢å™¨å—çš„è¾“å‡ºä»¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ã€‚æœ€åï¼Œå®šä¹‰äº†ä¸€ä¸ªæ²¡æœ‰åç½®çš„çº¿æ€§è¾“å‡ºå¤´ï¼Œå®ƒå°†å˜æ¢å™¨çš„è¾“å‡ºæŠ•å½±åˆ°åˆ†è¯å™¨çš„è¯æ±‡ç©ºé—´ï¼Œä¸ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªæ ‡è®°ç”Ÿæˆlogitsã€‚
- en: The forward method takes a batch of input token indices, computes their embeddings,
    applies the positional embeddings, passes the sequence through the transformer
    blocks, normalizes the final output, and then computes the logits, representing
    the next tokenâ€™s unnormalized probabilities. We will convert these logits into
    tokens and text outputs in the next section.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å‘æ–¹æ³•æ¥æ”¶ä¸€ä¸ªè¾“å…¥æ ‡è®°ç´¢å¼•çš„æ‰¹æ¬¡ï¼Œè®¡ç®—å®ƒä»¬çš„åµŒå…¥ï¼Œåº”ç”¨ä½ç½®åµŒå…¥ï¼Œå°†åºåˆ—é€šè¿‡å˜æ¢å™¨å—ï¼Œå¯¹æœ€ç»ˆè¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼Œç„¶åè®¡ç®—logitsï¼Œè¡¨ç¤ºä¸‹ä¸€ä¸ªæ ‡è®°çš„éå½’ä¸€åŒ–æ¦‚ç‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚å°†è¿™äº›logitsè½¬æ¢ä¸ºæ ‡è®°å’Œæ–‡æœ¬è¾“å‡ºã€‚
- en: 'Letâ€™s now initialize the 124-million-parameter GPT model using the `GPT_CONFIG_
    124M` dictionary we pass into the `cfg` parameter and feed it with the batch text
    input we previously created:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¼ é€’ç»™`cfg`å‚æ•°çš„`GPT_CONFIG_ 124M`å­—å…¸åˆå§‹åŒ–124ç™¾ä¸‡å‚æ•°çš„GPTæ¨¡å‹ï¼Œå¹¶ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„æ‰¹æ–‡æœ¬è¾“å…¥è¿›è¡Œå–‚å…»ï¼š
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This code prints the contents of the input batch followed by the output tensor:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç æ‰“å°äº†è¾“å…¥æ‰¹æ¬¡çš„å†… å®¹ï¼Œç„¶åæ˜¯è¾“å‡ºå¼ é‡ï¼š
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#1 Token IDs of text 1'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 æ–‡æœ¬1çš„æ ‡è®°ID'
- en: '#2 Token IDs of text 2'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 æ–‡æœ¬2çš„æ ‡è®°ID'
- en: As we can see, the output tensor has the shape `[2,` `4,` `50257]`, since we
    passed in two input texts with four tokens each. The last dimension, `50257`,
    corresponds to the vocabulary size of the tokenizer. Later, we will see how to
    convert each of these 50,257-dimensional output vectors back into tokens.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ä¸º `[2,` `4,` `50257]`ï¼Œå› ä¸ºæˆ‘ä»¬è¾“å…¥äº†ä¸¤ä¸ªæ¯ä¸ªåŒ…å«å››ä¸ªæ ‡è®°çš„æ–‡æœ¬ã€‚æœ€åä¸€ä¸ªç»´åº¦ï¼Œ`50257`ï¼Œå¯¹åº”äºåˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°ã€‚ç¨åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†è¿™äº›50,257ç»´è¾“å‡ºå‘é‡ä¸­çš„æ¯ä¸€ä¸ªè½¬æ¢å›æ ‡è®°ã€‚
- en: 'Before we move on to coding the function that converts the model outputs into
    text, letâ€™s spend a bit more time with the model architecture itself and analyze
    its size. Using the `numel()` method, short for â€œnumber of elements,â€ we can collect
    the total number of parameters in the modelâ€™s parameter tensors:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç»§ç»­ç¼–å†™å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ–‡æœ¬çš„å‡½æ•°ä¹‹å‰ï¼Œè®©æˆ‘ä»¬èŠ±æ›´å¤šçš„æ—¶é—´æ¥åˆ†ææ¨¡å‹æ¶æ„æœ¬èº«çš„å¤§å°ã€‚ä½¿ç”¨`numel()`æ–¹æ³•ï¼Œå³â€œå…ƒç´ æ•°é‡â€ï¼Œæˆ‘ä»¬å¯ä»¥æ”¶é›†æ¨¡å‹å‚æ•°å¼ é‡ä¸­çš„æ€»å‚æ•°æ•°é‡ï¼š
- en: '[PRE35]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The result is
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, a curious reader might notice a discrepancy. Earlier, we spoke of initializing
    a 124-million-parameter GPT model, so why is the actual number of parameters 163
    million?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸€ä¸ªå¥½å¥‡çš„è¯»è€…å¯èƒ½ä¼šæ³¨æ„åˆ°ä¸€ä¸ªå·®å¼‚ã€‚ä¹‹å‰ï¼Œæˆ‘ä»¬æåˆ°åˆå§‹åŒ–äº†ä¸€ä¸ª124ç™¾ä¸‡å‚æ•°çš„GPTæ¨¡å‹ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆå®é™…çš„å‚æ•°æ•°é‡æ˜¯163ç™¾ä¸‡å‘¢ï¼Ÿ
- en: 'The reason is a concept called *weight tying*, which was used in the original
    GPT-2 architecture. It means that the original GPT-2 architecture reuses the weights
    from the token embedding layer in its output layer. To understand better, letâ€™s
    take a look at the shapes of the token embedding layer and linear output layer
    that we initialized on the `model` via the `GPTModel` earlier:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå› æ˜¯ä¸€ä¸ªç§°ä¸º*æƒé‡ç»‘å®š*çš„æ¦‚å¿µï¼Œå®ƒåœ¨åŸå§‹GPT-2æ¶æ„ä¸­ä½¿ç”¨ã€‚è¿™æ„å‘³ç€åŸå§‹GPT-2æ¶æ„åœ¨å…¶è¾“å‡ºå±‚ä¸­é‡æ–°ä½¿ç”¨äº†æ ‡è®°åµŒå…¥å±‚çš„æƒé‡ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬ä¹‹å‰é€šè¿‡`GPTModel`åˆå§‹åŒ–åœ¨`model`ä¸Šçš„æ ‡è®°åµŒå…¥å±‚å’Œçº¿æ€§è¾“å‡ºå±‚çš„å½¢çŠ¶ï¼š
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As we can see from the print outputs, the weight tensors for both these layers
    have the same shape:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä»æ‰“å°è¾“å‡ºä¸­å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸¤ä¸ªå±‚çš„æƒé‡å¼ é‡å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼š
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The token embedding and output layers are very large due to the number of rows
    for the 50,257 in the tokenizerâ€™s vocabulary. Letâ€™s remove the output layer parameter
    count from the total GPT-2 model count according to the weight tying:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåˆ†è¯å™¨è¯æ±‡è¡¨ä¸­æœ‰50,257ä¸ªè¡Œæ•°ï¼Œæ ‡è®°åµŒå…¥å’Œè¾“å‡ºå±‚éå¸¸å¤§ã€‚è®©æˆ‘ä»¬æ ¹æ®æƒé‡ç»‘å®šä»æ€»GPT-2æ¨¡å‹è®¡æ•°ä¸­å‡å»è¾“å‡ºå±‚å‚æ•°è®¡æ•°ï¼š
- en: '[PRE39]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The output is
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As we can see, the model is now only 124 million parameters large, matching
    the original size of the GPT-2 model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¯¥æ¨¡å‹ç°åœ¨åªæœ‰124ç™¾ä¸‡å‚æ•°å¤§ï¼Œä¸åŸå§‹çš„GPT-2æ¨¡å‹å¤§å°ç›¸åŒ¹é…ã€‚
- en: Weight tying reduces the overall memory footprint and computational complexity
    of the model. However, in my experience, using separate token embedding and output
    layers results in better training and model performance; hence, we use separate
    layers in our `GPTModel` implementation. The same is true for modern LLMs. However,
    we will revisit and implement the weight tying concept later in chapter 6 when
    we load the pretrained weights from OpenAI.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡ç»‘å®šå¯ä»¥å‡å°‘æ¨¡å‹çš„æ€»ä½“å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ã€‚ç„¶è€Œï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œä½¿ç”¨ç‹¬ç«‹çš„æ ‡è®°åµŒå…¥å±‚å’Œè¾“å‡ºå±‚å¯ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒå’Œæ¨¡å‹æ€§èƒ½ï¼›å› æ­¤ï¼Œæˆ‘ä»¬åœ¨`GPTModel`å®ç°ä¸­ä½¿ç”¨äº†ç‹¬ç«‹çš„å±‚ã€‚å¯¹äºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬6ç« ä¸­é‡æ–°å®¡è§†å¹¶å®ç°æƒé‡ç»‘å®šæ¦‚å¿µï¼Œé‚£æ—¶æˆ‘ä»¬å°†ä»OpenAIåŠ è½½é¢„è®­ç»ƒçš„æƒé‡ã€‚
- en: Exercise 4.1 Number of parameters in feed forward and attention modules
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 4.1 å‰é¦ˆå’Œæ³¨æ„åŠ›æ¨¡å—ä¸­çš„å‚æ•°æ•°é‡
- en: Calculate and compare the number of parameters that are contained in the feed
    forward module and those that are contained in the multi-head attention module.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å¹¶æ¯”è¾ƒåŒ…å«åœ¨å‰é¦ˆæ¨¡å—ä¸­çš„å‚æ•°æ•°é‡å’ŒåŒ…å«åœ¨å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ä¸­çš„å‚æ•°æ•°é‡ã€‚
- en: 'Lastly, letâ€™s compute the memory requirements of the 163 million parameters
    in our `GPTModel` object:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„`GPTModel`å¯¹è±¡ä¸­1.63äº¿ä¸ªå‚æ•°çš„å†…å­˜éœ€æ±‚ï¼š
- en: '[PRE41]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '#1 Calculates the total size in bytes (assuming float32, 4 bytes per parameter)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 è®¡ç®—æ€»å¤§å°ï¼ˆå‡è®¾ä¸ºfloat32ï¼Œæ¯ä¸ªå‚æ•°4å­—èŠ‚ï¼‰'
- en: '#2 Converts to megabytes'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 è½¬æ¢ä¸ºå…†å­—èŠ‚'
- en: The result is
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In conclusion, by calculating the memory requirements for the 163 million parameters
    in our `GPTModel` object and assuming each parameter is a 32-bit float taking
    up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating
    the relatively large storage capacity required to accommodate even relatively
    small LLMs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œé€šè¿‡è®¡ç®—æˆ‘ä»¬çš„`GPTModel`å¯¹è±¡ä¸­1.63äº¿ä¸ªå‚æ•°çš„å†…å­˜éœ€æ±‚ï¼Œå¹¶å‡è®¾æ¯ä¸ªå‚æ•°æ˜¯ä¸€ä¸ª32ä½çš„æµ®ç‚¹æ•°ï¼Œå ç”¨4å­—èŠ‚ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹çš„æ€»å¤§å°ä¸º621.83
    MBï¼Œè¿™è¯´æ˜äº†å³ä½¿æ˜¯ç›¸å¯¹è¾ƒå°çš„LLMsä¹Ÿéœ€è¦ç›¸å¯¹è¾ƒå¤§çš„å­˜å‚¨å®¹é‡ã€‚
- en: Now that weâ€™ve implemented the `GPTModel` architecture and saw that it outputs
    numeric tensors of shape `[batch_size,` `num_tokens,` `vocab_size]`, letâ€™s write
    the code to convert these output tensors into text.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†`GPTModel`æ¶æ„ï¼Œå¹¶çœ‹åˆ°å®ƒè¾“å‡ºå½¢çŠ¶ä¸º`[batch_size,` `num_tokens,` `vocab_size]`çš„æ•°å€¼å¼ é‡ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä»£ç å°†è¿™äº›è¾“å‡ºå¼ é‡è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
- en: Exercise 4.2 Initializing larger GPT models
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 4.2 åˆå§‹åŒ–æ›´å¤§çš„GPTæ¨¡å‹
- en: We initialized a 124-million-parameter GPT model, which is known as â€œGPT-2 small.â€
    Without making any code modifications besides updating the configuration file,
    use the `GPTModel` class to implement GPT-2 medium (using 1,024-dimensional embeddings,
    24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-dimensional
    embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL
    (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention
    heads). As a bonus, calculate the total number of parameters in each GPT model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆå§‹åŒ–äº†ä¸€ä¸ªå‚æ•°æ•°é‡ä¸º1.24äº¿çš„GPTæ¨¡å‹ï¼Œè¢«ç§°ä¸ºâ€œGPT-2 smallâ€ã€‚é™¤äº†æ›´æ–°é…ç½®æ–‡ä»¶å¤–ï¼Œä¸è¿›è¡Œä»»ä½•ä»£ç ä¿®æ”¹ï¼Œä½¿ç”¨`GPTModel`ç±»å®ç°GPT-2
    mediumï¼ˆä½¿ç”¨1,024ç»´åµŒå…¥ï¼Œ24ä¸ªtransformerå—ï¼Œ16ä¸ªå¤šå¤´æ³¨æ„åŠ›å¤´ï¼‰ï¼ŒGPT-2 largeï¼ˆ1,280ç»´åµŒå…¥ï¼Œ36ä¸ªtransformerå—ï¼Œ20ä¸ªå¤šå¤´æ³¨æ„åŠ›å¤´ï¼‰ï¼Œä»¥åŠGPT-2
    XLï¼ˆ1,600ç»´åµŒå…¥ï¼Œ48ä¸ªtransformerå—ï¼Œ25ä¸ªå¤šå¤´æ³¨æ„åŠ›å¤´ï¼‰ã€‚ä½œä¸ºé¢å¤–å¥–åŠ±ï¼Œè®¡ç®—æ¯ä¸ªGPTæ¨¡å‹ä¸­çš„å‚æ•°æ€»æ•°ã€‚
- en: 4.7 Generating text
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 ç”Ÿæˆæ–‡æœ¬
- en: We will now implement the code that converts the tensor outputs of the GPT model
    back into text. Before we get started, letâ€™s briefly review how a generative model
    like an LLM generates text one word (or token) at a time.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†å®ç°å°†GPTæ¨¡å‹çš„å¼ é‡è¾“å‡ºè½¬æ¢å›æ–‡æœ¬çš„ä»£ç ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç®€è¦å›é¡¾ä¸€ä¸‹åƒLLMè¿™æ ·çš„ç”Ÿæˆæ¨¡å‹æ˜¯å¦‚ä½•é€ä¸ªå•è¯ï¼ˆæˆ–æ ‡è®°ï¼‰ç”Ÿæˆæ–‡æœ¬çš„ã€‚
- en: '![figure](../Images/4-16.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-16.png)'
- en: Figure 4.16 The step-by-step process by which an LLM generates text, one token
    at a time. Starting with an initial input context (â€œHello, I amâ€), the model predicts
    a subsequent token during each iteration, appending it to the input context for
    the next round of prediction. As shown, the first iteration adds â€œa,â€ the second
    â€œmodel,â€ and the third â€œready,â€ progressively building the sentence.
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.16 LLMé€ä¸ªæ ‡è®°ç”Ÿæˆæ–‡æœ¬çš„é€æ­¥è¿‡ç¨‹ã€‚ä»åˆå§‹è¾“å…¥ä¸Šä¸‹æ–‡ï¼ˆâ€œä½ å¥½ï¼Œæˆ‘æ˜¯â€ï¼‰å¼€å§‹ï¼Œæ¨¡å‹åœ¨æ¯æ¬¡è¿­ä»£ä¸­é¢„æµ‹åç»­æ ‡è®°ï¼Œå¹¶å°†å…¶é™„åŠ åˆ°ä¸‹ä¸€æ¬¡é¢„æµ‹çš„è¾“å…¥ä¸Šä¸‹æ–‡ä¸­ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œç¬¬ä¸€æ¬¡è¿­ä»£æ·»åŠ äº†â€œaâ€ï¼Œç¬¬äºŒæ¬¡æ·»åŠ äº†â€œmodelâ€ï¼Œç¬¬ä¸‰æ¬¡æ·»åŠ äº†â€œreadyâ€ï¼Œé€æ­¥æ„å»ºå¥å­ã€‚
- en: 'Figure 4.16 illustrates the step-by-step process by which a GPT model generates
    text given an input context, such as â€œHello, I am.â€ With each iteration, the input
    context grows, allowing the model to generate coherent and contextually appropriate
    text. By the sixth iteration, the model has constructed a complete sentence: â€œHello,
    I am a model ready to help.â€ Weâ€™ve seen that our current `GPTModel` implementation
    outputs tensors with shape `[batch_size,` `num_token,` `vocab_size]`. Now the
    question is: How does a GPT model go from these output tensors to the generated
    text?'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.16 è¯´æ˜äº† GPT æ¨¡å‹æ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚â€œä½ å¥½ï¼Œæˆ‘æ˜¯ã€‚â€ï¼‰ç”Ÿæˆæ–‡æœ¬çš„é€æ­¥è¿‡ç¨‹ã€‚æ¯æ¬¡è¿­ä»£ï¼Œè¾“å…¥ä¸Šä¸‹æ–‡éƒ½ä¼šå¢é•¿ï¼Œå…è®¸æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡é€‚å½“çš„æ–‡æœ¬ã€‚åˆ°ç¬¬å…­æ¬¡è¿­ä»£æ—¶ï¼Œæ¨¡å‹å·²ç»æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼šâ€œä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªå‡†å¤‡å¸®åŠ©çš„æ¨¡å‹ã€‚â€æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œæˆ‘ä»¬å½“å‰çš„
    `GPTModel` å®ç°è¾“å‡ºå½¢çŠ¶ä¸º `[batch_size,` `num_token,` `vocab_size]` çš„å¼ é‡ã€‚ç°åœ¨çš„é—®é¢˜æ˜¯ï¼šGPT æ¨¡å‹æ˜¯å¦‚ä½•ä»è¿™äº›è¾“å‡ºå¼ é‡ç”Ÿæˆæ–‡æœ¬çš„ï¼Ÿ
- en: The process by which a GPT model goes from output tensors to generated text
    involves several steps, as illustrated in figure 4.17\. These steps include decoding
    the output tensors, selecting tokens based on a probability distribution, and
    converting these tokens into human-readable text.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: GPT æ¨¡å‹ä»è¾“å‡ºå¼ é‡åˆ°ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹æ¶‰åŠå‡ ä¸ªæ­¥éª¤ï¼Œå¦‚å›¾ 4.17 æ‰€ç¤ºã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬è§£ç è¾“å‡ºå¼ é‡ã€æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©æ ‡è®°ï¼Œå¹¶å°†è¿™äº›æ ‡è®°è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬ã€‚
- en: '![figure](../Images/4-17.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-17.png)'
- en: Figure 4.17 The mechanics of text generation in a GPT model by showing a single
    iteration in the token generation process. The process begins by encoding the
    input text into token IDs, which are then fed into the GPT model. The outputs
    of the model are then converted back into text and appended to the original input
    text.
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾ 4.17 é€šè¿‡å±•ç¤ºæ ‡è®°ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å•ä¸ªè¿­ä»£ï¼Œå±•ç¤ºäº† GPT æ¨¡å‹ä¸­æ–‡æœ¬ç”Ÿæˆçš„æœºåˆ¶ã€‚è¿™ä¸ªè¿‡ç¨‹é¦–å…ˆå°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ ‡è®° IDï¼Œç„¶åå°†è¿™äº› ID è¾“å…¥åˆ°
    GPT æ¨¡å‹ä¸­ã€‚æ¨¡å‹çš„è¾“å‡ºéšåè¢«è½¬æ¢å›æ–‡æœ¬ï¼Œå¹¶é™„åŠ åˆ°åŸå§‹è¾“å…¥æ–‡æœ¬ä¸Šã€‚
- en: The next-token generation process detailed in figure 4.17 illustrates a single
    step where the GPT model generates the next token given its input. In each step,
    the model outputs a matrix with vectors representing potential next tokens. The
    vector corresponding to the next token is extracted and converted into a probability
    distribution via the `softmax` function. Within the vector containing the resulting
    probability scores, the index of the highest value is located, which translates
    to the token ID. This token ID is then decoded back into text, producing the next
    token in the sequence. Finally, this token is appended to the previous inputs,
    forming a new input sequence for the subsequent iteration. This step-by-step process
    enables the model to generate text sequentially, building coherent phrases and
    sentences from the initial input context.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.17 ä¸­è¯¦ç»†è¯´æ˜äº†ä¸‹ä¸€æ ‡è®°ç”Ÿæˆè¿‡ç¨‹ï¼Œå±•ç¤ºäº† GPT æ¨¡å‹æ ¹æ®å…¶è¾“å…¥ç”Ÿæˆä¸‹ä¸€ä¸ªæ ‡è®°çš„å•ä¸ªæ­¥éª¤ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæ¨¡å‹è¾“å‡ºä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­åŒ…å«è¡¨ç¤ºæ½œåœ¨ä¸‹ä¸€ä¸ªæ ‡è®°çš„å‘é‡ã€‚ä¸ä¸‹ä¸€ä¸ªæ ‡è®°å¯¹åº”çš„å‘é‡è¢«æå–å‡ºæ¥ï¼Œå¹¶é€šè¿‡
    `softmax` å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚åœ¨åŒ…å«ç»“æœæ¦‚ç‡åˆ†æ•°çš„å‘é‡ä¸­ï¼Œæ‰¾åˆ°æœ€é«˜å€¼çš„ç´¢å¼•ï¼Œè¿™å¯¹åº”äºæ ‡è®° IDã€‚ç„¶åï¼Œè¿™ä¸ªæ ‡è®° ID è¢«è§£ç å›æ–‡æœ¬ï¼Œç”Ÿæˆåºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æœ€åï¼Œè¿™ä¸ªæ ‡è®°è¢«é™„åŠ åˆ°ä¹‹å‰çš„è¾“å…¥ä¸Šï¼Œå½¢æˆæ–°çš„è¾“å…¥åºåˆ—ï¼Œç”¨äºåç»­è¿­ä»£ã€‚è¿™ä¸ªé€æ­¥è¿‡ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿé¡ºåºç”Ÿæˆæ–‡æœ¬ï¼Œä»åˆå§‹è¾“å…¥ä¸Šä¸‹æ–‡æ„å»ºè¿è´¯çš„çŸ­è¯­å’Œå¥å­ã€‚
- en: In practice, we repeat this process over many iterations, such as shown in figure
    4.16, until we reach a user-specified number of generated tokens. In code, we
    can implement the token-generation process as shown in the following listing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹è®¸å¤šæ¬¡è¿­ä»£ï¼Œä¾‹å¦‚å›¾ 4.16 æ‰€ç¤ºï¼Œç›´åˆ°è¾¾åˆ°ç”¨æˆ·æŒ‡å®šçš„ç”Ÿæˆæ ‡è®°æ•°ã€‚åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‡è®°ç”Ÿæˆè¿‡ç¨‹å®ç°å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: Listing 4.8 A function for the GPT model to generate text
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 4.8 GPT æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„å‡½æ•°
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#1 idx is a (batch, n_tokens) array of indices in the current context.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 idx æ˜¯å½“å‰ä¸Šä¸‹æ–‡ä¸­ç´¢å¼•çš„ (batch, n_tokens) æ•°ç»„ã€‚'
- en: '#2 Crops current context if it exceeds the supported context size, e.g., if
    LLM supports only 5 tokens, and the context size is 10, then only the last 5 tokens
    are used as context'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 å¦‚æœå½“å‰ä¸Šä¸‹æ–‡è¶…è¿‡æ”¯æŒçš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œåˆ™è£å‰ªå½“å‰ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ LLM åªæ”¯æŒ 5 ä¸ªæ ‡è®°ï¼Œè€Œä¸Šä¸‹æ–‡å¤§å°ä¸º 10ï¼Œåˆ™åªä½¿ç”¨æœ€å 5 ä¸ªæ ‡è®°ä½œä¸ºä¸Šä¸‹æ–‡'
- en: '#3 Focuses only on the last time step, so that (batch, n_token, vocab_size)
    becomes (batch, vocab_size)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 ä»…å…³æ³¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå› æ­¤ (batch, n_token, vocab_size) å˜ä¸º (batch, vocab_size)'
- en: '#4 probas has shape (batch, vocab_size).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 probas çš„å½¢çŠ¶ä¸º (batch, vocab_size)ã€‚'
- en: '#5 idx_next has shape (batch, 1).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 idx_next çš„å½¢çŠ¶ä¸º (batch, 1)ã€‚'
- en: '#6 Appends sampled index to the running sequence, where idx has shape (batch,
    n_tokens+1)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 å°†é‡‡æ ·ç´¢å¼•æ·»åŠ åˆ°è¿è¡Œåºåˆ—ä¸­ï¼Œå…¶ä¸­ idx çš„å½¢çŠ¶ä¸º (batch, n_tokens+1)'
- en: This code demonstrates a simple implementation of a generative loop for a language
    model using PyTorch. It iterates for a specified number of new tokens to be generated,
    crops the current context to fit the modelâ€™s maximum context size, computes predictions,
    and then selects the next token based on the highest probability prediction.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ä»£ç æ¼”ç¤ºäº†ä½¿ç”¨PyTorchå®ç°è¯­è¨€æ¨¡å‹ç”Ÿæˆå¾ªç¯çš„ç®€å•å®ç°ã€‚å®ƒè¿­ä»£æŒ‡å®šæ•°é‡çš„æ–°tokenä»¥ç”Ÿæˆï¼Œè£å‰ªå½“å‰ä¸Šä¸‹æ–‡ä»¥é€‚åº”æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ï¼Œè®¡ç®—é¢„æµ‹ï¼Œç„¶åæ ¹æ®æœ€é«˜æ¦‚ç‡é¢„æµ‹é€‰æ‹©ä¸‹ä¸€ä¸ªtokenã€‚
- en: To code the `generate_text_simple` function, we use a `softmax` function to
    convert the logits into a probability distribution from which we identify the
    position with the highest value via `torch.argmax`. The `softmax` function is
    monotonic, meaning it preserves the order of its inputs when transformed into
    outputs. So, in practice, the softmax step is redundant since the position with
    the highest score in the softmax output tensor is the same position in the logit
    tensor. In other words, we could apply the `torch.argmax` function to the logits
    tensor directly and get identical results. However, I provide the code for the
    conversion to illustrate the full process of transforming logits to probabilities,
    which can add additional intuition so that the model generates the most likely
    next token, which is known as *greedy decoding*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¼–å†™`generate_text_simple`å‡½æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨`softmax`å‡½æ•°å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åé€šè¿‡`torch.argmax`è¯†åˆ«å…·æœ‰æœ€é«˜å€¼çš„ä½ã€‚`softmax`å‡½æ•°æ˜¯å•è°ƒçš„ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨è½¬æ¢ä¸ºè¾“å‡ºæ—¶ä¿ç•™äº†è¾“å…¥çš„é¡ºåºã€‚å› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œsoftmaxæ­¥éª¤æ˜¯å¤šä½™çš„ï¼Œå› ä¸ºsoftmaxè¾“å‡ºå¼ é‡ä¸­å¾—åˆ†æœ€é«˜çš„ä½ç½®ä¸logitå¼ é‡ä¸­çš„ç›¸åŒä½ç½®ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¯¹logitså¼ é‡åº”ç”¨`torch.argmax`å‡½æ•°å¹¶å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘æä¾›äº†è½¬æ¢çš„ä»£ç ï¼Œä»¥è¯´æ˜å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡çš„å®Œæ•´è¿‡ç¨‹ï¼Œè¿™å¯ä»¥å¢åŠ é¢å¤–çš„ç›´è§‚æ€§ï¼Œä»¥ä¾¿æ¨¡å‹ç”Ÿæˆæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtokenï¼Œè¿™è¢«ç§°ä¸º*è´ªå©ªè§£ç *ã€‚
- en: When we implement the GPT training code in the next chapter, we will use additional
    sampling techniques to modify the softmax outputs such that the model doesnâ€™t
    always select the most likely token. This introduces variability and creativity
    in the generated text.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åœ¨ä¸‹ä¸€ç« å®ç°GPTè®­ç»ƒä»£ç æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¢å¤–çš„é‡‡æ ·æŠ€æœ¯æ¥ä¿®æ”¹softmaxè¾“å‡ºï¼Œä½¿å¾—æ¨¡å‹ä¸æ€»æ˜¯é€‰æ‹©æœ€å¯èƒ½çš„tokenã€‚è¿™ä¸ºç”Ÿæˆçš„æ–‡æœ¬å¼•å…¥äº†å˜åŒ–æ€§å’Œåˆ›é€ æ€§ã€‚
- en: This process of generating one token ID at a time and appending it to the context
    using the `generate_text_simple` function is further illustrated in figure 4.18\.
    (The token ID generation process for each iteration is detailed in figure 4.17.)
    We generate the token IDs in an iterative fashion. For instance, in iteration
    1, the model is provided with the tokens corresponding to â€œHello, I am,â€ predicts
    the next token (with ID 257, which is â€œaâ€), and appends it to the input. This
    process is repeated until the model produces the complete sentence â€œHello, I am
    a model ready to helpâ€ after six iterations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`generate_text_simple`å‡½æ•°é€ä¸ªç”Ÿæˆtoken IDå¹¶å°†å…¶é™„åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­çš„è¿™ä¸ªè¿‡ç¨‹åœ¨å›¾4.18ä¸­è¿›ä¸€æ­¥è¯´æ˜ã€‚ï¼ˆæ¯ä¸ªè¿­ä»£çš„token
    IDç”Ÿæˆè¿‡ç¨‹åœ¨å›¾4.17ä¸­è¯¦ç»†è¯´æ˜ã€‚ï¼‰æˆ‘ä»¬ä»¥è¿­ä»£çš„æ–¹å¼ç”Ÿæˆtoken IDã€‚ä¾‹å¦‚ï¼Œåœ¨è¿­ä»£1ä¸­ï¼Œæ¨¡å‹è¢«æä¾›äº†å¯¹åº”äºâ€œHello, I am,â€çš„tokenï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆIDä¸º257ï¼Œå³â€œaâ€ï¼‰ï¼Œå¹¶å°†å…¶é™„åŠ åˆ°è¾“å…¥ä¸­ã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°æ¨¡å‹åœ¨å…­æ¬¡è¿­ä»£åç”Ÿæˆå®Œæ•´çš„å¥å­â€œHello,
    I am a model ready to helpâ€ã€‚
- en: '![figure](../Images/4-18.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-18.png)'
- en: Figure 4.18 The six iterations of a token prediction cycle, where the model
    takes a sequence of initial token IDs as input, predicts the next token, and appends
    this token to the input sequence for the next iteration. (The token IDs are also
    translated into their corresponding text for better understanding.)
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å›¾4.18 tokené¢„æµ‹å‘¨æœŸçš„å…­æ¬¡è¿­ä»£ï¼Œå…¶ä¸­æ¨¡å‹ä»¥ä¸€ç³»åˆ—åˆå§‹token IDä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œå¹¶å°†æ­¤tokené™„åŠ åˆ°ä¸‹ä¸€ä¸ªè¿­ä»£çš„è¾“å…¥åºåˆ—ä¸­ã€‚ï¼ˆtoken
    IDä¹Ÿè¢«è½¬æ¢æˆç›¸åº”çš„æ–‡æœ¬ä»¥æ›´å¥½åœ°ç†è§£ã€‚ï¼‰
- en: 'Letâ€™s now try out the `generate_text_simple` function with the `"Hello,` `I`
    `am"` context as model input. First, we encode the input context into token IDs:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•ä½¿ç”¨`"Hello,` `I` `am"`ä¸Šä¸‹æ–‡ä½œä¸ºæ¨¡å‹è¾“å…¥çš„`generate_text_simple`å‡½æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è¾“å…¥ä¸Šä¸‹æ–‡ç¼–ç ä¸ºtoken
    IDï¼š
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '#1 Adds batch dimension'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 æ·»åŠ æ‰¹å¤„ç†ç»´åº¦'
- en: The encoded IDs are
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç çš„IDæ˜¯
- en: '[PRE45]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we put the model into `.eval()` mode. This disables random components
    like dropout, which are only used during training, and use the `generate_text_simple`
    function on the encoded input tensor:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹ç½®äº`.eval()`æ¨¡å¼ã€‚è¿™ç¦ç”¨äº†ä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„éšæœºç»„ä»¶ï¼Œå¦‚dropoutï¼Œå¹¶ä½¿ç”¨`generate_text_simple`å‡½æ•°å¯¹ç¼–ç çš„è¾“å…¥å¼ é‡è¿›è¡Œæ“ä½œï¼š
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '#1 Disables dropout since we are not training the model'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 ç¦ç”¨dropoutï¼Œå› ä¸ºæˆ‘ä»¬ä¸åœ¨è®­ç»ƒæ¨¡å‹'
- en: The resulting output token IDs are
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„è¾“å‡ºtoken IDæ˜¯
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Using the `.decode` method of the tokenizer, we can convert the IDs back into
    text:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†è¯å™¨çš„`.decode`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å°†IDè½¬æ¢å›æ–‡æœ¬ï¼š
- en: '[PRE48]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The model output in text format is
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬æ ¼å¼æ˜¯
- en: '[PRE49]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As we can see, the model generated gibberish, which is not at all like the coherent
    text `Hello,` `I` `am` `a` `model` `ready` `to` `help`. What happened? The reason
    the model is unable to produce coherent text is that we havenâ€™t trained it yet.
    So far, we have only implemented the GPT architecture and initialized a GPT model
    instance with initial random weights. Model training is a large topic in itself,
    and we will tackle it in the next chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹ç”Ÿæˆäº†ä¹±ç ï¼Œè¿™ä¸è¿è´¯çš„æ–‡æœ¬â€œHello, I am a model ready to helpâ€å®Œå…¨ä¸åŒã€‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿæ¨¡å‹æ— æ³•ç”Ÿæˆè¿è´¯æ–‡æœ¬çš„åŸå› æ˜¯æˆ‘ä»¬è¿˜æ²¡æœ‰å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªå®ç°äº†GPTæ¶æ„ï¼Œå¹¶ä½¿ç”¨åˆå§‹éšæœºæƒé‡åˆå§‹åŒ–äº†ä¸€ä¸ªGPTæ¨¡å‹å®ä¾‹ã€‚æ¨¡å‹è®­ç»ƒæœ¬èº«æ˜¯ä¸€ä¸ªå¤§ä¸»é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­æ¢è®¨å®ƒã€‚
- en: Exercise 4.3 Using separate dropout parameters
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 4.3 ä½¿ç”¨å•ç‹¬çš„dropoutå‚æ•°
- en: 'At the beginning of this chapter, we defined a global `drop_rate` setting in
    the `GPT_ CONFIG_124M` dictionary to set the dropout rate in various places throughout
    the `GPTModel` architecture. Change the code to specify a separate dropout value
    for the various dropout layers throughout the model architecture. (Hint: there
    are three distinct places where we used dropout layers: the embedding layer, shortcut
    layer, and multi-head attention module.)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„å¼€å¤´ï¼Œæˆ‘ä»¬åœ¨`GPT_ CONFIG_124M`å­—å…¸ä¸­å®šä¹‰äº†ä¸€ä¸ªå…¨å±€`drop_rate`è®¾ç½®ï¼Œä»¥åœ¨`GPTModel`æ¶æ„çš„å„ä¸ªåœ°æ–¹è®¾ç½®dropoutç‡ã€‚å°†ä»£ç ä¿®æ”¹ä¸ºä¸ºæ¨¡å‹æ¶æ„ä¸­çš„å„ä¸ªdropoutå±‚æŒ‡å®šå•ç‹¬çš„dropoutå€¼ã€‚ï¼ˆæç¤ºï¼šæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„åœ°æ–¹ä½¿ç”¨äº†dropoutå±‚ï¼šåµŒå…¥å±‚ã€å¿«æ·å±‚å’Œå¤šå¤´æ³¨æ„åŠ›æ¨¡å—ã€‚ï¼‰
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Layer normalization stabilizes training by ensuring that each layerâ€™s outputs
    have a consistent mean and variance.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–é€šè¿‡ç¡®ä¿æ¯ä¸€å±‚çš„è¾“å‡ºå…·æœ‰ä¸€è‡´çš„å‡å€¼å’Œæ–¹å·®æ¥ç¨³å®šè®­ç»ƒã€‚
- en: Shortcut connections are connections that skip one or more layers by feeding
    the output of one layer directly to a deeper layer, which helps mitigate the vanishing
    gradient problem when training deep neural networks, such as LLMs.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¿«æ·è¿æ¥æ˜¯é€šè¿‡å°†ä¸€å±‚æˆ–å¤šå±‚çš„è¾“å‡ºç›´æ¥é¦ˆé€åˆ°æ›´æ·±çš„ä¸€å±‚æ¥è·³è¿‡ä¸€å±‚æˆ–æ›´å¤šå±‚çš„è¿æ¥ï¼Œè¿™æœ‰åŠ©äºç¼“è§£è®­ç»ƒæ·±å±‚ç¥ç»ç½‘ç»œï¼ˆå¦‚LLMï¼‰æ—¶çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- en: Transformer blocks are a core structural component of GPT models, combining
    masked multi-head attention modules with fully connected feed forward networks
    that use the GELU activation function.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformerå—æ˜¯GPTæ¨¡å‹çš„æ ¸å¿ƒç»“æ„ç»„ä»¶ï¼Œå®ƒç»“åˆäº†å¸¦æ©ç çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—å’Œä½¿ç”¨äº†GELUæ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å‰é¦ˆç½‘ç»œã€‚
- en: GPT models are LLMs with many repeated transformer blocks that have millions
    to billions of parameters.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPTæ¨¡å‹æ˜¯å…·æœ‰æ•°ç™¾ä¸‡åˆ°æ•°åäº¿å‚æ•°çš„å…·æœ‰è®¸å¤šé‡å¤transformerå—çš„LLMã€‚
- en: GPT models come in various sizes, for example, 124, 345, 762, and 1,542 million
    parameters, which we can implement with the same `GPTModel` Python class.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPTæ¨¡å‹æœ‰å¤šç§å¤§å°ï¼Œä¾‹å¦‚124ã€345ã€762å’Œ15.42äº¿å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„`GPTModel` Pythonç±»æ¥å®ç°ã€‚
- en: The text-generation capability of a GPT-like LLM involves decoding output tensors
    into human-readable text by sequentially predicting one token at a time based
    on a given input context.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»ä¼¼GPTçš„LLMçš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›æ¶‰åŠé€šè¿‡æŒ‰é¡ºåºé¢„æµ‹ç»™å®šè¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„ä¸€ä¸ªæ ‡è®°æ¥è§£ç è¾“å‡ºå¼ é‡ï¼Œä»è€Œå°†äººç±»å¯è¯»çš„æ–‡æœ¬è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
- en: Without training, a GPT model generates incoherent text, which underscores the
    importance of model training for coherent text generation.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰è®­ç»ƒï¼ŒGPTæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯ä¸è¿è´¯çš„ï¼Œè¿™å¼ºè°ƒäº†æ¨¡å‹è®­ç»ƒå¯¹äºç”Ÿæˆè¿è´¯æ–‡æœ¬çš„é‡è¦æ€§ã€‚
