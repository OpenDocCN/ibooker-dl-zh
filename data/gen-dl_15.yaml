- en: Chapter 11\. Music Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 音乐生成
- en: Musical composition is a complex and creative process that involves combining
    different musical elements such as melody, harmony, rhythm, and timbre. While
    this is traditionally seen as a uniquely human activity, recent advancements have
    made it possible to generate music that both is pleasing to the ear and has long-term
    structure.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐作曲是一个复杂而创造性的过程，涉及将不同的音乐元素（如旋律、和声、节奏和音色）结合在一起。虽然传统上认为这是一种独特的人类活动，但最近的进展使得生成既能让耳朵愉悦又具有长期结构的音乐成为可能。
- en: One of the most popular techniques for music generation is the Transformer,
    as music can be thought of as a sequence prediction problem. These models have
    been adapted to generate music by treating musical notes as a sequence of tokens,
    similar to words in a sentence. The Transformer model learns to predict the next
    note in the sequence based on the previous notes, resulting in a generated piece
    of music.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成最流行的技术之一是Transformer，因为音乐可以被视为一个序列预测问题。这些模型已经被调整为通过将音符视为一系列标记（类似于句子中的单词）来生成音乐。Transformer模型学会根据先前的音符预测序列中的下一个音符，从而生成一段音乐。
- en: MuseGAN takes a totally different approach to generating music. Unlike Transformers,
    which generate music note by note, MuseGAN generates entire musical tracks at
    once by treating music as an *image*, consisting of a pitch axis and a time axis.
    Moreover, MuseGAN separates out different musical components such as chords, style,
    melody, and groove so that they can be controlled independently.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN采用了一种完全不同的方法来生成音乐。与Transformer逐音符生成音乐不同，MuseGAN通过将音乐视为一个*图像*，由音高轴和时间轴组成，一次生成整个音乐曲目。此外，MuseGAN将不同的音乐组成部分（如和弦、风格、旋律和节奏）分开，以便可以独立控制它们。
- en: In this chapter we will learn how to process music data and apply both a Transformer
    and MuseGAN to generate music that is stylistically similar to a given training
    set.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何处理音乐数据，并应用Transformer和MuseGAN来生成与给定训练集风格相似的音乐。
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: For a machine to compose music that is pleasing to our ear, it must master many
    of the same technical challenges that we saw in [Chapter 9](ch09.xhtml#chapter_transformer)
    in relation to text. In particular, our model must be able to learn from and re-create
    the sequential structure of music and be able to choose from a discrete set of
    possibilities for subsequent notes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让机器创作出我们耳朵愉悦的音乐，它必须掌握我们在第9章中看到的与文本相关的许多技术挑战。特别是，我们的模型必须能够学习并重新创建音乐的顺序结构，并能够从一系列可能性中选择后续音符。
- en: However, music generation presents additional challenges that are not present
    for text generation, namely pitch and rhythm. Music is often polyphonic—that is,
    there are several streams of notes played simultaneously on different instruments,
    which combine to create harmonies that are either dissonant (clashing) or consonant
    (harmonious). Text generation only requires us to handle a single stream of text,
    in contrast to the parallel streams of chords that are present in music.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，音乐生成面临着文本生成所没有的额外挑战，即音高和节奏。音乐通常是复调的，即有几条音符流同时在不同乐器上演奏，这些音符组合在一起形成既不和谐（冲突）又和谐（和谐）的和声。文本生成只需要我们处理一条文本流，与音乐中存在的并行和弦流相比。
- en: Also, text generation can be handled one word at a time. Unlike text data, music
    is a multipart, interwoven tapestry of sounds that are not necessarily delivered
    at the same time—much of the interest that stems from listening to music is in
    the interplay between different rhythms across the ensemble. For example, a guitarist
    might play a flurry of quicker notes while the pianist holds a longer sustained
    chord. Therefore, generating music note by note is complex, because we often do
    not want all the instruments to change notes simultaneously.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文本生成可以逐字处理。与文本数据不同，音乐是一种多部分、交织在一起的声音织锦，不一定同时传递——听音乐的乐趣很大程度上来自于整个合奏中不同节奏之间的相互作用。例如，吉他手可能弹奏一连串更快的音符，而钢琴家则弹奏一个较长的持续和弦。因此，逐音符生成音乐是复杂的，因为我们通常不希望所有乐器同时改变音符。
- en: We will start this chapter by simplifying the problem to focus on music generation
    for a single (monophonic) line of music. Many of the techniques from [Chapter 9](ch09.xhtml#chapter_transformer)
    for text generation can also be used for music generation, as the two tasks share
    many common themes. We will start by training a Transformer to generate music
    in the style of the J.S. Bach cello suites and see how the attention mechanism
    allows the model to focus on previous notes in order to determine the most natural
    subsequent note. We’ll then tackle the task of polyphonic music generation and
    explore how we can deploy an architecture based around GANs to create music for
    multiple voices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简化问题开始，专注于为单一（单声部）音乐线生成音乐。许多来自第9章关于文本生成的技术也可以用于音乐生成，因为这两个任务有许多共同的主题。我们将首先训练一个Transformer来生成类似于J.S.巴赫大提琴组曲风格的音乐，并看看注意机制如何使模型能够专注于先前的音符，以确定最自然的后续音符。然后，我们将处理复调音乐生成的任务，并探讨如何使用基于GAN的架构来为多声部创作音乐。
- en: Transformers for Music Generation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于音乐生成的Transformer
- en: The model we will be building here is a decoder Transformer, taking inspiration
    from OpenAI’s [*MuseNet*](https://oreil.ly/OaCDY), which also utilizes a decoder
    Transformer (similar to GPT-3) trained to predict the next note given a sequence
    of previous notes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里构建的模型是一个解码器Transformer，灵感来自于OpenAI的*MuseNet*，它也利用了一个解码器Transformer（类似于GPT-3），训练以预测给定一系列先前音符的下一个音符。
- en: In music generation tasks, the length of the sequence <math alttext="upper N"><mi>N</mi></math>
    grows large as the music progresses, and this means that the <math alttext="upper
    N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math> attention
    matrix for each head becomes expensive to store and compute. We ideally do not
    want to clip the input sequence to a short number of tokens, as we would like
    the model to construct the piece around a long-term structure and repeat motifs
    and phrases from several minutes ago, as a human composer would.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在音乐生成任务中，随着音乐的进行，序列的长度<math alttext="upper N"><mi>N</mi></math>变得很大，这意味着每个头部的<math
    alttext="upper N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>注意力矩阵变得昂贵且难以存储和计算。我们理想情况下不希望将输入序列剪切为少量标记，因为我们希望模型围绕长期结构构建乐曲，并重复几分钟前的主题和乐句，就像人类作曲家一样。
- en: To tackle this problem, MuseNet utilizes a form of Transformer known as a [*Sparse
    Transformer*](https://oreil.ly/euQiL). Each output position in the attention matrix
    only computes weights for a subset of input positions, thereby reducing the computational
    complexity and memory required to train the model. MuseNet can therefore operate
    with full attention over 4,096 tokens and can learn long-term structure and melodic
    structure across a range of styles. (See, for example, OpenAI’s [Chopin](https://oreil.ly/cmwsO)
    and [Mozart](https://oreil.ly/-T-Je) recordings on SoundCloud.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，MuseNet利用了一种称为[*Sparse Transformer*](https://oreil.ly/euQiL)的Transformer形式。注意矩阵中的每个输出位置仅计算一部分输入位置的权重，从而减少了训练模型所需的计算复杂性和内存。MuseNet因此可以在4,096个标记上进行全注意力操作，并可以学习跨多种风格的长期结构和旋律结构。
    （例如，查看OpenAI在SoundCloud上的[肖邦](https://oreil.ly/cmwsO)和[莫扎特](https://oreil.ly/-T-Je)的录音。）
- en: To see how the continuation of a musical phrase is often influenced by notes
    from several bars ago, take a look at the opening bars of the Prelude to Bach’s
    Cello Suite No. 1 ([Figure 11-1](#bach_cello_prelude1)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到音乐短语的延续通常受几个小节前的音符影响，看看巴赫大提琴组曲第1号前奏的开头小节吧（[图11-1](#bach_cello_prelude1)）。
- en: '![](Images/gdl2_1101.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1101.png)'
- en: Figure 11-1\. The opening of Bach’s Cello Suite No. 1 (Prelude)
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1。巴赫的大提琴组曲第1号（前奏）
- en: Bars
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小节
- en: '*Bars* (or *measures*) are small units of music that contain a fixed, small
    number of beats and are marked out by vertical lines that cross the staff. If
    you can count 1, 2, 1, 2 along to a piece of music, then there are two beats in
    each bar and you’re probably listening to a march. If you can count 1, 2, 3, 1,
    2, 3, then there are three beats to each bar and you may be listening to a waltz.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*小节*（或*节拍*）是包含固定数量的拍子的音乐小单位，并由穿过五线谱的垂直线标记出来。如果你能够数1、2、1、2，那么每个小节有两拍，你可能在听进行曲。如果你能够数1、2、3、1、2、3，那么每个小节有三拍，你可能在听华尔兹。'
- en: What note do you think comes next? Even if you have no musical training you
    may still be able to guess. If you said G (the same as the very first note of
    the piece), then you’d be correct. How did you know this? You may have been able
    to see that every bar and half bar starts with the same note and used this information
    to inform your decision. We want our model to be able to perform the same trick—in
    particular, we want it to pay attention to a particular note from the previous
    half bar, when the previous low G was registered. An attention-based model such
    as a Transformer will be able to incorporate this long-term look-back without
    having to maintain a hidden state across many bars, as is the case with a recurrent
    neural network.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为接下来会是什么音符？即使你没有音乐训练，你可能仍然能猜到。如果你说是G（与乐曲的第一个音符相同），那么你是正确的。你是怎么知道的？你可能能够看到每个小节和半小节都以相同的音符开头，并利用这些信息来做出决定。我们希望我们的模型能够执行相同的技巧——特别是，我们希望它能够关注前半小节中的特定音符，当前一个低G被记录时。基于注意力的模型，如Transformer，将能够在不必在许多小节之间保持隐藏状态的情况下，合并这种长期回顾。
- en: Anyone tackling the task of music generation must first have a basic understanding
    of musical theory. In the next section we’ll go through the essential knowledge
    required to read music and how we can represent this numerically, in order to
    transform music into the input data required to train our Transformer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 任何尝试音乐生成任务的人首先必须对音乐理论有基本的了解。在下一节中，我们将介绍阅读音乐所需的基本知识以及如何将其数值化，以便将音乐转换为训练Transformer所需的输入数据。
- en: Running the Code for This Example
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/11_music/01_transformer/transformer.ipynb*
    in the book repository.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书存储库中的Jupyter笔记本*notebooks/11_music/01_transformer/transformer.ipynb*中找到。
- en: The Bach Cello Suite Dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 巴赫大提琴组曲数据集
- en: The raw dataset that we shall be using is a set of MIDI files for the Cello
    Suites by J.S. Bach. You can download the dataset by running the dataset downloader
    script in the book repository, as shown in [Example 11-1](#downloading-cello-dataset).
    This will save the MIDI files locally to the */data* folder.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的原始数据集是J.S.巴赫的大提琴组曲的一组MIDI文件。您可以通过在书的存储库中运行数据集下载脚本来下载数据集，如[示例11-1](#downloading-cello-dataset)所示。这将把MIDI文件保存到本地的*/data*文件夹中。
- en: Example 11-1\. Downloading the J.S. Bach Cello Suites dataset
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-1。下载J.S.巴赫大提琴组曲数据集
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`To view and listen to the music generated by the model, you’ll need some software
    that can produce musical notation. [MuseScore](https://musescore.org) is a great
    tool for this purpose and can be downloaded for free.`  `## Parsing MIDI Files'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`要查看并听取模型生成的音乐，您需要一些能够生成乐谱的软件。[MuseScore](https://musescore.org)是一个很好的工具，可以免费下载。`  `##
    解析MIDI文件'
- en: We’ll be using the Python library `music21` to load the MIDI files into Python
    for processing. [Example 11-2](#example0701) shows how to load a MIDI file and
    visualize it ([Figure 11-2](#musical_notation)), both as a score and as structured
    data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python库`music21`将MIDI文件加载到Python中进行处理。[示例11-2](#example0701)展示了如何加载一个MIDI文件并可视化它（[图11-2](#musical_notation)），既作为乐谱又作为结构化数据。
- en: '![](Images/gdl2_1102.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1102.png)'
- en: Figure 11-2\. Musical notation
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。音乐符号
- en: Example 11-2\. Importing a MIDI file
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-2。导入MIDI文件
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Octaves
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 八度
- en: The number after each note name indicates the *octave* that the note is in—since
    the note names (A to G) repeat, this is needed to uniquely identify the pitch
    of the note. For example, `G2` is an octave below `G3`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个音符名称后面的数字表示音符所在的*八度*——因为音符名称（A到G）重复，这是为了唯一标识音符的音高。例如，`G2`是低于`G3`的一个八度。
- en: Now it’s time to convert the scores into something that looks more like text!
    We start by looping over each score and extracting the note and duration of each
    element in the piece into two separate text strings, with elements separated by
    spaces. We encode the key and time signature of the piece as special symbols,
    with zero duration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将乐谱转换成更像文本的东西了！我们首先循环遍历每个乐谱，并将乐曲中每个元素的音符和持续时间提取到两个单独的文本字符串中，元素之间用空格分隔。我们将乐曲的调号和拍号编码为特殊符号，持续时间为零。
- en: Monophonic Versus Polyphonic Music
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单声部与复调音乐
- en: In this first example, we will treat the music as *monophonic* (one single line),
    taking just the top note of any chords. Sometimes we may wish to keep the parts
    separate to generate music that is *polyphonic* in nature. This presents additional
    challenges that we shall tackle later on in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个例子中，我们将把音乐视为*单声部*（一条单独的线），只取任何和弦的最高音。有时我们可能希望保持各声部分开，以生成*复调*性质的音乐。这带来了我们将在本章后面解决的额外挑战。
- en: The output from this process is shown in [Figure 11-3](#music_duration_pitch)—compare
    this to [Figure 11-2](#musical_notation) so that you can see how the raw music
    data has been transformed into the two strings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的输出显示在[图11-3](#music_duration_pitch)中——将其与[图11-2](#musical_notation)进行比较，以便看到原始音乐数据如何转换为这两个字符串。
- en: '![](Images/gdl2_1103.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1103.png)'
- en: Figure 11-3\. Samples of the *notes* text string and the *duration* text string,
    corresponding to [Figure 11-2](#musical_notation)
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3。*音符*文本字符串和*持续时间*文本字符串的示例，对应于[图11-2](#musical_notation)
- en: This looks a lot more like the text data that we have dealt with previously.
    The *words* are the note–duration combinations, and we should try to build a model
    that predicts the next note and duration, given a sequence of previous notes and
    durations. A key difference between music and text generation is that we need
    to build a model that can handle the note and duration prediction simultaneously—i.e.,
    there are two streams of information that we need to handle, compared to the single
    streams of text that we saw in [Chapter 9](ch09.xhtml#chapter_transformer).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来更像我们之前处理过的文本数据。*单词*是音符-持续时间组合，我们应该尝试构建一个模型，根据先前音符和持续时间的序列来预测下一个音符和持续时间。音乐和文本生成之间的一个关键区别是，我们需要构建一个可以同时处理音符和持续时间预测的模型——即，我们需要处理两个信息流，而不是我们在[第9章](ch09.xhtml#chapter_transformer)中看到的单一文本流。
- en: Tokenization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: To create the dataset that will train the model, we first need to tokenize each
    note and duration, exactly as we did previously for each word in a text corpus.
    We can achieve this by using a `TextVectorization` layer, applied to the notes
    and durations separately, as shown in [Example 11-3](#example0702).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建将训练模型的数据集，我们首先需要像之前为文本语料库中的每个单词所做的那样，对每个音符和持续时间进行标记化。我们可以通过使用`TextVectorization`层，分别应用于音符和持续时间，来实现这一点，如[示例11-3](#example0702)所示。
- en: Example 11-3\. Tokenizing the notes and durations
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-3。标记化音符和持续时间
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The full parsing and tokenization process is shown in [Figure 11-4](#music_lookups).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的解析和标记化过程显示在[图11-4](#music_lookups)中。
- en: '![](Images/gdl2_1104.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1104.png)'
- en: Figure 11-4\. Parsing the MIDI files and tokenizing the notes and durations
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4。解析MIDI文件并对音符和持续时间进行标记化
- en: Creating the Training Set
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练集
- en: The final step of preprocessing is to create the training set that we will feed
    to our Transformer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的最后一步是创建我们将馈送给Transformer的训练集。
- en: We do this by splitting both the note and duration strings into chunks of 50
    elements, using a sliding window technique. The output is simply the input window
    shifted by one note, so that the Transformer is trained to predict the note and
    duration of the element one timestep into the future, given previous elements
    in the window. An example of this (using a sliding window of only four elements
    for demonstration purposes) is shown in [Figure 11-5](#music_input_output).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将音符和持续时间字符串分成50个元素的块来实现这一点，使用滑动窗口技术。输出只是输入窗口向后移动一个音符，这样Transformer就被训练来预测未来一个时间步的元素的音符和持续时间，给定窗口中的先前元素。这个示例（仅用四个元素的滑动窗口进行演示）显示在[图11-5](#music_input_output)中。
- en: '![](Images/gdl2_1105.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1105.png)'
- en: Figure 11-5\. The inputs and outputs for the musical Transformer model—in this
    example, a sliding window of width 4 is used to create input chunks, which are
    then shifted by one element to create the target output
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5。音乐Transformer模型的输入和输出——在这个例子中，使用宽度为4的滑动窗口创建输入块，然后将其移动一个元素以创建目标输出
- en: The architecture we will be using for our Transformer is the same as we used
    for text generation in [Chapter 9](ch09.xhtml#chapter_transformer), with a few
    key differences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Transformer中使用的架构与我们在[第9章](ch09.xhtml#chapter_transformer)中用于文本生成的架构相同，但有一些关键的区别。
- en: Sine Position Encoding
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正弦位置编码
- en: Firstly, we will be introducing a different type of encoding for the token positions.
    In [Chapter 9](ch09.xhtml#chapter_transformer) we used a simple `Embedding` layer
    to encode the position of each token, effectively mapping each integer position
    to a distinct vector that was learned by the model. We therefore needed to define
    a maximum length ( <math alttext="upper N"><mi>N</mi></math> ) that the sequence
    could be and train on this length of sequence. The downside to this approach is
    that it is then impossible to extrapolate to sequences that are longer than this
    maximum length. You would have to clip the input to the last <math alttext="upper
    N"><mi>N</mi></math> tokens, which isn’t ideal if you are trying to generate long-form
    content.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍一种不同类型的令牌位置编码。在[第9章](ch09.xhtml#chapter_transformer)中，我们使用了一个简单的`Embedding`层来编码每个令牌的位置，有效地将每个整数位置映射到模型学习的不同向量。因此，我们需要定义一个最大长度（<math
    alttext="upper N"><mi>N</mi></math>），该序列可以是，并在这个序列长度上进行训练。这种方法的缺点是无法推断出比这个最大长度更长的序列。您将不得不将输入剪切到最后的<math
    alttext="upper N"><mi>N</mi></math>个令牌，如果您试图生成长篇内容，则这并不理想。
- en: 'To circumvent this problem, we can switch to using a different type of embedding
    called a *sine position embedding*. This is similar to the embedding that we used
    in [Chapter 8](ch08.xhtml#chapter_diffusion) to encode the noise variances of
    the diffusion model. Specifically, the following function is used to convert the
    position of the word ( <math alttext="p o s"><mrow><mi>p</mi> <mi>o</mi> <mi>s</mi></mrow></math>
    ) in the input sequence into a unique vector of length <math alttext="d"><mi>d</mi></math>
    :'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，我们可以转而使用一种称为*sine position embedding*的不同类型的嵌入。这类似于我们在[第8章](ch08.xhtml#chapter_diffusion)中用来编码扩散模型噪声方差的嵌入。具体来说，以下函数用于将输入序列中单词的位置（<math
    alttext="p o s"><mrow><mi>p</mi> <mi>o</mi> <mi>s</mi></mrow></math>）转换为长度为<math
    alttext="d"><mi>d</mi></math>的唯一向量：
- en: <math alttext="StartLayout 1st Row 1st Column upper P upper E Subscript p o
    s comma 2 i 2nd Column equals 3rd Column sine left-parenthesis StartFraction p
    o s Over 10 comma 000 Superscript 2 i slash d Baseline EndFraction right-parenthesis
    2nd Row 1st Column upper P upper E Subscript p o s comma 2 i plus 1 2nd Column
    equals 3rd Column cosine left-parenthesis StartFraction p o s Over 10 comma 000
    Superscript left-parenthesis 2 i plus 1 right-parenthesis slash d Baseline EndFraction
    right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>P</mi> <msub><mi>E</mi> <mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>,</mo><mn>2</mn><mi>i</mi></mrow></msub></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow> <mrow><mn>10</mn><mo>,</mo><msup><mn>000</mn>
    <mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup></mrow></mfrac> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>P</mi> <msub><mi>E</mi> <mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow> <mrow><mn>10</mn><mo>,</mo><msup><mn>000</mn>
    <mrow><mo>(</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup></mrow></mfrac>
    <mo>)</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column upper P upper E Subscript p o
    s comma 2 i 2nd Column equals 3rd Column sine left-parenthesis StartFraction p
    o s Over 10 comma 000 Superscript 2 i slash d Baseline EndFraction right-parenthesis
    2nd Row 1st Column upper P upper E Subscript p o s comma 2 i plus 1 2nd Column
    equals 3rd Column cosine left-parenthesis StartFraction p o s Over 10 comma 000
    Superscript left-parenthesis 2 i plus 1 right-parenthesis slash d Baseline EndFraction
    right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>P</mi> <msub><mi>E</mi> <mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>,</mo><mn>2</mn><mi>i</mi></mrow></msub></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow> <mrow><mn>10</mn><mo>,</mo><msup><mn>000</mn>
    <mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup></mrow></mfrac> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>P</mi> <msub><mi>E</mi> <mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow> <mrow><mn>10</mn><mo>,</mo><msup><mn>000</mn>
    <mrow><mo>(</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo><mo>/</mo><mi>d</mi></mrow></msup></mrow></mfrac>
    <mo>)</mo></mrow></mtd></mtr></mtable></math>
- en: For small <math alttext="i"><mi>i</mi></math> , the wavelength of this function
    is short and therefore the function value changes rapidly along the position axis.
    Larger values of <math alttext="i"><mi>i</mi></math> create a longer wavelength.
    Each position thus has its own unique encoding, which is a specific combination
    of the different wavelengths.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的<math alttext="i"><mi>i</mi></math>，这个函数的波长很短，因此函数值沿着位置轴快速变化。较大的<math alttext="i"><mi>i</mi></math>值会产生更长的波长。因此，每个位置都有自己独特的编码，这是不同波长的特定组合。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Notice that this embedding is defined for all possible position values. It is
    a deterministic function (i.e., it isn’t learned by the model) that uses trigonometric
    functions to define a unique encoding for each possible position.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此嵌入是为所有可能的位置值定义的。它是一个确定性函数（即，模型不会学习它），它使用三角函数来为每个可能的位置定义一个唯一的编码。
- en: The *Keras NLP* module has a built-in layer that implements this embedding for
    us—we can therefore define our `TokenAndPositionEmbedding` layer as shown in [Example 11-4](#music_token_pos_embedding).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Keras NLP*模块具有一个内置层，为我们实现了这种嵌入 - 因此，我们可以定义我们的`TokenAndPositionEmbedding`层，如[示例11-4](#music_token_pos_embedding)所示。'
- en: Example 11-4\. Tokenizing the notes and durations
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-4。对音符和持续时间进行标记化
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 11-6](#music_token_pos_embedding_img) shows how the two embeddings
    (token and position) are added to produce the overall embedding for the sequence.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-6](#music_token_pos_embedding_img)显示了如何将这两种嵌入（令牌和位置）相加以产生序列的整体嵌入。'
- en: '![](Images/gdl2_1106.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1106.png)'
- en: Figure 11-6\. The `TokenAndPositionEmbedding` layer adds the token embeddings
    to the sinusoidal position embeddings to produce the overall embedding for the
    sequence
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-6。`TokenAndPositionEmbedding`层将令牌嵌入添加到正弦位置嵌入中，以产生序列的整体嵌入
- en: Multiple Inputs and Outputs
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个输入和输出
- en: We now have two input streams (notes and durations) and two output streams (predicted
    notes and durations). We therefore need to adapt the architecture of our Transformer
    to cater for this.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个输入流（音符和持续时间）和两个输出流（预测音符和持续时间）。因此，我们需要调整我们的Transformer架构以适应这一点。
- en: There are many ways of handling the dual stream of inputs. We could create tokens
    that represent each note–duration pair and then treat the sequence as a single
    stream of tokens. However, this has the downside of not being able to represent
    note–duration pairs that have not been seen in the training set (for example,
    we may have seen a `G#2` note and a `1/3` duration independently, but never together,
    so there would be no token for `G#2:1/3`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 处理双输入流的方法有很多种。我们可以创建代表每个音符-持续时间对的令牌，然后将序列视为单个令牌流。然而，这样做的缺点是无法表示在训练集中未见过的音符-持续时间对（例如，我们可能独立地看到了`G#2`音符和`1/3`持续时间，但从未一起出现过，因此没有`G#2:1/3`的令牌）。
- en: Instead, we choose to embed the note and duration tokens separately and then
    use a concatenation layer to create a single representation of the input that
    can be used by the downstream Transformer block. Similarly, the output from the
    Transformer block is passed to two separate dense layers, which represent the
    predicted note and duration probabilities. The overall architecture is shown in
    [Figure 11-7](#music_gen_transformer). Layer output shapes are shown with batch
    size `b` and sequence length `l`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们选择分别嵌入音符和持续时间令牌，然后使用连接层创建输入的单一表示，该表示可以被下游Transformer块使用。类似地，Transformer块的输出传递给两个独立的密集层，代表了预测的音符和持续时间概率。整体架构如[图11-7](#music_gen_transformer)所示。层输出形状显示了批量大小`b`和序列长度`l`。
- en: '![](Images/gdl2_1107.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1107.png)'
- en: Figure 11-7\. The architecture of the music-generating Transformer
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7。音乐生成Transformer的架构
- en: An alternative approach would be to interleave the note and duration tokens
    into a single stream of input and let the model learn that the output should be
    a single stream where the note and duration tokens alternate. This comes with
    the added complexity of ensuring that the output can still be parsed when the
    model has not yet learned how to interleave the tokens correctly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将音符和持续时间标记交错到一个单一的输入流中，并让模型学习输出应该是一个音符和持续时间标记交替的单一流。这增加了确保当模型尚未学会如何正确交错标记时，输出仍然可以解析的复杂性。
- en: Tip
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: There is no *right* or *wrong* way to design your model—part of the fun is experimenting
    with different setups and seeing which works best for you!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 设计您的模型没有*对*或*错*的方式——其中一部分乐趣就是尝试不同的设置，看看哪种对您最有效！
- en: Analysis of the Music-Generating Transformer
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音乐生成变压器的分析
- en: 'We’ll start by generating some music from scratch, by seeding the network with
    a `START` note token and `0.0` duration token (i.e., we are telling the model
    to assume it is starting from the beginning of the piece). Then we can generate
    a musical passage using the same iterative technique we used in [Chapter 9](ch09.xhtml#chapter_transformer)
    for generating text sequences, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头开始生成一些音乐，通过向网络提供一个`START`音符标记和`0.0`持续时间标记（即，我们告诉模型假设它是从乐曲的开头开始）。然后我们可以使用与我们在[第9章](ch09.xhtml#chapter_transformer)中用于生成文本序列的相同迭代技术来生成一个音乐段落，如下所示：
- en: Given the current sequence (of notes and durations), the model predicts two
    distributions, one for the next note and one for the next duration.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定当前序列（音符和持续时间），模型预测两个分布，一个是下一个音符的分布，另一个是下一个持续时间的分布。
- en: We sample from both of these distributions, using a `temperature` parameter
    to control how much variation we would like in the sampling process.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从这两个分布中进行采样，使用一个`temperature`参数来控制在采样过程中我们希望有多少变化。
- en: The chosen note and duration are appended to the respective input sequences.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择的音符和持续时间被附加到相应的输入序列中。
- en: The process repeats with the new input sequences for as many elements as we
    wish to generate.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，对于我们希望生成的元素数量，会有新的输入序列。
- en: '[Figure 11-8](#transformer_generated_music) shows examples of music generated
    from scratch by the model at various epochs of the training process. We use a
    temperature of 0.5 for the notes and durations.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-8](#transformer_generated_music)展示了在训练过程的各个时期由模型从头开始生成的音乐示例。我们对音符和持续时间使用了0.5的温度。 '
- en: '![](Images/gdl2_1108.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1108.png)'
- en: Figure 11-8\. Some examples of passages generated by the model when seeded only
    with a `START` note token and `0.0` duration token
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8。当仅使用一个`START`音符标记和`0.0`持续时间标记作为种子时，模型生成的乐段示例
- en: Most of our analysis in this section will focus on the note predictions, rather
    than durations, as for Bach’s Cello Suites the harmonic intricacies are more difficult
    to capture and therefore more worthy of investigation. However, you can also apply
    the same analysis to the rhythmic predictions of the model, which may be particularly
    relevant for other styles of music that you could use to train this model (such
    as a drum track).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们大部分的分析将集中在音符预测上，而不是持续时间，因为对于巴赫的大提琴组曲来说，和声的复杂性更难捕捉，因此更值得研究。然而，您也可以将相同的分析应用于模型的节奏预测，这对于您可能用来训练该模型的其他音乐风格可能特别相关（比如鼓声）。
- en: There are several points to note about the generated passages in [Figure 11-8](#transformer_generated_music).
    First, see how the music is becoming more sophisticated as training progresses.
    To begin with, the model plays it safe by sticking to the same group of notes
    and rhythms. By epoch 10, the model has begun to generate small runs of notes,
    and by epoch 20 it is producing interesting rhythms and is firmly established
    in a set key (E ♭ major).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在[图11-8](#transformer_generated_music)中生成的乐段有几点需要注意。首先，看到随着训练的进行，音乐变得越来越复杂。一开始，模型通过坚持使用相同的音符和节奏来保险。到了第10个时期，模型已经开始生成小段音符，到了第20个时期，它产生了有趣的节奏，并且牢固地确立在一个固定的调（E
    ♭大调）中。
- en: Second, we can analyze the distribution of notes over time by plotting the predicted
    distribution at each timestep as a heatmap. [Figure 11-9](#pitch_values_transformer)
    shows this heatmap for the example from epoch 20 in [Figure 11-8](#transformer_generated_music).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以通过绘制每个时间步的预测分布的热图来分析随时间变化的音符分布。[图11-9](#pitch_values_transformer)展示了在[图11-8](#transformer_generated_music)中第20个时期的示例的热图。
- en: '![](Images/gdl2_1109.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1109.png)'
- en: 'Figure 11-9\. The distribution of possible next notes over time (at epoch 20):
    the darker the square, the more certain the model is that the next note is at
    this pitch'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9。随着时间推移可能的下一个音符的分布（在第20个时期）：方块越暗，模型对下一个音符在这个音高的确定性就越高
- en: An interesting point to note here is that the model has clearly learned which
    notes belong to particular *keys*, as there are gaps in the distribution at notes
    that do not belong to the key. For example, there is a gray gap along the row
    for note 54 (corresponding to G ♭/F ♯). This note is highly unlikely to appear
    in a piece of music in the key of E ♭ major. The model establishes the key early
    on in the generation process, and as the piece progresses, the model chooses notes
    that are more likely to feature in that key by attending to the token that represents
    it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个有趣的点是，模型显然已经学会了哪些音符属于特定的*调*，因为在不属于该调的音符处存在分布中的间隙。例如，在音符54（对应于G ♭/F
    ♯）的行上有一个灰色间隙。在E ♭大调的音乐作品中，这个音符极不可能出现。模型在生成过程的早期就确立了调，并且随着乐曲的进行，模型选择更有可能出现在该调中的音符，通过关注代表它的标记。
- en: It is also worth pointing out that the model has learned Bach’s characteristic
    style of dropping to a low note on the cello to end a phrase and bouncing back
    up again to start the next. See how around note 20, the phrase ends on a low E
    ♭—it is common in the Bach Cello Suites to then return to a higher, more sonorous
    range of the instrument for the start of next phrase, which is exactly what the
    model predicts. There is a large gray gap between the low E ♭ (pitch number 39)
    and the next note, which is predicted to be around pitch number 50, rather than
    continuing to rumble around the depths of the instrument.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，模型还学会了巴赫特有的风格，即在大提琴上降到低音结束一个乐句，然后又反弹回来开始下一个乐句。看看大约在第20个音符附近，乐句以低音E♭结束——在巴赫大提琴组曲中，通常会回到乐器更高、更响亮的音域开始下一个乐句，这正是模型的预测。在低音E♭（音高编号39）和下一个音符之间有一个很大的灰色间隙，预测下一个音符将在音高编号50左右，而不是继续在乐器的低音区域漂浮。
- en: Lastly, we should check to see if our attention mechanism is working as expected.
    The horizontal axis in [Figure 11-10](#attention_matrix) shows the generated sequence
    of notes; the vertical axis shows where the attention of the network was aimed
    when predicting each note along the horizontal axis. The color of each square
    shows the maximum attention weight across all heads at each point in the generated
    sequence. The darker the square, the more attention is being applied to this position
    in the sequence. For simplicity, we only show the notes in this diagram, but the
    durations of each note are also being attended to by the network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该检查我们的注意力机制是否按预期工作。[图11-10](#attention_matrix)中的水平轴显示了生成的音符序列；垂直轴显示了网络在预测水平轴上的每个音符时所关注的位置。每个方块的颜色显示了在生成序列的每个点上所有头部中的最大注意力权重。方块越暗，表示在序列中这个位置上应用的注意力越多。为简单起见，我们在这个图表中只显示了音符，但网络也会关注每个音符的持续时间。
- en: We can see that for the initial key signature, time signature, and rest, the
    network chose to place almost all of its attention on the `START` token. This
    makes sense, as these artifacts always appear at the start of a piece of music—once
    the notes start flowing the `START` token essentially stops being attended to.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在初始调号、拍号和休止符中，网络选择几乎全部注意力放在`START`标记上。这是有道理的，因为这些特征总是出现在音乐片段的开头——一旦音符开始流动，`START`标记基本上就不再受到关注。
- en: As we move beyond the initial few notes, we can see that the network places
    most attention on approximately the last two to four notes and rarely places significant
    weight on notes more than four notes ago. Again, this makes sense; there is probably
    enough information contained in the previous four notes to understand how the
    phrase might continue. Additionally, some notes attend more strongly back to the
    key signature of D minor—for example, the `E3` (7th note of the piece) and `B-2`
    (B ♭–14th note of the piece). This is fascinating, because these are the exact
    notes that rely on the key of D minor to relieve any ambiguity. The network must
    *look back* at the key signature in order to tell that there is a B ♭ in the key
    signature (rather than a B natural) but there isn’t an E ♭ in the key signature
    (E natural must be used instead).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们超过最初的几个音符时，我们可以看到网络主要关注大约最后两到四个音符，并很少对四个音符之前的音符给予重要权重。再次，这是有道理的；前四个音符中可能包含足够的信息，以了解乐句可能如何继续。此外，一些音符更强烈地回到D小调的调号上——例如`E3`（乐曲的第7个音符）和`B-2`（B♭-乐曲的第14个音符）。这很有趣，因为这些正是依赖D小调调号来消除任何模糊的确切音符。网络必须*回顾*调号才能知道调号中有一个B♭（而不是B自然音），但调号中没有一个E♭（必须使用E自然音）。
- en: '![](Images/gdl2_1110.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1110.png)'
- en: Figure 11-10\. The color of each square in the matrix indicates the amount of
    attention given to each position on the vertical axis, at the point of predicting
    the note on the horizontal axis
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-10。矩阵中每个方块的颜色表示在水平轴上预测音符时，对垂直轴上每个位置给予的注意力量
- en: There are also examples of where the network has chosen to ignore a certain
    note or rest nearby, as it doesn’t add any additional information to its understanding
    of the phrase. For example, the penultimate note (`A2`) is not particularly attentive
    to the `B-2` three notes back, but is slightly more attentive to the `A2` four
    notes back. It is more interesting for the model to look at the `A2` that falls
    on the beat, rather than the `B-2` off the beat, which is just a passing note.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些例子表明，网络选择忽略附近的某些音符或休止符，因为它们对理解乐句并没有提供额外信息。例如，倒数第二个音符（`A2`）对三个音符前的`B-2`并不特别关注，但对四个音符前的`A2`稍微更关注。对于模型来说，看位于节拍上的`A2`比看位于节拍外的`B-2`更有趣，后者只是一个过渡音。
- en: Remember we haven’t told the model anything about which notes are related or
    which notes belong to which key signatures—it has worked this out for itself just
    by studying the music of J.S. Bach.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们并没有告诉模型哪些音符相关，哪些音符属于哪个调号——它通过研究巴赫的音乐自己弄清楚了这一点。
- en: Tokenization of Polyphonic Music
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多声部音乐的标记化
- en: The Transformer we’ve been exploring in this section works well for single-line
    (monophonic) music, but could it be adapted to multiline (polyphonic) music?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中探讨的Transformer对单线（单声部）音乐效果很好，但它能够适应多线（复调）音乐吗？
- en: 'The challenge lies in how to represent the different lines of music as a single
    sequence of tokens. In the previous section we decided to split the notes and
    durations of the notes into two distinct inputs and outputs of the network, but
    we also saw that we could have interleaved these tokens into a single stream.
    We can use the same idea to handle polyphonic music. Two different approaches
    will be introduced here: *grid tokenization* and *event-based tokenization*, as
    discussed in the 2018 paper “Music Transformer: Generating Music with Long-Term
    Structure.”^([1](ch11.xhtml#idm45387004193120))'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于如何将不同的音乐线表示为单个令牌序列。在前一节中，我们决定将音符和音符持续时间分成网络的两个不同输入和输出，但我们也看到我们可以将这些令牌交错成一个单一流。我们可以使用相同的想法来处理复调音乐。这里将介绍两种不同的方法：*网格标记化*和*基于事件的标记化*，正如2018年的论文“音乐变压器：生成具有长期结构的音乐”中所讨论的那样。^([1](ch11.xhtml#idm45387004193120))
- en: Grid tokenization
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格标记化
- en: Consider the two bars of music from a J.S. Bach chorale in [Figure 11-11](#Bach_chorale).
    There are four distinct parts (soprano [S], alto [A], tenor [T], bass [B]), written
    on different staffs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[J.S.巴赫赞美诗](#Bach_chorale)中的两小节音乐。有四个不同的声部（女高音[S]，中音[A]，男高音[T]，低音[B]），分别写在不同的五线谱上。
- en: '![](Images/gdl2_1111.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1111.png)'
- en: Figure 11-11\. The first two bars of a J.S. Bach chorale
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-11。J.S.巴赫赞美诗的前两小节
- en: We can imagine drawing this music on a grid, where the y-axis represents the
    pitch of the note and the x-axis represents the number of 16th-notes (semiquavers)
    that have passed since the start of the piece. If the grid square is filled, then
    there is a note playing at that point in time. All four parts are drawn on the
    same grid. This grid is known as a *piano roll* because it resembles a physical
    roll of paper with holes punched into it, which was used as a recording mechanism
    before digital systems were invented.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象在网格上绘制这段音乐，其中y轴表示音符的音高，x轴表示自作品开始以来经过的16分音符（四分音符）的数量。如果网格方块被填充，那么在那个时间点有音符在播放。所有四个声部都绘制在同一个网格上。这个网格被称为*钢琴卷*，因为它类似于一卷纸上打孔的物理卷，这在数字系统发明之前被用作记录机制。
- en: We can serialize the grid into a stream of tokens by moving first through the
    four voices, then along the timesteps in sequence. This produces a sequence of
    tokens <math alttext="upper S 1 comma upper A 1 comma upper T 1 comma upper B
    1 comma upper S 2 comma upper A 2 comma upper T 2 comma upper B 2 comma ellipsis"><mrow><msub><mi>S</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>A</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>T</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>S</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>A</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>T</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo></mrow></math>
    , where the subscript denotes the timestep, as shown in [Figure 11-12](#grid_tokenization).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过首先沿着四个声部，然后沿着时间步骤顺序移动，将网格序列化为令牌流。这将产生一个令牌序列<math alttext="upper S 1 comma
    upper A 1 comma upper T 1 comma upper B 1 comma upper S 2 comma upper A 2 comma
    upper T 2 comma upper B 2 comma ellipsis"><mrow><msub><mi>S</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>A</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>T</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>S</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>A</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>T</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo></mrow></math>，其中下标表示时间步骤，如[图11-12](#grid_tokenization)所示。
- en: '![](Images/gdl2_1112.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1112.png)'
- en: Figure 11-12\. Creating the grid tokenization for the first two bars of the
    Bach chorale
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-12。为巴赫赞美诗的前两小节创建网格标记化
- en: We would then train our Transformer on this sequence of tokens, to predict the
    next token given the previous tokens. We can decode the generated sequence back
    into a grid structure by rolling the sequence back out over time in groups of
    four notes (one for each voice). This technique works surprisingly well, despite
    the same note often being split across multiple tokens with tokens from other
    voices in between.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练我们的变压器模型以预测给定先前令牌的下一个令牌。我们可以通过将序列在时间上以四个音符一组（每个声部一个）展开来将生成的序列解码回网格结构。尽管同一个音符经常被分割成多个令牌，并且在其他声部的令牌之间有令牌，但这种技术效果出奇地好。
- en: However, there are some disadvantages. Firstly, notice that there is no way
    for the model to tell the difference between one long note and two shorter adjacent
    notes of the same pitch. This is because the tokenization does not explicitly
    encode the duration of notes, only whether a note is present at each timestep.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也存在一些缺点。首先，请注意，模型无法区分一个长音符和相同音高的两个较短相邻音符。这是因为标记化并没有明确编码音符的持续时间，只是在每个时间步是否存在音符。
- en: Secondly, this method requires the music to have a regular beat that is divisible
    into reasonably sized chunks. For example, using the current system, we cannot
    encode triplets (a group of three notes played across a single beat). We could
    divide the music into 12 steps per quarter-note (crotchet) instead of 4, that
    would triple the number of tokens required to represent the same passage of music,
    adding overhead on the training process and affecting the lookback capacity of
    the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这种方法要求音乐具有可以分成合理大小块的规则节拍。例如，使用当前系统，我们无法编码三连音（在一个拍子中演奏的三个音符）。我们可以将音乐分成每个四分音符（四分音符）12个步骤，而不是4个步骤，这将使表示相同音乐段所需的令牌数量增加三倍，增加训练过程的开销，并影响模型的回溯能力。
- en: Lastly, it is not obvious how we might add other components to the tokenization,
    such as dynamics (how loud or quiet the music is in each part) or tempo changes.
    We are locked into the two-dimensional grid structure of the piano roll, which
    provides a convenient way to represent pitch and timing, but not necessarily an
    easy way to incorporate other components that make music interesting to listen
    to.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们不清楚如何将其他组件添加到标记化中，比如动态（每个声部的音乐是大声还是安静）或速度变化。我们被钢琴卷的二维网格结构所限制，这提供了一种方便的表示音高和节奏的方式，但不一定是一种容易融入使音乐听起来有趣的其他组件的方式。
- en: Event-based tokenization
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于事件的标记化
- en: A more flexible approach is to use event-based tokenization. This can be thought
    of as a vocabulary that literally describes how the music is created as a sequence
    of events, using a rich set of tokens.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 更灵活的方法是使用基于事件的令牌化。这可以被看作是一个词汇表，字面上描述了音乐是如何作为一系列事件创建的，使用丰富的令牌集。
- en: 'For example in [Figure 11-13](#event_tokenization), we use three types of tokens:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图11-13](#event_tokenization)中，我们使用三种类型的令牌：
- en: '`NOTE_ON<*pitch*>` (start playing a note of a given pitch)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NOTE_ON<*音高*>`（开始播放给定音高的音符）'
- en: '`NOTE_OFF<*pitch*>` (stop playing a note of a given pitch)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NOTE_OFF<*音高*>`（停止播放给定音高的音符）'
- en: '`TIME_SHIFT<*step*>` (shift forward in time by a given step)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TIME_SHIFT<*步骤*>`（按给定步骤向前移动时间）'
- en: This vocabulary can be used to create a sequence that describes the construction
    of the music as a set of instructions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个词汇表可以用来创建一个描述音乐构造的序列，作为一组指令。
- en: '![](Images/gdl2_1113.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1113.png)'
- en: Figure 11-13\. An event tokenization for the first bar of the Bach chorale
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-13。巴赫赞美诗第一小节的事件令牌化
- en: We could easily incorporate other types of tokens into this vocabulary, to represent
    dynamic and tempo changes for subsequent notes. This method also provides a way
    to generate triplets against a backdrop of quarter-notes, by separating the notes
    of the triplets with `TIME_SHIFT<0.33>` tokens. Overall, it is a more expressive
    framework for tokenization, though it is also potentially more complex for the
    Transformer to learn inherent patterns in the training set music, as it is by
    definition less structured than the grid method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地将其他类型的令牌纳入这个词汇表中，以表示后续音符的动态和速度变化。通过使用`TIME_SHIFT<0.33>`令牌，这种方法还提供了一种在四分音符背景下生成三连音的方法。总的来说，这是一个更具表现力的令牌化框架，尽管对于Transformer来说，学习训练集音乐中固有模式可能更复杂，因为它在定义上比网格方法更少结构化。
- en: Tip
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: I encourage you to try implementing these polyphonic techniques and train a
    Transformer on the new tokenized dataset using all the knowledge you have built
    up so far in this book. I would also recommend checking our Dr. Tristan Behrens’s
    guide to music generation research, available on [GitHub](https://oreil.ly/YfaiJ),
    which provides a comprehensive overview of different papers on the topic of music
    generation using deep learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励您尝试实施这些复调技术，并使用您在本书中迄今为止积累的所有知识在新的令牌化数据集上训练Transformer。我还建议查看我们的Tristan Behrens博士关于音乐生成研究的指南，可在[GitHub](https://oreil.ly/YfaiJ)上找到，该指南提供了关于使用深度学习进行音乐生成的不同论文的全面概述。
- en: In the next section we will take a completely different approach to music generation,
    using GANs.`  `# MuseGAN
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将采用完全不同的方法来进行音乐生成，使用GAN。# MuseGAN
- en: You may have thought that the piano roll shown in [Figure 11-12](#grid_tokenization)
    looks a bit like a piece of modern art. This begs the question—could we in fact
    treat this piano roll as a *picture* and utilize image generation methods instead
    of sequence generation techniques?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为[图11-12](#grid_tokenization)中显示的钢琴卷看起来有点像现代艺术品。这引发了一个问题——我们实际上是否可以将这个钢琴卷视为*图片*，并利用图像生成方法而不是序列生成技术？
- en: As we shall see, the answer to this question is yes, we can treat music generation
    directly as an image generation problem. This means that instead of using Transformers,
    we can apply the same convolutional-based techniques that work so well for image
    generation problems—in particular, GANs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，对于这个问题的答案是肯定的，我们可以直接将音乐生成视为图像生成问题。这意味着我们可以应用对图像生成问题非常有效的基于卷积的技术，特别是GAN。
- en: 'MuseGAN was introduced in the 2017 paper “MuseGAN: Multi-Track Sequential Generative
    Adversarial Networks for Symbolic Music Generation and Accompaniment.”^([2](ch11.xhtml#idm45387004128000))
    The authors show how it is possible to train a model to generate polyphonic, multitrack,
    multibar music through a novel GAN framework. Moreover, they show how, by dividing
    up the responsibilities of the noise vectors that feed the generator, they are
    able to maintain fine-grained control over the high-level temporal and track-based
    features of the music.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN是在2017年的论文“MuseGAN:用于符号音乐生成和伴奏的多轨序列生成对抗网络”中引入的。作者展示了通过一种新颖的GAN框架训练模型生成复调、多轨、多小节音乐是可能的。此外，他们展示了通过将喂给生成器的噪声向量的责任分解，他们能够对音乐的高级时间和基于轨道的特征进行精细控制。
- en: Let’s start by introducing the the J.S. Bach chorale dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先介绍J.S.巴赫赞美诗数据集。
- en: Running the Code for This Example
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/11_music/02_musegan/musegan.ipynb*
    in the book repository.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在书籍存储库中的*notebooks/11_music/02_musegan/musegan.ipynb*中找到。
- en: The Bach Chorale Dataset
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 巴赫赞美诗数据集
- en: To begin this project, you’ll first need to download the MIDI files that we’ll
    be using to train the MuseGAN. We’ll use a dataset of 229 J.S. Bach chorales for
    four voices.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始这个项目，您首先需要下载我们将用于训练MuseGAN的MIDI文件。我们将使用包含四声部的229首J.S.巴赫赞美诗数据集。
- en: You can download the dataset by running the Bach chorale dataset downloader
    script in the book repository, as shown in [Example 11-5](#downloading-bach-chorale-dataset).
    This will save the MIDI files locally to the */data* folder.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在书籍存储库中运行巴赫赞美诗数据集下载器脚本来下载数据集，如[示例11-5](#downloading-bach-chorale-dataset)所示。这将把MIDI文件保存到本地的*/data*文件夹中。
- en: Example 11-5\. Downloading the Bach chorale dataset
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-5。下载巴赫赞美诗数据集
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`The dataset consists of an array of four numbers for each timestep: the MIDI
    note pitches of each of the four voices. A timestep in this dataset is equal to
    a 16th note (a semiquaver). So, for example, in a single bar of 4 quarter (crotchet)
    beats, there are 16 timesteps. The dataset is automatically split into *train*,
    *validation*, and *test* sets. We will be using the *train* dataset to train the
    MuseGAN.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由每个时间步长的四个数字数组组成：每个声部的MIDI音符音高。在这个数据集中，一个时间步长等于一个16分音符（半音符）。因此，例如，在4个四分音符拍的单个小节中，有16个时间步长。数据集会自动分成*训练*、*验证*和*测试*集。我们将使用*训练*数据集来训练MuseGAN。
- en: To start, we need to get the data into the correct shape to feed the GAN. In
    this example we’ll generate two bars of music, so we’ll extract only the first
    two bars of each chorale. Each bar consists of 16 timesteps and there are a potential
    84 pitches across the 4 voices.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据整理成正确的形状以供GAN使用。在这个示例中，我们将生成两小节音乐，因此我们将提取每个赞美诗的前两小节。每小节包括16个时间步长，四个声部中有潜在的84个音高。
- en: Tip
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Voices will be referred to as *tracks* from here on, to keep the terminology
    in line with the original paper.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，声部将被称为*轨道*，以保持术语与原始论文一致。
- en: 'Therefore, the transformed data will have the following shape:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，转换后的数据将具有以下形状：
- en: '[PRE5]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'where:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To get the data into this shape, we one-hot encode the pitch numbers into a
    vector of length 84 and split each sequence of notes into two bars of 16 timesteps
    each. We are making the assumption here that each chorale in the dataset has four
    beats in each bar, which is reasonable, and even if this were not the case it
    would not adversely affect the training of the model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据整理成这种形状，我们将音高数字进行独热编码，转换为长度为84的向量，并将每个音符序列分成两小节，每小节包括16个时间步长。我们在这里做出的假设是数据集中的每个赞美诗每小节有四拍，这是合理的，即使不是这种情况，也不会对模型的训练产生不利影响。
- en: '[Figure 11-14](#musegan_data) shows how two bars of raw data are converted
    into the transformed piano roll dataset that we will use to train the GAN.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-14](#musegan_data)展示了两小节原始数据如何转换为我们将用来训练GAN的转换后的钢琴卷帘数据集。'
- en: '![](Images/gdl2_1114.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1114.png)'
- en: Figure 11-14\. Processing two bars of raw data into piano roll data that we
    can use to train the GAN`  `## The MuseGAN Generator
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-14。将两小节原始数据处理成我们可以用来训练GAN的钢琴卷帘数据`  `## MuseGAN生成器
- en: Like all GANs, MuseGAN consists of a generator and a critic. The generator tries
    to fool the critic with its musical creations, and the critic tries to prevent
    this from happening by ensuring it is able to tell the difference between the
    generator’s forged Bach chorales and the real thing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有GAN一样，MuseGAN由一个生成器和一个评论家组成。生成器试图用其音乐创作愚弄评论家，评论家试图通过确保能够区分生成器伪造的巴赫赞美诗和真实的赞美诗来阻止这种情况发生。
- en: 'Where MuseGAN differs is in the fact that the generator doesn’t just accept
    a single noise vector as input, but instead has four separate inputs, which correspond
    to four different characteristics of the music: chords, style, melody, and groove.
    By manipulating each of these inputs independently we can change high-level properties
    of the generated music.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN的不同之处在于生成器不仅接受单个噪声向量作为输入，而是有四个单独的输入，分别对应音乐的四个不同特征：和弦、风格、旋律和节奏。通过独立操纵这些输入中的每一个，我们可以改变生成音乐的高级属性。
- en: A high-level view of the generator is shown in [Figure 11-15](#musegan_generator).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的高级视图显示在[图11-15](#musegan_generator)中。
- en: '![](Images/gdl2_1115.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1115.png)'
- en: Figure 11-15\. High-level diagram of the MuseGAN generator
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: MuseGAN生成器的高级图表
- en: The diagram shows how the chords and melody inputs are first passed through
    a *temporal network* that outputs a tensor with one of the dimensions equal to
    the number of bars to be generated. The style and groove inputs are not stretched
    temporally in this way, as they remain constant through the piece.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示和弦和旋律输入首先通过一个*时间网络*，输出一个维度等于要生成的小节数的张量。风格和节奏输入不会以这种方式在时间上拉伸，因为它们在整个乐曲中保持不变。
- en: Then, to generate a particular bar for a particular track, the relevant outputs
    from the chords, style, melody, and groove parts of the network are concatenated
    to form a longer vector. This is then passed to a bar generator, which ultimately
    outputs the specified bar for the specified track.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了为特定轨道的特定小节生成特定小节，来自和弦、风格、旋律和节奏部分的相关输出被连接起来形成一个更长的向量。然后将其传递给小节生成器，最终输出指定轨道的指定小节。
- en: By concatenating the generated bars for all tracks, we create a score that can
    be compared with real scores by the critic.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接所有轨道的生成小节，我们创建了一个可以与评论家的真实分数进行比较的分数。
- en: Let’s first take a look at how to build a temporal network.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看如何构建一个时间网络。
- en: The temporal network
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间网络
- en: The job of a temporal network—a neural network consisting of convolutional transpose
    layers—is to transform a single input noise vector of length `Z_DIM = 32` into
    a different noise vector for every bar (also of length 32). The Keras code to
    build this is shown in [Example 11-6](#example0705).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 时间网络的工作是将长度为`Z_DIM = 32`的单个输入噪声向量转换为每个小节的不同噪声向量（长度也为32）的神经网络，由卷积转置层组成。构建这个网络的Keras代码在[示例11-6](#example0705)中显示。
- en: Example 11-6\. Building the temporal network
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-6。构建时间网络
- en: '[PRE7]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_music_generation_CO1-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_music_generation_CO1-1)'
- en: The input to the temporal network is a vector of length 32 (`Z_DIM`).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 时间网络的输入是长度为32的向量（`Z_DIM`）。
- en: '[![2](Images/2.png)](#co_music_generation_CO1-2)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_music_generation_CO1-2)'
- en: We reshape this vector to a 1 × 1 tensor with 32 channels, so that we can apply
    convolutional 2D transpose operations to it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个向量重塑为一个具有32个通道的1×1张量，以便我们可以对其应用二维卷积转置操作。
- en: '[![3](Images/3.png)](#co_music_generation_CO1-3)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_music_generation_CO1-3)'
- en: We apply `Conv2DTranspose` layers to expand the size of the tensor along one
    axis, so that it is the same length as `N_BARS`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用`Conv2DTranspose`层来沿一个轴扩展张量的大小，使其与`N_BARS`的长度相同。
- en: '[![4](Images/4.png)](#co_music_generation_CO1-4)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_music_generation_CO1-4)'
- en: We remove the unnecessary extra dimension with a `Reshape` layer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `Reshape` 层去除不必要的额外维度。
- en: The reason we use convolutional operations rather than requiring two independent
    vectors into the network is because we would like the network to learn how one
    bar should follow on from another in a consistent way. Using a neural network
    to expand the input vector along the time axis means the model has a chance to
    learn how music flows across bars, rather than treating each bar as completely
    independent of the last.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用卷积操作而不是要求两个独立的向量进入网络的原因是，我们希望网络学习如何以一种一致的方式让一个小节跟随另一个小节。使用神经网络沿着时间轴扩展输入向量意味着模型有机会学习音乐如何跨越小节流动，而不是将每个小节视为完全独立于上一个的。
- en: Chords, style, melody, and groove
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 和弦、风格、旋律和 groove
- en: 'Let’s now take a closer look at the four different inputs that feed the generator:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更仔细地看一下喂给生成器的四种不同输入：
- en: Chords
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 和弦
- en: The chords input is a single noise vector of length `Z_DIM`. This vector’s job
    is to control the general progression of the music over time, shared across tracks,
    so we use a `TemporalNetwork` to transform this single vector into a different
    latent vector for every bar. Note that while we call this input chords, it really
    could control anything about the music that changes per bar, such as general rhythmic
    style, without being specific to any particular track.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 和弦输入是一个长度为 `Z_DIM` 的单一噪声向量。这个向量的作用是控制音乐随时间的总体进展，跨越轨道共享，因此我们使用 `TemporalNetwork`
    将这个单一向量转换为每个小节的不同潜在向量。请注意，虽然我们称这个输入为和弦，但它实际上可以控制音乐中每个小节变化的任何内容，比如一般的节奏风格，而不是特定于任何特定轨道。
- en: Style
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 风格
- en: The style input is also a vector of length `Z_DIM`. This is carried forward
    without transformation, so it is the same across all bars and tracks. It can be
    thought of as the vector that controls the overall style of the piece (i.e., it
    affects all bars and tracks consistently).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 风格输入也是长度为 `Z_DIM` 的向量。这个向量在不经过转换的情况下传递，因此在所有小节和轨道上都是相同的。它可以被视为控制乐曲整体风格的向量（即，它会一致地影响所有小节和轨道）。
- en: Melody
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 旋律
- en: The melody input is an array of shape `[N_TRACKS, Z_DIM]`—that is, we provide
    the model with a random noise vector of length `Z_DIM` for each track.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 旋律输入是一个形状为 `[N_TRACKS, Z_DIM]` 的数组—也就是说，我们为每个轨道提供长度为 `Z_DIM` 的随机噪声向量。
- en: Each of these vectors is passed through a track-specific `TemporalNetwork`,
    where the weights are not shared between tracks. The output is a vector of length
    `Z_DIM` for every bar of every track. The model can therefore use these input
    vectors to fine-tune the content of every single bar and track independently.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量中的每一个都通过轨道特定的 `TemporalNetwork`，其中轨道之间的权重不共享。输出是每个轨道的每个小节的长度为 `Z_DIM` 的向量。因此，模型可以使用这些输入向量来独立地微调每个小节和轨道的内容。
- en: Groove
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Groove
- en: The groove input is also an array of shape `[N_TRACKS, Z_DIM]`—a random noise
    vector of length `Z_DIM` for each track. Unlike the melody input, these vectors
    are not passed through the temporal network but instead are fed straight through,
    just like the style vector. Therefore, each groove vector will affect the overall
    properties of a track, across all bars.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: groove 输入也是一个形状为 `[N_TRACKS, Z_DIM]` 的数组，即每个轨道的长度为 `Z_DIM` 的随机噪声向量。与旋律输入不同，这些向量不通过时间网络，而是直接传递，就像风格向量一样。因此，每个
    groove 向量将影响轨道的整体属性，跨越所有小节。
- en: We can summarize the responsibilities of each component of the MuseGAN generator
    as shown in [Table 11-1](#musegan_sections).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结每个 MuseGAN 生成器组件的责任，如 [表11-1](#musegan_sections) 所示。
- en: Table 11-1\. Components of the MuseGAN generator
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1\. MuseGAN 生成器的组件
- en: '|  | Output differs across bars? | Output differs across parts? |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | 输出在小节之间不同吗？ | 输出在部分之间不同吗？ |'
- en: '| --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Style | Ｘ | Ｘ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 风格 | Ｘ | Ｘ |'
- en: '| Groove | Ｘ | ✓ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Groove | Ｘ | ✓ |'
- en: '| Chords | ✓ | Ｘ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 和弦 | ✓ | Ｘ |'
- en: '| Melody | ✓ | ✓ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 旋律 | ✓ | ✓ |'
- en: The final piece of the MuseGAN generator is the *bar generator*—let’s see how
    we can use this to glue together the outputs from the chord, style, melody, and
    groove components.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN 生成器的最后一部分是 *小节生成器*—让我们看看如何使用它来将和弦、风格、旋律和 groove 组件的输出粘合在一起。
- en: The bar generator
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节生成器
- en: The bar generator receives four latent vectors—one from each of the chord, style,
    melody, and groove components. These are concatenated to produce a vector of length
    `4 * Z_DIM` as input. The output is a piano roll representation of a single bar
    for a single track—i.e., a tensor of shape `[1, n_steps_per_bar, n_pitches, 1]`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 小节生成器接收四个潜在向量——来自和弦、风格、旋律和 groove 组件。这些被连接起来产生长度为 `4 * Z_DIM` 的输入向量。输出是单个轨道的单个小节的钢琴卷表示—即，形状为
    `[1, n_steps_per_bar, n_pitches, 1]` 的张量。
- en: The bar generator is just a neural network that uses convolutional transpose
    layers to expand the time and pitch dimensions of the input vector. We create
    one bar generator for every track, and weights are not shared between tracks.
    The Keras code to build a `BarGenerator` is given in [Example 11-7](#example0706).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 小节生成器只是一个使用卷积转置层来扩展输入向量的时间和音高维度的神经网络。我们为每个轨道创建一个小节生成器，轨道之间的权重不共享。构建 `BarGenerator`
    的 Keras 代码在 [示例11-7](#example0706) 中给出。
- en: Example 11-7\. Building the `BarGenerator`
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-7\. 构建 `BarGenerator`
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_music_generation_CO2-1)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_music_generation_CO2-1)'
- en: The input to the bar generator is a vector of length `4 * Z_DIM`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: bar 生成器的输入是长度为 `4 * Z_DIM` 的向量。
- en: '[![2](Images/2.png)](#co_music_generation_CO2-2)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_music_generation_CO2-2)'
- en: After passing it through a `Dense` layer, we reshape the tensor to prepare it
    for the convolutional transpose operations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个 `Dense` 层后，我们重新塑造张量以准备进行卷积转置操作。
- en: '[![3](Images/3.png)](#co_music_generation_CO2-3)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_music_generation_CO2-3)'
- en: First we expand the tensor along the timestep axis…​
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们沿着时间步长轴扩展张量…​
- en: '[![4](Images/4.png)](#co_music_generation_CO2-4)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_music_generation_CO2-4)'
- en: …​then along the pitch axis.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: …​然后沿着音高轴。
- en: '[![5](Images/5.png)](#co_music_generation_CO2-5)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_music_generation_CO2-5)'
- en: The final layer has a tanh activation applied, as we will be using a WGAN-GP
    (which requires tanh output activation) to train the network.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层应用了tanh激活，因为我们将使用WGAN-GP（需要tanh输出激活）来训练网络。
- en: '[![6](Images/6.png)](#co_music_generation_CO2-6)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_music_generation_CO2-6)'
- en: The tensor is reshaped to add two extra dimensions of size 1, to prepare it
    for concatenation with other bars and tracks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 张量被重塑以添加两个大小为1的额外维度，以准备与其他小节和轨道连接。
- en: Putting it all together
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: Ultimately, the MuseGAN generator takes the four input noise tensors (chords,
    style, melody, and groove) and converts them into a multitrack, multibar score.
    The Keras code to build the MuseGAN generator is provided in [Example 11-8](#example0707).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，MuseGAN生成器接受四个输入噪声张量（和弦、风格、旋律和节奏），并将它们转换为一个多轨多小节乐谱。构建MuseGAN生成器的Keras代码在[示例11-8](#example0707)中提供。
- en: Example 11-8\. Building the MuseGAN generator
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-8。构建MuseGAN生成器
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_music_generation_CO3-1)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_music_generation_CO3-1)'
- en: Define the inputs to the generator.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器的输入。
- en: '[![2](Images/2.png)](#co_music_generation_CO3-2)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_music_generation_CO3-2)'
- en: Pass the chords input through the temporal network.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过时间网络传递和弦输入。
- en: '[![3](Images/3.png)](#co_music_generation_CO3-3)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_music_generation_CO3-3)'
- en: Pass the melody input through the temporal network.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过时间网络传递旋律输入。
- en: '[![4](Images/4.png)](#co_music_generation_CO3-4)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_music_generation_CO3-4)'
- en: Create an independent bar generator network for every track.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个轨道创建一个独立的小节生成器网络。
- en: '[![5](Images/5.png)](#co_music_generation_CO3-5)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_music_generation_CO3-5)'
- en: Loop over the tracks and bars, creating a generated bar for each combination.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 循环遍历轨道和小节，为每种组合创建一个生成的小节。
- en: '[![6](Images/6.png)](#co_music_generation_CO3-6)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_music_generation_CO3-6)'
- en: Concatenate everything together to form a single output tensor.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容连接在一起形成单个输出张量。
- en: '[![7](Images/7.png)](#co_music_generation_CO3-7)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_music_generation_CO3-7)'
- en: The MuseGAN model takes four distinct noise tensors as input and outputs a generated
    multitrack, multibar score.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN模型接受四个不同的噪声张量作为输入，并输出一个生成的多轨多小节乐谱。
- en: The MuseGAN Critic
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MuseGAN评论家
- en: In comparison to the generator, the critic architecture is much more straightforward
    (as is often the case with GANs).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成器相比，评论家的架构要简单得多（这在GAN中经常是这样）。
- en: The critic tries to distinguish full multitrack, multibar scores created by
    the generator from real excerpts from the Bach chorales. It is a convolutional
    neural network, consisting mostly of `Conv3D` layers that collapse the score into
    a single output prediction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家试图区分生成器创建的完整多轨多小节乐谱和巴赫赞美诗的真实节选。它是一个卷积神经网络，主要由将乐谱折叠成单个输出预测的`Conv3D`层组成。
- en: Conv3D Layers
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Conv3D层
- en: So far in this book, we have only worked with `Conv2D` layers, applicable to
    three-dimensional input images (width, height, channels). Here we have to use
    `Conv3D` layers, which are analogous to `Conv2D` layers but accept four-dimensional
    input tensors (`n_bars`, `n_steps_per_bar`, `n_pitches`, `n_tracks`).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们只使用了适用于三维输入图像（宽度、高度、通道）的`Conv2D`层。在这里，我们必须使用`Conv3D`层，它们类似于`Conv2D`层，但接受四维输入张量（`n_bars`、`n_steps_per_bar`、`n_pitches`、`n_tracks`）。
- en: We do not use batch normalization layers in the critic as we will be using the
    WGAN-GP framework for training the GAN, which forbids this.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评论家中不使用批量归一化层，因为我们将使用WGAN-GP框架来训练GAN，这是不允许的。
- en: The Keras code to build the critic is given in [Example 11-9](#example0708).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 构建评论家的Keras代码在[示例11-9](#example0708)中给出。
- en: Example 11-9\. Building the MuseGAN critic
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-9。构建MuseGAN评论家
- en: '[PRE10]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_music_generation_CO4-1)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_music_generation_CO4-1)'
- en: The input to the critic is an array of multitrack, multibar scores, each of
    shape `[N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS]`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家的输入是一个多轨多小节乐谱数组，每个形状为`[N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS]`。
- en: '[![2](Images/2.png)](#co_music_generation_CO4-2)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_music_generation_CO4-2)'
- en: First, we collapse the tensor along the bar axis. We apply `Conv3D` layers throughout
    the critic as we are working with 4D tensors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们沿着小节轴折叠张量。由于我们使用的是4D张量，所以在评论家中应用`Conv3D`层。
- en: '[![3](Images/3.png)](#co_music_generation_CO4-3)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_music_generation_CO4-3)'
- en: Next, we collapse the tensor along the pitch axis.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们沿着音高轴折叠张量。
- en: '[![4](Images/4.png)](#co_music_generation_CO4-4)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_music_generation_CO4-4)'
- en: Finally, we collapse the tensor along the timesteps axis.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们沿着时间步轴折叠张量。
- en: '[![5](Images/5.png)](#co_music_generation_CO4-5)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_music_generation_CO4-5)'
- en: The output is a `Dense` layer with a single unit and no activation function,
    as required by the WGAN-GP framework.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个具有单个单元且没有激活函数的`Dense`层，这是WGAN-GP框架所需的。
- en: Analysis of the MuseGAN
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MuseGAN的分析
- en: We can perform some experiments with our MuseGAN by generating a score, then
    tweaking some of the input noise parameters to see the effect on the output.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过生成一个乐谱，然后调整一些输入噪声参数来查看对输出的影响来进行一些实验。
- en: The output from the generator is an array of values in the range [–1, 1] (due
    to the tanh activation function of the final layer). To convert this to a single
    note for each track, we choose the note with the maximum value over all 84 pitches
    for each timestep. In the original MuseGAN paper the authors use a threshold of
    0, as each track can contain multiple notes; however, in this setting we can simply
    take the maximum to guarantee exactly one note per timestep per track, as is the
    case for the Bach chorales.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的输出是一个值范围在[-1, 1]的数组（由于最终层的tanh激活函数）。为了将其转换为每个轨道的单个音符，我们选择每个时间步的所有84个音高中具有最大值的音符。在原始的MuseGAN论文中，作者使用了阈值0，因为每个轨道可以包含多个音符；然而，在这种情况下，我们可以简单地取最大值来确保每个时间步每个轨道恰好有一个音符，这与巴赫赞美诗的情况相同。
- en: '[Figure 11-16](#musegan_experiments) shows a score that has been generated
    by the model from random normally distributed noise vectors (top left). We can
    find the closest score in the dataset (by Euclidean distance) and check that our
    generated score isn’t a copy of a piece of music that already exists in the dataset—the
    closest score is shown just below it, and we can see that it does not resemble
    our generated score.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-16](#musegan_experiments)显示了模型从随机正态分布的噪声向量生成的乐谱（左上角）。我们可以通过欧几里德距离找到数据集中最接近的乐谱，并检查我们生成的乐谱是否是数据集中已经存在的音乐片段的副本——最接近的乐谱显示在其正下方，我们可以看到它与我们生成的乐谱并不相似。'
- en: '![](Images/gdl2_1116.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: 现在让我们玩弄输入噪声来微调我们生成的乐谱。首先，我们可以尝试改变和弦噪声向量——[图11-16](#musegan_experiments)中左下角的乐谱显示了结果。我们可以看到每个轨道都已经改变，正如预期的那样，而且两个小节展现出不同的特性。在第二小节中，贝斯线更加动态，顶部音乐线的音高比第一小节更高。这是因为影响两个小节的潜在向量是不同的，因为输入和弦向量通过了一个时间网络。
- en: Figure 11-16\. Example of a MuseGAN predicted score, showing the closest real
    score in the training data and how the generated score is affected by changing
    the input noise
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-16。MuseGAN预测乐谱的示例，显示训练数据中最接近的真实乐谱以及通过改变输入噪声而影响生成乐谱的情况
- en: Let’s now play around with the input noise to tweak our generated score. First,
    we can try changing the chord noise vector—the bottom-left score in [Figure 11-16](#musegan_experiments)
    shows the result. We can see that every track has changed, as expected, and also
    that the two bars exhibit different properties. In the second bar, the baseline
    is more dynamic and the top line is higher in pitch than in the first bar. This
    is because the latent vectors that affect the two bars are different, as the input
    chord vector was passed through a temporal network.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '# 总结'
- en: When we change the style vector (top right), both bars change in a similar way.
    The whole passage has changed style from the original generated score, in a consistent
    way (i.e., the same latent vector is being used to adjust all tracks and bars).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们改变风格向量（右上角）时，两个小节以类似的方式改变。整个乐段的风格已经从原始生成的乐谱中改变，以一种一致的方式（即，相同的潜在向量被用来调整所有轨道和小节）。
- en: 'We can also alter tracks individually, through the melody and groove inputs.
    In the center-right score in [Figure 11-16](#musegan_experiments) we can see the
    effect of changing just the melody noise input for the top line of music. All
    other parts remain unaffected, but the top-line notes change significantly. Also,
    we can see a rhythmic change between the two bars in the top line: the second
    bar is more dynamic, containing faster notes than the first bar.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过旋律和节奏输入单独改变轨道。在[图11-16](#musegan_experiments)中间右侧的乐谱中，我们可以看到仅改变顶部音乐线的旋律噪声输入的效果。所有其他部分保持不变，但顶部音符发生了显著变化。此外，我们可以看到顶部音乐线两个小节之间的节奏变化：第二小节比第一小节更动态，包含比第一小节更快的音符。
- en: Lastly, the bottom-right score in the diagram shows the predicted score when
    we alter the groove input parameter for only the baseline. Again, all other parts
    remain unaffected, but the baseline is different. Moreover, the overall pattern
    of the baseline remains similar between bars, as we would expect.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图中右下角的乐谱显示了当我们仅改变贝斯的节奏输入参数时预测的乐谱。同样，所有其他部分保持不变，但贝斯部分是不同的。此外，贝斯的整体模式在小节之间保持相似，这是我们所期望的。
- en: This shows how each of the input parameters can be used to directly influence
    high-level features of the generated musical sequence, in much the same way as
    we were able to adjust the latent vectors of VAEs and GANs in previous chapters
    to alter the appearance of a generated image. One drawback to the model is that
    the number of bars to generate must be specified up front. To tackle this, the
    authors show an extension to the model that allows previous bars to be fed in
    as input, allowing the model to generate long-form scores by continually feeding
    the most recent predicted bars back in as additional input.`  `# Summary
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何每个输入参数可以直接影响生成的音乐序列的高级特征，就像我们之前能够调整VAE和GAN的潜在向量以改变生成图像的外观一样。模型的一个缺点是必须预先指定要生成的小节数。为了解决这个问题，作者展示了模型的一个扩展，允许将先前的小节作为输入馈入，使模型能够通过不断将最近预测的小节作为额外输入来生成长形乐谱。
- en: 'In this chapter we have explored two different kinds of models for music generation:
    a Transformer and a MuseGAN.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两种不同类型的音乐生成模型：Transformer和MuseGAN。
- en: The Transformer is similar in design to the networks we saw in [Chapter 9](ch09.xhtml#chapter_transformer)
    for text generation. Music and text generation share a lot of features in common,
    and often similar techniques can be used for both. We extended the Transformer
    architecture by incorporating two input and output streams, for note and duration.
    We saw how the model was able to learn about concepts such as keys and scales,
    simply by learning to accurately generate the music of Bach.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的设计类似于我们在[第9章](ch09.xhtml#chapter_transformer)中看到的用于文本生成的网络。音乐和文本生成有很多共同点，通常可以同时用于两者的类似技术。我们通过将两个输入和输出流（音符和持续时间）纳入Transformer架构来扩展了Transformer架构。我们看到模型能够通过准确生成巴赫音乐来学习关于调式和音阶等概念。
- en: We also explored how we can adapt the tokenization process to handle polyphonic
    (multitrack) music generation. Grid tokenization serializes a piano roll representation
    of the score, allowing us to train a Transformer on a single stream of tokens
    that describe which note is present in each voice, at discrete, equally spaced
    timestep intervals. Event-based tokenization produces a *recipe* that describes
    how to create the multiple lines of music in a sequential fashion, through a single
    stream of instructions. Both methods have advantages and disadvantages—the success
    or failure of a Transformer-based approach to music generation is often heavily
    dependent on the choice of tokenization method.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了如何调整标记化过程以处理多声部（多轨）音乐生成。网格标记化将乐谱的钢琴卷表示序列化，使我们能够在描述每个音轨中存在哪个音符的令牌的单个流上训练变压器，在离散的、等间隔的时间步长间隔内。基于事件的标记化产生了一个*配方*，描述了如何以顺序方式创建多行音乐，通过一系列指令的单个流。这两种方法都有优缺点——变压器基于的音乐生成方法的成功或失败往往严重依赖于标记化方法的选择。
- en: We also saw that generating music does not always require a sequential approach—MuseGAN
    uses convolutions to generate polyphonic musical scores with multiple tracks,
    by treating the score as an image where the tracks are individual channels of
    the image. The novelty of MuseGAN lies in the way the four input noise vectors
    (chords, style, melody, and groove) are organized so that it is possible to maintain
    full control over high-level features of the music. While the underlying harmonization
    is still not as perfect or varied as Bach’s, it is a good attempt at what is an
    extremely difficult problem to master and highlights the power of GANs to tackle
    a wide variety of problems.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到生成音乐并不总是需要顺序方法——MuseGAN使用卷积来生成具有多轨的多声部乐谱，将乐谱视为图像，其中轨道是图像的各个通道。MuseGAN的新颖之处在于四个输入噪声向量（和弦、风格、旋律和节奏）的组织方式，使得可以对音乐的高级特征保持完全控制。虽然底层的和声仍然不像巴赫的那样完美或多样化，但这是对一个极其难以掌握的问题的良好尝试，并突显了GAN处理各种问题的能力。
- en: '^([1](ch11.xhtml#idm45387004193120-marker)) Cheng-Zhi Anna Huang et al., “Music
    Transformer: Generating Music with Long-Term Structure,” September 12, 2018, [*https://arxiv.org/abs/1809.04281*](https://arxiv.org/abs/1809.04281).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#idm45387004193120-marker)) 黄成志安娜等人，“音乐变压器：生成具有长期结构的音乐”，2018年9月12日，[*https://arxiv.org/abs/1809.04281*](https://arxiv.org/abs/1809.04281)。
- en: '^([2](ch11.xhtml#idm45387004128000-marker)) Hao-Wen Dong et al., “MuseGAN:
    Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation
    and Accompaniment,” September 19, 2017, [*https://arxiv.org/abs/1709.06298*](https://arxiv.org/abs/1709.06298).``'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#idm45387004128000-marker)) 董浩文等人，“MuseGAN：用于符号音乐生成和伴奏的多轨序列生成对抗网络”，2017年9月19日，[*https://arxiv.org/abs/1709.06298*](https://arxiv.org/abs/1709.06298)。
