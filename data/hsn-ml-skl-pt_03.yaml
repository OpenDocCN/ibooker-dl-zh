- en: Chapter 2\. End-to-End Machine Learning Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章\. 全过程机器学习项目
- en: 'In this chapter you will work through an example project end to end, pretending
    to be a recently hired data scientist at a real estate company. This example is
    fictitious; the goal is to illustrate the main steps of a machine learning project,
    not to learn anything about the real estate business. Here are the main steps
    we will walk through:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将从头到尾完成一个示例项目，假装你是一家房地产公司新聘请的数据科学家。这个例子是虚构的；目的是说明机器学习项目的关键步骤，而不是学习有关房地产业务的知识。以下是我们将要经历的几个主要步骤：
- en: Look at the big picture.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看大局。
- en: Get the data.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据。
- en: Explore and visualize the data to gain insights.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索和可视化数据以获得洞察。
- en: Prepare the data for machine learning algorithms.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为机器学习算法准备数据。
- en: Select a model and train it.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个模型并对其进行训练。
- en: Fine-tune your model.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调你的模型。
- en: Present your solution.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示你的解决方案。
- en: Launch, monitor, and maintain your system.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动、监控和维护你的系统。
- en: Working with Real Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理真实数据
- en: 'When you are learning about machine learning, it is best to experiment with
    real-world data, not artificial datasets. Fortunately, there are thousands of
    open datasets to choose from, ranging across all sorts of domains. Here are a
    few popular open data repositories you can use to get data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你学习机器学习时，最好使用真实世界的数据进行实验，而不是人工数据集。幸运的是，有数千个公开数据集可供选择，涵盖各种领域。以下是一些你可以使用的流行公开数据仓库，以获取数据：
- en: '[Google Datasets Search](https://datasetsearch.research.google.com)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[谷歌数据集搜索](https://datasetsearch.research.google.com)'
- en: '[Hugging Face Datasets](https://huggingface.co/docs/datasets)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face 数据集](https://huggingface.co/docs/datasets)'
- en: '[OpenML.org](https://openml.org)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenML.org](https://openml.org)'
- en: '[Kaggle.com](https://kaggle.com/datasets)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle.com](https://kaggle.com/datasets)'
- en: '[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加州大学欧文分校机器学习数据仓库](https://archive.ics.uci.edu)'
- en: '[Stanford Large Network Dataset Collection](https://snap.stanford.edu/data)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[斯坦福大学大型网络数据集收藏](https://snap.stanford.edu/data)'
- en: '[Amazon’s AWS datasets](https://registry.opendata.aws)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[亚马逊的 AWS 数据集](https://registry.opendata.aws)'
- en: '[U.S. Government’s Open Data](https://data.gov)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[美国政府的开放数据](https://data.gov)'
- en: '[DataPortals.org](https://dataportals.org)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DataPortals.org](https://dataportals.org)'
- en: '[Wikipedia’s list of machine learning datasets](https://homl.info/9)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维基百科的机器学习数据集列表](https://homl.info/9)'
- en: In this chapter we’ll use the California Housing Prices dataset from the StatLib
    repository⁠^([1](ch02.html#id990)) (see [Figure 2-1](#california_housing_prices_plot)).
    This dataset is based on data from the 1990 California census. It is not exactly
    recent (a nice house in the Bay Area was still affordable at the time), but it
    has many qualities for learning, so we will pretend it is recent data. For teaching
    purposes I’ve added a categorical attribute and removed a few features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自 StatLib 仓库的加利福尼亚房价数据集（见[图 2-1](#california_housing_prices_plot)）。这个数据集基于
    1990 年加利福尼亚人口普查的数据。它并不完全是最新的（当时旧金山地区的一所好房子仍然负担得起），但它具有许多学习特性，因此我们将假装它是最近的数据。为了教学目的，我添加了一个分类属性并删除了一些特征。
- en: '![Map of California displaying housing price data with colored dots representing
    median house values and dot size indicating population density.](assets/hmls_0201.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![显示加利福尼亚房价数据，用彩色点表示中位数房屋价值，点的大小表示人口密度的加利福尼亚地图](assets/hmls_0201.png)'
- en: Figure 2-1\. California housing prices
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 加利福尼亚房价
- en: Look at the Big Picture
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看看大局
- en: Welcome to the Machine Learning Housing Corporation! Your first task is to use
    California census data to build a model of housing prices in the state. This data
    includes metrics such as the population, median income, and median housing price
    for each block group in California. Block groups are the smallest geographical
    unit for which the US Census Bureau publishes sample data (a block group typically
    has a population of 600 to 3,000 people). I will call them “districts” for short.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到机器学习住房公司！你的第一个任务是使用加利福尼亚人口普查数据来构建该州房价的模型。这些数据包括加利福尼亚每个街区组的指标，如人口、中位数收入和中位数房价。街区组是美国人口普查局发布样本数据的最小地理单位（街区组通常有
    600 到 3,000 的人口）。我将简称为“地区”。
- en: Your model should learn from this data and be able to predict the median housing
    price in any district, given all the other metrics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型应该从这些数据中学习，并能够预测任何地区的平均房价，给定所有其他指标。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since you are a well-organized data scientist, the first thing you should do
    is pull out your machine learning project checklist. You can start with the one
    at [*https://homl.info/checklist*](https://homl.info/checklist); it should work
    reasonably well for most machine learning projects, but make sure to adapt it
    to your needs. In this chapter we will go through many checklist items, but we
    will also skip a few, either because they are self-explanatory or because they
    will be discussed in later chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你是一个有组织的数据科学家，你应该做的第一件事是拿出你的机器学习项目清单。你可以从[*https://homl.info/checklist*](https://homl.info/checklist)上的清单开始；它应该对大多数机器学习项目来说都相当适用，但请确保根据你的需求进行调整。在本章中，我们将讨论许多清单项，但也会跳过一些，要么是因为它们是自我解释的，要么是因为它们将在后面的章节中讨论。
- en: Frame the Problem
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确问题
- en: The first question to ask your boss is what exactly the business objective is.
    Building a model is probably not the end goal. How does the company expect to
    use and benefit from this model? Knowing the objective is important because it
    will determine how you frame the problem, which algorithms you will select, which
    performance measure you will use to evaluate your model, and how much effort you
    will spend tweaking it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先要问你的老板的是业务目标究竟是什么。构建一个模型可能不是最终目标。公司期望如何使用和从该模型中获益？了解目标很重要，因为它将决定你如何明确问题，你将选择哪些算法，你将使用哪些性能指标来评估你的模型，以及你将投入多少精力来调整它。
- en: Your boss answers that your model’s output (a prediction of a district’s median
    housing price) will be essential to determine whether it is worth investing in
    a given area. More specifically, your model’s output will be fed to another machine
    learning system (see [Figure 2-2](#house_pricing_pipeline_diagram)), along with
    some other signals.⁠^([2](ch02.html#id994)) So it’s important to make our housing
    price model as accurate as we can.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你的老板回答说，你的模型输出（对一个区域中位数房价的预测）将对于确定是否值得在该地区投资至关重要。更具体地说，你的模型输出将被输入到另一个机器学习系统中（见[图2-2](#house_pricing_pipeline_diagram)），以及一些其他信号。⁠^([2](ch02.html#id994))
    因此，使我们的房价模型尽可能准确是非常重要的。
- en: 'The next question to ask your boss is what the current solution looks like
    (if any). The current situation will often give you a reference for performance,
    as well as insights on how to solve the problem. Your boss answers that the district
    housing prices are currently estimated manually by experts: a team gathers up-to-date
    information about a district, and when they cannot get the median housing price,
    they estimate it using complex rules.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要问你的老板当前解决方案（如果有的话）是什么样子。当前情况通常会为你提供性能的参考，以及如何解决问题的见解。你的老板回答说，区域房价目前是由专家手动估算的：一个团队收集一个区域最新的信息，当他们无法获得中位数房价时，他们会使用复杂的规则进行估算。
- en: '![Diagram showing a machine learning pipeline for real estate, highlighting
    data flow from district data to district pricing, investment analysis, and investments.](assets/hmls_0202.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![展示房地产机器学习流程的图表，突出从区域数据到区域定价、投资分析和投资的流程](assets/hmls_0202.png)'
- en: Figure 2-2\. A machine learning pipeline for real estate investments
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 房地产投资的机器学习流程
- en: This is costly and time-consuming, and their estimates are not great; in cases
    where they manage to find out the actual median housing price, they often realize
    that their estimates were off by more than 30%. This is why the company thinks
    that it would be useful to train a model to predict a district’s median housing
    price, given other data about that district. The census data looks like a great
    dataset to exploit for this purpose, since it includes the median housing prices
    of thousands of districts, as well as other data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这既昂贵又耗时，他们的估算并不理想；在那些他们设法找到实际中位数房价的情况下，他们常常发现他们的估算误差超过30%。这就是为什么公司认为，训练一个模型来预测一个区域的平均房价，给定该区域的其他数据，将会很有用。人口普查数据看起来是一个很好的数据集，可以用于此目的，因为它包括了数千个区域的中位数房价以及其他数据。
- en: 'With all this information, you are now ready to start designing your system.
    First, determine what kind of training supervision the model will need: is it
    a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement
    learning task? And is it a classification task, a regression task, or something
    else? Should you use batch learning or online learning techniques? Before you
    read on, pause and try to answer these questions for yourself.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得所有这些信息后，您现在可以开始设计您的系统了。首先，确定模型需要的训练监督类型：是监督学习、无监督学习、半监督学习、自监督学习还是强化学习任务？它是分类任务、回归任务还是其他任务？您应该使用批处理学习技术还是在线学习技术？在继续阅读之前，请暂停并尝试自己回答这些问题。
- en: Have you found the answers? Let’s see. This is clearly a typical supervised
    learning task, since the model can be trained with *labeled* examples (each instance
    comes with the expected output, i.e., the district’s median housing price). It
    is a typical regression task, since the model will be asked to predict a value.
    More specifically, this is a *multiple regression* problem, since the system will
    use multiple features to make a prediction (the district’s population, the median
    income, etc.). It is also a *univariate regression* problem, since we are only
    trying to predict a single value for each district. If we were trying to predict
    multiple values per district, it would be a *multivariate regression* problem.
    Finally, there is no continuous flow of data coming into the system, there is
    no particular need to adjust to changing data rapidly, and the data is small enough
    to fit in memory, so plain batch learning should do just fine.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您找到答案了吗？让我们看看。这显然是一个典型的监督学习任务，因为模型可以用*标记*的例子进行训练（每个实例都带有预期的输出，即该地区的平均房价）。它是一个典型的回归任务，因为模型将被要求预测一个值。更具体地说，这是一个*多元回归*问题，因为系统将使用多个特征进行预测（地区的总人口、平均收入等）。它也是一个*单变量回归*问题，因为我们只尝试预测每个地区的单个值。如果我们试图为每个地区预测多个值，那么它将是一个*多元回归*问题。最后，系统中没有连续的数据流进入，没有特别需要快速调整数据的需求，而且数据量足够小，可以放入内存，因此普通的批处理学习应该就足够了。
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the data were huge, you could either split your batch learning work across
    multiple servers (using the MapReduce technique) or use an online learning technique.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据量巨大，您可以选择将批处理学习工作分散到多个服务器上（使用MapReduce技术）或使用在线学习技术。
- en: Select a Performance Measure
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择性能指标
- en: Your next step is to select a performance measure. A typical performance measure
    for regression problems is the *root mean squared error* (RMSE). It gives an idea
    of how much error the system typically makes in its predictions, with a higher
    weight given to large errors. [Equation 2-1](#rmse_equation) shows the mathematical
    formula to compute the RMSE.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您的下一步是选择一个性能指标。对于回归问题，一个典型的性能指标是*均方根误差*（RMSE）。它给出了系统在预测中通常犯多少错误的概览，对大误差给予更高的权重。[方程式2-1](#rmse_equation)展示了计算RMSE的数学公式。
- en: Equation 2-1\. Root mean squared error (RMSE)
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式2-1\. 均方根误差 (RMSE)
- en: $RMSE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartRoot StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts left-parenthesis h left-parenthesis bold x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis minus y Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis squared EndRoot$
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $RMSE = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (h(\boldsymbol{x}_i, \boldsymbol{y},
    h) - y_i)^2}$
- en: 'Although the RMSE is generally the preferred performance measure for regression
    tasks, in some contexts you may prefer to use another function, especially when
    there are many outliers in the data, as the RMSE is quite sensitive to them. In
    that case, you may consider using the *mean absolute error* (MAE, also called
    the *average absolute deviation*), shown in [Equation 2-2](#mae_equation):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RMSE通常是回归任务的首选性能指标，但在某些情况下，您可能更喜欢使用另一个函数，尤其是在数据中有许多异常值时，因为RMSE对它们相当敏感。在这种情况下，您可以考虑使用*平均绝对误差*（MAE，也称为*平均绝对偏差*），如[方程式2-2](#mae_equation)所示：
- en: Equation 2-2\. Mean absolute error (MAE)
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式2-2\. 平均绝对误差 (MAE)
- en: $MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis
    i right-parenthesis Baseline EndAbsoluteValue$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $MAE left-parenthesis bold upper X comma bold y comma h right-parenthesis equals
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts StartAbsoluteValue h left-parenthesis bold x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis minus y Superscript left-parenthesis
    i right-parenthesis Baseline EndAbsoluteValue$
- en: 'Both the RMSE and the MAE are ways to measure the distance between two vectors:
    the vector of predictions and the vector of target values. Various distance measures,
    or *norms*, are possible:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE和MAE都是衡量两个向量之间距离的方法：预测向量和目标值向量。可能的距离度量，或称**范数**，有很多种：
- en: 'Computing the root of a sum of squares (RMSE) corresponds to the *Euclidean
    norm*: this is the notion of distance we are all familiar with. It is also called
    the ℓ[2] *norm*, denoted ∥ · ∥[2] (or just ∥ · ∥).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算平方和的根（RMSE）对应于**欧几里得范数**：这是我们所有人都熟悉的距离概念。它也称为ℓ[2] **范数**，表示为∥ · ∥[2]（或简称∥
    · ∥）。
- en: Computing the sum of absolutes (MAE) corresponds to the ℓ[1] *norm*, denoted
    ∥ · ∥[1]. This is sometimes called the *Manhattan norm* because it measures the
    distance between two points in a city if you can only travel along orthogonal
    city blocks.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算绝对值之和（MAE）对应于ℓ[1] **范数**，表示为∥ · ∥[1]。有时它被称为**曼哈顿范数**，因为它测量了在只能沿正交城市街区行走的条件下，两个点之间的距离。
- en: More generally, the ℓ[*k*] *norm* of a vector **v** containing *n* elements
    is defined as ∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*) + ... + |*v*[*n*]|^(*k*))^(1/*k*).
    ℓ[0] gives the number of nonzero elements in the vector, and ℓ[∞] gives the maximum
    absolute value in the vector.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更一般地，包含*n*个元素的向量**v**的ℓ[*k*] **范数**定义为 ∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*)
    + ... + |*v*[*n*]|^(*k*))^(1/*k*). ℓ[0]给出向量中非零元素的数量，而ℓ[∞]给出向量中的最大绝对值。
- en: The higher the norm index, the more it focuses on large values and neglects
    small ones. This is why the RMSE is more sensitive to outliers than the MAE. But
    when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
    very well and is generally preferred.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 范数指数越高，它就越关注大值而忽略小值。这就是为什么RMSE比MAE对异常值更敏感。但是，当异常值非常罕见（如钟形曲线中）时，RMSE表现非常好，通常更受欢迎。
- en: Check the Assumptions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查假设
- en: Lastly, it is good practice to list and verify the assumptions that have been
    made so far (by you or others); this can help you catch serious issues early on.
    For example, the district prices that your system outputs are going to be fed
    into a downstream machine learning system, and you assume that these prices are
    going to be used as such. But what if the downstream system converts the prices
    into categories (e.g., “cheap”, “medium”, or “expensive”) and then uses those
    categories instead of the prices themselves? In this case, getting the price perfectly
    right is not important at all; your system just needs to get the category right.
    If that’s so, then the problem should have been framed as a classification task,
    not a regression task. You don’t want to find this out after working on a regression
    system for months.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，列出并验证到目前为止（由你或其他人）所做出的假设是一个好的实践；这可以帮助你及早发现严重问题。例如，你的系统输出的区域价格将被输入到下游机器学习系统中，你假设这些价格将被用作此类。但是，如果下游系统将价格转换为类别（例如，“便宜”、“中等”或“昂贵”），然后使用这些类别而不是价格本身，那会怎样？在这种情况下，价格完全正确并不重要；你的系统只需要正确分类。如果是这样，那么问题应该被界定为分类任务，而不是回归任务。你不想在为回归系统工作了数月之后才发现这一点。
- en: Fortunately, after talking with the team in charge of the downstream system,
    you are confident that they do indeed need the actual prices, not just categories.
    Great! You’re all set, the lights are green, and you can start coding now!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在与负责下游系统的团队交谈后，你确信他们确实需要实际的价格，而不仅仅是类别。太好了！一切准备就绪，绿灯亮了，你现在可以开始编码了！
- en: Get the Data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and
    walk through the code examples. As I mentioned in the preface, all the code examples
    in this book are open source and available [online](https://github.com/ageron/handson-mlp)
    as Jupyter notebooks, which are interactive documents containing text, images,
    and executable code snippets (Python in our case). In this book I will assume
    you are running these notebooks on Google Colab, a free service that lets you
    run any Jupyter notebook directly online, without having to install anything on
    your machine. If you want to use another online platform (e.g., Kaggle) or if
    you want to install everything locally on your own machine, please see the instructions
    on the book’s GitHub page.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候动手实践了。不要犹豫，拿起您的笔记本电脑，浏览代码示例。正如我在前言中提到的，本书中的所有代码示例都是开源的，并以Jupyter笔记本的形式在线提供[在线](https://github.com/ageron/handson-mlp)，这些是包含文本、图像和可执行代码片段（在我们的案例中是Python）的交互式文档。在这本书中，我将假设您正在Google
    Colab上运行这些笔记本，这是一个免费服务，允许您直接在线运行任何Jupyter笔记本，而无需在您的机器上安装任何东西。如果您想使用另一个在线平台（例如Kaggle）或如果您想在您的机器上本地安装所有内容，请参阅本书GitHub页面上的说明。
- en: Running the Code Examples Using Google Colab
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Colab运行代码示例
- en: 'First, open a web browser and visit [*https://homl.info/colab-p*](https://homl.info/colab-p):
    this will lead you to Google Colab, and it will display the list of Jupyter notebooks
    for this book (see [Figure 2-3](#google_colab_notebook_list)). You will find one
    notebook per chapter, plus a few extra notebooks and tutorials for NumPy, Matplotlib,
    Pandas, linear algebra, and differential calculus. For example, if you click *02_end_to_end_machine_learning_project.ipynb*,
    the notebook from [Chapter 2](#project_chapter) will open up in Google Colab (see
    [Figure 2-4](#notebook_in_colab)).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开一个网页浏览器并访问[*https://homl.info/colab-p*](https://homl.info/colab-p)：这将带您进入Google
    Colab，并显示本书的Jupyter笔记本列表（见[图2-3](#google_colab_notebook_list)）。您将找到每个章节的一个笔记本，以及一些额外的NumPy、Matplotlib、Pandas、线性代数和微分计算的笔记本和教程。例如，如果您点击*02_end_to_end_machine_learning_project.ipynb*，来自[第2章](#project_chapter)的笔记本将在Google
    Colab中打开（见[图2-4](#notebook_in_colab)）。
- en: A Jupyter notebook is composed of a list of cells. Each cell contains either
    executable code or text. Try double-clicking the first text cell (which contains
    the sentence “Welcome to Machine Learning Housing Corp.!”). This will open the
    cell for editing. Notice that Jupyter notebooks use Markdown syntax for formatting
    (e.g., `**bold**`, `*italics*`, `# Title`, `[url](link text)`, and so on). Try
    modifying this text, then press Shift-Enter to see the result.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本由一系列单元格组成。每个单元格包含可执行代码或文本。尝试双击第一个文本单元格（包含句子“欢迎来到机器学习住房公司！”）。这将打开单元格以进行编辑。请注意，Jupyter笔记本使用Markdown语法进行格式化（例如，`**粗体**`，`*斜体*`，`#
    标题`，`[url](链接文本)`等）。尝试修改此文本，然后按Shift-Enter查看结果。
- en: '![Google Colab interface showing a list of Jupyter notebooks in the "ageron/handson-mlp"
    repository on GitHub, with "02_end_to_end_machine_learning_project.ipynb" highlighted.](assets/hmls_0203.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab界面显示GitHub上“ageron/handson-mlp”存储库中的Jupyter笔记本列表，其中“02_end_to_end_machine_learning_project.ipynb”突出显示。](assets/hmls_0203.png)'
- en: Figure 2-3\. List of notebooks in Google Colab
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. Google Colab中的笔记本列表
- en: '![Screenshot of a Google Colab notebook showing a section titled "Chapter 2
    – End-to-end Machine Learning project," with instructions for editing and running
    text and code cells.](assets/hmls_0204.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab笔记本的截图，显示标题为“第2章 – 端到端机器学习项目”的部分，包含编辑和运行文本和代码单元格的说明。](assets/hmls_0204.png)'
- en: Figure 2-4\. Your notebook in Google Colab
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 您在Google Colab中的笔记本
- en: Next, create a new code cell by selecting Insert → “Code cell” from the menu.
    Alternatively, you can click the + Code button in the toolbar, or hover your mouse
    over the bottom of a cell until you see + Code and + Text appear, then click +
    Code. In the new code cell, type some Python code, such as `print("Hello World")`,
    then press Shift-Enter to run this code (or click the ▷ button on the left side
    of the cell).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过选择菜单中的“插入”→“代码单元格”来创建一个新的代码单元格。或者，您也可以点击工具栏上的+代码按钮，或者将鼠标悬停在单元格底部直到看到+代码和+文本出现，然后点击+代码。在新的代码单元格中，输入一些Python代码，例如`print("Hello
    World")`，然后按Shift-Enter运行此代码（或点击单元格左侧的▷按钮）。
- en: If you’re not logged in to your Google account, you’ll be asked to log in now
    (if you don’t already have a Google account, you’ll need to create one). Once
    you are logged in, when you try to run the code you’ll see a security warning
    telling you that this notebook was not authored by Google. A malicious person
    could create a notebook that tries to trick you into entering your Google credentials
    so they can access your personal data, so before you run a notebook, always make
    sure you trust its author (or double-check what each code cell will do before
    running it). Assuming you trust me (or you plan to check every code cell), you
    can now click “Run anyway”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尚未登录您的Google账户，现在会被要求登录（如果您还没有Google账户，您需要创建一个）。一旦登录，当你尝试运行代码时，会看到一个安全警告，告诉你这个笔记本不是由Google编写的。恶意的人可能会创建一个试图诱骗你输入Google凭证的笔记本，以便他们可以访问你的个人数据，所以在运行笔记本之前，总是确保你信任其作者（或者运行之前双重检查每个代码单元将执行的操作）。假设你信任我（或者你计划检查每个代码单元），你现在可以点击“仍然运行”。
- en: 'Colab will then allocate a new *runtime* for you: this is a free virtual machine
    located on Google’s servers that contains a bunch of tools and Python libraries,
    including everything you’ll need for most chapters (in some chapters, you’ll need
    to run a command to install additional libraries). This will take a few seconds.
    Next, Colab will automatically connect to this runtime and use it to execute your
    new code cell. Importantly, the code runs on the runtime, *not* on your machine.
    The code’s output will be displayed under the cell. Congrats, you’ve run some
    Python code on Colab!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Colab将为您分配一个新的*运行时间*：这是一个位于Google服务器上的免费虚拟机，其中包含许多工具和Python库，包括大多数章节所需的所有内容（在某些章节中，您需要运行命令来安装额外的库）。这需要几秒钟。接下来，Colab将自动连接到这个运行时间并使用它来执行您的新代码单元。重要的是，代码是在运行时间上运行的，*而不是*在您的机器上。代码的输出将显示在单元格下方。恭喜你，你在Colab上运行了一些Python代码！
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed
    by A (to insert above the active cell) or B (to insert below). There are many
    other keyboard shortcuts available: you can view and edit them by typing Ctrl-M
    (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own
    machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter
    extension, you will see some minor differences—runtimes are called *kernels*,
    the user interface and keyboard shortcuts are slightly different, etc.—but switching
    from one Jupyter environment to another is not too hard.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要插入一个新的代码单元，你也可以输入Ctrl-M（或在macOS上输入Cmd-M）然后按A（在活动单元上方插入）或B（在活动单元下方插入）。还有许多其他的快捷键可用：你可以通过输入Ctrl-M（或在macOS上输入Cmd-M）然后按H来查看和编辑它们。如果你选择在Kaggle或在自己的机器上使用JupyterLab或带有Jupyter扩展的IDE（如Visual
    Studio Code）运行笔记本，你会看到一些细微的差异——运行时间被称为*kernels*，用户界面和快捷键略有不同等——但从一种Jupyter环境切换到另一种并不太难。
- en: Saving Your Code Changes and Your Data
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存您的代码更改和您的数据
- en: You can make changes to a Colab notebook, and they will persist for as long
    as you keep your browser tab open. But once you close it, the changes will be
    lost. To avoid this, make sure you save a copy of the notebook to your Google
    Drive by selecting File → “Save a copy in Drive”. Alternatively, you can download
    the notebook to your computer by selecting File → Download → “Download .ipynb”.
    Then you can later visit [*https://colab.research.google.com*](https://colab.research.google.com)
    and open the notebook again (either from Google Drive or by uploading it from
    your computer).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对Colab笔记本进行更改，并且只要你的浏览器标签页保持打开，这些更改就会持续存在。但一旦关闭，更改就会丢失。为了避免这种情况，请确保通过选择“文件”→“在驱动器中保存副本”将笔记本的副本保存到您的Google
    Drive。或者，您可以通过选择“文件”→“下载”→“下载 .ipynb”将笔记本下载到您的计算机。然后您可以在稍后访问[*https://colab.research.google.com*](https://colab.research.google.com)并再次打开笔记本（无论是从Google
    Drive还是从您的计算机上传）。
- en: Warning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Google Colab is meant only for interactive use: you can play around in the
    notebooks and tweak the code as you like, but you cannot let the notebooks run
    unattended for a long period of time, or else the runtime will be shut down and
    all of its data will be lost.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab仅适用于交互式使用：你可以在笔记本中随意操作并按需调整代码，但你不能让笔记本长时间无人看管，否则运行时间将被关闭，所有数据都将丢失。
- en: If the notebook generates data that you care about, make sure you download this
    data before the runtime shuts down. To do this, click the Files icon (see step
    1 in [Figure 2-5](#save_data_google_colab)), find the file you want to download,
    click the vertical dots next to it (step 2), and click Download (step 3). Alternatively,
    you can mount your Google Drive on the runtime, allowing the notebook to read
    and write files directly to Google Drive as if it were a local directory. For
    this, click the Files icon (step 1), then click the Google Drive icon (circled
    in [Figure 2-5](#save_data_google_colab)) and follow the on-screen instructions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果笔记本生成了你关心的数据，确保在运行时间关闭之前下载这些数据。为此，点击文件图标（见[图2-5](#save_data_google_colab)中的步骤1），找到你想要下载的文件，点击它旁边的垂直点（步骤2），然后点击下载（步骤3）。或者，你可以在运行时挂载你的Google
    Drive，使笔记本能够直接将文件读写到Google Drive，就像它是一个本地目录一样。为此，点击文件图标（步骤1），然后点击Google Drive图标（[图2-5](#save_data_google_colab)中圈出的图标）并遵循屏幕上的说明。
- en: '![Screenshot of Google Colab interface showing steps to download a file or
    mount Google Drive, with icons and menu options highlighted.](assets/hmls_0205.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Google Colab界面截图，显示下载文件或挂载Google Drive的步骤，图标和菜单选项已突出显示](assets/hmls_0205.png)'
- en: Figure 2-5\. Downloading a file from a Google Colab runtime (steps 1 to 3),
    or mounting your Google Drive (circled icon)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 从Google Colab运行时下载文件（步骤1到3），或挂载你的Google Drive（圈出图标）
- en: 'By default, your Google Drive will be mounted at */content/drive/MyDrive*.
    If you want to back up a data file, simply copy it to this directory by running
    `!cp [.keep-together]#/content/my_great_model /content/drive/MyDrive`.# Any command
    starting with a bang (`!`) is treated as a shell command, not as Python code:
    `cp` is the Linux shell command to copy a file from one path to another. Note
    that Colab runtimes run on Linux (specifically, Ubuntu).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，你的Google Drive将挂载在*/content/drive/MyDrive*。如果你想备份一个数据文件，只需运行`!cp [.keep-together]#/content/my_great_model
    /content/drive/MyDrive`将其复制到这个目录。# 任何以感叹号（`!`）开头的命令都被视为shell命令，而不是Python代码：`cp`是Linux
    shell命令，用于将文件从一个路径复制到另一个路径。请注意，Colab运行时在Linux上运行（具体来说是Ubuntu）。
- en: The Power and Danger of Interactivity
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互式功能的强大与危险
- en: 'Jupyter notebooks are interactive, and that’s a great thing: you can run each
    cell one by one, stop at any point, insert a cell, play with the code, go back
    and run the same cell again, etc., and I highly encourage you to do so. If you
    just run the cells one by one without ever playing around with them, you won’t
    learn as fast. However, this flexibility comes at a price: it’s very easy to run
    cells in the wrong order, or to forget to run a cell. If this happens, the subsequent
    code cells are likely to fail. For example, the very first code cell in each notebook
    contains setup code (such as imports), so make sure you run it first, or else
    nothing will work.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本是交互式的，这是一个非常好的特性：你可以逐个运行每个单元格，在任何地方停止，插入一个单元格，玩弄代码，返回并再次运行相同的单元格，等等，我强烈建议你这样做。如果你只是逐个运行单元格而不进行任何操作，你将不会学得很快。然而，这种灵活性是有代价的：很容易以错误的顺序运行单元格，或者忘记运行一个单元格。如果发生这种情况，后续的代码单元格很可能会失败。例如，每个笔记本中的第一个代码单元格包含设置代码（例如导入），所以请确保你首先运行它，否则将无法工作。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you ever run into a weird error, try restarting the runtime (by selecting
    Runtime → “Restart runtime” from the menu) and then run all the cells again from
    the beginning of the notebook. This often solves the problem. If not, it’s likely
    that one of the changes you made broke the notebook: just revert to the original
    notebook and try again. If it still fails, please file an issue on GitHub.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到任何奇怪的错误，尝试重新启动运行时（通过从菜单中选择“运行时”→“重新启动运行时”），然后从笔记本的开始处再次运行所有单元格。这通常可以解决问题。如果不奏效，很可能是你做的某个更改破坏了笔记本：只需恢复到原始笔记本并再次尝试。如果仍然失败，请在GitHub上提交一个问题。
- en: Book Code Versus Notebook Code
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 书中代码与笔记本代码的差异
- en: 'You may sometimes notice some little differences between the code in this book
    and the code in the notebooks. This may happen for several reasons:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时可能会注意到这本书中的代码和笔记本中的代码之间有一些小小的差异。这种情况可能由几个原因造成：
- en: 'A library may have changed slightly by the time you read these lines, or perhaps
    despite my best efforts I made an error in the book. Sadly, I cannot magically
    fix the code in your copy of this book (unless you are reading an electronic copy
    and you can download the latest version), but I *can* fix the notebooks. So, if
    you run into an error after copying code from this book, please look for the fixed
    code in the notebooks: I will strive to keep them error-free and up-to-date with
    the latest library versions.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你阅读这些内容时，一个库可能已经略有变化，或者也许尽管我尽了最大努力，但在书中我可能犯了一个错误。遗憾的是，我无法神奇地修复你这本书中的代码（除非你正在阅读电子版并且可以下载最新版本），但我可以修复笔记本。所以，如果你从这本书中复制代码后遇到错误，请查找笔记本中的修复代码：我将努力保持它们无错误并且与最新库版本保持更新。
- en: The notebooks contain some extra code to beautify the figures (adding labels,
    setting font sizes, etc.) and to save them in high resolution for this book. You
    can safely ignore this extra code if you want.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本包含一些额外的代码来美化图形（添加标签、设置字体大小等），并将它们以高分辨率保存到这本书中。如果你想忽略这些额外的代码，你可以安全地忽略它们。
- en: 'I optimized the code for readability and simplicity: I made it as linear and
    flat as possible, defining very few functions or classes. The goal is to ensure
    that the code you are running is generally right in front of you, and not nested
    within several layers of abstractions that you have to search through. This also
    makes it easier for you to play with the code. For simplicity, there’s limited
    error handling, and I placed some of the least common imports right where they
    are needed (instead of placing them at the top of the file, as is recommended
    by the PEP 8 Python style guide). That said, your production code will not be
    very different: just a bit more modular, and with additional tests and error handling.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我优化了代码的可读性和简洁性：我使其尽可能线性和平坦，定义了非常少的函数或类。目标是确保你运行的代码通常就在你面前，而不是嵌套在几层抽象之中，你需要搜索才能找到。这也使得你可以更容易地玩转代码。为了简单起见，错误处理有限，我将一些不太常见的导入放在了它们所需的位置（而不是像PEP
    8 Python风格指南推荐的那样放在文件顶部）。话虽如此，你的生产代码不会有很大不同：只是稍微模块化一些，并增加了额外的测试和错误处理。
- en: OK! Once you’re comfortable with Colab, you’re ready to download the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！一旦你熟悉了Colab，你就可以下载数据了。
- en: Download the Data
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'In typical environments your data would be available in a relational database
    or some other common data store, and spread across multiple tables/documents/files.
    To access it, you would first need to get your credentials and access authorizations
    and familiarize yourself with the data schema.⁠^([4](ch02.html#id1032)) In this
    project, however, things are much simpler: you will just download a single compressed
    file, *housing.tgz*, which contains a comma-separated values (CSV) file called
    *housing.csv* with all the data.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的环境中，你的数据将存储在关系数据库或其他常见的数据存储中，并分布在多个表/文档/文件中。要访问它，你首先需要获取你的凭证和访问授权，并熟悉数据模式。然而，在这个项目中，事情要简单得多：你只需下载一个单个的压缩文件*housing.tgz*，它包含一个名为*housing.csv*的逗号分隔值（CSV）文件，其中包含所有数据。
- en: 'Rather than manually downloading and decompressing the data, it’s usually preferable
    to write a function that does it for you. This is useful in particular if the
    data changes regularly: you can write a small script that uses the function to
    fetch the latest data (or you can set up a scheduled job to do that automatically
    at regular intervals). Automating the process of fetching the data is also useful
    if you need to install the dataset on multiple machines.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动下载和解压缩数据相比，通常最好是编写一个为你完成这些工作的函数。特别是如果数据经常变化，这很有用：你可以编写一个小脚本，使用该函数获取最新数据（或者你可以设置一个计划任务，定期自动执行该操作）。如果需要将数据集安装到多台机器上，自动化获取数据的过程也很有用。
- en: 'Here is the function to fetch and load the data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是获取和加载数据的函数：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you get an SSL `CERTIFICATE_VERIFY_FAILED` error on macOS, then you most
    likely need to install the `certifi` package, as explained at [*https://homl.info/sslerror*](https://homl.info/sslerror).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到SSL `CERTIFICATE_VERIFY_FAILED`错误，那么你很可能是需要安装`certifi`包，如[*https://homl.info/sslerror*](https://homl.info/sslerror)中所述。
- en: When `load_housing_data()` is called, it looks for the *datasets/housing.tgz*
    file. If it does not find it, it creates the *datasets* directory inside the current
    directory (which is */content* by default, in Colab), downloads the *housing.tgz*
    file from the *ageron/data* GitHub repository, and extracts its content into the
    *datasets* directory; this creates the *datasets*/*housing* directory with the
    *housing.csv* file inside it. Lastly, the function loads this CSV file into a
    Pandas DataFrame object containing all the data, and returns it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`load_housing_data()`时，它会寻找`datasets/housing.tgz`文件。如果找不到，它会在当前目录（在Colab中默认为`/content`）内创建`datasets`目录，从`ageron/data`
    GitHub仓库下载`housing.tgz`文件，并将其内容提取到`datasets`目录中；这将在`datasets`目录下创建一个名为`housing`的目录，其中包含`housing.csv`文件。最后，该函数将这个CSV文件加载到一个包含所有数据的Pandas
    DataFrame对象中，并返回它。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are using Python 3.12 or 3.13, you should add `filter=''data''` to the
    `extractall()` method’s arguments: this limits what the extraction algorithm can
    do and improves security (see the documentation for more details).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Python 3.12或3.13，你应该在`extractall()`方法的参数中添加`filter='data'`：这限制了提取算法可以执行的操作，并提高了安全性（更多详情请参阅文档）。
- en: Take a Quick Look at the Data Structure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速查看数据结构
- en: You start by looking at the top five rows of data using the DataFrame’s `head()`
    method (see [Figure 2-6](#housing_head_screenshot)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过DataFrame的`head()`方法查看数据的前五行（见图2-6）。
- en: '![A screenshot showing the top five rows of a housing dataset, including attributes
    such as longitude, latitude, housing median age, median income, ocean proximity,
    and median house value.](assets/hmls_0206.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![展示住房数据集前五行截图，包括经度、纬度、住房中位数年龄、中位数收入、海洋邻近度和中位数房价的截图](assets/hmls_0206.png)'
- en: Figure 2-6\. Top five rows in the dataset
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 数据集中的前五行
- en: 'Each row represents one district. There are 10 attributes (they are not all
    shown in the screenshot): `longitude`, `latitude`, `housing_median_age`, `total_rooms`,
    `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`,
    and `ocean_proximity`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行代表一个区域。共有10个属性（它们并非都在截图显示中）：`longitude`（经度）、`latitude`（纬度）、`housing_median_age`（住房中位数年龄）、`total_rooms`（总房间数）、`total_bedrooms`（总卧室数）、`population`（人口）、`households`（家庭数）、`median_income`（中位数收入）、`median_house_value`（中位数房价）和`ocean_proximity`（海洋邻近度）。
- en: 'The `info()` method is useful to get a quick description of the data, in particular
    the total number of rows, each attribute’s type, and the number of non-null values:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`info()` 方法非常有用，可以快速获取数据的描述，特别是总行数、每个属性的类型以及非空值的数量：'
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2] >>> housing_full["ocean_proximity"].value_counts() `ocean_proximity`
    `<1H OCEAN     9136` `INLAND        6551` `NEAR OCEAN    2658` `NEAR BAY      2290`
    `ISLAND           5` `Name: count, dtype: int64` [PRE3]`Let’s look at the other
    fields. The `describe()` method shows a summary of the numerical attributes ([Figure 2-7](#housing_describe_screenshot)).  ![A
    table generated by the `describe()` method, displaying statistical summaries for
    numerical attributes such as count, mean, standard deviation, minimum, maximum,
    and percentiles.](assets/hmls_0207.png)  ###### Figure 2-7\. Summary of each numerical
    attribute    The `count`, `mean`, `min`, and `max` rows are self-explanatory.
    Note that the null values are ignored (so, for example, the `count` of `total_bedrooms`
    is 20,433, not 20,640). The `std` row shows the *standard deviation*, which measures
    how dispersed the values are.⁠^([5](ch02.html#id1038)) The `25%`, `50%`, and `75%`
    rows show the corresponding *percentiles*: a percentile indicates the value below
    which a given percentage of observations in a group of observations fall. For
    example, 25% of the districts have a `housing_median_age` lower than 18, while
    50% are lower than 29, and 75% are lower than 37\. These are often called the
    25th percentile (or first *quartile*), the median, and the 75th percentile (or
    third quartile).    Another quick way to get a feel of the type of data you are
    dealing with is to plot a histogram for each numerical attribute. A histogram
    shows the number of instances (on the vertical axis) that have a given value range
    (on the horizontal axis). You can either plot this one attribute at a time, or
    you can call the `hist()` method on the whole dataset (as shown in the following
    code example), and it will plot a histogram for each numerical attribute (see
    [Figure 2-8](#attribute_histogram_plots)).  ![Histograms displaying the distribution
    of various numerical attributes such as longitude, latitude, housing median age,
    total rooms, total bedrooms, population, households, median income, and median
    house value.](assets/hmls_0208.png)  ###### Figure 2-8\. A histogram for each
    numerical attribute    The number of value ranges can be adjusted using the `bins`
    argument (try playing with it to see how it affects the histograms):    [PRE4]    Looking
    at these histograms, you notice a few things:    *   First, the median income
    attribute does not look like it is expressed in US dollars (USD). After checking
    with the team that collected the data, you are told that the data has been scaled
    and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually,
    0.4999) for lower median incomes. The numbers represent roughly tens of thousands
    of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes
    is common in machine learning, and it is not necessarily a problem, but you should
    try to understand how the data was computed.           *   The housing median
    age and the median house value were also capped. The latter may be a serious problem
    since it is your target attribute (your labels). Your machine learning algorithms
    may learn that prices never go beyond that limit. You need to check with your
    client team (the team that will use your system’s output) to see if this is a
    problem or not. If they tell you that they need precise predictions even beyond
    $500,000, then you have two options:               *   Collect proper labels for
    the districts whose labels were capped.                       *   Remove those
    districts from the training set (and also from the test set, since your system
    should not be evaluated poorly if it predicts values beyond $500,000).                   *   These
    attributes have very different scales. We will discuss this later in this chapter
    when we explore feature scaling.           *   Finally, many histograms are *skewed
    right*: they extend much farther to the right of the median than to the left.
    This may make it a bit harder for some machine learning algorithms to detect patterns.
    Later, you’ll try transforming these attributes to have more symmetrical and bell-shaped
    distributions.              You should now have a better understanding of the
    kind of data you’re dealing with.[PRE5]``  [PRE6][PRE7]py[PRE8] Well, this works,
    but it is not perfect: if you run the program again, it will generate a different
    test set! Over time, you (or your machine learning algorithms) will get to see
    the whole dataset, which is what you want to avoid.    One solution is to save
    the test set on the first run and then load it in subsequent runs. Another option
    is to set the random number generator’s seed (e.g., by passing `seed=42` to the
    `default_rng()` function)⁠^([6](ch02.html#id1054)) to ensure it always generates
    the same sequence of random numbers every time you run the program.    However,
    both these solutions will break the next time you fetch an updated dataset. To
    have a stable train/test split even after updating the dataset, a common solution
    is to use each instance’s identifier to decide whether it should go in the test
    set (assuming instances have unique and immutable identifiers). For example, you
    could compute a hash of each instance’s identifier and put that instance in the
    test set if the hash is lower than or equal to 20% of the maximum hash value.
    This ensures that the test set will remain consistent across multiple runs, even
    if you refresh the dataset. The new test set will contain 20% of the new instances,
    but it will not contain any instance that was previously in the training set.    Here
    is a possible implementation:    [PRE9]    Unfortunately, the housing dataset
    does not have an identifier column. The simplest solution is to use the row index
    as the ID:    [PRE10]    If you use the row index as a unique identifier, you
    need to make sure that new data gets appended to the end of the dataset and that
    no row ever gets deleted. If this is not possible, then you can try to use the
    most stable features to build a unique identifier. For example, a district’s latitude
    and longitude are guaranteed to be stable for a few million years, so you could
    combine them into an ID like so:⁠^([7](ch02.html#id1055))    [PRE11]    Scikit-Learn
    provides a few functions to split datasets into multiple subsets in various ways.
    The simplest function is `train_test_split()`, which does pretty much the same
    thing as the `shuffle_and_split_data()` function we defined earlier, with a couple
    of additional features. First, there is a `random_state` parameter that allows
    you to set the random generator seed. Second, you can pass it multiple datasets
    with an identical number of rows, and it will split them on the same indices (this
    is very useful, for example, if you have a separate DataFrame for labels):    [PRE12]    So
    far we have considered purely random sampling methods. This is generally fine
    if your dataset is large enough (especially relative to the number of attributes),
    but if it is not, you run the risk of introducing a significant sampling bias.
    When employees at a survey company decide to call 1,000 people to ask them a few
    questions, they don’t just pick 1,000 people randomly in a phone book. They try
    to ensure that these 1,000 people are representative of the whole population,
    with regard to the questions they want to ask. For example, according to the US
    Census Bureau, 51.6% of citizens of voting age are female, while 48.4% are male,
    so a well-conducted survey in the US would try to maintain this ratio in the sample:
    516 females and 484 males (at least if it seems likely that the answers may vary
    across genders). This is called *stratified sampling*: the population is divided
    into homogeneous subgroups called *strata*, and the right number of instances
    are sampled from each stratum to guarantee that the test set is representative
    of the overall population. If the people running the survey used purely random
    sampling, there would be over 10% chance of sampling a skewed test set with less
    than 49% female or more than 54% female participants. Either way, the survey results
    would likely be quite biased.    Suppose you’ve chatted with some experts who
    told you that the median income is a very important attribute to predict median
    housing prices. You may want to ensure that the test set is representative of
    the various categories of incomes in the whole dataset. Since the median income
    is a continuous numerical attribute, you first need to create an income category
    attribute. Let’s look at the median income histogram more closely (back in [Figure 2-8](#attribute_histogram_plots)):
    most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000),
    but some median incomes go far beyond 6\. It is important to have a sufficient
    number of instances in your dataset for each stratum, or else the estimate of
    a stratum’s importance may be biased. This means that you should not have too
    many strata, and each stratum should be large enough. The following code uses
    the `pd.cut()` function to create an income category attribute with five categories
    (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000),
    category 2 from 1.5 to 3, and so on:    [PRE13]    These income categories are
    represented in [Figure 2-9](#housing_income_cat_bar_plot):    [PRE14]    Now you
    are ready to do stratified sampling based on the income category. Scikit-Learn
    provides a number of splitter classes in the `sklearn.model_selection` package
    that implement various strategies to split your dataset into a training set and
    a test set. Each splitter has a `split()` method that returns an iterator over
    different training/test splits of the same data.  ![Bar chart illustrating the
    distribution of districts across five income categories, with categories two and
    three having the highest counts.](assets/hmls_0209.png)  ###### Figure 2-9\. Histogram
    of income categories    To be precise, the `split()` method yields the training
    and test *indices*, not the data itself. Having multiple splits can be useful
    if you want to better estimate the performance of your model, as you will see
    when we discuss cross-validation later in this chapter. For example, the following
    code generates 10 different stratified splits of the same dataset:    [PRE15]    For
    now, you can just use the first split:    [PRE16]    Or, since stratified sampling
    is fairly common, there’s a shorter way to get a single split using the `train_test_split()`
    function with the `stratify` argument:    [PRE17]    Let’s see if this worked
    as expected. You can start by looking at the income category proportions in the
    test set:    [PRE18]   [PRE19] for set_ in (strat_train_set, strat_test_set):     set_.drop("income_cat",
    axis=1, inplace=True) [PRE20]` [PRE21][PRE22][PRE23][PRE24][PRE25]  [PRE26]`py[PRE27]py
    [PRE28]`py[PRE29]py[PRE30]py` # Explore and Visualize the Data to Gain Insights    So
    far you have only taken a quick glance at the data to get a general understanding
    of the kind of data you are manipulating. Now the goal is to go into a little
    more depth.    First, make sure you have put the test set aside and you are only
    exploring the training set. Also, if the training set is very large, you may want
    to sample an exploration set, to make manipulations easy and fast during the exploration
    phase. In this case, the training set is quite small, so you can just work directly
    on the full set. Since you’re going to experiment with various transformations
    of the full training set, you should make a copy of the original so you can revert
    to it afterwards:    [PRE31]py    ## Visualizing Geographical Data    Because
    the dataset includes geographical information (latitude and longitude), it is
    a good idea to create a scatterplot of all the districts to visualize the data
    ([Figure 2-11](#bad_visualization_plot)):    [PRE32]py  ![Scatter plot displaying
    geographical data points with longitude on the x-axis and latitude on the y-axis,
    showing a distribution resembling California.](assets/hmls_0211.png)  ###### Figure
    2-11\. A geographical scatterplot of the data    This looks like California all
    right, but other than that it is hard to see any particular pattern. Setting the
    `alpha` option to `0.2` makes it much easier to visualize the places where there
    is a high density of data points ([Figure 2-12](#better_visualization_plot)):    [PRE33]py    Now
    that’s much better: you can clearly see the high-density areas, namely the Bay
    Area and around Los Angeles and San Diego, plus a long line of fairly high-density
    areas in the Central Valley (in particular, around Sacramento and Fresno).    Our
    brains are very good at spotting patterns in pictures, but you may need to play
    around with visualization parameters to make the patterns stand out.  ![Scatter
    plot showing the distribution of housing locations by latitude and longitude,
    illustrating high-density areas with darker blue clusters.](assets/hmls_0212.png)  ######
    Figure 2-12\. A better visualization that highlights high-density areas    Next,
    you look at the housing prices ([Figure 2-13](#housing_prices_scatterplot)). The
    radius of each circle represents the district’s population (option `s`), and the
    color represents the price (option `c`). Here you use a predefined color map (option
    `cmap`) called `jet`, which ranges from blue (low values) to red (high prices):⁠^([8](ch02.html#id1068))    [PRE34]py    This
    image tells you that the housing prices are very much related to the location
    (e.g., close to the ocean) and to the population density, as you probably knew
    already. A clustering algorithm should be useful for detecting the main cluster
    and for adding new features that measure the proximity to the cluster centers.
    The ocean proximity attribute may be useful as well, although in Northern California
    the housing prices in coastal districts are not too high, so it is not a simple
    rule.  ![Scatter plot showing California housing prices by location, with red
    indicating expensive areas and blue indicating cheaper ones; larger circles represent
    areas with larger population density.](assets/hmls_0213.png)  ###### Figure 2-13\.
    California housing prices: red is expensive, blue is cheap, larger circles indicate
    areas with a larger population    ## Look for Correlations    Since the dataset
    is not too large, you can easily compute the *standard correlation coefficient*
    (also called *Pearson’s r*) between every pair of numerical attributes using the
    `corr()` method:    [PRE35]py    Now you can look at how much each attribute correlates
    with the median house value:    [PRE36]py   [PRE37] ## Experiment with Attribute
    Combinations    Hopefully the previous sections gave you an idea of a few ways
    you can explore the data and gain insights. You identified a few data quirks that
    you may want to clean up before feeding the data to a machine learning algorithm,
    and you found interesting correlations between attributes, in particular with
    the target attribute. You also noticed that some attributes have a skewed-right
    distribution, so you may want to transform them (e.g., by computing their logarithm
    or square root). Of course, your mileage will vary considerably with each project,
    but the general ideas are similar.    One last thing you may want to do before
    preparing the data for machine learning algorithms is to try out various attribute
    combinations. For example, the total number of rooms in a district is not very
    useful if you don’t know how many households there are. What you really want is
    the number of rooms per household. Similarly, the total number of bedrooms by
    itself is not very useful: you probably want to compare it to the total number
    of rooms. And the population per household also seems like an interesting attribute
    combination to look at. You create these new attributes as follows:    [PRE38]    And
    then you look at the correlation matrix again:    [PRE39]   [PRE40] [PRE41][PRE42][PRE43][PRE44]`
    [PRE45][PRE46][PRE47][PRE48]py[PRE49]py[PRE50]  [PRE51][PRE52][PRE53]` [PRE54][PRE55][PRE56]
    ## Handling Text and Categorical Attributes    So far we have only dealt with
    numerical attributes, but your data may also contain text attributes. In this
    dataset, there is just one: the `ocean_proximity` attribute. Let’s look at its
    value for the first few instances:    [PRE57]py   [PRE58]`py[PRE59]`py[PRE60]py[PRE61][PRE62][PRE63]py
    from sklearn.preprocessing import OneHotEncoder  cat_encoder = OneHotEncoder()
    housing_cat_1hot = cat_encoder.fit_transform(housing_cat) [PRE64]py >>> housing_cat_1hot
    `<Compressed Sparse Row sparse matrix of dtype ''float64''`  `with 16512 stored
    elements and shape (16512, 5)>` [PRE65]py[PRE66]py[PRE67][PRE68] Pandas has a
    function called `get_dummies()`, which also converts each categorical feature
    into a one-hot representation, with one binary feature per category:    [PRE69]py   [PRE70]
    [PRE71][PRE72]``py[PRE73]`` [PRE74]`` But `OneHotEncoder` is smarter: it will
    detect the unknown category and raise an exception. If you prefer, you can set
    the `handle_unknown` hyperparameter to `"ignore"`, in which case it will just
    represent the unknown category with zeros:    [PRE75]   [PRE76]` [PRE77] ######
    Tip    If a categorical attribute has a large number of possible categories (e.g.,
    country code, profession, species), then one-hot encoding will result in a large
    number of input features. This may slow down training and degrade performance.
    If this happens, you may want to replace the categorical input with useful numerical
    features related to the categories: for example, you could replace the `ocean_proximity`
    feature with the distance to the ocean (similarly, a country code could be replaced
    with the country’s population and GDP per capita). Alternatively, you can use
    one of the encoders provided by the `category_encoders` package on [GitHub](https://github.com/scikit-learn-contrib/category_encoders).
    Or, when dealing with neural networks, you can replace each category with a learnable,
    low-dimensional vector called an *embedding* (see [Chapter 14](ch14.html#nlp_chapter)).
    This is an example of *representation learning* (we will see more examples in
    [Chapter 18](ch18.html#autoencoders_chapter)).    When you fit any Scikit-Learn
    estimator using a DataFrame, the estimator stores the column names in the `feature_names_in_`
    attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator
    after that (e.g., to `transform()` or `predict()`) has the same column names.
    Transformers also provide a `get_feature_names_out()` method that you can use
    to build a DataFrame around the transformer’s output:    [PRE78]py`` `array([''ocean_proximity_<1H
    OCEAN'', ''ocean_proximity_INLAND'',`  `''ocean_proximity_ISLAND'', ''ocean_proximity_NEAR
    BAY'',`  `''ocean_proximity_NEAR OCEAN''], dtype=object)` `>>>` `df_output` `=`
    `pd``.``DataFrame``(``cat_encoder``.``transform``(``df_test_unknown``),` [PRE79]`
    [PRE80] `` `This feature helps avoid column mismatches, and it’s also quite useful
    when debugging.` `` [PRE81][PRE82][PRE83][PRE84][PRE85] [PRE86]`py` [PRE87] [PRE88][PRE89]``py[PRE90]py``
    [PRE91]`py[PRE92][PRE93][PRE94] [PRE95][PRE96][PRE97][PRE98][PRE99]``  [PRE100][PRE101]``py[PRE102]``
    ## Feature Scaling and Transformation    One of the most important transformations
    you need to apply to your data is *feature scaling*. With few exceptions, machine
    learning algorithms don’t perform well when the input numerical attributes have
    very different scales. This is the case for the housing data: the total number
    of rooms ranges from about 6 to 39,320, while the median incomes only range from
    0 to 15\. Without any scaling, most models will be biased toward ignoring the
    median income and focusing more on the number of rooms.    There are two common
    ways to get all attributes to have the same scale: *min-max scaling* and *standardization*.    ######
    Warning    As with all estimators, it is important to fit the scalers to the training
    data only: never use `fit()` or `fit_transform()` for anything else than the training
    set. Once you have a trained scaler, you can then use it to `transform()` any
    other set, including the validation set, the test set, and new data. Note that
    while the training set values will always be scaled to the specified range, if
    new data contains outliers, these may end up scaled outside the range. If you
    want to avoid this, just set the `clip` hyperparameter to `True`.    Min-max scaling
    (many people call this *normalization*) is the simplest: for each attribute, the
    values are shifted and rescaled so that they end up ranging from 0 to 1\. This
    is performed by subtracting the min value from all values, and dividing the results
    by the difference between the min and the max. Scikit-Learn provides a transformer
    called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets
    you change the range if, for some reason, you don’t want 0–1 (e.g., neural networks
    work best with zero-mean inputs, so a range of –1 to 1 is preferable). It’s quite
    easy to use:    [PRE103]    Standardization is different: first it subtracts the
    mean value (so standardized values have a zero mean), then it divides the result
    by the standard deviation (so standardized values have a standard deviation equal
    to 1). Unlike min-max scaling, standardization does not restrict values to a specific
    range. However, standardization is much less affected by outliers. For example,
    suppose a district has a median income equal to 100 (by mistake), instead of the
    usual 0–15\. Min-max scaling to the 0–1 range would map this outlier down to 1
    and it would crush all the other values down to 0–0.15, whereas standardization
    would not be much affected. Scikit-Learn provides a transformer called `StandardScaler`
    for standardization:    [PRE104]    ###### Tip    If you want to scale a sparse
    matrix without converting it to a dense matrix first, you can use a `StandardScaler`
    with its `with_mean` hyperparameter set to `False`: it will only divide the data
    by the standard deviation, without subtracting the mean (as this would break sparsity).    When
    a feature’s distribution has a *heavy tail* (i.e., when values far from the mean
    are not exponentially rare), both min-max scaling and standardization will squash
    most values into a small range. Machine learning models generally don’t like this
    at all, as you will see in [Chapter 4](ch04.html#linear_models_chapter). So *before*
    you scale the feature, you should first transform it to shrink the heavy tail,
    and if possible to make the distribution roughly symmetrical. For example, a common
    way to do this for positive features with a heavy tail to the right is to replace
    the feature with its square root (or raise the feature to a power between 0 and
    1). If the feature has a really long and heavy tail, such as a *power law distribution*,
    then replacing the feature with its logarithm may help. For example, the `population`
    feature roughly follows a power law: districts with 10,000 inhabitants are only
    10 times less frequent than districts with 1,000 inhabitants, not exponentially
    less frequent. [Figure 2-17](#long_tail_plot) shows how much better this feature
    looks when you compute its log: it’s very close to a Gaussian distribution (i.e.,
    bell-shaped).  ![Two histograms compare the distribution of a population feature:
    the left graph shows a heavily right-skewed distribution, while the right graph
    shows a more symmetrical, Gaussian-like distribution after applying a logarithmic
    transformation.](assets/hmls_0217.png)  ###### Figure 2-17\. Transforming a feature
    to make it closer to a Gaussian distribution    Another approach to handle heavy-tailed
    features consists in *bucketizing* the feature. This means chopping its distribution
    into roughly equal-sized buckets, and replacing each feature value with the index
    of the bucket it belongs to, much like we did to create the `income_cat` feature
    (although we only used it for stratified sampling). For example, you could replace
    each value with its percentile. Bucketizing with equal-sized buckets results in
    a feature with an almost uniform distribution, so there’s no need for further
    scaling, or you can just divide by the number of buckets to force the values to
    the 0–1 range.    When a feature has a multimodal distribution (i.e., with two
    or more clear peaks, called *modes*), such as the `housing_median_age` feature,
    it can also be helpful to bucketize it, but this time treating the bucket IDs
    as categories, rather than as numerical values. This means that the bucket indices
    must be encoded, for example using a `OneHotEncoder` (so you usually don’t want
    to use too many buckets). This approach will allow the regression model to more
    easily learn different rules for different ranges of this feature value. For example,
    perhaps houses built around 35 years ago have a peculiar style that fell out of
    fashion, and therefore they’re cheaper than their age alone would suggest.    Another
    approach to transforming multimodal distributions is to add a feature for each
    of the modes (at least the main ones), representing the similarity between the
    housing median age and that particular mode. The similarity measure is typically
    computed using a *radial basis function* (RBF)—any function that depends only
    on the distance between the input value and a fixed point. The most commonly used
    RBF is the Gaussian RBF, whose output value decays exponentially as the input
    value moves away from the fixed point. For example, the Gaussian RBF similarity
    between the housing age *x* and 35 is given by the equation exp(–*γ*(*x* – 35)²).
    The hyperparameter *γ* (gamma) determines how quickly the similarity measure decays
    as *x* moves away from 35\. Using Scikit-Learn’s `rbf_kernel()` function, you
    can create a new Gaussian RBF feature measuring the similarity between the housing
    median age and 35:    [PRE105]    [Figure 2-18](#age_similarity_plot) shows this
    new feature as a function of the housing median age (solid line). It also shows
    what the feature would look like if you used a smaller `gamma` value. As the chart
    shows, the new age similarity feature peaks at 35, right around the spike in the
    housing median age distribution: if this particular age group is well correlated
    with lower prices, there’s a good chance that this new feature will help.  ![Histogram
    and line plot showing the Gaussian RBF feature for age similarity peaking at a
    housing median age of 35, with gamma values of 0.10 and 0.03.](assets/hmls_0218.png)  ######
    Figure 2-18\. Gaussian RBF feature measuring the similarity between the housing
    median age and 35    So far we’ve only looked at the input features, but the target
    values may also need to be transformed. For example, if the target distribution
    has a heavy tail, you may choose to replace the target with its logarithm. But
    if you do, the regression model will now predict the *log* of the median house
    value, not the median house value itself. You will need to compute the exponential
    of the model’s prediction if you want the predicted median house value.    Luckily,
    most of Scikit-Learn’s transformers have an `inverse_transform()` method, making
    it easy to compute the inverse of their transformations. For example, the following
    code example shows how to scale the labels using a `StandardScaler` (just like
    we did for inputs), then train a simple linear regression model on the resulting
    scaled labels and use it to make predictions on some new data, which we transform
    back to the original scale using the trained scaler’s `inverse_transform()` method.
    Note that we convert the labels from a Pandas Series to a DataFrame, since the
    `StandardScaler` expects 2D inputs. Also, in this example we just train the model
    on a single raw input feature (median income), for simplicity:    [PRE106]    This
    works fine, but it’s simpler and less error-prone to use a `TransformedTarget​Regressor`,
    avoiding potential scaling mismatches. We just need to construct it, giving it
    the regression model and the label transformer, then fit it on the training set,
    using the original unscaled labels. It will automatically use the transformer
    to scale the labels and train the regression model on the resulting scaled labels,
    just like we did previously. Then, when we want to make a prediction, it will
    call the regression model’s `predict()` method and use the scaler’s `inverse_transform()`
    method to produce the prediction:    [PRE107]    ## Custom Transformers    Although
    Scikit-Learn provides many useful transformers, you will occasionally need to
    write your own for tasks such as custom transformations, cleanup operations, or
    combining specific attributes.    For transformations that don’t require any training,
    you can just write a function that takes a NumPy array as input and outputs the
    transformed array. For example, as discussed in the previous section, it’s often
    a good idea to transform features with heavy-tailed distributions by replacing
    them with their logarithm (assuming the feature is positive and the tail is on
    the right). Let’s create a log-transformer and apply it to the `population` feature:    [PRE108]    The
    `inverse_func` argument is optional. It lets you specify an inverse transform
    function, e.g., if you plan to use your transformer in a `TransformedTargetRegressor`.    Your
    transformation function can take hyperparameters as additional arguments. For
    example, here’s how to create a transformer that computes the same Gaussian RBF
    similarity measure as earlier:    [PRE109]    Note that there’s no inverse function
    for the RBF kernel, since there are always two values at a given distance from
    a fixed point (except at distance 0). Also note that `rbf_kernel()` does not treat
    the features separately. If you pass it an array with two features, it will measure
    the 2D distance (Euclidean) to measure similarity. For example, here’s how to
    add a feature that will measure the geographic similarity between each district
    and San Francisco:    [PRE110]    Custom transformers are also useful to combine
    features. For example, here’s a `FunctionTransformer` that computes the ratio
    between the input features 0 and 1:    [PRE111]   [PRE112]` `FunctionTransformer`
    is very handy, but what if you would like your transformer to be trainable, learning
    some parameters in the `fit()` method and using them later in the `transform()`
    method? For this, you need to write a custom class.    ###### Note    The rest
    of this section shows how to define custom transformer classes. In particular,
    it defines a custom transformer that groups districts into 10 geographical clusters,
    then measures the distance between each district and the center of each cluster,
    adding 10 corresponding RBF similarity features to the data. Since defining custom
    transformer classes is somewhat advanced, please feel free to skip to the next
    section and come back whenever needed.    Scikit-Learn relies on duck typing,⁠^([13](ch02.html#id1177))
    so custom transformer classes do not have to inherit from any particular base
    class. All they need is three methods: `fit()` (which must return `self`), `transform()`,
    and `fit_transform()`. You can get `fit_transform()` for free by simply adding
    `TransformerMixin` as a base class: the default implementation will just call
    `fit()` and then `transform()`. If you add `BaseEstimator` as a base class (and
    avoid using `*args` and `**kwargs` in your constructor), you will also get two
    extra methods: `get_params()` and `set_params()`. These will be useful for automatic
    hyperparameter tuning.    For example, here’s a custom transformer that acts much
    like the `StandardScaler`:    [PRE113]    Here are a few things to note:    *   The
    `sklearn.utils.validation` package contains several functions we can use to validate
    the inputs. For simplicity, we will skip such tests in the rest of this book,
    but production code should have them.           *   Scikit-Learn pipelines require
    the `fit()` method to have two arguments `X` and `y`, which is why we need the
    `y=None` argument even though we don’t use `y`.           *   All Scikit-Learn
    estimators set `n_features_in_` in the `fit()` method, and they ensure that the
    data passed to `transform()` or `predict()` has this number of features.           *   The
    `fit()` method must return `self`.           *   This implementation is not 100%
    complete: all estimators should set `feature_​names_in_` in the `fit()` method
    when they are passed a DataFrame. Moreover, all transformers should provide a
    `get_feature_names_out()` method, as well as an `inverse_transform()` method when
    their transformation can be reversed. See the last exercise at the end of this
    chapter for more details.              A custom transformer can (and often does)
    use other estimators in its implementation. For example, the following code demonstrates
    a custom transformer that uses a `KMeans` clusterer in the `fit()` method to identify
    the main clusters in the training data, and then uses `rbf_kernel()` in the `transform()`
    method to measure how similar each sample is to each cluster center:    [PRE114]    ######
    Tip    You can check whether your custom estimator respects Scikit-Learn’s API
    by passing an instance to `check_estimator()` from the `sklearn.utils.estimator_checks`
    package. For the full API, check out [*https://scikit-learn.org/stable/developers*](https://scikit-learn.org/stable/developers).    As
    you will see in [Chapter 8](ch08.html#unsupervised_learning_chapter), *k*-means
    is a clustering algorithm that locates clusters in the data. For example, we can
    use it to find the most populated regions in California. How many clusters *k*-means
    searches for is controlled by the `n_clusters` hyperparameter. The `fit()` method
    of `KMeans` supports an optional argument `sample_weight`, which lets the user
    specify the relative weights of the samples. For example, we could pass it the
    median income if we wanted the clusters to be biased toward wealthy districts.
    After training, the cluster centers are available via the `cluster_centers_` attribute.
    *k*-means is a stochastic algorithm, meaning that it relies on randomness to locate
    the clusters, so if you want reproducible results, you must set the `random_state`
    parameter. As you can see, despite the complexity of the task, the code is fairly
    straightforward. Now let’s use this custom transformer:    [PRE115]    This code
    creates a `ClusterSimilarity` transformer, setting the number of clusters to 10\.
    Then it calls `fit_transform()` with the latitude and longitude of every district
    in the training set (you can try weighting each district by its median income
    to see how that affects the clusters). The transformer uses *k*-means to locate
    the clusters, then measures the Gaussian RBF similarity between each district
    and all 10 cluster centers. The result is a matrix with one row per district,
    and one column per cluster. Let’s look at the first three rows, rounding to two
    decimal places:    [PRE116]   `[Figure 2-19](#district_cluster_plot) shows the
    10 cluster centers found by *k*-means. The districts are colored according to
    their geographic similarity to their closest cluster center. Notice that most
    clusters are located in highly populated areas.  ![Scatter plot showing geographic
    clusters based on Gaussian RBF similarity, with highly populated areas highlighted.](assets/hmls_0219.png)  ######
    Figure 2-19\. Gaussian RBF similarity to the nearest cluster center` [PRE117]``  [PRE118]``
    [PRE119]` [PRE120] ## Transformation Pipelines    As you can see, there are many
    data transformation steps that need to be executed in the right order. Fortunately,
    Scikit-Learn provides the `Pipeline` class to help with such sequences of transformations.
    Here is a small pipeline for numerical attributes, which will first impute then
    scale the input features:    [PRE121]py    The `Pipeline` constructor takes a
    list of name/estimator pairs (2-tuples) defining a sequence of steps. The names
    can be anything you like, as long as they are unique and don’t contain double
    underscores (`__`). They will be useful later, when we discuss hyperparameter
    tuning. The estimators must all be transformers (i.e., they must have a `fit_transform()`
    method), except for the last one, which can be anything: a transformer, a predictor,
    or any other type of estimator.    ###### Tip    In a Jupyter notebook, if you
    `import` `sklearn` and run `sklearn.​set_config(display="diagram")`, all Scikit-Learn
    estimators will be rendered as interactive diagrams. This is particularly useful
    for visualizing pipelines. To visualize `num_pipeline`, run a cell with `num_pipeline`
    as the last line. Clicking an estimator will show more details.    If you don’t
    want to have to name the transformers, you can use the convenient `make_pipeline()`
    function instead; it takes transformers as positional arguments and creates a
    `Pipeline` using the names of the transformers’ classes, in lowercase and without
    underscores (e.g., `"simpleimputer"`):    [PRE122]py    If multiple transformers
    have the same name, an index is appended to their names (e.g., `"foo-1"`, `"foo-2"`,
    etc.).    When you call the pipeline’s `fit()` method, it calls `fit_transform()`
    sequentially on all the transformers, passing the output of each call as the parameter
    to the next call until it reaches the final estimator, for which it just calls
    the `fit()` method.    The pipeline exposes the same methods as the final estimator.
    In this example the last estimator is a `StandardScaler`, which is a transformer,
    so the pipeline also acts like a transformer. If you call the pipeline’s `transform()`
    method, it will sequentially apply all the transformations to the data. If the
    last estimator were a predictor instead of a transformer, then the pipeline would
    have a `predict()` method rather than a `transform()` method. Calling it would
    sequentially apply all the transformations to the data and pass the result to
    the predictor’s `predict()` method.    Let’s call the pipeline’s `fit_transform()`
    method and look at the output’s first two rows, rounded to two decimal places:    [PRE123]py   [PRE124]`py
    [PRE125]py`` [PRE126]py[PRE127][PRE128][PRE129][PRE130][PRE131][PRE132][PRE133][PRE134][PRE135][PRE136]``py[PRE137]py`
    # Select and Train a Model    At last! You framed the problem, you got the data
    and explored it, you sampled a training set and a test set, and you wrote a preprocessing
    pipeline to automatically clean up and prepare your data for machine learning
    algorithms. You are now ready to select and train a machine learning model.    ##
    Train and Evaluate on the Training Set    The good news is that thanks to all
    these previous steps, things are now going to be easy! You decide to train a very
    basic linear regression model to get started:    [PRE138]py    Done! You now have
    a working linear regression model. You try it out on the training set, looking
    at the first five predictions and comparing them to the labels:    [PRE139]py
    `array([246000., 372700., 135700.,  91400., 330900.])` `>>>` `housing_labels``.``iloc``[:``5``]``.``values`
    `` `array([458300., 483800., 101700.,  96100., 361800.])` `` [PRE140]py   [PRE141]py``
    [PRE142]py [PRE143]`py [PRE144]py`` [PRE145]py` [PRE146]py [PRE147]py[PRE148]py`
    # Fine-Tune Your Model    Let’s assume that you now have a shortlist of promising
    models. You now need to fine-tune them. Let’s look at a few ways you can do that.    ##
    Grid Search    One option would be to fiddle with the hyperparameters manually,
    until you find a great combination of hyperparameter values. This would be very
    tedious work, and you may not have time to explore many combinations.    Instead,
    you can use Scikit-Learn’s `GridSearchCV` class to search for you. All you need
    to do is tell it which hyperparameters you want it to experiment with and what
    values to try out, and it will use cross-validation to evaluate all the possible
    combinations of hyperparameter values. For example, the following code searches
    for the best combination of hyperparameter values for the `RandomForestRegressor`:    [PRE149]py    Notice
    that you can refer to any hyperparameter of any estimator in a pipeline, even
    if this estimator is nested deep inside several pipelines and column transformers.
    For example, when Scikit-Learn sees `"preprocessing__geo__n_clusters"`, it splits
    this string at the double underscores, then it looks for an estimator named `"preprocessing"`
    in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks
    for a transformer named `"geo"` inside this `ColumnTransformer` and finds the
    `ClusterSimilarity` transformer we used on the latitude and longitude attributes.
    Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features`
    refers to the `max_features` hyperparameter of the estimator named `"random_forest"`,
    which is of course the `RandomForestRegressor` model (the `max_features` hyperparameter
    will be explained in [Chapter 6](ch06.html#ensembles_chapter)).    ###### Tip    Wrapping
    preprocessing steps in a Scikit-Learn pipeline allows you to tune the preprocessing
    hyperparameters along with the model hyperparameters. This is a good thing since
    they often interact. For example, perhaps increasing `n_clusters` requires increasing
    `max_features` as well. If fitting the pipeline transformers is computationally
    expensive, you can set the pipeline’s `memory` parameter to the path of a caching
    directory: when you first fit the pipeline, Scikit-Learn will save the fitted
    transformers to this directory. If you then fit the pipeline again with the same
    hyperparameters, Scikit-Learn will just load the cached transformers.    There
    are two dictionaries in this `param_grid`, so `GridSearchCV` will first evaluate
    all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter values
    specified in the first `dict`, then it will try all 2 × 3 = 6 combinations of
    hyperparameter values in the second `dict`. So in total the grid search will explore
    9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline
    3 times per combination, since we are using 3-fold cross validation. This means
    there will be a grand total of 15 × 3 = 45 rounds of training! It may take a while,
    but when it is done you can get the best combination of parameters like this:    [PRE150]py   [PRE151]`py[PRE152]``
    [PRE153]`` ## Randomized Search    The grid search approach is fine when you are
    exploring relatively few combinations, like in the previous example, but `RandomizedSearchCV`
    is often preferable, especially when the hyperparameter search space is large.
    This class can be used in much the same way as the `GridSearchCV` class, but instead
    of trying out all possible combinations it evaluates a fixed number of combinations,
    selecting a random value for each hyperparameter at every iteration. This may
    sound surprising, but this approach has several benefits:    *   If some of your
    hyperparameters are continuous (or discrete but with many possible values), and
    you let randomized search run for, say, 1,000 iterations, then it will explore
    1,000 different values for each of these hyperparameters, whereas grid search
    would only explore the few values you listed for each one.           *   Suppose
    a hyperparameter does not actually make much difference, but you don’t know it
    yet. If it has 10 possible values and you add it to your grid search, then training
    will take 10 times longer. But if you add it to a random search, it will not make
    any difference.           *   If there are 6 hyperparameters to explore, each
    with 10 possible values, then grid search offers no other choice than training
    the model a million times, whereas random search can always run for any number
    of iterations you choose.              For each hyperparameter, you must provide
    either a list of possible values, or a probability distribution:    [PRE154]    Scikit-Learn
    also has `HalvingRandomSearchCV` and `HalvingGridSearchCV` hyperparameter search
    classes. Their goal is to use the computational resources more efficiently, either
    to train faster or to explore a larger hyperparameter space. Here’s how they work:
    in the first round, many hyperparameter combinations (called “candidates”) are
    generated using either the grid approach or the random approach. These candidates
    are then used to train models that are evaluated using cross-validation, as usual.
    However, training uses limited resources, which speeds up this first round considerably.
    By default, “limited resources” means that the models are trained on a small part
    of the training set. However, other limitations are possible, such as reducing
    the number of training iterations if the model has a hyperparameter to set it.
    Once every candidate has been evaluated, only the best ones go on to the second
    round, where they are allowed more resources to compete. After several rounds,
    the final candidates are evaluated using full resources. This may save you some
    time tuning hyperparameters.    ## Ensemble Methods    Another way to fine-tune
    your system is to try to combine the models that perform best. The group (or “ensemble”)
    will often perform better than the best individual model—just like random forests
    perform better than the individual decision trees they rely on—especially if the
    individual models make very different types of errors. For example, you could
    train and fine-tune a *k*-nearest neighbors model, then create an ensemble model
    that just predicts the mean of the random forest prediction and that model’s prediction.
    We will cover this topic in more detail in [Chapter 6](ch06.html#ensembles_chapter).    ##
    Analyzing the Best Models and Their Errors    You will often gain good insights
    on the problem by inspecting the best models. For example, the `RandomForestRegressor`
    can indicate the relative importance of each attribute for making accurate predictions:    [PRE155]
    `>>>` `feature_importances``.``round``(``2``)` `` `array([0.07, 0.05, 0.05, 0.01,
    0.01, 0.01, 0.01, 0.19, [...], 0\.  , 0.01])` `` [PRE156]   [PRE157]` [PRE158]
    [PRE159]`py [PRE160]py [PRE161]`py [PRE162]py[PRE163][PRE164]`  [PRE165] [PRE166]`py
    [PRE167]`py`` [PRE168]`py [PRE169]`py` [PRE170]`py`` [PRE171]`py[PRE172][PRE173][PRE174][PRE175][PRE176][PRE177][PRE178]
    [PRE179]`py[PRE180]py[PRE181]py` [PRE182]`py[PRE183]py[PRE184]```'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2] >>> housing_full["ocean_proximity"].value_counts() `ocean_proximity`
    `<1H OCEAN     9136` `INLAND        6551` `NEAR OCEAN    2658` `NEAR BAY      2290`
    `ISLAND           5` `Name: count, dtype: int64` [PRE3]`Let’s look at the other
    fields. The `describe()` method shows a summary of the numerical attributes ([Figure 2-7](#housing_describe_screenshot)).  ![A
    table generated by the `describe()` method, displaying statistical summaries for
    numerical attributes such as count, mean, standard deviation, minimum, maximum,
    and percentiles.](assets/hmls_0207.png)  ###### Figure 2-7\. Summary of each numerical
    attribute    The `count`, `mean`, `min`, and `max` rows are self-explanatory.
    Note that the null values are ignored (so, for example, the `count` of `total_bedrooms`
    is 20,433, not 20,640). The `std` row shows the *standard deviation*, which measures
    how dispersed the values are.⁠^([5](ch02.html#id1038)) The `25%`, `50%`, and `75%`
    rows show the corresponding *percentiles*: a percentile indicates the value below
    which a given percentage of observations in a group of observations fall. For
    example, 25% of the districts have a `housing_median_age` lower than 18, while
    50% are lower than 29, and 75% are lower than 37\. These are often called the
    25th percentile (or first *quartile*), the median, and the 75th percentile (or
    third quartile).    Another quick way to get a feel of the type of data you are
    dealing with is to plot a histogram for each numerical attribute. A histogram
    shows the number of instances (on the vertical axis) that have a given value range
    (on the horizontal axis). You can either plot this one attribute at a time, or
    you can call the `hist()` method on the whole dataset (as shown in the following
    code example), and it will plot a histogram for each numerical attribute (see
    [Figure 2-8](#attribute_histogram_plots)).  ![Histograms displaying the distribution
    of various numerical attributes such as longitude, latitude, housing median age,
    total rooms, total bedrooms, population, households, median income, and median
    house value.](assets/hmls_0208.png)  ###### Figure 2-8\. A histogram for each
    numerical attribute    The number of value ranges can be adjusted using the `bins`
    argument (try playing with it to see how it affects the histograms):    [PRE4]    Looking
    at these histograms, you notice a few things:    *   First, the median income
    attribute does not look like it is expressed in US dollars (USD). After checking
    with the team that collected the data, you are told that the data has been scaled
    and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually,
    0.4999) for lower median incomes. The numbers represent roughly tens of thousands
    of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes
    is common in machine learning, and it is not necessarily a problem, but you should
    try to understand how the data was computed.           *   The housing median
    age and the median house value were also capped. The latter may be a serious problem
    since it is your target attribute (your labels). Your machine learning algorithms
    may learn that prices never go beyond that limit. You need to check with your
    client team (the team that will use your system’s output) to see if this is a
    problem or not. If they tell you that they need precise predictions even beyond
    $500,000, then you have two options:               *   Collect proper labels for
    the districts whose labels were capped.                       *   Remove those
    districts from the training set (and also from the test set, since your system
    should not be evaluated poorly if it predicts values beyond $500,000).                   *   These
    attributes have very different scales. We will discuss this later in this chapter
    when we explore feature scaling.           *   Finally, many histograms are *skewed
    right*: they extend much farther to the right of the median than to the left.
    This may make it a bit harder for some machine learning algorithms to detect patterns.
    Later, you’ll try transforming these attributes to have more symmetrical and bell-shaped
    distributions.              You should now have a better understanding of the
    kind of data you’re dealing with.[PRE5]``  [PRE6][PRE7]py[PRE8] Well, this works,
    but it is not'
