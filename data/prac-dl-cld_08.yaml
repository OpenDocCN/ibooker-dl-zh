- en: 'Chapter 8\. Cloud APIs for Computer Vision: Up and Running in 15 Minutes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。云计算机视觉API：15分钟内上手
- en: Due to repeated incidents of near meltdown at the nearby nuclear power plant,
    the library of the city of Springfield (we are not allowed to mention the state)
    decided that it was too risky to store all their valuable archives in physical
    form. After hearing that the library from their rival city of Shelbyville started
    digitizing their records, they wanted to get in on the game as well. After all,
    their collection of articles such as “Old man yells at cloud” and “Local man thinks
    wrestling is real” and the hundred-year-old iconic photographs of the Gorge and
    the statue of the city’s founder Jebediah Springfield are irreplaceable. In addition
    to making their archives resilient to catastrophes, they would make their archives
    easily searchable and retrievable. And, of course, the residents of Springfield
    now would be able to access all of this material from the comfort of their living
    room couches.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 由于附近核电站多次发生近乎核泄漏的事件，斯普林菲尔德市图书馆（我们不允许提及州名）决定将他们所有宝贵的档案以数字形式存储太过危险。在听说对手城市Shelbyville的图书馆开始数字化他们的记录后，他们也想加入这场游戏。毕竟，他们的文章收藏，如“老人对着云喊叫”和“本地人认为摔跤是真的”，以及百年历史的峡谷和城市创始人杰比达·斯普林菲尔德的标志性照片是无法替代的。除了使他们的档案对灾难具有弹性外，他们还将使他们的档案易于搜索和检索。当然，现在斯普林菲尔德的居民可以在他们的客厅沙发上轻松访问所有这些材料。
- en: The first step in digitizing documents is, of course, scanning. That’s the easy
    part. Then starts the real challenge—processing and understanding all of this
    visual imagery. The team in Springfield had a few different options in front of
    them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数字化文件的第一步当然是扫描。那是容易的部分。然后开始真正的挑战——处理和理解所有这些视觉图像。斯普林菲尔德的团队面前有几种不同的选择。
- en: Perform manual data entry for every single page and every single photograph.
    Given that the city has more than 200 years of rich history, it would take a really
    long time, and would be error prone and expensive. It would be quite an ordeal
    to transcribe all of that material.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每一页和每张照片执行手动数据输入。考虑到这座城市拥有超过200年的丰富历史，这将需要很长时间，而且容易出错且昂贵。转录所有这些材料将是一场痛苦的经历。
- en: Hire a team of data scientists to build an image understanding system. That
    would be a much better approach, but there’s just one tiny hitch in the plan.
    For a library that runs on charitable donations, hiring a team of data scientists
    would quickly exhaust its budget. A single data scientist might not only be the
    highest-paid employee at the library, they might also be the highest-earning worker
    in the entire city of Springfield (barring the wealthy industrialist Montgomery
    Burns).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雇佣一支数据科学家团队来构建图像理解系统。这将是一个更好的方法，但计划中有一个小小的问题。对于依靠慈善捐款运行的图书馆来说，雇佣一支数据科学家团队将很快耗尽其预算。一个数据科学家不仅可能是图书馆中薪水最高的员工，也可能是整个斯普林菲尔德市（除了富有的实业家蒙哥马利·伯恩斯）中收入最高的工人。
- en: Get someone who knows enough coding to use the intelligence of ready-to-use
    vision APIs.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找一个懂得足够编码的人来使用现成的视觉API的智能。
- en: Logically they went with the quick and inexpensive third option. They had a
    stroke of luck, too. Martin Prince, an industrious fourth grader from Springfield
    Elementary who happened to know some coding, volunteered to build out the system
    for them. Although Martin did not know much deep learning (he’s just 10 years
    old, after all), he did know how to do some general coding, including making REST
    API calls using Python. And that was all he really needed to know. In fact, it
    took him just under 15 minutes to figure out how to make his first API call.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑上，他们选择了快速且廉价的第三种选择。他们也有一点运气。斯普林菲尔德小学勤奋的四年级学生马丁·普林斯恰好懂一些编码，自愿为他们建立系统。尽管马丁并不懂太多深度学习（毕竟他只有10岁），但他知道如何进行一些一般编码，包括使用Python进行REST
    API调用。这就是他真正需要知道的。事实上，他只用了不到15分钟就弄清楚了如何进行第一次API调用。
- en: 'Martin’s *modus operandi* was simple: send a scanned image to the cloud API,
    get a prediction back, and store it in a database for future retrieval. And obviously,
    repeat this process for every single record the library owned. He just needed
    to select the correct tool for the job.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马丁的*工作方式*很简单：将扫描的图像发送到云API，获得预测结果，并将其存储在数据库中以供将来检索。显然，对于图书馆拥有的每一条记录，都要重复这个过程。他只需要选择正确的工具来完成这项工作。
- en: All the big names—Amazon, Google, IBM, Microsoft—provide a similar set of computer-vision
    APIs that label images, detect and recognize faces and celebrities, identify similar
    images, read text, and sometimes even discern handwriting. Some of them even provide
    the ability to train our own classifier without having to write a single line
    of code. Sounds really convenient!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大公司——亚马逊、谷歌、IBM、微软——都提供类似的计算机视觉API，可以标记图像、检测和识别人脸和名人、识别相似图像、读取文本，有时甚至可以识别手写。其中一些甚至提供了训练我们自己的分类器的能力，而无需编写一行代码。听起来真的很方便！
- en: In the background, these companies are constantly working to improve the state
    of the art in computer vision. They have spent millions in acquiring and labeling
    datasets with a granular taxonomy much beyond the ImageNet dataset. We might as
    well make good use of their researchers’ blood, sweat, and tears (and electricity
    bills).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在背景中，这些公司不断努力改进计算机视觉的最新技术。他们花费了数百万美元来获取和标记数据集，其分类法远远超出了ImageNet数据集。我们不妨充分利用他们研究人员的心血和汗水（以及电费）。
- en: The ease of use, speed of onboarding and development, the variety of functionality,
    richness of tags, and competitive pricing make cloud-based APIs difficult to ignore.
    And all of this without the need to hire an expensive data science team. Chapters
    [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b) and [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b)
    optimized for accuracy and performance, respectively; this chapter essentially
    optimizes for human resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 易用性、入职和开发速度、功能的多样性、标签的丰富性以及竞争性定价使得基于云的API难以忽视。所有这些都不需要雇佣昂贵的数据科学团队。第5章和第6章分别针对准确性和性能进行了优化；而本章主要针对人力资源进行了优化。
- en: In this chapter, we explore several cloud-based visual recognition APIs. We
    compare them all both quantitatively as well as qualitatively. This should hopefully
    make it easier to choose the one that best suits your target application. And
    if they still don’t match your needs, we’ll investigate how to train a custom
    classifier with just a few clicks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了几种基于云的视觉识别API。我们在数量和质量上对它们进行了比较。这应该会让您更容易选择最适合您目标应用程序的API。如果它们仍然不符合您的需求，我们将探讨如何通过只需几次点击来训练自定义分类器。
- en: (In the interest of full disclosure, some of the authors of this book were previously
    employed at Microsoft, whose offerings are discussed here. We have attempted not
    to let that bias our results by building reproducible experiments and justifying
    our methodology.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （为了公开透明，本书的一些作者以前曾在微软工作，这里讨论的是微软的产品。我们尝试通过构建可重现的实验和证明我们的方法论来遏制这种偏见。）
- en: The Landscape of Visual Recognition APIs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉识别API的景观
- en: Let’s explore some of the different visual recognition APIs out there.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些不同的视觉识别API。
- en: Clarifai
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clarifai
- en: Clarifai ([Figure 8-1](part0010.html#sample_of_clarifaiapostrophes_results))
    was the winner of the 2013 ILSVRC classification task. Started by Matthew Zeiler,
    a graduate student from New York University, this was one of the first visual
    recognition API companies out there.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Clarifai（[图8-1](part0010.html#sample_of_clarifaiapostrophes_results)）是2013年ILSVRC分类任务的获胜者。由纽约大学的研究生Matthew
    Zeiler创立，这是最早的视觉识别API公司之一。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Fun fact: While investigating a classifier to detect NSFW (Not Safe For Work)
    images, it became important to understand and debug what was being learned by
    the CNN in order to reduce false positives. This led Clarifai to invent a visualization
    technique to expose which images stimulate feature maps at any layer in the CNN.
    As they say, necessity is the mother of invention.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实：在研究一个分类器来检测不安全的图像时，理解和调试CNN所学到的内容变得很重要，以减少误报。这导致Clarifai发明了一种可视化技术，以暴露哪些图像在CNN的任何层中刺激特征映射。正如他们所说，需求是发明之母。
- en: What’s unique about this API?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: It offers multilingual tagging in more than 23 languages, visual similarity
    search among previously uploaded photographs, face-based multicultural appearance
    classifier, photograph aesthetic scorer, focus scorer, and embedding vector generation
    to help us build our own reverse-image search. It also offers recognition in specialized
    domains including clothing and fashion, travel and hospitality, and weddings.
    Through its public API, the image tagger supports 11,000 concepts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供超过23种语言的多语言标记，以及在先前上传的照片中进行视觉相似性搜索，基于面部的多元文化外观分类器，照片美学评分器，焦点评分器，以及嵌入向量生成，以帮助我们构建自己的反向图像搜索。它还提供专业领域的识别，包括服装和时尚、旅行和款待以及婚礼。通过其公共API，图像标记器支持11,000个概念。
- en: '![Sample of Clarifai’s results](../images/00215.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Clarifai结果示例](../images/00215.jpeg)'
- en: Figure 8-1\. Sample of Clarifai’s results
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Clarifai结果示例
- en: Microsoft Cognitive Services
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软认知服务
- en: With the creation of ResNet-152 in 2015, Microsoft was able to win seven tasks
    at the ILSVRC, the COCO Image Captioning Challenge as well as the Emotion Recognition
    in the Wild challenge, ranging from classification and detection (localization)
    to image descriptions. And most of this research was translated to cloud APIs.
    Originally starting out as Project Oxford from Microsoft Research in 2015, it
    was eventually renamed Cognitive Services in 2016\. It’s a comprehensive set of
    more than 50 APIs ranging from vision, natural language processing, speech, search,
    knowledge graph linkage, and more. Historically, many of the same libraries were
    being run at divisions at Xbox and Bing, but they are now being exposed to developers
    externally. Some viral applications showcasing creative ways developers use these
    APIs include *[how-old.net](http://how-old.net)* (How Old Do I Look?), Mimicker
    Alarm (which requires making a particular facial expression in order to defuse
    the morning alarm), and *CaptionBot.ai*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，随着ResNet-152的创建，微软能够在ILSVRC赢得七项任务，包括COCO图像字幕挑战以及野外情绪识别挑战，涵盖了从分类和检测（定位）到图像描述的各个方面。大部分这项研究都被转化为云API。最初是从2015年微软研究的Project
    Oxford开始，最终在2016年更名为认知服务。这是一个包含超过50个API的综合性集合，涵盖视觉、自然语言处理、语音、搜索、知识图链接等多个领域。历史上，许多相同的库在Xbox和必应的部门运行，但现在它们正在向外部开发人员开放。一些展示开发人员如何创造性地使用这些API的病毒应用程序包括*[how-old.net](http://how-old.net)*（我看起来多大？）、Mimicker
    Alarm（需要做出特定的面部表情才能关闭早晨的闹钟）和*CaptionBot.ai*。
- en: What’s unique about this API?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: As illustrated in [Figure 8-2](part0010.html#sample_of_microsoft_cognitive_services_r),
    the API offers image captioning, handwriting understanding, and headwear recognition.
    Due to many enterprise customers, Cognitive Services does not use customer image
    data for improving its services.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-2](part0010.html#sample_of_microsoft_cognitive_services_r)所示，该API提供图像字幕、手写理解和头饰识别。由于有许多企业客户，认知服务不使用客户图像数据来改进其服务。
- en: '![Sample of Microsoft Cognitive Services results](../images/00096.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![微软认知服务结果示例](../images/00096.jpeg)'
- en: Figure 8-2\. Sample of Microsoft Cognitive Services results
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 微软认知服务结果示例
- en: Google Cloud Vision
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌云视觉
- en: Google provided the winning entry at the 2014 ILSVRC with the help of the 22-layer
    GoogLeNet, which eventually paved the way for the now-staple Inception architectures.
    Supplementing the Inception models, in December 2015, Google released a suite
    of Vision APIs. In the world of deep learning, having large amounts of data is
    definitely an advantage to improve one’s classifier, and Google has a lot of consumer
    data. For example, with learnings from Google Street View, you should expect relatively
    good performance in real-world text extraction tasks, like on billboards.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在2014年ILSVRC比赛中凭借22层GoogLeNet获得了胜利，最终为现在常见的Inception架构铺平了道路。除了Inception模型，谷歌在2015年12月发布了一套视觉API。在深度学习领域，拥有大量数据肯定是提高分类器的优势，而谷歌拥有大量的消费者数据。例如，通过从Google街景中学到的知识，您应该期望在现实世界的文本提取任务中表现相对良好，比如在广告牌上。
- en: What’s unique about this API?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: For human faces, it provides the most detailed facial key points ([Figure 8-3](part0010.html#sample_of_google_cloud_visionapostrophes))
    including roll, tilt, and pan to accurately localize the facial features. The
    APIs also return similar images on the web to the given input. A simple way to
    try out the performance of Google’s system without writing code is by uploading
    photographs to Google Photos and searching through the tags.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人脸，它提供了最详细的面部关键点（[图8-3](part0010.html#sample_of_google_cloud_visionapostrophes)），包括滚动、倾斜和平移，以准确定位面部特征。API还会返回与给定输入相似的网络图片。尝试谷歌系统的性能的简单方法是将照片上传到Google照片并通过标签搜索。
- en: '![Sample of Google Cloud Vision’s results](../images/00093.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![谷歌云视觉结果示例](../images/00093.jpeg)'
- en: Figure 8-3\. Sample of Google Cloud Vision’s results
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3. 谷歌云视觉结果示例
- en: Amazon Rekognition
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊Rekognition
- en: No, that title is not a typo. Amazon Rekognition API ([Figure 8-4](part0010.html#sample_of_amazon_rekognitionapostrophes))
    is largely based on Orbeus, a Sunnyvale, California-based startup that was acquired
    by Amazon in late 2015\. Founded in 2012, its chief scientist also had winning
    entries in the ILSVRC 2014 detection challenge. The same APIs were used to power
    PhotoTime, a famous photo organization app. The API’s services are available as
    part of the AWS offerings. Considering most companies already offer photo analysis
    APIs, Amazon is doubling down on video recognition offerings to offer differentiation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不，这个标题并没有错别字。亚马逊Rekognition API（[图8-4](part0010.html#sample_of_amazon_rekognitionapostrophes)）主要基于Orbeus，这是一家位于加利福尼亚州圣尼维尔的初创公司，于2015年底被亚马逊收购。成立于2012年，其首席科学家还在2014年ILSVRC检测挑战中获奖。同样的API被用于推动著名的照片整理应用PhotoTime。该API的服务作为AWS产品的一部分提供。考虑到大多数公司已经提供了照片分析API，亚马逊正在加倍努力提供视频识别服务以实现差异化。
- en: What’s unique about this API?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: License plate recognition, video recognition APIs, and better end-to-end integration
    examples of Rekognition APIs with AWS offerings like Kinesis Video Streams, Lambda,
    and others. Also, Amazon’s API is the only one that can determine whether the
    subject’s eyes are open or closed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 车牌识别，视频识别API以及与AWS产品（如Kinesis视频流、Lambda等）的更好端到端集成示例是Rekognition API的亮点。此外，亚马逊的API是唯一一个可以确定被拍摄对象的眼睛是睁着还是闭着的API。
- en: '![Sample of Amazon Rekognition’s results](../images/00051.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![亚马逊Rekognition结果示例](../images/00051.jpeg)'
- en: Figure 8-4\. Sample of Amazon Rekognition’s results
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4. 亚马逊Rekognition结果示例
- en: IBM Watson Visual Recognition
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM Watson视觉识别
- en: Under the Watson brand, IBM’s Visual Recognition offering started in early 2015\.
    After purchasing AlchemyAPI, a Denver-based startup, AlchemyVision has been used
    for powering the Visual Recognition APIs ([Figure 8-5](part0010.html#sample_of_ibm_watsonapostrophes_visual_r)).
    Like others, IBM also offers custom classifier training. Surprisingly, Watson
    does not offer optical character recognition yet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Watson品牌下，IBM的视觉识别服务于2015年初开始。在收购了位于丹佛的初创公司AlchemyAPI之后，AlchemyVision被用于推动视觉识别API（[图8-5](part0010.html#sample_of_ibm_watsonapostrophes_visual_r)）。与其他公司一样，IBM也提供自定义分类器训练。令人惊讶的是，Watson目前还没有提供光学字符识别功能。
- en: '![Sample of IBM Watson’s Visual Recognition results](../images/00283.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![IBM Watson视觉识别结果示例](../images/00283.jpeg)'
- en: Figure 8-5\. Sample of IBM Watson’s Visual Recognition results
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5. IBM Watson视觉识别结果示例
- en: Algorithmia
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Algorithmia
- en: Algorithmia is a marketplace for hosting algorithms as APIs on the cloud. Founded
    in 2013, this Seattle-based startup has both its own in-house algorithms as well
    as those created by others (in which case creators earn revenue based on the number
    of calls). In our experience, this API did tend to have the slowest response time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Algorithmia是一个在云上托管算法作为API的市场。成立于2013年，这家位于西雅图的初创公司既有自己的内部算法，也有其他人创建的算法（在这种情况下，创建者根据调用次数赚取收入）。根据我们的经验，这个API的响应时间似乎是最慢的。
- en: What’s unique about this API?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: Colorization service for black and white photos ([Figure 8-6](part0010.html#sample_of_algorithmiaapostrophes_style_t)),
    image stylization, image similarity, and the ability to run these services on-premises,
    or on any cloud provider.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 黑白照片的着色服务（[图8-6](part0010.html#sample_of_algorithmiaapostrophes_style_t)），图像风格化，图像相似度以及在本地或任何云提供商上运行这些服务的能力。
- en: '![Sample of Algorithmia’s style transfer results](../images/00301.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Algorithmia风格转移结果示例](../images/00301.jpeg)'
- en: Figure 8-6\. Sample of Algorithmia’s style transfer results
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. Algorithmia风格转移结果示例
- en: With so many offerings, it can be overwhelming to choose a service. There are
    many reasons why we might choose one over another. Obviously, the biggest factors
    for most developers would be accuracy and price. Accuracy is the big promise that
    the deep learning revolution brings, and many applications require it on a consistent
    basis. Price of the service might be an additional factor to consider. We might
    also choose a service provider because our company already has a billing account
    with it, and it would take additional effort to integrate a different service
    provider. Speed of the API response might be another factor, especially if the
    user is waiting on the other end for a response. Because many of these API calls
    can be abstracted, it’s easy to switch between different providers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提供的服务太多，选择一个服务可能会让人感到不知所措。我们可能会因为各种原因选择一个服务而不是另一个。显然，对大多数开发人员来说，最重要的因素是准确性和价格。准确性是深度学习革命带来的重要承诺，许多应用程序需要它以保持一致。服务的价格可能是需要考虑的另一个因素。我们也可能选择一个服务提供商，因为我们的公司已经与其有结算账户，而与其他服务提供商集成将需要额外的努力。API响应速度可能是另一个因素，特别是如果用户正在等待响应。由于许多这些API调用可以被抽象化，因此很容易在不同的提供商之间切换。
- en: Comparing Visual Recognition APIs
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较视觉识别API
- en: To aid our decision making, let’s compare these APIs head to head. In this section,
    we examine service offerings, cost, and accuracy of each.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们做出决策，让我们逐个比较这些API。在本节中，我们将检查每个服务的提供、成本和准确性。
- en: Service Offerings
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务提供
- en: '[Table 8-1](part0010.html#comparison_shopping_of_vision_api_provid) lists what
    services are being offered by each cloud provider.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8-1](part0010.html#comparison_shopping_of_vision_api_provid)列出了每个云提供商提供的服务。'
- en: Table 8-1\. Comparison shopping of vision API providers (as of Aug. 2019)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1\. 视觉API提供商的比较（截至2019年8月）
- en: '|  | **Algorithmia** | **Amazon Rekognition** | **Clarifai** | **Microsoft
    Cognitive Services** | **Google Cloud Vision** | **IBM Watson Visual Recognition**
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | **Algorithmia** | **Amazon Rekognition** | **Clarifai** | **Microsoft
    Cognitive Services** | **Google Cloud Vision** | **IBM Watson Visual Recognition**
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Image classification | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Image detection | ✔ | ✔ |  | ✔ | ✔ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 图像检测 | ✔ | ✔ |  | ✔ | ✔ |  |'
- en: '| OCR | ✔ | ✔ |  | ✔ | ✔ |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| OCR | ✔ | ✔ |  | ✔ | ✔ |  |'
- en: '| Face recognition | ✔ | ✔ |  | ✔ |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 人脸识别 | ✔ | ✔ |  | ✔ |  |  |'
- en: '| Emotionrecognition | ✔ |  | ✔ | ✔ | ✔ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 情绪识别 | ✔ |  | ✔ | ✔ | ✔ |  |'
- en: '| Logo recognition |  |  | ✔ | ✔ | ✔ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 标志识别 |  |  | ✔ | ✔ | ✔ |  |'
- en: '| Landmark recognition |  |  | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 地标识别 |  |  | ✔ | ✔ | ✔ | ✔ |'
- en: '| Celebrityrecognition | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 名人识别 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Multilingual tagging |  |  | ✔ |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 多语言标记 |  |  | ✔ |  |  |  |'
- en: '| Image description |  |  |  | ✔ |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 图像描述 |  |  |  | ✔ |  |  |'
- en: '| Handwriting |  |  |  | ✔ | ✔ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 手写 |  |  |  | ✔ | ✔ |  |'
- en: '| Thumbnail generation | ✔ |  |  | ✔ | ✔ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 缩略图生成 | ✔ |  |  | ✔ | ✔ |  |'
- en: '| Content moderation | ✔ | ✔ | ✔ | ✔ | ✔ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 内容审核 | ✔ | ✔ | ✔ | ✔ | ✔ |  |'
- en: '| Custom classification training |  |  | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 自定义分类训练 |  |  | ✔ | ✔ | ✔ | ✔ |'
- en: '| Custom detector training |   |   |  | ✔ | ✔ |   |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 自定义检测器训练 |   |   |  | ✔ | ✔ |   |'
- en: '| Mobile custom models |   |   | ✔ | ✔ | ✔ |   |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 移动定制模型 |   |   | ✔ | ✔ | ✔ |   |'
- en: '| Free tier | 5,000 requests per month | 5,000 requests per month | 5,000 requests
    per month | 5,000 requests per month | 1,000 requests per month | 7,500 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 免费套餐 | 每月5000次请求 | 每月5000次请求 | 每月5000次请求 | 每月5000次请求 | 每月1000次请求 | 7500 |'
- en: 'That’s a mouthful of services already up and running, ready to be used in our
    application. Because numbers and hard data help make decisions easier, it’s time
    to analyze these services on two factors: cost and accuracy.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有很多服务在运行并准备在我们的应用程序中使用。因为数字和硬数据有助于做出决策，现在是时候在成本和准确性两个因素上分析这些服务了。
- en: Cost
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Money doesn’t grow on trees (yet), so it’s important to analyze the economics
    of using off-the-shelf APIs. Taking a heavy-duty example of querying these APIs
    at about 1 query per second (QPS) service for one full month (roughly 2.6 million
    requests per month), [Figure 8-7](part0010.html#a_cost_comparison_of_different_cloud-bas)
    presents a comparison of the different providers sorted by estimated costs (as
    of August 2019).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 金钱不是从树上长出来的（尚未），因此分析使用现成API的经济学是很重要的。以每秒约1次查询（QPS）的重型查询API为例，为一个完整的月份提供服务（大约每月260万次请求），[图8-7](part0010.html#a_cost_comparison_of_different_cloud-bas)按预估成本排序列出了不同提供商的比较（截至2019年8月）。
- en: '![A cost comparison of different cloud-based vision APIs](../images/00201.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![不同基于云的视觉API的成本比较](../images/00201.jpeg)'
- en: Figure 8-7\. A cost comparison of different cloud-based vision APIs
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7\. 不同基于云的视觉API的成本比较
- en: Although for most developers, this is an extreme scenario, this would be a pretty
    realistic load for large corporations. We will eventually compare these prices
    against running our own service in the cloud to make sure we get the most bang
    for the buck fitting our scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于大多数开发人员来说，这是一个极端的情况，但对于大型公司来说，这将是一个相当现实的负载。我们最终将比较这些价格与在云中运行我们自己的服务，以确保我们在符合我们情景的情况下获得最大的性价比。
- en: That said, many developers might find negligible charges, considering that all
    of the cloud providers we look at here have a free tier of 5,000 calls per month
    (except Google Vision, which gives only 1,000 calls per month for free), and then
    roughly $1 per 1,000 calls.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，许多开发人员可能会发现费用微不足道，因为我们在这里看到的所有云提供商都有每月5000次调用的免费套餐（除了Google Vision，它每月只提供1000次免费调用），然后大约每1000次调用收取1美元。
- en: Accuracy
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性
- en: In a world ruled by marketing departments who claim their organizations to be
    the market leaders, how do we judge who is actually the best? What we need are
    common metrics to compare these service providers on some external datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个由市场部门主导的世界中，他们声称自己是市场领导者，我们如何判断谁才是真正的最佳呢？我们需要一些共同的指标来比较这些服务提供商在一些外部数据集上的表现。
- en: To showcase building a reproducible benchmark, we assess the text extraction
    quality using the COCO-Text dataset, which is a subset of the MS COCO dataset.
    This 63,686-image set contains text in daily life settings, like on a banner,
    street sign, number on a bus, price tag in a grocery store, designer shirt, and
    more. This real-world imagery makes it a relatively tough set to test against.
    We use the Word Error Rate (WER) as our benchmarking metric. To keep things simple,
    we ignore the position of the word and focus only on whether a word is present
    (i.e., bag of words). To be a match, the entire word must be correct.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 展示构建可重复基准的过程中，我们使用COCO-Text数据集评估文本提取质量，该数据集是MS COCO数据集的子集。这个包含63,686张图像的数据集包含了日常生活场景中的文本，比如横幅上的文字、街道标志、公交车上的数字、杂货店的价格标签、设计师衬衫等等。这些真实世界的图像使得这个数据集相对较难测试。我们使用词错误率（WER）作为我们的基准度量。为了简化问题，我们忽略单词的位置，只关注单词是否存在（即词袋模型）。为了匹配，整个单词必须是正确的。
- en: In the COCO-Text validation dataset, we pick all images with one or more instances
    of legible text (full-text sequences without interruptions) and compare text instances
    of more than one-character length. We then send these images to various cloud
    vision APIs. [Figure 8-8](part0010.html#wer_for_different_text_extraction_apis_a)
    presents the results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在COCO-Text验证数据集中，我们挑选所有包含一个或多个可读文本实例（没有中断的完整文本序列）的图像，并比较长度超过一个字符的文本实例。然后我们将这些图像发送到各种云视觉API。[图8-8](part0010.html#wer_for_different_text_extraction_apis_a)呈现了结果。
- en: '![WER for different text extraction APIs as of August 2019](../images/00111.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![2019年8月不同文本提取API的WER](../images/00111.jpeg)'
- en: Figure 8-8\. WER for different text extraction APIs as of August 2019
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8. 2019年8月不同文本提取API的WER
- en: Considering how difficult the dataset is, these results are remarkable. Most
    state-of-the-art text extraction tools from earlier in the decade would not cross
    the 10% mark. This shows the power of deep learning. On a subset of manually tested
    images, we also noticed a year-on-year improvement in the performance of some
    of these APIs, which is another benefit enjoyed by cloud-based APIs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集的难度，这些结果是显著的。本十年早期的大多数最先进的文本提取工具都不会超过10%的标记。这展示了深度学习的力量。在一部分手动测试的图像中，我们还注意到这些API中一些的性能每年都有所提高，这是云端API所享有的另一个好处。
- en: As always, all of the code that we used for our experiment is hosted on GitHub
    (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们用于实验的所有代码都托管在GitHub上（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）。
- en: The results of our analysis depend significantly on the dataset we choose as
    well as our metrics. Depending on our dataset (which is in turn influenced by
    our use case) as well as our minimum quality metrics, our results can vary. Additionally,
    service providers are constantly improving their services in the background. As
    a consequence, these results are not set in stone and improve over time. These
    results can be replicated on any dataset with the scripts on GitHub.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析的结果在很大程度上取决于我们选择的数据集以及我们的度量标准。根据我们的数据集（又受我们的用例影响）以及我们的最低质量度量标准，我们的结果可能会有所不同。此外，服务提供商在背景中不断改进他们的服务。因此，这些结果并非一成不变，会随着时间的推移而改善。这些结果可以在GitHub上的脚本上复制到任何数据集上。
- en: Bias
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见
- en: In [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b), we explored
    how bias can creep into datasets and how it can have real-life consequences for
    people. The APIs we explore in this chapter are no exception. Joy Buolamwini,
    a researcher at the MIT Media Lab, discovered that among Microsoft, IBM, and Megvii
    (also known as Face++), none were able to detect her face and gender accurately.
    Wondering if she had unique facial features that made her undetectable to these
    APIs, she (working along with Timnit Gebru) compiled faces of members of legislative
    branches from six countries with a high representation of women, building the
    Pilot Parliaments Benchmark (PPB; see [Figure 8-9](part0010.html#averaged_faces_among_different_gender_an)).
    She chose members from three African countries and three European countries to
    test for how the APIs performed on different skin tones. If you haven’t been living
    under a rock, you can already see where this is going.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)中，我们探讨了偏见如何渗入数据集以及它对人们的现实生活造成的后果。本章中我们探索的API也不例外。麻省理工学院媒体实验室的研究员乔伊·布拉明尼发现，在微软、IBM和Megvii（也称为Face++）中，没有一个能够准确检测她的面部和性别。她怀疑自己是否有独特的面部特征，使得这些API无法检测到她（与Timnit
    Gebru合作），编制了六个国家立法机构成员的面部图像，这些国家女性代表比例较高，建立了Pilot Parliaments Benchmark（PPB；请参见[图8-9](part0010.html#averaged_faces_among_different_gender_an)）。她选择了来自三个非洲国家和三个欧洲国家的成员，以测试这些API在不同肤色上的表现。如果你没有生活在石头下，你已经可以看出这将会发展成什么样的情况。
- en: She observed that the APIs performed fairly well overall at accuracies between
    85% and 95%. It was only when she started slicing the data across the different
    categories that she observed there was a massive amount of difference in the accuracies
    for each. She first observed that there was a significant difference between detection
    accuracies of men and women. She also observed that breaking down by skin tone,
    the difference in the detection accuracy was even larger. Then, finally, taking
    both gender and skin tone into consideration, the differences grew painfully starker
    between the worse detected group (darker females) and the best detected group
    (lighter males). For example, in the case of IBM, the detection accuracy of African
    women was a mere 65.3%, whereas the same API gave a 99.7% accuracy for European
    men. A whopping 34.4% difference! Considering many of these APIs are used by law
    enforcement, the consequences of bias seeping in might have life or death consequences.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 她观察到API的整体准确率在85%至95%之间。只有当她开始根据不同类别划分数据时，她才观察到每个类别的准确率存在巨大差异。她首先观察到男性和女性的检测准确率存在显著差异。她还观察到，根据肤色划分，检测准确率的差异更大。最后，考虑到性别和肤色，检测准确率在最差检测组（较深色女性）和最佳检测组（较浅色男性）之间的差异变得更加明显。例如，在IBM的情况下，非洲女性的检测准确率仅为65.3%，而同一API对欧洲男性的准确率为99.7%。高达34.4%的差异！考虑到许多这些API被执法部门使用，偏见渗入可能会带来生死后果。
- en: '![Averaged faces among different gender and skin tone, from Pilot Parliaments
    Benchmark (PPB)](../images/00239.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![来自Pilot Parliaments Benchmark（PPB）的不同性别和肤色的平均脸](../images/00239.jpeg)'
- en: Figure 8-9\. Averaged faces among different gender and skin tone, from Pilot
    Parliaments Benchmark (PPB)
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。来自Pilot Parliaments Benchmark（PPB）的不同性别和肤色的平均脸
- en: 'Following are a few insights we learned from this study:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们从这项研究中学到的一些见解：
- en: The algorithm is only as good as the data on which it’s trained. And this shows
    the need for diversity in the training dataset.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法只有在其训练的数据良好时才有效。这表明训练数据集需要多样性。
- en: Often the aggregate numbers don’t always reveal the true picture. The bias in
    the dataset is apparent only when slicing it across different subgroups.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，综合数字并不能总是揭示真实情况。只有在将数据集划分到不同子组时，数据集中的偏见才会显现。
- en: The bias does not belong to any specific company; rather, it’s an industry-wide
    phenomenon.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见并不属于任何特定公司；相反，这是一个整个行业的现象。
- en: These numbers are not set in stone and reflect only the time at which the experiment
    was performed. As evident from the drastic change in numbers between 2017 ([Figure 8-10](part0010.html#face_detection_comparison_across_apiscom))
    and a subsequent study in 2018 ([Figure 8-11](part0010.html#face_detection_comparison_across_apis_in)),
    these companies are taking bias removal from their datasets quite seriously.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数字并非铁板钉钉，仅反映了实验进行时的时间。正如2017年（[图8-10](part0010.html#face_detection_comparison_across_apiscom)）和2018年后续研究（[图8-11](part0010.html#face_detection_comparison_across_apis_in)）之间数字的急剧变化所示，这些公司正在认真从数据集中消除偏见。
- en: Researchers putting commercial companies to the test with public benchmarks
    results in industry-wide improvements (even if for the fear of bad PR, then so
    be it).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究人员通过公共基准测试商业公司，导致整个行业的改进（即使是出于对不良公关的恐惧）。
- en: '![Face detection comparison across APIs, tested in April and May 2017 on the
    PPB](../images/00143.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![2017年4月和5月在PPB上进行的API之间的人脸检测比较](../images/00143.jpeg)'
- en: Figure 8-10\. Face detection comparison across APIs, tested in April and May
    2017 on the PPB
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。2017年4月和5月在PPB上进行的API之间的人脸检测比较
- en: '![Face detection comparison across APIs in August 2018 on the PPB, conducted
    by Inioluwa Deborah Raji et al.](../images/00099.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![2018年8月在PPB上由Inioluwa Deborah Raji等人进行的API之间的人脸检测比较](../images/00099.jpeg)'
- en: Figure 8-11\. Face detection comparison across APIs in August 2018 on the PPB,
    conducted by Inioluwa Deborah Raji et al.
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。2018年8月在PPB上由Inioluwa Deborah Raji等人进行的API之间的人脸检测比较。
- en: How about bias in image-tagging APIs? Facebook AI Research pondered over the
    question “Does Object Recognition Work for Everyone?” in a paper by the same title
    (Terrance DeVries et al.). The group tested multiple cloud APIs in February 2019
    on Dollar Street, a diverse collection of images of household items from 264 different
    homes across 50 countries ([Figure 8-12](part0010.html#image-tagging_api_performance_on_geograp)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 那么图像标记API中的偏见呢？Facebook AI研究在同名论文中思考了“对象识别对每个人都有效吗？”这个问题（Terrance DeVries等人）。该团队于2019年2月在Dollar
    Street上测试了多个云API，这是来自50个国家264个家庭的各种家庭物品图像的集合（[图8-12](part0010.html#image-tagging_api_performance_on_geograp)）。
- en: '![Image-tagging API performance on geographically diverse images from the Dollar
    Street dataset](../images/00157.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![来自Dollar Street数据集的地理多样化图像的图像标记API性能](../images/00157.jpeg)'
- en: Figure 8-12\. Image-tagging API performance on geographically diverse images
    from the Dollar Street dataset
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。来自Dollar Street数据集的地理多样化图像的图像标记API性能
- en: 'Here are some of the key learnings from this test:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这次测试的一些关键发现：
- en: Accuracy of object classification APIs was significantly lower in images from
    regions with lower income levels, as illustrated in [Figure 8-13](part0010.html#average_accuracy_left_parenthesisand_sta).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在收入较低地区的图像中，对象分类API的准确性明显较低，如[图8-13](part0010.html#average_accuracy_left_parenthesisand_sta)所示。
- en: Datasets such as ImageNet, COCO, and OpenImages severely undersample images
    from Africa, India, China, and Southeast Asia, hence leading to lower performance
    on images from the non-Western world.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像ImageNet、COCO和OpenImages这样的数据集严重缺乏来自非洲、印度、中国和东南亚的图像，因此在非西方世界的图像上表现较差。
- en: Most of the datasets were collected starting with keyword searches in English,
    omitting images that mentioned the same object with phrases in other languages.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数数据集是通过英语关键词搜索收集的，省略了提及同一对象的其他语言短语的图像。
- en: '![Average accuracy (and standard deviation) of six cloud APIs versus income
    of the household where the images were collected](../images/00019.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![六个云API的平均准确率（和标准差）与收集图像的家庭收入之间的关系](../images/00019.jpeg)'
- en: Figure 8-13\. Average accuracy (and standard deviation) of six cloud APIs versus
    income of the household where the images were collected
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13\. 六个云API的平均准确率（和标准差）与收集图像的家庭收入之间的关系
- en: In summary, depending on the scenario for which we want to use these cloud APIs,
    we should build our own benchmarks and test them periodically to evaluate whether
    these APIs are appropriate for the use case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，根据我们想要使用这些云API的场景，我们应该建立自己的基准，并定期测试它们，以评估这些API是否适合用例。
- en: Getting Up and Running with Cloud APIs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用云API
- en: Calling these cloud services requires minimal code. At a high level, get an
    API key, load the image, specify the intent, make a POST request with the proper
    encoding (e.g., base64 for the image), and receive the results. Most of the cloud
    providers offer software development kits (SDKs) and sample code showcasing how
    to call their services. They additionally provide pip-installable Python packages
    to further simplify calling them. If you’re using Amazon Rekognition, we highly
    recommend using its `pip` package.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 调用这些云服务只需要很少的代码。在高层次上，获取一个API密钥，加载图像，指定意图，使用正确的编码（例如，对于图像使用base64），发送POST请求并接收结果。大多数云提供商提供软件开发工具包（SDK）和展示如何调用他们服务的示例代码。他们还提供可通过pip安装的Python包，以进一步简化调用过程。如果您正在使用Amazon
    Rekognition，我们强烈建议使用其`pip`包。
- en: Let’s reuse our thrilling image to test-run these services.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复使用我们激动人心的图像来测试这些服务。
- en: 'First, let’s try it on Microsoft Cognitive Services. Get an API key and replace
    it in the following code (the first 5,000 calls are free—more than enough for
    our experiments):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在Microsoft Cognitive Services上尝试一下。获取一个API密钥，并在以下代码中替换它（前5000次调用是免费的——对于我们的实验来说足够了）：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: “A little girl sitting at a table with a dog”—pretty close! There are other
    options to generate more detailed results, including a probability along with
    each tag.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: “一个坐在桌子旁边的小女孩和一只狗”——非常接近！还有其他选项可以生成更详细的结果，包括每个标签的概率。
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Although the ImageNet dataset is primarily tagged with nouns, many of these
    services go beyond and return verbs like “eating,” “sitting,” “jumping.” Additionally,
    they might contain adjectives like “red.” Chances are, these might not be appropriate
    for our application. We might want to filter out these adjectives and verbs. One
    option is to check their linguistic type against Princeton’s WordNet. This is
    available in Python with the Natural Language Processing Toolkit (NLTK). Additionally,
    we might want to filter out words like “indoor” and “outdoor” (often shown by
    Clarifai and Cognitive Services).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ImageNet数据集主要标记为名词，但许多这些服务超越了这一点，返回动词如“吃”、“坐”、“跳”。此外，它们可能包含形容词如“红色”。这些可能不适合我们的应用。我们可能希望过滤掉这些形容词和动词。一个选择是将它们的语言类型与普林斯顿的WordNet进行比较。这在Python中使用自然语言处理工具包（NLTK）可用。此外，我们可能希望过滤掉诸如“室内”和“室外”之类的词语（通常由Clarifai和Cognitive
    Services显示）。
- en: 'Now, let’s test the same image using Google Vision APIs. Get an API key from
    their website and use it in the following code (and rejoice, because the first
    1,000 calls are free):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Google Vision API测试相同的图像。从他们的网站获取一个API密钥，并在以下代码中使用它（并且高兴，因为前1000次调用是免费的）：
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Wasn’t that a little too easy? These APIs help us get to state-of-the-art results
    without needing a Ph.D.—in just 15 minutes!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是太容易了吗？这些API帮助我们在不需要博士学位的情况下获得最先进的结果，仅需15分钟！
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Even though these services return tags and image captions with probabilities,
    it’s up to the developer to determine a threshold. Usually, 60% and 40% are good
    thresholds for image tags and image captions, respectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些服务返回带有概率的标签和图像标题，但开发人员需要确定一个阈值。通常，60%和40%分别是图像标签和图像标题的良好阈值。
- en: It’s also important to communicate the probability to the end-user from a UX
    standpoint. For example, if the result confidence is >80%, we might say prefix
    the tags with “This image *contains....*” For <80%, we might want to change that
    prefix to “This image *may contain…*” to reflect the lower confidence in the result.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户体验的角度，将概率传达给最终用户也很重要。例如，如果结果置信度>80%，我们可能会在标签前加上“这张图片*包含....*”。对于<80%，我们可能希望将该前缀更改为“这张图片*可能包含...*”以反映结果中较低的置信度。
- en: Training Our Own Custom Classifier
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们自己的自定义分类器
- en: Chances are these services were not quite sufficient to meet the requirements
    of our use case. Suppose that the photograph we sent to one of these services
    responded with the tag “dog.” We might be more interested in identifying the breed
    of the dog. Of course, we can follow [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)
    to train our own classifier in Keras. But wouldn’t it be more awesome if we didn’t
    need to write a single line of code? Help is on the way.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务可能并不完全满足我们用例的要求。假设我们发送给其中一个服务的照片返回标签“狗”，我们可能更感兴趣的是识别狗的品种。当然，我们可以按照[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)来在Keras中训练自己的分类器。但如果我们不需要编写一行代码，那不是更棒吗？帮助已经到来。
- en: 'A few of these cloud providers give us the ability to train our own custom
    classifier by merely using a drag-and-drop interface. The pretty user interfaces
    provide no indication that under the hood they are using transfer learning. As
    a result, Cognitive Services Custom Vision, Google AutoML, Clarifai, and IBM Watson
    all provide us the option for custom training. Additionally, some of them even
    allow building custom detectors, which can identify the location of objects with
    a bounding box. The key process in all of them being the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些云提供商通过简单的拖放界面让我们有能力训练自己的自定义分类器。漂亮的用户界面并没有表明它们在幕后使用迁移学习。因此，Cognitive Services
    Custom Vision、Google AutoML、Clarifai和IBM Watson都为我们提供了自定义训练的选项。此外，其中一些甚至允许构建自定义检测器，可以识别带有边界框的对象的位置。其中的关键过程如下：
- en: Upload images
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传图片
- en: Label them
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记它们
- en: Train a model
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Evaluate the model
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Publish the model as a REST API
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型发布为REST API
- en: 'Bonus: Download a mobile-friendly model for inference on smartphones and edge
    devices'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励：下载一个适用于智能手机和边缘设备推理的移动友好模型
- en: Let’s see a step-by-step example of Microsoft’s [Custom Vision](https://www.customvision.ai).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下微软的[Custom Vision](https://www.customvision.ai)的逐步示例。
- en: '*Create a project* ([Figure 8-14](part0010.html#creating_a_new_project_in_custom_vision)):
    Choose a domain that best describes our use case. For most purposes, “General”
    would be optimal. For more specialized scenarios, we might want to choose a relevant
    domain.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建一个项目* ([图8-14](part0010.html#creating_a_new_project_in_custom_vision)):
    选择最能描述我们用例的领域。对于大多数情况，“通用”可能是最佳选择。对于更专业的场景，我们可能想选择一个相关的领域。'
- en: '![Creating a new project in Custom Vision](../images/00115.jpeg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在Custom Vision中创建一个新项目](../images/00115.jpeg)'
- en: Figure 8-14\. Creating a new project in Custom Vision
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-14. 在Custom Vision中创建一个新项目
- en: As an example, if we have an ecommerce website with photos of products against
    a pure white background, we might want to select the “Retail” domain. If we intend
    to run this model on a mobile phone eventually, we should choose the “Compact”
    version of the model, instead; it is smaller in size with only a slight loss in
    accuracy.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个电子商务网站，网站上有产品照片背景是纯白色的，我们可能想选择“零售”领域。如果我们最终打算在手机上运行这个模型，我们应该选择模型的“紧凑”版本；它尺寸更小，只有轻微的准确度损失。
- en: '*Upload* ([Figure 8-15](part0010.html#uploading_images_on_customvisiondotai)):
    For each category, upload images and tag them. It’s important to upload at least
    30 photographs per category. For our test, we uploaded more than 30 images of
    Maltese dogs and tagged them appropriately.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*上传* ([图8-15](part0010.html#uploading_images_on_customvisiondotai)): 对于每个类别，上传图片并进行标记。每个类别至少上传30张照片是很重要的。在我们的测试中，我们上传了30多张马耳他犬的图片，并进行了适当的标记。'
- en: '![Uploading images on CustomVision.ai](../images/00326.jpeg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在CustomVision.ai上上传图片](../images/00326.jpeg)'
- en: Figure 8-15\. Uploading images on CustomVision.ai
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15. 在CustomVision.ai上上传图片
- en: '*Train* ([Figure 8-16](part0010.html#the_train_button_in_the_upper-right_corn)):
    Click the Train button, and then in about three minutes, we have a spanking new
    classifier ready.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练* ([图8-16](part0010.html#the_train_button_in_the_upper-right_corn)): 点击Train按钮，大约三分钟后，我们就有了一个全新的分类器。'
- en: '![The Train button in the upper-right corner of the CustomVision.ai page](../images/00039.jpeg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CustomVision.ai页面右上角的Train按钮](../images/00039.jpeg)'
- en: Figure 8-16\. The Train button in the upper-right corner of the CustomVision.ai
    page
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16. CustomVision.ai页面右上角的Train按钮
- en: '*Analyze the model’s performance*: Check the precision and recall of the model.
    By default, the system sets the threshold at 90% confidence and gives the precision
    and recall metrics at that value. For higher precision, increase the confidence
    threshold. This would come at the expense of reduced recall. [Figure 8-17](part0010.html#relative_precision_and_recall_for_our_sa)
    shows example output.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分析模型的性能*：检查模型的精度和召回率。系统默认将阈值设置为90%置信度，并给出该值下的精度和召回率指标。要获得更高的精度，增加置信度阈值。这将以减少召回率为代价。[图8-17](part0010.html#relative_precision_and_recall_for_our_sa)展示了示例输出。'
- en: '*Ready to go*: We now have a production-ready API endpoint that we can call
    from any application.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*准备就绪*：我们现在有一个可以从任何应用程序调用的生产就绪的API端点。'
- en: To highlight the effect of the amount of data on model quality, let’s train
    a dog breed classifier. We can use the Stanford Dogs dataset, a collection of
    more than 100 dog categories. For simplicity, we randomly chose 10 breeds, which
    have more than 200 images available. With 10 classes, a random classifier would
    have one-tenth, or 10%, the chance of correctly identifying an image. We should
    easily be able to beat this number. [Table 8-2](part0010.html#effect_of_number_of_training_images_on_p)
    shows the effect of training on datasets with different volumes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出数据量对模型质量的影响，让我们训练一个狗品种分类器。我们可以使用斯坦福狗数据集，这是一个包含100多种狗品种的集合。为简单起见，我们随机选择了10种品种，这些品种有超过200张可用图片。有了10个类别，一个随机分类器正确识别图像的几率是十分之一，或者10%。我们应该很容易地超过这个数字。表8-2显示了在不同数据集上训练的效果。
- en: Table 8-2\. Effect of number of training images on precision and recall
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2. 训练图片数量对精度和召回率的影响
- en: '|  | **30 training images/class** | **200 training images/class** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | **每类30张训练图片** | **每类200张训练图片** |'
- en: '| --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Precision | 91.2% | 93.5% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | 91.2% | 93.5% |'
- en: '| Recall | 85.3% | 89.6% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 85.3% | 89.6% |'
- en: '![Relative precision and recall for our sample training set with 200 images
    per class](../images/00087.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![相对精度和召回率，我们的样本训练集每类有200张图片](../images/00087.jpeg)'
- en: Figure 8-17\. Relative precision and recall for our sample training set with
    200 images per class
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-17. 相对精度和召回率，我们的样本训练集每类有200张图片
- en: Because we haven’t uploaded a test set, the performance figures reported here
    are on the full dataset using the common *k*-fold cross-validation technique.
    This means the data was randomly divided into *k* parts, then (*k –* 1) parts
    were used for training, and the remaining part was used for testing. This was
    performed a few times, each time with a randomized subset of images, and the averaged
    results are reported here.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们还没有上传测试集，所以这里报告的性能数据是在完整数据集上使用常见的*k*折交叉验证技术得出的。这意味着数据被随机分成*k*部分，然后(*k -
    1)部分用于训练，剩下的部分用于测试。这个过程进行了几次，每次使用随机子集的图像，并报告了平均结果。
- en: It is incredible that even with 30 images per class, the classifier’s precision
    is greater than 90%, as depicted in [Figure 8-18](part0010.html#some_of_the_possible_tags_returned_by_th).
    And, surprisingly, this took slightly less than 30 seconds to train.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是，即使每类只有30张图片，分类器的精度也超过90%，如[图8-18](part0010.html#some_of_the_possible_tags_returned_by_th)所示。令人惊讶的是，这个训练过程只花了不到30秒的时间。
- en: Not only this, we can dig down and investigate the performance on each class.
    Classes with high precision might visibly be more distinct, whereas those with
    low precision might look similar to another class.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，我们可以深入研究每个类别的性能。高精度的类别可能看起来更加明显，而低精度的类别可能与另一个类别相似。
- en: '![Some of the possible tags returned by the API](../images/00205.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![API返回的一些可能标签](../images/00205.jpeg)'
- en: Figure 8-18\. Some of the possible tags returned by the API
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-18\. API返回的一些可能标签
- en: This short and convenient approach is not without its downsides, as you will
    see in the following section. In that section, we also discuss mitigation strategies
    to help take advantage of this rather useful tool.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简短而方便的方法并非没有缺点，正如您将在以下部分中看到的那样。在该部分中，我们还讨论了缓解策略，以帮助利用这个相当有用的工具。
- en: Top Reasons Why Our Classifier Does Not Work Satisfactorily
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的分类器不令人满意的主要原因
- en: 'There are a number of reasons why a classifier would not perform well. The
    following are some of them:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器表现不佳的原因有很多。以下是其中一些：
- en: Not enough data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不足
- en: If we find that the accuracy is not quite sufficient for our needs, we might
    need to train the system with more data. Of course, 30 images per class just gets
    us started. But for a production-quality application, more images are better.
    200 images per class are usually recommended.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现准确性不够满足我们的需求，可能需要用更多数据训练系统。当然，每个类别30张图像只是一个开始。但对于生产质量的应用程序，更多的图像更好。通常建议每个类别使用200张图像。
- en: Nonrepresentative training data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不具代表性的训练数据
- en: Often, the images on the internet are far too clean, set up in studio lighting
    with clean backgrounds, and close to the center of the frame. Images that our
    application might see on a daily basis might not be represented quite so well.
    It’s really important to train our classifier with real-world images for the best
    performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，互联网上的图像过于干净，设置在工作室灯光下，背景干净，并且接近画面中心。我们的应用程序可能每天看到的图像可能没有那么好地表示。用真实世界的图像训练我们的分类器对于获得最佳性能非常重要。
- en: Unrelated domain
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不相关的领域
- en: Under the hood, Custom Vision is running transfer learning. This makes it really
    important to choose the correct domain when creating the project. As an example,
    if we are trying to classify X-ray images, transfer learning from an ImageNet-based
    model might not yield as accurate a result. For cases like that, training our
    own classifier manually in Keras would work best, as demonstrated in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)
    (though this will probably take more than three minutes).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，自定义视觉正在运行迁移学习。这使得在创建项目时选择正确的领域非常重要。举个例子，如果我们试图对X射线图像进行分类，从基于ImageNet的模型进行迁移学习可能不会产生如此准确的结果。对于这种情况，手动在Keras中训练我们自己的分类器会是最好的选择，就像在[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中演示的那样（尽管这可能需要超过三分钟）。
- en: Using it for regression
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它进行回归
- en: 'In machine learning, there are two common categories of problems: classification
    and regression. Classification is predicting one or more classes for input. Regression,
    on the other hand, is predicting a numerical value given an input; for example,
    predicting house prices. Custom Vision is primarily a classification system. Using
    it to count objects by tagging the number of objects is the wrong approach, and
    will lead to unsatisfactory results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，有两种常见的问题类别：分类和回归。分类是为输入预测一个或多个类别。另一方面，回归是根据输入预测一个数值；例如，预测房价。自定义视觉主要是一个分类系统。将其用于通过标记对象数量来计数对象是错误的方法，并将导致不令人满意的结果。
- en: Counting objects is a type of regression problem. We can do it by localizing
    each instance of the object in an image (aka object detection) and counting their
    occurrences. Another example of a regression problem is predicting the age of
    a person based on their profile photo. We tackle both problems in later chapters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 计数对象是一种回归问题。我们可以通过定位图像中每个对象的每个实例（也称为目标检测）并计算它们的出现次数来实现。另一个回归问题的例子是根据个人的头像预测其年龄。我们将在后面的章节中解决这两个问题。
- en: Classes are too similar
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类别太相似
- en: If our classes look too similar and rely heavily on smaller-level details for
    distinction, the model might not perform as well. For example, a five-dollar note
    and a 20-dollar note have very similar high-level features. It’s at the lower-level
    details that show they are really distinct. As another example, it might be easy
    to distinguish between a Chihuahua and a Siberian Husky, but it’s more difficult
    to distinguish between an Alaskan Malamute and a Siberian Husky. A fully retrained
    CNN, as demonstrated in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b),
    should perform better than this Custom Vision-based system.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的类看起来太相似，并且在区分上依赖较小级别的细节，那么模型可能表现不佳。例如，一张五美元和一张二十美元的纸币具有非常相似的高级特征。真正区别它们的是更低级别的细节。另一个例子，区分吉娃娃和西伯利亚哈士奇可能很容易，但区分阿拉斯加雪橇犬和西伯利亚哈士奇可能更困难。如在[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中演示的完全重新训练的CNN应该比这个基于自定义视觉的系统表现更好。
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'A great feature of Custom Vision is that if the model is unsure of any image
    that it encounters via its API endpoint, the web UI will show those images for
    a manual review. We can review and manually tag new images on a periodic basis
    and continuously improve the quality of the model. These images tend to improve
    the classifier the most for two reasons: first, they represent real-world usage.
    Second, and more importantly, they have more impact on the model in comparison
    to images that the model can already easily classify. This is known as semisupervised
    learning.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义视觉的一个很棒的功能是，如果模型对通过其API端点遇到的任何图像感到不确定，Web UI将显示这些图像供手动审查。我们可以定期审查和手动标记新图像，并持续改进模型的质量。这些图像往往对分类器的改进最大，原因有两个：首先，它们代表了真实世界的使用。其次，更重要的是，与模型已经可以轻松分类的图像相比，它们对模型的影响更大。这被称为半监督学习。
- en: In this section, we discussed a few different ways in which we can improve our
    model’s accuracy. In the real world, that is not the end-all-be-all of a user’s
    experience. How quickly we are able to respond to a request also matters a lot.
    In the following section, we cover a couple of different ways we can improve performance
    without sacrificing quality.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们讨论了一些可以提高模型准确性的方法。在现实世界中，这并不是用户体验的全部。我们能够多快地响应请求也非常重要。在接下来的部分中，我们将介绍一些不牺牲质量的情况下提高性能的方法。
- en: Comparing Custom Classification APIs
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较自定义分类API
- en: As you might have noticed throughout the book, we are pretty dogmatic about
    being data driven. If we are going to spend good money on a service, we better
    get the best bang for our buck. Time to put the hype to the test.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能在整本书中已经注意到的那样，我们非常坚持数据驱动。如果我们要在一个服务上花钱，我们最好能物有所值。是时候来测试一下这个炒作了。
- en: For a good number of classification problems, these custom cloud-based classifiers
    perform pretty well. To truly test their limits, we need something more challenging.
    We need to unleash the toughest doggone dataset, train this animal, and fetch
    some insightful results—using the Stanford Dogs dataset.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多分类问题，这些定制的基于云的分类器表现得相当不错。为了真正测试它们的极限，我们需要更具挑战性的数据集。我们需要释放最难缠的数据集，训练这个模型，并获得一些有见地的结果——使用斯坦福狗数据集。
- en: Using the entire dataset might make it too easy for these classifiers (after
    all, ImageNet already has so many dog breeds), so we took it up a notch. Instead,
    we trained our own Keras classifier on the entire dataset and built a mini-dataset
    out of the top 34 worst-performing classes (each containing at least 140 images).
    The reason these classes performed poorly was because they often became confused
    with other similar-looking dog breeds. To perform better, they require a fine-grained
    understanding of the features. We divide the images into 100 randomly chosen images
    per class in the training dataset and 40 randomly chosen images per class in the
    test dataset. To avoid any class imbalances, which can have an impact on predictions,
    we chose the same number of train and test images for each class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用整个数据集可能会使这些分类器变得太容易（毕竟，ImageNet已经包含了很多狗品种），所以我们提高了难度。相反，我们在整个数据集上训练了我们自己的Keras分类器，并从表现最差的前34个类别中构建了一个迷你数据集（每个类别至少包含140张图片）。这些类别表现不佳的原因是因为它们经常与其他看起来相似的狗品种混淆。为了表现更好，它们需要对特征有细致的理解。我们将图像分为训练数据集中每个类别100张随机选择的图像，以及测试数据集中每个类别40张随机选择的图像。为了避免任何类别不平衡对预测的影响，我们为每个类别选择了相同数量的训练和测试图像。
- en: Lastly, we selected a minimum confidence threshold of 0.5 as it appeared to
    strike a good balance between precision and recall across all services. At a high
    confidence threshold such as 0.99, a classifier might be very accurate, but there
    might be only a handful of images with predictions; in other words, really low
    recall. On the other hand, a really low threshold of 0.01 would result in predictions
    for nearly all images. However, we should not rely on many of these results. After
    all, the classifier is not confident.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们选择了最小置信阈值为0.5，因为它似乎在所有服务中在精度和召回率之间取得了良好的平衡。在高置信阈值（例如0.99）下，分类器可能非常准确，但可能只有少数图像有预测；换句话说，召回率非常低。另一方面，非常低的阈值（0.01）会导致几乎所有图像都有预测。然而，我们不应该依赖许多这样的结果。毕竟，分类器并不自信。
- en: 'Instead of reporting the precision and recall, we report the *F1 score* (also
    known as *F-measure*), which is a hybrid score that combines both of those values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告*F1分数*（也称为*F-度量*），它是结合了这两个值的混合分数：
- en: <math display="block"><mrow><mi>F</mi><mi>1</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo>
    <mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>F</mi><mi>1</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo>
    <mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math>
- en: Additionally, we report the time it took to train, as shown in [Figure 8-19](part0010.html#a_chart_showing_the_f1_score_for_custom).
    Beyond just the cloud, we also trained using Apple’s Create ML tool on a MacBook
    Pro with and without data augmentations (rotate, crop, and flip).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们报告了训练所需的时间，如[图8-19](part0010.html#a_chart_showing_the_f1_score_for_custom)所示。除了云端，我们还使用了苹果的Create
    ML工具在MacBook Pro上进行训练，有时使用数据增强（旋转、裁剪和翻转），有时不使用。
- en: Google and Microsoft provide the ability to customize the duration of training.
    Google Auto ML allows us to customize between 1 and 24 hours. Microsoft provides
    a free “Fast Training” option and a paid “Advanced Training” option (similar to
    Google’s offering) with which we can select the duration to be anywhere between
    1 and 24 hours.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌和微软提供了定制训练持续时间的能力。谷歌Auto ML允许我们在1到24小时之间进行定制。微软提供了一个免费的“快速训练”选项和一个付费的“高级训练”选项（类似于谷歌的提供），我们可以选择在1到24小时之间的任何时间段进行训练。
- en: '![A chart showing the F1 score for custom classifier services, as of August
    2019 (higher is better)](../images/00173.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![显示了自定义分类器服务的F1分数，截至2019年8月（分数越高越好）](../images/00173.jpeg)'
- en: Figure 8-19\. A chart showing the F1 score for custom classifier services, as
    of August 2019 (higher is better)
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-19。显示了自定义分类器服务的F1分数，截至2019年8月（分数越高越好）
- en: 'Following are some interesting takeaways from this experiment:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个实验的一些有趣的收获：
- en: Clarifai and Microsoft offered near-instant training for the 3,400 training
    images.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clarifai和微软为这3400张训练图像提供了几乎即时的训练。
- en: Compared to “Fast Training,” Microsoft’s “Advanced Training” performed slightly
    better (roughly a 1-point increase) for the extra one hour of training. Because
    “Fast Training” took less than 15 seconds to train, we can infer that its base
    featurizer was already good at extracting fine-grained features.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与“快速训练”相比，微软的“高级训练”表现稍好一些（大约增加了1个点）多出的一个小时的训练。因为“快速训练”只需要不到15秒的时间进行训练，我们可以推断它的基础特征提取器已经很擅长提取细粒度特征。
- en: Surprisingly, Apple’s Create ML actually performed worse after adding in the
    augmentations, despite taking more than two extra hours to train, most of which
    was spent creating the augmentations. This was done on a top-of-the-line MacBook
    Pro and showed 100% GPU utilization in Activity Monitor.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令人惊讶的是，尽管在添加增强功能后，苹果的Create ML表现更差，尽管训练时间超过两个小时，其中大部分时间用于创建增强功能。这是在一台顶级的MacBook
    Pro上完成的，并在Activity Monitor中显示100%的GPU利用率。
- en: Additionally, to test the featurizer’s strength, we varied the amount of training
    data supplied to the service ([Figure 8-20](part0010.html#effect_of_varying_size_of_training_data)).
    Because Microsoft took less than 15 seconds to train, it was easy (and cheap!)
    for us to perform the experiment there. We varied between 30 and 100 images per
    class for training while keeping the same 40 images per class for testing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了测试特征提取器的强度，我们改变了提供给服务的训练数据量（[图8-20](part0010.html#effect_of_varying_size_of_training_data)）。由于微软的训练时间不到15秒，我们很容易（而且便宜！）在那里进行实验。我们在训练时每类变化了30到100张图像，同时保持测试时每类40张图像不变。
- en: '![Effect of varying size of training data per class on test F1 score (higher
    is better)](../images/00219.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![不同类别训练数据大小对测试F1分数的影响（分数越高越好）](../images/00219.jpeg)'
- en: Figure 8-20\. Effect of varying size of training data per class on test F1 score
    (higher is better)
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-20\. 不同类别训练数据大小对测试F1分数的影响（分数越高越好）
- en: Even though Microsoft recommends using at least 50 images per class, going under
    that limit did not affect performance significantly. The fact that the F1 score
    did not vary as much as one would expect shows the value of transfer learning
    (enabling less data to build classifiers) and having a good featurizer capable
    of fine-grained classification.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微软建议每类至少使用50张图像，但低于这个限制并没有显著影响性能。F1分数没有像人们预期的那样变化，这显示了迁移学习的价值（使少量数据构建分类器）以及具有能够进行细粒度分类的良好特征提取器的重要性。
- en: It bears repeating that this experiment was intentionally made difficult to
    stress-test these classifiers. On average, they would have performed much better
    on the entire Stanford Dogs dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 值得重申的是，这个实验是故意设置为困难的，以对这些分类器进行压力测试。平均而言，它们在整个斯坦福狗数据集上的表现会更好。
- en: Performance Tuning for Cloud APIs
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云API的性能调优
- en: A photograph taken by a modern cell phone can have a resolution as high as 4000
    x 3000 pixels and be upward of 4 MB in size. Depending on the network quality,
    it can take a few seconds to upload such an image to the service. With each additional
    second, it can become more and more frustrating for our users. Could we make this
    faster?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现代手机拍摄的照片可以具有高达4000 x 3000像素的分辨率，并且大小可达4 MB。根据网络质量，上传这样的图像到服务可能需要几秒钟。随着每增加一秒，对我们的用户来说可能会变得越来越令人沮丧。我们能不能让这个过程更快一些呢？
- en: 'There are two ways to reduce the size of the image:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以减小图像的大小：
- en: Resizing
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小
- en: Most CNNs take an input image with a size of 224 x 224 or 448 x 448 pixels.
    Much of a cell phone photo’s resolution would be unnecessary for a CNN. It would
    make sense to downsize the image prior to sending it over the network, instead
    of sending a large image over the network and then downsizing it on the server.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数CNN接受大小为224 x 224或448 x 448像素的输入图像。对于CNN来说，手机照片的大部分分辨率都是不必要的。在将图像发送到网络之前缩小图像大小是有意义的，而不是将大图像发送到网络，然后在服务器上缩小图像。
- en: Compression
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩
- en: Most image libraries perform *lossy* compression while saving a file. Even a
    little bit of compression can go a long way in reducing the size of the image
    while minimally affecting the quality of the image itself. Compression does introduce
    noise, but CNNs are usually robust enough to deal with some of it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数图像库在保存文件时执行*有损*压缩。即使稍微压缩一点，也可以大大减小图像的大小，同时对图像本身的质量影响很小。压缩确实会引入噪音，但CNN通常足够强大，可以处理其中的一些噪音。
- en: Effect of Resizing on Image Labeling APIs
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小对图像标记API的影响
- en: 'We performed an experiment in which we took more than a hundred diverse unmodified
    images taken from an iPhone at the default resolution (4032 x 3024) and sent them
    to the Google Cloud Vision API to get labels for each of those images. We then
    downsized each of the original images in 5% increments (5%, 10%, 15%…95%) and
    collected the API results for those smaller images, too. We then calculated the
    agreement rate for each image using the following formula:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个实验，从iPhone拍摄的一百多张不同的未经修改的图像，分辨率为默认分辨率（4032 x 3024），并将它们发送到Google Cloud
    Vision API，为每张图像获取标签。然后我们逐步缩小原始图像的大小（5%，10%，15%…95%），并收集这些较小图像的API结果。然后我们使用以下公式计算每个图像的一致性率：
- en: <math display="block"><mrow><mo>%</mo> <mi>a</mi> <mi>g</mi> <mi>r</mi> <mi>e</mi>
    <mi>e</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>a</mi> <mi>t</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>o</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow>
    <mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></mfrac>
    <mo>×</mo> <mn>100</mn></mrow></math>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo>%</mo> <mi>a</mi> <mi>g</mi> <mi>r</mi> <mi>e</mi>
    <mi>e</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>a</mi> <mi>t</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>o</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow>
    <mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></mfrac>
    <mo>×</mo> <mn>100</mn></mrow></math>
- en: '[Figure 8-21](part0010.html#effect_of_resizing_an_image_on_agreement) shows
    the results of this experiment. In the figure, the solid line shows the reduction
    in file size, and the dotted line represents the agreement rate. Our main conclusion
    from the experiment was that a 60% reduction in resolution led to a 95% reduction
    in file size, with little change in accuracy compared to the original images.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-21](part0010.html#effect_of_resizing_an_image_on_agreement)显示了这个实验的结果。在图中，实线显示了文件大小的减小，虚线表示一致性率。我们从实验中得出的主要结论是，将分辨率减小60%会导致文件大小减小95%，与原始图像相比准确性几乎没有变化。'
- en: '![Effect of resizing an image on agreement rate and file size reduction relative
    to the original image](../images/00174.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![调整图像大小对一致性率和相对于原始图像的文件大小减小的影响](../images/00174.jpeg)'
- en: Figure 8-21\. Effect of resizing an image on agreement rate and file size reduction
    relative to the original image
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-21\. 调整图像大小对一致性率和相对于原始图像的文件大小减小的影响
- en: Effect of Compression on Image Labeling APIs
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩对图像标记API的影响
- en: We repeated the same experiment, but instead of changing the resolution, we
    changed the compression factor for each image incrementally. In [Figure 8-22](part0010.html#effect_of_compressing_an_image_on_agreem),
    the solid line shows the reduction in file size and the dotted line represents
    the agreement rate. The main takeaway here is that a 60% compression score (or
    40% quality) leads to an 85% reduction in file size, with little change in accuracy
    compared to the original image.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复了相同的实验，但是不改变分辨率，而是逐步改变每个图像的压缩因子。在[图8-22](part0010.html#effect_of_compressing_an_image_on_agreem)中，实线显示了文件大小的减少，虚线代表协议率。这里的主要要点是，60%的压缩得分（或40%的质量）导致文件大小减少85%，与原始图像相比，准确性几乎没有变化。
- en: '![Effect of compressing an image on agreement rate and file size reduction
    relative to the original image](../images/00042.jpeg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![压缩图像对协议率和文件大小相对于原始图像的影响](../images/00042.jpeg)'
- en: Figure 8-22\. Effect of compressing an image on agreement rate and file size
    reduction relative to the original image
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-22。压缩图像对协议率和文件大小相对于原始图像的影响
- en: Effect of Compression on OCR APIs
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩对OCR API的影响
- en: We took a document containing 300-plus words at the default resolution of an
    iPhone (4032 x 3024), and sent it to the Microsoft Cognitive Services API to test
    text recognition. We then compressed it at 5% increments and then sent each image
    and compressed it. We sent these images to the same API and compared their results
    against the baseline to calculate the percentage WER. We observed that even setting
    the compression factor to 95% (i.e., 5% quality of the original image) had no
    effect on the quality of results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拿一份包含300多个单词的文档，分辨率为iPhone的默认分辨率（4032 x 3024），并将其发送到Microsoft Cognitive Services
    API进行文本识别测试。然后我们逐步以5%的增量压缩它，然后发送每个图像并压缩它。我们将这些图像发送到同一API，并将它们的结果与基线进行比较，以计算百分比的错误率。我们观察到，即使将压缩因子设置为95%（即原始图像质量的5%），也不会对结果的质量产生影响。
- en: Effect of Resizing on OCR APIs
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小对OCR API的影响
- en: We repeated the previous experiment, but this time by resizing each image instead
    of compressing. After a certain point, the WER jumped from none to almost 100%,
    with nearly all words being misclassified. Retesting this with another document
    having each word at a different font size showed that all words under a particular
    font size were getting misclassified. To effectively recognize text, OCR engines
    need the text to be bigger than a minimum height (a good rule of thumb is larger
    than 20 pixels). Hence the higher the resolution, the higher the accuracy.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复了之前的实验，但这次是通过调整每个图像的大小而不是压缩。在某一点之后，错误率从零跳升到接近100%，几乎所有单词都被错误分类。重新测试另一份文档，其中每个单词的字体大小都不同，结果显示特定字体大小以下的所有单词都被错误分类。为了有效识别文本，OCR引擎需要文本的高度大于最小高度（一个很好的经验法则是大于20像素）。因此，分辨率越高，准确性就越高。
- en: What have we learned?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了什么？
- en: For text recognition, compress images heavily, but do not resize.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文本识别，大幅压缩图像，但不要调整大小。
- en: For image labeling, a combination of moderate resizing (say, 50%) and moderate
    compression (say, 30%) should lead to heavy file size reductions (and quicker
    API calls) without any difference in quality of API results.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像标记，适度调整大小（例如50%）和适度压缩（例如30%）的组合应该会导致文件大小大幅减少（并且API调用更快），而不会影响API结果的质量。
- en: Depending on your application, you might be working with already resized and
    compressed images. Every processing step can introduce a slight difference in
    the results of these APIs, so aim to minimize them.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据您的应用程序，您可能正在处理已经调整大小和压缩的图像。每个处理步骤都可能会对这些API的结果产生轻微差异，因此应该尽量减少它们。
- en: Tip
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'After receiving an image, cloud APIs internally resize it to fit their own
    implementation. For us, this means two levels of resizing: we first resize an
    image to reduce the size, then send it to the cloud API, which further resizes
    the image. Downsizing images introduces distortion, which is more evident at lower
    resolutions. We can minimize the effect of distortion by resizing from a higher
    resolution, which is bigger by a few multiples. For example, resizing 3024x3024
    (original) → 302x302 (being sent to cloud) → 224x224 (internally resized by APIs)
    would introduce much more distortion in the final image compared to 3024x3024
    → 896x896 → 224x224\. Hence, it’s best to find a happy intermediate size before
    sending the images. Additionally, specifying advanced interpolation options like
    `BICUBIC` and `LANCZOS` will lead to more accurate representation of the original
    image in the smaller version.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 收到图像后，云API会在内部调整大小以适应其自身的实现。对我们来说，这意味着两个级别的调整大小：我们首先调整图像大小以减小尺寸，然后将其发送到云API，后者会进一步调整图像大小。缩小图像会引入失真，这在较低分辨率下更为明显。我们可以通过从更高分辨率调整大小来最小化失真的影响，这个分辨率比原始分辨率大几倍。例如，将
    3024x3024（原始）→ 302x302（发送到云端）→ 224x224（API内部调整大小）相比于 3024x3024 → 896x896 → 224x224，最终图像中的失真会更多。因此，在发送图像之前最好找到一个合适的中间尺寸。此外，指定高级插值选项如
    `BICUBIC` 和 `LANCZOS` 将导致更准确地表示原始图像在较小版本中的效果。
- en: Case Studies
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Some people say that the best things in life don’t come easy. We believe this
    chapter proves otherwise. In the following section, we take a look at how some
    tech industry titans use cloud APIs for AI to drive some very compelling scenarios.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有人说生活中最好的事情并不容易获得。我们相信这一章证明了相反。在接下来的部分中，我们将看看一些科技行业巨头如何利用云API进行人工智能，从而推动一些非常引人注目的场景。
- en: The New York Times
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 纽约时报
- en: It might seem like the scenario painted at the beginning of the chapter was
    taken out of a cartoon, but it was, in fact, pretty close to the case of the *New
    York Times* (NYT). With more than 160 years of illustrious history, NYT has a
    treasure trove of photographs in its archives. It stored many of these artifacts
    in the basement of its building three stories below the ground level, aptly called
    the “morgue.” The value of this collection is priceless. In 2015, due to a plumbing
    leak, parts of the basement were damaged including some of these archived records.
    Thankfully the damage was minimal. However, this prompted NYT to consider digitally
    archiving them to protect against another catastrophe.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来像是本章开头描绘的情景取自卡通，但实际上与《纽约时报》（NYT）的情况非常接近。拥有160多年的光辉历史，纽约时报在其档案中拥有大量的照片珍藏品。它将许多这些文物存放在地下三层的建筑物地下室中，恰当地称为“尸房”。这个收藏的价值是无价的。2015年，由于管道泄漏，地下室的部分地方受损，包括其中一些存档记录。幸运的是，损坏很小。然而，这促使纽约时报考虑对其进行数字化存档，以防止另一场灾难。
- en: The photographs were scanned and stored in high quality. However, the photographs
    themselves did not have any identifying information. What many of them did have
    were handwritten or printed notes on the backside giving context for the photographs.
    NYT used the Google Vision API to scan this text and tag the respective images
    with that information. Additionally, this pipeline provided opportunities to extract
    more metadata from the photographs, including landmark recognition, celebrity
    recognition, and so on. These newly added tags powered its search feature so that
    anyone within the company and outside could explore the gallery and search using
    keywords, dates, and so on without having to visit the morgue, three stories down.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些照片被扫描并以高质量存储。然而，这些照片本身没有任何识别信息。许多照片的背面有手写或印刷的注释，为照片提供了背景信息。纽约时报使用Google Vision
    API扫描这些文本，并为相应的图像添加标签。此外，这个流程提供了从照片中提取更多元数据的机会，包括地标识别、名人识别等等。这些新添加的标签为其搜索功能提供动力，使公司内外的任何人都可以浏览画廊并使用关键词、日期等搜索，而无需访问地下三层的尸房。
- en: Uber
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Uber
- en: Uber uses Microsoft Cognitive Services to identify each of its seven million-plus
    drivers in a couple of milliseconds. Imagine the sheer scale at which Uber must
    operate its new feature called “Real-Time ID Check.” This feature verifies that
    the current driver is indeed the registered driver by prompting them to take a
    selfie either randomly or every time they are assigned to a new rider. This selfie
    is compared to the driver’s photo on file, and only if the face models are a match
    is the driver allowed to continue. This security feature is helpful for building
    accountability by ensuring the security of the passengers and by ensuring that
    the driver’s account is not compromised. This safety feature is able to detect
    changes in the selfie, including a hat, beard, sunglasses, and more, and then
    prompts the driver to take a selfie without the hat or sunglasses.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Uber使用Microsoft Cognitive Services在几毫秒内识别其700多万司机中的每一个。想象一下Uber必须以何种规模运营其名为“实时身份验证”的新功能。该功能通过提示司机随机或每次分配给新乘客时自拍来验证当前司机是否确实是注册司机。这张自拍照与文件中的司机照片进行比对，只有在脸部模型匹配时才允许司机继续。这一安全功能有助于通过确保乘客的安全和确保司机账户不受损害来建立问责制。这一安全功能能够检测自拍中的变化，包括帽子、胡须、太阳镜等，然后提示司机摘掉帽子或太阳镜自拍。
- en: '![The Uber Drivers app prompts the driver to take a selfie to verify the identity
    of the driver](../images/00208.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![Uber Drivers应用程序提示司机自拍以验证司机的身份](../images/00208.jpeg)'
- en: Figure 8-23\. The Uber Drivers app prompts the driver to take a selfie to verify
    the identity of the driver ([image source](https://oreil.ly/lw1Ho))
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-23\. Uber Drivers应用程序提示司机自拍以验证司机的身份（[图片来源](https://oreil.ly/lw1Ho)）
- en: Giphy
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Giphy
- en: Back in 1976, when Dr. Richard Dawkins coined the term “meme,” little did he
    know it would take on a life of its own four decades later. Instead of giving
    a simple textual reply, we live in a generation where most chat applications suggest
    an appropriate animated GIF matching the context. Several applications provide
    a search specific to memes and GIFs, such as Tenor, Facebook messenger, Swype,
    and Swiftkey. Most of them search through Giphy ([Figure 8-24](part0010.html#giphy_extracts_text_from_animations_as_m)),
    the world’s largest search engine for animated memes commonly in the GIF format.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 回到1976年，当理查德·道金斯博士创造了“模因”一词时，他并不知道四十年后它会变得如此活跃。我们生活在一个大多数聊天应用程序建议匹配上下文的适当动画GIF的时代，而不是给出简单的文本回复。几个应用程序提供了专门搜索模因和GIF的功能，如Tenor、Facebook
    Messenger、Swype和Swiftkey。它们大多通过Giphy进行搜索（[图8-24](part0010.html#giphy_extracts_text_from_animations_as_m)），这是世界上最大的动画模因搜索引擎，通常以GIF格式呈现。
- en: '![Giphy extracts text from animations as metadata for searching](../images/00291.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![Giphy从动画中提取文本作为搜索的元数据](../images/00291.jpeg)'
- en: Figure 8-24\. Giphy extracts text from animations as metadata for searching
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-24\. Giphy从动画中提取文本作为搜索的元数据
- en: GIFs often have text overlaid (like the dialogue being spoken) and sometimes
    we want to look for a GIF with a particular dialogue straight from a movie or
    TV show. For example, the image in [Figure 8-24](part0010.html#giphy_extracts_text_from_animations_as_m)
    from the 2010 *Futurama* episode in which the “eyePhone” (sic) was released is
    often used to express excitement toward a product or an idea. Having an understanding
    of the contents makes the GIFs more searchable. To make this happen, Giphy uses
    Google’s Vision API to extract the recognize text and objects—aiding the search
    for the perfect GIF.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: GIF通常有叠加的文本（比如正在说的对话），有时我们想要寻找一个具有特定对话的GIF，直接来自电影或电视节目。例如，来自2010年《未来派》剧集的图像[图8-24](part0010.html#giphy_extracts_text_from_animations_as_m)，其中“eyePhone”（错误）发布时经常被用来表达对产品或想法的兴奋。了解内容使GIF更易搜索。为了实现这一点，Giphy使用Google的Vision
    API来提取识别文本和对象，帮助搜索完美的GIF。
- en: It’s obvious that tagging GIFs is a difficult task because a person must sift
    through millions of these animations and manually annotate them frame by frame.
    In 2017, Giphy figured out two solutions to automate this process. The first approach
    was to detect text from within the image. The second approach was to generate
    tags based on the objects in the image to supplement the metadata for their search
    engine. This metadata is stored and searched using ElasticSearch to make a scalable
    search engine.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，给GIF打标签是一项困难的任务，因为一个人必须筛选数百万个这些动画，并逐帧手动注释它们。2017年，Giphy找到了两种自动化这一过程的解决方案。第一种方法是从图像中检测文本。第二种方法是根据图像中的对象生成标签，以补充其搜索引擎的元数据。这些元数据存储和搜索使用ElasticSearch来创建一个可扩展的搜索引擎。
- en: 'For text detection, the company used the OCR services from the Google Vision
    API on the first frame from the GIFs to confirm whether the GIF actually contained
    text. If the API replied in the affirmative, Giphy would send the next frames,
    receive their OCR-detected texts, and figure out the differences in the text;
    for instance, whether the text was static (remaining the same throughout the duration
    of the gif) or dynamic (different text in different frames). For generating the
    class labels corresponding to objects in the image, engineers had two options:
    label detection or web entities, both of which are available on Google Vision
    API. Label detection, as the name suggests, provides the actual class name of
    the object. Web entities provides an entity ID (which can be referenceable in
    the Google Knowledge Graph), which is the unique web URL for identical and similar
    images seen elsewhere on the net. Using these additional annotations gave the
    new system an increase in the click-through-rate (CTR) by 32%. Medium-to-long-tail
    searches (i.e., not-so-frequent searches) benefitted the most, becoming richer
    with relevant content as the extracted metadata surfaced previously unannotated
    GIFs that would have otherwise been hidden. Additionally, this metadata and click-through
    behavior of users provides data to make a similarity and deduplication feature.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本检测，该公司使用Google Vision API的OCR服务对GIF的第一帧进行确认，以确定GIF是否实际包含文本。如果API回复肯定，Giphy将发送下一帧，接收其OCR检测到的文本，并找出文本的差异；例如，文本是静态的（在gif的整个持续时间内保持不变）还是动态的（不同帧中的不同文本）。为了生成与图像中对象对应的类标签，工程师有两个选择：标签检测或网络实体，这两者都可以在Google
    Vision API上使用。标签检测提供对象的实际类名。网络实体提供一个实体ID（可在Google Knowledge Graph中引用），这是在网上看到的相同和相似图像的唯一网址。使用这些额外的注释使新系统的点击率（CTR）增加了32%。中长尾搜索（即不太频繁的搜索）受益最大，因为提取的元数据使以前未注释的GIF浮出水面，否则这些GIF将被隐藏。此外，用户的元数据和点击行为提供了数据，以制作相似性和去重功能。
- en: OmniEarth
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OmniEarth
- en: OmniEarth is a Virginia-based company that specializes in collecting, analyzing,
    and combining satellite and aerial imagery with other datasets to track water
    usage across the country, scalably, and at high speeds. The company is able to
    scan the entire United States at a total of 144 million parcels of land within
    hours. Internally, it uses the IBM Watson Visual Recognition API to classify images
    of land parcels for valuable information like how green it is. Combining this
    classification with other data points such as temperature and rainfall, OmniEarth
    can predict how much water was used to irrigate the field.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: OmniEarth是一家总部位于弗吉尼亚州的公司，专门收集、分析和结合卫星和航拍图像以及其他数据集，以可扩展和高速的方式跟踪全国的用水情况。该公司能够在几小时内扫描整个美国境内1.44亿个土地地块。在内部，它使用IBM
    Watson Visual Recognition API对土地地块的图像进行分类，以获取有价值的信息，比如绿化程度。将这种分类与温度和降雨等其他数据点结合起来，OmniEarth可以预测用于灌溉田地的水量。
- en: For house properties, it infers data points from the image such as the presence
    of pools, trees, or irrigable landscaping to predict the amount of water usage.
    The company even predicted where water is being wasted due to malpractices like
    overwatering or leaks. OmniEarth helped the state of California understand water
    consumption by analyzing more than 150,000 parcels of land, and then devised an
    effective strategy to curb water waste.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于房产，它从图像中推断出数据点，比如游泳池、树木或可灌溉的园艺景观的存在，以预测用水量。该公司甚至预测了由于过度浇水或漏水等不当做法而导致的水浪费情况。OmniEarth通过分析超过15万个土地地块，帮助加利福尼亚州了解用水情况，然后制定了一个有效的策略来遏制水资源浪费。
- en: Photobucket
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Photobucket
- en: Photobucket is a popular online image- and video-hosting community where more
    than two million images are uploaded every day. Using Clarifai’s NSFW models,
    Photobucket automatically flags unwanted or offensive user-generated content and
    sends it for further review to its human moderation team. Previously, the company’s
    human moderation team was able to monitor only about 1% of the incoming content.
    About 70% of the flagged images turned out to be unacceptable content. Compared
    to previous manual efforts, Photobucket identified 700 times more unwanted content,
    thus cleaning the website and creating a better UX. This automation also helped
    discover two child pornography accounts, which were reported to the FBI.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Photobucket是一个流行的在线图像和视频托管社区，每天上传超过两百万张图片。使用Clarifai的NSFW模型，Photobucket自动标记不受欢迎或冒犯性的用户生成内容，并将其发送给人工审核团队进行进一步审核。此前，该公司的人工审核团队只能监控到约1%的传入内容。大约70%的被标记图片被发现是不可接受的内容。与以往的手动努力相比，Photobucket识别出了700倍更多的不受欢迎内容，从而清理了网站并创造了更好的用户体验。这种自动化还帮助发现了两个儿童色情账户，并向FBI报告了这一情况。
- en: Staples
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Staples
- en: Ecommerce stores like Staples often rely on organic search engine traffic to
    drive sales. One of the methods to appear high in search engine rankings is to
    put descriptive image tags in the ALT text field for the image. Staples Europe,
    which serves 12 different languages, found tagging product images and translating
    keywords to be an expensive proposition, which is traditionally outsourced to
    human agencies. Fortunately, Clarifai provides tags in 20 languages at a much
    cheaper rate, saving Staples costs into five figures. Using these relevant keywords
    led to an increase in traffic and eventually increased sales through its ecommerce
    store due to a surge of visitors to the product pages.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 像Staples这样的电子商务商店通常依赖有机搜索引擎流量来推动销售。在搜索引擎排名中高居榜首的方法之一是在图像的ALT文本字段中放置描述性图像标签。为了在12种不同语言中提供服务的Staples
    Europe发现标记产品图像和翻译关键字是一项昂贵的任务，传统上是外包给人类机构的。幸运的是，Clarifai以更便宜的价格提供20种语言的标签，为Staples节省了数万美元的成本。使用这些相关关键字导致了流量的增加，最终通过其电子商务商店增加了销售额，因为产品页面的访问者激增。
- en: InDro Robotics
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InDro Robotics
- en: This Canadian drone company uses Microsoft Cognitive Services to power search
    and rescue operations, not only during natural disasters but also to proactively
    detect emergencies. The company utilizes Custom Vision to train models specifically
    for identifying objects such as boats and life vests in water ([Figure 8-25](part0010.html#detections_made_by_indro_robotics))
    and use this information to notify control stations. These drones are able to
    scan much larger ocean spans on their own, as compared to lifeguards. This automation
    alerts the lifeguard of emergencies, thus improving the speed of discovery and
    saving lives in the process.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这家加拿大无人机公司利用Microsoft Cognitive Services来支持搜索和救援行动，不仅在自然灾害期间，还主动检测紧急情况。该公司利用Custom
    Vision专门训练模型，用于识别水中的船只和救生衣（[图8-25](part0010.html#detections_made_by_indro_robotics)），并利用这些信息通知控制站。与救生员相比，这些无人机能够独自扫描更大范围的海洋。这种自动化可以提醒救生员有紧急情况，从而提高发现速度并在过程中挽救生命。
- en: Australia has begun using drones from other companies coupled with inflatable
    pods to be able to react until help reaches. Soon after deployment, these pods
    saved two teenagers stranded in the ocean, as demonstrated in [Figure 8-26](part0010.html#drone_identifies_two_stranded_swimmers_a).
    Australia is also utilizing drones to detect sharks so that beaches can be vacated.
    It’s easy to foresee the tremendous value these automated, custom training services
    can bring.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚已经开始使用其他公司的无人机配备充气舱，以便在帮助到达之前能够做出反应。部署后不久，这些充气舱拯救了两名被困在海洋中的青少年，如[图8-26](part0010.html#drone_identifies_two_stranded_swimmers_a)所示。澳大利亚还利用无人机来检测鲨鱼，以便撤离海滩。可以预见到这些自动化、定制培训服务所能带来的巨大价值。
- en: '![Detections made by InDro Robotics](../images/00250.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![InDro Robotics进行的检测](../images/00250.jpeg)'
- en: Figure 8-25\. Detections made by InDro Robotics
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-25\. InDro Robotics进行的检测
- en: '![Drone identifies two stranded swimmers and releases an inflatable pod that
    they cling onto (image source)](../images/00210.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![无人机识别两名被困游泳者并释放充气舱，他们抓住充气舱（图片来源）](../images/00210.jpeg)'
- en: Figure 8-26\. Drone identifies two stranded swimmers and releases an inflatable
    pod that they cling onto ([image source](https://oreil.ly/dPxBv))
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-26\. 无人机识别两名被困游泳者并释放充气舱，他们抓住充气舱（图片来源）
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored various cloud APIs for computer vision, first qualitatively
    comparing the breadth of services offered and then quantitatively comparing their
    accuracy and price. We also looked at potential sources of bias that might appear
    in the results. We saw that with just a short code snippet, we can get started
    using these APIs in less than 15 minutes. Because one model doesn’t fit all, we
    trained a custom classifier using a drag-and-drop interface, and tested multiple
    companies against one another. Finally, we discussed compression and resizing
    recommendations to speed up image transmission and how they affect different tasks.
    To top it all off, we examined how companies across industries use these cloud
    APIs for building real-world applications. Congratulations on making it this far!
    In the next chapter, we will see how to deploy our own inference server for custom
    scenarios.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种云API用于计算机视觉，首先定性比较提供的服务范围，然后定量比较它们的准确性和价格。我们还看到可能出现在结果中的偏见来源。我们看到，只需一小段代码片段，我们就可以在不到15分钟内开始使用这些API。因为一个模型并不适用于所有情况，我们使用拖放界面训练了一个自定义分类器，并将多家公司进行了对比测试。最后，我们讨论了压缩和调整大小的建议，以加快图像传输速度，并了解它们对不同任务的影响。最后，我们研究了各行业的公司如何利用这些云API构建真实应用程序。祝贺您走到了这一步！在下一章中，我们将看到如何为自定义场景部署我们自己的推理服务器。
