- en: 15 Deploying to production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 部署到生产环境
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Options for deploying PyTorch models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署PyTorch模型的选项
- en: Working with the PyTorch JIT
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch JIT
- en: Deploying a model server and exporting models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型服务器和导出模型
- en: Running exported and natively implemented models from C++
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在C++中运行导出和本地实现的模型
- en: Running models on mobile
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在移动设备上运行模型
- en: In part 1 of this book, we learned a lot about models; and part 2 left us with
    a detailed path for creating good models for a particular problem. Now that we
    have these great models, we need to take them where they can be useful. Maintaining
    infrastructure for executing inference of deep learning models at scale can be
    impactful from an architectural as well as cost standpoint. While PyTorch started
    off as a framework focused on research, beginning with the 1.0 release, a set
    of production-oriented features were added that today make PyTorch an ideal end-to-end
    platform from research to large-scale production.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分，我们学到了很多关于模型的知识；第二部分为我们提供了创建特定问题的好模型的详细路径。现在我们有了这些优秀的模型，我们需要将它们带到可以发挥作用的地方。在规模化执行深度学习模型推理的基础设施维护方面，从架构和成本的角度来看都具有影响力。虽然PyTorch最初是一个专注于研究的框架，但从1.0版本开始，添加了一组面向生产的功能，使PyTorch成为从研究到大规模生产的理想端到端平台。
- en: 'What deploying to production means will vary with the use case:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 部署到生产环境意味着会根据用例而有所不同：
- en: 'Perhaps the most natural deployment for the models we developed in part 2 would
    be to set up a network service providing access to our models. We’ll do this in
    two versions using lightweight Python web frameworks: Flask ([http:// flask.pocoo.org](http://flask.pocoo.org))
    and Sanic ([https://sanicframework.org](https://sanicframework.org)). The first
    is arguably one of the most popular of these frameworks, and the latter is similar
    in spirit but takes advantage of Python’s new async/await support for asynchronous
    operations for efficiency.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在第二部分开发的模型可能最自然的部署方式是建立一个网络服务，提供对我们模型的访问。我们将使用轻量级的Python Web框架来实现这一点：Flask
    ([http:// flask.pocoo.org](http://flask.pocoo.org)) 和 Sanic ([https://sanicframework.org](https://sanicframework.org))。前者可以说是这些框架中最受欢迎的之一，后者在精神上类似，但利用了Python的新的异步操作支持async/await来提高效率。
- en: We can export our model to a well-standardized format that allows us to ship
    it using optimized model processors, specialized hardware, or cloud services.
    For PyTorch models, the Open Neural Network Exchange (ONNX) format fills this
    role.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将我们的模型导出为一个标准化的格式，允许我们使用优化的模型处理器、专门的硬件或云服务进行部署。对于PyTorch模型，Open Neural Network
    Exchange (ONNX)格式起到了这样的作用。
- en: We may wish to integrate our models into larger applications. For this it would
    be handy if we were not limited to Python. Thus we will explore using PyTorch
    models from C++ with the idea that this also is a stepping-stone to any language.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能希望将我们的模型集成到更大的应用程序中。为此，如果我们不受Python的限制将会很方便。因此，我们将探讨使用PyTorch模型从C++中使用的想法，这也是通往任何语言的一个过渡。
- en: Finally, for some things like the image zebraification we saw in chapter 2,
    it may be nice to run our model on mobile devices. While it is unlikely that you
    will have a CT module for your mobile, other medical applications like do-it-yourself
    skin screenings may be more natural, and the user might prefer running on the
    device versus having their skin sent to a cloud service. Luckily for us, PyTorch
    has gained mobile support recently, and we will explore that.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，对于一些像我们在第2章中看到的图像斑马化这样的事情，可能很好地在移动设备上运行我们的模型。虽然你不太可能在手机上有一个CT模块，但其他医疗应用程序如自助皮肤检查可能更自然，用户可能更喜欢在设备上运行而不是将他们的皮肤发送到云服务。幸运的是，PyTorch最近增加了移动支持，我们将探索这一点。
- en: As we learn how to implement these use cases, we will use the classifier from
    chapter 14 as our first example for serving, and then switch to the zebraification
    model for the other bits of deployment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们学习如何实现这些用例时，我们将以第14章��分类器作为我们提供服务的第一个示例，然后切换到斑马化模型处理其他部署的内容。
- en: 15.1 Serving PyTorch models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 提供PyTorch模型
- en: We’ll begin with what it takes to put our model on a server. Staying true to
    our hands-on approach, we’ll start with the simplest possible server. Once we
    have something basic that works, we’ll take look at its shortfalls and take a
    stab at resolving them. Finally, we’ll look at what is, at the time of writing,
    the future. Let’s get something that listens on the network.[¹](#pgfId-1011921)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从将模型放在服务器上需要做什么开始。忠于我们的实践方法，我们将从最简单的服务器开始。一旦我们有了基本的工作内容，我们将看看它的不足之处，并尝试解决。最后，我们将看看在撰写本文时的未来。让我们创建一个监听网络的东西。[¹](#pgfId-1011921)
- en: 15.1.1 Our model behind a Flask server
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 我们的模型在Flask服务器后面
- en: Flask is one of the most widely used Python modules. It can be installed using
    `pip`:[²](#pgfId-1011965)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Flask是最广泛使用的Python模块之一。可以使用`pip`进行安装：[²](#pgfId-1011965)
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The API can be created by decorating functions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: API可以通过装饰函数创建。
- en: Listing 15.1 flask_hello_world.py:1
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.1 flask_hello_world.py:1
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When started, the application will run at port 8000 and expose one route, /hello,
    that returns the “Hello World” string. At this point, we can augment our Flask
    server by loading a previously saved model and exposing it through a `POST` route.
    We will use the nodule classifier from chapter 14 as an example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序启动后将在端口8000上运行，并公开一个路由`/hello`，返回“Hello World”字符串。此时，我们可以通过加载先前保存的模型并通过`POST`路由公开它来增强我们的Flask服务器。我们将以第14章的模块分类器为例。
- en: We’ll use Flask’s (somewhat curiously imported) `request` to get our data. More
    precisely, request.files contains a dictionary of file objects indexed by field
    names. We’ll use JSON to parse the input, and we’ll return a JSON string using
    flask’s `jsonify` helper.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Flask的（有点奇怪地导入的）`request`来获取我们的数据。更准确地说，request.files包含一个按字段名称索引的文件对象字典。我们将使用JSON来解析输入，并使用flask的`jsonify`助手返回一个JSON字符串。
- en: Instead of /hello, we will now expose a /predict route that takes a binary blob
    (the pixel content of the series) and the related metadata (a JSON object containing
    a dictionary with `shape` as a key) as input files provided with a `POST` request
    and returns a JSON response with the predicted diagnosis. More precisely, our
    server takes one sample (rather than a batch) and returns the probability that
    it is malignant.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将暴露一个/predict路由，该路由接受一个二进制块（系列的像素内容）和相关的元数据（包含一个以`shape`为键的字典的JSON对象）作为`POST`请求提供的输入文件，并返回一个JSON响应，其中包含预测的诊断。更确切地说，我们的服务器接受一个样本（而不是一批），并返回它是恶性的概率。
- en: In order to get to the data, we first need to decode the JSON to binary, which
    we can then decode into a one-dimensional array with `numpy.frombuffer`. We’ll
    convert this to a tensor with `torch.from_numpy` and view its actual shape.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取数据，我们首先需要将JSON解码为二进制，然后使用`numpy.frombuffer`将其解码为一维数组。我们将使用`torch.from_numpy`将其转换为张量，并查看其实际形状。
- en: 'The actual handling of the model is just like in chapter 14: we’ll instantiate
    `LunaModel` from chapter 14, load the weights we got from our training, and put
    the model in `eval` mode. As we are not training anything, we’ll tell PyTorch
    that we will not want gradients when running the model by running in a `with torch.no_grad()`
    block.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的实际处理方式就像第14章中一样：我们将从第14章实例化`LunaModel`，加载我们从训练中得到的权重，并将模型置于`eval`模式。由于我们不进行训练任何东西，我们会在`with
    torch.no_grad()`块中告诉PyTorch在运行模型时不需要梯度。
- en: Listing 15.2 flask_server.py:1
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.2 flask_server.py:1
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Sets up our model, loads the weights, and moves to evaluation mode
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置我们的模型，加载权重，并转换为评估模式
- en: ❷ No autograd for us.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对我们来说没有自动求导。
- en: ❸ We expect a form submission (HTTP POST) at the “/predict” endpoint.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们期望在“/predict”端点进行表单提交（HTTP POST）。
- en: ❹ Our request will have one file called meta.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们的请求将有一个名为meta的文件。
- en: ❺ Converts our data from binary blob to torch
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将我们的数据从二进制块转换为torch
- en: ❻ Encodes our response content as JSON
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将我们的响应内容编码为JSON
- en: 'Run the server as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行服务器的方法如下：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We prepared a trivial client at cls_client.py that sends a single example. From
    the code directory, you can run it as
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在cls_client.py中准备了一个简单的客户端，发送一个示例。从代码目录中，您可以运行它如下：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It should tell you that the nodule is very unlikely to be malignant. Clearly,
    our server takes inputs, runs them through our model, and returns the outputs.
    So are we done? Not quite. Let’s look at what could be better in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该告诉您结节极不可能是恶性的。显然，我们的服务器接受输入，通过我们的模型运行它们，并返回输出。那我们完成了吗？还不完全。让我们看看下一节中可以改进的地方。
- en: 15.1.2 What we want from deployment
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 部署的期望
- en: Let’s collect some things we desire for serving models.[³](#pgfId-1012837) First,
    we want to support *modern protocols and their features*. Old-school HTTP is deeply
    serial, which means when a client wants to send several requests in the same connection,
    the next requests will only be sent after the previous request has been answered.
    Not very efficient if you want to send a batch of things. We will partially deliver
    here--our upgrade to Sanic certainly moves us to a framework that has the ambition
    to be very efficient.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们收集一些为提供模型服务而期望的事情。首先，我们希望支持*现代协议及其特性*。老式的HTTP是深度串行的，这意味着当客户端想要在同一连接中发送多个请求时，下一个请求只会在前一个��求得到回答后才会发送。如果您想发送一批东西，这并不是很有效。我们在这里部分交付--我们升级到Sanic肯定会使我们转向一个有雄心成为非常高效的框架。
- en: When using GPUs, it is often much more efficient to *batch requests* than to
    process them one by one or fire them in parallel. So next, we have the task of
    collecting requests from several connections, assembling them into a batch to
    run on the GPU, and then getting the results back to the respective requesters.
    This sounds elaborate and (again, when we write this) seems not to be done very
    often in simple tutorials. That is reason enough for us to do it properly here!
    Note, though, that until latency induced by the duration of a model run is an
    issue (in that waiting for our own run is OK; but waiting for the batch that’s
    running when the request arrives to finish, and then waiting for our run to give
    results, is prohibitive), there is little reason to run multiple batches on one
    GPU at a given time. Increasing the maximum batch size will generally be more
    efficient.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GPU时，批量请求通常比逐个处理或并行处理更有效。因此，接下来，我们的任务是从几个连接收集请求，将它们组装成一个批次在GPU上运行，然后将结果返回给各自的请求者。这听起来很复杂，（再次，当我们编写这篇文章时）似乎在简单的教程中并不经常做。这足以让我们在这里正确地做。但请注意，直到由模型运行持续时间引起的延迟成为问题（在等待我们自己的运行时是可以的；但在请求到达时等待正在运行的批次完成，然后等待我们的运行给出结果是禁止的），在给定时间内在一个GPU上运行多个批次没有太多理由。增加最大批量大小通常更有效。
- en: We want to serve several things in *parallel*. Even with asynchronous serving,
    we need our model to run efficiently on a second thread--this means we want to
    escape the (in)famous Python global interpreter lock (GIL) with our model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望*并行*提供几件事情。即使使用异步提供服务，我们也需要我们的模型在第二个线程上高效运行--这意味着我们希望通过我们的模型摆脱（臭名昭著的）Python全局解释器锁（GIL）。
- en: We also want to do as *little copying* as possible. Both from a memory-consumption
    and a time perspective, copying things over and over is bad. Many HTTP things
    are encoded in Base64 (a format restricted to 6 bits per byte to encode binary
    in more or less alphanumeric strings), and--say, for images--decoding that to
    binary and then again to a tensor and then to the batch is clearly relatively
    expensive. We will partially deliver on this--we’ll use streaming `PUT` requests
    to not allocate Base64 strings and to avoid growing strings by successively appending
    to them (which is terrible for performance for strings as much as tensors). We
    say we do not deliver completely because we are not truly minimizing the copying,
    though.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望尽量减少*复制*。无论从内存消耗还是时间的角度来看，反复复制东西都是不好的。许多 HTTP 事物都是以 Base64 编码（一种将二进制编码为更多或更少字母数字字符串的格式，每字节限制为
    6 位）的形式编码的，比如，对于图像，将其解码为二进制，然后再转换为张量，然后再转换为批处理显然是相对昂贵的。我们将部分实现这一点——我们将使用流式`PUT`请求来避免分配
    Base64 字符串，并避免通过逐渐追加到字符串来增长字符串（对于字符串和张量来说，这对性能非常糟糕）。我们说我们没有完全实现，因为我们并没有真正最小化复制。
- en: The last desirable thing for serving is *safety*. Ideally, we would have safe
    decoding. We want to guard against both overflows and resource exhaustion. Once
    we have a fixed-size input tensor, we should be mostly good, as it is hard to
    crash PyTorch starting from fixed-sized inputs. The stretch to get there, decoding
    images and the like, is likely more of a headache, and we make no guarantees.
    Internet security is a large enough field that we will not cover it at all. We
    should note that neural networks are known to be susceptible to manipulation of
    the inputs to generate desired but wrong or unforeseen outputs (known as *adversarial
    examples*), but this isn’t extremely pertinent to our application, so we’ll skip
    it here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供服务，最后一个理想的事情是*安全性*。理想情况下，我们希望有安全的解码。我们希望防止溢出和资源耗尽。一旦我们有了固定大小的输入张量，我们应该大部分都没问题，因为从固定大小的输入开始很难使
    PyTorch 崩溃。为了达到这个目标，解码图像等工作可能更令人头疼，我们不做任何保证。互联网安全是一个足够庞大的领域，我们将完全不涉及它。我们应该注意到神经网络容易受到输入操纵以生成期望但错误或意想不到的输出（称为*对抗性示例*），但这与我们的应用并不是非常相关，所以我们会在这里跳过它。
- en: Enough talk. Let’s improve on our server.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 言归正传。让我们改进一下我们的服务器。
- en: 15.1.3 Request batching
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.3 请求批处理
- en: Our second example server will use the Sanic framework (installed via the Python
    package of the same name). This will give us the ability to serve many requests
    in parallel using asynchronous processing, so we’ll tick that off our list. While
    we are at it, we will also implement request batching.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个示例服务器将使用 Sanic 框架（通过同名的 Python 包安装）。这将使我们能够使用异步处理来并行处理许多请求，因此我们将在列表中勾选它。顺便说一句，我们还将实现请求批处理。
- en: '![](../Images/CH15_F01_Stevens2_GS.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F01_Stevens2_GS.png)'
- en: Figure 15.1 Dataflow with request batching
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1 请求批处理的数据流
- en: Asynchronous programming can sound scary, and it usually comes with lots of
    terminology. But what we are doing here is just allowing functions to non-blockingly
    wait for results of computations or events.[⁴](#pgfId-1013039)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程听起来可能很可怕，并且通常伴随着大量术语。但我们在这里所做的只是允许函数非阻塞地等待计算或事件的结果。
- en: In order to do request batching, we have to decouple the request handling from
    running the model. Figure 15.1 shows the flow of the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行请求批处理，我们必须将请求处理与运行模型分离。图 15.1 显示了数据的流动。
- en: At the top of figure 15.1 are the clients, making requests. One by one, these
    go through the top half of the request processor. They cause work items to be
    enqueued with the request information. When a full batch has been queued or the
    oldest request has waited for a specified maximum time, a model runner takes a
    batch from the queue, processes it, and attaches the result to the work items.
    These are then processed one by one by the bottom half of the request processor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 15.1 的顶部是客户端，发出请求。这些一个接��个地通过请求处理器的上半部分。它们导致工作项与请求信息一起入队。当已经排队了一个完整的批次或最老的请求等待了指定的最长时间时，模型运行器会从队列中取出一批，处理它，并将结果附加到工作项上。然后这些工作项一个接一个地由请求处理器的下半部分处理。
- en: Implementation
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现
- en: We implement this by writing two functions. The model runner function starts
    at the beginning and runs forever. Whenever we need to run the model, it assembles
    a batch of inputs, runs the model in a second thread (so other things can happen),
    and returns the result.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过编写两个函数来实现这一点。模型运行函数从头开始运行并永远运行。每当需要运行模型时，它会组装一批输入，在第二个线程中运行模型（以便其他事情可以发生），然后返回结果。
- en: The request processor then decodes the request, enqueues inputs, waits for the
    processing to be completed, and returns the output with the results. In order
    to appreciate what *asynchronous* means here, think of the model runner as a wastepaper
    basket. All the figures we scribble for this chapter can be quickly disposed of
    to the right of the desk. But every once in a while--either because the basket
    is full or when it is time to clean up in the evening--we need to take all the
    collected paper out to the trash can. Similarly, we enqueue new requests, trigger
    processing if needed, and wait for the results before sending them out as the
    answer to the request. Figure 15.2 shows our two functions in the blocks we execute
    uninterrupted before handing back to the event loop.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请求处理器然后解码请求，将输入加入队列，等待处理完成，并返回带有结果的输出。为了理解这里*异步*的含义，可以将模型运行器视为废纸篓。我们为本章所涂鸦的所有图纸都可以快速地放在桌子右侧的垃圾桶里处理掉。但是偶尔——无论是因为篮子已满还是因为到了晚上清理的时候——我们需要将所有收集的纸张拿出去扔到垃圾桶里。类似地，我们将新请求加入队列，如果需要则触发处理，并在发送结果作为请求答复之前等待结果。图
    15.2 展示了我们在执行的两个函数块之前无间断执行的情况。
- en: '![](../Images/CH15_F02_Stevens2_GS.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F02_Stevens2_GS.png)'
- en: 'Figure 15.2 Our asynchronous server consists of three blocks: request processor,
    model runner, and model execution. These blocks are a bit like functions, but
    the first two will yield to the event loop in between.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 我们的异步服务器由三个模块组成：请求处理器、模型运行器和模型执行。这些模块有点像函数，但前两个在中间会让出事件循环。
- en: 'A slight complication relative to this picture is that we have two occasions
    when we need to process events: if we have accumulated a full batch, we start
    right away; and when the oldest request reaches the maximum wait time, we also
    want to run. We solve this by setting a timer for the latter.[⁵](#pgfId-1013870)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于这个图片，一个轻微的复杂性是我们有两个需要处理事件的场合：如果我们积累了一个完整的批次，我们立即开始；当最老的请求达到最大等待时间时，我们也想运行。我们通过为后者设置一个定时器来解决这个问题。[⁵](#pgfId-1013870)
- en: All our interesting code is in a `ModelRunner` class, as shown in the following
    listing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所有我们感兴趣的代码都在一个`ModelRunner`类中，如下列表所示。
- en: Listing 15.3 request_batching_server.py:32, `ModelRunner`
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.3 request_batching_server.py:32, `ModelRunner`
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The queue
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 队列
- en: ❷ This will become our lock.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这将成为我们的锁。
- en: ❸ Loads and instantiates the model. This is the (only) thing we will need to
    change for switching to the JIT. For now, we import the CycleGAN (with the slight
    modification of standardizing to 0..1 input and output) from p3ch15/cyclegan.py.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 加载并实例化模型。这是我们将需要更改以切换到JIT的（唯一）事情。目前，我们从p3ch15/cyclegan.py导入CycleGAN（稍微修改为标准化为0..1的输入和输出）。
- en: ❹ Our signal to run the model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们运行模型的信号
- en: ❺ Finally, the timer
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最后，定时器
- en: '`ModelRunner` first loads our model and takes care of some administrative things.
    In addition to the model, we also need a few other ingredients. We enter our requests
    into a `queue`. This is a just a Python list in which we add work items at the
    back and remove them in the front.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelRunner` 首先加载我们的模型并处理一些管理事务。除了模型，我们还需要一些其他要素。我们将请求输入到一个`queue`中。这只是一个Python列表，我们在后面添加工作项，然后在前面删除它们。'
- en: 'When we modify the `queue`, we want to prevent other tasks from changing the
    queue out from under us. To this effect, we introduce a `queue_lock` that will
    be an `asyncio.Lock` provided by the `asyncio` module. As all `asyncio` objects
    we use here need to know the event loop, which is only available after we initialize
    the application, we temporarily set it to `None` in the instantiation. While locking
    like this may not be strictly necessary because our methods do not hand back to
    the event loop while holding the lock, and operations on the queue are atomic
    thanks to the GIL, it does explicitly encode our underlying assumption. If we
    had multiple workers, we would need to look at locking. One caveat: Python’s async
    locks are not threadsafe. (Sigh.)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们修改`queue`时，我们希望防止其他任务在我们下面更改队列。为此，我们引入了一个`queue_lock`，它将是由`asyncio`模块提供的`asyncio.Lock`。由于我们在这里使用的所有`asyncio`对象都需要知道事件循环，而事件循环只有在我们初始化应用程序后才可用，因此我们在实例化时将其临时设置为`None`。尽管像这样锁定可能并不是绝对必要的，因为我们的方法在持有锁时不会返回事件循环，并且由于GIL的原因，对队列的操作是原子的，但它确实明确地编码了我们的基本假设。如果我们有多个工作进程，我们需要考虑加锁。一个警告：Python的异步锁不是线程安全的。（叹气。）
- en: '`ModelRunner` waits when it has nothing to do. We need to signal it from `RequestProcessor`
    that it should stop slacking off and get to work. This is done via an `asyncio.Event`
    called `needs_processing`. `ModelRunner` uses the `wait()` method to wait for
    the `needs_processing` event. The `RequestProcessor` then uses `set()` to signal,
    and `ModelRunner` wakes up and `clear()`s the event.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelRunner` 在没有任务时等待。我们需要从`RequestProcessor`向其发出信号，告诉它停止偷懒，开始工作。这通过名为`needs_processing`的`asyncio.Event`完成。`ModelRunner`使用`wait()`方法等待`needs_processing`事件。然后，`RequestProcessor`使用`set()`来发出信号，`ModelRunner`会被唤醒并清除事件。'
- en: Finally, we need a timer to guarantee a maximal wait time. This timer is created
    when we need it by using `app.loop.call_at`. It sets the `needs_processing` event;
    we just reserve a slot now. So actually, sometimes the event will be set directly
    because a batch is complete or when the timer goes off. When we process a batch
    before the timer goes off, we will clear it so we don’t do too much work.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个定时器来保证最大等待时间。当我们需要时，通过使用`app.loop.call_at`来创建此定时器。它设置`needs_processing`事件；我们现在只是保留一个插槽。因此，实际���，有时事件将直接被设置，因为一个批次已经完成，或者当定时器到期时。当我们在定时器到期之前处理一个批次时，我们将清除它，以便不做太多的工作。
- en: From request to queue
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从请求到队列
- en: Next we need to be able to enqueue requests, the core of the first part of `RequestProcessor`
    in figure 15.2 (without the decoding and reencoding). We do this in our first
    `async` method, `process_input`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要能够将请求加入队列，这是图 15.2 中`RequestProcessor`的第一部分的核心（不包括解码和重新编码）。我们在我们的第一个`async`方法`process_input`中完成这个操作。
- en: Listing 15.4 request_batching_server.py:54
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.4 request_batching_server.py:54
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Sets up the task data
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置任务数据
- en: ❷ With the lock, we add our task and ...
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用锁，我们添加我们的任务和...
- en: ❸ ... schedule processing. Processing will set needs_processing if we have a
    full batch. If we don’t and no timer is set, it will set one to when the max wait
    time is up.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ ...安排处理。处理将设置`needs_processing`，如果我们有一个完整的批次。如果我们没有，并且没有设置定时器，它将在最大等待时间到达时设置一个定时器。
- en: ❹ Waits (and hands back to the loop using await) for the processing to finish
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 等待（并使用await将控制权交还给循环）处理完成。
- en: 'We set up a little Python dictionary to hold our task’s information: the `input`
    of course, the `time` it was queued, and a `done_event` to be set when the task
    has been processed. The processing adds an `output`.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置一个小的Python字典来保存我们任务的信息：当然是`input`，任务被排队的`time`，以及在任务被处理后将被设置的`done_event`。处理会添加一个`output`。
- en: Holding the queue lock (conveniently done in an `async with` block), we add
    our task to the queue and schedule processing if needed. As a precaution, we error
    out if the queue has become too large. Then all we have to do is wait for our
    task to be processed, and return it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 持有队列锁（方便地在`async with`块中完成），我们将我们的任务添加到队列中，并在需要时安排处理。作为预防措施，如果队列变得太大，我们会报错。然后，我们只需等待我们的任务被处理，并返回它。
- en: '*Note* It is important to use the loop time (typically a monotonic clock),
    which may be different from the `time.time()`. Otherwise, we might end up with
    events scheduled for processing before they have been queued, or no processing
    at all.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 使用循环时间（通常是单调时钟）非常重要，这可能与`time.time()`不同。否则，我们可能会在排队之前为处理安排事件，或者根本不进行处理。'
- en: This is all we need for the request processing (except decoding and encoding).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们处理请求所需的一切（除了解码和编码）。
- en: Running batches from the queue
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从队列中运行批处理
- en: Next, let’s look at the `model_runner` function on the right side of figure
    15.2, which does the model invocation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下图 15.2 右侧的`model_runner`函数，它执行模型调用。
- en: Listing 15.5 request_batching_server.py:71, `.run_model`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.5 request_batching_server.py:71，`.run_model`
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Waits until there is something to do
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 等待有事情要做
- en: ❷ Cancels the timer if it is set
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果设置了定时器，则取消定时器
- en: ❸ Grabs a batch and schedules the running of the next batch, if needed
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取一个批次并安排下一个批次的运行（如果需要）
- en: ❹ Runs the model in a separate thread, moving data to the device and then handing
    over to the model. We continue processing after it is done.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在单独的线程中运行模型，将数据移动到设备，然后交给模型处理。处理完成后我们继续进行处理。
- en: ❺ Adds the results to the work-item and sets the ready event
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将结果添加到工作项中并设置准备事件
- en: As indicated in figure 15.2, `model_runner` does some setup and then infinitely
    loops (but yields to the event loop in between). It is invoked when the app is
    instantiated, so it can set up `queue_lock` and the `needs_processing` event we
    discussed earlier. Then it goes into the loop, `await`-ing the `needs_processing`
    event.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 15.2 所示，`model_runner`进行一些设置，然后无限循环（但在之间让出事件循环）。它在应用程序实例化时被调用，因此它可以设置我们之前讨论过的`queue_lock`和`needs_processing`事件。然后它进入循环，等待`needs_processing`事件。
- en: When an event comes, first we check whether a time is set and, if so, clear
    it, because we’ll be processing things now. Then `model_runner` grabs a batch
    from the queue and, if needed, schedules the processing of the next batch. It
    assembles the batch from the individual tasks and launches a new thread that evaluates
    the model using `asyncio`'s `app.loop.run_in_executor`. Finally, it adds the outputs
    to the tasks and sets `done_event`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当事件发生时，首先我们检查是否设置了时间，如果设置了，就清除它，因为我们现在要处理事情了。然后`model_runner`从队列中获取一个批次，如果需要的话，安排下一个批次的处理。它从各个任务中组装批次，并启动一个使用`asyncio`的`app.loop.run_in_executor`评估模型的新线程。最后，它将输出添加到任务中并设置`done_event`。
- en: And that’s basically it. The web framework--roughly looking like Flask with
    `async` and `await` sprinkled in--needs a little wrapper. And we need to start
    the `model_runner` function on the event loop. As mentioned earlier, locking the
    queue really is not necessary if we do not have multiple runners taking from the
    queue and potentially interrupting each other, but knowing our code will be adapted
    to other projects, we stay on the safe side of losing requests.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上就是这样。Web 框架--大致看起来像是带有`async`和`await`的 Flask--需要一个小包装器。我们需要在事件循环中启动`model_runner`函数。正如之前提到的，如果我们没有多个运行程序从队列中取出并可能相互中断，那么锁定队列就不是必要的，但是考虑到我们的代码将被适应到其他项目，我们选择保守一点，以免丢失请求。
- en: We start our server with
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下方式启动我们的服务器
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can test by uploading the image data/p1ch2/horse.jpg and saving the
    result:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过上传图像数据/p1ch2/horse.jpg 进行测试并保存结果：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that this server does get a few things right--it batches requests for the
    GPU and runs asynchronously--but we still use the Python mode, so the GIL hampers
    running our model in parallel to the request serving in the main thread. It will
    not be safe for potentially hostile environments like the internet. In particular,
    the decoding of request data seems neither optimal in speed nor completely safe.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个服务器确实做了一些正确的事情--它为 GPU 批处理请求并异步运行--但我们仍然使用 Python 模式，因此 GIL 阻碍了我们在主线程中并行运行模型以响应请求。在潜在的敌对环境（如互联网）中，这是不安全的。特别是，请求数据的解码似乎既不是速度最优也不是完全安全的。
- en: In general, it would be nicer if we could have decoding where we pass the request
    stream to a function along with a preallocated memory chunk, and the function
    decodes the image from the stream to us. But we do not know of a library that
    does things this way.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果我们可以进行解码，那将会更好，我们将请求流传递给一个函数，同时传递一个预分配的内存块，函数将从流中为我们解码图像。但我们不知道有哪个库是这样做的。
- en: 15.2 Exporting models
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 导出模型
- en: 'So far, we have used PyTorch from the Python interpreter. But this is not always
    desirable: the GIL is still potentially blocking our improved web server. Or we
    might want to run on embedded systems where Python is too expensive or unavailable.
    This is when we export our model. There are several ways in which we can play
    this. We might go away from PyTorch entirely and move to more specialized frameworks.
    Or we might stay within the PyTorch ecosystem and use the JIT, a *just in time*
    compiler for a PyTorch-centric subset of Python. Even when we then run the JITed
    model in Python, we might be after two of its advantages: sometimes the JIT enables
    nifty optimizations, or--as in the case of our web server--we just want to escape
    the GIL, which JITed models do. Finally (but we take some time to get there),
    we might run our model under `libtorch`, the C++ library PyTorch offers, or with
    the derived Torch Mobile.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从 Python 解释器中使用了 PyTorch。但这并不总是理想的：GIL 仍然可能阻塞我们改进的 Web 服务器。或者我们可能希望在
    Python 过于昂贵或不可用的嵌入式系统上运行。这就是我们导出模型的时候。我们可以以几种方式进行操作。我们可能完全放弃 PyTorch 转向更专业的框架。或者我们可能留在
    PyTorch 生态系统内部并使用 JIT，这是 PyTorch 专用 Python 子集的*即时*编译器。即使我们在 Python 中运行 JIT 模型，我们可能也追求其中的两个优势：有时
    JIT 可以实现巧妙的优化，或者--就像我们的 Web 服务器一样--我们只是想摆脱 GIL，而 JIT 模型可以做到。最后（但我们需要一些时间才能到达那里），我们可能在`libtorch`下运行我们的模型，这是
    PyTorch 提供的 C++ 库，或者使用衍生的 Torch Mobile。
- en: 15.2.1 Interoperability beyond PyTorch with ONNX
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 与 ONNX 一起实现跨 PyTorch 的互操作性
- en: Sometimes we want to leave the PyTorch ecosystem with our model in hand--for
    example, to run on embedded hardware with a specialized model deployment pipeline.
    For this purpose, Open Neural Network Exchange provides an interoperational format
    for neural networks and machine learning models ([https://onnx.ai](https://onnx.ai)).
    Once exported, the model can be executed using any ONNX-compatible runtime, such
    as ONNX Runtime,[⁶](#pgfId-1238047) provided that the operations in use in our
    model are supported by the ONNX standard and the target runtime. It is, for example,
    quite a bit faster on the Raspberry Pi than running PyTorch directly. Beyond traditional
    hardware, a lot of specialized AI accelerator hardware supports ONNX ([https://onnx.ai/supported-tools
    .html#deployModel](https://onnx.ai/supported-tools.html#deployModel)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望带着手头的模型离开PyTorch生态系统--例如，为了在具有专门模型部署流程的嵌入式硬件上运行。为此，Open Neural Network
    Exchange提供了一个用于神经网络和机器学习模型的互操作格式（[https://onnx.ai](https://onnx.ai)）。一旦导出，模型可以使用任何兼容ONNX的运行时执行，例如ONNX
    Runtime，[⁶](#pgfId-1238047)前提是我们模型中使用的操作得到ONNX标准和目标运行时的支持。例如，在树莓派上比直接运行PyTorch要快得多。除了传统硬件外，许多专门的AI加速器硬件都支持ONNX（[https://onnx.ai/supported-tools
    .html#deployModel](https://onnx.ai/supported-tools.html#deployModel)）。
- en: In a way, a deep learning model is a program with a very specific instruction
    set, made of granular operations like matrix multiplication, convolution, `relu`,
    `tanh`, and so on. As such, if we can serialize the computation, we can reexecute
    it in another runtime that understands its low-level operations. ONNX is a standardization
    of a format describing those operations and their parameters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，深度学习模型是一个具有非常特定指令集的程序，由矩阵乘法、卷积、`relu`、`tanh`等粒度操作组成。因此，如果我们可以序列化计算，我们可以在另一个理解其低级操作的运行时中重新执行它。ONNX是描述这些操作及其参数的格式的标准化。
- en: Most of the modern deep learning frameworks support serialization of their computations
    to ONNX, and some of them can load an ONNX file and execute it (although this
    is not the case for PyTorch). Some low-footprint (“edge”) devices accept an ONNX
    files as input and generate low-level instructions for the specific device. And
    some cloud computing providers now make it possible to upload an ONNX file and
    see it exposed through a REST endpoint.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代深度学习框架支持将它们的计算序列化为ONNX，其中一些可以加载ONNX文件并执行它（尽管PyTorch不支持）。一些低占用量（“边缘”）设备接受ONNX文件作为输入，并为特定设备生成低级指令。一些云计算提供商现在可以上传ONNX文件并通过REST端点查看其暴露。
- en: 'In order to export a model to ONNX, we need to run a model with a dummy input:
    the values of the input tensors don’t really matter; what matters is that they
    are the correct shape and type. By invoking the `torch.onnx.export` function,
    PyTorch will *trace* the computations performed by the model and serialize them
    into an ONNX file with the provided name:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型导出到ONNX，我们需要使用虚拟输入运行模型：输入张量的值并不重要；重要的是它们具有正确的形状和类型。通过调用`torch.onnx.export`函数，PyTorch将*跟踪*模型执行的计算，并将其序列化为一个带有提供的名称的ONNX文件：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The resulting ONNX file can now be run in a runtime, compiled to an edge device,
    or uploaded to a cloud service. It can be used from Python after installing `onnxruntime`
    or `onnxruntime-gpu` and getting a `batch` as a NumPy array.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的ONNX文件现在可以在运行时运行，编译到边缘设备，或上传到云服务。在安装`onnxruntime`或`onnxruntime-gpu`并将`batch`作为NumPy数组获取后，可以从Python中使用它。
- en: Listing 15.6 onnx_example.py
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单15.6 onnx_example.py
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The ONNX runtime API uses sessions to define models and then calls the run
    method with a set of named inputs. This is a somewhat typical setup when dealing
    with computations defined in static graphs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ ONNX运行时API使用会话来定义模型，然后使用一组命名输入调用运行方法。这在处理静��图中定义的计算时是一种典型的设置。
- en: Not all TorchScript operators can be represented as standardized ONNX operators.
    If we export operations foreign to ONNX, we will get errors about unknown `aten`
    operators when we try to use the runtime.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有TorchScript运算符都可以表示为标准化的ONNX运算符。如果导出与ONNX不兼容的操作，当我们尝试使用运行时时，将会出现有关未知`aten`运算符的错误。
- en: '15.2.2 PyTorch’s own export: Tracing'
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 PyTorch自己的导出：跟踪
- en: When interoperability is not the key, but we need to escape the Python GIL or
    otherwise export our network, we can use PyTorch’s own representation, called
    the *TorchScript graph*. We will see what that is and how the JIT that generates
    it works in the next section. But let’s give it a spin right here and now.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当互操作性不是关键，但我们需要摆脱Python GIL或以其他方式导出我们的网络时，我们可以使用PyTorch自己的表示，称为*TorchScript图*。我们将在下一节中看到这是什么，以及生成它的JIT如何工作。但现在就让我们试一试。
- en: The simplest way to make a TorchScript model is to trace it. This looks exactly
    like ONNX exporting. This isn’t surprising, because that is what the ONNX model
    uses under the hood, too. Here we just feed dummy inputs into the model using
    the `torch.jit.trace` function. We import `UNetWrapper` from chapter 13, load
    the trained parameters, and put the model into evaluation mode.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 制作TorchScript模型的最简单方法是对其进行跟踪。这看起来与ONNX导出完全相同。这并不奇怪，因为在幕后ONNX模型也使用了这种方法。在这里，我们只需使用`torch.jit.trace`函数将虚拟输入馈送到模型中。我们从第13章导入`UNetWrapper`，加载训练参数，并将模型置于评估模式。
- en: 'Before we trace the model, there is one additional caveat: none of the parameters
    should require gradients, because using the `torch.no_grad()` context manager
    is strictly a runtime switch. Even if we trace the model within `no_grad` but
    then run it outside, PyTorch will record gradients. If we take a peek ahead at
    figure 15.4, we see why: after the model has been traced, we ask PyTorch to execute
    it. But the traced model will have parameters requiring gradients when executing
    the recorded operations, and they will make everything require gradients. To escape
    that, we would have to run the traced model in a `torch.no_grad` context. To spare
    us this--from experience, it is easy to forget and then be surprised by the lack
    of performance--we loop through the model parameters and set all of them to not
    require gradients.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们追踪模型之前，有一个额外的注意事项：任何参数都不应该需要梯度，因为使用`torch.no_grad()`上下文管理器严格来说是一个运行时开关。即使我们在`no_grad`内部追踪模型，然后在外部运行，PyTorch
    仍会记录梯度。如果我们提前看一眼图 15.4，我们就会明白为什么：在模型被追踪之后，我们要求 PyTorch 执行它。但是在执行记录的操作时，追踪的模型将需要梯度的参数，并且会使所有内容都需要梯度。为了避免这种情况，我们必须在`torch.no_grad`上下文中运行追踪的模型。为了避免这种情况--根据经验，很容易忘记然后对性能的缺乏感到惊讶--我们循环遍历模型参数并将它们全部设置为不需要梯度。
- en: But then all we need to do is call `torch.jit.trace`. [⁷](#pgfId-1019686)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们只需要调用`torch.jit.trace`。
- en: Listing 15.7 trace_example.py
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列出 15.7 trace_example.py
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Sets the parameters to not require gradients
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将参数设置为不需要梯度
- en: ❷ The tracing
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 追踪
- en: 'The tracing gives us a warning:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪给我们一个警告：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This stems from the cropping we do in U-Net, but as long as we only ever plan
    to feed images of size 512 × 512 into the model, we will be OK. In the next section,
    we’ll take a closer look at what causes the warning and how to get around the
    limitation it highlights if we need to. It will also be important when we want
    to convert models that are more complex than convolutional networks and U-Nets
    to TorchScript.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这源自我们在 U-Net 中进行的裁剪，但只要我们计划将大小为 512 × 512 的图像馈送到模型中，我们就没问题。在下一节中，我们将更仔细地看看是什么导致了警告，以及如何避开它突出的限制（如果需要的话）。当我们想要将比卷积网络和
    U-Net 更复杂的模型转换为 TorchScript 时，这也将很重要。
- en: We can save the traced model
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以保存追踪的模型
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'and load it back without needed anything but the saved file, and then we can
    call it:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载回来而不需要任何东西，然后我们可以调用它：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The PyTorch JIT will keep the model’s state from when we saved it: that we
    had put it into evaluation mode and that our parameters do not require gradients.
    If we had not taken care of it beforehand, we would need to use `with torch.no_grad():`
    in the execution.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch JIT 将保留我们保存模型时的状态：我们已经将其置于评估模式，并且我们的参数不需要梯度。如果我们之前没有注意到这一点，我们将需要在执行中使用`with
    torch.no_grad():`。
- en: '*tip* You can run the JITed and exported PyTorch model without keeping the
    source. However, we always want to establish a workflow where we automatically
    go from source model to installed JITed model for deployment. If we do not, we
    will find ourselves in a situation where we would like to tweak something with
    the model but have lost the ability to modify and regenerate. Always keep the
    source, Luke!'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示* 您可以运行 JIT 编译并导出的 PyTorch 模型而不保留源代码。但是，我们总是希望建立一个工作流程，自动从源模型转换为已安装的 JIT
    模型以进行部署。如果不这样做，我们将发现自己处于这样一种情况：我们想要调整模型的某些内容，但已经失去了修改和重新生成的能力。永远保留源代码，卢克！'
- en: 15.2.3 Our server with a traced model
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 带有追踪模型的服务器
- en: 'Now is a good time to iterate our web server to what is, in this case, our
    final version. We can export the traced CycleGAN model as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将我们的网络服务器迭代到这种情况下的最终版本了。我们可以将追踪的 CycleGAN 模型导出如下：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we just need to replace the call to `get_pretrained_model` with `torch.jit.load`
    in our server (and drop the now-unnecessary `import` of `get_pretrained_model`).
    This also means our model runs independent of the GIL--and this is what we wanted
    our server to achieve here. For your convenience, we have put the small modifications
    in request_batching_jit_server.py. We can run it with the traced model file path
    as a command-line argument.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要在服务器中用`torch.jit.load`替换对`get_pretrained_model`的调用（并删除现在不再需要的`import
    get_pretrained_model`）。这也意味着我们的模型独立于 GIL 运行--这正是我们希望我们的服务器在这里实现的。为了您的方便，我们已经将小的修改放在
    request_batching_jit_server.py 中。我们可以用追踪的模型文件路径作为命令行参数来运行它。
- en: Now that we have had a taste of what the JIT can do for us, let’s dive into
    the details!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经尝试了 JIT 对我们有什么帮助，让我们深入了解细节吧！
- en: 15.3 Interacting with the PyTorch JIT
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 与 PyTorch JIT 交互
- en: Debuting in PyTorch 1.0, the PyTorch JIT is at the center of quite a few recent
    innovations around PyTorch, not least of which is providing a rich set of deployment
    options.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 1.0 中首次亮相，PyTorch JIT 处于围绕 PyTorch 的许多最新创新的中心，其中之一是提供丰富的部署选项。
- en: 15.3.1 What to expect from moving beyond classic Python/PyTorch
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 超越经典 Python/PyTorch 时可以期待什么
- en: Quite often, Python is said to lack speed. While there is some truth to this,
    the tensor operations we use in PyTorch usually are in themselves large enough
    that the Python slowness between them is not a large issue. For small devices
    like smartphones, the memory overhead that Python brings might be more important.
    So keep in mind that frequently, the speedup gained by taking Python out of the
    computation is 10% or less.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 经常有人说 Python 缺乏速度。虽然这有一定道理，但我们在 PyTorch 中使用的张量操作通常本身足够大，以至于它们之间的 Python 速度慢并不是一个大问题。对于像智能手机这样的小设备，Python
    带来的内存开销可能更重要。因此，请记住，通常通过将 Python 排除在计算之外来加快速度的提��是 10% 或更少。
- en: 'Another immediate speedup from not running the model in Python only appears
    in multithreaded environments, but then it can be significant: because the intermediates
    are not Python objects, the computation is not affected by the menace of all Python
    parallelization, the GIL. This is what we had in mind earlier and realized when
    we used a traced model in our server.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不在 Python 中运行模型的即时加速仅在多线程环境中出现，但这时它可能是显著的：因为中间结果不是 Python 对象，计算不受所有 Python
    并行化的威胁，即 GIL。这是我们之前考虑到的，并且当我们在服务器上使用跟踪模型时实现了这一点。
- en: 'Moving from the classic PyTorch way of executing one operation before looking
    at the next does give PyTorch a holistic view of the calculation: that is, it
    can consider the calculation in its entirety. This opens the door to crucial optimizations
    and higher-level transformations. Some of those apply mostly to inference, while
    others can also provide a significant speedup in training.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从经典的 PyTorch 执行一项操作后再查看下一项的方式转变过来，确实让 PyTorch 能够全面考虑计算：也就是说，它可以将计算作为一个整体来考虑。这为关键的优化和更高级别的转换打开了大门。其中一些主要适用于推断，而其他一些也可以在训练中提供显著的加速。
- en: Let’s use a quick example to give you a taste of why looking at several operations
    at once can be beneficial. When PyTorch runs a sequence of operations on the GPU,
    it calls a subprogram (*kernel*, in CUDA parlance) for each of them. Every kernel
    reads the input from GPU memory, computes the result, and then stores the result.
    Thus most of the time is typically spent not computing things, but reading from
    and writing to memory. This can be improved on by reading only once, computing
    several operations, and then writing at the very end. This is precisely what the
    PyTorch JIT fuser does. To give you an idea of how this works, figure 15.3 shows
    the pointwise computation taking place in long short-term memory (LSTM; [https://en.wikipedia.org/wiki/
    Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory))
    cell, a popular building block for recurrent networks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个快速示例来让你体会一下为什么一次查看多个操作会有益。当 PyTorch 在 GPU 上运行一系列操作时，它为每个操作调用一个子程序（在 CUDA
    术语中称为*内核*）。每个内核从 GPU 内存中读取输入，计算结果，然后存储结果。因此，大部分时间通常不是用于计算，而是用于读取和写入内存。这可以通过仅读取一次，计算多个操作，然后在最后写入来改进。这正是
    PyTorch JIT 融合器所做的。为了让你了解这是如何工作的，图 15.3 展示了长短期记忆（LSTM；[https://en.wikipedia.org/wiki/
    Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory)）单元中进行的逐点计算，这是递归网络的流行构建块。
- en: The details of figure 15.3 are not important to us here, but there are 5 inputs
    at the top, 2 outputs at the bottom, and 7 intermediate results represented as
    rounded indices. By computing all of this in one go in a single CUDA function
    and keeping the intermediates in registers, the JIT reduces the number of memory
    reads from 12 to 5 and the number of writes from 9 to 2\. These are the large
    gains the JIT gets us; it can reduce the time to train an LSTM network by a factor
    of four. This seemingly simple trick allows PyTorch to significantly narrow the
    gap between the speed of LSTM and generalized LSTM cells flexibly defined in PyTorch
    and the rigid but highly optimized LSTM implementation provided by libraries like
    cuDNN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 的细节对我们来说并不重要，但顶部有 5 个输入，底部有 2 个输出，中间有 7 个圆角指数表示的中间结果。通过在一个单独的 CUDA 函数中一次性计算所有这些，并将中间结果保留在寄存器中，JIT
    将内存读取次数从 12 降低到 5，写入次数从 9 降低到 2。这就是 JIT 带来的巨大收益；它可以将训练 LSTM 网络的时间缩短四倍。这看似简单的技巧使得
    PyTorch 能够显著缩小 LSTM 和在 PyTorch 中灵活定义的通用 LSTM 单元与像 cuDNN 这样提供的高度优化 LSTM 实现之间速度差距。
- en: In summary, the speedup from using the JIT to escape Python is more modest than
    we might naively expect when we have been told that Python is awfully slow, but
    avoiding the GIL is a significant win for multithreaded applications. The large
    speedups in JITed models come from special optimizations that the JIT enables
    but that are more elaborate than just avoiding Python overhead.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，使用 JIT 来避免 Python 的加速并不像我们可能天真地期望的那样大，因为我们被告知 Python 非常慢，但避免 GIL 对于多线程应用程序来说是一个重大胜利。JIT
    模型的大幅加速来自 JIT 可以实现的特殊优化，但这些优化比仅仅避免 Python 开销更为复杂。
- en: '![](../Images/CH15_F03_Stevens2_GS.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F03_Stevens2_GS.png)'
- en: Figure 15.3 LSTM cell pointwise operations. From five inputs at the top, this
    block computes two outputs at the bottom. The boxes in between are intermediate
    results that vanilla PyTorch will store in memory but the JIT fuser will just
    keep in registers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 LSTM 单元逐点操作。从顶部的五个输入，该块计算出底部的两个输出。中间的方框是中间结果，普通的 PyTorch 会将其存储在内存中，但
    JIT 融合器只会保留在寄存器中。
- en: 15.3.2 The dual nature of PyTorch as interface and backend
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 PyTorch 作为接口和后端的双重性质
- en: 'To understand how moving beyond Python works, it is beneficial to mentally
    separate PyTorch into several parts. We saw a first glimpse of this in section
    1.4\. Our PyTorch `torch.nn` modules--which we first saw in chapter 6 and which
    have been our main tool for modeling ever since--hold the parameters of our network
    and are implemented using the functional interface: functions taking and returning
    tensors. These are implemented as a C++ extension, handed over to the C++-level
    autograd-enabled layer. (This then hands the actual computation to an internal
    library called ATen, performing the computation or relying on backends to do so,
    but this is not important.)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何摆脱 Python 的工作原理，有益的是在头脑中将 PyTorch 分为几个部分。我们在第 1.4 节中初步看到了这一点。我们的 PyTorch
    `torch.nn` 模块--我们在第 6 章首次看到它们，自那以后一直是我们建模的主要工具--保存网络的参数，并使用功能接口实现：接受和返回张量的函数。这些被实现为
    C++ 扩展，交给了 C++ 级别的自动求导启用层。 （然后将实际计算交给一个名为 ATen 的内部库，执行计算或依赖后端来执行，但这不重要。）
- en: Given that the C++ functions are already there, the PyTorch developers made
    them into an official API. This is the nucleus of LibTorch, which allows us to
    write C++ tensor operations that look almost like their Python counterparts. As
    the `torch.nn` modules are Python-only by nature, the C++ API mirrors them in
    a namespace `torch::nn` that is designed to look a lot like the Python part but
    is independent.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 C++ 函数已经存在，PyTorch 开发人员将它们制作成了官方 API。这就是 LibTorch 的核心，它允许我们编写几乎与其 Python
    对应物相似的 C++ 张量操作。由于`torch.nn`模块本质上只能在 Python 中使用，C++ API 在一个名为`torch::nn`的命名空间中镜像它们，设计上看起来很像
    Python 部分，但是独立的。
- en: 'This would allow us to redo in C++ what we did in Python. But that is not what
    we want: we want to *export* the model. Happily, there is another interface to
    the same functions provided by PyTorch: the PyTorch JIT. The PyTorch JIT provides
    a “symbolic” representation of the computation. This representation is the *TorchScript
    intermediate representation* (TorchScript IR, or sometimes just TorchScript).
    We mentioned TorchScript in section 15.2.2 when discussing delayed computation.
    In the following sections, we will see how to get this representation of our Python
    models and how they can be saved, loaded, and executed. Similar to what we discussed
    for the regular PyTorch API, the PyTorch JIT functions to load, inspect, and execute
    TorchScript modules can also be accessed both from Python and from C++.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够在 C++ 中重新做我们在 Python 中做的事情。但这不是我们想要的：我们想要*导出*模型。幸运的是，PyTorch 还提供了另一个接口来访问相同的函数：PyTorch
    JIT。PyTorch JIT 提供了计算的“符号”表示。这个表示是*TorchScript 中间表示*（TorchScript IR，有时只是 TorchScript）。我们在第
    15.2.2 节讨论延迟计算时提到了 TorchScript。在接下来的章节中，我们将看到如何获取我们 Python 模型的这种表示以及如何保存、加载和执行它们。与我们讨论常规
    PyTorch API 时所述类似，PyTorch JIT 函数用于加载、检查和执行 TorchScript 模块也可以从 Python 和 C++ 中访问。
- en: 'In summary, we have four ways of calling PyTorch functions, illustrated in
    figure 15.4: from both C++ and Python, we can either call functions directly or
    have the JIT as an intermediary. All of these eventually call the C++ LibTorch
    functions and from there ATen and the computational backend.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们有四种调用 PyTorch 函数的方式，如图 15.4 所示：从 C++ 和 Python 中，我们可以直接调用函数，也可以让 JIT 充当中介。所有这些最终都会调用
    C++ 的 LibTorch 函数，从那里进入 ATen 和计算后端。
- en: '![](../Images/CH15_F04_Stevens2_GS.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F04_Stevens2_GS.png)'
- en: Figure 15.4 Many ways of calling into PyTorch
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4 调用 PyTorch 的多种方式
- en: 15.3.3 TorchScript
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.3 TorchScript
- en: TorchScript is at the center of the deployment options envisioned by PyTorch.
    As such, it is worth taking a close look at how it works.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript 是 PyTorch 设想的部署选项的核心。因此，值得仔细研究它的工作原理。
- en: 'There are two straightforward ways to create a TorchScript model: tracing and
    scripting. We will look at each of them in the following sections. At a very high
    level, the two work as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 TorchScript 模型有两种简单直接的方式：追踪和脚本化。我们将在接下来的章节中分别介绍它们。在非常高的层面上，这两种方式的工作原理如下：
- en: In *tracing*, which we used in in section 15.2.2, we execute our usual PyTorch
    model using sample (random) inputs. The PyTorch JIT has hooks (in the C++ autograd
    interface) for every function that allows it to record the computation. In a way,
    it is like saying “Watch how I compute the outputs--now you can do the same.”
    Given that the JIT only comes into play when PyTorch functions (and also `nn.Module`s)
    are called, you can run any Python code while tracing, but the JIT will only notice
    those bits (and notably be ignorant of control flow). When we use tensor shapes--usually
    a tuple of integers--the JIT tries to follow what’s going on but may have to give
    up. This is what gave us the warning when tracing the U-Net.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在*追踪*中，我们在第 15.2.2 节中使用过，使用样本（随机）输入执行我们通常的 PyTorch 模型。PyTorch JIT 对每个函数都有钩子（在
    C++ autograd 接口中），允许它记录计算过程。在某种程度上，这就像在说“看我如何计算输出--现在你也可以这样做。”鉴于 JIT 仅在调用 PyTorch
    函数（以及`nn.Module`）时才起作用，你可以在追踪时运行任何 Python 代码，但 JIT 只会注意到那些部分（尤其是对控制流一无所知）。当我们使用张量形状--通常是整数元组--时，JIT
    会尝试跟踪发生的情况，但可能不得不放弃。这就是在追踪 U-Net 时给我们警告的原因。
- en: In *scripting*, the PyTorch JIT looks at the actual Python code of our computation
    and compiles it into the TorchScript IR. This means that, while we can be sure
    that every aspect of our program is captured by the JIT, we are restricted to
    those parts understood by the compiler. This is like saying “I am telling you
    how to do it--now you do the same.” Sounds like programming, really.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在*脚本化*中，PyTorch JIT 查看我们计算的实际 Python 代码，并将其编译成 TorchScript IR。这意味着，虽然我们可以确保
    JIT 捕获了程序的每个方面，但我们受限于编译器理解的部分。这就像在说“我告诉你如何做--现在你也这样做。”听起来真的像编程。
- en: 'We are not here for theory, so let’s try tracing and scripting with a very
    simple function that adds inefficiently over the first dimension:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是来讨论理论的，所以让我们尝试使用一个非常简单的函数进行追踪和脚本化，该函数在第一维上进行低效的加法：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can trace it:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以追踪它：
- en: '[PRE18]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Indexing in the first line of our function
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在我们函数的第一行中进行索引
- en: ❷ Our loop--but completely unrolled and fixed to 1...4 regardless of the size
    of x
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的循环--但完全展开并固定为 1...4，不管 x 的大小如何
- en: ❸ Scary, but so true!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 令人害怕，但却如此真实！
- en: We see the big warning--and indeed, the code has fixed indexing and additions
    for five rows, and it would not deal as intended with four or six rows.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一个重要的警告--实际上，这段代码已经为五行修复了索引和添加，但对于四行或六行的情况并不会按预期处理。
- en: 'This is where scripting helps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是脚本化的用处所在：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ PyTorch constructs the range length from the tensor size.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PyTorch 从张量大小构建范围长度。
- en: ❷ Our for loop--even if we have to take the funny-looking next line to get our
    index i
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的 for 循环--即使我们必须采取看起来有点奇怪的下一行来获取我们的索引 i
- en: ❸ Our loop body, which is just a tad more verbose
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们的循环体，稍微冗长一点
- en: 'We can also print the scripted graph, which is closer to the internal representation
    of TorchScript:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印脚本化的图，这更接近 TorchScript 的内部表示：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Seems a lot more verbose than we need
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 看起来比我们需要的要冗长得多
- en: ❷ The first assignment of y
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y 的第一个赋值
- en: ❸ Constructing the range is recognizable after we see the code.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在看到代码后，我们可以识别出构建范围的方法。
- en: ❹ Our for loop returns the value (y) it calculates.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们的for循环返回它计算的值（y）。
- en: '❺ Body of the for loop: selects a slice, and adds to y'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ for循环的主体：选择一个切片，并将其添加到y中
- en: 'In practice, you would most often use `torch.jit.script` in the form of a decorator:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您最常使用`torch.jit.script`作为装饰器的形式：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You could also do this with a custom `trace` decorator taking care of the inputs,
    but this has not caught on.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用自定义的`trace`装饰器来处理输入，但这并没有流行起来。
- en: 'Although TorchScript (the language) looks like a subset of Python, there are
    fundamental differences. If we look very closely, we see that PyTorch has added
    type specifications to the code. This hints at an important difference: TorchScript
    is statically typed--every value (variable) in the program has one and only one
    type. Also, the types are limited to those for which the TorchScript IR has a
    representation. Within the program, the JIT will usually infer the type automatically,
    but we need to annotate any non-tensor arguments of scripted functions with their
    types. This is in stark contrast to Python, where we can assign anything to any
    variable.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TorchScript（语言）看起来像Python的一个子集，但存在根本性差异。如果我们仔细观察，我们会发现PyTorch已经向代码添加了类型规范。这暗示了一个重要的区别：TorchScript是静态类型的--程序中的每个值（变量）都有且只有一个类型。此外，这些类型限于TorchScript
    IR具有表示的类型。在程序内部，JIT通常会自动推断类型，但我们需要用它们的类型注释脚本化函数的任何非张量参数。这与Python形成鲜明对比，Python中我们可以将任何内容分配给任何变量。
- en: So far, we’ve traced functions to get scripted functions. But we graduated from
    just using functions in chapter 5 to using modules a long time ago. Sure enough,
    we can also trace or script models. These will then behave roughly like the modules
    we know and love. For both tracing and scripting, we pass an instance of `Module`
    to `torch.jit.trace` (with sample inputs) or `torch.jit.script` (without sample
    inputs), respectively. This will give us the `forward` method we are used to.
    If we want to expose other methods (this only works in `scripting`) to be called
    from the outside, we decorate them with `@torch.jit.export` in the class definition.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经追踪函数以获取脚本化函数。但是我们很久以前就从仅在第5章中使用函数转向使用模块了。当然，我们也可以追踪或脚本化模型。然后，这些模型将大致表现得像我们熟悉和喜爱的模块。对于追踪和脚本化，我们分别将`Module`的实例传递给`torch.jit.trace`（带有示例输入）或`torch.jit.script`（不带示例输入）。这将给我们带来我们习惯的`forward`方法。如果我们想要暴露其他方法（这仅适用于`脚本化`）以便从外部调用，我们在类定义中用`@torch.jit.export`装饰它们。
- en: When we said that the JITed modules work like they did in Python, this includes
    the fact that we can use them for training, too. On the flip side, this means
    we need to set them up for inference (for example, using the `torch.no_grad()`
    context) just like our traditional models, to make them do the right thing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说JIT模块的工作方式与Python中的工作方式相同时，这包括我们也可以用它们进行训练。另一方面，这意味着我们需要为推断设置它们（例如，使用`torch.no_grad()`上下文），就像我们传统的模型一样，以使它们做正确的事情。
- en: With algorithmically relatively simple models--like the CycleGAN, classification
    models and U-Net-based segmentation--we can just trace the model as we did earlier.
    For more complex models, a nifty property is that we can use scripted or traced
    functions from other scripted or traced code, and that we can use scripted or
    traced submodules when constructing and tracing or scripting a module. We can
    also trace functions by calling `nn.Models`, but then we need to set all parameters
    to not require gradients, as the parameters will be constants for the traced model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算法相对简单的模型--如CycleGAN、分类模型和基于U-Net的分割--我们可以像之前一样追踪模型。对于更复杂的模型，一个巧妙的特性是我们可以在构建和追踪或脚本化模块时使用来自其他脚本化或追踪代码的脚本化或追踪函数，并且我们可以在调用`nn.Models`时追踪函数，但是我们需要将所有参数设置为不需要梯度，因为这些参数将成为追踪模型的常数。
- en: As we have seen tracing already, let’s look at a practical example of scripting
    in more detail.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经看到了追踪，让我们更详细地看一个脚本化的实际示例。
- en: 15.3.4 Scripting the gaps of traceability
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.4 脚本化追踪的间隙
- en: In more complex models, such as those from the Fast R-CNN family for detection
    or recurrent networks used in natural language processing, the bits with control
    flow like `for` loops need to be scripted. Similarly, if we needed the flexibility,
    we would find the code bit the tracer warned about.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复��的模型中，例如用于检测的Fast R-CNN系列或用于自然语言处理的循环网络，像`for`循环这样的控制流位需要进行脚本化。同样，如果我们需要灵活性，我们会找到追踪器警告的代码片段。
- en: Listing 15.8 From utils/unet.py
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 15.8 来自utils/unet.py
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The tracer warns here.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 追踪器在这里发出警告。
- en: What happens is that the JIT magically replaces the shape tuple `up.shape` with
    a 1D integer tensor with the same information. Now the slicing `[2:]` and the
    calculation of `diff_x` and `diff_y` are all traceable tensor operations. However,
    that does not save us, because the slicing then wants Python `int`s; and there,
    the reach of the JIT ends, giving us the warning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的情况是，JIT神奇地用包含相同信息的1D整数张量替换了形状元组`up.shape`。现在切片`[2:]`和计算`diff_x`和`diff_y`都是可追踪的张量操作。然而，这并不能拯救我们，因为切片然后需要Python
    `int`；在那里，JIT的作用范围结束，给我们警告。
- en: 'But we can solve this issue in a straightforward way: we script `center_crop`.
    We slightly change the cut between caller and callee by passing `up` to the scripted
    `center _crop` and extracting the sizes there. Other than that, all we need is
    to add the `@torch.jit.script` decorator. The result is the following code, which
    makes the U-Net model traceable without warnings.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们可以通过一种简单直接的方式解决这个问题：我们对`center_crop`进行脚本化。我们通过将`up`传递给脚本化的`center_crop`并在那里提取大小来略微更改调用者和被调用者之间的切割。除此之外，我们所需的只是添加`@torch.jit.script`装饰器。结果是以下代码，使U-Net模型可以无警告地进行追踪。
- en: Listing 15.9 Rewritten excerpt from utils/unet.py
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 15.9 从utils/unet.py重写的节选
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Changes the signature, taking target instead of target_size
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 更改签名，接受目标而不是目标大小
- en: ❷ Gets the sizes within the scripted part
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在脚本化部分内获取大小
- en: ❸ The indexing uses the size values we got.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 索引使用我们得到的大小值。
- en: ❹ We adapt our call to pass up rather than the size.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们调整我们的调用以传递上而不是大小。
- en: Another option we could choose--but that we will not use here--would be to move
    unscriptable things into custom operators implemented in C++. The TorchVision
    library does that for some specialty operations in Mask R-CNN models.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择的另一个选项--但我们这里不会使用--是将不可脚本化的内容移入在 C++ 中实现的自定义运算符中。TorchVision 库为 Mask R-CNN
    模型中的一些特殊操作执行此操作。
- en: '15.4 LibTorch: PyTorch in C++'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 LibTorch：在 C++ 中使用 PyTorch
- en: We have seen various way to export our models, but so far, we have used Python.
    We’ll now look at how we can forgo Python and work with C++ directly.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了各种导出模型的方式，但到目前为止，我们使用了 Python。现在我们将看看如何放弃 Python 直接使用 C++。
- en: Let’s go back to the horse-to-zebra CycleGAN example. We will now take the JITed
    model from section 15.2.3 and run it from a C++ program.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到从马到斑马的 CycleGAN 示例。我们现在将从第 15.2.3 节中获取 JITed 模型，并在 C++ 程序中运行它。
- en: 15.4.1 Running JITed models from C++
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 从 C++ 运行 JITed 模型
- en: The hardest part about deploying PyTorch vision models in C++ is choosing an
    image library to choose the data.[⁸](#pgfId-1024565) Here, we go with the very
    lightweight library CImg ([http://cimg.eu](http://cimg.eu)). If you are very familiar
    with OpenCV, you can adapt the code to use that instead; we just felt that CImg
    is easiest for our exposition.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C++ 中部署 PyTorch 视觉模型最困难的部分是选择一个图像库来选择数据。[⁸](#pgfId-1024565) 在这里，我们选择了非常轻量级的库
    CImg ([http://cimg.eu](http://cimg.eu))。如果你非常熟悉 OpenCV，你可以调整代码以使用它；我们只是觉得 CImg
    对我们的阐述最容易。
- en: Running a JITed model is very simple. We’ll first show the image handling; it
    is not really what we are after, so we will do this very quickly.[⁹](#pgfId-1024620)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 JITed 模型非常简单。我们首先展示图像处理；这并不是我们真正想要的，所以我们会很快地完成这部分。[⁹](#pgfId-1024620)
- en: Listing 15.10 cyclegan_jit.cpp
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 15.10 cyclegan_jit.cpp
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Includes the PyTorch script header and CImg with native JPEG support
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包括 PyTorch 脚本头文件和具有本地 JPEG 支持的 CImg
- en: ❷ Loads and decodes the image into a float array
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将图像加载并解码为浮点数组
- en: ❸ Resizes to a smaller size
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调整为较小的尺寸
- en: ❹ The method data_ptr<float>() gives us a pointer to the tensor storage. With
    it and the shape information, we can construct the output image.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 方法 data_ptr<float>() 给我们一个指向张量存储的指针。有了它和形状信息，我们可以构建输出图像。
- en: ❺ Saves the image
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 保存图像
- en: For the PyTorch side, we include a C++ header torch/script.h. Then we need to
    set up and include the `CImg` library. In the `main` function, we load an image
    from a file given on the command line and resize it (in CImg). So we now have
    a 227 × 227 image in the `CImg<float>` variable `image`. At the end of the program,
    we’ll create an `out_img` of the same type from our `(1, 3, 277, 277)`-shaped
    tensor and save it.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PyTorch 部分，我们包含了一个 C++ 头文件 `torch/script.h`。然后我们需要设置并包含 `CImg` 库。在 `main`
    函数中，我们从命令行中加载一个文件中的图像并调整大小（在 CImg 中）。所以现在我们有一个 `CImg<float>` 变量 `image` 中的 227
    × 227 图像。在程序的末尾，我们将从我们的形状为 `(1, 3, 277, 277)` 的张量创建一个相同类型的 `out_img` 并保存它。
- en: Don’t worry about these bits. They are not the PyTorch C++ we want to learn,
    so we can just take them as is.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心这些细节。它们不是我们想要学习的 PyTorch C++，所以我们可以直接接受它们。
- en: The actual computation is straightforward, too. We need to make an input tensor
    from the image, load our model, and run the input tensor through it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的计算也很简单。我们需要从图像创建一个输入张量，加载我们的模型，并将输入张量通过它运行。
- en: Listing 15.11 cyclegan_jit.cpp
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 15.11 cyclegan_jit.cpp
- en: '[PRE25]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Puts the image data into a tensor
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像数据放入张量中
- en: ❷ Reshapes and rescales to move from CImg conventions to PyTorch’s
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新调整和重新缩放以从 CImg 约定转换为 PyTorch 的
- en: ❸ Loads the JITed model or function from a file
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从文件加载 JITed 模型或函数
- en: ❹ Packs the input into a (one-element) vector of IValues
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将输入打包成一个（单元素）IValues 向量
- en: ❺ Calls the module and extracts the result tensor. For efficiency, the ownership
    is moved, so if we held on to the IValue, it would be empty afterward.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 调用模块并提取结果张量。为了效率，所有权被移动，所以如果我们保留了 IValue，之后它将为空。
- en: ❻ Makes sure our result is contiguous
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 确保我们的结果是连续的
- en: 'Recall from chapter 3 that PyTorch keeps the values of a tensor in a large
    chunk of memory in a particular order. So does CImg, and we can get a pointer
    to this memory chunk (as a `float` array) using `image.data()` and the number
    of elements using `image.size()`. With these two, we can create a somewhat smarter
    reference: a `torch::ArrayRef` (which is just shorthand for pointer plus size;
    PyTorch uses those at the C++ level for data but also for returning sizes without
    copying). Then we can just parse that into the `torch::tensor` constructor, just
    as we would with a list.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 3 章中回想起，PyTorch 将张量的值保存在特定顺序的大块内存中。CImg 也是如此，我们可以使用 `image.data()` 获取指向此内存块的指针（作为
    `float` 数组），并使用 `image.size()` 获取元素的数量。有了这两个，我们可以创建一个稍微更智能的引用：一个 `torch::ArrayRef`（这只是指针加大小的简写；PyTorch
    在 C++ 级别用于数据但也用于返回大小而不复制）。然后我们可以将其解析到 `torch::tensor` 构造函数中，就像我们对列表做的那样。
- en: '*tip* Sometimes you might want to use the similar-working `torch::from_blob`
    instead of `torch::tensor`. The difference is that `tensor` will copy the data.
    If you do not want copying, you can use `from_blob`, but then you need to take
    care that the underpinning memory is available during the lifetime of the tensor.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示* 有时候你可能想要使用类似工作的 `torch::from_blob` 而不是 `torch::tensor`。区别在于 `tensor` 会复制数据。如果你不想复制，可以使用
    `from_blob`，但是你需要确保在张量的生命周期内底层内存是可用的。'
- en: Our tensor is only 1D, so we need to reshape it. Conveniently, CImg uses the
    same ordering as PyTorch (channel, rows, columns). If not, we would need to adapt
    the reshaping and permute the axes as we did in chapter 4\. As CImg uses a range
    of 0...255 and we made our model to use 0...1, we divide here and multiply later.
    This could, of course, be absorbed into the model, but we wanted to reuse our
    traced model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的张量只有 1D，所以我们需要重新调整它。方便的是，CImg 使用与 PyTorch 相同的顺序（通道、行、列）。如果不是这样，我们需要调整重新调整并排列轴，就像我们在第
    4 章中所做的那样。由于 CImg 使用 0...255 的范围，而我们使我们的模型使用 0...1，所以我们在这里除以后面再乘以。当然，这可以被吸收到模型中，但我们想重用我们的跟踪模型。
- en: 'A common pitfall to avoid: pre- and postprocessing'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 避免的一个常见陷阱：预处理和后处理
- en: When switching from one library to another, it is easy to forget to check that
    the conversion steps are compatible. They are non-obvious unless we look up the
    memory layout and scaling convention of PyTorch and the image processing library
    we use. If we forget, we will be disappointed by not getting the results we anticipate.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当从一个库切换到另一个库时，很容易忘记检查转换步骤是否兼容。除非我们查看 PyTorch 和我们使用的图像处理库的内存布局和缩放约定，否则它们是不明显的。如果我们忘记了，我们将因为没有得到预期的结果而感到失望。
- en: Here, the model would go wild because it gets extremely large inputs. However,
    in the end, the output convention of our model is to give RGB values in the 0..1
    range. If we used this directly with CImg, the result would look all black.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型会变得疯狂，因为它接收到非常大的输入。然而，最终，我们模型的输出约定是在 0 到 1 的范围内给出 RGB 值。如果我们直接将其与 CImg
    一起使用，结果看起来会全是黑色。
- en: 'Other frameworks have other conventions: for example OpenCV likes to store
    images as BGR instead of RGB, requiring us to flip the channel dimension. We always
    want to make sure the input we feed to the model in the deployment is the same
    as what we fed into it in Python.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其他框架有其他约定：例如 OpenCV 喜欢将图像存储为 BGR 而不是 RGB，需要我们翻转通道维度。我们始终要确保在部署中向模型提供的输入���我们在
    Python 中输入的相同。
- en: 'Loading the traced model is very straightforward using `torch::jit::load`.
    Next, we have to deal with an abstraction PyTorch introduces to bridge between
    Python and C++: we need to wrap our input in an `IValue` (or several `IValue`s),
    the *generic* data type for any value. A function in the JIT is passed a vector
    of `IValue`s, so we declare that and then `push_back` our input tensor. This will
    automatically wrap our tensor into an `IValue`. We feed this vector of `IValue`s
    to the forward and get a single one back. We can then unpack the tensor in the
    resulting `IValue` with `.toTensor`.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch::jit::load` 加载跟踪模型非常简单。接下来，我们必须处理 PyTorch 引入的一个在 Python 和 C++ 之间桥接的抽象：我们需要将我们的输入包装在一个
    `IValue`（或多个 `IValue`）中，这是任何值的*通用*数据类型。 JIT 中的一个函数接收一个 `IValue` 向量，所以我们声明这个向量，然后
    `push_back` 我们的输入张量。这将自动将我们的张量包装成一个 `IValue`。我们将这个 `IValue` 向量传递给前向并得到一个返回的单个
    `IValue`。然后我们可以使用 `.toTensor` 解包结果 `IValue` 中的张量。
- en: 'Here we see a bit about `IValue`s: they have a type (here, `Tensor`), but they
    could also be holding `int64_t`s or `double`s or a list of tensors. For example,
    if we had multiple outputs, we would get an `IValue` holding a list of tensors,
    which ultimately stems from the Python calling conventions. When we unpack a tensor
    from an `IValue` using `.toTensor`, the `IValue` transfers ownership (becomes
    invalid). But let’s not worry about it; we got a tensor back. Because sometimes
    the model may return non-contiguous data (with gaps in the storage from chapter
    3), but `CImg` reasonably requires us to provide it with a contiguous block, we
    call `contiguous`. It is important that we assign this contiguous tensor to a
    variable that is in scope until we are done working with the underlying memory.
    Just like in Python, PyTorch will free memory if it sees that no tensors are using
    it anymore.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们了解一下 `IValue`：它们有一个类型（这里是 `Tensor`），但它们也可以持有 `int64_t` 或 `double` 或一组张量。例如，如果我们有多个输出，我们将得到一个持有张量列表的
    `IValue`，这最终源自于 Python 的调用约定。当我们使用 `.toTensor` 从 `IValue` 中解包张量时，`IValue` 将转移所有权（变为无效）。但让我们不要担心这个；我们得到了一个张量。因为有时模型可能返回非连续数据（从第
    3 章的存储中存在间隙），但 `CImg` 合理地要求我们提供一个连续的块，我们调用 `contiguous`。重要的是，我们将这个连续的张量分配给一个在使用底层内存时处于作用域内的变量。就像在
    Python 中一样，如果 PyTorch 发现没有张量在使用内存，它将释放内存。
- en: So let’s compile this! On Debian or Ubuntu, you need to install `cimg-dev`,
    `libjpeg-dev`, and `libx11-dev` to use `CImg`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们编译这个！在 Debian 或 Ubuntu 上，你需要安装 `cimg-dev`、`libjpeg-dev` 和 `libx11-dev`
    来使用 `CImg`。
- en: You can download a C++ library of PyTorch from the PyTorch page. But given that
    we already have PyTorch installed,[^(10)](#pgfId-1025738) we might as well use
    that; it comes with all we need for C++. We need to know where our PyTorch installation
    lives, so open Python and check `torch.__file__`, which may say /usr/local/lib/python3.7/dist-packages/
    torch/__init__.py. This means the CMake files we need are in /usr/local/lib/ python3.7/dist-packages/torch/share/cmake/.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 PyTorch 页面下载一个 PyTorch 的 C++ 库。但考虑到我们已经安装了 PyTorch，[^(10)](#pgfId-1025738)我们可能会选择使用它；它已经包含了我们在
    C++ 中所需的一切。我们需要知道我们的 PyTorch 安装位置在哪里，所以打开 Python 并检查 `torch.__file__`，它可能会显示 /usr/local/lib/python3.7/dist-packages/
    torch/__init__.py。这意味着我们需要的 CMake 文件在 /usr/local/lib/python3.7/dist-packages/torch/share/cmake/
    中。
- en: While using CMake seems like overkill for a single source file project, linking
    to PyTorch is a bit complex; so we just use the following as a boilerplate CMake
    file.[^(11)](#pgfId-1025792)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于一个单个源文件项目来说使用 CMake 似乎有点大材小用，但链接到 PyTorch 有点复杂；因此我们只需使用以下内容作为一个样板 CMake
    文件。[^(11)](#pgfId-1025792)
- en: Listing 15.12 CMakeLists.txt
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.12 CMakeLists.txt
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Project name. Replace it with your own here and on the other lines.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 项目名称。用你自己的项目名称替换这里和其他行。
- en: ❷ We need Torch.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们需要 Torch。
- en: ❸ We want to compile an executable named cyclegan-jit from the cyclegan_jit.cpp
    source file.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们想要从 cyclegan_jit.cpp 源文件编译一个名为 cyclegan-jit 的可执行文件。
- en: ❹ Links to the bits required for CImg. CImg itself is all-include, so it does
    not appear here.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 链接到 CImg 所需的部分。CImg 本身是全包含的，所以这里不会出现。
- en: 'It is best to make a build directory as a subdirectory of where the source
    code resides and then in it run CMake as[^(12)](#pgfId-1026061) `CMAKE_PREFIX_PATH=/usr/local/lib/python3.7/
    dist-packages/torch/share/cmake/ cmake ..` and finally `make`. This will build
    the `cyclegan-jit` program, which we can then run as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在源代码所在的子目录中创建一个构建目录，然后在其中运行 CMake，如[^(12)](#pgfId-1026061) `CMAKE_PREFIX_PATH=/usr/local/lib/python3.7/
    dist-packages/torch/share/cmake/ cmake ..`，最后 `make`。这将构建 `cyclegan-jit` 程序，然后我们可以运行如下：
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We just ran our PyTorch model without Python. Awesome! If you want to ship your
    application, you likely want to copy the libraries from /usr/local/lib/python3.7/dist-packages/torch/lib
    into where your executable is, so that they will always be found.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在没有 Python 的情况下运行了我们的 PyTorch 模型。太棒了！如果你想发布你的应用程序，你可能想将 /usr/local/lib/python3.7/dist-packages/torch/lib
    中的库复制到可执行文件所在的位置，这样它们就会始终被找到。
- en: '15.4.2 C++ from the start: The C++ API'
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 从头开始的 C++：C++ API
- en: The C++ modular API is intended to feel a lot like the Python one. To get a
    taste, we will translate the CycleGAN generator into a model natively defined
    in C++, but without the JIT. We do, however, need the pretrained weights, so we’ll
    save a traced version of the model (and here it is important to trace not a function
    but the model).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: C++ 模块化 API 旨在感觉很像 Python 的 API。为了体验一下，我们将把 CycleGAN 生成器翻译成在 C++ 中本地定义的模型，但没有
    JIT。但是，我们需要预训练的权重，因此我们将保存模型的跟踪版本（在这里重要的是跟踪模型而不是函数）。
- en: 'We’ll start with some administrative details: includes and namespaces.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一些行政细节开始：包括和命名空间。
- en: Listing 15.13 cyclegan_cpp_api.cpp
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.13 cyclegan_cpp_api.cpp
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Imports the one-stop torch/torch.h header and CImg
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入一站式 torch/torch.h 头文件和 CImg
- en: ❷ Spelling out torch::Tensor can be tedious, so we import the name into the
    main namespace.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 拼写`torch::Tensor`可能很繁琐，因此我们将名称导入主命名空间。
- en: When we look at the source code in the file, we find that `ConvTransposed2d`
    is ad hoc defined, when ideally it should be taken from the standard library.
    The issue here is that the C++ modular API is still under development; and with
    PyTorch 1.4, the premade `ConvTranspose2d` module cannot be used in `Sequential`
    because it takes an optional second argument.[^(13)](#pgfId-1026381) Usually we
    could just leave `Sequential`--as we did for Python--but we want our model to
    have the same structure as the Python CycleGAN generator from chapter 2.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看文件中的源代码时，我们发现`ConvTransposed2d`是临时定义的，理想情况下应该从标准库中获取。问题在于 C++ 模块化 API 仍在开发中；并且在
    PyTorch 1.4 中，预制的`ConvTranspose2d`模块无法在`Sequential`中使用，因为它需要一个可选的第二个参数。通常我们可以像我们为
    Python 所做的那样留下`Sequential`，但我们希望我们的模型具有与第 2 章 Python CycleGAN 生成器相同的结构。
- en: Next, let’s look at the residual block.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看残差块。
- en: Listing 15.14 Residual block in cyclegan_cpp_api.cpp
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.14 cyclegan_cpp_api.cpp 中的残差块
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Initializes Sequential, including its submodules
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化 Sequential，包括其子模块
- en: ❷ Always remember to register the modules you assign, or bad things will happen!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 始终记得注册您分配的模块，否则会发生糟糕的事情！
- en: ❸ As might be expected, our forward function is pretty simple.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 正如我们所预期的那样，我们的前向函数非常简单。
- en: Just as we would in Python, we register a subclass of `torch::nn::Module`. Our
    residual block has a sequential `conv_block` submodule.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Python 中所做的那样，我们注册`torch::nn::Module`的子类。我们的残差块有一个顺序的`conv_block`子模块。
- en: And just as we did in Python, we need to initialize our submodules, notably
    `Sequential`. We do so using the C++ initialization statement. This is similar
    to how we construct submodules in Python in the `__init__` constructor. Unlike
    Python, C++ does not have the introspection and hooking capabilities that enable
    redirection of `__setattr__` to combine assignment to a member and registration.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Python 中所做的那样，我们需要初始化我们的子模块，特别是`Sequential`。我们使用 C++ 初始化语句来做到这一点。这类似于我们在
    Python 中在`__init__`构造函数中构造子模块的方式。与 Python 不同，C++ 没有启发式和挂钩功能，使得将`__setattr__`重定向以结合对成员的赋值和注册成为可能。
- en: Since the lack of keyword arguments makes the parameter specification awkward
    with default arguments, modules (like tensor factory functions) typically take
    an `options` argument. Optional keyword arguments in Python correspond to methods
    of the options object that we can chain. For example, the Python module `nn.Conv2d(in_channels,
    out_channels, kernel_size, stride=2, padding=1)` that we need to convert translates
    to `torch::nn::Conv2d(torch::nn::Conv2dOptions (in_channels, out_channels, kernel_size).stride(2).padding(1))`.
    This is a bit more tedious, but you’re reading this because you love C++ and aren’t
    deterred by the hoops it makes you jump through.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏关键字参数使得带有默认参数的参数规范变得笨拙，模块（如张量工厂函数）通常需要一个`options`参数。Python 中的可选关键字参数对应于我们可以链接的选项对象的方法。例如，我们需要转换的
    Python 模块`nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=1)`对应于`torch::nn::Conv2d(torch::nn::Conv2dOptions
    (in_channels, out_channels, kernel_size).stride(2).padding(1))`。这有点繁琐，但您正在阅读这篇文章是因为您热爱
    C++，并且不会被它让您跳过的环节吓倒。
- en: 'We should always take care that registration and assignment to members is in
    sync, or things will not work as expected: for example, loading and updating parameters
    during training will happen to the registered module, but the actual module being
    called is a member. This synchronization was done behind the scenes by the Python
    `nn.Module` class, but it is not automatic in C++. Failing to do so will cause
    us many headaches.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应始终确保注册和分配给成员的同步，否则事情将不会按预期进行：例如，在训练期间加载和更新参数将发生在注册的模块上，但实际被调用的模块是一个成员。这种同步在
    Python 的 `nn.Module` 类后台完成，但在 C++ 中不是自动的。未能这样做将给我们带来许多头痛。
- en: In contrast to what we did (and should!) in Python, we need to call `m->forward(...)`
    for our modules. Some modules can also be called directly, but for `Sequential`,
    this is not currently the case.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 Python 中所做的（应该！）相反，我们需要为我们的模块调用`m->forward(...)`。一些模块也可以直接调用，但对于`Sequential`，目前不是这种情况。
- en: 'A final comment on calling conventions is in order: depending on whether you
    modify tensors provided to functions,[^(14)](#pgfId-1026839) tensor arguments
    should always be passed as `const Tensor&` for tensors that are left unchanged
    or `Tensor` if they are changed. Tensors should be returned as `Tensor`. Wrong
    argument types like non-const references (`Tensor&`) will lead to unparsable compiler
    errors.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后关于调用约定的评论是：根据您是否修改传递给函数的张量，张量参数应始终作为`const Tensor&`传递，对于不会更改的张量，或者如果它们被更改，则传递`Tensor`。应返回张量作为`Tensor`。错误的参数类型，如非
    const 引用（`Tensor&`），将导致无法解析的编译器错误。
- en: In the main generator class, we’ll follow a typical pattern in the C++ API more
    closely by naming our class `ResNetGeneratorImpl` and promoting it to a torch
    module `ResNetGenerator` using the `TORCH_MODULE` macro. The background is that
    we want to mostly handle modules as references or shared pointers. The wrapped
    class accomplishes this.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在主生成器类中，我们将更加密切地遵循 C++ API 中的典型模式，通过将我们的类命名为 `ResNetGeneratorImpl` 并使用 `TORCH_MODULE`
    宏将其提升为 torch 模块 `ResNetGenerator`。背景是我们希望大部分处理模块作为引用或共享指针。包装类实现了这一点。
- en: Listing 15.15 `ResNetGenerator` in cyclegan_cpp_api.cpp
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.15 cyclegan_cpp_api.cpp 中的 `ResNetGenerator`
- en: '[PRE30]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Adds modules to the Sequential container in the constructor. This allows us
    to add a variable number of modules in a for loop.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在构造函数中向 Sequential 容器添加模块。这使我们能够在 for 循环中添加可变数量的模块。
- en: ❷ Spares us from reproducing some tedious things
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使我们免于重复一些繁琐的事情
- en: ❸ An example of Options in action
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Options 的一个示例
- en: ❹ Creates a wrapper ResNetGenerator around our ResNetGeneratorImpl class. As
    archaic as it seems, the matching names are important here.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在我们的 ResNetGeneratorImpl 类周围创建一个包装器 ResNetGenerator。尽管看起来有些过时，但匹配的名称在这里很重要。
- en: That’s it--we’ve defined the perfect C++ analogue of the Python `ResNetGenerator`
    model. Now we only need a `main` function to load parameters and run our model.
    Loading the image with CImg and converting from image to tensor and tensor back
    to image are the same as in the previous section. To include some variation, we’ll
    display the image instead of writing it to disk.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样--我们定义了 Python `ResNetGenerator` 模型的完美 C++ 对应物。现在我们只需要一个 `main` 函数来加载参数并运行我们的模型。加载图像使用
    CImg 并将图像转换为张量，再将张量转换回图像与上一节中相同。为了增加一些变化，我们将显示图像而不是将其写入磁盘。
- en: Listing 15.16 cyclegan_cpp_api.cpp `main`
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.16 cyclegan_cpp_api.cpp `main`
- en: '[PRE31]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Instantiates our model
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化我们的模型
- en: ❷ Loads the parameters
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载参数
- en: ❸ Declaring a guard variable is the equivalent of the torch.no_grad() context.
    You can put it in a { ... } block if you need to limit how long you turn off gradients.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 声明一个守卫变量相当于 torch.no_grad() 上下文。如果需要限制关闭梯度的时间，可以将其放在 { ... } 块中。
- en: ❹ As in Python, eval mode is turned on (for our model, it would not be strictly
    relevant).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 就像在 Python 中��样，打开 eval 模式（对于我们的模型来说可能并不严格相关）。
- en: ❺ Again, we call forward rather than the model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 再次调用 forward 而不是 model。
- en: ❻ Displaying the image, we need to wait for a key rather than immediately exiting
    our program.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示图像时，我们需要等待按键而不是立即退出程序。
- en: The interesting changes are in how we create and run the model. Just as expected,
    we instantiate the model by declaring a variable of the model type. We load the
    model using `torch::load` (here it is important that we wrapped the model). While
    this looks very familiar to PyTorch practitioners, note that it will work on JIT-saved
    files rather than Python-serialized state dictionaries.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的变化在于我们如何创建和运行模型。正如预期的那样，我们通过声明模型类型的变量来实例化模型。我们使用 `torch::load` 加载模型（这里重要的是我们包装了模型）。虽然这看起来对于
    PyTorch 从业者来说非常熟悉，但请注意它将在 JIT 保存的文件上工作，而不是 Python 序列化的状态字典。
- en: When running the model, we need the equivalent of `with torch.no_grad():`. This
    is provided by instantiating a variable of type `NoGradGuard` and keeping it in
    scope for as long as we do not want gradients. Just like in Python, we set the
    model into evaluation mode calling `model->eval()`. This time around, we call
    `model->forward` with our input tensor and get a tensor as a result--no JIT is
    involved, so we do not need `IValue` packing and unpacking.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 运行模型时，我们需要相当于 `with torch.no_grad():` 的功能。这是通过实例化一个类型为 `NoGradGuard` 的变量并在我们不希望梯度时保持其范围来实现的。就像在
    Python 中一样，我们调用 `model->eval()` 将模型设置为评估模式。这一次，我们调用 `model->forward` 传入我们的输入张量并得到一个张量作为结果--不涉及
    JIT，因此我们不需要 `IValue` 的打包和解包。
- en: Phew. Writing this in C++ was a lot of work for the Python fans that we are.
    We are glad that we only promised to do inference here, but of course LibTorch
    also offers optimizers, data loaders, and much more. The main reason to use the
    API is, of course, when you want to create models and neither the JIT nor Python
    is a good fit.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。对于我们这些 Python 粉丝来说，在 C++ 中编写这个是很费力的。我们很高兴我们只承诺在这里进行推理，但当然 LibTorch 也提供了优化器、数据加载器等等。使用
    API 的主要原因当然是当你想要创建模型而 JIT 和 Python 都不合适时。
- en: For your convenience, CMakeLists.txt contains also the instructions for building
    `cyclegan-cpp-api`, so building is just like in the previous section.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了您的方便，CMakeLists.txt 中还包含了构建 `cyclegan-cpp-api` 的说明，因此构建就像在上一节中一样简单。
- en: We can run the program as
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行程序如下
- en: '[PRE32]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: But we knew what the model would be doing, didn’t we?
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们知道模型会做什么，不是吗？
- en: 15.5 Going mobile
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 走向移动
- en: As the last variant of deploying a model, we will consider deployment to mobile
    devices. When we want to bring our models to mobile, we are typically looking
    at Android and/or iOS. Here, we’ll focus on Android.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 作为部署模型的最后一个变体，我们将考虑部署到移动设备。当我们想要将我们的模型带到移动设备时，通常会考虑 Android 和/或 iOS。在这里，我们将专注于
    Android。
- en: The C++ parts of PyTorch--LibTorch--can be compiled for Android, and we could
    access that from an app written in Java using the Android Java Native Interface
    (JNI). But we really only need a handful of functions from PyTorch--loading a
    JITed model, making inputs into tensors and `IValue`s, running them through the
    model, and getting results back. To save us the trouble of using the JNI, the
    PyTorch developers wrapped these functions into a small library called PyTorch
    Mobile.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的 C++ 部分--LibTorch--可以编译为 Android，并且我们可以通过使用 Android Java Native Interface
    (JNI) 编写的应用程序从 Java 中访问它。但实际上我们只需要从 PyTorch 中使用少量函数--加载 JIT 模型，将输入转换为张量和 `IValue`，通过模型运行它们，并将结果返回。为了避免使用
    JNI 的麻烦，PyTorch 开发人员将这些函数封装到一个名为 PyTorch Mobile 的小型库中。
- en: The stock way of developing apps in Android is to use the Android Studio IDE,
    and we will be using it, too. But this means there are a few dozen files of administrativa--which
    also happen to change from one Android version to the next. As such, we focus
    on the bits that turn one of the Android Studio templates (Java App with Empty
    Activity) into an app that takes a picture, runs it through our zebra-CycleGAN,
    and displays the result. Sticking with the theme of the book, we will be efficient
    with the Android bits (and they can be painful compared with writing PyTorch code)
    in the example app.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在Android中开发应用程序的标准方式是使用Android Studio IDE，我们也将使用它。但这意味着有几十个管理文件--这些文件也会随着Android版本的更改而改变。因此，我们专注于将Android
    Studio模板（具有空活动的Java应用程序）转换为一个拍照、通过我们的斑马CycleGAN运行图片并显示结果的应用程序的部分。遵循本书的主题，我们将在示例应用程序中高效处理Android部分（与编写PyTorch代码相比可能会更痛苦）。
- en: 'To infuse life into the template, we need to do three things. First, we need
    to define a UI. To keep things as simple as we can, we have two elements: a `TextView`
    named `headline` that we can click to take and transform a picture; and an `ImageView`
    to show our picture, which we call `image_view`. We will leave the picture-taking
    to the camera app (which you would likely avoid doing in an app for a smoother
    user experience), because dealing with the camera directly would blur our focus
    on deploying PyTorch models.[^(15)](#pgfId-1027898)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要使模板生动起来，我们需要做三件事。首先，我们需要定义一个用户界面。为了尽可能简单，我们有两个元素：一个名为`headline`的`TextView`，我们可以点击以拍摄和转换图片；以及一个用于显示我们图片的`ImageView`，我们称之为`image_view`。我们将把拍照留给相机应用程序（在应用程序中可能会避免这样做以获得更流畅的用户体验），因为直接处理相机会模糊我们专注于部署PyTorch模型的焦点。
- en: Then, we need to include PyTorch as a dependency. This is done by editing our
    app’s build.gradle file and adding `pytorch_android` and `pytorch_android_torchvision`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将PyTorch作为依赖项包含进来。这是通过编辑我们应用程序的build.gradle文件并添加`pytorch_android`和`pytorch_android_torchvision`来完成的。
- en: Listing 15.17 Additions to build.gradle
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 15.17 build.gradle的添加部分
- en: '[PRE33]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ The dependencies section is very likely already there. If not, add it at the
    bottom.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 依赖部分很可能��经存在。如果没有，请在底部添加。
- en: ❷ The pytorch_android library gets the core things mentioned in the text.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ pytorch_android库获取了文本中提到的核心内容。
- en: ❸ The helper library pytorch_android_torchvision--perhaps a bit immodestly named
    when compared to its larger TorchVision sibling--contains a few utilities to convert
    bitmap objects to tensors, but at the time of writing not much more.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 辅助库pytorch_android_torchvision--与其更大的TorchVision兄弟相比可能有点自负地命名--包含一些将位图对象转换为张量的实用程序，但在撰写本文时没有更多内容。
- en: We need to add our traced model as an asset.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将我们的跟踪模型添加为资产。
- en: 'Finally, we can get to the meat of our shiny app: the Java class derived from
    activity that contains our main code. We’ll just discuss an excerpt here. It starts
    with imports and model setup.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以进入我们闪亮应用的核心部分：从活动派生的Java类，其中包含我们的主要代码。我们这里只讨论一个摘录。它以导入和模型设置开始。
- en: Listing 15.18 MainActivity.java part 1
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 15.18 MainActivity.java第1部分
- en: '[PRE34]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Don’t you love imports?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你喜欢导入吗？
- en: ❷ Holds our JITed model
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含我们的JIT模型
- en: ❸ In Java we have to catch the exceptions.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在Java中我们必须捕获异常。
- en: ❹ Loads the module from a file
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从文件加载模块
- en: We need some imports from the `org.pytorch` namespace. In the typical style
    that is a hallmark of Java, we import `IValue`, `Module`, and `Tensor`, which
    do what we might expect; and the class `org.pytorch.torchvision.TensorImageUtils`,
    which holds utility functions to convert between tensors and images.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从`org.pytorch`命名空间导入一些内容。在Java的典型风格中，我们导入`IValue`、`Module`和`Tensor`，它们的功能符合我们的预期；以及`org.pytorch.torchvision.TensorImageUtils`类，其中包含在张量和图像之间转换的实用函数。
- en: 'First, of course, we need to declare a variable holding our model. Then, when
    our app is started--in `onCreate` of our activity--we’ll load the module using
    the `Model.load` method from the location given as an argument. There is a slight
    complication though: apps’ data is provided by the supplier as *assets* that are
    not easily accessible from the filesystem. For this reason, a utility method called
    `assetFilePath` (taken from the PyTorch Android examples) copies the asset to
    a location in the filesystem. Finally, in Java, we need to catch exceptions that
    our code throws, unless we want to (and are able to) declare the method we are
    coding as throwing them in turn.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当然，我们需要声明一个变量来保存我们的模型。然后，在我们的应用启动时--在我们的活动的`onCreate`中--我们将使用`Model.load`方法从给定的位置加载模块。然而，有一个小复杂之处：应用程序的数据是由供应商提供的*资产*，这些资产不容易从文件系统中访问。因此，一个名为`assetFilePath`的实用方法（取自PyTorch
    Android示例）将资产复制到文件系统中的一个位置。最后，在Java中，我们需要捕获代码抛出的异常，除非我们想要（并且能够）依次声明我们编写的方法抛出异常。
- en: When we get an image from the camera app using Android’s `Intent` mechanism,
    we need to run it through our model and display it. This happens in the `onActivityResult`
    event handler.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用Android的`Intent`机制从相机应用程序获取图像时，我们需要运行它通过我们的模型并显示它。这发生在`onActivityResult`事件处理程序中。
- en: Listing 15.19 MainActivity.java, part 2
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 15.19 MainActivity.java，第2部分
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ This is executed when the camera app takes a picture.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当相机应用程序拍照时执行此操作。
- en: '❷ Performs normalization, but the default is images in the range of 0...1 so
    we do not need to transform: that is, have 0 shift and a scaling divisor of 1.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行归一化，但默认情况下图像范围为0...1，因此我们不需要转换：即具有0偏移和1的缩放除数。
- en: ❸ Gets a tensor from a bitmap, combining steps like TorchVision’s ToTensor (converting
    to a float tensor with entries between 0 and 1) and Normalize
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从位图获取张量，结合TorchVision的ToTensor步骤（将其转换为介于0和1之间的浮点张量）和Normalize
- en: ❹ This looks almost like what we did in C++.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这看起来几乎和我们在C++中做的一样。
- en: ❺ tensorToBitmap is our own invention.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ tensorToBitmap是我们自己的创造。
- en: Converting the bitmap we get from Android to a tensor is handled by the `TensorImageUtils
    .bitmapToFloat32Tensor` function (static method), which takes two float arrays,
    `means` and `stds`, in addition to `bitmap`. Here we specify the mean and standard
    deviation of our input data(set), which will then be mapped to have zero mean
    and unit standard deviation just like TorchVision’s `Normalize` transform. Android
    already gives us the images in the 0..1 range that we need to feed into our model,
    so we specify mean 0 and standard deviation 1 to prevent the normalization from
    changing our image.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 将从Android获取的位图转换为张量由`TensorImageUtils.bitmapToFloat32Tensor`函数（静态方法）处理，该函数除了`bitmap`之外还需要两个浮点数组`means`和`stds`。在这里，我们指定输入数据（集）的均值和标准差，然后将其映射为具有零均值和单位标准差的数据，就像TorchVision的`Normalize`变换一样。Android已经将图像给我们提供在0..1范围内，我们需要将其馈送到我们的模型中，因此我们指定均值为0，标准差为1，以防止归一化改变我们的图像。
- en: Around the actual call to `model.forward`, we then do the same `IValue` wrapping
    and unwrapping dance that we did when using the JIT in C++, except that our `forward`
    takes a single `IValue` rather than a vector of them. Finally, we need to get
    back to a bitmap. Here PyTorch will not help us, so we need to define our own
    `tensorToBitmap` (and submit the pull request to PyTorch). We spare you the details
    here, as they are tedious and full of copying (from the tensor to a `float[]`
    array to a `int[]` array containing ARGB values to the bitmap), but it is as it
    is. It is designed to be the inverse of `bitmapToFloat32Tensor`.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际调用`model.forward`时，我们执行与在C++中使用JIT时相同的`IValue`包装和解包操作，只是我们的`forward`接受一个`IValue`而不是一个向量。最后，我们需要回到位图。在这里，PyTorch不会帮助我们，因此我们需要定义自己的`tensorToBitmap`（并向PyTorch提交拉取请求）。我们在这里不详细介绍，因为这些细节很繁琐且充满复制（从张量到`float[]`数组到包含ARGB值的`int[]`数组到位图），但事实就是如此。它被设计为`bitmapToFloat32Tensor`的逆过程。
- en: '![](../Images/CH15_F05_Stevens2_GS.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH15_F05_Stevens2_GS.png)'
- en: Figure 15.5 Our CycleGAN zebra app
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 我们的CycleGAN斑马应用
- en: And that’s all we need to do to get PyTorch into Android. Using the minimal
    additions to the code we left out here to request a picture, we have a `Zebraify`
    Android app that looks like in what we see in figure 15.5\. Well done![^(16)](#pgfId-1029112)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要做的一切，就可以将PyTorch引入Android。使用我们在这里留下的最小代码补充来请求一张图片，我们就有了一个看起来像图15.5中所见的`Zebraify`
    Android应用程序。干得好！[^(16)](#pgfId-1029112)
- en: We should note that we end up with a full version of PyTorch with all ops on
    Android. This will, in general, also include operations you will not need for
    a given task, leading to the question of whether we could save some space by leaving
    them out. It turns out that starting with PyTorch 1.4, you can build a customized
    version of the PyTorch library that includes only the operations you need (see
    [https://pytorch.org/mobile/ android/#custom-build](https://pytorch.org/mobile/android/#custom-build)).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，我们在Android上使用了PyTorch的完整版本，其中包含所有操作。一般来说，这也会包括您在特定任务中不需要的操作，这就引出了一个问题，即我们是否可以通过将它们排除在外来节省一些空间。事实证明，从PyTorch
    1.4开始，您可以构建一个定制版本的PyTorch库，其中只包括您需要的操作（参见[https://pytorch.org/mobile/android/#custom-build](https://pytorch.org/mobile/android/#custom-build)）。
- en: '15.5.1 Improving efficiency: Model design and quantization'
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 提高效率：模型设计和量化
- en: 'If we want to explore mobile in more detail, our next step is to try to make
    our models faster. When we wish to reduce the memory and compute footprint of
    our models, the first thing to look at is streamlining the model itself: that
    is, computing the same or very similar mappings from inputs to outputs with fewer
    parameters and operations. This is often called *distillation*. The details of
    distillation vary--sometimes we try to shrink each weight by eliminating small
    or irrelevant weights;[^(17)](#pgfId-1031056) in other examples, we combine several
    layers of a net into one (DistilBERT) or even train a fully different, simpler
    model to reproduce the larger model’s outputs (OpenNMT’s original CTranslate).
    We mention this because these modifications are likely to be the first step in
    getting models to run faster.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想更详细地探索移动端，我们的下一步是尝试使我们的模型更快。当我们希望减少模型的内存和计算占用空间时，首先要看的是简化模型本身：也就是说，使用更少的参数和操作计算相同或非常相似的输入到输出的映射。这通常被称为*蒸馏*。蒸馏的细节各不相同--有时我们尝试通过消除小或无关的权重来缩小每个权重；在其他示例中，我们将网络的几层合并为一层（DistilBERT），甚至训练一个完全不同、更简单的模型来复制较大模型的输出（OpenNMT的原始CTranslate）。我们提到这一点是因��这些修改很可能是使模型运行更快的第一步。
- en: 'Another approach to is to reduce the footprint of each parameter and operation:
    instead of expending the usual 32-bit per parameter in the form of a float, we
    convert our model to work with integers (a typical choice is 8-bit). This is *quantization*.[^(18)](#pgfId-1031139)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是减少每个参数和操作的占用空间：我们将模型转换为使用整数（典型选择是8位）而不是以浮点数的形式花费通常的32位每个参数。这就是*量化*。[^(18)](#pgfId-1031139)
- en: PyTorch does offer quantized tensors for this purpose. They are exposed as a
    set of scalar types similar to `torch.float`, `torch.double`, and `torch.long`
    (compare section 3.5). The most common quantized tensor scalar types are `torch.quint8`
    and `torch.qint8`, representing numbers as unsigned and signed 8-bit integers,
    respectively. PyTorch uses a separate scalar type here in order to use the dispatch
    mechanism we briefly looked at in section 3.11.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch确实为此目的提供了量化张量。它们被公开为一组类似于`torch.float`、`torch.double`和`torch.long`的标量类型（请参阅第3.5节）。最常见的量化张量标量类型是`torch.quint8`和`torch.qint8`，分别表示无符号和有符号的8位整数。PyTorch在这里使用单独的标量类型，以便使用我们在第3.11节简要介绍的分派机制。
- en: 'It might seem surprising that using 8-bit integers instead of 32-bit floating-points
    works at all; and typically there is a slight degradation in results, but not
    much. Two things seem to contribute: if we consider rounding errors as essentially
    random, and convolutions and linear layers as weighted averages, we may expect
    rounding errors to typically cancel.[^(19)](#pgfId-1031206) This allows reducing
    the relative precision from more than 20 bits in 32-bit floating-points to the
    7 bits that signed integers offer. The other thing quantization does (in contrast
    to training with 16-bit floating-points) is move from floating-point to fixed
    precision (per tensor or channel). This means the largest values are resolved
    to 7-bit precision, and values that are one-eighth of the largest values to only
    7 - 3 = 4 bits. But if things like L1 regularization (briefly mentioned in chapter
    8) work, we might hope similar effects allow us to afford less precision to the
    smaller values in our weights when quantizing. In many cases, they do.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 8 位整数而不是 32 位浮点数似乎能够正常工作可能会让人感到惊讶；通常结果会有轻微的降级，但不会太多。有两个因素似乎起到作用：如果我们将舍入误差视为基本上是随机的，并且将卷积和线性层视为加权平均，我们可能期望舍入误差通常会抵消。[^(19)](#pgfId-1031206)
    这允许将相对精度从 32 位浮点数的 20 多位减少到有符号整数提供的 7 位。量化的另一件事（与使用 16 位浮点数进行训练相反）是从浮点数转换为固定精度（每个张量或通道）。这意味着最大值被解析为
    7 位精度，而是最大值的八分之一的值仅为 7 - 3 = 4 位。但如果像 L1 正则化（在第 8 章中简要提到）这样的事情起作用，我们可能希望类似的效果使我们在量化时能够为权重中的较小值提供更少的精度。在许多情况下，确实如此。
- en: Quantization debuted with PyTorch 1.3 and is still a bit rough in terms of supported
    operations in PyTorch 1.4\. It is rapidly maturing, though, and we recommend checking
    it out if you are serious about computationally efficient deployment.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 量化功能于 PyTorch 1.3 首次亮相，但在 PyTorch 1.4 中在支持的操作方面仍有些粗糙。不过，它正在迅速成熟，我们建议如果您真的关心计算效率的部署，不妨试试看。
- en: '15.6 Emerging technology: Enterprise serving of PyTorch models'
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.6 新兴技术：企业 PyTorch 模型服务
- en: We may ask ourselves whether all the deployment aspects discussed so far should
    involve as much coding as they do. Sure, it is common enough for someone to code
    all that. As of early 2020, while we are busy with the finishing touches to the
    book, we have great expectations for the near future; but at the same time, we
    feel that the deployment landscape will significantly change by the summer.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会问自己，迄今为止讨论的所有部署方面是否都需要像它们现在这样涉及大量编码。当然，有人编写所有这些代码是很常见的。截至2020年初，当我们忙于为这本书做最后的润色时，我们对不久的将来寄予厚望；但与此同时，我们感觉到部署领域将在夏季发生重大变化。
- en: Currently, RedisAI ([https://github.com/RedisAI/redisai-py](https://github.com/RedisAI/redisai-py)),
    which one of the authors is involved with, is waiting to apply Redis goodness
    to our models. PyTorch has just experimentally released TorchServe (after this
    book is finalized, see [https://pytorch.org/ blog/pytorch-library-updates-new-model-serving-library/#torchserve-experimental](https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/#torchserve-experimental)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，RedisAI（[https://github.com/RedisAI/redisai-py](https://github.com/RedisAI/redisai-py)）中的一位作者正在等待将
    Redis 的优势应用到我们的模型中。PyTorch 刚刚实验性发布了 TorchServe（在这本书完成后，请查看[https://pytorch.org/
    blog/pytorch-library-updates-new-model-serving-library/#torchserve-experimental](https://pytorch.org/blog/pytorch-library-updates-new-model-serving-library/#torchserve-experimental)）。
- en: Similarly, MLflow ([https://mlflow.org](https://mlflow.org)) is building out
    more and more support, and Cortex ([https://cortex.dev](https://cortex.dev)) wants
    us to use it to deploy models. For the more specific task of information retrieval,
    there also is EuclidesDB ([https://euclidesdb.readthedocs.io/ en/latest](https://euclidesdb.readthedocs.io/en/latest))
    to do AI-based feature databases.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，MLflow（[https://mlflow.org](https://mlflow.org)）正在不断扩展更多支持，而 Cortex（[https://cortex.dev](https://cortex.dev)）希望我们使用它来部署模型。对于更具体的信息检索任务，还有
    EuclidesDB（[https://euclidesdb.readthedocs.io/ en/latest](https://euclidesdb.readthedocs.io/en/latest)）来执行基于
    AI 的特征数据库。
- en: Exciting times, but unfortunately, they do not sync with our writing schedule.
    We hope to have more to tell in the second edition (or a second book)!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 令人兴奋的时刻，但不幸的是，它们与我们的写作��划不同步。我们希望在第二版（或第二本书）中有更多内容可以告诉您！
- en: 15.7 Conclusion
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.7 结论
- en: This concludes our short tour of how to get our models out to where we want
    to apply them. While the ready-made Torch serving is not quite there yet as we
    write this, when it arrives you will likely want to export your models through
    the JIT--so you’ll be glad we went through it here. In the meantime, you now know
    how to deploy your model to a network service, in a C++ application, or on mobile.
    We look forward to seeing what you will build!
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们如何将我们的模型部署到我们想要应用它们的地方的简短介绍。虽然现成的 Torch 服务在我们撰写本文时还不够完善，但当它到来时，您可能会希望通过
    JIT 导出您的模型--所以您会很高兴我们在这里经历了这一过程。与此同时，您现在知道如何将您的模型部署到网络服务、C++ 应用程序或移动设备上。我们期待看到您将会构建什么！
- en: 'Hopefully we’ve also delivered on the promise of this book: a working knowledge
    of deep learning basics, and a level of comfort with the PyTorch library. We hope
    you’ve enjoyed reading as much as we’ve enjoyed writing.[^(20)](#pgfId-1039147)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们也实现了这本书的承诺：对深度学习基础知识有所了解，并对 PyTorch 库感到舒适。我们希望您阅读的过程和我们写作的过程一样愉快。[^(20)](#pgfId-1039147)
- en: 15.8 Exercises
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.8 练习
- en: 'As we close out *Deep Learning with PyTorch*, we have one final exercise for
    you:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束 *使用 PyTorch 进行深度学习* 时，我们为您准备了最后一个练习：
- en: Pick a project that sounds exciting to you. Kaggle is a great place to start
    looking. Dive in.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个让您感到兴奋的项目。Kaggle 是一个很好的开始地方。开始吧。
- en: You have acquired the skills and learned the tools you need to succeed. We can’t
    wait to hear what you do next; drop us a line on the book’s forum and let us know!
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经掌握了成功所需的技能并学会了必要的工具。我们迫不及待想知道接下来您会做什么；在书的论坛上给我们留言，让我们知道！
- en: 15.9 Summary
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.9 总结
- en: We can serve PyTorch models by wrapping them in a Python web server framework
    such as Flask.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过将 PyTorch 模型包装在 Python Web 服务器框架（如 Flask）中来提供 PyTorch 模型的服务。
- en: By using JITed models, we can avoid the GIL even when calling them from Python,
    which is a good idea for serving.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用JIT模型，我们可以避免即使从Python调用它们时也避免GIL，这对于服务是一个好主意。
- en: Request batching and asynchronous processing helps use resources efficiently,
    in particular when inference is on the GPU.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求批处理和异步处理有助于有效利用资源，特别是在GPU上进行推理时。
- en: To export models beyond PyTorch, ONNX is a great format. ONNX Runtime provides
    a backend for many purposes, including the Raspberry Pi.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将模型导出到PyTorch之外，ONNX是一个很好的格式。ONNX Runtime为许多目的提供后端支持，包括树莓派。
- en: The JIT allows you to export and run arbitrary PyTorch code in C++ or on mobile
    with little effort.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JIT允许您轻松导出和运行任意PyTorch代码在C++中或在移动设备上。
- en: Tracing is the easiest way to get JITed models; you might need to use scripting
    for some particularly dynamic parts.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 追踪是获得JIT模型的最简单方法；对于一些特别动态的部分，您可能需要使用脚本。
- en: There also is good support for C++ (and an increasing number of other languages)
    for running models both JITed and natively.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于运行JIT和本地模型，C++（以及越来越多的其他语言）也有很好的支持。
- en: PyTorch Mobile lets us easily integrate JITed models into Android or iOS apps.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Mobile让我们可以轻松地将JIT模型集成到Android或iOS应用程序中。
- en: For mobile deployments, we want to streamline the model architecture and quantize
    models if possible.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于移动部署，我们希望简化模型架构并在可能的情况下对模型进行量化。
- en: A few deployment frameworks are emerging, but a standard isn’t quite visible
    yet.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几个部署框架正在兴起，但标准尚不太明显。
- en: '* * *'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)To play it safe, do not do this on an untrusted network.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)为了安全起见，请勿在不受信任的网络上执行此操作。
- en: ^(2.)Or `pip3` for Python3\. You also might want to run it from a Python virtual
    environment.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)或者对于Python3使用`pip3`。您可能还希望从Python虚拟环境中运行它。
- en: ^(3.)One of the earliest public talks discussing the inadequacy of Flask serving
    for PyTorch models is Christian Perone’s “PyTorch under the Hood,” [http://mng.bz/xWdW](http://mng.bz/xWdW).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)早期公开讨论Flask为PyTorch模型提供服务的不足之处之一是Christian Perone的“PyTorch under the Hood”，[http://mng.bz/xWdW](http://mng.bz/xWdW)。
- en: '^(4.)Fancy people call these asynchronous function *generators* or sometimes,
    more loosely, *coroutines*: [https:// en.wikipedia.org/wiki/Coroutine](https://en.wikipedia.org/wiki/Coroutine).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)高级人士将这些异步函数称为*生成器*，有时更宽松地称为*协程*：[https:// en.wikipedia.org/wiki/Coroutine](https://en.wikipedia.org/wiki/Coroutine)。
- en: ^(5.)An alternative might be to forgo the timer and just run whenever the queue
    is not empty. This would potentially run smaller “first” batches, but the overall
    performance impact might not be so large for most applications.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)另一种选择可能是放弃计时器，只有在队列不为空时才运行。这可能会运行较小的“第一”批次，但对于大多数应用程序来说，整体性能影响可能不会太大。
- en: ^(6.)The code lives at [https://github.com/microsoft/onnxruntime](https://github.com/microsoft/onnxruntime),
    but be sure to read the privacy statement! Currently, building ONNX Runtime yourself
    will get you a package that does not send things to the mothership.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^(6.)代码位于[https://github.com/microsoft/onnxruntime](https://github.com/microsoft/onnxruntime)，但请务必阅读隐私声明！目前，自行构建ONNX
    Runtime将为您提供一个不会向母公司发送信息的软件包。
- en: ^(7.)Strictly speaking, this traces the model as a function. Recently, PyTorch
    gained the ability to preserve more of the module structure using `torch.jit.trace_module`,
    but for us, the plain tracing is sufficient.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ^(7.)严格来说，这将模型追踪为一个函数。最近，PyTorch获得了使用`torch.jit.trace_module`保留更多模块结构的能力，但对我们来说，简单的追踪就足够了。
- en: ^(8.)But TorchVision may develop a convenience function for loading images.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^(8.)但TorchVision可能会开发一个方便的函数来加载图像。
- en: ^(9.)The code works with PyTorch 1.4 and, hopefully, above. In PyTorch versions
    before 1.3 you needed `data` in place of `data_ptr`.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^(9.)该代码适用于PyTorch 1.4及以上版本。在PyTorch 1.3之前的版本中，您需要使用`data`代替`data_ptr`。
- en: ^(10.)We hope you have not been slacking off about trying out things you read.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^(10.)我们希望您一直在尝试阅读的内容。
- en: ^(11.)The code directory has a bit longer version to work around Windows issues.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^(11.)代码目录有一个稍长版本，以解决Windows问题。
- en: '^(12.)You might have to replace the path with where your PyTorch or LibTorch
    installation is located. Note that the C++ library can be more picky than the
    Python one in terms of compatibility: If you are using a CUDA-enabled library,
    you need to have the matching CUDA headers installed. If you get cryptic error
    messages about “Caffe2 using CUDA,” you need to install a CPU-only version of
    the library, but CMake found a CUDA-enabled one.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^(12.)您可能需要将路径替换为您的PyTorch或LibTorch安装位置。请注意，与Python相比，C++库在兼容性方面可能更挑剔：如果您使用的是支持CUDA的库，则需要安装匹配的CUDA头文件。如果您收到关于“Caffe2使用CUDA”的神秘错误消息，则需要安装一个仅支持CPU的库版本，但CMake找到了一个支持CUDA的库。
- en: ^(13.)This is a great improvement over PyTorch 1.3, where we needed to implement
    custom modules for ReLU, `ÌnstanceNorm2d`, and others.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^(13.)这是对PyTorch 1.3的巨大改进，我们需要为ReLU、`ÌnstanceNorm2d`和其他模块实现自定义模块。
- en: ^(14.)This is a bit blurry because you can create a new tensor sharing memory
    with an input and modify it in place, but it’s best to avoid that if possible.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^(14.)这有点模糊，因为你可以创建一个与输入共享内存并就地修改的新张量，但最好尽量避免这样做。
- en: ^(15.)We are very proud of the topical metaphor.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^(15.)我们对这个主题隐喻感到非常自豪。
- en: ^(16.)At the time of writing, PyTorch Mobile is still relatively young, and
    you may hit rough edges. On Pytorch 1.3, the colors were off on an actual 32-bit
    ARM phone while working in the emulator. The reason is likely a bug in one of
    the computational backend functions that are only used on ARM. With PyTorch 1.4
    and a newer phone (64-bit ARM), it seemed to work better.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^(16.)撰写时，PyTorch Mobile仍然相对年轻，您可能会遇到一些问题。在Pytorch 1.3上，实际的32位ARM手机在模拟器中工作时颜色不正确。原因很可能是ARM上仅在使用的计算后端函数中存在错误。使用PyTorch
    1.4和更新的手机（64位ARM）似乎效果更好。
- en: ^(17.)Examples include the Lottery Ticket Hypothesis and WaveRNN.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^(17.)示例包括彩票假设和WaveRNN。
- en: ^(18.)In contrast to quantization, (partially) moving to 16-bit floating-point
    for training is usually called *reduced* or (if some bits stay 32-bit) *mixed-precision*
    training.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^(18.)与量化相比，（部分）转向16位浮点数进行训练通常被称为*减少*或（如果某些位保持32位）*混合精度*训练。
- en: ^(19.)Fancy people would refer to the Central Limit Theorem here. And indeed,
    we must take care that the independence (in the statistical sense) of rounding
    errors is preserved. For example, we usually want zero (a prominent output of
    ReLU) to be exactly representable. Otherwise, all the zeros would be changed by
    the exact same quantity in rounding, leading to errors adding up rather than canceling.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^(19.)时髦的人们可能会在这里提到中心极限定理。确实，我们必须注意保持舍入误差的独立性（在统计意义上）。例如，我们通常希望零（ReLU的一个显著输出）能够被精确表示。否则，所有的零将会在舍入中被完全相同的数量改变，导致误差累积而不是抵消。
- en: ^(20.)More, actually; writing books is hard!
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^(20.)实际上更多；写书真的很难！
