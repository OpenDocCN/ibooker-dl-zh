- en: 1 Understanding Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 理解大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: High-level explanations of the fundamental concepts behind large language models
    (LLMs)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）背后基本概念的高级解释
- en: Insights into the transformer architecture from which ChatGPT-like LLMs are
    derived
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于ChatGPT类LLMs所衍生的变换器架构的见解
- en: A plan for building an LLM from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头构建大型语言模型的计划
- en: Large language models (LLMs), such as those offered in OpenAI's ChatGPT, are
    deep neural network models that have been developed over the past few years. They
    ushered in a new era for Natural Language Processing (NLP). Before the advent
    of large language models, traditional methods excelled at categorization tasks
    such as email spam classification and straightforward pattern recognition that
    could be captured with handcrafted rules or simpler models. However, they typically
    underperformed in language tasks that demanded complex understanding and generation
    abilities, such as parsing detailed instructions, conducting contextual analysis,
    or creating coherent and contextually appropriate original text. For example,
    previous generations of language models could not write an email from a list of
    keywords—a task that is trivial for contemporary LLMs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如OpenAI的ChatGPT，是近年来开发的深度神经网络模型。它们为自然语言处理（NLP）带来了新时代。在大型语言模型出现之前，传统方法在电子邮件垃圾分类和能够用手工规则或简单模型捕捉的直观模式识别等分类任务上表现出色。然而，它们在需要复杂理解和生成能力的语言任务上通常表现不佳，例如解析详细指令、进行上下文分析或创建连贯且上下文适宜的原创文本。例如，以前的语言模型无法根据关键词列表撰写电子邮件，而这对于当代LLMs来说是微不足道的任务。
- en: LLMs have remarkable capabilities to understand, generate, and interpret human
    language. However, it's important to clarify that when we say language models
    "understand," we mean that they can process and generate text in ways that appear
    coherent and contextually relevant, not that they possess human-like consciousness
    or comprehension.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs具备理解、生成和解释人类语言的卓越能力。然而，重要的是要澄清，当我们说语言模型“理解”时，我们指的是它们能够以连贯且上下文相关的方式处理和生成文本，而不是说它们具有人类般的意识或理解能力。
- en: Enabled by advancements in deep learning, which is a subset of machine learning
    and artificial intelligence (AI) focused on neural networks, LLMs are trained
    on vast quantities of text data. This allows LLMs to capture deeper contextual
    information and subtleties of human language compared to previous approaches.
    As a result, LLMs have significantly improved performance in a wide range of NLP
    tasks, including text translation, sentiment analysis, question answering, and
    many more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习的进展，深度学习是机器学习和人工智能（AI）的一部分，专注于神经网络，大型语言模型（LLMs）在大量文本数据上进行训练。这使得LLMs能够捕捉比以往方法更深层次的上下文信息和人类语言的细微差别。因此，LLMs在文本翻译、情感分析、问答等多种自然语言处理（NLP）任务中表现显著提升。
- en: Another important distinction between contemporary LLMs and earlier NLP models
    is that the latter were typically designed for specific tasks; whereas those earlier
    NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency
    across a wide range of NLP tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当代LLMs与早期NLP模型之间另一个重要区别在于，后者通常是为特定任务设计的；而早期NLP模型在狭窄应用中表现出色，LLMs则在更广泛的NLP任务中展示了更广泛的能力。
- en: The success behind LLMs can be attributed to the transformer architecture which
    underpins many LLMs, and the vast amounts of data LLMs are trained on, allowing
    them to capture a wide variety of linguistic nuances, contexts, and patterns that
    would be challenging to manually encode.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的成功归因于支持许多LLMs的变换器架构，以及LLMs在海量数据上进行训练，使其能够捕捉多种语言细微差别、上下文和模式，这些都是手动编码时很难实现的。
- en: This shift towards implementing models based on the transformer architecture
    and using large training datasets to train LLMs has fundamentally transformed
    NLP, providing more capable tools for understanding and interacting with human
    language.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向基于变换器架构的模型实施和使用大型训练数据集训练LLMs的转变，根本改变了NLP，为理解和与人类语言互动提供了更强大的工具。
- en: 'Beginning with this chapter, we set the foundation to accomplish the primary
    objective of this book: understanding LLMs by implementing a ChatGPT-like LLM
    based on the transformer architecture step by step in code.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们奠定了实现本书主要目标的基础：通过逐步用代码实现基于变换器架构的ChatGPT类LLM，理解LLMs。
- en: 1.1 What is an LLM?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 什么是LLM？
- en: An LLM, a large language model, is a neural network designed to understand,
    generate, and respond to human-like text. These models are deep neural networks
    trained on massive amounts of text data, sometimes encompassing large portions
    of the entire publicly available text on the internet.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，即大型语言模型，是一种旨在理解、生成和响应类人文本的神经网络。这些模型是深度神经网络，经过大量文本数据的训练，有时涵盖了互联网上大量的公开文本。
- en: The "large" in large language model refers to both the model's size in terms
    of parameters and the immense dataset on which it's trained. Models like this
    often have tens or even hundreds of billions of parameters, which are the adjustable
    weights in the network that are optimized during training to predict the next
    word in a sequence. Next-word prediction is sensible because it harnesses the
    inherent sequential nature of language to train models on understanding context,
    structure, and relationships within text. Yet, it is a very simple task and so
    it is surprising to many researchers that it can produce such capable models.
    We will discuss and implement the next-word training procedure in later chapters
    step by step.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “大”语言模型中的“大”指的是模型在参数上的大小以及其训练所需的庞大数据集。像这样的模型通常有数十亿甚至数百亿的参数，这些参数是在训练过程中优化的可调权重，用于预测序列中的下一个词。下一个词的预测是合理的，因为它利用了语言固有的顺序特性来训练模型理解文本中的上下文、结构和关系。然而，这是一项非常简单的任务，因此许多研究人员对此能生成如此强大的模型感到惊讶。我们将在后面的章节中逐步讨论并实施下一个词的训练程序。
- en: LLMs utilize an architecture called the *transformer* (covered in more detail
    in section 1.4), which allows them to pay selective attention to different parts
    of the input when making predictions, making them especially adept at handling
    the nuances and complexities of human language.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs利用一种称为*transformer*的架构（在1.4节中有更详细的介绍），使它们在进行预测时能够对输入的不同部分进行选择性关注，从而特别擅长处理人类语言的细微差别和复杂性。
- en: Since LLMs are capable of *generating* text, LLMs are also often referred to
    as a form of generative artificial intelligence (AI), often abbreviated as *generative
    AI* or *GenAI*. As illustrated in Figure 1.1, AI encompasses the broader field
    of creating machines that can perform tasks requiring human-like intelligence,
    including understanding language, recognizing patterns, and making decisions,
    and includes subfields like machine learning and deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs能够*生成*文本，因此它们通常被称为生成式人工智能（AI）的一个形式，常缩写为*生成式AI*或*GenAI*。如图1.1所示，AI涵盖了创建能够执行人类智能所需任务的机器的广泛领域，包括理解语言、识别模式和做出决策，且包括机器学习和深度学习等子领域。
- en: Figure 1.1 As this hierarchical depiction of the relationship between the different
    fields suggests, LLMs represent a specific application of deep learning techniques,
    leveraging their ability to process and generate human-like text. Deep learning
    is a specialized branch of machine learning that focuses on using multi-layer
    neural networks. And machine learning and deep learning are fields aimed at implementing
    algorithms that enable computers to learn from data and perform tasks that typically
    require human intelligence.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 正如这个不同领域之间关系的层次化描述所暗示的，LLMs代表了深度学习技术的特定应用，利用它们处理和生成类人文本的能力。深度学习是机器学习的一个专门分支，专注于使用多层神经网络。而机器学习和深度学习都是旨在实现能够让计算机从数据中学习并执行通常需要人类智能的任务的算法的领域。
- en: '![](images/01__image001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image001.png)'
- en: The algorithms used to implement AI are the focus of the field of machine learning.
    Specifically, machine learning involves the development of algorithms that can
    learn from and make predictions or decisions based on data without being explicitly
    programmed. To illustrate this, imagine a spam filter as a practical application
    of machine learning. Instead of manually writing rules to identify spam emails,
    a machine learning algorithm is fed examples of emails labeled as spam and legitimate
    emails. By minimizing the error in its predictions on a training dataset, the
    model then learns to recognize patterns and characteristics indicative of spam,
    enabling it to classify new emails as either spam or legitimate.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现AI的算法是机器学习领域的重点。具体来说，机器学习涉及开发能够从数据中学习并根据数据做出预测或决策的算法，而无需明确编程。为了说明这一点，可以想象垃圾邮件过滤器作为机器学习的一个实际应用。机器学习算法会接收被标记为垃圾邮件和合法邮件的邮件示例，而不是手动编写规则来识别垃圾邮件。通过最小化在训练数据集上的预测误差，模型能够学习识别表明垃圾邮件的模式和特征，从而将新邮件分类为垃圾邮件或合法邮件。
- en: As illustrated in Figure 1.1, deep learning is a subset of machine learning
    that focuses on utilizing neural networks with three or more layers (also called
    deep neural networks) to model complex patterns and abstractions in data. In contrast
    to deep learning, traditional machine learning requires manual feature extraction.
    This means that human experts need to identify and select the most relevant features
    for the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如图1.1所示，深度学习是机器学习的一个子集，专注于利用三层或更多层的神经网络（也称为深度神经网络）来建模数据中的复杂模式和抽象。与深度学习相对，传统机器学习需要手动特征提取。这意味着人类专家需要识别和选择模型的最相关特征。
- en: While the field of AI is nowadays dominated by machine learning and deep learning,
    it also includes other approaches, for example, using rule-based systems, genetic
    algorithms, expert systems, fuzzy logic, or symbolic reasoning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如今AI领域主要由机器学习和深度学习主导，但它还包括其他方法，例如基于规则的系统、遗传算法、专家系统、模糊逻辑或符号推理。
- en: Returning to the spam classification example, in traditional machine learning,
    human experts might manually extract features from email text such as the frequency
    of certain trigger words ("prize," "win," "free"), the number of exclamation marks,
    use of all uppercase words, or the presence of suspicious links. This dataset,
    created based on these expert-defined features, would then be used to train the
    model. In contrast to traditional machine learning, deep learning does not require
    manual feature extraction. This means that human experts do not need to identify
    and select the most relevant features for a deep learning model. (However, in
    both traditional machine learning and deep learning for spam classification, you
    still require the collection of labels, such as spam or non-spam, which need to
    be gathered either by an expert or users.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 回到垃圾邮件分类的例子，在传统机器学习中，人类专家可能会手动从邮件文本中提取特征，例如某些触发词（“奖品”、“赢”、“免费”的频率）、感叹号的数量、全大写字母的使用或可疑链接的存在。根据这些专家定义的特征创建的数据集将被用于训练模型。与传统机器学习相比，深度学习不需要手动特征提取。这意味着人类专家不需要为深度学习模型识别和选择最相关的特征。（然而，在垃圾邮件分类的传统机器学习和深度学习中，你仍然需要收集标签，如垃圾邮件或非垃圾邮件，这些标签需要由专家或用户收集。）
- en: The upcoming sections will cover some of the problems LLMs can solve today,
    the challenges that LLMs address, and the general LLM architecture, which we will
    implement in this book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将讨论LLM今天能解决的一些问题、LLM所面临的挑战，以及我们将在本书中实现的LLM的基本架构。
- en: 1.2 Applications of LLMs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 LLM的应用
- en: Owing to their advanced capabilities to parse and understand unstructured text
    data, LLMs have a broad range of applications across various domains. Today, LLMs
    are employed for machine translation, generation of novel texts (see Figure 1.2),
    sentiment analysis, text summarization, and many other tasks. LLMs have recently
    been used for content creation, such as writing fiction, articles, and even computer
    code.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其解析和理解非结构化文本数据的先进能力，LLM在各个领域具有广泛的应用。今天，LLM被用于机器翻译、生成新文本（见图1.2）、情感分析、文本摘要及许多其他任务。最近，LLM也被用于内容创作，如写小说、文章，甚至计算机代码。
- en: Figure 1.2 LLM interfaces enable natural language communication between users
    and AI systems. This screenshot shows ChatGPT writing a poem according to a user's
    specifications.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 LLM接口使用户与AI系统之间能够进行自然语言交流。此屏幕截图显示了ChatGPT根据用户的要求写诗。
- en: '![](images/01__image003.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image003.png)'
- en: LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI's
    ChatGPT or Google's Gemini (formerly called Bard), which can answer user queries
    and augment traditional search engines such as Google Search or Microsoft Bing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLM还可以支持复杂的聊天机器人和虚拟助手，例如OpenAI的ChatGPT或谷歌的Gemini（以前称为Bard），这些助手能够回答用户查询，并增强传统搜索引擎如谷歌搜索或微软必应的功能。
- en: Moreover, LLMs may be used for effective knowledge retrieval from vast volumes
    of text in specialized areas such as medicine or law. This includes sifting through
    documents, summarizing lengthy passages, and answering technical questions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs可用于从大量专业领域文本中有效地检索知识，例如医学或法律。这包括筛选文档、总结冗长段落以及回答技术性问题。
- en: In short, LLMs are invaluable for automating almost any task that involves parsing
    and generating text. Their applications are virtually endless, and as we continue
    to innovate and explore new ways to use these models, it's clear that LLMs have
    the potential to redefine our relationship with technology, making it more conversational,
    intuitive, and accessible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LLMs对于自动化几乎任何涉及解析和生成文本的任务都是不可或缺的。它们的应用几乎是无穷无尽的，随着我们不断创新并探索使用这些模型的新方法，显然LLMs有潜力重新定义我们与技术的关系，使其变得更加对话式、直观和易于接触。
- en: In this book, we will focus on understanding how LLMs work from the ground up,
    coding an LLM that can generate texts. We will also learn about techniques that
    allow LLMs to carry out queries, ranging from answering questions to summarizing
    text, translating text into different languages, and more. In other words, in
    this book, we will learn how complex LLM assistants such as ChatGPT work by building
    one step by step.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将重点理解LLMs如何从零开始工作，编写一个能够生成文本的LLM。我们还将学习使LLMs能够执行查询的技术，从回答问题到总结文本、将文本翻译成不同语言等。换句话说，在本书中，我们将通过逐步构建一个复杂的LLM助手，如ChatGPT，来了解其工作原理。
- en: 1.3 Stages of building and using LLMs
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 构建和使用LLMs的阶段
- en: Why should we build our own LLMs? Coding an LLM from the ground up is an excellent
    exercise to understand its mechanics and limitations. Also, it equips us with
    the required knowledge for pretraining or finetuning existing open-source LLM
    architectures to our own domain-specific datasets or tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要构建自己的LLMs？从零开始编写LLM是一个很好的练习，可以帮助我们理解其机制和局限性。此外，这也为我们提供了预训练或微调现有开源LLM架构到我们特定领域的数据集或任务所需的知识。
- en: Research has shown that when it comes to modeling performance, custom-built
    LLMs—those tailored for specific tasks or domains—can outperform general-purpose
    LLMs, such as those provided by ChatGPT, which are designed for a wide array of
    applications. Examples of this include BloombergGPT, which is specialized for
    finance, and LLMs that are tailored for medical question answering (please see
    the *Further Reading and References* section in Appendix B for more details).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，在建模性能方面，定制的LLMs——那些针对特定任务或领域量身定制的LLMs——可以超越通用的LLMs，例如ChatGPT提供的那些，后者是为广泛应用而设计的。这方面的例子包括专注于金融的BloombergGPT，以及专门针对医学问答的LLMs（请参见附录B中的*进一步阅读与参考资料*部分以获取更多细节）。
- en: The general process of creating an LLM includes pretraining and finetuning.
    The term "pre" in "pretraining" refers to the initial phase where a model like
    an LLM is trained on a large, diverse dataset to develop a broad understanding
    of language. This pretrained model then serves as a foundational resource that
    can be further refined through finetuning, a process where the model is specifically
    trained on a narrower dataset that is more specific to particular tasks or domains.
    This two-stage training approach consisting of pretraining and finetuning is depicted
    in Figure 1.3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 创建LLM的一般过程包括预训练和微调。“预训练”中的“预”指的是模型（如LLM）在一个大型、多样化的数据集上进行训练的初始阶段，以发展对语言的广泛理解。这个预训练的模型随后作为基础资源，可以通过微调进一步精炼，微调的过程是指模型在一个更狭窄且特定于特定任务或领域的数据集上进行专门训练。这种包括预训练和微调的两阶段训练方法如图1.3所示。
- en: Figure 1.3 Pretraining an LLM involves next-word prediction on large text datasets.
    A pretrained LLM can then be finetuned using a smaller labeled dataset.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.3 预训练 LLM 涉及对大型文本数据集进行下一个词预测。预训练的 LLM 可以使用较小的标注数据集进行微调。
- en: '![](images/01__image005.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image005.png)'
- en: As illustrated in Figure 1.3, the first step in creating an LLM is to train
    it in on a large corpus of text data, sometimes referred to as *raw* text. Here,
    "raw" refers to the fact that this data is just regular text without any labeling
    information[[1]](#_ftn1). (Filtering may be applied, such as removing formatting
    characters or documents in unknown languages.)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 1.3 所示，创建 LLM 的第一步是在一个大型文本数据集上进行训练，有时称为 *原始* 文本。在这里，“原始”指的是这些数据仅为常规文本，没有任何标注信息[[1]](#_ftn1)。
    （可能会进行过滤，例如去除格式字符或未知语言的文档。）
- en: This first training stage of an LLM is also known as *pretraining*, creating
    an initial pretrained LLM, often called a *base* or *foundation* *model*. A typical
    example of such a model is the GPT-3 model (the precursor of the original model
    offered in ChatGPT). This model is capable of text completion, that is, finishing
    a half-written sentence provided by a user. It also has limited few-shot capabilities,
    which means it can learn to perform new tasks based on only a few examples instead
    of needing extensive training data. This is further illustrated in the next section*,
    Using transformers for different tasks*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的第一阶段训练也被称为 *预训练*，创建一个初始的预训练 LLM，通常称为 *基础* 或 *基础* *模型*。这种模型的一个典型例子是 GPT-3
    模型（ChatGPT 中提供的原始模型的前身）。该模型能够完成文本，即完成用户提供的半句话。它还具有限的少量示例能力，这意味着它可以根据仅有的几个示例学习执行新任务，而无需大量的训练数据。下一节*使用变换器进行不同任务*对此进行了进一步说明。
- en: After obtaining a *pretrained* LLM from training on large text datasets, where
    the LLM is trained to predict the next word in the text, we can further train
    the LLM on labeled data, also known as *finetuning*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过大型文本数据集训练获得 *预训练* LLM 后，该 LLM 被训练以预测文本中的下一个词，我们可以在标注数据上进一步训练 LLM，这也被称为 *微调*。
- en: The two most popular categories of finetuning LLMs include *instruction-finetuning*
    and finetuning for *classification* tasks. In instruction-finetuning, the labeled
    dataset consists of instruction and answer pairs, such as a query to translate
    a text accompanied by the correctly translated text. In classification finetuning,
    the labeled dataset consists of texts and associated class labels, for example,
    emails associated with *spam* and *non-spam* labels.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 LLM 的两个最流行的类别包括 *指令微调* 和 *分类* 任务的微调。在指令微调中，标注数据集由指令和答案对组成，例如翻译文本的查询及其对应的正确翻译文本。在分类微调中，标注数据集由文本及相关类别标签组成，例如与
    *垃圾邮件* 和 *非垃圾邮件* 标签相关的电子邮件。
- en: In this book, we will cover both code implementations for pretraining and finetuning
    LLM, and we will delve deeper into the specifics of instruction-finetuning and
    finetuning for classification later in this book after pretraining a base LLM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将涵盖预训练和微调 LLM 的代码实现，并将在本书后面深入探讨指令微调和分类微调的具体细节。
- en: 1.4 Using LLMs for different tasks
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 使用 LLM 进行不同任务
- en: Most modern LLMs rely on the *transformer* architecture, which is a deep neural
    network architecture introduced in the 2017 paper *Attention Is All You Need*.
    To understand LLMs we briefly have to go over the original transformer, which
    was originally developed for machine translation, translating English texts to
    German and French. A simplified version of the transformer architecture is depicted
    in Figure 1.4.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代 LLM 依赖于 *变换器* 架构，这是一种在 2017 年的论文 *注意力是你所需要的一切* 中提出的深度神经网络架构。为了理解 LLM，我们需要简要回顾原始的变换器，该变换器最初是为机器翻译开发的，用于将英语文本翻译成德语和法语。变换器架构的简化版本在图
    1.4 中进行了描述。
- en: Figure 1.4 A simplified depiction of the original transformer architecture,
    which is a deep learning model for language translation. The transformer consists
    of two parts, an encoder that processes the input text and produces an embedding
    representation (a numerical representation that captures many different factors
    in different dimensions) of the text that the decoder can use to generate the
    translated text one word at a time. Note that this figure shows the final stage
    of the translation process where the decoder has to generate only the final word
    ("Beispiel"), given the original input text ("This is an example") and a partially
    translated sentence ("Das ist ein"), to complete the translation.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 原始变压器架构的简化示意图，这是一个用于语言翻译的深度学习模型。变压器由两部分组成，一个编码器处理输入文本并生成嵌入表示（这是一个捕捉不同维度中多种因素的数值表示），解码器可以利用这些表示逐字生成翻译文本。请注意，这幅图展示了翻译过程的最终阶段，解码器仅需根据原始输入文本（“This
    is an example”）和部分翻译句子（“Das ist ein”）生成最终单词（“Beispiel”），以完成翻译。
- en: '![](images/01__image007.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image007.png)'
- en: The transformer architecture depicted in Figure 1.4 consists of two submodules,
    an encoder and a decoder. The encoder module processes the input text and encodes
    it into a series of numerical representations or vectors that capture the contextual
    information of the input. Then, the decoder module takes these encoded vectors
    and generates the output text from them. In a translation task, for example, the
    encoder would encode the text from the source language into vectors, and the decoder
    would decode these vectors to generate text in the target language. Both the encoder
    and decoder consist of many layers connected by a so-called self-attention mechanism.
    You may have many questions regarding how the inputs are preprocessed and encoded.
    These will be addressed in a step-by-step implementation in the subsequent chapters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4所示的变压器架构由两个子模块组成，编码器和解码器。编码器模块处理输入文本并将其编码为一系列捕捉输入上下文信息的数值表示或向量。然后，解码器模块利用这些编码向量生成输出文本。在翻译任务中，例如，编码器会将源语言的文本编码为向量，而解码器则会解码这些向量生成目标语言的文本。编码器和解码器都由许多层通过所谓的自注意力机制连接。你可能会对输入如何预处理和编码有很多问题。这些将在后续章节中逐步实现。
- en: A key component of transformers and LLMs is the self-attention mechanism (not
    shown), which allows the model to weigh the importance of different words or tokens
    in a sequence relative to each other. This mechanism enables the model to capture
    long-range dependencies and contextual relationships within the input data, enhancing
    its ability to generate coherent and contextually relevant output. However, due
    to its complexity, we will defer the explanation to chapter 3, where we will discuss
    and implement it step by step. Moreover, we will also discuss and implement the
    data preprocessing steps to create the model inputs in *chapter 2, Working with
    Text Data*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器和大型语言模型的一个关键组件是自注意力机制（未显示），它允许模型相对地权衡序列中不同单词或标记的重要性。该机制使模型能够捕捉输入数据中的长程依赖关系和上下文关系，从而增强其生成连贯且上下文相关输出的能力。然而，由于其复杂性，我们将在第3章推迟对此的解释，在那里我们将逐步讨论和实现。此外，我们还将在*第2章：处理文本数据*中讨论和实现数据预处理步骤，以创建模型输入。
- en: Later variants of the transformer architecture, such as the so-called BERT (short
    for *bidirectional encoder representations from transformers*) and the various
    GPT models (short for *generative pretrained transformers*), built on this concept
    to adapt this architecture for different tasks. (References can be found in Appendix
    B.)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的变种变压器架构，例如所谓的BERT（即*双向编码器表示来自变压器*）和各种GPT模型（即*生成式预训练变压器*），在此概念基础上对架构进行了适应，以用于不同的任务。（参考文献见附录B。）
- en: BERT, which is built upon the original transformer's encoder submodule, differs
    in its training approach from GPT. While GPT is designed for generative tasks,
    BERT and its variants specialize in masked word prediction, where the model predicts
    masked or hidden words in a given sentence as illustrated in Figure 1.5\. This
    unique training strategy equips BERT with strengths in text classification tasks,
    including sentiment prediction and document categorization. As an application
    of its capabilities, as of this writing, Twitter uses BERT to detect toxic content.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BERT建立在原始变换器的编码子模块之上，其训练方法与GPT有所不同。虽然GPT旨在用于生成任务，但BERT及其变体专注于被遮蔽的单词预测，模型在给定句子中预测被遮蔽或隐藏的单词，如图1.5所示。这种独特的训练策略使BERT在文本分类任务中具有优势，包括情感预测和文档分类。作为其能力的应用，截至目前，Twitter使用BERT来检测有害内容。
- en: Figure 1.5 A visual representation of the transformer's encoder and decoder
    submodules. On the left, the encoder segment exemplifies BERT-like LLMs, which
    focus on masked word prediction and are primarily used for tasks like text classification.
    On the right, the decoder segment showcases GPT-like LLMs, designed for generative
    tasks and producing coherent text sequences.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5 变换器的编码器和解码器子模块的可视化表示。左侧，编码器部分示例了类似BERT的LLM，这些模型专注于被遮蔽的单词预测，主要用于文本分类等任务。右侧，解码器部分展示了类似GPT的LLM，设计用于生成任务并生成连贯的文本序列。
- en: '![](images/01__image009.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image009.png)'
- en: GPT, on the other hand, focuses on the decoder portion of the original transformer
    architecture and is designed for tasks that require generating texts. This includes
    machine translation, text summarization, fiction writing, writing computer code,
    and more. We will discuss the GPT architecture in more detail in the remaining
    sections of this chapter and implement it from scratch in this book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GPT则专注于原始变换器架构的解码器部分，旨在用于需要生成文本的任务。这包括机器翻译、文本摘要、虚构写作、编写计算机代码等。我们将在本章的后续部分更详细地讨论GPT架构，并在本书中从头实现它。
- en: GPT models, primarily designed and trained to perform text completion tasks,
    also show remarkable versatility in their capabilities. These models are adept
    at executing both zero-shot and few-shot learning tasks. Zero-shot learning refers
    to the ability to generalize to completely unseen tasks without any prior specific
    examples. On the other hand, few-shot learning involves learning from a minimal
    number of examples the user provides as input, as shown in Figure 1.6.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型主要设计和训练用于执行文本完成任务，但在能力上也展现了显著的多样性。这些模型擅长执行零样本学习和少量样本学习任务。零样本学习是指在没有任何先前特定示例的情况下，对完全未见过的任务进行泛化。另一方面，少量样本学习涉及从用户提供的少量示例中进行学习，如图1.6所示。
- en: Figure 1.6 In addition to text completion, GPT-like LLMs can solve various tasks
    based on their inputs without needing retraining, finetuning, or task-specific
    model architecture changes. Sometimes, it is helpful to provide examples of the
    target within the input, which is known as a few-shot setting. However, GPT-like
    LLMs are also capable of carrying out tasks without a specific example, which
    is called zero-shot setting.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6 除了文本完成，类似GPT的LLM还可以根据输入解决各种任务，而无需重新训练、微调或更改特定任务的模型架构。有时，在输入中提供目标的示例是有帮助的，这被称为少量样本设置。然而，类似GPT的LLM也能够在没有具体示例的情况下执行任务，这被称为零样本设置。
- en: '![](images/01__image011.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image011.png)'
- en: Transformers versus LLMs
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 变换器与LLM
- en: Today's LLMs are based on the transformer architecture introduced in the previous
    section. Hence, transformers and LLMs are terms that are often used synonymously
    in the literature. However, note that not all transformers are LLMs since transformers
    can also be used for computer vision. Also, not all LLMs are transformers, as
    there are large language models based on recurrent and convolutional architectures.
    The main motivation behind these alternative approaches is to improve the computational
    efficiency of LLMs. However, whether these alternative LLM architectures can compete
    with the capabilities of transformer-based LLMs and whether they are going to
    be adopted in practice remains to be seen. (Interested readers can find literature
    references describing these architectures in the *Further Reading* section at
    the end of this chapter.)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的 LLM 基于上一节介绍的变换器架构。因此，变换器和 LLM 在文献中常被用作同义词。然而，请注意，并非所有变换器都是 LLM，因为变换器也可以用于计算机视觉。同时，并非所有
    LLM 都是变换器，因为有些大型语言模型基于递归和卷积架构。这些替代方法的主要动机是提高 LLM 的计算效率。然而，这些替代 LLM 架构是否能够与基于变换器的
    LLM 的能力竞争，以及它们是否会在实践中被采用，还有待观察。（有兴趣的读者可以在本章末尾的*进一步阅读*部分找到描述这些架构的文献参考。）
- en: 1.5 Utilizing large datasets
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 利用大型数据集
- en: The large training datasets for popular GPT- and BERT-like models represent
    diverse and comprehensive text corpora encompassing billions of words, which include
    a vast array of topics and natural and computer languages. To provide a concrete
    example, Table 1.1 summarizes the dataset used for pretraining GPT-3, which served
    as the base model for the first version of ChatGPT.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的 GPT 和 BERT 类模型的大型训练数据集代表了涵盖数十亿单词的多样且全面的文本语料库，其中包括广泛的主题以及自然语言和计算机语言。为了提供一个具体示例，表
    1.1 总结了用于预训练 GPT-3 的数据集，该模型作为第一版 ChatGPT 的基础模型。
- en: Table 1.1 The pretraining dataset of the popular GPT-3 LLM
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 1.1 流行的 GPT-3 LLM 的预训练数据集
- en: '| Dataset name | Dataset description | Number of tokens | Proportion in  training
    data |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 数据集描述 | 标记数量 | 训练数据中的比例 |'
- en: '| CommonCrawl (filtered) | Web crawl data | 410 billion | 60% |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| CommonCrawl（过滤） | 网页抓取数据 | 4100 亿 | 60% |'
- en: '| WebText2 | Web crawl data | 19 billion | 22% |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| WebText2 | 网页抓取数据 | 190 亿 | 22% |'
- en: '| Books1 | Internet-based book corpus | 12 billion | 8% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 书籍1 | 基于互联网的书籍语料库 | 120 亿 | 8% |'
- en: '| Books2 | Internet-based book corpus | 55 billion | 8% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 书籍2 | 基于互联网的书籍语料库 | 550 亿 | 8% |'
- en: '| Wikipedia | High-quality text | 3 billion | 3% |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科 | 高质量文本 | 30 亿 | 3% |'
- en: Table 1.1 reports the number of tokens, where a token is a unit of text that
    a model reads, and the number of tokens in a dataset is roughly equivalent to
    the number of words and punctuation characters in the text. We will cover tokenization,
    the process of converting text into tokens, in more detail in the next chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 报告了标记的数量，其中标记是模型读取的文本单元，数据集中的标记数量大致等同于文本中的单词和标点符号的数量。我们将在下一章中更详细地探讨标记化，即将文本转换为标记的过程。
- en: The main takeaway is that the scale and diversity of this training dataset allows
    these models to perform well on diverse tasks including language syntax, semantics,
    and context, and even some requiring general knowledge.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结论是，这个训练数据集的规模和多样性使这些模型能够在包括语言句法、语义和上下文在内的多种任务上表现良好，甚至还包括一些需要一般知识的任务。
- en: GPT-3 dataset details
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-3 数据集详情
- en: In Table 1.1, it's important to note that from each dataset, only a fraction
    of the data, (amounting to a total of 300 billion tokens) was used in the training
    process. This sampling approach means that the training didn't encompass every
    single piece of data available in each dataset. Instead, a selected subset of
    300 billion tokens, drawn from all datasets combined, was utilized. Also, while
    some datasets were not entirely covered in this subset, others might have been
    included multiple times to reach the total count of 300 billion tokens. The column
    indicating proportions in the table, when summed up without considering rounding
    errors, accounts for 100% of this sampled data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 1.1 中，重要的是要注意，从每个数据集中，仅有一小部分数据（总计 3000 亿个标记）用于训练过程。这种抽样方法意味着训练并没有涵盖每个数据集中可用的每一条数据。相反，利用了从所有数据集中组合而成的选定子集——3000
    亿个标记。此外，尽管某些数据集在此子集中并未完全覆盖，其他数据集可能被多次包含，以达到 3000 亿个标记的总数。表中表示比例的列在未考虑四舍五入误差的情况下，总和占据了
    100% 的抽样数据。
- en: For context, consider the size of the CommonCrawl dataset, which alone consists
    of 410 billion tokens and requires about 570 GB of storage. In comparison, later
    iterations of models like GPT-3, such as Meta's LLaMA, have expanded their training
    scope to include additional data sources like Arxiv research papers (92 GB) and
    StackExchange's code-related Q&As (78 GB).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供背景，请考虑CommonCrawl数据集的规模，该数据集本身由4100亿个标记组成，所需存储约为570 GB。相比之下，后续版本的模型如GPT-3，例如Meta的LLaMA，已扩展其训练范围，包含额外的数据源，如Arxiv研究论文（92
    GB）和StackExchange的代码相关问答（78 GB）。
- en: The *Wikipedia* corpus consists of *English-language Wikipedia. While the authors
    of the GPT-3 paper didn't further specify the details,* Books1 is likely a sample
    from Project Gutenberg ([https://www.gutenberg.org/](www.gutenberg.org.html)),
    and *Books*2 is likely from *Libge*n ([https://en.wikipedia.org/wiki/Library_Genesis](wiki.html)).
    *CommonCrawl* is a filtered subset of the CommonCrawl database ([https://commoncrawl.org/](commoncrawl.org.html)),
    and *WebText2* is the text of web pages from all outbound Reddit links from posts
    with 3+ upvotes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*Wikipedia*语料库由*英语维基百科*组成。虽然GPT-3论文的作者没有进一步指定细节，*Books1*很可能是来自古腾堡计划（[https://www.gutenberg.org/](www.gutenberg.org.html)）的样本，*Books2*很可能来自*Libgen*（[https://en.wikipedia.org/wiki/Library_Genesis](wiki.html)）。*CommonCrawl*是CommonCrawl数据库的一个过滤子集（[https://commoncrawl.org/](commoncrawl.org.html)），而*WebText2*则是来自所有3个以上点赞的Reddit帖子中所有外部链接的网页文本。'
- en: The authors of the GPT-3 paper did not share the training dataset but a comparable
    dataset that is publicly available is *The Pile* ([https://pile.eleuther.ai/](pile.eleuther.ai.html)).
    However, the collection may contain copyrighted works, and the exact usage terms
    may depend on the intended use case and country. For more information, see the
    HackerNews discussion at [https://news.ycombinator.com/item?id=25607809](news.ycombinator.com.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3论文的作者没有分享训练数据集，但一个可以公开获取的相似数据集是*The Pile*（[https://pile.eleuther.ai/](pile.eleuther.ai.html)）。然而，该集合可能包含受版权保护的作品，确切的使用条款可能取决于预期用途和国家。有关更多信息，请参见HackerNews的讨论，链接为[https://news.ycombinator.com/item?id=25607809](news.ycombinator.com.html)。
- en: The pretrained nature of these models makes them incredibly versatile for further
    finetuning on downstream tasks, which is why they are also known as base or foundation
    models. Pretraining LLMs requires access to significant resources and is very
    expensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million
    in terms of cloud computing credits[[2]](#_ftn2).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的预训练特性使其在下游任务中极为灵活，这也是它们被称为基础模型的原因。预训练大型语言模型需要访问大量资源，并且成本非常高。例如，GPT-3的预训练成本估计为460万美元的云计算信用[[2]](#_ftn2)。
- en: The good news is that many pretrained LLMs, available as open-source models,
    can be used as general purpose tools to write, extract, and edit texts that were
    not part of the training data. Also, LLMs can be finetuned on specific tasks with
    relatively smaller datasets, reducing the computational resources needed and improving
    performance on the specific task.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，许多作为开源模型提供的预训练大型语言模型，可以作为通用工具来编写、提取和编辑不在训练数据中的文本。此外，LLM可以在相对较小的数据集上进行微调，从而减少所需的计算资源并提高特定任务的性能。
- en: In this book, we will implement the code for pretraining and use it to pretrain
    an LLM for educational purposes. All computations will be executable on consumer
    hardware. After implementing the pretraining code we will learn how to reuse openly
    available model weights and load them into the architecture we will implement,
    allowing us to skip the expensive pretraining stage when we finetune LLMs later
    in this book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将实现预训练代码，并利用它来为教育目的预训练大型语言模型。所有计算将在消费硬件上可执行。实现预训练代码后，我们将学习如何重用公开可用的模型权重，并将其加载到我们将实现的架构中，从而在稍后的章节中微调大型语言模型时跳过昂贵的预训练阶段。
- en: 1.6 A closer look at the GPT architecture
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 更深入地了解GPT架构
- en: 'Previously in this chapter, we mentioned the terms GPT-like models, GPT-3,
    and ChatGPT. Let''s now take a closer look at the general GPT architecture. First,
    GPT stands for ***G***enerative ***P***retrained ***T***ransformer and was originally
    introduced in the following paper:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前，我们提到过GPT类模型、GPT-3和ChatGPT的术语。现在让我们更深入地了解一般的GPT架构。首先，GPT代表***G***enerative
    ***P***retrained ***T***ransformer，最初是在以下论文中介绍的：
- en: '*Improving Language Understanding by Generative Pre-Training* (2018) by *Radford
    et al.* from OpenAI, [http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](language-unsupervised.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过生成预训练改善语言理解*（2018），*Radford等*，来自OpenAI，[http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](language-unsupervised.html)'
- en: GPT-3 is a scaled-up version of this model that has more parameters and was
    trained on a larger dataset. And the original model offered in ChatGPT was created
    by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's
    InstructGPT paper, which we will cover in more detail in *chapter 7, Finetuning
    with Human Feedback To Follow Instructions*. As we have seen earlier in Figure
    1.6, these models are competent text completion models and can carry out other
    tasks such as spelling correction, classification, or language translation. This
    is actually very remarkable given that GPT models are pretrained on a relatively
    simple next-word prediction task, as illustrated in Figure 1.7.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是该模型的扩展版本，具有更多参数，并在更大的数据集上进行训练。而在ChatGPT中提供的原始模型是通过在一个大型指令数据集上微调GPT-3来创建的，采用了OpenAI的InstructGPT论文中的方法，我们将在*第7章，利用人类反馈进行微调*中更详细地讨论。正如我们之前在图1.6中所看到的，这些模型是高效的文本补全模型，并可以执行拼写纠正、分类或语言翻译等其他任务。这实际上非常值得注意，因为GPT模型是在相对简单的下一个词预测任务上进行预训练的，如图1.7所示。
- en: Figure 1.7 In the next-word pretraining task for GPT models, the system learns
    to predict the upcoming word in a sentence by looking at the words that have come
    before it. This approach helps the model understand how words and phrases typically
    fit together in language, forming a foundation that can be applied to various
    other tasks.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7 在GPT模型的下一个词预训练任务中，系统通过查看之前的单词来预测句子中的下一个单词。这种方法帮助模型理解单词和短语在语言中通常是如何组合在一起的，形成了可以应用于各种其他任务的基础。
- en: '![](images/01__image013.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image013.png)'
- en: 'The next-word prediction task is a form of self-supervised learning, which
    is a form of self-labeling. This means that we don''t need to collect labels for
    the training data explicitly but can leverage the structure of the data itself:
    we can use the next word in a sentence or document as the label that the model
    is supposed to predict. Since this next-word prediction task allows us to create
    labels "on the fly," it is possible to leverage massive unlabeled text datasets
    to train LLMs as previously discussed in section *1.5, Utilizing large datasets*.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个词预测任务是一种自我监督学习形式，这是一种自我标注的形式。这意味着我们不需要明确收集训练数据的标签，而是可以利用数据本身的结构：我们可以使用句子或文档中的下一个词作为模型要预测的标签。由于这个下一个词预测任务使我们能够“即时”创建标签，因此可以利用大量未标记的文本数据集来训练LLM，正如之前在*1.5节，利用大型数据集*中讨论的那样。
- en: Compared to the original transformer architecture we covered in section 1.4,
    *Using LLMs for different tasks*, the general GPT architecture is relatively simple.
    Essentially, it's just the decoder part without the encoder as illustrated in
    Figure 1.8\. Since decoder-style models like GPT generate text by predicting text
    one word at a time, they are considered a type of *autoregressive* model. Autoregressive
    models incorporate their previous outputs as inputs for future predictions. Consequently,
    in GPT, each new word is chosen based on the sequence that precedes it, which
    improves coherence of the resulting text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在1.4节*使用LLMs进行不同任务*中讨论的原始变压器架构相比，通用的GPT架构相对简单。本质上，它只是没有编码器的解码器部分，如图1.8所示。由于像GPT这样的解码器风格模型通过逐个预测文本生成文本，因此它们被视为一种*自回归*模型。自回归模型将其先前的输出作为未来预测的输入。因此，在GPT中，每个新单词是根据其前面的序列选择的，这提高了生成文本的一致性。
- en: Architectures such as GPT-3 are also significantly larger than the original
    transformer model. For instance, the original transformer repeated the encoder
    and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion
    parameters in total.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3这样的架构也显著大于原始的变压器模型。例如，原始的变压器重复了编码器和解码器块六次。GPT-3总共有96个变压器层和1750亿个参数。
- en: Figure 1.8 The GPT architecture employs only the decoder portion of the original
    transformer. It is designed for unidirectional, left-to-right processing, making
    it well-suited for text generation and next-word prediction tasks to generate
    text in iterative fashion one word at a time.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8 GPT架构仅使用了原始变换器的解码器部分。它被设计为单向的、从左到右处理，非常适合文本生成和下一个单词预测任务，以逐词迭代的方式生成文本。
- en: '![](images/01__image015.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image015.png)'
- en: GPT-3 was introduced in 2020, which, by the standards of deep learning and large
    language model (LLM) development, is considered a long time ago. However, more
    recent architectures, such as Meta's Llama models, are still based on the same
    underlying concepts, introducing only minor modifications. Hence, understanding
    GPT remains as relevant as ever, and this book focuses on implementing the prominent
    architecture behind GPT while providing pointers to specific tweaks employed by
    alternative LLMs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3于2020年推出，按照深度学习和大型语言模型（LLM）发展的标准来看，这已经是很久以前的事了。然而，像Meta的Llama模型这样的较新架构仍然基于相同的基本概念，仅进行了小幅修改。因此，理解GPT依然十分相关，而本书的重点是实现GPT背后的显著架构，同时提供其他LLM所采用的具体调整的指引。
- en: Lastly, it's interesting to note that although the original transformer model
    was explicitly designed for language translation, GPT models—despite their larger
    yet simpler architecture aimed at next-word prediction—are also capable of performing
    translation tasks. This capability was initially unexpected to researchers, as
    it emerged from a model primarily trained on a next-word prediction task, which
    is a task that did not specifically target translation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有趣的是，尽管原始的变换器模型是专门为语言翻译设计的，但GPT模型——尽管其较大但更简单的架构旨在预测下一个单词——同样能够执行翻译任务。研究人员最初并未预料到这一点，因为这一能力源于一个主要训练于下一个单词预测任务的模型，而该任务并没有特别针对翻译。
- en: The ability to perform tasks that the model wasn't explicitly trained to perform
    is called an "emergent behavior." This capability isn't explicitly taught during
    training but emerges as a natural consequence of the model's exposure to vast
    quantities of multilingual data in diverse contexts. The fact that GPT models
    can "learn" the translation patterns between languages and perform translation
    tasks even though they weren't specifically trained for it demonstrates the benefits
    and capabilities of these large-scale, generative language models. We can perform
    diverse tasks without using diverse models for each.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 执行模型没有明确训练的任务的能力被称为“涌现行为”。这种能力在训练期间并未明确教授，而是作为模型在多种上下文中接触大量多语言数据的自然结果而出现。GPT模型能够“学习”语言之间的翻译模式并执行翻译任务，尽管它们并未专门为此训练，体现了这些大规模生成语言模型的好处和能力。我们可以在不使用多种模型的情况下执行多样化的任务。
- en: 1.7 Building a large language model
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 构建大型语言模型
- en: In this chapter, we laid the groundwork for understanding LLMs. In the remainder
    of this book, we will be coding one from scratch. We will take the fundamental
    idea behind GPT as a blueprint and tackle this in three stages, as outlined in
    Figure 1.9.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们为理解LLM奠定了基础。在本书的剩余部分，我们将从零开始编码一个。我们将把GPT背后的基本理念作为蓝图，并按照图1.9中概述的三个阶段进行处理。
- en: Figure 1.9 The stages of building LLMs covered in this book include implementing
    the LLM architecture and data preparation process, pretraining an LLM to create
    a foundation model, and finetuning the foundation model to become a personal assistant
    or text classifier.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9 本书中涉及的构建LLM的阶段包括实现LLM架构和数据准备过程，预训练LLM以创建基础模型，以及微调基础模型以成为个人助手或文本分类器。
- en: '![](images/01__image017.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](images/01__image017.png)'
- en: First, we will learn about the fundamental data preprocessing steps and code
    the attention mechanism that is at the heart of every LLM.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习基本的数据预处理步骤，并编写每个LLM核心的注意力机制代码。
- en: Next, in stage 2, we will learn how to code and pretrain a GPT-like LLM capable
    of generating new texts. And we will also go over the fundamentals of evaluating
    LLMs, which is essential for developing capable NLP systems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在第二阶段，我们将学习如何编码和预训练一个类似GPT的LLM，能够生成新文本。同时，我们还将讨论评估LLM的基本原理，这对开发有能力的NLP系统至关重要。
- en: Note that pretraining a large LLM from scratch is a significant endeavor, demanding
    thousands to millions of dollars in computing costs for GPT-like models. Therefore,
    the focus of stage 2 is on implementing training for educational purposes using
    a small dataset. In addition, the book will also provide code examples for loading
    openly available model weights.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从头开始对大型语言模型（LLM）进行预训练是一项重大工作，GPT 类模型的计算成本可能高达数千到数百万美元。因此，第二阶段的重点是使用小型数据集实施教育目的的训练。此外，本书还将提供加载开放可用模型权重的代码示例。
- en: Finally, in stage 3, we will take a pretrained LLM and finetune it to follow
    instructions such as answering queries or classifying texts -- the most common
    tasks in many real-world applications and research.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第三阶段，我们将采用一个预训练的 LLM，并对其进行微调，以遵循指令，如回答查询或分类文本——这是许多实际应用和研究中最常见的任务。
- en: I hope you are looking forward to embarking on this exciting journey!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你期待开始这段令人兴奋的旅程！
- en: 1.8 Summary
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 摘要
- en: LLMs have transformed the field of natural language processing, which previously
    mostly relied on explicit rule-based systems and simpler statistical methods.
    The advent of LLMs introduced new deep learning-driven approaches that led to
    advancements in understanding, generating, and translating human language.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 已经改变了自然语言处理领域，之前主要依赖于显式的基于规则的系统和更简单的统计方法。LLM 的出现引入了新的深度学习驱动的方法，推动了对人类语言的理解、生成和翻译的进步。
- en: Modern LLMs are trained in two main steps.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代 LLM 的训练主要分为两个步骤。
- en: First, they are pretrained on a large corpus of unlabeled text by using the
    prediction of the next word in a sentence as a "label."
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它们在一个大型未标记文本语料库上进行预训练，通过使用句子中下一个单词的预测作为“标签”。
- en: Then, they are finetuned on a smaller, labeled target dataset to follow instructions
    or perform classification tasks.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它们在一个较小的标记目标数据集上进行微调，以遵循指令或执行分类任务。
- en: LLMs are based on the transformer architecture. The key idea of the transformer
    architecture is an attention mechanism that gives the LLM selective access to
    the whole input sequence when generating the output one word at a time.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 基于变压器架构。变压器架构的关键理念是注意机制，使 LLM 在逐字生成输出时能够选择性地访问整个输入序列。
- en: The original transformer architecture consists of an encoder for parsing text
    and a decoder for generating text.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的变压器架构由一个用于解析文本的编码器和一个用于生成文本的解码器组成。
- en: LLMs for generating text and following instructions, such as GPT-3 and ChatGPT,
    only implement decoder modules, simplifying the architecture.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于生成文本和遵循指令的 LLM，如 GPT-3 和 ChatGPT，仅实现解码器模块，从而简化了架构。
- en: Large datasets consisting of billions of words are essential for pretraining
    LLMs. In this book, we will implement and train LLMs on small datasets for educational
    purposes but also see how we can load openly available model weights.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由数十亿单词组成的大型数据集对 LLM 的预训练至关重要。在本书中，我们将实现并在小型数据集上训练 LLM，以实现教育目的，同时也会展示如何加载开放可用的模型权重。
- en: While the general pretraining task for GPT-like models is to predict the next
    word in a sentence, these LLMs exhibit "emergent" properties such as capabilities
    to classify, translate, or summarize texts.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然类似 GPT 的模型的一般预训练任务是预测句子中的下一个单词，但这些 LLM 展现出“突现”特性，例如分类、翻译或总结文本的能力。
- en: Once an LLM is pretrained, the resulting foundation model can be finetuned more
    efficiently for various downstream tasks.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦 LLM 预训练完成，得到的基础模型可以更高效地针对各种下游任务进行微调。
- en: LLMs finetuned on custom datasets can outperform general LLMs on specific tasks.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在定制数据集上微调的 LLM 在特定任务上可以超越通用 LLM。
- en: '[[1]](#_ftnref1) Readers with a background in machine learning may note that
    labeling information is typically required for traditional machine learning models
    and deep neural networks trained via the conventional supervised learning paradigm.
    However, this is not the case for the pretraining stage of LLMs. In this phase,
    LLMs leverage self-supervised learning, where the model generates its own labels
    from the input data. This concept is covered later in this chapter'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_ftnref1) 具有机器学习背景的读者可能注意到，传统机器学习模型和通过常规监督学习范式训练的深度神经网络通常需要标记信息。然而，这并不是
    LLM 预训练阶段的情况。在这个阶段，LLM 利用自监督学习，模型从输入数据中生成自己的标签。这一概念将在本章后面进行介绍。'
- en: '[[2]](#_ftnref2) *GPT-3, The $4,600,000 Language Model*, [https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_](h0jwoz.html)[4600000_language_model/](d_gpt3_the_4600000_language_model.html)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_ftnref2) *GPT-3，$4,600,000语言模型*，[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_](h0jwoz.html)[4600000_language_model/](d_gpt3_the_4600000_language_model.html)'
