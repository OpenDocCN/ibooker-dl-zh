- en: 1 Understanding Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 理解大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章包括
- en: High-level explanations of the fundamental concepts behind large language models
    (LLMs)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）背后的基本概念的高层次解释
- en: Insights into the transformer architecture from which ChatGPT-like LLMs are
    derived
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索ChatGPT类LLM源自的Transformer架构的深层次解释
- en: A plan for building an LLM from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建LLM的计划
- en: Large language models (LLMs) like ChatGPT are deep neural network models developed
    over the last few years. They ushered in a new era for Natural Language Processing
    (NLP). Before the advent of large language models, traditional methods excelled
    at categorization tasks such as email spam classification and straightforward
    pattern recognition that could be captured with handcrafted rules or simpler models.
    However, they typically underperformed in language tasks that demanded complex
    understanding and generation abilities, such as parsing detailed instructions,
    conducting contextual analysis, or creating coherent and contextually appropriate
    original text. For example, previous generations of language models could not
    write an email from a list of keywords—a task that is trivial for contemporary
    LLMs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 像ChatGPT这样的大型语言模型（LLM）是在过去几年中开发的深度神经网络模型。它们引领了自然语言处理（NLP）的新时代。在大型语言模型出现之前，传统方法擅长于分类任务，如电子邮件垃圾分类和可以通过手工制作的规则或简单模型捕获的简单模式识别。然而，在需要复杂理解和生成能力的语言任务方面，例如解析详细说明、进行上下文分析或创建连贯且上下文适当的原始文本时，它们通常表现不佳。例如，以前的语言模型无法根据关键字列表编写电子邮件-这对于当代LLM来说是微不足道的任务。
- en: LLMs have remarkable capabilities to understand, generate, and interpret human
    language. However, it's important to clarify that when we say language models
    "understand," we mean that they can process and generate text in ways that appear
    coherent and contextually relevant, not that they possess human-like consciousness
    or comprehension.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LLM具有出色的理解、生成和解释人类语言的能力。然而，重要的是澄清，当我们说语言模型“理解”时，我们指的是它们可以以看起来连贯和上下文相关的方式处理和生成文本，而不是它们具有类似人类的意识或理解能力。
- en: Enabled by advancements in deep learning, which is a subset of machine learning
    and artificial intelligence (AI) focused on neural networks, LLMs are trained
    on vast quantities of text data. This allows LLMs to capture deeper contextual
    information and subtleties of human language compared to previous approaches.
    As a result, LLMs have significantly improved performance in a wide range of NLP
    tasks, including text translation, sentiment analysis, question answering, and
    many more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的推动下，LLM受益于大量文本数据的训练。这使得LLM能够捕获比以前更深层次的语境信息和人类语言的微妙之处。因此，LLM在各种NLP任务中的性能显着提高，包括文本翻译、情感分析、问答等等。
- en: Another important distinction between contemporary LLMs and earlier NLP models
    is that the latter were typically designed for specific tasks; whereas those earlier
    NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency
    across a wide range of NLP tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当代LLM与早期NLP模型之间的另一个重要区别是，后者通常是为特定任务而设计的；而早期的NLP模型在其狭窄应用中表现出色，LLM则在各种NLP任务中展示出更广泛的熟练程度。
- en: The success behind LLMs can be attributed to the transformer architecture which
    underpins many LLMs, and the vast amounts of data LLMs are trained on, allowing
    them to capture a wide variety of linguistic nuances, contexts, and patterns that
    would be challenging to manually encode.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM背后的成功归功于Transformer架构，该架构支撑了许多LLM，并且LLM训练的大量数据，使它们能够捕捉到各种语言细微差别、语境和模式，这些模式是难以手工编码的。
- en: This shift towards implementing models based on the transformer architecture
    and using large training datasets to train LLMs has fundamentally transformed
    NLP, providing more capable tools for understanding and interacting with human
    language.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型基于Transformer架构实现，并使用大型训练数据集来训练LLM的这一转变，从根本上改变了NLP，为理解和与人类语言交互提供了更有能力的工具。
- en: 'Beginning with this chapter, we set the foundation to accomplish the primary
    objective of this book: understanding LLMs by implementing a ChatGPT-like LLM
    based on the transformer architecture step by step in code.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们为实现本书的主要目标奠定基础：通过逐步在代码中实现基于Transformer架构的ChatGPT样式LLM来理解LLM。
- en: 1.1 What is an LLM?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 什么是LLM？
- en: An LLM, a large language model, is a neural network designed to understand,
    generate, and respond to human-like text. These models are deep neural networks
    trained on massive amounts of text data, sometimes encompassing large portions
    of the entire publicly available text on the internet.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，即大型语言模型，是一种设计用于理解、生成和回应类似人类文本的神经网络。这些模型是在大量文本数据上训练的深度神经网络，有时包括互联网上整个可公开获取文本的大部分内容。
- en: The "large" in large language model refers to both the model's size in terms
    of parameters and the immense dataset on which it's trained. Models like this
    often have tens or even hundreds of billions of parameters, which are the adjustable
    weights in the network that are optimized during training to predict the next
    word in a sequence. Next-word prediction is sensible because it harnesses the
    inherent sequential nature of language to train models on understanding context,
    structure, and relationships within text. Yet, it is a very simple task and so
    it issurprising to many researchers that it can produce such capable models. We
    will discuss and implement the next-word training procedure in later chapters
    step by step.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"大型"语言模型中的"大"既指模型在参数方面的规模，也指其所训练的庞大数据集。这样的模型通常具有数百亿甚至数百亿个参数，这些参数是网络中的可调权重，在训练过程中进行优化，以预测序列中的下一个词。下一个词的预测是合理的，因为它利用了语言固有的顺序性质来训练模型，使其理解文本中的上下文、结构和关系。然而，这是一个非常简单的任务，许多研究人员会感到惊讶的是，它能够产生如此有能力的模型。我们将在后续章节逐步讨论并实施下一个词的训练过程。'
- en: LLMs utilize an architecture called the *transformer* (covered in more detail
    in section 1.4), which allows them to pay selective attention to different parts
    of the input when making predictions, making them especially adept at handling
    the nuances and complexities of human language.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs利用一种称为*transformer*的架构（在第1.4节中更详细地介绍），使它们在进行预测时能够有选择地关注输入的不同部分，使其特别擅长处理人类语言的细微差别和复杂性。
- en: Since LLMs are capable of *generating* text, LLMs are also often referred to
    as a form of generative artificial intelligence (AI), often abbreviated as *generative
    AI* or *GenAI*. As illustrated in figure 1.1, AI encompasses the broader field
    of creating machines that can perform tasks requiring human-like intelligence,
    including understanding language, recognizing patterns, and making decisions,
    and includes subfields like machine learning and deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs能够*生成*文本，所以LLMs也经常被称为一种生成人工智能(AI)的形式，通常简称为*生成AI*或*GenAI*。如图1.1所示，AI涵盖了创建能够执行需要类似人类智能的任务的机器的更广泛领域，包括理解语言、识别模式和做决策，并包括诸如机器学习和深度学习之类的子领域。
- en: Figure 1.1 As this hierarchical depiction of the relationship between the different
    fields suggests, LLMs represent a specific application of deep learning techniques,
    leveraging their ability to process and generate human-like text. Deep learning
    is a specialized branch of machine learning that focuses on using multi-layer
    neural networks. And machine learning and deep learning are fields aimed at implementing
    algorithms that enable computers to learn from data and perform tasks that typically
    require human intelligence. The field of artificial intelligence is nowadays dominated
    by machine learning and deep learning but it also includes other approaches, for
    example by using rule-based systems, genetic algorithms, expert systems, fuzzy
    logic, or symbolic reasoning.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 正如这一层次化的关系图所示，LLMs代表了深度学习技术的一种特定应用，利用它们处理和生成类似人类文本的能力。深度学习是一种专门的机器学习分支，专注于使用多层神经网络。机器学习和深度学习是旨在实现使计算机能够从数据中学习并执行通常需要人类智能的任务的算法领域。人工智能领域如今被机器学习和深度学习主导，但也包括其他方法，例如使用基于规则的系统、遗传算法、专家系统、模糊逻辑或符号推理。
- en: '![](images/ch-01__image002.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1](images/ch-01__image002.png)'
- en: The algorithms used to implement AI are the focus of the field of machine learning.
    Specifically, machine learning involves the development of algorithms that can
    learn from and make predictions or decisions based on data without being explicitly
    programmed. To illustrate this, imagine a spam filter as a practical application
    of machine learning. Instead of manually writing rules to identify spam emails,
    a machine learning algorithm is fed examples of emails labeled as spam and legitimate
    emails. By minimizing theerror in its predictions on a training dataset, the model
    then learns to recognize patterns and characteristics indicative of spam, enabling
    it to classify new emails as either spam or legitimate.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现人工智能的算法是机器学习领域的重点。具体而言，机器学习涉及开发可以从数据中学习并基于数据进行预测或决策而无需明确编程的算法。为了说明这一点，可以将垃圾邮件过滤器作为机器学习的实际应用。与手动编写规则来识别垃圾邮件不同，机器学习算法将被提供标记为垃圾邮件和合法邮件的示例。通过在训练数据集上最小化其预测错误，模型可以学习识别与垃圾邮件相关的模式和特征，从而能够将新邮件分类为垃圾邮件或合法邮件。
- en: Deep learning is a subset of machine learning that focuses on utilizing neural
    networks with three or more layers (also called deep neural networks) to model
    complex patterns and abstractions in data. In contrast to deep learning, traditional
    machine learning requires manual feature extraction. This means that human experts
    need to identify and select the most relevant features for the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，专注于利用有三层或更多层的神经网络（也称为深度神经网络）来对数据中的复杂模式和抽象进行建模。与深度学习相反，传统机器学习需要手动提取特征。这意味着人类专家需要识别和选择对模型最相关的特征。
- en: Returning to the spam classification example, in traditional machine learning,
    human experts might manually extract features from email text such as the frequency
    of certain trigger words ("prize," "win," "free"), the number of exclamation marks,
    use of all uppercase words, or the presence of suspicious links. This dataset,
    created based on these expert-defined features, would then be used to train the
    model. In contrast to traditional machine learning, deep learning does not require
    manual feature extraction. This means that human experts do not need to identify
    and select the most relevant features for a deep learning model
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾垃圾邮件分类的例子，在传统机器学习中，人类专家可能会从电子邮件文本中手动提取特征，例如特定触发词（"prize"，"win"，"free"）的频率，感叹号的数量，使用全大写单词或怀疑链接的存在。基于这些专家定义的特征创建的数据集将用于训练模型。与传统机器学习相比，深度学习不需要手动提取特征。这意味着人类专家不需要为深度学习模型识别和选择最相关的特征。
- en: The upcoming sections will cover some of the problems LLMs can solve today,
    the challenges that LLMs address, and the general LLM architecture, which we will
    implement in this book.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将涵盖LLM（大型语言模型）今天可以解决的一些问题，LLM解决的挑战，以及我们将在本书中实现的通用LLM架构。
- en: 1.2 Applications of LLMs
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 LLM的应用
- en: Owing to their advanced capabilities to parse and understand unstructured text
    data, LLMs have a broad range of applications across various domains. Today, LLMs
    are employed for machine translation, generation of novel texts (see figure 1.2),
    sentiment analysis, text summarization, and many other tasks. LLMs have recently
    been used for content creation, such as writing fiction, articles, and even computer
    code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM具有解析和理解非结构化文本数据的高级能力，LLM在各个领域都有广泛的应用。目前，LLM被应用于机器翻译、生成新颖文本（参见图1.2）、情感分析、文本摘要和许多其他任务。LLM最近还用于内容创作，如写小说、文章甚至是计算机代码。
- en: Figure 1.2 LLM interfaces enable natural language communication between users
    and AI systems. This screenshot shows ChatGPT writing a poem that according to
    a user's specifications.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 LLM接口实现了用户和人工智能系统之间的自然语言交流。该截图显示ChatGPT根据用户的规格要求写诗。
- en: '![](images/ch-01__image004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image004.png)'
- en: LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI's
    ChatGPT or Google's Bard, which can answer user queries and augment traditional
    search engines such as Google Search or Microsoft Bing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM还可以为复杂的聊天机器人和虚拟助手提供动力，例如OpenAI的ChatGPT或Google的Bard，它们可以回答用户提问并增强传统搜索引擎（如Google
    Search或Microsoft Bing）。
- en: Moreover, LLMs may be used for effective knowledge retrieval from vast volumes
    of text in specialized areas such as medicine or law. This includes sifting through
    documents, summarizing lengthy passages, and answering technical questions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM可以用于有效地从专业领域的大量文本中检索知识，如医学或法律。这包括筛选文件、总结长篇文章和回答技术问题。
- en: In short, LLMs are invaluable for automating almost any task that involves parsing
    and generating text. Their applications are virtually endless, and as we continue
    to innovate and explore new ways to use these models, it's clear that LLMs have
    the potential to redefine our relationship with technology, making it more conversational,
    intuitive, and accessible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LLM对于自动化几乎任何涉及解析和生成文本的任务都是无价的。它们的应用几乎是无限的，随着我们不断创新和探索使用这些模型的新方法，很明显，LLM有潜力重新定义我们与技术的关系，使其更具对话性、直观和可访问性。
- en: In this book, we will focus on understanding how LLMs work from the ground up,
    coding an LLM that can generate texts. We will also learn about techniques that
    allow LLMs to carry out queries, ranging from answering questions to summarizing
    text, translating text into different languages, and more. In other words, in
    this book, we will learn how complex LLM assistants such as ChatGPT work by building
    one step by step.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将重点关注从零开始理解LLM（大型语言模型）的工作原理，编写一个能够生成文本的LLM。我们还将学习一些技术，使LLM能够进行各种查询，从回答问题到总结文本、将文本翻译成不同语言等等。换句话说，在本书中，我们将通过逐步构建一个LLM来了解复杂的LLM助手（如ChatGPT）是如何工作的。
- en: 1.3 Stages of building and using LLMs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 构建和使用LLM的阶段
- en: Why should we build our own LLMs? Coding an LLM from the ground up is an excellent
    exercise to understand its mechanics and limitations. Also, it equips us with
    the required knowledge for pretaining or finetuning existing open-source LLM architectures
    to our own domain-specific datasets or tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要构建自己的LLM呢？从零开始编写一个LLM是一个很好的练习，可以理解其机制和局限性。此外，这使我们具备了必要的知识，可以对现有的开源LLM架构进行预训练或微调，以适应我们自己的领域特定数据集或任务。
- en: Research has shown that when it comes to modeling performance, custom-built
    LLMs—those tailored for specific tasks or domains—can outperform general-purpose
    LLMs like ChatGPT, which are designed for a wide array of applications. Examples
    of this include BloombergGPT, which is specialized for finance, and LLMs that
    are tailored for medical question answering (please see the *Further Reading and
    References* section at the end of this chapter for more details).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，就建模性能而言，定制的LLM——针对特定任务或领域定制的LLM——可能会优于ChatGPT等通用LLM，后者设计用于广泛的应用。其中的例子包括专门用于金融领域的BloombergGPT，以及专门用于医学问题回答的LLM（请参阅本章末尾的*进一步阅读和参考*部分了解更多细节）。
- en: The general process of creating an LLM, including pretraining and finetuning.
    The term "pre" in "pretraining" refers to the initial phase where a model like
    an LLM is trained on a large, diverse dataset to develop a broad understanding
    of language. This pretrained model then serves as a foundational resource that
    can be further refined through finetuning, a process where the model is specifically
    trained on a narrower dataset that is more specific to particular tasks or domains.
    This two-stage training approach consisting of pretraining and finetuning is depicted
    in figure 1.3.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 创建LLM的一般过程，包括预训练和微调。在“预训练”中的“pre”一词指的是初始阶段，其中像LLM这样的模型在大型、多样的数据集上进行训练，以开发对语言的广泛理解。然后，这个预训练模型作为一个基础资源，可以通过微调进一步完善，微调是指模型在更具体于特定任务或领域的较窄数据集上进行专门训练的过程。图1.3展示了这个包括预训练和微调的两阶段训练方法。
- en: Figure 1.3 Pretraining an LLM involves next-word prediction on large unlabeled
    text corpora (raw text). A pretrained LLM can then be finetuned using a smaller
    labeled dataset.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3 对LLM进行预训练包括对大型未标记文本语料库（原始文本）进行下一个词预测。然后，可以使用较小的标记数据集对预训练的LLM进行微调。
- en: '![](images/ch-01__image006.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image006.png)'
- en: As illustrated in figure 1.3, the first step in creating an LLM is to train
    it in on a large corpus of text data, sometimes referred to as *raw* text. Here,
    "raw" refers to the fact that this data is just regular text without any labeling
    information[[1]](#_ftn1). (Filtering may be applied, such as removing formatting
    characters or documents in unknown languages.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如图1.3所示，创建LLM的第一步是对大量文本数据进行训练，有时被称为*原始*文本。这里，“原始”指的是这些数据只是普通文本，没有任何标记信息[[1]](#_ftn1)。（可能会应用过滤，如删除格式字符或未知语言的文档。）
- en: This first training stage of an LLM is also known as *pretraining*, creating
    an initial pretrained LLM, often called a *bas*e or *foundation* *model*. A typical
    example of such a model is the GPT-3 model (the precursor of ChatGPT). This model
    is capable of text completion, that is, finishing a half-written sentence provided
    by a user. It also has limited few-shot capabilities, which means it can learn
    to perform new tasks based on only a few examples instead of needing extensive
    training data. This is further illustrated in the next section*, Using transformers
    for different tasks*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的第一个训练阶段也称为*预训练*，创建一个初始的预训练LLM，通常称为*基础*模型或*基础*模型。这种模型的典型例子是GPT-3模型（ChatGPT的前身）。该模型能够完成文本，即完成用户提供的半写句。它还具有有限的few-shot能力，这意味着它可以根据少量示例学习执行新任务，而不需要大量训练数据。这在接下来的部分*为不同任务使用变换器*中进一步阐述。
- en: After obtaining a *pretrained* LLM from training on unlabeled texts, we can
    further train the LLM on labeled data, also known as *finetuning*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从在未标记文本上训练的*预训练*LLM中获得之后，我们可以进一步在标记数据上训练LLM，也称为*微调*。
- en: The two most popular categories of finetuning LLMs include *instruction-finetuning*
    and finetuning for *classification* tasks. In instruction-finetuning, the labeled
    dataset consists of instruction and answer pairs, such as a query to translate
    a text accompanied by the correctly translated text. In classification finetuning,
    the labeled dataset consists of texts and associated class labels, for example,
    emails associated with *spam* and *non-spa*m labels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调LLM的两个最流行的类别包括*指导微调*和用于*分类*任务的微调。在指导微调中，标记的数据集包括指导和答案对，例如需要翻译文本的查询和正确翻译文本。在分类微调中，标记的数据集包括文本和相关的类别标签，例如与*垃圾邮件*和*非垃圾邮件*标签相关联的电子邮件。
- en: In this book, we will cover both code implementations for pretraining and finetuning
    LLM, and we will delve deeper into the specifics of instruction-finetuning and
    finetuning for classification later in this book after pretraining a base LLM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将涵盖预训练和微调LLM的代码实现，并且我们会更深入地研究指导微调和分类微调的具体内容，这将在本书中在预训练基础LLM后进行。
- en: 1.4 Using LLMs for different tasks
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 为不同任务使用LLM
- en: Most modern LLMs rely on the *transformer* architecture, which is a deep neural
    network architecture introduced in the 2017 paper *Attention Is All You Need*.
    To understand LLMs we briefly have to go over the original transformer, which
    was originally developed for machine translation, translating English texts to
    German and French. A simplified version of the transformer architecture is depicted
    in figure 1.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代LLM依赖*变换器*架构，这是一种深度神经网络架构，首次引入于2017年的论文*Attention Is All You Need*。要理解LLM，我们需要简要回顾原始变换器，它最初用于机器翻译，将英文文本翻译成德语和法语。图1.4描述了变换器架构的简化版本。
- en: Figure 1.4 A simplified depiction of the original transformer architecture,
    which is a deep learning model for language translation. The transformer consists
    of two parts, an encoder that processes the input text and produces an embedding
    representation (a numerical representation that captures many different factors
    in different dimensions) of the text that the decoder can use to generate the
    translated text one word at a time. Note that this figure shows the final stage
    of the translation process where the decoder has to generate only the final word
    ("Beispiel"), given the original input text ("This is an example") and a partially
    translated sentence ("Das ist ein"), to complete the translation. The figure numbering
    indicates the sequence in which the data is processed and provides guidance on
    the optimal order to read the figure.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 原始变换器架构的简化描述，这是一个用于语言翻译的深度学习模型。变换器由两部分组成，一个处理输入文本并生成嵌入表示的编码器（捕捉许多不同因素在不同维度中的数字表示）和一个解码器，后者可以使用该表示来逐字生成翻译文本。请注意，该图显示了翻译过程的最终阶段，其中解码器只需生成最终单词（"Beispiel"），给定原始输入文本（"This
    is an example"）和部分翻译的句子（"Das ist ein"），以完成翻译。图中的编号指示数据处理的顺序，并提供有关最佳阅读图的指导。
- en: '![](images/ch-01__image008.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image008.png)'
- en: The transformer architecture depicted in figure 1.4 consists of two submodules,
    an encoder and a decoder. The encoder module processes the input text and encodes
    it into a series of numerical representations or vectors that capture the contextual
    information of the input. Then, the decoder module takes these encoded vectors
    and generates the output text from them. In a translation task, for example, the
    encoder would encode the text from the source language into vectors, and the decoder
    would decode these vectors to generate text in the target language.. Both the
    encoder and decoder consist of many layers connected by a so-called self-attention
    mechanism. You may have many questions regarding how the inputs are preprocessed
    and encoded. These will be addressed in a step-by-step implementation in the subsequent
    chapters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 中描绘的 transformer 架构由两个子模块组成，一个编码器和一个解码器。编码器模块处理输入文本并将其编码为一系列捕捉输入上下文信息的数值表示或向量。然后，解码器模块会从这些编码向量中生成输出文本。例如，在翻译任务中，编码器会将源语言的文本编码成向量，解码器则会将这些向量解码为目标语言的文本。编码器和解码器都由许多层连接的所谓自注意机制组成。关于如何预处理和编码输入，你可能有很多问题。这些将在随后的章节中逐步实现中得到解答。
- en: A key component of transformers and LLMs is the self-attention mechanism (not
    shown), which allows the model to weigh the importance of different words or tokens
    in a sequence relative to each other. This mechanism enables the model to capture
    long-range dependencies and contextual relationships within the input data, enhancing
    its ability to generate coherent and contextually relevant output. However, due
    to its complexity, we will defer the explanation to Chapter 3, where we will discuss
    and implement it step by step. Moreover, we will also discuss and implement the
    data preprocessing steps to create the model inputs in *Chapter 2, Working with
    Text Data*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 和 LLMs 的关键组成部分是自注意机制（未显示），它允许模型权衡序列中不同单词或标记的重要性相对于彼此。这一机制使得模型能够捕捉长距离依赖和输入数据中的上下文关系，增强了生成连贯和有上下文相关性输出的能力。然而，由于其复杂性，我们将在第三章详细讨论并逐步实现这一解释。此外，我们还将在《第二章，处理文本数据》中讨论和实现数据预处理步骤来创建模型输入。
- en: Later variants of the transformer architecture, such as the so-called BERT (short
    for *bidirectional encoder representations from transformers*) and the various
    GPT models (short for *generative pretrained transformers*), built on this concept
    to adapt this architecture for different tasks. (References can be found in the
    *Further Reading* section at the end of this chapter.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的变种 transformer 架构，如所谓的 BERT（*双向编码器表示来自 transformer*）和各种 GPT 模型（*生成式预训练 transformer*），建立在这一概念上，以适应不同任务的体系结构。
    （参考可以在本章结束处的*进一步阅读*部分找到。）
- en: BERT, which is built upon the original transformer's encoder submodule, differs
    in its training approach from GPT. While GPT is designed for generative tasks,
    BERT and its variants specialize in masked word prediction, where the model predicts
    masked or hidden words in a given sentence as illustrated in figure 1.5\. This
    unique training strategy equips BERT with strengths in text classification tasks,
    including sentiment prediction and document categorization. As an application
    of its capabilities, as of this writing, Twitter uses BERT to detect toxic content.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BERT，它是建立在原始 transformer 编码器子模块基础之上的，与 GPT 在训练方法上有所不同。虽然 GPT 设计用于生成任务，BERT 及其变体专门用于掩码词预测，模型会预测给定句子中的掩码或隐藏单词，如图
    1.5 所示。这种独特的训练策略使得 BERT 在文本分类任务中具有优势，包括情绪预测和文档分类。截至目前，Twitter 使用 BERT 来检测有害内容，这是其能力的一个应用。
- en: Figure 1.5 A visual representation of the transformer's encoder and decoder
    submodules. On the left, the encoder segment exemplifies BERT-like LLMs, which
    focus on masked word prediction and are primarily used for tasks like text classification.
    On the right, the decoder segment showcases GPT-like LLMs, designed for generative
    tasks and producing coherent text sequences.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.5 transformer 的编码器和解码器子模块的可视化表示。在左侧，编码器部分举例说明了类似 BERT 的 LLMs，其专注于掩码词预测，主要用于文本分类等任务。在右侧，解码器部分展示了类似
    GPT 的 LLMs，设计用于生成任务并生成连贯的文本序列。
- en: '![](images/ch-01__image010.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image010.png)'
- en: GPT, on the other hand, focuses on the decoder portion of the original transformer
    architecture and is designed for tasks that require generating texts. This includes
    machine translation, text summarization, fiction writing, writing computer code,
    and more. We will discuss the GPT architecture in more detail in the remaining
    sections of this chapter and implement it from scratch in this book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPT侧重于原始变压器架构的解码器部分，旨在处理需要生成文本的任务。这包括机器翻译、文本摘要、虚构写作、编写计算机代码等。我们将在本章的其余部分更详细地讨论GPT架构，并在本书中从头开始实现它。
- en: GPT models, primarily designed and trained to perform text completion tasks,
    also show remarkable versatility in their capabilities. These models are adept
    at executing both zero-shot and few-shot learning tasks. Zero-shot learning refers
    to the ability to generalize to completely unseen tasks without any prior specific
    examples. On the other hand, few-shot learning involves learning from a minimal
    number of examples the user provides as input, as shown in figure 1.6.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型主要设计和训练用于执行文本完成任务，同时在其能力上显示出出色的多功能性。这些模型擅长执行零样本和少样本学习任务。零样本学习指的是在没有任何先前特定示例的情况下对完全不可见任务进行概括的能力。另一方面，少样本学习涉及从用户提供的最少数量示例中学习，如图1.6所示。
- en: Figure 1.6 Next to text completion, GPT-like LLMs can solve various tasks based
    on their inputs without needing retraining, finetuning, or task-specific model
    architecture changes. Sometimes, it is helpful to provide examples of the target
    within the input, which is known as a few-shot setting. However, GPT-like LLMs
    are also capable of carrying out tasks without a specific example, which is called
    zero-shot setting.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6 除了文本完成之外，类似GPT的LLM可以根据其输入解决各种任务，无需重新训练、微调或特定于任务的模型架构更改。有时，在输入中提供目标示例是有帮助的，这被称为少样本设置。然而，类似GPT的LLM也能够在没有具体示例的情况下执行任务，这被称为零样本设置。
- en: '![](images/ch-01__image012.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image012.png)'
- en: Transformers versus LLMs
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 变压器与LLM
- en: Today's LLMs are based on the transformer architecture introduced in the previous
    section. Hence, transformers and LLMs are terms that are often used synonymously
    in the literature. However, note that not all transformers are LLMs since transformers
    can also be used for computer vision. Also, not all LLMs are transformers, as
    there are large language models based on recurrent and convolutional architectures.
    The main motivation behind these alternative approaches is to improve the computational
    efficiency of LLMs. However, whether these alternative LLM architectures can compete
    with the capabilities of transformer-based LLMs and whether they are going to
    be adopted in practice remains to be seen. (Interested readers can find literature
    references describing these architectures in the *Further Reading* section at
    the end of this chapter.)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的LLM基于前面介绍的变压器架构。因此，在文献中常常将变压器和LLM等术语用作同义词。然而，需要注意的是，并非所有的变压器都是LLM，因为变压器也可以用于计算机视觉。另外，并非所有的LLM都是变压器，因为还有基于循环和卷积架构的大型语言模型。这些替代方法背后的主要动机是提高LLM的计算效率。然而，这些替代的LLM架构是否能与基于变压器的LLM的能力竞争，并且它们是否会被实际采用还有待观察。（感兴趣的读者可以在本章末尾的*进一步阅读*部分找到描述这些架构的文献引用。）
- en: 1.5 Utilizing large datasets
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 利用大型数据集
- en: The large training datasets for popular GPT- and BERT-like models represent
    diverse and comprehensive text corpora encompassing billions of words, which include
    a vast array of topics and natural and computer languages. To provide a concrete
    example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served
    as the base model for the first version of ChatGPT.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的GPT和BERT等模型的大型训练数据集包含数十亿字的多样化和全面的文本语料库，涵盖了大量主题以及自然语言和计算机语言。为了提供一个具体的例子，表1.1总结了用于预训练GPT-3的数据集，这为ChatGPT的第一个版本提供了基础模型。
- en: Table 1.1 The pretraining dataset of the popular GPT-3 LLM
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1 流行的GPT-3 LLM的预训练数据集
- en: '| Dataset name | Dataset description | Number of tokens | Proportion in training
    data |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 数据集描述 | 标记数量 | 在训练数据中的比例 |'
- en: '| CommonCrawl (filtered) | Web crawl data | 410 billion | 60% |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| CommonCrawl（经过过滤） | 网络爬虫数据 | 4100亿 | 60% |'
- en: '| WebText2 | Web crawl data | 19 billion | 22% |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 网页文本2 | 网络爬虫数据 | 190亿 | 22% |'
- en: '| Books1 | Internet-based book corpus | 12 billion | 8% |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 图书1 | 基于互联网的图书语料库 | 120亿 | 8% |'
- en: '| Books2 | Internet-based book corpus | 55 billion | 8% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 图书2 | 基于互联网的图书语料库 | 550亿 | 8% |'
- en: '| Wikipedia | High-quality text | 3 billion | 3% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科 | 高质量文本 | 30亿 | 3% |'
- en: Table 1.1 reports the number of tokens, where a token is a unit of text that
    a model reads, and the number of tokens in a dataset is roughly equivalent to
    the number of words and punctuation characters in the text. We will cover tokenization,
    the process of converting text into tokens, in more detail in the next chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1报告了标记的数量，其中一个标记是模型读取的文本单位，数据集中的标记数量大致相当于文本中的单词和标点符号的数量。我们将在下一章更详细地介绍标记化的过程，即将文本转换为标记的过程。
- en: The main takeaway is that the scale and diversity of this training dataset allows
    these models to perform well on diverse tasks including language syntax, semantics,
    and context, and even some requiring general knowledge.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的要点是这个训练数据集的规模和多样性，使这些模型在包括语言句法、语义和内容的各种任务上表现良好，甚至包括一些需要一般知识的任务。
- en: GPT-3 dataset details
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-3数据集详细信息
- en: Note that each subset in table 1.1 was sampled 300 billion tokens, which implies
    that not all datasets were seen completely, and some were seen multiple times.
    The proportion column, ignoring rounding, adds to 100%. For reference, the 410
    billion tokens in the CommonCrawl dataset require approximately 570 GB of storage.
    Later models based on GPT-3, for example, Meta's LLaMA, also include research
    papers from Arxiv (92 GB) and code-related Q&As from StackExchange (78 GB).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表1.1中的每个子集都是抽样自3000亿个标记，这意味着并非所有数据集都完全被看到，有些甚至被多次看到。除四舍五入之外，比例列加起来为100%。作为参考，CommonCrawl数据集中的4100亿个标记大约需要570GB的存储空间。基于GPT-3的后续模型，如Meta的LLaMA，还包括来自Arxiv的研究论文（92GB）和来自StackExchange的与代码相关的问答（78GB）。
- en: The Wikipedia corpus consists of English-language Wikipedia. While the authors
    of the GPT-3 paper didn't further specify the details, Books1 is likely a sample
    from Project Gutenberg ([https://www.gutenberg.org/](www.gutenberg.org.html)),
    and Books2 is likely from Libgen ([https://en.wikipedia.org/wiki/Library_Genesis](wiki.html)).
    CommonCrawl is a filtered subset of the CommonCrawl database ([https://commoncrawl.org/](commoncrawl.org.html)),
    and WebText2 is the text of web pages from all outbound Reddit links from posts
    with 3+ upvotes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科语料库由英语维基百科组成。虽然GPT-3论文的作者没有进一步说明细节，但Books1很可能是从古登堡计划([https://www.gutenberg.org/](www.gutenberg.org.html))中抽样而来，而Books2很可能是来自Libgen([https://en.wikipedia.org/wiki/Library_Genesis](wiki.html))。CommonCrawl是CommonCrawl数据库的筛选子集([https://commoncrawl.org/](commoncrawl.org.html))，而WebText2是来自帖子中出现过3个以上赞的Reddit链接的网页文本。
- en: The authors of the GPT-3 paper did not share the training dataset but a comparable
    dataset that is publicly available is The Pile ([https://pile.eleuther.ai/](pile.eleuther.ai.html)).
    However, the collection may contain copyrighted works, and the exact usage terms
    may depend on the intended use case and country. For more information, see the
    HackerNews discussion at [https://news.ycombinator.com/item?id=25607809](news.ycombinator.com.html).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3论文的作者没有公开训练数据集，但一个可比较的公开可用的数据集是The Pile([https://pile.eleuther.ai/](pile.eleuther.ai.html))。不过，这个收集可能包含有版权作品，而且确切的使用条款可能取决于使用案例和国家。有关更多信息，请参阅HackerNews上的讨论[https://news.ycombinator.com/item?id=25607809](news.ycombinator.com.html)。
- en: The pretrained nature of these models makes them incredibly versatile for further
    finetuning on downstream tasks, which is why they are also known as base or foundation
    models. Pretraining LLMs requires access to significant resources and is very
    expensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million
    in terms of cloud computing credits[[2]](#_ftn2).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的预训练性使它们在进一步微调下游任务时变得非常灵活，这也是它们被称为基础模型的原因。预训练LLM需要大量资源，并且代价非常高昂。例如，据估计GPT-3的预训练成本为460万美元的云计算费用[[2]](#_ftn2)。
- en: The good news is that many pretrained LLMs, available as open-source models,
    can be used as general purpose tools to write, extract, and edit texts that were
    not part of the training data. Also, LLMs can be finetuned on specific tasks with
    relatively smaller datasets, reducing the computational resources needed and improving
    performance on the specific task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，许多预训练的LLM模型可以作为通用工具用于写作、提取和编辑不属于训练数据的文本，并且这些模型也可以在相对较小的数据集上进行微调，以降低所需的计算资源，并且改善在特定任务上的性能。
- en: In this book, we will implement the code for pretraining and use it to pretrain
    an LLM for educational purposes.. All computations will be executable on consumer
    hardware. After implementing the pretraining code we will learn how to reuse openly
    available model weights and load them into the architecture we will implement,
    allowing us to skip the expensive pretraining stage when we finetune LLMs later
    in this book.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将实现用于预训练的代码，并将其用于教育目的。所有计算都可以在消费者硬件上执行。在实现预训练代码之后，我们将学习如何重用公开可用的模型权重，并将它们加载到我们将要实现的架构中，从而使我们能够在本书后期微调LLM时跳过昂贵的预训练阶段。
- en: 1.6 A closer look at the GPT architecture
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 更详细地了解GPT架构
- en: 'Previously in this chapter, we mentioned the terms GPT-like models, GPT-3,
    and ChatGPT. Let''s now take a closer look at the general GPT architecture. First,
    GPT stands for ***G***enerative ***P***retrained ***T***ransformer and was originally
    introduced in the following paper:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中之前，我们提到了类似GPT模型、GPT-3和ChatGPT的术语。现在让我们更仔细地看一下通用的GPT架构。首先，GPT代表***G***enerative
    ***P***retrained ***T***ransformer，最初是在以下论文中介绍的：
- en: '*Improving Language Understanding by Generative Pre-Training* (2018) by *Radford
    et al.* from OpenAI, [http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](language-unsupervised.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过生成预训练提高语言理解* (2018) 由OpenAI的*Radford等人*提出，[http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](language-unsupervised.html)'
- en: GPT-3 is a scaled-up version of this model that has more parameters and was
    trained on a larger dataset. And the original ChatGPT model was created by finetuning
    GPT-3 on a large instruction dataset using a method from OpenAI'sInstructGPT paper,
    which we will cover in more detail in *Chapter 8, Finetuning with Human Feedback
    To Follow Instructions*. As we have seen earlier in figure 1.6, these models are
    competent text completion models and can carry out other tasks such as spelling
    correction, classification, or language translation. This is actually very remarkable
    given that GPT models are pretrained on a relatively simple next-word prediction
    task, as illustrated in figure 1.7.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是该模型的一个规模扩大版本，具有更多的参数，并且是在一个更大的数据集上训练的。而最初的ChatGPT模型是通过对GPT-3在一个大型指导数据集上进行微调而创建的，使用了OpenAI的InstructGPT论文中的一种方法，我们将在*第8章，通过人类反馈微调以遵循指示*中详细介绍这种方法。正如我们在图1.6中早期看到的那样，这些模型是称职的文本补全模型，并且可以执行其他任务，比如拼写校正、分类或语言翻译。鉴于GPT模型是在一个相对简单的下一个单词预测任务上进行预训练的，正如图1.7所示，这实际上是非常了不起的。
- en: Figure 1.7 In the next-word pretraining task for GPT models, the system learns
    to predict the upcoming word in a sentence by looking at the words that have come
    before it. This approach helps the model understand how words and phrases typically
    fit together in language, forming a foundation that can be applied to various
    other tasks.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7 在GPT模型的下一个单词预训练任务中，系统通过查看之前出现过的单词来预测句子中即将出现的单词。这种方法有助于模型理解单词和短语在语言中通常是如何配合使用的，形成一个可以应用于各种其他任务的基础。
- en: '![](images/ch-01__image014.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image014.png)'
- en: 'The next-word prediction task is a form of self-supervised learning, which
    is a form of self-labeling. This means that we don''t need to collect labels for
    the training data explicitly but can leverage the structure of the data itself:
    we can use the next word in a sentence or document as the label that the model
    is supposed to predict. Since this next-word prediction task allows us to create
    labels "on the fly," it is possible to leverage massive unlabeled text datasets
    to train LLMs as previously discussed in section *1.5, Utilizing large datasets*.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单词预测任务是一种自监督学习形式，是一种自我标记形式。这意味着我们不需要显式地为训练数据收集标签，而是可以利用数据本身的结构：我们可以使用句子或文档中的下一个单词作为模型要预测的标签。由于这个下一个单词预测任务允许我们"即兴"地创建标签，所以可以利用大规模的未标记文本数据集来训练LLM，如前面第1.5节中所讨论的。
- en: Compared to the original transformer architecture we covered in section 1.4,
    *Using LLMs for different tasks*, the general GPT architecture is relatively simple.
    Essentially, it's just the decoder part without the encoder as illustrated in
    figure 1.8\. Since decoder-style models like GPT generate text by predicting text
    one word at a time, they are considered a type of autoregressive model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在第1.4节中介绍的原始Transformer架构相比，*使用LLM执行不同任务*，通用GPT架构相对简单。从本质上讲，它只是解码器部分，没有编码器，如图1.8所示。由于像GPT这样的解码器样式模型通过逐字预测文本生成文本，因此它们被认为是一种自回归模型。
- en: Architectures such as GPT-3 are also significantly larger than the original
    transformer model. For instance, the original transformer repeated the encoder
    and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion
    parameters in total.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如GPT-3之类的架构也比原始的transformer模型要大得多。例如，原始的transformer将编码器和解码器块重复六次。GPT-3共有96个transformer层和1750亿个参数。
- en: Figure 1.8 The GPT architecture, employs only the decoder portion of the original
    transformer. It is designed for unidirectional, left-to-right processing, making
    it well-suited for text generation and next-word prediction tasks to generate
    text in iterative fashion one word at a time.
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8 GPT架构仅使用原始transformer的解码器部分。它被设计为单向从左到右的处理，非常适合文本生成和下一个单词预测任务，以逐步生成一次一个单词的文本。
- en: '![](images/ch-01__image016.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image016.png)'
- en: GPT-3 was introduced in 2020, which is a long time ago by the standard of deep
    learning and LLM development, more recent architectures like Meta's Llama models
    are still based on the same underlying concepts, introducing only minor modifications.
    Hence, understanding GPT remains as relevant as ever, and this book focuses on
    implementing the prominent architecture behind GPT while providing pointers to
    specific tweaks employed by alternative LLMs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是在2020年推出的，从深度学习和LLM发展的标准来看，这已经是很久之前了，而像Meta的Llama模型这样更近期的架构仍然基于相同的基本概念，只是进行了一些细微的修改。因此，理解GPT仍然像以往一样重要，而本书侧重于实现GPT背后突出的架构，并提供指向替代LLMs使用的具体调整。
- en: Lastly, it's interesting to note that although the original transformer model
    was explicitly designed for language translation, GPT models—despite their larger
    yet simpler architecture aimed at next-word prediction—are also capable of performing
    translation tasks. This capability was initially unexpected to researchers, as
    it emerged from a model primarily trained on a next-word prediction task, which
    is a task that did not specifically target translation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有趣的是，尽管原始的transformer模型明确设计用于语言翻译，但GPT模型——尽管其更大但更简单的架构旨在进行下一个单词的预测——也能够执行翻译任务。这种能力最初对研究人员来说是意外的，因为它源自一个主要训练于下一个单词预测任务上的模型，而这是一个并不专门针对翻译的任务。
- en: The ability to perform tasks that the model wasn't explicitly trained to perform
    is called an "emerging property." This capability isn't explicitly taught during
    training but emerges as a natural consequence of the model's exposure to vast
    quantities of multilingual data in diverse contexts. The fact that GPT models
    can "learn" the translation patterns between languages and perform translation
    tasks even though they weren't specifically trained for it demonstrates the benefits
    and capabilities of these large-scale, generative language models. We can perform
    diverse tasks without using diverse models for each.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能够执行其未明确接受训练的任务称为“新兴属性”。这种能力在训练期间并未得到明确教导，但是作为模型暴露于各种多语言环境下的大量数据的自然结果而出现。事实上，GPT模型可以“学习”语言之间的翻译模式，并执行翻译任务，即使它们并没有针对此进行专门训练，这显示了这些大规模生成语言模型的优势和能力。我们可以执行各种任务而无需为每个任务使用不同的模型。
- en: 1.7 Building a large language model
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 构建大型语言模型
- en: In this chapter, we laid the groundwork for understanding LLMs. In the remainder
    of this book, we will be coding one from scratch. We will take the fundamental
    idea behind GPT as a blueprint and tackle this in three stages, as outlined in
    figure 1.9.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为理解LLMs奠定了基础。在本书的剩余部分中，我们将从头开始编写一个LLM。我们将以GPT背后的基本思想作为蓝本，并按照图1.9中的概述分三个阶段来解决这个问题。
- en: Figure 1.9 The stages of building LLMs covered in this book include implementing
    the LLM architecture and data preparation process, pretraining an LLM to create
    a foundation model, and finetuning the foundation model to become a personal assistant
    or text classifier.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9 本书涵盖的构建LLMs的阶段包括实现LLM架构和数据准备过程，预训练LLM以创建基础模型，以及对基础模型进行微调以成为个人助理或文本分类器。
- en: '![](images/ch-01__image018.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](images/ch-01__image018.png)'
- en: First, we will learn about the fundamental data preprocessing steps and code
    the attention mechanism that is at the heart of every LLM.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习基本的数据预处理步骤，并编写是每个LLM核心的注意力机制。
- en: Next, in stage 2, we will learn how to code and pretrain a GPT-like LLM capable
    of generating new texts. And we will also go over the fundamentals of evaluating
    LLMs, which is essential for developing capable NLP systems.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在第2阶段，我们将学习如何编码和预训练一个类似GPT的LLM，能够生成新的文本。并且我们还将深入研究评估LLMs的基础知识，这对于开发功能强大的自然语言处理系统至关重要。
- en: Note that pretraining a large LLM from scratch is a significant endeavor, demanding
    thousands to millions of dollars in computing costs for GPT-like models. Therefore,
    the focus of stage 2 is on implementing training for educational purposes using
    a small dataset. In addition, the book will also provide code examples for loading
    openly available model weights.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从零开始预训练大型LLM是一项重大工作，在GPT-like模型的计算成本中需要数千到数百万美元。因此，第2阶段的重点是利用小型数据集进行教育目的的训练实施。此外，本书还将提供加载公开可用模型权重的代码示例。
- en: Finally, in stage 3, we will take a pretrained LLM and finetune it to follow
    instructions such as answering queries or classifying texts -- the most common
    tasks in many real-world applications and research.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第3阶段，我们将获取一个预训练的LLM，并对其进行微调，以遵循诸如回答查询或分类文本等指令--这是许多现实应用和研究中最常见的任务。
- en: I hope you are looking forward to embarking on this exciting journey!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您期待着踏上这段令人兴奋的旅程！
- en: 1.8 Summary
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 总结
- en: LLMs have transformed the field of natural language processing, which previouslyrelied
    on explicit rule-based systems and simpler statistical methods. The advent of
    LLMs introduced new deep learning-driven approaches that led to advancements in
    understanding, generating, and translating human language.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs已经改变了自然语言处理领域，之前依赖于显式基于规则的系统和更简单的统计方法。LLMs的出现引入了新的深度学习驱动方法，推动了对人类语言的理解、生成和翻译的进步。
- en: Modern LLMs are trained in two main steps.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代LLMs的训练主要分为两个步骤。
- en: First, they are pretrained on a large corpus of unlabeled text by using the
    prediction of the next word in a sentence as a "label."
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它们通过使用句子中下一个单词的预测作为"标签"，在大型未标记文本语料库上进行预训练。
- en: Then, they are finetuned on a smaller, labeled target dataset to follow instructions
    or perform classification tasks.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它们在较小的、标记的目标数据集上进行微调，以遵循指令或执行分类任务。
- en: LLMs are based on the transformer architecture. The key idea of the transformer
    architecture is an attention mechanism that gives the LLM selective access to
    the whole input sequence when generating the output one word at a time.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs基于Transformer架构。Transformer架构的关键思想是一个注意力机制，在逐词生成输出时，给予LLM对整个输入序列的选择性访问。
- en: The original transformer architecture consists of an encoder for parsing text
    and a decoder for generating text.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的Transformer架构包括一个用于解析文本的编码器和一个用于生成文本的解码器。
- en: LLMs for generating text and following instructions, such as GPT-3 and ChatGPT,
    only implement decoder modules, simplifying the architecture.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于生成文本和遵循指令的LLMs，如GPT-3和ChatGPT，仅实现解码器模块，简化了架构。
- en: Large datasets consisting of billions of words are essential for pretraining
    LLMs. in this book, we will implement and train LLMs on small datasets for educational
    purposes but also see how we can load openly available model weights.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由数十亿字组成的大型数据集对于LLMs的预训练至关重要。在本书中，我们将实现并训练LLMs以用于教育目的的小型数据集，还将了解如何加载公开可用的模型权重。
- en: While the general pretraining task for GPT-like models is to predict the next
    word in a sentence, these LLMs exhibit "emergent" properties such as capabilities
    to classify, translate, or summarize texts.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管GPT-like模型的一般预训练任务是预测句子中的下一个单词，但这些LLMs展现出"新兴"属性，如分类、翻译或总结文本的能力。
- en: Once an LLM is pretrained, the resulting foundation model can be finetuned more
    efficiently for various downstream tasks.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦LLM被预训练，产生的基础模型可以更高效地针对各种下游任务进行微调。
- en: LLMs finetuned on custom datasets can outperform general LLMs on specific tasks.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用定制数据集进行微调的LLMs可以在特定任务上胜过通用LLMs。
- en: 1.9 References and further reading
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9 参考和进一步阅读
- en: 'Custom-built LLMs are able to outperform general-purpose LLMs as a team at
    Bloomberg showed via a a version of GPT pretrained on finance data from scratch.
    The custom LLM outperformed ChatGPT on financial tasks while maintaining good
    performance on general LLM benchmarks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一支彭博团队展示的LLMs在金融数据上从零开始预训练的GPT版本，定制LLMs能够在金融任务上胜过ChatGPT，同时在通用LLM基准测试中表现良好：
- en: '*BloombergGPT: A Large Language Model for Finance* (2023) by Wu *et al.*, [https://arxiv.org/abs/2303.17564](abs.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BloombergGPT：一种用于金融的大型语言模型* （2023）由吴等人撰写，[https://arxiv.org/abs/2303.17564](abs.html)'
- en: 'Existing LLMs can be adapted and finetuned to outperform general LLMs as well,
    which teams from Google Research and Google DeepMind showed in a medical context:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的LLM也可以被调整和微调，以表现出优于一般LLM的潜力，谷歌研究组和谷歌DeepMind团队在医疗领域展示了这一点：
- en: '*Towards Expert-Level Medical Question Answering with Large Language Models*
    (2023) by Singhal *et al.*, [https://arxiv.org/abs/2305.09617](abs.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过大型语言模型实现专业水平医学问答* （2023）由辛哈尔等人撰写，[https://arxiv.org/abs/2305.09617](abs.html)'
- en: 'The paper that proposed the original transformer architecture:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出原始变压器架构的论文：
- en: '*Attention Is All You Need* (2017) by Vaswani *et al.*, [https://arxiv.org/abs/1706.03762](abs.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力机制就是一切* （2017）由瓦斯瓦尼等人撰写，[https://arxiv.org/abs/1706.03762](abs.html)'
- en: 'The original encoder-style transformer, called BERT:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的编码器式变压器，称为BERT：
- en: '*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*
    (2018) by Devlin *et al.*, [https://arxiv.org/abs/1810.04805](abs.html).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BERT：深度双向变压器进行语言理解的预训练* （2018）由德夫林等人撰写，[https://arxiv.org/abs/1810.04805](abs.html)。'
- en: 'The paper describing the decoder-style GPT-3 model, which inspired modern LLMs
    and will be used as a template for implementing an LLM from scratch in this book:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述解码器式GPT-3模型的论文，这激发了现代LLM的开发，并将被用作在本书中从头开始实现LLM的模板：
- en: '*Language Models are Few-Shot Learners* (2020) by Brown *et al.*, [https://arxiv.org/abs/2005.14165](abs.html).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言模型是少样本学习者* （2020）由布朗等人撰写，[https://arxiv.org/abs/2005.14165](abs.html)。'
- en: 'The original vision transformer for classifying images, which illustrates that
    transformer architectures are not only restricted to text inputs:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类图像的原始视觉变压器，说明变压器架构不仅限于文本输入：
- en: '*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*
    (2020) by Dosovitskiy *et al.*, [https://arxiv.org/abs/2010.11929](abs.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一幅图等于16x16个字：大规模图像识别的变压器* （2020）由多索维茨基等人撰写，[https://arxiv.org/abs/2010.11929](abs.html)'
- en: 'Two experimental (but less popular) LLM architectures that serve as examples
    that not all LLMs need to be based on the transformer architecture:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种实验性（但较不流行）的LLM架构作为示例，说明不是所有的LLM都必须基于变压器架构：
- en: '*RWKV: Reinventing RNNs for the Transformer Era* (2023) by Peng *et al.*, [https://arxiv.org/abs/2305.13048](abs.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RWKV：为变压器时代重新设计RNN* （2023）由彭等人撰写，[https://arxiv.org/abs/2305.13048](abs.html)'
- en: '*Hyena Hierarchy: Towards Larger Convolutional Language Models (2023)* by Poli
    *et al.,* [https://arxiv.org/abs/2302.10866](abs.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鬣狗等级结构：向更大的卷积语言模型迈进（2023年）*由波利等人撰写，[https://arxiv.org/abs/2302.10866](abs.html)'
- en: 'Meta AI''s model is a popular implementation of a GPT-like model that is openly
    available in contrast to GPT-3 and ChatGPT:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta AI的模型是一个流行的GPT样式模型的实现，与GPT-3和ChatGPT相比是开放可用的：
- en: '*Llama 2: Open Foundation and Fine-Tuned Chat Models* (2023) by Touvron *et
    al.*, [https://arxiv.org/abs/2307.09288](abs.html)[1](abs.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Llama 2：开放基础和微调的聊天模型* （2023）由特文等人撰写，[https://arxiv.org/abs/2307.09288](abs.html)[1](abs.html)'
- en: 'For readers interested in additional details about the dataset references in
    section 1.5, this paper describes the publicly available *The Pile* dataset curated
    by Eleuther AI:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于对第1.5节中提到的数据集引用感兴趣的读者，这篇论文描述了由Eleuther AI策划的公开可用的*The Pile*数据集：
- en: '*The Pile: An 800GB Dataset of Diverse Text for Language Modeling* (2020) by
    Gao *et al.*, [https://arxiv.org/abs/2101.00027](abs.html).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*堆叠：一份包含多样文本的800GB数据集用于语言建模* （2020）由高等人撰写，[https://arxiv.org/abs/2101.00027](abs.html)。'
- en: '*Training Language Models to Follow Instructions with Human Feedback* (2022)
    by *Ouyang et al.*, [https://arxiv.org/abs/2203.02155](abs.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练语言模型遵循人类反馈指令* （2022）由欧阳等人撰写，[https://arxiv.org/abs/2203.02155](abs.html)'
- en: '[[1]](#_ftnref1) Readers with a background in machine learning may note that
    labeling information is typically required for traditional machine learning models
    and deep neural networks trained via the conventional supervised learning paradigm.
    However, this is not the case for the pretraining stage of LLMs. In this phase,
    LLMs leverage self-supervised learning, where the model generates its own labels
    from the input data. This concept is covered later in this chapter'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_ftnref1) 有机器学习背景的读者可能会注意到，传统机器学习模型和通过传统监督学习范式训练的深度神经网络通常需要标签信息。但是，这并不适用于LLM的预训练阶段。在这个阶段，LLM利用自监督学习，模型从输入数据中生成自己的标签。这个概念稍后在本章中会有介绍'
- en: '[[2]](#_ftnref2) *GPT-3, The $4,600,000 Language Model*, [https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_](h0jwoz.html)[4600000_language_model/](d_gpt3_the_4600000_language_model.html)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_ftnref2) *GPT-3，460 万美元的语言模型*，[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_](h0jwoz.html)[4600000_language_model/](d_gpt3_the_4600000_language_model.html)'
