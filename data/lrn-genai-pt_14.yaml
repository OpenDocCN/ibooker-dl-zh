- en: 12 Training a Transformer to generate text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 训练一个Transformer生成文本
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building a scaled-down version of the GPT-2XL model tailored to your needs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个适合你需求的GPT-2XL模型的缩小版
- en: Preparing data for training a GPT-style Transformer
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练GPT风格的Transformer准备数据
- en: Training a GPT-style Transformer from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练GPT风格的Transformer
- en: Generating text using the trained GPT model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的GPT模型生成文本
- en: In chapter 11, we developed the GPT-2XL model from scratch but were unable to
    train it due to its vast number of parameters. Training a model with 1.5 billion
    parameters requires supercomputing facilities and an enormous amount of data.
    Consequently, we loaded pretrained weights from OpenAI into our model and then
    used the GPT-2XL model to generate text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，我们从头开始开发了GPT-2XL模型，但由于其参数数量庞大，我们无法对其进行训练。训练具有15亿个参数的模型需要超级计算设施和大量的数据。因此，我们将OpenAI的预训练权重加载到我们的模型中，然后使用GPT-2XL模型生成文本。
- en: However, learning how to train a Transformer model from scratch is crucial for
    several reasons. First, while this book doesn’t directly cover fine-tuning a pretrained
    model, understanding how to train a Transformer equips you with the skills needed
    for fine-tuning. Training a model involves initializing parameters randomly, whereas
    fine-tuning involves loading pretrained weights and further training the model.
    Second, training or fine-tuning a Transformer enables you to customize the model
    to meet your specific needs and domain, which can significantly enhance its performance
    and relevance for your use case. Finally, training your own Transformer or fine-tuning
    an existing one provides greater control over data and privacy, which is particularly
    important for sensitive applications or handling proprietary data. In summary,
    mastering the training and fine-tuning of Transformers is essential for anyone
    looking to harness the power of language models for specific applications while
    maintaining privacy and control.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从头开始学习如何训练Transformer模型对于几个原因来说至关重要。首先，虽然这本书没有直接涵盖微调预训练模型，但了解如何训练Transformer能让你掌握微调所需的技能。训练一个模型涉及随机初始化参数，而微调则涉及加载预训练权重并进一步训练模型。其次，训练或微调Transformer能让你根据特定需求和领域定制模型，这可以显著提高其在特定用例中的性能和相关性。最后，训练自己的Transformer或微调现有的Transformer可以提供对数据和隐私的更大控制，这对于敏感应用或处理专有数据尤为重要。总之，掌握Transformer的训练和微调对于任何希望利用语言模型的力量进行特定应用同时保持隐私和控制的人来说是必不可少的。
- en: Therefore, in this chapter, we’ll construct a scaled-down version of the GPT
    model with approximately 5 million parameters. This smaller model follows the
    architecture of the GPT-2XL model; the significant differences are its composition
    of only 3 decoder blocks and an embedding dimension of 256, compared to the original
    GPT-2XL’s 48 decoder blocks and an embedding dimension of 1,600\. By scaling down
    the GPT model to about 5 million parameters, we can train it on a regular computer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将构建一个具有大约500万个参数的GPT模型缩小版。这个较小的模型遵循GPT-2XL模型的架构；与原始GPT-2XL的48个解码器块和1600维的嵌入维度相比，其显著差异是只有3个解码器块和256维的嵌入维度。通过将GPT模型缩小到大约500万个参数，我们可以在普通计算机上对其进行训练。
- en: 'The generated text’s style will depend on the training data. When training
    a model from scratch for text generation, both text length and variation are crucial.
    The training material must be extensive enough for the model to learn and mimic
    a particular writing style effectively. At the same time, if the training material
    lacks variation, the model may simply replicate passages from the training text.
    On the other hand, if the material is too long, training may require excessive
    computational resources. Therefore, we will use three novels by Ernest Hemingway
    as our training material: *The Old Man and the Sea*, *A Farewell to Arms*, and
    *For Whom the Bell Tolls*. This selection ensures that our training data has sufficient
    length and variation for effective learning without being so long that training
    becomes impractical.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本风格将取决于训练数据。当从头开始训练模型进行文本生成时，文本长度和变化都是至关重要的。训练材料必须足够广泛，以便模型能够有效地学习和模仿特定的写作风格。同时，如果训练材料缺乏变化，模型可能会简单地复制训练文本中的段落。另一方面，如果材料太长，训练可能需要过多的计算资源。因此，我们将使用欧内斯特·海明威的三部小说作为我们的训练材料：*《老人与海》*，*《永别了，武器》*，和*《丧钟为谁而鸣》*。这个选择确保我们的训练数据具有足够的长度和变化，以便有效地学习，同时又不会太长以至于训练变得不切实际。
- en: Since GPT models cannot process raw text directly, we will first tokenize the
    text into words. We will then create a dictionary to map each unique token to
    a different index. Using this dictionary, we will convert the text into a long
    sequence of integers, ready for input into a neural network.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT模型不能直接处理原始文本，我们首先将文本分词成单词。然后，我们将创建一个字典，将每个唯一的标记映射到不同的索引。使用这个字典，我们将文本转换成一个长序列的整数，以便输入到神经网络中。
- en: We will use sequences of 128 indexes as input to train the GPT model. As in
    chapters 8 and 10, we will shift the input sequence by one token to the right
    and use it as the output. This approach forces the model to predict the next word
    in a sentence based on the current token and all previous tokens in the sequence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用128个索引的序列作为输入来训练GPT模型。正如第8章和第10章所述，我们将输入序列向右移动一个标记，并将其用作输出。这种方法迫使模型根据当前标记和序列中所有之前的标记来预测句子中的下一个单词。
- en: A key challenge is determining the optimal number of epochs for training the
    model. Our goal is not merely to minimize the cross-entropy loss in the training
    set, as doing so could lead to overfitting, where the model simply replicates
    passages from the training text. To tackle this problem, we plan to train the
    model for 40 epochs. We will save the model at 10-epoch intervals and evaluate
    which version can generate coherent text without merely copying passages from
    the training material. Alternatively, one could potentially use a validation set
    to assess the performance of the model and decide when to stop training, as we
    did in chapter 2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键挑战是确定训练模型的最佳epoch数量。我们的目标不仅仅是最小化训练集中的交叉熵损失，因为这样做可能会导致过拟合，即模型只是简单地复制训练文本中的段落。为了解决这个问题，我们计划训练模型40个epoch。我们将每隔10个epoch保存一次模型，并评估哪个版本可以生成连贯的文本，而不仅仅是复制训练材料中的段落。或者，一个人可能潜在地使用验证集来评估模型的性能，并决定何时停止训练，就像我们在第2章中所做的那样。
- en: Once our GPT model is trained, we will use it to generate text autoregressively,
    as we did in chapter 11\. We’ll test different versions of the trained model.
    The model trained for 40 epochs produces very coherent text, capturing Hemingway’s
    distinctive style. However, it may also generate text partly copied from the training
    material, especially if the prompt is similar to passages in the training text.
    The model trained for 20 epochs also generates coherent text, albeit with occasional
    grammatical errors, but is less likely to directly copy from the training text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的GPT模型训练完成，我们将使用它来自动回归地生成文本，就像我们在第11章中所做的那样。我们将测试训练模型的多个版本。训练了40个epoch的模型产生了非常连贯的文本，捕捉到了海明威的独特风格。然而，它也可能生成部分复制自训练材料的文本，特别是如果提示与训练文本中的段落相似。训练了20个epoch的模型也生成了连贯的文本，尽管偶尔会有语法错误，但不太可能直接从训练文本中复制。
- en: The primary goal of this chapter is not necessarily to generate the most coherent
    text possible, which presents significant challenges. Instead, our objective is
    to teach you how to build a GPT-style model from scratch, tailored to real-world
    applications and your specific needs. More importantly, this chapter outlines
    the steps involved in training a GPT model from scratch. You will learn how to
    select training text based on your objectives, tokenize the text and convert it
    to indexes, and prepare batches of training data. You will also learn how to determine
    the number of epochs for training. Once the model is trained, you will learn how
    to generate text using the model and how to avoid generating text directly copied
    from the training material.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标并非一定是生成尽可能连贯的文本，这本身就是一个巨大的挑战。相反，我们的目标是教会你如何从头开始构建一个GPT风格的模型，使其适用于现实世界的应用和你的特定需求。更重要的是，本章概述了从头开始训练GPT模型所需的步骤。你将学习如何根据你的目标选择训练文本，对文本进行分词并将其转换为索引，以及准备训练数据批次。你还将学习如何确定训练的epoch数量。一旦模型训练完成，你将学习如何使用模型生成文本，以及如何避免直接从训练材料中复制文本。
- en: 12.1 Building and training a GPT from scratch
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 从头开始构建和训练GPT
- en: Our objective is to master building and training a GPT model from scratch, tailored
    to specific tasks. This skill is crucial for applying the concepts in this book
    to real-world problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是掌握从头开始构建和训练GPT模型，使其针对特定任务进行定制。这项技能对于将本书中的概念应用于现实世界问题至关重要。
- en: Imagine you are an avid fan of Ernest Hemingway’s work and wish to train a GPT
    model to generate text in Hemingway’s style. How would you approach this? This
    section discusses the steps involved in this task.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一位热衷于欧内斯特·海明威作品的粉丝，并希望训练一个GPT模型以生成海明威风格的文本。你将如何着手？本节将讨论完成此任务所需的步骤。
- en: The first step is to configure a GPT model suitable for training. You’ll create
    a GPT model with a structure similar to the GPT-2 model you built in chapter 11
    but with significantly fewer parameters to make training feasible in just a few
    hours. As a result, you’ll need to determine key hyperparameters of the model,
    such as sequence length, embedding dimension, number of decoder blocks, and dropout
    rates. These hyperparameters are crucial as they influence both the quality of
    the output from the trained model and the speed of training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是配置一个适合训练的GPT模型。你将创建一个与第11章中构建的GPT-2模型结构相似的GPT模型，但参数数量显著减少，以便在几小时内进行训练成为可能。因此，你需要确定模型的关键超参数，如序列长度、嵌入维度、解码器块数量和dropout率。这些超参数至关重要，因为它们会影响训练模型输出的质量以及训练速度。
- en: Following that, you will gather the raw text of several Hemingway novels and
    clean it up to ensure it is suitable for training. You will tokenize the text
    and assign a different integer to each unique token so that you can feed it to
    the model. To prepare the training data, you will break down the text into sequences
    of integers of a certain length and use them as inputs. You will then shift the
    inputs one token to the right and use them as outputs. This approach forces the
    model to predict the next token based on the current token and all previous tokens
    in the sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你将收集几部海明威小说的原始文本，并将其清理以确保适合训练。你需要对文本进行分词，并为每个唯一的标记分配一个不同的整数，以便将其输入到模型中。为了准备训练数据，你需要将文本分解成一定长度的整数序列，并将它们用作输入。然后，你将输入向右移动一个标记，并将它们用作输出。这种方法迫使模型根据序列中的当前标记和所有先前标记来预测下一个标记。
- en: Once the model is trained, you will use it to generate text based on a prompt.
    You will first convert the text in the prompt to a sequence of indexes and feed
    it to the trained model. The model uses the sequence to predict the most likely
    next token iteratively. After that, you will convert the sequence of tokens generated
    by the model back to text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，你将使用它根据提示生成文本。首先，你将提示文本转换为索引序列，并将其输入到训练好的模型中。模型使用该序列迭代地预测最可能的下一个标记。之后，你将模型生成的标记序列转换回文本。
- en: In this section, we will first discuss the architecture of the GPT model for
    the task. After that, we will discuss the steps involved in training the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论用于此任务的GPT模型架构。然后，我们将讨论训练模型涉及的步骤。
- en: 12.1.1 The architecture of a GPT to generate text
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 用于生成文本的GPT架构
- en: Although GPT-2 is available in various sizes, they all share a similar architecture.
    The GPT model we construct in this chapter follows the same structural design
    as GPT-2 but is significantly smaller, making it feasible to train without the
    need for supercomputing facilities. Table 12.1 presents a comparison between our
    GPT model and the four versions of the GPT-2 models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-2有多种大小，但它们都具有相似的架构。本章中我们构建的GPT模型遵循与GPT-2相同的结构设计，但规模显著减小，这使得在没有超级计算设施的情况下进行训练成为可能。表12.1展示了我们的GPT模型与GPT-2四个版本模型的比较。
- en: Table 12.1 A comparison of our GPT with different versions of GPT-2 models
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 我们GPT与不同版本的GPT-2模型的比较
- en: '|  | GPT-2S | GPT-2M | GPT-2L | GPT-2XL | Our GPT |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-2S | GPT-2M | GPT-2L | GPT-2XL | 我们GPT |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Embedding dimension | 768 | 1,024 | 1,280 | 1,600 | 256 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入维度 | 768 | 1,024 | 1,280 | 1,600 | 256 |'
- en: '| Number of decoder layers | 12 | 24 | 36 | 48 | 3 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 解码器层数 | 12 | 24 | 36 | 48 | 3 |'
- en: '| Number of heads | 12 | 16 | 20 | 25 | 4 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 头数量 | 12 | 16 | 20 | 25 | 4 |'
- en: '| Sequence length | 1,024 | 1,024 | 1,024 | 1,024 | 128 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 1,024 | 1,024 | 1,024 | 1,024 | 128 |'
- en: '| Vocabulary size | 50,257 | 50,257 | 50,257 | 50,257 | 10,600 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 词汇量大小 | 50,257 | 50,257 | 50,257 | 50,257 | 10,600 |'
- en: '| Number of parameters | 124 million | 350 million | 774 million | 1,558 million
    | 5.12 million |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 参数数量 | 1.24亿 | 3.5亿 | 7.74亿 | 15.58亿 | 512万 |'
- en: In this chapter, we’ll construct a GPT model with three decoder layers and an
    embedding dimension of 256 (meaning each token is represented by a 256-value vector
    after word embedding). As we mentioned in chapter 11, GPT models use a different
    positional encoding method than the one used in the 2017 paper “Attention Is All
    You Need.” Instead, we use embedding layers to learn the positional encodings
    for different positions in a sequence. As a result, each position in a sequence
    is also represented by a 256-value vector. For calculating causal self-attention,
    we use four parallel attention heads to capture different aspects of the meanings
    of a token in the sequence. Thus, each attention head has a dimension of 256/4
    = 64, similar to that in GPT-2 models. For example, in GPT-2XL, each attention
    head has a dimension of 1,600/25 = 64.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个具有三个解码器层和 256 维嵌入维度（这意味着每个标记在词嵌入后由一个 256 位的向量表示）的 GPT 模型。正如我们在第
    11 章中提到的，GPT 模型使用与 2017 年论文“Attention Is All You Need”中使用的不同位置编码方法。相反，我们使用嵌入层来学习序列中不同位置的位置编码。因此，序列中的每个位置也由一个
    256 位的向量表示。为了计算因果自注意力，我们使用四个并行注意力头来捕捉序列中标记意义的各个方面。因此，每个注意力头的维度为 256/4 = 64，与 GPT-2
    模型中的相似。例如，在 GPT-2XL 中，每个注意力头的维度为 1,600/25 = 64。
- en: The maximum sequence length in our GPT model is 128, which is much shorter than
    the maximum sequence length of 1,024 in GPT-2 models. This reduction is necessary
    to keep the number of parameters in the model manageable. However, even with 128
    elements in a sequence, the model can learn the relationship between tokens in
    a sequence and generate coherent text.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 GPT 模型的最大序列长度为 128，这比 GPT-2 模型的最大序列长度 1,024 短得多。这种减少是必要的，以保持模型中参数的数量可管理。然而，即使序列中有
    128 个元素，模型也能学习序列中标记之间的关系并生成连贯的文本。
- en: While GPT-2 models have a vocabulary size of 50,257, our model has a much smaller
    vocabulary size of 10,600\. It’s important to note that the vocabulary size is
    mainly determined by the training data, rather than being a predefined choice.
    If you choose to use more text for training, you may end up with a larger vocabulary.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GPT-2 模型的词汇量为 50,257，但我们的模型词汇量要小得多，为 10,600。重要的是要注意，词汇量主要是由训练数据决定的，而不是一个预定义的选择。如果你选择使用更多文本进行训练，你可能会得到一个更大的词汇量。
- en: Figure 12.1 illustrates the architecture of the decoder-only Transformer we
    will create in this chapter. It is similar to the architecture of GPT-2 that you
    have seen in chapter 11, except that it is smaller in size. As a result, the total
    number of parameters in our model is 5.12 million, compared to the 1.558 billion
    in the GPT-2XL model that we built in chapter 11\. Figure 12.1 shows the size
    of the training data at each step of training.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 展示了我们将在本章中创建的仅解码器 Transformer 的架构。它与第 11 章中看到的 GPT-2 架构相似，只是规模更小。因此，我们模型中的总参数数量为
    512 万，而第 11 章中构建的 GPT-2XL 模型的参数数量为 15.58 亿。图 12.1 显示了训练过程中每个步骤的训练数据大小。
- en: '![](../../OEBPS/Images/CH12_F01_Liu.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F01_Liu.png)'
- en: Figure 12.1 The architecture of a decoder-only Transformer, designed to generate
    text. The text from three Hemingway novels is tokenized and then converted into
    indexes. We arrange 128 indexes into a sequence, and each batch contains 32 such
    sequences. The input first undergoes word embedding and positional encoding, with
    the input embedding being the sum of these two components. This input embedding
    is then processed through three decoder layers. Following this, the output undergoes
    layer normalization and passes through a linear layer, resulting in an output
    size of 10,600, which corresponds to the number of unique tokens in the vocabulary.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 展示了用于生成文本的仅解码器 Transformer 架构。来自三个海明威小说的文本被标记化，然后转换为索引。我们将 128 个索引排列成一个序列，每个批次包含
    32 个这样的序列。输入首先进行词嵌入和位置编码，输入嵌入是这两个组件的总和。然后，这个输入嵌入通过三个解码器层进行处理。随后，输出经过层归一化并通过一个线性层，结果输出大小为
    10,600，这对应于词汇表中的唯一标记数量。
- en: The input to the GPT model we create consists of input embeddings, which are
    illustrated at the bottom of figure 12.1\. We will discuss how to calculate these
    embeddings in detail in the next subsection. Briefly, they are the sum of word
    embeddings and positional encodings from the input sequence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的 GPT 模型的输入由输入嵌入组成，如图 12.1 底部所示。我们将在下一小节中详细讨论如何计算这些嵌入。简而言之，它们是输入序列中词嵌入和位置编码的总和。
- en: 'The input embedding is then passed sequentially through three decoder layers.
    Similar to the GPT-2XL model we built in chapter 11, each decoder layer consists
    of two sublayers: a causal self-attention layer and a feed-forward network. Additionally,
    we apply layer normalization and residual connections to each sublayer. After
    this, the output goes through a layer normalization and a linear layer. The number
    of outputs in our GPT model corresponds to the number of unique tokens in the
    vocabulary, which is 10,600\. The output of the model is the logits for the next
    token. Later, we will apply the softmax function to these logits to obtain the
    probability distribution over the vocabulary. The model is designed to predict
    the next token based on the current token and all previous tokens in the sequence.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入随后按顺序通过三个解码器层。与我们在第11章构建的GPT-2XL模型类似，每个解码器层由两个子层组成：一个因果自注意力层和一个前馈网络。此外，我们对每个子层应用层归一化和残差连接。之后，输出经过层归一化和线性层。我们GPT模型中的输出数量对应于词汇表中的独特标记数量，即10,600。模型的输出是下一个标记的logits。稍后，我们将对这些logits应用softmax函数，以获得词汇表上的概率分布。该模型旨在根据当前标记和序列中所有之前的标记来预测下一个标记。
- en: 12.1.2 The training process of the GPT model to generate text
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 GPT模型生成文本的训练过程
- en: Now that we know how to construct the GPT model for text generation, let’s explore
    the steps involved in training the model. We aim to provide an overview of the
    training process before diving into the coding aspect of the project.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何构建用于文本生成的GPT模型，让我们来探讨训练模型所涉及的步骤。我们旨在在深入项目编码方面之前提供一个训练过程的概述。
- en: 'The style of the generated text is influenced by the training text. Since our
    objective is to train the model to generate text in the style of Ernest Hemingway,
    we’ll use the text from three of his novels: *The Old Man and the Sea*, *A Farewell
    to Arms*, and *For Whom the Bell Tolls*. If we were to choose just one novel,
    the training data would lack variety, leading the model to memorize passages from
    the novel and generate text identical to the training data. Conversely, using
    too many novels would increase the number of unique tokens, making it challenging
    to train the model effectively in a short amount of time. Therefore, we strike
    a balance by selecting three novels and combining them as our training data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本风格受训练文本的影响。由于我们的目标是训练模型以生成类似欧内斯特·海明威风格的文本，我们将使用他三部小说中的文本：*《老人与海》*，*《永别了，武器》*，和*《丧钟为谁而鸣》*。如果我们只选择一部小说，训练数据将缺乏多样性，导致模型记住小说中的段落并生成与训练数据相同的文本。相反，使用过多的小说会增加独特标记的数量，使得在短时间内有效训练模型变得具有挑战性。因此，我们通过选择三部小说并将它们组合作为我们的训练数据来达到平衡。
- en: Figure 12.2 illustrates the steps involved in training the GPT model to generate
    text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2展示了训练GPT模型生成文本所涉及的步骤。
- en: '![](../../OEBPS/Images/CH12_F02_Liu.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F02_Liu.png)'
- en: Figure 12.2 The training process for a decoder-only Transformer to generate
    text, Hemingway-style.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 仅解码器的Transformer生成Hemingway风格文本的训练过程。
- en: As in the previous three chapters, the first step in the training process is
    to convert text into a numerical form so that we can feed the training data to
    the model. Specifically, we first break down the text of the three novels into
    tokens using word-level tokenization, as we did in chapter 8\. In this case, each
    token is a whole word or a punctuation mark (such as a colon, a parenthesis, or
    a comma). Word-level tokenization is easy to implement, and we can control the
    number of unique tokens. After tokenization, we assign a unique index (i.e., an
    integer) to each token, converting the training text into a sequence of integers
    (see step 1 in figure 12.2).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前三个章节所述，训练过程中的第一步是将文本转换为数值形式，以便我们可以将训练数据输入到模型中。具体来说，我们首先使用与第8章相同的方法，将三部小说的文本分解成标记。在这种情况下，每个标记是一个完整的单词或标点符号（如冒号、括号或逗号）。词级标记化易于实现，我们可以控制独特标记的数量。标记化后，我们为每个标记分配一个唯一的索引（即一个整数），将训练文本转换为整数序列（见图12.2中的步骤1）。
- en: 'Next, we transform the sequence of integers into training data by first dividing
    this sequence into sequences of equal length (step 2 in figure 12.2). We allow
    a maximum length of 128 indexes in each sequence. The choice of 128 allows us
    to capture long-range dependencies among tokens in a sentence while keeping the
    model size manageable. However, the number 128 is not magical: changing the number
    to, say, 100 or 150 will lead to similar results. These sequences form the features
    (the x variable) of our model. As we did in previous chapters, we shift the input
    sequence one token to the right and use it as the output in the training data
    (the y variable; step 3 in figure 12.2).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过首先将这个整数序列分成等长的序列（图 12.2 中的步骤 2）来将序列转换为训练数据。我们允许每个序列中最多有 128 个索引。选择 128
    允许我们在保持模型大小可管理的同时捕捉句子中标记之间的长距离依赖关系。然而，数字 128 并非神奇：将其更改为，比如说，100 或 150，会导致类似的结果。这些序列形成我们模型的特征（x
    变量）。正如我们在前面的章节中所做的那样，我们将输入序列向右移动一个标记，并将其用作训练数据中的输出（y 变量；图 12.2 中的步骤 3）。
- en: The pairs of input and output serve as the training data (x, y). In the example
    of the sentence “the old man and the sea,” we use indexes corresponding to “the
    old man and the” as the input x. We shift the input one token to the right and
    use the indexes for “old man and the sea” as the output y. In the first time step,
    the model uses “the” to predict “old.” In the second time step, the model uses
    “the old” to predict “man,” and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出的配对作为训练数据（x, y）。在“老人与海”这个句子的例子中，我们使用对应于“老人与海”的索引作为输入 x。我们将输入向右移动一个标记，并使用“老人与海”的索引作为输出
    y。在第一次时间步，模型使用“the”来预测“old”。在第二次时间步，模型使用“the old”来预测“man”，依此类推。
- en: 'During training, you will iterate through the training data. In the forward
    passes, you feed the input sequence x through the GPT model (step 4). The GPT
    then makes a prediction based on the current parameters in the model (step 5).
    You compute the cross-entropy loss by comparing the predicted next tokens with
    the output obtained from step 3\. In other words, you compare the model’s prediction
    with the ground truth (step 6). Finally, you will adjust the parameters in the
    GPT model so that in the next iteration, the model’s predictions move closer to
    the actual output, minimizing the cross-entropy loss (step 7). Note that the model
    is essentially performing a multicategory classification problem: it’s predicting
    the next token from all unique tokens in the vocabulary.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您将遍历训练数据。在正向传递中，您将输入序列 x 通过 GPT 模型（步骤 4）。然后 GPT 根据模型中当前的参数进行预测（步骤 5）。您通过比较预测的下一个标记与步骤
    3 获得的输出来计算交叉熵损失。换句话说，您将模型的预测与真实值进行比较（步骤 6）。最后，您将调整 GPT 模型中的参数，以便在下一个迭代中，模型的预测更接近实际输出，最小化交叉熵损失（步骤
    7）。请注意，模型本质上是在执行一个多类别分类问题：它从词汇表中的所有唯一标记中预测下一个标记。
- en: You will repeat steps 3 to 7 through many iterations. After each iteration,
    the model parameters are adjusted to improve the prediction of the next token.
    We will repeat this process for 40 epochs and save the trained model after every
    10 epochs. As you will see later, if we train the model for too long, it becomes
    overfit, memorizing passages from the training data. The generated text then becomes
    identical to those in the original novels. We will test ex post which version
    of the model generates coherent text and, at the same time, does not simply copy
    from the training data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您将通过多次迭代重复步骤 3 到 7。每次迭代后，模型参数都会调整以改进下一个标记的预测。我们将重复此过程 40 个周期，并在每个 10 个周期后保存训练好的模型。如您稍后所见，如果我们训练模型时间过长，它就会过拟合，记住训练数据中的段落。生成的文本随后就会与原始小说中的文本相同。我们将事后测试哪个模型版本生成的文本既连贯，又没有简单地从训练数据中复制。
- en: 12.2 Tokenizing text of Hemingway novels
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 海明威小说文本的分词
- en: 'Now that you understand the architecture of the GPT model and the training
    process, let’s begin with the first step: tokenizing and indexing the text of
    Hemingway’s novels.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 GPT 模型的架构和训练过程，让我们从第一步开始：对海明威小说的文本进行分词和索引。
- en: First, we’ll process the text data to prepare it for training. We’ll break down
    the text into individual tokens, as we did in chapter 8\. Since deep neural networks
    cannot directly process raw text, we’ll create a dictionary that assigns an index
    to each token, effectively mapping them to integers. After that, we’ll organize
    these indexes into batches of training data, which will be crucial for training
    the GPT model in the subsequent steps.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将处理文本数据，为训练做准备。我们将文本分解为单个标记，就像我们在第 8 章所做的那样。由于深度神经网络不能直接处理原始文本，我们将创建一个字典，为每个标记分配一个索引，有效地将它们映射到整数。之后，我们将这些索引组织成训练数据批次，这对于在后续步骤中训练
    GPT 模型至关重要。
- en: We’ll use word-level tokenization for its simplicity in dividing text into words,
    as opposed to the more complex subword tokenization that requires a nuanced understanding
    of linguistic structure. Additionally, word-level tokenization results in a smaller
    number of unique tokens than subword tokenization, reducing the number of parameters
    in the GPT model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用词级分词，因为它在将文本划分为单词方面简单，而不是更复杂的子词分词，后者需要细微的语言结构理解。此外，词级分词产生的唯一标记数量比子词分词少，从而减少了
    GPT 模型中的参数数量。
- en: 12.2.1 Tokenizing the text
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 文本分词
- en: 'To train the GPT model, we’ll use the raw text files of three novels by Ernest
    Hemingway*: The Old Man and the Sea*, *A Farewell to Arms*, and *For Whom the
    Bell Tolls*. The text files are downloaded from the Faded Page website: [https://www.fadedpage.com](https://www.fadedpage.com).
    I have cleaned up the text by removing the top and bottom paragraphs that are
    not part of the original book. When preparing your own training text, it’s crucial
    to eliminate all irrelevant information, such as vendor details, formatting, and
    license information. This ensures that the model focuses solely on learning the
    writing style present in the text. I have also removed the text between chapters
    that are not relevant to the main text. You can download the three files OldManAndSea.txt,
    FarewellToArms.txt, and ToWhomTheBellTolls.txt from the book’s GitHub repository:
    [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI). Place them
    in the /files/ folder on your computer.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练 GPT 模型，我们将使用欧内斯特·海明威的三部小说的原始文本文件：*《老人与海》*，*《永别了，武器》*，和 *《丧钟为谁而鸣》*。文本文件是从
    Faded Page 网站下载的：[https://www.fadedpage.com](https://www.fadedpage.com)。我已经清理了文本，移除了不属于原始书籍的顶部和底部段落。在准备自己的训练文本时，消除所有无关信息至关重要，例如供应商详情、格式和许可信息。这确保了模型只专注于学习文本中存在的写作风格。我还移除了章节之间的不相关文本。您可以从书籍的
    GitHub 仓库下载三个文件 OldManAndSea.txt，FarewellToArms.txt，和 ToWhomTheBellTolls.txt：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。将它们放在您计算机上的
    /files/ 文件夹中。
- en: 'In the text file for *The Old Man and the Sea*, both the opening double quote
    (“) and the closing double quote (”) are represented by straight double quotes
    ("). This is not the case in the text files for the other two novels. Therefore,
    we load up the text for *The Old Man and the Sea* and change straight quotes to
    either an opening quote or a closing quote. Doing so allows us to differentiate
    between the opening and closing quotes. This will also aid in formatting the generated
    text later on: we’ll remove the space after the opening quote and the space before
    the closing quote. This step is implemented as shown in the following listing.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *《老人与海》* 的文本文件中，开双引号（“）和闭双引号（”）都表示为直双引号（"）。在其他两部小说的文本文件中并非如此。因此，我们加载 *《老人与海》*
    的文本，并将直引号更改为开引号或闭引号。这样做可以让我们区分开引号和闭引号。这还将有助于稍后格式化生成的文本：我们将移除开引号后的空格和闭引号前的空格。此步骤的实现方式如下所示。
- en: Listing 12.1 Changing straight quotes to opening and closing quotes
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 将直引号更改为开引号和闭引号
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads up the raw text and breaks it into individual characters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载原始文本并将其分解为单个字符
- en: ② If a straight double quote is followed by a space or a line break, changes
    it to a closing quote
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果一个直引号后面跟着一个空格或换行符，则将其更改为闭合引号
- en: ③ Otherwise, changes it to an opening quote
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 否则，将其更改为开引号
- en: ④ Converts a straight single quote to an apostrophe
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将直单引号转换为撇号
- en: ⑤ Joins individual characters back to text
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将单个字符重新组合成文本
- en: If a double quote is followed by a space or a line break, we’ll change it to
    a closing quote; otherwise, we’ll change it to an opening quote. The apostrophe
    was entered as a single straight quote, and we have changed it to an apostrophe
    in the form of a closing single quote in listing 12.1.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果双引号后面跟着空格或换行符，我们将将其更改为闭合引号；否则，我们将将其更改为开引号。撇号被输入为单个直引号，我们在列表12.1中将其更改为闭合单引号的形式。
- en: Next, we load the text for the other two novels and combine the three novels
    into one single file.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载另外两本小说的文本，并将这三本小说合并成一个单独的文件。
- en: Listing 12.2 Combining the text from three novels
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.2 合并三本小说的文本
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Reads the text from the second novel
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从第二本小说读取文本
- en: ② Reads the text from the third novel
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从第三本小说读取文本
- en: ③ Combines the text from the three novels
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 合并三本小说的文本
- en: ④ Saves the combined text in the local folder
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将合并的文本保存到本地文件夹
- en: We load the text from the other two novels, *A Farewell to Arms* and *For Whom
    the Bell Tolls*. We then combine the text from all three novels to use as our
    training data. Additionally, we save the combined text in a local file named ThreeNovels.txt
    so that we can later verify if the generated text is directly copied from the
    original text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载另外两本小说的文本，*《永别了，武器》*和*《丧钟为谁而鸣》*。然后我们将三本小说的文本合并起来作为我们的训练数据。此外，我们还将合并的文本保存到本地文件名为ThreeNovels.txt中，以便我们可以在以后验证生成的文本是否直接复制自原始文本。
- en: The output from the preceding code listing is
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码列表的输出是
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is the first 250 characters in the combined text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是合并文本的前250个字符。
- en: We’ll tokenize the text by using a space as the delimiter. As seen in the preceding
    output, punctuation marks such as periods (.), hyphens (-), and apostrophes (’)
    are attached to the preceding words without a space. Therefore, we need to insert
    a space around all punctuation marks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用空格作为分隔符来标记文本。如前所述的输出所示，像句号（.）、连字符（-）和撇号（'）这样的标点符号没有空格地附加在前面单词上。因此，我们需要在所有标点符号周围插入空格。
- en: Additionally, we’ll convert line breaks (\n) into spaces so that they are not
    included in the vocabulary. Converting all words to lowercase is also beneficial
    in our setting, as it ensures that words like “The” and “the” are recognized as
    the same token. This step helps reduce the number of unique tokens, thereby making
    the training process more efficient. To address these problems, we’ll clean up
    the text as shown in the following listing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将换行符（\n）转换为空格，这样它们就不会包含在词汇表中。在我们的设置中，将所有单词转换为小写也是有益的，因为它确保了像“The”和“the”这样的单词被视为相同的标记。这一步骤有助于减少唯一标记的数量，从而使得训练过程更加高效。为了解决这些问题，我们将按照以下列表所示清理文本。
- en: Listing 12.3 Adding spaces around punctuation marks
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.3 在标点符号周围添加空格
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Replaces line breaks with spaces
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将换行符替换为空格
- en: ② Identifies all punctuation marks
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ② 识别所有标点符号
- en: ③ Inserts spaces around punctuation marks
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在标点符号周围插入空格
- en: ④ Counts the number of unique tokens
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算唯一标记的数量
- en: We use the `set()` method to obtain all unique characters in the text. We then
    use the `isalpha()` and `isdigit()` methods to identify and remove letters and
    numbers from the set of unique characters, leaving us with only punctuation marks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`set()`方法获取文本中的所有唯一字符。然后我们使用`isalpha()`和`isdigit()`方法从唯一字符集中识别并移除字母和数字，从而只留下标点符号。
- en: 'If you execute the preceding code block, the output will be as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你执行前面的代码块，输出将如下所示：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This list includes all punctuation marks in the text. We add spaces around them
    and break the text into individual tokens using the `split()` method. The output
    indicates that there are 10,599 unique tokens in the text from the three novels
    by Hemingway, a size that’s much smaller than the 50,257 tokens in GPT-2\. This
    will significantly reduce the model size and training time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表包括文本中的所有标点符号。我们在它们周围添加空格，并使用`split()`方法将文本分解成单个标记。输出表明，海明威的三本小说文本中有10,599个唯一标记，这个数量比GPT-2中的50,257个标记小得多。这将显著减少模型大小和训练时间。
- en: Additionally, we’ll add one more token `"UNK"` to represent unknown tokens.
    This is useful in case we encounter a prompt with unknown tokens, allowing us
    to convert them to an index to feed to the model. Otherwise, we can only use a
    prompt with the preceding 10,599 tokens. Suppose you include the word “technology”
    in the prompt. Since “technology” is not one of the tokens in the dictionary `word_to_int`,
    the program will crash. By including the `"UNK"` token, you can prevent the program
    from crashing in such scenarios. When you train your own GPT, you should always
    include the `"UNK"` token since it’s impossible to include all tokens in your
    vocabulary. To that end, we add `"UNK"` to the list of unique tokens and map them
    to indexes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将添加一个额外的标记“UNK”来表示未知标记。这在遇到包含未知标记的提示时很有用，允许我们将它们转换为索引以输入到模型中。否则，我们只能使用包含前10,599个标记的提示。假设你在提示中包含单词“technology”。由于“technology”不是`word_to_int`字典中的标记之一，程序将会崩溃。通过包含“UNK”标记，你可以防止程序在这种情况下崩溃。当你训练自己的GPT时，你应该始终包含“UNK”标记，因为不可能包含词汇表中的所有标记。为此，我们将“UNK”添加到独特标记列表中，并将它们映射到索引。
- en: Listing 12.4 Mapping tokens to indexes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.4 将标记映射到索引
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Adds “UNK” to the list of unique tokens
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将“UNK”添加到独特标记列表中
- en: ② Counts the size of the vocabulary, ntokens, which will be a hyperparamter
    in our model
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算词汇表的大小，ntokens，这将成为我们模型的一个超参数
- en: ③ Maps tokens to indexes
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将标记映射到索引
- en: ④ Maps indexes to tokens
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将索引映射到标记
- en: The output from the preceding code block is
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块输出的结果是
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The text from the three novels contains 698,207 tokens. After including `"UNK"`
    in the vocabulary, the total number of *unique* tokens is now 10,600\. The dictionary
    `word_to_int` assigns a different index to each unique token. For example, the
    most frequent token, the period (.), is assigned an index of 0, and the word “the”
    is assigned an index of 1\. The dictionary `int_to_word` translates an index back
    to a token. For example, index 3 is translated back to the opening quote (“),
    and index 4 is translated back to the closing quote (”).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 三部小说的文本包含698,207个标记。在词汇表中包含“UNK”后，现在独特的标记总数为10,600。字典`word_to_int`为每个独特的标记分配一个不同的索引。例如，最频繁的标记，句号(.)，被分配了索引0，而单词“the”被分配了索引1。字典`int_to_word`将索引转换回标记。例如，索引3被转换回开引号（“），而索引4被转换回闭引号（”）。
- en: 'We print out the first 20 tokens in the text and their corresponding indexes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印出文本中的前20个标记及其对应的索引：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output is
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we’ll break the indexes into sequences of equal length to use as training
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将索引分解成等长的序列，用作训练数据。
- en: 12.2.2 Creating batches for training
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 创建训练批次
- en: We’ll use a sequence of 128 tokens as the input to the model. We then shift
    the sequence one token to the right and use it as the output.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用128个标记的序列作为模型的输入。然后我们将序列向右移动一个标记，并将其用作输出。
- en: Specifically, we create pairs of (x, y) for training purposes. Each x is a sequence
    with 128 indexes. We choose 128 to strike a balance between training speed and
    the model’s ability to capture long-range dependencies. Setting the number too
    high may slow down training, while setting it too low may prevent the model from
    capturing long-range dependencies effectively.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们创建(x, y)对用于训练目的。每个x是一个包含128个索引的序列。我们选择128个索引是为了在训练速度和模型捕捉长距离依赖的能力之间取得平衡。设置得太高可能会减慢训练速度，而设置得太低可能会阻止模型有效地捕捉长距离依赖。 '
- en: 'Once we have the sequence x, we slide the sequence window to the right by one
    token and use it as the target y. Shifting the sequence by one token to the right
    and using it as the output during sequence generation is a common technique in
    training language models, including GPTs. We have done this in chapters 8 to 10\.
    The following code block creates the training data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了序列x，我们将序列窗口向右滑动一个标记，并将其用作目标y。在序列生成训练中，将序列向右移动一个标记并用作输出是一个常见的技术，包括GPTs。我们在第8章到第10章中已经这样做过了。以下代码块创建了训练数据：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Sets the sequence length to 128 indexes
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将序列长度设置为128个索引
- en: ② The input sequence x contains 128 consecutive indexes in the training text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输入序列x包含训练文本中的128个连续索引。
- en: ③ Shifts x one position to the right and uses it as output y
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将x向右移动一个位置，并将其用作输出y
- en: ④ Adds the pair (x, y) to the training data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将(x, y)对添加到训练数据中。
- en: 'We have created a list xys to contain pairs of (x, y) as our training data.
    As we did in previous chapters, we organize the training data into batches to
    stabilize training. We choose a batch size of 32:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为xys的列表来包含(x, y)对作为我们的训练数据。正如我们在前面的章节中所做的那样，我们将训练数据组织成批次以稳定训练。我们选择批次大小为32：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We print out a pair of x and y as an example. The output is
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印出一对x和y作为示例。输出如下
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Each x and y have a shape of (32, 128). This means that in each batch of training
    data, there are 32 pairs of sequences, with each sequence containing 128 indexes.
    When an index is passed through the `nn.Embedding()` layer, PyTorch looks up the
    corresponding row in the embedding matrix and returns the embedding vector for
    that index, avoiding the need to create potentially very large one-hot vectors.
    Therefore, when x is passed through the word embedding layer, it’s as if x is
    first converted to a one-hot tensor with a dimension of (32, 128, 256). Similarly,
    when x is passed through the positional encoding layer (which is implemented by
    the `nn.Embedding()` layer), it’s as if x is first converted to a one-hot tensor
    with a dimension of (32, 128, 128).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个x和y的形状为(32, 128)。这意味着在每个训练数据批次中，有32对序列，每个序列包含128个索引。当一个索引通过`nn.Embedding()`层传递时，PyTorch会查找嵌入矩阵中对应的行，并返回该索引的嵌入向量，从而避免了创建可能非常大的one-hot向量的需要。因此，当x通过词嵌入层传递时，它就像x首先被转换为一个维度为(32,
    128, 256)的one-hot张量。同样，当x通过位置编码层（由`nn.Embedding()`层实现）传递时，它就像x首先被转换为一个维度为(32,
    128, 128)的one-hot张量。
- en: 12.3 Building a GPT to generate text
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 构建用于生成文本的GPT
- en: Now that we have the training data ready, we’ll create a GPT model from scratch
    to generate text. The model we’ll build has a similar architecture as the GPT-2XL
    model we built in chapter 11\. However, instead of having 48 decoder layers, we’ll
    use only 3 decoder layers. The embedding dimensions and the vocabulary size are
    both much smaller, as I have explained earlier in this chapter. As a result, our
    GPT model will have far fewer parameters than GPT-2XL.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了训练数据，我们将从头开始创建一个GPT模型来生成文本。我们将构建的模型架构与第11章中构建的GPT-2XL模型相似。然而，我们不会使用48个解码器层，而只会使用3个解码器层。嵌入维度和词汇量都小得多，正如我在本章前面所解释的。因此，我们的GPT模型将比GPT-2XL拥有远少的参数。
- en: We’ll follow the same steps as those in chapter 11\. Along the way, we’ll highlight
    the differences between our GPT model and GPT-2XL and explain the reasons for
    these modifications.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循第11章中的相同步骤。在这个过程中，我们将突出我们GPT模型与GPT-2XL之间的差异，并解释这些修改的原因。
- en: 12.3.1 Model hyperparameters
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 模型超参数
- en: 'The feed-forward network in the decoder block uses the Gaussian error linear
    unit (GELU) activation function. GELU has been shown to enhance model performance
    in deep learning tasks, particularly in natural language processing. This has
    become a standard practice in GPT models. Therefore, we define a GELU class as
    follows, as we did in Chapter 11:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块中的前馈网络使用高斯误差线性单元（GELU）激活函数。GELU已被证明可以增强深度学习任务中的模型性能，尤其是在自然语言处理中。这已成为GPT模型中的标准做法。因此，我们定义了一个GELU类，就像我们在第11章中所做的那样：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In chapter 11, we didn’t use a GPU even during the text generation stage, as
    the model was simply too large and a regular GPU would run out of memory if we
    loaded the model onto it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，即使在文本生成阶段，我们也没有使用GPU，因为模型本身太大，如果我们将模型加载到常规GPU上，它就会耗尽内存。
- en: In this chapter, however, our model is significantly smaller. We’ll move the
    model to the GPU for faster training. We’ll also generate text using the model
    on the GPU.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本章中，我们的模型显著更小。我们将模型移动到GPU上进行更快的训练。我们还将使用GPU上的模型生成文本。
- en: 'We use a `Config()` class to include all the hyperparameters used in the model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个`Config()`类来包含模型中使用的所有超参数：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The attributes in the `Config()` class are used as hyperparameters in our GPT
    model. We set the `n_layer` attribute to 3, indicating our GPT model has three
    decoder layers. The `n_head` attribute is set to 4, meaning we’ll split the query
    Q, key K, and value V vectors into 4 parallel heads when calculating causal self-attention.
    The `n_embd` attribute is set to 256, meaning the embedding dimension is 256:
    each token will be represented by a 256-value vector. The `vocab_size` attribute
    is determined by the number of unique tokens in the vocabulary. As explained in
    the last section, there are 10,600 unique tokens in our training text. The `block_size`
    attribute is set to 128, meaning the input sequence contains a maximum of 128
    tokens. We set the dropout rates to 0.1, as we did in chapter 11\.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Config()`类中的属性被用作我们GPT模型的超参数。我们将`n_layer`属性设置为3，表示我们的GPT模型有三个解码器层。`n_head`属性设置为4，意味着在计算因果自注意力时，我们将查询Q、键K和值V向量分割成4个并行头。`n_embd`属性设置为256，表示嵌入维度为256：每个标记将由一个256值的向量表示。`vocab_size`属性由词汇表中的唯一标记数量确定。如上一节所述，我们的训练文本中有10,600个唯一标记。`block_size`属性设置为128，表示输入序列最多包含128个标记。我们将dropout率设置为0.1，与第11章中设置的一样。'
- en: 12.3.2 Modeling the causal self-attention mechanism
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 建模因果自注意力机制
- en: 'The causal self-attention is defined in the same way as in chapter 11:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力机制的定义与第11章相同：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When calculating causal self-attention, the input embedding is passed through
    three neural networks to obtain the query Q, key K, and value V. We then split
    each of them into four parallel heads and calculate masked self-attention within
    each head. After that, we concatenate the four attention vectors back into a single
    attention vector, which is then used as the output of the `CausalSelfAttention()`
    class.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算因果自注意力时，输入嵌入通过三个神经网络传递，以获得查询Q、键K和值V。然后我们将它们各自分割成四个并行头，并在每个头内部计算掩码自注意力。之后，我们将四个注意力向量重新连接成一个单一的注意力向量，然后将其用作`CausalSelfAttention()`类的输出。
- en: 12.3.3 Building the GPT model
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 构建GPT模型
- en: 'We combine a feed-forward network with the causal self-attention sublayer to
    form a decoder block. The feed-forward network injects nonlinearity into the model.
    Without it, the Transformer would simply be a series of linear operations, constraining
    its capacity to capture complex data relationships. Moreover, the feed-forward
    network processes each position independently and uniformly, enabling the transformation
    of features identified by the self-attention mechanism. This facilitates the capture
    of diverse aspects of the input data, thereby augmenting the model’s ability to
    represent information. A decoder block is defined as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前馈网络与因果自注意力子层结合，形成一个解码器块。前馈网络向模型注入非线性。没有它，Transformer将只是一个线性操作的序列，限制了其捕捉复杂数据关系的能力。此外，前馈网络独立且均匀地处理每个位置，使得自注意力机制识别的特征能够进行转换。这有助于捕捉输入数据的各个方面，从而增强模型表示信息的能力。解码器块的定义如下：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Each decoder block in our GPT model consists of two sublayers: a causal self-attention
    sublayer and a feed-forward network. We apply layer normalization and a residual
    connection to each sublayer for improved stability and performance. We then stack
    three decoder layers on top of each other to form the main body of our GPT model.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们GPT模型中的每个解码器块由两个子层组成：一个因果自注意力子层和一个前馈网络。我们对每个子层应用层归一化和残差连接，以提高稳定性和性能。然后，我们将三个解码器层堆叠在一起，形成GPT模型的主体。
- en: Listing 12.5 Building a GPT model
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.5 构建GPT模型
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Moves the positional encoding to CUDA-enabled GPU, if available
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如果可用，将位置编码移动到支持CUDA的GPU上
- en: The positional encoding is created within the `Model()` class. Therefore, we
    need to move it to a compute unified device architecture (CUDA)-enabled GPU (if
    available) to ensure that all inputs to the model are on the same device. Failing
    to do this will result in an error message.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是在`Model()`类中创建的。因此，我们需要将其移动到支持CUDA的GPU（如果可用）上，以确保模型的所有输入都在同一设备上。未能这样做将导致错误信息。
- en: The input to the model consists of sequences of indexes corresponding to tokens
    in the vocabulary. We pass the input through word embedding and positional encoding
    and add the two to form the input embedding. The input embedding then goes through
    the three decoder blocks. After that, we apply layer normalization to the output
    and attach a linear head to it so that the number of outputs is 10,600, the size
    of the vocabulary. The outputs are the logits corresponding to the 10,600 tokens
    in the vocabulary. Later, we’ll apply the softmax activation function to the logits
    to obtain the probability distribution over the unique tokens in the vocabulary
    when generating text.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入由与词汇表中的标记对应的索引序列组成。我们通过词嵌入和位置编码传递输入，并将两者相加形成输入嵌入。然后输入嵌入通过三个解码器块。之后，我们对输出应用层归一化，并将其附加一个线性头，以便输出的数量为10,600，即词汇表的大小。输出是词汇表中10,600个标记的logits。稍后，我们将对logits应用softmax激活函数，以获得生成文本时词汇表中唯一标记的概率分布。
- en: 'Next, we’ll create our GPT model by instantiating the `Model()` class we defined
    earlier:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过实例化我们之前定义的`Model()`类来创建我们的GPT模型：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output is
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Our GPT model has 5.12 million parameters. The structure of our model is similar
    to that of GPT-2XL. If you compare the output above with that from chapter 11,
    you’ll see that the only differences are in the hyperparameters, such as the embedding
    dimension, number of decoder layers, vocabulary size, and so on.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPT模型有512万个参数。我们模型的结构与GPT-2XL相似。如果您将上面的输出与第11章的输出进行比较，您会看到唯一的不同之处在于超参数，如嵌入维度、解码器层数、词汇表大小等。
- en: 12.4 Training the GPT model to generate text
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 训练GPT模型生成文本
- en: In this section, you’ll train the GPT model you just built using the batches
    of training data we prepared earlier in this chapter. A related question is how
    many epochs we should train the model. While training too few epochs may lead
    to incoherent text, training too many epochs may lead to an overfitted model,
    which may generate text identical to passages in the training text.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用本章前面准备好的训练数据批量来训练您刚刚构建的GPT模型。一个相关的问题是应该训练多少个epoch。训练epoch太少可能会导致文本不连贯，而训练epoch太多可能会导致模型过拟合，这可能会导致生成的文本与训练文本中的段落完全相同。
- en: Therefore, we will train the model for 40 epochs. We’ll save the model after
    every 10 epochs and assess which version of the trained model can generate coherent
    text without simply copying passages from the training text. Another potential
    approach is to create a validation set and stop training when the model’s performance
    converges in the validation set, as we did in chapter 2.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将训练模型40个epoch。我们将在每个10个epoch后保存模型，并评估哪个版本的训练模型可以生成连贯的文本，而不仅仅是复制训练文本中的段落。另一种潜在的方法是创建一个验证集，并在模型在验证集中的性能收敛时停止训练，就像我们在第2章中所做的那样。
- en: 12.4.1 Training the GPT model
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 训练GPT模型
- en: 'As always, we’ll use the Adam optimizer. Since our GPT model is essentially
    performing a multicategory classification, we’ll use cross-entropy loss as our
    loss function:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将使用Adam优化器。由于我们的GPT模型本质上执行的是多类别分类，因此我们将使用交叉熵损失作为我们的损失函数：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will train the model for 40 epochs, as shown in the following listing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练模型40个epoch，如下所示。
- en: Listing 12.6 Training the GPT model to generate text
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.6 训练GPT模型生成文本
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Iterates through all batches of training data
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历所有训练数据批量
- en: ② Compares model predictions with actual outputs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将模型预测与实际输出进行比较
- en: ③ Clips gradient norm to 1
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将梯度范数剪裁到1
- en: ④ Tweaks model parameters to minimize loss
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调整模型参数以最小化损失
- en: ⑤ Saves model after every ten epochs
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 每十个epoch后保存模型
- en: During training, we pass all the input sequences x in a batch through the model
    to obtain predictions. We compare these predictions with the output sequences
    y in the batch and calculate the cross-entropy loss. We then adjust the model
    parameters to minimize this loss. Note that we have clipped the gradient norm
    to 1 to avoid the potential problem of exploding gradients.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将所有输入序列x批量通过模型以获得预测。我们将这些预测与批量的输出序列y进行比较，并计算交叉熵损失。然后调整模型参数以最小化这个损失。请注意，我们已经将梯度范数剪裁到1以避免潜在的梯度爆炸问题。
- en: Gradient norm clipping
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度范数剪裁
- en: Gradient norm clipping is a technique used in training neural networks to prevent
    the exploding gradient problem. This problem occurs when the gradients of the
    loss function with respect to the model’s parameters become excessively large,
    leading to unstable training and poor model performance. In gradient norm clipping,
    the gradients are scaled down if their norm (magnitude) exceeds a certain threshold.
    This ensures that the gradients do not become too large, maintaining stable training
    and improving convergence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度范数裁剪是训练神经网络时用来防止梯度爆炸问题的技术。这个问题发生在损失函数相对于模型参数的梯度变得过大时，导致训练不稳定和模型性能差。在梯度范数裁剪中，如果梯度的范数（大小）超过某个阈值，则将梯度缩放。这确保了梯度不会变得过大，从而保持稳定的训练并提高收敛速度。
- en: 'This training process takes a couple of hours if you have a CUDA-enabled GPU.
    After training, four files, GPTe10.pth, GPTe20.pth, ..., GPTe40.pth, will be saved
    on your computer. Alternatively, you can download the trained models from my website:
    [https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip](https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有CUDA支持的GPU，这个过程可能需要几个小时。训练完成后，四个文件，GPTe10.pth, GPTe20.pth, ..., GPTe40.pth，将保存在你的电脑上。或者，你也可以从我的网站下载训练好的模型：[https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip](https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip)。
- en: 12.4.2 A function to generate text
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 生成文本的函数
- en: Now that we have multiple versions of the trained model, we can proceed to text
    generation and compare the performance of different versions. We can assess which
    version performs the best and use that version to generate text.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了多个训练模型的版本，我们可以继续进行文本生成并比较不同版本的性能。我们可以评估哪个版本表现最好，并使用该版本生成文本。
- en: Similar to the process in GPT-2XL, text generation begins with feeding a sequence
    of indexes (representing tokens) to the model as a prompt. The model predicts
    the index of the next token, which is then appended to the prompt to form a new
    sequence. This new sequence is fed back into the model for further predictions,
    and this process is repeated until a desired number of new tokens is generated.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT-2XL中的过程类似，文本生成开始于将一个索引序列（代表标记）作为提示输入到模型中。模型预测下一个标记的索引，然后将这个索引添加到提示中形成一个新的序列。这个新序列被反馈到模型中进行进一步的预测，这个过程重复进行，直到生成所需数量的新标记。
- en: To facilitate this process, we define a `sample()` function. This function takes
    a sequence of indexes as input, representing the current state of the text. It
    then iteratively predicts and appends new indexes to the sequence until the specified
    number of new tokens, `max_new_tokens`, is reached. The following listing shows
    the implementation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便这个过程，我们定义了一个`sample()`函数。这个函数接收一个索引序列作为输入，代表文本的当前状态。然后它迭代地预测并添加新的索引到序列中，直到达到指定的新的标记数`max_new_tokens`。下面的列表展示了实现。
- en: Listing 12.7 A `sample()` function to predict subsequent indexes
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.7 用于预测后续索引的`sample()`函数
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Loads up a version of the trained model
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载一个训练模型的版本
- en: ② Generates a fixed number of new indexes
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成固定数量的新索引
- en: ③ Uses the model to make predictions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用模型进行预测
- en: ④ Attaches the new index to the end of the sequence
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将新索引附加到序列的末尾
- en: ⑤ Outputs only the new indexes
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 只输出新索引
- en: One of the arguments of the `sample()` function is `weights`, which represents
    the trained weights of one of the models saved on your computer. Unlike the `sample()`
    function we defined in chapter 11, our function here returns only the newly generated
    indexes, not including the original indexes that were fed to the `sample()` function.
    We made this change to accommodate cases where the prompt contains unknown tokens.
    In such cases, our `sample()` function ensures that the final output retains the
    original prompt. Otherwise, all unknown tokens would be replaced with `"UNK"`
    in the final output.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数的一个参数是`weights`，它代表保存在你电脑上的某个模型的训练权重。与第11章中定义的`sample()`函数不同，我们这里的函数只返回新生成的索引，不包括输入到`sample()`函数中的原始索引。我们做出这个改变是为了适应提示中包含未知标记的情况。在这种情况下，我们的`sample()`函数确保最终输出保留原始提示。否则，所有未知标记在最终输出中都会被替换为`"UNK"`。'
- en: Next, we define a `generate()` function to generate text based on a prompt.
    The function first converts the prompt to a sequence of indexes. It then uses
    the `sample()` function to generate a new sequence of indexes. After that, the
    `generate()` function concatenates all indexes together and converts them back
    to text. The implementation is shown in the following listing.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`generate()`函数，根据提示生成文本。该函数首先将提示转换为一系列索引。然后，它使用`sample()`函数生成一个新的索引序列。之后，`generate()`函数将所有索引连接起来，并将它们转换回文本。实现方式如下所示。
- en: Listing 12.8 A function to generate text with the trained GPT model
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.8 使用训练好的GPT模型生成文本的函数
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Makes sure the prompt is not empty
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ① 确保提示不为空
- en: ② Converts prompt into a sequence of indexes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将提示转换为一系列索引
- en: ③ Uses the sample() function to generate new indexes
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用sample()函数生成新的索引
- en: ④ Converts the new sequence of indexes back to text
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将新的索引序列转换回文本
- en: We ensure that the prompt is not empty. If it is, you’ll receive an error message
    saying “prompt must contain at least one token.” The `generate()` function allows
    you to select which version of the model to use by specifying the weights saved
    on your computer. For example, you can choose ‘files/GPTe10.pth’ as the value
    of the weights argument for the function. The function converts the prompt into
    a series of indexes, which are then fed into the model to predict the next index.
    After generating a fixed number of new indexes, the function converts the entire
    index sequence back into textual form.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保提示不为空。如果为空，你将收到一个错误消息，提示“提示必须至少包含一个标记。”`generate()`函数允许你通过指定计算机上保存的权重来选择使用模型的哪个版本。例如，你可以选择将‘files/GPTe10.pth’作为函数的权重参数的值。该函数将提示转换为一系列索引，然后将这些索引输入模型以预测下一个索引。在生成一定数量的新索引后，该函数将整个索引序列转换回文本形式。
- en: 12.4.3 Text generation with different versions of the trained model
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.3 使用不同版本的训练模型进行文本生成
- en: Next, we’ll experiment with different versions of the trained model to generate
    text.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试使用不同版本的训练模型来生成文本。
- en: We can use the unknown token `"UNK"` as the prompt for unconditional text generation.
    This is especially beneficial in our context because we want to check if the generated
    text is directly copied from the training text. While a unique prompt that’s very
    different from the training text unlikely leads to passages directly from the
    training text, unconditionally generated text is more likely to be from the training
    text.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用未知标记`"UNK"`作为无条件文本生成的提示。这在我们的情境中特别有益，因为我们想检查生成的文本是否直接复制自训练文本。虽然一个与训练文本非常不同的独特提示不太可能导致直接来自训练文本的段落，但无条件生成的文本更有可能是来自训练文本的。
- en: 'We first use the model after 20 epochs of training to generate text unconditionally:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用经过20个epoch训练后的模型无条件生成文本：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We set the prompt to `"UNK"` and ask the `generate()` function to unconditionally
    generate 20 new tokens 10 times. We use the `manual_seed()` method to fix the
    random seeds so results are reproducible. As you can see, the 10 short passages
    generated here are all grammatically correct, and they sound like passages from
    Hemingway’s novels. For example, the word “kummel” in the first passage was a
    type of liqueur that was mentioned in *A Farewell to Arms* quite often. At the
    same time, none of the above 10 passages are directly copied from the training
    text.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提示设置为`"UNK"`，并要求`generate()`函数无条件地生成20个新标记，重复10次。我们使用`manual_seed()`方法固定随机种子，以确保结果可重复。正如你所见，这里生成的10个短篇段落在语法上都是正确的，听起来像是海明威小说中的段落。例如，第一段中的“kummel”一词在《永别了，武器》中经常被提及。同时，上述10个段落中没有任何一个是直接从训练文本中复制的。
- en: 'Next, we use the model after 40 epochs of training instead to generate text
    unconditionally and see what happens:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用经过40个epoch训练后的模型无条件生成文本，看看会发生什么：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The 10 short passages generated here are again all grammatically correct, and
    they sound like passages from Hemingway’s novels. However, if you examine them
    closely, a large part of the eighth passage is directly copied from the novel
    *A Farewell to Arms*. The part `they don't marry." i reached for her hand. "don`
    appeared in the novel as well. You can verify by searching in the file ThreeNovels.txt
    that was saved on your computer earlier.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里生成的10个简短段落再次都是语法正确的，听起来像海明威小说的段落。然而，如果你仔细检查，第八个段落的大部分内容直接复制自小说《永别了，武器》。部分“他们不结婚。”我伸出手。“不。”在小说中也有出现。你可以通过搜索之前保存在你电脑上的文件ThreeNovels.txt来验证。
- en: Exercise 12.1
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.1
- en: Generate a passage of text with 50 new tokens unconditionally using the model
    trained for 10 epochs. Set the random seed to 42 and keep the `temperature` and
    `top-K` sampling at the default setting. Examine whether the generated passage
    is grammatically correct and if any parts are directly copied from the training
    text.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练了10个epoch的模型无条件地生成包含50个新token的文本段落。设置随机种子为42，并保持`temperature`和`top-K`采样为默认设置。检查生成的段落是否语法正确，以及是否有任何部分直接复制自训练文本。
- en: 'Alternatively, you can use a unique prompt that’s not in the training text
    to generate new text. For example, you might use “the old man saw the shark near
    the” as the prompt and ask the `generate()` function to add 20 new tokens to the
    prompt, repeating this process 10 times:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用不在训练文本中的独特提示来生成新文本。例如，你可能使用“老人看到鲨鱼靠近的”作为提示，并要求`generate()`函数向提示中添加20个新token，重复此过程10次：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The generated text is grammatically correct and coherent, closely resembling
    passages from Hemingway’s novel *The Old Man and the Sea*. Since we used the model
    trained for 40 epochs, there’s a higher likelihood of generating text that directly
    mirrors the training data. However, using a unique prompt can reduce this probability.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本语法正确且连贯，与海明威小说《老人与海》中的段落非常相似。由于我们使用了训练了40个epoch的模型，生成直接反映训练数据的文本的可能性更高。然而，使用独特的提示可以降低这种可能性。
- en: 'By setting the `temperature` and using `top-K` sampling, we can further control
    the diversity of the generated text. In this case, with a prompt like “the old
    man saw the shark near the,” and a temperature of 0.9 with top-50 sampling, the
    output remains mostly grammatically correct:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`temperature`和`top-K`采样，我们可以进一步控制生成文本的多样性。在这种情况下，使用提示“老人看到鲨鱼靠近的”，温度为0.9，top-50采样，输出仍然主要语法正确：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE30]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since we used the model trained for 20 epochs instead of 40 epochs, the output
    is less coherent, with occasional grammar errors. For example, “with its long
    dip sharply” in the third passage is not grammatically correct. However, the risk
    of generating text directly copied from the training data is also lower.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了训练了20个epoch的模型而不是40个epoch，输出不太连贯，偶尔有语法错误。例如，第三段中的“with its long dip sharply”在语法上是不正确的。然而，生成直接复制自训练数据的文本的风险也较低。
- en: Exercise 12.2
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.2
- en: Generate a passage of text with 50 new tokens using the model trained for 40
    epochs. Use “the old man saw the shark near the” as the prompt; set the `random
    seed` to 42, the `temperature` to 0.95, and the `top_k` to 100\. Check if the
    generated passage is grammatically correct and if any part of the text is directly
    copied from the training text.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练了40个epoch的模型生成包含50个新token的文本段落。使用“老人看到鲨鱼靠近的”作为提示；设置`random seed`为42，`temperature`为0.95，`top_k`为100。检查生成的段落是否语法正确，以及文本的任何部分是否直接复制自训练文本。
- en: In this chapter, you’ve learned how to construct and train a GPT-style Transformer
    model from the ground up. Specifically, you’ve created a simplified version of
    the GPT-2 model with only 5.12 million parameters. Using three novels by Ernest
    Hemingway as training data, you have successfully trained the model. You have
    also generated text that is coherent and stylistically consistent with Hemingway’s
    writing.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何从头开始构建和训练一个GPT风格的Transformer模型。具体来说，你创建了一个只有512万个参数的GPT-2模型的简化版本。使用欧内斯特·海明威的三部小说作为训练数据，你成功训练了该模型。你还生成了与海明威的写作风格一致且连贯的文本。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The style of the generated text from a GPT model will be heavily influenced
    by the training data. For effective text generation, it’s important to have a
    balance of text length and variation in the training material. The training dataset
    should be sufficiently large for the model to learn and emulate a specific writing
    style accurately. However, if the dataset lacks diversity, the model might end
    up reproducing passages directly from the training text. Conversely, overly long
    training datasets can require excessive computational resources for training.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 GPT 模型生成的文本风格将受到训练数据的高度影响。为了有效地生成文本，训练材料中文本长度和变化的平衡非常重要。训练数据集应该足够大，以便模型能够准确学习并模仿特定的写作风格。然而，如果数据集缺乏多样性，模型可能会直接从训练文本中复制段落。相反，过长的训练数据集可能需要过多的计算资源进行训练。
- en: Choosing the right hyperparameters in the GPT model is crucial for successful
    model training and text generation. Setting the hyperparameters too large may
    lead to too many parameters. This results in longer training time and an overfitted
    model. Setting the hyperparameters too small may hinder the model’s ability to
    learn effectively and capture the writing style in the training data. This may
    lead to incoherent generated text.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GPT 模型中选择合适的超参数对于成功的模型训练和文本生成至关重要。设置超参数过大可能会导致参数过多，这会导致训练时间更长，模型过拟合。设置超参数过小可能会阻碍模型有效学习并捕捉训练数据中的写作风格。这可能会导致生成的文本不连贯。
- en: The appropriate number of training epochs is important for text generation.
    While training too few epochs may lead to incoherent text, training for too many
    epochs may lead to an overfitted model that generates text identical to passages
    in the training text.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的合适轮数对于文本生成至关重要。训练轮数过少可能导致生成的文本不连贯，而训练轮数过多可能会导致模型过拟合，生成的文本与训练文本中的段落完全相同。
