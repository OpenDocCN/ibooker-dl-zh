- en: 8 Designing solutions with large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用大型语言模型设计解决方案
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using retrieval augmented generation to reduce errors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检索增强生成来减少错误
- en: How LLMs can supervise humans to mitigate automation bias
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM如何监督人类以减轻自动化偏差
- en: Enabling classic machine learning tools with embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过嵌入启用经典机器学习工具
- en: Ways to present LLMs that are mutually beneficial to companies and users
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对公司和用户都有益的展示LLM的方法
- en: By now you should have a strong understanding of LLMs and their capabilities.
    They produce text that is very similar to human text because they are trained
    on hundreds of millions of human text documents. The content they produce is valuable
    but also subject to errors. And, as you know, you can mitigate these errors by
    incorporating domain knowledge or tools like parsers for computer source code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '到现在为止，你应该对LLMs及其能力有了深刻的理解。它们产生的文本非常类似于人类文本，因为它们是在数十亿份人类文本文档上训练的。它们产生的内容很有价值，但也可能存在错误。而且，正如你所知，你可以通过结合领域知识或像解析器这样的工具来减轻这些错误。 '
- en: 'Now you are ready to design a solution using an LLM. How do you consider everything
    we have discussed thus far and convert it into an effective implementation plan?
    This chapter will walk you through the process, trade-offs, and considerations
    in designing that plan. To do so, we will use a running example that we can all
    relate to: contacting tech support when help is needed.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好使用LLM来设计解决方案了。你如何考虑到目前为止我们所讨论的所有内容，并将其转化为一个有效的实施计划？本章将引导你了解设计该计划的过程、权衡和考虑因素。为此，我们将使用一个我们都能相关联的运行示例：在需要帮助时联系技术支持。
- en: 'First, we will consider the obvious path: building a chatbot. Chatbots are
    the vehicle that introduced many people to LLMs because generally, they can do
    an excellent job of generating output interactively. We’ll evaluate the risks
    of deploying an LLM-powered chatbot in a customer service scenario. Through this
    discussion, you’ll see that using an LLM can increase risk compared to other options.
    However, a simple chatbot may be a valid option if the risks are sufficiently
    minimal.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将考虑显而易见的路径：构建一个聊天机器人。聊天机器人是许多人了解大型语言模型（LLMs）的媒介，因为它们通常能够很好地进行交互式输出。我们将评估在客户服务场景中部署由LLM驱动的聊天机器人的风险。通过这次讨论，你会发现使用LLM相比其他选项可能会增加风险。然而，如果风险足够小，一个简单的聊天机器人可能是一个有效的选择。
- en: Next, we will explore ways to manage the risks by using application designs
    that improve how customers interact with the LLM. We’ll discuss how having a person
    check each output produced by an LLM is fraught with problems due to a phenomenon
    known as automation bias. We’ll discuss how automation bias can be somewhat counterintuitively
    avoided by having the LLM supervise the person instead. We’ll explore how an LLM’s
    embeddings, the semantic representation of text encoded as numbers, can be combined
    with classical machine learning algorithms to address this risk and handle tasks
    that an LLM can’t perform independently.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨通过使用改进客户与LLM交互的应用设计来管理风险的方法。我们将讨论由于自动化偏差这一现象，让一个人检查LLM产生的每个输出充满了问题。我们将讨论如何通过让LLM监督人来在一定程度上避免自动化偏差。我们将探讨如何将LLM的嵌入（文本编码为数字的语义表示）与经典机器学习算法相结合，以应对这一风险并处理LLM无法独立执行的任务。
- en: Finally, we’ll investigate how technology is presented to users and plays a
    vital role in establishing trust and conveying an understanding of its inner workings.
    We’ll discuss the area of “explainable AI,” where a machine learning algorithm
    produces output that describes or explains how it arrived at a specific output.
    Explainable AI is often the approach adopted to handle situations where people
    need to understand how an LLM works, but studies show that although explainability
    may shed some light on the inner workings of LLMs by describing the behavior of
    these models in human terms, it does not tend to help for its own sake. Instead,
    we’ll describe the benefits of focusing on transparency, aligning incentives with
    customers, and creating feedback cycles to design solutions that better meet the
    needs of both the companies that employ them and the customers that interact with
    them by providing accurate output and creating efficiencies in business processes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨技术如何呈现给用户，并在建立信任和传达其内部工作原理方面发挥关键作用。我们将讨论“可解释人工智能”这一领域，其中机器学习算法产生描述或解释其如何到达特定输出的输出。可解释人工智能通常被采用来处理人们需要了解LLM如何工作的情况，但研究表明，尽管可解释性可能通过用人类术语描述这些模型的行为来揭示LLM的内部工作原理，但它并不总是为了自身的目的而有所帮助。相反，我们将描述关注透明度、与客户利益一致以及创建反馈循环以设计解决方案的好处，这些解决方案能够更好地满足雇用它们的公司和与之互动的客户的需求，通过提供准确的输出并在业务流程中创造效率。
- en: 8.1 Just make a chatbot?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 只做一个聊天机器人？
- en: Unsurprisingly, many people are building chatbots using LLMs based on transformer
    architectures, the same technology that underpins ChatGPT. It’s an obvious and
    seemingly reasonable first step. ChatGPT’s fantastic ability to interact with
    people, adapt to conversations, and retrieve and present information demonstrates
    how well LLM technology supports customer interaction applications. With the advent
    and availability of LLMs, it would likely be short-sighted to attempt to implement
    a customer service agent using any other approach, such as using an expert system
    trained to use a decision tree of canned responses. When an unhappy customer has
    some technical problem, instead of searching an online Frequently Asked Questions
    (FAQ) document, sending an email into the black hole of a trouble ticket system,
    or calling a phone number with an automated interactive voice response system,
    they can start directly interacting with an AI-powered tool and make progress
    on getting their problems solved. This sounds wonderful on paper, and if you draw
    a little diagram like figure [8.1](#fig__hci_chatbot), it sure looks like we are
    simplifying life.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，许多人正在使用基于transformer架构的LLM来构建聊天机器人，这正是ChatGPT所依赖的技术。这是一个明显且看似合理的第一步。ChatGPT与人类互动、适应对话、检索和呈现信息的出色能力展示了LLM技术在客户交互应用中的支持效果。随着LLM的出现和可用性，尝试使用任何其他方法（如使用训练有素的专家系统，该系统使用决策树和预设响应）来实现客户服务代理可能过于短视。当不满意的客户遇到一些技术问题时，他们可以直接与一个AI驱动的工具进行交互，以解决他们的问题，而不是搜索在线常见问题解答（FAQ）文档，将电子邮件发送到工单系统的黑洞中，或者拨打一个具有自动交互语音响应系统的电话号码。这听起来在纸上很美好，如果你画一个像图[8.1](#fig__hci_chatbot)那样的简单图，确实看起来我们正在简化生活。
- en: '![figure](../Images/CH08_F01_Boozallen.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_F01_Boozallen.png)'
- en: Figure 8.1 When looking at the process diagram, it would seem like replacing
    FAQs, email tickets, and support numbers could be simplified and streamlined with
    an LLM-based chatbot. However, the folly of this view is that the process is incomplete.
    The potential errors and remediation processes required to ensure an LLM will
    perform accurately are hidden and create more complexity.
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 当查看流程图时，似乎用基于LLM的聊天机器人来替换FAQ、电子邮件工单和支持电话号码可以简化并优化流程。然而，这种观点的愚蠢之处在于流程是不完整的。确保LLM能够准确执行所需的潜在错误和补救措施是隐藏的，并增加了复杂性。
- en: There are certainly cases where a chatbot is a good idea. But surprisingly,
    an online LLM-based chatbot that handles support probably is not at the top of
    the list of customer support tools for most companies because of the effort required
    to build a system that will be accurate and reliable in many cases and not create
    unexpected output when confronted with unexpected input. Ultimately, the decision
    to use an LLM to implement a customer support chatbot comes down to our ongoing
    discussion of the errors an LLM might make when generating customer responses.
    We know that LLMs are not error-free, and while machine learning is sometimes
    practical, the expense of those potential errors is the primary decision criterion
    when considering deploying this technology. Fundamentally, using an LLM potentially
    increases the cost of those errors. The bottom line is that in their current form,
    LLMs can provide incorrect answers, and the liability for these falls on the shoulders
    of the companies or individuals who deploy and maintain them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有些情况下使用聊天机器人是个好主意。但令人惊讶的是，由于需要构建一个在许多情况下准确可靠且不会在遇到意外输入时产生意外输出的系统，基于在线LLM的聊天机器人可能并不是大多数公司客户支持工具列表上的首选。最终，决定使用LLM来实现客户支持聊天机器人的决策，归结于我们对LLM在生成客户响应时可能犯的错误持续讨论。我们知道LLM并非没有错误，虽然机器学习有时是实用的，但在考虑部署这项技术时，潜在错误的成本是主要的决策标准。从根本上说，使用LLM可能会增加这些错误的成本。底线是，在当前形式下，LLM可能会提供错误的答案，而这些错误的责任落在部署和维护它们的公司或个人身上。
- en: Executives or product managers might consider the cost of errors in the context
    of a few classic business key performance indicators. For example, customer retention
    rates might decrease if they entrust support to chatbots. Perhaps the retention
    rate would be higher than if the customer relations functions were outsourced
    to a call center in another country. Indeed, these considerations are important
    to evaluate, and you should probably do a trial deployment to see what customers
    think before replacing your customer support function with an LLM wholesale.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 高管或产品经理可能会在几个经典商业关键绩效指标（KPI）的背景下考虑错误的成本。例如，如果他们把客户支持委托给聊天机器人，客户保留率可能会下降。也许如果客户关系职能外包给另一个国家的呼叫中心，保留率会更高。确实，这些考虑因素很重要，你可能会在用大型语言模型（LLM）全面替换客户支持功能之前，先进行一次试验部署，看看客户对它们的看法。
- en: Note We almost always recommend trial deployments of any machine learning system.
    The investing adage “Past performance is not a guarantee of future returns” is
    true of any AI. One way to do this is through *phantom deployments*, where you
    run your new AI system alongside the existing process for some weeks or months.
    You may choose to ignore its outcomes while the existing business processes are
    in place. This gives you time to observe the discrepancies between your current
    and new processes, identify and address problems, and determine whether the performance
    of the machine learning system degrades over time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们几乎总是推荐对任何机器学习系统进行试验部署。投资格言“过去的业绩并不能保证未来的回报”适用于任何人工智能。一种方法是进行“幽灵部署”，即在几周或几个月内，将您的新AI系统与现有流程并行运行。在现有业务流程存在的情况下，您可以选择忽略其结果。这为您提供了时间来观察当前流程和新流程之间的差异，识别和解决问题，并确定机器学习系统的性能是否会随时间退化。
- en: Most critically, your LLM can give advice that causes harm to your users. Since
    an LLM is not a person who can be held legally liable for their actions, you and
    your company will be held liable instead. This has already happened with an airline
    that deployed a chatbot that gave errant policy statements. A court decided that
    the company had to abide by the policy incorrectly generated and shared by their
    chatbot [1].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，您的LLM可能会提供可能对您的用户造成伤害的建议。由于LLM不是一个可以对其行为承担法律责任的人，因此您和您的公司将承担责任。这已经在一家部署了给出错误政策声明的聊天机器人的航空公司中发生过。法院裁定，该公司必须遵守其聊天机器人错误生成和分享的政策[1]。
- en: We recommend always considering an *adversarial* mindset when deploying an LLM.
    Asking “What could a motivated bad actor do if they knew how this worked?” will
    help you identify and mitigate significant risks and is often the best way to
    determine whether your intended LLM application is a good or bad idea. For example,
    a car company integrated an LLM into their website to help sell cars and answer
    questions. After realizing this, it took less than a day for users to convince
    the website to sell them a car for just $1 [2].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在部署大型语言模型（LLM）时始终考虑一种**对抗性**的思维模式。问自己“如果有人知道它是如何工作的，一个有动机的恶意行为者会做什么？”将帮助你识别和减轻重大风险，并且通常是确定你打算使用的LLM应用是好是坏的最佳方式。例如，一家汽车公司将其网站集成LLM以帮助销售汽车并回答问题。在意识到这一点后，用户仅用不到一天的时间就说服网站以仅1美元的价格卖给他们一辆车[2]。
- en: 'If the potential cost or risk of errors is low, you can feel comfortable deploying
    an LLM chatbot if you so choose. But for the sake of this chapter, let us assume
    that this technical support agent we are hypothesizing is very important, and
    the mistakes it makes could cost the company a lot of money. The question now
    becomes: How do we design a solution that gives us benefits in productivity and
    efficiency yet limits users’ direct access to an LLM? If you are new to AI/ML
    and a chatbot is your primary exposure to the field, this might sound like a contradiction,
    but there are some easy, repeatable design patterns you can apply to do this.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果潜在的成本或错误风险较低，如果你选择的话，可以放心地部署LLM聊天机器人。但为了本章的目的，让我们假设我们假设的这个技术支持代理非常重要，它犯的错误可能会给公司造成大量损失。现在的问题变成了：我们如何设计一个既能带来生产力和效率上的好处，又能限制用户直接访问LLM的解决方案？如果你是AI/ML的新手，并且聊天机器人是你对这个领域的首次接触，这可能会听起来像是一种矛盾，但有一些简单、可重复的设计模式你可以应用来实现这一点。
- en: 8.2 Automation bias
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 自动化偏差
- en: A common approach to addressing the risk of using LLMs for direct customer interactions
    is to have the LLM interact with support staff or technicians instead. This is
    often referred to as “human in the loop” because there’s a person who is reviewing
    the feedback loop between the LLM and the customer, providing a critical assessment
    of the automated system’s output, and intervening and adjusting the output when
    they detect an error. The technician will still be employed, but we will increase
    their efficiency by having the LLM generate an initial response to each question
    from a user and a technician curating those responses to ensure that they are
    accurate and relevant. If the LLM generates a potentially costly or incorrect
    response, our trusty technicians will intervene and reply with something more
    appropriate. In this context, it is ultimately up to the technician to choose
    the proper authoritative response.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 针对使用LLM进行直接客户互动的风险，一种常见的应对方法是将LLM与支持人员或技术人员进行交互。这通常被称为“人工在环”，因为有一个人在审查LLM与客户之间的反馈循环，对自动化系统的输出进行关键评估，并在发现错误时介入和调整输出。技术人员仍然会被雇佣，但我们将通过让LLM为每个用户的问题生成初始响应，并由技术人员整理这些响应以确保其准确性和相关性来提高他们的效率。如果LLM生成可能代价高昂或错误的响应，我们可靠的技术人员将介入并回复更合适的内容。在这种情况下，最终由技术人员选择适当的权威性回复。
- en: The clever reader who remembers our discussion about retrieval augmentedgeneration
    (RAG) from chapter [5](../Text/chapter-5.html) might even identify ways to improve
    upon this idea. You’ll say, “Ah, we can put all our training manuals and documentation
    inside a database, and then we can use RAG so that the LLM can retrieve the most
    relevant information to a user’s question.”. This approach is outlined in figure
    [8.2](#fig__human_in_loop_naive_rag), which shows a process where a user’s questions
    are first sent to the LLM to focus output generation using a collection of known
    answers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在第[5](../Text/chapter-5.html)章讨论检索增强生成（RAG）的聪明读者甚至可能识别出改进这一想法的方法。你会说：“啊，我们可以把所有的培训手册和文档都放在数据库里，然后我们可以使用RAG，这样LLM就可以检索到与用户问题最相关的信息。”这种方法在图[8.2](#fig__human_in_loop_naive_rag)中进行了概述，该图显示了用户的问题首先被发送到LLM，以使用一组已知答案来集中生成输出。
- en: '![figure](../Images/CH08_F02_Boozallen.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F02_Boozallen.png)'
- en: Figure 8.2 A naive approach toward implementing a “human in the loop” system
    that uses an LLM paired with a database of relevant information to produce output
    that is ultimately reviewed by and possibly corrected by a human worker
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 一种实现“人工在环”系统的天真方法，该系统使用LLM与相关信息的数据库配对以生成输出，最终由人工工作者进行审查，并可能进行纠正
- en: The RAG approach will likely mitigate a lot of risk, but it also has the potential
    to hit the pitfall of *automation bias*. Automation bias refers to the fact that
    people, in general, tend to pick automated or default choices presented by a system
    because it is easier than applying critical thinking to determine which choice
    is most appropriate to the situation at hand. If a system works well and does
    not need you to intervene often, it becomes incredibly challenging to remain hypervigilant
    and detect the occasional error. The paradox is that if the system is so inaccurate
    in its suggestions that you can maintain your vigilance, the chances are good
    that the system is slowing you down when compared to directly answering questions
    using no automation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RAG方法可能会减轻很多风险，但也可能陷入*自动化偏差*的陷阱。自动化偏差指的是人们普遍倾向于选择系统提供的自动化或默认选项，因为这比应用批判性思维来确定哪种选择最适合当前情况要容易得多。如果一个系统运行良好且不需要你经常干预，那么保持高度警惕并检测偶尔的错误就变得极其困难。悖论在于，如果系统在建议上的不准确程度足以让你保持警惕，那么系统在直接回答问题时可能比使用无自动化工具要慢。
- en: 'This is where trial or phantom deployments become incredibly important. If
    your system is so accurate that automation bias is the real source of risk, you
    have two options that do not require deviating from the “human in the loop” design:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是试验或幻影部署变得极其重要的地方。如果你的系统如此准确，以至于自动化偏差是真正的风险来源，你有两个选项，不需要偏离“人工在环”的设计：
- en: Add an “escape to a human” path to the pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将“逃逸到人工服务”的路径添加到管道中
- en: Mitigate the risk of errors externally via process changes
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过流程变更来外部减轻错误风险
- en: The first point is pretty straightforward. Eventually, a novel situation will
    occur that the LLM cannot answer. In this case, it would be best to provide a
    way for a customer to “escape” from an infinite loop with a computer to get to
    a higher tier of support. This could be a maximum conversation length measured
    in the number of messages exchanged or the amount of time spent chatting, an option
    to contact a human representative that appears based on multiple failed attempts
    to communicate, or other possible designs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一点相当直接。最终，会出现LLM无法回答的新情况。在这种情况下，最好提供一种让客户能够“逃逸”出与计算机的无穷循环，以获得更高层次支持的方法。这可能是一个以消息交换次数或聊天时间衡量的最大对话长度，或者在多次尝试沟通失败后出现联系人工代表的选项，或其他可能的设计。
- en: Note Suppose you are going to do the work to create an RLHF or SFT dataset to
    fine-tune your LLM to your situation as we discussed in chapter [5](../Text/chapter-5.html).
    In that case, you can even add training examples where the LLM’s expected response
    is “I’m sorry, this situation sounds more complex than what I can assist with;
    allow me to get a human to help.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：假设你打算做创建RLHF或SFT数据集的工作，以微调你的LLM以适应你的情况，正如我们在第[5](../Text/chapter-5.html)章中讨论的那样。在这种情况下，你甚至可以添加训练示例，其中LLM的预期响应是“很抱歉，这种情况听起来比我能够协助的要复杂；让我找个人来帮忙。”
- en: 8.2.1 Changing the process
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 改变流程
- en: The second suggestion, changing the process, is not as difficult as it may sound.
    If one of your bosses has an MBA, they are (allegedly) trained to think in these
    terms. (One of the authors has an MBA, so it is OK for us to say that.) For example,
    interactions with the chatbot could include a caveat about any outcome requiring
    “a human’s final approval.” In this case, having the entire conversation reviewed
    by a person is far less of an automation bias risk than requiring someone to maintain
    constant vigilance throughout a continuous conversation. Ultimately, adversarial
    users know a human is going to check and so are demotivated from trying to game
    the system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个建议，改变流程，听起来可能并不困难。如果你的老板之一有MBA学位，他们（据称）受过这方面的训练。（我们的一位作者有MBA学位，所以我们可以说这是可以的。）例如，与聊天机器人的互动可以包括任何需要“人工最终批准”的任何结果的免责声明。在这种情况下，让整个对话由人审查，比要求某人在整个连续对话中保持持续警惕要小得多自动化偏差风险。最终，对抗性用户知道有人会检查，因此他们不太可能试图操纵系统。
- en: Depending on the context, preventing adversarial use of an LLM can be achieved
    by requiring the user to provide collateral to ensure they act in good faith.
    For example, you could take actions equivalent to putting a hold on the user’s
    credit card as a kind of insurance against bad-faith interactions. Such a hold
    would be released when the transaction is completed successfully. You could also
    limit how much of the process is automated, require authentication, or randomize
    how often people are routed to a human versus an AI so that it becomes unpredictable
    when a situation that could be exploited will arise.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文，通过要求用户提供抵押品以确保他们诚信行动，可以防止LLM的对抗性使用。例如，你可以采取相当于冻结用户信用卡的行动，作为一种对不良诚信互动的保险。当交易成功完成时，这种冻结将被解除。你也可以限制自动化过程的程度，要求身份验证，或者随机化人们被路由到人类还是AI的频率，以便在可能被利用的情况出现时变得不可预测。
- en: All of these actions will depend on your specific application, the risks, the
    tolerance of those risks, and the nature of your users. Some customers might be
    turned off by a credit hold and be upset. Or maybe you frame it as an optional
    method in which the user gets $2 off their bill if an AI system successfully helped
    them with their problem, presuming that it is less than what the old system would
    have cost per call. Either way, it is case by case and will depend on your creativity
    to manage the risk.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些行动都将取决于你的具体应用、风险、对风险的容忍度以及你用户的性质。一些客户可能会因为信用冻结而感到沮丧。或者，你可能将其作为一项可选方法，如果AI系统成功帮助用户解决问题，用户可以从账单中节省2美元，前提是这比旧系统每通电话的成本要低。无论如何，这都是个案，将取决于你的创造力来管理风险。
- en: 8.2.2 When things are too risky for autonomous LLMs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 当事情对自主LLM来说风险太高时
- en: So now you have done a trial deployment, evaluated the risks and your users’
    adversarial proclivities, and concluded that it is too risky for LLMs to provide
    the initial answers. How could an LLM still provide some level of efficiency?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经进行了试验部署，评估了风险和用户对抗性倾向，并得出结论，LLM提供初始答案的风险太高。LLM如何仍然提供一定程度的效率呢？
- en: An unintuitive approach is to have the LLM check the person rather than the
    person check the LLM. This may sound strange. Why would we let the LLM supervise
    if we cannot trust it to act alone? To consider this further, imagine you have
    an LLM system in this supervisory role, checking each response, as shown in figure
    [8.3](#fig__HCI_LLM_supervise_human).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不直观的方法是让大型语言模型（LLM）检查人，而不是让人检查LLM。这听起来可能很奇怪。如果我们不能信任LLM独立行动，我们为什么要让它进行监督？为了进一步考虑这个问题，想象一下你有一个LLM系统在这个监督角色中，检查每个响应，如图[8.3](#fig__HCI_LLM_supervise_human)所示。
- en: If the LLM and the person are correct, action will be taken, and the message
    will be relayed to the customer. It will be as if the user is chatting with the
    technician. But if the technician and the LLM disagree on the answer, we can prompt
    the technician to double-check their response before sending it to the user.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM和人都正确，就会采取行动，并将消息传达给客户。这就像用户在与技术人员聊天一样。但如果技术人员和LLM对答案有分歧，我们可以提示技术人员在将答案发送给用户之前进行双重检查。
- en: '![figure](../Images/CH08_F03_Boozallen.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F03_Boozallen.png)'
- en: Figure 8.3 Notice that the direction of the arrows in this diagram has changed
    from figure [8.2](#fig__human_in_loop_naive_rag). Everything goes to a human first,
    and we use LLMs to catch mistakes before they happen.
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3 注意到这个图中的箭头方向与图[8.2](#fig__human_in_loop_naive_rag)不同。所有内容都首先发送给人类，我们使用LLM在错误发生之前捕捉错误。
- en: This double-check could be as simple as telling the technician, “Hey, this looks
    like it may be abnormal for a solution; please confirm before sending.” You could
    try having the LLM produce its own suggested alternative. Or you could keep the
    LLM out of the process and use it to notify a more experienced technician to join
    the process and assist. Regardless of how this is structured, the purpose is to
    signal that there may be a risk of a negative customer interaction, such as an
    incorrect answer. While this risk existed previously, we now have a chance to
    mitigate it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种双重检查可能只是简单地告诉技术人员，“嘿，这个解决方案看起来可能不正常；请在发送前确认。”你可以尝试让LLM生成它自己的建议替代方案。或者，你可以让LLM不参与这个过程，并使用它来通知一个更有经验的技术人员加入过程并提供帮助。无论这种结构如何，目的都是为了表明可能存在负面客户互动的风险，比如错误的答案。虽然这种风险以前就存在，但现在我们有机会减轻它。
- en: 'Additionally, because we are considering human-initiated customer support errors,
    we are generally not taking on any new risk because a support representative acting
    alone could just as easily make a mistake. So if the LLM and human are both wrong
    simultaneously, you were already doomed to make that process error anyway. Such
    is life. Technically, we could argue that technicians could question their responses
    too much based on an LLM’s assessment of their interactions, thus reducing efficiency.
    Additionally, an overly sensitive LLM may ask technicians to double-check their
    work too often, which would cause alert fatigue that could lead to technicians
    ignoring the LLM suggestions entirely. If your use case is prone to these sorts
    of problems, that fact will be uncovered during trial deployments that provide
    context-specific feedback on how an LLM should be tuned to address this problem.
    The general caveat that applies to all machine learning is especially important
    here: always test; do not assume.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为我们正在考虑由人类发起的客户支持错误，所以我们通常不会承担任何新的风险，因为单独行动的支持代表同样可能犯错。所以如果LLM和人类同时出错，你本来就已经注定要犯那个过程错误了。这就是生活。从技术上讲，我们可以争论技术人员可能会过多地根据LLM对他们的互动评估来质疑他们的回答，从而降低效率。此外，过于敏感的LLM可能会频繁要求技术人员复查他们的工作，这会导致警报疲劳，可能导致技术人员完全忽略LLM的建议。如果你的用例容易遇到这类问题，那么在提供关于如何调整LLM以解决此类问题的特定上下文反馈的试验部署过程中，这一事实将会被发现。适用于所有机器学习的普遍警告在这里尤其重要：总是测试；不要假设。
- en: 'Employing an LLM to double-check human performance can reduce errors in the
    process as a whole. It may not seem like this approach makes anything faster because
    humans are still generating the initial response. However, this approach still
    creates opportunities for increased efficiency:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM来复查人类表现可以减少整个过程中的错误。虽然这种方法看起来并没有使任何事情变得更快，因为人类仍在生成初始回复，但这种方法仍然创造了提高效率的机会：
- en: It can reduce the conversation length by helping to catch errors and reach a
    solution faster.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过帮助捕捉错误和更快地解决问题来缩短对话长度。
- en: It can identify staff who need more training or information to answer customer
    questions or recognize when specific error situations occur.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以识别需要更多培训或信息来回答客户问题或识别特定错误情况发生的员工。
- en: It may help avoid escalation to more costly levels of support or managers,reducing
    the frequency and cost of troublesome customers.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能有助于避免升级到更昂贵的支持级别或经理，从而减少麻烦客户的频率和成本。
- en: 8.3 Using more than LLMs to reduce risk
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 使用LLM以外的工具来降低风险
- en: Everything we have discussed has involved a “fight fire with fire” approach
    in which, although there are risks to using LLMs, we have considered different
    ways to use LLMs to mitigate those risks. While we’ve changed how we use the LLM,
    the LLM is still the primary component. Alternatively, we can consider using tools
    other than LLMs to address our design challenges. Other approaches in the scope
    of generative AI, such as text-to-speech and speech-to-text, can be used to build
    more accessible or simply convenient user experiences. For example, users with
    arthritis or low vision may greatly prefer a phone call over typing responses
    into a chatbot prompt window.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所讨论的一切都涉及一种“以火攻火”的方法，即在考虑使用LLM（大型语言模型）存在风险的情况下，我们考虑了不同的使用LLM来减轻这些风险的方法。虽然我们已经改变了使用LLM的方式，但LLM仍然是主要组件。或者，我们可以考虑使用除LLM之外的工具来解决我们的设计挑战。在生成式AI的范围内，其他方法，如文本转语音和语音转文本，可以用来构建更易于访问或更方便的用户体验。例如，患有关节炎或视力低下的用户可能更愿意通过电话通话而不是在聊天机器人提示窗口中键入回复。
- en: If we think about our customer service problem and when LLMs work well, we will
    discover that the ingredients for a broader class of tools are also available.
    LLMs work best when there is repetition in scenarios where problems reoccur and
    formulaic solutions and responses can be given. LLMs are very flexible in recognizing
    broad patterns in the fuzzy nature of language. If the LLM can correctly interpret
    a user’s problem, and there is a known solution, it can potentially walk a user
    through that solution. This might sound much like an unsupervised chatbot, but
    the critical distinction is that in the cases where the LLM takes a subordinate
    role in the solution, the output was ultimately generated by customer support
    technicians, as described in figure [8.3](#fig__HCI_LLM_supervise_human).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑我们的客户服务问题以及 LLM 工作得好的时候，我们会发现更广泛类别工具的成分也同样是可用的。LLM 在场景中表现最佳，在这些场景中，问题会重复出现，并且可以给出公式化的解决方案和响应。LLM
    在识别语言模糊性中的广泛模式方面非常灵活。如果 LLM 可以正确解释用户的问题，并且存在已知的解决方案，它可能能够引导用户通过该解决方案。这听起来可能很像一个无监督的聊天机器人，但关键的区别在于，在
    LLM 在解决方案中扮演从属角色的案例中，输出最终是由客户支持技术人员生成的，如图 [8.3](#fig__HCI_LLM_supervise_human)
    所描述。
- en: This section will also discuss how we can use classic machine learning techniques,
    such as classification, to tackle existing problems. We can do this by using the
    knowl-edge within LLMs to enable machine learning techniques by producing embeddings
    of the user’s text.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节还将讨论我们如何利用经典的机器学习技术，例如分类，来解决现有问题。我们可以通过使用 LLM 中的知识来生成用户的文本嵌入，从而实现机器学习技术。
- en: 8.3.1 Combining LLM embeddings with other tools
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 将 LLM 嵌入与其它工具结合
- en: In chapter [3](../Text/chapter-3.html), we described how an LLM transforms tokens
    into embeddings, which are vectors that encode a semantic representation of the
    meaning of each token as a series of numbers. These vector embeddings are useful
    in other ways outside the context of LLM’s transformer architecture. While vector
    embeddings are essential for making the LLM operate, they are themselves an extraordinarily
    useful tool.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3](../Text/chapter-3.html) 章中，我们描述了 LLM 如何将标记转换为嵌入，这些嵌入是向量，它们将每个标记的意义作为一个数字序列进行编码。这些向量嵌入在
    LLM 的变压器架构之外也有其他用途。虽然向量嵌入对于使 LLM 运作至关重要，但它们本身也是一个极其有用的工具。
- en: The semantic nature of the vectors produced by LLMs is important because hundreds
    of other practical machine learning algorithms operate on vector representations.
    LLMs are essentially a very powerful way of converting complex human language
    text into a form compatible with the rest of the machine learning field. Utilizing
    the vector outputs of LLMs with other algorithms has been such an extraordinarily
    useful strategy that practitioners will describe it as “creating embeddings.”
    The description comes from the idea that the LLM is taking one representation
    (human text) and embedding it into another representation (a mathematical vector).
    Because these numbers encode information about the original text, you can plot
    them like numbers and see that similar texts end up in similar locations on the
    plot, as shown in figure [8.4](#fig__embeddings_are_useful).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 产生的向量语义性质很重要，因为数百个其他实用的机器学习算法在向量表示上操作。LLM 实质上是一种将复杂的人类语言文本转换为与机器学习领域其他部分兼容形式的一种非常强大的方式。利用
    LLM 的向量输出与其他算法相结合已被证明是一种极其有用的策略，从业者将其描述为“创建嵌入”。这种描述来源于 LLM 将一种表示（人类文本）嵌入到另一种表示（数学向量）中的想法。因为这些数字编码了关于原始文本的信息，你可以像数字一样绘制它们，并看到相似文本最终在图表上的相似位置，如图
    [8.4](#fig__embeddings_are_useful) 所示。
- en: '![figure](../Images/CH08_F04_Boozallen.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F04_Boozallen.png)'
- en: Figure 8.4 LLMs produce numeric vectors known as embeddings as an intrinsic
    part of their function-ing. The utility of these embeddings is dependent on the
    fact that these numbers only change a little bit when given similar text. The
    two example tests here will have similar embeddings, and thus, their plots look
    similar, even though they don’t share any of the same words. This is a powerful
    feature that was present in older machine learning techniques.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4 LLM 在其功能中产生称为嵌入的数值向量。这些嵌入的效用取决于这些数字在给定相似文本时只发生微小变化的事实。这里提供的两个示例测试将具有相似的嵌入，因此它们的图表看起来相似，尽管它们没有共享任何相同的单词。这是在较老的机器学习技术中存在的一个强大功能。
- en: 'Let’s look at a quick description of four types of machine learning algorithms
    you can use once you have embeddings. We consider each type of machine learning
    to be particularly useful for most real-world use with LLMs; we will also note
    some popular algorithms you can find that are relatively reliable and easy to
    use. The critical takeaway is that if you break out of the mindset that only an
    LLM can solve a problem, a more extensive set of tools becomes available to you.
    This list is your starting map for some of those tools:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下四种类型的机器学习算法，您在拥有嵌入后可以使用。我们认为每种类型的机器学习对于大多数使用LLM的现实世界应用都特别有用；我们还将注意一些相对可靠且易于使用的流行算法。关键要点是，如果您跳出只有LLM才能解决问题的思维定式，您将获得更广泛的一套工具。这个列表是您开始探索这些工具的起点图：
- en: '*Clustering algorithms*—Grouping texts by similarity to each other that are
    distinct from the larger amount of text available (e.g., used for market segment
    analysis). Popular algorithms include k-means and HDBSCAN.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类算法*—根据彼此之间的相似性对文本进行分组，这些文本与大量可用的文本不同（例如，用于市场细分分析）。流行的算法包括k-means和HDBSCAN。'
- en: '*Outlier detection*—Finding texts that are dissimilar from essentially all
    other texts available (i.e., finding contrarian customers or novel problems).
    Popular algorithms include Isolation Forests and Local Outlier Factor (LoF).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测*—寻找与几乎所有其他文本都不同的文本（即，寻找持不同意见的客户或新颖的问题）。流行的算法包括隔离森林和局部离群因子（LoF）。'
- en: '*Information visualization*—Creating a 2D plot of your data to allow visual
    inspection/exploration, especially when combined with interactive tools (i.e.,
    data exploration). Popular algorithms include UMAP and PCA.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息可视化*—创建数据的2D图，以便进行视觉检查/探索，尤其是在结合交互式工具（即，数据探索）时。流行的算法包括UMAP和PCA。'
- en: '*Classification and regression*—If you label your old texts with known outcomes
    (e.g., net promoter score rating), you can use classification (i.e., pick one
    of A, B, or C) or regression (i.e., predict a continuous number like 3.14 or 42)
    to predict what the score would be on a new text (i.e., data categorization and
    value prediction). Using embeddings as input for simple algorithms like logistic
    regression and linear regression works well for classification or regression,
    respectively.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类和回归*—如果您用已知的成果（例如，净推荐者评分）标记旧文本，您可以使用分类（即，选择A、B或C中的一个）或回归（即，预测一个连续的数字，如3.14或42）来预测新文本的评分（即，数据分类和值预测）。使用嵌入作为简单算法（如逻辑回归和线性回归）的输入，对于分类或回归都很有用。'
- en: Note Embeddings are not something new that was invented as a part of LLMs. An
    algorithm known as Word2Vec, which could embed single words, popularized embeddings
    as a go-to strategy for representing the meaning in text back in 2013\. Despite
    this, LLMs tend to produce embeddings with greater utility than other older algorithms.
    However, an LLM is far more computationally demanding than older algorithms like
    Word2Vec. For this reason, you may want to use an older or faster algorithm for
    this task. The existence of generative AI methods in images, video, and speech
    means you can also use embeddings for domains such as images, video, and speech
    in addition to text.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意嵌入并不是作为LLM的一部分新发明的。一个名为Word2Vec的算法，它可以嵌入单个单词，在2013年将嵌入推广为表示文本意义的首选策略。尽管如此，LLM倾向于产生比其他较老算法更有用的嵌入。然而，LLM的计算需求远大于Word2Vec等较老算法。因此，您可能希望使用较老或更快的算法来完成这项任务。图像、视频和语音中生成AI方法的存在意味着您还可以在文本之外，为图像、视频和语音等域使用嵌入。
- en: 8.3.2 Designing a solution that uses embeddings
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 设计使用嵌入的解决方案
- en: Now that we have described the concept of embeddings and how they offer us more
    tools, let’s build an enhanced tech-support call center solution. We will continue
    to use LLMs for their text-generating capability and their embeddings and incorporate
    other machine learning techniques to enable the voice interaction that people
    are accustomed to, reduce wait times, and increase efficiency.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了嵌入的概念以及它们如何为我们提供更多工具，让我们构建一个增强型技术支持呼叫中心解决方案。我们将继续使用LLM的文本生成能力以及它们的嵌入，并纳入其他机器学习技术，以实现人们习惯的语音交互，减少等待时间，并提高效率。
- en: First, to support voice interaction, we will use speech-to-text to convert the
    words spoken by a user into text that is used as input into an LLM. It would be
    reasonable to think, “I’ve used some really horrible voice-controlled systems
    before,” and yes, you likely have. This is why adding a “bail-out” mechanism is
    essential to escape the automated system (e.g., max tries, times, or opt-out)
    for cases where the system can’t understand a user’s speech. In addition to speech-to-text,
    we will also use text-to-speech to give the LLM a way to convert the text output
    it produces into something that a user should be able to hear and understand.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了支持语音交互，我们将使用语音转文本将用户说出的单词转换为用于输入LLM的文本。合理的想法是，“我之前使用过一些非常糟糕的语音控制系统”，是的，你很可能有。这就是为什么在系统无法理解用户的语音的情况下，添加一个“退出”机制（例如，最大尝试次数、时间或选择退出）是至关重要的。除了语音转文本之外，我们还将使用文本转语音，以便LLM有一种将产生的文本转换为用户应该能够听到和理解的方式。
- en: Second, to reduce wait times, we can implement a system where, if a queue of
    callers has formed due to the number of support requests incoming, we will ask
    the customer to describe their problem so that they can be routed to the most
    appropriate analyst. Assuming that customers may have novel problems, we do not
    attempt to use the LLM to address their problems outright. Instead, we will use
    a customer’s problem description to call the LLM’s embedding API to produce a
    representation of their problem. Once we have that problem description embedding,
    we can use clustering to group the users in the queue. Users with similar problems
    can be assigned to the same team of analysts to help analysts solve problems faster.
    That alone is a win.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，为了减少等待时间，我们可以实施一个系统，如果由于支持请求的数量而形成了一个呼叫者队列，我们将要求客户描述他们的问题，以便他们可以被路由到最合适的分析师。假设客户可能有新颖的问题，我们不尝试直接使用LLM来解决问题。相反，我们将使用客户的问题描述来调用LLM的嵌入API以产生他们问题的表示。一旦我们有了这个问题描述嵌入，我们可以使用聚类来将队列中的用户分组。具有相似问题的用户可以被分配到同一个分析师团队，以帮助分析师更快地解决问题。这本身就是一种胜利。
- en: 'We can use this problem grouping to gain further efficiency. Say an analyst
    has identified that a user has a common problem for which there is a consistent,
    predefined solution. Instead of relying on the LLM to dynamically generate a hypothetical
    solution, your human analyst can share the predefined solution that has already
    been vetted by real users. Additionally, you can push that solution out to the
    users who are waiting in the queue via the LLM. You will be able to inform the
    users: “An automated solution has been developed that we believe will solve your
    problem. While you wait, let us try to solve this with our automated AI.” This
    approach is summarized in figure [8.5](#fig__HCI_MoreApproaches).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这种问题分组来进一步提高效率。假设分析师已经确定用户有一个常见问题，对于这个问题有一个一致且预定义的解决方案。而不是依赖LLM动态生成一个假设性的解决方案，您的人类分析师可以分享已经由真实用户验证过的预定义解决方案。此外，您可以通过LLM将此解决方案推送给正在排队等待的用户。您将能够通知用户：“我们已经开发了一个自动化的解决方案，我们相信这将解决您的问题。当您等待时，让我们尝试用我们的自动化AI来解决这个问题。”这种方法总结在图[8.5](#fig__HCI_MoreApproaches)中。
- en: '![figure](../Images/CH08_F05_Boozallen.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F05_Boozallen.png)'
- en: Figure 8.5 This diagram describes our “better solution” to customer support
    requests, where customers describe their problem while waiting to talk to someone.
    The LLM uses an embedding representation of the problem to compare similar problems
    with known solutions. While the user waits, an automated system can provide information
    that may help them solve their problem without support personnel intervention.
    If that fails, there’s always the possibility to “bail out” and talk to a real
    person. The model used to generate the embeddings does not necessarily have to
    be the same as the LLM that walks the user through the solution.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 该图描述了我们针对客户支持请求的“更好解决方案”，在客户等待与某人交谈时描述他们的问题。LLM使用问题的嵌入表示来比较已知解决方案的类似问题。在用户等待期间，一个自动化的系统可以提供可能帮助他们解决问题的信息，而无需支持人员的干预。如果这失败了，总有“退出”并与真人交谈的可能性。用于生成嵌入的模型不一定是引导用户通过解决方案的LLM。
- en: It’s entirely possible to combine the solutions we have described so far. For
    example, the analyst-to-customer interaction loop in the top-right of figure [8.5](#fig__HCI_MoreApproaches)
    could involve two people talking through the problem, or it could be the LLM-supervised
    validation solution we designed in figure [8.3](#fig__HCI_LLM_supervise_human).
    Depending on what problems need to be solved, there are many opportunities to
    extend these solutions now that we have embeddings. For example, if analysts saved
    information about how angry or upset a customer is, you could train a regression
    model to predict how angry a customer may be from their embedding. Then, you could
    distribute the angry customers evenly amongst analysts to avoid someone being
    overwhelmed or try to route angry customers away from new analysts who are still
    learning how to help customers solve their problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们迄今为止描述的解决方案结合起来是完全可能的。例如，图[8.5](#fig__HCI_MoreApproaches)右上角的分析师与客户互动循环可能涉及两个人讨论问题，或者可能是我们在图[8.3](#fig__HCI_LLM_supervise_human)中设计的LLM监督验证解决方案。根据需要解决的问题，现在我们有了嵌入，有许多机会扩展这些解决方案。例如，如果分析师保存了有关客户愤怒或沮丧的信息，您就可以训练一个回归模型来预测客户从其嵌入中可能有多愤怒。然后，您可以将愤怒的客户均匀分配给分析师，以避免有人感到不堪重负，或者尝试将愤怒的客户引导远离仍在学习如何帮助客户解决问题的初级分析师。
- en: 'To be clear, we are not saying that all customer service tech support systems
    will be better if they use this approach. The goal is to show you that there are
    ways to build solutions with LLMs that work around their shortcomings, such as
    their tendency to hallucinate and their inability to incorporate new knowledge
    dynamically. In summary, we present two basic strategies:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，我们并不是说如果客户服务技术支持系统采用这种方法，它们就会变得更好。目标是向您展示，有方法可以利用LLM（大型语言模型）构建解决方案，以克服它们的不足，例如它们倾向于产生幻觉以及无法动态地融入新知识。总之，我们提出了两种基本策略：
- en: Use LLMs as a second set of eyes on what is happening. If the LLM agrees, all
    is good. If it disagrees, you perform a double-check that could be simple or complex,
    depending on the nature of the problem.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将LLM作为观察正在发生的事情的第二双眼睛。如果LLM同意，那么一切正常。如果它不同意，您需要进行双重检查，这可能是简单或复杂的，具体取决于问题的性质。
- en: Use embeddings to apply classic machine learning to the problem. Clustering
    (grouping similar things) and outlier detection (finding unique or unusual things)
    will be particularly useful for many real-world applications.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入将经典机器学习应用于问题。聚类（将相似的事物分组）和异常检测（寻找独特或不寻常的事物）对于许多实际应用将特别有用。
- en: We don’t solely rely on the LLM to create output at any point in these solutions
    because LLMs can generate incorrect or inappropriate output. However, we can still
    use the LLMs to reduce workload, errors, and the time to resolution by being careful
    in how we design the system as a whole.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些解决方案的任何阶段，我们并不完全依赖LLM来创建输出，因为LLM可能会生成错误或不适当的输出。然而，我们仍然可以通过谨慎地设计整个系统来利用LLM减少工作量、错误和解决问题的耗时。
- en: 8.4 Technology presentation matters
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 技术展示很重要
- en: Some of you may be incredulous after reading through this example of how we
    would design a tech support system that uses LLMs. We often hear folks who fully
    believe in LLM technology say, “If you have the LLM explain its reasoning, the
    user or analyst can figure out if it makes sense, and all of the problems related
    to hallucinations and errors will be solved.” We often receive similar requests
    to create “explainable AI” from those on the more skeptical end of the spectrum
    who are concerned about the errors LLMs produce and who don’t understand what
    is happening. Thus, there is a perception on both sides that explanations will
    provide the means to establish trust in the technology and believe that the LLM
    (or any machine learning algorithm) is working properly and effectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了如何设计使用LLM的客服支持系统示例之后，有些人可能会感到难以置信。我们经常听到那些完全相信LLM技术的人说：“如果你让LLM解释其推理，用户或分析师就可以判断其是否合理，并且所有与幻觉和错误相关的问题都将得到解决。”我们也经常收到来自那些对LLM产生的错误表示担忧且不理解正在发生什么的人的类似请求，他们希望创建“可解释的人工智能”。因此，在双方都有一种观念，即解释将提供建立对技术的信任并相信LLM（或任何机器学习算法）正在正常和有效地工作的手段。
- en: In this section, we want to discuss some points that support the notion that
    explain-ability is not the solution to these problems. Explainability is not the
    single solution that will help catch errors or make a system more transparent
    and trustworthy. The unfortunate truth is that our assumptions about how an LLM
    will work with people are often wrong and must be carefully evaluated. In fact,
    recent research has shown that when explainable AI techniques are employed by
    a system, people erroneously trust the AI to be correct solely based on the fact
    that an explanation is present, regardless of its accuracy. This is true even
    when the user could perform the task independently without an AI’s support, and
    the user has been taught about how the AI systems actually work [3]. The bottom
    line is that explanations can be harmful to the very goals that they attempt to
    advance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们想要讨论一些支持以下观点的论点：可解释性并不是这些问题的解决方案。可解释性并不是单一的解决方案，它将有助于捕捉错误或使系统更加透明和值得信赖。不幸的事实是，我们对大型语言模型如何与人类合作的假设往往是不正确的，并且必须仔细评估。事实上，最近的研究表明，当系统采用可解释人工智能技术时，人们错误地相信人工智能是正确的，仅仅因为存在解释，而不管其准确性如何。即使在用户可以在没有人工智能支持的情况下独立完成任务，并且用户已经了解人工智能系统实际工作方式的情况下，这也是正确的
    [3]。底线是，解释可能会对它们试图推进的目标产生有害影响。
- en: Why use explainable AI at all?
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么要使用可解释人工智能？
- en: In our professional experience, many requests for explainable AI come from a
    place of fear or anxiety. Ideally, explainable AI would not be the way to calm
    such fears because it is counterproductive to the actual goals being solved. So
    why would anyone do any explainable AI of any form?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的专业经验中，许多对可解释人工智能的请求都源于恐惧或焦虑。理想情况下，可解释人工智能不应该成为平息这种恐惧的方法，因为这与实际要解决的问题的目标是相反的。那么，为什么有人会进行任何形式的可解释人工智能呢？
- en: 'Two key things make explainable AI useful from a practical perspective:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，有两个关键因素使可解释人工智能变得有用：
- en: Answering the question, explainable *to whom*?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答问题，可解释**给谁**？
- en: Reaching explainable AI from the problem statement
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从问题陈述到达可解释人工智能
- en: For example, a real-world problem statement may describe the need to develop
    a scientific understanding of a physical or chemical process. With this goal,
    a useful explanation from the algorithm may be to generate an equation that produces
    the answers rather than producing the answers directly. With the equation, a physicist
    or chemist can inspect it for logical consistency and use it as a starting point
    for further scientific exploration.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个现实世界的问题陈述可能描述了需要发展对物理或化学过程的科学理解。有了这个目标，算法的一个有用的解释可能是生成一个产生答案的方程式，而不是直接产生答案。有了这个方程式，物理学家或化学家可以检查其逻辑一致性，并将其作为进一步科学探索的起点。
- en: In this case, the solution is explainable only to someone with significant expertise,
    but that is the only person who needs the explanation. The explanation in the
    form of an equation also directly tackles the problem of scientific understanding
    rather than merely understanding the inner workings of the AI algorithm. We do
    not have any explanation of how the AI came up with the equation itself, and the
    equation is (hopefully) a logically consistent form that explains the physical
    or chemical process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，解决方案只对具有显著专业知识的人可解释，但只有这个人需要解释。以方程式形式提供的解释也直接解决了科学理解的问题，而不仅仅是理解人工智能算法的内部工作原理。我们没有关于人工智能如何得出方程式的解释，而这个方程式（希望）是一个逻辑上一致的形式，可以解释物理或化学过程。
- en: 'This example reflects the general situation in which we find explainable AI
    the most helpful: when it is used to aid a narrow and specific audience of potentially
    expert users in performing a very specific goal. For example, it is indeed common
    for data scientists to use explainable AI to help them figure out why a particular
    model is making a particular set of errors, even if the tools they use are not
    comprehensible to a nondata scientist audience.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子反映了我们在哪种情况下发现可解释人工智能最有帮助的一般情况：当它被用来帮助一个狭窄且具体的潜在专家用户群体实现一个非常具体的目标时。例如，数据科学家确实经常使用可解释人工智能来帮助他们弄清楚为什么某个模型会犯特定的错误集，即使他们使用的工具对非数据科学家用户群体来说并不易理解。
- en: So if explainable AI is not a solution for building trust in an AI system or
    solution, what is? Unfortunately, there is no agreed-upon generic and rigorously
    evaluated way to build trust in AI. Our unoriginal suggestion is to focus on transparency,
    user evaluation, and the specifics of the use cases involved.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果可解释人工智能不是建立对人工智能系统或解决方案信任的解决方案，那又是什么呢？遗憾的是，目前还没有一个普遍认同的、经过严格评估的方法来建立对人工智能的信任。我们非原创的建议是关注透明度、用户评估以及涉及的使用案例的细节。
- en: 8.4.1 How can you be transparent?
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 如何做到透明？
- en: 'Transparency can be as simple as informing users about the AI system that is
    being used: Which model was it designed with and, at a high level, how was it
    modified? If the system is meant to mimic a specific person (“Get tutored by Albert
    A.I. Einstein”) or a type of credentialed person (“Ask Dr. GPT about that mole
    on your back”), has that person or similarly credentialed person consented to
    this or approved its efficacy? How can the consumer verify this information?'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度可以简单到通知用户正在使用的AI系统：它是用哪个模型设计的，在较高层次上是如何修改的？如果系统旨在模仿一个特定的人（“由Albert A.I. Einstein辅导”）或一种有资格的人（“向Dr.
    GPT询问你背上的那个痣”），那个人或类似有资格的人是否同意这样做或批准其有效性？消费者如何验证这些信息？
- en: Essentially, enumerating these kinds of reasonable questions and their answers
    that an auditor or skeptical user might want to know will put you far ahead of
    the average in making your system more transparent. These do not need to be presented
    in detail to every user, but having a way for users to discover this information
    is helpful. It not only helps sophisticated users understand what is happening
    but also helps set the expectations of users in general about what is and is not
    possible with a given system. Furthermore, it is essential to inform users when
    they are interacting with a system that is generating automated responses. There
    is a big difference between trying to pretend a human is in control and thus should
    be able to solve any reasonable challenge versus an automated AI that you inform
    the customer has limited capability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，列出这些合理的疑问及其答案，审计员或怀疑的用户可能想知道，将使你在使你的系统更加透明方面远远领先于平均水平。这些不需要向每个用户详细展示，但为用户提供一种发现这些信息的方式是有帮助的。这不仅有助于复杂的用户理解正在发生的事情，还有助于设定用户对给定系统可能性和不可能性的普遍期望。此外，当用户与生成自动响应的系统交互时，告知用户这一点至关重要。试图假装人类在控制并因此应该能够解决任何合理的挑战与告知客户该AI具有有限能力的自动化AI之间有很大的区别。
- en: 8.4.2 Aligning incentives with users
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 与用户对齐激励
- en: Part of transparency and system presentation involves aligning the incentives
    involved. This isn’t just a feel-good statement about management practices but
    a practical unit of advice. Remember from chapter [4](../Text/chapter-4.html)
    that AI algorithms are greedy machines that optimize for what you ask, not what
    you intend. If you start building an LLM system where the incentives of the system
    are not well aligned with your broader goals, you risk overfitting to what you
    asked, not what both you and your users need.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度和系统展示的一部分涉及调整涉及的激励措施。这不仅仅是一句关于管理实践的好听的话，而是一个实用的建议。记得从第[4](../Text/chapter-4.html)章中，人工智能算法是贪婪的机器，它们优化的是你所要求的，而不是你所意图的。如果你开始构建一个LLM系统，而这个系统的激励措施与你的更广泛目标没有很好地对齐，你可能会过度拟合你所要求的，而不是你和你用户所需要的。
- en: With aligned incentives (e.g., our example of “try out the LLM and get $2 off
    your bill if it worked”), you are much more likely to have a positive outcome.
    They also give you more ways to advertise using an LLM as a mechanism for providing
    value to your customers instead of coming across as the evil people trying to
    outsource all the jobs. Presenting and discussing the aligned incentives between
    a business and its customers and how you are using LLMs to achieve those goals
    describes what needs to be said without any need for hiding the information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在激励措施对齐的情况下（例如，我们的例子是“试用LLM并在它有效时减免2美元”），你更有可能获得积极的结果。它们还为你提供了更多使用LLM作为向客户提供价值机制而不是作为试图外包所有工作的邪恶人的广告方式。讨论业务与其客户之间的对齐激励以及你是如何使用LLMs来实现这些目标的，描述了需要说的话，而不需要隐藏信息。
- en: 8.4.3 Incorporating feedback cycles
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 引入反馈循环
- en: 'The world is not a static place. Things change, and what works today may not
    work tomorrow. This is one reason why you should have regular and continuous auditing
    of any automated AI/ML system: because they do not improve or adapt independently
    with experience.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 世界不是静止的。事物在变化，今天有效的东西明天可能就不行。这就是为什么你应该定期和持续地对任何自动化 AI/ML 系统进行审计的原因：因为它们不会独立地随着经验而改进或适应。
- en: But it will also help you catch potentially negative feedback cycles, something
    you want to try to think about in advance. Negative feedback cycles are not always
    possible to predict. To help you catch these, try to think about which users will
    or won’t find the most benefit with a new system and what happens as that repeats
    over and over again.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也有助于你捕捉潜在的负面反馈循环，这是你想要提前考虑的事情。负面反馈循环并不总是可以预测的。为了帮助你捕捉这些，尝试思考哪些用户会或不会从新系统中获得最大利益，以及当这种情况反复发生时会发生什么。
- en: For example, we mentioned that speech-to-text and text-to-speech can be helpful
    for older customers or any hearing or movement-impaired customer. If we did not
    include such an option, we might alienate those customers over time, because every
    time they have a problem, they must use a physically difficult system.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们提到语音转文本和文本转语音可以帮助老年客户或任何听力或运动障碍的客户。如果我们不包括这样的选项，我们可能会随着时间的推移疏远这些客户，因为每次他们有问题时，他们都必须使用一个身体上困难的系统。
- en: Imagine you were a cell phone company that relied on family plans for some of
    your revenue. Your previously middle-aged customers who first bought your family
    plans are getting frustrated with your support system, so they move their entire
    family plan over to a new provider who puts in the extra work to ensure that the
    customer support process is accurate and efficient. Now you’re losing both your
    older and younger customers at once!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一家依赖家庭计划获得部分收入的手机公司。你之前的中年客户首次购买你的家庭计划，现在对你的支持系统感到沮丧，所以他们把整个家庭计划转移到一家新的提供商那里，该提供商投入额外的工作以确保客户支持流程准确高效。现在你同时失去了老年和年轻客户！
- en: The point here is to think things through and train yourself to do these thought
    experiments. You will not catch every case, but you will improve. Regular auditing
    and testing then help you catch the failure cases, document them, and improve
    how you think about future situations and repeat problems.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是深思熟虑，并训练自己进行这些思维实验。你可能不会捕捉到每一个案例，但你将会进步。定期的审计和测试帮助你捕捉到失败案例，记录它们，并改善你对未来情况和重复问题的思考方式。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs will have errors, and you first need to determine the risk and potential
    cost of errors to design an appropriate solution. If the risk and cost of errors
    are low, you can potentially use a normal chatbot-style LLM.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 会出错，你首先需要确定错误的风险和潜在成本，以设计适当的解决方案。如果错误的风险和成本较低，你可能会使用正常的聊天机器人风格的 LLM。
- en: It is possible to control the risk of using an LLM by changing how users interact
    with the system or shifting automation to a different part of the business process.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过改变用户与系统交互的方式或将自动化转移到业务流程的另一个部分，可以控制使用 LLM 的风险。
- en: Including a “human in the loop” to supervise an LLM creates automation bias
    risk, even when using techniques such as RAG to reduce the risk of errors.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括“人工介入”来监督 LLM 会产生自动化偏差风险，即使使用 RAG 等技术来降低错误风险。
- en: LLMs can convert text into embeddings, numeric representations where similar
    sentences receive similar values. This allows you to use additional machine learning
    approaches, including classic techniques like clustering and outlier detection.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 可以将文本转换为嵌入，这是一种数值表示，其中相似的句子会收到相似的价值。这允许你使用额外的机器学习方法，包括经典技术，如聚类和异常检测。
- en: While LLMs can explain their decisions, their explanations are often ineffective
    because people become dependent on them. Instead, focus on producing explanations
    to satisfy a specific need or use case rather than generic “needing to explain.”
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 LLMs 可以解释它们的决策，但它们的解释通常无效，因为人们会依赖它们。相反，专注于产生满足特定需求或用例的解释，而不是通用的“需要解释”。
- en: Design your system’s incentives to align with your user’s incentives. This is
    both a good way to avoid mistakes from an LLM optimizing for what you asked instead
    of what you intended and a good way to communicate and present your LLM to users.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计你系统的激励措施，使其与用户的激励措施相一致。这不仅是一种避免 LLM 优化你所要求的内容而不是你意图的内容的错误的好方法，也是一种向用户传达和展示你的
    LLM 的好方法。
