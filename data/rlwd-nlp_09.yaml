- en: 6 Sequence-to-sequence models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 序列到序列模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Building a machine translation system using Fairseq
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fairseq构建机器翻译系统
- en: Transforming one sentence to another using a Seq2Seq model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Seq2Seq模型将一句话转换成另一句话
- en: Using a beam search decoder to generate better output
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用束搜索解码器生成更好的输出
- en: Evaluating the quality of machine translation systems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估机器翻译系统的质量
- en: Building a dialogue system (chatbot) using a Seq2Seq model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Seq2Seq模型构建对话系统（聊天机器人）
- en: In this chapter, we are going to discuss sequence-to-sequence (Seq2Seq) models,
    which are some of the most important complex NLP models and are used for a wide
    range of applications, including machine translation. Seq2Seq models and their
    variations are already used as the fundamental building blocks in many real-world
    applications, including Google Translate and speech recognition. We are going
    to build a simple neural machine translation system using a powerful framework
    to learn how the models work and how to generate the output using greedy and beam
    search algorithms. At the end of this chapter, we will build a chatbot—an NLP
    application with which you can have a conversation. We’ll also discuss the challenges
    and limitations of simple Seq2Seq models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论序列到序列（Seq2Seq）模型，这些模型是一些最重要的复杂自然语言处理模型，被用于广泛的应用场景，包括机器翻译。Seq2Seq模型及其变种已经在许多实际应用中作为基本构建块使用，包括谷歌翻译和语音识别。我们将使用一个强大的框架来构建一个简单的神经机器翻译系统，以了解这些模型的工作原理以及如何使用贪婪和束搜索算法生成输出。在本章的结尾，我们将构建一个聊天机器人——一个可以与之对话的自然语言处理应用。我们还将讨论简单Seq2Seq模型的挑战和局限性。
- en: 6.1 Introducing sequence-to-sequence models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 介绍序列到序列模型
- en: In the previous chapter, we discussed two types of powerful NLP models, namely,
    sequential labeling and language models. To recap, a sequence-labeling model takes
    a sequence of some units (e.g., words) and assigns a label (e.g., a part-of-speech
    (POS) tag) to each unit, whereas a language model takes a sequence of some units
    (e.g., words) and estimates how probable the given sequence is in the domain in
    which the model is trained. You can also use a language model to generate realistic-looking
    text from scratch. See figure 6.1 for the overview of these two models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们讨论了两种强大的自然语言处理模型，即序列标记和语言模型。回顾一下，序列标记模型接收一些单元的序列（例如，单词）并为每个单元分配一个标签（例如，词性标注）。而语言模型接收一些单元的序列（例如，单词），并估计给定序列在模型训练的领域中出现的概率。你还可以使用语言模型从零开始生成看起来真实的文本。请参阅图
    6.1 以了解这两种模型的概况。
- en: '![CH06_F01_Hagiwara](../Images/CH06_F01_Hagiwara.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Hagiwara](../Images/CH06_F01_Hagiwara.png)'
- en: Figure 6.1 Sequential labeling and language models
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 序列标记和语言模型
- en: Although these two models are useful for a number of NLP tasks, for some, you
    may want the best of both worlds—you may want your model to take some input (e.g.,
    a sentence) and generate something else (e.g., another sentence) in response.
    For example, if you wish to translate some text written in one language into another,
    you need your model to take a sentence and produce another. Can you do this with
    sequential-labeling models? No, because they can produce only the same number
    of output labels as there are tokens in the input sentence. This is obviously
    too limiting for translation—one expression in a language (say, “Enchanté” in
    French) can have an arbitrarily large or small number of words in another (say,
    “Nice to meet you” in English). Can you do this with language models? Again, not
    really. Although you can generate realistic-looking text using language models,
    you have almost no control over the text they generate. In fact, language models
    do not take any input.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种模型对于许多自然语言处理任务都非常有用，但对于某些任务，你可能希望兼顾这两者——让你的模型接收一些输入（例如，一句句子）并产生另一个东西（例如，另一句句子）作为响应。例如，如果你希望将用一种语言写的文本翻译成另一种语言，你需要让模型接收一个句子并产生另一个句子。你能用序列标记模型实现吗？不能，因为它们只能产生与输入句子中标记数量相同数量的输出标签。这显然对于翻译来说太过有限——一种语言中的表达（比如法语中的“Enchanté”）在另一种语言中可以有任意多或少的单词（比如英语中的“Nice
    to meet you”）。你能用语言模型实现吗？还是不能。虽然你可以使用语言模型生成看起来真实的文本，但你几乎无法控制它们生成的文本。事实上，语言模型不接受任何输入。
- en: But if you look at figure 6.1 more carefully, you might notice something. The
    model on the left (the sequential-labeling model) takes a sentence as its input
    and produces some form of representations, whereas the model on the right produces
    a sentence with variable length that looks like natural language text. We already
    have the components needed to build what we want, that is, a model that takes
    a sentence and transforms it into another. The only missing part is a way to connect
    these two so that we can control what the language model generates.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你仔细看图6.1，你可能会注意到一些东西。左侧模型（序列标记模型）以句子作为输入，并生成某种形式的表示，而右侧模型则生成一个看起来像自然语言文本的长度可变的句子。我们已经有了构建我们想要的东西所需的组件，即一个接受句子并将其转换为另一个句子的模型。唯一缺失的部分是一种连接这两者的方法，以便我们可以控制语言模型生成什么。
- en: In fact, by the time the model on the left finishes processing the input sentence,
    the RNN has already produced its abstract representation, which is encoded in
    the RNN’s hidden states. If you can simply connect these two so that the sentence
    representation is passed from left to right and the language model can generate
    another sentence based on the representation, it seems like you can achieve what
    you wanted to do in the first place!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当左侧模型完成处理输入句子时，循环神经网络已经生成了其抽象表示，该表示被编码在循环神经网络的隐藏状态中。如果你能简单地将这两者连接起来，使得句子表示从左到右传递，并且语言模型可以根据这个表示生成另一个句子，那么似乎你可以实现最初想要做的事情！
- en: Sequence-to-sequence models—or *Seq2Seq* models, in short—are built on this
    insight. A Seq2Seq model consists of two subcomponents—an encoder and a decoder.
    See figure 6.2 for an illustration. An encoder takes a sequence of some units
    (e.g., a sentence) and converts it into some internal representation. A decoder,
    on the other hand, generates a sequence of some units (e.g., a sentence) from
    the internal representation. As a whole, a Seq2Seq model takes a sequence and
    generates another sequence. As with the language model, the generation stops when
    the decoder produces a special token, <END>, which enables a Seq2Seq model to
    generate an output that can be longer or shorter than the input sequence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型，简称*Seq2Seq*模型，是基于这一见解构建的。Seq2Seq模型由两个子组件组成，即编码器和解码器。见图6.2进行说明。编码器接受一系列单位（例如，一个句子）并将其转换为某种内部表示。另一方面，解码器从内部表示生成一系列单位（例如，一个句子）。总的来说，Seq2Seq模型接受一个序列并生成另一个序列。与语言模型一样，生成过程在解码器产生一个特殊标记<END>时停止，这使得Seq2Seq模型可以生成比输入序列更长或更短的输出。
- en: '![CH06_F02_Hagiwara](../Images/CH06_F02_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Hagiwara](../Images/CH06_F02_Hagiwara.png)'
- en: Figure 6.2 Sequence-to-sequence model
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 序列到序列模型
- en: Many variants of Seq2Seq models exist, depending on what architecture you use
    for the encoder, what architecture you use for the decoder, and how information
    flows between the two. This chapter covers the most basic type of Seq2Seq model—simply
    connecting two RNNs via the sentence representation. We’ll discuss more advanced
    variants in chapter 8.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多Seq2Seq模型的变体存在，这取决于你用于编码器的架构，你用于解码器的架构以及两者之间信息流动的方式。本章涵盖了最基本类型的Seq2Seq模型——简单地通过句子表示连接两个循环神经网络。我们将在第8章中讨论更高级的变体。
- en: Machine translation is the first, and by far the most popular, application of
    Seq2Seq models. However, the Seq2Seq architecture is a generic model applicable
    to numerous NLP tasks. In one such task, summarization, an NLP system takes a
    long text (e.g., a news article) and produces its summary (e.g., a news headline).
    A Seq2Seq model can be used to “translate” the longer text into the shorter one.
    Another task is a dialogue system, or a *chatbot*. If you think of a user’s utterance
    as the input and the system’s response as the output, the dialogue system’s job
    is to “translate” the former into the latter. Later in this chapter, we will discuss
    a case study where we actually build a chatbot using a Seq2Seq model. Yet another
    (somewhat surprising) application is parsing—if you think of the input text as
    one language and its syntax representation as another, you can parse natural language
    texts with a Seq2Seq model.[¹](#pgfId-1103720)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译是 Seq2Seq 模型的第一个，也是迄今为止最流行的应用。然而，Seq2Seq 架构是一个通用模型，适用于许多自然语言处理任务。在其中一项任务中，摘要生成，一个自然语言处理系统接受长文本（例如新闻文章）并生成其摘要（例如新闻标题）。Seq2Seq
    模型可以用来将较长的文本“翻译”成较短的文本。另一个任务是对话系统，或者*聊天机器人*。如果你将用户的话语视为输入，系统的回应视为输出，对话系统的工作就是将前者“翻译”成后者。在本章后面，我们将讨论一个案例研究，在这个案例中，我们实际上使用了
    Seq2Seq 模型构建了一个聊天机器人。另一个（有些令人惊讶的）应用是解析——如果你将输入文本视为一种语言，将其语法表示视为另一种语言，你可以使用 Seq2Seq
    模型解析自然语言文本。
- en: 6.2 Machine translation 101
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 机器翻译 101
- en: We briefly touched upon machine translation in section 1.2.1\. To recap, machine
    translation (MT) systems are NLP systems that translate a given text from one
    language to another. The language the input text is written in is called the *source
    language*, whereas the one for the output is called the *target language*. The
    combination of the source and target languages is called the *language pair*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 1.2.1 节简要提及了机器翻译。简而言之，机器翻译（MT）系统是将给定文本从一种语言翻译成另一种语言的自然语言处理系统。输入文本所用语言称为*源语言*，而输出文本所用语言称为*目标语言*。源语言和目标语言的组合称为*语言对*。
- en: First, let’s look at a couple of examples to see what it’s like and why it’s
    difficult to translate a foreign language to English (or any other language you
    understand). In the first example, let’s translate a Spanish sentence, “Maria
    no daba una bofetada a la bruja verde.” to the English counterpart, “Mary did
    not slap the green witch.” A common practice in illustrating the process of translation
    is to draw how words or phrases of the same meaning map between the two sentences.
    Correspondence of linguistic units between two instances is called *alignment*.
    Figure 6.3 shows the alignment between the Spanish and English sentences.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一些例子，看看是什么样子，以及为什么将外语翻译成英语（或者任何其他你理解的语言）是困难的。在第一个例子中，让我们将一个西班牙句子翻译成英文，即，“Maria
    no daba una bofetada a la bruja verde.” 翻译成英文对应的是，“Mary did not slap the green
    witch.” 在说明翻译过程时的一个常见做法是绘制两个句子之间具有相同意思的单词或短语如何映射的图。两个实例之间的语言单位的对应称为*对齐*。图 6.3
    显示了西班牙语和英语句子之间的对齐。
- en: '![CH06_F03_Hagiwara](../Images/CH06_F03_Hagiwara.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F03_Hagiwara](../Images/CH06_F03_Hagiwara.png)'
- en: Figure 6.3 Translation and word alignment between Spanish and English
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 西班牙语和英语之间的翻译和词对齐
- en: Some words (e.g., “Maria” and “Mary,” “bruja” and “witch,” and “verde” and “green”)
    match exactly one to one. However, some expressions (e.g., “daba una bofetada”
    and “slap”) differ in such a significant way that you can only align phrases between
    Spanish and English. Finally, even where there’s one-to-one correspondence between
    words, the way words are arranged, or *word order*, may differ between the two
    languages. For example, adjectives are added after nouns in Spanish (“la bruja
    verde”) whereas in English, they come before nouns (“the green witch”). Spanish
    and English are linguistically similar in terms of grammar and vocabulary, especially
    when compared to, say, Chinese and English, although this single example shows
    translating between the two may be a challenging task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单词（例如，“Maria” 和 “Mary”，“bruja” 和 “witch”，以及 “verde” 和 “green”）完全一一对应。然而，一些表达（例如，“daba
    una bofetada” 和 “slap”）在某种程度上有很大不同，以至于你只能在西班牙语和英语之间对齐短语。最后，即使单词之间有一对一的对应关系，单词的排列方式，或者*词序*，在两种语言之间可能也会有所不同。例如，形容词在西班牙语中在名词之后添加（“la
    bruja verde”），而在英语中，它们在名词之前（“the green witch”）。在语法和词汇方面，西班牙语和英语在某种程度上是相似的，尤其是与中文和英语相比，尽管这个单一的例子显示了在两种语言之间进行翻译可能是一项具有挑战性的任务。
- en: Things start to look more complicated between Mandarin Chinese and English.
    Figure 6.4 illustrates the alignment between a Chinese sentence (“Bushi yu Shalong
    juxing le huitan.”) and its English translation (“Bush held a talk with Shalon.”).
    Although Chinese uses ideographic characters of its own, we use romanized sentences
    here for simplicity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 汉语和英语之间的情况开始变得更加复杂。图6.4展示了一句汉语句子（“布什与沙龙举行了会谈。”）和其英文翻译（“Bush held a talk with
    Shalon.”）之间的对齐。尽管汉语使用了自己的表意文字，但我们在这里使用了罗马化的句子以示简便。
- en: '![CH06_F04_Hagiwara](../Images/CH06_F04_Hagiwara.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04_Hagiwara](../Images/CH06_F04_Hagiwara.png)'
- en: Figure 6.4 Translation and word alignment between Mandarin Chinese and English
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 汉语和英语之间的翻译和词对齐
- en: You can now see more crossing arrows in the figure. Unlike English, Chinese
    prepositional phrases such as “with Shalon” are usually attached to verbs from
    the left. Also, the Chinese language doesn’t explicitly mark tense, and MT systems
    (and human translators alike) need to “guess” the correct tense to use for the
    English translation. Finally, Chinese-to-English MT systems also need to infer
    the correct number (singular or plural) of each noun, because Chinese nouns are
    not explicitly marked according to their number (e.g., “huitan” just means “talk”
    with no explicit mention of number). This is a good example showing how the difficulty
    of translation depends on the language pair. Development of MT systems between
    linguistically different languages (such as Chinese and English) is usually more
    challenging than those between linguistically similar ones (such as Spanish and
    Portuguese).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在图中看到更多交叉的箭头。与英语不同，汉语介词短语（比如“和沙龙一起”）通常从左边附着在动词上。此外，汉语不明确标记时态，机器翻译系统（以及人工翻译）需要“猜测”英文翻译中应该使用的正确时态。最后，汉译英的机器翻译系统还需要推断每个名词的正确数量（单数或复数），因为汉语名词没有根据数量明确标记（例如，“会谈”只是表示“谈话”，没有明确提及数量）。这是一个很好的例子，说明了翻译的难度取决于语言对。在语言学上不同的语言之间开发机器翻译系统（如中文和英文）通常比在语言学上类似的语言之间（如西班牙语和葡萄牙语）更具挑战性。
- en: '![CH06_F05_Hagiwara](../Images/CH06_F05_Hagiwara.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Hagiwara](../Images/CH06_F05_Hagiwara.png)'
- en: Figure 6.5 Translation and word alignment between Japanese and English
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 日语和英语之间的翻译和词对齐
- en: Let’s take a look at one more example—translating from Japanese to English,
    illustrated in figure 6.5\. All the arrows in the figure are crossed, meaning
    that the word order is almost exactly opposite in these two sentences. In addition
    to the fact that Japanese prepositional phrases (“to music”) and relative clauses
    attach from the left like Chinese, objects (such as “listening” in “I love listening”
    in the example) come before the verb. In other words, Japanese is an SOV (subject-object-verb)
    language, whereas all the other languages we mentioned so far (English, Spanish,
    and Chinese) are SVO (subject-verb-object) languages. Structural differences are
    a reason why direct, word-to-word translation doesn’t work very well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个例子——从日语翻译成英语，在图6.5中有说明。图中所有的箭头都是交叉的，表示这两个句子的词序几乎完全相反。除了日语介词短语（例如“to music”）和关系从句从左边附着，跟汉语一样，宾语（例如例句中的“listening”在“我喜爱听”中）出现在动词之前。换句话说，日语是一种SOV（主语-宾语-动词）的语言，而到目前为止我们提到的其他语言（英语、西班牙语和汉语）都是SVO（主语-动词-宾语）的语言。结构上的差异是直接、逐字翻译效果不佳的原因之一。
- en: NOTE This word-order classification system of language (such as SOV and SVO)
    is often used in linguistic typology. The vast majority of world languages are
    either SOV (most common) or SVO (slightly less common), although a small number
    of languages follow other word-order systems, such as VSO (verb-subject-object),
    used by Arabic and Irish, for example. Very few languages (less than 3% of all
    languages) follow other types (VOS, OVS, and OSV).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注 这种语言的词序分类系统（如SOV和SVO）常常用于语言类型学。世界上绝大多数语言都是SOV（最常见）或SVO（稍少一些），尽管少数语言遵循其他词序系统，例如阿拉伯语和爱尔兰语使用的VSO（动词-主语-宾语）。很少一部分语言（不到所有语言的3%）使用其他类型（VOS、OVS和OSV）。
- en: Besides the structural differences shown in the previous figures, many other
    factors can make MT a difficult task. One such factor is lexical difference. If
    you are translating, for example, the Japanese word “ongaku” to the English “music,”
    there’s little ambiguity. “Ongaku” is almost always “music.” However, if you are
    translating, say, the English word “brother” to Chinese, you face ambiguity, because
    Chinese uses distinct words for “elder brother” and “younger brother.” In an even
    more extreme case, if you are translating “cousin” to Chinese, you have eight
    different choices, because in the Chinese family system, you need to use distinct
    words depending on whether your cousin is maternal or paternal, female or male,
    and older or younger than you.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面图示的结构差异之外，许多其他因素也会使机器翻译成为一项困难的任务。其中之一是词汇差异。例如，如果你将日语单词“音楽”翻译成英语“music”，几乎没有歧义。“音楽”几乎总是“music”。然而，如果你将英语单词“brother”翻译成中文，你会面临歧义，因为中文对“哥哥”和“弟弟”使用不同的词语。在更极端的情况下，如果你将“cousin”翻译成中文，你会有八种不同的选择，因为在中国家庭制度中，你需要根据你的表兄弟是母亲的还是父亲的，是女性还是男性，比你大还是小，使用不同的词语。
- en: Another factor that makes MT challenging is omission. You can see that in figure
    6.5, there’s no Japanese word for “I.” In languages such as Chinese, Japanese,
    Spanish, and many others, you can omit the subject pronoun when it’s clear from
    the context and/or the verb form. This is called *zero pronoun*, and it can become
    a problem when translating from a pronoun-dropping language to a language where
    it happens less often (e.g., English).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使机器翻译具有挑战性的因素是省略。你可以在图6.5中看到，日语中没有“我”的单词。在诸如中文、日语、西班牙语等许多其他语言中，当主语代词在上下文和/或动词形式中是明确的时候，你可以省略主语代词。这被称为*zero
    pronoun*，当从一个省略代词的语言翻译成一个省略频率较低的语言时（例如英语），它可能会成为一个问题。
- en: One of the earliest MT systems, developed during the Georgetown-IBM experiment,
    was built to translate Russian sentences into English during the Cold War. But
    all it did was not much different from looking up each word in a bilingual dictionary
    and replacing it with its translation. The three examples shown above should be
    enough to convince you that simply replacing word by word is too limiting. Later
    systems incorporated a larger set of lexicons and grammar rules, but these rules
    are written manually by linguists and are not enough to capture the complexities
    of language (again, remember the poor software engineer from chapter 1).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在乔治敦-IBM实验期间开发的最早的机器翻译系统之一是在冷战期间将俄语句子翻译成英语的。但它所做的不过是不比用双语词典查找每个单词并用其翻译替换它有多不同。上面展示的三个例子应该足以让你相信，简单地逐词替换太过于限制了。后来的系统包含了更大的词典和语法规则，但这些规则是由语言学家手动编写的，并不足以捕捉语言的复杂性（再次记住第1章中可怜的软件工程师）。
- en: 'The main paradigm for MT that remained dominant both in academia and industry
    before the advent of neural machine translation (NMT) is called *statistical machine
    translation* (SMT). The idea behind it is simple: learn how to translate from
    data, not by manually crafting rules. Specifically, SMT systems learn how to translate
    from datasets that contain a collection of texts in the source language and their
    translation in the target language. Such datasets are called *parallel corpora*
    (or *parallel texts*, or *bitexts*). By looking at a collection of paired sentences
    in both languages, the algorithm seeks patterns of how words in one language should
    be translated to another. The resulting statistical model is called a *translation
    model*. At the same time, by looking at a collection of target sentences, the
    algorithm can learn what valid sentences in the target languages should look like.
    Sounds familiar? This is exactly what a *language model* is all about (see the
    previous chapter). The final SMT model combines these two models and produces
    output that is a plausible translation of the input and is a valid, fluent sentence
    in the target language on its own.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经机器翻译（NMT）出现之前，在学术界和工业界主导的机器翻译的主要范式称为*统计机器翻译*（SMT）。其背后的理念很简单：通过数据学习如何翻译，而不是通过手工制定规则。具体而言，SMT系统学习如何从包含源语言文本和其在目标语言中的翻译的数据集中进行翻译。这些数据集称为*平行语料库*（或*平行文本*或*双文本*）。通过查看两种语言中成对句子的集合，算法寻找一种语言中的单词应如何翻译为另一种语言的模式。由此产生的统计模型称为*翻译模型*。同时，通过查看一系列目标句子，算法可以学习目标语言中有效句子的外观。听起来耳熟吗？这正是*语言模型*的全部内容（请参阅前一章）。最终的SMT模型结合了这两个模型，并生成一种对输入的合理翻译，并且在目标语言中是一句有效、流畅的句子。
- en: Around 2015, the advent of powerful neural machine translation (NMT) models
    subverted the dominance of SMT. SMT and NMT have two key differences. First, by
    definition, NMT is based on neural networks, which are well known for their power
    to model language accurately. As a result, target sentences generated by NMT tend
    to be more fluent and natural than those generated by SMT. Second, NMT models
    are trained end-to-end, as I briefly touched on in chapter 1\. This means that
    NMT models consist of a single neural network that takes an input and directly
    produces an output, instead of a patchwork of submodels and submodules that you
    need to train independently. As a result, NMT models are simpler to train and
    smaller in code size than SMT models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2015年，强大的神经机器翻译（NMT）模型的出现颠覆了SMT的主导地位。SMT和NMT有两个关键区别。首先，根据定义，NMT基于神经网络，而神经网络以其准确建模语言的能力而闻名。因此，由NMT生成的目标句子往往比由SMT生成的句子更流畅和自然。其次，NMT模型是端到端训练的，正如我在第1章中简要提到的那样。这意味着NMT模型由一个单一的神经网络组成，该网络接受输入并直接产生输出，而不是您需要独立训练的子模型和子模块的拼接。因此，与SMT模型相比，NMT模型更容易训练，代码规模更小。
- en: MT is already used in many different industries and aspects of our lives. Translating
    foreign text into a language that you understand to grasp its meaning quickly
    is called *gisting*. If the text is deemed important enough after gisting, it
    may be sent to formal, manual translation. Professional translators also use MT
    for their work. Oftentimes, the source text is first translated to the target
    language using an MT system, then the produced text is edited by human translators.
    Such editing is called *postediting*. The use of automated systems (called *computer-aided
    translation*, or CAT) can accelerate the translation process and reduce the cost.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MT已经在许多不同的行业和我们生活的方方面面得到了应用。将外语文本翻译成您理解的语言以快速抓住其含义的过程称为*摘要*。如果在摘要后认为文本足够重要，则可能会将其发送到正式的手动翻译中。专业翻译人员也使用MT进行工作。通常，源文本首先使用MT系统翻译为目标语言，然后由人类翻译人员编辑生成的文本。这种编辑称为*后编辑*。使用自动化系统（称为*计算机辅助翻译*或CAT）可以加速翻译过程并降低成本。
- en: 6.3 Building your first translator
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 构建你的第一个翻译器
- en: 'In this section, we are going to build a working MT system. Instead of writing
    any Python code to do that, we’ll make the most of existing MT frameworks. A number
    of open source frameworks make it easier to build MT systems, including Moses
    ([http://www.statmt.org/moses/](http://www.statmt.org/moses/)) for SMT and OpenNMT
    ([http://opennmt.net/](http://opennmt.net/)) for NMT. In this section, we will
    use Fairseq ([https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)),
    an NMT toolkit developed by Facebook that is becoming more and more popular among
    NLP practitioners these days. The following aspects make Fairseq a good choice
    for developing an NMT system quickly: 1) it is a modern framework that comes with
    a number of predefined state-of-the-art NMT models that you can use out of the
    box; 2) it is very extensible, meaning you can quickly implement your own model
    by following their API; and 3) it is very fast, supporting multi-GPU and distributed
    training by default. Thanks to its powerful models, you can build a decent quality
    NMT system within a couple of hours.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个可工作的 MT 系统。我们不会编写任何 Python 代码来实现，而是会充分利用现有的 MT 框架。许多开源框架使构建 MT 系统变得更加容易，包括
    Moses（[http://www.statmt.org/moses/](http://www.statmt.org/moses/)）用于 SMT 和 OpenNMT（[http://opennmt.net/](http://opennmt.net/)）用于
    NMT。在本节中，我们将使用 Fairseq（[https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)），这是
    Facebook 开发的一个 NMT 工具包，如今在 NLP 从业者中变得越来越流行。以下几个方面使 Fairseq 成为快速开发 NMT 系统的不错选择：1）它是一个现代化的框架，提供了许多预定义的最先进的
    NMT 模型，您可以立即使用；2）它非常可扩展，意味着您可以通过遵循它们的 API 快速实现自己的模型；3）它非常快速，默认支持多 GPU 和分布式训练。由于其强大的模型，您可以在几小时内构建一个质量不错的
    NMT 系统。
- en: Before you start, install Fairseq by running pip install fairseq in the root
    of your project directory. Also, run the following commands in your shell to download
    and expand the dataset (you may need to install unzip if you are using Ubuntu
    by running sudo apt-get install unzip):[²](#pgfId-1103788)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请在项目目录的根目录中运行`pip install fairseq`来安装 Fairseq。此外，请在您的 shell 中运行以下命令来下载并展开数据集（如果您使用的是
    Ubuntu，则可能需要安装 unzip，可以通过运行`sudo apt-get install unzip`来安装）:[²](#pgfId-1103788)
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are going to use Spanish and English parallel sentences from the Tatoeba
    project, which we used previously in chapter 4, to train a Spanish-to-English
    MT system. The corpus consists of approximately 200,000 English sentences and
    their Spanish translations. I went ahead and already formatted the dataset so
    that you can use it without worrying about obtaining the data, tokenizing the
    text, and so on. The dataset is already split into train, validate, and test subsets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Tatoeba 项目中的西班牙语和英语平行句子来训练一个西班牙语到英语的 MT 系统，这是我们在第 4 章中已经使用过的。该语料库包含大约
    20 万个英语句子及其西班牙语翻译。我已经提前格式化了数据集，这样您就可以在不必担心获取数据、标记文本等方面的情况下使用它。数据集已经分为训练、验证和测试子集。
- en: 6.3.1 Preparing the datasets
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 准备数据集
- en: As mentioned previously, MT systems (both SMT and NMT) are machine learning
    models and thus are trained from data. The development process of MT systems looks
    similar to any other modern NLP systems, as shown in figure 6.6\. First, the training
    portion of the parallel corpus is preprocessed and used to train a set of NMT
    model candidates. Next, the validation portion is used to choose the best-performing
    model out of all the candidates. This process is called *model selection* (see
    chapter 2 for a review). Finally, the best model is tested on the test portion
    of the dataset to obtain evaluation metrics, which reflect how good the model
    is.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，MT 系统（包括 SMT 和 NMT）是机器学习模型，因此是根据数据训练的。MT 系统的开发过程看起来与任何其他现代 NLP 系统相似，如图
    6.6 所示。首先，对平行语料库的训练部分进行预处理，并用于训练一组 NMT 模型候选者。接下来，使用验证部分来选择所有候选模型中表现最佳的模型。这个过程称为*模型选择*（请参阅第
    2 章进行复习）。最后，最佳模型将在数据集的测试部分上进行测试，以获得反映模型优劣的评估指标。
- en: '![CH06_F06_Hagiwara](../Images/CH06_F06_Hagiwara.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Hagiwara](../Images/CH06_F06_Hagiwara.png)'
- en: Figure 6.6 Pipeline for building an NMT system
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 构建 NMT 系统的流水线
- en: 'The first step in MT development is preprocessing the dataset. But before preprocessing,
    you need to convert the dataset into an easy-to-use format, which is usually plain
    text in NLP. In practice, the raw data for training MT systems come in many different
    formats, for example, plain text files (if you are lucky), XML formats of proprietary
    software, PDF files, and database records. Your first job is to format the raw
    files so that source sentences and their target translations are aligned sentence
    by sentence. The resulting file is often a TSV file where each line is a tab-separated
    *sentence pair*, which looks like the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MT 开发的第一步是对数据集进行预处理。但在进行预处理之前，你需要将数据集转换为易于使用的格式，通常是自然语言处理中的纯文本格式。实践中，用于训练 MT
    系统的原始数据以多种不同格式出现，例如，纯文本文件（如果你很幸运的话）、专有软件的 XML 格式、PDF 文件和数据库记录。你的第一项任务是对原始文件进行格式化，使源句子和它们的目标翻译按句子对齐。结果文件通常是一个
    TSV 文件，每行都是一个以制表符分隔的句子对，如下所示：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After the translations are aligned, the parallel corpus is fed into the preprocessing
    pipeline. Specific operations applied in this process differ from application
    to application, and from language to language, but the following steps are most
    common:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在翻译对齐后，平行语料被输入到预处理管道中处理。具体的操作因应用程序和语言而异，但以下步骤最为常见：
- en: Filtering
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤
- en: Cleaning
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理
- en: Tokenization
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: In the filtering step, any sentence pairs that are not suitable for training
    an MT system are removed from the dataset. What makes a sentence pair not suitable
    depends on many factors, but, for example, any sentence pair where either text
    is too long (say, more than 1,000 words) is not useful, because most MT models
    are not capable of modeling such a long sentence. Also, any sentence pairs where
    one sentence is too long but the other is too short are probably noise caused
    by a data processing or alignment error. For example, if a Spanish sentence is
    10 words long, the length of its English translation should fall within a 5- to
    15-word range. Finally, if, for any reason, the parallel corpus contains any languages
    other than the source and target languages, you should remove such sentence pairs.
    This happens a lot more often than you’d imagine—many documents are multilingual
    due to, for example, quotes, explanation, or code switching (mixing more than
    one language in a sentence). Language detection (see chapter 4) can help detect
    such anomalies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤步骤中，将从数据集中移除任何不适合用于训练 MT 系统的句子对。一个句子对是否太长、是否有用等因素影响很大，例如，任何其中一个文本长度过长（例如超过
    1000 个单词）的句子对都无用，因为大多数 MT 模型不能建模这样长的句子。此外，任何其中一个句子过长但另一个句子过短的句子对都可能是由于数据处理或对齐错误而引起的噪音。例如，如果一个西班牙语句子有
    10 个单词，其英语翻译的长度应该在 5 到 15 个单词之间。最后，如果平行语料库包含除源语言和目标语言之外的任何语言，应该移除这样的句子对。这种情况比你想象的要多得多——许多文档由于引用、解释或代码切换（在一个句子中混合多种语言）而成为多语言文档。语言检测（见第
    4 章）可以帮助检测到这些异常情况。
- en: After filtering, sentences in the dataset can be cleaned further. This process
    may include such things as removal of HTML tags and any special characters and
    normalization of characters (e.g., traditional and simplified Chinese) and spelling
    (e.g., American and British English).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后，数据集中的句子可以进一步清理。该过程可能包括删除 HTML 标签和任何特殊字符，以及对字符（例如，繁体中文和简体中文）和拼写（例如，美式英语和英式英语）进行归一化。
- en: If the target language uses scripts such as the Latin (a, b, c, ...) or Cyrillic
    (а, б, в, ...) alphabets, which distinguish upper- and lowercases, you may want
    to normalize case. By doing so, your MT system will group “NLP” with “nlp” and
    “Nlp.” This step is usually a good thing, because by having three different representations
    of a single concept, the MT model needs to learn that they are in fact a single
    concept purely from the data. Normalizing cases also reduces the number of distinct
    words, which makes training and prediction faster. However, this also groups “US”
    and “Us” and “us,” which might not be a desirable behavior, depending on the type
    of data and the domain you are working with. In practice, such decisions, including
    whether to normalize cases, are carefully made by observing their effect on the
    validation data performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标语言使用类似于拉丁（a，b，c，...）或西里尔（а，б，в，...）字母表的脚本，区分大小写，您可能需要规范化大小写。通过这样做，您的MT系统将“NLP”与“nlp”和“Nlp”分组在一起。通常，这是一件好事，因为通过具有三个不同表示的单一概念，MT模型必须从数据中学习它们实际上是单一概念。规范化大小写也会减少不同单词的数量，从而使训练和预测更快。但是，这也将“US”和“Us”以及“us”分组在一起，这可能不是一种理想的行为，具体取决于您处理的数据类型和领域。在实践中，这些决策，包括是否规范化大小写，都是通过观察它们对验证数据性能的影响来谨慎做出的。
- en: Data cleaning for machine translation and NLP
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译和NLP的数据清理
- en: Note that the cleaning techniques mentioned here are not specific to MT. Any
    NLP applications and tasks can benefit from a carefully crafted pipeline of filtering
    and cleaning operations. However, cleaning of the training data is particularly
    important for MT, because the consistency of translation goes a long way in building
    a robust MT model. If your training data uses “NLP” in some cases and “nlp” in
    others, the model will have a difficulty figuring out the proper way to translate
    the word, whereas humans would easily understand that the two words represent
    a single concept.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里提到的清理技术并不特定于MT。任何NLP应用和任务都可以从经过精心设计的过滤和清理操作的流程中受益。然而，对于MT来说，清理训练数据尤为重要，因为翻译的一致性对于构建强大的MT模型至关重要。如果您的训练数据在某些情况下使用“NLP”，而在其他情况下使用“nlp”，则模型将难以找到正确翻译该单词的方法，而人类很容易理解这两个单词代表一个概念。
- en: At this point, the dataset is still a bunch of strings of characters. Most MT
    systems operate on words, so you need to tokenize the input (section 3.3) to identify
    words. Depending on the language, you may need to run a different pipeline (e.g.,
    word segmentation is needed for Chinese and Japanese).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，数据集仍然是一堆字符字符串。大多数MT系统操作单词，因此您需要对输入进行标记化（第3.3节）以识别单词。根据语言，您可能需要运行不同的流程（例如，对于中文和日文，需要进行词段切分）。
- en: 'The Tatoeba dataset you downloaded and expanded earlier has already gone through
    all this preprocessing pipeline. Now you are ready to hand the dataset over to
    Fairseq. The first step is to tell Fairseq to convert the input files to the binary
    format so that the training script can read them easily, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您之前下载和展开的Tatoeba数据集已经通过了所有这些预处理流程。现在，您已经准备好将数据集交给Fairseq了。第一步是告诉Fairseq将输入文件转换为二进制格式，以便训练脚本可以轻松读取它们，如下所示：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When this succeeds, you should see a message Wrote preprocessed data to data/mt-bin
    on your terminal. You should also find the following group of files under the
    data/mt-bin directory:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当成功时，您应该在终端上看到一条“Wrote preprocessed data to data/mt-bin”的消息。您还应该在data/mt-bin目录下找到以下一组文件：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One of the key functionalities of this preprocessing step is to build the vocabulary
    (called the *dictionary* in Fairseq), which is a mapping from vocabulary items
    (usually words) to their IDs. Notice the two dictionary files in the directory,
    dict.en.txt and dict.es.txt. MT deals with two languages, so the system needs
    to maintain two mappings, one for each language.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此预处理步骤的关键功能之一是构建词汇表（在Fairseq中称为*dictionary*），它是从词汇项（通常为单词）到它们的ID的映射。注意目录中的两个字典文件dict.en.txt和dict.es.txt。MT涉及两种语言，因此系统需要维护两个映射，每个语言一个。
- en: 6.3.2 Training the model
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 训练模型
- en: 'Now that the train data is converted into the binary format, you are ready
    to train the MT model. Invoke the fairseq-train command with the directory where
    the binary files are located, along with several hyperparameters, as shown next:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练数据已转换为二进制格式，您可以准备好训练MT模型了。按下面所示使用包含二进制文件的目录以及几个超参数来调用fairseq-train命令：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You don’t have to worry about understanding what most of the parameters here
    mean (just yet). At this point, you need to know only that you are training a
    model using the data stored in the directory specified by the first parameter
    (data/mt-bin) using an LSTM architecture (—arch lstm) with a bunch of other hyperparameters,
    and saving the results in data/mt-ckpt (short for “checkpoint”).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必担心理解大多数参数的含义（至少暂时不用）。此时，您只需要知道使用指定目录中存储的数据（data/mt-bin）使用LSTM架构（-arch lstm）和一堆其他超参数来训练模型，并将结果保存在data/mt-ckpt（checkpoint的缩写）中即可。
- en: 'When you run this command, your terminal will show two types of progress bars
    alternatively—one for training and another for validating, as shown here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令时，终端会交替显示两种进度条——一个用于训练，另一个用于验证，如下所示：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The lines corresponding to validation results are easily distinguishable by
    their contents—they say “valid” subset. For each epoch, the training process alternates
    two stages: training and validation. An *epoch*, a concept used in machine learning,
    means one pass through the entire train data. In the training stage, the loss
    is calculated using the training data, then the model parameters are adjusted
    in such a way that the new set of parameters lowers the loss. In the validation
    stage, the model parameters are fixed, and a separate dataset (validation set)
    is used to measure how well the model is performing against the dataset.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 验证结果对应的行内容很容易区分——它们会说“验证”子集。每个时期，训练过程会轮流进行两个阶段：训练和验证。机器学习中使用的一个概念——一个*时期*，意味着对整个训练数据的一次遍历。在训练阶段，使用训练数据计算损失，然后以使新的参数集降低损失的方式调整模型参数。在验证阶段，模型参数被固定，使用一个单独的数据集（验证集）来衡量模型在该数据集上的表现。
- en: I mentioned in chapter 1 that validation sets are used for model selection,
    a process where the best machine learning model is chosen among all the possible
    models trained from a single training set. Here, by alternating between training
    and validation stages, we use the validation set to check the performance of all
    the intermediary models (i.e., the model after the first epoch, the one after
    two epochs, and so on). In other words, we use the validation stage to monitor
    the progress of the training.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第1章中提到过，验证集用于模型选择，这是从单个培训集中选择最佳的机器学习模型的过程。在这里，通过交替进行训练和验证阶段，我们使用验证集来检查所有中间模型（即第一个时期后的模型，第二个时期后的模型，等等）的性能。换言之，我们使用验证阶段来监视培训的进展情况。
- en: Why is this a good idea? We gain many benefits by inserting the validation stage
    after every epoch, but the most important one is to avoid overfitting—the very
    reason why a validation data is important in the first place. To illustrate this
    further, let’s look at how the loss changes over the course of the training of
    our Spanish-to-English MT model, for both the train and the validation sets, as
    shown in figure 6.7.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是个好方法？我们通过在每个时期之后插入验证阶段获得了许多好处，但最重要的好处是避免过度拟合——验证数据之所以重要正是因为这个原因。为了进一步说明这一点，让我们看看在我们的西班牙语到英语机器翻译模型的训练过程中，训练集和验证集的损失如何随着时间变化，如图6.7所示。
- en: 'As the training continues, the train loss becomes smaller and smaller and gradually
    approaches zero, because this is exactly what we told the optimizer to do: decrease
    the loss as much as possible. Checking whether the train loss is decreasing steadily
    epoch after epoch is a good “sanity check” that your model and the training pipeline
    are working as expected.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，训练损失变得越来越小，并逐渐趋近于零，因为这正是我们告诉优化器要做的：使损失尽可能地降低。检查训练损失是否在一个个时期后稳步下降是一个很好的“健全性检查”，可以验证您的模型和培训流水线是否按预期工作。
- en: On the other hand, if you look at the validation loss, it goes down at first
    for several epochs, but after a certain point, it gradually goes back up, forming
    a U-shaped curve—a typical sign of overfitting. After several epochs of training,
    your model fits the train set so well that it begins to lose its generalizability
    on the validation set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您看一下验证损失，它在前几个时期内会下降，但在一定点之后逐渐回升，形成一个U形曲线——这是过度拟合的一个典型迹象。经过几个时期的培训后，您的模型在训练集上表现得非常好，开始失去其对验证集的泛化性。
- en: '![CH06_F07_Hagiwara](../Images/CH06_F07_Hagiwara.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F07_Hagiwara](../Images/CH06_F07_Hagiwara.png)'
- en: Figure 6.7 Train and validation loss
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7训练和验证损失
- en: Let’s use a concrete example in MT to illustrate what’s really going on when
    a model is overfitted. For example, if your training data contains the English
    sentence “It is raining hard” and its Spanish translation “Esta lloviendo fuerte,”
    with no other sentences having the word “hard” in them, the overfitted model may
    believe that “fuerte” is the only possible translation of “hard.” A properly fitted
    model might leave some wiggle room for other Spanish words to appear as a translation
    for “hard,” but an overfitted MT system would always translate “hard” to “fuerte,”
    which is the “correct” thing to do according to the train set but obviously not
    ideal if you’d like to build a robust MT system. For example, the best way to
    translate “hard” in “She is trying hard” is not “fuerte.”
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用机器翻译中的一个具体例子来说明当模型过度拟合时实际发生了什么。例如，如果您的训练数据包含了英文句子“It is raining hard”及其西班牙语翻译“Esta
    lloviendo fuerte”，而其他句子中没有包含“hard”一词，那么过拟合的模型可能会认为“fuerte”是“hard”的唯一可能翻译。一个正确拟合的模型可能会留下一些余地，让其他西班牙语单词出现作为“hard”的翻译，但一个过拟合的机器翻译系统总是会将“hard”翻译为“fuerte”，这是根据训练集“正确”的做法，但显然不是您想要构建健壮的机器翻译系统的理想选择。例如，“She
    is trying hard”中“hard”的最佳翻译方式并不是“fuerte”。
- en: If you see your validation loss starting to creep up, there’s little point keeping
    the training process running, because chances are, your model has already overfitted
    to the data to some extent. A common practice in such a situation, called *early
    stopping*, is to terminate the training. Specifically, if your validation loss
    is not improving for a certain number of epochs, you stop the training and use
    the model at the point when the validation loss was the lowest. The number of
    epochs you wait until the training is terminated is called *patience*. In practice,
    the metric you care about the most (such as BLEU; see section 6.5.2) is used for
    early stopping instead of the validation loss.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到验证损失开始上升，那么继续保持训练过程是没有意义的，因为很有可能您的模型已经在某种程度上过度拟合了数据。在这种情况下的一种常见做法，称为*提前停止*，是终止训练。具体来说，如果您的验证损失在一定数量的轮次内没有改善，您就停止训练，并使用验证损失最低时的模型。等待训练终止的轮次数称为*耐心*。在实践中，最关心的指标（例如
    BLEU；请参阅第 6.5.2 节）用于提前停止，而不是验证损失。
- en: OK, that was enough about training and validating for now. The graph in figure
    6.7 indicates that the validation loss is lowest around epoch 8, so you can stop
    (by pressing Ctrl + C) the fairseq-train command after around 10 epochs; otherwise,
    the command would keep running indefinitely. Fairseq will automatically save the
    best model parameters (in terms of the validation loss) to the checkpoint_best.pt
    file.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在关于训练和验证就说到这里。图 6.7 中的图表表明验证损失在第 8 轮左右最低，所以你可以在大约 10 轮后停止（通过按 Ctrl + C），否则该命令会一直运行下去。Fairseq
    将自动将最佳模型参数（根据验证损失）保存到 checkpoint_best.pt 文件中。
- en: WARNING Note that the training may take a long time if you are just using a
    CPU. Chapter 11 explains how to use GPUs to accelerate the training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 如果您只使用 CPU 进行训练，可能需要很长时间。第 11 章解释了如何使用 GPU 加速训练。
- en: 6.3.3 Running the translator
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 运行翻译器
- en: 'After the model is trained, you can invoke the fairseq-interactive command
    to run your MT model on any input in an interactive way. You can run the command
    by specifying the binary file location and the model parameter file as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您可以调用 fairseq-interactive 命令以交互方式在任何输入上运行您的机器翻译模型。您可以通过指定二进制文件位置和模型参数文件来运行该命令，如下所示：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After you see the prompt Type the input sentence and press return, try typing
    (or copying and pasting) the following Spanish sentences one by one:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 看到提示“Type the input sentence and press return”后，尝试逐一输入（或复制粘贴）以下西班牙语句子：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note the punctuation and the whitespace in these sentences—Fairseq assumes
    that the input is already tokenized. Your results may vary slightly, depending
    on many factors (the training of deep learning models usually involves some randomness),
    but you get something along the line of the following (I added boldface for emphasis):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意这些句子中的标点和空白——Fairseq 假定输入已经进行了分词。您的结果可能会略有不同，这取决于许多因素（深度学习模型的训练通常涉及一些随机性），但您会得到类似以下的结果（我加粗了以示强调）：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Most of the output sentences here are almost perfect, except the fourth one
    (I would translate to “Are there free rooms?”). Even considering the fact that
    these sentences are all simple examples you can find in any travel Spanish phrasebook,
    this is not a bad start for a system built within an hour!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里大部分的输出句子都几乎完美，除了第四句（我会翻译成“有免费的房间吗？”）。即使考虑到这些句子都是任何一本旅行西班牙短语书中都可以找到的简单例子，但对于一个在一个小时内构建的系统来说，这并不是一个坏的开始！
- en: 6.4 How Seq2Seq models work
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 Seq2Seq模型的工作原理
- en: In this section, we will dive deep into the individual components that constitute
    a Seq2Seq model, which include the encoder and the decoder. We’ll also cover the
    algorithms used for decoding the target sentence—greedy decoding and beam search
    decoding.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨构成Seq2Seq模型的各个组件，包括编码器和解码器。我们还将涵盖用于解码目标句子的算法——贪婪解码和波束搜索解码。
- en: 6.4.1 Encoder
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 编码器
- en: As we saw in the beginning of this chapter, the encoder of a Seq2Seq model is
    not much different from the sequential-labeling models we covered in chapter 5\.
    Its main job is to take the input sequence (usually a sentence) and convert it
    into a vector representation of a fixed length. You can use an LSTM-RNN as shown
    in figure 6.8.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开始看到的，Seq2Seq模型的编码器与我们在第5章中讨论的顺序标记模型并没有太大的不同。它的主要工作是接受输入序列（通常是一个句子）并将其转换为固定长度的向量表示。你可以使用像图6.8中所示的LSTM-RNN。
- en: '![CH06_F08_Hagiwara](../Images/CH06_F08_Hagiwara.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Hagiwara](../Images/CH06_F08_Hagiwara.png)'
- en: Figure 6.8 Encoder of a Seq2Seq model
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 Seq2Seq模型的编码器
- en: Unlike sequential-labeling models, we need only the final hidden state of an
    RNN, which is then passed to the decoder to generate the target sentence. You
    can also use a multilayer RNN as an encoder, in which case the sentence representation
    is the concatenation of the output of each layer, as illustrated in figure 6.9.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与顺序标记模型不同，我们只需要RNN的最终隐藏状态，然后将其传递给解码器生成目标句子。你也可以使用多层RNN作为编码器，这种情况下句子表示是每一层输出的串联，如图6.9所示。
- en: '![CH06_F09_Hagiwara](../Images/CH06_F09_Hagiwara.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Hagiwara](../Images/CH06_F09_Hagiwara.png)'
- en: Figure 6.9 Using a multilayer RNN as an encoder
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 使用多层RNN作为编码器
- en: Similarly, you can use a bidirectional (or even a bidirectional multilayer)
    RNN as an encoder. The final sentence representation is a concatenation of the
    output of the forward and the backward layers, as shown in figure 6.10.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，你可以使用双向（甚至是双向多层）RNN作为编码器。最终的句子表示是正向层和反向层输出的串联，如图6.10所示。
- en: '![CH06_F10_Hagiwara](../Images/CH06_F10_Hagiwara.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F10_Hagiwara](../Images/CH06_F10_Hagiwara.png)'
- en: Figure 6.10 Using a bidirectional RNN as an encoder
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 使用双向RNN作为编码器
- en: 'NOTE This is a small detail, but remember that an LSTM cell produces two types
    of output: the cell state and the hidden state (see section 4.2.2 for review).
    When using LSTM for encoding a sequence, we usually just use the final hidden
    state while discarding the cell state. Think of the cell state as something like
    a temporary loop variable used for computing the final outcome (the hidden state).
    See figure 6.11 for an illustration.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这是一个小细节，但要记得LSTM单元产生两种类型的输出：单元状态和隐藏状态（请参阅4.2.2节）。在使用LSTM编码序列时，我们通常只使用最终隐藏状态，而丢弃单元状态。把单元状态看作是类似于临时循环变量，用于计算最终结果（隐藏状态）。请参见图6.11进行说明。
- en: '![CH06_F11_Hagiwara](../Images/CH06_F11_Hagiwara.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F11_Hagiwara](../Images/CH06_F11_Hagiwara.png)'
- en: Figure 6.11 An encoder using LSTM cells
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 使用LSTM单元的编码器
- en: 6.4.2 Decoder
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 解码器
- en: Likewise, the decoder of a Seq2Seq model is similar to the language model we
    covered in chapter 5\. In fact, they are identical except for one crucial difference—a
    decoder takes an input from the encoder. The language models we covered in chapter
    5 are called *unconditional language models* because they generate language without
    any input or precondition. On the other hand, language models that generate language
    based on some input (condition) are called *conditional language models*. A Seq2Seq
    decoder is one type of conditional language model, where the condition is the
    sentence representation produced by the encoder. See figure 6.12 for an illustration
    of how a Seq2Seq decoder works.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Seq2Seq 模型的解码器与我们在第 5 章中介绍的语言模型类似。实际上，它们除了一个关键的区别外完全相同——解码器从编码器那里获取输入。我们在第
    5 章中介绍的语言模型称为*无条件语言模型*，因为它们在没有任何输入或前提条件的情况下生成语言。另一方面，根据某些输入（条件）生成语言的语言模型称为*条件语言模型*。Seq2Seq
    解码器是一种条件语言模型，其中条件是编码器生成的句子表示。请参见图 6.12，了解 Seq2Seq 解码器的工作原理的示例。
- en: '![CH06_F12_Hagiwara](../Images/CH06_F12_Hagiwara.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F12_Hagiwara](../Images/CH06_F12_Hagiwara.png)'
- en: Figure 6.12 A decoder of a Seq2Seq model
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 Seq2Seq 模型的解码器
- en: Just as with language models, Seq2Seq decoders generate text from left to right.
    Like the encoder, you can use an RNN to do this. A decoder can also be a multilayer
    RNN. However, a decoder cannot be bidirectional—you cannot generate a sentence
    from both sides. As was mentioned in chapter 5, models that operate on the past
    sequence they produced are called *autoregressive models*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 就像语言模型一样，Seq2Seq 解码器从左到右生成文本。与编码器一样，您可以使用 RNN 来实现这一点。解码器也可以是多层 RNN。然而，解码器不能是双向的——你不能从两边生成一个句子。正如第
    5 章中提到的那样，对过去生成的序列进行操作的模型被称为*自回归模型*。
- en: Non-autoregressive models
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 非自回归模型
- en: 'If you think simply generating text from left to right is too limiting, you
    have a good point. Humans also do not always write language linearly—we often
    revise, add, and delete words and phrases afterward. Also, generating text in
    a linear fashion is not very efficient. The latter half of a sentence needs to
    wait until its first half is completed, which makes it very difficult to parallelize
    the generation process. As of this writing, researchers are putting a lot of effort
    into developing non-autoregressive MT models that do not generate the target sentence
    in a linear fashion (see, for example, this paper from Salesforce Research: [https://arxiv.org/abs/1711.02281](https://arxiv.org/abs/1711.02281)).
    However, they haven’t exceeded autoregressive models in terms of translation quality,
    and most research and production MT systems still adopt autoregressive models.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为简单地从左到右生成文本太过受限制，那么你有道理。人类也不总是线性地写语言——我们经常在之后修订、添加和删除单词和短语。此外，线性地生成文本并不是很高效。句子的后半部分需要等待直到它的前半部分完成，这使得并行化生成过程非常困难。截至本文撰写时，研究人员正在大力开发非自回归的机器翻译模型，这些模型不会以线性方式生成目标句子（例如，请参阅
    Salesforce Research 的这篇论文：[https://arxiv.org/abs/1711.02281](https://arxiv.org/abs/1711.02281)）。然而，它们在翻译质量上还没有超过自回归模型，大多数研究和生产的机器翻译系统仍然采用自回归模型。
- en: How the decoder behaves is a bit different between the training and the prediction
    stages. Let’s see how it is trained first. At the training stage, we know exactly
    how the source sentence should be translated into the target sentence. In other
    words, we know exactly what the decoder should produce, word by word. Because
    of this, decoders are trained in a similar way to how sequential-labeling models
    are trained (see chapter 5).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器在训练阶段和预测阶段的行为略有不同。让我们先看看它是如何训练的。在训练阶段，我们确切地知道源句应该被翻译成目标句。换句话说，我们确切地知道解码器应该逐词生成什么。因此，解码器的训练方式与顺序标记模型的训练方式相似（参见第
    5 章）。
- en: First, the decoder is fed the sentence representation produced by the encoder
    and a special token <START>, which indicates the start of a sentence. The first
    RNN cell processes these two inputs and produces the first hidden state. The hidden
    state vector is fed to a linear layer that shrinks or expands this vector to match
    the size of the vocabulary. The resulting vector then goes through softmax, which
    converts it to a probability distribution. This distribution dictates how likely
    each word in the vocabulary is to come next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，解码器被喂入由编码器产生的句子表示和一个特殊标记<START>，该标记表示句子的开始。 第一个RNN单元处理这两个输入并产生第一个隐藏状态。 隐藏状态向量被馈送到一个线性层，该层收缩或扩展此向量以匹配词汇表的大小。
    然后得到的向量通过softmax，将其转换为概率分布。 此分布规定了词汇表中每个单词在接下来出现的可能性。  '
- en: 'Then, this is where the training happens. If the input is “Maria no daba una
    bofetada a la bruja verde,” then we would like the decoder to produce its English
    equivalent: “Mary did not slap the green witch.” This means that we would like
    to maximize the probability that the first RNN cell generates “Mary” given the
    input sentence. This is a multiclass classification problem we have seen many
    times so far in this book—word embeddings (chapter 3), sentence classification
    (chapter 4), and sequential labeling (chapter 5). You use the cross-entropy loss
    to measure how far apart the desired outcome is from the actual output of your
    network. If the probability for “Mary” is large, then good—the network incurs
    a small loss. On the other hand, if the probability for “Mary” is small, then
    the network incurs a large loss, which encourages the optimization algorithm to
    change the parameters (magic constants) by a large amount.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这就是训练发生的地方。 如果输入是“Maria no daba una bofetada a la bruja verde”，那么我们希望解码器生成其英文等效句子：“Mary
    did not slap the green witch.” 这意味着我们希望最大化第一个RNN单元生成“Mary”的概率，给定输入句子。 这是本书中我们在很多地方见过的一个多类别分类问题——词嵌入（第3章），句子分类（第4章）和序列标记（第5章）。
    您使用交叉熵损失来衡量期望结果与网络实际输出之间的差距有多远。 如果“Mary”的概率很大，那么好——网络会产生较小的损失。 另一方面，如果“Mary”的概率很小，则网络会产生较大的损失，这会鼓励优化算法大幅更改参数（魔法常量）。
- en: Then, we move on to the next cell. The next cell receives the hidden state computed
    by the first cell and the word “Mary,” *regardless of what the first cell generated*.
    Instead of feeding the token generated by the previous cell, as we did when generating
    text using a language model, we constrain the input to the decoder so that it
    won’t “go astray.” The second cell produces the hidden state based on these two
    inputs, which is then used to compute the probability distribution for the second
    word. We compute the cross-entropy loss by comparing the distribution against
    the desired output “did” and move on to the next cell. We keep doing this until
    we reach the final token, which is <END>. The total loss for the sentence is the
    average of all the losses incurred for all the words in the sentence, as shown
    in figure 6.13.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们移动到下一个单元。 下一个单元接收由第一个单元计算的隐藏状态和单词“Mary”，*不管第一个单元生成了什么*。 与使用语言模型生成文本时喂入先前单元生成的标记不同，我们约束解码器的输入，以防止其“偏离”。
    第二个单元基于这两个输入产生隐藏状态，然后用于计算第二个单词的概率分布。 我们通过将分布与期望输出“did”进行比较来计算交叉熵损失，并继续移动到下一个单元。
    我们一直这样做，直到达到最终标记，即<END>。 句子的总损失是句子中所有单词产生的所有损失的平均值，如图6.13所示。
- en: '![CH06_F13_Hagiwara](../Images/CH06_F13_Hagiwara.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_Hagiwara](../Images/CH06_F13_Hagiwara.png)'
- en: Figure 6.13 Training a Seq2Seq decoder
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 训练Seq2Seq解码器
- en: Finally, the loss computed this way is used to adjust the model parameters of
    the decoder, so that it can generate the desired output the next time around.
    Note that the parameters of the encoder are also adjusted in this process, because
    the loss propagates all the way back to the encoder through the sentence representation.
    If the sentence representation produced by the encoder is not good, the decoder
    won’t be able to produce high-quality target sentences no matter how hard it tries.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以这种方式计算的损失用于调整解码器的模型参数，以便下一次它能生成期望的输出。 请注意，在此过程中也会调整编码器的参数，因为损失通过句子表示一直传播回编码器。
    如果编码器产生的句子表示不好，那么解码器无论如何努力，都无法生成高质量的目标句子。
- en: 6.4.3 Greedy decoding
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 贪婪解码
- en: Now let’s look at how the decoder behaves at the prediction stage, where a source
    sentence is given to the network, but we don’t know what the correct translation
    should be. At this stage, a decoder behaves a lot like the language models we
    discussed in chapter 5\. It is fed the sentence representation produced by the
    encoder, as well as a special token <START>, which indicates the start of a sentence.
    The first RNN cell processes these two inputs and produces the first hidden state,
    which is then fed to the linear layer and the softmax layer to produce the probability
    distribution over the target vocabulary. Here comes the key part—unlike the training
    phase, you don’t know the correct word to come next, so you have multiple options.
    You can choose any random word that has a reasonably high probability (say, “dog”),
    but probably the best you can do is pick the word whose probability is the highest
    (you are lucky if it’s “Mary”). The MT system produces the word that was just
    picked and then feeds it to the next RNN cell. This is repeated until the special
    token <END> is encountered. Figure 6.14 illustrates this process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看解码器在预测阶段的行为，其中给定了一个源句子给网络，但我们不知道正确的翻译应该是什么。在这个阶段，解码器的行为很像我们在第5章讨论过的语言模型。它被提供了由编码器产生的句子表示，以及一个特殊的标记<START>，表示句子的开头。第一个循环神经网络单元处理这两个输入并产生第一个隐藏状态，然后将其馈送到线性层和softmax层，以产生目标词汇的概率分布。关键部分来了——与训练阶段不同，你不知道接下来应该出现的正确单词，所以你有多个选项。你可以选择任何一个具有相当高概率的随机单词（比如“dog”），但最好的选择可能是选择概率最高的单词（如果是“Mary”那就太幸运了）。机器翻译系统生成刚刚选择的单词，然后将其馈送到下一个循环神经网络单元。这个过程重复进行，直到遇到特殊标记<END>。图6.14说明了这个过程。
- en: '![CH06_F14_Hagiwara](../Images/CH06_F14_Hagiwara.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F14_Hagiwara](../Images/CH06_F14_Hagiwara.png)'
- en: Figure 6.14 A prediction using a Seq2Seq decoder
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 使用Seq2Seq解码器进行预测
- en: OK, so are we all good, then? Can we move on to evaluating our MT system, because
    it is doing everything it can to produce the best possible translation? Not so
    fast—many things could go wrong by decoding the target sentence in this manner.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们都准备好了吗？我们可以继续评估我们的机器翻译系统了吗，因为它正在尽其所能产生最佳的翻译？不要那么快——在这种方式解码目标句子时可能会出现许多问题。
- en: First of all, the goal of MT decoding is to maximize the probability of the
    target sentence as a whole, not just individual words. This is exactly what you
    trained the network to do—to produce the largest probability for correct sentences.
    However, the way words are picked at each step described earlier is to maximize
    the probability of that word only. In other words, this decoding process guarantees
    only the locally maximum probability. This type of myopic, locally optimal algorithm
    is called *greedy* in computer science, and the decoding algorithm I just explained
    is called *greedy decoding*. However, just because you are maximizing the probability
    of individual words at each step doesn’t mean you are maximizing the probability
    of the whole sentence. Greedy algorithms, in general, are not guaranteed to produce
    the globally optimal solution, and using greedy decoding can leave you stuck with
    suboptimal translations. This is not very intuitive to understand, so let me use
    a simple example to illustrate this.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，机器翻译解码的目标是最大化整个目标句子的概率，而不仅仅是单个单词。这正是你训练网络要做的事情——为正确的句子产生最大的概率。然而，前面描述的每一步选择单词的方式是为了最大化该单词的概率。换句话说，这种解码过程只保证了局部最大概率。这种短视、局部最优的算法在计算机科学中被称为*贪婪*，我刚刚解释的解码算法被称为*贪婪解码*。然而，仅仅因为你在每一步都在最大化单词的概率并不意味着你在最大化整个句子的概率。一般来说，贪婪算法不能保证产生全局最优解，而使用贪婪解码可能会让你陷入次优翻译的困境。这并不是很直观，所以让我用一个简单的例子来说明这一点。
- en: When you are picking words at each timestep, you have multiple words to pick
    from. You pick one of them and move on to the next RNN cell, which produces another
    set of possible words to pick from, depending on the word you picked previously.
    This can be represented using a tree structure like the one shown in figure 6.15\.
    The diagram shows how the word you pick at one timestep (e.g., “did”) branches
    out to another set of possible words (“you” and “not”) to pick from at the next
    timestep.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在每个时间步选择单词时，你有多个单词可以选择。你选择其中一个然后移动到下一个循环神经网络单元，它会产生另一组可能选择的单词，这取决于你之前选择的单词。这可以用一个树状结构来表示，就像图6.15所示的那样。该图显示了你在一个时间步选择的单词（例如“did”）如何分支到下一个时间步可以选择的一组可能单词（“you”和“not”）。
- en: '![CH06_F15_Hagiwara](../Images/CH06_F15_Hagiwara.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F15_Hagiwara](../Images/CH06_F15_Hagiwara.png)'
- en: Figure 6.15 A decoding decision tree
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 解码决策树
- en: Each transition from word to word is labeled with a score, which corresponds
    to how large the probability of choosing that transition is. Your goal here is
    to maximize the total sum of the scores when you traverse one path from timestep
    1 to 4\. Mathematically, probabilities are real numbers between 0 to 1, and you
    should multiply (instead of add) each probability to get the total, but I’m simplifying
    things here. For example, if you go from “Mary” to “did,” then on to “you” and
    “do,” you just generated a sentence “Mary did you do” and the total score is 1
    + 5 + 1 = 7.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词到单词的转换都加上了一个分数，该分数对应于选择该转换的概率有多大。你的目标是在从时间步1到4遍历一条路径时最大化得分的总和。在数学上，概率是0到1之间的实数，并且你应该将（而不是相加）每个概率相乘以获得总数，但我在这里简化了问题。例如，如果你从“Mary”到“did”，然后到“you”和“do”，你刚生成了一个句子“Mary
    did you do”，总分是1 + 5 + 1 = 7。
- en: 'The greedy decoder we saw earlier will face two choices after it generates
    “did” at timestep 2: either generate “you” with a score of 5 or “not” with a score
    of 3\. Because all it does is pick the one with the highest score, it will pick
    “you” and move on. Then it will face another branch after timestep 3—generating
    “do” with a score of 1 or generating “know” with a score of 2\. Again, it will
    pick the largest score, and you will end up with the translation “Mary did you
    know” whose score is 1+ 5 + 1 = 8.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前看到的贪婪解码器生成时间步2的“did”后，它将面临两个选择：用5分数生成“you”或用3分数生成“not”。因为它只是选择得分最高的那个，它会选择“you”并继续前进。然后在时间步3之后，它将面临另一个分支——用1分数生成“do”或用2分数生成“know”。同样，它将选择最大的分数，这样你就会得到“Mary
    did you know”的翻译，其分数为1+ 5 + 1 = 8。
- en: This is not a bad result. At least, it is not as bad as the first path, which
    sums up to a score of 7\. By picking the maximum score at each branch, you are
    making sure that your final result is at least decent. However, what if you picked
    “not” at timestep 3? At first glance, this doesn’t seem like a good idea, because
    the score you get is only 3, which is smaller than you’d get by taking the other
    path, 5\. But at the next timestep, by generating “slap,” you get a score of 5\.
    In retrospect, this was the right thing to do—in total, you get 1 + 3 + 5 = 9,
    which is larger than any scores you’d get by taking the other “you” path. By sacrificing
    short-term rewards, you are able to gain even larger rewards in the long run.
    But due to the myopic nature of the greedy decoder, it will never choose this
    path—it can’t backtrack and change its mind once it’s taken one branch over another.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一个坏结果。至少，它不像第一条路径一样糟糕，它的总得分为7。通过在每个分支上选择最大分数，你确保你的最终结果至少是像样的。然而，如果你在时间步3选择了“not”呢？乍一看，这似乎不是个好主意，因为你得到的分数只有3，比你走另一条路径的5小。但在下一个时间步，通过生成“slap”，你得到了5分的分数。回顾起来，这是正确的决定——总体而言，你得到了1
    + 3 + 5 = 9分，这比沿着另一个“you”路径得到的分数要高。通过牺牲短期回报，你能够在长期获得更大的回报。但是由于贪婪解码器的近视性质，它永远不会选择这条路径——它无法回溯并改变其心意，一旦选择了一条而不是另一条路径。
- en: Choosing which way to go to maximize the total score seems easy if you look
    at the toy example in figure 6.15, but in reality, you can’t “foresee” the future—if
    you are at timestep t, you can’t predict what will happen at timestep t + 1 and
    onward, until you actually choose one word and feed it to the RNN. But the path
    that maximizes the individual probability is not necessarily the optimal solution.
    You just can’t try every possible path and see what score you’d get, either, because
    the vocabulary usually contains tens of thousands of unique words, meaning the
    number of possible paths is exponentially large.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果看一下图6.15中的玩具示例，选择哪个方向以最大化总分数似乎很容易，但在现实中，你不能“预见”未来——如果你处于时间步t，你无法预测在时间步t +
    1及以后会发生什么，直到你实际选择一个单词并将其馈送到RNN中为止。但是，最大化单个概率的路径不一定是最优解。你无法尝试每个可能的路径并查看你得到的分数，因为词汇表通常包含成千上万个独特的单词，这意味着可能的路径数呈指数增长。
- en: The sad truth is that you can’t realistically expect to find the optimal path
    that maximizes the probability for the entire sentence within a reasonable amount
    of time. But you can avoid being stuck (or at least, make it less likely to be
    stuck) with a suboptimal solution, which is what the beam search decoder does.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 令人沮丧的事实是，您无法合理地期望在短时间内找到最大化整个句子概率的最优解路径。但是你可以避免陷入困境（或至少减少陷入困境的可能性），这就是梁搜索解码器的作用。
- en: 6.4.4 Beam search decoding
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 梁搜索解码
- en: Let’s think what you would do if you were in the same situation. Let’s use an
    analogy and say you are a college sophomore and need to decide which major to
    pursue by the end of the school year. Your goal is to maximize the total amount
    of income (or happiness or whatever thing you care about) over the course of your
    lifetime, but you don’t know which major is the best for this. You can’t simply
    try every possible major and see what happens after a couple of years—there are
    too many majors and you can’t go back in time. Also, just because some particular
    majors look appealing in the short run (e.g., choosing an economics major may
    lead to some good internship opportunities at large investment banks) doesn’t
    mean that path is the best in the long run (see what happened in 2008).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象如果你处于同样的情境下应该怎么做。假设你是一名大学大二的学生，在本学年结束前，你需要决定选择哪个专业。你的目标是在你一生中最大化收入（或幸福或其他你关心的东西），但你不知道哪个专业对于这一目标来说是最好的。你不能尝试每个可能的专业并观察几年后的结果——专业太多，你也无法回到过去。并且仅仅因为有些专业在短期内看起来很有吸引力（例如选择经济学专业可能会带来在大型投资银行的好实习机会），并不意味着这条道路在长期来看是最好的（请看2008年发生了什么）。
- en: In such a situation, one thing you could do is to hedge your bet by pursuing
    more than one major (as a double major or a minor) at the same time instead of
    committing 100% to one particular major. After a couple of years, if the situation
    is more different than you had imagined, you can still change your mind and pursue
    another option, which is not possible if you choose your major greedily (i.e.,
    based only on the short-term prospects).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以做的一件事是通过同时选择多个专业（双专业或辅修）而不是100%致力于特定的专业，来进行投机。几年后，如果情况与你想象的不同，仍然可以更改主意并追求另一种选择，如果你贪婪地选择专业（即只考虑短期前景），则这是不可能的。
- en: The main idea of beam search decoding is similar to this—instead of committing
    to one path, it purses multiple paths (called *hypotheses*) at the same time.
    In this way, you leave some room for “dark horses,” that is, hypotheses that had
    low scores at first but may prove promising later. Let’s see this in action using
    the example in figure 6.16, a slightly modified version of figure 6.15.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 梁搜索解码的主要思想类似于这个——不是只选择一条路径，而是同时追求多条路径（称为*假设*）。这样，你就为“黑马”留下了一些空间，也就是那些在最初得分不高但可能后来表现出色的假设。让我们使用图6.16中的示例，这是图6.15的略微修改版本。
- en: The key idea of beam search decoding is to use a *beam* (figure 6.16 bottom),
    which you can think of as some sort of buffer that can retain multiple hypotheses
    at the same time. The size of the beam, that is, the number of hypotheses it can
    retain, is called the *beam width*. Let’s use a beam of size 2 and see what happens.
    Initially, your first hypothesis consists of only one word, “Mary,” and a score
    of 0\. When you move on to the next word, the word you chose is appended to the
    hypothesis, and the score is incremented by the score of the path you have just
    taken. For example, when you move on to “did,” it will make a new hypothesis consisting
    of “Mary did” and a score of 1.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 梁搜索解码的关键思想是使用*梁*（图6.16底部），可以将其看作是一种缓冲区，可以同时保留多个假设。梁的大小，即它可以保持的假设数，称为*梁宽度*。让我们使用大小为2的梁并看看会发生什么。最初，你的第一个假设只包含一个词“Mary”，得分为0。当你转向下一个单词时，你选择的单词被附加到假设中，并且得分增加了你刚刚走过的路径的得分。例如，当你转到“did”时，它会生成一个新的假设，包含“Mary
    did”和得分1。
- en: '![CH06_F16_Hagiwara](../Images/CH06_F16_Hagiwara.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F16_Hagiwara](../Images/CH06_F16_Hagiwara.png)'
- en: Figure 6.16 Beam search decoding
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 梁搜索解码
- en: 'If you have multiple words to choose from at any particular timestep, a hypothesis
    can spawn multiple child hypotheses. At timestep 2, you have three different choices—“you,”
    “not,” and “n’t”—which generate three new child hypotheses: [Mary did you] (6),
    [Mary did not] (4), and [Mary did n’t] (3). And here’s the key part of beam search
    decoding: because there’s only so much room in the beam, any hypotheses that are
    not good enough fall off of the beam after sorting them by their scores. Because
    the beam can hold up to only two hypotheses in this example, anything except the
    top two gets kicked out of the beam, which leaves [Mary did you] (6) and [Mary
    did not] (4).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在任何特定时间步有多个词可供选择，假设可能会产生多个子假设。在时间步2，你有三个不同的选择——“你”，“不”和“n’t”——这会生成三个新的子假设：[Mary
    did you] (6)，[Mary did not] (4) 和 [Mary did n’t] (3)。这就是束搜索解码的关键部分：因为束中只有有限的空间，任何不够好的假设在按分数排序后都会从束中掉下来。因为在这个例子中束只能容纳两个假设，除了前两个以外的任何东西都会被挤出束外，这就留下了[Mary
    did you] (6)和[Mary did not] (4)。
- en: At timestep 3, each remaining hypothesis can spawn up to two child hypotheses.
    The first one ([Mary did you] (6)) will generate [Mary did you know] (8) and [Mary
    did you do] (7), whereas the second one ([Mary did not] (4)) turns into [Mary
    did not slap] (9). These three hypotheses are sorted by their scores, and the
    best two will be returned as the result of the beam search decoding.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步3，每个剩余的假设可以产生多达两个子假设。第一个（[Mary did you] (6)）将生成[Mary did you know] (8)和[Mary
    did you do] (7)，而第二个（[Mary did not] (4)）会变成[Mary did not slap] (9)。这三个假设按分数排序，最好的两个将作为束搜索解码的结果返回。
- en: 'Congratulations—now your algorithm was able to find the path that maximizes
    the total sum of the scores. By considering multiple hypotheses at the same time,
    beam search decoding can increase the chance that you will find better solutions.
    However, it is never perfect—notice that an equally good path [Mary did n’t do]
    with a score of 9 fell out of the beam as early as timestep 3\. To “rescue” it,
    you’d need to increase the beam width to 3 or larger. In general, the larger the
    beam width, the higher the expected quality of the translation results will be.
    However, there’s a tradeoff: because the computer needs to consider multiple hypotheses,
    it will be linearly slower as the beam width increases.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜——现在你的算法能够找到最大化总分数的路径。通过同时考虑多个假设，束搜索解码可以增加你找到更好解决方案的机会。然而，它永远不是完美的——注意，一个同样好的路径[Mary
    did n’t do]，得分为9，在时间步3 就从束中掉出来了。要“拯救”它，你需要增加束宽度到3或更大。一般来说，束宽度越大，翻译结果的期望质量就越高。然而，这是有一个折衷的：因为计算机需要考虑多个假设，随着束宽度的增加，它会线性变慢。
- en: 'In Fairseq, you can use the —beam option to change the beam size. In the example
    in section 6.3.3, I used —beam 5 to use a beam width of 5\. You were already using
    beam search without noticing. If you invoke the same command with —beam 1, which
    means you are using greedy decoding instead of a beam search, you may get slightly
    different results. When I tried this, I got almost the same results except the
    last one: “counts, please,” which is not a great translation for “La cuenta, por
    favor.” This means using a beam search indeed helps improve the translation quality!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fairseq 中，你可以使用 —beam 选项来改变束大小。在第 6.3.3 节的示例中，我使用了 —beam 5 来使用束宽度为 5。你已经在不知不觉中使用了束搜索。如果你使用相同的命令调用
    —beam 1，这意味着你使用的是贪婪解码而不是束搜索，你可能会得到略有不同的结果。当我尝试这样做时，我得到的结果几乎相同，除了最后一个：“counts,
    please”，这不是“La cuenta, por favor.” 的一个很好的翻译。这意味着使用束搜索确实有助于提高翻译质量！
- en: 6.5 Evaluating translation systems
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 评估翻译系统
- en: In this section, I’d like to briefly touch on the topic of evaluating machine
    translation systems. Accurately evaluating MT systems is an important topic, both
    in theory and practice.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我想简要谈谈评估机器翻译系统的话题。准确评估机器翻译系统是一个重要的话题，无论在理论上还是在实践中。
- en: 6.5.1 Human evaluation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 人工评估
- en: The simplest and the most accurate way to evaluate MT systems’ output is to
    use human evaluation. After all, language is translated for humans. Translations
    that are deemed good by humans should be good.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器翻译系统输出的最简单、最准确的方法是使用人工评估。毕竟，语言是为人类翻译的。被人类认为好的翻译应该是好的。
- en: As mentioned previously, we have a few considerations for what makes a translation
    good. There are two most important and commonly used concepts for this—*adequacy*
    (also called *fidelity*) and *fluency* (also closely related to intelligibility).
    Adequacy is the degree to which the information in the source sentence is reflected
    in the translation. If you can reconstruct a lot of information expressed by the
    source sentence by reading its translation, then the translation has high adequacy.
    Fluency is, on the other hand, how natural the translation is in the target language.
    If you are translating into English, for example, “Mary did not slap the green
    witch” is a fluent translation, whereas “Mary no had a hit with witch, green”
    is not, although both translations are almost equally adequate. Note that these
    two aspects are somewhat independent—you can think of a translation that is fluent
    but not adequate (e.g., “Mary saw a witch in the forest” is a perfectly fluent
    but inadequate translation) and vice versa, like the earlier example. MT systems
    that produce output that is both adequate and fluent are desirable.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，我们对好的翻译有一些考虑因素。对于这两个方面最重要且常用的概念是充分性(也称为忠实度)和流畅性(也与可理解性密切相关)。充分性是源句子中的信息在翻译中反映的程度。如果通过阅读它的翻译，你可以重构源句子表达的大量信息，那么该翻译具有很高的充分性。流畅性则是翻译在目标语言中的自然程度。例如，如果你正在翻译为英语，“Mary
    did not slap the green witch”是一种流畅的翻译，而“Mary no had a hit with witch, green”则不是，尽管这两种翻译几乎是同样充分的。请注意，这两个方面在某种程度上是独立的-你可以想象一种流畅但不充分的翻译(例如，“Mary
    saw a witch in the forest”是一种完全流畅但不充分的翻译)，反之亦然，就像之前的例子一样。能够产生既充分又流畅输出的MT系统是有价值的。
- en: An MT system is usually evaluated by presenting its translations to human annotators
    and having them judge its output on a 5- or 7-point scale for each aspect. Fluency
    is easier to judge because it requires only monolingual speakers of the target
    sentence, whereas adequacy requires bilingual speakers of both the source and
    target languages.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: MT系统通常通过将其翻译呈现给人类注释器并要求他们对其每个方面进行5或7点的评价来进行评估。流畅性更容易判断，因为它只需要目标句子的单语种听众，而充分性需要源语言和目标语言的双语种人员。
- en: 6.5.2 Automatic evaluation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 自动评估
- en: Although human evaluation gives the most accurate assessment to MT systems’
    quality, it’s not always feasible. In most cases, you cannot afford to hire human
    evaluators to assess an MT system’s output every time you need it. If you are
    dealing with language pairs that are not common, you might not be able to find
    bilingual speakers for evaluating adequacy at all.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工评估给出了MT系统质量的最准确评估，但并不总是可行的。在大多数情况下，你可能无法负担雇用人类评估员以在你需要时评估MT系统的输出。如果你处理的是不常见的语言对，你可能根本找不到双语的说话者来评估其充分性。
- en: But more importantly, you need to constantly evaluate and monitor an MT system’s
    quality when you are developing one. For example, if you use a Seq2Seq model to
    train an NMT system, you need to reevaluate its performance every time you adjust
    one of the hyperparameters. Otherwise, you wouldn’t know whether your change has
    a good or bad effect on its final performance. Even worse, if you were to do something
    like early stopping (see section 6.3.2) to determine when to stop the training
    process, you would need to evaluate its performance *after every epoch*. You can’t
    possibly hire somebody and have them evaluate your intermediate models at each
    epoch—that would be a terribly slow way to develop an MT system. It’s also a huge
    waste of time, because the output of initial models is largely garbage and does
    not warrant human evaluation. A large amount of correlation exists between the
    outputs of intermediate models, and human evaluators would be spending a lot of
    time evaluating very similar, if not identical, sentences.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 但更重要的是，在开发MT系统时需要不断地评估和监测其质量。例如，如果你使用Seq2Seq模型去训练一个NMT系统，你需要每次调整一个超参数就重新评估其性能。否则，你就不知道你的更改对它的最终性能是否有好或坏的影响。更糟糕的是，如果你要做像“early
    stopping”(见第6.3.2节)这样的事情来决定何时停止训练过程，你需要在每个周期之后评估它的性能。你不可能雇用人来在每个周期评估你的中间模型-这将是开发MT系统的一个可怕的缓慢的方法。这也是浪费时间，因为最初模型的输出在很大程度上是垃圾，不值得人类评估。中间模型的输出之间存在大量的相关性，人类评估员将花费大量的时间评估非常相似甚至相同的句子。
- en: For this reason, it’d be desirable if we could use some automatic way to assess
    translation quality. The way this works is similar to some automatic metrics for
    other NLP tasks that we saw earlier, such as accuracy, precision, recall, and
    F1-measure for classification. The idea is to create the desirable output for
    each input instance in advance and compare a system’s output against it. This
    is usually done by preparing a set of human-created translations called *reference*
    for each source sentence and calculating some sort of similarity between the reference
    and a system’s output. Once you create the reference and define the metric, you
    can automatically evaluate translation quality as many times as you want.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够使用某种自动方式来评估翻译质量，将是可取的。这种工作方式类似于我们之前看到的其他 NLP 任务的一些自动度量，例如分类的准确度、精确度、召回率和
    F1 度量。其思想是提前为每个输入实例创建期望的输出，并将系统的输出与之进行比较。通常，这是通过为每个源句准备一组人为创建的翻译（称为*参考*）并计算参考和系统输出之间的某种相似度来完成的。一旦你创建了参考并定义了指标，你就可以根据需要自动评估翻译质量多少次。
- en: One of the simplest ways to compute the similarity between the reference and
    a system output is to use the *word error rate* (WER). WER reflects how many errors
    the system made compared to the reference, measured by the relative number of
    insertions, deletions, and substitutions. The concept is similar to the edit distance,
    except that WER is counted for words, not characters. For example, when the reference
    sentence is “Mary did not slap the green witch” and a system translation is “Mary
    did hit the green wicked witch,” you need three “edits” to match the latter to
    the former—insert “not,” replace “hit” with “slap,” and delete “wicked.” If you
    divide three by the length of the reference (= 7), it’s your WER (= 3/7, or 0.43).
    The lower the WER, the better the quality of your translation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计算参考和系统输出之间相似度的最简单方法之一是使用*单词错误率*（WER）。WER 反映系统相对于参考的错误数量，通过插入、删除和替换的相对数量来衡量。该概念类似于编辑距离，不同之处在于WER
    是针对单词而不是字符计数的。例如，当参考句子是“玛丽没有打绿色的女巫”，系统翻译为“玛丽打了绿色的邪恶女巫”时，你需要进行三次“编辑”才能将后者与前者匹配——插入“没有”，用“打”替换“击中”，删除“邪恶”。如果你将三除以参考长度（=
    7），就是你的WER（= 3/7，或 0.43）。WER 越低，翻译质量越好。
- en: Although WER is simple and easy to compute, it is not widely used for evaluating
    MT systems nowadays. One reason is related to multiple references. There may be
    multiple, equally valid translations for a single source sentence, but it is not
    clear how to apply WER when there are multiple references. A slightly more advanced
    and by far the most commonly used metric for automatic evaluation in MT is BLEU
    (bilingual evaluation understudy). BLEU solves the problem of multiple references
    by using *modified precision*. I’ll illustrate this next using a simple example.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管WER 简单易用，但在评估机器翻译系统时如今并不常用。一个原因与多个参考有关。对于单个源句可能有多个同样有效的翻译，但是当存在多个参考时如何应用WER
    并不清楚。对于机器翻译中自动评估最常用的稍微复杂一点的指标是 BLEU（双语评估学习）。BLEU 通过使用*修改后的精确度*来解决多个参考的问题。接下来我将用一个简单的例子来说明这一点。
- en: 'In the following table, we are evaluating a candidate (a system’s output) “the
    the the the the the the” (which is, by the way, a terrible translation) against
    two references: “the cat is on the mat” and “there is a cat on the mat.” The basic
    idea of BLEU is to calculate the precision of all unique words in the candidate.
    Because there’s only one unique word in the candidate, “the,” if you calculate
    its precision, it will automatically become the candidate’s score, which is 1,
    or 100%. But there seems to be something wrong about this.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中，我们评估一个候选人（系统的输出）“the the the the the the the”（顺便说一句，这是一个糟糕的翻译）与两个参考：“猫在地毯上”和“地毯上有只猫”。BLEU
    的基本思想是计算候选中所有唯一单词的精度。因为候选中只有一个唯一单词“the”，如果计算其精度，它将自动成为候选的分数，即为 1，或 100%。但这似乎有些不对。
- en: '| Candidate | the | the | the | the | the | the | the |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 候选 | the | the | the | the | the | the | the |'
- en: '| Reference 1 | the | cat | is | on | the | mat |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 参考 1 | 猫 | 在 | 地毯 | 上 |  |'
- en: '| Reference 2 | there | is | a | cat | on | the | mat |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 参考 2 | 那里 | 有 | 一只 | 猫 | 在 | 地毯 | 上 |'
- en: Because only two “thes” exist in the references, the spurious “thes” generated
    by the system shouldn’t count toward the precision. In other words, we should
    treat them as false positives. We can do this by capping the denominator of precision
    by the maximum number of occurrences of that word in any of the references. Because
    it’s 2 in this case (in reference 1), its modified precision will be 2/7, or about
    29%. In practice, BLEU uses not only unique words (i.e., unigrams) but also all
    unique sequences of words (n-grams) up to a length of 4 in the candidate and the
    references.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因为参考译文中只有两个“the”，系统生成的虚假“the”不应该计入精度。换句话说，我们应该将它们视为误报。我们可以通过将精度的分母限制为参考译文中该词的最大出现次数来做到这一点。因为在这种情况下（在参考译文
    1 中）是 2，其修改后的精度将为 2/7，约为 29%。在实践中，BLEU 不仅使用唯一的词（即一元词），还使用候选译文和参考译文中长度不超过 4 的所有唯一单词序列（n
    元词）。
- en: However, we can game this metric in another way—because it’s based on precision,
    not on recall, an MT system can easily obtain high scores by producing very few
    words that the system is confident about. In the previous example, you can simply
    produce “cat” (or even more simply, “the”), and the BLEU score will be 100%, which
    is obviously not a good translation. BLEU solves this issue by introducing the
    brevity penalty, which discounts the score if the candidate is shorter than the
    references.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过另一种方式操纵这个指标——因为它基于精度而不是召回率，一个机器翻译系统可以通过产生很少的系统确信的词语来轻松获得高分。在前面的例子中，你只需简单地产生“cat”（甚至更简单地，“the”），BLEU
    分数将达到100%，这显然不是一个好的翻译。BLEU 通过引入简洁惩罚来解决这个问题，如果候选翻译比参考译文短，就会打折扣。
- en: Development of accurate automatic metrics has been an active research area.
    Many new metrics are proposed and used to address the shortcomings of BLEU. We
    barely scratched the surface in this section. Although new metrics show higher
    correlations with human evaluations and are claimed to be better, BLEU is still
    by far the most widely used metric, mainly due to its simplicity and long tradition.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 精确自动评估指标的开发是一个活跃的研究领域。许多新的指标被提出并用于解决 BLEU 的缺点。我们在这一部分只是浅尝辄止。虽然新的指标显示与人类评估的相关性更高，并声称更好，但
    BLEU 仍然是目前最广泛使用的指标，主要是因为其简单性和悠久的传统。
- en: '6.6 Case study: Building a chatbot'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 案例研究：构建聊天机器人
- en: In this section, I’m going to go over another application of a Seq2Seq model—a
    chatbot, which is an NLP application with which you can have a conversation. We
    are going to build a very simple yet functional chatbot using a Seq2Seq model
    and discuss techniques and challenges in building intelligent agents.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将讨论 Seq2Seq 模型的另一个应用——聊天机器人，这是一种 NLP 应用，你可以与之进行对话。我们将使用 Seq2Seq 模型构建一个非常简单但功能齐全的聊天机器人，并讨论构建智能代理的技术和挑战。
- en: 6.6.1 Introducing dialogue systems
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 引入对话系统
- en: 'I briefly touched upon dialogue systems in section 1.2.1\. To recap, two main
    types of dialogue systems exist: task-oriented and chatbots. Although task-oriented
    dialogue systems are used to achieve some specific goals, such as making a reservation
    at a restaurant and obtaining some information, chatbots are used to have conversations
    with humans. Conversational technologies are currently a hot topic among NLP practitioners,
    due to the success and proliferation of commercial conversational AI systems such
    as Amazon Alexa, Apple Siri, and Google Assistant.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第 1.2.1 节简要介绍了对话系统。简而言之，主要有两种类型的对话系统：面向任务和聊天机器人。尽管面向任务的对话系统用于实现一些特定目标，例如在餐厅预订和获取一些信息，但聊天机器人用于与人类进行对话。由于商业对话人工智能系统如亚马逊
    Alexa、苹果 Siri 和谷歌助手的成功和大量普及，对话技术目前是自然语言处理从业者的热门话题。
- en: 'You may not have a clue as to how we can get started with building an NLP application
    that can have conversations. How can we build something “intelligent” that “thinks”
    so that it can generate meaningful responses to human input? This seems farfetched
    and difficult. But if you step back and look at a typical conversation we have
    with other people, how much of it is actually “intelligent?” If you are like most
    of us, a large fraction of the conversation you are having is autopilot: “How
    are you?” “I’m doing good, thanks” “Have a good day” “You, too!” and so on. You
    may also have a set of “template” responses to a lot of everyday questions such
    as “What do you do?” and “Where are you from?” These questions can be answered
    just by looking at the input. Even more complex questions like “What’s your favorite
    restaurant in X?” (where X is the name of a neighborhood in your city) and “Did
    you see any Y movies lately?” (where Y is a genre) can be answered just by “pattern
    matching” and retrieving relevant information from your memory.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能不知道如何开始构建可以进行会话交流的自然语言处理应用程序。我们该如何构建一个“智能”的东西来“思考”，以便它能为人类输入生成有意义的响应？这似乎有些遥远和困难。但是，如果您退后一步，看看我们与其他人的典型对话，有多少实际上是“智能”的呢？如果您像我们大多数人一样，那么您正在进行的会话中有很大一部分都是自动驾驶的：“你好吗？”“我没事，谢谢”“祝你有个愉快的一天”“你也是！”等等。您可能还有一组“模板”回复，对于许多日常问题，例如“你在干什么？”和“你来自哪里？”这些问题可以通过查看输入来回答。甚至更复杂的问题，如“X中你最喜欢的餐厅是什么？”（其中X是您城市的一个地区的名称）和“你最近看过任何Y类型的电影吗？”（其中Y是一种类型），都可以通过“模式匹配”并从记忆中检索相关信息来回答。
- en: If you think of a conversation as a set of “turns” where the response is generated
    by pattern matching against the previous utterance, this starts to look a lot
    like a typical NLP problem. In particular, if you regard dialogues as a problem
    where an NLP system is simply converting your question to its response, this is
    exactly where we can apply the Seq2Seq models we covered in this chapter so far.
    We can treat the previous (human’s) utterance as a foreign sentence and have the
    chatbot “translate” it into another language. Even though these two languages
    are both English in this case, it is a common practice in NLP to treat the input
    and the output as two different languages and apply a Seq2Seq model to them, including
    summarization (longer text to a shorter one) and grammatical error correction
    (text with errors to one without).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将对话视为一组“回合”，其中响应是通过模式匹配来生成的，那么这看起来与典型的自然语言处理问题非常相似。特别是，如果您认为对话是一个问题，其中NLP系统只是将您的问题“翻译”为其响应，那么这正是我们可以使用本章迄今涵盖的Seq2Seq模型的地方。我们可以将之前（人类的）话语视为外国语句子，并将聊天机器人“翻译”成另一种语言。尽管在这种情况下，这两种语言都是英语，但是在NLP中，通常将输入和输出视为两种不同的语言，并将Seq2Seq模型应用于它们，包括摘要（将更长的文本缩短）和语法纠错（将有错误的文本纠正为无错误的文本）。
- en: 6.6.2 Preparing a dataset
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 准备数据集
- en: In this case study, we are going to use The Self-dialogue Corpus ([https://github.com/jfainberg/self_dialogue_corpus](https://github.com/jfainberg/self_dialogue_corpus)),
    a collection of 24,165 conversations. What’s special about this dataset is that
    these conversations are not actual ones between two people, but fictitious ones
    written by one person who plays both sides. You could use several conversation
    datasets for text-based chatbots (e.g., the OpenSubtitles dataset, [http://opus.nlpl.eu/OpenSubtitles-v2018.php](http://opus.nlpl.eu/OpenSubtitles-v2018.php)),
    but these datasets are often noisy and often contain obscenities. By collecting
    made-up conversations instead, the Self-dialogue Corpus improves the quality for
    half the original cost (because you need only one person versus two people!).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例中，我们将使用自我对话语料库（[https://github.com/jfainberg/self_dialogue_corpus](https://github.com/jfainberg/self_dialogue_corpus)），其中包含24,165个对话。这个数据集的特殊之处在于，这些对话并不是两个人之间的实际对话，而是由一人扮演双方所写的虚构对话。虽然还有其他几个基于文本的聊天机器人的对话数据集（例如OpenSubtitles数据集，[http://opus.nlpl.eu/OpenSubtitles-v2018.php](http://opus.nlpl.eu/OpenSubtitles-v2018.php)），但这些数据集通常存在噪声并且常常包含粗言秽语。相比之下，通过收集编造的对话，自我对话语料库在仅有一人的情况下提高了一半的质量（因为你只需要一个人而不是两个人！）。
- en: 'The same as earlier, I tokenized and converted the corpus into a format that
    is interpretable by Fairseq. You can obtain the converted dataset as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前相同，我对语料进行了分词和转换，使其可被Fairseq解读。您可以按以下方式获取转换后的数据集：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can use the following combination of the paste command (to stitch files
    horizontally) and the head command to peek at the beginning of the training portion.
    Note that we are using fr (for “foreign,” not “French”) to denote the “language”
    we are translating from:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下 paste 命令的组合（以水平方式拼接文件）和 head 命令来查看训练部分的开头。请注意，我们使用 fr（表示“外语”，而不是“法语”）来表示我们正在从中翻译的“语言”：
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see, each line consists of an utterance (on the left) and a response
    to it (on the right). Notice that this dataset has the same structure as the Spanish-English
    parallel corpus we used in section 6.3.1\. The next step is to run the fairseq-preprocess
    command to convert it to a binary format as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每一行都包含一个话语（在左侧）和对其的回应（在右侧）。请注意，此数据集与我们在第 6.3.1 节中使用的西班牙语-英语平行语料库具有相同的结构。下一步是运行
    fairseq-preprocess 命令将其转换为二进制格式，如下所示：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Again, this is similar to what we ran for the Spanish translator example. Just
    pay attention to what you specify as the source language—we are using fr instead
    of es here.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这与我们为西班牙语翻译器示例运行的内容类似。只需注意您指定的源语言—我们在这里使用的是 fr 而不是 es。
- en: 6.6.3 Training and running a chatbot
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 训练和运行聊天机器人
- en: 'Now that the training data for the chatbot is ready, let’s train a Seq2Seq
    model from this data. You can invoke the fairseq-train command with almost identical
    parameters to the last time, as shown next:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，聊天机器人的训练数据准备好了，让我们从这些数据中训练一个 Seq2Seq 模型。您可以像下面这样调用 fairseq-train 命令，几乎使用与上次相同的参数：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As previously, pay attention to how the validation loss changes every epoch.
    When I tried this, the validation loss decreased for about five epochs but then
    started to slowly creep back up. Feel free to stop the training command by pressing
    Ctrl + C after you observe the validation loss leveling out. Fairseq will automatically
    save the best model (measured by the validation loss) to checkpoint_best.pt.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前一样，请注意验证损失随每个时代的变化。当我尝试时，验证损失在约五个时代内减小，然后开始缓慢上升。在观察到验证损失趋于稳定后，随时按 Ctrl +
    C 停止训练命令。Fairseq 将自动将最佳模型（按验证损失测量）保存为 checkpoint_best.pt。
- en: 'Finally, you can run the chatbot model by invoking the fairseq-interactive
    command, as shown here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以通过调用 fairseq-interactive 命令来运行聊天机器人模型，如下所示：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As with the previous example, you can type your source sentences and have a
    conversion with your chatbot by having them “translate” to another language! Here’s
    part of a conversation that I had with the model that I trained (I added boldface
    for clarity). Again, your results might be different:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例一样，您可以输入源句并与聊天机器人对话，让它将它们“翻译”为另一种语言！以下是我与我训练的模型进行的一部分对话（为了清晰起见，我添加了粗体）。再次强调，您的结果可能不同：
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this example, the conversation looks natural. Because the Self-dialogue Corpus
    is built by restricting the set of possible conversation topics, the conversation
    is more likely to go smoothly if you stay on such topics (movie, sports, music,
    and so on).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，对话看起来很自然。因为自我对话语料库是通过限制可能的对话主题集来构建的，所以如果您保持在这些主题（电影、体育、音乐等）上，对话更有可能顺利进行。
- en: 'However, as soon as you start talking about unfamiliar topics, the chatbot
    loses its confidence in its answers, as shown next:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦您开始谈论不熟悉的话题，聊天机器人就会对其答案失去信心，如下所示：
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is a well-known phenomenon—a simple Seq2Seq-based chatbot quickly regresses
    to producing cookie-cutter answers such as “I don’t know” and “I’m not sure” whenever
    asked about something it’s not familiar with. This has to do with the way we trained
    this chatbot. Because we trained the model so that it minimizes the loss in the
    training data, the best strategy it can take to reduce the loss is to produce
    something applicable to as many input sentences as possible. Very generic phrases
    such as “I don’t know” can be an answer for many questions, so it’s a great way
    to play it safe and reduce the loss!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个众所周知的现象—一个简单的基于 Seq2Seq 的聊天机器人很快就会退化到生成“我不知道”和“我不确定”等模板回答，每当被问及它不熟悉的事物时。这与我们训练这个聊天机器人的方式有关。因为我们训练模型是为了使其在训练数据中最小化损失，它能采取的最佳策略是生成适用于尽可能多的输入句子的东西。非常通用的短语，比如“我不知道”，可以成为许多问题的答案，所以这是一个安全的策略，可以减少损失！
- en: 6.6.4 Next steps
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.4 后续步骤
- en: 'Although our chatbot can produce realistic-looking responses for many inputs,
    it’s far from perfect. One issue that it’s not great at dealing with is proper
    nouns. You can see this when you ask questions that solicit specific answers,
    like the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的聊天机器人可以为许多输入产生逼真的响应，但它离完美还有很远的路要走。它处理不好的一个问题是专有名词。当你询问需要具体答案的问题时，就会看到这一点，比如：
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here <unk> is the catch-all special symbol for unknown words. The chatbot is
    trying to answer something, but that something occurs too infrequently in the
    training data to be treated as an independent word. This is an issue seen in simple
    NMT systems in general. Because the models need to cram everything about a word
    in a 200-something-dimensional vector of numbers, many details and distinctions
    between similar words are sacrificed. Imagine compressing all the information
    about all the restaurants in your city into a 200-dimensional vector!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<unk>是未知词的通用特殊符号。聊天机器人正在尝试回答某些问题，但该问题在训练数据中出现的频率太低，以至于不能被视为独立的词。这是一般简单 NMT
    系统中出现的问题。因为模型需要将有关单词的所有信息压缩成 200 多维的数字向量，所以许多细节和类似单词之间的区别都被牺牲了。想像一下将关于你所在城市所有餐厅的信息压缩成一个
    200 维的向量！
- en: 'Also, the chatbot we trained doesn’t have any “memory” or any notion of context
    whatsoever. You can test this by asking a series of related questions as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们训练的聊天机器人没有任何 "记忆" 或任何上下文概念。你可以通过问一系列相关问题来测试这一点，如下所示：
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the second question, the chatbot is having difficulties understanding the
    context and produces a completely irrelevant response. To answer such questions
    correctly, the model needs to understand that the pronoun “it” refers to a previous
    noun, namely, “Mexican food” in this case. The task where NLP systems resolve
    which mentions refer to which entities in the real world is called *coreference
    resolution*. The system also needs to maintain some type of memory to keep track
    of what was discussed so far in the dialogue.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个问题中，聊天机器人在理解上下文方面有困难，并产生了完全无关的响应。要正确回答这样的问题，模型需要理解代词 "it" 指的是前面的名词，即本例中的
    "Mexican food"。NLP 系统在现实世界中解决哪些提及指向哪些实体的任务被称为*共指消解*。系统还需要维护某种类型的记忆，以跟踪到目前为止在对话中讨论了哪些内容。
- en: Finally, the simple Seq2Seq models we discussed in this chapter are not great
    at dealing with long sentences. If you look back at figure 6.2, you’ll understand
    why—the model reads the input sentence using an RNN and represents everything
    about the sentence using a fixed-length sentence representation vector and then
    generates the target sentence from that vector. It doesn’t matter whether the
    input is “Hi!” or “The quick brown fox jumped over the lazy dog.” The sentence
    representation becomes a bottleneck, especially for longer input. Because of this,
    neural MT models couldn’t beat traditional phrase-based statistical MT models
    until around 2015, when a mechanism called *attention* was invented to tackle
    this very problem. We’ll discuss attention in detail in chapter 8.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本章中讨论的简单 Seq2Seq 模型在处理长句子方面并不擅长。如果您回顾一下图6.2，就会理解这一点 - 模型使用 RNN 读取输入语句，并使用固定长度的句子表示向量表示有关句子的所有内容，然后从该向量生成目标语句。无论输入是“Hi！”还是“The
    quick brown fox jumped over the lazy dog.”，句子表示都会成为瓶颈，特别是对于更长的输入。因此，在2015年左右，直到发明了一种称为
    *注意力* 的机制来解决这个问题之前，神经 MT 模型无法击败传统的基于短语的统计 MT 模型。我们将在第8章详细讨论注意力。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概括
- en: Sequence-to-sequence (Seq2Seq) models transform one sequence into another using
    an encoder and a decoder.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq 模型使用编码器和解码器将一个序列转换为另一个序列。
- en: You can use the fairseq framework to build a working MT system within an hour.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 fairseq 框架在一小时内构建工作中的翻译系统。
- en: A Seq2Seq model uses a decoding algorithm to generate the target sequence. Greedy
    decoding maximizes the probability at each step, whereas beam search tries to
    find better solutions by considering multiple hypotheses at once.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq 模型使用解码算法生成目标序列。贪心解码每一步都最大化概率，而束搜索则尝试同时考虑多个假设来寻找更好的解决方案。
- en: A metric called BLEU is commonly used for automatically evaluating MT systems.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自动评估翻译系统的一个指标叫做 BLEU。
- en: A simple chatbot can be built by using a Seq2Seq model and a conversation dataset.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 Seq2Seq 模型和对话数据集，可以构建一个简单的聊天机器人。
- en: ^(1.)See Oriol Vinyals et al., “Grammar as a Foreign Language,” (2015; [https://arxiv.org/abs/1412.7449](https://arxiv.org/abs/1412.7449))
    for more details.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.) 详细信息请参阅 Oriol Vinyals 等人的“Grammar as a Foreign Language”（2015 年；[https://arxiv.org/abs/1412.7449](https://arxiv.org/abs/1412.7449)）。
- en: ^(2.) Note that $ at the beginning of every line is rendered by the shell, and
    you don’t need to type it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.) 请注意每行开头的 $ 是由 shell 渲染的，您无需输入它。
