- en: Chapter 7\. Introduction to Diffusion Models for Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章\. 图像生成扩散模型的介绍
- en: This chapter introduces the most popular diffusion models for AI image generation.
    You’ll learn the benefits and limitations of each of the top models, so that you
    can be confident in choosing between them based on the task at hand.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了最流行的 AI 图像生成扩散模型。您将了解每个顶级模型的优缺点，以便您可以根据手头的任务自信地选择它们。
- en: Introduced in 2015, *diffusion models* are a class of generative models that
    have shown spectacular results for generating images from text. The release of
    [DALL-E 2](https://oreil.ly/dalle2) in 2022 marked a great leap forward in the
    quality of generated images from diffusion models, with open source [Stable Diffusion](https://oreil.ly/gjNJ_),
    and community favorite [Midjourney](https://oreil.ly/j51L0) quickly following
    to forge a competitive category. With the integration of [DALL-E 3](https://oreil.ly/dalle3)
    into ChatGPT, the lines will continue to blur between text and image generation.
    However, advanced users will likely continue to require direct access to the underlying
    image generation model, to get the best results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年引入的 *扩散模型* 是一类生成模型，它在从文本生成图像方面取得了显著成果。2022 年 [DALL-E 2](https://oreil.ly/dalle2)
    的发布标志着扩散模型生成图像质量的重大飞跃，开源 [Stable Diffusion](https://oreil.ly/gjNJ_) 和社区喜爱的 [Midjourney](https://oreil.ly/j51L0)
    迅速跟进，形成了一个具有竞争力的类别。随着 [DALL-E 3](https://oreil.ly/dalle3) 集成到 ChatGPT 中，文本和图像生成之间的界限将继续模糊。然而，高级用户可能仍需要直接访问底层图像生成模型，以获得最佳结果。
- en: Diffusion models are trained by many steps of [adding random noise](https://oreil.ly/OrAHA)
    to an image and then predicting how to reverse the diffusion process by *denoising*
    (removing noise). The approach comes from physics, where it has been used for
    simulating how particles *diffuse* (spread out) through a medium. The predictions
    are conditioned on the description of the image, so if the resulting image doesn’t
    match, the neural network weights of the model are adjusted to make it better
    at predicting the image from the description. When trained, the model is able
    to take random noise and turn it into an image that matches the description provided
    in the prompt.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过向图像添加随机噪声的多个步骤进行训练，然后通过 *去噪*（去除噪声）预测如何逆转扩散过程。这种方法来自物理学，在那里它已被用于模拟粒子如何通过介质
    *扩散*（扩散）。预测取决于图像的描述，因此如果生成的图像不匹配，模型的神经网络权重将进行调整，以使其更好地从描述中预测图像。当模型训练完成后，它能够将随机噪声转换为与提示中提供的描述相匹配的图像。
- en: '[Figure 7-1](#figure-7-1) illustrates the denoising process, as demonstrated
    by Binxu Wang in [“Mathematical Foundation of Diffusion Generative Models”](https://oreil.ly/57szp).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-1](#figure-7-1) 展示了去噪过程，如王斌旭在[“扩散生成模型的数学基础”](https://oreil.ly/57szp)中演示的那样。'
- en: '![pega 0701](assets/pega_0701.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0701](assets/pega_0701.png)'
- en: Figure 7-1\. Diffusion schematics
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 扩散示意图
- en: These models were trained on large datasets of billions of images scraped from
    the internet (and accompanying captions) and can therefore replicate most popular
    art styles or artists. This has been the source of much controversy, as copyright
    holders seek to [enforce their legal claims](https://oreil.ly/a4Fyp), while model
    creators argue in favor of fair use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型是在从互联网上抓取的数十亿张图片（及其相关标题）的大数据集上训练的，因此可以复制大多数流行的艺术风格或艺术家。这引发了诸多争议，因为版权所有者寻求[执行他们的法律主张](https://oreil.ly/a4Fyp)，而模型创造者则支持合理使用。
- en: 'A diffusion model is not simply a “complex collage tool” that regurgitates
    replicas of copyrighted images: it’s only a few gigabytes in size and therefore
    can’t possibly contain copies of all its training data. When researchers attempted
    to reproduce 350,000 images from Stable Diffusion’s training data, they only succeeded
    with 109 of them ([Carlini et al.](https://oreil.ly/SGn9B), 2023).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型并非仅仅是“复杂的拼贴工具”，它只是几个吉字节大小，因此不可能包含所有训练数据的副本。当研究人员试图从 Stable Diffusion 的训练数据中复制
    350,000 张图片时，他们只成功复制了其中的 109 张([Carlini 等人](https://oreil.ly/SGn9B)，2023)。
- en: 'What the model is doing is more analogous to a human artist looking at every
    image on the internet and learning the patterns that define every subject and
    style. These patterns are encoded as a *vector representation* (a list of numbers)
    referring to a location in *latent space*: a map of all possible combinations
    of images that could be generated by the model. The prompt input by the user is
    first encoded into vectors; the diffusion model then generates an image matching
    these vectors, before the resulting image is decoded back into pixels for the
    user.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模型所做的是更类似于一个人类艺术家查看互联网上的每一张图片，并学习定义每个主题和风格的模式。这些模式被编码为*向量表示*（一系列数字），指代潜在空间中的一个位置：模型可以生成所有可能图像组合的映射。用户输入的提示首先被编码成向量；然后扩散模型生成与这些向量匹配的图像，最后将生成的图像解码回像素供用户查看。
- en: '[Figure 7-2](#figure-7-2) illustrates the encoding and decoding process, from
    Ian Stenbit’s [“A Walk Through Latent Space with Stable Diffusion”](https://oreil.ly/qOpis).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-2](#figure-7-2) 展示了编码和解码过程，来自伊恩·斯坦比特的[“通过稳定扩散漫步潜在空间”](https://oreil.ly/qOpis)。'
- en: '![pega 0702](assets/pega_0702.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0702](assets/pega_0702.png)'
- en: Figure 7-2\. Encoding and decoding process
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 编码和解码过程
- en: These vectors, also referred to as *embeddings*, act as a location or address
    for a point in the model’s map of every image, and as such images that are similar
    will be closer together in latent space. The latent space is continuous, and you
    can travel between two points (interpolate) and still get valid images along the
    way. For example, if you interpolate from a picture of a dog to a bowl of fruit,
    the intermediate images will be coherent-looking images, demonstrating a progressive
    shift between the two concepts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量，也被称为*嵌入*，在模型对每个图像的映射中充当一个位置或地址，因此相似的图像在潜在空间中会彼此靠近。潜在空间是连续的，你可以在两个点之间（插值）移动，并且仍然可以得到有效的图像。例如，如果你从一只狗的图片插值到一个水果碗，中间的图像将看起来连贯，展示了两个概念之间的渐进式转变。
- en: '[Figure 7-3](#figure-7-3) contains a grid, also from Ian Stenbit, showing the
    [intermediate steps between four images](https://oreil.ly/cjm8A): a dog (top left),
    a bowl of fruit (top right), the Eiffel Tower (bottom left), and a skyscraper
    (bottom right).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-3](#figure-7-3) 包含一个网格，也来自伊恩·斯坦比特，显示了[四张图像之间的中间步骤](https://oreil.ly/cjm8A)：一只狗（左上角），一个水果碗（右上角），埃菲尔铁塔（左下角），和一座摩天大楼（右下角）。'
- en: '![pega 0703](assets/pega_0703.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0703](assets/pega_0703.png)'
- en: Figure 7-3\. A random walk through latent space
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 潜在空间中的随机游走
- en: Within the domain of diffusion models, prompt engineering can be seen as navigating
    the latent space, searching for an image that matches your vision, out of all
    of the possible images available. There are many techniques and best practices
    for locating the right combination of words to conjure up your desired image,
    and an active community of AI artists and researchers have worked to build a set
    of tools to help. Each model and method has its own quirks and behaviors depending
    on its architecture, training method, and the data on which it was trained. The
    three main organizations responsible for building the most popular text-to-image
    diffusion models have all taken radically different approaches in terms of business
    models and functionality, and as such there is a greater diversity of choice in
    diffusion models than there is in the OpenAI-dominated LLM space.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散模型的领域内，提示工程可以看作是在潜在空间中导航，在所有可能的图像中寻找与你的愿景相匹配的图像。有许多技术和最佳实践用于定位正确的单词组合来召唤你想要的图像，一个活跃的AI艺术家和研究人员社区已经努力构建了一套工具来帮助。每个模型和方法都有其自身的特点和行为，这取决于其架构、训练方法和训练数据。负责构建最受欢迎的文本到图像扩散模型的三家主要组织在商业模式和功能方面都采取了截然不同的方法，因此扩散模型的选择比OpenAI主导的LLM空间更加多样化。
- en: OpenAI DALL-E
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI DALL-E
- en: In January 2021, OpenAI released the text-to-image model DALL-E, its name being
    a play on surrealist artist Salvador Dali and the Pixar animated robot WALL-E.
    The model was based on a modified version of OpenAI’s remarkable GPT-3 text model,
    which had been released seven months before. DALL-E was a breakthrough in generative
    AI, demonstrating artistic abilities most people thought were impossible for a
    computer to possess. [Figure 7-4](#figure-7-4) shows an example of the [first
    version](https://oreil.ly/dalle1) of DALL-E’s capabilities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2021 年 1 月，OpenAI 发布了文本到图像模型 DALL-E，其名称是对超现实主义艺术家萨尔瓦多·达利和皮克斯动画机器人 WALL-E 的双关语。该模型基于
    OpenAI 的杰出 GPT-3 文本模型的修改版，该模型在七个月前发布。DALL-E 是生成式 AI 的一次突破，展示了人们认为计算机不可能拥有的艺术能力。[图
    7-4](#figure-7-4) 展示了 DALL-E 功能的[第一版](https://oreil.ly/dalle1)。
- en: '![pega 0704](assets/pega_0704.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0704](assets/pega_0704.png)'
- en: Figure 7-4\. DALL-E capabilities
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. DALL-E 功能
- en: The DALL-E model was not open sourced nor released to the public, but it inspired
    multiple researchers and hobbyists to attempt to replicate the research. The most
    popular of these models was DALL-E Mini, released in July 2021 (renamed Craiyon
    a year later at the request of OpenAI), and although it gained a cult following
    on social media, the quality was considerably poorer than the official DALL-E
    model. OpenAI published a [paper announcing DALL-E 2](https://oreil.ly/EqdtP)
    in April 2022, and the quality was significantly higher, attracting a waitlist
    of one million people. [Figure 7-5](#figure-7-5) shows an example of the now iconic
    astronaut riding a horse image from the paper that captured the public’s imagination.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 模型并未开源，也未向公众发布，但它激发了多位研究人员和爱好者的兴趣，尝试复制这项研究。其中最受欢迎的模型是 DALL-E Mini，于 2021
    年 7 月发布（一年后在 OpenAI 的要求下更名为 Craiyon），尽管它在社交媒体上获得了狂热追随者，但其质量远不如官方的 DALL-E 模型。OpenAI
    于 2022 年 4 月发布了一篇关于 DALL-E 2 的[论文](https://oreil.ly/EqdtP)，其质量显著提高，吸引了 100 万人的等待名单。[图
    7-5](#figure-7-5) 展示了论文中现在标志性的宇航员骑马图像，该图像激发了公众的想象力。
- en: '![pega 0705](assets/pega_0705.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0705](assets/pega_0705.png)'
- en: Figure 7-5\. DALL-E 2 image quality
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. DALL-E 2 图像质量
- en: Access was limited to waitlist users until September 2022, due to concerns about
    AI ethics and safety. Generation of images containing people was initially banned,
    as were a long list of sensitive words. Researchers identified DALL-E 2 [adding
    the words *black* or *female*](https://oreil.ly/ot4vw) to some image prompts like
    a photo of a doctor in a hamfisted attempt to address bias inherited from the
    dataset (images of doctors on the internet are disproportionally of white males).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对 AI 伦理和安全性的担忧，直到 2022 年 9 月，访问权限仅限于等待名单用户。最初禁止生成包含人物的图像，以及一系列敏感词汇。研究人员发现
    DALL-E 2 在一些图像提示（如一位医生的照片）中添加了“*黑人*”或“*女性*”等词汇，这是一种笨拙的尝试，旨在解决从数据集中继承的偏见（互联网上的医生照片中，白人男性的比例不正常）。
- en: The team added inpainting and outpainting to the user interface in August 2022,
    which was a further leap forward, garnering attention in the press and on social
    media. These features allowed users to generate only selected parts of an image
    or to *zoom out* by generating around the border of an existing image. However,
    users have little control over the parameters of the model and could not fine-tune
    it on their own data. The model would generate garbled text on some images and
    struggled with realistic depictions of people, generating disfigured or deformed
    hands, feet, and eyes, as demonstrated in [Figure 7-6](#figure-7-6).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 团队在 2022 年 8 月将修复和扩展功能添加到用户界面中，这又是一个重大进步，引起了媒体和社交媒体的关注。这些功能允许用户仅生成图像的选定部分，或者通过生成现有图像边缘的图像来*缩小视图*。然而，用户对模型的参数控制很少，无法在自己的数据上微调它。该模型在某些图像上会生成混乱的文本，并且在描绘人物方面存在困难，生成了扭曲或变形的双手、双脚和眼睛，如[图
    7-6](#figure-7-6)所示。
- en: '![pega 0706](assets/pega_0706.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0706](assets/pega_0706.png)'
- en: Figure 7-6\. Deformed hands and eyes
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 变形的双手和眼睛
- en: Google’s Imagen demonstrated impressive results and was introduced in a paper
    in May 2022 ([Ho et al.](https://oreil.ly/sFaeW), 2022), but the model was not
    made available to the general public, citing AI ethics and safety concerns. Competitors
    like Midjourney (July 2022) moved quickly and capitalized on huge demand from
    people who had seen impressive demos of DALL-E on social media but were stuck
    on the waitlist. The open source release of Stable Diffusion (August 2022) broke
    what had seemed to be an unassailable lead for OpenAI just a few months before.
    Although the rollout of the more advanced [DALL-E 3 model](https://oreil.ly/dalle3)
    as a feature of ChatGPT has helped OpenAI regain lost ground, and Google has gotten
    into the game with [Gemini 1.5](https://oreil.ly/XzQrU), there remains everything
    to play for.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Imagen展示了令人印象深刻的结果，并在2022年5月的一篇论文中介绍([Ho等人](https://oreil.ly/sFaeW)，2022)，但该模型并未向公众开放，理由是AI伦理和安全问题。像Midjourney（2022年7月）这样的竞争对手迅速行动，利用了那些在社交媒体上看到DALL-E令人印象深刻的演示但被困在等待名单上的人的巨大需求。Stable
    Diffusion（2022年8月）的开源发布打破了OpenAI几个月前看似不可动摇的领先地位。尽管ChatGPT作为功能推出的更先进的[DALL-E 3模型](https://oreil.ly/dalle3)帮助OpenAI收复失地，谷歌也加入了游戏，推出了[Gemini
    1.5](https://oreil.ly/XzQrU)，但仍有很大的发展空间。
- en: Midjourney
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Midjourney
- en: In July 2022, just three months after the release of DALL-E 2, Midjourney put
    its v3 model in open beta. This was a uniquely good time to launch an image generation
    model, because the demonstrations of what DALL-E 2 could do from early users looked
    like magic, and yet access was initially limited. Eager early-adopters flocked
    to Midjourney, and its notable fantasy aesthetic gained a cult following among
    the gaming and digital art crowds, showcased in the [now famous image](https://oreil.ly/dqshh),
    which won first prize in a digital art competition, in [Figure 7-7](#figure-7-7).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年7月，在DALL-E 2发布后的仅仅三个月，Midjourney就推出了其v3模型的开源测试版。这是一个推出图像生成模型的独特好时机，因为早期用户对DALL-E
    2能做什么的演示看起来像是魔法，但最初访问是有限的。热切的早期采用者纷纷涌入Midjourney，其引人注目的幻想美学在游戏和数字艺术人群中赢得了狂热追随，如图[现在著名的图像](https://oreil.ly/dqshh)所示，该图像在数字艺术比赛中获得了一等奖，如图7-7所示。
- en: '![pega 0707](assets/pega_0707.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0707](assets/pega_0707.png)'
- en: Figure 7-7\. Théâtre d’Opéra Spatial
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7. 空间歌剧院
- en: Midjourney was one of the first viable image models that had a business model
    and commercial license, making it suitable for more than just experimentation.
    The subscription model was favored by many artists accustomed to paying monthly
    for other software like Adobe Photoshop. It also helped the creative process to
    not be charged per image generated, particularly in the early days when you’d
    have to try multiple images before you found one that was high-enough quality.
    If you were a paying customer of Midjourney, you owned the rights to any image
    generated, unlike DALL-E, where OpenAI was retaining the copyright.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney是第一个具有商业模式和商业许可的可行图像模型之一，使其不仅适合实验。订阅模式受到许多习惯于为其他软件如Adobe Photoshop按月付费的艺术家们的青睐。它还帮助创意过程不按生成的图像收费，尤其是在早期，你必须尝试多张图像才能找到一张足够高质量的图像。如果你是Midjourney的付费客户，你拥有任何生成的图像的权利，与DALL-E不同，OpenAI保留着版权。
- en: Unique to Midjourney is its heavy community focus. To use the tool, you must
    sign into a [Discord server](https://oreil.ly/JKZzD) ([Figure 7-8](#figure-7-8))
    and submit your prompt in an open channel or direct message. Given that all image
    generations are shared in open channels by default, and private mode is available
    only on the [most expensive plan](https://oreil.ly/OV46r), the vast majority of
    images created through Midjourney are available for others to learn from. This
    led to rapid copying and iteration between users, making it easy for novices to
    quickly learn from others. As early as July 2022, the Discord community was nearing
    1 million people (shown in [Figure 7-8](#figure-7-8)), and a year later, there
    were more than 13 million members.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney的独特之处在于其强烈的社区关注。要使用这个工具，你必须登录到一个[Discord服务器](https://oreil.ly/JKZzD)
    ([图7-8](#figure-7-8)) 并在一个公开频道或直接消息中提交你的提示。鉴于所有图像生成默认都在公开频道共享，并且私人模式仅在[最昂贵的计划](https://oreil.ly/OV46r)上可用，通过Midjourney创建的大多数图像都可供他人学习。这导致了用户之间的快速复制和迭代，使得新手能够快速从他人那里学习。早在2022年7月，Discord社区的人数就接近100万（如图7-8所示），一年后，会员人数超过1300万。
- en: '![pega 0708](assets/pega_0708.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0708](assets/pega_0708.png)'
- en: Figure 7-8\. Midjourney’s Discord server, July 2022
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8. Midjourney的Discord服务器，2022年7月
- en: 'When you find an image you like, you can click a button to *upscale* the image
    (make it higher resolution) for use. Many have speculated that this procedure
    acts as training data for reinforcement learning, similar to [reinforcement learning
    from human feedback](https://oreil.ly/3ISZk) (RLHF), the method touted as the
    key to success of ChatGPT. In addition, the team regularly asks for ratings of
    images generated by newer models in order to improve the performance. Midjourney
    released v4 of its model in November 2022, followed by v5 in March 2023 and v6
    in December 2023\. The quality is significantly improved: hands and eyes issues
    identified in [Figure 7-6](#figure-7-6) have largely gone away, and the model
    has a larger stylistic range, demonstrated in [Figure 7-9](#figure-7-9).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你找到喜欢的图片时，你可以点击一个按钮来*提升*图片的分辨率（使其更高）以供使用。许多人推测这个过程充当了强化学习的训练数据，类似于[从人类反馈中进行强化学习](https://oreil.ly/3ISZk)（RLHF），这种方法被誉为ChatGPT成功的关键。此外，团队定期要求对由较新模型生成的图片进行评分，以提高性能。Midjourney于2022年11月发布了其模型的v4版本，随后在2023年3月发布了v5版本，在2023年12月发布了v6版本。质量显著提高：在[图7-6](#figure-7-6)中识别出的手和眼睛问题在很大程度上已经消失，模型具有更广泛的艺术风格范围，这在[图7-9](#figure-7-9)中得到了展示。
- en: 'Input:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 7-9](#figure-7-9) shows the output.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#figure-7-9)展示了输出。'
- en: '![pega 0709](assets/pega_0709.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0709](assets/pega_0709.png)'
- en: Figure 7-9\. Women eating salads and laughing
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。妇女吃沙拉和笑
- en: Remarkably, the Midjourney team has remained small, with just [11 employees](https://oreil.ly/YrmA_)
    as of March 2023\. The founder of Midjourney, David Holz, formerly of hardware
    startup Leap Motion, [confirmed in an interview](https://oreil.ly/jeFYV) that
    the company was already profitable as of August 2022\. What is even more remarkable
    is that without the billions of dollars of funding that OpenAI enjoys, the team
    has built significant functionality over what’s available in DALL-E, including
    negative prompting (removing concepts from an image), weighted terms (increasing
    the prevalance of other concepts), and their *describe* feature (reverse engineering
    the prompt from an uploaded image). However, there is no API available; the only
    way to access the model is through Discord, which has likely acted as a drag on
    mainstream adoption.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Midjourney团队一直保持规模较小，截至2023年3月仅有[11名员工](https://oreil.ly/YrmA_)。Midjourney的创始人David
    Holz，此前是硬件创业公司Leap Motion的员工，[在一次采访中确认](https://oreil.ly/jeFYV)公司自2022年8月起就已经盈利。更令人印象深刻的是，在没有OpenAI所享受的数十亿美元资金的情况下，该团队已经构建了比DALL-E更多的功能，包括负面提示（从图片中移除概念）、加权术语（增加其他概念的普遍性）以及他们的*描述*功能（从上传的图片中逆向工程提示）。然而，没有API可用；访问该模型唯一的方式是通过Discord，这可能会对主流采用产生拖累。
- en: Stable Diffusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusion
- en: While DALL-E 2’s waitlist continued to build, researchers from the CompVis Group
    at LMU Munich and applied research company Runway ML received a donation of computing
    power from Stability AI to train Stable Diffusion. The model shocked the generative
    AI world when it was released open source in August 2022, because the results
    were comparable to DALL-E 2 and Midjourney, but it could be run for free on your
    own computer (assuming you had a modest GPU with 8GB VRAM). Stable Diffusion had
    one of the [fastest climbs in GitHub stars of any software](https://oreil.ly/pwPGX),
    rising to 33,600 stars in its first 90 days ([Figure 7-10](#figure-7-10)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当DALL-E 2的等待名单继续增长时，慕尼黑LMU的CompVis组和应用研究公司Runway ML从Stability AI那里获得了计算能力的捐赠，用于训练Stable
    Diffusion。当它在2022年8月开源发布时，该模型震惊了生成式人工智能界，因为其结果与DALL-E 2和Midjourney相当，但可以在你自己的电脑上免费运行（假设你有一个配备8GB
    VRAM的适度GPU）。Stable Diffusion在GitHub上任何软件的[GitHub星标增长速度中排名最快](https://oreil.ly/pwPGX)，在最初的90天内达到了33,600个星标（[图7-10](#figure-7-10)）。
- en: '![pega 0710](assets/pega_0710.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0710](assets/pega_0710.png)'
- en: Figure 7-10\. GitHub developer adoption of Stable Diffusion
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。GitHub开发者对Stable Diffusion的采用
- en: The move to open source the model was controversial, and raised concerns about
    AI ethics and safety. Indeed, many of the initial use cases were to generate AI
    porn, as evidenced by the not safe for work (NSFW) models shared on platforms
    like [Civitai](https://civitai.com). However, the ability for hobbyists and tinkerers
    to modify and extend the model, as well as fine-tune it on their own data, led
    to rapid evolution and improvement of the model’s functionality. The decision
    to surface all of the model’s parameters to users, such as Classifier Free Guidance
    (how closely to follow a prompt), Denoising (how much noise to add to the base
    image for the model to remove during inference), and Seed (the random noise to
    start denoising from), has led to more creativity and innovative artwork. The
    accessibility and reliability of open source have also enticed several small businesses
    to build on top of Stable Diffusion, such as Pieter Level’s [PhotoAI](https://photoai.com)
    and [InteriorAI](http://interiorai.com) (together raking in more than $100,000
    in monthly revenue), and Danny Postma’s [Headshot Pro](https://www.headshotpro.com).
    As well as matching DALL-E’s inpainting and outpainting functionality, open source
    contributions have also kept pace with Midjourney’s features, such as negative
    prompts, weighted terms, and the ability to reverse engineer prompts from images.
    In addition, advanced functionality like ControlNet (matching the posture or composition
    of an image) and Segment Anything (clicking on an element to generate a mask for
    inpainting), have been quickly added as extensions for use with Stable Diffusion
    (both released in April 2023), most commonly accessed via [AUTOMATIC1111’s web
    UI](https://oreil.ly/0inw3) ([Figure 7-11](#figure-7-11)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型开源的决定引起了争议，并引发了关于AI伦理和安全性的担忧。确实，许多最初的用例都是用于生成AI色情内容，正如在[Civitai](https://civitai.com)等平台上分享的不适宜工作环境（NSFW）模型所证明的那样。然而，业余爱好者和DIY爱好者能够修改和扩展模型，以及在自己的数据上微调它，这导致了模型功能的快速演化和改进。将所有模型参数（如分类器自由引导（如何紧密遵循提示）、去噪（模型在推理期间添加到基础图像中的噪声量）和种子（从哪里开始去噪的随机噪声））呈现给用户的决定，导致了更多创造性和创新的艺术作品。开源的易用性和可靠性也吸引了几家小型企业在其基础上构建，例如Pieter
    Level的[PhotoAI](https://photoai.com)和[InteriorAI](http://interiorai.com)（每月收入超过10万美元），以及Danny
    Postma的[Headshot Pro](https://www.headshotpro.com)。除了匹配DALL-E的修复和扩展功能外，开源贡献也保持了与Midjourney的功能同步，例如负面提示、加权术语以及从图像中逆向工程提示的能力。此外，像ControlNet（匹配图像的姿势或构图）和Segment
    Anything（点击元素以生成修复用遮罩）这样的高级功能也被迅速添加为稳定扩散的扩展（均于2023年4月发布），通常通过[AUTOMATIC1111的Web用户界面](https://oreil.ly/0inw3)（[图7-11](#figure-7-11)）访问。
- en: '![pega 0711](assets/pega_0711.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0711](assets/pega_0711.png)'
- en: Figure 7-11\. AUTOMATIC1111’s web UI for Stable Diffusion
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. AUTOMATIC1111的稳定扩散Web用户界面
- en: Version 1.5 of Stable Diffusion was released in October 2022 and is still in
    use today. Therefore, it will form the basis for the ControlNet examples in [Chapter 10](ch10.html#building_applications_10),
    the advanced section for image generation in this book. The weights for Stable
    Diffusion were released on Hugging Face, introducing a generation of AI engineers
    to the open source AI model hub. Version 2.0 of Stable Diffusion came out a month
    later in November 2022, trained on a more aesthetic subset of the original [LAION-5B
    dataset](https://oreil.ly/K5vX2) (a large-scale dataset of image and text pairs
    for research purposes), with NSFW (not safe for work) images filtered out. Power
    users of Stable Diffusion complained of censorship as well as a degradation in
    model performance, [speculating](https://oreil.ly/2mgh5) that NSFW images in the
    training set were necessary to generate realistic human anatomy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散的1.5版本于2022年10月发布，至今仍在使用。因此，它将成为本书第10章[构建应用程序](ch10.html#building_applications_10)中高级图像生成部分的ControlNet示例的基础。稳定扩散的权重在Hugging
    Face上发布，向一代AI工程师介绍了开源AI模型库。稳定扩散的2.0版本于一个月后的2022年11月发布，在原始[LAION-5B数据集](https://oreil.ly/K5vX2)（一个用于研究目的的大规模图像和文本对数据集）的更具美感的子集上训练，过滤掉了NSFW（不适宜工作环境）图像。稳定扩散的高级用户对审查制度以及模型性能的下降表示了不满，[推测](https://oreil.ly/2mgh5)训练集中的NSFW图像对于生成逼真的人体解剖结构是必要的。
- en: Stability AI [raised over $100 million](https://oreil.ly/BT-k5) and has continued
    to develop newer models, including [DeepFloyd](https://oreil.ly/UCQ3I), a model
    better able to generate real text on images (an issue that plagues other models)
    and the current favorite [Stable Diffusion XL 1.0](https://oreil.ly/gcT4t) (abbreviated
    to SDXL). This model has overcome the misgivings of the community over censorship
    in version 2.0, not least due to the impressive results of this more powerful
    model, which has 6.6 billion parameters, compared with 0.98 billion for the v1.5
    model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Stability AI [筹集了超过1亿美元](https://oreil.ly/BT-k5)并继续开发新的模型，包括[DeepFloyd](https://oreil.ly/UCQ3I)，这是一个能够更好地在图像上生成真实文本的模型（这是困扰其他模型的问题），以及当前最受欢迎的[稳定扩散XL
    1.0](https://oreil.ly/gcT4t)（简称SDXL）。这个模型克服了社区对2.0版本中审查的疑虑，这很大程度上归功于这个更强大的模型，它有66亿个参数，而v1.5模型有9.8亿个参数。
- en: Google Gemini
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌Gemini
- en: Google long threatened to be a competitor in the space with their [Imagen](https://oreil.ly/K8oWv)
    model (not [released publicly](https://oreil.ly/341QB)), and indeed ex-Googlers
    have since founded a promising new image model [Ideogram](https://ideogram.ai),
    released in August 2023\. They finally entered the image generation game with
    Gemini in December 2023, though quickly faced criticism over a clumsy attempt
    to [promote diversity](https://oreil.ly/u-Glg). It remains to be seen whether
    Google’s internal politics will prevent them from capitalizing on their significant
    resources.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌长期以来一直威胁要成为该领域的竞争对手，他们的[Imagen](https://oreil.ly/K8oWv)模型（尚未[公开发布](https://oreil.ly/341QB)），而且确实有前谷歌员工创立了一个有前途的新图像模型[Ideogram](https://ideogram.ai)，该模型于2023年8月发布。他们终于在2023年12月推出了Gemini，尽管很快因为笨拙地尝试[推广多样性](https://oreil.ly/u-Glg)而受到批评。谷歌的内部政治是否会阻止他们利用其显著资源，还有待观察。
- en: Text to Video
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到视频
- en: Much of the attention in the image space is also likely to shift toward *text-to-video*,
    *image-to-video*, and even *video-to-video*, as the Stable Diffusion community
    [extends the capabilities](https://oreil.ly/l7KHB) of the model to generate consistent
    images frame by frame, including promising open source projects such as [AnimateDiff](https://oreil.ly/CsJgT).
    In addition, one of the cocreators of Stable Diffusion, RunwayML, has become the
    leading pioneer in text-to-video, and is starting to get usable results with their
    [Gen-2 model](https://oreil.ly/vS0mA). [Stable Video Diffusion](https://oreil.ly/UuApM)
    was released in November 2023, capable of turning text into short video clips
    or animating existing images, and [Stable Diffusion Turbo](https://oreil.ly/uMAkh)
    can generate images in near real time. The release of [Sora](https://oreil.ly/sora)
    in February 2024 shows that OpenAI isn’t sleeping on this space either. Although
    we don’t cover text-to-video prompting techniques explicitly, everything you learn
    about prompting for image generation also applies directly to video.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像空间中，大部分的关注力也可能转向 *文本到视频*、*图像到视频*，甚至 *视频到视频*，因为稳定扩散社区正在扩展模型的能力，以逐帧生成一致性的图像，包括像[AnimateDiff](https://oreil.ly/CsJgT)这样的有前景的开源项目。此外，稳定扩散的一个共同创造者RunwayML已经成为文本到视频领域的领先先驱，并开始使用他们的[Gen-2模型](https://oreil.ly/vS0mA)获得可用的结果。[稳定视频扩散](https://oreil.ly/UuApM)于2023年11月发布，能够将文本转换为短视频剪辑或对现有图像进行动画处理，而[稳定扩散Turbo](https://oreil.ly/uMAkh)可以几乎实时地生成图像。2024年2月发布的[Sora](https://oreil.ly/sora)表明，OpenAI也没有在这个领域休息。尽管我们没有明确涵盖文本到视频提示技术，但你所学到的关于图像生成提示的一切也直接适用于视频。
- en: Model Comparison
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型比较
- en: As demand for AI image generation increases and competition heats up, new entrants
    will emerge, and the major players will diversify. In our own workflows we already
    find ourselves using different models for different reasons. DALL-E 3 is great
    at composition, and the integration with ChatGPT is convenient. Midjourney still
    has the best aesthetics, both for fantasy and photorealism. Stable Diffusion being
    open source makes it the most flexible and extendable model, and is what most
    AI businesses build their products on top of. Each model has evolved toward a
    distinct style and set of capabilities, as can be discerned when comparing the
    same prompt across multiple models, as in [Figure 7-12](#figure-7-12).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对AI图像生成需求的增加和竞争的加剧，新参与者将出现，主要玩家将多样化。在我们的工作流程中，我们已经开始使用不同的模型来满足不同的需求。DALL-E
    3在构图方面表现优秀，与ChatGPT的集成也很方便。Midjourney在幻想和真实照片方面仍然具有最佳的美学效果。Stable Diffusion作为开源模型，使其成为最灵活和可扩展的模型，也是大多数AI企业在其产品之上构建的基础。每个模型都朝着独特的风格和功能集发展，正如在比较多个模型上的相同提示时可以辨别出来，如[图7-12](#figure-7-12)所示。
- en: 'Input:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Figure 7-12](#figure-7-12) shows the output.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-12](#figure-7-12)显示了输出。'
- en: '![pega 0712](assets/pega_0712.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0712](assets/pega_0712.png)'
- en: Figure 7-12\. A corgi on top of the Brandenburg Gate
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12\. 一只柯基站在勃兰登堡门上
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you were introduced to diffusion models for AI image generation.
    These models, such as DALL-E, Stable Diffusion, and Midjourney, use random noise
    and denoising techniques to generate images based on text descriptions. They have
    been trained on large datasets and can replicate various art styles. However,
    there is controversy surrounding copyright issues. You learned how prompt engineering
    principles apply to image generation when navigating the latent space to find
    the desired image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你被介绍到了用于AI图像生成的扩散模型。这些模型，如DALL-E、Stable Diffusion和Midjourney，使用随机噪声和去噪技术根据文本描述生成图像。它们在大型数据集上进行了训练，可以复制各种艺术风格。然而，关于版权问题存在争议。你学习了当在潜在空间中导航以找到所需图像时，提示工程原则如何应用于图像生成。
- en: In this chapter, you explored the different approaches taken by organizations
    like OpenAI, Stability AI, and Midjourney in developing text-to-image models.
    OpenAI’s DALL-E gained popularity for its artistic abilities, but access was limited,
    and the quality of replicated models was poorer. Midjourney, on the other hand,
    capitalized on the demand for DALL-E alternatives and gained a cult following
    with its v3 and v4 models. It had a subscription-based pricing model and a strong
    community focus. Stable Diffusion, on the other hand, gained attention for its
    comparable results to DALL-E and Midjourney, but with the advantage of being open
    source and free to run on personal computers. By reading this chapter, you also
    gained insights into the history of AI image generation and the advancements made
    by organizations like OpenAI, Midjourney, and Stable Diffusion.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你探讨了OpenAI、Stability AI和Midjourney等组织在开发文本到图像模型时采取的不同方法。OpenAI的DALL-E因其艺术能力而受到欢迎，但访问受限，复制的模型质量较差。另一方面，Midjourney利用了对DALL-E替代品的需求，并通过其v3和v4模型获得了狂热追随者。它采用基于订阅的定价模式，并注重社区建设。另一方面，Stable
    Diffusion因其与DALL-E和Midjourney相当的结果而受到关注，但具有开源和免费在个人电脑上运行的优势。通过阅读本章，你还获得了对AI图像生成历史以及OpenAI、Midjourney和Stable
    Diffusion等组织所取得的进步的见解。
- en: In the next chapter, you will learn practical tips for handling image generation
    with AI. The chapter will equip you with the necessary knowledge and techniques
    to create visually stunning and unique images. From format modifiers to art-style
    replication, you will discover the power of prompt engineering in creating captivating
    and original visual content. Get ready to unleash your creativity and take your
    image generation skills to the next level.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习处理AI图像生成的实用技巧。本章将为你提供创建视觉上令人惊叹且独特的图像所需的知识和技术。从格式修饰符到艺术风格复制，你将发现提示工程在创建吸引人和原创视觉内容方面的力量。准备好释放你的创造力，并将你的图像生成技能提升到新的水平。
