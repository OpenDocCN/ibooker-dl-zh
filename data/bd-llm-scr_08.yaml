- en: appendix B References and further reading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B 参考文献及进一步阅读
- en: Chapter 1
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章
- en: 'Custom-built LLMs are able to outperform general-purpose LLMs as a team at
    Bloomberg showed via a version of GPT pretrained on finance data from scratch.
    The custom LLM outperformed ChatGPT on financial tasks while maintaining good
    performance on general LLM benchmarks:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如彭博团队通过从零开始预训练金融数据的 GPT 版本所展示，定制构建的 LLM 能够超越通用 LLM，同时在通用 LLM 基准测试中保持良好的性能：
- en: '“BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al., [https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人撰写的《“BloombergGPT：用于金融的大型语言模型”》（2023），[https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)
- en: 'Existing LLMs can be adapted and fine-tuned to outperform general LLMs as well,
    which teams from Google Research and Google DeepMind showed in a medical context:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的 LLM 可以进行适配和微调，以超越通用 LLM，谷歌研究和谷歌 DeepMind 团队在医疗环境中展示了这一点：
- en: “Towards Expert-Level Medical Question Answering with Large Language Models”
    (2023) by Singhal et al., [https://arxiv.org/abs/2305.09617](https://arxiv.org/abs/2305.09617)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等人撰写的《“使用大型语言模型实现专家级医学问答”》（2023），[https://arxiv.org/abs/2305.09617](https://arxiv.org/abs/2305.09617)
- en: 'The following paper proposed the original transformer architecture:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文提出了原始的 Transformer 架构：
- en: “Attention Is All You Need” (2017) by Vaswani et al., [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人撰写的《“注意力即一切”》（2017），[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: On the original encoder-style transformer, called BERT, see
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关于原始编码器风格的 Transformer，即 BERT，请参阅
- en: '“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
    (2018) by Devlin et al., [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人撰写的《“BERT：用于语言理解的深度双向 Transformer 的预训练”》（2018），[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: The paper describing the decoder-style GPT-3 model, which inspired modern LLMs
    and will be used as a template for implementing an LLM from scratch in this book,
    is
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 描述解码器风格的 GPT-3 模型的论文，该模型启发了现代 LLM，并将作为本书中从头实现 LLM 的模板，该论文是
- en: “Language Models are Few-Shot Learners” (2020) by Brown et al., [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人撰写的《“语言模型是少样本学习者”》（2020），[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
- en: 'The following covers the original vision transformer for classifying images,
    which illustrates that transformer architectures are not only restricted to text
    inputs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下涵盖了用于图像分类的原版视觉 Transformer，这表明 Transformer 架构不仅限于文本输入：
- en: '“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”
    (2020) by Dosovitskiy et al., [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人撰写的《“一张图片等于 16x16 个单词：大规模图像识别的 Transformer”》（2020），[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
- en: 'The following experimental (but less popular) LLM architectures serve as examples
    that not all LLMs need to be based on the transformer architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验（但不太流行）的 LLM 架构作为例子，说明并非所有 LLM 都必须基于 Transformer 架构：
- en: '“RWKV: Reinventing RNNs for the Transformer Era” (2023) by Peng et al., [https://arxiv.org/abs/2305.13048](https://arxiv.org/abs/2305.13048)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人撰写的《“RWKV：为 Transformer 时代重新发明 RNN”》（2023），[https://arxiv.org/abs/2305.13048](https://arxiv.org/abs/2305.13048)
- en: '“Hyena Hierarchy: Towards Larger Convolutional Language Models” (2023) by Poli
    et al., [https://arxiv.org/abs/2302.10866](https://arxiv.org/abs/2302.10866)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poli 等人撰写的《“鬣狗等级：向更大型的卷积语言模型迈进”》（2023），[https://arxiv.org/abs/2302.10866](https://arxiv.org/abs/2302.10866)
- en: '“Mamba: Linear-Time Sequence Modeling with Selective State Spaces” (2023) by
    Gu and Dao, [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 和 Dao 撰写的《“Mamba：具有选择性状态空间的线性时间序列建模”》（2023），[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)
- en: 'Meta AI’s model is a popular implementation of a GPT-like model that is openly
    available in contrast to GPT-3 and ChatGPT:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI 的模型是 GPT 类模型的一种流行实现，与 GPT-3 和 ChatGPT 相比，它是公开可用的：
- en: '“Llama 2: Open Foundation and Fine-Tuned Chat Models” (2023) by Touvron et
    al., [https://arxiv.org/abs/2307.092881](https://arxiv.org/abs/2307.092881)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人撰写的《“Llama 2：开放基础和微调的聊天模型”》（2023），[https://arxiv.org/abs/2307.092881](https://arxiv.org/abs/2307.092881)
- en: 'For readers interested in additional details about the dataset references in
    section 1.5, this paper describes the publicly available *The Pile* dataset curated
    by Eleuther AI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对第 1.5 节中数据集引用的详细信息感兴趣的读者，本文描述了由 Eleuther AI 精心整理的公开可用 *The Pile* 数据集：
- en: '“The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by
    Gao et al., [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “The Pile：用于语言建模的800GB多样化文本数据集”（2020）由Gao等人所著，[https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)
- en: 'The following paper provides the reference for InstructGPT for fine-tuning
    GPT-3, which was mentioned in section 1.6 and will be discussed in more detail
    in chapter 7:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文提供了InstructGPT微调GPT-3的参考，这在第1.6节中提到，将在第7章中更详细地讨论：
- en: “Training Language Models to Follow Instructions with Human Feedback” (2022)
    by Ouyang et al., [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “通过人类反馈训练语言模型以遵循指令”（2022）由Ouyang等人所著，[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)
- en: Chapter 2
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章
- en: 'Readers who are interested in discussion and comparison of embedding spaces
    with latent spaces and the general notion of vector representations can find more
    information in the first chapter of my book:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对讨论和比较嵌入空间与潜在空间以及向量表示的通用概念感兴趣的读者，可以在我的书的第一章中找到更多信息：
- en: Machine Learning Q and AI (2023) by Sebastian Raschka, [https://leanpub.com/machine-learning-q-and-ai](https://leanpub.com/machine-learning-q-and-ai)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习Q和AI（2023）由Sebastian Raschka所著，[https://leanpub.com/machine-learning-q-and-ai](https://leanpub.com/machine-learning-q-and-ai)
- en: 'The following paper provides more in-depth discussions of how byte pair encoding
    is used as a tokenization method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文更深入地讨论了字节对编码作为分词方法的使用：
- en: “Neural Machine Translation of Rare Words with Subword Units” (2015) by Sennrich
    et al., [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “使用子词单元进行罕见词的神经机器翻译”（2015）由Sennrich等人所著，[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)
- en: 'The code for the byte pair encoding tokenizer used to train GPT-2 was open-sourced
    by OpenAI:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI开源了用于训练GPT-2的字节对编码分词器的代码：
- en: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)'
- en: 'OpenAI provides an interactive web UI to illustrate how the byte pair tokenizer
    in GPT models works:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一个交互式的Web用户界面，以展示GPT模型中的字节对分词器是如何工作的：
- en: '[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)'
- en: 'For readers interested in coding and training a BPE tokenizer from the ground
    up, Andrej Karpathy’s GitHub repository `minbpe` offers a minimal and readable
    implementation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对从零开始编码和训练BPE分词器感兴趣的读者，Andrej Karpathy的GitHub仓库`minbpe`提供了一个最小化和可读的实现：
- en: “A Minimal Implementation of a BPE Tokenizer,” [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “一个BPE分词器的最小实现”，[https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)
- en: 'Readers who are interested in studying alternative tokenization schemes that
    are used by some other popular LLMs can find more information in the SentencePiece
    and WordPiece papers:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对研究某些其他流行LLM使用的替代分词方案感兴趣的读者，可以在SentencePiece和WordPiece论文中找到更多信息：
- en: '“SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer
    for Neural Text Processing” (2018) by Kudo and Richardson, [https://aclanthology.org/D18-2012/](https://aclanthology.org/D18-2012/)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “SentencePiece：用于神经文本处理的一个简单且语言无关的子词分词器和反分词器”（2018）由Kudo和Richardson所著，[https://aclanthology.org/D18-2012/](https://aclanthology.org/D18-2012/)
- en: “Fast WordPiece Tokenization” (2020) by Song et al., [https://arxiv.org/abs/2012.15524](https://arxiv.org/abs/2012.15524)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “快速WordPiece分词”（2020）由Song等人所著，[https://arxiv.org/abs/2012.15524](https://arxiv.org/abs/2012.15524)
- en: Chapter 3
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章
- en: 'Readers interested in learning more about Bahdanau attention for RNN and language
    translation can find detailed insights in the following paper:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想要了解更多关于RNN和语言翻译中Bahdanau注意力机制的读者，可以在以下论文中找到详细见解：
- en: “A Minimal Implementation of a BPE Tokenizer,” [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “一个BPE分词器的最小实现”，[https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)
- en: 'The concept of self-attention as scaled dot-product attention was introduced
    in the original transformer paper:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力作为缩放点积注意力在原始的Transformer论文中被引入：
- en: “Attention Is All You Need” (2017) by Vaswani et al., [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Attention Is All You Need”（2017）由Vaswani等人所著，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'FlashAttention is a highly efficient implementation of a self-attention mechanism,
    which accelerates the computation process by optimizing memory access patterns.
    FlashAttention is mathematically the same as the standard self-attention mechanism
    but optimizes the computational process for efficiency:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention是自注意力机制的高效实现，通过优化内存访问模式加速计算过程。从数学上讲，FlashAttention与标准自注意力机制相同，但优化了计算过程以提高效率：
- en: '“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”
    (2022) by Dao et al., [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”
    (2022) by Dao et al., [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)'
- en: '“FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”
    (2023) by Dao, [https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”
    (2023) by Dao, [https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)'
- en: 'PyTorch implements a function for self-attention and causal attention that
    supports FlashAttention for efficiency. This function is beta and subject to change:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch实现了一个支持FlashAttention以提高效率的自注意力和因果注意力的函数。此函数处于测试阶段，可能会发生变化：
- en: '`scaled_dot_product_attention` documentation: [https://mng.bz/NRJd](https://mng.bz/NRJd)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaled_dot_product_attention`文档：[https://mng.bz/NRJd](https://mng.bz/NRJd)'
- en: 'PyTorch also implements an efficient `MultiHeadAttention` class based on the
    `scaled_` `dot_product` function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch也实现了一个基于`scaled_` `dot_product`函数的高效的`MultiHeadAttention`类：
- en: '`MultiHeadAttention` documentation: [https://mng.bz/DdJV](https://mng.bz/DdJV)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiHeadAttention`文档：[https://mng.bz/DdJV](https://mng.bz/DdJV)'
- en: 'Dropout is a regularization technique used in neural networks to prevent overfitting
    by randomly dropping units (along with their connections) from the neural network
    during training:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种在训练过程中随机从神经网络中丢弃单元（及其连接）的正则化技术，以防止过拟合：
- en: '“Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014)
    by Srivastava et al., [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014)
    by Srivastava et al., [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
- en: 'While using the multi-head attention based on scaled-dot product attention
    remains the most common variant of self-attention in practice, authors have found
    that it’s possible to also achieve good performance without the value weight matrix
    and projection layer:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在实践中最常见的自注意力变体是基于缩放点积注意力的多头注意力，但作者发现，即使没有值权重矩阵和投影层，也可以实现良好的性能：
- en: “Simplifying Transformer Blocks” (2023) by He and Hofmann, [https://arxiv.org/abs/2311.01906](https://arxiv.org/abs/2311.01906)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Simplifying Transformer Blocks” (2023) by He and Hofmann, [https://arxiv.org/abs/2311.01906](https://arxiv.org/abs/2311.01906)
- en: Chapter 4
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章
- en: 'The following paper introduces a technique that stabilizes the hidden state
    dynamics neural networks by normalizing the summed inputs to the neurons within
    a hidden layer, significantly reducing training time compared to previously published
    methods:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文介绍了一种通过标准化隐藏层中神经元的总和输入来稳定隐藏状态动态的神经网络的技术，与之前发表的方法相比，显著减少了训练时间：
- en: “Layer Normalization” (2016) by Ba, Kiros, and Hinton, [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Layer Normalization” (2016) by Ba, Kiros, and Hinton, [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)
- en: 'Post-LayerNorm, used in the original transformer model, applies layer normalization
    after the self-attention and feed forward networks. In contrast, Pre-LayerNorm,
    as adopted in models like GPT-2 and newer LLMs, applies layer normalization before
    these components, which can lead to more stable training dynamics and has been
    shown to improve performance in some cases, as discussed in the following papers:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Post-LayerNorm，在原始Transformer模型中使用，在自注意力和前馈网络之后应用层归一化。相比之下，Pre-LayerNorm，如GPT-2和更新的LLMs中采用，在这些组件之前应用层归一化，这可能导致更稳定的训练动态，并且已经证明在某些情况下可以提高性能，如以下论文所述：
- en: “On Layer Normalization in the Transformer Architecture” (2020) by Xiong et
    al., [https://arxiv.org/abs/2002.04745](https://arxiv.org/abs/2002.04745)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “On Layer Normalization in the Transformer Architecture” (2020) by Xiong et
    al., [https://arxiv.org/abs/2002.04745](https://arxiv.org/abs/2002.04745)
- en: '“ResiDual: Transformer with Dual Residual Connections” (2023) by Tie et al.,
    [https://arxiv.org/abs/2304.14802](https://arxiv.org/abs/2304.14802)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“ResiDual: Transformer with Dual Residual Connections” (2023) by Tie et al.,
    [https://arxiv.org/abs/2304.14802](https://arxiv.org/abs/2304.14802)'
- en: A popular variant of LayerNorm used in modern LLMs is RMSNorm due to its improved
    computing efficiency. This variant simplifies the normalization process by normalizing
    the inputs using only the root mean square of the inputs, without subtracting
    the mean before squaring. This means it does not center the data before computing
    the scale. RMSNorm is described in more detail in
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代LLM中使用的LayerNorm的一个流行变体是RMSNorm，因为它提高了计算效率。这种变体通过仅使用输入的均方根来归一化输入，而不在平方之前减去均值，从而简化了归一化过程。这意味着在计算尺度之前不会对数据进行中心化。RMSNorm在以下内容中描述得更详细：
- en: “Root Mean Square Layer Normalization” (2019) by Zhang and Sennrich, [https://arxiv.org/abs/1910.07467](https://arxiv.org/abs/1910.07467)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “均方根层归一化”（2019）由Zhang和Sennrich撰写，[https://arxiv.org/abs/1910.07467](https://arxiv.org/abs/1910.07467)
- en: 'The Gaussian Error Linear Unit (GELU) activation function combines the properties
    of both the classic ReLU activation function and the normal distribution’s cumulative
    distribution function to model layer outputs, allowing for stochastic regularization
    and nonlinearities in deep learning models:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯误差线性单元（GELU）激活函数结合了经典ReLU激活函数和正态分布的累积分布函数的特性，以建模层输出，允许在深度学习模型中进行随机正则化和非线性：
- en: “Gaussian Error Linear Units (GELUs)” (2016) by Hendricks and Gimpel, [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “高斯误差线性单元（GELUs）”（2016）由Hendricks和Gimpel撰写，[https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)
- en: 'The GPT-2 paper introduced a series of transformer-based LLMs with varying
    sizes—124 million, 355 million, 774 million, and 1.5 billion parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2论文介绍了一系列基于transformer的、大小各异的LLM——1.24亿、3.55亿、7.74亿和15亿个参数：
- en: “Language Models Are Unsupervised Multitask Learners” (2019) by Radford et al.,
    [http://mng.bz/DMv0](http://mng.bz/DMv0)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “语言模型是无监督多任务学习者”（2019）由Radford等人撰写，[http://mng.bz/DMv0](http://mng.bz/DMv0)
- en: 'OpenAI’s GPT-3 uses fundamentally the same architecture as GPT-2, except that
    the largest version (175 billion) is 100x larger than the largest GPT-2 model
    and has been trained on much more data. Interested readers can refer to the official
    GPT-3 paper by OpenAI and the technical overview by Lambda Labs, which calculates
    that training GPT-3 on a single RTX 8000 consumer GPU would take 665 years:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3基本上使用了与GPT-2相同的架构，除了最大的版本（1750亿）比最大的GPT-2模型大100倍，并且使用了更多的数据进行训练。感兴趣的读者可以参考OpenAI的官方GPT-3论文以及Lambda
    Labs的技术概述，该概述计算在单个RTX 8000消费级GPU上训练GPT-3需要665年：
- en: “Language Models are Few-Shot Learners” (2023) by Brown et al., [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “语言模型是少样本学习者”（2023）由Brown等人撰写，[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
- en: '“OpenAI’s GPT-3 Language Model: A Technical Overview,” [https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “OpenAI的GPT-3语言模型：技术概述”，[https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)
- en: 'NanoGPT is a code repository with a minimalist yet efficient implementation
    of a GPT-2 model, similar to the model implemented in this book. While the code
    in this book is different from nanoGPT, this repository inspired the reorganization
    of a large GPT Python parent class implementation into smaller submodules:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NanoGPT是一个代码仓库，它以简约而高效的方式实现了GPT-2模型，类似于本书中实现的模型。虽然本书中的代码与nanoGPT不同，但这个仓库激发了将大型GPT
    Python父类实现重组为较小子模块的灵感：
- en: “NanoGPT, a Repository for Training Medium-Sized GPTs, [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “NanoGPT，一个用于训练中等大小GPT的仓库，[https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
- en: 'An informative blog post showing that most of the computation in LLMs is spent
    in the feed forward layers rather than attention layers when the context size
    is smaller than 32,000 tokens is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇信息丰富的博客文章展示了，当上下文大小小于32,000个标记时，大多数LLM的计算都是在前馈层而不是注意力层中进行的：
- en: “In the Long (Context) Run” by Harm de Vries, [https://www.harmdevries.com/post/context-length/](https://www.harmdevries.com/post/context-length/)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在长（上下文）运行中”由Harm de Vries撰写，[https://www.harmdevries.com/post/context-length/](https://www.harmdevries.com/post/context-length/)
- en: Chapter 5
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章
- en: 'For information on detailing the loss function and applying a log transformation
    to make it easier to handle for mathematical optimization, see my lecture video:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于详细说明损失函数和应用对数变换以使其更容易进行数学优化的信息，请参阅我的讲座视频：
- en: L8.2 Logistic Regression Loss Function, [https://www.youtube.com/watch?v=GxJe0DZvydM](https://www.youtube.com/watch?v=GxJe0DZvydM)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L8.2逻辑回归损失函数，[https://www.youtube.com/watch?v=GxJe0DZvydM](https://www.youtube.com/watch?v=GxJe0DZvydM)
- en: 'The following lecture and code example by the author explain how PyTorch’s
    cross-entropy functions works under the hood:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的以下讲座和代码示例解释了PyTorch的交叉熵函数在底层是如何工作的：
- en: L8.7.1 OneHot Encoding and Multi-category Cross Entropy, [https://www.youtube.com/watch?v=4n71-tZ94yk](https://www.youtube.com/watch?v=4n71-tZ94yk)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L8.7.1 OneHot编码和多类别交叉熵，[https://www.youtube.com/watch?v=4n71-tZ94yk](https://www.youtube.com/watch?v=4n71-tZ94yk)
- en: Understanding Onehot Encoding and Cross Entropy in PyTorch, [https://mng.bz/o05v](https://mng.bz/o05v)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PyTorch中的Onehot编码和交叉熵，[https://mng.bz/o05v](https://mng.bz/o05v)
- en: 'The following two papers detail the dataset, hyperparameter, and architecture
    details used for pretraining LLMs:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两篇论文详细介绍了用于预训练LLM的数据集、超参数和架构细节：
- en: '“Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling”
    (2023) by Biderman et al., [https://arxiv.org/abs/2304.01373](https://arxiv.org/abs/2304.01373)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Pythia：用于分析大型语言模型训练和扩展的套件”（2023）由Biderman等人撰写，[https://arxiv.org/abs/2304.01373](https://arxiv.org/abs/2304.01373)
- en: '“OLMo: Accelerating the Science of Language Models” (2024) by Groeneveld et
    al., [https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “OLMo：加速语言模型科学”（2024）由Groeneveld等人撰写，[https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)
- en: 'The following supplementary code available for this book contains instructions
    for preparing 60,000 public domain books from Project Gutenberg for LLM training:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为本书提供的补充代码，包含从Project Gutenberg项目准备60,000本公共领域书籍用于LLM训练的说明：
- en: Pretraining GPT on the Project Gutenberg Dataset, [https://mng.bz/Bdw2](https://mng.bz/Bdw2)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Project Gutenberg数据集上预训练GPT，[https://mng.bz/Bdw2](https://mng.bz/Bdw2)
- en: 'Chapter 5 discusses the pretraining of LLMs, and appendix D covers more advanced
    training functions, such as linear warmup and cosine annealing. The following
    paper finds that similar techniques can be successfully applied to continue pretraining
    already pretrained LLMs, along with additional tips and insights:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章讨论了LLM的预训练，附录D涵盖了更高级的训练函数，例如线性预热和余弦退火。以下论文发现，类似的技巧可以成功应用于继续预训练已经预训练的LLM，并附带额外的技巧和见解：
- en: “Simple and Scalable Strategies to Continually Pre-train Large Language Models”
    (2024) by Ibrahim et al., [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “简单且可扩展的策略以持续预训练大型语言模型”（2024）由Ibrahim等人撰写，[https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)
- en: 'BloombergGPT is an example of a domain-specific LLM created by training on
    both general and domain-specific text corpora, specifically in the field of finance:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: BloombergGPT是通过对通用和特定领域文本语料库进行训练创建的特定领域LLM的示例，特别是在金融领域：
- en: '“BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al., [https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “BloombergGPT：用于金融的LLM”（2023）由Wu等人撰写，[https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)
- en: 'GaLore is a recent research project that aims to make LLM pretraining more
    efficient. The required code change boils down to just replacing PyTorch’s `AdamW`
    optimizer in the training function with the `GaLoreAdamW` optimizer provided by
    the `galore-torch` Python package:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: GaLore是一个旨在使LLM预训练更高效的研究项目。所需的代码更改仅限于将训练函数中的PyTorch的`AdamW`优化器替换为`galore-torch`Python包提供的`GaLoreAdamW`优化器：
- en: '“GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection” (2024)
    by Zhao et al., [https://arxiv.org/abs/2403.03507](https://arxiv.org/abs/2403.03507)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “GaLore：通过梯度低秩投影提高LLM训练效率”（2024）由Zhao等人撰写，[https://arxiv.org/abs/2403.03507](https://arxiv.org/abs/2403.03507)
- en: GaLore code repository, [https://github.com/jiaweizzhao/GaLore](https://github.com/jiaweizzhao/GaLore)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GaLore代码仓库，[https://github.com/jiaweizzhao/GaLore](https://github.com/jiaweizzhao/GaLore)
- en: 'The following papers and resources share openly available, large-scale pretraining
    datasets for LLMs that consist of hundreds of gigabytes to terabytes of text data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文和资源公开分享了适用于LLM的大规模预训练数据集，这些数据集包含数百GB到TB的文本数据：
- en: '“Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research”
    (2024) by Soldaini et al., [https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Dolma：用于LLM预训练研究的3000亿标记开放语料库”（2024）由Soldaini等人撰写，[https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159)
- en: '“The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by
    Gao et al., [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “The Pile：用于语言建模的800GB多样化文本数据集”（2020）由Gao等人撰写，[https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)
- en: '“The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with
    Web Data, and Web Data Only,” (2023) by Penedo et al., [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Falcon LLM的RefinedWeb数据集：仅使用网页数据超越精选语料库，”（2023）由Penedo等人撰写，[https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116)
- en: “RedPajama,” by Together AI, [https://mng.bz/d6nw](https://mng.bz/d6nw)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “RedPajama”，由Together AI编写，[https://mng.bz/d6nw](https://mng.bz/d6nw)
- en: The FineWeb Dataset, which includes more than 15 trillion tokens of cleaned
    and deduplicated English web data sourced from CommonCrawl, [https://mng.bz/rVzy](https://mng.bz/rVzy)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FineWeb数据集，包括来自CommonCrawl的超过1500万亿个清洗和去重的英文网页数据，[https://mng.bz/rVzy](https://mng.bz/rVzy)
- en: The paper that originally introduced top-k sampling is
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 原始介绍top-k采样的论文是
- en: “Hierarchical Neural Story Generation” (2018) by Fan et al., [https://arxiv.org/abs/1805.04833](https://arxiv.org/abs/1805.04833)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “分层神经故事生成”（2018）由Fan等人撰写，[https://arxiv.org/abs/1805.04833](https://arxiv.org/abs/1805.04833)
- en: 'An alternative to top-k sampling is top-p sampling (not covered in chapter
    5), which selects from the smallest set of top tokens whose cumulative probability
    exceeds a threshold *p*, while top-k sampling picks from the top *k* tokens by
    probability:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: top-k采样的替代方法是top-p采样（第5章未涉及），它从累积概率超过阈值*p*的最小top tokens集中选择，而top-k采样则按概率从top
    *k* tokens中选择：
- en: Top-p sampling, [https://en.wikipedia.org/wiki/Top-p_sampling](https://en.wikipedia.org/wiki/Top-p_sampling)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top-p采样，[https://en.wikipedia.org/wiki/Top-p_sampling](https://en.wikipedia.org/wiki/Top-p_sampling)
- en: 'Beam search (not covered in chapter 5) is an alternative decoding algorithm
    that generates output sequences by keeping only the top-scoring partial sequences
    at each step to balance efficiency and quality:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索（第5章未涉及）是一种替代解码算法，通过在每个步骤中仅保留得分最高的部分序列来生成输出序列，以平衡效率和质量：
- en: '“Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models”
    (2016) by Vijayakumar et al., [https://arxiv.org/abs/1610.02424](https://arxiv.org/abs/1610.02424)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “多样化的束搜索：从神经序列模型解码多样化的解决方案”（2016）由Vijayakumar等人撰写，[https://arxiv.org/abs/1610.02424](https://arxiv.org/abs/1610.02424)
- en: Chapter 6
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章
- en: Additional resources that discuss the different types of fine-tuning are
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论不同类型微调的额外资源包括
- en: “Using and Finetuning Pretrained Transformers,” [https://mng.bz/VxJG](https://mng.bz/VxJG)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “使用和微调预训练的Transformer，”[https://mng.bz/VxJG](https://mng.bz/VxJG)
- en: “Finetuning Large Language Models,” [https://mng.bz/x28X](https://mng.bz/x28X)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “微调大型语言模型”，[https://mng.bz/x28X](https://mng.bz/x28X)
- en: 'Additional experiments, including a comparison of fine-tuning the first output
    token versus the last output token, can be found in the supplementary code material
    on GitHub:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 包括比较微调第一个输出标记与最后一个输出标记的额外实验，可以在GitHub上的补充代码材料中找到：
- en: Additional spam classification experiments, [https://mng.bz/AdJx](https://mng.bz/AdJx)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的垃圾邮件分类实验，[https://mng.bz/AdJx](https://mng.bz/AdJx)
- en: 'For a binary classification task, such as spam classification, it is technically
    possible to use only a single output node instead of two output nodes, as I discuss
    in the following article:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像垃圾邮件分类这样的二分类任务，技术上可以使用单个输出节点而不是两个输出节点，正如我在以下文章中讨论的那样：
- en: “Losses Learned—Optimizing Negative Log-Likelihood and Cross-Entropy in PyTorch,”
    [https://mng.bz/ZEJA](https://mng.bz/ZEJA)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “损失学习——在PyTorch中优化负对数似然和交叉熵”，[https://mng.bz/ZEJA](https://mng.bz/ZEJA)
- en: 'You can find additional experiments on fine-tuning different layers of an LLM
    in the following article, which shows that fine-tuning the last transformer block,
    in addition to the output layer, improves the predictive performance substantially:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下文章中找到关于微调LLM不同层的额外实验，该文章表明除了输出层外，微调最后一个Transformer块可以显著提高预测性能：
- en: “Finetuning Large Language Models,” [https://mng.bz/RZJv](https://mng.bz/RZJv)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “微调大型语言模型”，[https://mng.bz/RZJv](https://mng.bz/RZJv)
- en: 'Readers can find additional resources and information for dealing with imbalanced
    classification datasets in the imbalanced-learn documentation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以在imbalanced-learn文档中找到处理不平衡分类数据集的额外资源和信息：
- en: “Imbalanced-Learn User Guide,” [https://mng.bz/2KNa](https://mng.bz/2KNa)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Imbalanced-Learn用户指南”，[https://mng.bz/2KNa](https://mng.bz/2KNa)
- en: 'For readers interested in classifying spam emails rather than spam text messages,
    the following resource provides a large email spam classification dataset in a
    convenient CSV format similar to the dataset format used in chapter 6:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对分类垃圾邮件而非垃圾短信感兴趣的读者，以下资源提供了一个方便的CSV格式的大电子邮件垃圾邮件分类数据集，类似于第6章中使用的数据集格式：
- en: Email Spam Classification Dataset, [https://mng.bz/1GEq](https://mng.bz/1GEq)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件垃圾邮件分类数据集，[https://mng.bz/1GEq](https://mng.bz/1GEq)
- en: 'GPT-2 is a model based on the decoder module of the transformer architecture,
    and its primary purpose is to generate new text. As an alternative, encoder-based
    models such as BERT and RoBERTa can be effective for classification tasks:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是基于transformer架构的解码器模块的模型，其主要目的是生成新的文本。作为替代，基于编码器的模型，如BERT和RoBERTa，在分类任务中可能非常有效：
- en: '“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
    (2018) by Devlin et al., [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “BERT：用于语言理解的深度双向变换器预训练”（2018）由Devlin等人撰写，[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: '“RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019) by Liu et
    al., [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人撰写的“RoBERTa：一种鲁棒优化的BERT预训练方法”（2019），[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)
- en: “Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews,”
    [https://mng.bz/PZJR](https://mng.bz/PZJR)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “对50k IMDB电影评论进行情感分类的额外实验”，[https://mng.bz/PZJR](https://mng.bz/PZJR)
- en: 'Recent papers are showing that the classification performance can be further
    improved by removing the causal mask during classification fine-tuning alongside
    other modifications:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 近期论文显示，通过在分类微调期间移除因果掩码以及进行其他修改，可以进一步提高分类性能：
- en: “Label Supervised LLaMA Finetuning” (2023) by Li et al., [https://arxiv.org/abs/2310.01208](https://arxiv.org/abs/2310.01208)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “标签监督的LLaMA微调”（2023）由Li等人撰写，[https://arxiv.org/abs/2310.01208](https://arxiv.org/abs/2310.01208)
- en: '“LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders” (2024)
    by BehnamGhader et al., [https://arxiv.org/abs/2404.05961](https://arxiv.org/abs/2404.05961)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “LLM2Vec：大型语言模型是秘密强大的文本编码器”（2024）由BehnamGhader等人撰写，[https://arxiv.org/abs/2404.05961](https://arxiv.org/abs/2404.05961)
- en: Chapter 7
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第七章
- en: 'The Alpaca dataset for instruction fine-tuning contains 52,000 instruction–response
    pairs and is one of the first and most popular publicly available datasets for
    instruction fine-tuning:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指令微调的Alpaca数据集包含52,000个指令-响应对，是第一个也是最流行的公开可用的指令微调数据集之一：
- en: '“Stanford Alpaca: An Instruction-Following Llama Model,” [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “斯坦福Alpaca：一个遵循指令的Llama模型”，[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- en: Additional publicly accessible datasets suitable for instruction fine-tuning
    include
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于指令微调的额外公开可访问数据集包括
- en: LIMA, [https://huggingface.co/datasets/GAIR/lima](https://huggingface.co/datasets/GAIR/lima)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIMA，[https://huggingface.co/datasets/GAIR/lima](https://huggingface.co/datasets/GAIR/lima)
- en: 'For more information, see “LIMA: Less Is More for Alignment,” Zhou et al.,
    [https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206)'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多信息，请参阅周等人撰写的“LIMA：对齐的‘少即是多’”，[https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206)
- en: UltraChat, [https://huggingface.co/datasets/openchat/ultrachat-sharegpt](https://huggingface.co/datasets/openchat/ultrachat-sharegpt)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UltraChat，[https://huggingface.co/datasets/openchat/ultrachat-sharegpt](https://huggingface.co/datasets/openchat/ultrachat-sharegpt)
- en: A large-scale dataset consisting of 805,000 instruction–response pairs; for
    more information, see “Enhancing Chat Language Models by Scaling High-quality
    Instructional Conversations,” by Ding et al., [https://arxiv.org/abs/2305.14233](https://arxiv.org/abs/2305.14233)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含805,000个指令-响应对的庞大数据集；更多信息，请参阅Ding等人撰写的“通过扩展高质量指令对话来增强聊天语言模型”，[https://arxiv.org/abs/2305.14233](https://arxiv.org/abs/2305.14233)
- en: Alpaca GPT4, [https://mng.bz/Aa0p](https://mng.bz/Aa0p)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpaca GPT4，[https://mng.bz/Aa0p](https://mng.bz/Aa0p)
- en: An Alpaca-like dataset with 52,000 instruction–response pairs generated with
    GPT-4 instead of GPT-3.5
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个类似Alpaca的数据集，包含52,000个指令-响应对，使用GPT-4而不是GPT-3.5生成
- en: 'Phi-3 is a 3.8-billion-parameter model with an instruction-fine-tuned variant
    that is reported to be comparable to much larger proprietary models, such as GPT-3.5:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3是一个拥有38亿参数的模型，其指令微调变体据称与许多更大的专有模型相当，例如GPT-3.5：
- en: '“Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone”
    (2024) by Abdin et al., [https://arxiv.org/abs/2404.14219](https://arxiv.org/abs/2404.14219)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Phi-3技术报告：在您的手机上本地运行的高性能语言模型”（2024）由Abdin等人撰写，[https://arxiv.org/abs/2404.14219](https://arxiv.org/abs/2404.14219)
- en: 'Researchers propose a synthetic instruction data generation method that generates
    300,000 high-quality instruction-response pairs from an instruction fine-tuned
    Llama-3 model. A pretrained Llama 3 base model fine-tuned on these instruction
    examples performs comparably to the original instruction fine-tuned Llama-3 model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了一种合成指令数据生成方法，从指令微调的 Llama-3 模型中生成 300,000 个高质量的指令-响应对。在这些指令示例上微调的预训练
    Llama 3 基础模型的表现与原始指令微调的 Llama-3 模型相当：
- en: '“Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing” (2024) by Xu et al., [https://arxiv.org/abs/2406.08464](https://arxiv.org/abs/2406.08464)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing” (2024) by Xu et al., [https://arxiv.org/abs/2406.08464](https://arxiv.org/abs/2406.08464)'
- en: 'Research has shown that not masking the instructions and inputs in instruction
    fine-tuning effectively improves performance on various NLP tasks and open-ended
    generation benchmarks, particularly when trained on datasets with lengthy instructions
    and brief outputs or when using a small number of training examples:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，在指令微调中不屏蔽指令和输入可以有效提高各种 NLP 任务和开放式生成基准测试的性能，尤其是在训练数据集包含长指令和简短输出或使用少量训练示例时：
- en: “Instruction Tuning with Loss Over Instructions” (2024) by Shi, [https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Instruction Tuning with Loss Over Instructions” (2024) by Shi, [https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)
- en: 'Prometheus and PHUDGE are openly available LLMs that match GPT-4 in evaluating
    long-form responses with customizable criteria. We don’t use these because at
    the time of this writing, they are not supported by Ollama and thus cannot be
    executed efficiently on a laptop:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 和 PHUDGE 是公开可用的 LLM，它们在评估长文本响应时可以与 GPT-4 相媲美，并支持自定义标准。我们之所以没有使用这些模型，是因为在撰写本文时，它们没有得到
    Ollama 的支持，因此无法在笔记本电脑上高效执行：
- en: '“Prometheus: Inducing Finegrained Evaluation Capability in Language Models”
    (2023) by Kim et al., [https://arxiv.org/abs/2310.08491](https://arxiv.org/abs/2310.08491)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“Prometheus: Inducing Finegrained Evaluation Capability in Language Models”
    (2023) by Kim et al., [https://arxiv.org/abs/2310.08491](https://arxiv.org/abs/2310.08491)'
- en: '“PHUDGE: Phi-3 as Scalable Judge” (2024) by Deshwal and Chawla, “[https://arxiv.org/abs/2405.08029](https://arxiv.org/abs/2405.08029)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“PHUDGE: Phi-3 as Scalable Judge” (2024) by Deshwal and Chawla, “[https://arxiv.org/abs/2405.08029](https://arxiv.org/abs/2405.08029)'
- en: '“Prometheus 2: An Open Source Language Model Specialized in Evaluating Other
    Language Models” (2024), by Kim et al., [https://arxiv.org/abs/2405.01535](https://arxiv.org/abs/2405.01535)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“Prometheus 2: An Open Source Language Model Specialized in Evaluating Other
    Language Models” (2024), by Kim et al., [https://arxiv.org/abs/2405.01535](https://arxiv.org/abs/2405.01535)'
- en: 'The results in the following report support the view that large language models
    primarily acquire factual knowledge during pretraining and that fine-tuning mainly
    enhances their efficiency in using this knowledge. Furthermore, this study explores
    how fine-tuning large language models with new factual information affects their
    ability to use preexisting knowledge, revealing that models learn new facts more
    slowly and their introduction during fine-tuning increases the model’s tendency
    to generate incorrect information:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下报告中的结果支持了这样一个观点：大型语言模型在预训练期间主要获取事实知识，而微调主要增强了它们使用这种知识的能力。此外，这项研究探讨了使用新事实信息微调大型语言模型如何影响它们使用现有知识的能力，揭示了模型学习新事实的速度较慢，并且在微调期间引入新事实增加了模型生成错误信息的倾向：
- en: “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” (2024) by
    Gekhman, [https://arxiv.org/abs/2405.05904](https://arxiv.org/abs/2405.05904)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” (2024) by
    Gekhman, [https://arxiv.org/abs/2405.05904](https://arxiv.org/abs/2405.05904)
- en: 'Preference fine-tuning is an optional step after instruction fine-tuning to
    align the LLM more closely with human preferences. The following articles by the
    author provide more information about this process:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 预设偏好微调是在指令微调之后的一个可选步骤，目的是使 LLM 更接近人类的偏好。作者以下文章提供了更多关于此过程的信息：
- en: '“LLM Training: RLHF and Its Alternatives,” [https://mng.bz/ZVPm](https://mng.bz/ZVPm)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“LLM Training: RLHF and Its Alternatives,” [https://mng.bz/ZVPm](https://mng.bz/ZVPm)'
- en: “Tips for LLM Pretraining and Evaluating Reward Models,” [https://mng.bz/RNXj](https://mng.bz/RNXj)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Tips for LLM Pretraining and Evaluating Reward Models,” [https://mng.bz/RNXj](https://mng.bz/RNXj)
- en: Appendix A
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A
- en: 'While appendix A should be sufficient to get you up to speed, if you are looking
    for more comprehensive introductions to deep learning, I recommend the following
    books:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然附录 A 应该足以让您跟上进度，但如果您正在寻找更全面的深度学习介绍，我推荐以下书籍：
- en: '*Machine Learning with PyTorch and Scikit-Learn* (2022) by Sebastian Raschka,
    Hayden Liu, and Vahid Mirjalili. ISBN 978-1801819312'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《使用 PyTorch 和 Scikit-Learn 进行机器学习》（2022）由 Sebastian Raschka、Hayden Liu 和 Vahid
    Mirjalili 著。ISBN 978-1801819312
- en: '*Deep Learning with PyTorch* (2021) by Eli Stevens, Luca Antiga, and Thomas
    Viehmann. ISBN 978-1617295263'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《使用 PyTorch 进行深度学习》（2021）由 Eli Stevens、Luca Antiga 和 Thomas Viehmann 著。ISBN
    978-1617295263
- en: 'For a more thorough introduction to the concepts of tensors, readers can find
    a 15-minute video tutorial that I recorded:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更深入的张量概念介绍，读者可以找到我录制的一个 15 分钟的视频教程：
- en: '“Lecture 4.1: Tensors in Deep Learning,” [https://www.youtube.com/watch?v=JXfDlgrfOBY](https://www.youtube.com/watch?v=JXfDlgrfOBY)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “第 4.1 节：深度学习中的张量，” [https://www.youtube.com/watch?v=JXfDlgrfOBY](https://www.youtube.com/watch?v=JXfDlgrfOBY)
- en: If you want to learn more about model evaluation in machine learning, I recommend
    my article
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于机器学习中模型评估的内容，我推荐我的文章
- en: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”
    (2018) by Sebastian Raschka, [https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “机器学习中的模型评估、模型选择和算法选择”（2018）由 Sebastian Raschka 著，[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)
- en: 'For readers who are interested in a refresher or gentle introduction to calculus,
    I’ve written a chapter on calculus that is freely available on my website:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对微积分复习或温和介绍感兴趣的读者，我在我的网站上写了一章关于微积分的内容，免费提供：
- en: “Introduction to Calculus,” by Sebastian Raschka, [https://mng.bz/WEyW](https://mng.bz/WEyW)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “微积分导论，”由 Sebastian Raschka 著，[https://mng.bz/WEyW](https://mng.bz/WEyW)
- en: 'Why does PyTorch not call `optimizer.zero_grad()` automatically for us in the
    background? In some instances, it may be desirable to accumulate the gradients,
    and PyTorch will leave this as an option for us. If you want to learn more about
    gradient accumulation, please see the following article:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 PyTorch 不在后台自动为我们调用 `optimizer.zero_grad()`？在某些情况下，可能希望累积梯度，PyTorch 将将其作为选项留给我们。如果您想了解更多关于梯度累积的信息，请参阅以下文章：
- en: “Finetuning Large Language Models on a Single GPU Using Gradient Accumulation”
    by Sebastian Raschka, [https://mng.bz/8wPD](https://mng.bz/8wPD)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “使用梯度累积在单个 GPU 上微调大型语言模型”由 Sebastian Raschka 著，[https://mng.bz/8wPD](https://mng.bz/8wPD)
- en: 'This appendix covers DDP, which is a popular approach for training deep learning
    models across multiple GPUs. For more advanced use cases where a single model
    doesn’t fit onto the GPU, you may also consider PyTorch’s Fully Sharded Data Parallel
    (FSDP) method, which performs distributed data parallelism and distributes large
    layers across different GPUs. For more information, see this overview with further
    links to the API documentation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖了 DDP，这是一种在多个 GPU 上训练深度学习模型的流行方法。对于单个模型无法适应 GPU 的更高级用例，您还可以考虑 PyTorch 的完全分片数据并行（FSDP）方法，该方法执行分布式数据并行并在不同的
    GPU 上分配大型层。更多信息，请参阅以下概述，其中包含进一步链接到 API 文档：
- en: “Introducing PyTorch Fully Sharded Data Parallel (FSDP) API,” [https://mng.bz/EZJR](https://mng.bz/EZJR)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “介绍 PyTorch 完全分片数据并行（FSDP）API，” [https://mng.bz/EZJR](https://mng.bz/EZJR)
